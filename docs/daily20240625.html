<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240624.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "RaDe-GS: Rasterizing Depth in Gaussian Splatting", "author": "Baowen Zhang and Chuan Fang and Rakesh Shrestha and Yixun Liang and Xiaoxiao Long and Ping Tan", "abstract": "  Gaussian Splatting (GS) has proven to be highly effective in novel view\nsynthesis, achieving high-quality and real-time rendering. However, its\npotential for reconstructing detailed 3D shapes has not been fully explored.\nExisting methods often suffer from limited shape accuracy due to the discrete\nand unstructured nature of Gaussian splats, which complicates the shape\nextraction. While recent techniques like 2D GS have attempted to improve shape\nreconstruction, they often reformulate the Gaussian primitives in ways that\nreduce both rendering quality and computational efficiency. To address these\nproblems, our work introduces a rasterized approach to render the depth maps\nand surface normal maps of general 3D Gaussian splats. Our method not only\nsignificantly enhances shape reconstruction accuracy but also maintains the\ncomputational efficiency intrinsic to Gaussian Splatting. It achieves a Chamfer\ndistance error comparable to NeuraLangelo on the DTU dataset and maintains\nsimilar computational efficiency as the original 3D GS methods. Our method is a\nsignificant advancement in Gaussian Splatting and can be directly integrated\ninto existing Gaussian Splatting-based methods.\n", "link": "http://arxiv.org/abs/2406.01467v2", "date": "2024-06-24", "relevancy": 3.2984, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7822}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6479}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RaDe-GS%3A%20Rasterizing%20Depth%20in%20Gaussian%20Splatting&body=Title%3A%20RaDe-GS%3A%20Rasterizing%20Depth%20in%20Gaussian%20Splatting%0AAuthor%3A%20Baowen%20Zhang%20and%20Chuan%20Fang%20and%20Rakesh%20Shrestha%20and%20Yixun%20Liang%20and%20Xiaoxiao%20Long%20and%20Ping%20Tan%0AAbstract%3A%20%20%20Gaussian%20Splatting%20%28GS%29%20has%20proven%20to%20be%20highly%20effective%20in%20novel%20view%0Asynthesis%2C%20achieving%20high-quality%20and%20real-time%20rendering.%20However%2C%20its%0Apotential%20for%20reconstructing%20detailed%203D%20shapes%20has%20not%20been%20fully%20explored.%0AExisting%20methods%20often%20suffer%20from%20limited%20shape%20accuracy%20due%20to%20the%20discrete%0Aand%20unstructured%20nature%20of%20Gaussian%20splats%2C%20which%20complicates%20the%20shape%0Aextraction.%20While%20recent%20techniques%20like%202D%20GS%20have%20attempted%20to%20improve%20shape%0Areconstruction%2C%20they%20often%20reformulate%20the%20Gaussian%20primitives%20in%20ways%20that%0Areduce%20both%20rendering%20quality%20and%20computational%20efficiency.%20To%20address%20these%0Aproblems%2C%20our%20work%20introduces%20a%20rasterized%20approach%20to%20render%20the%20depth%20maps%0Aand%20surface%20normal%20maps%20of%20general%203D%20Gaussian%20splats.%20Our%20method%20not%20only%0Asignificantly%20enhances%20shape%20reconstruction%20accuracy%20but%20also%20maintains%20the%0Acomputational%20efficiency%20intrinsic%20to%20Gaussian%20Splatting.%20It%20achieves%20a%20Chamfer%0Adistance%20error%20comparable%20to%20NeuraLangelo%20on%20the%20DTU%20dataset%20and%20maintains%0Asimilar%20computational%20efficiency%20as%20the%20original%203D%20GS%20methods.%20Our%20method%20is%20a%0Asignificant%20advancement%20in%20Gaussian%20Splatting%20and%20can%20be%20directly%20integrated%0Ainto%20existing%20Gaussian%20Splatting-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01467v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaDe-GS%253A%2520Rasterizing%2520Depth%2520in%2520Gaussian%2520Splatting%26entry.906535625%3DBaowen%2520Zhang%2520and%2520Chuan%2520Fang%2520and%2520Rakesh%2520Shrestha%2520and%2520Yixun%2520Liang%2520and%2520Xiaoxiao%2520Long%2520and%2520Ping%2520Tan%26entry.1292438233%3D%2520%2520Gaussian%2520Splatting%2520%2528GS%2529%2520has%2520proven%2520to%2520be%2520highly%2520effective%2520in%2520novel%2520view%250Asynthesis%252C%2520achieving%2520high-quality%2520and%2520real-time%2520rendering.%2520However%252C%2520its%250Apotential%2520for%2520reconstructing%2520detailed%25203D%2520shapes%2520has%2520not%2520been%2520fully%2520explored.%250AExisting%2520methods%2520often%2520suffer%2520from%2520limited%2520shape%2520accuracy%2520due%2520to%2520the%2520discrete%250Aand%2520unstructured%2520nature%2520of%2520Gaussian%2520splats%252C%2520which%2520complicates%2520the%2520shape%250Aextraction.%2520While%2520recent%2520techniques%2520like%25202D%2520GS%2520have%2520attempted%2520to%2520improve%2520shape%250Areconstruction%252C%2520they%2520often%2520reformulate%2520the%2520Gaussian%2520primitives%2520in%2520ways%2520that%250Areduce%2520both%2520rendering%2520quality%2520and%2520computational%2520efficiency.%2520To%2520address%2520these%250Aproblems%252C%2520our%2520work%2520introduces%2520a%2520rasterized%2520approach%2520to%2520render%2520the%2520depth%2520maps%250Aand%2520surface%2520normal%2520maps%2520of%2520general%25203D%2520Gaussian%2520splats.%2520Our%2520method%2520not%2520only%250Asignificantly%2520enhances%2520shape%2520reconstruction%2520accuracy%2520but%2520also%2520maintains%2520the%250Acomputational%2520efficiency%2520intrinsic%2520to%2520Gaussian%2520Splatting.%2520It%2520achieves%2520a%2520Chamfer%250Adistance%2520error%2520comparable%2520to%2520NeuraLangelo%2520on%2520the%2520DTU%2520dataset%2520and%2520maintains%250Asimilar%2520computational%2520efficiency%2520as%2520the%2520original%25203D%2520GS%2520methods.%2520Our%2520method%2520is%2520a%250Asignificant%2520advancement%2520in%2520Gaussian%2520Splatting%2520and%2520can%2520be%2520directly%2520integrated%250Ainto%2520existing%2520Gaussian%2520Splatting-based%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01467v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RaDe-GS%3A%20Rasterizing%20Depth%20in%20Gaussian%20Splatting&entry.906535625=Baowen%20Zhang%20and%20Chuan%20Fang%20and%20Rakesh%20Shrestha%20and%20Yixun%20Liang%20and%20Xiaoxiao%20Long%20and%20Ping%20Tan&entry.1292438233=%20%20Gaussian%20Splatting%20%28GS%29%20has%20proven%20to%20be%20highly%20effective%20in%20novel%20view%0Asynthesis%2C%20achieving%20high-quality%20and%20real-time%20rendering.%20However%2C%20its%0Apotential%20for%20reconstructing%20detailed%203D%20shapes%20has%20not%20been%20fully%20explored.%0AExisting%20methods%20often%20suffer%20from%20limited%20shape%20accuracy%20due%20to%20the%20discrete%0Aand%20unstructured%20nature%20of%20Gaussian%20splats%2C%20which%20complicates%20the%20shape%0Aextraction.%20While%20recent%20techniques%20like%202D%20GS%20have%20attempted%20to%20improve%20shape%0Areconstruction%2C%20they%20often%20reformulate%20the%20Gaussian%20primitives%20in%20ways%20that%0Areduce%20both%20rendering%20quality%20and%20computational%20efficiency.%20To%20address%20these%0Aproblems%2C%20our%20work%20introduces%20a%20rasterized%20approach%20to%20render%20the%20depth%20maps%0Aand%20surface%20normal%20maps%20of%20general%203D%20Gaussian%20splats.%20Our%20method%20not%20only%0Asignificantly%20enhances%20shape%20reconstruction%20accuracy%20but%20also%20maintains%20the%0Acomputational%20efficiency%20intrinsic%20to%20Gaussian%20Splatting.%20It%20achieves%20a%20Chamfer%0Adistance%20error%20comparable%20to%20NeuraLangelo%20on%20the%20DTU%20dataset%20and%20maintains%0Asimilar%20computational%20efficiency%20as%20the%20original%203D%20GS%20methods.%20Our%20method%20is%20a%0Asignificant%20advancement%20in%20Gaussian%20Splatting%20and%20can%20be%20directly%20integrated%0Ainto%20existing%20Gaussian%20Splatting-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01467v2&entry.124074799=Read"},
{"title": "\u03bc-Net: A Deep Learning-Based Architecture for \u03bc-CT Segmentation", "author": "Pierangela Bruno and Edoardo De Rose and Carlo Adornetto and Francesco Calimeri and Sandro Donato and Raffaele Giuseppe Agostino and Daniela Amelio and Riccardo Barberi and Maria Carmela Cerra and Maria Caterina Crocco and Mariacristina Filice and Raffaele Filosa and Gianluigi Greco and Sandra Imbrogno and Vincenzo Formoso", "abstract": "  X-ray computed microtomography ({\\mu}-CT) is a non-destructive technique that\ncan generate high-resolution 3D images of the internal anatomy of medical and\nbiological samples. These images enable clinicians to examine internal anatomy\nand gain insights into the disease or anatomical morphology. However,\nextracting relevant information from 3D images requires semantic segmentation\nof the regions of interest, which is usually done manually and results\ntime-consuming and tedious. In this work, we propose a novel framework that\nuses a convolutional neural network (CNN) to automatically segment the full\nmorphology of the heart of Carassius auratus. The framework employs an\noptimized 2D CNN architecture that can infer a 3D segmentation of the sample,\navoiding the high computational cost of a 3D CNN architecture. We tackle the\nchallenges of handling large and high-resoluted image data (over a thousand\npixels in each dimension) and a small training database (only three samples) by\nproposing a standard protocol for data normalization and processing. Moreover,\nwe investigate how the noise, contrast, and spatial resolution of the sample\nand the training of the architecture are affected by the reconstruction\ntechnique, which depends on the number of input images. Experiments show that\nour framework significantly reduces the time required to segment new samples,\nallowing a faster microtomography analysis of the Carassius auratus heart\nshape. Furthermore, our framework can work with any bio-image (biological and\nmedical) from {\\mu}-CT with high-resolution and small dataset size\n", "link": "http://arxiv.org/abs/2406.16724v1", "date": "2024-06-24", "relevancy": 2.7917, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5768}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5491}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%CE%BC-Net%3A%20A%20Deep%20Learning-Based%20Architecture%20for%20%CE%BC-CT%20Segmentation&body=Title%3A%20%CE%BC-Net%3A%20A%20Deep%20Learning-Based%20Architecture%20for%20%CE%BC-CT%20Segmentation%0AAuthor%3A%20Pierangela%20Bruno%20and%20Edoardo%20De%20Rose%20and%20Carlo%20Adornetto%20and%20Francesco%20Calimeri%20and%20Sandro%20Donato%20and%20Raffaele%20Giuseppe%20Agostino%20and%20Daniela%20Amelio%20and%20Riccardo%20Barberi%20and%20Maria%20Carmela%20Cerra%20and%20Maria%20Caterina%20Crocco%20and%20Mariacristina%20Filice%20and%20Raffaele%20Filosa%20and%20Gianluigi%20Greco%20and%20Sandra%20Imbrogno%20and%20Vincenzo%20Formoso%0AAbstract%3A%20%20%20X-ray%20computed%20microtomography%20%28%7B%5Cmu%7D-CT%29%20is%20a%20non-destructive%20technique%20that%0Acan%20generate%20high-resolution%203D%20images%20of%20the%20internal%20anatomy%20of%20medical%20and%0Abiological%20samples.%20These%20images%20enable%20clinicians%20to%20examine%20internal%20anatomy%0Aand%20gain%20insights%20into%20the%20disease%20or%20anatomical%20morphology.%20However%2C%0Aextracting%20relevant%20information%20from%203D%20images%20requires%20semantic%20segmentation%0Aof%20the%20regions%20of%20interest%2C%20which%20is%20usually%20done%20manually%20and%20results%0Atime-consuming%20and%20tedious.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%20that%0Auses%20a%20convolutional%20neural%20network%20%28CNN%29%20to%20automatically%20segment%20the%20full%0Amorphology%20of%20the%20heart%20of%20Carassius%20auratus.%20The%20framework%20employs%20an%0Aoptimized%202D%20CNN%20architecture%20that%20can%20infer%20a%203D%20segmentation%20of%20the%20sample%2C%0Aavoiding%20the%20high%20computational%20cost%20of%20a%203D%20CNN%20architecture.%20We%20tackle%20the%0Achallenges%20of%20handling%20large%20and%20high-resoluted%20image%20data%20%28over%20a%20thousand%0Apixels%20in%20each%20dimension%29%20and%20a%20small%20training%20database%20%28only%20three%20samples%29%20by%0Aproposing%20a%20standard%20protocol%20for%20data%20normalization%20and%20processing.%20Moreover%2C%0Awe%20investigate%20how%20the%20noise%2C%20contrast%2C%20and%20spatial%20resolution%20of%20the%20sample%0Aand%20the%20training%20of%20the%20architecture%20are%20affected%20by%20the%20reconstruction%0Atechnique%2C%20which%20depends%20on%20the%20number%20of%20input%20images.%20Experiments%20show%20that%0Aour%20framework%20significantly%20reduces%20the%20time%20required%20to%20segment%20new%20samples%2C%0Aallowing%20a%20faster%20microtomography%20analysis%20of%20the%20Carassius%20auratus%20heart%0Ashape.%20Furthermore%2C%20our%20framework%20can%20work%20with%20any%20bio-image%20%28biological%20and%0Amedical%29%20from%20%7B%5Cmu%7D-CT%20with%20high-resolution%20and%20small%20dataset%20size%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%25CE%25BC-Net%253A%2520A%2520Deep%2520Learning-Based%2520Architecture%2520for%2520%25CE%25BC-CT%2520Segmentation%26entry.906535625%3DPierangela%2520Bruno%2520and%2520Edoardo%2520De%2520Rose%2520and%2520Carlo%2520Adornetto%2520and%2520Francesco%2520Calimeri%2520and%2520Sandro%2520Donato%2520and%2520Raffaele%2520Giuseppe%2520Agostino%2520and%2520Daniela%2520Amelio%2520and%2520Riccardo%2520Barberi%2520and%2520Maria%2520Carmela%2520Cerra%2520and%2520Maria%2520Caterina%2520Crocco%2520and%2520Mariacristina%2520Filice%2520and%2520Raffaele%2520Filosa%2520and%2520Gianluigi%2520Greco%2520and%2520Sandra%2520Imbrogno%2520and%2520Vincenzo%2520Formoso%26entry.1292438233%3D%2520%2520X-ray%2520computed%2520microtomography%2520%2528%257B%255Cmu%257D-CT%2529%2520is%2520a%2520non-destructive%2520technique%2520that%250Acan%2520generate%2520high-resolution%25203D%2520images%2520of%2520the%2520internal%2520anatomy%2520of%2520medical%2520and%250Abiological%2520samples.%2520These%2520images%2520enable%2520clinicians%2520to%2520examine%2520internal%2520anatomy%250Aand%2520gain%2520insights%2520into%2520the%2520disease%2520or%2520anatomical%2520morphology.%2520However%252C%250Aextracting%2520relevant%2520information%2520from%25203D%2520images%2520requires%2520semantic%2520segmentation%250Aof%2520the%2520regions%2520of%2520interest%252C%2520which%2520is%2520usually%2520done%2520manually%2520and%2520results%250Atime-consuming%2520and%2520tedious.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%250Auses%2520a%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520to%2520automatically%2520segment%2520the%2520full%250Amorphology%2520of%2520the%2520heart%2520of%2520Carassius%2520auratus.%2520The%2520framework%2520employs%2520an%250Aoptimized%25202D%2520CNN%2520architecture%2520that%2520can%2520infer%2520a%25203D%2520segmentation%2520of%2520the%2520sample%252C%250Aavoiding%2520the%2520high%2520computational%2520cost%2520of%2520a%25203D%2520CNN%2520architecture.%2520We%2520tackle%2520the%250Achallenges%2520of%2520handling%2520large%2520and%2520high-resoluted%2520image%2520data%2520%2528over%2520a%2520thousand%250Apixels%2520in%2520each%2520dimension%2529%2520and%2520a%2520small%2520training%2520database%2520%2528only%2520three%2520samples%2529%2520by%250Aproposing%2520a%2520standard%2520protocol%2520for%2520data%2520normalization%2520and%2520processing.%2520Moreover%252C%250Awe%2520investigate%2520how%2520the%2520noise%252C%2520contrast%252C%2520and%2520spatial%2520resolution%2520of%2520the%2520sample%250Aand%2520the%2520training%2520of%2520the%2520architecture%2520are%2520affected%2520by%2520the%2520reconstruction%250Atechnique%252C%2520which%2520depends%2520on%2520the%2520number%2520of%2520input%2520images.%2520Experiments%2520show%2520that%250Aour%2520framework%2520significantly%2520reduces%2520the%2520time%2520required%2520to%2520segment%2520new%2520samples%252C%250Aallowing%2520a%2520faster%2520microtomography%2520analysis%2520of%2520the%2520Carassius%2520auratus%2520heart%250Ashape.%2520Furthermore%252C%2520our%2520framework%2520can%2520work%2520with%2520any%2520bio-image%2520%2528biological%2520and%250Amedical%2529%2520from%2520%257B%255Cmu%257D-CT%2520with%2520high-resolution%2520and%2520small%2520dataset%2520size%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%CE%BC-Net%3A%20A%20Deep%20Learning-Based%20Architecture%20for%20%CE%BC-CT%20Segmentation&entry.906535625=Pierangela%20Bruno%20and%20Edoardo%20De%20Rose%20and%20Carlo%20Adornetto%20and%20Francesco%20Calimeri%20and%20Sandro%20Donato%20and%20Raffaele%20Giuseppe%20Agostino%20and%20Daniela%20Amelio%20and%20Riccardo%20Barberi%20and%20Maria%20Carmela%20Cerra%20and%20Maria%20Caterina%20Crocco%20and%20Mariacristina%20Filice%20and%20Raffaele%20Filosa%20and%20Gianluigi%20Greco%20and%20Sandra%20Imbrogno%20and%20Vincenzo%20Formoso&entry.1292438233=%20%20X-ray%20computed%20microtomography%20%28%7B%5Cmu%7D-CT%29%20is%20a%20non-destructive%20technique%20that%0Acan%20generate%20high-resolution%203D%20images%20of%20the%20internal%20anatomy%20of%20medical%20and%0Abiological%20samples.%20These%20images%20enable%20clinicians%20to%20examine%20internal%20anatomy%0Aand%20gain%20insights%20into%20the%20disease%20or%20anatomical%20morphology.%20However%2C%0Aextracting%20relevant%20information%20from%203D%20images%20requires%20semantic%20segmentation%0Aof%20the%20regions%20of%20interest%2C%20which%20is%20usually%20done%20manually%20and%20results%0Atime-consuming%20and%20tedious.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%20that%0Auses%20a%20convolutional%20neural%20network%20%28CNN%29%20to%20automatically%20segment%20the%20full%0Amorphology%20of%20the%20heart%20of%20Carassius%20auratus.%20The%20framework%20employs%20an%0Aoptimized%202D%20CNN%20architecture%20that%20can%20infer%20a%203D%20segmentation%20of%20the%20sample%2C%0Aavoiding%20the%20high%20computational%20cost%20of%20a%203D%20CNN%20architecture.%20We%20tackle%20the%0Achallenges%20of%20handling%20large%20and%20high-resoluted%20image%20data%20%28over%20a%20thousand%0Apixels%20in%20each%20dimension%29%20and%20a%20small%20training%20database%20%28only%20three%20samples%29%20by%0Aproposing%20a%20standard%20protocol%20for%20data%20normalization%20and%20processing.%20Moreover%2C%0Awe%20investigate%20how%20the%20noise%2C%20contrast%2C%20and%20spatial%20resolution%20of%20the%20sample%0Aand%20the%20training%20of%20the%20architecture%20are%20affected%20by%20the%20reconstruction%0Atechnique%2C%20which%20depends%20on%20the%20number%20of%20input%20images.%20Experiments%20show%20that%0Aour%20framework%20significantly%20reduces%20the%20time%20required%20to%20segment%20new%20samples%2C%0Aallowing%20a%20faster%20microtomography%20analysis%20of%20the%20Carassius%20auratus%20heart%0Ashape.%20Furthermore%2C%20our%20framework%20can%20work%20with%20any%20bio-image%20%28biological%20and%0Amedical%29%20from%20%7B%5Cmu%7D-CT%20with%20high-resolution%20and%20small%20dataset%20size%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16724v1&entry.124074799=Read"},
{"title": "MLAAN: Scaling Supervised Local Learning with Multilaminar Leap\n  Augmented Auxiliary Network", "author": "Yuming Zhang and Shouxin Zhang and Peizhe Wang and Feiyu Zhu and Dongzhi Guan and Jiabin Liu and Changpeng Cai", "abstract": "  End-to-end (E2E) training approaches are commonly plagued by high memory\nconsumption, reduced efficiency in training, challenges in model\nparallelization, and suboptimal biocompatibility. Local learning is considered\na novel interactive training method that holds promise as an alternative to\nE2E. Nonetheless, conventional local learning methods fall short in achieving\nhigh model accuracy due to inadequate local inter-module interactions. In this\npaper, we introduce a new model known as the Scaling Supervised Local Learning\nwith Multilaminar Leap Augmented Auxiliary Network (MLAAN). MLAAN features an\ninnovative supervised local learning approach coupled with a robust\nreinforcement module. This dual-component design enables the MLAAN to integrate\nsmoothly with established local learning techniques, thereby enhancing the\nefficacy of the foundational methods. The method simultaneously acquires the\nlocal and global features of the model separately by constructing an\nindependent auxiliary network and a cascade auxiliary network on the one hand\nand incorporates a leap augmented module, which serves to counteract the\nreduced learning capacity often associated with weaker supervision. This\narchitecture not only augments the exchange of information amongst the local\nmodules but also effectively mitigates the model's tendency toward myopia. The\nexperimental evaluations conducted on four benchmark datasets, CIFAR-10,\nSTL-10, SVHN, and ImageNet, demonstrate that the integration of MLAAN with\nexisting supervised local learning methods significantly enhances the original\nmethodologies. Of particular note, MLAAN enables local learning methods to\ncomprehensively outperform end-to-end training approaches in terms of optimal\nperformance while saving GPU memory.\n", "link": "http://arxiv.org/abs/2406.16633v1", "date": "2024-06-24", "relevancy": 2.7843, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5714}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5597}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLAAN%3A%20Scaling%20Supervised%20Local%20Learning%20with%20Multilaminar%20Leap%0A%20%20Augmented%20Auxiliary%20Network&body=Title%3A%20MLAAN%3A%20Scaling%20Supervised%20Local%20Learning%20with%20Multilaminar%20Leap%0A%20%20Augmented%20Auxiliary%20Network%0AAuthor%3A%20Yuming%20Zhang%20and%20Shouxin%20Zhang%20and%20Peizhe%20Wang%20and%20Feiyu%20Zhu%20and%20Dongzhi%20Guan%20and%20Jiabin%20Liu%20and%20Changpeng%20Cai%0AAbstract%3A%20%20%20End-to-end%20%28E2E%29%20training%20approaches%20are%20commonly%20plagued%20by%20high%20memory%0Aconsumption%2C%20reduced%20efficiency%20in%20training%2C%20challenges%20in%20model%0Aparallelization%2C%20and%20suboptimal%20biocompatibility.%20Local%20learning%20is%20considered%0Aa%20novel%20interactive%20training%20method%20that%20holds%20promise%20as%20an%20alternative%20to%0AE2E.%20Nonetheless%2C%20conventional%20local%20learning%20methods%20fall%20short%20in%20achieving%0Ahigh%20model%20accuracy%20due%20to%20inadequate%20local%20inter-module%20interactions.%20In%20this%0Apaper%2C%20we%20introduce%20a%20new%20model%20known%20as%20the%20Scaling%20Supervised%20Local%20Learning%0Awith%20Multilaminar%20Leap%20Augmented%20Auxiliary%20Network%20%28MLAAN%29.%20MLAAN%20features%20an%0Ainnovative%20supervised%20local%20learning%20approach%20coupled%20with%20a%20robust%0Areinforcement%20module.%20This%20dual-component%20design%20enables%20the%20MLAAN%20to%20integrate%0Asmoothly%20with%20established%20local%20learning%20techniques%2C%20thereby%20enhancing%20the%0Aefficacy%20of%20the%20foundational%20methods.%20The%20method%20simultaneously%20acquires%20the%0Alocal%20and%20global%20features%20of%20the%20model%20separately%20by%20constructing%20an%0Aindependent%20auxiliary%20network%20and%20a%20cascade%20auxiliary%20network%20on%20the%20one%20hand%0Aand%20incorporates%20a%20leap%20augmented%20module%2C%20which%20serves%20to%20counteract%20the%0Areduced%20learning%20capacity%20often%20associated%20with%20weaker%20supervision.%20This%0Aarchitecture%20not%20only%20augments%20the%20exchange%20of%20information%20amongst%20the%20local%0Amodules%20but%20also%20effectively%20mitigates%20the%20model%27s%20tendency%20toward%20myopia.%20The%0Aexperimental%20evaluations%20conducted%20on%20four%20benchmark%20datasets%2C%20CIFAR-10%2C%0ASTL-10%2C%20SVHN%2C%20and%20ImageNet%2C%20demonstrate%20that%20the%20integration%20of%20MLAAN%20with%0Aexisting%20supervised%20local%20learning%20methods%20significantly%20enhances%20the%20original%0Amethodologies.%20Of%20particular%20note%2C%20MLAAN%20enables%20local%20learning%20methods%20to%0Acomprehensively%20outperform%20end-to-end%20training%20approaches%20in%20terms%20of%20optimal%0Aperformance%20while%20saving%20GPU%20memory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLAAN%253A%2520Scaling%2520Supervised%2520Local%2520Learning%2520with%2520Multilaminar%2520Leap%250A%2520%2520Augmented%2520Auxiliary%2520Network%26entry.906535625%3DYuming%2520Zhang%2520and%2520Shouxin%2520Zhang%2520and%2520Peizhe%2520Wang%2520and%2520Feiyu%2520Zhu%2520and%2520Dongzhi%2520Guan%2520and%2520Jiabin%2520Liu%2520and%2520Changpeng%2520Cai%26entry.1292438233%3D%2520%2520End-to-end%2520%2528E2E%2529%2520training%2520approaches%2520are%2520commonly%2520plagued%2520by%2520high%2520memory%250Aconsumption%252C%2520reduced%2520efficiency%2520in%2520training%252C%2520challenges%2520in%2520model%250Aparallelization%252C%2520and%2520suboptimal%2520biocompatibility.%2520Local%2520learning%2520is%2520considered%250Aa%2520novel%2520interactive%2520training%2520method%2520that%2520holds%2520promise%2520as%2520an%2520alternative%2520to%250AE2E.%2520Nonetheless%252C%2520conventional%2520local%2520learning%2520methods%2520fall%2520short%2520in%2520achieving%250Ahigh%2520model%2520accuracy%2520due%2520to%2520inadequate%2520local%2520inter-module%2520interactions.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520new%2520model%2520known%2520as%2520the%2520Scaling%2520Supervised%2520Local%2520Learning%250Awith%2520Multilaminar%2520Leap%2520Augmented%2520Auxiliary%2520Network%2520%2528MLAAN%2529.%2520MLAAN%2520features%2520an%250Ainnovative%2520supervised%2520local%2520learning%2520approach%2520coupled%2520with%2520a%2520robust%250Areinforcement%2520module.%2520This%2520dual-component%2520design%2520enables%2520the%2520MLAAN%2520to%2520integrate%250Asmoothly%2520with%2520established%2520local%2520learning%2520techniques%252C%2520thereby%2520enhancing%2520the%250Aefficacy%2520of%2520the%2520foundational%2520methods.%2520The%2520method%2520simultaneously%2520acquires%2520the%250Alocal%2520and%2520global%2520features%2520of%2520the%2520model%2520separately%2520by%2520constructing%2520an%250Aindependent%2520auxiliary%2520network%2520and%2520a%2520cascade%2520auxiliary%2520network%2520on%2520the%2520one%2520hand%250Aand%2520incorporates%2520a%2520leap%2520augmented%2520module%252C%2520which%2520serves%2520to%2520counteract%2520the%250Areduced%2520learning%2520capacity%2520often%2520associated%2520with%2520weaker%2520supervision.%2520This%250Aarchitecture%2520not%2520only%2520augments%2520the%2520exchange%2520of%2520information%2520amongst%2520the%2520local%250Amodules%2520but%2520also%2520effectively%2520mitigates%2520the%2520model%2527s%2520tendency%2520toward%2520myopia.%2520The%250Aexperimental%2520evaluations%2520conducted%2520on%2520four%2520benchmark%2520datasets%252C%2520CIFAR-10%252C%250ASTL-10%252C%2520SVHN%252C%2520and%2520ImageNet%252C%2520demonstrate%2520that%2520the%2520integration%2520of%2520MLAAN%2520with%250Aexisting%2520supervised%2520local%2520learning%2520methods%2520significantly%2520enhances%2520the%2520original%250Amethodologies.%2520Of%2520particular%2520note%252C%2520MLAAN%2520enables%2520local%2520learning%2520methods%2520to%250Acomprehensively%2520outperform%2520end-to-end%2520training%2520approaches%2520in%2520terms%2520of%2520optimal%250Aperformance%2520while%2520saving%2520GPU%2520memory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLAAN%3A%20Scaling%20Supervised%20Local%20Learning%20with%20Multilaminar%20Leap%0A%20%20Augmented%20Auxiliary%20Network&entry.906535625=Yuming%20Zhang%20and%20Shouxin%20Zhang%20and%20Peizhe%20Wang%20and%20Feiyu%20Zhu%20and%20Dongzhi%20Guan%20and%20Jiabin%20Liu%20and%20Changpeng%20Cai&entry.1292438233=%20%20End-to-end%20%28E2E%29%20training%20approaches%20are%20commonly%20plagued%20by%20high%20memory%0Aconsumption%2C%20reduced%20efficiency%20in%20training%2C%20challenges%20in%20model%0Aparallelization%2C%20and%20suboptimal%20biocompatibility.%20Local%20learning%20is%20considered%0Aa%20novel%20interactive%20training%20method%20that%20holds%20promise%20as%20an%20alternative%20to%0AE2E.%20Nonetheless%2C%20conventional%20local%20learning%20methods%20fall%20short%20in%20achieving%0Ahigh%20model%20accuracy%20due%20to%20inadequate%20local%20inter-module%20interactions.%20In%20this%0Apaper%2C%20we%20introduce%20a%20new%20model%20known%20as%20the%20Scaling%20Supervised%20Local%20Learning%0Awith%20Multilaminar%20Leap%20Augmented%20Auxiliary%20Network%20%28MLAAN%29.%20MLAAN%20features%20an%0Ainnovative%20supervised%20local%20learning%20approach%20coupled%20with%20a%20robust%0Areinforcement%20module.%20This%20dual-component%20design%20enables%20the%20MLAAN%20to%20integrate%0Asmoothly%20with%20established%20local%20learning%20techniques%2C%20thereby%20enhancing%20the%0Aefficacy%20of%20the%20foundational%20methods.%20The%20method%20simultaneously%20acquires%20the%0Alocal%20and%20global%20features%20of%20the%20model%20separately%20by%20constructing%20an%0Aindependent%20auxiliary%20network%20and%20a%20cascade%20auxiliary%20network%20on%20the%20one%20hand%0Aand%20incorporates%20a%20leap%20augmented%20module%2C%20which%20serves%20to%20counteract%20the%0Areduced%20learning%20capacity%20often%20associated%20with%20weaker%20supervision.%20This%0Aarchitecture%20not%20only%20augments%20the%20exchange%20of%20information%20amongst%20the%20local%0Amodules%20but%20also%20effectively%20mitigates%20the%20model%27s%20tendency%20toward%20myopia.%20The%0Aexperimental%20evaluations%20conducted%20on%20four%20benchmark%20datasets%2C%20CIFAR-10%2C%0ASTL-10%2C%20SVHN%2C%20and%20ImageNet%2C%20demonstrate%20that%20the%20integration%20of%20MLAAN%20with%0Aexisting%20supervised%20local%20learning%20methods%20significantly%20enhances%20the%20original%0Amethodologies.%20Of%20particular%20note%2C%20MLAAN%20enables%20local%20learning%20methods%20to%0Acomprehensively%20outperform%20end-to-end%20training%20approaches%20in%20terms%20of%20optimal%0Aperformance%20while%20saving%20GPU%20memory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16633v1&entry.124074799=Read"},
{"title": "GIM: A Million-scale Benchmark for Generative Image Manipulation\n  Detection and Localization", "author": "Yirui Chen and Xudong Huang and Quan Zhang and Wei Li and Mingjian Zhu and Qiangyu Yan and Simiao Li and Hanting Chen and Hailin Hu and Jie Yang and Wei Liu and Jie Hu", "abstract": "  The extraordinary ability of generative models emerges as a new trend in\nimage editing and generating realistic images, posing a serious threat to the\ntrustworthiness of multimedia data and driving the research of image\nmanipulation detection and location(IMDL). However, the lack of a large-scale\ndata foundation makes IMDL task unattainable. In this paper, a local\nmanipulation pipeline is designed, incorporating the powerful SAM, ChatGPT and\ngenerative models. Upon this basis, We propose the GIM dataset, which has the\nfollowing advantages: 1) Large scale, including over one million pairs of\nAI-manipulated images and real images. 2) Rich Image Content, encompassing a\nbroad range of image classes 3) Diverse Generative Manipulation, manipulated\nimages with state-of-the-art generators and various manipulation tasks. The\naforementioned advantages allow for a more comprehensive evaluation of IMDL\nmethods, extending their applicability to diverse images. We introduce two\nbenchmark settings to evaluate the generalization capability and comprehensive\nperformance of baseline methods. In addition, we propose a novel IMDL\nframework, termed GIMFormer, which consists of a ShadowTracer,\nFrequency-Spatial Block (FSB), and a Multi-window Anomalous Modelling (MWAM)\nModule. Extensive experiments on the GIM demonstrate that GIMFormer surpasses\nprevious state-of-the-art works significantly on two different benchmarks.\n", "link": "http://arxiv.org/abs/2406.16531v1", "date": "2024-06-24", "relevancy": 2.7764, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5662}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5524}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GIM%3A%20A%20Million-scale%20Benchmark%20for%20Generative%20Image%20Manipulation%0A%20%20Detection%20and%20Localization&body=Title%3A%20GIM%3A%20A%20Million-scale%20Benchmark%20for%20Generative%20Image%20Manipulation%0A%20%20Detection%20and%20Localization%0AAuthor%3A%20Yirui%20Chen%20and%20Xudong%20Huang%20and%20Quan%20Zhang%20and%20Wei%20Li%20and%20Mingjian%20Zhu%20and%20Qiangyu%20Yan%20and%20Simiao%20Li%20and%20Hanting%20Chen%20and%20Hailin%20Hu%20and%20Jie%20Yang%20and%20Wei%20Liu%20and%20Jie%20Hu%0AAbstract%3A%20%20%20The%20extraordinary%20ability%20of%20generative%20models%20emerges%20as%20a%20new%20trend%20in%0Aimage%20editing%20and%20generating%20realistic%20images%2C%20posing%20a%20serious%20threat%20to%20the%0Atrustworthiness%20of%20multimedia%20data%20and%20driving%20the%20research%20of%20image%0Amanipulation%20detection%20and%20location%28IMDL%29.%20However%2C%20the%20lack%20of%20a%20large-scale%0Adata%20foundation%20makes%20IMDL%20task%20unattainable.%20In%20this%20paper%2C%20a%20local%0Amanipulation%20pipeline%20is%20designed%2C%20incorporating%20the%20powerful%20SAM%2C%20ChatGPT%20and%0Agenerative%20models.%20Upon%20this%20basis%2C%20We%20propose%20the%20GIM%20dataset%2C%20which%20has%20the%0Afollowing%20advantages%3A%201%29%20Large%20scale%2C%20including%20over%20one%20million%20pairs%20of%0AAI-manipulated%20images%20and%20real%20images.%202%29%20Rich%20Image%20Content%2C%20encompassing%20a%0Abroad%20range%20of%20image%20classes%203%29%20Diverse%20Generative%20Manipulation%2C%20manipulated%0Aimages%20with%20state-of-the-art%20generators%20and%20various%20manipulation%20tasks.%20The%0Aaforementioned%20advantages%20allow%20for%20a%20more%20comprehensive%20evaluation%20of%20IMDL%0Amethods%2C%20extending%20their%20applicability%20to%20diverse%20images.%20We%20introduce%20two%0Abenchmark%20settings%20to%20evaluate%20the%20generalization%20capability%20and%20comprehensive%0Aperformance%20of%20baseline%20methods.%20In%20addition%2C%20we%20propose%20a%20novel%20IMDL%0Aframework%2C%20termed%20GIMFormer%2C%20which%20consists%20of%20a%20ShadowTracer%2C%0AFrequency-Spatial%20Block%20%28FSB%29%2C%20and%20a%20Multi-window%20Anomalous%20Modelling%20%28MWAM%29%0AModule.%20Extensive%20experiments%20on%20the%20GIM%20demonstrate%20that%20GIMFormer%20surpasses%0Aprevious%20state-of-the-art%20works%20significantly%20on%20two%20different%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGIM%253A%2520A%2520Million-scale%2520Benchmark%2520for%2520Generative%2520Image%2520Manipulation%250A%2520%2520Detection%2520and%2520Localization%26entry.906535625%3DYirui%2520Chen%2520and%2520Xudong%2520Huang%2520and%2520Quan%2520Zhang%2520and%2520Wei%2520Li%2520and%2520Mingjian%2520Zhu%2520and%2520Qiangyu%2520Yan%2520and%2520Simiao%2520Li%2520and%2520Hanting%2520Chen%2520and%2520Hailin%2520Hu%2520and%2520Jie%2520Yang%2520and%2520Wei%2520Liu%2520and%2520Jie%2520Hu%26entry.1292438233%3D%2520%2520The%2520extraordinary%2520ability%2520of%2520generative%2520models%2520emerges%2520as%2520a%2520new%2520trend%2520in%250Aimage%2520editing%2520and%2520generating%2520realistic%2520images%252C%2520posing%2520a%2520serious%2520threat%2520to%2520the%250Atrustworthiness%2520of%2520multimedia%2520data%2520and%2520driving%2520the%2520research%2520of%2520image%250Amanipulation%2520detection%2520and%2520location%2528IMDL%2529.%2520However%252C%2520the%2520lack%2520of%2520a%2520large-scale%250Adata%2520foundation%2520makes%2520IMDL%2520task%2520unattainable.%2520In%2520this%2520paper%252C%2520a%2520local%250Amanipulation%2520pipeline%2520is%2520designed%252C%2520incorporating%2520the%2520powerful%2520SAM%252C%2520ChatGPT%2520and%250Agenerative%2520models.%2520Upon%2520this%2520basis%252C%2520We%2520propose%2520the%2520GIM%2520dataset%252C%2520which%2520has%2520the%250Afollowing%2520advantages%253A%25201%2529%2520Large%2520scale%252C%2520including%2520over%2520one%2520million%2520pairs%2520of%250AAI-manipulated%2520images%2520and%2520real%2520images.%25202%2529%2520Rich%2520Image%2520Content%252C%2520encompassing%2520a%250Abroad%2520range%2520of%2520image%2520classes%25203%2529%2520Diverse%2520Generative%2520Manipulation%252C%2520manipulated%250Aimages%2520with%2520state-of-the-art%2520generators%2520and%2520various%2520manipulation%2520tasks.%2520The%250Aaforementioned%2520advantages%2520allow%2520for%2520a%2520more%2520comprehensive%2520evaluation%2520of%2520IMDL%250Amethods%252C%2520extending%2520their%2520applicability%2520to%2520diverse%2520images.%2520We%2520introduce%2520two%250Abenchmark%2520settings%2520to%2520evaluate%2520the%2520generalization%2520capability%2520and%2520comprehensive%250Aperformance%2520of%2520baseline%2520methods.%2520In%2520addition%252C%2520we%2520propose%2520a%2520novel%2520IMDL%250Aframework%252C%2520termed%2520GIMFormer%252C%2520which%2520consists%2520of%2520a%2520ShadowTracer%252C%250AFrequency-Spatial%2520Block%2520%2528FSB%2529%252C%2520and%2520a%2520Multi-window%2520Anomalous%2520Modelling%2520%2528MWAM%2529%250AModule.%2520Extensive%2520experiments%2520on%2520the%2520GIM%2520demonstrate%2520that%2520GIMFormer%2520surpasses%250Aprevious%2520state-of-the-art%2520works%2520significantly%2520on%2520two%2520different%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GIM%3A%20A%20Million-scale%20Benchmark%20for%20Generative%20Image%20Manipulation%0A%20%20Detection%20and%20Localization&entry.906535625=Yirui%20Chen%20and%20Xudong%20Huang%20and%20Quan%20Zhang%20and%20Wei%20Li%20and%20Mingjian%20Zhu%20and%20Qiangyu%20Yan%20and%20Simiao%20Li%20and%20Hanting%20Chen%20and%20Hailin%20Hu%20and%20Jie%20Yang%20and%20Wei%20Liu%20and%20Jie%20Hu&entry.1292438233=%20%20The%20extraordinary%20ability%20of%20generative%20models%20emerges%20as%20a%20new%20trend%20in%0Aimage%20editing%20and%20generating%20realistic%20images%2C%20posing%20a%20serious%20threat%20to%20the%0Atrustworthiness%20of%20multimedia%20data%20and%20driving%20the%20research%20of%20image%0Amanipulation%20detection%20and%20location%28IMDL%29.%20However%2C%20the%20lack%20of%20a%20large-scale%0Adata%20foundation%20makes%20IMDL%20task%20unattainable.%20In%20this%20paper%2C%20a%20local%0Amanipulation%20pipeline%20is%20designed%2C%20incorporating%20the%20powerful%20SAM%2C%20ChatGPT%20and%0Agenerative%20models.%20Upon%20this%20basis%2C%20We%20propose%20the%20GIM%20dataset%2C%20which%20has%20the%0Afollowing%20advantages%3A%201%29%20Large%20scale%2C%20including%20over%20one%20million%20pairs%20of%0AAI-manipulated%20images%20and%20real%20images.%202%29%20Rich%20Image%20Content%2C%20encompassing%20a%0Abroad%20range%20of%20image%20classes%203%29%20Diverse%20Generative%20Manipulation%2C%20manipulated%0Aimages%20with%20state-of-the-art%20generators%20and%20various%20manipulation%20tasks.%20The%0Aaforementioned%20advantages%20allow%20for%20a%20more%20comprehensive%20evaluation%20of%20IMDL%0Amethods%2C%20extending%20their%20applicability%20to%20diverse%20images.%20We%20introduce%20two%0Abenchmark%20settings%20to%20evaluate%20the%20generalization%20capability%20and%20comprehensive%0Aperformance%20of%20baseline%20methods.%20In%20addition%2C%20we%20propose%20a%20novel%20IMDL%0Aframework%2C%20termed%20GIMFormer%2C%20which%20consists%20of%20a%20ShadowTracer%2C%0AFrequency-Spatial%20Block%20%28FSB%29%2C%20and%20a%20Multi-window%20Anomalous%20Modelling%20%28MWAM%29%0AModule.%20Extensive%20experiments%20on%20the%20GIM%20demonstrate%20that%20GIMFormer%20surpasses%0Aprevious%20state-of-the-art%20works%20significantly%20on%20two%20different%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16531v1&entry.124074799=Read"},
{"title": "State Representation Learning Using an Unbalanced Atlas", "author": "Li Meng and Morten Goodwin and Anis Yazidi and Paal Engelstad", "abstract": "  The manifold hypothesis posits that high-dimensional data often lies on a\nlower-dimensional manifold and that utilizing this manifold as the target space\nyields more efficient representations. While numerous traditional\nmanifold-based techniques exist for dimensionality reduction, their application\nin self-supervised learning has witnessed slow progress. The recent MSimCLR\nmethod combines manifold encoding with SimCLR but requires extremely low target\nencoding dimensions to outperform SimCLR, limiting its applicability. This\npaper introduces a novel learning paradigm using an unbalanced atlas (UA),\ncapable of surpassing state-of-the-art self-supervised learning approaches. We\ninvestigated and engineered the DeepInfomax with an unbalanced atlas (DIM-UA)\nmethod by adapting the Spatiotemporal DeepInfomax (ST-DIM) framework to align\nwith our proposed UA paradigm. The efficacy of DIM-UA is demonstrated through\ntraining and evaluation on the Atari Annotated RAM Interface (AtariARI)\nbenchmark, a modified version of the Atari 2600 framework that produces\nannotated image samples for representation learning. The UA paradigm improves\nexisting algorithms significantly as the number of target encoding dimensions\ngrows. For instance, the mean F1 score averaged over categories of DIM-UA is\n~75% compared to ~70% of ST-DIM when using 16384 hidden units.\n", "link": "http://arxiv.org/abs/2305.10267v3", "date": "2024-06-24", "relevancy": 2.7714, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5837}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5406}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20State%20Representation%20Learning%20Using%20an%20Unbalanced%20Atlas&body=Title%3A%20State%20Representation%20Learning%20Using%20an%20Unbalanced%20Atlas%0AAuthor%3A%20Li%20Meng%20and%20Morten%20Goodwin%20and%20Anis%20Yazidi%20and%20Paal%20Engelstad%0AAbstract%3A%20%20%20The%20manifold%20hypothesis%20posits%20that%20high-dimensional%20data%20often%20lies%20on%20a%0Alower-dimensional%20manifold%20and%20that%20utilizing%20this%20manifold%20as%20the%20target%20space%0Ayields%20more%20efficient%20representations.%20While%20numerous%20traditional%0Amanifold-based%20techniques%20exist%20for%20dimensionality%20reduction%2C%20their%20application%0Ain%20self-supervised%20learning%20has%20witnessed%20slow%20progress.%20The%20recent%20MSimCLR%0Amethod%20combines%20manifold%20encoding%20with%20SimCLR%20but%20requires%20extremely%20low%20target%0Aencoding%20dimensions%20to%20outperform%20SimCLR%2C%20limiting%20its%20applicability.%20This%0Apaper%20introduces%20a%20novel%20learning%20paradigm%20using%20an%20unbalanced%20atlas%20%28UA%29%2C%0Acapable%20of%20surpassing%20state-of-the-art%20self-supervised%20learning%20approaches.%20We%0Ainvestigated%20and%20engineered%20the%20DeepInfomax%20with%20an%20unbalanced%20atlas%20%28DIM-UA%29%0Amethod%20by%20adapting%20the%20Spatiotemporal%20DeepInfomax%20%28ST-DIM%29%20framework%20to%20align%0Awith%20our%20proposed%20UA%20paradigm.%20The%20efficacy%20of%20DIM-UA%20is%20demonstrated%20through%0Atraining%20and%20evaluation%20on%20the%20Atari%20Annotated%20RAM%20Interface%20%28AtariARI%29%0Abenchmark%2C%20a%20modified%20version%20of%20the%20Atari%202600%20framework%20that%20produces%0Aannotated%20image%20samples%20for%20representation%20learning.%20The%20UA%20paradigm%20improves%0Aexisting%20algorithms%20significantly%20as%20the%20number%20of%20target%20encoding%20dimensions%0Agrows.%20For%20instance%2C%20the%20mean%20F1%20score%20averaged%20over%20categories%20of%20DIM-UA%20is%0A~75%25%20compared%20to%20~70%25%20of%20ST-DIM%20when%20using%2016384%20hidden%20units.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.10267v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DState%2520Representation%2520Learning%2520Using%2520an%2520Unbalanced%2520Atlas%26entry.906535625%3DLi%2520Meng%2520and%2520Morten%2520Goodwin%2520and%2520Anis%2520Yazidi%2520and%2520Paal%2520Engelstad%26entry.1292438233%3D%2520%2520The%2520manifold%2520hypothesis%2520posits%2520that%2520high-dimensional%2520data%2520often%2520lies%2520on%2520a%250Alower-dimensional%2520manifold%2520and%2520that%2520utilizing%2520this%2520manifold%2520as%2520the%2520target%2520space%250Ayields%2520more%2520efficient%2520representations.%2520While%2520numerous%2520traditional%250Amanifold-based%2520techniques%2520exist%2520for%2520dimensionality%2520reduction%252C%2520their%2520application%250Ain%2520self-supervised%2520learning%2520has%2520witnessed%2520slow%2520progress.%2520The%2520recent%2520MSimCLR%250Amethod%2520combines%2520manifold%2520encoding%2520with%2520SimCLR%2520but%2520requires%2520extremely%2520low%2520target%250Aencoding%2520dimensions%2520to%2520outperform%2520SimCLR%252C%2520limiting%2520its%2520applicability.%2520This%250Apaper%2520introduces%2520a%2520novel%2520learning%2520paradigm%2520using%2520an%2520unbalanced%2520atlas%2520%2528UA%2529%252C%250Acapable%2520of%2520surpassing%2520state-of-the-art%2520self-supervised%2520learning%2520approaches.%2520We%250Ainvestigated%2520and%2520engineered%2520the%2520DeepInfomax%2520with%2520an%2520unbalanced%2520atlas%2520%2528DIM-UA%2529%250Amethod%2520by%2520adapting%2520the%2520Spatiotemporal%2520DeepInfomax%2520%2528ST-DIM%2529%2520framework%2520to%2520align%250Awith%2520our%2520proposed%2520UA%2520paradigm.%2520The%2520efficacy%2520of%2520DIM-UA%2520is%2520demonstrated%2520through%250Atraining%2520and%2520evaluation%2520on%2520the%2520Atari%2520Annotated%2520RAM%2520Interface%2520%2528AtariARI%2529%250Abenchmark%252C%2520a%2520modified%2520version%2520of%2520the%2520Atari%25202600%2520framework%2520that%2520produces%250Aannotated%2520image%2520samples%2520for%2520representation%2520learning.%2520The%2520UA%2520paradigm%2520improves%250Aexisting%2520algorithms%2520significantly%2520as%2520the%2520number%2520of%2520target%2520encoding%2520dimensions%250Agrows.%2520For%2520instance%252C%2520the%2520mean%2520F1%2520score%2520averaged%2520over%2520categories%2520of%2520DIM-UA%2520is%250A~75%2525%2520compared%2520to%2520~70%2525%2520of%2520ST-DIM%2520when%2520using%252016384%2520hidden%2520units.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.10267v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=State%20Representation%20Learning%20Using%20an%20Unbalanced%20Atlas&entry.906535625=Li%20Meng%20and%20Morten%20Goodwin%20and%20Anis%20Yazidi%20and%20Paal%20Engelstad&entry.1292438233=%20%20The%20manifold%20hypothesis%20posits%20that%20high-dimensional%20data%20often%20lies%20on%20a%0Alower-dimensional%20manifold%20and%20that%20utilizing%20this%20manifold%20as%20the%20target%20space%0Ayields%20more%20efficient%20representations.%20While%20numerous%20traditional%0Amanifold-based%20techniques%20exist%20for%20dimensionality%20reduction%2C%20their%20application%0Ain%20self-supervised%20learning%20has%20witnessed%20slow%20progress.%20The%20recent%20MSimCLR%0Amethod%20combines%20manifold%20encoding%20with%20SimCLR%20but%20requires%20extremely%20low%20target%0Aencoding%20dimensions%20to%20outperform%20SimCLR%2C%20limiting%20its%20applicability.%20This%0Apaper%20introduces%20a%20novel%20learning%20paradigm%20using%20an%20unbalanced%20atlas%20%28UA%29%2C%0Acapable%20of%20surpassing%20state-of-the-art%20self-supervised%20learning%20approaches.%20We%0Ainvestigated%20and%20engineered%20the%20DeepInfomax%20with%20an%20unbalanced%20atlas%20%28DIM-UA%29%0Amethod%20by%20adapting%20the%20Spatiotemporal%20DeepInfomax%20%28ST-DIM%29%20framework%20to%20align%0Awith%20our%20proposed%20UA%20paradigm.%20The%20efficacy%20of%20DIM-UA%20is%20demonstrated%20through%0Atraining%20and%20evaluation%20on%20the%20Atari%20Annotated%20RAM%20Interface%20%28AtariARI%29%0Abenchmark%2C%20a%20modified%20version%20of%20the%20Atari%202600%20framework%20that%20produces%0Aannotated%20image%20samples%20for%20representation%20learning.%20The%20UA%20paradigm%20improves%0Aexisting%20algorithms%20significantly%20as%20the%20number%20of%20target%20encoding%20dimensions%0Agrows.%20For%20instance%2C%20the%20mean%20F1%20score%20averaged%20over%20categories%20of%20DIM-UA%20is%0A~75%25%20compared%20to%20~70%25%20of%20ST-DIM%20when%20using%2016384%20hidden%20units.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.10267v3&entry.124074799=Read"},
{"title": "RGB-Sonar Tracking Benchmark and Spatial Cross-Attention Transformer\n  Tracker", "author": "Yunfeng Li and Bo Wang and Jiuran Sun and Xueyi Wu and Ye Li", "abstract": "  Vision camera and sonar are naturally complementary in the underwater\nenvironment. Combining the information from two modalities will promote better\nobservation of underwater targets. However, this problem has not received\nsufficient attention in previous research. Therefore, this paper introduces a\nnew challenging RGB-Sonar (RGB-S) tracking task and investigates how to achieve\nefficient tracking of an underwater target through the interaction of RGB and\nsonar modalities. Specifically, we first propose an RGBS50 benchmark dataset\ncontaining 50 sequences and more than 87000 high-quality annotated bounding\nboxes. Experimental results show that the RGBS50 benchmark poses a challenge to\ncurrently popular SOT trackers. Second, we propose an RGB-S tracker called\nSCANet, which includes a spatial cross-attention module (SCAM) consisting of a\nnovel spatial cross-attention layer and two independent global integration\nmodules. The spatial cross-attention is used to overcome the problem of spatial\nmisalignment of between RGB and sonar images. Third, we propose a SOT\ndata-based RGB-S simulation training method (SRST) to overcome the lack of\nRGB-S training datasets. It converts RGB images into sonar-like saliency images\nto construct pseudo-data pairs, enabling the model to learn the semantic\nstructure of RGB-S-like data. Comprehensive experiments show that the proposed\nspatial cross-attention effectively achieves the interaction between RGB and\nsonar modalities and SCANet achieves state-of-the-art performance on the\nproposed benchmark. The code is available at\nhttps://github.com/LiYunfengLYF/RGBS50.\n", "link": "http://arxiv.org/abs/2406.07189v2", "date": "2024-06-24", "relevancy": 2.758, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6079}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5309}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGB-Sonar%20Tracking%20Benchmark%20and%20Spatial%20Cross-Attention%20Transformer%0A%20%20Tracker&body=Title%3A%20RGB-Sonar%20Tracking%20Benchmark%20and%20Spatial%20Cross-Attention%20Transformer%0A%20%20Tracker%0AAuthor%3A%20Yunfeng%20Li%20and%20Bo%20Wang%20and%20Jiuran%20Sun%20and%20Xueyi%20Wu%20and%20Ye%20Li%0AAbstract%3A%20%20%20Vision%20camera%20and%20sonar%20are%20naturally%20complementary%20in%20the%20underwater%0Aenvironment.%20Combining%20the%20information%20from%20two%20modalities%20will%20promote%20better%0Aobservation%20of%20underwater%20targets.%20However%2C%20this%20problem%20has%20not%20received%0Asufficient%20attention%20in%20previous%20research.%20Therefore%2C%20this%20paper%20introduces%20a%0Anew%20challenging%20RGB-Sonar%20%28RGB-S%29%20tracking%20task%20and%20investigates%20how%20to%20achieve%0Aefficient%20tracking%20of%20an%20underwater%20target%20through%20the%20interaction%20of%20RGB%20and%0Asonar%20modalities.%20Specifically%2C%20we%20first%20propose%20an%20RGBS50%20benchmark%20dataset%0Acontaining%2050%20sequences%20and%20more%20than%2087000%20high-quality%20annotated%20bounding%0Aboxes.%20Experimental%20results%20show%20that%20the%20RGBS50%20benchmark%20poses%20a%20challenge%20to%0Acurrently%20popular%20SOT%20trackers.%20Second%2C%20we%20propose%20an%20RGB-S%20tracker%20called%0ASCANet%2C%20which%20includes%20a%20spatial%20cross-attention%20module%20%28SCAM%29%20consisting%20of%20a%0Anovel%20spatial%20cross-attention%20layer%20and%20two%20independent%20global%20integration%0Amodules.%20The%20spatial%20cross-attention%20is%20used%20to%20overcome%20the%20problem%20of%20spatial%0Amisalignment%20of%20between%20RGB%20and%20sonar%20images.%20Third%2C%20we%20propose%20a%20SOT%0Adata-based%20RGB-S%20simulation%20training%20method%20%28SRST%29%20to%20overcome%20the%20lack%20of%0ARGB-S%20training%20datasets.%20It%20converts%20RGB%20images%20into%20sonar-like%20saliency%20images%0Ato%20construct%20pseudo-data%20pairs%2C%20enabling%20the%20model%20to%20learn%20the%20semantic%0Astructure%20of%20RGB-S-like%20data.%20Comprehensive%20experiments%20show%20that%20the%20proposed%0Aspatial%20cross-attention%20effectively%20achieves%20the%20interaction%20between%20RGB%20and%0Asonar%20modalities%20and%20SCANet%20achieves%20state-of-the-art%20performance%20on%20the%0Aproposed%20benchmark.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LiYunfengLYF/RGBS50.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07189v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGB-Sonar%2520Tracking%2520Benchmark%2520and%2520Spatial%2520Cross-Attention%2520Transformer%250A%2520%2520Tracker%26entry.906535625%3DYunfeng%2520Li%2520and%2520Bo%2520Wang%2520and%2520Jiuran%2520Sun%2520and%2520Xueyi%2520Wu%2520and%2520Ye%2520Li%26entry.1292438233%3D%2520%2520Vision%2520camera%2520and%2520sonar%2520are%2520naturally%2520complementary%2520in%2520the%2520underwater%250Aenvironment.%2520Combining%2520the%2520information%2520from%2520two%2520modalities%2520will%2520promote%2520better%250Aobservation%2520of%2520underwater%2520targets.%2520However%252C%2520this%2520problem%2520has%2520not%2520received%250Asufficient%2520attention%2520in%2520previous%2520research.%2520Therefore%252C%2520this%2520paper%2520introduces%2520a%250Anew%2520challenging%2520RGB-Sonar%2520%2528RGB-S%2529%2520tracking%2520task%2520and%2520investigates%2520how%2520to%2520achieve%250Aefficient%2520tracking%2520of%2520an%2520underwater%2520target%2520through%2520the%2520interaction%2520of%2520RGB%2520and%250Asonar%2520modalities.%2520Specifically%252C%2520we%2520first%2520propose%2520an%2520RGBS50%2520benchmark%2520dataset%250Acontaining%252050%2520sequences%2520and%2520more%2520than%252087000%2520high-quality%2520annotated%2520bounding%250Aboxes.%2520Experimental%2520results%2520show%2520that%2520the%2520RGBS50%2520benchmark%2520poses%2520a%2520challenge%2520to%250Acurrently%2520popular%2520SOT%2520trackers.%2520Second%252C%2520we%2520propose%2520an%2520RGB-S%2520tracker%2520called%250ASCANet%252C%2520which%2520includes%2520a%2520spatial%2520cross-attention%2520module%2520%2528SCAM%2529%2520consisting%2520of%2520a%250Anovel%2520spatial%2520cross-attention%2520layer%2520and%2520two%2520independent%2520global%2520integration%250Amodules.%2520The%2520spatial%2520cross-attention%2520is%2520used%2520to%2520overcome%2520the%2520problem%2520of%2520spatial%250Amisalignment%2520of%2520between%2520RGB%2520and%2520sonar%2520images.%2520Third%252C%2520we%2520propose%2520a%2520SOT%250Adata-based%2520RGB-S%2520simulation%2520training%2520method%2520%2528SRST%2529%2520to%2520overcome%2520the%2520lack%2520of%250ARGB-S%2520training%2520datasets.%2520It%2520converts%2520RGB%2520images%2520into%2520sonar-like%2520saliency%2520images%250Ato%2520construct%2520pseudo-data%2520pairs%252C%2520enabling%2520the%2520model%2520to%2520learn%2520the%2520semantic%250Astructure%2520of%2520RGB-S-like%2520data.%2520Comprehensive%2520experiments%2520show%2520that%2520the%2520proposed%250Aspatial%2520cross-attention%2520effectively%2520achieves%2520the%2520interaction%2520between%2520RGB%2520and%250Asonar%2520modalities%2520and%2520SCANet%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%250Aproposed%2520benchmark.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/LiYunfengLYF/RGBS50.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07189v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGB-Sonar%20Tracking%20Benchmark%20and%20Spatial%20Cross-Attention%20Transformer%0A%20%20Tracker&entry.906535625=Yunfeng%20Li%20and%20Bo%20Wang%20and%20Jiuran%20Sun%20and%20Xueyi%20Wu%20and%20Ye%20Li&entry.1292438233=%20%20Vision%20camera%20and%20sonar%20are%20naturally%20complementary%20in%20the%20underwater%0Aenvironment.%20Combining%20the%20information%20from%20two%20modalities%20will%20promote%20better%0Aobservation%20of%20underwater%20targets.%20However%2C%20this%20problem%20has%20not%20received%0Asufficient%20attention%20in%20previous%20research.%20Therefore%2C%20this%20paper%20introduces%20a%0Anew%20challenging%20RGB-Sonar%20%28RGB-S%29%20tracking%20task%20and%20investigates%20how%20to%20achieve%0Aefficient%20tracking%20of%20an%20underwater%20target%20through%20the%20interaction%20of%20RGB%20and%0Asonar%20modalities.%20Specifically%2C%20we%20first%20propose%20an%20RGBS50%20benchmark%20dataset%0Acontaining%2050%20sequences%20and%20more%20than%2087000%20high-quality%20annotated%20bounding%0Aboxes.%20Experimental%20results%20show%20that%20the%20RGBS50%20benchmark%20poses%20a%20challenge%20to%0Acurrently%20popular%20SOT%20trackers.%20Second%2C%20we%20propose%20an%20RGB-S%20tracker%20called%0ASCANet%2C%20which%20includes%20a%20spatial%20cross-attention%20module%20%28SCAM%29%20consisting%20of%20a%0Anovel%20spatial%20cross-attention%20layer%20and%20two%20independent%20global%20integration%0Amodules.%20The%20spatial%20cross-attention%20is%20used%20to%20overcome%20the%20problem%20of%20spatial%0Amisalignment%20of%20between%20RGB%20and%20sonar%20images.%20Third%2C%20we%20propose%20a%20SOT%0Adata-based%20RGB-S%20simulation%20training%20method%20%28SRST%29%20to%20overcome%20the%20lack%20of%0ARGB-S%20training%20datasets.%20It%20converts%20RGB%20images%20into%20sonar-like%20saliency%20images%0Ato%20construct%20pseudo-data%20pairs%2C%20enabling%20the%20model%20to%20learn%20the%20semantic%0Astructure%20of%20RGB-S-like%20data.%20Comprehensive%20experiments%20show%20that%20the%20proposed%0Aspatial%20cross-attention%20effectively%20achieves%20the%20interaction%20between%20RGB%20and%0Asonar%20modalities%20and%20SCANet%20achieves%20state-of-the-art%20performance%20on%20the%0Aproposed%20benchmark.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LiYunfengLYF/RGBS50.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07189v2&entry.124074799=Read"},
{"title": "AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a\n  Single High-Resolution Image", "author": "Hamed Amini Amirkolaee and Miaojing Shi and Lianghua He and Mark Mulligan", "abstract": "  The process of estimating and counting tree density using only a single\naerial or satellite image is a difficult task in the fields of photogrammetry\nand remote sensing. However, it plays a crucial role in the management of\nforests. The huge variety of trees in varied topography severely hinders tree\ncounting models to perform well. The purpose of this paper is to propose a\nframework that is learnt from the source domain with sufficient labeled trees\nand is adapted to the target domain with only a limited number of labeled\ntrees. Our method, termed as AdaTreeFormer, contains one shared encoder with a\nhierarchical feature extraction scheme to extract robust features from the\nsource and target domains. It also consists of three subnets: two for\nextracting self-domain attention maps from source and target domains\nrespectively and one for extracting cross-domain attention maps. For the\nlatter, an attention-to-adapt mechanism is introduced to distill relevant\ninformation from different domains while generating tree density maps; a\nhierarchical cross-domain feature alignment scheme is proposed that\nprogressively aligns the features from the source and target domains. We also\nadopt adversarial learning into the framework to further reduce the gap between\nsource and target domains. Our AdaTreeFormer is evaluated on six designed\ndomain adaptation tasks using three tree counting datasets, \\ie Jiangsu,\nYosemite, and London. Experimental results show that AdaTreeFormer\nsignificantly surpasses the state of the art, \\eg in the cross domain from the\nYosemite to Jiangsu dataset, it achieves a reduction of 15.9 points in terms of\nthe absolute counting errors and an increase of 10.8\\% in the accuracy of the\ndetected trees' locations. The codes and datasets are available at\n\\emph{\\color{magenta}{https://github.com/HAAClassic/AdaTreeFormer}}.\n", "link": "http://arxiv.org/abs/2402.02956v2", "date": "2024-06-24", "relevancy": 2.7335, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5622}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5472}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaTreeFormer%3A%20Few%20Shot%20Domain%20Adaptation%20for%20Tree%20Counting%20from%20a%0A%20%20Single%20High-Resolution%20Image&body=Title%3A%20AdaTreeFormer%3A%20Few%20Shot%20Domain%20Adaptation%20for%20Tree%20Counting%20from%20a%0A%20%20Single%20High-Resolution%20Image%0AAuthor%3A%20Hamed%20Amini%20Amirkolaee%20and%20Miaojing%20Shi%20and%20Lianghua%20He%20and%20Mark%20Mulligan%0AAbstract%3A%20%20%20The%20process%20of%20estimating%20and%20counting%20tree%20density%20using%20only%20a%20single%0Aaerial%20or%20satellite%20image%20is%20a%20difficult%20task%20in%20the%20fields%20of%20photogrammetry%0Aand%20remote%20sensing.%20However%2C%20it%20plays%20a%20crucial%20role%20in%20the%20management%20of%0Aforests.%20The%20huge%20variety%20of%20trees%20in%20varied%20topography%20severely%20hinders%20tree%0Acounting%20models%20to%20perform%20well.%20The%20purpose%20of%20this%20paper%20is%20to%20propose%20a%0Aframework%20that%20is%20learnt%20from%20the%20source%20domain%20with%20sufficient%20labeled%20trees%0Aand%20is%20adapted%20to%20the%20target%20domain%20with%20only%20a%20limited%20number%20of%20labeled%0Atrees.%20Our%20method%2C%20termed%20as%20AdaTreeFormer%2C%20contains%20one%20shared%20encoder%20with%20a%0Ahierarchical%20feature%20extraction%20scheme%20to%20extract%20robust%20features%20from%20the%0Asource%20and%20target%20domains.%20It%20also%20consists%20of%20three%20subnets%3A%20two%20for%0Aextracting%20self-domain%20attention%20maps%20from%20source%20and%20target%20domains%0Arespectively%20and%20one%20for%20extracting%20cross-domain%20attention%20maps.%20For%20the%0Alatter%2C%20an%20attention-to-adapt%20mechanism%20is%20introduced%20to%20distill%20relevant%0Ainformation%20from%20different%20domains%20while%20generating%20tree%20density%20maps%3B%20a%0Ahierarchical%20cross-domain%20feature%20alignment%20scheme%20is%20proposed%20that%0Aprogressively%20aligns%20the%20features%20from%20the%20source%20and%20target%20domains.%20We%20also%0Aadopt%20adversarial%20learning%20into%20the%20framework%20to%20further%20reduce%20the%20gap%20between%0Asource%20and%20target%20domains.%20Our%20AdaTreeFormer%20is%20evaluated%20on%20six%20designed%0Adomain%20adaptation%20tasks%20using%20three%20tree%20counting%20datasets%2C%20%5Cie%20Jiangsu%2C%0AYosemite%2C%20and%20London.%20Experimental%20results%20show%20that%20AdaTreeFormer%0Asignificantly%20surpasses%20the%20state%20of%20the%20art%2C%20%5Ceg%20in%20the%20cross%20domain%20from%20the%0AYosemite%20to%20Jiangsu%20dataset%2C%20it%20achieves%20a%20reduction%20of%2015.9%20points%20in%20terms%20of%0Athe%20absolute%20counting%20errors%20and%20an%20increase%20of%2010.8%5C%25%20in%20the%20accuracy%20of%20the%0Adetected%20trees%27%20locations.%20The%20codes%20and%20datasets%20are%20available%20at%0A%5Cemph%7B%5Ccolor%7Bmagenta%7D%7Bhttps%3A//github.com/HAAClassic/AdaTreeFormer%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02956v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaTreeFormer%253A%2520Few%2520Shot%2520Domain%2520Adaptation%2520for%2520Tree%2520Counting%2520from%2520a%250A%2520%2520Single%2520High-Resolution%2520Image%26entry.906535625%3DHamed%2520Amini%2520Amirkolaee%2520and%2520Miaojing%2520Shi%2520and%2520Lianghua%2520He%2520and%2520Mark%2520Mulligan%26entry.1292438233%3D%2520%2520The%2520process%2520of%2520estimating%2520and%2520counting%2520tree%2520density%2520using%2520only%2520a%2520single%250Aaerial%2520or%2520satellite%2520image%2520is%2520a%2520difficult%2520task%2520in%2520the%2520fields%2520of%2520photogrammetry%250Aand%2520remote%2520sensing.%2520However%252C%2520it%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520management%2520of%250Aforests.%2520The%2520huge%2520variety%2520of%2520trees%2520in%2520varied%2520topography%2520severely%2520hinders%2520tree%250Acounting%2520models%2520to%2520perform%2520well.%2520The%2520purpose%2520of%2520this%2520paper%2520is%2520to%2520propose%2520a%250Aframework%2520that%2520is%2520learnt%2520from%2520the%2520source%2520domain%2520with%2520sufficient%2520labeled%2520trees%250Aand%2520is%2520adapted%2520to%2520the%2520target%2520domain%2520with%2520only%2520a%2520limited%2520number%2520of%2520labeled%250Atrees.%2520Our%2520method%252C%2520termed%2520as%2520AdaTreeFormer%252C%2520contains%2520one%2520shared%2520encoder%2520with%2520a%250Ahierarchical%2520feature%2520extraction%2520scheme%2520to%2520extract%2520robust%2520features%2520from%2520the%250Asource%2520and%2520target%2520domains.%2520It%2520also%2520consists%2520of%2520three%2520subnets%253A%2520two%2520for%250Aextracting%2520self-domain%2520attention%2520maps%2520from%2520source%2520and%2520target%2520domains%250Arespectively%2520and%2520one%2520for%2520extracting%2520cross-domain%2520attention%2520maps.%2520For%2520the%250Alatter%252C%2520an%2520attention-to-adapt%2520mechanism%2520is%2520introduced%2520to%2520distill%2520relevant%250Ainformation%2520from%2520different%2520domains%2520while%2520generating%2520tree%2520density%2520maps%253B%2520a%250Ahierarchical%2520cross-domain%2520feature%2520alignment%2520scheme%2520is%2520proposed%2520that%250Aprogressively%2520aligns%2520the%2520features%2520from%2520the%2520source%2520and%2520target%2520domains.%2520We%2520also%250Aadopt%2520adversarial%2520learning%2520into%2520the%2520framework%2520to%2520further%2520reduce%2520the%2520gap%2520between%250Asource%2520and%2520target%2520domains.%2520Our%2520AdaTreeFormer%2520is%2520evaluated%2520on%2520six%2520designed%250Adomain%2520adaptation%2520tasks%2520using%2520three%2520tree%2520counting%2520datasets%252C%2520%255Cie%2520Jiangsu%252C%250AYosemite%252C%2520and%2520London.%2520Experimental%2520results%2520show%2520that%2520AdaTreeFormer%250Asignificantly%2520surpasses%2520the%2520state%2520of%2520the%2520art%252C%2520%255Ceg%2520in%2520the%2520cross%2520domain%2520from%2520the%250AYosemite%2520to%2520Jiangsu%2520dataset%252C%2520it%2520achieves%2520a%2520reduction%2520of%252015.9%2520points%2520in%2520terms%2520of%250Athe%2520absolute%2520counting%2520errors%2520and%2520an%2520increase%2520of%252010.8%255C%2525%2520in%2520the%2520accuracy%2520of%2520the%250Adetected%2520trees%2527%2520locations.%2520The%2520codes%2520and%2520datasets%2520are%2520available%2520at%250A%255Cemph%257B%255Ccolor%257Bmagenta%257D%257Bhttps%253A//github.com/HAAClassic/AdaTreeFormer%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02956v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaTreeFormer%3A%20Few%20Shot%20Domain%20Adaptation%20for%20Tree%20Counting%20from%20a%0A%20%20Single%20High-Resolution%20Image&entry.906535625=Hamed%20Amini%20Amirkolaee%20and%20Miaojing%20Shi%20and%20Lianghua%20He%20and%20Mark%20Mulligan&entry.1292438233=%20%20The%20process%20of%20estimating%20and%20counting%20tree%20density%20using%20only%20a%20single%0Aaerial%20or%20satellite%20image%20is%20a%20difficult%20task%20in%20the%20fields%20of%20photogrammetry%0Aand%20remote%20sensing.%20However%2C%20it%20plays%20a%20crucial%20role%20in%20the%20management%20of%0Aforests.%20The%20huge%20variety%20of%20trees%20in%20varied%20topography%20severely%20hinders%20tree%0Acounting%20models%20to%20perform%20well.%20The%20purpose%20of%20this%20paper%20is%20to%20propose%20a%0Aframework%20that%20is%20learnt%20from%20the%20source%20domain%20with%20sufficient%20labeled%20trees%0Aand%20is%20adapted%20to%20the%20target%20domain%20with%20only%20a%20limited%20number%20of%20labeled%0Atrees.%20Our%20method%2C%20termed%20as%20AdaTreeFormer%2C%20contains%20one%20shared%20encoder%20with%20a%0Ahierarchical%20feature%20extraction%20scheme%20to%20extract%20robust%20features%20from%20the%0Asource%20and%20target%20domains.%20It%20also%20consists%20of%20three%20subnets%3A%20two%20for%0Aextracting%20self-domain%20attention%20maps%20from%20source%20and%20target%20domains%0Arespectively%20and%20one%20for%20extracting%20cross-domain%20attention%20maps.%20For%20the%0Alatter%2C%20an%20attention-to-adapt%20mechanism%20is%20introduced%20to%20distill%20relevant%0Ainformation%20from%20different%20domains%20while%20generating%20tree%20density%20maps%3B%20a%0Ahierarchical%20cross-domain%20feature%20alignment%20scheme%20is%20proposed%20that%0Aprogressively%20aligns%20the%20features%20from%20the%20source%20and%20target%20domains.%20We%20also%0Aadopt%20adversarial%20learning%20into%20the%20framework%20to%20further%20reduce%20the%20gap%20between%0Asource%20and%20target%20domains.%20Our%20AdaTreeFormer%20is%20evaluated%20on%20six%20designed%0Adomain%20adaptation%20tasks%20using%20three%20tree%20counting%20datasets%2C%20%5Cie%20Jiangsu%2C%0AYosemite%2C%20and%20London.%20Experimental%20results%20show%20that%20AdaTreeFormer%0Asignificantly%20surpasses%20the%20state%20of%20the%20art%2C%20%5Ceg%20in%20the%20cross%20domain%20from%20the%0AYosemite%20to%20Jiangsu%20dataset%2C%20it%20achieves%20a%20reduction%20of%2015.9%20points%20in%20terms%20of%0Athe%20absolute%20counting%20errors%20and%20an%20increase%20of%2010.8%5C%25%20in%20the%20accuracy%20of%20the%0Adetected%20trees%27%20locations.%20The%20codes%20and%20datasets%20are%20available%20at%0A%5Cemph%7B%5Ccolor%7Bmagenta%7D%7Bhttps%3A//github.com/HAAClassic/AdaTreeFormer%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02956v2&entry.124074799=Read"},
{"title": "ClotheDreamer: Text-Guided Garment Generation with 3D Gaussians", "author": "Yufei Liu and Junshu Tang and Chu Zheng and Shijie Zhang and Jinkun Hao and Junwei Zhu and Dongjin Huang", "abstract": "  High-fidelity 3D garment synthesis from text is desirable yet challenging for\ndigital avatar creation. Recent diffusion-based approaches via Score\nDistillation Sampling (SDS) have enabled new possibilities but either\nintricately couple with human body or struggle to reuse. We introduce\nClotheDreamer, a 3D Gaussian-based method for generating wearable,\nproduction-ready 3D garment assets from text prompts. We propose a novel\nrepresentation Disentangled Clothe Gaussian Splatting (DCGS) to enable separate\noptimization. DCGS represents clothed avatar as one Gaussian model but freezes\nbody Gaussian splats. To enhance quality and completeness, we incorporate\nbidirectional SDS to supervise clothed avatar and garment RGBD renderings\nrespectively with pose conditions and propose a new pruning strategy for loose\nclothing. Our approach can also support custom clothing templates as input.\nBenefiting from our design, the synthetic 3D garment can be easily applied to\nvirtual try-on and support physically accurate animation. Extensive experiments\nshowcase our method's superior and competitive performance. Our project page is\nat https://ggxxii.github.io/clothedreamer.\n", "link": "http://arxiv.org/abs/2406.16815v1", "date": "2024-06-24", "relevancy": 2.7165, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.8017}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5987}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ClotheDreamer%3A%20Text-Guided%20Garment%20Generation%20with%203D%20Gaussians&body=Title%3A%20ClotheDreamer%3A%20Text-Guided%20Garment%20Generation%20with%203D%20Gaussians%0AAuthor%3A%20Yufei%20Liu%20and%20Junshu%20Tang%20and%20Chu%20Zheng%20and%20Shijie%20Zhang%20and%20Jinkun%20Hao%20and%20Junwei%20Zhu%20and%20Dongjin%20Huang%0AAbstract%3A%20%20%20High-fidelity%203D%20garment%20synthesis%20from%20text%20is%20desirable%20yet%20challenging%20for%0Adigital%20avatar%20creation.%20Recent%20diffusion-based%20approaches%20via%20Score%0ADistillation%20Sampling%20%28SDS%29%20have%20enabled%20new%20possibilities%20but%20either%0Aintricately%20couple%20with%20human%20body%20or%20struggle%20to%20reuse.%20We%20introduce%0AClotheDreamer%2C%20a%203D%20Gaussian-based%20method%20for%20generating%20wearable%2C%0Aproduction-ready%203D%20garment%20assets%20from%20text%20prompts.%20We%20propose%20a%20novel%0Arepresentation%20Disentangled%20Clothe%20Gaussian%20Splatting%20%28DCGS%29%20to%20enable%20separate%0Aoptimization.%20DCGS%20represents%20clothed%20avatar%20as%20one%20Gaussian%20model%20but%20freezes%0Abody%20Gaussian%20splats.%20To%20enhance%20quality%20and%20completeness%2C%20we%20incorporate%0Abidirectional%20SDS%20to%20supervise%20clothed%20avatar%20and%20garment%20RGBD%20renderings%0Arespectively%20with%20pose%20conditions%20and%20propose%20a%20new%20pruning%20strategy%20for%20loose%0Aclothing.%20Our%20approach%20can%20also%20support%20custom%20clothing%20templates%20as%20input.%0ABenefiting%20from%20our%20design%2C%20the%20synthetic%203D%20garment%20can%20be%20easily%20applied%20to%0Avirtual%20try-on%20and%20support%20physically%20accurate%20animation.%20Extensive%20experiments%0Ashowcase%20our%20method%27s%20superior%20and%20competitive%20performance.%20Our%20project%20page%20is%0Aat%20https%3A//ggxxii.github.io/clothedreamer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClotheDreamer%253A%2520Text-Guided%2520Garment%2520Generation%2520with%25203D%2520Gaussians%26entry.906535625%3DYufei%2520Liu%2520and%2520Junshu%2520Tang%2520and%2520Chu%2520Zheng%2520and%2520Shijie%2520Zhang%2520and%2520Jinkun%2520Hao%2520and%2520Junwei%2520Zhu%2520and%2520Dongjin%2520Huang%26entry.1292438233%3D%2520%2520High-fidelity%25203D%2520garment%2520synthesis%2520from%2520text%2520is%2520desirable%2520yet%2520challenging%2520for%250Adigital%2520avatar%2520creation.%2520Recent%2520diffusion-based%2520approaches%2520via%2520Score%250ADistillation%2520Sampling%2520%2528SDS%2529%2520have%2520enabled%2520new%2520possibilities%2520but%2520either%250Aintricately%2520couple%2520with%2520human%2520body%2520or%2520struggle%2520to%2520reuse.%2520We%2520introduce%250AClotheDreamer%252C%2520a%25203D%2520Gaussian-based%2520method%2520for%2520generating%2520wearable%252C%250Aproduction-ready%25203D%2520garment%2520assets%2520from%2520text%2520prompts.%2520We%2520propose%2520a%2520novel%250Arepresentation%2520Disentangled%2520Clothe%2520Gaussian%2520Splatting%2520%2528DCGS%2529%2520to%2520enable%2520separate%250Aoptimization.%2520DCGS%2520represents%2520clothed%2520avatar%2520as%2520one%2520Gaussian%2520model%2520but%2520freezes%250Abody%2520Gaussian%2520splats.%2520To%2520enhance%2520quality%2520and%2520completeness%252C%2520we%2520incorporate%250Abidirectional%2520SDS%2520to%2520supervise%2520clothed%2520avatar%2520and%2520garment%2520RGBD%2520renderings%250Arespectively%2520with%2520pose%2520conditions%2520and%2520propose%2520a%2520new%2520pruning%2520strategy%2520for%2520loose%250Aclothing.%2520Our%2520approach%2520can%2520also%2520support%2520custom%2520clothing%2520templates%2520as%2520input.%250ABenefiting%2520from%2520our%2520design%252C%2520the%2520synthetic%25203D%2520garment%2520can%2520be%2520easily%2520applied%2520to%250Avirtual%2520try-on%2520and%2520support%2520physically%2520accurate%2520animation.%2520Extensive%2520experiments%250Ashowcase%2520our%2520method%2527s%2520superior%2520and%2520competitive%2520performance.%2520Our%2520project%2520page%2520is%250Aat%2520https%253A//ggxxii.github.io/clothedreamer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClotheDreamer%3A%20Text-Guided%20Garment%20Generation%20with%203D%20Gaussians&entry.906535625=Yufei%20Liu%20and%20Junshu%20Tang%20and%20Chu%20Zheng%20and%20Shijie%20Zhang%20and%20Jinkun%20Hao%20and%20Junwei%20Zhu%20and%20Dongjin%20Huang&entry.1292438233=%20%20High-fidelity%203D%20garment%20synthesis%20from%20text%20is%20desirable%20yet%20challenging%20for%0Adigital%20avatar%20creation.%20Recent%20diffusion-based%20approaches%20via%20Score%0ADistillation%20Sampling%20%28SDS%29%20have%20enabled%20new%20possibilities%20but%20either%0Aintricately%20couple%20with%20human%20body%20or%20struggle%20to%20reuse.%20We%20introduce%0AClotheDreamer%2C%20a%203D%20Gaussian-based%20method%20for%20generating%20wearable%2C%0Aproduction-ready%203D%20garment%20assets%20from%20text%20prompts.%20We%20propose%20a%20novel%0Arepresentation%20Disentangled%20Clothe%20Gaussian%20Splatting%20%28DCGS%29%20to%20enable%20separate%0Aoptimization.%20DCGS%20represents%20clothed%20avatar%20as%20one%20Gaussian%20model%20but%20freezes%0Abody%20Gaussian%20splats.%20To%20enhance%20quality%20and%20completeness%2C%20we%20incorporate%0Abidirectional%20SDS%20to%20supervise%20clothed%20avatar%20and%20garment%20RGBD%20renderings%0Arespectively%20with%20pose%20conditions%20and%20propose%20a%20new%20pruning%20strategy%20for%20loose%0Aclothing.%20Our%20approach%20can%20also%20support%20custom%20clothing%20templates%20as%20input.%0ABenefiting%20from%20our%20design%2C%20the%20synthetic%203D%20garment%20can%20be%20easily%20applied%20to%0Avirtual%20try-on%20and%20support%20physically%20accurate%20animation.%20Extensive%20experiments%0Ashowcase%20our%20method%27s%20superior%20and%20competitive%20performance.%20Our%20project%20page%20is%0Aat%20https%3A//ggxxii.github.io/clothedreamer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16815v1&entry.124074799=Read"},
{"title": "Asymmetrical Siamese Network for Point Clouds Normal Estimation", "author": "Wei Jin and Jun Zhou and Nannan Li and Haba Madeline and Xiuping Liu", "abstract": "  In recent years, deep learning-based point cloud normal estimation has made\ngreat progress. However, existing methods mainly rely on the PCPNet dataset,\nleading to overfitting. In addition, the correlation between point clouds with\ndifferent noise scales remains unexplored, resulting in poor performance in\ncross-domain scenarios. In this paper, we explore the consistency of intrinsic\nfeatures learned from clean and noisy point clouds using an Asymmetric Siamese\nNetwork architecture. By applying reasonable constraints between features\nextracted from different branches, we enhance the quality of normal estimation.\nMoreover, we introduce a novel multi-view normal estimation dataset that\nincludes a larger variety of shapes with different noise levels. Evaluation of\nexisting methods on this new dataset reveals their inability to adapt to\ndifferent types of shapes, indicating a degree of overfitting. Extensive\nexperiments show that the proposed dataset poses significant challenges for\npoint cloud normal estimation and that our feature constraint mechanism\neffectively improves upon existing methods and reduces overfitting in current\narchitectures.\n", "link": "http://arxiv.org/abs/2406.09681v2", "date": "2024-06-24", "relevancy": 2.7074, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5549}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5548}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asymmetrical%20Siamese%20Network%20for%20Point%20Clouds%20Normal%20Estimation&body=Title%3A%20Asymmetrical%20Siamese%20Network%20for%20Point%20Clouds%20Normal%20Estimation%0AAuthor%3A%20Wei%20Jin%20and%20Jun%20Zhou%20and%20Nannan%20Li%20and%20Haba%20Madeline%20and%20Xiuping%20Liu%0AAbstract%3A%20%20%20In%20recent%20years%2C%20deep%20learning-based%20point%20cloud%20normal%20estimation%20has%20made%0Agreat%20progress.%20However%2C%20existing%20methods%20mainly%20rely%20on%20the%20PCPNet%20dataset%2C%0Aleading%20to%20overfitting.%20In%20addition%2C%20the%20correlation%20between%20point%20clouds%20with%0Adifferent%20noise%20scales%20remains%20unexplored%2C%20resulting%20in%20poor%20performance%20in%0Across-domain%20scenarios.%20In%20this%20paper%2C%20we%20explore%20the%20consistency%20of%20intrinsic%0Afeatures%20learned%20from%20clean%20and%20noisy%20point%20clouds%20using%20an%20Asymmetric%20Siamese%0ANetwork%20architecture.%20By%20applying%20reasonable%20constraints%20between%20features%0Aextracted%20from%20different%20branches%2C%20we%20enhance%20the%20quality%20of%20normal%20estimation.%0AMoreover%2C%20we%20introduce%20a%20novel%20multi-view%20normal%20estimation%20dataset%20that%0Aincludes%20a%20larger%20variety%20of%20shapes%20with%20different%20noise%20levels.%20Evaluation%20of%0Aexisting%20methods%20on%20this%20new%20dataset%20reveals%20their%20inability%20to%20adapt%20to%0Adifferent%20types%20of%20shapes%2C%20indicating%20a%20degree%20of%20overfitting.%20Extensive%0Aexperiments%20show%20that%20the%20proposed%20dataset%20poses%20significant%20challenges%20for%0Apoint%20cloud%20normal%20estimation%20and%20that%20our%20feature%20constraint%20mechanism%0Aeffectively%20improves%20upon%20existing%20methods%20and%20reduces%20overfitting%20in%20current%0Aarchitectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09681v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsymmetrical%2520Siamese%2520Network%2520for%2520Point%2520Clouds%2520Normal%2520Estimation%26entry.906535625%3DWei%2520Jin%2520and%2520Jun%2520Zhou%2520and%2520Nannan%2520Li%2520and%2520Haba%2520Madeline%2520and%2520Xiuping%2520Liu%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520deep%2520learning-based%2520point%2520cloud%2520normal%2520estimation%2520has%2520made%250Agreat%2520progress.%2520However%252C%2520existing%2520methods%2520mainly%2520rely%2520on%2520the%2520PCPNet%2520dataset%252C%250Aleading%2520to%2520overfitting.%2520In%2520addition%252C%2520the%2520correlation%2520between%2520point%2520clouds%2520with%250Adifferent%2520noise%2520scales%2520remains%2520unexplored%252C%2520resulting%2520in%2520poor%2520performance%2520in%250Across-domain%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520consistency%2520of%2520intrinsic%250Afeatures%2520learned%2520from%2520clean%2520and%2520noisy%2520point%2520clouds%2520using%2520an%2520Asymmetric%2520Siamese%250ANetwork%2520architecture.%2520By%2520applying%2520reasonable%2520constraints%2520between%2520features%250Aextracted%2520from%2520different%2520branches%252C%2520we%2520enhance%2520the%2520quality%2520of%2520normal%2520estimation.%250AMoreover%252C%2520we%2520introduce%2520a%2520novel%2520multi-view%2520normal%2520estimation%2520dataset%2520that%250Aincludes%2520a%2520larger%2520variety%2520of%2520shapes%2520with%2520different%2520noise%2520levels.%2520Evaluation%2520of%250Aexisting%2520methods%2520on%2520this%2520new%2520dataset%2520reveals%2520their%2520inability%2520to%2520adapt%2520to%250Adifferent%2520types%2520of%2520shapes%252C%2520indicating%2520a%2520degree%2520of%2520overfitting.%2520Extensive%250Aexperiments%2520show%2520that%2520the%2520proposed%2520dataset%2520poses%2520significant%2520challenges%2520for%250Apoint%2520cloud%2520normal%2520estimation%2520and%2520that%2520our%2520feature%2520constraint%2520mechanism%250Aeffectively%2520improves%2520upon%2520existing%2520methods%2520and%2520reduces%2520overfitting%2520in%2520current%250Aarchitectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09681v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asymmetrical%20Siamese%20Network%20for%20Point%20Clouds%20Normal%20Estimation&entry.906535625=Wei%20Jin%20and%20Jun%20Zhou%20and%20Nannan%20Li%20and%20Haba%20Madeline%20and%20Xiuping%20Liu&entry.1292438233=%20%20In%20recent%20years%2C%20deep%20learning-based%20point%20cloud%20normal%20estimation%20has%20made%0Agreat%20progress.%20However%2C%20existing%20methods%20mainly%20rely%20on%20the%20PCPNet%20dataset%2C%0Aleading%20to%20overfitting.%20In%20addition%2C%20the%20correlation%20between%20point%20clouds%20with%0Adifferent%20noise%20scales%20remains%20unexplored%2C%20resulting%20in%20poor%20performance%20in%0Across-domain%20scenarios.%20In%20this%20paper%2C%20we%20explore%20the%20consistency%20of%20intrinsic%0Afeatures%20learned%20from%20clean%20and%20noisy%20point%20clouds%20using%20an%20Asymmetric%20Siamese%0ANetwork%20architecture.%20By%20applying%20reasonable%20constraints%20between%20features%0Aextracted%20from%20different%20branches%2C%20we%20enhance%20the%20quality%20of%20normal%20estimation.%0AMoreover%2C%20we%20introduce%20a%20novel%20multi-view%20normal%20estimation%20dataset%20that%0Aincludes%20a%20larger%20variety%20of%20shapes%20with%20different%20noise%20levels.%20Evaluation%20of%0Aexisting%20methods%20on%20this%20new%20dataset%20reveals%20their%20inability%20to%20adapt%20to%0Adifferent%20types%20of%20shapes%2C%20indicating%20a%20degree%20of%20overfitting.%20Extensive%0Aexperiments%20show%20that%20the%20proposed%20dataset%20poses%20significant%20challenges%20for%0Apoint%20cloud%20normal%20estimation%20and%20that%20our%20feature%20constraint%20mechanism%0Aeffectively%20improves%20upon%20existing%20methods%20and%20reduces%20overfitting%20in%20current%0Aarchitectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09681v2&entry.124074799=Read"},
{"title": "DreamBench++: A Human-Aligned Benchmark for Personalized Image\n  Generation", "author": "Yuang Peng and Yuxin Cui and Haomiao Tang and Zekun Qi and Runpei Dong and Jing Bai and Chunrui Han and Zheng Ge and Xiangyu Zhang and Shu-Tao Xia", "abstract": "  Personalized image generation holds great promise in assisting humans in\neveryday work and life due to its impressive function in creatively generating\npersonalized content. However, current evaluations either are automated but\nmisalign with humans or require human evaluations that are time-consuming and\nexpensive. In this work, we present DreamBench++, a human-aligned benchmark\nautomated by advanced multimodal GPT models. Specifically, we systematically\ndesign the prompts to let GPT be both human-aligned and self-aligned, empowered\nwith task reinforcement. Further, we construct a comprehensive dataset\ncomprising diverse images and prompts. By benchmarking 7 modern generative\nmodels, we demonstrate that DreamBench++ results in significantly more\nhuman-aligned evaluation, helping boost the community with innovative findings.\n", "link": "http://arxiv.org/abs/2406.16855v1", "date": "2024-06-24", "relevancy": 2.655, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5417}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.529}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamBench%2B%2B%3A%20A%20Human-Aligned%20Benchmark%20for%20Personalized%20Image%0A%20%20Generation&body=Title%3A%20DreamBench%2B%2B%3A%20A%20Human-Aligned%20Benchmark%20for%20Personalized%20Image%0A%20%20Generation%0AAuthor%3A%20Yuang%20Peng%20and%20Yuxin%20Cui%20and%20Haomiao%20Tang%20and%20Zekun%20Qi%20and%20Runpei%20Dong%20and%20Jing%20Bai%20and%20Chunrui%20Han%20and%20Zheng%20Ge%20and%20Xiangyu%20Zhang%20and%20Shu-Tao%20Xia%0AAbstract%3A%20%20%20Personalized%20image%20generation%20holds%20great%20promise%20in%20assisting%20humans%20in%0Aeveryday%20work%20and%20life%20due%20to%20its%20impressive%20function%20in%20creatively%20generating%0Apersonalized%20content.%20However%2C%20current%20evaluations%20either%20are%20automated%20but%0Amisalign%20with%20humans%20or%20require%20human%20evaluations%20that%20are%20time-consuming%20and%0Aexpensive.%20In%20this%20work%2C%20we%20present%20DreamBench%2B%2B%2C%20a%20human-aligned%20benchmark%0Aautomated%20by%20advanced%20multimodal%20GPT%20models.%20Specifically%2C%20we%20systematically%0Adesign%20the%20prompts%20to%20let%20GPT%20be%20both%20human-aligned%20and%20self-aligned%2C%20empowered%0Awith%20task%20reinforcement.%20Further%2C%20we%20construct%20a%20comprehensive%20dataset%0Acomprising%20diverse%20images%20and%20prompts.%20By%20benchmarking%207%20modern%20generative%0Amodels%2C%20we%20demonstrate%20that%20DreamBench%2B%2B%20results%20in%20significantly%20more%0Ahuman-aligned%20evaluation%2C%20helping%20boost%20the%20community%20with%20innovative%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamBench%252B%252B%253A%2520A%2520Human-Aligned%2520Benchmark%2520for%2520Personalized%2520Image%250A%2520%2520Generation%26entry.906535625%3DYuang%2520Peng%2520and%2520Yuxin%2520Cui%2520and%2520Haomiao%2520Tang%2520and%2520Zekun%2520Qi%2520and%2520Runpei%2520Dong%2520and%2520Jing%2520Bai%2520and%2520Chunrui%2520Han%2520and%2520Zheng%2520Ge%2520and%2520Xiangyu%2520Zhang%2520and%2520Shu-Tao%2520Xia%26entry.1292438233%3D%2520%2520Personalized%2520image%2520generation%2520holds%2520great%2520promise%2520in%2520assisting%2520humans%2520in%250Aeveryday%2520work%2520and%2520life%2520due%2520to%2520its%2520impressive%2520function%2520in%2520creatively%2520generating%250Apersonalized%2520content.%2520However%252C%2520current%2520evaluations%2520either%2520are%2520automated%2520but%250Amisalign%2520with%2520humans%2520or%2520require%2520human%2520evaluations%2520that%2520are%2520time-consuming%2520and%250Aexpensive.%2520In%2520this%2520work%252C%2520we%2520present%2520DreamBench%252B%252B%252C%2520a%2520human-aligned%2520benchmark%250Aautomated%2520by%2520advanced%2520multimodal%2520GPT%2520models.%2520Specifically%252C%2520we%2520systematically%250Adesign%2520the%2520prompts%2520to%2520let%2520GPT%2520be%2520both%2520human-aligned%2520and%2520self-aligned%252C%2520empowered%250Awith%2520task%2520reinforcement.%2520Further%252C%2520we%2520construct%2520a%2520comprehensive%2520dataset%250Acomprising%2520diverse%2520images%2520and%2520prompts.%2520By%2520benchmarking%25207%2520modern%2520generative%250Amodels%252C%2520we%2520demonstrate%2520that%2520DreamBench%252B%252B%2520results%2520in%2520significantly%2520more%250Ahuman-aligned%2520evaluation%252C%2520helping%2520boost%2520the%2520community%2520with%2520innovative%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamBench%2B%2B%3A%20A%20Human-Aligned%20Benchmark%20for%20Personalized%20Image%0A%20%20Generation&entry.906535625=Yuang%20Peng%20and%20Yuxin%20Cui%20and%20Haomiao%20Tang%20and%20Zekun%20Qi%20and%20Runpei%20Dong%20and%20Jing%20Bai%20and%20Chunrui%20Han%20and%20Zheng%20Ge%20and%20Xiangyu%20Zhang%20and%20Shu-Tao%20Xia&entry.1292438233=%20%20Personalized%20image%20generation%20holds%20great%20promise%20in%20assisting%20humans%20in%0Aeveryday%20work%20and%20life%20due%20to%20its%20impressive%20function%20in%20creatively%20generating%0Apersonalized%20content.%20However%2C%20current%20evaluations%20either%20are%20automated%20but%0Amisalign%20with%20humans%20or%20require%20human%20evaluations%20that%20are%20time-consuming%20and%0Aexpensive.%20In%20this%20work%2C%20we%20present%20DreamBench%2B%2B%2C%20a%20human-aligned%20benchmark%0Aautomated%20by%20advanced%20multimodal%20GPT%20models.%20Specifically%2C%20we%20systematically%0Adesign%20the%20prompts%20to%20let%20GPT%20be%20both%20human-aligned%20and%20self-aligned%2C%20empowered%0Awith%20task%20reinforcement.%20Further%2C%20we%20construct%20a%20comprehensive%20dataset%0Acomprising%20diverse%20images%20and%20prompts.%20By%20benchmarking%207%20modern%20generative%0Amodels%2C%20we%20demonstrate%20that%20DreamBench%2B%2B%20results%20in%20significantly%20more%0Ahuman-aligned%20evaluation%2C%20helping%20boost%20the%20community%20with%20innovative%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16855v1&entry.124074799=Read"},
{"title": "Learning Interpretable Fair Representations", "author": "Tianhao Wang and Zana Bu\u00e7inca and Zilin Ma", "abstract": "  Numerous approaches have been recently proposed for learning fair\nrepresentations that mitigate unfair outcomes in prediction tasks. A key\nmotivation for these methods is that the representations can be used by third\nparties with unknown objectives. However, because current fair representations\nare generally not interpretable, the third party cannot use these fair\nrepresentations for exploration, or to obtain any additional insights, besides\nthe pre-contracted prediction tasks. Thus, to increase data utility beyond\nprediction tasks, we argue that the representations need to be fair, yet\ninterpretable. We propose a general framework for learning interpretable fair\nrepresentations by introducing an interpretable \"prior knowledge\" during the\nrepresentation learning process. We implement this idea and conduct experiments\nwith ColorMNIST and Dsprite datasets. The results indicate that in addition to\nbeing interpretable, our representations attain slightly higher accuracy and\nfairer outcomes in a downstream classification task compared to\nstate-of-the-art fair representations.\n", "link": "http://arxiv.org/abs/2406.16698v1", "date": "2024-06-24", "relevancy": 2.6109, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5254}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5224}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Interpretable%20Fair%20Representations&body=Title%3A%20Learning%20Interpretable%20Fair%20Representations%0AAuthor%3A%20Tianhao%20Wang%20and%20Zana%20Bu%C3%A7inca%20and%20Zilin%20Ma%0AAbstract%3A%20%20%20Numerous%20approaches%20have%20been%20recently%20proposed%20for%20learning%20fair%0Arepresentations%20that%20mitigate%20unfair%20outcomes%20in%20prediction%20tasks.%20A%20key%0Amotivation%20for%20these%20methods%20is%20that%20the%20representations%20can%20be%20used%20by%20third%0Aparties%20with%20unknown%20objectives.%20However%2C%20because%20current%20fair%20representations%0Aare%20generally%20not%20interpretable%2C%20the%20third%20party%20cannot%20use%20these%20fair%0Arepresentations%20for%20exploration%2C%20or%20to%20obtain%20any%20additional%20insights%2C%20besides%0Athe%20pre-contracted%20prediction%20tasks.%20Thus%2C%20to%20increase%20data%20utility%20beyond%0Aprediction%20tasks%2C%20we%20argue%20that%20the%20representations%20need%20to%20be%20fair%2C%20yet%0Ainterpretable.%20We%20propose%20a%20general%20framework%20for%20learning%20interpretable%20fair%0Arepresentations%20by%20introducing%20an%20interpretable%20%22prior%20knowledge%22%20during%20the%0Arepresentation%20learning%20process.%20We%20implement%20this%20idea%20and%20conduct%20experiments%0Awith%20ColorMNIST%20and%20Dsprite%20datasets.%20The%20results%20indicate%20that%20in%20addition%20to%0Abeing%20interpretable%2C%20our%20representations%20attain%20slightly%20higher%20accuracy%20and%0Afairer%20outcomes%20in%20a%20downstream%20classification%20task%20compared%20to%0Astate-of-the-art%20fair%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Interpretable%2520Fair%2520Representations%26entry.906535625%3DTianhao%2520Wang%2520and%2520Zana%2520Bu%25C3%25A7inca%2520and%2520Zilin%2520Ma%26entry.1292438233%3D%2520%2520Numerous%2520approaches%2520have%2520been%2520recently%2520proposed%2520for%2520learning%2520fair%250Arepresentations%2520that%2520mitigate%2520unfair%2520outcomes%2520in%2520prediction%2520tasks.%2520A%2520key%250Amotivation%2520for%2520these%2520methods%2520is%2520that%2520the%2520representations%2520can%2520be%2520used%2520by%2520third%250Aparties%2520with%2520unknown%2520objectives.%2520However%252C%2520because%2520current%2520fair%2520representations%250Aare%2520generally%2520not%2520interpretable%252C%2520the%2520third%2520party%2520cannot%2520use%2520these%2520fair%250Arepresentations%2520for%2520exploration%252C%2520or%2520to%2520obtain%2520any%2520additional%2520insights%252C%2520besides%250Athe%2520pre-contracted%2520prediction%2520tasks.%2520Thus%252C%2520to%2520increase%2520data%2520utility%2520beyond%250Aprediction%2520tasks%252C%2520we%2520argue%2520that%2520the%2520representations%2520need%2520to%2520be%2520fair%252C%2520yet%250Ainterpretable.%2520We%2520propose%2520a%2520general%2520framework%2520for%2520learning%2520interpretable%2520fair%250Arepresentations%2520by%2520introducing%2520an%2520interpretable%2520%2522prior%2520knowledge%2522%2520during%2520the%250Arepresentation%2520learning%2520process.%2520We%2520implement%2520this%2520idea%2520and%2520conduct%2520experiments%250Awith%2520ColorMNIST%2520and%2520Dsprite%2520datasets.%2520The%2520results%2520indicate%2520that%2520in%2520addition%2520to%250Abeing%2520interpretable%252C%2520our%2520representations%2520attain%2520slightly%2520higher%2520accuracy%2520and%250Afairer%2520outcomes%2520in%2520a%2520downstream%2520classification%2520task%2520compared%2520to%250Astate-of-the-art%2520fair%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Interpretable%20Fair%20Representations&entry.906535625=Tianhao%20Wang%20and%20Zana%20Bu%C3%A7inca%20and%20Zilin%20Ma&entry.1292438233=%20%20Numerous%20approaches%20have%20been%20recently%20proposed%20for%20learning%20fair%0Arepresentations%20that%20mitigate%20unfair%20outcomes%20in%20prediction%20tasks.%20A%20key%0Amotivation%20for%20these%20methods%20is%20that%20the%20representations%20can%20be%20used%20by%20third%0Aparties%20with%20unknown%20objectives.%20However%2C%20because%20current%20fair%20representations%0Aare%20generally%20not%20interpretable%2C%20the%20third%20party%20cannot%20use%20these%20fair%0Arepresentations%20for%20exploration%2C%20or%20to%20obtain%20any%20additional%20insights%2C%20besides%0Athe%20pre-contracted%20prediction%20tasks.%20Thus%2C%20to%20increase%20data%20utility%20beyond%0Aprediction%20tasks%2C%20we%20argue%20that%20the%20representations%20need%20to%20be%20fair%2C%20yet%0Ainterpretable.%20We%20propose%20a%20general%20framework%20for%20learning%20interpretable%20fair%0Arepresentations%20by%20introducing%20an%20interpretable%20%22prior%20knowledge%22%20during%20the%0Arepresentation%20learning%20process.%20We%20implement%20this%20idea%20and%20conduct%20experiments%0Awith%20ColorMNIST%20and%20Dsprite%20datasets.%20The%20results%20indicate%20that%20in%20addition%20to%0Abeing%20interpretable%2C%20our%20representations%20attain%20slightly%20higher%20accuracy%20and%0Afairer%20outcomes%20in%20a%20downstream%20classification%20task%20compared%20to%0Astate-of-the-art%20fair%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16698v1&entry.124074799=Read"},
{"title": "The Championship-Winning Solution for the 5th CLVISION Challenge 2024", "author": "Sishun Pan and Tingmin Li and Yang Yang", "abstract": "  In this paper, we introduce our approach to the 5th CLVision Challenge, which\npresents distinctive challenges beyond traditional class incremental learning.\nUnlike standard settings, this competition features the recurrence of\npreviously encountered classes and includes unlabeled data that may contain\nOut-of-Distribution (OOD) categories. Our approach is based on Winning\nSubnetworks to allocate independent parameter spaces for each task addressing\nthe catastrophic forgetting problem in class incremental learning and employ\nthree training strategies: supervised classification learning, unsupervised\ncontrastive learning, and pseudo-label classification learning to fully utilize\nthe information in both labeled and unlabeled data, enhancing the\nclassification performance of each subnetwork. Furthermore, during the\ninference stage, we have devised an interaction strategy between subnetworks,\nwhere the prediction for a specific class of a particular sample is the average\nlogits across different subnetworks corresponding to that class, leveraging the\nknowledge learned from different subnetworks on recurring classes to improve\nclassification accuracy. These strategies can be simultaneously applied to the\nthree scenarios of the competition, effectively solving the difficulties in the\ncompetition scenarios. Experimentally, our method ranks first in both the\npre-selection and final evaluation stages, with an average accuracy of 0.4535\nduring the preselection stage and an average accuracy of 0.4805 during the\nfinal evaluation stage.\n", "link": "http://arxiv.org/abs/2406.16615v1", "date": "2024-06-24", "relevancy": 2.5962, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5282}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.517}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Championship-Winning%20Solution%20for%20the%205th%20CLVISION%20Challenge%202024&body=Title%3A%20The%20Championship-Winning%20Solution%20for%20the%205th%20CLVISION%20Challenge%202024%0AAuthor%3A%20Sishun%20Pan%20and%20Tingmin%20Li%20and%20Yang%20Yang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20our%20approach%20to%20the%205th%20CLVision%20Challenge%2C%20which%0Apresents%20distinctive%20challenges%20beyond%20traditional%20class%20incremental%20learning.%0AUnlike%20standard%20settings%2C%20this%20competition%20features%20the%20recurrence%20of%0Apreviously%20encountered%20classes%20and%20includes%20unlabeled%20data%20that%20may%20contain%0AOut-of-Distribution%20%28OOD%29%20categories.%20Our%20approach%20is%20based%20on%20Winning%0ASubnetworks%20to%20allocate%20independent%20parameter%20spaces%20for%20each%20task%20addressing%0Athe%20catastrophic%20forgetting%20problem%20in%20class%20incremental%20learning%20and%20employ%0Athree%20training%20strategies%3A%20supervised%20classification%20learning%2C%20unsupervised%0Acontrastive%20learning%2C%20and%20pseudo-label%20classification%20learning%20to%20fully%20utilize%0Athe%20information%20in%20both%20labeled%20and%20unlabeled%20data%2C%20enhancing%20the%0Aclassification%20performance%20of%20each%20subnetwork.%20Furthermore%2C%20during%20the%0Ainference%20stage%2C%20we%20have%20devised%20an%20interaction%20strategy%20between%20subnetworks%2C%0Awhere%20the%20prediction%20for%20a%20specific%20class%20of%20a%20particular%20sample%20is%20the%20average%0Alogits%20across%20different%20subnetworks%20corresponding%20to%20that%20class%2C%20leveraging%20the%0Aknowledge%20learned%20from%20different%20subnetworks%20on%20recurring%20classes%20to%20improve%0Aclassification%20accuracy.%20These%20strategies%20can%20be%20simultaneously%20applied%20to%20the%0Athree%20scenarios%20of%20the%20competition%2C%20effectively%20solving%20the%20difficulties%20in%20the%0Acompetition%20scenarios.%20Experimentally%2C%20our%20method%20ranks%20first%20in%20both%20the%0Apre-selection%20and%20final%20evaluation%20stages%2C%20with%20an%20average%20accuracy%20of%200.4535%0Aduring%20the%20preselection%20stage%20and%20an%20average%20accuracy%20of%200.4805%20during%20the%0Afinal%20evaluation%20stage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Championship-Winning%2520Solution%2520for%2520the%25205th%2520CLVISION%2520Challenge%25202024%26entry.906535625%3DSishun%2520Pan%2520and%2520Tingmin%2520Li%2520and%2520Yang%2520Yang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520our%2520approach%2520to%2520the%25205th%2520CLVision%2520Challenge%252C%2520which%250Apresents%2520distinctive%2520challenges%2520beyond%2520traditional%2520class%2520incremental%2520learning.%250AUnlike%2520standard%2520settings%252C%2520this%2520competition%2520features%2520the%2520recurrence%2520of%250Apreviously%2520encountered%2520classes%2520and%2520includes%2520unlabeled%2520data%2520that%2520may%2520contain%250AOut-of-Distribution%2520%2528OOD%2529%2520categories.%2520Our%2520approach%2520is%2520based%2520on%2520Winning%250ASubnetworks%2520to%2520allocate%2520independent%2520parameter%2520spaces%2520for%2520each%2520task%2520addressing%250Athe%2520catastrophic%2520forgetting%2520problem%2520in%2520class%2520incremental%2520learning%2520and%2520employ%250Athree%2520training%2520strategies%253A%2520supervised%2520classification%2520learning%252C%2520unsupervised%250Acontrastive%2520learning%252C%2520and%2520pseudo-label%2520classification%2520learning%2520to%2520fully%2520utilize%250Athe%2520information%2520in%2520both%2520labeled%2520and%2520unlabeled%2520data%252C%2520enhancing%2520the%250Aclassification%2520performance%2520of%2520each%2520subnetwork.%2520Furthermore%252C%2520during%2520the%250Ainference%2520stage%252C%2520we%2520have%2520devised%2520an%2520interaction%2520strategy%2520between%2520subnetworks%252C%250Awhere%2520the%2520prediction%2520for%2520a%2520specific%2520class%2520of%2520a%2520particular%2520sample%2520is%2520the%2520average%250Alogits%2520across%2520different%2520subnetworks%2520corresponding%2520to%2520that%2520class%252C%2520leveraging%2520the%250Aknowledge%2520learned%2520from%2520different%2520subnetworks%2520on%2520recurring%2520classes%2520to%2520improve%250Aclassification%2520accuracy.%2520These%2520strategies%2520can%2520be%2520simultaneously%2520applied%2520to%2520the%250Athree%2520scenarios%2520of%2520the%2520competition%252C%2520effectively%2520solving%2520the%2520difficulties%2520in%2520the%250Acompetition%2520scenarios.%2520Experimentally%252C%2520our%2520method%2520ranks%2520first%2520in%2520both%2520the%250Apre-selection%2520and%2520final%2520evaluation%2520stages%252C%2520with%2520an%2520average%2520accuracy%2520of%25200.4535%250Aduring%2520the%2520preselection%2520stage%2520and%2520an%2520average%2520accuracy%2520of%25200.4805%2520during%2520the%250Afinal%2520evaluation%2520stage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Championship-Winning%20Solution%20for%20the%205th%20CLVISION%20Challenge%202024&entry.906535625=Sishun%20Pan%20and%20Tingmin%20Li%20and%20Yang%20Yang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20our%20approach%20to%20the%205th%20CLVision%20Challenge%2C%20which%0Apresents%20distinctive%20challenges%20beyond%20traditional%20class%20incremental%20learning.%0AUnlike%20standard%20settings%2C%20this%20competition%20features%20the%20recurrence%20of%0Apreviously%20encountered%20classes%20and%20includes%20unlabeled%20data%20that%20may%20contain%0AOut-of-Distribution%20%28OOD%29%20categories.%20Our%20approach%20is%20based%20on%20Winning%0ASubnetworks%20to%20allocate%20independent%20parameter%20spaces%20for%20each%20task%20addressing%0Athe%20catastrophic%20forgetting%20problem%20in%20class%20incremental%20learning%20and%20employ%0Athree%20training%20strategies%3A%20supervised%20classification%20learning%2C%20unsupervised%0Acontrastive%20learning%2C%20and%20pseudo-label%20classification%20learning%20to%20fully%20utilize%0Athe%20information%20in%20both%20labeled%20and%20unlabeled%20data%2C%20enhancing%20the%0Aclassification%20performance%20of%20each%20subnetwork.%20Furthermore%2C%20during%20the%0Ainference%20stage%2C%20we%20have%20devised%20an%20interaction%20strategy%20between%20subnetworks%2C%0Awhere%20the%20prediction%20for%20a%20specific%20class%20of%20a%20particular%20sample%20is%20the%20average%0Alogits%20across%20different%20subnetworks%20corresponding%20to%20that%20class%2C%20leveraging%20the%0Aknowledge%20learned%20from%20different%20subnetworks%20on%20recurring%20classes%20to%20improve%0Aclassification%20accuracy.%20These%20strategies%20can%20be%20simultaneously%20applied%20to%20the%0Athree%20scenarios%20of%20the%20competition%2C%20effectively%20solving%20the%20difficulties%20in%20the%0Acompetition%20scenarios.%20Experimentally%2C%20our%20method%20ranks%20first%20in%20both%20the%0Apre-selection%20and%20final%20evaluation%20stages%2C%20with%20an%20average%20accuracy%20of%200.4535%0Aduring%20the%20preselection%20stage%20and%20an%20average%20accuracy%20of%200.4805%20during%20the%0Afinal%20evaluation%20stage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16615v1&entry.124074799=Read"},
{"title": "Single-image camera calibration with model-free distortion correction", "author": "Katia Genovese", "abstract": "  Camera calibration is a process of paramount importance in computer vision\napplications that require accurate quantitative measurements. The popular\nmethod developed by Zhang relies on the use of a large number of images of a\nplanar grid of fiducial points captured in multiple poses. Although flexible\nand easy to implement, Zhang's method has some limitations. The simultaneous\noptimization of the entire parameter set, including the coefficients of a\npredefined distortion model, may result in poor distortion correction at the\nimage boundaries or in miscalculation of the intrinsic parameters, even with a\nreasonably small reprojection error. Indeed, applications involving image\nstitching (e.g. multi-camera systems) require accurate mapping of distortion up\nto the outermost regions of the image. Moreover, intrinsic parameters affect\nthe accuracy of camera pose estimation, which is fundamental for applications\nsuch as vision servoing in robot navigation and automated assembly. This paper\nproposes a method for estimating the complete set of calibration parameters\nfrom a single image of a planar speckle pattern covering the entire sensor. The\ncorrespondence between image points and physical points on the calibration\ntarget is obtained using Digital Image Correlation. The effective focal length\nand the extrinsic parameters are calculated separately after a prior evaluation\nof the principal point. At the end of the procedure, a dense and uniform\nmodel-free distortion map is obtained over the entire image. Synthetic data\nwith different noise levels were used to test the feasibility of the proposed\nmethod and to compare its metrological performance with Zhang's method.\nReal-world tests demonstrate the potential of the developed method to reveal\naspects of the image formation that are hidden by averaging over multiple\nimages.\n", "link": "http://arxiv.org/abs/2403.01263v2", "date": "2024-06-24", "relevancy": 2.5952, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5281}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5206}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-image%20camera%20calibration%20with%20model-free%20distortion%20correction&body=Title%3A%20Single-image%20camera%20calibration%20with%20model-free%20distortion%20correction%0AAuthor%3A%20Katia%20Genovese%0AAbstract%3A%20%20%20Camera%20calibration%20is%20a%20process%20of%20paramount%20importance%20in%20computer%20vision%0Aapplications%20that%20require%20accurate%20quantitative%20measurements.%20The%20popular%0Amethod%20developed%20by%20Zhang%20relies%20on%20the%20use%20of%20a%20large%20number%20of%20images%20of%20a%0Aplanar%20grid%20of%20fiducial%20points%20captured%20in%20multiple%20poses.%20Although%20flexible%0Aand%20easy%20to%20implement%2C%20Zhang%27s%20method%20has%20some%20limitations.%20The%20simultaneous%0Aoptimization%20of%20the%20entire%20parameter%20set%2C%20including%20the%20coefficients%20of%20a%0Apredefined%20distortion%20model%2C%20may%20result%20in%20poor%20distortion%20correction%20at%20the%0Aimage%20boundaries%20or%20in%20miscalculation%20of%20the%20intrinsic%20parameters%2C%20even%20with%20a%0Areasonably%20small%20reprojection%20error.%20Indeed%2C%20applications%20involving%20image%0Astitching%20%28e.g.%20multi-camera%20systems%29%20require%20accurate%20mapping%20of%20distortion%20up%0Ato%20the%20outermost%20regions%20of%20the%20image.%20Moreover%2C%20intrinsic%20parameters%20affect%0Athe%20accuracy%20of%20camera%20pose%20estimation%2C%20which%20is%20fundamental%20for%20applications%0Asuch%20as%20vision%20servoing%20in%20robot%20navigation%20and%20automated%20assembly.%20This%20paper%0Aproposes%20a%20method%20for%20estimating%20the%20complete%20set%20of%20calibration%20parameters%0Afrom%20a%20single%20image%20of%20a%20planar%20speckle%20pattern%20covering%20the%20entire%20sensor.%20The%0Acorrespondence%20between%20image%20points%20and%20physical%20points%20on%20the%20calibration%0Atarget%20is%20obtained%20using%20Digital%20Image%20Correlation.%20The%20effective%20focal%20length%0Aand%20the%20extrinsic%20parameters%20are%20calculated%20separately%20after%20a%20prior%20evaluation%0Aof%20the%20principal%20point.%20At%20the%20end%20of%20the%20procedure%2C%20a%20dense%20and%20uniform%0Amodel-free%20distortion%20map%20is%20obtained%20over%20the%20entire%20image.%20Synthetic%20data%0Awith%20different%20noise%20levels%20were%20used%20to%20test%20the%20feasibility%20of%20the%20proposed%0Amethod%20and%20to%20compare%20its%20metrological%20performance%20with%20Zhang%27s%20method.%0AReal-world%20tests%20demonstrate%20the%20potential%20of%20the%20developed%20method%20to%20reveal%0Aaspects%20of%20the%20image%20formation%20that%20are%20hidden%20by%20averaging%20over%20multiple%0Aimages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01263v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-image%2520camera%2520calibration%2520with%2520model-free%2520distortion%2520correction%26entry.906535625%3DKatia%2520Genovese%26entry.1292438233%3D%2520%2520Camera%2520calibration%2520is%2520a%2520process%2520of%2520paramount%2520importance%2520in%2520computer%2520vision%250Aapplications%2520that%2520require%2520accurate%2520quantitative%2520measurements.%2520The%2520popular%250Amethod%2520developed%2520by%2520Zhang%2520relies%2520on%2520the%2520use%2520of%2520a%2520large%2520number%2520of%2520images%2520of%2520a%250Aplanar%2520grid%2520of%2520fiducial%2520points%2520captured%2520in%2520multiple%2520poses.%2520Although%2520flexible%250Aand%2520easy%2520to%2520implement%252C%2520Zhang%2527s%2520method%2520has%2520some%2520limitations.%2520The%2520simultaneous%250Aoptimization%2520of%2520the%2520entire%2520parameter%2520set%252C%2520including%2520the%2520coefficients%2520of%2520a%250Apredefined%2520distortion%2520model%252C%2520may%2520result%2520in%2520poor%2520distortion%2520correction%2520at%2520the%250Aimage%2520boundaries%2520or%2520in%2520miscalculation%2520of%2520the%2520intrinsic%2520parameters%252C%2520even%2520with%2520a%250Areasonably%2520small%2520reprojection%2520error.%2520Indeed%252C%2520applications%2520involving%2520image%250Astitching%2520%2528e.g.%2520multi-camera%2520systems%2529%2520require%2520accurate%2520mapping%2520of%2520distortion%2520up%250Ato%2520the%2520outermost%2520regions%2520of%2520the%2520image.%2520Moreover%252C%2520intrinsic%2520parameters%2520affect%250Athe%2520accuracy%2520of%2520camera%2520pose%2520estimation%252C%2520which%2520is%2520fundamental%2520for%2520applications%250Asuch%2520as%2520vision%2520servoing%2520in%2520robot%2520navigation%2520and%2520automated%2520assembly.%2520This%2520paper%250Aproposes%2520a%2520method%2520for%2520estimating%2520the%2520complete%2520set%2520of%2520calibration%2520parameters%250Afrom%2520a%2520single%2520image%2520of%2520a%2520planar%2520speckle%2520pattern%2520covering%2520the%2520entire%2520sensor.%2520The%250Acorrespondence%2520between%2520image%2520points%2520and%2520physical%2520points%2520on%2520the%2520calibration%250Atarget%2520is%2520obtained%2520using%2520Digital%2520Image%2520Correlation.%2520The%2520effective%2520focal%2520length%250Aand%2520the%2520extrinsic%2520parameters%2520are%2520calculated%2520separately%2520after%2520a%2520prior%2520evaluation%250Aof%2520the%2520principal%2520point.%2520At%2520the%2520end%2520of%2520the%2520procedure%252C%2520a%2520dense%2520and%2520uniform%250Amodel-free%2520distortion%2520map%2520is%2520obtained%2520over%2520the%2520entire%2520image.%2520Synthetic%2520data%250Awith%2520different%2520noise%2520levels%2520were%2520used%2520to%2520test%2520the%2520feasibility%2520of%2520the%2520proposed%250Amethod%2520and%2520to%2520compare%2520its%2520metrological%2520performance%2520with%2520Zhang%2527s%2520method.%250AReal-world%2520tests%2520demonstrate%2520the%2520potential%2520of%2520the%2520developed%2520method%2520to%2520reveal%250Aaspects%2520of%2520the%2520image%2520formation%2520that%2520are%2520hidden%2520by%2520averaging%2520over%2520multiple%250Aimages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01263v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-image%20camera%20calibration%20with%20model-free%20distortion%20correction&entry.906535625=Katia%20Genovese&entry.1292438233=%20%20Camera%20calibration%20is%20a%20process%20of%20paramount%20importance%20in%20computer%20vision%0Aapplications%20that%20require%20accurate%20quantitative%20measurements.%20The%20popular%0Amethod%20developed%20by%20Zhang%20relies%20on%20the%20use%20of%20a%20large%20number%20of%20images%20of%20a%0Aplanar%20grid%20of%20fiducial%20points%20captured%20in%20multiple%20poses.%20Although%20flexible%0Aand%20easy%20to%20implement%2C%20Zhang%27s%20method%20has%20some%20limitations.%20The%20simultaneous%0Aoptimization%20of%20the%20entire%20parameter%20set%2C%20including%20the%20coefficients%20of%20a%0Apredefined%20distortion%20model%2C%20may%20result%20in%20poor%20distortion%20correction%20at%20the%0Aimage%20boundaries%20or%20in%20miscalculation%20of%20the%20intrinsic%20parameters%2C%20even%20with%20a%0Areasonably%20small%20reprojection%20error.%20Indeed%2C%20applications%20involving%20image%0Astitching%20%28e.g.%20multi-camera%20systems%29%20require%20accurate%20mapping%20of%20distortion%20up%0Ato%20the%20outermost%20regions%20of%20the%20image.%20Moreover%2C%20intrinsic%20parameters%20affect%0Athe%20accuracy%20of%20camera%20pose%20estimation%2C%20which%20is%20fundamental%20for%20applications%0Asuch%20as%20vision%20servoing%20in%20robot%20navigation%20and%20automated%20assembly.%20This%20paper%0Aproposes%20a%20method%20for%20estimating%20the%20complete%20set%20of%20calibration%20parameters%0Afrom%20a%20single%20image%20of%20a%20planar%20speckle%20pattern%20covering%20the%20entire%20sensor.%20The%0Acorrespondence%20between%20image%20points%20and%20physical%20points%20on%20the%20calibration%0Atarget%20is%20obtained%20using%20Digital%20Image%20Correlation.%20The%20effective%20focal%20length%0Aand%20the%20extrinsic%20parameters%20are%20calculated%20separately%20after%20a%20prior%20evaluation%0Aof%20the%20principal%20point.%20At%20the%20end%20of%20the%20procedure%2C%20a%20dense%20and%20uniform%0Amodel-free%20distortion%20map%20is%20obtained%20over%20the%20entire%20image.%20Synthetic%20data%0Awith%20different%20noise%20levels%20were%20used%20to%20test%20the%20feasibility%20of%20the%20proposed%0Amethod%20and%20to%20compare%20its%20metrological%20performance%20with%20Zhang%27s%20method.%0AReal-world%20tests%20demonstrate%20the%20potential%20of%20the%20developed%20method%20to%20reveal%0Aaspects%20of%20the%20image%20formation%20that%20are%20hidden%20by%20averaging%20over%20multiple%0Aimages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01263v2&entry.124074799=Read"},
{"title": "NeST: Neural Stress Tensor Tomography by leveraging 3D Photoelasticity", "author": "Akshat Dave and Tianyi Zhang and Aaron Young and Ramesh Raskar and Wolfgang Heidrich and Ashok Veeraraghavan", "abstract": "  Photoelasticity enables full-field stress analysis in transparent objects\nthrough stress-induced birefringence. Existing techniques are limited to 2D\nslices and require destructively slicing the object. Recovering the internal 3D\nstress distribution of the entire object is challenging as it involves solving\na tensor tomography problem and handling phase wrapping ambiguities. We\nintroduce NeST, an analysis-by-synthesis approach for reconstructing 3D stress\ntensor fields as neural implicit representations from polarization\nmeasurements. Our key insight is to jointly handle phase unwrapping and tensor\ntomography using a differentiable forward model based on Jones calculus. Our\nnon-linear model faithfully matches real captures, unlike prior linear\napproximations. We develop an experimental multi-axis polariscope setup to\ncapture 3D photoelasticity and experimentally demonstrate that NeST\nreconstructs the internal stress distribution for objects with varying shape\nand force conditions. Additionally, we showcase novel applications in stress\nanalysis, such as visualizing photoelastic fringes by virtually slicing the\nobject and viewing photoelastic fringes from unseen viewpoints. NeST paves the\nway for scalable non-destructive 3D photoelastic analysis.\n", "link": "http://arxiv.org/abs/2406.10212v2", "date": "2024-06-24", "relevancy": 2.5646, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5177}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5177}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeST%3A%20Neural%20Stress%20Tensor%20Tomography%20by%20leveraging%203D%20Photoelasticity&body=Title%3A%20NeST%3A%20Neural%20Stress%20Tensor%20Tomography%20by%20leveraging%203D%20Photoelasticity%0AAuthor%3A%20Akshat%20Dave%20and%20Tianyi%20Zhang%20and%20Aaron%20Young%20and%20Ramesh%20Raskar%20and%20Wolfgang%20Heidrich%20and%20Ashok%20Veeraraghavan%0AAbstract%3A%20%20%20Photoelasticity%20enables%20full-field%20stress%20analysis%20in%20transparent%20objects%0Athrough%20stress-induced%20birefringence.%20Existing%20techniques%20are%20limited%20to%202D%0Aslices%20and%20require%20destructively%20slicing%20the%20object.%20Recovering%20the%20internal%203D%0Astress%20distribution%20of%20the%20entire%20object%20is%20challenging%20as%20it%20involves%20solving%0Aa%20tensor%20tomography%20problem%20and%20handling%20phase%20wrapping%20ambiguities.%20We%0Aintroduce%20NeST%2C%20an%20analysis-by-synthesis%20approach%20for%20reconstructing%203D%20stress%0Atensor%20fields%20as%20neural%20implicit%20representations%20from%20polarization%0Ameasurements.%20Our%20key%20insight%20is%20to%20jointly%20handle%20phase%20unwrapping%20and%20tensor%0Atomography%20using%20a%20differentiable%20forward%20model%20based%20on%20Jones%20calculus.%20Our%0Anon-linear%20model%20faithfully%20matches%20real%20captures%2C%20unlike%20prior%20linear%0Aapproximations.%20We%20develop%20an%20experimental%20multi-axis%20polariscope%20setup%20to%0Acapture%203D%20photoelasticity%20and%20experimentally%20demonstrate%20that%20NeST%0Areconstructs%20the%20internal%20stress%20distribution%20for%20objects%20with%20varying%20shape%0Aand%20force%20conditions.%20Additionally%2C%20we%20showcase%20novel%20applications%20in%20stress%0Aanalysis%2C%20such%20as%20visualizing%20photoelastic%20fringes%20by%20virtually%20slicing%20the%0Aobject%20and%20viewing%20photoelastic%20fringes%20from%20unseen%20viewpoints.%20NeST%20paves%20the%0Away%20for%20scalable%20non-destructive%203D%20photoelastic%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10212v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeST%253A%2520Neural%2520Stress%2520Tensor%2520Tomography%2520by%2520leveraging%25203D%2520Photoelasticity%26entry.906535625%3DAkshat%2520Dave%2520and%2520Tianyi%2520Zhang%2520and%2520Aaron%2520Young%2520and%2520Ramesh%2520Raskar%2520and%2520Wolfgang%2520Heidrich%2520and%2520Ashok%2520Veeraraghavan%26entry.1292438233%3D%2520%2520Photoelasticity%2520enables%2520full-field%2520stress%2520analysis%2520in%2520transparent%2520objects%250Athrough%2520stress-induced%2520birefringence.%2520Existing%2520techniques%2520are%2520limited%2520to%25202D%250Aslices%2520and%2520require%2520destructively%2520slicing%2520the%2520object.%2520Recovering%2520the%2520internal%25203D%250Astress%2520distribution%2520of%2520the%2520entire%2520object%2520is%2520challenging%2520as%2520it%2520involves%2520solving%250Aa%2520tensor%2520tomography%2520problem%2520and%2520handling%2520phase%2520wrapping%2520ambiguities.%2520We%250Aintroduce%2520NeST%252C%2520an%2520analysis-by-synthesis%2520approach%2520for%2520reconstructing%25203D%2520stress%250Atensor%2520fields%2520as%2520neural%2520implicit%2520representations%2520from%2520polarization%250Ameasurements.%2520Our%2520key%2520insight%2520is%2520to%2520jointly%2520handle%2520phase%2520unwrapping%2520and%2520tensor%250Atomography%2520using%2520a%2520differentiable%2520forward%2520model%2520based%2520on%2520Jones%2520calculus.%2520Our%250Anon-linear%2520model%2520faithfully%2520matches%2520real%2520captures%252C%2520unlike%2520prior%2520linear%250Aapproximations.%2520We%2520develop%2520an%2520experimental%2520multi-axis%2520polariscope%2520setup%2520to%250Acapture%25203D%2520photoelasticity%2520and%2520experimentally%2520demonstrate%2520that%2520NeST%250Areconstructs%2520the%2520internal%2520stress%2520distribution%2520for%2520objects%2520with%2520varying%2520shape%250Aand%2520force%2520conditions.%2520Additionally%252C%2520we%2520showcase%2520novel%2520applications%2520in%2520stress%250Aanalysis%252C%2520such%2520as%2520visualizing%2520photoelastic%2520fringes%2520by%2520virtually%2520slicing%2520the%250Aobject%2520and%2520viewing%2520photoelastic%2520fringes%2520from%2520unseen%2520viewpoints.%2520NeST%2520paves%2520the%250Away%2520for%2520scalable%2520non-destructive%25203D%2520photoelastic%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10212v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeST%3A%20Neural%20Stress%20Tensor%20Tomography%20by%20leveraging%203D%20Photoelasticity&entry.906535625=Akshat%20Dave%20and%20Tianyi%20Zhang%20and%20Aaron%20Young%20and%20Ramesh%20Raskar%20and%20Wolfgang%20Heidrich%20and%20Ashok%20Veeraraghavan&entry.1292438233=%20%20Photoelasticity%20enables%20full-field%20stress%20analysis%20in%20transparent%20objects%0Athrough%20stress-induced%20birefringence.%20Existing%20techniques%20are%20limited%20to%202D%0Aslices%20and%20require%20destructively%20slicing%20the%20object.%20Recovering%20the%20internal%203D%0Astress%20distribution%20of%20the%20entire%20object%20is%20challenging%20as%20it%20involves%20solving%0Aa%20tensor%20tomography%20problem%20and%20handling%20phase%20wrapping%20ambiguities.%20We%0Aintroduce%20NeST%2C%20an%20analysis-by-synthesis%20approach%20for%20reconstructing%203D%20stress%0Atensor%20fields%20as%20neural%20implicit%20representations%20from%20polarization%0Ameasurements.%20Our%20key%20insight%20is%20to%20jointly%20handle%20phase%20unwrapping%20and%20tensor%0Atomography%20using%20a%20differentiable%20forward%20model%20based%20on%20Jones%20calculus.%20Our%0Anon-linear%20model%20faithfully%20matches%20real%20captures%2C%20unlike%20prior%20linear%0Aapproximations.%20We%20develop%20an%20experimental%20multi-axis%20polariscope%20setup%20to%0Acapture%203D%20photoelasticity%20and%20experimentally%20demonstrate%20that%20NeST%0Areconstructs%20the%20internal%20stress%20distribution%20for%20objects%20with%20varying%20shape%0Aand%20force%20conditions.%20Additionally%2C%20we%20showcase%20novel%20applications%20in%20stress%0Aanalysis%2C%20such%20as%20visualizing%20photoelastic%20fringes%20by%20virtually%20slicing%20the%0Aobject%20and%20viewing%20photoelastic%20fringes%20from%20unseen%20viewpoints.%20NeST%20paves%20the%0Away%20for%20scalable%20non-destructive%203D%20photoelastic%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10212v2&entry.124074799=Read"},
{"title": "Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs", "author": "Ashwinee Panda and Berivan Isik and Xiangyu Qi and Sanmi Koyejo and Tsachy Weissman and Prateek Mittal", "abstract": "  Existing methods for adapting large language models (LLMs) to new tasks are\nnot suited to multi-task adaptation because they modify all the model weights\n-- causing destructive interference between tasks. The resulting effects, such\nas catastrophic forgetting of earlier tasks, make it challenging to obtain good\nperformance on multiple tasks at the same time. To mitigate this, we propose\nLottery Ticket Adaptation (LoTA), a sparse adaptation method that identifies\nand optimizes only a sparse subnetwork of the model. We evaluate LoTA on a wide\nrange of challenging tasks such as instruction following, reasoning, math, and\nsummarization. LoTA obtains better performance than full fine-tuning and\nlow-rank adaptation (LoRA), and maintains good performance even after training\non other tasks -- thus, avoiding catastrophic forgetting. By extracting and\nfine-tuning over \\emph{lottery tickets} (or \\emph{sparse task vectors}), LoTA\nalso enables model merging over highly dissimilar tasks.\n", "link": "http://arxiv.org/abs/2406.16797v1", "date": "2024-06-24", "relevancy": 2.5611, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5345}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5136}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lottery%20Ticket%20Adaptation%3A%20Mitigating%20Destructive%20Interference%20in%20LLMs&body=Title%3A%20Lottery%20Ticket%20Adaptation%3A%20Mitigating%20Destructive%20Interference%20in%20LLMs%0AAuthor%3A%20Ashwinee%20Panda%20and%20Berivan%20Isik%20and%20Xiangyu%20Qi%20and%20Sanmi%20Koyejo%20and%20Tsachy%20Weissman%20and%20Prateek%20Mittal%0AAbstract%3A%20%20%20Existing%20methods%20for%20adapting%20large%20language%20models%20%28LLMs%29%20to%20new%20tasks%20are%0Anot%20suited%20to%20multi-task%20adaptation%20because%20they%20modify%20all%20the%20model%20weights%0A--%20causing%20destructive%20interference%20between%20tasks.%20The%20resulting%20effects%2C%20such%0Aas%20catastrophic%20forgetting%20of%20earlier%20tasks%2C%20make%20it%20challenging%20to%20obtain%20good%0Aperformance%20on%20multiple%20tasks%20at%20the%20same%20time.%20To%20mitigate%20this%2C%20we%20propose%0ALottery%20Ticket%20Adaptation%20%28LoTA%29%2C%20a%20sparse%20adaptation%20method%20that%20identifies%0Aand%20optimizes%20only%20a%20sparse%20subnetwork%20of%20the%20model.%20We%20evaluate%20LoTA%20on%20a%20wide%0Arange%20of%20challenging%20tasks%20such%20as%20instruction%20following%2C%20reasoning%2C%20math%2C%20and%0Asummarization.%20LoTA%20obtains%20better%20performance%20than%20full%20fine-tuning%20and%0Alow-rank%20adaptation%20%28LoRA%29%2C%20and%20maintains%20good%20performance%20even%20after%20training%0Aon%20other%20tasks%20--%20thus%2C%20avoiding%20catastrophic%20forgetting.%20By%20extracting%20and%0Afine-tuning%20over%20%5Cemph%7Blottery%20tickets%7D%20%28or%20%5Cemph%7Bsparse%20task%20vectors%7D%29%2C%20LoTA%0Aalso%20enables%20model%20merging%20over%20highly%20dissimilar%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16797v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLottery%2520Ticket%2520Adaptation%253A%2520Mitigating%2520Destructive%2520Interference%2520in%2520LLMs%26entry.906535625%3DAshwinee%2520Panda%2520and%2520Berivan%2520Isik%2520and%2520Xiangyu%2520Qi%2520and%2520Sanmi%2520Koyejo%2520and%2520Tsachy%2520Weissman%2520and%2520Prateek%2520Mittal%26entry.1292438233%3D%2520%2520Existing%2520methods%2520for%2520adapting%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520new%2520tasks%2520are%250Anot%2520suited%2520to%2520multi-task%2520adaptation%2520because%2520they%2520modify%2520all%2520the%2520model%2520weights%250A--%2520causing%2520destructive%2520interference%2520between%2520tasks.%2520The%2520resulting%2520effects%252C%2520such%250Aas%2520catastrophic%2520forgetting%2520of%2520earlier%2520tasks%252C%2520make%2520it%2520challenging%2520to%2520obtain%2520good%250Aperformance%2520on%2520multiple%2520tasks%2520at%2520the%2520same%2520time.%2520To%2520mitigate%2520this%252C%2520we%2520propose%250ALottery%2520Ticket%2520Adaptation%2520%2528LoTA%2529%252C%2520a%2520sparse%2520adaptation%2520method%2520that%2520identifies%250Aand%2520optimizes%2520only%2520a%2520sparse%2520subnetwork%2520of%2520the%2520model.%2520We%2520evaluate%2520LoTA%2520on%2520a%2520wide%250Arange%2520of%2520challenging%2520tasks%2520such%2520as%2520instruction%2520following%252C%2520reasoning%252C%2520math%252C%2520and%250Asummarization.%2520LoTA%2520obtains%2520better%2520performance%2520than%2520full%2520fine-tuning%2520and%250Alow-rank%2520adaptation%2520%2528LoRA%2529%252C%2520and%2520maintains%2520good%2520performance%2520even%2520after%2520training%250Aon%2520other%2520tasks%2520--%2520thus%252C%2520avoiding%2520catastrophic%2520forgetting.%2520By%2520extracting%2520and%250Afine-tuning%2520over%2520%255Cemph%257Blottery%2520tickets%257D%2520%2528or%2520%255Cemph%257Bsparse%2520task%2520vectors%257D%2529%252C%2520LoTA%250Aalso%2520enables%2520model%2520merging%2520over%2520highly%2520dissimilar%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16797v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lottery%20Ticket%20Adaptation%3A%20Mitigating%20Destructive%20Interference%20in%20LLMs&entry.906535625=Ashwinee%20Panda%20and%20Berivan%20Isik%20and%20Xiangyu%20Qi%20and%20Sanmi%20Koyejo%20and%20Tsachy%20Weissman%20and%20Prateek%20Mittal&entry.1292438233=%20%20Existing%20methods%20for%20adapting%20large%20language%20models%20%28LLMs%29%20to%20new%20tasks%20are%0Anot%20suited%20to%20multi-task%20adaptation%20because%20they%20modify%20all%20the%20model%20weights%0A--%20causing%20destructive%20interference%20between%20tasks.%20The%20resulting%20effects%2C%20such%0Aas%20catastrophic%20forgetting%20of%20earlier%20tasks%2C%20make%20it%20challenging%20to%20obtain%20good%0Aperformance%20on%20multiple%20tasks%20at%20the%20same%20time.%20To%20mitigate%20this%2C%20we%20propose%0ALottery%20Ticket%20Adaptation%20%28LoTA%29%2C%20a%20sparse%20adaptation%20method%20that%20identifies%0Aand%20optimizes%20only%20a%20sparse%20subnetwork%20of%20the%20model.%20We%20evaluate%20LoTA%20on%20a%20wide%0Arange%20of%20challenging%20tasks%20such%20as%20instruction%20following%2C%20reasoning%2C%20math%2C%20and%0Asummarization.%20LoTA%20obtains%20better%20performance%20than%20full%20fine-tuning%20and%0Alow-rank%20adaptation%20%28LoRA%29%2C%20and%20maintains%20good%20performance%20even%20after%20training%0Aon%20other%20tasks%20--%20thus%2C%20avoiding%20catastrophic%20forgetting.%20By%20extracting%20and%0Afine-tuning%20over%20%5Cemph%7Blottery%20tickets%7D%20%28or%20%5Cemph%7Bsparse%20task%20vectors%7D%29%2C%20LoTA%0Aalso%20enables%20model%20merging%20over%20highly%20dissimilar%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16797v1&entry.124074799=Read"},
{"title": "Do As I Do: Pose Guided Human Motion Copy", "author": "Sifan Wu and Zhenguang Liu and Beibei Zhang and Roger Zimmermann and Zhongjie Ba and Xiaosong Zhang and Kui Ren", "abstract": "  Human motion copy is an intriguing yet challenging task in artificial\nintelligence and computer vision, which strives to generate a fake video of a\ntarget person performing the motion of a source person. The problem is\ninherently challenging due to the subtle human-body texture details to be\ngenerated and the temporal consistency to be considered. Existing approaches\ntypically adopt a conventional GAN with an L1 or L2 loss to produce the target\nfake video, which intrinsically necessitates a large number of training samples\nthat are challenging to acquire. Meanwhile, current methods still have\ndifficulties in attaining realistic image details and temporal consistency,\nwhich unfortunately can be easily perceived by human observers. Motivated by\nthis, we try to tackle the issues from three aspects: (1) We constrain\npose-to-appearance generation with a perceptual loss and a theoretically\nmotivated Gromov-Wasserstein loss to bridge the gap between pose and\nappearance. (2) We present an episodic memory module in the pose-to-appearance\ngeneration to propel continuous learning that helps the model learn from its\npast poor generations. We also utilize geometrical cues of the face to optimize\nfacial details and refine each key body part with a dedicated local GAN. (3) We\nadvocate generating the foreground in a sequence-to-sequence manner rather than\na single-frame manner, explicitly enforcing temporal inconsistency. Empirical\nresults on five datasets, iPER, ComplexMotion, SoloDance, Fish, and Mouse\ndatasets, demonstrate that our method is capable of generating realistic target\nvideos while precisely copying motion from a source video. Our method\nsignificantly outperforms state-of-the-art approaches and gains 7.2% and 12.4%\nimprovements in PSNR and FID respectively.\n", "link": "http://arxiv.org/abs/2406.16601v1", "date": "2024-06-24", "relevancy": 2.5576, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6743}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6176}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20As%20I%20Do%3A%20Pose%20Guided%20Human%20Motion%20Copy&body=Title%3A%20Do%20As%20I%20Do%3A%20Pose%20Guided%20Human%20Motion%20Copy%0AAuthor%3A%20Sifan%20Wu%20and%20Zhenguang%20Liu%20and%20Beibei%20Zhang%20and%20Roger%20Zimmermann%20and%20Zhongjie%20Ba%20and%20Xiaosong%20Zhang%20and%20Kui%20Ren%0AAbstract%3A%20%20%20Human%20motion%20copy%20is%20an%20intriguing%20yet%20challenging%20task%20in%20artificial%0Aintelligence%20and%20computer%20vision%2C%20which%20strives%20to%20generate%20a%20fake%20video%20of%20a%0Atarget%20person%20performing%20the%20motion%20of%20a%20source%20person.%20The%20problem%20is%0Ainherently%20challenging%20due%20to%20the%20subtle%20human-body%20texture%20details%20to%20be%0Agenerated%20and%20the%20temporal%20consistency%20to%20be%20considered.%20Existing%20approaches%0Atypically%20adopt%20a%20conventional%20GAN%20with%20an%20L1%20or%20L2%20loss%20to%20produce%20the%20target%0Afake%20video%2C%20which%20intrinsically%20necessitates%20a%20large%20number%20of%20training%20samples%0Athat%20are%20challenging%20to%20acquire.%20Meanwhile%2C%20current%20methods%20still%20have%0Adifficulties%20in%20attaining%20realistic%20image%20details%20and%20temporal%20consistency%2C%0Awhich%20unfortunately%20can%20be%20easily%20perceived%20by%20human%20observers.%20Motivated%20by%0Athis%2C%20we%20try%20to%20tackle%20the%20issues%20from%20three%20aspects%3A%20%281%29%20We%20constrain%0Apose-to-appearance%20generation%20with%20a%20perceptual%20loss%20and%20a%20theoretically%0Amotivated%20Gromov-Wasserstein%20loss%20to%20bridge%20the%20gap%20between%20pose%20and%0Aappearance.%20%282%29%20We%20present%20an%20episodic%20memory%20module%20in%20the%20pose-to-appearance%0Ageneration%20to%20propel%20continuous%20learning%20that%20helps%20the%20model%20learn%20from%20its%0Apast%20poor%20generations.%20We%20also%20utilize%20geometrical%20cues%20of%20the%20face%20to%20optimize%0Afacial%20details%20and%20refine%20each%20key%20body%20part%20with%20a%20dedicated%20local%20GAN.%20%283%29%20We%0Aadvocate%20generating%20the%20foreground%20in%20a%20sequence-to-sequence%20manner%20rather%20than%0Aa%20single-frame%20manner%2C%20explicitly%20enforcing%20temporal%20inconsistency.%20Empirical%0Aresults%20on%20five%20datasets%2C%20iPER%2C%20ComplexMotion%2C%20SoloDance%2C%20Fish%2C%20and%20Mouse%0Adatasets%2C%20demonstrate%20that%20our%20method%20is%20capable%20of%20generating%20realistic%20target%0Avideos%20while%20precisely%20copying%20motion%20from%20a%20source%20video.%20Our%20method%0Asignificantly%20outperforms%20state-of-the-art%20approaches%20and%20gains%207.2%25%20and%2012.4%25%0Aimprovements%20in%20PSNR%20and%20FID%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520As%2520I%2520Do%253A%2520Pose%2520Guided%2520Human%2520Motion%2520Copy%26entry.906535625%3DSifan%2520Wu%2520and%2520Zhenguang%2520Liu%2520and%2520Beibei%2520Zhang%2520and%2520Roger%2520Zimmermann%2520and%2520Zhongjie%2520Ba%2520and%2520Xiaosong%2520Zhang%2520and%2520Kui%2520Ren%26entry.1292438233%3D%2520%2520Human%2520motion%2520copy%2520is%2520an%2520intriguing%2520yet%2520challenging%2520task%2520in%2520artificial%250Aintelligence%2520and%2520computer%2520vision%252C%2520which%2520strives%2520to%2520generate%2520a%2520fake%2520video%2520of%2520a%250Atarget%2520person%2520performing%2520the%2520motion%2520of%2520a%2520source%2520person.%2520The%2520problem%2520is%250Ainherently%2520challenging%2520due%2520to%2520the%2520subtle%2520human-body%2520texture%2520details%2520to%2520be%250Agenerated%2520and%2520the%2520temporal%2520consistency%2520to%2520be%2520considered.%2520Existing%2520approaches%250Atypically%2520adopt%2520a%2520conventional%2520GAN%2520with%2520an%2520L1%2520or%2520L2%2520loss%2520to%2520produce%2520the%2520target%250Afake%2520video%252C%2520which%2520intrinsically%2520necessitates%2520a%2520large%2520number%2520of%2520training%2520samples%250Athat%2520are%2520challenging%2520to%2520acquire.%2520Meanwhile%252C%2520current%2520methods%2520still%2520have%250Adifficulties%2520in%2520attaining%2520realistic%2520image%2520details%2520and%2520temporal%2520consistency%252C%250Awhich%2520unfortunately%2520can%2520be%2520easily%2520perceived%2520by%2520human%2520observers.%2520Motivated%2520by%250Athis%252C%2520we%2520try%2520to%2520tackle%2520the%2520issues%2520from%2520three%2520aspects%253A%2520%25281%2529%2520We%2520constrain%250Apose-to-appearance%2520generation%2520with%2520a%2520perceptual%2520loss%2520and%2520a%2520theoretically%250Amotivated%2520Gromov-Wasserstein%2520loss%2520to%2520bridge%2520the%2520gap%2520between%2520pose%2520and%250Aappearance.%2520%25282%2529%2520We%2520present%2520an%2520episodic%2520memory%2520module%2520in%2520the%2520pose-to-appearance%250Ageneration%2520to%2520propel%2520continuous%2520learning%2520that%2520helps%2520the%2520model%2520learn%2520from%2520its%250Apast%2520poor%2520generations.%2520We%2520also%2520utilize%2520geometrical%2520cues%2520of%2520the%2520face%2520to%2520optimize%250Afacial%2520details%2520and%2520refine%2520each%2520key%2520body%2520part%2520with%2520a%2520dedicated%2520local%2520GAN.%2520%25283%2529%2520We%250Aadvocate%2520generating%2520the%2520foreground%2520in%2520a%2520sequence-to-sequence%2520manner%2520rather%2520than%250Aa%2520single-frame%2520manner%252C%2520explicitly%2520enforcing%2520temporal%2520inconsistency.%2520Empirical%250Aresults%2520on%2520five%2520datasets%252C%2520iPER%252C%2520ComplexMotion%252C%2520SoloDance%252C%2520Fish%252C%2520and%2520Mouse%250Adatasets%252C%2520demonstrate%2520that%2520our%2520method%2520is%2520capable%2520of%2520generating%2520realistic%2520target%250Avideos%2520while%2520precisely%2520copying%2520motion%2520from%2520a%2520source%2520video.%2520Our%2520method%250Asignificantly%2520outperforms%2520state-of-the-art%2520approaches%2520and%2520gains%25207.2%2525%2520and%252012.4%2525%250Aimprovements%2520in%2520PSNR%2520and%2520FID%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20As%20I%20Do%3A%20Pose%20Guided%20Human%20Motion%20Copy&entry.906535625=Sifan%20Wu%20and%20Zhenguang%20Liu%20and%20Beibei%20Zhang%20and%20Roger%20Zimmermann%20and%20Zhongjie%20Ba%20and%20Xiaosong%20Zhang%20and%20Kui%20Ren&entry.1292438233=%20%20Human%20motion%20copy%20is%20an%20intriguing%20yet%20challenging%20task%20in%20artificial%0Aintelligence%20and%20computer%20vision%2C%20which%20strives%20to%20generate%20a%20fake%20video%20of%20a%0Atarget%20person%20performing%20the%20motion%20of%20a%20source%20person.%20The%20problem%20is%0Ainherently%20challenging%20due%20to%20the%20subtle%20human-body%20texture%20details%20to%20be%0Agenerated%20and%20the%20temporal%20consistency%20to%20be%20considered.%20Existing%20approaches%0Atypically%20adopt%20a%20conventional%20GAN%20with%20an%20L1%20or%20L2%20loss%20to%20produce%20the%20target%0Afake%20video%2C%20which%20intrinsically%20necessitates%20a%20large%20number%20of%20training%20samples%0Athat%20are%20challenging%20to%20acquire.%20Meanwhile%2C%20current%20methods%20still%20have%0Adifficulties%20in%20attaining%20realistic%20image%20details%20and%20temporal%20consistency%2C%0Awhich%20unfortunately%20can%20be%20easily%20perceived%20by%20human%20observers.%20Motivated%20by%0Athis%2C%20we%20try%20to%20tackle%20the%20issues%20from%20three%20aspects%3A%20%281%29%20We%20constrain%0Apose-to-appearance%20generation%20with%20a%20perceptual%20loss%20and%20a%20theoretically%0Amotivated%20Gromov-Wasserstein%20loss%20to%20bridge%20the%20gap%20between%20pose%20and%0Aappearance.%20%282%29%20We%20present%20an%20episodic%20memory%20module%20in%20the%20pose-to-appearance%0Ageneration%20to%20propel%20continuous%20learning%20that%20helps%20the%20model%20learn%20from%20its%0Apast%20poor%20generations.%20We%20also%20utilize%20geometrical%20cues%20of%20the%20face%20to%20optimize%0Afacial%20details%20and%20refine%20each%20key%20body%20part%20with%20a%20dedicated%20local%20GAN.%20%283%29%20We%0Aadvocate%20generating%20the%20foreground%20in%20a%20sequence-to-sequence%20manner%20rather%20than%0Aa%20single-frame%20manner%2C%20explicitly%20enforcing%20temporal%20inconsistency.%20Empirical%0Aresults%20on%20five%20datasets%2C%20iPER%2C%20ComplexMotion%2C%20SoloDance%2C%20Fish%2C%20and%20Mouse%0Adatasets%2C%20demonstrate%20that%20our%20method%20is%20capable%20of%20generating%20realistic%20target%0Avideos%20while%20precisely%20copying%20motion%20from%20a%20source%20video.%20Our%20method%0Asignificantly%20outperforms%20state-of-the-art%20approaches%20and%20gains%207.2%25%20and%2012.4%25%0Aimprovements%20in%20PSNR%20and%20FID%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16601v1&entry.124074799=Read"},
{"title": "Portrait3D: 3D Head Generation from Single In-the-wild Portrait Image", "author": "Jinkun Hao and Junshu Tang and Jiangning Zhang and Ran Yi and Yijia Hong and Moran Li and Weijian Cao and Yating Wang and Lizhuang Ma", "abstract": "  While recent works have achieved great success on one-shot 3D common object\ngeneration, high quality and fidelity 3D head generation from a single image\nremains a great challenge. Previous text-based methods for generating 3D heads\nwere limited by text descriptions and image-based methods struggled to produce\nhigh-quality head geometry. To handle this challenging problem, we propose a\nnovel framework, Portrait3D, to generate high-quality 3D heads while preserving\ntheir identities. Our work incorporates the identity information of the\nportrait image into three parts: 1) geometry initialization, 2) geometry\nsculpting, and 3) texture generation stages. Given a reference portrait image,\nwe first align the identity features with text features to realize ID-aware\nguidance enhancement, which contains the control signals representing the face\ninformation. We then use the canny map, ID features of the portrait image, and\na pre-trained text-to-normal/depth diffusion model to generate ID-aware\ngeometry supervision, and 3D-GAN inversion is employed to generate ID-aware\ngeometry initialization. Furthermore, with the ability to inject identity\ninformation into 3D head generation, we use ID-aware guidance to calculate\nID-aware Score Distillation (ISD) for geometry sculpting. For texture\ngeneration, we adopt the ID Consistent Texture Inpainting and Refinement which\nprogressively expands the view for texture inpainting to obtain an\ninitialization UV texture map. We then use the id-aware guidance to provide\nimage-level supervision for noisy multi-view images to obtain a refined texture\nmap. Extensive experiments demonstrate that we can generate high-quality 3D\nheads with accurate geometry and texture from single in-the-wild portrait\nimages. The project page is at https://jinkun-hao.github.io/Portrait3D/.\n", "link": "http://arxiv.org/abs/2406.16710v1", "date": "2024-06-24", "relevancy": 2.5124, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.6522}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6233}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Portrait3D%3A%203D%20Head%20Generation%20from%20Single%20In-the-wild%20Portrait%20Image&body=Title%3A%20Portrait3D%3A%203D%20Head%20Generation%20from%20Single%20In-the-wild%20Portrait%20Image%0AAuthor%3A%20Jinkun%20Hao%20and%20Junshu%20Tang%20and%20Jiangning%20Zhang%20and%20Ran%20Yi%20and%20Yijia%20Hong%20and%20Moran%20Li%20and%20Weijian%20Cao%20and%20Yating%20Wang%20and%20Lizhuang%20Ma%0AAbstract%3A%20%20%20While%20recent%20works%20have%20achieved%20great%20success%20on%20one-shot%203D%20common%20object%0Ageneration%2C%20high%20quality%20and%20fidelity%203D%20head%20generation%20from%20a%20single%20image%0Aremains%20a%20great%20challenge.%20Previous%20text-based%20methods%20for%20generating%203D%20heads%0Awere%20limited%20by%20text%20descriptions%20and%20image-based%20methods%20struggled%20to%20produce%0Ahigh-quality%20head%20geometry.%20To%20handle%20this%20challenging%20problem%2C%20we%20propose%20a%0Anovel%20framework%2C%20Portrait3D%2C%20to%20generate%20high-quality%203D%20heads%20while%20preserving%0Atheir%20identities.%20Our%20work%20incorporates%20the%20identity%20information%20of%20the%0Aportrait%20image%20into%20three%20parts%3A%201%29%20geometry%20initialization%2C%202%29%20geometry%0Asculpting%2C%20and%203%29%20texture%20generation%20stages.%20Given%20a%20reference%20portrait%20image%2C%0Awe%20first%20align%20the%20identity%20features%20with%20text%20features%20to%20realize%20ID-aware%0Aguidance%20enhancement%2C%20which%20contains%20the%20control%20signals%20representing%20the%20face%0Ainformation.%20We%20then%20use%20the%20canny%20map%2C%20ID%20features%20of%20the%20portrait%20image%2C%20and%0Aa%20pre-trained%20text-to-normal/depth%20diffusion%20model%20to%20generate%20ID-aware%0Ageometry%20supervision%2C%20and%203D-GAN%20inversion%20is%20employed%20to%20generate%20ID-aware%0Ageometry%20initialization.%20Furthermore%2C%20with%20the%20ability%20to%20inject%20identity%0Ainformation%20into%203D%20head%20generation%2C%20we%20use%20ID-aware%20guidance%20to%20calculate%0AID-aware%20Score%20Distillation%20%28ISD%29%20for%20geometry%20sculpting.%20For%20texture%0Ageneration%2C%20we%20adopt%20the%20ID%20Consistent%20Texture%20Inpainting%20and%20Refinement%20which%0Aprogressively%20expands%20the%20view%20for%20texture%20inpainting%20to%20obtain%20an%0Ainitialization%20UV%20texture%20map.%20We%20then%20use%20the%20id-aware%20guidance%20to%20provide%0Aimage-level%20supervision%20for%20noisy%20multi-view%20images%20to%20obtain%20a%20refined%20texture%0Amap.%20Extensive%20experiments%20demonstrate%20that%20we%20can%20generate%20high-quality%203D%0Aheads%20with%20accurate%20geometry%20and%20texture%20from%20single%20in-the-wild%20portrait%0Aimages.%20The%20project%20page%20is%20at%20https%3A//jinkun-hao.github.io/Portrait3D/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPortrait3D%253A%25203D%2520Head%2520Generation%2520from%2520Single%2520In-the-wild%2520Portrait%2520Image%26entry.906535625%3DJinkun%2520Hao%2520and%2520Junshu%2520Tang%2520and%2520Jiangning%2520Zhang%2520and%2520Ran%2520Yi%2520and%2520Yijia%2520Hong%2520and%2520Moran%2520Li%2520and%2520Weijian%2520Cao%2520and%2520Yating%2520Wang%2520and%2520Lizhuang%2520Ma%26entry.1292438233%3D%2520%2520While%2520recent%2520works%2520have%2520achieved%2520great%2520success%2520on%2520one-shot%25203D%2520common%2520object%250Ageneration%252C%2520high%2520quality%2520and%2520fidelity%25203D%2520head%2520generation%2520from%2520a%2520single%2520image%250Aremains%2520a%2520great%2520challenge.%2520Previous%2520text-based%2520methods%2520for%2520generating%25203D%2520heads%250Awere%2520limited%2520by%2520text%2520descriptions%2520and%2520image-based%2520methods%2520struggled%2520to%2520produce%250Ahigh-quality%2520head%2520geometry.%2520To%2520handle%2520this%2520challenging%2520problem%252C%2520we%2520propose%2520a%250Anovel%2520framework%252C%2520Portrait3D%252C%2520to%2520generate%2520high-quality%25203D%2520heads%2520while%2520preserving%250Atheir%2520identities.%2520Our%2520work%2520incorporates%2520the%2520identity%2520information%2520of%2520the%250Aportrait%2520image%2520into%2520three%2520parts%253A%25201%2529%2520geometry%2520initialization%252C%25202%2529%2520geometry%250Asculpting%252C%2520and%25203%2529%2520texture%2520generation%2520stages.%2520Given%2520a%2520reference%2520portrait%2520image%252C%250Awe%2520first%2520align%2520the%2520identity%2520features%2520with%2520text%2520features%2520to%2520realize%2520ID-aware%250Aguidance%2520enhancement%252C%2520which%2520contains%2520the%2520control%2520signals%2520representing%2520the%2520face%250Ainformation.%2520We%2520then%2520use%2520the%2520canny%2520map%252C%2520ID%2520features%2520of%2520the%2520portrait%2520image%252C%2520and%250Aa%2520pre-trained%2520text-to-normal/depth%2520diffusion%2520model%2520to%2520generate%2520ID-aware%250Ageometry%2520supervision%252C%2520and%25203D-GAN%2520inversion%2520is%2520employed%2520to%2520generate%2520ID-aware%250Ageometry%2520initialization.%2520Furthermore%252C%2520with%2520the%2520ability%2520to%2520inject%2520identity%250Ainformation%2520into%25203D%2520head%2520generation%252C%2520we%2520use%2520ID-aware%2520guidance%2520to%2520calculate%250AID-aware%2520Score%2520Distillation%2520%2528ISD%2529%2520for%2520geometry%2520sculpting.%2520For%2520texture%250Ageneration%252C%2520we%2520adopt%2520the%2520ID%2520Consistent%2520Texture%2520Inpainting%2520and%2520Refinement%2520which%250Aprogressively%2520expands%2520the%2520view%2520for%2520texture%2520inpainting%2520to%2520obtain%2520an%250Ainitialization%2520UV%2520texture%2520map.%2520We%2520then%2520use%2520the%2520id-aware%2520guidance%2520to%2520provide%250Aimage-level%2520supervision%2520for%2520noisy%2520multi-view%2520images%2520to%2520obtain%2520a%2520refined%2520texture%250Amap.%2520Extensive%2520experiments%2520demonstrate%2520that%2520we%2520can%2520generate%2520high-quality%25203D%250Aheads%2520with%2520accurate%2520geometry%2520and%2520texture%2520from%2520single%2520in-the-wild%2520portrait%250Aimages.%2520The%2520project%2520page%2520is%2520at%2520https%253A//jinkun-hao.github.io/Portrait3D/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Portrait3D%3A%203D%20Head%20Generation%20from%20Single%20In-the-wild%20Portrait%20Image&entry.906535625=Jinkun%20Hao%20and%20Junshu%20Tang%20and%20Jiangning%20Zhang%20and%20Ran%20Yi%20and%20Yijia%20Hong%20and%20Moran%20Li%20and%20Weijian%20Cao%20and%20Yating%20Wang%20and%20Lizhuang%20Ma&entry.1292438233=%20%20While%20recent%20works%20have%20achieved%20great%20success%20on%20one-shot%203D%20common%20object%0Ageneration%2C%20high%20quality%20and%20fidelity%203D%20head%20generation%20from%20a%20single%20image%0Aremains%20a%20great%20challenge.%20Previous%20text-based%20methods%20for%20generating%203D%20heads%0Awere%20limited%20by%20text%20descriptions%20and%20image-based%20methods%20struggled%20to%20produce%0Ahigh-quality%20head%20geometry.%20To%20handle%20this%20challenging%20problem%2C%20we%20propose%20a%0Anovel%20framework%2C%20Portrait3D%2C%20to%20generate%20high-quality%203D%20heads%20while%20preserving%0Atheir%20identities.%20Our%20work%20incorporates%20the%20identity%20information%20of%20the%0Aportrait%20image%20into%20three%20parts%3A%201%29%20geometry%20initialization%2C%202%29%20geometry%0Asculpting%2C%20and%203%29%20texture%20generation%20stages.%20Given%20a%20reference%20portrait%20image%2C%0Awe%20first%20align%20the%20identity%20features%20with%20text%20features%20to%20realize%20ID-aware%0Aguidance%20enhancement%2C%20which%20contains%20the%20control%20signals%20representing%20the%20face%0Ainformation.%20We%20then%20use%20the%20canny%20map%2C%20ID%20features%20of%20the%20portrait%20image%2C%20and%0Aa%20pre-trained%20text-to-normal/depth%20diffusion%20model%20to%20generate%20ID-aware%0Ageometry%20supervision%2C%20and%203D-GAN%20inversion%20is%20employed%20to%20generate%20ID-aware%0Ageometry%20initialization.%20Furthermore%2C%20with%20the%20ability%20to%20inject%20identity%0Ainformation%20into%203D%20head%20generation%2C%20we%20use%20ID-aware%20guidance%20to%20calculate%0AID-aware%20Score%20Distillation%20%28ISD%29%20for%20geometry%20sculpting.%20For%20texture%0Ageneration%2C%20we%20adopt%20the%20ID%20Consistent%20Texture%20Inpainting%20and%20Refinement%20which%0Aprogressively%20expands%20the%20view%20for%20texture%20inpainting%20to%20obtain%20an%0Ainitialization%20UV%20texture%20map.%20We%20then%20use%20the%20id-aware%20guidance%20to%20provide%0Aimage-level%20supervision%20for%20noisy%20multi-view%20images%20to%20obtain%20a%20refined%20texture%0Amap.%20Extensive%20experiments%20demonstrate%20that%20we%20can%20generate%20high-quality%203D%0Aheads%20with%20accurate%20geometry%20and%20texture%20from%20single%20in-the-wild%20portrait%0Aimages.%20The%20project%20page%20is%20at%20https%3A//jinkun-hao.github.io/Portrait3D/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16710v1&entry.124074799=Read"},
{"title": "OpenDlign: Enhancing Open-World 3D Learning with Depth-Aligned Images", "author": "Ye Mao and Junpeng Jing and Krystian Mikolajczyk", "abstract": "  Recent open-world 3D representation learning methods using Vision-Language\nModels (VLMs) to align 3D data with image-text information have shown superior\n3D zero-shot performance. However, CAD-rendered images for this alignment often\nlack realism and texture variation, compromising alignment robustness.\nMoreover, the volume discrepancy between 3D and 2D pretraining datasets\nhighlights the need for effective strategies to transfer the representational\nabilities of VLMs to 3D learning. In this paper, we present OpenDlign, a novel\nopen-world 3D model using depth-aligned images generated from a diffusion model\nfor robust multimodal alignment. These images exhibit greater texture diversity\nthan CAD renderings due to the stochastic nature of the diffusion model. By\nrefining the depth map projection pipeline and designing depth-specific\nprompts, OpenDlign leverages rich knowledge in pre-trained VLM for 3D\nrepresentation learning with streamlined fine-tuning. Our experiments show that\nOpenDlign achieves high zero-shot and few-shot performance on diverse 3D tasks,\ndespite only fine-tuning 6 million parameters on a limited ShapeNet dataset. In\nzero-shot classification, OpenDlign surpasses previous models by 8.0% on\nModelNet40 and 16.4% on OmniObject3D. Additionally, using depth-aligned images\nfor multimodal alignment consistently enhances the performance of other\nstate-of-the-art models.\n", "link": "http://arxiv.org/abs/2404.16538v2", "date": "2024-06-24", "relevancy": 2.5123, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6628}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6077}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenDlign%3A%20Enhancing%20Open-World%203D%20Learning%20with%20Depth-Aligned%20Images&body=Title%3A%20OpenDlign%3A%20Enhancing%20Open-World%203D%20Learning%20with%20Depth-Aligned%20Images%0AAuthor%3A%20Ye%20Mao%20and%20Junpeng%20Jing%20and%20Krystian%20Mikolajczyk%0AAbstract%3A%20%20%20Recent%20open-world%203D%20representation%20learning%20methods%20using%20Vision-Language%0AModels%20%28VLMs%29%20to%20align%203D%20data%20with%20image-text%20information%20have%20shown%20superior%0A3D%20zero-shot%20performance.%20However%2C%20CAD-rendered%20images%20for%20this%20alignment%20often%0Alack%20realism%20and%20texture%20variation%2C%20compromising%20alignment%20robustness.%0AMoreover%2C%20the%20volume%20discrepancy%20between%203D%20and%202D%20pretraining%20datasets%0Ahighlights%20the%20need%20for%20effective%20strategies%20to%20transfer%20the%20representational%0Aabilities%20of%20VLMs%20to%203D%20learning.%20In%20this%20paper%2C%20we%20present%20OpenDlign%2C%20a%20novel%0Aopen-world%203D%20model%20using%20depth-aligned%20images%20generated%20from%20a%20diffusion%20model%0Afor%20robust%20multimodal%20alignment.%20These%20images%20exhibit%20greater%20texture%20diversity%0Athan%20CAD%20renderings%20due%20to%20the%20stochastic%20nature%20of%20the%20diffusion%20model.%20By%0Arefining%20the%20depth%20map%20projection%20pipeline%20and%20designing%20depth-specific%0Aprompts%2C%20OpenDlign%20leverages%20rich%20knowledge%20in%20pre-trained%20VLM%20for%203D%0Arepresentation%20learning%20with%20streamlined%20fine-tuning.%20Our%20experiments%20show%20that%0AOpenDlign%20achieves%20high%20zero-shot%20and%20few-shot%20performance%20on%20diverse%203D%20tasks%2C%0Adespite%20only%20fine-tuning%206%20million%20parameters%20on%20a%20limited%20ShapeNet%20dataset.%20In%0Azero-shot%20classification%2C%20OpenDlign%20surpasses%20previous%20models%20by%208.0%25%20on%0AModelNet40%20and%2016.4%25%20on%20OmniObject3D.%20Additionally%2C%20using%20depth-aligned%20images%0Afor%20multimodal%20alignment%20consistently%20enhances%20the%20performance%20of%20other%0Astate-of-the-art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16538v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenDlign%253A%2520Enhancing%2520Open-World%25203D%2520Learning%2520with%2520Depth-Aligned%2520Images%26entry.906535625%3DYe%2520Mao%2520and%2520Junpeng%2520Jing%2520and%2520Krystian%2520Mikolajczyk%26entry.1292438233%3D%2520%2520Recent%2520open-world%25203D%2520representation%2520learning%2520methods%2520using%2520Vision-Language%250AModels%2520%2528VLMs%2529%2520to%2520align%25203D%2520data%2520with%2520image-text%2520information%2520have%2520shown%2520superior%250A3D%2520zero-shot%2520performance.%2520However%252C%2520CAD-rendered%2520images%2520for%2520this%2520alignment%2520often%250Alack%2520realism%2520and%2520texture%2520variation%252C%2520compromising%2520alignment%2520robustness.%250AMoreover%252C%2520the%2520volume%2520discrepancy%2520between%25203D%2520and%25202D%2520pretraining%2520datasets%250Ahighlights%2520the%2520need%2520for%2520effective%2520strategies%2520to%2520transfer%2520the%2520representational%250Aabilities%2520of%2520VLMs%2520to%25203D%2520learning.%2520In%2520this%2520paper%252C%2520we%2520present%2520OpenDlign%252C%2520a%2520novel%250Aopen-world%25203D%2520model%2520using%2520depth-aligned%2520images%2520generated%2520from%2520a%2520diffusion%2520model%250Afor%2520robust%2520multimodal%2520alignment.%2520These%2520images%2520exhibit%2520greater%2520texture%2520diversity%250Athan%2520CAD%2520renderings%2520due%2520to%2520the%2520stochastic%2520nature%2520of%2520the%2520diffusion%2520model.%2520By%250Arefining%2520the%2520depth%2520map%2520projection%2520pipeline%2520and%2520designing%2520depth-specific%250Aprompts%252C%2520OpenDlign%2520leverages%2520rich%2520knowledge%2520in%2520pre-trained%2520VLM%2520for%25203D%250Arepresentation%2520learning%2520with%2520streamlined%2520fine-tuning.%2520Our%2520experiments%2520show%2520that%250AOpenDlign%2520achieves%2520high%2520zero-shot%2520and%2520few-shot%2520performance%2520on%2520diverse%25203D%2520tasks%252C%250Adespite%2520only%2520fine-tuning%25206%2520million%2520parameters%2520on%2520a%2520limited%2520ShapeNet%2520dataset.%2520In%250Azero-shot%2520classification%252C%2520OpenDlign%2520surpasses%2520previous%2520models%2520by%25208.0%2525%2520on%250AModelNet40%2520and%252016.4%2525%2520on%2520OmniObject3D.%2520Additionally%252C%2520using%2520depth-aligned%2520images%250Afor%2520multimodal%2520alignment%2520consistently%2520enhances%2520the%2520performance%2520of%2520other%250Astate-of-the-art%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16538v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenDlign%3A%20Enhancing%20Open-World%203D%20Learning%20with%20Depth-Aligned%20Images&entry.906535625=Ye%20Mao%20and%20Junpeng%20Jing%20and%20Krystian%20Mikolajczyk&entry.1292438233=%20%20Recent%20open-world%203D%20representation%20learning%20methods%20using%20Vision-Language%0AModels%20%28VLMs%29%20to%20align%203D%20data%20with%20image-text%20information%20have%20shown%20superior%0A3D%20zero-shot%20performance.%20However%2C%20CAD-rendered%20images%20for%20this%20alignment%20often%0Alack%20realism%20and%20texture%20variation%2C%20compromising%20alignment%20robustness.%0AMoreover%2C%20the%20volume%20discrepancy%20between%203D%20and%202D%20pretraining%20datasets%0Ahighlights%20the%20need%20for%20effective%20strategies%20to%20transfer%20the%20representational%0Aabilities%20of%20VLMs%20to%203D%20learning.%20In%20this%20paper%2C%20we%20present%20OpenDlign%2C%20a%20novel%0Aopen-world%203D%20model%20using%20depth-aligned%20images%20generated%20from%20a%20diffusion%20model%0Afor%20robust%20multimodal%20alignment.%20These%20images%20exhibit%20greater%20texture%20diversity%0Athan%20CAD%20renderings%20due%20to%20the%20stochastic%20nature%20of%20the%20diffusion%20model.%20By%0Arefining%20the%20depth%20map%20projection%20pipeline%20and%20designing%20depth-specific%0Aprompts%2C%20OpenDlign%20leverages%20rich%20knowledge%20in%20pre-trained%20VLM%20for%203D%0Arepresentation%20learning%20with%20streamlined%20fine-tuning.%20Our%20experiments%20show%20that%0AOpenDlign%20achieves%20high%20zero-shot%20and%20few-shot%20performance%20on%20diverse%203D%20tasks%2C%0Adespite%20only%20fine-tuning%206%20million%20parameters%20on%20a%20limited%20ShapeNet%20dataset.%20In%0Azero-shot%20classification%2C%20OpenDlign%20surpasses%20previous%20models%20by%208.0%25%20on%0AModelNet40%20and%2016.4%25%20on%20OmniObject3D.%20Additionally%2C%20using%20depth-aligned%20images%0Afor%20multimodal%20alignment%20consistently%20enhances%20the%20performance%20of%20other%0Astate-of-the-art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16538v2&entry.124074799=Read"},
{"title": "Inference of Sequential Patterns for Neural Message Passing in Temporal\n  Graphs", "author": "Jan von Pichowski and Vincenzo Perri and Lisi Qarkaxhija and Ingo Scholtes", "abstract": "  The modelling of temporal patterns in dynamic graphs is an important current\nresearch issue in the development of time-aware GNNs. Whether or not a specific\nsequence of events in a temporal graph constitutes a temporal pattern not only\ndepends on the frequency of its occurrence. We consider whether it deviates\nfrom what is expected in a temporal graph where timestamps are randomly\nshuffled. While accounting for such a random baseline is important to model\ntemporal patterns, it has mostly been ignored by current temporal graph neural\nnetworks. To address this issue we propose HYPA-DBGNN, a novel two-step\napproach that combines (i) the inference of anomalous sequential patterns in\ntime series data on graphs based on a statistically principled null model, with\n(ii) a neural message passing approach that utilizes a higher-order De Bruijn\ngraph whose edges capture overrepresented sequential patterns. Our method\nleverages hypergeometric graph ensembles to identify anomalous edges within\nboth first- and higher-order De Bruijn graphs, which encode the temporal\nordering of events. The model introduces an inductive bias that enhances model\ninterpretability. We evaluate our approach for static node classification using\nbenchmark datasets and a synthetic dataset that showcases its ability to\nincorporate the observed inductive bias regarding over- and under-represented\ntemporal edges. We demonstrate the framework's effectiveness in detecting\nsimilar patterns within empirical datasets, resulting in superior performance\ncompared to baseline methods in node classification tasks. To the best of our\nknowledge, our work is the first to introduce statistically informed GNNs that\nleverage temporal and causal sequence anomalies. HYPA-DBGNN represents a path\nfor bridging the gap between statistical graph inference and neural graph\nrepresentation learning, with potential applications to static GNNs.\n", "link": "http://arxiv.org/abs/2406.16552v1", "date": "2024-06-24", "relevancy": 2.508, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5341}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4977}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inference%20of%20Sequential%20Patterns%20for%20Neural%20Message%20Passing%20in%20Temporal%0A%20%20Graphs&body=Title%3A%20Inference%20of%20Sequential%20Patterns%20for%20Neural%20Message%20Passing%20in%20Temporal%0A%20%20Graphs%0AAuthor%3A%20Jan%20von%20Pichowski%20and%20Vincenzo%20Perri%20and%20Lisi%20Qarkaxhija%20and%20Ingo%20Scholtes%0AAbstract%3A%20%20%20The%20modelling%20of%20temporal%20patterns%20in%20dynamic%20graphs%20is%20an%20important%20current%0Aresearch%20issue%20in%20the%20development%20of%20time-aware%20GNNs.%20Whether%20or%20not%20a%20specific%0Asequence%20of%20events%20in%20a%20temporal%20graph%20constitutes%20a%20temporal%20pattern%20not%20only%0Adepends%20on%20the%20frequency%20of%20its%20occurrence.%20We%20consider%20whether%20it%20deviates%0Afrom%20what%20is%20expected%20in%20a%20temporal%20graph%20where%20timestamps%20are%20randomly%0Ashuffled.%20While%20accounting%20for%20such%20a%20random%20baseline%20is%20important%20to%20model%0Atemporal%20patterns%2C%20it%20has%20mostly%20been%20ignored%20by%20current%20temporal%20graph%20neural%0Anetworks.%20To%20address%20this%20issue%20we%20propose%20HYPA-DBGNN%2C%20a%20novel%20two-step%0Aapproach%20that%20combines%20%28i%29%20the%20inference%20of%20anomalous%20sequential%20patterns%20in%0Atime%20series%20data%20on%20graphs%20based%20on%20a%20statistically%20principled%20null%20model%2C%20with%0A%28ii%29%20a%20neural%20message%20passing%20approach%20that%20utilizes%20a%20higher-order%20De%20Bruijn%0Agraph%20whose%20edges%20capture%20overrepresented%20sequential%20patterns.%20Our%20method%0Aleverages%20hypergeometric%20graph%20ensembles%20to%20identify%20anomalous%20edges%20within%0Aboth%20first-%20and%20higher-order%20De%20Bruijn%20graphs%2C%20which%20encode%20the%20temporal%0Aordering%20of%20events.%20The%20model%20introduces%20an%20inductive%20bias%20that%20enhances%20model%0Ainterpretability.%20We%20evaluate%20our%20approach%20for%20static%20node%20classification%20using%0Abenchmark%20datasets%20and%20a%20synthetic%20dataset%20that%20showcases%20its%20ability%20to%0Aincorporate%20the%20observed%20inductive%20bias%20regarding%20over-%20and%20under-represented%0Atemporal%20edges.%20We%20demonstrate%20the%20framework%27s%20effectiveness%20in%20detecting%0Asimilar%20patterns%20within%20empirical%20datasets%2C%20resulting%20in%20superior%20performance%0Acompared%20to%20baseline%20methods%20in%20node%20classification%20tasks.%20To%20the%20best%20of%20our%0Aknowledge%2C%20our%20work%20is%20the%20first%20to%20introduce%20statistically%20informed%20GNNs%20that%0Aleverage%20temporal%20and%20causal%20sequence%20anomalies.%20HYPA-DBGNN%20represents%20a%20path%0Afor%20bridging%20the%20gap%20between%20statistical%20graph%20inference%20and%20neural%20graph%0Arepresentation%20learning%2C%20with%20potential%20applications%20to%20static%20GNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInference%2520of%2520Sequential%2520Patterns%2520for%2520Neural%2520Message%2520Passing%2520in%2520Temporal%250A%2520%2520Graphs%26entry.906535625%3DJan%2520von%2520Pichowski%2520and%2520Vincenzo%2520Perri%2520and%2520Lisi%2520Qarkaxhija%2520and%2520Ingo%2520Scholtes%26entry.1292438233%3D%2520%2520The%2520modelling%2520of%2520temporal%2520patterns%2520in%2520dynamic%2520graphs%2520is%2520an%2520important%2520current%250Aresearch%2520issue%2520in%2520the%2520development%2520of%2520time-aware%2520GNNs.%2520Whether%2520or%2520not%2520a%2520specific%250Asequence%2520of%2520events%2520in%2520a%2520temporal%2520graph%2520constitutes%2520a%2520temporal%2520pattern%2520not%2520only%250Adepends%2520on%2520the%2520frequency%2520of%2520its%2520occurrence.%2520We%2520consider%2520whether%2520it%2520deviates%250Afrom%2520what%2520is%2520expected%2520in%2520a%2520temporal%2520graph%2520where%2520timestamps%2520are%2520randomly%250Ashuffled.%2520While%2520accounting%2520for%2520such%2520a%2520random%2520baseline%2520is%2520important%2520to%2520model%250Atemporal%2520patterns%252C%2520it%2520has%2520mostly%2520been%2520ignored%2520by%2520current%2520temporal%2520graph%2520neural%250Anetworks.%2520To%2520address%2520this%2520issue%2520we%2520propose%2520HYPA-DBGNN%252C%2520a%2520novel%2520two-step%250Aapproach%2520that%2520combines%2520%2528i%2529%2520the%2520inference%2520of%2520anomalous%2520sequential%2520patterns%2520in%250Atime%2520series%2520data%2520on%2520graphs%2520based%2520on%2520a%2520statistically%2520principled%2520null%2520model%252C%2520with%250A%2528ii%2529%2520a%2520neural%2520message%2520passing%2520approach%2520that%2520utilizes%2520a%2520higher-order%2520De%2520Bruijn%250Agraph%2520whose%2520edges%2520capture%2520overrepresented%2520sequential%2520patterns.%2520Our%2520method%250Aleverages%2520hypergeometric%2520graph%2520ensembles%2520to%2520identify%2520anomalous%2520edges%2520within%250Aboth%2520first-%2520and%2520higher-order%2520De%2520Bruijn%2520graphs%252C%2520which%2520encode%2520the%2520temporal%250Aordering%2520of%2520events.%2520The%2520model%2520introduces%2520an%2520inductive%2520bias%2520that%2520enhances%2520model%250Ainterpretability.%2520We%2520evaluate%2520our%2520approach%2520for%2520static%2520node%2520classification%2520using%250Abenchmark%2520datasets%2520and%2520a%2520synthetic%2520dataset%2520that%2520showcases%2520its%2520ability%2520to%250Aincorporate%2520the%2520observed%2520inductive%2520bias%2520regarding%2520over-%2520and%2520under-represented%250Atemporal%2520edges.%2520We%2520demonstrate%2520the%2520framework%2527s%2520effectiveness%2520in%2520detecting%250Asimilar%2520patterns%2520within%2520empirical%2520datasets%252C%2520resulting%2520in%2520superior%2520performance%250Acompared%2520to%2520baseline%2520methods%2520in%2520node%2520classification%2520tasks.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520our%2520work%2520is%2520the%2520first%2520to%2520introduce%2520statistically%2520informed%2520GNNs%2520that%250Aleverage%2520temporal%2520and%2520causal%2520sequence%2520anomalies.%2520HYPA-DBGNN%2520represents%2520a%2520path%250Afor%2520bridging%2520the%2520gap%2520between%2520statistical%2520graph%2520inference%2520and%2520neural%2520graph%250Arepresentation%2520learning%252C%2520with%2520potential%2520applications%2520to%2520static%2520GNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inference%20of%20Sequential%20Patterns%20for%20Neural%20Message%20Passing%20in%20Temporal%0A%20%20Graphs&entry.906535625=Jan%20von%20Pichowski%20and%20Vincenzo%20Perri%20and%20Lisi%20Qarkaxhija%20and%20Ingo%20Scholtes&entry.1292438233=%20%20The%20modelling%20of%20temporal%20patterns%20in%20dynamic%20graphs%20is%20an%20important%20current%0Aresearch%20issue%20in%20the%20development%20of%20time-aware%20GNNs.%20Whether%20or%20not%20a%20specific%0Asequence%20of%20events%20in%20a%20temporal%20graph%20constitutes%20a%20temporal%20pattern%20not%20only%0Adepends%20on%20the%20frequency%20of%20its%20occurrence.%20We%20consider%20whether%20it%20deviates%0Afrom%20what%20is%20expected%20in%20a%20temporal%20graph%20where%20timestamps%20are%20randomly%0Ashuffled.%20While%20accounting%20for%20such%20a%20random%20baseline%20is%20important%20to%20model%0Atemporal%20patterns%2C%20it%20has%20mostly%20been%20ignored%20by%20current%20temporal%20graph%20neural%0Anetworks.%20To%20address%20this%20issue%20we%20propose%20HYPA-DBGNN%2C%20a%20novel%20two-step%0Aapproach%20that%20combines%20%28i%29%20the%20inference%20of%20anomalous%20sequential%20patterns%20in%0Atime%20series%20data%20on%20graphs%20based%20on%20a%20statistically%20principled%20null%20model%2C%20with%0A%28ii%29%20a%20neural%20message%20passing%20approach%20that%20utilizes%20a%20higher-order%20De%20Bruijn%0Agraph%20whose%20edges%20capture%20overrepresented%20sequential%20patterns.%20Our%20method%0Aleverages%20hypergeometric%20graph%20ensembles%20to%20identify%20anomalous%20edges%20within%0Aboth%20first-%20and%20higher-order%20De%20Bruijn%20graphs%2C%20which%20encode%20the%20temporal%0Aordering%20of%20events.%20The%20model%20introduces%20an%20inductive%20bias%20that%20enhances%20model%0Ainterpretability.%20We%20evaluate%20our%20approach%20for%20static%20node%20classification%20using%0Abenchmark%20datasets%20and%20a%20synthetic%20dataset%20that%20showcases%20its%20ability%20to%0Aincorporate%20the%20observed%20inductive%20bias%20regarding%20over-%20and%20under-represented%0Atemporal%20edges.%20We%20demonstrate%20the%20framework%27s%20effectiveness%20in%20detecting%0Asimilar%20patterns%20within%20empirical%20datasets%2C%20resulting%20in%20superior%20performance%0Acompared%20to%20baseline%20methods%20in%20node%20classification%20tasks.%20To%20the%20best%20of%20our%0Aknowledge%2C%20our%20work%20is%20the%20first%20to%20introduce%20statistically%20informed%20GNNs%20that%0Aleverage%20temporal%20and%20causal%20sequence%20anomalies.%20HYPA-DBGNN%20represents%20a%20path%0Afor%20bridging%20the%20gap%20between%20statistical%20graph%20inference%20and%20neural%20graph%0Arepresentation%20learning%2C%20with%20potential%20applications%20to%20static%20GNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16552v1&entry.124074799=Read"},
{"title": "Feature Fusion for Human Activity Recognition using Parameter-Optimized\n  Multi-Stage Graph Convolutional Network and Transformer Models", "author": "Mohammad Belal and Taimur Hassan and Abdelfatah Ahmed and Ahmad Aljarah and Nael Alsheikh and Irfan Hussain", "abstract": "  Human activity recognition (HAR) is a crucial area of research that involves\nunderstanding human movements using computer and machine vision technology.\nDeep learning has emerged as a powerful tool for this task, with models such as\nConvolutional Neural Networks (CNNs) and Transformers being employed to capture\nvarious aspects of human motion. One of the key contributions of this work is\nthe demonstration of the effectiveness of feature fusion in improving HAR\naccuracy by capturing spatial and temporal features, which has important\nimplications for the development of more accurate and robust activity\nrecognition systems. The study uses sensory data from HuGaDB, PKU-MMD, LARa,\nand TUG datasets. Two model, the PO-MS-GCN and a Transformer were trained and\nevaluated, with PO-MS-GCN outperforming state-of-the-art models. HuGaDB and TUG\nachieved high accuracies and f1-scores, while LARa and PKU-MMD had lower\nscores. Feature fusion improved results across datasets.\n", "link": "http://arxiv.org/abs/2406.16638v1", "date": "2024-06-24", "relevancy": 2.4789, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5031}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.497}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Fusion%20for%20Human%20Activity%20Recognition%20using%20Parameter-Optimized%0A%20%20Multi-Stage%20Graph%20Convolutional%20Network%20and%20Transformer%20Models&body=Title%3A%20Feature%20Fusion%20for%20Human%20Activity%20Recognition%20using%20Parameter-Optimized%0A%20%20Multi-Stage%20Graph%20Convolutional%20Network%20and%20Transformer%20Models%0AAuthor%3A%20Mohammad%20Belal%20and%20Taimur%20Hassan%20and%20Abdelfatah%20Ahmed%20and%20Ahmad%20Aljarah%20and%20Nael%20Alsheikh%20and%20Irfan%20Hussain%0AAbstract%3A%20%20%20Human%20activity%20recognition%20%28HAR%29%20is%20a%20crucial%20area%20of%20research%20that%20involves%0Aunderstanding%20human%20movements%20using%20computer%20and%20machine%20vision%20technology.%0ADeep%20learning%20has%20emerged%20as%20a%20powerful%20tool%20for%20this%20task%2C%20with%20models%20such%20as%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20and%20Transformers%20being%20employed%20to%20capture%0Avarious%20aspects%20of%20human%20motion.%20One%20of%20the%20key%20contributions%20of%20this%20work%20is%0Athe%20demonstration%20of%20the%20effectiveness%20of%20feature%20fusion%20in%20improving%20HAR%0Aaccuracy%20by%20capturing%20spatial%20and%20temporal%20features%2C%20which%20has%20important%0Aimplications%20for%20the%20development%20of%20more%20accurate%20and%20robust%20activity%0Arecognition%20systems.%20The%20study%20uses%20sensory%20data%20from%20HuGaDB%2C%20PKU-MMD%2C%20LARa%2C%0Aand%20TUG%20datasets.%20Two%20model%2C%20the%20PO-MS-GCN%20and%20a%20Transformer%20were%20trained%20and%0Aevaluated%2C%20with%20PO-MS-GCN%20outperforming%20state-of-the-art%20models.%20HuGaDB%20and%20TUG%0Aachieved%20high%20accuracies%20and%20f1-scores%2C%20while%20LARa%20and%20PKU-MMD%20had%20lower%0Ascores.%20Feature%20fusion%20improved%20results%20across%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Fusion%2520for%2520Human%2520Activity%2520Recognition%2520using%2520Parameter-Optimized%250A%2520%2520Multi-Stage%2520Graph%2520Convolutional%2520Network%2520and%2520Transformer%2520Models%26entry.906535625%3DMohammad%2520Belal%2520and%2520Taimur%2520Hassan%2520and%2520Abdelfatah%2520Ahmed%2520and%2520Ahmad%2520Aljarah%2520and%2520Nael%2520Alsheikh%2520and%2520Irfan%2520Hussain%26entry.1292438233%3D%2520%2520Human%2520activity%2520recognition%2520%2528HAR%2529%2520is%2520a%2520crucial%2520area%2520of%2520research%2520that%2520involves%250Aunderstanding%2520human%2520movements%2520using%2520computer%2520and%2520machine%2520vision%2520technology.%250ADeep%2520learning%2520has%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520this%2520task%252C%2520with%2520models%2520such%2520as%250AConvolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%2520Transformers%2520being%2520employed%2520to%2520capture%250Avarious%2520aspects%2520of%2520human%2520motion.%2520One%2520of%2520the%2520key%2520contributions%2520of%2520this%2520work%2520is%250Athe%2520demonstration%2520of%2520the%2520effectiveness%2520of%2520feature%2520fusion%2520in%2520improving%2520HAR%250Aaccuracy%2520by%2520capturing%2520spatial%2520and%2520temporal%2520features%252C%2520which%2520has%2520important%250Aimplications%2520for%2520the%2520development%2520of%2520more%2520accurate%2520and%2520robust%2520activity%250Arecognition%2520systems.%2520The%2520study%2520uses%2520sensory%2520data%2520from%2520HuGaDB%252C%2520PKU-MMD%252C%2520LARa%252C%250Aand%2520TUG%2520datasets.%2520Two%2520model%252C%2520the%2520PO-MS-GCN%2520and%2520a%2520Transformer%2520were%2520trained%2520and%250Aevaluated%252C%2520with%2520PO-MS-GCN%2520outperforming%2520state-of-the-art%2520models.%2520HuGaDB%2520and%2520TUG%250Aachieved%2520high%2520accuracies%2520and%2520f1-scores%252C%2520while%2520LARa%2520and%2520PKU-MMD%2520had%2520lower%250Ascores.%2520Feature%2520fusion%2520improved%2520results%2520across%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Fusion%20for%20Human%20Activity%20Recognition%20using%20Parameter-Optimized%0A%20%20Multi-Stage%20Graph%20Convolutional%20Network%20and%20Transformer%20Models&entry.906535625=Mohammad%20Belal%20and%20Taimur%20Hassan%20and%20Abdelfatah%20Ahmed%20and%20Ahmad%20Aljarah%20and%20Nael%20Alsheikh%20and%20Irfan%20Hussain&entry.1292438233=%20%20Human%20activity%20recognition%20%28HAR%29%20is%20a%20crucial%20area%20of%20research%20that%20involves%0Aunderstanding%20human%20movements%20using%20computer%20and%20machine%20vision%20technology.%0ADeep%20learning%20has%20emerged%20as%20a%20powerful%20tool%20for%20this%20task%2C%20with%20models%20such%20as%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20and%20Transformers%20being%20employed%20to%20capture%0Avarious%20aspects%20of%20human%20motion.%20One%20of%20the%20key%20contributions%20of%20this%20work%20is%0Athe%20demonstration%20of%20the%20effectiveness%20of%20feature%20fusion%20in%20improving%20HAR%0Aaccuracy%20by%20capturing%20spatial%20and%20temporal%20features%2C%20which%20has%20important%0Aimplications%20for%20the%20development%20of%20more%20accurate%20and%20robust%20activity%0Arecognition%20systems.%20The%20study%20uses%20sensory%20data%20from%20HuGaDB%2C%20PKU-MMD%2C%20LARa%2C%0Aand%20TUG%20datasets.%20Two%20model%2C%20the%20PO-MS-GCN%20and%20a%20Transformer%20were%20trained%20and%0Aevaluated%2C%20with%20PO-MS-GCN%20outperforming%20state-of-the-art%20models.%20HuGaDB%20and%20TUG%0Aachieved%20high%20accuracies%20and%20f1-scores%2C%20while%20LARa%20and%20PKU-MMD%20had%20lower%0Ascores.%20Feature%20fusion%20improved%20results%20across%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16638v1&entry.124074799=Read"},
{"title": "FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models", "author": "Haonan Qiu and Zhaoxi Chen and Zhouxia Wang and Yingqing He and Menghan Xia and Ziwei Liu", "abstract": "  Diffusion model has demonstrated remarkable capability in video generation,\nwhich further sparks interest in introducing trajectory control into the\ngeneration process. While existing works mainly focus on training-based methods\n(e.g., conditional adapter), we argue that diffusion model itself allows decent\ncontrol over the generated content without requiring any training. In this\nstudy, we introduce a tuning-free framework to achieve trajectory-controllable\nvideo generation, by imposing guidance on both noise construction and attention\ncomputation. Specifically, 1) we first show several instructive phenomenons and\nanalyze how initial noises influence the motion trajectory of generated\ncontent. 2) Subsequently, we propose FreeTraj, a tuning-free approach that\nenables trajectory control by modifying noise sampling and attention\nmechanisms. 3) Furthermore, we extend FreeTraj to facilitate longer and larger\nvideo generation with controllable trajectories. Equipped with these designs,\nusers have the flexibility to provide trajectories manually or opt for\ntrajectories automatically generated by the LLM trajectory planner. Extensive\nexperiments validate the efficacy of our approach in enhancing the trajectory\ncontrollability of video diffusion models.\n", "link": "http://arxiv.org/abs/2406.16863v1", "date": "2024-06-24", "relevancy": 2.4782, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6552}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6347}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeTraj%3A%20Tuning-Free%20Trajectory%20Control%20in%20Video%20Diffusion%20Models&body=Title%3A%20FreeTraj%3A%20Tuning-Free%20Trajectory%20Control%20in%20Video%20Diffusion%20Models%0AAuthor%3A%20Haonan%20Qiu%20and%20Zhaoxi%20Chen%20and%20Zhouxia%20Wang%20and%20Yingqing%20He%20and%20Menghan%20Xia%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Diffusion%20model%20has%20demonstrated%20remarkable%20capability%20in%20video%20generation%2C%0Awhich%20further%20sparks%20interest%20in%20introducing%20trajectory%20control%20into%20the%0Ageneration%20process.%20While%20existing%20works%20mainly%20focus%20on%20training-based%20methods%0A%28e.g.%2C%20conditional%20adapter%29%2C%20we%20argue%20that%20diffusion%20model%20itself%20allows%20decent%0Acontrol%20over%20the%20generated%20content%20without%20requiring%20any%20training.%20In%20this%0Astudy%2C%20we%20introduce%20a%20tuning-free%20framework%20to%20achieve%20trajectory-controllable%0Avideo%20generation%2C%20by%20imposing%20guidance%20on%20both%20noise%20construction%20and%20attention%0Acomputation.%20Specifically%2C%201%29%20we%20first%20show%20several%20instructive%20phenomenons%20and%0Aanalyze%20how%20initial%20noises%20influence%20the%20motion%20trajectory%20of%20generated%0Acontent.%202%29%20Subsequently%2C%20we%20propose%20FreeTraj%2C%20a%20tuning-free%20approach%20that%0Aenables%20trajectory%20control%20by%20modifying%20noise%20sampling%20and%20attention%0Amechanisms.%203%29%20Furthermore%2C%20we%20extend%20FreeTraj%20to%20facilitate%20longer%20and%20larger%0Avideo%20generation%20with%20controllable%20trajectories.%20Equipped%20with%20these%20designs%2C%0Ausers%20have%20the%20flexibility%20to%20provide%20trajectories%20manually%20or%20opt%20for%0Atrajectories%20automatically%20generated%20by%20the%20LLM%20trajectory%20planner.%20Extensive%0Aexperiments%20validate%20the%20efficacy%20of%20our%20approach%20in%20enhancing%20the%20trajectory%0Acontrollability%20of%20video%20diffusion%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeTraj%253A%2520Tuning-Free%2520Trajectory%2520Control%2520in%2520Video%2520Diffusion%2520Models%26entry.906535625%3DHaonan%2520Qiu%2520and%2520Zhaoxi%2520Chen%2520and%2520Zhouxia%2520Wang%2520and%2520Yingqing%2520He%2520and%2520Menghan%2520Xia%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Diffusion%2520model%2520has%2520demonstrated%2520remarkable%2520capability%2520in%2520video%2520generation%252C%250Awhich%2520further%2520sparks%2520interest%2520in%2520introducing%2520trajectory%2520control%2520into%2520the%250Ageneration%2520process.%2520While%2520existing%2520works%2520mainly%2520focus%2520on%2520training-based%2520methods%250A%2528e.g.%252C%2520conditional%2520adapter%2529%252C%2520we%2520argue%2520that%2520diffusion%2520model%2520itself%2520allows%2520decent%250Acontrol%2520over%2520the%2520generated%2520content%2520without%2520requiring%2520any%2520training.%2520In%2520this%250Astudy%252C%2520we%2520introduce%2520a%2520tuning-free%2520framework%2520to%2520achieve%2520trajectory-controllable%250Avideo%2520generation%252C%2520by%2520imposing%2520guidance%2520on%2520both%2520noise%2520construction%2520and%2520attention%250Acomputation.%2520Specifically%252C%25201%2529%2520we%2520first%2520show%2520several%2520instructive%2520phenomenons%2520and%250Aanalyze%2520how%2520initial%2520noises%2520influence%2520the%2520motion%2520trajectory%2520of%2520generated%250Acontent.%25202%2529%2520Subsequently%252C%2520we%2520propose%2520FreeTraj%252C%2520a%2520tuning-free%2520approach%2520that%250Aenables%2520trajectory%2520control%2520by%2520modifying%2520noise%2520sampling%2520and%2520attention%250Amechanisms.%25203%2529%2520Furthermore%252C%2520we%2520extend%2520FreeTraj%2520to%2520facilitate%2520longer%2520and%2520larger%250Avideo%2520generation%2520with%2520controllable%2520trajectories.%2520Equipped%2520with%2520these%2520designs%252C%250Ausers%2520have%2520the%2520flexibility%2520to%2520provide%2520trajectories%2520manually%2520or%2520opt%2520for%250Atrajectories%2520automatically%2520generated%2520by%2520the%2520LLM%2520trajectory%2520planner.%2520Extensive%250Aexperiments%2520validate%2520the%2520efficacy%2520of%2520our%2520approach%2520in%2520enhancing%2520the%2520trajectory%250Acontrollability%2520of%2520video%2520diffusion%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeTraj%3A%20Tuning-Free%20Trajectory%20Control%20in%20Video%20Diffusion%20Models&entry.906535625=Haonan%20Qiu%20and%20Zhaoxi%20Chen%20and%20Zhouxia%20Wang%20and%20Yingqing%20He%20and%20Menghan%20Xia%20and%20Ziwei%20Liu&entry.1292438233=%20%20Diffusion%20model%20has%20demonstrated%20remarkable%20capability%20in%20video%20generation%2C%0Awhich%20further%20sparks%20interest%20in%20introducing%20trajectory%20control%20into%20the%0Ageneration%20process.%20While%20existing%20works%20mainly%20focus%20on%20training-based%20methods%0A%28e.g.%2C%20conditional%20adapter%29%2C%20we%20argue%20that%20diffusion%20model%20itself%20allows%20decent%0Acontrol%20over%20the%20generated%20content%20without%20requiring%20any%20training.%20In%20this%0Astudy%2C%20we%20introduce%20a%20tuning-free%20framework%20to%20achieve%20trajectory-controllable%0Avideo%20generation%2C%20by%20imposing%20guidance%20on%20both%20noise%20construction%20and%20attention%0Acomputation.%20Specifically%2C%201%29%20we%20first%20show%20several%20instructive%20phenomenons%20and%0Aanalyze%20how%20initial%20noises%20influence%20the%20motion%20trajectory%20of%20generated%0Acontent.%202%29%20Subsequently%2C%20we%20propose%20FreeTraj%2C%20a%20tuning-free%20approach%20that%0Aenables%20trajectory%20control%20by%20modifying%20noise%20sampling%20and%20attention%0Amechanisms.%203%29%20Furthermore%2C%20we%20extend%20FreeTraj%20to%20facilitate%20longer%20and%20larger%0Avideo%20generation%20with%20controllable%20trajectories.%20Equipped%20with%20these%20designs%2C%0Ausers%20have%20the%20flexibility%20to%20provide%20trajectories%20manually%20or%20opt%20for%0Atrajectories%20automatically%20generated%20by%20the%20LLM%20trajectory%20planner.%20Extensive%0Aexperiments%20validate%20the%20efficacy%20of%20our%20approach%20in%20enhancing%20the%20trajectory%0Acontrollability%20of%20video%20diffusion%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16863v1&entry.124074799=Read"},
{"title": "Dreamitate: Real-World Visuomotor Policy Learning via Video Generation", "author": "Junbang Liang and Ruoshi Liu and Ege Ozguroglu and Sruthi Sudhakar and Achal Dave and Pavel Tokmakov and Shuran Song and Carl Vondrick", "abstract": "  A key challenge in manipulation is learning a policy that can robustly\ngeneralize to diverse visual environments. A promising mechanism for learning\nrobust policies is to leverage video generative models, which are pretrained on\nlarge-scale datasets of internet videos. In this paper, we propose a visuomotor\npolicy learning framework that fine-tunes a video diffusion model on human\ndemonstrations of a given task. At test time, we generate an example of an\nexecution of the task conditioned on images of a novel scene, and use this\nsynthesized execution directly to control the robot. Our key insight is that\nusing common tools allows us to effortlessly bridge the embodiment gap between\nthe human hand and the robot manipulator. We evaluate our approach on four\ntasks of increasing complexity and demonstrate that harnessing internet-scale\ngenerative models allows the learned policy to achieve a significantly higher\ndegree of generalization than existing behavior cloning approaches.\n", "link": "http://arxiv.org/abs/2406.16862v1", "date": "2024-06-24", "relevancy": 2.4594, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6358}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6004}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dreamitate%3A%20Real-World%20Visuomotor%20Policy%20Learning%20via%20Video%20Generation&body=Title%3A%20Dreamitate%3A%20Real-World%20Visuomotor%20Policy%20Learning%20via%20Video%20Generation%0AAuthor%3A%20Junbang%20Liang%20and%20Ruoshi%20Liu%20and%20Ege%20Ozguroglu%20and%20Sruthi%20Sudhakar%20and%20Achal%20Dave%20and%20Pavel%20Tokmakov%20and%20Shuran%20Song%20and%20Carl%20Vondrick%0AAbstract%3A%20%20%20A%20key%20challenge%20in%20manipulation%20is%20learning%20a%20policy%20that%20can%20robustly%0Ageneralize%20to%20diverse%20visual%20environments.%20A%20promising%20mechanism%20for%20learning%0Arobust%20policies%20is%20to%20leverage%20video%20generative%20models%2C%20which%20are%20pretrained%20on%0Alarge-scale%20datasets%20of%20internet%20videos.%20In%20this%20paper%2C%20we%20propose%20a%20visuomotor%0Apolicy%20learning%20framework%20that%20fine-tunes%20a%20video%20diffusion%20model%20on%20human%0Ademonstrations%20of%20a%20given%20task.%20At%20test%20time%2C%20we%20generate%20an%20example%20of%20an%0Aexecution%20of%20the%20task%20conditioned%20on%20images%20of%20a%20novel%20scene%2C%20and%20use%20this%0Asynthesized%20execution%20directly%20to%20control%20the%20robot.%20Our%20key%20insight%20is%20that%0Ausing%20common%20tools%20allows%20us%20to%20effortlessly%20bridge%20the%20embodiment%20gap%20between%0Athe%20human%20hand%20and%20the%20robot%20manipulator.%20We%20evaluate%20our%20approach%20on%20four%0Atasks%20of%20increasing%20complexity%20and%20demonstrate%20that%20harnessing%20internet-scale%0Agenerative%20models%20allows%20the%20learned%20policy%20to%20achieve%20a%20significantly%20higher%0Adegree%20of%20generalization%20than%20existing%20behavior%20cloning%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamitate%253A%2520Real-World%2520Visuomotor%2520Policy%2520Learning%2520via%2520Video%2520Generation%26entry.906535625%3DJunbang%2520Liang%2520and%2520Ruoshi%2520Liu%2520and%2520Ege%2520Ozguroglu%2520and%2520Sruthi%2520Sudhakar%2520and%2520Achal%2520Dave%2520and%2520Pavel%2520Tokmakov%2520and%2520Shuran%2520Song%2520and%2520Carl%2520Vondrick%26entry.1292438233%3D%2520%2520A%2520key%2520challenge%2520in%2520manipulation%2520is%2520learning%2520a%2520policy%2520that%2520can%2520robustly%250Ageneralize%2520to%2520diverse%2520visual%2520environments.%2520A%2520promising%2520mechanism%2520for%2520learning%250Arobust%2520policies%2520is%2520to%2520leverage%2520video%2520generative%2520models%252C%2520which%2520are%2520pretrained%2520on%250Alarge-scale%2520datasets%2520of%2520internet%2520videos.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520visuomotor%250Apolicy%2520learning%2520framework%2520that%2520fine-tunes%2520a%2520video%2520diffusion%2520model%2520on%2520human%250Ademonstrations%2520of%2520a%2520given%2520task.%2520At%2520test%2520time%252C%2520we%2520generate%2520an%2520example%2520of%2520an%250Aexecution%2520of%2520the%2520task%2520conditioned%2520on%2520images%2520of%2520a%2520novel%2520scene%252C%2520and%2520use%2520this%250Asynthesized%2520execution%2520directly%2520to%2520control%2520the%2520robot.%2520Our%2520key%2520insight%2520is%2520that%250Ausing%2520common%2520tools%2520allows%2520us%2520to%2520effortlessly%2520bridge%2520the%2520embodiment%2520gap%2520between%250Athe%2520human%2520hand%2520and%2520the%2520robot%2520manipulator.%2520We%2520evaluate%2520our%2520approach%2520on%2520four%250Atasks%2520of%2520increasing%2520complexity%2520and%2520demonstrate%2520that%2520harnessing%2520internet-scale%250Agenerative%2520models%2520allows%2520the%2520learned%2520policy%2520to%2520achieve%2520a%2520significantly%2520higher%250Adegree%2520of%2520generalization%2520than%2520existing%2520behavior%2520cloning%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dreamitate%3A%20Real-World%20Visuomotor%20Policy%20Learning%20via%20Video%20Generation&entry.906535625=Junbang%20Liang%20and%20Ruoshi%20Liu%20and%20Ege%20Ozguroglu%20and%20Sruthi%20Sudhakar%20and%20Achal%20Dave%20and%20Pavel%20Tokmakov%20and%20Shuran%20Song%20and%20Carl%20Vondrick&entry.1292438233=%20%20A%20key%20challenge%20in%20manipulation%20is%20learning%20a%20policy%20that%20can%20robustly%0Ageneralize%20to%20diverse%20visual%20environments.%20A%20promising%20mechanism%20for%20learning%0Arobust%20policies%20is%20to%20leverage%20video%20generative%20models%2C%20which%20are%20pretrained%20on%0Alarge-scale%20datasets%20of%20internet%20videos.%20In%20this%20paper%2C%20we%20propose%20a%20visuomotor%0Apolicy%20learning%20framework%20that%20fine-tunes%20a%20video%20diffusion%20model%20on%20human%0Ademonstrations%20of%20a%20given%20task.%20At%20test%20time%2C%20we%20generate%20an%20example%20of%20an%0Aexecution%20of%20the%20task%20conditioned%20on%20images%20of%20a%20novel%20scene%2C%20and%20use%20this%0Asynthesized%20execution%20directly%20to%20control%20the%20robot.%20Our%20key%20insight%20is%20that%0Ausing%20common%20tools%20allows%20us%20to%20effortlessly%20bridge%20the%20embodiment%20gap%20between%0Athe%20human%20hand%20and%20the%20robot%20manipulator.%20We%20evaluate%20our%20approach%20on%20four%0Atasks%20of%20increasing%20complexity%20and%20demonstrate%20that%20harnessing%20internet-scale%0Agenerative%20models%20allows%20the%20learned%20policy%20to%20achieve%20a%20significantly%20higher%0Adegree%20of%20generalization%20than%20existing%20behavior%20cloning%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16862v1&entry.124074799=Read"},
{"title": "Continual Road-Scene Semantic Segmentation via Feature-Aligned Symmetric\n  Multi-Modal Network", "author": "Francesco Barbato and Elena Camuffo and Simone Milani and Pietro Zanuttigh", "abstract": "  State-of-the-art multimodal semantic segmentation strategies combining LiDAR\nand color data are usually designed on top of asymmetric information-sharing\nschemes and assume that both modalities are always available. This strong\nassumption may not hold in real-world scenarios, where sensors are prone to\nfailure or can face adverse conditions that make the acquired information\nunreliable. This problem is exacerbated when continual learning scenarios are\nconsidered since they have stringent data reliability constraints. In this\nwork, we re-frame the task of multimodal semantic segmentation by enforcing a\ntightly coupled feature representation and a symmetric information-sharing\nscheme, which allows our approach to work even when one of the input modalities\nis missing. We also introduce an ad-hoc class-incremental continual learning\nscheme, proving our approach's effectiveness and reliability even in\nsafety-critical settings, such as autonomous driving. We evaluate our approach\non the SemanticKITTI dataset, achieving impressive performances.\n", "link": "http://arxiv.org/abs/2308.04702v2", "date": "2024-06-24", "relevancy": 2.4221, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6297}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6089}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Road-Scene%20Semantic%20Segmentation%20via%20Feature-Aligned%20Symmetric%0A%20%20Multi-Modal%20Network&body=Title%3A%20Continual%20Road-Scene%20Semantic%20Segmentation%20via%20Feature-Aligned%20Symmetric%0A%20%20Multi-Modal%20Network%0AAuthor%3A%20Francesco%20Barbato%20and%20Elena%20Camuffo%20and%20Simone%20Milani%20and%20Pietro%20Zanuttigh%0AAbstract%3A%20%20%20State-of-the-art%20multimodal%20semantic%20segmentation%20strategies%20combining%20LiDAR%0Aand%20color%20data%20are%20usually%20designed%20on%20top%20of%20asymmetric%20information-sharing%0Aschemes%20and%20assume%20that%20both%20modalities%20are%20always%20available.%20This%20strong%0Aassumption%20may%20not%20hold%20in%20real-world%20scenarios%2C%20where%20sensors%20are%20prone%20to%0Afailure%20or%20can%20face%20adverse%20conditions%20that%20make%20the%20acquired%20information%0Aunreliable.%20This%20problem%20is%20exacerbated%20when%20continual%20learning%20scenarios%20are%0Aconsidered%20since%20they%20have%20stringent%20data%20reliability%20constraints.%20In%20this%0Awork%2C%20we%20re-frame%20the%20task%20of%20multimodal%20semantic%20segmentation%20by%20enforcing%20a%0Atightly%20coupled%20feature%20representation%20and%20a%20symmetric%20information-sharing%0Ascheme%2C%20which%20allows%20our%20approach%20to%20work%20even%20when%20one%20of%20the%20input%20modalities%0Ais%20missing.%20We%20also%20introduce%20an%20ad-hoc%20class-incremental%20continual%20learning%0Ascheme%2C%20proving%20our%20approach%27s%20effectiveness%20and%20reliability%20even%20in%0Asafety-critical%20settings%2C%20such%20as%20autonomous%20driving.%20We%20evaluate%20our%20approach%0Aon%20the%20SemanticKITTI%20dataset%2C%20achieving%20impressive%20performances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.04702v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Road-Scene%2520Semantic%2520Segmentation%2520via%2520Feature-Aligned%2520Symmetric%250A%2520%2520Multi-Modal%2520Network%26entry.906535625%3DFrancesco%2520Barbato%2520and%2520Elena%2520Camuffo%2520and%2520Simone%2520Milani%2520and%2520Pietro%2520Zanuttigh%26entry.1292438233%3D%2520%2520State-of-the-art%2520multimodal%2520semantic%2520segmentation%2520strategies%2520combining%2520LiDAR%250Aand%2520color%2520data%2520are%2520usually%2520designed%2520on%2520top%2520of%2520asymmetric%2520information-sharing%250Aschemes%2520and%2520assume%2520that%2520both%2520modalities%2520are%2520always%2520available.%2520This%2520strong%250Aassumption%2520may%2520not%2520hold%2520in%2520real-world%2520scenarios%252C%2520where%2520sensors%2520are%2520prone%2520to%250Afailure%2520or%2520can%2520face%2520adverse%2520conditions%2520that%2520make%2520the%2520acquired%2520information%250Aunreliable.%2520This%2520problem%2520is%2520exacerbated%2520when%2520continual%2520learning%2520scenarios%2520are%250Aconsidered%2520since%2520they%2520have%2520stringent%2520data%2520reliability%2520constraints.%2520In%2520this%250Awork%252C%2520we%2520re-frame%2520the%2520task%2520of%2520multimodal%2520semantic%2520segmentation%2520by%2520enforcing%2520a%250Atightly%2520coupled%2520feature%2520representation%2520and%2520a%2520symmetric%2520information-sharing%250Ascheme%252C%2520which%2520allows%2520our%2520approach%2520to%2520work%2520even%2520when%2520one%2520of%2520the%2520input%2520modalities%250Ais%2520missing.%2520We%2520also%2520introduce%2520an%2520ad-hoc%2520class-incremental%2520continual%2520learning%250Ascheme%252C%2520proving%2520our%2520approach%2527s%2520effectiveness%2520and%2520reliability%2520even%2520in%250Asafety-critical%2520settings%252C%2520such%2520as%2520autonomous%2520driving.%2520We%2520evaluate%2520our%2520approach%250Aon%2520the%2520SemanticKITTI%2520dataset%252C%2520achieving%2520impressive%2520performances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.04702v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Road-Scene%20Semantic%20Segmentation%20via%20Feature-Aligned%20Symmetric%0A%20%20Multi-Modal%20Network&entry.906535625=Francesco%20Barbato%20and%20Elena%20Camuffo%20and%20Simone%20Milani%20and%20Pietro%20Zanuttigh&entry.1292438233=%20%20State-of-the-art%20multimodal%20semantic%20segmentation%20strategies%20combining%20LiDAR%0Aand%20color%20data%20are%20usually%20designed%20on%20top%20of%20asymmetric%20information-sharing%0Aschemes%20and%20assume%20that%20both%20modalities%20are%20always%20available.%20This%20strong%0Aassumption%20may%20not%20hold%20in%20real-world%20scenarios%2C%20where%20sensors%20are%20prone%20to%0Afailure%20or%20can%20face%20adverse%20conditions%20that%20make%20the%20acquired%20information%0Aunreliable.%20This%20problem%20is%20exacerbated%20when%20continual%20learning%20scenarios%20are%0Aconsidered%20since%20they%20have%20stringent%20data%20reliability%20constraints.%20In%20this%0Awork%2C%20we%20re-frame%20the%20task%20of%20multimodal%20semantic%20segmentation%20by%20enforcing%20a%0Atightly%20coupled%20feature%20representation%20and%20a%20symmetric%20information-sharing%0Ascheme%2C%20which%20allows%20our%20approach%20to%20work%20even%20when%20one%20of%20the%20input%20modalities%0Ais%20missing.%20We%20also%20introduce%20an%20ad-hoc%20class-incremental%20continual%20learning%0Ascheme%2C%20proving%20our%20approach%27s%20effectiveness%20and%20reliability%20even%20in%0Asafety-critical%20settings%2C%20such%20as%20autonomous%20driving.%20We%20evaluate%20our%20approach%0Aon%20the%20SemanticKITTI%20dataset%2C%20achieving%20impressive%20performances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.04702v2&entry.124074799=Read"},
{"title": "Deep Reinforcement Learning: A Convex Optimization Approach", "author": "Ather Gattami", "abstract": "  In this paper, we consider reinforcement learning of nonlinear systems with\ncontinuous state and action spaces. We present an episodic learning algorithm,\nwhere we for each episode use convex optimization to find a two-layer neural\nnetwork approximation of the optimal $Q$-function. The convex optimization\napproach guarantees that the weights calculated at each episode are optimal,\nwith respect to the given sampled states and actions of the current episode.\nFor stable nonlinear systems, we show that the algorithm converges and that the\nconverging parameters of the trained neural network can be made arbitrarily\nclose to the optimal neural network parameters. In particular, if the\nregularization parameter in the training phase is given by $\\rho$, then the\nparameters of the trained neural network converge to $w$, where the distance\nbetween $w$ and the optimal parameters $w^\\star$ is bounded by\n$\\mathcal{O}(\\rho)$. That is, when the number of episodes goes to infinity,\nthere exists a constant $C$ such that \\[\n  \\|w-w^\\star\\| \\le C\\rho. \\]\n  In particular, our algorithm converges arbitrarily close to the optimal\nneural network parameters as the regularization parameter goes to zero. As a\nconsequence, our algorithm converges fast due to the polynomial-time\nconvergence of convex optimization algorithms.\n", "link": "http://arxiv.org/abs/2402.19212v6", "date": "2024-06-24", "relevancy": 2.422, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5061}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4856}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Reinforcement%20Learning%3A%20A%20Convex%20Optimization%20Approach&body=Title%3A%20Deep%20Reinforcement%20Learning%3A%20A%20Convex%20Optimization%20Approach%0AAuthor%3A%20Ather%20Gattami%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20consider%20reinforcement%20learning%20of%20nonlinear%20systems%20with%0Acontinuous%20state%20and%20action%20spaces.%20We%20present%20an%20episodic%20learning%20algorithm%2C%0Awhere%20we%20for%20each%20episode%20use%20convex%20optimization%20to%20find%20a%20two-layer%20neural%0Anetwork%20approximation%20of%20the%20optimal%20%24Q%24-function.%20The%20convex%20optimization%0Aapproach%20guarantees%20that%20the%20weights%20calculated%20at%20each%20episode%20are%20optimal%2C%0Awith%20respect%20to%20the%20given%20sampled%20states%20and%20actions%20of%20the%20current%20episode.%0AFor%20stable%20nonlinear%20systems%2C%20we%20show%20that%20the%20algorithm%20converges%20and%20that%20the%0Aconverging%20parameters%20of%20the%20trained%20neural%20network%20can%20be%20made%20arbitrarily%0Aclose%20to%20the%20optimal%20neural%20network%20parameters.%20In%20particular%2C%20if%20the%0Aregularization%20parameter%20in%20the%20training%20phase%20is%20given%20by%20%24%5Crho%24%2C%20then%20the%0Aparameters%20of%20the%20trained%20neural%20network%20converge%20to%20%24w%24%2C%20where%20the%20distance%0Abetween%20%24w%24%20and%20the%20optimal%20parameters%20%24w%5E%5Cstar%24%20is%20bounded%20by%0A%24%5Cmathcal%7BO%7D%28%5Crho%29%24.%20That%20is%2C%20when%20the%20number%20of%20episodes%20goes%20to%20infinity%2C%0Athere%20exists%20a%20constant%20%24C%24%20such%20that%20%5C%5B%0A%20%20%5C%7Cw-w%5E%5Cstar%5C%7C%20%5Cle%20C%5Crho.%20%5C%5D%0A%20%20In%20particular%2C%20our%20algorithm%20converges%20arbitrarily%20close%20to%20the%20optimal%0Aneural%20network%20parameters%20as%20the%20regularization%20parameter%20goes%20to%20zero.%20As%20a%0Aconsequence%2C%20our%20algorithm%20converges%20fast%20due%20to%20the%20polynomial-time%0Aconvergence%20of%20convex%20optimization%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19212v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Reinforcement%2520Learning%253A%2520A%2520Convex%2520Optimization%2520Approach%26entry.906535625%3DAther%2520Gattami%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520consider%2520reinforcement%2520learning%2520of%2520nonlinear%2520systems%2520with%250Acontinuous%2520state%2520and%2520action%2520spaces.%2520We%2520present%2520an%2520episodic%2520learning%2520algorithm%252C%250Awhere%2520we%2520for%2520each%2520episode%2520use%2520convex%2520optimization%2520to%2520find%2520a%2520two-layer%2520neural%250Anetwork%2520approximation%2520of%2520the%2520optimal%2520%2524Q%2524-function.%2520The%2520convex%2520optimization%250Aapproach%2520guarantees%2520that%2520the%2520weights%2520calculated%2520at%2520each%2520episode%2520are%2520optimal%252C%250Awith%2520respect%2520to%2520the%2520given%2520sampled%2520states%2520and%2520actions%2520of%2520the%2520current%2520episode.%250AFor%2520stable%2520nonlinear%2520systems%252C%2520we%2520show%2520that%2520the%2520algorithm%2520converges%2520and%2520that%2520the%250Aconverging%2520parameters%2520of%2520the%2520trained%2520neural%2520network%2520can%2520be%2520made%2520arbitrarily%250Aclose%2520to%2520the%2520optimal%2520neural%2520network%2520parameters.%2520In%2520particular%252C%2520if%2520the%250Aregularization%2520parameter%2520in%2520the%2520training%2520phase%2520is%2520given%2520by%2520%2524%255Crho%2524%252C%2520then%2520the%250Aparameters%2520of%2520the%2520trained%2520neural%2520network%2520converge%2520to%2520%2524w%2524%252C%2520where%2520the%2520distance%250Abetween%2520%2524w%2524%2520and%2520the%2520optimal%2520parameters%2520%2524w%255E%255Cstar%2524%2520is%2520bounded%2520by%250A%2524%255Cmathcal%257BO%257D%2528%255Crho%2529%2524.%2520That%2520is%252C%2520when%2520the%2520number%2520of%2520episodes%2520goes%2520to%2520infinity%252C%250Athere%2520exists%2520a%2520constant%2520%2524C%2524%2520such%2520that%2520%255C%255B%250A%2520%2520%255C%257Cw-w%255E%255Cstar%255C%257C%2520%255Cle%2520C%255Crho.%2520%255C%255D%250A%2520%2520In%2520particular%252C%2520our%2520algorithm%2520converges%2520arbitrarily%2520close%2520to%2520the%2520optimal%250Aneural%2520network%2520parameters%2520as%2520the%2520regularization%2520parameter%2520goes%2520to%2520zero.%2520As%2520a%250Aconsequence%252C%2520our%2520algorithm%2520converges%2520fast%2520due%2520to%2520the%2520polynomial-time%250Aconvergence%2520of%2520convex%2520optimization%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19212v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Reinforcement%20Learning%3A%20A%20Convex%20Optimization%20Approach&entry.906535625=Ather%20Gattami&entry.1292438233=%20%20In%20this%20paper%2C%20we%20consider%20reinforcement%20learning%20of%20nonlinear%20systems%20with%0Acontinuous%20state%20and%20action%20spaces.%20We%20present%20an%20episodic%20learning%20algorithm%2C%0Awhere%20we%20for%20each%20episode%20use%20convex%20optimization%20to%20find%20a%20two-layer%20neural%0Anetwork%20approximation%20of%20the%20optimal%20%24Q%24-function.%20The%20convex%20optimization%0Aapproach%20guarantees%20that%20the%20weights%20calculated%20at%20each%20episode%20are%20optimal%2C%0Awith%20respect%20to%20the%20given%20sampled%20states%20and%20actions%20of%20the%20current%20episode.%0AFor%20stable%20nonlinear%20systems%2C%20we%20show%20that%20the%20algorithm%20converges%20and%20that%20the%0Aconverging%20parameters%20of%20the%20trained%20neural%20network%20can%20be%20made%20arbitrarily%0Aclose%20to%20the%20optimal%20neural%20network%20parameters.%20In%20particular%2C%20if%20the%0Aregularization%20parameter%20in%20the%20training%20phase%20is%20given%20by%20%24%5Crho%24%2C%20then%20the%0Aparameters%20of%20the%20trained%20neural%20network%20converge%20to%20%24w%24%2C%20where%20the%20distance%0Abetween%20%24w%24%20and%20the%20optimal%20parameters%20%24w%5E%5Cstar%24%20is%20bounded%20by%0A%24%5Cmathcal%7BO%7D%28%5Crho%29%24.%20That%20is%2C%20when%20the%20number%20of%20episodes%20goes%20to%20infinity%2C%0Athere%20exists%20a%20constant%20%24C%24%20such%20that%20%5C%5B%0A%20%20%5C%7Cw-w%5E%5Cstar%5C%7C%20%5Cle%20C%5Crho.%20%5C%5D%0A%20%20In%20particular%2C%20our%20algorithm%20converges%20arbitrarily%20close%20to%20the%20optimal%0Aneural%20network%20parameters%20as%20the%20regularization%20parameter%20goes%20to%20zero.%20As%20a%0Aconsequence%2C%20our%20algorithm%20converges%20fast%20due%20to%20the%20polynomial-time%0Aconvergence%20of%20convex%20optimization%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19212v6&entry.124074799=Read"},
{"title": "From Perfect to Noisy World Simulation: Customizable Embodied\n  Multi-modal Perturbations for SLAM Robustness Benchmarking", "author": "Xiaohao Xu and Tianyi Zhang and Sibo Wang and Xiang Li and Yongqi Chen and Ye Li and Bhiksha Raj and Matthew Johnson-Roberson and Xiaonan Huang", "abstract": "  Embodied agents require robust navigation systems to operate in unstructured\nenvironments, making the robustness of Simultaneous Localization and Mapping\n(SLAM) models critical to embodied agent autonomy. While real-world datasets\nare invaluable, simulation-based benchmarks offer a scalable approach for\nrobustness evaluations. However, the creation of a challenging and controllable\nnoisy world with diverse perturbations remains under-explored. To this end, we\npropose a novel, customizable pipeline for noisy data synthesis, aimed at\nassessing the resilience of multi-modal SLAM models against various\nperturbations. The pipeline comprises a comprehensive taxonomy of sensor and\nmotion perturbations for embodied multi-modal (specifically RGB-D) sensing,\ncategorized by their sources and propagation order, allowing for procedural\ncomposition. We also provide a toolbox for synthesizing these perturbations,\nenabling the transformation of clean environments into challenging noisy\nsimulations. Utilizing the pipeline, we instantiate the large-scale\nNoisy-Replica benchmark, which includes diverse perturbation types, to evaluate\nthe risk tolerance of existing advanced RGB-D SLAM models. Our extensive\nanalysis uncovers the susceptibilities of both neural (NeRF and Gaussian\nSplatting -based) and non-neural SLAM models to disturbances, despite their\ndemonstrated accuracy in standard benchmarks. Our code is publicly available at\nhttps://github.com/Xiaohao-Xu/SLAM-under-Perturbation.\n", "link": "http://arxiv.org/abs/2406.16850v1", "date": "2024-06-24", "relevancy": 2.4219, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6327}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6242}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Perfect%20to%20Noisy%20World%20Simulation%3A%20Customizable%20Embodied%0A%20%20Multi-modal%20Perturbations%20for%20SLAM%20Robustness%20Benchmarking&body=Title%3A%20From%20Perfect%20to%20Noisy%20World%20Simulation%3A%20Customizable%20Embodied%0A%20%20Multi-modal%20Perturbations%20for%20SLAM%20Robustness%20Benchmarking%0AAuthor%3A%20Xiaohao%20Xu%20and%20Tianyi%20Zhang%20and%20Sibo%20Wang%20and%20Xiang%20Li%20and%20Yongqi%20Chen%20and%20Ye%20Li%20and%20Bhiksha%20Raj%20and%20Matthew%20Johnson-Roberson%20and%20Xiaonan%20Huang%0AAbstract%3A%20%20%20Embodied%20agents%20require%20robust%20navigation%20systems%20to%20operate%20in%20unstructured%0Aenvironments%2C%20making%20the%20robustness%20of%20Simultaneous%20Localization%20and%20Mapping%0A%28SLAM%29%20models%20critical%20to%20embodied%20agent%20autonomy.%20While%20real-world%20datasets%0Aare%20invaluable%2C%20simulation-based%20benchmarks%20offer%20a%20scalable%20approach%20for%0Arobustness%20evaluations.%20However%2C%20the%20creation%20of%20a%20challenging%20and%20controllable%0Anoisy%20world%20with%20diverse%20perturbations%20remains%20under-explored.%20To%20this%20end%2C%20we%0Apropose%20a%20novel%2C%20customizable%20pipeline%20for%20noisy%20data%20synthesis%2C%20aimed%20at%0Aassessing%20the%20resilience%20of%20multi-modal%20SLAM%20models%20against%20various%0Aperturbations.%20The%20pipeline%20comprises%20a%20comprehensive%20taxonomy%20of%20sensor%20and%0Amotion%20perturbations%20for%20embodied%20multi-modal%20%28specifically%20RGB-D%29%20sensing%2C%0Acategorized%20by%20their%20sources%20and%20propagation%20order%2C%20allowing%20for%20procedural%0Acomposition.%20We%20also%20provide%20a%20toolbox%20for%20synthesizing%20these%20perturbations%2C%0Aenabling%20the%20transformation%20of%20clean%20environments%20into%20challenging%20noisy%0Asimulations.%20Utilizing%20the%20pipeline%2C%20we%20instantiate%20the%20large-scale%0ANoisy-Replica%20benchmark%2C%20which%20includes%20diverse%20perturbation%20types%2C%20to%20evaluate%0Athe%20risk%20tolerance%20of%20existing%20advanced%20RGB-D%20SLAM%20models.%20Our%20extensive%0Aanalysis%20uncovers%20the%20susceptibilities%20of%20both%20neural%20%28NeRF%20and%20Gaussian%0ASplatting%20-based%29%20and%20non-neural%20SLAM%20models%20to%20disturbances%2C%20despite%20their%0Ademonstrated%20accuracy%20in%20standard%20benchmarks.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Xiaohao-Xu/SLAM-under-Perturbation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Perfect%2520to%2520Noisy%2520World%2520Simulation%253A%2520Customizable%2520Embodied%250A%2520%2520Multi-modal%2520Perturbations%2520for%2520SLAM%2520Robustness%2520Benchmarking%26entry.906535625%3DXiaohao%2520Xu%2520and%2520Tianyi%2520Zhang%2520and%2520Sibo%2520Wang%2520and%2520Xiang%2520Li%2520and%2520Yongqi%2520Chen%2520and%2520Ye%2520Li%2520and%2520Bhiksha%2520Raj%2520and%2520Matthew%2520Johnson-Roberson%2520and%2520Xiaonan%2520Huang%26entry.1292438233%3D%2520%2520Embodied%2520agents%2520require%2520robust%2520navigation%2520systems%2520to%2520operate%2520in%2520unstructured%250Aenvironments%252C%2520making%2520the%2520robustness%2520of%2520Simultaneous%2520Localization%2520and%2520Mapping%250A%2528SLAM%2529%2520models%2520critical%2520to%2520embodied%2520agent%2520autonomy.%2520While%2520real-world%2520datasets%250Aare%2520invaluable%252C%2520simulation-based%2520benchmarks%2520offer%2520a%2520scalable%2520approach%2520for%250Arobustness%2520evaluations.%2520However%252C%2520the%2520creation%2520of%2520a%2520challenging%2520and%2520controllable%250Anoisy%2520world%2520with%2520diverse%2520perturbations%2520remains%2520under-explored.%2520To%2520this%2520end%252C%2520we%250Apropose%2520a%2520novel%252C%2520customizable%2520pipeline%2520for%2520noisy%2520data%2520synthesis%252C%2520aimed%2520at%250Aassessing%2520the%2520resilience%2520of%2520multi-modal%2520SLAM%2520models%2520against%2520various%250Aperturbations.%2520The%2520pipeline%2520comprises%2520a%2520comprehensive%2520taxonomy%2520of%2520sensor%2520and%250Amotion%2520perturbations%2520for%2520embodied%2520multi-modal%2520%2528specifically%2520RGB-D%2529%2520sensing%252C%250Acategorized%2520by%2520their%2520sources%2520and%2520propagation%2520order%252C%2520allowing%2520for%2520procedural%250Acomposition.%2520We%2520also%2520provide%2520a%2520toolbox%2520for%2520synthesizing%2520these%2520perturbations%252C%250Aenabling%2520the%2520transformation%2520of%2520clean%2520environments%2520into%2520challenging%2520noisy%250Asimulations.%2520Utilizing%2520the%2520pipeline%252C%2520we%2520instantiate%2520the%2520large-scale%250ANoisy-Replica%2520benchmark%252C%2520which%2520includes%2520diverse%2520perturbation%2520types%252C%2520to%2520evaluate%250Athe%2520risk%2520tolerance%2520of%2520existing%2520advanced%2520RGB-D%2520SLAM%2520models.%2520Our%2520extensive%250Aanalysis%2520uncovers%2520the%2520susceptibilities%2520of%2520both%2520neural%2520%2528NeRF%2520and%2520Gaussian%250ASplatting%2520-based%2529%2520and%2520non-neural%2520SLAM%2520models%2520to%2520disturbances%252C%2520despite%2520their%250Ademonstrated%2520accuracy%2520in%2520standard%2520benchmarks.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/Xiaohao-Xu/SLAM-under-Perturbation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Perfect%20to%20Noisy%20World%20Simulation%3A%20Customizable%20Embodied%0A%20%20Multi-modal%20Perturbations%20for%20SLAM%20Robustness%20Benchmarking&entry.906535625=Xiaohao%20Xu%20and%20Tianyi%20Zhang%20and%20Sibo%20Wang%20and%20Xiang%20Li%20and%20Yongqi%20Chen%20and%20Ye%20Li%20and%20Bhiksha%20Raj%20and%20Matthew%20Johnson-Roberson%20and%20Xiaonan%20Huang&entry.1292438233=%20%20Embodied%20agents%20require%20robust%20navigation%20systems%20to%20operate%20in%20unstructured%0Aenvironments%2C%20making%20the%20robustness%20of%20Simultaneous%20Localization%20and%20Mapping%0A%28SLAM%29%20models%20critical%20to%20embodied%20agent%20autonomy.%20While%20real-world%20datasets%0Aare%20invaluable%2C%20simulation-based%20benchmarks%20offer%20a%20scalable%20approach%20for%0Arobustness%20evaluations.%20However%2C%20the%20creation%20of%20a%20challenging%20and%20controllable%0Anoisy%20world%20with%20diverse%20perturbations%20remains%20under-explored.%20To%20this%20end%2C%20we%0Apropose%20a%20novel%2C%20customizable%20pipeline%20for%20noisy%20data%20synthesis%2C%20aimed%20at%0Aassessing%20the%20resilience%20of%20multi-modal%20SLAM%20models%20against%20various%0Aperturbations.%20The%20pipeline%20comprises%20a%20comprehensive%20taxonomy%20of%20sensor%20and%0Amotion%20perturbations%20for%20embodied%20multi-modal%20%28specifically%20RGB-D%29%20sensing%2C%0Acategorized%20by%20their%20sources%20and%20propagation%20order%2C%20allowing%20for%20procedural%0Acomposition.%20We%20also%20provide%20a%20toolbox%20for%20synthesizing%20these%20perturbations%2C%0Aenabling%20the%20transformation%20of%20clean%20environments%20into%20challenging%20noisy%0Asimulations.%20Utilizing%20the%20pipeline%2C%20we%20instantiate%20the%20large-scale%0ANoisy-Replica%20benchmark%2C%20which%20includes%20diverse%20perturbation%20types%2C%20to%20evaluate%0Athe%20risk%20tolerance%20of%20existing%20advanced%20RGB-D%20SLAM%20models.%20Our%20extensive%0Aanalysis%20uncovers%20the%20susceptibilities%20of%20both%20neural%20%28NeRF%20and%20Gaussian%0ASplatting%20-based%29%20and%20non-neural%20SLAM%20models%20to%20disturbances%2C%20despite%20their%0Ademonstrated%20accuracy%20in%20standard%20benchmarks.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Xiaohao-Xu/SLAM-under-Perturbation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16850v1&entry.124074799=Read"},
{"title": "Segment Any Text: A Universal Approach for Robust, Efficient and\n  Adaptable Sentence Segmentation", "author": "Markus Frohmann and Igor Sterner and Ivan Vuli\u0107 and Benjamin Minixhofer and Markus Schedl", "abstract": "  Segmenting text into sentences plays an early and crucial role in many NLP\nsystems. This is commonly achieved by using rule-based or statistical methods\nrelying on lexical features such as punctuation. Although some recent works no\nlonger exclusively rely on punctuation, we find that no prior method achieves\nall of (i) robustness to missing punctuation, (ii) effective adaptability to\nnew domains, and (iii) high efficiency. We introduce a new model - Segment any\nText (SaT) - to solve this problem. To enhance robustness, we propose a new\npretraining scheme that ensures less reliance on punctuation. To address\nadaptability, we introduce an extra stage of parameter-efficient fine-tuning,\nestablishing state-of-the-art performance in distinct domains such as verses\nfrom lyrics and legal documents. Along the way, we introduce architectural\nmodifications that result in a threefold gain in speed over the previous state\nof the art and solve spurious reliance on context far in the future. Finally,\nwe introduce a variant of our model with fine-tuning on a diverse, multilingual\nmixture of sentence-segmented data, acting as a drop-in replacement and\nenhancement for existing segmentation tools. Overall, our contributions provide\na universal approach for segmenting any text. Our method outperforms all\nbaselines - including strong LLMs - across 8 corpora spanning diverse domains\nand languages, especially in practically relevant situations where text is\npoorly formatted. Our models and code, including documentation, are available\nat https://huggingface.co/segment-any-text under the MIT license.\n", "link": "http://arxiv.org/abs/2406.16678v1", "date": "2024-06-24", "relevancy": 2.4218, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.49}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4857}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20Any%20Text%3A%20A%20Universal%20Approach%20for%20Robust%2C%20Efficient%20and%0A%20%20Adaptable%20Sentence%20Segmentation&body=Title%3A%20Segment%20Any%20Text%3A%20A%20Universal%20Approach%20for%20Robust%2C%20Efficient%20and%0A%20%20Adaptable%20Sentence%20Segmentation%0AAuthor%3A%20Markus%20Frohmann%20and%20Igor%20Sterner%20and%20Ivan%20Vuli%C4%87%20and%20Benjamin%20Minixhofer%20and%20Markus%20Schedl%0AAbstract%3A%20%20%20Segmenting%20text%20into%20sentences%20plays%20an%20early%20and%20crucial%20role%20in%20many%20NLP%0Asystems.%20This%20is%20commonly%20achieved%20by%20using%20rule-based%20or%20statistical%20methods%0Arelying%20on%20lexical%20features%20such%20as%20punctuation.%20Although%20some%20recent%20works%20no%0Alonger%20exclusively%20rely%20on%20punctuation%2C%20we%20find%20that%20no%20prior%20method%20achieves%0Aall%20of%20%28i%29%20robustness%20to%20missing%20punctuation%2C%20%28ii%29%20effective%20adaptability%20to%0Anew%20domains%2C%20and%20%28iii%29%20high%20efficiency.%20We%20introduce%20a%20new%20model%20-%20Segment%20any%0AText%20%28SaT%29%20-%20to%20solve%20this%20problem.%20To%20enhance%20robustness%2C%20we%20propose%20a%20new%0Apretraining%20scheme%20that%20ensures%20less%20reliance%20on%20punctuation.%20To%20address%0Aadaptability%2C%20we%20introduce%20an%20extra%20stage%20of%20parameter-efficient%20fine-tuning%2C%0Aestablishing%20state-of-the-art%20performance%20in%20distinct%20domains%20such%20as%20verses%0Afrom%20lyrics%20and%20legal%20documents.%20Along%20the%20way%2C%20we%20introduce%20architectural%0Amodifications%20that%20result%20in%20a%20threefold%20gain%20in%20speed%20over%20the%20previous%20state%0Aof%20the%20art%20and%20solve%20spurious%20reliance%20on%20context%20far%20in%20the%20future.%20Finally%2C%0Awe%20introduce%20a%20variant%20of%20our%20model%20with%20fine-tuning%20on%20a%20diverse%2C%20multilingual%0Amixture%20of%20sentence-segmented%20data%2C%20acting%20as%20a%20drop-in%20replacement%20and%0Aenhancement%20for%20existing%20segmentation%20tools.%20Overall%2C%20our%20contributions%20provide%0Aa%20universal%20approach%20for%20segmenting%20any%20text.%20Our%20method%20outperforms%20all%0Abaselines%20-%20including%20strong%20LLMs%20-%20across%208%20corpora%20spanning%20diverse%20domains%0Aand%20languages%2C%20especially%20in%20practically%20relevant%20situations%20where%20text%20is%0Apoorly%20formatted.%20Our%20models%20and%20code%2C%20including%20documentation%2C%20are%20available%0Aat%20https%3A//huggingface.co/segment-any-text%20under%20the%20MIT%20license.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520Any%2520Text%253A%2520A%2520Universal%2520Approach%2520for%2520Robust%252C%2520Efficient%2520and%250A%2520%2520Adaptable%2520Sentence%2520Segmentation%26entry.906535625%3DMarkus%2520Frohmann%2520and%2520Igor%2520Sterner%2520and%2520Ivan%2520Vuli%25C4%2587%2520and%2520Benjamin%2520Minixhofer%2520and%2520Markus%2520Schedl%26entry.1292438233%3D%2520%2520Segmenting%2520text%2520into%2520sentences%2520plays%2520an%2520early%2520and%2520crucial%2520role%2520in%2520many%2520NLP%250Asystems.%2520This%2520is%2520commonly%2520achieved%2520by%2520using%2520rule-based%2520or%2520statistical%2520methods%250Arelying%2520on%2520lexical%2520features%2520such%2520as%2520punctuation.%2520Although%2520some%2520recent%2520works%2520no%250Alonger%2520exclusively%2520rely%2520on%2520punctuation%252C%2520we%2520find%2520that%2520no%2520prior%2520method%2520achieves%250Aall%2520of%2520%2528i%2529%2520robustness%2520to%2520missing%2520punctuation%252C%2520%2528ii%2529%2520effective%2520adaptability%2520to%250Anew%2520domains%252C%2520and%2520%2528iii%2529%2520high%2520efficiency.%2520We%2520introduce%2520a%2520new%2520model%2520-%2520Segment%2520any%250AText%2520%2528SaT%2529%2520-%2520to%2520solve%2520this%2520problem.%2520To%2520enhance%2520robustness%252C%2520we%2520propose%2520a%2520new%250Apretraining%2520scheme%2520that%2520ensures%2520less%2520reliance%2520on%2520punctuation.%2520To%2520address%250Aadaptability%252C%2520we%2520introduce%2520an%2520extra%2520stage%2520of%2520parameter-efficient%2520fine-tuning%252C%250Aestablishing%2520state-of-the-art%2520performance%2520in%2520distinct%2520domains%2520such%2520as%2520verses%250Afrom%2520lyrics%2520and%2520legal%2520documents.%2520Along%2520the%2520way%252C%2520we%2520introduce%2520architectural%250Amodifications%2520that%2520result%2520in%2520a%2520threefold%2520gain%2520in%2520speed%2520over%2520the%2520previous%2520state%250Aof%2520the%2520art%2520and%2520solve%2520spurious%2520reliance%2520on%2520context%2520far%2520in%2520the%2520future.%2520Finally%252C%250Awe%2520introduce%2520a%2520variant%2520of%2520our%2520model%2520with%2520fine-tuning%2520on%2520a%2520diverse%252C%2520multilingual%250Amixture%2520of%2520sentence-segmented%2520data%252C%2520acting%2520as%2520a%2520drop-in%2520replacement%2520and%250Aenhancement%2520for%2520existing%2520segmentation%2520tools.%2520Overall%252C%2520our%2520contributions%2520provide%250Aa%2520universal%2520approach%2520for%2520segmenting%2520any%2520text.%2520Our%2520method%2520outperforms%2520all%250Abaselines%2520-%2520including%2520strong%2520LLMs%2520-%2520across%25208%2520corpora%2520spanning%2520diverse%2520domains%250Aand%2520languages%252C%2520especially%2520in%2520practically%2520relevant%2520situations%2520where%2520text%2520is%250Apoorly%2520formatted.%2520Our%2520models%2520and%2520code%252C%2520including%2520documentation%252C%2520are%2520available%250Aat%2520https%253A//huggingface.co/segment-any-text%2520under%2520the%2520MIT%2520license.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20Any%20Text%3A%20A%20Universal%20Approach%20for%20Robust%2C%20Efficient%20and%0A%20%20Adaptable%20Sentence%20Segmentation&entry.906535625=Markus%20Frohmann%20and%20Igor%20Sterner%20and%20Ivan%20Vuli%C4%87%20and%20Benjamin%20Minixhofer%20and%20Markus%20Schedl&entry.1292438233=%20%20Segmenting%20text%20into%20sentences%20plays%20an%20early%20and%20crucial%20role%20in%20many%20NLP%0Asystems.%20This%20is%20commonly%20achieved%20by%20using%20rule-based%20or%20statistical%20methods%0Arelying%20on%20lexical%20features%20such%20as%20punctuation.%20Although%20some%20recent%20works%20no%0Alonger%20exclusively%20rely%20on%20punctuation%2C%20we%20find%20that%20no%20prior%20method%20achieves%0Aall%20of%20%28i%29%20robustness%20to%20missing%20punctuation%2C%20%28ii%29%20effective%20adaptability%20to%0Anew%20domains%2C%20and%20%28iii%29%20high%20efficiency.%20We%20introduce%20a%20new%20model%20-%20Segment%20any%0AText%20%28SaT%29%20-%20to%20solve%20this%20problem.%20To%20enhance%20robustness%2C%20we%20propose%20a%20new%0Apretraining%20scheme%20that%20ensures%20less%20reliance%20on%20punctuation.%20To%20address%0Aadaptability%2C%20we%20introduce%20an%20extra%20stage%20of%20parameter-efficient%20fine-tuning%2C%0Aestablishing%20state-of-the-art%20performance%20in%20distinct%20domains%20such%20as%20verses%0Afrom%20lyrics%20and%20legal%20documents.%20Along%20the%20way%2C%20we%20introduce%20architectural%0Amodifications%20that%20result%20in%20a%20threefold%20gain%20in%20speed%20over%20the%20previous%20state%0Aof%20the%20art%20and%20solve%20spurious%20reliance%20on%20context%20far%20in%20the%20future.%20Finally%2C%0Awe%20introduce%20a%20variant%20of%20our%20model%20with%20fine-tuning%20on%20a%20diverse%2C%20multilingual%0Amixture%20of%20sentence-segmented%20data%2C%20acting%20as%20a%20drop-in%20replacement%20and%0Aenhancement%20for%20existing%20segmentation%20tools.%20Overall%2C%20our%20contributions%20provide%0Aa%20universal%20approach%20for%20segmenting%20any%20text.%20Our%20method%20outperforms%20all%0Abaselines%20-%20including%20strong%20LLMs%20-%20across%208%20corpora%20spanning%20diverse%20domains%0Aand%20languages%2C%20especially%20in%20practically%20relevant%20situations%20where%20text%20is%0Apoorly%20formatted.%20Our%20models%20and%20code%2C%20including%20documentation%2C%20are%20available%0Aat%20https%3A//huggingface.co/segment-any-text%20under%20the%20MIT%20license.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16678v1&entry.124074799=Read"},
{"title": "Deep Prompt Multi-task Network for Abuse Language Detection", "author": "Jian Zhu and Yuping Ruan and Jingfei Chang and Wenhui Sun and Hui Wan and Jian Long and Cheng Luo", "abstract": "  The detection of abusive language remains a long-standing challenge with the\nextensive use of social networks. The detection task of abusive language\nsuffers from limited accuracy. We argue that the existing detection methods\nutilize the fine-tuning technique of the pre-trained language models (PLMs) to\nhandle downstream tasks. Hence, these methods fail to stimulate the general\nknowledge of the PLMs. To address the problem, we propose a novel Deep Prompt\nMulti-task Network (DPMN) for abuse language detection. Specifically, DPMN\nfirst attempts to design two forms of deep prompt tuning and light prompt\ntuning for the PLMs. The effects of different prompt lengths, tuning\nstrategies, and prompt initialization methods on detecting abusive language are\nstudied. In addition, we propose a Task Head based on Bi-LSTM and FFN, which\ncan be used as a short text classifier. Eventually, DPMN utilizes multi-task\nlearning to improve detection metrics further. The multi-task network has the\nfunction of transferring effective knowledge. The proposed DPMN is evaluated\nagainst eight typical methods on three public datasets: OLID, SOLID, and\nAbuseAnalyzer. The experimental results show that our DPMN outperforms the\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2403.05268v2", "date": "2024-06-24", "relevancy": 2.4188, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5032}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4751}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Prompt%20Multi-task%20Network%20for%20Abuse%20Language%20Detection&body=Title%3A%20Deep%20Prompt%20Multi-task%20Network%20for%20Abuse%20Language%20Detection%0AAuthor%3A%20Jian%20Zhu%20and%20Yuping%20Ruan%20and%20Jingfei%20Chang%20and%20Wenhui%20Sun%20and%20Hui%20Wan%20and%20Jian%20Long%20and%20Cheng%20Luo%0AAbstract%3A%20%20%20The%20detection%20of%20abusive%20language%20remains%20a%20long-standing%20challenge%20with%20the%0Aextensive%20use%20of%20social%20networks.%20The%20detection%20task%20of%20abusive%20language%0Asuffers%20from%20limited%20accuracy.%20We%20argue%20that%20the%20existing%20detection%20methods%0Autilize%20the%20fine-tuning%20technique%20of%20the%20pre-trained%20language%20models%20%28PLMs%29%20to%0Ahandle%20downstream%20tasks.%20Hence%2C%20these%20methods%20fail%20to%20stimulate%20the%20general%0Aknowledge%20of%20the%20PLMs.%20To%20address%20the%20problem%2C%20we%20propose%20a%20novel%20Deep%20Prompt%0AMulti-task%20Network%20%28DPMN%29%20for%20abuse%20language%20detection.%20Specifically%2C%20DPMN%0Afirst%20attempts%20to%20design%20two%20forms%20of%20deep%20prompt%20tuning%20and%20light%20prompt%0Atuning%20for%20the%20PLMs.%20The%20effects%20of%20different%20prompt%20lengths%2C%20tuning%0Astrategies%2C%20and%20prompt%20initialization%20methods%20on%20detecting%20abusive%20language%20are%0Astudied.%20In%20addition%2C%20we%20propose%20a%20Task%20Head%20based%20on%20Bi-LSTM%20and%20FFN%2C%20which%0Acan%20be%20used%20as%20a%20short%20text%20classifier.%20Eventually%2C%20DPMN%20utilizes%20multi-task%0Alearning%20to%20improve%20detection%20metrics%20further.%20The%20multi-task%20network%20has%20the%0Afunction%20of%20transferring%20effective%20knowledge.%20The%20proposed%20DPMN%20is%20evaluated%0Aagainst%20eight%20typical%20methods%20on%20three%20public%20datasets%3A%20OLID%2C%20SOLID%2C%20and%0AAbuseAnalyzer.%20The%20experimental%20results%20show%20that%20our%20DPMN%20outperforms%20the%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05268v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Prompt%2520Multi-task%2520Network%2520for%2520Abuse%2520Language%2520Detection%26entry.906535625%3DJian%2520Zhu%2520and%2520Yuping%2520Ruan%2520and%2520Jingfei%2520Chang%2520and%2520Wenhui%2520Sun%2520and%2520Hui%2520Wan%2520and%2520Jian%2520Long%2520and%2520Cheng%2520Luo%26entry.1292438233%3D%2520%2520The%2520detection%2520of%2520abusive%2520language%2520remains%2520a%2520long-standing%2520challenge%2520with%2520the%250Aextensive%2520use%2520of%2520social%2520networks.%2520The%2520detection%2520task%2520of%2520abusive%2520language%250Asuffers%2520from%2520limited%2520accuracy.%2520We%2520argue%2520that%2520the%2520existing%2520detection%2520methods%250Autilize%2520the%2520fine-tuning%2520technique%2520of%2520the%2520pre-trained%2520language%2520models%2520%2528PLMs%2529%2520to%250Ahandle%2520downstream%2520tasks.%2520Hence%252C%2520these%2520methods%2520fail%2520to%2520stimulate%2520the%2520general%250Aknowledge%2520of%2520the%2520PLMs.%2520To%2520address%2520the%2520problem%252C%2520we%2520propose%2520a%2520novel%2520Deep%2520Prompt%250AMulti-task%2520Network%2520%2528DPMN%2529%2520for%2520abuse%2520language%2520detection.%2520Specifically%252C%2520DPMN%250Afirst%2520attempts%2520to%2520design%2520two%2520forms%2520of%2520deep%2520prompt%2520tuning%2520and%2520light%2520prompt%250Atuning%2520for%2520the%2520PLMs.%2520The%2520effects%2520of%2520different%2520prompt%2520lengths%252C%2520tuning%250Astrategies%252C%2520and%2520prompt%2520initialization%2520methods%2520on%2520detecting%2520abusive%2520language%2520are%250Astudied.%2520In%2520addition%252C%2520we%2520propose%2520a%2520Task%2520Head%2520based%2520on%2520Bi-LSTM%2520and%2520FFN%252C%2520which%250Acan%2520be%2520used%2520as%2520a%2520short%2520text%2520classifier.%2520Eventually%252C%2520DPMN%2520utilizes%2520multi-task%250Alearning%2520to%2520improve%2520detection%2520metrics%2520further.%2520The%2520multi-task%2520network%2520has%2520the%250Afunction%2520of%2520transferring%2520effective%2520knowledge.%2520The%2520proposed%2520DPMN%2520is%2520evaluated%250Aagainst%2520eight%2520typical%2520methods%2520on%2520three%2520public%2520datasets%253A%2520OLID%252C%2520SOLID%252C%2520and%250AAbuseAnalyzer.%2520The%2520experimental%2520results%2520show%2520that%2520our%2520DPMN%2520outperforms%2520the%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05268v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Prompt%20Multi-task%20Network%20for%20Abuse%20Language%20Detection&entry.906535625=Jian%20Zhu%20and%20Yuping%20Ruan%20and%20Jingfei%20Chang%20and%20Wenhui%20Sun%20and%20Hui%20Wan%20and%20Jian%20Long%20and%20Cheng%20Luo&entry.1292438233=%20%20The%20detection%20of%20abusive%20language%20remains%20a%20long-standing%20challenge%20with%20the%0Aextensive%20use%20of%20social%20networks.%20The%20detection%20task%20of%20abusive%20language%0Asuffers%20from%20limited%20accuracy.%20We%20argue%20that%20the%20existing%20detection%20methods%0Autilize%20the%20fine-tuning%20technique%20of%20the%20pre-trained%20language%20models%20%28PLMs%29%20to%0Ahandle%20downstream%20tasks.%20Hence%2C%20these%20methods%20fail%20to%20stimulate%20the%20general%0Aknowledge%20of%20the%20PLMs.%20To%20address%20the%20problem%2C%20we%20propose%20a%20novel%20Deep%20Prompt%0AMulti-task%20Network%20%28DPMN%29%20for%20abuse%20language%20detection.%20Specifically%2C%20DPMN%0Afirst%20attempts%20to%20design%20two%20forms%20of%20deep%20prompt%20tuning%20and%20light%20prompt%0Atuning%20for%20the%20PLMs.%20The%20effects%20of%20different%20prompt%20lengths%2C%20tuning%0Astrategies%2C%20and%20prompt%20initialization%20methods%20on%20detecting%20abusive%20language%20are%0Astudied.%20In%20addition%2C%20we%20propose%20a%20Task%20Head%20based%20on%20Bi-LSTM%20and%20FFN%2C%20which%0Acan%20be%20used%20as%20a%20short%20text%20classifier.%20Eventually%2C%20DPMN%20utilizes%20multi-task%0Alearning%20to%20improve%20detection%20metrics%20further.%20The%20multi-task%20network%20has%20the%0Afunction%20of%20transferring%20effective%20knowledge.%20The%20proposed%20DPMN%20is%20evaluated%0Aagainst%20eight%20typical%20methods%20on%20three%20public%20datasets%3A%20OLID%2C%20SOLID%2C%20and%0AAbuseAnalyzer.%20The%20experimental%20results%20show%20that%20our%20DPMN%20outperforms%20the%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05268v2&entry.124074799=Read"},
{"title": "GC-Bench: A Benchmark Framework for Graph Condensation with New Insights", "author": "Shengbo Gong and Juntong Ni and Noveen Sachdeva and Carl Yang and Wei Jin", "abstract": "  Graph condensation (GC) is an emerging technique designed to learn a\nsignificantly smaller graph that retains the essential information of the\noriginal graph. This condensed graph has shown promise in accelerating graph\nneural networks while preserving performance comparable to those achieved with\nthe original, larger graphs. Additionally, this technique facilitates\ndownstream applications such as neural architecture search and enhances our\nunderstanding of redundancy in large graphs. Despite the rapid development of\nGC methods, a systematic evaluation framework remains absent, which is\nnecessary to clarify the critical designs for particular evaluative aspects.\nFurthermore, several meaningful questions have not been investigated, such as\nwhether GC inherently preserves certain graph properties and offers robustness\neven without targeted design efforts. In this paper, we introduce GC-Bench, a\ncomprehensive framework to evaluate recent GC methods across multiple\ndimensions and to generate new insights. Our experimental findings provide a\ndeeper insights into the GC process and the characteristics of condensed\ngraphs, guiding future efforts in enhancing performance and exploring new\napplications. Our code is available at\n\\url{https://github.com/Emory-Melody/GraphSlim/tree/main/benchmark}.\n", "link": "http://arxiv.org/abs/2406.16715v1", "date": "2024-06-24", "relevancy": 2.3939, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4999}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.472}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GC-Bench%3A%20A%20Benchmark%20Framework%20for%20Graph%20Condensation%20with%20New%20Insights&body=Title%3A%20GC-Bench%3A%20A%20Benchmark%20Framework%20for%20Graph%20Condensation%20with%20New%20Insights%0AAuthor%3A%20Shengbo%20Gong%20and%20Juntong%20Ni%20and%20Noveen%20Sachdeva%20and%20Carl%20Yang%20and%20Wei%20Jin%0AAbstract%3A%20%20%20Graph%20condensation%20%28GC%29%20is%20an%20emerging%20technique%20designed%20to%20learn%20a%0Asignificantly%20smaller%20graph%20that%20retains%20the%20essential%20information%20of%20the%0Aoriginal%20graph.%20This%20condensed%20graph%20has%20shown%20promise%20in%20accelerating%20graph%0Aneural%20networks%20while%20preserving%20performance%20comparable%20to%20those%20achieved%20with%0Athe%20original%2C%20larger%20graphs.%20Additionally%2C%20this%20technique%20facilitates%0Adownstream%20applications%20such%20as%20neural%20architecture%20search%20and%20enhances%20our%0Aunderstanding%20of%20redundancy%20in%20large%20graphs.%20Despite%20the%20rapid%20development%20of%0AGC%20methods%2C%20a%20systematic%20evaluation%20framework%20remains%20absent%2C%20which%20is%0Anecessary%20to%20clarify%20the%20critical%20designs%20for%20particular%20evaluative%20aspects.%0AFurthermore%2C%20several%20meaningful%20questions%20have%20not%20been%20investigated%2C%20such%20as%0Awhether%20GC%20inherently%20preserves%20certain%20graph%20properties%20and%20offers%20robustness%0Aeven%20without%20targeted%20design%20efforts.%20In%20this%20paper%2C%20we%20introduce%20GC-Bench%2C%20a%0Acomprehensive%20framework%20to%20evaluate%20recent%20GC%20methods%20across%20multiple%0Adimensions%20and%20to%20generate%20new%20insights.%20Our%20experimental%20findings%20provide%20a%0Adeeper%20insights%20into%20the%20GC%20process%20and%20the%20characteristics%20of%20condensed%0Agraphs%2C%20guiding%20future%20efforts%20in%20enhancing%20performance%20and%20exploring%20new%0Aapplications.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Emory-Melody/GraphSlim/tree/main/benchmark%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGC-Bench%253A%2520A%2520Benchmark%2520Framework%2520for%2520Graph%2520Condensation%2520with%2520New%2520Insights%26entry.906535625%3DShengbo%2520Gong%2520and%2520Juntong%2520Ni%2520and%2520Noveen%2520Sachdeva%2520and%2520Carl%2520Yang%2520and%2520Wei%2520Jin%26entry.1292438233%3D%2520%2520Graph%2520condensation%2520%2528GC%2529%2520is%2520an%2520emerging%2520technique%2520designed%2520to%2520learn%2520a%250Asignificantly%2520smaller%2520graph%2520that%2520retains%2520the%2520essential%2520information%2520of%2520the%250Aoriginal%2520graph.%2520This%2520condensed%2520graph%2520has%2520shown%2520promise%2520in%2520accelerating%2520graph%250Aneural%2520networks%2520while%2520preserving%2520performance%2520comparable%2520to%2520those%2520achieved%2520with%250Athe%2520original%252C%2520larger%2520graphs.%2520Additionally%252C%2520this%2520technique%2520facilitates%250Adownstream%2520applications%2520such%2520as%2520neural%2520architecture%2520search%2520and%2520enhances%2520our%250Aunderstanding%2520of%2520redundancy%2520in%2520large%2520graphs.%2520Despite%2520the%2520rapid%2520development%2520of%250AGC%2520methods%252C%2520a%2520systematic%2520evaluation%2520framework%2520remains%2520absent%252C%2520which%2520is%250Anecessary%2520to%2520clarify%2520the%2520critical%2520designs%2520for%2520particular%2520evaluative%2520aspects.%250AFurthermore%252C%2520several%2520meaningful%2520questions%2520have%2520not%2520been%2520investigated%252C%2520such%2520as%250Awhether%2520GC%2520inherently%2520preserves%2520certain%2520graph%2520properties%2520and%2520offers%2520robustness%250Aeven%2520without%2520targeted%2520design%2520efforts.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520GC-Bench%252C%2520a%250Acomprehensive%2520framework%2520to%2520evaluate%2520recent%2520GC%2520methods%2520across%2520multiple%250Adimensions%2520and%2520to%2520generate%2520new%2520insights.%2520Our%2520experimental%2520findings%2520provide%2520a%250Adeeper%2520insights%2520into%2520the%2520GC%2520process%2520and%2520the%2520characteristics%2520of%2520condensed%250Agraphs%252C%2520guiding%2520future%2520efforts%2520in%2520enhancing%2520performance%2520and%2520exploring%2520new%250Aapplications.%2520Our%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/Emory-Melody/GraphSlim/tree/main/benchmark%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GC-Bench%3A%20A%20Benchmark%20Framework%20for%20Graph%20Condensation%20with%20New%20Insights&entry.906535625=Shengbo%20Gong%20and%20Juntong%20Ni%20and%20Noveen%20Sachdeva%20and%20Carl%20Yang%20and%20Wei%20Jin&entry.1292438233=%20%20Graph%20condensation%20%28GC%29%20is%20an%20emerging%20technique%20designed%20to%20learn%20a%0Asignificantly%20smaller%20graph%20that%20retains%20the%20essential%20information%20of%20the%0Aoriginal%20graph.%20This%20condensed%20graph%20has%20shown%20promise%20in%20accelerating%20graph%0Aneural%20networks%20while%20preserving%20performance%20comparable%20to%20those%20achieved%20with%0Athe%20original%2C%20larger%20graphs.%20Additionally%2C%20this%20technique%20facilitates%0Adownstream%20applications%20such%20as%20neural%20architecture%20search%20and%20enhances%20our%0Aunderstanding%20of%20redundancy%20in%20large%20graphs.%20Despite%20the%20rapid%20development%20of%0AGC%20methods%2C%20a%20systematic%20evaluation%20framework%20remains%20absent%2C%20which%20is%0Anecessary%20to%20clarify%20the%20critical%20designs%20for%20particular%20evaluative%20aspects.%0AFurthermore%2C%20several%20meaningful%20questions%20have%20not%20been%20investigated%2C%20such%20as%0Awhether%20GC%20inherently%20preserves%20certain%20graph%20properties%20and%20offers%20robustness%0Aeven%20without%20targeted%20design%20efforts.%20In%20this%20paper%2C%20we%20introduce%20GC-Bench%2C%20a%0Acomprehensive%20framework%20to%20evaluate%20recent%20GC%20methods%20across%20multiple%0Adimensions%20and%20to%20generate%20new%20insights.%20Our%20experimental%20findings%20provide%20a%0Adeeper%20insights%20into%20the%20GC%20process%20and%20the%20characteristics%20of%20condensed%0Agraphs%2C%20guiding%20future%20efforts%20in%20enhancing%20performance%20and%20exploring%20new%0Aapplications.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Emory-Melody/GraphSlim/tree/main/benchmark%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16715v1&entry.124074799=Read"},
{"title": "Token-based Decision Criteria Are Suboptimal in In-context Learning", "author": "Hakaze Cho and Yoshihiro Sakai and Mariko Kato and Kenshiro Tanaka and Akira Ishii and Naoya Inoue", "abstract": "  In-Context Learning (ICL) typically utilizes classification criteria from\nprobabilities of manually selected label tokens. However, we argue that such\ntoken-based classification criteria lead to suboptimal decision boundaries,\ndespite delicate calibrations through translation and constrained rotation. To\naddress this problem, we propose Hidden Calibration, which renounces token\nprobabilities and uses the nearest centroid classifier on the LM's last hidden\nstates. In detail, we use the nearest centroid classification on the hidden\nstates, assigning the category of the nearest centroid previously observed from\na few-shot calibration set to the test sample as the predicted label. Our\nexperiments on 3 models and 10 classification datasets indicate that Hidden\nCalibration consistently outperforms current token-based calibrations by about\n20%. Our further analysis demonstrates that Hidden Calibration finds better\nclassification criteria with less inter-categories overlap, and LMs provide\nlinearly separable intra-category clusters with the help of demonstrations,\nwhich supports Hidden Calibration and gives new insights into the conventional\nICL.\n", "link": "http://arxiv.org/abs/2406.16535v1", "date": "2024-06-24", "relevancy": 2.3925, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4875}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4777}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token-based%20Decision%20Criteria%20Are%20Suboptimal%20in%20In-context%20Learning&body=Title%3A%20Token-based%20Decision%20Criteria%20Are%20Suboptimal%20in%20In-context%20Learning%0AAuthor%3A%20Hakaze%20Cho%20and%20Yoshihiro%20Sakai%20and%20Mariko%20Kato%20and%20Kenshiro%20Tanaka%20and%20Akira%20Ishii%20and%20Naoya%20Inoue%0AAbstract%3A%20%20%20In-Context%20Learning%20%28ICL%29%20typically%20utilizes%20classification%20criteria%20from%0Aprobabilities%20of%20manually%20selected%20label%20tokens.%20However%2C%20we%20argue%20that%20such%0Atoken-based%20classification%20criteria%20lead%20to%20suboptimal%20decision%20boundaries%2C%0Adespite%20delicate%20calibrations%20through%20translation%20and%20constrained%20rotation.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20Hidden%20Calibration%2C%20which%20renounces%20token%0Aprobabilities%20and%20uses%20the%20nearest%20centroid%20classifier%20on%20the%20LM%27s%20last%20hidden%0Astates.%20In%20detail%2C%20we%20use%20the%20nearest%20centroid%20classification%20on%20the%20hidden%0Astates%2C%20assigning%20the%20category%20of%20the%20nearest%20centroid%20previously%20observed%20from%0Aa%20few-shot%20calibration%20set%20to%20the%20test%20sample%20as%20the%20predicted%20label.%20Our%0Aexperiments%20on%203%20models%20and%2010%20classification%20datasets%20indicate%20that%20Hidden%0ACalibration%20consistently%20outperforms%20current%20token-based%20calibrations%20by%20about%0A20%25.%20Our%20further%20analysis%20demonstrates%20that%20Hidden%20Calibration%20finds%20better%0Aclassification%20criteria%20with%20less%20inter-categories%20overlap%2C%20and%20LMs%20provide%0Alinearly%20separable%20intra-category%20clusters%20with%20the%20help%20of%20demonstrations%2C%0Awhich%20supports%20Hidden%20Calibration%20and%20gives%20new%20insights%20into%20the%20conventional%0AICL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken-based%2520Decision%2520Criteria%2520Are%2520Suboptimal%2520in%2520In-context%2520Learning%26entry.906535625%3DHakaze%2520Cho%2520and%2520Yoshihiro%2520Sakai%2520and%2520Mariko%2520Kato%2520and%2520Kenshiro%2520Tanaka%2520and%2520Akira%2520Ishii%2520and%2520Naoya%2520Inoue%26entry.1292438233%3D%2520%2520In-Context%2520Learning%2520%2528ICL%2529%2520typically%2520utilizes%2520classification%2520criteria%2520from%250Aprobabilities%2520of%2520manually%2520selected%2520label%2520tokens.%2520However%252C%2520we%2520argue%2520that%2520such%250Atoken-based%2520classification%2520criteria%2520lead%2520to%2520suboptimal%2520decision%2520boundaries%252C%250Adespite%2520delicate%2520calibrations%2520through%2520translation%2520and%2520constrained%2520rotation.%2520To%250Aaddress%2520this%2520problem%252C%2520we%2520propose%2520Hidden%2520Calibration%252C%2520which%2520renounces%2520token%250Aprobabilities%2520and%2520uses%2520the%2520nearest%2520centroid%2520classifier%2520on%2520the%2520LM%2527s%2520last%2520hidden%250Astates.%2520In%2520detail%252C%2520we%2520use%2520the%2520nearest%2520centroid%2520classification%2520on%2520the%2520hidden%250Astates%252C%2520assigning%2520the%2520category%2520of%2520the%2520nearest%2520centroid%2520previously%2520observed%2520from%250Aa%2520few-shot%2520calibration%2520set%2520to%2520the%2520test%2520sample%2520as%2520the%2520predicted%2520label.%2520Our%250Aexperiments%2520on%25203%2520models%2520and%252010%2520classification%2520datasets%2520indicate%2520that%2520Hidden%250ACalibration%2520consistently%2520outperforms%2520current%2520token-based%2520calibrations%2520by%2520about%250A20%2525.%2520Our%2520further%2520analysis%2520demonstrates%2520that%2520Hidden%2520Calibration%2520finds%2520better%250Aclassification%2520criteria%2520with%2520less%2520inter-categories%2520overlap%252C%2520and%2520LMs%2520provide%250Alinearly%2520separable%2520intra-category%2520clusters%2520with%2520the%2520help%2520of%2520demonstrations%252C%250Awhich%2520supports%2520Hidden%2520Calibration%2520and%2520gives%2520new%2520insights%2520into%2520the%2520conventional%250AICL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token-based%20Decision%20Criteria%20Are%20Suboptimal%20in%20In-context%20Learning&entry.906535625=Hakaze%20Cho%20and%20Yoshihiro%20Sakai%20and%20Mariko%20Kato%20and%20Kenshiro%20Tanaka%20and%20Akira%20Ishii%20and%20Naoya%20Inoue&entry.1292438233=%20%20In-Context%20Learning%20%28ICL%29%20typically%20utilizes%20classification%20criteria%20from%0Aprobabilities%20of%20manually%20selected%20label%20tokens.%20However%2C%20we%20argue%20that%20such%0Atoken-based%20classification%20criteria%20lead%20to%20suboptimal%20decision%20boundaries%2C%0Adespite%20delicate%20calibrations%20through%20translation%20and%20constrained%20rotation.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20Hidden%20Calibration%2C%20which%20renounces%20token%0Aprobabilities%20and%20uses%20the%20nearest%20centroid%20classifier%20on%20the%20LM%27s%20last%20hidden%0Astates.%20In%20detail%2C%20we%20use%20the%20nearest%20centroid%20classification%20on%20the%20hidden%0Astates%2C%20assigning%20the%20category%20of%20the%20nearest%20centroid%20previously%20observed%20from%0Aa%20few-shot%20calibration%20set%20to%20the%20test%20sample%20as%20the%20predicted%20label.%20Our%0Aexperiments%20on%203%20models%20and%2010%20classification%20datasets%20indicate%20that%20Hidden%0ACalibration%20consistently%20outperforms%20current%20token-based%20calibrations%20by%20about%0A20%25.%20Our%20further%20analysis%20demonstrates%20that%20Hidden%20Calibration%20finds%20better%0Aclassification%20criteria%20with%20less%20inter-categories%20overlap%2C%20and%20LMs%20provide%0Alinearly%20separable%20intra-category%20clusters%20with%20the%20help%20of%20demonstrations%2C%0Awhich%20supports%20Hidden%20Calibration%20and%20gives%20new%20insights%20into%20the%20conventional%0AICL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16535v1&entry.124074799=Read"},
{"title": "MAP-NBV: Multi-agent Prediction-guided Next-Best-View Planning for\n  Active 3D Object Reconstruction", "author": "Harnaik Dhami and Vishnu D. Sharma and Pratap Tokekar", "abstract": "  Next-Best View (NBV) planning is a long-standing problem of determining where\nto obtain the next best view of an object from, by a robot that is viewing the\nobject. There are a number of methods for choosing NBV based on the observed\npart of the object. In this paper, we investigate how predicting the unobserved\npart helps with the efficiency of reconstructing the object. We present,\nMulti-Agent Prediction-Guided NBV (MAP-NBV), a decentralized coordination\nalgorithm for active 3D reconstruction with multi-agent systems.\nPrediction-based approaches have shown great improvement in active perception\ntasks by learning the cues about structures in the environment from data.\nHowever, these methods primarily focus on single-agent systems. We design a\ndecentralized next-best-view approach that utilizes geometric measures over the\npredictions and jointly optimizes the information gain and control effort for\nefficient collaborative 3D reconstruction of the object. Our method achieves\n19% improvement over the non-predictive multi-agent approach in simulations\nusing AirSim and ShapeNet. We make our code publicly available through our\nproject website: http://raaslab.org/projects/MAPNBV/.\n", "link": "http://arxiv.org/abs/2307.04004v3", "date": "2024-06-24", "relevancy": 2.3467, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6287}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.587}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAP-NBV%3A%20Multi-agent%20Prediction-guided%20Next-Best-View%20Planning%20for%0A%20%20Active%203D%20Object%20Reconstruction&body=Title%3A%20MAP-NBV%3A%20Multi-agent%20Prediction-guided%20Next-Best-View%20Planning%20for%0A%20%20Active%203D%20Object%20Reconstruction%0AAuthor%3A%20Harnaik%20Dhami%20and%20Vishnu%20D.%20Sharma%20and%20Pratap%20Tokekar%0AAbstract%3A%20%20%20Next-Best%20View%20%28NBV%29%20planning%20is%20a%20long-standing%20problem%20of%20determining%20where%0Ato%20obtain%20the%20next%20best%20view%20of%20an%20object%20from%2C%20by%20a%20robot%20that%20is%20viewing%20the%0Aobject.%20There%20are%20a%20number%20of%20methods%20for%20choosing%20NBV%20based%20on%20the%20observed%0Apart%20of%20the%20object.%20In%20this%20paper%2C%20we%20investigate%20how%20predicting%20the%20unobserved%0Apart%20helps%20with%20the%20efficiency%20of%20reconstructing%20the%20object.%20We%20present%2C%0AMulti-Agent%20Prediction-Guided%20NBV%20%28MAP-NBV%29%2C%20a%20decentralized%20coordination%0Aalgorithm%20for%20active%203D%20reconstruction%20with%20multi-agent%20systems.%0APrediction-based%20approaches%20have%20shown%20great%20improvement%20in%20active%20perception%0Atasks%20by%20learning%20the%20cues%20about%20structures%20in%20the%20environment%20from%20data.%0AHowever%2C%20these%20methods%20primarily%20focus%20on%20single-agent%20systems.%20We%20design%20a%0Adecentralized%20next-best-view%20approach%20that%20utilizes%20geometric%20measures%20over%20the%0Apredictions%20and%20jointly%20optimizes%20the%20information%20gain%20and%20control%20effort%20for%0Aefficient%20collaborative%203D%20reconstruction%20of%20the%20object.%20Our%20method%20achieves%0A19%25%20improvement%20over%20the%20non-predictive%20multi-agent%20approach%20in%20simulations%0Ausing%20AirSim%20and%20ShapeNet.%20We%20make%20our%20code%20publicly%20available%20through%20our%0Aproject%20website%3A%20http%3A//raaslab.org/projects/MAPNBV/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.04004v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAP-NBV%253A%2520Multi-agent%2520Prediction-guided%2520Next-Best-View%2520Planning%2520for%250A%2520%2520Active%25203D%2520Object%2520Reconstruction%26entry.906535625%3DHarnaik%2520Dhami%2520and%2520Vishnu%2520D.%2520Sharma%2520and%2520Pratap%2520Tokekar%26entry.1292438233%3D%2520%2520Next-Best%2520View%2520%2528NBV%2529%2520planning%2520is%2520a%2520long-standing%2520problem%2520of%2520determining%2520where%250Ato%2520obtain%2520the%2520next%2520best%2520view%2520of%2520an%2520object%2520from%252C%2520by%2520a%2520robot%2520that%2520is%2520viewing%2520the%250Aobject.%2520There%2520are%2520a%2520number%2520of%2520methods%2520for%2520choosing%2520NBV%2520based%2520on%2520the%2520observed%250Apart%2520of%2520the%2520object.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520how%2520predicting%2520the%2520unobserved%250Apart%2520helps%2520with%2520the%2520efficiency%2520of%2520reconstructing%2520the%2520object.%2520We%2520present%252C%250AMulti-Agent%2520Prediction-Guided%2520NBV%2520%2528MAP-NBV%2529%252C%2520a%2520decentralized%2520coordination%250Aalgorithm%2520for%2520active%25203D%2520reconstruction%2520with%2520multi-agent%2520systems.%250APrediction-based%2520approaches%2520have%2520shown%2520great%2520improvement%2520in%2520active%2520perception%250Atasks%2520by%2520learning%2520the%2520cues%2520about%2520structures%2520in%2520the%2520environment%2520from%2520data.%250AHowever%252C%2520these%2520methods%2520primarily%2520focus%2520on%2520single-agent%2520systems.%2520We%2520design%2520a%250Adecentralized%2520next-best-view%2520approach%2520that%2520utilizes%2520geometric%2520measures%2520over%2520the%250Apredictions%2520and%2520jointly%2520optimizes%2520the%2520information%2520gain%2520and%2520control%2520effort%2520for%250Aefficient%2520collaborative%25203D%2520reconstruction%2520of%2520the%2520object.%2520Our%2520method%2520achieves%250A19%2525%2520improvement%2520over%2520the%2520non-predictive%2520multi-agent%2520approach%2520in%2520simulations%250Ausing%2520AirSim%2520and%2520ShapeNet.%2520We%2520make%2520our%2520code%2520publicly%2520available%2520through%2520our%250Aproject%2520website%253A%2520http%253A//raaslab.org/projects/MAPNBV/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.04004v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAP-NBV%3A%20Multi-agent%20Prediction-guided%20Next-Best-View%20Planning%20for%0A%20%20Active%203D%20Object%20Reconstruction&entry.906535625=Harnaik%20Dhami%20and%20Vishnu%20D.%20Sharma%20and%20Pratap%20Tokekar&entry.1292438233=%20%20Next-Best%20View%20%28NBV%29%20planning%20is%20a%20long-standing%20problem%20of%20determining%20where%0Ato%20obtain%20the%20next%20best%20view%20of%20an%20object%20from%2C%20by%20a%20robot%20that%20is%20viewing%20the%0Aobject.%20There%20are%20a%20number%20of%20methods%20for%20choosing%20NBV%20based%20on%20the%20observed%0Apart%20of%20the%20object.%20In%20this%20paper%2C%20we%20investigate%20how%20predicting%20the%20unobserved%0Apart%20helps%20with%20the%20efficiency%20of%20reconstructing%20the%20object.%20We%20present%2C%0AMulti-Agent%20Prediction-Guided%20NBV%20%28MAP-NBV%29%2C%20a%20decentralized%20coordination%0Aalgorithm%20for%20active%203D%20reconstruction%20with%20multi-agent%20systems.%0APrediction-based%20approaches%20have%20shown%20great%20improvement%20in%20active%20perception%0Atasks%20by%20learning%20the%20cues%20about%20structures%20in%20the%20environment%20from%20data.%0AHowever%2C%20these%20methods%20primarily%20focus%20on%20single-agent%20systems.%20We%20design%20a%0Adecentralized%20next-best-view%20approach%20that%20utilizes%20geometric%20measures%20over%20the%0Apredictions%20and%20jointly%20optimizes%20the%20information%20gain%20and%20control%20effort%20for%0Aefficient%20collaborative%203D%20reconstruction%20of%20the%20object.%20Our%20method%20achieves%0A19%25%20improvement%20over%20the%20non-predictive%20multi-agent%20approach%20in%20simulations%0Ausing%20AirSim%20and%20ShapeNet.%20We%20make%20our%20code%20publicly%20available%20through%20our%0Aproject%20website%3A%20http%3A//raaslab.org/projects/MAPNBV/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.04004v3&entry.124074799=Read"},
{"title": "Multi-Robot Collaborative Localization and Planning with Inter-Ranging", "author": "Derek Knowles and Adam Dai and Grace Gao", "abstract": "  Robots often use feature-based image tracking to identify their position in\ntheir surrounding environment; however, feature-based image tracking is prone\nto errors in low-textured and poorly lit environments. Specifically, we\ninvestigate a scenario where robots are tasked with exploring the surface of\nthe Moon and are required to have an accurate estimate of their position to be\nable to correctly geotag scientific measurements. To reduce localization error,\nwe complement traditional feature-based image tracking with ultra-wideband\n(UWB) distance measurements between the robots. The robots use an advanced\nmesh-ranging protocol that allows them to continuously share distance\nmeasurements amongst each other rather than relying on the common \"anchor\" and\n\"tag\" UWB architecture. We develop a decentralized multi-robot coordination\nalgorithm that actively plans paths based on measurement line-of-sight vectors\namongst all robots to minimize collective localization error. We then\ndemonstrate the emergent behavior of the proposed multi-robot coordination\nalgorithm both in simulation and hardware to lower a geometry-based uncertainty\nmetric and reduce localization error.\n", "link": "http://arxiv.org/abs/2406.16679v1", "date": "2024-06-24", "relevancy": 2.3322, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6238}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.587}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Robot%20Collaborative%20Localization%20and%20Planning%20with%20Inter-Ranging&body=Title%3A%20Multi-Robot%20Collaborative%20Localization%20and%20Planning%20with%20Inter-Ranging%0AAuthor%3A%20Derek%20Knowles%20and%20Adam%20Dai%20and%20Grace%20Gao%0AAbstract%3A%20%20%20Robots%20often%20use%20feature-based%20image%20tracking%20to%20identify%20their%20position%20in%0Atheir%20surrounding%20environment%3B%20however%2C%20feature-based%20image%20tracking%20is%20prone%0Ato%20errors%20in%20low-textured%20and%20poorly%20lit%20environments.%20Specifically%2C%20we%0Ainvestigate%20a%20scenario%20where%20robots%20are%20tasked%20with%20exploring%20the%20surface%20of%0Athe%20Moon%20and%20are%20required%20to%20have%20an%20accurate%20estimate%20of%20their%20position%20to%20be%0Aable%20to%20correctly%20geotag%20scientific%20measurements.%20To%20reduce%20localization%20error%2C%0Awe%20complement%20traditional%20feature-based%20image%20tracking%20with%20ultra-wideband%0A%28UWB%29%20distance%20measurements%20between%20the%20robots.%20The%20robots%20use%20an%20advanced%0Amesh-ranging%20protocol%20that%20allows%20them%20to%20continuously%20share%20distance%0Ameasurements%20amongst%20each%20other%20rather%20than%20relying%20on%20the%20common%20%22anchor%22%20and%0A%22tag%22%20UWB%20architecture.%20We%20develop%20a%20decentralized%20multi-robot%20coordination%0Aalgorithm%20that%20actively%20plans%20paths%20based%20on%20measurement%20line-of-sight%20vectors%0Aamongst%20all%20robots%20to%20minimize%20collective%20localization%20error.%20We%20then%0Ademonstrate%20the%20emergent%20behavior%20of%20the%20proposed%20multi-robot%20coordination%0Aalgorithm%20both%20in%20simulation%20and%20hardware%20to%20lower%20a%20geometry-based%20uncertainty%0Ametric%20and%20reduce%20localization%20error.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Robot%2520Collaborative%2520Localization%2520and%2520Planning%2520with%2520Inter-Ranging%26entry.906535625%3DDerek%2520Knowles%2520and%2520Adam%2520Dai%2520and%2520Grace%2520Gao%26entry.1292438233%3D%2520%2520Robots%2520often%2520use%2520feature-based%2520image%2520tracking%2520to%2520identify%2520their%2520position%2520in%250Atheir%2520surrounding%2520environment%253B%2520however%252C%2520feature-based%2520image%2520tracking%2520is%2520prone%250Ato%2520errors%2520in%2520low-textured%2520and%2520poorly%2520lit%2520environments.%2520Specifically%252C%2520we%250Ainvestigate%2520a%2520scenario%2520where%2520robots%2520are%2520tasked%2520with%2520exploring%2520the%2520surface%2520of%250Athe%2520Moon%2520and%2520are%2520required%2520to%2520have%2520an%2520accurate%2520estimate%2520of%2520their%2520position%2520to%2520be%250Aable%2520to%2520correctly%2520geotag%2520scientific%2520measurements.%2520To%2520reduce%2520localization%2520error%252C%250Awe%2520complement%2520traditional%2520feature-based%2520image%2520tracking%2520with%2520ultra-wideband%250A%2528UWB%2529%2520distance%2520measurements%2520between%2520the%2520robots.%2520The%2520robots%2520use%2520an%2520advanced%250Amesh-ranging%2520protocol%2520that%2520allows%2520them%2520to%2520continuously%2520share%2520distance%250Ameasurements%2520amongst%2520each%2520other%2520rather%2520than%2520relying%2520on%2520the%2520common%2520%2522anchor%2522%2520and%250A%2522tag%2522%2520UWB%2520architecture.%2520We%2520develop%2520a%2520decentralized%2520multi-robot%2520coordination%250Aalgorithm%2520that%2520actively%2520plans%2520paths%2520based%2520on%2520measurement%2520line-of-sight%2520vectors%250Aamongst%2520all%2520robots%2520to%2520minimize%2520collective%2520localization%2520error.%2520We%2520then%250Ademonstrate%2520the%2520emergent%2520behavior%2520of%2520the%2520proposed%2520multi-robot%2520coordination%250Aalgorithm%2520both%2520in%2520simulation%2520and%2520hardware%2520to%2520lower%2520a%2520geometry-based%2520uncertainty%250Ametric%2520and%2520reduce%2520localization%2520error.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Robot%20Collaborative%20Localization%20and%20Planning%20with%20Inter-Ranging&entry.906535625=Derek%20Knowles%20and%20Adam%20Dai%20and%20Grace%20Gao&entry.1292438233=%20%20Robots%20often%20use%20feature-based%20image%20tracking%20to%20identify%20their%20position%20in%0Atheir%20surrounding%20environment%3B%20however%2C%20feature-based%20image%20tracking%20is%20prone%0Ato%20errors%20in%20low-textured%20and%20poorly%20lit%20environments.%20Specifically%2C%20we%0Ainvestigate%20a%20scenario%20where%20robots%20are%20tasked%20with%20exploring%20the%20surface%20of%0Athe%20Moon%20and%20are%20required%20to%20have%20an%20accurate%20estimate%20of%20their%20position%20to%20be%0Aable%20to%20correctly%20geotag%20scientific%20measurements.%20To%20reduce%20localization%20error%2C%0Awe%20complement%20traditional%20feature-based%20image%20tracking%20with%20ultra-wideband%0A%28UWB%29%20distance%20measurements%20between%20the%20robots.%20The%20robots%20use%20an%20advanced%0Amesh-ranging%20protocol%20that%20allows%20them%20to%20continuously%20share%20distance%0Ameasurements%20amongst%20each%20other%20rather%20than%20relying%20on%20the%20common%20%22anchor%22%20and%0A%22tag%22%20UWB%20architecture.%20We%20develop%20a%20decentralized%20multi-robot%20coordination%0Aalgorithm%20that%20actively%20plans%20paths%20based%20on%20measurement%20line-of-sight%20vectors%0Aamongst%20all%20robots%20to%20minimize%20collective%20localization%20error.%20We%20then%0Ademonstrate%20the%20emergent%20behavior%20of%20the%20proposed%20multi-robot%20coordination%0Aalgorithm%20both%20in%20simulation%20and%20hardware%20to%20lower%20a%20geometry-based%20uncertainty%0Ametric%20and%20reduce%20localization%20error.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16679v1&entry.124074799=Read"},
{"title": "ShanghaiTech Mapping Robot is All You Need: Robot System for Collecting\n  Universal Ground Vehicle Datasets", "author": "Bowen Xu and Xiting Zhao and Delin Feng and Yuanyuan Yang and S\u00f6ren Schwertfeger", "abstract": "  This paper presents the ShanghaiTech Mapping Robot, a state-of-the-art\nunmanned ground vehicle (UGV) designed for collecting comprehensive\nmulti-sensor datasets to support research in robotics, computer vision, and\nautonomous driving. The robot is equipped with a wide array of sensors\nincluding RGB cameras, RGB-D cameras, event-based cameras, IR cameras, LiDARs,\nmmWave radars, IMUs, ultrasonic range finders, and a GNSS RTK receiver. The\nsensor suite is integrated onto a specially designed mechanical structure with\na centralized power system and a synchronization mechanism to ensure spatial\nand temporal alignment of the sensor data. A 16-node on-board computing cluster\nhandles sensor control, data collection, and storage. We describe the hardware\nand software architecture of the robot in detail and discuss the calibration\nprocedures for the various sensors. The capabilities of the platform are\ndemonstrated through an extensive dataset collected in diverse real-world\nenvironments. To facilitate research, we make the dataset publicly available\nalong with the associated robot sensor calibration data. Performance\nevaluations on a set of standard perception and localization tasks showcase the\npotential of the dataset to support developments in Robot Autonomy.\n", "link": "http://arxiv.org/abs/2406.16713v1", "date": "2024-06-24", "relevancy": 2.3228, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5924}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5757}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShanghaiTech%20Mapping%20Robot%20is%20All%20You%20Need%3A%20Robot%20System%20for%20Collecting%0A%20%20Universal%20Ground%20Vehicle%20Datasets&body=Title%3A%20ShanghaiTech%20Mapping%20Robot%20is%20All%20You%20Need%3A%20Robot%20System%20for%20Collecting%0A%20%20Universal%20Ground%20Vehicle%20Datasets%0AAuthor%3A%20Bowen%20Xu%20and%20Xiting%20Zhao%20and%20Delin%20Feng%20and%20Yuanyuan%20Yang%20and%20S%C3%B6ren%20Schwertfeger%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20ShanghaiTech%20Mapping%20Robot%2C%20a%20state-of-the-art%0Aunmanned%20ground%20vehicle%20%28UGV%29%20designed%20for%20collecting%20comprehensive%0Amulti-sensor%20datasets%20to%20support%20research%20in%20robotics%2C%20computer%20vision%2C%20and%0Aautonomous%20driving.%20The%20robot%20is%20equipped%20with%20a%20wide%20array%20of%20sensors%0Aincluding%20RGB%20cameras%2C%20RGB-D%20cameras%2C%20event-based%20cameras%2C%20IR%20cameras%2C%20LiDARs%2C%0AmmWave%20radars%2C%20IMUs%2C%20ultrasonic%20range%20finders%2C%20and%20a%20GNSS%20RTK%20receiver.%20The%0Asensor%20suite%20is%20integrated%20onto%20a%20specially%20designed%20mechanical%20structure%20with%0Aa%20centralized%20power%20system%20and%20a%20synchronization%20mechanism%20to%20ensure%20spatial%0Aand%20temporal%20alignment%20of%20the%20sensor%20data.%20A%2016-node%20on-board%20computing%20cluster%0Ahandles%20sensor%20control%2C%20data%20collection%2C%20and%20storage.%20We%20describe%20the%20hardware%0Aand%20software%20architecture%20of%20the%20robot%20in%20detail%20and%20discuss%20the%20calibration%0Aprocedures%20for%20the%20various%20sensors.%20The%20capabilities%20of%20the%20platform%20are%0Ademonstrated%20through%20an%20extensive%20dataset%20collected%20in%20diverse%20real-world%0Aenvironments.%20To%20facilitate%20research%2C%20we%20make%20the%20dataset%20publicly%20available%0Aalong%20with%20the%20associated%20robot%20sensor%20calibration%20data.%20Performance%0Aevaluations%20on%20a%20set%20of%20standard%20perception%20and%20localization%20tasks%20showcase%20the%0Apotential%20of%20the%20dataset%20to%20support%20developments%20in%20Robot%20Autonomy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShanghaiTech%2520Mapping%2520Robot%2520is%2520All%2520You%2520Need%253A%2520Robot%2520System%2520for%2520Collecting%250A%2520%2520Universal%2520Ground%2520Vehicle%2520Datasets%26entry.906535625%3DBowen%2520Xu%2520and%2520Xiting%2520Zhao%2520and%2520Delin%2520Feng%2520and%2520Yuanyuan%2520Yang%2520and%2520S%25C3%25B6ren%2520Schwertfeger%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520ShanghaiTech%2520Mapping%2520Robot%252C%2520a%2520state-of-the-art%250Aunmanned%2520ground%2520vehicle%2520%2528UGV%2529%2520designed%2520for%2520collecting%2520comprehensive%250Amulti-sensor%2520datasets%2520to%2520support%2520research%2520in%2520robotics%252C%2520computer%2520vision%252C%2520and%250Aautonomous%2520driving.%2520The%2520robot%2520is%2520equipped%2520with%2520a%2520wide%2520array%2520of%2520sensors%250Aincluding%2520RGB%2520cameras%252C%2520RGB-D%2520cameras%252C%2520event-based%2520cameras%252C%2520IR%2520cameras%252C%2520LiDARs%252C%250AmmWave%2520radars%252C%2520IMUs%252C%2520ultrasonic%2520range%2520finders%252C%2520and%2520a%2520GNSS%2520RTK%2520receiver.%2520The%250Asensor%2520suite%2520is%2520integrated%2520onto%2520a%2520specially%2520designed%2520mechanical%2520structure%2520with%250Aa%2520centralized%2520power%2520system%2520and%2520a%2520synchronization%2520mechanism%2520to%2520ensure%2520spatial%250Aand%2520temporal%2520alignment%2520of%2520the%2520sensor%2520data.%2520A%252016-node%2520on-board%2520computing%2520cluster%250Ahandles%2520sensor%2520control%252C%2520data%2520collection%252C%2520and%2520storage.%2520We%2520describe%2520the%2520hardware%250Aand%2520software%2520architecture%2520of%2520the%2520robot%2520in%2520detail%2520and%2520discuss%2520the%2520calibration%250Aprocedures%2520for%2520the%2520various%2520sensors.%2520The%2520capabilities%2520of%2520the%2520platform%2520are%250Ademonstrated%2520through%2520an%2520extensive%2520dataset%2520collected%2520in%2520diverse%2520real-world%250Aenvironments.%2520To%2520facilitate%2520research%252C%2520we%2520make%2520the%2520dataset%2520publicly%2520available%250Aalong%2520with%2520the%2520associated%2520robot%2520sensor%2520calibration%2520data.%2520Performance%250Aevaluations%2520on%2520a%2520set%2520of%2520standard%2520perception%2520and%2520localization%2520tasks%2520showcase%2520the%250Apotential%2520of%2520the%2520dataset%2520to%2520support%2520developments%2520in%2520Robot%2520Autonomy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShanghaiTech%20Mapping%20Robot%20is%20All%20You%20Need%3A%20Robot%20System%20for%20Collecting%0A%20%20Universal%20Ground%20Vehicle%20Datasets&entry.906535625=Bowen%20Xu%20and%20Xiting%20Zhao%20and%20Delin%20Feng%20and%20Yuanyuan%20Yang%20and%20S%C3%B6ren%20Schwertfeger&entry.1292438233=%20%20This%20paper%20presents%20the%20ShanghaiTech%20Mapping%20Robot%2C%20a%20state-of-the-art%0Aunmanned%20ground%20vehicle%20%28UGV%29%20designed%20for%20collecting%20comprehensive%0Amulti-sensor%20datasets%20to%20support%20research%20in%20robotics%2C%20computer%20vision%2C%20and%0Aautonomous%20driving.%20The%20robot%20is%20equipped%20with%20a%20wide%20array%20of%20sensors%0Aincluding%20RGB%20cameras%2C%20RGB-D%20cameras%2C%20event-based%20cameras%2C%20IR%20cameras%2C%20LiDARs%2C%0AmmWave%20radars%2C%20IMUs%2C%20ultrasonic%20range%20finders%2C%20and%20a%20GNSS%20RTK%20receiver.%20The%0Asensor%20suite%20is%20integrated%20onto%20a%20specially%20designed%20mechanical%20structure%20with%0Aa%20centralized%20power%20system%20and%20a%20synchronization%20mechanism%20to%20ensure%20spatial%0Aand%20temporal%20alignment%20of%20the%20sensor%20data.%20A%2016-node%20on-board%20computing%20cluster%0Ahandles%20sensor%20control%2C%20data%20collection%2C%20and%20storage.%20We%20describe%20the%20hardware%0Aand%20software%20architecture%20of%20the%20robot%20in%20detail%20and%20discuss%20the%20calibration%0Aprocedures%20for%20the%20various%20sensors.%20The%20capabilities%20of%20the%20platform%20are%0Ademonstrated%20through%20an%20extensive%20dataset%20collected%20in%20diverse%20real-world%0Aenvironments.%20To%20facilitate%20research%2C%20we%20make%20the%20dataset%20publicly%20available%0Aalong%20with%20the%20associated%20robot%20sensor%20calibration%20data.%20Performance%0Aevaluations%20on%20a%20set%20of%20standard%20perception%20and%20localization%20tasks%20showcase%20the%0Apotential%20of%20the%20dataset%20to%20support%20developments%20in%20Robot%20Autonomy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16713v1&entry.124074799=Read"},
{"title": "Towards Theoretical Understandings of Self-Consuming Generative Models", "author": "Shi Fu and Sen Zhang and Yingjie Wang and Xinmei Tian and Dacheng Tao", "abstract": "  This paper tackles the emerging challenge of training generative models\nwithin a self-consuming loop, wherein successive generations of models are\nrecursively trained on mixtures of real and synthetic data from previous\ngenerations. We construct a theoretical framework to rigorously evaluate how\nthis training procedure impacts the data distributions learned by future\nmodels, including parametric and non-parametric models. Specifically, we derive\nbounds on the total variation (TV) distance between the synthetic data\ndistributions produced by future models and the original real data distribution\nunder various mixed training scenarios for diffusion models with a\none-hidden-layer neural network score function. Our analysis demonstrates that\nthis distance can be effectively controlled under the condition that mixed\ntraining dataset sizes or proportions of real data are large enough.\nInterestingly, we further unveil a phase transition induced by expanding\nsynthetic data amounts, proving theoretically that while the TV distance\nexhibits an initial ascent, it declines beyond a threshold point. Finally, we\npresent results for kernel density estimation, delivering nuanced insights such\nas the impact of mixed data training on error propagation.\n", "link": "http://arxiv.org/abs/2402.11778v2", "date": "2024-06-24", "relevancy": 2.3219, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6146}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5902}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Theoretical%20Understandings%20of%20Self-Consuming%20Generative%20Models&body=Title%3A%20Towards%20Theoretical%20Understandings%20of%20Self-Consuming%20Generative%20Models%0AAuthor%3A%20Shi%20Fu%20and%20Sen%20Zhang%20and%20Yingjie%20Wang%20and%20Xinmei%20Tian%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20This%20paper%20tackles%20the%20emerging%20challenge%20of%20training%20generative%20models%0Awithin%20a%20self-consuming%20loop%2C%20wherein%20successive%20generations%20of%20models%20are%0Arecursively%20trained%20on%20mixtures%20of%20real%20and%20synthetic%20data%20from%20previous%0Agenerations.%20We%20construct%20a%20theoretical%20framework%20to%20rigorously%20evaluate%20how%0Athis%20training%20procedure%20impacts%20the%20data%20distributions%20learned%20by%20future%0Amodels%2C%20including%20parametric%20and%20non-parametric%20models.%20Specifically%2C%20we%20derive%0Abounds%20on%20the%20total%20variation%20%28TV%29%20distance%20between%20the%20synthetic%20data%0Adistributions%20produced%20by%20future%20models%20and%20the%20original%20real%20data%20distribution%0Aunder%20various%20mixed%20training%20scenarios%20for%20diffusion%20models%20with%20a%0Aone-hidden-layer%20neural%20network%20score%20function.%20Our%20analysis%20demonstrates%20that%0Athis%20distance%20can%20be%20effectively%20controlled%20under%20the%20condition%20that%20mixed%0Atraining%20dataset%20sizes%20or%20proportions%20of%20real%20data%20are%20large%20enough.%0AInterestingly%2C%20we%20further%20unveil%20a%20phase%20transition%20induced%20by%20expanding%0Asynthetic%20data%20amounts%2C%20proving%20theoretically%20that%20while%20the%20TV%20distance%0Aexhibits%20an%20initial%20ascent%2C%20it%20declines%20beyond%20a%20threshold%20point.%20Finally%2C%20we%0Apresent%20results%20for%20kernel%20density%20estimation%2C%20delivering%20nuanced%20insights%20such%0Aas%20the%20impact%20of%20mixed%20data%20training%20on%20error%20propagation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11778v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Theoretical%2520Understandings%2520of%2520Self-Consuming%2520Generative%2520Models%26entry.906535625%3DShi%2520Fu%2520and%2520Sen%2520Zhang%2520and%2520Yingjie%2520Wang%2520and%2520Xinmei%2520Tian%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520This%2520paper%2520tackles%2520the%2520emerging%2520challenge%2520of%2520training%2520generative%2520models%250Awithin%2520a%2520self-consuming%2520loop%252C%2520wherein%2520successive%2520generations%2520of%2520models%2520are%250Arecursively%2520trained%2520on%2520mixtures%2520of%2520real%2520and%2520synthetic%2520data%2520from%2520previous%250Agenerations.%2520We%2520construct%2520a%2520theoretical%2520framework%2520to%2520rigorously%2520evaluate%2520how%250Athis%2520training%2520procedure%2520impacts%2520the%2520data%2520distributions%2520learned%2520by%2520future%250Amodels%252C%2520including%2520parametric%2520and%2520non-parametric%2520models.%2520Specifically%252C%2520we%2520derive%250Abounds%2520on%2520the%2520total%2520variation%2520%2528TV%2529%2520distance%2520between%2520the%2520synthetic%2520data%250Adistributions%2520produced%2520by%2520future%2520models%2520and%2520the%2520original%2520real%2520data%2520distribution%250Aunder%2520various%2520mixed%2520training%2520scenarios%2520for%2520diffusion%2520models%2520with%2520a%250Aone-hidden-layer%2520neural%2520network%2520score%2520function.%2520Our%2520analysis%2520demonstrates%2520that%250Athis%2520distance%2520can%2520be%2520effectively%2520controlled%2520under%2520the%2520condition%2520that%2520mixed%250Atraining%2520dataset%2520sizes%2520or%2520proportions%2520of%2520real%2520data%2520are%2520large%2520enough.%250AInterestingly%252C%2520we%2520further%2520unveil%2520a%2520phase%2520transition%2520induced%2520by%2520expanding%250Asynthetic%2520data%2520amounts%252C%2520proving%2520theoretically%2520that%2520while%2520the%2520TV%2520distance%250Aexhibits%2520an%2520initial%2520ascent%252C%2520it%2520declines%2520beyond%2520a%2520threshold%2520point.%2520Finally%252C%2520we%250Apresent%2520results%2520for%2520kernel%2520density%2520estimation%252C%2520delivering%2520nuanced%2520insights%2520such%250Aas%2520the%2520impact%2520of%2520mixed%2520data%2520training%2520on%2520error%2520propagation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11778v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Theoretical%20Understandings%20of%20Self-Consuming%20Generative%20Models&entry.906535625=Shi%20Fu%20and%20Sen%20Zhang%20and%20Yingjie%20Wang%20and%20Xinmei%20Tian%20and%20Dacheng%20Tao&entry.1292438233=%20%20This%20paper%20tackles%20the%20emerging%20challenge%20of%20training%20generative%20models%0Awithin%20a%20self-consuming%20loop%2C%20wherein%20successive%20generations%20of%20models%20are%0Arecursively%20trained%20on%20mixtures%20of%20real%20and%20synthetic%20data%20from%20previous%0Agenerations.%20We%20construct%20a%20theoretical%20framework%20to%20rigorously%20evaluate%20how%0Athis%20training%20procedure%20impacts%20the%20data%20distributions%20learned%20by%20future%0Amodels%2C%20including%20parametric%20and%20non-parametric%20models.%20Specifically%2C%20we%20derive%0Abounds%20on%20the%20total%20variation%20%28TV%29%20distance%20between%20the%20synthetic%20data%0Adistributions%20produced%20by%20future%20models%20and%20the%20original%20real%20data%20distribution%0Aunder%20various%20mixed%20training%20scenarios%20for%20diffusion%20models%20with%20a%0Aone-hidden-layer%20neural%20network%20score%20function.%20Our%20analysis%20demonstrates%20that%0Athis%20distance%20can%20be%20effectively%20controlled%20under%20the%20condition%20that%20mixed%0Atraining%20dataset%20sizes%20or%20proportions%20of%20real%20data%20are%20large%20enough.%0AInterestingly%2C%20we%20further%20unveil%20a%20phase%20transition%20induced%20by%20expanding%0Asynthetic%20data%20amounts%2C%20proving%20theoretically%20that%20while%20the%20TV%20distance%0Aexhibits%20an%20initial%20ascent%2C%20it%20declines%20beyond%20a%20threshold%20point.%20Finally%2C%20we%0Apresent%20results%20for%20kernel%20density%20estimation%2C%20delivering%20nuanced%20insights%20such%0Aas%20the%20impact%20of%20mixed%20data%20training%20on%20error%20propagation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11778v2&entry.124074799=Read"},
{"title": "Knowledge Accumulation in Continually Learned Representations and the\n  Issue of Feature Forgetting", "author": "Timm Hess and Eli Verwimp and Gido M. van de Ven and Tinne Tuytelaars", "abstract": "  Continual learning research has shown that neural networks suffer from\ncatastrophic forgetting \"at the output level\", but it is debated whether this\nis also the case at the level of learned representations. Multiple recent\nstudies ascribe representations a certain level of innate robustness against\nforgetting -- that they only forget minimally in comparison with forgetting at\nthe output level. We revisit and expand upon the experiments that revealed this\ndifference in forgetting and illustrate the coexistence of two phenomena that\naffect the quality of continually learned representations: knowledge\naccumulation and feature forgetting. Taking both aspects into account, we show\nthat, even though forgetting in the representation (i.e. feature forgetting)\ncan be small in absolute terms, when measuring relative to how much was learned\nduring a task, forgetting in the representation tends to be just as\ncatastrophic as forgetting at the output level. Next we show that this feature\nforgetting is problematic as it substantially slows down the incremental\nlearning of good general representations (i.e. knowledge accumulation).\nFinally, we study how feature forgetting and knowledge accumulation are\naffected by different types of continual learning methods.\n", "link": "http://arxiv.org/abs/2304.00933v4", "date": "2024-06-24", "relevancy": 2.3118, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4684}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4632}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Accumulation%20in%20Continually%20Learned%20Representations%20and%20the%0A%20%20Issue%20of%20Feature%20Forgetting&body=Title%3A%20Knowledge%20Accumulation%20in%20Continually%20Learned%20Representations%20and%20the%0A%20%20Issue%20of%20Feature%20Forgetting%0AAuthor%3A%20Timm%20Hess%20and%20Eli%20Verwimp%20and%20Gido%20M.%20van%20de%20Ven%20and%20Tinne%20Tuytelaars%0AAbstract%3A%20%20%20Continual%20learning%20research%20has%20shown%20that%20neural%20networks%20suffer%20from%0Acatastrophic%20forgetting%20%22at%20the%20output%20level%22%2C%20but%20it%20is%20debated%20whether%20this%0Ais%20also%20the%20case%20at%20the%20level%20of%20learned%20representations.%20Multiple%20recent%0Astudies%20ascribe%20representations%20a%20certain%20level%20of%20innate%20robustness%20against%0Aforgetting%20--%20that%20they%20only%20forget%20minimally%20in%20comparison%20with%20forgetting%20at%0Athe%20output%20level.%20We%20revisit%20and%20expand%20upon%20the%20experiments%20that%20revealed%20this%0Adifference%20in%20forgetting%20and%20illustrate%20the%20coexistence%20of%20two%20phenomena%20that%0Aaffect%20the%20quality%20of%20continually%20learned%20representations%3A%20knowledge%0Aaccumulation%20and%20feature%20forgetting.%20Taking%20both%20aspects%20into%20account%2C%20we%20show%0Athat%2C%20even%20though%20forgetting%20in%20the%20representation%20%28i.e.%20feature%20forgetting%29%0Acan%20be%20small%20in%20absolute%20terms%2C%20when%20measuring%20relative%20to%20how%20much%20was%20learned%0Aduring%20a%20task%2C%20forgetting%20in%20the%20representation%20tends%20to%20be%20just%20as%0Acatastrophic%20as%20forgetting%20at%20the%20output%20level.%20Next%20we%20show%20that%20this%20feature%0Aforgetting%20is%20problematic%20as%20it%20substantially%20slows%20down%20the%20incremental%0Alearning%20of%20good%20general%20representations%20%28i.e.%20knowledge%20accumulation%29.%0AFinally%2C%20we%20study%20how%20feature%20forgetting%20and%20knowledge%20accumulation%20are%0Aaffected%20by%20different%20types%20of%20continual%20learning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.00933v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Accumulation%2520in%2520Continually%2520Learned%2520Representations%2520and%2520the%250A%2520%2520Issue%2520of%2520Feature%2520Forgetting%26entry.906535625%3DTimm%2520Hess%2520and%2520Eli%2520Verwimp%2520and%2520Gido%2520M.%2520van%2520de%2520Ven%2520and%2520Tinne%2520Tuytelaars%26entry.1292438233%3D%2520%2520Continual%2520learning%2520research%2520has%2520shown%2520that%2520neural%2520networks%2520suffer%2520from%250Acatastrophic%2520forgetting%2520%2522at%2520the%2520output%2520level%2522%252C%2520but%2520it%2520is%2520debated%2520whether%2520this%250Ais%2520also%2520the%2520case%2520at%2520the%2520level%2520of%2520learned%2520representations.%2520Multiple%2520recent%250Astudies%2520ascribe%2520representations%2520a%2520certain%2520level%2520of%2520innate%2520robustness%2520against%250Aforgetting%2520--%2520that%2520they%2520only%2520forget%2520minimally%2520in%2520comparison%2520with%2520forgetting%2520at%250Athe%2520output%2520level.%2520We%2520revisit%2520and%2520expand%2520upon%2520the%2520experiments%2520that%2520revealed%2520this%250Adifference%2520in%2520forgetting%2520and%2520illustrate%2520the%2520coexistence%2520of%2520two%2520phenomena%2520that%250Aaffect%2520the%2520quality%2520of%2520continually%2520learned%2520representations%253A%2520knowledge%250Aaccumulation%2520and%2520feature%2520forgetting.%2520Taking%2520both%2520aspects%2520into%2520account%252C%2520we%2520show%250Athat%252C%2520even%2520though%2520forgetting%2520in%2520the%2520representation%2520%2528i.e.%2520feature%2520forgetting%2529%250Acan%2520be%2520small%2520in%2520absolute%2520terms%252C%2520when%2520measuring%2520relative%2520to%2520how%2520much%2520was%2520learned%250Aduring%2520a%2520task%252C%2520forgetting%2520in%2520the%2520representation%2520tends%2520to%2520be%2520just%2520as%250Acatastrophic%2520as%2520forgetting%2520at%2520the%2520output%2520level.%2520Next%2520we%2520show%2520that%2520this%2520feature%250Aforgetting%2520is%2520problematic%2520as%2520it%2520substantially%2520slows%2520down%2520the%2520incremental%250Alearning%2520of%2520good%2520general%2520representations%2520%2528i.e.%2520knowledge%2520accumulation%2529.%250AFinally%252C%2520we%2520study%2520how%2520feature%2520forgetting%2520and%2520knowledge%2520accumulation%2520are%250Aaffected%2520by%2520different%2520types%2520of%2520continual%2520learning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.00933v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Accumulation%20in%20Continually%20Learned%20Representations%20and%20the%0A%20%20Issue%20of%20Feature%20Forgetting&entry.906535625=Timm%20Hess%20and%20Eli%20Verwimp%20and%20Gido%20M.%20van%20de%20Ven%20and%20Tinne%20Tuytelaars&entry.1292438233=%20%20Continual%20learning%20research%20has%20shown%20that%20neural%20networks%20suffer%20from%0Acatastrophic%20forgetting%20%22at%20the%20output%20level%22%2C%20but%20it%20is%20debated%20whether%20this%0Ais%20also%20the%20case%20at%20the%20level%20of%20learned%20representations.%20Multiple%20recent%0Astudies%20ascribe%20representations%20a%20certain%20level%20of%20innate%20robustness%20against%0Aforgetting%20--%20that%20they%20only%20forget%20minimally%20in%20comparison%20with%20forgetting%20at%0Athe%20output%20level.%20We%20revisit%20and%20expand%20upon%20the%20experiments%20that%20revealed%20this%0Adifference%20in%20forgetting%20and%20illustrate%20the%20coexistence%20of%20two%20phenomena%20that%0Aaffect%20the%20quality%20of%20continually%20learned%20representations%3A%20knowledge%0Aaccumulation%20and%20feature%20forgetting.%20Taking%20both%20aspects%20into%20account%2C%20we%20show%0Athat%2C%20even%20though%20forgetting%20in%20the%20representation%20%28i.e.%20feature%20forgetting%29%0Acan%20be%20small%20in%20absolute%20terms%2C%20when%20measuring%20relative%20to%20how%20much%20was%20learned%0Aduring%20a%20task%2C%20forgetting%20in%20the%20representation%20tends%20to%20be%20just%20as%0Acatastrophic%20as%20forgetting%20at%20the%20output%20level.%20Next%20we%20show%20that%20this%20feature%0Aforgetting%20is%20problematic%20as%20it%20substantially%20slows%20down%20the%20incremental%0Alearning%20of%20good%20general%20representations%20%28i.e.%20knowledge%20accumulation%29.%0AFinally%2C%20we%20study%20how%20feature%20forgetting%20and%20knowledge%20accumulation%20are%0Aaffected%20by%20different%20types%20of%20continual%20learning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.00933v4&entry.124074799=Read"},
{"title": "Concentration Inequalities for $(f,\u0393)$-GANs", "author": "Jeremiah Birrell", "abstract": "  Generative adversarial networks (GANs) are unsupervised learning methods for\ntraining a generator distribution to produce samples that approximate those\ndrawn from a target distribution. Many such methods can be formulated as\nminimization of a metric or divergence. Recent works have proven the\nstatistical consistency of GANs that are based on integral probability metrics\n(IPMs), e.g., WGAN which is based on the 1-Wasserstein metric. IPMs are defined\nby optimizing a linear functional (difference of expectations) over a space of\ndiscriminators. A much larger class of GANs, which allow for the use of\nnonlinear objective functionals, can be constructed using\n$(f,\\Gamma)$-divergences; these generalize and interpolate between IPMs and\n$f$-divergences (e.g., KL or $\\alpha$-divergences). Instances of\n$(f,\\Gamma)$-GANs have been shown to exhibit improved performance in a number\nof applications. In this work we study the statistical consistency of\n$(f,\\Gamma)$-GANs for general $f$ and $\\Gamma$. Specifically, we derive\nfinite-sample concentration inequalities. These derivations require novel\narguments due to nonlinearity of the objective functional. We demonstrate that\nour new results reduce to the known results for IPM-GANs in the appropriate\nlimit while also significantly extending the domain of applicability of this\ntheory.\n", "link": "http://arxiv.org/abs/2406.16834v1", "date": "2024-06-24", "relevancy": 2.3099, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4632}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4615}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concentration%20Inequalities%20for%20%24%28f%2C%CE%93%29%24-GANs&body=Title%3A%20Concentration%20Inequalities%20for%20%24%28f%2C%CE%93%29%24-GANs%0AAuthor%3A%20Jeremiah%20Birrell%0AAbstract%3A%20%20%20Generative%20adversarial%20networks%20%28GANs%29%20are%20unsupervised%20learning%20methods%20for%0Atraining%20a%20generator%20distribution%20to%20produce%20samples%20that%20approximate%20those%0Adrawn%20from%20a%20target%20distribution.%20Many%20such%20methods%20can%20be%20formulated%20as%0Aminimization%20of%20a%20metric%20or%20divergence.%20Recent%20works%20have%20proven%20the%0Astatistical%20consistency%20of%20GANs%20that%20are%20based%20on%20integral%20probability%20metrics%0A%28IPMs%29%2C%20e.g.%2C%20WGAN%20which%20is%20based%20on%20the%201-Wasserstein%20metric.%20IPMs%20are%20defined%0Aby%20optimizing%20a%20linear%20functional%20%28difference%20of%20expectations%29%20over%20a%20space%20of%0Adiscriminators.%20A%20much%20larger%20class%20of%20GANs%2C%20which%20allow%20for%20the%20use%20of%0Anonlinear%20objective%20functionals%2C%20can%20be%20constructed%20using%0A%24%28f%2C%5CGamma%29%24-divergences%3B%20these%20generalize%20and%20interpolate%20between%20IPMs%20and%0A%24f%24-divergences%20%28e.g.%2C%20KL%20or%20%24%5Calpha%24-divergences%29.%20Instances%20of%0A%24%28f%2C%5CGamma%29%24-GANs%20have%20been%20shown%20to%20exhibit%20improved%20performance%20in%20a%20number%0Aof%20applications.%20In%20this%20work%20we%20study%20the%20statistical%20consistency%20of%0A%24%28f%2C%5CGamma%29%24-GANs%20for%20general%20%24f%24%20and%20%24%5CGamma%24.%20Specifically%2C%20we%20derive%0Afinite-sample%20concentration%20inequalities.%20These%20derivations%20require%20novel%0Aarguments%20due%20to%20nonlinearity%20of%20the%20objective%20functional.%20We%20demonstrate%20that%0Aour%20new%20results%20reduce%20to%20the%20known%20results%20for%20IPM-GANs%20in%20the%20appropriate%0Alimit%20while%20also%20significantly%20extending%20the%20domain%20of%20applicability%20of%20this%0Atheory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcentration%2520Inequalities%2520for%2520%2524%2528f%252C%25CE%2593%2529%2524-GANs%26entry.906535625%3DJeremiah%2520Birrell%26entry.1292438233%3D%2520%2520Generative%2520adversarial%2520networks%2520%2528GANs%2529%2520are%2520unsupervised%2520learning%2520methods%2520for%250Atraining%2520a%2520generator%2520distribution%2520to%2520produce%2520samples%2520that%2520approximate%2520those%250Adrawn%2520from%2520a%2520target%2520distribution.%2520Many%2520such%2520methods%2520can%2520be%2520formulated%2520as%250Aminimization%2520of%2520a%2520metric%2520or%2520divergence.%2520Recent%2520works%2520have%2520proven%2520the%250Astatistical%2520consistency%2520of%2520GANs%2520that%2520are%2520based%2520on%2520integral%2520probability%2520metrics%250A%2528IPMs%2529%252C%2520e.g.%252C%2520WGAN%2520which%2520is%2520based%2520on%2520the%25201-Wasserstein%2520metric.%2520IPMs%2520are%2520defined%250Aby%2520optimizing%2520a%2520linear%2520functional%2520%2528difference%2520of%2520expectations%2529%2520over%2520a%2520space%2520of%250Adiscriminators.%2520A%2520much%2520larger%2520class%2520of%2520GANs%252C%2520which%2520allow%2520for%2520the%2520use%2520of%250Anonlinear%2520objective%2520functionals%252C%2520can%2520be%2520constructed%2520using%250A%2524%2528f%252C%255CGamma%2529%2524-divergences%253B%2520these%2520generalize%2520and%2520interpolate%2520between%2520IPMs%2520and%250A%2524f%2524-divergences%2520%2528e.g.%252C%2520KL%2520or%2520%2524%255Calpha%2524-divergences%2529.%2520Instances%2520of%250A%2524%2528f%252C%255CGamma%2529%2524-GANs%2520have%2520been%2520shown%2520to%2520exhibit%2520improved%2520performance%2520in%2520a%2520number%250Aof%2520applications.%2520In%2520this%2520work%2520we%2520study%2520the%2520statistical%2520consistency%2520of%250A%2524%2528f%252C%255CGamma%2529%2524-GANs%2520for%2520general%2520%2524f%2524%2520and%2520%2524%255CGamma%2524.%2520Specifically%252C%2520we%2520derive%250Afinite-sample%2520concentration%2520inequalities.%2520These%2520derivations%2520require%2520novel%250Aarguments%2520due%2520to%2520nonlinearity%2520of%2520the%2520objective%2520functional.%2520We%2520demonstrate%2520that%250Aour%2520new%2520results%2520reduce%2520to%2520the%2520known%2520results%2520for%2520IPM-GANs%2520in%2520the%2520appropriate%250Alimit%2520while%2520also%2520significantly%2520extending%2520the%2520domain%2520of%2520applicability%2520of%2520this%250Atheory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concentration%20Inequalities%20for%20%24%28f%2C%CE%93%29%24-GANs&entry.906535625=Jeremiah%20Birrell&entry.1292438233=%20%20Generative%20adversarial%20networks%20%28GANs%29%20are%20unsupervised%20learning%20methods%20for%0Atraining%20a%20generator%20distribution%20to%20produce%20samples%20that%20approximate%20those%0Adrawn%20from%20a%20target%20distribution.%20Many%20such%20methods%20can%20be%20formulated%20as%0Aminimization%20of%20a%20metric%20or%20divergence.%20Recent%20works%20have%20proven%20the%0Astatistical%20consistency%20of%20GANs%20that%20are%20based%20on%20integral%20probability%20metrics%0A%28IPMs%29%2C%20e.g.%2C%20WGAN%20which%20is%20based%20on%20the%201-Wasserstein%20metric.%20IPMs%20are%20defined%0Aby%20optimizing%20a%20linear%20functional%20%28difference%20of%20expectations%29%20over%20a%20space%20of%0Adiscriminators.%20A%20much%20larger%20class%20of%20GANs%2C%20which%20allow%20for%20the%20use%20of%0Anonlinear%20objective%20functionals%2C%20can%20be%20constructed%20using%0A%24%28f%2C%5CGamma%29%24-divergences%3B%20these%20generalize%20and%20interpolate%20between%20IPMs%20and%0A%24f%24-divergences%20%28e.g.%2C%20KL%20or%20%24%5Calpha%24-divergences%29.%20Instances%20of%0A%24%28f%2C%5CGamma%29%24-GANs%20have%20been%20shown%20to%20exhibit%20improved%20performance%20in%20a%20number%0Aof%20applications.%20In%20this%20work%20we%20study%20the%20statistical%20consistency%20of%0A%24%28f%2C%5CGamma%29%24-GANs%20for%20general%20%24f%24%20and%20%24%5CGamma%24.%20Specifically%2C%20we%20derive%0Afinite-sample%20concentration%20inequalities.%20These%20derivations%20require%20novel%0Aarguments%20due%20to%20nonlinearity%20of%20the%20objective%20functional.%20We%20demonstrate%20that%0Aour%20new%20results%20reduce%20to%20the%20known%20results%20for%20IPM-GANs%20in%20the%20appropriate%0Alimit%20while%20also%20significantly%20extending%20the%20domain%20of%20applicability%20of%20this%0Atheory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16834v1&entry.124074799=Read"},
{"title": "VideoScore: Building Automatic Metrics to Simulate Fine-grained Human\n  Feedback for Video Generation", "author": "Xuan He and Dongfu Jiang and Ge Zhang and Max Ku and Achint Soni and Sherman Siu and Haonan Chen and Abhranil Chandra and Ziyan Jiang and Aaran Arulraj and Kai Wang and Quy Duc Do and Yuansheng Ni and Bohan Lyu and Yaswanth Narsupalli and Rongqi Fan and Zhiheng Lyu and Yuchen Lin and Wenhu Chen", "abstract": "  The recent years have witnessed great advances in video generation. However,\nthe development of automatic video metrics is lagging significantly behind.\nNone of the existing metric is able to provide reliable scores over generated\nvideos. The main barrier is the lack of large-scale human-annotated dataset. In\nthis paper, we release VideoFeedback, the first large-scale dataset containing\nhuman-provided multi-aspect score over 37.6K synthesized videos from 11\nexisting video generative models. We train VideoScore (initialized from Mantis)\nbased on VideoFeedback to enable automatic video quality assessment.\nExperiments show that the Spearman correlation between VideoScore and humans\ncan reach 77.1 on VideoFeedback-test, beating the prior best metrics by about\n50 points. Further result on other held-out EvalCrafter, GenAI-Bench, and\nVBench show that VideoScore has consistently much higher correlation with human\njudges than other metrics. Due to these results, we believe VideoScore can\nserve as a great proxy for human raters to (1) rate different video models to\ntrack progress (2) simulate fine-grained human feedback in Reinforcement\nLearning with Human Feedback (RLHF) to improve current video generation models.\n", "link": "http://arxiv.org/abs/2406.15252v2", "date": "2024-06-24", "relevancy": 2.2975, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5808}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5777}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.55}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoScore%3A%20Building%20Automatic%20Metrics%20to%20Simulate%20Fine-grained%20Human%0A%20%20Feedback%20for%20Video%20Generation&body=Title%3A%20VideoScore%3A%20Building%20Automatic%20Metrics%20to%20Simulate%20Fine-grained%20Human%0A%20%20Feedback%20for%20Video%20Generation%0AAuthor%3A%20Xuan%20He%20and%20Dongfu%20Jiang%20and%20Ge%20Zhang%20and%20Max%20Ku%20and%20Achint%20Soni%20and%20Sherman%20Siu%20and%20Haonan%20Chen%20and%20Abhranil%20Chandra%20and%20Ziyan%20Jiang%20and%20Aaran%20Arulraj%20and%20Kai%20Wang%20and%20Quy%20Duc%20Do%20and%20Yuansheng%20Ni%20and%20Bohan%20Lyu%20and%20Yaswanth%20Narsupalli%20and%20Rongqi%20Fan%20and%20Zhiheng%20Lyu%20and%20Yuchen%20Lin%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20The%20recent%20years%20have%20witnessed%20great%20advances%20in%20video%20generation.%20However%2C%0Athe%20development%20of%20automatic%20video%20metrics%20is%20lagging%20significantly%20behind.%0ANone%20of%20the%20existing%20metric%20is%20able%20to%20provide%20reliable%20scores%20over%20generated%0Avideos.%20The%20main%20barrier%20is%20the%20lack%20of%20large-scale%20human-annotated%20dataset.%20In%0Athis%20paper%2C%20we%20release%20VideoFeedback%2C%20the%20first%20large-scale%20dataset%20containing%0Ahuman-provided%20multi-aspect%20score%20over%2037.6K%20synthesized%20videos%20from%2011%0Aexisting%20video%20generative%20models.%20We%20train%20VideoScore%20%28initialized%20from%20Mantis%29%0Abased%20on%20VideoFeedback%20to%20enable%20automatic%20video%20quality%20assessment.%0AExperiments%20show%20that%20the%20Spearman%20correlation%20between%20VideoScore%20and%20humans%0Acan%20reach%2077.1%20on%20VideoFeedback-test%2C%20beating%20the%20prior%20best%20metrics%20by%20about%0A50%20points.%20Further%20result%20on%20other%20held-out%20EvalCrafter%2C%20GenAI-Bench%2C%20and%0AVBench%20show%20that%20VideoScore%20has%20consistently%20much%20higher%20correlation%20with%20human%0Ajudges%20than%20other%20metrics.%20Due%20to%20these%20results%2C%20we%20believe%20VideoScore%20can%0Aserve%20as%20a%20great%20proxy%20for%20human%20raters%20to%20%281%29%20rate%20different%20video%20models%20to%0Atrack%20progress%20%282%29%20simulate%20fine-grained%20human%20feedback%20in%20Reinforcement%0ALearning%20with%20Human%20Feedback%20%28RLHF%29%20to%20improve%20current%20video%20generation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15252v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoScore%253A%2520Building%2520Automatic%2520Metrics%2520to%2520Simulate%2520Fine-grained%2520Human%250A%2520%2520Feedback%2520for%2520Video%2520Generation%26entry.906535625%3DXuan%2520He%2520and%2520Dongfu%2520Jiang%2520and%2520Ge%2520Zhang%2520and%2520Max%2520Ku%2520and%2520Achint%2520Soni%2520and%2520Sherman%2520Siu%2520and%2520Haonan%2520Chen%2520and%2520Abhranil%2520Chandra%2520and%2520Ziyan%2520Jiang%2520and%2520Aaran%2520Arulraj%2520and%2520Kai%2520Wang%2520and%2520Quy%2520Duc%2520Do%2520and%2520Yuansheng%2520Ni%2520and%2520Bohan%2520Lyu%2520and%2520Yaswanth%2520Narsupalli%2520and%2520Rongqi%2520Fan%2520and%2520Zhiheng%2520Lyu%2520and%2520Yuchen%2520Lin%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520The%2520recent%2520years%2520have%2520witnessed%2520great%2520advances%2520in%2520video%2520generation.%2520However%252C%250Athe%2520development%2520of%2520automatic%2520video%2520metrics%2520is%2520lagging%2520significantly%2520behind.%250ANone%2520of%2520the%2520existing%2520metric%2520is%2520able%2520to%2520provide%2520reliable%2520scores%2520over%2520generated%250Avideos.%2520The%2520main%2520barrier%2520is%2520the%2520lack%2520of%2520large-scale%2520human-annotated%2520dataset.%2520In%250Athis%2520paper%252C%2520we%2520release%2520VideoFeedback%252C%2520the%2520first%2520large-scale%2520dataset%2520containing%250Ahuman-provided%2520multi-aspect%2520score%2520over%252037.6K%2520synthesized%2520videos%2520from%252011%250Aexisting%2520video%2520generative%2520models.%2520We%2520train%2520VideoScore%2520%2528initialized%2520from%2520Mantis%2529%250Abased%2520on%2520VideoFeedback%2520to%2520enable%2520automatic%2520video%2520quality%2520assessment.%250AExperiments%2520show%2520that%2520the%2520Spearman%2520correlation%2520between%2520VideoScore%2520and%2520humans%250Acan%2520reach%252077.1%2520on%2520VideoFeedback-test%252C%2520beating%2520the%2520prior%2520best%2520metrics%2520by%2520about%250A50%2520points.%2520Further%2520result%2520on%2520other%2520held-out%2520EvalCrafter%252C%2520GenAI-Bench%252C%2520and%250AVBench%2520show%2520that%2520VideoScore%2520has%2520consistently%2520much%2520higher%2520correlation%2520with%2520human%250Ajudges%2520than%2520other%2520metrics.%2520Due%2520to%2520these%2520results%252C%2520we%2520believe%2520VideoScore%2520can%250Aserve%2520as%2520a%2520great%2520proxy%2520for%2520human%2520raters%2520to%2520%25281%2529%2520rate%2520different%2520video%2520models%2520to%250Atrack%2520progress%2520%25282%2529%2520simulate%2520fine-grained%2520human%2520feedback%2520in%2520Reinforcement%250ALearning%2520with%2520Human%2520Feedback%2520%2528RLHF%2529%2520to%2520improve%2520current%2520video%2520generation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15252v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoScore%3A%20Building%20Automatic%20Metrics%20to%20Simulate%20Fine-grained%20Human%0A%20%20Feedback%20for%20Video%20Generation&entry.906535625=Xuan%20He%20and%20Dongfu%20Jiang%20and%20Ge%20Zhang%20and%20Max%20Ku%20and%20Achint%20Soni%20and%20Sherman%20Siu%20and%20Haonan%20Chen%20and%20Abhranil%20Chandra%20and%20Ziyan%20Jiang%20and%20Aaran%20Arulraj%20and%20Kai%20Wang%20and%20Quy%20Duc%20Do%20and%20Yuansheng%20Ni%20and%20Bohan%20Lyu%20and%20Yaswanth%20Narsupalli%20and%20Rongqi%20Fan%20and%20Zhiheng%20Lyu%20and%20Yuchen%20Lin%20and%20Wenhu%20Chen&entry.1292438233=%20%20The%20recent%20years%20have%20witnessed%20great%20advances%20in%20video%20generation.%20However%2C%0Athe%20development%20of%20automatic%20video%20metrics%20is%20lagging%20significantly%20behind.%0ANone%20of%20the%20existing%20metric%20is%20able%20to%20provide%20reliable%20scores%20over%20generated%0Avideos.%20The%20main%20barrier%20is%20the%20lack%20of%20large-scale%20human-annotated%20dataset.%20In%0Athis%20paper%2C%20we%20release%20VideoFeedback%2C%20the%20first%20large-scale%20dataset%20containing%0Ahuman-provided%20multi-aspect%20score%20over%2037.6K%20synthesized%20videos%20from%2011%0Aexisting%20video%20generative%20models.%20We%20train%20VideoScore%20%28initialized%20from%20Mantis%29%0Abased%20on%20VideoFeedback%20to%20enable%20automatic%20video%20quality%20assessment.%0AExperiments%20show%20that%20the%20Spearman%20correlation%20between%20VideoScore%20and%20humans%0Acan%20reach%2077.1%20on%20VideoFeedback-test%2C%20beating%20the%20prior%20best%20metrics%20by%20about%0A50%20points.%20Further%20result%20on%20other%20held-out%20EvalCrafter%2C%20GenAI-Bench%2C%20and%0AVBench%20show%20that%20VideoScore%20has%20consistently%20much%20higher%20correlation%20with%20human%0Ajudges%20than%20other%20metrics.%20Due%20to%20these%20results%2C%20we%20believe%20VideoScore%20can%0Aserve%20as%20a%20great%20proxy%20for%20human%20raters%20to%20%281%29%20rate%20different%20video%20models%20to%0Atrack%20progress%20%282%29%20simulate%20fine-grained%20human%20feedback%20in%20Reinforcement%0ALearning%20with%20Human%20Feedback%20%28RLHF%29%20to%20improve%20current%20video%20generation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15252v2&entry.124074799=Read"},
{"title": "StableNormal: Reducing Diffusion Variance for Stable and Sharp Normal", "author": "Chongjie Ye and Lingteng Qiu and Xiaodong Gu and Qi Zuo and Yushuang Wu and Zilong Dong and Liefeng Bo and Yuliang Xiu and Xiaoguang Han", "abstract": "  This work addresses the challenge of high-quality surface normal estimation\nfrom monocular colored inputs (i.e., images and videos), a field which has\nrecently been revolutionized by repurposing diffusion priors. However, previous\nattempts still struggle with stochastic inference, conflicting with the\ndeterministic nature of the Image2Normal task, and costly ensembling step,\nwhich slows down the estimation process. Our method, StableNormal, mitigates\nthe stochasticity of the diffusion process by reducing inference variance, thus\nproducing \"Stable-and-Sharp\" normal estimates without any additional ensembling\nprocess. StableNormal works robustly under challenging imaging conditions, such\nas extreme lighting, blurring, and low quality. It is also robust against\ntransparent and reflective surfaces, as well as cluttered scenes with numerous\nobjects. Specifically, StableNormal employs a coarse-to-fine strategy, which\nstarts with a one-step normal estimator (YOSO) to derive an initial normal\nguess, that is relatively coarse but reliable, then followed by a\nsemantic-guided refinement process (SG-DRN) that refines the normals to recover\ngeometric details. The effectiveness of StableNormal is demonstrated through\ncompetitive performance in standard datasets such as DIODE-indoor, iBims,\nScannetV2 and NYUv2, and also in various downstream tasks, such as surface\nreconstruction and normal enhancement. These results evidence that StableNormal\nretains both the \"stability\" and \"sharpness\" for accurate normal estimation.\nStableNormal represents a baby attempt to repurpose diffusion priors for\ndeterministic estimation. To democratize this, code and models have been\npublicly available in hf.co/Stable-X\n", "link": "http://arxiv.org/abs/2406.16864v1", "date": "2024-06-24", "relevancy": 2.2879, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5883}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5728}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StableNormal%3A%20Reducing%20Diffusion%20Variance%20for%20Stable%20and%20Sharp%20Normal&body=Title%3A%20StableNormal%3A%20Reducing%20Diffusion%20Variance%20for%20Stable%20and%20Sharp%20Normal%0AAuthor%3A%20Chongjie%20Ye%20and%20Lingteng%20Qiu%20and%20Xiaodong%20Gu%20and%20Qi%20Zuo%20and%20Yushuang%20Wu%20and%20Zilong%20Dong%20and%20Liefeng%20Bo%20and%20Yuliang%20Xiu%20and%20Xiaoguang%20Han%0AAbstract%3A%20%20%20This%20work%20addresses%20the%20challenge%20of%20high-quality%20surface%20normal%20estimation%0Afrom%20monocular%20colored%20inputs%20%28i.e.%2C%20images%20and%20videos%29%2C%20a%20field%20which%20has%0Arecently%20been%20revolutionized%20by%20repurposing%20diffusion%20priors.%20However%2C%20previous%0Aattempts%20still%20struggle%20with%20stochastic%20inference%2C%20conflicting%20with%20the%0Adeterministic%20nature%20of%20the%20Image2Normal%20task%2C%20and%20costly%20ensembling%20step%2C%0Awhich%20slows%20down%20the%20estimation%20process.%20Our%20method%2C%20StableNormal%2C%20mitigates%0Athe%20stochasticity%20of%20the%20diffusion%20process%20by%20reducing%20inference%20variance%2C%20thus%0Aproducing%20%22Stable-and-Sharp%22%20normal%20estimates%20without%20any%20additional%20ensembling%0Aprocess.%20StableNormal%20works%20robustly%20under%20challenging%20imaging%20conditions%2C%20such%0Aas%20extreme%20lighting%2C%20blurring%2C%20and%20low%20quality.%20It%20is%20also%20robust%20against%0Atransparent%20and%20reflective%20surfaces%2C%20as%20well%20as%20cluttered%20scenes%20with%20numerous%0Aobjects.%20Specifically%2C%20StableNormal%20employs%20a%20coarse-to-fine%20strategy%2C%20which%0Astarts%20with%20a%20one-step%20normal%20estimator%20%28YOSO%29%20to%20derive%20an%20initial%20normal%0Aguess%2C%20that%20is%20relatively%20coarse%20but%20reliable%2C%20then%20followed%20by%20a%0Asemantic-guided%20refinement%20process%20%28SG-DRN%29%20that%20refines%20the%20normals%20to%20recover%0Ageometric%20details.%20The%20effectiveness%20of%20StableNormal%20is%20demonstrated%20through%0Acompetitive%20performance%20in%20standard%20datasets%20such%20as%20DIODE-indoor%2C%20iBims%2C%0AScannetV2%20and%20NYUv2%2C%20and%20also%20in%20various%20downstream%20tasks%2C%20such%20as%20surface%0Areconstruction%20and%20normal%20enhancement.%20These%20results%20evidence%20that%20StableNormal%0Aretains%20both%20the%20%22stability%22%20and%20%22sharpness%22%20for%20accurate%20normal%20estimation.%0AStableNormal%20represents%20a%20baby%20attempt%20to%20repurpose%20diffusion%20priors%20for%0Adeterministic%20estimation.%20To%20democratize%20this%2C%20code%20and%20models%20have%20been%0Apublicly%20available%20in%20hf.co/Stable-X%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStableNormal%253A%2520Reducing%2520Diffusion%2520Variance%2520for%2520Stable%2520and%2520Sharp%2520Normal%26entry.906535625%3DChongjie%2520Ye%2520and%2520Lingteng%2520Qiu%2520and%2520Xiaodong%2520Gu%2520and%2520Qi%2520Zuo%2520and%2520Yushuang%2520Wu%2520and%2520Zilong%2520Dong%2520and%2520Liefeng%2520Bo%2520and%2520Yuliang%2520Xiu%2520and%2520Xiaoguang%2520Han%26entry.1292438233%3D%2520%2520This%2520work%2520addresses%2520the%2520challenge%2520of%2520high-quality%2520surface%2520normal%2520estimation%250Afrom%2520monocular%2520colored%2520inputs%2520%2528i.e.%252C%2520images%2520and%2520videos%2529%252C%2520a%2520field%2520which%2520has%250Arecently%2520been%2520revolutionized%2520by%2520repurposing%2520diffusion%2520priors.%2520However%252C%2520previous%250Aattempts%2520still%2520struggle%2520with%2520stochastic%2520inference%252C%2520conflicting%2520with%2520the%250Adeterministic%2520nature%2520of%2520the%2520Image2Normal%2520task%252C%2520and%2520costly%2520ensembling%2520step%252C%250Awhich%2520slows%2520down%2520the%2520estimation%2520process.%2520Our%2520method%252C%2520StableNormal%252C%2520mitigates%250Athe%2520stochasticity%2520of%2520the%2520diffusion%2520process%2520by%2520reducing%2520inference%2520variance%252C%2520thus%250Aproducing%2520%2522Stable-and-Sharp%2522%2520normal%2520estimates%2520without%2520any%2520additional%2520ensembling%250Aprocess.%2520StableNormal%2520works%2520robustly%2520under%2520challenging%2520imaging%2520conditions%252C%2520such%250Aas%2520extreme%2520lighting%252C%2520blurring%252C%2520and%2520low%2520quality.%2520It%2520is%2520also%2520robust%2520against%250Atransparent%2520and%2520reflective%2520surfaces%252C%2520as%2520well%2520as%2520cluttered%2520scenes%2520with%2520numerous%250Aobjects.%2520Specifically%252C%2520StableNormal%2520employs%2520a%2520coarse-to-fine%2520strategy%252C%2520which%250Astarts%2520with%2520a%2520one-step%2520normal%2520estimator%2520%2528YOSO%2529%2520to%2520derive%2520an%2520initial%2520normal%250Aguess%252C%2520that%2520is%2520relatively%2520coarse%2520but%2520reliable%252C%2520then%2520followed%2520by%2520a%250Asemantic-guided%2520refinement%2520process%2520%2528SG-DRN%2529%2520that%2520refines%2520the%2520normals%2520to%2520recover%250Ageometric%2520details.%2520The%2520effectiveness%2520of%2520StableNormal%2520is%2520demonstrated%2520through%250Acompetitive%2520performance%2520in%2520standard%2520datasets%2520such%2520as%2520DIODE-indoor%252C%2520iBims%252C%250AScannetV2%2520and%2520NYUv2%252C%2520and%2520also%2520in%2520various%2520downstream%2520tasks%252C%2520such%2520as%2520surface%250Areconstruction%2520and%2520normal%2520enhancement.%2520These%2520results%2520evidence%2520that%2520StableNormal%250Aretains%2520both%2520the%2520%2522stability%2522%2520and%2520%2522sharpness%2522%2520for%2520accurate%2520normal%2520estimation.%250AStableNormal%2520represents%2520a%2520baby%2520attempt%2520to%2520repurpose%2520diffusion%2520priors%2520for%250Adeterministic%2520estimation.%2520To%2520democratize%2520this%252C%2520code%2520and%2520models%2520have%2520been%250Apublicly%2520available%2520in%2520hf.co/Stable-X%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StableNormal%3A%20Reducing%20Diffusion%20Variance%20for%20Stable%20and%20Sharp%20Normal&entry.906535625=Chongjie%20Ye%20and%20Lingteng%20Qiu%20and%20Xiaodong%20Gu%20and%20Qi%20Zuo%20and%20Yushuang%20Wu%20and%20Zilong%20Dong%20and%20Liefeng%20Bo%20and%20Yuliang%20Xiu%20and%20Xiaoguang%20Han&entry.1292438233=%20%20This%20work%20addresses%20the%20challenge%20of%20high-quality%20surface%20normal%20estimation%0Afrom%20monocular%20colored%20inputs%20%28i.e.%2C%20images%20and%20videos%29%2C%20a%20field%20which%20has%0Arecently%20been%20revolutionized%20by%20repurposing%20diffusion%20priors.%20However%2C%20previous%0Aattempts%20still%20struggle%20with%20stochastic%20inference%2C%20conflicting%20with%20the%0Adeterministic%20nature%20of%20the%20Image2Normal%20task%2C%20and%20costly%20ensembling%20step%2C%0Awhich%20slows%20down%20the%20estimation%20process.%20Our%20method%2C%20StableNormal%2C%20mitigates%0Athe%20stochasticity%20of%20the%20diffusion%20process%20by%20reducing%20inference%20variance%2C%20thus%0Aproducing%20%22Stable-and-Sharp%22%20normal%20estimates%20without%20any%20additional%20ensembling%0Aprocess.%20StableNormal%20works%20robustly%20under%20challenging%20imaging%20conditions%2C%20such%0Aas%20extreme%20lighting%2C%20blurring%2C%20and%20low%20quality.%20It%20is%20also%20robust%20against%0Atransparent%20and%20reflective%20surfaces%2C%20as%20well%20as%20cluttered%20scenes%20with%20numerous%0Aobjects.%20Specifically%2C%20StableNormal%20employs%20a%20coarse-to-fine%20strategy%2C%20which%0Astarts%20with%20a%20one-step%20normal%20estimator%20%28YOSO%29%20to%20derive%20an%20initial%20normal%0Aguess%2C%20that%20is%20relatively%20coarse%20but%20reliable%2C%20then%20followed%20by%20a%0Asemantic-guided%20refinement%20process%20%28SG-DRN%29%20that%20refines%20the%20normals%20to%20recover%0Ageometric%20details.%20The%20effectiveness%20of%20StableNormal%20is%20demonstrated%20through%0Acompetitive%20performance%20in%20standard%20datasets%20such%20as%20DIODE-indoor%2C%20iBims%2C%0AScannetV2%20and%20NYUv2%2C%20and%20also%20in%20various%20downstream%20tasks%2C%20such%20as%20surface%0Areconstruction%20and%20normal%20enhancement.%20These%20results%20evidence%20that%20StableNormal%0Aretains%20both%20the%20%22stability%22%20and%20%22sharpness%22%20for%20accurate%20normal%20estimation.%0AStableNormal%20represents%20a%20baby%20attempt%20to%20repurpose%20diffusion%20priors%20for%0Adeterministic%20estimation.%20To%20democratize%20this%2C%20code%20and%20models%20have%20been%0Apublicly%20available%20in%20hf.co/Stable-X%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16864v1&entry.124074799=Read"},
{"title": "Geometry-Aware Score Distillation via 3D Consistent Noising and Gradient\n  Consistency Modeling", "author": "Min-Seop Kwak and Donghoon Ahn and Ines Hyeonsu Kim and Jin-wha Kim and Seungryong Kim", "abstract": "  Score distillation sampling (SDS), the methodology in which the score from\npretrained 2D diffusion models is distilled into 3D representation, has\nrecently brought significant advancements in text-to-3D generation task.\nHowever, this approach is still confronted with critical geometric\ninconsistency problems such as the Janus problem. Starting from a hypothesis\nthat such inconsistency problems may be induced by multiview inconsistencies\nbetween 2D scores predicted from various viewpoints, we introduce GSD, a simple\nand general plug-and-play framework for incorporating 3D consistency and\ntherefore geometry awareness into the SDS process. Our methodology is composed\nof three components: 3D consistent noising, designed to produce 3D consistent\nnoise maps that perfectly follow the standard Gaussian distribution,\ngeometry-based gradient warping for identifying correspondences between\npredicted gradients of different viewpoints, and novel gradient consistency\nloss to optimize the scene geometry toward producing more consistent gradients.\nWe demonstrate that our method significantly improves performance, successfully\naddressing the geometric inconsistency problems in text-to-3D generation task\nwith minimal computation cost and being compatible with existing score\ndistillation-based models. Our project page is available at\nhttps://ku-cvlab.github.io/GSD/.\n", "link": "http://arxiv.org/abs/2406.16695v1", "date": "2024-06-24", "relevancy": 2.2765, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5927}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5595}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-Aware%20Score%20Distillation%20via%203D%20Consistent%20Noising%20and%20Gradient%0A%20%20Consistency%20Modeling&body=Title%3A%20Geometry-Aware%20Score%20Distillation%20via%203D%20Consistent%20Noising%20and%20Gradient%0A%20%20Consistency%20Modeling%0AAuthor%3A%20Min-Seop%20Kwak%20and%20Donghoon%20Ahn%20and%20Ines%20Hyeonsu%20Kim%20and%20Jin-wha%20Kim%20and%20Seungryong%20Kim%0AAbstract%3A%20%20%20Score%20distillation%20sampling%20%28SDS%29%2C%20the%20methodology%20in%20which%20the%20score%20from%0Apretrained%202D%20diffusion%20models%20is%20distilled%20into%203D%20representation%2C%20has%0Arecently%20brought%20significant%20advancements%20in%20text-to-3D%20generation%20task.%0AHowever%2C%20this%20approach%20is%20still%20confronted%20with%20critical%20geometric%0Ainconsistency%20problems%20such%20as%20the%20Janus%20problem.%20Starting%20from%20a%20hypothesis%0Athat%20such%20inconsistency%20problems%20may%20be%20induced%20by%20multiview%20inconsistencies%0Abetween%202D%20scores%20predicted%20from%20various%20viewpoints%2C%20we%20introduce%20GSD%2C%20a%20simple%0Aand%20general%20plug-and-play%20framework%20for%20incorporating%203D%20consistency%20and%0Atherefore%20geometry%20awareness%20into%20the%20SDS%20process.%20Our%20methodology%20is%20composed%0Aof%20three%20components%3A%203D%20consistent%20noising%2C%20designed%20to%20produce%203D%20consistent%0Anoise%20maps%20that%20perfectly%20follow%20the%20standard%20Gaussian%20distribution%2C%0Ageometry-based%20gradient%20warping%20for%20identifying%20correspondences%20between%0Apredicted%20gradients%20of%20different%20viewpoints%2C%20and%20novel%20gradient%20consistency%0Aloss%20to%20optimize%20the%20scene%20geometry%20toward%20producing%20more%20consistent%20gradients.%0AWe%20demonstrate%20that%20our%20method%20significantly%20improves%20performance%2C%20successfully%0Aaddressing%20the%20geometric%20inconsistency%20problems%20in%20text-to-3D%20generation%20task%0Awith%20minimal%20computation%20cost%20and%20being%20compatible%20with%20existing%20score%0Adistillation-based%20models.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//ku-cvlab.github.io/GSD/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-Aware%2520Score%2520Distillation%2520via%25203D%2520Consistent%2520Noising%2520and%2520Gradient%250A%2520%2520Consistency%2520Modeling%26entry.906535625%3DMin-Seop%2520Kwak%2520and%2520Donghoon%2520Ahn%2520and%2520Ines%2520Hyeonsu%2520Kim%2520and%2520Jin-wha%2520Kim%2520and%2520Seungryong%2520Kim%26entry.1292438233%3D%2520%2520Score%2520distillation%2520sampling%2520%2528SDS%2529%252C%2520the%2520methodology%2520in%2520which%2520the%2520score%2520from%250Apretrained%25202D%2520diffusion%2520models%2520is%2520distilled%2520into%25203D%2520representation%252C%2520has%250Arecently%2520brought%2520significant%2520advancements%2520in%2520text-to-3D%2520generation%2520task.%250AHowever%252C%2520this%2520approach%2520is%2520still%2520confronted%2520with%2520critical%2520geometric%250Ainconsistency%2520problems%2520such%2520as%2520the%2520Janus%2520problem.%2520Starting%2520from%2520a%2520hypothesis%250Athat%2520such%2520inconsistency%2520problems%2520may%2520be%2520induced%2520by%2520multiview%2520inconsistencies%250Abetween%25202D%2520scores%2520predicted%2520from%2520various%2520viewpoints%252C%2520we%2520introduce%2520GSD%252C%2520a%2520simple%250Aand%2520general%2520plug-and-play%2520framework%2520for%2520incorporating%25203D%2520consistency%2520and%250Atherefore%2520geometry%2520awareness%2520into%2520the%2520SDS%2520process.%2520Our%2520methodology%2520is%2520composed%250Aof%2520three%2520components%253A%25203D%2520consistent%2520noising%252C%2520designed%2520to%2520produce%25203D%2520consistent%250Anoise%2520maps%2520that%2520perfectly%2520follow%2520the%2520standard%2520Gaussian%2520distribution%252C%250Ageometry-based%2520gradient%2520warping%2520for%2520identifying%2520correspondences%2520between%250Apredicted%2520gradients%2520of%2520different%2520viewpoints%252C%2520and%2520novel%2520gradient%2520consistency%250Aloss%2520to%2520optimize%2520the%2520scene%2520geometry%2520toward%2520producing%2520more%2520consistent%2520gradients.%250AWe%2520demonstrate%2520that%2520our%2520method%2520significantly%2520improves%2520performance%252C%2520successfully%250Aaddressing%2520the%2520geometric%2520inconsistency%2520problems%2520in%2520text-to-3D%2520generation%2520task%250Awith%2520minimal%2520computation%2520cost%2520and%2520being%2520compatible%2520with%2520existing%2520score%250Adistillation-based%2520models.%2520Our%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//ku-cvlab.github.io/GSD/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-Aware%20Score%20Distillation%20via%203D%20Consistent%20Noising%20and%20Gradient%0A%20%20Consistency%20Modeling&entry.906535625=Min-Seop%20Kwak%20and%20Donghoon%20Ahn%20and%20Ines%20Hyeonsu%20Kim%20and%20Jin-wha%20Kim%20and%20Seungryong%20Kim&entry.1292438233=%20%20Score%20distillation%20sampling%20%28SDS%29%2C%20the%20methodology%20in%20which%20the%20score%20from%0Apretrained%202D%20diffusion%20models%20is%20distilled%20into%203D%20representation%2C%20has%0Arecently%20brought%20significant%20advancements%20in%20text-to-3D%20generation%20task.%0AHowever%2C%20this%20approach%20is%20still%20confronted%20with%20critical%20geometric%0Ainconsistency%20problems%20such%20as%20the%20Janus%20problem.%20Starting%20from%20a%20hypothesis%0Athat%20such%20inconsistency%20problems%20may%20be%20induced%20by%20multiview%20inconsistencies%0Abetween%202D%20scores%20predicted%20from%20various%20viewpoints%2C%20we%20introduce%20GSD%2C%20a%20simple%0Aand%20general%20plug-and-play%20framework%20for%20incorporating%203D%20consistency%20and%0Atherefore%20geometry%20awareness%20into%20the%20SDS%20process.%20Our%20methodology%20is%20composed%0Aof%20three%20components%3A%203D%20consistent%20noising%2C%20designed%20to%20produce%203D%20consistent%0Anoise%20maps%20that%20perfectly%20follow%20the%20standard%20Gaussian%20distribution%2C%0Ageometry-based%20gradient%20warping%20for%20identifying%20correspondences%20between%0Apredicted%20gradients%20of%20different%20viewpoints%2C%20and%20novel%20gradient%20consistency%0Aloss%20to%20optimize%20the%20scene%20geometry%20toward%20producing%20more%20consistent%20gradients.%0AWe%20demonstrate%20that%20our%20method%20significantly%20improves%20performance%2C%20successfully%0Aaddressing%20the%20geometric%20inconsistency%20problems%20in%20text-to-3D%20generation%20task%0Awith%20minimal%20computation%20cost%20and%20being%20compatible%20with%20existing%20score%0Adistillation-based%20models.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//ku-cvlab.github.io/GSD/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16695v1&entry.124074799=Read"},
{"title": "Sparser is Faster and Less is More: Efficient Sparse Attention for\n  Long-Range Transformers", "author": "Chao Lou and Zixia Jia and Zilong Zheng and Kewei Tu", "abstract": "  Accommodating long sequences efficiently in autoregressive Transformers,\nespecially within an extended context window, poses significant challenges due\nto the quadratic computational complexity and substantial KV memory\nrequirements inherent in self-attention mechanisms. In this work, we introduce\nSPARSEK Attention, a novel sparse attention mechanism designed to overcome\nthese computational and memory obstacles while maintaining performance. Our\napproach integrates a scoring network and a differentiable top-k mask operator,\nSPARSEK, to select a constant number of KV pairs for each query, thereby\nenabling gradient-based optimization. As a result, SPARSEK Attention offers\nlinear time complexity and constant memory footprint during generation.\nExperimental results reveal that SPARSEK Attention outperforms previous sparse\nattention methods and provides significant speed improvements during both\ntraining and inference, particularly in language modeling and downstream tasks.\nFurthermore, our method can be seamlessly integrated into pre-trained Large\nLanguage Models (LLMs) with minimal fine-tuning, offering a practical solution\nfor effectively managing long-range dependencies in diverse applications.\n", "link": "http://arxiv.org/abs/2406.16747v1", "date": "2024-06-24", "relevancy": 2.2619, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6239}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5378}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparser%20is%20Faster%20and%20Less%20is%20More%3A%20Efficient%20Sparse%20Attention%20for%0A%20%20Long-Range%20Transformers&body=Title%3A%20Sparser%20is%20Faster%20and%20Less%20is%20More%3A%20Efficient%20Sparse%20Attention%20for%0A%20%20Long-Range%20Transformers%0AAuthor%3A%20Chao%20Lou%20and%20Zixia%20Jia%20and%20Zilong%20Zheng%20and%20Kewei%20Tu%0AAbstract%3A%20%20%20Accommodating%20long%20sequences%20efficiently%20in%20autoregressive%20Transformers%2C%0Aespecially%20within%20an%20extended%20context%20window%2C%20poses%20significant%20challenges%20due%0Ato%20the%20quadratic%20computational%20complexity%20and%20substantial%20KV%20memory%0Arequirements%20inherent%20in%20self-attention%20mechanisms.%20In%20this%20work%2C%20we%20introduce%0ASPARSEK%20Attention%2C%20a%20novel%20sparse%20attention%20mechanism%20designed%20to%20overcome%0Athese%20computational%20and%20memory%20obstacles%20while%20maintaining%20performance.%20Our%0Aapproach%20integrates%20a%20scoring%20network%20and%20a%20differentiable%20top-k%20mask%20operator%2C%0ASPARSEK%2C%20to%20select%20a%20constant%20number%20of%20KV%20pairs%20for%20each%20query%2C%20thereby%0Aenabling%20gradient-based%20optimization.%20As%20a%20result%2C%20SPARSEK%20Attention%20offers%0Alinear%20time%20complexity%20and%20constant%20memory%20footprint%20during%20generation.%0AExperimental%20results%20reveal%20that%20SPARSEK%20Attention%20outperforms%20previous%20sparse%0Aattention%20methods%20and%20provides%20significant%20speed%20improvements%20during%20both%0Atraining%20and%20inference%2C%20particularly%20in%20language%20modeling%20and%20downstream%20tasks.%0AFurthermore%2C%20our%20method%20can%20be%20seamlessly%20integrated%20into%20pre-trained%20Large%0ALanguage%20Models%20%28LLMs%29%20with%20minimal%20fine-tuning%2C%20offering%20a%20practical%20solution%0Afor%20effectively%20managing%20long-range%20dependencies%20in%20diverse%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparser%2520is%2520Faster%2520and%2520Less%2520is%2520More%253A%2520Efficient%2520Sparse%2520Attention%2520for%250A%2520%2520Long-Range%2520Transformers%26entry.906535625%3DChao%2520Lou%2520and%2520Zixia%2520Jia%2520and%2520Zilong%2520Zheng%2520and%2520Kewei%2520Tu%26entry.1292438233%3D%2520%2520Accommodating%2520long%2520sequences%2520efficiently%2520in%2520autoregressive%2520Transformers%252C%250Aespecially%2520within%2520an%2520extended%2520context%2520window%252C%2520poses%2520significant%2520challenges%2520due%250Ato%2520the%2520quadratic%2520computational%2520complexity%2520and%2520substantial%2520KV%2520memory%250Arequirements%2520inherent%2520in%2520self-attention%2520mechanisms.%2520In%2520this%2520work%252C%2520we%2520introduce%250ASPARSEK%2520Attention%252C%2520a%2520novel%2520sparse%2520attention%2520mechanism%2520designed%2520to%2520overcome%250Athese%2520computational%2520and%2520memory%2520obstacles%2520while%2520maintaining%2520performance.%2520Our%250Aapproach%2520integrates%2520a%2520scoring%2520network%2520and%2520a%2520differentiable%2520top-k%2520mask%2520operator%252C%250ASPARSEK%252C%2520to%2520select%2520a%2520constant%2520number%2520of%2520KV%2520pairs%2520for%2520each%2520query%252C%2520thereby%250Aenabling%2520gradient-based%2520optimization.%2520As%2520a%2520result%252C%2520SPARSEK%2520Attention%2520offers%250Alinear%2520time%2520complexity%2520and%2520constant%2520memory%2520footprint%2520during%2520generation.%250AExperimental%2520results%2520reveal%2520that%2520SPARSEK%2520Attention%2520outperforms%2520previous%2520sparse%250Aattention%2520methods%2520and%2520provides%2520significant%2520speed%2520improvements%2520during%2520both%250Atraining%2520and%2520inference%252C%2520particularly%2520in%2520language%2520modeling%2520and%2520downstream%2520tasks.%250AFurthermore%252C%2520our%2520method%2520can%2520be%2520seamlessly%2520integrated%2520into%2520pre-trained%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520with%2520minimal%2520fine-tuning%252C%2520offering%2520a%2520practical%2520solution%250Afor%2520effectively%2520managing%2520long-range%2520dependencies%2520in%2520diverse%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparser%20is%20Faster%20and%20Less%20is%20More%3A%20Efficient%20Sparse%20Attention%20for%0A%20%20Long-Range%20Transformers&entry.906535625=Chao%20Lou%20and%20Zixia%20Jia%20and%20Zilong%20Zheng%20and%20Kewei%20Tu&entry.1292438233=%20%20Accommodating%20long%20sequences%20efficiently%20in%20autoregressive%20Transformers%2C%0Aespecially%20within%20an%20extended%20context%20window%2C%20poses%20significant%20challenges%20due%0Ato%20the%20quadratic%20computational%20complexity%20and%20substantial%20KV%20memory%0Arequirements%20inherent%20in%20self-attention%20mechanisms.%20In%20this%20work%2C%20we%20introduce%0ASPARSEK%20Attention%2C%20a%20novel%20sparse%20attention%20mechanism%20designed%20to%20overcome%0Athese%20computational%20and%20memory%20obstacles%20while%20maintaining%20performance.%20Our%0Aapproach%20integrates%20a%20scoring%20network%20and%20a%20differentiable%20top-k%20mask%20operator%2C%0ASPARSEK%2C%20to%20select%20a%20constant%20number%20of%20KV%20pairs%20for%20each%20query%2C%20thereby%0Aenabling%20gradient-based%20optimization.%20As%20a%20result%2C%20SPARSEK%20Attention%20offers%0Alinear%20time%20complexity%20and%20constant%20memory%20footprint%20during%20generation.%0AExperimental%20results%20reveal%20that%20SPARSEK%20Attention%20outperforms%20previous%20sparse%0Aattention%20methods%20and%20provides%20significant%20speed%20improvements%20during%20both%0Atraining%20and%20inference%2C%20particularly%20in%20language%20modeling%20and%20downstream%20tasks.%0AFurthermore%2C%20our%20method%20can%20be%20seamlessly%20integrated%20into%20pre-trained%20Large%0ALanguage%20Models%20%28LLMs%29%20with%20minimal%20fine-tuning%2C%20offering%20a%20practical%20solution%0Afor%20effectively%20managing%20long-range%20dependencies%20in%20diverse%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16747v1&entry.124074799=Read"},
{"title": "Character-Adapter: Prompt-Guided Region Control for High-Fidelity\n  Character Customization", "author": "Yuhang Ma and Wenting Xu and Jiji Tang and Qinfeng Jin and Rongsheng Zhang and Zeng Zhao and Changjie Fan and Zhipeng Hu", "abstract": "  Customized image generation, which seeks to synthesize images with consistent\ncharacters, holds significant relevance for applications such as storytelling,\nportrait generation, and character design. However, previous approaches have\nencountered challenges in preserving characters with high-fidelity consistency\ndue to inadequate feature extraction and concept confusion of reference\ncharacters. Therefore, we propose Character-Adapter, a plug-and-play framework\ndesigned to generate images that preserve the details of reference characters,\nensuring high-fidelity consistency. Character-Adapter employs prompt-guided\nsegmentation to ensure fine-grained regional features of reference characters\nand dynamic region-level adapters to mitigate concept confusion. Extensive\nexperiments are conducted to validate the effectiveness of Character-Adapter.\nBoth quantitative and qualitative results demonstrate that Character-Adapter\nachieves the state-of-the-art performance of consistent character generation,\nwith an improvement of 24.8% compared with other methods\n", "link": "http://arxiv.org/abs/2406.16537v1", "date": "2024-06-24", "relevancy": 2.254, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5856}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5507}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Character-Adapter%3A%20Prompt-Guided%20Region%20Control%20for%20High-Fidelity%0A%20%20Character%20Customization&body=Title%3A%20Character-Adapter%3A%20Prompt-Guided%20Region%20Control%20for%20High-Fidelity%0A%20%20Character%20Customization%0AAuthor%3A%20Yuhang%20Ma%20and%20Wenting%20Xu%20and%20Jiji%20Tang%20and%20Qinfeng%20Jin%20and%20Rongsheng%20Zhang%20and%20Zeng%20Zhao%20and%20Changjie%20Fan%20and%20Zhipeng%20Hu%0AAbstract%3A%20%20%20Customized%20image%20generation%2C%20which%20seeks%20to%20synthesize%20images%20with%20consistent%0Acharacters%2C%20holds%20significant%20relevance%20for%20applications%20such%20as%20storytelling%2C%0Aportrait%20generation%2C%20and%20character%20design.%20However%2C%20previous%20approaches%20have%0Aencountered%20challenges%20in%20preserving%20characters%20with%20high-fidelity%20consistency%0Adue%20to%20inadequate%20feature%20extraction%20and%20concept%20confusion%20of%20reference%0Acharacters.%20Therefore%2C%20we%20propose%20Character-Adapter%2C%20a%20plug-and-play%20framework%0Adesigned%20to%20generate%20images%20that%20preserve%20the%20details%20of%20reference%20characters%2C%0Aensuring%20high-fidelity%20consistency.%20Character-Adapter%20employs%20prompt-guided%0Asegmentation%20to%20ensure%20fine-grained%20regional%20features%20of%20reference%20characters%0Aand%20dynamic%20region-level%20adapters%20to%20mitigate%20concept%20confusion.%20Extensive%0Aexperiments%20are%20conducted%20to%20validate%20the%20effectiveness%20of%20Character-Adapter.%0ABoth%20quantitative%20and%20qualitative%20results%20demonstrate%20that%20Character-Adapter%0Aachieves%20the%20state-of-the-art%20performance%20of%20consistent%20character%20generation%2C%0Awith%20an%20improvement%20of%2024.8%25%20compared%20with%20other%20methods%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacter-Adapter%253A%2520Prompt-Guided%2520Region%2520Control%2520for%2520High-Fidelity%250A%2520%2520Character%2520Customization%26entry.906535625%3DYuhang%2520Ma%2520and%2520Wenting%2520Xu%2520and%2520Jiji%2520Tang%2520and%2520Qinfeng%2520Jin%2520and%2520Rongsheng%2520Zhang%2520and%2520Zeng%2520Zhao%2520and%2520Changjie%2520Fan%2520and%2520Zhipeng%2520Hu%26entry.1292438233%3D%2520%2520Customized%2520image%2520generation%252C%2520which%2520seeks%2520to%2520synthesize%2520images%2520with%2520consistent%250Acharacters%252C%2520holds%2520significant%2520relevance%2520for%2520applications%2520such%2520as%2520storytelling%252C%250Aportrait%2520generation%252C%2520and%2520character%2520design.%2520However%252C%2520previous%2520approaches%2520have%250Aencountered%2520challenges%2520in%2520preserving%2520characters%2520with%2520high-fidelity%2520consistency%250Adue%2520to%2520inadequate%2520feature%2520extraction%2520and%2520concept%2520confusion%2520of%2520reference%250Acharacters.%2520Therefore%252C%2520we%2520propose%2520Character-Adapter%252C%2520a%2520plug-and-play%2520framework%250Adesigned%2520to%2520generate%2520images%2520that%2520preserve%2520the%2520details%2520of%2520reference%2520characters%252C%250Aensuring%2520high-fidelity%2520consistency.%2520Character-Adapter%2520employs%2520prompt-guided%250Asegmentation%2520to%2520ensure%2520fine-grained%2520regional%2520features%2520of%2520reference%2520characters%250Aand%2520dynamic%2520region-level%2520adapters%2520to%2520mitigate%2520concept%2520confusion.%2520Extensive%250Aexperiments%2520are%2520conducted%2520to%2520validate%2520the%2520effectiveness%2520of%2520Character-Adapter.%250ABoth%2520quantitative%2520and%2520qualitative%2520results%2520demonstrate%2520that%2520Character-Adapter%250Aachieves%2520the%2520state-of-the-art%2520performance%2520of%2520consistent%2520character%2520generation%252C%250Awith%2520an%2520improvement%2520of%252024.8%2525%2520compared%2520with%2520other%2520methods%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Character-Adapter%3A%20Prompt-Guided%20Region%20Control%20for%20High-Fidelity%0A%20%20Character%20Customization&entry.906535625=Yuhang%20Ma%20and%20Wenting%20Xu%20and%20Jiji%20Tang%20and%20Qinfeng%20Jin%20and%20Rongsheng%20Zhang%20and%20Zeng%20Zhao%20and%20Changjie%20Fan%20and%20Zhipeng%20Hu&entry.1292438233=%20%20Customized%20image%20generation%2C%20which%20seeks%20to%20synthesize%20images%20with%20consistent%0Acharacters%2C%20holds%20significant%20relevance%20for%20applications%20such%20as%20storytelling%2C%0Aportrait%20generation%2C%20and%20character%20design.%20However%2C%20previous%20approaches%20have%0Aencountered%20challenges%20in%20preserving%20characters%20with%20high-fidelity%20consistency%0Adue%20to%20inadequate%20feature%20extraction%20and%20concept%20confusion%20of%20reference%0Acharacters.%20Therefore%2C%20we%20propose%20Character-Adapter%2C%20a%20plug-and-play%20framework%0Adesigned%20to%20generate%20images%20that%20preserve%20the%20details%20of%20reference%20characters%2C%0Aensuring%20high-fidelity%20consistency.%20Character-Adapter%20employs%20prompt-guided%0Asegmentation%20to%20ensure%20fine-grained%20regional%20features%20of%20reference%20characters%0Aand%20dynamic%20region-level%20adapters%20to%20mitigate%20concept%20confusion.%20Extensive%0Aexperiments%20are%20conducted%20to%20validate%20the%20effectiveness%20of%20Character-Adapter.%0ABoth%20quantitative%20and%20qualitative%20results%20demonstrate%20that%20Character-Adapter%0Aachieves%20the%20state-of-the-art%20performance%20of%20consistent%20character%20generation%2C%0Awith%20an%20improvement%20of%2024.8%25%20compared%20with%20other%20methods%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16537v1&entry.124074799=Read"},
{"title": "Instance Consistency Regularization for Semi-Supervised 3D Instance\n  Segmentation", "author": "Yizheng Wu and Zhiyu Pan and Kewei Wang and Xingyi Li and Jiahao Cui and Liwen Xiao and Guosheng Lin and Zhiguo Cao", "abstract": "  Large-scale datasets with point-wise semantic and instance labels are crucial\nto 3D instance segmentation but also expensive. To leverage unlabeled data,\nprevious semi-supervised 3D instance segmentation approaches have explored\nself-training frameworks, which rely on high-quality pseudo labels for\nconsistency regularization. They intuitively utilize both instance and semantic\npseudo labels in a joint learning manner. However, semantic pseudo labels\ncontain numerous noise derived from the imbalanced category distribution and\nnatural confusion of similar but distinct categories, which leads to severe\ncollapses in self-training. Motivated by the observation that 3D instances are\nnon-overlapping and spatially separable, we ask whether we can solely rely on\ninstance consistency regularization for improved semi-supervised segmentation.\nTo this end, we propose a novel self-training network InsTeacher3D to explore\nand exploit pure instance knowledge from unlabeled data. We first build a\nparallel base 3D instance segmentation model DKNet, which distinguishes each\ninstance from the others via discriminative instance kernels without reliance\non semantic segmentation. Based on DKNet, we further design a novel instance\nconsistency regularization framework to generate and leverage high-quality\ninstance pseudo labels. Experimental results on multiple large-scale datasets\nshow that the InsTeacher3D significantly outperforms prior state-of-the-art\nsemi-supervised approaches. Code is available:\nhttps://github.com/W1zheng/InsTeacher3D.\n", "link": "http://arxiv.org/abs/2406.16776v1", "date": "2024-06-24", "relevancy": 2.2485, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5781}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5674}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instance%20Consistency%20Regularization%20for%20Semi-Supervised%203D%20Instance%0A%20%20Segmentation&body=Title%3A%20Instance%20Consistency%20Regularization%20for%20Semi-Supervised%203D%20Instance%0A%20%20Segmentation%0AAuthor%3A%20Yizheng%20Wu%20and%20Zhiyu%20Pan%20and%20Kewei%20Wang%20and%20Xingyi%20Li%20and%20Jiahao%20Cui%20and%20Liwen%20Xiao%20and%20Guosheng%20Lin%20and%20Zhiguo%20Cao%0AAbstract%3A%20%20%20Large-scale%20datasets%20with%20point-wise%20semantic%20and%20instance%20labels%20are%20crucial%0Ato%203D%20instance%20segmentation%20but%20also%20expensive.%20To%20leverage%20unlabeled%20data%2C%0Aprevious%20semi-supervised%203D%20instance%20segmentation%20approaches%20have%20explored%0Aself-training%20frameworks%2C%20which%20rely%20on%20high-quality%20pseudo%20labels%20for%0Aconsistency%20regularization.%20They%20intuitively%20utilize%20both%20instance%20and%20semantic%0Apseudo%20labels%20in%20a%20joint%20learning%20manner.%20However%2C%20semantic%20pseudo%20labels%0Acontain%20numerous%20noise%20derived%20from%20the%20imbalanced%20category%20distribution%20and%0Anatural%20confusion%20of%20similar%20but%20distinct%20categories%2C%20which%20leads%20to%20severe%0Acollapses%20in%20self-training.%20Motivated%20by%20the%20observation%20that%203D%20instances%20are%0Anon-overlapping%20and%20spatially%20separable%2C%20we%20ask%20whether%20we%20can%20solely%20rely%20on%0Ainstance%20consistency%20regularization%20for%20improved%20semi-supervised%20segmentation.%0ATo%20this%20end%2C%20we%20propose%20a%20novel%20self-training%20network%20InsTeacher3D%20to%20explore%0Aand%20exploit%20pure%20instance%20knowledge%20from%20unlabeled%20data.%20We%20first%20build%20a%0Aparallel%20base%203D%20instance%20segmentation%20model%20DKNet%2C%20which%20distinguishes%20each%0Ainstance%20from%20the%20others%20via%20discriminative%20instance%20kernels%20without%20reliance%0Aon%20semantic%20segmentation.%20Based%20on%20DKNet%2C%20we%20further%20design%20a%20novel%20instance%0Aconsistency%20regularization%20framework%20to%20generate%20and%20leverage%20high-quality%0Ainstance%20pseudo%20labels.%20Experimental%20results%20on%20multiple%20large-scale%20datasets%0Ashow%20that%20the%20InsTeacher3D%20significantly%20outperforms%20prior%20state-of-the-art%0Asemi-supervised%20approaches.%20Code%20is%20available%3A%0Ahttps%3A//github.com/W1zheng/InsTeacher3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstance%2520Consistency%2520Regularization%2520for%2520Semi-Supervised%25203D%2520Instance%250A%2520%2520Segmentation%26entry.906535625%3DYizheng%2520Wu%2520and%2520Zhiyu%2520Pan%2520and%2520Kewei%2520Wang%2520and%2520Xingyi%2520Li%2520and%2520Jiahao%2520Cui%2520and%2520Liwen%2520Xiao%2520and%2520Guosheng%2520Lin%2520and%2520Zhiguo%2520Cao%26entry.1292438233%3D%2520%2520Large-scale%2520datasets%2520with%2520point-wise%2520semantic%2520and%2520instance%2520labels%2520are%2520crucial%250Ato%25203D%2520instance%2520segmentation%2520but%2520also%2520expensive.%2520To%2520leverage%2520unlabeled%2520data%252C%250Aprevious%2520semi-supervised%25203D%2520instance%2520segmentation%2520approaches%2520have%2520explored%250Aself-training%2520frameworks%252C%2520which%2520rely%2520on%2520high-quality%2520pseudo%2520labels%2520for%250Aconsistency%2520regularization.%2520They%2520intuitively%2520utilize%2520both%2520instance%2520and%2520semantic%250Apseudo%2520labels%2520in%2520a%2520joint%2520learning%2520manner.%2520However%252C%2520semantic%2520pseudo%2520labels%250Acontain%2520numerous%2520noise%2520derived%2520from%2520the%2520imbalanced%2520category%2520distribution%2520and%250Anatural%2520confusion%2520of%2520similar%2520but%2520distinct%2520categories%252C%2520which%2520leads%2520to%2520severe%250Acollapses%2520in%2520self-training.%2520Motivated%2520by%2520the%2520observation%2520that%25203D%2520instances%2520are%250Anon-overlapping%2520and%2520spatially%2520separable%252C%2520we%2520ask%2520whether%2520we%2520can%2520solely%2520rely%2520on%250Ainstance%2520consistency%2520regularization%2520for%2520improved%2520semi-supervised%2520segmentation.%250ATo%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520self-training%2520network%2520InsTeacher3D%2520to%2520explore%250Aand%2520exploit%2520pure%2520instance%2520knowledge%2520from%2520unlabeled%2520data.%2520We%2520first%2520build%2520a%250Aparallel%2520base%25203D%2520instance%2520segmentation%2520model%2520DKNet%252C%2520which%2520distinguishes%2520each%250Ainstance%2520from%2520the%2520others%2520via%2520discriminative%2520instance%2520kernels%2520without%2520reliance%250Aon%2520semantic%2520segmentation.%2520Based%2520on%2520DKNet%252C%2520we%2520further%2520design%2520a%2520novel%2520instance%250Aconsistency%2520regularization%2520framework%2520to%2520generate%2520and%2520leverage%2520high-quality%250Ainstance%2520pseudo%2520labels.%2520Experimental%2520results%2520on%2520multiple%2520large-scale%2520datasets%250Ashow%2520that%2520the%2520InsTeacher3D%2520significantly%2520outperforms%2520prior%2520state-of-the-art%250Asemi-supervised%2520approaches.%2520Code%2520is%2520available%253A%250Ahttps%253A//github.com/W1zheng/InsTeacher3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instance%20Consistency%20Regularization%20for%20Semi-Supervised%203D%20Instance%0A%20%20Segmentation&entry.906535625=Yizheng%20Wu%20and%20Zhiyu%20Pan%20and%20Kewei%20Wang%20and%20Xingyi%20Li%20and%20Jiahao%20Cui%20and%20Liwen%20Xiao%20and%20Guosheng%20Lin%20and%20Zhiguo%20Cao&entry.1292438233=%20%20Large-scale%20datasets%20with%20point-wise%20semantic%20and%20instance%20labels%20are%20crucial%0Ato%203D%20instance%20segmentation%20but%20also%20expensive.%20To%20leverage%20unlabeled%20data%2C%0Aprevious%20semi-supervised%203D%20instance%20segmentation%20approaches%20have%20explored%0Aself-training%20frameworks%2C%20which%20rely%20on%20high-quality%20pseudo%20labels%20for%0Aconsistency%20regularization.%20They%20intuitively%20utilize%20both%20instance%20and%20semantic%0Apseudo%20labels%20in%20a%20joint%20learning%20manner.%20However%2C%20semantic%20pseudo%20labels%0Acontain%20numerous%20noise%20derived%20from%20the%20imbalanced%20category%20distribution%20and%0Anatural%20confusion%20of%20similar%20but%20distinct%20categories%2C%20which%20leads%20to%20severe%0Acollapses%20in%20self-training.%20Motivated%20by%20the%20observation%20that%203D%20instances%20are%0Anon-overlapping%20and%20spatially%20separable%2C%20we%20ask%20whether%20we%20can%20solely%20rely%20on%0Ainstance%20consistency%20regularization%20for%20improved%20semi-supervised%20segmentation.%0ATo%20this%20end%2C%20we%20propose%20a%20novel%20self-training%20network%20InsTeacher3D%20to%20explore%0Aand%20exploit%20pure%20instance%20knowledge%20from%20unlabeled%20data.%20We%20first%20build%20a%0Aparallel%20base%203D%20instance%20segmentation%20model%20DKNet%2C%20which%20distinguishes%20each%0Ainstance%20from%20the%20others%20via%20discriminative%20instance%20kernels%20without%20reliance%0Aon%20semantic%20segmentation.%20Based%20on%20DKNet%2C%20we%20further%20design%20a%20novel%20instance%0Aconsistency%20regularization%20framework%20to%20generate%20and%20leverage%20high-quality%0Ainstance%20pseudo%20labels.%20Experimental%20results%20on%20multiple%20large-scale%20datasets%0Ashow%20that%20the%20InsTeacher3D%20significantly%20outperforms%20prior%20state-of-the-art%0Asemi-supervised%20approaches.%20Code%20is%20available%3A%0Ahttps%3A//github.com/W1zheng/InsTeacher3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16776v1&entry.124074799=Read"},
{"title": "Sim2Real Bilevel Adaptation for Object Surface Classification using\n  Vision-Based Tactile Sensors", "author": "Gabriele M. Caddeo and Andrea Maracani and Paolo D. Alfano and Nicola A. Piga and Lorenzo Rosasco and Lorenzo Natale", "abstract": "  In this paper, we address the Sim2Real gap in the field of vision-based\ntactile sensors for classifying object surfaces. We train a Diffusion Model to\nbridge this gap using a relatively small dataset of real-world images randomly\ncollected from unlabeled everyday objects via the DIGIT sensor. Subsequently,\nwe employ a simulator to generate images by uniformly sampling the surface of\nobjects from the YCB Model Set. These simulated images are then translated into\nthe real domain using the Diffusion Model and automatically labeled to train a\nclassifier. During this training, we further align features of the two domains\nusing an adversarial procedure. Our evaluation is conducted on a dataset of\ntactile images obtained from a set of ten 3D printed YCB objects. The results\nreveal a total accuracy of 81.9%, a significant improvement compared to the\n34.7% achieved by the classifier trained solely on simulated images. This\ndemonstrates the effectiveness of our approach. We further validate our\napproach using the classifier on a 6D object pose estimation task from tactile\ndata.\n", "link": "http://arxiv.org/abs/2311.01380v2", "date": "2024-06-24", "relevancy": 2.2446, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5897}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5635}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sim2Real%20Bilevel%20Adaptation%20for%20Object%20Surface%20Classification%20using%0A%20%20Vision-Based%20Tactile%20Sensors&body=Title%3A%20Sim2Real%20Bilevel%20Adaptation%20for%20Object%20Surface%20Classification%20using%0A%20%20Vision-Based%20Tactile%20Sensors%0AAuthor%3A%20Gabriele%20M.%20Caddeo%20and%20Andrea%20Maracani%20and%20Paolo%20D.%20Alfano%20and%20Nicola%20A.%20Piga%20and%20Lorenzo%20Rosasco%20and%20Lorenzo%20Natale%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20the%20Sim2Real%20gap%20in%20the%20field%20of%20vision-based%0Atactile%20sensors%20for%20classifying%20object%20surfaces.%20We%20train%20a%20Diffusion%20Model%20to%0Abridge%20this%20gap%20using%20a%20relatively%20small%20dataset%20of%20real-world%20images%20randomly%0Acollected%20from%20unlabeled%20everyday%20objects%20via%20the%20DIGIT%20sensor.%20Subsequently%2C%0Awe%20employ%20a%20simulator%20to%20generate%20images%20by%20uniformly%20sampling%20the%20surface%20of%0Aobjects%20from%20the%20YCB%20Model%20Set.%20These%20simulated%20images%20are%20then%20translated%20into%0Athe%20real%20domain%20using%20the%20Diffusion%20Model%20and%20automatically%20labeled%20to%20train%20a%0Aclassifier.%20During%20this%20training%2C%20we%20further%20align%20features%20of%20the%20two%20domains%0Ausing%20an%20adversarial%20procedure.%20Our%20evaluation%20is%20conducted%20on%20a%20dataset%20of%0Atactile%20images%20obtained%20from%20a%20set%20of%20ten%203D%20printed%20YCB%20objects.%20The%20results%0Areveal%20a%20total%20accuracy%20of%2081.9%25%2C%20a%20significant%20improvement%20compared%20to%20the%0A34.7%25%20achieved%20by%20the%20classifier%20trained%20solely%20on%20simulated%20images.%20This%0Ademonstrates%20the%20effectiveness%20of%20our%20approach.%20We%20further%20validate%20our%0Aapproach%20using%20the%20classifier%20on%20a%206D%20object%20pose%20estimation%20task%20from%20tactile%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.01380v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSim2Real%2520Bilevel%2520Adaptation%2520for%2520Object%2520Surface%2520Classification%2520using%250A%2520%2520Vision-Based%2520Tactile%2520Sensors%26entry.906535625%3DGabriele%2520M.%2520Caddeo%2520and%2520Andrea%2520Maracani%2520and%2520Paolo%2520D.%2520Alfano%2520and%2520Nicola%2520A.%2520Piga%2520and%2520Lorenzo%2520Rosasco%2520and%2520Lorenzo%2520Natale%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520Sim2Real%2520gap%2520in%2520the%2520field%2520of%2520vision-based%250Atactile%2520sensors%2520for%2520classifying%2520object%2520surfaces.%2520We%2520train%2520a%2520Diffusion%2520Model%2520to%250Abridge%2520this%2520gap%2520using%2520a%2520relatively%2520small%2520dataset%2520of%2520real-world%2520images%2520randomly%250Acollected%2520from%2520unlabeled%2520everyday%2520objects%2520via%2520the%2520DIGIT%2520sensor.%2520Subsequently%252C%250Awe%2520employ%2520a%2520simulator%2520to%2520generate%2520images%2520by%2520uniformly%2520sampling%2520the%2520surface%2520of%250Aobjects%2520from%2520the%2520YCB%2520Model%2520Set.%2520These%2520simulated%2520images%2520are%2520then%2520translated%2520into%250Athe%2520real%2520domain%2520using%2520the%2520Diffusion%2520Model%2520and%2520automatically%2520labeled%2520to%2520train%2520a%250Aclassifier.%2520During%2520this%2520training%252C%2520we%2520further%2520align%2520features%2520of%2520the%2520two%2520domains%250Ausing%2520an%2520adversarial%2520procedure.%2520Our%2520evaluation%2520is%2520conducted%2520on%2520a%2520dataset%2520of%250Atactile%2520images%2520obtained%2520from%2520a%2520set%2520of%2520ten%25203D%2520printed%2520YCB%2520objects.%2520The%2520results%250Areveal%2520a%2520total%2520accuracy%2520of%252081.9%2525%252C%2520a%2520significant%2520improvement%2520compared%2520to%2520the%250A34.7%2525%2520achieved%2520by%2520the%2520classifier%2520trained%2520solely%2520on%2520simulated%2520images.%2520This%250Ademonstrates%2520the%2520effectiveness%2520of%2520our%2520approach.%2520We%2520further%2520validate%2520our%250Aapproach%2520using%2520the%2520classifier%2520on%2520a%25206D%2520object%2520pose%2520estimation%2520task%2520from%2520tactile%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.01380v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sim2Real%20Bilevel%20Adaptation%20for%20Object%20Surface%20Classification%20using%0A%20%20Vision-Based%20Tactile%20Sensors&entry.906535625=Gabriele%20M.%20Caddeo%20and%20Andrea%20Maracani%20and%20Paolo%20D.%20Alfano%20and%20Nicola%20A.%20Piga%20and%20Lorenzo%20Rosasco%20and%20Lorenzo%20Natale&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20Sim2Real%20gap%20in%20the%20field%20of%20vision-based%0Atactile%20sensors%20for%20classifying%20object%20surfaces.%20We%20train%20a%20Diffusion%20Model%20to%0Abridge%20this%20gap%20using%20a%20relatively%20small%20dataset%20of%20real-world%20images%20randomly%0Acollected%20from%20unlabeled%20everyday%20objects%20via%20the%20DIGIT%20sensor.%20Subsequently%2C%0Awe%20employ%20a%20simulator%20to%20generate%20images%20by%20uniformly%20sampling%20the%20surface%20of%0Aobjects%20from%20the%20YCB%20Model%20Set.%20These%20simulated%20images%20are%20then%20translated%20into%0Athe%20real%20domain%20using%20the%20Diffusion%20Model%20and%20automatically%20labeled%20to%20train%20a%0Aclassifier.%20During%20this%20training%2C%20we%20further%20align%20features%20of%20the%20two%20domains%0Ausing%20an%20adversarial%20procedure.%20Our%20evaluation%20is%20conducted%20on%20a%20dataset%20of%0Atactile%20images%20obtained%20from%20a%20set%20of%20ten%203D%20printed%20YCB%20objects.%20The%20results%0Areveal%20a%20total%20accuracy%20of%2081.9%25%2C%20a%20significant%20improvement%20compared%20to%20the%0A34.7%25%20achieved%20by%20the%20classifier%20trained%20solely%20on%20simulated%20images.%20This%0Ademonstrates%20the%20effectiveness%20of%20our%20approach.%20We%20further%20validate%20our%0Aapproach%20using%20the%20classifier%20on%20a%206D%20object%20pose%20estimation%20task%20from%20tactile%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.01380v2&entry.124074799=Read"},
{"title": "Vision Beyond Boundaries: An Initial Design Space of Domain-specific\n  Large Vision Models in Human-robot Interaction", "author": "Yuchong Zhang and Yong Ma and Danica Kragic", "abstract": "  The emergence of large vision models (LVMs) is following in the footsteps of\nthe recent prosperity of Large Language Models (LLMs) in following years.\nHowever, there's a noticeable gap in structured research applying LVMs to\nhuman-robot interaction (HRI), despite extensive evidence supporting the\nefficacy of vision models in enhancing interactions between humans and robots.\nRecognizing the vast and anticipated potential, we introduce an initial design\nspace that incorporates domain-specific LVMs, chosen for their superior\nperformance over normal models. We delve into three primary dimensions: HRI\ncontexts, vision-based tasks, and specific domains. The empirical evaluation\nwas implemented among 15 experts across six evaluated metrics, showcasing the\nprimary efficacy in relevant decision-making scenarios. We explore the process\nof ideation and potential application scenarios, envisioning this design space\nas a foundational guideline for future HRI system design, emphasizing accurate\ndomain alignment and model selection.\n", "link": "http://arxiv.org/abs/2404.14965v4", "date": "2024-06-24", "relevancy": 2.2398, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5672}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5615}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Beyond%20Boundaries%3A%20An%20Initial%20Design%20Space%20of%20Domain-specific%0A%20%20Large%20Vision%20Models%20in%20Human-robot%20Interaction&body=Title%3A%20Vision%20Beyond%20Boundaries%3A%20An%20Initial%20Design%20Space%20of%20Domain-specific%0A%20%20Large%20Vision%20Models%20in%20Human-robot%20Interaction%0AAuthor%3A%20Yuchong%20Zhang%20and%20Yong%20Ma%20and%20Danica%20Kragic%0AAbstract%3A%20%20%20The%20emergence%20of%20large%20vision%20models%20%28LVMs%29%20is%20following%20in%20the%20footsteps%20of%0Athe%20recent%20prosperity%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20following%20years.%0AHowever%2C%20there%27s%20a%20noticeable%20gap%20in%20structured%20research%20applying%20LVMs%20to%0Ahuman-robot%20interaction%20%28HRI%29%2C%20despite%20extensive%20evidence%20supporting%20the%0Aefficacy%20of%20vision%20models%20in%20enhancing%20interactions%20between%20humans%20and%20robots.%0ARecognizing%20the%20vast%20and%20anticipated%20potential%2C%20we%20introduce%20an%20initial%20design%0Aspace%20that%20incorporates%20domain-specific%20LVMs%2C%20chosen%20for%20their%20superior%0Aperformance%20over%20normal%20models.%20We%20delve%20into%20three%20primary%20dimensions%3A%20HRI%0Acontexts%2C%20vision-based%20tasks%2C%20and%20specific%20domains.%20The%20empirical%20evaluation%0Awas%20implemented%20among%2015%20experts%20across%20six%20evaluated%20metrics%2C%20showcasing%20the%0Aprimary%20efficacy%20in%20relevant%20decision-making%20scenarios.%20We%20explore%20the%20process%0Aof%20ideation%20and%20potential%20application%20scenarios%2C%20envisioning%20this%20design%20space%0Aas%20a%20foundational%20guideline%20for%20future%20HRI%20system%20design%2C%20emphasizing%20accurate%0Adomain%20alignment%20and%20model%20selection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14965v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Beyond%2520Boundaries%253A%2520An%2520Initial%2520Design%2520Space%2520of%2520Domain-specific%250A%2520%2520Large%2520Vision%2520Models%2520in%2520Human-robot%2520Interaction%26entry.906535625%3DYuchong%2520Zhang%2520and%2520Yong%2520Ma%2520and%2520Danica%2520Kragic%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520large%2520vision%2520models%2520%2528LVMs%2529%2520is%2520following%2520in%2520the%2520footsteps%2520of%250Athe%2520recent%2520prosperity%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520following%2520years.%250AHowever%252C%2520there%2527s%2520a%2520noticeable%2520gap%2520in%2520structured%2520research%2520applying%2520LVMs%2520to%250Ahuman-robot%2520interaction%2520%2528HRI%2529%252C%2520despite%2520extensive%2520evidence%2520supporting%2520the%250Aefficacy%2520of%2520vision%2520models%2520in%2520enhancing%2520interactions%2520between%2520humans%2520and%2520robots.%250ARecognizing%2520the%2520vast%2520and%2520anticipated%2520potential%252C%2520we%2520introduce%2520an%2520initial%2520design%250Aspace%2520that%2520incorporates%2520domain-specific%2520LVMs%252C%2520chosen%2520for%2520their%2520superior%250Aperformance%2520over%2520normal%2520models.%2520We%2520delve%2520into%2520three%2520primary%2520dimensions%253A%2520HRI%250Acontexts%252C%2520vision-based%2520tasks%252C%2520and%2520specific%2520domains.%2520The%2520empirical%2520evaluation%250Awas%2520implemented%2520among%252015%2520experts%2520across%2520six%2520evaluated%2520metrics%252C%2520showcasing%2520the%250Aprimary%2520efficacy%2520in%2520relevant%2520decision-making%2520scenarios.%2520We%2520explore%2520the%2520process%250Aof%2520ideation%2520and%2520potential%2520application%2520scenarios%252C%2520envisioning%2520this%2520design%2520space%250Aas%2520a%2520foundational%2520guideline%2520for%2520future%2520HRI%2520system%2520design%252C%2520emphasizing%2520accurate%250Adomain%2520alignment%2520and%2520model%2520selection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14965v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Beyond%20Boundaries%3A%20An%20Initial%20Design%20Space%20of%20Domain-specific%0A%20%20Large%20Vision%20Models%20in%20Human-robot%20Interaction&entry.906535625=Yuchong%20Zhang%20and%20Yong%20Ma%20and%20Danica%20Kragic&entry.1292438233=%20%20The%20emergence%20of%20large%20vision%20models%20%28LVMs%29%20is%20following%20in%20the%20footsteps%20of%0Athe%20recent%20prosperity%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20following%20years.%0AHowever%2C%20there%27s%20a%20noticeable%20gap%20in%20structured%20research%20applying%20LVMs%20to%0Ahuman-robot%20interaction%20%28HRI%29%2C%20despite%20extensive%20evidence%20supporting%20the%0Aefficacy%20of%20vision%20models%20in%20enhancing%20interactions%20between%20humans%20and%20robots.%0ARecognizing%20the%20vast%20and%20anticipated%20potential%2C%20we%20introduce%20an%20initial%20design%0Aspace%20that%20incorporates%20domain-specific%20LVMs%2C%20chosen%20for%20their%20superior%0Aperformance%20over%20normal%20models.%20We%20delve%20into%20three%20primary%20dimensions%3A%20HRI%0Acontexts%2C%20vision-based%20tasks%2C%20and%20specific%20domains.%20The%20empirical%20evaluation%0Awas%20implemented%20among%2015%20experts%20across%20six%20evaluated%20metrics%2C%20showcasing%20the%0Aprimary%20efficacy%20in%20relevant%20decision-making%20scenarios.%20We%20explore%20the%20process%0Aof%20ideation%20and%20potential%20application%20scenarios%2C%20envisioning%20this%20design%20space%0Aas%20a%20foundational%20guideline%20for%20future%20HRI%20system%20design%2C%20emphasizing%20accurate%0Adomain%20alignment%20and%20model%20selection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14965v4&entry.124074799=Read"},
{"title": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs", "author": "Shengbang Tong and Ellis Brown and Penghao Wu and Sanghyun Woo and Manoj Middepogu and Sai Charitha Akula and Jihan Yang and Shusheng Yang and Adithya Iyer and Xichen Pan and Austin Wang and Rob Fergus and Yann LeCun and Saining Xie", "abstract": "  We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a\nvision-centric approach. While stronger language models can enhance multimodal\ncapabilities, the design choices for vision components are often insufficiently\nexplored and disconnected from visual representation learning research. This\ngap hinders accurate sensory grounding in real-world scenarios. Our study uses\nLLMs and visual instruction tuning as an interface to evaluate various visual\nrepresentations, offering new insights into different models and architectures\n-- self-supervised, strongly supervised, or combinations thereof -- based on\nexperiments with over 20 vision encoders. We critically examine existing MLLM\nbenchmarks, addressing the difficulties involved in consolidating and\ninterpreting results from various tasks, and introduce a new vision-centric\nbenchmark, CV-Bench. To further improve visual grounding, we propose the\nSpatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that\nintegrates high-resolution vision features with LLMs while reducing the number\nof tokens. Additionally, we discuss the curation of high-quality visual\ninstruction-tuning data from publicly available sources, emphasizing the\nimportance of data source balancing and distribution ratio. Collectively,\nCambrian-1 not only achieves state-of-the-art performance but also serves as a\ncomprehensive, open cookbook for instruction-tuned MLLMs. We provide model\nweights, code, supporting tools, datasets, and detailed instruction-tuning and\nevaluation recipes. We hope our release will inspire and accelerate\nadvancements in multimodal systems and visual representation learning.\n", "link": "http://arxiv.org/abs/2406.16860v1", "date": "2024-06-24", "relevancy": 2.2296, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5861}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5661}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cambrian-1%3A%20A%20Fully%20Open%2C%20Vision-Centric%20Exploration%20of%20Multimodal%20LLMs&body=Title%3A%20Cambrian-1%3A%20A%20Fully%20Open%2C%20Vision-Centric%20Exploration%20of%20Multimodal%20LLMs%0AAuthor%3A%20Shengbang%20Tong%20and%20Ellis%20Brown%20and%20Penghao%20Wu%20and%20Sanghyun%20Woo%20and%20Manoj%20Middepogu%20and%20Sai%20Charitha%20Akula%20and%20Jihan%20Yang%20and%20Shusheng%20Yang%20and%20Adithya%20Iyer%20and%20Xichen%20Pan%20and%20Austin%20Wang%20and%20Rob%20Fergus%20and%20Yann%20LeCun%20and%20Saining%20Xie%0AAbstract%3A%20%20%20We%20introduce%20Cambrian-1%2C%20a%20family%20of%20multimodal%20LLMs%20%28MLLMs%29%20designed%20with%20a%0Avision-centric%20approach.%20While%20stronger%20language%20models%20can%20enhance%20multimodal%0Acapabilities%2C%20the%20design%20choices%20for%20vision%20components%20are%20often%20insufficiently%0Aexplored%20and%20disconnected%20from%20visual%20representation%20learning%20research.%20This%0Agap%20hinders%20accurate%20sensory%20grounding%20in%20real-world%20scenarios.%20Our%20study%20uses%0ALLMs%20and%20visual%20instruction%20tuning%20as%20an%20interface%20to%20evaluate%20various%20visual%0Arepresentations%2C%20offering%20new%20insights%20into%20different%20models%20and%20architectures%0A--%20self-supervised%2C%20strongly%20supervised%2C%20or%20combinations%20thereof%20--%20based%20on%0Aexperiments%20with%20over%2020%20vision%20encoders.%20We%20critically%20examine%20existing%20MLLM%0Abenchmarks%2C%20addressing%20the%20difficulties%20involved%20in%20consolidating%20and%0Ainterpreting%20results%20from%20various%20tasks%2C%20and%20introduce%20a%20new%20vision-centric%0Abenchmark%2C%20CV-Bench.%20To%20further%20improve%20visual%20grounding%2C%20we%20propose%20the%0ASpatial%20Vision%20Aggregator%20%28SVA%29%2C%20a%20dynamic%20and%20spatially-aware%20connector%20that%0Aintegrates%20high-resolution%20vision%20features%20with%20LLMs%20while%20reducing%20the%20number%0Aof%20tokens.%20Additionally%2C%20we%20discuss%20the%20curation%20of%20high-quality%20visual%0Ainstruction-tuning%20data%20from%20publicly%20available%20sources%2C%20emphasizing%20the%0Aimportance%20of%20data%20source%20balancing%20and%20distribution%20ratio.%20Collectively%2C%0ACambrian-1%20not%20only%20achieves%20state-of-the-art%20performance%20but%20also%20serves%20as%20a%0Acomprehensive%2C%20open%20cookbook%20for%20instruction-tuned%20MLLMs.%20We%20provide%20model%0Aweights%2C%20code%2C%20supporting%20tools%2C%20datasets%2C%20and%20detailed%20instruction-tuning%20and%0Aevaluation%20recipes.%20We%20hope%20our%20release%20will%20inspire%20and%20accelerate%0Aadvancements%20in%20multimodal%20systems%20and%20visual%20representation%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCambrian-1%253A%2520A%2520Fully%2520Open%252C%2520Vision-Centric%2520Exploration%2520of%2520Multimodal%2520LLMs%26entry.906535625%3DShengbang%2520Tong%2520and%2520Ellis%2520Brown%2520and%2520Penghao%2520Wu%2520and%2520Sanghyun%2520Woo%2520and%2520Manoj%2520Middepogu%2520and%2520Sai%2520Charitha%2520Akula%2520and%2520Jihan%2520Yang%2520and%2520Shusheng%2520Yang%2520and%2520Adithya%2520Iyer%2520and%2520Xichen%2520Pan%2520and%2520Austin%2520Wang%2520and%2520Rob%2520Fergus%2520and%2520Yann%2520LeCun%2520and%2520Saining%2520Xie%26entry.1292438233%3D%2520%2520We%2520introduce%2520Cambrian-1%252C%2520a%2520family%2520of%2520multimodal%2520LLMs%2520%2528MLLMs%2529%2520designed%2520with%2520a%250Avision-centric%2520approach.%2520While%2520stronger%2520language%2520models%2520can%2520enhance%2520multimodal%250Acapabilities%252C%2520the%2520design%2520choices%2520for%2520vision%2520components%2520are%2520often%2520insufficiently%250Aexplored%2520and%2520disconnected%2520from%2520visual%2520representation%2520learning%2520research.%2520This%250Agap%2520hinders%2520accurate%2520sensory%2520grounding%2520in%2520real-world%2520scenarios.%2520Our%2520study%2520uses%250ALLMs%2520and%2520visual%2520instruction%2520tuning%2520as%2520an%2520interface%2520to%2520evaluate%2520various%2520visual%250Arepresentations%252C%2520offering%2520new%2520insights%2520into%2520different%2520models%2520and%2520architectures%250A--%2520self-supervised%252C%2520strongly%2520supervised%252C%2520or%2520combinations%2520thereof%2520--%2520based%2520on%250Aexperiments%2520with%2520over%252020%2520vision%2520encoders.%2520We%2520critically%2520examine%2520existing%2520MLLM%250Abenchmarks%252C%2520addressing%2520the%2520difficulties%2520involved%2520in%2520consolidating%2520and%250Ainterpreting%2520results%2520from%2520various%2520tasks%252C%2520and%2520introduce%2520a%2520new%2520vision-centric%250Abenchmark%252C%2520CV-Bench.%2520To%2520further%2520improve%2520visual%2520grounding%252C%2520we%2520propose%2520the%250ASpatial%2520Vision%2520Aggregator%2520%2528SVA%2529%252C%2520a%2520dynamic%2520and%2520spatially-aware%2520connector%2520that%250Aintegrates%2520high-resolution%2520vision%2520features%2520with%2520LLMs%2520while%2520reducing%2520the%2520number%250Aof%2520tokens.%2520Additionally%252C%2520we%2520discuss%2520the%2520curation%2520of%2520high-quality%2520visual%250Ainstruction-tuning%2520data%2520from%2520publicly%2520available%2520sources%252C%2520emphasizing%2520the%250Aimportance%2520of%2520data%2520source%2520balancing%2520and%2520distribution%2520ratio.%2520Collectively%252C%250ACambrian-1%2520not%2520only%2520achieves%2520state-of-the-art%2520performance%2520but%2520also%2520serves%2520as%2520a%250Acomprehensive%252C%2520open%2520cookbook%2520for%2520instruction-tuned%2520MLLMs.%2520We%2520provide%2520model%250Aweights%252C%2520code%252C%2520supporting%2520tools%252C%2520datasets%252C%2520and%2520detailed%2520instruction-tuning%2520and%250Aevaluation%2520recipes.%2520We%2520hope%2520our%2520release%2520will%2520inspire%2520and%2520accelerate%250Aadvancements%2520in%2520multimodal%2520systems%2520and%2520visual%2520representation%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cambrian-1%3A%20A%20Fully%20Open%2C%20Vision-Centric%20Exploration%20of%20Multimodal%20LLMs&entry.906535625=Shengbang%20Tong%20and%20Ellis%20Brown%20and%20Penghao%20Wu%20and%20Sanghyun%20Woo%20and%20Manoj%20Middepogu%20and%20Sai%20Charitha%20Akula%20and%20Jihan%20Yang%20and%20Shusheng%20Yang%20and%20Adithya%20Iyer%20and%20Xichen%20Pan%20and%20Austin%20Wang%20and%20Rob%20Fergus%20and%20Yann%20LeCun%20and%20Saining%20Xie&entry.1292438233=%20%20We%20introduce%20Cambrian-1%2C%20a%20family%20of%20multimodal%20LLMs%20%28MLLMs%29%20designed%20with%20a%0Avision-centric%20approach.%20While%20stronger%20language%20models%20can%20enhance%20multimodal%0Acapabilities%2C%20the%20design%20choices%20for%20vision%20components%20are%20often%20insufficiently%0Aexplored%20and%20disconnected%20from%20visual%20representation%20learning%20research.%20This%0Agap%20hinders%20accurate%20sensory%20grounding%20in%20real-world%20scenarios.%20Our%20study%20uses%0ALLMs%20and%20visual%20instruction%20tuning%20as%20an%20interface%20to%20evaluate%20various%20visual%0Arepresentations%2C%20offering%20new%20insights%20into%20different%20models%20and%20architectures%0A--%20self-supervised%2C%20strongly%20supervised%2C%20or%20combinations%20thereof%20--%20based%20on%0Aexperiments%20with%20over%2020%20vision%20encoders.%20We%20critically%20examine%20existing%20MLLM%0Abenchmarks%2C%20addressing%20the%20difficulties%20involved%20in%20consolidating%20and%0Ainterpreting%20results%20from%20various%20tasks%2C%20and%20introduce%20a%20new%20vision-centric%0Abenchmark%2C%20CV-Bench.%20To%20further%20improve%20visual%20grounding%2C%20we%20propose%20the%0ASpatial%20Vision%20Aggregator%20%28SVA%29%2C%20a%20dynamic%20and%20spatially-aware%20connector%20that%0Aintegrates%20high-resolution%20vision%20features%20with%20LLMs%20while%20reducing%20the%20number%0Aof%20tokens.%20Additionally%2C%20we%20discuss%20the%20curation%20of%20high-quality%20visual%0Ainstruction-tuning%20data%20from%20publicly%20available%20sources%2C%20emphasizing%20the%0Aimportance%20of%20data%20source%20balancing%20and%20distribution%20ratio.%20Collectively%2C%0ACambrian-1%20not%20only%20achieves%20state-of-the-art%20performance%20but%20also%20serves%20as%20a%0Acomprehensive%2C%20open%20cookbook%20for%20instruction-tuned%20MLLMs.%20We%20provide%20model%0Aweights%2C%20code%2C%20supporting%20tools%2C%20datasets%2C%20and%20detailed%20instruction-tuning%20and%0Aevaluation%20recipes.%20We%20hope%20our%20release%20will%20inspire%20and%20accelerate%0Aadvancements%20in%20multimodal%20systems%20and%20visual%20representation%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16860v1&entry.124074799=Read"},
{"title": "FASTC: A Fast Attentional Framework for Semantic Traversability\n  Classification Using Point Cloud", "author": "Yirui Chen and Pengjin Wei and Zhenhuan Liu and Bingchao Wang and Jie Yang and Wei Liu", "abstract": "  Producing traversability maps and understanding the surroundings are crucial\nprerequisites for autonomous navigation. In this paper, we address the problem\nof traversability assessment using point clouds. We propose a novel pillar\nfeature extraction module that utilizes PointNet to capture features from point\nclouds organized in vertical volume and a 2D encoder-decoder structure to\nconduct traversability classification instead of the widely used 3D\nconvolutions. This results in less computational cost while even better\nperformance is achieved at the same time. We then propose a new spatio-temporal\nattention module to fuse multi-frame information, which can properly handle the\nvarying density problem of LIDAR point clouds, and this makes our module able\nto assess distant areas more accurately. Comprehensive experimental results on\naugmented Semantic KITTI and RELLIS-3D datasets show that our method is able to\nachieve superior performance over existing approaches both quantitatively and\nquantitatively.\n", "link": "http://arxiv.org/abs/2406.16564v1", "date": "2024-06-24", "relevancy": 2.2142, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.565}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5487}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FASTC%3A%20A%20Fast%20Attentional%20Framework%20for%20Semantic%20Traversability%0A%20%20Classification%20Using%20Point%20Cloud&body=Title%3A%20FASTC%3A%20A%20Fast%20Attentional%20Framework%20for%20Semantic%20Traversability%0A%20%20Classification%20Using%20Point%20Cloud%0AAuthor%3A%20Yirui%20Chen%20and%20Pengjin%20Wei%20and%20Zhenhuan%20Liu%20and%20Bingchao%20Wang%20and%20Jie%20Yang%20and%20Wei%20Liu%0AAbstract%3A%20%20%20Producing%20traversability%20maps%20and%20understanding%20the%20surroundings%20are%20crucial%0Aprerequisites%20for%20autonomous%20navigation.%20In%20this%20paper%2C%20we%20address%20the%20problem%0Aof%20traversability%20assessment%20using%20point%20clouds.%20We%20propose%20a%20novel%20pillar%0Afeature%20extraction%20module%20that%20utilizes%20PointNet%20to%20capture%20features%20from%20point%0Aclouds%20organized%20in%20vertical%20volume%20and%20a%202D%20encoder-decoder%20structure%20to%0Aconduct%20traversability%20classification%20instead%20of%20the%20widely%20used%203D%0Aconvolutions.%20This%20results%20in%20less%20computational%20cost%20while%20even%20better%0Aperformance%20is%20achieved%20at%20the%20same%20time.%20We%20then%20propose%20a%20new%20spatio-temporal%0Aattention%20module%20to%20fuse%20multi-frame%20information%2C%20which%20can%20properly%20handle%20the%0Avarying%20density%20problem%20of%20LIDAR%20point%20clouds%2C%20and%20this%20makes%20our%20module%20able%0Ato%20assess%20distant%20areas%20more%20accurately.%20Comprehensive%20experimental%20results%20on%0Aaugmented%20Semantic%20KITTI%20and%20RELLIS-3D%20datasets%20show%20that%20our%20method%20is%20able%20to%0Aachieve%20superior%20performance%20over%20existing%20approaches%20both%20quantitatively%20and%0Aquantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFASTC%253A%2520A%2520Fast%2520Attentional%2520Framework%2520for%2520Semantic%2520Traversability%250A%2520%2520Classification%2520Using%2520Point%2520Cloud%26entry.906535625%3DYirui%2520Chen%2520and%2520Pengjin%2520Wei%2520and%2520Zhenhuan%2520Liu%2520and%2520Bingchao%2520Wang%2520and%2520Jie%2520Yang%2520and%2520Wei%2520Liu%26entry.1292438233%3D%2520%2520Producing%2520traversability%2520maps%2520and%2520understanding%2520the%2520surroundings%2520are%2520crucial%250Aprerequisites%2520for%2520autonomous%2520navigation.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520problem%250Aof%2520traversability%2520assessment%2520using%2520point%2520clouds.%2520We%2520propose%2520a%2520novel%2520pillar%250Afeature%2520extraction%2520module%2520that%2520utilizes%2520PointNet%2520to%2520capture%2520features%2520from%2520point%250Aclouds%2520organized%2520in%2520vertical%2520volume%2520and%2520a%25202D%2520encoder-decoder%2520structure%2520to%250Aconduct%2520traversability%2520classification%2520instead%2520of%2520the%2520widely%2520used%25203D%250Aconvolutions.%2520This%2520results%2520in%2520less%2520computational%2520cost%2520while%2520even%2520better%250Aperformance%2520is%2520achieved%2520at%2520the%2520same%2520time.%2520We%2520then%2520propose%2520a%2520new%2520spatio-temporal%250Aattention%2520module%2520to%2520fuse%2520multi-frame%2520information%252C%2520which%2520can%2520properly%2520handle%2520the%250Avarying%2520density%2520problem%2520of%2520LIDAR%2520point%2520clouds%252C%2520and%2520this%2520makes%2520our%2520module%2520able%250Ato%2520assess%2520distant%2520areas%2520more%2520accurately.%2520Comprehensive%2520experimental%2520results%2520on%250Aaugmented%2520Semantic%2520KITTI%2520and%2520RELLIS-3D%2520datasets%2520show%2520that%2520our%2520method%2520is%2520able%2520to%250Aachieve%2520superior%2520performance%2520over%2520existing%2520approaches%2520both%2520quantitatively%2520and%250Aquantitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FASTC%3A%20A%20Fast%20Attentional%20Framework%20for%20Semantic%20Traversability%0A%20%20Classification%20Using%20Point%20Cloud&entry.906535625=Yirui%20Chen%20and%20Pengjin%20Wei%20and%20Zhenhuan%20Liu%20and%20Bingchao%20Wang%20and%20Jie%20Yang%20and%20Wei%20Liu&entry.1292438233=%20%20Producing%20traversability%20maps%20and%20understanding%20the%20surroundings%20are%20crucial%0Aprerequisites%20for%20autonomous%20navigation.%20In%20this%20paper%2C%20we%20address%20the%20problem%0Aof%20traversability%20assessment%20using%20point%20clouds.%20We%20propose%20a%20novel%20pillar%0Afeature%20extraction%20module%20that%20utilizes%20PointNet%20to%20capture%20features%20from%20point%0Aclouds%20organized%20in%20vertical%20volume%20and%20a%202D%20encoder-decoder%20structure%20to%0Aconduct%20traversability%20classification%20instead%20of%20the%20widely%20used%203D%0Aconvolutions.%20This%20results%20in%20less%20computational%20cost%20while%20even%20better%0Aperformance%20is%20achieved%20at%20the%20same%20time.%20We%20then%20propose%20a%20new%20spatio-temporal%0Aattention%20module%20to%20fuse%20multi-frame%20information%2C%20which%20can%20properly%20handle%20the%0Avarying%20density%20problem%20of%20LIDAR%20point%20clouds%2C%20and%20this%20makes%20our%20module%20able%0Ato%20assess%20distant%20areas%20more%20accurately.%20Comprehensive%20experimental%20results%20on%0Aaugmented%20Semantic%20KITTI%20and%20RELLIS-3D%20datasets%20show%20that%20our%20method%20is%20able%20to%0Aachieve%20superior%20performance%20over%20existing%20approaches%20both%20quantitatively%20and%0Aquantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16564v1&entry.124074799=Read"},
{"title": "Can Protective Perturbation Safeguard Personal Data from Being Exploited\n  by Stable Diffusion?", "author": "Zhengyue Zhao and Jinhao Duan and Kaidi Xu and Chenan Wang and Rui Zhang and Zidong Du and Qi Guo and Xing Hu", "abstract": "  Stable Diffusion has established itself as a foundation model in generative\nAI artistic applications, receiving widespread research and application. Some\nrecent fine-tuning methods have made it feasible for individuals to implant\npersonalized concepts onto the basic Stable Diffusion model with minimal\ncomputational costs on small datasets. However, these innovations have also\ngiven rise to issues like facial privacy forgery and artistic copyright\ninfringement. In recent studies, researchers have explored the addition of\nimperceptible adversarial perturbations to images to prevent potential\nunauthorized exploitation and infringements when personal data is used for\nfine-tuning Stable Diffusion. Although these studies have demonstrated the\nability to protect images, it is essential to consider that these methods may\nnot be entirely applicable in real-world scenarios. In this paper, we\nsystematically evaluate the use of perturbations to protect images within a\npractical threat model. The results suggest that these approaches may not be\nsufficient to safeguard image privacy and copyright effectively. Furthermore,\nwe introduce a purification method capable of removing protected perturbations\nwhile preserving the original image structure to the greatest extent possible.\nExperiments reveal that Stable Diffusion can effectively learn from purified\nimages over all protective methods.\n", "link": "http://arxiv.org/abs/2312.00084v2", "date": "2024-06-24", "relevancy": 2.213, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5548}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5526}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Protective%20Perturbation%20Safeguard%20Personal%20Data%20from%20Being%20Exploited%0A%20%20by%20Stable%20Diffusion%3F&body=Title%3A%20Can%20Protective%20Perturbation%20Safeguard%20Personal%20Data%20from%20Being%20Exploited%0A%20%20by%20Stable%20Diffusion%3F%0AAuthor%3A%20Zhengyue%20Zhao%20and%20Jinhao%20Duan%20and%20Kaidi%20Xu%20and%20Chenan%20Wang%20and%20Rui%20Zhang%20and%20Zidong%20Du%20and%20Qi%20Guo%20and%20Xing%20Hu%0AAbstract%3A%20%20%20Stable%20Diffusion%20has%20established%20itself%20as%20a%20foundation%20model%20in%20generative%0AAI%20artistic%20applications%2C%20receiving%20widespread%20research%20and%20application.%20Some%0Arecent%20fine-tuning%20methods%20have%20made%20it%20feasible%20for%20individuals%20to%20implant%0Apersonalized%20concepts%20onto%20the%20basic%20Stable%20Diffusion%20model%20with%20minimal%0Acomputational%20costs%20on%20small%20datasets.%20However%2C%20these%20innovations%20have%20also%0Agiven%20rise%20to%20issues%20like%20facial%20privacy%20forgery%20and%20artistic%20copyright%0Ainfringement.%20In%20recent%20studies%2C%20researchers%20have%20explored%20the%20addition%20of%0Aimperceptible%20adversarial%20perturbations%20to%20images%20to%20prevent%20potential%0Aunauthorized%20exploitation%20and%20infringements%20when%20personal%20data%20is%20used%20for%0Afine-tuning%20Stable%20Diffusion.%20Although%20these%20studies%20have%20demonstrated%20the%0Aability%20to%20protect%20images%2C%20it%20is%20essential%20to%20consider%20that%20these%20methods%20may%0Anot%20be%20entirely%20applicable%20in%20real-world%20scenarios.%20In%20this%20paper%2C%20we%0Asystematically%20evaluate%20the%20use%20of%20perturbations%20to%20protect%20images%20within%20a%0Apractical%20threat%20model.%20The%20results%20suggest%20that%20these%20approaches%20may%20not%20be%0Asufficient%20to%20safeguard%20image%20privacy%20and%20copyright%20effectively.%20Furthermore%2C%0Awe%20introduce%20a%20purification%20method%20capable%20of%20removing%20protected%20perturbations%0Awhile%20preserving%20the%20original%20image%20structure%20to%20the%20greatest%20extent%20possible.%0AExperiments%20reveal%20that%20Stable%20Diffusion%20can%20effectively%20learn%20from%20purified%0Aimages%20over%20all%20protective%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00084v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Protective%2520Perturbation%2520Safeguard%2520Personal%2520Data%2520from%2520Being%2520Exploited%250A%2520%2520by%2520Stable%2520Diffusion%253F%26entry.906535625%3DZhengyue%2520Zhao%2520and%2520Jinhao%2520Duan%2520and%2520Kaidi%2520Xu%2520and%2520Chenan%2520Wang%2520and%2520Rui%2520Zhang%2520and%2520Zidong%2520Du%2520and%2520Qi%2520Guo%2520and%2520Xing%2520Hu%26entry.1292438233%3D%2520%2520Stable%2520Diffusion%2520has%2520established%2520itself%2520as%2520a%2520foundation%2520model%2520in%2520generative%250AAI%2520artistic%2520applications%252C%2520receiving%2520widespread%2520research%2520and%2520application.%2520Some%250Arecent%2520fine-tuning%2520methods%2520have%2520made%2520it%2520feasible%2520for%2520individuals%2520to%2520implant%250Apersonalized%2520concepts%2520onto%2520the%2520basic%2520Stable%2520Diffusion%2520model%2520with%2520minimal%250Acomputational%2520costs%2520on%2520small%2520datasets.%2520However%252C%2520these%2520innovations%2520have%2520also%250Agiven%2520rise%2520to%2520issues%2520like%2520facial%2520privacy%2520forgery%2520and%2520artistic%2520copyright%250Ainfringement.%2520In%2520recent%2520studies%252C%2520researchers%2520have%2520explored%2520the%2520addition%2520of%250Aimperceptible%2520adversarial%2520perturbations%2520to%2520images%2520to%2520prevent%2520potential%250Aunauthorized%2520exploitation%2520and%2520infringements%2520when%2520personal%2520data%2520is%2520used%2520for%250Afine-tuning%2520Stable%2520Diffusion.%2520Although%2520these%2520studies%2520have%2520demonstrated%2520the%250Aability%2520to%2520protect%2520images%252C%2520it%2520is%2520essential%2520to%2520consider%2520that%2520these%2520methods%2520may%250Anot%2520be%2520entirely%2520applicable%2520in%2520real-world%2520scenarios.%2520In%2520this%2520paper%252C%2520we%250Asystematically%2520evaluate%2520the%2520use%2520of%2520perturbations%2520to%2520protect%2520images%2520within%2520a%250Apractical%2520threat%2520model.%2520The%2520results%2520suggest%2520that%2520these%2520approaches%2520may%2520not%2520be%250Asufficient%2520to%2520safeguard%2520image%2520privacy%2520and%2520copyright%2520effectively.%2520Furthermore%252C%250Awe%2520introduce%2520a%2520purification%2520method%2520capable%2520of%2520removing%2520protected%2520perturbations%250Awhile%2520preserving%2520the%2520original%2520image%2520structure%2520to%2520the%2520greatest%2520extent%2520possible.%250AExperiments%2520reveal%2520that%2520Stable%2520Diffusion%2520can%2520effectively%2520learn%2520from%2520purified%250Aimages%2520over%2520all%2520protective%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00084v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Protective%20Perturbation%20Safeguard%20Personal%20Data%20from%20Being%20Exploited%0A%20%20by%20Stable%20Diffusion%3F&entry.906535625=Zhengyue%20Zhao%20and%20Jinhao%20Duan%20and%20Kaidi%20Xu%20and%20Chenan%20Wang%20and%20Rui%20Zhang%20and%20Zidong%20Du%20and%20Qi%20Guo%20and%20Xing%20Hu&entry.1292438233=%20%20Stable%20Diffusion%20has%20established%20itself%20as%20a%20foundation%20model%20in%20generative%0AAI%20artistic%20applications%2C%20receiving%20widespread%20research%20and%20application.%20Some%0Arecent%20fine-tuning%20methods%20have%20made%20it%20feasible%20for%20individuals%20to%20implant%0Apersonalized%20concepts%20onto%20the%20basic%20Stable%20Diffusion%20model%20with%20minimal%0Acomputational%20costs%20on%20small%20datasets.%20However%2C%20these%20innovations%20have%20also%0Agiven%20rise%20to%20issues%20like%20facial%20privacy%20forgery%20and%20artistic%20copyright%0Ainfringement.%20In%20recent%20studies%2C%20researchers%20have%20explored%20the%20addition%20of%0Aimperceptible%20adversarial%20perturbations%20to%20images%20to%20prevent%20potential%0Aunauthorized%20exploitation%20and%20infringements%20when%20personal%20data%20is%20used%20for%0Afine-tuning%20Stable%20Diffusion.%20Although%20these%20studies%20have%20demonstrated%20the%0Aability%20to%20protect%20images%2C%20it%20is%20essential%20to%20consider%20that%20these%20methods%20may%0Anot%20be%20entirely%20applicable%20in%20real-world%20scenarios.%20In%20this%20paper%2C%20we%0Asystematically%20evaluate%20the%20use%20of%20perturbations%20to%20protect%20images%20within%20a%0Apractical%20threat%20model.%20The%20results%20suggest%20that%20these%20approaches%20may%20not%20be%0Asufficient%20to%20safeguard%20image%20privacy%20and%20copyright%20effectively.%20Furthermore%2C%0Awe%20introduce%20a%20purification%20method%20capable%20of%20removing%20protected%20perturbations%0Awhile%20preserving%20the%20original%20image%20structure%20to%20the%20greatest%20extent%20possible.%0AExperiments%20reveal%20that%20Stable%20Diffusion%20can%20effectively%20learn%20from%20purified%0Aimages%20over%20all%20protective%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00084v2&entry.124074799=Read"},
{"title": "LOGCAN++: Local-global class-aware network for semantic segmentation of\n  remote sensing images", "author": "Xiaowen Ma and Rongrong Lian and Zhenkai Wu and Hongbo Guo and Mengting Ma and Sensen Wu and Zhenhong Du and Siyang Song and Wei Zhang", "abstract": "  Remote sensing images usually characterized by complex backgrounds, scale and\norientation variations, and large intra-class variance. General semantic\nsegmentation methods usually fail to fully investigate the above issues, and\nthus their performances on remote sensing image segmentation are limited. In\nthis paper, we propose our LOGCAN++, a semantic segmentation model customized\nfor remote sensing images, which is made up of a Global Class Awareness (GCA)\nmodule and several Local Class Awareness (LCA) modules. The GCA module captures\nglobal representations for class-level context modeling to reduce the\ninterference of background noise. The LCA module generates local class\nrepresentations as intermediate perceptual elements to indirectly associate\npixels with the global class representations, targeting at dealing with the\nlarge intra-class variance problem. In particular, we introduce affine\ntransformations in the LCA module for adaptive extraction of local class\nrepresentations to effectively tolerate scale and orientation variations in\nremotely sensed images. Extensive experiments on three benchmark datasets show\nthat our LOGCAN++ outperforms current mainstream general and remote sensing\nsemantic segmentation methods and achieves a better trade-off between speed and\naccuracy. Code is available at https://github.com/xwmaxwma/rssegmentation.\n", "link": "http://arxiv.org/abs/2406.16502v1", "date": "2024-06-24", "relevancy": 2.2106, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5743}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5429}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOGCAN%2B%2B%3A%20Local-global%20class-aware%20network%20for%20semantic%20segmentation%20of%0A%20%20remote%20sensing%20images&body=Title%3A%20LOGCAN%2B%2B%3A%20Local-global%20class-aware%20network%20for%20semantic%20segmentation%20of%0A%20%20remote%20sensing%20images%0AAuthor%3A%20Xiaowen%20Ma%20and%20Rongrong%20Lian%20and%20Zhenkai%20Wu%20and%20Hongbo%20Guo%20and%20Mengting%20Ma%20and%20Sensen%20Wu%20and%20Zhenhong%20Du%20and%20Siyang%20Song%20and%20Wei%20Zhang%0AAbstract%3A%20%20%20Remote%20sensing%20images%20usually%20characterized%20by%20complex%20backgrounds%2C%20scale%20and%0Aorientation%20variations%2C%20and%20large%20intra-class%20variance.%20General%20semantic%0Asegmentation%20methods%20usually%20fail%20to%20fully%20investigate%20the%20above%20issues%2C%20and%0Athus%20their%20performances%20on%20remote%20sensing%20image%20segmentation%20are%20limited.%20In%0Athis%20paper%2C%20we%20propose%20our%20LOGCAN%2B%2B%2C%20a%20semantic%20segmentation%20model%20customized%0Afor%20remote%20sensing%20images%2C%20which%20is%20made%20up%20of%20a%20Global%20Class%20Awareness%20%28GCA%29%0Amodule%20and%20several%20Local%20Class%20Awareness%20%28LCA%29%20modules.%20The%20GCA%20module%20captures%0Aglobal%20representations%20for%20class-level%20context%20modeling%20to%20reduce%20the%0Ainterference%20of%20background%20noise.%20The%20LCA%20module%20generates%20local%20class%0Arepresentations%20as%20intermediate%20perceptual%20elements%20to%20indirectly%20associate%0Apixels%20with%20the%20global%20class%20representations%2C%20targeting%20at%20dealing%20with%20the%0Alarge%20intra-class%20variance%20problem.%20In%20particular%2C%20we%20introduce%20affine%0Atransformations%20in%20the%20LCA%20module%20for%20adaptive%20extraction%20of%20local%20class%0Arepresentations%20to%20effectively%20tolerate%20scale%20and%20orientation%20variations%20in%0Aremotely%20sensed%20images.%20Extensive%20experiments%20on%20three%20benchmark%20datasets%20show%0Athat%20our%20LOGCAN%2B%2B%20outperforms%20current%20mainstream%20general%20and%20remote%20sensing%0Asemantic%20segmentation%20methods%20and%20achieves%20a%20better%20trade-off%20between%20speed%20and%0Aaccuracy.%20Code%20is%20available%20at%20https%3A//github.com/xwmaxwma/rssegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOGCAN%252B%252B%253A%2520Local-global%2520class-aware%2520network%2520for%2520semantic%2520segmentation%2520of%250A%2520%2520remote%2520sensing%2520images%26entry.906535625%3DXiaowen%2520Ma%2520and%2520Rongrong%2520Lian%2520and%2520Zhenkai%2520Wu%2520and%2520Hongbo%2520Guo%2520and%2520Mengting%2520Ma%2520and%2520Sensen%2520Wu%2520and%2520Zhenhong%2520Du%2520and%2520Siyang%2520Song%2520and%2520Wei%2520Zhang%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520images%2520usually%2520characterized%2520by%2520complex%2520backgrounds%252C%2520scale%2520and%250Aorientation%2520variations%252C%2520and%2520large%2520intra-class%2520variance.%2520General%2520semantic%250Asegmentation%2520methods%2520usually%2520fail%2520to%2520fully%2520investigate%2520the%2520above%2520issues%252C%2520and%250Athus%2520their%2520performances%2520on%2520remote%2520sensing%2520image%2520segmentation%2520are%2520limited.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520our%2520LOGCAN%252B%252B%252C%2520a%2520semantic%2520segmentation%2520model%2520customized%250Afor%2520remote%2520sensing%2520images%252C%2520which%2520is%2520made%2520up%2520of%2520a%2520Global%2520Class%2520Awareness%2520%2528GCA%2529%250Amodule%2520and%2520several%2520Local%2520Class%2520Awareness%2520%2528LCA%2529%2520modules.%2520The%2520GCA%2520module%2520captures%250Aglobal%2520representations%2520for%2520class-level%2520context%2520modeling%2520to%2520reduce%2520the%250Ainterference%2520of%2520background%2520noise.%2520The%2520LCA%2520module%2520generates%2520local%2520class%250Arepresentations%2520as%2520intermediate%2520perceptual%2520elements%2520to%2520indirectly%2520associate%250Apixels%2520with%2520the%2520global%2520class%2520representations%252C%2520targeting%2520at%2520dealing%2520with%2520the%250Alarge%2520intra-class%2520variance%2520problem.%2520In%2520particular%252C%2520we%2520introduce%2520affine%250Atransformations%2520in%2520the%2520LCA%2520module%2520for%2520adaptive%2520extraction%2520of%2520local%2520class%250Arepresentations%2520to%2520effectively%2520tolerate%2520scale%2520and%2520orientation%2520variations%2520in%250Aremotely%2520sensed%2520images.%2520Extensive%2520experiments%2520on%2520three%2520benchmark%2520datasets%2520show%250Athat%2520our%2520LOGCAN%252B%252B%2520outperforms%2520current%2520mainstream%2520general%2520and%2520remote%2520sensing%250Asemantic%2520segmentation%2520methods%2520and%2520achieves%2520a%2520better%2520trade-off%2520between%2520speed%2520and%250Aaccuracy.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/xwmaxwma/rssegmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOGCAN%2B%2B%3A%20Local-global%20class-aware%20network%20for%20semantic%20segmentation%20of%0A%20%20remote%20sensing%20images&entry.906535625=Xiaowen%20Ma%20and%20Rongrong%20Lian%20and%20Zhenkai%20Wu%20and%20Hongbo%20Guo%20and%20Mengting%20Ma%20and%20Sensen%20Wu%20and%20Zhenhong%20Du%20and%20Siyang%20Song%20and%20Wei%20Zhang&entry.1292438233=%20%20Remote%20sensing%20images%20usually%20characterized%20by%20complex%20backgrounds%2C%20scale%20and%0Aorientation%20variations%2C%20and%20large%20intra-class%20variance.%20General%20semantic%0Asegmentation%20methods%20usually%20fail%20to%20fully%20investigate%20the%20above%20issues%2C%20and%0Athus%20their%20performances%20on%20remote%20sensing%20image%20segmentation%20are%20limited.%20In%0Athis%20paper%2C%20we%20propose%20our%20LOGCAN%2B%2B%2C%20a%20semantic%20segmentation%20model%20customized%0Afor%20remote%20sensing%20images%2C%20which%20is%20made%20up%20of%20a%20Global%20Class%20Awareness%20%28GCA%29%0Amodule%20and%20several%20Local%20Class%20Awareness%20%28LCA%29%20modules.%20The%20GCA%20module%20captures%0Aglobal%20representations%20for%20class-level%20context%20modeling%20to%20reduce%20the%0Ainterference%20of%20background%20noise.%20The%20LCA%20module%20generates%20local%20class%0Arepresentations%20as%20intermediate%20perceptual%20elements%20to%20indirectly%20associate%0Apixels%20with%20the%20global%20class%20representations%2C%20targeting%20at%20dealing%20with%20the%0Alarge%20intra-class%20variance%20problem.%20In%20particular%2C%20we%20introduce%20affine%0Atransformations%20in%20the%20LCA%20module%20for%20adaptive%20extraction%20of%20local%20class%0Arepresentations%20to%20effectively%20tolerate%20scale%20and%20orientation%20variations%20in%0Aremotely%20sensed%20images.%20Extensive%20experiments%20on%20three%20benchmark%20datasets%20show%0Athat%20our%20LOGCAN%2B%2B%20outperforms%20current%20mainstream%20general%20and%20remote%20sensing%0Asemantic%20segmentation%20methods%20and%20achieves%20a%20better%20trade-off%20between%20speed%20and%0Aaccuracy.%20Code%20is%20available%20at%20https%3A//github.com/xwmaxwma/rssegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16502v1&entry.124074799=Read"},
{"title": "A Certifiable Algorithm for Simultaneous Shape Estimation and Object\n  Tracking", "author": "Lorenzo Shaikewitz and Samuel Ubellacker and Luca Carlone", "abstract": "  Applications from manipulation to autonomous vehicles rely on robust and\ngeneral object tracking to safely perform tasks in dynamic environments. We\npropose the first certifiably optimal category-level approach for simultaneous\nshape estimation and pose tracking of an object of known category (e.g. a car).\nOur approach uses 3D semantic keypoint measurements extracted from an RGB-D\nimage sequence, and phrases the estimation as a fixed-lag smoothing problem.\nTemporal constraints enforce the object's rigidity (fixed shape) and smooth\nmotion according to a constant-twist motion model. The solutions to this\nproblem are the estimates of the object's state (poses, velocities) and shape\n(paramaterized according to the active shape model) over the smoothing horizon.\nOur key contribution is to show that despite the non-convexity of the fixed-lag\nsmoothing problem, we can solve it to certifiable optimality using a small-size\nsemidefinite relaxation. We also present a fast outlier rejection scheme that\nfilters out incorrect keypoint detections with shape and time compatibility\ntests, and wrap our certifiable solver in a graduated non-convexity scheme. We\nevaluate the proposed approach on synthetic and real data, showcasing its\nperformance in a table-top manipulation scenario and a drone-based vehicle\ntracking application.\n", "link": "http://arxiv.org/abs/2406.16837v1", "date": "2024-06-24", "relevancy": 2.2017, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5715}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5476}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Certifiable%20Algorithm%20for%20Simultaneous%20Shape%20Estimation%20and%20Object%0A%20%20Tracking&body=Title%3A%20A%20Certifiable%20Algorithm%20for%20Simultaneous%20Shape%20Estimation%20and%20Object%0A%20%20Tracking%0AAuthor%3A%20Lorenzo%20Shaikewitz%20and%20Samuel%20Ubellacker%20and%20Luca%20Carlone%0AAbstract%3A%20%20%20Applications%20from%20manipulation%20to%20autonomous%20vehicles%20rely%20on%20robust%20and%0Ageneral%20object%20tracking%20to%20safely%20perform%20tasks%20in%20dynamic%20environments.%20We%0Apropose%20the%20first%20certifiably%20optimal%20category-level%20approach%20for%20simultaneous%0Ashape%20estimation%20and%20pose%20tracking%20of%20an%20object%20of%20known%20category%20%28e.g.%20a%20car%29.%0AOur%20approach%20uses%203D%20semantic%20keypoint%20measurements%20extracted%20from%20an%20RGB-D%0Aimage%20sequence%2C%20and%20phrases%20the%20estimation%20as%20a%20fixed-lag%20smoothing%20problem.%0ATemporal%20constraints%20enforce%20the%20object%27s%20rigidity%20%28fixed%20shape%29%20and%20smooth%0Amotion%20according%20to%20a%20constant-twist%20motion%20model.%20The%20solutions%20to%20this%0Aproblem%20are%20the%20estimates%20of%20the%20object%27s%20state%20%28poses%2C%20velocities%29%20and%20shape%0A%28paramaterized%20according%20to%20the%20active%20shape%20model%29%20over%20the%20smoothing%20horizon.%0AOur%20key%20contribution%20is%20to%20show%20that%20despite%20the%20non-convexity%20of%20the%20fixed-lag%0Asmoothing%20problem%2C%20we%20can%20solve%20it%20to%20certifiable%20optimality%20using%20a%20small-size%0Asemidefinite%20relaxation.%20We%20also%20present%20a%20fast%20outlier%20rejection%20scheme%20that%0Afilters%20out%20incorrect%20keypoint%20detections%20with%20shape%20and%20time%20compatibility%0Atests%2C%20and%20wrap%20our%20certifiable%20solver%20in%20a%20graduated%20non-convexity%20scheme.%20We%0Aevaluate%20the%20proposed%20approach%20on%20synthetic%20and%20real%20data%2C%20showcasing%20its%0Aperformance%20in%20a%20table-top%20manipulation%20scenario%20and%20a%20drone-based%20vehicle%0Atracking%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Certifiable%2520Algorithm%2520for%2520Simultaneous%2520Shape%2520Estimation%2520and%2520Object%250A%2520%2520Tracking%26entry.906535625%3DLorenzo%2520Shaikewitz%2520and%2520Samuel%2520Ubellacker%2520and%2520Luca%2520Carlone%26entry.1292438233%3D%2520%2520Applications%2520from%2520manipulation%2520to%2520autonomous%2520vehicles%2520rely%2520on%2520robust%2520and%250Ageneral%2520object%2520tracking%2520to%2520safely%2520perform%2520tasks%2520in%2520dynamic%2520environments.%2520We%250Apropose%2520the%2520first%2520certifiably%2520optimal%2520category-level%2520approach%2520for%2520simultaneous%250Ashape%2520estimation%2520and%2520pose%2520tracking%2520of%2520an%2520object%2520of%2520known%2520category%2520%2528e.g.%2520a%2520car%2529.%250AOur%2520approach%2520uses%25203D%2520semantic%2520keypoint%2520measurements%2520extracted%2520from%2520an%2520RGB-D%250Aimage%2520sequence%252C%2520and%2520phrases%2520the%2520estimation%2520as%2520a%2520fixed-lag%2520smoothing%2520problem.%250ATemporal%2520constraints%2520enforce%2520the%2520object%2527s%2520rigidity%2520%2528fixed%2520shape%2529%2520and%2520smooth%250Amotion%2520according%2520to%2520a%2520constant-twist%2520motion%2520model.%2520The%2520solutions%2520to%2520this%250Aproblem%2520are%2520the%2520estimates%2520of%2520the%2520object%2527s%2520state%2520%2528poses%252C%2520velocities%2529%2520and%2520shape%250A%2528paramaterized%2520according%2520to%2520the%2520active%2520shape%2520model%2529%2520over%2520the%2520smoothing%2520horizon.%250AOur%2520key%2520contribution%2520is%2520to%2520show%2520that%2520despite%2520the%2520non-convexity%2520of%2520the%2520fixed-lag%250Asmoothing%2520problem%252C%2520we%2520can%2520solve%2520it%2520to%2520certifiable%2520optimality%2520using%2520a%2520small-size%250Asemidefinite%2520relaxation.%2520We%2520also%2520present%2520a%2520fast%2520outlier%2520rejection%2520scheme%2520that%250Afilters%2520out%2520incorrect%2520keypoint%2520detections%2520with%2520shape%2520and%2520time%2520compatibility%250Atests%252C%2520and%2520wrap%2520our%2520certifiable%2520solver%2520in%2520a%2520graduated%2520non-convexity%2520scheme.%2520We%250Aevaluate%2520the%2520proposed%2520approach%2520on%2520synthetic%2520and%2520real%2520data%252C%2520showcasing%2520its%250Aperformance%2520in%2520a%2520table-top%2520manipulation%2520scenario%2520and%2520a%2520drone-based%2520vehicle%250Atracking%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Certifiable%20Algorithm%20for%20Simultaneous%20Shape%20Estimation%20and%20Object%0A%20%20Tracking&entry.906535625=Lorenzo%20Shaikewitz%20and%20Samuel%20Ubellacker%20and%20Luca%20Carlone&entry.1292438233=%20%20Applications%20from%20manipulation%20to%20autonomous%20vehicles%20rely%20on%20robust%20and%0Ageneral%20object%20tracking%20to%20safely%20perform%20tasks%20in%20dynamic%20environments.%20We%0Apropose%20the%20first%20certifiably%20optimal%20category-level%20approach%20for%20simultaneous%0Ashape%20estimation%20and%20pose%20tracking%20of%20an%20object%20of%20known%20category%20%28e.g.%20a%20car%29.%0AOur%20approach%20uses%203D%20semantic%20keypoint%20measurements%20extracted%20from%20an%20RGB-D%0Aimage%20sequence%2C%20and%20phrases%20the%20estimation%20as%20a%20fixed-lag%20smoothing%20problem.%0ATemporal%20constraints%20enforce%20the%20object%27s%20rigidity%20%28fixed%20shape%29%20and%20smooth%0Amotion%20according%20to%20a%20constant-twist%20motion%20model.%20The%20solutions%20to%20this%0Aproblem%20are%20the%20estimates%20of%20the%20object%27s%20state%20%28poses%2C%20velocities%29%20and%20shape%0A%28paramaterized%20according%20to%20the%20active%20shape%20model%29%20over%20the%20smoothing%20horizon.%0AOur%20key%20contribution%20is%20to%20show%20that%20despite%20the%20non-convexity%20of%20the%20fixed-lag%0Asmoothing%20problem%2C%20we%20can%20solve%20it%20to%20certifiable%20optimality%20using%20a%20small-size%0Asemidefinite%20relaxation.%20We%20also%20present%20a%20fast%20outlier%20rejection%20scheme%20that%0Afilters%20out%20incorrect%20keypoint%20detections%20with%20shape%20and%20time%20compatibility%0Atests%2C%20and%20wrap%20our%20certifiable%20solver%20in%20a%20graduated%20non-convexity%20scheme.%20We%0Aevaluate%20the%20proposed%20approach%20on%20synthetic%20and%20real%20data%2C%20showcasing%20its%0Aperformance%20in%20a%20table-top%20manipulation%20scenario%20and%20a%20drone-based%20vehicle%0Atracking%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16837v1&entry.124074799=Read"},
{"title": "Local primordial non-Gaussianity from the large-scale clustering of\n  photometric DESI luminous red galaxies", "author": "Mehdi Rezaie and Ashley J. Ross and Hee-Jong Seo and Hui Kong and Anna Porredon and Lado Samushia and Edmond Chaussidon and Alex Krolewski and Arnaud de Mattia and Florian Beutler and Jessica Nicole Aguilar and Steven Ahlen and Shadab Alam and Santiago Avila and Benedict Bahr-Kalus and Jose Bermejo-Climent and David Brooks and Todd Claybaugh and Shaun Cole and Kyle Dawson and Axel de la Macorra and Peter Doel and Andreu Font-Ribera and Jaime E. Forero-Romero and Satya Gontcho A Gontcho and Julien Guy and Klaus Honscheid and Dragan Huterer and Theodore Kisner and Martin Landriau and Michael Levi and Marc Manera and Aaron Meisner and Ramon Miquel and Eva-Maria Mueller and Adam Myers and Jeffrey A. Newman and Jundan Nie and Nathalie Palanque-Delabrouille and Will Percival and Claire Poppett and Graziano Rossi and Eusebio Sanchez and Michael Schubnell and Gregory Tarl\u00e9 and Benjamin Alan Weaver and Christophe Y\u00e8che and Zhimin Zhou and Hu Zou", "abstract": "  We use angular clustering of luminous red galaxies from the Dark Energy\nSpectroscopic Instrument (DESI) imaging surveys to constrain the local\nprimordial non-Gaussianity parameter $\\fnl$. Our sample comprises over 12\nmillion targets, covering 14,000 square degrees of the sky, with redshifts in\nthe range $0.2< z < 1.35$. We identify Galactic extinction, survey depth, and\nastronomical seeing as the primary sources of systematic error, and employ\nlinear regression and artificial neural networks to alleviate non-cosmological\nexcess clustering on large scales. Our methods are tested against simulations\nwith and without $\\fnl$ and systematics, showing superior performance of the\nneural network treatment. The neural network with a set of nine imaging\nproperty maps passes our systematic null test criteria, and is chosen as the\nfiducial treatment. Assuming the universality relation, we find $\\fnl =\n34^{+24(+50)}_{-44(-73)}$ at 68\\%(95\\%) confidence. We apply a series of\nrobustness tests (e.g., cuts on imaging, declination, or scales used) that show\nconsistency in the obtained constraints. We study how the regression method\nbiases the measured angular power-spectrum and degrades the $\\fnl$ constraining\npower. The use of the nine maps more than doubles the uncertainty compared to\nusing only the three primary maps in the regression. Our results thus motivate\nthe development of more efficient methods that avoid over-correction, protect\nlarge-scale clustering information, and preserve constraining power.\nAdditionally, our results encourage further studies of $\\fnl$ with DESI\nspectroscopic samples, where the inclusion of 3D clustering modes should help\nseparate imaging systematics and lessen the degradation in the $\\fnl$\nuncertainty.\n", "link": "http://arxiv.org/abs/2307.01753v2", "date": "2024-06-24", "relevancy": 2.1837, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4467}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4377}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20primordial%20non-Gaussianity%20from%20the%20large-scale%20clustering%20of%0A%20%20photometric%20DESI%20luminous%20red%20galaxies&body=Title%3A%20Local%20primordial%20non-Gaussianity%20from%20the%20large-scale%20clustering%20of%0A%20%20photometric%20DESI%20luminous%20red%20galaxies%0AAuthor%3A%20Mehdi%20Rezaie%20and%20Ashley%20J.%20Ross%20and%20Hee-Jong%20Seo%20and%20Hui%20Kong%20and%20Anna%20Porredon%20and%20Lado%20Samushia%20and%20Edmond%20Chaussidon%20and%20Alex%20Krolewski%20and%20Arnaud%20de%20Mattia%20and%20Florian%20Beutler%20and%20Jessica%20Nicole%20Aguilar%20and%20Steven%20Ahlen%20and%20Shadab%20Alam%20and%20Santiago%20Avila%20and%20Benedict%20Bahr-Kalus%20and%20Jose%20Bermejo-Climent%20and%20David%20Brooks%20and%20Todd%20Claybaugh%20and%20Shaun%20Cole%20and%20Kyle%20Dawson%20and%20Axel%20de%20la%20Macorra%20and%20Peter%20Doel%20and%20Andreu%20Font-Ribera%20and%20Jaime%20E.%20Forero-Romero%20and%20Satya%20Gontcho%20A%20Gontcho%20and%20Julien%20Guy%20and%20Klaus%20Honscheid%20and%20Dragan%20Huterer%20and%20Theodore%20Kisner%20and%20Martin%20Landriau%20and%20Michael%20Levi%20and%20Marc%20Manera%20and%20Aaron%20Meisner%20and%20Ramon%20Miquel%20and%20Eva-Maria%20Mueller%20and%20Adam%20Myers%20and%20Jeffrey%20A.%20Newman%20and%20Jundan%20Nie%20and%20Nathalie%20Palanque-Delabrouille%20and%20Will%20Percival%20and%20Claire%20Poppett%20and%20Graziano%20Rossi%20and%20Eusebio%20Sanchez%20and%20Michael%20Schubnell%20and%20Gregory%20Tarl%C3%A9%20and%20Benjamin%20Alan%20Weaver%20and%20Christophe%20Y%C3%A8che%20and%20Zhimin%20Zhou%20and%20Hu%20Zou%0AAbstract%3A%20%20%20We%20use%20angular%20clustering%20of%20luminous%20red%20galaxies%20from%20the%20Dark%20Energy%0ASpectroscopic%20Instrument%20%28DESI%29%20imaging%20surveys%20to%20constrain%20the%20local%0Aprimordial%20non-Gaussianity%20parameter%20%24%5Cfnl%24.%20Our%20sample%20comprises%20over%2012%0Amillion%20targets%2C%20covering%2014%2C000%20square%20degrees%20of%20the%20sky%2C%20with%20redshifts%20in%0Athe%20range%20%240.2%3C%20z%20%3C%201.35%24.%20We%20identify%20Galactic%20extinction%2C%20survey%20depth%2C%20and%0Aastronomical%20seeing%20as%20the%20primary%20sources%20of%20systematic%20error%2C%20and%20employ%0Alinear%20regression%20and%20artificial%20neural%20networks%20to%20alleviate%20non-cosmological%0Aexcess%20clustering%20on%20large%20scales.%20Our%20methods%20are%20tested%20against%20simulations%0Awith%20and%20without%20%24%5Cfnl%24%20and%20systematics%2C%20showing%20superior%20performance%20of%20the%0Aneural%20network%20treatment.%20The%20neural%20network%20with%20a%20set%20of%20nine%20imaging%0Aproperty%20maps%20passes%20our%20systematic%20null%20test%20criteria%2C%20and%20is%20chosen%20as%20the%0Afiducial%20treatment.%20Assuming%20the%20universality%20relation%2C%20we%20find%20%24%5Cfnl%20%3D%0A34%5E%7B%2B24%28%2B50%29%7D_%7B-44%28-73%29%7D%24%20at%2068%5C%25%2895%5C%25%29%20confidence.%20We%20apply%20a%20series%20of%0Arobustness%20tests%20%28e.g.%2C%20cuts%20on%20imaging%2C%20declination%2C%20or%20scales%20used%29%20that%20show%0Aconsistency%20in%20the%20obtained%20constraints.%20We%20study%20how%20the%20regression%20method%0Abiases%20the%20measured%20angular%20power-spectrum%20and%20degrades%20the%20%24%5Cfnl%24%20constraining%0Apower.%20The%20use%20of%20the%20nine%20maps%20more%20than%20doubles%20the%20uncertainty%20compared%20to%0Ausing%20only%20the%20three%20primary%20maps%20in%20the%20regression.%20Our%20results%20thus%20motivate%0Athe%20development%20of%20more%20efficient%20methods%20that%20avoid%20over-correction%2C%20protect%0Alarge-scale%20clustering%20information%2C%20and%20preserve%20constraining%20power.%0AAdditionally%2C%20our%20results%20encourage%20further%20studies%20of%20%24%5Cfnl%24%20with%20DESI%0Aspectroscopic%20samples%2C%20where%20the%20inclusion%20of%203D%20clustering%20modes%20should%20help%0Aseparate%20imaging%20systematics%20and%20lessen%20the%20degradation%20in%20the%20%24%5Cfnl%24%0Auncertainty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.01753v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520primordial%2520non-Gaussianity%2520from%2520the%2520large-scale%2520clustering%2520of%250A%2520%2520photometric%2520DESI%2520luminous%2520red%2520galaxies%26entry.906535625%3DMehdi%2520Rezaie%2520and%2520Ashley%2520J.%2520Ross%2520and%2520Hee-Jong%2520Seo%2520and%2520Hui%2520Kong%2520and%2520Anna%2520Porredon%2520and%2520Lado%2520Samushia%2520and%2520Edmond%2520Chaussidon%2520and%2520Alex%2520Krolewski%2520and%2520Arnaud%2520de%2520Mattia%2520and%2520Florian%2520Beutler%2520and%2520Jessica%2520Nicole%2520Aguilar%2520and%2520Steven%2520Ahlen%2520and%2520Shadab%2520Alam%2520and%2520Santiago%2520Avila%2520and%2520Benedict%2520Bahr-Kalus%2520and%2520Jose%2520Bermejo-Climent%2520and%2520David%2520Brooks%2520and%2520Todd%2520Claybaugh%2520and%2520Shaun%2520Cole%2520and%2520Kyle%2520Dawson%2520and%2520Axel%2520de%2520la%2520Macorra%2520and%2520Peter%2520Doel%2520and%2520Andreu%2520Font-Ribera%2520and%2520Jaime%2520E.%2520Forero-Romero%2520and%2520Satya%2520Gontcho%2520A%2520Gontcho%2520and%2520Julien%2520Guy%2520and%2520Klaus%2520Honscheid%2520and%2520Dragan%2520Huterer%2520and%2520Theodore%2520Kisner%2520and%2520Martin%2520Landriau%2520and%2520Michael%2520Levi%2520and%2520Marc%2520Manera%2520and%2520Aaron%2520Meisner%2520and%2520Ramon%2520Miquel%2520and%2520Eva-Maria%2520Mueller%2520and%2520Adam%2520Myers%2520and%2520Jeffrey%2520A.%2520Newman%2520and%2520Jundan%2520Nie%2520and%2520Nathalie%2520Palanque-Delabrouille%2520and%2520Will%2520Percival%2520and%2520Claire%2520Poppett%2520and%2520Graziano%2520Rossi%2520and%2520Eusebio%2520Sanchez%2520and%2520Michael%2520Schubnell%2520and%2520Gregory%2520Tarl%25C3%25A9%2520and%2520Benjamin%2520Alan%2520Weaver%2520and%2520Christophe%2520Y%25C3%25A8che%2520and%2520Zhimin%2520Zhou%2520and%2520Hu%2520Zou%26entry.1292438233%3D%2520%2520We%2520use%2520angular%2520clustering%2520of%2520luminous%2520red%2520galaxies%2520from%2520the%2520Dark%2520Energy%250ASpectroscopic%2520Instrument%2520%2528DESI%2529%2520imaging%2520surveys%2520to%2520constrain%2520the%2520local%250Aprimordial%2520non-Gaussianity%2520parameter%2520%2524%255Cfnl%2524.%2520Our%2520sample%2520comprises%2520over%252012%250Amillion%2520targets%252C%2520covering%252014%252C000%2520square%2520degrees%2520of%2520the%2520sky%252C%2520with%2520redshifts%2520in%250Athe%2520range%2520%25240.2%253C%2520z%2520%253C%25201.35%2524.%2520We%2520identify%2520Galactic%2520extinction%252C%2520survey%2520depth%252C%2520and%250Aastronomical%2520seeing%2520as%2520the%2520primary%2520sources%2520of%2520systematic%2520error%252C%2520and%2520employ%250Alinear%2520regression%2520and%2520artificial%2520neural%2520networks%2520to%2520alleviate%2520non-cosmological%250Aexcess%2520clustering%2520on%2520large%2520scales.%2520Our%2520methods%2520are%2520tested%2520against%2520simulations%250Awith%2520and%2520without%2520%2524%255Cfnl%2524%2520and%2520systematics%252C%2520showing%2520superior%2520performance%2520of%2520the%250Aneural%2520network%2520treatment.%2520The%2520neural%2520network%2520with%2520a%2520set%2520of%2520nine%2520imaging%250Aproperty%2520maps%2520passes%2520our%2520systematic%2520null%2520test%2520criteria%252C%2520and%2520is%2520chosen%2520as%2520the%250Afiducial%2520treatment.%2520Assuming%2520the%2520universality%2520relation%252C%2520we%2520find%2520%2524%255Cfnl%2520%253D%250A34%255E%257B%252B24%2528%252B50%2529%257D_%257B-44%2528-73%2529%257D%2524%2520at%252068%255C%2525%252895%255C%2525%2529%2520confidence.%2520We%2520apply%2520a%2520series%2520of%250Arobustness%2520tests%2520%2528e.g.%252C%2520cuts%2520on%2520imaging%252C%2520declination%252C%2520or%2520scales%2520used%2529%2520that%2520show%250Aconsistency%2520in%2520the%2520obtained%2520constraints.%2520We%2520study%2520how%2520the%2520regression%2520method%250Abiases%2520the%2520measured%2520angular%2520power-spectrum%2520and%2520degrades%2520the%2520%2524%255Cfnl%2524%2520constraining%250Apower.%2520The%2520use%2520of%2520the%2520nine%2520maps%2520more%2520than%2520doubles%2520the%2520uncertainty%2520compared%2520to%250Ausing%2520only%2520the%2520three%2520primary%2520maps%2520in%2520the%2520regression.%2520Our%2520results%2520thus%2520motivate%250Athe%2520development%2520of%2520more%2520efficient%2520methods%2520that%2520avoid%2520over-correction%252C%2520protect%250Alarge-scale%2520clustering%2520information%252C%2520and%2520preserve%2520constraining%2520power.%250AAdditionally%252C%2520our%2520results%2520encourage%2520further%2520studies%2520of%2520%2524%255Cfnl%2524%2520with%2520DESI%250Aspectroscopic%2520samples%252C%2520where%2520the%2520inclusion%2520of%25203D%2520clustering%2520modes%2520should%2520help%250Aseparate%2520imaging%2520systematics%2520and%2520lessen%2520the%2520degradation%2520in%2520the%2520%2524%255Cfnl%2524%250Auncertainty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.01753v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20primordial%20non-Gaussianity%20from%20the%20large-scale%20clustering%20of%0A%20%20photometric%20DESI%20luminous%20red%20galaxies&entry.906535625=Mehdi%20Rezaie%20and%20Ashley%20J.%20Ross%20and%20Hee-Jong%20Seo%20and%20Hui%20Kong%20and%20Anna%20Porredon%20and%20Lado%20Samushia%20and%20Edmond%20Chaussidon%20and%20Alex%20Krolewski%20and%20Arnaud%20de%20Mattia%20and%20Florian%20Beutler%20and%20Jessica%20Nicole%20Aguilar%20and%20Steven%20Ahlen%20and%20Shadab%20Alam%20and%20Santiago%20Avila%20and%20Benedict%20Bahr-Kalus%20and%20Jose%20Bermejo-Climent%20and%20David%20Brooks%20and%20Todd%20Claybaugh%20and%20Shaun%20Cole%20and%20Kyle%20Dawson%20and%20Axel%20de%20la%20Macorra%20and%20Peter%20Doel%20and%20Andreu%20Font-Ribera%20and%20Jaime%20E.%20Forero-Romero%20and%20Satya%20Gontcho%20A%20Gontcho%20and%20Julien%20Guy%20and%20Klaus%20Honscheid%20and%20Dragan%20Huterer%20and%20Theodore%20Kisner%20and%20Martin%20Landriau%20and%20Michael%20Levi%20and%20Marc%20Manera%20and%20Aaron%20Meisner%20and%20Ramon%20Miquel%20and%20Eva-Maria%20Mueller%20and%20Adam%20Myers%20and%20Jeffrey%20A.%20Newman%20and%20Jundan%20Nie%20and%20Nathalie%20Palanque-Delabrouille%20and%20Will%20Percival%20and%20Claire%20Poppett%20and%20Graziano%20Rossi%20and%20Eusebio%20Sanchez%20and%20Michael%20Schubnell%20and%20Gregory%20Tarl%C3%A9%20and%20Benjamin%20Alan%20Weaver%20and%20Christophe%20Y%C3%A8che%20and%20Zhimin%20Zhou%20and%20Hu%20Zou&entry.1292438233=%20%20We%20use%20angular%20clustering%20of%20luminous%20red%20galaxies%20from%20the%20Dark%20Energy%0ASpectroscopic%20Instrument%20%28DESI%29%20imaging%20surveys%20to%20constrain%20the%20local%0Aprimordial%20non-Gaussianity%20parameter%20%24%5Cfnl%24.%20Our%20sample%20comprises%20over%2012%0Amillion%20targets%2C%20covering%2014%2C000%20square%20degrees%20of%20the%20sky%2C%20with%20redshifts%20in%0Athe%20range%20%240.2%3C%20z%20%3C%201.35%24.%20We%20identify%20Galactic%20extinction%2C%20survey%20depth%2C%20and%0Aastronomical%20seeing%20as%20the%20primary%20sources%20of%20systematic%20error%2C%20and%20employ%0Alinear%20regression%20and%20artificial%20neural%20networks%20to%20alleviate%20non-cosmological%0Aexcess%20clustering%20on%20large%20scales.%20Our%20methods%20are%20tested%20against%20simulations%0Awith%20and%20without%20%24%5Cfnl%24%20and%20systematics%2C%20showing%20superior%20performance%20of%20the%0Aneural%20network%20treatment.%20The%20neural%20network%20with%20a%20set%20of%20nine%20imaging%0Aproperty%20maps%20passes%20our%20systematic%20null%20test%20criteria%2C%20and%20is%20chosen%20as%20the%0Afiducial%20treatment.%20Assuming%20the%20universality%20relation%2C%20we%20find%20%24%5Cfnl%20%3D%0A34%5E%7B%2B24%28%2B50%29%7D_%7B-44%28-73%29%7D%24%20at%2068%5C%25%2895%5C%25%29%20confidence.%20We%20apply%20a%20series%20of%0Arobustness%20tests%20%28e.g.%2C%20cuts%20on%20imaging%2C%20declination%2C%20or%20scales%20used%29%20that%20show%0Aconsistency%20in%20the%20obtained%20constraints.%20We%20study%20how%20the%20regression%20method%0Abiases%20the%20measured%20angular%20power-spectrum%20and%20degrades%20the%20%24%5Cfnl%24%20constraining%0Apower.%20The%20use%20of%20the%20nine%20maps%20more%20than%20doubles%20the%20uncertainty%20compared%20to%0Ausing%20only%20the%20three%20primary%20maps%20in%20the%20regression.%20Our%20results%20thus%20motivate%0Athe%20development%20of%20more%20efficient%20methods%20that%20avoid%20over-correction%2C%20protect%0Alarge-scale%20clustering%20information%2C%20and%20preserve%20constraining%20power.%0AAdditionally%2C%20our%20results%20encourage%20further%20studies%20of%20%24%5Cfnl%24%20with%20DESI%0Aspectroscopic%20samples%2C%20where%20the%20inclusion%20of%203D%20clustering%20modes%20should%20help%0Aseparate%20imaging%20systematics%20and%20lessen%20the%20degradation%20in%20the%20%24%5Cfnl%24%0Auncertainty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.01753v2&entry.124074799=Read"},
{"title": "Detach-ROCKET: Sequential feature selection for time series\n  classification with random convolutional kernels", "author": "Gonzalo Uribarri and Federico Barone and Alessio Ansuini and Erik Frans\u00e9n", "abstract": "  Time Series Classification (TSC) is essential in fields like medicine,\nenvironmental science, and finance, enabling tasks such as disease diagnosis,\nanomaly detection, and stock price analysis. While machine learning models like\nRecurrent Neural Networks and InceptionTime are successful in numerous\napplications, they can face scalability issues due to computational\nrequirements. Recently, ROCKET has emerged as an efficient alternative,\nachieving state-of-the-art performance and simplifying training by utilizing a\nlarge number of randomly generated features from the time series data. However,\nmany of these features are redundant or non-informative, increasing\ncomputational load and compromising generalization. Here we introduce\nSequential Feature Detachment (SFD) to identify and prune non-essential\nfeatures in ROCKET-based models, such as ROCKET, MiniRocket, and MultiRocket.\nSFD estimates feature importance using model coefficients and can handle large\nfeature sets without complex hyperparameter tuning. Testing on the UCR archive\nshows that SFD can produce models with better test accuracy using only 10\\% of\nthe original features. We named these pruned models Detach-ROCKET. We also\npresent an end-to-end procedure for determining an optimal balance between the\nnumber of features and model accuracy. On the largest binary UCR dataset,\nDetach-ROCKET improves test accuracy by 0.6\\% while reducing features by\n98.9\\%. By enabling a significant reduction in model size without sacrificing\naccuracy, our methodology improves computational efficiency and contributes to\nmodel interpretability. We believe that Detach-ROCKET will be a valuable tool\nfor researchers and practitioners working with time series data, who can find a\nuser-friendly implementation of the model at\n\\url{https://github.com/gon-uri/detach_rocket}.\n", "link": "http://arxiv.org/abs/2309.14518v3", "date": "2024-06-24", "relevancy": 2.1821, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4469}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4436}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detach-ROCKET%3A%20Sequential%20feature%20selection%20for%20time%20series%0A%20%20classification%20with%20random%20convolutional%20kernels&body=Title%3A%20Detach-ROCKET%3A%20Sequential%20feature%20selection%20for%20time%20series%0A%20%20classification%20with%20random%20convolutional%20kernels%0AAuthor%3A%20Gonzalo%20Uribarri%20and%20Federico%20Barone%20and%20Alessio%20Ansuini%20and%20Erik%20Frans%C3%A9n%0AAbstract%3A%20%20%20Time%20Series%20Classification%20%28TSC%29%20is%20essential%20in%20fields%20like%20medicine%2C%0Aenvironmental%20science%2C%20and%20finance%2C%20enabling%20tasks%20such%20as%20disease%20diagnosis%2C%0Aanomaly%20detection%2C%20and%20stock%20price%20analysis.%20While%20machine%20learning%20models%20like%0ARecurrent%20Neural%20Networks%20and%20InceptionTime%20are%20successful%20in%20numerous%0Aapplications%2C%20they%20can%20face%20scalability%20issues%20due%20to%20computational%0Arequirements.%20Recently%2C%20ROCKET%20has%20emerged%20as%20an%20efficient%20alternative%2C%0Aachieving%20state-of-the-art%20performance%20and%20simplifying%20training%20by%20utilizing%20a%0Alarge%20number%20of%20randomly%20generated%20features%20from%20the%20time%20series%20data.%20However%2C%0Amany%20of%20these%20features%20are%20redundant%20or%20non-informative%2C%20increasing%0Acomputational%20load%20and%20compromising%20generalization.%20Here%20we%20introduce%0ASequential%20Feature%20Detachment%20%28SFD%29%20to%20identify%20and%20prune%20non-essential%0Afeatures%20in%20ROCKET-based%20models%2C%20such%20as%20ROCKET%2C%20MiniRocket%2C%20and%20MultiRocket.%0ASFD%20estimates%20feature%20importance%20using%20model%20coefficients%20and%20can%20handle%20large%0Afeature%20sets%20without%20complex%20hyperparameter%20tuning.%20Testing%20on%20the%20UCR%20archive%0Ashows%20that%20SFD%20can%20produce%20models%20with%20better%20test%20accuracy%20using%20only%2010%5C%25%20of%0Athe%20original%20features.%20We%20named%20these%20pruned%20models%20Detach-ROCKET.%20We%20also%0Apresent%20an%20end-to-end%20procedure%20for%20determining%20an%20optimal%20balance%20between%20the%0Anumber%20of%20features%20and%20model%20accuracy.%20On%20the%20largest%20binary%20UCR%20dataset%2C%0ADetach-ROCKET%20improves%20test%20accuracy%20by%200.6%5C%25%20while%20reducing%20features%20by%0A98.9%5C%25.%20By%20enabling%20a%20significant%20reduction%20in%20model%20size%20without%20sacrificing%0Aaccuracy%2C%20our%20methodology%20improves%20computational%20efficiency%20and%20contributes%20to%0Amodel%20interpretability.%20We%20believe%20that%20Detach-ROCKET%20will%20be%20a%20valuable%20tool%0Afor%20researchers%20and%20practitioners%20working%20with%20time%20series%20data%2C%20who%20can%20find%20a%0Auser-friendly%20implementation%20of%20the%20model%20at%0A%5Curl%7Bhttps%3A//github.com/gon-uri/detach_rocket%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.14518v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetach-ROCKET%253A%2520Sequential%2520feature%2520selection%2520for%2520time%2520series%250A%2520%2520classification%2520with%2520random%2520convolutional%2520kernels%26entry.906535625%3DGonzalo%2520Uribarri%2520and%2520Federico%2520Barone%2520and%2520Alessio%2520Ansuini%2520and%2520Erik%2520Frans%25C3%25A9n%26entry.1292438233%3D%2520%2520Time%2520Series%2520Classification%2520%2528TSC%2529%2520is%2520essential%2520in%2520fields%2520like%2520medicine%252C%250Aenvironmental%2520science%252C%2520and%2520finance%252C%2520enabling%2520tasks%2520such%2520as%2520disease%2520diagnosis%252C%250Aanomaly%2520detection%252C%2520and%2520stock%2520price%2520analysis.%2520While%2520machine%2520learning%2520models%2520like%250ARecurrent%2520Neural%2520Networks%2520and%2520InceptionTime%2520are%2520successful%2520in%2520numerous%250Aapplications%252C%2520they%2520can%2520face%2520scalability%2520issues%2520due%2520to%2520computational%250Arequirements.%2520Recently%252C%2520ROCKET%2520has%2520emerged%2520as%2520an%2520efficient%2520alternative%252C%250Aachieving%2520state-of-the-art%2520performance%2520and%2520simplifying%2520training%2520by%2520utilizing%2520a%250Alarge%2520number%2520of%2520randomly%2520generated%2520features%2520from%2520the%2520time%2520series%2520data.%2520However%252C%250Amany%2520of%2520these%2520features%2520are%2520redundant%2520or%2520non-informative%252C%2520increasing%250Acomputational%2520load%2520and%2520compromising%2520generalization.%2520Here%2520we%2520introduce%250ASequential%2520Feature%2520Detachment%2520%2528SFD%2529%2520to%2520identify%2520and%2520prune%2520non-essential%250Afeatures%2520in%2520ROCKET-based%2520models%252C%2520such%2520as%2520ROCKET%252C%2520MiniRocket%252C%2520and%2520MultiRocket.%250ASFD%2520estimates%2520feature%2520importance%2520using%2520model%2520coefficients%2520and%2520can%2520handle%2520large%250Afeature%2520sets%2520without%2520complex%2520hyperparameter%2520tuning.%2520Testing%2520on%2520the%2520UCR%2520archive%250Ashows%2520that%2520SFD%2520can%2520produce%2520models%2520with%2520better%2520test%2520accuracy%2520using%2520only%252010%255C%2525%2520of%250Athe%2520original%2520features.%2520We%2520named%2520these%2520pruned%2520models%2520Detach-ROCKET.%2520We%2520also%250Apresent%2520an%2520end-to-end%2520procedure%2520for%2520determining%2520an%2520optimal%2520balance%2520between%2520the%250Anumber%2520of%2520features%2520and%2520model%2520accuracy.%2520On%2520the%2520largest%2520binary%2520UCR%2520dataset%252C%250ADetach-ROCKET%2520improves%2520test%2520accuracy%2520by%25200.6%255C%2525%2520while%2520reducing%2520features%2520by%250A98.9%255C%2525.%2520By%2520enabling%2520a%2520significant%2520reduction%2520in%2520model%2520size%2520without%2520sacrificing%250Aaccuracy%252C%2520our%2520methodology%2520improves%2520computational%2520efficiency%2520and%2520contributes%2520to%250Amodel%2520interpretability.%2520We%2520believe%2520that%2520Detach-ROCKET%2520will%2520be%2520a%2520valuable%2520tool%250Afor%2520researchers%2520and%2520practitioners%2520working%2520with%2520time%2520series%2520data%252C%2520who%2520can%2520find%2520a%250Auser-friendly%2520implementation%2520of%2520the%2520model%2520at%250A%255Curl%257Bhttps%253A//github.com/gon-uri/detach_rocket%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.14518v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detach-ROCKET%3A%20Sequential%20feature%20selection%20for%20time%20series%0A%20%20classification%20with%20random%20convolutional%20kernels&entry.906535625=Gonzalo%20Uribarri%20and%20Federico%20Barone%20and%20Alessio%20Ansuini%20and%20Erik%20Frans%C3%A9n&entry.1292438233=%20%20Time%20Series%20Classification%20%28TSC%29%20is%20essential%20in%20fields%20like%20medicine%2C%0Aenvironmental%20science%2C%20and%20finance%2C%20enabling%20tasks%20such%20as%20disease%20diagnosis%2C%0Aanomaly%20detection%2C%20and%20stock%20price%20analysis.%20While%20machine%20learning%20models%20like%0ARecurrent%20Neural%20Networks%20and%20InceptionTime%20are%20successful%20in%20numerous%0Aapplications%2C%20they%20can%20face%20scalability%20issues%20due%20to%20computational%0Arequirements.%20Recently%2C%20ROCKET%20has%20emerged%20as%20an%20efficient%20alternative%2C%0Aachieving%20state-of-the-art%20performance%20and%20simplifying%20training%20by%20utilizing%20a%0Alarge%20number%20of%20randomly%20generated%20features%20from%20the%20time%20series%20data.%20However%2C%0Amany%20of%20these%20features%20are%20redundant%20or%20non-informative%2C%20increasing%0Acomputational%20load%20and%20compromising%20generalization.%20Here%20we%20introduce%0ASequential%20Feature%20Detachment%20%28SFD%29%20to%20identify%20and%20prune%20non-essential%0Afeatures%20in%20ROCKET-based%20models%2C%20such%20as%20ROCKET%2C%20MiniRocket%2C%20and%20MultiRocket.%0ASFD%20estimates%20feature%20importance%20using%20model%20coefficients%20and%20can%20handle%20large%0Afeature%20sets%20without%20complex%20hyperparameter%20tuning.%20Testing%20on%20the%20UCR%20archive%0Ashows%20that%20SFD%20can%20produce%20models%20with%20better%20test%20accuracy%20using%20only%2010%5C%25%20of%0Athe%20original%20features.%20We%20named%20these%20pruned%20models%20Detach-ROCKET.%20We%20also%0Apresent%20an%20end-to-end%20procedure%20for%20determining%20an%20optimal%20balance%20between%20the%0Anumber%20of%20features%20and%20model%20accuracy.%20On%20the%20largest%20binary%20UCR%20dataset%2C%0ADetach-ROCKET%20improves%20test%20accuracy%20by%200.6%5C%25%20while%20reducing%20features%20by%0A98.9%5C%25.%20By%20enabling%20a%20significant%20reduction%20in%20model%20size%20without%20sacrificing%0Aaccuracy%2C%20our%20methodology%20improves%20computational%20efficiency%20and%20contributes%20to%0Amodel%20interpretability.%20We%20believe%20that%20Detach-ROCKET%20will%20be%20a%20valuable%20tool%0Afor%20researchers%20and%20practitioners%20working%20with%20time%20series%20data%2C%20who%20can%20find%20a%0Auser-friendly%20implementation%20of%20the%20model%20at%0A%5Curl%7Bhttps%3A//github.com/gon-uri/detach_rocket%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.14518v3&entry.124074799=Read"},
{"title": "Long Context Transfer from Language to Vision", "author": "Peiyuan Zhang and Kaichen Zhang and Bo Li and Guangtao Zeng and Jingkang Yang and Yuanhan Zhang and Ziyue Wang and Haoran Tan and Chunyuan Li and Ziwei Liu", "abstract": "  Video sequences offer valuable temporal information, but existing large\nmultimodal models (LMMs) fall short in understanding extremely long videos.\nMany works address this by reducing the number of visual tokens using visual\nresamplers. Alternatively, in this paper, we approach this problem from the\nperspective of the language model. By simply extrapolating the context length\nof the language backbone, we enable LMMs to comprehend orders of magnitude more\nvisual tokens without any video training. We call this phenomenon long context\ntransfer and carefully ablate its properties. To effectively measure LMMs'\nability to generalize to long contexts in the vision modality, we develop\nV-NIAH (Visual Needle-In-A-Haystack), a purely synthetic long vision benchmark\ninspired by the language model's NIAH test. Our proposed Long Video Assistant\n(LongVA) can process 2000 frames or over 200K visual tokens without additional\ncomplexities. With its extended context length, LongVA achieves\nstate-of-the-art performance on Video-MME among 7B-scale models by densely\nsampling more input frames. Our work is open-sourced at\nhttps://github.com/EvolvingLMMs-Lab/LongVA.\n", "link": "http://arxiv.org/abs/2406.16852v1", "date": "2024-06-24", "relevancy": 2.176, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5488}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5463}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long%20Context%20Transfer%20from%20Language%20to%20Vision&body=Title%3A%20Long%20Context%20Transfer%20from%20Language%20to%20Vision%0AAuthor%3A%20Peiyuan%20Zhang%20and%20Kaichen%20Zhang%20and%20Bo%20Li%20and%20Guangtao%20Zeng%20and%20Jingkang%20Yang%20and%20Yuanhan%20Zhang%20and%20Ziyue%20Wang%20and%20Haoran%20Tan%20and%20Chunyuan%20Li%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Video%20sequences%20offer%20valuable%20temporal%20information%2C%20but%20existing%20large%0Amultimodal%20models%20%28LMMs%29%20fall%20short%20in%20understanding%20extremely%20long%20videos.%0AMany%20works%20address%20this%20by%20reducing%20the%20number%20of%20visual%20tokens%20using%20visual%0Aresamplers.%20Alternatively%2C%20in%20this%20paper%2C%20we%20approach%20this%20problem%20from%20the%0Aperspective%20of%20the%20language%20model.%20By%20simply%20extrapolating%20the%20context%20length%0Aof%20the%20language%20backbone%2C%20we%20enable%20LMMs%20to%20comprehend%20orders%20of%20magnitude%20more%0Avisual%20tokens%20without%20any%20video%20training.%20We%20call%20this%20phenomenon%20long%20context%0Atransfer%20and%20carefully%20ablate%20its%20properties.%20To%20effectively%20measure%20LMMs%27%0Aability%20to%20generalize%20to%20long%20contexts%20in%20the%20vision%20modality%2C%20we%20develop%0AV-NIAH%20%28Visual%20Needle-In-A-Haystack%29%2C%20a%20purely%20synthetic%20long%20vision%20benchmark%0Ainspired%20by%20the%20language%20model%27s%20NIAH%20test.%20Our%20proposed%20Long%20Video%20Assistant%0A%28LongVA%29%20can%20process%202000%20frames%20or%20over%20200K%20visual%20tokens%20without%20additional%0Acomplexities.%20With%20its%20extended%20context%20length%2C%20LongVA%20achieves%0Astate-of-the-art%20performance%20on%20Video-MME%20among%207B-scale%20models%20by%20densely%0Asampling%20more%20input%20frames.%20Our%20work%20is%20open-sourced%20at%0Ahttps%3A//github.com/EvolvingLMMs-Lab/LongVA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong%2520Context%2520Transfer%2520from%2520Language%2520to%2520Vision%26entry.906535625%3DPeiyuan%2520Zhang%2520and%2520Kaichen%2520Zhang%2520and%2520Bo%2520Li%2520and%2520Guangtao%2520Zeng%2520and%2520Jingkang%2520Yang%2520and%2520Yuanhan%2520Zhang%2520and%2520Ziyue%2520Wang%2520and%2520Haoran%2520Tan%2520and%2520Chunyuan%2520Li%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Video%2520sequences%2520offer%2520valuable%2520temporal%2520information%252C%2520but%2520existing%2520large%250Amultimodal%2520models%2520%2528LMMs%2529%2520fall%2520short%2520in%2520understanding%2520extremely%2520long%2520videos.%250AMany%2520works%2520address%2520this%2520by%2520reducing%2520the%2520number%2520of%2520visual%2520tokens%2520using%2520visual%250Aresamplers.%2520Alternatively%252C%2520in%2520this%2520paper%252C%2520we%2520approach%2520this%2520problem%2520from%2520the%250Aperspective%2520of%2520the%2520language%2520model.%2520By%2520simply%2520extrapolating%2520the%2520context%2520length%250Aof%2520the%2520language%2520backbone%252C%2520we%2520enable%2520LMMs%2520to%2520comprehend%2520orders%2520of%2520magnitude%2520more%250Avisual%2520tokens%2520without%2520any%2520video%2520training.%2520We%2520call%2520this%2520phenomenon%2520long%2520context%250Atransfer%2520and%2520carefully%2520ablate%2520its%2520properties.%2520To%2520effectively%2520measure%2520LMMs%2527%250Aability%2520to%2520generalize%2520to%2520long%2520contexts%2520in%2520the%2520vision%2520modality%252C%2520we%2520develop%250AV-NIAH%2520%2528Visual%2520Needle-In-A-Haystack%2529%252C%2520a%2520purely%2520synthetic%2520long%2520vision%2520benchmark%250Ainspired%2520by%2520the%2520language%2520model%2527s%2520NIAH%2520test.%2520Our%2520proposed%2520Long%2520Video%2520Assistant%250A%2528LongVA%2529%2520can%2520process%25202000%2520frames%2520or%2520over%2520200K%2520visual%2520tokens%2520without%2520additional%250Acomplexities.%2520With%2520its%2520extended%2520context%2520length%252C%2520LongVA%2520achieves%250Astate-of-the-art%2520performance%2520on%2520Video-MME%2520among%25207B-scale%2520models%2520by%2520densely%250Asampling%2520more%2520input%2520frames.%2520Our%2520work%2520is%2520open-sourced%2520at%250Ahttps%253A//github.com/EvolvingLMMs-Lab/LongVA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long%20Context%20Transfer%20from%20Language%20to%20Vision&entry.906535625=Peiyuan%20Zhang%20and%20Kaichen%20Zhang%20and%20Bo%20Li%20and%20Guangtao%20Zeng%20and%20Jingkang%20Yang%20and%20Yuanhan%20Zhang%20and%20Ziyue%20Wang%20and%20Haoran%20Tan%20and%20Chunyuan%20Li%20and%20Ziwei%20Liu&entry.1292438233=%20%20Video%20sequences%20offer%20valuable%20temporal%20information%2C%20but%20existing%20large%0Amultimodal%20models%20%28LMMs%29%20fall%20short%20in%20understanding%20extremely%20long%20videos.%0AMany%20works%20address%20this%20by%20reducing%20the%20number%20of%20visual%20tokens%20using%20visual%0Aresamplers.%20Alternatively%2C%20in%20this%20paper%2C%20we%20approach%20this%20problem%20from%20the%0Aperspective%20of%20the%20language%20model.%20By%20simply%20extrapolating%20the%20context%20length%0Aof%20the%20language%20backbone%2C%20we%20enable%20LMMs%20to%20comprehend%20orders%20of%20magnitude%20more%0Avisual%20tokens%20without%20any%20video%20training.%20We%20call%20this%20phenomenon%20long%20context%0Atransfer%20and%20carefully%20ablate%20its%20properties.%20To%20effectively%20measure%20LMMs%27%0Aability%20to%20generalize%20to%20long%20contexts%20in%20the%20vision%20modality%2C%20we%20develop%0AV-NIAH%20%28Visual%20Needle-In-A-Haystack%29%2C%20a%20purely%20synthetic%20long%20vision%20benchmark%0Ainspired%20by%20the%20language%20model%27s%20NIAH%20test.%20Our%20proposed%20Long%20Video%20Assistant%0A%28LongVA%29%20can%20process%202000%20frames%20or%20over%20200K%20visual%20tokens%20without%20additional%0Acomplexities.%20With%20its%20extended%20context%20length%2C%20LongVA%20achieves%0Astate-of-the-art%20performance%20on%20Video-MME%20among%207B-scale%20models%20by%20densely%0Asampling%20more%20input%20frames.%20Our%20work%20is%20open-sourced%20at%0Ahttps%3A//github.com/EvolvingLMMs-Lab/LongVA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16852v1&entry.124074799=Read"},
{"title": "OAML: Outlier Aware Metric Learning for OOD Detection Enhancement", "author": "Heng Gao and Zhuolin He and Shoumeng Qiu and Jian Pu", "abstract": "  Out-of-distribution (OOD) detection methods have been developed to identify\nobjects that a model has not seen during training. The Outlier Exposure (OE)\nmethods use auxiliary datasets to train OOD detectors directly. However, the\ncollection and learning of representative OOD samples may pose challenges. To\ntackle these issues, we propose the Outlier Aware Metric Learning (OAML)\nframework. The main idea of our method is to use the k-NN algorithm and Stable\nDiffusion model to generate outliers for training at the feature level without\nmaking any distributional assumptions. To increase feature discrepancies in the\nsemantic space, we develop a mutual information-based contrastive learning\napproach for learning from OOD data effectively. Both theoretical and empirical\nresults confirm the effectiveness of this contrastive learning technique.\nFurthermore, we incorporate knowledge distillation into our learning framework\nto prevent degradation of in-distribution classification accuracy. The\ncombination of contrastive learning and knowledge distillation algorithms\nsignificantly enhances the performance of OOD detection. Experimental results\nacross various datasets show that our method significantly outperforms previous\nOE methods.\n", "link": "http://arxiv.org/abs/2406.16525v1", "date": "2024-06-24", "relevancy": 2.1689, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5678}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5595}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OAML%3A%20Outlier%20Aware%20Metric%20Learning%20for%20OOD%20Detection%20Enhancement&body=Title%3A%20OAML%3A%20Outlier%20Aware%20Metric%20Learning%20for%20OOD%20Detection%20Enhancement%0AAuthor%3A%20Heng%20Gao%20and%20Zhuolin%20He%20and%20Shoumeng%20Qiu%20and%20Jian%20Pu%0AAbstract%3A%20%20%20Out-of-distribution%20%28OOD%29%20detection%20methods%20have%20been%20developed%20to%20identify%0Aobjects%20that%20a%20model%20has%20not%20seen%20during%20training.%20The%20Outlier%20Exposure%20%28OE%29%0Amethods%20use%20auxiliary%20datasets%20to%20train%20OOD%20detectors%20directly.%20However%2C%20the%0Acollection%20and%20learning%20of%20representative%20OOD%20samples%20may%20pose%20challenges.%20To%0Atackle%20these%20issues%2C%20we%20propose%20the%20Outlier%20Aware%20Metric%20Learning%20%28OAML%29%0Aframework.%20The%20main%20idea%20of%20our%20method%20is%20to%20use%20the%20k-NN%20algorithm%20and%20Stable%0ADiffusion%20model%20to%20generate%20outliers%20for%20training%20at%20the%20feature%20level%20without%0Amaking%20any%20distributional%20assumptions.%20To%20increase%20feature%20discrepancies%20in%20the%0Asemantic%20space%2C%20we%20develop%20a%20mutual%20information-based%20contrastive%20learning%0Aapproach%20for%20learning%20from%20OOD%20data%20effectively.%20Both%20theoretical%20and%20empirical%0Aresults%20confirm%20the%20effectiveness%20of%20this%20contrastive%20learning%20technique.%0AFurthermore%2C%20we%20incorporate%20knowledge%20distillation%20into%20our%20learning%20framework%0Ato%20prevent%20degradation%20of%20in-distribution%20classification%20accuracy.%20The%0Acombination%20of%20contrastive%20learning%20and%20knowledge%20distillation%20algorithms%0Asignificantly%20enhances%20the%20performance%20of%20OOD%20detection.%20Experimental%20results%0Aacross%20various%20datasets%20show%20that%20our%20method%20significantly%20outperforms%20previous%0AOE%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOAML%253A%2520Outlier%2520Aware%2520Metric%2520Learning%2520for%2520OOD%2520Detection%2520Enhancement%26entry.906535625%3DHeng%2520Gao%2520and%2520Zhuolin%2520He%2520and%2520Shoumeng%2520Qiu%2520and%2520Jian%2520Pu%26entry.1292438233%3D%2520%2520Out-of-distribution%2520%2528OOD%2529%2520detection%2520methods%2520have%2520been%2520developed%2520to%2520identify%250Aobjects%2520that%2520a%2520model%2520has%2520not%2520seen%2520during%2520training.%2520The%2520Outlier%2520Exposure%2520%2528OE%2529%250Amethods%2520use%2520auxiliary%2520datasets%2520to%2520train%2520OOD%2520detectors%2520directly.%2520However%252C%2520the%250Acollection%2520and%2520learning%2520of%2520representative%2520OOD%2520samples%2520may%2520pose%2520challenges.%2520To%250Atackle%2520these%2520issues%252C%2520we%2520propose%2520the%2520Outlier%2520Aware%2520Metric%2520Learning%2520%2528OAML%2529%250Aframework.%2520The%2520main%2520idea%2520of%2520our%2520method%2520is%2520to%2520use%2520the%2520k-NN%2520algorithm%2520and%2520Stable%250ADiffusion%2520model%2520to%2520generate%2520outliers%2520for%2520training%2520at%2520the%2520feature%2520level%2520without%250Amaking%2520any%2520distributional%2520assumptions.%2520To%2520increase%2520feature%2520discrepancies%2520in%2520the%250Asemantic%2520space%252C%2520we%2520develop%2520a%2520mutual%2520information-based%2520contrastive%2520learning%250Aapproach%2520for%2520learning%2520from%2520OOD%2520data%2520effectively.%2520Both%2520theoretical%2520and%2520empirical%250Aresults%2520confirm%2520the%2520effectiveness%2520of%2520this%2520contrastive%2520learning%2520technique.%250AFurthermore%252C%2520we%2520incorporate%2520knowledge%2520distillation%2520into%2520our%2520learning%2520framework%250Ato%2520prevent%2520degradation%2520of%2520in-distribution%2520classification%2520accuracy.%2520The%250Acombination%2520of%2520contrastive%2520learning%2520and%2520knowledge%2520distillation%2520algorithms%250Asignificantly%2520enhances%2520the%2520performance%2520of%2520OOD%2520detection.%2520Experimental%2520results%250Aacross%2520various%2520datasets%2520show%2520that%2520our%2520method%2520significantly%2520outperforms%2520previous%250AOE%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OAML%3A%20Outlier%20Aware%20Metric%20Learning%20for%20OOD%20Detection%20Enhancement&entry.906535625=Heng%20Gao%20and%20Zhuolin%20He%20and%20Shoumeng%20Qiu%20and%20Jian%20Pu&entry.1292438233=%20%20Out-of-distribution%20%28OOD%29%20detection%20methods%20have%20been%20developed%20to%20identify%0Aobjects%20that%20a%20model%20has%20not%20seen%20during%20training.%20The%20Outlier%20Exposure%20%28OE%29%0Amethods%20use%20auxiliary%20datasets%20to%20train%20OOD%20detectors%20directly.%20However%2C%20the%0Acollection%20and%20learning%20of%20representative%20OOD%20samples%20may%20pose%20challenges.%20To%0Atackle%20these%20issues%2C%20we%20propose%20the%20Outlier%20Aware%20Metric%20Learning%20%28OAML%29%0Aframework.%20The%20main%20idea%20of%20our%20method%20is%20to%20use%20the%20k-NN%20algorithm%20and%20Stable%0ADiffusion%20model%20to%20generate%20outliers%20for%20training%20at%20the%20feature%20level%20without%0Amaking%20any%20distributional%20assumptions.%20To%20increase%20feature%20discrepancies%20in%20the%0Asemantic%20space%2C%20we%20develop%20a%20mutual%20information-based%20contrastive%20learning%0Aapproach%20for%20learning%20from%20OOD%20data%20effectively.%20Both%20theoretical%20and%20empirical%0Aresults%20confirm%20the%20effectiveness%20of%20this%20contrastive%20learning%20technique.%0AFurthermore%2C%20we%20incorporate%20knowledge%20distillation%20into%20our%20learning%20framework%0Ato%20prevent%20degradation%20of%20in-distribution%20classification%20accuracy.%20The%0Acombination%20of%20contrastive%20learning%20and%20knowledge%20distillation%20algorithms%0Asignificantly%20enhances%20the%20performance%20of%20OOD%20detection.%20Experimental%20results%0Aacross%20various%20datasets%20show%20that%20our%20method%20significantly%20outperforms%20previous%0AOE%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16525v1&entry.124074799=Read"},
{"title": "GATSBI: An Online GTSP-Based Algorithm for Targeted Surface Bridge\n  Inspection and Defect Detection", "author": "Harnaik Dhami and Charith Reddy and Vishnu Dutt Sharma and Troi Williams and Pratap Tokekar", "abstract": "  We study the problem of visual surface inspection of infrastructure for\ndefects using an Unmanned Aerial Vehicle (UAV). We do not assume that the\ngeometric model of the infrastructure is known beforehand. Our planner, termed\nGATSBI, plans a path in a receding horizon fashion to inspect all points on the\nsurface of the infrastructure. The input to GATSBI consists of a 3D occupancy\nmap created online with 3D pointclouds. Occupied voxels corresponding to the\ninfrastructure in this map are semantically segmented and used to create an\ninfrastructure-only occupancy map. Inspecting an infrastructure voxel requires\nthe UAV to take images from a desired viewing angle and distance. We then\ncreate a Generalized Traveling Salesperson Problem (GTSP) instance to cluster\ncandidate viewpoints for inspecting the infrastructure voxels and use an\noff-the-shelf GTSP solver to find the optimal path for the given instance. As\nthe algorithm sees more parts of the environment over time, it replans the path\nto inspect uninspected parts of the infrastructure while avoiding obstacles. We\nevaluate the performance of our algorithm through high-fidelity simulations\nconducted in AirSim and real-world experiments. We compare the performance of\nGATSBI with a baseline inspection algorithm where the map is known a priori.\nOur evaluation reveals that targeting the inspection to only the segmented\ninfrastructure voxels and planning carefully using a GTSP solver leads to a\nmore efficient and thorough inspection than the baseline inspection algorithm.\n", "link": "http://arxiv.org/abs/2406.16625v1", "date": "2024-06-24", "relevancy": 2.1512, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5783}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5094}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GATSBI%3A%20An%20Online%20GTSP-Based%20Algorithm%20for%20Targeted%20Surface%20Bridge%0A%20%20Inspection%20and%20Defect%20Detection&body=Title%3A%20GATSBI%3A%20An%20Online%20GTSP-Based%20Algorithm%20for%20Targeted%20Surface%20Bridge%0A%20%20Inspection%20and%20Defect%20Detection%0AAuthor%3A%20Harnaik%20Dhami%20and%20Charith%20Reddy%20and%20Vishnu%20Dutt%20Sharma%20and%20Troi%20Williams%20and%20Pratap%20Tokekar%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20visual%20surface%20inspection%20of%20infrastructure%20for%0Adefects%20using%20an%20Unmanned%20Aerial%20Vehicle%20%28UAV%29.%20We%20do%20not%20assume%20that%20the%0Ageometric%20model%20of%20the%20infrastructure%20is%20known%20beforehand.%20Our%20planner%2C%20termed%0AGATSBI%2C%20plans%20a%20path%20in%20a%20receding%20horizon%20fashion%20to%20inspect%20all%20points%20on%20the%0Asurface%20of%20the%20infrastructure.%20The%20input%20to%20GATSBI%20consists%20of%20a%203D%20occupancy%0Amap%20created%20online%20with%203D%20pointclouds.%20Occupied%20voxels%20corresponding%20to%20the%0Ainfrastructure%20in%20this%20map%20are%20semantically%20segmented%20and%20used%20to%20create%20an%0Ainfrastructure-only%20occupancy%20map.%20Inspecting%20an%20infrastructure%20voxel%20requires%0Athe%20UAV%20to%20take%20images%20from%20a%20desired%20viewing%20angle%20and%20distance.%20We%20then%0Acreate%20a%20Generalized%20Traveling%20Salesperson%20Problem%20%28GTSP%29%20instance%20to%20cluster%0Acandidate%20viewpoints%20for%20inspecting%20the%20infrastructure%20voxels%20and%20use%20an%0Aoff-the-shelf%20GTSP%20solver%20to%20find%20the%20optimal%20path%20for%20the%20given%20instance.%20As%0Athe%20algorithm%20sees%20more%20parts%20of%20the%20environment%20over%20time%2C%20it%20replans%20the%20path%0Ato%20inspect%20uninspected%20parts%20of%20the%20infrastructure%20while%20avoiding%20obstacles.%20We%0Aevaluate%20the%20performance%20of%20our%20algorithm%20through%20high-fidelity%20simulations%0Aconducted%20in%20AirSim%20and%20real-world%20experiments.%20We%20compare%20the%20performance%20of%0AGATSBI%20with%20a%20baseline%20inspection%20algorithm%20where%20the%20map%20is%20known%20a%20priori.%0AOur%20evaluation%20reveals%20that%20targeting%20the%20inspection%20to%20only%20the%20segmented%0Ainfrastructure%20voxels%20and%20planning%20carefully%20using%20a%20GTSP%20solver%20leads%20to%20a%0Amore%20efficient%20and%20thorough%20inspection%20than%20the%20baseline%20inspection%20algorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGATSBI%253A%2520An%2520Online%2520GTSP-Based%2520Algorithm%2520for%2520Targeted%2520Surface%2520Bridge%250A%2520%2520Inspection%2520and%2520Defect%2520Detection%26entry.906535625%3DHarnaik%2520Dhami%2520and%2520Charith%2520Reddy%2520and%2520Vishnu%2520Dutt%2520Sharma%2520and%2520Troi%2520Williams%2520and%2520Pratap%2520Tokekar%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520visual%2520surface%2520inspection%2520of%2520infrastructure%2520for%250Adefects%2520using%2520an%2520Unmanned%2520Aerial%2520Vehicle%2520%2528UAV%2529.%2520We%2520do%2520not%2520assume%2520that%2520the%250Ageometric%2520model%2520of%2520the%2520infrastructure%2520is%2520known%2520beforehand.%2520Our%2520planner%252C%2520termed%250AGATSBI%252C%2520plans%2520a%2520path%2520in%2520a%2520receding%2520horizon%2520fashion%2520to%2520inspect%2520all%2520points%2520on%2520the%250Asurface%2520of%2520the%2520infrastructure.%2520The%2520input%2520to%2520GATSBI%2520consists%2520of%2520a%25203D%2520occupancy%250Amap%2520created%2520online%2520with%25203D%2520pointclouds.%2520Occupied%2520voxels%2520corresponding%2520to%2520the%250Ainfrastructure%2520in%2520this%2520map%2520are%2520semantically%2520segmented%2520and%2520used%2520to%2520create%2520an%250Ainfrastructure-only%2520occupancy%2520map.%2520Inspecting%2520an%2520infrastructure%2520voxel%2520requires%250Athe%2520UAV%2520to%2520take%2520images%2520from%2520a%2520desired%2520viewing%2520angle%2520and%2520distance.%2520We%2520then%250Acreate%2520a%2520Generalized%2520Traveling%2520Salesperson%2520Problem%2520%2528GTSP%2529%2520instance%2520to%2520cluster%250Acandidate%2520viewpoints%2520for%2520inspecting%2520the%2520infrastructure%2520voxels%2520and%2520use%2520an%250Aoff-the-shelf%2520GTSP%2520solver%2520to%2520find%2520the%2520optimal%2520path%2520for%2520the%2520given%2520instance.%2520As%250Athe%2520algorithm%2520sees%2520more%2520parts%2520of%2520the%2520environment%2520over%2520time%252C%2520it%2520replans%2520the%2520path%250Ato%2520inspect%2520uninspected%2520parts%2520of%2520the%2520infrastructure%2520while%2520avoiding%2520obstacles.%2520We%250Aevaluate%2520the%2520performance%2520of%2520our%2520algorithm%2520through%2520high-fidelity%2520simulations%250Aconducted%2520in%2520AirSim%2520and%2520real-world%2520experiments.%2520We%2520compare%2520the%2520performance%2520of%250AGATSBI%2520with%2520a%2520baseline%2520inspection%2520algorithm%2520where%2520the%2520map%2520is%2520known%2520a%2520priori.%250AOur%2520evaluation%2520reveals%2520that%2520targeting%2520the%2520inspection%2520to%2520only%2520the%2520segmented%250Ainfrastructure%2520voxels%2520and%2520planning%2520carefully%2520using%2520a%2520GTSP%2520solver%2520leads%2520to%2520a%250Amore%2520efficient%2520and%2520thorough%2520inspection%2520than%2520the%2520baseline%2520inspection%2520algorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GATSBI%3A%20An%20Online%20GTSP-Based%20Algorithm%20for%20Targeted%20Surface%20Bridge%0A%20%20Inspection%20and%20Defect%20Detection&entry.906535625=Harnaik%20Dhami%20and%20Charith%20Reddy%20and%20Vishnu%20Dutt%20Sharma%20and%20Troi%20Williams%20and%20Pratap%20Tokekar&entry.1292438233=%20%20We%20study%20the%20problem%20of%20visual%20surface%20inspection%20of%20infrastructure%20for%0Adefects%20using%20an%20Unmanned%20Aerial%20Vehicle%20%28UAV%29.%20We%20do%20not%20assume%20that%20the%0Ageometric%20model%20of%20the%20infrastructure%20is%20known%20beforehand.%20Our%20planner%2C%20termed%0AGATSBI%2C%20plans%20a%20path%20in%20a%20receding%20horizon%20fashion%20to%20inspect%20all%20points%20on%20the%0Asurface%20of%20the%20infrastructure.%20The%20input%20to%20GATSBI%20consists%20of%20a%203D%20occupancy%0Amap%20created%20online%20with%203D%20pointclouds.%20Occupied%20voxels%20corresponding%20to%20the%0Ainfrastructure%20in%20this%20map%20are%20semantically%20segmented%20and%20used%20to%20create%20an%0Ainfrastructure-only%20occupancy%20map.%20Inspecting%20an%20infrastructure%20voxel%20requires%0Athe%20UAV%20to%20take%20images%20from%20a%20desired%20viewing%20angle%20and%20distance.%20We%20then%0Acreate%20a%20Generalized%20Traveling%20Salesperson%20Problem%20%28GTSP%29%20instance%20to%20cluster%0Acandidate%20viewpoints%20for%20inspecting%20the%20infrastructure%20voxels%20and%20use%20an%0Aoff-the-shelf%20GTSP%20solver%20to%20find%20the%20optimal%20path%20for%20the%20given%20instance.%20As%0Athe%20algorithm%20sees%20more%20parts%20of%20the%20environment%20over%20time%2C%20it%20replans%20the%20path%0Ato%20inspect%20uninspected%20parts%20of%20the%20infrastructure%20while%20avoiding%20obstacles.%20We%0Aevaluate%20the%20performance%20of%20our%20algorithm%20through%20high-fidelity%20simulations%0Aconducted%20in%20AirSim%20and%20real-world%20experiments.%20We%20compare%20the%20performance%20of%0AGATSBI%20with%20a%20baseline%20inspection%20algorithm%20where%20the%20map%20is%20known%20a%20priori.%0AOur%20evaluation%20reveals%20that%20targeting%20the%20inspection%20to%20only%20the%20segmented%0Ainfrastructure%20voxels%20and%20planning%20carefully%20using%20a%20GTSP%20solver%20leads%20to%20a%0Amore%20efficient%20and%20thorough%20inspection%20than%20the%20baseline%20inspection%20algorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16625v1&entry.124074799=Read"},
{"title": "EGTR: Extracting Graph from Transformer for Scene Graph Generation", "author": "Jinbae Im and JeongYeon Nam and Nokyung Park and Hyungmin Lee and Seunghyun Park", "abstract": "  Scene Graph Generation (SGG) is a challenging task of detecting objects and\npredicting relationships between objects. After DETR was developed, one-stage\nSGG models based on a one-stage object detector have been actively studied.\nHowever, complex modeling is used to predict the relationship between objects,\nand the inherent relationship between object queries learned in the multi-head\nself-attention of the object detector has been neglected. We propose a\nlightweight one-stage SGG model that extracts the relation graph from the\nvarious relationships learned in the multi-head self-attention layers of the\nDETR decoder. By fully utilizing the self-attention by-products, the relation\ngraph can be extracted effectively with a shallow relation extraction head.\nConsidering the dependency of the relation extraction task on the object\ndetection task, we propose a novel relation smoothing technique that adjusts\nthe relation label adaptively according to the quality of the detected objects.\nBy the relation smoothing, the model is trained according to the continuous\ncurriculum that focuses on object detection task at the beginning of training\nand performs multi-task learning as the object detection performance gradually\nimproves. Furthermore, we propose a connectivity prediction task that predicts\nwhether a relation exists between object pairs as an auxiliary task of the\nrelation extraction. We demonstrate the effectiveness and efficiency of our\nmethod for the Visual Genome and Open Image V6 datasets. Our code is publicly\navailable at https://github.com/naver-ai/egtr.\n", "link": "http://arxiv.org/abs/2404.02072v5", "date": "2024-06-24", "relevancy": 2.1475, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5633}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5242}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EGTR%3A%20Extracting%20Graph%20from%20Transformer%20for%20Scene%20Graph%20Generation&body=Title%3A%20EGTR%3A%20Extracting%20Graph%20from%20Transformer%20for%20Scene%20Graph%20Generation%0AAuthor%3A%20Jinbae%20Im%20and%20JeongYeon%20Nam%20and%20Nokyung%20Park%20and%20Hyungmin%20Lee%20and%20Seunghyun%20Park%0AAbstract%3A%20%20%20Scene%20Graph%20Generation%20%28SGG%29%20is%20a%20challenging%20task%20of%20detecting%20objects%20and%0Apredicting%20relationships%20between%20objects.%20After%20DETR%20was%20developed%2C%20one-stage%0ASGG%20models%20based%20on%20a%20one-stage%20object%20detector%20have%20been%20actively%20studied.%0AHowever%2C%20complex%20modeling%20is%20used%20to%20predict%20the%20relationship%20between%20objects%2C%0Aand%20the%20inherent%20relationship%20between%20object%20queries%20learned%20in%20the%20multi-head%0Aself-attention%20of%20the%20object%20detector%20has%20been%20neglected.%20We%20propose%20a%0Alightweight%20one-stage%20SGG%20model%20that%20extracts%20the%20relation%20graph%20from%20the%0Avarious%20relationships%20learned%20in%20the%20multi-head%20self-attention%20layers%20of%20the%0ADETR%20decoder.%20By%20fully%20utilizing%20the%20self-attention%20by-products%2C%20the%20relation%0Agraph%20can%20be%20extracted%20effectively%20with%20a%20shallow%20relation%20extraction%20head.%0AConsidering%20the%20dependency%20of%20the%20relation%20extraction%20task%20on%20the%20object%0Adetection%20task%2C%20we%20propose%20a%20novel%20relation%20smoothing%20technique%20that%20adjusts%0Athe%20relation%20label%20adaptively%20according%20to%20the%20quality%20of%20the%20detected%20objects.%0ABy%20the%20relation%20smoothing%2C%20the%20model%20is%20trained%20according%20to%20the%20continuous%0Acurriculum%20that%20focuses%20on%20object%20detection%20task%20at%20the%20beginning%20of%20training%0Aand%20performs%20multi-task%20learning%20as%20the%20object%20detection%20performance%20gradually%0Aimproves.%20Furthermore%2C%20we%20propose%20a%20connectivity%20prediction%20task%20that%20predicts%0Awhether%20a%20relation%20exists%20between%20object%20pairs%20as%20an%20auxiliary%20task%20of%20the%0Arelation%20extraction.%20We%20demonstrate%20the%20effectiveness%20and%20efficiency%20of%20our%0Amethod%20for%20the%20Visual%20Genome%20and%20Open%20Image%20V6%20datasets.%20Our%20code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/naver-ai/egtr.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.02072v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEGTR%253A%2520Extracting%2520Graph%2520from%2520Transformer%2520for%2520Scene%2520Graph%2520Generation%26entry.906535625%3DJinbae%2520Im%2520and%2520JeongYeon%2520Nam%2520and%2520Nokyung%2520Park%2520and%2520Hyungmin%2520Lee%2520and%2520Seunghyun%2520Park%26entry.1292438233%3D%2520%2520Scene%2520Graph%2520Generation%2520%2528SGG%2529%2520is%2520a%2520challenging%2520task%2520of%2520detecting%2520objects%2520and%250Apredicting%2520relationships%2520between%2520objects.%2520After%2520DETR%2520was%2520developed%252C%2520one-stage%250ASGG%2520models%2520based%2520on%2520a%2520one-stage%2520object%2520detector%2520have%2520been%2520actively%2520studied.%250AHowever%252C%2520complex%2520modeling%2520is%2520used%2520to%2520predict%2520the%2520relationship%2520between%2520objects%252C%250Aand%2520the%2520inherent%2520relationship%2520between%2520object%2520queries%2520learned%2520in%2520the%2520multi-head%250Aself-attention%2520of%2520the%2520object%2520detector%2520has%2520been%2520neglected.%2520We%2520propose%2520a%250Alightweight%2520one-stage%2520SGG%2520model%2520that%2520extracts%2520the%2520relation%2520graph%2520from%2520the%250Avarious%2520relationships%2520learned%2520in%2520the%2520multi-head%2520self-attention%2520layers%2520of%2520the%250ADETR%2520decoder.%2520By%2520fully%2520utilizing%2520the%2520self-attention%2520by-products%252C%2520the%2520relation%250Agraph%2520can%2520be%2520extracted%2520effectively%2520with%2520a%2520shallow%2520relation%2520extraction%2520head.%250AConsidering%2520the%2520dependency%2520of%2520the%2520relation%2520extraction%2520task%2520on%2520the%2520object%250Adetection%2520task%252C%2520we%2520propose%2520a%2520novel%2520relation%2520smoothing%2520technique%2520that%2520adjusts%250Athe%2520relation%2520label%2520adaptively%2520according%2520to%2520the%2520quality%2520of%2520the%2520detected%2520objects.%250ABy%2520the%2520relation%2520smoothing%252C%2520the%2520model%2520is%2520trained%2520according%2520to%2520the%2520continuous%250Acurriculum%2520that%2520focuses%2520on%2520object%2520detection%2520task%2520at%2520the%2520beginning%2520of%2520training%250Aand%2520performs%2520multi-task%2520learning%2520as%2520the%2520object%2520detection%2520performance%2520gradually%250Aimproves.%2520Furthermore%252C%2520we%2520propose%2520a%2520connectivity%2520prediction%2520task%2520that%2520predicts%250Awhether%2520a%2520relation%2520exists%2520between%2520object%2520pairs%2520as%2520an%2520auxiliary%2520task%2520of%2520the%250Arelation%2520extraction.%2520We%2520demonstrate%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520our%250Amethod%2520for%2520the%2520Visual%2520Genome%2520and%2520Open%2520Image%2520V6%2520datasets.%2520Our%2520code%2520is%2520publicly%250Aavailable%2520at%2520https%253A//github.com/naver-ai/egtr.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.02072v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EGTR%3A%20Extracting%20Graph%20from%20Transformer%20for%20Scene%20Graph%20Generation&entry.906535625=Jinbae%20Im%20and%20JeongYeon%20Nam%20and%20Nokyung%20Park%20and%20Hyungmin%20Lee%20and%20Seunghyun%20Park&entry.1292438233=%20%20Scene%20Graph%20Generation%20%28SGG%29%20is%20a%20challenging%20task%20of%20detecting%20objects%20and%0Apredicting%20relationships%20between%20objects.%20After%20DETR%20was%20developed%2C%20one-stage%0ASGG%20models%20based%20on%20a%20one-stage%20object%20detector%20have%20been%20actively%20studied.%0AHowever%2C%20complex%20modeling%20is%20used%20to%20predict%20the%20relationship%20between%20objects%2C%0Aand%20the%20inherent%20relationship%20between%20object%20queries%20learned%20in%20the%20multi-head%0Aself-attention%20of%20the%20object%20detector%20has%20been%20neglected.%20We%20propose%20a%0Alightweight%20one-stage%20SGG%20model%20that%20extracts%20the%20relation%20graph%20from%20the%0Avarious%20relationships%20learned%20in%20the%20multi-head%20self-attention%20layers%20of%20the%0ADETR%20decoder.%20By%20fully%20utilizing%20the%20self-attention%20by-products%2C%20the%20relation%0Agraph%20can%20be%20extracted%20effectively%20with%20a%20shallow%20relation%20extraction%20head.%0AConsidering%20the%20dependency%20of%20the%20relation%20extraction%20task%20on%20the%20object%0Adetection%20task%2C%20we%20propose%20a%20novel%20relation%20smoothing%20technique%20that%20adjusts%0Athe%20relation%20label%20adaptively%20according%20to%20the%20quality%20of%20the%20detected%20objects.%0ABy%20the%20relation%20smoothing%2C%20the%20model%20is%20trained%20according%20to%20the%20continuous%0Acurriculum%20that%20focuses%20on%20object%20detection%20task%20at%20the%20beginning%20of%20training%0Aand%20performs%20multi-task%20learning%20as%20the%20object%20detection%20performance%20gradually%0Aimproves.%20Furthermore%2C%20we%20propose%20a%20connectivity%20prediction%20task%20that%20predicts%0Awhether%20a%20relation%20exists%20between%20object%20pairs%20as%20an%20auxiliary%20task%20of%20the%0Arelation%20extraction.%20We%20demonstrate%20the%20effectiveness%20and%20efficiency%20of%20our%0Amethod%20for%20the%20Visual%20Genome%20and%20Open%20Image%20V6%20datasets.%20Our%20code%20is%20publicly%0Aavailable%20at%20https%3A//github.com/naver-ai/egtr.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.02072v5&entry.124074799=Read"},
{"title": "GeoMFormer: A General Architecture for Geometric Molecular\n  Representation Learning", "author": "Tianlang Chen and Shengjie Luo and Di He and Shuxin Zheng and Tie-Yan Liu and Liwei Wang", "abstract": "  Molecular modeling, a central topic in quantum mechanics, aims to accurately\ncalculate the properties and simulate the behaviors of molecular systems. The\nmolecular model is governed by physical laws, which impose geometric\nconstraints such as invariance and equivariance to coordinate rotation and\ntranslation. While numerous deep learning approaches have been developed to\nlearn molecular representations under these constraints, most of them are built\nupon heuristic and costly modules. We argue that there is a strong need for a\ngeneral and flexible framework for learning both invariant and equivariant\nfeatures. In this work, we introduce a novel Transformer-based molecular model\ncalled GeoMFormer to achieve this goal. Using the standard Transformer modules,\ntwo separate streams are developed to maintain and learn invariant and\nequivariant representations. Carefully designed cross-attention modules bridge\nthe two streams, allowing information fusion and enhancing geometric modeling\nin each stream. As a general and flexible architecture, we show that many\nprevious architectures can be viewed as special instantiations of GeoMFormer.\nExtensive experiments are conducted to demonstrate the power of GeoMFormer. All\nempirical results show that GeoMFormer achieves strong performance on both\ninvariant and equivariant tasks of different types and scales. Code and models\nwill be made publicly available at https://github.com/c-tl/GeoMFormer.\n", "link": "http://arxiv.org/abs/2406.16853v1", "date": "2024-06-24", "relevancy": 2.1269, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5505}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.521}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoMFormer%3A%20A%20General%20Architecture%20for%20Geometric%20Molecular%0A%20%20Representation%20Learning&body=Title%3A%20GeoMFormer%3A%20A%20General%20Architecture%20for%20Geometric%20Molecular%0A%20%20Representation%20Learning%0AAuthor%3A%20Tianlang%20Chen%20and%20Shengjie%20Luo%20and%20Di%20He%20and%20Shuxin%20Zheng%20and%20Tie-Yan%20Liu%20and%20Liwei%20Wang%0AAbstract%3A%20%20%20Molecular%20modeling%2C%20a%20central%20topic%20in%20quantum%20mechanics%2C%20aims%20to%20accurately%0Acalculate%20the%20properties%20and%20simulate%20the%20behaviors%20of%20molecular%20systems.%20The%0Amolecular%20model%20is%20governed%20by%20physical%20laws%2C%20which%20impose%20geometric%0Aconstraints%20such%20as%20invariance%20and%20equivariance%20to%20coordinate%20rotation%20and%0Atranslation.%20While%20numerous%20deep%20learning%20approaches%20have%20been%20developed%20to%0Alearn%20molecular%20representations%20under%20these%20constraints%2C%20most%20of%20them%20are%20built%0Aupon%20heuristic%20and%20costly%20modules.%20We%20argue%20that%20there%20is%20a%20strong%20need%20for%20a%0Ageneral%20and%20flexible%20framework%20for%20learning%20both%20invariant%20and%20equivariant%0Afeatures.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20Transformer-based%20molecular%20model%0Acalled%20GeoMFormer%20to%20achieve%20this%20goal.%20Using%20the%20standard%20Transformer%20modules%2C%0Atwo%20separate%20streams%20are%20developed%20to%20maintain%20and%20learn%20invariant%20and%0Aequivariant%20representations.%20Carefully%20designed%20cross-attention%20modules%20bridge%0Athe%20two%20streams%2C%20allowing%20information%20fusion%20and%20enhancing%20geometric%20modeling%0Ain%20each%20stream.%20As%20a%20general%20and%20flexible%20architecture%2C%20we%20show%20that%20many%0Aprevious%20architectures%20can%20be%20viewed%20as%20special%20instantiations%20of%20GeoMFormer.%0AExtensive%20experiments%20are%20conducted%20to%20demonstrate%20the%20power%20of%20GeoMFormer.%20All%0Aempirical%20results%20show%20that%20GeoMFormer%20achieves%20strong%20performance%20on%20both%0Ainvariant%20and%20equivariant%20tasks%20of%20different%20types%20and%20scales.%20Code%20and%20models%0Awill%20be%20made%20publicly%20available%20at%20https%3A//github.com/c-tl/GeoMFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoMFormer%253A%2520A%2520General%2520Architecture%2520for%2520Geometric%2520Molecular%250A%2520%2520Representation%2520Learning%26entry.906535625%3DTianlang%2520Chen%2520and%2520Shengjie%2520Luo%2520and%2520Di%2520He%2520and%2520Shuxin%2520Zheng%2520and%2520Tie-Yan%2520Liu%2520and%2520Liwei%2520Wang%26entry.1292438233%3D%2520%2520Molecular%2520modeling%252C%2520a%2520central%2520topic%2520in%2520quantum%2520mechanics%252C%2520aims%2520to%2520accurately%250Acalculate%2520the%2520properties%2520and%2520simulate%2520the%2520behaviors%2520of%2520molecular%2520systems.%2520The%250Amolecular%2520model%2520is%2520governed%2520by%2520physical%2520laws%252C%2520which%2520impose%2520geometric%250Aconstraints%2520such%2520as%2520invariance%2520and%2520equivariance%2520to%2520coordinate%2520rotation%2520and%250Atranslation.%2520While%2520numerous%2520deep%2520learning%2520approaches%2520have%2520been%2520developed%2520to%250Alearn%2520molecular%2520representations%2520under%2520these%2520constraints%252C%2520most%2520of%2520them%2520are%2520built%250Aupon%2520heuristic%2520and%2520costly%2520modules.%2520We%2520argue%2520that%2520there%2520is%2520a%2520strong%2520need%2520for%2520a%250Ageneral%2520and%2520flexible%2520framework%2520for%2520learning%2520both%2520invariant%2520and%2520equivariant%250Afeatures.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520Transformer-based%2520molecular%2520model%250Acalled%2520GeoMFormer%2520to%2520achieve%2520this%2520goal.%2520Using%2520the%2520standard%2520Transformer%2520modules%252C%250Atwo%2520separate%2520streams%2520are%2520developed%2520to%2520maintain%2520and%2520learn%2520invariant%2520and%250Aequivariant%2520representations.%2520Carefully%2520designed%2520cross-attention%2520modules%2520bridge%250Athe%2520two%2520streams%252C%2520allowing%2520information%2520fusion%2520and%2520enhancing%2520geometric%2520modeling%250Ain%2520each%2520stream.%2520As%2520a%2520general%2520and%2520flexible%2520architecture%252C%2520we%2520show%2520that%2520many%250Aprevious%2520architectures%2520can%2520be%2520viewed%2520as%2520special%2520instantiations%2520of%2520GeoMFormer.%250AExtensive%2520experiments%2520are%2520conducted%2520to%2520demonstrate%2520the%2520power%2520of%2520GeoMFormer.%2520All%250Aempirical%2520results%2520show%2520that%2520GeoMFormer%2520achieves%2520strong%2520performance%2520on%2520both%250Ainvariant%2520and%2520equivariant%2520tasks%2520of%2520different%2520types%2520and%2520scales.%2520Code%2520and%2520models%250Awill%2520be%2520made%2520publicly%2520available%2520at%2520https%253A//github.com/c-tl/GeoMFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoMFormer%3A%20A%20General%20Architecture%20for%20Geometric%20Molecular%0A%20%20Representation%20Learning&entry.906535625=Tianlang%20Chen%20and%20Shengjie%20Luo%20and%20Di%20He%20and%20Shuxin%20Zheng%20and%20Tie-Yan%20Liu%20and%20Liwei%20Wang&entry.1292438233=%20%20Molecular%20modeling%2C%20a%20central%20topic%20in%20quantum%20mechanics%2C%20aims%20to%20accurately%0Acalculate%20the%20properties%20and%20simulate%20the%20behaviors%20of%20molecular%20systems.%20The%0Amolecular%20model%20is%20governed%20by%20physical%20laws%2C%20which%20impose%20geometric%0Aconstraints%20such%20as%20invariance%20and%20equivariance%20to%20coordinate%20rotation%20and%0Atranslation.%20While%20numerous%20deep%20learning%20approaches%20have%20been%20developed%20to%0Alearn%20molecular%20representations%20under%20these%20constraints%2C%20most%20of%20them%20are%20built%0Aupon%20heuristic%20and%20costly%20modules.%20We%20argue%20that%20there%20is%20a%20strong%20need%20for%20a%0Ageneral%20and%20flexible%20framework%20for%20learning%20both%20invariant%20and%20equivariant%0Afeatures.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20Transformer-based%20molecular%20model%0Acalled%20GeoMFormer%20to%20achieve%20this%20goal.%20Using%20the%20standard%20Transformer%20modules%2C%0Atwo%20separate%20streams%20are%20developed%20to%20maintain%20and%20learn%20invariant%20and%0Aequivariant%20representations.%20Carefully%20designed%20cross-attention%20modules%20bridge%0Athe%20two%20streams%2C%20allowing%20information%20fusion%20and%20enhancing%20geometric%20modeling%0Ain%20each%20stream.%20As%20a%20general%20and%20flexible%20architecture%2C%20we%20show%20that%20many%0Aprevious%20architectures%20can%20be%20viewed%20as%20special%20instantiations%20of%20GeoMFormer.%0AExtensive%20experiments%20are%20conducted%20to%20demonstrate%20the%20power%20of%20GeoMFormer.%20All%0Aempirical%20results%20show%20that%20GeoMFormer%20achieves%20strong%20performance%20on%20both%0Ainvariant%20and%20equivariant%20tasks%20of%20different%20types%20and%20scales.%20Code%20and%20models%0Awill%20be%20made%20publicly%20available%20at%20https%3A//github.com/c-tl/GeoMFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16853v1&entry.124074799=Read"},
{"title": "Cross-domain Transfer of Valence Preferences via a Meta-optimization\n  Approach", "author": "Chuang Zhao and Hongke Zhao and Ming He and Xiaomeng Li and Jianping Fan", "abstract": "  Cross-domain recommendation offers a potential avenue for alleviating data\nsparsity and cold-start problems. Embedding and mapping, as a classic\ncross-domain research genre, aims to identify a common mapping function to\nperform representation transformation between two domains. Nevertheless,\nprevious coarse-grained preference representations, non-personalized mapping\nfunctions, and excessive reliance on overlapping users limit their performance,\nespecially in scenarios where overlapping users are sparse. To address\naforementioned challenges, we propose a novel cross-domain approach, namely\nCVPM. CVPM formalizes cross-domain interest transfer as a hybrid architecture\nof parametric meta-learning and self-supervised learning, which not only\ntransfers user preferences at a finer level, but also enables signal\nenhancement with the knowledge of non-overlapping users. Specifically, with\ndeep insights into user preferences and valence preference theory, we believe\nthat there exists significant difference between users' positive preferences\nand negative behaviors, and thus employ differentiated encoders to learn their\ndistributions. In particular, we further utilize the pre-trained model and item\npopularity to sample pseudo-interaction items to ensure the integrity of both\ndistributions. To guarantee the personalization of preference transfer, we\ntreat each user's mapping as two parts, the common transformation and the\npersonalized bias, where the network used to generate the personalized bias is\noutput by a meta-learner. Furthermore, in addition to the supervised loss for\noverlapping users, we design contrastive tasks for non-overlapping users from\nboth group and individual-levels to avoid model skew and enhance the semantics\nof representations. Exhaustive data analysis and extensive experimental results\ndemonstrate the effectiveness and advancement of our proposed framework.\n", "link": "http://arxiv.org/abs/2406.16494v1", "date": "2024-06-24", "relevancy": 2.1229, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5359}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5332}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-domain%20Transfer%20of%20Valence%20Preferences%20via%20a%20Meta-optimization%0A%20%20Approach&body=Title%3A%20Cross-domain%20Transfer%20of%20Valence%20Preferences%20via%20a%20Meta-optimization%0A%20%20Approach%0AAuthor%3A%20Chuang%20Zhao%20and%20Hongke%20Zhao%20and%20Ming%20He%20and%20Xiaomeng%20Li%20and%20Jianping%20Fan%0AAbstract%3A%20%20%20Cross-domain%20recommendation%20offers%20a%20potential%20avenue%20for%20alleviating%20data%0Asparsity%20and%20cold-start%20problems.%20Embedding%20and%20mapping%2C%20as%20a%20classic%0Across-domain%20research%20genre%2C%20aims%20to%20identify%20a%20common%20mapping%20function%20to%0Aperform%20representation%20transformation%20between%20two%20domains.%20Nevertheless%2C%0Aprevious%20coarse-grained%20preference%20representations%2C%20non-personalized%20mapping%0Afunctions%2C%20and%20excessive%20reliance%20on%20overlapping%20users%20limit%20their%20performance%2C%0Aespecially%20in%20scenarios%20where%20overlapping%20users%20are%20sparse.%20To%20address%0Aaforementioned%20challenges%2C%20we%20propose%20a%20novel%20cross-domain%20approach%2C%20namely%0ACVPM.%20CVPM%20formalizes%20cross-domain%20interest%20transfer%20as%20a%20hybrid%20architecture%0Aof%20parametric%20meta-learning%20and%20self-supervised%20learning%2C%20which%20not%20only%0Atransfers%20user%20preferences%20at%20a%20finer%20level%2C%20but%20also%20enables%20signal%0Aenhancement%20with%20the%20knowledge%20of%20non-overlapping%20users.%20Specifically%2C%20with%0Adeep%20insights%20into%20user%20preferences%20and%20valence%20preference%20theory%2C%20we%20believe%0Athat%20there%20exists%20significant%20difference%20between%20users%27%20positive%20preferences%0Aand%20negative%20behaviors%2C%20and%20thus%20employ%20differentiated%20encoders%20to%20learn%20their%0Adistributions.%20In%20particular%2C%20we%20further%20utilize%20the%20pre-trained%20model%20and%20item%0Apopularity%20to%20sample%20pseudo-interaction%20items%20to%20ensure%20the%20integrity%20of%20both%0Adistributions.%20To%20guarantee%20the%20personalization%20of%20preference%20transfer%2C%20we%0Atreat%20each%20user%27s%20mapping%20as%20two%20parts%2C%20the%20common%20transformation%20and%20the%0Apersonalized%20bias%2C%20where%20the%20network%20used%20to%20generate%20the%20personalized%20bias%20is%0Aoutput%20by%20a%20meta-learner.%20Furthermore%2C%20in%20addition%20to%20the%20supervised%20loss%20for%0Aoverlapping%20users%2C%20we%20design%20contrastive%20tasks%20for%20non-overlapping%20users%20from%0Aboth%20group%20and%20individual-levels%20to%20avoid%20model%20skew%20and%20enhance%20the%20semantics%0Aof%20representations.%20Exhaustive%20data%20analysis%20and%20extensive%20experimental%20results%0Ademonstrate%20the%20effectiveness%20and%20advancement%20of%20our%20proposed%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-domain%2520Transfer%2520of%2520Valence%2520Preferences%2520via%2520a%2520Meta-optimization%250A%2520%2520Approach%26entry.906535625%3DChuang%2520Zhao%2520and%2520Hongke%2520Zhao%2520and%2520Ming%2520He%2520and%2520Xiaomeng%2520Li%2520and%2520Jianping%2520Fan%26entry.1292438233%3D%2520%2520Cross-domain%2520recommendation%2520offers%2520a%2520potential%2520avenue%2520for%2520alleviating%2520data%250Asparsity%2520and%2520cold-start%2520problems.%2520Embedding%2520and%2520mapping%252C%2520as%2520a%2520classic%250Across-domain%2520research%2520genre%252C%2520aims%2520to%2520identify%2520a%2520common%2520mapping%2520function%2520to%250Aperform%2520representation%2520transformation%2520between%2520two%2520domains.%2520Nevertheless%252C%250Aprevious%2520coarse-grained%2520preference%2520representations%252C%2520non-personalized%2520mapping%250Afunctions%252C%2520and%2520excessive%2520reliance%2520on%2520overlapping%2520users%2520limit%2520their%2520performance%252C%250Aespecially%2520in%2520scenarios%2520where%2520overlapping%2520users%2520are%2520sparse.%2520To%2520address%250Aaforementioned%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520cross-domain%2520approach%252C%2520namely%250ACVPM.%2520CVPM%2520formalizes%2520cross-domain%2520interest%2520transfer%2520as%2520a%2520hybrid%2520architecture%250Aof%2520parametric%2520meta-learning%2520and%2520self-supervised%2520learning%252C%2520which%2520not%2520only%250Atransfers%2520user%2520preferences%2520at%2520a%2520finer%2520level%252C%2520but%2520also%2520enables%2520signal%250Aenhancement%2520with%2520the%2520knowledge%2520of%2520non-overlapping%2520users.%2520Specifically%252C%2520with%250Adeep%2520insights%2520into%2520user%2520preferences%2520and%2520valence%2520preference%2520theory%252C%2520we%2520believe%250Athat%2520there%2520exists%2520significant%2520difference%2520between%2520users%2527%2520positive%2520preferences%250Aand%2520negative%2520behaviors%252C%2520and%2520thus%2520employ%2520differentiated%2520encoders%2520to%2520learn%2520their%250Adistributions.%2520In%2520particular%252C%2520we%2520further%2520utilize%2520the%2520pre-trained%2520model%2520and%2520item%250Apopularity%2520to%2520sample%2520pseudo-interaction%2520items%2520to%2520ensure%2520the%2520integrity%2520of%2520both%250Adistributions.%2520To%2520guarantee%2520the%2520personalization%2520of%2520preference%2520transfer%252C%2520we%250Atreat%2520each%2520user%2527s%2520mapping%2520as%2520two%2520parts%252C%2520the%2520common%2520transformation%2520and%2520the%250Apersonalized%2520bias%252C%2520where%2520the%2520network%2520used%2520to%2520generate%2520the%2520personalized%2520bias%2520is%250Aoutput%2520by%2520a%2520meta-learner.%2520Furthermore%252C%2520in%2520addition%2520to%2520the%2520supervised%2520loss%2520for%250Aoverlapping%2520users%252C%2520we%2520design%2520contrastive%2520tasks%2520for%2520non-overlapping%2520users%2520from%250Aboth%2520group%2520and%2520individual-levels%2520to%2520avoid%2520model%2520skew%2520and%2520enhance%2520the%2520semantics%250Aof%2520representations.%2520Exhaustive%2520data%2520analysis%2520and%2520extensive%2520experimental%2520results%250Ademonstrate%2520the%2520effectiveness%2520and%2520advancement%2520of%2520our%2520proposed%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-domain%20Transfer%20of%20Valence%20Preferences%20via%20a%20Meta-optimization%0A%20%20Approach&entry.906535625=Chuang%20Zhao%20and%20Hongke%20Zhao%20and%20Ming%20He%20and%20Xiaomeng%20Li%20and%20Jianping%20Fan&entry.1292438233=%20%20Cross-domain%20recommendation%20offers%20a%20potential%20avenue%20for%20alleviating%20data%0Asparsity%20and%20cold-start%20problems.%20Embedding%20and%20mapping%2C%20as%20a%20classic%0Across-domain%20research%20genre%2C%20aims%20to%20identify%20a%20common%20mapping%20function%20to%0Aperform%20representation%20transformation%20between%20two%20domains.%20Nevertheless%2C%0Aprevious%20coarse-grained%20preference%20representations%2C%20non-personalized%20mapping%0Afunctions%2C%20and%20excessive%20reliance%20on%20overlapping%20users%20limit%20their%20performance%2C%0Aespecially%20in%20scenarios%20where%20overlapping%20users%20are%20sparse.%20To%20address%0Aaforementioned%20challenges%2C%20we%20propose%20a%20novel%20cross-domain%20approach%2C%20namely%0ACVPM.%20CVPM%20formalizes%20cross-domain%20interest%20transfer%20as%20a%20hybrid%20architecture%0Aof%20parametric%20meta-learning%20and%20self-supervised%20learning%2C%20which%20not%20only%0Atransfers%20user%20preferences%20at%20a%20finer%20level%2C%20but%20also%20enables%20signal%0Aenhancement%20with%20the%20knowledge%20of%20non-overlapping%20users.%20Specifically%2C%20with%0Adeep%20insights%20into%20user%20preferences%20and%20valence%20preference%20theory%2C%20we%20believe%0Athat%20there%20exists%20significant%20difference%20between%20users%27%20positive%20preferences%0Aand%20negative%20behaviors%2C%20and%20thus%20employ%20differentiated%20encoders%20to%20learn%20their%0Adistributions.%20In%20particular%2C%20we%20further%20utilize%20the%20pre-trained%20model%20and%20item%0Apopularity%20to%20sample%20pseudo-interaction%20items%20to%20ensure%20the%20integrity%20of%20both%0Adistributions.%20To%20guarantee%20the%20personalization%20of%20preference%20transfer%2C%20we%0Atreat%20each%20user%27s%20mapping%20as%20two%20parts%2C%20the%20common%20transformation%20and%20the%0Apersonalized%20bias%2C%20where%20the%20network%20used%20to%20generate%20the%20personalized%20bias%20is%0Aoutput%20by%20a%20meta-learner.%20Furthermore%2C%20in%20addition%20to%20the%20supervised%20loss%20for%0Aoverlapping%20users%2C%20we%20design%20contrastive%20tasks%20for%20non-overlapping%20users%20from%0Aboth%20group%20and%20individual-levels%20to%20avoid%20model%20skew%20and%20enhance%20the%20semantics%0Aof%20representations.%20Exhaustive%20data%20analysis%20and%20extensive%20experimental%20results%0Ademonstrate%20the%20effectiveness%20and%20advancement%20of%20our%20proposed%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16494v1&entry.124074799=Read"},
{"title": "Cherry on the Cake: Fairness is NOT an Optimization Problem", "author": "Marco Favier and Toon Calders", "abstract": "  Fair cake-cutting is a mathematical subfield that studies the problem of\nfairly dividing a resource among a number of participants. The so-called\n``cake,'' as an object, represents any resource that can be distributed among\nplayers. This concept is connected to supervised multi-label classification:\nany dataset can be thought of as a cake that needs to be distributed, where\neach label is a player that receives its share of the dataset. In particular,\nany efficient cake-cutting solution for the dataset is equivalent to an optimal\ndecision function. Although we are not the first to demonstrate this\nconnection, the important ramifications of this parallel seem to have been\npartially forgotten. We revisit these classical results and demonstrate how\nthis connection can be prolifically used for fairness in machine learning\nproblems. Understanding the set of achievable fair decisions is a fundamental\nstep in finding optimal fair solutions and satisfying fairness requirements. By\nemploying the tools of cake-cutting theory, we have been able to describe the\nbehavior of optimal fair decisions, which, counterintuitively, often exhibit\nquite unfair properties. Specifically, in order to satisfy fairness\nconstraints, it is sometimes preferable, in the name of optimality, to\npurposefully make mistakes and deny giving the positive label to deserving\nindividuals in a community in favor of less worthy individuals within the same\ncommunity. This practice is known in the literature as cherry-picking and has\nbeen described as ``blatantly unfair.''\n", "link": "http://arxiv.org/abs/2406.16606v1", "date": "2024-06-24", "relevancy": 2.1204, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4301}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4231}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cherry%20on%20the%20Cake%3A%20Fairness%20is%20NOT%20an%20Optimization%20Problem&body=Title%3A%20Cherry%20on%20the%20Cake%3A%20Fairness%20is%20NOT%20an%20Optimization%20Problem%0AAuthor%3A%20Marco%20Favier%20and%20Toon%20Calders%0AAbstract%3A%20%20%20Fair%20cake-cutting%20is%20a%20mathematical%20subfield%20that%20studies%20the%20problem%20of%0Afairly%20dividing%20a%20resource%20among%20a%20number%20of%20participants.%20The%20so-called%0A%60%60cake%2C%27%27%20as%20an%20object%2C%20represents%20any%20resource%20that%20can%20be%20distributed%20among%0Aplayers.%20This%20concept%20is%20connected%20to%20supervised%20multi-label%20classification%3A%0Aany%20dataset%20can%20be%20thought%20of%20as%20a%20cake%20that%20needs%20to%20be%20distributed%2C%20where%0Aeach%20label%20is%20a%20player%20that%20receives%20its%20share%20of%20the%20dataset.%20In%20particular%2C%0Aany%20efficient%20cake-cutting%20solution%20for%20the%20dataset%20is%20equivalent%20to%20an%20optimal%0Adecision%20function.%20Although%20we%20are%20not%20the%20first%20to%20demonstrate%20this%0Aconnection%2C%20the%20important%20ramifications%20of%20this%20parallel%20seem%20to%20have%20been%0Apartially%20forgotten.%20We%20revisit%20these%20classical%20results%20and%20demonstrate%20how%0Athis%20connection%20can%20be%20prolifically%20used%20for%20fairness%20in%20machine%20learning%0Aproblems.%20Understanding%20the%20set%20of%20achievable%20fair%20decisions%20is%20a%20fundamental%0Astep%20in%20finding%20optimal%20fair%20solutions%20and%20satisfying%20fairness%20requirements.%20By%0Aemploying%20the%20tools%20of%20cake-cutting%20theory%2C%20we%20have%20been%20able%20to%20describe%20the%0Abehavior%20of%20optimal%20fair%20decisions%2C%20which%2C%20counterintuitively%2C%20often%20exhibit%0Aquite%20unfair%20properties.%20Specifically%2C%20in%20order%20to%20satisfy%20fairness%0Aconstraints%2C%20it%20is%20sometimes%20preferable%2C%20in%20the%20name%20of%20optimality%2C%20to%0Apurposefully%20make%20mistakes%20and%20deny%20giving%20the%20positive%20label%20to%20deserving%0Aindividuals%20in%20a%20community%20in%20favor%20of%20less%20worthy%20individuals%20within%20the%20same%0Acommunity.%20This%20practice%20is%20known%20in%20the%20literature%20as%20cherry-picking%20and%20has%0Abeen%20described%20as%20%60%60blatantly%20unfair.%27%27%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCherry%2520on%2520the%2520Cake%253A%2520Fairness%2520is%2520NOT%2520an%2520Optimization%2520Problem%26entry.906535625%3DMarco%2520Favier%2520and%2520Toon%2520Calders%26entry.1292438233%3D%2520%2520Fair%2520cake-cutting%2520is%2520a%2520mathematical%2520subfield%2520that%2520studies%2520the%2520problem%2520of%250Afairly%2520dividing%2520a%2520resource%2520among%2520a%2520number%2520of%2520participants.%2520The%2520so-called%250A%2560%2560cake%252C%2527%2527%2520as%2520an%2520object%252C%2520represents%2520any%2520resource%2520that%2520can%2520be%2520distributed%2520among%250Aplayers.%2520This%2520concept%2520is%2520connected%2520to%2520supervised%2520multi-label%2520classification%253A%250Aany%2520dataset%2520can%2520be%2520thought%2520of%2520as%2520a%2520cake%2520that%2520needs%2520to%2520be%2520distributed%252C%2520where%250Aeach%2520label%2520is%2520a%2520player%2520that%2520receives%2520its%2520share%2520of%2520the%2520dataset.%2520In%2520particular%252C%250Aany%2520efficient%2520cake-cutting%2520solution%2520for%2520the%2520dataset%2520is%2520equivalent%2520to%2520an%2520optimal%250Adecision%2520function.%2520Although%2520we%2520are%2520not%2520the%2520first%2520to%2520demonstrate%2520this%250Aconnection%252C%2520the%2520important%2520ramifications%2520of%2520this%2520parallel%2520seem%2520to%2520have%2520been%250Apartially%2520forgotten.%2520We%2520revisit%2520these%2520classical%2520results%2520and%2520demonstrate%2520how%250Athis%2520connection%2520can%2520be%2520prolifically%2520used%2520for%2520fairness%2520in%2520machine%2520learning%250Aproblems.%2520Understanding%2520the%2520set%2520of%2520achievable%2520fair%2520decisions%2520is%2520a%2520fundamental%250Astep%2520in%2520finding%2520optimal%2520fair%2520solutions%2520and%2520satisfying%2520fairness%2520requirements.%2520By%250Aemploying%2520the%2520tools%2520of%2520cake-cutting%2520theory%252C%2520we%2520have%2520been%2520able%2520to%2520describe%2520the%250Abehavior%2520of%2520optimal%2520fair%2520decisions%252C%2520which%252C%2520counterintuitively%252C%2520often%2520exhibit%250Aquite%2520unfair%2520properties.%2520Specifically%252C%2520in%2520order%2520to%2520satisfy%2520fairness%250Aconstraints%252C%2520it%2520is%2520sometimes%2520preferable%252C%2520in%2520the%2520name%2520of%2520optimality%252C%2520to%250Apurposefully%2520make%2520mistakes%2520and%2520deny%2520giving%2520the%2520positive%2520label%2520to%2520deserving%250Aindividuals%2520in%2520a%2520community%2520in%2520favor%2520of%2520less%2520worthy%2520individuals%2520within%2520the%2520same%250Acommunity.%2520This%2520practice%2520is%2520known%2520in%2520the%2520literature%2520as%2520cherry-picking%2520and%2520has%250Abeen%2520described%2520as%2520%2560%2560blatantly%2520unfair.%2527%2527%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cherry%20on%20the%20Cake%3A%20Fairness%20is%20NOT%20an%20Optimization%20Problem&entry.906535625=Marco%20Favier%20and%20Toon%20Calders&entry.1292438233=%20%20Fair%20cake-cutting%20is%20a%20mathematical%20subfield%20that%20studies%20the%20problem%20of%0Afairly%20dividing%20a%20resource%20among%20a%20number%20of%20participants.%20The%20so-called%0A%60%60cake%2C%27%27%20as%20an%20object%2C%20represents%20any%20resource%20that%20can%20be%20distributed%20among%0Aplayers.%20This%20concept%20is%20connected%20to%20supervised%20multi-label%20classification%3A%0Aany%20dataset%20can%20be%20thought%20of%20as%20a%20cake%20that%20needs%20to%20be%20distributed%2C%20where%0Aeach%20label%20is%20a%20player%20that%20receives%20its%20share%20of%20the%20dataset.%20In%20particular%2C%0Aany%20efficient%20cake-cutting%20solution%20for%20the%20dataset%20is%20equivalent%20to%20an%20optimal%0Adecision%20function.%20Although%20we%20are%20not%20the%20first%20to%20demonstrate%20this%0Aconnection%2C%20the%20important%20ramifications%20of%20this%20parallel%20seem%20to%20have%20been%0Apartially%20forgotten.%20We%20revisit%20these%20classical%20results%20and%20demonstrate%20how%0Athis%20connection%20can%20be%20prolifically%20used%20for%20fairness%20in%20machine%20learning%0Aproblems.%20Understanding%20the%20set%20of%20achievable%20fair%20decisions%20is%20a%20fundamental%0Astep%20in%20finding%20optimal%20fair%20solutions%20and%20satisfying%20fairness%20requirements.%20By%0Aemploying%20the%20tools%20of%20cake-cutting%20theory%2C%20we%20have%20been%20able%20to%20describe%20the%0Abehavior%20of%20optimal%20fair%20decisions%2C%20which%2C%20counterintuitively%2C%20often%20exhibit%0Aquite%20unfair%20properties.%20Specifically%2C%20in%20order%20to%20satisfy%20fairness%0Aconstraints%2C%20it%20is%20sometimes%20preferable%2C%20in%20the%20name%20of%20optimality%2C%20to%0Apurposefully%20make%20mistakes%20and%20deny%20giving%20the%20positive%20label%20to%20deserving%0Aindividuals%20in%20a%20community%20in%20favor%20of%20less%20worthy%20individuals%20within%20the%20same%0Acommunity.%20This%20practice%20is%20known%20in%20the%20literature%20as%20cherry-picking%20and%20has%0Abeen%20described%20as%20%60%60blatantly%20unfair.%27%27%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16606v1&entry.124074799=Read"},
{"title": "OTCE: Hybrid SSM and Attention with Cross Domain Mixture of Experts to\n  construct Observer-Thinker-Conceiver-Expresser", "author": "Jingze Shi and Ting Xie and Bingheng Wu and Chunjun Zheng and Kai Wang", "abstract": "  Recent research has shown that combining Mamba with Transformer architecture,\nwhich has selective state space and quadratic self-attention mechanism,\noutperforms using Mamba or Transformer architecture alone in language modeling\ntasks. The quadratic self-attention mechanism effectively alleviates the\nshortcomings of selective state space in handling long-term dependencies of any\nelement in the sequence. We propose a position information injection method\nthat connects the selective state space model with the quadratic attention, and\nintegrates these two architectures with hybrid experts with cross-sharing\ndomains, so that we can enjoy the advantages of both. We design a new\narchitecture with a more biomimetic idea: Observer-Thinker-Conceiver-Expresser\n(OTCE), which can compete with well-known medium-scale open-source language\nmodels on a small scale in language modeling tasks.\n", "link": "http://arxiv.org/abs/2406.16495v1", "date": "2024-06-24", "relevancy": 2.1124, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5527}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5146}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OTCE%3A%20Hybrid%20SSM%20and%20Attention%20with%20Cross%20Domain%20Mixture%20of%20Experts%20to%0A%20%20construct%20Observer-Thinker-Conceiver-Expresser&body=Title%3A%20OTCE%3A%20Hybrid%20SSM%20and%20Attention%20with%20Cross%20Domain%20Mixture%20of%20Experts%20to%0A%20%20construct%20Observer-Thinker-Conceiver-Expresser%0AAuthor%3A%20Jingze%20Shi%20and%20Ting%20Xie%20and%20Bingheng%20Wu%20and%20Chunjun%20Zheng%20and%20Kai%20Wang%0AAbstract%3A%20%20%20Recent%20research%20has%20shown%20that%20combining%20Mamba%20with%20Transformer%20architecture%2C%0Awhich%20has%20selective%20state%20space%20and%20quadratic%20self-attention%20mechanism%2C%0Aoutperforms%20using%20Mamba%20or%20Transformer%20architecture%20alone%20in%20language%20modeling%0Atasks.%20The%20quadratic%20self-attention%20mechanism%20effectively%20alleviates%20the%0Ashortcomings%20of%20selective%20state%20space%20in%20handling%20long-term%20dependencies%20of%20any%0Aelement%20in%20the%20sequence.%20We%20propose%20a%20position%20information%20injection%20method%0Athat%20connects%20the%20selective%20state%20space%20model%20with%20the%20quadratic%20attention%2C%20and%0Aintegrates%20these%20two%20architectures%20with%20hybrid%20experts%20with%20cross-sharing%0Adomains%2C%20so%20that%20we%20can%20enjoy%20the%20advantages%20of%20both.%20We%20design%20a%20new%0Aarchitecture%20with%20a%20more%20biomimetic%20idea%3A%20Observer-Thinker-Conceiver-Expresser%0A%28OTCE%29%2C%20which%20can%20compete%20with%20well-known%20medium-scale%20open-source%20language%0Amodels%20on%20a%20small%20scale%20in%20language%20modeling%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16495v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOTCE%253A%2520Hybrid%2520SSM%2520and%2520Attention%2520with%2520Cross%2520Domain%2520Mixture%2520of%2520Experts%2520to%250A%2520%2520construct%2520Observer-Thinker-Conceiver-Expresser%26entry.906535625%3DJingze%2520Shi%2520and%2520Ting%2520Xie%2520and%2520Bingheng%2520Wu%2520and%2520Chunjun%2520Zheng%2520and%2520Kai%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520research%2520has%2520shown%2520that%2520combining%2520Mamba%2520with%2520Transformer%2520architecture%252C%250Awhich%2520has%2520selective%2520state%2520space%2520and%2520quadratic%2520self-attention%2520mechanism%252C%250Aoutperforms%2520using%2520Mamba%2520or%2520Transformer%2520architecture%2520alone%2520in%2520language%2520modeling%250Atasks.%2520The%2520quadratic%2520self-attention%2520mechanism%2520effectively%2520alleviates%2520the%250Ashortcomings%2520of%2520selective%2520state%2520space%2520in%2520handling%2520long-term%2520dependencies%2520of%2520any%250Aelement%2520in%2520the%2520sequence.%2520We%2520propose%2520a%2520position%2520information%2520injection%2520method%250Athat%2520connects%2520the%2520selective%2520state%2520space%2520model%2520with%2520the%2520quadratic%2520attention%252C%2520and%250Aintegrates%2520these%2520two%2520architectures%2520with%2520hybrid%2520experts%2520with%2520cross-sharing%250Adomains%252C%2520so%2520that%2520we%2520can%2520enjoy%2520the%2520advantages%2520of%2520both.%2520We%2520design%2520a%2520new%250Aarchitecture%2520with%2520a%2520more%2520biomimetic%2520idea%253A%2520Observer-Thinker-Conceiver-Expresser%250A%2528OTCE%2529%252C%2520which%2520can%2520compete%2520with%2520well-known%2520medium-scale%2520open-source%2520language%250Amodels%2520on%2520a%2520small%2520scale%2520in%2520language%2520modeling%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16495v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OTCE%3A%20Hybrid%20SSM%20and%20Attention%20with%20Cross%20Domain%20Mixture%20of%20Experts%20to%0A%20%20construct%20Observer-Thinker-Conceiver-Expresser&entry.906535625=Jingze%20Shi%20and%20Ting%20Xie%20and%20Bingheng%20Wu%20and%20Chunjun%20Zheng%20and%20Kai%20Wang&entry.1292438233=%20%20Recent%20research%20has%20shown%20that%20combining%20Mamba%20with%20Transformer%20architecture%2C%0Awhich%20has%20selective%20state%20space%20and%20quadratic%20self-attention%20mechanism%2C%0Aoutperforms%20using%20Mamba%20or%20Transformer%20architecture%20alone%20in%20language%20modeling%0Atasks.%20The%20quadratic%20self-attention%20mechanism%20effectively%20alleviates%20the%0Ashortcomings%20of%20selective%20state%20space%20in%20handling%20long-term%20dependencies%20of%20any%0Aelement%20in%20the%20sequence.%20We%20propose%20a%20position%20information%20injection%20method%0Athat%20connects%20the%20selective%20state%20space%20model%20with%20the%20quadratic%20attention%2C%20and%0Aintegrates%20these%20two%20architectures%20with%20hybrid%20experts%20with%20cross-sharing%0Adomains%2C%20so%20that%20we%20can%20enjoy%20the%20advantages%20of%20both.%20We%20design%20a%20new%0Aarchitecture%20with%20a%20more%20biomimetic%20idea%3A%20Observer-Thinker-Conceiver-Expresser%0A%28OTCE%29%2C%20which%20can%20compete%20with%20well-known%20medium-scale%20open-source%20language%0Amodels%20on%20a%20small%20scale%20in%20language%20modeling%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16495v1&entry.124074799=Read"},
{"title": "Cubic regularized subspace Newton for non-convex optimization", "author": "Jim Zhao and Aurelien Lucchi and Nikita Doikov", "abstract": "  This paper addresses the optimization problem of minimizing non-convex\ncontinuous functions, which is relevant in the context of high-dimensional\nmachine learning applications characterized by over-parametrization. We analyze\na randomized coordinate second-order method named SSCN which can be interpreted\nas applying cubic regularization in random subspaces. This approach effectively\nreduces the computational complexity associated with utilizing second-order\ninformation, rendering it applicable in higher-dimensional scenarios.\nTheoretically, we establish convergence guarantees for non-convex functions,\nwith interpolating rates for arbitrary subspace sizes and allowing inexact\ncurvature estimation. When increasing subspace size, our complexity matches\n$\\mathcal{O}(\\epsilon^{-3/2})$ of the cubic regularization (CR) rate.\nAdditionally, we propose an adaptive sampling scheme ensuring exact convergence\nrate of $\\mathcal{O}(\\epsilon^{-3/2}, \\epsilon^{-3})$ to a second-order\nstationary point, even without sampling all coordinates. Experimental results\ndemonstrate substantial speed-ups achieved by SSCN compared to conventional\nfirst-order methods.\n", "link": "http://arxiv.org/abs/2406.16666v1", "date": "2024-06-24", "relevancy": 2.1076, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4352}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4159}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cubic%20regularized%20subspace%20Newton%20for%20non-convex%20optimization&body=Title%3A%20Cubic%20regularized%20subspace%20Newton%20for%20non-convex%20optimization%0AAuthor%3A%20Jim%20Zhao%20and%20Aurelien%20Lucchi%20and%20Nikita%20Doikov%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20optimization%20problem%20of%20minimizing%20non-convex%0Acontinuous%20functions%2C%20which%20is%20relevant%20in%20the%20context%20of%20high-dimensional%0Amachine%20learning%20applications%20characterized%20by%20over-parametrization.%20We%20analyze%0Aa%20randomized%20coordinate%20second-order%20method%20named%20SSCN%20which%20can%20be%20interpreted%0Aas%20applying%20cubic%20regularization%20in%20random%20subspaces.%20This%20approach%20effectively%0Areduces%20the%20computational%20complexity%20associated%20with%20utilizing%20second-order%0Ainformation%2C%20rendering%20it%20applicable%20in%20higher-dimensional%20scenarios.%0ATheoretically%2C%20we%20establish%20convergence%20guarantees%20for%20non-convex%20functions%2C%0Awith%20interpolating%20rates%20for%20arbitrary%20subspace%20sizes%20and%20allowing%20inexact%0Acurvature%20estimation.%20When%20increasing%20subspace%20size%2C%20our%20complexity%20matches%0A%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-3/2%7D%29%24%20of%20the%20cubic%20regularization%20%28CR%29%20rate.%0AAdditionally%2C%20we%20propose%20an%20adaptive%20sampling%20scheme%20ensuring%20exact%20convergence%0Arate%20of%20%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-3/2%7D%2C%20%5Cepsilon%5E%7B-3%7D%29%24%20to%20a%20second-order%0Astationary%20point%2C%20even%20without%20sampling%20all%20coordinates.%20Experimental%20results%0Ademonstrate%20substantial%20speed-ups%20achieved%20by%20SSCN%20compared%20to%20conventional%0Afirst-order%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCubic%2520regularized%2520subspace%2520Newton%2520for%2520non-convex%2520optimization%26entry.906535625%3DJim%2520Zhao%2520and%2520Aurelien%2520Lucchi%2520and%2520Nikita%2520Doikov%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520optimization%2520problem%2520of%2520minimizing%2520non-convex%250Acontinuous%2520functions%252C%2520which%2520is%2520relevant%2520in%2520the%2520context%2520of%2520high-dimensional%250Amachine%2520learning%2520applications%2520characterized%2520by%2520over-parametrization.%2520We%2520analyze%250Aa%2520randomized%2520coordinate%2520second-order%2520method%2520named%2520SSCN%2520which%2520can%2520be%2520interpreted%250Aas%2520applying%2520cubic%2520regularization%2520in%2520random%2520subspaces.%2520This%2520approach%2520effectively%250Areduces%2520the%2520computational%2520complexity%2520associated%2520with%2520utilizing%2520second-order%250Ainformation%252C%2520rendering%2520it%2520applicable%2520in%2520higher-dimensional%2520scenarios.%250ATheoretically%252C%2520we%2520establish%2520convergence%2520guarantees%2520for%2520non-convex%2520functions%252C%250Awith%2520interpolating%2520rates%2520for%2520arbitrary%2520subspace%2520sizes%2520and%2520allowing%2520inexact%250Acurvature%2520estimation.%2520When%2520increasing%2520subspace%2520size%252C%2520our%2520complexity%2520matches%250A%2524%255Cmathcal%257BO%257D%2528%255Cepsilon%255E%257B-3/2%257D%2529%2524%2520of%2520the%2520cubic%2520regularization%2520%2528CR%2529%2520rate.%250AAdditionally%252C%2520we%2520propose%2520an%2520adaptive%2520sampling%2520scheme%2520ensuring%2520exact%2520convergence%250Arate%2520of%2520%2524%255Cmathcal%257BO%257D%2528%255Cepsilon%255E%257B-3/2%257D%252C%2520%255Cepsilon%255E%257B-3%257D%2529%2524%2520to%2520a%2520second-order%250Astationary%2520point%252C%2520even%2520without%2520sampling%2520all%2520coordinates.%2520Experimental%2520results%250Ademonstrate%2520substantial%2520speed-ups%2520achieved%2520by%2520SSCN%2520compared%2520to%2520conventional%250Afirst-order%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cubic%20regularized%20subspace%20Newton%20for%20non-convex%20optimization&entry.906535625=Jim%20Zhao%20and%20Aurelien%20Lucchi%20and%20Nikita%20Doikov&entry.1292438233=%20%20This%20paper%20addresses%20the%20optimization%20problem%20of%20minimizing%20non-convex%0Acontinuous%20functions%2C%20which%20is%20relevant%20in%20the%20context%20of%20high-dimensional%0Amachine%20learning%20applications%20characterized%20by%20over-parametrization.%20We%20analyze%0Aa%20randomized%20coordinate%20second-order%20method%20named%20SSCN%20which%20can%20be%20interpreted%0Aas%20applying%20cubic%20regularization%20in%20random%20subspaces.%20This%20approach%20effectively%0Areduces%20the%20computational%20complexity%20associated%20with%20utilizing%20second-order%0Ainformation%2C%20rendering%20it%20applicable%20in%20higher-dimensional%20scenarios.%0ATheoretically%2C%20we%20establish%20convergence%20guarantees%20for%20non-convex%20functions%2C%0Awith%20interpolating%20rates%20for%20arbitrary%20subspace%20sizes%20and%20allowing%20inexact%0Acurvature%20estimation.%20When%20increasing%20subspace%20size%2C%20our%20complexity%20matches%0A%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-3/2%7D%29%24%20of%20the%20cubic%20regularization%20%28CR%29%20rate.%0AAdditionally%2C%20we%20propose%20an%20adaptive%20sampling%20scheme%20ensuring%20exact%20convergence%0Arate%20of%20%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-3/2%7D%2C%20%5Cepsilon%5E%7B-3%7D%29%24%20to%20a%20second-order%0Astationary%20point%2C%20even%20without%20sampling%20all%20coordinates.%20Experimental%20results%0Ademonstrate%20substantial%20speed-ups%20achieved%20by%20SSCN%20compared%20to%20conventional%0Afirst-order%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16666v1&entry.124074799=Read"},
{"title": "LatentExplainer: Explaining Latent Representations in Deep Generative\n  Models with Multi-modal Foundation Models", "author": "Mengdan Zhu and Raasikh Kanjiani and Jiahui Lu and Andrew Choi and Qirui Ye and Liang Zhao", "abstract": "  Deep generative models like VAEs and diffusion models have advanced various\ngeneration tasks by leveraging latent variables to learn data distributions and\ngenerate high-quality samples. Despite the field of explainable AI making\nstrides in interpreting machine learning models, understanding latent variables\nin generative models remains challenging. This paper introduces\nLatentExplainer, a framework for automatically generating semantically\nmeaningful explanations of latent variables in deep generative models.\nLatentExplainer tackles three main challenges: inferring the meaning of latent\nvariables, aligning explanations with inductive biases, and handling varying\ndegrees of explainability. By perturbing latent variables and interpreting\nchanges in generated data, the framework provides a systematic approach to\nunderstanding and controlling the data generation process, enhancing the\ntransparency and interpretability of deep generative models. We evaluate our\nproposed method on several real-world and synthetic datasets, and the results\ndemonstrate superior performance in generating high-quality explanations of\nlatent variables.\n", "link": "http://arxiv.org/abs/2406.14862v2", "date": "2024-06-24", "relevancy": 2.1039, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5326}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5214}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LatentExplainer%3A%20Explaining%20Latent%20Representations%20in%20Deep%20Generative%0A%20%20Models%20with%20Multi-modal%20Foundation%20Models&body=Title%3A%20LatentExplainer%3A%20Explaining%20Latent%20Representations%20in%20Deep%20Generative%0A%20%20Models%20with%20Multi-modal%20Foundation%20Models%0AAuthor%3A%20Mengdan%20Zhu%20and%20Raasikh%20Kanjiani%20and%20Jiahui%20Lu%20and%20Andrew%20Choi%20and%20Qirui%20Ye%20and%20Liang%20Zhao%0AAbstract%3A%20%20%20Deep%20generative%20models%20like%20VAEs%20and%20diffusion%20models%20have%20advanced%20various%0Ageneration%20tasks%20by%20leveraging%20latent%20variables%20to%20learn%20data%20distributions%20and%0Agenerate%20high-quality%20samples.%20Despite%20the%20field%20of%20explainable%20AI%20making%0Astrides%20in%20interpreting%20machine%20learning%20models%2C%20understanding%20latent%20variables%0Ain%20generative%20models%20remains%20challenging.%20This%20paper%20introduces%0ALatentExplainer%2C%20a%20framework%20for%20automatically%20generating%20semantically%0Ameaningful%20explanations%20of%20latent%20variables%20in%20deep%20generative%20models.%0ALatentExplainer%20tackles%20three%20main%20challenges%3A%20inferring%20the%20meaning%20of%20latent%0Avariables%2C%20aligning%20explanations%20with%20inductive%20biases%2C%20and%20handling%20varying%0Adegrees%20of%20explainability.%20By%20perturbing%20latent%20variables%20and%20interpreting%0Achanges%20in%20generated%20data%2C%20the%20framework%20provides%20a%20systematic%20approach%20to%0Aunderstanding%20and%20controlling%20the%20data%20generation%20process%2C%20enhancing%20the%0Atransparency%20and%20interpretability%20of%20deep%20generative%20models.%20We%20evaluate%20our%0Aproposed%20method%20on%20several%20real-world%20and%20synthetic%20datasets%2C%20and%20the%20results%0Ademonstrate%20superior%20performance%20in%20generating%20high-quality%20explanations%20of%0Alatent%20variables.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14862v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatentExplainer%253A%2520Explaining%2520Latent%2520Representations%2520in%2520Deep%2520Generative%250A%2520%2520Models%2520with%2520Multi-modal%2520Foundation%2520Models%26entry.906535625%3DMengdan%2520Zhu%2520and%2520Raasikh%2520Kanjiani%2520and%2520Jiahui%2520Lu%2520and%2520Andrew%2520Choi%2520and%2520Qirui%2520Ye%2520and%2520Liang%2520Zhao%26entry.1292438233%3D%2520%2520Deep%2520generative%2520models%2520like%2520VAEs%2520and%2520diffusion%2520models%2520have%2520advanced%2520various%250Ageneration%2520tasks%2520by%2520leveraging%2520latent%2520variables%2520to%2520learn%2520data%2520distributions%2520and%250Agenerate%2520high-quality%2520samples.%2520Despite%2520the%2520field%2520of%2520explainable%2520AI%2520making%250Astrides%2520in%2520interpreting%2520machine%2520learning%2520models%252C%2520understanding%2520latent%2520variables%250Ain%2520generative%2520models%2520remains%2520challenging.%2520This%2520paper%2520introduces%250ALatentExplainer%252C%2520a%2520framework%2520for%2520automatically%2520generating%2520semantically%250Ameaningful%2520explanations%2520of%2520latent%2520variables%2520in%2520deep%2520generative%2520models.%250ALatentExplainer%2520tackles%2520three%2520main%2520challenges%253A%2520inferring%2520the%2520meaning%2520of%2520latent%250Avariables%252C%2520aligning%2520explanations%2520with%2520inductive%2520biases%252C%2520and%2520handling%2520varying%250Adegrees%2520of%2520explainability.%2520By%2520perturbing%2520latent%2520variables%2520and%2520interpreting%250Achanges%2520in%2520generated%2520data%252C%2520the%2520framework%2520provides%2520a%2520systematic%2520approach%2520to%250Aunderstanding%2520and%2520controlling%2520the%2520data%2520generation%2520process%252C%2520enhancing%2520the%250Atransparency%2520and%2520interpretability%2520of%2520deep%2520generative%2520models.%2520We%2520evaluate%2520our%250Aproposed%2520method%2520on%2520several%2520real-world%2520and%2520synthetic%2520datasets%252C%2520and%2520the%2520results%250Ademonstrate%2520superior%2520performance%2520in%2520generating%2520high-quality%2520explanations%2520of%250Alatent%2520variables.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14862v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LatentExplainer%3A%20Explaining%20Latent%20Representations%20in%20Deep%20Generative%0A%20%20Models%20with%20Multi-modal%20Foundation%20Models&entry.906535625=Mengdan%20Zhu%20and%20Raasikh%20Kanjiani%20and%20Jiahui%20Lu%20and%20Andrew%20Choi%20and%20Qirui%20Ye%20and%20Liang%20Zhao&entry.1292438233=%20%20Deep%20generative%20models%20like%20VAEs%20and%20diffusion%20models%20have%20advanced%20various%0Ageneration%20tasks%20by%20leveraging%20latent%20variables%20to%20learn%20data%20distributions%20and%0Agenerate%20high-quality%20samples.%20Despite%20the%20field%20of%20explainable%20AI%20making%0Astrides%20in%20interpreting%20machine%20learning%20models%2C%20understanding%20latent%20variables%0Ain%20generative%20models%20remains%20challenging.%20This%20paper%20introduces%0ALatentExplainer%2C%20a%20framework%20for%20automatically%20generating%20semantically%0Ameaningful%20explanations%20of%20latent%20variables%20in%20deep%20generative%20models.%0ALatentExplainer%20tackles%20three%20main%20challenges%3A%20inferring%20the%20meaning%20of%20latent%0Avariables%2C%20aligning%20explanations%20with%20inductive%20biases%2C%20and%20handling%20varying%0Adegrees%20of%20explainability.%20By%20perturbing%20latent%20variables%20and%20interpreting%0Achanges%20in%20generated%20data%2C%20the%20framework%20provides%20a%20systematic%20approach%20to%0Aunderstanding%20and%20controlling%20the%20data%20generation%20process%2C%20enhancing%20the%0Atransparency%20and%20interpretability%20of%20deep%20generative%20models.%20We%20evaluate%20our%0Aproposed%20method%20on%20several%20real-world%20and%20synthetic%20datasets%2C%20and%20the%20results%0Ademonstrate%20superior%20performance%20in%20generating%20high-quality%20explanations%20of%0Alatent%20variables.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14862v2&entry.124074799=Read"},
{"title": "AutoDetect: Towards a Unified Framework for Automated Weakness Detection\n  in Large Language Models", "author": "Jiale Cheng and Yida Lu and Xiaotao Gu and Pei Ke and Xiao Liu and Yuxiao Dong and Hongning Wang and Jie Tang and Minlie Huang", "abstract": "  Although Large Language Models (LLMs) are becoming increasingly powerful,\nthey still exhibit significant but subtle weaknesses, such as mistakes in\ninstruction-following or coding tasks. As these unexpected errors could lead to\nsevere consequences in practical deployments, it is crucial to investigate the\nlimitations within LLMs systematically. Traditional benchmarking approaches\ncannot thoroughly pinpoint specific model deficiencies, while manual\ninspections are costly and not scalable. In this paper, we introduce a unified\nframework, AutoDetect, to automatically expose weaknesses in LLMs across\nvarious tasks. Inspired by the educational assessment process that measures\nstudents' learning outcomes, AutoDetect consists of three LLM-powered agents:\nExaminer, Questioner, and Assessor. The collaboration among these three agents\nis designed to realize comprehensive and in-depth weakness identification. Our\nframework demonstrates significant success in uncovering flaws, with an\nidentification success rate exceeding 30% in prominent models such as ChatGPT\nand Claude. More importantly, these identified weaknesses can guide specific\nmodel improvements, proving more effective than untargeted data augmentation\nmethods like Self-Instruct. Our approach has led to substantial enhancements in\npopular LLMs, including the Llama series and Mistral-7b, boosting their\nperformance by over 10% across several benchmarks. Code and data are publicly\navailable at https://github.com/thu-coai/AutoDetect.\n", "link": "http://arxiv.org/abs/2406.16714v1", "date": "2024-06-24", "relevancy": 2.1017, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5615}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5311}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoDetect%3A%20Towards%20a%20Unified%20Framework%20for%20Automated%20Weakness%20Detection%0A%20%20in%20Large%20Language%20Models&body=Title%3A%20AutoDetect%3A%20Towards%20a%20Unified%20Framework%20for%20Automated%20Weakness%20Detection%0A%20%20in%20Large%20Language%20Models%0AAuthor%3A%20Jiale%20Cheng%20and%20Yida%20Lu%20and%20Xiaotao%20Gu%20and%20Pei%20Ke%20and%20Xiao%20Liu%20and%20Yuxiao%20Dong%20and%20Hongning%20Wang%20and%20Jie%20Tang%20and%20Minlie%20Huang%0AAbstract%3A%20%20%20Although%20Large%20Language%20Models%20%28LLMs%29%20are%20becoming%20increasingly%20powerful%2C%0Athey%20still%20exhibit%20significant%20but%20subtle%20weaknesses%2C%20such%20as%20mistakes%20in%0Ainstruction-following%20or%20coding%20tasks.%20As%20these%20unexpected%20errors%20could%20lead%20to%0Asevere%20consequences%20in%20practical%20deployments%2C%20it%20is%20crucial%20to%20investigate%20the%0Alimitations%20within%20LLMs%20systematically.%20Traditional%20benchmarking%20approaches%0Acannot%20thoroughly%20pinpoint%20specific%20model%20deficiencies%2C%20while%20manual%0Ainspections%20are%20costly%20and%20not%20scalable.%20In%20this%20paper%2C%20we%20introduce%20a%20unified%0Aframework%2C%20AutoDetect%2C%20to%20automatically%20expose%20weaknesses%20in%20LLMs%20across%0Avarious%20tasks.%20Inspired%20by%20the%20educational%20assessment%20process%20that%20measures%0Astudents%27%20learning%20outcomes%2C%20AutoDetect%20consists%20of%20three%20LLM-powered%20agents%3A%0AExaminer%2C%20Questioner%2C%20and%20Assessor.%20The%20collaboration%20among%20these%20three%20agents%0Ais%20designed%20to%20realize%20comprehensive%20and%20in-depth%20weakness%20identification.%20Our%0Aframework%20demonstrates%20significant%20success%20in%20uncovering%20flaws%2C%20with%20an%0Aidentification%20success%20rate%20exceeding%2030%25%20in%20prominent%20models%20such%20as%20ChatGPT%0Aand%20Claude.%20More%20importantly%2C%20these%20identified%20weaknesses%20can%20guide%20specific%0Amodel%20improvements%2C%20proving%20more%20effective%20than%20untargeted%20data%20augmentation%0Amethods%20like%20Self-Instruct.%20Our%20approach%20has%20led%20to%20substantial%20enhancements%20in%0Apopular%20LLMs%2C%20including%20the%20Llama%20series%20and%20Mistral-7b%2C%20boosting%20their%0Aperformance%20by%20over%2010%25%20across%20several%20benchmarks.%20Code%20and%20data%20are%20publicly%0Aavailable%20at%20https%3A//github.com/thu-coai/AutoDetect.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoDetect%253A%2520Towards%2520a%2520Unified%2520Framework%2520for%2520Automated%2520Weakness%2520Detection%250A%2520%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DJiale%2520Cheng%2520and%2520Yida%2520Lu%2520and%2520Xiaotao%2520Gu%2520and%2520Pei%2520Ke%2520and%2520Xiao%2520Liu%2520and%2520Yuxiao%2520Dong%2520and%2520Hongning%2520Wang%2520and%2520Jie%2520Tang%2520and%2520Minlie%2520Huang%26entry.1292438233%3D%2520%2520Although%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520becoming%2520increasingly%2520powerful%252C%250Athey%2520still%2520exhibit%2520significant%2520but%2520subtle%2520weaknesses%252C%2520such%2520as%2520mistakes%2520in%250Ainstruction-following%2520or%2520coding%2520tasks.%2520As%2520these%2520unexpected%2520errors%2520could%2520lead%2520to%250Asevere%2520consequences%2520in%2520practical%2520deployments%252C%2520it%2520is%2520crucial%2520to%2520investigate%2520the%250Alimitations%2520within%2520LLMs%2520systematically.%2520Traditional%2520benchmarking%2520approaches%250Acannot%2520thoroughly%2520pinpoint%2520specific%2520model%2520deficiencies%252C%2520while%2520manual%250Ainspections%2520are%2520costly%2520and%2520not%2520scalable.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520unified%250Aframework%252C%2520AutoDetect%252C%2520to%2520automatically%2520expose%2520weaknesses%2520in%2520LLMs%2520across%250Avarious%2520tasks.%2520Inspired%2520by%2520the%2520educational%2520assessment%2520process%2520that%2520measures%250Astudents%2527%2520learning%2520outcomes%252C%2520AutoDetect%2520consists%2520of%2520three%2520LLM-powered%2520agents%253A%250AExaminer%252C%2520Questioner%252C%2520and%2520Assessor.%2520The%2520collaboration%2520among%2520these%2520three%2520agents%250Ais%2520designed%2520to%2520realize%2520comprehensive%2520and%2520in-depth%2520weakness%2520identification.%2520Our%250Aframework%2520demonstrates%2520significant%2520success%2520in%2520uncovering%2520flaws%252C%2520with%2520an%250Aidentification%2520success%2520rate%2520exceeding%252030%2525%2520in%2520prominent%2520models%2520such%2520as%2520ChatGPT%250Aand%2520Claude.%2520More%2520importantly%252C%2520these%2520identified%2520weaknesses%2520can%2520guide%2520specific%250Amodel%2520improvements%252C%2520proving%2520more%2520effective%2520than%2520untargeted%2520data%2520augmentation%250Amethods%2520like%2520Self-Instruct.%2520Our%2520approach%2520has%2520led%2520to%2520substantial%2520enhancements%2520in%250Apopular%2520LLMs%252C%2520including%2520the%2520Llama%2520series%2520and%2520Mistral-7b%252C%2520boosting%2520their%250Aperformance%2520by%2520over%252010%2525%2520across%2520several%2520benchmarks.%2520Code%2520and%2520data%2520are%2520publicly%250Aavailable%2520at%2520https%253A//github.com/thu-coai/AutoDetect.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoDetect%3A%20Towards%20a%20Unified%20Framework%20for%20Automated%20Weakness%20Detection%0A%20%20in%20Large%20Language%20Models&entry.906535625=Jiale%20Cheng%20and%20Yida%20Lu%20and%20Xiaotao%20Gu%20and%20Pei%20Ke%20and%20Xiao%20Liu%20and%20Yuxiao%20Dong%20and%20Hongning%20Wang%20and%20Jie%20Tang%20and%20Minlie%20Huang&entry.1292438233=%20%20Although%20Large%20Language%20Models%20%28LLMs%29%20are%20becoming%20increasingly%20powerful%2C%0Athey%20still%20exhibit%20significant%20but%20subtle%20weaknesses%2C%20such%20as%20mistakes%20in%0Ainstruction-following%20or%20coding%20tasks.%20As%20these%20unexpected%20errors%20could%20lead%20to%0Asevere%20consequences%20in%20practical%20deployments%2C%20it%20is%20crucial%20to%20investigate%20the%0Alimitations%20within%20LLMs%20systematically.%20Traditional%20benchmarking%20approaches%0Acannot%20thoroughly%20pinpoint%20specific%20model%20deficiencies%2C%20while%20manual%0Ainspections%20are%20costly%20and%20not%20scalable.%20In%20this%20paper%2C%20we%20introduce%20a%20unified%0Aframework%2C%20AutoDetect%2C%20to%20automatically%20expose%20weaknesses%20in%20LLMs%20across%0Avarious%20tasks.%20Inspired%20by%20the%20educational%20assessment%20process%20that%20measures%0Astudents%27%20learning%20outcomes%2C%20AutoDetect%20consists%20of%20three%20LLM-powered%20agents%3A%0AExaminer%2C%20Questioner%2C%20and%20Assessor.%20The%20collaboration%20among%20these%20three%20agents%0Ais%20designed%20to%20realize%20comprehensive%20and%20in-depth%20weakness%20identification.%20Our%0Aframework%20demonstrates%20significant%20success%20in%20uncovering%20flaws%2C%20with%20an%0Aidentification%20success%20rate%20exceeding%2030%25%20in%20prominent%20models%20such%20as%20ChatGPT%0Aand%20Claude.%20More%20importantly%2C%20these%20identified%20weaknesses%20can%20guide%20specific%0Amodel%20improvements%2C%20proving%20more%20effective%20than%20untargeted%20data%20augmentation%0Amethods%20like%20Self-Instruct.%20Our%20approach%20has%20led%20to%20substantial%20enhancements%20in%0Apopular%20LLMs%2C%20including%20the%20Llama%20series%20and%20Mistral-7b%2C%20boosting%20their%0Aperformance%20by%20over%2010%25%20across%20several%20benchmarks.%20Code%20and%20data%20are%20publicly%0Aavailable%20at%20https%3A//github.com/thu-coai/AutoDetect.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16714v1&entry.124074799=Read"},
{"title": "QUBIQ: Uncertainty Quantification for Biomedical Image Segmentation\n  Challenge", "author": "Hongwei Bran Li and Fernando Navarro and Ivan Ezhov and Amirhossein Bayat and Dhritiman Das and Florian Kofler and Suprosanna Shit and Diana Waldmannstetter and Johannes C. Paetzold and Xiaobin Hu and Benedikt Wiestler and Lucas Zimmer and Tamaz Amiranashvili and Chinmay Prabhakar and Christoph Berger and Jonas Weidner and Michelle Alonso-Basant and Arif Rashid and Ujjwal Baid and Wesam Adel and Deniz Ali and Bhakti Baheti and Yingbin Bai and Ishaan Bhatt and Sabri Can Cetindag and Wenting Chen and Li Cheng and Prasad Dutand and Lara Dular and Mustafa A. Elattar and Ming Feng and Shengbo Gao and Henkjan Huisman and Weifeng Hu and Shubham Innani and Wei Jiat and Davood Karimi and Hugo J. Kuijf and Jin Tae Kwak and Hoang Long Le and Xiang Lia and Huiyan Lin and Tongliang Liu and Jun Ma and Kai Ma and Ting Ma and Ilkay Oksuz and Robbie Holland and Arlindo L. Oliveira and Jimut Bahan Pal and Xuan Pei and Maoying Qiao and Anindo Saha and Raghavendra Selvan and Linlin Shen and Joao Lourenco Silva and Ziga Spiclin and Sanjay Talbar and Dadong Wang and Wei Wang and Xiong Wang and Yin Wang and Ruiling Xia and Kele Xu and Yanwu Yan and Mert Yergin and Shuang Yu and Lingxi Zeng and YingLin Zhang and Jiachen Zhao and Yefeng Zheng and Martin Zukovec and Richard Do and Anton Becker and Amber Simpson and Ender Konukoglu and Andras Jakab and Spyridon Bakas and Leo Joskowicz and Bjoern Menze", "abstract": "  Uncertainty in medical image segmentation tasks, especially inter-rater\nvariability, arising from differences in interpretations and annotations by\nvarious experts, presents a significant challenge in achieving consistent and\nreliable image segmentation. This variability not only reflects the inherent\ncomplexity and subjective nature of medical image interpretation but also\ndirectly impacts the development and evaluation of automated segmentation\nalgorithms. Accurately modeling and quantifying this variability is essential\nfor enhancing the robustness and clinical applicability of these algorithms. We\nreport the set-up and summarize the benchmark results of the Quantification of\nUncertainties in Biomedical Image Quantification Challenge (QUBIQ), which was\norganized in conjunction with International Conferences on Medical Image\nComputing and Computer-Assisted Intervention (MICCAI) 2020 and 2021. The\nchallenge focuses on the uncertainty quantification of medical image\nsegmentation which considers the omnipresence of inter-rater variability in\nimaging datasets. The large collection of images with multi-rater annotations\nfeatures various modalities such as MRI and CT; various organs such as the\nbrain, prostate, kidney, and pancreas; and different image dimensions 2D-vs-3D.\nA total of 24 teams submitted different solutions to the problem, combining\nvarious baseline models, Bayesian neural networks, and ensemble model\ntechniques. The obtained results indicate the importance of the ensemble\nmodels, as well as the need for further research to develop efficient 3D\nmethods for uncertainty quantification methods in 3D segmentation tasks.\n", "link": "http://arxiv.org/abs/2405.18435v2", "date": "2024-06-24", "relevancy": 2.0906, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5989}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.536}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QUBIQ%3A%20Uncertainty%20Quantification%20for%20Biomedical%20Image%20Segmentation%0A%20%20Challenge&body=Title%3A%20QUBIQ%3A%20Uncertainty%20Quantification%20for%20Biomedical%20Image%20Segmentation%0A%20%20Challenge%0AAuthor%3A%20Hongwei%20Bran%20Li%20and%20Fernando%20Navarro%20and%20Ivan%20Ezhov%20and%20Amirhossein%20Bayat%20and%20Dhritiman%20Das%20and%20Florian%20Kofler%20and%20Suprosanna%20Shit%20and%20Diana%20Waldmannstetter%20and%20Johannes%20C.%20Paetzold%20and%20Xiaobin%20Hu%20and%20Benedikt%20Wiestler%20and%20Lucas%20Zimmer%20and%20Tamaz%20Amiranashvili%20and%20Chinmay%20Prabhakar%20and%20Christoph%20Berger%20and%20Jonas%20Weidner%20and%20Michelle%20Alonso-Basant%20and%20Arif%20Rashid%20and%20Ujjwal%20Baid%20and%20Wesam%20Adel%20and%20Deniz%20Ali%20and%20Bhakti%20Baheti%20and%20Yingbin%20Bai%20and%20Ishaan%20Bhatt%20and%20Sabri%20Can%20Cetindag%20and%20Wenting%20Chen%20and%20Li%20Cheng%20and%20Prasad%20Dutand%20and%20Lara%20Dular%20and%20Mustafa%20A.%20Elattar%20and%20Ming%20Feng%20and%20Shengbo%20Gao%20and%20Henkjan%20Huisman%20and%20Weifeng%20Hu%20and%20Shubham%20Innani%20and%20Wei%20Jiat%20and%20Davood%20Karimi%20and%20Hugo%20J.%20Kuijf%20and%20Jin%20Tae%20Kwak%20and%20Hoang%20Long%20Le%20and%20Xiang%20Lia%20and%20Huiyan%20Lin%20and%20Tongliang%20Liu%20and%20Jun%20Ma%20and%20Kai%20Ma%20and%20Ting%20Ma%20and%20Ilkay%20Oksuz%20and%20Robbie%20Holland%20and%20Arlindo%20L.%20Oliveira%20and%20Jimut%20Bahan%20Pal%20and%20Xuan%20Pei%20and%20Maoying%20Qiao%20and%20Anindo%20Saha%20and%20Raghavendra%20Selvan%20and%20Linlin%20Shen%20and%20Joao%20Lourenco%20Silva%20and%20Ziga%20Spiclin%20and%20Sanjay%20Talbar%20and%20Dadong%20Wang%20and%20Wei%20Wang%20and%20Xiong%20Wang%20and%20Yin%20Wang%20and%20Ruiling%20Xia%20and%20Kele%20Xu%20and%20Yanwu%20Yan%20and%20Mert%20Yergin%20and%20Shuang%20Yu%20and%20Lingxi%20Zeng%20and%20YingLin%20Zhang%20and%20Jiachen%20Zhao%20and%20Yefeng%20Zheng%20and%20Martin%20Zukovec%20and%20Richard%20Do%20and%20Anton%20Becker%20and%20Amber%20Simpson%20and%20Ender%20Konukoglu%20and%20Andras%20Jakab%20and%20Spyridon%20Bakas%20and%20Leo%20Joskowicz%20and%20Bjoern%20Menze%0AAbstract%3A%20%20%20Uncertainty%20in%20medical%20image%20segmentation%20tasks%2C%20especially%20inter-rater%0Avariability%2C%20arising%20from%20differences%20in%20interpretations%20and%20annotations%20by%0Avarious%20experts%2C%20presents%20a%20significant%20challenge%20in%20achieving%20consistent%20and%0Areliable%20image%20segmentation.%20This%20variability%20not%20only%20reflects%20the%20inherent%0Acomplexity%20and%20subjective%20nature%20of%20medical%20image%20interpretation%20but%20also%0Adirectly%20impacts%20the%20development%20and%20evaluation%20of%20automated%20segmentation%0Aalgorithms.%20Accurately%20modeling%20and%20quantifying%20this%20variability%20is%20essential%0Afor%20enhancing%20the%20robustness%20and%20clinical%20applicability%20of%20these%20algorithms.%20We%0Areport%20the%20set-up%20and%20summarize%20the%20benchmark%20results%20of%20the%20Quantification%20of%0AUncertainties%20in%20Biomedical%20Image%20Quantification%20Challenge%20%28QUBIQ%29%2C%20which%20was%0Aorganized%20in%20conjunction%20with%20International%20Conferences%20on%20Medical%20Image%0AComputing%20and%20Computer-Assisted%20Intervention%20%28MICCAI%29%202020%20and%202021.%20The%0Achallenge%20focuses%20on%20the%20uncertainty%20quantification%20of%20medical%20image%0Asegmentation%20which%20considers%20the%20omnipresence%20of%20inter-rater%20variability%20in%0Aimaging%20datasets.%20The%20large%20collection%20of%20images%20with%20multi-rater%20annotations%0Afeatures%20various%20modalities%20such%20as%20MRI%20and%20CT%3B%20various%20organs%20such%20as%20the%0Abrain%2C%20prostate%2C%20kidney%2C%20and%20pancreas%3B%20and%20different%20image%20dimensions%202D-vs-3D.%0AA%20total%20of%2024%20teams%20submitted%20different%20solutions%20to%20the%20problem%2C%20combining%0Avarious%20baseline%20models%2C%20Bayesian%20neural%20networks%2C%20and%20ensemble%20model%0Atechniques.%20The%20obtained%20results%20indicate%20the%20importance%20of%20the%20ensemble%0Amodels%2C%20as%20well%20as%20the%20need%20for%20further%20research%20to%20develop%20efficient%203D%0Amethods%20for%20uncertainty%20quantification%20methods%20in%203D%20segmentation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18435v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQUBIQ%253A%2520Uncertainty%2520Quantification%2520for%2520Biomedical%2520Image%2520Segmentation%250A%2520%2520Challenge%26entry.906535625%3DHongwei%2520Bran%2520Li%2520and%2520Fernando%2520Navarro%2520and%2520Ivan%2520Ezhov%2520and%2520Amirhossein%2520Bayat%2520and%2520Dhritiman%2520Das%2520and%2520Florian%2520Kofler%2520and%2520Suprosanna%2520Shit%2520and%2520Diana%2520Waldmannstetter%2520and%2520Johannes%2520C.%2520Paetzold%2520and%2520Xiaobin%2520Hu%2520and%2520Benedikt%2520Wiestler%2520and%2520Lucas%2520Zimmer%2520and%2520Tamaz%2520Amiranashvili%2520and%2520Chinmay%2520Prabhakar%2520and%2520Christoph%2520Berger%2520and%2520Jonas%2520Weidner%2520and%2520Michelle%2520Alonso-Basant%2520and%2520Arif%2520Rashid%2520and%2520Ujjwal%2520Baid%2520and%2520Wesam%2520Adel%2520and%2520Deniz%2520Ali%2520and%2520Bhakti%2520Baheti%2520and%2520Yingbin%2520Bai%2520and%2520Ishaan%2520Bhatt%2520and%2520Sabri%2520Can%2520Cetindag%2520and%2520Wenting%2520Chen%2520and%2520Li%2520Cheng%2520and%2520Prasad%2520Dutand%2520and%2520Lara%2520Dular%2520and%2520Mustafa%2520A.%2520Elattar%2520and%2520Ming%2520Feng%2520and%2520Shengbo%2520Gao%2520and%2520Henkjan%2520Huisman%2520and%2520Weifeng%2520Hu%2520and%2520Shubham%2520Innani%2520and%2520Wei%2520Jiat%2520and%2520Davood%2520Karimi%2520and%2520Hugo%2520J.%2520Kuijf%2520and%2520Jin%2520Tae%2520Kwak%2520and%2520Hoang%2520Long%2520Le%2520and%2520Xiang%2520Lia%2520and%2520Huiyan%2520Lin%2520and%2520Tongliang%2520Liu%2520and%2520Jun%2520Ma%2520and%2520Kai%2520Ma%2520and%2520Ting%2520Ma%2520and%2520Ilkay%2520Oksuz%2520and%2520Robbie%2520Holland%2520and%2520Arlindo%2520L.%2520Oliveira%2520and%2520Jimut%2520Bahan%2520Pal%2520and%2520Xuan%2520Pei%2520and%2520Maoying%2520Qiao%2520and%2520Anindo%2520Saha%2520and%2520Raghavendra%2520Selvan%2520and%2520Linlin%2520Shen%2520and%2520Joao%2520Lourenco%2520Silva%2520and%2520Ziga%2520Spiclin%2520and%2520Sanjay%2520Talbar%2520and%2520Dadong%2520Wang%2520and%2520Wei%2520Wang%2520and%2520Xiong%2520Wang%2520and%2520Yin%2520Wang%2520and%2520Ruiling%2520Xia%2520and%2520Kele%2520Xu%2520and%2520Yanwu%2520Yan%2520and%2520Mert%2520Yergin%2520and%2520Shuang%2520Yu%2520and%2520Lingxi%2520Zeng%2520and%2520YingLin%2520Zhang%2520and%2520Jiachen%2520Zhao%2520and%2520Yefeng%2520Zheng%2520and%2520Martin%2520Zukovec%2520and%2520Richard%2520Do%2520and%2520Anton%2520Becker%2520and%2520Amber%2520Simpson%2520and%2520Ender%2520Konukoglu%2520and%2520Andras%2520Jakab%2520and%2520Spyridon%2520Bakas%2520and%2520Leo%2520Joskowicz%2520and%2520Bjoern%2520Menze%26entry.1292438233%3D%2520%2520Uncertainty%2520in%2520medical%2520image%2520segmentation%2520tasks%252C%2520especially%2520inter-rater%250Avariability%252C%2520arising%2520from%2520differences%2520in%2520interpretations%2520and%2520annotations%2520by%250Avarious%2520experts%252C%2520presents%2520a%2520significant%2520challenge%2520in%2520achieving%2520consistent%2520and%250Areliable%2520image%2520segmentation.%2520This%2520variability%2520not%2520only%2520reflects%2520the%2520inherent%250Acomplexity%2520and%2520subjective%2520nature%2520of%2520medical%2520image%2520interpretation%2520but%2520also%250Adirectly%2520impacts%2520the%2520development%2520and%2520evaluation%2520of%2520automated%2520segmentation%250Aalgorithms.%2520Accurately%2520modeling%2520and%2520quantifying%2520this%2520variability%2520is%2520essential%250Afor%2520enhancing%2520the%2520robustness%2520and%2520clinical%2520applicability%2520of%2520these%2520algorithms.%2520We%250Areport%2520the%2520set-up%2520and%2520summarize%2520the%2520benchmark%2520results%2520of%2520the%2520Quantification%2520of%250AUncertainties%2520in%2520Biomedical%2520Image%2520Quantification%2520Challenge%2520%2528QUBIQ%2529%252C%2520which%2520was%250Aorganized%2520in%2520conjunction%2520with%2520International%2520Conferences%2520on%2520Medical%2520Image%250AComputing%2520and%2520Computer-Assisted%2520Intervention%2520%2528MICCAI%2529%25202020%2520and%25202021.%2520The%250Achallenge%2520focuses%2520on%2520the%2520uncertainty%2520quantification%2520of%2520medical%2520image%250Asegmentation%2520which%2520considers%2520the%2520omnipresence%2520of%2520inter-rater%2520variability%2520in%250Aimaging%2520datasets.%2520The%2520large%2520collection%2520of%2520images%2520with%2520multi-rater%2520annotations%250Afeatures%2520various%2520modalities%2520such%2520as%2520MRI%2520and%2520CT%253B%2520various%2520organs%2520such%2520as%2520the%250Abrain%252C%2520prostate%252C%2520kidney%252C%2520and%2520pancreas%253B%2520and%2520different%2520image%2520dimensions%25202D-vs-3D.%250AA%2520total%2520of%252024%2520teams%2520submitted%2520different%2520solutions%2520to%2520the%2520problem%252C%2520combining%250Avarious%2520baseline%2520models%252C%2520Bayesian%2520neural%2520networks%252C%2520and%2520ensemble%2520model%250Atechniques.%2520The%2520obtained%2520results%2520indicate%2520the%2520importance%2520of%2520the%2520ensemble%250Amodels%252C%2520as%2520well%2520as%2520the%2520need%2520for%2520further%2520research%2520to%2520develop%2520efficient%25203D%250Amethods%2520for%2520uncertainty%2520quantification%2520methods%2520in%25203D%2520segmentation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18435v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QUBIQ%3A%20Uncertainty%20Quantification%20for%20Biomedical%20Image%20Segmentation%0A%20%20Challenge&entry.906535625=Hongwei%20Bran%20Li%20and%20Fernando%20Navarro%20and%20Ivan%20Ezhov%20and%20Amirhossein%20Bayat%20and%20Dhritiman%20Das%20and%20Florian%20Kofler%20and%20Suprosanna%20Shit%20and%20Diana%20Waldmannstetter%20and%20Johannes%20C.%20Paetzold%20and%20Xiaobin%20Hu%20and%20Benedikt%20Wiestler%20and%20Lucas%20Zimmer%20and%20Tamaz%20Amiranashvili%20and%20Chinmay%20Prabhakar%20and%20Christoph%20Berger%20and%20Jonas%20Weidner%20and%20Michelle%20Alonso-Basant%20and%20Arif%20Rashid%20and%20Ujjwal%20Baid%20and%20Wesam%20Adel%20and%20Deniz%20Ali%20and%20Bhakti%20Baheti%20and%20Yingbin%20Bai%20and%20Ishaan%20Bhatt%20and%20Sabri%20Can%20Cetindag%20and%20Wenting%20Chen%20and%20Li%20Cheng%20and%20Prasad%20Dutand%20and%20Lara%20Dular%20and%20Mustafa%20A.%20Elattar%20and%20Ming%20Feng%20and%20Shengbo%20Gao%20and%20Henkjan%20Huisman%20and%20Weifeng%20Hu%20and%20Shubham%20Innani%20and%20Wei%20Jiat%20and%20Davood%20Karimi%20and%20Hugo%20J.%20Kuijf%20and%20Jin%20Tae%20Kwak%20and%20Hoang%20Long%20Le%20and%20Xiang%20Lia%20and%20Huiyan%20Lin%20and%20Tongliang%20Liu%20and%20Jun%20Ma%20and%20Kai%20Ma%20and%20Ting%20Ma%20and%20Ilkay%20Oksuz%20and%20Robbie%20Holland%20and%20Arlindo%20L.%20Oliveira%20and%20Jimut%20Bahan%20Pal%20and%20Xuan%20Pei%20and%20Maoying%20Qiao%20and%20Anindo%20Saha%20and%20Raghavendra%20Selvan%20and%20Linlin%20Shen%20and%20Joao%20Lourenco%20Silva%20and%20Ziga%20Spiclin%20and%20Sanjay%20Talbar%20and%20Dadong%20Wang%20and%20Wei%20Wang%20and%20Xiong%20Wang%20and%20Yin%20Wang%20and%20Ruiling%20Xia%20and%20Kele%20Xu%20and%20Yanwu%20Yan%20and%20Mert%20Yergin%20and%20Shuang%20Yu%20and%20Lingxi%20Zeng%20and%20YingLin%20Zhang%20and%20Jiachen%20Zhao%20and%20Yefeng%20Zheng%20and%20Martin%20Zukovec%20and%20Richard%20Do%20and%20Anton%20Becker%20and%20Amber%20Simpson%20and%20Ender%20Konukoglu%20and%20Andras%20Jakab%20and%20Spyridon%20Bakas%20and%20Leo%20Joskowicz%20and%20Bjoern%20Menze&entry.1292438233=%20%20Uncertainty%20in%20medical%20image%20segmentation%20tasks%2C%20especially%20inter-rater%0Avariability%2C%20arising%20from%20differences%20in%20interpretations%20and%20annotations%20by%0Avarious%20experts%2C%20presents%20a%20significant%20challenge%20in%20achieving%20consistent%20and%0Areliable%20image%20segmentation.%20This%20variability%20not%20only%20reflects%20the%20inherent%0Acomplexity%20and%20subjective%20nature%20of%20medical%20image%20interpretation%20but%20also%0Adirectly%20impacts%20the%20development%20and%20evaluation%20of%20automated%20segmentation%0Aalgorithms.%20Accurately%20modeling%20and%20quantifying%20this%20variability%20is%20essential%0Afor%20enhancing%20the%20robustness%20and%20clinical%20applicability%20of%20these%20algorithms.%20We%0Areport%20the%20set-up%20and%20summarize%20the%20benchmark%20results%20of%20the%20Quantification%20of%0AUncertainties%20in%20Biomedical%20Image%20Quantification%20Challenge%20%28QUBIQ%29%2C%20which%20was%0Aorganized%20in%20conjunction%20with%20International%20Conferences%20on%20Medical%20Image%0AComputing%20and%20Computer-Assisted%20Intervention%20%28MICCAI%29%202020%20and%202021.%20The%0Achallenge%20focuses%20on%20the%20uncertainty%20quantification%20of%20medical%20image%0Asegmentation%20which%20considers%20the%20omnipresence%20of%20inter-rater%20variability%20in%0Aimaging%20datasets.%20The%20large%20collection%20of%20images%20with%20multi-rater%20annotations%0Afeatures%20various%20modalities%20such%20as%20MRI%20and%20CT%3B%20various%20organs%20such%20as%20the%0Abrain%2C%20prostate%2C%20kidney%2C%20and%20pancreas%3B%20and%20different%20image%20dimensions%202D-vs-3D.%0AA%20total%20of%2024%20teams%20submitted%20different%20solutions%20to%20the%20problem%2C%20combining%0Avarious%20baseline%20models%2C%20Bayesian%20neural%20networks%2C%20and%20ensemble%20model%0Atechniques.%20The%20obtained%20results%20indicate%20the%20importance%20of%20the%20ensemble%0Amodels%2C%20as%20well%20as%20the%20need%20for%20further%20research%20to%20develop%20efficient%203D%0Amethods%20for%20uncertainty%20quantification%20methods%20in%203D%20segmentation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18435v2&entry.124074799=Read"},
{"title": "Confidence Aware Inverse Constrained Reinforcement Learning", "author": "Sriram Ganapathi Subramanian and Guiliang Liu and Mohammed Elmahgiubi and Kasra Rezaee and Pascal Poupart", "abstract": "  In coming up with solutions to real-world problems, humans implicitly adhere\nto constraints that are too numerous and complex to be specified completely.\nHowever, reinforcement learning (RL) agents need these constraints to learn the\ncorrect optimal policy in these settings. The field of Inverse Constraint\nReinforcement Learning (ICRL) deals with this problem and provides algorithms\nthat aim to estimate the constraints from expert demonstrations collected\noffline. Practitioners prefer to know a measure of confidence in the estimated\nconstraints, before deciding to use these constraints, which allows them to\nonly use the constraints that satisfy a desired level of confidence. However,\nprior works do not allow users to provide the desired level of confidence for\nthe inferred constraints. This work provides a principled ICRL method that can\ntake a confidence level with a set of expert demonstrations and outputs a\nconstraint that is at least as constraining as the true underlying constraint\nwith the desired level of confidence. Further, unlike previous methods, this\nmethod allows a user to know if the number of expert trajectories is\ninsufficient to learn a constraint with a desired level of confidence, and\ntherefore collect more expert trajectories as required to simultaneously learn\nconstraints with the desired level of confidence and a policy that achieves the\ndesired level of performance.\n", "link": "http://arxiv.org/abs/2406.16782v1", "date": "2024-06-24", "relevancy": 2.0804, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.53}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5197}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Confidence%20Aware%20Inverse%20Constrained%20Reinforcement%20Learning&body=Title%3A%20Confidence%20Aware%20Inverse%20Constrained%20Reinforcement%20Learning%0AAuthor%3A%20Sriram%20Ganapathi%20Subramanian%20and%20Guiliang%20Liu%20and%20Mohammed%20Elmahgiubi%20and%20Kasra%20Rezaee%20and%20Pascal%20Poupart%0AAbstract%3A%20%20%20In%20coming%20up%20with%20solutions%20to%20real-world%20problems%2C%20humans%20implicitly%20adhere%0Ato%20constraints%20that%20are%20too%20numerous%20and%20complex%20to%20be%20specified%20completely.%0AHowever%2C%20reinforcement%20learning%20%28RL%29%20agents%20need%20these%20constraints%20to%20learn%20the%0Acorrect%20optimal%20policy%20in%20these%20settings.%20The%20field%20of%20Inverse%20Constraint%0AReinforcement%20Learning%20%28ICRL%29%20deals%20with%20this%20problem%20and%20provides%20algorithms%0Athat%20aim%20to%20estimate%20the%20constraints%20from%20expert%20demonstrations%20collected%0Aoffline.%20Practitioners%20prefer%20to%20know%20a%20measure%20of%20confidence%20in%20the%20estimated%0Aconstraints%2C%20before%20deciding%20to%20use%20these%20constraints%2C%20which%20allows%20them%20to%0Aonly%20use%20the%20constraints%20that%20satisfy%20a%20desired%20level%20of%20confidence.%20However%2C%0Aprior%20works%20do%20not%20allow%20users%20to%20provide%20the%20desired%20level%20of%20confidence%20for%0Athe%20inferred%20constraints.%20This%20work%20provides%20a%20principled%20ICRL%20method%20that%20can%0Atake%20a%20confidence%20level%20with%20a%20set%20of%20expert%20demonstrations%20and%20outputs%20a%0Aconstraint%20that%20is%20at%20least%20as%20constraining%20as%20the%20true%20underlying%20constraint%0Awith%20the%20desired%20level%20of%20confidence.%20Further%2C%20unlike%20previous%20methods%2C%20this%0Amethod%20allows%20a%20user%20to%20know%20if%20the%20number%20of%20expert%20trajectories%20is%0Ainsufficient%20to%20learn%20a%20constraint%20with%20a%20desired%20level%20of%20confidence%2C%20and%0Atherefore%20collect%20more%20expert%20trajectories%20as%20required%20to%20simultaneously%20learn%0Aconstraints%20with%20the%20desired%20level%20of%20confidence%20and%20a%20policy%20that%20achieves%20the%0Adesired%20level%20of%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConfidence%2520Aware%2520Inverse%2520Constrained%2520Reinforcement%2520Learning%26entry.906535625%3DSriram%2520Ganapathi%2520Subramanian%2520and%2520Guiliang%2520Liu%2520and%2520Mohammed%2520Elmahgiubi%2520and%2520Kasra%2520Rezaee%2520and%2520Pascal%2520Poupart%26entry.1292438233%3D%2520%2520In%2520coming%2520up%2520with%2520solutions%2520to%2520real-world%2520problems%252C%2520humans%2520implicitly%2520adhere%250Ato%2520constraints%2520that%2520are%2520too%2520numerous%2520and%2520complex%2520to%2520be%2520specified%2520completely.%250AHowever%252C%2520reinforcement%2520learning%2520%2528RL%2529%2520agents%2520need%2520these%2520constraints%2520to%2520learn%2520the%250Acorrect%2520optimal%2520policy%2520in%2520these%2520settings.%2520The%2520field%2520of%2520Inverse%2520Constraint%250AReinforcement%2520Learning%2520%2528ICRL%2529%2520deals%2520with%2520this%2520problem%2520and%2520provides%2520algorithms%250Athat%2520aim%2520to%2520estimate%2520the%2520constraints%2520from%2520expert%2520demonstrations%2520collected%250Aoffline.%2520Practitioners%2520prefer%2520to%2520know%2520a%2520measure%2520of%2520confidence%2520in%2520the%2520estimated%250Aconstraints%252C%2520before%2520deciding%2520to%2520use%2520these%2520constraints%252C%2520which%2520allows%2520them%2520to%250Aonly%2520use%2520the%2520constraints%2520that%2520satisfy%2520a%2520desired%2520level%2520of%2520confidence.%2520However%252C%250Aprior%2520works%2520do%2520not%2520allow%2520users%2520to%2520provide%2520the%2520desired%2520level%2520of%2520confidence%2520for%250Athe%2520inferred%2520constraints.%2520This%2520work%2520provides%2520a%2520principled%2520ICRL%2520method%2520that%2520can%250Atake%2520a%2520confidence%2520level%2520with%2520a%2520set%2520of%2520expert%2520demonstrations%2520and%2520outputs%2520a%250Aconstraint%2520that%2520is%2520at%2520least%2520as%2520constraining%2520as%2520the%2520true%2520underlying%2520constraint%250Awith%2520the%2520desired%2520level%2520of%2520confidence.%2520Further%252C%2520unlike%2520previous%2520methods%252C%2520this%250Amethod%2520allows%2520a%2520user%2520to%2520know%2520if%2520the%2520number%2520of%2520expert%2520trajectories%2520is%250Ainsufficient%2520to%2520learn%2520a%2520constraint%2520with%2520a%2520desired%2520level%2520of%2520confidence%252C%2520and%250Atherefore%2520collect%2520more%2520expert%2520trajectories%2520as%2520required%2520to%2520simultaneously%2520learn%250Aconstraints%2520with%2520the%2520desired%2520level%2520of%2520confidence%2520and%2520a%2520policy%2520that%2520achieves%2520the%250Adesired%2520level%2520of%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Confidence%20Aware%20Inverse%20Constrained%20Reinforcement%20Learning&entry.906535625=Sriram%20Ganapathi%20Subramanian%20and%20Guiliang%20Liu%20and%20Mohammed%20Elmahgiubi%20and%20Kasra%20Rezaee%20and%20Pascal%20Poupart&entry.1292438233=%20%20In%20coming%20up%20with%20solutions%20to%20real-world%20problems%2C%20humans%20implicitly%20adhere%0Ato%20constraints%20that%20are%20too%20numerous%20and%20complex%20to%20be%20specified%20completely.%0AHowever%2C%20reinforcement%20learning%20%28RL%29%20agents%20need%20these%20constraints%20to%20learn%20the%0Acorrect%20optimal%20policy%20in%20these%20settings.%20The%20field%20of%20Inverse%20Constraint%0AReinforcement%20Learning%20%28ICRL%29%20deals%20with%20this%20problem%20and%20provides%20algorithms%0Athat%20aim%20to%20estimate%20the%20constraints%20from%20expert%20demonstrations%20collected%0Aoffline.%20Practitioners%20prefer%20to%20know%20a%20measure%20of%20confidence%20in%20the%20estimated%0Aconstraints%2C%20before%20deciding%20to%20use%20these%20constraints%2C%20which%20allows%20them%20to%0Aonly%20use%20the%20constraints%20that%20satisfy%20a%20desired%20level%20of%20confidence.%20However%2C%0Aprior%20works%20do%20not%20allow%20users%20to%20provide%20the%20desired%20level%20of%20confidence%20for%0Athe%20inferred%20constraints.%20This%20work%20provides%20a%20principled%20ICRL%20method%20that%20can%0Atake%20a%20confidence%20level%20with%20a%20set%20of%20expert%20demonstrations%20and%20outputs%20a%0Aconstraint%20that%20is%20at%20least%20as%20constraining%20as%20the%20true%20underlying%20constraint%0Awith%20the%20desired%20level%20of%20confidence.%20Further%2C%20unlike%20previous%20methods%2C%20this%0Amethod%20allows%20a%20user%20to%20know%20if%20the%20number%20of%20expert%20trajectories%20is%0Ainsufficient%20to%20learn%20a%20constraint%20with%20a%20desired%20level%20of%20confidence%2C%20and%0Atherefore%20collect%20more%20expert%20trajectories%20as%20required%20to%20simultaneously%20learn%0Aconstraints%20with%20the%20desired%20level%20of%20confidence%20and%20a%20policy%20that%20achieves%20the%0Adesired%20level%20of%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16782v1&entry.124074799=Read"},
{"title": "Evaluating the Robustness of Deep-Learning Algorithm-Selection Models by\n  Evolving Adversarial Instances", "author": "Emma Hart and Quentin Renau and Kevin Sim and Mohamad Alissa", "abstract": "  Deep neural networks (DNN) are increasingly being used to perform\nalgorithm-selection in combinatorial optimisation domains, particularly as they\naccommodate input representations which avoid designing and calculating\nfeatures. Mounting evidence from domains that use images as input shows that\ndeep convolutional networks are vulnerable to adversarial samples, in which a\nsmall perturbation of an instance can cause the DNN to misclassify. However, it\nremains unknown as to whether deep recurrent networks (DRN) which have recently\nbeen shown promise as algorithm-selectors in the bin-packing domain are equally\nvulnerable. We use an evolutionary algorithm (EA) to find perturbations of\ninstances from two existing benchmarks for online bin packing that cause\ntrained DRNs to misclassify: adversarial samples are successfully generated\nfrom up to 56% of the original instances depending on the dataset. Analysis of\nthe new misclassified instances sheds light on the `fragility' of some training\ninstances, i.e. instances where it is trivial to find a small perturbation that\nresults in a misclassification and the factors that influence this. Finally,\nthe method generates a large number of new instances misclassified with a wide\nvariation in confidence, providing a rich new source of training data to create\nmore robust models.\n", "link": "http://arxiv.org/abs/2406.16609v1", "date": "2024-06-24", "relevancy": 2.0791, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5552}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4949}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20Robustness%20of%20Deep-Learning%20Algorithm-Selection%20Models%20by%0A%20%20Evolving%20Adversarial%20Instances&body=Title%3A%20Evaluating%20the%20Robustness%20of%20Deep-Learning%20Algorithm-Selection%20Models%20by%0A%20%20Evolving%20Adversarial%20Instances%0AAuthor%3A%20Emma%20Hart%20and%20Quentin%20Renau%20and%20Kevin%20Sim%20and%20Mohamad%20Alissa%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNN%29%20are%20increasingly%20being%20used%20to%20perform%0Aalgorithm-selection%20in%20combinatorial%20optimisation%20domains%2C%20particularly%20as%20they%0Aaccommodate%20input%20representations%20which%20avoid%20designing%20and%20calculating%0Afeatures.%20Mounting%20evidence%20from%20domains%20that%20use%20images%20as%20input%20shows%20that%0Adeep%20convolutional%20networks%20are%20vulnerable%20to%20adversarial%20samples%2C%20in%20which%20a%0Asmall%20perturbation%20of%20an%20instance%20can%20cause%20the%20DNN%20to%20misclassify.%20However%2C%20it%0Aremains%20unknown%20as%20to%20whether%20deep%20recurrent%20networks%20%28DRN%29%20which%20have%20recently%0Abeen%20shown%20promise%20as%20algorithm-selectors%20in%20the%20bin-packing%20domain%20are%20equally%0Avulnerable.%20We%20use%20an%20evolutionary%20algorithm%20%28EA%29%20to%20find%20perturbations%20of%0Ainstances%20from%20two%20existing%20benchmarks%20for%20online%20bin%20packing%20that%20cause%0Atrained%20DRNs%20to%20misclassify%3A%20adversarial%20samples%20are%20successfully%20generated%0Afrom%20up%20to%2056%25%20of%20the%20original%20instances%20depending%20on%20the%20dataset.%20Analysis%20of%0Athe%20new%20misclassified%20instances%20sheds%20light%20on%20the%20%60fragility%27%20of%20some%20training%0Ainstances%2C%20i.e.%20instances%20where%20it%20is%20trivial%20to%20find%20a%20small%20perturbation%20that%0Aresults%20in%20a%20misclassification%20and%20the%20factors%20that%20influence%20this.%20Finally%2C%0Athe%20method%20generates%20a%20large%20number%20of%20new%20instances%20misclassified%20with%20a%20wide%0Avariation%20in%20confidence%2C%20providing%20a%20rich%20new%20source%20of%20training%20data%20to%20create%0Amore%20robust%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520Robustness%2520of%2520Deep-Learning%2520Algorithm-Selection%2520Models%2520by%250A%2520%2520Evolving%2520Adversarial%2520Instances%26entry.906535625%3DEmma%2520Hart%2520and%2520Quentin%2520Renau%2520and%2520Kevin%2520Sim%2520and%2520Mohamad%2520Alissa%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNN%2529%2520are%2520increasingly%2520being%2520used%2520to%2520perform%250Aalgorithm-selection%2520in%2520combinatorial%2520optimisation%2520domains%252C%2520particularly%2520as%2520they%250Aaccommodate%2520input%2520representations%2520which%2520avoid%2520designing%2520and%2520calculating%250Afeatures.%2520Mounting%2520evidence%2520from%2520domains%2520that%2520use%2520images%2520as%2520input%2520shows%2520that%250Adeep%2520convolutional%2520networks%2520are%2520vulnerable%2520to%2520adversarial%2520samples%252C%2520in%2520which%2520a%250Asmall%2520perturbation%2520of%2520an%2520instance%2520can%2520cause%2520the%2520DNN%2520to%2520misclassify.%2520However%252C%2520it%250Aremains%2520unknown%2520as%2520to%2520whether%2520deep%2520recurrent%2520networks%2520%2528DRN%2529%2520which%2520have%2520recently%250Abeen%2520shown%2520promise%2520as%2520algorithm-selectors%2520in%2520the%2520bin-packing%2520domain%2520are%2520equally%250Avulnerable.%2520We%2520use%2520an%2520evolutionary%2520algorithm%2520%2528EA%2529%2520to%2520find%2520perturbations%2520of%250Ainstances%2520from%2520two%2520existing%2520benchmarks%2520for%2520online%2520bin%2520packing%2520that%2520cause%250Atrained%2520DRNs%2520to%2520misclassify%253A%2520adversarial%2520samples%2520are%2520successfully%2520generated%250Afrom%2520up%2520to%252056%2525%2520of%2520the%2520original%2520instances%2520depending%2520on%2520the%2520dataset.%2520Analysis%2520of%250Athe%2520new%2520misclassified%2520instances%2520sheds%2520light%2520on%2520the%2520%2560fragility%2527%2520of%2520some%2520training%250Ainstances%252C%2520i.e.%2520instances%2520where%2520it%2520is%2520trivial%2520to%2520find%2520a%2520small%2520perturbation%2520that%250Aresults%2520in%2520a%2520misclassification%2520and%2520the%2520factors%2520that%2520influence%2520this.%2520Finally%252C%250Athe%2520method%2520generates%2520a%2520large%2520number%2520of%2520new%2520instances%2520misclassified%2520with%2520a%2520wide%250Avariation%2520in%2520confidence%252C%2520providing%2520a%2520rich%2520new%2520source%2520of%2520training%2520data%2520to%2520create%250Amore%2520robust%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Robustness%20of%20Deep-Learning%20Algorithm-Selection%20Models%20by%0A%20%20Evolving%20Adversarial%20Instances&entry.906535625=Emma%20Hart%20and%20Quentin%20Renau%20and%20Kevin%20Sim%20and%20Mohamad%20Alissa&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNN%29%20are%20increasingly%20being%20used%20to%20perform%0Aalgorithm-selection%20in%20combinatorial%20optimisation%20domains%2C%20particularly%20as%20they%0Aaccommodate%20input%20representations%20which%20avoid%20designing%20and%20calculating%0Afeatures.%20Mounting%20evidence%20from%20domains%20that%20use%20images%20as%20input%20shows%20that%0Adeep%20convolutional%20networks%20are%20vulnerable%20to%20adversarial%20samples%2C%20in%20which%20a%0Asmall%20perturbation%20of%20an%20instance%20can%20cause%20the%20DNN%20to%20misclassify.%20However%2C%20it%0Aremains%20unknown%20as%20to%20whether%20deep%20recurrent%20networks%20%28DRN%29%20which%20have%20recently%0Abeen%20shown%20promise%20as%20algorithm-selectors%20in%20the%20bin-packing%20domain%20are%20equally%0Avulnerable.%20We%20use%20an%20evolutionary%20algorithm%20%28EA%29%20to%20find%20perturbations%20of%0Ainstances%20from%20two%20existing%20benchmarks%20for%20online%20bin%20packing%20that%20cause%0Atrained%20DRNs%20to%20misclassify%3A%20adversarial%20samples%20are%20successfully%20generated%0Afrom%20up%20to%2056%25%20of%20the%20original%20instances%20depending%20on%20the%20dataset.%20Analysis%20of%0Athe%20new%20misclassified%20instances%20sheds%20light%20on%20the%20%60fragility%27%20of%20some%20training%0Ainstances%2C%20i.e.%20instances%20where%20it%20is%20trivial%20to%20find%20a%20small%20perturbation%20that%0Aresults%20in%20a%20misclassification%20and%20the%20factors%20that%20influence%20this.%20Finally%2C%0Athe%20method%20generates%20a%20large%20number%20of%20new%20instances%20misclassified%20with%20a%20wide%0Avariation%20in%20confidence%2C%20providing%20a%20rich%20new%20source%20of%20training%20data%20to%20create%0Amore%20robust%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16609v1&entry.124074799=Read"},
{"title": "From Decoding to Meta-Generation: Inference-time Algorithms for Large\n  Language Models", "author": "Sean Welleck and Amanda Bertsch and Matthew Finlayson and Hailey Schoelkopf and Alex Xie and Graham Neubig and Ilia Kulikov and Zaid Harchaoui", "abstract": "  One of the most striking findings in modern research on large language models\n(LLMs) is that scaling up compute during training leads to better results.\nHowever, less attention has been given to the benefits of scaling compute\nduring inference. This survey focuses on these inference-time approaches. We\nexplore three areas under a unified mathematical formalism: token-level\ngeneration algorithms, meta-generation algorithms, and efficient generation.\nToken-level generation algorithms, often called decoding algorithms, operate by\nsampling a single token at a time or constructing a token-level search space\nand then selecting an output. These methods typically assume access to a\nlanguage model's logits, next-token distributions, or probability scores.\nMeta-generation algorithms work on partial or full sequences, incorporating\ndomain knowledge, enabling backtracking, and integrating external information.\nEfficient generation methods aim to reduce token costs and improve the speed of\ngeneration. Our survey unifies perspectives from three research communities:\ntraditional natural language processing, modern LLMs, and machine learning\nsystems.\n", "link": "http://arxiv.org/abs/2406.16838v1", "date": "2024-06-24", "relevancy": 2.0744, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.533}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5186}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Decoding%20to%20Meta-Generation%3A%20Inference-time%20Algorithms%20for%20Large%0A%20%20Language%20Models&body=Title%3A%20From%20Decoding%20to%20Meta-Generation%3A%20Inference-time%20Algorithms%20for%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Sean%20Welleck%20and%20Amanda%20Bertsch%20and%20Matthew%20Finlayson%20and%20Hailey%20Schoelkopf%20and%20Alex%20Xie%20and%20Graham%20Neubig%20and%20Ilia%20Kulikov%20and%20Zaid%20Harchaoui%0AAbstract%3A%20%20%20One%20of%20the%20most%20striking%20findings%20in%20modern%20research%20on%20large%20language%20models%0A%28LLMs%29%20is%20that%20scaling%20up%20compute%20during%20training%20leads%20to%20better%20results.%0AHowever%2C%20less%20attention%20has%20been%20given%20to%20the%20benefits%20of%20scaling%20compute%0Aduring%20inference.%20This%20survey%20focuses%20on%20these%20inference-time%20approaches.%20We%0Aexplore%20three%20areas%20under%20a%20unified%20mathematical%20formalism%3A%20token-level%0Ageneration%20algorithms%2C%20meta-generation%20algorithms%2C%20and%20efficient%20generation.%0AToken-level%20generation%20algorithms%2C%20often%20called%20decoding%20algorithms%2C%20operate%20by%0Asampling%20a%20single%20token%20at%20a%20time%20or%20constructing%20a%20token-level%20search%20space%0Aand%20then%20selecting%20an%20output.%20These%20methods%20typically%20assume%20access%20to%20a%0Alanguage%20model%27s%20logits%2C%20next-token%20distributions%2C%20or%20probability%20scores.%0AMeta-generation%20algorithms%20work%20on%20partial%20or%20full%20sequences%2C%20incorporating%0Adomain%20knowledge%2C%20enabling%20backtracking%2C%20and%20integrating%20external%20information.%0AEfficient%20generation%20methods%20aim%20to%20reduce%20token%20costs%20and%20improve%20the%20speed%20of%0Ageneration.%20Our%20survey%20unifies%20perspectives%20from%20three%20research%20communities%3A%0Atraditional%20natural%20language%20processing%2C%20modern%20LLMs%2C%20and%20machine%20learning%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16838v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Decoding%2520to%2520Meta-Generation%253A%2520Inference-time%2520Algorithms%2520for%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DSean%2520Welleck%2520and%2520Amanda%2520Bertsch%2520and%2520Matthew%2520Finlayson%2520and%2520Hailey%2520Schoelkopf%2520and%2520Alex%2520Xie%2520and%2520Graham%2520Neubig%2520and%2520Ilia%2520Kulikov%2520and%2520Zaid%2520Harchaoui%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520most%2520striking%2520findings%2520in%2520modern%2520research%2520on%2520large%2520language%2520models%250A%2528LLMs%2529%2520is%2520that%2520scaling%2520up%2520compute%2520during%2520training%2520leads%2520to%2520better%2520results.%250AHowever%252C%2520less%2520attention%2520has%2520been%2520given%2520to%2520the%2520benefits%2520of%2520scaling%2520compute%250Aduring%2520inference.%2520This%2520survey%2520focuses%2520on%2520these%2520inference-time%2520approaches.%2520We%250Aexplore%2520three%2520areas%2520under%2520a%2520unified%2520mathematical%2520formalism%253A%2520token-level%250Ageneration%2520algorithms%252C%2520meta-generation%2520algorithms%252C%2520and%2520efficient%2520generation.%250AToken-level%2520generation%2520algorithms%252C%2520often%2520called%2520decoding%2520algorithms%252C%2520operate%2520by%250Asampling%2520a%2520single%2520token%2520at%2520a%2520time%2520or%2520constructing%2520a%2520token-level%2520search%2520space%250Aand%2520then%2520selecting%2520an%2520output.%2520These%2520methods%2520typically%2520assume%2520access%2520to%2520a%250Alanguage%2520model%2527s%2520logits%252C%2520next-token%2520distributions%252C%2520or%2520probability%2520scores.%250AMeta-generation%2520algorithms%2520work%2520on%2520partial%2520or%2520full%2520sequences%252C%2520incorporating%250Adomain%2520knowledge%252C%2520enabling%2520backtracking%252C%2520and%2520integrating%2520external%2520information.%250AEfficient%2520generation%2520methods%2520aim%2520to%2520reduce%2520token%2520costs%2520and%2520improve%2520the%2520speed%2520of%250Ageneration.%2520Our%2520survey%2520unifies%2520perspectives%2520from%2520three%2520research%2520communities%253A%250Atraditional%2520natural%2520language%2520processing%252C%2520modern%2520LLMs%252C%2520and%2520machine%2520learning%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16838v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Decoding%20to%20Meta-Generation%3A%20Inference-time%20Algorithms%20for%20Large%0A%20%20Language%20Models&entry.906535625=Sean%20Welleck%20and%20Amanda%20Bertsch%20and%20Matthew%20Finlayson%20and%20Hailey%20Schoelkopf%20and%20Alex%20Xie%20and%20Graham%20Neubig%20and%20Ilia%20Kulikov%20and%20Zaid%20Harchaoui&entry.1292438233=%20%20One%20of%20the%20most%20striking%20findings%20in%20modern%20research%20on%20large%20language%20models%0A%28LLMs%29%20is%20that%20scaling%20up%20compute%20during%20training%20leads%20to%20better%20results.%0AHowever%2C%20less%20attention%20has%20been%20given%20to%20the%20benefits%20of%20scaling%20compute%0Aduring%20inference.%20This%20survey%20focuses%20on%20these%20inference-time%20approaches.%20We%0Aexplore%20three%20areas%20under%20a%20unified%20mathematical%20formalism%3A%20token-level%0Ageneration%20algorithms%2C%20meta-generation%20algorithms%2C%20and%20efficient%20generation.%0AToken-level%20generation%20algorithms%2C%20often%20called%20decoding%20algorithms%2C%20operate%20by%0Asampling%20a%20single%20token%20at%20a%20time%20or%20constructing%20a%20token-level%20search%20space%0Aand%20then%20selecting%20an%20output.%20These%20methods%20typically%20assume%20access%20to%20a%0Alanguage%20model%27s%20logits%2C%20next-token%20distributions%2C%20or%20probability%20scores.%0AMeta-generation%20algorithms%20work%20on%20partial%20or%20full%20sequences%2C%20incorporating%0Adomain%20knowledge%2C%20enabling%20backtracking%2C%20and%20integrating%20external%20information.%0AEfficient%20generation%20methods%20aim%20to%20reduce%20token%20costs%20and%20improve%20the%20speed%20of%0Ageneration.%20Our%20survey%20unifies%20perspectives%20from%20three%20research%20communities%3A%0Atraditional%20natural%20language%20processing%2C%20modern%20LLMs%2C%20and%20machine%20learning%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16838v1&entry.124074799=Read"},
{"title": "Toward Fairer Face Recognition Datasets", "author": "Alexandre Fournier-Mongieux and Michael Soumm and Adrian Popescu and Bertrand Luvison and Herv\u00e9 Le Borgne", "abstract": "  Face recognition and verification are two computer vision tasks whose\nperformance has progressed with the introduction of deep representations.\nHowever, ethical, legal, and technical challenges due to the sensitive\ncharacter of face data and biases in real training datasets hinder their\ndevelopment. Generative AI addresses privacy by creating fictitious identities,\nbut fairness problems persist. We promote fairness by introducing a demographic\nattributes balancing mechanism in generated training datasets. We experiment\nwith an existing real dataset, three generated training datasets, and the\nbalanced versions of a diffusion-based dataset. We propose a comprehensive\nevaluation that considers accuracy and fairness equally and includes a rigorous\nregression-based statistical analysis of attributes. The analysis shows that\nbalancing reduces demographic unfairness. Also, a performance gap persists\ndespite generation becoming more accurate with time. The proposed balancing\nmethod and comprehensive verification evaluation promote fairer and transparent\nface recognition and verification.\n", "link": "http://arxiv.org/abs/2406.16592v1", "date": "2024-06-24", "relevancy": 2.0732, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5386}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5227}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Fairer%20Face%20Recognition%20Datasets&body=Title%3A%20Toward%20Fairer%20Face%20Recognition%20Datasets%0AAuthor%3A%20Alexandre%20Fournier-Mongieux%20and%20Michael%20Soumm%20and%20Adrian%20Popescu%20and%20Bertrand%20Luvison%20and%20Herv%C3%A9%20Le%20Borgne%0AAbstract%3A%20%20%20Face%20recognition%20and%20verification%20are%20two%20computer%20vision%20tasks%20whose%0Aperformance%20has%20progressed%20with%20the%20introduction%20of%20deep%20representations.%0AHowever%2C%20ethical%2C%20legal%2C%20and%20technical%20challenges%20due%20to%20the%20sensitive%0Acharacter%20of%20face%20data%20and%20biases%20in%20real%20training%20datasets%20hinder%20their%0Adevelopment.%20Generative%20AI%20addresses%20privacy%20by%20creating%20fictitious%20identities%2C%0Abut%20fairness%20problems%20persist.%20We%20promote%20fairness%20by%20introducing%20a%20demographic%0Aattributes%20balancing%20mechanism%20in%20generated%20training%20datasets.%20We%20experiment%0Awith%20an%20existing%20real%20dataset%2C%20three%20generated%20training%20datasets%2C%20and%20the%0Abalanced%20versions%20of%20a%20diffusion-based%20dataset.%20We%20propose%20a%20comprehensive%0Aevaluation%20that%20considers%20accuracy%20and%20fairness%20equally%20and%20includes%20a%20rigorous%0Aregression-based%20statistical%20analysis%20of%20attributes.%20The%20analysis%20shows%20that%0Abalancing%20reduces%20demographic%20unfairness.%20Also%2C%20a%20performance%20gap%20persists%0Adespite%20generation%20becoming%20more%20accurate%20with%20time.%20The%20proposed%20balancing%0Amethod%20and%20comprehensive%20verification%20evaluation%20promote%20fairer%20and%20transparent%0Aface%20recognition%20and%20verification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Fairer%2520Face%2520Recognition%2520Datasets%26entry.906535625%3DAlexandre%2520Fournier-Mongieux%2520and%2520Michael%2520Soumm%2520and%2520Adrian%2520Popescu%2520and%2520Bertrand%2520Luvison%2520and%2520Herv%25C3%25A9%2520Le%2520Borgne%26entry.1292438233%3D%2520%2520Face%2520recognition%2520and%2520verification%2520are%2520two%2520computer%2520vision%2520tasks%2520whose%250Aperformance%2520has%2520progressed%2520with%2520the%2520introduction%2520of%2520deep%2520representations.%250AHowever%252C%2520ethical%252C%2520legal%252C%2520and%2520technical%2520challenges%2520due%2520to%2520the%2520sensitive%250Acharacter%2520of%2520face%2520data%2520and%2520biases%2520in%2520real%2520training%2520datasets%2520hinder%2520their%250Adevelopment.%2520Generative%2520AI%2520addresses%2520privacy%2520by%2520creating%2520fictitious%2520identities%252C%250Abut%2520fairness%2520problems%2520persist.%2520We%2520promote%2520fairness%2520by%2520introducing%2520a%2520demographic%250Aattributes%2520balancing%2520mechanism%2520in%2520generated%2520training%2520datasets.%2520We%2520experiment%250Awith%2520an%2520existing%2520real%2520dataset%252C%2520three%2520generated%2520training%2520datasets%252C%2520and%2520the%250Abalanced%2520versions%2520of%2520a%2520diffusion-based%2520dataset.%2520We%2520propose%2520a%2520comprehensive%250Aevaluation%2520that%2520considers%2520accuracy%2520and%2520fairness%2520equally%2520and%2520includes%2520a%2520rigorous%250Aregression-based%2520statistical%2520analysis%2520of%2520attributes.%2520The%2520analysis%2520shows%2520that%250Abalancing%2520reduces%2520demographic%2520unfairness.%2520Also%252C%2520a%2520performance%2520gap%2520persists%250Adespite%2520generation%2520becoming%2520more%2520accurate%2520with%2520time.%2520The%2520proposed%2520balancing%250Amethod%2520and%2520comprehensive%2520verification%2520evaluation%2520promote%2520fairer%2520and%2520transparent%250Aface%2520recognition%2520and%2520verification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Fairer%20Face%20Recognition%20Datasets&entry.906535625=Alexandre%20Fournier-Mongieux%20and%20Michael%20Soumm%20and%20Adrian%20Popescu%20and%20Bertrand%20Luvison%20and%20Herv%C3%A9%20Le%20Borgne&entry.1292438233=%20%20Face%20recognition%20and%20verification%20are%20two%20computer%20vision%20tasks%20whose%0Aperformance%20has%20progressed%20with%20the%20introduction%20of%20deep%20representations.%0AHowever%2C%20ethical%2C%20legal%2C%20and%20technical%20challenges%20due%20to%20the%20sensitive%0Acharacter%20of%20face%20data%20and%20biases%20in%20real%20training%20datasets%20hinder%20their%0Adevelopment.%20Generative%20AI%20addresses%20privacy%20by%20creating%20fictitious%20identities%2C%0Abut%20fairness%20problems%20persist.%20We%20promote%20fairness%20by%20introducing%20a%20demographic%0Aattributes%20balancing%20mechanism%20in%20generated%20training%20datasets.%20We%20experiment%0Awith%20an%20existing%20real%20dataset%2C%20three%20generated%20training%20datasets%2C%20and%20the%0Abalanced%20versions%20of%20a%20diffusion-based%20dataset.%20We%20propose%20a%20comprehensive%0Aevaluation%20that%20considers%20accuracy%20and%20fairness%20equally%20and%20includes%20a%20rigorous%0Aregression-based%20statistical%20analysis%20of%20attributes.%20The%20analysis%20shows%20that%0Abalancing%20reduces%20demographic%20unfairness.%20Also%2C%20a%20performance%20gap%20persists%0Adespite%20generation%20becoming%20more%20accurate%20with%20time.%20The%20proposed%20balancing%0Amethod%20and%20comprehensive%20verification%20evaluation%20promote%20fairer%20and%20transparent%0Aface%20recognition%20and%20verification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16592v1&entry.124074799=Read"},
{"title": "PISTOL: Dataset Compilation Pipeline for Structural Unlearning of LLMs", "author": "Xinchi Qiu and William F. Shen and Yihong Chen and Nicola Cancedda and Pontus Stenetorp and Nicholas D. Lane", "abstract": "  Recently, machine unlearning, which seeks to erase specific data stored in\nthe pre-trained or fine-tuned models, has emerged as a crucial protective\nmeasure for LLMs. However, unlearning approaches for LLMs that have been\nconsidered thus far have focused on the removal of independent data points and\nhave not taken into account that the stored facts are logically connected to\none another and form an implicit knowledge graph. To facilitate the development\nof structural unlearning methods, which are essential for the practical\napplication of unlearning, we propose PISTOL, a pipeline for compiling\nmulti-scenario datasets for benchmarking structural LLM unlearning.\nAdditionally, leveraging sample datasets synthesized using PISTOL, we conducted\nbenchmarks with four distinct unlearning methods on both Llama2-7B and\nMistral-7B models. This analysis helps to illustrate the prevailing challenges\nin effectively and robustly removing highly inter-connected data, batched data,\nor data skewed towards a specific domain. It also highlights the choice of\npre-trained model can impact unlearning performance. This work not only\nadvances our understandings on the limitation of current LLMs unlearning\nmethods and proposes future research directions, but also provides a replicable\nframework for ongoing exploration and validation in the field.\n", "link": "http://arxiv.org/abs/2406.16810v1", "date": "2024-06-24", "relevancy": 2.0631, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5338}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5243}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PISTOL%3A%20Dataset%20Compilation%20Pipeline%20for%20Structural%20Unlearning%20of%20LLMs&body=Title%3A%20PISTOL%3A%20Dataset%20Compilation%20Pipeline%20for%20Structural%20Unlearning%20of%20LLMs%0AAuthor%3A%20Xinchi%20Qiu%20and%20William%20F.%20Shen%20and%20Yihong%20Chen%20and%20Nicola%20Cancedda%20and%20Pontus%20Stenetorp%20and%20Nicholas%20D.%20Lane%0AAbstract%3A%20%20%20Recently%2C%20machine%20unlearning%2C%20which%20seeks%20to%20erase%20specific%20data%20stored%20in%0Athe%20pre-trained%20or%20fine-tuned%20models%2C%20has%20emerged%20as%20a%20crucial%20protective%0Ameasure%20for%20LLMs.%20However%2C%20unlearning%20approaches%20for%20LLMs%20that%20have%20been%0Aconsidered%20thus%20far%20have%20focused%20on%20the%20removal%20of%20independent%20data%20points%20and%0Ahave%20not%20taken%20into%20account%20that%20the%20stored%20facts%20are%20logically%20connected%20to%0Aone%20another%20and%20form%20an%20implicit%20knowledge%20graph.%20To%20facilitate%20the%20development%0Aof%20structural%20unlearning%20methods%2C%20which%20are%20essential%20for%20the%20practical%0Aapplication%20of%20unlearning%2C%20we%20propose%20PISTOL%2C%20a%20pipeline%20for%20compiling%0Amulti-scenario%20datasets%20for%20benchmarking%20structural%20LLM%20unlearning.%0AAdditionally%2C%20leveraging%20sample%20datasets%20synthesized%20using%20PISTOL%2C%20we%20conducted%0Abenchmarks%20with%20four%20distinct%20unlearning%20methods%20on%20both%20Llama2-7B%20and%0AMistral-7B%20models.%20This%20analysis%20helps%20to%20illustrate%20the%20prevailing%20challenges%0Ain%20effectively%20and%20robustly%20removing%20highly%20inter-connected%20data%2C%20batched%20data%2C%0Aor%20data%20skewed%20towards%20a%20specific%20domain.%20It%20also%20highlights%20the%20choice%20of%0Apre-trained%20model%20can%20impact%20unlearning%20performance.%20This%20work%20not%20only%0Aadvances%20our%20understandings%20on%20the%20limitation%20of%20current%20LLMs%20unlearning%0Amethods%20and%20proposes%20future%20research%20directions%2C%20but%20also%20provides%20a%20replicable%0Aframework%20for%20ongoing%20exploration%20and%20validation%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPISTOL%253A%2520Dataset%2520Compilation%2520Pipeline%2520for%2520Structural%2520Unlearning%2520of%2520LLMs%26entry.906535625%3DXinchi%2520Qiu%2520and%2520William%2520F.%2520Shen%2520and%2520Yihong%2520Chen%2520and%2520Nicola%2520Cancedda%2520and%2520Pontus%2520Stenetorp%2520and%2520Nicholas%2520D.%2520Lane%26entry.1292438233%3D%2520%2520Recently%252C%2520machine%2520unlearning%252C%2520which%2520seeks%2520to%2520erase%2520specific%2520data%2520stored%2520in%250Athe%2520pre-trained%2520or%2520fine-tuned%2520models%252C%2520has%2520emerged%2520as%2520a%2520crucial%2520protective%250Ameasure%2520for%2520LLMs.%2520However%252C%2520unlearning%2520approaches%2520for%2520LLMs%2520that%2520have%2520been%250Aconsidered%2520thus%2520far%2520have%2520focused%2520on%2520the%2520removal%2520of%2520independent%2520data%2520points%2520and%250Ahave%2520not%2520taken%2520into%2520account%2520that%2520the%2520stored%2520facts%2520are%2520logically%2520connected%2520to%250Aone%2520another%2520and%2520form%2520an%2520implicit%2520knowledge%2520graph.%2520To%2520facilitate%2520the%2520development%250Aof%2520structural%2520unlearning%2520methods%252C%2520which%2520are%2520essential%2520for%2520the%2520practical%250Aapplication%2520of%2520unlearning%252C%2520we%2520propose%2520PISTOL%252C%2520a%2520pipeline%2520for%2520compiling%250Amulti-scenario%2520datasets%2520for%2520benchmarking%2520structural%2520LLM%2520unlearning.%250AAdditionally%252C%2520leveraging%2520sample%2520datasets%2520synthesized%2520using%2520PISTOL%252C%2520we%2520conducted%250Abenchmarks%2520with%2520four%2520distinct%2520unlearning%2520methods%2520on%2520both%2520Llama2-7B%2520and%250AMistral-7B%2520models.%2520This%2520analysis%2520helps%2520to%2520illustrate%2520the%2520prevailing%2520challenges%250Ain%2520effectively%2520and%2520robustly%2520removing%2520highly%2520inter-connected%2520data%252C%2520batched%2520data%252C%250Aor%2520data%2520skewed%2520towards%2520a%2520specific%2520domain.%2520It%2520also%2520highlights%2520the%2520choice%2520of%250Apre-trained%2520model%2520can%2520impact%2520unlearning%2520performance.%2520This%2520work%2520not%2520only%250Aadvances%2520our%2520understandings%2520on%2520the%2520limitation%2520of%2520current%2520LLMs%2520unlearning%250Amethods%2520and%2520proposes%2520future%2520research%2520directions%252C%2520but%2520also%2520provides%2520a%2520replicable%250Aframework%2520for%2520ongoing%2520exploration%2520and%2520validation%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PISTOL%3A%20Dataset%20Compilation%20Pipeline%20for%20Structural%20Unlearning%20of%20LLMs&entry.906535625=Xinchi%20Qiu%20and%20William%20F.%20Shen%20and%20Yihong%20Chen%20and%20Nicola%20Cancedda%20and%20Pontus%20Stenetorp%20and%20Nicholas%20D.%20Lane&entry.1292438233=%20%20Recently%2C%20machine%20unlearning%2C%20which%20seeks%20to%20erase%20specific%20data%20stored%20in%0Athe%20pre-trained%20or%20fine-tuned%20models%2C%20has%20emerged%20as%20a%20crucial%20protective%0Ameasure%20for%20LLMs.%20However%2C%20unlearning%20approaches%20for%20LLMs%20that%20have%20been%0Aconsidered%20thus%20far%20have%20focused%20on%20the%20removal%20of%20independent%20data%20points%20and%0Ahave%20not%20taken%20into%20account%20that%20the%20stored%20facts%20are%20logically%20connected%20to%0Aone%20another%20and%20form%20an%20implicit%20knowledge%20graph.%20To%20facilitate%20the%20development%0Aof%20structural%20unlearning%20methods%2C%20which%20are%20essential%20for%20the%20practical%0Aapplication%20of%20unlearning%2C%20we%20propose%20PISTOL%2C%20a%20pipeline%20for%20compiling%0Amulti-scenario%20datasets%20for%20benchmarking%20structural%20LLM%20unlearning.%0AAdditionally%2C%20leveraging%20sample%20datasets%20synthesized%20using%20PISTOL%2C%20we%20conducted%0Abenchmarks%20with%20four%20distinct%20unlearning%20methods%20on%20both%20Llama2-7B%20and%0AMistral-7B%20models.%20This%20analysis%20helps%20to%20illustrate%20the%20prevailing%20challenges%0Ain%20effectively%20and%20robustly%20removing%20highly%20inter-connected%20data%2C%20batched%20data%2C%0Aor%20data%20skewed%20towards%20a%20specific%20domain.%20It%20also%20highlights%20the%20choice%20of%0Apre-trained%20model%20can%20impact%20unlearning%20performance.%20This%20work%20not%20only%0Aadvances%20our%20understandings%20on%20the%20limitation%20of%20current%20LLMs%20unlearning%0Amethods%20and%20proposes%20future%20research%20directions%2C%20but%20also%20provides%20a%20replicable%0Aframework%20for%20ongoing%20exploration%20and%20validation%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16810v1&entry.124074799=Read"},
{"title": "Improving robustness to corruptions with multiplicative weight\n  perturbations", "author": "Trung Trinh and Markus Heinonen and Luigi Acerbi and Samuel Kaski", "abstract": "  Deep neural networks (DNNs) excel on clean images but struggle with corrupted\nones. Incorporating specific corruptions into the data augmentation pipeline\ncan improve robustness to those corruptions but may harm performance on clean\nimages and other types of distortion. In this paper, we introduce an\nalternative approach that improves the robustness of DNNs to a wide range of\ncorruptions without compromising accuracy on clean images. We first demonstrate\nthat input perturbations can be mimicked by multiplicative perturbations in the\nweight space. Leveraging this, we propose Data Augmentation via Multiplicative\nPerturbation (DAMP), a training method that optimizes DNNs under random\nmultiplicative weight perturbations. We also examine the recently proposed\nAdaptive Sharpness-Aware Minimization (ASAM) and show that it optimizes DNNs\nunder adversarial multiplicative weight perturbations. Experiments on image\nclassification datasets (CIFAR-10/100, TinyImageNet and ImageNet) and neural\nnetwork architectures (ResNet50, ViT-S/16) show that DAMP enhances model\ngeneralization performance in the presence of corruptions across different\nsettings. Notably, DAMP is able to train a ViT-S/16 on ImageNet from scratch,\nreaching the top-1 error of 23.7% which is comparable to ResNet50 without\nextensive data augmentations.\n", "link": "http://arxiv.org/abs/2406.16540v1", "date": "2024-06-24", "relevancy": 2.0606, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5204}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5194}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20robustness%20to%20corruptions%20with%20multiplicative%20weight%0A%20%20perturbations&body=Title%3A%20Improving%20robustness%20to%20corruptions%20with%20multiplicative%20weight%0A%20%20perturbations%0AAuthor%3A%20Trung%20Trinh%20and%20Markus%20Heinonen%20and%20Luigi%20Acerbi%20and%20Samuel%20Kaski%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20excel%20on%20clean%20images%20but%20struggle%20with%20corrupted%0Aones.%20Incorporating%20specific%20corruptions%20into%20the%20data%20augmentation%20pipeline%0Acan%20improve%20robustness%20to%20those%20corruptions%20but%20may%20harm%20performance%20on%20clean%0Aimages%20and%20other%20types%20of%20distortion.%20In%20this%20paper%2C%20we%20introduce%20an%0Aalternative%20approach%20that%20improves%20the%20robustness%20of%20DNNs%20to%20a%20wide%20range%20of%0Acorruptions%20without%20compromising%20accuracy%20on%20clean%20images.%20We%20first%20demonstrate%0Athat%20input%20perturbations%20can%20be%20mimicked%20by%20multiplicative%20perturbations%20in%20the%0Aweight%20space.%20Leveraging%20this%2C%20we%20propose%20Data%20Augmentation%20via%20Multiplicative%0APerturbation%20%28DAMP%29%2C%20a%20training%20method%20that%20optimizes%20DNNs%20under%20random%0Amultiplicative%20weight%20perturbations.%20We%20also%20examine%20the%20recently%20proposed%0AAdaptive%20Sharpness-Aware%20Minimization%20%28ASAM%29%20and%20show%20that%20it%20optimizes%20DNNs%0Aunder%20adversarial%20multiplicative%20weight%20perturbations.%20Experiments%20on%20image%0Aclassification%20datasets%20%28CIFAR-10/100%2C%20TinyImageNet%20and%20ImageNet%29%20and%20neural%0Anetwork%20architectures%20%28ResNet50%2C%20ViT-S/16%29%20show%20that%20DAMP%20enhances%20model%0Ageneralization%20performance%20in%20the%20presence%20of%20corruptions%20across%20different%0Asettings.%20Notably%2C%20DAMP%20is%20able%20to%20train%20a%20ViT-S/16%20on%20ImageNet%20from%20scratch%2C%0Areaching%20the%20top-1%20error%20of%2023.7%25%20which%20is%20comparable%20to%20ResNet50%20without%0Aextensive%20data%20augmentations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520robustness%2520to%2520corruptions%2520with%2520multiplicative%2520weight%250A%2520%2520perturbations%26entry.906535625%3DTrung%2520Trinh%2520and%2520Markus%2520Heinonen%2520and%2520Luigi%2520Acerbi%2520and%2520Samuel%2520Kaski%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520excel%2520on%2520clean%2520images%2520but%2520struggle%2520with%2520corrupted%250Aones.%2520Incorporating%2520specific%2520corruptions%2520into%2520the%2520data%2520augmentation%2520pipeline%250Acan%2520improve%2520robustness%2520to%2520those%2520corruptions%2520but%2520may%2520harm%2520performance%2520on%2520clean%250Aimages%2520and%2520other%2520types%2520of%2520distortion.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520an%250Aalternative%2520approach%2520that%2520improves%2520the%2520robustness%2520of%2520DNNs%2520to%2520a%2520wide%2520range%2520of%250Acorruptions%2520without%2520compromising%2520accuracy%2520on%2520clean%2520images.%2520We%2520first%2520demonstrate%250Athat%2520input%2520perturbations%2520can%2520be%2520mimicked%2520by%2520multiplicative%2520perturbations%2520in%2520the%250Aweight%2520space.%2520Leveraging%2520this%252C%2520we%2520propose%2520Data%2520Augmentation%2520via%2520Multiplicative%250APerturbation%2520%2528DAMP%2529%252C%2520a%2520training%2520method%2520that%2520optimizes%2520DNNs%2520under%2520random%250Amultiplicative%2520weight%2520perturbations.%2520We%2520also%2520examine%2520the%2520recently%2520proposed%250AAdaptive%2520Sharpness-Aware%2520Minimization%2520%2528ASAM%2529%2520and%2520show%2520that%2520it%2520optimizes%2520DNNs%250Aunder%2520adversarial%2520multiplicative%2520weight%2520perturbations.%2520Experiments%2520on%2520image%250Aclassification%2520datasets%2520%2528CIFAR-10/100%252C%2520TinyImageNet%2520and%2520ImageNet%2529%2520and%2520neural%250Anetwork%2520architectures%2520%2528ResNet50%252C%2520ViT-S/16%2529%2520show%2520that%2520DAMP%2520enhances%2520model%250Ageneralization%2520performance%2520in%2520the%2520presence%2520of%2520corruptions%2520across%2520different%250Asettings.%2520Notably%252C%2520DAMP%2520is%2520able%2520to%2520train%2520a%2520ViT-S/16%2520on%2520ImageNet%2520from%2520scratch%252C%250Areaching%2520the%2520top-1%2520error%2520of%252023.7%2525%2520which%2520is%2520comparable%2520to%2520ResNet50%2520without%250Aextensive%2520data%2520augmentations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20robustness%20to%20corruptions%20with%20multiplicative%20weight%0A%20%20perturbations&entry.906535625=Trung%20Trinh%20and%20Markus%20Heinonen%20and%20Luigi%20Acerbi%20and%20Samuel%20Kaski&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20excel%20on%20clean%20images%20but%20struggle%20with%20corrupted%0Aones.%20Incorporating%20specific%20corruptions%20into%20the%20data%20augmentation%20pipeline%0Acan%20improve%20robustness%20to%20those%20corruptions%20but%20may%20harm%20performance%20on%20clean%0Aimages%20and%20other%20types%20of%20distortion.%20In%20this%20paper%2C%20we%20introduce%20an%0Aalternative%20approach%20that%20improves%20the%20robustness%20of%20DNNs%20to%20a%20wide%20range%20of%0Acorruptions%20without%20compromising%20accuracy%20on%20clean%20images.%20We%20first%20demonstrate%0Athat%20input%20perturbations%20can%20be%20mimicked%20by%20multiplicative%20perturbations%20in%20the%0Aweight%20space.%20Leveraging%20this%2C%20we%20propose%20Data%20Augmentation%20via%20Multiplicative%0APerturbation%20%28DAMP%29%2C%20a%20training%20method%20that%20optimizes%20DNNs%20under%20random%0Amultiplicative%20weight%20perturbations.%20We%20also%20examine%20the%20recently%20proposed%0AAdaptive%20Sharpness-Aware%20Minimization%20%28ASAM%29%20and%20show%20that%20it%20optimizes%20DNNs%0Aunder%20adversarial%20multiplicative%20weight%20perturbations.%20Experiments%20on%20image%0Aclassification%20datasets%20%28CIFAR-10/100%2C%20TinyImageNet%20and%20ImageNet%29%20and%20neural%0Anetwork%20architectures%20%28ResNet50%2C%20ViT-S/16%29%20show%20that%20DAMP%20enhances%20model%0Ageneralization%20performance%20in%20the%20presence%20of%20corruptions%20across%20different%0Asettings.%20Notably%2C%20DAMP%20is%20able%20to%20train%20a%20ViT-S/16%20on%20ImageNet%20from%20scratch%2C%0Areaching%20the%20top-1%20error%20of%2023.7%25%20which%20is%20comparable%20to%20ResNet50%20without%0Aextensive%20data%20augmentations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16540v1&entry.124074799=Read"},
{"title": "Public Constitutional AI", "author": "Gilad Abiri", "abstract": "  We are increasingly subjected to the power of AI authorities. As AI decisions\nbecome inescapable, entering domains such as healthcare, education, and law, we\nmust confront a vital question: how can we ensure AI systems have the\nlegitimacy necessary for effective governance? This essay argues that to secure\nAI legitimacy, we need methods that engage the public in designing and\nconstraining AI systems, ensuring these technologies reflect the community's\nshared values. Constitutional AI, proposed by Anthropic, represents a step\ntowards this goal, offering a model for democratic control of AI. However,\nwhile Constitutional AI's commitment to hardcoding explicit principles into AI\nmodels enhances transparency and accountability, it falls short in two crucial\naspects: addressing the opacity of individual AI decisions and fostering\ngenuine democratic legitimacy. To overcome these limitations, this essay\nproposes \"Public Constitutional AI.\" This approach envisions a participatory\nprocess where diverse stakeholders, including ordinary citizens, deliberate on\nthe principles guiding AI development. The resulting \"AI Constitution\" would\ncarry the legitimacy of popular authorship, grounding AI governance in the\npublic will. Furthermore, the essay proposes \"AI Courts\" to develop \"AI case\nlaw,\" providing concrete examples for operationalizing constitutional\nprinciples in AI training. This evolving combination of constitutional\nprinciples and case law aims to make AI governance more responsive to public\nvalues. By grounding AI governance in deliberative democratic processes, Public\nConstitutional AI offers a path to imbue automated authorities with genuine\ndemocratic legitimacy, addressing the unique challenges posed by increasingly\npowerful AI systems while ensuring their alignment with the public interest.\n", "link": "http://arxiv.org/abs/2406.16696v1", "date": "2024-06-24", "relevancy": 2.0591, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4216}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4212}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.3927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Public%20Constitutional%20AI&body=Title%3A%20Public%20Constitutional%20AI%0AAuthor%3A%20Gilad%20Abiri%0AAbstract%3A%20%20%20We%20are%20increasingly%20subjected%20to%20the%20power%20of%20AI%20authorities.%20As%20AI%20decisions%0Abecome%20inescapable%2C%20entering%20domains%20such%20as%20healthcare%2C%20education%2C%20and%20law%2C%20we%0Amust%20confront%20a%20vital%20question%3A%20how%20can%20we%20ensure%20AI%20systems%20have%20the%0Alegitimacy%20necessary%20for%20effective%20governance%3F%20This%20essay%20argues%20that%20to%20secure%0AAI%20legitimacy%2C%20we%20need%20methods%20that%20engage%20the%20public%20in%20designing%20and%0Aconstraining%20AI%20systems%2C%20ensuring%20these%20technologies%20reflect%20the%20community%27s%0Ashared%20values.%20Constitutional%20AI%2C%20proposed%20by%20Anthropic%2C%20represents%20a%20step%0Atowards%20this%20goal%2C%20offering%20a%20model%20for%20democratic%20control%20of%20AI.%20However%2C%0Awhile%20Constitutional%20AI%27s%20commitment%20to%20hardcoding%20explicit%20principles%20into%20AI%0Amodels%20enhances%20transparency%20and%20accountability%2C%20it%20falls%20short%20in%20two%20crucial%0Aaspects%3A%20addressing%20the%20opacity%20of%20individual%20AI%20decisions%20and%20fostering%0Agenuine%20democratic%20legitimacy.%20To%20overcome%20these%20limitations%2C%20this%20essay%0Aproposes%20%22Public%20Constitutional%20AI.%22%20This%20approach%20envisions%20a%20participatory%0Aprocess%20where%20diverse%20stakeholders%2C%20including%20ordinary%20citizens%2C%20deliberate%20on%0Athe%20principles%20guiding%20AI%20development.%20The%20resulting%20%22AI%20Constitution%22%20would%0Acarry%20the%20legitimacy%20of%20popular%20authorship%2C%20grounding%20AI%20governance%20in%20the%0Apublic%20will.%20Furthermore%2C%20the%20essay%20proposes%20%22AI%20Courts%22%20to%20develop%20%22AI%20case%0Alaw%2C%22%20providing%20concrete%20examples%20for%20operationalizing%20constitutional%0Aprinciples%20in%20AI%20training.%20This%20evolving%20combination%20of%20constitutional%0Aprinciples%20and%20case%20law%20aims%20to%20make%20AI%20governance%20more%20responsive%20to%20public%0Avalues.%20By%20grounding%20AI%20governance%20in%20deliberative%20democratic%20processes%2C%20Public%0AConstitutional%20AI%20offers%20a%20path%20to%20imbue%20automated%20authorities%20with%20genuine%0Ademocratic%20legitimacy%2C%20addressing%20the%20unique%20challenges%20posed%20by%20increasingly%0Apowerful%20AI%20systems%20while%20ensuring%20their%20alignment%20with%20the%20public%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPublic%2520Constitutional%2520AI%26entry.906535625%3DGilad%2520Abiri%26entry.1292438233%3D%2520%2520We%2520are%2520increasingly%2520subjected%2520to%2520the%2520power%2520of%2520AI%2520authorities.%2520As%2520AI%2520decisions%250Abecome%2520inescapable%252C%2520entering%2520domains%2520such%2520as%2520healthcare%252C%2520education%252C%2520and%2520law%252C%2520we%250Amust%2520confront%2520a%2520vital%2520question%253A%2520how%2520can%2520we%2520ensure%2520AI%2520systems%2520have%2520the%250Alegitimacy%2520necessary%2520for%2520effective%2520governance%253F%2520This%2520essay%2520argues%2520that%2520to%2520secure%250AAI%2520legitimacy%252C%2520we%2520need%2520methods%2520that%2520engage%2520the%2520public%2520in%2520designing%2520and%250Aconstraining%2520AI%2520systems%252C%2520ensuring%2520these%2520technologies%2520reflect%2520the%2520community%2527s%250Ashared%2520values.%2520Constitutional%2520AI%252C%2520proposed%2520by%2520Anthropic%252C%2520represents%2520a%2520step%250Atowards%2520this%2520goal%252C%2520offering%2520a%2520model%2520for%2520democratic%2520control%2520of%2520AI.%2520However%252C%250Awhile%2520Constitutional%2520AI%2527s%2520commitment%2520to%2520hardcoding%2520explicit%2520principles%2520into%2520AI%250Amodels%2520enhances%2520transparency%2520and%2520accountability%252C%2520it%2520falls%2520short%2520in%2520two%2520crucial%250Aaspects%253A%2520addressing%2520the%2520opacity%2520of%2520individual%2520AI%2520decisions%2520and%2520fostering%250Agenuine%2520democratic%2520legitimacy.%2520To%2520overcome%2520these%2520limitations%252C%2520this%2520essay%250Aproposes%2520%2522Public%2520Constitutional%2520AI.%2522%2520This%2520approach%2520envisions%2520a%2520participatory%250Aprocess%2520where%2520diverse%2520stakeholders%252C%2520including%2520ordinary%2520citizens%252C%2520deliberate%2520on%250Athe%2520principles%2520guiding%2520AI%2520development.%2520The%2520resulting%2520%2522AI%2520Constitution%2522%2520would%250Acarry%2520the%2520legitimacy%2520of%2520popular%2520authorship%252C%2520grounding%2520AI%2520governance%2520in%2520the%250Apublic%2520will.%2520Furthermore%252C%2520the%2520essay%2520proposes%2520%2522AI%2520Courts%2522%2520to%2520develop%2520%2522AI%2520case%250Alaw%252C%2522%2520providing%2520concrete%2520examples%2520for%2520operationalizing%2520constitutional%250Aprinciples%2520in%2520AI%2520training.%2520This%2520evolving%2520combination%2520of%2520constitutional%250Aprinciples%2520and%2520case%2520law%2520aims%2520to%2520make%2520AI%2520governance%2520more%2520responsive%2520to%2520public%250Avalues.%2520By%2520grounding%2520AI%2520governance%2520in%2520deliberative%2520democratic%2520processes%252C%2520Public%250AConstitutional%2520AI%2520offers%2520a%2520path%2520to%2520imbue%2520automated%2520authorities%2520with%2520genuine%250Ademocratic%2520legitimacy%252C%2520addressing%2520the%2520unique%2520challenges%2520posed%2520by%2520increasingly%250Apowerful%2520AI%2520systems%2520while%2520ensuring%2520their%2520alignment%2520with%2520the%2520public%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Public%20Constitutional%20AI&entry.906535625=Gilad%20Abiri&entry.1292438233=%20%20We%20are%20increasingly%20subjected%20to%20the%20power%20of%20AI%20authorities.%20As%20AI%20decisions%0Abecome%20inescapable%2C%20entering%20domains%20such%20as%20healthcare%2C%20education%2C%20and%20law%2C%20we%0Amust%20confront%20a%20vital%20question%3A%20how%20can%20we%20ensure%20AI%20systems%20have%20the%0Alegitimacy%20necessary%20for%20effective%20governance%3F%20This%20essay%20argues%20that%20to%20secure%0AAI%20legitimacy%2C%20we%20need%20methods%20that%20engage%20the%20public%20in%20designing%20and%0Aconstraining%20AI%20systems%2C%20ensuring%20these%20technologies%20reflect%20the%20community%27s%0Ashared%20values.%20Constitutional%20AI%2C%20proposed%20by%20Anthropic%2C%20represents%20a%20step%0Atowards%20this%20goal%2C%20offering%20a%20model%20for%20democratic%20control%20of%20AI.%20However%2C%0Awhile%20Constitutional%20AI%27s%20commitment%20to%20hardcoding%20explicit%20principles%20into%20AI%0Amodels%20enhances%20transparency%20and%20accountability%2C%20it%20falls%20short%20in%20two%20crucial%0Aaspects%3A%20addressing%20the%20opacity%20of%20individual%20AI%20decisions%20and%20fostering%0Agenuine%20democratic%20legitimacy.%20To%20overcome%20these%20limitations%2C%20this%20essay%0Aproposes%20%22Public%20Constitutional%20AI.%22%20This%20approach%20envisions%20a%20participatory%0Aprocess%20where%20diverse%20stakeholders%2C%20including%20ordinary%20citizens%2C%20deliberate%20on%0Athe%20principles%20guiding%20AI%20development.%20The%20resulting%20%22AI%20Constitution%22%20would%0Acarry%20the%20legitimacy%20of%20popular%20authorship%2C%20grounding%20AI%20governance%20in%20the%0Apublic%20will.%20Furthermore%2C%20the%20essay%20proposes%20%22AI%20Courts%22%20to%20develop%20%22AI%20case%0Alaw%2C%22%20providing%20concrete%20examples%20for%20operationalizing%20constitutional%0Aprinciples%20in%20AI%20training.%20This%20evolving%20combination%20of%20constitutional%0Aprinciples%20and%20case%20law%20aims%20to%20make%20AI%20governance%20more%20responsive%20to%20public%0Avalues.%20By%20grounding%20AI%20governance%20in%20deliberative%20democratic%20processes%2C%20Public%0AConstitutional%20AI%20offers%20a%20path%20to%20imbue%20automated%20authorities%20with%20genuine%0Ademocratic%20legitimacy%2C%20addressing%20the%20unique%20challenges%20posed%20by%20increasingly%0Apowerful%20AI%20systems%20while%20ensuring%20their%20alignment%20with%20the%20public%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16696v1&entry.124074799=Read"},
{"title": "When Invariant Representation Learning Meets Label Shift: Insufficiency\n  and Theoretical Insights", "author": "You-Wei Luo and Chuan-Xian Ren", "abstract": "  As a crucial step toward real-world learning scenarios with changing\nenvironments, dataset shift theory and invariant representation learning\nalgorithm have been extensively studied to relax the identical distribution\nassumption in classical learning setting. Among the different assumptions on\nthe essential of shifting distributions, generalized label shift (GLS) is the\nlatest developed one which shows great potential to deal with the complex\nfactors within the shift. In this paper, we aim to explore the limitations of\ncurrent dataset shift theory and algorithm, and further provide new insights by\npresenting a comprehensive understanding of GLS. From theoretical aspect, two\ninformative generalization bounds are derived, and the GLS learner is proved to\nbe sufficiently close to optimal target model from the Bayesian perspective.\nThe main results show the insufficiency of invariant representation learning,\nand prove the sufficiency and necessity of GLS correction for generalization,\nwhich provide theoretical supports and innovations for exploring generalizable\nmodel under dataset shift. From methodological aspect, we provide a unified\nview of existing shift correction frameworks, and propose a kernel\nembedding-based correction algorithm (KECA) to minimize the generalization\nerror and achieve successful knowledge transfer. Both theoretical results and\nextensive experiment evaluations demonstrate the sufficiency and necessity of\nGLS correction for addressing dataset shift and the superiority of proposed\nalgorithm.\n", "link": "http://arxiv.org/abs/2406.16608v1", "date": "2024-06-24", "relevancy": 2.0454, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5236}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5036}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Invariant%20Representation%20Learning%20Meets%20Label%20Shift%3A%20Insufficiency%0A%20%20and%20Theoretical%20Insights&body=Title%3A%20When%20Invariant%20Representation%20Learning%20Meets%20Label%20Shift%3A%20Insufficiency%0A%20%20and%20Theoretical%20Insights%0AAuthor%3A%20You-Wei%20Luo%20and%20Chuan-Xian%20Ren%0AAbstract%3A%20%20%20As%20a%20crucial%20step%20toward%20real-world%20learning%20scenarios%20with%20changing%0Aenvironments%2C%20dataset%20shift%20theory%20and%20invariant%20representation%20learning%0Aalgorithm%20have%20been%20extensively%20studied%20to%20relax%20the%20identical%20distribution%0Aassumption%20in%20classical%20learning%20setting.%20Among%20the%20different%20assumptions%20on%0Athe%20essential%20of%20shifting%20distributions%2C%20generalized%20label%20shift%20%28GLS%29%20is%20the%0Alatest%20developed%20one%20which%20shows%20great%20potential%20to%20deal%20with%20the%20complex%0Afactors%20within%20the%20shift.%20In%20this%20paper%2C%20we%20aim%20to%20explore%20the%20limitations%20of%0Acurrent%20dataset%20shift%20theory%20and%20algorithm%2C%20and%20further%20provide%20new%20insights%20by%0Apresenting%20a%20comprehensive%20understanding%20of%20GLS.%20From%20theoretical%20aspect%2C%20two%0Ainformative%20generalization%20bounds%20are%20derived%2C%20and%20the%20GLS%20learner%20is%20proved%20to%0Abe%20sufficiently%20close%20to%20optimal%20target%20model%20from%20the%20Bayesian%20perspective.%0AThe%20main%20results%20show%20the%20insufficiency%20of%20invariant%20representation%20learning%2C%0Aand%20prove%20the%20sufficiency%20and%20necessity%20of%20GLS%20correction%20for%20generalization%2C%0Awhich%20provide%20theoretical%20supports%20and%20innovations%20for%20exploring%20generalizable%0Amodel%20under%20dataset%20shift.%20From%20methodological%20aspect%2C%20we%20provide%20a%20unified%0Aview%20of%20existing%20shift%20correction%20frameworks%2C%20and%20propose%20a%20kernel%0Aembedding-based%20correction%20algorithm%20%28KECA%29%20to%20minimize%20the%20generalization%0Aerror%20and%20achieve%20successful%20knowledge%20transfer.%20Both%20theoretical%20results%20and%0Aextensive%20experiment%20evaluations%20demonstrate%20the%20sufficiency%20and%20necessity%20of%0AGLS%20correction%20for%20addressing%20dataset%20shift%20and%20the%20superiority%20of%20proposed%0Aalgorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Invariant%2520Representation%2520Learning%2520Meets%2520Label%2520Shift%253A%2520Insufficiency%250A%2520%2520and%2520Theoretical%2520Insights%26entry.906535625%3DYou-Wei%2520Luo%2520and%2520Chuan-Xian%2520Ren%26entry.1292438233%3D%2520%2520As%2520a%2520crucial%2520step%2520toward%2520real-world%2520learning%2520scenarios%2520with%2520changing%250Aenvironments%252C%2520dataset%2520shift%2520theory%2520and%2520invariant%2520representation%2520learning%250Aalgorithm%2520have%2520been%2520extensively%2520studied%2520to%2520relax%2520the%2520identical%2520distribution%250Aassumption%2520in%2520classical%2520learning%2520setting.%2520Among%2520the%2520different%2520assumptions%2520on%250Athe%2520essential%2520of%2520shifting%2520distributions%252C%2520generalized%2520label%2520shift%2520%2528GLS%2529%2520is%2520the%250Alatest%2520developed%2520one%2520which%2520shows%2520great%2520potential%2520to%2520deal%2520with%2520the%2520complex%250Afactors%2520within%2520the%2520shift.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520explore%2520the%2520limitations%2520of%250Acurrent%2520dataset%2520shift%2520theory%2520and%2520algorithm%252C%2520and%2520further%2520provide%2520new%2520insights%2520by%250Apresenting%2520a%2520comprehensive%2520understanding%2520of%2520GLS.%2520From%2520theoretical%2520aspect%252C%2520two%250Ainformative%2520generalization%2520bounds%2520are%2520derived%252C%2520and%2520the%2520GLS%2520learner%2520is%2520proved%2520to%250Abe%2520sufficiently%2520close%2520to%2520optimal%2520target%2520model%2520from%2520the%2520Bayesian%2520perspective.%250AThe%2520main%2520results%2520show%2520the%2520insufficiency%2520of%2520invariant%2520representation%2520learning%252C%250Aand%2520prove%2520the%2520sufficiency%2520and%2520necessity%2520of%2520GLS%2520correction%2520for%2520generalization%252C%250Awhich%2520provide%2520theoretical%2520supports%2520and%2520innovations%2520for%2520exploring%2520generalizable%250Amodel%2520under%2520dataset%2520shift.%2520From%2520methodological%2520aspect%252C%2520we%2520provide%2520a%2520unified%250Aview%2520of%2520existing%2520shift%2520correction%2520frameworks%252C%2520and%2520propose%2520a%2520kernel%250Aembedding-based%2520correction%2520algorithm%2520%2528KECA%2529%2520to%2520minimize%2520the%2520generalization%250Aerror%2520and%2520achieve%2520successful%2520knowledge%2520transfer.%2520Both%2520theoretical%2520results%2520and%250Aextensive%2520experiment%2520evaluations%2520demonstrate%2520the%2520sufficiency%2520and%2520necessity%2520of%250AGLS%2520correction%2520for%2520addressing%2520dataset%2520shift%2520and%2520the%2520superiority%2520of%2520proposed%250Aalgorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Invariant%20Representation%20Learning%20Meets%20Label%20Shift%3A%20Insufficiency%0A%20%20and%20Theoretical%20Insights&entry.906535625=You-Wei%20Luo%20and%20Chuan-Xian%20Ren&entry.1292438233=%20%20As%20a%20crucial%20step%20toward%20real-world%20learning%20scenarios%20with%20changing%0Aenvironments%2C%20dataset%20shift%20theory%20and%20invariant%20representation%20learning%0Aalgorithm%20have%20been%20extensively%20studied%20to%20relax%20the%20identical%20distribution%0Aassumption%20in%20classical%20learning%20setting.%20Among%20the%20different%20assumptions%20on%0Athe%20essential%20of%20shifting%20distributions%2C%20generalized%20label%20shift%20%28GLS%29%20is%20the%0Alatest%20developed%20one%20which%20shows%20great%20potential%20to%20deal%20with%20the%20complex%0Afactors%20within%20the%20shift.%20In%20this%20paper%2C%20we%20aim%20to%20explore%20the%20limitations%20of%0Acurrent%20dataset%20shift%20theory%20and%20algorithm%2C%20and%20further%20provide%20new%20insights%20by%0Apresenting%20a%20comprehensive%20understanding%20of%20GLS.%20From%20theoretical%20aspect%2C%20two%0Ainformative%20generalization%20bounds%20are%20derived%2C%20and%20the%20GLS%20learner%20is%20proved%20to%0Abe%20sufficiently%20close%20to%20optimal%20target%20model%20from%20the%20Bayesian%20perspective.%0AThe%20main%20results%20show%20the%20insufficiency%20of%20invariant%20representation%20learning%2C%0Aand%20prove%20the%20sufficiency%20and%20necessity%20of%20GLS%20correction%20for%20generalization%2C%0Awhich%20provide%20theoretical%20supports%20and%20innovations%20for%20exploring%20generalizable%0Amodel%20under%20dataset%20shift.%20From%20methodological%20aspect%2C%20we%20provide%20a%20unified%0Aview%20of%20existing%20shift%20correction%20frameworks%2C%20and%20propose%20a%20kernel%0Aembedding-based%20correction%20algorithm%20%28KECA%29%20to%20minimize%20the%20generalization%0Aerror%20and%20achieve%20successful%20knowledge%20transfer.%20Both%20theoretical%20results%20and%0Aextensive%20experiment%20evaluations%20demonstrate%20the%20sufficiency%20and%20necessity%20of%0AGLS%20correction%20for%20addressing%20dataset%20shift%20and%20the%20superiority%20of%20proposed%0Aalgorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16608v1&entry.124074799=Read"},
{"title": "Vision Mamba-based autonomous crack segmentation on concrete, asphalt,\n  and masonry surfaces", "author": "Zhaohui Chen and Elyas Asadi Shamsabadi and Sheng Jiang and Luming Shen and Daniel Dias-da-Costa", "abstract": "  Convolutional neural networks (CNNs) and Transformers have shown advanced\naccuracy in crack detection under certain conditions. Yet, the fixed local\nattention can compromise the generalisation of CNNs, and the quadratic\ncomplexity of the global self-attention restricts the practical deployment of\nTransformers. Given the emergence of the new-generation architecture of Mamba,\nthis paper proposes a Vision Mamba (VMamba)-based framework for crack\nsegmentation on concrete, asphalt, and masonry surfaces, with high accuracy,\ngeneralisation, and less computational complexity. Having 15.6% - 74.5% fewer\nparameters, the encoder-decoder network integrated with VMamba could obtain up\nto 2.8% higher mDS than representative CNN-based models while showing about the\nsame performance as Transformer-based models. Moreover, the VMamba-based\nencoder-decoder network could process high-resolution image input with up to\n90.6% lower floating-point operations.\n", "link": "http://arxiv.org/abs/2406.16518v1", "date": "2024-06-24", "relevancy": 2.0191, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5234}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.511}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Mamba-based%20autonomous%20crack%20segmentation%20on%20concrete%2C%20asphalt%2C%0A%20%20and%20masonry%20surfaces&body=Title%3A%20Vision%20Mamba-based%20autonomous%20crack%20segmentation%20on%20concrete%2C%20asphalt%2C%0A%20%20and%20masonry%20surfaces%0AAuthor%3A%20Zhaohui%20Chen%20and%20Elyas%20Asadi%20Shamsabadi%20and%20Sheng%20Jiang%20and%20Luming%20Shen%20and%20Daniel%20Dias-da-Costa%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNNs%29%20and%20Transformers%20have%20shown%20advanced%0Aaccuracy%20in%20crack%20detection%20under%20certain%20conditions.%20Yet%2C%20the%20fixed%20local%0Aattention%20can%20compromise%20the%20generalisation%20of%20CNNs%2C%20and%20the%20quadratic%0Acomplexity%20of%20the%20global%20self-attention%20restricts%20the%20practical%20deployment%20of%0ATransformers.%20Given%20the%20emergence%20of%20the%20new-generation%20architecture%20of%20Mamba%2C%0Athis%20paper%20proposes%20a%20Vision%20Mamba%20%28VMamba%29-based%20framework%20for%20crack%0Asegmentation%20on%20concrete%2C%20asphalt%2C%20and%20masonry%20surfaces%2C%20with%20high%20accuracy%2C%0Ageneralisation%2C%20and%20less%20computational%20complexity.%20Having%2015.6%25%20-%2074.5%25%20fewer%0Aparameters%2C%20the%20encoder-decoder%20network%20integrated%20with%20VMamba%20could%20obtain%20up%0Ato%202.8%25%20higher%20mDS%20than%20representative%20CNN-based%20models%20while%20showing%20about%20the%0Asame%20performance%20as%20Transformer-based%20models.%20Moreover%2C%20the%20VMamba-based%0Aencoder-decoder%20network%20could%20process%20high-resolution%20image%20input%20with%20up%20to%0A90.6%25%20lower%20floating-point%20operations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16518v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Mamba-based%2520autonomous%2520crack%2520segmentation%2520on%2520concrete%252C%2520asphalt%252C%250A%2520%2520and%2520masonry%2520surfaces%26entry.906535625%3DZhaohui%2520Chen%2520and%2520Elyas%2520Asadi%2520Shamsabadi%2520and%2520Sheng%2520Jiang%2520and%2520Luming%2520Shen%2520and%2520Daniel%2520Dias-da-Costa%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520and%2520Transformers%2520have%2520shown%2520advanced%250Aaccuracy%2520in%2520crack%2520detection%2520under%2520certain%2520conditions.%2520Yet%252C%2520the%2520fixed%2520local%250Aattention%2520can%2520compromise%2520the%2520generalisation%2520of%2520CNNs%252C%2520and%2520the%2520quadratic%250Acomplexity%2520of%2520the%2520global%2520self-attention%2520restricts%2520the%2520practical%2520deployment%2520of%250ATransformers.%2520Given%2520the%2520emergence%2520of%2520the%2520new-generation%2520architecture%2520of%2520Mamba%252C%250Athis%2520paper%2520proposes%2520a%2520Vision%2520Mamba%2520%2528VMamba%2529-based%2520framework%2520for%2520crack%250Asegmentation%2520on%2520concrete%252C%2520asphalt%252C%2520and%2520masonry%2520surfaces%252C%2520with%2520high%2520accuracy%252C%250Ageneralisation%252C%2520and%2520less%2520computational%2520complexity.%2520Having%252015.6%2525%2520-%252074.5%2525%2520fewer%250Aparameters%252C%2520the%2520encoder-decoder%2520network%2520integrated%2520with%2520VMamba%2520could%2520obtain%2520up%250Ato%25202.8%2525%2520higher%2520mDS%2520than%2520representative%2520CNN-based%2520models%2520while%2520showing%2520about%2520the%250Asame%2520performance%2520as%2520Transformer-based%2520models.%2520Moreover%252C%2520the%2520VMamba-based%250Aencoder-decoder%2520network%2520could%2520process%2520high-resolution%2520image%2520input%2520with%2520up%2520to%250A90.6%2525%2520lower%2520floating-point%2520operations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16518v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Mamba-based%20autonomous%20crack%20segmentation%20on%20concrete%2C%20asphalt%2C%0A%20%20and%20masonry%20surfaces&entry.906535625=Zhaohui%20Chen%20and%20Elyas%20Asadi%20Shamsabadi%20and%20Sheng%20Jiang%20and%20Luming%20Shen%20and%20Daniel%20Dias-da-Costa&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNNs%29%20and%20Transformers%20have%20shown%20advanced%0Aaccuracy%20in%20crack%20detection%20under%20certain%20conditions.%20Yet%2C%20the%20fixed%20local%0Aattention%20can%20compromise%20the%20generalisation%20of%20CNNs%2C%20and%20the%20quadratic%0Acomplexity%20of%20the%20global%20self-attention%20restricts%20the%20practical%20deployment%20of%0ATransformers.%20Given%20the%20emergence%20of%20the%20new-generation%20architecture%20of%20Mamba%2C%0Athis%20paper%20proposes%20a%20Vision%20Mamba%20%28VMamba%29-based%20framework%20for%20crack%0Asegmentation%20on%20concrete%2C%20asphalt%2C%20and%20masonry%20surfaces%2C%20with%20high%20accuracy%2C%0Ageneralisation%2C%20and%20less%20computational%20complexity.%20Having%2015.6%25%20-%2074.5%25%20fewer%0Aparameters%2C%20the%20encoder-decoder%20network%20integrated%20with%20VMamba%20could%20obtain%20up%0Ato%202.8%25%20higher%20mDS%20than%20representative%20CNN-based%20models%20while%20showing%20about%20the%0Asame%20performance%20as%20Transformer-based%20models.%20Moreover%2C%20the%20VMamba-based%0Aencoder-decoder%20network%20could%20process%20high-resolution%20image%20input%20with%20up%20to%0A90.6%25%20lower%20floating-point%20operations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16518v1&entry.124074799=Read"},
{"title": "Unsupervised Domain Adaptation for Pediatric Brain Tumor Segmentation", "author": "Jingru Fu and Simone Bendazzoli and \u00d6rjan Smedby and Rodrigo Moreno", "abstract": "  Significant advances have been made toward building accurate automatic\nsegmentation models for adult gliomas. However, the performance of these models\noften degrades when applied to pediatric glioma due to their imaging and\nclinical differences (domain shift). Obtaining sufficient annotated data for\npediatric glioma is typically difficult because of its rare nature. Also,\nmanual annotations are scarce and expensive. In this work, we propose\nDomain-Adapted nnU-Net (DA-nnUNet) to perform unsupervised domain adaptation\nfrom adult glioma (source domain) to pediatric glioma (target domain).\nSpecifically, we add a domain classifier connected with a gradient reversal\nlayer (GRL) to a backbone nnU-Net. Once the classifier reaches a very high\naccuracy, the GRL is activated with the goal of transferring domain-invariant\nfeatures from the classifier to the segmentation model while preserving\nsegmentation accuracy on the source domain. The accuracy of the classifier\nslowly degrades to chance levels. No annotations are used in the target domain.\nThe method is compared to 8 different supervised models using BraTS-Adult\nglioma (N=1251) and BraTS-PED glioma data (N=99). The proposed method shows\nnotable performance enhancements in the tumor core (TC) region compared to the\nmodel that only uses adult data: ~32% better Dice scores and ~20 better 95th\npercentile Hausdorff distances. Moreover, our unsupervised approach shows no\nstatistically significant difference compared to the practical upper bound\nmodel using manual annotations from both datasets in TC region. The code is\nshared at https://github.com/Fjr9516/DA_nnUNet.\n", "link": "http://arxiv.org/abs/2406.16848v1", "date": "2024-06-24", "relevancy": 2.0188, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5104}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5084}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Domain%20Adaptation%20for%20Pediatric%20Brain%20Tumor%20Segmentation&body=Title%3A%20Unsupervised%20Domain%20Adaptation%20for%20Pediatric%20Brain%20Tumor%20Segmentation%0AAuthor%3A%20Jingru%20Fu%20and%20Simone%20Bendazzoli%20and%20%C3%96rjan%20Smedby%20and%20Rodrigo%20Moreno%0AAbstract%3A%20%20%20Significant%20advances%20have%20been%20made%20toward%20building%20accurate%20automatic%0Asegmentation%20models%20for%20adult%20gliomas.%20However%2C%20the%20performance%20of%20these%20models%0Aoften%20degrades%20when%20applied%20to%20pediatric%20glioma%20due%20to%20their%20imaging%20and%0Aclinical%20differences%20%28domain%20shift%29.%20Obtaining%20sufficient%20annotated%20data%20for%0Apediatric%20glioma%20is%20typically%20difficult%20because%20of%20its%20rare%20nature.%20Also%2C%0Amanual%20annotations%20are%20scarce%20and%20expensive.%20In%20this%20work%2C%20we%20propose%0ADomain-Adapted%20nnU-Net%20%28DA-nnUNet%29%20to%20perform%20unsupervised%20domain%20adaptation%0Afrom%20adult%20glioma%20%28source%20domain%29%20to%20pediatric%20glioma%20%28target%20domain%29.%0ASpecifically%2C%20we%20add%20a%20domain%20classifier%20connected%20with%20a%20gradient%20reversal%0Alayer%20%28GRL%29%20to%20a%20backbone%20nnU-Net.%20Once%20the%20classifier%20reaches%20a%20very%20high%0Aaccuracy%2C%20the%20GRL%20is%20activated%20with%20the%20goal%20of%20transferring%20domain-invariant%0Afeatures%20from%20the%20classifier%20to%20the%20segmentation%20model%20while%20preserving%0Asegmentation%20accuracy%20on%20the%20source%20domain.%20The%20accuracy%20of%20the%20classifier%0Aslowly%20degrades%20to%20chance%20levels.%20No%20annotations%20are%20used%20in%20the%20target%20domain.%0AThe%20method%20is%20compared%20to%208%20different%20supervised%20models%20using%20BraTS-Adult%0Aglioma%20%28N%3D1251%29%20and%20BraTS-PED%20glioma%20data%20%28N%3D99%29.%20The%20proposed%20method%20shows%0Anotable%20performance%20enhancements%20in%20the%20tumor%20core%20%28TC%29%20region%20compared%20to%20the%0Amodel%20that%20only%20uses%20adult%20data%3A%20~32%25%20better%20Dice%20scores%20and%20~20%20better%2095th%0Apercentile%20Hausdorff%20distances.%20Moreover%2C%20our%20unsupervised%20approach%20shows%20no%0Astatistically%20significant%20difference%20compared%20to%20the%20practical%20upper%20bound%0Amodel%20using%20manual%20annotations%20from%20both%20datasets%20in%20TC%20region.%20The%20code%20is%0Ashared%20at%20https%3A//github.com/Fjr9516/DA_nnUNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16848v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Domain%2520Adaptation%2520for%2520Pediatric%2520Brain%2520Tumor%2520Segmentation%26entry.906535625%3DJingru%2520Fu%2520and%2520Simone%2520Bendazzoli%2520and%2520%25C3%2596rjan%2520Smedby%2520and%2520Rodrigo%2520Moreno%26entry.1292438233%3D%2520%2520Significant%2520advances%2520have%2520been%2520made%2520toward%2520building%2520accurate%2520automatic%250Asegmentation%2520models%2520for%2520adult%2520gliomas.%2520However%252C%2520the%2520performance%2520of%2520these%2520models%250Aoften%2520degrades%2520when%2520applied%2520to%2520pediatric%2520glioma%2520due%2520to%2520their%2520imaging%2520and%250Aclinical%2520differences%2520%2528domain%2520shift%2529.%2520Obtaining%2520sufficient%2520annotated%2520data%2520for%250Apediatric%2520glioma%2520is%2520typically%2520difficult%2520because%2520of%2520its%2520rare%2520nature.%2520Also%252C%250Amanual%2520annotations%2520are%2520scarce%2520and%2520expensive.%2520In%2520this%2520work%252C%2520we%2520propose%250ADomain-Adapted%2520nnU-Net%2520%2528DA-nnUNet%2529%2520to%2520perform%2520unsupervised%2520domain%2520adaptation%250Afrom%2520adult%2520glioma%2520%2528source%2520domain%2529%2520to%2520pediatric%2520glioma%2520%2528target%2520domain%2529.%250ASpecifically%252C%2520we%2520add%2520a%2520domain%2520classifier%2520connected%2520with%2520a%2520gradient%2520reversal%250Alayer%2520%2528GRL%2529%2520to%2520a%2520backbone%2520nnU-Net.%2520Once%2520the%2520classifier%2520reaches%2520a%2520very%2520high%250Aaccuracy%252C%2520the%2520GRL%2520is%2520activated%2520with%2520the%2520goal%2520of%2520transferring%2520domain-invariant%250Afeatures%2520from%2520the%2520classifier%2520to%2520the%2520segmentation%2520model%2520while%2520preserving%250Asegmentation%2520accuracy%2520on%2520the%2520source%2520domain.%2520The%2520accuracy%2520of%2520the%2520classifier%250Aslowly%2520degrades%2520to%2520chance%2520levels.%2520No%2520annotations%2520are%2520used%2520in%2520the%2520target%2520domain.%250AThe%2520method%2520is%2520compared%2520to%25208%2520different%2520supervised%2520models%2520using%2520BraTS-Adult%250Aglioma%2520%2528N%253D1251%2529%2520and%2520BraTS-PED%2520glioma%2520data%2520%2528N%253D99%2529.%2520The%2520proposed%2520method%2520shows%250Anotable%2520performance%2520enhancements%2520in%2520the%2520tumor%2520core%2520%2528TC%2529%2520region%2520compared%2520to%2520the%250Amodel%2520that%2520only%2520uses%2520adult%2520data%253A%2520~32%2525%2520better%2520Dice%2520scores%2520and%2520~20%2520better%252095th%250Apercentile%2520Hausdorff%2520distances.%2520Moreover%252C%2520our%2520unsupervised%2520approach%2520shows%2520no%250Astatistically%2520significant%2520difference%2520compared%2520to%2520the%2520practical%2520upper%2520bound%250Amodel%2520using%2520manual%2520annotations%2520from%2520both%2520datasets%2520in%2520TC%2520region.%2520The%2520code%2520is%250Ashared%2520at%2520https%253A//github.com/Fjr9516/DA_nnUNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16848v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Domain%20Adaptation%20for%20Pediatric%20Brain%20Tumor%20Segmentation&entry.906535625=Jingru%20Fu%20and%20Simone%20Bendazzoli%20and%20%C3%96rjan%20Smedby%20and%20Rodrigo%20Moreno&entry.1292438233=%20%20Significant%20advances%20have%20been%20made%20toward%20building%20accurate%20automatic%0Asegmentation%20models%20for%20adult%20gliomas.%20However%2C%20the%20performance%20of%20these%20models%0Aoften%20degrades%20when%20applied%20to%20pediatric%20glioma%20due%20to%20their%20imaging%20and%0Aclinical%20differences%20%28domain%20shift%29.%20Obtaining%20sufficient%20annotated%20data%20for%0Apediatric%20glioma%20is%20typically%20difficult%20because%20of%20its%20rare%20nature.%20Also%2C%0Amanual%20annotations%20are%20scarce%20and%20expensive.%20In%20this%20work%2C%20we%20propose%0ADomain-Adapted%20nnU-Net%20%28DA-nnUNet%29%20to%20perform%20unsupervised%20domain%20adaptation%0Afrom%20adult%20glioma%20%28source%20domain%29%20to%20pediatric%20glioma%20%28target%20domain%29.%0ASpecifically%2C%20we%20add%20a%20domain%20classifier%20connected%20with%20a%20gradient%20reversal%0Alayer%20%28GRL%29%20to%20a%20backbone%20nnU-Net.%20Once%20the%20classifier%20reaches%20a%20very%20high%0Aaccuracy%2C%20the%20GRL%20is%20activated%20with%20the%20goal%20of%20transferring%20domain-invariant%0Afeatures%20from%20the%20classifier%20to%20the%20segmentation%20model%20while%20preserving%0Asegmentation%20accuracy%20on%20the%20source%20domain.%20The%20accuracy%20of%20the%20classifier%0Aslowly%20degrades%20to%20chance%20levels.%20No%20annotations%20are%20used%20in%20the%20target%20domain.%0AThe%20method%20is%20compared%20to%208%20different%20supervised%20models%20using%20BraTS-Adult%0Aglioma%20%28N%3D1251%29%20and%20BraTS-PED%20glioma%20data%20%28N%3D99%29.%20The%20proposed%20method%20shows%0Anotable%20performance%20enhancements%20in%20the%20tumor%20core%20%28TC%29%20region%20compared%20to%20the%0Amodel%20that%20only%20uses%20adult%20data%3A%20~32%25%20better%20Dice%20scores%20and%20~20%20better%2095th%0Apercentile%20Hausdorff%20distances.%20Moreover%2C%20our%20unsupervised%20approach%20shows%20no%0Astatistically%20significant%20difference%20compared%20to%20the%20practical%20upper%20bound%0Amodel%20using%20manual%20annotations%20from%20both%20datasets%20in%20TC%20region.%20The%20code%20is%0Ashared%20at%20https%3A//github.com/Fjr9516/DA_nnUNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16848v1&entry.124074799=Read"},
{"title": "An Embarrassingly Simple Approach to Enhance Transformer Performance in\n  Genomic Selection for Crop Breeding", "author": "Renqi Chen and Wenwei Han and Haohao Zhang and Haoyang Su and Zhefan Wang and Xiaolei Liu and Hao Jiang and Wanli Ouyang and Nanqing Dong", "abstract": "  Genomic selection (GS), as a critical crop breeding strategy, plays a key\nrole in enhancing food production and addressing the global hunger crisis. The\npredominant approaches in GS currently revolve around employing statistical\nmethods for prediction. However, statistical methods often come with two main\nlimitations: strong statistical priors and linear assumptions. A recent trend\nis to capture the non-linear relationships between markers by deep learning.\nHowever, as crop datasets are commonly long sequences with limited samples, the\nrobustness of deep learning models, especially Transformers, remains a\nchallenge. In this work, to unleash the unexplored potential of attention\nmechanism for the task of interest, we propose a simple yet effective\nTransformer-based framework that enables end-to-end training of the whole\nsequence. Via experiments on rice3k and wheat3k datasets, we show that, with\nsimple tricks such as k-mer tokenization and random masking, Transformer can\nachieve overall superior performance against seminal methods on GS tasks of\ninterest.\n", "link": "http://arxiv.org/abs/2405.09585v3", "date": "2024-06-24", "relevancy": 2.0113, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.531}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5017}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Embarrassingly%20Simple%20Approach%20to%20Enhance%20Transformer%20Performance%20in%0A%20%20Genomic%20Selection%20for%20Crop%20Breeding&body=Title%3A%20An%20Embarrassingly%20Simple%20Approach%20to%20Enhance%20Transformer%20Performance%20in%0A%20%20Genomic%20Selection%20for%20Crop%20Breeding%0AAuthor%3A%20Renqi%20Chen%20and%20Wenwei%20Han%20and%20Haohao%20Zhang%20and%20Haoyang%20Su%20and%20Zhefan%20Wang%20and%20Xiaolei%20Liu%20and%20Hao%20Jiang%20and%20Wanli%20Ouyang%20and%20Nanqing%20Dong%0AAbstract%3A%20%20%20Genomic%20selection%20%28GS%29%2C%20as%20a%20critical%20crop%20breeding%20strategy%2C%20plays%20a%20key%0Arole%20in%20enhancing%20food%20production%20and%20addressing%20the%20global%20hunger%20crisis.%20The%0Apredominant%20approaches%20in%20GS%20currently%20revolve%20around%20employing%20statistical%0Amethods%20for%20prediction.%20However%2C%20statistical%20methods%20often%20come%20with%20two%20main%0Alimitations%3A%20strong%20statistical%20priors%20and%20linear%20assumptions.%20A%20recent%20trend%0Ais%20to%20capture%20the%20non-linear%20relationships%20between%20markers%20by%20deep%20learning.%0AHowever%2C%20as%20crop%20datasets%20are%20commonly%20long%20sequences%20with%20limited%20samples%2C%20the%0Arobustness%20of%20deep%20learning%20models%2C%20especially%20Transformers%2C%20remains%20a%0Achallenge.%20In%20this%20work%2C%20to%20unleash%20the%20unexplored%20potential%20of%20attention%0Amechanism%20for%20the%20task%20of%20interest%2C%20we%20propose%20a%20simple%20yet%20effective%0ATransformer-based%20framework%20that%20enables%20end-to-end%20training%20of%20the%20whole%0Asequence.%20Via%20experiments%20on%20rice3k%20and%20wheat3k%20datasets%2C%20we%20show%20that%2C%20with%0Asimple%20tricks%20such%20as%20k-mer%20tokenization%20and%20random%20masking%2C%20Transformer%20can%0Aachieve%20overall%20superior%20performance%20against%20seminal%20methods%20on%20GS%20tasks%20of%0Ainterest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.09585v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Embarrassingly%2520Simple%2520Approach%2520to%2520Enhance%2520Transformer%2520Performance%2520in%250A%2520%2520Genomic%2520Selection%2520for%2520Crop%2520Breeding%26entry.906535625%3DRenqi%2520Chen%2520and%2520Wenwei%2520Han%2520and%2520Haohao%2520Zhang%2520and%2520Haoyang%2520Su%2520and%2520Zhefan%2520Wang%2520and%2520Xiaolei%2520Liu%2520and%2520Hao%2520Jiang%2520and%2520Wanli%2520Ouyang%2520and%2520Nanqing%2520Dong%26entry.1292438233%3D%2520%2520Genomic%2520selection%2520%2528GS%2529%252C%2520as%2520a%2520critical%2520crop%2520breeding%2520strategy%252C%2520plays%2520a%2520key%250Arole%2520in%2520enhancing%2520food%2520production%2520and%2520addressing%2520the%2520global%2520hunger%2520crisis.%2520The%250Apredominant%2520approaches%2520in%2520GS%2520currently%2520revolve%2520around%2520employing%2520statistical%250Amethods%2520for%2520prediction.%2520However%252C%2520statistical%2520methods%2520often%2520come%2520with%2520two%2520main%250Alimitations%253A%2520strong%2520statistical%2520priors%2520and%2520linear%2520assumptions.%2520A%2520recent%2520trend%250Ais%2520to%2520capture%2520the%2520non-linear%2520relationships%2520between%2520markers%2520by%2520deep%2520learning.%250AHowever%252C%2520as%2520crop%2520datasets%2520are%2520commonly%2520long%2520sequences%2520with%2520limited%2520samples%252C%2520the%250Arobustness%2520of%2520deep%2520learning%2520models%252C%2520especially%2520Transformers%252C%2520remains%2520a%250Achallenge.%2520In%2520this%2520work%252C%2520to%2520unleash%2520the%2520unexplored%2520potential%2520of%2520attention%250Amechanism%2520for%2520the%2520task%2520of%2520interest%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%250ATransformer-based%2520framework%2520that%2520enables%2520end-to-end%2520training%2520of%2520the%2520whole%250Asequence.%2520Via%2520experiments%2520on%2520rice3k%2520and%2520wheat3k%2520datasets%252C%2520we%2520show%2520that%252C%2520with%250Asimple%2520tricks%2520such%2520as%2520k-mer%2520tokenization%2520and%2520random%2520masking%252C%2520Transformer%2520can%250Aachieve%2520overall%2520superior%2520performance%2520against%2520seminal%2520methods%2520on%2520GS%2520tasks%2520of%250Ainterest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.09585v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Embarrassingly%20Simple%20Approach%20to%20Enhance%20Transformer%20Performance%20in%0A%20%20Genomic%20Selection%20for%20Crop%20Breeding&entry.906535625=Renqi%20Chen%20and%20Wenwei%20Han%20and%20Haohao%20Zhang%20and%20Haoyang%20Su%20and%20Zhefan%20Wang%20and%20Xiaolei%20Liu%20and%20Hao%20Jiang%20and%20Wanli%20Ouyang%20and%20Nanqing%20Dong&entry.1292438233=%20%20Genomic%20selection%20%28GS%29%2C%20as%20a%20critical%20crop%20breeding%20strategy%2C%20plays%20a%20key%0Arole%20in%20enhancing%20food%20production%20and%20addressing%20the%20global%20hunger%20crisis.%20The%0Apredominant%20approaches%20in%20GS%20currently%20revolve%20around%20employing%20statistical%0Amethods%20for%20prediction.%20However%2C%20statistical%20methods%20often%20come%20with%20two%20main%0Alimitations%3A%20strong%20statistical%20priors%20and%20linear%20assumptions.%20A%20recent%20trend%0Ais%20to%20capture%20the%20non-linear%20relationships%20between%20markers%20by%20deep%20learning.%0AHowever%2C%20as%20crop%20datasets%20are%20commonly%20long%20sequences%20with%20limited%20samples%2C%20the%0Arobustness%20of%20deep%20learning%20models%2C%20especially%20Transformers%2C%20remains%20a%0Achallenge.%20In%20this%20work%2C%20to%20unleash%20the%20unexplored%20potential%20of%20attention%0Amechanism%20for%20the%20task%20of%20interest%2C%20we%20propose%20a%20simple%20yet%20effective%0ATransformer-based%20framework%20that%20enables%20end-to-end%20training%20of%20the%20whole%0Asequence.%20Via%20experiments%20on%20rice3k%20and%20wheat3k%20datasets%2C%20we%20show%20that%2C%20with%0Asimple%20tricks%20such%20as%20k-mer%20tokenization%20and%20random%20masking%2C%20Transformer%20can%0Aachieve%20overall%20superior%20performance%20against%20seminal%20methods%20on%20GS%20tasks%20of%0Ainterest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.09585v3&entry.124074799=Read"},
{"title": "Hierarchical B-frame Video Coding for Long Group of Pictures", "author": "Ivan Kirillov and Denis Parkhomenko and Kirill Chernyshev and Alexander Pletnev and Yibo Shi and Kai Lin and Dmitry Babin", "abstract": "  Learned video compression methods already outperform VVC in the low-delay\n(LD) case, but the random-access (RA) scenario remains challenging. Most works\non learned RA video compression either use HEVC as an anchor or compare it to\nVVC in specific test conditions, using RGB-PSNR metric instead of Y-PSNR and\navoiding comprehensive evaluation. Here, we present an end-to-end learned video\ncodec for random access that combines training on long sequences of frames,\nrate allocation designed for hierarchical coding and content adaptation on\ninference. We show that under common test conditions (JVET-CTC), it achieves\nresults comparable to VTM (VVC reference software) in terms of YUV-PSNR BD-Rate\non some classes of videos, and outperforms it on almost all test sets in terms\nof VMAF BD-Rate. On average it surpasses open LD and RA end-to-end solutions in\nterms of VMAF and YUV BD-Rates.\n", "link": "http://arxiv.org/abs/2406.16544v1", "date": "2024-06-24", "relevancy": 2.0058, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5085}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.499}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20B-frame%20Video%20Coding%20for%20Long%20Group%20of%20Pictures&body=Title%3A%20Hierarchical%20B-frame%20Video%20Coding%20for%20Long%20Group%20of%20Pictures%0AAuthor%3A%20Ivan%20Kirillov%20and%20Denis%20Parkhomenko%20and%20Kirill%20Chernyshev%20and%20Alexander%20Pletnev%20and%20Yibo%20Shi%20and%20Kai%20Lin%20and%20Dmitry%20Babin%0AAbstract%3A%20%20%20Learned%20video%20compression%20methods%20already%20outperform%20VVC%20in%20the%20low-delay%0A%28LD%29%20case%2C%20but%20the%20random-access%20%28RA%29%20scenario%20remains%20challenging.%20Most%20works%0Aon%20learned%20RA%20video%20compression%20either%20use%20HEVC%20as%20an%20anchor%20or%20compare%20it%20to%0AVVC%20in%20specific%20test%20conditions%2C%20using%20RGB-PSNR%20metric%20instead%20of%20Y-PSNR%20and%0Aavoiding%20comprehensive%20evaluation.%20Here%2C%20we%20present%20an%20end-to-end%20learned%20video%0Acodec%20for%20random%20access%20that%20combines%20training%20on%20long%20sequences%20of%20frames%2C%0Arate%20allocation%20designed%20for%20hierarchical%20coding%20and%20content%20adaptation%20on%0Ainference.%20We%20show%20that%20under%20common%20test%20conditions%20%28JVET-CTC%29%2C%20it%20achieves%0Aresults%20comparable%20to%20VTM%20%28VVC%20reference%20software%29%20in%20terms%20of%20YUV-PSNR%20BD-Rate%0Aon%20some%20classes%20of%20videos%2C%20and%20outperforms%20it%20on%20almost%20all%20test%20sets%20in%20terms%0Aof%20VMAF%20BD-Rate.%20On%20average%20it%20surpasses%20open%20LD%20and%20RA%20end-to-end%20solutions%20in%0Aterms%20of%20VMAF%20and%20YUV%20BD-Rates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16544v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520B-frame%2520Video%2520Coding%2520for%2520Long%2520Group%2520of%2520Pictures%26entry.906535625%3DIvan%2520Kirillov%2520and%2520Denis%2520Parkhomenko%2520and%2520Kirill%2520Chernyshev%2520and%2520Alexander%2520Pletnev%2520and%2520Yibo%2520Shi%2520and%2520Kai%2520Lin%2520and%2520Dmitry%2520Babin%26entry.1292438233%3D%2520%2520Learned%2520video%2520compression%2520methods%2520already%2520outperform%2520VVC%2520in%2520the%2520low-delay%250A%2528LD%2529%2520case%252C%2520but%2520the%2520random-access%2520%2528RA%2529%2520scenario%2520remains%2520challenging.%2520Most%2520works%250Aon%2520learned%2520RA%2520video%2520compression%2520either%2520use%2520HEVC%2520as%2520an%2520anchor%2520or%2520compare%2520it%2520to%250AVVC%2520in%2520specific%2520test%2520conditions%252C%2520using%2520RGB-PSNR%2520metric%2520instead%2520of%2520Y-PSNR%2520and%250Aavoiding%2520comprehensive%2520evaluation.%2520Here%252C%2520we%2520present%2520an%2520end-to-end%2520learned%2520video%250Acodec%2520for%2520random%2520access%2520that%2520combines%2520training%2520on%2520long%2520sequences%2520of%2520frames%252C%250Arate%2520allocation%2520designed%2520for%2520hierarchical%2520coding%2520and%2520content%2520adaptation%2520on%250Ainference.%2520We%2520show%2520that%2520under%2520common%2520test%2520conditions%2520%2528JVET-CTC%2529%252C%2520it%2520achieves%250Aresults%2520comparable%2520to%2520VTM%2520%2528VVC%2520reference%2520software%2529%2520in%2520terms%2520of%2520YUV-PSNR%2520BD-Rate%250Aon%2520some%2520classes%2520of%2520videos%252C%2520and%2520outperforms%2520it%2520on%2520almost%2520all%2520test%2520sets%2520in%2520terms%250Aof%2520VMAF%2520BD-Rate.%2520On%2520average%2520it%2520surpasses%2520open%2520LD%2520and%2520RA%2520end-to-end%2520solutions%2520in%250Aterms%2520of%2520VMAF%2520and%2520YUV%2520BD-Rates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16544v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20B-frame%20Video%20Coding%20for%20Long%20Group%20of%20Pictures&entry.906535625=Ivan%20Kirillov%20and%20Denis%20Parkhomenko%20and%20Kirill%20Chernyshev%20and%20Alexander%20Pletnev%20and%20Yibo%20Shi%20and%20Kai%20Lin%20and%20Dmitry%20Babin&entry.1292438233=%20%20Learned%20video%20compression%20methods%20already%20outperform%20VVC%20in%20the%20low-delay%0A%28LD%29%20case%2C%20but%20the%20random-access%20%28RA%29%20scenario%20remains%20challenging.%20Most%20works%0Aon%20learned%20RA%20video%20compression%20either%20use%20HEVC%20as%20an%20anchor%20or%20compare%20it%20to%0AVVC%20in%20specific%20test%20conditions%2C%20using%20RGB-PSNR%20metric%20instead%20of%20Y-PSNR%20and%0Aavoiding%20comprehensive%20evaluation.%20Here%2C%20we%20present%20an%20end-to-end%20learned%20video%0Acodec%20for%20random%20access%20that%20combines%20training%20on%20long%20sequences%20of%20frames%2C%0Arate%20allocation%20designed%20for%20hierarchical%20coding%20and%20content%20adaptation%20on%0Ainference.%20We%20show%20that%20under%20common%20test%20conditions%20%28JVET-CTC%29%2C%20it%20achieves%0Aresults%20comparable%20to%20VTM%20%28VVC%20reference%20software%29%20in%20terms%20of%20YUV-PSNR%20BD-Rate%0Aon%20some%20classes%20of%20videos%2C%20and%20outperforms%20it%20on%20almost%20all%20test%20sets%20in%20terms%0Aof%20VMAF%20BD-Rate.%20On%20average%20it%20surpasses%20open%20LD%20and%20RA%20end-to-end%20solutions%20in%0Aterms%20of%20VMAF%20and%20YUV%20BD-Rates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16544v1&entry.124074799=Read"},
{"title": "Attribute Diversity Determines the Systematicity Gap in VQA", "author": "Ian Berlot-Attwell and Kumar Krishna Agrawal and A. Michael Carrell and Yash Sharma and Naomi Saphra", "abstract": "  The degree to which neural networks can generalize to new combinations of\nfamiliar concepts, and the conditions under which they are able to do so, has\nlong been an open question. In this work, we study the systematicity gap in\nvisual question answering: the performance difference between reasoning on\npreviously seen and unseen combinations of object attributes. To test, we\nintroduce a novel diagnostic dataset, CLEVR-HOPE. We find that while increased\nquantity of training data does not reduce the systematicity gap, increased\ntraining data diversity of the attributes in the unseen combination does. In\nall, our experiments suggest that the more distinct attribute type combinations\nare seen during training, the more systematic we can expect the resulting model\nto be.\n", "link": "http://arxiv.org/abs/2311.08695v2", "date": "2024-06-24", "relevancy": 2.0052, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5248}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4881}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attribute%20Diversity%20Determines%20the%20Systematicity%20Gap%20in%20VQA&body=Title%3A%20Attribute%20Diversity%20Determines%20the%20Systematicity%20Gap%20in%20VQA%0AAuthor%3A%20Ian%20Berlot-Attwell%20and%20Kumar%20Krishna%20Agrawal%20and%20A.%20Michael%20Carrell%20and%20Yash%20Sharma%20and%20Naomi%20Saphra%0AAbstract%3A%20%20%20The%20degree%20to%20which%20neural%20networks%20can%20generalize%20to%20new%20combinations%20of%0Afamiliar%20concepts%2C%20and%20the%20conditions%20under%20which%20they%20are%20able%20to%20do%20so%2C%20has%0Along%20been%20an%20open%20question.%20In%20this%20work%2C%20we%20study%20the%20systematicity%20gap%20in%0Avisual%20question%20answering%3A%20the%20performance%20difference%20between%20reasoning%20on%0Apreviously%20seen%20and%20unseen%20combinations%20of%20object%20attributes.%20To%20test%2C%20we%0Aintroduce%20a%20novel%20diagnostic%20dataset%2C%20CLEVR-HOPE.%20We%20find%20that%20while%20increased%0Aquantity%20of%20training%20data%20does%20not%20reduce%20the%20systematicity%20gap%2C%20increased%0Atraining%20data%20diversity%20of%20the%20attributes%20in%20the%20unseen%20combination%20does.%20In%0Aall%2C%20our%20experiments%20suggest%20that%20the%20more%20distinct%20attribute%20type%20combinations%0Aare%20seen%20during%20training%2C%20the%20more%20systematic%20we%20can%20expect%20the%20resulting%20model%0Ato%20be.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.08695v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttribute%2520Diversity%2520Determines%2520the%2520Systematicity%2520Gap%2520in%2520VQA%26entry.906535625%3DIan%2520Berlot-Attwell%2520and%2520Kumar%2520Krishna%2520Agrawal%2520and%2520A.%2520Michael%2520Carrell%2520and%2520Yash%2520Sharma%2520and%2520Naomi%2520Saphra%26entry.1292438233%3D%2520%2520The%2520degree%2520to%2520which%2520neural%2520networks%2520can%2520generalize%2520to%2520new%2520combinations%2520of%250Afamiliar%2520concepts%252C%2520and%2520the%2520conditions%2520under%2520which%2520they%2520are%2520able%2520to%2520do%2520so%252C%2520has%250Along%2520been%2520an%2520open%2520question.%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520systematicity%2520gap%2520in%250Avisual%2520question%2520answering%253A%2520the%2520performance%2520difference%2520between%2520reasoning%2520on%250Apreviously%2520seen%2520and%2520unseen%2520combinations%2520of%2520object%2520attributes.%2520To%2520test%252C%2520we%250Aintroduce%2520a%2520novel%2520diagnostic%2520dataset%252C%2520CLEVR-HOPE.%2520We%2520find%2520that%2520while%2520increased%250Aquantity%2520of%2520training%2520data%2520does%2520not%2520reduce%2520the%2520systematicity%2520gap%252C%2520increased%250Atraining%2520data%2520diversity%2520of%2520the%2520attributes%2520in%2520the%2520unseen%2520combination%2520does.%2520In%250Aall%252C%2520our%2520experiments%2520suggest%2520that%2520the%2520more%2520distinct%2520attribute%2520type%2520combinations%250Aare%2520seen%2520during%2520training%252C%2520the%2520more%2520systematic%2520we%2520can%2520expect%2520the%2520resulting%2520model%250Ato%2520be.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.08695v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attribute%20Diversity%20Determines%20the%20Systematicity%20Gap%20in%20VQA&entry.906535625=Ian%20Berlot-Attwell%20and%20Kumar%20Krishna%20Agrawal%20and%20A.%20Michael%20Carrell%20and%20Yash%20Sharma%20and%20Naomi%20Saphra&entry.1292438233=%20%20The%20degree%20to%20which%20neural%20networks%20can%20generalize%20to%20new%20combinations%20of%0Afamiliar%20concepts%2C%20and%20the%20conditions%20under%20which%20they%20are%20able%20to%20do%20so%2C%20has%0Along%20been%20an%20open%20question.%20In%20this%20work%2C%20we%20study%20the%20systematicity%20gap%20in%0Avisual%20question%20answering%3A%20the%20performance%20difference%20between%20reasoning%20on%0Apreviously%20seen%20and%20unseen%20combinations%20of%20object%20attributes.%20To%20test%2C%20we%0Aintroduce%20a%20novel%20diagnostic%20dataset%2C%20CLEVR-HOPE.%20We%20find%20that%20while%20increased%0Aquantity%20of%20training%20data%20does%20not%20reduce%20the%20systematicity%20gap%2C%20increased%0Atraining%20data%20diversity%20of%20the%20attributes%20in%20the%20unseen%20combination%20does.%20In%0Aall%2C%20our%20experiments%20suggest%20that%20the%20more%20distinct%20attribute%20type%20combinations%0Aare%20seen%20during%20training%2C%20the%20more%20systematic%20we%20can%20expect%20the%20resulting%20model%0Ato%20be.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.08695v2&entry.124074799=Read"},
{"title": "Improved Dynamic Regret for Online Frank-Wolfe", "author": "Yuanyu Wan and Lijun Zhang and Mingli Song", "abstract": "  To deal with non-stationary online problems with complex constraints, we\ninvestigate the dynamic regret of online Frank-Wolfe (OFW), which is an\nefficient projection-free algorithm for online convex optimization. It is\nwell-known that in the setting of offline optimization, the smoothness of\nfunctions and the strong convexity of functions accompanying specific\nproperties of constraint sets can be utilized to achieve fast convergence rates\nfor the Frank-Wolfe (FW) algorithm. However, for OFW, previous studies only\nestablish a dynamic regret bound of $O(\\sqrt{T}(V_T+\\sqrt{D_T}+1))$ by\nutilizing the convexity of problems, where $T$ is the number of rounds, $V_T$\nis the function variation, and $D_T$ is the gradient variation. In this paper,\nwe derive improved dynamic regret bounds for OFW by extending the fast\nconvergence rates of FW from offline optimization to online optimization. The\nkey technique for this extension is to set the step size of OFW with a line\nsearch rule. In this way, we first show that the dynamic regret bound of OFW\ncan be improved to $O(\\sqrt{T(V_T+1)})$ for smooth functions. Second, we\nachieve a better dynamic regret bound of $O(T^{1/3}(V_T+1)^{2/3})$ when\nfunctions are smooth and strongly convex, and the constraint set is strongly\nconvex. Finally, for smooth and strongly convex functions with minimizers in\nthe interior of the constraint set, we demonstrate that the dynamic regret of\nOFW reduces to $O(V_T+1)$, and can be further strengthened to\n$O(\\min\\{P_T^\\ast,S_T^\\ast,V_T\\}+1)$ by performing a constant number of FW\niterations per round, where $P_T^\\ast$ and $S_T^\\ast$ denote the path length\nand squared path length of minimizers, respectively.\n", "link": "http://arxiv.org/abs/2302.05620v2", "date": "2024-06-24", "relevancy": 1.9946, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4097}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3948}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Dynamic%20Regret%20for%20Online%20Frank-Wolfe&body=Title%3A%20Improved%20Dynamic%20Regret%20for%20Online%20Frank-Wolfe%0AAuthor%3A%20Yuanyu%20Wan%20and%20Lijun%20Zhang%20and%20Mingli%20Song%0AAbstract%3A%20%20%20To%20deal%20with%20non-stationary%20online%20problems%20with%20complex%20constraints%2C%20we%0Ainvestigate%20the%20dynamic%20regret%20of%20online%20Frank-Wolfe%20%28OFW%29%2C%20which%20is%20an%0Aefficient%20projection-free%20algorithm%20for%20online%20convex%20optimization.%20It%20is%0Awell-known%20that%20in%20the%20setting%20of%20offline%20optimization%2C%20the%20smoothness%20of%0Afunctions%20and%20the%20strong%20convexity%20of%20functions%20accompanying%20specific%0Aproperties%20of%20constraint%20sets%20can%20be%20utilized%20to%20achieve%20fast%20convergence%20rates%0Afor%20the%20Frank-Wolfe%20%28FW%29%20algorithm.%20However%2C%20for%20OFW%2C%20previous%20studies%20only%0Aestablish%20a%20dynamic%20regret%20bound%20of%20%24O%28%5Csqrt%7BT%7D%28V_T%2B%5Csqrt%7BD_T%7D%2B1%29%29%24%20by%0Autilizing%20the%20convexity%20of%20problems%2C%20where%20%24T%24%20is%20the%20number%20of%20rounds%2C%20%24V_T%24%0Ais%20the%20function%20variation%2C%20and%20%24D_T%24%20is%20the%20gradient%20variation.%20In%20this%20paper%2C%0Awe%20derive%20improved%20dynamic%20regret%20bounds%20for%20OFW%20by%20extending%20the%20fast%0Aconvergence%20rates%20of%20FW%20from%20offline%20optimization%20to%20online%20optimization.%20The%0Akey%20technique%20for%20this%20extension%20is%20to%20set%20the%20step%20size%20of%20OFW%20with%20a%20line%0Asearch%20rule.%20In%20this%20way%2C%20we%20first%20show%20that%20the%20dynamic%20regret%20bound%20of%20OFW%0Acan%20be%20improved%20to%20%24O%28%5Csqrt%7BT%28V_T%2B1%29%7D%29%24%20for%20smooth%20functions.%20Second%2C%20we%0Aachieve%20a%20better%20dynamic%20regret%20bound%20of%20%24O%28T%5E%7B1/3%7D%28V_T%2B1%29%5E%7B2/3%7D%29%24%20when%0Afunctions%20are%20smooth%20and%20strongly%20convex%2C%20and%20the%20constraint%20set%20is%20strongly%0Aconvex.%20Finally%2C%20for%20smooth%20and%20strongly%20convex%20functions%20with%20minimizers%20in%0Athe%20interior%20of%20the%20constraint%20set%2C%20we%20demonstrate%20that%20the%20dynamic%20regret%20of%0AOFW%20reduces%20to%20%24O%28V_T%2B1%29%24%2C%20and%20can%20be%20further%20strengthened%20to%0A%24O%28%5Cmin%5C%7BP_T%5E%5Cast%2CS_T%5E%5Cast%2CV_T%5C%7D%2B1%29%24%20by%20performing%20a%20constant%20number%20of%20FW%0Aiterations%20per%20round%2C%20where%20%24P_T%5E%5Cast%24%20and%20%24S_T%5E%5Cast%24%20denote%20the%20path%20length%0Aand%20squared%20path%20length%20of%20minimizers%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.05620v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Dynamic%2520Regret%2520for%2520Online%2520Frank-Wolfe%26entry.906535625%3DYuanyu%2520Wan%2520and%2520Lijun%2520Zhang%2520and%2520Mingli%2520Song%26entry.1292438233%3D%2520%2520To%2520deal%2520with%2520non-stationary%2520online%2520problems%2520with%2520complex%2520constraints%252C%2520we%250Ainvestigate%2520the%2520dynamic%2520regret%2520of%2520online%2520Frank-Wolfe%2520%2528OFW%2529%252C%2520which%2520is%2520an%250Aefficient%2520projection-free%2520algorithm%2520for%2520online%2520convex%2520optimization.%2520It%2520is%250Awell-known%2520that%2520in%2520the%2520setting%2520of%2520offline%2520optimization%252C%2520the%2520smoothness%2520of%250Afunctions%2520and%2520the%2520strong%2520convexity%2520of%2520functions%2520accompanying%2520specific%250Aproperties%2520of%2520constraint%2520sets%2520can%2520be%2520utilized%2520to%2520achieve%2520fast%2520convergence%2520rates%250Afor%2520the%2520Frank-Wolfe%2520%2528FW%2529%2520algorithm.%2520However%252C%2520for%2520OFW%252C%2520previous%2520studies%2520only%250Aestablish%2520a%2520dynamic%2520regret%2520bound%2520of%2520%2524O%2528%255Csqrt%257BT%257D%2528V_T%252B%255Csqrt%257BD_T%257D%252B1%2529%2529%2524%2520by%250Autilizing%2520the%2520convexity%2520of%2520problems%252C%2520where%2520%2524T%2524%2520is%2520the%2520number%2520of%2520rounds%252C%2520%2524V_T%2524%250Ais%2520the%2520function%2520variation%252C%2520and%2520%2524D_T%2524%2520is%2520the%2520gradient%2520variation.%2520In%2520this%2520paper%252C%250Awe%2520derive%2520improved%2520dynamic%2520regret%2520bounds%2520for%2520OFW%2520by%2520extending%2520the%2520fast%250Aconvergence%2520rates%2520of%2520FW%2520from%2520offline%2520optimization%2520to%2520online%2520optimization.%2520The%250Akey%2520technique%2520for%2520this%2520extension%2520is%2520to%2520set%2520the%2520step%2520size%2520of%2520OFW%2520with%2520a%2520line%250Asearch%2520rule.%2520In%2520this%2520way%252C%2520we%2520first%2520show%2520that%2520the%2520dynamic%2520regret%2520bound%2520of%2520OFW%250Acan%2520be%2520improved%2520to%2520%2524O%2528%255Csqrt%257BT%2528V_T%252B1%2529%257D%2529%2524%2520for%2520smooth%2520functions.%2520Second%252C%2520we%250Aachieve%2520a%2520better%2520dynamic%2520regret%2520bound%2520of%2520%2524O%2528T%255E%257B1/3%257D%2528V_T%252B1%2529%255E%257B2/3%257D%2529%2524%2520when%250Afunctions%2520are%2520smooth%2520and%2520strongly%2520convex%252C%2520and%2520the%2520constraint%2520set%2520is%2520strongly%250Aconvex.%2520Finally%252C%2520for%2520smooth%2520and%2520strongly%2520convex%2520functions%2520with%2520minimizers%2520in%250Athe%2520interior%2520of%2520the%2520constraint%2520set%252C%2520we%2520demonstrate%2520that%2520the%2520dynamic%2520regret%2520of%250AOFW%2520reduces%2520to%2520%2524O%2528V_T%252B1%2529%2524%252C%2520and%2520can%2520be%2520further%2520strengthened%2520to%250A%2524O%2528%255Cmin%255C%257BP_T%255E%255Cast%252CS_T%255E%255Cast%252CV_T%255C%257D%252B1%2529%2524%2520by%2520performing%2520a%2520constant%2520number%2520of%2520FW%250Aiterations%2520per%2520round%252C%2520where%2520%2524P_T%255E%255Cast%2524%2520and%2520%2524S_T%255E%255Cast%2524%2520denote%2520the%2520path%2520length%250Aand%2520squared%2520path%2520length%2520of%2520minimizers%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.05620v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Dynamic%20Regret%20for%20Online%20Frank-Wolfe&entry.906535625=Yuanyu%20Wan%20and%20Lijun%20Zhang%20and%20Mingli%20Song&entry.1292438233=%20%20To%20deal%20with%20non-stationary%20online%20problems%20with%20complex%20constraints%2C%20we%0Ainvestigate%20the%20dynamic%20regret%20of%20online%20Frank-Wolfe%20%28OFW%29%2C%20which%20is%20an%0Aefficient%20projection-free%20algorithm%20for%20online%20convex%20optimization.%20It%20is%0Awell-known%20that%20in%20the%20setting%20of%20offline%20optimization%2C%20the%20smoothness%20of%0Afunctions%20and%20the%20strong%20convexity%20of%20functions%20accompanying%20specific%0Aproperties%20of%20constraint%20sets%20can%20be%20utilized%20to%20achieve%20fast%20convergence%20rates%0Afor%20the%20Frank-Wolfe%20%28FW%29%20algorithm.%20However%2C%20for%20OFW%2C%20previous%20studies%20only%0Aestablish%20a%20dynamic%20regret%20bound%20of%20%24O%28%5Csqrt%7BT%7D%28V_T%2B%5Csqrt%7BD_T%7D%2B1%29%29%24%20by%0Autilizing%20the%20convexity%20of%20problems%2C%20where%20%24T%24%20is%20the%20number%20of%20rounds%2C%20%24V_T%24%0Ais%20the%20function%20variation%2C%20and%20%24D_T%24%20is%20the%20gradient%20variation.%20In%20this%20paper%2C%0Awe%20derive%20improved%20dynamic%20regret%20bounds%20for%20OFW%20by%20extending%20the%20fast%0Aconvergence%20rates%20of%20FW%20from%20offline%20optimization%20to%20online%20optimization.%20The%0Akey%20technique%20for%20this%20extension%20is%20to%20set%20the%20step%20size%20of%20OFW%20with%20a%20line%0Asearch%20rule.%20In%20this%20way%2C%20we%20first%20show%20that%20the%20dynamic%20regret%20bound%20of%20OFW%0Acan%20be%20improved%20to%20%24O%28%5Csqrt%7BT%28V_T%2B1%29%7D%29%24%20for%20smooth%20functions.%20Second%2C%20we%0Aachieve%20a%20better%20dynamic%20regret%20bound%20of%20%24O%28T%5E%7B1/3%7D%28V_T%2B1%29%5E%7B2/3%7D%29%24%20when%0Afunctions%20are%20smooth%20and%20strongly%20convex%2C%20and%20the%20constraint%20set%20is%20strongly%0Aconvex.%20Finally%2C%20for%20smooth%20and%20strongly%20convex%20functions%20with%20minimizers%20in%0Athe%20interior%20of%20the%20constraint%20set%2C%20we%20demonstrate%20that%20the%20dynamic%20regret%20of%0AOFW%20reduces%20to%20%24O%28V_T%2B1%29%24%2C%20and%20can%20be%20further%20strengthened%20to%0A%24O%28%5Cmin%5C%7BP_T%5E%5Cast%2CS_T%5E%5Cast%2CV_T%5C%7D%2B1%29%24%20by%20performing%20a%20constant%20number%20of%20FW%0Aiterations%20per%20round%2C%20where%20%24P_T%5E%5Cast%24%20and%20%24S_T%5E%5Cast%24%20denote%20the%20path%20length%0Aand%20squared%20path%20length%20of%20minimizers%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.05620v2&entry.124074799=Read"},
{"title": "Enhancing Dropout-based Bayesian Neural Networks with Multi-Exit on FPGA", "author": "Hao Mark Chen and Liam Castelli and Martin Ferianc and Hongyu Zhou and Shuanglong Liu and Wayne Luk and Hongxiang Fan", "abstract": "  Reliable uncertainty estimation plays a crucial role in various\nsafety-critical applications such as medical diagnosis and autonomous driving.\nIn recent years, Bayesian neural networks (BayesNNs) have gained substantial\nresearch and industrial interests due to their capability to make accurate\npredictions with reliable uncertainty estimation. However, the algorithmic\ncomplexity and the resulting hardware performance of BayesNNs hinder their\nadoption in real-life applications. To bridge this gap, this paper proposes an\nalgorithm and hardware co-design framework that can generate field-programmable\ngate array (FPGA)-based accelerators for efficient BayesNNs. At the algorithm\nlevel, we propose novel multi-exit dropout-based BayesNNs with reduced\ncomputational and memory overheads while achieving high accuracy and quality of\nuncertainty estimation. At the hardware level, this paper introduces a\ntransformation framework that can generate FPGA-based accelerators for the\nproposed efficient multi-exit BayesNNs. Several optimization techniques such as\nthe mix of spatial and temporal mappings are introduced to reduce resource\nconsumption and improve the overall hardware performance. Comprehensive\nexperiments demonstrate that our approach can achieve higher energy efficiency\ncompared to CPU, GPU, and other state-of-the-art hardware implementations. To\nsupport the future development of this research, we have open-sourced our code\nat: https://github.com/os-hxfan/MCME_FPGA_Acc.git\n", "link": "http://arxiv.org/abs/2406.14593v2", "date": "2024-06-24", "relevancy": 1.9908, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5241}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5028}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Dropout-based%20Bayesian%20Neural%20Networks%20with%20Multi-Exit%20on%20FPGA&body=Title%3A%20Enhancing%20Dropout-based%20Bayesian%20Neural%20Networks%20with%20Multi-Exit%20on%20FPGA%0AAuthor%3A%20Hao%20Mark%20Chen%20and%20Liam%20Castelli%20and%20Martin%20Ferianc%20and%20Hongyu%20Zhou%20and%20Shuanglong%20Liu%20and%20Wayne%20Luk%20and%20Hongxiang%20Fan%0AAbstract%3A%20%20%20Reliable%20uncertainty%20estimation%20plays%20a%20crucial%20role%20in%20various%0Asafety-critical%20applications%20such%20as%20medical%20diagnosis%20and%20autonomous%20driving.%0AIn%20recent%20years%2C%20Bayesian%20neural%20networks%20%28BayesNNs%29%20have%20gained%20substantial%0Aresearch%20and%20industrial%20interests%20due%20to%20their%20capability%20to%20make%20accurate%0Apredictions%20with%20reliable%20uncertainty%20estimation.%20However%2C%20the%20algorithmic%0Acomplexity%20and%20the%20resulting%20hardware%20performance%20of%20BayesNNs%20hinder%20their%0Aadoption%20in%20real-life%20applications.%20To%20bridge%20this%20gap%2C%20this%20paper%20proposes%20an%0Aalgorithm%20and%20hardware%20co-design%20framework%20that%20can%20generate%20field-programmable%0Agate%20array%20%28FPGA%29-based%20accelerators%20for%20efficient%20BayesNNs.%20At%20the%20algorithm%0Alevel%2C%20we%20propose%20novel%20multi-exit%20dropout-based%20BayesNNs%20with%20reduced%0Acomputational%20and%20memory%20overheads%20while%20achieving%20high%20accuracy%20and%20quality%20of%0Auncertainty%20estimation.%20At%20the%20hardware%20level%2C%20this%20paper%20introduces%20a%0Atransformation%20framework%20that%20can%20generate%20FPGA-based%20accelerators%20for%20the%0Aproposed%20efficient%20multi-exit%20BayesNNs.%20Several%20optimization%20techniques%20such%20as%0Athe%20mix%20of%20spatial%20and%20temporal%20mappings%20are%20introduced%20to%20reduce%20resource%0Aconsumption%20and%20improve%20the%20overall%20hardware%20performance.%20Comprehensive%0Aexperiments%20demonstrate%20that%20our%20approach%20can%20achieve%20higher%20energy%20efficiency%0Acompared%20to%20CPU%2C%20GPU%2C%20and%20other%20state-of-the-art%20hardware%20implementations.%20To%0Asupport%20the%20future%20development%20of%20this%20research%2C%20we%20have%20open-sourced%20our%20code%0Aat%3A%20https%3A//github.com/os-hxfan/MCME_FPGA_Acc.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14593v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Dropout-based%2520Bayesian%2520Neural%2520Networks%2520with%2520Multi-Exit%2520on%2520FPGA%26entry.906535625%3DHao%2520Mark%2520Chen%2520and%2520Liam%2520Castelli%2520and%2520Martin%2520Ferianc%2520and%2520Hongyu%2520Zhou%2520and%2520Shuanglong%2520Liu%2520and%2520Wayne%2520Luk%2520and%2520Hongxiang%2520Fan%26entry.1292438233%3D%2520%2520Reliable%2520uncertainty%2520estimation%2520plays%2520a%2520crucial%2520role%2520in%2520various%250Asafety-critical%2520applications%2520such%2520as%2520medical%2520diagnosis%2520and%2520autonomous%2520driving.%250AIn%2520recent%2520years%252C%2520Bayesian%2520neural%2520networks%2520%2528BayesNNs%2529%2520have%2520gained%2520substantial%250Aresearch%2520and%2520industrial%2520interests%2520due%2520to%2520their%2520capability%2520to%2520make%2520accurate%250Apredictions%2520with%2520reliable%2520uncertainty%2520estimation.%2520However%252C%2520the%2520algorithmic%250Acomplexity%2520and%2520the%2520resulting%2520hardware%2520performance%2520of%2520BayesNNs%2520hinder%2520their%250Aadoption%2520in%2520real-life%2520applications.%2520To%2520bridge%2520this%2520gap%252C%2520this%2520paper%2520proposes%2520an%250Aalgorithm%2520and%2520hardware%2520co-design%2520framework%2520that%2520can%2520generate%2520field-programmable%250Agate%2520array%2520%2528FPGA%2529-based%2520accelerators%2520for%2520efficient%2520BayesNNs.%2520At%2520the%2520algorithm%250Alevel%252C%2520we%2520propose%2520novel%2520multi-exit%2520dropout-based%2520BayesNNs%2520with%2520reduced%250Acomputational%2520and%2520memory%2520overheads%2520while%2520achieving%2520high%2520accuracy%2520and%2520quality%2520of%250Auncertainty%2520estimation.%2520At%2520the%2520hardware%2520level%252C%2520this%2520paper%2520introduces%2520a%250Atransformation%2520framework%2520that%2520can%2520generate%2520FPGA-based%2520accelerators%2520for%2520the%250Aproposed%2520efficient%2520multi-exit%2520BayesNNs.%2520Several%2520optimization%2520techniques%2520such%2520as%250Athe%2520mix%2520of%2520spatial%2520and%2520temporal%2520mappings%2520are%2520introduced%2520to%2520reduce%2520resource%250Aconsumption%2520and%2520improve%2520the%2520overall%2520hardware%2520performance.%2520Comprehensive%250Aexperiments%2520demonstrate%2520that%2520our%2520approach%2520can%2520achieve%2520higher%2520energy%2520efficiency%250Acompared%2520to%2520CPU%252C%2520GPU%252C%2520and%2520other%2520state-of-the-art%2520hardware%2520implementations.%2520To%250Asupport%2520the%2520future%2520development%2520of%2520this%2520research%252C%2520we%2520have%2520open-sourced%2520our%2520code%250Aat%253A%2520https%253A//github.com/os-hxfan/MCME_FPGA_Acc.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14593v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Dropout-based%20Bayesian%20Neural%20Networks%20with%20Multi-Exit%20on%20FPGA&entry.906535625=Hao%20Mark%20Chen%20and%20Liam%20Castelli%20and%20Martin%20Ferianc%20and%20Hongyu%20Zhou%20and%20Shuanglong%20Liu%20and%20Wayne%20Luk%20and%20Hongxiang%20Fan&entry.1292438233=%20%20Reliable%20uncertainty%20estimation%20plays%20a%20crucial%20role%20in%20various%0Asafety-critical%20applications%20such%20as%20medical%20diagnosis%20and%20autonomous%20driving.%0AIn%20recent%20years%2C%20Bayesian%20neural%20networks%20%28BayesNNs%29%20have%20gained%20substantial%0Aresearch%20and%20industrial%20interests%20due%20to%20their%20capability%20to%20make%20accurate%0Apredictions%20with%20reliable%20uncertainty%20estimation.%20However%2C%20the%20algorithmic%0Acomplexity%20and%20the%20resulting%20hardware%20performance%20of%20BayesNNs%20hinder%20their%0Aadoption%20in%20real-life%20applications.%20To%20bridge%20this%20gap%2C%20this%20paper%20proposes%20an%0Aalgorithm%20and%20hardware%20co-design%20framework%20that%20can%20generate%20field-programmable%0Agate%20array%20%28FPGA%29-based%20accelerators%20for%20efficient%20BayesNNs.%20At%20the%20algorithm%0Alevel%2C%20we%20propose%20novel%20multi-exit%20dropout-based%20BayesNNs%20with%20reduced%0Acomputational%20and%20memory%20overheads%20while%20achieving%20high%20accuracy%20and%20quality%20of%0Auncertainty%20estimation.%20At%20the%20hardware%20level%2C%20this%20paper%20introduces%20a%0Atransformation%20framework%20that%20can%20generate%20FPGA-based%20accelerators%20for%20the%0Aproposed%20efficient%20multi-exit%20BayesNNs.%20Several%20optimization%20techniques%20such%20as%0Athe%20mix%20of%20spatial%20and%20temporal%20mappings%20are%20introduced%20to%20reduce%20resource%0Aconsumption%20and%20improve%20the%20overall%20hardware%20performance.%20Comprehensive%0Aexperiments%20demonstrate%20that%20our%20approach%20can%20achieve%20higher%20energy%20efficiency%0Acompared%20to%20CPU%2C%20GPU%2C%20and%20other%20state-of-the-art%20hardware%20implementations.%20To%0Asupport%20the%20future%20development%20of%20this%20research%2C%20we%20have%20open-sourced%20our%20code%0Aat%3A%20https%3A//github.com/os-hxfan/MCME_FPGA_Acc.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14593v2&entry.124074799=Read"},
{"title": "Generation of Asset Administration Shell with Large Language Model\n  Agents: Toward Semantic Interoperability in Digital Twins in the Context of\n  Industry 4.0", "author": "Yuchen Xia and Zhewen Xiao and Nasser Jazdi and Michael Weyrich", "abstract": "  This research introduces a novel approach for achieving semantic\ninteroperability in digital twins and assisting the creation of Asset\nAdministration Shell (AAS) as digital twin model within the context of Industry\n4.0. The foundational idea of our research is that the communication based on\nsemantics and the generation of meaningful textual data are directly linked,\nand we posit that these processes are equivalent if the exchanged information\ncan be serialized in text form. Based on this, we construct a \"semantic node\"\ndata structure in our research to capture the semantic essence of textual data.\nThen, a system powered by large language models is designed and implemented to\nprocess the \"semantic node\" and generate standardized digital twin models from\nraw textual data collected from datasheets describing technical assets. Our\nevaluation demonstrates an effective generation rate of 62-79%, indicating a\nsubstantial proportion of the information from the source text can be\ntranslated error-free to the target digital twin instance model with the\ngenerative capability of large language models. This result has a direct\napplication in the context of Industry 4.0, and the designed system is\nimplemented as a data model generation tool for reducing the manual effort in\ncreating AAS model. In our evaluation, a comparative analysis of different LLMs\nand an in-depth ablation study of Retrieval-Augmented Generation (RAG)\nmechanisms provide insights into the effectiveness of LLM systems for\ninterpreting technical concepts and translating data. Our findings emphasize\nLLMs' capability to automate AAS instance creation and contribute to the\nbroader field of semantic interoperability for digital twins in industrial\napplications. The prototype implementation and evaluation results are presented\non our GitHub Repository: https://github.com/YuchenXia/AASbyLLM.\n", "link": "http://arxiv.org/abs/2403.17209v4", "date": "2024-06-24", "relevancy": 1.9687, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4981}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4892}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generation%20of%20Asset%20Administration%20Shell%20with%20Large%20Language%20Model%0A%20%20Agents%3A%20Toward%20Semantic%20Interoperability%20in%20Digital%20Twins%20in%20the%20Context%20of%0A%20%20Industry%204.0&body=Title%3A%20Generation%20of%20Asset%20Administration%20Shell%20with%20Large%20Language%20Model%0A%20%20Agents%3A%20Toward%20Semantic%20Interoperability%20in%20Digital%20Twins%20in%20the%20Context%20of%0A%20%20Industry%204.0%0AAuthor%3A%20Yuchen%20Xia%20and%20Zhewen%20Xiao%20and%20Nasser%20Jazdi%20and%20Michael%20Weyrich%0AAbstract%3A%20%20%20This%20research%20introduces%20a%20novel%20approach%20for%20achieving%20semantic%0Ainteroperability%20in%20digital%20twins%20and%20assisting%20the%20creation%20of%20Asset%0AAdministration%20Shell%20%28AAS%29%20as%20digital%20twin%20model%20within%20the%20context%20of%20Industry%0A4.0.%20The%20foundational%20idea%20of%20our%20research%20is%20that%20the%20communication%20based%20on%0Asemantics%20and%20the%20generation%20of%20meaningful%20textual%20data%20are%20directly%20linked%2C%0Aand%20we%20posit%20that%20these%20processes%20are%20equivalent%20if%20the%20exchanged%20information%0Acan%20be%20serialized%20in%20text%20form.%20Based%20on%20this%2C%20we%20construct%20a%20%22semantic%20node%22%0Adata%20structure%20in%20our%20research%20to%20capture%20the%20semantic%20essence%20of%20textual%20data.%0AThen%2C%20a%20system%20powered%20by%20large%20language%20models%20is%20designed%20and%20implemented%20to%0Aprocess%20the%20%22semantic%20node%22%20and%20generate%20standardized%20digital%20twin%20models%20from%0Araw%20textual%20data%20collected%20from%20datasheets%20describing%20technical%20assets.%20Our%0Aevaluation%20demonstrates%20an%20effective%20generation%20rate%20of%2062-79%25%2C%20indicating%20a%0Asubstantial%20proportion%20of%20the%20information%20from%20the%20source%20text%20can%20be%0Atranslated%20error-free%20to%20the%20target%20digital%20twin%20instance%20model%20with%20the%0Agenerative%20capability%20of%20large%20language%20models.%20This%20result%20has%20a%20direct%0Aapplication%20in%20the%20context%20of%20Industry%204.0%2C%20and%20the%20designed%20system%20is%0Aimplemented%20as%20a%20data%20model%20generation%20tool%20for%20reducing%20the%20manual%20effort%20in%0Acreating%20AAS%20model.%20In%20our%20evaluation%2C%20a%20comparative%20analysis%20of%20different%20LLMs%0Aand%20an%20in-depth%20ablation%20study%20of%20Retrieval-Augmented%20Generation%20%28RAG%29%0Amechanisms%20provide%20insights%20into%20the%20effectiveness%20of%20LLM%20systems%20for%0Ainterpreting%20technical%20concepts%20and%20translating%20data.%20Our%20findings%20emphasize%0ALLMs%27%20capability%20to%20automate%20AAS%20instance%20creation%20and%20contribute%20to%20the%0Abroader%20field%20of%20semantic%20interoperability%20for%20digital%20twins%20in%20industrial%0Aapplications.%20The%20prototype%20implementation%20and%20evaluation%20results%20are%20presented%0Aon%20our%20GitHub%20Repository%3A%20https%3A//github.com/YuchenXia/AASbyLLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17209v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneration%2520of%2520Asset%2520Administration%2520Shell%2520with%2520Large%2520Language%2520Model%250A%2520%2520Agents%253A%2520Toward%2520Semantic%2520Interoperability%2520in%2520Digital%2520Twins%2520in%2520the%2520Context%2520of%250A%2520%2520Industry%25204.0%26entry.906535625%3DYuchen%2520Xia%2520and%2520Zhewen%2520Xiao%2520and%2520Nasser%2520Jazdi%2520and%2520Michael%2520Weyrich%26entry.1292438233%3D%2520%2520This%2520research%2520introduces%2520a%2520novel%2520approach%2520for%2520achieving%2520semantic%250Ainteroperability%2520in%2520digital%2520twins%2520and%2520assisting%2520the%2520creation%2520of%2520Asset%250AAdministration%2520Shell%2520%2528AAS%2529%2520as%2520digital%2520twin%2520model%2520within%2520the%2520context%2520of%2520Industry%250A4.0.%2520The%2520foundational%2520idea%2520of%2520our%2520research%2520is%2520that%2520the%2520communication%2520based%2520on%250Asemantics%2520and%2520the%2520generation%2520of%2520meaningful%2520textual%2520data%2520are%2520directly%2520linked%252C%250Aand%2520we%2520posit%2520that%2520these%2520processes%2520are%2520equivalent%2520if%2520the%2520exchanged%2520information%250Acan%2520be%2520serialized%2520in%2520text%2520form.%2520Based%2520on%2520this%252C%2520we%2520construct%2520a%2520%2522semantic%2520node%2522%250Adata%2520structure%2520in%2520our%2520research%2520to%2520capture%2520the%2520semantic%2520essence%2520of%2520textual%2520data.%250AThen%252C%2520a%2520system%2520powered%2520by%2520large%2520language%2520models%2520is%2520designed%2520and%2520implemented%2520to%250Aprocess%2520the%2520%2522semantic%2520node%2522%2520and%2520generate%2520standardized%2520digital%2520twin%2520models%2520from%250Araw%2520textual%2520data%2520collected%2520from%2520datasheets%2520describing%2520technical%2520assets.%2520Our%250Aevaluation%2520demonstrates%2520an%2520effective%2520generation%2520rate%2520of%252062-79%2525%252C%2520indicating%2520a%250Asubstantial%2520proportion%2520of%2520the%2520information%2520from%2520the%2520source%2520text%2520can%2520be%250Atranslated%2520error-free%2520to%2520the%2520target%2520digital%2520twin%2520instance%2520model%2520with%2520the%250Agenerative%2520capability%2520of%2520large%2520language%2520models.%2520This%2520result%2520has%2520a%2520direct%250Aapplication%2520in%2520the%2520context%2520of%2520Industry%25204.0%252C%2520and%2520the%2520designed%2520system%2520is%250Aimplemented%2520as%2520a%2520data%2520model%2520generation%2520tool%2520for%2520reducing%2520the%2520manual%2520effort%2520in%250Acreating%2520AAS%2520model.%2520In%2520our%2520evaluation%252C%2520a%2520comparative%2520analysis%2520of%2520different%2520LLMs%250Aand%2520an%2520in-depth%2520ablation%2520study%2520of%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%250Amechanisms%2520provide%2520insights%2520into%2520the%2520effectiveness%2520of%2520LLM%2520systems%2520for%250Ainterpreting%2520technical%2520concepts%2520and%2520translating%2520data.%2520Our%2520findings%2520emphasize%250ALLMs%2527%2520capability%2520to%2520automate%2520AAS%2520instance%2520creation%2520and%2520contribute%2520to%2520the%250Abroader%2520field%2520of%2520semantic%2520interoperability%2520for%2520digital%2520twins%2520in%2520industrial%250Aapplications.%2520The%2520prototype%2520implementation%2520and%2520evaluation%2520results%2520are%2520presented%250Aon%2520our%2520GitHub%2520Repository%253A%2520https%253A//github.com/YuchenXia/AASbyLLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17209v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generation%20of%20Asset%20Administration%20Shell%20with%20Large%20Language%20Model%0A%20%20Agents%3A%20Toward%20Semantic%20Interoperability%20in%20Digital%20Twins%20in%20the%20Context%20of%0A%20%20Industry%204.0&entry.906535625=Yuchen%20Xia%20and%20Zhewen%20Xiao%20and%20Nasser%20Jazdi%20and%20Michael%20Weyrich&entry.1292438233=%20%20This%20research%20introduces%20a%20novel%20approach%20for%20achieving%20semantic%0Ainteroperability%20in%20digital%20twins%20and%20assisting%20the%20creation%20of%20Asset%0AAdministration%20Shell%20%28AAS%29%20as%20digital%20twin%20model%20within%20the%20context%20of%20Industry%0A4.0.%20The%20foundational%20idea%20of%20our%20research%20is%20that%20the%20communication%20based%20on%0Asemantics%20and%20the%20generation%20of%20meaningful%20textual%20data%20are%20directly%20linked%2C%0Aand%20we%20posit%20that%20these%20processes%20are%20equivalent%20if%20the%20exchanged%20information%0Acan%20be%20serialized%20in%20text%20form.%20Based%20on%20this%2C%20we%20construct%20a%20%22semantic%20node%22%0Adata%20structure%20in%20our%20research%20to%20capture%20the%20semantic%20essence%20of%20textual%20data.%0AThen%2C%20a%20system%20powered%20by%20large%20language%20models%20is%20designed%20and%20implemented%20to%0Aprocess%20the%20%22semantic%20node%22%20and%20generate%20standardized%20digital%20twin%20models%20from%0Araw%20textual%20data%20collected%20from%20datasheets%20describing%20technical%20assets.%20Our%0Aevaluation%20demonstrates%20an%20effective%20generation%20rate%20of%2062-79%25%2C%20indicating%20a%0Asubstantial%20proportion%20of%20the%20information%20from%20the%20source%20text%20can%20be%0Atranslated%20error-free%20to%20the%20target%20digital%20twin%20instance%20model%20with%20the%0Agenerative%20capability%20of%20large%20language%20models.%20This%20result%20has%20a%20direct%0Aapplication%20in%20the%20context%20of%20Industry%204.0%2C%20and%20the%20designed%20system%20is%0Aimplemented%20as%20a%20data%20model%20generation%20tool%20for%20reducing%20the%20manual%20effort%20in%0Acreating%20AAS%20model.%20In%20our%20evaluation%2C%20a%20comparative%20analysis%20of%20different%20LLMs%0Aand%20an%20in-depth%20ablation%20study%20of%20Retrieval-Augmented%20Generation%20%28RAG%29%0Amechanisms%20provide%20insights%20into%20the%20effectiveness%20of%20LLM%20systems%20for%0Ainterpreting%20technical%20concepts%20and%20translating%20data.%20Our%20findings%20emphasize%0ALLMs%27%20capability%20to%20automate%20AAS%20instance%20creation%20and%20contribute%20to%20the%0Abroader%20field%20of%20semantic%20interoperability%20for%20digital%20twins%20in%20industrial%0Aapplications.%20The%20prototype%20implementation%20and%20evaluation%20results%20are%20presented%0Aon%20our%20GitHub%20Repository%3A%20https%3A//github.com/YuchenXia/AASbyLLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17209v4&entry.124074799=Read"},
{"title": "Losing Visual Needles in Image Haystacks: Vision Language Models are\n  Easily Distracted in Short and Long Contexts", "author": "Aditya Sharma and Michael Saxon and William Yang Wang", "abstract": "  We present LoCoVQA, a dynamic benchmark generator for evaluating long-context\nextractive reasoning in vision language models (VLMs). LoCoVQA augments test\nexamples for mathematical reasoning, VQA, and character recognition tasks with\nincreasingly long visual contexts composed of both in-distribution and\nout-of-distribution distractor images.\n  Across these tasks, a diverse set of VLMs rapidly lose performance as the\nvisual context length grows, often exhibiting a striking exponential decay\ntrend. This test assesses how well VLMs can ignore irrelevant information when\nanswering queries -- a task that is quite easy for language models (LMs) in the\ntext domain -- demonstrating that current state-of-the-art VLMs lack this\nessential capability for many long-context applications.\n", "link": "http://arxiv.org/abs/2406.16851v1", "date": "2024-06-24", "relevancy": 1.9639, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4958}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4922}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Losing%20Visual%20Needles%20in%20Image%20Haystacks%3A%20Vision%20Language%20Models%20are%0A%20%20Easily%20Distracted%20in%20Short%20and%20Long%20Contexts&body=Title%3A%20Losing%20Visual%20Needles%20in%20Image%20Haystacks%3A%20Vision%20Language%20Models%20are%0A%20%20Easily%20Distracted%20in%20Short%20and%20Long%20Contexts%0AAuthor%3A%20Aditya%20Sharma%20and%20Michael%20Saxon%20and%20William%20Yang%20Wang%0AAbstract%3A%20%20%20We%20present%20LoCoVQA%2C%20a%20dynamic%20benchmark%20generator%20for%20evaluating%20long-context%0Aextractive%20reasoning%20in%20vision%20language%20models%20%28VLMs%29.%20LoCoVQA%20augments%20test%0Aexamples%20for%20mathematical%20reasoning%2C%20VQA%2C%20and%20character%20recognition%20tasks%20with%0Aincreasingly%20long%20visual%20contexts%20composed%20of%20both%20in-distribution%20and%0Aout-of-distribution%20distractor%20images.%0A%20%20Across%20these%20tasks%2C%20a%20diverse%20set%20of%20VLMs%20rapidly%20lose%20performance%20as%20the%0Avisual%20context%20length%20grows%2C%20often%20exhibiting%20a%20striking%20exponential%20decay%0Atrend.%20This%20test%20assesses%20how%20well%20VLMs%20can%20ignore%20irrelevant%20information%20when%0Aanswering%20queries%20--%20a%20task%20that%20is%20quite%20easy%20for%20language%20models%20%28LMs%29%20in%20the%0Atext%20domain%20--%20demonstrating%20that%20current%20state-of-the-art%20VLMs%20lack%20this%0Aessential%20capability%20for%20many%20long-context%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16851v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLosing%2520Visual%2520Needles%2520in%2520Image%2520Haystacks%253A%2520Vision%2520Language%2520Models%2520are%250A%2520%2520Easily%2520Distracted%2520in%2520Short%2520and%2520Long%2520Contexts%26entry.906535625%3DAditya%2520Sharma%2520and%2520Michael%2520Saxon%2520and%2520William%2520Yang%2520Wang%26entry.1292438233%3D%2520%2520We%2520present%2520LoCoVQA%252C%2520a%2520dynamic%2520benchmark%2520generator%2520for%2520evaluating%2520long-context%250Aextractive%2520reasoning%2520in%2520vision%2520language%2520models%2520%2528VLMs%2529.%2520LoCoVQA%2520augments%2520test%250Aexamples%2520for%2520mathematical%2520reasoning%252C%2520VQA%252C%2520and%2520character%2520recognition%2520tasks%2520with%250Aincreasingly%2520long%2520visual%2520contexts%2520composed%2520of%2520both%2520in-distribution%2520and%250Aout-of-distribution%2520distractor%2520images.%250A%2520%2520Across%2520these%2520tasks%252C%2520a%2520diverse%2520set%2520of%2520VLMs%2520rapidly%2520lose%2520performance%2520as%2520the%250Avisual%2520context%2520length%2520grows%252C%2520often%2520exhibiting%2520a%2520striking%2520exponential%2520decay%250Atrend.%2520This%2520test%2520assesses%2520how%2520well%2520VLMs%2520can%2520ignore%2520irrelevant%2520information%2520when%250Aanswering%2520queries%2520--%2520a%2520task%2520that%2520is%2520quite%2520easy%2520for%2520language%2520models%2520%2528LMs%2529%2520in%2520the%250Atext%2520domain%2520--%2520demonstrating%2520that%2520current%2520state-of-the-art%2520VLMs%2520lack%2520this%250Aessential%2520capability%2520for%2520many%2520long-context%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16851v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Losing%20Visual%20Needles%20in%20Image%20Haystacks%3A%20Vision%20Language%20Models%20are%0A%20%20Easily%20Distracted%20in%20Short%20and%20Long%20Contexts&entry.906535625=Aditya%20Sharma%20and%20Michael%20Saxon%20and%20William%20Yang%20Wang&entry.1292438233=%20%20We%20present%20LoCoVQA%2C%20a%20dynamic%20benchmark%20generator%20for%20evaluating%20long-context%0Aextractive%20reasoning%20in%20vision%20language%20models%20%28VLMs%29.%20LoCoVQA%20augments%20test%0Aexamples%20for%20mathematical%20reasoning%2C%20VQA%2C%20and%20character%20recognition%20tasks%20with%0Aincreasingly%20long%20visual%20contexts%20composed%20of%20both%20in-distribution%20and%0Aout-of-distribution%20distractor%20images.%0A%20%20Across%20these%20tasks%2C%20a%20diverse%20set%20of%20VLMs%20rapidly%20lose%20performance%20as%20the%0Avisual%20context%20length%20grows%2C%20often%20exhibiting%20a%20striking%20exponential%20decay%0Atrend.%20This%20test%20assesses%20how%20well%20VLMs%20can%20ignore%20irrelevant%20information%20when%0Aanswering%20queries%20--%20a%20task%20that%20is%20quite%20easy%20for%20language%20models%20%28LMs%29%20in%20the%0Atext%20domain%20--%20demonstrating%20that%20current%20state-of-the-art%20VLMs%20lack%20this%0Aessential%20capability%20for%20many%20long-context%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16851v1&entry.124074799=Read"},
{"title": "Personalized federated learning based on feature fusion", "author": "Wolong Xing and Zhenkui Shi and Hongyan Peng and Xiantao Hu and Xianxian Li", "abstract": "  Federated learning enables distributed clients to collaborate on training\nwhile storing their data locally to protect client privacy. However, due to the\nheterogeneity of data, models, and devices, the final global model may need to\nperform better for tasks on each client. Communication bottlenecks, data\nheterogeneity, and model heterogeneity have been common challenges in federated\nlearning. In this work, we considered a label distribution skew problem, a type\nof data heterogeneity easily overlooked. In the context of classification, we\npropose a personalized federated learning approach called pFedPM. In our\nprocess, we replace traditional gradient uploading with feature uploading,\nwhich helps reduce communication costs and allows for heterogeneous client\nmodels. These feature representations play a role in preserving privacy to some\nextent.\n  We use a hyperparameter $a$ to mix local and global features, which enables\nus to control the degree of personalization. We also introduced a relation\nnetwork as an additional decision layer, which provides a non-linear learnable\nclassifier to predict labels. Experimental results show that, with an\nappropriate setting of $a$, our scheme outperforms several recent FL methods on\nMNIST, FEMNIST, and CRIFAR10 datasets and achieves fewer communications.\n", "link": "http://arxiv.org/abs/2406.16583v1", "date": "2024-06-24", "relevancy": 1.9507, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5044}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4776}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20federated%20learning%20based%20on%20feature%20fusion&body=Title%3A%20Personalized%20federated%20learning%20based%20on%20feature%20fusion%0AAuthor%3A%20Wolong%20Xing%20and%20Zhenkui%20Shi%20and%20Hongyan%20Peng%20and%20Xiantao%20Hu%20and%20Xianxian%20Li%0AAbstract%3A%20%20%20Federated%20learning%20enables%20distributed%20clients%20to%20collaborate%20on%20training%0Awhile%20storing%20their%20data%20locally%20to%20protect%20client%20privacy.%20However%2C%20due%20to%20the%0Aheterogeneity%20of%20data%2C%20models%2C%20and%20devices%2C%20the%20final%20global%20model%20may%20need%20to%0Aperform%20better%20for%20tasks%20on%20each%20client.%20Communication%20bottlenecks%2C%20data%0Aheterogeneity%2C%20and%20model%20heterogeneity%20have%20been%20common%20challenges%20in%20federated%0Alearning.%20In%20this%20work%2C%20we%20considered%20a%20label%20distribution%20skew%20problem%2C%20a%20type%0Aof%20data%20heterogeneity%20easily%20overlooked.%20In%20the%20context%20of%20classification%2C%20we%0Apropose%20a%20personalized%20federated%20learning%20approach%20called%20pFedPM.%20In%20our%0Aprocess%2C%20we%20replace%20traditional%20gradient%20uploading%20with%20feature%20uploading%2C%0Awhich%20helps%20reduce%20communication%20costs%20and%20allows%20for%20heterogeneous%20client%0Amodels.%20These%20feature%20representations%20play%20a%20role%20in%20preserving%20privacy%20to%20some%0Aextent.%0A%20%20We%20use%20a%20hyperparameter%20%24a%24%20to%20mix%20local%20and%20global%20features%2C%20which%20enables%0Aus%20to%20control%20the%20degree%20of%20personalization.%20We%20also%20introduced%20a%20relation%0Anetwork%20as%20an%20additional%20decision%20layer%2C%20which%20provides%20a%20non-linear%20learnable%0Aclassifier%20to%20predict%20labels.%20Experimental%20results%20show%20that%2C%20with%20an%0Aappropriate%20setting%20of%20%24a%24%2C%20our%20scheme%20outperforms%20several%20recent%20FL%20methods%20on%0AMNIST%2C%20FEMNIST%2C%20and%20CRIFAR10%20datasets%20and%20achieves%20fewer%20communications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520federated%2520learning%2520based%2520on%2520feature%2520fusion%26entry.906535625%3DWolong%2520Xing%2520and%2520Zhenkui%2520Shi%2520and%2520Hongyan%2520Peng%2520and%2520Xiantao%2520Hu%2520and%2520Xianxian%2520Li%26entry.1292438233%3D%2520%2520Federated%2520learning%2520enables%2520distributed%2520clients%2520to%2520collaborate%2520on%2520training%250Awhile%2520storing%2520their%2520data%2520locally%2520to%2520protect%2520client%2520privacy.%2520However%252C%2520due%2520to%2520the%250Aheterogeneity%2520of%2520data%252C%2520models%252C%2520and%2520devices%252C%2520the%2520final%2520global%2520model%2520may%2520need%2520to%250Aperform%2520better%2520for%2520tasks%2520on%2520each%2520client.%2520Communication%2520bottlenecks%252C%2520data%250Aheterogeneity%252C%2520and%2520model%2520heterogeneity%2520have%2520been%2520common%2520challenges%2520in%2520federated%250Alearning.%2520In%2520this%2520work%252C%2520we%2520considered%2520a%2520label%2520distribution%2520skew%2520problem%252C%2520a%2520type%250Aof%2520data%2520heterogeneity%2520easily%2520overlooked.%2520In%2520the%2520context%2520of%2520classification%252C%2520we%250Apropose%2520a%2520personalized%2520federated%2520learning%2520approach%2520called%2520pFedPM.%2520In%2520our%250Aprocess%252C%2520we%2520replace%2520traditional%2520gradient%2520uploading%2520with%2520feature%2520uploading%252C%250Awhich%2520helps%2520reduce%2520communication%2520costs%2520and%2520allows%2520for%2520heterogeneous%2520client%250Amodels.%2520These%2520feature%2520representations%2520play%2520a%2520role%2520in%2520preserving%2520privacy%2520to%2520some%250Aextent.%250A%2520%2520We%2520use%2520a%2520hyperparameter%2520%2524a%2524%2520to%2520mix%2520local%2520and%2520global%2520features%252C%2520which%2520enables%250Aus%2520to%2520control%2520the%2520degree%2520of%2520personalization.%2520We%2520also%2520introduced%2520a%2520relation%250Anetwork%2520as%2520an%2520additional%2520decision%2520layer%252C%2520which%2520provides%2520a%2520non-linear%2520learnable%250Aclassifier%2520to%2520predict%2520labels.%2520Experimental%2520results%2520show%2520that%252C%2520with%2520an%250Aappropriate%2520setting%2520of%2520%2524a%2524%252C%2520our%2520scheme%2520outperforms%2520several%2520recent%2520FL%2520methods%2520on%250AMNIST%252C%2520FEMNIST%252C%2520and%2520CRIFAR10%2520datasets%2520and%2520achieves%2520fewer%2520communications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20federated%20learning%20based%20on%20feature%20fusion&entry.906535625=Wolong%20Xing%20and%20Zhenkui%20Shi%20and%20Hongyan%20Peng%20and%20Xiantao%20Hu%20and%20Xianxian%20Li&entry.1292438233=%20%20Federated%20learning%20enables%20distributed%20clients%20to%20collaborate%20on%20training%0Awhile%20storing%20their%20data%20locally%20to%20protect%20client%20privacy.%20However%2C%20due%20to%20the%0Aheterogeneity%20of%20data%2C%20models%2C%20and%20devices%2C%20the%20final%20global%20model%20may%20need%20to%0Aperform%20better%20for%20tasks%20on%20each%20client.%20Communication%20bottlenecks%2C%20data%0Aheterogeneity%2C%20and%20model%20heterogeneity%20have%20been%20common%20challenges%20in%20federated%0Alearning.%20In%20this%20work%2C%20we%20considered%20a%20label%20distribution%20skew%20problem%2C%20a%20type%0Aof%20data%20heterogeneity%20easily%20overlooked.%20In%20the%20context%20of%20classification%2C%20we%0Apropose%20a%20personalized%20federated%20learning%20approach%20called%20pFedPM.%20In%20our%0Aprocess%2C%20we%20replace%20traditional%20gradient%20uploading%20with%20feature%20uploading%2C%0Awhich%20helps%20reduce%20communication%20costs%20and%20allows%20for%20heterogeneous%20client%0Amodels.%20These%20feature%20representations%20play%20a%20role%20in%20preserving%20privacy%20to%20some%0Aextent.%0A%20%20We%20use%20a%20hyperparameter%20%24a%24%20to%20mix%20local%20and%20global%20features%2C%20which%20enables%0Aus%20to%20control%20the%20degree%20of%20personalization.%20We%20also%20introduced%20a%20relation%0Anetwork%20as%20an%20additional%20decision%20layer%2C%20which%20provides%20a%20non-linear%20learnable%0Aclassifier%20to%20predict%20labels.%20Experimental%20results%20show%20that%2C%20with%20an%0Aappropriate%20setting%20of%20%24a%24%2C%20our%20scheme%20outperforms%20several%20recent%20FL%20methods%20on%0AMNIST%2C%20FEMNIST%2C%20and%20CRIFAR10%20datasets%20and%20achieves%20fewer%20communications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16583v1&entry.124074799=Read"},
{"title": "Pandora's White-Box: Precise Training Data Detection and Extraction in\n  Large Language Models", "author": "Jeffrey G. Wang and Jason Wang and Marvin Li and Seth Neel", "abstract": "  In this paper we develop state-of-the-art privacy attacks against Large\nLanguage Models (LLMs), where an adversary with some access to the model tries\nto learn something about the underlying training data. Our headline results are\nnew membership inference attacks (MIAs) against pretrained LLMs that perform\nhundreds of times better than baseline attacks, and a pipeline showing that\nover 50% (!) of the fine-tuning dataset can be extracted from a fine-tuned LLM\nin natural settings. We consider varying degrees of access to the underlying\nmodel, pretraining and fine-tuning data, and both MIAs and training data\nextraction. For pretraining data, we propose two new MIAs: a supervised neural\nnetwork classifier that predicts training data membership on the basis of\n(dimensionality-reduced) model gradients, as well as a variant of this attack\nthat only requires logit access to the model by leveraging recent\nmodel-stealing work on LLMs. To our knowledge this is the first MIA that\nexplicitly incorporates model-stealing information. Both attacks outperform\nexisting black-box baselines, and our supervised attack closes the gap between\nMIA attack success against LLMs and the strongest known attacks for other\nmachine learning models. In fine-tuning, we find that a simple attack based on\nthe ratio of the loss between the base and fine-tuned models is able to achieve\nnear-perfect MIA performance; we then leverage our MIA to extract a large\nfraction of the fine-tuning dataset from fine-tuned Pythia and Llama models.\nOur code is available at github.com/safr-ai-lab/pandora-llm.\n", "link": "http://arxiv.org/abs/2402.17012v3", "date": "2024-06-24", "relevancy": 1.9471, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4993}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4916}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pandora%27s%20White-Box%3A%20Precise%20Training%20Data%20Detection%20and%20Extraction%20in%0A%20%20Large%20Language%20Models&body=Title%3A%20Pandora%27s%20White-Box%3A%20Precise%20Training%20Data%20Detection%20and%20Extraction%20in%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Jeffrey%20G.%20Wang%20and%20Jason%20Wang%20and%20Marvin%20Li%20and%20Seth%20Neel%0AAbstract%3A%20%20%20In%20this%20paper%20we%20develop%20state-of-the-art%20privacy%20attacks%20against%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20where%20an%20adversary%20with%20some%20access%20to%20the%20model%20tries%0Ato%20learn%20something%20about%20the%20underlying%20training%20data.%20Our%20headline%20results%20are%0Anew%20membership%20inference%20attacks%20%28MIAs%29%20against%20pretrained%20LLMs%20that%20perform%0Ahundreds%20of%20times%20better%20than%20baseline%20attacks%2C%20and%20a%20pipeline%20showing%20that%0Aover%2050%25%20%28%21%29%20of%20the%20fine-tuning%20dataset%20can%20be%20extracted%20from%20a%20fine-tuned%20LLM%0Ain%20natural%20settings.%20We%20consider%20varying%20degrees%20of%20access%20to%20the%20underlying%0Amodel%2C%20pretraining%20and%20fine-tuning%20data%2C%20and%20both%20MIAs%20and%20training%20data%0Aextraction.%20For%20pretraining%20data%2C%20we%20propose%20two%20new%20MIAs%3A%20a%20supervised%20neural%0Anetwork%20classifier%20that%20predicts%20training%20data%20membership%20on%20the%20basis%20of%0A%28dimensionality-reduced%29%20model%20gradients%2C%20as%20well%20as%20a%20variant%20of%20this%20attack%0Athat%20only%20requires%20logit%20access%20to%20the%20model%20by%20leveraging%20recent%0Amodel-stealing%20work%20on%20LLMs.%20To%20our%20knowledge%20this%20is%20the%20first%20MIA%20that%0Aexplicitly%20incorporates%20model-stealing%20information.%20Both%20attacks%20outperform%0Aexisting%20black-box%20baselines%2C%20and%20our%20supervised%20attack%20closes%20the%20gap%20between%0AMIA%20attack%20success%20against%20LLMs%20and%20the%20strongest%20known%20attacks%20for%20other%0Amachine%20learning%20models.%20In%20fine-tuning%2C%20we%20find%20that%20a%20simple%20attack%20based%20on%0Athe%20ratio%20of%20the%20loss%20between%20the%20base%20and%20fine-tuned%20models%20is%20able%20to%20achieve%0Anear-perfect%20MIA%20performance%3B%20we%20then%20leverage%20our%20MIA%20to%20extract%20a%20large%0Afraction%20of%20the%20fine-tuning%20dataset%20from%20fine-tuned%20Pythia%20and%20Llama%20models.%0AOur%20code%20is%20available%20at%20github.com/safr-ai-lab/pandora-llm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17012v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPandora%2527s%2520White-Box%253A%2520Precise%2520Training%2520Data%2520Detection%2520and%2520Extraction%2520in%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DJeffrey%2520G.%2520Wang%2520and%2520Jason%2520Wang%2520and%2520Marvin%2520Li%2520and%2520Seth%2520Neel%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520develop%2520state-of-the-art%2520privacy%2520attacks%2520against%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%252C%2520where%2520an%2520adversary%2520with%2520some%2520access%2520to%2520the%2520model%2520tries%250Ato%2520learn%2520something%2520about%2520the%2520underlying%2520training%2520data.%2520Our%2520headline%2520results%2520are%250Anew%2520membership%2520inference%2520attacks%2520%2528MIAs%2529%2520against%2520pretrained%2520LLMs%2520that%2520perform%250Ahundreds%2520of%2520times%2520better%2520than%2520baseline%2520attacks%252C%2520and%2520a%2520pipeline%2520showing%2520that%250Aover%252050%2525%2520%2528%2521%2529%2520of%2520the%2520fine-tuning%2520dataset%2520can%2520be%2520extracted%2520from%2520a%2520fine-tuned%2520LLM%250Ain%2520natural%2520settings.%2520We%2520consider%2520varying%2520degrees%2520of%2520access%2520to%2520the%2520underlying%250Amodel%252C%2520pretraining%2520and%2520fine-tuning%2520data%252C%2520and%2520both%2520MIAs%2520and%2520training%2520data%250Aextraction.%2520For%2520pretraining%2520data%252C%2520we%2520propose%2520two%2520new%2520MIAs%253A%2520a%2520supervised%2520neural%250Anetwork%2520classifier%2520that%2520predicts%2520training%2520data%2520membership%2520on%2520the%2520basis%2520of%250A%2528dimensionality-reduced%2529%2520model%2520gradients%252C%2520as%2520well%2520as%2520a%2520variant%2520of%2520this%2520attack%250Athat%2520only%2520requires%2520logit%2520access%2520to%2520the%2520model%2520by%2520leveraging%2520recent%250Amodel-stealing%2520work%2520on%2520LLMs.%2520To%2520our%2520knowledge%2520this%2520is%2520the%2520first%2520MIA%2520that%250Aexplicitly%2520incorporates%2520model-stealing%2520information.%2520Both%2520attacks%2520outperform%250Aexisting%2520black-box%2520baselines%252C%2520and%2520our%2520supervised%2520attack%2520closes%2520the%2520gap%2520between%250AMIA%2520attack%2520success%2520against%2520LLMs%2520and%2520the%2520strongest%2520known%2520attacks%2520for%2520other%250Amachine%2520learning%2520models.%2520In%2520fine-tuning%252C%2520we%2520find%2520that%2520a%2520simple%2520attack%2520based%2520on%250Athe%2520ratio%2520of%2520the%2520loss%2520between%2520the%2520base%2520and%2520fine-tuned%2520models%2520is%2520able%2520to%2520achieve%250Anear-perfect%2520MIA%2520performance%253B%2520we%2520then%2520leverage%2520our%2520MIA%2520to%2520extract%2520a%2520large%250Afraction%2520of%2520the%2520fine-tuning%2520dataset%2520from%2520fine-tuned%2520Pythia%2520and%2520Llama%2520models.%250AOur%2520code%2520is%2520available%2520at%2520github.com/safr-ai-lab/pandora-llm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17012v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pandora%27s%20White-Box%3A%20Precise%20Training%20Data%20Detection%20and%20Extraction%20in%0A%20%20Large%20Language%20Models&entry.906535625=Jeffrey%20G.%20Wang%20and%20Jason%20Wang%20and%20Marvin%20Li%20and%20Seth%20Neel&entry.1292438233=%20%20In%20this%20paper%20we%20develop%20state-of-the-art%20privacy%20attacks%20against%20Large%0ALanguage%20Models%20%28LLMs%29%2C%20where%20an%20adversary%20with%20some%20access%20to%20the%20model%20tries%0Ato%20learn%20something%20about%20the%20underlying%20training%20data.%20Our%20headline%20results%20are%0Anew%20membership%20inference%20attacks%20%28MIAs%29%20against%20pretrained%20LLMs%20that%20perform%0Ahundreds%20of%20times%20better%20than%20baseline%20attacks%2C%20and%20a%20pipeline%20showing%20that%0Aover%2050%25%20%28%21%29%20of%20the%20fine-tuning%20dataset%20can%20be%20extracted%20from%20a%20fine-tuned%20LLM%0Ain%20natural%20settings.%20We%20consider%20varying%20degrees%20of%20access%20to%20the%20underlying%0Amodel%2C%20pretraining%20and%20fine-tuning%20data%2C%20and%20both%20MIAs%20and%20training%20data%0Aextraction.%20For%20pretraining%20data%2C%20we%20propose%20two%20new%20MIAs%3A%20a%20supervised%20neural%0Anetwork%20classifier%20that%20predicts%20training%20data%20membership%20on%20the%20basis%20of%0A%28dimensionality-reduced%29%20model%20gradients%2C%20as%20well%20as%20a%20variant%20of%20this%20attack%0Athat%20only%20requires%20logit%20access%20to%20the%20model%20by%20leveraging%20recent%0Amodel-stealing%20work%20on%20LLMs.%20To%20our%20knowledge%20this%20is%20the%20first%20MIA%20that%0Aexplicitly%20incorporates%20model-stealing%20information.%20Both%20attacks%20outperform%0Aexisting%20black-box%20baselines%2C%20and%20our%20supervised%20attack%20closes%20the%20gap%20between%0AMIA%20attack%20success%20against%20LLMs%20and%20the%20strongest%20known%20attacks%20for%20other%0Amachine%20learning%20models.%20In%20fine-tuning%2C%20we%20find%20that%20a%20simple%20attack%20based%20on%0Athe%20ratio%20of%20the%20loss%20between%20the%20base%20and%20fine-tuned%20models%20is%20able%20to%20achieve%0Anear-perfect%20MIA%20performance%3B%20we%20then%20leverage%20our%20MIA%20to%20extract%20a%20large%0Afraction%20of%20the%20fine-tuning%20dataset%20from%20fine-tuned%20Pythia%20and%20Llama%20models.%0AOur%20code%20is%20available%20at%20github.com/safr-ai-lab/pandora-llm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17012v3&entry.124074799=Read"},
{"title": "Coding schemes in neural networks learning classification tasks", "author": "Alexander van Meegen and Haim Sompolinsky", "abstract": "  Neural networks posses the crucial ability to generate meaningful\nrepresentations of task-dependent features. Indeed, with appropriate scaling,\nsupervised learning in neural networks can result in strong, task-dependent\nfeature learning. However, the nature of the emergent representations, which we\ncall the `coding scheme', is still unclear. To understand the emergent coding\nscheme, we investigate fully-connected, wide neural networks learning\nclassification tasks using the Bayesian framework where learning shapes the\nposterior distribution of the network weights. Consistent with previous\nfindings, our analysis of the feature learning regime (also known as\n`non-lazy', `rich', or `mean-field' regime) shows that the networks acquire\nstrong, data-dependent features. Surprisingly, the nature of the internal\nrepresentations depends crucially on the neuronal nonlinearity. In linear\nnetworks, an analog coding scheme of the task emerges. Despite the strong\nrepresentations, the mean predictor is identical to the lazy case. In nonlinear\nnetworks, spontaneous symmetry breaking leads to either redundant or sparse\ncoding schemes. Our findings highlight how network properties such as scaling\nof weights and neuronal nonlinearity can profoundly influence the emergent\nrepresentations.\n", "link": "http://arxiv.org/abs/2406.16689v1", "date": "2024-06-24", "relevancy": 1.9433, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4917}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4857}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coding%20schemes%20in%20neural%20networks%20learning%20classification%20tasks&body=Title%3A%20Coding%20schemes%20in%20neural%20networks%20learning%20classification%20tasks%0AAuthor%3A%20Alexander%20van%20Meegen%20and%20Haim%20Sompolinsky%0AAbstract%3A%20%20%20Neural%20networks%20posses%20the%20crucial%20ability%20to%20generate%20meaningful%0Arepresentations%20of%20task-dependent%20features.%20Indeed%2C%20with%20appropriate%20scaling%2C%0Asupervised%20learning%20in%20neural%20networks%20can%20result%20in%20strong%2C%20task-dependent%0Afeature%20learning.%20However%2C%20the%20nature%20of%20the%20emergent%20representations%2C%20which%20we%0Acall%20the%20%60coding%20scheme%27%2C%20is%20still%20unclear.%20To%20understand%20the%20emergent%20coding%0Ascheme%2C%20we%20investigate%20fully-connected%2C%20wide%20neural%20networks%20learning%0Aclassification%20tasks%20using%20the%20Bayesian%20framework%20where%20learning%20shapes%20the%0Aposterior%20distribution%20of%20the%20network%20weights.%20Consistent%20with%20previous%0Afindings%2C%20our%20analysis%20of%20the%20feature%20learning%20regime%20%28also%20known%20as%0A%60non-lazy%27%2C%20%60rich%27%2C%20or%20%60mean-field%27%20regime%29%20shows%20that%20the%20networks%20acquire%0Astrong%2C%20data-dependent%20features.%20Surprisingly%2C%20the%20nature%20of%20the%20internal%0Arepresentations%20depends%20crucially%20on%20the%20neuronal%20nonlinearity.%20In%20linear%0Anetworks%2C%20an%20analog%20coding%20scheme%20of%20the%20task%20emerges.%20Despite%20the%20strong%0Arepresentations%2C%20the%20mean%20predictor%20is%20identical%20to%20the%20lazy%20case.%20In%20nonlinear%0Anetworks%2C%20spontaneous%20symmetry%20breaking%20leads%20to%20either%20redundant%20or%20sparse%0Acoding%20schemes.%20Our%20findings%20highlight%20how%20network%20properties%20such%20as%20scaling%0Aof%20weights%20and%20neuronal%20nonlinearity%20can%20profoundly%20influence%20the%20emergent%0Arepresentations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoding%2520schemes%2520in%2520neural%2520networks%2520learning%2520classification%2520tasks%26entry.906535625%3DAlexander%2520van%2520Meegen%2520and%2520Haim%2520Sompolinsky%26entry.1292438233%3D%2520%2520Neural%2520networks%2520posses%2520the%2520crucial%2520ability%2520to%2520generate%2520meaningful%250Arepresentations%2520of%2520task-dependent%2520features.%2520Indeed%252C%2520with%2520appropriate%2520scaling%252C%250Asupervised%2520learning%2520in%2520neural%2520networks%2520can%2520result%2520in%2520strong%252C%2520task-dependent%250Afeature%2520learning.%2520However%252C%2520the%2520nature%2520of%2520the%2520emergent%2520representations%252C%2520which%2520we%250Acall%2520the%2520%2560coding%2520scheme%2527%252C%2520is%2520still%2520unclear.%2520To%2520understand%2520the%2520emergent%2520coding%250Ascheme%252C%2520we%2520investigate%2520fully-connected%252C%2520wide%2520neural%2520networks%2520learning%250Aclassification%2520tasks%2520using%2520the%2520Bayesian%2520framework%2520where%2520learning%2520shapes%2520the%250Aposterior%2520distribution%2520of%2520the%2520network%2520weights.%2520Consistent%2520with%2520previous%250Afindings%252C%2520our%2520analysis%2520of%2520the%2520feature%2520learning%2520regime%2520%2528also%2520known%2520as%250A%2560non-lazy%2527%252C%2520%2560rich%2527%252C%2520or%2520%2560mean-field%2527%2520regime%2529%2520shows%2520that%2520the%2520networks%2520acquire%250Astrong%252C%2520data-dependent%2520features.%2520Surprisingly%252C%2520the%2520nature%2520of%2520the%2520internal%250Arepresentations%2520depends%2520crucially%2520on%2520the%2520neuronal%2520nonlinearity.%2520In%2520linear%250Anetworks%252C%2520an%2520analog%2520coding%2520scheme%2520of%2520the%2520task%2520emerges.%2520Despite%2520the%2520strong%250Arepresentations%252C%2520the%2520mean%2520predictor%2520is%2520identical%2520to%2520the%2520lazy%2520case.%2520In%2520nonlinear%250Anetworks%252C%2520spontaneous%2520symmetry%2520breaking%2520leads%2520to%2520either%2520redundant%2520or%2520sparse%250Acoding%2520schemes.%2520Our%2520findings%2520highlight%2520how%2520network%2520properties%2520such%2520as%2520scaling%250Aof%2520weights%2520and%2520neuronal%2520nonlinearity%2520can%2520profoundly%2520influence%2520the%2520emergent%250Arepresentations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coding%20schemes%20in%20neural%20networks%20learning%20classification%20tasks&entry.906535625=Alexander%20van%20Meegen%20and%20Haim%20Sompolinsky&entry.1292438233=%20%20Neural%20networks%20posses%20the%20crucial%20ability%20to%20generate%20meaningful%0Arepresentations%20of%20task-dependent%20features.%20Indeed%2C%20with%20appropriate%20scaling%2C%0Asupervised%20learning%20in%20neural%20networks%20can%20result%20in%20strong%2C%20task-dependent%0Afeature%20learning.%20However%2C%20the%20nature%20of%20the%20emergent%20representations%2C%20which%20we%0Acall%20the%20%60coding%20scheme%27%2C%20is%20still%20unclear.%20To%20understand%20the%20emergent%20coding%0Ascheme%2C%20we%20investigate%20fully-connected%2C%20wide%20neural%20networks%20learning%0Aclassification%20tasks%20using%20the%20Bayesian%20framework%20where%20learning%20shapes%20the%0Aposterior%20distribution%20of%20the%20network%20weights.%20Consistent%20with%20previous%0Afindings%2C%20our%20analysis%20of%20the%20feature%20learning%20regime%20%28also%20known%20as%0A%60non-lazy%27%2C%20%60rich%27%2C%20or%20%60mean-field%27%20regime%29%20shows%20that%20the%20networks%20acquire%0Astrong%2C%20data-dependent%20features.%20Surprisingly%2C%20the%20nature%20of%20the%20internal%0Arepresentations%20depends%20crucially%20on%20the%20neuronal%20nonlinearity.%20In%20linear%0Anetworks%2C%20an%20analog%20coding%20scheme%20of%20the%20task%20emerges.%20Despite%20the%20strong%0Arepresentations%2C%20the%20mean%20predictor%20is%20identical%20to%20the%20lazy%20case.%20In%20nonlinear%0Anetworks%2C%20spontaneous%20symmetry%20breaking%20leads%20to%20either%20redundant%20or%20sparse%0Acoding%20schemes.%20Our%20findings%20highlight%20how%20network%20properties%20such%20as%20scaling%0Aof%20weights%20and%20neuronal%20nonlinearity%20can%20profoundly%20influence%20the%20emergent%0Arepresentations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16689v1&entry.124074799=Read"},
{"title": "Understanding and Mitigating Tokenization Bias in Language Models", "author": "Buu Phan and Marton Havasi and Matthew Muckley and Karen Ullrich", "abstract": "  State-of-the-art language models are autoregressive and operate on subword\nunits known as tokens. Specifically, one must encode the conditioning string\ninto a list of tokens before passing to the language models for next-token\nprediction. We show that, for encoding schemes such as maximum prefix matching,\ntokenization induces a sampling bias that cannot be mitigated with more\ntraining or data. To counter this universal problem, we propose a novel\nalgorithm to obtain unbiased estimates from a model that was trained on\ntokenized data. Our method does not require finetuning the model, and its\ncomplexity, defined as the number of model runs, scales linearly with the\nsequence length. As a consequence, we show that one can simulate token-free\nbehavior from a tokenized language model. We empirically verify the correctness\nof our method through a Markov-chain setup, where it accurately recovers the\ntransition probabilities, as opposed to the conventional method of directly\nprompting tokens into the language model.\n", "link": "http://arxiv.org/abs/2406.16829v1", "date": "2024-06-24", "relevancy": 1.9408, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5138}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4693}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20and%20Mitigating%20Tokenization%20Bias%20in%20Language%20Models&body=Title%3A%20Understanding%20and%20Mitigating%20Tokenization%20Bias%20in%20Language%20Models%0AAuthor%3A%20Buu%20Phan%20and%20Marton%20Havasi%20and%20Matthew%20Muckley%20and%20Karen%20Ullrich%0AAbstract%3A%20%20%20State-of-the-art%20language%20models%20are%20autoregressive%20and%20operate%20on%20subword%0Aunits%20known%20as%20tokens.%20Specifically%2C%20one%20must%20encode%20the%20conditioning%20string%0Ainto%20a%20list%20of%20tokens%20before%20passing%20to%20the%20language%20models%20for%20next-token%0Aprediction.%20We%20show%20that%2C%20for%20encoding%20schemes%20such%20as%20maximum%20prefix%20matching%2C%0Atokenization%20induces%20a%20sampling%20bias%20that%20cannot%20be%20mitigated%20with%20more%0Atraining%20or%20data.%20To%20counter%20this%20universal%20problem%2C%20we%20propose%20a%20novel%0Aalgorithm%20to%20obtain%20unbiased%20estimates%20from%20a%20model%20that%20was%20trained%20on%0Atokenized%20data.%20Our%20method%20does%20not%20require%20finetuning%20the%20model%2C%20and%20its%0Acomplexity%2C%20defined%20as%20the%20number%20of%20model%20runs%2C%20scales%20linearly%20with%20the%0Asequence%20length.%20As%20a%20consequence%2C%20we%20show%20that%20one%20can%20simulate%20token-free%0Abehavior%20from%20a%20tokenized%20language%20model.%20We%20empirically%20verify%20the%20correctness%0Aof%20our%20method%20through%20a%20Markov-chain%20setup%2C%20where%20it%20accurately%20recovers%20the%0Atransition%20probabilities%2C%20as%20opposed%20to%20the%20conventional%20method%20of%20directly%0Aprompting%20tokens%20into%20the%20language%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16829v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520and%2520Mitigating%2520Tokenization%2520Bias%2520in%2520Language%2520Models%26entry.906535625%3DBuu%2520Phan%2520and%2520Marton%2520Havasi%2520and%2520Matthew%2520Muckley%2520and%2520Karen%2520Ullrich%26entry.1292438233%3D%2520%2520State-of-the-art%2520language%2520models%2520are%2520autoregressive%2520and%2520operate%2520on%2520subword%250Aunits%2520known%2520as%2520tokens.%2520Specifically%252C%2520one%2520must%2520encode%2520the%2520conditioning%2520string%250Ainto%2520a%2520list%2520of%2520tokens%2520before%2520passing%2520to%2520the%2520language%2520models%2520for%2520next-token%250Aprediction.%2520We%2520show%2520that%252C%2520for%2520encoding%2520schemes%2520such%2520as%2520maximum%2520prefix%2520matching%252C%250Atokenization%2520induces%2520a%2520sampling%2520bias%2520that%2520cannot%2520be%2520mitigated%2520with%2520more%250Atraining%2520or%2520data.%2520To%2520counter%2520this%2520universal%2520problem%252C%2520we%2520propose%2520a%2520novel%250Aalgorithm%2520to%2520obtain%2520unbiased%2520estimates%2520from%2520a%2520model%2520that%2520was%2520trained%2520on%250Atokenized%2520data.%2520Our%2520method%2520does%2520not%2520require%2520finetuning%2520the%2520model%252C%2520and%2520its%250Acomplexity%252C%2520defined%2520as%2520the%2520number%2520of%2520model%2520runs%252C%2520scales%2520linearly%2520with%2520the%250Asequence%2520length.%2520As%2520a%2520consequence%252C%2520we%2520show%2520that%2520one%2520can%2520simulate%2520token-free%250Abehavior%2520from%2520a%2520tokenized%2520language%2520model.%2520We%2520empirically%2520verify%2520the%2520correctness%250Aof%2520our%2520method%2520through%2520a%2520Markov-chain%2520setup%252C%2520where%2520it%2520accurately%2520recovers%2520the%250Atransition%2520probabilities%252C%2520as%2520opposed%2520to%2520the%2520conventional%2520method%2520of%2520directly%250Aprompting%2520tokens%2520into%2520the%2520language%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16829v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20and%20Mitigating%20Tokenization%20Bias%20in%20Language%20Models&entry.906535625=Buu%20Phan%20and%20Marton%20Havasi%20and%20Matthew%20Muckley%20and%20Karen%20Ullrich&entry.1292438233=%20%20State-of-the-art%20language%20models%20are%20autoregressive%20and%20operate%20on%20subword%0Aunits%20known%20as%20tokens.%20Specifically%2C%20one%20must%20encode%20the%20conditioning%20string%0Ainto%20a%20list%20of%20tokens%20before%20passing%20to%20the%20language%20models%20for%20next-token%0Aprediction.%20We%20show%20that%2C%20for%20encoding%20schemes%20such%20as%20maximum%20prefix%20matching%2C%0Atokenization%20induces%20a%20sampling%20bias%20that%20cannot%20be%20mitigated%20with%20more%0Atraining%20or%20data.%20To%20counter%20this%20universal%20problem%2C%20we%20propose%20a%20novel%0Aalgorithm%20to%20obtain%20unbiased%20estimates%20from%20a%20model%20that%20was%20trained%20on%0Atokenized%20data.%20Our%20method%20does%20not%20require%20finetuning%20the%20model%2C%20and%20its%0Acomplexity%2C%20defined%20as%20the%20number%20of%20model%20runs%2C%20scales%20linearly%20with%20the%0Asequence%20length.%20As%20a%20consequence%2C%20we%20show%20that%20one%20can%20simulate%20token-free%0Abehavior%20from%20a%20tokenized%20language%20model.%20We%20empirically%20verify%20the%20correctness%0Aof%20our%20method%20through%20a%20Markov-chain%20setup%2C%20where%20it%20accurately%20recovers%20the%0Atransition%20probabilities%2C%20as%20opposed%20to%20the%20conventional%20method%20of%20directly%0Aprompting%20tokens%20into%20the%20language%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16829v1&entry.124074799=Read"},
{"title": "MonoBox: Tightness-free Box-supervised Polyp Segmentation using\n  Monotonicity Constraint", "author": "Qiang Hu and Zhenyu Yi and Ying Zhou and Ting Li and Fan Huang and Mei Liu and Qiang Li and Zhiwei Wang", "abstract": "  We propose MonoBox, an innovative box-supervised segmentation method\nconstrained by monotonicity to liberate its training from the user-unfriendly\nbox-tightness assumption. In contrast to conventional box-supervised\nsegmentation, where the box edges must precisely touch the target boundaries,\nMonoBox leverages imprecisely-annotated boxes to achieve robust pixel-wise\nsegmentation. The 'linchpin' is that, within the noisy zones around box edges,\nMonoBox discards the traditional misguiding multiple-instance learning loss,\nand instead optimizes a carefully-designed objective, termed monotonicity\nconstraint. Along directions transitioning from the foreground to background,\nthis new constraint steers responses to adhere to a trend of monotonically\ndecreasing values. Consequently, the originally unreliable learning within the\nnoisy zones is transformed into a correct and effective monotonicity\noptimization. Moreover, an adaptive label correction is introduced, enabling\nMonoBox to enhance the tightness of box annotations using predicted masks from\nthe previous epoch and dynamically shrink the noisy zones as training\nprogresses. We verify MonoBox in the box-supervised segmentation task of\npolyps, where satisfying box-tightness is challenging due to the vague\nboundaries between the polyp and normal tissues. Experiments on both public\nsynthetic and in-house real noisy datasets demonstrate that MonoBox exceeds\nother anti-noise state-of-the-arts by improving Dice by at least 5.5% and 3.3%,\nrespectively. Codes are at https://github.com/Huster-Hq/MonoBox.\n", "link": "http://arxiv.org/abs/2404.01188v4", "date": "2024-06-24", "relevancy": 1.9406, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5089}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4867}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MonoBox%3A%20Tightness-free%20Box-supervised%20Polyp%20Segmentation%20using%0A%20%20Monotonicity%20Constraint&body=Title%3A%20MonoBox%3A%20Tightness-free%20Box-supervised%20Polyp%20Segmentation%20using%0A%20%20Monotonicity%20Constraint%0AAuthor%3A%20Qiang%20Hu%20and%20Zhenyu%20Yi%20and%20Ying%20Zhou%20and%20Ting%20Li%20and%20Fan%20Huang%20and%20Mei%20Liu%20and%20Qiang%20Li%20and%20Zhiwei%20Wang%0AAbstract%3A%20%20%20We%20propose%20MonoBox%2C%20an%20innovative%20box-supervised%20segmentation%20method%0Aconstrained%20by%20monotonicity%20to%20liberate%20its%20training%20from%20the%20user-unfriendly%0Abox-tightness%20assumption.%20In%20contrast%20to%20conventional%20box-supervised%0Asegmentation%2C%20where%20the%20box%20edges%20must%20precisely%20touch%20the%20target%20boundaries%2C%0AMonoBox%20leverages%20imprecisely-annotated%20boxes%20to%20achieve%20robust%20pixel-wise%0Asegmentation.%20The%20%27linchpin%27%20is%20that%2C%20within%20the%20noisy%20zones%20around%20box%20edges%2C%0AMonoBox%20discards%20the%20traditional%20misguiding%20multiple-instance%20learning%20loss%2C%0Aand%20instead%20optimizes%20a%20carefully-designed%20objective%2C%20termed%20monotonicity%0Aconstraint.%20Along%20directions%20transitioning%20from%20the%20foreground%20to%20background%2C%0Athis%20new%20constraint%20steers%20responses%20to%20adhere%20to%20a%20trend%20of%20monotonically%0Adecreasing%20values.%20Consequently%2C%20the%20originally%20unreliable%20learning%20within%20the%0Anoisy%20zones%20is%20transformed%20into%20a%20correct%20and%20effective%20monotonicity%0Aoptimization.%20Moreover%2C%20an%20adaptive%20label%20correction%20is%20introduced%2C%20enabling%0AMonoBox%20to%20enhance%20the%20tightness%20of%20box%20annotations%20using%20predicted%20masks%20from%0Athe%20previous%20epoch%20and%20dynamically%20shrink%20the%20noisy%20zones%20as%20training%0Aprogresses.%20We%20verify%20MonoBox%20in%20the%20box-supervised%20segmentation%20task%20of%0Apolyps%2C%20where%20satisfying%20box-tightness%20is%20challenging%20due%20to%20the%20vague%0Aboundaries%20between%20the%20polyp%20and%20normal%20tissues.%20Experiments%20on%20both%20public%0Asynthetic%20and%20in-house%20real%20noisy%20datasets%20demonstrate%20that%20MonoBox%20exceeds%0Aother%20anti-noise%20state-of-the-arts%20by%20improving%20Dice%20by%20at%20least%205.5%25%20and%203.3%25%2C%0Arespectively.%20Codes%20are%20at%20https%3A//github.com/Huster-Hq/MonoBox.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01188v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonoBox%253A%2520Tightness-free%2520Box-supervised%2520Polyp%2520Segmentation%2520using%250A%2520%2520Monotonicity%2520Constraint%26entry.906535625%3DQiang%2520Hu%2520and%2520Zhenyu%2520Yi%2520and%2520Ying%2520Zhou%2520and%2520Ting%2520Li%2520and%2520Fan%2520Huang%2520and%2520Mei%2520Liu%2520and%2520Qiang%2520Li%2520and%2520Zhiwei%2520Wang%26entry.1292438233%3D%2520%2520We%2520propose%2520MonoBox%252C%2520an%2520innovative%2520box-supervised%2520segmentation%2520method%250Aconstrained%2520by%2520monotonicity%2520to%2520liberate%2520its%2520training%2520from%2520the%2520user-unfriendly%250Abox-tightness%2520assumption.%2520In%2520contrast%2520to%2520conventional%2520box-supervised%250Asegmentation%252C%2520where%2520the%2520box%2520edges%2520must%2520precisely%2520touch%2520the%2520target%2520boundaries%252C%250AMonoBox%2520leverages%2520imprecisely-annotated%2520boxes%2520to%2520achieve%2520robust%2520pixel-wise%250Asegmentation.%2520The%2520%2527linchpin%2527%2520is%2520that%252C%2520within%2520the%2520noisy%2520zones%2520around%2520box%2520edges%252C%250AMonoBox%2520discards%2520the%2520traditional%2520misguiding%2520multiple-instance%2520learning%2520loss%252C%250Aand%2520instead%2520optimizes%2520a%2520carefully-designed%2520objective%252C%2520termed%2520monotonicity%250Aconstraint.%2520Along%2520directions%2520transitioning%2520from%2520the%2520foreground%2520to%2520background%252C%250Athis%2520new%2520constraint%2520steers%2520responses%2520to%2520adhere%2520to%2520a%2520trend%2520of%2520monotonically%250Adecreasing%2520values.%2520Consequently%252C%2520the%2520originally%2520unreliable%2520learning%2520within%2520the%250Anoisy%2520zones%2520is%2520transformed%2520into%2520a%2520correct%2520and%2520effective%2520monotonicity%250Aoptimization.%2520Moreover%252C%2520an%2520adaptive%2520label%2520correction%2520is%2520introduced%252C%2520enabling%250AMonoBox%2520to%2520enhance%2520the%2520tightness%2520of%2520box%2520annotations%2520using%2520predicted%2520masks%2520from%250Athe%2520previous%2520epoch%2520and%2520dynamically%2520shrink%2520the%2520noisy%2520zones%2520as%2520training%250Aprogresses.%2520We%2520verify%2520MonoBox%2520in%2520the%2520box-supervised%2520segmentation%2520task%2520of%250Apolyps%252C%2520where%2520satisfying%2520box-tightness%2520is%2520challenging%2520due%2520to%2520the%2520vague%250Aboundaries%2520between%2520the%2520polyp%2520and%2520normal%2520tissues.%2520Experiments%2520on%2520both%2520public%250Asynthetic%2520and%2520in-house%2520real%2520noisy%2520datasets%2520demonstrate%2520that%2520MonoBox%2520exceeds%250Aother%2520anti-noise%2520state-of-the-arts%2520by%2520improving%2520Dice%2520by%2520at%2520least%25205.5%2525%2520and%25203.3%2525%252C%250Arespectively.%2520Codes%2520are%2520at%2520https%253A//github.com/Huster-Hq/MonoBox.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01188v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonoBox%3A%20Tightness-free%20Box-supervised%20Polyp%20Segmentation%20using%0A%20%20Monotonicity%20Constraint&entry.906535625=Qiang%20Hu%20and%20Zhenyu%20Yi%20and%20Ying%20Zhou%20and%20Ting%20Li%20and%20Fan%20Huang%20and%20Mei%20Liu%20and%20Qiang%20Li%20and%20Zhiwei%20Wang&entry.1292438233=%20%20We%20propose%20MonoBox%2C%20an%20innovative%20box-supervised%20segmentation%20method%0Aconstrained%20by%20monotonicity%20to%20liberate%20its%20training%20from%20the%20user-unfriendly%0Abox-tightness%20assumption.%20In%20contrast%20to%20conventional%20box-supervised%0Asegmentation%2C%20where%20the%20box%20edges%20must%20precisely%20touch%20the%20target%20boundaries%2C%0AMonoBox%20leverages%20imprecisely-annotated%20boxes%20to%20achieve%20robust%20pixel-wise%0Asegmentation.%20The%20%27linchpin%27%20is%20that%2C%20within%20the%20noisy%20zones%20around%20box%20edges%2C%0AMonoBox%20discards%20the%20traditional%20misguiding%20multiple-instance%20learning%20loss%2C%0Aand%20instead%20optimizes%20a%20carefully-designed%20objective%2C%20termed%20monotonicity%0Aconstraint.%20Along%20directions%20transitioning%20from%20the%20foreground%20to%20background%2C%0Athis%20new%20constraint%20steers%20responses%20to%20adhere%20to%20a%20trend%20of%20monotonically%0Adecreasing%20values.%20Consequently%2C%20the%20originally%20unreliable%20learning%20within%20the%0Anoisy%20zones%20is%20transformed%20into%20a%20correct%20and%20effective%20monotonicity%0Aoptimization.%20Moreover%2C%20an%20adaptive%20label%20correction%20is%20introduced%2C%20enabling%0AMonoBox%20to%20enhance%20the%20tightness%20of%20box%20annotations%20using%20predicted%20masks%20from%0Athe%20previous%20epoch%20and%20dynamically%20shrink%20the%20noisy%20zones%20as%20training%0Aprogresses.%20We%20verify%20MonoBox%20in%20the%20box-supervised%20segmentation%20task%20of%0Apolyps%2C%20where%20satisfying%20box-tightness%20is%20challenging%20due%20to%20the%20vague%0Aboundaries%20between%20the%20polyp%20and%20normal%20tissues.%20Experiments%20on%20both%20public%0Asynthetic%20and%20in-house%20real%20noisy%20datasets%20demonstrate%20that%20MonoBox%20exceeds%0Aother%20anti-noise%20state-of-the-arts%20by%20improving%20Dice%20by%20at%20least%205.5%25%20and%203.3%25%2C%0Arespectively.%20Codes%20are%20at%20https%3A//github.com/Huster-Hq/MonoBox.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01188v4&entry.124074799=Read"},
{"title": "CLUE: A Clinical Language Understanding Evaluation for LLMs", "author": "Amin Dada and Marie Bauer and Amanda Butler Contreras and Osman Alperen Kora\u015f and Constantin Marc Seibold and Kaleb E Smith and Jens Kleesiek", "abstract": "  Large Language Models (LLMs) are expected to significantly contribute to\npatient care, diagnostics, and administrative processes. Emerging biomedical\nLLMs aim to address healthcare-specific challenges, including privacy demands\nand computational constraints. Assessing the models' suitability for this\nsensitive application area is of the utmost importance. However, evaluation has\nprimarily been limited to non-clinical tasks, which do not reflect the\ncomplexity of practical clinical applications. To fill this gap, we present the\nClinical Language Understanding Evaluation (CLUE), a benchmark tailored to\nevaluate LLMs on clinical tasks. CLUE includes six tasks to test the practical\napplicability of LLMs in complex healthcare settings. Our evaluation includes a\ntotal of $25$ LLMs. In contrast to previous evaluations, CLUE shows a decrease\nin performance for nine out of twelve biomedical models. Our benchmark\nrepresents a step towards a standardized approach to evaluating and developing\nLLMs in healthcare to align future model development with the real-world needs\nof clinical application. We open-source all evaluation scripts and datasets for\nfuture research at https://github.com/TIO-IKIM/CLUE.\n", "link": "http://arxiv.org/abs/2404.04067v3", "date": "2024-06-24", "relevancy": 1.9401, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5373}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4986}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4506}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLUE%3A%20A%20Clinical%20Language%20Understanding%20Evaluation%20for%20LLMs&body=Title%3A%20CLUE%3A%20A%20Clinical%20Language%20Understanding%20Evaluation%20for%20LLMs%0AAuthor%3A%20Amin%20Dada%20and%20Marie%20Bauer%20and%20Amanda%20Butler%20Contreras%20and%20Osman%20Alperen%20Kora%C5%9F%20and%20Constantin%20Marc%20Seibold%20and%20Kaleb%20E%20Smith%20and%20Jens%20Kleesiek%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20expected%20to%20significantly%20contribute%20to%0Apatient%20care%2C%20diagnostics%2C%20and%20administrative%20processes.%20Emerging%20biomedical%0ALLMs%20aim%20to%20address%20healthcare-specific%20challenges%2C%20including%20privacy%20demands%0Aand%20computational%20constraints.%20Assessing%20the%20models%27%20suitability%20for%20this%0Asensitive%20application%20area%20is%20of%20the%20utmost%20importance.%20However%2C%20evaluation%20has%0Aprimarily%20been%20limited%20to%20non-clinical%20tasks%2C%20which%20do%20not%20reflect%20the%0Acomplexity%20of%20practical%20clinical%20applications.%20To%20fill%20this%20gap%2C%20we%20present%20the%0AClinical%20Language%20Understanding%20Evaluation%20%28CLUE%29%2C%20a%20benchmark%20tailored%20to%0Aevaluate%20LLMs%20on%20clinical%20tasks.%20CLUE%20includes%20six%20tasks%20to%20test%20the%20practical%0Aapplicability%20of%20LLMs%20in%20complex%20healthcare%20settings.%20Our%20evaluation%20includes%20a%0Atotal%20of%20%2425%24%20LLMs.%20In%20contrast%20to%20previous%20evaluations%2C%20CLUE%20shows%20a%20decrease%0Ain%20performance%20for%20nine%20out%20of%20twelve%20biomedical%20models.%20Our%20benchmark%0Arepresents%20a%20step%20towards%20a%20standardized%20approach%20to%20evaluating%20and%20developing%0ALLMs%20in%20healthcare%20to%20align%20future%20model%20development%20with%20the%20real-world%20needs%0Aof%20clinical%20application.%20We%20open-source%20all%20evaluation%20scripts%20and%20datasets%20for%0Afuture%20research%20at%20https%3A//github.com/TIO-IKIM/CLUE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04067v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLUE%253A%2520A%2520Clinical%2520Language%2520Understanding%2520Evaluation%2520for%2520LLMs%26entry.906535625%3DAmin%2520Dada%2520and%2520Marie%2520Bauer%2520and%2520Amanda%2520Butler%2520Contreras%2520and%2520Osman%2520Alperen%2520Kora%25C5%259F%2520and%2520Constantin%2520Marc%2520Seibold%2520and%2520Kaleb%2520E%2520Smith%2520and%2520Jens%2520Kleesiek%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520expected%2520to%2520significantly%2520contribute%2520to%250Apatient%2520care%252C%2520diagnostics%252C%2520and%2520administrative%2520processes.%2520Emerging%2520biomedical%250ALLMs%2520aim%2520to%2520address%2520healthcare-specific%2520challenges%252C%2520including%2520privacy%2520demands%250Aand%2520computational%2520constraints.%2520Assessing%2520the%2520models%2527%2520suitability%2520for%2520this%250Asensitive%2520application%2520area%2520is%2520of%2520the%2520utmost%2520importance.%2520However%252C%2520evaluation%2520has%250Aprimarily%2520been%2520limited%2520to%2520non-clinical%2520tasks%252C%2520which%2520do%2520not%2520reflect%2520the%250Acomplexity%2520of%2520practical%2520clinical%2520applications.%2520To%2520fill%2520this%2520gap%252C%2520we%2520present%2520the%250AClinical%2520Language%2520Understanding%2520Evaluation%2520%2528CLUE%2529%252C%2520a%2520benchmark%2520tailored%2520to%250Aevaluate%2520LLMs%2520on%2520clinical%2520tasks.%2520CLUE%2520includes%2520six%2520tasks%2520to%2520test%2520the%2520practical%250Aapplicability%2520of%2520LLMs%2520in%2520complex%2520healthcare%2520settings.%2520Our%2520evaluation%2520includes%2520a%250Atotal%2520of%2520%252425%2524%2520LLMs.%2520In%2520contrast%2520to%2520previous%2520evaluations%252C%2520CLUE%2520shows%2520a%2520decrease%250Ain%2520performance%2520for%2520nine%2520out%2520of%2520twelve%2520biomedical%2520models.%2520Our%2520benchmark%250Arepresents%2520a%2520step%2520towards%2520a%2520standardized%2520approach%2520to%2520evaluating%2520and%2520developing%250ALLMs%2520in%2520healthcare%2520to%2520align%2520future%2520model%2520development%2520with%2520the%2520real-world%2520needs%250Aof%2520clinical%2520application.%2520We%2520open-source%2520all%2520evaluation%2520scripts%2520and%2520datasets%2520for%250Afuture%2520research%2520at%2520https%253A//github.com/TIO-IKIM/CLUE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04067v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLUE%3A%20A%20Clinical%20Language%20Understanding%20Evaluation%20for%20LLMs&entry.906535625=Amin%20Dada%20and%20Marie%20Bauer%20and%20Amanda%20Butler%20Contreras%20and%20Osman%20Alperen%20Kora%C5%9F%20and%20Constantin%20Marc%20Seibold%20and%20Kaleb%20E%20Smith%20and%20Jens%20Kleesiek&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20expected%20to%20significantly%20contribute%20to%0Apatient%20care%2C%20diagnostics%2C%20and%20administrative%20processes.%20Emerging%20biomedical%0ALLMs%20aim%20to%20address%20healthcare-specific%20challenges%2C%20including%20privacy%20demands%0Aand%20computational%20constraints.%20Assessing%20the%20models%27%20suitability%20for%20this%0Asensitive%20application%20area%20is%20of%20the%20utmost%20importance.%20However%2C%20evaluation%20has%0Aprimarily%20been%20limited%20to%20non-clinical%20tasks%2C%20which%20do%20not%20reflect%20the%0Acomplexity%20of%20practical%20clinical%20applications.%20To%20fill%20this%20gap%2C%20we%20present%20the%0AClinical%20Language%20Understanding%20Evaluation%20%28CLUE%29%2C%20a%20benchmark%20tailored%20to%0Aevaluate%20LLMs%20on%20clinical%20tasks.%20CLUE%20includes%20six%20tasks%20to%20test%20the%20practical%0Aapplicability%20of%20LLMs%20in%20complex%20healthcare%20settings.%20Our%20evaluation%20includes%20a%0Atotal%20of%20%2425%24%20LLMs.%20In%20contrast%20to%20previous%20evaluations%2C%20CLUE%20shows%20a%20decrease%0Ain%20performance%20for%20nine%20out%20of%20twelve%20biomedical%20models.%20Our%20benchmark%0Arepresents%20a%20step%20towards%20a%20standardized%20approach%20to%20evaluating%20and%20developing%0ALLMs%20in%20healthcare%20to%20align%20future%20model%20development%20with%20the%20real-world%20needs%0Aof%20clinical%20application.%20We%20open-source%20all%20evaluation%20scripts%20and%20datasets%20for%0Afuture%20research%20at%20https%3A//github.com/TIO-IKIM/CLUE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04067v3&entry.124074799=Read"},
{"title": "A Survey of Large Language Models for Graphs", "author": "Xubin Ren and Jiabin Tang and Dawei Yin and Nitesh Chawla and Chao Huang", "abstract": "  Graphs are an essential data structure utilized to represent relationships in\nreal-world scenarios. Prior research has established that Graph Neural Networks\n(GNNs) deliver impressive outcomes in graph-centric tasks, such as link\nprediction and node classification. Despite these advancements, challenges like\ndata sparsity and limited generalization capabilities continue to persist.\nRecently, Large Language Models (LLMs) have gained attention in natural\nlanguage processing. They excel in language comprehension and summarization.\nIntegrating LLMs with graph learning techniques has attracted interest as a way\nto enhance performance in graph learning tasks. In this survey, we conduct an\nin-depth review of the latest state-of-the-art LLMs applied in graph learning\nand introduce a novel taxonomy to categorize existing methods based on their\nframework design. We detail four unique designs: i) GNNs as Prefix, ii) LLMs as\nPrefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key\nmethodologies within each category. We explore the strengths and limitations of\neach framework, and emphasize potential avenues for future research, including\novercoming current integration challenges between LLMs and graph learning\ntechniques, and venturing into new application areas. This survey aims to serve\nas a valuable resource for researchers and practitioners eager to leverage\nlarge language models in graph learning, and to inspire continued progress in\nthis dynamic field. We consistently maintain the related open-source materials\nat \\url{https://github.com/HKUDS/Awesome-LLM4Graph-Papers}.\n", "link": "http://arxiv.org/abs/2405.08011v2", "date": "2024-06-24", "relevancy": 1.9386, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5254}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4556}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Large%20Language%20Models%20for%20Graphs&body=Title%3A%20A%20Survey%20of%20Large%20Language%20Models%20for%20Graphs%0AAuthor%3A%20Xubin%20Ren%20and%20Jiabin%20Tang%20and%20Dawei%20Yin%20and%20Nitesh%20Chawla%20and%20Chao%20Huang%0AAbstract%3A%20%20%20Graphs%20are%20an%20essential%20data%20structure%20utilized%20to%20represent%20relationships%20in%0Areal-world%20scenarios.%20Prior%20research%20has%20established%20that%20Graph%20Neural%20Networks%0A%28GNNs%29%20deliver%20impressive%20outcomes%20in%20graph-centric%20tasks%2C%20such%20as%20link%0Aprediction%20and%20node%20classification.%20Despite%20these%20advancements%2C%20challenges%20like%0Adata%20sparsity%20and%20limited%20generalization%20capabilities%20continue%20to%20persist.%0ARecently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20gained%20attention%20in%20natural%0Alanguage%20processing.%20They%20excel%20in%20language%20comprehension%20and%20summarization.%0AIntegrating%20LLMs%20with%20graph%20learning%20techniques%20has%20attracted%20interest%20as%20a%20way%0Ato%20enhance%20performance%20in%20graph%20learning%20tasks.%20In%20this%20survey%2C%20we%20conduct%20an%0Ain-depth%20review%20of%20the%20latest%20state-of-the-art%20LLMs%20applied%20in%20graph%20learning%0Aand%20introduce%20a%20novel%20taxonomy%20to%20categorize%20existing%20methods%20based%20on%20their%0Aframework%20design.%20We%20detail%20four%20unique%20designs%3A%20i%29%20GNNs%20as%20Prefix%2C%20ii%29%20LLMs%20as%0APrefix%2C%20iii%29%20LLMs-Graphs%20Integration%2C%20and%20iv%29%20LLMs-Only%2C%20highlighting%20key%0Amethodologies%20within%20each%20category.%20We%20explore%20the%20strengths%20and%20limitations%20of%0Aeach%20framework%2C%20and%20emphasize%20potential%20avenues%20for%20future%20research%2C%20including%0Aovercoming%20current%20integration%20challenges%20between%20LLMs%20and%20graph%20learning%0Atechniques%2C%20and%20venturing%20into%20new%20application%20areas.%20This%20survey%20aims%20to%20serve%0Aas%20a%20valuable%20resource%20for%20researchers%20and%20practitioners%20eager%20to%20leverage%0Alarge%20language%20models%20in%20graph%20learning%2C%20and%20to%20inspire%20continued%20progress%20in%0Athis%20dynamic%20field.%20We%20consistently%20maintain%20the%20related%20open-source%20materials%0Aat%20%5Curl%7Bhttps%3A//github.com/HKUDS/Awesome-LLM4Graph-Papers%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.08011v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Large%2520Language%2520Models%2520for%2520Graphs%26entry.906535625%3DXubin%2520Ren%2520and%2520Jiabin%2520Tang%2520and%2520Dawei%2520Yin%2520and%2520Nitesh%2520Chawla%2520and%2520Chao%2520Huang%26entry.1292438233%3D%2520%2520Graphs%2520are%2520an%2520essential%2520data%2520structure%2520utilized%2520to%2520represent%2520relationships%2520in%250Areal-world%2520scenarios.%2520Prior%2520research%2520has%2520established%2520that%2520Graph%2520Neural%2520Networks%250A%2528GNNs%2529%2520deliver%2520impressive%2520outcomes%2520in%2520graph-centric%2520tasks%252C%2520such%2520as%2520link%250Aprediction%2520and%2520node%2520classification.%2520Despite%2520these%2520advancements%252C%2520challenges%2520like%250Adata%2520sparsity%2520and%2520limited%2520generalization%2520capabilities%2520continue%2520to%2520persist.%250ARecently%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520gained%2520attention%2520in%2520natural%250Alanguage%2520processing.%2520They%2520excel%2520in%2520language%2520comprehension%2520and%2520summarization.%250AIntegrating%2520LLMs%2520with%2520graph%2520learning%2520techniques%2520has%2520attracted%2520interest%2520as%2520a%2520way%250Ato%2520enhance%2520performance%2520in%2520graph%2520learning%2520tasks.%2520In%2520this%2520survey%252C%2520we%2520conduct%2520an%250Ain-depth%2520review%2520of%2520the%2520latest%2520state-of-the-art%2520LLMs%2520applied%2520in%2520graph%2520learning%250Aand%2520introduce%2520a%2520novel%2520taxonomy%2520to%2520categorize%2520existing%2520methods%2520based%2520on%2520their%250Aframework%2520design.%2520We%2520detail%2520four%2520unique%2520designs%253A%2520i%2529%2520GNNs%2520as%2520Prefix%252C%2520ii%2529%2520LLMs%2520as%250APrefix%252C%2520iii%2529%2520LLMs-Graphs%2520Integration%252C%2520and%2520iv%2529%2520LLMs-Only%252C%2520highlighting%2520key%250Amethodologies%2520within%2520each%2520category.%2520We%2520explore%2520the%2520strengths%2520and%2520limitations%2520of%250Aeach%2520framework%252C%2520and%2520emphasize%2520potential%2520avenues%2520for%2520future%2520research%252C%2520including%250Aovercoming%2520current%2520integration%2520challenges%2520between%2520LLMs%2520and%2520graph%2520learning%250Atechniques%252C%2520and%2520venturing%2520into%2520new%2520application%2520areas.%2520This%2520survey%2520aims%2520to%2520serve%250Aas%2520a%2520valuable%2520resource%2520for%2520researchers%2520and%2520practitioners%2520eager%2520to%2520leverage%250Alarge%2520language%2520models%2520in%2520graph%2520learning%252C%2520and%2520to%2520inspire%2520continued%2520progress%2520in%250Athis%2520dynamic%2520field.%2520We%2520consistently%2520maintain%2520the%2520related%2520open-source%2520materials%250Aat%2520%255Curl%257Bhttps%253A//github.com/HKUDS/Awesome-LLM4Graph-Papers%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.08011v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Large%20Language%20Models%20for%20Graphs&entry.906535625=Xubin%20Ren%20and%20Jiabin%20Tang%20and%20Dawei%20Yin%20and%20Nitesh%20Chawla%20and%20Chao%20Huang&entry.1292438233=%20%20Graphs%20are%20an%20essential%20data%20structure%20utilized%20to%20represent%20relationships%20in%0Areal-world%20scenarios.%20Prior%20research%20has%20established%20that%20Graph%20Neural%20Networks%0A%28GNNs%29%20deliver%20impressive%20outcomes%20in%20graph-centric%20tasks%2C%20such%20as%20link%0Aprediction%20and%20node%20classification.%20Despite%20these%20advancements%2C%20challenges%20like%0Adata%20sparsity%20and%20limited%20generalization%20capabilities%20continue%20to%20persist.%0ARecently%2C%20Large%20Language%20Models%20%28LLMs%29%20have%20gained%20attention%20in%20natural%0Alanguage%20processing.%20They%20excel%20in%20language%20comprehension%20and%20summarization.%0AIntegrating%20LLMs%20with%20graph%20learning%20techniques%20has%20attracted%20interest%20as%20a%20way%0Ato%20enhance%20performance%20in%20graph%20learning%20tasks.%20In%20this%20survey%2C%20we%20conduct%20an%0Ain-depth%20review%20of%20the%20latest%20state-of-the-art%20LLMs%20applied%20in%20graph%20learning%0Aand%20introduce%20a%20novel%20taxonomy%20to%20categorize%20existing%20methods%20based%20on%20their%0Aframework%20design.%20We%20detail%20four%20unique%20designs%3A%20i%29%20GNNs%20as%20Prefix%2C%20ii%29%20LLMs%20as%0APrefix%2C%20iii%29%20LLMs-Graphs%20Integration%2C%20and%20iv%29%20LLMs-Only%2C%20highlighting%20key%0Amethodologies%20within%20each%20category.%20We%20explore%20the%20strengths%20and%20limitations%20of%0Aeach%20framework%2C%20and%20emphasize%20potential%20avenues%20for%20future%20research%2C%20including%0Aovercoming%20current%20integration%20challenges%20between%20LLMs%20and%20graph%20learning%0Atechniques%2C%20and%20venturing%20into%20new%20application%20areas.%20This%20survey%20aims%20to%20serve%0Aas%20a%20valuable%20resource%20for%20researchers%20and%20practitioners%20eager%20to%20leverage%0Alarge%20language%20models%20in%20graph%20learning%2C%20and%20to%20inspire%20continued%20progress%20in%0Athis%20dynamic%20field.%20We%20consistently%20maintain%20the%20related%20open-source%20materials%0Aat%20%5Curl%7Bhttps%3A//github.com/HKUDS/Awesome-LLM4Graph-Papers%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.08011v2&entry.124074799=Read"},
{"title": "The MRI Scanner as a Diagnostic: Image-less Active Sampling", "author": "Yuning Du and Rohan Dharmakumar and Sotirios A. Tsaftaris", "abstract": "  Despite the high diagnostic accuracy of Magnetic Resonance Imaging (MRI),\nusing MRI as a Point-of-Care (POC) disease identification tool poses\nsignificant accessibility challenges due to the use of high magnetic field\nstrength and lengthy acquisition times. We ask a simple question: Can we\ndynamically optimise acquired samples, at the patient level, according to an\n(automated) downstream decision task, while discounting image reconstruction?\nWe propose an ML-based framework that learns an active sampling strategy, via\nreinforcement learning, at a patient-level to directly infer disease from\nundersampled k-space. We validate our approach by inferring Meniscus Tear in\nundersampled knee MRI data, where we achieve diagnostic performance comparable\nwith ML-based diagnosis, using fully sampled k-space data. We analyse\ntask-specific sampling policies, showcasing the adaptability of our active\nsampling approach. The introduced frugal sampling strategies have the potential\nto reduce high field strength requirements that in turn strengthen the\nviability of MRI-based POC disease identification and associated preliminary\nscreening tools.\n", "link": "http://arxiv.org/abs/2406.16754v1", "date": "2024-06-24", "relevancy": 1.9314, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4829}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4829}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20MRI%20Scanner%20as%20a%20Diagnostic%3A%20Image-less%20Active%20Sampling&body=Title%3A%20The%20MRI%20Scanner%20as%20a%20Diagnostic%3A%20Image-less%20Active%20Sampling%0AAuthor%3A%20Yuning%20Du%20and%20Rohan%20Dharmakumar%20and%20Sotirios%20A.%20Tsaftaris%0AAbstract%3A%20%20%20Despite%20the%20high%20diagnostic%20accuracy%20of%20Magnetic%20Resonance%20Imaging%20%28MRI%29%2C%0Ausing%20MRI%20as%20a%20Point-of-Care%20%28POC%29%20disease%20identification%20tool%20poses%0Asignificant%20accessibility%20challenges%20due%20to%20the%20use%20of%20high%20magnetic%20field%0Astrength%20and%20lengthy%20acquisition%20times.%20We%20ask%20a%20simple%20question%3A%20Can%20we%0Adynamically%20optimise%20acquired%20samples%2C%20at%20the%20patient%20level%2C%20according%20to%20an%0A%28automated%29%20downstream%20decision%20task%2C%20while%20discounting%20image%20reconstruction%3F%0AWe%20propose%20an%20ML-based%20framework%20that%20learns%20an%20active%20sampling%20strategy%2C%20via%0Areinforcement%20learning%2C%20at%20a%20patient-level%20to%20directly%20infer%20disease%20from%0Aundersampled%20k-space.%20We%20validate%20our%20approach%20by%20inferring%20Meniscus%20Tear%20in%0Aundersampled%20knee%20MRI%20data%2C%20where%20we%20achieve%20diagnostic%20performance%20comparable%0Awith%20ML-based%20diagnosis%2C%20using%20fully%20sampled%20k-space%20data.%20We%20analyse%0Atask-specific%20sampling%20policies%2C%20showcasing%20the%20adaptability%20of%20our%20active%0Asampling%20approach.%20The%20introduced%20frugal%20sampling%20strategies%20have%20the%20potential%0Ato%20reduce%20high%20field%20strength%20requirements%20that%20in%20turn%20strengthen%20the%0Aviability%20of%20MRI-based%20POC%20disease%20identification%20and%20associated%20preliminary%0Ascreening%20tools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16754v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520MRI%2520Scanner%2520as%2520a%2520Diagnostic%253A%2520Image-less%2520Active%2520Sampling%26entry.906535625%3DYuning%2520Du%2520and%2520Rohan%2520Dharmakumar%2520and%2520Sotirios%2520A.%2520Tsaftaris%26entry.1292438233%3D%2520%2520Despite%2520the%2520high%2520diagnostic%2520accuracy%2520of%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%252C%250Ausing%2520MRI%2520as%2520a%2520Point-of-Care%2520%2528POC%2529%2520disease%2520identification%2520tool%2520poses%250Asignificant%2520accessibility%2520challenges%2520due%2520to%2520the%2520use%2520of%2520high%2520magnetic%2520field%250Astrength%2520and%2520lengthy%2520acquisition%2520times.%2520We%2520ask%2520a%2520simple%2520question%253A%2520Can%2520we%250Adynamically%2520optimise%2520acquired%2520samples%252C%2520at%2520the%2520patient%2520level%252C%2520according%2520to%2520an%250A%2528automated%2529%2520downstream%2520decision%2520task%252C%2520while%2520discounting%2520image%2520reconstruction%253F%250AWe%2520propose%2520an%2520ML-based%2520framework%2520that%2520learns%2520an%2520active%2520sampling%2520strategy%252C%2520via%250Areinforcement%2520learning%252C%2520at%2520a%2520patient-level%2520to%2520directly%2520infer%2520disease%2520from%250Aundersampled%2520k-space.%2520We%2520validate%2520our%2520approach%2520by%2520inferring%2520Meniscus%2520Tear%2520in%250Aundersampled%2520knee%2520MRI%2520data%252C%2520where%2520we%2520achieve%2520diagnostic%2520performance%2520comparable%250Awith%2520ML-based%2520diagnosis%252C%2520using%2520fully%2520sampled%2520k-space%2520data.%2520We%2520analyse%250Atask-specific%2520sampling%2520policies%252C%2520showcasing%2520the%2520adaptability%2520of%2520our%2520active%250Asampling%2520approach.%2520The%2520introduced%2520frugal%2520sampling%2520strategies%2520have%2520the%2520potential%250Ato%2520reduce%2520high%2520field%2520strength%2520requirements%2520that%2520in%2520turn%2520strengthen%2520the%250Aviability%2520of%2520MRI-based%2520POC%2520disease%2520identification%2520and%2520associated%2520preliminary%250Ascreening%2520tools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16754v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20MRI%20Scanner%20as%20a%20Diagnostic%3A%20Image-less%20Active%20Sampling&entry.906535625=Yuning%20Du%20and%20Rohan%20Dharmakumar%20and%20Sotirios%20A.%20Tsaftaris&entry.1292438233=%20%20Despite%20the%20high%20diagnostic%20accuracy%20of%20Magnetic%20Resonance%20Imaging%20%28MRI%29%2C%0Ausing%20MRI%20as%20a%20Point-of-Care%20%28POC%29%20disease%20identification%20tool%20poses%0Asignificant%20accessibility%20challenges%20due%20to%20the%20use%20of%20high%20magnetic%20field%0Astrength%20and%20lengthy%20acquisition%20times.%20We%20ask%20a%20simple%20question%3A%20Can%20we%0Adynamically%20optimise%20acquired%20samples%2C%20at%20the%20patient%20level%2C%20according%20to%20an%0A%28automated%29%20downstream%20decision%20task%2C%20while%20discounting%20image%20reconstruction%3F%0AWe%20propose%20an%20ML-based%20framework%20that%20learns%20an%20active%20sampling%20strategy%2C%20via%0Areinforcement%20learning%2C%20at%20a%20patient-level%20to%20directly%20infer%20disease%20from%0Aundersampled%20k-space.%20We%20validate%20our%20approach%20by%20inferring%20Meniscus%20Tear%20in%0Aundersampled%20knee%20MRI%20data%2C%20where%20we%20achieve%20diagnostic%20performance%20comparable%0Awith%20ML-based%20diagnosis%2C%20using%20fully%20sampled%20k-space%20data.%20We%20analyse%0Atask-specific%20sampling%20policies%2C%20showcasing%20the%20adaptability%20of%20our%20active%0Asampling%20approach.%20The%20introduced%20frugal%20sampling%20strategies%20have%20the%20potential%0Ato%20reduce%20high%20field%20strength%20requirements%20that%20in%20turn%20strengthen%20the%0Aviability%20of%20MRI-based%20POC%20disease%20identification%20and%20associated%20preliminary%0Ascreening%20tools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16754v1&entry.124074799=Read"},
{"title": "Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich\n  Reasoning", "author": "Hanqi Yan and Qinglin Zhu and Xinyu Wang and Lin Gui and Yulan He", "abstract": "  While Large language models (LLMs) have the capability to iteratively reflect\non their own outputs, recent studies have observed their struggles with\nknowledge-rich problems without access to external resources. In addition to\nthe inefficiency of LLMs in self-assessment, we also observe that LLMs struggle\nto revisit their predictions despite receiving explicit negative feedback.\nTherefore, We propose Mirror, a Multiple-perspective self-reflection method for\nknowledge-rich reasoning, to avoid getting stuck at a particular reflection\niteration. Mirror enables LLMs to reflect from multiple-perspective clues,\nachieved through a heuristic interaction between a Navigator and a Reasoner. It\nguides agents toward diverse yet plausibly reliable reasoning trajectory\nwithout access to ground truth by encouraging (1) diversity of directions\ngenerated by Navigator and (2) agreement among strategically induced\nperturbations in responses generated by the Reasoner. The experiments on five\nreasoning datasets demonstrate that Mirror's superiority over several\ncontemporary self-reflection approaches. Additionally, the ablation study\nstudies clearly indicate that our strategies alleviate the aforementioned\nchallenges.\n", "link": "http://arxiv.org/abs/2402.14963v2", "date": "2024-06-24", "relevancy": 1.9298, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4959}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4749}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mirror%3A%20A%20Multiple-perspective%20Self-Reflection%20Method%20for%20Knowledge-rich%0A%20%20Reasoning&body=Title%3A%20Mirror%3A%20A%20Multiple-perspective%20Self-Reflection%20Method%20for%20Knowledge-rich%0A%20%20Reasoning%0AAuthor%3A%20Hanqi%20Yan%20and%20Qinglin%20Zhu%20and%20Xinyu%20Wang%20and%20Lin%20Gui%20and%20Yulan%20He%0AAbstract%3A%20%20%20While%20Large%20language%20models%20%28LLMs%29%20have%20the%20capability%20to%20iteratively%20reflect%0Aon%20their%20own%20outputs%2C%20recent%20studies%20have%20observed%20their%20struggles%20with%0Aknowledge-rich%20problems%20without%20access%20to%20external%20resources.%20In%20addition%20to%0Athe%20inefficiency%20of%20LLMs%20in%20self-assessment%2C%20we%20also%20observe%20that%20LLMs%20struggle%0Ato%20revisit%20their%20predictions%20despite%20receiving%20explicit%20negative%20feedback.%0ATherefore%2C%20We%20propose%20Mirror%2C%20a%20Multiple-perspective%20self-reflection%20method%20for%0Aknowledge-rich%20reasoning%2C%20to%20avoid%20getting%20stuck%20at%20a%20particular%20reflection%0Aiteration.%20Mirror%20enables%20LLMs%20to%20reflect%20from%20multiple-perspective%20clues%2C%0Aachieved%20through%20a%20heuristic%20interaction%20between%20a%20Navigator%20and%20a%20Reasoner.%20It%0Aguides%20agents%20toward%20diverse%20yet%20plausibly%20reliable%20reasoning%20trajectory%0Awithout%20access%20to%20ground%20truth%20by%20encouraging%20%281%29%20diversity%20of%20directions%0Agenerated%20by%20Navigator%20and%20%282%29%20agreement%20among%20strategically%20induced%0Aperturbations%20in%20responses%20generated%20by%20the%20Reasoner.%20The%20experiments%20on%20five%0Areasoning%20datasets%20demonstrate%20that%20Mirror%27s%20superiority%20over%20several%0Acontemporary%20self-reflection%20approaches.%20Additionally%2C%20the%20ablation%20study%0Astudies%20clearly%20indicate%20that%20our%20strategies%20alleviate%20the%20aforementioned%0Achallenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14963v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMirror%253A%2520A%2520Multiple-perspective%2520Self-Reflection%2520Method%2520for%2520Knowledge-rich%250A%2520%2520Reasoning%26entry.906535625%3DHanqi%2520Yan%2520and%2520Qinglin%2520Zhu%2520and%2520Xinyu%2520Wang%2520and%2520Lin%2520Gui%2520and%2520Yulan%2520He%26entry.1292438233%3D%2520%2520While%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520the%2520capability%2520to%2520iteratively%2520reflect%250Aon%2520their%2520own%2520outputs%252C%2520recent%2520studies%2520have%2520observed%2520their%2520struggles%2520with%250Aknowledge-rich%2520problems%2520without%2520access%2520to%2520external%2520resources.%2520In%2520addition%2520to%250Athe%2520inefficiency%2520of%2520LLMs%2520in%2520self-assessment%252C%2520we%2520also%2520observe%2520that%2520LLMs%2520struggle%250Ato%2520revisit%2520their%2520predictions%2520despite%2520receiving%2520explicit%2520negative%2520feedback.%250ATherefore%252C%2520We%2520propose%2520Mirror%252C%2520a%2520Multiple-perspective%2520self-reflection%2520method%2520for%250Aknowledge-rich%2520reasoning%252C%2520to%2520avoid%2520getting%2520stuck%2520at%2520a%2520particular%2520reflection%250Aiteration.%2520Mirror%2520enables%2520LLMs%2520to%2520reflect%2520from%2520multiple-perspective%2520clues%252C%250Aachieved%2520through%2520a%2520heuristic%2520interaction%2520between%2520a%2520Navigator%2520and%2520a%2520Reasoner.%2520It%250Aguides%2520agents%2520toward%2520diverse%2520yet%2520plausibly%2520reliable%2520reasoning%2520trajectory%250Awithout%2520access%2520to%2520ground%2520truth%2520by%2520encouraging%2520%25281%2529%2520diversity%2520of%2520directions%250Agenerated%2520by%2520Navigator%2520and%2520%25282%2529%2520agreement%2520among%2520strategically%2520induced%250Aperturbations%2520in%2520responses%2520generated%2520by%2520the%2520Reasoner.%2520The%2520experiments%2520on%2520five%250Areasoning%2520datasets%2520demonstrate%2520that%2520Mirror%2527s%2520superiority%2520over%2520several%250Acontemporary%2520self-reflection%2520approaches.%2520Additionally%252C%2520the%2520ablation%2520study%250Astudies%2520clearly%2520indicate%2520that%2520our%2520strategies%2520alleviate%2520the%2520aforementioned%250Achallenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14963v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mirror%3A%20A%20Multiple-perspective%20Self-Reflection%20Method%20for%20Knowledge-rich%0A%20%20Reasoning&entry.906535625=Hanqi%20Yan%20and%20Qinglin%20Zhu%20and%20Xinyu%20Wang%20and%20Lin%20Gui%20and%20Yulan%20He&entry.1292438233=%20%20While%20Large%20language%20models%20%28LLMs%29%20have%20the%20capability%20to%20iteratively%20reflect%0Aon%20their%20own%20outputs%2C%20recent%20studies%20have%20observed%20their%20struggles%20with%0Aknowledge-rich%20problems%20without%20access%20to%20external%20resources.%20In%20addition%20to%0Athe%20inefficiency%20of%20LLMs%20in%20self-assessment%2C%20we%20also%20observe%20that%20LLMs%20struggle%0Ato%20revisit%20their%20predictions%20despite%20receiving%20explicit%20negative%20feedback.%0ATherefore%2C%20We%20propose%20Mirror%2C%20a%20Multiple-perspective%20self-reflection%20method%20for%0Aknowledge-rich%20reasoning%2C%20to%20avoid%20getting%20stuck%20at%20a%20particular%20reflection%0Aiteration.%20Mirror%20enables%20LLMs%20to%20reflect%20from%20multiple-perspective%20clues%2C%0Aachieved%20through%20a%20heuristic%20interaction%20between%20a%20Navigator%20and%20a%20Reasoner.%20It%0Aguides%20agents%20toward%20diverse%20yet%20plausibly%20reliable%20reasoning%20trajectory%0Awithout%20access%20to%20ground%20truth%20by%20encouraging%20%281%29%20diversity%20of%20directions%0Agenerated%20by%20Navigator%20and%20%282%29%20agreement%20among%20strategically%20induced%0Aperturbations%20in%20responses%20generated%20by%20the%20Reasoner.%20The%20experiments%20on%20five%0Areasoning%20datasets%20demonstrate%20that%20Mirror%27s%20superiority%20over%20several%0Acontemporary%20self-reflection%20approaches.%20Additionally%2C%20the%20ablation%20study%0Astudies%20clearly%20indicate%20that%20our%20strategies%20alleviate%20the%20aforementioned%0Achallenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14963v2&entry.124074799=Read"},
{"title": "Link Prediction with Untrained Message Passing Layers", "author": "Lisi Qarkaxhija and Anatol E. Wegner and Ingo Scholtes", "abstract": "  Message passing neural networks (MPNNs) operate on graphs by exchanging\ninformation between neigbouring nodes. MPNNs have been successfully applied to\nvarious node-, edge-, and graph-level tasks in areas like molecular science,\ncomputer vision, natural language processing, and combinatorial optimization.\nHowever, most MPNNs require training on large amounts of labeled data, which\ncan be costly and time-consuming. In this work, we explore the use of various\nuntrained message passing layers in graph neural networks, i.e. variants of\npopular message passing architecture where we remove all trainable parameters\nthat are used to transform node features in the message passing step. Focusing\non link prediction, we find that untrained message passing layers can lead to\ncompetitive and even superior performance compared to fully trained MPNNs,\nespecially in the presence of high-dimensional features. We provide a\ntheoretical analysis of untrained message passing by relating the inner\nproducts of features implicitly produced by untrained message passing layers to\npath-based topological node similarity measures. As such, untrained message\npassing architectures can be viewed as a highly efficient and interpretable\napproach to link prediction.\n", "link": "http://arxiv.org/abs/2406.16687v1", "date": "2024-06-24", "relevancy": 1.929, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4891}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4827}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Link%20Prediction%20with%20Untrained%20Message%20Passing%20Layers&body=Title%3A%20Link%20Prediction%20with%20Untrained%20Message%20Passing%20Layers%0AAuthor%3A%20Lisi%20Qarkaxhija%20and%20Anatol%20E.%20Wegner%20and%20Ingo%20Scholtes%0AAbstract%3A%20%20%20Message%20passing%20neural%20networks%20%28MPNNs%29%20operate%20on%20graphs%20by%20exchanging%0Ainformation%20between%20neigbouring%20nodes.%20MPNNs%20have%20been%20successfully%20applied%20to%0Avarious%20node-%2C%20edge-%2C%20and%20graph-level%20tasks%20in%20areas%20like%20molecular%20science%2C%0Acomputer%20vision%2C%20natural%20language%20processing%2C%20and%20combinatorial%20optimization.%0AHowever%2C%20most%20MPNNs%20require%20training%20on%20large%20amounts%20of%20labeled%20data%2C%20which%0Acan%20be%20costly%20and%20time-consuming.%20In%20this%20work%2C%20we%20explore%20the%20use%20of%20various%0Auntrained%20message%20passing%20layers%20in%20graph%20neural%20networks%2C%20i.e.%20variants%20of%0Apopular%20message%20passing%20architecture%20where%20we%20remove%20all%20trainable%20parameters%0Athat%20are%20used%20to%20transform%20node%20features%20in%20the%20message%20passing%20step.%20Focusing%0Aon%20link%20prediction%2C%20we%20find%20that%20untrained%20message%20passing%20layers%20can%20lead%20to%0Acompetitive%20and%20even%20superior%20performance%20compared%20to%20fully%20trained%20MPNNs%2C%0Aespecially%20in%20the%20presence%20of%20high-dimensional%20features.%20We%20provide%20a%0Atheoretical%20analysis%20of%20untrained%20message%20passing%20by%20relating%20the%20inner%0Aproducts%20of%20features%20implicitly%20produced%20by%20untrained%20message%20passing%20layers%20to%0Apath-based%20topological%20node%20similarity%20measures.%20As%20such%2C%20untrained%20message%0Apassing%20architectures%20can%20be%20viewed%20as%20a%20highly%20efficient%20and%20interpretable%0Aapproach%20to%20link%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLink%2520Prediction%2520with%2520Untrained%2520Message%2520Passing%2520Layers%26entry.906535625%3DLisi%2520Qarkaxhija%2520and%2520Anatol%2520E.%2520Wegner%2520and%2520Ingo%2520Scholtes%26entry.1292438233%3D%2520%2520Message%2520passing%2520neural%2520networks%2520%2528MPNNs%2529%2520operate%2520on%2520graphs%2520by%2520exchanging%250Ainformation%2520between%2520neigbouring%2520nodes.%2520MPNNs%2520have%2520been%2520successfully%2520applied%2520to%250Avarious%2520node-%252C%2520edge-%252C%2520and%2520graph-level%2520tasks%2520in%2520areas%2520like%2520molecular%2520science%252C%250Acomputer%2520vision%252C%2520natural%2520language%2520processing%252C%2520and%2520combinatorial%2520optimization.%250AHowever%252C%2520most%2520MPNNs%2520require%2520training%2520on%2520large%2520amounts%2520of%2520labeled%2520data%252C%2520which%250Acan%2520be%2520costly%2520and%2520time-consuming.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520use%2520of%2520various%250Auntrained%2520message%2520passing%2520layers%2520in%2520graph%2520neural%2520networks%252C%2520i.e.%2520variants%2520of%250Apopular%2520message%2520passing%2520architecture%2520where%2520we%2520remove%2520all%2520trainable%2520parameters%250Athat%2520are%2520used%2520to%2520transform%2520node%2520features%2520in%2520the%2520message%2520passing%2520step.%2520Focusing%250Aon%2520link%2520prediction%252C%2520we%2520find%2520that%2520untrained%2520message%2520passing%2520layers%2520can%2520lead%2520to%250Acompetitive%2520and%2520even%2520superior%2520performance%2520compared%2520to%2520fully%2520trained%2520MPNNs%252C%250Aespecially%2520in%2520the%2520presence%2520of%2520high-dimensional%2520features.%2520We%2520provide%2520a%250Atheoretical%2520analysis%2520of%2520untrained%2520message%2520passing%2520by%2520relating%2520the%2520inner%250Aproducts%2520of%2520features%2520implicitly%2520produced%2520by%2520untrained%2520message%2520passing%2520layers%2520to%250Apath-based%2520topological%2520node%2520similarity%2520measures.%2520As%2520such%252C%2520untrained%2520message%250Apassing%2520architectures%2520can%2520be%2520viewed%2520as%2520a%2520highly%2520efficient%2520and%2520interpretable%250Aapproach%2520to%2520link%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Link%20Prediction%20with%20Untrained%20Message%20Passing%20Layers&entry.906535625=Lisi%20Qarkaxhija%20and%20Anatol%20E.%20Wegner%20and%20Ingo%20Scholtes&entry.1292438233=%20%20Message%20passing%20neural%20networks%20%28MPNNs%29%20operate%20on%20graphs%20by%20exchanging%0Ainformation%20between%20neigbouring%20nodes.%20MPNNs%20have%20been%20successfully%20applied%20to%0Avarious%20node-%2C%20edge-%2C%20and%20graph-level%20tasks%20in%20areas%20like%20molecular%20science%2C%0Acomputer%20vision%2C%20natural%20language%20processing%2C%20and%20combinatorial%20optimization.%0AHowever%2C%20most%20MPNNs%20require%20training%20on%20large%20amounts%20of%20labeled%20data%2C%20which%0Acan%20be%20costly%20and%20time-consuming.%20In%20this%20work%2C%20we%20explore%20the%20use%20of%20various%0Auntrained%20message%20passing%20layers%20in%20graph%20neural%20networks%2C%20i.e.%20variants%20of%0Apopular%20message%20passing%20architecture%20where%20we%20remove%20all%20trainable%20parameters%0Athat%20are%20used%20to%20transform%20node%20features%20in%20the%20message%20passing%20step.%20Focusing%0Aon%20link%20prediction%2C%20we%20find%20that%20untrained%20message%20passing%20layers%20can%20lead%20to%0Acompetitive%20and%20even%20superior%20performance%20compared%20to%20fully%20trained%20MPNNs%2C%0Aespecially%20in%20the%20presence%20of%20high-dimensional%20features.%20We%20provide%20a%0Atheoretical%20analysis%20of%20untrained%20message%20passing%20by%20relating%20the%20inner%0Aproducts%20of%20features%20implicitly%20produced%20by%20untrained%20message%20passing%20layers%20to%0Apath-based%20topological%20node%20similarity%20measures.%20As%20such%2C%20untrained%20message%0Apassing%20architectures%20can%20be%20viewed%20as%20a%20highly%20efficient%20and%20interpretable%0Aapproach%20to%20link%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16687v1&entry.124074799=Read"},
{"title": "CausalFormer: An Interpretable Transformer for Temporal Causal Discovery", "author": "Lingbai Kong and Wengen Li and Hanchen Yang and Yichao Zhang and Jihong Guan and Shuigeng Zhou", "abstract": "  Temporal causal discovery is a crucial task aimed at uncovering the causal\nrelations within time series data. The latest temporal causal discovery methods\nusually train deep learning models on prediction tasks to uncover the causality\nbetween time series. They capture causal relations by analyzing the parameters\nof some components of the trained models, e.g., attention weights and\nconvolution weights. However, this is an incomplete mapping process from the\nmodel parameters to the causality and fails to investigate the other\ncomponents, e.g., fully connected layers and activation functions, that are\nalso significant for causal discovery. To facilitate the utilization of the\nwhole deep learning models in temporal causal discovery, we proposed an\ninterpretable transformer-based causal discovery model termed CausalFormer,\nwhich consists of the causality-aware transformer and the decomposition-based\ncausality detector. The causality-aware transformer learns the causal\nrepresentation of time series data using a prediction task with the designed\nmulti-kernel causal convolution which aggregates each input time series along\nthe temporal dimension under the temporal priority constraint. Then, the\ndecomposition-based causality detector interprets the global structure of the\ntrained causality-aware transformer with the proposed regression relevance\npropagation to identify potential causal relations and finally construct the\ncausal graph. Experiments on synthetic, simulated, and real datasets\ndemonstrate the state-of-the-art performance of CausalFormer on discovering\ntemporal causality. Our code is available at\nhttps://github.com/lingbai-kong/CausalFormer.\n", "link": "http://arxiv.org/abs/2406.16708v1", "date": "2024-06-24", "relevancy": 1.9242, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5043}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4891}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CausalFormer%3A%20An%20Interpretable%20Transformer%20for%20Temporal%20Causal%20Discovery&body=Title%3A%20CausalFormer%3A%20An%20Interpretable%20Transformer%20for%20Temporal%20Causal%20Discovery%0AAuthor%3A%20Lingbai%20Kong%20and%20Wengen%20Li%20and%20Hanchen%20Yang%20and%20Yichao%20Zhang%20and%20Jihong%20Guan%20and%20Shuigeng%20Zhou%0AAbstract%3A%20%20%20Temporal%20causal%20discovery%20is%20a%20crucial%20task%20aimed%20at%20uncovering%20the%20causal%0Arelations%20within%20time%20series%20data.%20The%20latest%20temporal%20causal%20discovery%20methods%0Ausually%20train%20deep%20learning%20models%20on%20prediction%20tasks%20to%20uncover%20the%20causality%0Abetween%20time%20series.%20They%20capture%20causal%20relations%20by%20analyzing%20the%20parameters%0Aof%20some%20components%20of%20the%20trained%20models%2C%20e.g.%2C%20attention%20weights%20and%0Aconvolution%20weights.%20However%2C%20this%20is%20an%20incomplete%20mapping%20process%20from%20the%0Amodel%20parameters%20to%20the%20causality%20and%20fails%20to%20investigate%20the%20other%0Acomponents%2C%20e.g.%2C%20fully%20connected%20layers%20and%20activation%20functions%2C%20that%20are%0Aalso%20significant%20for%20causal%20discovery.%20To%20facilitate%20the%20utilization%20of%20the%0Awhole%20deep%20learning%20models%20in%20temporal%20causal%20discovery%2C%20we%20proposed%20an%0Ainterpretable%20transformer-based%20causal%20discovery%20model%20termed%20CausalFormer%2C%0Awhich%20consists%20of%20the%20causality-aware%20transformer%20and%20the%20decomposition-based%0Acausality%20detector.%20The%20causality-aware%20transformer%20learns%20the%20causal%0Arepresentation%20of%20time%20series%20data%20using%20a%20prediction%20task%20with%20the%20designed%0Amulti-kernel%20causal%20convolution%20which%20aggregates%20each%20input%20time%20series%20along%0Athe%20temporal%20dimension%20under%20the%20temporal%20priority%20constraint.%20Then%2C%20the%0Adecomposition-based%20causality%20detector%20interprets%20the%20global%20structure%20of%20the%0Atrained%20causality-aware%20transformer%20with%20the%20proposed%20regression%20relevance%0Apropagation%20to%20identify%20potential%20causal%20relations%20and%20finally%20construct%20the%0Acausal%20graph.%20Experiments%20on%20synthetic%2C%20simulated%2C%20and%20real%20datasets%0Ademonstrate%20the%20state-of-the-art%20performance%20of%20CausalFormer%20on%20discovering%0Atemporal%20causality.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/lingbai-kong/CausalFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16708v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausalFormer%253A%2520An%2520Interpretable%2520Transformer%2520for%2520Temporal%2520Causal%2520Discovery%26entry.906535625%3DLingbai%2520Kong%2520and%2520Wengen%2520Li%2520and%2520Hanchen%2520Yang%2520and%2520Yichao%2520Zhang%2520and%2520Jihong%2520Guan%2520and%2520Shuigeng%2520Zhou%26entry.1292438233%3D%2520%2520Temporal%2520causal%2520discovery%2520is%2520a%2520crucial%2520task%2520aimed%2520at%2520uncovering%2520the%2520causal%250Arelations%2520within%2520time%2520series%2520data.%2520The%2520latest%2520temporal%2520causal%2520discovery%2520methods%250Ausually%2520train%2520deep%2520learning%2520models%2520on%2520prediction%2520tasks%2520to%2520uncover%2520the%2520causality%250Abetween%2520time%2520series.%2520They%2520capture%2520causal%2520relations%2520by%2520analyzing%2520the%2520parameters%250Aof%2520some%2520components%2520of%2520the%2520trained%2520models%252C%2520e.g.%252C%2520attention%2520weights%2520and%250Aconvolution%2520weights.%2520However%252C%2520this%2520is%2520an%2520incomplete%2520mapping%2520process%2520from%2520the%250Amodel%2520parameters%2520to%2520the%2520causality%2520and%2520fails%2520to%2520investigate%2520the%2520other%250Acomponents%252C%2520e.g.%252C%2520fully%2520connected%2520layers%2520and%2520activation%2520functions%252C%2520that%2520are%250Aalso%2520significant%2520for%2520causal%2520discovery.%2520To%2520facilitate%2520the%2520utilization%2520of%2520the%250Awhole%2520deep%2520learning%2520models%2520in%2520temporal%2520causal%2520discovery%252C%2520we%2520proposed%2520an%250Ainterpretable%2520transformer-based%2520causal%2520discovery%2520model%2520termed%2520CausalFormer%252C%250Awhich%2520consists%2520of%2520the%2520causality-aware%2520transformer%2520and%2520the%2520decomposition-based%250Acausality%2520detector.%2520The%2520causality-aware%2520transformer%2520learns%2520the%2520causal%250Arepresentation%2520of%2520time%2520series%2520data%2520using%2520a%2520prediction%2520task%2520with%2520the%2520designed%250Amulti-kernel%2520causal%2520convolution%2520which%2520aggregates%2520each%2520input%2520time%2520series%2520along%250Athe%2520temporal%2520dimension%2520under%2520the%2520temporal%2520priority%2520constraint.%2520Then%252C%2520the%250Adecomposition-based%2520causality%2520detector%2520interprets%2520the%2520global%2520structure%2520of%2520the%250Atrained%2520causality-aware%2520transformer%2520with%2520the%2520proposed%2520regression%2520relevance%250Apropagation%2520to%2520identify%2520potential%2520causal%2520relations%2520and%2520finally%2520construct%2520the%250Acausal%2520graph.%2520Experiments%2520on%2520synthetic%252C%2520simulated%252C%2520and%2520real%2520datasets%250Ademonstrate%2520the%2520state-of-the-art%2520performance%2520of%2520CausalFormer%2520on%2520discovering%250Atemporal%2520causality.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/lingbai-kong/CausalFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16708v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CausalFormer%3A%20An%20Interpretable%20Transformer%20for%20Temporal%20Causal%20Discovery&entry.906535625=Lingbai%20Kong%20and%20Wengen%20Li%20and%20Hanchen%20Yang%20and%20Yichao%20Zhang%20and%20Jihong%20Guan%20and%20Shuigeng%20Zhou&entry.1292438233=%20%20Temporal%20causal%20discovery%20is%20a%20crucial%20task%20aimed%20at%20uncovering%20the%20causal%0Arelations%20within%20time%20series%20data.%20The%20latest%20temporal%20causal%20discovery%20methods%0Ausually%20train%20deep%20learning%20models%20on%20prediction%20tasks%20to%20uncover%20the%20causality%0Abetween%20time%20series.%20They%20capture%20causal%20relations%20by%20analyzing%20the%20parameters%0Aof%20some%20components%20of%20the%20trained%20models%2C%20e.g.%2C%20attention%20weights%20and%0Aconvolution%20weights.%20However%2C%20this%20is%20an%20incomplete%20mapping%20process%20from%20the%0Amodel%20parameters%20to%20the%20causality%20and%20fails%20to%20investigate%20the%20other%0Acomponents%2C%20e.g.%2C%20fully%20connected%20layers%20and%20activation%20functions%2C%20that%20are%0Aalso%20significant%20for%20causal%20discovery.%20To%20facilitate%20the%20utilization%20of%20the%0Awhole%20deep%20learning%20models%20in%20temporal%20causal%20discovery%2C%20we%20proposed%20an%0Ainterpretable%20transformer-based%20causal%20discovery%20model%20termed%20CausalFormer%2C%0Awhich%20consists%20of%20the%20causality-aware%20transformer%20and%20the%20decomposition-based%0Acausality%20detector.%20The%20causality-aware%20transformer%20learns%20the%20causal%0Arepresentation%20of%20time%20series%20data%20using%20a%20prediction%20task%20with%20the%20designed%0Amulti-kernel%20causal%20convolution%20which%20aggregates%20each%20input%20time%20series%20along%0Athe%20temporal%20dimension%20under%20the%20temporal%20priority%20constraint.%20Then%2C%20the%0Adecomposition-based%20causality%20detector%20interprets%20the%20global%20structure%20of%20the%0Atrained%20causality-aware%20transformer%20with%20the%20proposed%20regression%20relevance%0Apropagation%20to%20identify%20potential%20causal%20relations%20and%20finally%20construct%20the%0Acausal%20graph.%20Experiments%20on%20synthetic%2C%20simulated%2C%20and%20real%20datasets%0Ademonstrate%20the%20state-of-the-art%20performance%20of%20CausalFormer%20on%20discovering%0Atemporal%20causality.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/lingbai-kong/CausalFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16708v1&entry.124074799=Read"},
{"title": "Provable Adaptivity of Adam under Non-uniform Smoothness", "author": "Bohan Wang and Yushun Zhang and Huishuai Zhang and Qi Meng and Ruoyu Sun and Zhi-Ming Ma and Tie-Yan Liu and Zhi-Quan Luo and Wei Chen", "abstract": "  Adam is widely adopted in practical applications due to its fast convergence.\nHowever, its theoretical analysis is still far from satisfactory. Existing\nconvergence analyses for Adam rely on the bounded smoothness assumption,\nreferred to as the \\emph{L-smooth condition}. Unfortunately, this assumption\ndoes not hold for many deep learning tasks. Moreover, we believe that this\nassumption obscures the true benefit of Adam, as the algorithm can adapt its\nupdate magnitude according to local smoothness. This important feature of Adam\nbecomes irrelevant when assuming globally bounded smoothness. This paper\nstudies the convergence of randomly reshuffled Adam (RR Adam) with diminishing\nlearning rate, which is the major version of Adam adopted in deep learning\ntasks. We present the first convergence analysis of RR Adam without the bounded\nsmoothness assumption. We demonstrate that RR Adam can maintain its convergence\nproperties when smoothness is linearly bounded by the gradient norm, referred\nto as the \\emph{$(L_0, L_1)$-smooth condition. We further compare Adam to SGD\nwhen both methods use diminishing learning rate. We refine the existing lower\nbound of SGD and show that SGD can be slower than Adam. To our knowledge, this\nis the first time that Adam and SGD are rigorously compared in the same setting\nand the advantage of Adam is revealed.\n", "link": "http://arxiv.org/abs/2208.09900v2", "date": "2024-06-24", "relevancy": 1.836, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4671}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4591}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provable%20Adaptivity%20of%20Adam%20under%20Non-uniform%20Smoothness&body=Title%3A%20Provable%20Adaptivity%20of%20Adam%20under%20Non-uniform%20Smoothness%0AAuthor%3A%20Bohan%20Wang%20and%20Yushun%20Zhang%20and%20Huishuai%20Zhang%20and%20Qi%20Meng%20and%20Ruoyu%20Sun%20and%20Zhi-Ming%20Ma%20and%20Tie-Yan%20Liu%20and%20Zhi-Quan%20Luo%20and%20Wei%20Chen%0AAbstract%3A%20%20%20Adam%20is%20widely%20adopted%20in%20practical%20applications%20due%20to%20its%20fast%20convergence.%0AHowever%2C%20its%20theoretical%20analysis%20is%20still%20far%20from%20satisfactory.%20Existing%0Aconvergence%20analyses%20for%20Adam%20rely%20on%20the%20bounded%20smoothness%20assumption%2C%0Areferred%20to%20as%20the%20%5Cemph%7BL-smooth%20condition%7D.%20Unfortunately%2C%20this%20assumption%0Adoes%20not%20hold%20for%20many%20deep%20learning%20tasks.%20Moreover%2C%20we%20believe%20that%20this%0Aassumption%20obscures%20the%20true%20benefit%20of%20Adam%2C%20as%20the%20algorithm%20can%20adapt%20its%0Aupdate%20magnitude%20according%20to%20local%20smoothness.%20This%20important%20feature%20of%20Adam%0Abecomes%20irrelevant%20when%20assuming%20globally%20bounded%20smoothness.%20This%20paper%0Astudies%20the%20convergence%20of%20randomly%20reshuffled%20Adam%20%28RR%20Adam%29%20with%20diminishing%0Alearning%20rate%2C%20which%20is%20the%20major%20version%20of%20Adam%20adopted%20in%20deep%20learning%0Atasks.%20We%20present%20the%20first%20convergence%20analysis%20of%20RR%20Adam%20without%20the%20bounded%0Asmoothness%20assumption.%20We%20demonstrate%20that%20RR%20Adam%20can%20maintain%20its%20convergence%0Aproperties%20when%20smoothness%20is%20linearly%20bounded%20by%20the%20gradient%20norm%2C%20referred%0Ato%20as%20the%20%5Cemph%7B%24%28L_0%2C%20L_1%29%24-smooth%20condition.%20We%20further%20compare%20Adam%20to%20SGD%0Awhen%20both%20methods%20use%20diminishing%20learning%20rate.%20We%20refine%20the%20existing%20lower%0Abound%20of%20SGD%20and%20show%20that%20SGD%20can%20be%20slower%20than%20Adam.%20To%20our%20knowledge%2C%20this%0Ais%20the%20first%20time%20that%20Adam%20and%20SGD%20are%20rigorously%20compared%20in%20the%20same%20setting%0Aand%20the%20advantage%20of%20Adam%20is%20revealed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.09900v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvable%2520Adaptivity%2520of%2520Adam%2520under%2520Non-uniform%2520Smoothness%26entry.906535625%3DBohan%2520Wang%2520and%2520Yushun%2520Zhang%2520and%2520Huishuai%2520Zhang%2520and%2520Qi%2520Meng%2520and%2520Ruoyu%2520Sun%2520and%2520Zhi-Ming%2520Ma%2520and%2520Tie-Yan%2520Liu%2520and%2520Zhi-Quan%2520Luo%2520and%2520Wei%2520Chen%26entry.1292438233%3D%2520%2520Adam%2520is%2520widely%2520adopted%2520in%2520practical%2520applications%2520due%2520to%2520its%2520fast%2520convergence.%250AHowever%252C%2520its%2520theoretical%2520analysis%2520is%2520still%2520far%2520from%2520satisfactory.%2520Existing%250Aconvergence%2520analyses%2520for%2520Adam%2520rely%2520on%2520the%2520bounded%2520smoothness%2520assumption%252C%250Areferred%2520to%2520as%2520the%2520%255Cemph%257BL-smooth%2520condition%257D.%2520Unfortunately%252C%2520this%2520assumption%250Adoes%2520not%2520hold%2520for%2520many%2520deep%2520learning%2520tasks.%2520Moreover%252C%2520we%2520believe%2520that%2520this%250Aassumption%2520obscures%2520the%2520true%2520benefit%2520of%2520Adam%252C%2520as%2520the%2520algorithm%2520can%2520adapt%2520its%250Aupdate%2520magnitude%2520according%2520to%2520local%2520smoothness.%2520This%2520important%2520feature%2520of%2520Adam%250Abecomes%2520irrelevant%2520when%2520assuming%2520globally%2520bounded%2520smoothness.%2520This%2520paper%250Astudies%2520the%2520convergence%2520of%2520randomly%2520reshuffled%2520Adam%2520%2528RR%2520Adam%2529%2520with%2520diminishing%250Alearning%2520rate%252C%2520which%2520is%2520the%2520major%2520version%2520of%2520Adam%2520adopted%2520in%2520deep%2520learning%250Atasks.%2520We%2520present%2520the%2520first%2520convergence%2520analysis%2520of%2520RR%2520Adam%2520without%2520the%2520bounded%250Asmoothness%2520assumption.%2520We%2520demonstrate%2520that%2520RR%2520Adam%2520can%2520maintain%2520its%2520convergence%250Aproperties%2520when%2520smoothness%2520is%2520linearly%2520bounded%2520by%2520the%2520gradient%2520norm%252C%2520referred%250Ato%2520as%2520the%2520%255Cemph%257B%2524%2528L_0%252C%2520L_1%2529%2524-smooth%2520condition.%2520We%2520further%2520compare%2520Adam%2520to%2520SGD%250Awhen%2520both%2520methods%2520use%2520diminishing%2520learning%2520rate.%2520We%2520refine%2520the%2520existing%2520lower%250Abound%2520of%2520SGD%2520and%2520show%2520that%2520SGD%2520can%2520be%2520slower%2520than%2520Adam.%2520To%2520our%2520knowledge%252C%2520this%250Ais%2520the%2520first%2520time%2520that%2520Adam%2520and%2520SGD%2520are%2520rigorously%2520compared%2520in%2520the%2520same%2520setting%250Aand%2520the%2520advantage%2520of%2520Adam%2520is%2520revealed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.09900v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provable%20Adaptivity%20of%20Adam%20under%20Non-uniform%20Smoothness&entry.906535625=Bohan%20Wang%20and%20Yushun%20Zhang%20and%20Huishuai%20Zhang%20and%20Qi%20Meng%20and%20Ruoyu%20Sun%20and%20Zhi-Ming%20Ma%20and%20Tie-Yan%20Liu%20and%20Zhi-Quan%20Luo%20and%20Wei%20Chen&entry.1292438233=%20%20Adam%20is%20widely%20adopted%20in%20practical%20applications%20due%20to%20its%20fast%20convergence.%0AHowever%2C%20its%20theoretical%20analysis%20is%20still%20far%20from%20satisfactory.%20Existing%0Aconvergence%20analyses%20for%20Adam%20rely%20on%20the%20bounded%20smoothness%20assumption%2C%0Areferred%20to%20as%20the%20%5Cemph%7BL-smooth%20condition%7D.%20Unfortunately%2C%20this%20assumption%0Adoes%20not%20hold%20for%20many%20deep%20learning%20tasks.%20Moreover%2C%20we%20believe%20that%20this%0Aassumption%20obscures%20the%20true%20benefit%20of%20Adam%2C%20as%20the%20algorithm%20can%20adapt%20its%0Aupdate%20magnitude%20according%20to%20local%20smoothness.%20This%20important%20feature%20of%20Adam%0Abecomes%20irrelevant%20when%20assuming%20globally%20bounded%20smoothness.%20This%20paper%0Astudies%20the%20convergence%20of%20randomly%20reshuffled%20Adam%20%28RR%20Adam%29%20with%20diminishing%0Alearning%20rate%2C%20which%20is%20the%20major%20version%20of%20Adam%20adopted%20in%20deep%20learning%0Atasks.%20We%20present%20the%20first%20convergence%20analysis%20of%20RR%20Adam%20without%20the%20bounded%0Asmoothness%20assumption.%20We%20demonstrate%20that%20RR%20Adam%20can%20maintain%20its%20convergence%0Aproperties%20when%20smoothness%20is%20linearly%20bounded%20by%20the%20gradient%20norm%2C%20referred%0Ato%20as%20the%20%5Cemph%7B%24%28L_0%2C%20L_1%29%24-smooth%20condition.%20We%20further%20compare%20Adam%20to%20SGD%0Awhen%20both%20methods%20use%20diminishing%20learning%20rate.%20We%20refine%20the%20existing%20lower%0Abound%20of%20SGD%20and%20show%20that%20SGD%20can%20be%20slower%20than%20Adam.%20To%20our%20knowledge%2C%20this%0Ais%20the%20first%20time%20that%20Adam%20and%20SGD%20are%20rigorously%20compared%20in%20the%20same%20setting%0Aand%20the%20advantage%20of%20Adam%20is%20revealed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.09900v2&entry.124074799=Read"},
{"title": "GIEBench: Towards Holistic Evaluation of Group Identity-based Empathy\n  for Large Language Models", "author": "Leyan Wang and Yonggang Jin and Tianhao Shen and Tianyu Zheng and Xinrun Du and Chenchen Zhang and Wenhao Huang and Jiaheng Liu and Shi Wang and Ge Zhang and Liuyu Xiang and Zhaofeng He", "abstract": "  As large language models (LLMs) continue to develop and gain widespread\napplication, the ability of LLMs to exhibit empathy towards diverse group\nidentities and understand their perspectives is increasingly recognized as\ncritical. Most existing benchmarks for empathy evaluation of LLMs focus\nprimarily on universal human emotions, such as sadness and pain, often\noverlooking the context of individuals' group identities. To address this gap,\nwe introduce GIEBench, a comprehensive benchmark that includes 11 identity\ndimensions, covering 97 group identities with a total of 999 single-choice\nquestions related to specific group identities. GIEBench is designed to\nevaluate the empathy of LLMs when presented with specific group identities such\nas gender, age, occupation, and race, emphasizing their ability to respond from\nthe standpoint of the identified group. This supports the ongoing development\nof empathetic LLM applications tailored to users with different identities. Our\nevaluation of 23 LLMs revealed that while these LLMs understand different\nidentity standpoints, they fail to consistently exhibit equal empathy across\nthese identities without explicit instructions to adopt those perspectives.\nThis highlights the need for improved alignment of LLMs with diverse values to\nbetter accommodate the multifaceted nature of human identities. Our datasets\nare available at https://github.com/GIEBench/GIEBench.\n", "link": "http://arxiv.org/abs/2406.14903v2", "date": "2024-06-24", "relevancy": 1.7138, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4481}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4286}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GIEBench%3A%20Towards%20Holistic%20Evaluation%20of%20Group%20Identity-based%20Empathy%0A%20%20for%20Large%20Language%20Models&body=Title%3A%20GIEBench%3A%20Towards%20Holistic%20Evaluation%20of%20Group%20Identity-based%20Empathy%0A%20%20for%20Large%20Language%20Models%0AAuthor%3A%20Leyan%20Wang%20and%20Yonggang%20Jin%20and%20Tianhao%20Shen%20and%20Tianyu%20Zheng%20and%20Xinrun%20Du%20and%20Chenchen%20Zhang%20and%20Wenhao%20Huang%20and%20Jiaheng%20Liu%20and%20Shi%20Wang%20and%20Ge%20Zhang%20and%20Liuyu%20Xiang%20and%20Zhaofeng%20He%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20continue%20to%20develop%20and%20gain%20widespread%0Aapplication%2C%20the%20ability%20of%20LLMs%20to%20exhibit%20empathy%20towards%20diverse%20group%0Aidentities%20and%20understand%20their%20perspectives%20is%20increasingly%20recognized%20as%0Acritical.%20Most%20existing%20benchmarks%20for%20empathy%20evaluation%20of%20LLMs%20focus%0Aprimarily%20on%20universal%20human%20emotions%2C%20such%20as%20sadness%20and%20pain%2C%20often%0Aoverlooking%20the%20context%20of%20individuals%27%20group%20identities.%20To%20address%20this%20gap%2C%0Awe%20introduce%20GIEBench%2C%20a%20comprehensive%20benchmark%20that%20includes%2011%20identity%0Adimensions%2C%20covering%2097%20group%20identities%20with%20a%20total%20of%20999%20single-choice%0Aquestions%20related%20to%20specific%20group%20identities.%20GIEBench%20is%20designed%20to%0Aevaluate%20the%20empathy%20of%20LLMs%20when%20presented%20with%20specific%20group%20identities%20such%0Aas%20gender%2C%20age%2C%20occupation%2C%20and%20race%2C%20emphasizing%20their%20ability%20to%20respond%20from%0Athe%20standpoint%20of%20the%20identified%20group.%20This%20supports%20the%20ongoing%20development%0Aof%20empathetic%20LLM%20applications%20tailored%20to%20users%20with%20different%20identities.%20Our%0Aevaluation%20of%2023%20LLMs%20revealed%20that%20while%20these%20LLMs%20understand%20different%0Aidentity%20standpoints%2C%20they%20fail%20to%20consistently%20exhibit%20equal%20empathy%20across%0Athese%20identities%20without%20explicit%20instructions%20to%20adopt%20those%20perspectives.%0AThis%20highlights%20the%20need%20for%20improved%20alignment%20of%20LLMs%20with%20diverse%20values%20to%0Abetter%20accommodate%20the%20multifaceted%20nature%20of%20human%20identities.%20Our%20datasets%0Aare%20available%20at%20https%3A//github.com/GIEBench/GIEBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14903v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGIEBench%253A%2520Towards%2520Holistic%2520Evaluation%2520of%2520Group%2520Identity-based%2520Empathy%250A%2520%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DLeyan%2520Wang%2520and%2520Yonggang%2520Jin%2520and%2520Tianhao%2520Shen%2520and%2520Tianyu%2520Zheng%2520and%2520Xinrun%2520Du%2520and%2520Chenchen%2520Zhang%2520and%2520Wenhao%2520Huang%2520and%2520Jiaheng%2520Liu%2520and%2520Shi%2520Wang%2520and%2520Ge%2520Zhang%2520and%2520Liuyu%2520Xiang%2520and%2520Zhaofeng%2520He%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520continue%2520to%2520develop%2520and%2520gain%2520widespread%250Aapplication%252C%2520the%2520ability%2520of%2520LLMs%2520to%2520exhibit%2520empathy%2520towards%2520diverse%2520group%250Aidentities%2520and%2520understand%2520their%2520perspectives%2520is%2520increasingly%2520recognized%2520as%250Acritical.%2520Most%2520existing%2520benchmarks%2520for%2520empathy%2520evaluation%2520of%2520LLMs%2520focus%250Aprimarily%2520on%2520universal%2520human%2520emotions%252C%2520such%2520as%2520sadness%2520and%2520pain%252C%2520often%250Aoverlooking%2520the%2520context%2520of%2520individuals%2527%2520group%2520identities.%2520To%2520address%2520this%2520gap%252C%250Awe%2520introduce%2520GIEBench%252C%2520a%2520comprehensive%2520benchmark%2520that%2520includes%252011%2520identity%250Adimensions%252C%2520covering%252097%2520group%2520identities%2520with%2520a%2520total%2520of%2520999%2520single-choice%250Aquestions%2520related%2520to%2520specific%2520group%2520identities.%2520GIEBench%2520is%2520designed%2520to%250Aevaluate%2520the%2520empathy%2520of%2520LLMs%2520when%2520presented%2520with%2520specific%2520group%2520identities%2520such%250Aas%2520gender%252C%2520age%252C%2520occupation%252C%2520and%2520race%252C%2520emphasizing%2520their%2520ability%2520to%2520respond%2520from%250Athe%2520standpoint%2520of%2520the%2520identified%2520group.%2520This%2520supports%2520the%2520ongoing%2520development%250Aof%2520empathetic%2520LLM%2520applications%2520tailored%2520to%2520users%2520with%2520different%2520identities.%2520Our%250Aevaluation%2520of%252023%2520LLMs%2520revealed%2520that%2520while%2520these%2520LLMs%2520understand%2520different%250Aidentity%2520standpoints%252C%2520they%2520fail%2520to%2520consistently%2520exhibit%2520equal%2520empathy%2520across%250Athese%2520identities%2520without%2520explicit%2520instructions%2520to%2520adopt%2520those%2520perspectives.%250AThis%2520highlights%2520the%2520need%2520for%2520improved%2520alignment%2520of%2520LLMs%2520with%2520diverse%2520values%2520to%250Abetter%2520accommodate%2520the%2520multifaceted%2520nature%2520of%2520human%2520identities.%2520Our%2520datasets%250Aare%2520available%2520at%2520https%253A//github.com/GIEBench/GIEBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14903v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GIEBench%3A%20Towards%20Holistic%20Evaluation%20of%20Group%20Identity-based%20Empathy%0A%20%20for%20Large%20Language%20Models&entry.906535625=Leyan%20Wang%20and%20Yonggang%20Jin%20and%20Tianhao%20Shen%20and%20Tianyu%20Zheng%20and%20Xinrun%20Du%20and%20Chenchen%20Zhang%20and%20Wenhao%20Huang%20and%20Jiaheng%20Liu%20and%20Shi%20Wang%20and%20Ge%20Zhang%20and%20Liuyu%20Xiang%20and%20Zhaofeng%20He&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20continue%20to%20develop%20and%20gain%20widespread%0Aapplication%2C%20the%20ability%20of%20LLMs%20to%20exhibit%20empathy%20towards%20diverse%20group%0Aidentities%20and%20understand%20their%20perspectives%20is%20increasingly%20recognized%20as%0Acritical.%20Most%20existing%20benchmarks%20for%20empathy%20evaluation%20of%20LLMs%20focus%0Aprimarily%20on%20universal%20human%20emotions%2C%20such%20as%20sadness%20and%20pain%2C%20often%0Aoverlooking%20the%20context%20of%20individuals%27%20group%20identities.%20To%20address%20this%20gap%2C%0Awe%20introduce%20GIEBench%2C%20a%20comprehensive%20benchmark%20that%20includes%2011%20identity%0Adimensions%2C%20covering%2097%20group%20identities%20with%20a%20total%20of%20999%20single-choice%0Aquestions%20related%20to%20specific%20group%20identities.%20GIEBench%20is%20designed%20to%0Aevaluate%20the%20empathy%20of%20LLMs%20when%20presented%20with%20specific%20group%20identities%20such%0Aas%20gender%2C%20age%2C%20occupation%2C%20and%20race%2C%20emphasizing%20their%20ability%20to%20respond%20from%0Athe%20standpoint%20of%20the%20identified%20group.%20This%20supports%20the%20ongoing%20development%0Aof%20empathetic%20LLM%20applications%20tailored%20to%20users%20with%20different%20identities.%20Our%0Aevaluation%20of%2023%20LLMs%20revealed%20that%20while%20these%20LLMs%20understand%20different%0Aidentity%20standpoints%2C%20they%20fail%20to%20consistently%20exhibit%20equal%20empathy%20across%0Athese%20identities%20without%20explicit%20instructions%20to%20adopt%20those%20perspectives.%0AThis%20highlights%20the%20need%20for%20improved%20alignment%20of%20LLMs%20with%20diverse%20values%20to%0Abetter%20accommodate%20the%20multifaceted%20nature%20of%20human%20identities.%20Our%20datasets%0Aare%20available%20at%20https%3A//github.com/GIEBench/GIEBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14903v2&entry.124074799=Read"},
{"title": "SyROCCo: Enhancing Systematic Reviews using Machine Learning", "author": "Zheng Fang and Miguel Arana-Catania and Felix-Anselm van Lier and Juliana Outes Velarde and Harry Bregazzi and Mara Airoldi and Eleanor Carter and Rob Procter", "abstract": "  The sheer number of research outputs published every year makes systematic\nreviewing increasingly time- and resource-intensive. This paper explores the\nuse of machine learning techniques to help navigate the systematic review\nprocess. ML has previously been used to reliably 'screen' articles for review -\nthat is, identify relevant articles based on reviewers' inclusion criteria. The\napplication of ML techniques to subsequent stages of a review, however, such as\ndata extraction and evidence mapping, is in its infancy. We therefore set out\nto develop a series of tools that would assist in the profiling and analysis of\n1,952 publications on the theme of 'outcomes-based contracting'. Tools were\ndeveloped for the following tasks: assign publications into 'policy area'\ncategories; identify and extract key information for evidence mapping, such as\norganisations, laws, and geographical information; connect the evidence base to\nan existing dataset on the same topic; and identify subgroups of articles that\nmay share thematic content. An interactive tool using these techniques and a\npublic dataset with their outputs have been released. Our results demonstrate\nthe utility of ML techniques to enhance evidence accessibility and analysis\nwithin the systematic review processes. These efforts show promise in\npotentially yielding substantial efficiencies for future systematic reviewing\nand for broadening their analytical scope. Our work suggests that there may be\nimplications for the ease with which policymakers and practitioners can access\nevidence. While ML techniques seem poised to play a significant role in\nbridging the gap between research and policy by offering innovative ways of\ngathering, accessing, and analysing data from systematic reviews, we also\nhighlight their current limitations and the need to exercise caution in their\napplication, particularly given the potential for errors and biases.\n", "link": "http://arxiv.org/abs/2406.16527v1", "date": "2024-06-24", "relevancy": 0.9381, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4751}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4747}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SyROCCo%3A%20Enhancing%20Systematic%20Reviews%20using%20Machine%20Learning&body=Title%3A%20SyROCCo%3A%20Enhancing%20Systematic%20Reviews%20using%20Machine%20Learning%0AAuthor%3A%20Zheng%20Fang%20and%20Miguel%20Arana-Catania%20and%20Felix-Anselm%20van%20Lier%20and%20Juliana%20Outes%20Velarde%20and%20Harry%20Bregazzi%20and%20Mara%20Airoldi%20and%20Eleanor%20Carter%20and%20Rob%20Procter%0AAbstract%3A%20%20%20The%20sheer%20number%20of%20research%20outputs%20published%20every%20year%20makes%20systematic%0Areviewing%20increasingly%20time-%20and%20resource-intensive.%20This%20paper%20explores%20the%0Ause%20of%20machine%20learning%20techniques%20to%20help%20navigate%20the%20systematic%20review%0Aprocess.%20ML%20has%20previously%20been%20used%20to%20reliably%20%27screen%27%20articles%20for%20review%20-%0Athat%20is%2C%20identify%20relevant%20articles%20based%20on%20reviewers%27%20inclusion%20criteria.%20The%0Aapplication%20of%20ML%20techniques%20to%20subsequent%20stages%20of%20a%20review%2C%20however%2C%20such%20as%0Adata%20extraction%20and%20evidence%20mapping%2C%20is%20in%20its%20infancy.%20We%20therefore%20set%20out%0Ato%20develop%20a%20series%20of%20tools%20that%20would%20assist%20in%20the%20profiling%20and%20analysis%20of%0A1%2C952%20publications%20on%20the%20theme%20of%20%27outcomes-based%20contracting%27.%20Tools%20were%0Adeveloped%20for%20the%20following%20tasks%3A%20assign%20publications%20into%20%27policy%20area%27%0Acategories%3B%20identify%20and%20extract%20key%20information%20for%20evidence%20mapping%2C%20such%20as%0Aorganisations%2C%20laws%2C%20and%20geographical%20information%3B%20connect%20the%20evidence%20base%20to%0Aan%20existing%20dataset%20on%20the%20same%20topic%3B%20and%20identify%20subgroups%20of%20articles%20that%0Amay%20share%20thematic%20content.%20An%20interactive%20tool%20using%20these%20techniques%20and%20a%0Apublic%20dataset%20with%20their%20outputs%20have%20been%20released.%20Our%20results%20demonstrate%0Athe%20utility%20of%20ML%20techniques%20to%20enhance%20evidence%20accessibility%20and%20analysis%0Awithin%20the%20systematic%20review%20processes.%20These%20efforts%20show%20promise%20in%0Apotentially%20yielding%20substantial%20efficiencies%20for%20future%20systematic%20reviewing%0Aand%20for%20broadening%20their%20analytical%20scope.%20Our%20work%20suggests%20that%20there%20may%20be%0Aimplications%20for%20the%20ease%20with%20which%20policymakers%20and%20practitioners%20can%20access%0Aevidence.%20While%20ML%20techniques%20seem%20poised%20to%20play%20a%20significant%20role%20in%0Abridging%20the%20gap%20between%20research%20and%20policy%20by%20offering%20innovative%20ways%20of%0Agathering%2C%20accessing%2C%20and%20analysing%20data%20from%20systematic%20reviews%2C%20we%20also%0Ahighlight%20their%20current%20limitations%20and%20the%20need%20to%20exercise%20caution%20in%20their%0Aapplication%2C%20particularly%20given%20the%20potential%20for%20errors%20and%20biases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSyROCCo%253A%2520Enhancing%2520Systematic%2520Reviews%2520using%2520Machine%2520Learning%26entry.906535625%3DZheng%2520Fang%2520and%2520Miguel%2520Arana-Catania%2520and%2520Felix-Anselm%2520van%2520Lier%2520and%2520Juliana%2520Outes%2520Velarde%2520and%2520Harry%2520Bregazzi%2520and%2520Mara%2520Airoldi%2520and%2520Eleanor%2520Carter%2520and%2520Rob%2520Procter%26entry.1292438233%3D%2520%2520The%2520sheer%2520number%2520of%2520research%2520outputs%2520published%2520every%2520year%2520makes%2520systematic%250Areviewing%2520increasingly%2520time-%2520and%2520resource-intensive.%2520This%2520paper%2520explores%2520the%250Ause%2520of%2520machine%2520learning%2520techniques%2520to%2520help%2520navigate%2520the%2520systematic%2520review%250Aprocess.%2520ML%2520has%2520previously%2520been%2520used%2520to%2520reliably%2520%2527screen%2527%2520articles%2520for%2520review%2520-%250Athat%2520is%252C%2520identify%2520relevant%2520articles%2520based%2520on%2520reviewers%2527%2520inclusion%2520criteria.%2520The%250Aapplication%2520of%2520ML%2520techniques%2520to%2520subsequent%2520stages%2520of%2520a%2520review%252C%2520however%252C%2520such%2520as%250Adata%2520extraction%2520and%2520evidence%2520mapping%252C%2520is%2520in%2520its%2520infancy.%2520We%2520therefore%2520set%2520out%250Ato%2520develop%2520a%2520series%2520of%2520tools%2520that%2520would%2520assist%2520in%2520the%2520profiling%2520and%2520analysis%2520of%250A1%252C952%2520publications%2520on%2520the%2520theme%2520of%2520%2527outcomes-based%2520contracting%2527.%2520Tools%2520were%250Adeveloped%2520for%2520the%2520following%2520tasks%253A%2520assign%2520publications%2520into%2520%2527policy%2520area%2527%250Acategories%253B%2520identify%2520and%2520extract%2520key%2520information%2520for%2520evidence%2520mapping%252C%2520such%2520as%250Aorganisations%252C%2520laws%252C%2520and%2520geographical%2520information%253B%2520connect%2520the%2520evidence%2520base%2520to%250Aan%2520existing%2520dataset%2520on%2520the%2520same%2520topic%253B%2520and%2520identify%2520subgroups%2520of%2520articles%2520that%250Amay%2520share%2520thematic%2520content.%2520An%2520interactive%2520tool%2520using%2520these%2520techniques%2520and%2520a%250Apublic%2520dataset%2520with%2520their%2520outputs%2520have%2520been%2520released.%2520Our%2520results%2520demonstrate%250Athe%2520utility%2520of%2520ML%2520techniques%2520to%2520enhance%2520evidence%2520accessibility%2520and%2520analysis%250Awithin%2520the%2520systematic%2520review%2520processes.%2520These%2520efforts%2520show%2520promise%2520in%250Apotentially%2520yielding%2520substantial%2520efficiencies%2520for%2520future%2520systematic%2520reviewing%250Aand%2520for%2520broadening%2520their%2520analytical%2520scope.%2520Our%2520work%2520suggests%2520that%2520there%2520may%2520be%250Aimplications%2520for%2520the%2520ease%2520with%2520which%2520policymakers%2520and%2520practitioners%2520can%2520access%250Aevidence.%2520While%2520ML%2520techniques%2520seem%2520poised%2520to%2520play%2520a%2520significant%2520role%2520in%250Abridging%2520the%2520gap%2520between%2520research%2520and%2520policy%2520by%2520offering%2520innovative%2520ways%2520of%250Agathering%252C%2520accessing%252C%2520and%2520analysing%2520data%2520from%2520systematic%2520reviews%252C%2520we%2520also%250Ahighlight%2520their%2520current%2520limitations%2520and%2520the%2520need%2520to%2520exercise%2520caution%2520in%2520their%250Aapplication%252C%2520particularly%2520given%2520the%2520potential%2520for%2520errors%2520and%2520biases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SyROCCo%3A%20Enhancing%20Systematic%20Reviews%20using%20Machine%20Learning&entry.906535625=Zheng%20Fang%20and%20Miguel%20Arana-Catania%20and%20Felix-Anselm%20van%20Lier%20and%20Juliana%20Outes%20Velarde%20and%20Harry%20Bregazzi%20and%20Mara%20Airoldi%20and%20Eleanor%20Carter%20and%20Rob%20Procter&entry.1292438233=%20%20The%20sheer%20number%20of%20research%20outputs%20published%20every%20year%20makes%20systematic%0Areviewing%20increasingly%20time-%20and%20resource-intensive.%20This%20paper%20explores%20the%0Ause%20of%20machine%20learning%20techniques%20to%20help%20navigate%20the%20systematic%20review%0Aprocess.%20ML%20has%20previously%20been%20used%20to%20reliably%20%27screen%27%20articles%20for%20review%20-%0Athat%20is%2C%20identify%20relevant%20articles%20based%20on%20reviewers%27%20inclusion%20criteria.%20The%0Aapplication%20of%20ML%20techniques%20to%20subsequent%20stages%20of%20a%20review%2C%20however%2C%20such%20as%0Adata%20extraction%20and%20evidence%20mapping%2C%20is%20in%20its%20infancy.%20We%20therefore%20set%20out%0Ato%20develop%20a%20series%20of%20tools%20that%20would%20assist%20in%20the%20profiling%20and%20analysis%20of%0A1%2C952%20publications%20on%20the%20theme%20of%20%27outcomes-based%20contracting%27.%20Tools%20were%0Adeveloped%20for%20the%20following%20tasks%3A%20assign%20publications%20into%20%27policy%20area%27%0Acategories%3B%20identify%20and%20extract%20key%20information%20for%20evidence%20mapping%2C%20such%20as%0Aorganisations%2C%20laws%2C%20and%20geographical%20information%3B%20connect%20the%20evidence%20base%20to%0Aan%20existing%20dataset%20on%20the%20same%20topic%3B%20and%20identify%20subgroups%20of%20articles%20that%0Amay%20share%20thematic%20content.%20An%20interactive%20tool%20using%20these%20techniques%20and%20a%0Apublic%20dataset%20with%20their%20outputs%20have%20been%20released.%20Our%20results%20demonstrate%0Athe%20utility%20of%20ML%20techniques%20to%20enhance%20evidence%20accessibility%20and%20analysis%0Awithin%20the%20systematic%20review%20processes.%20These%20efforts%20show%20promise%20in%0Apotentially%20yielding%20substantial%20efficiencies%20for%20future%20systematic%20reviewing%0Aand%20for%20broadening%20their%20analytical%20scope.%20Our%20work%20suggests%20that%20there%20may%20be%0Aimplications%20for%20the%20ease%20with%20which%20policymakers%20and%20practitioners%20can%20access%0Aevidence.%20While%20ML%20techniques%20seem%20poised%20to%20play%20a%20significant%20role%20in%0Abridging%20the%20gap%20between%20research%20and%20policy%20by%20offering%20innovative%20ways%20of%0Agathering%2C%20accessing%2C%20and%20analysing%20data%20from%20systematic%20reviews%2C%20we%20also%0Ahighlight%20their%20current%20limitations%20and%20the%20need%20to%20exercise%20caution%20in%20their%0Aapplication%2C%20particularly%20given%20the%20potential%20for%20errors%20and%20biases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16527v1&entry.124074799=Read"},
{"title": "Multi-Modal Vision Transformers for Crop Mapping from Satellite Image\n  Time Series", "author": "Theresa Follath and David Mickisch and Jan Hemmerling and Stefan Erasmi and Marcel Schwieder and Beg\u00fcm Demir", "abstract": "  Using images acquired by different satellite sensors has shown to improve\nclassification performance in the framework of crop mapping from satellite\nimage time series (SITS). Existing state-of-the-art architectures use\nself-attention mechanisms to process the temporal dimension and convolutions\nfor the spatial dimension of SITS. Motivated by the success of purely\nattention-based architectures in crop mapping from single-modal SITS, we\nintroduce several multi-modal multi-temporal transformer-based architectures.\nSpecifically, we investigate the effectiveness of Early Fusion, Cross Attention\nFusion and Synchronized Class Token Fusion within the Temporo-Spatial Vision\nTransformer (TSViT). Experimental results demonstrate significant improvements\nover state-of-the-art architectures with both convolutional and self-attention\ncomponents.\n", "link": "http://arxiv.org/abs/2406.16513v1", "date": "2024-06-24", "relevancy": 1.653, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5836}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5148}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Modal%20Vision%20Transformers%20for%20Crop%20Mapping%20from%20Satellite%20Image%0A%20%20Time%20Series&body=Title%3A%20Multi-Modal%20Vision%20Transformers%20for%20Crop%20Mapping%20from%20Satellite%20Image%0A%20%20Time%20Series%0AAuthor%3A%20Theresa%20Follath%20and%20David%20Mickisch%20and%20Jan%20Hemmerling%20and%20Stefan%20Erasmi%20and%20Marcel%20Schwieder%20and%20Beg%C3%BCm%20Demir%0AAbstract%3A%20%20%20Using%20images%20acquired%20by%20different%20satellite%20sensors%20has%20shown%20to%20improve%0Aclassification%20performance%20in%20the%20framework%20of%20crop%20mapping%20from%20satellite%0Aimage%20time%20series%20%28SITS%29.%20Existing%20state-of-the-art%20architectures%20use%0Aself-attention%20mechanisms%20to%20process%20the%20temporal%20dimension%20and%20convolutions%0Afor%20the%20spatial%20dimension%20of%20SITS.%20Motivated%20by%20the%20success%20of%20purely%0Aattention-based%20architectures%20in%20crop%20mapping%20from%20single-modal%20SITS%2C%20we%0Aintroduce%20several%20multi-modal%20multi-temporal%20transformer-based%20architectures.%0ASpecifically%2C%20we%20investigate%20the%20effectiveness%20of%20Early%20Fusion%2C%20Cross%20Attention%0AFusion%20and%20Synchronized%20Class%20Token%20Fusion%20within%20the%20Temporo-Spatial%20Vision%0ATransformer%20%28TSViT%29.%20Experimental%20results%20demonstrate%20significant%20improvements%0Aover%20state-of-the-art%20architectures%20with%20both%20convolutional%20and%20self-attention%0Acomponents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16513v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Modal%2520Vision%2520Transformers%2520for%2520Crop%2520Mapping%2520from%2520Satellite%2520Image%250A%2520%2520Time%2520Series%26entry.906535625%3DTheresa%2520Follath%2520and%2520David%2520Mickisch%2520and%2520Jan%2520Hemmerling%2520and%2520Stefan%2520Erasmi%2520and%2520Marcel%2520Schwieder%2520and%2520Beg%25C3%25BCm%2520Demir%26entry.1292438233%3D%2520%2520Using%2520images%2520acquired%2520by%2520different%2520satellite%2520sensors%2520has%2520shown%2520to%2520improve%250Aclassification%2520performance%2520in%2520the%2520framework%2520of%2520crop%2520mapping%2520from%2520satellite%250Aimage%2520time%2520series%2520%2528SITS%2529.%2520Existing%2520state-of-the-art%2520architectures%2520use%250Aself-attention%2520mechanisms%2520to%2520process%2520the%2520temporal%2520dimension%2520and%2520convolutions%250Afor%2520the%2520spatial%2520dimension%2520of%2520SITS.%2520Motivated%2520by%2520the%2520success%2520of%2520purely%250Aattention-based%2520architectures%2520in%2520crop%2520mapping%2520from%2520single-modal%2520SITS%252C%2520we%250Aintroduce%2520several%2520multi-modal%2520multi-temporal%2520transformer-based%2520architectures.%250ASpecifically%252C%2520we%2520investigate%2520the%2520effectiveness%2520of%2520Early%2520Fusion%252C%2520Cross%2520Attention%250AFusion%2520and%2520Synchronized%2520Class%2520Token%2520Fusion%2520within%2520the%2520Temporo-Spatial%2520Vision%250ATransformer%2520%2528TSViT%2529.%2520Experimental%2520results%2520demonstrate%2520significant%2520improvements%250Aover%2520state-of-the-art%2520architectures%2520with%2520both%2520convolutional%2520and%2520self-attention%250Acomponents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16513v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Modal%20Vision%20Transformers%20for%20Crop%20Mapping%20from%20Satellite%20Image%0A%20%20Time%20Series&entry.906535625=Theresa%20Follath%20and%20David%20Mickisch%20and%20Jan%20Hemmerling%20and%20Stefan%20Erasmi%20and%20Marcel%20Schwieder%20and%20Beg%C3%BCm%20Demir&entry.1292438233=%20%20Using%20images%20acquired%20by%20different%20satellite%20sensors%20has%20shown%20to%20improve%0Aclassification%20performance%20in%20the%20framework%20of%20crop%20mapping%20from%20satellite%0Aimage%20time%20series%20%28SITS%29.%20Existing%20state-of-the-art%20architectures%20use%0Aself-attention%20mechanisms%20to%20process%20the%20temporal%20dimension%20and%20convolutions%0Afor%20the%20spatial%20dimension%20of%20SITS.%20Motivated%20by%20the%20success%20of%20purely%0Aattention-based%20architectures%20in%20crop%20mapping%20from%20single-modal%20SITS%2C%20we%0Aintroduce%20several%20multi-modal%20multi-temporal%20transformer-based%20architectures.%0ASpecifically%2C%20we%20investigate%20the%20effectiveness%20of%20Early%20Fusion%2C%20Cross%20Attention%0AFusion%20and%20Synchronized%20Class%20Token%20Fusion%20within%20the%20Temporo-Spatial%20Vision%0ATransformer%20%28TSViT%29.%20Experimental%20results%20demonstrate%20significant%20improvements%0Aover%20state-of-the-art%20architectures%20with%20both%20convolutional%20and%20self-attention%0Acomponents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16513v1&entry.124074799=Read"},
{"title": "ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models", "author": "Yash Akhauri and Ahmed F AbouElhamayed and Jordan Dotzel and Zhiru Zhang and Alexander M Rush and Safeen Huda and Mohamed S Abdelfattah", "abstract": "  The high power consumption and latency-sensitive deployments of large\nlanguage models (LLMs) have motivated techniques like quantization and\nsparsity. Contextual sparsity, where the sparsity pattern is input-dependent,\nis crucial in LLMs because the permanent removal of attention heads or neurons\nfrom LLMs can significantly degrade accuracy. Prior work has attempted to model\ncontextual sparsity using neural networks trained to predict activation\nmagnitudes, which can be used to dynamically prune structures with low\npredicted activation magnitude. In this paper, we look beyond magnitude-based\npruning criteria to assess attention head and neuron importance in LLMs. We\ndeveloped a novel predictor called ShadowLLM, which can shadow the LLM behavior\nand enforce better sparsity patterns, resulting in over 15% improvement in\nend-to-end accuracy without increasing latency compared to previous methods.\nShadowLLM achieves up to a 20\\% speed-up over the state-of-the-art DejaVu\nframework. These enhancements are validated on models with up to 30 billion\nparameters. Our code is available at\n\\href{https://github.com/abdelfattah-lab/shadow_llm/}{ShadowLLM}.\n", "link": "http://arxiv.org/abs/2406.16635v1", "date": "2024-06-24", "relevancy": 1.4925, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.524}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4956}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShadowLLM%3A%20Predictor-based%20Contextual%20Sparsity%20for%20Large%20Language%20Models&body=Title%3A%20ShadowLLM%3A%20Predictor-based%20Contextual%20Sparsity%20for%20Large%20Language%20Models%0AAuthor%3A%20Yash%20Akhauri%20and%20Ahmed%20F%20AbouElhamayed%20and%20Jordan%20Dotzel%20and%20Zhiru%20Zhang%20and%20Alexander%20M%20Rush%20and%20Safeen%20Huda%20and%20Mohamed%20S%20Abdelfattah%0AAbstract%3A%20%20%20The%20high%20power%20consumption%20and%20latency-sensitive%20deployments%20of%20large%0Alanguage%20models%20%28LLMs%29%20have%20motivated%20techniques%20like%20quantization%20and%0Asparsity.%20Contextual%20sparsity%2C%20where%20the%20sparsity%20pattern%20is%20input-dependent%2C%0Ais%20crucial%20in%20LLMs%20because%20the%20permanent%20removal%20of%20attention%20heads%20or%20neurons%0Afrom%20LLMs%20can%20significantly%20degrade%20accuracy.%20Prior%20work%20has%20attempted%20to%20model%0Acontextual%20sparsity%20using%20neural%20networks%20trained%20to%20predict%20activation%0Amagnitudes%2C%20which%20can%20be%20used%20to%20dynamically%20prune%20structures%20with%20low%0Apredicted%20activation%20magnitude.%20In%20this%20paper%2C%20we%20look%20beyond%20magnitude-based%0Apruning%20criteria%20to%20assess%20attention%20head%20and%20neuron%20importance%20in%20LLMs.%20We%0Adeveloped%20a%20novel%20predictor%20called%20ShadowLLM%2C%20which%20can%20shadow%20the%20LLM%20behavior%0Aand%20enforce%20better%20sparsity%20patterns%2C%20resulting%20in%20over%2015%25%20improvement%20in%0Aend-to-end%20accuracy%20without%20increasing%20latency%20compared%20to%20previous%20methods.%0AShadowLLM%20achieves%20up%20to%20a%2020%5C%25%20speed-up%20over%20the%20state-of-the-art%20DejaVu%0Aframework.%20These%20enhancements%20are%20validated%20on%20models%20with%20up%20to%2030%20billion%0Aparameters.%20Our%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/abdelfattah-lab/shadow_llm/%7D%7BShadowLLM%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShadowLLM%253A%2520Predictor-based%2520Contextual%2520Sparsity%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DYash%2520Akhauri%2520and%2520Ahmed%2520F%2520AbouElhamayed%2520and%2520Jordan%2520Dotzel%2520and%2520Zhiru%2520Zhang%2520and%2520Alexander%2520M%2520Rush%2520and%2520Safeen%2520Huda%2520and%2520Mohamed%2520S%2520Abdelfattah%26entry.1292438233%3D%2520%2520The%2520high%2520power%2520consumption%2520and%2520latency-sensitive%2520deployments%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520have%2520motivated%2520techniques%2520like%2520quantization%2520and%250Asparsity.%2520Contextual%2520sparsity%252C%2520where%2520the%2520sparsity%2520pattern%2520is%2520input-dependent%252C%250Ais%2520crucial%2520in%2520LLMs%2520because%2520the%2520permanent%2520removal%2520of%2520attention%2520heads%2520or%2520neurons%250Afrom%2520LLMs%2520can%2520significantly%2520degrade%2520accuracy.%2520Prior%2520work%2520has%2520attempted%2520to%2520model%250Acontextual%2520sparsity%2520using%2520neural%2520networks%2520trained%2520to%2520predict%2520activation%250Amagnitudes%252C%2520which%2520can%2520be%2520used%2520to%2520dynamically%2520prune%2520structures%2520with%2520low%250Apredicted%2520activation%2520magnitude.%2520In%2520this%2520paper%252C%2520we%2520look%2520beyond%2520magnitude-based%250Apruning%2520criteria%2520to%2520assess%2520attention%2520head%2520and%2520neuron%2520importance%2520in%2520LLMs.%2520We%250Adeveloped%2520a%2520novel%2520predictor%2520called%2520ShadowLLM%252C%2520which%2520can%2520shadow%2520the%2520LLM%2520behavior%250Aand%2520enforce%2520better%2520sparsity%2520patterns%252C%2520resulting%2520in%2520over%252015%2525%2520improvement%2520in%250Aend-to-end%2520accuracy%2520without%2520increasing%2520latency%2520compared%2520to%2520previous%2520methods.%250AShadowLLM%2520achieves%2520up%2520to%2520a%252020%255C%2525%2520speed-up%2520over%2520the%2520state-of-the-art%2520DejaVu%250Aframework.%2520These%2520enhancements%2520are%2520validated%2520on%2520models%2520with%2520up%2520to%252030%2520billion%250Aparameters.%2520Our%2520code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/abdelfattah-lab/shadow_llm/%257D%257BShadowLLM%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShadowLLM%3A%20Predictor-based%20Contextual%20Sparsity%20for%20Large%20Language%20Models&entry.906535625=Yash%20Akhauri%20and%20Ahmed%20F%20AbouElhamayed%20and%20Jordan%20Dotzel%20and%20Zhiru%20Zhang%20and%20Alexander%20M%20Rush%20and%20Safeen%20Huda%20and%20Mohamed%20S%20Abdelfattah&entry.1292438233=%20%20The%20high%20power%20consumption%20and%20latency-sensitive%20deployments%20of%20large%0Alanguage%20models%20%28LLMs%29%20have%20motivated%20techniques%20like%20quantization%20and%0Asparsity.%20Contextual%20sparsity%2C%20where%20the%20sparsity%20pattern%20is%20input-dependent%2C%0Ais%20crucial%20in%20LLMs%20because%20the%20permanent%20removal%20of%20attention%20heads%20or%20neurons%0Afrom%20LLMs%20can%20significantly%20degrade%20accuracy.%20Prior%20work%20has%20attempted%20to%20model%0Acontextual%20sparsity%20using%20neural%20networks%20trained%20to%20predict%20activation%0Amagnitudes%2C%20which%20can%20be%20used%20to%20dynamically%20prune%20structures%20with%20low%0Apredicted%20activation%20magnitude.%20In%20this%20paper%2C%20we%20look%20beyond%20magnitude-based%0Apruning%20criteria%20to%20assess%20attention%20head%20and%20neuron%20importance%20in%20LLMs.%20We%0Adeveloped%20a%20novel%20predictor%20called%20ShadowLLM%2C%20which%20can%20shadow%20the%20LLM%20behavior%0Aand%20enforce%20better%20sparsity%20patterns%2C%20resulting%20in%20over%2015%25%20improvement%20in%0Aend-to-end%20accuracy%20without%20increasing%20latency%20compared%20to%20previous%20methods.%0AShadowLLM%20achieves%20up%20to%20a%2020%5C%25%20speed-up%20over%20the%20state-of-the-art%20DejaVu%0Aframework.%20These%20enhancements%20are%20validated%20on%20models%20with%20up%20to%2030%20billion%0Aparameters.%20Our%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/abdelfattah-lab/shadow_llm/%7D%7BShadowLLM%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16635v1&entry.124074799=Read"},
{"title": "Sigma-point Kalman Filter with Nonlinear Unknown Input Estimation via\n  Optimization and Data-driven Approach for Dynamic Systems", "author": "Junn Yong Loo and Ze Yang Ding and Vishnu Monn Baskaran and Surya Girinatha Nurzaman and Chee Pin Tan", "abstract": "  Most works on joint state and unknown input (UI) estimation require the\nassumption that the UIs are linear; this is potentially restrictive as it does\nnot hold in many intelligent autonomous systems. To overcome this restriction\nand circumvent the need to linearize the system, we propose a derivative-free\nUnknown Input Sigma-point Kalman Filter (SPKF-nUI) where the SPKF is\ninterconnected with a general nonlinear UI estimator that can be implemented\nvia nonlinear optimization and data-driven approaches. The nonlinear UI\nestimator uses the posterior state estimate which is less susceptible to state\nprediction error. In addition, we introduce a joint sigma-point transformation\nscheme to incorporate both the state and UI uncertainties in the estimation of\nSPKF-nUI. An in-depth stochastic stability analysis proves that the proposed\nSPKF-nUI yields exponentially converging estimation error bounds under\nreasonable assumptions. Finally, two case studies are carried out on a\nsimulation-based rigid robot and a physical soft robot, i.e., robots made of\nsoft materials with complex dynamics to validate effectiveness of the proposed\nfilter on nonlinear dynamic systems. Our results demonstrate that the proposed\nSPKF-nUI achieves the lowest state and UI estimation errors when compared to\nthe existing nonlinear state-UI filters.\n", "link": "http://arxiv.org/abs/2306.12361v2", "date": "2024-06-24", "relevancy": 1.5232, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5568}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5062}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sigma-point%20Kalman%20Filter%20with%20Nonlinear%20Unknown%20Input%20Estimation%20via%0A%20%20Optimization%20and%20Data-driven%20Approach%20for%20Dynamic%20Systems&body=Title%3A%20Sigma-point%20Kalman%20Filter%20with%20Nonlinear%20Unknown%20Input%20Estimation%20via%0A%20%20Optimization%20and%20Data-driven%20Approach%20for%20Dynamic%20Systems%0AAuthor%3A%20Junn%20Yong%20Loo%20and%20Ze%20Yang%20Ding%20and%20Vishnu%20Monn%20Baskaran%20and%20Surya%20Girinatha%20Nurzaman%20and%20Chee%20Pin%20Tan%0AAbstract%3A%20%20%20Most%20works%20on%20joint%20state%20and%20unknown%20input%20%28UI%29%20estimation%20require%20the%0Aassumption%20that%20the%20UIs%20are%20linear%3B%20this%20is%20potentially%20restrictive%20as%20it%20does%0Anot%20hold%20in%20many%20intelligent%20autonomous%20systems.%20To%20overcome%20this%20restriction%0Aand%20circumvent%20the%20need%20to%20linearize%20the%20system%2C%20we%20propose%20a%20derivative-free%0AUnknown%20Input%20Sigma-point%20Kalman%20Filter%20%28SPKF-nUI%29%20where%20the%20SPKF%20is%0Ainterconnected%20with%20a%20general%20nonlinear%20UI%20estimator%20that%20can%20be%20implemented%0Avia%20nonlinear%20optimization%20and%20data-driven%20approaches.%20The%20nonlinear%20UI%0Aestimator%20uses%20the%20posterior%20state%20estimate%20which%20is%20less%20susceptible%20to%20state%0Aprediction%20error.%20In%20addition%2C%20we%20introduce%20a%20joint%20sigma-point%20transformation%0Ascheme%20to%20incorporate%20both%20the%20state%20and%20UI%20uncertainties%20in%20the%20estimation%20of%0ASPKF-nUI.%20An%20in-depth%20stochastic%20stability%20analysis%20proves%20that%20the%20proposed%0ASPKF-nUI%20yields%20exponentially%20converging%20estimation%20error%20bounds%20under%0Areasonable%20assumptions.%20Finally%2C%20two%20case%20studies%20are%20carried%20out%20on%20a%0Asimulation-based%20rigid%20robot%20and%20a%20physical%20soft%20robot%2C%20i.e.%2C%20robots%20made%20of%0Asoft%20materials%20with%20complex%20dynamics%20to%20validate%20effectiveness%20of%20the%20proposed%0Afilter%20on%20nonlinear%20dynamic%20systems.%20Our%20results%20demonstrate%20that%20the%20proposed%0ASPKF-nUI%20achieves%20the%20lowest%20state%20and%20UI%20estimation%20errors%20when%20compared%20to%0Athe%20existing%20nonlinear%20state-UI%20filters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.12361v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSigma-point%2520Kalman%2520Filter%2520with%2520Nonlinear%2520Unknown%2520Input%2520Estimation%2520via%250A%2520%2520Optimization%2520and%2520Data-driven%2520Approach%2520for%2520Dynamic%2520Systems%26entry.906535625%3DJunn%2520Yong%2520Loo%2520and%2520Ze%2520Yang%2520Ding%2520and%2520Vishnu%2520Monn%2520Baskaran%2520and%2520Surya%2520Girinatha%2520Nurzaman%2520and%2520Chee%2520Pin%2520Tan%26entry.1292438233%3D%2520%2520Most%2520works%2520on%2520joint%2520state%2520and%2520unknown%2520input%2520%2528UI%2529%2520estimation%2520require%2520the%250Aassumption%2520that%2520the%2520UIs%2520are%2520linear%253B%2520this%2520is%2520potentially%2520restrictive%2520as%2520it%2520does%250Anot%2520hold%2520in%2520many%2520intelligent%2520autonomous%2520systems.%2520To%2520overcome%2520this%2520restriction%250Aand%2520circumvent%2520the%2520need%2520to%2520linearize%2520the%2520system%252C%2520we%2520propose%2520a%2520derivative-free%250AUnknown%2520Input%2520Sigma-point%2520Kalman%2520Filter%2520%2528SPKF-nUI%2529%2520where%2520the%2520SPKF%2520is%250Ainterconnected%2520with%2520a%2520general%2520nonlinear%2520UI%2520estimator%2520that%2520can%2520be%2520implemented%250Avia%2520nonlinear%2520optimization%2520and%2520data-driven%2520approaches.%2520The%2520nonlinear%2520UI%250Aestimator%2520uses%2520the%2520posterior%2520state%2520estimate%2520which%2520is%2520less%2520susceptible%2520to%2520state%250Aprediction%2520error.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520joint%2520sigma-point%2520transformation%250Ascheme%2520to%2520incorporate%2520both%2520the%2520state%2520and%2520UI%2520uncertainties%2520in%2520the%2520estimation%2520of%250ASPKF-nUI.%2520An%2520in-depth%2520stochastic%2520stability%2520analysis%2520proves%2520that%2520the%2520proposed%250ASPKF-nUI%2520yields%2520exponentially%2520converging%2520estimation%2520error%2520bounds%2520under%250Areasonable%2520assumptions.%2520Finally%252C%2520two%2520case%2520studies%2520are%2520carried%2520out%2520on%2520a%250Asimulation-based%2520rigid%2520robot%2520and%2520a%2520physical%2520soft%2520robot%252C%2520i.e.%252C%2520robots%2520made%2520of%250Asoft%2520materials%2520with%2520complex%2520dynamics%2520to%2520validate%2520effectiveness%2520of%2520the%2520proposed%250Afilter%2520on%2520nonlinear%2520dynamic%2520systems.%2520Our%2520results%2520demonstrate%2520that%2520the%2520proposed%250ASPKF-nUI%2520achieves%2520the%2520lowest%2520state%2520and%2520UI%2520estimation%2520errors%2520when%2520compared%2520to%250Athe%2520existing%2520nonlinear%2520state-UI%2520filters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.12361v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sigma-point%20Kalman%20Filter%20with%20Nonlinear%20Unknown%20Input%20Estimation%20via%0A%20%20Optimization%20and%20Data-driven%20Approach%20for%20Dynamic%20Systems&entry.906535625=Junn%20Yong%20Loo%20and%20Ze%20Yang%20Ding%20and%20Vishnu%20Monn%20Baskaran%20and%20Surya%20Girinatha%20Nurzaman%20and%20Chee%20Pin%20Tan&entry.1292438233=%20%20Most%20works%20on%20joint%20state%20and%20unknown%20input%20%28UI%29%20estimation%20require%20the%0Aassumption%20that%20the%20UIs%20are%20linear%3B%20this%20is%20potentially%20restrictive%20as%20it%20does%0Anot%20hold%20in%20many%20intelligent%20autonomous%20systems.%20To%20overcome%20this%20restriction%0Aand%20circumvent%20the%20need%20to%20linearize%20the%20system%2C%20we%20propose%20a%20derivative-free%0AUnknown%20Input%20Sigma-point%20Kalman%20Filter%20%28SPKF-nUI%29%20where%20the%20SPKF%20is%0Ainterconnected%20with%20a%20general%20nonlinear%20UI%20estimator%20that%20can%20be%20implemented%0Avia%20nonlinear%20optimization%20and%20data-driven%20approaches.%20The%20nonlinear%20UI%0Aestimator%20uses%20the%20posterior%20state%20estimate%20which%20is%20less%20susceptible%20to%20state%0Aprediction%20error.%20In%20addition%2C%20we%20introduce%20a%20joint%20sigma-point%20transformation%0Ascheme%20to%20incorporate%20both%20the%20state%20and%20UI%20uncertainties%20in%20the%20estimation%20of%0ASPKF-nUI.%20An%20in-depth%20stochastic%20stability%20analysis%20proves%20that%20the%20proposed%0ASPKF-nUI%20yields%20exponentially%20converging%20estimation%20error%20bounds%20under%0Areasonable%20assumptions.%20Finally%2C%20two%20case%20studies%20are%20carried%20out%20on%20a%0Asimulation-based%20rigid%20robot%20and%20a%20physical%20soft%20robot%2C%20i.e.%2C%20robots%20made%20of%0Asoft%20materials%20with%20complex%20dynamics%20to%20validate%20effectiveness%20of%20the%20proposed%0Afilter%20on%20nonlinear%20dynamic%20systems.%20Our%20results%20demonstrate%20that%20the%20proposed%0ASPKF-nUI%20achieves%20the%20lowest%20state%20and%20UI%20estimation%20errors%20when%20compared%20to%0Athe%20existing%20nonlinear%20state-UI%20filters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.12361v2&entry.124074799=Read"},
{"title": "Enabling more efficient and cost-effective AI/ML systems with Collective\n  Mind, virtualized MLOps, MLPerf, Collective Knowledge Playground and\n  reproducible optimization tournaments", "author": "Grigori Fursin", "abstract": "  In this white paper, I present my community effort to automatically co-design\ncheaper, faster and more energy-efficient software and hardware for AI, ML and\nother popular workloads with the help of the Collective Mind framework (CM),\nvirtualized MLOps, MLPerf benchmarks and reproducible optimization tournaments.\nI developed CM to modularize, automate and virtualize the tedious process of\nbuilding, running, profiling and optimizing complex applications across rapidly\nevolving open-source and proprietary AI/ML models, datasets, software and\nhardware. I achieved that with the help of portable, reusable and\ntechnology-agnostic automation recipes (ResearchOps) for MLOps and DevOps\n(CM4MLOps) discovered in close collaboration with academia and industry when\nreproducing more than 150 research papers and organizing the 1st mass-scale\ncommunity benchmarking of ML and AI systems using CM and MLPerf.\n  I donated CM and CM4MLOps to MLCommons to help connect academia and industry\nto learn how to build and run AI and other emerging workloads in the most\nefficient and cost-effective way using a common and technology-agnostic\nautomation, virtualization and reproducibility framework while unifying\nknowledge exchange, protecting everyone's intellectual property, enabling\nportable skills, and accelerating transfer of the state-of-the-art research to\nproduction. My long-term vision is to make AI accessible to everyone by making\nit a commodity automatically produced from the most suitable open-source and\nproprietary components from different vendors based on user demand,\nrequirements and constraints such as cost, latency, throughput, accuracy,\nenergy, size and other important characteristics.\n", "link": "http://arxiv.org/abs/2406.16791v1", "date": "2024-06-24", "relevancy": 1.3112, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4514}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4462}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enabling%20more%20efficient%20and%20cost-effective%20AI/ML%20systems%20with%20Collective%0A%20%20Mind%2C%20virtualized%20MLOps%2C%20MLPerf%2C%20Collective%20Knowledge%20Playground%20and%0A%20%20reproducible%20optimization%20tournaments&body=Title%3A%20Enabling%20more%20efficient%20and%20cost-effective%20AI/ML%20systems%20with%20Collective%0A%20%20Mind%2C%20virtualized%20MLOps%2C%20MLPerf%2C%20Collective%20Knowledge%20Playground%20and%0A%20%20reproducible%20optimization%20tournaments%0AAuthor%3A%20Grigori%20Fursin%0AAbstract%3A%20%20%20In%20this%20white%20paper%2C%20I%20present%20my%20community%20effort%20to%20automatically%20co-design%0Acheaper%2C%20faster%20and%20more%20energy-efficient%20software%20and%20hardware%20for%20AI%2C%20ML%20and%0Aother%20popular%20workloads%20with%20the%20help%20of%20the%20Collective%20Mind%20framework%20%28CM%29%2C%0Avirtualized%20MLOps%2C%20MLPerf%20benchmarks%20and%20reproducible%20optimization%20tournaments.%0AI%20developed%20CM%20to%20modularize%2C%20automate%20and%20virtualize%20the%20tedious%20process%20of%0Abuilding%2C%20running%2C%20profiling%20and%20optimizing%20complex%20applications%20across%20rapidly%0Aevolving%20open-source%20and%20proprietary%20AI/ML%20models%2C%20datasets%2C%20software%20and%0Ahardware.%20I%20achieved%20that%20with%20the%20help%20of%20portable%2C%20reusable%20and%0Atechnology-agnostic%20automation%20recipes%20%28ResearchOps%29%20for%20MLOps%20and%20DevOps%0A%28CM4MLOps%29%20discovered%20in%20close%20collaboration%20with%20academia%20and%20industry%20when%0Areproducing%20more%20than%20150%20research%20papers%20and%20organizing%20the%201st%20mass-scale%0Acommunity%20benchmarking%20of%20ML%20and%20AI%20systems%20using%20CM%20and%20MLPerf.%0A%20%20I%20donated%20CM%20and%20CM4MLOps%20to%20MLCommons%20to%20help%20connect%20academia%20and%20industry%0Ato%20learn%20how%20to%20build%20and%20run%20AI%20and%20other%20emerging%20workloads%20in%20the%20most%0Aefficient%20and%20cost-effective%20way%20using%20a%20common%20and%20technology-agnostic%0Aautomation%2C%20virtualization%20and%20reproducibility%20framework%20while%20unifying%0Aknowledge%20exchange%2C%20protecting%20everyone%27s%20intellectual%20property%2C%20enabling%0Aportable%20skills%2C%20and%20accelerating%20transfer%20of%20the%20state-of-the-art%20research%20to%0Aproduction.%20My%20long-term%20vision%20is%20to%20make%20AI%20accessible%20to%20everyone%20by%20making%0Ait%20a%20commodity%20automatically%20produced%20from%20the%20most%20suitable%20open-source%20and%0Aproprietary%20components%20from%20different%20vendors%20based%20on%20user%20demand%2C%0Arequirements%20and%20constraints%20such%20as%20cost%2C%20latency%2C%20throughput%2C%20accuracy%2C%0Aenergy%2C%20size%20and%20other%20important%20characteristics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnabling%2520more%2520efficient%2520and%2520cost-effective%2520AI/ML%2520systems%2520with%2520Collective%250A%2520%2520Mind%252C%2520virtualized%2520MLOps%252C%2520MLPerf%252C%2520Collective%2520Knowledge%2520Playground%2520and%250A%2520%2520reproducible%2520optimization%2520tournaments%26entry.906535625%3DGrigori%2520Fursin%26entry.1292438233%3D%2520%2520In%2520this%2520white%2520paper%252C%2520I%2520present%2520my%2520community%2520effort%2520to%2520automatically%2520co-design%250Acheaper%252C%2520faster%2520and%2520more%2520energy-efficient%2520software%2520and%2520hardware%2520for%2520AI%252C%2520ML%2520and%250Aother%2520popular%2520workloads%2520with%2520the%2520help%2520of%2520the%2520Collective%2520Mind%2520framework%2520%2528CM%2529%252C%250Avirtualized%2520MLOps%252C%2520MLPerf%2520benchmarks%2520and%2520reproducible%2520optimization%2520tournaments.%250AI%2520developed%2520CM%2520to%2520modularize%252C%2520automate%2520and%2520virtualize%2520the%2520tedious%2520process%2520of%250Abuilding%252C%2520running%252C%2520profiling%2520and%2520optimizing%2520complex%2520applications%2520across%2520rapidly%250Aevolving%2520open-source%2520and%2520proprietary%2520AI/ML%2520models%252C%2520datasets%252C%2520software%2520and%250Ahardware.%2520I%2520achieved%2520that%2520with%2520the%2520help%2520of%2520portable%252C%2520reusable%2520and%250Atechnology-agnostic%2520automation%2520recipes%2520%2528ResearchOps%2529%2520for%2520MLOps%2520and%2520DevOps%250A%2528CM4MLOps%2529%2520discovered%2520in%2520close%2520collaboration%2520with%2520academia%2520and%2520industry%2520when%250Areproducing%2520more%2520than%2520150%2520research%2520papers%2520and%2520organizing%2520the%25201st%2520mass-scale%250Acommunity%2520benchmarking%2520of%2520ML%2520and%2520AI%2520systems%2520using%2520CM%2520and%2520MLPerf.%250A%2520%2520I%2520donated%2520CM%2520and%2520CM4MLOps%2520to%2520MLCommons%2520to%2520help%2520connect%2520academia%2520and%2520industry%250Ato%2520learn%2520how%2520to%2520build%2520and%2520run%2520AI%2520and%2520other%2520emerging%2520workloads%2520in%2520the%2520most%250Aefficient%2520and%2520cost-effective%2520way%2520using%2520a%2520common%2520and%2520technology-agnostic%250Aautomation%252C%2520virtualization%2520and%2520reproducibility%2520framework%2520while%2520unifying%250Aknowledge%2520exchange%252C%2520protecting%2520everyone%2527s%2520intellectual%2520property%252C%2520enabling%250Aportable%2520skills%252C%2520and%2520accelerating%2520transfer%2520of%2520the%2520state-of-the-art%2520research%2520to%250Aproduction.%2520My%2520long-term%2520vision%2520is%2520to%2520make%2520AI%2520accessible%2520to%2520everyone%2520by%2520making%250Ait%2520a%2520commodity%2520automatically%2520produced%2520from%2520the%2520most%2520suitable%2520open-source%2520and%250Aproprietary%2520components%2520from%2520different%2520vendors%2520based%2520on%2520user%2520demand%252C%250Arequirements%2520and%2520constraints%2520such%2520as%2520cost%252C%2520latency%252C%2520throughput%252C%2520accuracy%252C%250Aenergy%252C%2520size%2520and%2520other%2520important%2520characteristics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enabling%20more%20efficient%20and%20cost-effective%20AI/ML%20systems%20with%20Collective%0A%20%20Mind%2C%20virtualized%20MLOps%2C%20MLPerf%2C%20Collective%20Knowledge%20Playground%20and%0A%20%20reproducible%20optimization%20tournaments&entry.906535625=Grigori%20Fursin&entry.1292438233=%20%20In%20this%20white%20paper%2C%20I%20present%20my%20community%20effort%20to%20automatically%20co-design%0Acheaper%2C%20faster%20and%20more%20energy-efficient%20software%20and%20hardware%20for%20AI%2C%20ML%20and%0Aother%20popular%20workloads%20with%20the%20help%20of%20the%20Collective%20Mind%20framework%20%28CM%29%2C%0Avirtualized%20MLOps%2C%20MLPerf%20benchmarks%20and%20reproducible%20optimization%20tournaments.%0AI%20developed%20CM%20to%20modularize%2C%20automate%20and%20virtualize%20the%20tedious%20process%20of%0Abuilding%2C%20running%2C%20profiling%20and%20optimizing%20complex%20applications%20across%20rapidly%0Aevolving%20open-source%20and%20proprietary%20AI/ML%20models%2C%20datasets%2C%20software%20and%0Ahardware.%20I%20achieved%20that%20with%20the%20help%20of%20portable%2C%20reusable%20and%0Atechnology-agnostic%20automation%20recipes%20%28ResearchOps%29%20for%20MLOps%20and%20DevOps%0A%28CM4MLOps%29%20discovered%20in%20close%20collaboration%20with%20academia%20and%20industry%20when%0Areproducing%20more%20than%20150%20research%20papers%20and%20organizing%20the%201st%20mass-scale%0Acommunity%20benchmarking%20of%20ML%20and%20AI%20systems%20using%20CM%20and%20MLPerf.%0A%20%20I%20donated%20CM%20and%20CM4MLOps%20to%20MLCommons%20to%20help%20connect%20academia%20and%20industry%0Ato%20learn%20how%20to%20build%20and%20run%20AI%20and%20other%20emerging%20workloads%20in%20the%20most%0Aefficient%20and%20cost-effective%20way%20using%20a%20common%20and%20technology-agnostic%0Aautomation%2C%20virtualization%20and%20reproducibility%20framework%20while%20unifying%0Aknowledge%20exchange%2C%20protecting%20everyone%27s%20intellectual%20property%2C%20enabling%0Aportable%20skills%2C%20and%20accelerating%20transfer%20of%20the%20state-of-the-art%20research%20to%0Aproduction.%20My%20long-term%20vision%20is%20to%20make%20AI%20accessible%20to%20everyone%20by%20making%0Ait%20a%20commodity%20automatically%20produced%20from%20the%20most%20suitable%20open-source%20and%0Aproprietary%20components%20from%20different%20vendors%20based%20on%20user%20demand%2C%0Arequirements%20and%20constraints%20such%20as%20cost%2C%20latency%2C%20throughput%2C%20accuracy%2C%0Aenergy%2C%20size%20and%20other%20important%20characteristics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16791v1&entry.124074799=Read"},
{"title": "Carrot and Stick: Inducing Self-Motivation with Positive & Negative\n  Feedback", "author": "Jimin Sohn and Jeihee Cho and Junyong Lee and Songmu Heo and Ji-Eun Han and David R. Mortensen", "abstract": "  Positive thinking is thought to be an important component of self-motivation\nin various practical fields such as education and the workplace. Previous work,\nincluding sentiment transfer and positive reframing, has focused on the\npositive side of language. However, self-motivation that drives people to reach\ntheir goals has not yet been studied from a computational perspective.\nMoreover, negative feedback has not yet been explored, even though positive and\nnegative feedback are both necessary to grow self-motivation. To facilitate\nself-motivation, we propose CArrot and STICk (CASTIC) dataset, consisting of\n12,590 sentences with 5 different strategies for enhancing self-motivation. Our\ndata and code are publicly available at here.\n", "link": "http://arxiv.org/abs/2406.16521v1", "date": "2024-06-24", "relevancy": 1.279, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4327}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4272}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Carrot%20and%20Stick%3A%20Inducing%20Self-Motivation%20with%20Positive%20%26%20Negative%0A%20%20Feedback&body=Title%3A%20Carrot%20and%20Stick%3A%20Inducing%20Self-Motivation%20with%20Positive%20%26%20Negative%0A%20%20Feedback%0AAuthor%3A%20Jimin%20Sohn%20and%20Jeihee%20Cho%20and%20Junyong%20Lee%20and%20Songmu%20Heo%20and%20Ji-Eun%20Han%20and%20David%20R.%20Mortensen%0AAbstract%3A%20%20%20Positive%20thinking%20is%20thought%20to%20be%20an%20important%20component%20of%20self-motivation%0Ain%20various%20practical%20fields%20such%20as%20education%20and%20the%20workplace.%20Previous%20work%2C%0Aincluding%20sentiment%20transfer%20and%20positive%20reframing%2C%20has%20focused%20on%20the%0Apositive%20side%20of%20language.%20However%2C%20self-motivation%20that%20drives%20people%20to%20reach%0Atheir%20goals%20has%20not%20yet%20been%20studied%20from%20a%20computational%20perspective.%0AMoreover%2C%20negative%20feedback%20has%20not%20yet%20been%20explored%2C%20even%20though%20positive%20and%0Anegative%20feedback%20are%20both%20necessary%20to%20grow%20self-motivation.%20To%20facilitate%0Aself-motivation%2C%20we%20propose%20CArrot%20and%20STICk%20%28CASTIC%29%20dataset%2C%20consisting%20of%0A12%2C590%20sentences%20with%205%20different%20strategies%20for%20enhancing%20self-motivation.%20Our%0Adata%20and%20code%20are%20publicly%20available%20at%20here.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCarrot%2520and%2520Stick%253A%2520Inducing%2520Self-Motivation%2520with%2520Positive%2520%2526%2520Negative%250A%2520%2520Feedback%26entry.906535625%3DJimin%2520Sohn%2520and%2520Jeihee%2520Cho%2520and%2520Junyong%2520Lee%2520and%2520Songmu%2520Heo%2520and%2520Ji-Eun%2520Han%2520and%2520David%2520R.%2520Mortensen%26entry.1292438233%3D%2520%2520Positive%2520thinking%2520is%2520thought%2520to%2520be%2520an%2520important%2520component%2520of%2520self-motivation%250Ain%2520various%2520practical%2520fields%2520such%2520as%2520education%2520and%2520the%2520workplace.%2520Previous%2520work%252C%250Aincluding%2520sentiment%2520transfer%2520and%2520positive%2520reframing%252C%2520has%2520focused%2520on%2520the%250Apositive%2520side%2520of%2520language.%2520However%252C%2520self-motivation%2520that%2520drives%2520people%2520to%2520reach%250Atheir%2520goals%2520has%2520not%2520yet%2520been%2520studied%2520from%2520a%2520computational%2520perspective.%250AMoreover%252C%2520negative%2520feedback%2520has%2520not%2520yet%2520been%2520explored%252C%2520even%2520though%2520positive%2520and%250Anegative%2520feedback%2520are%2520both%2520necessary%2520to%2520grow%2520self-motivation.%2520To%2520facilitate%250Aself-motivation%252C%2520we%2520propose%2520CArrot%2520and%2520STICk%2520%2528CASTIC%2529%2520dataset%252C%2520consisting%2520of%250A12%252C590%2520sentences%2520with%25205%2520different%2520strategies%2520for%2520enhancing%2520self-motivation.%2520Our%250Adata%2520and%2520code%2520are%2520publicly%2520available%2520at%2520here.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Carrot%20and%20Stick%3A%20Inducing%20Self-Motivation%20with%20Positive%20%26%20Negative%0A%20%20Feedback&entry.906535625=Jimin%20Sohn%20and%20Jeihee%20Cho%20and%20Junyong%20Lee%20and%20Songmu%20Heo%20and%20Ji-Eun%20Han%20and%20David%20R.%20Mortensen&entry.1292438233=%20%20Positive%20thinking%20is%20thought%20to%20be%20an%20important%20component%20of%20self-motivation%0Ain%20various%20practical%20fields%20such%20as%20education%20and%20the%20workplace.%20Previous%20work%2C%0Aincluding%20sentiment%20transfer%20and%20positive%20reframing%2C%20has%20focused%20on%20the%0Apositive%20side%20of%20language.%20However%2C%20self-motivation%20that%20drives%20people%20to%20reach%0Atheir%20goals%20has%20not%20yet%20been%20studied%20from%20a%20computational%20perspective.%0AMoreover%2C%20negative%20feedback%20has%20not%20yet%20been%20explored%2C%20even%20though%20positive%20and%0Anegative%20feedback%20are%20both%20necessary%20to%20grow%20self-motivation.%20To%20facilitate%0Aself-motivation%2C%20we%20propose%20CArrot%20and%20STICk%20%28CASTIC%29%20dataset%2C%20consisting%20of%0A12%2C590%20sentences%20with%205%20different%20strategies%20for%20enhancing%20self-motivation.%20Our%0Adata%20and%20code%20are%20publicly%20available%20at%20here.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16521v1&entry.124074799=Read"},
{"title": "Inducing Group Fairness in LLM-Based Decisions", "author": "James Atwood and Preethi Lahoti and Ananth Balashankar and Flavien Prost and Ahmad Beirami", "abstract": "  Prompting Large Language Models (LLMs) has created new and interesting means\nfor classifying textual data. While evaluating and remediating group fairness\nis a well-studied problem in classifier fairness literature, some classical\napproaches (e.g., regularization) do not carry over, and some new opportunities\narise (e.g., prompt-based remediation). We measure fairness of LLM-based\nclassifiers on a toxicity classification task, and empirically show that\nprompt-based classifiers may lead to unfair decisions. We introduce several\nremediation techniques and benchmark their fairness and performance trade-offs.\nWe hope our work encourages more research on group fairness in LLM-based\nclassifiers.\n", "link": "http://arxiv.org/abs/2406.16738v1", "date": "2024-06-24", "relevancy": 1.3423, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4581}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.448}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inducing%20Group%20Fairness%20in%20LLM-Based%20Decisions&body=Title%3A%20Inducing%20Group%20Fairness%20in%20LLM-Based%20Decisions%0AAuthor%3A%20James%20Atwood%20and%20Preethi%20Lahoti%20and%20Ananth%20Balashankar%20and%20Flavien%20Prost%20and%20Ahmad%20Beirami%0AAbstract%3A%20%20%20Prompting%20Large%20Language%20Models%20%28LLMs%29%20has%20created%20new%20and%20interesting%20means%0Afor%20classifying%20textual%20data.%20While%20evaluating%20and%20remediating%20group%20fairness%0Ais%20a%20well-studied%20problem%20in%20classifier%20fairness%20literature%2C%20some%20classical%0Aapproaches%20%28e.g.%2C%20regularization%29%20do%20not%20carry%20over%2C%20and%20some%20new%20opportunities%0Aarise%20%28e.g.%2C%20prompt-based%20remediation%29.%20We%20measure%20fairness%20of%20LLM-based%0Aclassifiers%20on%20a%20toxicity%20classification%20task%2C%20and%20empirically%20show%20that%0Aprompt-based%20classifiers%20may%20lead%20to%20unfair%20decisions.%20We%20introduce%20several%0Aremediation%20techniques%20and%20benchmark%20their%20fairness%20and%20performance%20trade-offs.%0AWe%20hope%20our%20work%20encourages%20more%20research%20on%20group%20fairness%20in%20LLM-based%0Aclassifiers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInducing%2520Group%2520Fairness%2520in%2520LLM-Based%2520Decisions%26entry.906535625%3DJames%2520Atwood%2520and%2520Preethi%2520Lahoti%2520and%2520Ananth%2520Balashankar%2520and%2520Flavien%2520Prost%2520and%2520Ahmad%2520Beirami%26entry.1292438233%3D%2520%2520Prompting%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520created%2520new%2520and%2520interesting%2520means%250Afor%2520classifying%2520textual%2520data.%2520While%2520evaluating%2520and%2520remediating%2520group%2520fairness%250Ais%2520a%2520well-studied%2520problem%2520in%2520classifier%2520fairness%2520literature%252C%2520some%2520classical%250Aapproaches%2520%2528e.g.%252C%2520regularization%2529%2520do%2520not%2520carry%2520over%252C%2520and%2520some%2520new%2520opportunities%250Aarise%2520%2528e.g.%252C%2520prompt-based%2520remediation%2529.%2520We%2520measure%2520fairness%2520of%2520LLM-based%250Aclassifiers%2520on%2520a%2520toxicity%2520classification%2520task%252C%2520and%2520empirically%2520show%2520that%250Aprompt-based%2520classifiers%2520may%2520lead%2520to%2520unfair%2520decisions.%2520We%2520introduce%2520several%250Aremediation%2520techniques%2520and%2520benchmark%2520their%2520fairness%2520and%2520performance%2520trade-offs.%250AWe%2520hope%2520our%2520work%2520encourages%2520more%2520research%2520on%2520group%2520fairness%2520in%2520LLM-based%250Aclassifiers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inducing%20Group%20Fairness%20in%20LLM-Based%20Decisions&entry.906535625=James%20Atwood%20and%20Preethi%20Lahoti%20and%20Ananth%20Balashankar%20and%20Flavien%20Prost%20and%20Ahmad%20Beirami&entry.1292438233=%20%20Prompting%20Large%20Language%20Models%20%28LLMs%29%20has%20created%20new%20and%20interesting%20means%0Afor%20classifying%20textual%20data.%20While%20evaluating%20and%20remediating%20group%20fairness%0Ais%20a%20well-studied%20problem%20in%20classifier%20fairness%20literature%2C%20some%20classical%0Aapproaches%20%28e.g.%2C%20regularization%29%20do%20not%20carry%20over%2C%20and%20some%20new%20opportunities%0Aarise%20%28e.g.%2C%20prompt-based%20remediation%29.%20We%20measure%20fairness%20of%20LLM-based%0Aclassifiers%20on%20a%20toxicity%20classification%20task%2C%20and%20empirically%20show%20that%0Aprompt-based%20classifiers%20may%20lead%20to%20unfair%20decisions.%20We%20introduce%20several%0Aremediation%20techniques%20and%20benchmark%20their%20fairness%20and%20performance%20trade-offs.%0AWe%20hope%20our%20work%20encourages%20more%20research%20on%20group%20fairness%20in%20LLM-based%0Aclassifiers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16738v1&entry.124074799=Read"},
{"title": "Data-driven Modeling in Metrology -- A Short Introduction, Current\n  Developments and Future Perspectives", "author": "Linda-Sophie Schneider and Patrick Krauss and Nadine Schiering and Christopher Syben and Richard Schielein and Andreas Maier", "abstract": "  Mathematical models are vital to the field of metrology, playing a key role\nin the derivation of measurement results and the calculation of uncertainties\nfrom measurement data, informed by an understanding of the measurement process.\nThese models generally represent the correlation between the quantity being\nmeasured and all other pertinent quantities. Such relationships are used to\nconstruct measurement systems that can interpret measurement data to generate\nconclusions and predictions about the measurement system itself. Classic models\nare typically analytical, built on fundamental physical principles. However,\nthe rise of digital technology, expansive sensor networks, and high-performance\ncomputing hardware have led to a growing shift towards data-driven\nmethodologies. This trend is especially prominent when dealing with large,\nintricate networked sensor systems in situations where there is limited expert\nunderstanding of the frequently changing real-world contexts. Here, we\ndemonstrate the variety of opportunities that data-driven modeling presents,\nand how they have been already implemented in various real-world applications.\n", "link": "http://arxiv.org/abs/2406.16659v1", "date": "2024-06-24", "relevancy": 1.4216, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4893}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4735}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-driven%20Modeling%20in%20Metrology%20--%20A%20Short%20Introduction%2C%20Current%0A%20%20Developments%20and%20Future%20Perspectives&body=Title%3A%20Data-driven%20Modeling%20in%20Metrology%20--%20A%20Short%20Introduction%2C%20Current%0A%20%20Developments%20and%20Future%20Perspectives%0AAuthor%3A%20Linda-Sophie%20Schneider%20and%20Patrick%20Krauss%20and%20Nadine%20Schiering%20and%20Christopher%20Syben%20and%20Richard%20Schielein%20and%20Andreas%20Maier%0AAbstract%3A%20%20%20Mathematical%20models%20are%20vital%20to%20the%20field%20of%20metrology%2C%20playing%20a%20key%20role%0Ain%20the%20derivation%20of%20measurement%20results%20and%20the%20calculation%20of%20uncertainties%0Afrom%20measurement%20data%2C%20informed%20by%20an%20understanding%20of%20the%20measurement%20process.%0AThese%20models%20generally%20represent%20the%20correlation%20between%20the%20quantity%20being%0Ameasured%20and%20all%20other%20pertinent%20quantities.%20Such%20relationships%20are%20used%20to%0Aconstruct%20measurement%20systems%20that%20can%20interpret%20measurement%20data%20to%20generate%0Aconclusions%20and%20predictions%20about%20the%20measurement%20system%20itself.%20Classic%20models%0Aare%20typically%20analytical%2C%20built%20on%20fundamental%20physical%20principles.%20However%2C%0Athe%20rise%20of%20digital%20technology%2C%20expansive%20sensor%20networks%2C%20and%20high-performance%0Acomputing%20hardware%20have%20led%20to%20a%20growing%20shift%20towards%20data-driven%0Amethodologies.%20This%20trend%20is%20especially%20prominent%20when%20dealing%20with%20large%2C%0Aintricate%20networked%20sensor%20systems%20in%20situations%20where%20there%20is%20limited%20expert%0Aunderstanding%20of%20the%20frequently%20changing%20real-world%20contexts.%20Here%2C%20we%0Ademonstrate%20the%20variety%20of%20opportunities%20that%20data-driven%20modeling%20presents%2C%0Aand%20how%20they%20have%20been%20already%20implemented%20in%20various%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-driven%2520Modeling%2520in%2520Metrology%2520--%2520A%2520Short%2520Introduction%252C%2520Current%250A%2520%2520Developments%2520and%2520Future%2520Perspectives%26entry.906535625%3DLinda-Sophie%2520Schneider%2520and%2520Patrick%2520Krauss%2520and%2520Nadine%2520Schiering%2520and%2520Christopher%2520Syben%2520and%2520Richard%2520Schielein%2520and%2520Andreas%2520Maier%26entry.1292438233%3D%2520%2520Mathematical%2520models%2520are%2520vital%2520to%2520the%2520field%2520of%2520metrology%252C%2520playing%2520a%2520key%2520role%250Ain%2520the%2520derivation%2520of%2520measurement%2520results%2520and%2520the%2520calculation%2520of%2520uncertainties%250Afrom%2520measurement%2520data%252C%2520informed%2520by%2520an%2520understanding%2520of%2520the%2520measurement%2520process.%250AThese%2520models%2520generally%2520represent%2520the%2520correlation%2520between%2520the%2520quantity%2520being%250Ameasured%2520and%2520all%2520other%2520pertinent%2520quantities.%2520Such%2520relationships%2520are%2520used%2520to%250Aconstruct%2520measurement%2520systems%2520that%2520can%2520interpret%2520measurement%2520data%2520to%2520generate%250Aconclusions%2520and%2520predictions%2520about%2520the%2520measurement%2520system%2520itself.%2520Classic%2520models%250Aare%2520typically%2520analytical%252C%2520built%2520on%2520fundamental%2520physical%2520principles.%2520However%252C%250Athe%2520rise%2520of%2520digital%2520technology%252C%2520expansive%2520sensor%2520networks%252C%2520and%2520high-performance%250Acomputing%2520hardware%2520have%2520led%2520to%2520a%2520growing%2520shift%2520towards%2520data-driven%250Amethodologies.%2520This%2520trend%2520is%2520especially%2520prominent%2520when%2520dealing%2520with%2520large%252C%250Aintricate%2520networked%2520sensor%2520systems%2520in%2520situations%2520where%2520there%2520is%2520limited%2520expert%250Aunderstanding%2520of%2520the%2520frequently%2520changing%2520real-world%2520contexts.%2520Here%252C%2520we%250Ademonstrate%2520the%2520variety%2520of%2520opportunities%2520that%2520data-driven%2520modeling%2520presents%252C%250Aand%2520how%2520they%2520have%2520been%2520already%2520implemented%2520in%2520various%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-driven%20Modeling%20in%20Metrology%20--%20A%20Short%20Introduction%2C%20Current%0A%20%20Developments%20and%20Future%20Perspectives&entry.906535625=Linda-Sophie%20Schneider%20and%20Patrick%20Krauss%20and%20Nadine%20Schiering%20and%20Christopher%20Syben%20and%20Richard%20Schielein%20and%20Andreas%20Maier&entry.1292438233=%20%20Mathematical%20models%20are%20vital%20to%20the%20field%20of%20metrology%2C%20playing%20a%20key%20role%0Ain%20the%20derivation%20of%20measurement%20results%20and%20the%20calculation%20of%20uncertainties%0Afrom%20measurement%20data%2C%20informed%20by%20an%20understanding%20of%20the%20measurement%20process.%0AThese%20models%20generally%20represent%20the%20correlation%20between%20the%20quantity%20being%0Ameasured%20and%20all%20other%20pertinent%20quantities.%20Such%20relationships%20are%20used%20to%0Aconstruct%20measurement%20systems%20that%20can%20interpret%20measurement%20data%20to%20generate%0Aconclusions%20and%20predictions%20about%20the%20measurement%20system%20itself.%20Classic%20models%0Aare%20typically%20analytical%2C%20built%20on%20fundamental%20physical%20principles.%20However%2C%0Athe%20rise%20of%20digital%20technology%2C%20expansive%20sensor%20networks%2C%20and%20high-performance%0Acomputing%20hardware%20have%20led%20to%20a%20growing%20shift%20towards%20data-driven%0Amethodologies.%20This%20trend%20is%20especially%20prominent%20when%20dealing%20with%20large%2C%0Aintricate%20networked%20sensor%20systems%20in%20situations%20where%20there%20is%20limited%20expert%0Aunderstanding%20of%20the%20frequently%20changing%20real-world%20contexts.%20Here%2C%20we%0Ademonstrate%20the%20variety%20of%20opportunities%20that%20data-driven%20modeling%20presents%2C%0Aand%20how%20they%20have%20been%20already%20implemented%20in%20various%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16659v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


