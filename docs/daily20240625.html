<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240624.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "PSAvatar: A Point-based Shape Model for Real-Time Head Avatar Animation\n  with 3D Gaussian Splatting", "author": "Zhongyuan Zhao and Zhenyu Bao and Qing Li and Guoping Qiu and Kanglin Liu", "abstract": "  Despite much progress, achieving real-time high-fidelity head avatar\nanimation is still difficult and existing methods have to trade-off between\nspeed and quality. 3DMM based methods often fail to model non-facial structures\nsuch as eyeglasses and hairstyles, while neural implicit models suffer from\ndeformation inflexibility and rendering inefficiency. Although 3D Gaussian has\nbeen demonstrated to possess promising capability for geometry representation\nand radiance field reconstruction, applying 3D Gaussian in head avatar creation\nremains a major challenge since it is difficult for 3D Gaussian to model the\nhead shape variations caused by changing poses and expressions. In this paper,\nwe introduce PSAvatar, a novel framework for animatable head avatar creation\nthat utilizes discrete geometric primitive to create a parametric morphable\nshape model and employs 3D Gaussian for fine detail representation and high\nfidelity rendering. The parametric morphable shape model is a Point-based\nMorphable Shape Model (PMSM) which uses points instead of meshes for 3D\nrepresentation to achieve enhanced representation flexibility. The PMSM first\nconverts the FLAME mesh to points by sampling on the surfaces as well as off\nthe meshes to enable the reconstruction of not only surface-like structures but\nalso complex geometries such as eyeglasses and hairstyles. By aligning these\npoints with the head shape in an analysis-by-synthesis manner, the PMSM makes\nit possible to utilize 3D Gaussian for fine detail representation and\nappearance modeling, thus enabling the creation of high-fidelity avatars. We\nshow that PSAvatar can reconstruct high-fidelity head avatars of a variety of\nsubjects and the avatars can be animated in real-time ($\\ge$ 25 fps at a\nresolution of 512 $\\times$ 512 ).\n", "link": "http://arxiv.org/abs/2401.12900v5", "date": "2024-06-24", "relevancy": 3.7221, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.8059}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.8059}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PSAvatar%3A%20A%20Point-based%20Shape%20Model%20for%20Real-Time%20Head%20Avatar%20Animation%0A%20%20with%203D%20Gaussian%20Splatting&body=Title%3A%20PSAvatar%3A%20A%20Point-based%20Shape%20Model%20for%20Real-Time%20Head%20Avatar%20Animation%0A%20%20with%203D%20Gaussian%20Splatting%0AAuthor%3A%20Zhongyuan%20Zhao%20and%20Zhenyu%20Bao%20and%20Qing%20Li%20and%20Guoping%20Qiu%20and%20Kanglin%20Liu%0AAbstract%3A%20%20%20Despite%20much%20progress%2C%20achieving%20real-time%20high-fidelity%20head%20avatar%0Aanimation%20is%20still%20difficult%20and%20existing%20methods%20have%20to%20trade-off%20between%0Aspeed%20and%20quality.%203DMM%20based%20methods%20often%20fail%20to%20model%20non-facial%20structures%0Asuch%20as%20eyeglasses%20and%20hairstyles%2C%20while%20neural%20implicit%20models%20suffer%20from%0Adeformation%20inflexibility%20and%20rendering%20inefficiency.%20Although%203D%20Gaussian%20has%0Abeen%20demonstrated%20to%20possess%20promising%20capability%20for%20geometry%20representation%0Aand%20radiance%20field%20reconstruction%2C%20applying%203D%20Gaussian%20in%20head%20avatar%20creation%0Aremains%20a%20major%20challenge%20since%20it%20is%20difficult%20for%203D%20Gaussian%20to%20model%20the%0Ahead%20shape%20variations%20caused%20by%20changing%20poses%20and%20expressions.%20In%20this%20paper%2C%0Awe%20introduce%20PSAvatar%2C%20a%20novel%20framework%20for%20animatable%20head%20avatar%20creation%0Athat%20utilizes%20discrete%20geometric%20primitive%20to%20create%20a%20parametric%20morphable%0Ashape%20model%20and%20employs%203D%20Gaussian%20for%20fine%20detail%20representation%20and%20high%0Afidelity%20rendering.%20The%20parametric%20morphable%20shape%20model%20is%20a%20Point-based%0AMorphable%20Shape%20Model%20%28PMSM%29%20which%20uses%20points%20instead%20of%20meshes%20for%203D%0Arepresentation%20to%20achieve%20enhanced%20representation%20flexibility.%20The%20PMSM%20first%0Aconverts%20the%20FLAME%20mesh%20to%20points%20by%20sampling%20on%20the%20surfaces%20as%20well%20as%20off%0Athe%20meshes%20to%20enable%20the%20reconstruction%20of%20not%20only%20surface-like%20structures%20but%0Aalso%20complex%20geometries%20such%20as%20eyeglasses%20and%20hairstyles.%20By%20aligning%20these%0Apoints%20with%20the%20head%20shape%20in%20an%20analysis-by-synthesis%20manner%2C%20the%20PMSM%20makes%0Ait%20possible%20to%20utilize%203D%20Gaussian%20for%20fine%20detail%20representation%20and%0Aappearance%20modeling%2C%20thus%20enabling%20the%20creation%20of%20high-fidelity%20avatars.%20We%0Ashow%20that%20PSAvatar%20can%20reconstruct%20high-fidelity%20head%20avatars%20of%20a%20variety%20of%0Asubjects%20and%20the%20avatars%20can%20be%20animated%20in%20real-time%20%28%24%5Cge%24%2025%20fps%20at%20a%0Aresolution%20of%20512%20%24%5Ctimes%24%20512%20%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.12900v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPSAvatar%253A%2520A%2520Point-based%2520Shape%2520Model%2520for%2520Real-Time%2520Head%2520Avatar%2520Animation%250A%2520%2520with%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DZhongyuan%2520Zhao%2520and%2520Zhenyu%2520Bao%2520and%2520Qing%2520Li%2520and%2520Guoping%2520Qiu%2520and%2520Kanglin%2520Liu%26entry.1292438233%3D%2520%2520Despite%2520much%2520progress%252C%2520achieving%2520real-time%2520high-fidelity%2520head%2520avatar%250Aanimation%2520is%2520still%2520difficult%2520and%2520existing%2520methods%2520have%2520to%2520trade-off%2520between%250Aspeed%2520and%2520quality.%25203DMM%2520based%2520methods%2520often%2520fail%2520to%2520model%2520non-facial%2520structures%250Asuch%2520as%2520eyeglasses%2520and%2520hairstyles%252C%2520while%2520neural%2520implicit%2520models%2520suffer%2520from%250Adeformation%2520inflexibility%2520and%2520rendering%2520inefficiency.%2520Although%25203D%2520Gaussian%2520has%250Abeen%2520demonstrated%2520to%2520possess%2520promising%2520capability%2520for%2520geometry%2520representation%250Aand%2520radiance%2520field%2520reconstruction%252C%2520applying%25203D%2520Gaussian%2520in%2520head%2520avatar%2520creation%250Aremains%2520a%2520major%2520challenge%2520since%2520it%2520is%2520difficult%2520for%25203D%2520Gaussian%2520to%2520model%2520the%250Ahead%2520shape%2520variations%2520caused%2520by%2520changing%2520poses%2520and%2520expressions.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520PSAvatar%252C%2520a%2520novel%2520framework%2520for%2520animatable%2520head%2520avatar%2520creation%250Athat%2520utilizes%2520discrete%2520geometric%2520primitive%2520to%2520create%2520a%2520parametric%2520morphable%250Ashape%2520model%2520and%2520employs%25203D%2520Gaussian%2520for%2520fine%2520detail%2520representation%2520and%2520high%250Afidelity%2520rendering.%2520The%2520parametric%2520morphable%2520shape%2520model%2520is%2520a%2520Point-based%250AMorphable%2520Shape%2520Model%2520%2528PMSM%2529%2520which%2520uses%2520points%2520instead%2520of%2520meshes%2520for%25203D%250Arepresentation%2520to%2520achieve%2520enhanced%2520representation%2520flexibility.%2520The%2520PMSM%2520first%250Aconverts%2520the%2520FLAME%2520mesh%2520to%2520points%2520by%2520sampling%2520on%2520the%2520surfaces%2520as%2520well%2520as%2520off%250Athe%2520meshes%2520to%2520enable%2520the%2520reconstruction%2520of%2520not%2520only%2520surface-like%2520structures%2520but%250Aalso%2520complex%2520geometries%2520such%2520as%2520eyeglasses%2520and%2520hairstyles.%2520By%2520aligning%2520these%250Apoints%2520with%2520the%2520head%2520shape%2520in%2520an%2520analysis-by-synthesis%2520manner%252C%2520the%2520PMSM%2520makes%250Ait%2520possible%2520to%2520utilize%25203D%2520Gaussian%2520for%2520fine%2520detail%2520representation%2520and%250Aappearance%2520modeling%252C%2520thus%2520enabling%2520the%2520creation%2520of%2520high-fidelity%2520avatars.%2520We%250Ashow%2520that%2520PSAvatar%2520can%2520reconstruct%2520high-fidelity%2520head%2520avatars%2520of%2520a%2520variety%2520of%250Asubjects%2520and%2520the%2520avatars%2520can%2520be%2520animated%2520in%2520real-time%2520%2528%2524%255Cge%2524%252025%2520fps%2520at%2520a%250Aresolution%2520of%2520512%2520%2524%255Ctimes%2524%2520512%2520%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.12900v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PSAvatar%3A%20A%20Point-based%20Shape%20Model%20for%20Real-Time%20Head%20Avatar%20Animation%0A%20%20with%203D%20Gaussian%20Splatting&entry.906535625=Zhongyuan%20Zhao%20and%20Zhenyu%20Bao%20and%20Qing%20Li%20and%20Guoping%20Qiu%20and%20Kanglin%20Liu&entry.1292438233=%20%20Despite%20much%20progress%2C%20achieving%20real-time%20high-fidelity%20head%20avatar%0Aanimation%20is%20still%20difficult%20and%20existing%20methods%20have%20to%20trade-off%20between%0Aspeed%20and%20quality.%203DMM%20based%20methods%20often%20fail%20to%20model%20non-facial%20structures%0Asuch%20as%20eyeglasses%20and%20hairstyles%2C%20while%20neural%20implicit%20models%20suffer%20from%0Adeformation%20inflexibility%20and%20rendering%20inefficiency.%20Although%203D%20Gaussian%20has%0Abeen%20demonstrated%20to%20possess%20promising%20capability%20for%20geometry%20representation%0Aand%20radiance%20field%20reconstruction%2C%20applying%203D%20Gaussian%20in%20head%20avatar%20creation%0Aremains%20a%20major%20challenge%20since%20it%20is%20difficult%20for%203D%20Gaussian%20to%20model%20the%0Ahead%20shape%20variations%20caused%20by%20changing%20poses%20and%20expressions.%20In%20this%20paper%2C%0Awe%20introduce%20PSAvatar%2C%20a%20novel%20framework%20for%20animatable%20head%20avatar%20creation%0Athat%20utilizes%20discrete%20geometric%20primitive%20to%20create%20a%20parametric%20morphable%0Ashape%20model%20and%20employs%203D%20Gaussian%20for%20fine%20detail%20representation%20and%20high%0Afidelity%20rendering.%20The%20parametric%20morphable%20shape%20model%20is%20a%20Point-based%0AMorphable%20Shape%20Model%20%28PMSM%29%20which%20uses%20points%20instead%20of%20meshes%20for%203D%0Arepresentation%20to%20achieve%20enhanced%20representation%20flexibility.%20The%20PMSM%20first%0Aconverts%20the%20FLAME%20mesh%20to%20points%20by%20sampling%20on%20the%20surfaces%20as%20well%20as%20off%0Athe%20meshes%20to%20enable%20the%20reconstruction%20of%20not%20only%20surface-like%20structures%20but%0Aalso%20complex%20geometries%20such%20as%20eyeglasses%20and%20hairstyles.%20By%20aligning%20these%0Apoints%20with%20the%20head%20shape%20in%20an%20analysis-by-synthesis%20manner%2C%20the%20PMSM%20makes%0Ait%20possible%20to%20utilize%203D%20Gaussian%20for%20fine%20detail%20representation%20and%0Aappearance%20modeling%2C%20thus%20enabling%20the%20creation%20of%20high-fidelity%20avatars.%20We%0Ashow%20that%20PSAvatar%20can%20reconstruct%20high-fidelity%20head%20avatars%20of%20a%20variety%20of%0Asubjects%20and%20the%20avatars%20can%20be%20animated%20in%20real-time%20%28%24%5Cge%24%2025%20fps%20at%20a%0Aresolution%20of%20512%20%24%5Ctimes%24%20512%20%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.12900v5&entry.124074799=Read"},
{"title": "RaDe-GS: Rasterizing Depth in Gaussian Splatting", "author": "Baowen Zhang and Chuan Fang and Rakesh Shrestha and Yixun Liang and Xiaoxiao Long and Ping Tan", "abstract": "  Gaussian Splatting (GS) has proven to be highly effective in novel view\nsynthesis, achieving high-quality and real-time rendering. However, its\npotential for reconstructing detailed 3D shapes has not been fully explored.\nExisting methods often suffer from limited shape accuracy due to the discrete\nand unstructured nature of Gaussian splats, which complicates the shape\nextraction. While recent techniques like 2D GS have attempted to improve shape\nreconstruction, they often reformulate the Gaussian primitives in ways that\nreduce both rendering quality and computational efficiency. To address these\nproblems, our work introduces a rasterized approach to render the depth maps\nand surface normal maps of general 3D Gaussian splats. Our method not only\nsignificantly enhances shape reconstruction accuracy but also maintains the\ncomputational efficiency intrinsic to Gaussian Splatting. It achieves a Chamfer\ndistance error comparable to NeuraLangelo on the DTU dataset and maintains\nsimilar computational efficiency as the original 3D GS methods. Our method is a\nsignificant advancement in Gaussian Splatting and can be directly integrated\ninto existing Gaussian Splatting-based methods.\n", "link": "http://arxiv.org/abs/2406.01467v2", "date": "2024-06-24", "relevancy": 3.2984, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7822}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6479}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RaDe-GS%3A%20Rasterizing%20Depth%20in%20Gaussian%20Splatting&body=Title%3A%20RaDe-GS%3A%20Rasterizing%20Depth%20in%20Gaussian%20Splatting%0AAuthor%3A%20Baowen%20Zhang%20and%20Chuan%20Fang%20and%20Rakesh%20Shrestha%20and%20Yixun%20Liang%20and%20Xiaoxiao%20Long%20and%20Ping%20Tan%0AAbstract%3A%20%20%20Gaussian%20Splatting%20%28GS%29%20has%20proven%20to%20be%20highly%20effective%20in%20novel%20view%0Asynthesis%2C%20achieving%20high-quality%20and%20real-time%20rendering.%20However%2C%20its%0Apotential%20for%20reconstructing%20detailed%203D%20shapes%20has%20not%20been%20fully%20explored.%0AExisting%20methods%20often%20suffer%20from%20limited%20shape%20accuracy%20due%20to%20the%20discrete%0Aand%20unstructured%20nature%20of%20Gaussian%20splats%2C%20which%20complicates%20the%20shape%0Aextraction.%20While%20recent%20techniques%20like%202D%20GS%20have%20attempted%20to%20improve%20shape%0Areconstruction%2C%20they%20often%20reformulate%20the%20Gaussian%20primitives%20in%20ways%20that%0Areduce%20both%20rendering%20quality%20and%20computational%20efficiency.%20To%20address%20these%0Aproblems%2C%20our%20work%20introduces%20a%20rasterized%20approach%20to%20render%20the%20depth%20maps%0Aand%20surface%20normal%20maps%20of%20general%203D%20Gaussian%20splats.%20Our%20method%20not%20only%0Asignificantly%20enhances%20shape%20reconstruction%20accuracy%20but%20also%20maintains%20the%0Acomputational%20efficiency%20intrinsic%20to%20Gaussian%20Splatting.%20It%20achieves%20a%20Chamfer%0Adistance%20error%20comparable%20to%20NeuraLangelo%20on%20the%20DTU%20dataset%20and%20maintains%0Asimilar%20computational%20efficiency%20as%20the%20original%203D%20GS%20methods.%20Our%20method%20is%20a%0Asignificant%20advancement%20in%20Gaussian%20Splatting%20and%20can%20be%20directly%20integrated%0Ainto%20existing%20Gaussian%20Splatting-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01467v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaDe-GS%253A%2520Rasterizing%2520Depth%2520in%2520Gaussian%2520Splatting%26entry.906535625%3DBaowen%2520Zhang%2520and%2520Chuan%2520Fang%2520and%2520Rakesh%2520Shrestha%2520and%2520Yixun%2520Liang%2520and%2520Xiaoxiao%2520Long%2520and%2520Ping%2520Tan%26entry.1292438233%3D%2520%2520Gaussian%2520Splatting%2520%2528GS%2529%2520has%2520proven%2520to%2520be%2520highly%2520effective%2520in%2520novel%2520view%250Asynthesis%252C%2520achieving%2520high-quality%2520and%2520real-time%2520rendering.%2520However%252C%2520its%250Apotential%2520for%2520reconstructing%2520detailed%25203D%2520shapes%2520has%2520not%2520been%2520fully%2520explored.%250AExisting%2520methods%2520often%2520suffer%2520from%2520limited%2520shape%2520accuracy%2520due%2520to%2520the%2520discrete%250Aand%2520unstructured%2520nature%2520of%2520Gaussian%2520splats%252C%2520which%2520complicates%2520the%2520shape%250Aextraction.%2520While%2520recent%2520techniques%2520like%25202D%2520GS%2520have%2520attempted%2520to%2520improve%2520shape%250Areconstruction%252C%2520they%2520often%2520reformulate%2520the%2520Gaussian%2520primitives%2520in%2520ways%2520that%250Areduce%2520both%2520rendering%2520quality%2520and%2520computational%2520efficiency.%2520To%2520address%2520these%250Aproblems%252C%2520our%2520work%2520introduces%2520a%2520rasterized%2520approach%2520to%2520render%2520the%2520depth%2520maps%250Aand%2520surface%2520normal%2520maps%2520of%2520general%25203D%2520Gaussian%2520splats.%2520Our%2520method%2520not%2520only%250Asignificantly%2520enhances%2520shape%2520reconstruction%2520accuracy%2520but%2520also%2520maintains%2520the%250Acomputational%2520efficiency%2520intrinsic%2520to%2520Gaussian%2520Splatting.%2520It%2520achieves%2520a%2520Chamfer%250Adistance%2520error%2520comparable%2520to%2520NeuraLangelo%2520on%2520the%2520DTU%2520dataset%2520and%2520maintains%250Asimilar%2520computational%2520efficiency%2520as%2520the%2520original%25203D%2520GS%2520methods.%2520Our%2520method%2520is%2520a%250Asignificant%2520advancement%2520in%2520Gaussian%2520Splatting%2520and%2520can%2520be%2520directly%2520integrated%250Ainto%2520existing%2520Gaussian%2520Splatting-based%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01467v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RaDe-GS%3A%20Rasterizing%20Depth%20in%20Gaussian%20Splatting&entry.906535625=Baowen%20Zhang%20and%20Chuan%20Fang%20and%20Rakesh%20Shrestha%20and%20Yixun%20Liang%20and%20Xiaoxiao%20Long%20and%20Ping%20Tan&entry.1292438233=%20%20Gaussian%20Splatting%20%28GS%29%20has%20proven%20to%20be%20highly%20effective%20in%20novel%20view%0Asynthesis%2C%20achieving%20high-quality%20and%20real-time%20rendering.%20However%2C%20its%0Apotential%20for%20reconstructing%20detailed%203D%20shapes%20has%20not%20been%20fully%20explored.%0AExisting%20methods%20often%20suffer%20from%20limited%20shape%20accuracy%20due%20to%20the%20discrete%0Aand%20unstructured%20nature%20of%20Gaussian%20splats%2C%20which%20complicates%20the%20shape%0Aextraction.%20While%20recent%20techniques%20like%202D%20GS%20have%20attempted%20to%20improve%20shape%0Areconstruction%2C%20they%20often%20reformulate%20the%20Gaussian%20primitives%20in%20ways%20that%0Areduce%20both%20rendering%20quality%20and%20computational%20efficiency.%20To%20address%20these%0Aproblems%2C%20our%20work%20introduces%20a%20rasterized%20approach%20to%20render%20the%20depth%20maps%0Aand%20surface%20normal%20maps%20of%20general%203D%20Gaussian%20splats.%20Our%20method%20not%20only%0Asignificantly%20enhances%20shape%20reconstruction%20accuracy%20but%20also%20maintains%20the%0Acomputational%20efficiency%20intrinsic%20to%20Gaussian%20Splatting.%20It%20achieves%20a%20Chamfer%0Adistance%20error%20comparable%20to%20NeuraLangelo%20on%20the%20DTU%20dataset%20and%20maintains%0Asimilar%20computational%20efficiency%20as%20the%20original%203D%20GS%20methods.%20Our%20method%20is%20a%0Asignificant%20advancement%20in%20Gaussian%20Splatting%20and%20can%20be%20directly%20integrated%0Ainto%20existing%20Gaussian%20Splatting-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01467v2&entry.124074799=Read"},
{"title": "YouDream: Generating Anatomically Controllable Consistent Text-to-3D\n  Animals", "author": "Sandeep Mishra and Oindrila Saha and Alan C. Bovik", "abstract": "  3D generation guided by text-to-image diffusion models enables the creation\nof visually compelling assets. However previous methods explore generation\nbased on image or text. The boundaries of creativity are limited by what can be\nexpressed through words or the images that can be sourced. We present YouDream,\na method to generate high-quality anatomically controllable animals. YouDream\nis guided using a text-to-image diffusion model controlled by 2D views of a 3D\npose prior. Our method generates 3D animals that are not possible to create\nusing previous text-to-3D generative methods. Additionally, our method is\ncapable of preserving anatomic consistency in the generated animals, an area\nwhere prior text-to-3D approaches often struggle. Moreover, we design a fully\nautomated pipeline for generating commonly found animals. To circumvent the\nneed for human intervention to create a 3D pose, we propose a multi-agent LLM\nthat adapts poses from a limited library of animal 3D poses to represent the\ndesired animal. A user study conducted on the outcomes of YouDream demonstrates\nthe preference of the animal models generated by our method over others.\nTurntable results and code are released at https://youdream3d.github.io/\n", "link": "http://arxiv.org/abs/2406.16273v1", "date": "2024-06-24", "relevancy": 3.2192, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6618}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6618}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YouDream%3A%20Generating%20Anatomically%20Controllable%20Consistent%20Text-to-3D%0A%20%20Animals&body=Title%3A%20YouDream%3A%20Generating%20Anatomically%20Controllable%20Consistent%20Text-to-3D%0A%20%20Animals%0AAuthor%3A%20Sandeep%20Mishra%20and%20Oindrila%20Saha%20and%20Alan%20C.%20Bovik%0AAbstract%3A%20%20%203D%20generation%20guided%20by%20text-to-image%20diffusion%20models%20enables%20the%20creation%0Aof%20visually%20compelling%20assets.%20However%20previous%20methods%20explore%20generation%0Abased%20on%20image%20or%20text.%20The%20boundaries%20of%20creativity%20are%20limited%20by%20what%20can%20be%0Aexpressed%20through%20words%20or%20the%20images%20that%20can%20be%20sourced.%20We%20present%20YouDream%2C%0Aa%20method%20to%20generate%20high-quality%20anatomically%20controllable%20animals.%20YouDream%0Ais%20guided%20using%20a%20text-to-image%20diffusion%20model%20controlled%20by%202D%20views%20of%20a%203D%0Apose%20prior.%20Our%20method%20generates%203D%20animals%20that%20are%20not%20possible%20to%20create%0Ausing%20previous%20text-to-3D%20generative%20methods.%20Additionally%2C%20our%20method%20is%0Acapable%20of%20preserving%20anatomic%20consistency%20in%20the%20generated%20animals%2C%20an%20area%0Awhere%20prior%20text-to-3D%20approaches%20often%20struggle.%20Moreover%2C%20we%20design%20a%20fully%0Aautomated%20pipeline%20for%20generating%20commonly%20found%20animals.%20To%20circumvent%20the%0Aneed%20for%20human%20intervention%20to%20create%20a%203D%20pose%2C%20we%20propose%20a%20multi-agent%20LLM%0Athat%20adapts%20poses%20from%20a%20limited%20library%20of%20animal%203D%20poses%20to%20represent%20the%0Adesired%20animal.%20A%20user%20study%20conducted%20on%20the%20outcomes%20of%20YouDream%20demonstrates%0Athe%20preference%20of%20the%20animal%20models%20generated%20by%20our%20method%20over%20others.%0ATurntable%20results%20and%20code%20are%20released%20at%20https%3A//youdream3d.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYouDream%253A%2520Generating%2520Anatomically%2520Controllable%2520Consistent%2520Text-to-3D%250A%2520%2520Animals%26entry.906535625%3DSandeep%2520Mishra%2520and%2520Oindrila%2520Saha%2520and%2520Alan%2520C.%2520Bovik%26entry.1292438233%3D%2520%25203D%2520generation%2520guided%2520by%2520text-to-image%2520diffusion%2520models%2520enables%2520the%2520creation%250Aof%2520visually%2520compelling%2520assets.%2520However%2520previous%2520methods%2520explore%2520generation%250Abased%2520on%2520image%2520or%2520text.%2520The%2520boundaries%2520of%2520creativity%2520are%2520limited%2520by%2520what%2520can%2520be%250Aexpressed%2520through%2520words%2520or%2520the%2520images%2520that%2520can%2520be%2520sourced.%2520We%2520present%2520YouDream%252C%250Aa%2520method%2520to%2520generate%2520high-quality%2520anatomically%2520controllable%2520animals.%2520YouDream%250Ais%2520guided%2520using%2520a%2520text-to-image%2520diffusion%2520model%2520controlled%2520by%25202D%2520views%2520of%2520a%25203D%250Apose%2520prior.%2520Our%2520method%2520generates%25203D%2520animals%2520that%2520are%2520not%2520possible%2520to%2520create%250Ausing%2520previous%2520text-to-3D%2520generative%2520methods.%2520Additionally%252C%2520our%2520method%2520is%250Acapable%2520of%2520preserving%2520anatomic%2520consistency%2520in%2520the%2520generated%2520animals%252C%2520an%2520area%250Awhere%2520prior%2520text-to-3D%2520approaches%2520often%2520struggle.%2520Moreover%252C%2520we%2520design%2520a%2520fully%250Aautomated%2520pipeline%2520for%2520generating%2520commonly%2520found%2520animals.%2520To%2520circumvent%2520the%250Aneed%2520for%2520human%2520intervention%2520to%2520create%2520a%25203D%2520pose%252C%2520we%2520propose%2520a%2520multi-agent%2520LLM%250Athat%2520adapts%2520poses%2520from%2520a%2520limited%2520library%2520of%2520animal%25203D%2520poses%2520to%2520represent%2520the%250Adesired%2520animal.%2520A%2520user%2520study%2520conducted%2520on%2520the%2520outcomes%2520of%2520YouDream%2520demonstrates%250Athe%2520preference%2520of%2520the%2520animal%2520models%2520generated%2520by%2520our%2520method%2520over%2520others.%250ATurntable%2520results%2520and%2520code%2520are%2520released%2520at%2520https%253A//youdream3d.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YouDream%3A%20Generating%20Anatomically%20Controllable%20Consistent%20Text-to-3D%0A%20%20Animals&entry.906535625=Sandeep%20Mishra%20and%20Oindrila%20Saha%20and%20Alan%20C.%20Bovik&entry.1292438233=%20%203D%20generation%20guided%20by%20text-to-image%20diffusion%20models%20enables%20the%20creation%0Aof%20visually%20compelling%20assets.%20However%20previous%20methods%20explore%20generation%0Abased%20on%20image%20or%20text.%20The%20boundaries%20of%20creativity%20are%20limited%20by%20what%20can%20be%0Aexpressed%20through%20words%20or%20the%20images%20that%20can%20be%20sourced.%20We%20present%20YouDream%2C%0Aa%20method%20to%20generate%20high-quality%20anatomically%20controllable%20animals.%20YouDream%0Ais%20guided%20using%20a%20text-to-image%20diffusion%20model%20controlled%20by%202D%20views%20of%20a%203D%0Apose%20prior.%20Our%20method%20generates%203D%20animals%20that%20are%20not%20possible%20to%20create%0Ausing%20previous%20text-to-3D%20generative%20methods.%20Additionally%2C%20our%20method%20is%0Acapable%20of%20preserving%20anatomic%20consistency%20in%20the%20generated%20animals%2C%20an%20area%0Awhere%20prior%20text-to-3D%20approaches%20often%20struggle.%20Moreover%2C%20we%20design%20a%20fully%0Aautomated%20pipeline%20for%20generating%20commonly%20found%20animals.%20To%20circumvent%20the%0Aneed%20for%20human%20intervention%20to%20create%20a%203D%20pose%2C%20we%20propose%20a%20multi-agent%20LLM%0Athat%20adapts%20poses%20from%20a%20limited%20library%20of%20animal%203D%20poses%20to%20represent%20the%0Adesired%20animal.%20A%20user%20study%20conducted%20on%20the%20outcomes%20of%20YouDream%20demonstrates%0Athe%20preference%20of%20the%20animal%20models%20generated%20by%20our%20method%20over%20others.%0ATurntable%20results%20and%20code%20are%20released%20at%20https%3A//youdream3d.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16273v1&entry.124074799=Read"},
{"title": "Crowd-Sourced NeRF: Collecting Data from Production Vehicles for 3D\n  Street View Reconstruction", "author": "Tong Qin and Changze Li and Haoyang Ye and Shaowei Wan and Minzhen Li and Hongwei Liu and Ming Yang", "abstract": "  Recently, Neural Radiance Fields (NeRF) achieved impressive results in novel\nview synthesis. Block-NeRF showed the capability of leveraging NeRF to build\nlarge city-scale models. For large-scale modeling, a mass of image data is\nnecessary. Collecting images from specially designed data-collection vehicles\ncan not support large-scale applications. How to acquire massive high-quality\ndata remains an opening problem. Noting that the automotive industry has a huge\namount of image data, crowd-sourcing is a convenient way for large-scale data\ncollection. In this paper, we present a crowd-sourced framework, which utilizes\nsubstantial data captured by production vehicles to reconstruct the scene with\nthe NeRF model. This approach solves the key problem of large-scale\nreconstruction, that is where the data comes from and how to use them. Firstly,\nthe crowd-sourced massive data is filtered to remove redundancy and keep a\nbalanced distribution in terms of time and space. Then a structure-from-motion\nmodule is performed to refine camera poses. Finally, images, as well as poses,\nare used to train the NeRF model in a certain block. We highlight that we\npresent a comprehensive framework that integrates multiple modules, including\ndata selection, sparse 3D reconstruction, sequence appearance embedding, depth\nsupervision of ground surface, and occlusion completion. The complete system is\ncapable of effectively processing and reconstructing high-quality 3D scenes\nfrom crowd-sourced data. Extensive quantitative and qualitative experiments\nwere conducted to validate the performance of our system. Moreover, we proposed\nan application, named first-view navigation, which leveraged the NeRF model to\ngenerate 3D street view and guide the driver with a synthesized video.\n", "link": "http://arxiv.org/abs/2406.16289v1", "date": "2024-06-24", "relevancy": 3.1747, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6373}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6373}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Crowd-Sourced%20NeRF%3A%20Collecting%20Data%20from%20Production%20Vehicles%20for%203D%0A%20%20Street%20View%20Reconstruction&body=Title%3A%20Crowd-Sourced%20NeRF%3A%20Collecting%20Data%20from%20Production%20Vehicles%20for%203D%0A%20%20Street%20View%20Reconstruction%0AAuthor%3A%20Tong%20Qin%20and%20Changze%20Li%20and%20Haoyang%20Ye%20and%20Shaowei%20Wan%20and%20Minzhen%20Li%20and%20Hongwei%20Liu%20and%20Ming%20Yang%0AAbstract%3A%20%20%20Recently%2C%20Neural%20Radiance%20Fields%20%28NeRF%29%20achieved%20impressive%20results%20in%20novel%0Aview%20synthesis.%20Block-NeRF%20showed%20the%20capability%20of%20leveraging%20NeRF%20to%20build%0Alarge%20city-scale%20models.%20For%20large-scale%20modeling%2C%20a%20mass%20of%20image%20data%20is%0Anecessary.%20Collecting%20images%20from%20specially%20designed%20data-collection%20vehicles%0Acan%20not%20support%20large-scale%20applications.%20How%20to%20acquire%20massive%20high-quality%0Adata%20remains%20an%20opening%20problem.%20Noting%20that%20the%20automotive%20industry%20has%20a%20huge%0Aamount%20of%20image%20data%2C%20crowd-sourcing%20is%20a%20convenient%20way%20for%20large-scale%20data%0Acollection.%20In%20this%20paper%2C%20we%20present%20a%20crowd-sourced%20framework%2C%20which%20utilizes%0Asubstantial%20data%20captured%20by%20production%20vehicles%20to%20reconstruct%20the%20scene%20with%0Athe%20NeRF%20model.%20This%20approach%20solves%20the%20key%20problem%20of%20large-scale%0Areconstruction%2C%20that%20is%20where%20the%20data%20comes%20from%20and%20how%20to%20use%20them.%20Firstly%2C%0Athe%20crowd-sourced%20massive%20data%20is%20filtered%20to%20remove%20redundancy%20and%20keep%20a%0Abalanced%20distribution%20in%20terms%20of%20time%20and%20space.%20Then%20a%20structure-from-motion%0Amodule%20is%20performed%20to%20refine%20camera%20poses.%20Finally%2C%20images%2C%20as%20well%20as%20poses%2C%0Aare%20used%20to%20train%20the%20NeRF%20model%20in%20a%20certain%20block.%20We%20highlight%20that%20we%0Apresent%20a%20comprehensive%20framework%20that%20integrates%20multiple%20modules%2C%20including%0Adata%20selection%2C%20sparse%203D%20reconstruction%2C%20sequence%20appearance%20embedding%2C%20depth%0Asupervision%20of%20ground%20surface%2C%20and%20occlusion%20completion.%20The%20complete%20system%20is%0Acapable%20of%20effectively%20processing%20and%20reconstructing%20high-quality%203D%20scenes%0Afrom%20crowd-sourced%20data.%20Extensive%20quantitative%20and%20qualitative%20experiments%0Awere%20conducted%20to%20validate%20the%20performance%20of%20our%20system.%20Moreover%2C%20we%20proposed%0Aan%20application%2C%20named%20first-view%20navigation%2C%20which%20leveraged%20the%20NeRF%20model%20to%0Agenerate%203D%20street%20view%20and%20guide%20the%20driver%20with%20a%20synthesized%20video.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16289v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrowd-Sourced%2520NeRF%253A%2520Collecting%2520Data%2520from%2520Production%2520Vehicles%2520for%25203D%250A%2520%2520Street%2520View%2520Reconstruction%26entry.906535625%3DTong%2520Qin%2520and%2520Changze%2520Li%2520and%2520Haoyang%2520Ye%2520and%2520Shaowei%2520Wan%2520and%2520Minzhen%2520Li%2520and%2520Hongwei%2520Liu%2520and%2520Ming%2520Yang%26entry.1292438233%3D%2520%2520Recently%252C%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520achieved%2520impressive%2520results%2520in%2520novel%250Aview%2520synthesis.%2520Block-NeRF%2520showed%2520the%2520capability%2520of%2520leveraging%2520NeRF%2520to%2520build%250Alarge%2520city-scale%2520models.%2520For%2520large-scale%2520modeling%252C%2520a%2520mass%2520of%2520image%2520data%2520is%250Anecessary.%2520Collecting%2520images%2520from%2520specially%2520designed%2520data-collection%2520vehicles%250Acan%2520not%2520support%2520large-scale%2520applications.%2520How%2520to%2520acquire%2520massive%2520high-quality%250Adata%2520remains%2520an%2520opening%2520problem.%2520Noting%2520that%2520the%2520automotive%2520industry%2520has%2520a%2520huge%250Aamount%2520of%2520image%2520data%252C%2520crowd-sourcing%2520is%2520a%2520convenient%2520way%2520for%2520large-scale%2520data%250Acollection.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520crowd-sourced%2520framework%252C%2520which%2520utilizes%250Asubstantial%2520data%2520captured%2520by%2520production%2520vehicles%2520to%2520reconstruct%2520the%2520scene%2520with%250Athe%2520NeRF%2520model.%2520This%2520approach%2520solves%2520the%2520key%2520problem%2520of%2520large-scale%250Areconstruction%252C%2520that%2520is%2520where%2520the%2520data%2520comes%2520from%2520and%2520how%2520to%2520use%2520them.%2520Firstly%252C%250Athe%2520crowd-sourced%2520massive%2520data%2520is%2520filtered%2520to%2520remove%2520redundancy%2520and%2520keep%2520a%250Abalanced%2520distribution%2520in%2520terms%2520of%2520time%2520and%2520space.%2520Then%2520a%2520structure-from-motion%250Amodule%2520is%2520performed%2520to%2520refine%2520camera%2520poses.%2520Finally%252C%2520images%252C%2520as%2520well%2520as%2520poses%252C%250Aare%2520used%2520to%2520train%2520the%2520NeRF%2520model%2520in%2520a%2520certain%2520block.%2520We%2520highlight%2520that%2520we%250Apresent%2520a%2520comprehensive%2520framework%2520that%2520integrates%2520multiple%2520modules%252C%2520including%250Adata%2520selection%252C%2520sparse%25203D%2520reconstruction%252C%2520sequence%2520appearance%2520embedding%252C%2520depth%250Asupervision%2520of%2520ground%2520surface%252C%2520and%2520occlusion%2520completion.%2520The%2520complete%2520system%2520is%250Acapable%2520of%2520effectively%2520processing%2520and%2520reconstructing%2520high-quality%25203D%2520scenes%250Afrom%2520crowd-sourced%2520data.%2520Extensive%2520quantitative%2520and%2520qualitative%2520experiments%250Awere%2520conducted%2520to%2520validate%2520the%2520performance%2520of%2520our%2520system.%2520Moreover%252C%2520we%2520proposed%250Aan%2520application%252C%2520named%2520first-view%2520navigation%252C%2520which%2520leveraged%2520the%2520NeRF%2520model%2520to%250Agenerate%25203D%2520street%2520view%2520and%2520guide%2520the%2520driver%2520with%2520a%2520synthesized%2520video.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16289v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Crowd-Sourced%20NeRF%3A%20Collecting%20Data%20from%20Production%20Vehicles%20for%203D%0A%20%20Street%20View%20Reconstruction&entry.906535625=Tong%20Qin%20and%20Changze%20Li%20and%20Haoyang%20Ye%20and%20Shaowei%20Wan%20and%20Minzhen%20Li%20and%20Hongwei%20Liu%20and%20Ming%20Yang&entry.1292438233=%20%20Recently%2C%20Neural%20Radiance%20Fields%20%28NeRF%29%20achieved%20impressive%20results%20in%20novel%0Aview%20synthesis.%20Block-NeRF%20showed%20the%20capability%20of%20leveraging%20NeRF%20to%20build%0Alarge%20city-scale%20models.%20For%20large-scale%20modeling%2C%20a%20mass%20of%20image%20data%20is%0Anecessary.%20Collecting%20images%20from%20specially%20designed%20data-collection%20vehicles%0Acan%20not%20support%20large-scale%20applications.%20How%20to%20acquire%20massive%20high-quality%0Adata%20remains%20an%20opening%20problem.%20Noting%20that%20the%20automotive%20industry%20has%20a%20huge%0Aamount%20of%20image%20data%2C%20crowd-sourcing%20is%20a%20convenient%20way%20for%20large-scale%20data%0Acollection.%20In%20this%20paper%2C%20we%20present%20a%20crowd-sourced%20framework%2C%20which%20utilizes%0Asubstantial%20data%20captured%20by%20production%20vehicles%20to%20reconstruct%20the%20scene%20with%0Athe%20NeRF%20model.%20This%20approach%20solves%20the%20key%20problem%20of%20large-scale%0Areconstruction%2C%20that%20is%20where%20the%20data%20comes%20from%20and%20how%20to%20use%20them.%20Firstly%2C%0Athe%20crowd-sourced%20massive%20data%20is%20filtered%20to%20remove%20redundancy%20and%20keep%20a%0Abalanced%20distribution%20in%20terms%20of%20time%20and%20space.%20Then%20a%20structure-from-motion%0Amodule%20is%20performed%20to%20refine%20camera%20poses.%20Finally%2C%20images%2C%20as%20well%20as%20poses%2C%0Aare%20used%20to%20train%20the%20NeRF%20model%20in%20a%20certain%20block.%20We%20highlight%20that%20we%0Apresent%20a%20comprehensive%20framework%20that%20integrates%20multiple%20modules%2C%20including%0Adata%20selection%2C%20sparse%203D%20reconstruction%2C%20sequence%20appearance%20embedding%2C%20depth%0Asupervision%20of%20ground%20surface%2C%20and%20occlusion%20completion.%20The%20complete%20system%20is%0Acapable%20of%20effectively%20processing%20and%20reconstructing%20high-quality%203D%20scenes%0Afrom%20crowd-sourced%20data.%20Extensive%20quantitative%20and%20qualitative%20experiments%0Awere%20conducted%20to%20validate%20the%20performance%20of%20our%20system.%20Moreover%2C%20we%20proposed%0Aan%20application%2C%20named%20first-view%20navigation%2C%20which%20leveraged%20the%20NeRF%20model%20to%0Agenerate%203D%20street%20view%20and%20guide%20the%20driver%20with%20a%20synthesized%20video.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16289v1&entry.124074799=Read"},
{"title": "MIRReS: Multi-bounce Inverse Rendering using Reservoir Sampling", "author": "Yuxin Dai and Qi Wang and Jingsen Zhu and Dianbing Xi and Yuchi Huo and Chen Qian and Ying He", "abstract": "  We present MIRReS, a novel two-stage inverse rendering framework that jointly\nreconstructs and optimizes the explicit geometry, material, and lighting from\nmulti-view images. Unlike previous methods that rely on implicit irradiance\nfields or simplified path tracing algorithms, our method extracts an explicit\ngeometry (triangular mesh) in stage one, and introduces a more realistic\nphysically-based inverse rendering model that utilizes multi-bounce path\ntracing and Monte Carlo integration. By leveraging multi-bounce path tracing,\nour method effectively estimates indirect illumination, including\nself-shadowing and internal reflections, which improves the intrinsic\ndecomposition of shape, material, and lighting. Moreover, we incorporate\nreservoir sampling into our framework to address the noise in Monte Carlo\nintegration, enhancing convergence and facilitating gradient-based optimization\nwith low sample counts. Through qualitative and quantitative evaluation of\nseveral scenarios, especially in challenging scenarios with complex shadows, we\ndemonstrate that our method achieves state-of-the-art performance on\ndecomposition results. Additionally, our optimized explicit geometry enables\napplications such as scene editing, relighting, and material editing with\nmodern graphics engines or CAD software. The source code is available at\nhttps://brabbitdousha.github.io/MIRReS/\n", "link": "http://arxiv.org/abs/2406.16360v1", "date": "2024-06-24", "relevancy": 2.8301, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5661}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.566}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIRReS%3A%20Multi-bounce%20Inverse%20Rendering%20using%20Reservoir%20Sampling&body=Title%3A%20MIRReS%3A%20Multi-bounce%20Inverse%20Rendering%20using%20Reservoir%20Sampling%0AAuthor%3A%20Yuxin%20Dai%20and%20Qi%20Wang%20and%20Jingsen%20Zhu%20and%20Dianbing%20Xi%20and%20Yuchi%20Huo%20and%20Chen%20Qian%20and%20Ying%20He%0AAbstract%3A%20%20%20We%20present%20MIRReS%2C%20a%20novel%20two-stage%20inverse%20rendering%20framework%20that%20jointly%0Areconstructs%20and%20optimizes%20the%20explicit%20geometry%2C%20material%2C%20and%20lighting%20from%0Amulti-view%20images.%20Unlike%20previous%20methods%20that%20rely%20on%20implicit%20irradiance%0Afields%20or%20simplified%20path%20tracing%20algorithms%2C%20our%20method%20extracts%20an%20explicit%0Ageometry%20%28triangular%20mesh%29%20in%20stage%20one%2C%20and%20introduces%20a%20more%20realistic%0Aphysically-based%20inverse%20rendering%20model%20that%20utilizes%20multi-bounce%20path%0Atracing%20and%20Monte%20Carlo%20integration.%20By%20leveraging%20multi-bounce%20path%20tracing%2C%0Aour%20method%20effectively%20estimates%20indirect%20illumination%2C%20including%0Aself-shadowing%20and%20internal%20reflections%2C%20which%20improves%20the%20intrinsic%0Adecomposition%20of%20shape%2C%20material%2C%20and%20lighting.%20Moreover%2C%20we%20incorporate%0Areservoir%20sampling%20into%20our%20framework%20to%20address%20the%20noise%20in%20Monte%20Carlo%0Aintegration%2C%20enhancing%20convergence%20and%20facilitating%20gradient-based%20optimization%0Awith%20low%20sample%20counts.%20Through%20qualitative%20and%20quantitative%20evaluation%20of%0Aseveral%20scenarios%2C%20especially%20in%20challenging%20scenarios%20with%20complex%20shadows%2C%20we%0Ademonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20on%0Adecomposition%20results.%20Additionally%2C%20our%20optimized%20explicit%20geometry%20enables%0Aapplications%20such%20as%20scene%20editing%2C%20relighting%2C%20and%20material%20editing%20with%0Amodern%20graphics%20engines%20or%20CAD%20software.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//brabbitdousha.github.io/MIRReS/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIRReS%253A%2520Multi-bounce%2520Inverse%2520Rendering%2520using%2520Reservoir%2520Sampling%26entry.906535625%3DYuxin%2520Dai%2520and%2520Qi%2520Wang%2520and%2520Jingsen%2520Zhu%2520and%2520Dianbing%2520Xi%2520and%2520Yuchi%2520Huo%2520and%2520Chen%2520Qian%2520and%2520Ying%2520He%26entry.1292438233%3D%2520%2520We%2520present%2520MIRReS%252C%2520a%2520novel%2520two-stage%2520inverse%2520rendering%2520framework%2520that%2520jointly%250Areconstructs%2520and%2520optimizes%2520the%2520explicit%2520geometry%252C%2520material%252C%2520and%2520lighting%2520from%250Amulti-view%2520images.%2520Unlike%2520previous%2520methods%2520that%2520rely%2520on%2520implicit%2520irradiance%250Afields%2520or%2520simplified%2520path%2520tracing%2520algorithms%252C%2520our%2520method%2520extracts%2520an%2520explicit%250Ageometry%2520%2528triangular%2520mesh%2529%2520in%2520stage%2520one%252C%2520and%2520introduces%2520a%2520more%2520realistic%250Aphysically-based%2520inverse%2520rendering%2520model%2520that%2520utilizes%2520multi-bounce%2520path%250Atracing%2520and%2520Monte%2520Carlo%2520integration.%2520By%2520leveraging%2520multi-bounce%2520path%2520tracing%252C%250Aour%2520method%2520effectively%2520estimates%2520indirect%2520illumination%252C%2520including%250Aself-shadowing%2520and%2520internal%2520reflections%252C%2520which%2520improves%2520the%2520intrinsic%250Adecomposition%2520of%2520shape%252C%2520material%252C%2520and%2520lighting.%2520Moreover%252C%2520we%2520incorporate%250Areservoir%2520sampling%2520into%2520our%2520framework%2520to%2520address%2520the%2520noise%2520in%2520Monte%2520Carlo%250Aintegration%252C%2520enhancing%2520convergence%2520and%2520facilitating%2520gradient-based%2520optimization%250Awith%2520low%2520sample%2520counts.%2520Through%2520qualitative%2520and%2520quantitative%2520evaluation%2520of%250Aseveral%2520scenarios%252C%2520especially%2520in%2520challenging%2520scenarios%2520with%2520complex%2520shadows%252C%2520we%250Ademonstrate%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%2520on%250Adecomposition%2520results.%2520Additionally%252C%2520our%2520optimized%2520explicit%2520geometry%2520enables%250Aapplications%2520such%2520as%2520scene%2520editing%252C%2520relighting%252C%2520and%2520material%2520editing%2520with%250Amodern%2520graphics%2520engines%2520or%2520CAD%2520software.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//brabbitdousha.github.io/MIRReS/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIRReS%3A%20Multi-bounce%20Inverse%20Rendering%20using%20Reservoir%20Sampling&entry.906535625=Yuxin%20Dai%20and%20Qi%20Wang%20and%20Jingsen%20Zhu%20and%20Dianbing%20Xi%20and%20Yuchi%20Huo%20and%20Chen%20Qian%20and%20Ying%20He&entry.1292438233=%20%20We%20present%20MIRReS%2C%20a%20novel%20two-stage%20inverse%20rendering%20framework%20that%20jointly%0Areconstructs%20and%20optimizes%20the%20explicit%20geometry%2C%20material%2C%20and%20lighting%20from%0Amulti-view%20images.%20Unlike%20previous%20methods%20that%20rely%20on%20implicit%20irradiance%0Afields%20or%20simplified%20path%20tracing%20algorithms%2C%20our%20method%20extracts%20an%20explicit%0Ageometry%20%28triangular%20mesh%29%20in%20stage%20one%2C%20and%20introduces%20a%20more%20realistic%0Aphysically-based%20inverse%20rendering%20model%20that%20utilizes%20multi-bounce%20path%0Atracing%20and%20Monte%20Carlo%20integration.%20By%20leveraging%20multi-bounce%20path%20tracing%2C%0Aour%20method%20effectively%20estimates%20indirect%20illumination%2C%20including%0Aself-shadowing%20and%20internal%20reflections%2C%20which%20improves%20the%20intrinsic%0Adecomposition%20of%20shape%2C%20material%2C%20and%20lighting.%20Moreover%2C%20we%20incorporate%0Areservoir%20sampling%20into%20our%20framework%20to%20address%20the%20noise%20in%20Monte%20Carlo%0Aintegration%2C%20enhancing%20convergence%20and%20facilitating%20gradient-based%20optimization%0Awith%20low%20sample%20counts.%20Through%20qualitative%20and%20quantitative%20evaluation%20of%0Aseveral%20scenarios%2C%20especially%20in%20challenging%20scenarios%20with%20complex%20shadows%2C%20we%0Ademonstrate%20that%20our%20method%20achieves%20state-of-the-art%20performance%20on%0Adecomposition%20results.%20Additionally%2C%20our%20optimized%20explicit%20geometry%20enables%0Aapplications%20such%20as%20scene%20editing%2C%20relighting%2C%20and%20material%20editing%20with%0Amodern%20graphics%20engines%20or%20CAD%20software.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//brabbitdousha.github.io/MIRReS/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16360v1&entry.124074799=Read"},
{"title": "Non-rigid Structure-from-Motion: Temporally-smooth Procrustean Alignment\n  and Spatially-variant Deformation Modeling", "author": "Jiawei Shi and Hui Deng and Yuchao Dai", "abstract": "  Even though Non-rigid Structure-from-Motion (NRSfM) has been extensively\nstudied and great progress has been made, there are still key challenges that\nhinder their broad real-world applications: 1) the inherent motion/rotation\nambiguity requires either explicit camera motion recovery with extra constraint\nor complex Procrustean Alignment; 2) existing low-rank modeling of the global\nshape can over-penalize drastic deformations in the 3D shape sequence. This\npaper proposes to resolve the above issues from a spatial-temporal modeling\nperspective. First, we propose a novel Temporally-smooth Procrustean Alignment\nmodule that estimates 3D deforming shapes and adjusts the camera motion by\naligning the 3D shape sequence consecutively. Our new alignment module remedies\nthe requirement of complex reference 3D shape during alignment, which is more\nconductive to non-isotropic deformation modeling. Second, we propose a\nspatial-weighted approach to enforce the low-rank constraint adaptively at\ndifferent locations to accommodate drastic spatially-variant deformation\nreconstruction better. Our modeling outperform existing low-rank based methods,\nand extensive experiments across different datasets validate the effectiveness\nof our method.\n", "link": "http://arxiv.org/abs/2405.04309v2", "date": "2024-06-24", "relevancy": 2.8059, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5802}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5536}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-rigid%20Structure-from-Motion%3A%20Temporally-smooth%20Procrustean%20Alignment%0A%20%20and%20Spatially-variant%20Deformation%20Modeling&body=Title%3A%20Non-rigid%20Structure-from-Motion%3A%20Temporally-smooth%20Procrustean%20Alignment%0A%20%20and%20Spatially-variant%20Deformation%20Modeling%0AAuthor%3A%20Jiawei%20Shi%20and%20Hui%20Deng%20and%20Yuchao%20Dai%0AAbstract%3A%20%20%20Even%20though%20Non-rigid%20Structure-from-Motion%20%28NRSfM%29%20has%20been%20extensively%0Astudied%20and%20great%20progress%20has%20been%20made%2C%20there%20are%20still%20key%20challenges%20that%0Ahinder%20their%20broad%20real-world%20applications%3A%201%29%20the%20inherent%20motion/rotation%0Aambiguity%20requires%20either%20explicit%20camera%20motion%20recovery%20with%20extra%20constraint%0Aor%20complex%20Procrustean%20Alignment%3B%202%29%20existing%20low-rank%20modeling%20of%20the%20global%0Ashape%20can%20over-penalize%20drastic%20deformations%20in%20the%203D%20shape%20sequence.%20This%0Apaper%20proposes%20to%20resolve%20the%20above%20issues%20from%20a%20spatial-temporal%20modeling%0Aperspective.%20First%2C%20we%20propose%20a%20novel%20Temporally-smooth%20Procrustean%20Alignment%0Amodule%20that%20estimates%203D%20deforming%20shapes%20and%20adjusts%20the%20camera%20motion%20by%0Aaligning%20the%203D%20shape%20sequence%20consecutively.%20Our%20new%20alignment%20module%20remedies%0Athe%20requirement%20of%20complex%20reference%203D%20shape%20during%20alignment%2C%20which%20is%20more%0Aconductive%20to%20non-isotropic%20deformation%20modeling.%20Second%2C%20we%20propose%20a%0Aspatial-weighted%20approach%20to%20enforce%20the%20low-rank%20constraint%20adaptively%20at%0Adifferent%20locations%20to%20accommodate%20drastic%20spatially-variant%20deformation%0Areconstruction%20better.%20Our%20modeling%20outperform%20existing%20low-rank%20based%20methods%2C%0Aand%20extensive%20experiments%20across%20different%20datasets%20validate%20the%20effectiveness%0Aof%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04309v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-rigid%2520Structure-from-Motion%253A%2520Temporally-smooth%2520Procrustean%2520Alignment%250A%2520%2520and%2520Spatially-variant%2520Deformation%2520Modeling%26entry.906535625%3DJiawei%2520Shi%2520and%2520Hui%2520Deng%2520and%2520Yuchao%2520Dai%26entry.1292438233%3D%2520%2520Even%2520though%2520Non-rigid%2520Structure-from-Motion%2520%2528NRSfM%2529%2520has%2520been%2520extensively%250Astudied%2520and%2520great%2520progress%2520has%2520been%2520made%252C%2520there%2520are%2520still%2520key%2520challenges%2520that%250Ahinder%2520their%2520broad%2520real-world%2520applications%253A%25201%2529%2520the%2520inherent%2520motion/rotation%250Aambiguity%2520requires%2520either%2520explicit%2520camera%2520motion%2520recovery%2520with%2520extra%2520constraint%250Aor%2520complex%2520Procrustean%2520Alignment%253B%25202%2529%2520existing%2520low-rank%2520modeling%2520of%2520the%2520global%250Ashape%2520can%2520over-penalize%2520drastic%2520deformations%2520in%2520the%25203D%2520shape%2520sequence.%2520This%250Apaper%2520proposes%2520to%2520resolve%2520the%2520above%2520issues%2520from%2520a%2520spatial-temporal%2520modeling%250Aperspective.%2520First%252C%2520we%2520propose%2520a%2520novel%2520Temporally-smooth%2520Procrustean%2520Alignment%250Amodule%2520that%2520estimates%25203D%2520deforming%2520shapes%2520and%2520adjusts%2520the%2520camera%2520motion%2520by%250Aaligning%2520the%25203D%2520shape%2520sequence%2520consecutively.%2520Our%2520new%2520alignment%2520module%2520remedies%250Athe%2520requirement%2520of%2520complex%2520reference%25203D%2520shape%2520during%2520alignment%252C%2520which%2520is%2520more%250Aconductive%2520to%2520non-isotropic%2520deformation%2520modeling.%2520Second%252C%2520we%2520propose%2520a%250Aspatial-weighted%2520approach%2520to%2520enforce%2520the%2520low-rank%2520constraint%2520adaptively%2520at%250Adifferent%2520locations%2520to%2520accommodate%2520drastic%2520spatially-variant%2520deformation%250Areconstruction%2520better.%2520Our%2520modeling%2520outperform%2520existing%2520low-rank%2520based%2520methods%252C%250Aand%2520extensive%2520experiments%2520across%2520different%2520datasets%2520validate%2520the%2520effectiveness%250Aof%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04309v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-rigid%20Structure-from-Motion%3A%20Temporally-smooth%20Procrustean%20Alignment%0A%20%20and%20Spatially-variant%20Deformation%20Modeling&entry.906535625=Jiawei%20Shi%20and%20Hui%20Deng%20and%20Yuchao%20Dai&entry.1292438233=%20%20Even%20though%20Non-rigid%20Structure-from-Motion%20%28NRSfM%29%20has%20been%20extensively%0Astudied%20and%20great%20progress%20has%20been%20made%2C%20there%20are%20still%20key%20challenges%20that%0Ahinder%20their%20broad%20real-world%20applications%3A%201%29%20the%20inherent%20motion/rotation%0Aambiguity%20requires%20either%20explicit%20camera%20motion%20recovery%20with%20extra%20constraint%0Aor%20complex%20Procrustean%20Alignment%3B%202%29%20existing%20low-rank%20modeling%20of%20the%20global%0Ashape%20can%20over-penalize%20drastic%20deformations%20in%20the%203D%20shape%20sequence.%20This%0Apaper%20proposes%20to%20resolve%20the%20above%20issues%20from%20a%20spatial-temporal%20modeling%0Aperspective.%20First%2C%20we%20propose%20a%20novel%20Temporally-smooth%20Procrustean%20Alignment%0Amodule%20that%20estimates%203D%20deforming%20shapes%20and%20adjusts%20the%20camera%20motion%20by%0Aaligning%20the%203D%20shape%20sequence%20consecutively.%20Our%20new%20alignment%20module%20remedies%0Athe%20requirement%20of%20complex%20reference%203D%20shape%20during%20alignment%2C%20which%20is%20more%0Aconductive%20to%20non-isotropic%20deformation%20modeling.%20Second%2C%20we%20propose%20a%0Aspatial-weighted%20approach%20to%20enforce%20the%20low-rank%20constraint%20adaptively%20at%0Adifferent%20locations%20to%20accommodate%20drastic%20spatially-variant%20deformation%0Areconstruction%20better.%20Our%20modeling%20outperform%20existing%20low-rank%20based%20methods%2C%0Aand%20extensive%20experiments%20across%20different%20datasets%20validate%20the%20effectiveness%0Aof%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04309v2&entry.124074799=Read"},
{"title": "\u03bc-Net: A Deep Learning-Based Architecture for \u03bc-CT Segmentation", "author": "Pierangela Bruno and Edoardo De Rose and Carlo Adornetto and Francesco Calimeri and Sandro Donato and Raffaele Giuseppe Agostino and Daniela Amelio and Riccardo Barberi and Maria Carmela Cerra and Maria Caterina Crocco and Mariacristina Filice and Raffaele Filosa and Gianluigi Greco and Sandra Imbrogno and Vincenzo Formoso", "abstract": "  X-ray computed microtomography ({\\mu}-CT) is a non-destructive technique that\ncan generate high-resolution 3D images of the internal anatomy of medical and\nbiological samples. These images enable clinicians to examine internal anatomy\nand gain insights into the disease or anatomical morphology. However,\nextracting relevant information from 3D images requires semantic segmentation\nof the regions of interest, which is usually done manually and results\ntime-consuming and tedious. In this work, we propose a novel framework that\nuses a convolutional neural network (CNN) to automatically segment the full\nmorphology of the heart of Carassius auratus. The framework employs an\noptimized 2D CNN architecture that can infer a 3D segmentation of the sample,\navoiding the high computational cost of a 3D CNN architecture. We tackle the\nchallenges of handling large and high-resoluted image data (over a thousand\npixels in each dimension) and a small training database (only three samples) by\nproposing a standard protocol for data normalization and processing. Moreover,\nwe investigate how the noise, contrast, and spatial resolution of the sample\nand the training of the architecture are affected by the reconstruction\ntechnique, which depends on the number of input images. Experiments show that\nour framework significantly reduces the time required to segment new samples,\nallowing a faster microtomography analysis of the Carassius auratus heart\nshape. Furthermore, our framework can work with any bio-image (biological and\nmedical) from {\\mu}-CT with high-resolution and small dataset size\n", "link": "http://arxiv.org/abs/2406.16724v1", "date": "2024-06-24", "relevancy": 2.7926, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5771}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5493}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%CE%BC-Net%3A%20A%20Deep%20Learning-Based%20Architecture%20for%20%CE%BC-CT%20Segmentation&body=Title%3A%20%CE%BC-Net%3A%20A%20Deep%20Learning-Based%20Architecture%20for%20%CE%BC-CT%20Segmentation%0AAuthor%3A%20Pierangela%20Bruno%20and%20Edoardo%20De%20Rose%20and%20Carlo%20Adornetto%20and%20Francesco%20Calimeri%20and%20Sandro%20Donato%20and%20Raffaele%20Giuseppe%20Agostino%20and%20Daniela%20Amelio%20and%20Riccardo%20Barberi%20and%20Maria%20Carmela%20Cerra%20and%20Maria%20Caterina%20Crocco%20and%20Mariacristina%20Filice%20and%20Raffaele%20Filosa%20and%20Gianluigi%20Greco%20and%20Sandra%20Imbrogno%20and%20Vincenzo%20Formoso%0AAbstract%3A%20%20%20X-ray%20computed%20microtomography%20%28%7B%5Cmu%7D-CT%29%20is%20a%20non-destructive%20technique%20that%0Acan%20generate%20high-resolution%203D%20images%20of%20the%20internal%20anatomy%20of%20medical%20and%0Abiological%20samples.%20These%20images%20enable%20clinicians%20to%20examine%20internal%20anatomy%0Aand%20gain%20insights%20into%20the%20disease%20or%20anatomical%20morphology.%20However%2C%0Aextracting%20relevant%20information%20from%203D%20images%20requires%20semantic%20segmentation%0Aof%20the%20regions%20of%20interest%2C%20which%20is%20usually%20done%20manually%20and%20results%0Atime-consuming%20and%20tedious.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%20that%0Auses%20a%20convolutional%20neural%20network%20%28CNN%29%20to%20automatically%20segment%20the%20full%0Amorphology%20of%20the%20heart%20of%20Carassius%20auratus.%20The%20framework%20employs%20an%0Aoptimized%202D%20CNN%20architecture%20that%20can%20infer%20a%203D%20segmentation%20of%20the%20sample%2C%0Aavoiding%20the%20high%20computational%20cost%20of%20a%203D%20CNN%20architecture.%20We%20tackle%20the%0Achallenges%20of%20handling%20large%20and%20high-resoluted%20image%20data%20%28over%20a%20thousand%0Apixels%20in%20each%20dimension%29%20and%20a%20small%20training%20database%20%28only%20three%20samples%29%20by%0Aproposing%20a%20standard%20protocol%20for%20data%20normalization%20and%20processing.%20Moreover%2C%0Awe%20investigate%20how%20the%20noise%2C%20contrast%2C%20and%20spatial%20resolution%20of%20the%20sample%0Aand%20the%20training%20of%20the%20architecture%20are%20affected%20by%20the%20reconstruction%0Atechnique%2C%20which%20depends%20on%20the%20number%20of%20input%20images.%20Experiments%20show%20that%0Aour%20framework%20significantly%20reduces%20the%20time%20required%20to%20segment%20new%20samples%2C%0Aallowing%20a%20faster%20microtomography%20analysis%20of%20the%20Carassius%20auratus%20heart%0Ashape.%20Furthermore%2C%20our%20framework%20can%20work%20with%20any%20bio-image%20%28biological%20and%0Amedical%29%20from%20%7B%5Cmu%7D-CT%20with%20high-resolution%20and%20small%20dataset%20size%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%25CE%25BC-Net%253A%2520A%2520Deep%2520Learning-Based%2520Architecture%2520for%2520%25CE%25BC-CT%2520Segmentation%26entry.906535625%3DPierangela%2520Bruno%2520and%2520Edoardo%2520De%2520Rose%2520and%2520Carlo%2520Adornetto%2520and%2520Francesco%2520Calimeri%2520and%2520Sandro%2520Donato%2520and%2520Raffaele%2520Giuseppe%2520Agostino%2520and%2520Daniela%2520Amelio%2520and%2520Riccardo%2520Barberi%2520and%2520Maria%2520Carmela%2520Cerra%2520and%2520Maria%2520Caterina%2520Crocco%2520and%2520Mariacristina%2520Filice%2520and%2520Raffaele%2520Filosa%2520and%2520Gianluigi%2520Greco%2520and%2520Sandra%2520Imbrogno%2520and%2520Vincenzo%2520Formoso%26entry.1292438233%3D%2520%2520X-ray%2520computed%2520microtomography%2520%2528%257B%255Cmu%257D-CT%2529%2520is%2520a%2520non-destructive%2520technique%2520that%250Acan%2520generate%2520high-resolution%25203D%2520images%2520of%2520the%2520internal%2520anatomy%2520of%2520medical%2520and%250Abiological%2520samples.%2520These%2520images%2520enable%2520clinicians%2520to%2520examine%2520internal%2520anatomy%250Aand%2520gain%2520insights%2520into%2520the%2520disease%2520or%2520anatomical%2520morphology.%2520However%252C%250Aextracting%2520relevant%2520information%2520from%25203D%2520images%2520requires%2520semantic%2520segmentation%250Aof%2520the%2520regions%2520of%2520interest%252C%2520which%2520is%2520usually%2520done%2520manually%2520and%2520results%250Atime-consuming%2520and%2520tedious.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520framework%2520that%250Auses%2520a%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520to%2520automatically%2520segment%2520the%2520full%250Amorphology%2520of%2520the%2520heart%2520of%2520Carassius%2520auratus.%2520The%2520framework%2520employs%2520an%250Aoptimized%25202D%2520CNN%2520architecture%2520that%2520can%2520infer%2520a%25203D%2520segmentation%2520of%2520the%2520sample%252C%250Aavoiding%2520the%2520high%2520computational%2520cost%2520of%2520a%25203D%2520CNN%2520architecture.%2520We%2520tackle%2520the%250Achallenges%2520of%2520handling%2520large%2520and%2520high-resoluted%2520image%2520data%2520%2528over%2520a%2520thousand%250Apixels%2520in%2520each%2520dimension%2529%2520and%2520a%2520small%2520training%2520database%2520%2528only%2520three%2520samples%2529%2520by%250Aproposing%2520a%2520standard%2520protocol%2520for%2520data%2520normalization%2520and%2520processing.%2520Moreover%252C%250Awe%2520investigate%2520how%2520the%2520noise%252C%2520contrast%252C%2520and%2520spatial%2520resolution%2520of%2520the%2520sample%250Aand%2520the%2520training%2520of%2520the%2520architecture%2520are%2520affected%2520by%2520the%2520reconstruction%250Atechnique%252C%2520which%2520depends%2520on%2520the%2520number%2520of%2520input%2520images.%2520Experiments%2520show%2520that%250Aour%2520framework%2520significantly%2520reduces%2520the%2520time%2520required%2520to%2520segment%2520new%2520samples%252C%250Aallowing%2520a%2520faster%2520microtomography%2520analysis%2520of%2520the%2520Carassius%2520auratus%2520heart%250Ashape.%2520Furthermore%252C%2520our%2520framework%2520can%2520work%2520with%2520any%2520bio-image%2520%2528biological%2520and%250Amedical%2529%2520from%2520%257B%255Cmu%257D-CT%2520with%2520high-resolution%2520and%2520small%2520dataset%2520size%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%CE%BC-Net%3A%20A%20Deep%20Learning-Based%20Architecture%20for%20%CE%BC-CT%20Segmentation&entry.906535625=Pierangela%20Bruno%20and%20Edoardo%20De%20Rose%20and%20Carlo%20Adornetto%20and%20Francesco%20Calimeri%20and%20Sandro%20Donato%20and%20Raffaele%20Giuseppe%20Agostino%20and%20Daniela%20Amelio%20and%20Riccardo%20Barberi%20and%20Maria%20Carmela%20Cerra%20and%20Maria%20Caterina%20Crocco%20and%20Mariacristina%20Filice%20and%20Raffaele%20Filosa%20and%20Gianluigi%20Greco%20and%20Sandra%20Imbrogno%20and%20Vincenzo%20Formoso&entry.1292438233=%20%20X-ray%20computed%20microtomography%20%28%7B%5Cmu%7D-CT%29%20is%20a%20non-destructive%20technique%20that%0Acan%20generate%20high-resolution%203D%20images%20of%20the%20internal%20anatomy%20of%20medical%20and%0Abiological%20samples.%20These%20images%20enable%20clinicians%20to%20examine%20internal%20anatomy%0Aand%20gain%20insights%20into%20the%20disease%20or%20anatomical%20morphology.%20However%2C%0Aextracting%20relevant%20information%20from%203D%20images%20requires%20semantic%20segmentation%0Aof%20the%20regions%20of%20interest%2C%20which%20is%20usually%20done%20manually%20and%20results%0Atime-consuming%20and%20tedious.%20In%20this%20work%2C%20we%20propose%20a%20novel%20framework%20that%0Auses%20a%20convolutional%20neural%20network%20%28CNN%29%20to%20automatically%20segment%20the%20full%0Amorphology%20of%20the%20heart%20of%20Carassius%20auratus.%20The%20framework%20employs%20an%0Aoptimized%202D%20CNN%20architecture%20that%20can%20infer%20a%203D%20segmentation%20of%20the%20sample%2C%0Aavoiding%20the%20high%20computational%20cost%20of%20a%203D%20CNN%20architecture.%20We%20tackle%20the%0Achallenges%20of%20handling%20large%20and%20high-resoluted%20image%20data%20%28over%20a%20thousand%0Apixels%20in%20each%20dimension%29%20and%20a%20small%20training%20database%20%28only%20three%20samples%29%20by%0Aproposing%20a%20standard%20protocol%20for%20data%20normalization%20and%20processing.%20Moreover%2C%0Awe%20investigate%20how%20the%20noise%2C%20contrast%2C%20and%20spatial%20resolution%20of%20the%20sample%0Aand%20the%20training%20of%20the%20architecture%20are%20affected%20by%20the%20reconstruction%0Atechnique%2C%20which%20depends%20on%20the%20number%20of%20input%20images.%20Experiments%20show%20that%0Aour%20framework%20significantly%20reduces%20the%20time%20required%20to%20segment%20new%20samples%2C%0Aallowing%20a%20faster%20microtomography%20analysis%20of%20the%20Carassius%20auratus%20heart%0Ashape.%20Furthermore%2C%20our%20framework%20can%20work%20with%20any%20bio-image%20%28biological%20and%0Amedical%29%20from%20%7B%5Cmu%7D-CT%20with%20high-resolution%20and%20small%20dataset%20size%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16724v1&entry.124074799=Read"},
{"title": "MLAAN: Scaling Supervised Local Learning with Multilaminar Leap\n  Augmented Auxiliary Network", "author": "Yuming Zhang and Shouxin Zhang and Peizhe Wang and Feiyu Zhu and Dongzhi Guan and Jiabin Liu and Changpeng Cai", "abstract": "  End-to-end (E2E) training approaches are commonly plagued by high memory\nconsumption, reduced efficiency in training, challenges in model\nparallelization, and suboptimal biocompatibility. Local learning is considered\na novel interactive training method that holds promise as an alternative to\nE2E. Nonetheless, conventional local learning methods fall short in achieving\nhigh model accuracy due to inadequate local inter-module interactions. In this\npaper, we introduce a new model known as the Scaling Supervised Local Learning\nwith Multilaminar Leap Augmented Auxiliary Network (MLAAN). MLAAN features an\ninnovative supervised local learning approach coupled with a robust\nreinforcement module. This dual-component design enables the MLAAN to integrate\nsmoothly with established local learning techniques, thereby enhancing the\nefficacy of the foundational methods. The method simultaneously acquires the\nlocal and global features of the model separately by constructing an\nindependent auxiliary network and a cascade auxiliary network on the one hand\nand incorporates a leap augmented module, which serves to counteract the\nreduced learning capacity often associated with weaker supervision. This\narchitecture not only augments the exchange of information amongst the local\nmodules but also effectively mitigates the model's tendency toward myopia. The\nexperimental evaluations conducted on four benchmark datasets, CIFAR-10,\nSTL-10, SVHN, and ImageNet, demonstrate that the integration of MLAAN with\nexisting supervised local learning methods significantly enhances the original\nmethodologies. Of particular note, MLAAN enables local learning methods to\ncomprehensively outperform end-to-end training approaches in terms of optimal\nperformance while saving GPU memory.\n", "link": "http://arxiv.org/abs/2406.16633v1", "date": "2024-06-24", "relevancy": 2.7843, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5714}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5597}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLAAN%3A%20Scaling%20Supervised%20Local%20Learning%20with%20Multilaminar%20Leap%0A%20%20Augmented%20Auxiliary%20Network&body=Title%3A%20MLAAN%3A%20Scaling%20Supervised%20Local%20Learning%20with%20Multilaminar%20Leap%0A%20%20Augmented%20Auxiliary%20Network%0AAuthor%3A%20Yuming%20Zhang%20and%20Shouxin%20Zhang%20and%20Peizhe%20Wang%20and%20Feiyu%20Zhu%20and%20Dongzhi%20Guan%20and%20Jiabin%20Liu%20and%20Changpeng%20Cai%0AAbstract%3A%20%20%20End-to-end%20%28E2E%29%20training%20approaches%20are%20commonly%20plagued%20by%20high%20memory%0Aconsumption%2C%20reduced%20efficiency%20in%20training%2C%20challenges%20in%20model%0Aparallelization%2C%20and%20suboptimal%20biocompatibility.%20Local%20learning%20is%20considered%0Aa%20novel%20interactive%20training%20method%20that%20holds%20promise%20as%20an%20alternative%20to%0AE2E.%20Nonetheless%2C%20conventional%20local%20learning%20methods%20fall%20short%20in%20achieving%0Ahigh%20model%20accuracy%20due%20to%20inadequate%20local%20inter-module%20interactions.%20In%20this%0Apaper%2C%20we%20introduce%20a%20new%20model%20known%20as%20the%20Scaling%20Supervised%20Local%20Learning%0Awith%20Multilaminar%20Leap%20Augmented%20Auxiliary%20Network%20%28MLAAN%29.%20MLAAN%20features%20an%0Ainnovative%20supervised%20local%20learning%20approach%20coupled%20with%20a%20robust%0Areinforcement%20module.%20This%20dual-component%20design%20enables%20the%20MLAAN%20to%20integrate%0Asmoothly%20with%20established%20local%20learning%20techniques%2C%20thereby%20enhancing%20the%0Aefficacy%20of%20the%20foundational%20methods.%20The%20method%20simultaneously%20acquires%20the%0Alocal%20and%20global%20features%20of%20the%20model%20separately%20by%20constructing%20an%0Aindependent%20auxiliary%20network%20and%20a%20cascade%20auxiliary%20network%20on%20the%20one%20hand%0Aand%20incorporates%20a%20leap%20augmented%20module%2C%20which%20serves%20to%20counteract%20the%0Areduced%20learning%20capacity%20often%20associated%20with%20weaker%20supervision.%20This%0Aarchitecture%20not%20only%20augments%20the%20exchange%20of%20information%20amongst%20the%20local%0Amodules%20but%20also%20effectively%20mitigates%20the%20model%27s%20tendency%20toward%20myopia.%20The%0Aexperimental%20evaluations%20conducted%20on%20four%20benchmark%20datasets%2C%20CIFAR-10%2C%0ASTL-10%2C%20SVHN%2C%20and%20ImageNet%2C%20demonstrate%20that%20the%20integration%20of%20MLAAN%20with%0Aexisting%20supervised%20local%20learning%20methods%20significantly%20enhances%20the%20original%0Amethodologies.%20Of%20particular%20note%2C%20MLAAN%20enables%20local%20learning%20methods%20to%0Acomprehensively%20outperform%20end-to-end%20training%20approaches%20in%20terms%20of%20optimal%0Aperformance%20while%20saving%20GPU%20memory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLAAN%253A%2520Scaling%2520Supervised%2520Local%2520Learning%2520with%2520Multilaminar%2520Leap%250A%2520%2520Augmented%2520Auxiliary%2520Network%26entry.906535625%3DYuming%2520Zhang%2520and%2520Shouxin%2520Zhang%2520and%2520Peizhe%2520Wang%2520and%2520Feiyu%2520Zhu%2520and%2520Dongzhi%2520Guan%2520and%2520Jiabin%2520Liu%2520and%2520Changpeng%2520Cai%26entry.1292438233%3D%2520%2520End-to-end%2520%2528E2E%2529%2520training%2520approaches%2520are%2520commonly%2520plagued%2520by%2520high%2520memory%250Aconsumption%252C%2520reduced%2520efficiency%2520in%2520training%252C%2520challenges%2520in%2520model%250Aparallelization%252C%2520and%2520suboptimal%2520biocompatibility.%2520Local%2520learning%2520is%2520considered%250Aa%2520novel%2520interactive%2520training%2520method%2520that%2520holds%2520promise%2520as%2520an%2520alternative%2520to%250AE2E.%2520Nonetheless%252C%2520conventional%2520local%2520learning%2520methods%2520fall%2520short%2520in%2520achieving%250Ahigh%2520model%2520accuracy%2520due%2520to%2520inadequate%2520local%2520inter-module%2520interactions.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520new%2520model%2520known%2520as%2520the%2520Scaling%2520Supervised%2520Local%2520Learning%250Awith%2520Multilaminar%2520Leap%2520Augmented%2520Auxiliary%2520Network%2520%2528MLAAN%2529.%2520MLAAN%2520features%2520an%250Ainnovative%2520supervised%2520local%2520learning%2520approach%2520coupled%2520with%2520a%2520robust%250Areinforcement%2520module.%2520This%2520dual-component%2520design%2520enables%2520the%2520MLAAN%2520to%2520integrate%250Asmoothly%2520with%2520established%2520local%2520learning%2520techniques%252C%2520thereby%2520enhancing%2520the%250Aefficacy%2520of%2520the%2520foundational%2520methods.%2520The%2520method%2520simultaneously%2520acquires%2520the%250Alocal%2520and%2520global%2520features%2520of%2520the%2520model%2520separately%2520by%2520constructing%2520an%250Aindependent%2520auxiliary%2520network%2520and%2520a%2520cascade%2520auxiliary%2520network%2520on%2520the%2520one%2520hand%250Aand%2520incorporates%2520a%2520leap%2520augmented%2520module%252C%2520which%2520serves%2520to%2520counteract%2520the%250Areduced%2520learning%2520capacity%2520often%2520associated%2520with%2520weaker%2520supervision.%2520This%250Aarchitecture%2520not%2520only%2520augments%2520the%2520exchange%2520of%2520information%2520amongst%2520the%2520local%250Amodules%2520but%2520also%2520effectively%2520mitigates%2520the%2520model%2527s%2520tendency%2520toward%2520myopia.%2520The%250Aexperimental%2520evaluations%2520conducted%2520on%2520four%2520benchmark%2520datasets%252C%2520CIFAR-10%252C%250ASTL-10%252C%2520SVHN%252C%2520and%2520ImageNet%252C%2520demonstrate%2520that%2520the%2520integration%2520of%2520MLAAN%2520with%250Aexisting%2520supervised%2520local%2520learning%2520methods%2520significantly%2520enhances%2520the%2520original%250Amethodologies.%2520Of%2520particular%2520note%252C%2520MLAAN%2520enables%2520local%2520learning%2520methods%2520to%250Acomprehensively%2520outperform%2520end-to-end%2520training%2520approaches%2520in%2520terms%2520of%2520optimal%250Aperformance%2520while%2520saving%2520GPU%2520memory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLAAN%3A%20Scaling%20Supervised%20Local%20Learning%20with%20Multilaminar%20Leap%0A%20%20Augmented%20Auxiliary%20Network&entry.906535625=Yuming%20Zhang%20and%20Shouxin%20Zhang%20and%20Peizhe%20Wang%20and%20Feiyu%20Zhu%20and%20Dongzhi%20Guan%20and%20Jiabin%20Liu%20and%20Changpeng%20Cai&entry.1292438233=%20%20End-to-end%20%28E2E%29%20training%20approaches%20are%20commonly%20plagued%20by%20high%20memory%0Aconsumption%2C%20reduced%20efficiency%20in%20training%2C%20challenges%20in%20model%0Aparallelization%2C%20and%20suboptimal%20biocompatibility.%20Local%20learning%20is%20considered%0Aa%20novel%20interactive%20training%20method%20that%20holds%20promise%20as%20an%20alternative%20to%0AE2E.%20Nonetheless%2C%20conventional%20local%20learning%20methods%20fall%20short%20in%20achieving%0Ahigh%20model%20accuracy%20due%20to%20inadequate%20local%20inter-module%20interactions.%20In%20this%0Apaper%2C%20we%20introduce%20a%20new%20model%20known%20as%20the%20Scaling%20Supervised%20Local%20Learning%0Awith%20Multilaminar%20Leap%20Augmented%20Auxiliary%20Network%20%28MLAAN%29.%20MLAAN%20features%20an%0Ainnovative%20supervised%20local%20learning%20approach%20coupled%20with%20a%20robust%0Areinforcement%20module.%20This%20dual-component%20design%20enables%20the%20MLAAN%20to%20integrate%0Asmoothly%20with%20established%20local%20learning%20techniques%2C%20thereby%20enhancing%20the%0Aefficacy%20of%20the%20foundational%20methods.%20The%20method%20simultaneously%20acquires%20the%0Alocal%20and%20global%20features%20of%20the%20model%20separately%20by%20constructing%20an%0Aindependent%20auxiliary%20network%20and%20a%20cascade%20auxiliary%20network%20on%20the%20one%20hand%0Aand%20incorporates%20a%20leap%20augmented%20module%2C%20which%20serves%20to%20counteract%20the%0Areduced%20learning%20capacity%20often%20associated%20with%20weaker%20supervision.%20This%0Aarchitecture%20not%20only%20augments%20the%20exchange%20of%20information%20amongst%20the%20local%0Amodules%20but%20also%20effectively%20mitigates%20the%20model%27s%20tendency%20toward%20myopia.%20The%0Aexperimental%20evaluations%20conducted%20on%20four%20benchmark%20datasets%2C%20CIFAR-10%2C%0ASTL-10%2C%20SVHN%2C%20and%20ImageNet%2C%20demonstrate%20that%20the%20integration%20of%20MLAAN%20with%0Aexisting%20supervised%20local%20learning%20methods%20significantly%20enhances%20the%20original%0Amethodologies.%20Of%20particular%20note%2C%20MLAAN%20enables%20local%20learning%20methods%20to%0Acomprehensively%20outperform%20end-to-end%20training%20approaches%20in%20terms%20of%20optimal%0Aperformance%20while%20saving%20GPU%20memory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16633v1&entry.124074799=Read"},
{"title": "GIM: A Million-scale Benchmark for Generative Image Manipulation\n  Detection and Localization", "author": "Yirui Chen and Xudong Huang and Quan Zhang and Wei Li and Mingjian Zhu and Qiangyu Yan and Simiao Li and Hanting Chen and Hailin Hu and Jie Yang and Wei Liu and Jie Hu", "abstract": "  The extraordinary ability of generative models emerges as a new trend in\nimage editing and generating realistic images, posing a serious threat to the\ntrustworthiness of multimedia data and driving the research of image\nmanipulation detection and location(IMDL). However, the lack of a large-scale\ndata foundation makes IMDL task unattainable. In this paper, a local\nmanipulation pipeline is designed, incorporating the powerful SAM, ChatGPT and\ngenerative models. Upon this basis, We propose the GIM dataset, which has the\nfollowing advantages: 1) Large scale, including over one million pairs of\nAI-manipulated images and real images. 2) Rich Image Content, encompassing a\nbroad range of image classes 3) Diverse Generative Manipulation, manipulated\nimages with state-of-the-art generators and various manipulation tasks. The\naforementioned advantages allow for a more comprehensive evaluation of IMDL\nmethods, extending their applicability to diverse images. We introduce two\nbenchmark settings to evaluate the generalization capability and comprehensive\nperformance of baseline methods. In addition, we propose a novel IMDL\nframework, termed GIMFormer, which consists of a ShadowTracer,\nFrequency-Spatial Block (FSB), and a Multi-window Anomalous Modelling (MWAM)\nModule. Extensive experiments on the GIM demonstrate that GIMFormer surpasses\nprevious state-of-the-art works significantly on two different benchmarks.\n", "link": "http://arxiv.org/abs/2406.16531v1", "date": "2024-06-24", "relevancy": 2.7778, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5663}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5529}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GIM%3A%20A%20Million-scale%20Benchmark%20for%20Generative%20Image%20Manipulation%0A%20%20Detection%20and%20Localization&body=Title%3A%20GIM%3A%20A%20Million-scale%20Benchmark%20for%20Generative%20Image%20Manipulation%0A%20%20Detection%20and%20Localization%0AAuthor%3A%20Yirui%20Chen%20and%20Xudong%20Huang%20and%20Quan%20Zhang%20and%20Wei%20Li%20and%20Mingjian%20Zhu%20and%20Qiangyu%20Yan%20and%20Simiao%20Li%20and%20Hanting%20Chen%20and%20Hailin%20Hu%20and%20Jie%20Yang%20and%20Wei%20Liu%20and%20Jie%20Hu%0AAbstract%3A%20%20%20The%20extraordinary%20ability%20of%20generative%20models%20emerges%20as%20a%20new%20trend%20in%0Aimage%20editing%20and%20generating%20realistic%20images%2C%20posing%20a%20serious%20threat%20to%20the%0Atrustworthiness%20of%20multimedia%20data%20and%20driving%20the%20research%20of%20image%0Amanipulation%20detection%20and%20location%28IMDL%29.%20However%2C%20the%20lack%20of%20a%20large-scale%0Adata%20foundation%20makes%20IMDL%20task%20unattainable.%20In%20this%20paper%2C%20a%20local%0Amanipulation%20pipeline%20is%20designed%2C%20incorporating%20the%20powerful%20SAM%2C%20ChatGPT%20and%0Agenerative%20models.%20Upon%20this%20basis%2C%20We%20propose%20the%20GIM%20dataset%2C%20which%20has%20the%0Afollowing%20advantages%3A%201%29%20Large%20scale%2C%20including%20over%20one%20million%20pairs%20of%0AAI-manipulated%20images%20and%20real%20images.%202%29%20Rich%20Image%20Content%2C%20encompassing%20a%0Abroad%20range%20of%20image%20classes%203%29%20Diverse%20Generative%20Manipulation%2C%20manipulated%0Aimages%20with%20state-of-the-art%20generators%20and%20various%20manipulation%20tasks.%20The%0Aaforementioned%20advantages%20allow%20for%20a%20more%20comprehensive%20evaluation%20of%20IMDL%0Amethods%2C%20extending%20their%20applicability%20to%20diverse%20images.%20We%20introduce%20two%0Abenchmark%20settings%20to%20evaluate%20the%20generalization%20capability%20and%20comprehensive%0Aperformance%20of%20baseline%20methods.%20In%20addition%2C%20we%20propose%20a%20novel%20IMDL%0Aframework%2C%20termed%20GIMFormer%2C%20which%20consists%20of%20a%20ShadowTracer%2C%0AFrequency-Spatial%20Block%20%28FSB%29%2C%20and%20a%20Multi-window%20Anomalous%20Modelling%20%28MWAM%29%0AModule.%20Extensive%20experiments%20on%20the%20GIM%20demonstrate%20that%20GIMFormer%20surpasses%0Aprevious%20state-of-the-art%20works%20significantly%20on%20two%20different%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGIM%253A%2520A%2520Million-scale%2520Benchmark%2520for%2520Generative%2520Image%2520Manipulation%250A%2520%2520Detection%2520and%2520Localization%26entry.906535625%3DYirui%2520Chen%2520and%2520Xudong%2520Huang%2520and%2520Quan%2520Zhang%2520and%2520Wei%2520Li%2520and%2520Mingjian%2520Zhu%2520and%2520Qiangyu%2520Yan%2520and%2520Simiao%2520Li%2520and%2520Hanting%2520Chen%2520and%2520Hailin%2520Hu%2520and%2520Jie%2520Yang%2520and%2520Wei%2520Liu%2520and%2520Jie%2520Hu%26entry.1292438233%3D%2520%2520The%2520extraordinary%2520ability%2520of%2520generative%2520models%2520emerges%2520as%2520a%2520new%2520trend%2520in%250Aimage%2520editing%2520and%2520generating%2520realistic%2520images%252C%2520posing%2520a%2520serious%2520threat%2520to%2520the%250Atrustworthiness%2520of%2520multimedia%2520data%2520and%2520driving%2520the%2520research%2520of%2520image%250Amanipulation%2520detection%2520and%2520location%2528IMDL%2529.%2520However%252C%2520the%2520lack%2520of%2520a%2520large-scale%250Adata%2520foundation%2520makes%2520IMDL%2520task%2520unattainable.%2520In%2520this%2520paper%252C%2520a%2520local%250Amanipulation%2520pipeline%2520is%2520designed%252C%2520incorporating%2520the%2520powerful%2520SAM%252C%2520ChatGPT%2520and%250Agenerative%2520models.%2520Upon%2520this%2520basis%252C%2520We%2520propose%2520the%2520GIM%2520dataset%252C%2520which%2520has%2520the%250Afollowing%2520advantages%253A%25201%2529%2520Large%2520scale%252C%2520including%2520over%2520one%2520million%2520pairs%2520of%250AAI-manipulated%2520images%2520and%2520real%2520images.%25202%2529%2520Rich%2520Image%2520Content%252C%2520encompassing%2520a%250Abroad%2520range%2520of%2520image%2520classes%25203%2529%2520Diverse%2520Generative%2520Manipulation%252C%2520manipulated%250Aimages%2520with%2520state-of-the-art%2520generators%2520and%2520various%2520manipulation%2520tasks.%2520The%250Aaforementioned%2520advantages%2520allow%2520for%2520a%2520more%2520comprehensive%2520evaluation%2520of%2520IMDL%250Amethods%252C%2520extending%2520their%2520applicability%2520to%2520diverse%2520images.%2520We%2520introduce%2520two%250Abenchmark%2520settings%2520to%2520evaluate%2520the%2520generalization%2520capability%2520and%2520comprehensive%250Aperformance%2520of%2520baseline%2520methods.%2520In%2520addition%252C%2520we%2520propose%2520a%2520novel%2520IMDL%250Aframework%252C%2520termed%2520GIMFormer%252C%2520which%2520consists%2520of%2520a%2520ShadowTracer%252C%250AFrequency-Spatial%2520Block%2520%2528FSB%2529%252C%2520and%2520a%2520Multi-window%2520Anomalous%2520Modelling%2520%2528MWAM%2529%250AModule.%2520Extensive%2520experiments%2520on%2520the%2520GIM%2520demonstrate%2520that%2520GIMFormer%2520surpasses%250Aprevious%2520state-of-the-art%2520works%2520significantly%2520on%2520two%2520different%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GIM%3A%20A%20Million-scale%20Benchmark%20for%20Generative%20Image%20Manipulation%0A%20%20Detection%20and%20Localization&entry.906535625=Yirui%20Chen%20and%20Xudong%20Huang%20and%20Quan%20Zhang%20and%20Wei%20Li%20and%20Mingjian%20Zhu%20and%20Qiangyu%20Yan%20and%20Simiao%20Li%20and%20Hanting%20Chen%20and%20Hailin%20Hu%20and%20Jie%20Yang%20and%20Wei%20Liu%20and%20Jie%20Hu&entry.1292438233=%20%20The%20extraordinary%20ability%20of%20generative%20models%20emerges%20as%20a%20new%20trend%20in%0Aimage%20editing%20and%20generating%20realistic%20images%2C%20posing%20a%20serious%20threat%20to%20the%0Atrustworthiness%20of%20multimedia%20data%20and%20driving%20the%20research%20of%20image%0Amanipulation%20detection%20and%20location%28IMDL%29.%20However%2C%20the%20lack%20of%20a%20large-scale%0Adata%20foundation%20makes%20IMDL%20task%20unattainable.%20In%20this%20paper%2C%20a%20local%0Amanipulation%20pipeline%20is%20designed%2C%20incorporating%20the%20powerful%20SAM%2C%20ChatGPT%20and%0Agenerative%20models.%20Upon%20this%20basis%2C%20We%20propose%20the%20GIM%20dataset%2C%20which%20has%20the%0Afollowing%20advantages%3A%201%29%20Large%20scale%2C%20including%20over%20one%20million%20pairs%20of%0AAI-manipulated%20images%20and%20real%20images.%202%29%20Rich%20Image%20Content%2C%20encompassing%20a%0Abroad%20range%20of%20image%20classes%203%29%20Diverse%20Generative%20Manipulation%2C%20manipulated%0Aimages%20with%20state-of-the-art%20generators%20and%20various%20manipulation%20tasks.%20The%0Aaforementioned%20advantages%20allow%20for%20a%20more%20comprehensive%20evaluation%20of%20IMDL%0Amethods%2C%20extending%20their%20applicability%20to%20diverse%20images.%20We%20introduce%20two%0Abenchmark%20settings%20to%20evaluate%20the%20generalization%20capability%20and%20comprehensive%0Aperformance%20of%20baseline%20methods.%20In%20addition%2C%20we%20propose%20a%20novel%20IMDL%0Aframework%2C%20termed%20GIMFormer%2C%20which%20consists%20of%20a%20ShadowTracer%2C%0AFrequency-Spatial%20Block%20%28FSB%29%2C%20and%20a%20Multi-window%20Anomalous%20Modelling%20%28MWAM%29%0AModule.%20Extensive%20experiments%20on%20the%20GIM%20demonstrate%20that%20GIMFormer%20surpasses%0Aprevious%20state-of-the-art%20works%20significantly%20on%20two%20different%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16531v1&entry.124074799=Read"},
{"title": "State Representation Learning Using an Unbalanced Atlas", "author": "Li Meng and Morten Goodwin and Anis Yazidi and Paal Engelstad", "abstract": "  The manifold hypothesis posits that high-dimensional data often lies on a\nlower-dimensional manifold and that utilizing this manifold as the target space\nyields more efficient representations. While numerous traditional\nmanifold-based techniques exist for dimensionality reduction, their application\nin self-supervised learning has witnessed slow progress. The recent MSimCLR\nmethod combines manifold encoding with SimCLR but requires extremely low target\nencoding dimensions to outperform SimCLR, limiting its applicability. This\npaper introduces a novel learning paradigm using an unbalanced atlas (UA),\ncapable of surpassing state-of-the-art self-supervised learning approaches. We\ninvestigated and engineered the DeepInfomax with an unbalanced atlas (DIM-UA)\nmethod by adapting the Spatiotemporal DeepInfomax (ST-DIM) framework to align\nwith our proposed UA paradigm. The efficacy of DIM-UA is demonstrated through\ntraining and evaluation on the Atari Annotated RAM Interface (AtariARI)\nbenchmark, a modified version of the Atari 2600 framework that produces\nannotated image samples for representation learning. The UA paradigm improves\nexisting algorithms significantly as the number of target encoding dimensions\ngrows. For instance, the mean F1 score averaged over categories of DIM-UA is\n~75% compared to ~70% of ST-DIM when using 16384 hidden units.\n", "link": "http://arxiv.org/abs/2305.10267v3", "date": "2024-06-24", "relevancy": 2.7714, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5837}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5406}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20State%20Representation%20Learning%20Using%20an%20Unbalanced%20Atlas&body=Title%3A%20State%20Representation%20Learning%20Using%20an%20Unbalanced%20Atlas%0AAuthor%3A%20Li%20Meng%20and%20Morten%20Goodwin%20and%20Anis%20Yazidi%20and%20Paal%20Engelstad%0AAbstract%3A%20%20%20The%20manifold%20hypothesis%20posits%20that%20high-dimensional%20data%20often%20lies%20on%20a%0Alower-dimensional%20manifold%20and%20that%20utilizing%20this%20manifold%20as%20the%20target%20space%0Ayields%20more%20efficient%20representations.%20While%20numerous%20traditional%0Amanifold-based%20techniques%20exist%20for%20dimensionality%20reduction%2C%20their%20application%0Ain%20self-supervised%20learning%20has%20witnessed%20slow%20progress.%20The%20recent%20MSimCLR%0Amethod%20combines%20manifold%20encoding%20with%20SimCLR%20but%20requires%20extremely%20low%20target%0Aencoding%20dimensions%20to%20outperform%20SimCLR%2C%20limiting%20its%20applicability.%20This%0Apaper%20introduces%20a%20novel%20learning%20paradigm%20using%20an%20unbalanced%20atlas%20%28UA%29%2C%0Acapable%20of%20surpassing%20state-of-the-art%20self-supervised%20learning%20approaches.%20We%0Ainvestigated%20and%20engineered%20the%20DeepInfomax%20with%20an%20unbalanced%20atlas%20%28DIM-UA%29%0Amethod%20by%20adapting%20the%20Spatiotemporal%20DeepInfomax%20%28ST-DIM%29%20framework%20to%20align%0Awith%20our%20proposed%20UA%20paradigm.%20The%20efficacy%20of%20DIM-UA%20is%20demonstrated%20through%0Atraining%20and%20evaluation%20on%20the%20Atari%20Annotated%20RAM%20Interface%20%28AtariARI%29%0Abenchmark%2C%20a%20modified%20version%20of%20the%20Atari%202600%20framework%20that%20produces%0Aannotated%20image%20samples%20for%20representation%20learning.%20The%20UA%20paradigm%20improves%0Aexisting%20algorithms%20significantly%20as%20the%20number%20of%20target%20encoding%20dimensions%0Agrows.%20For%20instance%2C%20the%20mean%20F1%20score%20averaged%20over%20categories%20of%20DIM-UA%20is%0A~75%25%20compared%20to%20~70%25%20of%20ST-DIM%20when%20using%2016384%20hidden%20units.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.10267v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DState%2520Representation%2520Learning%2520Using%2520an%2520Unbalanced%2520Atlas%26entry.906535625%3DLi%2520Meng%2520and%2520Morten%2520Goodwin%2520and%2520Anis%2520Yazidi%2520and%2520Paal%2520Engelstad%26entry.1292438233%3D%2520%2520The%2520manifold%2520hypothesis%2520posits%2520that%2520high-dimensional%2520data%2520often%2520lies%2520on%2520a%250Alower-dimensional%2520manifold%2520and%2520that%2520utilizing%2520this%2520manifold%2520as%2520the%2520target%2520space%250Ayields%2520more%2520efficient%2520representations.%2520While%2520numerous%2520traditional%250Amanifold-based%2520techniques%2520exist%2520for%2520dimensionality%2520reduction%252C%2520their%2520application%250Ain%2520self-supervised%2520learning%2520has%2520witnessed%2520slow%2520progress.%2520The%2520recent%2520MSimCLR%250Amethod%2520combines%2520manifold%2520encoding%2520with%2520SimCLR%2520but%2520requires%2520extremely%2520low%2520target%250Aencoding%2520dimensions%2520to%2520outperform%2520SimCLR%252C%2520limiting%2520its%2520applicability.%2520This%250Apaper%2520introduces%2520a%2520novel%2520learning%2520paradigm%2520using%2520an%2520unbalanced%2520atlas%2520%2528UA%2529%252C%250Acapable%2520of%2520surpassing%2520state-of-the-art%2520self-supervised%2520learning%2520approaches.%2520We%250Ainvestigated%2520and%2520engineered%2520the%2520DeepInfomax%2520with%2520an%2520unbalanced%2520atlas%2520%2528DIM-UA%2529%250Amethod%2520by%2520adapting%2520the%2520Spatiotemporal%2520DeepInfomax%2520%2528ST-DIM%2529%2520framework%2520to%2520align%250Awith%2520our%2520proposed%2520UA%2520paradigm.%2520The%2520efficacy%2520of%2520DIM-UA%2520is%2520demonstrated%2520through%250Atraining%2520and%2520evaluation%2520on%2520the%2520Atari%2520Annotated%2520RAM%2520Interface%2520%2528AtariARI%2529%250Abenchmark%252C%2520a%2520modified%2520version%2520of%2520the%2520Atari%25202600%2520framework%2520that%2520produces%250Aannotated%2520image%2520samples%2520for%2520representation%2520learning.%2520The%2520UA%2520paradigm%2520improves%250Aexisting%2520algorithms%2520significantly%2520as%2520the%2520number%2520of%2520target%2520encoding%2520dimensions%250Agrows.%2520For%2520instance%252C%2520the%2520mean%2520F1%2520score%2520averaged%2520over%2520categories%2520of%2520DIM-UA%2520is%250A~75%2525%2520compared%2520to%2520~70%2525%2520of%2520ST-DIM%2520when%2520using%252016384%2520hidden%2520units.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.10267v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=State%20Representation%20Learning%20Using%20an%20Unbalanced%20Atlas&entry.906535625=Li%20Meng%20and%20Morten%20Goodwin%20and%20Anis%20Yazidi%20and%20Paal%20Engelstad&entry.1292438233=%20%20The%20manifold%20hypothesis%20posits%20that%20high-dimensional%20data%20often%20lies%20on%20a%0Alower-dimensional%20manifold%20and%20that%20utilizing%20this%20manifold%20as%20the%20target%20space%0Ayields%20more%20efficient%20representations.%20While%20numerous%20traditional%0Amanifold-based%20techniques%20exist%20for%20dimensionality%20reduction%2C%20their%20application%0Ain%20self-supervised%20learning%20has%20witnessed%20slow%20progress.%20The%20recent%20MSimCLR%0Amethod%20combines%20manifold%20encoding%20with%20SimCLR%20but%20requires%20extremely%20low%20target%0Aencoding%20dimensions%20to%20outperform%20SimCLR%2C%20limiting%20its%20applicability.%20This%0Apaper%20introduces%20a%20novel%20learning%20paradigm%20using%20an%20unbalanced%20atlas%20%28UA%29%2C%0Acapable%20of%20surpassing%20state-of-the-art%20self-supervised%20learning%20approaches.%20We%0Ainvestigated%20and%20engineered%20the%20DeepInfomax%20with%20an%20unbalanced%20atlas%20%28DIM-UA%29%0Amethod%20by%20adapting%20the%20Spatiotemporal%20DeepInfomax%20%28ST-DIM%29%20framework%20to%20align%0Awith%20our%20proposed%20UA%20paradigm.%20The%20efficacy%20of%20DIM-UA%20is%20demonstrated%20through%0Atraining%20and%20evaluation%20on%20the%20Atari%20Annotated%20RAM%20Interface%20%28AtariARI%29%0Abenchmark%2C%20a%20modified%20version%20of%20the%20Atari%202600%20framework%20that%20produces%0Aannotated%20image%20samples%20for%20representation%20learning.%20The%20UA%20paradigm%20improves%0Aexisting%20algorithms%20significantly%20as%20the%20number%20of%20target%20encoding%20dimensions%0Agrows.%20For%20instance%2C%20the%20mean%20F1%20score%20averaged%20over%20categories%20of%20DIM-UA%20is%0A~75%25%20compared%20to%20~70%25%20of%20ST-DIM%20when%20using%2016384%20hidden%20units.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.10267v3&entry.124074799=Read"},
{"title": "RGB-Sonar Tracking Benchmark and Spatial Cross-Attention Transformer\n  Tracker", "author": "Yunfeng Li and Bo Wang and Jiuran Sun and Xueyi Wu and Ye Li", "abstract": "  Vision camera and sonar are naturally complementary in the underwater\nenvironment. Combining the information from two modalities will promote better\nobservation of underwater targets. However, this problem has not received\nsufficient attention in previous research. Therefore, this paper introduces a\nnew challenging RGB-Sonar (RGB-S) tracking task and investigates how to achieve\nefficient tracking of an underwater target through the interaction of RGB and\nsonar modalities. Specifically, we first propose an RGBS50 benchmark dataset\ncontaining 50 sequences and more than 87000 high-quality annotated bounding\nboxes. Experimental results show that the RGBS50 benchmark poses a challenge to\ncurrently popular SOT trackers. Second, we propose an RGB-S tracker called\nSCANet, which includes a spatial cross-attention module (SCAM) consisting of a\nnovel spatial cross-attention layer and two independent global integration\nmodules. The spatial cross-attention is used to overcome the problem of spatial\nmisalignment of between RGB and sonar images. Third, we propose a SOT\ndata-based RGB-S simulation training method (SRST) to overcome the lack of\nRGB-S training datasets. It converts RGB images into sonar-like saliency images\nto construct pseudo-data pairs, enabling the model to learn the semantic\nstructure of RGB-S-like data. Comprehensive experiments show that the proposed\nspatial cross-attention effectively achieves the interaction between RGB and\nsonar modalities and SCANet achieves state-of-the-art performance on the\nproposed benchmark. The code is available at\nhttps://github.com/LiYunfengLYF/RGBS50.\n", "link": "http://arxiv.org/abs/2406.07189v2", "date": "2024-06-24", "relevancy": 2.758, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6079}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5309}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGB-Sonar%20Tracking%20Benchmark%20and%20Spatial%20Cross-Attention%20Transformer%0A%20%20Tracker&body=Title%3A%20RGB-Sonar%20Tracking%20Benchmark%20and%20Spatial%20Cross-Attention%20Transformer%0A%20%20Tracker%0AAuthor%3A%20Yunfeng%20Li%20and%20Bo%20Wang%20and%20Jiuran%20Sun%20and%20Xueyi%20Wu%20and%20Ye%20Li%0AAbstract%3A%20%20%20Vision%20camera%20and%20sonar%20are%20naturally%20complementary%20in%20the%20underwater%0Aenvironment.%20Combining%20the%20information%20from%20two%20modalities%20will%20promote%20better%0Aobservation%20of%20underwater%20targets.%20However%2C%20this%20problem%20has%20not%20received%0Asufficient%20attention%20in%20previous%20research.%20Therefore%2C%20this%20paper%20introduces%20a%0Anew%20challenging%20RGB-Sonar%20%28RGB-S%29%20tracking%20task%20and%20investigates%20how%20to%20achieve%0Aefficient%20tracking%20of%20an%20underwater%20target%20through%20the%20interaction%20of%20RGB%20and%0Asonar%20modalities.%20Specifically%2C%20we%20first%20propose%20an%20RGBS50%20benchmark%20dataset%0Acontaining%2050%20sequences%20and%20more%20than%2087000%20high-quality%20annotated%20bounding%0Aboxes.%20Experimental%20results%20show%20that%20the%20RGBS50%20benchmark%20poses%20a%20challenge%20to%0Acurrently%20popular%20SOT%20trackers.%20Second%2C%20we%20propose%20an%20RGB-S%20tracker%20called%0ASCANet%2C%20which%20includes%20a%20spatial%20cross-attention%20module%20%28SCAM%29%20consisting%20of%20a%0Anovel%20spatial%20cross-attention%20layer%20and%20two%20independent%20global%20integration%0Amodules.%20The%20spatial%20cross-attention%20is%20used%20to%20overcome%20the%20problem%20of%20spatial%0Amisalignment%20of%20between%20RGB%20and%20sonar%20images.%20Third%2C%20we%20propose%20a%20SOT%0Adata-based%20RGB-S%20simulation%20training%20method%20%28SRST%29%20to%20overcome%20the%20lack%20of%0ARGB-S%20training%20datasets.%20It%20converts%20RGB%20images%20into%20sonar-like%20saliency%20images%0Ato%20construct%20pseudo-data%20pairs%2C%20enabling%20the%20model%20to%20learn%20the%20semantic%0Astructure%20of%20RGB-S-like%20data.%20Comprehensive%20experiments%20show%20that%20the%20proposed%0Aspatial%20cross-attention%20effectively%20achieves%20the%20interaction%20between%20RGB%20and%0Asonar%20modalities%20and%20SCANet%20achieves%20state-of-the-art%20performance%20on%20the%0Aproposed%20benchmark.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LiYunfengLYF/RGBS50.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07189v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGB-Sonar%2520Tracking%2520Benchmark%2520and%2520Spatial%2520Cross-Attention%2520Transformer%250A%2520%2520Tracker%26entry.906535625%3DYunfeng%2520Li%2520and%2520Bo%2520Wang%2520and%2520Jiuran%2520Sun%2520and%2520Xueyi%2520Wu%2520and%2520Ye%2520Li%26entry.1292438233%3D%2520%2520Vision%2520camera%2520and%2520sonar%2520are%2520naturally%2520complementary%2520in%2520the%2520underwater%250Aenvironment.%2520Combining%2520the%2520information%2520from%2520two%2520modalities%2520will%2520promote%2520better%250Aobservation%2520of%2520underwater%2520targets.%2520However%252C%2520this%2520problem%2520has%2520not%2520received%250Asufficient%2520attention%2520in%2520previous%2520research.%2520Therefore%252C%2520this%2520paper%2520introduces%2520a%250Anew%2520challenging%2520RGB-Sonar%2520%2528RGB-S%2529%2520tracking%2520task%2520and%2520investigates%2520how%2520to%2520achieve%250Aefficient%2520tracking%2520of%2520an%2520underwater%2520target%2520through%2520the%2520interaction%2520of%2520RGB%2520and%250Asonar%2520modalities.%2520Specifically%252C%2520we%2520first%2520propose%2520an%2520RGBS50%2520benchmark%2520dataset%250Acontaining%252050%2520sequences%2520and%2520more%2520than%252087000%2520high-quality%2520annotated%2520bounding%250Aboxes.%2520Experimental%2520results%2520show%2520that%2520the%2520RGBS50%2520benchmark%2520poses%2520a%2520challenge%2520to%250Acurrently%2520popular%2520SOT%2520trackers.%2520Second%252C%2520we%2520propose%2520an%2520RGB-S%2520tracker%2520called%250ASCANet%252C%2520which%2520includes%2520a%2520spatial%2520cross-attention%2520module%2520%2528SCAM%2529%2520consisting%2520of%2520a%250Anovel%2520spatial%2520cross-attention%2520layer%2520and%2520two%2520independent%2520global%2520integration%250Amodules.%2520The%2520spatial%2520cross-attention%2520is%2520used%2520to%2520overcome%2520the%2520problem%2520of%2520spatial%250Amisalignment%2520of%2520between%2520RGB%2520and%2520sonar%2520images.%2520Third%252C%2520we%2520propose%2520a%2520SOT%250Adata-based%2520RGB-S%2520simulation%2520training%2520method%2520%2528SRST%2529%2520to%2520overcome%2520the%2520lack%2520of%250ARGB-S%2520training%2520datasets.%2520It%2520converts%2520RGB%2520images%2520into%2520sonar-like%2520saliency%2520images%250Ato%2520construct%2520pseudo-data%2520pairs%252C%2520enabling%2520the%2520model%2520to%2520learn%2520the%2520semantic%250Astructure%2520of%2520RGB-S-like%2520data.%2520Comprehensive%2520experiments%2520show%2520that%2520the%2520proposed%250Aspatial%2520cross-attention%2520effectively%2520achieves%2520the%2520interaction%2520between%2520RGB%2520and%250Asonar%2520modalities%2520and%2520SCANet%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%250Aproposed%2520benchmark.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/LiYunfengLYF/RGBS50.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07189v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGB-Sonar%20Tracking%20Benchmark%20and%20Spatial%20Cross-Attention%20Transformer%0A%20%20Tracker&entry.906535625=Yunfeng%20Li%20and%20Bo%20Wang%20and%20Jiuran%20Sun%20and%20Xueyi%20Wu%20and%20Ye%20Li&entry.1292438233=%20%20Vision%20camera%20and%20sonar%20are%20naturally%20complementary%20in%20the%20underwater%0Aenvironment.%20Combining%20the%20information%20from%20two%20modalities%20will%20promote%20better%0Aobservation%20of%20underwater%20targets.%20However%2C%20this%20problem%20has%20not%20received%0Asufficient%20attention%20in%20previous%20research.%20Therefore%2C%20this%20paper%20introduces%20a%0Anew%20challenging%20RGB-Sonar%20%28RGB-S%29%20tracking%20task%20and%20investigates%20how%20to%20achieve%0Aefficient%20tracking%20of%20an%20underwater%20target%20through%20the%20interaction%20of%20RGB%20and%0Asonar%20modalities.%20Specifically%2C%20we%20first%20propose%20an%20RGBS50%20benchmark%20dataset%0Acontaining%2050%20sequences%20and%20more%20than%2087000%20high-quality%20annotated%20bounding%0Aboxes.%20Experimental%20results%20show%20that%20the%20RGBS50%20benchmark%20poses%20a%20challenge%20to%0Acurrently%20popular%20SOT%20trackers.%20Second%2C%20we%20propose%20an%20RGB-S%20tracker%20called%0ASCANet%2C%20which%20includes%20a%20spatial%20cross-attention%20module%20%28SCAM%29%20consisting%20of%20a%0Anovel%20spatial%20cross-attention%20layer%20and%20two%20independent%20global%20integration%0Amodules.%20The%20spatial%20cross-attention%20is%20used%20to%20overcome%20the%20problem%20of%20spatial%0Amisalignment%20of%20between%20RGB%20and%20sonar%20images.%20Third%2C%20we%20propose%20a%20SOT%0Adata-based%20RGB-S%20simulation%20training%20method%20%28SRST%29%20to%20overcome%20the%20lack%20of%0ARGB-S%20training%20datasets.%20It%20converts%20RGB%20images%20into%20sonar-like%20saliency%20images%0Ato%20construct%20pseudo-data%20pairs%2C%20enabling%20the%20model%20to%20learn%20the%20semantic%0Astructure%20of%20RGB-S-like%20data.%20Comprehensive%20experiments%20show%20that%20the%20proposed%0Aspatial%20cross-attention%20effectively%20achieves%20the%20interaction%20between%20RGB%20and%0Asonar%20modalities%20and%20SCANet%20achieves%20state-of-the-art%20performance%20on%20the%0Aproposed%20benchmark.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LiYunfengLYF/RGBS50.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07189v2&entry.124074799=Read"},
{"title": "RGB-Sonar Tracking Benchmark and Spatial Cross-Attention Transformer\n  Tracker", "author": "Yunfeng Li and Bo Wang and Jiuran Sun and Xueyi Wu and Ye Li", "abstract": "  Vision camera and sonar are naturally complementary in the underwater\nenvironment. Combining the information from two modalities will promote better\nobservation of underwater targets. However, this problem has not received\nsufficient attention in previous research. Therefore, this paper introduces a\nnew challenging RGB-Sonar (RGB-S) tracking task and investigates how to achieve\nefficient tracking of an underwater target through the interaction of RGB and\nsonar modalities. Specifically, we first propose an RGBS50 benchmark dataset\ncontaining 50 sequences and more than 87000 high-quality annotated bounding\nboxes. Experimental results show that the RGBS50 benchmark poses a challenge to\ncurrently popular SOT trackers. Second, we propose an RGB-S tracker called\nSCANet, which includes a spatial cross-attention module (SCAM) consisting of a\nnovel spatial cross-attention layer and two independent global integration\nmodules. The spatial cross-attention is used to overcome the problem of spatial\nmisalignment of between RGB and sonar images. Third, we propose a SOT\ndata-based RGB-S simulation training method (SRST) to overcome the lack of\nRGB-S training datasets. It converts RGB images into sonar-like saliency images\nto construct pseudo-data pairs, enabling the model to learn the semantic\nstructure of RGB-S-like data. Comprehensive experiments show that the proposed\nspatial cross-attention effectively achieves the interaction between RGB and\nsonar modalities and SCANet achieves state-of-the-art performance on the\nproposed benchmark. The code is available at\nhttps://github.com/LiYunfengLYF/RGBS50.\n", "link": "http://arxiv.org/abs/2406.07189v2", "date": "2024-06-24", "relevancy": 2.758, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6079}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5309}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGB-Sonar%20Tracking%20Benchmark%20and%20Spatial%20Cross-Attention%20Transformer%0A%20%20Tracker&body=Title%3A%20RGB-Sonar%20Tracking%20Benchmark%20and%20Spatial%20Cross-Attention%20Transformer%0A%20%20Tracker%0AAuthor%3A%20Yunfeng%20Li%20and%20Bo%20Wang%20and%20Jiuran%20Sun%20and%20Xueyi%20Wu%20and%20Ye%20Li%0AAbstract%3A%20%20%20Vision%20camera%20and%20sonar%20are%20naturally%20complementary%20in%20the%20underwater%0Aenvironment.%20Combining%20the%20information%20from%20two%20modalities%20will%20promote%20better%0Aobservation%20of%20underwater%20targets.%20However%2C%20this%20problem%20has%20not%20received%0Asufficient%20attention%20in%20previous%20research.%20Therefore%2C%20this%20paper%20introduces%20a%0Anew%20challenging%20RGB-Sonar%20%28RGB-S%29%20tracking%20task%20and%20investigates%20how%20to%20achieve%0Aefficient%20tracking%20of%20an%20underwater%20target%20through%20the%20interaction%20of%20RGB%20and%0Asonar%20modalities.%20Specifically%2C%20we%20first%20propose%20an%20RGBS50%20benchmark%20dataset%0Acontaining%2050%20sequences%20and%20more%20than%2087000%20high-quality%20annotated%20bounding%0Aboxes.%20Experimental%20results%20show%20that%20the%20RGBS50%20benchmark%20poses%20a%20challenge%20to%0Acurrently%20popular%20SOT%20trackers.%20Second%2C%20we%20propose%20an%20RGB-S%20tracker%20called%0ASCANet%2C%20which%20includes%20a%20spatial%20cross-attention%20module%20%28SCAM%29%20consisting%20of%20a%0Anovel%20spatial%20cross-attention%20layer%20and%20two%20independent%20global%20integration%0Amodules.%20The%20spatial%20cross-attention%20is%20used%20to%20overcome%20the%20problem%20of%20spatial%0Amisalignment%20of%20between%20RGB%20and%20sonar%20images.%20Third%2C%20we%20propose%20a%20SOT%0Adata-based%20RGB-S%20simulation%20training%20method%20%28SRST%29%20to%20overcome%20the%20lack%20of%0ARGB-S%20training%20datasets.%20It%20converts%20RGB%20images%20into%20sonar-like%20saliency%20images%0Ato%20construct%20pseudo-data%20pairs%2C%20enabling%20the%20model%20to%20learn%20the%20semantic%0Astructure%20of%20RGB-S-like%20data.%20Comprehensive%20experiments%20show%20that%20the%20proposed%0Aspatial%20cross-attention%20effectively%20achieves%20the%20interaction%20between%20RGB%20and%0Asonar%20modalities%20and%20SCANet%20achieves%20state-of-the-art%20performance%20on%20the%0Aproposed%20benchmark.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LiYunfengLYF/RGBS50.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07189v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGB-Sonar%2520Tracking%2520Benchmark%2520and%2520Spatial%2520Cross-Attention%2520Transformer%250A%2520%2520Tracker%26entry.906535625%3DYunfeng%2520Li%2520and%2520Bo%2520Wang%2520and%2520Jiuran%2520Sun%2520and%2520Xueyi%2520Wu%2520and%2520Ye%2520Li%26entry.1292438233%3D%2520%2520Vision%2520camera%2520and%2520sonar%2520are%2520naturally%2520complementary%2520in%2520the%2520underwater%250Aenvironment.%2520Combining%2520the%2520information%2520from%2520two%2520modalities%2520will%2520promote%2520better%250Aobservation%2520of%2520underwater%2520targets.%2520However%252C%2520this%2520problem%2520has%2520not%2520received%250Asufficient%2520attention%2520in%2520previous%2520research.%2520Therefore%252C%2520this%2520paper%2520introduces%2520a%250Anew%2520challenging%2520RGB-Sonar%2520%2528RGB-S%2529%2520tracking%2520task%2520and%2520investigates%2520how%2520to%2520achieve%250Aefficient%2520tracking%2520of%2520an%2520underwater%2520target%2520through%2520the%2520interaction%2520of%2520RGB%2520and%250Asonar%2520modalities.%2520Specifically%252C%2520we%2520first%2520propose%2520an%2520RGBS50%2520benchmark%2520dataset%250Acontaining%252050%2520sequences%2520and%2520more%2520than%252087000%2520high-quality%2520annotated%2520bounding%250Aboxes.%2520Experimental%2520results%2520show%2520that%2520the%2520RGBS50%2520benchmark%2520poses%2520a%2520challenge%2520to%250Acurrently%2520popular%2520SOT%2520trackers.%2520Second%252C%2520we%2520propose%2520an%2520RGB-S%2520tracker%2520called%250ASCANet%252C%2520which%2520includes%2520a%2520spatial%2520cross-attention%2520module%2520%2528SCAM%2529%2520consisting%2520of%2520a%250Anovel%2520spatial%2520cross-attention%2520layer%2520and%2520two%2520independent%2520global%2520integration%250Amodules.%2520The%2520spatial%2520cross-attention%2520is%2520used%2520to%2520overcome%2520the%2520problem%2520of%2520spatial%250Amisalignment%2520of%2520between%2520RGB%2520and%2520sonar%2520images.%2520Third%252C%2520we%2520propose%2520a%2520SOT%250Adata-based%2520RGB-S%2520simulation%2520training%2520method%2520%2528SRST%2529%2520to%2520overcome%2520the%2520lack%2520of%250ARGB-S%2520training%2520datasets.%2520It%2520converts%2520RGB%2520images%2520into%2520sonar-like%2520saliency%2520images%250Ato%2520construct%2520pseudo-data%2520pairs%252C%2520enabling%2520the%2520model%2520to%2520learn%2520the%2520semantic%250Astructure%2520of%2520RGB-S-like%2520data.%2520Comprehensive%2520experiments%2520show%2520that%2520the%2520proposed%250Aspatial%2520cross-attention%2520effectively%2520achieves%2520the%2520interaction%2520between%2520RGB%2520and%250Asonar%2520modalities%2520and%2520SCANet%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%250Aproposed%2520benchmark.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/LiYunfengLYF/RGBS50.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07189v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGB-Sonar%20Tracking%20Benchmark%20and%20Spatial%20Cross-Attention%20Transformer%0A%20%20Tracker&entry.906535625=Yunfeng%20Li%20and%20Bo%20Wang%20and%20Jiuran%20Sun%20and%20Xueyi%20Wu%20and%20Ye%20Li&entry.1292438233=%20%20Vision%20camera%20and%20sonar%20are%20naturally%20complementary%20in%20the%20underwater%0Aenvironment.%20Combining%20the%20information%20from%20two%20modalities%20will%20promote%20better%0Aobservation%20of%20underwater%20targets.%20However%2C%20this%20problem%20has%20not%20received%0Asufficient%20attention%20in%20previous%20research.%20Therefore%2C%20this%20paper%20introduces%20a%0Anew%20challenging%20RGB-Sonar%20%28RGB-S%29%20tracking%20task%20and%20investigates%20how%20to%20achieve%0Aefficient%20tracking%20of%20an%20underwater%20target%20through%20the%20interaction%20of%20RGB%20and%0Asonar%20modalities.%20Specifically%2C%20we%20first%20propose%20an%20RGBS50%20benchmark%20dataset%0Acontaining%2050%20sequences%20and%20more%20than%2087000%20high-quality%20annotated%20bounding%0Aboxes.%20Experimental%20results%20show%20that%20the%20RGBS50%20benchmark%20poses%20a%20challenge%20to%0Acurrently%20popular%20SOT%20trackers.%20Second%2C%20we%20propose%20an%20RGB-S%20tracker%20called%0ASCANet%2C%20which%20includes%20a%20spatial%20cross-attention%20module%20%28SCAM%29%20consisting%20of%20a%0Anovel%20spatial%20cross-attention%20layer%20and%20two%20independent%20global%20integration%0Amodules.%20The%20spatial%20cross-attention%20is%20used%20to%20overcome%20the%20problem%20of%20spatial%0Amisalignment%20of%20between%20RGB%20and%20sonar%20images.%20Third%2C%20we%20propose%20a%20SOT%0Adata-based%20RGB-S%20simulation%20training%20method%20%28SRST%29%20to%20overcome%20the%20lack%20of%0ARGB-S%20training%20datasets.%20It%20converts%20RGB%20images%20into%20sonar-like%20saliency%20images%0Ato%20construct%20pseudo-data%20pairs%2C%20enabling%20the%20model%20to%20learn%20the%20semantic%0Astructure%20of%20RGB-S-like%20data.%20Comprehensive%20experiments%20show%20that%20the%20proposed%0Aspatial%20cross-attention%20effectively%20achieves%20the%20interaction%20between%20RGB%20and%0Asonar%20modalities%20and%20SCANet%20achieves%20state-of-the-art%20performance%20on%20the%0Aproposed%20benchmark.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/LiYunfengLYF/RGBS50.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07189v2&entry.124074799=Read"},
{"title": "MLPHand: Real Time Multi-View 3D Hand Mesh Reconstruction via MLP\n  Modeling", "author": "Jian Yang and Jiakun Li and Guoming Li and Zhen Shen and Huai-Yu Wu and Zhaoxin Fan and Heng Huang", "abstract": "  Multi-view hand mesh reconstruction is a critical task for applications in\nvirtual reality and human-computer interaction, but it remains a formidable\nchallenge. Although existing multi-view hand reconstruction methods achieve\nremarkable accuracy, they typically come with an intensive computational burden\nthat hinders real-time inference. To this end, we propose MLPHand, a novel\nmethod designed for real-time multi-view single hand reconstruction. MLP Hand\nconsists of two primary modules: (1) a lightweight MLP-based Skeleton2Mesh\nmodel that efficiently recovers hand meshes from hand skeletons, and (2) a\nmulti-view geometry feature fusion prediction module that enhances the\nSkeleton2Mesh model with detailed geometric information from multiple views.\nExperiments on three widely used datasets demonstrate that MLPHand can reduce\ncomputational complexity by 90% while achieving comparable reconstruction\naccuracy to existing state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2406.16137v1", "date": "2024-06-23", "relevancy": 2.7374, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5794}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5315}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLPHand%3A%20Real%20Time%20Multi-View%203D%20Hand%20Mesh%20Reconstruction%20via%20MLP%0A%20%20Modeling&body=Title%3A%20MLPHand%3A%20Real%20Time%20Multi-View%203D%20Hand%20Mesh%20Reconstruction%20via%20MLP%0A%20%20Modeling%0AAuthor%3A%20Jian%20Yang%20and%20Jiakun%20Li%20and%20Guoming%20Li%20and%20Zhen%20Shen%20and%20Huai-Yu%20Wu%20and%20Zhaoxin%20Fan%20and%20Heng%20Huang%0AAbstract%3A%20%20%20Multi-view%20hand%20mesh%20reconstruction%20is%20a%20critical%20task%20for%20applications%20in%0Avirtual%20reality%20and%20human-computer%20interaction%2C%20but%20it%20remains%20a%20formidable%0Achallenge.%20Although%20existing%20multi-view%20hand%20reconstruction%20methods%20achieve%0Aremarkable%20accuracy%2C%20they%20typically%20come%20with%20an%20intensive%20computational%20burden%0Athat%20hinders%20real-time%20inference.%20To%20this%20end%2C%20we%20propose%20MLPHand%2C%20a%20novel%0Amethod%20designed%20for%20real-time%20multi-view%20single%20hand%20reconstruction.%20MLP%20Hand%0Aconsists%20of%20two%20primary%20modules%3A%20%281%29%20a%20lightweight%20MLP-based%20Skeleton2Mesh%0Amodel%20that%20efficiently%20recovers%20hand%20meshes%20from%20hand%20skeletons%2C%20and%20%282%29%20a%0Amulti-view%20geometry%20feature%20fusion%20prediction%20module%20that%20enhances%20the%0ASkeleton2Mesh%20model%20with%20detailed%20geometric%20information%20from%20multiple%20views.%0AExperiments%20on%20three%20widely%20used%20datasets%20demonstrate%20that%20MLPHand%20can%20reduce%0Acomputational%20complexity%20by%2090%25%20while%20achieving%20comparable%20reconstruction%0Aaccuracy%20to%20existing%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLPHand%253A%2520Real%2520Time%2520Multi-View%25203D%2520Hand%2520Mesh%2520Reconstruction%2520via%2520MLP%250A%2520%2520Modeling%26entry.906535625%3DJian%2520Yang%2520and%2520Jiakun%2520Li%2520and%2520Guoming%2520Li%2520and%2520Zhen%2520Shen%2520and%2520Huai-Yu%2520Wu%2520and%2520Zhaoxin%2520Fan%2520and%2520Heng%2520Huang%26entry.1292438233%3D%2520%2520Multi-view%2520hand%2520mesh%2520reconstruction%2520is%2520a%2520critical%2520task%2520for%2520applications%2520in%250Avirtual%2520reality%2520and%2520human-computer%2520interaction%252C%2520but%2520it%2520remains%2520a%2520formidable%250Achallenge.%2520Although%2520existing%2520multi-view%2520hand%2520reconstruction%2520methods%2520achieve%250Aremarkable%2520accuracy%252C%2520they%2520typically%2520come%2520with%2520an%2520intensive%2520computational%2520burden%250Athat%2520hinders%2520real-time%2520inference.%2520To%2520this%2520end%252C%2520we%2520propose%2520MLPHand%252C%2520a%2520novel%250Amethod%2520designed%2520for%2520real-time%2520multi-view%2520single%2520hand%2520reconstruction.%2520MLP%2520Hand%250Aconsists%2520of%2520two%2520primary%2520modules%253A%2520%25281%2529%2520a%2520lightweight%2520MLP-based%2520Skeleton2Mesh%250Amodel%2520that%2520efficiently%2520recovers%2520hand%2520meshes%2520from%2520hand%2520skeletons%252C%2520and%2520%25282%2529%2520a%250Amulti-view%2520geometry%2520feature%2520fusion%2520prediction%2520module%2520that%2520enhances%2520the%250ASkeleton2Mesh%2520model%2520with%2520detailed%2520geometric%2520information%2520from%2520multiple%2520views.%250AExperiments%2520on%2520three%2520widely%2520used%2520datasets%2520demonstrate%2520that%2520MLPHand%2520can%2520reduce%250Acomputational%2520complexity%2520by%252090%2525%2520while%2520achieving%2520comparable%2520reconstruction%250Aaccuracy%2520to%2520existing%2520state-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLPHand%3A%20Real%20Time%20Multi-View%203D%20Hand%20Mesh%20Reconstruction%20via%20MLP%0A%20%20Modeling&entry.906535625=Jian%20Yang%20and%20Jiakun%20Li%20and%20Guoming%20Li%20and%20Zhen%20Shen%20and%20Huai-Yu%20Wu%20and%20Zhaoxin%20Fan%20and%20Heng%20Huang&entry.1292438233=%20%20Multi-view%20hand%20mesh%20reconstruction%20is%20a%20critical%20task%20for%20applications%20in%0Avirtual%20reality%20and%20human-computer%20interaction%2C%20but%20it%20remains%20a%20formidable%0Achallenge.%20Although%20existing%20multi-view%20hand%20reconstruction%20methods%20achieve%0Aremarkable%20accuracy%2C%20they%20typically%20come%20with%20an%20intensive%20computational%20burden%0Athat%20hinders%20real-time%20inference.%20To%20this%20end%2C%20we%20propose%20MLPHand%2C%20a%20novel%0Amethod%20designed%20for%20real-time%20multi-view%20single%20hand%20reconstruction.%20MLP%20Hand%0Aconsists%20of%20two%20primary%20modules%3A%20%281%29%20a%20lightweight%20MLP-based%20Skeleton2Mesh%0Amodel%20that%20efficiently%20recovers%20hand%20meshes%20from%20hand%20skeletons%2C%20and%20%282%29%20a%0Amulti-view%20geometry%20feature%20fusion%20prediction%20module%20that%20enhances%20the%0ASkeleton2Mesh%20model%20with%20detailed%20geometric%20information%20from%20multiple%20views.%0AExperiments%20on%20three%20widely%20used%20datasets%20demonstrate%20that%20MLPHand%20can%20reduce%0Acomputational%20complexity%20by%2090%25%20while%20achieving%20comparable%20reconstruction%0Aaccuracy%20to%20existing%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16137v1&entry.124074799=Read"},
{"title": "AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a\n  Single High-Resolution Image", "author": "Hamed Amini Amirkolaee and Miaojing Shi and Lianghua He and Mark Mulligan", "abstract": "  The process of estimating and counting tree density using only a single\naerial or satellite image is a difficult task in the fields of photogrammetry\nand remote sensing. However, it plays a crucial role in the management of\nforests. The huge variety of trees in varied topography severely hinders tree\ncounting models to perform well. The purpose of this paper is to propose a\nframework that is learnt from the source domain with sufficient labeled trees\nand is adapted to the target domain with only a limited number of labeled\ntrees. Our method, termed as AdaTreeFormer, contains one shared encoder with a\nhierarchical feature extraction scheme to extract robust features from the\nsource and target domains. It also consists of three subnets: two for\nextracting self-domain attention maps from source and target domains\nrespectively and one for extracting cross-domain attention maps. For the\nlatter, an attention-to-adapt mechanism is introduced to distill relevant\ninformation from different domains while generating tree density maps; a\nhierarchical cross-domain feature alignment scheme is proposed that\nprogressively aligns the features from the source and target domains. We also\nadopt adversarial learning into the framework to further reduce the gap between\nsource and target domains. Our AdaTreeFormer is evaluated on six designed\ndomain adaptation tasks using three tree counting datasets, \\ie Jiangsu,\nYosemite, and London. Experimental results show that AdaTreeFormer\nsignificantly surpasses the state of the art, \\eg in the cross domain from the\nYosemite to Jiangsu dataset, it achieves a reduction of 15.9 points in terms of\nthe absolute counting errors and an increase of 10.8\\% in the accuracy of the\ndetected trees' locations. The codes and datasets are available at\n\\emph{\\color{magenta}{https://github.com/HAAClassic/AdaTreeFormer}}.\n", "link": "http://arxiv.org/abs/2402.02956v2", "date": "2024-06-24", "relevancy": 2.7335, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5622}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5472}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5307}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaTreeFormer%3A%20Few%20Shot%20Domain%20Adaptation%20for%20Tree%20Counting%20from%20a%0A%20%20Single%20High-Resolution%20Image&body=Title%3A%20AdaTreeFormer%3A%20Few%20Shot%20Domain%20Adaptation%20for%20Tree%20Counting%20from%20a%0A%20%20Single%20High-Resolution%20Image%0AAuthor%3A%20Hamed%20Amini%20Amirkolaee%20and%20Miaojing%20Shi%20and%20Lianghua%20He%20and%20Mark%20Mulligan%0AAbstract%3A%20%20%20The%20process%20of%20estimating%20and%20counting%20tree%20density%20using%20only%20a%20single%0Aaerial%20or%20satellite%20image%20is%20a%20difficult%20task%20in%20the%20fields%20of%20photogrammetry%0Aand%20remote%20sensing.%20However%2C%20it%20plays%20a%20crucial%20role%20in%20the%20management%20of%0Aforests.%20The%20huge%20variety%20of%20trees%20in%20varied%20topography%20severely%20hinders%20tree%0Acounting%20models%20to%20perform%20well.%20The%20purpose%20of%20this%20paper%20is%20to%20propose%20a%0Aframework%20that%20is%20learnt%20from%20the%20source%20domain%20with%20sufficient%20labeled%20trees%0Aand%20is%20adapted%20to%20the%20target%20domain%20with%20only%20a%20limited%20number%20of%20labeled%0Atrees.%20Our%20method%2C%20termed%20as%20AdaTreeFormer%2C%20contains%20one%20shared%20encoder%20with%20a%0Ahierarchical%20feature%20extraction%20scheme%20to%20extract%20robust%20features%20from%20the%0Asource%20and%20target%20domains.%20It%20also%20consists%20of%20three%20subnets%3A%20two%20for%0Aextracting%20self-domain%20attention%20maps%20from%20source%20and%20target%20domains%0Arespectively%20and%20one%20for%20extracting%20cross-domain%20attention%20maps.%20For%20the%0Alatter%2C%20an%20attention-to-adapt%20mechanism%20is%20introduced%20to%20distill%20relevant%0Ainformation%20from%20different%20domains%20while%20generating%20tree%20density%20maps%3B%20a%0Ahierarchical%20cross-domain%20feature%20alignment%20scheme%20is%20proposed%20that%0Aprogressively%20aligns%20the%20features%20from%20the%20source%20and%20target%20domains.%20We%20also%0Aadopt%20adversarial%20learning%20into%20the%20framework%20to%20further%20reduce%20the%20gap%20between%0Asource%20and%20target%20domains.%20Our%20AdaTreeFormer%20is%20evaluated%20on%20six%20designed%0Adomain%20adaptation%20tasks%20using%20three%20tree%20counting%20datasets%2C%20%5Cie%20Jiangsu%2C%0AYosemite%2C%20and%20London.%20Experimental%20results%20show%20that%20AdaTreeFormer%0Asignificantly%20surpasses%20the%20state%20of%20the%20art%2C%20%5Ceg%20in%20the%20cross%20domain%20from%20the%0AYosemite%20to%20Jiangsu%20dataset%2C%20it%20achieves%20a%20reduction%20of%2015.9%20points%20in%20terms%20of%0Athe%20absolute%20counting%20errors%20and%20an%20increase%20of%2010.8%5C%25%20in%20the%20accuracy%20of%20the%0Adetected%20trees%27%20locations.%20The%20codes%20and%20datasets%20are%20available%20at%0A%5Cemph%7B%5Ccolor%7Bmagenta%7D%7Bhttps%3A//github.com/HAAClassic/AdaTreeFormer%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02956v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaTreeFormer%253A%2520Few%2520Shot%2520Domain%2520Adaptation%2520for%2520Tree%2520Counting%2520from%2520a%250A%2520%2520Single%2520High-Resolution%2520Image%26entry.906535625%3DHamed%2520Amini%2520Amirkolaee%2520and%2520Miaojing%2520Shi%2520and%2520Lianghua%2520He%2520and%2520Mark%2520Mulligan%26entry.1292438233%3D%2520%2520The%2520process%2520of%2520estimating%2520and%2520counting%2520tree%2520density%2520using%2520only%2520a%2520single%250Aaerial%2520or%2520satellite%2520image%2520is%2520a%2520difficult%2520task%2520in%2520the%2520fields%2520of%2520photogrammetry%250Aand%2520remote%2520sensing.%2520However%252C%2520it%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520management%2520of%250Aforests.%2520The%2520huge%2520variety%2520of%2520trees%2520in%2520varied%2520topography%2520severely%2520hinders%2520tree%250Acounting%2520models%2520to%2520perform%2520well.%2520The%2520purpose%2520of%2520this%2520paper%2520is%2520to%2520propose%2520a%250Aframework%2520that%2520is%2520learnt%2520from%2520the%2520source%2520domain%2520with%2520sufficient%2520labeled%2520trees%250Aand%2520is%2520adapted%2520to%2520the%2520target%2520domain%2520with%2520only%2520a%2520limited%2520number%2520of%2520labeled%250Atrees.%2520Our%2520method%252C%2520termed%2520as%2520AdaTreeFormer%252C%2520contains%2520one%2520shared%2520encoder%2520with%2520a%250Ahierarchical%2520feature%2520extraction%2520scheme%2520to%2520extract%2520robust%2520features%2520from%2520the%250Asource%2520and%2520target%2520domains.%2520It%2520also%2520consists%2520of%2520three%2520subnets%253A%2520two%2520for%250Aextracting%2520self-domain%2520attention%2520maps%2520from%2520source%2520and%2520target%2520domains%250Arespectively%2520and%2520one%2520for%2520extracting%2520cross-domain%2520attention%2520maps.%2520For%2520the%250Alatter%252C%2520an%2520attention-to-adapt%2520mechanism%2520is%2520introduced%2520to%2520distill%2520relevant%250Ainformation%2520from%2520different%2520domains%2520while%2520generating%2520tree%2520density%2520maps%253B%2520a%250Ahierarchical%2520cross-domain%2520feature%2520alignment%2520scheme%2520is%2520proposed%2520that%250Aprogressively%2520aligns%2520the%2520features%2520from%2520the%2520source%2520and%2520target%2520domains.%2520We%2520also%250Aadopt%2520adversarial%2520learning%2520into%2520the%2520framework%2520to%2520further%2520reduce%2520the%2520gap%2520between%250Asource%2520and%2520target%2520domains.%2520Our%2520AdaTreeFormer%2520is%2520evaluated%2520on%2520six%2520designed%250Adomain%2520adaptation%2520tasks%2520using%2520three%2520tree%2520counting%2520datasets%252C%2520%255Cie%2520Jiangsu%252C%250AYosemite%252C%2520and%2520London.%2520Experimental%2520results%2520show%2520that%2520AdaTreeFormer%250Asignificantly%2520surpasses%2520the%2520state%2520of%2520the%2520art%252C%2520%255Ceg%2520in%2520the%2520cross%2520domain%2520from%2520the%250AYosemite%2520to%2520Jiangsu%2520dataset%252C%2520it%2520achieves%2520a%2520reduction%2520of%252015.9%2520points%2520in%2520terms%2520of%250Athe%2520absolute%2520counting%2520errors%2520and%2520an%2520increase%2520of%252010.8%255C%2525%2520in%2520the%2520accuracy%2520of%2520the%250Adetected%2520trees%2527%2520locations.%2520The%2520codes%2520and%2520datasets%2520are%2520available%2520at%250A%255Cemph%257B%255Ccolor%257Bmagenta%257D%257Bhttps%253A//github.com/HAAClassic/AdaTreeFormer%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02956v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaTreeFormer%3A%20Few%20Shot%20Domain%20Adaptation%20for%20Tree%20Counting%20from%20a%0A%20%20Single%20High-Resolution%20Image&entry.906535625=Hamed%20Amini%20Amirkolaee%20and%20Miaojing%20Shi%20and%20Lianghua%20He%20and%20Mark%20Mulligan&entry.1292438233=%20%20The%20process%20of%20estimating%20and%20counting%20tree%20density%20using%20only%20a%20single%0Aaerial%20or%20satellite%20image%20is%20a%20difficult%20task%20in%20the%20fields%20of%20photogrammetry%0Aand%20remote%20sensing.%20However%2C%20it%20plays%20a%20crucial%20role%20in%20the%20management%20of%0Aforests.%20The%20huge%20variety%20of%20trees%20in%20varied%20topography%20severely%20hinders%20tree%0Acounting%20models%20to%20perform%20well.%20The%20purpose%20of%20this%20paper%20is%20to%20propose%20a%0Aframework%20that%20is%20learnt%20from%20the%20source%20domain%20with%20sufficient%20labeled%20trees%0Aand%20is%20adapted%20to%20the%20target%20domain%20with%20only%20a%20limited%20number%20of%20labeled%0Atrees.%20Our%20method%2C%20termed%20as%20AdaTreeFormer%2C%20contains%20one%20shared%20encoder%20with%20a%0Ahierarchical%20feature%20extraction%20scheme%20to%20extract%20robust%20features%20from%20the%0Asource%20and%20target%20domains.%20It%20also%20consists%20of%20three%20subnets%3A%20two%20for%0Aextracting%20self-domain%20attention%20maps%20from%20source%20and%20target%20domains%0Arespectively%20and%20one%20for%20extracting%20cross-domain%20attention%20maps.%20For%20the%0Alatter%2C%20an%20attention-to-adapt%20mechanism%20is%20introduced%20to%20distill%20relevant%0Ainformation%20from%20different%20domains%20while%20generating%20tree%20density%20maps%3B%20a%0Ahierarchical%20cross-domain%20feature%20alignment%20scheme%20is%20proposed%20that%0Aprogressively%20aligns%20the%20features%20from%20the%20source%20and%20target%20domains.%20We%20also%0Aadopt%20adversarial%20learning%20into%20the%20framework%20to%20further%20reduce%20the%20gap%20between%0Asource%20and%20target%20domains.%20Our%20AdaTreeFormer%20is%20evaluated%20on%20six%20designed%0Adomain%20adaptation%20tasks%20using%20three%20tree%20counting%20datasets%2C%20%5Cie%20Jiangsu%2C%0AYosemite%2C%20and%20London.%20Experimental%20results%20show%20that%20AdaTreeFormer%0Asignificantly%20surpasses%20the%20state%20of%20the%20art%2C%20%5Ceg%20in%20the%20cross%20domain%20from%20the%0AYosemite%20to%20Jiangsu%20dataset%2C%20it%20achieves%20a%20reduction%20of%2015.9%20points%20in%20terms%20of%0Athe%20absolute%20counting%20errors%20and%20an%20increase%20of%2010.8%5C%25%20in%20the%20accuracy%20of%20the%0Adetected%20trees%27%20locations.%20The%20codes%20and%20datasets%20are%20available%20at%0A%5Cemph%7B%5Ccolor%7Bmagenta%7D%7Bhttps%3A//github.com/HAAClassic/AdaTreeFormer%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02956v2&entry.124074799=Read"},
{"title": "ClotheDreamer: Text-Guided Garment Generation with 3D Gaussians", "author": "Yufei Liu and Junshu Tang and Chu Zheng and Shijie Zhang and Jinkun Hao and Junwei Zhu and Dongjin Huang", "abstract": "  High-fidelity 3D garment synthesis from text is desirable yet challenging for\ndigital avatar creation. Recent diffusion-based approaches via Score\nDistillation Sampling (SDS) have enabled new possibilities but either\nintricately couple with human body or struggle to reuse. We introduce\nClotheDreamer, a 3D Gaussian-based method for generating wearable,\nproduction-ready 3D garment assets from text prompts. We propose a novel\nrepresentation Disentangled Clothe Gaussian Splatting (DCGS) to enable separate\noptimization. DCGS represents clothed avatar as one Gaussian model but freezes\nbody Gaussian splats. To enhance quality and completeness, we incorporate\nbidirectional SDS to supervise clothed avatar and garment RGBD renderings\nrespectively with pose conditions and propose a new pruning strategy for loose\nclothing. Our approach can also support custom clothing templates as input.\nBenefiting from our design, the synthetic 3D garment can be easily applied to\nvirtual try-on and support physically accurate animation. Extensive experiments\nshowcase our method's superior and competitive performance. Our project page is\nat https://ggxxii.github.io/clothedreamer.\n", "link": "http://arxiv.org/abs/2406.16815v1", "date": "2024-06-24", "relevancy": 2.7165, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.8017}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5987}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ClotheDreamer%3A%20Text-Guided%20Garment%20Generation%20with%203D%20Gaussians&body=Title%3A%20ClotheDreamer%3A%20Text-Guided%20Garment%20Generation%20with%203D%20Gaussians%0AAuthor%3A%20Yufei%20Liu%20and%20Junshu%20Tang%20and%20Chu%20Zheng%20and%20Shijie%20Zhang%20and%20Jinkun%20Hao%20and%20Junwei%20Zhu%20and%20Dongjin%20Huang%0AAbstract%3A%20%20%20High-fidelity%203D%20garment%20synthesis%20from%20text%20is%20desirable%20yet%20challenging%20for%0Adigital%20avatar%20creation.%20Recent%20diffusion-based%20approaches%20via%20Score%0ADistillation%20Sampling%20%28SDS%29%20have%20enabled%20new%20possibilities%20but%20either%0Aintricately%20couple%20with%20human%20body%20or%20struggle%20to%20reuse.%20We%20introduce%0AClotheDreamer%2C%20a%203D%20Gaussian-based%20method%20for%20generating%20wearable%2C%0Aproduction-ready%203D%20garment%20assets%20from%20text%20prompts.%20We%20propose%20a%20novel%0Arepresentation%20Disentangled%20Clothe%20Gaussian%20Splatting%20%28DCGS%29%20to%20enable%20separate%0Aoptimization.%20DCGS%20represents%20clothed%20avatar%20as%20one%20Gaussian%20model%20but%20freezes%0Abody%20Gaussian%20splats.%20To%20enhance%20quality%20and%20completeness%2C%20we%20incorporate%0Abidirectional%20SDS%20to%20supervise%20clothed%20avatar%20and%20garment%20RGBD%20renderings%0Arespectively%20with%20pose%20conditions%20and%20propose%20a%20new%20pruning%20strategy%20for%20loose%0Aclothing.%20Our%20approach%20can%20also%20support%20custom%20clothing%20templates%20as%20input.%0ABenefiting%20from%20our%20design%2C%20the%20synthetic%203D%20garment%20can%20be%20easily%20applied%20to%0Avirtual%20try-on%20and%20support%20physically%20accurate%20animation.%20Extensive%20experiments%0Ashowcase%20our%20method%27s%20superior%20and%20competitive%20performance.%20Our%20project%20page%20is%0Aat%20https%3A//ggxxii.github.io/clothedreamer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClotheDreamer%253A%2520Text-Guided%2520Garment%2520Generation%2520with%25203D%2520Gaussians%26entry.906535625%3DYufei%2520Liu%2520and%2520Junshu%2520Tang%2520and%2520Chu%2520Zheng%2520and%2520Shijie%2520Zhang%2520and%2520Jinkun%2520Hao%2520and%2520Junwei%2520Zhu%2520and%2520Dongjin%2520Huang%26entry.1292438233%3D%2520%2520High-fidelity%25203D%2520garment%2520synthesis%2520from%2520text%2520is%2520desirable%2520yet%2520challenging%2520for%250Adigital%2520avatar%2520creation.%2520Recent%2520diffusion-based%2520approaches%2520via%2520Score%250ADistillation%2520Sampling%2520%2528SDS%2529%2520have%2520enabled%2520new%2520possibilities%2520but%2520either%250Aintricately%2520couple%2520with%2520human%2520body%2520or%2520struggle%2520to%2520reuse.%2520We%2520introduce%250AClotheDreamer%252C%2520a%25203D%2520Gaussian-based%2520method%2520for%2520generating%2520wearable%252C%250Aproduction-ready%25203D%2520garment%2520assets%2520from%2520text%2520prompts.%2520We%2520propose%2520a%2520novel%250Arepresentation%2520Disentangled%2520Clothe%2520Gaussian%2520Splatting%2520%2528DCGS%2529%2520to%2520enable%2520separate%250Aoptimization.%2520DCGS%2520represents%2520clothed%2520avatar%2520as%2520one%2520Gaussian%2520model%2520but%2520freezes%250Abody%2520Gaussian%2520splats.%2520To%2520enhance%2520quality%2520and%2520completeness%252C%2520we%2520incorporate%250Abidirectional%2520SDS%2520to%2520supervise%2520clothed%2520avatar%2520and%2520garment%2520RGBD%2520renderings%250Arespectively%2520with%2520pose%2520conditions%2520and%2520propose%2520a%2520new%2520pruning%2520strategy%2520for%2520loose%250Aclothing.%2520Our%2520approach%2520can%2520also%2520support%2520custom%2520clothing%2520templates%2520as%2520input.%250ABenefiting%2520from%2520our%2520design%252C%2520the%2520synthetic%25203D%2520garment%2520can%2520be%2520easily%2520applied%2520to%250Avirtual%2520try-on%2520and%2520support%2520physically%2520accurate%2520animation.%2520Extensive%2520experiments%250Ashowcase%2520our%2520method%2527s%2520superior%2520and%2520competitive%2520performance.%2520Our%2520project%2520page%2520is%250Aat%2520https%253A//ggxxii.github.io/clothedreamer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClotheDreamer%3A%20Text-Guided%20Garment%20Generation%20with%203D%20Gaussians&entry.906535625=Yufei%20Liu%20and%20Junshu%20Tang%20and%20Chu%20Zheng%20and%20Shijie%20Zhang%20and%20Jinkun%20Hao%20and%20Junwei%20Zhu%20and%20Dongjin%20Huang&entry.1292438233=%20%20High-fidelity%203D%20garment%20synthesis%20from%20text%20is%20desirable%20yet%20challenging%20for%0Adigital%20avatar%20creation.%20Recent%20diffusion-based%20approaches%20via%20Score%0ADistillation%20Sampling%20%28SDS%29%20have%20enabled%20new%20possibilities%20but%20either%0Aintricately%20couple%20with%20human%20body%20or%20struggle%20to%20reuse.%20We%20introduce%0AClotheDreamer%2C%20a%203D%20Gaussian-based%20method%20for%20generating%20wearable%2C%0Aproduction-ready%203D%20garment%20assets%20from%20text%20prompts.%20We%20propose%20a%20novel%0Arepresentation%20Disentangled%20Clothe%20Gaussian%20Splatting%20%28DCGS%29%20to%20enable%20separate%0Aoptimization.%20DCGS%20represents%20clothed%20avatar%20as%20one%20Gaussian%20model%20but%20freezes%0Abody%20Gaussian%20splats.%20To%20enhance%20quality%20and%20completeness%2C%20we%20incorporate%0Abidirectional%20SDS%20to%20supervise%20clothed%20avatar%20and%20garment%20RGBD%20renderings%0Arespectively%20with%20pose%20conditions%20and%20propose%20a%20new%20pruning%20strategy%20for%20loose%0Aclothing.%20Our%20approach%20can%20also%20support%20custom%20clothing%20templates%20as%20input.%0ABenefiting%20from%20our%20design%2C%20the%20synthetic%203D%20garment%20can%20be%20easily%20applied%20to%0Avirtual%20try-on%20and%20support%20physically%20accurate%20animation.%20Extensive%20experiments%0Ashowcase%20our%20method%27s%20superior%20and%20competitive%20performance.%20Our%20project%20page%20is%0Aat%20https%3A//ggxxii.github.io/clothedreamer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16815v1&entry.124074799=Read"},
{"title": "Asymmetrical Siamese Network for Point Clouds Normal Estimation", "author": "Wei Jin and Jun Zhou and Nannan Li and Haba Madeline and Xiuping Liu", "abstract": "  In recent years, deep learning-based point cloud normal estimation has made\ngreat progress. However, existing methods mainly rely on the PCPNet dataset,\nleading to overfitting. In addition, the correlation between point clouds with\ndifferent noise scales remains unexplored, resulting in poor performance in\ncross-domain scenarios. In this paper, we explore the consistency of intrinsic\nfeatures learned from clean and noisy point clouds using an Asymmetric Siamese\nNetwork architecture. By applying reasonable constraints between features\nextracted from different branches, we enhance the quality of normal estimation.\nMoreover, we introduce a novel multi-view normal estimation dataset that\nincludes a larger variety of shapes with different noise levels. Evaluation of\nexisting methods on this new dataset reveals their inability to adapt to\ndifferent types of shapes, indicating a degree of overfitting. Extensive\nexperiments show that the proposed dataset poses significant challenges for\npoint cloud normal estimation and that our feature constraint mechanism\neffectively improves upon existing methods and reduces overfitting in current\narchitectures.\n", "link": "http://arxiv.org/abs/2406.09681v2", "date": "2024-06-24", "relevancy": 2.7074, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5549}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5548}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asymmetrical%20Siamese%20Network%20for%20Point%20Clouds%20Normal%20Estimation&body=Title%3A%20Asymmetrical%20Siamese%20Network%20for%20Point%20Clouds%20Normal%20Estimation%0AAuthor%3A%20Wei%20Jin%20and%20Jun%20Zhou%20and%20Nannan%20Li%20and%20Haba%20Madeline%20and%20Xiuping%20Liu%0AAbstract%3A%20%20%20In%20recent%20years%2C%20deep%20learning-based%20point%20cloud%20normal%20estimation%20has%20made%0Agreat%20progress.%20However%2C%20existing%20methods%20mainly%20rely%20on%20the%20PCPNet%20dataset%2C%0Aleading%20to%20overfitting.%20In%20addition%2C%20the%20correlation%20between%20point%20clouds%20with%0Adifferent%20noise%20scales%20remains%20unexplored%2C%20resulting%20in%20poor%20performance%20in%0Across-domain%20scenarios.%20In%20this%20paper%2C%20we%20explore%20the%20consistency%20of%20intrinsic%0Afeatures%20learned%20from%20clean%20and%20noisy%20point%20clouds%20using%20an%20Asymmetric%20Siamese%0ANetwork%20architecture.%20By%20applying%20reasonable%20constraints%20between%20features%0Aextracted%20from%20different%20branches%2C%20we%20enhance%20the%20quality%20of%20normal%20estimation.%0AMoreover%2C%20we%20introduce%20a%20novel%20multi-view%20normal%20estimation%20dataset%20that%0Aincludes%20a%20larger%20variety%20of%20shapes%20with%20different%20noise%20levels.%20Evaluation%20of%0Aexisting%20methods%20on%20this%20new%20dataset%20reveals%20their%20inability%20to%20adapt%20to%0Adifferent%20types%20of%20shapes%2C%20indicating%20a%20degree%20of%20overfitting.%20Extensive%0Aexperiments%20show%20that%20the%20proposed%20dataset%20poses%20significant%20challenges%20for%0Apoint%20cloud%20normal%20estimation%20and%20that%20our%20feature%20constraint%20mechanism%0Aeffectively%20improves%20upon%20existing%20methods%20and%20reduces%20overfitting%20in%20current%0Aarchitectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09681v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsymmetrical%2520Siamese%2520Network%2520for%2520Point%2520Clouds%2520Normal%2520Estimation%26entry.906535625%3DWei%2520Jin%2520and%2520Jun%2520Zhou%2520and%2520Nannan%2520Li%2520and%2520Haba%2520Madeline%2520and%2520Xiuping%2520Liu%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520deep%2520learning-based%2520point%2520cloud%2520normal%2520estimation%2520has%2520made%250Agreat%2520progress.%2520However%252C%2520existing%2520methods%2520mainly%2520rely%2520on%2520the%2520PCPNet%2520dataset%252C%250Aleading%2520to%2520overfitting.%2520In%2520addition%252C%2520the%2520correlation%2520between%2520point%2520clouds%2520with%250Adifferent%2520noise%2520scales%2520remains%2520unexplored%252C%2520resulting%2520in%2520poor%2520performance%2520in%250Across-domain%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520consistency%2520of%2520intrinsic%250Afeatures%2520learned%2520from%2520clean%2520and%2520noisy%2520point%2520clouds%2520using%2520an%2520Asymmetric%2520Siamese%250ANetwork%2520architecture.%2520By%2520applying%2520reasonable%2520constraints%2520between%2520features%250Aextracted%2520from%2520different%2520branches%252C%2520we%2520enhance%2520the%2520quality%2520of%2520normal%2520estimation.%250AMoreover%252C%2520we%2520introduce%2520a%2520novel%2520multi-view%2520normal%2520estimation%2520dataset%2520that%250Aincludes%2520a%2520larger%2520variety%2520of%2520shapes%2520with%2520different%2520noise%2520levels.%2520Evaluation%2520of%250Aexisting%2520methods%2520on%2520this%2520new%2520dataset%2520reveals%2520their%2520inability%2520to%2520adapt%2520to%250Adifferent%2520types%2520of%2520shapes%252C%2520indicating%2520a%2520degree%2520of%2520overfitting.%2520Extensive%250Aexperiments%2520show%2520that%2520the%2520proposed%2520dataset%2520poses%2520significant%2520challenges%2520for%250Apoint%2520cloud%2520normal%2520estimation%2520and%2520that%2520our%2520feature%2520constraint%2520mechanism%250Aeffectively%2520improves%2520upon%2520existing%2520methods%2520and%2520reduces%2520overfitting%2520in%2520current%250Aarchitectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09681v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asymmetrical%20Siamese%20Network%20for%20Point%20Clouds%20Normal%20Estimation&entry.906535625=Wei%20Jin%20and%20Jun%20Zhou%20and%20Nannan%20Li%20and%20Haba%20Madeline%20and%20Xiuping%20Liu&entry.1292438233=%20%20In%20recent%20years%2C%20deep%20learning-based%20point%20cloud%20normal%20estimation%20has%20made%0Agreat%20progress.%20However%2C%20existing%20methods%20mainly%20rely%20on%20the%20PCPNet%20dataset%2C%0Aleading%20to%20overfitting.%20In%20addition%2C%20the%20correlation%20between%20point%20clouds%20with%0Adifferent%20noise%20scales%20remains%20unexplored%2C%20resulting%20in%20poor%20performance%20in%0Across-domain%20scenarios.%20In%20this%20paper%2C%20we%20explore%20the%20consistency%20of%20intrinsic%0Afeatures%20learned%20from%20clean%20and%20noisy%20point%20clouds%20using%20an%20Asymmetric%20Siamese%0ANetwork%20architecture.%20By%20applying%20reasonable%20constraints%20between%20features%0Aextracted%20from%20different%20branches%2C%20we%20enhance%20the%20quality%20of%20normal%20estimation.%0AMoreover%2C%20we%20introduce%20a%20novel%20multi-view%20normal%20estimation%20dataset%20that%0Aincludes%20a%20larger%20variety%20of%20shapes%20with%20different%20noise%20levels.%20Evaluation%20of%0Aexisting%20methods%20on%20this%20new%20dataset%20reveals%20their%20inability%20to%20adapt%20to%0Adifferent%20types%20of%20shapes%2C%20indicating%20a%20degree%20of%20overfitting.%20Extensive%0Aexperiments%20show%20that%20the%20proposed%20dataset%20poses%20significant%20challenges%20for%0Apoint%20cloud%20normal%20estimation%20and%20that%20our%20feature%20constraint%20mechanism%0Aeffectively%20improves%20upon%20existing%20methods%20and%20reduces%20overfitting%20in%20current%0Aarchitectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09681v2&entry.124074799=Read"},
{"title": "Relaxing Continuous Constraints of Equivariant Graph Neural Networks for\n  Physical Dynamics Learning", "author": "Zinan Zheng and Yang Liu and Jia Li and Jianhua Yao and Yu Rong", "abstract": "  Incorporating Euclidean symmetries (e.g. rotation equivariance) as inductive\nbiases into graph neural networks has improved their generalization ability and\ndata efficiency in unbounded physical dynamics modeling. However, in various\nscientific and engineering applications, the symmetries of dynamics are\nfrequently discrete due to the boundary conditions. Thus, existing GNNs either\noverlook necessary symmetry, resulting in suboptimal representation ability, or\nimpose excessive equivariance, which fails to generalize to unobserved\nsymmetric dynamics. In this work, we propose a general Discrete Equivariant\nGraph Neural Network (DEGNN) that guarantees equivariance to a given discrete\npoint group. Specifically, we show that such discrete equivariant message\npassing could be constructed by transforming geometric features into\npermutation-invariant embeddings. Through relaxing continuous equivariant\nconstraints, DEGNN can employ more geometric feature combinations to\napproximate unobserved physical object interaction functions. Two\nimplementation approaches of DEGNN are proposed based on ranking or pooling\npermutation-invariant functions. We apply DEGNN to various physical dynamics,\nranging from particle, molecular, crowd to vehicle dynamics. In twenty\nscenarios, DEGNN significantly outperforms existing state-of-the-art\napproaches. Moreover, we show that DEGNN is data efficient, learning with less\ndata, and can generalize across scenarios such as unobserved orientation.\n", "link": "http://arxiv.org/abs/2406.16295v1", "date": "2024-06-24", "relevancy": 2.7003, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5664}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5422}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relaxing%20Continuous%20Constraints%20of%20Equivariant%20Graph%20Neural%20Networks%20for%0A%20%20Physical%20Dynamics%20Learning&body=Title%3A%20Relaxing%20Continuous%20Constraints%20of%20Equivariant%20Graph%20Neural%20Networks%20for%0A%20%20Physical%20Dynamics%20Learning%0AAuthor%3A%20Zinan%20Zheng%20and%20Yang%20Liu%20and%20Jia%20Li%20and%20Jianhua%20Yao%20and%20Yu%20Rong%0AAbstract%3A%20%20%20Incorporating%20Euclidean%20symmetries%20%28e.g.%20rotation%20equivariance%29%20as%20inductive%0Abiases%20into%20graph%20neural%20networks%20has%20improved%20their%20generalization%20ability%20and%0Adata%20efficiency%20in%20unbounded%20physical%20dynamics%20modeling.%20However%2C%20in%20various%0Ascientific%20and%20engineering%20applications%2C%20the%20symmetries%20of%20dynamics%20are%0Afrequently%20discrete%20due%20to%20the%20boundary%20conditions.%20Thus%2C%20existing%20GNNs%20either%0Aoverlook%20necessary%20symmetry%2C%20resulting%20in%20suboptimal%20representation%20ability%2C%20or%0Aimpose%20excessive%20equivariance%2C%20which%20fails%20to%20generalize%20to%20unobserved%0Asymmetric%20dynamics.%20In%20this%20work%2C%20we%20propose%20a%20general%20Discrete%20Equivariant%0AGraph%20Neural%20Network%20%28DEGNN%29%20that%20guarantees%20equivariance%20to%20a%20given%20discrete%0Apoint%20group.%20Specifically%2C%20we%20show%20that%20such%20discrete%20equivariant%20message%0Apassing%20could%20be%20constructed%20by%20transforming%20geometric%20features%20into%0Apermutation-invariant%20embeddings.%20Through%20relaxing%20continuous%20equivariant%0Aconstraints%2C%20DEGNN%20can%20employ%20more%20geometric%20feature%20combinations%20to%0Aapproximate%20unobserved%20physical%20object%20interaction%20functions.%20Two%0Aimplementation%20approaches%20of%20DEGNN%20are%20proposed%20based%20on%20ranking%20or%20pooling%0Apermutation-invariant%20functions.%20We%20apply%20DEGNN%20to%20various%20physical%20dynamics%2C%0Aranging%20from%20particle%2C%20molecular%2C%20crowd%20to%20vehicle%20dynamics.%20In%20twenty%0Ascenarios%2C%20DEGNN%20significantly%20outperforms%20existing%20state-of-the-art%0Aapproaches.%20Moreover%2C%20we%20show%20that%20DEGNN%20is%20data%20efficient%2C%20learning%20with%20less%0Adata%2C%20and%20can%20generalize%20across%20scenarios%20such%20as%20unobserved%20orientation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16295v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelaxing%2520Continuous%2520Constraints%2520of%2520Equivariant%2520Graph%2520Neural%2520Networks%2520for%250A%2520%2520Physical%2520Dynamics%2520Learning%26entry.906535625%3DZinan%2520Zheng%2520and%2520Yang%2520Liu%2520and%2520Jia%2520Li%2520and%2520Jianhua%2520Yao%2520and%2520Yu%2520Rong%26entry.1292438233%3D%2520%2520Incorporating%2520Euclidean%2520symmetries%2520%2528e.g.%2520rotation%2520equivariance%2529%2520as%2520inductive%250Abiases%2520into%2520graph%2520neural%2520networks%2520has%2520improved%2520their%2520generalization%2520ability%2520and%250Adata%2520efficiency%2520in%2520unbounded%2520physical%2520dynamics%2520modeling.%2520However%252C%2520in%2520various%250Ascientific%2520and%2520engineering%2520applications%252C%2520the%2520symmetries%2520of%2520dynamics%2520are%250Afrequently%2520discrete%2520due%2520to%2520the%2520boundary%2520conditions.%2520Thus%252C%2520existing%2520GNNs%2520either%250Aoverlook%2520necessary%2520symmetry%252C%2520resulting%2520in%2520suboptimal%2520representation%2520ability%252C%2520or%250Aimpose%2520excessive%2520equivariance%252C%2520which%2520fails%2520to%2520generalize%2520to%2520unobserved%250Asymmetric%2520dynamics.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520general%2520Discrete%2520Equivariant%250AGraph%2520Neural%2520Network%2520%2528DEGNN%2529%2520that%2520guarantees%2520equivariance%2520to%2520a%2520given%2520discrete%250Apoint%2520group.%2520Specifically%252C%2520we%2520show%2520that%2520such%2520discrete%2520equivariant%2520message%250Apassing%2520could%2520be%2520constructed%2520by%2520transforming%2520geometric%2520features%2520into%250Apermutation-invariant%2520embeddings.%2520Through%2520relaxing%2520continuous%2520equivariant%250Aconstraints%252C%2520DEGNN%2520can%2520employ%2520more%2520geometric%2520feature%2520combinations%2520to%250Aapproximate%2520unobserved%2520physical%2520object%2520interaction%2520functions.%2520Two%250Aimplementation%2520approaches%2520of%2520DEGNN%2520are%2520proposed%2520based%2520on%2520ranking%2520or%2520pooling%250Apermutation-invariant%2520functions.%2520We%2520apply%2520DEGNN%2520to%2520various%2520physical%2520dynamics%252C%250Aranging%2520from%2520particle%252C%2520molecular%252C%2520crowd%2520to%2520vehicle%2520dynamics.%2520In%2520twenty%250Ascenarios%252C%2520DEGNN%2520significantly%2520outperforms%2520existing%2520state-of-the-art%250Aapproaches.%2520Moreover%252C%2520we%2520show%2520that%2520DEGNN%2520is%2520data%2520efficient%252C%2520learning%2520with%2520less%250Adata%252C%2520and%2520can%2520generalize%2520across%2520scenarios%2520such%2520as%2520unobserved%2520orientation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16295v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relaxing%20Continuous%20Constraints%20of%20Equivariant%20Graph%20Neural%20Networks%20for%0A%20%20Physical%20Dynamics%20Learning&entry.906535625=Zinan%20Zheng%20and%20Yang%20Liu%20and%20Jia%20Li%20and%20Jianhua%20Yao%20and%20Yu%20Rong&entry.1292438233=%20%20Incorporating%20Euclidean%20symmetries%20%28e.g.%20rotation%20equivariance%29%20as%20inductive%0Abiases%20into%20graph%20neural%20networks%20has%20improved%20their%20generalization%20ability%20and%0Adata%20efficiency%20in%20unbounded%20physical%20dynamics%20modeling.%20However%2C%20in%20various%0Ascientific%20and%20engineering%20applications%2C%20the%20symmetries%20of%20dynamics%20are%0Afrequently%20discrete%20due%20to%20the%20boundary%20conditions.%20Thus%2C%20existing%20GNNs%20either%0Aoverlook%20necessary%20symmetry%2C%20resulting%20in%20suboptimal%20representation%20ability%2C%20or%0Aimpose%20excessive%20equivariance%2C%20which%20fails%20to%20generalize%20to%20unobserved%0Asymmetric%20dynamics.%20In%20this%20work%2C%20we%20propose%20a%20general%20Discrete%20Equivariant%0AGraph%20Neural%20Network%20%28DEGNN%29%20that%20guarantees%20equivariance%20to%20a%20given%20discrete%0Apoint%20group.%20Specifically%2C%20we%20show%20that%20such%20discrete%20equivariant%20message%0Apassing%20could%20be%20constructed%20by%20transforming%20geometric%20features%20into%0Apermutation-invariant%20embeddings.%20Through%20relaxing%20continuous%20equivariant%0Aconstraints%2C%20DEGNN%20can%20employ%20more%20geometric%20feature%20combinations%20to%0Aapproximate%20unobserved%20physical%20object%20interaction%20functions.%20Two%0Aimplementation%20approaches%20of%20DEGNN%20are%20proposed%20based%20on%20ranking%20or%20pooling%0Apermutation-invariant%20functions.%20We%20apply%20DEGNN%20to%20various%20physical%20dynamics%2C%0Aranging%20from%20particle%2C%20molecular%2C%20crowd%20to%20vehicle%20dynamics.%20In%20twenty%0Ascenarios%2C%20DEGNN%20significantly%20outperforms%20existing%20state-of-the-art%0Aapproaches.%20Moreover%2C%20we%20show%20that%20DEGNN%20is%20data%20efficient%2C%20learning%20with%20less%0Adata%2C%20and%20can%20generalize%20across%20scenarios%20such%20as%20unobserved%20orientation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16295v1&entry.124074799=Read"},
{"title": "Automatically Generating UI Code from Screenshot: A\n  Divide-and-Conquer-Based Approach", "author": "Yuxuan Wan and Chaozheng Wang and Yi Dong and Wenxuan Wang and Shuqing Li and Yintong Huo and Michael R. Lyu", "abstract": "  Websites are critical in today's digital world, with over 1.11 billion\ncurrently active and approximately 252,000 new sites launched daily. Converting\nwebsite layout design into functional UI code is a time-consuming yet\nindispensable step of website development. Manual methods of converting visual\ndesigns into functional code present significant challenges, especially for\nnon-experts. To explore automatic design-to-code solutions, we first conduct a\nmotivating study on GPT-4o and identify three types of issues in generating UI\ncode: element omission, element distortion, and element misarrangement. We\nfurther reveal that a focus on smaller visual segments can help multimodal\nlarge language models (MLLMs) mitigate these failures in the generation\nprocess. In this paper, we propose DCGen, a divide-and-conquer-based approach\nto automate the translation of webpage design to UI code. DCGen starts by\ndividing screenshots into manageable segments, generating descriptions for each\nsegment, and then reassembling them into complete UI code for the entire\nscreenshot. We conduct extensive testing with a dataset comprised of real-world\nwebsites and various MLLMs and demonstrate that DCGen achieves up to a 14%\nimprovement in visual similarity over competing methods. To the best of our\nknowledge, DCGen is the first segment-aware prompt-based approach for\ngenerating UI code directly from screenshots.\n", "link": "http://arxiv.org/abs/2406.16386v1", "date": "2024-06-24", "relevancy": 2.6812, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.589}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5166}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatically%20Generating%20UI%20Code%20from%20Screenshot%3A%20A%0A%20%20Divide-and-Conquer-Based%20Approach&body=Title%3A%20Automatically%20Generating%20UI%20Code%20from%20Screenshot%3A%20A%0A%20%20Divide-and-Conquer-Based%20Approach%0AAuthor%3A%20Yuxuan%20Wan%20and%20Chaozheng%20Wang%20and%20Yi%20Dong%20and%20Wenxuan%20Wang%20and%20Shuqing%20Li%20and%20Yintong%20Huo%20and%20Michael%20R.%20Lyu%0AAbstract%3A%20%20%20Websites%20are%20critical%20in%20today%27s%20digital%20world%2C%20with%20over%201.11%20billion%0Acurrently%20active%20and%20approximately%20252%2C000%20new%20sites%20launched%20daily.%20Converting%0Awebsite%20layout%20design%20into%20functional%20UI%20code%20is%20a%20time-consuming%20yet%0Aindispensable%20step%20of%20website%20development.%20Manual%20methods%20of%20converting%20visual%0Adesigns%20into%20functional%20code%20present%20significant%20challenges%2C%20especially%20for%0Anon-experts.%20To%20explore%20automatic%20design-to-code%20solutions%2C%20we%20first%20conduct%20a%0Amotivating%20study%20on%20GPT-4o%20and%20identify%20three%20types%20of%20issues%20in%20generating%20UI%0Acode%3A%20element%20omission%2C%20element%20distortion%2C%20and%20element%20misarrangement.%20We%0Afurther%20reveal%20that%20a%20focus%20on%20smaller%20visual%20segments%20can%20help%20multimodal%0Alarge%20language%20models%20%28MLLMs%29%20mitigate%20these%20failures%20in%20the%20generation%0Aprocess.%20In%20this%20paper%2C%20we%20propose%20DCGen%2C%20a%20divide-and-conquer-based%20approach%0Ato%20automate%20the%20translation%20of%20webpage%20design%20to%20UI%20code.%20DCGen%20starts%20by%0Adividing%20screenshots%20into%20manageable%20segments%2C%20generating%20descriptions%20for%20each%0Asegment%2C%20and%20then%20reassembling%20them%20into%20complete%20UI%20code%20for%20the%20entire%0Ascreenshot.%20We%20conduct%20extensive%20testing%20with%20a%20dataset%20comprised%20of%20real-world%0Awebsites%20and%20various%20MLLMs%20and%20demonstrate%20that%20DCGen%20achieves%20up%20to%20a%2014%25%0Aimprovement%20in%20visual%20similarity%20over%20competing%20methods.%20To%20the%20best%20of%20our%0Aknowledge%2C%20DCGen%20is%20the%20first%20segment-aware%20prompt-based%20approach%20for%0Agenerating%20UI%20code%20directly%20from%20screenshots.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatically%2520Generating%2520UI%2520Code%2520from%2520Screenshot%253A%2520A%250A%2520%2520Divide-and-Conquer-Based%2520Approach%26entry.906535625%3DYuxuan%2520Wan%2520and%2520Chaozheng%2520Wang%2520and%2520Yi%2520Dong%2520and%2520Wenxuan%2520Wang%2520and%2520Shuqing%2520Li%2520and%2520Yintong%2520Huo%2520and%2520Michael%2520R.%2520Lyu%26entry.1292438233%3D%2520%2520Websites%2520are%2520critical%2520in%2520today%2527s%2520digital%2520world%252C%2520with%2520over%25201.11%2520billion%250Acurrently%2520active%2520and%2520approximately%2520252%252C000%2520new%2520sites%2520launched%2520daily.%2520Converting%250Awebsite%2520layout%2520design%2520into%2520functional%2520UI%2520code%2520is%2520a%2520time-consuming%2520yet%250Aindispensable%2520step%2520of%2520website%2520development.%2520Manual%2520methods%2520of%2520converting%2520visual%250Adesigns%2520into%2520functional%2520code%2520present%2520significant%2520challenges%252C%2520especially%2520for%250Anon-experts.%2520To%2520explore%2520automatic%2520design-to-code%2520solutions%252C%2520we%2520first%2520conduct%2520a%250Amotivating%2520study%2520on%2520GPT-4o%2520and%2520identify%2520three%2520types%2520of%2520issues%2520in%2520generating%2520UI%250Acode%253A%2520element%2520omission%252C%2520element%2520distortion%252C%2520and%2520element%2520misarrangement.%2520We%250Afurther%2520reveal%2520that%2520a%2520focus%2520on%2520smaller%2520visual%2520segments%2520can%2520help%2520multimodal%250Alarge%2520language%2520models%2520%2528MLLMs%2529%2520mitigate%2520these%2520failures%2520in%2520the%2520generation%250Aprocess.%2520In%2520this%2520paper%252C%2520we%2520propose%2520DCGen%252C%2520a%2520divide-and-conquer-based%2520approach%250Ato%2520automate%2520the%2520translation%2520of%2520webpage%2520design%2520to%2520UI%2520code.%2520DCGen%2520starts%2520by%250Adividing%2520screenshots%2520into%2520manageable%2520segments%252C%2520generating%2520descriptions%2520for%2520each%250Asegment%252C%2520and%2520then%2520reassembling%2520them%2520into%2520complete%2520UI%2520code%2520for%2520the%2520entire%250Ascreenshot.%2520We%2520conduct%2520extensive%2520testing%2520with%2520a%2520dataset%2520comprised%2520of%2520real-world%250Awebsites%2520and%2520various%2520MLLMs%2520and%2520demonstrate%2520that%2520DCGen%2520achieves%2520up%2520to%2520a%252014%2525%250Aimprovement%2520in%2520visual%2520similarity%2520over%2520competing%2520methods.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520DCGen%2520is%2520the%2520first%2520segment-aware%2520prompt-based%2520approach%2520for%250Agenerating%2520UI%2520code%2520directly%2520from%2520screenshots.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatically%20Generating%20UI%20Code%20from%20Screenshot%3A%20A%0A%20%20Divide-and-Conquer-Based%20Approach&entry.906535625=Yuxuan%20Wan%20and%20Chaozheng%20Wang%20and%20Yi%20Dong%20and%20Wenxuan%20Wang%20and%20Shuqing%20Li%20and%20Yintong%20Huo%20and%20Michael%20R.%20Lyu&entry.1292438233=%20%20Websites%20are%20critical%20in%20today%27s%20digital%20world%2C%20with%20over%201.11%20billion%0Acurrently%20active%20and%20approximately%20252%2C000%20new%20sites%20launched%20daily.%20Converting%0Awebsite%20layout%20design%20into%20functional%20UI%20code%20is%20a%20time-consuming%20yet%0Aindispensable%20step%20of%20website%20development.%20Manual%20methods%20of%20converting%20visual%0Adesigns%20into%20functional%20code%20present%20significant%20challenges%2C%20especially%20for%0Anon-experts.%20To%20explore%20automatic%20design-to-code%20solutions%2C%20we%20first%20conduct%20a%0Amotivating%20study%20on%20GPT-4o%20and%20identify%20three%20types%20of%20issues%20in%20generating%20UI%0Acode%3A%20element%20omission%2C%20element%20distortion%2C%20and%20element%20misarrangement.%20We%0Afurther%20reveal%20that%20a%20focus%20on%20smaller%20visual%20segments%20can%20help%20multimodal%0Alarge%20language%20models%20%28MLLMs%29%20mitigate%20these%20failures%20in%20the%20generation%0Aprocess.%20In%20this%20paper%2C%20we%20propose%20DCGen%2C%20a%20divide-and-conquer-based%20approach%0Ato%20automate%20the%20translation%20of%20webpage%20design%20to%20UI%20code.%20DCGen%20starts%20by%0Adividing%20screenshots%20into%20manageable%20segments%2C%20generating%20descriptions%20for%20each%0Asegment%2C%20and%20then%20reassembling%20them%20into%20complete%20UI%20code%20for%20the%20entire%0Ascreenshot.%20We%20conduct%20extensive%20testing%20with%20a%20dataset%20comprised%20of%20real-world%0Awebsites%20and%20various%20MLLMs%20and%20demonstrate%20that%20DCGen%20achieves%20up%20to%20a%2014%25%0Aimprovement%20in%20visual%20similarity%20over%20competing%20methods.%20To%20the%20best%20of%20our%0Aknowledge%2C%20DCGen%20is%20the%20first%20segment-aware%20prompt-based%20approach%20for%0Agenerating%20UI%20code%20directly%20from%20screenshots.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16386v1&entry.124074799=Read"},
{"title": "DreamBench++: A Human-Aligned Benchmark for Personalized Image\n  Generation", "author": "Yuang Peng and Yuxin Cui and Haomiao Tang and Zekun Qi and Runpei Dong and Jing Bai and Chunrui Han and Zheng Ge and Xiangyu Zhang and Shu-Tao Xia", "abstract": "  Personalized image generation holds great promise in assisting humans in\neveryday work and life due to its impressive function in creatively generating\npersonalized content. However, current evaluations either are automated but\nmisalign with humans or require human evaluations that are time-consuming and\nexpensive. In this work, we present DreamBench++, a human-aligned benchmark\nautomated by advanced multimodal GPT models. Specifically, we systematically\ndesign the prompts to let GPT be both human-aligned and self-aligned, empowered\nwith task reinforcement. Further, we construct a comprehensive dataset\ncomprising diverse images and prompts. By benchmarking 7 modern generative\nmodels, we demonstrate that DreamBench++ results in significantly more\nhuman-aligned evaluation, helping boost the community with innovative findings.\n", "link": "http://arxiv.org/abs/2406.16855v1", "date": "2024-06-24", "relevancy": 2.655, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5417}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.529}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamBench%2B%2B%3A%20A%20Human-Aligned%20Benchmark%20for%20Personalized%20Image%0A%20%20Generation&body=Title%3A%20DreamBench%2B%2B%3A%20A%20Human-Aligned%20Benchmark%20for%20Personalized%20Image%0A%20%20Generation%0AAuthor%3A%20Yuang%20Peng%20and%20Yuxin%20Cui%20and%20Haomiao%20Tang%20and%20Zekun%20Qi%20and%20Runpei%20Dong%20and%20Jing%20Bai%20and%20Chunrui%20Han%20and%20Zheng%20Ge%20and%20Xiangyu%20Zhang%20and%20Shu-Tao%20Xia%0AAbstract%3A%20%20%20Personalized%20image%20generation%20holds%20great%20promise%20in%20assisting%20humans%20in%0Aeveryday%20work%20and%20life%20due%20to%20its%20impressive%20function%20in%20creatively%20generating%0Apersonalized%20content.%20However%2C%20current%20evaluations%20either%20are%20automated%20but%0Amisalign%20with%20humans%20or%20require%20human%20evaluations%20that%20are%20time-consuming%20and%0Aexpensive.%20In%20this%20work%2C%20we%20present%20DreamBench%2B%2B%2C%20a%20human-aligned%20benchmark%0Aautomated%20by%20advanced%20multimodal%20GPT%20models.%20Specifically%2C%20we%20systematically%0Adesign%20the%20prompts%20to%20let%20GPT%20be%20both%20human-aligned%20and%20self-aligned%2C%20empowered%0Awith%20task%20reinforcement.%20Further%2C%20we%20construct%20a%20comprehensive%20dataset%0Acomprising%20diverse%20images%20and%20prompts.%20By%20benchmarking%207%20modern%20generative%0Amodels%2C%20we%20demonstrate%20that%20DreamBench%2B%2B%20results%20in%20significantly%20more%0Ahuman-aligned%20evaluation%2C%20helping%20boost%20the%20community%20with%20innovative%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamBench%252B%252B%253A%2520A%2520Human-Aligned%2520Benchmark%2520for%2520Personalized%2520Image%250A%2520%2520Generation%26entry.906535625%3DYuang%2520Peng%2520and%2520Yuxin%2520Cui%2520and%2520Haomiao%2520Tang%2520and%2520Zekun%2520Qi%2520and%2520Runpei%2520Dong%2520and%2520Jing%2520Bai%2520and%2520Chunrui%2520Han%2520and%2520Zheng%2520Ge%2520and%2520Xiangyu%2520Zhang%2520and%2520Shu-Tao%2520Xia%26entry.1292438233%3D%2520%2520Personalized%2520image%2520generation%2520holds%2520great%2520promise%2520in%2520assisting%2520humans%2520in%250Aeveryday%2520work%2520and%2520life%2520due%2520to%2520its%2520impressive%2520function%2520in%2520creatively%2520generating%250Apersonalized%2520content.%2520However%252C%2520current%2520evaluations%2520either%2520are%2520automated%2520but%250Amisalign%2520with%2520humans%2520or%2520require%2520human%2520evaluations%2520that%2520are%2520time-consuming%2520and%250Aexpensive.%2520In%2520this%2520work%252C%2520we%2520present%2520DreamBench%252B%252B%252C%2520a%2520human-aligned%2520benchmark%250Aautomated%2520by%2520advanced%2520multimodal%2520GPT%2520models.%2520Specifically%252C%2520we%2520systematically%250Adesign%2520the%2520prompts%2520to%2520let%2520GPT%2520be%2520both%2520human-aligned%2520and%2520self-aligned%252C%2520empowered%250Awith%2520task%2520reinforcement.%2520Further%252C%2520we%2520construct%2520a%2520comprehensive%2520dataset%250Acomprising%2520diverse%2520images%2520and%2520prompts.%2520By%2520benchmarking%25207%2520modern%2520generative%250Amodels%252C%2520we%2520demonstrate%2520that%2520DreamBench%252B%252B%2520results%2520in%2520significantly%2520more%250Ahuman-aligned%2520evaluation%252C%2520helping%2520boost%2520the%2520community%2520with%2520innovative%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamBench%2B%2B%3A%20A%20Human-Aligned%20Benchmark%20for%20Personalized%20Image%0A%20%20Generation&entry.906535625=Yuang%20Peng%20and%20Yuxin%20Cui%20and%20Haomiao%20Tang%20and%20Zekun%20Qi%20and%20Runpei%20Dong%20and%20Jing%20Bai%20and%20Chunrui%20Han%20and%20Zheng%20Ge%20and%20Xiangyu%20Zhang%20and%20Shu-Tao%20Xia&entry.1292438233=%20%20Personalized%20image%20generation%20holds%20great%20promise%20in%20assisting%20humans%20in%0Aeveryday%20work%20and%20life%20due%20to%20its%20impressive%20function%20in%20creatively%20generating%0Apersonalized%20content.%20However%2C%20current%20evaluations%20either%20are%20automated%20but%0Amisalign%20with%20humans%20or%20require%20human%20evaluations%20that%20are%20time-consuming%20and%0Aexpensive.%20In%20this%20work%2C%20we%20present%20DreamBench%2B%2B%2C%20a%20human-aligned%20benchmark%0Aautomated%20by%20advanced%20multimodal%20GPT%20models.%20Specifically%2C%20we%20systematically%0Adesign%20the%20prompts%20to%20let%20GPT%20be%20both%20human-aligned%20and%20self-aligned%2C%20empowered%0Awith%20task%20reinforcement.%20Further%2C%20we%20construct%20a%20comprehensive%20dataset%0Acomprising%20diverse%20images%20and%20prompts.%20By%20benchmarking%207%20modern%20generative%0Amodels%2C%20we%20demonstrate%20that%20DreamBench%2B%2B%20results%20in%20significantly%20more%0Ahuman-aligned%20evaluation%2C%20helping%20boost%20the%20community%20with%20innovative%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16855v1&entry.124074799=Read"},
{"title": "Breaking the Frame: Image Retrieval by Visual Overlap Prediction", "author": "Tong Wei and Philipp Lindenberger and Jiri Matas and Daniel Barath", "abstract": "  We propose a novel visual place recognition approach, VOP, that efficiently\naddresses occlusions and complex scenes by shifting from traditional reliance\non global image similarities and local features to image overlap prediction.\nThe proposed method enables the identification of visible image sections\nwithout requiring expensive feature detection and matching. By focusing on\nobtaining patch-level embeddings by a Vision Transformer backbone and\nestablishing patch-to-patch correspondences, our approach uses a voting\nmechanism to assess overlap scores for potential database images, thereby\nproviding a nuanced image retrieval metric in challenging scenarios. VOP leads\nto more accurate relative pose estimation and localization results on the\nretrieved image pairs than state-of-the-art baselines on a number of\nlarge-scale, real-world datasets. The code is available at\nhttps://github.com/weitong8591/vop.\n", "link": "http://arxiv.org/abs/2406.16204v1", "date": "2024-06-23", "relevancy": 2.6368, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5334}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5255}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5232}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20the%20Frame%3A%20Image%20Retrieval%20by%20Visual%20Overlap%20Prediction&body=Title%3A%20Breaking%20the%20Frame%3A%20Image%20Retrieval%20by%20Visual%20Overlap%20Prediction%0AAuthor%3A%20Tong%20Wei%20and%20Philipp%20Lindenberger%20and%20Jiri%20Matas%20and%20Daniel%20Barath%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20visual%20place%20recognition%20approach%2C%20VOP%2C%20that%20efficiently%0Aaddresses%20occlusions%20and%20complex%20scenes%20by%20shifting%20from%20traditional%20reliance%0Aon%20global%20image%20similarities%20and%20local%20features%20to%20image%20overlap%20prediction.%0AThe%20proposed%20method%20enables%20the%20identification%20of%20visible%20image%20sections%0Awithout%20requiring%20expensive%20feature%20detection%20and%20matching.%20By%20focusing%20on%0Aobtaining%20patch-level%20embeddings%20by%20a%20Vision%20Transformer%20backbone%20and%0Aestablishing%20patch-to-patch%20correspondences%2C%20our%20approach%20uses%20a%20voting%0Amechanism%20to%20assess%20overlap%20scores%20for%20potential%20database%20images%2C%20thereby%0Aproviding%20a%20nuanced%20image%20retrieval%20metric%20in%20challenging%20scenarios.%20VOP%20leads%0Ato%20more%20accurate%20relative%20pose%20estimation%20and%20localization%20results%20on%20the%0Aretrieved%20image%20pairs%20than%20state-of-the-art%20baselines%20on%20a%20number%20of%0Alarge-scale%2C%20real-world%20datasets.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/weitong8591/vop.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520the%2520Frame%253A%2520Image%2520Retrieval%2520by%2520Visual%2520Overlap%2520Prediction%26entry.906535625%3DTong%2520Wei%2520and%2520Philipp%2520Lindenberger%2520and%2520Jiri%2520Matas%2520and%2520Daniel%2520Barath%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520visual%2520place%2520recognition%2520approach%252C%2520VOP%252C%2520that%2520efficiently%250Aaddresses%2520occlusions%2520and%2520complex%2520scenes%2520by%2520shifting%2520from%2520traditional%2520reliance%250Aon%2520global%2520image%2520similarities%2520and%2520local%2520features%2520to%2520image%2520overlap%2520prediction.%250AThe%2520proposed%2520method%2520enables%2520the%2520identification%2520of%2520visible%2520image%2520sections%250Awithout%2520requiring%2520expensive%2520feature%2520detection%2520and%2520matching.%2520By%2520focusing%2520on%250Aobtaining%2520patch-level%2520embeddings%2520by%2520a%2520Vision%2520Transformer%2520backbone%2520and%250Aestablishing%2520patch-to-patch%2520correspondences%252C%2520our%2520approach%2520uses%2520a%2520voting%250Amechanism%2520to%2520assess%2520overlap%2520scores%2520for%2520potential%2520database%2520images%252C%2520thereby%250Aproviding%2520a%2520nuanced%2520image%2520retrieval%2520metric%2520in%2520challenging%2520scenarios.%2520VOP%2520leads%250Ato%2520more%2520accurate%2520relative%2520pose%2520estimation%2520and%2520localization%2520results%2520on%2520the%250Aretrieved%2520image%2520pairs%2520than%2520state-of-the-art%2520baselines%2520on%2520a%2520number%2520of%250Alarge-scale%252C%2520real-world%2520datasets.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/weitong8591/vop.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20the%20Frame%3A%20Image%20Retrieval%20by%20Visual%20Overlap%20Prediction&entry.906535625=Tong%20Wei%20and%20Philipp%20Lindenberger%20and%20Jiri%20Matas%20and%20Daniel%20Barath&entry.1292438233=%20%20We%20propose%20a%20novel%20visual%20place%20recognition%20approach%2C%20VOP%2C%20that%20efficiently%0Aaddresses%20occlusions%20and%20complex%20scenes%20by%20shifting%20from%20traditional%20reliance%0Aon%20global%20image%20similarities%20and%20local%20features%20to%20image%20overlap%20prediction.%0AThe%20proposed%20method%20enables%20the%20identification%20of%20visible%20image%20sections%0Awithout%20requiring%20expensive%20feature%20detection%20and%20matching.%20By%20focusing%20on%0Aobtaining%20patch-level%20embeddings%20by%20a%20Vision%20Transformer%20backbone%20and%0Aestablishing%20patch-to-patch%20correspondences%2C%20our%20approach%20uses%20a%20voting%0Amechanism%20to%20assess%20overlap%20scores%20for%20potential%20database%20images%2C%20thereby%0Aproviding%20a%20nuanced%20image%20retrieval%20metric%20in%20challenging%20scenarios.%20VOP%20leads%0Ato%20more%20accurate%20relative%20pose%20estimation%20and%20localization%20results%20on%20the%0Aretrieved%20image%20pairs%20than%20state-of-the-art%20baselines%20on%20a%20number%20of%0Alarge-scale%2C%20real-world%20datasets.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/weitong8591/vop.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16204v1&entry.124074799=Read"},
{"title": "Multicam-SLAM: Non-overlapping Multi-camera SLAM for Indirect Visual\n  Localization and Navigation", "author": "Shenghao Li and Luchao Pang and Xianglong Hu", "abstract": "  This paper presents a novel approach to visual simultaneous localization and\nmapping (SLAM) using multiple RGB-D cameras. The proposed method,\nMulticam-SLAM, significantly enhances the robustness and accuracy of SLAM\nsystems by capturing more comprehensive spatial information from various\nperspectives. This method enables the accurate determination of pose\nrelationships among multiple cameras without the need for overlapping fields of\nview. The proposed Muticam-SLAM includes a unique multi-camera model, a\nmulti-keyframes structure, and several parallel SLAM threads. The multi-camera\nmodel allows for the integration of data from multiple cameras, while the\nmulti-keyframes and parallel SLAM threads ensure efficient and accurate pose\nestimation and mapping. Extensive experiments in various environments\ndemonstrate the superior accuracy and robustness of the proposed method\ncompared to conventional single-camera SLAM systems. The results highlight the\npotential of the proposed Multicam-SLAM for more complex and challenging\napplications. Code is available at\n\\url{https://github.com/AlterPang/Multi_ORB_SLAM}.\n", "link": "http://arxiv.org/abs/2406.06374v2", "date": "2024-06-23", "relevancy": 2.6137, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6712}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6498}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multicam-SLAM%3A%20Non-overlapping%20Multi-camera%20SLAM%20for%20Indirect%20Visual%0A%20%20Localization%20and%20Navigation&body=Title%3A%20Multicam-SLAM%3A%20Non-overlapping%20Multi-camera%20SLAM%20for%20Indirect%20Visual%0A%20%20Localization%20and%20Navigation%0AAuthor%3A%20Shenghao%20Li%20and%20Luchao%20Pang%20and%20Xianglong%20Hu%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20approach%20to%20visual%20simultaneous%20localization%20and%0Amapping%20%28SLAM%29%20using%20multiple%20RGB-D%20cameras.%20The%20proposed%20method%2C%0AMulticam-SLAM%2C%20significantly%20enhances%20the%20robustness%20and%20accuracy%20of%20SLAM%0Asystems%20by%20capturing%20more%20comprehensive%20spatial%20information%20from%20various%0Aperspectives.%20This%20method%20enables%20the%20accurate%20determination%20of%20pose%0Arelationships%20among%20multiple%20cameras%20without%20the%20need%20for%20overlapping%20fields%20of%0Aview.%20The%20proposed%20Muticam-SLAM%20includes%20a%20unique%20multi-camera%20model%2C%20a%0Amulti-keyframes%20structure%2C%20and%20several%20parallel%20SLAM%20threads.%20The%20multi-camera%0Amodel%20allows%20for%20the%20integration%20of%20data%20from%20multiple%20cameras%2C%20while%20the%0Amulti-keyframes%20and%20parallel%20SLAM%20threads%20ensure%20efficient%20and%20accurate%20pose%0Aestimation%20and%20mapping.%20Extensive%20experiments%20in%20various%20environments%0Ademonstrate%20the%20superior%20accuracy%20and%20robustness%20of%20the%20proposed%20method%0Acompared%20to%20conventional%20single-camera%20SLAM%20systems.%20The%20results%20highlight%20the%0Apotential%20of%20the%20proposed%20Multicam-SLAM%20for%20more%20complex%20and%20challenging%0Aapplications.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/AlterPang/Multi_ORB_SLAM%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06374v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulticam-SLAM%253A%2520Non-overlapping%2520Multi-camera%2520SLAM%2520for%2520Indirect%2520Visual%250A%2520%2520Localization%2520and%2520Navigation%26entry.906535625%3DShenghao%2520Li%2520and%2520Luchao%2520Pang%2520and%2520Xianglong%2520Hu%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520to%2520visual%2520simultaneous%2520localization%2520and%250Amapping%2520%2528SLAM%2529%2520using%2520multiple%2520RGB-D%2520cameras.%2520The%2520proposed%2520method%252C%250AMulticam-SLAM%252C%2520significantly%2520enhances%2520the%2520robustness%2520and%2520accuracy%2520of%2520SLAM%250Asystems%2520by%2520capturing%2520more%2520comprehensive%2520spatial%2520information%2520from%2520various%250Aperspectives.%2520This%2520method%2520enables%2520the%2520accurate%2520determination%2520of%2520pose%250Arelationships%2520among%2520multiple%2520cameras%2520without%2520the%2520need%2520for%2520overlapping%2520fields%2520of%250Aview.%2520The%2520proposed%2520Muticam-SLAM%2520includes%2520a%2520unique%2520multi-camera%2520model%252C%2520a%250Amulti-keyframes%2520structure%252C%2520and%2520several%2520parallel%2520SLAM%2520threads.%2520The%2520multi-camera%250Amodel%2520allows%2520for%2520the%2520integration%2520of%2520data%2520from%2520multiple%2520cameras%252C%2520while%2520the%250Amulti-keyframes%2520and%2520parallel%2520SLAM%2520threads%2520ensure%2520efficient%2520and%2520accurate%2520pose%250Aestimation%2520and%2520mapping.%2520Extensive%2520experiments%2520in%2520various%2520environments%250Ademonstrate%2520the%2520superior%2520accuracy%2520and%2520robustness%2520of%2520the%2520proposed%2520method%250Acompared%2520to%2520conventional%2520single-camera%2520SLAM%2520systems.%2520The%2520results%2520highlight%2520the%250Apotential%2520of%2520the%2520proposed%2520Multicam-SLAM%2520for%2520more%2520complex%2520and%2520challenging%250Aapplications.%2520Code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/AlterPang/Multi_ORB_SLAM%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06374v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multicam-SLAM%3A%20Non-overlapping%20Multi-camera%20SLAM%20for%20Indirect%20Visual%0A%20%20Localization%20and%20Navigation&entry.906535625=Shenghao%20Li%20and%20Luchao%20Pang%20and%20Xianglong%20Hu&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20approach%20to%20visual%20simultaneous%20localization%20and%0Amapping%20%28SLAM%29%20using%20multiple%20RGB-D%20cameras.%20The%20proposed%20method%2C%0AMulticam-SLAM%2C%20significantly%20enhances%20the%20robustness%20and%20accuracy%20of%20SLAM%0Asystems%20by%20capturing%20more%20comprehensive%20spatial%20information%20from%20various%0Aperspectives.%20This%20method%20enables%20the%20accurate%20determination%20of%20pose%0Arelationships%20among%20multiple%20cameras%20without%20the%20need%20for%20overlapping%20fields%20of%0Aview.%20The%20proposed%20Muticam-SLAM%20includes%20a%20unique%20multi-camera%20model%2C%20a%0Amulti-keyframes%20structure%2C%20and%20several%20parallel%20SLAM%20threads.%20The%20multi-camera%0Amodel%20allows%20for%20the%20integration%20of%20data%20from%20multiple%20cameras%2C%20while%20the%0Amulti-keyframes%20and%20parallel%20SLAM%20threads%20ensure%20efficient%20and%20accurate%20pose%0Aestimation%20and%20mapping.%20Extensive%20experiments%20in%20various%20environments%0Ademonstrate%20the%20superior%20accuracy%20and%20robustness%20of%20the%20proposed%20method%0Acompared%20to%20conventional%20single-camera%20SLAM%20systems.%20The%20results%20highlight%20the%0Apotential%20of%20the%20proposed%20Multicam-SLAM%20for%20more%20complex%20and%20challenging%0Aapplications.%20Code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/AlterPang/Multi_ORB_SLAM%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06374v2&entry.124074799=Read"},
{"title": "Learning Interpretable Fair Representations", "author": "Tianhao Wang and Zana Bu\u00e7inca and Zilin Ma", "abstract": "  Numerous approaches have been recently proposed for learning fair\nrepresentations that mitigate unfair outcomes in prediction tasks. A key\nmotivation for these methods is that the representations can be used by third\nparties with unknown objectives. However, because current fair representations\nare generally not interpretable, the third party cannot use these fair\nrepresentations for exploration, or to obtain any additional insights, besides\nthe pre-contracted prediction tasks. Thus, to increase data utility beyond\nprediction tasks, we argue that the representations need to be fair, yet\ninterpretable. We propose a general framework for learning interpretable fair\nrepresentations by introducing an interpretable \"prior knowledge\" during the\nrepresentation learning process. We implement this idea and conduct experiments\nwith ColorMNIST and Dsprite datasets. The results indicate that in addition to\nbeing interpretable, our representations attain slightly higher accuracy and\nfairer outcomes in a downstream classification task compared to\nstate-of-the-art fair representations.\n", "link": "http://arxiv.org/abs/2406.16698v1", "date": "2024-06-24", "relevancy": 2.611, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5254}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5225}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Interpretable%20Fair%20Representations&body=Title%3A%20Learning%20Interpretable%20Fair%20Representations%0AAuthor%3A%20Tianhao%20Wang%20and%20Zana%20Bu%C3%A7inca%20and%20Zilin%20Ma%0AAbstract%3A%20%20%20Numerous%20approaches%20have%20been%20recently%20proposed%20for%20learning%20fair%0Arepresentations%20that%20mitigate%20unfair%20outcomes%20in%20prediction%20tasks.%20A%20key%0Amotivation%20for%20these%20methods%20is%20that%20the%20representations%20can%20be%20used%20by%20third%0Aparties%20with%20unknown%20objectives.%20However%2C%20because%20current%20fair%20representations%0Aare%20generally%20not%20interpretable%2C%20the%20third%20party%20cannot%20use%20these%20fair%0Arepresentations%20for%20exploration%2C%20or%20to%20obtain%20any%20additional%20insights%2C%20besides%0Athe%20pre-contracted%20prediction%20tasks.%20Thus%2C%20to%20increase%20data%20utility%20beyond%0Aprediction%20tasks%2C%20we%20argue%20that%20the%20representations%20need%20to%20be%20fair%2C%20yet%0Ainterpretable.%20We%20propose%20a%20general%20framework%20for%20learning%20interpretable%20fair%0Arepresentations%20by%20introducing%20an%20interpretable%20%22prior%20knowledge%22%20during%20the%0Arepresentation%20learning%20process.%20We%20implement%20this%20idea%20and%20conduct%20experiments%0Awith%20ColorMNIST%20and%20Dsprite%20datasets.%20The%20results%20indicate%20that%20in%20addition%20to%0Abeing%20interpretable%2C%20our%20representations%20attain%20slightly%20higher%20accuracy%20and%0Afairer%20outcomes%20in%20a%20downstream%20classification%20task%20compared%20to%0Astate-of-the-art%20fair%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Interpretable%2520Fair%2520Representations%26entry.906535625%3DTianhao%2520Wang%2520and%2520Zana%2520Bu%25C3%25A7inca%2520and%2520Zilin%2520Ma%26entry.1292438233%3D%2520%2520Numerous%2520approaches%2520have%2520been%2520recently%2520proposed%2520for%2520learning%2520fair%250Arepresentations%2520that%2520mitigate%2520unfair%2520outcomes%2520in%2520prediction%2520tasks.%2520A%2520key%250Amotivation%2520for%2520these%2520methods%2520is%2520that%2520the%2520representations%2520can%2520be%2520used%2520by%2520third%250Aparties%2520with%2520unknown%2520objectives.%2520However%252C%2520because%2520current%2520fair%2520representations%250Aare%2520generally%2520not%2520interpretable%252C%2520the%2520third%2520party%2520cannot%2520use%2520these%2520fair%250Arepresentations%2520for%2520exploration%252C%2520or%2520to%2520obtain%2520any%2520additional%2520insights%252C%2520besides%250Athe%2520pre-contracted%2520prediction%2520tasks.%2520Thus%252C%2520to%2520increase%2520data%2520utility%2520beyond%250Aprediction%2520tasks%252C%2520we%2520argue%2520that%2520the%2520representations%2520need%2520to%2520be%2520fair%252C%2520yet%250Ainterpretable.%2520We%2520propose%2520a%2520general%2520framework%2520for%2520learning%2520interpretable%2520fair%250Arepresentations%2520by%2520introducing%2520an%2520interpretable%2520%2522prior%2520knowledge%2522%2520during%2520the%250Arepresentation%2520learning%2520process.%2520We%2520implement%2520this%2520idea%2520and%2520conduct%2520experiments%250Awith%2520ColorMNIST%2520and%2520Dsprite%2520datasets.%2520The%2520results%2520indicate%2520that%2520in%2520addition%2520to%250Abeing%2520interpretable%252C%2520our%2520representations%2520attain%2520slightly%2520higher%2520accuracy%2520and%250Afairer%2520outcomes%2520in%2520a%2520downstream%2520classification%2520task%2520compared%2520to%250Astate-of-the-art%2520fair%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Interpretable%20Fair%20Representations&entry.906535625=Tianhao%20Wang%20and%20Zana%20Bu%C3%A7inca%20and%20Zilin%20Ma&entry.1292438233=%20%20Numerous%20approaches%20have%20been%20recently%20proposed%20for%20learning%20fair%0Arepresentations%20that%20mitigate%20unfair%20outcomes%20in%20prediction%20tasks.%20A%20key%0Amotivation%20for%20these%20methods%20is%20that%20the%20representations%20can%20be%20used%20by%20third%0Aparties%20with%20unknown%20objectives.%20However%2C%20because%20current%20fair%20representations%0Aare%20generally%20not%20interpretable%2C%20the%20third%20party%20cannot%20use%20these%20fair%0Arepresentations%20for%20exploration%2C%20or%20to%20obtain%20any%20additional%20insights%2C%20besides%0Athe%20pre-contracted%20prediction%20tasks.%20Thus%2C%20to%20increase%20data%20utility%20beyond%0Aprediction%20tasks%2C%20we%20argue%20that%20the%20representations%20need%20to%20be%20fair%2C%20yet%0Ainterpretable.%20We%20propose%20a%20general%20framework%20for%20learning%20interpretable%20fair%0Arepresentations%20by%20introducing%20an%20interpretable%20%22prior%20knowledge%22%20during%20the%0Arepresentation%20learning%20process.%20We%20implement%20this%20idea%20and%20conduct%20experiments%0Awith%20ColorMNIST%20and%20Dsprite%20datasets.%20The%20results%20indicate%20that%20in%20addition%20to%0Abeing%20interpretable%2C%20our%20representations%20attain%20slightly%20higher%20accuracy%20and%0Afairer%20outcomes%20in%20a%20downstream%20classification%20task%20compared%20to%0Astate-of-the-art%20fair%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16698v1&entry.124074799=Read"},
{"title": "The Championship-Winning Solution for the 5th CLVISION Challenge 2024", "author": "Sishun Pan and Tingmin Li and Yang Yang", "abstract": "  In this paper, we introduce our approach to the 5th CLVision Challenge, which\npresents distinctive challenges beyond traditional class incremental learning.\nUnlike standard settings, this competition features the recurrence of\npreviously encountered classes and includes unlabeled data that may contain\nOut-of-Distribution (OOD) categories. Our approach is based on Winning\nSubnetworks to allocate independent parameter spaces for each task addressing\nthe catastrophic forgetting problem in class incremental learning and employ\nthree training strategies: supervised classification learning, unsupervised\ncontrastive learning, and pseudo-label classification learning to fully utilize\nthe information in both labeled and unlabeled data, enhancing the\nclassification performance of each subnetwork. Furthermore, during the\ninference stage, we have devised an interaction strategy between subnetworks,\nwhere the prediction for a specific class of a particular sample is the average\nlogits across different subnetworks corresponding to that class, leveraging the\nknowledge learned from different subnetworks on recurring classes to improve\nclassification accuracy. These strategies can be simultaneously applied to the\nthree scenarios of the competition, effectively solving the difficulties in the\ncompetition scenarios. Experimentally, our method ranks first in both the\npre-selection and final evaluation stages, with an average accuracy of 0.4535\nduring the preselection stage and an average accuracy of 0.4805 during the\nfinal evaluation stage.\n", "link": "http://arxiv.org/abs/2406.16615v1", "date": "2024-06-24", "relevancy": 2.5962, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5282}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.517}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Championship-Winning%20Solution%20for%20the%205th%20CLVISION%20Challenge%202024&body=Title%3A%20The%20Championship-Winning%20Solution%20for%20the%205th%20CLVISION%20Challenge%202024%0AAuthor%3A%20Sishun%20Pan%20and%20Tingmin%20Li%20and%20Yang%20Yang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20our%20approach%20to%20the%205th%20CLVision%20Challenge%2C%20which%0Apresents%20distinctive%20challenges%20beyond%20traditional%20class%20incremental%20learning.%0AUnlike%20standard%20settings%2C%20this%20competition%20features%20the%20recurrence%20of%0Apreviously%20encountered%20classes%20and%20includes%20unlabeled%20data%20that%20may%20contain%0AOut-of-Distribution%20%28OOD%29%20categories.%20Our%20approach%20is%20based%20on%20Winning%0ASubnetworks%20to%20allocate%20independent%20parameter%20spaces%20for%20each%20task%20addressing%0Athe%20catastrophic%20forgetting%20problem%20in%20class%20incremental%20learning%20and%20employ%0Athree%20training%20strategies%3A%20supervised%20classification%20learning%2C%20unsupervised%0Acontrastive%20learning%2C%20and%20pseudo-label%20classification%20learning%20to%20fully%20utilize%0Athe%20information%20in%20both%20labeled%20and%20unlabeled%20data%2C%20enhancing%20the%0Aclassification%20performance%20of%20each%20subnetwork.%20Furthermore%2C%20during%20the%0Ainference%20stage%2C%20we%20have%20devised%20an%20interaction%20strategy%20between%20subnetworks%2C%0Awhere%20the%20prediction%20for%20a%20specific%20class%20of%20a%20particular%20sample%20is%20the%20average%0Alogits%20across%20different%20subnetworks%20corresponding%20to%20that%20class%2C%20leveraging%20the%0Aknowledge%20learned%20from%20different%20subnetworks%20on%20recurring%20classes%20to%20improve%0Aclassification%20accuracy.%20These%20strategies%20can%20be%20simultaneously%20applied%20to%20the%0Athree%20scenarios%20of%20the%20competition%2C%20effectively%20solving%20the%20difficulties%20in%20the%0Acompetition%20scenarios.%20Experimentally%2C%20our%20method%20ranks%20first%20in%20both%20the%0Apre-selection%20and%20final%20evaluation%20stages%2C%20with%20an%20average%20accuracy%20of%200.4535%0Aduring%20the%20preselection%20stage%20and%20an%20average%20accuracy%20of%200.4805%20during%20the%0Afinal%20evaluation%20stage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16615v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Championship-Winning%2520Solution%2520for%2520the%25205th%2520CLVISION%2520Challenge%25202024%26entry.906535625%3DSishun%2520Pan%2520and%2520Tingmin%2520Li%2520and%2520Yang%2520Yang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520our%2520approach%2520to%2520the%25205th%2520CLVision%2520Challenge%252C%2520which%250Apresents%2520distinctive%2520challenges%2520beyond%2520traditional%2520class%2520incremental%2520learning.%250AUnlike%2520standard%2520settings%252C%2520this%2520competition%2520features%2520the%2520recurrence%2520of%250Apreviously%2520encountered%2520classes%2520and%2520includes%2520unlabeled%2520data%2520that%2520may%2520contain%250AOut-of-Distribution%2520%2528OOD%2529%2520categories.%2520Our%2520approach%2520is%2520based%2520on%2520Winning%250ASubnetworks%2520to%2520allocate%2520independent%2520parameter%2520spaces%2520for%2520each%2520task%2520addressing%250Athe%2520catastrophic%2520forgetting%2520problem%2520in%2520class%2520incremental%2520learning%2520and%2520employ%250Athree%2520training%2520strategies%253A%2520supervised%2520classification%2520learning%252C%2520unsupervised%250Acontrastive%2520learning%252C%2520and%2520pseudo-label%2520classification%2520learning%2520to%2520fully%2520utilize%250Athe%2520information%2520in%2520both%2520labeled%2520and%2520unlabeled%2520data%252C%2520enhancing%2520the%250Aclassification%2520performance%2520of%2520each%2520subnetwork.%2520Furthermore%252C%2520during%2520the%250Ainference%2520stage%252C%2520we%2520have%2520devised%2520an%2520interaction%2520strategy%2520between%2520subnetworks%252C%250Awhere%2520the%2520prediction%2520for%2520a%2520specific%2520class%2520of%2520a%2520particular%2520sample%2520is%2520the%2520average%250Alogits%2520across%2520different%2520subnetworks%2520corresponding%2520to%2520that%2520class%252C%2520leveraging%2520the%250Aknowledge%2520learned%2520from%2520different%2520subnetworks%2520on%2520recurring%2520classes%2520to%2520improve%250Aclassification%2520accuracy.%2520These%2520strategies%2520can%2520be%2520simultaneously%2520applied%2520to%2520the%250Athree%2520scenarios%2520of%2520the%2520competition%252C%2520effectively%2520solving%2520the%2520difficulties%2520in%2520the%250Acompetition%2520scenarios.%2520Experimentally%252C%2520our%2520method%2520ranks%2520first%2520in%2520both%2520the%250Apre-selection%2520and%2520final%2520evaluation%2520stages%252C%2520with%2520an%2520average%2520accuracy%2520of%25200.4535%250Aduring%2520the%2520preselection%2520stage%2520and%2520an%2520average%2520accuracy%2520of%25200.4805%2520during%2520the%250Afinal%2520evaluation%2520stage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16615v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Championship-Winning%20Solution%20for%20the%205th%20CLVISION%20Challenge%202024&entry.906535625=Sishun%20Pan%20and%20Tingmin%20Li%20and%20Yang%20Yang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20our%20approach%20to%20the%205th%20CLVision%20Challenge%2C%20which%0Apresents%20distinctive%20challenges%20beyond%20traditional%20class%20incremental%20learning.%0AUnlike%20standard%20settings%2C%20this%20competition%20features%20the%20recurrence%20of%0Apreviously%20encountered%20classes%20and%20includes%20unlabeled%20data%20that%20may%20contain%0AOut-of-Distribution%20%28OOD%29%20categories.%20Our%20approach%20is%20based%20on%20Winning%0ASubnetworks%20to%20allocate%20independent%20parameter%20spaces%20for%20each%20task%20addressing%0Athe%20catastrophic%20forgetting%20problem%20in%20class%20incremental%20learning%20and%20employ%0Athree%20training%20strategies%3A%20supervised%20classification%20learning%2C%20unsupervised%0Acontrastive%20learning%2C%20and%20pseudo-label%20classification%20learning%20to%20fully%20utilize%0Athe%20information%20in%20both%20labeled%20and%20unlabeled%20data%2C%20enhancing%20the%0Aclassification%20performance%20of%20each%20subnetwork.%20Furthermore%2C%20during%20the%0Ainference%20stage%2C%20we%20have%20devised%20an%20interaction%20strategy%20between%20subnetworks%2C%0Awhere%20the%20prediction%20for%20a%20specific%20class%20of%20a%20particular%20sample%20is%20the%20average%0Alogits%20across%20different%20subnetworks%20corresponding%20to%20that%20class%2C%20leveraging%20the%0Aknowledge%20learned%20from%20different%20subnetworks%20on%20recurring%20classes%20to%20improve%0Aclassification%20accuracy.%20These%20strategies%20can%20be%20simultaneously%20applied%20to%20the%0Athree%20scenarios%20of%20the%20competition%2C%20effectively%20solving%20the%20difficulties%20in%20the%0Acompetition%20scenarios.%20Experimentally%2C%20our%20method%20ranks%20first%20in%20both%20the%0Apre-selection%20and%20final%20evaluation%20stages%2C%20with%20an%20average%20accuracy%20of%200.4535%0Aduring%20the%20preselection%20stage%20and%20an%20average%20accuracy%20of%200.4805%20during%20the%0Afinal%20evaluation%20stage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16615v1&entry.124074799=Read"},
{"title": "Single-image camera calibration with model-free distortion correction", "author": "Katia Genovese", "abstract": "  Camera calibration is a process of paramount importance in computer vision\napplications that require accurate quantitative measurements. The popular\nmethod developed by Zhang relies on the use of a large number of images of a\nplanar grid of fiducial points captured in multiple poses. Although flexible\nand easy to implement, Zhang's method has some limitations. The simultaneous\noptimization of the entire parameter set, including the coefficients of a\npredefined distortion model, may result in poor distortion correction at the\nimage boundaries or in miscalculation of the intrinsic parameters, even with a\nreasonably small reprojection error. Indeed, applications involving image\nstitching (e.g. multi-camera systems) require accurate mapping of distortion up\nto the outermost regions of the image. Moreover, intrinsic parameters affect\nthe accuracy of camera pose estimation, which is fundamental for applications\nsuch as vision servoing in robot navigation and automated assembly. This paper\nproposes a method for estimating the complete set of calibration parameters\nfrom a single image of a planar speckle pattern covering the entire sensor. The\ncorrespondence between image points and physical points on the calibration\ntarget is obtained using Digital Image Correlation. The effective focal length\nand the extrinsic parameters are calculated separately after a prior evaluation\nof the principal point. At the end of the procedure, a dense and uniform\nmodel-free distortion map is obtained over the entire image. Synthetic data\nwith different noise levels were used to test the feasibility of the proposed\nmethod and to compare its metrological performance with Zhang's method.\nReal-world tests demonstrate the potential of the developed method to reveal\naspects of the image formation that are hidden by averaging over multiple\nimages.\n", "link": "http://arxiv.org/abs/2403.01263v2", "date": "2024-06-24", "relevancy": 2.5952, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5281}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5206}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-image%20camera%20calibration%20with%20model-free%20distortion%20correction&body=Title%3A%20Single-image%20camera%20calibration%20with%20model-free%20distortion%20correction%0AAuthor%3A%20Katia%20Genovese%0AAbstract%3A%20%20%20Camera%20calibration%20is%20a%20process%20of%20paramount%20importance%20in%20computer%20vision%0Aapplications%20that%20require%20accurate%20quantitative%20measurements.%20The%20popular%0Amethod%20developed%20by%20Zhang%20relies%20on%20the%20use%20of%20a%20large%20number%20of%20images%20of%20a%0Aplanar%20grid%20of%20fiducial%20points%20captured%20in%20multiple%20poses.%20Although%20flexible%0Aand%20easy%20to%20implement%2C%20Zhang%27s%20method%20has%20some%20limitations.%20The%20simultaneous%0Aoptimization%20of%20the%20entire%20parameter%20set%2C%20including%20the%20coefficients%20of%20a%0Apredefined%20distortion%20model%2C%20may%20result%20in%20poor%20distortion%20correction%20at%20the%0Aimage%20boundaries%20or%20in%20miscalculation%20of%20the%20intrinsic%20parameters%2C%20even%20with%20a%0Areasonably%20small%20reprojection%20error.%20Indeed%2C%20applications%20involving%20image%0Astitching%20%28e.g.%20multi-camera%20systems%29%20require%20accurate%20mapping%20of%20distortion%20up%0Ato%20the%20outermost%20regions%20of%20the%20image.%20Moreover%2C%20intrinsic%20parameters%20affect%0Athe%20accuracy%20of%20camera%20pose%20estimation%2C%20which%20is%20fundamental%20for%20applications%0Asuch%20as%20vision%20servoing%20in%20robot%20navigation%20and%20automated%20assembly.%20This%20paper%0Aproposes%20a%20method%20for%20estimating%20the%20complete%20set%20of%20calibration%20parameters%0Afrom%20a%20single%20image%20of%20a%20planar%20speckle%20pattern%20covering%20the%20entire%20sensor.%20The%0Acorrespondence%20between%20image%20points%20and%20physical%20points%20on%20the%20calibration%0Atarget%20is%20obtained%20using%20Digital%20Image%20Correlation.%20The%20effective%20focal%20length%0Aand%20the%20extrinsic%20parameters%20are%20calculated%20separately%20after%20a%20prior%20evaluation%0Aof%20the%20principal%20point.%20At%20the%20end%20of%20the%20procedure%2C%20a%20dense%20and%20uniform%0Amodel-free%20distortion%20map%20is%20obtained%20over%20the%20entire%20image.%20Synthetic%20data%0Awith%20different%20noise%20levels%20were%20used%20to%20test%20the%20feasibility%20of%20the%20proposed%0Amethod%20and%20to%20compare%20its%20metrological%20performance%20with%20Zhang%27s%20method.%0AReal-world%20tests%20demonstrate%20the%20potential%20of%20the%20developed%20method%20to%20reveal%0Aaspects%20of%20the%20image%20formation%20that%20are%20hidden%20by%20averaging%20over%20multiple%0Aimages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01263v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-image%2520camera%2520calibration%2520with%2520model-free%2520distortion%2520correction%26entry.906535625%3DKatia%2520Genovese%26entry.1292438233%3D%2520%2520Camera%2520calibration%2520is%2520a%2520process%2520of%2520paramount%2520importance%2520in%2520computer%2520vision%250Aapplications%2520that%2520require%2520accurate%2520quantitative%2520measurements.%2520The%2520popular%250Amethod%2520developed%2520by%2520Zhang%2520relies%2520on%2520the%2520use%2520of%2520a%2520large%2520number%2520of%2520images%2520of%2520a%250Aplanar%2520grid%2520of%2520fiducial%2520points%2520captured%2520in%2520multiple%2520poses.%2520Although%2520flexible%250Aand%2520easy%2520to%2520implement%252C%2520Zhang%2527s%2520method%2520has%2520some%2520limitations.%2520The%2520simultaneous%250Aoptimization%2520of%2520the%2520entire%2520parameter%2520set%252C%2520including%2520the%2520coefficients%2520of%2520a%250Apredefined%2520distortion%2520model%252C%2520may%2520result%2520in%2520poor%2520distortion%2520correction%2520at%2520the%250Aimage%2520boundaries%2520or%2520in%2520miscalculation%2520of%2520the%2520intrinsic%2520parameters%252C%2520even%2520with%2520a%250Areasonably%2520small%2520reprojection%2520error.%2520Indeed%252C%2520applications%2520involving%2520image%250Astitching%2520%2528e.g.%2520multi-camera%2520systems%2529%2520require%2520accurate%2520mapping%2520of%2520distortion%2520up%250Ato%2520the%2520outermost%2520regions%2520of%2520the%2520image.%2520Moreover%252C%2520intrinsic%2520parameters%2520affect%250Athe%2520accuracy%2520of%2520camera%2520pose%2520estimation%252C%2520which%2520is%2520fundamental%2520for%2520applications%250Asuch%2520as%2520vision%2520servoing%2520in%2520robot%2520navigation%2520and%2520automated%2520assembly.%2520This%2520paper%250Aproposes%2520a%2520method%2520for%2520estimating%2520the%2520complete%2520set%2520of%2520calibration%2520parameters%250Afrom%2520a%2520single%2520image%2520of%2520a%2520planar%2520speckle%2520pattern%2520covering%2520the%2520entire%2520sensor.%2520The%250Acorrespondence%2520between%2520image%2520points%2520and%2520physical%2520points%2520on%2520the%2520calibration%250Atarget%2520is%2520obtained%2520using%2520Digital%2520Image%2520Correlation.%2520The%2520effective%2520focal%2520length%250Aand%2520the%2520extrinsic%2520parameters%2520are%2520calculated%2520separately%2520after%2520a%2520prior%2520evaluation%250Aof%2520the%2520principal%2520point.%2520At%2520the%2520end%2520of%2520the%2520procedure%252C%2520a%2520dense%2520and%2520uniform%250Amodel-free%2520distortion%2520map%2520is%2520obtained%2520over%2520the%2520entire%2520image.%2520Synthetic%2520data%250Awith%2520different%2520noise%2520levels%2520were%2520used%2520to%2520test%2520the%2520feasibility%2520of%2520the%2520proposed%250Amethod%2520and%2520to%2520compare%2520its%2520metrological%2520performance%2520with%2520Zhang%2527s%2520method.%250AReal-world%2520tests%2520demonstrate%2520the%2520potential%2520of%2520the%2520developed%2520method%2520to%2520reveal%250Aaspects%2520of%2520the%2520image%2520formation%2520that%2520are%2520hidden%2520by%2520averaging%2520over%2520multiple%250Aimages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01263v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-image%20camera%20calibration%20with%20model-free%20distortion%20correction&entry.906535625=Katia%20Genovese&entry.1292438233=%20%20Camera%20calibration%20is%20a%20process%20of%20paramount%20importance%20in%20computer%20vision%0Aapplications%20that%20require%20accurate%20quantitative%20measurements.%20The%20popular%0Amethod%20developed%20by%20Zhang%20relies%20on%20the%20use%20of%20a%20large%20number%20of%20images%20of%20a%0Aplanar%20grid%20of%20fiducial%20points%20captured%20in%20multiple%20poses.%20Although%20flexible%0Aand%20easy%20to%20implement%2C%20Zhang%27s%20method%20has%20some%20limitations.%20The%20simultaneous%0Aoptimization%20of%20the%20entire%20parameter%20set%2C%20including%20the%20coefficients%20of%20a%0Apredefined%20distortion%20model%2C%20may%20result%20in%20poor%20distortion%20correction%20at%20the%0Aimage%20boundaries%20or%20in%20miscalculation%20of%20the%20intrinsic%20parameters%2C%20even%20with%20a%0Areasonably%20small%20reprojection%20error.%20Indeed%2C%20applications%20involving%20image%0Astitching%20%28e.g.%20multi-camera%20systems%29%20require%20accurate%20mapping%20of%20distortion%20up%0Ato%20the%20outermost%20regions%20of%20the%20image.%20Moreover%2C%20intrinsic%20parameters%20affect%0Athe%20accuracy%20of%20camera%20pose%20estimation%2C%20which%20is%20fundamental%20for%20applications%0Asuch%20as%20vision%20servoing%20in%20robot%20navigation%20and%20automated%20assembly.%20This%20paper%0Aproposes%20a%20method%20for%20estimating%20the%20complete%20set%20of%20calibration%20parameters%0Afrom%20a%20single%20image%20of%20a%20planar%20speckle%20pattern%20covering%20the%20entire%20sensor.%20The%0Acorrespondence%20between%20image%20points%20and%20physical%20points%20on%20the%20calibration%0Atarget%20is%20obtained%20using%20Digital%20Image%20Correlation.%20The%20effective%20focal%20length%0Aand%20the%20extrinsic%20parameters%20are%20calculated%20separately%20after%20a%20prior%20evaluation%0Aof%20the%20principal%20point.%20At%20the%20end%20of%20the%20procedure%2C%20a%20dense%20and%20uniform%0Amodel-free%20distortion%20map%20is%20obtained%20over%20the%20entire%20image.%20Synthetic%20data%0Awith%20different%20noise%20levels%20were%20used%20to%20test%20the%20feasibility%20of%20the%20proposed%0Amethod%20and%20to%20compare%20its%20metrological%20performance%20with%20Zhang%27s%20method.%0AReal-world%20tests%20demonstrate%20the%20potential%20of%20the%20developed%20method%20to%20reveal%0Aaspects%20of%20the%20image%20formation%20that%20are%20hidden%20by%20averaging%20over%20multiple%0Aimages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01263v2&entry.124074799=Read"},
{"title": "NeST: Neural Stress Tensor Tomography by leveraging 3D Photoelasticity", "author": "Akshat Dave and Tianyi Zhang and Aaron Young and Ramesh Raskar and Wolfgang Heidrich and Ashok Veeraraghavan", "abstract": "  Photoelasticity enables full-field stress analysis in transparent objects\nthrough stress-induced birefringence. Existing techniques are limited to 2D\nslices and require destructively slicing the object. Recovering the internal 3D\nstress distribution of the entire object is challenging as it involves solving\na tensor tomography problem and handling phase wrapping ambiguities. We\nintroduce NeST, an analysis-by-synthesis approach for reconstructing 3D stress\ntensor fields as neural implicit representations from polarization\nmeasurements. Our key insight is to jointly handle phase unwrapping and tensor\ntomography using a differentiable forward model based on Jones calculus. Our\nnon-linear model faithfully matches real captures, unlike prior linear\napproximations. We develop an experimental multi-axis polariscope setup to\ncapture 3D photoelasticity and experimentally demonstrate that NeST\nreconstructs the internal stress distribution for objects with varying shape\nand force conditions. Additionally, we showcase novel applications in stress\nanalysis, such as visualizing photoelastic fringes by virtually slicing the\nobject and viewing photoelastic fringes from unseen viewpoints. NeST paves the\nway for scalable non-destructive 3D photoelastic analysis.\n", "link": "http://arxiv.org/abs/2406.10212v2", "date": "2024-06-24", "relevancy": 2.5646, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5177}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5177}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeST%3A%20Neural%20Stress%20Tensor%20Tomography%20by%20leveraging%203D%20Photoelasticity&body=Title%3A%20NeST%3A%20Neural%20Stress%20Tensor%20Tomography%20by%20leveraging%203D%20Photoelasticity%0AAuthor%3A%20Akshat%20Dave%20and%20Tianyi%20Zhang%20and%20Aaron%20Young%20and%20Ramesh%20Raskar%20and%20Wolfgang%20Heidrich%20and%20Ashok%20Veeraraghavan%0AAbstract%3A%20%20%20Photoelasticity%20enables%20full-field%20stress%20analysis%20in%20transparent%20objects%0Athrough%20stress-induced%20birefringence.%20Existing%20techniques%20are%20limited%20to%202D%0Aslices%20and%20require%20destructively%20slicing%20the%20object.%20Recovering%20the%20internal%203D%0Astress%20distribution%20of%20the%20entire%20object%20is%20challenging%20as%20it%20involves%20solving%0Aa%20tensor%20tomography%20problem%20and%20handling%20phase%20wrapping%20ambiguities.%20We%0Aintroduce%20NeST%2C%20an%20analysis-by-synthesis%20approach%20for%20reconstructing%203D%20stress%0Atensor%20fields%20as%20neural%20implicit%20representations%20from%20polarization%0Ameasurements.%20Our%20key%20insight%20is%20to%20jointly%20handle%20phase%20unwrapping%20and%20tensor%0Atomography%20using%20a%20differentiable%20forward%20model%20based%20on%20Jones%20calculus.%20Our%0Anon-linear%20model%20faithfully%20matches%20real%20captures%2C%20unlike%20prior%20linear%0Aapproximations.%20We%20develop%20an%20experimental%20multi-axis%20polariscope%20setup%20to%0Acapture%203D%20photoelasticity%20and%20experimentally%20demonstrate%20that%20NeST%0Areconstructs%20the%20internal%20stress%20distribution%20for%20objects%20with%20varying%20shape%0Aand%20force%20conditions.%20Additionally%2C%20we%20showcase%20novel%20applications%20in%20stress%0Aanalysis%2C%20such%20as%20visualizing%20photoelastic%20fringes%20by%20virtually%20slicing%20the%0Aobject%20and%20viewing%20photoelastic%20fringes%20from%20unseen%20viewpoints.%20NeST%20paves%20the%0Away%20for%20scalable%20non-destructive%203D%20photoelastic%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10212v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeST%253A%2520Neural%2520Stress%2520Tensor%2520Tomography%2520by%2520leveraging%25203D%2520Photoelasticity%26entry.906535625%3DAkshat%2520Dave%2520and%2520Tianyi%2520Zhang%2520and%2520Aaron%2520Young%2520and%2520Ramesh%2520Raskar%2520and%2520Wolfgang%2520Heidrich%2520and%2520Ashok%2520Veeraraghavan%26entry.1292438233%3D%2520%2520Photoelasticity%2520enables%2520full-field%2520stress%2520analysis%2520in%2520transparent%2520objects%250Athrough%2520stress-induced%2520birefringence.%2520Existing%2520techniques%2520are%2520limited%2520to%25202D%250Aslices%2520and%2520require%2520destructively%2520slicing%2520the%2520object.%2520Recovering%2520the%2520internal%25203D%250Astress%2520distribution%2520of%2520the%2520entire%2520object%2520is%2520challenging%2520as%2520it%2520involves%2520solving%250Aa%2520tensor%2520tomography%2520problem%2520and%2520handling%2520phase%2520wrapping%2520ambiguities.%2520We%250Aintroduce%2520NeST%252C%2520an%2520analysis-by-synthesis%2520approach%2520for%2520reconstructing%25203D%2520stress%250Atensor%2520fields%2520as%2520neural%2520implicit%2520representations%2520from%2520polarization%250Ameasurements.%2520Our%2520key%2520insight%2520is%2520to%2520jointly%2520handle%2520phase%2520unwrapping%2520and%2520tensor%250Atomography%2520using%2520a%2520differentiable%2520forward%2520model%2520based%2520on%2520Jones%2520calculus.%2520Our%250Anon-linear%2520model%2520faithfully%2520matches%2520real%2520captures%252C%2520unlike%2520prior%2520linear%250Aapproximations.%2520We%2520develop%2520an%2520experimental%2520multi-axis%2520polariscope%2520setup%2520to%250Acapture%25203D%2520photoelasticity%2520and%2520experimentally%2520demonstrate%2520that%2520NeST%250Areconstructs%2520the%2520internal%2520stress%2520distribution%2520for%2520objects%2520with%2520varying%2520shape%250Aand%2520force%2520conditions.%2520Additionally%252C%2520we%2520showcase%2520novel%2520applications%2520in%2520stress%250Aanalysis%252C%2520such%2520as%2520visualizing%2520photoelastic%2520fringes%2520by%2520virtually%2520slicing%2520the%250Aobject%2520and%2520viewing%2520photoelastic%2520fringes%2520from%2520unseen%2520viewpoints.%2520NeST%2520paves%2520the%250Away%2520for%2520scalable%2520non-destructive%25203D%2520photoelastic%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10212v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeST%3A%20Neural%20Stress%20Tensor%20Tomography%20by%20leveraging%203D%20Photoelasticity&entry.906535625=Akshat%20Dave%20and%20Tianyi%20Zhang%20and%20Aaron%20Young%20and%20Ramesh%20Raskar%20and%20Wolfgang%20Heidrich%20and%20Ashok%20Veeraraghavan&entry.1292438233=%20%20Photoelasticity%20enables%20full-field%20stress%20analysis%20in%20transparent%20objects%0Athrough%20stress-induced%20birefringence.%20Existing%20techniques%20are%20limited%20to%202D%0Aslices%20and%20require%20destructively%20slicing%20the%20object.%20Recovering%20the%20internal%203D%0Astress%20distribution%20of%20the%20entire%20object%20is%20challenging%20as%20it%20involves%20solving%0Aa%20tensor%20tomography%20problem%20and%20handling%20phase%20wrapping%20ambiguities.%20We%0Aintroduce%20NeST%2C%20an%20analysis-by-synthesis%20approach%20for%20reconstructing%203D%20stress%0Atensor%20fields%20as%20neural%20implicit%20representations%20from%20polarization%0Ameasurements.%20Our%20key%20insight%20is%20to%20jointly%20handle%20phase%20unwrapping%20and%20tensor%0Atomography%20using%20a%20differentiable%20forward%20model%20based%20on%20Jones%20calculus.%20Our%0Anon-linear%20model%20faithfully%20matches%20real%20captures%2C%20unlike%20prior%20linear%0Aapproximations.%20We%20develop%20an%20experimental%20multi-axis%20polariscope%20setup%20to%0Acapture%203D%20photoelasticity%20and%20experimentally%20demonstrate%20that%20NeST%0Areconstructs%20the%20internal%20stress%20distribution%20for%20objects%20with%20varying%20shape%0Aand%20force%20conditions.%20Additionally%2C%20we%20showcase%20novel%20applications%20in%20stress%0Aanalysis%2C%20such%20as%20visualizing%20photoelastic%20fringes%20by%20virtually%20slicing%20the%0Aobject%20and%20viewing%20photoelastic%20fringes%20from%20unseen%20viewpoints.%20NeST%20paves%20the%0Away%20for%20scalable%20non-destructive%203D%20photoelastic%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10212v2&entry.124074799=Read"},
{"title": "ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs", "author": "Yuhan Li and Peisong Wang and Zhixun Li and Jeffrey Xu Yu and Jia Li", "abstract": "  With the development of foundation models such as large language models,\nzero-shot transfer learning has become increasingly significant. This is\nhighlighted by the generative capabilities of NLP models like GPT-4, and the\nretrieval-based approaches of CV models like CLIP, both of which effectively\nbridge the gap between seen and unseen data. In the realm of graph learning,\nthe continuous emergence of new graphs and the challenges of human labeling\nalso amplify the necessity for zero-shot transfer learning, driving the\nexploration of approaches that can generalize across diverse graph data without\nnecessitating dataset-specific and label-specific fine-tuning. In this study,\nwe extend such paradigms to zero-shot transferability in graphs by introducing\nZeroG, a new framework tailored to enable cross-dataset generalization.\nAddressing the inherent challenges such as feature misalignment, mismatched\nlabel spaces, and negative transfer, we leverage a language model to encode\nboth node attributes and class semantics, ensuring consistent feature\ndimensions across datasets. We also propose a prompt-based subgraph sampling\nmodule that enriches the semantic information and structure information of\nextracted subgraphs using prompting nodes and neighborhood aggregation,\nrespectively. We further adopt a lightweight fine-tuning strategy that reduces\nthe risk of overfitting and maintains the zero-shot learning efficacy of the\nlanguage model. The results underscore the effectiveness of our model in\nachieving significant cross-dataset zero-shot transferability, opening pathways\nfor the development of graph foundation models. Codes and data are available at\nhttps://github.com/NineAbyss/ZeroG.\n", "link": "http://arxiv.org/abs/2402.11235v2", "date": "2024-06-24", "relevancy": 2.5627, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5175}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5135}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZeroG%3A%20Investigating%20Cross-dataset%20Zero-shot%20Transferability%20in%20Graphs&body=Title%3A%20ZeroG%3A%20Investigating%20Cross-dataset%20Zero-shot%20Transferability%20in%20Graphs%0AAuthor%3A%20Yuhan%20Li%20and%20Peisong%20Wang%20and%20Zhixun%20Li%20and%20Jeffrey%20Xu%20Yu%20and%20Jia%20Li%0AAbstract%3A%20%20%20With%20the%20development%20of%20foundation%20models%20such%20as%20large%20language%20models%2C%0Azero-shot%20transfer%20learning%20has%20become%20increasingly%20significant.%20This%20is%0Ahighlighted%20by%20the%20generative%20capabilities%20of%20NLP%20models%20like%20GPT-4%2C%20and%20the%0Aretrieval-based%20approaches%20of%20CV%20models%20like%20CLIP%2C%20both%20of%20which%20effectively%0Abridge%20the%20gap%20between%20seen%20and%20unseen%20data.%20In%20the%20realm%20of%20graph%20learning%2C%0Athe%20continuous%20emergence%20of%20new%20graphs%20and%20the%20challenges%20of%20human%20labeling%0Aalso%20amplify%20the%20necessity%20for%20zero-shot%20transfer%20learning%2C%20driving%20the%0Aexploration%20of%20approaches%20that%20can%20generalize%20across%20diverse%20graph%20data%20without%0Anecessitating%20dataset-specific%20and%20label-specific%20fine-tuning.%20In%20this%20study%2C%0Awe%20extend%20such%20paradigms%20to%20zero-shot%20transferability%20in%20graphs%20by%20introducing%0AZeroG%2C%20a%20new%20framework%20tailored%20to%20enable%20cross-dataset%20generalization.%0AAddressing%20the%20inherent%20challenges%20such%20as%20feature%20misalignment%2C%20mismatched%0Alabel%20spaces%2C%20and%20negative%20transfer%2C%20we%20leverage%20a%20language%20model%20to%20encode%0Aboth%20node%20attributes%20and%20class%20semantics%2C%20ensuring%20consistent%20feature%0Adimensions%20across%20datasets.%20We%20also%20propose%20a%20prompt-based%20subgraph%20sampling%0Amodule%20that%20enriches%20the%20semantic%20information%20and%20structure%20information%20of%0Aextracted%20subgraphs%20using%20prompting%20nodes%20and%20neighborhood%20aggregation%2C%0Arespectively.%20We%20further%20adopt%20a%20lightweight%20fine-tuning%20strategy%20that%20reduces%0Athe%20risk%20of%20overfitting%20and%20maintains%20the%20zero-shot%20learning%20efficacy%20of%20the%0Alanguage%20model.%20The%20results%20underscore%20the%20effectiveness%20of%20our%20model%20in%0Aachieving%20significant%20cross-dataset%20zero-shot%20transferability%2C%20opening%20pathways%0Afor%20the%20development%20of%20graph%20foundation%20models.%20Codes%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/NineAbyss/ZeroG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11235v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZeroG%253A%2520Investigating%2520Cross-dataset%2520Zero-shot%2520Transferability%2520in%2520Graphs%26entry.906535625%3DYuhan%2520Li%2520and%2520Peisong%2520Wang%2520and%2520Zhixun%2520Li%2520and%2520Jeffrey%2520Xu%2520Yu%2520and%2520Jia%2520Li%26entry.1292438233%3D%2520%2520With%2520the%2520development%2520of%2520foundation%2520models%2520such%2520as%2520large%2520language%2520models%252C%250Azero-shot%2520transfer%2520learning%2520has%2520become%2520increasingly%2520significant.%2520This%2520is%250Ahighlighted%2520by%2520the%2520generative%2520capabilities%2520of%2520NLP%2520models%2520like%2520GPT-4%252C%2520and%2520the%250Aretrieval-based%2520approaches%2520of%2520CV%2520models%2520like%2520CLIP%252C%2520both%2520of%2520which%2520effectively%250Abridge%2520the%2520gap%2520between%2520seen%2520and%2520unseen%2520data.%2520In%2520the%2520realm%2520of%2520graph%2520learning%252C%250Athe%2520continuous%2520emergence%2520of%2520new%2520graphs%2520and%2520the%2520challenges%2520of%2520human%2520labeling%250Aalso%2520amplify%2520the%2520necessity%2520for%2520zero-shot%2520transfer%2520learning%252C%2520driving%2520the%250Aexploration%2520of%2520approaches%2520that%2520can%2520generalize%2520across%2520diverse%2520graph%2520data%2520without%250Anecessitating%2520dataset-specific%2520and%2520label-specific%2520fine-tuning.%2520In%2520this%2520study%252C%250Awe%2520extend%2520such%2520paradigms%2520to%2520zero-shot%2520transferability%2520in%2520graphs%2520by%2520introducing%250AZeroG%252C%2520a%2520new%2520framework%2520tailored%2520to%2520enable%2520cross-dataset%2520generalization.%250AAddressing%2520the%2520inherent%2520challenges%2520such%2520as%2520feature%2520misalignment%252C%2520mismatched%250Alabel%2520spaces%252C%2520and%2520negative%2520transfer%252C%2520we%2520leverage%2520a%2520language%2520model%2520to%2520encode%250Aboth%2520node%2520attributes%2520and%2520class%2520semantics%252C%2520ensuring%2520consistent%2520feature%250Adimensions%2520across%2520datasets.%2520We%2520also%2520propose%2520a%2520prompt-based%2520subgraph%2520sampling%250Amodule%2520that%2520enriches%2520the%2520semantic%2520information%2520and%2520structure%2520information%2520of%250Aextracted%2520subgraphs%2520using%2520prompting%2520nodes%2520and%2520neighborhood%2520aggregation%252C%250Arespectively.%2520We%2520further%2520adopt%2520a%2520lightweight%2520fine-tuning%2520strategy%2520that%2520reduces%250Athe%2520risk%2520of%2520overfitting%2520and%2520maintains%2520the%2520zero-shot%2520learning%2520efficacy%2520of%2520the%250Alanguage%2520model.%2520The%2520results%2520underscore%2520the%2520effectiveness%2520of%2520our%2520model%2520in%250Aachieving%2520significant%2520cross-dataset%2520zero-shot%2520transferability%252C%2520opening%2520pathways%250Afor%2520the%2520development%2520of%2520graph%2520foundation%2520models.%2520Codes%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/NineAbyss/ZeroG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11235v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZeroG%3A%20Investigating%20Cross-dataset%20Zero-shot%20Transferability%20in%20Graphs&entry.906535625=Yuhan%20Li%20and%20Peisong%20Wang%20and%20Zhixun%20Li%20and%20Jeffrey%20Xu%20Yu%20and%20Jia%20Li&entry.1292438233=%20%20With%20the%20development%20of%20foundation%20models%20such%20as%20large%20language%20models%2C%0Azero-shot%20transfer%20learning%20has%20become%20increasingly%20significant.%20This%20is%0Ahighlighted%20by%20the%20generative%20capabilities%20of%20NLP%20models%20like%20GPT-4%2C%20and%20the%0Aretrieval-based%20approaches%20of%20CV%20models%20like%20CLIP%2C%20both%20of%20which%20effectively%0Abridge%20the%20gap%20between%20seen%20and%20unseen%20data.%20In%20the%20realm%20of%20graph%20learning%2C%0Athe%20continuous%20emergence%20of%20new%20graphs%20and%20the%20challenges%20of%20human%20labeling%0Aalso%20amplify%20the%20necessity%20for%20zero-shot%20transfer%20learning%2C%20driving%20the%0Aexploration%20of%20approaches%20that%20can%20generalize%20across%20diverse%20graph%20data%20without%0Anecessitating%20dataset-specific%20and%20label-specific%20fine-tuning.%20In%20this%20study%2C%0Awe%20extend%20such%20paradigms%20to%20zero-shot%20transferability%20in%20graphs%20by%20introducing%0AZeroG%2C%20a%20new%20framework%20tailored%20to%20enable%20cross-dataset%20generalization.%0AAddressing%20the%20inherent%20challenges%20such%20as%20feature%20misalignment%2C%20mismatched%0Alabel%20spaces%2C%20and%20negative%20transfer%2C%20we%20leverage%20a%20language%20model%20to%20encode%0Aboth%20node%20attributes%20and%20class%20semantics%2C%20ensuring%20consistent%20feature%0Adimensions%20across%20datasets.%20We%20also%20propose%20a%20prompt-based%20subgraph%20sampling%0Amodule%20that%20enriches%20the%20semantic%20information%20and%20structure%20information%20of%0Aextracted%20subgraphs%20using%20prompting%20nodes%20and%20neighborhood%20aggregation%2C%0Arespectively.%20We%20further%20adopt%20a%20lightweight%20fine-tuning%20strategy%20that%20reduces%0Athe%20risk%20of%20overfitting%20and%20maintains%20the%20zero-shot%20learning%20efficacy%20of%20the%0Alanguage%20model.%20The%20results%20underscore%20the%20effectiveness%20of%20our%20model%20in%0Aachieving%20significant%20cross-dataset%20zero-shot%20transferability%2C%20opening%20pathways%0Afor%20the%20development%20of%20graph%20foundation%20models.%20Codes%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/NineAbyss/ZeroG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11235v2&entry.124074799=Read"},
{"title": "Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs", "author": "Ashwinee Panda and Berivan Isik and Xiangyu Qi and Sanmi Koyejo and Tsachy Weissman and Prateek Mittal", "abstract": "  Existing methods for adapting large language models (LLMs) to new tasks are\nnot suited to multi-task adaptation because they modify all the model weights\n-- causing destructive interference between tasks. The resulting effects, such\nas catastrophic forgetting of earlier tasks, make it challenging to obtain good\nperformance on multiple tasks at the same time. To mitigate this, we propose\nLottery Ticket Adaptation (LoTA), a sparse adaptation method that identifies\nand optimizes only a sparse subnetwork of the model. We evaluate LoTA on a wide\nrange of challenging tasks such as instruction following, reasoning, math, and\nsummarization. LoTA obtains better performance than full fine-tuning and\nlow-rank adaptation (LoRA), and maintains good performance even after training\non other tasks -- thus, avoiding catastrophic forgetting. By extracting and\nfine-tuning over \\emph{lottery tickets} (or \\emph{sparse task vectors}), LoTA\nalso enables model merging over highly dissimilar tasks.\n", "link": "http://arxiv.org/abs/2406.16797v1", "date": "2024-06-24", "relevancy": 2.5611, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5345}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5136}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lottery%20Ticket%20Adaptation%3A%20Mitigating%20Destructive%20Interference%20in%20LLMs&body=Title%3A%20Lottery%20Ticket%20Adaptation%3A%20Mitigating%20Destructive%20Interference%20in%20LLMs%0AAuthor%3A%20Ashwinee%20Panda%20and%20Berivan%20Isik%20and%20Xiangyu%20Qi%20and%20Sanmi%20Koyejo%20and%20Tsachy%20Weissman%20and%20Prateek%20Mittal%0AAbstract%3A%20%20%20Existing%20methods%20for%20adapting%20large%20language%20models%20%28LLMs%29%20to%20new%20tasks%20are%0Anot%20suited%20to%20multi-task%20adaptation%20because%20they%20modify%20all%20the%20model%20weights%0A--%20causing%20destructive%20interference%20between%20tasks.%20The%20resulting%20effects%2C%20such%0Aas%20catastrophic%20forgetting%20of%20earlier%20tasks%2C%20make%20it%20challenging%20to%20obtain%20good%0Aperformance%20on%20multiple%20tasks%20at%20the%20same%20time.%20To%20mitigate%20this%2C%20we%20propose%0ALottery%20Ticket%20Adaptation%20%28LoTA%29%2C%20a%20sparse%20adaptation%20method%20that%20identifies%0Aand%20optimizes%20only%20a%20sparse%20subnetwork%20of%20the%20model.%20We%20evaluate%20LoTA%20on%20a%20wide%0Arange%20of%20challenging%20tasks%20such%20as%20instruction%20following%2C%20reasoning%2C%20math%2C%20and%0Asummarization.%20LoTA%20obtains%20better%20performance%20than%20full%20fine-tuning%20and%0Alow-rank%20adaptation%20%28LoRA%29%2C%20and%20maintains%20good%20performance%20even%20after%20training%0Aon%20other%20tasks%20--%20thus%2C%20avoiding%20catastrophic%20forgetting.%20By%20extracting%20and%0Afine-tuning%20over%20%5Cemph%7Blottery%20tickets%7D%20%28or%20%5Cemph%7Bsparse%20task%20vectors%7D%29%2C%20LoTA%0Aalso%20enables%20model%20merging%20over%20highly%20dissimilar%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16797v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLottery%2520Ticket%2520Adaptation%253A%2520Mitigating%2520Destructive%2520Interference%2520in%2520LLMs%26entry.906535625%3DAshwinee%2520Panda%2520and%2520Berivan%2520Isik%2520and%2520Xiangyu%2520Qi%2520and%2520Sanmi%2520Koyejo%2520and%2520Tsachy%2520Weissman%2520and%2520Prateek%2520Mittal%26entry.1292438233%3D%2520%2520Existing%2520methods%2520for%2520adapting%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520new%2520tasks%2520are%250Anot%2520suited%2520to%2520multi-task%2520adaptation%2520because%2520they%2520modify%2520all%2520the%2520model%2520weights%250A--%2520causing%2520destructive%2520interference%2520between%2520tasks.%2520The%2520resulting%2520effects%252C%2520such%250Aas%2520catastrophic%2520forgetting%2520of%2520earlier%2520tasks%252C%2520make%2520it%2520challenging%2520to%2520obtain%2520good%250Aperformance%2520on%2520multiple%2520tasks%2520at%2520the%2520same%2520time.%2520To%2520mitigate%2520this%252C%2520we%2520propose%250ALottery%2520Ticket%2520Adaptation%2520%2528LoTA%2529%252C%2520a%2520sparse%2520adaptation%2520method%2520that%2520identifies%250Aand%2520optimizes%2520only%2520a%2520sparse%2520subnetwork%2520of%2520the%2520model.%2520We%2520evaluate%2520LoTA%2520on%2520a%2520wide%250Arange%2520of%2520challenging%2520tasks%2520such%2520as%2520instruction%2520following%252C%2520reasoning%252C%2520math%252C%2520and%250Asummarization.%2520LoTA%2520obtains%2520better%2520performance%2520than%2520full%2520fine-tuning%2520and%250Alow-rank%2520adaptation%2520%2528LoRA%2529%252C%2520and%2520maintains%2520good%2520performance%2520even%2520after%2520training%250Aon%2520other%2520tasks%2520--%2520thus%252C%2520avoiding%2520catastrophic%2520forgetting.%2520By%2520extracting%2520and%250Afine-tuning%2520over%2520%255Cemph%257Blottery%2520tickets%257D%2520%2528or%2520%255Cemph%257Bsparse%2520task%2520vectors%257D%2529%252C%2520LoTA%250Aalso%2520enables%2520model%2520merging%2520over%2520highly%2520dissimilar%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16797v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lottery%20Ticket%20Adaptation%3A%20Mitigating%20Destructive%20Interference%20in%20LLMs&entry.906535625=Ashwinee%20Panda%20and%20Berivan%20Isik%20and%20Xiangyu%20Qi%20and%20Sanmi%20Koyejo%20and%20Tsachy%20Weissman%20and%20Prateek%20Mittal&entry.1292438233=%20%20Existing%20methods%20for%20adapting%20large%20language%20models%20%28LLMs%29%20to%20new%20tasks%20are%0Anot%20suited%20to%20multi-task%20adaptation%20because%20they%20modify%20all%20the%20model%20weights%0A--%20causing%20destructive%20interference%20between%20tasks.%20The%20resulting%20effects%2C%20such%0Aas%20catastrophic%20forgetting%20of%20earlier%20tasks%2C%20make%20it%20challenging%20to%20obtain%20good%0Aperformance%20on%20multiple%20tasks%20at%20the%20same%20time.%20To%20mitigate%20this%2C%20we%20propose%0ALottery%20Ticket%20Adaptation%20%28LoTA%29%2C%20a%20sparse%20adaptation%20method%20that%20identifies%0Aand%20optimizes%20only%20a%20sparse%20subnetwork%20of%20the%20model.%20We%20evaluate%20LoTA%20on%20a%20wide%0Arange%20of%20challenging%20tasks%20such%20as%20instruction%20following%2C%20reasoning%2C%20math%2C%20and%0Asummarization.%20LoTA%20obtains%20better%20performance%20than%20full%20fine-tuning%20and%0Alow-rank%20adaptation%20%28LoRA%29%2C%20and%20maintains%20good%20performance%20even%20after%20training%0Aon%20other%20tasks%20--%20thus%2C%20avoiding%20catastrophic%20forgetting.%20By%20extracting%20and%0Afine-tuning%20over%20%5Cemph%7Blottery%20tickets%7D%20%28or%20%5Cemph%7Bsparse%20task%20vectors%7D%29%2C%20LoTA%0Aalso%20enables%20model%20merging%20over%20highly%20dissimilar%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16797v1&entry.124074799=Read"},
{"title": "Do As I Do: Pose Guided Human Motion Copy", "author": "Sifan Wu and Zhenguang Liu and Beibei Zhang and Roger Zimmermann and Zhongjie Ba and Xiaosong Zhang and Kui Ren", "abstract": "  Human motion copy is an intriguing yet challenging task in artificial\nintelligence and computer vision, which strives to generate a fake video of a\ntarget person performing the motion of a source person. The problem is\ninherently challenging due to the subtle human-body texture details to be\ngenerated and the temporal consistency to be considered. Existing approaches\ntypically adopt a conventional GAN with an L1 or L2 loss to produce the target\nfake video, which intrinsically necessitates a large number of training samples\nthat are challenging to acquire. Meanwhile, current methods still have\ndifficulties in attaining realistic image details and temporal consistency,\nwhich unfortunately can be easily perceived by human observers. Motivated by\nthis, we try to tackle the issues from three aspects: (1) We constrain\npose-to-appearance generation with a perceptual loss and a theoretically\nmotivated Gromov-Wasserstein loss to bridge the gap between pose and\nappearance. (2) We present an episodic memory module in the pose-to-appearance\ngeneration to propel continuous learning that helps the model learn from its\npast poor generations. We also utilize geometrical cues of the face to optimize\nfacial details and refine each key body part with a dedicated local GAN. (3) We\nadvocate generating the foreground in a sequence-to-sequence manner rather than\na single-frame manner, explicitly enforcing temporal inconsistency. Empirical\nresults on five datasets, iPER, ComplexMotion, SoloDance, Fish, and Mouse\ndatasets, demonstrate that our method is capable of generating realistic target\nvideos while precisely copying motion from a source video. Our method\nsignificantly outperforms state-of-the-art approaches and gains 7.2% and 12.4%\nimprovements in PSNR and FID respectively.\n", "link": "http://arxiv.org/abs/2406.16601v1", "date": "2024-06-24", "relevancy": 2.5576, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6743}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6176}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20As%20I%20Do%3A%20Pose%20Guided%20Human%20Motion%20Copy&body=Title%3A%20Do%20As%20I%20Do%3A%20Pose%20Guided%20Human%20Motion%20Copy%0AAuthor%3A%20Sifan%20Wu%20and%20Zhenguang%20Liu%20and%20Beibei%20Zhang%20and%20Roger%20Zimmermann%20and%20Zhongjie%20Ba%20and%20Xiaosong%20Zhang%20and%20Kui%20Ren%0AAbstract%3A%20%20%20Human%20motion%20copy%20is%20an%20intriguing%20yet%20challenging%20task%20in%20artificial%0Aintelligence%20and%20computer%20vision%2C%20which%20strives%20to%20generate%20a%20fake%20video%20of%20a%0Atarget%20person%20performing%20the%20motion%20of%20a%20source%20person.%20The%20problem%20is%0Ainherently%20challenging%20due%20to%20the%20subtle%20human-body%20texture%20details%20to%20be%0Agenerated%20and%20the%20temporal%20consistency%20to%20be%20considered.%20Existing%20approaches%0Atypically%20adopt%20a%20conventional%20GAN%20with%20an%20L1%20or%20L2%20loss%20to%20produce%20the%20target%0Afake%20video%2C%20which%20intrinsically%20necessitates%20a%20large%20number%20of%20training%20samples%0Athat%20are%20challenging%20to%20acquire.%20Meanwhile%2C%20current%20methods%20still%20have%0Adifficulties%20in%20attaining%20realistic%20image%20details%20and%20temporal%20consistency%2C%0Awhich%20unfortunately%20can%20be%20easily%20perceived%20by%20human%20observers.%20Motivated%20by%0Athis%2C%20we%20try%20to%20tackle%20the%20issues%20from%20three%20aspects%3A%20%281%29%20We%20constrain%0Apose-to-appearance%20generation%20with%20a%20perceptual%20loss%20and%20a%20theoretically%0Amotivated%20Gromov-Wasserstein%20loss%20to%20bridge%20the%20gap%20between%20pose%20and%0Aappearance.%20%282%29%20We%20present%20an%20episodic%20memory%20module%20in%20the%20pose-to-appearance%0Ageneration%20to%20propel%20continuous%20learning%20that%20helps%20the%20model%20learn%20from%20its%0Apast%20poor%20generations.%20We%20also%20utilize%20geometrical%20cues%20of%20the%20face%20to%20optimize%0Afacial%20details%20and%20refine%20each%20key%20body%20part%20with%20a%20dedicated%20local%20GAN.%20%283%29%20We%0Aadvocate%20generating%20the%20foreground%20in%20a%20sequence-to-sequence%20manner%20rather%20than%0Aa%20single-frame%20manner%2C%20explicitly%20enforcing%20temporal%20inconsistency.%20Empirical%0Aresults%20on%20five%20datasets%2C%20iPER%2C%20ComplexMotion%2C%20SoloDance%2C%20Fish%2C%20and%20Mouse%0Adatasets%2C%20demonstrate%20that%20our%20method%20is%20capable%20of%20generating%20realistic%20target%0Avideos%20while%20precisely%20copying%20motion%20from%20a%20source%20video.%20Our%20method%0Asignificantly%20outperforms%20state-of-the-art%20approaches%20and%20gains%207.2%25%20and%2012.4%25%0Aimprovements%20in%20PSNR%20and%20FID%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520As%2520I%2520Do%253A%2520Pose%2520Guided%2520Human%2520Motion%2520Copy%26entry.906535625%3DSifan%2520Wu%2520and%2520Zhenguang%2520Liu%2520and%2520Beibei%2520Zhang%2520and%2520Roger%2520Zimmermann%2520and%2520Zhongjie%2520Ba%2520and%2520Xiaosong%2520Zhang%2520and%2520Kui%2520Ren%26entry.1292438233%3D%2520%2520Human%2520motion%2520copy%2520is%2520an%2520intriguing%2520yet%2520challenging%2520task%2520in%2520artificial%250Aintelligence%2520and%2520computer%2520vision%252C%2520which%2520strives%2520to%2520generate%2520a%2520fake%2520video%2520of%2520a%250Atarget%2520person%2520performing%2520the%2520motion%2520of%2520a%2520source%2520person.%2520The%2520problem%2520is%250Ainherently%2520challenging%2520due%2520to%2520the%2520subtle%2520human-body%2520texture%2520details%2520to%2520be%250Agenerated%2520and%2520the%2520temporal%2520consistency%2520to%2520be%2520considered.%2520Existing%2520approaches%250Atypically%2520adopt%2520a%2520conventional%2520GAN%2520with%2520an%2520L1%2520or%2520L2%2520loss%2520to%2520produce%2520the%2520target%250Afake%2520video%252C%2520which%2520intrinsically%2520necessitates%2520a%2520large%2520number%2520of%2520training%2520samples%250Athat%2520are%2520challenging%2520to%2520acquire.%2520Meanwhile%252C%2520current%2520methods%2520still%2520have%250Adifficulties%2520in%2520attaining%2520realistic%2520image%2520details%2520and%2520temporal%2520consistency%252C%250Awhich%2520unfortunately%2520can%2520be%2520easily%2520perceived%2520by%2520human%2520observers.%2520Motivated%2520by%250Athis%252C%2520we%2520try%2520to%2520tackle%2520the%2520issues%2520from%2520three%2520aspects%253A%2520%25281%2529%2520We%2520constrain%250Apose-to-appearance%2520generation%2520with%2520a%2520perceptual%2520loss%2520and%2520a%2520theoretically%250Amotivated%2520Gromov-Wasserstein%2520loss%2520to%2520bridge%2520the%2520gap%2520between%2520pose%2520and%250Aappearance.%2520%25282%2529%2520We%2520present%2520an%2520episodic%2520memory%2520module%2520in%2520the%2520pose-to-appearance%250Ageneration%2520to%2520propel%2520continuous%2520learning%2520that%2520helps%2520the%2520model%2520learn%2520from%2520its%250Apast%2520poor%2520generations.%2520We%2520also%2520utilize%2520geometrical%2520cues%2520of%2520the%2520face%2520to%2520optimize%250Afacial%2520details%2520and%2520refine%2520each%2520key%2520body%2520part%2520with%2520a%2520dedicated%2520local%2520GAN.%2520%25283%2529%2520We%250Aadvocate%2520generating%2520the%2520foreground%2520in%2520a%2520sequence-to-sequence%2520manner%2520rather%2520than%250Aa%2520single-frame%2520manner%252C%2520explicitly%2520enforcing%2520temporal%2520inconsistency.%2520Empirical%250Aresults%2520on%2520five%2520datasets%252C%2520iPER%252C%2520ComplexMotion%252C%2520SoloDance%252C%2520Fish%252C%2520and%2520Mouse%250Adatasets%252C%2520demonstrate%2520that%2520our%2520method%2520is%2520capable%2520of%2520generating%2520realistic%2520target%250Avideos%2520while%2520precisely%2520copying%2520motion%2520from%2520a%2520source%2520video.%2520Our%2520method%250Asignificantly%2520outperforms%2520state-of-the-art%2520approaches%2520and%2520gains%25207.2%2525%2520and%252012.4%2525%250Aimprovements%2520in%2520PSNR%2520and%2520FID%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20As%20I%20Do%3A%20Pose%20Guided%20Human%20Motion%20Copy&entry.906535625=Sifan%20Wu%20and%20Zhenguang%20Liu%20and%20Beibei%20Zhang%20and%20Roger%20Zimmermann%20and%20Zhongjie%20Ba%20and%20Xiaosong%20Zhang%20and%20Kui%20Ren&entry.1292438233=%20%20Human%20motion%20copy%20is%20an%20intriguing%20yet%20challenging%20task%20in%20artificial%0Aintelligence%20and%20computer%20vision%2C%20which%20strives%20to%20generate%20a%20fake%20video%20of%20a%0Atarget%20person%20performing%20the%20motion%20of%20a%20source%20person.%20The%20problem%20is%0Ainherently%20challenging%20due%20to%20the%20subtle%20human-body%20texture%20details%20to%20be%0Agenerated%20and%20the%20temporal%20consistency%20to%20be%20considered.%20Existing%20approaches%0Atypically%20adopt%20a%20conventional%20GAN%20with%20an%20L1%20or%20L2%20loss%20to%20produce%20the%20target%0Afake%20video%2C%20which%20intrinsically%20necessitates%20a%20large%20number%20of%20training%20samples%0Athat%20are%20challenging%20to%20acquire.%20Meanwhile%2C%20current%20methods%20still%20have%0Adifficulties%20in%20attaining%20realistic%20image%20details%20and%20temporal%20consistency%2C%0Awhich%20unfortunately%20can%20be%20easily%20perceived%20by%20human%20observers.%20Motivated%20by%0Athis%2C%20we%20try%20to%20tackle%20the%20issues%20from%20three%20aspects%3A%20%281%29%20We%20constrain%0Apose-to-appearance%20generation%20with%20a%20perceptual%20loss%20and%20a%20theoretically%0Amotivated%20Gromov-Wasserstein%20loss%20to%20bridge%20the%20gap%20between%20pose%20and%0Aappearance.%20%282%29%20We%20present%20an%20episodic%20memory%20module%20in%20the%20pose-to-appearance%0Ageneration%20to%20propel%20continuous%20learning%20that%20helps%20the%20model%20learn%20from%20its%0Apast%20poor%20generations.%20We%20also%20utilize%20geometrical%20cues%20of%20the%20face%20to%20optimize%0Afacial%20details%20and%20refine%20each%20key%20body%20part%20with%20a%20dedicated%20local%20GAN.%20%283%29%20We%0Aadvocate%20generating%20the%20foreground%20in%20a%20sequence-to-sequence%20manner%20rather%20than%0Aa%20single-frame%20manner%2C%20explicitly%20enforcing%20temporal%20inconsistency.%20Empirical%0Aresults%20on%20five%20datasets%2C%20iPER%2C%20ComplexMotion%2C%20SoloDance%2C%20Fish%2C%20and%20Mouse%0Adatasets%2C%20demonstrate%20that%20our%20method%20is%20capable%20of%20generating%20realistic%20target%0Avideos%20while%20precisely%20copying%20motion%20from%20a%20source%20video.%20Our%20method%0Asignificantly%20outperforms%20state-of-the-art%20approaches%20and%20gains%207.2%25%20and%2012.4%25%0Aimprovements%20in%20PSNR%20and%20FID%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16601v1&entry.124074799=Read"},
{"title": "Unleashing the Power of Meta-tuning for Few-shot Generalization Through\n  Sparse Interpolated Experts", "author": "Shengzhuang Chen and Jihoon Tack and Yunqiao Yang and Yee Whye Teh and Jonathan Richard Schwarz and Ying Wei", "abstract": "  Recent successes suggest that parameter-efficient fine-tuning of foundation\nmodels as the state-of-the-art method for transfer learning in vision,\nreplacing the rich literature of alternatives such as meta-learning. In trying\nto harness the best of both worlds, meta-tuning introduces a subsequent\noptimization stage of foundation models but has so far only shown limited\nsuccess and crucially tends to underperform on out-of-distribution (OOD) tasks.\nIn this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by\nsparse mixture-of-experts approaches and trained to isolate subsets of\npre-trained parameters automatically for meta-tuning on each task. SMAT\nsuccessfully overcomes OOD sensitivity and delivers on the promise of enhancing\nthe transfer abilities of vision foundation models beyond parameter-efficient\nfine-tuning. We establish new state-of-the-art results on a challenging\ncombination of Meta-Dataset augmented with additional OOD tasks in both\nzero-shot and gradient-based adaptation settings. In addition, we provide a\nthorough analysis of the superiority of learned over hand-designed sparsity\npatterns for sparse expert methods and the pivotal importance of the sparsity\nlevel in balancing between in-distribution and out-of-distribution\ngeneralization. Our code is publicly available.\n", "link": "http://arxiv.org/abs/2403.08477v2", "date": "2024-06-23", "relevancy": 2.5477, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5332}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5071}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20the%20Power%20of%20Meta-tuning%20for%20Few-shot%20Generalization%20Through%0A%20%20Sparse%20Interpolated%20Experts&body=Title%3A%20Unleashing%20the%20Power%20of%20Meta-tuning%20for%20Few-shot%20Generalization%20Through%0A%20%20Sparse%20Interpolated%20Experts%0AAuthor%3A%20Shengzhuang%20Chen%20and%20Jihoon%20Tack%20and%20Yunqiao%20Yang%20and%20Yee%20Whye%20Teh%20and%20Jonathan%20Richard%20Schwarz%20and%20Ying%20Wei%0AAbstract%3A%20%20%20Recent%20successes%20suggest%20that%20parameter-efficient%20fine-tuning%20of%20foundation%0Amodels%20as%20the%20state-of-the-art%20method%20for%20transfer%20learning%20in%20vision%2C%0Areplacing%20the%20rich%20literature%20of%20alternatives%20such%20as%20meta-learning.%20In%20trying%0Ato%20harness%20the%20best%20of%20both%20worlds%2C%20meta-tuning%20introduces%20a%20subsequent%0Aoptimization%20stage%20of%20foundation%20models%20but%20has%20so%20far%20only%20shown%20limited%0Asuccess%20and%20crucially%20tends%20to%20underperform%20on%20out-of-distribution%20%28OOD%29%20tasks.%0AIn%20this%20paper%2C%20we%20introduce%20Sparse%20MetA-Tuning%20%28SMAT%29%2C%20a%20method%20inspired%20by%0Asparse%20mixture-of-experts%20approaches%20and%20trained%20to%20isolate%20subsets%20of%0Apre-trained%20parameters%20automatically%20for%20meta-tuning%20on%20each%20task.%20SMAT%0Asuccessfully%20overcomes%20OOD%20sensitivity%20and%20delivers%20on%20the%20promise%20of%20enhancing%0Athe%20transfer%20abilities%20of%20vision%20foundation%20models%20beyond%20parameter-efficient%0Afine-tuning.%20We%20establish%20new%20state-of-the-art%20results%20on%20a%20challenging%0Acombination%20of%20Meta-Dataset%20augmented%20with%20additional%20OOD%20tasks%20in%20both%0Azero-shot%20and%20gradient-based%20adaptation%20settings.%20In%20addition%2C%20we%20provide%20a%0Athorough%20analysis%20of%20the%20superiority%20of%20learned%20over%20hand-designed%20sparsity%0Apatterns%20for%20sparse%20expert%20methods%20and%20the%20pivotal%20importance%20of%20the%20sparsity%0Alevel%20in%20balancing%20between%20in-distribution%20and%20out-of-distribution%0Ageneralization.%20Our%20code%20is%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08477v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520the%2520Power%2520of%2520Meta-tuning%2520for%2520Few-shot%2520Generalization%2520Through%250A%2520%2520Sparse%2520Interpolated%2520Experts%26entry.906535625%3DShengzhuang%2520Chen%2520and%2520Jihoon%2520Tack%2520and%2520Yunqiao%2520Yang%2520and%2520Yee%2520Whye%2520Teh%2520and%2520Jonathan%2520Richard%2520Schwarz%2520and%2520Ying%2520Wei%26entry.1292438233%3D%2520%2520Recent%2520successes%2520suggest%2520that%2520parameter-efficient%2520fine-tuning%2520of%2520foundation%250Amodels%2520as%2520the%2520state-of-the-art%2520method%2520for%2520transfer%2520learning%2520in%2520vision%252C%250Areplacing%2520the%2520rich%2520literature%2520of%2520alternatives%2520such%2520as%2520meta-learning.%2520In%2520trying%250Ato%2520harness%2520the%2520best%2520of%2520both%2520worlds%252C%2520meta-tuning%2520introduces%2520a%2520subsequent%250Aoptimization%2520stage%2520of%2520foundation%2520models%2520but%2520has%2520so%2520far%2520only%2520shown%2520limited%250Asuccess%2520and%2520crucially%2520tends%2520to%2520underperform%2520on%2520out-of-distribution%2520%2528OOD%2529%2520tasks.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520Sparse%2520MetA-Tuning%2520%2528SMAT%2529%252C%2520a%2520method%2520inspired%2520by%250Asparse%2520mixture-of-experts%2520approaches%2520and%2520trained%2520to%2520isolate%2520subsets%2520of%250Apre-trained%2520parameters%2520automatically%2520for%2520meta-tuning%2520on%2520each%2520task.%2520SMAT%250Asuccessfully%2520overcomes%2520OOD%2520sensitivity%2520and%2520delivers%2520on%2520the%2520promise%2520of%2520enhancing%250Athe%2520transfer%2520abilities%2520of%2520vision%2520foundation%2520models%2520beyond%2520parameter-efficient%250Afine-tuning.%2520We%2520establish%2520new%2520state-of-the-art%2520results%2520on%2520a%2520challenging%250Acombination%2520of%2520Meta-Dataset%2520augmented%2520with%2520additional%2520OOD%2520tasks%2520in%2520both%250Azero-shot%2520and%2520gradient-based%2520adaptation%2520settings.%2520In%2520addition%252C%2520we%2520provide%2520a%250Athorough%2520analysis%2520of%2520the%2520superiority%2520of%2520learned%2520over%2520hand-designed%2520sparsity%250Apatterns%2520for%2520sparse%2520expert%2520methods%2520and%2520the%2520pivotal%2520importance%2520of%2520the%2520sparsity%250Alevel%2520in%2520balancing%2520between%2520in-distribution%2520and%2520out-of-distribution%250Ageneralization.%2520Our%2520code%2520is%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08477v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20the%20Power%20of%20Meta-tuning%20for%20Few-shot%20Generalization%20Through%0A%20%20Sparse%20Interpolated%20Experts&entry.906535625=Shengzhuang%20Chen%20and%20Jihoon%20Tack%20and%20Yunqiao%20Yang%20and%20Yee%20Whye%20Teh%20and%20Jonathan%20Richard%20Schwarz%20and%20Ying%20Wei&entry.1292438233=%20%20Recent%20successes%20suggest%20that%20parameter-efficient%20fine-tuning%20of%20foundation%0Amodels%20as%20the%20state-of-the-art%20method%20for%20transfer%20learning%20in%20vision%2C%0Areplacing%20the%20rich%20literature%20of%20alternatives%20such%20as%20meta-learning.%20In%20trying%0Ato%20harness%20the%20best%20of%20both%20worlds%2C%20meta-tuning%20introduces%20a%20subsequent%0Aoptimization%20stage%20of%20foundation%20models%20but%20has%20so%20far%20only%20shown%20limited%0Asuccess%20and%20crucially%20tends%20to%20underperform%20on%20out-of-distribution%20%28OOD%29%20tasks.%0AIn%20this%20paper%2C%20we%20introduce%20Sparse%20MetA-Tuning%20%28SMAT%29%2C%20a%20method%20inspired%20by%0Asparse%20mixture-of-experts%20approaches%20and%20trained%20to%20isolate%20subsets%20of%0Apre-trained%20parameters%20automatically%20for%20meta-tuning%20on%20each%20task.%20SMAT%0Asuccessfully%20overcomes%20OOD%20sensitivity%20and%20delivers%20on%20the%20promise%20of%20enhancing%0Athe%20transfer%20abilities%20of%20vision%20foundation%20models%20beyond%20parameter-efficient%0Afine-tuning.%20We%20establish%20new%20state-of-the-art%20results%20on%20a%20challenging%0Acombination%20of%20Meta-Dataset%20augmented%20with%20additional%20OOD%20tasks%20in%20both%0Azero-shot%20and%20gradient-based%20adaptation%20settings.%20In%20addition%2C%20we%20provide%20a%0Athorough%20analysis%20of%20the%20superiority%20of%20learned%20over%20hand-designed%20sparsity%0Apatterns%20for%20sparse%20expert%20methods%20and%20the%20pivotal%20importance%20of%20the%20sparsity%0Alevel%20in%20balancing%20between%20in-distribution%20and%20out-of-distribution%0Ageneralization.%20Our%20code%20is%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08477v2&entry.124074799=Read"},
{"title": "Lesion-Aware Cross-Phase Attention Network for Renal Tumor Subtype\n  Classification on Multi-Phase CT Scans", "author": "Kwang-Hyun Uhm and Seung-Won Jung and Sung-Hoo Hong and Sung-Jea Ko", "abstract": "  Multi-phase computed tomography (CT) has been widely used for the\npreoperative diagnosis of kidney cancer due to its non-invasive nature and\nability to characterize renal lesions. However, since enhancement patterns of\nrenal lesions across CT phases are different even for the same lesion type, the\nvisual assessment by radiologists suffers from inter-observer variability in\nclinical practice. Although deep learning-based approaches have been recently\nexplored for differential diagnosis of kidney cancer, they do not explicitly\nmodel the relationships between CT phases in the network design, limiting the\ndiagnostic performance. In this paper, we propose a novel lesion-aware\ncross-phase attention network (LACPANet) that can effectively capture temporal\ndependencies of renal lesions across CT phases to accurately classify the\nlesions into five major pathological subtypes from time-series multi-phase CT\nimages. We introduce a 3D inter-phase lesion-aware attention mechanism to learn\neffective 3D lesion features that are used to estimate attention weights\ndescribing the inter-phase relations of the enhancement patterns. We also\npresent a multi-scale attention scheme to capture and aggregate temporal\npatterns of lesion features at different spatial scales for further\nimprovement. Extensive experiments on multi-phase CT scans of kidney cancer\npatients from the collected dataset demonstrate that our LACPANet outperforms\nstate-of-the-art approaches in diagnostic accuracy.\n", "link": "http://arxiv.org/abs/2406.16322v1", "date": "2024-06-24", "relevancy": 2.542, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5204}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5102}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lesion-Aware%20Cross-Phase%20Attention%20Network%20for%20Renal%20Tumor%20Subtype%0A%20%20Classification%20on%20Multi-Phase%20CT%20Scans&body=Title%3A%20Lesion-Aware%20Cross-Phase%20Attention%20Network%20for%20Renal%20Tumor%20Subtype%0A%20%20Classification%20on%20Multi-Phase%20CT%20Scans%0AAuthor%3A%20Kwang-Hyun%20Uhm%20and%20Seung-Won%20Jung%20and%20Sung-Hoo%20Hong%20and%20Sung-Jea%20Ko%0AAbstract%3A%20%20%20Multi-phase%20computed%20tomography%20%28CT%29%20has%20been%20widely%20used%20for%20the%0Apreoperative%20diagnosis%20of%20kidney%20cancer%20due%20to%20its%20non-invasive%20nature%20and%0Aability%20to%20characterize%20renal%20lesions.%20However%2C%20since%20enhancement%20patterns%20of%0Arenal%20lesions%20across%20CT%20phases%20are%20different%20even%20for%20the%20same%20lesion%20type%2C%20the%0Avisual%20assessment%20by%20radiologists%20suffers%20from%20inter-observer%20variability%20in%0Aclinical%20practice.%20Although%20deep%20learning-based%20approaches%20have%20been%20recently%0Aexplored%20for%20differential%20diagnosis%20of%20kidney%20cancer%2C%20they%20do%20not%20explicitly%0Amodel%20the%20relationships%20between%20CT%20phases%20in%20the%20network%20design%2C%20limiting%20the%0Adiagnostic%20performance.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20lesion-aware%0Across-phase%20attention%20network%20%28LACPANet%29%20that%20can%20effectively%20capture%20temporal%0Adependencies%20of%20renal%20lesions%20across%20CT%20phases%20to%20accurately%20classify%20the%0Alesions%20into%20five%20major%20pathological%20subtypes%20from%20time-series%20multi-phase%20CT%0Aimages.%20We%20introduce%20a%203D%20inter-phase%20lesion-aware%20attention%20mechanism%20to%20learn%0Aeffective%203D%20lesion%20features%20that%20are%20used%20to%20estimate%20attention%20weights%0Adescribing%20the%20inter-phase%20relations%20of%20the%20enhancement%20patterns.%20We%20also%0Apresent%20a%20multi-scale%20attention%20scheme%20to%20capture%20and%20aggregate%20temporal%0Apatterns%20of%20lesion%20features%20at%20different%20spatial%20scales%20for%20further%0Aimprovement.%20Extensive%20experiments%20on%20multi-phase%20CT%20scans%20of%20kidney%20cancer%0Apatients%20from%20the%20collected%20dataset%20demonstrate%20that%20our%20LACPANet%20outperforms%0Astate-of-the-art%20approaches%20in%20diagnostic%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLesion-Aware%2520Cross-Phase%2520Attention%2520Network%2520for%2520Renal%2520Tumor%2520Subtype%250A%2520%2520Classification%2520on%2520Multi-Phase%2520CT%2520Scans%26entry.906535625%3DKwang-Hyun%2520Uhm%2520and%2520Seung-Won%2520Jung%2520and%2520Sung-Hoo%2520Hong%2520and%2520Sung-Jea%2520Ko%26entry.1292438233%3D%2520%2520Multi-phase%2520computed%2520tomography%2520%2528CT%2529%2520has%2520been%2520widely%2520used%2520for%2520the%250Apreoperative%2520diagnosis%2520of%2520kidney%2520cancer%2520due%2520to%2520its%2520non-invasive%2520nature%2520and%250Aability%2520to%2520characterize%2520renal%2520lesions.%2520However%252C%2520since%2520enhancement%2520patterns%2520of%250Arenal%2520lesions%2520across%2520CT%2520phases%2520are%2520different%2520even%2520for%2520the%2520same%2520lesion%2520type%252C%2520the%250Avisual%2520assessment%2520by%2520radiologists%2520suffers%2520from%2520inter-observer%2520variability%2520in%250Aclinical%2520practice.%2520Although%2520deep%2520learning-based%2520approaches%2520have%2520been%2520recently%250Aexplored%2520for%2520differential%2520diagnosis%2520of%2520kidney%2520cancer%252C%2520they%2520do%2520not%2520explicitly%250Amodel%2520the%2520relationships%2520between%2520CT%2520phases%2520in%2520the%2520network%2520design%252C%2520limiting%2520the%250Adiagnostic%2520performance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520lesion-aware%250Across-phase%2520attention%2520network%2520%2528LACPANet%2529%2520that%2520can%2520effectively%2520capture%2520temporal%250Adependencies%2520of%2520renal%2520lesions%2520across%2520CT%2520phases%2520to%2520accurately%2520classify%2520the%250Alesions%2520into%2520five%2520major%2520pathological%2520subtypes%2520from%2520time-series%2520multi-phase%2520CT%250Aimages.%2520We%2520introduce%2520a%25203D%2520inter-phase%2520lesion-aware%2520attention%2520mechanism%2520to%2520learn%250Aeffective%25203D%2520lesion%2520features%2520that%2520are%2520used%2520to%2520estimate%2520attention%2520weights%250Adescribing%2520the%2520inter-phase%2520relations%2520of%2520the%2520enhancement%2520patterns.%2520We%2520also%250Apresent%2520a%2520multi-scale%2520attention%2520scheme%2520to%2520capture%2520and%2520aggregate%2520temporal%250Apatterns%2520of%2520lesion%2520features%2520at%2520different%2520spatial%2520scales%2520for%2520further%250Aimprovement.%2520Extensive%2520experiments%2520on%2520multi-phase%2520CT%2520scans%2520of%2520kidney%2520cancer%250Apatients%2520from%2520the%2520collected%2520dataset%2520demonstrate%2520that%2520our%2520LACPANet%2520outperforms%250Astate-of-the-art%2520approaches%2520in%2520diagnostic%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lesion-Aware%20Cross-Phase%20Attention%20Network%20for%20Renal%20Tumor%20Subtype%0A%20%20Classification%20on%20Multi-Phase%20CT%20Scans&entry.906535625=Kwang-Hyun%20Uhm%20and%20Seung-Won%20Jung%20and%20Sung-Hoo%20Hong%20and%20Sung-Jea%20Ko&entry.1292438233=%20%20Multi-phase%20computed%20tomography%20%28CT%29%20has%20been%20widely%20used%20for%20the%0Apreoperative%20diagnosis%20of%20kidney%20cancer%20due%20to%20its%20non-invasive%20nature%20and%0Aability%20to%20characterize%20renal%20lesions.%20However%2C%20since%20enhancement%20patterns%20of%0Arenal%20lesions%20across%20CT%20phases%20are%20different%20even%20for%20the%20same%20lesion%20type%2C%20the%0Avisual%20assessment%20by%20radiologists%20suffers%20from%20inter-observer%20variability%20in%0Aclinical%20practice.%20Although%20deep%20learning-based%20approaches%20have%20been%20recently%0Aexplored%20for%20differential%20diagnosis%20of%20kidney%20cancer%2C%20they%20do%20not%20explicitly%0Amodel%20the%20relationships%20between%20CT%20phases%20in%20the%20network%20design%2C%20limiting%20the%0Adiagnostic%20performance.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20lesion-aware%0Across-phase%20attention%20network%20%28LACPANet%29%20that%20can%20effectively%20capture%20temporal%0Adependencies%20of%20renal%20lesions%20across%20CT%20phases%20to%20accurately%20classify%20the%0Alesions%20into%20five%20major%20pathological%20subtypes%20from%20time-series%20multi-phase%20CT%0Aimages.%20We%20introduce%20a%203D%20inter-phase%20lesion-aware%20attention%20mechanism%20to%20learn%0Aeffective%203D%20lesion%20features%20that%20are%20used%20to%20estimate%20attention%20weights%0Adescribing%20the%20inter-phase%20relations%20of%20the%20enhancement%20patterns.%20We%20also%0Apresent%20a%20multi-scale%20attention%20scheme%20to%20capture%20and%20aggregate%20temporal%0Apatterns%20of%20lesion%20features%20at%20different%20spatial%20scales%20for%20further%0Aimprovement.%20Extensive%20experiments%20on%20multi-phase%20CT%20scans%20of%20kidney%20cancer%0Apatients%20from%20the%20collected%20dataset%20demonstrate%20that%20our%20LACPANet%20outperforms%0Astate-of-the-art%20approaches%20in%20diagnostic%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16322v1&entry.124074799=Read"},
{"title": "Multi-Margin Loss: Proposal and Application in Recommender Systems", "author": "Makbule Gulcin Ozsoy", "abstract": "  Recommender systems guide users through vast amounts of information by\nsuggesting items based on their predicted preferences. Collaborative\nfiltering-based deep learning techniques have regained popularity due to their\nsimplicity, using only user-item interactions. Typically, these systems consist\nof three main components: an interaction module, a loss function, and a\nnegative sampling strategy. Initially, researchers focused on enhancing\nperformance by developing complex interaction modules with techniques like\nmulti-layer perceptrons, transformers, or graph neural networks. However, there\nhas been a recent shift toward refining loss functions and negative sampling\nstrategies. This shift has increased interest in contrastive learning, which\npulls similar pairs closer while pushing dissimilar ones apart. Contrastive\nlearning involves key practices such as heavy data augmentation, large batch\nsizes, and hard-negative sampling, but these also bring challenges like high\nmemory demands and under-utilization of some negative samples. The proposed\nMulti-Margin Loss (MML) addresses these challenges by introducing multiple\nmargins and varying weights for negative samples. MML efficiently utilizes not\nonly the hardest negatives but also other non-trivial negatives, offering a\nsimpler yet effective loss function that outperforms more complex methods,\nespecially when resources are limited. Experiments on two well-known datasets\nshowed MML achieved up to a 20\\% performance improvement compared to a baseline\ncontrastive loss function with fewer negative samples.\n", "link": "http://arxiv.org/abs/2405.04614v2", "date": "2024-06-23", "relevancy": 2.5386, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5189}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5022}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Margin%20Loss%3A%20Proposal%20and%20Application%20in%20Recommender%20Systems&body=Title%3A%20Multi-Margin%20Loss%3A%20Proposal%20and%20Application%20in%20Recommender%20Systems%0AAuthor%3A%20Makbule%20Gulcin%20Ozsoy%0AAbstract%3A%20%20%20Recommender%20systems%20guide%20users%20through%20vast%20amounts%20of%20information%20by%0Asuggesting%20items%20based%20on%20their%20predicted%20preferences.%20Collaborative%0Afiltering-based%20deep%20learning%20techniques%20have%20regained%20popularity%20due%20to%20their%0Asimplicity%2C%20using%20only%20user-item%20interactions.%20Typically%2C%20these%20systems%20consist%0Aof%20three%20main%20components%3A%20an%20interaction%20module%2C%20a%20loss%20function%2C%20and%20a%0Anegative%20sampling%20strategy.%20Initially%2C%20researchers%20focused%20on%20enhancing%0Aperformance%20by%20developing%20complex%20interaction%20modules%20with%20techniques%20like%0Amulti-layer%20perceptrons%2C%20transformers%2C%20or%20graph%20neural%20networks.%20However%2C%20there%0Ahas%20been%20a%20recent%20shift%20toward%20refining%20loss%20functions%20and%20negative%20sampling%0Astrategies.%20This%20shift%20has%20increased%20interest%20in%20contrastive%20learning%2C%20which%0Apulls%20similar%20pairs%20closer%20while%20pushing%20dissimilar%20ones%20apart.%20Contrastive%0Alearning%20involves%20key%20practices%20such%20as%20heavy%20data%20augmentation%2C%20large%20batch%0Asizes%2C%20and%20hard-negative%20sampling%2C%20but%20these%20also%20bring%20challenges%20like%20high%0Amemory%20demands%20and%20under-utilization%20of%20some%20negative%20samples.%20The%20proposed%0AMulti-Margin%20Loss%20%28MML%29%20addresses%20these%20challenges%20by%20introducing%20multiple%0Amargins%20and%20varying%20weights%20for%20negative%20samples.%20MML%20efficiently%20utilizes%20not%0Aonly%20the%20hardest%20negatives%20but%20also%20other%20non-trivial%20negatives%2C%20offering%20a%0Asimpler%20yet%20effective%20loss%20function%20that%20outperforms%20more%20complex%20methods%2C%0Aespecially%20when%20resources%20are%20limited.%20Experiments%20on%20two%20well-known%20datasets%0Ashowed%20MML%20achieved%20up%20to%20a%2020%5C%25%20performance%20improvement%20compared%20to%20a%20baseline%0Acontrastive%20loss%20function%20with%20fewer%20negative%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04614v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Margin%2520Loss%253A%2520Proposal%2520and%2520Application%2520in%2520Recommender%2520Systems%26entry.906535625%3DMakbule%2520Gulcin%2520Ozsoy%26entry.1292438233%3D%2520%2520Recommender%2520systems%2520guide%2520users%2520through%2520vast%2520amounts%2520of%2520information%2520by%250Asuggesting%2520items%2520based%2520on%2520their%2520predicted%2520preferences.%2520Collaborative%250Afiltering-based%2520deep%2520learning%2520techniques%2520have%2520regained%2520popularity%2520due%2520to%2520their%250Asimplicity%252C%2520using%2520only%2520user-item%2520interactions.%2520Typically%252C%2520these%2520systems%2520consist%250Aof%2520three%2520main%2520components%253A%2520an%2520interaction%2520module%252C%2520a%2520loss%2520function%252C%2520and%2520a%250Anegative%2520sampling%2520strategy.%2520Initially%252C%2520researchers%2520focused%2520on%2520enhancing%250Aperformance%2520by%2520developing%2520complex%2520interaction%2520modules%2520with%2520techniques%2520like%250Amulti-layer%2520perceptrons%252C%2520transformers%252C%2520or%2520graph%2520neural%2520networks.%2520However%252C%2520there%250Ahas%2520been%2520a%2520recent%2520shift%2520toward%2520refining%2520loss%2520functions%2520and%2520negative%2520sampling%250Astrategies.%2520This%2520shift%2520has%2520increased%2520interest%2520in%2520contrastive%2520learning%252C%2520which%250Apulls%2520similar%2520pairs%2520closer%2520while%2520pushing%2520dissimilar%2520ones%2520apart.%2520Contrastive%250Alearning%2520involves%2520key%2520practices%2520such%2520as%2520heavy%2520data%2520augmentation%252C%2520large%2520batch%250Asizes%252C%2520and%2520hard-negative%2520sampling%252C%2520but%2520these%2520also%2520bring%2520challenges%2520like%2520high%250Amemory%2520demands%2520and%2520under-utilization%2520of%2520some%2520negative%2520samples.%2520The%2520proposed%250AMulti-Margin%2520Loss%2520%2528MML%2529%2520addresses%2520these%2520challenges%2520by%2520introducing%2520multiple%250Amargins%2520and%2520varying%2520weights%2520for%2520negative%2520samples.%2520MML%2520efficiently%2520utilizes%2520not%250Aonly%2520the%2520hardest%2520negatives%2520but%2520also%2520other%2520non-trivial%2520negatives%252C%2520offering%2520a%250Asimpler%2520yet%2520effective%2520loss%2520function%2520that%2520outperforms%2520more%2520complex%2520methods%252C%250Aespecially%2520when%2520resources%2520are%2520limited.%2520Experiments%2520on%2520two%2520well-known%2520datasets%250Ashowed%2520MML%2520achieved%2520up%2520to%2520a%252020%255C%2525%2520performance%2520improvement%2520compared%2520to%2520a%2520baseline%250Acontrastive%2520loss%2520function%2520with%2520fewer%2520negative%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04614v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Margin%20Loss%3A%20Proposal%20and%20Application%20in%20Recommender%20Systems&entry.906535625=Makbule%20Gulcin%20Ozsoy&entry.1292438233=%20%20Recommender%20systems%20guide%20users%20through%20vast%20amounts%20of%20information%20by%0Asuggesting%20items%20based%20on%20their%20predicted%20preferences.%20Collaborative%0Afiltering-based%20deep%20learning%20techniques%20have%20regained%20popularity%20due%20to%20their%0Asimplicity%2C%20using%20only%20user-item%20interactions.%20Typically%2C%20these%20systems%20consist%0Aof%20three%20main%20components%3A%20an%20interaction%20module%2C%20a%20loss%20function%2C%20and%20a%0Anegative%20sampling%20strategy.%20Initially%2C%20researchers%20focused%20on%20enhancing%0Aperformance%20by%20developing%20complex%20interaction%20modules%20with%20techniques%20like%0Amulti-layer%20perceptrons%2C%20transformers%2C%20or%20graph%20neural%20networks.%20However%2C%20there%0Ahas%20been%20a%20recent%20shift%20toward%20refining%20loss%20functions%20and%20negative%20sampling%0Astrategies.%20This%20shift%20has%20increased%20interest%20in%20contrastive%20learning%2C%20which%0Apulls%20similar%20pairs%20closer%20while%20pushing%20dissimilar%20ones%20apart.%20Contrastive%0Alearning%20involves%20key%20practices%20such%20as%20heavy%20data%20augmentation%2C%20large%20batch%0Asizes%2C%20and%20hard-negative%20sampling%2C%20but%20these%20also%20bring%20challenges%20like%20high%0Amemory%20demands%20and%20under-utilization%20of%20some%20negative%20samples.%20The%20proposed%0AMulti-Margin%20Loss%20%28MML%29%20addresses%20these%20challenges%20by%20introducing%20multiple%0Amargins%20and%20varying%20weights%20for%20negative%20samples.%20MML%20efficiently%20utilizes%20not%0Aonly%20the%20hardest%20negatives%20but%20also%20other%20non-trivial%20negatives%2C%20offering%20a%0Asimpler%20yet%20effective%20loss%20function%20that%20outperforms%20more%20complex%20methods%2C%0Aespecially%20when%20resources%20are%20limited.%20Experiments%20on%20two%20well-known%20datasets%0Ashowed%20MML%20achieved%20up%20to%20a%2020%5C%25%20performance%20improvement%20compared%20to%20a%20baseline%0Acontrastive%20loss%20function%20with%20fewer%20negative%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04614v2&entry.124074799=Read"},
{"title": "Hallmarks of Optimization Trajectories in Neural Networks: Directional\n  Exploration and Redundancy", "author": "Sidak Pal Singh and Bobby He and Thomas Hofmann and Bernhard Sch\u00f6lkopf", "abstract": "  We propose a fresh take on understanding the mechanisms of neural networks by\nanalyzing the rich directional structure of optimization trajectories,\nrepresented by their pointwise parameters. Towards this end, we introduce some\nnatural notions of the complexity of optimization trajectories, both\nqualitative and quantitative, which hallmark the directional nature of\noptimization in neural networks: when is there redundancy, and when\nexploration. We use them to reveal the inherent nuance and interplay involved\nbetween various optimization choices, such as momentum and weight decay.\nFurther, the trajectory perspective helps us see the effect of scale on\nregularizing the directional nature of trajectories, and as a by-product, we\nalso observe an intriguing heterogeneity of Q,K,V dynamics in the middle\nattention layers in LLMs and which is homogenized by scale. Importantly, we put\nthe significant directional redundancy observed to the test by demonstrating\nthat training only scalar batchnorm parameters some while into training matches\nthe performance of training the entire network, which thus exhibits the\npotential of hybrid optimization schemes that are geared towards efficiency.\n", "link": "http://arxiv.org/abs/2403.07379v2", "date": "2024-06-24", "relevancy": 2.5301, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5344}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4934}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hallmarks%20of%20Optimization%20Trajectories%20in%20Neural%20Networks%3A%20Directional%0A%20%20Exploration%20and%20Redundancy&body=Title%3A%20Hallmarks%20of%20Optimization%20Trajectories%20in%20Neural%20Networks%3A%20Directional%0A%20%20Exploration%20and%20Redundancy%0AAuthor%3A%20Sidak%20Pal%20Singh%20and%20Bobby%20He%20and%20Thomas%20Hofmann%20and%20Bernhard%20Sch%C3%B6lkopf%0AAbstract%3A%20%20%20We%20propose%20a%20fresh%20take%20on%20understanding%20the%20mechanisms%20of%20neural%20networks%20by%0Aanalyzing%20the%20rich%20directional%20structure%20of%20optimization%20trajectories%2C%0Arepresented%20by%20their%20pointwise%20parameters.%20Towards%20this%20end%2C%20we%20introduce%20some%0Anatural%20notions%20of%20the%20complexity%20of%20optimization%20trajectories%2C%20both%0Aqualitative%20and%20quantitative%2C%20which%20hallmark%20the%20directional%20nature%20of%0Aoptimization%20in%20neural%20networks%3A%20when%20is%20there%20redundancy%2C%20and%20when%0Aexploration.%20We%20use%20them%20to%20reveal%20the%20inherent%20nuance%20and%20interplay%20involved%0Abetween%20various%20optimization%20choices%2C%20such%20as%20momentum%20and%20weight%20decay.%0AFurther%2C%20the%20trajectory%20perspective%20helps%20us%20see%20the%20effect%20of%20scale%20on%0Aregularizing%20the%20directional%20nature%20of%20trajectories%2C%20and%20as%20a%20by-product%2C%20we%0Aalso%20observe%20an%20intriguing%20heterogeneity%20of%20Q%2CK%2CV%20dynamics%20in%20the%20middle%0Aattention%20layers%20in%20LLMs%20and%20which%20is%20homogenized%20by%20scale.%20Importantly%2C%20we%20put%0Athe%20significant%20directional%20redundancy%20observed%20to%20the%20test%20by%20demonstrating%0Athat%20training%20only%20scalar%20batchnorm%20parameters%20some%20while%20into%20training%20matches%0Athe%20performance%20of%20training%20the%20entire%20network%2C%20which%20thus%20exhibits%20the%0Apotential%20of%20hybrid%20optimization%20schemes%20that%20are%20geared%20towards%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07379v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHallmarks%2520of%2520Optimization%2520Trajectories%2520in%2520Neural%2520Networks%253A%2520Directional%250A%2520%2520Exploration%2520and%2520Redundancy%26entry.906535625%3DSidak%2520Pal%2520Singh%2520and%2520Bobby%2520He%2520and%2520Thomas%2520Hofmann%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520fresh%2520take%2520on%2520understanding%2520the%2520mechanisms%2520of%2520neural%2520networks%2520by%250Aanalyzing%2520the%2520rich%2520directional%2520structure%2520of%2520optimization%2520trajectories%252C%250Arepresented%2520by%2520their%2520pointwise%2520parameters.%2520Towards%2520this%2520end%252C%2520we%2520introduce%2520some%250Anatural%2520notions%2520of%2520the%2520complexity%2520of%2520optimization%2520trajectories%252C%2520both%250Aqualitative%2520and%2520quantitative%252C%2520which%2520hallmark%2520the%2520directional%2520nature%2520of%250Aoptimization%2520in%2520neural%2520networks%253A%2520when%2520is%2520there%2520redundancy%252C%2520and%2520when%250Aexploration.%2520We%2520use%2520them%2520to%2520reveal%2520the%2520inherent%2520nuance%2520and%2520interplay%2520involved%250Abetween%2520various%2520optimization%2520choices%252C%2520such%2520as%2520momentum%2520and%2520weight%2520decay.%250AFurther%252C%2520the%2520trajectory%2520perspective%2520helps%2520us%2520see%2520the%2520effect%2520of%2520scale%2520on%250Aregularizing%2520the%2520directional%2520nature%2520of%2520trajectories%252C%2520and%2520as%2520a%2520by-product%252C%2520we%250Aalso%2520observe%2520an%2520intriguing%2520heterogeneity%2520of%2520Q%252CK%252CV%2520dynamics%2520in%2520the%2520middle%250Aattention%2520layers%2520in%2520LLMs%2520and%2520which%2520is%2520homogenized%2520by%2520scale.%2520Importantly%252C%2520we%2520put%250Athe%2520significant%2520directional%2520redundancy%2520observed%2520to%2520the%2520test%2520by%2520demonstrating%250Athat%2520training%2520only%2520scalar%2520batchnorm%2520parameters%2520some%2520while%2520into%2520training%2520matches%250Athe%2520performance%2520of%2520training%2520the%2520entire%2520network%252C%2520which%2520thus%2520exhibits%2520the%250Apotential%2520of%2520hybrid%2520optimization%2520schemes%2520that%2520are%2520geared%2520towards%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07379v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hallmarks%20of%20Optimization%20Trajectories%20in%20Neural%20Networks%3A%20Directional%0A%20%20Exploration%20and%20Redundancy&entry.906535625=Sidak%20Pal%20Singh%20and%20Bobby%20He%20and%20Thomas%20Hofmann%20and%20Bernhard%20Sch%C3%B6lkopf&entry.1292438233=%20%20We%20propose%20a%20fresh%20take%20on%20understanding%20the%20mechanisms%20of%20neural%20networks%20by%0Aanalyzing%20the%20rich%20directional%20structure%20of%20optimization%20trajectories%2C%0Arepresented%20by%20their%20pointwise%20parameters.%20Towards%20this%20end%2C%20we%20introduce%20some%0Anatural%20notions%20of%20the%20complexity%20of%20optimization%20trajectories%2C%20both%0Aqualitative%20and%20quantitative%2C%20which%20hallmark%20the%20directional%20nature%20of%0Aoptimization%20in%20neural%20networks%3A%20when%20is%20there%20redundancy%2C%20and%20when%0Aexploration.%20We%20use%20them%20to%20reveal%20the%20inherent%20nuance%20and%20interplay%20involved%0Abetween%20various%20optimization%20choices%2C%20such%20as%20momentum%20and%20weight%20decay.%0AFurther%2C%20the%20trajectory%20perspective%20helps%20us%20see%20the%20effect%20of%20scale%20on%0Aregularizing%20the%20directional%20nature%20of%20trajectories%2C%20and%20as%20a%20by-product%2C%20we%0Aalso%20observe%20an%20intriguing%20heterogeneity%20of%20Q%2CK%2CV%20dynamics%20in%20the%20middle%0Aattention%20layers%20in%20LLMs%20and%20which%20is%20homogenized%20by%20scale.%20Importantly%2C%20we%20put%0Athe%20significant%20directional%20redundancy%20observed%20to%20the%20test%20by%20demonstrating%0Athat%20training%20only%20scalar%20batchnorm%20parameters%20some%20while%20into%20training%20matches%0Athe%20performance%20of%20training%20the%20entire%20network%2C%20which%20thus%20exhibits%20the%0Apotential%20of%20hybrid%20optimization%20schemes%20that%20are%20geared%20towards%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07379v2&entry.124074799=Read"},
{"title": "Portrait3D: 3D Head Generation from Single In-the-wild Portrait Image", "author": "Jinkun Hao and Junshu Tang and Jiangning Zhang and Ran Yi and Yijia Hong and Moran Li and Weijian Cao and Yating Wang and Lizhuang Ma", "abstract": "  While recent works have achieved great success on one-shot 3D common object\ngeneration, high quality and fidelity 3D head generation from a single image\nremains a great challenge. Previous text-based methods for generating 3D heads\nwere limited by text descriptions and image-based methods struggled to produce\nhigh-quality head geometry. To handle this challenging problem, we propose a\nnovel framework, Portrait3D, to generate high-quality 3D heads while preserving\ntheir identities. Our work incorporates the identity information of the\nportrait image into three parts: 1) geometry initialization, 2) geometry\nsculpting, and 3) texture generation stages. Given a reference portrait image,\nwe first align the identity features with text features to realize ID-aware\nguidance enhancement, which contains the control signals representing the face\ninformation. We then use the canny map, ID features of the portrait image, and\na pre-trained text-to-normal/depth diffusion model to generate ID-aware\ngeometry supervision, and 3D-GAN inversion is employed to generate ID-aware\ngeometry initialization. Furthermore, with the ability to inject identity\ninformation into 3D head generation, we use ID-aware guidance to calculate\nID-aware Score Distillation (ISD) for geometry sculpting. For texture\ngeneration, we adopt the ID Consistent Texture Inpainting and Refinement which\nprogressively expands the view for texture inpainting to obtain an\ninitialization UV texture map. We then use the id-aware guidance to provide\nimage-level supervision for noisy multi-view images to obtain a refined texture\nmap. Extensive experiments demonstrate that we can generate high-quality 3D\nheads with accurate geometry and texture from single in-the-wild portrait\nimages. The project page is at https://jinkun-hao.github.io/Portrait3D/.\n", "link": "http://arxiv.org/abs/2406.16710v1", "date": "2024-06-24", "relevancy": 2.5124, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.6522}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6233}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Portrait3D%3A%203D%20Head%20Generation%20from%20Single%20In-the-wild%20Portrait%20Image&body=Title%3A%20Portrait3D%3A%203D%20Head%20Generation%20from%20Single%20In-the-wild%20Portrait%20Image%0AAuthor%3A%20Jinkun%20Hao%20and%20Junshu%20Tang%20and%20Jiangning%20Zhang%20and%20Ran%20Yi%20and%20Yijia%20Hong%20and%20Moran%20Li%20and%20Weijian%20Cao%20and%20Yating%20Wang%20and%20Lizhuang%20Ma%0AAbstract%3A%20%20%20While%20recent%20works%20have%20achieved%20great%20success%20on%20one-shot%203D%20common%20object%0Ageneration%2C%20high%20quality%20and%20fidelity%203D%20head%20generation%20from%20a%20single%20image%0Aremains%20a%20great%20challenge.%20Previous%20text-based%20methods%20for%20generating%203D%20heads%0Awere%20limited%20by%20text%20descriptions%20and%20image-based%20methods%20struggled%20to%20produce%0Ahigh-quality%20head%20geometry.%20To%20handle%20this%20challenging%20problem%2C%20we%20propose%20a%0Anovel%20framework%2C%20Portrait3D%2C%20to%20generate%20high-quality%203D%20heads%20while%20preserving%0Atheir%20identities.%20Our%20work%20incorporates%20the%20identity%20information%20of%20the%0Aportrait%20image%20into%20three%20parts%3A%201%29%20geometry%20initialization%2C%202%29%20geometry%0Asculpting%2C%20and%203%29%20texture%20generation%20stages.%20Given%20a%20reference%20portrait%20image%2C%0Awe%20first%20align%20the%20identity%20features%20with%20text%20features%20to%20realize%20ID-aware%0Aguidance%20enhancement%2C%20which%20contains%20the%20control%20signals%20representing%20the%20face%0Ainformation.%20We%20then%20use%20the%20canny%20map%2C%20ID%20features%20of%20the%20portrait%20image%2C%20and%0Aa%20pre-trained%20text-to-normal/depth%20diffusion%20model%20to%20generate%20ID-aware%0Ageometry%20supervision%2C%20and%203D-GAN%20inversion%20is%20employed%20to%20generate%20ID-aware%0Ageometry%20initialization.%20Furthermore%2C%20with%20the%20ability%20to%20inject%20identity%0Ainformation%20into%203D%20head%20generation%2C%20we%20use%20ID-aware%20guidance%20to%20calculate%0AID-aware%20Score%20Distillation%20%28ISD%29%20for%20geometry%20sculpting.%20For%20texture%0Ageneration%2C%20we%20adopt%20the%20ID%20Consistent%20Texture%20Inpainting%20and%20Refinement%20which%0Aprogressively%20expands%20the%20view%20for%20texture%20inpainting%20to%20obtain%20an%0Ainitialization%20UV%20texture%20map.%20We%20then%20use%20the%20id-aware%20guidance%20to%20provide%0Aimage-level%20supervision%20for%20noisy%20multi-view%20images%20to%20obtain%20a%20refined%20texture%0Amap.%20Extensive%20experiments%20demonstrate%20that%20we%20can%20generate%20high-quality%203D%0Aheads%20with%20accurate%20geometry%20and%20texture%20from%20single%20in-the-wild%20portrait%0Aimages.%20The%20project%20page%20is%20at%20https%3A//jinkun-hao.github.io/Portrait3D/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPortrait3D%253A%25203D%2520Head%2520Generation%2520from%2520Single%2520In-the-wild%2520Portrait%2520Image%26entry.906535625%3DJinkun%2520Hao%2520and%2520Junshu%2520Tang%2520and%2520Jiangning%2520Zhang%2520and%2520Ran%2520Yi%2520and%2520Yijia%2520Hong%2520and%2520Moran%2520Li%2520and%2520Weijian%2520Cao%2520and%2520Yating%2520Wang%2520and%2520Lizhuang%2520Ma%26entry.1292438233%3D%2520%2520While%2520recent%2520works%2520have%2520achieved%2520great%2520success%2520on%2520one-shot%25203D%2520common%2520object%250Ageneration%252C%2520high%2520quality%2520and%2520fidelity%25203D%2520head%2520generation%2520from%2520a%2520single%2520image%250Aremains%2520a%2520great%2520challenge.%2520Previous%2520text-based%2520methods%2520for%2520generating%25203D%2520heads%250Awere%2520limited%2520by%2520text%2520descriptions%2520and%2520image-based%2520methods%2520struggled%2520to%2520produce%250Ahigh-quality%2520head%2520geometry.%2520To%2520handle%2520this%2520challenging%2520problem%252C%2520we%2520propose%2520a%250Anovel%2520framework%252C%2520Portrait3D%252C%2520to%2520generate%2520high-quality%25203D%2520heads%2520while%2520preserving%250Atheir%2520identities.%2520Our%2520work%2520incorporates%2520the%2520identity%2520information%2520of%2520the%250Aportrait%2520image%2520into%2520three%2520parts%253A%25201%2529%2520geometry%2520initialization%252C%25202%2529%2520geometry%250Asculpting%252C%2520and%25203%2529%2520texture%2520generation%2520stages.%2520Given%2520a%2520reference%2520portrait%2520image%252C%250Awe%2520first%2520align%2520the%2520identity%2520features%2520with%2520text%2520features%2520to%2520realize%2520ID-aware%250Aguidance%2520enhancement%252C%2520which%2520contains%2520the%2520control%2520signals%2520representing%2520the%2520face%250Ainformation.%2520We%2520then%2520use%2520the%2520canny%2520map%252C%2520ID%2520features%2520of%2520the%2520portrait%2520image%252C%2520and%250Aa%2520pre-trained%2520text-to-normal/depth%2520diffusion%2520model%2520to%2520generate%2520ID-aware%250Ageometry%2520supervision%252C%2520and%25203D-GAN%2520inversion%2520is%2520employed%2520to%2520generate%2520ID-aware%250Ageometry%2520initialization.%2520Furthermore%252C%2520with%2520the%2520ability%2520to%2520inject%2520identity%250Ainformation%2520into%25203D%2520head%2520generation%252C%2520we%2520use%2520ID-aware%2520guidance%2520to%2520calculate%250AID-aware%2520Score%2520Distillation%2520%2528ISD%2529%2520for%2520geometry%2520sculpting.%2520For%2520texture%250Ageneration%252C%2520we%2520adopt%2520the%2520ID%2520Consistent%2520Texture%2520Inpainting%2520and%2520Refinement%2520which%250Aprogressively%2520expands%2520the%2520view%2520for%2520texture%2520inpainting%2520to%2520obtain%2520an%250Ainitialization%2520UV%2520texture%2520map.%2520We%2520then%2520use%2520the%2520id-aware%2520guidance%2520to%2520provide%250Aimage-level%2520supervision%2520for%2520noisy%2520multi-view%2520images%2520to%2520obtain%2520a%2520refined%2520texture%250Amap.%2520Extensive%2520experiments%2520demonstrate%2520that%2520we%2520can%2520generate%2520high-quality%25203D%250Aheads%2520with%2520accurate%2520geometry%2520and%2520texture%2520from%2520single%2520in-the-wild%2520portrait%250Aimages.%2520The%2520project%2520page%2520is%2520at%2520https%253A//jinkun-hao.github.io/Portrait3D/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Portrait3D%3A%203D%20Head%20Generation%20from%20Single%20In-the-wild%20Portrait%20Image&entry.906535625=Jinkun%20Hao%20and%20Junshu%20Tang%20and%20Jiangning%20Zhang%20and%20Ran%20Yi%20and%20Yijia%20Hong%20and%20Moran%20Li%20and%20Weijian%20Cao%20and%20Yating%20Wang%20and%20Lizhuang%20Ma&entry.1292438233=%20%20While%20recent%20works%20have%20achieved%20great%20success%20on%20one-shot%203D%20common%20object%0Ageneration%2C%20high%20quality%20and%20fidelity%203D%20head%20generation%20from%20a%20single%20image%0Aremains%20a%20great%20challenge.%20Previous%20text-based%20methods%20for%20generating%203D%20heads%0Awere%20limited%20by%20text%20descriptions%20and%20image-based%20methods%20struggled%20to%20produce%0Ahigh-quality%20head%20geometry.%20To%20handle%20this%20challenging%20problem%2C%20we%20propose%20a%0Anovel%20framework%2C%20Portrait3D%2C%20to%20generate%20high-quality%203D%20heads%20while%20preserving%0Atheir%20identities.%20Our%20work%20incorporates%20the%20identity%20information%20of%20the%0Aportrait%20image%20into%20three%20parts%3A%201%29%20geometry%20initialization%2C%202%29%20geometry%0Asculpting%2C%20and%203%29%20texture%20generation%20stages.%20Given%20a%20reference%20portrait%20image%2C%0Awe%20first%20align%20the%20identity%20features%20with%20text%20features%20to%20realize%20ID-aware%0Aguidance%20enhancement%2C%20which%20contains%20the%20control%20signals%20representing%20the%20face%0Ainformation.%20We%20then%20use%20the%20canny%20map%2C%20ID%20features%20of%20the%20portrait%20image%2C%20and%0Aa%20pre-trained%20text-to-normal/depth%20diffusion%20model%20to%20generate%20ID-aware%0Ageometry%20supervision%2C%20and%203D-GAN%20inversion%20is%20employed%20to%20generate%20ID-aware%0Ageometry%20initialization.%20Furthermore%2C%20with%20the%20ability%20to%20inject%20identity%0Ainformation%20into%203D%20head%20generation%2C%20we%20use%20ID-aware%20guidance%20to%20calculate%0AID-aware%20Score%20Distillation%20%28ISD%29%20for%20geometry%20sculpting.%20For%20texture%0Ageneration%2C%20we%20adopt%20the%20ID%20Consistent%20Texture%20Inpainting%20and%20Refinement%20which%0Aprogressively%20expands%20the%20view%20for%20texture%20inpainting%20to%20obtain%20an%0Ainitialization%20UV%20texture%20map.%20We%20then%20use%20the%20id-aware%20guidance%20to%20provide%0Aimage-level%20supervision%20for%20noisy%20multi-view%20images%20to%20obtain%20a%20refined%20texture%0Amap.%20Extensive%20experiments%20demonstrate%20that%20we%20can%20generate%20high-quality%203D%0Aheads%20with%20accurate%20geometry%20and%20texture%20from%20single%20in-the-wild%20portrait%0Aimages.%20The%20project%20page%20is%20at%20https%3A//jinkun-hao.github.io/Portrait3D/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16710v1&entry.124074799=Read"},
{"title": "OpenDlign: Enhancing Open-World 3D Learning with Depth-Aligned Images", "author": "Ye Mao and Junpeng Jing and Krystian Mikolajczyk", "abstract": "  Recent open-world 3D representation learning methods using Vision-Language\nModels (VLMs) to align 3D data with image-text information have shown superior\n3D zero-shot performance. However, CAD-rendered images for this alignment often\nlack realism and texture variation, compromising alignment robustness.\nMoreover, the volume discrepancy between 3D and 2D pretraining datasets\nhighlights the need for effective strategies to transfer the representational\nabilities of VLMs to 3D learning. In this paper, we present OpenDlign, a novel\nopen-world 3D model using depth-aligned images generated from a diffusion model\nfor robust multimodal alignment. These images exhibit greater texture diversity\nthan CAD renderings due to the stochastic nature of the diffusion model. By\nrefining the depth map projection pipeline and designing depth-specific\nprompts, OpenDlign leverages rich knowledge in pre-trained VLM for 3D\nrepresentation learning with streamlined fine-tuning. Our experiments show that\nOpenDlign achieves high zero-shot and few-shot performance on diverse 3D tasks,\ndespite only fine-tuning 6 million parameters on a limited ShapeNet dataset. In\nzero-shot classification, OpenDlign surpasses previous models by 8.0% on\nModelNet40 and 16.4% on OmniObject3D. Additionally, using depth-aligned images\nfor multimodal alignment consistently enhances the performance of other\nstate-of-the-art models.\n", "link": "http://arxiv.org/abs/2404.16538v2", "date": "2024-06-24", "relevancy": 2.5123, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6628}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6077}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenDlign%3A%20Enhancing%20Open-World%203D%20Learning%20with%20Depth-Aligned%20Images&body=Title%3A%20OpenDlign%3A%20Enhancing%20Open-World%203D%20Learning%20with%20Depth-Aligned%20Images%0AAuthor%3A%20Ye%20Mao%20and%20Junpeng%20Jing%20and%20Krystian%20Mikolajczyk%0AAbstract%3A%20%20%20Recent%20open-world%203D%20representation%20learning%20methods%20using%20Vision-Language%0AModels%20%28VLMs%29%20to%20align%203D%20data%20with%20image-text%20information%20have%20shown%20superior%0A3D%20zero-shot%20performance.%20However%2C%20CAD-rendered%20images%20for%20this%20alignment%20often%0Alack%20realism%20and%20texture%20variation%2C%20compromising%20alignment%20robustness.%0AMoreover%2C%20the%20volume%20discrepancy%20between%203D%20and%202D%20pretraining%20datasets%0Ahighlights%20the%20need%20for%20effective%20strategies%20to%20transfer%20the%20representational%0Aabilities%20of%20VLMs%20to%203D%20learning.%20In%20this%20paper%2C%20we%20present%20OpenDlign%2C%20a%20novel%0Aopen-world%203D%20model%20using%20depth-aligned%20images%20generated%20from%20a%20diffusion%20model%0Afor%20robust%20multimodal%20alignment.%20These%20images%20exhibit%20greater%20texture%20diversity%0Athan%20CAD%20renderings%20due%20to%20the%20stochastic%20nature%20of%20the%20diffusion%20model.%20By%0Arefining%20the%20depth%20map%20projection%20pipeline%20and%20designing%20depth-specific%0Aprompts%2C%20OpenDlign%20leverages%20rich%20knowledge%20in%20pre-trained%20VLM%20for%203D%0Arepresentation%20learning%20with%20streamlined%20fine-tuning.%20Our%20experiments%20show%20that%0AOpenDlign%20achieves%20high%20zero-shot%20and%20few-shot%20performance%20on%20diverse%203D%20tasks%2C%0Adespite%20only%20fine-tuning%206%20million%20parameters%20on%20a%20limited%20ShapeNet%20dataset.%20In%0Azero-shot%20classification%2C%20OpenDlign%20surpasses%20previous%20models%20by%208.0%25%20on%0AModelNet40%20and%2016.4%25%20on%20OmniObject3D.%20Additionally%2C%20using%20depth-aligned%20images%0Afor%20multimodal%20alignment%20consistently%20enhances%20the%20performance%20of%20other%0Astate-of-the-art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16538v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenDlign%253A%2520Enhancing%2520Open-World%25203D%2520Learning%2520with%2520Depth-Aligned%2520Images%26entry.906535625%3DYe%2520Mao%2520and%2520Junpeng%2520Jing%2520and%2520Krystian%2520Mikolajczyk%26entry.1292438233%3D%2520%2520Recent%2520open-world%25203D%2520representation%2520learning%2520methods%2520using%2520Vision-Language%250AModels%2520%2528VLMs%2529%2520to%2520align%25203D%2520data%2520with%2520image-text%2520information%2520have%2520shown%2520superior%250A3D%2520zero-shot%2520performance.%2520However%252C%2520CAD-rendered%2520images%2520for%2520this%2520alignment%2520often%250Alack%2520realism%2520and%2520texture%2520variation%252C%2520compromising%2520alignment%2520robustness.%250AMoreover%252C%2520the%2520volume%2520discrepancy%2520between%25203D%2520and%25202D%2520pretraining%2520datasets%250Ahighlights%2520the%2520need%2520for%2520effective%2520strategies%2520to%2520transfer%2520the%2520representational%250Aabilities%2520of%2520VLMs%2520to%25203D%2520learning.%2520In%2520this%2520paper%252C%2520we%2520present%2520OpenDlign%252C%2520a%2520novel%250Aopen-world%25203D%2520model%2520using%2520depth-aligned%2520images%2520generated%2520from%2520a%2520diffusion%2520model%250Afor%2520robust%2520multimodal%2520alignment.%2520These%2520images%2520exhibit%2520greater%2520texture%2520diversity%250Athan%2520CAD%2520renderings%2520due%2520to%2520the%2520stochastic%2520nature%2520of%2520the%2520diffusion%2520model.%2520By%250Arefining%2520the%2520depth%2520map%2520projection%2520pipeline%2520and%2520designing%2520depth-specific%250Aprompts%252C%2520OpenDlign%2520leverages%2520rich%2520knowledge%2520in%2520pre-trained%2520VLM%2520for%25203D%250Arepresentation%2520learning%2520with%2520streamlined%2520fine-tuning.%2520Our%2520experiments%2520show%2520that%250AOpenDlign%2520achieves%2520high%2520zero-shot%2520and%2520few-shot%2520performance%2520on%2520diverse%25203D%2520tasks%252C%250Adespite%2520only%2520fine-tuning%25206%2520million%2520parameters%2520on%2520a%2520limited%2520ShapeNet%2520dataset.%2520In%250Azero-shot%2520classification%252C%2520OpenDlign%2520surpasses%2520previous%2520models%2520by%25208.0%2525%2520on%250AModelNet40%2520and%252016.4%2525%2520on%2520OmniObject3D.%2520Additionally%252C%2520using%2520depth-aligned%2520images%250Afor%2520multimodal%2520alignment%2520consistently%2520enhances%2520the%2520performance%2520of%2520other%250Astate-of-the-art%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.16538v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenDlign%3A%20Enhancing%20Open-World%203D%20Learning%20with%20Depth-Aligned%20Images&entry.906535625=Ye%20Mao%20and%20Junpeng%20Jing%20and%20Krystian%20Mikolajczyk&entry.1292438233=%20%20Recent%20open-world%203D%20representation%20learning%20methods%20using%20Vision-Language%0AModels%20%28VLMs%29%20to%20align%203D%20data%20with%20image-text%20information%20have%20shown%20superior%0A3D%20zero-shot%20performance.%20However%2C%20CAD-rendered%20images%20for%20this%20alignment%20often%0Alack%20realism%20and%20texture%20variation%2C%20compromising%20alignment%20robustness.%0AMoreover%2C%20the%20volume%20discrepancy%20between%203D%20and%202D%20pretraining%20datasets%0Ahighlights%20the%20need%20for%20effective%20strategies%20to%20transfer%20the%20representational%0Aabilities%20of%20VLMs%20to%203D%20learning.%20In%20this%20paper%2C%20we%20present%20OpenDlign%2C%20a%20novel%0Aopen-world%203D%20model%20using%20depth-aligned%20images%20generated%20from%20a%20diffusion%20model%0Afor%20robust%20multimodal%20alignment.%20These%20images%20exhibit%20greater%20texture%20diversity%0Athan%20CAD%20renderings%20due%20to%20the%20stochastic%20nature%20of%20the%20diffusion%20model.%20By%0Arefining%20the%20depth%20map%20projection%20pipeline%20and%20designing%20depth-specific%0Aprompts%2C%20OpenDlign%20leverages%20rich%20knowledge%20in%20pre-trained%20VLM%20for%203D%0Arepresentation%20learning%20with%20streamlined%20fine-tuning.%20Our%20experiments%20show%20that%0AOpenDlign%20achieves%20high%20zero-shot%20and%20few-shot%20performance%20on%20diverse%203D%20tasks%2C%0Adespite%20only%20fine-tuning%206%20million%20parameters%20on%20a%20limited%20ShapeNet%20dataset.%20In%0Azero-shot%20classification%2C%20OpenDlign%20surpasses%20previous%20models%20by%208.0%25%20on%0AModelNet40%20and%2016.4%25%20on%20OmniObject3D.%20Additionally%2C%20using%20depth-aligned%20images%0Afor%20multimodal%20alignment%20consistently%20enhances%20the%20performance%20of%20other%0Astate-of-the-art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16538v2&entry.124074799=Read"},
{"title": "Theory on Mixture-of-Experts in Continual Learning", "author": "Hongbo Li and Sen Lin and Lingjie Duan and Yingbin Liang and Ness B. Shroff", "abstract": "  Continual learning (CL) has garnered significant attention because of its\nability to adapt to new tasks that arrive over time. Catastrophic forgetting\n(of old tasks) has been identified as a major issue in CL, as the model adapts\nto new tasks. The Mixture-of-Experts (MoE) model has recently been shown to\neffectively mitigate catastrophic forgetting in CL, by employing a gating\nnetwork to sparsify and distribute diverse tasks among multiple experts.\nHowever, there is a lack of theoretical analysis of MoE and its impact on the\nlearning performance in CL. This paper provides the first theoretical results\nto characterize the impact of MoE in CL via the lens of overparameterized\nlinear regression tasks. We establish the benefit of MoE over a single expert\nby proving that the MoE model can diversify its experts to specialize in\ndifferent tasks, while its router learns to select the right expert for each\ntask and balance the loads across all experts. Our study further suggests an\nintriguing fact that the MoE in CL needs to terminate the update of the gating\nnetwork after sufficient training rounds to attain system convergence, which is\nnot needed in the existing MoE studies that do not consider the continual task\narrival. Furthermore, we provide explicit expressions for the expected\nforgetting and overall generalization error to characterize the benefit of MoE\nin the learning performance in CL. Interestingly, adding more experts requires\nadditional rounds before convergence, which may not enhance the learning\nperformance. Finally, we conduct experiments on both synthetic and real\ndatasets to extend these insights from linear models to deep neural networks\n(DNNs), which also shed light on the practical algorithm design for MoE in CL.\n", "link": "http://arxiv.org/abs/2406.16437v1", "date": "2024-06-24", "relevancy": 2.5095, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5277}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4924}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Theory%20on%20Mixture-of-Experts%20in%20Continual%20Learning&body=Title%3A%20Theory%20on%20Mixture-of-Experts%20in%20Continual%20Learning%0AAuthor%3A%20Hongbo%20Li%20and%20Sen%20Lin%20and%20Lingjie%20Duan%20and%20Yingbin%20Liang%20and%20Ness%20B.%20Shroff%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20has%20garnered%20significant%20attention%20because%20of%20its%0Aability%20to%20adapt%20to%20new%20tasks%20that%20arrive%20over%20time.%20Catastrophic%20forgetting%0A%28of%20old%20tasks%29%20has%20been%20identified%20as%20a%20major%20issue%20in%20CL%2C%20as%20the%20model%20adapts%0Ato%20new%20tasks.%20The%20Mixture-of-Experts%20%28MoE%29%20model%20has%20recently%20been%20shown%20to%0Aeffectively%20mitigate%20catastrophic%20forgetting%20in%20CL%2C%20by%20employing%20a%20gating%0Anetwork%20to%20sparsify%20and%20distribute%20diverse%20tasks%20among%20multiple%20experts.%0AHowever%2C%20there%20is%20a%20lack%20of%20theoretical%20analysis%20of%20MoE%20and%20its%20impact%20on%20the%0Alearning%20performance%20in%20CL.%20This%20paper%20provides%20the%20first%20theoretical%20results%0Ato%20characterize%20the%20impact%20of%20MoE%20in%20CL%20via%20the%20lens%20of%20overparameterized%0Alinear%20regression%20tasks.%20We%20establish%20the%20benefit%20of%20MoE%20over%20a%20single%20expert%0Aby%20proving%20that%20the%20MoE%20model%20can%20diversify%20its%20experts%20to%20specialize%20in%0Adifferent%20tasks%2C%20while%20its%20router%20learns%20to%20select%20the%20right%20expert%20for%20each%0Atask%20and%20balance%20the%20loads%20across%20all%20experts.%20Our%20study%20further%20suggests%20an%0Aintriguing%20fact%20that%20the%20MoE%20in%20CL%20needs%20to%20terminate%20the%20update%20of%20the%20gating%0Anetwork%20after%20sufficient%20training%20rounds%20to%20attain%20system%20convergence%2C%20which%20is%0Anot%20needed%20in%20the%20existing%20MoE%20studies%20that%20do%20not%20consider%20the%20continual%20task%0Aarrival.%20Furthermore%2C%20we%20provide%20explicit%20expressions%20for%20the%20expected%0Aforgetting%20and%20overall%20generalization%20error%20to%20characterize%20the%20benefit%20of%20MoE%0Ain%20the%20learning%20performance%20in%20CL.%20Interestingly%2C%20adding%20more%20experts%20requires%0Aadditional%20rounds%20before%20convergence%2C%20which%20may%20not%20enhance%20the%20learning%0Aperformance.%20Finally%2C%20we%20conduct%20experiments%20on%20both%20synthetic%20and%20real%0Adatasets%20to%20extend%20these%20insights%20from%20linear%20models%20to%20deep%20neural%20networks%0A%28DNNs%29%2C%20which%20also%20shed%20light%20on%20the%20practical%20algorithm%20design%20for%20MoE%20in%20CL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTheory%2520on%2520Mixture-of-Experts%2520in%2520Continual%2520Learning%26entry.906535625%3DHongbo%2520Li%2520and%2520Sen%2520Lin%2520and%2520Lingjie%2520Duan%2520and%2520Yingbin%2520Liang%2520and%2520Ness%2520B.%2520Shroff%26entry.1292438233%3D%2520%2520Continual%2520learning%2520%2528CL%2529%2520has%2520garnered%2520significant%2520attention%2520because%2520of%2520its%250Aability%2520to%2520adapt%2520to%2520new%2520tasks%2520that%2520arrive%2520over%2520time.%2520Catastrophic%2520forgetting%250A%2528of%2520old%2520tasks%2529%2520has%2520been%2520identified%2520as%2520a%2520major%2520issue%2520in%2520CL%252C%2520as%2520the%2520model%2520adapts%250Ato%2520new%2520tasks.%2520The%2520Mixture-of-Experts%2520%2528MoE%2529%2520model%2520has%2520recently%2520been%2520shown%2520to%250Aeffectively%2520mitigate%2520catastrophic%2520forgetting%2520in%2520CL%252C%2520by%2520employing%2520a%2520gating%250Anetwork%2520to%2520sparsify%2520and%2520distribute%2520diverse%2520tasks%2520among%2520multiple%2520experts.%250AHowever%252C%2520there%2520is%2520a%2520lack%2520of%2520theoretical%2520analysis%2520of%2520MoE%2520and%2520its%2520impact%2520on%2520the%250Alearning%2520performance%2520in%2520CL.%2520This%2520paper%2520provides%2520the%2520first%2520theoretical%2520results%250Ato%2520characterize%2520the%2520impact%2520of%2520MoE%2520in%2520CL%2520via%2520the%2520lens%2520of%2520overparameterized%250Alinear%2520regression%2520tasks.%2520We%2520establish%2520the%2520benefit%2520of%2520MoE%2520over%2520a%2520single%2520expert%250Aby%2520proving%2520that%2520the%2520MoE%2520model%2520can%2520diversify%2520its%2520experts%2520to%2520specialize%2520in%250Adifferent%2520tasks%252C%2520while%2520its%2520router%2520learns%2520to%2520select%2520the%2520right%2520expert%2520for%2520each%250Atask%2520and%2520balance%2520the%2520loads%2520across%2520all%2520experts.%2520Our%2520study%2520further%2520suggests%2520an%250Aintriguing%2520fact%2520that%2520the%2520MoE%2520in%2520CL%2520needs%2520to%2520terminate%2520the%2520update%2520of%2520the%2520gating%250Anetwork%2520after%2520sufficient%2520training%2520rounds%2520to%2520attain%2520system%2520convergence%252C%2520which%2520is%250Anot%2520needed%2520in%2520the%2520existing%2520MoE%2520studies%2520that%2520do%2520not%2520consider%2520the%2520continual%2520task%250Aarrival.%2520Furthermore%252C%2520we%2520provide%2520explicit%2520expressions%2520for%2520the%2520expected%250Aforgetting%2520and%2520overall%2520generalization%2520error%2520to%2520characterize%2520the%2520benefit%2520of%2520MoE%250Ain%2520the%2520learning%2520performance%2520in%2520CL.%2520Interestingly%252C%2520adding%2520more%2520experts%2520requires%250Aadditional%2520rounds%2520before%2520convergence%252C%2520which%2520may%2520not%2520enhance%2520the%2520learning%250Aperformance.%2520Finally%252C%2520we%2520conduct%2520experiments%2520on%2520both%2520synthetic%2520and%2520real%250Adatasets%2520to%2520extend%2520these%2520insights%2520from%2520linear%2520models%2520to%2520deep%2520neural%2520networks%250A%2528DNNs%2529%252C%2520which%2520also%2520shed%2520light%2520on%2520the%2520practical%2520algorithm%2520design%2520for%2520MoE%2520in%2520CL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Theory%20on%20Mixture-of-Experts%20in%20Continual%20Learning&entry.906535625=Hongbo%20Li%20and%20Sen%20Lin%20and%20Lingjie%20Duan%20and%20Yingbin%20Liang%20and%20Ness%20B.%20Shroff&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20has%20garnered%20significant%20attention%20because%20of%20its%0Aability%20to%20adapt%20to%20new%20tasks%20that%20arrive%20over%20time.%20Catastrophic%20forgetting%0A%28of%20old%20tasks%29%20has%20been%20identified%20as%20a%20major%20issue%20in%20CL%2C%20as%20the%20model%20adapts%0Ato%20new%20tasks.%20The%20Mixture-of-Experts%20%28MoE%29%20model%20has%20recently%20been%20shown%20to%0Aeffectively%20mitigate%20catastrophic%20forgetting%20in%20CL%2C%20by%20employing%20a%20gating%0Anetwork%20to%20sparsify%20and%20distribute%20diverse%20tasks%20among%20multiple%20experts.%0AHowever%2C%20there%20is%20a%20lack%20of%20theoretical%20analysis%20of%20MoE%20and%20its%20impact%20on%20the%0Alearning%20performance%20in%20CL.%20This%20paper%20provides%20the%20first%20theoretical%20results%0Ato%20characterize%20the%20impact%20of%20MoE%20in%20CL%20via%20the%20lens%20of%20overparameterized%0Alinear%20regression%20tasks.%20We%20establish%20the%20benefit%20of%20MoE%20over%20a%20single%20expert%0Aby%20proving%20that%20the%20MoE%20model%20can%20diversify%20its%20experts%20to%20specialize%20in%0Adifferent%20tasks%2C%20while%20its%20router%20learns%20to%20select%20the%20right%20expert%20for%20each%0Atask%20and%20balance%20the%20loads%20across%20all%20experts.%20Our%20study%20further%20suggests%20an%0Aintriguing%20fact%20that%20the%20MoE%20in%20CL%20needs%20to%20terminate%20the%20update%20of%20the%20gating%0Anetwork%20after%20sufficient%20training%20rounds%20to%20attain%20system%20convergence%2C%20which%20is%0Anot%20needed%20in%20the%20existing%20MoE%20studies%20that%20do%20not%20consider%20the%20continual%20task%0Aarrival.%20Furthermore%2C%20we%20provide%20explicit%20expressions%20for%20the%20expected%0Aforgetting%20and%20overall%20generalization%20error%20to%20characterize%20the%20benefit%20of%20MoE%0Ain%20the%20learning%20performance%20in%20CL.%20Interestingly%2C%20adding%20more%20experts%20requires%0Aadditional%20rounds%20before%20convergence%2C%20which%20may%20not%20enhance%20the%20learning%0Aperformance.%20Finally%2C%20we%20conduct%20experiments%20on%20both%20synthetic%20and%20real%0Adatasets%20to%20extend%20these%20insights%20from%20linear%20models%20to%20deep%20neural%20networks%0A%28DNNs%29%2C%20which%20also%20shed%20light%20on%20the%20practical%20algorithm%20design%20for%20MoE%20in%20CL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16437v1&entry.124074799=Read"},
{"title": "Inference of Sequential Patterns for Neural Message Passing in Temporal\n  Graphs", "author": "Jan von Pichowski and Vincenzo Perri and Lisi Qarkaxhija and Ingo Scholtes", "abstract": "  The modelling of temporal patterns in dynamic graphs is an important current\nresearch issue in the development of time-aware GNNs. Whether or not a specific\nsequence of events in a temporal graph constitutes a temporal pattern not only\ndepends on the frequency of its occurrence. We consider whether it deviates\nfrom what is expected in a temporal graph where timestamps are randomly\nshuffled. While accounting for such a random baseline is important to model\ntemporal patterns, it has mostly been ignored by current temporal graph neural\nnetworks. To address this issue we propose HYPA-DBGNN, a novel two-step\napproach that combines (i) the inference of anomalous sequential patterns in\ntime series data on graphs based on a statistically principled null model, with\n(ii) a neural message passing approach that utilizes a higher-order De Bruijn\ngraph whose edges capture overrepresented sequential patterns. Our method\nleverages hypergeometric graph ensembles to identify anomalous edges within\nboth first- and higher-order De Bruijn graphs, which encode the temporal\nordering of events. The model introduces an inductive bias that enhances model\ninterpretability. We evaluate our approach for static node classification using\nbenchmark datasets and a synthetic dataset that showcases its ability to\nincorporate the observed inductive bias regarding over- and under-represented\ntemporal edges. We demonstrate the framework's effectiveness in detecting\nsimilar patterns within empirical datasets, resulting in superior performance\ncompared to baseline methods in node classification tasks. To the best of our\nknowledge, our work is the first to introduce statistically informed GNNs that\nleverage temporal and causal sequence anomalies. HYPA-DBGNN represents a path\nfor bridging the gap between statistical graph inference and neural graph\nrepresentation learning, with potential applications to static GNNs.\n", "link": "http://arxiv.org/abs/2406.16552v1", "date": "2024-06-24", "relevancy": 2.508, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5341}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4977}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inference%20of%20Sequential%20Patterns%20for%20Neural%20Message%20Passing%20in%20Temporal%0A%20%20Graphs&body=Title%3A%20Inference%20of%20Sequential%20Patterns%20for%20Neural%20Message%20Passing%20in%20Temporal%0A%20%20Graphs%0AAuthor%3A%20Jan%20von%20Pichowski%20and%20Vincenzo%20Perri%20and%20Lisi%20Qarkaxhija%20and%20Ingo%20Scholtes%0AAbstract%3A%20%20%20The%20modelling%20of%20temporal%20patterns%20in%20dynamic%20graphs%20is%20an%20important%20current%0Aresearch%20issue%20in%20the%20development%20of%20time-aware%20GNNs.%20Whether%20or%20not%20a%20specific%0Asequence%20of%20events%20in%20a%20temporal%20graph%20constitutes%20a%20temporal%20pattern%20not%20only%0Adepends%20on%20the%20frequency%20of%20its%20occurrence.%20We%20consider%20whether%20it%20deviates%0Afrom%20what%20is%20expected%20in%20a%20temporal%20graph%20where%20timestamps%20are%20randomly%0Ashuffled.%20While%20accounting%20for%20such%20a%20random%20baseline%20is%20important%20to%20model%0Atemporal%20patterns%2C%20it%20has%20mostly%20been%20ignored%20by%20current%20temporal%20graph%20neural%0Anetworks.%20To%20address%20this%20issue%20we%20propose%20HYPA-DBGNN%2C%20a%20novel%20two-step%0Aapproach%20that%20combines%20%28i%29%20the%20inference%20of%20anomalous%20sequential%20patterns%20in%0Atime%20series%20data%20on%20graphs%20based%20on%20a%20statistically%20principled%20null%20model%2C%20with%0A%28ii%29%20a%20neural%20message%20passing%20approach%20that%20utilizes%20a%20higher-order%20De%20Bruijn%0Agraph%20whose%20edges%20capture%20overrepresented%20sequential%20patterns.%20Our%20method%0Aleverages%20hypergeometric%20graph%20ensembles%20to%20identify%20anomalous%20edges%20within%0Aboth%20first-%20and%20higher-order%20De%20Bruijn%20graphs%2C%20which%20encode%20the%20temporal%0Aordering%20of%20events.%20The%20model%20introduces%20an%20inductive%20bias%20that%20enhances%20model%0Ainterpretability.%20We%20evaluate%20our%20approach%20for%20static%20node%20classification%20using%0Abenchmark%20datasets%20and%20a%20synthetic%20dataset%20that%20showcases%20its%20ability%20to%0Aincorporate%20the%20observed%20inductive%20bias%20regarding%20over-%20and%20under-represented%0Atemporal%20edges.%20We%20demonstrate%20the%20framework%27s%20effectiveness%20in%20detecting%0Asimilar%20patterns%20within%20empirical%20datasets%2C%20resulting%20in%20superior%20performance%0Acompared%20to%20baseline%20methods%20in%20node%20classification%20tasks.%20To%20the%20best%20of%20our%0Aknowledge%2C%20our%20work%20is%20the%20first%20to%20introduce%20statistically%20informed%20GNNs%20that%0Aleverage%20temporal%20and%20causal%20sequence%20anomalies.%20HYPA-DBGNN%20represents%20a%20path%0Afor%20bridging%20the%20gap%20between%20statistical%20graph%20inference%20and%20neural%20graph%0Arepresentation%20learning%2C%20with%20potential%20applications%20to%20static%20GNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInference%2520of%2520Sequential%2520Patterns%2520for%2520Neural%2520Message%2520Passing%2520in%2520Temporal%250A%2520%2520Graphs%26entry.906535625%3DJan%2520von%2520Pichowski%2520and%2520Vincenzo%2520Perri%2520and%2520Lisi%2520Qarkaxhija%2520and%2520Ingo%2520Scholtes%26entry.1292438233%3D%2520%2520The%2520modelling%2520of%2520temporal%2520patterns%2520in%2520dynamic%2520graphs%2520is%2520an%2520important%2520current%250Aresearch%2520issue%2520in%2520the%2520development%2520of%2520time-aware%2520GNNs.%2520Whether%2520or%2520not%2520a%2520specific%250Asequence%2520of%2520events%2520in%2520a%2520temporal%2520graph%2520constitutes%2520a%2520temporal%2520pattern%2520not%2520only%250Adepends%2520on%2520the%2520frequency%2520of%2520its%2520occurrence.%2520We%2520consider%2520whether%2520it%2520deviates%250Afrom%2520what%2520is%2520expected%2520in%2520a%2520temporal%2520graph%2520where%2520timestamps%2520are%2520randomly%250Ashuffled.%2520While%2520accounting%2520for%2520such%2520a%2520random%2520baseline%2520is%2520important%2520to%2520model%250Atemporal%2520patterns%252C%2520it%2520has%2520mostly%2520been%2520ignored%2520by%2520current%2520temporal%2520graph%2520neural%250Anetworks.%2520To%2520address%2520this%2520issue%2520we%2520propose%2520HYPA-DBGNN%252C%2520a%2520novel%2520two-step%250Aapproach%2520that%2520combines%2520%2528i%2529%2520the%2520inference%2520of%2520anomalous%2520sequential%2520patterns%2520in%250Atime%2520series%2520data%2520on%2520graphs%2520based%2520on%2520a%2520statistically%2520principled%2520null%2520model%252C%2520with%250A%2528ii%2529%2520a%2520neural%2520message%2520passing%2520approach%2520that%2520utilizes%2520a%2520higher-order%2520De%2520Bruijn%250Agraph%2520whose%2520edges%2520capture%2520overrepresented%2520sequential%2520patterns.%2520Our%2520method%250Aleverages%2520hypergeometric%2520graph%2520ensembles%2520to%2520identify%2520anomalous%2520edges%2520within%250Aboth%2520first-%2520and%2520higher-order%2520De%2520Bruijn%2520graphs%252C%2520which%2520encode%2520the%2520temporal%250Aordering%2520of%2520events.%2520The%2520model%2520introduces%2520an%2520inductive%2520bias%2520that%2520enhances%2520model%250Ainterpretability.%2520We%2520evaluate%2520our%2520approach%2520for%2520static%2520node%2520classification%2520using%250Abenchmark%2520datasets%2520and%2520a%2520synthetic%2520dataset%2520that%2520showcases%2520its%2520ability%2520to%250Aincorporate%2520the%2520observed%2520inductive%2520bias%2520regarding%2520over-%2520and%2520under-represented%250Atemporal%2520edges.%2520We%2520demonstrate%2520the%2520framework%2527s%2520effectiveness%2520in%2520detecting%250Asimilar%2520patterns%2520within%2520empirical%2520datasets%252C%2520resulting%2520in%2520superior%2520performance%250Acompared%2520to%2520baseline%2520methods%2520in%2520node%2520classification%2520tasks.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520our%2520work%2520is%2520the%2520first%2520to%2520introduce%2520statistically%2520informed%2520GNNs%2520that%250Aleverage%2520temporal%2520and%2520causal%2520sequence%2520anomalies.%2520HYPA-DBGNN%2520represents%2520a%2520path%250Afor%2520bridging%2520the%2520gap%2520between%2520statistical%2520graph%2520inference%2520and%2520neural%2520graph%250Arepresentation%2520learning%252C%2520with%2520potential%2520applications%2520to%2520static%2520GNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inference%20of%20Sequential%20Patterns%20for%20Neural%20Message%20Passing%20in%20Temporal%0A%20%20Graphs&entry.906535625=Jan%20von%20Pichowski%20and%20Vincenzo%20Perri%20and%20Lisi%20Qarkaxhija%20and%20Ingo%20Scholtes&entry.1292438233=%20%20The%20modelling%20of%20temporal%20patterns%20in%20dynamic%20graphs%20is%20an%20important%20current%0Aresearch%20issue%20in%20the%20development%20of%20time-aware%20GNNs.%20Whether%20or%20not%20a%20specific%0Asequence%20of%20events%20in%20a%20temporal%20graph%20constitutes%20a%20temporal%20pattern%20not%20only%0Adepends%20on%20the%20frequency%20of%20its%20occurrence.%20We%20consider%20whether%20it%20deviates%0Afrom%20what%20is%20expected%20in%20a%20temporal%20graph%20where%20timestamps%20are%20randomly%0Ashuffled.%20While%20accounting%20for%20such%20a%20random%20baseline%20is%20important%20to%20model%0Atemporal%20patterns%2C%20it%20has%20mostly%20been%20ignored%20by%20current%20temporal%20graph%20neural%0Anetworks.%20To%20address%20this%20issue%20we%20propose%20HYPA-DBGNN%2C%20a%20novel%20two-step%0Aapproach%20that%20combines%20%28i%29%20the%20inference%20of%20anomalous%20sequential%20patterns%20in%0Atime%20series%20data%20on%20graphs%20based%20on%20a%20statistically%20principled%20null%20model%2C%20with%0A%28ii%29%20a%20neural%20message%20passing%20approach%20that%20utilizes%20a%20higher-order%20De%20Bruijn%0Agraph%20whose%20edges%20capture%20overrepresented%20sequential%20patterns.%20Our%20method%0Aleverages%20hypergeometric%20graph%20ensembles%20to%20identify%20anomalous%20edges%20within%0Aboth%20first-%20and%20higher-order%20De%20Bruijn%20graphs%2C%20which%20encode%20the%20temporal%0Aordering%20of%20events.%20The%20model%20introduces%20an%20inductive%20bias%20that%20enhances%20model%0Ainterpretability.%20We%20evaluate%20our%20approach%20for%20static%20node%20classification%20using%0Abenchmark%20datasets%20and%20a%20synthetic%20dataset%20that%20showcases%20its%20ability%20to%0Aincorporate%20the%20observed%20inductive%20bias%20regarding%20over-%20and%20under-represented%0Atemporal%20edges.%20We%20demonstrate%20the%20framework%27s%20effectiveness%20in%20detecting%0Asimilar%20patterns%20within%20empirical%20datasets%2C%20resulting%20in%20superior%20performance%0Acompared%20to%20baseline%20methods%20in%20node%20classification%20tasks.%20To%20the%20best%20of%20our%0Aknowledge%2C%20our%20work%20is%20the%20first%20to%20introduce%20statistically%20informed%20GNNs%20that%0Aleverage%20temporal%20and%20causal%20sequence%20anomalies.%20HYPA-DBGNN%20represents%20a%20path%0Afor%20bridging%20the%20gap%20between%20statistical%20graph%20inference%20and%20neural%20graph%0Arepresentation%20learning%2C%20with%20potential%20applications%20to%20static%20GNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16552v1&entry.124074799=Read"},
{"title": "Towards Lightweight Graph Neural Network Search with Curriculum Graph\n  Sparsification", "author": "Beini Xie and Heng Chang and Ziwei Zhang and Zeyang Zhang and Simin Wu and Xin Wang and Yuan Meng and Wenwu Zhu", "abstract": "  Graph Neural Architecture Search (GNAS) has achieved superior performance on\nvarious graph-structured tasks. However, existing GNAS studies overlook the\napplications of GNAS in resource-constraint scenarios. This paper proposes to\ndesign a joint graph data and architecture mechanism, which identifies\nimportant sub-architectures via the valuable graph data. To search for optimal\nlightweight Graph Neural Networks (GNNs), we propose a Lightweight Graph Neural\nArchitecture Search with Graph SparsIfication and Network Pruning (GASSIP)\nmethod. In particular, GASSIP comprises an operation-pruned architecture search\nmodule to enable efficient lightweight GNN search. Meanwhile, we design a novel\ncurriculum graph data sparsification module with an architecture-aware\nedge-removing difficulty measurement to help select optimal sub-architectures.\nWith the aid of two differentiable masks, we iteratively optimize these two\nmodules to efficiently search for the optimal lightweight architecture.\nExtensive experiments on five benchmarks demonstrate the effectiveness of\nGASSIP. Particularly, our method achieves on-par or even higher node\nclassification performance with half or fewer model parameters of searched GNNs\nand a sparser graph.\n", "link": "http://arxiv.org/abs/2406.16357v1", "date": "2024-06-24", "relevancy": 2.495, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5204}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4978}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Lightweight%20Graph%20Neural%20Network%20Search%20with%20Curriculum%20Graph%0A%20%20Sparsification&body=Title%3A%20Towards%20Lightweight%20Graph%20Neural%20Network%20Search%20with%20Curriculum%20Graph%0A%20%20Sparsification%0AAuthor%3A%20Beini%20Xie%20and%20Heng%20Chang%20and%20Ziwei%20Zhang%20and%20Zeyang%20Zhang%20and%20Simin%20Wu%20and%20Xin%20Wang%20and%20Yuan%20Meng%20and%20Wenwu%20Zhu%0AAbstract%3A%20%20%20Graph%20Neural%20Architecture%20Search%20%28GNAS%29%20has%20achieved%20superior%20performance%20on%0Avarious%20graph-structured%20tasks.%20However%2C%20existing%20GNAS%20studies%20overlook%20the%0Aapplications%20of%20GNAS%20in%20resource-constraint%20scenarios.%20This%20paper%20proposes%20to%0Adesign%20a%20joint%20graph%20data%20and%20architecture%20mechanism%2C%20which%20identifies%0Aimportant%20sub-architectures%20via%20the%20valuable%20graph%20data.%20To%20search%20for%20optimal%0Alightweight%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20we%20propose%20a%20Lightweight%20Graph%20Neural%0AArchitecture%20Search%20with%20Graph%20SparsIfication%20and%20Network%20Pruning%20%28GASSIP%29%0Amethod.%20In%20particular%2C%20GASSIP%20comprises%20an%20operation-pruned%20architecture%20search%0Amodule%20to%20enable%20efficient%20lightweight%20GNN%20search.%20Meanwhile%2C%20we%20design%20a%20novel%0Acurriculum%20graph%20data%20sparsification%20module%20with%20an%20architecture-aware%0Aedge-removing%20difficulty%20measurement%20to%20help%20select%20optimal%20sub-architectures.%0AWith%20the%20aid%20of%20two%20differentiable%20masks%2C%20we%20iteratively%20optimize%20these%20two%0Amodules%20to%20efficiently%20search%20for%20the%20optimal%20lightweight%20architecture.%0AExtensive%20experiments%20on%20five%20benchmarks%20demonstrate%20the%20effectiveness%20of%0AGASSIP.%20Particularly%2C%20our%20method%20achieves%20on-par%20or%20even%20higher%20node%0Aclassification%20performance%20with%20half%20or%20fewer%20model%20parameters%20of%20searched%20GNNs%0Aand%20a%20sparser%20graph.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16357v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Lightweight%2520Graph%2520Neural%2520Network%2520Search%2520with%2520Curriculum%2520Graph%250A%2520%2520Sparsification%26entry.906535625%3DBeini%2520Xie%2520and%2520Heng%2520Chang%2520and%2520Ziwei%2520Zhang%2520and%2520Zeyang%2520Zhang%2520and%2520Simin%2520Wu%2520and%2520Xin%2520Wang%2520and%2520Yuan%2520Meng%2520and%2520Wenwu%2520Zhu%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Architecture%2520Search%2520%2528GNAS%2529%2520has%2520achieved%2520superior%2520performance%2520on%250Avarious%2520graph-structured%2520tasks.%2520However%252C%2520existing%2520GNAS%2520studies%2520overlook%2520the%250Aapplications%2520of%2520GNAS%2520in%2520resource-constraint%2520scenarios.%2520This%2520paper%2520proposes%2520to%250Adesign%2520a%2520joint%2520graph%2520data%2520and%2520architecture%2520mechanism%252C%2520which%2520identifies%250Aimportant%2520sub-architectures%2520via%2520the%2520valuable%2520graph%2520data.%2520To%2520search%2520for%2520optimal%250Alightweight%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%252C%2520we%2520propose%2520a%2520Lightweight%2520Graph%2520Neural%250AArchitecture%2520Search%2520with%2520Graph%2520SparsIfication%2520and%2520Network%2520Pruning%2520%2528GASSIP%2529%250Amethod.%2520In%2520particular%252C%2520GASSIP%2520comprises%2520an%2520operation-pruned%2520architecture%2520search%250Amodule%2520to%2520enable%2520efficient%2520lightweight%2520GNN%2520search.%2520Meanwhile%252C%2520we%2520design%2520a%2520novel%250Acurriculum%2520graph%2520data%2520sparsification%2520module%2520with%2520an%2520architecture-aware%250Aedge-removing%2520difficulty%2520measurement%2520to%2520help%2520select%2520optimal%2520sub-architectures.%250AWith%2520the%2520aid%2520of%2520two%2520differentiable%2520masks%252C%2520we%2520iteratively%2520optimize%2520these%2520two%250Amodules%2520to%2520efficiently%2520search%2520for%2520the%2520optimal%2520lightweight%2520architecture.%250AExtensive%2520experiments%2520on%2520five%2520benchmarks%2520demonstrate%2520the%2520effectiveness%2520of%250AGASSIP.%2520Particularly%252C%2520our%2520method%2520achieves%2520on-par%2520or%2520even%2520higher%2520node%250Aclassification%2520performance%2520with%2520half%2520or%2520fewer%2520model%2520parameters%2520of%2520searched%2520GNNs%250Aand%2520a%2520sparser%2520graph.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16357v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Lightweight%20Graph%20Neural%20Network%20Search%20with%20Curriculum%20Graph%0A%20%20Sparsification&entry.906535625=Beini%20Xie%20and%20Heng%20Chang%20and%20Ziwei%20Zhang%20and%20Zeyang%20Zhang%20and%20Simin%20Wu%20and%20Xin%20Wang%20and%20Yuan%20Meng%20and%20Wenwu%20Zhu&entry.1292438233=%20%20Graph%20Neural%20Architecture%20Search%20%28GNAS%29%20has%20achieved%20superior%20performance%20on%0Avarious%20graph-structured%20tasks.%20However%2C%20existing%20GNAS%20studies%20overlook%20the%0Aapplications%20of%20GNAS%20in%20resource-constraint%20scenarios.%20This%20paper%20proposes%20to%0Adesign%20a%20joint%20graph%20data%20and%20architecture%20mechanism%2C%20which%20identifies%0Aimportant%20sub-architectures%20via%20the%20valuable%20graph%20data.%20To%20search%20for%20optimal%0Alightweight%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20we%20propose%20a%20Lightweight%20Graph%20Neural%0AArchitecture%20Search%20with%20Graph%20SparsIfication%20and%20Network%20Pruning%20%28GASSIP%29%0Amethod.%20In%20particular%2C%20GASSIP%20comprises%20an%20operation-pruned%20architecture%20search%0Amodule%20to%20enable%20efficient%20lightweight%20GNN%20search.%20Meanwhile%2C%20we%20design%20a%20novel%0Acurriculum%20graph%20data%20sparsification%20module%20with%20an%20architecture-aware%0Aedge-removing%20difficulty%20measurement%20to%20help%20select%20optimal%20sub-architectures.%0AWith%20the%20aid%20of%20two%20differentiable%20masks%2C%20we%20iteratively%20optimize%20these%20two%0Amodules%20to%20efficiently%20search%20for%20the%20optimal%20lightweight%20architecture.%0AExtensive%20experiments%20on%20five%20benchmarks%20demonstrate%20the%20effectiveness%20of%0AGASSIP.%20Particularly%2C%20our%20method%20achieves%20on-par%20or%20even%20higher%20node%0Aclassification%20performance%20with%20half%20or%20fewer%20model%20parameters%20of%20searched%20GNNs%0Aand%20a%20sparser%20graph.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16357v1&entry.124074799=Read"},
{"title": "Feature Fusion for Human Activity Recognition using Parameter-Optimized\n  Multi-Stage Graph Convolutional Network and Transformer Models", "author": "Mohammad Belal and Taimur Hassan and Abdelfatah Ahmed and Ahmad Aljarah and Nael Alsheikh and Irfan Hussain", "abstract": "  Human activity recognition (HAR) is a crucial area of research that involves\nunderstanding human movements using computer and machine vision technology.\nDeep learning has emerged as a powerful tool for this task, with models such as\nConvolutional Neural Networks (CNNs) and Transformers being employed to capture\nvarious aspects of human motion. One of the key contributions of this work is\nthe demonstration of the effectiveness of feature fusion in improving HAR\naccuracy by capturing spatial and temporal features, which has important\nimplications for the development of more accurate and robust activity\nrecognition systems. The study uses sensory data from HuGaDB, PKU-MMD, LARa,\nand TUG datasets. Two model, the PO-MS-GCN and a Transformer were trained and\nevaluated, with PO-MS-GCN outperforming state-of-the-art models. HuGaDB and TUG\nachieved high accuracies and f1-scores, while LARa and PKU-MMD had lower\nscores. Feature fusion improved results across datasets.\n", "link": "http://arxiv.org/abs/2406.16638v1", "date": "2024-06-24", "relevancy": 2.4789, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5031}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.497}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Fusion%20for%20Human%20Activity%20Recognition%20using%20Parameter-Optimized%0A%20%20Multi-Stage%20Graph%20Convolutional%20Network%20and%20Transformer%20Models&body=Title%3A%20Feature%20Fusion%20for%20Human%20Activity%20Recognition%20using%20Parameter-Optimized%0A%20%20Multi-Stage%20Graph%20Convolutional%20Network%20and%20Transformer%20Models%0AAuthor%3A%20Mohammad%20Belal%20and%20Taimur%20Hassan%20and%20Abdelfatah%20Ahmed%20and%20Ahmad%20Aljarah%20and%20Nael%20Alsheikh%20and%20Irfan%20Hussain%0AAbstract%3A%20%20%20Human%20activity%20recognition%20%28HAR%29%20is%20a%20crucial%20area%20of%20research%20that%20involves%0Aunderstanding%20human%20movements%20using%20computer%20and%20machine%20vision%20technology.%0ADeep%20learning%20has%20emerged%20as%20a%20powerful%20tool%20for%20this%20task%2C%20with%20models%20such%20as%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20and%20Transformers%20being%20employed%20to%20capture%0Avarious%20aspects%20of%20human%20motion.%20One%20of%20the%20key%20contributions%20of%20this%20work%20is%0Athe%20demonstration%20of%20the%20effectiveness%20of%20feature%20fusion%20in%20improving%20HAR%0Aaccuracy%20by%20capturing%20spatial%20and%20temporal%20features%2C%20which%20has%20important%0Aimplications%20for%20the%20development%20of%20more%20accurate%20and%20robust%20activity%0Arecognition%20systems.%20The%20study%20uses%20sensory%20data%20from%20HuGaDB%2C%20PKU-MMD%2C%20LARa%2C%0Aand%20TUG%20datasets.%20Two%20model%2C%20the%20PO-MS-GCN%20and%20a%20Transformer%20were%20trained%20and%0Aevaluated%2C%20with%20PO-MS-GCN%20outperforming%20state-of-the-art%20models.%20HuGaDB%20and%20TUG%0Aachieved%20high%20accuracies%20and%20f1-scores%2C%20while%20LARa%20and%20PKU-MMD%20had%20lower%0Ascores.%20Feature%20fusion%20improved%20results%20across%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Fusion%2520for%2520Human%2520Activity%2520Recognition%2520using%2520Parameter-Optimized%250A%2520%2520Multi-Stage%2520Graph%2520Convolutional%2520Network%2520and%2520Transformer%2520Models%26entry.906535625%3DMohammad%2520Belal%2520and%2520Taimur%2520Hassan%2520and%2520Abdelfatah%2520Ahmed%2520and%2520Ahmad%2520Aljarah%2520and%2520Nael%2520Alsheikh%2520and%2520Irfan%2520Hussain%26entry.1292438233%3D%2520%2520Human%2520activity%2520recognition%2520%2528HAR%2529%2520is%2520a%2520crucial%2520area%2520of%2520research%2520that%2520involves%250Aunderstanding%2520human%2520movements%2520using%2520computer%2520and%2520machine%2520vision%2520technology.%250ADeep%2520learning%2520has%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520this%2520task%252C%2520with%2520models%2520such%2520as%250AConvolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%2520Transformers%2520being%2520employed%2520to%2520capture%250Avarious%2520aspects%2520of%2520human%2520motion.%2520One%2520of%2520the%2520key%2520contributions%2520of%2520this%2520work%2520is%250Athe%2520demonstration%2520of%2520the%2520effectiveness%2520of%2520feature%2520fusion%2520in%2520improving%2520HAR%250Aaccuracy%2520by%2520capturing%2520spatial%2520and%2520temporal%2520features%252C%2520which%2520has%2520important%250Aimplications%2520for%2520the%2520development%2520of%2520more%2520accurate%2520and%2520robust%2520activity%250Arecognition%2520systems.%2520The%2520study%2520uses%2520sensory%2520data%2520from%2520HuGaDB%252C%2520PKU-MMD%252C%2520LARa%252C%250Aand%2520TUG%2520datasets.%2520Two%2520model%252C%2520the%2520PO-MS-GCN%2520and%2520a%2520Transformer%2520were%2520trained%2520and%250Aevaluated%252C%2520with%2520PO-MS-GCN%2520outperforming%2520state-of-the-art%2520models.%2520HuGaDB%2520and%2520TUG%250Aachieved%2520high%2520accuracies%2520and%2520f1-scores%252C%2520while%2520LARa%2520and%2520PKU-MMD%2520had%2520lower%250Ascores.%2520Feature%2520fusion%2520improved%2520results%2520across%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Fusion%20for%20Human%20Activity%20Recognition%20using%20Parameter-Optimized%0A%20%20Multi-Stage%20Graph%20Convolutional%20Network%20and%20Transformer%20Models&entry.906535625=Mohammad%20Belal%20and%20Taimur%20Hassan%20and%20Abdelfatah%20Ahmed%20and%20Ahmad%20Aljarah%20and%20Nael%20Alsheikh%20and%20Irfan%20Hussain&entry.1292438233=%20%20Human%20activity%20recognition%20%28HAR%29%20is%20a%20crucial%20area%20of%20research%20that%20involves%0Aunderstanding%20human%20movements%20using%20computer%20and%20machine%20vision%20technology.%0ADeep%20learning%20has%20emerged%20as%20a%20powerful%20tool%20for%20this%20task%2C%20with%20models%20such%20as%0AConvolutional%20Neural%20Networks%20%28CNNs%29%20and%20Transformers%20being%20employed%20to%20capture%0Avarious%20aspects%20of%20human%20motion.%20One%20of%20the%20key%20contributions%20of%20this%20work%20is%0Athe%20demonstration%20of%20the%20effectiveness%20of%20feature%20fusion%20in%20improving%20HAR%0Aaccuracy%20by%20capturing%20spatial%20and%20temporal%20features%2C%20which%20has%20important%0Aimplications%20for%20the%20development%20of%20more%20accurate%20and%20robust%20activity%0Arecognition%20systems.%20The%20study%20uses%20sensory%20data%20from%20HuGaDB%2C%20PKU-MMD%2C%20LARa%2C%0Aand%20TUG%20datasets.%20Two%20model%2C%20the%20PO-MS-GCN%20and%20a%20Transformer%20were%20trained%20and%0Aevaluated%2C%20with%20PO-MS-GCN%20outperforming%20state-of-the-art%20models.%20HuGaDB%20and%20TUG%0Aachieved%20high%20accuracies%20and%20f1-scores%2C%20while%20LARa%20and%20PKU-MMD%20had%20lower%0Ascores.%20Feature%20fusion%20improved%20results%20across%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16638v1&entry.124074799=Read"},
{"title": "FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models", "author": "Haonan Qiu and Zhaoxi Chen and Zhouxia Wang and Yingqing He and Menghan Xia and Ziwei Liu", "abstract": "  Diffusion model has demonstrated remarkable capability in video generation,\nwhich further sparks interest in introducing trajectory control into the\ngeneration process. While existing works mainly focus on training-based methods\n(e.g., conditional adapter), we argue that diffusion model itself allows decent\ncontrol over the generated content without requiring any training. In this\nstudy, we introduce a tuning-free framework to achieve trajectory-controllable\nvideo generation, by imposing guidance on both noise construction and attention\ncomputation. Specifically, 1) we first show several instructive phenomenons and\nanalyze how initial noises influence the motion trajectory of generated\ncontent. 2) Subsequently, we propose FreeTraj, a tuning-free approach that\nenables trajectory control by modifying noise sampling and attention\nmechanisms. 3) Furthermore, we extend FreeTraj to facilitate longer and larger\nvideo generation with controllable trajectories. Equipped with these designs,\nusers have the flexibility to provide trajectories manually or opt for\ntrajectories automatically generated by the LLM trajectory planner. Extensive\nexperiments validate the efficacy of our approach in enhancing the trajectory\ncontrollability of video diffusion models.\n", "link": "http://arxiv.org/abs/2406.16863v1", "date": "2024-06-24", "relevancy": 2.4782, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6552}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6347}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeTraj%3A%20Tuning-Free%20Trajectory%20Control%20in%20Video%20Diffusion%20Models&body=Title%3A%20FreeTraj%3A%20Tuning-Free%20Trajectory%20Control%20in%20Video%20Diffusion%20Models%0AAuthor%3A%20Haonan%20Qiu%20and%20Zhaoxi%20Chen%20and%20Zhouxia%20Wang%20and%20Yingqing%20He%20and%20Menghan%20Xia%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20Diffusion%20model%20has%20demonstrated%20remarkable%20capability%20in%20video%20generation%2C%0Awhich%20further%20sparks%20interest%20in%20introducing%20trajectory%20control%20into%20the%0Ageneration%20process.%20While%20existing%20works%20mainly%20focus%20on%20training-based%20methods%0A%28e.g.%2C%20conditional%20adapter%29%2C%20we%20argue%20that%20diffusion%20model%20itself%20allows%20decent%0Acontrol%20over%20the%20generated%20content%20without%20requiring%20any%20training.%20In%20this%0Astudy%2C%20we%20introduce%20a%20tuning-free%20framework%20to%20achieve%20trajectory-controllable%0Avideo%20generation%2C%20by%20imposing%20guidance%20on%20both%20noise%20construction%20and%20attention%0Acomputation.%20Specifically%2C%201%29%20we%20first%20show%20several%20instructive%20phenomenons%20and%0Aanalyze%20how%20initial%20noises%20influence%20the%20motion%20trajectory%20of%20generated%0Acontent.%202%29%20Subsequently%2C%20we%20propose%20FreeTraj%2C%20a%20tuning-free%20approach%20that%0Aenables%20trajectory%20control%20by%20modifying%20noise%20sampling%20and%20attention%0Amechanisms.%203%29%20Furthermore%2C%20we%20extend%20FreeTraj%20to%20facilitate%20longer%20and%20larger%0Avideo%20generation%20with%20controllable%20trajectories.%20Equipped%20with%20these%20designs%2C%0Ausers%20have%20the%20flexibility%20to%20provide%20trajectories%20manually%20or%20opt%20for%0Atrajectories%20automatically%20generated%20by%20the%20LLM%20trajectory%20planner.%20Extensive%0Aexperiments%20validate%20the%20efficacy%20of%20our%20approach%20in%20enhancing%20the%20trajectory%0Acontrollability%20of%20video%20diffusion%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeTraj%253A%2520Tuning-Free%2520Trajectory%2520Control%2520in%2520Video%2520Diffusion%2520Models%26entry.906535625%3DHaonan%2520Qiu%2520and%2520Zhaoxi%2520Chen%2520and%2520Zhouxia%2520Wang%2520and%2520Yingqing%2520He%2520and%2520Menghan%2520Xia%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520Diffusion%2520model%2520has%2520demonstrated%2520remarkable%2520capability%2520in%2520video%2520generation%252C%250Awhich%2520further%2520sparks%2520interest%2520in%2520introducing%2520trajectory%2520control%2520into%2520the%250Ageneration%2520process.%2520While%2520existing%2520works%2520mainly%2520focus%2520on%2520training-based%2520methods%250A%2528e.g.%252C%2520conditional%2520adapter%2529%252C%2520we%2520argue%2520that%2520diffusion%2520model%2520itself%2520allows%2520decent%250Acontrol%2520over%2520the%2520generated%2520content%2520without%2520requiring%2520any%2520training.%2520In%2520this%250Astudy%252C%2520we%2520introduce%2520a%2520tuning-free%2520framework%2520to%2520achieve%2520trajectory-controllable%250Avideo%2520generation%252C%2520by%2520imposing%2520guidance%2520on%2520both%2520noise%2520construction%2520and%2520attention%250Acomputation.%2520Specifically%252C%25201%2529%2520we%2520first%2520show%2520several%2520instructive%2520phenomenons%2520and%250Aanalyze%2520how%2520initial%2520noises%2520influence%2520the%2520motion%2520trajectory%2520of%2520generated%250Acontent.%25202%2529%2520Subsequently%252C%2520we%2520propose%2520FreeTraj%252C%2520a%2520tuning-free%2520approach%2520that%250Aenables%2520trajectory%2520control%2520by%2520modifying%2520noise%2520sampling%2520and%2520attention%250Amechanisms.%25203%2529%2520Furthermore%252C%2520we%2520extend%2520FreeTraj%2520to%2520facilitate%2520longer%2520and%2520larger%250Avideo%2520generation%2520with%2520controllable%2520trajectories.%2520Equipped%2520with%2520these%2520designs%252C%250Ausers%2520have%2520the%2520flexibility%2520to%2520provide%2520trajectories%2520manually%2520or%2520opt%2520for%250Atrajectories%2520automatically%2520generated%2520by%2520the%2520LLM%2520trajectory%2520planner.%2520Extensive%250Aexperiments%2520validate%2520the%2520efficacy%2520of%2520our%2520approach%2520in%2520enhancing%2520the%2520trajectory%250Acontrollability%2520of%2520video%2520diffusion%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeTraj%3A%20Tuning-Free%20Trajectory%20Control%20in%20Video%20Diffusion%20Models&entry.906535625=Haonan%20Qiu%20and%20Zhaoxi%20Chen%20and%20Zhouxia%20Wang%20and%20Yingqing%20He%20and%20Menghan%20Xia%20and%20Ziwei%20Liu&entry.1292438233=%20%20Diffusion%20model%20has%20demonstrated%20remarkable%20capability%20in%20video%20generation%2C%0Awhich%20further%20sparks%20interest%20in%20introducing%20trajectory%20control%20into%20the%0Ageneration%20process.%20While%20existing%20works%20mainly%20focus%20on%20training-based%20methods%0A%28e.g.%2C%20conditional%20adapter%29%2C%20we%20argue%20that%20diffusion%20model%20itself%20allows%20decent%0Acontrol%20over%20the%20generated%20content%20without%20requiring%20any%20training.%20In%20this%0Astudy%2C%20we%20introduce%20a%20tuning-free%20framework%20to%20achieve%20trajectory-controllable%0Avideo%20generation%2C%20by%20imposing%20guidance%20on%20both%20noise%20construction%20and%20attention%0Acomputation.%20Specifically%2C%201%29%20we%20first%20show%20several%20instructive%20phenomenons%20and%0Aanalyze%20how%20initial%20noises%20influence%20the%20motion%20trajectory%20of%20generated%0Acontent.%202%29%20Subsequently%2C%20we%20propose%20FreeTraj%2C%20a%20tuning-free%20approach%20that%0Aenables%20trajectory%20control%20by%20modifying%20noise%20sampling%20and%20attention%0Amechanisms.%203%29%20Furthermore%2C%20we%20extend%20FreeTraj%20to%20facilitate%20longer%20and%20larger%0Avideo%20generation%20with%20controllable%20trajectories.%20Equipped%20with%20these%20designs%2C%0Ausers%20have%20the%20flexibility%20to%20provide%20trajectories%20manually%20or%20opt%20for%0Atrajectories%20automatically%20generated%20by%20the%20LLM%20trajectory%20planner.%20Extensive%0Aexperiments%20validate%20the%20efficacy%20of%20our%20approach%20in%20enhancing%20the%20trajectory%0Acontrollability%20of%20video%20diffusion%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16863v1&entry.124074799=Read"},
{"title": "Dreamitate: Real-World Visuomotor Policy Learning via Video Generation", "author": "Junbang Liang and Ruoshi Liu and Ege Ozguroglu and Sruthi Sudhakar and Achal Dave and Pavel Tokmakov and Shuran Song and Carl Vondrick", "abstract": "  A key challenge in manipulation is learning a policy that can robustly\ngeneralize to diverse visual environments. A promising mechanism for learning\nrobust policies is to leverage video generative models, which are pretrained on\nlarge-scale datasets of internet videos. In this paper, we propose a visuomotor\npolicy learning framework that fine-tunes a video diffusion model on human\ndemonstrations of a given task. At test time, we generate an example of an\nexecution of the task conditioned on images of a novel scene, and use this\nsynthesized execution directly to control the robot. Our key insight is that\nusing common tools allows us to effortlessly bridge the embodiment gap between\nthe human hand and the robot manipulator. We evaluate our approach on four\ntasks of increasing complexity and demonstrate that harnessing internet-scale\ngenerative models allows the learned policy to achieve a significantly higher\ndegree of generalization than existing behavior cloning approaches.\n", "link": "http://arxiv.org/abs/2406.16862v1", "date": "2024-06-24", "relevancy": 2.4594, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6358}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6004}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dreamitate%3A%20Real-World%20Visuomotor%20Policy%20Learning%20via%20Video%20Generation&body=Title%3A%20Dreamitate%3A%20Real-World%20Visuomotor%20Policy%20Learning%20via%20Video%20Generation%0AAuthor%3A%20Junbang%20Liang%20and%20Ruoshi%20Liu%20and%20Ege%20Ozguroglu%20and%20Sruthi%20Sudhakar%20and%20Achal%20Dave%20and%20Pavel%20Tokmakov%20and%20Shuran%20Song%20and%20Carl%20Vondrick%0AAbstract%3A%20%20%20A%20key%20challenge%20in%20manipulation%20is%20learning%20a%20policy%20that%20can%20robustly%0Ageneralize%20to%20diverse%20visual%20environments.%20A%20promising%20mechanism%20for%20learning%0Arobust%20policies%20is%20to%20leverage%20video%20generative%20models%2C%20which%20are%20pretrained%20on%0Alarge-scale%20datasets%20of%20internet%20videos.%20In%20this%20paper%2C%20we%20propose%20a%20visuomotor%0Apolicy%20learning%20framework%20that%20fine-tunes%20a%20video%20diffusion%20model%20on%20human%0Ademonstrations%20of%20a%20given%20task.%20At%20test%20time%2C%20we%20generate%20an%20example%20of%20an%0Aexecution%20of%20the%20task%20conditioned%20on%20images%20of%20a%20novel%20scene%2C%20and%20use%20this%0Asynthesized%20execution%20directly%20to%20control%20the%20robot.%20Our%20key%20insight%20is%20that%0Ausing%20common%20tools%20allows%20us%20to%20effortlessly%20bridge%20the%20embodiment%20gap%20between%0Athe%20human%20hand%20and%20the%20robot%20manipulator.%20We%20evaluate%20our%20approach%20on%20four%0Atasks%20of%20increasing%20complexity%20and%20demonstrate%20that%20harnessing%20internet-scale%0Agenerative%20models%20allows%20the%20learned%20policy%20to%20achieve%20a%20significantly%20higher%0Adegree%20of%20generalization%20than%20existing%20behavior%20cloning%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16862v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamitate%253A%2520Real-World%2520Visuomotor%2520Policy%2520Learning%2520via%2520Video%2520Generation%26entry.906535625%3DJunbang%2520Liang%2520and%2520Ruoshi%2520Liu%2520and%2520Ege%2520Ozguroglu%2520and%2520Sruthi%2520Sudhakar%2520and%2520Achal%2520Dave%2520and%2520Pavel%2520Tokmakov%2520and%2520Shuran%2520Song%2520and%2520Carl%2520Vondrick%26entry.1292438233%3D%2520%2520A%2520key%2520challenge%2520in%2520manipulation%2520is%2520learning%2520a%2520policy%2520that%2520can%2520robustly%250Ageneralize%2520to%2520diverse%2520visual%2520environments.%2520A%2520promising%2520mechanism%2520for%2520learning%250Arobust%2520policies%2520is%2520to%2520leverage%2520video%2520generative%2520models%252C%2520which%2520are%2520pretrained%2520on%250Alarge-scale%2520datasets%2520of%2520internet%2520videos.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520visuomotor%250Apolicy%2520learning%2520framework%2520that%2520fine-tunes%2520a%2520video%2520diffusion%2520model%2520on%2520human%250Ademonstrations%2520of%2520a%2520given%2520task.%2520At%2520test%2520time%252C%2520we%2520generate%2520an%2520example%2520of%2520an%250Aexecution%2520of%2520the%2520task%2520conditioned%2520on%2520images%2520of%2520a%2520novel%2520scene%252C%2520and%2520use%2520this%250Asynthesized%2520execution%2520directly%2520to%2520control%2520the%2520robot.%2520Our%2520key%2520insight%2520is%2520that%250Ausing%2520common%2520tools%2520allows%2520us%2520to%2520effortlessly%2520bridge%2520the%2520embodiment%2520gap%2520between%250Athe%2520human%2520hand%2520and%2520the%2520robot%2520manipulator.%2520We%2520evaluate%2520our%2520approach%2520on%2520four%250Atasks%2520of%2520increasing%2520complexity%2520and%2520demonstrate%2520that%2520harnessing%2520internet-scale%250Agenerative%2520models%2520allows%2520the%2520learned%2520policy%2520to%2520achieve%2520a%2520significantly%2520higher%250Adegree%2520of%2520generalization%2520than%2520existing%2520behavior%2520cloning%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16862v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dreamitate%3A%20Real-World%20Visuomotor%20Policy%20Learning%20via%20Video%20Generation&entry.906535625=Junbang%20Liang%20and%20Ruoshi%20Liu%20and%20Ege%20Ozguroglu%20and%20Sruthi%20Sudhakar%20and%20Achal%20Dave%20and%20Pavel%20Tokmakov%20and%20Shuran%20Song%20and%20Carl%20Vondrick&entry.1292438233=%20%20A%20key%20challenge%20in%20manipulation%20is%20learning%20a%20policy%20that%20can%20robustly%0Ageneralize%20to%20diverse%20visual%20environments.%20A%20promising%20mechanism%20for%20learning%0Arobust%20policies%20is%20to%20leverage%20video%20generative%20models%2C%20which%20are%20pretrained%20on%0Alarge-scale%20datasets%20of%20internet%20videos.%20In%20this%20paper%2C%20we%20propose%20a%20visuomotor%0Apolicy%20learning%20framework%20that%20fine-tunes%20a%20video%20diffusion%20model%20on%20human%0Ademonstrations%20of%20a%20given%20task.%20At%20test%20time%2C%20we%20generate%20an%20example%20of%20an%0Aexecution%20of%20the%20task%20conditioned%20on%20images%20of%20a%20novel%20scene%2C%20and%20use%20this%0Asynthesized%20execution%20directly%20to%20control%20the%20robot.%20Our%20key%20insight%20is%20that%0Ausing%20common%20tools%20allows%20us%20to%20effortlessly%20bridge%20the%20embodiment%20gap%20between%0Athe%20human%20hand%20and%20the%20robot%20manipulator.%20We%20evaluate%20our%20approach%20on%20four%0Atasks%20of%20increasing%20complexity%20and%20demonstrate%20that%20harnessing%20internet-scale%0Agenerative%20models%20allows%20the%20learned%20policy%20to%20achieve%20a%20significantly%20higher%0Adegree%20of%20generalization%20than%20existing%20behavior%20cloning%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16862v1&entry.124074799=Read"},
{"title": "Feature-prompting GBMSeg: One-Shot Reference Guided Training-Free Prompt\n  Engineering for Glomerular Basement Membrane Segmentation", "author": "Xueyu Liu and Guangze Shi and Rui Wang and Yexin Lai and Jianan Zhang and Lele Sun and Quan Yang and Yongfei Wu and MIng Li and Weixia Han and Wen Zheng", "abstract": "  Assessment of the glomerular basement membrane (GBM) in transmission electron\nmicroscopy (TEM) is crucial for diagnosing chronic kidney disease (CKD). The\nlack of domain-independent automatic segmentation tools for the GBM\nnecessitates an AI-based solution to automate the process. In this study, we\nintroduce GBMSeg, a training-free framework designed to automatically segment\nthe GBM in TEM images guided only by a one-shot annotated reference.\nSpecifically, GBMSeg first exploits the robust feature matching capabilities of\nthe pretrained foundation model to generate initial prompt points, then\nintroduces a series of novel automatic prompt engineering techniques across the\nfeature and physical space to optimize the prompt scheme. Finally, GBMSeg\nemploys a class-agnostic foundation segmentation model with the generated\nprompt scheme to obtain accurate segmentation results. Experimental results on\nour collected 2538 TEM images confirm that GBMSeg achieves superior\nsegmentation performance with a Dice similarity coefficient (DSC) of 87.27%\nusing only one labeled reference image in a training-free manner, outperforming\nrecently proposed one-shot or few-shot methods. In summary, GBMSeg introduces a\ndistinctive automatic prompt framework that facilitates robust\ndomain-independent segmentation performance without training, particularly\nadvancing the automatic prompting of foundation segmentation models for medical\nimages. Future work involves automating the thickness measurement of segmented\nGBM and quantifying pathological indicators, holding significant potential for\nadvancing pathology assessments in clinical applications. The source code is\navailable on https://github.com/SnowRain510/GBMSeg\n", "link": "http://arxiv.org/abs/2406.16271v1", "date": "2024-06-24", "relevancy": 2.4382, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4905}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4896}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature-prompting%20GBMSeg%3A%20One-Shot%20Reference%20Guided%20Training-Free%20Prompt%0A%20%20Engineering%20for%20Glomerular%20Basement%20Membrane%20Segmentation&body=Title%3A%20Feature-prompting%20GBMSeg%3A%20One-Shot%20Reference%20Guided%20Training-Free%20Prompt%0A%20%20Engineering%20for%20Glomerular%20Basement%20Membrane%20Segmentation%0AAuthor%3A%20Xueyu%20Liu%20and%20Guangze%20Shi%20and%20Rui%20Wang%20and%20Yexin%20Lai%20and%20Jianan%20Zhang%20and%20Lele%20Sun%20and%20Quan%20Yang%20and%20Yongfei%20Wu%20and%20MIng%20Li%20and%20Weixia%20Han%20and%20Wen%20Zheng%0AAbstract%3A%20%20%20Assessment%20of%20the%20glomerular%20basement%20membrane%20%28GBM%29%20in%20transmission%20electron%0Amicroscopy%20%28TEM%29%20is%20crucial%20for%20diagnosing%20chronic%20kidney%20disease%20%28CKD%29.%20The%0Alack%20of%20domain-independent%20automatic%20segmentation%20tools%20for%20the%20GBM%0Anecessitates%20an%20AI-based%20solution%20to%20automate%20the%20process.%20In%20this%20study%2C%20we%0Aintroduce%20GBMSeg%2C%20a%20training-free%20framework%20designed%20to%20automatically%20segment%0Athe%20GBM%20in%20TEM%20images%20guided%20only%20by%20a%20one-shot%20annotated%20reference.%0ASpecifically%2C%20GBMSeg%20first%20exploits%20the%20robust%20feature%20matching%20capabilities%20of%0Athe%20pretrained%20foundation%20model%20to%20generate%20initial%20prompt%20points%2C%20then%0Aintroduces%20a%20series%20of%20novel%20automatic%20prompt%20engineering%20techniques%20across%20the%0Afeature%20and%20physical%20space%20to%20optimize%20the%20prompt%20scheme.%20Finally%2C%20GBMSeg%0Aemploys%20a%20class-agnostic%20foundation%20segmentation%20model%20with%20the%20generated%0Aprompt%20scheme%20to%20obtain%20accurate%20segmentation%20results.%20Experimental%20results%20on%0Aour%20collected%202538%20TEM%20images%20confirm%20that%20GBMSeg%20achieves%20superior%0Asegmentation%20performance%20with%20a%20Dice%20similarity%20coefficient%20%28DSC%29%20of%2087.27%25%0Ausing%20only%20one%20labeled%20reference%20image%20in%20a%20training-free%20manner%2C%20outperforming%0Arecently%20proposed%20one-shot%20or%20few-shot%20methods.%20In%20summary%2C%20GBMSeg%20introduces%20a%0Adistinctive%20automatic%20prompt%20framework%20that%20facilitates%20robust%0Adomain-independent%20segmentation%20performance%20without%20training%2C%20particularly%0Aadvancing%20the%20automatic%20prompting%20of%20foundation%20segmentation%20models%20for%20medical%0Aimages.%20Future%20work%20involves%20automating%20the%20thickness%20measurement%20of%20segmented%0AGBM%20and%20quantifying%20pathological%20indicators%2C%20holding%20significant%20potential%20for%0Aadvancing%20pathology%20assessments%20in%20clinical%20applications.%20The%20source%20code%20is%0Aavailable%20on%20https%3A//github.com/SnowRain510/GBMSeg%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature-prompting%2520GBMSeg%253A%2520One-Shot%2520Reference%2520Guided%2520Training-Free%2520Prompt%250A%2520%2520Engineering%2520for%2520Glomerular%2520Basement%2520Membrane%2520Segmentation%26entry.906535625%3DXueyu%2520Liu%2520and%2520Guangze%2520Shi%2520and%2520Rui%2520Wang%2520and%2520Yexin%2520Lai%2520and%2520Jianan%2520Zhang%2520and%2520Lele%2520Sun%2520and%2520Quan%2520Yang%2520and%2520Yongfei%2520Wu%2520and%2520MIng%2520Li%2520and%2520Weixia%2520Han%2520and%2520Wen%2520Zheng%26entry.1292438233%3D%2520%2520Assessment%2520of%2520the%2520glomerular%2520basement%2520membrane%2520%2528GBM%2529%2520in%2520transmission%2520electron%250Amicroscopy%2520%2528TEM%2529%2520is%2520crucial%2520for%2520diagnosing%2520chronic%2520kidney%2520disease%2520%2528CKD%2529.%2520The%250Alack%2520of%2520domain-independent%2520automatic%2520segmentation%2520tools%2520for%2520the%2520GBM%250Anecessitates%2520an%2520AI-based%2520solution%2520to%2520automate%2520the%2520process.%2520In%2520this%2520study%252C%2520we%250Aintroduce%2520GBMSeg%252C%2520a%2520training-free%2520framework%2520designed%2520to%2520automatically%2520segment%250Athe%2520GBM%2520in%2520TEM%2520images%2520guided%2520only%2520by%2520a%2520one-shot%2520annotated%2520reference.%250ASpecifically%252C%2520GBMSeg%2520first%2520exploits%2520the%2520robust%2520feature%2520matching%2520capabilities%2520of%250Athe%2520pretrained%2520foundation%2520model%2520to%2520generate%2520initial%2520prompt%2520points%252C%2520then%250Aintroduces%2520a%2520series%2520of%2520novel%2520automatic%2520prompt%2520engineering%2520techniques%2520across%2520the%250Afeature%2520and%2520physical%2520space%2520to%2520optimize%2520the%2520prompt%2520scheme.%2520Finally%252C%2520GBMSeg%250Aemploys%2520a%2520class-agnostic%2520foundation%2520segmentation%2520model%2520with%2520the%2520generated%250Aprompt%2520scheme%2520to%2520obtain%2520accurate%2520segmentation%2520results.%2520Experimental%2520results%2520on%250Aour%2520collected%25202538%2520TEM%2520images%2520confirm%2520that%2520GBMSeg%2520achieves%2520superior%250Asegmentation%2520performance%2520with%2520a%2520Dice%2520similarity%2520coefficient%2520%2528DSC%2529%2520of%252087.27%2525%250Ausing%2520only%2520one%2520labeled%2520reference%2520image%2520in%2520a%2520training-free%2520manner%252C%2520outperforming%250Arecently%2520proposed%2520one-shot%2520or%2520few-shot%2520methods.%2520In%2520summary%252C%2520GBMSeg%2520introduces%2520a%250Adistinctive%2520automatic%2520prompt%2520framework%2520that%2520facilitates%2520robust%250Adomain-independent%2520segmentation%2520performance%2520without%2520training%252C%2520particularly%250Aadvancing%2520the%2520automatic%2520prompting%2520of%2520foundation%2520segmentation%2520models%2520for%2520medical%250Aimages.%2520Future%2520work%2520involves%2520automating%2520the%2520thickness%2520measurement%2520of%2520segmented%250AGBM%2520and%2520quantifying%2520pathological%2520indicators%252C%2520holding%2520significant%2520potential%2520for%250Aadvancing%2520pathology%2520assessments%2520in%2520clinical%2520applications.%2520The%2520source%2520code%2520is%250Aavailable%2520on%2520https%253A//github.com/SnowRain510/GBMSeg%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature-prompting%20GBMSeg%3A%20One-Shot%20Reference%20Guided%20Training-Free%20Prompt%0A%20%20Engineering%20for%20Glomerular%20Basement%20Membrane%20Segmentation&entry.906535625=Xueyu%20Liu%20and%20Guangze%20Shi%20and%20Rui%20Wang%20and%20Yexin%20Lai%20and%20Jianan%20Zhang%20and%20Lele%20Sun%20and%20Quan%20Yang%20and%20Yongfei%20Wu%20and%20MIng%20Li%20and%20Weixia%20Han%20and%20Wen%20Zheng&entry.1292438233=%20%20Assessment%20of%20the%20glomerular%20basement%20membrane%20%28GBM%29%20in%20transmission%20electron%0Amicroscopy%20%28TEM%29%20is%20crucial%20for%20diagnosing%20chronic%20kidney%20disease%20%28CKD%29.%20The%0Alack%20of%20domain-independent%20automatic%20segmentation%20tools%20for%20the%20GBM%0Anecessitates%20an%20AI-based%20solution%20to%20automate%20the%20process.%20In%20this%20study%2C%20we%0Aintroduce%20GBMSeg%2C%20a%20training-free%20framework%20designed%20to%20automatically%20segment%0Athe%20GBM%20in%20TEM%20images%20guided%20only%20by%20a%20one-shot%20annotated%20reference.%0ASpecifically%2C%20GBMSeg%20first%20exploits%20the%20robust%20feature%20matching%20capabilities%20of%0Athe%20pretrained%20foundation%20model%20to%20generate%20initial%20prompt%20points%2C%20then%0Aintroduces%20a%20series%20of%20novel%20automatic%20prompt%20engineering%20techniques%20across%20the%0Afeature%20and%20physical%20space%20to%20optimize%20the%20prompt%20scheme.%20Finally%2C%20GBMSeg%0Aemploys%20a%20class-agnostic%20foundation%20segmentation%20model%20with%20the%20generated%0Aprompt%20scheme%20to%20obtain%20accurate%20segmentation%20results.%20Experimental%20results%20on%0Aour%20collected%202538%20TEM%20images%20confirm%20that%20GBMSeg%20achieves%20superior%0Asegmentation%20performance%20with%20a%20Dice%20similarity%20coefficient%20%28DSC%29%20of%2087.27%25%0Ausing%20only%20one%20labeled%20reference%20image%20in%20a%20training-free%20manner%2C%20outperforming%0Arecently%20proposed%20one-shot%20or%20few-shot%20methods.%20In%20summary%2C%20GBMSeg%20introduces%20a%0Adistinctive%20automatic%20prompt%20framework%20that%20facilitates%20robust%0Adomain-independent%20segmentation%20performance%20without%20training%2C%20particularly%0Aadvancing%20the%20automatic%20prompting%20of%20foundation%20segmentation%20models%20for%20medical%0Aimages.%20Future%20work%20involves%20automating%20the%20thickness%20measurement%20of%20segmented%0AGBM%20and%20quantifying%20pathological%20indicators%2C%20holding%20significant%20potential%20for%0Aadvancing%20pathology%20assessments%20in%20clinical%20applications.%20The%20source%20code%20is%0Aavailable%20on%20https%3A//github.com/SnowRain510/GBMSeg%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16271v1&entry.124074799=Read"},
{"title": "Improving the Diversity of Bootstrapped DQN by Replacing Priors With\n  Noise", "author": "Li Meng and Morten Goodwin and Anis Yazidi and Paal Engelstad", "abstract": "  Q-learning is one of the most well-known Reinforcement Learning algorithms.\nThere have been tremendous efforts to develop this algorithm using neural\nnetworks. Bootstrapped Deep Q-Learning Network is amongst them. It utilizes\nmultiple neural network heads to introduce diversity into Q-learning. Diversity\ncan sometimes be viewed as the amount of reasonable moves an agent can take at\na given state, analogous to the definition of the exploration ratio in RL.\nThus, the performance of Bootstrapped Deep Q-Learning Network is deeply\nconnected with the level of diversity within the algorithm. In the original\nresearch, it was pointed out that a random prior could improve the performance\nof the model. In this article, we further explore the possibility of replacing\npriors with noise and sample the noise from a Gaussian distribution to\nintroduce more diversity into this algorithm. We conduct our experiment on the\nAtari benchmark and compare our algorithm to both the original and other\nrelated algorithms. The results show that our modification of the Bootstrapped\nDeep Q-Learning algorithm achieves significantly higher evaluation scores\nacross different types of Atari games. Thus, we conclude that replacing priors\nwith noise can improve Bootstrapped Deep Q-Learning's performance by ensuring\nthe integrity of diversities.\n", "link": "http://arxiv.org/abs/2203.01004v3", "date": "2024-06-24", "relevancy": 2.4346, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5007}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4849}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20the%20Diversity%20of%20Bootstrapped%20DQN%20by%20Replacing%20Priors%20With%0A%20%20Noise&body=Title%3A%20Improving%20the%20Diversity%20of%20Bootstrapped%20DQN%20by%20Replacing%20Priors%20With%0A%20%20Noise%0AAuthor%3A%20Li%20Meng%20and%20Morten%20Goodwin%20and%20Anis%20Yazidi%20and%20Paal%20Engelstad%0AAbstract%3A%20%20%20Q-learning%20is%20one%20of%20the%20most%20well-known%20Reinforcement%20Learning%20algorithms.%0AThere%20have%20been%20tremendous%20efforts%20to%20develop%20this%20algorithm%20using%20neural%0Anetworks.%20Bootstrapped%20Deep%20Q-Learning%20Network%20is%20amongst%20them.%20It%20utilizes%0Amultiple%20neural%20network%20heads%20to%20introduce%20diversity%20into%20Q-learning.%20Diversity%0Acan%20sometimes%20be%20viewed%20as%20the%20amount%20of%20reasonable%20moves%20an%20agent%20can%20take%20at%0Aa%20given%20state%2C%20analogous%20to%20the%20definition%20of%20the%20exploration%20ratio%20in%20RL.%0AThus%2C%20the%20performance%20of%20Bootstrapped%20Deep%20Q-Learning%20Network%20is%20deeply%0Aconnected%20with%20the%20level%20of%20diversity%20within%20the%20algorithm.%20In%20the%20original%0Aresearch%2C%20it%20was%20pointed%20out%20that%20a%20random%20prior%20could%20improve%20the%20performance%0Aof%20the%20model.%20In%20this%20article%2C%20we%20further%20explore%20the%20possibility%20of%20replacing%0Apriors%20with%20noise%20and%20sample%20the%20noise%20from%20a%20Gaussian%20distribution%20to%0Aintroduce%20more%20diversity%20into%20this%20algorithm.%20We%20conduct%20our%20experiment%20on%20the%0AAtari%20benchmark%20and%20compare%20our%20algorithm%20to%20both%20the%20original%20and%20other%0Arelated%20algorithms.%20The%20results%20show%20that%20our%20modification%20of%20the%20Bootstrapped%0ADeep%20Q-Learning%20algorithm%20achieves%20significantly%20higher%20evaluation%20scores%0Aacross%20different%20types%20of%20Atari%20games.%20Thus%2C%20we%20conclude%20that%20replacing%20priors%0Awith%20noise%20can%20improve%20Bootstrapped%20Deep%20Q-Learning%27s%20performance%20by%20ensuring%0Athe%20integrity%20of%20diversities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2203.01004v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520the%2520Diversity%2520of%2520Bootstrapped%2520DQN%2520by%2520Replacing%2520Priors%2520With%250A%2520%2520Noise%26entry.906535625%3DLi%2520Meng%2520and%2520Morten%2520Goodwin%2520and%2520Anis%2520Yazidi%2520and%2520Paal%2520Engelstad%26entry.1292438233%3D%2520%2520Q-learning%2520is%2520one%2520of%2520the%2520most%2520well-known%2520Reinforcement%2520Learning%2520algorithms.%250AThere%2520have%2520been%2520tremendous%2520efforts%2520to%2520develop%2520this%2520algorithm%2520using%2520neural%250Anetworks.%2520Bootstrapped%2520Deep%2520Q-Learning%2520Network%2520is%2520amongst%2520them.%2520It%2520utilizes%250Amultiple%2520neural%2520network%2520heads%2520to%2520introduce%2520diversity%2520into%2520Q-learning.%2520Diversity%250Acan%2520sometimes%2520be%2520viewed%2520as%2520the%2520amount%2520of%2520reasonable%2520moves%2520an%2520agent%2520can%2520take%2520at%250Aa%2520given%2520state%252C%2520analogous%2520to%2520the%2520definition%2520of%2520the%2520exploration%2520ratio%2520in%2520RL.%250AThus%252C%2520the%2520performance%2520of%2520Bootstrapped%2520Deep%2520Q-Learning%2520Network%2520is%2520deeply%250Aconnected%2520with%2520the%2520level%2520of%2520diversity%2520within%2520the%2520algorithm.%2520In%2520the%2520original%250Aresearch%252C%2520it%2520was%2520pointed%2520out%2520that%2520a%2520random%2520prior%2520could%2520improve%2520the%2520performance%250Aof%2520the%2520model.%2520In%2520this%2520article%252C%2520we%2520further%2520explore%2520the%2520possibility%2520of%2520replacing%250Apriors%2520with%2520noise%2520and%2520sample%2520the%2520noise%2520from%2520a%2520Gaussian%2520distribution%2520to%250Aintroduce%2520more%2520diversity%2520into%2520this%2520algorithm.%2520We%2520conduct%2520our%2520experiment%2520on%2520the%250AAtari%2520benchmark%2520and%2520compare%2520our%2520algorithm%2520to%2520both%2520the%2520original%2520and%2520other%250Arelated%2520algorithms.%2520The%2520results%2520show%2520that%2520our%2520modification%2520of%2520the%2520Bootstrapped%250ADeep%2520Q-Learning%2520algorithm%2520achieves%2520significantly%2520higher%2520evaluation%2520scores%250Aacross%2520different%2520types%2520of%2520Atari%2520games.%2520Thus%252C%2520we%2520conclude%2520that%2520replacing%2520priors%250Awith%2520noise%2520can%2520improve%2520Bootstrapped%2520Deep%2520Q-Learning%2527s%2520performance%2520by%2520ensuring%250Athe%2520integrity%2520of%2520diversities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2203.01004v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20the%20Diversity%20of%20Bootstrapped%20DQN%20by%20Replacing%20Priors%20With%0A%20%20Noise&entry.906535625=Li%20Meng%20and%20Morten%20Goodwin%20and%20Anis%20Yazidi%20and%20Paal%20Engelstad&entry.1292438233=%20%20Q-learning%20is%20one%20of%20the%20most%20well-known%20Reinforcement%20Learning%20algorithms.%0AThere%20have%20been%20tremendous%20efforts%20to%20develop%20this%20algorithm%20using%20neural%0Anetworks.%20Bootstrapped%20Deep%20Q-Learning%20Network%20is%20amongst%20them.%20It%20utilizes%0Amultiple%20neural%20network%20heads%20to%20introduce%20diversity%20into%20Q-learning.%20Diversity%0Acan%20sometimes%20be%20viewed%20as%20the%20amount%20of%20reasonable%20moves%20an%20agent%20can%20take%20at%0Aa%20given%20state%2C%20analogous%20to%20the%20definition%20of%20the%20exploration%20ratio%20in%20RL.%0AThus%2C%20the%20performance%20of%20Bootstrapped%20Deep%20Q-Learning%20Network%20is%20deeply%0Aconnected%20with%20the%20level%20of%20diversity%20within%20the%20algorithm.%20In%20the%20original%0Aresearch%2C%20it%20was%20pointed%20out%20that%20a%20random%20prior%20could%20improve%20the%20performance%0Aof%20the%20model.%20In%20this%20article%2C%20we%20further%20explore%20the%20possibility%20of%20replacing%0Apriors%20with%20noise%20and%20sample%20the%20noise%20from%20a%20Gaussian%20distribution%20to%0Aintroduce%20more%20diversity%20into%20this%20algorithm.%20We%20conduct%20our%20experiment%20on%20the%0AAtari%20benchmark%20and%20compare%20our%20algorithm%20to%20both%20the%20original%20and%20other%0Arelated%20algorithms.%20The%20results%20show%20that%20our%20modification%20of%20the%20Bootstrapped%0ADeep%20Q-Learning%20algorithm%20achieves%20significantly%20higher%20evaluation%20scores%0Aacross%20different%20types%20of%20Atari%20games.%20Thus%2C%20we%20conclude%20that%20replacing%20priors%0Awith%20noise%20can%20improve%20Bootstrapped%20Deep%20Q-Learning%27s%20performance%20by%20ensuring%0Athe%20integrity%20of%20diversities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2203.01004v3&entry.124074799=Read"},
{"title": "From Perfect to Noisy World Simulation: Customizable Embodied\n  Multi-modal Perturbations for SLAM Robustness Benchmarking", "author": "Xiaohao Xu and Tianyi Zhang and Sibo Wang and Xiang Li and Yongqi Chen and Ye Li and Bhiksha Raj and Matthew Johnson-Roberson and Xiaonan Huang", "abstract": "  Embodied agents require robust navigation systems to operate in unstructured\nenvironments, making the robustness of Simultaneous Localization and Mapping\n(SLAM) models critical to embodied agent autonomy. While real-world datasets\nare invaluable, simulation-based benchmarks offer a scalable approach for\nrobustness evaluations. However, the creation of a challenging and controllable\nnoisy world with diverse perturbations remains under-explored. To this end, we\npropose a novel, customizable pipeline for noisy data synthesis, aimed at\nassessing the resilience of multi-modal SLAM models against various\nperturbations. The pipeline comprises a comprehensive taxonomy of sensor and\nmotion perturbations for embodied multi-modal (specifically RGB-D) sensing,\ncategorized by their sources and propagation order, allowing for procedural\ncomposition. We also provide a toolbox for synthesizing these perturbations,\nenabling the transformation of clean environments into challenging noisy\nsimulations. Utilizing the pipeline, we instantiate the large-scale\nNoisy-Replica benchmark, which includes diverse perturbation types, to evaluate\nthe risk tolerance of existing advanced RGB-D SLAM models. Our extensive\nanalysis uncovers the susceptibilities of both neural (NeRF and Gaussian\nSplatting -based) and non-neural SLAM models to disturbances, despite their\ndemonstrated accuracy in standard benchmarks. Our code is publicly available at\nhttps://github.com/Xiaohao-Xu/SLAM-under-Perturbation.\n", "link": "http://arxiv.org/abs/2406.16850v1", "date": "2024-06-24", "relevancy": 2.4222, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6328}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6243}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Perfect%20to%20Noisy%20World%20Simulation%3A%20Customizable%20Embodied%0A%20%20Multi-modal%20Perturbations%20for%20SLAM%20Robustness%20Benchmarking&body=Title%3A%20From%20Perfect%20to%20Noisy%20World%20Simulation%3A%20Customizable%20Embodied%0A%20%20Multi-modal%20Perturbations%20for%20SLAM%20Robustness%20Benchmarking%0AAuthor%3A%20Xiaohao%20Xu%20and%20Tianyi%20Zhang%20and%20Sibo%20Wang%20and%20Xiang%20Li%20and%20Yongqi%20Chen%20and%20Ye%20Li%20and%20Bhiksha%20Raj%20and%20Matthew%20Johnson-Roberson%20and%20Xiaonan%20Huang%0AAbstract%3A%20%20%20Embodied%20agents%20require%20robust%20navigation%20systems%20to%20operate%20in%20unstructured%0Aenvironments%2C%20making%20the%20robustness%20of%20Simultaneous%20Localization%20and%20Mapping%0A%28SLAM%29%20models%20critical%20to%20embodied%20agent%20autonomy.%20While%20real-world%20datasets%0Aare%20invaluable%2C%20simulation-based%20benchmarks%20offer%20a%20scalable%20approach%20for%0Arobustness%20evaluations.%20However%2C%20the%20creation%20of%20a%20challenging%20and%20controllable%0Anoisy%20world%20with%20diverse%20perturbations%20remains%20under-explored.%20To%20this%20end%2C%20we%0Apropose%20a%20novel%2C%20customizable%20pipeline%20for%20noisy%20data%20synthesis%2C%20aimed%20at%0Aassessing%20the%20resilience%20of%20multi-modal%20SLAM%20models%20against%20various%0Aperturbations.%20The%20pipeline%20comprises%20a%20comprehensive%20taxonomy%20of%20sensor%20and%0Amotion%20perturbations%20for%20embodied%20multi-modal%20%28specifically%20RGB-D%29%20sensing%2C%0Acategorized%20by%20their%20sources%20and%20propagation%20order%2C%20allowing%20for%20procedural%0Acomposition.%20We%20also%20provide%20a%20toolbox%20for%20synthesizing%20these%20perturbations%2C%0Aenabling%20the%20transformation%20of%20clean%20environments%20into%20challenging%20noisy%0Asimulations.%20Utilizing%20the%20pipeline%2C%20we%20instantiate%20the%20large-scale%0ANoisy-Replica%20benchmark%2C%20which%20includes%20diverse%20perturbation%20types%2C%20to%20evaluate%0Athe%20risk%20tolerance%20of%20existing%20advanced%20RGB-D%20SLAM%20models.%20Our%20extensive%0Aanalysis%20uncovers%20the%20susceptibilities%20of%20both%20neural%20%28NeRF%20and%20Gaussian%0ASplatting%20-based%29%20and%20non-neural%20SLAM%20models%20to%20disturbances%2C%20despite%20their%0Ademonstrated%20accuracy%20in%20standard%20benchmarks.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Xiaohao-Xu/SLAM-under-Perturbation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Perfect%2520to%2520Noisy%2520World%2520Simulation%253A%2520Customizable%2520Embodied%250A%2520%2520Multi-modal%2520Perturbations%2520for%2520SLAM%2520Robustness%2520Benchmarking%26entry.906535625%3DXiaohao%2520Xu%2520and%2520Tianyi%2520Zhang%2520and%2520Sibo%2520Wang%2520and%2520Xiang%2520Li%2520and%2520Yongqi%2520Chen%2520and%2520Ye%2520Li%2520and%2520Bhiksha%2520Raj%2520and%2520Matthew%2520Johnson-Roberson%2520and%2520Xiaonan%2520Huang%26entry.1292438233%3D%2520%2520Embodied%2520agents%2520require%2520robust%2520navigation%2520systems%2520to%2520operate%2520in%2520unstructured%250Aenvironments%252C%2520making%2520the%2520robustness%2520of%2520Simultaneous%2520Localization%2520and%2520Mapping%250A%2528SLAM%2529%2520models%2520critical%2520to%2520embodied%2520agent%2520autonomy.%2520While%2520real-world%2520datasets%250Aare%2520invaluable%252C%2520simulation-based%2520benchmarks%2520offer%2520a%2520scalable%2520approach%2520for%250Arobustness%2520evaluations.%2520However%252C%2520the%2520creation%2520of%2520a%2520challenging%2520and%2520controllable%250Anoisy%2520world%2520with%2520diverse%2520perturbations%2520remains%2520under-explored.%2520To%2520this%2520end%252C%2520we%250Apropose%2520a%2520novel%252C%2520customizable%2520pipeline%2520for%2520noisy%2520data%2520synthesis%252C%2520aimed%2520at%250Aassessing%2520the%2520resilience%2520of%2520multi-modal%2520SLAM%2520models%2520against%2520various%250Aperturbations.%2520The%2520pipeline%2520comprises%2520a%2520comprehensive%2520taxonomy%2520of%2520sensor%2520and%250Amotion%2520perturbations%2520for%2520embodied%2520multi-modal%2520%2528specifically%2520RGB-D%2529%2520sensing%252C%250Acategorized%2520by%2520their%2520sources%2520and%2520propagation%2520order%252C%2520allowing%2520for%2520procedural%250Acomposition.%2520We%2520also%2520provide%2520a%2520toolbox%2520for%2520synthesizing%2520these%2520perturbations%252C%250Aenabling%2520the%2520transformation%2520of%2520clean%2520environments%2520into%2520challenging%2520noisy%250Asimulations.%2520Utilizing%2520the%2520pipeline%252C%2520we%2520instantiate%2520the%2520large-scale%250ANoisy-Replica%2520benchmark%252C%2520which%2520includes%2520diverse%2520perturbation%2520types%252C%2520to%2520evaluate%250Athe%2520risk%2520tolerance%2520of%2520existing%2520advanced%2520RGB-D%2520SLAM%2520models.%2520Our%2520extensive%250Aanalysis%2520uncovers%2520the%2520susceptibilities%2520of%2520both%2520neural%2520%2528NeRF%2520and%2520Gaussian%250ASplatting%2520-based%2529%2520and%2520non-neural%2520SLAM%2520models%2520to%2520disturbances%252C%2520despite%2520their%250Ademonstrated%2520accuracy%2520in%2520standard%2520benchmarks.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/Xiaohao-Xu/SLAM-under-Perturbation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Perfect%20to%20Noisy%20World%20Simulation%3A%20Customizable%20Embodied%0A%20%20Multi-modal%20Perturbations%20for%20SLAM%20Robustness%20Benchmarking&entry.906535625=Xiaohao%20Xu%20and%20Tianyi%20Zhang%20and%20Sibo%20Wang%20and%20Xiang%20Li%20and%20Yongqi%20Chen%20and%20Ye%20Li%20and%20Bhiksha%20Raj%20and%20Matthew%20Johnson-Roberson%20and%20Xiaonan%20Huang&entry.1292438233=%20%20Embodied%20agents%20require%20robust%20navigation%20systems%20to%20operate%20in%20unstructured%0Aenvironments%2C%20making%20the%20robustness%20of%20Simultaneous%20Localization%20and%20Mapping%0A%28SLAM%29%20models%20critical%20to%20embodied%20agent%20autonomy.%20While%20real-world%20datasets%0Aare%20invaluable%2C%20simulation-based%20benchmarks%20offer%20a%20scalable%20approach%20for%0Arobustness%20evaluations.%20However%2C%20the%20creation%20of%20a%20challenging%20and%20controllable%0Anoisy%20world%20with%20diverse%20perturbations%20remains%20under-explored.%20To%20this%20end%2C%20we%0Apropose%20a%20novel%2C%20customizable%20pipeline%20for%20noisy%20data%20synthesis%2C%20aimed%20at%0Aassessing%20the%20resilience%20of%20multi-modal%20SLAM%20models%20against%20various%0Aperturbations.%20The%20pipeline%20comprises%20a%20comprehensive%20taxonomy%20of%20sensor%20and%0Amotion%20perturbations%20for%20embodied%20multi-modal%20%28specifically%20RGB-D%29%20sensing%2C%0Acategorized%20by%20their%20sources%20and%20propagation%20order%2C%20allowing%20for%20procedural%0Acomposition.%20We%20also%20provide%20a%20toolbox%20for%20synthesizing%20these%20perturbations%2C%0Aenabling%20the%20transformation%20of%20clean%20environments%20into%20challenging%20noisy%0Asimulations.%20Utilizing%20the%20pipeline%2C%20we%20instantiate%20the%20large-scale%0ANoisy-Replica%20benchmark%2C%20which%20includes%20diverse%20perturbation%20types%2C%20to%20evaluate%0Athe%20risk%20tolerance%20of%20existing%20advanced%20RGB-D%20SLAM%20models.%20Our%20extensive%0Aanalysis%20uncovers%20the%20susceptibilities%20of%20both%20neural%20%28NeRF%20and%20Gaussian%0ASplatting%20-based%29%20and%20non-neural%20SLAM%20models%20to%20disturbances%2C%20despite%20their%0Ademonstrated%20accuracy%20in%20standard%20benchmarks.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/Xiaohao-Xu/SLAM-under-Perturbation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16850v1&entry.124074799=Read"},
{"title": "Continual Road-Scene Semantic Segmentation via Feature-Aligned Symmetric\n  Multi-Modal Network", "author": "Francesco Barbato and Elena Camuffo and Simone Milani and Pietro Zanuttigh", "abstract": "  State-of-the-art multimodal semantic segmentation strategies combining LiDAR\nand color data are usually designed on top of asymmetric information-sharing\nschemes and assume that both modalities are always available. This strong\nassumption may not hold in real-world scenarios, where sensors are prone to\nfailure or can face adverse conditions that make the acquired information\nunreliable. This problem is exacerbated when continual learning scenarios are\nconsidered since they have stringent data reliability constraints. In this\nwork, we re-frame the task of multimodal semantic segmentation by enforcing a\ntightly coupled feature representation and a symmetric information-sharing\nscheme, which allows our approach to work even when one of the input modalities\nis missing. We also introduce an ad-hoc class-incremental continual learning\nscheme, proving our approach's effectiveness and reliability even in\nsafety-critical settings, such as autonomous driving. We evaluate our approach\non the SemanticKITTI dataset, achieving impressive performances.\n", "link": "http://arxiv.org/abs/2308.04702v2", "date": "2024-06-24", "relevancy": 2.4221, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6297}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6089}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Road-Scene%20Semantic%20Segmentation%20via%20Feature-Aligned%20Symmetric%0A%20%20Multi-Modal%20Network&body=Title%3A%20Continual%20Road-Scene%20Semantic%20Segmentation%20via%20Feature-Aligned%20Symmetric%0A%20%20Multi-Modal%20Network%0AAuthor%3A%20Francesco%20Barbato%20and%20Elena%20Camuffo%20and%20Simone%20Milani%20and%20Pietro%20Zanuttigh%0AAbstract%3A%20%20%20State-of-the-art%20multimodal%20semantic%20segmentation%20strategies%20combining%20LiDAR%0Aand%20color%20data%20are%20usually%20designed%20on%20top%20of%20asymmetric%20information-sharing%0Aschemes%20and%20assume%20that%20both%20modalities%20are%20always%20available.%20This%20strong%0Aassumption%20may%20not%20hold%20in%20real-world%20scenarios%2C%20where%20sensors%20are%20prone%20to%0Afailure%20or%20can%20face%20adverse%20conditions%20that%20make%20the%20acquired%20information%0Aunreliable.%20This%20problem%20is%20exacerbated%20when%20continual%20learning%20scenarios%20are%0Aconsidered%20since%20they%20have%20stringent%20data%20reliability%20constraints.%20In%20this%0Awork%2C%20we%20re-frame%20the%20task%20of%20multimodal%20semantic%20segmentation%20by%20enforcing%20a%0Atightly%20coupled%20feature%20representation%20and%20a%20symmetric%20information-sharing%0Ascheme%2C%20which%20allows%20our%20approach%20to%20work%20even%20when%20one%20of%20the%20input%20modalities%0Ais%20missing.%20We%20also%20introduce%20an%20ad-hoc%20class-incremental%20continual%20learning%0Ascheme%2C%20proving%20our%20approach%27s%20effectiveness%20and%20reliability%20even%20in%0Asafety-critical%20settings%2C%20such%20as%20autonomous%20driving.%20We%20evaluate%20our%20approach%0Aon%20the%20SemanticKITTI%20dataset%2C%20achieving%20impressive%20performances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.04702v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Road-Scene%2520Semantic%2520Segmentation%2520via%2520Feature-Aligned%2520Symmetric%250A%2520%2520Multi-Modal%2520Network%26entry.906535625%3DFrancesco%2520Barbato%2520and%2520Elena%2520Camuffo%2520and%2520Simone%2520Milani%2520and%2520Pietro%2520Zanuttigh%26entry.1292438233%3D%2520%2520State-of-the-art%2520multimodal%2520semantic%2520segmentation%2520strategies%2520combining%2520LiDAR%250Aand%2520color%2520data%2520are%2520usually%2520designed%2520on%2520top%2520of%2520asymmetric%2520information-sharing%250Aschemes%2520and%2520assume%2520that%2520both%2520modalities%2520are%2520always%2520available.%2520This%2520strong%250Aassumption%2520may%2520not%2520hold%2520in%2520real-world%2520scenarios%252C%2520where%2520sensors%2520are%2520prone%2520to%250Afailure%2520or%2520can%2520face%2520adverse%2520conditions%2520that%2520make%2520the%2520acquired%2520information%250Aunreliable.%2520This%2520problem%2520is%2520exacerbated%2520when%2520continual%2520learning%2520scenarios%2520are%250Aconsidered%2520since%2520they%2520have%2520stringent%2520data%2520reliability%2520constraints.%2520In%2520this%250Awork%252C%2520we%2520re-frame%2520the%2520task%2520of%2520multimodal%2520semantic%2520segmentation%2520by%2520enforcing%2520a%250Atightly%2520coupled%2520feature%2520representation%2520and%2520a%2520symmetric%2520information-sharing%250Ascheme%252C%2520which%2520allows%2520our%2520approach%2520to%2520work%2520even%2520when%2520one%2520of%2520the%2520input%2520modalities%250Ais%2520missing.%2520We%2520also%2520introduce%2520an%2520ad-hoc%2520class-incremental%2520continual%2520learning%250Ascheme%252C%2520proving%2520our%2520approach%2527s%2520effectiveness%2520and%2520reliability%2520even%2520in%250Asafety-critical%2520settings%252C%2520such%2520as%2520autonomous%2520driving.%2520We%2520evaluate%2520our%2520approach%250Aon%2520the%2520SemanticKITTI%2520dataset%252C%2520achieving%2520impressive%2520performances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.04702v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Road-Scene%20Semantic%20Segmentation%20via%20Feature-Aligned%20Symmetric%0A%20%20Multi-Modal%20Network&entry.906535625=Francesco%20Barbato%20and%20Elena%20Camuffo%20and%20Simone%20Milani%20and%20Pietro%20Zanuttigh&entry.1292438233=%20%20State-of-the-art%20multimodal%20semantic%20segmentation%20strategies%20combining%20LiDAR%0Aand%20color%20data%20are%20usually%20designed%20on%20top%20of%20asymmetric%20information-sharing%0Aschemes%20and%20assume%20that%20both%20modalities%20are%20always%20available.%20This%20strong%0Aassumption%20may%20not%20hold%20in%20real-world%20scenarios%2C%20where%20sensors%20are%20prone%20to%0Afailure%20or%20can%20face%20adverse%20conditions%20that%20make%20the%20acquired%20information%0Aunreliable.%20This%20problem%20is%20exacerbated%20when%20continual%20learning%20scenarios%20are%0Aconsidered%20since%20they%20have%20stringent%20data%20reliability%20constraints.%20In%20this%0Awork%2C%20we%20re-frame%20the%20task%20of%20multimodal%20semantic%20segmentation%20by%20enforcing%20a%0Atightly%20coupled%20feature%20representation%20and%20a%20symmetric%20information-sharing%0Ascheme%2C%20which%20allows%20our%20approach%20to%20work%20even%20when%20one%20of%20the%20input%20modalities%0Ais%20missing.%20We%20also%20introduce%20an%20ad-hoc%20class-incremental%20continual%20learning%0Ascheme%2C%20proving%20our%20approach%27s%20effectiveness%20and%20reliability%20even%20in%0Asafety-critical%20settings%2C%20such%20as%20autonomous%20driving.%20We%20evaluate%20our%20approach%0Aon%20the%20SemanticKITTI%20dataset%2C%20achieving%20impressive%20performances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.04702v2&entry.124074799=Read"},
{"title": "Deep Reinforcement Learning: A Convex Optimization Approach", "author": "Ather Gattami", "abstract": "  In this paper, we consider reinforcement learning of nonlinear systems with\ncontinuous state and action spaces. We present an episodic learning algorithm,\nwhere we for each episode use convex optimization to find a two-layer neural\nnetwork approximation of the optimal $Q$-function. The convex optimization\napproach guarantees that the weights calculated at each episode are optimal,\nwith respect to the given sampled states and actions of the current episode.\nFor stable nonlinear systems, we show that the algorithm converges and that the\nconverging parameters of the trained neural network can be made arbitrarily\nclose to the optimal neural network parameters. In particular, if the\nregularization parameter in the training phase is given by $\\rho$, then the\nparameters of the trained neural network converge to $w$, where the distance\nbetween $w$ and the optimal parameters $w^\\star$ is bounded by\n$\\mathcal{O}(\\rho)$. That is, when the number of episodes goes to infinity,\nthere exists a constant $C$ such that \\[\n  \\|w-w^\\star\\| \\le C\\rho. \\]\n  In particular, our algorithm converges arbitrarily close to the optimal\nneural network parameters as the regularization parameter goes to zero. As a\nconsequence, our algorithm converges fast due to the polynomial-time\nconvergence of convex optimization algorithms.\n", "link": "http://arxiv.org/abs/2402.19212v6", "date": "2024-06-24", "relevancy": 2.422, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5061}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4856}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Reinforcement%20Learning%3A%20A%20Convex%20Optimization%20Approach&body=Title%3A%20Deep%20Reinforcement%20Learning%3A%20A%20Convex%20Optimization%20Approach%0AAuthor%3A%20Ather%20Gattami%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20consider%20reinforcement%20learning%20of%20nonlinear%20systems%20with%0Acontinuous%20state%20and%20action%20spaces.%20We%20present%20an%20episodic%20learning%20algorithm%2C%0Awhere%20we%20for%20each%20episode%20use%20convex%20optimization%20to%20find%20a%20two-layer%20neural%0Anetwork%20approximation%20of%20the%20optimal%20%24Q%24-function.%20The%20convex%20optimization%0Aapproach%20guarantees%20that%20the%20weights%20calculated%20at%20each%20episode%20are%20optimal%2C%0Awith%20respect%20to%20the%20given%20sampled%20states%20and%20actions%20of%20the%20current%20episode.%0AFor%20stable%20nonlinear%20systems%2C%20we%20show%20that%20the%20algorithm%20converges%20and%20that%20the%0Aconverging%20parameters%20of%20the%20trained%20neural%20network%20can%20be%20made%20arbitrarily%0Aclose%20to%20the%20optimal%20neural%20network%20parameters.%20In%20particular%2C%20if%20the%0Aregularization%20parameter%20in%20the%20training%20phase%20is%20given%20by%20%24%5Crho%24%2C%20then%20the%0Aparameters%20of%20the%20trained%20neural%20network%20converge%20to%20%24w%24%2C%20where%20the%20distance%0Abetween%20%24w%24%20and%20the%20optimal%20parameters%20%24w%5E%5Cstar%24%20is%20bounded%20by%0A%24%5Cmathcal%7BO%7D%28%5Crho%29%24.%20That%20is%2C%20when%20the%20number%20of%20episodes%20goes%20to%20infinity%2C%0Athere%20exists%20a%20constant%20%24C%24%20such%20that%20%5C%5B%0A%20%20%5C%7Cw-w%5E%5Cstar%5C%7C%20%5Cle%20C%5Crho.%20%5C%5D%0A%20%20In%20particular%2C%20our%20algorithm%20converges%20arbitrarily%20close%20to%20the%20optimal%0Aneural%20network%20parameters%20as%20the%20regularization%20parameter%20goes%20to%20zero.%20As%20a%0Aconsequence%2C%20our%20algorithm%20converges%20fast%20due%20to%20the%20polynomial-time%0Aconvergence%20of%20convex%20optimization%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19212v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Reinforcement%2520Learning%253A%2520A%2520Convex%2520Optimization%2520Approach%26entry.906535625%3DAther%2520Gattami%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520consider%2520reinforcement%2520learning%2520of%2520nonlinear%2520systems%2520with%250Acontinuous%2520state%2520and%2520action%2520spaces.%2520We%2520present%2520an%2520episodic%2520learning%2520algorithm%252C%250Awhere%2520we%2520for%2520each%2520episode%2520use%2520convex%2520optimization%2520to%2520find%2520a%2520two-layer%2520neural%250Anetwork%2520approximation%2520of%2520the%2520optimal%2520%2524Q%2524-function.%2520The%2520convex%2520optimization%250Aapproach%2520guarantees%2520that%2520the%2520weights%2520calculated%2520at%2520each%2520episode%2520are%2520optimal%252C%250Awith%2520respect%2520to%2520the%2520given%2520sampled%2520states%2520and%2520actions%2520of%2520the%2520current%2520episode.%250AFor%2520stable%2520nonlinear%2520systems%252C%2520we%2520show%2520that%2520the%2520algorithm%2520converges%2520and%2520that%2520the%250Aconverging%2520parameters%2520of%2520the%2520trained%2520neural%2520network%2520can%2520be%2520made%2520arbitrarily%250Aclose%2520to%2520the%2520optimal%2520neural%2520network%2520parameters.%2520In%2520particular%252C%2520if%2520the%250Aregularization%2520parameter%2520in%2520the%2520training%2520phase%2520is%2520given%2520by%2520%2524%255Crho%2524%252C%2520then%2520the%250Aparameters%2520of%2520the%2520trained%2520neural%2520network%2520converge%2520to%2520%2524w%2524%252C%2520where%2520the%2520distance%250Abetween%2520%2524w%2524%2520and%2520the%2520optimal%2520parameters%2520%2524w%255E%255Cstar%2524%2520is%2520bounded%2520by%250A%2524%255Cmathcal%257BO%257D%2528%255Crho%2529%2524.%2520That%2520is%252C%2520when%2520the%2520number%2520of%2520episodes%2520goes%2520to%2520infinity%252C%250Athere%2520exists%2520a%2520constant%2520%2524C%2524%2520such%2520that%2520%255C%255B%250A%2520%2520%255C%257Cw-w%255E%255Cstar%255C%257C%2520%255Cle%2520C%255Crho.%2520%255C%255D%250A%2520%2520In%2520particular%252C%2520our%2520algorithm%2520converges%2520arbitrarily%2520close%2520to%2520the%2520optimal%250Aneural%2520network%2520parameters%2520as%2520the%2520regularization%2520parameter%2520goes%2520to%2520zero.%2520As%2520a%250Aconsequence%252C%2520our%2520algorithm%2520converges%2520fast%2520due%2520to%2520the%2520polynomial-time%250Aconvergence%2520of%2520convex%2520optimization%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.19212v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Reinforcement%20Learning%3A%20A%20Convex%20Optimization%20Approach&entry.906535625=Ather%20Gattami&entry.1292438233=%20%20In%20this%20paper%2C%20we%20consider%20reinforcement%20learning%20of%20nonlinear%20systems%20with%0Acontinuous%20state%20and%20action%20spaces.%20We%20present%20an%20episodic%20learning%20algorithm%2C%0Awhere%20we%20for%20each%20episode%20use%20convex%20optimization%20to%20find%20a%20two-layer%20neural%0Anetwork%20approximation%20of%20the%20optimal%20%24Q%24-function.%20The%20convex%20optimization%0Aapproach%20guarantees%20that%20the%20weights%20calculated%20at%20each%20episode%20are%20optimal%2C%0Awith%20respect%20to%20the%20given%20sampled%20states%20and%20actions%20of%20the%20current%20episode.%0AFor%20stable%20nonlinear%20systems%2C%20we%20show%20that%20the%20algorithm%20converges%20and%20that%20the%0Aconverging%20parameters%20of%20the%20trained%20neural%20network%20can%20be%20made%20arbitrarily%0Aclose%20to%20the%20optimal%20neural%20network%20parameters.%20In%20particular%2C%20if%20the%0Aregularization%20parameter%20in%20the%20training%20phase%20is%20given%20by%20%24%5Crho%24%2C%20then%20the%0Aparameters%20of%20the%20trained%20neural%20network%20converge%20to%20%24w%24%2C%20where%20the%20distance%0Abetween%20%24w%24%20and%20the%20optimal%20parameters%20%24w%5E%5Cstar%24%20is%20bounded%20by%0A%24%5Cmathcal%7BO%7D%28%5Crho%29%24.%20That%20is%2C%20when%20the%20number%20of%20episodes%20goes%20to%20infinity%2C%0Athere%20exists%20a%20constant%20%24C%24%20such%20that%20%5C%5B%0A%20%20%5C%7Cw-w%5E%5Cstar%5C%7C%20%5Cle%20C%5Crho.%20%5C%5D%0A%20%20In%20particular%2C%20our%20algorithm%20converges%20arbitrarily%20close%20to%20the%20optimal%0Aneural%20network%20parameters%20as%20the%20regularization%20parameter%20goes%20to%20zero.%20As%20a%0Aconsequence%2C%20our%20algorithm%20converges%20fast%20due%20to%20the%20polynomial-time%0Aconvergence%20of%20convex%20optimization%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19212v6&entry.124074799=Read"},
{"title": "Segment Any Text: A Universal Approach for Robust, Efficient and\n  Adaptable Sentence Segmentation", "author": "Markus Frohmann and Igor Sterner and Ivan Vuli\u0107 and Benjamin Minixhofer and Markus Schedl", "abstract": "  Segmenting text into sentences plays an early and crucial role in many NLP\nsystems. This is commonly achieved by using rule-based or statistical methods\nrelying on lexical features such as punctuation. Although some recent works no\nlonger exclusively rely on punctuation, we find that no prior method achieves\nall of (i) robustness to missing punctuation, (ii) effective adaptability to\nnew domains, and (iii) high efficiency. We introduce a new model - Segment any\nText (SaT) - to solve this problem. To enhance robustness, we propose a new\npretraining scheme that ensures less reliance on punctuation. To address\nadaptability, we introduce an extra stage of parameter-efficient fine-tuning,\nestablishing state-of-the-art performance in distinct domains such as verses\nfrom lyrics and legal documents. Along the way, we introduce architectural\nmodifications that result in a threefold gain in speed over the previous state\nof the art and solve spurious reliance on context far in the future. Finally,\nwe introduce a variant of our model with fine-tuning on a diverse, multilingual\nmixture of sentence-segmented data, acting as a drop-in replacement and\nenhancement for existing segmentation tools. Overall, our contributions provide\na universal approach for segmenting any text. Our method outperforms all\nbaselines - including strong LLMs - across 8 corpora spanning diverse domains\nand languages, especially in practically relevant situations where text is\npoorly formatted. Our models and code, including documentation, are available\nat https://huggingface.co/segment-any-text under the MIT license.\n", "link": "http://arxiv.org/abs/2406.16678v1", "date": "2024-06-24", "relevancy": 2.4218, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.49}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4857}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20Any%20Text%3A%20A%20Universal%20Approach%20for%20Robust%2C%20Efficient%20and%0A%20%20Adaptable%20Sentence%20Segmentation&body=Title%3A%20Segment%20Any%20Text%3A%20A%20Universal%20Approach%20for%20Robust%2C%20Efficient%20and%0A%20%20Adaptable%20Sentence%20Segmentation%0AAuthor%3A%20Markus%20Frohmann%20and%20Igor%20Sterner%20and%20Ivan%20Vuli%C4%87%20and%20Benjamin%20Minixhofer%20and%20Markus%20Schedl%0AAbstract%3A%20%20%20Segmenting%20text%20into%20sentences%20plays%20an%20early%20and%20crucial%20role%20in%20many%20NLP%0Asystems.%20This%20is%20commonly%20achieved%20by%20using%20rule-based%20or%20statistical%20methods%0Arelying%20on%20lexical%20features%20such%20as%20punctuation.%20Although%20some%20recent%20works%20no%0Alonger%20exclusively%20rely%20on%20punctuation%2C%20we%20find%20that%20no%20prior%20method%20achieves%0Aall%20of%20%28i%29%20robustness%20to%20missing%20punctuation%2C%20%28ii%29%20effective%20adaptability%20to%0Anew%20domains%2C%20and%20%28iii%29%20high%20efficiency.%20We%20introduce%20a%20new%20model%20-%20Segment%20any%0AText%20%28SaT%29%20-%20to%20solve%20this%20problem.%20To%20enhance%20robustness%2C%20we%20propose%20a%20new%0Apretraining%20scheme%20that%20ensures%20less%20reliance%20on%20punctuation.%20To%20address%0Aadaptability%2C%20we%20introduce%20an%20extra%20stage%20of%20parameter-efficient%20fine-tuning%2C%0Aestablishing%20state-of-the-art%20performance%20in%20distinct%20domains%20such%20as%20verses%0Afrom%20lyrics%20and%20legal%20documents.%20Along%20the%20way%2C%20we%20introduce%20architectural%0Amodifications%20that%20result%20in%20a%20threefold%20gain%20in%20speed%20over%20the%20previous%20state%0Aof%20the%20art%20and%20solve%20spurious%20reliance%20on%20context%20far%20in%20the%20future.%20Finally%2C%0Awe%20introduce%20a%20variant%20of%20our%20model%20with%20fine-tuning%20on%20a%20diverse%2C%20multilingual%0Amixture%20of%20sentence-segmented%20data%2C%20acting%20as%20a%20drop-in%20replacement%20and%0Aenhancement%20for%20existing%20segmentation%20tools.%20Overall%2C%20our%20contributions%20provide%0Aa%20universal%20approach%20for%20segmenting%20any%20text.%20Our%20method%20outperforms%20all%0Abaselines%20-%20including%20strong%20LLMs%20-%20across%208%20corpora%20spanning%20diverse%20domains%0Aand%20languages%2C%20especially%20in%20practically%20relevant%20situations%20where%20text%20is%0Apoorly%20formatted.%20Our%20models%20and%20code%2C%20including%20documentation%2C%20are%20available%0Aat%20https%3A//huggingface.co/segment-any-text%20under%20the%20MIT%20license.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520Any%2520Text%253A%2520A%2520Universal%2520Approach%2520for%2520Robust%252C%2520Efficient%2520and%250A%2520%2520Adaptable%2520Sentence%2520Segmentation%26entry.906535625%3DMarkus%2520Frohmann%2520and%2520Igor%2520Sterner%2520and%2520Ivan%2520Vuli%25C4%2587%2520and%2520Benjamin%2520Minixhofer%2520and%2520Markus%2520Schedl%26entry.1292438233%3D%2520%2520Segmenting%2520text%2520into%2520sentences%2520plays%2520an%2520early%2520and%2520crucial%2520role%2520in%2520many%2520NLP%250Asystems.%2520This%2520is%2520commonly%2520achieved%2520by%2520using%2520rule-based%2520or%2520statistical%2520methods%250Arelying%2520on%2520lexical%2520features%2520such%2520as%2520punctuation.%2520Although%2520some%2520recent%2520works%2520no%250Alonger%2520exclusively%2520rely%2520on%2520punctuation%252C%2520we%2520find%2520that%2520no%2520prior%2520method%2520achieves%250Aall%2520of%2520%2528i%2529%2520robustness%2520to%2520missing%2520punctuation%252C%2520%2528ii%2529%2520effective%2520adaptability%2520to%250Anew%2520domains%252C%2520and%2520%2528iii%2529%2520high%2520efficiency.%2520We%2520introduce%2520a%2520new%2520model%2520-%2520Segment%2520any%250AText%2520%2528SaT%2529%2520-%2520to%2520solve%2520this%2520problem.%2520To%2520enhance%2520robustness%252C%2520we%2520propose%2520a%2520new%250Apretraining%2520scheme%2520that%2520ensures%2520less%2520reliance%2520on%2520punctuation.%2520To%2520address%250Aadaptability%252C%2520we%2520introduce%2520an%2520extra%2520stage%2520of%2520parameter-efficient%2520fine-tuning%252C%250Aestablishing%2520state-of-the-art%2520performance%2520in%2520distinct%2520domains%2520such%2520as%2520verses%250Afrom%2520lyrics%2520and%2520legal%2520documents.%2520Along%2520the%2520way%252C%2520we%2520introduce%2520architectural%250Amodifications%2520that%2520result%2520in%2520a%2520threefold%2520gain%2520in%2520speed%2520over%2520the%2520previous%2520state%250Aof%2520the%2520art%2520and%2520solve%2520spurious%2520reliance%2520on%2520context%2520far%2520in%2520the%2520future.%2520Finally%252C%250Awe%2520introduce%2520a%2520variant%2520of%2520our%2520model%2520with%2520fine-tuning%2520on%2520a%2520diverse%252C%2520multilingual%250Amixture%2520of%2520sentence-segmented%2520data%252C%2520acting%2520as%2520a%2520drop-in%2520replacement%2520and%250Aenhancement%2520for%2520existing%2520segmentation%2520tools.%2520Overall%252C%2520our%2520contributions%2520provide%250Aa%2520universal%2520approach%2520for%2520segmenting%2520any%2520text.%2520Our%2520method%2520outperforms%2520all%250Abaselines%2520-%2520including%2520strong%2520LLMs%2520-%2520across%25208%2520corpora%2520spanning%2520diverse%2520domains%250Aand%2520languages%252C%2520especially%2520in%2520practically%2520relevant%2520situations%2520where%2520text%2520is%250Apoorly%2520formatted.%2520Our%2520models%2520and%2520code%252C%2520including%2520documentation%252C%2520are%2520available%250Aat%2520https%253A//huggingface.co/segment-any-text%2520under%2520the%2520MIT%2520license.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20Any%20Text%3A%20A%20Universal%20Approach%20for%20Robust%2C%20Efficient%20and%0A%20%20Adaptable%20Sentence%20Segmentation&entry.906535625=Markus%20Frohmann%20and%20Igor%20Sterner%20and%20Ivan%20Vuli%C4%87%20and%20Benjamin%20Minixhofer%20and%20Markus%20Schedl&entry.1292438233=%20%20Segmenting%20text%20into%20sentences%20plays%20an%20early%20and%20crucial%20role%20in%20many%20NLP%0Asystems.%20This%20is%20commonly%20achieved%20by%20using%20rule-based%20or%20statistical%20methods%0Arelying%20on%20lexical%20features%20such%20as%20punctuation.%20Although%20some%20recent%20works%20no%0Alonger%20exclusively%20rely%20on%20punctuation%2C%20we%20find%20that%20no%20prior%20method%20achieves%0Aall%20of%20%28i%29%20robustness%20to%20missing%20punctuation%2C%20%28ii%29%20effective%20adaptability%20to%0Anew%20domains%2C%20and%20%28iii%29%20high%20efficiency.%20We%20introduce%20a%20new%20model%20-%20Segment%20any%0AText%20%28SaT%29%20-%20to%20solve%20this%20problem.%20To%20enhance%20robustness%2C%20we%20propose%20a%20new%0Apretraining%20scheme%20that%20ensures%20less%20reliance%20on%20punctuation.%20To%20address%0Aadaptability%2C%20we%20introduce%20an%20extra%20stage%20of%20parameter-efficient%20fine-tuning%2C%0Aestablishing%20state-of-the-art%20performance%20in%20distinct%20domains%20such%20as%20verses%0Afrom%20lyrics%20and%20legal%20documents.%20Along%20the%20way%2C%20we%20introduce%20architectural%0Amodifications%20that%20result%20in%20a%20threefold%20gain%20in%20speed%20over%20the%20previous%20state%0Aof%20the%20art%20and%20solve%20spurious%20reliance%20on%20context%20far%20in%20the%20future.%20Finally%2C%0Awe%20introduce%20a%20variant%20of%20our%20model%20with%20fine-tuning%20on%20a%20diverse%2C%20multilingual%0Amixture%20of%20sentence-segmented%20data%2C%20acting%20as%20a%20drop-in%20replacement%20and%0Aenhancement%20for%20existing%20segmentation%20tools.%20Overall%2C%20our%20contributions%20provide%0Aa%20universal%20approach%20for%20segmenting%20any%20text.%20Our%20method%20outperforms%20all%0Abaselines%20-%20including%20strong%20LLMs%20-%20across%208%20corpora%20spanning%20diverse%20domains%0Aand%20languages%2C%20especially%20in%20practically%20relevant%20situations%20where%20text%20is%0Apoorly%20formatted.%20Our%20models%20and%20code%2C%20including%20documentation%2C%20are%20available%0Aat%20https%3A//huggingface.co/segment-any-text%20under%20the%20MIT%20license.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16678v1&entry.124074799=Read"},
{"title": "Deep Prompt Multi-task Network for Abuse Language Detection", "author": "Jian Zhu and Yuping Ruan and Jingfei Chang and Wenhui Sun and Hui Wan and Jian Long and Cheng Luo", "abstract": "  The detection of abusive language remains a long-standing challenge with the\nextensive use of social networks. The detection task of abusive language\nsuffers from limited accuracy. We argue that the existing detection methods\nutilize the fine-tuning technique of the pre-trained language models (PLMs) to\nhandle downstream tasks. Hence, these methods fail to stimulate the general\nknowledge of the PLMs. To address the problem, we propose a novel Deep Prompt\nMulti-task Network (DPMN) for abuse language detection. Specifically, DPMN\nfirst attempts to design two forms of deep prompt tuning and light prompt\ntuning for the PLMs. The effects of different prompt lengths, tuning\nstrategies, and prompt initialization methods on detecting abusive language are\nstudied. In addition, we propose a Task Head based on Bi-LSTM and FFN, which\ncan be used as a short text classifier. Eventually, DPMN utilizes multi-task\nlearning to improve detection metrics further. The multi-task network has the\nfunction of transferring effective knowledge. The proposed DPMN is evaluated\nagainst eight typical methods on three public datasets: OLID, SOLID, and\nAbuseAnalyzer. The experimental results show that our DPMN outperforms the\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2403.05268v2", "date": "2024-06-24", "relevancy": 2.4187, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5031}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4751}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Prompt%20Multi-task%20Network%20for%20Abuse%20Language%20Detection&body=Title%3A%20Deep%20Prompt%20Multi-task%20Network%20for%20Abuse%20Language%20Detection%0AAuthor%3A%20Jian%20Zhu%20and%20Yuping%20Ruan%20and%20Jingfei%20Chang%20and%20Wenhui%20Sun%20and%20Hui%20Wan%20and%20Jian%20Long%20and%20Cheng%20Luo%0AAbstract%3A%20%20%20The%20detection%20of%20abusive%20language%20remains%20a%20long-standing%20challenge%20with%20the%0Aextensive%20use%20of%20social%20networks.%20The%20detection%20task%20of%20abusive%20language%0Asuffers%20from%20limited%20accuracy.%20We%20argue%20that%20the%20existing%20detection%20methods%0Autilize%20the%20fine-tuning%20technique%20of%20the%20pre-trained%20language%20models%20%28PLMs%29%20to%0Ahandle%20downstream%20tasks.%20Hence%2C%20these%20methods%20fail%20to%20stimulate%20the%20general%0Aknowledge%20of%20the%20PLMs.%20To%20address%20the%20problem%2C%20we%20propose%20a%20novel%20Deep%20Prompt%0AMulti-task%20Network%20%28DPMN%29%20for%20abuse%20language%20detection.%20Specifically%2C%20DPMN%0Afirst%20attempts%20to%20design%20two%20forms%20of%20deep%20prompt%20tuning%20and%20light%20prompt%0Atuning%20for%20the%20PLMs.%20The%20effects%20of%20different%20prompt%20lengths%2C%20tuning%0Astrategies%2C%20and%20prompt%20initialization%20methods%20on%20detecting%20abusive%20language%20are%0Astudied.%20In%20addition%2C%20we%20propose%20a%20Task%20Head%20based%20on%20Bi-LSTM%20and%20FFN%2C%20which%0Acan%20be%20used%20as%20a%20short%20text%20classifier.%20Eventually%2C%20DPMN%20utilizes%20multi-task%0Alearning%20to%20improve%20detection%20metrics%20further.%20The%20multi-task%20network%20has%20the%0Afunction%20of%20transferring%20effective%20knowledge.%20The%20proposed%20DPMN%20is%20evaluated%0Aagainst%20eight%20typical%20methods%20on%20three%20public%20datasets%3A%20OLID%2C%20SOLID%2C%20and%0AAbuseAnalyzer.%20The%20experimental%20results%20show%20that%20our%20DPMN%20outperforms%20the%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05268v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Prompt%2520Multi-task%2520Network%2520for%2520Abuse%2520Language%2520Detection%26entry.906535625%3DJian%2520Zhu%2520and%2520Yuping%2520Ruan%2520and%2520Jingfei%2520Chang%2520and%2520Wenhui%2520Sun%2520and%2520Hui%2520Wan%2520and%2520Jian%2520Long%2520and%2520Cheng%2520Luo%26entry.1292438233%3D%2520%2520The%2520detection%2520of%2520abusive%2520language%2520remains%2520a%2520long-standing%2520challenge%2520with%2520the%250Aextensive%2520use%2520of%2520social%2520networks.%2520The%2520detection%2520task%2520of%2520abusive%2520language%250Asuffers%2520from%2520limited%2520accuracy.%2520We%2520argue%2520that%2520the%2520existing%2520detection%2520methods%250Autilize%2520the%2520fine-tuning%2520technique%2520of%2520the%2520pre-trained%2520language%2520models%2520%2528PLMs%2529%2520to%250Ahandle%2520downstream%2520tasks.%2520Hence%252C%2520these%2520methods%2520fail%2520to%2520stimulate%2520the%2520general%250Aknowledge%2520of%2520the%2520PLMs.%2520To%2520address%2520the%2520problem%252C%2520we%2520propose%2520a%2520novel%2520Deep%2520Prompt%250AMulti-task%2520Network%2520%2528DPMN%2529%2520for%2520abuse%2520language%2520detection.%2520Specifically%252C%2520DPMN%250Afirst%2520attempts%2520to%2520design%2520two%2520forms%2520of%2520deep%2520prompt%2520tuning%2520and%2520light%2520prompt%250Atuning%2520for%2520the%2520PLMs.%2520The%2520effects%2520of%2520different%2520prompt%2520lengths%252C%2520tuning%250Astrategies%252C%2520and%2520prompt%2520initialization%2520methods%2520on%2520detecting%2520abusive%2520language%2520are%250Astudied.%2520In%2520addition%252C%2520we%2520propose%2520a%2520Task%2520Head%2520based%2520on%2520Bi-LSTM%2520and%2520FFN%252C%2520which%250Acan%2520be%2520used%2520as%2520a%2520short%2520text%2520classifier.%2520Eventually%252C%2520DPMN%2520utilizes%2520multi-task%250Alearning%2520to%2520improve%2520detection%2520metrics%2520further.%2520The%2520multi-task%2520network%2520has%2520the%250Afunction%2520of%2520transferring%2520effective%2520knowledge.%2520The%2520proposed%2520DPMN%2520is%2520evaluated%250Aagainst%2520eight%2520typical%2520methods%2520on%2520three%2520public%2520datasets%253A%2520OLID%252C%2520SOLID%252C%2520and%250AAbuseAnalyzer.%2520The%2520experimental%2520results%2520show%2520that%2520our%2520DPMN%2520outperforms%2520the%250Astate-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05268v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Prompt%20Multi-task%20Network%20for%20Abuse%20Language%20Detection&entry.906535625=Jian%20Zhu%20and%20Yuping%20Ruan%20and%20Jingfei%20Chang%20and%20Wenhui%20Sun%20and%20Hui%20Wan%20and%20Jian%20Long%20and%20Cheng%20Luo&entry.1292438233=%20%20The%20detection%20of%20abusive%20language%20remains%20a%20long-standing%20challenge%20with%20the%0Aextensive%20use%20of%20social%20networks.%20The%20detection%20task%20of%20abusive%20language%0Asuffers%20from%20limited%20accuracy.%20We%20argue%20that%20the%20existing%20detection%20methods%0Autilize%20the%20fine-tuning%20technique%20of%20the%20pre-trained%20language%20models%20%28PLMs%29%20to%0Ahandle%20downstream%20tasks.%20Hence%2C%20these%20methods%20fail%20to%20stimulate%20the%20general%0Aknowledge%20of%20the%20PLMs.%20To%20address%20the%20problem%2C%20we%20propose%20a%20novel%20Deep%20Prompt%0AMulti-task%20Network%20%28DPMN%29%20for%20abuse%20language%20detection.%20Specifically%2C%20DPMN%0Afirst%20attempts%20to%20design%20two%20forms%20of%20deep%20prompt%20tuning%20and%20light%20prompt%0Atuning%20for%20the%20PLMs.%20The%20effects%20of%20different%20prompt%20lengths%2C%20tuning%0Astrategies%2C%20and%20prompt%20initialization%20methods%20on%20detecting%20abusive%20language%20are%0Astudied.%20In%20addition%2C%20we%20propose%20a%20Task%20Head%20based%20on%20Bi-LSTM%20and%20FFN%2C%20which%0Acan%20be%20used%20as%20a%20short%20text%20classifier.%20Eventually%2C%20DPMN%20utilizes%20multi-task%0Alearning%20to%20improve%20detection%20metrics%20further.%20The%20multi-task%20network%20has%20the%0Afunction%20of%20transferring%20effective%20knowledge.%20The%20proposed%20DPMN%20is%20evaluated%0Aagainst%20eight%20typical%20methods%20on%20three%20public%20datasets%3A%20OLID%2C%20SOLID%2C%20and%0AAbuseAnalyzer.%20The%20experimental%20results%20show%20that%20our%20DPMN%20outperforms%20the%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05268v2&entry.124074799=Read"},
{"title": "Predefined Prototypes for Intra-Class Separation and Disentanglement", "author": "Antonio Almud\u00e9var and Th\u00e9o Mariotte and Alfonso Ortega and Marie Tahon and Luis Vicente and Antonio Miguel and Eduardo Lleida", "abstract": "  Prototypical Learning is based on the idea that there is a point (which we\ncall prototype) around which the embeddings of a class are clustered. It has\nshown promising results in scenarios with little labeled data or to design\nexplainable models. Typically, prototypes are either defined as the average of\nthe embeddings of a class or are designed to be trainable. In this work, we\npropose to predefine prototypes following human-specified criteria, which\nsimplify the training pipeline and brings different advantages. Specifically,\nin this work we explore two of these advantages: increasing the inter-class\nseparability of embeddings and disentangling embeddings with respect to\ndifferent variance factors, which can translate into the possibility of having\nexplainable predictions. Finally, we propose different experiments that help to\nunderstand our proposal and demonstrate empirically the mentioned advantages.\n", "link": "http://arxiv.org/abs/2406.16145v1", "date": "2024-06-23", "relevancy": 2.4104, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4992}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4985}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4486}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predefined%20Prototypes%20for%20Intra-Class%20Separation%20and%20Disentanglement&body=Title%3A%20Predefined%20Prototypes%20for%20Intra-Class%20Separation%20and%20Disentanglement%0AAuthor%3A%20Antonio%20Almud%C3%A9var%20and%20Th%C3%A9o%20Mariotte%20and%20Alfonso%20Ortega%20and%20Marie%20Tahon%20and%20Luis%20Vicente%20and%20Antonio%20Miguel%20and%20Eduardo%20Lleida%0AAbstract%3A%20%20%20Prototypical%20Learning%20is%20based%20on%20the%20idea%20that%20there%20is%20a%20point%20%28which%20we%0Acall%20prototype%29%20around%20which%20the%20embeddings%20of%20a%20class%20are%20clustered.%20It%20has%0Ashown%20promising%20results%20in%20scenarios%20with%20little%20labeled%20data%20or%20to%20design%0Aexplainable%20models.%20Typically%2C%20prototypes%20are%20either%20defined%20as%20the%20average%20of%0Athe%20embeddings%20of%20a%20class%20or%20are%20designed%20to%20be%20trainable.%20In%20this%20work%2C%20we%0Apropose%20to%20predefine%20prototypes%20following%20human-specified%20criteria%2C%20which%0Asimplify%20the%20training%20pipeline%20and%20brings%20different%20advantages.%20Specifically%2C%0Ain%20this%20work%20we%20explore%20two%20of%20these%20advantages%3A%20increasing%20the%20inter-class%0Aseparability%20of%20embeddings%20and%20disentangling%20embeddings%20with%20respect%20to%0Adifferent%20variance%20factors%2C%20which%20can%20translate%20into%20the%20possibility%20of%20having%0Aexplainable%20predictions.%20Finally%2C%20we%20propose%20different%20experiments%20that%20help%20to%0Aunderstand%20our%20proposal%20and%20demonstrate%20empirically%20the%20mentioned%20advantages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredefined%2520Prototypes%2520for%2520Intra-Class%2520Separation%2520and%2520Disentanglement%26entry.906535625%3DAntonio%2520Almud%25C3%25A9var%2520and%2520Th%25C3%25A9o%2520Mariotte%2520and%2520Alfonso%2520Ortega%2520and%2520Marie%2520Tahon%2520and%2520Luis%2520Vicente%2520and%2520Antonio%2520Miguel%2520and%2520Eduardo%2520Lleida%26entry.1292438233%3D%2520%2520Prototypical%2520Learning%2520is%2520based%2520on%2520the%2520idea%2520that%2520there%2520is%2520a%2520point%2520%2528which%2520we%250Acall%2520prototype%2529%2520around%2520which%2520the%2520embeddings%2520of%2520a%2520class%2520are%2520clustered.%2520It%2520has%250Ashown%2520promising%2520results%2520in%2520scenarios%2520with%2520little%2520labeled%2520data%2520or%2520to%2520design%250Aexplainable%2520models.%2520Typically%252C%2520prototypes%2520are%2520either%2520defined%2520as%2520the%2520average%2520of%250Athe%2520embeddings%2520of%2520a%2520class%2520or%2520are%2520designed%2520to%2520be%2520trainable.%2520In%2520this%2520work%252C%2520we%250Apropose%2520to%2520predefine%2520prototypes%2520following%2520human-specified%2520criteria%252C%2520which%250Asimplify%2520the%2520training%2520pipeline%2520and%2520brings%2520different%2520advantages.%2520Specifically%252C%250Ain%2520this%2520work%2520we%2520explore%2520two%2520of%2520these%2520advantages%253A%2520increasing%2520the%2520inter-class%250Aseparability%2520of%2520embeddings%2520and%2520disentangling%2520embeddings%2520with%2520respect%2520to%250Adifferent%2520variance%2520factors%252C%2520which%2520can%2520translate%2520into%2520the%2520possibility%2520of%2520having%250Aexplainable%2520predictions.%2520Finally%252C%2520we%2520propose%2520different%2520experiments%2520that%2520help%2520to%250Aunderstand%2520our%2520proposal%2520and%2520demonstrate%2520empirically%2520the%2520mentioned%2520advantages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predefined%20Prototypes%20for%20Intra-Class%20Separation%20and%20Disentanglement&entry.906535625=Antonio%20Almud%C3%A9var%20and%20Th%C3%A9o%20Mariotte%20and%20Alfonso%20Ortega%20and%20Marie%20Tahon%20and%20Luis%20Vicente%20and%20Antonio%20Miguel%20and%20Eduardo%20Lleida&entry.1292438233=%20%20Prototypical%20Learning%20is%20based%20on%20the%20idea%20that%20there%20is%20a%20point%20%28which%20we%0Acall%20prototype%29%20around%20which%20the%20embeddings%20of%20a%20class%20are%20clustered.%20It%20has%0Ashown%20promising%20results%20in%20scenarios%20with%20little%20labeled%20data%20or%20to%20design%0Aexplainable%20models.%20Typically%2C%20prototypes%20are%20either%20defined%20as%20the%20average%20of%0Athe%20embeddings%20of%20a%20class%20or%20are%20designed%20to%20be%20trainable.%20In%20this%20work%2C%20we%0Apropose%20to%20predefine%20prototypes%20following%20human-specified%20criteria%2C%20which%0Asimplify%20the%20training%20pipeline%20and%20brings%20different%20advantages.%20Specifically%2C%0Ain%20this%20work%20we%20explore%20two%20of%20these%20advantages%3A%20increasing%20the%20inter-class%0Aseparability%20of%20embeddings%20and%20disentangling%20embeddings%20with%20respect%20to%0Adifferent%20variance%20factors%2C%20which%20can%20translate%20into%20the%20possibility%20of%20having%0Aexplainable%20predictions.%20Finally%2C%20we%20propose%20different%20experiments%20that%20help%20to%0Aunderstand%20our%20proposal%20and%20demonstrate%20empirically%20the%20mentioned%20advantages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16145v1&entry.124074799=Read"},
{"title": "GC-Bench: A Benchmark Framework for Graph Condensation with New Insights", "author": "Shengbo Gong and Juntong Ni and Noveen Sachdeva and Carl Yang and Wei Jin", "abstract": "  Graph condensation (GC) is an emerging technique designed to learn a\nsignificantly smaller graph that retains the essential information of the\noriginal graph. This condensed graph has shown promise in accelerating graph\nneural networks while preserving performance comparable to those achieved with\nthe original, larger graphs. Additionally, this technique facilitates\ndownstream applications such as neural architecture search and enhances our\nunderstanding of redundancy in large graphs. Despite the rapid development of\nGC methods, a systematic evaluation framework remains absent, which is\nnecessary to clarify the critical designs for particular evaluative aspects.\nFurthermore, several meaningful questions have not been investigated, such as\nwhether GC inherently preserves certain graph properties and offers robustness\neven without targeted design efforts. In this paper, we introduce GC-Bench, a\ncomprehensive framework to evaluate recent GC methods across multiple\ndimensions and to generate new insights. Our experimental findings provide a\ndeeper insights into the GC process and the characteristics of condensed\ngraphs, guiding future efforts in enhancing performance and exploring new\napplications. Our code is available at\n\\url{https://github.com/Emory-Melody/GraphSlim/tree/main/benchmark}.\n", "link": "http://arxiv.org/abs/2406.16715v1", "date": "2024-06-24", "relevancy": 2.3939, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4999}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.472}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GC-Bench%3A%20A%20Benchmark%20Framework%20for%20Graph%20Condensation%20with%20New%20Insights&body=Title%3A%20GC-Bench%3A%20A%20Benchmark%20Framework%20for%20Graph%20Condensation%20with%20New%20Insights%0AAuthor%3A%20Shengbo%20Gong%20and%20Juntong%20Ni%20and%20Noveen%20Sachdeva%20and%20Carl%20Yang%20and%20Wei%20Jin%0AAbstract%3A%20%20%20Graph%20condensation%20%28GC%29%20is%20an%20emerging%20technique%20designed%20to%20learn%20a%0Asignificantly%20smaller%20graph%20that%20retains%20the%20essential%20information%20of%20the%0Aoriginal%20graph.%20This%20condensed%20graph%20has%20shown%20promise%20in%20accelerating%20graph%0Aneural%20networks%20while%20preserving%20performance%20comparable%20to%20those%20achieved%20with%0Athe%20original%2C%20larger%20graphs.%20Additionally%2C%20this%20technique%20facilitates%0Adownstream%20applications%20such%20as%20neural%20architecture%20search%20and%20enhances%20our%0Aunderstanding%20of%20redundancy%20in%20large%20graphs.%20Despite%20the%20rapid%20development%20of%0AGC%20methods%2C%20a%20systematic%20evaluation%20framework%20remains%20absent%2C%20which%20is%0Anecessary%20to%20clarify%20the%20critical%20designs%20for%20particular%20evaluative%20aspects.%0AFurthermore%2C%20several%20meaningful%20questions%20have%20not%20been%20investigated%2C%20such%20as%0Awhether%20GC%20inherently%20preserves%20certain%20graph%20properties%20and%20offers%20robustness%0Aeven%20without%20targeted%20design%20efforts.%20In%20this%20paper%2C%20we%20introduce%20GC-Bench%2C%20a%0Acomprehensive%20framework%20to%20evaluate%20recent%20GC%20methods%20across%20multiple%0Adimensions%20and%20to%20generate%20new%20insights.%20Our%20experimental%20findings%20provide%20a%0Adeeper%20insights%20into%20the%20GC%20process%20and%20the%20characteristics%20of%20condensed%0Agraphs%2C%20guiding%20future%20efforts%20in%20enhancing%20performance%20and%20exploring%20new%0Aapplications.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Emory-Melody/GraphSlim/tree/main/benchmark%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGC-Bench%253A%2520A%2520Benchmark%2520Framework%2520for%2520Graph%2520Condensation%2520with%2520New%2520Insights%26entry.906535625%3DShengbo%2520Gong%2520and%2520Juntong%2520Ni%2520and%2520Noveen%2520Sachdeva%2520and%2520Carl%2520Yang%2520and%2520Wei%2520Jin%26entry.1292438233%3D%2520%2520Graph%2520condensation%2520%2528GC%2529%2520is%2520an%2520emerging%2520technique%2520designed%2520to%2520learn%2520a%250Asignificantly%2520smaller%2520graph%2520that%2520retains%2520the%2520essential%2520information%2520of%2520the%250Aoriginal%2520graph.%2520This%2520condensed%2520graph%2520has%2520shown%2520promise%2520in%2520accelerating%2520graph%250Aneural%2520networks%2520while%2520preserving%2520performance%2520comparable%2520to%2520those%2520achieved%2520with%250Athe%2520original%252C%2520larger%2520graphs.%2520Additionally%252C%2520this%2520technique%2520facilitates%250Adownstream%2520applications%2520such%2520as%2520neural%2520architecture%2520search%2520and%2520enhances%2520our%250Aunderstanding%2520of%2520redundancy%2520in%2520large%2520graphs.%2520Despite%2520the%2520rapid%2520development%2520of%250AGC%2520methods%252C%2520a%2520systematic%2520evaluation%2520framework%2520remains%2520absent%252C%2520which%2520is%250Anecessary%2520to%2520clarify%2520the%2520critical%2520designs%2520for%2520particular%2520evaluative%2520aspects.%250AFurthermore%252C%2520several%2520meaningful%2520questions%2520have%2520not%2520been%2520investigated%252C%2520such%2520as%250Awhether%2520GC%2520inherently%2520preserves%2520certain%2520graph%2520properties%2520and%2520offers%2520robustness%250Aeven%2520without%2520targeted%2520design%2520efforts.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520GC-Bench%252C%2520a%250Acomprehensive%2520framework%2520to%2520evaluate%2520recent%2520GC%2520methods%2520across%2520multiple%250Adimensions%2520and%2520to%2520generate%2520new%2520insights.%2520Our%2520experimental%2520findings%2520provide%2520a%250Adeeper%2520insights%2520into%2520the%2520GC%2520process%2520and%2520the%2520characteristics%2520of%2520condensed%250Agraphs%252C%2520guiding%2520future%2520efforts%2520in%2520enhancing%2520performance%2520and%2520exploring%2520new%250Aapplications.%2520Our%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/Emory-Melody/GraphSlim/tree/main/benchmark%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GC-Bench%3A%20A%20Benchmark%20Framework%20for%20Graph%20Condensation%20with%20New%20Insights&entry.906535625=Shengbo%20Gong%20and%20Juntong%20Ni%20and%20Noveen%20Sachdeva%20and%20Carl%20Yang%20and%20Wei%20Jin&entry.1292438233=%20%20Graph%20condensation%20%28GC%29%20is%20an%20emerging%20technique%20designed%20to%20learn%20a%0Asignificantly%20smaller%20graph%20that%20retains%20the%20essential%20information%20of%20the%0Aoriginal%20graph.%20This%20condensed%20graph%20has%20shown%20promise%20in%20accelerating%20graph%0Aneural%20networks%20while%20preserving%20performance%20comparable%20to%20those%20achieved%20with%0Athe%20original%2C%20larger%20graphs.%20Additionally%2C%20this%20technique%20facilitates%0Adownstream%20applications%20such%20as%20neural%20architecture%20search%20and%20enhances%20our%0Aunderstanding%20of%20redundancy%20in%20large%20graphs.%20Despite%20the%20rapid%20development%20of%0AGC%20methods%2C%20a%20systematic%20evaluation%20framework%20remains%20absent%2C%20which%20is%0Anecessary%20to%20clarify%20the%20critical%20designs%20for%20particular%20evaluative%20aspects.%0AFurthermore%2C%20several%20meaningful%20questions%20have%20not%20been%20investigated%2C%20such%20as%0Awhether%20GC%20inherently%20preserves%20certain%20graph%20properties%20and%20offers%20robustness%0Aeven%20without%20targeted%20design%20efforts.%20In%20this%20paper%2C%20we%20introduce%20GC-Bench%2C%20a%0Acomprehensive%20framework%20to%20evaluate%20recent%20GC%20methods%20across%20multiple%0Adimensions%20and%20to%20generate%20new%20insights.%20Our%20experimental%20findings%20provide%20a%0Adeeper%20insights%20into%20the%20GC%20process%20and%20the%20characteristics%20of%20condensed%0Agraphs%2C%20guiding%20future%20efforts%20in%20enhancing%20performance%20and%20exploring%20new%0Aapplications.%20Our%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/Emory-Melody/GraphSlim/tree/main/benchmark%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16715v1&entry.124074799=Read"},
{"title": "Token-based Decision Criteria Are Suboptimal in In-context Learning", "author": "Hakaze Cho and Yoshihiro Sakai and Mariko Kato and Kenshiro Tanaka and Akira Ishii and Naoya Inoue", "abstract": "  In-Context Learning (ICL) typically utilizes classification criteria from\nprobabilities of manually selected label tokens. However, we argue that such\ntoken-based classification criteria lead to suboptimal decision boundaries,\ndespite delicate calibrations through translation and constrained rotation. To\naddress this problem, we propose Hidden Calibration, which renounces token\nprobabilities and uses the nearest centroid classifier on the LM's last hidden\nstates. In detail, we use the nearest centroid classification on the hidden\nstates, assigning the category of the nearest centroid previously observed from\na few-shot calibration set to the test sample as the predicted label. Our\nexperiments on 3 models and 10 classification datasets indicate that Hidden\nCalibration consistently outperforms current token-based calibrations by about\n20%. Our further analysis demonstrates that Hidden Calibration finds better\nclassification criteria with less inter-categories overlap, and LMs provide\nlinearly separable intra-category clusters with the help of demonstrations,\nwhich supports Hidden Calibration and gives new insights into the conventional\nICL.\n", "link": "http://arxiv.org/abs/2406.16535v1", "date": "2024-06-24", "relevancy": 2.3925, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4875}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4777}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token-based%20Decision%20Criteria%20Are%20Suboptimal%20in%20In-context%20Learning&body=Title%3A%20Token-based%20Decision%20Criteria%20Are%20Suboptimal%20in%20In-context%20Learning%0AAuthor%3A%20Hakaze%20Cho%20and%20Yoshihiro%20Sakai%20and%20Mariko%20Kato%20and%20Kenshiro%20Tanaka%20and%20Akira%20Ishii%20and%20Naoya%20Inoue%0AAbstract%3A%20%20%20In-Context%20Learning%20%28ICL%29%20typically%20utilizes%20classification%20criteria%20from%0Aprobabilities%20of%20manually%20selected%20label%20tokens.%20However%2C%20we%20argue%20that%20such%0Atoken-based%20classification%20criteria%20lead%20to%20suboptimal%20decision%20boundaries%2C%0Adespite%20delicate%20calibrations%20through%20translation%20and%20constrained%20rotation.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20Hidden%20Calibration%2C%20which%20renounces%20token%0Aprobabilities%20and%20uses%20the%20nearest%20centroid%20classifier%20on%20the%20LM%27s%20last%20hidden%0Astates.%20In%20detail%2C%20we%20use%20the%20nearest%20centroid%20classification%20on%20the%20hidden%0Astates%2C%20assigning%20the%20category%20of%20the%20nearest%20centroid%20previously%20observed%20from%0Aa%20few-shot%20calibration%20set%20to%20the%20test%20sample%20as%20the%20predicted%20label.%20Our%0Aexperiments%20on%203%20models%20and%2010%20classification%20datasets%20indicate%20that%20Hidden%0ACalibration%20consistently%20outperforms%20current%20token-based%20calibrations%20by%20about%0A20%25.%20Our%20further%20analysis%20demonstrates%20that%20Hidden%20Calibration%20finds%20better%0Aclassification%20criteria%20with%20less%20inter-categories%20overlap%2C%20and%20LMs%20provide%0Alinearly%20separable%20intra-category%20clusters%20with%20the%20help%20of%20demonstrations%2C%0Awhich%20supports%20Hidden%20Calibration%20and%20gives%20new%20insights%20into%20the%20conventional%0AICL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken-based%2520Decision%2520Criteria%2520Are%2520Suboptimal%2520in%2520In-context%2520Learning%26entry.906535625%3DHakaze%2520Cho%2520and%2520Yoshihiro%2520Sakai%2520and%2520Mariko%2520Kato%2520and%2520Kenshiro%2520Tanaka%2520and%2520Akira%2520Ishii%2520and%2520Naoya%2520Inoue%26entry.1292438233%3D%2520%2520In-Context%2520Learning%2520%2528ICL%2529%2520typically%2520utilizes%2520classification%2520criteria%2520from%250Aprobabilities%2520of%2520manually%2520selected%2520label%2520tokens.%2520However%252C%2520we%2520argue%2520that%2520such%250Atoken-based%2520classification%2520criteria%2520lead%2520to%2520suboptimal%2520decision%2520boundaries%252C%250Adespite%2520delicate%2520calibrations%2520through%2520translation%2520and%2520constrained%2520rotation.%2520To%250Aaddress%2520this%2520problem%252C%2520we%2520propose%2520Hidden%2520Calibration%252C%2520which%2520renounces%2520token%250Aprobabilities%2520and%2520uses%2520the%2520nearest%2520centroid%2520classifier%2520on%2520the%2520LM%2527s%2520last%2520hidden%250Astates.%2520In%2520detail%252C%2520we%2520use%2520the%2520nearest%2520centroid%2520classification%2520on%2520the%2520hidden%250Astates%252C%2520assigning%2520the%2520category%2520of%2520the%2520nearest%2520centroid%2520previously%2520observed%2520from%250Aa%2520few-shot%2520calibration%2520set%2520to%2520the%2520test%2520sample%2520as%2520the%2520predicted%2520label.%2520Our%250Aexperiments%2520on%25203%2520models%2520and%252010%2520classification%2520datasets%2520indicate%2520that%2520Hidden%250ACalibration%2520consistently%2520outperforms%2520current%2520token-based%2520calibrations%2520by%2520about%250A20%2525.%2520Our%2520further%2520analysis%2520demonstrates%2520that%2520Hidden%2520Calibration%2520finds%2520better%250Aclassification%2520criteria%2520with%2520less%2520inter-categories%2520overlap%252C%2520and%2520LMs%2520provide%250Alinearly%2520separable%2520intra-category%2520clusters%2520with%2520the%2520help%2520of%2520demonstrations%252C%250Awhich%2520supports%2520Hidden%2520Calibration%2520and%2520gives%2520new%2520insights%2520into%2520the%2520conventional%250AICL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token-based%20Decision%20Criteria%20Are%20Suboptimal%20in%20In-context%20Learning&entry.906535625=Hakaze%20Cho%20and%20Yoshihiro%20Sakai%20and%20Mariko%20Kato%20and%20Kenshiro%20Tanaka%20and%20Akira%20Ishii%20and%20Naoya%20Inoue&entry.1292438233=%20%20In-Context%20Learning%20%28ICL%29%20typically%20utilizes%20classification%20criteria%20from%0Aprobabilities%20of%20manually%20selected%20label%20tokens.%20However%2C%20we%20argue%20that%20such%0Atoken-based%20classification%20criteria%20lead%20to%20suboptimal%20decision%20boundaries%2C%0Adespite%20delicate%20calibrations%20through%20translation%20and%20constrained%20rotation.%20To%0Aaddress%20this%20problem%2C%20we%20propose%20Hidden%20Calibration%2C%20which%20renounces%20token%0Aprobabilities%20and%20uses%20the%20nearest%20centroid%20classifier%20on%20the%20LM%27s%20last%20hidden%0Astates.%20In%20detail%2C%20we%20use%20the%20nearest%20centroid%20classification%20on%20the%20hidden%0Astates%2C%20assigning%20the%20category%20of%20the%20nearest%20centroid%20previously%20observed%20from%0Aa%20few-shot%20calibration%20set%20to%20the%20test%20sample%20as%20the%20predicted%20label.%20Our%0Aexperiments%20on%203%20models%20and%2010%20classification%20datasets%20indicate%20that%20Hidden%0ACalibration%20consistently%20outperforms%20current%20token-based%20calibrations%20by%20about%0A20%25.%20Our%20further%20analysis%20demonstrates%20that%20Hidden%20Calibration%20finds%20better%0Aclassification%20criteria%20with%20less%20inter-categories%20overlap%2C%20and%20LMs%20provide%0Alinearly%20separable%20intra-category%20clusters%20with%20the%20help%20of%20demonstrations%2C%0Awhich%20supports%20Hidden%20Calibration%20and%20gives%20new%20insights%20into%20the%20conventional%0AICL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16535v1&entry.124074799=Read"},
{"title": "Seeking Certainty In Uncertainty: Dual-Stage Unified Framework Solving\n  Uncertainty in Dynamic Facial Expression Recognition", "author": "Haoran Wang and Xinji Mai and Zeng Tao and Xuan Tong and Junxiong Lin and Yan Wang and Jiawen Yu and Boyang Wang and Shaoqi Yan and Qing Zhao and Ziheng Zhou and Shuyong Gao and Wenqiang Zhang", "abstract": "  The contemporary state-of-the-art of Dynamic Facial Expression Recognition\n(DFER) technology facilitates remarkable progress by deriving emotional\nmappings of facial expressions from video content, underpinned by training on\nvoluminous datasets. Yet, the DFER datasets encompass a substantial volume of\nnoise data. Noise arises from low-quality captures that defy logical labeling,\nand instances that suffer from mislabeling due to annotation bias, engendering\ntwo principal types of uncertainty: the uncertainty regarding data usability\nand the uncertainty concerning label reliability. Addressing the two types of\nuncertainty, we have meticulously crafted a two-stage framework aiming at\n\\textbf{S}eeking \\textbf{C}ertain data \\textbf{I}n extensive \\textbf{U}ncertain\ndata (SCIU). This initiative aims to purge the DFER datasets of these\nuncertainties, thereby ensuring that only clean, verified data is employed in\ntraining processes. To mitigate the issue of low-quality samples, we introduce\nthe Coarse-Grained Pruning (CGP) stage, which assesses sample weights and\nprunes those deemed unusable due to their low weight. For samples with\nincorrect annotations, the Fine-Grained Correction (FGC) stage evaluates\nprediction stability to rectify mislabeled data. Moreover, SCIU is conceived as\na universally compatible, plug-and-play framework, tailored to integrate\nseamlessly with prevailing DFER methodologies. Rigorous experiments across\nprevalent DFER datasets and against numerous benchmark methods substantiates\nSCIU's capacity to markedly elevate performance metrics.\n", "link": "http://arxiv.org/abs/2406.16473v1", "date": "2024-06-24", "relevancy": 2.3713, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6392}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.6096}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeking%20Certainty%20In%20Uncertainty%3A%20Dual-Stage%20Unified%20Framework%20Solving%0A%20%20Uncertainty%20in%20Dynamic%20Facial%20Expression%20Recognition&body=Title%3A%20Seeking%20Certainty%20In%20Uncertainty%3A%20Dual-Stage%20Unified%20Framework%20Solving%0A%20%20Uncertainty%20in%20Dynamic%20Facial%20Expression%20Recognition%0AAuthor%3A%20Haoran%20Wang%20and%20Xinji%20Mai%20and%20Zeng%20Tao%20and%20Xuan%20Tong%20and%20Junxiong%20Lin%20and%20Yan%20Wang%20and%20Jiawen%20Yu%20and%20Boyang%20Wang%20and%20Shaoqi%20Yan%20and%20Qing%20Zhao%20and%20Ziheng%20Zhou%20and%20Shuyong%20Gao%20and%20Wenqiang%20Zhang%0AAbstract%3A%20%20%20The%20contemporary%20state-of-the-art%20of%20Dynamic%20Facial%20Expression%20Recognition%0A%28DFER%29%20technology%20facilitates%20remarkable%20progress%20by%20deriving%20emotional%0Amappings%20of%20facial%20expressions%20from%20video%20content%2C%20underpinned%20by%20training%20on%0Avoluminous%20datasets.%20Yet%2C%20the%20DFER%20datasets%20encompass%20a%20substantial%20volume%20of%0Anoise%20data.%20Noise%20arises%20from%20low-quality%20captures%20that%20defy%20logical%20labeling%2C%0Aand%20instances%20that%20suffer%20from%20mislabeling%20due%20to%20annotation%20bias%2C%20engendering%0Atwo%20principal%20types%20of%20uncertainty%3A%20the%20uncertainty%20regarding%20data%20usability%0Aand%20the%20uncertainty%20concerning%20label%20reliability.%20Addressing%20the%20two%20types%20of%0Auncertainty%2C%20we%20have%20meticulously%20crafted%20a%20two-stage%20framework%20aiming%20at%0A%5Ctextbf%7BS%7Deeking%20%5Ctextbf%7BC%7Dertain%20data%20%5Ctextbf%7BI%7Dn%20extensive%20%5Ctextbf%7BU%7Dncertain%0Adata%20%28SCIU%29.%20This%20initiative%20aims%20to%20purge%20the%20DFER%20datasets%20of%20these%0Auncertainties%2C%20thereby%20ensuring%20that%20only%20clean%2C%20verified%20data%20is%20employed%20in%0Atraining%20processes.%20To%20mitigate%20the%20issue%20of%20low-quality%20samples%2C%20we%20introduce%0Athe%20Coarse-Grained%20Pruning%20%28CGP%29%20stage%2C%20which%20assesses%20sample%20weights%20and%0Aprunes%20those%20deemed%20unusable%20due%20to%20their%20low%20weight.%20For%20samples%20with%0Aincorrect%20annotations%2C%20the%20Fine-Grained%20Correction%20%28FGC%29%20stage%20evaluates%0Aprediction%20stability%20to%20rectify%20mislabeled%20data.%20Moreover%2C%20SCIU%20is%20conceived%20as%0Aa%20universally%20compatible%2C%20plug-and-play%20framework%2C%20tailored%20to%20integrate%0Aseamlessly%20with%20prevailing%20DFER%20methodologies.%20Rigorous%20experiments%20across%0Aprevalent%20DFER%20datasets%20and%20against%20numerous%20benchmark%20methods%20substantiates%0ASCIU%27s%20capacity%20to%20markedly%20elevate%20performance%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeking%2520Certainty%2520In%2520Uncertainty%253A%2520Dual-Stage%2520Unified%2520Framework%2520Solving%250A%2520%2520Uncertainty%2520in%2520Dynamic%2520Facial%2520Expression%2520Recognition%26entry.906535625%3DHaoran%2520Wang%2520and%2520Xinji%2520Mai%2520and%2520Zeng%2520Tao%2520and%2520Xuan%2520Tong%2520and%2520Junxiong%2520Lin%2520and%2520Yan%2520Wang%2520and%2520Jiawen%2520Yu%2520and%2520Boyang%2520Wang%2520and%2520Shaoqi%2520Yan%2520and%2520Qing%2520Zhao%2520and%2520Ziheng%2520Zhou%2520and%2520Shuyong%2520Gao%2520and%2520Wenqiang%2520Zhang%26entry.1292438233%3D%2520%2520The%2520contemporary%2520state-of-the-art%2520of%2520Dynamic%2520Facial%2520Expression%2520Recognition%250A%2528DFER%2529%2520technology%2520facilitates%2520remarkable%2520progress%2520by%2520deriving%2520emotional%250Amappings%2520of%2520facial%2520expressions%2520from%2520video%2520content%252C%2520underpinned%2520by%2520training%2520on%250Avoluminous%2520datasets.%2520Yet%252C%2520the%2520DFER%2520datasets%2520encompass%2520a%2520substantial%2520volume%2520of%250Anoise%2520data.%2520Noise%2520arises%2520from%2520low-quality%2520captures%2520that%2520defy%2520logical%2520labeling%252C%250Aand%2520instances%2520that%2520suffer%2520from%2520mislabeling%2520due%2520to%2520annotation%2520bias%252C%2520engendering%250Atwo%2520principal%2520types%2520of%2520uncertainty%253A%2520the%2520uncertainty%2520regarding%2520data%2520usability%250Aand%2520the%2520uncertainty%2520concerning%2520label%2520reliability.%2520Addressing%2520the%2520two%2520types%2520of%250Auncertainty%252C%2520we%2520have%2520meticulously%2520crafted%2520a%2520two-stage%2520framework%2520aiming%2520at%250A%255Ctextbf%257BS%257Deeking%2520%255Ctextbf%257BC%257Dertain%2520data%2520%255Ctextbf%257BI%257Dn%2520extensive%2520%255Ctextbf%257BU%257Dncertain%250Adata%2520%2528SCIU%2529.%2520This%2520initiative%2520aims%2520to%2520purge%2520the%2520DFER%2520datasets%2520of%2520these%250Auncertainties%252C%2520thereby%2520ensuring%2520that%2520only%2520clean%252C%2520verified%2520data%2520is%2520employed%2520in%250Atraining%2520processes.%2520To%2520mitigate%2520the%2520issue%2520of%2520low-quality%2520samples%252C%2520we%2520introduce%250Athe%2520Coarse-Grained%2520Pruning%2520%2528CGP%2529%2520stage%252C%2520which%2520assesses%2520sample%2520weights%2520and%250Aprunes%2520those%2520deemed%2520unusable%2520due%2520to%2520their%2520low%2520weight.%2520For%2520samples%2520with%250Aincorrect%2520annotations%252C%2520the%2520Fine-Grained%2520Correction%2520%2528FGC%2529%2520stage%2520evaluates%250Aprediction%2520stability%2520to%2520rectify%2520mislabeled%2520data.%2520Moreover%252C%2520SCIU%2520is%2520conceived%2520as%250Aa%2520universally%2520compatible%252C%2520plug-and-play%2520framework%252C%2520tailored%2520to%2520integrate%250Aseamlessly%2520with%2520prevailing%2520DFER%2520methodologies.%2520Rigorous%2520experiments%2520across%250Aprevalent%2520DFER%2520datasets%2520and%2520against%2520numerous%2520benchmark%2520methods%2520substantiates%250ASCIU%2527s%2520capacity%2520to%2520markedly%2520elevate%2520performance%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeking%20Certainty%20In%20Uncertainty%3A%20Dual-Stage%20Unified%20Framework%20Solving%0A%20%20Uncertainty%20in%20Dynamic%20Facial%20Expression%20Recognition&entry.906535625=Haoran%20Wang%20and%20Xinji%20Mai%20and%20Zeng%20Tao%20and%20Xuan%20Tong%20and%20Junxiong%20Lin%20and%20Yan%20Wang%20and%20Jiawen%20Yu%20and%20Boyang%20Wang%20and%20Shaoqi%20Yan%20and%20Qing%20Zhao%20and%20Ziheng%20Zhou%20and%20Shuyong%20Gao%20and%20Wenqiang%20Zhang&entry.1292438233=%20%20The%20contemporary%20state-of-the-art%20of%20Dynamic%20Facial%20Expression%20Recognition%0A%28DFER%29%20technology%20facilitates%20remarkable%20progress%20by%20deriving%20emotional%0Amappings%20of%20facial%20expressions%20from%20video%20content%2C%20underpinned%20by%20training%20on%0Avoluminous%20datasets.%20Yet%2C%20the%20DFER%20datasets%20encompass%20a%20substantial%20volume%20of%0Anoise%20data.%20Noise%20arises%20from%20low-quality%20captures%20that%20defy%20logical%20labeling%2C%0Aand%20instances%20that%20suffer%20from%20mislabeling%20due%20to%20annotation%20bias%2C%20engendering%0Atwo%20principal%20types%20of%20uncertainty%3A%20the%20uncertainty%20regarding%20data%20usability%0Aand%20the%20uncertainty%20concerning%20label%20reliability.%20Addressing%20the%20two%20types%20of%0Auncertainty%2C%20we%20have%20meticulously%20crafted%20a%20two-stage%20framework%20aiming%20at%0A%5Ctextbf%7BS%7Deeking%20%5Ctextbf%7BC%7Dertain%20data%20%5Ctextbf%7BI%7Dn%20extensive%20%5Ctextbf%7BU%7Dncertain%0Adata%20%28SCIU%29.%20This%20initiative%20aims%20to%20purge%20the%20DFER%20datasets%20of%20these%0Auncertainties%2C%20thereby%20ensuring%20that%20only%20clean%2C%20verified%20data%20is%20employed%20in%0Atraining%20processes.%20To%20mitigate%20the%20issue%20of%20low-quality%20samples%2C%20we%20introduce%0Athe%20Coarse-Grained%20Pruning%20%28CGP%29%20stage%2C%20which%20assesses%20sample%20weights%20and%0Aprunes%20those%20deemed%20unusable%20due%20to%20their%20low%20weight.%20For%20samples%20with%0Aincorrect%20annotations%2C%20the%20Fine-Grained%20Correction%20%28FGC%29%20stage%20evaluates%0Aprediction%20stability%20to%20rectify%20mislabeled%20data.%20Moreover%2C%20SCIU%20is%20conceived%20as%0Aa%20universally%20compatible%2C%20plug-and-play%20framework%2C%20tailored%20to%20integrate%0Aseamlessly%20with%20prevailing%20DFER%20methodologies.%20Rigorous%20experiments%20across%0Aprevalent%20DFER%20datasets%20and%20against%20numerous%20benchmark%20methods%20substantiates%0ASCIU%27s%20capacity%20to%20markedly%20elevate%20performance%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16473v1&entry.124074799=Read"},
{"title": "Continuous-time Autoencoders for Regular and Irregular Time Series\n  Imputation", "author": "Hyowon Wi and Yehjin Shin and Noseong Park", "abstract": "  Time series imputation is one of the most fundamental tasks for time series.\nReal-world time series datasets are frequently incomplete (or irregular with\nmissing observations), in which case imputation is strongly required. Many\ndifferent time series imputation methods have been proposed. Recent\nself-attention-based methods show the state-of-the-art imputation performance.\nHowever, it has been overlooked for a long time to design an imputation method\nbased on continuous-time recurrent neural networks (RNNs), i.e., neural\ncontrolled differential equations (NCDEs). To this end, we redesign time series\n(variational) autoencoders based on NCDEs. Our method, called continuous-time\nautoencoder (CTA), encodes an input time series sample into a continuous hidden\npath (rather than a hidden vector) and decodes it to reconstruct and impute the\ninput. In our experiments with 4 datasets and 19 baselines, our method shows\nthe best imputation performance in almost all cases.\n", "link": "http://arxiv.org/abs/2312.16581v3", "date": "2024-06-24", "relevancy": 2.3692, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5305}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4461}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous-time%20Autoencoders%20for%20Regular%20and%20Irregular%20Time%20Series%0A%20%20Imputation&body=Title%3A%20Continuous-time%20Autoencoders%20for%20Regular%20and%20Irregular%20Time%20Series%0A%20%20Imputation%0AAuthor%3A%20Hyowon%20Wi%20and%20Yehjin%20Shin%20and%20Noseong%20Park%0AAbstract%3A%20%20%20Time%20series%20imputation%20is%20one%20of%20the%20most%20fundamental%20tasks%20for%20time%20series.%0AReal-world%20time%20series%20datasets%20are%20frequently%20incomplete%20%28or%20irregular%20with%0Amissing%20observations%29%2C%20in%20which%20case%20imputation%20is%20strongly%20required.%20Many%0Adifferent%20time%20series%20imputation%20methods%20have%20been%20proposed.%20Recent%0Aself-attention-based%20methods%20show%20the%20state-of-the-art%20imputation%20performance.%0AHowever%2C%20it%20has%20been%20overlooked%20for%20a%20long%20time%20to%20design%20an%20imputation%20method%0Abased%20on%20continuous-time%20recurrent%20neural%20networks%20%28RNNs%29%2C%20i.e.%2C%20neural%0Acontrolled%20differential%20equations%20%28NCDEs%29.%20To%20this%20end%2C%20we%20redesign%20time%20series%0A%28variational%29%20autoencoders%20based%20on%20NCDEs.%20Our%20method%2C%20called%20continuous-time%0Aautoencoder%20%28CTA%29%2C%20encodes%20an%20input%20time%20series%20sample%20into%20a%20continuous%20hidden%0Apath%20%28rather%20than%20a%20hidden%20vector%29%20and%20decodes%20it%20to%20reconstruct%20and%20impute%20the%0Ainput.%20In%20our%20experiments%20with%204%20datasets%20and%2019%20baselines%2C%20our%20method%20shows%0Athe%20best%20imputation%20performance%20in%20almost%20all%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.16581v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous-time%2520Autoencoders%2520for%2520Regular%2520and%2520Irregular%2520Time%2520Series%250A%2520%2520Imputation%26entry.906535625%3DHyowon%2520Wi%2520and%2520Yehjin%2520Shin%2520and%2520Noseong%2520Park%26entry.1292438233%3D%2520%2520Time%2520series%2520imputation%2520is%2520one%2520of%2520the%2520most%2520fundamental%2520tasks%2520for%2520time%2520series.%250AReal-world%2520time%2520series%2520datasets%2520are%2520frequently%2520incomplete%2520%2528or%2520irregular%2520with%250Amissing%2520observations%2529%252C%2520in%2520which%2520case%2520imputation%2520is%2520strongly%2520required.%2520Many%250Adifferent%2520time%2520series%2520imputation%2520methods%2520have%2520been%2520proposed.%2520Recent%250Aself-attention-based%2520methods%2520show%2520the%2520state-of-the-art%2520imputation%2520performance.%250AHowever%252C%2520it%2520has%2520been%2520overlooked%2520for%2520a%2520long%2520time%2520to%2520design%2520an%2520imputation%2520method%250Abased%2520on%2520continuous-time%2520recurrent%2520neural%2520networks%2520%2528RNNs%2529%252C%2520i.e.%252C%2520neural%250Acontrolled%2520differential%2520equations%2520%2528NCDEs%2529.%2520To%2520this%2520end%252C%2520we%2520redesign%2520time%2520series%250A%2528variational%2529%2520autoencoders%2520based%2520on%2520NCDEs.%2520Our%2520method%252C%2520called%2520continuous-time%250Aautoencoder%2520%2528CTA%2529%252C%2520encodes%2520an%2520input%2520time%2520series%2520sample%2520into%2520a%2520continuous%2520hidden%250Apath%2520%2528rather%2520than%2520a%2520hidden%2520vector%2529%2520and%2520decodes%2520it%2520to%2520reconstruct%2520and%2520impute%2520the%250Ainput.%2520In%2520our%2520experiments%2520with%25204%2520datasets%2520and%252019%2520baselines%252C%2520our%2520method%2520shows%250Athe%2520best%2520imputation%2520performance%2520in%2520almost%2520all%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.16581v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous-time%20Autoencoders%20for%20Regular%20and%20Irregular%20Time%20Series%0A%20%20Imputation&entry.906535625=Hyowon%20Wi%20and%20Yehjin%20Shin%20and%20Noseong%20Park&entry.1292438233=%20%20Time%20series%20imputation%20is%20one%20of%20the%20most%20fundamental%20tasks%20for%20time%20series.%0AReal-world%20time%20series%20datasets%20are%20frequently%20incomplete%20%28or%20irregular%20with%0Amissing%20observations%29%2C%20in%20which%20case%20imputation%20is%20strongly%20required.%20Many%0Adifferent%20time%20series%20imputation%20methods%20have%20been%20proposed.%20Recent%0Aself-attention-based%20methods%20show%20the%20state-of-the-art%20imputation%20performance.%0AHowever%2C%20it%20has%20been%20overlooked%20for%20a%20long%20time%20to%20design%20an%20imputation%20method%0Abased%20on%20continuous-time%20recurrent%20neural%20networks%20%28RNNs%29%2C%20i.e.%2C%20neural%0Acontrolled%20differential%20equations%20%28NCDEs%29.%20To%20this%20end%2C%20we%20redesign%20time%20series%0A%28variational%29%20autoencoders%20based%20on%20NCDEs.%20Our%20method%2C%20called%20continuous-time%0Aautoencoder%20%28CTA%29%2C%20encodes%20an%20input%20time%20series%20sample%20into%20a%20continuous%20hidden%0Apath%20%28rather%20than%20a%20hidden%20vector%29%20and%20decodes%20it%20to%20reconstruct%20and%20impute%20the%0Ainput.%20In%20our%20experiments%20with%204%20datasets%20and%2019%20baselines%2C%20our%20method%20shows%0Athe%20best%20imputation%20performance%20in%20almost%20all%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.16581v3&entry.124074799=Read"},
{"title": "Prompt-Consistency Image Generation (PCIG): A Unified Framework\n  Integrating LLMs, Knowledge Graphs, and Controllable Diffusion Models", "author": "Yichen Sun and Zhixuan Chu and Zhan Qin and Kui Ren", "abstract": "  The rapid advancement of Text-to-Image(T2I) generative models has enabled the\nsynthesis of high-quality images guided by textual descriptions. Despite this\nsignificant progress, these models are often susceptible in generating contents\nthat contradict the input text, which poses a challenge to their reliability\nand practical deployment. To address this problem, we introduce a novel\ndiffusion-based framework to significantly enhance the alignment of generated\nimages with their corresponding descriptions, addressing the inconsistency\nbetween visual output and textual input. Our framework is built upon a\ncomprehensive analysis of inconsistency phenomena, categorizing them based on\ntheir manifestation in the image. Leveraging a state-of-the-art large language\nmodule, we first extract objects and construct a knowledge graph to predict the\nlocations of these objects in potentially generated images. We then integrate a\nstate-of-the-art controllable image generation model with a visual text\ngeneration module to generate an image that is consistent with the original\nprompt, guided by the predicted object locations. Through extensive experiments\non an advanced multimodal hallucination benchmark, we demonstrate the efficacy\nof our approach in accurately generating the images without the inconsistency\nwith the original prompt. The code can be accessed via\nhttps://github.com/TruthAI-Lab/PCIG.\n", "link": "http://arxiv.org/abs/2406.16333v1", "date": "2024-06-24", "relevancy": 2.3667, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6157}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5755}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompt-Consistency%20Image%20Generation%20%28PCIG%29%3A%20A%20Unified%20Framework%0A%20%20Integrating%20LLMs%2C%20Knowledge%20Graphs%2C%20and%20Controllable%20Diffusion%20Models&body=Title%3A%20Prompt-Consistency%20Image%20Generation%20%28PCIG%29%3A%20A%20Unified%20Framework%0A%20%20Integrating%20LLMs%2C%20Knowledge%20Graphs%2C%20and%20Controllable%20Diffusion%20Models%0AAuthor%3A%20Yichen%20Sun%20and%20Zhixuan%20Chu%20and%20Zhan%20Qin%20and%20Kui%20Ren%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20Text-to-Image%28T2I%29%20generative%20models%20has%20enabled%20the%0Asynthesis%20of%20high-quality%20images%20guided%20by%20textual%20descriptions.%20Despite%20this%0Asignificant%20progress%2C%20these%20models%20are%20often%20susceptible%20in%20generating%20contents%0Athat%20contradict%20the%20input%20text%2C%20which%20poses%20a%20challenge%20to%20their%20reliability%0Aand%20practical%20deployment.%20To%20address%20this%20problem%2C%20we%20introduce%20a%20novel%0Adiffusion-based%20framework%20to%20significantly%20enhance%20the%20alignment%20of%20generated%0Aimages%20with%20their%20corresponding%20descriptions%2C%20addressing%20the%20inconsistency%0Abetween%20visual%20output%20and%20textual%20input.%20Our%20framework%20is%20built%20upon%20a%0Acomprehensive%20analysis%20of%20inconsistency%20phenomena%2C%20categorizing%20them%20based%20on%0Atheir%20manifestation%20in%20the%20image.%20Leveraging%20a%20state-of-the-art%20large%20language%0Amodule%2C%20we%20first%20extract%20objects%20and%20construct%20a%20knowledge%20graph%20to%20predict%20the%0Alocations%20of%20these%20objects%20in%20potentially%20generated%20images.%20We%20then%20integrate%20a%0Astate-of-the-art%20controllable%20image%20generation%20model%20with%20a%20visual%20text%0Ageneration%20module%20to%20generate%20an%20image%20that%20is%20consistent%20with%20the%20original%0Aprompt%2C%20guided%20by%20the%20predicted%20object%20locations.%20Through%20extensive%20experiments%0Aon%20an%20advanced%20multimodal%20hallucination%20benchmark%2C%20we%20demonstrate%20the%20efficacy%0Aof%20our%20approach%20in%20accurately%20generating%20the%20images%20without%20the%20inconsistency%0Awith%20the%20original%20prompt.%20The%20code%20can%20be%20accessed%20via%0Ahttps%3A//github.com/TruthAI-Lab/PCIG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompt-Consistency%2520Image%2520Generation%2520%2528PCIG%2529%253A%2520A%2520Unified%2520Framework%250A%2520%2520Integrating%2520LLMs%252C%2520Knowledge%2520Graphs%252C%2520and%2520Controllable%2520Diffusion%2520Models%26entry.906535625%3DYichen%2520Sun%2520and%2520Zhixuan%2520Chu%2520and%2520Zhan%2520Qin%2520and%2520Kui%2520Ren%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520Text-to-Image%2528T2I%2529%2520generative%2520models%2520has%2520enabled%2520the%250Asynthesis%2520of%2520high-quality%2520images%2520guided%2520by%2520textual%2520descriptions.%2520Despite%2520this%250Asignificant%2520progress%252C%2520these%2520models%2520are%2520often%2520susceptible%2520in%2520generating%2520contents%250Athat%2520contradict%2520the%2520input%2520text%252C%2520which%2520poses%2520a%2520challenge%2520to%2520their%2520reliability%250Aand%2520practical%2520deployment.%2520To%2520address%2520this%2520problem%252C%2520we%2520introduce%2520a%2520novel%250Adiffusion-based%2520framework%2520to%2520significantly%2520enhance%2520the%2520alignment%2520of%2520generated%250Aimages%2520with%2520their%2520corresponding%2520descriptions%252C%2520addressing%2520the%2520inconsistency%250Abetween%2520visual%2520output%2520and%2520textual%2520input.%2520Our%2520framework%2520is%2520built%2520upon%2520a%250Acomprehensive%2520analysis%2520of%2520inconsistency%2520phenomena%252C%2520categorizing%2520them%2520based%2520on%250Atheir%2520manifestation%2520in%2520the%2520image.%2520Leveraging%2520a%2520state-of-the-art%2520large%2520language%250Amodule%252C%2520we%2520first%2520extract%2520objects%2520and%2520construct%2520a%2520knowledge%2520graph%2520to%2520predict%2520the%250Alocations%2520of%2520these%2520objects%2520in%2520potentially%2520generated%2520images.%2520We%2520then%2520integrate%2520a%250Astate-of-the-art%2520controllable%2520image%2520generation%2520model%2520with%2520a%2520visual%2520text%250Ageneration%2520module%2520to%2520generate%2520an%2520image%2520that%2520is%2520consistent%2520with%2520the%2520original%250Aprompt%252C%2520guided%2520by%2520the%2520predicted%2520object%2520locations.%2520Through%2520extensive%2520experiments%250Aon%2520an%2520advanced%2520multimodal%2520hallucination%2520benchmark%252C%2520we%2520demonstrate%2520the%2520efficacy%250Aof%2520our%2520approach%2520in%2520accurately%2520generating%2520the%2520images%2520without%2520the%2520inconsistency%250Awith%2520the%2520original%2520prompt.%2520The%2520code%2520can%2520be%2520accessed%2520via%250Ahttps%253A//github.com/TruthAI-Lab/PCIG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompt-Consistency%20Image%20Generation%20%28PCIG%29%3A%20A%20Unified%20Framework%0A%20%20Integrating%20LLMs%2C%20Knowledge%20Graphs%2C%20and%20Controllable%20Diffusion%20Models&entry.906535625=Yichen%20Sun%20and%20Zhixuan%20Chu%20and%20Zhan%20Qin%20and%20Kui%20Ren&entry.1292438233=%20%20The%20rapid%20advancement%20of%20Text-to-Image%28T2I%29%20generative%20models%20has%20enabled%20the%0Asynthesis%20of%20high-quality%20images%20guided%20by%20textual%20descriptions.%20Despite%20this%0Asignificant%20progress%2C%20these%20models%20are%20often%20susceptible%20in%20generating%20contents%0Athat%20contradict%20the%20input%20text%2C%20which%20poses%20a%20challenge%20to%20their%20reliability%0Aand%20practical%20deployment.%20To%20address%20this%20problem%2C%20we%20introduce%20a%20novel%0Adiffusion-based%20framework%20to%20significantly%20enhance%20the%20alignment%20of%20generated%0Aimages%20with%20their%20corresponding%20descriptions%2C%20addressing%20the%20inconsistency%0Abetween%20visual%20output%20and%20textual%20input.%20Our%20framework%20is%20built%20upon%20a%0Acomprehensive%20analysis%20of%20inconsistency%20phenomena%2C%20categorizing%20them%20based%20on%0Atheir%20manifestation%20in%20the%20image.%20Leveraging%20a%20state-of-the-art%20large%20language%0Amodule%2C%20we%20first%20extract%20objects%20and%20construct%20a%20knowledge%20graph%20to%20predict%20the%0Alocations%20of%20these%20objects%20in%20potentially%20generated%20images.%20We%20then%20integrate%20a%0Astate-of-the-art%20controllable%20image%20generation%20model%20with%20a%20visual%20text%0Ageneration%20module%20to%20generate%20an%20image%20that%20is%20consistent%20with%20the%20original%0Aprompt%2C%20guided%20by%20the%20predicted%20object%20locations.%20Through%20extensive%20experiments%0Aon%20an%20advanced%20multimodal%20hallucination%20benchmark%2C%20we%20demonstrate%20the%20efficacy%0Aof%20our%20approach%20in%20accurately%20generating%20the%20images%20without%20the%20inconsistency%0Awith%20the%20original%20prompt.%20The%20code%20can%20be%20accessed%20via%0Ahttps%3A//github.com/TruthAI-Lab/PCIG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16333v1&entry.124074799=Read"},
{"title": "Video-Infinity: Distributed Long Video Generation", "author": "Zhenxiong Tan and Xingyi Yang and Songhua Liu and Xinchao Wang", "abstract": "  Diffusion models have recently achieved remarkable results for video\ngeneration. Despite the encouraging performances, the generated videos are\ntypically constrained to a small number of frames, resulting in clips lasting\nmerely a few seconds. The primary challenges in producing longer videos include\nthe substantial memory requirements and the extended processing time required\non a single GPU. A straightforward solution would be to split the workload\nacross multiple GPUs, which, however, leads to two issues: (1) ensuring all\nGPUs communicate effectively to share timing and context information, and (2)\nmodifying existing video diffusion models, which are usually trained on short\nsequences, to create longer videos without additional training. To tackle\nthese, in this paper we introduce Video-Infinity, a distributed inference\npipeline that enables parallel processing across multiple GPUs for long-form\nvideo generation. Specifically, we propose two coherent mechanisms: Clip\nparallelism and Dual-scope attention. Clip parallelism optimizes the gathering\nand sharing of context information across GPUs which minimizes communication\noverhead, while Dual-scope attention modulates the temporal self-attention to\nbalance local and global contexts efficiently across the devices. Together, the\ntwo mechanisms join forces to distribute the workload and enable the fast\ngeneration of long videos. Under an 8 x Nvidia 6000 Ada GPU (48G) setup, our\nmethod generates videos up to 2,300 frames in approximately 5 minutes, enabling\nlong video generation at a speed 100 times faster than the prior methods.\n", "link": "http://arxiv.org/abs/2406.16260v1", "date": "2024-06-24", "relevancy": 2.3566, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6159}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5871}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video-Infinity%3A%20Distributed%20Long%20Video%20Generation&body=Title%3A%20Video-Infinity%3A%20Distributed%20Long%20Video%20Generation%0AAuthor%3A%20Zhenxiong%20Tan%20and%20Xingyi%20Yang%20and%20Songhua%20Liu%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Diffusion%20models%20have%20recently%20achieved%20remarkable%20results%20for%20video%0Ageneration.%20Despite%20the%20encouraging%20performances%2C%20the%20generated%20videos%20are%0Atypically%20constrained%20to%20a%20small%20number%20of%20frames%2C%20resulting%20in%20clips%20lasting%0Amerely%20a%20few%20seconds.%20The%20primary%20challenges%20in%20producing%20longer%20videos%20include%0Athe%20substantial%20memory%20requirements%20and%20the%20extended%20processing%20time%20required%0Aon%20a%20single%20GPU.%20A%20straightforward%20solution%20would%20be%20to%20split%20the%20workload%0Aacross%20multiple%20GPUs%2C%20which%2C%20however%2C%20leads%20to%20two%20issues%3A%20%281%29%20ensuring%20all%0AGPUs%20communicate%20effectively%20to%20share%20timing%20and%20context%20information%2C%20and%20%282%29%0Amodifying%20existing%20video%20diffusion%20models%2C%20which%20are%20usually%20trained%20on%20short%0Asequences%2C%20to%20create%20longer%20videos%20without%20additional%20training.%20To%20tackle%0Athese%2C%20in%20this%20paper%20we%20introduce%20Video-Infinity%2C%20a%20distributed%20inference%0Apipeline%20that%20enables%20parallel%20processing%20across%20multiple%20GPUs%20for%20long-form%0Avideo%20generation.%20Specifically%2C%20we%20propose%20two%20coherent%20mechanisms%3A%20Clip%0Aparallelism%20and%20Dual-scope%20attention.%20Clip%20parallelism%20optimizes%20the%20gathering%0Aand%20sharing%20of%20context%20information%20across%20GPUs%20which%20minimizes%20communication%0Aoverhead%2C%20while%20Dual-scope%20attention%20modulates%20the%20temporal%20self-attention%20to%0Abalance%20local%20and%20global%20contexts%20efficiently%20across%20the%20devices.%20Together%2C%20the%0Atwo%20mechanisms%20join%20forces%20to%20distribute%20the%20workload%20and%20enable%20the%20fast%0Ageneration%20of%20long%20videos.%20Under%20an%208%20x%20Nvidia%206000%20Ada%20GPU%20%2848G%29%20setup%2C%20our%0Amethod%20generates%20videos%20up%20to%202%2C300%20frames%20in%20approximately%205%20minutes%2C%20enabling%0Along%20video%20generation%20at%20a%20speed%20100%20times%20faster%20than%20the%20prior%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16260v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo-Infinity%253A%2520Distributed%2520Long%2520Video%2520Generation%26entry.906535625%3DZhenxiong%2520Tan%2520and%2520Xingyi%2520Yang%2520and%2520Songhua%2520Liu%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520recently%2520achieved%2520remarkable%2520results%2520for%2520video%250Ageneration.%2520Despite%2520the%2520encouraging%2520performances%252C%2520the%2520generated%2520videos%2520are%250Atypically%2520constrained%2520to%2520a%2520small%2520number%2520of%2520frames%252C%2520resulting%2520in%2520clips%2520lasting%250Amerely%2520a%2520few%2520seconds.%2520The%2520primary%2520challenges%2520in%2520producing%2520longer%2520videos%2520include%250Athe%2520substantial%2520memory%2520requirements%2520and%2520the%2520extended%2520processing%2520time%2520required%250Aon%2520a%2520single%2520GPU.%2520A%2520straightforward%2520solution%2520would%2520be%2520to%2520split%2520the%2520workload%250Aacross%2520multiple%2520GPUs%252C%2520which%252C%2520however%252C%2520leads%2520to%2520two%2520issues%253A%2520%25281%2529%2520ensuring%2520all%250AGPUs%2520communicate%2520effectively%2520to%2520share%2520timing%2520and%2520context%2520information%252C%2520and%2520%25282%2529%250Amodifying%2520existing%2520video%2520diffusion%2520models%252C%2520which%2520are%2520usually%2520trained%2520on%2520short%250Asequences%252C%2520to%2520create%2520longer%2520videos%2520without%2520additional%2520training.%2520To%2520tackle%250Athese%252C%2520in%2520this%2520paper%2520we%2520introduce%2520Video-Infinity%252C%2520a%2520distributed%2520inference%250Apipeline%2520that%2520enables%2520parallel%2520processing%2520across%2520multiple%2520GPUs%2520for%2520long-form%250Avideo%2520generation.%2520Specifically%252C%2520we%2520propose%2520two%2520coherent%2520mechanisms%253A%2520Clip%250Aparallelism%2520and%2520Dual-scope%2520attention.%2520Clip%2520parallelism%2520optimizes%2520the%2520gathering%250Aand%2520sharing%2520of%2520context%2520information%2520across%2520GPUs%2520which%2520minimizes%2520communication%250Aoverhead%252C%2520while%2520Dual-scope%2520attention%2520modulates%2520the%2520temporal%2520self-attention%2520to%250Abalance%2520local%2520and%2520global%2520contexts%2520efficiently%2520across%2520the%2520devices.%2520Together%252C%2520the%250Atwo%2520mechanisms%2520join%2520forces%2520to%2520distribute%2520the%2520workload%2520and%2520enable%2520the%2520fast%250Ageneration%2520of%2520long%2520videos.%2520Under%2520an%25208%2520x%2520Nvidia%25206000%2520Ada%2520GPU%2520%252848G%2529%2520setup%252C%2520our%250Amethod%2520generates%2520videos%2520up%2520to%25202%252C300%2520frames%2520in%2520approximately%25205%2520minutes%252C%2520enabling%250Along%2520video%2520generation%2520at%2520a%2520speed%2520100%2520times%2520faster%2520than%2520the%2520prior%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16260v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video-Infinity%3A%20Distributed%20Long%20Video%20Generation&entry.906535625=Zhenxiong%20Tan%20and%20Xingyi%20Yang%20and%20Songhua%20Liu%20and%20Xinchao%20Wang&entry.1292438233=%20%20Diffusion%20models%20have%20recently%20achieved%20remarkable%20results%20for%20video%0Ageneration.%20Despite%20the%20encouraging%20performances%2C%20the%20generated%20videos%20are%0Atypically%20constrained%20to%20a%20small%20number%20of%20frames%2C%20resulting%20in%20clips%20lasting%0Amerely%20a%20few%20seconds.%20The%20primary%20challenges%20in%20producing%20longer%20videos%20include%0Athe%20substantial%20memory%20requirements%20and%20the%20extended%20processing%20time%20required%0Aon%20a%20single%20GPU.%20A%20straightforward%20solution%20would%20be%20to%20split%20the%20workload%0Aacross%20multiple%20GPUs%2C%20which%2C%20however%2C%20leads%20to%20two%20issues%3A%20%281%29%20ensuring%20all%0AGPUs%20communicate%20effectively%20to%20share%20timing%20and%20context%20information%2C%20and%20%282%29%0Amodifying%20existing%20video%20diffusion%20models%2C%20which%20are%20usually%20trained%20on%20short%0Asequences%2C%20to%20create%20longer%20videos%20without%20additional%20training.%20To%20tackle%0Athese%2C%20in%20this%20paper%20we%20introduce%20Video-Infinity%2C%20a%20distributed%20inference%0Apipeline%20that%20enables%20parallel%20processing%20across%20multiple%20GPUs%20for%20long-form%0Avideo%20generation.%20Specifically%2C%20we%20propose%20two%20coherent%20mechanisms%3A%20Clip%0Aparallelism%20and%20Dual-scope%20attention.%20Clip%20parallelism%20optimizes%20the%20gathering%0Aand%20sharing%20of%20context%20information%20across%20GPUs%20which%20minimizes%20communication%0Aoverhead%2C%20while%20Dual-scope%20attention%20modulates%20the%20temporal%20self-attention%20to%0Abalance%20local%20and%20global%20contexts%20efficiently%20across%20the%20devices.%20Together%2C%20the%0Atwo%20mechanisms%20join%20forces%20to%20distribute%20the%20workload%20and%20enable%20the%20fast%0Ageneration%20of%20long%20videos.%20Under%20an%208%20x%20Nvidia%206000%20Ada%20GPU%20%2848G%29%20setup%2C%20our%0Amethod%20generates%20videos%20up%20to%202%2C300%20frames%20in%20approximately%205%20minutes%2C%20enabling%0Along%20video%20generation%20at%20a%20speed%20100%20times%20faster%20than%20the%20prior%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16260v1&entry.124074799=Read"},
{"title": "MAP-NBV: Multi-agent Prediction-guided Next-Best-View Planning for\n  Active 3D Object Reconstruction", "author": "Harnaik Dhami and Vishnu D. Sharma and Pratap Tokekar", "abstract": "  Next-Best View (NBV) planning is a long-standing problem of determining where\nto obtain the next best view of an object from, by a robot that is viewing the\nobject. There are a number of methods for choosing NBV based on the observed\npart of the object. In this paper, we investigate how predicting the unobserved\npart helps with the efficiency of reconstructing the object. We present,\nMulti-Agent Prediction-Guided NBV (MAP-NBV), a decentralized coordination\nalgorithm for active 3D reconstruction with multi-agent systems.\nPrediction-based approaches have shown great improvement in active perception\ntasks by learning the cues about structures in the environment from data.\nHowever, these methods primarily focus on single-agent systems. We design a\ndecentralized next-best-view approach that utilizes geometric measures over the\npredictions and jointly optimizes the information gain and control effort for\nefficient collaborative 3D reconstruction of the object. Our method achieves\n19% improvement over the non-predictive multi-agent approach in simulations\nusing AirSim and ShapeNet. We make our code publicly available through our\nproject website: http://raaslab.org/projects/MAPNBV/.\n", "link": "http://arxiv.org/abs/2307.04004v3", "date": "2024-06-24", "relevancy": 2.3467, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6287}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.587}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAP-NBV%3A%20Multi-agent%20Prediction-guided%20Next-Best-View%20Planning%20for%0A%20%20Active%203D%20Object%20Reconstruction&body=Title%3A%20MAP-NBV%3A%20Multi-agent%20Prediction-guided%20Next-Best-View%20Planning%20for%0A%20%20Active%203D%20Object%20Reconstruction%0AAuthor%3A%20Harnaik%20Dhami%20and%20Vishnu%20D.%20Sharma%20and%20Pratap%20Tokekar%0AAbstract%3A%20%20%20Next-Best%20View%20%28NBV%29%20planning%20is%20a%20long-standing%20problem%20of%20determining%20where%0Ato%20obtain%20the%20next%20best%20view%20of%20an%20object%20from%2C%20by%20a%20robot%20that%20is%20viewing%20the%0Aobject.%20There%20are%20a%20number%20of%20methods%20for%20choosing%20NBV%20based%20on%20the%20observed%0Apart%20of%20the%20object.%20In%20this%20paper%2C%20we%20investigate%20how%20predicting%20the%20unobserved%0Apart%20helps%20with%20the%20efficiency%20of%20reconstructing%20the%20object.%20We%20present%2C%0AMulti-Agent%20Prediction-Guided%20NBV%20%28MAP-NBV%29%2C%20a%20decentralized%20coordination%0Aalgorithm%20for%20active%203D%20reconstruction%20with%20multi-agent%20systems.%0APrediction-based%20approaches%20have%20shown%20great%20improvement%20in%20active%20perception%0Atasks%20by%20learning%20the%20cues%20about%20structures%20in%20the%20environment%20from%20data.%0AHowever%2C%20these%20methods%20primarily%20focus%20on%20single-agent%20systems.%20We%20design%20a%0Adecentralized%20next-best-view%20approach%20that%20utilizes%20geometric%20measures%20over%20the%0Apredictions%20and%20jointly%20optimizes%20the%20information%20gain%20and%20control%20effort%20for%0Aefficient%20collaborative%203D%20reconstruction%20of%20the%20object.%20Our%20method%20achieves%0A19%25%20improvement%20over%20the%20non-predictive%20multi-agent%20approach%20in%20simulations%0Ausing%20AirSim%20and%20ShapeNet.%20We%20make%20our%20code%20publicly%20available%20through%20our%0Aproject%20website%3A%20http%3A//raaslab.org/projects/MAPNBV/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.04004v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAP-NBV%253A%2520Multi-agent%2520Prediction-guided%2520Next-Best-View%2520Planning%2520for%250A%2520%2520Active%25203D%2520Object%2520Reconstruction%26entry.906535625%3DHarnaik%2520Dhami%2520and%2520Vishnu%2520D.%2520Sharma%2520and%2520Pratap%2520Tokekar%26entry.1292438233%3D%2520%2520Next-Best%2520View%2520%2528NBV%2529%2520planning%2520is%2520a%2520long-standing%2520problem%2520of%2520determining%2520where%250Ato%2520obtain%2520the%2520next%2520best%2520view%2520of%2520an%2520object%2520from%252C%2520by%2520a%2520robot%2520that%2520is%2520viewing%2520the%250Aobject.%2520There%2520are%2520a%2520number%2520of%2520methods%2520for%2520choosing%2520NBV%2520based%2520on%2520the%2520observed%250Apart%2520of%2520the%2520object.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520how%2520predicting%2520the%2520unobserved%250Apart%2520helps%2520with%2520the%2520efficiency%2520of%2520reconstructing%2520the%2520object.%2520We%2520present%252C%250AMulti-Agent%2520Prediction-Guided%2520NBV%2520%2528MAP-NBV%2529%252C%2520a%2520decentralized%2520coordination%250Aalgorithm%2520for%2520active%25203D%2520reconstruction%2520with%2520multi-agent%2520systems.%250APrediction-based%2520approaches%2520have%2520shown%2520great%2520improvement%2520in%2520active%2520perception%250Atasks%2520by%2520learning%2520the%2520cues%2520about%2520structures%2520in%2520the%2520environment%2520from%2520data.%250AHowever%252C%2520these%2520methods%2520primarily%2520focus%2520on%2520single-agent%2520systems.%2520We%2520design%2520a%250Adecentralized%2520next-best-view%2520approach%2520that%2520utilizes%2520geometric%2520measures%2520over%2520the%250Apredictions%2520and%2520jointly%2520optimizes%2520the%2520information%2520gain%2520and%2520control%2520effort%2520for%250Aefficient%2520collaborative%25203D%2520reconstruction%2520of%2520the%2520object.%2520Our%2520method%2520achieves%250A19%2525%2520improvement%2520over%2520the%2520non-predictive%2520multi-agent%2520approach%2520in%2520simulations%250Ausing%2520AirSim%2520and%2520ShapeNet.%2520We%2520make%2520our%2520code%2520publicly%2520available%2520through%2520our%250Aproject%2520website%253A%2520http%253A//raaslab.org/projects/MAPNBV/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.04004v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAP-NBV%3A%20Multi-agent%20Prediction-guided%20Next-Best-View%20Planning%20for%0A%20%20Active%203D%20Object%20Reconstruction&entry.906535625=Harnaik%20Dhami%20and%20Vishnu%20D.%20Sharma%20and%20Pratap%20Tokekar&entry.1292438233=%20%20Next-Best%20View%20%28NBV%29%20planning%20is%20a%20long-standing%20problem%20of%20determining%20where%0Ato%20obtain%20the%20next%20best%20view%20of%20an%20object%20from%2C%20by%20a%20robot%20that%20is%20viewing%20the%0Aobject.%20There%20are%20a%20number%20of%20methods%20for%20choosing%20NBV%20based%20on%20the%20observed%0Apart%20of%20the%20object.%20In%20this%20paper%2C%20we%20investigate%20how%20predicting%20the%20unobserved%0Apart%20helps%20with%20the%20efficiency%20of%20reconstructing%20the%20object.%20We%20present%2C%0AMulti-Agent%20Prediction-Guided%20NBV%20%28MAP-NBV%29%2C%20a%20decentralized%20coordination%0Aalgorithm%20for%20active%203D%20reconstruction%20with%20multi-agent%20systems.%0APrediction-based%20approaches%20have%20shown%20great%20improvement%20in%20active%20perception%0Atasks%20by%20learning%20the%20cues%20about%20structures%20in%20the%20environment%20from%20data.%0AHowever%2C%20these%20methods%20primarily%20focus%20on%20single-agent%20systems.%20We%20design%20a%0Adecentralized%20next-best-view%20approach%20that%20utilizes%20geometric%20measures%20over%20the%0Apredictions%20and%20jointly%20optimizes%20the%20information%20gain%20and%20control%20effort%20for%0Aefficient%20collaborative%203D%20reconstruction%20of%20the%20object.%20Our%20method%20achieves%0A19%25%20improvement%20over%20the%20non-predictive%20multi-agent%20approach%20in%20simulations%0Ausing%20AirSim%20and%20ShapeNet.%20We%20make%20our%20code%20publicly%20available%20through%20our%0Aproject%20website%3A%20http%3A//raaslab.org/projects/MAPNBV/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.04004v3&entry.124074799=Read"},
{"title": "MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies", "author": "Zhende Song and Chenchen Wang and Jiamu Sheng and Chi Zhang and Gang Yu and Jiayuan Fan and Tao Chen", "abstract": "  Development of multimodal models has marked a significant step forward in how\nmachines understand videos. These models have shown promise in analyzing short\nvideo clips. However, when it comes to longer formats like movies, they often\nfall short. The main hurdles are the lack of high-quality, diverse video data\nand the intensive work required to collect or annotate such data. In face of\nthese challenges, we propose MovieLLM, a novel framework designed to synthesize\nconsistent and high-quality video data for instruction tuning. The pipeline is\ncarefully designed to control the style of videos by improving textual\ninversion technique with powerful text generation capability of GPT-4. As the\nfirst framework to do such thing, our approach stands out for its flexibility\nand scalability, empowering users to create customized movies with only one\ndescription. This makes it a superior alternative to traditional data\ncollection methods. Our extensive experiments validate that the data produced\nby MovieLLM significantly improves the performance of multimodal models in\nunderstanding complex video narratives, overcoming the limitations of existing\ndatasets regarding scarcity and bias.\n", "link": "http://arxiv.org/abs/2403.01422v2", "date": "2024-06-24", "relevancy": 2.3405, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6086}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5828}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MovieLLM%3A%20Enhancing%20Long%20Video%20Understanding%20with%20AI-Generated%20Movies&body=Title%3A%20MovieLLM%3A%20Enhancing%20Long%20Video%20Understanding%20with%20AI-Generated%20Movies%0AAuthor%3A%20Zhende%20Song%20and%20Chenchen%20Wang%20and%20Jiamu%20Sheng%20and%20Chi%20Zhang%20and%20Gang%20Yu%20and%20Jiayuan%20Fan%20and%20Tao%20Chen%0AAbstract%3A%20%20%20Development%20of%20multimodal%20models%20has%20marked%20a%20significant%20step%20forward%20in%20how%0Amachines%20understand%20videos.%20These%20models%20have%20shown%20promise%20in%20analyzing%20short%0Avideo%20clips.%20However%2C%20when%20it%20comes%20to%20longer%20formats%20like%20movies%2C%20they%20often%0Afall%20short.%20The%20main%20hurdles%20are%20the%20lack%20of%20high-quality%2C%20diverse%20video%20data%0Aand%20the%20intensive%20work%20required%20to%20collect%20or%20annotate%20such%20data.%20In%20face%20of%0Athese%20challenges%2C%20we%20propose%20MovieLLM%2C%20a%20novel%20framework%20designed%20to%20synthesize%0Aconsistent%20and%20high-quality%20video%20data%20for%20instruction%20tuning.%20The%20pipeline%20is%0Acarefully%20designed%20to%20control%20the%20style%20of%20videos%20by%20improving%20textual%0Ainversion%20technique%20with%20powerful%20text%20generation%20capability%20of%20GPT-4.%20As%20the%0Afirst%20framework%20to%20do%20such%20thing%2C%20our%20approach%20stands%20out%20for%20its%20flexibility%0Aand%20scalability%2C%20empowering%20users%20to%20create%20customized%20movies%20with%20only%20one%0Adescription.%20This%20makes%20it%20a%20superior%20alternative%20to%20traditional%20data%0Acollection%20methods.%20Our%20extensive%20experiments%20validate%20that%20the%20data%20produced%0Aby%20MovieLLM%20significantly%20improves%20the%20performance%20of%20multimodal%20models%20in%0Aunderstanding%20complex%20video%20narratives%2C%20overcoming%20the%20limitations%20of%20existing%0Adatasets%20regarding%20scarcity%20and%20bias.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01422v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMovieLLM%253A%2520Enhancing%2520Long%2520Video%2520Understanding%2520with%2520AI-Generated%2520Movies%26entry.906535625%3DZhende%2520Song%2520and%2520Chenchen%2520Wang%2520and%2520Jiamu%2520Sheng%2520and%2520Chi%2520Zhang%2520and%2520Gang%2520Yu%2520and%2520Jiayuan%2520Fan%2520and%2520Tao%2520Chen%26entry.1292438233%3D%2520%2520Development%2520of%2520multimodal%2520models%2520has%2520marked%2520a%2520significant%2520step%2520forward%2520in%2520how%250Amachines%2520understand%2520videos.%2520These%2520models%2520have%2520shown%2520promise%2520in%2520analyzing%2520short%250Avideo%2520clips.%2520However%252C%2520when%2520it%2520comes%2520to%2520longer%2520formats%2520like%2520movies%252C%2520they%2520often%250Afall%2520short.%2520The%2520main%2520hurdles%2520are%2520the%2520lack%2520of%2520high-quality%252C%2520diverse%2520video%2520data%250Aand%2520the%2520intensive%2520work%2520required%2520to%2520collect%2520or%2520annotate%2520such%2520data.%2520In%2520face%2520of%250Athese%2520challenges%252C%2520we%2520propose%2520MovieLLM%252C%2520a%2520novel%2520framework%2520designed%2520to%2520synthesize%250Aconsistent%2520and%2520high-quality%2520video%2520data%2520for%2520instruction%2520tuning.%2520The%2520pipeline%2520is%250Acarefully%2520designed%2520to%2520control%2520the%2520style%2520of%2520videos%2520by%2520improving%2520textual%250Ainversion%2520technique%2520with%2520powerful%2520text%2520generation%2520capability%2520of%2520GPT-4.%2520As%2520the%250Afirst%2520framework%2520to%2520do%2520such%2520thing%252C%2520our%2520approach%2520stands%2520out%2520for%2520its%2520flexibility%250Aand%2520scalability%252C%2520empowering%2520users%2520to%2520create%2520customized%2520movies%2520with%2520only%2520one%250Adescription.%2520This%2520makes%2520it%2520a%2520superior%2520alternative%2520to%2520traditional%2520data%250Acollection%2520methods.%2520Our%2520extensive%2520experiments%2520validate%2520that%2520the%2520data%2520produced%250Aby%2520MovieLLM%2520significantly%2520improves%2520the%2520performance%2520of%2520multimodal%2520models%2520in%250Aunderstanding%2520complex%2520video%2520narratives%252C%2520overcoming%2520the%2520limitations%2520of%2520existing%250Adatasets%2520regarding%2520scarcity%2520and%2520bias.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.01422v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MovieLLM%3A%20Enhancing%20Long%20Video%20Understanding%20with%20AI-Generated%20Movies&entry.906535625=Zhende%20Song%20and%20Chenchen%20Wang%20and%20Jiamu%20Sheng%20and%20Chi%20Zhang%20and%20Gang%20Yu%20and%20Jiayuan%20Fan%20and%20Tao%20Chen&entry.1292438233=%20%20Development%20of%20multimodal%20models%20has%20marked%20a%20significant%20step%20forward%20in%20how%0Amachines%20understand%20videos.%20These%20models%20have%20shown%20promise%20in%20analyzing%20short%0Avideo%20clips.%20However%2C%20when%20it%20comes%20to%20longer%20formats%20like%20movies%2C%20they%20often%0Afall%20short.%20The%20main%20hurdles%20are%20the%20lack%20of%20high-quality%2C%20diverse%20video%20data%0Aand%20the%20intensive%20work%20required%20to%20collect%20or%20annotate%20such%20data.%20In%20face%20of%0Athese%20challenges%2C%20we%20propose%20MovieLLM%2C%20a%20novel%20framework%20designed%20to%20synthesize%0Aconsistent%20and%20high-quality%20video%20data%20for%20instruction%20tuning.%20The%20pipeline%20is%0Acarefully%20designed%20to%20control%20the%20style%20of%20videos%20by%20improving%20textual%0Ainversion%20technique%20with%20powerful%20text%20generation%20capability%20of%20GPT-4.%20As%20the%0Afirst%20framework%20to%20do%20such%20thing%2C%20our%20approach%20stands%20out%20for%20its%20flexibility%0Aand%20scalability%2C%20empowering%20users%20to%20create%20customized%20movies%20with%20only%20one%0Adescription.%20This%20makes%20it%20a%20superior%20alternative%20to%20traditional%20data%0Acollection%20methods.%20Our%20extensive%20experiments%20validate%20that%20the%20data%20produced%0Aby%20MovieLLM%20significantly%20improves%20the%20performance%20of%20multimodal%20models%20in%0Aunderstanding%20complex%20video%20narratives%2C%20overcoming%20the%20limitations%20of%20existing%0Adatasets%20regarding%20scarcity%20and%20bias.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01422v2&entry.124074799=Read"},
{"title": "Multi-Robot Collaborative Localization and Planning with Inter-Ranging", "author": "Derek Knowles and Adam Dai and Grace Gao", "abstract": "  Robots often use feature-based image tracking to identify their position in\ntheir surrounding environment; however, feature-based image tracking is prone\nto errors in low-textured and poorly lit environments. Specifically, we\ninvestigate a scenario where robots are tasked with exploring the surface of\nthe Moon and are required to have an accurate estimate of their position to be\nable to correctly geotag scientific measurements. To reduce localization error,\nwe complement traditional feature-based image tracking with ultra-wideband\n(UWB) distance measurements between the robots. The robots use an advanced\nmesh-ranging protocol that allows them to continuously share distance\nmeasurements amongst each other rather than relying on the common \"anchor\" and\n\"tag\" UWB architecture. We develop a decentralized multi-robot coordination\nalgorithm that actively plans paths based on measurement line-of-sight vectors\namongst all robots to minimize collective localization error. We then\ndemonstrate the emergent behavior of the proposed multi-robot coordination\nalgorithm both in simulation and hardware to lower a geometry-based uncertainty\nmetric and reduce localization error.\n", "link": "http://arxiv.org/abs/2406.16679v1", "date": "2024-06-24", "relevancy": 2.3322, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6238}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.587}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Robot%20Collaborative%20Localization%20and%20Planning%20with%20Inter-Ranging&body=Title%3A%20Multi-Robot%20Collaborative%20Localization%20and%20Planning%20with%20Inter-Ranging%0AAuthor%3A%20Derek%20Knowles%20and%20Adam%20Dai%20and%20Grace%20Gao%0AAbstract%3A%20%20%20Robots%20often%20use%20feature-based%20image%20tracking%20to%20identify%20their%20position%20in%0Atheir%20surrounding%20environment%3B%20however%2C%20feature-based%20image%20tracking%20is%20prone%0Ato%20errors%20in%20low-textured%20and%20poorly%20lit%20environments.%20Specifically%2C%20we%0Ainvestigate%20a%20scenario%20where%20robots%20are%20tasked%20with%20exploring%20the%20surface%20of%0Athe%20Moon%20and%20are%20required%20to%20have%20an%20accurate%20estimate%20of%20their%20position%20to%20be%0Aable%20to%20correctly%20geotag%20scientific%20measurements.%20To%20reduce%20localization%20error%2C%0Awe%20complement%20traditional%20feature-based%20image%20tracking%20with%20ultra-wideband%0A%28UWB%29%20distance%20measurements%20between%20the%20robots.%20The%20robots%20use%20an%20advanced%0Amesh-ranging%20protocol%20that%20allows%20them%20to%20continuously%20share%20distance%0Ameasurements%20amongst%20each%20other%20rather%20than%20relying%20on%20the%20common%20%22anchor%22%20and%0A%22tag%22%20UWB%20architecture.%20We%20develop%20a%20decentralized%20multi-robot%20coordination%0Aalgorithm%20that%20actively%20plans%20paths%20based%20on%20measurement%20line-of-sight%20vectors%0Aamongst%20all%20robots%20to%20minimize%20collective%20localization%20error.%20We%20then%0Ademonstrate%20the%20emergent%20behavior%20of%20the%20proposed%20multi-robot%20coordination%0Aalgorithm%20both%20in%20simulation%20and%20hardware%20to%20lower%20a%20geometry-based%20uncertainty%0Ametric%20and%20reduce%20localization%20error.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Robot%2520Collaborative%2520Localization%2520and%2520Planning%2520with%2520Inter-Ranging%26entry.906535625%3DDerek%2520Knowles%2520and%2520Adam%2520Dai%2520and%2520Grace%2520Gao%26entry.1292438233%3D%2520%2520Robots%2520often%2520use%2520feature-based%2520image%2520tracking%2520to%2520identify%2520their%2520position%2520in%250Atheir%2520surrounding%2520environment%253B%2520however%252C%2520feature-based%2520image%2520tracking%2520is%2520prone%250Ato%2520errors%2520in%2520low-textured%2520and%2520poorly%2520lit%2520environments.%2520Specifically%252C%2520we%250Ainvestigate%2520a%2520scenario%2520where%2520robots%2520are%2520tasked%2520with%2520exploring%2520the%2520surface%2520of%250Athe%2520Moon%2520and%2520are%2520required%2520to%2520have%2520an%2520accurate%2520estimate%2520of%2520their%2520position%2520to%2520be%250Aable%2520to%2520correctly%2520geotag%2520scientific%2520measurements.%2520To%2520reduce%2520localization%2520error%252C%250Awe%2520complement%2520traditional%2520feature-based%2520image%2520tracking%2520with%2520ultra-wideband%250A%2528UWB%2529%2520distance%2520measurements%2520between%2520the%2520robots.%2520The%2520robots%2520use%2520an%2520advanced%250Amesh-ranging%2520protocol%2520that%2520allows%2520them%2520to%2520continuously%2520share%2520distance%250Ameasurements%2520amongst%2520each%2520other%2520rather%2520than%2520relying%2520on%2520the%2520common%2520%2522anchor%2522%2520and%250A%2522tag%2522%2520UWB%2520architecture.%2520We%2520develop%2520a%2520decentralized%2520multi-robot%2520coordination%250Aalgorithm%2520that%2520actively%2520plans%2520paths%2520based%2520on%2520measurement%2520line-of-sight%2520vectors%250Aamongst%2520all%2520robots%2520to%2520minimize%2520collective%2520localization%2520error.%2520We%2520then%250Ademonstrate%2520the%2520emergent%2520behavior%2520of%2520the%2520proposed%2520multi-robot%2520coordination%250Aalgorithm%2520both%2520in%2520simulation%2520and%2520hardware%2520to%2520lower%2520a%2520geometry-based%2520uncertainty%250Ametric%2520and%2520reduce%2520localization%2520error.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Robot%20Collaborative%20Localization%20and%20Planning%20with%20Inter-Ranging&entry.906535625=Derek%20Knowles%20and%20Adam%20Dai%20and%20Grace%20Gao&entry.1292438233=%20%20Robots%20often%20use%20feature-based%20image%20tracking%20to%20identify%20their%20position%20in%0Atheir%20surrounding%20environment%3B%20however%2C%20feature-based%20image%20tracking%20is%20prone%0Ato%20errors%20in%20low-textured%20and%20poorly%20lit%20environments.%20Specifically%2C%20we%0Ainvestigate%20a%20scenario%20where%20robots%20are%20tasked%20with%20exploring%20the%20surface%20of%0Athe%20Moon%20and%20are%20required%20to%20have%20an%20accurate%20estimate%20of%20their%20position%20to%20be%0Aable%20to%20correctly%20geotag%20scientific%20measurements.%20To%20reduce%20localization%20error%2C%0Awe%20complement%20traditional%20feature-based%20image%20tracking%20with%20ultra-wideband%0A%28UWB%29%20distance%20measurements%20between%20the%20robots.%20The%20robots%20use%20an%20advanced%0Amesh-ranging%20protocol%20that%20allows%20them%20to%20continuously%20share%20distance%0Ameasurements%20amongst%20each%20other%20rather%20than%20relying%20on%20the%20common%20%22anchor%22%20and%0A%22tag%22%20UWB%20architecture.%20We%20develop%20a%20decentralized%20multi-robot%20coordination%0Aalgorithm%20that%20actively%20plans%20paths%20based%20on%20measurement%20line-of-sight%20vectors%0Aamongst%20all%20robots%20to%20minimize%20collective%20localization%20error.%20We%20then%0Ademonstrate%20the%20emergent%20behavior%20of%20the%20proposed%20multi-robot%20coordination%0Aalgorithm%20both%20in%20simulation%20and%20hardware%20to%20lower%20a%20geometry-based%20uncertainty%0Ametric%20and%20reduce%20localization%20error.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16679v1&entry.124074799=Read"},
{"title": "SegNet4D: Effective and Efficient 4D LiDAR Semantic Segmentation in\n  Autonomous Driving Environments", "author": "Neng Wang and Ruibin Guo and Chenghao Shi and Hui Zhang and Huimin Lu and Zhiqiang Zheng and Xieyuanli Chen", "abstract": "  4D LiDAR semantic segmentation, also referred to as multi-scan semantic\nsegmentation, plays a crucial role in enhancing the environmental understanding\ncapabilities of autonomous vehicles. It entails identifying the semantic\ncategory of each point in the LiDAR scan and distinguishing whether it is\ndynamic, a critical aspect in downstream tasks such as path planning and\nautonomous navigation. Existing methods for 4D semantic segmentation often rely\non computationally intensive 4D convolutions for multi-scan input, resulting in\npoor real-time performance. In this article, we introduce SegNet4D, a novel\nreal-time multi-scan semantic segmentation method leveraging a projection-based\napproach for fast motion feature encoding, showcasing outstanding performance.\nSegNet4D treats 4D semantic segmentation as two distinct tasks: single-scan\nsemantic segmentation and moving object segmentation, each addressed by\ndedicated head. These results are then fused in the proposed motion-semantic\nfusion module to achieve comprehensive multi-scan semantic segmentation.\nBesides, we propose extracting instance information from the current scan and\nincorporating it into the network for instance-aware segmentation. Our approach\nexhibits state-of-the-art performance across multiple datasets and stands out\nas a real-time multi-scan semantic segmentation method. The implementation of\nSegNet4D will be made available at\n\\url{https://github.com/nubot-nudt/SegNet4D}.\n", "link": "http://arxiv.org/abs/2406.16279v1", "date": "2024-06-24", "relevancy": 2.3251, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5951}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5825}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SegNet4D%3A%20Effective%20and%20Efficient%204D%20LiDAR%20Semantic%20Segmentation%20in%0A%20%20Autonomous%20Driving%20Environments&body=Title%3A%20SegNet4D%3A%20Effective%20and%20Efficient%204D%20LiDAR%20Semantic%20Segmentation%20in%0A%20%20Autonomous%20Driving%20Environments%0AAuthor%3A%20Neng%20Wang%20and%20Ruibin%20Guo%20and%20Chenghao%20Shi%20and%20Hui%20Zhang%20and%20Huimin%20Lu%20and%20Zhiqiang%20Zheng%20and%20Xieyuanli%20Chen%0AAbstract%3A%20%20%204D%20LiDAR%20semantic%20segmentation%2C%20also%20referred%20to%20as%20multi-scan%20semantic%0Asegmentation%2C%20plays%20a%20crucial%20role%20in%20enhancing%20the%20environmental%20understanding%0Acapabilities%20of%20autonomous%20vehicles.%20It%20entails%20identifying%20the%20semantic%0Acategory%20of%20each%20point%20in%20the%20LiDAR%20scan%20and%20distinguishing%20whether%20it%20is%0Adynamic%2C%20a%20critical%20aspect%20in%20downstream%20tasks%20such%20as%20path%20planning%20and%0Aautonomous%20navigation.%20Existing%20methods%20for%204D%20semantic%20segmentation%20often%20rely%0Aon%20computationally%20intensive%204D%20convolutions%20for%20multi-scan%20input%2C%20resulting%20in%0Apoor%20real-time%20performance.%20In%20this%20article%2C%20we%20introduce%20SegNet4D%2C%20a%20novel%0Areal-time%20multi-scan%20semantic%20segmentation%20method%20leveraging%20a%20projection-based%0Aapproach%20for%20fast%20motion%20feature%20encoding%2C%20showcasing%20outstanding%20performance.%0ASegNet4D%20treats%204D%20semantic%20segmentation%20as%20two%20distinct%20tasks%3A%20single-scan%0Asemantic%20segmentation%20and%20moving%20object%20segmentation%2C%20each%20addressed%20by%0Adedicated%20head.%20These%20results%20are%20then%20fused%20in%20the%20proposed%20motion-semantic%0Afusion%20module%20to%20achieve%20comprehensive%20multi-scan%20semantic%20segmentation.%0ABesides%2C%20we%20propose%20extracting%20instance%20information%20from%20the%20current%20scan%20and%0Aincorporating%20it%20into%20the%20network%20for%20instance-aware%20segmentation.%20Our%20approach%0Aexhibits%20state-of-the-art%20performance%20across%20multiple%20datasets%20and%20stands%20out%0Aas%20a%20real-time%20multi-scan%20semantic%20segmentation%20method.%20The%20implementation%20of%0ASegNet4D%20will%20be%20made%20available%20at%0A%5Curl%7Bhttps%3A//github.com/nubot-nudt/SegNet4D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegNet4D%253A%2520Effective%2520and%2520Efficient%25204D%2520LiDAR%2520Semantic%2520Segmentation%2520in%250A%2520%2520Autonomous%2520Driving%2520Environments%26entry.906535625%3DNeng%2520Wang%2520and%2520Ruibin%2520Guo%2520and%2520Chenghao%2520Shi%2520and%2520Hui%2520Zhang%2520and%2520Huimin%2520Lu%2520and%2520Zhiqiang%2520Zheng%2520and%2520Xieyuanli%2520Chen%26entry.1292438233%3D%2520%25204D%2520LiDAR%2520semantic%2520segmentation%252C%2520also%2520referred%2520to%2520as%2520multi-scan%2520semantic%250Asegmentation%252C%2520plays%2520a%2520crucial%2520role%2520in%2520enhancing%2520the%2520environmental%2520understanding%250Acapabilities%2520of%2520autonomous%2520vehicles.%2520It%2520entails%2520identifying%2520the%2520semantic%250Acategory%2520of%2520each%2520point%2520in%2520the%2520LiDAR%2520scan%2520and%2520distinguishing%2520whether%2520it%2520is%250Adynamic%252C%2520a%2520critical%2520aspect%2520in%2520downstream%2520tasks%2520such%2520as%2520path%2520planning%2520and%250Aautonomous%2520navigation.%2520Existing%2520methods%2520for%25204D%2520semantic%2520segmentation%2520often%2520rely%250Aon%2520computationally%2520intensive%25204D%2520convolutions%2520for%2520multi-scan%2520input%252C%2520resulting%2520in%250Apoor%2520real-time%2520performance.%2520In%2520this%2520article%252C%2520we%2520introduce%2520SegNet4D%252C%2520a%2520novel%250Areal-time%2520multi-scan%2520semantic%2520segmentation%2520method%2520leveraging%2520a%2520projection-based%250Aapproach%2520for%2520fast%2520motion%2520feature%2520encoding%252C%2520showcasing%2520outstanding%2520performance.%250ASegNet4D%2520treats%25204D%2520semantic%2520segmentation%2520as%2520two%2520distinct%2520tasks%253A%2520single-scan%250Asemantic%2520segmentation%2520and%2520moving%2520object%2520segmentation%252C%2520each%2520addressed%2520by%250Adedicated%2520head.%2520These%2520results%2520are%2520then%2520fused%2520in%2520the%2520proposed%2520motion-semantic%250Afusion%2520module%2520to%2520achieve%2520comprehensive%2520multi-scan%2520semantic%2520segmentation.%250ABesides%252C%2520we%2520propose%2520extracting%2520instance%2520information%2520from%2520the%2520current%2520scan%2520and%250Aincorporating%2520it%2520into%2520the%2520network%2520for%2520instance-aware%2520segmentation.%2520Our%2520approach%250Aexhibits%2520state-of-the-art%2520performance%2520across%2520multiple%2520datasets%2520and%2520stands%2520out%250Aas%2520a%2520real-time%2520multi-scan%2520semantic%2520segmentation%2520method.%2520The%2520implementation%2520of%250ASegNet4D%2520will%2520be%2520made%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/nubot-nudt/SegNet4D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SegNet4D%3A%20Effective%20and%20Efficient%204D%20LiDAR%20Semantic%20Segmentation%20in%0A%20%20Autonomous%20Driving%20Environments&entry.906535625=Neng%20Wang%20and%20Ruibin%20Guo%20and%20Chenghao%20Shi%20and%20Hui%20Zhang%20and%20Huimin%20Lu%20and%20Zhiqiang%20Zheng%20and%20Xieyuanli%20Chen&entry.1292438233=%20%204D%20LiDAR%20semantic%20segmentation%2C%20also%20referred%20to%20as%20multi-scan%20semantic%0Asegmentation%2C%20plays%20a%20crucial%20role%20in%20enhancing%20the%20environmental%20understanding%0Acapabilities%20of%20autonomous%20vehicles.%20It%20entails%20identifying%20the%20semantic%0Acategory%20of%20each%20point%20in%20the%20LiDAR%20scan%20and%20distinguishing%20whether%20it%20is%0Adynamic%2C%20a%20critical%20aspect%20in%20downstream%20tasks%20such%20as%20path%20planning%20and%0Aautonomous%20navigation.%20Existing%20methods%20for%204D%20semantic%20segmentation%20often%20rely%0Aon%20computationally%20intensive%204D%20convolutions%20for%20multi-scan%20input%2C%20resulting%20in%0Apoor%20real-time%20performance.%20In%20this%20article%2C%20we%20introduce%20SegNet4D%2C%20a%20novel%0Areal-time%20multi-scan%20semantic%20segmentation%20method%20leveraging%20a%20projection-based%0Aapproach%20for%20fast%20motion%20feature%20encoding%2C%20showcasing%20outstanding%20performance.%0ASegNet4D%20treats%204D%20semantic%20segmentation%20as%20two%20distinct%20tasks%3A%20single-scan%0Asemantic%20segmentation%20and%20moving%20object%20segmentation%2C%20each%20addressed%20by%0Adedicated%20head.%20These%20results%20are%20then%20fused%20in%20the%20proposed%20motion-semantic%0Afusion%20module%20to%20achieve%20comprehensive%20multi-scan%20semantic%20segmentation.%0ABesides%2C%20we%20propose%20extracting%20instance%20information%20from%20the%20current%20scan%20and%0Aincorporating%20it%20into%20the%20network%20for%20instance-aware%20segmentation.%20Our%20approach%0Aexhibits%20state-of-the-art%20performance%20across%20multiple%20datasets%20and%20stands%20out%0Aas%20a%20real-time%20multi-scan%20semantic%20segmentation%20method.%20The%20implementation%20of%0ASegNet4D%20will%20be%20made%20available%20at%0A%5Curl%7Bhttps%3A//github.com/nubot-nudt/SegNet4D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16279v1&entry.124074799=Read"},
{"title": "ShanghaiTech Mapping Robot is All You Need: Robot System for Collecting\n  Universal Ground Vehicle Datasets", "author": "Bowen Xu and Xiting Zhao and Delin Feng and Yuanyuan Yang and S\u00f6ren Schwertfeger", "abstract": "  This paper presents the ShanghaiTech Mapping Robot, a state-of-the-art\nunmanned ground vehicle (UGV) designed for collecting comprehensive\nmulti-sensor datasets to support research in robotics, computer vision, and\nautonomous driving. The robot is equipped with a wide array of sensors\nincluding RGB cameras, RGB-D cameras, event-based cameras, IR cameras, LiDARs,\nmmWave radars, IMUs, ultrasonic range finders, and a GNSS RTK receiver. The\nsensor suite is integrated onto a specially designed mechanical structure with\na centralized power system and a synchronization mechanism to ensure spatial\nand temporal alignment of the sensor data. A 16-node on-board computing cluster\nhandles sensor control, data collection, and storage. We describe the hardware\nand software architecture of the robot in detail and discuss the calibration\nprocedures for the various sensors. The capabilities of the platform are\ndemonstrated through an extensive dataset collected in diverse real-world\nenvironments. To facilitate research, we make the dataset publicly available\nalong with the associated robot sensor calibration data. Performance\nevaluations on a set of standard perception and localization tasks showcase the\npotential of the dataset to support developments in Robot Autonomy.\n", "link": "http://arxiv.org/abs/2406.16713v1", "date": "2024-06-24", "relevancy": 2.3228, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5924}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5757}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShanghaiTech%20Mapping%20Robot%20is%20All%20You%20Need%3A%20Robot%20System%20for%20Collecting%0A%20%20Universal%20Ground%20Vehicle%20Datasets&body=Title%3A%20ShanghaiTech%20Mapping%20Robot%20is%20All%20You%20Need%3A%20Robot%20System%20for%20Collecting%0A%20%20Universal%20Ground%20Vehicle%20Datasets%0AAuthor%3A%20Bowen%20Xu%20and%20Xiting%20Zhao%20and%20Delin%20Feng%20and%20Yuanyuan%20Yang%20and%20S%C3%B6ren%20Schwertfeger%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20ShanghaiTech%20Mapping%20Robot%2C%20a%20state-of-the-art%0Aunmanned%20ground%20vehicle%20%28UGV%29%20designed%20for%20collecting%20comprehensive%0Amulti-sensor%20datasets%20to%20support%20research%20in%20robotics%2C%20computer%20vision%2C%20and%0Aautonomous%20driving.%20The%20robot%20is%20equipped%20with%20a%20wide%20array%20of%20sensors%0Aincluding%20RGB%20cameras%2C%20RGB-D%20cameras%2C%20event-based%20cameras%2C%20IR%20cameras%2C%20LiDARs%2C%0AmmWave%20radars%2C%20IMUs%2C%20ultrasonic%20range%20finders%2C%20and%20a%20GNSS%20RTK%20receiver.%20The%0Asensor%20suite%20is%20integrated%20onto%20a%20specially%20designed%20mechanical%20structure%20with%0Aa%20centralized%20power%20system%20and%20a%20synchronization%20mechanism%20to%20ensure%20spatial%0Aand%20temporal%20alignment%20of%20the%20sensor%20data.%20A%2016-node%20on-board%20computing%20cluster%0Ahandles%20sensor%20control%2C%20data%20collection%2C%20and%20storage.%20We%20describe%20the%20hardware%0Aand%20software%20architecture%20of%20the%20robot%20in%20detail%20and%20discuss%20the%20calibration%0Aprocedures%20for%20the%20various%20sensors.%20The%20capabilities%20of%20the%20platform%20are%0Ademonstrated%20through%20an%20extensive%20dataset%20collected%20in%20diverse%20real-world%0Aenvironments.%20To%20facilitate%20research%2C%20we%20make%20the%20dataset%20publicly%20available%0Aalong%20with%20the%20associated%20robot%20sensor%20calibration%20data.%20Performance%0Aevaluations%20on%20a%20set%20of%20standard%20perception%20and%20localization%20tasks%20showcase%20the%0Apotential%20of%20the%20dataset%20to%20support%20developments%20in%20Robot%20Autonomy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShanghaiTech%2520Mapping%2520Robot%2520is%2520All%2520You%2520Need%253A%2520Robot%2520System%2520for%2520Collecting%250A%2520%2520Universal%2520Ground%2520Vehicle%2520Datasets%26entry.906535625%3DBowen%2520Xu%2520and%2520Xiting%2520Zhao%2520and%2520Delin%2520Feng%2520and%2520Yuanyuan%2520Yang%2520and%2520S%25C3%25B6ren%2520Schwertfeger%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520ShanghaiTech%2520Mapping%2520Robot%252C%2520a%2520state-of-the-art%250Aunmanned%2520ground%2520vehicle%2520%2528UGV%2529%2520designed%2520for%2520collecting%2520comprehensive%250Amulti-sensor%2520datasets%2520to%2520support%2520research%2520in%2520robotics%252C%2520computer%2520vision%252C%2520and%250Aautonomous%2520driving.%2520The%2520robot%2520is%2520equipped%2520with%2520a%2520wide%2520array%2520of%2520sensors%250Aincluding%2520RGB%2520cameras%252C%2520RGB-D%2520cameras%252C%2520event-based%2520cameras%252C%2520IR%2520cameras%252C%2520LiDARs%252C%250AmmWave%2520radars%252C%2520IMUs%252C%2520ultrasonic%2520range%2520finders%252C%2520and%2520a%2520GNSS%2520RTK%2520receiver.%2520The%250Asensor%2520suite%2520is%2520integrated%2520onto%2520a%2520specially%2520designed%2520mechanical%2520structure%2520with%250Aa%2520centralized%2520power%2520system%2520and%2520a%2520synchronization%2520mechanism%2520to%2520ensure%2520spatial%250Aand%2520temporal%2520alignment%2520of%2520the%2520sensor%2520data.%2520A%252016-node%2520on-board%2520computing%2520cluster%250Ahandles%2520sensor%2520control%252C%2520data%2520collection%252C%2520and%2520storage.%2520We%2520describe%2520the%2520hardware%250Aand%2520software%2520architecture%2520of%2520the%2520robot%2520in%2520detail%2520and%2520discuss%2520the%2520calibration%250Aprocedures%2520for%2520the%2520various%2520sensors.%2520The%2520capabilities%2520of%2520the%2520platform%2520are%250Ademonstrated%2520through%2520an%2520extensive%2520dataset%2520collected%2520in%2520diverse%2520real-world%250Aenvironments.%2520To%2520facilitate%2520research%252C%2520we%2520make%2520the%2520dataset%2520publicly%2520available%250Aalong%2520with%2520the%2520associated%2520robot%2520sensor%2520calibration%2520data.%2520Performance%250Aevaluations%2520on%2520a%2520set%2520of%2520standard%2520perception%2520and%2520localization%2520tasks%2520showcase%2520the%250Apotential%2520of%2520the%2520dataset%2520to%2520support%2520developments%2520in%2520Robot%2520Autonomy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShanghaiTech%20Mapping%20Robot%20is%20All%20You%20Need%3A%20Robot%20System%20for%20Collecting%0A%20%20Universal%20Ground%20Vehicle%20Datasets&entry.906535625=Bowen%20Xu%20and%20Xiting%20Zhao%20and%20Delin%20Feng%20and%20Yuanyuan%20Yang%20and%20S%C3%B6ren%20Schwertfeger&entry.1292438233=%20%20This%20paper%20presents%20the%20ShanghaiTech%20Mapping%20Robot%2C%20a%20state-of-the-art%0Aunmanned%20ground%20vehicle%20%28UGV%29%20designed%20for%20collecting%20comprehensive%0Amulti-sensor%20datasets%20to%20support%20research%20in%20robotics%2C%20computer%20vision%2C%20and%0Aautonomous%20driving.%20The%20robot%20is%20equipped%20with%20a%20wide%20array%20of%20sensors%0Aincluding%20RGB%20cameras%2C%20RGB-D%20cameras%2C%20event-based%20cameras%2C%20IR%20cameras%2C%20LiDARs%2C%0AmmWave%20radars%2C%20IMUs%2C%20ultrasonic%20range%20finders%2C%20and%20a%20GNSS%20RTK%20receiver.%20The%0Asensor%20suite%20is%20integrated%20onto%20a%20specially%20designed%20mechanical%20structure%20with%0Aa%20centralized%20power%20system%20and%20a%20synchronization%20mechanism%20to%20ensure%20spatial%0Aand%20temporal%20alignment%20of%20the%20sensor%20data.%20A%2016-node%20on-board%20computing%20cluster%0Ahandles%20sensor%20control%2C%20data%20collection%2C%20and%20storage.%20We%20describe%20the%20hardware%0Aand%20software%20architecture%20of%20the%20robot%20in%20detail%20and%20discuss%20the%20calibration%0Aprocedures%20for%20the%20various%20sensors.%20The%20capabilities%20of%20the%20platform%20are%0Ademonstrated%20through%20an%20extensive%20dataset%20collected%20in%20diverse%20real-world%0Aenvironments.%20To%20facilitate%20research%2C%20we%20make%20the%20dataset%20publicly%20available%0Aalong%20with%20the%20associated%20robot%20sensor%20calibration%20data.%20Performance%0Aevaluations%20on%20a%20set%20of%20standard%20perception%20and%20localization%20tasks%20showcase%20the%0Apotential%20of%20the%20dataset%20to%20support%20developments%20in%20Robot%20Autonomy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16713v1&entry.124074799=Read"},
{"title": "Towards Theoretical Understandings of Self-Consuming Generative Models", "author": "Shi Fu and Sen Zhang and Yingjie Wang and Xinmei Tian and Dacheng Tao", "abstract": "  This paper tackles the emerging challenge of training generative models\nwithin a self-consuming loop, wherein successive generations of models are\nrecursively trained on mixtures of real and synthetic data from previous\ngenerations. We construct a theoretical framework to rigorously evaluate how\nthis training procedure impacts the data distributions learned by future\nmodels, including parametric and non-parametric models. Specifically, we derive\nbounds on the total variation (TV) distance between the synthetic data\ndistributions produced by future models and the original real data distribution\nunder various mixed training scenarios for diffusion models with a\none-hidden-layer neural network score function. Our analysis demonstrates that\nthis distance can be effectively controlled under the condition that mixed\ntraining dataset sizes or proportions of real data are large enough.\nInterestingly, we further unveil a phase transition induced by expanding\nsynthetic data amounts, proving theoretically that while the TV distance\nexhibits an initial ascent, it declines beyond a threshold point. Finally, we\npresent results for kernel density estimation, delivering nuanced insights such\nas the impact of mixed data training on error propagation.\n", "link": "http://arxiv.org/abs/2402.11778v2", "date": "2024-06-24", "relevancy": 2.3219, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6146}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5902}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Theoretical%20Understandings%20of%20Self-Consuming%20Generative%20Models&body=Title%3A%20Towards%20Theoretical%20Understandings%20of%20Self-Consuming%20Generative%20Models%0AAuthor%3A%20Shi%20Fu%20and%20Sen%20Zhang%20and%20Yingjie%20Wang%20and%20Xinmei%20Tian%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20This%20paper%20tackles%20the%20emerging%20challenge%20of%20training%20generative%20models%0Awithin%20a%20self-consuming%20loop%2C%20wherein%20successive%20generations%20of%20models%20are%0Arecursively%20trained%20on%20mixtures%20of%20real%20and%20synthetic%20data%20from%20previous%0Agenerations.%20We%20construct%20a%20theoretical%20framework%20to%20rigorously%20evaluate%20how%0Athis%20training%20procedure%20impacts%20the%20data%20distributions%20learned%20by%20future%0Amodels%2C%20including%20parametric%20and%20non-parametric%20models.%20Specifically%2C%20we%20derive%0Abounds%20on%20the%20total%20variation%20%28TV%29%20distance%20between%20the%20synthetic%20data%0Adistributions%20produced%20by%20future%20models%20and%20the%20original%20real%20data%20distribution%0Aunder%20various%20mixed%20training%20scenarios%20for%20diffusion%20models%20with%20a%0Aone-hidden-layer%20neural%20network%20score%20function.%20Our%20analysis%20demonstrates%20that%0Athis%20distance%20can%20be%20effectively%20controlled%20under%20the%20condition%20that%20mixed%0Atraining%20dataset%20sizes%20or%20proportions%20of%20real%20data%20are%20large%20enough.%0AInterestingly%2C%20we%20further%20unveil%20a%20phase%20transition%20induced%20by%20expanding%0Asynthetic%20data%20amounts%2C%20proving%20theoretically%20that%20while%20the%20TV%20distance%0Aexhibits%20an%20initial%20ascent%2C%20it%20declines%20beyond%20a%20threshold%20point.%20Finally%2C%20we%0Apresent%20results%20for%20kernel%20density%20estimation%2C%20delivering%20nuanced%20insights%20such%0Aas%20the%20impact%20of%20mixed%20data%20training%20on%20error%20propagation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11778v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Theoretical%2520Understandings%2520of%2520Self-Consuming%2520Generative%2520Models%26entry.906535625%3DShi%2520Fu%2520and%2520Sen%2520Zhang%2520and%2520Yingjie%2520Wang%2520and%2520Xinmei%2520Tian%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520This%2520paper%2520tackles%2520the%2520emerging%2520challenge%2520of%2520training%2520generative%2520models%250Awithin%2520a%2520self-consuming%2520loop%252C%2520wherein%2520successive%2520generations%2520of%2520models%2520are%250Arecursively%2520trained%2520on%2520mixtures%2520of%2520real%2520and%2520synthetic%2520data%2520from%2520previous%250Agenerations.%2520We%2520construct%2520a%2520theoretical%2520framework%2520to%2520rigorously%2520evaluate%2520how%250Athis%2520training%2520procedure%2520impacts%2520the%2520data%2520distributions%2520learned%2520by%2520future%250Amodels%252C%2520including%2520parametric%2520and%2520non-parametric%2520models.%2520Specifically%252C%2520we%2520derive%250Abounds%2520on%2520the%2520total%2520variation%2520%2528TV%2529%2520distance%2520between%2520the%2520synthetic%2520data%250Adistributions%2520produced%2520by%2520future%2520models%2520and%2520the%2520original%2520real%2520data%2520distribution%250Aunder%2520various%2520mixed%2520training%2520scenarios%2520for%2520diffusion%2520models%2520with%2520a%250Aone-hidden-layer%2520neural%2520network%2520score%2520function.%2520Our%2520analysis%2520demonstrates%2520that%250Athis%2520distance%2520can%2520be%2520effectively%2520controlled%2520under%2520the%2520condition%2520that%2520mixed%250Atraining%2520dataset%2520sizes%2520or%2520proportions%2520of%2520real%2520data%2520are%2520large%2520enough.%250AInterestingly%252C%2520we%2520further%2520unveil%2520a%2520phase%2520transition%2520induced%2520by%2520expanding%250Asynthetic%2520data%2520amounts%252C%2520proving%2520theoretically%2520that%2520while%2520the%2520TV%2520distance%250Aexhibits%2520an%2520initial%2520ascent%252C%2520it%2520declines%2520beyond%2520a%2520threshold%2520point.%2520Finally%252C%2520we%250Apresent%2520results%2520for%2520kernel%2520density%2520estimation%252C%2520delivering%2520nuanced%2520insights%2520such%250Aas%2520the%2520impact%2520of%2520mixed%2520data%2520training%2520on%2520error%2520propagation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11778v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Theoretical%20Understandings%20of%20Self-Consuming%20Generative%20Models&entry.906535625=Shi%20Fu%20and%20Sen%20Zhang%20and%20Yingjie%20Wang%20and%20Xinmei%20Tian%20and%20Dacheng%20Tao&entry.1292438233=%20%20This%20paper%20tackles%20the%20emerging%20challenge%20of%20training%20generative%20models%0Awithin%20a%20self-consuming%20loop%2C%20wherein%20successive%20generations%20of%20models%20are%0Arecursively%20trained%20on%20mixtures%20of%20real%20and%20synthetic%20data%20from%20previous%0Agenerations.%20We%20construct%20a%20theoretical%20framework%20to%20rigorously%20evaluate%20how%0Athis%20training%20procedure%20impacts%20the%20data%20distributions%20learned%20by%20future%0Amodels%2C%20including%20parametric%20and%20non-parametric%20models.%20Specifically%2C%20we%20derive%0Abounds%20on%20the%20total%20variation%20%28TV%29%20distance%20between%20the%20synthetic%20data%0Adistributions%20produced%20by%20future%20models%20and%20the%20original%20real%20data%20distribution%0Aunder%20various%20mixed%20training%20scenarios%20for%20diffusion%20models%20with%20a%0Aone-hidden-layer%20neural%20network%20score%20function.%20Our%20analysis%20demonstrates%20that%0Athis%20distance%20can%20be%20effectively%20controlled%20under%20the%20condition%20that%20mixed%0Atraining%20dataset%20sizes%20or%20proportions%20of%20real%20data%20are%20large%20enough.%0AInterestingly%2C%20we%20further%20unveil%20a%20phase%20transition%20induced%20by%20expanding%0Asynthetic%20data%20amounts%2C%20proving%20theoretically%20that%20while%20the%20TV%20distance%0Aexhibits%20an%20initial%20ascent%2C%20it%20declines%20beyond%20a%20threshold%20point.%20Finally%2C%20we%0Apresent%20results%20for%20kernel%20density%20estimation%2C%20delivering%20nuanced%20insights%20such%0Aas%20the%20impact%20of%20mixed%20data%20training%20on%20error%20propagation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11778v2&entry.124074799=Read"},
{"title": "SLOctolyzer: Fully automatic analysis toolkit for segmentation and\n  feature extracting in scanning laser ophthalmoscopy images", "author": "Jamie Burke and Samuel Gibbon and Justin Engelmann and Adam Threlfall and Ylenia Giarratano and Charlene Hamid and Stuart King and Ian J. C. MacCormick and Tom MacGillivray", "abstract": "  Purpose: To describe SLOctolyzer: an open-source analysis toolkit for en face\nretinal vessels appearing in infrared reflectance scanning laser ophthalmoscopy\n(SLO) images.\n  Methods: SLOctolyzer includes two main modules: segmentation and measurement.\nThe segmentation module use deep learning methods to delineate retinal anatomy,\nwhile the measurement module quantifies key retinal vascular features such as\nvessel complexity, density, tortuosity, and calibre. We evaluate the\nsegmentation module using unseen data and measure its reproducibility.\n  Results: SLOctolyzer's segmentation module performed well against unseen\ninternal test data (Dice for all-vessels, 0.9097; arteries, 0.8376; veins,\n0.8525; optic disc, 0.9430; fovea, 0.8837). External validation against severe\nretinal pathology showed decreased performance (Dice for arteries, 0.7180;\nveins, 0.7470; optic disc, 0.9032). SLOctolyzer had good reproducibility (mean\ndifference for fractal dimension, -0.0007; vessel density, -0.0003; vessel\ncalibre, -0.3154 $\\mu$m; tortuosity density, 0.0013). SLOctolyzer can process a\nmacula-centred SLO image in under 20 seconds and a disc-centred SLO image in\nunder 30 seconds using a standard laptop CPU.\n  Conclusions: To our knowledge, SLOctolyzer is the first open-source tool to\nconvert raw SLO images into reproducible and clinically meaningful retinal\nvascular parameters. SLO images are captured simultaneous to optical coherence\ntomography (OCT), and we believe our software will be useful for extracting\nretinal vascular measurements from large OCT image sets and linking them to\nocular or systemic diseases. It requires no specialist knowledge or proprietary\nsoftware, and allows manual correction of segmentations and re-computing of\nvascular metrics. SLOctolyzer is freely available at\nhttps://github.com/jaburke166/SLOctolyzer.\n", "link": "http://arxiv.org/abs/2406.16466v1", "date": "2024-06-24", "relevancy": 2.3189, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4965}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4501}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLOctolyzer%3A%20Fully%20automatic%20analysis%20toolkit%20for%20segmentation%20and%0A%20%20feature%20extracting%20in%20scanning%20laser%20ophthalmoscopy%20images&body=Title%3A%20SLOctolyzer%3A%20Fully%20automatic%20analysis%20toolkit%20for%20segmentation%20and%0A%20%20feature%20extracting%20in%20scanning%20laser%20ophthalmoscopy%20images%0AAuthor%3A%20Jamie%20Burke%20and%20Samuel%20Gibbon%20and%20Justin%20Engelmann%20and%20Adam%20Threlfall%20and%20Ylenia%20Giarratano%20and%20Charlene%20Hamid%20and%20Stuart%20King%20and%20Ian%20J.%20C.%20MacCormick%20and%20Tom%20MacGillivray%0AAbstract%3A%20%20%20Purpose%3A%20To%20describe%20SLOctolyzer%3A%20an%20open-source%20analysis%20toolkit%20for%20en%20face%0Aretinal%20vessels%20appearing%20in%20infrared%20reflectance%20scanning%20laser%20ophthalmoscopy%0A%28SLO%29%20images.%0A%20%20Methods%3A%20SLOctolyzer%20includes%20two%20main%20modules%3A%20segmentation%20and%20measurement.%0AThe%20segmentation%20module%20use%20deep%20learning%20methods%20to%20delineate%20retinal%20anatomy%2C%0Awhile%20the%20measurement%20module%20quantifies%20key%20retinal%20vascular%20features%20such%20as%0Avessel%20complexity%2C%20density%2C%20tortuosity%2C%20and%20calibre.%20We%20evaluate%20the%0Asegmentation%20module%20using%20unseen%20data%20and%20measure%20its%20reproducibility.%0A%20%20Results%3A%20SLOctolyzer%27s%20segmentation%20module%20performed%20well%20against%20unseen%0Ainternal%20test%20data%20%28Dice%20for%20all-vessels%2C%200.9097%3B%20arteries%2C%200.8376%3B%20veins%2C%0A0.8525%3B%20optic%20disc%2C%200.9430%3B%20fovea%2C%200.8837%29.%20External%20validation%20against%20severe%0Aretinal%20pathology%20showed%20decreased%20performance%20%28Dice%20for%20arteries%2C%200.7180%3B%0Aveins%2C%200.7470%3B%20optic%20disc%2C%200.9032%29.%20SLOctolyzer%20had%20good%20reproducibility%20%28mean%0Adifference%20for%20fractal%20dimension%2C%20-0.0007%3B%20vessel%20density%2C%20-0.0003%3B%20vessel%0Acalibre%2C%20-0.3154%20%24%5Cmu%24m%3B%20tortuosity%20density%2C%200.0013%29.%20SLOctolyzer%20can%20process%20a%0Amacula-centred%20SLO%20image%20in%20under%2020%20seconds%20and%20a%20disc-centred%20SLO%20image%20in%0Aunder%2030%20seconds%20using%20a%20standard%20laptop%20CPU.%0A%20%20Conclusions%3A%20To%20our%20knowledge%2C%20SLOctolyzer%20is%20the%20first%20open-source%20tool%20to%0Aconvert%20raw%20SLO%20images%20into%20reproducible%20and%20clinically%20meaningful%20retinal%0Avascular%20parameters.%20SLO%20images%20are%20captured%20simultaneous%20to%20optical%20coherence%0Atomography%20%28OCT%29%2C%20and%20we%20believe%20our%20software%20will%20be%20useful%20for%20extracting%0Aretinal%20vascular%20measurements%20from%20large%20OCT%20image%20sets%20and%20linking%20them%20to%0Aocular%20or%20systemic%20diseases.%20It%20requires%20no%20specialist%20knowledge%20or%20proprietary%0Asoftware%2C%20and%20allows%20manual%20correction%20of%20segmentations%20and%20re-computing%20of%0Avascular%20metrics.%20SLOctolyzer%20is%20freely%20available%20at%0Ahttps%3A//github.com/jaburke166/SLOctolyzer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16466v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLOctolyzer%253A%2520Fully%2520automatic%2520analysis%2520toolkit%2520for%2520segmentation%2520and%250A%2520%2520feature%2520extracting%2520in%2520scanning%2520laser%2520ophthalmoscopy%2520images%26entry.906535625%3DJamie%2520Burke%2520and%2520Samuel%2520Gibbon%2520and%2520Justin%2520Engelmann%2520and%2520Adam%2520Threlfall%2520and%2520Ylenia%2520Giarratano%2520and%2520Charlene%2520Hamid%2520and%2520Stuart%2520King%2520and%2520Ian%2520J.%2520C.%2520MacCormick%2520and%2520Tom%2520MacGillivray%26entry.1292438233%3D%2520%2520Purpose%253A%2520To%2520describe%2520SLOctolyzer%253A%2520an%2520open-source%2520analysis%2520toolkit%2520for%2520en%2520face%250Aretinal%2520vessels%2520appearing%2520in%2520infrared%2520reflectance%2520scanning%2520laser%2520ophthalmoscopy%250A%2528SLO%2529%2520images.%250A%2520%2520Methods%253A%2520SLOctolyzer%2520includes%2520two%2520main%2520modules%253A%2520segmentation%2520and%2520measurement.%250AThe%2520segmentation%2520module%2520use%2520deep%2520learning%2520methods%2520to%2520delineate%2520retinal%2520anatomy%252C%250Awhile%2520the%2520measurement%2520module%2520quantifies%2520key%2520retinal%2520vascular%2520features%2520such%2520as%250Avessel%2520complexity%252C%2520density%252C%2520tortuosity%252C%2520and%2520calibre.%2520We%2520evaluate%2520the%250Asegmentation%2520module%2520using%2520unseen%2520data%2520and%2520measure%2520its%2520reproducibility.%250A%2520%2520Results%253A%2520SLOctolyzer%2527s%2520segmentation%2520module%2520performed%2520well%2520against%2520unseen%250Ainternal%2520test%2520data%2520%2528Dice%2520for%2520all-vessels%252C%25200.9097%253B%2520arteries%252C%25200.8376%253B%2520veins%252C%250A0.8525%253B%2520optic%2520disc%252C%25200.9430%253B%2520fovea%252C%25200.8837%2529.%2520External%2520validation%2520against%2520severe%250Aretinal%2520pathology%2520showed%2520decreased%2520performance%2520%2528Dice%2520for%2520arteries%252C%25200.7180%253B%250Aveins%252C%25200.7470%253B%2520optic%2520disc%252C%25200.9032%2529.%2520SLOctolyzer%2520had%2520good%2520reproducibility%2520%2528mean%250Adifference%2520for%2520fractal%2520dimension%252C%2520-0.0007%253B%2520vessel%2520density%252C%2520-0.0003%253B%2520vessel%250Acalibre%252C%2520-0.3154%2520%2524%255Cmu%2524m%253B%2520tortuosity%2520density%252C%25200.0013%2529.%2520SLOctolyzer%2520can%2520process%2520a%250Amacula-centred%2520SLO%2520image%2520in%2520under%252020%2520seconds%2520and%2520a%2520disc-centred%2520SLO%2520image%2520in%250Aunder%252030%2520seconds%2520using%2520a%2520standard%2520laptop%2520CPU.%250A%2520%2520Conclusions%253A%2520To%2520our%2520knowledge%252C%2520SLOctolyzer%2520is%2520the%2520first%2520open-source%2520tool%2520to%250Aconvert%2520raw%2520SLO%2520images%2520into%2520reproducible%2520and%2520clinically%2520meaningful%2520retinal%250Avascular%2520parameters.%2520SLO%2520images%2520are%2520captured%2520simultaneous%2520to%2520optical%2520coherence%250Atomography%2520%2528OCT%2529%252C%2520and%2520we%2520believe%2520our%2520software%2520will%2520be%2520useful%2520for%2520extracting%250Aretinal%2520vascular%2520measurements%2520from%2520large%2520OCT%2520image%2520sets%2520and%2520linking%2520them%2520to%250Aocular%2520or%2520systemic%2520diseases.%2520It%2520requires%2520no%2520specialist%2520knowledge%2520or%2520proprietary%250Asoftware%252C%2520and%2520allows%2520manual%2520correction%2520of%2520segmentations%2520and%2520re-computing%2520of%250Avascular%2520metrics.%2520SLOctolyzer%2520is%2520freely%2520available%2520at%250Ahttps%253A//github.com/jaburke166/SLOctolyzer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16466v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLOctolyzer%3A%20Fully%20automatic%20analysis%20toolkit%20for%20segmentation%20and%0A%20%20feature%20extracting%20in%20scanning%20laser%20ophthalmoscopy%20images&entry.906535625=Jamie%20Burke%20and%20Samuel%20Gibbon%20and%20Justin%20Engelmann%20and%20Adam%20Threlfall%20and%20Ylenia%20Giarratano%20and%20Charlene%20Hamid%20and%20Stuart%20King%20and%20Ian%20J.%20C.%20MacCormick%20and%20Tom%20MacGillivray&entry.1292438233=%20%20Purpose%3A%20To%20describe%20SLOctolyzer%3A%20an%20open-source%20analysis%20toolkit%20for%20en%20face%0Aretinal%20vessels%20appearing%20in%20infrared%20reflectance%20scanning%20laser%20ophthalmoscopy%0A%28SLO%29%20images.%0A%20%20Methods%3A%20SLOctolyzer%20includes%20two%20main%20modules%3A%20segmentation%20and%20measurement.%0AThe%20segmentation%20module%20use%20deep%20learning%20methods%20to%20delineate%20retinal%20anatomy%2C%0Awhile%20the%20measurement%20module%20quantifies%20key%20retinal%20vascular%20features%20such%20as%0Avessel%20complexity%2C%20density%2C%20tortuosity%2C%20and%20calibre.%20We%20evaluate%20the%0Asegmentation%20module%20using%20unseen%20data%20and%20measure%20its%20reproducibility.%0A%20%20Results%3A%20SLOctolyzer%27s%20segmentation%20module%20performed%20well%20against%20unseen%0Ainternal%20test%20data%20%28Dice%20for%20all-vessels%2C%200.9097%3B%20arteries%2C%200.8376%3B%20veins%2C%0A0.8525%3B%20optic%20disc%2C%200.9430%3B%20fovea%2C%200.8837%29.%20External%20validation%20against%20severe%0Aretinal%20pathology%20showed%20decreased%20performance%20%28Dice%20for%20arteries%2C%200.7180%3B%0Aveins%2C%200.7470%3B%20optic%20disc%2C%200.9032%29.%20SLOctolyzer%20had%20good%20reproducibility%20%28mean%0Adifference%20for%20fractal%20dimension%2C%20-0.0007%3B%20vessel%20density%2C%20-0.0003%3B%20vessel%0Acalibre%2C%20-0.3154%20%24%5Cmu%24m%3B%20tortuosity%20density%2C%200.0013%29.%20SLOctolyzer%20can%20process%20a%0Amacula-centred%20SLO%20image%20in%20under%2020%20seconds%20and%20a%20disc-centred%20SLO%20image%20in%0Aunder%2030%20seconds%20using%20a%20standard%20laptop%20CPU.%0A%20%20Conclusions%3A%20To%20our%20knowledge%2C%20SLOctolyzer%20is%20the%20first%20open-source%20tool%20to%0Aconvert%20raw%20SLO%20images%20into%20reproducible%20and%20clinically%20meaningful%20retinal%0Avascular%20parameters.%20SLO%20images%20are%20captured%20simultaneous%20to%20optical%20coherence%0Atomography%20%28OCT%29%2C%20and%20we%20believe%20our%20software%20will%20be%20useful%20for%20extracting%0Aretinal%20vascular%20measurements%20from%20large%20OCT%20image%20sets%20and%20linking%20them%20to%0Aocular%20or%20systemic%20diseases.%20It%20requires%20no%20specialist%20knowledge%20or%20proprietary%0Asoftware%2C%20and%20allows%20manual%20correction%20of%20segmentations%20and%20re-computing%20of%0Avascular%20metrics.%20SLOctolyzer%20is%20freely%20available%20at%0Ahttps%3A//github.com/jaburke166/SLOctolyzer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16466v1&entry.124074799=Read"},
{"title": "Style-NeRF2NeRF: 3D Style Transfer From Style-Aligned Multi-View Images", "author": "Haruo Fujiwara and Yusuke Mukuta and Tatsuya Harada", "abstract": "  We propose a simple yet effective pipeline for stylizing a 3D scene,\nharnessing the power of 2D image diffusion models. Given a NeRF model\nreconstructed from a set of multi-view images, we perform 3D style transfer by\nrefining the source NeRF model using stylized images generated by a\nstyle-aligned image-to-image diffusion model. Given a target style prompt, we\nfirst generate perceptually similar multi-view images by leveraging a\ndepth-conditioned diffusion model with an attention-sharing mechanism. Next,\nbased on the stylized multi-view images, we propose to guide the style transfer\nprocess with the sliced Wasserstein loss based on the feature maps extracted\nfrom a pre-trained CNN model. Our pipeline consists of decoupled steps,\nallowing users to test various prompt ideas and preview the stylized 3D result\nbefore proceeding to the NeRF fine-tuning stage. We demonstrate that our method\ncan transfer diverse artistic styles to real-world 3D scenes with competitive\nquality. Result videos are also available on our project page:\nhttps://haruolabs.github.io/style-n2n/\n", "link": "http://arxiv.org/abs/2406.13393v2", "date": "2024-06-24", "relevancy": 2.3135, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5802}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5802}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Style-NeRF2NeRF%3A%203D%20Style%20Transfer%20From%20Style-Aligned%20Multi-View%20Images&body=Title%3A%20Style-NeRF2NeRF%3A%203D%20Style%20Transfer%20From%20Style-Aligned%20Multi-View%20Images%0AAuthor%3A%20Haruo%20Fujiwara%20and%20Yusuke%20Mukuta%20and%20Tatsuya%20Harada%0AAbstract%3A%20%20%20We%20propose%20a%20simple%20yet%20effective%20pipeline%20for%20stylizing%20a%203D%20scene%2C%0Aharnessing%20the%20power%20of%202D%20image%20diffusion%20models.%20Given%20a%20NeRF%20model%0Areconstructed%20from%20a%20set%20of%20multi-view%20images%2C%20we%20perform%203D%20style%20transfer%20by%0Arefining%20the%20source%20NeRF%20model%20using%20stylized%20images%20generated%20by%20a%0Astyle-aligned%20image-to-image%20diffusion%20model.%20Given%20a%20target%20style%20prompt%2C%20we%0Afirst%20generate%20perceptually%20similar%20multi-view%20images%20by%20leveraging%20a%0Adepth-conditioned%20diffusion%20model%20with%20an%20attention-sharing%20mechanism.%20Next%2C%0Abased%20on%20the%20stylized%20multi-view%20images%2C%20we%20propose%20to%20guide%20the%20style%20transfer%0Aprocess%20with%20the%20sliced%20Wasserstein%20loss%20based%20on%20the%20feature%20maps%20extracted%0Afrom%20a%20pre-trained%20CNN%20model.%20Our%20pipeline%20consists%20of%20decoupled%20steps%2C%0Aallowing%20users%20to%20test%20various%20prompt%20ideas%20and%20preview%20the%20stylized%203D%20result%0Abefore%20proceeding%20to%20the%20NeRF%20fine-tuning%20stage.%20We%20demonstrate%20that%20our%20method%0Acan%20transfer%20diverse%20artistic%20styles%20to%20real-world%203D%20scenes%20with%20competitive%0Aquality.%20Result%20videos%20are%20also%20available%20on%20our%20project%20page%3A%0Ahttps%3A//haruolabs.github.io/style-n2n/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13393v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStyle-NeRF2NeRF%253A%25203D%2520Style%2520Transfer%2520From%2520Style-Aligned%2520Multi-View%2520Images%26entry.906535625%3DHaruo%2520Fujiwara%2520and%2520Yusuke%2520Mukuta%2520and%2520Tatsuya%2520Harada%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520simple%2520yet%2520effective%2520pipeline%2520for%2520stylizing%2520a%25203D%2520scene%252C%250Aharnessing%2520the%2520power%2520of%25202D%2520image%2520diffusion%2520models.%2520Given%2520a%2520NeRF%2520model%250Areconstructed%2520from%2520a%2520set%2520of%2520multi-view%2520images%252C%2520we%2520perform%25203D%2520style%2520transfer%2520by%250Arefining%2520the%2520source%2520NeRF%2520model%2520using%2520stylized%2520images%2520generated%2520by%2520a%250Astyle-aligned%2520image-to-image%2520diffusion%2520model.%2520Given%2520a%2520target%2520style%2520prompt%252C%2520we%250Afirst%2520generate%2520perceptually%2520similar%2520multi-view%2520images%2520by%2520leveraging%2520a%250Adepth-conditioned%2520diffusion%2520model%2520with%2520an%2520attention-sharing%2520mechanism.%2520Next%252C%250Abased%2520on%2520the%2520stylized%2520multi-view%2520images%252C%2520we%2520propose%2520to%2520guide%2520the%2520style%2520transfer%250Aprocess%2520with%2520the%2520sliced%2520Wasserstein%2520loss%2520based%2520on%2520the%2520feature%2520maps%2520extracted%250Afrom%2520a%2520pre-trained%2520CNN%2520model.%2520Our%2520pipeline%2520consists%2520of%2520decoupled%2520steps%252C%250Aallowing%2520users%2520to%2520test%2520various%2520prompt%2520ideas%2520and%2520preview%2520the%2520stylized%25203D%2520result%250Abefore%2520proceeding%2520to%2520the%2520NeRF%2520fine-tuning%2520stage.%2520We%2520demonstrate%2520that%2520our%2520method%250Acan%2520transfer%2520diverse%2520artistic%2520styles%2520to%2520real-world%25203D%2520scenes%2520with%2520competitive%250Aquality.%2520Result%2520videos%2520are%2520also%2520available%2520on%2520our%2520project%2520page%253A%250Ahttps%253A//haruolabs.github.io/style-n2n/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13393v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Style-NeRF2NeRF%3A%203D%20Style%20Transfer%20From%20Style-Aligned%20Multi-View%20Images&entry.906535625=Haruo%20Fujiwara%20and%20Yusuke%20Mukuta%20and%20Tatsuya%20Harada&entry.1292438233=%20%20We%20propose%20a%20simple%20yet%20effective%20pipeline%20for%20stylizing%20a%203D%20scene%2C%0Aharnessing%20the%20power%20of%202D%20image%20diffusion%20models.%20Given%20a%20NeRF%20model%0Areconstructed%20from%20a%20set%20of%20multi-view%20images%2C%20we%20perform%203D%20style%20transfer%20by%0Arefining%20the%20source%20NeRF%20model%20using%20stylized%20images%20generated%20by%20a%0Astyle-aligned%20image-to-image%20diffusion%20model.%20Given%20a%20target%20style%20prompt%2C%20we%0Afirst%20generate%20perceptually%20similar%20multi-view%20images%20by%20leveraging%20a%0Adepth-conditioned%20diffusion%20model%20with%20an%20attention-sharing%20mechanism.%20Next%2C%0Abased%20on%20the%20stylized%20multi-view%20images%2C%20we%20propose%20to%20guide%20the%20style%20transfer%0Aprocess%20with%20the%20sliced%20Wasserstein%20loss%20based%20on%20the%20feature%20maps%20extracted%0Afrom%20a%20pre-trained%20CNN%20model.%20Our%20pipeline%20consists%20of%20decoupled%20steps%2C%0Aallowing%20users%20to%20test%20various%20prompt%20ideas%20and%20preview%20the%20stylized%203D%20result%0Abefore%20proceeding%20to%20the%20NeRF%20fine-tuning%20stage.%20We%20demonstrate%20that%20our%20method%0Acan%20transfer%20diverse%20artistic%20styles%20to%20real-world%203D%20scenes%20with%20competitive%0Aquality.%20Result%20videos%20are%20also%20available%20on%20our%20project%20page%3A%0Ahttps%3A//haruolabs.github.io/style-n2n/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13393v2&entry.124074799=Read"},
{"title": "Knowledge Accumulation in Continually Learned Representations and the\n  Issue of Feature Forgetting", "author": "Timm Hess and Eli Verwimp and Gido M. van de Ven and Tinne Tuytelaars", "abstract": "  Continual learning research has shown that neural networks suffer from\ncatastrophic forgetting \"at the output level\", but it is debated whether this\nis also the case at the level of learned representations. Multiple recent\nstudies ascribe representations a certain level of innate robustness against\nforgetting -- that they only forget minimally in comparison with forgetting at\nthe output level. We revisit and expand upon the experiments that revealed this\ndifference in forgetting and illustrate the coexistence of two phenomena that\naffect the quality of continually learned representations: knowledge\naccumulation and feature forgetting. Taking both aspects into account, we show\nthat, even though forgetting in the representation (i.e. feature forgetting)\ncan be small in absolute terms, when measuring relative to how much was learned\nduring a task, forgetting in the representation tends to be just as\ncatastrophic as forgetting at the output level. Next we show that this feature\nforgetting is problematic as it substantially slows down the incremental\nlearning of good general representations (i.e. knowledge accumulation).\nFinally, we study how feature forgetting and knowledge accumulation are\naffected by different types of continual learning methods.\n", "link": "http://arxiv.org/abs/2304.00933v4", "date": "2024-06-24", "relevancy": 2.3118, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4684}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4632}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Accumulation%20in%20Continually%20Learned%20Representations%20and%20the%0A%20%20Issue%20of%20Feature%20Forgetting&body=Title%3A%20Knowledge%20Accumulation%20in%20Continually%20Learned%20Representations%20and%20the%0A%20%20Issue%20of%20Feature%20Forgetting%0AAuthor%3A%20Timm%20Hess%20and%20Eli%20Verwimp%20and%20Gido%20M.%20van%20de%20Ven%20and%20Tinne%20Tuytelaars%0AAbstract%3A%20%20%20Continual%20learning%20research%20has%20shown%20that%20neural%20networks%20suffer%20from%0Acatastrophic%20forgetting%20%22at%20the%20output%20level%22%2C%20but%20it%20is%20debated%20whether%20this%0Ais%20also%20the%20case%20at%20the%20level%20of%20learned%20representations.%20Multiple%20recent%0Astudies%20ascribe%20representations%20a%20certain%20level%20of%20innate%20robustness%20against%0Aforgetting%20--%20that%20they%20only%20forget%20minimally%20in%20comparison%20with%20forgetting%20at%0Athe%20output%20level.%20We%20revisit%20and%20expand%20upon%20the%20experiments%20that%20revealed%20this%0Adifference%20in%20forgetting%20and%20illustrate%20the%20coexistence%20of%20two%20phenomena%20that%0Aaffect%20the%20quality%20of%20continually%20learned%20representations%3A%20knowledge%0Aaccumulation%20and%20feature%20forgetting.%20Taking%20both%20aspects%20into%20account%2C%20we%20show%0Athat%2C%20even%20though%20forgetting%20in%20the%20representation%20%28i.e.%20feature%20forgetting%29%0Acan%20be%20small%20in%20absolute%20terms%2C%20when%20measuring%20relative%20to%20how%20much%20was%20learned%0Aduring%20a%20task%2C%20forgetting%20in%20the%20representation%20tends%20to%20be%20just%20as%0Acatastrophic%20as%20forgetting%20at%20the%20output%20level.%20Next%20we%20show%20that%20this%20feature%0Aforgetting%20is%20problematic%20as%20it%20substantially%20slows%20down%20the%20incremental%0Alearning%20of%20good%20general%20representations%20%28i.e.%20knowledge%20accumulation%29.%0AFinally%2C%20we%20study%20how%20feature%20forgetting%20and%20knowledge%20accumulation%20are%0Aaffected%20by%20different%20types%20of%20continual%20learning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.00933v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Accumulation%2520in%2520Continually%2520Learned%2520Representations%2520and%2520the%250A%2520%2520Issue%2520of%2520Feature%2520Forgetting%26entry.906535625%3DTimm%2520Hess%2520and%2520Eli%2520Verwimp%2520and%2520Gido%2520M.%2520van%2520de%2520Ven%2520and%2520Tinne%2520Tuytelaars%26entry.1292438233%3D%2520%2520Continual%2520learning%2520research%2520has%2520shown%2520that%2520neural%2520networks%2520suffer%2520from%250Acatastrophic%2520forgetting%2520%2522at%2520the%2520output%2520level%2522%252C%2520but%2520it%2520is%2520debated%2520whether%2520this%250Ais%2520also%2520the%2520case%2520at%2520the%2520level%2520of%2520learned%2520representations.%2520Multiple%2520recent%250Astudies%2520ascribe%2520representations%2520a%2520certain%2520level%2520of%2520innate%2520robustness%2520against%250Aforgetting%2520--%2520that%2520they%2520only%2520forget%2520minimally%2520in%2520comparison%2520with%2520forgetting%2520at%250Athe%2520output%2520level.%2520We%2520revisit%2520and%2520expand%2520upon%2520the%2520experiments%2520that%2520revealed%2520this%250Adifference%2520in%2520forgetting%2520and%2520illustrate%2520the%2520coexistence%2520of%2520two%2520phenomena%2520that%250Aaffect%2520the%2520quality%2520of%2520continually%2520learned%2520representations%253A%2520knowledge%250Aaccumulation%2520and%2520feature%2520forgetting.%2520Taking%2520both%2520aspects%2520into%2520account%252C%2520we%2520show%250Athat%252C%2520even%2520though%2520forgetting%2520in%2520the%2520representation%2520%2528i.e.%2520feature%2520forgetting%2529%250Acan%2520be%2520small%2520in%2520absolute%2520terms%252C%2520when%2520measuring%2520relative%2520to%2520how%2520much%2520was%2520learned%250Aduring%2520a%2520task%252C%2520forgetting%2520in%2520the%2520representation%2520tends%2520to%2520be%2520just%2520as%250Acatastrophic%2520as%2520forgetting%2520at%2520the%2520output%2520level.%2520Next%2520we%2520show%2520that%2520this%2520feature%250Aforgetting%2520is%2520problematic%2520as%2520it%2520substantially%2520slows%2520down%2520the%2520incremental%250Alearning%2520of%2520good%2520general%2520representations%2520%2528i.e.%2520knowledge%2520accumulation%2529.%250AFinally%252C%2520we%2520study%2520how%2520feature%2520forgetting%2520and%2520knowledge%2520accumulation%2520are%250Aaffected%2520by%2520different%2520types%2520of%2520continual%2520learning%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.00933v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Accumulation%20in%20Continually%20Learned%20Representations%20and%20the%0A%20%20Issue%20of%20Feature%20Forgetting&entry.906535625=Timm%20Hess%20and%20Eli%20Verwimp%20and%20Gido%20M.%20van%20de%20Ven%20and%20Tinne%20Tuytelaars&entry.1292438233=%20%20Continual%20learning%20research%20has%20shown%20that%20neural%20networks%20suffer%20from%0Acatastrophic%20forgetting%20%22at%20the%20output%20level%22%2C%20but%20it%20is%20debated%20whether%20this%0Ais%20also%20the%20case%20at%20the%20level%20of%20learned%20representations.%20Multiple%20recent%0Astudies%20ascribe%20representations%20a%20certain%20level%20of%20innate%20robustness%20against%0Aforgetting%20--%20that%20they%20only%20forget%20minimally%20in%20comparison%20with%20forgetting%20at%0Athe%20output%20level.%20We%20revisit%20and%20expand%20upon%20the%20experiments%20that%20revealed%20this%0Adifference%20in%20forgetting%20and%20illustrate%20the%20coexistence%20of%20two%20phenomena%20that%0Aaffect%20the%20quality%20of%20continually%20learned%20representations%3A%20knowledge%0Aaccumulation%20and%20feature%20forgetting.%20Taking%20both%20aspects%20into%20account%2C%20we%20show%0Athat%2C%20even%20though%20forgetting%20in%20the%20representation%20%28i.e.%20feature%20forgetting%29%0Acan%20be%20small%20in%20absolute%20terms%2C%20when%20measuring%20relative%20to%20how%20much%20was%20learned%0Aduring%20a%20task%2C%20forgetting%20in%20the%20representation%20tends%20to%20be%20just%20as%0Acatastrophic%20as%20forgetting%20at%20the%20output%20level.%20Next%20we%20show%20that%20this%20feature%0Aforgetting%20is%20problematic%20as%20it%20substantially%20slows%20down%20the%20incremental%0Alearning%20of%20good%20general%20representations%20%28i.e.%20knowledge%20accumulation%29.%0AFinally%2C%20we%20study%20how%20feature%20forgetting%20and%20knowledge%20accumulation%20are%0Aaffected%20by%20different%20types%20of%20continual%20learning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.00933v4&entry.124074799=Read"},
{"title": "Concentration Inequalities for $(f,\u0393)$-GANs", "author": "Jeremiah Birrell", "abstract": "  Generative adversarial networks (GANs) are unsupervised learning methods for\ntraining a generator distribution to produce samples that approximate those\ndrawn from a target distribution. Many such methods can be formulated as\nminimization of a metric or divergence. Recent works have proven the\nstatistical consistency of GANs that are based on integral probability metrics\n(IPMs), e.g., WGAN which is based on the 1-Wasserstein metric. IPMs are defined\nby optimizing a linear functional (difference of expectations) over a space of\ndiscriminators. A much larger class of GANs, which allow for the use of\nnonlinear objective functionals, can be constructed using\n$(f,\\Gamma)$-divergences; these generalize and interpolate between IPMs and\n$f$-divergences (e.g., KL or $\\alpha$-divergences). Instances of\n$(f,\\Gamma)$-GANs have been shown to exhibit improved performance in a number\nof applications. In this work we study the statistical consistency of\n$(f,\\Gamma)$-GANs for general $f$ and $\\Gamma$. Specifically, we derive\nfinite-sample concentration inequalities. These derivations require novel\narguments due to nonlinearity of the objective functional. We demonstrate that\nour new results reduce to the known results for IPM-GANs in the appropriate\nlimit while also significantly extending the domain of applicability of this\ntheory.\n", "link": "http://arxiv.org/abs/2406.16834v1", "date": "2024-06-24", "relevancy": 2.3099, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4632}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4615}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concentration%20Inequalities%20for%20%24%28f%2C%CE%93%29%24-GANs&body=Title%3A%20Concentration%20Inequalities%20for%20%24%28f%2C%CE%93%29%24-GANs%0AAuthor%3A%20Jeremiah%20Birrell%0AAbstract%3A%20%20%20Generative%20adversarial%20networks%20%28GANs%29%20are%20unsupervised%20learning%20methods%20for%0Atraining%20a%20generator%20distribution%20to%20produce%20samples%20that%20approximate%20those%0Adrawn%20from%20a%20target%20distribution.%20Many%20such%20methods%20can%20be%20formulated%20as%0Aminimization%20of%20a%20metric%20or%20divergence.%20Recent%20works%20have%20proven%20the%0Astatistical%20consistency%20of%20GANs%20that%20are%20based%20on%20integral%20probability%20metrics%0A%28IPMs%29%2C%20e.g.%2C%20WGAN%20which%20is%20based%20on%20the%201-Wasserstein%20metric.%20IPMs%20are%20defined%0Aby%20optimizing%20a%20linear%20functional%20%28difference%20of%20expectations%29%20over%20a%20space%20of%0Adiscriminators.%20A%20much%20larger%20class%20of%20GANs%2C%20which%20allow%20for%20the%20use%20of%0Anonlinear%20objective%20functionals%2C%20can%20be%20constructed%20using%0A%24%28f%2C%5CGamma%29%24-divergences%3B%20these%20generalize%20and%20interpolate%20between%20IPMs%20and%0A%24f%24-divergences%20%28e.g.%2C%20KL%20or%20%24%5Calpha%24-divergences%29.%20Instances%20of%0A%24%28f%2C%5CGamma%29%24-GANs%20have%20been%20shown%20to%20exhibit%20improved%20performance%20in%20a%20number%0Aof%20applications.%20In%20this%20work%20we%20study%20the%20statistical%20consistency%20of%0A%24%28f%2C%5CGamma%29%24-GANs%20for%20general%20%24f%24%20and%20%24%5CGamma%24.%20Specifically%2C%20we%20derive%0Afinite-sample%20concentration%20inequalities.%20These%20derivations%20require%20novel%0Aarguments%20due%20to%20nonlinearity%20of%20the%20objective%20functional.%20We%20demonstrate%20that%0Aour%20new%20results%20reduce%20to%20the%20known%20results%20for%20IPM-GANs%20in%20the%20appropriate%0Alimit%20while%20also%20significantly%20extending%20the%20domain%20of%20applicability%20of%20this%0Atheory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcentration%2520Inequalities%2520for%2520%2524%2528f%252C%25CE%2593%2529%2524-GANs%26entry.906535625%3DJeremiah%2520Birrell%26entry.1292438233%3D%2520%2520Generative%2520adversarial%2520networks%2520%2528GANs%2529%2520are%2520unsupervised%2520learning%2520methods%2520for%250Atraining%2520a%2520generator%2520distribution%2520to%2520produce%2520samples%2520that%2520approximate%2520those%250Adrawn%2520from%2520a%2520target%2520distribution.%2520Many%2520such%2520methods%2520can%2520be%2520formulated%2520as%250Aminimization%2520of%2520a%2520metric%2520or%2520divergence.%2520Recent%2520works%2520have%2520proven%2520the%250Astatistical%2520consistency%2520of%2520GANs%2520that%2520are%2520based%2520on%2520integral%2520probability%2520metrics%250A%2528IPMs%2529%252C%2520e.g.%252C%2520WGAN%2520which%2520is%2520based%2520on%2520the%25201-Wasserstein%2520metric.%2520IPMs%2520are%2520defined%250Aby%2520optimizing%2520a%2520linear%2520functional%2520%2528difference%2520of%2520expectations%2529%2520over%2520a%2520space%2520of%250Adiscriminators.%2520A%2520much%2520larger%2520class%2520of%2520GANs%252C%2520which%2520allow%2520for%2520the%2520use%2520of%250Anonlinear%2520objective%2520functionals%252C%2520can%2520be%2520constructed%2520using%250A%2524%2528f%252C%255CGamma%2529%2524-divergences%253B%2520these%2520generalize%2520and%2520interpolate%2520between%2520IPMs%2520and%250A%2524f%2524-divergences%2520%2528e.g.%252C%2520KL%2520or%2520%2524%255Calpha%2524-divergences%2529.%2520Instances%2520of%250A%2524%2528f%252C%255CGamma%2529%2524-GANs%2520have%2520been%2520shown%2520to%2520exhibit%2520improved%2520performance%2520in%2520a%2520number%250Aof%2520applications.%2520In%2520this%2520work%2520we%2520study%2520the%2520statistical%2520consistency%2520of%250A%2524%2528f%252C%255CGamma%2529%2524-GANs%2520for%2520general%2520%2524f%2524%2520and%2520%2524%255CGamma%2524.%2520Specifically%252C%2520we%2520derive%250Afinite-sample%2520concentration%2520inequalities.%2520These%2520derivations%2520require%2520novel%250Aarguments%2520due%2520to%2520nonlinearity%2520of%2520the%2520objective%2520functional.%2520We%2520demonstrate%2520that%250Aour%2520new%2520results%2520reduce%2520to%2520the%2520known%2520results%2520for%2520IPM-GANs%2520in%2520the%2520appropriate%250Alimit%2520while%2520also%2520significantly%2520extending%2520the%2520domain%2520of%2520applicability%2520of%2520this%250Atheory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concentration%20Inequalities%20for%20%24%28f%2C%CE%93%29%24-GANs&entry.906535625=Jeremiah%20Birrell&entry.1292438233=%20%20Generative%20adversarial%20networks%20%28GANs%29%20are%20unsupervised%20learning%20methods%20for%0Atraining%20a%20generator%20distribution%20to%20produce%20samples%20that%20approximate%20those%0Adrawn%20from%20a%20target%20distribution.%20Many%20such%20methods%20can%20be%20formulated%20as%0Aminimization%20of%20a%20metric%20or%20divergence.%20Recent%20works%20have%20proven%20the%0Astatistical%20consistency%20of%20GANs%20that%20are%20based%20on%20integral%20probability%20metrics%0A%28IPMs%29%2C%20e.g.%2C%20WGAN%20which%20is%20based%20on%20the%201-Wasserstein%20metric.%20IPMs%20are%20defined%0Aby%20optimizing%20a%20linear%20functional%20%28difference%20of%20expectations%29%20over%20a%20space%20of%0Adiscriminators.%20A%20much%20larger%20class%20of%20GANs%2C%20which%20allow%20for%20the%20use%20of%0Anonlinear%20objective%20functionals%2C%20can%20be%20constructed%20using%0A%24%28f%2C%5CGamma%29%24-divergences%3B%20these%20generalize%20and%20interpolate%20between%20IPMs%20and%0A%24f%24-divergences%20%28e.g.%2C%20KL%20or%20%24%5Calpha%24-divergences%29.%20Instances%20of%0A%24%28f%2C%5CGamma%29%24-GANs%20have%20been%20shown%20to%20exhibit%20improved%20performance%20in%20a%20number%0Aof%20applications.%20In%20this%20work%20we%20study%20the%20statistical%20consistency%20of%0A%24%28f%2C%5CGamma%29%24-GANs%20for%20general%20%24f%24%20and%20%24%5CGamma%24.%20Specifically%2C%20we%20derive%0Afinite-sample%20concentration%20inequalities.%20These%20derivations%20require%20novel%0Aarguments%20due%20to%20nonlinearity%20of%20the%20objective%20functional.%20We%20demonstrate%20that%0Aour%20new%20results%20reduce%20to%20the%20known%20results%20for%20IPM-GANs%20in%20the%20appropriate%0Alimit%20while%20also%20significantly%20extending%20the%20domain%20of%20applicability%20of%20this%0Atheory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16834v1&entry.124074799=Read"},
{"title": "VideoScore: Building Automatic Metrics to Simulate Fine-grained Human\n  Feedback for Video Generation", "author": "Xuan He and Dongfu Jiang and Ge Zhang and Max Ku and Achint Soni and Sherman Siu and Haonan Chen and Abhranil Chandra and Ziyan Jiang and Aaran Arulraj and Kai Wang and Quy Duc Do and Yuansheng Ni and Bohan Lyu and Yaswanth Narsupalli and Rongqi Fan and Zhiheng Lyu and Yuchen Lin and Wenhu Chen", "abstract": "  The recent years have witnessed great advances in video generation. However,\nthe development of automatic video metrics is lagging significantly behind.\nNone of the existing metric is able to provide reliable scores over generated\nvideos. The main barrier is the lack of large-scale human-annotated dataset. In\nthis paper, we release VideoFeedback, the first large-scale dataset containing\nhuman-provided multi-aspect score over 37.6K synthesized videos from 11\nexisting video generative models. We train VideoScore (initialized from Mantis)\nbased on VideoFeedback to enable automatic video quality assessment.\nExperiments show that the Spearman correlation between VideoScore and humans\ncan reach 77.1 on VideoFeedback-test, beating the prior best metrics by about\n50 points. Further result on other held-out EvalCrafter, GenAI-Bench, and\nVBench show that VideoScore has consistently much higher correlation with human\njudges than other metrics. Due to these results, we believe VideoScore can\nserve as a great proxy for human raters to (1) rate different video models to\ntrack progress (2) simulate fine-grained human feedback in Reinforcement\nLearning with Human Feedback (RLHF) to improve current video generation models.\n", "link": "http://arxiv.org/abs/2406.15252v2", "date": "2024-06-24", "relevancy": 2.2975, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5808}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5777}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.55}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoScore%3A%20Building%20Automatic%20Metrics%20to%20Simulate%20Fine-grained%20Human%0A%20%20Feedback%20for%20Video%20Generation&body=Title%3A%20VideoScore%3A%20Building%20Automatic%20Metrics%20to%20Simulate%20Fine-grained%20Human%0A%20%20Feedback%20for%20Video%20Generation%0AAuthor%3A%20Xuan%20He%20and%20Dongfu%20Jiang%20and%20Ge%20Zhang%20and%20Max%20Ku%20and%20Achint%20Soni%20and%20Sherman%20Siu%20and%20Haonan%20Chen%20and%20Abhranil%20Chandra%20and%20Ziyan%20Jiang%20and%20Aaran%20Arulraj%20and%20Kai%20Wang%20and%20Quy%20Duc%20Do%20and%20Yuansheng%20Ni%20and%20Bohan%20Lyu%20and%20Yaswanth%20Narsupalli%20and%20Rongqi%20Fan%20and%20Zhiheng%20Lyu%20and%20Yuchen%20Lin%20and%20Wenhu%20Chen%0AAbstract%3A%20%20%20The%20recent%20years%20have%20witnessed%20great%20advances%20in%20video%20generation.%20However%2C%0Athe%20development%20of%20automatic%20video%20metrics%20is%20lagging%20significantly%20behind.%0ANone%20of%20the%20existing%20metric%20is%20able%20to%20provide%20reliable%20scores%20over%20generated%0Avideos.%20The%20main%20barrier%20is%20the%20lack%20of%20large-scale%20human-annotated%20dataset.%20In%0Athis%20paper%2C%20we%20release%20VideoFeedback%2C%20the%20first%20large-scale%20dataset%20containing%0Ahuman-provided%20multi-aspect%20score%20over%2037.6K%20synthesized%20videos%20from%2011%0Aexisting%20video%20generative%20models.%20We%20train%20VideoScore%20%28initialized%20from%20Mantis%29%0Abased%20on%20VideoFeedback%20to%20enable%20automatic%20video%20quality%20assessment.%0AExperiments%20show%20that%20the%20Spearman%20correlation%20between%20VideoScore%20and%20humans%0Acan%20reach%2077.1%20on%20VideoFeedback-test%2C%20beating%20the%20prior%20best%20metrics%20by%20about%0A50%20points.%20Further%20result%20on%20other%20held-out%20EvalCrafter%2C%20GenAI-Bench%2C%20and%0AVBench%20show%20that%20VideoScore%20has%20consistently%20much%20higher%20correlation%20with%20human%0Ajudges%20than%20other%20metrics.%20Due%20to%20these%20results%2C%20we%20believe%20VideoScore%20can%0Aserve%20as%20a%20great%20proxy%20for%20human%20raters%20to%20%281%29%20rate%20different%20video%20models%20to%0Atrack%20progress%20%282%29%20simulate%20fine-grained%20human%20feedback%20in%20Reinforcement%0ALearning%20with%20Human%20Feedback%20%28RLHF%29%20to%20improve%20current%20video%20generation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15252v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoScore%253A%2520Building%2520Automatic%2520Metrics%2520to%2520Simulate%2520Fine-grained%2520Human%250A%2520%2520Feedback%2520for%2520Video%2520Generation%26entry.906535625%3DXuan%2520He%2520and%2520Dongfu%2520Jiang%2520and%2520Ge%2520Zhang%2520and%2520Max%2520Ku%2520and%2520Achint%2520Soni%2520and%2520Sherman%2520Siu%2520and%2520Haonan%2520Chen%2520and%2520Abhranil%2520Chandra%2520and%2520Ziyan%2520Jiang%2520and%2520Aaran%2520Arulraj%2520and%2520Kai%2520Wang%2520and%2520Quy%2520Duc%2520Do%2520and%2520Yuansheng%2520Ni%2520and%2520Bohan%2520Lyu%2520and%2520Yaswanth%2520Narsupalli%2520and%2520Rongqi%2520Fan%2520and%2520Zhiheng%2520Lyu%2520and%2520Yuchen%2520Lin%2520and%2520Wenhu%2520Chen%26entry.1292438233%3D%2520%2520The%2520recent%2520years%2520have%2520witnessed%2520great%2520advances%2520in%2520video%2520generation.%2520However%252C%250Athe%2520development%2520of%2520automatic%2520video%2520metrics%2520is%2520lagging%2520significantly%2520behind.%250ANone%2520of%2520the%2520existing%2520metric%2520is%2520able%2520to%2520provide%2520reliable%2520scores%2520over%2520generated%250Avideos.%2520The%2520main%2520barrier%2520is%2520the%2520lack%2520of%2520large-scale%2520human-annotated%2520dataset.%2520In%250Athis%2520paper%252C%2520we%2520release%2520VideoFeedback%252C%2520the%2520first%2520large-scale%2520dataset%2520containing%250Ahuman-provided%2520multi-aspect%2520score%2520over%252037.6K%2520synthesized%2520videos%2520from%252011%250Aexisting%2520video%2520generative%2520models.%2520We%2520train%2520VideoScore%2520%2528initialized%2520from%2520Mantis%2529%250Abased%2520on%2520VideoFeedback%2520to%2520enable%2520automatic%2520video%2520quality%2520assessment.%250AExperiments%2520show%2520that%2520the%2520Spearman%2520correlation%2520between%2520VideoScore%2520and%2520humans%250Acan%2520reach%252077.1%2520on%2520VideoFeedback-test%252C%2520beating%2520the%2520prior%2520best%2520metrics%2520by%2520about%250A50%2520points.%2520Further%2520result%2520on%2520other%2520held-out%2520EvalCrafter%252C%2520GenAI-Bench%252C%2520and%250AVBench%2520show%2520that%2520VideoScore%2520has%2520consistently%2520much%2520higher%2520correlation%2520with%2520human%250Ajudges%2520than%2520other%2520metrics.%2520Due%2520to%2520these%2520results%252C%2520we%2520believe%2520VideoScore%2520can%250Aserve%2520as%2520a%2520great%2520proxy%2520for%2520human%2520raters%2520to%2520%25281%2529%2520rate%2520different%2520video%2520models%2520to%250Atrack%2520progress%2520%25282%2529%2520simulate%2520fine-grained%2520human%2520feedback%2520in%2520Reinforcement%250ALearning%2520with%2520Human%2520Feedback%2520%2528RLHF%2529%2520to%2520improve%2520current%2520video%2520generation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15252v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoScore%3A%20Building%20Automatic%20Metrics%20to%20Simulate%20Fine-grained%20Human%0A%20%20Feedback%20for%20Video%20Generation&entry.906535625=Xuan%20He%20and%20Dongfu%20Jiang%20and%20Ge%20Zhang%20and%20Max%20Ku%20and%20Achint%20Soni%20and%20Sherman%20Siu%20and%20Haonan%20Chen%20and%20Abhranil%20Chandra%20and%20Ziyan%20Jiang%20and%20Aaran%20Arulraj%20and%20Kai%20Wang%20and%20Quy%20Duc%20Do%20and%20Yuansheng%20Ni%20and%20Bohan%20Lyu%20and%20Yaswanth%20Narsupalli%20and%20Rongqi%20Fan%20and%20Zhiheng%20Lyu%20and%20Yuchen%20Lin%20and%20Wenhu%20Chen&entry.1292438233=%20%20The%20recent%20years%20have%20witnessed%20great%20advances%20in%20video%20generation.%20However%2C%0Athe%20development%20of%20automatic%20video%20metrics%20is%20lagging%20significantly%20behind.%0ANone%20of%20the%20existing%20metric%20is%20able%20to%20provide%20reliable%20scores%20over%20generated%0Avideos.%20The%20main%20barrier%20is%20the%20lack%20of%20large-scale%20human-annotated%20dataset.%20In%0Athis%20paper%2C%20we%20release%20VideoFeedback%2C%20the%20first%20large-scale%20dataset%20containing%0Ahuman-provided%20multi-aspect%20score%20over%2037.6K%20synthesized%20videos%20from%2011%0Aexisting%20video%20generative%20models.%20We%20train%20VideoScore%20%28initialized%20from%20Mantis%29%0Abased%20on%20VideoFeedback%20to%20enable%20automatic%20video%20quality%20assessment.%0AExperiments%20show%20that%20the%20Spearman%20correlation%20between%20VideoScore%20and%20humans%0Acan%20reach%2077.1%20on%20VideoFeedback-test%2C%20beating%20the%20prior%20best%20metrics%20by%20about%0A50%20points.%20Further%20result%20on%20other%20held-out%20EvalCrafter%2C%20GenAI-Bench%2C%20and%0AVBench%20show%20that%20VideoScore%20has%20consistently%20much%20higher%20correlation%20with%20human%0Ajudges%20than%20other%20metrics.%20Due%20to%20these%20results%2C%20we%20believe%20VideoScore%20can%0Aserve%20as%20a%20great%20proxy%20for%20human%20raters%20to%20%281%29%20rate%20different%20video%20models%20to%0Atrack%20progress%20%282%29%20simulate%20fine-grained%20human%20feedback%20in%20Reinforcement%0ALearning%20with%20Human%20Feedback%20%28RLHF%29%20to%20improve%20current%20video%20generation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15252v2&entry.124074799=Read"},
{"title": "StableNormal: Reducing Diffusion Variance for Stable and Sharp Normal", "author": "Chongjie Ye and Lingteng Qiu and Xiaodong Gu and Qi Zuo and Yushuang Wu and Zilong Dong and Liefeng Bo and Yuliang Xiu and Xiaoguang Han", "abstract": "  This work addresses the challenge of high-quality surface normal estimation\nfrom monocular colored inputs (i.e., images and videos), a field which has\nrecently been revolutionized by repurposing diffusion priors. However, previous\nattempts still struggle with stochastic inference, conflicting with the\ndeterministic nature of the Image2Normal task, and costly ensembling step,\nwhich slows down the estimation process. Our method, StableNormal, mitigates\nthe stochasticity of the diffusion process by reducing inference variance, thus\nproducing \"Stable-and-Sharp\" normal estimates without any additional ensembling\nprocess. StableNormal works robustly under challenging imaging conditions, such\nas extreme lighting, blurring, and low quality. It is also robust against\ntransparent and reflective surfaces, as well as cluttered scenes with numerous\nobjects. Specifically, StableNormal employs a coarse-to-fine strategy, which\nstarts with a one-step normal estimator (YOSO) to derive an initial normal\nguess, that is relatively coarse but reliable, then followed by a\nsemantic-guided refinement process (SG-DRN) that refines the normals to recover\ngeometric details. The effectiveness of StableNormal is demonstrated through\ncompetitive performance in standard datasets such as DIODE-indoor, iBims,\nScannetV2 and NYUv2, and also in various downstream tasks, such as surface\nreconstruction and normal enhancement. These results evidence that StableNormal\nretains both the \"stability\" and \"sharpness\" for accurate normal estimation.\nStableNormal represents a baby attempt to repurpose diffusion priors for\ndeterministic estimation. To democratize this, code and models have been\npublicly available in hf.co/Stable-X\n", "link": "http://arxiv.org/abs/2406.16864v1", "date": "2024-06-24", "relevancy": 2.2879, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5883}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5728}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StableNormal%3A%20Reducing%20Diffusion%20Variance%20for%20Stable%20and%20Sharp%20Normal&body=Title%3A%20StableNormal%3A%20Reducing%20Diffusion%20Variance%20for%20Stable%20and%20Sharp%20Normal%0AAuthor%3A%20Chongjie%20Ye%20and%20Lingteng%20Qiu%20and%20Xiaodong%20Gu%20and%20Qi%20Zuo%20and%20Yushuang%20Wu%20and%20Zilong%20Dong%20and%20Liefeng%20Bo%20and%20Yuliang%20Xiu%20and%20Xiaoguang%20Han%0AAbstract%3A%20%20%20This%20work%20addresses%20the%20challenge%20of%20high-quality%20surface%20normal%20estimation%0Afrom%20monocular%20colored%20inputs%20%28i.e.%2C%20images%20and%20videos%29%2C%20a%20field%20which%20has%0Arecently%20been%20revolutionized%20by%20repurposing%20diffusion%20priors.%20However%2C%20previous%0Aattempts%20still%20struggle%20with%20stochastic%20inference%2C%20conflicting%20with%20the%0Adeterministic%20nature%20of%20the%20Image2Normal%20task%2C%20and%20costly%20ensembling%20step%2C%0Awhich%20slows%20down%20the%20estimation%20process.%20Our%20method%2C%20StableNormal%2C%20mitigates%0Athe%20stochasticity%20of%20the%20diffusion%20process%20by%20reducing%20inference%20variance%2C%20thus%0Aproducing%20%22Stable-and-Sharp%22%20normal%20estimates%20without%20any%20additional%20ensembling%0Aprocess.%20StableNormal%20works%20robustly%20under%20challenging%20imaging%20conditions%2C%20such%0Aas%20extreme%20lighting%2C%20blurring%2C%20and%20low%20quality.%20It%20is%20also%20robust%20against%0Atransparent%20and%20reflective%20surfaces%2C%20as%20well%20as%20cluttered%20scenes%20with%20numerous%0Aobjects.%20Specifically%2C%20StableNormal%20employs%20a%20coarse-to-fine%20strategy%2C%20which%0Astarts%20with%20a%20one-step%20normal%20estimator%20%28YOSO%29%20to%20derive%20an%20initial%20normal%0Aguess%2C%20that%20is%20relatively%20coarse%20but%20reliable%2C%20then%20followed%20by%20a%0Asemantic-guided%20refinement%20process%20%28SG-DRN%29%20that%20refines%20the%20normals%20to%20recover%0Ageometric%20details.%20The%20effectiveness%20of%20StableNormal%20is%20demonstrated%20through%0Acompetitive%20performance%20in%20standard%20datasets%20such%20as%20DIODE-indoor%2C%20iBims%2C%0AScannetV2%20and%20NYUv2%2C%20and%20also%20in%20various%20downstream%20tasks%2C%20such%20as%20surface%0Areconstruction%20and%20normal%20enhancement.%20These%20results%20evidence%20that%20StableNormal%0Aretains%20both%20the%20%22stability%22%20and%20%22sharpness%22%20for%20accurate%20normal%20estimation.%0AStableNormal%20represents%20a%20baby%20attempt%20to%20repurpose%20diffusion%20priors%20for%0Adeterministic%20estimation.%20To%20democratize%20this%2C%20code%20and%20models%20have%20been%0Apublicly%20available%20in%20hf.co/Stable-X%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStableNormal%253A%2520Reducing%2520Diffusion%2520Variance%2520for%2520Stable%2520and%2520Sharp%2520Normal%26entry.906535625%3DChongjie%2520Ye%2520and%2520Lingteng%2520Qiu%2520and%2520Xiaodong%2520Gu%2520and%2520Qi%2520Zuo%2520and%2520Yushuang%2520Wu%2520and%2520Zilong%2520Dong%2520and%2520Liefeng%2520Bo%2520and%2520Yuliang%2520Xiu%2520and%2520Xiaoguang%2520Han%26entry.1292438233%3D%2520%2520This%2520work%2520addresses%2520the%2520challenge%2520of%2520high-quality%2520surface%2520normal%2520estimation%250Afrom%2520monocular%2520colored%2520inputs%2520%2528i.e.%252C%2520images%2520and%2520videos%2529%252C%2520a%2520field%2520which%2520has%250Arecently%2520been%2520revolutionized%2520by%2520repurposing%2520diffusion%2520priors.%2520However%252C%2520previous%250Aattempts%2520still%2520struggle%2520with%2520stochastic%2520inference%252C%2520conflicting%2520with%2520the%250Adeterministic%2520nature%2520of%2520the%2520Image2Normal%2520task%252C%2520and%2520costly%2520ensembling%2520step%252C%250Awhich%2520slows%2520down%2520the%2520estimation%2520process.%2520Our%2520method%252C%2520StableNormal%252C%2520mitigates%250Athe%2520stochasticity%2520of%2520the%2520diffusion%2520process%2520by%2520reducing%2520inference%2520variance%252C%2520thus%250Aproducing%2520%2522Stable-and-Sharp%2522%2520normal%2520estimates%2520without%2520any%2520additional%2520ensembling%250Aprocess.%2520StableNormal%2520works%2520robustly%2520under%2520challenging%2520imaging%2520conditions%252C%2520such%250Aas%2520extreme%2520lighting%252C%2520blurring%252C%2520and%2520low%2520quality.%2520It%2520is%2520also%2520robust%2520against%250Atransparent%2520and%2520reflective%2520surfaces%252C%2520as%2520well%2520as%2520cluttered%2520scenes%2520with%2520numerous%250Aobjects.%2520Specifically%252C%2520StableNormal%2520employs%2520a%2520coarse-to-fine%2520strategy%252C%2520which%250Astarts%2520with%2520a%2520one-step%2520normal%2520estimator%2520%2528YOSO%2529%2520to%2520derive%2520an%2520initial%2520normal%250Aguess%252C%2520that%2520is%2520relatively%2520coarse%2520but%2520reliable%252C%2520then%2520followed%2520by%2520a%250Asemantic-guided%2520refinement%2520process%2520%2528SG-DRN%2529%2520that%2520refines%2520the%2520normals%2520to%2520recover%250Ageometric%2520details.%2520The%2520effectiveness%2520of%2520StableNormal%2520is%2520demonstrated%2520through%250Acompetitive%2520performance%2520in%2520standard%2520datasets%2520such%2520as%2520DIODE-indoor%252C%2520iBims%252C%250AScannetV2%2520and%2520NYUv2%252C%2520and%2520also%2520in%2520various%2520downstream%2520tasks%252C%2520such%2520as%2520surface%250Areconstruction%2520and%2520normal%2520enhancement.%2520These%2520results%2520evidence%2520that%2520StableNormal%250Aretains%2520both%2520the%2520%2522stability%2522%2520and%2520%2522sharpness%2522%2520for%2520accurate%2520normal%2520estimation.%250AStableNormal%2520represents%2520a%2520baby%2520attempt%2520to%2520repurpose%2520diffusion%2520priors%2520for%250Adeterministic%2520estimation.%2520To%2520democratize%2520this%252C%2520code%2520and%2520models%2520have%2520been%250Apublicly%2520available%2520in%2520hf.co/Stable-X%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StableNormal%3A%20Reducing%20Diffusion%20Variance%20for%20Stable%20and%20Sharp%20Normal&entry.906535625=Chongjie%20Ye%20and%20Lingteng%20Qiu%20and%20Xiaodong%20Gu%20and%20Qi%20Zuo%20and%20Yushuang%20Wu%20and%20Zilong%20Dong%20and%20Liefeng%20Bo%20and%20Yuliang%20Xiu%20and%20Xiaoguang%20Han&entry.1292438233=%20%20This%20work%20addresses%20the%20challenge%20of%20high-quality%20surface%20normal%20estimation%0Afrom%20monocular%20colored%20inputs%20%28i.e.%2C%20images%20and%20videos%29%2C%20a%20field%20which%20has%0Arecently%20been%20revolutionized%20by%20repurposing%20diffusion%20priors.%20However%2C%20previous%0Aattempts%20still%20struggle%20with%20stochastic%20inference%2C%20conflicting%20with%20the%0Adeterministic%20nature%20of%20the%20Image2Normal%20task%2C%20and%20costly%20ensembling%20step%2C%0Awhich%20slows%20down%20the%20estimation%20process.%20Our%20method%2C%20StableNormal%2C%20mitigates%0Athe%20stochasticity%20of%20the%20diffusion%20process%20by%20reducing%20inference%20variance%2C%20thus%0Aproducing%20%22Stable-and-Sharp%22%20normal%20estimates%20without%20any%20additional%20ensembling%0Aprocess.%20StableNormal%20works%20robustly%20under%20challenging%20imaging%20conditions%2C%20such%0Aas%20extreme%20lighting%2C%20blurring%2C%20and%20low%20quality.%20It%20is%20also%20robust%20against%0Atransparent%20and%20reflective%20surfaces%2C%20as%20well%20as%20cluttered%20scenes%20with%20numerous%0Aobjects.%20Specifically%2C%20StableNormal%20employs%20a%20coarse-to-fine%20strategy%2C%20which%0Astarts%20with%20a%20one-step%20normal%20estimator%20%28YOSO%29%20to%20derive%20an%20initial%20normal%0Aguess%2C%20that%20is%20relatively%20coarse%20but%20reliable%2C%20then%20followed%20by%20a%0Asemantic-guided%20refinement%20process%20%28SG-DRN%29%20that%20refines%20the%20normals%20to%20recover%0Ageometric%20details.%20The%20effectiveness%20of%20StableNormal%20is%20demonstrated%20through%0Acompetitive%20performance%20in%20standard%20datasets%20such%20as%20DIODE-indoor%2C%20iBims%2C%0AScannetV2%20and%20NYUv2%2C%20and%20also%20in%20various%20downstream%20tasks%2C%20such%20as%20surface%0Areconstruction%20and%20normal%20enhancement.%20These%20results%20evidence%20that%20StableNormal%0Aretains%20both%20the%20%22stability%22%20and%20%22sharpness%22%20for%20accurate%20normal%20estimation.%0AStableNormal%20represents%20a%20baby%20attempt%20to%20repurpose%20diffusion%20priors%20for%0Adeterministic%20estimation.%20To%20democratize%20this%2C%20code%20and%20models%20have%20been%0Apublicly%20available%20in%20hf.co/Stable-X%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16864v1&entry.124074799=Read"},
{"title": "Fault Detection for agents on power grid topology optimization: A\n  Comprehensive analysis", "author": "Malte Lehna and Mohamed Hassouna and Dmitry Degtyar and Sven Tomforde and Christoph Scholz", "abstract": "  The topology optimization of transmission networks using Deep Reinforcement\nLearning (DRL) has increasingly come into focus. Various researchers have\nproposed different DRL agents, which are often benchmarked on the Grid2Op\nenvironment from the Learning to Run a Power Network (L2RPN) challenges. The\nenvironments have many advantages with their realistic chronics and underlying\npower flow backends. However, the interpretation of agent survival or failure\nis not always clear, as there are a variety of potential causes. In this work,\nwe focus on the failures of the power grid to identify patterns and detect them\na priori. We collect the failed chronics of three different agents on the WCCI\n2022 L2RPN environment, totaling about 40k data points. By clustering, we are\nable to detect five distinct clusters, identifying different failure types.\nFurther, we propose a multi-class prediction approach to detect failures\nbeforehand and evaluate five different models. Here, the Light\nGradient-Boosting Machine (LightGBM) shows the best performance, with an\naccuracy of 86%. It also correctly identifies in 91% of the time failure and\nsurvival observations. Finally, we provide a detailed feature importance\nanalysis that identifies critical features and regions in the grid.\n", "link": "http://arxiv.org/abs/2406.16426v1", "date": "2024-06-24", "relevancy": 2.2847, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4835}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4458}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fault%20Detection%20for%20agents%20on%20power%20grid%20topology%20optimization%3A%20A%0A%20%20Comprehensive%20analysis&body=Title%3A%20Fault%20Detection%20for%20agents%20on%20power%20grid%20topology%20optimization%3A%20A%0A%20%20Comprehensive%20analysis%0AAuthor%3A%20Malte%20Lehna%20and%20Mohamed%20Hassouna%20and%20Dmitry%20Degtyar%20and%20Sven%20Tomforde%20and%20Christoph%20Scholz%0AAbstract%3A%20%20%20The%20topology%20optimization%20of%20transmission%20networks%20using%20Deep%20Reinforcement%0ALearning%20%28DRL%29%20has%20increasingly%20come%20into%20focus.%20Various%20researchers%20have%0Aproposed%20different%20DRL%20agents%2C%20which%20are%20often%20benchmarked%20on%20the%20Grid2Op%0Aenvironment%20from%20the%20Learning%20to%20Run%20a%20Power%20Network%20%28L2RPN%29%20challenges.%20The%0Aenvironments%20have%20many%20advantages%20with%20their%20realistic%20chronics%20and%20underlying%0Apower%20flow%20backends.%20However%2C%20the%20interpretation%20of%20agent%20survival%20or%20failure%0Ais%20not%20always%20clear%2C%20as%20there%20are%20a%20variety%20of%20potential%20causes.%20In%20this%20work%2C%0Awe%20focus%20on%20the%20failures%20of%20the%20power%20grid%20to%20identify%20patterns%20and%20detect%20them%0Aa%20priori.%20We%20collect%20the%20failed%20chronics%20of%20three%20different%20agents%20on%20the%20WCCI%0A2022%20L2RPN%20environment%2C%20totaling%20about%2040k%20data%20points.%20By%20clustering%2C%20we%20are%0Aable%20to%20detect%20five%20distinct%20clusters%2C%20identifying%20different%20failure%20types.%0AFurther%2C%20we%20propose%20a%20multi-class%20prediction%20approach%20to%20detect%20failures%0Abeforehand%20and%20evaluate%20five%20different%20models.%20Here%2C%20the%20Light%0AGradient-Boosting%20Machine%20%28LightGBM%29%20shows%20the%20best%20performance%2C%20with%20an%0Aaccuracy%20of%2086%25.%20It%20also%20correctly%20identifies%20in%2091%25%20of%20the%20time%20failure%20and%0Asurvival%20observations.%20Finally%2C%20we%20provide%20a%20detailed%20feature%20importance%0Aanalysis%20that%20identifies%20critical%20features%20and%20regions%20in%20the%20grid.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFault%2520Detection%2520for%2520agents%2520on%2520power%2520grid%2520topology%2520optimization%253A%2520A%250A%2520%2520Comprehensive%2520analysis%26entry.906535625%3DMalte%2520Lehna%2520and%2520Mohamed%2520Hassouna%2520and%2520Dmitry%2520Degtyar%2520and%2520Sven%2520Tomforde%2520and%2520Christoph%2520Scholz%26entry.1292438233%3D%2520%2520The%2520topology%2520optimization%2520of%2520transmission%2520networks%2520using%2520Deep%2520Reinforcement%250ALearning%2520%2528DRL%2529%2520has%2520increasingly%2520come%2520into%2520focus.%2520Various%2520researchers%2520have%250Aproposed%2520different%2520DRL%2520agents%252C%2520which%2520are%2520often%2520benchmarked%2520on%2520the%2520Grid2Op%250Aenvironment%2520from%2520the%2520Learning%2520to%2520Run%2520a%2520Power%2520Network%2520%2528L2RPN%2529%2520challenges.%2520The%250Aenvironments%2520have%2520many%2520advantages%2520with%2520their%2520realistic%2520chronics%2520and%2520underlying%250Apower%2520flow%2520backends.%2520However%252C%2520the%2520interpretation%2520of%2520agent%2520survival%2520or%2520failure%250Ais%2520not%2520always%2520clear%252C%2520as%2520there%2520are%2520a%2520variety%2520of%2520potential%2520causes.%2520In%2520this%2520work%252C%250Awe%2520focus%2520on%2520the%2520failures%2520of%2520the%2520power%2520grid%2520to%2520identify%2520patterns%2520and%2520detect%2520them%250Aa%2520priori.%2520We%2520collect%2520the%2520failed%2520chronics%2520of%2520three%2520different%2520agents%2520on%2520the%2520WCCI%250A2022%2520L2RPN%2520environment%252C%2520totaling%2520about%252040k%2520data%2520points.%2520By%2520clustering%252C%2520we%2520are%250Aable%2520to%2520detect%2520five%2520distinct%2520clusters%252C%2520identifying%2520different%2520failure%2520types.%250AFurther%252C%2520we%2520propose%2520a%2520multi-class%2520prediction%2520approach%2520to%2520detect%2520failures%250Abeforehand%2520and%2520evaluate%2520five%2520different%2520models.%2520Here%252C%2520the%2520Light%250AGradient-Boosting%2520Machine%2520%2528LightGBM%2529%2520shows%2520the%2520best%2520performance%252C%2520with%2520an%250Aaccuracy%2520of%252086%2525.%2520It%2520also%2520correctly%2520identifies%2520in%252091%2525%2520of%2520the%2520time%2520failure%2520and%250Asurvival%2520observations.%2520Finally%252C%2520we%2520provide%2520a%2520detailed%2520feature%2520importance%250Aanalysis%2520that%2520identifies%2520critical%2520features%2520and%2520regions%2520in%2520the%2520grid.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fault%20Detection%20for%20agents%20on%20power%20grid%20topology%20optimization%3A%20A%0A%20%20Comprehensive%20analysis&entry.906535625=Malte%20Lehna%20and%20Mohamed%20Hassouna%20and%20Dmitry%20Degtyar%20and%20Sven%20Tomforde%20and%20Christoph%20Scholz&entry.1292438233=%20%20The%20topology%20optimization%20of%20transmission%20networks%20using%20Deep%20Reinforcement%0ALearning%20%28DRL%29%20has%20increasingly%20come%20into%20focus.%20Various%20researchers%20have%0Aproposed%20different%20DRL%20agents%2C%20which%20are%20often%20benchmarked%20on%20the%20Grid2Op%0Aenvironment%20from%20the%20Learning%20to%20Run%20a%20Power%20Network%20%28L2RPN%29%20challenges.%20The%0Aenvironments%20have%20many%20advantages%20with%20their%20realistic%20chronics%20and%20underlying%0Apower%20flow%20backends.%20However%2C%20the%20interpretation%20of%20agent%20survival%20or%20failure%0Ais%20not%20always%20clear%2C%20as%20there%20are%20a%20variety%20of%20potential%20causes.%20In%20this%20work%2C%0Awe%20focus%20on%20the%20failures%20of%20the%20power%20grid%20to%20identify%20patterns%20and%20detect%20them%0Aa%20priori.%20We%20collect%20the%20failed%20chronics%20of%20three%20different%20agents%20on%20the%20WCCI%0A2022%20L2RPN%20environment%2C%20totaling%20about%2040k%20data%20points.%20By%20clustering%2C%20we%20are%0Aable%20to%20detect%20five%20distinct%20clusters%2C%20identifying%20different%20failure%20types.%0AFurther%2C%20we%20propose%20a%20multi-class%20prediction%20approach%20to%20detect%20failures%0Abeforehand%20and%20evaluate%20five%20different%20models.%20Here%2C%20the%20Light%0AGradient-Boosting%20Machine%20%28LightGBM%29%20shows%20the%20best%20performance%2C%20with%20an%0Aaccuracy%20of%2086%25.%20It%20also%20correctly%20identifies%20in%2091%25%20of%20the%20time%20failure%20and%0Asurvival%20observations.%20Finally%2C%20we%20provide%20a%20detailed%20feature%20importance%0Aanalysis%20that%20identifies%20critical%20features%20and%20regions%20in%20the%20grid.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16426v1&entry.124074799=Read"},
{"title": "Geometry-Aware Score Distillation via 3D Consistent Noising and Gradient\n  Consistency Modeling", "author": "Min-Seop Kwak and Donghoon Ahn and Ines Hyeonsu Kim and Jin-wha Kim and Seungryong Kim", "abstract": "  Score distillation sampling (SDS), the methodology in which the score from\npretrained 2D diffusion models is distilled into 3D representation, has\nrecently brought significant advancements in text-to-3D generation task.\nHowever, this approach is still confronted with critical geometric\ninconsistency problems such as the Janus problem. Starting from a hypothesis\nthat such inconsistency problems may be induced by multiview inconsistencies\nbetween 2D scores predicted from various viewpoints, we introduce GSD, a simple\nand general plug-and-play framework for incorporating 3D consistency and\ntherefore geometry awareness into the SDS process. Our methodology is composed\nof three components: 3D consistent noising, designed to produce 3D consistent\nnoise maps that perfectly follow the standard Gaussian distribution,\ngeometry-based gradient warping for identifying correspondences between\npredicted gradients of different viewpoints, and novel gradient consistency\nloss to optimize the scene geometry toward producing more consistent gradients.\nWe demonstrate that our method significantly improves performance, successfully\naddressing the geometric inconsistency problems in text-to-3D generation task\nwith minimal computation cost and being compatible with existing score\ndistillation-based models. Our project page is available at\nhttps://ku-cvlab.github.io/GSD/.\n", "link": "http://arxiv.org/abs/2406.16695v1", "date": "2024-06-24", "relevancy": 2.2765, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5927}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5595}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry-Aware%20Score%20Distillation%20via%203D%20Consistent%20Noising%20and%20Gradient%0A%20%20Consistency%20Modeling&body=Title%3A%20Geometry-Aware%20Score%20Distillation%20via%203D%20Consistent%20Noising%20and%20Gradient%0A%20%20Consistency%20Modeling%0AAuthor%3A%20Min-Seop%20Kwak%20and%20Donghoon%20Ahn%20and%20Ines%20Hyeonsu%20Kim%20and%20Jin-wha%20Kim%20and%20Seungryong%20Kim%0AAbstract%3A%20%20%20Score%20distillation%20sampling%20%28SDS%29%2C%20the%20methodology%20in%20which%20the%20score%20from%0Apretrained%202D%20diffusion%20models%20is%20distilled%20into%203D%20representation%2C%20has%0Arecently%20brought%20significant%20advancements%20in%20text-to-3D%20generation%20task.%0AHowever%2C%20this%20approach%20is%20still%20confronted%20with%20critical%20geometric%0Ainconsistency%20problems%20such%20as%20the%20Janus%20problem.%20Starting%20from%20a%20hypothesis%0Athat%20such%20inconsistency%20problems%20may%20be%20induced%20by%20multiview%20inconsistencies%0Abetween%202D%20scores%20predicted%20from%20various%20viewpoints%2C%20we%20introduce%20GSD%2C%20a%20simple%0Aand%20general%20plug-and-play%20framework%20for%20incorporating%203D%20consistency%20and%0Atherefore%20geometry%20awareness%20into%20the%20SDS%20process.%20Our%20methodology%20is%20composed%0Aof%20three%20components%3A%203D%20consistent%20noising%2C%20designed%20to%20produce%203D%20consistent%0Anoise%20maps%20that%20perfectly%20follow%20the%20standard%20Gaussian%20distribution%2C%0Ageometry-based%20gradient%20warping%20for%20identifying%20correspondences%20between%0Apredicted%20gradients%20of%20different%20viewpoints%2C%20and%20novel%20gradient%20consistency%0Aloss%20to%20optimize%20the%20scene%20geometry%20toward%20producing%20more%20consistent%20gradients.%0AWe%20demonstrate%20that%20our%20method%20significantly%20improves%20performance%2C%20successfully%0Aaddressing%20the%20geometric%20inconsistency%20problems%20in%20text-to-3D%20generation%20task%0Awith%20minimal%20computation%20cost%20and%20being%20compatible%20with%20existing%20score%0Adistillation-based%20models.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//ku-cvlab.github.io/GSD/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry-Aware%2520Score%2520Distillation%2520via%25203D%2520Consistent%2520Noising%2520and%2520Gradient%250A%2520%2520Consistency%2520Modeling%26entry.906535625%3DMin-Seop%2520Kwak%2520and%2520Donghoon%2520Ahn%2520and%2520Ines%2520Hyeonsu%2520Kim%2520and%2520Jin-wha%2520Kim%2520and%2520Seungryong%2520Kim%26entry.1292438233%3D%2520%2520Score%2520distillation%2520sampling%2520%2528SDS%2529%252C%2520the%2520methodology%2520in%2520which%2520the%2520score%2520from%250Apretrained%25202D%2520diffusion%2520models%2520is%2520distilled%2520into%25203D%2520representation%252C%2520has%250Arecently%2520brought%2520significant%2520advancements%2520in%2520text-to-3D%2520generation%2520task.%250AHowever%252C%2520this%2520approach%2520is%2520still%2520confronted%2520with%2520critical%2520geometric%250Ainconsistency%2520problems%2520such%2520as%2520the%2520Janus%2520problem.%2520Starting%2520from%2520a%2520hypothesis%250Athat%2520such%2520inconsistency%2520problems%2520may%2520be%2520induced%2520by%2520multiview%2520inconsistencies%250Abetween%25202D%2520scores%2520predicted%2520from%2520various%2520viewpoints%252C%2520we%2520introduce%2520GSD%252C%2520a%2520simple%250Aand%2520general%2520plug-and-play%2520framework%2520for%2520incorporating%25203D%2520consistency%2520and%250Atherefore%2520geometry%2520awareness%2520into%2520the%2520SDS%2520process.%2520Our%2520methodology%2520is%2520composed%250Aof%2520three%2520components%253A%25203D%2520consistent%2520noising%252C%2520designed%2520to%2520produce%25203D%2520consistent%250Anoise%2520maps%2520that%2520perfectly%2520follow%2520the%2520standard%2520Gaussian%2520distribution%252C%250Ageometry-based%2520gradient%2520warping%2520for%2520identifying%2520correspondences%2520between%250Apredicted%2520gradients%2520of%2520different%2520viewpoints%252C%2520and%2520novel%2520gradient%2520consistency%250Aloss%2520to%2520optimize%2520the%2520scene%2520geometry%2520toward%2520producing%2520more%2520consistent%2520gradients.%250AWe%2520demonstrate%2520that%2520our%2520method%2520significantly%2520improves%2520performance%252C%2520successfully%250Aaddressing%2520the%2520geometric%2520inconsistency%2520problems%2520in%2520text-to-3D%2520generation%2520task%250Awith%2520minimal%2520computation%2520cost%2520and%2520being%2520compatible%2520with%2520existing%2520score%250Adistillation-based%2520models.%2520Our%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//ku-cvlab.github.io/GSD/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry-Aware%20Score%20Distillation%20via%203D%20Consistent%20Noising%20and%20Gradient%0A%20%20Consistency%20Modeling&entry.906535625=Min-Seop%20Kwak%20and%20Donghoon%20Ahn%20and%20Ines%20Hyeonsu%20Kim%20and%20Jin-wha%20Kim%20and%20Seungryong%20Kim&entry.1292438233=%20%20Score%20distillation%20sampling%20%28SDS%29%2C%20the%20methodology%20in%20which%20the%20score%20from%0Apretrained%202D%20diffusion%20models%20is%20distilled%20into%203D%20representation%2C%20has%0Arecently%20brought%20significant%20advancements%20in%20text-to-3D%20generation%20task.%0AHowever%2C%20this%20approach%20is%20still%20confronted%20with%20critical%20geometric%0Ainconsistency%20problems%20such%20as%20the%20Janus%20problem.%20Starting%20from%20a%20hypothesis%0Athat%20such%20inconsistency%20problems%20may%20be%20induced%20by%20multiview%20inconsistencies%0Abetween%202D%20scores%20predicted%20from%20various%20viewpoints%2C%20we%20introduce%20GSD%2C%20a%20simple%0Aand%20general%20plug-and-play%20framework%20for%20incorporating%203D%20consistency%20and%0Atherefore%20geometry%20awareness%20into%20the%20SDS%20process.%20Our%20methodology%20is%20composed%0Aof%20three%20components%3A%203D%20consistent%20noising%2C%20designed%20to%20produce%203D%20consistent%0Anoise%20maps%20that%20perfectly%20follow%20the%20standard%20Gaussian%20distribution%2C%0Ageometry-based%20gradient%20warping%20for%20identifying%20correspondences%20between%0Apredicted%20gradients%20of%20different%20viewpoints%2C%20and%20novel%20gradient%20consistency%0Aloss%20to%20optimize%20the%20scene%20geometry%20toward%20producing%20more%20consistent%20gradients.%0AWe%20demonstrate%20that%20our%20method%20significantly%20improves%20performance%2C%20successfully%0Aaddressing%20the%20geometric%20inconsistency%20problems%20in%20text-to-3D%20generation%20task%0Awith%20minimal%20computation%20cost%20and%20being%20compatible%20with%20existing%20score%0Adistillation-based%20models.%20Our%20project%20page%20is%20available%20at%0Ahttps%3A//ku-cvlab.github.io/GSD/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16695v1&entry.124074799=Read"},
{"title": "Exploring Test-Time Adaptation for Object Detection in Continually\n  Changing Environments", "author": "Shilei Cao and Yan Liu and Juepeng Zheng and Weijia Li and Runmin Dong and Haohuan Fu", "abstract": "  For real-world applications, neural network models are commonly deployed in\ndynamic environments, where the distribution of the target domain undergoes\ntemporal changes. Continual Test-Time Adaptation (CTTA) has recently emerged as\na promising technique to gradually adapt a source-trained model to test data\ndrawn from a continually changing target domain. Despite recent advancements in\naddressing CTTA, two critical issues remain: 1) The use of a fixed threshold\nfor pseudo-labeling in existing methodologies leads to the generation of\nlow-quality pseudo-labels, as model confidence varies across categories and\ndomains; 2) While current solutions utilize stochastic parameter restoration to\nmitigate catastrophic forgetting, their capacity to preserve critical\ninformation is undermined by its intrinsic randomness. To tackle these\nchallenges, we present CTAOD, aiming to enhance the performance of detection\nmodels in CTTA scenarios. Inspired by prior CTTA works for effective\nadaptation, CTAOD is founded on the mean-teacher framework, characterized by\nthree core components. Firstly, the object-level contrastive learning module\ntailored for object detection extracts object-level features using the\nteacher's region of interest features and optimizes them through contrastive\nlearning. Secondly, the dynamic threshold strategy updates the\ncategory-specific threshold based on predicted confidence scores to improve the\nquality of pseudo-labels. Lastly, we design a data-driven stochastic\nrestoration mechanism to selectively reset inactive parameters using the\ngradients as weights for a random mask matrix, thereby ensuring the retention\nof essential knowledge. We demonstrate the effectiveness of our approach on\nfour CTTA tasks for object detection, where CTAOD outperforms existing methods,\nespecially achieving a 3.0 mAP improvement on the Cityscapes-to-Cityscapes-C\nCTTA task.\n", "link": "http://arxiv.org/abs/2406.16439v1", "date": "2024-06-24", "relevancy": 2.262, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5872}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5509}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Test-Time%20Adaptation%20for%20Object%20Detection%20in%20Continually%0A%20%20Changing%20Environments&body=Title%3A%20Exploring%20Test-Time%20Adaptation%20for%20Object%20Detection%20in%20Continually%0A%20%20Changing%20Environments%0AAuthor%3A%20Shilei%20Cao%20and%20Yan%20Liu%20and%20Juepeng%20Zheng%20and%20Weijia%20Li%20and%20Runmin%20Dong%20and%20Haohuan%20Fu%0AAbstract%3A%20%20%20For%20real-world%20applications%2C%20neural%20network%20models%20are%20commonly%20deployed%20in%0Adynamic%20environments%2C%20where%20the%20distribution%20of%20the%20target%20domain%20undergoes%0Atemporal%20changes.%20Continual%20Test-Time%20Adaptation%20%28CTTA%29%20has%20recently%20emerged%20as%0Aa%20promising%20technique%20to%20gradually%20adapt%20a%20source-trained%20model%20to%20test%20data%0Adrawn%20from%20a%20continually%20changing%20target%20domain.%20Despite%20recent%20advancements%20in%0Aaddressing%20CTTA%2C%20two%20critical%20issues%20remain%3A%201%29%20The%20use%20of%20a%20fixed%20threshold%0Afor%20pseudo-labeling%20in%20existing%20methodologies%20leads%20to%20the%20generation%20of%0Alow-quality%20pseudo-labels%2C%20as%20model%20confidence%20varies%20across%20categories%20and%0Adomains%3B%202%29%20While%20current%20solutions%20utilize%20stochastic%20parameter%20restoration%20to%0Amitigate%20catastrophic%20forgetting%2C%20their%20capacity%20to%20preserve%20critical%0Ainformation%20is%20undermined%20by%20its%20intrinsic%20randomness.%20To%20tackle%20these%0Achallenges%2C%20we%20present%20CTAOD%2C%20aiming%20to%20enhance%20the%20performance%20of%20detection%0Amodels%20in%20CTTA%20scenarios.%20Inspired%20by%20prior%20CTTA%20works%20for%20effective%0Aadaptation%2C%20CTAOD%20is%20founded%20on%20the%20mean-teacher%20framework%2C%20characterized%20by%0Athree%20core%20components.%20Firstly%2C%20the%20object-level%20contrastive%20learning%20module%0Atailored%20for%20object%20detection%20extracts%20object-level%20features%20using%20the%0Ateacher%27s%20region%20of%20interest%20features%20and%20optimizes%20them%20through%20contrastive%0Alearning.%20Secondly%2C%20the%20dynamic%20threshold%20strategy%20updates%20the%0Acategory-specific%20threshold%20based%20on%20predicted%20confidence%20scores%20to%20improve%20the%0Aquality%20of%20pseudo-labels.%20Lastly%2C%20we%20design%20a%20data-driven%20stochastic%0Arestoration%20mechanism%20to%20selectively%20reset%20inactive%20parameters%20using%20the%0Agradients%20as%20weights%20for%20a%20random%20mask%20matrix%2C%20thereby%20ensuring%20the%20retention%0Aof%20essential%20knowledge.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20on%0Afour%20CTTA%20tasks%20for%20object%20detection%2C%20where%20CTAOD%20outperforms%20existing%20methods%2C%0Aespecially%20achieving%20a%203.0%20mAP%20improvement%20on%20the%20Cityscapes-to-Cityscapes-C%0ACTTA%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Test-Time%2520Adaptation%2520for%2520Object%2520Detection%2520in%2520Continually%250A%2520%2520Changing%2520Environments%26entry.906535625%3DShilei%2520Cao%2520and%2520Yan%2520Liu%2520and%2520Juepeng%2520Zheng%2520and%2520Weijia%2520Li%2520and%2520Runmin%2520Dong%2520and%2520Haohuan%2520Fu%26entry.1292438233%3D%2520%2520For%2520real-world%2520applications%252C%2520neural%2520network%2520models%2520are%2520commonly%2520deployed%2520in%250Adynamic%2520environments%252C%2520where%2520the%2520distribution%2520of%2520the%2520target%2520domain%2520undergoes%250Atemporal%2520changes.%2520Continual%2520Test-Time%2520Adaptation%2520%2528CTTA%2529%2520has%2520recently%2520emerged%2520as%250Aa%2520promising%2520technique%2520to%2520gradually%2520adapt%2520a%2520source-trained%2520model%2520to%2520test%2520data%250Adrawn%2520from%2520a%2520continually%2520changing%2520target%2520domain.%2520Despite%2520recent%2520advancements%2520in%250Aaddressing%2520CTTA%252C%2520two%2520critical%2520issues%2520remain%253A%25201%2529%2520The%2520use%2520of%2520a%2520fixed%2520threshold%250Afor%2520pseudo-labeling%2520in%2520existing%2520methodologies%2520leads%2520to%2520the%2520generation%2520of%250Alow-quality%2520pseudo-labels%252C%2520as%2520model%2520confidence%2520varies%2520across%2520categories%2520and%250Adomains%253B%25202%2529%2520While%2520current%2520solutions%2520utilize%2520stochastic%2520parameter%2520restoration%2520to%250Amitigate%2520catastrophic%2520forgetting%252C%2520their%2520capacity%2520to%2520preserve%2520critical%250Ainformation%2520is%2520undermined%2520by%2520its%2520intrinsic%2520randomness.%2520To%2520tackle%2520these%250Achallenges%252C%2520we%2520present%2520CTAOD%252C%2520aiming%2520to%2520enhance%2520the%2520performance%2520of%2520detection%250Amodels%2520in%2520CTTA%2520scenarios.%2520Inspired%2520by%2520prior%2520CTTA%2520works%2520for%2520effective%250Aadaptation%252C%2520CTAOD%2520is%2520founded%2520on%2520the%2520mean-teacher%2520framework%252C%2520characterized%2520by%250Athree%2520core%2520components.%2520Firstly%252C%2520the%2520object-level%2520contrastive%2520learning%2520module%250Atailored%2520for%2520object%2520detection%2520extracts%2520object-level%2520features%2520using%2520the%250Ateacher%2527s%2520region%2520of%2520interest%2520features%2520and%2520optimizes%2520them%2520through%2520contrastive%250Alearning.%2520Secondly%252C%2520the%2520dynamic%2520threshold%2520strategy%2520updates%2520the%250Acategory-specific%2520threshold%2520based%2520on%2520predicted%2520confidence%2520scores%2520to%2520improve%2520the%250Aquality%2520of%2520pseudo-labels.%2520Lastly%252C%2520we%2520design%2520a%2520data-driven%2520stochastic%250Arestoration%2520mechanism%2520to%2520selectively%2520reset%2520inactive%2520parameters%2520using%2520the%250Agradients%2520as%2520weights%2520for%2520a%2520random%2520mask%2520matrix%252C%2520thereby%2520ensuring%2520the%2520retention%250Aof%2520essential%2520knowledge.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520on%250Afour%2520CTTA%2520tasks%2520for%2520object%2520detection%252C%2520where%2520CTAOD%2520outperforms%2520existing%2520methods%252C%250Aespecially%2520achieving%2520a%25203.0%2520mAP%2520improvement%2520on%2520the%2520Cityscapes-to-Cityscapes-C%250ACTTA%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Test-Time%20Adaptation%20for%20Object%20Detection%20in%20Continually%0A%20%20Changing%20Environments&entry.906535625=Shilei%20Cao%20and%20Yan%20Liu%20and%20Juepeng%20Zheng%20and%20Weijia%20Li%20and%20Runmin%20Dong%20and%20Haohuan%20Fu&entry.1292438233=%20%20For%20real-world%20applications%2C%20neural%20network%20models%20are%20commonly%20deployed%20in%0Adynamic%20environments%2C%20where%20the%20distribution%20of%20the%20target%20domain%20undergoes%0Atemporal%20changes.%20Continual%20Test-Time%20Adaptation%20%28CTTA%29%20has%20recently%20emerged%20as%0Aa%20promising%20technique%20to%20gradually%20adapt%20a%20source-trained%20model%20to%20test%20data%0Adrawn%20from%20a%20continually%20changing%20target%20domain.%20Despite%20recent%20advancements%20in%0Aaddressing%20CTTA%2C%20two%20critical%20issues%20remain%3A%201%29%20The%20use%20of%20a%20fixed%20threshold%0Afor%20pseudo-labeling%20in%20existing%20methodologies%20leads%20to%20the%20generation%20of%0Alow-quality%20pseudo-labels%2C%20as%20model%20confidence%20varies%20across%20categories%20and%0Adomains%3B%202%29%20While%20current%20solutions%20utilize%20stochastic%20parameter%20restoration%20to%0Amitigate%20catastrophic%20forgetting%2C%20their%20capacity%20to%20preserve%20critical%0Ainformation%20is%20undermined%20by%20its%20intrinsic%20randomness.%20To%20tackle%20these%0Achallenges%2C%20we%20present%20CTAOD%2C%20aiming%20to%20enhance%20the%20performance%20of%20detection%0Amodels%20in%20CTTA%20scenarios.%20Inspired%20by%20prior%20CTTA%20works%20for%20effective%0Aadaptation%2C%20CTAOD%20is%20founded%20on%20the%20mean-teacher%20framework%2C%20characterized%20by%0Athree%20core%20components.%20Firstly%2C%20the%20object-level%20contrastive%20learning%20module%0Atailored%20for%20object%20detection%20extracts%20object-level%20features%20using%20the%0Ateacher%27s%20region%20of%20interest%20features%20and%20optimizes%20them%20through%20contrastive%0Alearning.%20Secondly%2C%20the%20dynamic%20threshold%20strategy%20updates%20the%0Acategory-specific%20threshold%20based%20on%20predicted%20confidence%20scores%20to%20improve%20the%0Aquality%20of%20pseudo-labels.%20Lastly%2C%20we%20design%20a%20data-driven%20stochastic%0Arestoration%20mechanism%20to%20selectively%20reset%20inactive%20parameters%20using%20the%0Agradients%20as%20weights%20for%20a%20random%20mask%20matrix%2C%20thereby%20ensuring%20the%20retention%0Aof%20essential%20knowledge.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20on%0Afour%20CTTA%20tasks%20for%20object%20detection%2C%20where%20CTAOD%20outperforms%20existing%20methods%2C%0Aespecially%20achieving%20a%203.0%20mAP%20improvement%20on%20the%20Cityscapes-to-Cityscapes-C%0ACTTA%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16439v1&entry.124074799=Read"},
{"title": "Sparser is Faster and Less is More: Efficient Sparse Attention for\n  Long-Range Transformers", "author": "Chao Lou and Zixia Jia and Zilong Zheng and Kewei Tu", "abstract": "  Accommodating long sequences efficiently in autoregressive Transformers,\nespecially within an extended context window, poses significant challenges due\nto the quadratic computational complexity and substantial KV memory\nrequirements inherent in self-attention mechanisms. In this work, we introduce\nSPARSEK Attention, a novel sparse attention mechanism designed to overcome\nthese computational and memory obstacles while maintaining performance. Our\napproach integrates a scoring network and a differentiable top-k mask operator,\nSPARSEK, to select a constant number of KV pairs for each query, thereby\nenabling gradient-based optimization. As a result, SPARSEK Attention offers\nlinear time complexity and constant memory footprint during generation.\nExperimental results reveal that SPARSEK Attention outperforms previous sparse\nattention methods and provides significant speed improvements during both\ntraining and inference, particularly in language modeling and downstream tasks.\nFurthermore, our method can be seamlessly integrated into pre-trained Large\nLanguage Models (LLMs) with minimal fine-tuning, offering a practical solution\nfor effectively managing long-range dependencies in diverse applications.\n", "link": "http://arxiv.org/abs/2406.16747v1", "date": "2024-06-24", "relevancy": 2.2619, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6239}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5378}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5181}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparser%20is%20Faster%20and%20Less%20is%20More%3A%20Efficient%20Sparse%20Attention%20for%0A%20%20Long-Range%20Transformers&body=Title%3A%20Sparser%20is%20Faster%20and%20Less%20is%20More%3A%20Efficient%20Sparse%20Attention%20for%0A%20%20Long-Range%20Transformers%0AAuthor%3A%20Chao%20Lou%20and%20Zixia%20Jia%20and%20Zilong%20Zheng%20and%20Kewei%20Tu%0AAbstract%3A%20%20%20Accommodating%20long%20sequences%20efficiently%20in%20autoregressive%20Transformers%2C%0Aespecially%20within%20an%20extended%20context%20window%2C%20poses%20significant%20challenges%20due%0Ato%20the%20quadratic%20computational%20complexity%20and%20substantial%20KV%20memory%0Arequirements%20inherent%20in%20self-attention%20mechanisms.%20In%20this%20work%2C%20we%20introduce%0ASPARSEK%20Attention%2C%20a%20novel%20sparse%20attention%20mechanism%20designed%20to%20overcome%0Athese%20computational%20and%20memory%20obstacles%20while%20maintaining%20performance.%20Our%0Aapproach%20integrates%20a%20scoring%20network%20and%20a%20differentiable%20top-k%20mask%20operator%2C%0ASPARSEK%2C%20to%20select%20a%20constant%20number%20of%20KV%20pairs%20for%20each%20query%2C%20thereby%0Aenabling%20gradient-based%20optimization.%20As%20a%20result%2C%20SPARSEK%20Attention%20offers%0Alinear%20time%20complexity%20and%20constant%20memory%20footprint%20during%20generation.%0AExperimental%20results%20reveal%20that%20SPARSEK%20Attention%20outperforms%20previous%20sparse%0Aattention%20methods%20and%20provides%20significant%20speed%20improvements%20during%20both%0Atraining%20and%20inference%2C%20particularly%20in%20language%20modeling%20and%20downstream%20tasks.%0AFurthermore%2C%20our%20method%20can%20be%20seamlessly%20integrated%20into%20pre-trained%20Large%0ALanguage%20Models%20%28LLMs%29%20with%20minimal%20fine-tuning%2C%20offering%20a%20practical%20solution%0Afor%20effectively%20managing%20long-range%20dependencies%20in%20diverse%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparser%2520is%2520Faster%2520and%2520Less%2520is%2520More%253A%2520Efficient%2520Sparse%2520Attention%2520for%250A%2520%2520Long-Range%2520Transformers%26entry.906535625%3DChao%2520Lou%2520and%2520Zixia%2520Jia%2520and%2520Zilong%2520Zheng%2520and%2520Kewei%2520Tu%26entry.1292438233%3D%2520%2520Accommodating%2520long%2520sequences%2520efficiently%2520in%2520autoregressive%2520Transformers%252C%250Aespecially%2520within%2520an%2520extended%2520context%2520window%252C%2520poses%2520significant%2520challenges%2520due%250Ato%2520the%2520quadratic%2520computational%2520complexity%2520and%2520substantial%2520KV%2520memory%250Arequirements%2520inherent%2520in%2520self-attention%2520mechanisms.%2520In%2520this%2520work%252C%2520we%2520introduce%250ASPARSEK%2520Attention%252C%2520a%2520novel%2520sparse%2520attention%2520mechanism%2520designed%2520to%2520overcome%250Athese%2520computational%2520and%2520memory%2520obstacles%2520while%2520maintaining%2520performance.%2520Our%250Aapproach%2520integrates%2520a%2520scoring%2520network%2520and%2520a%2520differentiable%2520top-k%2520mask%2520operator%252C%250ASPARSEK%252C%2520to%2520select%2520a%2520constant%2520number%2520of%2520KV%2520pairs%2520for%2520each%2520query%252C%2520thereby%250Aenabling%2520gradient-based%2520optimization.%2520As%2520a%2520result%252C%2520SPARSEK%2520Attention%2520offers%250Alinear%2520time%2520complexity%2520and%2520constant%2520memory%2520footprint%2520during%2520generation.%250AExperimental%2520results%2520reveal%2520that%2520SPARSEK%2520Attention%2520outperforms%2520previous%2520sparse%250Aattention%2520methods%2520and%2520provides%2520significant%2520speed%2520improvements%2520during%2520both%250Atraining%2520and%2520inference%252C%2520particularly%2520in%2520language%2520modeling%2520and%2520downstream%2520tasks.%250AFurthermore%252C%2520our%2520method%2520can%2520be%2520seamlessly%2520integrated%2520into%2520pre-trained%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520with%2520minimal%2520fine-tuning%252C%2520offering%2520a%2520practical%2520solution%250Afor%2520effectively%2520managing%2520long-range%2520dependencies%2520in%2520diverse%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparser%20is%20Faster%20and%20Less%20is%20More%3A%20Efficient%20Sparse%20Attention%20for%0A%20%20Long-Range%20Transformers&entry.906535625=Chao%20Lou%20and%20Zixia%20Jia%20and%20Zilong%20Zheng%20and%20Kewei%20Tu&entry.1292438233=%20%20Accommodating%20long%20sequences%20efficiently%20in%20autoregressive%20Transformers%2C%0Aespecially%20within%20an%20extended%20context%20window%2C%20poses%20significant%20challenges%20due%0Ato%20the%20quadratic%20computational%20complexity%20and%20substantial%20KV%20memory%0Arequirements%20inherent%20in%20self-attention%20mechanisms.%20In%20this%20work%2C%20we%20introduce%0ASPARSEK%20Attention%2C%20a%20novel%20sparse%20attention%20mechanism%20designed%20to%20overcome%0Athese%20computational%20and%20memory%20obstacles%20while%20maintaining%20performance.%20Our%0Aapproach%20integrates%20a%20scoring%20network%20and%20a%20differentiable%20top-k%20mask%20operator%2C%0ASPARSEK%2C%20to%20select%20a%20constant%20number%20of%20KV%20pairs%20for%20each%20query%2C%20thereby%0Aenabling%20gradient-based%20optimization.%20As%20a%20result%2C%20SPARSEK%20Attention%20offers%0Alinear%20time%20complexity%20and%20constant%20memory%20footprint%20during%20generation.%0AExperimental%20results%20reveal%20that%20SPARSEK%20Attention%20outperforms%20previous%20sparse%0Aattention%20methods%20and%20provides%20significant%20speed%20improvements%20during%20both%0Atraining%20and%20inference%2C%20particularly%20in%20language%20modeling%20and%20downstream%20tasks.%0AFurthermore%2C%20our%20method%20can%20be%20seamlessly%20integrated%20into%20pre-trained%20Large%0ALanguage%20Models%20%28LLMs%29%20with%20minimal%20fine-tuning%2C%20offering%20a%20practical%20solution%0Afor%20effectively%20managing%20long-range%20dependencies%20in%20diverse%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16747v1&entry.124074799=Read"},
{"title": "Adaptively Clustering Neighbor Elements for Image-Text Generation", "author": "Zihua Wang and Xu Yang and Hanwang Zhang and Haiyang Xu and Ming Yan and Fei Huang and Yu Zhang", "abstract": "  We propose a novel Transformer-based image-to-text generation model termed as\n\\textbf{ACF} that adaptively clusters vision patches into object regions and\nlanguage words into phrases to implicitly learn object-phrase alignments for\nbetter visual-text coherence. To achieve this, we design a novel self-attention\nlayer that applies self-attention over the elements in a local cluster window\ninstead of the whole sequence. The window size is softly decided by a\nclustering matrix that is calculated by the current input data and thus this\nprocess is adaptive. By stacking these revised self-attention layers to\nconstruct ACF, the small clusters in the lower layers can be grouped into a\nbigger cluster, \\eg vision/language. ACF clusters small objects/phrases into\nbigger ones. In this gradual clustering process, a parsing tree is generated\nwhich embeds the hierarchical knowledge of the input sequence. As a result, by\nusing ACF to build the vision encoder and language decoder, the hierarchical\nobject-phrase alignments are embedded and then transferred from vision to\nlanguage domains in two popular image-to-text tasks: Image captioning and\nVisual Question Answering. The experiment results demonstrate the effectiveness\nof ACF, which outperforms most SOTA captioning and VQA models and achieves\ncomparable scores compared with some large-scale pre-trained models. Our code\nis available \\href{https://github.com/ZihuaEvan/ACFModel/}{[here]}.\n", "link": "http://arxiv.org/abs/2301.01955v3", "date": "2024-06-24", "relevancy": 2.2576, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5926}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.574}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptively%20Clustering%20Neighbor%20Elements%20for%20Image-Text%20Generation&body=Title%3A%20Adaptively%20Clustering%20Neighbor%20Elements%20for%20Image-Text%20Generation%0AAuthor%3A%20Zihua%20Wang%20and%20Xu%20Yang%20and%20Hanwang%20Zhang%20and%20Haiyang%20Xu%20and%20Ming%20Yan%20and%20Fei%20Huang%20and%20Yu%20Zhang%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20Transformer-based%20image-to-text%20generation%20model%20termed%20as%0A%5Ctextbf%7BACF%7D%20that%20adaptively%20clusters%20vision%20patches%20into%20object%20regions%20and%0Alanguage%20words%20into%20phrases%20to%20implicitly%20learn%20object-phrase%20alignments%20for%0Abetter%20visual-text%20coherence.%20To%20achieve%20this%2C%20we%20design%20a%20novel%20self-attention%0Alayer%20that%20applies%20self-attention%20over%20the%20elements%20in%20a%20local%20cluster%20window%0Ainstead%20of%20the%20whole%20sequence.%20The%20window%20size%20is%20softly%20decided%20by%20a%0Aclustering%20matrix%20that%20is%20calculated%20by%20the%20current%20input%20data%20and%20thus%20this%0Aprocess%20is%20adaptive.%20By%20stacking%20these%20revised%20self-attention%20layers%20to%0Aconstruct%20ACF%2C%20the%20small%20clusters%20in%20the%20lower%20layers%20can%20be%20grouped%20into%20a%0Abigger%20cluster%2C%20%5Ceg%20vision/language.%20ACF%20clusters%20small%20objects/phrases%20into%0Abigger%20ones.%20In%20this%20gradual%20clustering%20process%2C%20a%20parsing%20tree%20is%20generated%0Awhich%20embeds%20the%20hierarchical%20knowledge%20of%20the%20input%20sequence.%20As%20a%20result%2C%20by%0Ausing%20ACF%20to%20build%20the%20vision%20encoder%20and%20language%20decoder%2C%20the%20hierarchical%0Aobject-phrase%20alignments%20are%20embedded%20and%20then%20transferred%20from%20vision%20to%0Alanguage%20domains%20in%20two%20popular%20image-to-text%20tasks%3A%20Image%20captioning%20and%0AVisual%20Question%20Answering.%20The%20experiment%20results%20demonstrate%20the%20effectiveness%0Aof%20ACF%2C%20which%20outperforms%20most%20SOTA%20captioning%20and%20VQA%20models%20and%20achieves%0Acomparable%20scores%20compared%20with%20some%20large-scale%20pre-trained%20models.%20Our%20code%0Ais%20available%20%5Chref%7Bhttps%3A//github.com/ZihuaEvan/ACFModel/%7D%7B%5Bhere%5D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.01955v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptively%2520Clustering%2520Neighbor%2520Elements%2520for%2520Image-Text%2520Generation%26entry.906535625%3DZihua%2520Wang%2520and%2520Xu%2520Yang%2520and%2520Hanwang%2520Zhang%2520and%2520Haiyang%2520Xu%2520and%2520Ming%2520Yan%2520and%2520Fei%2520Huang%2520and%2520Yu%2520Zhang%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520Transformer-based%2520image-to-text%2520generation%2520model%2520termed%2520as%250A%255Ctextbf%257BACF%257D%2520that%2520adaptively%2520clusters%2520vision%2520patches%2520into%2520object%2520regions%2520and%250Alanguage%2520words%2520into%2520phrases%2520to%2520implicitly%2520learn%2520object-phrase%2520alignments%2520for%250Abetter%2520visual-text%2520coherence.%2520To%2520achieve%2520this%252C%2520we%2520design%2520a%2520novel%2520self-attention%250Alayer%2520that%2520applies%2520self-attention%2520over%2520the%2520elements%2520in%2520a%2520local%2520cluster%2520window%250Ainstead%2520of%2520the%2520whole%2520sequence.%2520The%2520window%2520size%2520is%2520softly%2520decided%2520by%2520a%250Aclustering%2520matrix%2520that%2520is%2520calculated%2520by%2520the%2520current%2520input%2520data%2520and%2520thus%2520this%250Aprocess%2520is%2520adaptive.%2520By%2520stacking%2520these%2520revised%2520self-attention%2520layers%2520to%250Aconstruct%2520ACF%252C%2520the%2520small%2520clusters%2520in%2520the%2520lower%2520layers%2520can%2520be%2520grouped%2520into%2520a%250Abigger%2520cluster%252C%2520%255Ceg%2520vision/language.%2520ACF%2520clusters%2520small%2520objects/phrases%2520into%250Abigger%2520ones.%2520In%2520this%2520gradual%2520clustering%2520process%252C%2520a%2520parsing%2520tree%2520is%2520generated%250Awhich%2520embeds%2520the%2520hierarchical%2520knowledge%2520of%2520the%2520input%2520sequence.%2520As%2520a%2520result%252C%2520by%250Ausing%2520ACF%2520to%2520build%2520the%2520vision%2520encoder%2520and%2520language%2520decoder%252C%2520the%2520hierarchical%250Aobject-phrase%2520alignments%2520are%2520embedded%2520and%2520then%2520transferred%2520from%2520vision%2520to%250Alanguage%2520domains%2520in%2520two%2520popular%2520image-to-text%2520tasks%253A%2520Image%2520captioning%2520and%250AVisual%2520Question%2520Answering.%2520The%2520experiment%2520results%2520demonstrate%2520the%2520effectiveness%250Aof%2520ACF%252C%2520which%2520outperforms%2520most%2520SOTA%2520captioning%2520and%2520VQA%2520models%2520and%2520achieves%250Acomparable%2520scores%2520compared%2520with%2520some%2520large-scale%2520pre-trained%2520models.%2520Our%2520code%250Ais%2520available%2520%255Chref%257Bhttps%253A//github.com/ZihuaEvan/ACFModel/%257D%257B%255Bhere%255D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.01955v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptively%20Clustering%20Neighbor%20Elements%20for%20Image-Text%20Generation&entry.906535625=Zihua%20Wang%20and%20Xu%20Yang%20and%20Hanwang%20Zhang%20and%20Haiyang%20Xu%20and%20Ming%20Yan%20and%20Fei%20Huang%20and%20Yu%20Zhang&entry.1292438233=%20%20We%20propose%20a%20novel%20Transformer-based%20image-to-text%20generation%20model%20termed%20as%0A%5Ctextbf%7BACF%7D%20that%20adaptively%20clusters%20vision%20patches%20into%20object%20regions%20and%0Alanguage%20words%20into%20phrases%20to%20implicitly%20learn%20object-phrase%20alignments%20for%0Abetter%20visual-text%20coherence.%20To%20achieve%20this%2C%20we%20design%20a%20novel%20self-attention%0Alayer%20that%20applies%20self-attention%20over%20the%20elements%20in%20a%20local%20cluster%20window%0Ainstead%20of%20the%20whole%20sequence.%20The%20window%20size%20is%20softly%20decided%20by%20a%0Aclustering%20matrix%20that%20is%20calculated%20by%20the%20current%20input%20data%20and%20thus%20this%0Aprocess%20is%20adaptive.%20By%20stacking%20these%20revised%20self-attention%20layers%20to%0Aconstruct%20ACF%2C%20the%20small%20clusters%20in%20the%20lower%20layers%20can%20be%20grouped%20into%20a%0Abigger%20cluster%2C%20%5Ceg%20vision/language.%20ACF%20clusters%20small%20objects/phrases%20into%0Abigger%20ones.%20In%20this%20gradual%20clustering%20process%2C%20a%20parsing%20tree%20is%20generated%0Awhich%20embeds%20the%20hierarchical%20knowledge%20of%20the%20input%20sequence.%20As%20a%20result%2C%20by%0Ausing%20ACF%20to%20build%20the%20vision%20encoder%20and%20language%20decoder%2C%20the%20hierarchical%0Aobject-phrase%20alignments%20are%20embedded%20and%20then%20transferred%20from%20vision%20to%0Alanguage%20domains%20in%20two%20popular%20image-to-text%20tasks%3A%20Image%20captioning%20and%0AVisual%20Question%20Answering.%20The%20experiment%20results%20demonstrate%20the%20effectiveness%0Aof%20ACF%2C%20which%20outperforms%20most%20SOTA%20captioning%20and%20VQA%20models%20and%20achieves%0Acomparable%20scores%20compared%20with%20some%20large-scale%20pre-trained%20models.%20Our%20code%0Ais%20available%20%5Chref%7Bhttps%3A//github.com/ZihuaEvan/ACFModel/%7D%7B%5Bhere%5D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.01955v3&entry.124074799=Read"},
{"title": "The Hidden Pitfalls of the Cosine Similarity Loss", "author": "Andrew Draganov and Sharvaree Vadgama and Erik J. Bekkers", "abstract": "  We show that the gradient of the cosine similarity between two points goes to\nzero in two under-explored settings: (1) if a point has large magnitude or (2)\nif the points are on opposite ends of the latent space. Counterintuitively, we\nprove that optimizing the cosine similarity between points forces them to grow\nin magnitude. Thus, (1) is unavoidable in practice. We then observe that these\nderivations are extremely general -- they hold across deep learning\narchitectures and for many of the standard self-supervised learning (SSL) loss\nfunctions. This leads us to propose cut-initialization: a simple change to\nnetwork initialization that helps all studied SSL methods converge faster.\n", "link": "http://arxiv.org/abs/2406.16468v1", "date": "2024-06-24", "relevancy": 2.2542, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4744}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4436}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Hidden%20Pitfalls%20of%20the%20Cosine%20Similarity%20Loss&body=Title%3A%20The%20Hidden%20Pitfalls%20of%20the%20Cosine%20Similarity%20Loss%0AAuthor%3A%20Andrew%20Draganov%20and%20Sharvaree%20Vadgama%20and%20Erik%20J.%20Bekkers%0AAbstract%3A%20%20%20We%20show%20that%20the%20gradient%20of%20the%20cosine%20similarity%20between%20two%20points%20goes%20to%0Azero%20in%20two%20under-explored%20settings%3A%20%281%29%20if%20a%20point%20has%20large%20magnitude%20or%20%282%29%0Aif%20the%20points%20are%20on%20opposite%20ends%20of%20the%20latent%20space.%20Counterintuitively%2C%20we%0Aprove%20that%20optimizing%20the%20cosine%20similarity%20between%20points%20forces%20them%20to%20grow%0Ain%20magnitude.%20Thus%2C%20%281%29%20is%20unavoidable%20in%20practice.%20We%20then%20observe%20that%20these%0Aderivations%20are%20extremely%20general%20--%20they%20hold%20across%20deep%20learning%0Aarchitectures%20and%20for%20many%20of%20the%20standard%20self-supervised%20learning%20%28SSL%29%20loss%0Afunctions.%20This%20leads%20us%20to%20propose%20cut-initialization%3A%20a%20simple%20change%20to%0Anetwork%20initialization%20that%20helps%20all%20studied%20SSL%20methods%20converge%20faster.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Hidden%2520Pitfalls%2520of%2520the%2520Cosine%2520Similarity%2520Loss%26entry.906535625%3DAndrew%2520Draganov%2520and%2520Sharvaree%2520Vadgama%2520and%2520Erik%2520J.%2520Bekkers%26entry.1292438233%3D%2520%2520We%2520show%2520that%2520the%2520gradient%2520of%2520the%2520cosine%2520similarity%2520between%2520two%2520points%2520goes%2520to%250Azero%2520in%2520two%2520under-explored%2520settings%253A%2520%25281%2529%2520if%2520a%2520point%2520has%2520large%2520magnitude%2520or%2520%25282%2529%250Aif%2520the%2520points%2520are%2520on%2520opposite%2520ends%2520of%2520the%2520latent%2520space.%2520Counterintuitively%252C%2520we%250Aprove%2520that%2520optimizing%2520the%2520cosine%2520similarity%2520between%2520points%2520forces%2520them%2520to%2520grow%250Ain%2520magnitude.%2520Thus%252C%2520%25281%2529%2520is%2520unavoidable%2520in%2520practice.%2520We%2520then%2520observe%2520that%2520these%250Aderivations%2520are%2520extremely%2520general%2520--%2520they%2520hold%2520across%2520deep%2520learning%250Aarchitectures%2520and%2520for%2520many%2520of%2520the%2520standard%2520self-supervised%2520learning%2520%2528SSL%2529%2520loss%250Afunctions.%2520This%2520leads%2520us%2520to%2520propose%2520cut-initialization%253A%2520a%2520simple%2520change%2520to%250Anetwork%2520initialization%2520that%2520helps%2520all%2520studied%2520SSL%2520methods%2520converge%2520faster.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Hidden%20Pitfalls%20of%20the%20Cosine%20Similarity%20Loss&entry.906535625=Andrew%20Draganov%20and%20Sharvaree%20Vadgama%20and%20Erik%20J.%20Bekkers&entry.1292438233=%20%20We%20show%20that%20the%20gradient%20of%20the%20cosine%20similarity%20between%20two%20points%20goes%20to%0Azero%20in%20two%20under-explored%20settings%3A%20%281%29%20if%20a%20point%20has%20large%20magnitude%20or%20%282%29%0Aif%20the%20points%20are%20on%20opposite%20ends%20of%20the%20latent%20space.%20Counterintuitively%2C%20we%0Aprove%20that%20optimizing%20the%20cosine%20similarity%20between%20points%20forces%20them%20to%20grow%0Ain%20magnitude.%20Thus%2C%20%281%29%20is%20unavoidable%20in%20practice.%20We%20then%20observe%20that%20these%0Aderivations%20are%20extremely%20general%20--%20they%20hold%20across%20deep%20learning%0Aarchitectures%20and%20for%20many%20of%20the%20standard%20self-supervised%20learning%20%28SSL%29%20loss%0Afunctions.%20This%20leads%20us%20to%20propose%20cut-initialization%3A%20a%20simple%20change%20to%0Anetwork%20initialization%20that%20helps%20all%20studied%20SSL%20methods%20converge%20faster.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16468v1&entry.124074799=Read"},
{"title": "Character-Adapter: Prompt-Guided Region Control for High-Fidelity\n  Character Customization", "author": "Yuhang Ma and Wenting Xu and Jiji Tang and Qinfeng Jin and Rongsheng Zhang and Zeng Zhao and Changjie Fan and Zhipeng Hu", "abstract": "  Customized image generation, which seeks to synthesize images with consistent\ncharacters, holds significant relevance for applications such as storytelling,\nportrait generation, and character design. However, previous approaches have\nencountered challenges in preserving characters with high-fidelity consistency\ndue to inadequate feature extraction and concept confusion of reference\ncharacters. Therefore, we propose Character-Adapter, a plug-and-play framework\ndesigned to generate images that preserve the details of reference characters,\nensuring high-fidelity consistency. Character-Adapter employs prompt-guided\nsegmentation to ensure fine-grained regional features of reference characters\nand dynamic region-level adapters to mitigate concept confusion. Extensive\nexperiments are conducted to validate the effectiveness of Character-Adapter.\nBoth quantitative and qualitative results demonstrate that Character-Adapter\nachieves the state-of-the-art performance of consistent character generation,\nwith an improvement of 24.8% compared with other methods\n", "link": "http://arxiv.org/abs/2406.16537v1", "date": "2024-06-24", "relevancy": 2.254, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5856}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5507}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5402}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Character-Adapter%3A%20Prompt-Guided%20Region%20Control%20for%20High-Fidelity%0A%20%20Character%20Customization&body=Title%3A%20Character-Adapter%3A%20Prompt-Guided%20Region%20Control%20for%20High-Fidelity%0A%20%20Character%20Customization%0AAuthor%3A%20Yuhang%20Ma%20and%20Wenting%20Xu%20and%20Jiji%20Tang%20and%20Qinfeng%20Jin%20and%20Rongsheng%20Zhang%20and%20Zeng%20Zhao%20and%20Changjie%20Fan%20and%20Zhipeng%20Hu%0AAbstract%3A%20%20%20Customized%20image%20generation%2C%20which%20seeks%20to%20synthesize%20images%20with%20consistent%0Acharacters%2C%20holds%20significant%20relevance%20for%20applications%20such%20as%20storytelling%2C%0Aportrait%20generation%2C%20and%20character%20design.%20However%2C%20previous%20approaches%20have%0Aencountered%20challenges%20in%20preserving%20characters%20with%20high-fidelity%20consistency%0Adue%20to%20inadequate%20feature%20extraction%20and%20concept%20confusion%20of%20reference%0Acharacters.%20Therefore%2C%20we%20propose%20Character-Adapter%2C%20a%20plug-and-play%20framework%0Adesigned%20to%20generate%20images%20that%20preserve%20the%20details%20of%20reference%20characters%2C%0Aensuring%20high-fidelity%20consistency.%20Character-Adapter%20employs%20prompt-guided%0Asegmentation%20to%20ensure%20fine-grained%20regional%20features%20of%20reference%20characters%0Aand%20dynamic%20region-level%20adapters%20to%20mitigate%20concept%20confusion.%20Extensive%0Aexperiments%20are%20conducted%20to%20validate%20the%20effectiveness%20of%20Character-Adapter.%0ABoth%20quantitative%20and%20qualitative%20results%20demonstrate%20that%20Character-Adapter%0Aachieves%20the%20state-of-the-art%20performance%20of%20consistent%20character%20generation%2C%0Awith%20an%20improvement%20of%2024.8%25%20compared%20with%20other%20methods%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacter-Adapter%253A%2520Prompt-Guided%2520Region%2520Control%2520for%2520High-Fidelity%250A%2520%2520Character%2520Customization%26entry.906535625%3DYuhang%2520Ma%2520and%2520Wenting%2520Xu%2520and%2520Jiji%2520Tang%2520and%2520Qinfeng%2520Jin%2520and%2520Rongsheng%2520Zhang%2520and%2520Zeng%2520Zhao%2520and%2520Changjie%2520Fan%2520and%2520Zhipeng%2520Hu%26entry.1292438233%3D%2520%2520Customized%2520image%2520generation%252C%2520which%2520seeks%2520to%2520synthesize%2520images%2520with%2520consistent%250Acharacters%252C%2520holds%2520significant%2520relevance%2520for%2520applications%2520such%2520as%2520storytelling%252C%250Aportrait%2520generation%252C%2520and%2520character%2520design.%2520However%252C%2520previous%2520approaches%2520have%250Aencountered%2520challenges%2520in%2520preserving%2520characters%2520with%2520high-fidelity%2520consistency%250Adue%2520to%2520inadequate%2520feature%2520extraction%2520and%2520concept%2520confusion%2520of%2520reference%250Acharacters.%2520Therefore%252C%2520we%2520propose%2520Character-Adapter%252C%2520a%2520plug-and-play%2520framework%250Adesigned%2520to%2520generate%2520images%2520that%2520preserve%2520the%2520details%2520of%2520reference%2520characters%252C%250Aensuring%2520high-fidelity%2520consistency.%2520Character-Adapter%2520employs%2520prompt-guided%250Asegmentation%2520to%2520ensure%2520fine-grained%2520regional%2520features%2520of%2520reference%2520characters%250Aand%2520dynamic%2520region-level%2520adapters%2520to%2520mitigate%2520concept%2520confusion.%2520Extensive%250Aexperiments%2520are%2520conducted%2520to%2520validate%2520the%2520effectiveness%2520of%2520Character-Adapter.%250ABoth%2520quantitative%2520and%2520qualitative%2520results%2520demonstrate%2520that%2520Character-Adapter%250Aachieves%2520the%2520state-of-the-art%2520performance%2520of%2520consistent%2520character%2520generation%252C%250Awith%2520an%2520improvement%2520of%252024.8%2525%2520compared%2520with%2520other%2520methods%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Character-Adapter%3A%20Prompt-Guided%20Region%20Control%20for%20High-Fidelity%0A%20%20Character%20Customization&entry.906535625=Yuhang%20Ma%20and%20Wenting%20Xu%20and%20Jiji%20Tang%20and%20Qinfeng%20Jin%20and%20Rongsheng%20Zhang%20and%20Zeng%20Zhao%20and%20Changjie%20Fan%20and%20Zhipeng%20Hu&entry.1292438233=%20%20Customized%20image%20generation%2C%20which%20seeks%20to%20synthesize%20images%20with%20consistent%0Acharacters%2C%20holds%20significant%20relevance%20for%20applications%20such%20as%20storytelling%2C%0Aportrait%20generation%2C%20and%20character%20design.%20However%2C%20previous%20approaches%20have%0Aencountered%20challenges%20in%20preserving%20characters%20with%20high-fidelity%20consistency%0Adue%20to%20inadequate%20feature%20extraction%20and%20concept%20confusion%20of%20reference%0Acharacters.%20Therefore%2C%20we%20propose%20Character-Adapter%2C%20a%20plug-and-play%20framework%0Adesigned%20to%20generate%20images%20that%20preserve%20the%20details%20of%20reference%20characters%2C%0Aensuring%20high-fidelity%20consistency.%20Character-Adapter%20employs%20prompt-guided%0Asegmentation%20to%20ensure%20fine-grained%20regional%20features%20of%20reference%20characters%0Aand%20dynamic%20region-level%20adapters%20to%20mitigate%20concept%20confusion.%20Extensive%0Aexperiments%20are%20conducted%20to%20validate%20the%20effectiveness%20of%20Character-Adapter.%0ABoth%20quantitative%20and%20qualitative%20results%20demonstrate%20that%20Character-Adapter%0Aachieves%20the%20state-of-the-art%20performance%20of%20consistent%20character%20generation%2C%0Awith%20an%20improvement%20of%2024.8%25%20compared%20with%20other%20methods%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16537v1&entry.124074799=Read"},
{"title": "Multimodal Multilabel Classification by CLIP", "author": "Yanming Guo", "abstract": "  Multimodal multilabel classification (MMC) is a challenging task that aims to\ndesign a learning algorithm to handle two data sources, the image and text, and\nlearn a comprehensive semantic feature presentation across the modalities. In\nthis task, we review the extensive number of state-of-the-art approaches in MMC\nand leverage a novel technique that utilises the Contrastive Language-Image\nPre-training (CLIP) as the feature extractor and fine-tune the model by\nexploring different classification heads, fusion methods and loss functions.\nFinally, our best result achieved more than 90% F_1 score in the public Kaggle\ncompetition leaderboard. This paper provides detailed descriptions of novel\ntraining methods and quantitative analysis through the experimental results.\n", "link": "http://arxiv.org/abs/2406.16141v1", "date": "2024-06-23", "relevancy": 2.2514, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6298}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5339}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Multilabel%20Classification%20by%20CLIP&body=Title%3A%20Multimodal%20Multilabel%20Classification%20by%20CLIP%0AAuthor%3A%20Yanming%20Guo%0AAbstract%3A%20%20%20Multimodal%20multilabel%20classification%20%28MMC%29%20is%20a%20challenging%20task%20that%20aims%20to%0Adesign%20a%20learning%20algorithm%20to%20handle%20two%20data%20sources%2C%20the%20image%20and%20text%2C%20and%0Alearn%20a%20comprehensive%20semantic%20feature%20presentation%20across%20the%20modalities.%20In%0Athis%20task%2C%20we%20review%20the%20extensive%20number%20of%20state-of-the-art%20approaches%20in%20MMC%0Aand%20leverage%20a%20novel%20technique%20that%20utilises%20the%20Contrastive%20Language-Image%0APre-training%20%28CLIP%29%20as%20the%20feature%20extractor%20and%20fine-tune%20the%20model%20by%0Aexploring%20different%20classification%20heads%2C%20fusion%20methods%20and%20loss%20functions.%0AFinally%2C%20our%20best%20result%20achieved%20more%20than%2090%25%20F_1%20score%20in%20the%20public%20Kaggle%0Acompetition%20leaderboard.%20This%20paper%20provides%20detailed%20descriptions%20of%20novel%0Atraining%20methods%20and%20quantitative%20analysis%20through%20the%20experimental%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16141v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Multilabel%2520Classification%2520by%2520CLIP%26entry.906535625%3DYanming%2520Guo%26entry.1292438233%3D%2520%2520Multimodal%2520multilabel%2520classification%2520%2528MMC%2529%2520is%2520a%2520challenging%2520task%2520that%2520aims%2520to%250Adesign%2520a%2520learning%2520algorithm%2520to%2520handle%2520two%2520data%2520sources%252C%2520the%2520image%2520and%2520text%252C%2520and%250Alearn%2520a%2520comprehensive%2520semantic%2520feature%2520presentation%2520across%2520the%2520modalities.%2520In%250Athis%2520task%252C%2520we%2520review%2520the%2520extensive%2520number%2520of%2520state-of-the-art%2520approaches%2520in%2520MMC%250Aand%2520leverage%2520a%2520novel%2520technique%2520that%2520utilises%2520the%2520Contrastive%2520Language-Image%250APre-training%2520%2528CLIP%2529%2520as%2520the%2520feature%2520extractor%2520and%2520fine-tune%2520the%2520model%2520by%250Aexploring%2520different%2520classification%2520heads%252C%2520fusion%2520methods%2520and%2520loss%2520functions.%250AFinally%252C%2520our%2520best%2520result%2520achieved%2520more%2520than%252090%2525%2520F_1%2520score%2520in%2520the%2520public%2520Kaggle%250Acompetition%2520leaderboard.%2520This%2520paper%2520provides%2520detailed%2520descriptions%2520of%2520novel%250Atraining%2520methods%2520and%2520quantitative%2520analysis%2520through%2520the%2520experimental%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16141v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Multilabel%20Classification%20by%20CLIP&entry.906535625=Yanming%20Guo&entry.1292438233=%20%20Multimodal%20multilabel%20classification%20%28MMC%29%20is%20a%20challenging%20task%20that%20aims%20to%0Adesign%20a%20learning%20algorithm%20to%20handle%20two%20data%20sources%2C%20the%20image%20and%20text%2C%20and%0Alearn%20a%20comprehensive%20semantic%20feature%20presentation%20across%20the%20modalities.%20In%0Athis%20task%2C%20we%20review%20the%20extensive%20number%20of%20state-of-the-art%20approaches%20in%20MMC%0Aand%20leverage%20a%20novel%20technique%20that%20utilises%20the%20Contrastive%20Language-Image%0APre-training%20%28CLIP%29%20as%20the%20feature%20extractor%20and%20fine-tune%20the%20model%20by%0Aexploring%20different%20classification%20heads%2C%20fusion%20methods%20and%20loss%20functions.%0AFinally%2C%20our%20best%20result%20achieved%20more%20than%2090%25%20F_1%20score%20in%20the%20public%20Kaggle%0Acompetition%20leaderboard.%20This%20paper%20provides%20detailed%20descriptions%20of%20novel%0Atraining%20methods%20and%20quantitative%20analysis%20through%20the%20experimental%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16141v1&entry.124074799=Read"},
{"title": "Instance Consistency Regularization for Semi-Supervised 3D Instance\n  Segmentation", "author": "Yizheng Wu and Zhiyu Pan and Kewei Wang and Xingyi Li and Jiahao Cui and Liwen Xiao and Guosheng Lin and Zhiguo Cao", "abstract": "  Large-scale datasets with point-wise semantic and instance labels are crucial\nto 3D instance segmentation but also expensive. To leverage unlabeled data,\nprevious semi-supervised 3D instance segmentation approaches have explored\nself-training frameworks, which rely on high-quality pseudo labels for\nconsistency regularization. They intuitively utilize both instance and semantic\npseudo labels in a joint learning manner. However, semantic pseudo labels\ncontain numerous noise derived from the imbalanced category distribution and\nnatural confusion of similar but distinct categories, which leads to severe\ncollapses in self-training. Motivated by the observation that 3D instances are\nnon-overlapping and spatially separable, we ask whether we can solely rely on\ninstance consistency regularization for improved semi-supervised segmentation.\nTo this end, we propose a novel self-training network InsTeacher3D to explore\nand exploit pure instance knowledge from unlabeled data. We first build a\nparallel base 3D instance segmentation model DKNet, which distinguishes each\ninstance from the others via discriminative instance kernels without reliance\non semantic segmentation. Based on DKNet, we further design a novel instance\nconsistency regularization framework to generate and leverage high-quality\ninstance pseudo labels. Experimental results on multiple large-scale datasets\nshow that the InsTeacher3D significantly outperforms prior state-of-the-art\nsemi-supervised approaches. Code is available:\nhttps://github.com/W1zheng/InsTeacher3D.\n", "link": "http://arxiv.org/abs/2406.16776v1", "date": "2024-06-24", "relevancy": 2.2485, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5781}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5674}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instance%20Consistency%20Regularization%20for%20Semi-Supervised%203D%20Instance%0A%20%20Segmentation&body=Title%3A%20Instance%20Consistency%20Regularization%20for%20Semi-Supervised%203D%20Instance%0A%20%20Segmentation%0AAuthor%3A%20Yizheng%20Wu%20and%20Zhiyu%20Pan%20and%20Kewei%20Wang%20and%20Xingyi%20Li%20and%20Jiahao%20Cui%20and%20Liwen%20Xiao%20and%20Guosheng%20Lin%20and%20Zhiguo%20Cao%0AAbstract%3A%20%20%20Large-scale%20datasets%20with%20point-wise%20semantic%20and%20instance%20labels%20are%20crucial%0Ato%203D%20instance%20segmentation%20but%20also%20expensive.%20To%20leverage%20unlabeled%20data%2C%0Aprevious%20semi-supervised%203D%20instance%20segmentation%20approaches%20have%20explored%0Aself-training%20frameworks%2C%20which%20rely%20on%20high-quality%20pseudo%20labels%20for%0Aconsistency%20regularization.%20They%20intuitively%20utilize%20both%20instance%20and%20semantic%0Apseudo%20labels%20in%20a%20joint%20learning%20manner.%20However%2C%20semantic%20pseudo%20labels%0Acontain%20numerous%20noise%20derived%20from%20the%20imbalanced%20category%20distribution%20and%0Anatural%20confusion%20of%20similar%20but%20distinct%20categories%2C%20which%20leads%20to%20severe%0Acollapses%20in%20self-training.%20Motivated%20by%20the%20observation%20that%203D%20instances%20are%0Anon-overlapping%20and%20spatially%20separable%2C%20we%20ask%20whether%20we%20can%20solely%20rely%20on%0Ainstance%20consistency%20regularization%20for%20improved%20semi-supervised%20segmentation.%0ATo%20this%20end%2C%20we%20propose%20a%20novel%20self-training%20network%20InsTeacher3D%20to%20explore%0Aand%20exploit%20pure%20instance%20knowledge%20from%20unlabeled%20data.%20We%20first%20build%20a%0Aparallel%20base%203D%20instance%20segmentation%20model%20DKNet%2C%20which%20distinguishes%20each%0Ainstance%20from%20the%20others%20via%20discriminative%20instance%20kernels%20without%20reliance%0Aon%20semantic%20segmentation.%20Based%20on%20DKNet%2C%20we%20further%20design%20a%20novel%20instance%0Aconsistency%20regularization%20framework%20to%20generate%20and%20leverage%20high-quality%0Ainstance%20pseudo%20labels.%20Experimental%20results%20on%20multiple%20large-scale%20datasets%0Ashow%20that%20the%20InsTeacher3D%20significantly%20outperforms%20prior%20state-of-the-art%0Asemi-supervised%20approaches.%20Code%20is%20available%3A%0Ahttps%3A//github.com/W1zheng/InsTeacher3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstance%2520Consistency%2520Regularization%2520for%2520Semi-Supervised%25203D%2520Instance%250A%2520%2520Segmentation%26entry.906535625%3DYizheng%2520Wu%2520and%2520Zhiyu%2520Pan%2520and%2520Kewei%2520Wang%2520and%2520Xingyi%2520Li%2520and%2520Jiahao%2520Cui%2520and%2520Liwen%2520Xiao%2520and%2520Guosheng%2520Lin%2520and%2520Zhiguo%2520Cao%26entry.1292438233%3D%2520%2520Large-scale%2520datasets%2520with%2520point-wise%2520semantic%2520and%2520instance%2520labels%2520are%2520crucial%250Ato%25203D%2520instance%2520segmentation%2520but%2520also%2520expensive.%2520To%2520leverage%2520unlabeled%2520data%252C%250Aprevious%2520semi-supervised%25203D%2520instance%2520segmentation%2520approaches%2520have%2520explored%250Aself-training%2520frameworks%252C%2520which%2520rely%2520on%2520high-quality%2520pseudo%2520labels%2520for%250Aconsistency%2520regularization.%2520They%2520intuitively%2520utilize%2520both%2520instance%2520and%2520semantic%250Apseudo%2520labels%2520in%2520a%2520joint%2520learning%2520manner.%2520However%252C%2520semantic%2520pseudo%2520labels%250Acontain%2520numerous%2520noise%2520derived%2520from%2520the%2520imbalanced%2520category%2520distribution%2520and%250Anatural%2520confusion%2520of%2520similar%2520but%2520distinct%2520categories%252C%2520which%2520leads%2520to%2520severe%250Acollapses%2520in%2520self-training.%2520Motivated%2520by%2520the%2520observation%2520that%25203D%2520instances%2520are%250Anon-overlapping%2520and%2520spatially%2520separable%252C%2520we%2520ask%2520whether%2520we%2520can%2520solely%2520rely%2520on%250Ainstance%2520consistency%2520regularization%2520for%2520improved%2520semi-supervised%2520segmentation.%250ATo%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520self-training%2520network%2520InsTeacher3D%2520to%2520explore%250Aand%2520exploit%2520pure%2520instance%2520knowledge%2520from%2520unlabeled%2520data.%2520We%2520first%2520build%2520a%250Aparallel%2520base%25203D%2520instance%2520segmentation%2520model%2520DKNet%252C%2520which%2520distinguishes%2520each%250Ainstance%2520from%2520the%2520others%2520via%2520discriminative%2520instance%2520kernels%2520without%2520reliance%250Aon%2520semantic%2520segmentation.%2520Based%2520on%2520DKNet%252C%2520we%2520further%2520design%2520a%2520novel%2520instance%250Aconsistency%2520regularization%2520framework%2520to%2520generate%2520and%2520leverage%2520high-quality%250Ainstance%2520pseudo%2520labels.%2520Experimental%2520results%2520on%2520multiple%2520large-scale%2520datasets%250Ashow%2520that%2520the%2520InsTeacher3D%2520significantly%2520outperforms%2520prior%2520state-of-the-art%250Asemi-supervised%2520approaches.%2520Code%2520is%2520available%253A%250Ahttps%253A//github.com/W1zheng/InsTeacher3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instance%20Consistency%20Regularization%20for%20Semi-Supervised%203D%20Instance%0A%20%20Segmentation&entry.906535625=Yizheng%20Wu%20and%20Zhiyu%20Pan%20and%20Kewei%20Wang%20and%20Xingyi%20Li%20and%20Jiahao%20Cui%20and%20Liwen%20Xiao%20and%20Guosheng%20Lin%20and%20Zhiguo%20Cao&entry.1292438233=%20%20Large-scale%20datasets%20with%20point-wise%20semantic%20and%20instance%20labels%20are%20crucial%0Ato%203D%20instance%20segmentation%20but%20also%20expensive.%20To%20leverage%20unlabeled%20data%2C%0Aprevious%20semi-supervised%203D%20instance%20segmentation%20approaches%20have%20explored%0Aself-training%20frameworks%2C%20which%20rely%20on%20high-quality%20pseudo%20labels%20for%0Aconsistency%20regularization.%20They%20intuitively%20utilize%20both%20instance%20and%20semantic%0Apseudo%20labels%20in%20a%20joint%20learning%20manner.%20However%2C%20semantic%20pseudo%20labels%0Acontain%20numerous%20noise%20derived%20from%20the%20imbalanced%20category%20distribution%20and%0Anatural%20confusion%20of%20similar%20but%20distinct%20categories%2C%20which%20leads%20to%20severe%0Acollapses%20in%20self-training.%20Motivated%20by%20the%20observation%20that%203D%20instances%20are%0Anon-overlapping%20and%20spatially%20separable%2C%20we%20ask%20whether%20we%20can%20solely%20rely%20on%0Ainstance%20consistency%20regularization%20for%20improved%20semi-supervised%20segmentation.%0ATo%20this%20end%2C%20we%20propose%20a%20novel%20self-training%20network%20InsTeacher3D%20to%20explore%0Aand%20exploit%20pure%20instance%20knowledge%20from%20unlabeled%20data.%20We%20first%20build%20a%0Aparallel%20base%203D%20instance%20segmentation%20model%20DKNet%2C%20which%20distinguishes%20each%0Ainstance%20from%20the%20others%20via%20discriminative%20instance%20kernels%20without%20reliance%0Aon%20semantic%20segmentation.%20Based%20on%20DKNet%2C%20we%20further%20design%20a%20novel%20instance%0Aconsistency%20regularization%20framework%20to%20generate%20and%20leverage%20high-quality%0Ainstance%20pseudo%20labels.%20Experimental%20results%20on%20multiple%20large-scale%20datasets%0Ashow%20that%20the%20InsTeacher3D%20significantly%20outperforms%20prior%20state-of-the-art%0Asemi-supervised%20approaches.%20Code%20is%20available%3A%0Ahttps%3A//github.com/W1zheng/InsTeacher3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16776v1&entry.124074799=Read"},
{"title": "Sim2Real Bilevel Adaptation for Object Surface Classification using\n  Vision-Based Tactile Sensors", "author": "Gabriele M. Caddeo and Andrea Maracani and Paolo D. Alfano and Nicola A. Piga and Lorenzo Rosasco and Lorenzo Natale", "abstract": "  In this paper, we address the Sim2Real gap in the field of vision-based\ntactile sensors for classifying object surfaces. We train a Diffusion Model to\nbridge this gap using a relatively small dataset of real-world images randomly\ncollected from unlabeled everyday objects via the DIGIT sensor. Subsequently,\nwe employ a simulator to generate images by uniformly sampling the surface of\nobjects from the YCB Model Set. These simulated images are then translated into\nthe real domain using the Diffusion Model and automatically labeled to train a\nclassifier. During this training, we further align features of the two domains\nusing an adversarial procedure. Our evaluation is conducted on a dataset of\ntactile images obtained from a set of ten 3D printed YCB objects. The results\nreveal a total accuracy of 81.9%, a significant improvement compared to the\n34.7% achieved by the classifier trained solely on simulated images. This\ndemonstrates the effectiveness of our approach. We further validate our\napproach using the classifier on a 6D object pose estimation task from tactile\ndata.\n", "link": "http://arxiv.org/abs/2311.01380v2", "date": "2024-06-24", "relevancy": 2.2446, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5897}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5635}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sim2Real%20Bilevel%20Adaptation%20for%20Object%20Surface%20Classification%20using%0A%20%20Vision-Based%20Tactile%20Sensors&body=Title%3A%20Sim2Real%20Bilevel%20Adaptation%20for%20Object%20Surface%20Classification%20using%0A%20%20Vision-Based%20Tactile%20Sensors%0AAuthor%3A%20Gabriele%20M.%20Caddeo%20and%20Andrea%20Maracani%20and%20Paolo%20D.%20Alfano%20and%20Nicola%20A.%20Piga%20and%20Lorenzo%20Rosasco%20and%20Lorenzo%20Natale%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20the%20Sim2Real%20gap%20in%20the%20field%20of%20vision-based%0Atactile%20sensors%20for%20classifying%20object%20surfaces.%20We%20train%20a%20Diffusion%20Model%20to%0Abridge%20this%20gap%20using%20a%20relatively%20small%20dataset%20of%20real-world%20images%20randomly%0Acollected%20from%20unlabeled%20everyday%20objects%20via%20the%20DIGIT%20sensor.%20Subsequently%2C%0Awe%20employ%20a%20simulator%20to%20generate%20images%20by%20uniformly%20sampling%20the%20surface%20of%0Aobjects%20from%20the%20YCB%20Model%20Set.%20These%20simulated%20images%20are%20then%20translated%20into%0Athe%20real%20domain%20using%20the%20Diffusion%20Model%20and%20automatically%20labeled%20to%20train%20a%0Aclassifier.%20During%20this%20training%2C%20we%20further%20align%20features%20of%20the%20two%20domains%0Ausing%20an%20adversarial%20procedure.%20Our%20evaluation%20is%20conducted%20on%20a%20dataset%20of%0Atactile%20images%20obtained%20from%20a%20set%20of%20ten%203D%20printed%20YCB%20objects.%20The%20results%0Areveal%20a%20total%20accuracy%20of%2081.9%25%2C%20a%20significant%20improvement%20compared%20to%20the%0A34.7%25%20achieved%20by%20the%20classifier%20trained%20solely%20on%20simulated%20images.%20This%0Ademonstrates%20the%20effectiveness%20of%20our%20approach.%20We%20further%20validate%20our%0Aapproach%20using%20the%20classifier%20on%20a%206D%20object%20pose%20estimation%20task%20from%20tactile%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.01380v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSim2Real%2520Bilevel%2520Adaptation%2520for%2520Object%2520Surface%2520Classification%2520using%250A%2520%2520Vision-Based%2520Tactile%2520Sensors%26entry.906535625%3DGabriele%2520M.%2520Caddeo%2520and%2520Andrea%2520Maracani%2520and%2520Paolo%2520D.%2520Alfano%2520and%2520Nicola%2520A.%2520Piga%2520and%2520Lorenzo%2520Rosasco%2520and%2520Lorenzo%2520Natale%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520Sim2Real%2520gap%2520in%2520the%2520field%2520of%2520vision-based%250Atactile%2520sensors%2520for%2520classifying%2520object%2520surfaces.%2520We%2520train%2520a%2520Diffusion%2520Model%2520to%250Abridge%2520this%2520gap%2520using%2520a%2520relatively%2520small%2520dataset%2520of%2520real-world%2520images%2520randomly%250Acollected%2520from%2520unlabeled%2520everyday%2520objects%2520via%2520the%2520DIGIT%2520sensor.%2520Subsequently%252C%250Awe%2520employ%2520a%2520simulator%2520to%2520generate%2520images%2520by%2520uniformly%2520sampling%2520the%2520surface%2520of%250Aobjects%2520from%2520the%2520YCB%2520Model%2520Set.%2520These%2520simulated%2520images%2520are%2520then%2520translated%2520into%250Athe%2520real%2520domain%2520using%2520the%2520Diffusion%2520Model%2520and%2520automatically%2520labeled%2520to%2520train%2520a%250Aclassifier.%2520During%2520this%2520training%252C%2520we%2520further%2520align%2520features%2520of%2520the%2520two%2520domains%250Ausing%2520an%2520adversarial%2520procedure.%2520Our%2520evaluation%2520is%2520conducted%2520on%2520a%2520dataset%2520of%250Atactile%2520images%2520obtained%2520from%2520a%2520set%2520of%2520ten%25203D%2520printed%2520YCB%2520objects.%2520The%2520results%250Areveal%2520a%2520total%2520accuracy%2520of%252081.9%2525%252C%2520a%2520significant%2520improvement%2520compared%2520to%2520the%250A34.7%2525%2520achieved%2520by%2520the%2520classifier%2520trained%2520solely%2520on%2520simulated%2520images.%2520This%250Ademonstrates%2520the%2520effectiveness%2520of%2520our%2520approach.%2520We%2520further%2520validate%2520our%250Aapproach%2520using%2520the%2520classifier%2520on%2520a%25206D%2520object%2520pose%2520estimation%2520task%2520from%2520tactile%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.01380v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sim2Real%20Bilevel%20Adaptation%20for%20Object%20Surface%20Classification%20using%0A%20%20Vision-Based%20Tactile%20Sensors&entry.906535625=Gabriele%20M.%20Caddeo%20and%20Andrea%20Maracani%20and%20Paolo%20D.%20Alfano%20and%20Nicola%20A.%20Piga%20and%20Lorenzo%20Rosasco%20and%20Lorenzo%20Natale&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20Sim2Real%20gap%20in%20the%20field%20of%20vision-based%0Atactile%20sensors%20for%20classifying%20object%20surfaces.%20We%20train%20a%20Diffusion%20Model%20to%0Abridge%20this%20gap%20using%20a%20relatively%20small%20dataset%20of%20real-world%20images%20randomly%0Acollected%20from%20unlabeled%20everyday%20objects%20via%20the%20DIGIT%20sensor.%20Subsequently%2C%0Awe%20employ%20a%20simulator%20to%20generate%20images%20by%20uniformly%20sampling%20the%20surface%20of%0Aobjects%20from%20the%20YCB%20Model%20Set.%20These%20simulated%20images%20are%20then%20translated%20into%0Athe%20real%20domain%20using%20the%20Diffusion%20Model%20and%20automatically%20labeled%20to%20train%20a%0Aclassifier.%20During%20this%20training%2C%20we%20further%20align%20features%20of%20the%20two%20domains%0Ausing%20an%20adversarial%20procedure.%20Our%20evaluation%20is%20conducted%20on%20a%20dataset%20of%0Atactile%20images%20obtained%20from%20a%20set%20of%20ten%203D%20printed%20YCB%20objects.%20The%20results%0Areveal%20a%20total%20accuracy%20of%2081.9%25%2C%20a%20significant%20improvement%20compared%20to%20the%0A34.7%25%20achieved%20by%20the%20classifier%20trained%20solely%20on%20simulated%20images.%20This%0Ademonstrates%20the%20effectiveness%20of%20our%20approach.%20We%20further%20validate%20our%0Aapproach%20using%20the%20classifier%20on%20a%206D%20object%20pose%20estimation%20task%20from%20tactile%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.01380v2&entry.124074799=Read"},
{"title": "High-resolution open-vocabulary object 6D pose estimation", "author": "Jaime Corsetti and Davide Boscaini and Francesco Giuliari and Changjae Oh and Andrea Cavallaro and Fabio Poiesi", "abstract": "  The generalisation to unseen objects in the 6D pose estimation task is very\nchallenging. While Vision-Language Models (VLMs) enable using natural language\ndescriptions to support 6D pose estimation of unseen objects, these solutions\nunderperform compared to model-based methods. In this work we present Horyon,\nan open-vocabulary VLM-based architecture that addresses relative pose\nestimation between two scenes of an unseen object, described by a textual\nprompt only. We use the textual prompt to identify the unseen object in the\nscenes and then obtain high-resolution multi-scale features. These features are\nused to extract cross-scene matches for registration. We evaluate our model on\na benchmark with a large variety of unseen objects across four datasets, namely\nREAL275, Toyota-Light, Linemod, and YCB-Video. Our method achieves\nstate-of-the-art performance on all datasets, outperforming by 12.6 in Average\nRecall the previous best-performing approach.\n", "link": "http://arxiv.org/abs/2406.16384v1", "date": "2024-06-24", "relevancy": 2.2443, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5775}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5526}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-resolution%20open-vocabulary%20object%206D%20pose%20estimation&body=Title%3A%20High-resolution%20open-vocabulary%20object%206D%20pose%20estimation%0AAuthor%3A%20Jaime%20Corsetti%20and%20Davide%20Boscaini%20and%20Francesco%20Giuliari%20and%20Changjae%20Oh%20and%20Andrea%20Cavallaro%20and%20Fabio%20Poiesi%0AAbstract%3A%20%20%20The%20generalisation%20to%20unseen%20objects%20in%20the%206D%20pose%20estimation%20task%20is%20very%0Achallenging.%20While%20Vision-Language%20Models%20%28VLMs%29%20enable%20using%20natural%20language%0Adescriptions%20to%20support%206D%20pose%20estimation%20of%20unseen%20objects%2C%20these%20solutions%0Aunderperform%20compared%20to%20model-based%20methods.%20In%20this%20work%20we%20present%20Horyon%2C%0Aan%20open-vocabulary%20VLM-based%20architecture%20that%20addresses%20relative%20pose%0Aestimation%20between%20two%20scenes%20of%20an%20unseen%20object%2C%20described%20by%20a%20textual%0Aprompt%20only.%20We%20use%20the%20textual%20prompt%20to%20identify%20the%20unseen%20object%20in%20the%0Ascenes%20and%20then%20obtain%20high-resolution%20multi-scale%20features.%20These%20features%20are%0Aused%20to%20extract%20cross-scene%20matches%20for%20registration.%20We%20evaluate%20our%20model%20on%0Aa%20benchmark%20with%20a%20large%20variety%20of%20unseen%20objects%20across%20four%20datasets%2C%20namely%0AREAL275%2C%20Toyota-Light%2C%20Linemod%2C%20and%20YCB-Video.%20Our%20method%20achieves%0Astate-of-the-art%20performance%20on%20all%20datasets%2C%20outperforming%20by%2012.6%20in%20Average%0ARecall%20the%20previous%20best-performing%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16384v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-resolution%2520open-vocabulary%2520object%25206D%2520pose%2520estimation%26entry.906535625%3DJaime%2520Corsetti%2520and%2520Davide%2520Boscaini%2520and%2520Francesco%2520Giuliari%2520and%2520Changjae%2520Oh%2520and%2520Andrea%2520Cavallaro%2520and%2520Fabio%2520Poiesi%26entry.1292438233%3D%2520%2520The%2520generalisation%2520to%2520unseen%2520objects%2520in%2520the%25206D%2520pose%2520estimation%2520task%2520is%2520very%250Achallenging.%2520While%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520enable%2520using%2520natural%2520language%250Adescriptions%2520to%2520support%25206D%2520pose%2520estimation%2520of%2520unseen%2520objects%252C%2520these%2520solutions%250Aunderperform%2520compared%2520to%2520model-based%2520methods.%2520In%2520this%2520work%2520we%2520present%2520Horyon%252C%250Aan%2520open-vocabulary%2520VLM-based%2520architecture%2520that%2520addresses%2520relative%2520pose%250Aestimation%2520between%2520two%2520scenes%2520of%2520an%2520unseen%2520object%252C%2520described%2520by%2520a%2520textual%250Aprompt%2520only.%2520We%2520use%2520the%2520textual%2520prompt%2520to%2520identify%2520the%2520unseen%2520object%2520in%2520the%250Ascenes%2520and%2520then%2520obtain%2520high-resolution%2520multi-scale%2520features.%2520These%2520features%2520are%250Aused%2520to%2520extract%2520cross-scene%2520matches%2520for%2520registration.%2520We%2520evaluate%2520our%2520model%2520on%250Aa%2520benchmark%2520with%2520a%2520large%2520variety%2520of%2520unseen%2520objects%2520across%2520four%2520datasets%252C%2520namely%250AREAL275%252C%2520Toyota-Light%252C%2520Linemod%252C%2520and%2520YCB-Video.%2520Our%2520method%2520achieves%250Astate-of-the-art%2520performance%2520on%2520all%2520datasets%252C%2520outperforming%2520by%252012.6%2520in%2520Average%250ARecall%2520the%2520previous%2520best-performing%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16384v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-resolution%20open-vocabulary%20object%206D%20pose%20estimation&entry.906535625=Jaime%20Corsetti%20and%20Davide%20Boscaini%20and%20Francesco%20Giuliari%20and%20Changjae%20Oh%20and%20Andrea%20Cavallaro%20and%20Fabio%20Poiesi&entry.1292438233=%20%20The%20generalisation%20to%20unseen%20objects%20in%20the%206D%20pose%20estimation%20task%20is%20very%0Achallenging.%20While%20Vision-Language%20Models%20%28VLMs%29%20enable%20using%20natural%20language%0Adescriptions%20to%20support%206D%20pose%20estimation%20of%20unseen%20objects%2C%20these%20solutions%0Aunderperform%20compared%20to%20model-based%20methods.%20In%20this%20work%20we%20present%20Horyon%2C%0Aan%20open-vocabulary%20VLM-based%20architecture%20that%20addresses%20relative%20pose%0Aestimation%20between%20two%20scenes%20of%20an%20unseen%20object%2C%20described%20by%20a%20textual%0Aprompt%20only.%20We%20use%20the%20textual%20prompt%20to%20identify%20the%20unseen%20object%20in%20the%0Ascenes%20and%20then%20obtain%20high-resolution%20multi-scale%20features.%20These%20features%20are%0Aused%20to%20extract%20cross-scene%20matches%20for%20registration.%20We%20evaluate%20our%20model%20on%0Aa%20benchmark%20with%20a%20large%20variety%20of%20unseen%20objects%20across%20four%20datasets%2C%20namely%0AREAL275%2C%20Toyota-Light%2C%20Linemod%2C%20and%20YCB-Video.%20Our%20method%20achieves%0Astate-of-the-art%20performance%20on%20all%20datasets%2C%20outperforming%20by%2012.6%20in%20Average%0ARecall%20the%20previous%20best-performing%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16384v1&entry.124074799=Read"},
{"title": "Vision Beyond Boundaries: An Initial Design Space of Domain-specific\n  Large Vision Models in Human-robot Interaction", "author": "Yuchong Zhang and Yong Ma and Danica Kragic", "abstract": "  The emergence of large vision models (LVMs) is following in the footsteps of\nthe recent prosperity of Large Language Models (LLMs) in following years.\nHowever, there's a noticeable gap in structured research applying LVMs to\nhuman-robot interaction (HRI), despite extensive evidence supporting the\nefficacy of vision models in enhancing interactions between humans and robots.\nRecognizing the vast and anticipated potential, we introduce an initial design\nspace that incorporates domain-specific LVMs, chosen for their superior\nperformance over normal models. We delve into three primary dimensions: HRI\ncontexts, vision-based tasks, and specific domains. The empirical evaluation\nwas implemented among 15 experts across six evaluated metrics, showcasing the\nprimary efficacy in relevant decision-making scenarios. We explore the process\nof ideation and potential application scenarios, envisioning this design space\nas a foundational guideline for future HRI system design, emphasizing accurate\ndomain alignment and model selection.\n", "link": "http://arxiv.org/abs/2404.14965v4", "date": "2024-06-24", "relevancy": 2.2398, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5672}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5615}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision%20Beyond%20Boundaries%3A%20An%20Initial%20Design%20Space%20of%20Domain-specific%0A%20%20Large%20Vision%20Models%20in%20Human-robot%20Interaction&body=Title%3A%20Vision%20Beyond%20Boundaries%3A%20An%20Initial%20Design%20Space%20of%20Domain-specific%0A%20%20Large%20Vision%20Models%20in%20Human-robot%20Interaction%0AAuthor%3A%20Yuchong%20Zhang%20and%20Yong%20Ma%20and%20Danica%20Kragic%0AAbstract%3A%20%20%20The%20emergence%20of%20large%20vision%20models%20%28LVMs%29%20is%20following%20in%20the%20footsteps%20of%0Athe%20recent%20prosperity%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20following%20years.%0AHowever%2C%20there%27s%20a%20noticeable%20gap%20in%20structured%20research%20applying%20LVMs%20to%0Ahuman-robot%20interaction%20%28HRI%29%2C%20despite%20extensive%20evidence%20supporting%20the%0Aefficacy%20of%20vision%20models%20in%20enhancing%20interactions%20between%20humans%20and%20robots.%0ARecognizing%20the%20vast%20and%20anticipated%20potential%2C%20we%20introduce%20an%20initial%20design%0Aspace%20that%20incorporates%20domain-specific%20LVMs%2C%20chosen%20for%20their%20superior%0Aperformance%20over%20normal%20models.%20We%20delve%20into%20three%20primary%20dimensions%3A%20HRI%0Acontexts%2C%20vision-based%20tasks%2C%20and%20specific%20domains.%20The%20empirical%20evaluation%0Awas%20implemented%20among%2015%20experts%20across%20six%20evaluated%20metrics%2C%20showcasing%20the%0Aprimary%20efficacy%20in%20relevant%20decision-making%20scenarios.%20We%20explore%20the%20process%0Aof%20ideation%20and%20potential%20application%20scenarios%2C%20envisioning%20this%20design%20space%0Aas%20a%20foundational%20guideline%20for%20future%20HRI%20system%20design%2C%20emphasizing%20accurate%0Adomain%20alignment%20and%20model%20selection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14965v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision%2520Beyond%2520Boundaries%253A%2520An%2520Initial%2520Design%2520Space%2520of%2520Domain-specific%250A%2520%2520Large%2520Vision%2520Models%2520in%2520Human-robot%2520Interaction%26entry.906535625%3DYuchong%2520Zhang%2520and%2520Yong%2520Ma%2520and%2520Danica%2520Kragic%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520large%2520vision%2520models%2520%2528LVMs%2529%2520is%2520following%2520in%2520the%2520footsteps%2520of%250Athe%2520recent%2520prosperity%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520following%2520years.%250AHowever%252C%2520there%2527s%2520a%2520noticeable%2520gap%2520in%2520structured%2520research%2520applying%2520LVMs%2520to%250Ahuman-robot%2520interaction%2520%2528HRI%2529%252C%2520despite%2520extensive%2520evidence%2520supporting%2520the%250Aefficacy%2520of%2520vision%2520models%2520in%2520enhancing%2520interactions%2520between%2520humans%2520and%2520robots.%250ARecognizing%2520the%2520vast%2520and%2520anticipated%2520potential%252C%2520we%2520introduce%2520an%2520initial%2520design%250Aspace%2520that%2520incorporates%2520domain-specific%2520LVMs%252C%2520chosen%2520for%2520their%2520superior%250Aperformance%2520over%2520normal%2520models.%2520We%2520delve%2520into%2520three%2520primary%2520dimensions%253A%2520HRI%250Acontexts%252C%2520vision-based%2520tasks%252C%2520and%2520specific%2520domains.%2520The%2520empirical%2520evaluation%250Awas%2520implemented%2520among%252015%2520experts%2520across%2520six%2520evaluated%2520metrics%252C%2520showcasing%2520the%250Aprimary%2520efficacy%2520in%2520relevant%2520decision-making%2520scenarios.%2520We%2520explore%2520the%2520process%250Aof%2520ideation%2520and%2520potential%2520application%2520scenarios%252C%2520envisioning%2520this%2520design%2520space%250Aas%2520a%2520foundational%2520guideline%2520for%2520future%2520HRI%2520system%2520design%252C%2520emphasizing%2520accurate%250Adomain%2520alignment%2520and%2520model%2520selection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14965v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision%20Beyond%20Boundaries%3A%20An%20Initial%20Design%20Space%20of%20Domain-specific%0A%20%20Large%20Vision%20Models%20in%20Human-robot%20Interaction&entry.906535625=Yuchong%20Zhang%20and%20Yong%20Ma%20and%20Danica%20Kragic&entry.1292438233=%20%20The%20emergence%20of%20large%20vision%20models%20%28LVMs%29%20is%20following%20in%20the%20footsteps%20of%0Athe%20recent%20prosperity%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20following%20years.%0AHowever%2C%20there%27s%20a%20noticeable%20gap%20in%20structured%20research%20applying%20LVMs%20to%0Ahuman-robot%20interaction%20%28HRI%29%2C%20despite%20extensive%20evidence%20supporting%20the%0Aefficacy%20of%20vision%20models%20in%20enhancing%20interactions%20between%20humans%20and%20robots.%0ARecognizing%20the%20vast%20and%20anticipated%20potential%2C%20we%20introduce%20an%20initial%20design%0Aspace%20that%20incorporates%20domain-specific%20LVMs%2C%20chosen%20for%20their%20superior%0Aperformance%20over%20normal%20models.%20We%20delve%20into%20three%20primary%20dimensions%3A%20HRI%0Acontexts%2C%20vision-based%20tasks%2C%20and%20specific%20domains.%20The%20empirical%20evaluation%0Awas%20implemented%20among%2015%20experts%20across%20six%20evaluated%20metrics%2C%20showcasing%20the%0Aprimary%20efficacy%20in%20relevant%20decision-making%20scenarios.%20We%20explore%20the%20process%0Aof%20ideation%20and%20potential%20application%20scenarios%2C%20envisioning%20this%20design%20space%0Aas%20a%20foundational%20guideline%20for%20future%20HRI%20system%20design%2C%20emphasizing%20accurate%0Adomain%20alignment%20and%20model%20selection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14965v4&entry.124074799=Read"},
{"title": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs", "author": "Shengbang Tong and Ellis Brown and Penghao Wu and Sanghyun Woo and Manoj Middepogu and Sai Charitha Akula and Jihan Yang and Shusheng Yang and Adithya Iyer and Xichen Pan and Austin Wang and Rob Fergus and Yann LeCun and Saining Xie", "abstract": "  We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a\nvision-centric approach. While stronger language models can enhance multimodal\ncapabilities, the design choices for vision components are often insufficiently\nexplored and disconnected from visual representation learning research. This\ngap hinders accurate sensory grounding in real-world scenarios. Our study uses\nLLMs and visual instruction tuning as an interface to evaluate various visual\nrepresentations, offering new insights into different models and architectures\n-- self-supervised, strongly supervised, or combinations thereof -- based on\nexperiments with over 20 vision encoders. We critically examine existing MLLM\nbenchmarks, addressing the difficulties involved in consolidating and\ninterpreting results from various tasks, and introduce a new vision-centric\nbenchmark, CV-Bench. To further improve visual grounding, we propose the\nSpatial Vision Aggregator (SVA), a dynamic and spatially-aware connector that\nintegrates high-resolution vision features with LLMs while reducing the number\nof tokens. Additionally, we discuss the curation of high-quality visual\ninstruction-tuning data from publicly available sources, emphasizing the\nimportance of data source balancing and distribution ratio. Collectively,\nCambrian-1 not only achieves state-of-the-art performance but also serves as a\ncomprehensive, open cookbook for instruction-tuned MLLMs. We provide model\nweights, code, supporting tools, datasets, and detailed instruction-tuning and\nevaluation recipes. We hope our release will inspire and accelerate\nadvancements in multimodal systems and visual representation learning.\n", "link": "http://arxiv.org/abs/2406.16860v1", "date": "2024-06-24", "relevancy": 2.2296, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5861}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5661}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cambrian-1%3A%20A%20Fully%20Open%2C%20Vision-Centric%20Exploration%20of%20Multimodal%20LLMs&body=Title%3A%20Cambrian-1%3A%20A%20Fully%20Open%2C%20Vision-Centric%20Exploration%20of%20Multimodal%20LLMs%0AAuthor%3A%20Shengbang%20Tong%20and%20Ellis%20Brown%20and%20Penghao%20Wu%20and%20Sanghyun%20Woo%20and%20Manoj%20Middepogu%20and%20Sai%20Charitha%20Akula%20and%20Jihan%20Yang%20and%20Shusheng%20Yang%20and%20Adithya%20Iyer%20and%20Xichen%20Pan%20and%20Austin%20Wang%20and%20Rob%20Fergus%20and%20Yann%20LeCun%20and%20Saining%20Xie%0AAbstract%3A%20%20%20We%20introduce%20Cambrian-1%2C%20a%20family%20of%20multimodal%20LLMs%20%28MLLMs%29%20designed%20with%20a%0Avision-centric%20approach.%20While%20stronger%20language%20models%20can%20enhance%20multimodal%0Acapabilities%2C%20the%20design%20choices%20for%20vision%20components%20are%20often%20insufficiently%0Aexplored%20and%20disconnected%20from%20visual%20representation%20learning%20research.%20This%0Agap%20hinders%20accurate%20sensory%20grounding%20in%20real-world%20scenarios.%20Our%20study%20uses%0ALLMs%20and%20visual%20instruction%20tuning%20as%20an%20interface%20to%20evaluate%20various%20visual%0Arepresentations%2C%20offering%20new%20insights%20into%20different%20models%20and%20architectures%0A--%20self-supervised%2C%20strongly%20supervised%2C%20or%20combinations%20thereof%20--%20based%20on%0Aexperiments%20with%20over%2020%20vision%20encoders.%20We%20critically%20examine%20existing%20MLLM%0Abenchmarks%2C%20addressing%20the%20difficulties%20involved%20in%20consolidating%20and%0Ainterpreting%20results%20from%20various%20tasks%2C%20and%20introduce%20a%20new%20vision-centric%0Abenchmark%2C%20CV-Bench.%20To%20further%20improve%20visual%20grounding%2C%20we%20propose%20the%0ASpatial%20Vision%20Aggregator%20%28SVA%29%2C%20a%20dynamic%20and%20spatially-aware%20connector%20that%0Aintegrates%20high-resolution%20vision%20features%20with%20LLMs%20while%20reducing%20the%20number%0Aof%20tokens.%20Additionally%2C%20we%20discuss%20the%20curation%20of%20high-quality%20visual%0Ainstruction-tuning%20data%20from%20publicly%20available%20sources%2C%20emphasizing%20the%0Aimportance%20of%20data%20source%20balancing%20and%20distribution%20ratio.%20Collectively%2C%0ACambrian-1%20not%20only%20achieves%20state-of-the-art%20performance%20but%20also%20serves%20as%20a%0Acomprehensive%2C%20open%20cookbook%20for%20instruction-tuned%20MLLMs.%20We%20provide%20model%0Aweights%2C%20code%2C%20supporting%20tools%2C%20datasets%2C%20and%20detailed%20instruction-tuning%20and%0Aevaluation%20recipes.%20We%20hope%20our%20release%20will%20inspire%20and%20accelerate%0Aadvancements%20in%20multimodal%20systems%20and%20visual%20representation%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCambrian-1%253A%2520A%2520Fully%2520Open%252C%2520Vision-Centric%2520Exploration%2520of%2520Multimodal%2520LLMs%26entry.906535625%3DShengbang%2520Tong%2520and%2520Ellis%2520Brown%2520and%2520Penghao%2520Wu%2520and%2520Sanghyun%2520Woo%2520and%2520Manoj%2520Middepogu%2520and%2520Sai%2520Charitha%2520Akula%2520and%2520Jihan%2520Yang%2520and%2520Shusheng%2520Yang%2520and%2520Adithya%2520Iyer%2520and%2520Xichen%2520Pan%2520and%2520Austin%2520Wang%2520and%2520Rob%2520Fergus%2520and%2520Yann%2520LeCun%2520and%2520Saining%2520Xie%26entry.1292438233%3D%2520%2520We%2520introduce%2520Cambrian-1%252C%2520a%2520family%2520of%2520multimodal%2520LLMs%2520%2528MLLMs%2529%2520designed%2520with%2520a%250Avision-centric%2520approach.%2520While%2520stronger%2520language%2520models%2520can%2520enhance%2520multimodal%250Acapabilities%252C%2520the%2520design%2520choices%2520for%2520vision%2520components%2520are%2520often%2520insufficiently%250Aexplored%2520and%2520disconnected%2520from%2520visual%2520representation%2520learning%2520research.%2520This%250Agap%2520hinders%2520accurate%2520sensory%2520grounding%2520in%2520real-world%2520scenarios.%2520Our%2520study%2520uses%250ALLMs%2520and%2520visual%2520instruction%2520tuning%2520as%2520an%2520interface%2520to%2520evaluate%2520various%2520visual%250Arepresentations%252C%2520offering%2520new%2520insights%2520into%2520different%2520models%2520and%2520architectures%250A--%2520self-supervised%252C%2520strongly%2520supervised%252C%2520or%2520combinations%2520thereof%2520--%2520based%2520on%250Aexperiments%2520with%2520over%252020%2520vision%2520encoders.%2520We%2520critically%2520examine%2520existing%2520MLLM%250Abenchmarks%252C%2520addressing%2520the%2520difficulties%2520involved%2520in%2520consolidating%2520and%250Ainterpreting%2520results%2520from%2520various%2520tasks%252C%2520and%2520introduce%2520a%2520new%2520vision-centric%250Abenchmark%252C%2520CV-Bench.%2520To%2520further%2520improve%2520visual%2520grounding%252C%2520we%2520propose%2520the%250ASpatial%2520Vision%2520Aggregator%2520%2528SVA%2529%252C%2520a%2520dynamic%2520and%2520spatially-aware%2520connector%2520that%250Aintegrates%2520high-resolution%2520vision%2520features%2520with%2520LLMs%2520while%2520reducing%2520the%2520number%250Aof%2520tokens.%2520Additionally%252C%2520we%2520discuss%2520the%2520curation%2520of%2520high-quality%2520visual%250Ainstruction-tuning%2520data%2520from%2520publicly%2520available%2520sources%252C%2520emphasizing%2520the%250Aimportance%2520of%2520data%2520source%2520balancing%2520and%2520distribution%2520ratio.%2520Collectively%252C%250ACambrian-1%2520not%2520only%2520achieves%2520state-of-the-art%2520performance%2520but%2520also%2520serves%2520as%2520a%250Acomprehensive%252C%2520open%2520cookbook%2520for%2520instruction-tuned%2520MLLMs.%2520We%2520provide%2520model%250Aweights%252C%2520code%252C%2520supporting%2520tools%252C%2520datasets%252C%2520and%2520detailed%2520instruction-tuning%2520and%250Aevaluation%2520recipes.%2520We%2520hope%2520our%2520release%2520will%2520inspire%2520and%2520accelerate%250Aadvancements%2520in%2520multimodal%2520systems%2520and%2520visual%2520representation%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cambrian-1%3A%20A%20Fully%20Open%2C%20Vision-Centric%20Exploration%20of%20Multimodal%20LLMs&entry.906535625=Shengbang%20Tong%20and%20Ellis%20Brown%20and%20Penghao%20Wu%20and%20Sanghyun%20Woo%20and%20Manoj%20Middepogu%20and%20Sai%20Charitha%20Akula%20and%20Jihan%20Yang%20and%20Shusheng%20Yang%20and%20Adithya%20Iyer%20and%20Xichen%20Pan%20and%20Austin%20Wang%20and%20Rob%20Fergus%20and%20Yann%20LeCun%20and%20Saining%20Xie&entry.1292438233=%20%20We%20introduce%20Cambrian-1%2C%20a%20family%20of%20multimodal%20LLMs%20%28MLLMs%29%20designed%20with%20a%0Avision-centric%20approach.%20While%20stronger%20language%20models%20can%20enhance%20multimodal%0Acapabilities%2C%20the%20design%20choices%20for%20vision%20components%20are%20often%20insufficiently%0Aexplored%20and%20disconnected%20from%20visual%20representation%20learning%20research.%20This%0Agap%20hinders%20accurate%20sensory%20grounding%20in%20real-world%20scenarios.%20Our%20study%20uses%0ALLMs%20and%20visual%20instruction%20tuning%20as%20an%20interface%20to%20evaluate%20various%20visual%0Arepresentations%2C%20offering%20new%20insights%20into%20different%20models%20and%20architectures%0A--%20self-supervised%2C%20strongly%20supervised%2C%20or%20combinations%20thereof%20--%20based%20on%0Aexperiments%20with%20over%2020%20vision%20encoders.%20We%20critically%20examine%20existing%20MLLM%0Abenchmarks%2C%20addressing%20the%20difficulties%20involved%20in%20consolidating%20and%0Ainterpreting%20results%20from%20various%20tasks%2C%20and%20introduce%20a%20new%20vision-centric%0Abenchmark%2C%20CV-Bench.%20To%20further%20improve%20visual%20grounding%2C%20we%20propose%20the%0ASpatial%20Vision%20Aggregator%20%28SVA%29%2C%20a%20dynamic%20and%20spatially-aware%20connector%20that%0Aintegrates%20high-resolution%20vision%20features%20with%20LLMs%20while%20reducing%20the%20number%0Aof%20tokens.%20Additionally%2C%20we%20discuss%20the%20curation%20of%20high-quality%20visual%0Ainstruction-tuning%20data%20from%20publicly%20available%20sources%2C%20emphasizing%20the%0Aimportance%20of%20data%20source%20balancing%20and%20distribution%20ratio.%20Collectively%2C%0ACambrian-1%20not%20only%20achieves%20state-of-the-art%20performance%20but%20also%20serves%20as%20a%0Acomprehensive%2C%20open%20cookbook%20for%20instruction-tuned%20MLLMs.%20We%20provide%20model%0Aweights%2C%20code%2C%20supporting%20tools%2C%20datasets%2C%20and%20detailed%20instruction-tuning%20and%0Aevaluation%20recipes.%20We%20hope%20our%20release%20will%20inspire%20and%20accelerate%0Aadvancements%20in%20multimodal%20systems%20and%20visual%20representation%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16860v1&entry.124074799=Read"},
{"title": "LIR: A Lightweight Baseline for Image Restoration", "author": "Dongqi Fan and Ting Yue and Xin Zhao and Renjing Xu and Liang Chang", "abstract": "  Recently, there have been significant advancements in Image Restoration based\non CNN and transformer. However, the inherent characteristics of the Image\nRestoration task are often overlooked in many works. They, instead, tend to\nfocus on the basic block design and stack numerous such blocks to the model,\nleading to parameters redundant and computations unnecessary. Thus, the\nefficiency of the image restoration is hindered. In this paper, we propose a\nLightweight Baseline network for Image Restoration called LIR to efficiently\nrestore the image and remove degradations. First of all, through an ingenious\nstructural design, LIR removes the degradations existing in the local and\nglobal residual connections that are ignored by modern networks. Then, a\nLightweight Adaptive Attention (LAA) Block is introduced which is mainly\ncomposed of proposed Adaptive Filters and Attention Blocks. The proposed\nAdaptive Filter is used to adaptively extract high-frequency information and\nenhance object contours in various IR tasks, and Attention Block involves a\nnovel Patch Attention module to approximate the self-attention part of the\ntransformer. On the deraining task, our LIR achieves the state-of-the-art\nStructure Similarity Index Measure (SSIM) and comparable performance to\nstate-of-the-art models on Peak Signal-to-Noise Ratio (PSNR). For denoising,\ndehazing, and deblurring tasks, LIR also achieves a comparable performance to\nstate-of-the-art models with a parameter size of about 30\\%. In addition, it is\nworth noting that our LIR produces better visual results that are more in line\nwith the human aesthetic.\n", "link": "http://arxiv.org/abs/2402.01368v3", "date": "2024-06-24", "relevancy": 2.2273, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.564}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5538}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LIR%3A%20A%20Lightweight%20Baseline%20for%20Image%20Restoration&body=Title%3A%20LIR%3A%20A%20Lightweight%20Baseline%20for%20Image%20Restoration%0AAuthor%3A%20Dongqi%20Fan%20and%20Ting%20Yue%20and%20Xin%20Zhao%20and%20Renjing%20Xu%20and%20Liang%20Chang%0AAbstract%3A%20%20%20Recently%2C%20there%20have%20been%20significant%20advancements%20in%20Image%20Restoration%20based%0Aon%20CNN%20and%20transformer.%20However%2C%20the%20inherent%20characteristics%20of%20the%20Image%0ARestoration%20task%20are%20often%20overlooked%20in%20many%20works.%20They%2C%20instead%2C%20tend%20to%0Afocus%20on%20the%20basic%20block%20design%20and%20stack%20numerous%20such%20blocks%20to%20the%20model%2C%0Aleading%20to%20parameters%20redundant%20and%20computations%20unnecessary.%20Thus%2C%20the%0Aefficiency%20of%20the%20image%20restoration%20is%20hindered.%20In%20this%20paper%2C%20we%20propose%20a%0ALightweight%20Baseline%20network%20for%20Image%20Restoration%20called%20LIR%20to%20efficiently%0Arestore%20the%20image%20and%20remove%20degradations.%20First%20of%20all%2C%20through%20an%20ingenious%0Astructural%20design%2C%20LIR%20removes%20the%20degradations%20existing%20in%20the%20local%20and%0Aglobal%20residual%20connections%20that%20are%20ignored%20by%20modern%20networks.%20Then%2C%20a%0ALightweight%20Adaptive%20Attention%20%28LAA%29%20Block%20is%20introduced%20which%20is%20mainly%0Acomposed%20of%20proposed%20Adaptive%20Filters%20and%20Attention%20Blocks.%20The%20proposed%0AAdaptive%20Filter%20is%20used%20to%20adaptively%20extract%20high-frequency%20information%20and%0Aenhance%20object%20contours%20in%20various%20IR%20tasks%2C%20and%20Attention%20Block%20involves%20a%0Anovel%20Patch%20Attention%20module%20to%20approximate%20the%20self-attention%20part%20of%20the%0Atransformer.%20On%20the%20deraining%20task%2C%20our%20LIR%20achieves%20the%20state-of-the-art%0AStructure%20Similarity%20Index%20Measure%20%28SSIM%29%20and%20comparable%20performance%20to%0Astate-of-the-art%20models%20on%20Peak%20Signal-to-Noise%20Ratio%20%28PSNR%29.%20For%20denoising%2C%0Adehazing%2C%20and%20deblurring%20tasks%2C%20LIR%20also%20achieves%20a%20comparable%20performance%20to%0Astate-of-the-art%20models%20with%20a%20parameter%20size%20of%20about%2030%5C%25.%20In%20addition%2C%20it%20is%0Aworth%20noting%20that%20our%20LIR%20produces%20better%20visual%20results%20that%20are%20more%20in%20line%0Awith%20the%20human%20aesthetic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01368v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLIR%253A%2520A%2520Lightweight%2520Baseline%2520for%2520Image%2520Restoration%26entry.906535625%3DDongqi%2520Fan%2520and%2520Ting%2520Yue%2520and%2520Xin%2520Zhao%2520and%2520Renjing%2520Xu%2520and%2520Liang%2520Chang%26entry.1292438233%3D%2520%2520Recently%252C%2520there%2520have%2520been%2520significant%2520advancements%2520in%2520Image%2520Restoration%2520based%250Aon%2520CNN%2520and%2520transformer.%2520However%252C%2520the%2520inherent%2520characteristics%2520of%2520the%2520Image%250ARestoration%2520task%2520are%2520often%2520overlooked%2520in%2520many%2520works.%2520They%252C%2520instead%252C%2520tend%2520to%250Afocus%2520on%2520the%2520basic%2520block%2520design%2520and%2520stack%2520numerous%2520such%2520blocks%2520to%2520the%2520model%252C%250Aleading%2520to%2520parameters%2520redundant%2520and%2520computations%2520unnecessary.%2520Thus%252C%2520the%250Aefficiency%2520of%2520the%2520image%2520restoration%2520is%2520hindered.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250ALightweight%2520Baseline%2520network%2520for%2520Image%2520Restoration%2520called%2520LIR%2520to%2520efficiently%250Arestore%2520the%2520image%2520and%2520remove%2520degradations.%2520First%2520of%2520all%252C%2520through%2520an%2520ingenious%250Astructural%2520design%252C%2520LIR%2520removes%2520the%2520degradations%2520existing%2520in%2520the%2520local%2520and%250Aglobal%2520residual%2520connections%2520that%2520are%2520ignored%2520by%2520modern%2520networks.%2520Then%252C%2520a%250ALightweight%2520Adaptive%2520Attention%2520%2528LAA%2529%2520Block%2520is%2520introduced%2520which%2520is%2520mainly%250Acomposed%2520of%2520proposed%2520Adaptive%2520Filters%2520and%2520Attention%2520Blocks.%2520The%2520proposed%250AAdaptive%2520Filter%2520is%2520used%2520to%2520adaptively%2520extract%2520high-frequency%2520information%2520and%250Aenhance%2520object%2520contours%2520in%2520various%2520IR%2520tasks%252C%2520and%2520Attention%2520Block%2520involves%2520a%250Anovel%2520Patch%2520Attention%2520module%2520to%2520approximate%2520the%2520self-attention%2520part%2520of%2520the%250Atransformer.%2520On%2520the%2520deraining%2520task%252C%2520our%2520LIR%2520achieves%2520the%2520state-of-the-art%250AStructure%2520Similarity%2520Index%2520Measure%2520%2528SSIM%2529%2520and%2520comparable%2520performance%2520to%250Astate-of-the-art%2520models%2520on%2520Peak%2520Signal-to-Noise%2520Ratio%2520%2528PSNR%2529.%2520For%2520denoising%252C%250Adehazing%252C%2520and%2520deblurring%2520tasks%252C%2520LIR%2520also%2520achieves%2520a%2520comparable%2520performance%2520to%250Astate-of-the-art%2520models%2520with%2520a%2520parameter%2520size%2520of%2520about%252030%255C%2525.%2520In%2520addition%252C%2520it%2520is%250Aworth%2520noting%2520that%2520our%2520LIR%2520produces%2520better%2520visual%2520results%2520that%2520are%2520more%2520in%2520line%250Awith%2520the%2520human%2520aesthetic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01368v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LIR%3A%20A%20Lightweight%20Baseline%20for%20Image%20Restoration&entry.906535625=Dongqi%20Fan%20and%20Ting%20Yue%20and%20Xin%20Zhao%20and%20Renjing%20Xu%20and%20Liang%20Chang&entry.1292438233=%20%20Recently%2C%20there%20have%20been%20significant%20advancements%20in%20Image%20Restoration%20based%0Aon%20CNN%20and%20transformer.%20However%2C%20the%20inherent%20characteristics%20of%20the%20Image%0ARestoration%20task%20are%20often%20overlooked%20in%20many%20works.%20They%2C%20instead%2C%20tend%20to%0Afocus%20on%20the%20basic%20block%20design%20and%20stack%20numerous%20such%20blocks%20to%20the%20model%2C%0Aleading%20to%20parameters%20redundant%20and%20computations%20unnecessary.%20Thus%2C%20the%0Aefficiency%20of%20the%20image%20restoration%20is%20hindered.%20In%20this%20paper%2C%20we%20propose%20a%0ALightweight%20Baseline%20network%20for%20Image%20Restoration%20called%20LIR%20to%20efficiently%0Arestore%20the%20image%20and%20remove%20degradations.%20First%20of%20all%2C%20through%20an%20ingenious%0Astructural%20design%2C%20LIR%20removes%20the%20degradations%20existing%20in%20the%20local%20and%0Aglobal%20residual%20connections%20that%20are%20ignored%20by%20modern%20networks.%20Then%2C%20a%0ALightweight%20Adaptive%20Attention%20%28LAA%29%20Block%20is%20introduced%20which%20is%20mainly%0Acomposed%20of%20proposed%20Adaptive%20Filters%20and%20Attention%20Blocks.%20The%20proposed%0AAdaptive%20Filter%20is%20used%20to%20adaptively%20extract%20high-frequency%20information%20and%0Aenhance%20object%20contours%20in%20various%20IR%20tasks%2C%20and%20Attention%20Block%20involves%20a%0Anovel%20Patch%20Attention%20module%20to%20approximate%20the%20self-attention%20part%20of%20the%0Atransformer.%20On%20the%20deraining%20task%2C%20our%20LIR%20achieves%20the%20state-of-the-art%0AStructure%20Similarity%20Index%20Measure%20%28SSIM%29%20and%20comparable%20performance%20to%0Astate-of-the-art%20models%20on%20Peak%20Signal-to-Noise%20Ratio%20%28PSNR%29.%20For%20denoising%2C%0Adehazing%2C%20and%20deblurring%20tasks%2C%20LIR%20also%20achieves%20a%20comparable%20performance%20to%0Astate-of-the-art%20models%20with%20a%20parameter%20size%20of%20about%2030%5C%25.%20In%20addition%2C%20it%20is%0Aworth%20noting%20that%20our%20LIR%20produces%20better%20visual%20results%20that%20are%20more%20in%20line%0Awith%20the%20human%20aesthetic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01368v3&entry.124074799=Read"},
{"title": "DriveVLM: The Convergence of Autonomous Driving and Large\n  Vision-Language Models", "author": "Xiaoyu Tian and Junru Gu and Bailin Li and Yicheng Liu and Yang Wang and Zhiyong Zhao and Kun Zhan and Peng Jia and Xianpeng Lang and Hang Zhao", "abstract": "  A primary hurdle of autonomous driving in urban environments is understanding\ncomplex and long-tail scenarios, such as challenging road conditions and\ndelicate human behaviors. We introduce DriveVLM, an autonomous driving system\nleveraging Vision-Language Models (VLMs) for enhanced scene understanding and\nplanning capabilities. DriveVLM integrates a unique combination of reasoning\nmodules for scene description, scene analysis, and hierarchical planning.\nFurthermore, recognizing the limitations of VLMs in spatial reasoning and heavy\ncomputational requirements, we propose DriveVLM-Dual, a hybrid system that\nsynergizes the strengths of DriveVLM with the traditional autonomous driving\npipeline. Experiments on both the nuScenes dataset and our SUP-AD dataset\ndemonstrate the efficacy of DriveVLM and DriveVLM-Dual in handling complex and\nunpredictable driving conditions. Finally, we deploy the DriveVLM-Dual on a\nproduction vehicle, verifying it is effective in real-world autonomous driving\nenvironments.\n", "link": "http://arxiv.org/abs/2402.12289v4", "date": "2024-06-23", "relevancy": 2.2271, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5781}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5447}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DriveVLM%3A%20The%20Convergence%20of%20Autonomous%20Driving%20and%20Large%0A%20%20Vision-Language%20Models&body=Title%3A%20DriveVLM%3A%20The%20Convergence%20of%20Autonomous%20Driving%20and%20Large%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Xiaoyu%20Tian%20and%20Junru%20Gu%20and%20Bailin%20Li%20and%20Yicheng%20Liu%20and%20Yang%20Wang%20and%20Zhiyong%20Zhao%20and%20Kun%20Zhan%20and%20Peng%20Jia%20and%20Xianpeng%20Lang%20and%20Hang%20Zhao%0AAbstract%3A%20%20%20A%20primary%20hurdle%20of%20autonomous%20driving%20in%20urban%20environments%20is%20understanding%0Acomplex%20and%20long-tail%20scenarios%2C%20such%20as%20challenging%20road%20conditions%20and%0Adelicate%20human%20behaviors.%20We%20introduce%20DriveVLM%2C%20an%20autonomous%20driving%20system%0Aleveraging%20Vision-Language%20Models%20%28VLMs%29%20for%20enhanced%20scene%20understanding%20and%0Aplanning%20capabilities.%20DriveVLM%20integrates%20a%20unique%20combination%20of%20reasoning%0Amodules%20for%20scene%20description%2C%20scene%20analysis%2C%20and%20hierarchical%20planning.%0AFurthermore%2C%20recognizing%20the%20limitations%20of%20VLMs%20in%20spatial%20reasoning%20and%20heavy%0Acomputational%20requirements%2C%20we%20propose%20DriveVLM-Dual%2C%20a%20hybrid%20system%20that%0Asynergizes%20the%20strengths%20of%20DriveVLM%20with%20the%20traditional%20autonomous%20driving%0Apipeline.%20Experiments%20on%20both%20the%20nuScenes%20dataset%20and%20our%20SUP-AD%20dataset%0Ademonstrate%20the%20efficacy%20of%20DriveVLM%20and%20DriveVLM-Dual%20in%20handling%20complex%20and%0Aunpredictable%20driving%20conditions.%20Finally%2C%20we%20deploy%20the%20DriveVLM-Dual%20on%20a%0Aproduction%20vehicle%2C%20verifying%20it%20is%20effective%20in%20real-world%20autonomous%20driving%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12289v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriveVLM%253A%2520The%2520Convergence%2520of%2520Autonomous%2520Driving%2520and%2520Large%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DXiaoyu%2520Tian%2520and%2520Junru%2520Gu%2520and%2520Bailin%2520Li%2520and%2520Yicheng%2520Liu%2520and%2520Yang%2520Wang%2520and%2520Zhiyong%2520Zhao%2520and%2520Kun%2520Zhan%2520and%2520Peng%2520Jia%2520and%2520Xianpeng%2520Lang%2520and%2520Hang%2520Zhao%26entry.1292438233%3D%2520%2520A%2520primary%2520hurdle%2520of%2520autonomous%2520driving%2520in%2520urban%2520environments%2520is%2520understanding%250Acomplex%2520and%2520long-tail%2520scenarios%252C%2520such%2520as%2520challenging%2520road%2520conditions%2520and%250Adelicate%2520human%2520behaviors.%2520We%2520introduce%2520DriveVLM%252C%2520an%2520autonomous%2520driving%2520system%250Aleveraging%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520for%2520enhanced%2520scene%2520understanding%2520and%250Aplanning%2520capabilities.%2520DriveVLM%2520integrates%2520a%2520unique%2520combination%2520of%2520reasoning%250Amodules%2520for%2520scene%2520description%252C%2520scene%2520analysis%252C%2520and%2520hierarchical%2520planning.%250AFurthermore%252C%2520recognizing%2520the%2520limitations%2520of%2520VLMs%2520in%2520spatial%2520reasoning%2520and%2520heavy%250Acomputational%2520requirements%252C%2520we%2520propose%2520DriveVLM-Dual%252C%2520a%2520hybrid%2520system%2520that%250Asynergizes%2520the%2520strengths%2520of%2520DriveVLM%2520with%2520the%2520traditional%2520autonomous%2520driving%250Apipeline.%2520Experiments%2520on%2520both%2520the%2520nuScenes%2520dataset%2520and%2520our%2520SUP-AD%2520dataset%250Ademonstrate%2520the%2520efficacy%2520of%2520DriveVLM%2520and%2520DriveVLM-Dual%2520in%2520handling%2520complex%2520and%250Aunpredictable%2520driving%2520conditions.%2520Finally%252C%2520we%2520deploy%2520the%2520DriveVLM-Dual%2520on%2520a%250Aproduction%2520vehicle%252C%2520verifying%2520it%2520is%2520effective%2520in%2520real-world%2520autonomous%2520driving%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12289v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DriveVLM%3A%20The%20Convergence%20of%20Autonomous%20Driving%20and%20Large%0A%20%20Vision-Language%20Models&entry.906535625=Xiaoyu%20Tian%20and%20Junru%20Gu%20and%20Bailin%20Li%20and%20Yicheng%20Liu%20and%20Yang%20Wang%20and%20Zhiyong%20Zhao%20and%20Kun%20Zhan%20and%20Peng%20Jia%20and%20Xianpeng%20Lang%20and%20Hang%20Zhao&entry.1292438233=%20%20A%20primary%20hurdle%20of%20autonomous%20driving%20in%20urban%20environments%20is%20understanding%0Acomplex%20and%20long-tail%20scenarios%2C%20such%20as%20challenging%20road%20conditions%20and%0Adelicate%20human%20behaviors.%20We%20introduce%20DriveVLM%2C%20an%20autonomous%20driving%20system%0Aleveraging%20Vision-Language%20Models%20%28VLMs%29%20for%20enhanced%20scene%20understanding%20and%0Aplanning%20capabilities.%20DriveVLM%20integrates%20a%20unique%20combination%20of%20reasoning%0Amodules%20for%20scene%20description%2C%20scene%20analysis%2C%20and%20hierarchical%20planning.%0AFurthermore%2C%20recognizing%20the%20limitations%20of%20VLMs%20in%20spatial%20reasoning%20and%20heavy%0Acomputational%20requirements%2C%20we%20propose%20DriveVLM-Dual%2C%20a%20hybrid%20system%20that%0Asynergizes%20the%20strengths%20of%20DriveVLM%20with%20the%20traditional%20autonomous%20driving%0Apipeline.%20Experiments%20on%20both%20the%20nuScenes%20dataset%20and%20our%20SUP-AD%20dataset%0Ademonstrate%20the%20efficacy%20of%20DriveVLM%20and%20DriveVLM-Dual%20in%20handling%20complex%20and%0Aunpredictable%20driving%20conditions.%20Finally%2C%20we%20deploy%20the%20DriveVLM-Dual%20on%20a%0Aproduction%20vehicle%2C%20verifying%20it%20is%20effective%20in%20real-world%20autonomous%20driving%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12289v4&entry.124074799=Read"},
{"title": "FASTC: A Fast Attentional Framework for Semantic Traversability\n  Classification Using Point Cloud", "author": "Yirui Chen and Pengjin Wei and Zhenhuan Liu and Bingchao Wang and Jie Yang and Wei Liu", "abstract": "  Producing traversability maps and understanding the surroundings are crucial\nprerequisites for autonomous navigation. In this paper, we address the problem\nof traversability assessment using point clouds. We propose a novel pillar\nfeature extraction module that utilizes PointNet to capture features from point\nclouds organized in vertical volume and a 2D encoder-decoder structure to\nconduct traversability classification instead of the widely used 3D\nconvolutions. This results in less computational cost while even better\nperformance is achieved at the same time. We then propose a new spatio-temporal\nattention module to fuse multi-frame information, which can properly handle the\nvarying density problem of LIDAR point clouds, and this makes our module able\nto assess distant areas more accurately. Comprehensive experimental results on\naugmented Semantic KITTI and RELLIS-3D datasets show that our method is able to\nachieve superior performance over existing approaches both quantitatively and\nquantitatively.\n", "link": "http://arxiv.org/abs/2406.16564v1", "date": "2024-06-24", "relevancy": 2.2142, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.565}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5487}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FASTC%3A%20A%20Fast%20Attentional%20Framework%20for%20Semantic%20Traversability%0A%20%20Classification%20Using%20Point%20Cloud&body=Title%3A%20FASTC%3A%20A%20Fast%20Attentional%20Framework%20for%20Semantic%20Traversability%0A%20%20Classification%20Using%20Point%20Cloud%0AAuthor%3A%20Yirui%20Chen%20and%20Pengjin%20Wei%20and%20Zhenhuan%20Liu%20and%20Bingchao%20Wang%20and%20Jie%20Yang%20and%20Wei%20Liu%0AAbstract%3A%20%20%20Producing%20traversability%20maps%20and%20understanding%20the%20surroundings%20are%20crucial%0Aprerequisites%20for%20autonomous%20navigation.%20In%20this%20paper%2C%20we%20address%20the%20problem%0Aof%20traversability%20assessment%20using%20point%20clouds.%20We%20propose%20a%20novel%20pillar%0Afeature%20extraction%20module%20that%20utilizes%20PointNet%20to%20capture%20features%20from%20point%0Aclouds%20organized%20in%20vertical%20volume%20and%20a%202D%20encoder-decoder%20structure%20to%0Aconduct%20traversability%20classification%20instead%20of%20the%20widely%20used%203D%0Aconvolutions.%20This%20results%20in%20less%20computational%20cost%20while%20even%20better%0Aperformance%20is%20achieved%20at%20the%20same%20time.%20We%20then%20propose%20a%20new%20spatio-temporal%0Aattention%20module%20to%20fuse%20multi-frame%20information%2C%20which%20can%20properly%20handle%20the%0Avarying%20density%20problem%20of%20LIDAR%20point%20clouds%2C%20and%20this%20makes%20our%20module%20able%0Ato%20assess%20distant%20areas%20more%20accurately.%20Comprehensive%20experimental%20results%20on%0Aaugmented%20Semantic%20KITTI%20and%20RELLIS-3D%20datasets%20show%20that%20our%20method%20is%20able%20to%0Aachieve%20superior%20performance%20over%20existing%20approaches%20both%20quantitatively%20and%0Aquantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFASTC%253A%2520A%2520Fast%2520Attentional%2520Framework%2520for%2520Semantic%2520Traversability%250A%2520%2520Classification%2520Using%2520Point%2520Cloud%26entry.906535625%3DYirui%2520Chen%2520and%2520Pengjin%2520Wei%2520and%2520Zhenhuan%2520Liu%2520and%2520Bingchao%2520Wang%2520and%2520Jie%2520Yang%2520and%2520Wei%2520Liu%26entry.1292438233%3D%2520%2520Producing%2520traversability%2520maps%2520and%2520understanding%2520the%2520surroundings%2520are%2520crucial%250Aprerequisites%2520for%2520autonomous%2520navigation.%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520problem%250Aof%2520traversability%2520assessment%2520using%2520point%2520clouds.%2520We%2520propose%2520a%2520novel%2520pillar%250Afeature%2520extraction%2520module%2520that%2520utilizes%2520PointNet%2520to%2520capture%2520features%2520from%2520point%250Aclouds%2520organized%2520in%2520vertical%2520volume%2520and%2520a%25202D%2520encoder-decoder%2520structure%2520to%250Aconduct%2520traversability%2520classification%2520instead%2520of%2520the%2520widely%2520used%25203D%250Aconvolutions.%2520This%2520results%2520in%2520less%2520computational%2520cost%2520while%2520even%2520better%250Aperformance%2520is%2520achieved%2520at%2520the%2520same%2520time.%2520We%2520then%2520propose%2520a%2520new%2520spatio-temporal%250Aattention%2520module%2520to%2520fuse%2520multi-frame%2520information%252C%2520which%2520can%2520properly%2520handle%2520the%250Avarying%2520density%2520problem%2520of%2520LIDAR%2520point%2520clouds%252C%2520and%2520this%2520makes%2520our%2520module%2520able%250Ato%2520assess%2520distant%2520areas%2520more%2520accurately.%2520Comprehensive%2520experimental%2520results%2520on%250Aaugmented%2520Semantic%2520KITTI%2520and%2520RELLIS-3D%2520datasets%2520show%2520that%2520our%2520method%2520is%2520able%2520to%250Aachieve%2520superior%2520performance%2520over%2520existing%2520approaches%2520both%2520quantitatively%2520and%250Aquantitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FASTC%3A%20A%20Fast%20Attentional%20Framework%20for%20Semantic%20Traversability%0A%20%20Classification%20Using%20Point%20Cloud&entry.906535625=Yirui%20Chen%20and%20Pengjin%20Wei%20and%20Zhenhuan%20Liu%20and%20Bingchao%20Wang%20and%20Jie%20Yang%20and%20Wei%20Liu&entry.1292438233=%20%20Producing%20traversability%20maps%20and%20understanding%20the%20surroundings%20are%20crucial%0Aprerequisites%20for%20autonomous%20navigation.%20In%20this%20paper%2C%20we%20address%20the%20problem%0Aof%20traversability%20assessment%20using%20point%20clouds.%20We%20propose%20a%20novel%20pillar%0Afeature%20extraction%20module%20that%20utilizes%20PointNet%20to%20capture%20features%20from%20point%0Aclouds%20organized%20in%20vertical%20volume%20and%20a%202D%20encoder-decoder%20structure%20to%0Aconduct%20traversability%20classification%20instead%20of%20the%20widely%20used%203D%0Aconvolutions.%20This%20results%20in%20less%20computational%20cost%20while%20even%20better%0Aperformance%20is%20achieved%20at%20the%20same%20time.%20We%20then%20propose%20a%20new%20spatio-temporal%0Aattention%20module%20to%20fuse%20multi-frame%20information%2C%20which%20can%20properly%20handle%20the%0Avarying%20density%20problem%20of%20LIDAR%20point%20clouds%2C%20and%20this%20makes%20our%20module%20able%0Ato%20assess%20distant%20areas%20more%20accurately.%20Comprehensive%20experimental%20results%20on%0Aaugmented%20Semantic%20KITTI%20and%20RELLIS-3D%20datasets%20show%20that%20our%20method%20is%20able%20to%0Aachieve%20superior%20performance%20over%20existing%20approaches%20both%20quantitatively%20and%0Aquantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16564v1&entry.124074799=Read"},
{"title": "Can Protective Perturbation Safeguard Personal Data from Being Exploited\n  by Stable Diffusion?", "author": "Zhengyue Zhao and Jinhao Duan and Kaidi Xu and Chenan Wang and Rui Zhang and Zidong Du and Qi Guo and Xing Hu", "abstract": "  Stable Diffusion has established itself as a foundation model in generative\nAI artistic applications, receiving widespread research and application. Some\nrecent fine-tuning methods have made it feasible for individuals to implant\npersonalized concepts onto the basic Stable Diffusion model with minimal\ncomputational costs on small datasets. However, these innovations have also\ngiven rise to issues like facial privacy forgery and artistic copyright\ninfringement. In recent studies, researchers have explored the addition of\nimperceptible adversarial perturbations to images to prevent potential\nunauthorized exploitation and infringements when personal data is used for\nfine-tuning Stable Diffusion. Although these studies have demonstrated the\nability to protect images, it is essential to consider that these methods may\nnot be entirely applicable in real-world scenarios. In this paper, we\nsystematically evaluate the use of perturbations to protect images within a\npractical threat model. The results suggest that these approaches may not be\nsufficient to safeguard image privacy and copyright effectively. Furthermore,\nwe introduce a purification method capable of removing protected perturbations\nwhile preserving the original image structure to the greatest extent possible.\nExperiments reveal that Stable Diffusion can effectively learn from purified\nimages over all protective methods.\n", "link": "http://arxiv.org/abs/2312.00084v2", "date": "2024-06-24", "relevancy": 2.213, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5548}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5526}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Protective%20Perturbation%20Safeguard%20Personal%20Data%20from%20Being%20Exploited%0A%20%20by%20Stable%20Diffusion%3F&body=Title%3A%20Can%20Protective%20Perturbation%20Safeguard%20Personal%20Data%20from%20Being%20Exploited%0A%20%20by%20Stable%20Diffusion%3F%0AAuthor%3A%20Zhengyue%20Zhao%20and%20Jinhao%20Duan%20and%20Kaidi%20Xu%20and%20Chenan%20Wang%20and%20Rui%20Zhang%20and%20Zidong%20Du%20and%20Qi%20Guo%20and%20Xing%20Hu%0AAbstract%3A%20%20%20Stable%20Diffusion%20has%20established%20itself%20as%20a%20foundation%20model%20in%20generative%0AAI%20artistic%20applications%2C%20receiving%20widespread%20research%20and%20application.%20Some%0Arecent%20fine-tuning%20methods%20have%20made%20it%20feasible%20for%20individuals%20to%20implant%0Apersonalized%20concepts%20onto%20the%20basic%20Stable%20Diffusion%20model%20with%20minimal%0Acomputational%20costs%20on%20small%20datasets.%20However%2C%20these%20innovations%20have%20also%0Agiven%20rise%20to%20issues%20like%20facial%20privacy%20forgery%20and%20artistic%20copyright%0Ainfringement.%20In%20recent%20studies%2C%20researchers%20have%20explored%20the%20addition%20of%0Aimperceptible%20adversarial%20perturbations%20to%20images%20to%20prevent%20potential%0Aunauthorized%20exploitation%20and%20infringements%20when%20personal%20data%20is%20used%20for%0Afine-tuning%20Stable%20Diffusion.%20Although%20these%20studies%20have%20demonstrated%20the%0Aability%20to%20protect%20images%2C%20it%20is%20essential%20to%20consider%20that%20these%20methods%20may%0Anot%20be%20entirely%20applicable%20in%20real-world%20scenarios.%20In%20this%20paper%2C%20we%0Asystematically%20evaluate%20the%20use%20of%20perturbations%20to%20protect%20images%20within%20a%0Apractical%20threat%20model.%20The%20results%20suggest%20that%20these%20approaches%20may%20not%20be%0Asufficient%20to%20safeguard%20image%20privacy%20and%20copyright%20effectively.%20Furthermore%2C%0Awe%20introduce%20a%20purification%20method%20capable%20of%20removing%20protected%20perturbations%0Awhile%20preserving%20the%20original%20image%20structure%20to%20the%20greatest%20extent%20possible.%0AExperiments%20reveal%20that%20Stable%20Diffusion%20can%20effectively%20learn%20from%20purified%0Aimages%20over%20all%20protective%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00084v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Protective%2520Perturbation%2520Safeguard%2520Personal%2520Data%2520from%2520Being%2520Exploited%250A%2520%2520by%2520Stable%2520Diffusion%253F%26entry.906535625%3DZhengyue%2520Zhao%2520and%2520Jinhao%2520Duan%2520and%2520Kaidi%2520Xu%2520and%2520Chenan%2520Wang%2520and%2520Rui%2520Zhang%2520and%2520Zidong%2520Du%2520and%2520Qi%2520Guo%2520and%2520Xing%2520Hu%26entry.1292438233%3D%2520%2520Stable%2520Diffusion%2520has%2520established%2520itself%2520as%2520a%2520foundation%2520model%2520in%2520generative%250AAI%2520artistic%2520applications%252C%2520receiving%2520widespread%2520research%2520and%2520application.%2520Some%250Arecent%2520fine-tuning%2520methods%2520have%2520made%2520it%2520feasible%2520for%2520individuals%2520to%2520implant%250Apersonalized%2520concepts%2520onto%2520the%2520basic%2520Stable%2520Diffusion%2520model%2520with%2520minimal%250Acomputational%2520costs%2520on%2520small%2520datasets.%2520However%252C%2520these%2520innovations%2520have%2520also%250Agiven%2520rise%2520to%2520issues%2520like%2520facial%2520privacy%2520forgery%2520and%2520artistic%2520copyright%250Ainfringement.%2520In%2520recent%2520studies%252C%2520researchers%2520have%2520explored%2520the%2520addition%2520of%250Aimperceptible%2520adversarial%2520perturbations%2520to%2520images%2520to%2520prevent%2520potential%250Aunauthorized%2520exploitation%2520and%2520infringements%2520when%2520personal%2520data%2520is%2520used%2520for%250Afine-tuning%2520Stable%2520Diffusion.%2520Although%2520these%2520studies%2520have%2520demonstrated%2520the%250Aability%2520to%2520protect%2520images%252C%2520it%2520is%2520essential%2520to%2520consider%2520that%2520these%2520methods%2520may%250Anot%2520be%2520entirely%2520applicable%2520in%2520real-world%2520scenarios.%2520In%2520this%2520paper%252C%2520we%250Asystematically%2520evaluate%2520the%2520use%2520of%2520perturbations%2520to%2520protect%2520images%2520within%2520a%250Apractical%2520threat%2520model.%2520The%2520results%2520suggest%2520that%2520these%2520approaches%2520may%2520not%2520be%250Asufficient%2520to%2520safeguard%2520image%2520privacy%2520and%2520copyright%2520effectively.%2520Furthermore%252C%250Awe%2520introduce%2520a%2520purification%2520method%2520capable%2520of%2520removing%2520protected%2520perturbations%250Awhile%2520preserving%2520the%2520original%2520image%2520structure%2520to%2520the%2520greatest%2520extent%2520possible.%250AExperiments%2520reveal%2520that%2520Stable%2520Diffusion%2520can%2520effectively%2520learn%2520from%2520purified%250Aimages%2520over%2520all%2520protective%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00084v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Protective%20Perturbation%20Safeguard%20Personal%20Data%20from%20Being%20Exploited%0A%20%20by%20Stable%20Diffusion%3F&entry.906535625=Zhengyue%20Zhao%20and%20Jinhao%20Duan%20and%20Kaidi%20Xu%20and%20Chenan%20Wang%20and%20Rui%20Zhang%20and%20Zidong%20Du%20and%20Qi%20Guo%20and%20Xing%20Hu&entry.1292438233=%20%20Stable%20Diffusion%20has%20established%20itself%20as%20a%20foundation%20model%20in%20generative%0AAI%20artistic%20applications%2C%20receiving%20widespread%20research%20and%20application.%20Some%0Arecent%20fine-tuning%20methods%20have%20made%20it%20feasible%20for%20individuals%20to%20implant%0Apersonalized%20concepts%20onto%20the%20basic%20Stable%20Diffusion%20model%20with%20minimal%0Acomputational%20costs%20on%20small%20datasets.%20However%2C%20these%20innovations%20have%20also%0Agiven%20rise%20to%20issues%20like%20facial%20privacy%20forgery%20and%20artistic%20copyright%0Ainfringement.%20In%20recent%20studies%2C%20researchers%20have%20explored%20the%20addition%20of%0Aimperceptible%20adversarial%20perturbations%20to%20images%20to%20prevent%20potential%0Aunauthorized%20exploitation%20and%20infringements%20when%20personal%20data%20is%20used%20for%0Afine-tuning%20Stable%20Diffusion.%20Although%20these%20studies%20have%20demonstrated%20the%0Aability%20to%20protect%20images%2C%20it%20is%20essential%20to%20consider%20that%20these%20methods%20may%0Anot%20be%20entirely%20applicable%20in%20real-world%20scenarios.%20In%20this%20paper%2C%20we%0Asystematically%20evaluate%20the%20use%20of%20perturbations%20to%20protect%20images%20within%20a%0Apractical%20threat%20model.%20The%20results%20suggest%20that%20these%20approaches%20may%20not%20be%0Asufficient%20to%20safeguard%20image%20privacy%20and%20copyright%20effectively.%20Furthermore%2C%0Awe%20introduce%20a%20purification%20method%20capable%20of%20removing%20protected%20perturbations%0Awhile%20preserving%20the%20original%20image%20structure%20to%20the%20greatest%20extent%20possible.%0AExperiments%20reveal%20that%20Stable%20Diffusion%20can%20effectively%20learn%20from%20purified%0Aimages%20over%20all%20protective%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00084v2&entry.124074799=Read"},
{"title": "LOGCAN++: Local-global class-aware network for semantic segmentation of\n  remote sensing images", "author": "Xiaowen Ma and Rongrong Lian and Zhenkai Wu and Hongbo Guo and Mengting Ma and Sensen Wu and Zhenhong Du and Siyang Song and Wei Zhang", "abstract": "  Remote sensing images usually characterized by complex backgrounds, scale and\norientation variations, and large intra-class variance. General semantic\nsegmentation methods usually fail to fully investigate the above issues, and\nthus their performances on remote sensing image segmentation are limited. In\nthis paper, we propose our LOGCAN++, a semantic segmentation model customized\nfor remote sensing images, which is made up of a Global Class Awareness (GCA)\nmodule and several Local Class Awareness (LCA) modules. The GCA module captures\nglobal representations for class-level context modeling to reduce the\ninterference of background noise. The LCA module generates local class\nrepresentations as intermediate perceptual elements to indirectly associate\npixels with the global class representations, targeting at dealing with the\nlarge intra-class variance problem. In particular, we introduce affine\ntransformations in the LCA module for adaptive extraction of local class\nrepresentations to effectively tolerate scale and orientation variations in\nremotely sensed images. Extensive experiments on three benchmark datasets show\nthat our LOGCAN++ outperforms current mainstream general and remote sensing\nsemantic segmentation methods and achieves a better trade-off between speed and\naccuracy. Code is available at https://github.com/xwmaxwma/rssegmentation.\n", "link": "http://arxiv.org/abs/2406.16502v1", "date": "2024-06-24", "relevancy": 2.2106, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5743}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5429}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LOGCAN%2B%2B%3A%20Local-global%20class-aware%20network%20for%20semantic%20segmentation%20of%0A%20%20remote%20sensing%20images&body=Title%3A%20LOGCAN%2B%2B%3A%20Local-global%20class-aware%20network%20for%20semantic%20segmentation%20of%0A%20%20remote%20sensing%20images%0AAuthor%3A%20Xiaowen%20Ma%20and%20Rongrong%20Lian%20and%20Zhenkai%20Wu%20and%20Hongbo%20Guo%20and%20Mengting%20Ma%20and%20Sensen%20Wu%20and%20Zhenhong%20Du%20and%20Siyang%20Song%20and%20Wei%20Zhang%0AAbstract%3A%20%20%20Remote%20sensing%20images%20usually%20characterized%20by%20complex%20backgrounds%2C%20scale%20and%0Aorientation%20variations%2C%20and%20large%20intra-class%20variance.%20General%20semantic%0Asegmentation%20methods%20usually%20fail%20to%20fully%20investigate%20the%20above%20issues%2C%20and%0Athus%20their%20performances%20on%20remote%20sensing%20image%20segmentation%20are%20limited.%20In%0Athis%20paper%2C%20we%20propose%20our%20LOGCAN%2B%2B%2C%20a%20semantic%20segmentation%20model%20customized%0Afor%20remote%20sensing%20images%2C%20which%20is%20made%20up%20of%20a%20Global%20Class%20Awareness%20%28GCA%29%0Amodule%20and%20several%20Local%20Class%20Awareness%20%28LCA%29%20modules.%20The%20GCA%20module%20captures%0Aglobal%20representations%20for%20class-level%20context%20modeling%20to%20reduce%20the%0Ainterference%20of%20background%20noise.%20The%20LCA%20module%20generates%20local%20class%0Arepresentations%20as%20intermediate%20perceptual%20elements%20to%20indirectly%20associate%0Apixels%20with%20the%20global%20class%20representations%2C%20targeting%20at%20dealing%20with%20the%0Alarge%20intra-class%20variance%20problem.%20In%20particular%2C%20we%20introduce%20affine%0Atransformations%20in%20the%20LCA%20module%20for%20adaptive%20extraction%20of%20local%20class%0Arepresentations%20to%20effectively%20tolerate%20scale%20and%20orientation%20variations%20in%0Aremotely%20sensed%20images.%20Extensive%20experiments%20on%20three%20benchmark%20datasets%20show%0Athat%20our%20LOGCAN%2B%2B%20outperforms%20current%20mainstream%20general%20and%20remote%20sensing%0Asemantic%20segmentation%20methods%20and%20achieves%20a%20better%20trade-off%20between%20speed%20and%0Aaccuracy.%20Code%20is%20available%20at%20https%3A//github.com/xwmaxwma/rssegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16502v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLOGCAN%252B%252B%253A%2520Local-global%2520class-aware%2520network%2520for%2520semantic%2520segmentation%2520of%250A%2520%2520remote%2520sensing%2520images%26entry.906535625%3DXiaowen%2520Ma%2520and%2520Rongrong%2520Lian%2520and%2520Zhenkai%2520Wu%2520and%2520Hongbo%2520Guo%2520and%2520Mengting%2520Ma%2520and%2520Sensen%2520Wu%2520and%2520Zhenhong%2520Du%2520and%2520Siyang%2520Song%2520and%2520Wei%2520Zhang%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520images%2520usually%2520characterized%2520by%2520complex%2520backgrounds%252C%2520scale%2520and%250Aorientation%2520variations%252C%2520and%2520large%2520intra-class%2520variance.%2520General%2520semantic%250Asegmentation%2520methods%2520usually%2520fail%2520to%2520fully%2520investigate%2520the%2520above%2520issues%252C%2520and%250Athus%2520their%2520performances%2520on%2520remote%2520sensing%2520image%2520segmentation%2520are%2520limited.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520our%2520LOGCAN%252B%252B%252C%2520a%2520semantic%2520segmentation%2520model%2520customized%250Afor%2520remote%2520sensing%2520images%252C%2520which%2520is%2520made%2520up%2520of%2520a%2520Global%2520Class%2520Awareness%2520%2528GCA%2529%250Amodule%2520and%2520several%2520Local%2520Class%2520Awareness%2520%2528LCA%2529%2520modules.%2520The%2520GCA%2520module%2520captures%250Aglobal%2520representations%2520for%2520class-level%2520context%2520modeling%2520to%2520reduce%2520the%250Ainterference%2520of%2520background%2520noise.%2520The%2520LCA%2520module%2520generates%2520local%2520class%250Arepresentations%2520as%2520intermediate%2520perceptual%2520elements%2520to%2520indirectly%2520associate%250Apixels%2520with%2520the%2520global%2520class%2520representations%252C%2520targeting%2520at%2520dealing%2520with%2520the%250Alarge%2520intra-class%2520variance%2520problem.%2520In%2520particular%252C%2520we%2520introduce%2520affine%250Atransformations%2520in%2520the%2520LCA%2520module%2520for%2520adaptive%2520extraction%2520of%2520local%2520class%250Arepresentations%2520to%2520effectively%2520tolerate%2520scale%2520and%2520orientation%2520variations%2520in%250Aremotely%2520sensed%2520images.%2520Extensive%2520experiments%2520on%2520three%2520benchmark%2520datasets%2520show%250Athat%2520our%2520LOGCAN%252B%252B%2520outperforms%2520current%2520mainstream%2520general%2520and%2520remote%2520sensing%250Asemantic%2520segmentation%2520methods%2520and%2520achieves%2520a%2520better%2520trade-off%2520between%2520speed%2520and%250Aaccuracy.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/xwmaxwma/rssegmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16502v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LOGCAN%2B%2B%3A%20Local-global%20class-aware%20network%20for%20semantic%20segmentation%20of%0A%20%20remote%20sensing%20images&entry.906535625=Xiaowen%20Ma%20and%20Rongrong%20Lian%20and%20Zhenkai%20Wu%20and%20Hongbo%20Guo%20and%20Mengting%20Ma%20and%20Sensen%20Wu%20and%20Zhenhong%20Du%20and%20Siyang%20Song%20and%20Wei%20Zhang&entry.1292438233=%20%20Remote%20sensing%20images%20usually%20characterized%20by%20complex%20backgrounds%2C%20scale%20and%0Aorientation%20variations%2C%20and%20large%20intra-class%20variance.%20General%20semantic%0Asegmentation%20methods%20usually%20fail%20to%20fully%20investigate%20the%20above%20issues%2C%20and%0Athus%20their%20performances%20on%20remote%20sensing%20image%20segmentation%20are%20limited.%20In%0Athis%20paper%2C%20we%20propose%20our%20LOGCAN%2B%2B%2C%20a%20semantic%20segmentation%20model%20customized%0Afor%20remote%20sensing%20images%2C%20which%20is%20made%20up%20of%20a%20Global%20Class%20Awareness%20%28GCA%29%0Amodule%20and%20several%20Local%20Class%20Awareness%20%28LCA%29%20modules.%20The%20GCA%20module%20captures%0Aglobal%20representations%20for%20class-level%20context%20modeling%20to%20reduce%20the%0Ainterference%20of%20background%20noise.%20The%20LCA%20module%20generates%20local%20class%0Arepresentations%20as%20intermediate%20perceptual%20elements%20to%20indirectly%20associate%0Apixels%20with%20the%20global%20class%20representations%2C%20targeting%20at%20dealing%20with%20the%0Alarge%20intra-class%20variance%20problem.%20In%20particular%2C%20we%20introduce%20affine%0Atransformations%20in%20the%20LCA%20module%20for%20adaptive%20extraction%20of%20local%20class%0Arepresentations%20to%20effectively%20tolerate%20scale%20and%20orientation%20variations%20in%0Aremotely%20sensed%20images.%20Extensive%20experiments%20on%20three%20benchmark%20datasets%20show%0Athat%20our%20LOGCAN%2B%2B%20outperforms%20current%20mainstream%20general%20and%20remote%20sensing%0Asemantic%20segmentation%20methods%20and%20achieves%20a%20better%20trade-off%20between%20speed%20and%0Aaccuracy.%20Code%20is%20available%20at%20https%3A//github.com/xwmaxwma/rssegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16502v1&entry.124074799=Read"},
{"title": "Dynamic Pseudo Label Optimization in Point-Supervised Nuclei\n  Segmentation", "author": "Ziyue Wang and Ye Zhang and Yifeng Wang and Linghan Cai and Yongbing Zhang", "abstract": "  Deep learning has achieved impressive results in nuclei segmentation, but the\nmassive requirement for pixel-wise labels remains a significant challenge. To\nalleviate the annotation burden, existing methods generate pseudo masks for\nmodel training using point labels. However, the generated masks are inevitably\ndifferent from the ground truth, and these dissimilarities are not handled\nreasonably during the network training, resulting in the subpar performance of\nthe segmentation model. To tackle this issue, we propose a framework named\nDoNuSeg, enabling \\textbf{D}ynamic pseudo label \\textbf{O}ptimization in\npoint-supervised \\textbf{Nu}clei \\textbf{Seg}mentation. Specifically, DoNuSeg\ntakes advantage of class activation maps (CAMs) to adaptively capture regions\nwith semantics similar to annotated points. To leverage semantic diversity in\nthe hierarchical feature levels, we design a dynamic selection module to choose\nthe optimal one among CAMs from different encoder blocks as pseudo masks.\nMeanwhile, a CAM-guided contrastive module is proposed to further enhance the\naccuracy of pseudo masks. In addition to exploiting the semantic information\nprovided by CAMs, we consider location priors inherent to point labels,\ndeveloping a task-decoupled structure for effectively differentiating nuclei.\nExtensive experiments demonstrate that DoNuSeg outperforms state-of-the-art\npoint-supervised methods. The code is available at\nhttps://github.com/shinning0821/MICCAI24-DoNuSeg.\n", "link": "http://arxiv.org/abs/2406.16427v1", "date": "2024-06-24", "relevancy": 2.2106, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5597}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5572}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Pseudo%20Label%20Optimization%20in%20Point-Supervised%20Nuclei%0A%20%20Segmentation&body=Title%3A%20Dynamic%20Pseudo%20Label%20Optimization%20in%20Point-Supervised%20Nuclei%0A%20%20Segmentation%0AAuthor%3A%20Ziyue%20Wang%20and%20Ye%20Zhang%20and%20Yifeng%20Wang%20and%20Linghan%20Cai%20and%20Yongbing%20Zhang%0AAbstract%3A%20%20%20Deep%20learning%20has%20achieved%20impressive%20results%20in%20nuclei%20segmentation%2C%20but%20the%0Amassive%20requirement%20for%20pixel-wise%20labels%20remains%20a%20significant%20challenge.%20To%0Aalleviate%20the%20annotation%20burden%2C%20existing%20methods%20generate%20pseudo%20masks%20for%0Amodel%20training%20using%20point%20labels.%20However%2C%20the%20generated%20masks%20are%20inevitably%0Adifferent%20from%20the%20ground%20truth%2C%20and%20these%20dissimilarities%20are%20not%20handled%0Areasonably%20during%20the%20network%20training%2C%20resulting%20in%20the%20subpar%20performance%20of%0Athe%20segmentation%20model.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%20framework%20named%0ADoNuSeg%2C%20enabling%20%5Ctextbf%7BD%7Dynamic%20pseudo%20label%20%5Ctextbf%7BO%7Dptimization%20in%0Apoint-supervised%20%5Ctextbf%7BNu%7Dclei%20%5Ctextbf%7BSeg%7Dmentation.%20Specifically%2C%20DoNuSeg%0Atakes%20advantage%20of%20class%20activation%20maps%20%28CAMs%29%20to%20adaptively%20capture%20regions%0Awith%20semantics%20similar%20to%20annotated%20points.%20To%20leverage%20semantic%20diversity%20in%0Athe%20hierarchical%20feature%20levels%2C%20we%20design%20a%20dynamic%20selection%20module%20to%20choose%0Athe%20optimal%20one%20among%20CAMs%20from%20different%20encoder%20blocks%20as%20pseudo%20masks.%0AMeanwhile%2C%20a%20CAM-guided%20contrastive%20module%20is%20proposed%20to%20further%20enhance%20the%0Aaccuracy%20of%20pseudo%20masks.%20In%20addition%20to%20exploiting%20the%20semantic%20information%0Aprovided%20by%20CAMs%2C%20we%20consider%20location%20priors%20inherent%20to%20point%20labels%2C%0Adeveloping%20a%20task-decoupled%20structure%20for%20effectively%20differentiating%20nuclei.%0AExtensive%20experiments%20demonstrate%20that%20DoNuSeg%20outperforms%20state-of-the-art%0Apoint-supervised%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/shinning0821/MICCAI24-DoNuSeg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Pseudo%2520Label%2520Optimization%2520in%2520Point-Supervised%2520Nuclei%250A%2520%2520Segmentation%26entry.906535625%3DZiyue%2520Wang%2520and%2520Ye%2520Zhang%2520and%2520Yifeng%2520Wang%2520and%2520Linghan%2520Cai%2520and%2520Yongbing%2520Zhang%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520achieved%2520impressive%2520results%2520in%2520nuclei%2520segmentation%252C%2520but%2520the%250Amassive%2520requirement%2520for%2520pixel-wise%2520labels%2520remains%2520a%2520significant%2520challenge.%2520To%250Aalleviate%2520the%2520annotation%2520burden%252C%2520existing%2520methods%2520generate%2520pseudo%2520masks%2520for%250Amodel%2520training%2520using%2520point%2520labels.%2520However%252C%2520the%2520generated%2520masks%2520are%2520inevitably%250Adifferent%2520from%2520the%2520ground%2520truth%252C%2520and%2520these%2520dissimilarities%2520are%2520not%2520handled%250Areasonably%2520during%2520the%2520network%2520training%252C%2520resulting%2520in%2520the%2520subpar%2520performance%2520of%250Athe%2520segmentation%2520model.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520propose%2520a%2520framework%2520named%250ADoNuSeg%252C%2520enabling%2520%255Ctextbf%257BD%257Dynamic%2520pseudo%2520label%2520%255Ctextbf%257BO%257Dptimization%2520in%250Apoint-supervised%2520%255Ctextbf%257BNu%257Dclei%2520%255Ctextbf%257BSeg%257Dmentation.%2520Specifically%252C%2520DoNuSeg%250Atakes%2520advantage%2520of%2520class%2520activation%2520maps%2520%2528CAMs%2529%2520to%2520adaptively%2520capture%2520regions%250Awith%2520semantics%2520similar%2520to%2520annotated%2520points.%2520To%2520leverage%2520semantic%2520diversity%2520in%250Athe%2520hierarchical%2520feature%2520levels%252C%2520we%2520design%2520a%2520dynamic%2520selection%2520module%2520to%2520choose%250Athe%2520optimal%2520one%2520among%2520CAMs%2520from%2520different%2520encoder%2520blocks%2520as%2520pseudo%2520masks.%250AMeanwhile%252C%2520a%2520CAM-guided%2520contrastive%2520module%2520is%2520proposed%2520to%2520further%2520enhance%2520the%250Aaccuracy%2520of%2520pseudo%2520masks.%2520In%2520addition%2520to%2520exploiting%2520the%2520semantic%2520information%250Aprovided%2520by%2520CAMs%252C%2520we%2520consider%2520location%2520priors%2520inherent%2520to%2520point%2520labels%252C%250Adeveloping%2520a%2520task-decoupled%2520structure%2520for%2520effectively%2520differentiating%2520nuclei.%250AExtensive%2520experiments%2520demonstrate%2520that%2520DoNuSeg%2520outperforms%2520state-of-the-art%250Apoint-supervised%2520methods.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/shinning0821/MICCAI24-DoNuSeg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Pseudo%20Label%20Optimization%20in%20Point-Supervised%20Nuclei%0A%20%20Segmentation&entry.906535625=Ziyue%20Wang%20and%20Ye%20Zhang%20and%20Yifeng%20Wang%20and%20Linghan%20Cai%20and%20Yongbing%20Zhang&entry.1292438233=%20%20Deep%20learning%20has%20achieved%20impressive%20results%20in%20nuclei%20segmentation%2C%20but%20the%0Amassive%20requirement%20for%20pixel-wise%20labels%20remains%20a%20significant%20challenge.%20To%0Aalleviate%20the%20annotation%20burden%2C%20existing%20methods%20generate%20pseudo%20masks%20for%0Amodel%20training%20using%20point%20labels.%20However%2C%20the%20generated%20masks%20are%20inevitably%0Adifferent%20from%20the%20ground%20truth%2C%20and%20these%20dissimilarities%20are%20not%20handled%0Areasonably%20during%20the%20network%20training%2C%20resulting%20in%20the%20subpar%20performance%20of%0Athe%20segmentation%20model.%20To%20tackle%20this%20issue%2C%20we%20propose%20a%20framework%20named%0ADoNuSeg%2C%20enabling%20%5Ctextbf%7BD%7Dynamic%20pseudo%20label%20%5Ctextbf%7BO%7Dptimization%20in%0Apoint-supervised%20%5Ctextbf%7BNu%7Dclei%20%5Ctextbf%7BSeg%7Dmentation.%20Specifically%2C%20DoNuSeg%0Atakes%20advantage%20of%20class%20activation%20maps%20%28CAMs%29%20to%20adaptively%20capture%20regions%0Awith%20semantics%20similar%20to%20annotated%20points.%20To%20leverage%20semantic%20diversity%20in%0Athe%20hierarchical%20feature%20levels%2C%20we%20design%20a%20dynamic%20selection%20module%20to%20choose%0Athe%20optimal%20one%20among%20CAMs%20from%20different%20encoder%20blocks%20as%20pseudo%20masks.%0AMeanwhile%2C%20a%20CAM-guided%20contrastive%20module%20is%20proposed%20to%20further%20enhance%20the%0Aaccuracy%20of%20pseudo%20masks.%20In%20addition%20to%20exploiting%20the%20semantic%20information%0Aprovided%20by%20CAMs%2C%20we%20consider%20location%20priors%20inherent%20to%20point%20labels%2C%0Adeveloping%20a%20task-decoupled%20structure%20for%20effectively%20differentiating%20nuclei.%0AExtensive%20experiments%20demonstrate%20that%20DoNuSeg%20outperforms%20state-of-the-art%0Apoint-supervised%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/shinning0821/MICCAI24-DoNuSeg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16427v1&entry.124074799=Read"},
{"title": "A Certifiable Algorithm for Simultaneous Shape Estimation and Object\n  Tracking", "author": "Lorenzo Shaikewitz and Samuel Ubellacker and Luca Carlone", "abstract": "  Applications from manipulation to autonomous vehicles rely on robust and\ngeneral object tracking to safely perform tasks in dynamic environments. We\npropose the first certifiably optimal category-level approach for simultaneous\nshape estimation and pose tracking of an object of known category (e.g. a car).\nOur approach uses 3D semantic keypoint measurements extracted from an RGB-D\nimage sequence, and phrases the estimation as a fixed-lag smoothing problem.\nTemporal constraints enforce the object's rigidity (fixed shape) and smooth\nmotion according to a constant-twist motion model. The solutions to this\nproblem are the estimates of the object's state (poses, velocities) and shape\n(paramaterized according to the active shape model) over the smoothing horizon.\nOur key contribution is to show that despite the non-convexity of the fixed-lag\nsmoothing problem, we can solve it to certifiable optimality using a small-size\nsemidefinite relaxation. We also present a fast outlier rejection scheme that\nfilters out incorrect keypoint detections with shape and time compatibility\ntests, and wrap our certifiable solver in a graduated non-convexity scheme. We\nevaluate the proposed approach on synthetic and real data, showcasing its\nperformance in a table-top manipulation scenario and a drone-based vehicle\ntracking application.\n", "link": "http://arxiv.org/abs/2406.16837v1", "date": "2024-06-24", "relevancy": 2.202, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5715}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5477}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Certifiable%20Algorithm%20for%20Simultaneous%20Shape%20Estimation%20and%20Object%0A%20%20Tracking&body=Title%3A%20A%20Certifiable%20Algorithm%20for%20Simultaneous%20Shape%20Estimation%20and%20Object%0A%20%20Tracking%0AAuthor%3A%20Lorenzo%20Shaikewitz%20and%20Samuel%20Ubellacker%20and%20Luca%20Carlone%0AAbstract%3A%20%20%20Applications%20from%20manipulation%20to%20autonomous%20vehicles%20rely%20on%20robust%20and%0Ageneral%20object%20tracking%20to%20safely%20perform%20tasks%20in%20dynamic%20environments.%20We%0Apropose%20the%20first%20certifiably%20optimal%20category-level%20approach%20for%20simultaneous%0Ashape%20estimation%20and%20pose%20tracking%20of%20an%20object%20of%20known%20category%20%28e.g.%20a%20car%29.%0AOur%20approach%20uses%203D%20semantic%20keypoint%20measurements%20extracted%20from%20an%20RGB-D%0Aimage%20sequence%2C%20and%20phrases%20the%20estimation%20as%20a%20fixed-lag%20smoothing%20problem.%0ATemporal%20constraints%20enforce%20the%20object%27s%20rigidity%20%28fixed%20shape%29%20and%20smooth%0Amotion%20according%20to%20a%20constant-twist%20motion%20model.%20The%20solutions%20to%20this%0Aproblem%20are%20the%20estimates%20of%20the%20object%27s%20state%20%28poses%2C%20velocities%29%20and%20shape%0A%28paramaterized%20according%20to%20the%20active%20shape%20model%29%20over%20the%20smoothing%20horizon.%0AOur%20key%20contribution%20is%20to%20show%20that%20despite%20the%20non-convexity%20of%20the%20fixed-lag%0Asmoothing%20problem%2C%20we%20can%20solve%20it%20to%20certifiable%20optimality%20using%20a%20small-size%0Asemidefinite%20relaxation.%20We%20also%20present%20a%20fast%20outlier%20rejection%20scheme%20that%0Afilters%20out%20incorrect%20keypoint%20detections%20with%20shape%20and%20time%20compatibility%0Atests%2C%20and%20wrap%20our%20certifiable%20solver%20in%20a%20graduated%20non-convexity%20scheme.%20We%0Aevaluate%20the%20proposed%20approach%20on%20synthetic%20and%20real%20data%2C%20showcasing%20its%0Aperformance%20in%20a%20table-top%20manipulation%20scenario%20and%20a%20drone-based%20vehicle%0Atracking%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Certifiable%2520Algorithm%2520for%2520Simultaneous%2520Shape%2520Estimation%2520and%2520Object%250A%2520%2520Tracking%26entry.906535625%3DLorenzo%2520Shaikewitz%2520and%2520Samuel%2520Ubellacker%2520and%2520Luca%2520Carlone%26entry.1292438233%3D%2520%2520Applications%2520from%2520manipulation%2520to%2520autonomous%2520vehicles%2520rely%2520on%2520robust%2520and%250Ageneral%2520object%2520tracking%2520to%2520safely%2520perform%2520tasks%2520in%2520dynamic%2520environments.%2520We%250Apropose%2520the%2520first%2520certifiably%2520optimal%2520category-level%2520approach%2520for%2520simultaneous%250Ashape%2520estimation%2520and%2520pose%2520tracking%2520of%2520an%2520object%2520of%2520known%2520category%2520%2528e.g.%2520a%2520car%2529.%250AOur%2520approach%2520uses%25203D%2520semantic%2520keypoint%2520measurements%2520extracted%2520from%2520an%2520RGB-D%250Aimage%2520sequence%252C%2520and%2520phrases%2520the%2520estimation%2520as%2520a%2520fixed-lag%2520smoothing%2520problem.%250ATemporal%2520constraints%2520enforce%2520the%2520object%2527s%2520rigidity%2520%2528fixed%2520shape%2529%2520and%2520smooth%250Amotion%2520according%2520to%2520a%2520constant-twist%2520motion%2520model.%2520The%2520solutions%2520to%2520this%250Aproblem%2520are%2520the%2520estimates%2520of%2520the%2520object%2527s%2520state%2520%2528poses%252C%2520velocities%2529%2520and%2520shape%250A%2528paramaterized%2520according%2520to%2520the%2520active%2520shape%2520model%2529%2520over%2520the%2520smoothing%2520horizon.%250AOur%2520key%2520contribution%2520is%2520to%2520show%2520that%2520despite%2520the%2520non-convexity%2520of%2520the%2520fixed-lag%250Asmoothing%2520problem%252C%2520we%2520can%2520solve%2520it%2520to%2520certifiable%2520optimality%2520using%2520a%2520small-size%250Asemidefinite%2520relaxation.%2520We%2520also%2520present%2520a%2520fast%2520outlier%2520rejection%2520scheme%2520that%250Afilters%2520out%2520incorrect%2520keypoint%2520detections%2520with%2520shape%2520and%2520time%2520compatibility%250Atests%252C%2520and%2520wrap%2520our%2520certifiable%2520solver%2520in%2520a%2520graduated%2520non-convexity%2520scheme.%2520We%250Aevaluate%2520the%2520proposed%2520approach%2520on%2520synthetic%2520and%2520real%2520data%252C%2520showcasing%2520its%250Aperformance%2520in%2520a%2520table-top%2520manipulation%2520scenario%2520and%2520a%2520drone-based%2520vehicle%250Atracking%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Certifiable%20Algorithm%20for%20Simultaneous%20Shape%20Estimation%20and%20Object%0A%20%20Tracking&entry.906535625=Lorenzo%20Shaikewitz%20and%20Samuel%20Ubellacker%20and%20Luca%20Carlone&entry.1292438233=%20%20Applications%20from%20manipulation%20to%20autonomous%20vehicles%20rely%20on%20robust%20and%0Ageneral%20object%20tracking%20to%20safely%20perform%20tasks%20in%20dynamic%20environments.%20We%0Apropose%20the%20first%20certifiably%20optimal%20category-level%20approach%20for%20simultaneous%0Ashape%20estimation%20and%20pose%20tracking%20of%20an%20object%20of%20known%20category%20%28e.g.%20a%20car%29.%0AOur%20approach%20uses%203D%20semantic%20keypoint%20measurements%20extracted%20from%20an%20RGB-D%0Aimage%20sequence%2C%20and%20phrases%20the%20estimation%20as%20a%20fixed-lag%20smoothing%20problem.%0ATemporal%20constraints%20enforce%20the%20object%27s%20rigidity%20%28fixed%20shape%29%20and%20smooth%0Amotion%20according%20to%20a%20constant-twist%20motion%20model.%20The%20solutions%20to%20this%0Aproblem%20are%20the%20estimates%20of%20the%20object%27s%20state%20%28poses%2C%20velocities%29%20and%20shape%0A%28paramaterized%20according%20to%20the%20active%20shape%20model%29%20over%20the%20smoothing%20horizon.%0AOur%20key%20contribution%20is%20to%20show%20that%20despite%20the%20non-convexity%20of%20the%20fixed-lag%0Asmoothing%20problem%2C%20we%20can%20solve%20it%20to%20certifiable%20optimality%20using%20a%20small-size%0Asemidefinite%20relaxation.%20We%20also%20present%20a%20fast%20outlier%20rejection%20scheme%20that%0Afilters%20out%20incorrect%20keypoint%20detections%20with%20shape%20and%20time%20compatibility%0Atests%2C%20and%20wrap%20our%20certifiable%20solver%20in%20a%20graduated%20non-convexity%20scheme.%20We%0Aevaluate%20the%20proposed%20approach%20on%20synthetic%20and%20real%20data%2C%20showcasing%20its%0Aperformance%20in%20a%20table-top%20manipulation%20scenario%20and%20a%20drone-based%20vehicle%0Atracking%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16837v1&entry.124074799=Read"},
{"title": "Priorformer: A UGC-VQA Method with content and distortion priors", "author": "Yajing Pei and Shiyu Huang and Yiting Lu and Xin Li and Zhibo Chen", "abstract": "  User Generated Content (UGC) videos are susceptible to complicated and\nvariant degradations and contents, which prevents the existing blind video\nquality assessment (BVQA) models from good performance since the lack of the\nadapability of distortions and contents. To mitigate this, we propose a novel\nprior-augmented perceptual vision transformer (PriorFormer) for the BVQA of\nUGC, which boots its adaptability and representation capability for divergent\ncontents and distortions. Concretely, we introduce two powerful priors, i.e.,\nthe content and distortion priors, by extracting the content and distortion\nembeddings from two pre-trained feature extractors. Then we adopt these two\npowerful embeddings as the adaptive prior tokens, which are transferred to the\nvision transformer backbone jointly with implicit quality features. Based on\nthe above strategy, the proposed PriorFormer achieves state-of-the-art\nperformance on three public UGC VQA datasets including KoNViD-1K, LIVE-VQC and\nYouTube-UGC.\n", "link": "http://arxiv.org/abs/2406.16297v1", "date": "2024-06-24", "relevancy": 2.1902, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5497}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5466}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Priorformer%3A%20A%20UGC-VQA%20Method%20with%20content%20and%20distortion%20priors&body=Title%3A%20Priorformer%3A%20A%20UGC-VQA%20Method%20with%20content%20and%20distortion%20priors%0AAuthor%3A%20Yajing%20Pei%20and%20Shiyu%20Huang%20and%20Yiting%20Lu%20and%20Xin%20Li%20and%20Zhibo%20Chen%0AAbstract%3A%20%20%20User%20Generated%20Content%20%28UGC%29%20videos%20are%20susceptible%20to%20complicated%20and%0Avariant%20degradations%20and%20contents%2C%20which%20prevents%20the%20existing%20blind%20video%0Aquality%20assessment%20%28BVQA%29%20models%20from%20good%20performance%20since%20the%20lack%20of%20the%0Aadapability%20of%20distortions%20and%20contents.%20To%20mitigate%20this%2C%20we%20propose%20a%20novel%0Aprior-augmented%20perceptual%20vision%20transformer%20%28PriorFormer%29%20for%20the%20BVQA%20of%0AUGC%2C%20which%20boots%20its%20adaptability%20and%20representation%20capability%20for%20divergent%0Acontents%20and%20distortions.%20Concretely%2C%20we%20introduce%20two%20powerful%20priors%2C%20i.e.%2C%0Athe%20content%20and%20distortion%20priors%2C%20by%20extracting%20the%20content%20and%20distortion%0Aembeddings%20from%20two%20pre-trained%20feature%20extractors.%20Then%20we%20adopt%20these%20two%0Apowerful%20embeddings%20as%20the%20adaptive%20prior%20tokens%2C%20which%20are%20transferred%20to%20the%0Avision%20transformer%20backbone%20jointly%20with%20implicit%20quality%20features.%20Based%20on%0Athe%20above%20strategy%2C%20the%20proposed%20PriorFormer%20achieves%20state-of-the-art%0Aperformance%20on%20three%20public%20UGC%20VQA%20datasets%20including%20KoNViD-1K%2C%20LIVE-VQC%20and%0AYouTube-UGC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16297v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPriorformer%253A%2520A%2520UGC-VQA%2520Method%2520with%2520content%2520and%2520distortion%2520priors%26entry.906535625%3DYajing%2520Pei%2520and%2520Shiyu%2520Huang%2520and%2520Yiting%2520Lu%2520and%2520Xin%2520Li%2520and%2520Zhibo%2520Chen%26entry.1292438233%3D%2520%2520User%2520Generated%2520Content%2520%2528UGC%2529%2520videos%2520are%2520susceptible%2520to%2520complicated%2520and%250Avariant%2520degradations%2520and%2520contents%252C%2520which%2520prevents%2520the%2520existing%2520blind%2520video%250Aquality%2520assessment%2520%2528BVQA%2529%2520models%2520from%2520good%2520performance%2520since%2520the%2520lack%2520of%2520the%250Aadapability%2520of%2520distortions%2520and%2520contents.%2520To%2520mitigate%2520this%252C%2520we%2520propose%2520a%2520novel%250Aprior-augmented%2520perceptual%2520vision%2520transformer%2520%2528PriorFormer%2529%2520for%2520the%2520BVQA%2520of%250AUGC%252C%2520which%2520boots%2520its%2520adaptability%2520and%2520representation%2520capability%2520for%2520divergent%250Acontents%2520and%2520distortions.%2520Concretely%252C%2520we%2520introduce%2520two%2520powerful%2520priors%252C%2520i.e.%252C%250Athe%2520content%2520and%2520distortion%2520priors%252C%2520by%2520extracting%2520the%2520content%2520and%2520distortion%250Aembeddings%2520from%2520two%2520pre-trained%2520feature%2520extractors.%2520Then%2520we%2520adopt%2520these%2520two%250Apowerful%2520embeddings%2520as%2520the%2520adaptive%2520prior%2520tokens%252C%2520which%2520are%2520transferred%2520to%2520the%250Avision%2520transformer%2520backbone%2520jointly%2520with%2520implicit%2520quality%2520features.%2520Based%2520on%250Athe%2520above%2520strategy%252C%2520the%2520proposed%2520PriorFormer%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520three%2520public%2520UGC%2520VQA%2520datasets%2520including%2520KoNViD-1K%252C%2520LIVE-VQC%2520and%250AYouTube-UGC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16297v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Priorformer%3A%20A%20UGC-VQA%20Method%20with%20content%20and%20distortion%20priors&entry.906535625=Yajing%20Pei%20and%20Shiyu%20Huang%20and%20Yiting%20Lu%20and%20Xin%20Li%20and%20Zhibo%20Chen&entry.1292438233=%20%20User%20Generated%20Content%20%28UGC%29%20videos%20are%20susceptible%20to%20complicated%20and%0Avariant%20degradations%20and%20contents%2C%20which%20prevents%20the%20existing%20blind%20video%0Aquality%20assessment%20%28BVQA%29%20models%20from%20good%20performance%20since%20the%20lack%20of%20the%0Aadapability%20of%20distortions%20and%20contents.%20To%20mitigate%20this%2C%20we%20propose%20a%20novel%0Aprior-augmented%20perceptual%20vision%20transformer%20%28PriorFormer%29%20for%20the%20BVQA%20of%0AUGC%2C%20which%20boots%20its%20adaptability%20and%20representation%20capability%20for%20divergent%0Acontents%20and%20distortions.%20Concretely%2C%20we%20introduce%20two%20powerful%20priors%2C%20i.e.%2C%0Athe%20content%20and%20distortion%20priors%2C%20by%20extracting%20the%20content%20and%20distortion%0Aembeddings%20from%20two%20pre-trained%20feature%20extractors.%20Then%20we%20adopt%20these%20two%0Apowerful%20embeddings%20as%20the%20adaptive%20prior%20tokens%2C%20which%20are%20transferred%20to%20the%0Avision%20transformer%20backbone%20jointly%20with%20implicit%20quality%20features.%20Based%20on%0Athe%20above%20strategy%2C%20the%20proposed%20PriorFormer%20achieves%20state-of-the-art%0Aperformance%20on%20three%20public%20UGC%20VQA%20datasets%20including%20KoNViD-1K%2C%20LIVE-VQC%20and%0AYouTube-UGC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16297v1&entry.124074799=Read"},
{"title": "HEST-1k: A Dataset for Spatial Transcriptomics and Histology Image\n  Analysis", "author": "Guillaume Jaume and Paul Doucet and Andrew H. Song and Ming Y. Lu and Cristina Almagro-P\u00e9rez and Sophia J. Wagner and Anurag J. Vaidya and Richard J. Chen and Drew F. K. Williamson and Ahrong Kim and Faisal Mahmood", "abstract": "  Spatial transcriptomics (ST) enables interrogating the molecular composition\nof tissue with ever-increasing resolution, depth, and sensitivity. However,\ncosts, rapidly evolving technology, and lack of standards have constrained\ncomputational methods in ST to narrow tasks and small cohorts. In addition, the\nunderlying tissue morphology as reflected by H&E-stained whole slide images\n(WSIs) encodes rich information often overlooked in ST studies. Here, we\nintroduce HEST-1k, a collection of 1,108 spatial transcriptomic profiles, each\nlinked to a WSI and metadata. HEST-1k was assembled using HEST-Library from 131\npublic and internal cohorts encompassing 25 organs, two species (Homo Sapiens\nand Mus Musculus), and 320 cancer samples from 25 cancer types. HEST-1k\nprocessing enabled the identification of 1.5 million expression--morphology\npairs and 60 million nuclei. HEST-1k is tested on three use cases: (1)\nbenchmarking foundation models for histopathology (HEST-Benchmark), (2)\nbiomarker identification, and (3) multimodal representation learning. HEST-1k,\nHEST-Library, and HEST-Benchmark can be freely accessed via\nhttps://github.com/mahmoodlab/hest.\n", "link": "http://arxiv.org/abs/2406.16192v1", "date": "2024-06-23", "relevancy": 2.1858, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.445}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.445}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HEST-1k%3A%20A%20Dataset%20for%20Spatial%20Transcriptomics%20and%20Histology%20Image%0A%20%20Analysis&body=Title%3A%20HEST-1k%3A%20A%20Dataset%20for%20Spatial%20Transcriptomics%20and%20Histology%20Image%0A%20%20Analysis%0AAuthor%3A%20Guillaume%20Jaume%20and%20Paul%20Doucet%20and%20Andrew%20H.%20Song%20and%20Ming%20Y.%20Lu%20and%20Cristina%20Almagro-P%C3%A9rez%20and%20Sophia%20J.%20Wagner%20and%20Anurag%20J.%20Vaidya%20and%20Richard%20J.%20Chen%20and%20Drew%20F.%20K.%20Williamson%20and%20Ahrong%20Kim%20and%20Faisal%20Mahmood%0AAbstract%3A%20%20%20Spatial%20transcriptomics%20%28ST%29%20enables%20interrogating%20the%20molecular%20composition%0Aof%20tissue%20with%20ever-increasing%20resolution%2C%20depth%2C%20and%20sensitivity.%20However%2C%0Acosts%2C%20rapidly%20evolving%20technology%2C%20and%20lack%20of%20standards%20have%20constrained%0Acomputational%20methods%20in%20ST%20to%20narrow%20tasks%20and%20small%20cohorts.%20In%20addition%2C%20the%0Aunderlying%20tissue%20morphology%20as%20reflected%20by%20H%26E-stained%20whole%20slide%20images%0A%28WSIs%29%20encodes%20rich%20information%20often%20overlooked%20in%20ST%20studies.%20Here%2C%20we%0Aintroduce%20HEST-1k%2C%20a%20collection%20of%201%2C108%20spatial%20transcriptomic%20profiles%2C%20each%0Alinked%20to%20a%20WSI%20and%20metadata.%20HEST-1k%20was%20assembled%20using%20HEST-Library%20from%20131%0Apublic%20and%20internal%20cohorts%20encompassing%2025%20organs%2C%20two%20species%20%28Homo%20Sapiens%0Aand%20Mus%20Musculus%29%2C%20and%20320%20cancer%20samples%20from%2025%20cancer%20types.%20HEST-1k%0Aprocessing%20enabled%20the%20identification%20of%201.5%20million%20expression--morphology%0Apairs%20and%2060%20million%20nuclei.%20HEST-1k%20is%20tested%20on%20three%20use%20cases%3A%20%281%29%0Abenchmarking%20foundation%20models%20for%20histopathology%20%28HEST-Benchmark%29%2C%20%282%29%0Abiomarker%20identification%2C%20and%20%283%29%20multimodal%20representation%20learning.%20HEST-1k%2C%0AHEST-Library%2C%20and%20HEST-Benchmark%20can%20be%20freely%20accessed%20via%0Ahttps%3A//github.com/mahmoodlab/hest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16192v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHEST-1k%253A%2520A%2520Dataset%2520for%2520Spatial%2520Transcriptomics%2520and%2520Histology%2520Image%250A%2520%2520Analysis%26entry.906535625%3DGuillaume%2520Jaume%2520and%2520Paul%2520Doucet%2520and%2520Andrew%2520H.%2520Song%2520and%2520Ming%2520Y.%2520Lu%2520and%2520Cristina%2520Almagro-P%25C3%25A9rez%2520and%2520Sophia%2520J.%2520Wagner%2520and%2520Anurag%2520J.%2520Vaidya%2520and%2520Richard%2520J.%2520Chen%2520and%2520Drew%2520F.%2520K.%2520Williamson%2520and%2520Ahrong%2520Kim%2520and%2520Faisal%2520Mahmood%26entry.1292438233%3D%2520%2520Spatial%2520transcriptomics%2520%2528ST%2529%2520enables%2520interrogating%2520the%2520molecular%2520composition%250Aof%2520tissue%2520with%2520ever-increasing%2520resolution%252C%2520depth%252C%2520and%2520sensitivity.%2520However%252C%250Acosts%252C%2520rapidly%2520evolving%2520technology%252C%2520and%2520lack%2520of%2520standards%2520have%2520constrained%250Acomputational%2520methods%2520in%2520ST%2520to%2520narrow%2520tasks%2520and%2520small%2520cohorts.%2520In%2520addition%252C%2520the%250Aunderlying%2520tissue%2520morphology%2520as%2520reflected%2520by%2520H%2526E-stained%2520whole%2520slide%2520images%250A%2528WSIs%2529%2520encodes%2520rich%2520information%2520often%2520overlooked%2520in%2520ST%2520studies.%2520Here%252C%2520we%250Aintroduce%2520HEST-1k%252C%2520a%2520collection%2520of%25201%252C108%2520spatial%2520transcriptomic%2520profiles%252C%2520each%250Alinked%2520to%2520a%2520WSI%2520and%2520metadata.%2520HEST-1k%2520was%2520assembled%2520using%2520HEST-Library%2520from%2520131%250Apublic%2520and%2520internal%2520cohorts%2520encompassing%252025%2520organs%252C%2520two%2520species%2520%2528Homo%2520Sapiens%250Aand%2520Mus%2520Musculus%2529%252C%2520and%2520320%2520cancer%2520samples%2520from%252025%2520cancer%2520types.%2520HEST-1k%250Aprocessing%2520enabled%2520the%2520identification%2520of%25201.5%2520million%2520expression--morphology%250Apairs%2520and%252060%2520million%2520nuclei.%2520HEST-1k%2520is%2520tested%2520on%2520three%2520use%2520cases%253A%2520%25281%2529%250Abenchmarking%2520foundation%2520models%2520for%2520histopathology%2520%2528HEST-Benchmark%2529%252C%2520%25282%2529%250Abiomarker%2520identification%252C%2520and%2520%25283%2529%2520multimodal%2520representation%2520learning.%2520HEST-1k%252C%250AHEST-Library%252C%2520and%2520HEST-Benchmark%2520can%2520be%2520freely%2520accessed%2520via%250Ahttps%253A//github.com/mahmoodlab/hest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16192v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HEST-1k%3A%20A%20Dataset%20for%20Spatial%20Transcriptomics%20and%20Histology%20Image%0A%20%20Analysis&entry.906535625=Guillaume%20Jaume%20and%20Paul%20Doucet%20and%20Andrew%20H.%20Song%20and%20Ming%20Y.%20Lu%20and%20Cristina%20Almagro-P%C3%A9rez%20and%20Sophia%20J.%20Wagner%20and%20Anurag%20J.%20Vaidya%20and%20Richard%20J.%20Chen%20and%20Drew%20F.%20K.%20Williamson%20and%20Ahrong%20Kim%20and%20Faisal%20Mahmood&entry.1292438233=%20%20Spatial%20transcriptomics%20%28ST%29%20enables%20interrogating%20the%20molecular%20composition%0Aof%20tissue%20with%20ever-increasing%20resolution%2C%20depth%2C%20and%20sensitivity.%20However%2C%0Acosts%2C%20rapidly%20evolving%20technology%2C%20and%20lack%20of%20standards%20have%20constrained%0Acomputational%20methods%20in%20ST%20to%20narrow%20tasks%20and%20small%20cohorts.%20In%20addition%2C%20the%0Aunderlying%20tissue%20morphology%20as%20reflected%20by%20H%26E-stained%20whole%20slide%20images%0A%28WSIs%29%20encodes%20rich%20information%20often%20overlooked%20in%20ST%20studies.%20Here%2C%20we%0Aintroduce%20HEST-1k%2C%20a%20collection%20of%201%2C108%20spatial%20transcriptomic%20profiles%2C%20each%0Alinked%20to%20a%20WSI%20and%20metadata.%20HEST-1k%20was%20assembled%20using%20HEST-Library%20from%20131%0Apublic%20and%20internal%20cohorts%20encompassing%2025%20organs%2C%20two%20species%20%28Homo%20Sapiens%0Aand%20Mus%20Musculus%29%2C%20and%20320%20cancer%20samples%20from%2025%20cancer%20types.%20HEST-1k%0Aprocessing%20enabled%20the%20identification%20of%201.5%20million%20expression--morphology%0Apairs%20and%2060%20million%20nuclei.%20HEST-1k%20is%20tested%20on%20three%20use%20cases%3A%20%281%29%0Abenchmarking%20foundation%20models%20for%20histopathology%20%28HEST-Benchmark%29%2C%20%282%29%0Abiomarker%20identification%2C%20and%20%283%29%20multimodal%20representation%20learning.%20HEST-1k%2C%0AHEST-Library%2C%20and%20HEST-Benchmark%20can%20be%20freely%20accessed%20via%0Ahttps%3A//github.com/mahmoodlab/hest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16192v1&entry.124074799=Read"},
{"title": "Local primordial non-Gaussianity from the large-scale clustering of\n  photometric DESI luminous red galaxies", "author": "Mehdi Rezaie and Ashley J. Ross and Hee-Jong Seo and Hui Kong and Anna Porredon and Lado Samushia and Edmond Chaussidon and Alex Krolewski and Arnaud de Mattia and Florian Beutler and Jessica Nicole Aguilar and Steven Ahlen and Shadab Alam and Santiago Avila and Benedict Bahr-Kalus and Jose Bermejo-Climent and David Brooks and Todd Claybaugh and Shaun Cole and Kyle Dawson and Axel de la Macorra and Peter Doel and Andreu Font-Ribera and Jaime E. Forero-Romero and Satya Gontcho A Gontcho and Julien Guy and Klaus Honscheid and Dragan Huterer and Theodore Kisner and Martin Landriau and Michael Levi and Marc Manera and Aaron Meisner and Ramon Miquel and Eva-Maria Mueller and Adam Myers and Jeffrey A. Newman and Jundan Nie and Nathalie Palanque-Delabrouille and Will Percival and Claire Poppett and Graziano Rossi and Eusebio Sanchez and Michael Schubnell and Gregory Tarl\u00e9 and Benjamin Alan Weaver and Christophe Y\u00e8che and Zhimin Zhou and Hu Zou", "abstract": "  We use angular clustering of luminous red galaxies from the Dark Energy\nSpectroscopic Instrument (DESI) imaging surveys to constrain the local\nprimordial non-Gaussianity parameter $\\fnl$. Our sample comprises over 12\nmillion targets, covering 14,000 square degrees of the sky, with redshifts in\nthe range $0.2< z < 1.35$. We identify Galactic extinction, survey depth, and\nastronomical seeing as the primary sources of systematic error, and employ\nlinear regression and artificial neural networks to alleviate non-cosmological\nexcess clustering on large scales. Our methods are tested against simulations\nwith and without $\\fnl$ and systematics, showing superior performance of the\nneural network treatment. The neural network with a set of nine imaging\nproperty maps passes our systematic null test criteria, and is chosen as the\nfiducial treatment. Assuming the universality relation, we find $\\fnl =\n34^{+24(+50)}_{-44(-73)}$ at 68\\%(95\\%) confidence. We apply a series of\nrobustness tests (e.g., cuts on imaging, declination, or scales used) that show\nconsistency in the obtained constraints. We study how the regression method\nbiases the measured angular power-spectrum and degrades the $\\fnl$ constraining\npower. The use of the nine maps more than doubles the uncertainty compared to\nusing only the three primary maps in the regression. Our results thus motivate\nthe development of more efficient methods that avoid over-correction, protect\nlarge-scale clustering information, and preserve constraining power.\nAdditionally, our results encourage further studies of $\\fnl$ with DESI\nspectroscopic samples, where the inclusion of 3D clustering modes should help\nseparate imaging systematics and lessen the degradation in the $\\fnl$\nuncertainty.\n", "link": "http://arxiv.org/abs/2307.01753v2", "date": "2024-06-24", "relevancy": 2.1837, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4467}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4377}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20primordial%20non-Gaussianity%20from%20the%20large-scale%20clustering%20of%0A%20%20photometric%20DESI%20luminous%20red%20galaxies&body=Title%3A%20Local%20primordial%20non-Gaussianity%20from%20the%20large-scale%20clustering%20of%0A%20%20photometric%20DESI%20luminous%20red%20galaxies%0AAuthor%3A%20Mehdi%20Rezaie%20and%20Ashley%20J.%20Ross%20and%20Hee-Jong%20Seo%20and%20Hui%20Kong%20and%20Anna%20Porredon%20and%20Lado%20Samushia%20and%20Edmond%20Chaussidon%20and%20Alex%20Krolewski%20and%20Arnaud%20de%20Mattia%20and%20Florian%20Beutler%20and%20Jessica%20Nicole%20Aguilar%20and%20Steven%20Ahlen%20and%20Shadab%20Alam%20and%20Santiago%20Avila%20and%20Benedict%20Bahr-Kalus%20and%20Jose%20Bermejo-Climent%20and%20David%20Brooks%20and%20Todd%20Claybaugh%20and%20Shaun%20Cole%20and%20Kyle%20Dawson%20and%20Axel%20de%20la%20Macorra%20and%20Peter%20Doel%20and%20Andreu%20Font-Ribera%20and%20Jaime%20E.%20Forero-Romero%20and%20Satya%20Gontcho%20A%20Gontcho%20and%20Julien%20Guy%20and%20Klaus%20Honscheid%20and%20Dragan%20Huterer%20and%20Theodore%20Kisner%20and%20Martin%20Landriau%20and%20Michael%20Levi%20and%20Marc%20Manera%20and%20Aaron%20Meisner%20and%20Ramon%20Miquel%20and%20Eva-Maria%20Mueller%20and%20Adam%20Myers%20and%20Jeffrey%20A.%20Newman%20and%20Jundan%20Nie%20and%20Nathalie%20Palanque-Delabrouille%20and%20Will%20Percival%20and%20Claire%20Poppett%20and%20Graziano%20Rossi%20and%20Eusebio%20Sanchez%20and%20Michael%20Schubnell%20and%20Gregory%20Tarl%C3%A9%20and%20Benjamin%20Alan%20Weaver%20and%20Christophe%20Y%C3%A8che%20and%20Zhimin%20Zhou%20and%20Hu%20Zou%0AAbstract%3A%20%20%20We%20use%20angular%20clustering%20of%20luminous%20red%20galaxies%20from%20the%20Dark%20Energy%0ASpectroscopic%20Instrument%20%28DESI%29%20imaging%20surveys%20to%20constrain%20the%20local%0Aprimordial%20non-Gaussianity%20parameter%20%24%5Cfnl%24.%20Our%20sample%20comprises%20over%2012%0Amillion%20targets%2C%20covering%2014%2C000%20square%20degrees%20of%20the%20sky%2C%20with%20redshifts%20in%0Athe%20range%20%240.2%3C%20z%20%3C%201.35%24.%20We%20identify%20Galactic%20extinction%2C%20survey%20depth%2C%20and%0Aastronomical%20seeing%20as%20the%20primary%20sources%20of%20systematic%20error%2C%20and%20employ%0Alinear%20regression%20and%20artificial%20neural%20networks%20to%20alleviate%20non-cosmological%0Aexcess%20clustering%20on%20large%20scales.%20Our%20methods%20are%20tested%20against%20simulations%0Awith%20and%20without%20%24%5Cfnl%24%20and%20systematics%2C%20showing%20superior%20performance%20of%20the%0Aneural%20network%20treatment.%20The%20neural%20network%20with%20a%20set%20of%20nine%20imaging%0Aproperty%20maps%20passes%20our%20systematic%20null%20test%20criteria%2C%20and%20is%20chosen%20as%20the%0Afiducial%20treatment.%20Assuming%20the%20universality%20relation%2C%20we%20find%20%24%5Cfnl%20%3D%0A34%5E%7B%2B24%28%2B50%29%7D_%7B-44%28-73%29%7D%24%20at%2068%5C%25%2895%5C%25%29%20confidence.%20We%20apply%20a%20series%20of%0Arobustness%20tests%20%28e.g.%2C%20cuts%20on%20imaging%2C%20declination%2C%20or%20scales%20used%29%20that%20show%0Aconsistency%20in%20the%20obtained%20constraints.%20We%20study%20how%20the%20regression%20method%0Abiases%20the%20measured%20angular%20power-spectrum%20and%20degrades%20the%20%24%5Cfnl%24%20constraining%0Apower.%20The%20use%20of%20the%20nine%20maps%20more%20than%20doubles%20the%20uncertainty%20compared%20to%0Ausing%20only%20the%20three%20primary%20maps%20in%20the%20regression.%20Our%20results%20thus%20motivate%0Athe%20development%20of%20more%20efficient%20methods%20that%20avoid%20over-correction%2C%20protect%0Alarge-scale%20clustering%20information%2C%20and%20preserve%20constraining%20power.%0AAdditionally%2C%20our%20results%20encourage%20further%20studies%20of%20%24%5Cfnl%24%20with%20DESI%0Aspectroscopic%20samples%2C%20where%20the%20inclusion%20of%203D%20clustering%20modes%20should%20help%0Aseparate%20imaging%20systematics%20and%20lessen%20the%20degradation%20in%20the%20%24%5Cfnl%24%0Auncertainty.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.01753v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520primordial%2520non-Gaussianity%2520from%2520the%2520large-scale%2520clustering%2520of%250A%2520%2520photometric%2520DESI%2520luminous%2520red%2520galaxies%26entry.906535625%3DMehdi%2520Rezaie%2520and%2520Ashley%2520J.%2520Ross%2520and%2520Hee-Jong%2520Seo%2520and%2520Hui%2520Kong%2520and%2520Anna%2520Porredon%2520and%2520Lado%2520Samushia%2520and%2520Edmond%2520Chaussidon%2520and%2520Alex%2520Krolewski%2520and%2520Arnaud%2520de%2520Mattia%2520and%2520Florian%2520Beutler%2520and%2520Jessica%2520Nicole%2520Aguilar%2520and%2520Steven%2520Ahlen%2520and%2520Shadab%2520Alam%2520and%2520Santiago%2520Avila%2520and%2520Benedict%2520Bahr-Kalus%2520and%2520Jose%2520Bermejo-Climent%2520and%2520David%2520Brooks%2520and%2520Todd%2520Claybaugh%2520and%2520Shaun%2520Cole%2520and%2520Kyle%2520Dawson%2520and%2520Axel%2520de%2520la%2520Macorra%2520and%2520Peter%2520Doel%2520and%2520Andreu%2520Font-Ribera%2520and%2520Jaime%2520E.%2520Forero-Romero%2520and%2520Satya%2520Gontcho%2520A%2520Gontcho%2520and%2520Julien%2520Guy%2520and%2520Klaus%2520Honscheid%2520and%2520Dragan%2520Huterer%2520and%2520Theodore%2520Kisner%2520and%2520Martin%2520Landriau%2520and%2520Michael%2520Levi%2520and%2520Marc%2520Manera%2520and%2520Aaron%2520Meisner%2520and%2520Ramon%2520Miquel%2520and%2520Eva-Maria%2520Mueller%2520and%2520Adam%2520Myers%2520and%2520Jeffrey%2520A.%2520Newman%2520and%2520Jundan%2520Nie%2520and%2520Nathalie%2520Palanque-Delabrouille%2520and%2520Will%2520Percival%2520and%2520Claire%2520Poppett%2520and%2520Graziano%2520Rossi%2520and%2520Eusebio%2520Sanchez%2520and%2520Michael%2520Schubnell%2520and%2520Gregory%2520Tarl%25C3%25A9%2520and%2520Benjamin%2520Alan%2520Weaver%2520and%2520Christophe%2520Y%25C3%25A8che%2520and%2520Zhimin%2520Zhou%2520and%2520Hu%2520Zou%26entry.1292438233%3D%2520%2520We%2520use%2520angular%2520clustering%2520of%2520luminous%2520red%2520galaxies%2520from%2520the%2520Dark%2520Energy%250ASpectroscopic%2520Instrument%2520%2528DESI%2529%2520imaging%2520surveys%2520to%2520constrain%2520the%2520local%250Aprimordial%2520non-Gaussianity%2520parameter%2520%2524%255Cfnl%2524.%2520Our%2520sample%2520comprises%2520over%252012%250Amillion%2520targets%252C%2520covering%252014%252C000%2520square%2520degrees%2520of%2520the%2520sky%252C%2520with%2520redshifts%2520in%250Athe%2520range%2520%25240.2%253C%2520z%2520%253C%25201.35%2524.%2520We%2520identify%2520Galactic%2520extinction%252C%2520survey%2520depth%252C%2520and%250Aastronomical%2520seeing%2520as%2520the%2520primary%2520sources%2520of%2520systematic%2520error%252C%2520and%2520employ%250Alinear%2520regression%2520and%2520artificial%2520neural%2520networks%2520to%2520alleviate%2520non-cosmological%250Aexcess%2520clustering%2520on%2520large%2520scales.%2520Our%2520methods%2520are%2520tested%2520against%2520simulations%250Awith%2520and%2520without%2520%2524%255Cfnl%2524%2520and%2520systematics%252C%2520showing%2520superior%2520performance%2520of%2520the%250Aneural%2520network%2520treatment.%2520The%2520neural%2520network%2520with%2520a%2520set%2520of%2520nine%2520imaging%250Aproperty%2520maps%2520passes%2520our%2520systematic%2520null%2520test%2520criteria%252C%2520and%2520is%2520chosen%2520as%2520the%250Afiducial%2520treatment.%2520Assuming%2520the%2520universality%2520relation%252C%2520we%2520find%2520%2524%255Cfnl%2520%253D%250A34%255E%257B%252B24%2528%252B50%2529%257D_%257B-44%2528-73%2529%257D%2524%2520at%252068%255C%2525%252895%255C%2525%2529%2520confidence.%2520We%2520apply%2520a%2520series%2520of%250Arobustness%2520tests%2520%2528e.g.%252C%2520cuts%2520on%2520imaging%252C%2520declination%252C%2520or%2520scales%2520used%2529%2520that%2520show%250Aconsistency%2520in%2520the%2520obtained%2520constraints.%2520We%2520study%2520how%2520the%2520regression%2520method%250Abiases%2520the%2520measured%2520angular%2520power-spectrum%2520and%2520degrades%2520the%2520%2524%255Cfnl%2524%2520constraining%250Apower.%2520The%2520use%2520of%2520the%2520nine%2520maps%2520more%2520than%2520doubles%2520the%2520uncertainty%2520compared%2520to%250Ausing%2520only%2520the%2520three%2520primary%2520maps%2520in%2520the%2520regression.%2520Our%2520results%2520thus%2520motivate%250Athe%2520development%2520of%2520more%2520efficient%2520methods%2520that%2520avoid%2520over-correction%252C%2520protect%250Alarge-scale%2520clustering%2520information%252C%2520and%2520preserve%2520constraining%2520power.%250AAdditionally%252C%2520our%2520results%2520encourage%2520further%2520studies%2520of%2520%2524%255Cfnl%2524%2520with%2520DESI%250Aspectroscopic%2520samples%252C%2520where%2520the%2520inclusion%2520of%25203D%2520clustering%2520modes%2520should%2520help%250Aseparate%2520imaging%2520systematics%2520and%2520lessen%2520the%2520degradation%2520in%2520the%2520%2524%255Cfnl%2524%250Auncertainty.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.01753v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20primordial%20non-Gaussianity%20from%20the%20large-scale%20clustering%20of%0A%20%20photometric%20DESI%20luminous%20red%20galaxies&entry.906535625=Mehdi%20Rezaie%20and%20Ashley%20J.%20Ross%20and%20Hee-Jong%20Seo%20and%20Hui%20Kong%20and%20Anna%20Porredon%20and%20Lado%20Samushia%20and%20Edmond%20Chaussidon%20and%20Alex%20Krolewski%20and%20Arnaud%20de%20Mattia%20and%20Florian%20Beutler%20and%20Jessica%20Nicole%20Aguilar%20and%20Steven%20Ahlen%20and%20Shadab%20Alam%20and%20Santiago%20Avila%20and%20Benedict%20Bahr-Kalus%20and%20Jose%20Bermejo-Climent%20and%20David%20Brooks%20and%20Todd%20Claybaugh%20and%20Shaun%20Cole%20and%20Kyle%20Dawson%20and%20Axel%20de%20la%20Macorra%20and%20Peter%20Doel%20and%20Andreu%20Font-Ribera%20and%20Jaime%20E.%20Forero-Romero%20and%20Satya%20Gontcho%20A%20Gontcho%20and%20Julien%20Guy%20and%20Klaus%20Honscheid%20and%20Dragan%20Huterer%20and%20Theodore%20Kisner%20and%20Martin%20Landriau%20and%20Michael%20Levi%20and%20Marc%20Manera%20and%20Aaron%20Meisner%20and%20Ramon%20Miquel%20and%20Eva-Maria%20Mueller%20and%20Adam%20Myers%20and%20Jeffrey%20A.%20Newman%20and%20Jundan%20Nie%20and%20Nathalie%20Palanque-Delabrouille%20and%20Will%20Percival%20and%20Claire%20Poppett%20and%20Graziano%20Rossi%20and%20Eusebio%20Sanchez%20and%20Michael%20Schubnell%20and%20Gregory%20Tarl%C3%A9%20and%20Benjamin%20Alan%20Weaver%20and%20Christophe%20Y%C3%A8che%20and%20Zhimin%20Zhou%20and%20Hu%20Zou&entry.1292438233=%20%20We%20use%20angular%20clustering%20of%20luminous%20red%20galaxies%20from%20the%20Dark%20Energy%0ASpectroscopic%20Instrument%20%28DESI%29%20imaging%20surveys%20to%20constrain%20the%20local%0Aprimordial%20non-Gaussianity%20parameter%20%24%5Cfnl%24.%20Our%20sample%20comprises%20over%2012%0Amillion%20targets%2C%20covering%2014%2C000%20square%20degrees%20of%20the%20sky%2C%20with%20redshifts%20in%0Athe%20range%20%240.2%3C%20z%20%3C%201.35%24.%20We%20identify%20Galactic%20extinction%2C%20survey%20depth%2C%20and%0Aastronomical%20seeing%20as%20the%20primary%20sources%20of%20systematic%20error%2C%20and%20employ%0Alinear%20regression%20and%20artificial%20neural%20networks%20to%20alleviate%20non-cosmological%0Aexcess%20clustering%20on%20large%20scales.%20Our%20methods%20are%20tested%20against%20simulations%0Awith%20and%20without%20%24%5Cfnl%24%20and%20systematics%2C%20showing%20superior%20performance%20of%20the%0Aneural%20network%20treatment.%20The%20neural%20network%20with%20a%20set%20of%20nine%20imaging%0Aproperty%20maps%20passes%20our%20systematic%20null%20test%20criteria%2C%20and%20is%20chosen%20as%20the%0Afiducial%20treatment.%20Assuming%20the%20universality%20relation%2C%20we%20find%20%24%5Cfnl%20%3D%0A34%5E%7B%2B24%28%2B50%29%7D_%7B-44%28-73%29%7D%24%20at%2068%5C%25%2895%5C%25%29%20confidence.%20We%20apply%20a%20series%20of%0Arobustness%20tests%20%28e.g.%2C%20cuts%20on%20imaging%2C%20declination%2C%20or%20scales%20used%29%20that%20show%0Aconsistency%20in%20the%20obtained%20constraints.%20We%20study%20how%20the%20regression%20method%0Abiases%20the%20measured%20angular%20power-spectrum%20and%20degrades%20the%20%24%5Cfnl%24%20constraining%0Apower.%20The%20use%20of%20the%20nine%20maps%20more%20than%20doubles%20the%20uncertainty%20compared%20to%0Ausing%20only%20the%20three%20primary%20maps%20in%20the%20regression.%20Our%20results%20thus%20motivate%0Athe%20development%20of%20more%20efficient%20methods%20that%20avoid%20over-correction%2C%20protect%0Alarge-scale%20clustering%20information%2C%20and%20preserve%20constraining%20power.%0AAdditionally%2C%20our%20results%20encourage%20further%20studies%20of%20%24%5Cfnl%24%20with%20DESI%0Aspectroscopic%20samples%2C%20where%20the%20inclusion%20of%203D%20clustering%20modes%20should%20help%0Aseparate%20imaging%20systematics%20and%20lessen%20the%20degradation%20in%20the%20%24%5Cfnl%24%0Auncertainty.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.01753v2&entry.124074799=Read"},
{"title": "Detach-ROCKET: Sequential feature selection for time series\n  classification with random convolutional kernels", "author": "Gonzalo Uribarri and Federico Barone and Alessio Ansuini and Erik Frans\u00e9n", "abstract": "  Time Series Classification (TSC) is essential in fields like medicine,\nenvironmental science, and finance, enabling tasks such as disease diagnosis,\nanomaly detection, and stock price analysis. While machine learning models like\nRecurrent Neural Networks and InceptionTime are successful in numerous\napplications, they can face scalability issues due to computational\nrequirements. Recently, ROCKET has emerged as an efficient alternative,\nachieving state-of-the-art performance and simplifying training by utilizing a\nlarge number of randomly generated features from the time series data. However,\nmany of these features are redundant or non-informative, increasing\ncomputational load and compromising generalization. Here we introduce\nSequential Feature Detachment (SFD) to identify and prune non-essential\nfeatures in ROCKET-based models, such as ROCKET, MiniRocket, and MultiRocket.\nSFD estimates feature importance using model coefficients and can handle large\nfeature sets without complex hyperparameter tuning. Testing on the UCR archive\nshows that SFD can produce models with better test accuracy using only 10\\% of\nthe original features. We named these pruned models Detach-ROCKET. We also\npresent an end-to-end procedure for determining an optimal balance between the\nnumber of features and model accuracy. On the largest binary UCR dataset,\nDetach-ROCKET improves test accuracy by 0.6\\% while reducing features by\n98.9\\%. By enabling a significant reduction in model size without sacrificing\naccuracy, our methodology improves computational efficiency and contributes to\nmodel interpretability. We believe that Detach-ROCKET will be a valuable tool\nfor researchers and practitioners working with time series data, who can find a\nuser-friendly implementation of the model at\n\\url{https://github.com/gon-uri/detach_rocket}.\n", "link": "http://arxiv.org/abs/2309.14518v3", "date": "2024-06-24", "relevancy": 2.1821, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4469}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4436}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4188}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detach-ROCKET%3A%20Sequential%20feature%20selection%20for%20time%20series%0A%20%20classification%20with%20random%20convolutional%20kernels&body=Title%3A%20Detach-ROCKET%3A%20Sequential%20feature%20selection%20for%20time%20series%0A%20%20classification%20with%20random%20convolutional%20kernels%0AAuthor%3A%20Gonzalo%20Uribarri%20and%20Federico%20Barone%20and%20Alessio%20Ansuini%20and%20Erik%20Frans%C3%A9n%0AAbstract%3A%20%20%20Time%20Series%20Classification%20%28TSC%29%20is%20essential%20in%20fields%20like%20medicine%2C%0Aenvironmental%20science%2C%20and%20finance%2C%20enabling%20tasks%20such%20as%20disease%20diagnosis%2C%0Aanomaly%20detection%2C%20and%20stock%20price%20analysis.%20While%20machine%20learning%20models%20like%0ARecurrent%20Neural%20Networks%20and%20InceptionTime%20are%20successful%20in%20numerous%0Aapplications%2C%20they%20can%20face%20scalability%20issues%20due%20to%20computational%0Arequirements.%20Recently%2C%20ROCKET%20has%20emerged%20as%20an%20efficient%20alternative%2C%0Aachieving%20state-of-the-art%20performance%20and%20simplifying%20training%20by%20utilizing%20a%0Alarge%20number%20of%20randomly%20generated%20features%20from%20the%20time%20series%20data.%20However%2C%0Amany%20of%20these%20features%20are%20redundant%20or%20non-informative%2C%20increasing%0Acomputational%20load%20and%20compromising%20generalization.%20Here%20we%20introduce%0ASequential%20Feature%20Detachment%20%28SFD%29%20to%20identify%20and%20prune%20non-essential%0Afeatures%20in%20ROCKET-based%20models%2C%20such%20as%20ROCKET%2C%20MiniRocket%2C%20and%20MultiRocket.%0ASFD%20estimates%20feature%20importance%20using%20model%20coefficients%20and%20can%20handle%20large%0Afeature%20sets%20without%20complex%20hyperparameter%20tuning.%20Testing%20on%20the%20UCR%20archive%0Ashows%20that%20SFD%20can%20produce%20models%20with%20better%20test%20accuracy%20using%20only%2010%5C%25%20of%0Athe%20original%20features.%20We%20named%20these%20pruned%20models%20Detach-ROCKET.%20We%20also%0Apresent%20an%20end-to-end%20procedure%20for%20determining%20an%20optimal%20balance%20between%20the%0Anumber%20of%20features%20and%20model%20accuracy.%20On%20the%20largest%20binary%20UCR%20dataset%2C%0ADetach-ROCKET%20improves%20test%20accuracy%20by%200.6%5C%25%20while%20reducing%20features%20by%0A98.9%5C%25.%20By%20enabling%20a%20significant%20reduction%20in%20model%20size%20without%20sacrificing%0Aaccuracy%2C%20our%20methodology%20improves%20computational%20efficiency%20and%20contributes%20to%0Amodel%20interpretability.%20We%20believe%20that%20Detach-ROCKET%20will%20be%20a%20valuable%20tool%0Afor%20researchers%20and%20practitioners%20working%20with%20time%20series%20data%2C%20who%20can%20find%20a%0Auser-friendly%20implementation%20of%20the%20model%20at%0A%5Curl%7Bhttps%3A//github.com/gon-uri/detach_rocket%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.14518v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetach-ROCKET%253A%2520Sequential%2520feature%2520selection%2520for%2520time%2520series%250A%2520%2520classification%2520with%2520random%2520convolutional%2520kernels%26entry.906535625%3DGonzalo%2520Uribarri%2520and%2520Federico%2520Barone%2520and%2520Alessio%2520Ansuini%2520and%2520Erik%2520Frans%25C3%25A9n%26entry.1292438233%3D%2520%2520Time%2520Series%2520Classification%2520%2528TSC%2529%2520is%2520essential%2520in%2520fields%2520like%2520medicine%252C%250Aenvironmental%2520science%252C%2520and%2520finance%252C%2520enabling%2520tasks%2520such%2520as%2520disease%2520diagnosis%252C%250Aanomaly%2520detection%252C%2520and%2520stock%2520price%2520analysis.%2520While%2520machine%2520learning%2520models%2520like%250ARecurrent%2520Neural%2520Networks%2520and%2520InceptionTime%2520are%2520successful%2520in%2520numerous%250Aapplications%252C%2520they%2520can%2520face%2520scalability%2520issues%2520due%2520to%2520computational%250Arequirements.%2520Recently%252C%2520ROCKET%2520has%2520emerged%2520as%2520an%2520efficient%2520alternative%252C%250Aachieving%2520state-of-the-art%2520performance%2520and%2520simplifying%2520training%2520by%2520utilizing%2520a%250Alarge%2520number%2520of%2520randomly%2520generated%2520features%2520from%2520the%2520time%2520series%2520data.%2520However%252C%250Amany%2520of%2520these%2520features%2520are%2520redundant%2520or%2520non-informative%252C%2520increasing%250Acomputational%2520load%2520and%2520compromising%2520generalization.%2520Here%2520we%2520introduce%250ASequential%2520Feature%2520Detachment%2520%2528SFD%2529%2520to%2520identify%2520and%2520prune%2520non-essential%250Afeatures%2520in%2520ROCKET-based%2520models%252C%2520such%2520as%2520ROCKET%252C%2520MiniRocket%252C%2520and%2520MultiRocket.%250ASFD%2520estimates%2520feature%2520importance%2520using%2520model%2520coefficients%2520and%2520can%2520handle%2520large%250Afeature%2520sets%2520without%2520complex%2520hyperparameter%2520tuning.%2520Testing%2520on%2520the%2520UCR%2520archive%250Ashows%2520that%2520SFD%2520can%2520produce%2520models%2520with%2520better%2520test%2520accuracy%2520using%2520only%252010%255C%2525%2520of%250Athe%2520original%2520features.%2520We%2520named%2520these%2520pruned%2520models%2520Detach-ROCKET.%2520We%2520also%250Apresent%2520an%2520end-to-end%2520procedure%2520for%2520determining%2520an%2520optimal%2520balance%2520between%2520the%250Anumber%2520of%2520features%2520and%2520model%2520accuracy.%2520On%2520the%2520largest%2520binary%2520UCR%2520dataset%252C%250ADetach-ROCKET%2520improves%2520test%2520accuracy%2520by%25200.6%255C%2525%2520while%2520reducing%2520features%2520by%250A98.9%255C%2525.%2520By%2520enabling%2520a%2520significant%2520reduction%2520in%2520model%2520size%2520without%2520sacrificing%250Aaccuracy%252C%2520our%2520methodology%2520improves%2520computational%2520efficiency%2520and%2520contributes%2520to%250Amodel%2520interpretability.%2520We%2520believe%2520that%2520Detach-ROCKET%2520will%2520be%2520a%2520valuable%2520tool%250Afor%2520researchers%2520and%2520practitioners%2520working%2520with%2520time%2520series%2520data%252C%2520who%2520can%2520find%2520a%250Auser-friendly%2520implementation%2520of%2520the%2520model%2520at%250A%255Curl%257Bhttps%253A//github.com/gon-uri/detach_rocket%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.14518v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detach-ROCKET%3A%20Sequential%20feature%20selection%20for%20time%20series%0A%20%20classification%20with%20random%20convolutional%20kernels&entry.906535625=Gonzalo%20Uribarri%20and%20Federico%20Barone%20and%20Alessio%20Ansuini%20and%20Erik%20Frans%C3%A9n&entry.1292438233=%20%20Time%20Series%20Classification%20%28TSC%29%20is%20essential%20in%20fields%20like%20medicine%2C%0Aenvironmental%20science%2C%20and%20finance%2C%20enabling%20tasks%20such%20as%20disease%20diagnosis%2C%0Aanomaly%20detection%2C%20and%20stock%20price%20analysis.%20While%20machine%20learning%20models%20like%0ARecurrent%20Neural%20Networks%20and%20InceptionTime%20are%20successful%20in%20numerous%0Aapplications%2C%20they%20can%20face%20scalability%20issues%20due%20to%20computational%0Arequirements.%20Recently%2C%20ROCKET%20has%20emerged%20as%20an%20efficient%20alternative%2C%0Aachieving%20state-of-the-art%20performance%20and%20simplifying%20training%20by%20utilizing%20a%0Alarge%20number%20of%20randomly%20generated%20features%20from%20the%20time%20series%20data.%20However%2C%0Amany%20of%20these%20features%20are%20redundant%20or%20non-informative%2C%20increasing%0Acomputational%20load%20and%20compromising%20generalization.%20Here%20we%20introduce%0ASequential%20Feature%20Detachment%20%28SFD%29%20to%20identify%20and%20prune%20non-essential%0Afeatures%20in%20ROCKET-based%20models%2C%20such%20as%20ROCKET%2C%20MiniRocket%2C%20and%20MultiRocket.%0ASFD%20estimates%20feature%20importance%20using%20model%20coefficients%20and%20can%20handle%20large%0Afeature%20sets%20without%20complex%20hyperparameter%20tuning.%20Testing%20on%20the%20UCR%20archive%0Ashows%20that%20SFD%20can%20produce%20models%20with%20better%20test%20accuracy%20using%20only%2010%5C%25%20of%0Athe%20original%20features.%20We%20named%20these%20pruned%20models%20Detach-ROCKET.%20We%20also%0Apresent%20an%20end-to-end%20procedure%20for%20determining%20an%20optimal%20balance%20between%20the%0Anumber%20of%20features%20and%20model%20accuracy.%20On%20the%20largest%20binary%20UCR%20dataset%2C%0ADetach-ROCKET%20improves%20test%20accuracy%20by%200.6%5C%25%20while%20reducing%20features%20by%0A98.9%5C%25.%20By%20enabling%20a%20significant%20reduction%20in%20model%20size%20without%20sacrificing%0Aaccuracy%2C%20our%20methodology%20improves%20computational%20efficiency%20and%20contributes%20to%0Amodel%20interpretability.%20We%20believe%20that%20Detach-ROCKET%20will%20be%20a%20valuable%20tool%0Afor%20researchers%20and%20practitioners%20working%20with%20time%20series%20data%2C%20who%20can%20find%20a%0Auser-friendly%20implementation%20of%20the%20model%20at%0A%5Curl%7Bhttps%3A//github.com/gon-uri/detach_rocket%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.14518v3&entry.124074799=Read"},
{"title": "Position: Benchmarking is Limited in Reinforcement Learning Research", "author": "Scott M. Jordan and Adam White and Bruno Castro da Silva and Martha White and Philip S. Thomas", "abstract": "  Novel reinforcement learning algorithms, or improvements on existing ones,\nare commonly justified by evaluating their performance on benchmark\nenvironments and are compared to an ever-changing set of standard algorithms.\nHowever, despite numerous calls for improvements, experimental practices\ncontinue to produce misleading or unsupported claims. One reason for the\nongoing substandard practices is that conducting rigorous benchmarking\nexperiments requires substantial computational time. This work investigates the\nsources of increased computation costs in rigorous experiment designs. We show\nthat conducting rigorous performance benchmarks will likely have computational\ncosts that are often prohibitive. As a result, we argue for using an additional\nexperimentation paradigm to overcome the limitations of benchmarking.\n", "link": "http://arxiv.org/abs/2406.16241v1", "date": "2024-06-23", "relevancy": 1.3118, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.449}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4383}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Position%3A%20Benchmarking%20is%20Limited%20in%20Reinforcement%20Learning%20Research&body=Title%3A%20Position%3A%20Benchmarking%20is%20Limited%20in%20Reinforcement%20Learning%20Research%0AAuthor%3A%20Scott%20M.%20Jordan%20and%20Adam%20White%20and%20Bruno%20Castro%20da%20Silva%20and%20Martha%20White%20and%20Philip%20S.%20Thomas%0AAbstract%3A%20%20%20Novel%20reinforcement%20learning%20algorithms%2C%20or%20improvements%20on%20existing%20ones%2C%0Aare%20commonly%20justified%20by%20evaluating%20their%20performance%20on%20benchmark%0Aenvironments%20and%20are%20compared%20to%20an%20ever-changing%20set%20of%20standard%20algorithms.%0AHowever%2C%20despite%20numerous%20calls%20for%20improvements%2C%20experimental%20practices%0Acontinue%20to%20produce%20misleading%20or%20unsupported%20claims.%20One%20reason%20for%20the%0Aongoing%20substandard%20practices%20is%20that%20conducting%20rigorous%20benchmarking%0Aexperiments%20requires%20substantial%20computational%20time.%20This%20work%20investigates%20the%0Asources%20of%20increased%20computation%20costs%20in%20rigorous%20experiment%20designs.%20We%20show%0Athat%20conducting%20rigorous%20performance%20benchmarks%20will%20likely%20have%20computational%0Acosts%20that%20are%20often%20prohibitive.%20As%20a%20result%2C%20we%20argue%20for%20using%20an%20additional%0Aexperimentation%20paradigm%20to%20overcome%20the%20limitations%20of%20benchmarking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16241v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosition%253A%2520Benchmarking%2520is%2520Limited%2520in%2520Reinforcement%2520Learning%2520Research%26entry.906535625%3DScott%2520M.%2520Jordan%2520and%2520Adam%2520White%2520and%2520Bruno%2520Castro%2520da%2520Silva%2520and%2520Martha%2520White%2520and%2520Philip%2520S.%2520Thomas%26entry.1292438233%3D%2520%2520Novel%2520reinforcement%2520learning%2520algorithms%252C%2520or%2520improvements%2520on%2520existing%2520ones%252C%250Aare%2520commonly%2520justified%2520by%2520evaluating%2520their%2520performance%2520on%2520benchmark%250Aenvironments%2520and%2520are%2520compared%2520to%2520an%2520ever-changing%2520set%2520of%2520standard%2520algorithms.%250AHowever%252C%2520despite%2520numerous%2520calls%2520for%2520improvements%252C%2520experimental%2520practices%250Acontinue%2520to%2520produce%2520misleading%2520or%2520unsupported%2520claims.%2520One%2520reason%2520for%2520the%250Aongoing%2520substandard%2520practices%2520is%2520that%2520conducting%2520rigorous%2520benchmarking%250Aexperiments%2520requires%2520substantial%2520computational%2520time.%2520This%2520work%2520investigates%2520the%250Asources%2520of%2520increased%2520computation%2520costs%2520in%2520rigorous%2520experiment%2520designs.%2520We%2520show%250Athat%2520conducting%2520rigorous%2520performance%2520benchmarks%2520will%2520likely%2520have%2520computational%250Acosts%2520that%2520are%2520often%2520prohibitive.%2520As%2520a%2520result%252C%2520we%2520argue%2520for%2520using%2520an%2520additional%250Aexperimentation%2520paradigm%2520to%2520overcome%2520the%2520limitations%2520of%2520benchmarking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16241v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Position%3A%20Benchmarking%20is%20Limited%20in%20Reinforcement%20Learning%20Research&entry.906535625=Scott%20M.%20Jordan%20and%20Adam%20White%20and%20Bruno%20Castro%20da%20Silva%20and%20Martha%20White%20and%20Philip%20S.%20Thomas&entry.1292438233=%20%20Novel%20reinforcement%20learning%20algorithms%2C%20or%20improvements%20on%20existing%20ones%2C%0Aare%20commonly%20justified%20by%20evaluating%20their%20performance%20on%20benchmark%0Aenvironments%20and%20are%20compared%20to%20an%20ever-changing%20set%20of%20standard%20algorithms.%0AHowever%2C%20despite%20numerous%20calls%20for%20improvements%2C%20experimental%20practices%0Acontinue%20to%20produce%20misleading%20or%20unsupported%20claims.%20One%20reason%20for%20the%0Aongoing%20substandard%20practices%20is%20that%20conducting%20rigorous%20benchmarking%0Aexperiments%20requires%20substantial%20computational%20time.%20This%20work%20investigates%20the%0Asources%20of%20increased%20computation%20costs%20in%20rigorous%20experiment%20designs.%20We%20show%0Athat%20conducting%20rigorous%20performance%20benchmarks%20will%20likely%20have%20computational%0Acosts%20that%20are%20often%20prohibitive.%20As%20a%20result%2C%20we%20argue%20for%20using%20an%20additional%0Aexperimentation%20paradigm%20to%20overcome%20the%20limitations%20of%20benchmarking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16241v1&entry.124074799=Read"},
{"title": "CausalFormer: An Interpretable Transformer for Temporal Causal Discovery", "author": "Lingbai Kong and Wengen Li and Hanchen Yang and Yichao Zhang and Jihong Guan and Shuigeng Zhou", "abstract": "  Temporal causal discovery is a crucial task aimed at uncovering the causal\nrelations within time series data. The latest temporal causal discovery methods\nusually train deep learning models on prediction tasks to uncover the causality\nbetween time series. They capture causal relations by analyzing the parameters\nof some components of the trained models, e.g., attention weights and\nconvolution weights. However, this is an incomplete mapping process from the\nmodel parameters to the causality and fails to investigate the other\ncomponents, e.g., fully connected layers and activation functions, that are\nalso significant for causal discovery. To facilitate the utilization of the\nwhole deep learning models in temporal causal discovery, we proposed an\ninterpretable transformer-based causal discovery model termed CausalFormer,\nwhich consists of the causality-aware transformer and the decomposition-based\ncausality detector. The causality-aware transformer learns the causal\nrepresentation of time series data using a prediction task with the designed\nmulti-kernel causal convolution which aggregates each input time series along\nthe temporal dimension under the temporal priority constraint. Then, the\ndecomposition-based causality detector interprets the global structure of the\ntrained causality-aware transformer with the proposed regression relevance\npropagation to identify potential causal relations and finally construct the\ncausal graph. Experiments on synthetic, simulated, and real datasets\ndemonstrate the state-of-the-art performance of CausalFormer on discovering\ntemporal causality. Our code is available at\nhttps://github.com/lingbai-kong/CausalFormer.\n", "link": "http://arxiv.org/abs/2406.16708v1", "date": "2024-06-24", "relevancy": 1.9242, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5043}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4891}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CausalFormer%3A%20An%20Interpretable%20Transformer%20for%20Temporal%20Causal%20Discovery&body=Title%3A%20CausalFormer%3A%20An%20Interpretable%20Transformer%20for%20Temporal%20Causal%20Discovery%0AAuthor%3A%20Lingbai%20Kong%20and%20Wengen%20Li%20and%20Hanchen%20Yang%20and%20Yichao%20Zhang%20and%20Jihong%20Guan%20and%20Shuigeng%20Zhou%0AAbstract%3A%20%20%20Temporal%20causal%20discovery%20is%20a%20crucial%20task%20aimed%20at%20uncovering%20the%20causal%0Arelations%20within%20time%20series%20data.%20The%20latest%20temporal%20causal%20discovery%20methods%0Ausually%20train%20deep%20learning%20models%20on%20prediction%20tasks%20to%20uncover%20the%20causality%0Abetween%20time%20series.%20They%20capture%20causal%20relations%20by%20analyzing%20the%20parameters%0Aof%20some%20components%20of%20the%20trained%20models%2C%20e.g.%2C%20attention%20weights%20and%0Aconvolution%20weights.%20However%2C%20this%20is%20an%20incomplete%20mapping%20process%20from%20the%0Amodel%20parameters%20to%20the%20causality%20and%20fails%20to%20investigate%20the%20other%0Acomponents%2C%20e.g.%2C%20fully%20connected%20layers%20and%20activation%20functions%2C%20that%20are%0Aalso%20significant%20for%20causal%20discovery.%20To%20facilitate%20the%20utilization%20of%20the%0Awhole%20deep%20learning%20models%20in%20temporal%20causal%20discovery%2C%20we%20proposed%20an%0Ainterpretable%20transformer-based%20causal%20discovery%20model%20termed%20CausalFormer%2C%0Awhich%20consists%20of%20the%20causality-aware%20transformer%20and%20the%20decomposition-based%0Acausality%20detector.%20The%20causality-aware%20transformer%20learns%20the%20causal%0Arepresentation%20of%20time%20series%20data%20using%20a%20prediction%20task%20with%20the%20designed%0Amulti-kernel%20causal%20convolution%20which%20aggregates%20each%20input%20time%20series%20along%0Athe%20temporal%20dimension%20under%20the%20temporal%20priority%20constraint.%20Then%2C%20the%0Adecomposition-based%20causality%20detector%20interprets%20the%20global%20structure%20of%20the%0Atrained%20causality-aware%20transformer%20with%20the%20proposed%20regression%20relevance%0Apropagation%20to%20identify%20potential%20causal%20relations%20and%20finally%20construct%20the%0Acausal%20graph.%20Experiments%20on%20synthetic%2C%20simulated%2C%20and%20real%20datasets%0Ademonstrate%20the%20state-of-the-art%20performance%20of%20CausalFormer%20on%20discovering%0Atemporal%20causality.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/lingbai-kong/CausalFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16708v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausalFormer%253A%2520An%2520Interpretable%2520Transformer%2520for%2520Temporal%2520Causal%2520Discovery%26entry.906535625%3DLingbai%2520Kong%2520and%2520Wengen%2520Li%2520and%2520Hanchen%2520Yang%2520and%2520Yichao%2520Zhang%2520and%2520Jihong%2520Guan%2520and%2520Shuigeng%2520Zhou%26entry.1292438233%3D%2520%2520Temporal%2520causal%2520discovery%2520is%2520a%2520crucial%2520task%2520aimed%2520at%2520uncovering%2520the%2520causal%250Arelations%2520within%2520time%2520series%2520data.%2520The%2520latest%2520temporal%2520causal%2520discovery%2520methods%250Ausually%2520train%2520deep%2520learning%2520models%2520on%2520prediction%2520tasks%2520to%2520uncover%2520the%2520causality%250Abetween%2520time%2520series.%2520They%2520capture%2520causal%2520relations%2520by%2520analyzing%2520the%2520parameters%250Aof%2520some%2520components%2520of%2520the%2520trained%2520models%252C%2520e.g.%252C%2520attention%2520weights%2520and%250Aconvolution%2520weights.%2520However%252C%2520this%2520is%2520an%2520incomplete%2520mapping%2520process%2520from%2520the%250Amodel%2520parameters%2520to%2520the%2520causality%2520and%2520fails%2520to%2520investigate%2520the%2520other%250Acomponents%252C%2520e.g.%252C%2520fully%2520connected%2520layers%2520and%2520activation%2520functions%252C%2520that%2520are%250Aalso%2520significant%2520for%2520causal%2520discovery.%2520To%2520facilitate%2520the%2520utilization%2520of%2520the%250Awhole%2520deep%2520learning%2520models%2520in%2520temporal%2520causal%2520discovery%252C%2520we%2520proposed%2520an%250Ainterpretable%2520transformer-based%2520causal%2520discovery%2520model%2520termed%2520CausalFormer%252C%250Awhich%2520consists%2520of%2520the%2520causality-aware%2520transformer%2520and%2520the%2520decomposition-based%250Acausality%2520detector.%2520The%2520causality-aware%2520transformer%2520learns%2520the%2520causal%250Arepresentation%2520of%2520time%2520series%2520data%2520using%2520a%2520prediction%2520task%2520with%2520the%2520designed%250Amulti-kernel%2520causal%2520convolution%2520which%2520aggregates%2520each%2520input%2520time%2520series%2520along%250Athe%2520temporal%2520dimension%2520under%2520the%2520temporal%2520priority%2520constraint.%2520Then%252C%2520the%250Adecomposition-based%2520causality%2520detector%2520interprets%2520the%2520global%2520structure%2520of%2520the%250Atrained%2520causality-aware%2520transformer%2520with%2520the%2520proposed%2520regression%2520relevance%250Apropagation%2520to%2520identify%2520potential%2520causal%2520relations%2520and%2520finally%2520construct%2520the%250Acausal%2520graph.%2520Experiments%2520on%2520synthetic%252C%2520simulated%252C%2520and%2520real%2520datasets%250Ademonstrate%2520the%2520state-of-the-art%2520performance%2520of%2520CausalFormer%2520on%2520discovering%250Atemporal%2520causality.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/lingbai-kong/CausalFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16708v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CausalFormer%3A%20An%20Interpretable%20Transformer%20for%20Temporal%20Causal%20Discovery&entry.906535625=Lingbai%20Kong%20and%20Wengen%20Li%20and%20Hanchen%20Yang%20and%20Yichao%20Zhang%20and%20Jihong%20Guan%20and%20Shuigeng%20Zhou&entry.1292438233=%20%20Temporal%20causal%20discovery%20is%20a%20crucial%20task%20aimed%20at%20uncovering%20the%20causal%0Arelations%20within%20time%20series%20data.%20The%20latest%20temporal%20causal%20discovery%20methods%0Ausually%20train%20deep%20learning%20models%20on%20prediction%20tasks%20to%20uncover%20the%20causality%0Abetween%20time%20series.%20They%20capture%20causal%20relations%20by%20analyzing%20the%20parameters%0Aof%20some%20components%20of%20the%20trained%20models%2C%20e.g.%2C%20attention%20weights%20and%0Aconvolution%20weights.%20However%2C%20this%20is%20an%20incomplete%20mapping%20process%20from%20the%0Amodel%20parameters%20to%20the%20causality%20and%20fails%20to%20investigate%20the%20other%0Acomponents%2C%20e.g.%2C%20fully%20connected%20layers%20and%20activation%20functions%2C%20that%20are%0Aalso%20significant%20for%20causal%20discovery.%20To%20facilitate%20the%20utilization%20of%20the%0Awhole%20deep%20learning%20models%20in%20temporal%20causal%20discovery%2C%20we%20proposed%20an%0Ainterpretable%20transformer-based%20causal%20discovery%20model%20termed%20CausalFormer%2C%0Awhich%20consists%20of%20the%20causality-aware%20transformer%20and%20the%20decomposition-based%0Acausality%20detector.%20The%20causality-aware%20transformer%20learns%20the%20causal%0Arepresentation%20of%20time%20series%20data%20using%20a%20prediction%20task%20with%20the%20designed%0Amulti-kernel%20causal%20convolution%20which%20aggregates%20each%20input%20time%20series%20along%0Athe%20temporal%20dimension%20under%20the%20temporal%20priority%20constraint.%20Then%2C%20the%0Adecomposition-based%20causality%20detector%20interprets%20the%20global%20structure%20of%20the%0Atrained%20causality-aware%20transformer%20with%20the%20proposed%20regression%20relevance%0Apropagation%20to%20identify%20potential%20causal%20relations%20and%20finally%20construct%20the%0Acausal%20graph.%20Experiments%20on%20synthetic%2C%20simulated%2C%20and%20real%20datasets%0Ademonstrate%20the%20state-of-the-art%20performance%20of%20CausalFormer%20on%20discovering%0Atemporal%20causality.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/lingbai-kong/CausalFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16708v1&entry.124074799=Read"},
{"title": "Agent Design Pattern Catalogue: A Collection of Architectural Patterns\n  for Foundation Model based Agents", "author": "Yue Liu and Sin Kit Lo and Qinghua Lu and Liming Zhu and Dehai Zhao and Xiwei Xu and Stefan Harrer and Jon Whittle", "abstract": "  Foundation model-enabled generative artificial intelligence facilitates the\ndevelopment and implementation of agents, which can leverage distinguished\nreasoning and language processing capabilities to takes a proactive, autonomous\nrole to pursue users' goals. Nevertheless, there is a lack of systematic\nknowledge to guide practitioners in designing the agents considering challenges\nof goal-seeking (including generating instrumental goals and plans), such as\nhallucinations inherent in foundation models, explainability of reasoning\nprocess, complex accountability, etc. To address this issue, we have performed\na systematic literature review to understand the state-of-the-art foundation\nmodel-based agents and the broader ecosystem. In this paper, we present a\npattern catalogue consisting of 17 architectural patterns with analyses of the\ncontext, forces, and trade-offs as the outcomes from the previous literature\nreview. The proposed catalogue can provide holistic guidance for the effective\nuse of patterns, and support the architecture design of foundation model-based\nagents by facilitating goal-seeking and plan generation.\n", "link": "http://arxiv.org/abs/2405.10467v3", "date": "2024-06-24", "relevancy": 1.8202, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4694}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4499}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agent%20Design%20Pattern%20Catalogue%3A%20A%20Collection%20of%20Architectural%20Patterns%0A%20%20for%20Foundation%20Model%20based%20Agents&body=Title%3A%20Agent%20Design%20Pattern%20Catalogue%3A%20A%20Collection%20of%20Architectural%20Patterns%0A%20%20for%20Foundation%20Model%20based%20Agents%0AAuthor%3A%20Yue%20Liu%20and%20Sin%20Kit%20Lo%20and%20Qinghua%20Lu%20and%20Liming%20Zhu%20and%20Dehai%20Zhao%20and%20Xiwei%20Xu%20and%20Stefan%20Harrer%20and%20Jon%20Whittle%0AAbstract%3A%20%20%20Foundation%20model-enabled%20generative%20artificial%20intelligence%20facilitates%20the%0Adevelopment%20and%20implementation%20of%20agents%2C%20which%20can%20leverage%20distinguished%0Areasoning%20and%20language%20processing%20capabilities%20to%20takes%20a%20proactive%2C%20autonomous%0Arole%20to%20pursue%20users%27%20goals.%20Nevertheless%2C%20there%20is%20a%20lack%20of%20systematic%0Aknowledge%20to%20guide%20practitioners%20in%20designing%20the%20agents%20considering%20challenges%0Aof%20goal-seeking%20%28including%20generating%20instrumental%20goals%20and%20plans%29%2C%20such%20as%0Ahallucinations%20inherent%20in%20foundation%20models%2C%20explainability%20of%20reasoning%0Aprocess%2C%20complex%20accountability%2C%20etc.%20To%20address%20this%20issue%2C%20we%20have%20performed%0Aa%20systematic%20literature%20review%20to%20understand%20the%20state-of-the-art%20foundation%0Amodel-based%20agents%20and%20the%20broader%20ecosystem.%20In%20this%20paper%2C%20we%20present%20a%0Apattern%20catalogue%20consisting%20of%2017%20architectural%20patterns%20with%20analyses%20of%20the%0Acontext%2C%20forces%2C%20and%20trade-offs%20as%20the%20outcomes%20from%20the%20previous%20literature%0Areview.%20The%20proposed%20catalogue%20can%20provide%20holistic%20guidance%20for%20the%20effective%0Ause%20of%20patterns%2C%20and%20support%20the%20architecture%20design%20of%20foundation%20model-based%0Aagents%20by%20facilitating%20goal-seeking%20and%20plan%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10467v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgent%2520Design%2520Pattern%2520Catalogue%253A%2520A%2520Collection%2520of%2520Architectural%2520Patterns%250A%2520%2520for%2520Foundation%2520Model%2520based%2520Agents%26entry.906535625%3DYue%2520Liu%2520and%2520Sin%2520Kit%2520Lo%2520and%2520Qinghua%2520Lu%2520and%2520Liming%2520Zhu%2520and%2520Dehai%2520Zhao%2520and%2520Xiwei%2520Xu%2520and%2520Stefan%2520Harrer%2520and%2520Jon%2520Whittle%26entry.1292438233%3D%2520%2520Foundation%2520model-enabled%2520generative%2520artificial%2520intelligence%2520facilitates%2520the%250Adevelopment%2520and%2520implementation%2520of%2520agents%252C%2520which%2520can%2520leverage%2520distinguished%250Areasoning%2520and%2520language%2520processing%2520capabilities%2520to%2520takes%2520a%2520proactive%252C%2520autonomous%250Arole%2520to%2520pursue%2520users%2527%2520goals.%2520Nevertheless%252C%2520there%2520is%2520a%2520lack%2520of%2520systematic%250Aknowledge%2520to%2520guide%2520practitioners%2520in%2520designing%2520the%2520agents%2520considering%2520challenges%250Aof%2520goal-seeking%2520%2528including%2520generating%2520instrumental%2520goals%2520and%2520plans%2529%252C%2520such%2520as%250Ahallucinations%2520inherent%2520in%2520foundation%2520models%252C%2520explainability%2520of%2520reasoning%250Aprocess%252C%2520complex%2520accountability%252C%2520etc.%2520To%2520address%2520this%2520issue%252C%2520we%2520have%2520performed%250Aa%2520systematic%2520literature%2520review%2520to%2520understand%2520the%2520state-of-the-art%2520foundation%250Amodel-based%2520agents%2520and%2520the%2520broader%2520ecosystem.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%250Apattern%2520catalogue%2520consisting%2520of%252017%2520architectural%2520patterns%2520with%2520analyses%2520of%2520the%250Acontext%252C%2520forces%252C%2520and%2520trade-offs%2520as%2520the%2520outcomes%2520from%2520the%2520previous%2520literature%250Areview.%2520The%2520proposed%2520catalogue%2520can%2520provide%2520holistic%2520guidance%2520for%2520the%2520effective%250Ause%2520of%2520patterns%252C%2520and%2520support%2520the%2520architecture%2520design%2520of%2520foundation%2520model-based%250Aagents%2520by%2520facilitating%2520goal-seeking%2520and%2520plan%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10467v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agent%20Design%20Pattern%20Catalogue%3A%20A%20Collection%20of%20Architectural%20Patterns%0A%20%20for%20Foundation%20Model%20based%20Agents&entry.906535625=Yue%20Liu%20and%20Sin%20Kit%20Lo%20and%20Qinghua%20Lu%20and%20Liming%20Zhu%20and%20Dehai%20Zhao%20and%20Xiwei%20Xu%20and%20Stefan%20Harrer%20and%20Jon%20Whittle&entry.1292438233=%20%20Foundation%20model-enabled%20generative%20artificial%20intelligence%20facilitates%20the%0Adevelopment%20and%20implementation%20of%20agents%2C%20which%20can%20leverage%20distinguished%0Areasoning%20and%20language%20processing%20capabilities%20to%20takes%20a%20proactive%2C%20autonomous%0Arole%20to%20pursue%20users%27%20goals.%20Nevertheless%2C%20there%20is%20a%20lack%20of%20systematic%0Aknowledge%20to%20guide%20practitioners%20in%20designing%20the%20agents%20considering%20challenges%0Aof%20goal-seeking%20%28including%20generating%20instrumental%20goals%20and%20plans%29%2C%20such%20as%0Ahallucinations%20inherent%20in%20foundation%20models%2C%20explainability%20of%20reasoning%0Aprocess%2C%20complex%20accountability%2C%20etc.%20To%20address%20this%20issue%2C%20we%20have%20performed%0Aa%20systematic%20literature%20review%20to%20understand%20the%20state-of-the-art%20foundation%0Amodel-based%20agents%20and%20the%20broader%20ecosystem.%20In%20this%20paper%2C%20we%20present%20a%0Apattern%20catalogue%20consisting%20of%2017%20architectural%20patterns%20with%20analyses%20of%20the%0Acontext%2C%20forces%2C%20and%20trade-offs%20as%20the%20outcomes%20from%20the%20previous%20literature%0Areview.%20The%20proposed%20catalogue%20can%20provide%20holistic%20guidance%20for%20the%20effective%0Ause%20of%20patterns%2C%20and%20support%20the%20architecture%20design%20of%20foundation%20model-based%0Aagents%20by%20facilitating%20goal-seeking%20and%20plan%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10467v3&entry.124074799=Read"},
{"title": "Guardrails for avoiding harmful medical product recommendations and\n  off-label promotion in generative AI models", "author": "Daniel Lopez-Martinez", "abstract": "  Generative AI (GenAI) models have demonstrated remarkable capabilities in a\nwide variety of medical tasks. However, as these models are trained using\ngeneralist datasets with very limited human oversight, they can learn uses of\nmedical products that have not been adequately evaluated for safety and\nefficacy, nor approved by regulatory agencies. Given the scale at which GenAI\nmay reach users, unvetted recommendations pose a public health risk. In this\nwork, we propose an approach to identify potentially harmful product\nrecommendations, and demonstrate it using a recent multimodal large language\nmodel.\n", "link": "http://arxiv.org/abs/2406.16455v1", "date": "2024-06-24", "relevancy": 2.0601, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5345}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5029}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guardrails%20for%20avoiding%20harmful%20medical%20product%20recommendations%20and%0A%20%20off-label%20promotion%20in%20generative%20AI%20models&body=Title%3A%20Guardrails%20for%20avoiding%20harmful%20medical%20product%20recommendations%20and%0A%20%20off-label%20promotion%20in%20generative%20AI%20models%0AAuthor%3A%20Daniel%20Lopez-Martinez%0AAbstract%3A%20%20%20Generative%20AI%20%28GenAI%29%20models%20have%20demonstrated%20remarkable%20capabilities%20in%20a%0Awide%20variety%20of%20medical%20tasks.%20However%2C%20as%20these%20models%20are%20trained%20using%0Ageneralist%20datasets%20with%20very%20limited%20human%20oversight%2C%20they%20can%20learn%20uses%20of%0Amedical%20products%20that%20have%20not%20been%20adequately%20evaluated%20for%20safety%20and%0Aefficacy%2C%20nor%20approved%20by%20regulatory%20agencies.%20Given%20the%20scale%20at%20which%20GenAI%0Amay%20reach%20users%2C%20unvetted%20recommendations%20pose%20a%20public%20health%20risk.%20In%20this%0Awork%2C%20we%20propose%20an%20approach%20to%20identify%20potentially%20harmful%20product%0Arecommendations%2C%20and%20demonstrate%20it%20using%20a%20recent%20multimodal%20large%20language%0Amodel.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16455v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuardrails%2520for%2520avoiding%2520harmful%2520medical%2520product%2520recommendations%2520and%250A%2520%2520off-label%2520promotion%2520in%2520generative%2520AI%2520models%26entry.906535625%3DDaniel%2520Lopez-Martinez%26entry.1292438233%3D%2520%2520Generative%2520AI%2520%2528GenAI%2529%2520models%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%2520a%250Awide%2520variety%2520of%2520medical%2520tasks.%2520However%252C%2520as%2520these%2520models%2520are%2520trained%2520using%250Ageneralist%2520datasets%2520with%2520very%2520limited%2520human%2520oversight%252C%2520they%2520can%2520learn%2520uses%2520of%250Amedical%2520products%2520that%2520have%2520not%2520been%2520adequately%2520evaluated%2520for%2520safety%2520and%250Aefficacy%252C%2520nor%2520approved%2520by%2520regulatory%2520agencies.%2520Given%2520the%2520scale%2520at%2520which%2520GenAI%250Amay%2520reach%2520users%252C%2520unvetted%2520recommendations%2520pose%2520a%2520public%2520health%2520risk.%2520In%2520this%250Awork%252C%2520we%2520propose%2520an%2520approach%2520to%2520identify%2520potentially%2520harmful%2520product%250Arecommendations%252C%2520and%2520demonstrate%2520it%2520using%2520a%2520recent%2520multimodal%2520large%2520language%250Amodel.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16455v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guardrails%20for%20avoiding%20harmful%20medical%20product%20recommendations%20and%0A%20%20off-label%20promotion%20in%20generative%20AI%20models&entry.906535625=Daniel%20Lopez-Martinez&entry.1292438233=%20%20Generative%20AI%20%28GenAI%29%20models%20have%20demonstrated%20remarkable%20capabilities%20in%20a%0Awide%20variety%20of%20medical%20tasks.%20However%2C%20as%20these%20models%20are%20trained%20using%0Ageneralist%20datasets%20with%20very%20limited%20human%20oversight%2C%20they%20can%20learn%20uses%20of%0Amedical%20products%20that%20have%20not%20been%20adequately%20evaluated%20for%20safety%20and%0Aefficacy%2C%20nor%20approved%20by%20regulatory%20agencies.%20Given%20the%20scale%20at%20which%20GenAI%0Amay%20reach%20users%2C%20unvetted%20recommendations%20pose%20a%20public%20health%20risk.%20In%20this%0Awork%2C%20we%20propose%20an%20approach%20to%20identify%20potentially%20harmful%20product%0Arecommendations%2C%20and%20demonstrate%20it%20using%20a%20recent%20multimodal%20large%20language%0Amodel.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16455v1&entry.124074799=Read"},
{"title": "Machine Learning Applications of Quantum Computing: A Review", "author": "Thien Nguyen and Tuomo Sipola and Jari Hautam\u00e4ki", "abstract": "  At the intersection of quantum computing and machine learning, this review\npaper explores the transformative impact these technologies are having on the\ncapabilities of data processing and analysis, far surpassing the bounds of\ntraditional computational methods. Drawing upon an in-depth analysis of 32\nseminal papers, this review delves into the interplay between quantum computing\nand machine learning, focusing on transcending the limitations of classical\ncomputing in advanced data processing and applications. This review emphasizes\nthe potential of quantum-enhanced methods in enhancing cybersecurity, a\ncritical sector that stands to benefit significantly from these advancements.\nThe literature review, primarily leveraging Science Direct as an academic\ndatabase, delves into the transformative effects of quantum technologies on\nmachine learning, drawing insights from a diverse collection of studies and\nscholarly articles. While the focus is primarily on the growing significance of\nquantum computing in cybersecurity, the review also acknowledges the promising\nimplications for other sectors as the field matures. Our systematic approach\ncategorizes sources based on quantum machine learning algorithms, applications,\nchallenges, and potential future developments, uncovering that quantum\ncomputing is increasingly being implemented in practical machine learning\nscenarios. The review highlights advancements in quantum-enhanced machine\nlearning algorithms and their potential applications in sectors such as\ncybersecurity, emphasizing the need for industry-specific solutions while\nconsidering ethical and security concerns. By presenting an overview of the\ncurrent state and projecting future directions, the paper sets a foundation for\nongoing research and strategic advancement in quantum machine learning.\n", "link": "http://arxiv.org/abs/2406.13262v2", "date": "2024-06-24", "relevancy": 1.1308, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4062}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3692}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3683}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Learning%20Applications%20of%20Quantum%20Computing%3A%20A%20Review&body=Title%3A%20Machine%20Learning%20Applications%20of%20Quantum%20Computing%3A%20A%20Review%0AAuthor%3A%20Thien%20Nguyen%20and%20Tuomo%20Sipola%20and%20Jari%20Hautam%C3%A4ki%0AAbstract%3A%20%20%20At%20the%20intersection%20of%20quantum%20computing%20and%20machine%20learning%2C%20this%20review%0Apaper%20explores%20the%20transformative%20impact%20these%20technologies%20are%20having%20on%20the%0Acapabilities%20of%20data%20processing%20and%20analysis%2C%20far%20surpassing%20the%20bounds%20of%0Atraditional%20computational%20methods.%20Drawing%20upon%20an%20in-depth%20analysis%20of%2032%0Aseminal%20papers%2C%20this%20review%20delves%20into%20the%20interplay%20between%20quantum%20computing%0Aand%20machine%20learning%2C%20focusing%20on%20transcending%20the%20limitations%20of%20classical%0Acomputing%20in%20advanced%20data%20processing%20and%20applications.%20This%20review%20emphasizes%0Athe%20potential%20of%20quantum-enhanced%20methods%20in%20enhancing%20cybersecurity%2C%20a%0Acritical%20sector%20that%20stands%20to%20benefit%20significantly%20from%20these%20advancements.%0AThe%20literature%20review%2C%20primarily%20leveraging%20Science%20Direct%20as%20an%20academic%0Adatabase%2C%20delves%20into%20the%20transformative%20effects%20of%20quantum%20technologies%20on%0Amachine%20learning%2C%20drawing%20insights%20from%20a%20diverse%20collection%20of%20studies%20and%0Ascholarly%20articles.%20While%20the%20focus%20is%20primarily%20on%20the%20growing%20significance%20of%0Aquantum%20computing%20in%20cybersecurity%2C%20the%20review%20also%20acknowledges%20the%20promising%0Aimplications%20for%20other%20sectors%20as%20the%20field%20matures.%20Our%20systematic%20approach%0Acategorizes%20sources%20based%20on%20quantum%20machine%20learning%20algorithms%2C%20applications%2C%0Achallenges%2C%20and%20potential%20future%20developments%2C%20uncovering%20that%20quantum%0Acomputing%20is%20increasingly%20being%20implemented%20in%20practical%20machine%20learning%0Ascenarios.%20The%20review%20highlights%20advancements%20in%20quantum-enhanced%20machine%0Alearning%20algorithms%20and%20their%20potential%20applications%20in%20sectors%20such%20as%0Acybersecurity%2C%20emphasizing%20the%20need%20for%20industry-specific%20solutions%20while%0Aconsidering%20ethical%20and%20security%20concerns.%20By%20presenting%20an%20overview%20of%20the%0Acurrent%20state%20and%20projecting%20future%20directions%2C%20the%20paper%20sets%20a%20foundation%20for%0Aongoing%20research%20and%20strategic%20advancement%20in%20quantum%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13262v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Learning%2520Applications%2520of%2520Quantum%2520Computing%253A%2520A%2520Review%26entry.906535625%3DThien%2520Nguyen%2520and%2520Tuomo%2520Sipola%2520and%2520Jari%2520Hautam%25C3%25A4ki%26entry.1292438233%3D%2520%2520At%2520the%2520intersection%2520of%2520quantum%2520computing%2520and%2520machine%2520learning%252C%2520this%2520review%250Apaper%2520explores%2520the%2520transformative%2520impact%2520these%2520technologies%2520are%2520having%2520on%2520the%250Acapabilities%2520of%2520data%2520processing%2520and%2520analysis%252C%2520far%2520surpassing%2520the%2520bounds%2520of%250Atraditional%2520computational%2520methods.%2520Drawing%2520upon%2520an%2520in-depth%2520analysis%2520of%252032%250Aseminal%2520papers%252C%2520this%2520review%2520delves%2520into%2520the%2520interplay%2520between%2520quantum%2520computing%250Aand%2520machine%2520learning%252C%2520focusing%2520on%2520transcending%2520the%2520limitations%2520of%2520classical%250Acomputing%2520in%2520advanced%2520data%2520processing%2520and%2520applications.%2520This%2520review%2520emphasizes%250Athe%2520potential%2520of%2520quantum-enhanced%2520methods%2520in%2520enhancing%2520cybersecurity%252C%2520a%250Acritical%2520sector%2520that%2520stands%2520to%2520benefit%2520significantly%2520from%2520these%2520advancements.%250AThe%2520literature%2520review%252C%2520primarily%2520leveraging%2520Science%2520Direct%2520as%2520an%2520academic%250Adatabase%252C%2520delves%2520into%2520the%2520transformative%2520effects%2520of%2520quantum%2520technologies%2520on%250Amachine%2520learning%252C%2520drawing%2520insights%2520from%2520a%2520diverse%2520collection%2520of%2520studies%2520and%250Ascholarly%2520articles.%2520While%2520the%2520focus%2520is%2520primarily%2520on%2520the%2520growing%2520significance%2520of%250Aquantum%2520computing%2520in%2520cybersecurity%252C%2520the%2520review%2520also%2520acknowledges%2520the%2520promising%250Aimplications%2520for%2520other%2520sectors%2520as%2520the%2520field%2520matures.%2520Our%2520systematic%2520approach%250Acategorizes%2520sources%2520based%2520on%2520quantum%2520machine%2520learning%2520algorithms%252C%2520applications%252C%250Achallenges%252C%2520and%2520potential%2520future%2520developments%252C%2520uncovering%2520that%2520quantum%250Acomputing%2520is%2520increasingly%2520being%2520implemented%2520in%2520practical%2520machine%2520learning%250Ascenarios.%2520The%2520review%2520highlights%2520advancements%2520in%2520quantum-enhanced%2520machine%250Alearning%2520algorithms%2520and%2520their%2520potential%2520applications%2520in%2520sectors%2520such%2520as%250Acybersecurity%252C%2520emphasizing%2520the%2520need%2520for%2520industry-specific%2520solutions%2520while%250Aconsidering%2520ethical%2520and%2520security%2520concerns.%2520By%2520presenting%2520an%2520overview%2520of%2520the%250Acurrent%2520state%2520and%2520projecting%2520future%2520directions%252C%2520the%2520paper%2520sets%2520a%2520foundation%2520for%250Aongoing%2520research%2520and%2520strategic%2520advancement%2520in%2520quantum%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13262v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning%20Applications%20of%20Quantum%20Computing%3A%20A%20Review&entry.906535625=Thien%20Nguyen%20and%20Tuomo%20Sipola%20and%20Jari%20Hautam%C3%A4ki&entry.1292438233=%20%20At%20the%20intersection%20of%20quantum%20computing%20and%20machine%20learning%2C%20this%20review%0Apaper%20explores%20the%20transformative%20impact%20these%20technologies%20are%20having%20on%20the%0Acapabilities%20of%20data%20processing%20and%20analysis%2C%20far%20surpassing%20the%20bounds%20of%0Atraditional%20computational%20methods.%20Drawing%20upon%20an%20in-depth%20analysis%20of%2032%0Aseminal%20papers%2C%20this%20review%20delves%20into%20the%20interplay%20between%20quantum%20computing%0Aand%20machine%20learning%2C%20focusing%20on%20transcending%20the%20limitations%20of%20classical%0Acomputing%20in%20advanced%20data%20processing%20and%20applications.%20This%20review%20emphasizes%0Athe%20potential%20of%20quantum-enhanced%20methods%20in%20enhancing%20cybersecurity%2C%20a%0Acritical%20sector%20that%20stands%20to%20benefit%20significantly%20from%20these%20advancements.%0AThe%20literature%20review%2C%20primarily%20leveraging%20Science%20Direct%20as%20an%20academic%0Adatabase%2C%20delves%20into%20the%20transformative%20effects%20of%20quantum%20technologies%20on%0Amachine%20learning%2C%20drawing%20insights%20from%20a%20diverse%20collection%20of%20studies%20and%0Ascholarly%20articles.%20While%20the%20focus%20is%20primarily%20on%20the%20growing%20significance%20of%0Aquantum%20computing%20in%20cybersecurity%2C%20the%20review%20also%20acknowledges%20the%20promising%0Aimplications%20for%20other%20sectors%20as%20the%20field%20matures.%20Our%20systematic%20approach%0Acategorizes%20sources%20based%20on%20quantum%20machine%20learning%20algorithms%2C%20applications%2C%0Achallenges%2C%20and%20potential%20future%20developments%2C%20uncovering%20that%20quantum%0Acomputing%20is%20increasingly%20being%20implemented%20in%20practical%20machine%20learning%0Ascenarios.%20The%20review%20highlights%20advancements%20in%20quantum-enhanced%20machine%0Alearning%20algorithms%20and%20their%20potential%20applications%20in%20sectors%20such%20as%0Acybersecurity%2C%20emphasizing%20the%20need%20for%20industry-specific%20solutions%20while%0Aconsidering%20ethical%20and%20security%20concerns.%20By%20presenting%20an%20overview%20of%20the%0Acurrent%20state%20and%20projecting%20future%20directions%2C%20the%20paper%20sets%20a%20foundation%20for%0Aongoing%20research%20and%20strategic%20advancement%20in%20quantum%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13262v2&entry.124074799=Read"},
{"title": "Personalized federated learning based on feature fusion", "author": "Wolong Xing and Zhenkui Shi and Hongyan Peng and Xiantao Hu and Xianxian Li", "abstract": "  Federated learning enables distributed clients to collaborate on training\nwhile storing their data locally to protect client privacy. However, due to the\nheterogeneity of data, models, and devices, the final global model may need to\nperform better for tasks on each client. Communication bottlenecks, data\nheterogeneity, and model heterogeneity have been common challenges in federated\nlearning. In this work, we considered a label distribution skew problem, a type\nof data heterogeneity easily overlooked. In the context of classification, we\npropose a personalized federated learning approach called pFedPM. In our\nprocess, we replace traditional gradient uploading with feature uploading,\nwhich helps reduce communication costs and allows for heterogeneous client\nmodels. These feature representations play a role in preserving privacy to some\nextent.\n  We use a hyperparameter $a$ to mix local and global features, which enables\nus to control the degree of personalization. We also introduced a relation\nnetwork as an additional decision layer, which provides a non-linear learnable\nclassifier to predict labels. Experimental results show that, with an\nappropriate setting of $a$, our scheme outperforms several recent FL methods on\nMNIST, FEMNIST, and CRIFAR10 datasets and achieves fewer communications.\n", "link": "http://arxiv.org/abs/2406.16583v1", "date": "2024-06-24", "relevancy": 1.9507, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5044}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4776}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20federated%20learning%20based%20on%20feature%20fusion&body=Title%3A%20Personalized%20federated%20learning%20based%20on%20feature%20fusion%0AAuthor%3A%20Wolong%20Xing%20and%20Zhenkui%20Shi%20and%20Hongyan%20Peng%20and%20Xiantao%20Hu%20and%20Xianxian%20Li%0AAbstract%3A%20%20%20Federated%20learning%20enables%20distributed%20clients%20to%20collaborate%20on%20training%0Awhile%20storing%20their%20data%20locally%20to%20protect%20client%20privacy.%20However%2C%20due%20to%20the%0Aheterogeneity%20of%20data%2C%20models%2C%20and%20devices%2C%20the%20final%20global%20model%20may%20need%20to%0Aperform%20better%20for%20tasks%20on%20each%20client.%20Communication%20bottlenecks%2C%20data%0Aheterogeneity%2C%20and%20model%20heterogeneity%20have%20been%20common%20challenges%20in%20federated%0Alearning.%20In%20this%20work%2C%20we%20considered%20a%20label%20distribution%20skew%20problem%2C%20a%20type%0Aof%20data%20heterogeneity%20easily%20overlooked.%20In%20the%20context%20of%20classification%2C%20we%0Apropose%20a%20personalized%20federated%20learning%20approach%20called%20pFedPM.%20In%20our%0Aprocess%2C%20we%20replace%20traditional%20gradient%20uploading%20with%20feature%20uploading%2C%0Awhich%20helps%20reduce%20communication%20costs%20and%20allows%20for%20heterogeneous%20client%0Amodels.%20These%20feature%20representations%20play%20a%20role%20in%20preserving%20privacy%20to%20some%0Aextent.%0A%20%20We%20use%20a%20hyperparameter%20%24a%24%20to%20mix%20local%20and%20global%20features%2C%20which%20enables%0Aus%20to%20control%20the%20degree%20of%20personalization.%20We%20also%20introduced%20a%20relation%0Anetwork%20as%20an%20additional%20decision%20layer%2C%20which%20provides%20a%20non-linear%20learnable%0Aclassifier%20to%20predict%20labels.%20Experimental%20results%20show%20that%2C%20with%20an%0Aappropriate%20setting%20of%20%24a%24%2C%20our%20scheme%20outperforms%20several%20recent%20FL%20methods%20on%0AMNIST%2C%20FEMNIST%2C%20and%20CRIFAR10%20datasets%20and%20achieves%20fewer%20communications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520federated%2520learning%2520based%2520on%2520feature%2520fusion%26entry.906535625%3DWolong%2520Xing%2520and%2520Zhenkui%2520Shi%2520and%2520Hongyan%2520Peng%2520and%2520Xiantao%2520Hu%2520and%2520Xianxian%2520Li%26entry.1292438233%3D%2520%2520Federated%2520learning%2520enables%2520distributed%2520clients%2520to%2520collaborate%2520on%2520training%250Awhile%2520storing%2520their%2520data%2520locally%2520to%2520protect%2520client%2520privacy.%2520However%252C%2520due%2520to%2520the%250Aheterogeneity%2520of%2520data%252C%2520models%252C%2520and%2520devices%252C%2520the%2520final%2520global%2520model%2520may%2520need%2520to%250Aperform%2520better%2520for%2520tasks%2520on%2520each%2520client.%2520Communication%2520bottlenecks%252C%2520data%250Aheterogeneity%252C%2520and%2520model%2520heterogeneity%2520have%2520been%2520common%2520challenges%2520in%2520federated%250Alearning.%2520In%2520this%2520work%252C%2520we%2520considered%2520a%2520label%2520distribution%2520skew%2520problem%252C%2520a%2520type%250Aof%2520data%2520heterogeneity%2520easily%2520overlooked.%2520In%2520the%2520context%2520of%2520classification%252C%2520we%250Apropose%2520a%2520personalized%2520federated%2520learning%2520approach%2520called%2520pFedPM.%2520In%2520our%250Aprocess%252C%2520we%2520replace%2520traditional%2520gradient%2520uploading%2520with%2520feature%2520uploading%252C%250Awhich%2520helps%2520reduce%2520communication%2520costs%2520and%2520allows%2520for%2520heterogeneous%2520client%250Amodels.%2520These%2520feature%2520representations%2520play%2520a%2520role%2520in%2520preserving%2520privacy%2520to%2520some%250Aextent.%250A%2520%2520We%2520use%2520a%2520hyperparameter%2520%2524a%2524%2520to%2520mix%2520local%2520and%2520global%2520features%252C%2520which%2520enables%250Aus%2520to%2520control%2520the%2520degree%2520of%2520personalization.%2520We%2520also%2520introduced%2520a%2520relation%250Anetwork%2520as%2520an%2520additional%2520decision%2520layer%252C%2520which%2520provides%2520a%2520non-linear%2520learnable%250Aclassifier%2520to%2520predict%2520labels.%2520Experimental%2520results%2520show%2520that%252C%2520with%2520an%250Aappropriate%2520setting%2520of%2520%2524a%2524%252C%2520our%2520scheme%2520outperforms%2520several%2520recent%2520FL%2520methods%2520on%250AMNIST%252C%2520FEMNIST%252C%2520and%2520CRIFAR10%2520datasets%2520and%2520achieves%2520fewer%2520communications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20federated%20learning%20based%20on%20feature%20fusion&entry.906535625=Wolong%20Xing%20and%20Zhenkui%20Shi%20and%20Hongyan%20Peng%20and%20Xiantao%20Hu%20and%20Xianxian%20Li&entry.1292438233=%20%20Federated%20learning%20enables%20distributed%20clients%20to%20collaborate%20on%20training%0Awhile%20storing%20their%20data%20locally%20to%20protect%20client%20privacy.%20However%2C%20due%20to%20the%0Aheterogeneity%20of%20data%2C%20models%2C%20and%20devices%2C%20the%20final%20global%20model%20may%20need%20to%0Aperform%20better%20for%20tasks%20on%20each%20client.%20Communication%20bottlenecks%2C%20data%0Aheterogeneity%2C%20and%20model%20heterogeneity%20have%20been%20common%20challenges%20in%20federated%0Alearning.%20In%20this%20work%2C%20we%20considered%20a%20label%20distribution%20skew%20problem%2C%20a%20type%0Aof%20data%20heterogeneity%20easily%20overlooked.%20In%20the%20context%20of%20classification%2C%20we%0Apropose%20a%20personalized%20federated%20learning%20approach%20called%20pFedPM.%20In%20our%0Aprocess%2C%20we%20replace%20traditional%20gradient%20uploading%20with%20feature%20uploading%2C%0Awhich%20helps%20reduce%20communication%20costs%20and%20allows%20for%20heterogeneous%20client%0Amodels.%20These%20feature%20representations%20play%20a%20role%20in%20preserving%20privacy%20to%20some%0Aextent.%0A%20%20We%20use%20a%20hyperparameter%20%24a%24%20to%20mix%20local%20and%20global%20features%2C%20which%20enables%0Aus%20to%20control%20the%20degree%20of%20personalization.%20We%20also%20introduced%20a%20relation%0Anetwork%20as%20an%20additional%20decision%20layer%2C%20which%20provides%20a%20non-linear%20learnable%0Aclassifier%20to%20predict%20labels.%20Experimental%20results%20show%20that%2C%20with%20an%0Aappropriate%20setting%20of%20%24a%24%2C%20our%20scheme%20outperforms%20several%20recent%20FL%20methods%20on%0AMNIST%2C%20FEMNIST%2C%20and%20CRIFAR10%20datasets%20and%20achieves%20fewer%20communications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16583v1&entry.124074799=Read"},
{"title": "A Non-autoregressive Multi-Horizon Flight Trajectory Prediction\n  Framework with Gray Code Representation", "author": "Dongyue Guo and Zheng Zhang and Zhen Yan and Jianwei Zhang and Yi Lin", "abstract": "  Flight Trajectory Prediction (FTP) is an essential task in Air Traffic\nControl (ATC), which can assist air traffic controllers in managing airspace\nmore safely and efficiently. Existing approaches generally perform\nmulti-horizon FTP tasks in an autoregressive manner, thereby suffering from\nerror accumulation and low-efficiency problems. In this paper, a novel\nframework, called FlightBERT++, is proposed to i) forecast multi-horizon flight\ntrajectories directly in a non-autoregressive way, and ii) improve the\nlimitation of the binary encoding (BE) representation in the FlightBERT\nframework. Specifically, the proposed framework is implemented by a generalized\nencoder-decoder architecture, in which the encoder learns the temporal-spatial\npatterns from historical observations and the decoder predicts the flight\nstatus for the future horizons. Compared to conventional architecture, an\ninnovative horizon-aware contexts generator is dedicatedly designed to consider\nthe prior horizon information, which further enables non-autoregressive\nmulti-horizon prediction. Additionally, the Gray code representation and the\ndifferential prediction paradigm are designed to cope with the high-bit\nmisclassifications of the BE representation, which significantly reduces the\noutliers in the predictions. Moreover, a differential prompted decoder is\nproposed to enhance the capability of the differential predictions by\nleveraging the stationarity of the differential sequence. Extensive experiments\nare conducted to validate the proposed framework on a real-world flight\ntrajectory dataset. The experimental results demonstrated that the proposed\nframework outperformed the competitive baselines in both FTP performance and\ncomputational efficiency.\n", "link": "http://arxiv.org/abs/2305.01658v4", "date": "2024-06-24", "relevancy": 1.4296, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5304}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4615}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Non-autoregressive%20Multi-Horizon%20Flight%20Trajectory%20Prediction%0A%20%20Framework%20with%20Gray%20Code%20Representation&body=Title%3A%20A%20Non-autoregressive%20Multi-Horizon%20Flight%20Trajectory%20Prediction%0A%20%20Framework%20with%20Gray%20Code%20Representation%0AAuthor%3A%20Dongyue%20Guo%20and%20Zheng%20Zhang%20and%20Zhen%20Yan%20and%20Jianwei%20Zhang%20and%20Yi%20Lin%0AAbstract%3A%20%20%20Flight%20Trajectory%20Prediction%20%28FTP%29%20is%20an%20essential%20task%20in%20Air%20Traffic%0AControl%20%28ATC%29%2C%20which%20can%20assist%20air%20traffic%20controllers%20in%20managing%20airspace%0Amore%20safely%20and%20efficiently.%20Existing%20approaches%20generally%20perform%0Amulti-horizon%20FTP%20tasks%20in%20an%20autoregressive%20manner%2C%20thereby%20suffering%20from%0Aerror%20accumulation%20and%20low-efficiency%20problems.%20In%20this%20paper%2C%20a%20novel%0Aframework%2C%20called%20FlightBERT%2B%2B%2C%20is%20proposed%20to%20i%29%20forecast%20multi-horizon%20flight%0Atrajectories%20directly%20in%20a%20non-autoregressive%20way%2C%20and%20ii%29%20improve%20the%0Alimitation%20of%20the%20binary%20encoding%20%28BE%29%20representation%20in%20the%20FlightBERT%0Aframework.%20Specifically%2C%20the%20proposed%20framework%20is%20implemented%20by%20a%20generalized%0Aencoder-decoder%20architecture%2C%20in%20which%20the%20encoder%20learns%20the%20temporal-spatial%0Apatterns%20from%20historical%20observations%20and%20the%20decoder%20predicts%20the%20flight%0Astatus%20for%20the%20future%20horizons.%20Compared%20to%20conventional%20architecture%2C%20an%0Ainnovative%20horizon-aware%20contexts%20generator%20is%20dedicatedly%20designed%20to%20consider%0Athe%20prior%20horizon%20information%2C%20which%20further%20enables%20non-autoregressive%0Amulti-horizon%20prediction.%20Additionally%2C%20the%20Gray%20code%20representation%20and%20the%0Adifferential%20prediction%20paradigm%20are%20designed%20to%20cope%20with%20the%20high-bit%0Amisclassifications%20of%20the%20BE%20representation%2C%20which%20significantly%20reduces%20the%0Aoutliers%20in%20the%20predictions.%20Moreover%2C%20a%20differential%20prompted%20decoder%20is%0Aproposed%20to%20enhance%20the%20capability%20of%20the%20differential%20predictions%20by%0Aleveraging%20the%20stationarity%20of%20the%20differential%20sequence.%20Extensive%20experiments%0Aare%20conducted%20to%20validate%20the%20proposed%20framework%20on%20a%20real-world%20flight%0Atrajectory%20dataset.%20The%20experimental%20results%20demonstrated%20that%20the%20proposed%0Aframework%20outperformed%20the%20competitive%20baselines%20in%20both%20FTP%20performance%20and%0Acomputational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.01658v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Non-autoregressive%2520Multi-Horizon%2520Flight%2520Trajectory%2520Prediction%250A%2520%2520Framework%2520with%2520Gray%2520Code%2520Representation%26entry.906535625%3DDongyue%2520Guo%2520and%2520Zheng%2520Zhang%2520and%2520Zhen%2520Yan%2520and%2520Jianwei%2520Zhang%2520and%2520Yi%2520Lin%26entry.1292438233%3D%2520%2520Flight%2520Trajectory%2520Prediction%2520%2528FTP%2529%2520is%2520an%2520essential%2520task%2520in%2520Air%2520Traffic%250AControl%2520%2528ATC%2529%252C%2520which%2520can%2520assist%2520air%2520traffic%2520controllers%2520in%2520managing%2520airspace%250Amore%2520safely%2520and%2520efficiently.%2520Existing%2520approaches%2520generally%2520perform%250Amulti-horizon%2520FTP%2520tasks%2520in%2520an%2520autoregressive%2520manner%252C%2520thereby%2520suffering%2520from%250Aerror%2520accumulation%2520and%2520low-efficiency%2520problems.%2520In%2520this%2520paper%252C%2520a%2520novel%250Aframework%252C%2520called%2520FlightBERT%252B%252B%252C%2520is%2520proposed%2520to%2520i%2529%2520forecast%2520multi-horizon%2520flight%250Atrajectories%2520directly%2520in%2520a%2520non-autoregressive%2520way%252C%2520and%2520ii%2529%2520improve%2520the%250Alimitation%2520of%2520the%2520binary%2520encoding%2520%2528BE%2529%2520representation%2520in%2520the%2520FlightBERT%250Aframework.%2520Specifically%252C%2520the%2520proposed%2520framework%2520is%2520implemented%2520by%2520a%2520generalized%250Aencoder-decoder%2520architecture%252C%2520in%2520which%2520the%2520encoder%2520learns%2520the%2520temporal-spatial%250Apatterns%2520from%2520historical%2520observations%2520and%2520the%2520decoder%2520predicts%2520the%2520flight%250Astatus%2520for%2520the%2520future%2520horizons.%2520Compared%2520to%2520conventional%2520architecture%252C%2520an%250Ainnovative%2520horizon-aware%2520contexts%2520generator%2520is%2520dedicatedly%2520designed%2520to%2520consider%250Athe%2520prior%2520horizon%2520information%252C%2520which%2520further%2520enables%2520non-autoregressive%250Amulti-horizon%2520prediction.%2520Additionally%252C%2520the%2520Gray%2520code%2520representation%2520and%2520the%250Adifferential%2520prediction%2520paradigm%2520are%2520designed%2520to%2520cope%2520with%2520the%2520high-bit%250Amisclassifications%2520of%2520the%2520BE%2520representation%252C%2520which%2520significantly%2520reduces%2520the%250Aoutliers%2520in%2520the%2520predictions.%2520Moreover%252C%2520a%2520differential%2520prompted%2520decoder%2520is%250Aproposed%2520to%2520enhance%2520the%2520capability%2520of%2520the%2520differential%2520predictions%2520by%250Aleveraging%2520the%2520stationarity%2520of%2520the%2520differential%2520sequence.%2520Extensive%2520experiments%250Aare%2520conducted%2520to%2520validate%2520the%2520proposed%2520framework%2520on%2520a%2520real-world%2520flight%250Atrajectory%2520dataset.%2520The%2520experimental%2520results%2520demonstrated%2520that%2520the%2520proposed%250Aframework%2520outperformed%2520the%2520competitive%2520baselines%2520in%2520both%2520FTP%2520performance%2520and%250Acomputational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.01658v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Non-autoregressive%20Multi-Horizon%20Flight%20Trajectory%20Prediction%0A%20%20Framework%20with%20Gray%20Code%20Representation&entry.906535625=Dongyue%20Guo%20and%20Zheng%20Zhang%20and%20Zhen%20Yan%20and%20Jianwei%20Zhang%20and%20Yi%20Lin&entry.1292438233=%20%20Flight%20Trajectory%20Prediction%20%28FTP%29%20is%20an%20essential%20task%20in%20Air%20Traffic%0AControl%20%28ATC%29%2C%20which%20can%20assist%20air%20traffic%20controllers%20in%20managing%20airspace%0Amore%20safely%20and%20efficiently.%20Existing%20approaches%20generally%20perform%0Amulti-horizon%20FTP%20tasks%20in%20an%20autoregressive%20manner%2C%20thereby%20suffering%20from%0Aerror%20accumulation%20and%20low-efficiency%20problems.%20In%20this%20paper%2C%20a%20novel%0Aframework%2C%20called%20FlightBERT%2B%2B%2C%20is%20proposed%20to%20i%29%20forecast%20multi-horizon%20flight%0Atrajectories%20directly%20in%20a%20non-autoregressive%20way%2C%20and%20ii%29%20improve%20the%0Alimitation%20of%20the%20binary%20encoding%20%28BE%29%20representation%20in%20the%20FlightBERT%0Aframework.%20Specifically%2C%20the%20proposed%20framework%20is%20implemented%20by%20a%20generalized%0Aencoder-decoder%20architecture%2C%20in%20which%20the%20encoder%20learns%20the%20temporal-spatial%0Apatterns%20from%20historical%20observations%20and%20the%20decoder%20predicts%20the%20flight%0Astatus%20for%20the%20future%20horizons.%20Compared%20to%20conventional%20architecture%2C%20an%0Ainnovative%20horizon-aware%20contexts%20generator%20is%20dedicatedly%20designed%20to%20consider%0Athe%20prior%20horizon%20information%2C%20which%20further%20enables%20non-autoregressive%0Amulti-horizon%20prediction.%20Additionally%2C%20the%20Gray%20code%20representation%20and%20the%0Adifferential%20prediction%20paradigm%20are%20designed%20to%20cope%20with%20the%20high-bit%0Amisclassifications%20of%20the%20BE%20representation%2C%20which%20significantly%20reduces%20the%0Aoutliers%20in%20the%20predictions.%20Moreover%2C%20a%20differential%20prompted%20decoder%20is%0Aproposed%20to%20enhance%20the%20capability%20of%20the%20differential%20predictions%20by%0Aleveraging%20the%20stationarity%20of%20the%20differential%20sequence.%20Extensive%20experiments%0Aare%20conducted%20to%20validate%20the%20proposed%20framework%20on%20a%20real-world%20flight%0Atrajectory%20dataset.%20The%20experimental%20results%20demonstrated%20that%20the%20proposed%0Aframework%20outperformed%20the%20competitive%20baselines%20in%20both%20FTP%20performance%20and%0Acomputational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.01658v4&entry.124074799=Read"},
{"title": "Confidence Aware Inverse Constrained Reinforcement Learning", "author": "Sriram Ganapathi Subramanian and Guiliang Liu and Mohammed Elmahgiubi and Kasra Rezaee and Pascal Poupart", "abstract": "  In coming up with solutions to real-world problems, humans implicitly adhere\nto constraints that are too numerous and complex to be specified completely.\nHowever, reinforcement learning (RL) agents need these constraints to learn the\ncorrect optimal policy in these settings. The field of Inverse Constraint\nReinforcement Learning (ICRL) deals with this problem and provides algorithms\nthat aim to estimate the constraints from expert demonstrations collected\noffline. Practitioners prefer to know a measure of confidence in the estimated\nconstraints, before deciding to use these constraints, which allows them to\nonly use the constraints that satisfy a desired level of confidence. However,\nprior works do not allow users to provide the desired level of confidence for\nthe inferred constraints. This work provides a principled ICRL method that can\ntake a confidence level with a set of expert demonstrations and outputs a\nconstraint that is at least as constraining as the true underlying constraint\nwith the desired level of confidence. Further, unlike previous methods, this\nmethod allows a user to know if the number of expert trajectories is\ninsufficient to learn a constraint with a desired level of confidence, and\ntherefore collect more expert trajectories as required to simultaneously learn\nconstraints with the desired level of confidence and a policy that achieves the\ndesired level of performance.\n", "link": "http://arxiv.org/abs/2406.16782v1", "date": "2024-06-24", "relevancy": 2.0804, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.53}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5197}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Confidence%20Aware%20Inverse%20Constrained%20Reinforcement%20Learning&body=Title%3A%20Confidence%20Aware%20Inverse%20Constrained%20Reinforcement%20Learning%0AAuthor%3A%20Sriram%20Ganapathi%20Subramanian%20and%20Guiliang%20Liu%20and%20Mohammed%20Elmahgiubi%20and%20Kasra%20Rezaee%20and%20Pascal%20Poupart%0AAbstract%3A%20%20%20In%20coming%20up%20with%20solutions%20to%20real-world%20problems%2C%20humans%20implicitly%20adhere%0Ato%20constraints%20that%20are%20too%20numerous%20and%20complex%20to%20be%20specified%20completely.%0AHowever%2C%20reinforcement%20learning%20%28RL%29%20agents%20need%20these%20constraints%20to%20learn%20the%0Acorrect%20optimal%20policy%20in%20these%20settings.%20The%20field%20of%20Inverse%20Constraint%0AReinforcement%20Learning%20%28ICRL%29%20deals%20with%20this%20problem%20and%20provides%20algorithms%0Athat%20aim%20to%20estimate%20the%20constraints%20from%20expert%20demonstrations%20collected%0Aoffline.%20Practitioners%20prefer%20to%20know%20a%20measure%20of%20confidence%20in%20the%20estimated%0Aconstraints%2C%20before%20deciding%20to%20use%20these%20constraints%2C%20which%20allows%20them%20to%0Aonly%20use%20the%20constraints%20that%20satisfy%20a%20desired%20level%20of%20confidence.%20However%2C%0Aprior%20works%20do%20not%20allow%20users%20to%20provide%20the%20desired%20level%20of%20confidence%20for%0Athe%20inferred%20constraints.%20This%20work%20provides%20a%20principled%20ICRL%20method%20that%20can%0Atake%20a%20confidence%20level%20with%20a%20set%20of%20expert%20demonstrations%20and%20outputs%20a%0Aconstraint%20that%20is%20at%20least%20as%20constraining%20as%20the%20true%20underlying%20constraint%0Awith%20the%20desired%20level%20of%20confidence.%20Further%2C%20unlike%20previous%20methods%2C%20this%0Amethod%20allows%20a%20user%20to%20know%20if%20the%20number%20of%20expert%20trajectories%20is%0Ainsufficient%20to%20learn%20a%20constraint%20with%20a%20desired%20level%20of%20confidence%2C%20and%0Atherefore%20collect%20more%20expert%20trajectories%20as%20required%20to%20simultaneously%20learn%0Aconstraints%20with%20the%20desired%20level%20of%20confidence%20and%20a%20policy%20that%20achieves%20the%0Adesired%20level%20of%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConfidence%2520Aware%2520Inverse%2520Constrained%2520Reinforcement%2520Learning%26entry.906535625%3DSriram%2520Ganapathi%2520Subramanian%2520and%2520Guiliang%2520Liu%2520and%2520Mohammed%2520Elmahgiubi%2520and%2520Kasra%2520Rezaee%2520and%2520Pascal%2520Poupart%26entry.1292438233%3D%2520%2520In%2520coming%2520up%2520with%2520solutions%2520to%2520real-world%2520problems%252C%2520humans%2520implicitly%2520adhere%250Ato%2520constraints%2520that%2520are%2520too%2520numerous%2520and%2520complex%2520to%2520be%2520specified%2520completely.%250AHowever%252C%2520reinforcement%2520learning%2520%2528RL%2529%2520agents%2520need%2520these%2520constraints%2520to%2520learn%2520the%250Acorrect%2520optimal%2520policy%2520in%2520these%2520settings.%2520The%2520field%2520of%2520Inverse%2520Constraint%250AReinforcement%2520Learning%2520%2528ICRL%2529%2520deals%2520with%2520this%2520problem%2520and%2520provides%2520algorithms%250Athat%2520aim%2520to%2520estimate%2520the%2520constraints%2520from%2520expert%2520demonstrations%2520collected%250Aoffline.%2520Practitioners%2520prefer%2520to%2520know%2520a%2520measure%2520of%2520confidence%2520in%2520the%2520estimated%250Aconstraints%252C%2520before%2520deciding%2520to%2520use%2520these%2520constraints%252C%2520which%2520allows%2520them%2520to%250Aonly%2520use%2520the%2520constraints%2520that%2520satisfy%2520a%2520desired%2520level%2520of%2520confidence.%2520However%252C%250Aprior%2520works%2520do%2520not%2520allow%2520users%2520to%2520provide%2520the%2520desired%2520level%2520of%2520confidence%2520for%250Athe%2520inferred%2520constraints.%2520This%2520work%2520provides%2520a%2520principled%2520ICRL%2520method%2520that%2520can%250Atake%2520a%2520confidence%2520level%2520with%2520a%2520set%2520of%2520expert%2520demonstrations%2520and%2520outputs%2520a%250Aconstraint%2520that%2520is%2520at%2520least%2520as%2520constraining%2520as%2520the%2520true%2520underlying%2520constraint%250Awith%2520the%2520desired%2520level%2520of%2520confidence.%2520Further%252C%2520unlike%2520previous%2520methods%252C%2520this%250Amethod%2520allows%2520a%2520user%2520to%2520know%2520if%2520the%2520number%2520of%2520expert%2520trajectories%2520is%250Ainsufficient%2520to%2520learn%2520a%2520constraint%2520with%2520a%2520desired%2520level%2520of%2520confidence%252C%2520and%250Atherefore%2520collect%2520more%2520expert%2520trajectories%2520as%2520required%2520to%2520simultaneously%2520learn%250Aconstraints%2520with%2520the%2520desired%2520level%2520of%2520confidence%2520and%2520a%2520policy%2520that%2520achieves%2520the%250Adesired%2520level%2520of%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Confidence%20Aware%20Inverse%20Constrained%20Reinforcement%20Learning&entry.906535625=Sriram%20Ganapathi%20Subramanian%20and%20Guiliang%20Liu%20and%20Mohammed%20Elmahgiubi%20and%20Kasra%20Rezaee%20and%20Pascal%20Poupart&entry.1292438233=%20%20In%20coming%20up%20with%20solutions%20to%20real-world%20problems%2C%20humans%20implicitly%20adhere%0Ato%20constraints%20that%20are%20too%20numerous%20and%20complex%20to%20be%20specified%20completely.%0AHowever%2C%20reinforcement%20learning%20%28RL%29%20agents%20need%20these%20constraints%20to%20learn%20the%0Acorrect%20optimal%20policy%20in%20these%20settings.%20The%20field%20of%20Inverse%20Constraint%0AReinforcement%20Learning%20%28ICRL%29%20deals%20with%20this%20problem%20and%20provides%20algorithms%0Athat%20aim%20to%20estimate%20the%20constraints%20from%20expert%20demonstrations%20collected%0Aoffline.%20Practitioners%20prefer%20to%20know%20a%20measure%20of%20confidence%20in%20the%20estimated%0Aconstraints%2C%20before%20deciding%20to%20use%20these%20constraints%2C%20which%20allows%20them%20to%0Aonly%20use%20the%20constraints%20that%20satisfy%20a%20desired%20level%20of%20confidence.%20However%2C%0Aprior%20works%20do%20not%20allow%20users%20to%20provide%20the%20desired%20level%20of%20confidence%20for%0Athe%20inferred%20constraints.%20This%20work%20provides%20a%20principled%20ICRL%20method%20that%20can%0Atake%20a%20confidence%20level%20with%20a%20set%20of%20expert%20demonstrations%20and%20outputs%20a%0Aconstraint%20that%20is%20at%20least%20as%20constraining%20as%20the%20true%20underlying%20constraint%0Awith%20the%20desired%20level%20of%20confidence.%20Further%2C%20unlike%20previous%20methods%2C%20this%0Amethod%20allows%20a%20user%20to%20know%20if%20the%20number%20of%20expert%20trajectories%20is%0Ainsufficient%20to%20learn%20a%20constraint%20with%20a%20desired%20level%20of%20confidence%2C%20and%0Atherefore%20collect%20more%20expert%20trajectories%20as%20required%20to%20simultaneously%20learn%0Aconstraints%20with%20the%20desired%20level%20of%20confidence%20and%20a%20policy%20that%20achieves%20the%0Adesired%20level%20of%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16782v1&entry.124074799=Read"},
{"title": "Validation of ML-UQ calibration statistics using simulated reference\n  values: a sensitivity analysis", "author": "Pascal Pernot", "abstract": "  Some popular Machine Learning Uncertainty Quantification (ML-UQ) calibration\nstatistics do not have predefined reference values and are mostly used in\ncomparative studies. In consequence, calibration is almost never validated and\nthe diagnostic is left to the appreciation of the reader. Simulated reference\nvalues, based on synthetic calibrated datasets derived from actual\nuncertainties, have been proposed to palliate this problem. As the generative\nprobability distribution for the simulation of synthetic errors is often not\nconstrained, the sensitivity of simulated reference values to the choice of\ngenerative distribution might be problematic, shedding a doubt on the\ncalibration diagnostic. This study explores various facets of this problem, and\nshows that some statistics are excessively sensitive to the choice of\ngenerative distribution to be used for validation when the generative\ndistribution is unknown. This is the case, for instance, of the correlation\ncoefficient between absolute errors and uncertainties (CC) and of the expected\nnormalized calibration error (ENCE). A robust validation workflow to deal with\nsimulated reference values is proposed.\n", "link": "http://arxiv.org/abs/2403.00423v2", "date": "2024-06-24", "relevancy": 1.457, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5159}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4912}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Validation%20of%20ML-UQ%20calibration%20statistics%20using%20simulated%20reference%0A%20%20values%3A%20a%20sensitivity%20analysis&body=Title%3A%20Validation%20of%20ML-UQ%20calibration%20statistics%20using%20simulated%20reference%0A%20%20values%3A%20a%20sensitivity%20analysis%0AAuthor%3A%20Pascal%20Pernot%0AAbstract%3A%20%20%20Some%20popular%20Machine%20Learning%20Uncertainty%20Quantification%20%28ML-UQ%29%20calibration%0Astatistics%20do%20not%20have%20predefined%20reference%20values%20and%20are%20mostly%20used%20in%0Acomparative%20studies.%20In%20consequence%2C%20calibration%20is%20almost%20never%20validated%20and%0Athe%20diagnostic%20is%20left%20to%20the%20appreciation%20of%20the%20reader.%20Simulated%20reference%0Avalues%2C%20based%20on%20synthetic%20calibrated%20datasets%20derived%20from%20actual%0Auncertainties%2C%20have%20been%20proposed%20to%20palliate%20this%20problem.%20As%20the%20generative%0Aprobability%20distribution%20for%20the%20simulation%20of%20synthetic%20errors%20is%20often%20not%0Aconstrained%2C%20the%20sensitivity%20of%20simulated%20reference%20values%20to%20the%20choice%20of%0Agenerative%20distribution%20might%20be%20problematic%2C%20shedding%20a%20doubt%20on%20the%0Acalibration%20diagnostic.%20This%20study%20explores%20various%20facets%20of%20this%20problem%2C%20and%0Ashows%20that%20some%20statistics%20are%20excessively%20sensitive%20to%20the%20choice%20of%0Agenerative%20distribution%20to%20be%20used%20for%20validation%20when%20the%20generative%0Adistribution%20is%20unknown.%20This%20is%20the%20case%2C%20for%20instance%2C%20of%20the%20correlation%0Acoefficient%20between%20absolute%20errors%20and%20uncertainties%20%28CC%29%20and%20of%20the%20expected%0Anormalized%20calibration%20error%20%28ENCE%29.%20A%20robust%20validation%20workflow%20to%20deal%20with%0Asimulated%20reference%20values%20is%20proposed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00423v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DValidation%2520of%2520ML-UQ%2520calibration%2520statistics%2520using%2520simulated%2520reference%250A%2520%2520values%253A%2520a%2520sensitivity%2520analysis%26entry.906535625%3DPascal%2520Pernot%26entry.1292438233%3D%2520%2520Some%2520popular%2520Machine%2520Learning%2520Uncertainty%2520Quantification%2520%2528ML-UQ%2529%2520calibration%250Astatistics%2520do%2520not%2520have%2520predefined%2520reference%2520values%2520and%2520are%2520mostly%2520used%2520in%250Acomparative%2520studies.%2520In%2520consequence%252C%2520calibration%2520is%2520almost%2520never%2520validated%2520and%250Athe%2520diagnostic%2520is%2520left%2520to%2520the%2520appreciation%2520of%2520the%2520reader.%2520Simulated%2520reference%250Avalues%252C%2520based%2520on%2520synthetic%2520calibrated%2520datasets%2520derived%2520from%2520actual%250Auncertainties%252C%2520have%2520been%2520proposed%2520to%2520palliate%2520this%2520problem.%2520As%2520the%2520generative%250Aprobability%2520distribution%2520for%2520the%2520simulation%2520of%2520synthetic%2520errors%2520is%2520often%2520not%250Aconstrained%252C%2520the%2520sensitivity%2520of%2520simulated%2520reference%2520values%2520to%2520the%2520choice%2520of%250Agenerative%2520distribution%2520might%2520be%2520problematic%252C%2520shedding%2520a%2520doubt%2520on%2520the%250Acalibration%2520diagnostic.%2520This%2520study%2520explores%2520various%2520facets%2520of%2520this%2520problem%252C%2520and%250Ashows%2520that%2520some%2520statistics%2520are%2520excessively%2520sensitive%2520to%2520the%2520choice%2520of%250Agenerative%2520distribution%2520to%2520be%2520used%2520for%2520validation%2520when%2520the%2520generative%250Adistribution%2520is%2520unknown.%2520This%2520is%2520the%2520case%252C%2520for%2520instance%252C%2520of%2520the%2520correlation%250Acoefficient%2520between%2520absolute%2520errors%2520and%2520uncertainties%2520%2528CC%2529%2520and%2520of%2520the%2520expected%250Anormalized%2520calibration%2520error%2520%2528ENCE%2529.%2520A%2520robust%2520validation%2520workflow%2520to%2520deal%2520with%250Asimulated%2520reference%2520values%2520is%2520proposed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.00423v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Validation%20of%20ML-UQ%20calibration%20statistics%20using%20simulated%20reference%0A%20%20values%3A%20a%20sensitivity%20analysis&entry.906535625=Pascal%20Pernot&entry.1292438233=%20%20Some%20popular%20Machine%20Learning%20Uncertainty%20Quantification%20%28ML-UQ%29%20calibration%0Astatistics%20do%20not%20have%20predefined%20reference%20values%20and%20are%20mostly%20used%20in%0Acomparative%20studies.%20In%20consequence%2C%20calibration%20is%20almost%20never%20validated%20and%0Athe%20diagnostic%20is%20left%20to%20the%20appreciation%20of%20the%20reader.%20Simulated%20reference%0Avalues%2C%20based%20on%20synthetic%20calibrated%20datasets%20derived%20from%20actual%0Auncertainties%2C%20have%20been%20proposed%20to%20palliate%20this%20problem.%20As%20the%20generative%0Aprobability%20distribution%20for%20the%20simulation%20of%20synthetic%20errors%20is%20often%20not%0Aconstrained%2C%20the%20sensitivity%20of%20simulated%20reference%20values%20to%20the%20choice%20of%0Agenerative%20distribution%20might%20be%20problematic%2C%20shedding%20a%20doubt%20on%20the%0Acalibration%20diagnostic.%20This%20study%20explores%20various%20facets%20of%20this%20problem%2C%20and%0Ashows%20that%20some%20statistics%20are%20excessively%20sensitive%20to%20the%20choice%20of%0Agenerative%20distribution%20to%20be%20used%20for%20validation%20when%20the%20generative%0Adistribution%20is%20unknown.%20This%20is%20the%20case%2C%20for%20instance%2C%20of%20the%20correlation%0Acoefficient%20between%20absolute%20errors%20and%20uncertainties%20%28CC%29%20and%20of%20the%20expected%0Anormalized%20calibration%20error%20%28ENCE%29.%20A%20robust%20validation%20workflow%20to%20deal%20with%0Asimulated%20reference%20values%20is%20proposed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00423v2&entry.124074799=Read"},
{"title": "Fusion of Movement and Naive Predictions for Point Forecasting in\n  Univariate Random Walks", "author": "Cheng Zhang", "abstract": "  Traditional methods for point forecasting in univariate random walks often\nfail to surpass naive benchmarks due to data unpredictability. This study\nintroduces a novel forecasting method that fuses movement prediction (binary\nclassification) with naive forecasts for accurate one-step-ahead point\nforecasting. The method's efficacy is demonstrated through theoretical\nanalysis, simulations, and real-world data experiments. It reliably exceeds\nnaive forecasts with movement prediction accuracies as low as 0.55,\noutperforming baseline models like ARIMA, linear regression, MLP, and LSTM\nnetworks in forecasting the S\\&P 500 index and Bitcoin prices. This method is\nparticularly advantageous when accurate point predictions are challenging but\naccurate movement predictions are attainable, translating movement predictions\ninto point forecasts in random walk contexts.\n", "link": "http://arxiv.org/abs/2406.14469v2", "date": "2024-06-24", "relevancy": 1.3967, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5135}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4609}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fusion%20of%20Movement%20and%20Naive%20Predictions%20for%20Point%20Forecasting%20in%0A%20%20Univariate%20Random%20Walks&body=Title%3A%20Fusion%20of%20Movement%20and%20Naive%20Predictions%20for%20Point%20Forecasting%20in%0A%20%20Univariate%20Random%20Walks%0AAuthor%3A%20Cheng%20Zhang%0AAbstract%3A%20%20%20Traditional%20methods%20for%20point%20forecasting%20in%20univariate%20random%20walks%20often%0Afail%20to%20surpass%20naive%20benchmarks%20due%20to%20data%20unpredictability.%20This%20study%0Aintroduces%20a%20novel%20forecasting%20method%20that%20fuses%20movement%20prediction%20%28binary%0Aclassification%29%20with%20naive%20forecasts%20for%20accurate%20one-step-ahead%20point%0Aforecasting.%20The%20method%27s%20efficacy%20is%20demonstrated%20through%20theoretical%0Aanalysis%2C%20simulations%2C%20and%20real-world%20data%20experiments.%20It%20reliably%20exceeds%0Anaive%20forecasts%20with%20movement%20prediction%20accuracies%20as%20low%20as%200.55%2C%0Aoutperforming%20baseline%20models%20like%20ARIMA%2C%20linear%20regression%2C%20MLP%2C%20and%20LSTM%0Anetworks%20in%20forecasting%20the%20S%5C%26P%20500%20index%20and%20Bitcoin%20prices.%20This%20method%20is%0Aparticularly%20advantageous%20when%20accurate%20point%20predictions%20are%20challenging%20but%0Aaccurate%20movement%20predictions%20are%20attainable%2C%20translating%20movement%20predictions%0Ainto%20point%20forecasts%20in%20random%20walk%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14469v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusion%2520of%2520Movement%2520and%2520Naive%2520Predictions%2520for%2520Point%2520Forecasting%2520in%250A%2520%2520Univariate%2520Random%2520Walks%26entry.906535625%3DCheng%2520Zhang%26entry.1292438233%3D%2520%2520Traditional%2520methods%2520for%2520point%2520forecasting%2520in%2520univariate%2520random%2520walks%2520often%250Afail%2520to%2520surpass%2520naive%2520benchmarks%2520due%2520to%2520data%2520unpredictability.%2520This%2520study%250Aintroduces%2520a%2520novel%2520forecasting%2520method%2520that%2520fuses%2520movement%2520prediction%2520%2528binary%250Aclassification%2529%2520with%2520naive%2520forecasts%2520for%2520accurate%2520one-step-ahead%2520point%250Aforecasting.%2520The%2520method%2527s%2520efficacy%2520is%2520demonstrated%2520through%2520theoretical%250Aanalysis%252C%2520simulations%252C%2520and%2520real-world%2520data%2520experiments.%2520It%2520reliably%2520exceeds%250Anaive%2520forecasts%2520with%2520movement%2520prediction%2520accuracies%2520as%2520low%2520as%25200.55%252C%250Aoutperforming%2520baseline%2520models%2520like%2520ARIMA%252C%2520linear%2520regression%252C%2520MLP%252C%2520and%2520LSTM%250Anetworks%2520in%2520forecasting%2520the%2520S%255C%2526P%2520500%2520index%2520and%2520Bitcoin%2520prices.%2520This%2520method%2520is%250Aparticularly%2520advantageous%2520when%2520accurate%2520point%2520predictions%2520are%2520challenging%2520but%250Aaccurate%2520movement%2520predictions%2520are%2520attainable%252C%2520translating%2520movement%2520predictions%250Ainto%2520point%2520forecasts%2520in%2520random%2520walk%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14469v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusion%20of%20Movement%20and%20Naive%20Predictions%20for%20Point%20Forecasting%20in%0A%20%20Univariate%20Random%20Walks&entry.906535625=Cheng%20Zhang&entry.1292438233=%20%20Traditional%20methods%20for%20point%20forecasting%20in%20univariate%20random%20walks%20often%0Afail%20to%20surpass%20naive%20benchmarks%20due%20to%20data%20unpredictability.%20This%20study%0Aintroduces%20a%20novel%20forecasting%20method%20that%20fuses%20movement%20prediction%20%28binary%0Aclassification%29%20with%20naive%20forecasts%20for%20accurate%20one-step-ahead%20point%0Aforecasting.%20The%20method%27s%20efficacy%20is%20demonstrated%20through%20theoretical%0Aanalysis%2C%20simulations%2C%20and%20real-world%20data%20experiments.%20It%20reliably%20exceeds%0Anaive%20forecasts%20with%20movement%20prediction%20accuracies%20as%20low%20as%200.55%2C%0Aoutperforming%20baseline%20models%20like%20ARIMA%2C%20linear%20regression%2C%20MLP%2C%20and%20LSTM%0Anetworks%20in%20forecasting%20the%20S%5C%26P%20500%20index%20and%20Bitcoin%20prices.%20This%20method%20is%0Aparticularly%20advantageous%20when%20accurate%20point%20predictions%20are%20challenging%20but%0Aaccurate%20movement%20predictions%20are%20attainable%2C%20translating%20movement%20predictions%0Ainto%20point%20forecasts%20in%20random%20walk%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14469v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


