<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240513.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging\n  2D and 3D Diffusion Models", "author": "Taoran Yi and Jiemin Fang and Junjie Wang and Guanjun Wu and Lingxi Xie and Xiaopeng Zhang and Wenyu Liu and Qi Tian and Xinggang Wang", "abstract": "  In recent times, the generation of 3D assets from text prompts has shown\nimpressive results. Both 2D and 3D diffusion models can help generate decent 3D\nobjects based on prompts. 3D diffusion models have good 3D consistency, but\ntheir quality and generalization are limited as trainable 3D data is expensive\nand hard to obtain. 2D diffusion models enjoy strong abilities of\ngeneralization and fine generation, but 3D consistency is hard to guarantee.\nThis paper attempts to bridge the power from the two types of diffusion models\nvia the recent explicit and efficient 3D Gaussian splatting representation. A\nfast 3D object generation framework, named as GaussianDreamer, is proposed,\nwhere the 3D diffusion model provides priors for initialization and the 2D\ndiffusion model enriches the geometry and appearance. Operations of noisy point\ngrowing and color perturbation are introduced to enhance the initialized\nGaussians. Our GaussianDreamer can generate a high-quality 3D instance or 3D\navatar within 15 minutes on one GPU, much faster than previous methods, while\nthe generated instances can be directly rendered in real time. Demos and code\nare available at https://taoranyi.com/gaussiandreamer/.\n", "link": "http://arxiv.org/abs/2310.08529v3", "date": "2024-05-13", "relevancy": 3.1187, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6449}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6138}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianDreamer%3A%20Fast%20Generation%20from%20Text%20to%203D%20Gaussians%20by%20Bridging%0A%20%202D%20and%203D%20Diffusion%20Models&body=Title%3A%20GaussianDreamer%3A%20Fast%20Generation%20from%20Text%20to%203D%20Gaussians%20by%20Bridging%0A%20%202D%20and%203D%20Diffusion%20Models%0AAuthor%3A%20Taoran%20Yi%20and%20Jiemin%20Fang%20and%20Junjie%20Wang%20and%20Guanjun%20Wu%20and%20Lingxi%20Xie%20and%20Xiaopeng%20Zhang%20and%20Wenyu%20Liu%20and%20Qi%20Tian%20and%20Xinggang%20Wang%0AAbstract%3A%20%20%20In%20recent%20times%2C%20the%20generation%20of%203D%20assets%20from%20text%20prompts%20has%20shown%0Aimpressive%20results.%20Both%202D%20and%203D%20diffusion%20models%20can%20help%20generate%20decent%203D%0Aobjects%20based%20on%20prompts.%203D%20diffusion%20models%20have%20good%203D%20consistency%2C%20but%0Atheir%20quality%20and%20generalization%20are%20limited%20as%20trainable%203D%20data%20is%20expensive%0Aand%20hard%20to%20obtain.%202D%20diffusion%20models%20enjoy%20strong%20abilities%20of%0Ageneralization%20and%20fine%20generation%2C%20but%203D%20consistency%20is%20hard%20to%20guarantee.%0AThis%20paper%20attempts%20to%20bridge%20the%20power%20from%20the%20two%20types%20of%20diffusion%20models%0Avia%20the%20recent%20explicit%20and%20efficient%203D%20Gaussian%20splatting%20representation.%20A%0Afast%203D%20object%20generation%20framework%2C%20named%20as%20GaussianDreamer%2C%20is%20proposed%2C%0Awhere%20the%203D%20diffusion%20model%20provides%20priors%20for%20initialization%20and%20the%202D%0Adiffusion%20model%20enriches%20the%20geometry%20and%20appearance.%20Operations%20of%20noisy%20point%0Agrowing%20and%20color%20perturbation%20are%20introduced%20to%20enhance%20the%20initialized%0AGaussians.%20Our%20GaussianDreamer%20can%20generate%20a%20high-quality%203D%20instance%20or%203D%0Aavatar%20within%2015%20minutes%20on%20one%20GPU%2C%20much%20faster%20than%20previous%20methods%2C%20while%0Athe%20generated%20instances%20can%20be%20directly%20rendered%20in%20real%20time.%20Demos%20and%20code%0Aare%20available%20at%20https%3A//taoranyi.com/gaussiandreamer/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.08529v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianDreamer%253A%2520Fast%2520Generation%2520from%2520Text%2520to%25203D%2520Gaussians%2520by%2520Bridging%250A%2520%25202D%2520and%25203D%2520Diffusion%2520Models%26entry.906535625%3DTaoran%2520Yi%2520and%2520Jiemin%2520Fang%2520and%2520Junjie%2520Wang%2520and%2520Guanjun%2520Wu%2520and%2520Lingxi%2520Xie%2520and%2520Xiaopeng%2520Zhang%2520and%2520Wenyu%2520Liu%2520and%2520Qi%2520Tian%2520and%2520Xinggang%2520Wang%26entry.1292438233%3D%2520%2520In%2520recent%2520times%252C%2520the%2520generation%2520of%25203D%2520assets%2520from%2520text%2520prompts%2520has%2520shown%250Aimpressive%2520results.%2520Both%25202D%2520and%25203D%2520diffusion%2520models%2520can%2520help%2520generate%2520decent%25203D%250Aobjects%2520based%2520on%2520prompts.%25203D%2520diffusion%2520models%2520have%2520good%25203D%2520consistency%252C%2520but%250Atheir%2520quality%2520and%2520generalization%2520are%2520limited%2520as%2520trainable%25203D%2520data%2520is%2520expensive%250Aand%2520hard%2520to%2520obtain.%25202D%2520diffusion%2520models%2520enjoy%2520strong%2520abilities%2520of%250Ageneralization%2520and%2520fine%2520generation%252C%2520but%25203D%2520consistency%2520is%2520hard%2520to%2520guarantee.%250AThis%2520paper%2520attempts%2520to%2520bridge%2520the%2520power%2520from%2520the%2520two%2520types%2520of%2520diffusion%2520models%250Avia%2520the%2520recent%2520explicit%2520and%2520efficient%25203D%2520Gaussian%2520splatting%2520representation.%2520A%250Afast%25203D%2520object%2520generation%2520framework%252C%2520named%2520as%2520GaussianDreamer%252C%2520is%2520proposed%252C%250Awhere%2520the%25203D%2520diffusion%2520model%2520provides%2520priors%2520for%2520initialization%2520and%2520the%25202D%250Adiffusion%2520model%2520enriches%2520the%2520geometry%2520and%2520appearance.%2520Operations%2520of%2520noisy%2520point%250Agrowing%2520and%2520color%2520perturbation%2520are%2520introduced%2520to%2520enhance%2520the%2520initialized%250AGaussians.%2520Our%2520GaussianDreamer%2520can%2520generate%2520a%2520high-quality%25203D%2520instance%2520or%25203D%250Aavatar%2520within%252015%2520minutes%2520on%2520one%2520GPU%252C%2520much%2520faster%2520than%2520previous%2520methods%252C%2520while%250Athe%2520generated%2520instances%2520can%2520be%2520directly%2520rendered%2520in%2520real%2520time.%2520Demos%2520and%2520code%250Aare%2520available%2520at%2520https%253A//taoranyi.com/gaussiandreamer/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.08529v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianDreamer%3A%20Fast%20Generation%20from%20Text%20to%203D%20Gaussians%20by%20Bridging%0A%20%202D%20and%203D%20Diffusion%20Models&entry.906535625=Taoran%20Yi%20and%20Jiemin%20Fang%20and%20Junjie%20Wang%20and%20Guanjun%20Wu%20and%20Lingxi%20Xie%20and%20Xiaopeng%20Zhang%20and%20Wenyu%20Liu%20and%20Qi%20Tian%20and%20Xinggang%20Wang&entry.1292438233=%20%20In%20recent%20times%2C%20the%20generation%20of%203D%20assets%20from%20text%20prompts%20has%20shown%0Aimpressive%20results.%20Both%202D%20and%203D%20diffusion%20models%20can%20help%20generate%20decent%203D%0Aobjects%20based%20on%20prompts.%203D%20diffusion%20models%20have%20good%203D%20consistency%2C%20but%0Atheir%20quality%20and%20generalization%20are%20limited%20as%20trainable%203D%20data%20is%20expensive%0Aand%20hard%20to%20obtain.%202D%20diffusion%20models%20enjoy%20strong%20abilities%20of%0Ageneralization%20and%20fine%20generation%2C%20but%203D%20consistency%20is%20hard%20to%20guarantee.%0AThis%20paper%20attempts%20to%20bridge%20the%20power%20from%20the%20two%20types%20of%20diffusion%20models%0Avia%20the%20recent%20explicit%20and%20efficient%203D%20Gaussian%20splatting%20representation.%20A%0Afast%203D%20object%20generation%20framework%2C%20named%20as%20GaussianDreamer%2C%20is%20proposed%2C%0Awhere%20the%203D%20diffusion%20model%20provides%20priors%20for%20initialization%20and%20the%202D%0Adiffusion%20model%20enriches%20the%20geometry%20and%20appearance.%20Operations%20of%20noisy%20point%0Agrowing%20and%20color%20perturbation%20are%20introduced%20to%20enhance%20the%20initialized%0AGaussians.%20Our%20GaussianDreamer%20can%20generate%20a%20high-quality%203D%20instance%20or%203D%0Aavatar%20within%2015%20minutes%20on%20one%20GPU%2C%20much%20faster%20than%20previous%20methods%2C%20while%0Athe%20generated%20instances%20can%20be%20directly%20rendered%20in%20real%20time.%20Demos%20and%20code%0Aare%20available%20at%20https%3A//taoranyi.com/gaussiandreamer/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08529v3&entry.124074799=Read"},
{"title": "RGBD-Glue: General Feature Combination for Robust RGB-D Point Cloud\n  Registration", "author": "Congjia Chen and Xiaoyu Jia and Yanhong Zheng and Yufu Qu", "abstract": "  Point cloud registration is a fundamental task for estimating rigid\ntransformations between point clouds. Previous studies have used geometric\ninformation for extracting features, matching and estimating transformation.\nRecently, owing to the advancement of RGB-D sensors, researchers have attempted\nto utilize visual information to improve registration performance. However,\nthese studies focused on extracting distinctive features by deep feature\nfusion, which cannot effectively solve the negative effects of each feature's\nweakness, and cannot sufficiently leverage the valid information. In this\npaper, we propose a new feature combination framework, which applies a looser\nbut more effective fusion and can achieve better performance. An explicit\nfilter based on transformation consistency is designed for the combination\nframework, which can overcome each feature's weakness. And an adaptive\nthreshold determined by the error distribution is proposed to extract more\nvalid information from the two types of features. Owing to the distinctive\ndesign, our proposed framework can estimate more accurate correspondences and\nis applicable to both hand-crafted and learning-based feature descriptors.\nExperiments on ScanNet show that our method achieves a state-of-the-art\nperformance and the rotation accuracy of 99.1%.\n", "link": "http://arxiv.org/abs/2405.07594v1", "date": "2024-05-13", "relevancy": 2.8811, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5948}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5832}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGBD-Glue%3A%20General%20Feature%20Combination%20for%20Robust%20RGB-D%20Point%20Cloud%0A%20%20Registration&body=Title%3A%20RGBD-Glue%3A%20General%20Feature%20Combination%20for%20Robust%20RGB-D%20Point%20Cloud%0A%20%20Registration%0AAuthor%3A%20Congjia%20Chen%20and%20Xiaoyu%20Jia%20and%20Yanhong%20Zheng%20and%20Yufu%20Qu%0AAbstract%3A%20%20%20Point%20cloud%20registration%20is%20a%20fundamental%20task%20for%20estimating%20rigid%0Atransformations%20between%20point%20clouds.%20Previous%20studies%20have%20used%20geometric%0Ainformation%20for%20extracting%20features%2C%20matching%20and%20estimating%20transformation.%0ARecently%2C%20owing%20to%20the%20advancement%20of%20RGB-D%20sensors%2C%20researchers%20have%20attempted%0Ato%20utilize%20visual%20information%20to%20improve%20registration%20performance.%20However%2C%0Athese%20studies%20focused%20on%20extracting%20distinctive%20features%20by%20deep%20feature%0Afusion%2C%20which%20cannot%20effectively%20solve%20the%20negative%20effects%20of%20each%20feature%27s%0Aweakness%2C%20and%20cannot%20sufficiently%20leverage%20the%20valid%20information.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20feature%20combination%20framework%2C%20which%20applies%20a%20looser%0Abut%20more%20effective%20fusion%20and%20can%20achieve%20better%20performance.%20An%20explicit%0Afilter%20based%20on%20transformation%20consistency%20is%20designed%20for%20the%20combination%0Aframework%2C%20which%20can%20overcome%20each%20feature%27s%20weakness.%20And%20an%20adaptive%0Athreshold%20determined%20by%20the%20error%20distribution%20is%20proposed%20to%20extract%20more%0Avalid%20information%20from%20the%20two%20types%20of%20features.%20Owing%20to%20the%20distinctive%0Adesign%2C%20our%20proposed%20framework%20can%20estimate%20more%20accurate%20correspondences%20and%0Ais%20applicable%20to%20both%20hand-crafted%20and%20learning-based%20feature%20descriptors.%0AExperiments%20on%20ScanNet%20show%20that%20our%20method%20achieves%20a%20state-of-the-art%0Aperformance%20and%20the%20rotation%20accuracy%20of%2099.1%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGBD-Glue%253A%2520General%2520Feature%2520Combination%2520for%2520Robust%2520RGB-D%2520Point%2520Cloud%250A%2520%2520Registration%26entry.906535625%3DCongjia%2520Chen%2520and%2520Xiaoyu%2520Jia%2520and%2520Yanhong%2520Zheng%2520and%2520Yufu%2520Qu%26entry.1292438233%3D%2520%2520Point%2520cloud%2520registration%2520is%2520a%2520fundamental%2520task%2520for%2520estimating%2520rigid%250Atransformations%2520between%2520point%2520clouds.%2520Previous%2520studies%2520have%2520used%2520geometric%250Ainformation%2520for%2520extracting%2520features%252C%2520matching%2520and%2520estimating%2520transformation.%250ARecently%252C%2520owing%2520to%2520the%2520advancement%2520of%2520RGB-D%2520sensors%252C%2520researchers%2520have%2520attempted%250Ato%2520utilize%2520visual%2520information%2520to%2520improve%2520registration%2520performance.%2520However%252C%250Athese%2520studies%2520focused%2520on%2520extracting%2520distinctive%2520features%2520by%2520deep%2520feature%250Afusion%252C%2520which%2520cannot%2520effectively%2520solve%2520the%2520negative%2520effects%2520of%2520each%2520feature%2527s%250Aweakness%252C%2520and%2520cannot%2520sufficiently%2520leverage%2520the%2520valid%2520information.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520new%2520feature%2520combination%2520framework%252C%2520which%2520applies%2520a%2520looser%250Abut%2520more%2520effective%2520fusion%2520and%2520can%2520achieve%2520better%2520performance.%2520An%2520explicit%250Afilter%2520based%2520on%2520transformation%2520consistency%2520is%2520designed%2520for%2520the%2520combination%250Aframework%252C%2520which%2520can%2520overcome%2520each%2520feature%2527s%2520weakness.%2520And%2520an%2520adaptive%250Athreshold%2520determined%2520by%2520the%2520error%2520distribution%2520is%2520proposed%2520to%2520extract%2520more%250Avalid%2520information%2520from%2520the%2520two%2520types%2520of%2520features.%2520Owing%2520to%2520the%2520distinctive%250Adesign%252C%2520our%2520proposed%2520framework%2520can%2520estimate%2520more%2520accurate%2520correspondences%2520and%250Ais%2520applicable%2520to%2520both%2520hand-crafted%2520and%2520learning-based%2520feature%2520descriptors.%250AExperiments%2520on%2520ScanNet%2520show%2520that%2520our%2520method%2520achieves%2520a%2520state-of-the-art%250Aperformance%2520and%2520the%2520rotation%2520accuracy%2520of%252099.1%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGBD-Glue%3A%20General%20Feature%20Combination%20for%20Robust%20RGB-D%20Point%20Cloud%0A%20%20Registration&entry.906535625=Congjia%20Chen%20and%20Xiaoyu%20Jia%20and%20Yanhong%20Zheng%20and%20Yufu%20Qu&entry.1292438233=%20%20Point%20cloud%20registration%20is%20a%20fundamental%20task%20for%20estimating%20rigid%0Atransformations%20between%20point%20clouds.%20Previous%20studies%20have%20used%20geometric%0Ainformation%20for%20extracting%20features%2C%20matching%20and%20estimating%20transformation.%0ARecently%2C%20owing%20to%20the%20advancement%20of%20RGB-D%20sensors%2C%20researchers%20have%20attempted%0Ato%20utilize%20visual%20information%20to%20improve%20registration%20performance.%20However%2C%0Athese%20studies%20focused%20on%20extracting%20distinctive%20features%20by%20deep%20feature%0Afusion%2C%20which%20cannot%20effectively%20solve%20the%20negative%20effects%20of%20each%20feature%27s%0Aweakness%2C%20and%20cannot%20sufficiently%20leverage%20the%20valid%20information.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20feature%20combination%20framework%2C%20which%20applies%20a%20looser%0Abut%20more%20effective%20fusion%20and%20can%20achieve%20better%20performance.%20An%20explicit%0Afilter%20based%20on%20transformation%20consistency%20is%20designed%20for%20the%20combination%0Aframework%2C%20which%20can%20overcome%20each%20feature%27s%20weakness.%20And%20an%20adaptive%0Athreshold%20determined%20by%20the%20error%20distribution%20is%20proposed%20to%20extract%20more%0Avalid%20information%20from%20the%20two%20types%20of%20features.%20Owing%20to%20the%20distinctive%0Adesign%2C%20our%20proposed%20framework%20can%20estimate%20more%20accurate%20correspondences%20and%0Ais%20applicable%20to%20both%20hand-crafted%20and%20learning-based%20feature%20descriptors.%0AExperiments%20on%20ScanNet%20show%20that%20our%20method%20achieves%20a%20state-of-the-art%0Aperformance%20and%20the%20rotation%20accuracy%20of%2099.1%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07594v1&entry.124074799=Read"},
{"title": "Auto-Linear Phenomenon in Subsurface Imaging", "author": "Yinan Feng and Yinpeng Chen and Peng Jin and Shihang Feng and Zicheng Liu and Youzuo Lin", "abstract": "  Subsurface imaging involves solving full waveform inversion (FWI) to predict\ngeophysical properties from measurements. This problem can be reframed as an\nimage-to-image translation, with the usual approach being to train an\nencoder-decoder network using paired data from two domains: geophysical\nproperty and measurement. A recent seminal work (InvLINT) demonstrates there is\nonly a linear mapping between the latent spaces of the two domains, and the\ndecoder requires paired data for training.\n  This paper extends this direction by demonstrating that only linear mapping\nnecessitates paired data, while both the encoder and decoder can be learned\nfrom their respective domains through self-supervised learning. This unveils an\nintriguing phenomenon (named Auto-Linear) where the self-learned features of\ntwo separate domains are automatically linearly correlated. Compared with\nexisting methods, our Auto-Linear has four advantages: (a) solving both forward\nand inverse modeling simultaneously, (b) applicable to different subsurface\nimaging tasks and achieving markedly better results than previous methods,\n(c)enhanced performance, especially in scenarios with limited paired data and\nin the presence of noisy data, and (d) strong generalization ability of the\ntrained encoder and decoder.\n", "link": "http://arxiv.org/abs/2305.13314v2", "date": "2024-05-13", "relevancy": 2.8755, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5974}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.587}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auto-Linear%20Phenomenon%20in%20Subsurface%20Imaging&body=Title%3A%20Auto-Linear%20Phenomenon%20in%20Subsurface%20Imaging%0AAuthor%3A%20Yinan%20Feng%20and%20Yinpeng%20Chen%20and%20Peng%20Jin%20and%20Shihang%20Feng%20and%20Zicheng%20Liu%20and%20Youzuo%20Lin%0AAbstract%3A%20%20%20Subsurface%20imaging%20involves%20solving%20full%20waveform%20inversion%20%28FWI%29%20to%20predict%0Ageophysical%20properties%20from%20measurements.%20This%20problem%20can%20be%20reframed%20as%20an%0Aimage-to-image%20translation%2C%20with%20the%20usual%20approach%20being%20to%20train%20an%0Aencoder-decoder%20network%20using%20paired%20data%20from%20two%20domains%3A%20geophysical%0Aproperty%20and%20measurement.%20A%20recent%20seminal%20work%20%28InvLINT%29%20demonstrates%20there%20is%0Aonly%20a%20linear%20mapping%20between%20the%20latent%20spaces%20of%20the%20two%20domains%2C%20and%20the%0Adecoder%20requires%20paired%20data%20for%20training.%0A%20%20This%20paper%20extends%20this%20direction%20by%20demonstrating%20that%20only%20linear%20mapping%0Anecessitates%20paired%20data%2C%20while%20both%20the%20encoder%20and%20decoder%20can%20be%20learned%0Afrom%20their%20respective%20domains%20through%20self-supervised%20learning.%20This%20unveils%20an%0Aintriguing%20phenomenon%20%28named%20Auto-Linear%29%20where%20the%20self-learned%20features%20of%0Atwo%20separate%20domains%20are%20automatically%20linearly%20correlated.%20Compared%20with%0Aexisting%20methods%2C%20our%20Auto-Linear%20has%20four%20advantages%3A%20%28a%29%20solving%20both%20forward%0Aand%20inverse%20modeling%20simultaneously%2C%20%28b%29%20applicable%20to%20different%20subsurface%0Aimaging%20tasks%20and%20achieving%20markedly%20better%20results%20than%20previous%20methods%2C%0A%28c%29enhanced%20performance%2C%20especially%20in%20scenarios%20with%20limited%20paired%20data%20and%0Ain%20the%20presence%20of%20noisy%20data%2C%20and%20%28d%29%20strong%20generalization%20ability%20of%20the%0Atrained%20encoder%20and%20decoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.13314v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuto-Linear%2520Phenomenon%2520in%2520Subsurface%2520Imaging%26entry.906535625%3DYinan%2520Feng%2520and%2520Yinpeng%2520Chen%2520and%2520Peng%2520Jin%2520and%2520Shihang%2520Feng%2520and%2520Zicheng%2520Liu%2520and%2520Youzuo%2520Lin%26entry.1292438233%3D%2520%2520Subsurface%2520imaging%2520involves%2520solving%2520full%2520waveform%2520inversion%2520%2528FWI%2529%2520to%2520predict%250Ageophysical%2520properties%2520from%2520measurements.%2520This%2520problem%2520can%2520be%2520reframed%2520as%2520an%250Aimage-to-image%2520translation%252C%2520with%2520the%2520usual%2520approach%2520being%2520to%2520train%2520an%250Aencoder-decoder%2520network%2520using%2520paired%2520data%2520from%2520two%2520domains%253A%2520geophysical%250Aproperty%2520and%2520measurement.%2520A%2520recent%2520seminal%2520work%2520%2528InvLINT%2529%2520demonstrates%2520there%2520is%250Aonly%2520a%2520linear%2520mapping%2520between%2520the%2520latent%2520spaces%2520of%2520the%2520two%2520domains%252C%2520and%2520the%250Adecoder%2520requires%2520paired%2520data%2520for%2520training.%250A%2520%2520This%2520paper%2520extends%2520this%2520direction%2520by%2520demonstrating%2520that%2520only%2520linear%2520mapping%250Anecessitates%2520paired%2520data%252C%2520while%2520both%2520the%2520encoder%2520and%2520decoder%2520can%2520be%2520learned%250Afrom%2520their%2520respective%2520domains%2520through%2520self-supervised%2520learning.%2520This%2520unveils%2520an%250Aintriguing%2520phenomenon%2520%2528named%2520Auto-Linear%2529%2520where%2520the%2520self-learned%2520features%2520of%250Atwo%2520separate%2520domains%2520are%2520automatically%2520linearly%2520correlated.%2520Compared%2520with%250Aexisting%2520methods%252C%2520our%2520Auto-Linear%2520has%2520four%2520advantages%253A%2520%2528a%2529%2520solving%2520both%2520forward%250Aand%2520inverse%2520modeling%2520simultaneously%252C%2520%2528b%2529%2520applicable%2520to%2520different%2520subsurface%250Aimaging%2520tasks%2520and%2520achieving%2520markedly%2520better%2520results%2520than%2520previous%2520methods%252C%250A%2528c%2529enhanced%2520performance%252C%2520especially%2520in%2520scenarios%2520with%2520limited%2520paired%2520data%2520and%250Ain%2520the%2520presence%2520of%2520noisy%2520data%252C%2520and%2520%2528d%2529%2520strong%2520generalization%2520ability%2520of%2520the%250Atrained%2520encoder%2520and%2520decoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.13314v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auto-Linear%20Phenomenon%20in%20Subsurface%20Imaging&entry.906535625=Yinan%20Feng%20and%20Yinpeng%20Chen%20and%20Peng%20Jin%20and%20Shihang%20Feng%20and%20Zicheng%20Liu%20and%20Youzuo%20Lin&entry.1292438233=%20%20Subsurface%20imaging%20involves%20solving%20full%20waveform%20inversion%20%28FWI%29%20to%20predict%0Ageophysical%20properties%20from%20measurements.%20This%20problem%20can%20be%20reframed%20as%20an%0Aimage-to-image%20translation%2C%20with%20the%20usual%20approach%20being%20to%20train%20an%0Aencoder-decoder%20network%20using%20paired%20data%20from%20two%20domains%3A%20geophysical%0Aproperty%20and%20measurement.%20A%20recent%20seminal%20work%20%28InvLINT%29%20demonstrates%20there%20is%0Aonly%20a%20linear%20mapping%20between%20the%20latent%20spaces%20of%20the%20two%20domains%2C%20and%20the%0Adecoder%20requires%20paired%20data%20for%20training.%0A%20%20This%20paper%20extends%20this%20direction%20by%20demonstrating%20that%20only%20linear%20mapping%0Anecessitates%20paired%20data%2C%20while%20both%20the%20encoder%20and%20decoder%20can%20be%20learned%0Afrom%20their%20respective%20domains%20through%20self-supervised%20learning.%20This%20unveils%20an%0Aintriguing%20phenomenon%20%28named%20Auto-Linear%29%20where%20the%20self-learned%20features%20of%0Atwo%20separate%20domains%20are%20automatically%20linearly%20correlated.%20Compared%20with%0Aexisting%20methods%2C%20our%20Auto-Linear%20has%20four%20advantages%3A%20%28a%29%20solving%20both%20forward%0Aand%20inverse%20modeling%20simultaneously%2C%20%28b%29%20applicable%20to%20different%20subsurface%0Aimaging%20tasks%20and%20achieving%20markedly%20better%20results%20than%20previous%20methods%2C%0A%28c%29enhanced%20performance%2C%20especially%20in%20scenarios%20with%20limited%20paired%20data%20and%0Ain%20the%20presence%20of%20noisy%20data%2C%20and%20%28d%29%20strong%20generalization%20ability%20of%20the%0Atrained%20encoder%20and%20decoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.13314v2&entry.124074799=Read"},
{"title": "SignAvatar: Sign Language 3D Motion Reconstruction and Generation", "author": "Lu Dong and Lipisha Chaudhary and Fei Xu and Xiao Wang and Mason Lary and Ifeoma Nwogu", "abstract": "  Achieving expressive 3D motion reconstruction and automatic generation for\nisolated sign words can be challenging, due to the lack of real-world 3D\nsign-word data, the complex nuances of signing motions, and the cross-modal\nunderstanding of sign language semantics. To address these challenges, we\nintroduce SignAvatar, a framework capable of both word-level sign language\nreconstruction and generation. SignAvatar employs a transformer-based\nconditional variational autoencoder architecture, effectively establishing\nrelationships across different semantic modalities. Additionally, this approach\nincorporates a curriculum learning strategy to enhance the model's robustness\nand generalization, resulting in more realistic motions. Furthermore, we\ncontribute the ASL3DWord dataset, composed of 3D joint rotation data for the\nbody, hands, and face, for unique sign words. We demonstrate the effectiveness\nof SignAvatar through extensive experiments, showcasing its superior\nreconstruction and automatic generation capabilities. The code and dataset are\navailable on the project page.\n", "link": "http://arxiv.org/abs/2405.07974v1", "date": "2024-05-13", "relevancy": 2.7728, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5725}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5456}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SignAvatar%3A%20Sign%20Language%203D%20Motion%20Reconstruction%20and%20Generation&body=Title%3A%20SignAvatar%3A%20Sign%20Language%203D%20Motion%20Reconstruction%20and%20Generation%0AAuthor%3A%20Lu%20Dong%20and%20Lipisha%20Chaudhary%20and%20Fei%20Xu%20and%20Xiao%20Wang%20and%20Mason%20Lary%20and%20Ifeoma%20Nwogu%0AAbstract%3A%20%20%20Achieving%20expressive%203D%20motion%20reconstruction%20and%20automatic%20generation%20for%0Aisolated%20sign%20words%20can%20be%20challenging%2C%20due%20to%20the%20lack%20of%20real-world%203D%0Asign-word%20data%2C%20the%20complex%20nuances%20of%20signing%20motions%2C%20and%20the%20cross-modal%0Aunderstanding%20of%20sign%20language%20semantics.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20SignAvatar%2C%20a%20framework%20capable%20of%20both%20word-level%20sign%20language%0Areconstruction%20and%20generation.%20SignAvatar%20employs%20a%20transformer-based%0Aconditional%20variational%20autoencoder%20architecture%2C%20effectively%20establishing%0Arelationships%20across%20different%20semantic%20modalities.%20Additionally%2C%20this%20approach%0Aincorporates%20a%20curriculum%20learning%20strategy%20to%20enhance%20the%20model%27s%20robustness%0Aand%20generalization%2C%20resulting%20in%20more%20realistic%20motions.%20Furthermore%2C%20we%0Acontribute%20the%20ASL3DWord%20dataset%2C%20composed%20of%203D%20joint%20rotation%20data%20for%20the%0Abody%2C%20hands%2C%20and%20face%2C%20for%20unique%20sign%20words.%20We%20demonstrate%20the%20effectiveness%0Aof%20SignAvatar%20through%20extensive%20experiments%2C%20showcasing%20its%20superior%0Areconstruction%20and%20automatic%20generation%20capabilities.%20The%20code%20and%20dataset%20are%0Aavailable%20on%20the%20project%20page.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSignAvatar%253A%2520Sign%2520Language%25203D%2520Motion%2520Reconstruction%2520and%2520Generation%26entry.906535625%3DLu%2520Dong%2520and%2520Lipisha%2520Chaudhary%2520and%2520Fei%2520Xu%2520and%2520Xiao%2520Wang%2520and%2520Mason%2520Lary%2520and%2520Ifeoma%2520Nwogu%26entry.1292438233%3D%2520%2520Achieving%2520expressive%25203D%2520motion%2520reconstruction%2520and%2520automatic%2520generation%2520for%250Aisolated%2520sign%2520words%2520can%2520be%2520challenging%252C%2520due%2520to%2520the%2520lack%2520of%2520real-world%25203D%250Asign-word%2520data%252C%2520the%2520complex%2520nuances%2520of%2520signing%2520motions%252C%2520and%2520the%2520cross-modal%250Aunderstanding%2520of%2520sign%2520language%2520semantics.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520SignAvatar%252C%2520a%2520framework%2520capable%2520of%2520both%2520word-level%2520sign%2520language%250Areconstruction%2520and%2520generation.%2520SignAvatar%2520employs%2520a%2520transformer-based%250Aconditional%2520variational%2520autoencoder%2520architecture%252C%2520effectively%2520establishing%250Arelationships%2520across%2520different%2520semantic%2520modalities.%2520Additionally%252C%2520this%2520approach%250Aincorporates%2520a%2520curriculum%2520learning%2520strategy%2520to%2520enhance%2520the%2520model%2527s%2520robustness%250Aand%2520generalization%252C%2520resulting%2520in%2520more%2520realistic%2520motions.%2520Furthermore%252C%2520we%250Acontribute%2520the%2520ASL3DWord%2520dataset%252C%2520composed%2520of%25203D%2520joint%2520rotation%2520data%2520for%2520the%250Abody%252C%2520hands%252C%2520and%2520face%252C%2520for%2520unique%2520sign%2520words.%2520We%2520demonstrate%2520the%2520effectiveness%250Aof%2520SignAvatar%2520through%2520extensive%2520experiments%252C%2520showcasing%2520its%2520superior%250Areconstruction%2520and%2520automatic%2520generation%2520capabilities.%2520The%2520code%2520and%2520dataset%2520are%250Aavailable%2520on%2520the%2520project%2520page.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SignAvatar%3A%20Sign%20Language%203D%20Motion%20Reconstruction%20and%20Generation&entry.906535625=Lu%20Dong%20and%20Lipisha%20Chaudhary%20and%20Fei%20Xu%20and%20Xiao%20Wang%20and%20Mason%20Lary%20and%20Ifeoma%20Nwogu&entry.1292438233=%20%20Achieving%20expressive%203D%20motion%20reconstruction%20and%20automatic%20generation%20for%0Aisolated%20sign%20words%20can%20be%20challenging%2C%20due%20to%20the%20lack%20of%20real-world%203D%0Asign-word%20data%2C%20the%20complex%20nuances%20of%20signing%20motions%2C%20and%20the%20cross-modal%0Aunderstanding%20of%20sign%20language%20semantics.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20SignAvatar%2C%20a%20framework%20capable%20of%20both%20word-level%20sign%20language%0Areconstruction%20and%20generation.%20SignAvatar%20employs%20a%20transformer-based%0Aconditional%20variational%20autoencoder%20architecture%2C%20effectively%20establishing%0Arelationships%20across%20different%20semantic%20modalities.%20Additionally%2C%20this%20approach%0Aincorporates%20a%20curriculum%20learning%20strategy%20to%20enhance%20the%20model%27s%20robustness%0Aand%20generalization%2C%20resulting%20in%20more%20realistic%20motions.%20Furthermore%2C%20we%0Acontribute%20the%20ASL3DWord%20dataset%2C%20composed%20of%203D%20joint%20rotation%20data%20for%20the%0Abody%2C%20hands%2C%20and%20face%2C%20for%20unique%20sign%20words.%20We%20demonstrate%20the%20effectiveness%0Aof%20SignAvatar%20through%20extensive%20experiments%2C%20showcasing%20its%20superior%0Areconstruction%20and%20automatic%20generation%20capabilities.%20The%20code%20and%20dataset%20are%0Aavailable%20on%20the%20project%20page.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07974v1&entry.124074799=Read"},
{"title": "Guided Interpretable Facial Expression Recognition via Spatial Action\n  Unit Cues", "author": "Soufiane Belharbi and Marco Pedersoli and Alessandro Lameiras Koerich and Simon Bacon and Eric Granger", "abstract": "  Although state-of-the-art classifiers for facial expression recognition (FER)\ncan achieve a high level of accuracy, they lack interpretability, an important\nfeature for end-users. Experts typically associate spatial action units (\\aus)\nfrom a codebook to facial regions for the visual interpretation of expressions.\nIn this paper, the same expert steps are followed. A new learning strategy is\nproposed to explicitly incorporate \\au cues into classifier training, allowing\nto train deep interpretable models. During training, this \\au codebook is used,\nalong with the input image expression label, and facial landmarks, to construct\na \\au heatmap that indicates the most discriminative image regions of interest\nw.r.t the facial expression. This valuable spatial cue is leveraged to train a\ndeep interpretable classifier for FER. This is achieved by constraining the\nspatial layer features of a classifier to be correlated with \\au heatmaps.\nUsing a composite loss, the classifier is trained to correctly classify an\nimage while yielding interpretable visual layer-wise attention correlated with\n\\au maps, simulating the expert decision process. Our strategy only relies on\nimage class expression for supervision, without additional manual annotations.\nOur new strategy is generic, and can be applied to any deep CNN- or\ntransformer-based classifier without requiring any architectural change or\nsignificant additional training time. Our extensive evaluation on two public\nbenchmarks \\rafdb, and \\affectnet datasets shows that our proposed strategy can\nimprove layer-wise interpretability without degrading classification\nperformance. In addition, we explore a common type of interpretable classifiers\nthat rely on class activation mapping (CAM) methods, and show that our approach\ncan also improve CAM interpretability.\n", "link": "http://arxiv.org/abs/2402.00281v4", "date": "2024-05-13", "relevancy": 2.7398, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5604}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5435}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guided%20Interpretable%20Facial%20Expression%20Recognition%20via%20Spatial%20Action%0A%20%20Unit%20Cues&body=Title%3A%20Guided%20Interpretable%20Facial%20Expression%20Recognition%20via%20Spatial%20Action%0A%20%20Unit%20Cues%0AAuthor%3A%20Soufiane%20Belharbi%20and%20Marco%20Pedersoli%20and%20Alessandro%20Lameiras%20Koerich%20and%20Simon%20Bacon%20and%20Eric%20Granger%0AAbstract%3A%20%20%20Although%20state-of-the-art%20classifiers%20for%20facial%20expression%20recognition%20%28FER%29%0Acan%20achieve%20a%20high%20level%20of%20accuracy%2C%20they%20lack%20interpretability%2C%20an%20important%0Afeature%20for%20end-users.%20Experts%20typically%20associate%20spatial%20action%20units%20%28%5Caus%29%0Afrom%20a%20codebook%20to%20facial%20regions%20for%20the%20visual%20interpretation%20of%20expressions.%0AIn%20this%20paper%2C%20the%20same%20expert%20steps%20are%20followed.%20A%20new%20learning%20strategy%20is%0Aproposed%20to%20explicitly%20incorporate%20%5Cau%20cues%20into%20classifier%20training%2C%20allowing%0Ato%20train%20deep%20interpretable%20models.%20During%20training%2C%20this%20%5Cau%20codebook%20is%20used%2C%0Aalong%20with%20the%20input%20image%20expression%20label%2C%20and%20facial%20landmarks%2C%20to%20construct%0Aa%20%5Cau%20heatmap%20that%20indicates%20the%20most%20discriminative%20image%20regions%20of%20interest%0Aw.r.t%20the%20facial%20expression.%20This%20valuable%20spatial%20cue%20is%20leveraged%20to%20train%20a%0Adeep%20interpretable%20classifier%20for%20FER.%20This%20is%20achieved%20by%20constraining%20the%0Aspatial%20layer%20features%20of%20a%20classifier%20to%20be%20correlated%20with%20%5Cau%20heatmaps.%0AUsing%20a%20composite%20loss%2C%20the%20classifier%20is%20trained%20to%20correctly%20classify%20an%0Aimage%20while%20yielding%20interpretable%20visual%20layer-wise%20attention%20correlated%20with%0A%5Cau%20maps%2C%20simulating%20the%20expert%20decision%20process.%20Our%20strategy%20only%20relies%20on%0Aimage%20class%20expression%20for%20supervision%2C%20without%20additional%20manual%20annotations.%0AOur%20new%20strategy%20is%20generic%2C%20and%20can%20be%20applied%20to%20any%20deep%20CNN-%20or%0Atransformer-based%20classifier%20without%20requiring%20any%20architectural%20change%20or%0Asignificant%20additional%20training%20time.%20Our%20extensive%20evaluation%20on%20two%20public%0Abenchmarks%20%5Crafdb%2C%20and%20%5Caffectnet%20datasets%20shows%20that%20our%20proposed%20strategy%20can%0Aimprove%20layer-wise%20interpretability%20without%20degrading%20classification%0Aperformance.%20In%20addition%2C%20we%20explore%20a%20common%20type%20of%20interpretable%20classifiers%0Athat%20rely%20on%20class%20activation%20mapping%20%28CAM%29%20methods%2C%20and%20show%20that%20our%20approach%0Acan%20also%20improve%20CAM%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00281v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuided%2520Interpretable%2520Facial%2520Expression%2520Recognition%2520via%2520Spatial%2520Action%250A%2520%2520Unit%2520Cues%26entry.906535625%3DSoufiane%2520Belharbi%2520and%2520Marco%2520Pedersoli%2520and%2520Alessandro%2520Lameiras%2520Koerich%2520and%2520Simon%2520Bacon%2520and%2520Eric%2520Granger%26entry.1292438233%3D%2520%2520Although%2520state-of-the-art%2520classifiers%2520for%2520facial%2520expression%2520recognition%2520%2528FER%2529%250Acan%2520achieve%2520a%2520high%2520level%2520of%2520accuracy%252C%2520they%2520lack%2520interpretability%252C%2520an%2520important%250Afeature%2520for%2520end-users.%2520Experts%2520typically%2520associate%2520spatial%2520action%2520units%2520%2528%255Caus%2529%250Afrom%2520a%2520codebook%2520to%2520facial%2520regions%2520for%2520the%2520visual%2520interpretation%2520of%2520expressions.%250AIn%2520this%2520paper%252C%2520the%2520same%2520expert%2520steps%2520are%2520followed.%2520A%2520new%2520learning%2520strategy%2520is%250Aproposed%2520to%2520explicitly%2520incorporate%2520%255Cau%2520cues%2520into%2520classifier%2520training%252C%2520allowing%250Ato%2520train%2520deep%2520interpretable%2520models.%2520During%2520training%252C%2520this%2520%255Cau%2520codebook%2520is%2520used%252C%250Aalong%2520with%2520the%2520input%2520image%2520expression%2520label%252C%2520and%2520facial%2520landmarks%252C%2520to%2520construct%250Aa%2520%255Cau%2520heatmap%2520that%2520indicates%2520the%2520most%2520discriminative%2520image%2520regions%2520of%2520interest%250Aw.r.t%2520the%2520facial%2520expression.%2520This%2520valuable%2520spatial%2520cue%2520is%2520leveraged%2520to%2520train%2520a%250Adeep%2520interpretable%2520classifier%2520for%2520FER.%2520This%2520is%2520achieved%2520by%2520constraining%2520the%250Aspatial%2520layer%2520features%2520of%2520a%2520classifier%2520to%2520be%2520correlated%2520with%2520%255Cau%2520heatmaps.%250AUsing%2520a%2520composite%2520loss%252C%2520the%2520classifier%2520is%2520trained%2520to%2520correctly%2520classify%2520an%250Aimage%2520while%2520yielding%2520interpretable%2520visual%2520layer-wise%2520attention%2520correlated%2520with%250A%255Cau%2520maps%252C%2520simulating%2520the%2520expert%2520decision%2520process.%2520Our%2520strategy%2520only%2520relies%2520on%250Aimage%2520class%2520expression%2520for%2520supervision%252C%2520without%2520additional%2520manual%2520annotations.%250AOur%2520new%2520strategy%2520is%2520generic%252C%2520and%2520can%2520be%2520applied%2520to%2520any%2520deep%2520CNN-%2520or%250Atransformer-based%2520classifier%2520without%2520requiring%2520any%2520architectural%2520change%2520or%250Asignificant%2520additional%2520training%2520time.%2520Our%2520extensive%2520evaluation%2520on%2520two%2520public%250Abenchmarks%2520%255Crafdb%252C%2520and%2520%255Caffectnet%2520datasets%2520shows%2520that%2520our%2520proposed%2520strategy%2520can%250Aimprove%2520layer-wise%2520interpretability%2520without%2520degrading%2520classification%250Aperformance.%2520In%2520addition%252C%2520we%2520explore%2520a%2520common%2520type%2520of%2520interpretable%2520classifiers%250Athat%2520rely%2520on%2520class%2520activation%2520mapping%2520%2528CAM%2529%2520methods%252C%2520and%2520show%2520that%2520our%2520approach%250Acan%2520also%2520improve%2520CAM%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00281v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guided%20Interpretable%20Facial%20Expression%20Recognition%20via%20Spatial%20Action%0A%20%20Unit%20Cues&entry.906535625=Soufiane%20Belharbi%20and%20Marco%20Pedersoli%20and%20Alessandro%20Lameiras%20Koerich%20and%20Simon%20Bacon%20and%20Eric%20Granger&entry.1292438233=%20%20Although%20state-of-the-art%20classifiers%20for%20facial%20expression%20recognition%20%28FER%29%0Acan%20achieve%20a%20high%20level%20of%20accuracy%2C%20they%20lack%20interpretability%2C%20an%20important%0Afeature%20for%20end-users.%20Experts%20typically%20associate%20spatial%20action%20units%20%28%5Caus%29%0Afrom%20a%20codebook%20to%20facial%20regions%20for%20the%20visual%20interpretation%20of%20expressions.%0AIn%20this%20paper%2C%20the%20same%20expert%20steps%20are%20followed.%20A%20new%20learning%20strategy%20is%0Aproposed%20to%20explicitly%20incorporate%20%5Cau%20cues%20into%20classifier%20training%2C%20allowing%0Ato%20train%20deep%20interpretable%20models.%20During%20training%2C%20this%20%5Cau%20codebook%20is%20used%2C%0Aalong%20with%20the%20input%20image%20expression%20label%2C%20and%20facial%20landmarks%2C%20to%20construct%0Aa%20%5Cau%20heatmap%20that%20indicates%20the%20most%20discriminative%20image%20regions%20of%20interest%0Aw.r.t%20the%20facial%20expression.%20This%20valuable%20spatial%20cue%20is%20leveraged%20to%20train%20a%0Adeep%20interpretable%20classifier%20for%20FER.%20This%20is%20achieved%20by%20constraining%20the%0Aspatial%20layer%20features%20of%20a%20classifier%20to%20be%20correlated%20with%20%5Cau%20heatmaps.%0AUsing%20a%20composite%20loss%2C%20the%20classifier%20is%20trained%20to%20correctly%20classify%20an%0Aimage%20while%20yielding%20interpretable%20visual%20layer-wise%20attention%20correlated%20with%0A%5Cau%20maps%2C%20simulating%20the%20expert%20decision%20process.%20Our%20strategy%20only%20relies%20on%0Aimage%20class%20expression%20for%20supervision%2C%20without%20additional%20manual%20annotations.%0AOur%20new%20strategy%20is%20generic%2C%20and%20can%20be%20applied%20to%20any%20deep%20CNN-%20or%0Atransformer-based%20classifier%20without%20requiring%20any%20architectural%20change%20or%0Asignificant%20additional%20training%20time.%20Our%20extensive%20evaluation%20on%20two%20public%0Abenchmarks%20%5Crafdb%2C%20and%20%5Caffectnet%20datasets%20shows%20that%20our%20proposed%20strategy%20can%0Aimprove%20layer-wise%20interpretability%20without%20degrading%20classification%0Aperformance.%20In%20addition%2C%20we%20explore%20a%20common%20type%20of%20interpretable%20classifiers%0Athat%20rely%20on%20class%20activation%20mapping%20%28CAM%29%20methods%2C%20and%20show%20that%20our%20approach%0Acan%20also%20improve%20CAM%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00281v4&entry.124074799=Read"},
{"title": "Authentic Hand Avatar from a Phone Scan via Universal Hand Model", "author": "Gyeongsik Moon and Weipeng Xu and Rohan Joshi and Chenglei Wu and Takaaki Shiratori", "abstract": "  The authentic 3D hand avatar with every identifiable information, such as\nhand shapes and textures, is necessary for immersive experiences in AR/VR. In\nthis paper, we present a universal hand model (UHM), which 1) can universally\nrepresent high-fidelity 3D hand meshes of arbitrary identities (IDs) and 2) can\nbe adapted to each person with a short phone scan for the authentic hand\navatar. For effective universal hand modeling, we perform tracking and modeling\nat the same time, while previous 3D hand models perform them separately. The\nconventional separate pipeline suffers from the accumulated errors from the\ntracking stage, which cannot be recovered in the modeling stage. On the other\nhand, ours does not suffer from the accumulated errors while having a much more\nconcise overall pipeline. We additionally introduce a novel image matching loss\nfunction to address a skin sliding during the tracking and modeling, while\nexisting works have not focused on it much. Finally, using learned priors from\nour UHM, we effectively adapt our UHM to each person's short phone scan for the\nauthentic hand avatar.\n", "link": "http://arxiv.org/abs/2405.07933v1", "date": "2024-05-13", "relevancy": 2.7384, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5505}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5505}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Authentic%20Hand%20Avatar%20from%20a%20Phone%20Scan%20via%20Universal%20Hand%20Model&body=Title%3A%20Authentic%20Hand%20Avatar%20from%20a%20Phone%20Scan%20via%20Universal%20Hand%20Model%0AAuthor%3A%20Gyeongsik%20Moon%20and%20Weipeng%20Xu%20and%20Rohan%20Joshi%20and%20Chenglei%20Wu%20and%20Takaaki%20Shiratori%0AAbstract%3A%20%20%20The%20authentic%203D%20hand%20avatar%20with%20every%20identifiable%20information%2C%20such%20as%0Ahand%20shapes%20and%20textures%2C%20is%20necessary%20for%20immersive%20experiences%20in%20AR/VR.%20In%0Athis%20paper%2C%20we%20present%20a%20universal%20hand%20model%20%28UHM%29%2C%20which%201%29%20can%20universally%0Arepresent%20high-fidelity%203D%20hand%20meshes%20of%20arbitrary%20identities%20%28IDs%29%20and%202%29%20can%0Abe%20adapted%20to%20each%20person%20with%20a%20short%20phone%20scan%20for%20the%20authentic%20hand%0Aavatar.%20For%20effective%20universal%20hand%20modeling%2C%20we%20perform%20tracking%20and%20modeling%0Aat%20the%20same%20time%2C%20while%20previous%203D%20hand%20models%20perform%20them%20separately.%20The%0Aconventional%20separate%20pipeline%20suffers%20from%20the%20accumulated%20errors%20from%20the%0Atracking%20stage%2C%20which%20cannot%20be%20recovered%20in%20the%20modeling%20stage.%20On%20the%20other%0Ahand%2C%20ours%20does%20not%20suffer%20from%20the%20accumulated%20errors%20while%20having%20a%20much%20more%0Aconcise%20overall%20pipeline.%20We%20additionally%20introduce%20a%20novel%20image%20matching%20loss%0Afunction%20to%20address%20a%20skin%20sliding%20during%20the%20tracking%20and%20modeling%2C%20while%0Aexisting%20works%20have%20not%20focused%20on%20it%20much.%20Finally%2C%20using%20learned%20priors%20from%0Aour%20UHM%2C%20we%20effectively%20adapt%20our%20UHM%20to%20each%20person%27s%20short%20phone%20scan%20for%20the%0Aauthentic%20hand%20avatar.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07933v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuthentic%2520Hand%2520Avatar%2520from%2520a%2520Phone%2520Scan%2520via%2520Universal%2520Hand%2520Model%26entry.906535625%3DGyeongsik%2520Moon%2520and%2520Weipeng%2520Xu%2520and%2520Rohan%2520Joshi%2520and%2520Chenglei%2520Wu%2520and%2520Takaaki%2520Shiratori%26entry.1292438233%3D%2520%2520The%2520authentic%25203D%2520hand%2520avatar%2520with%2520every%2520identifiable%2520information%252C%2520such%2520as%250Ahand%2520shapes%2520and%2520textures%252C%2520is%2520necessary%2520for%2520immersive%2520experiences%2520in%2520AR/VR.%2520In%250Athis%2520paper%252C%2520we%2520present%2520a%2520universal%2520hand%2520model%2520%2528UHM%2529%252C%2520which%25201%2529%2520can%2520universally%250Arepresent%2520high-fidelity%25203D%2520hand%2520meshes%2520of%2520arbitrary%2520identities%2520%2528IDs%2529%2520and%25202%2529%2520can%250Abe%2520adapted%2520to%2520each%2520person%2520with%2520a%2520short%2520phone%2520scan%2520for%2520the%2520authentic%2520hand%250Aavatar.%2520For%2520effective%2520universal%2520hand%2520modeling%252C%2520we%2520perform%2520tracking%2520and%2520modeling%250Aat%2520the%2520same%2520time%252C%2520while%2520previous%25203D%2520hand%2520models%2520perform%2520them%2520separately.%2520The%250Aconventional%2520separate%2520pipeline%2520suffers%2520from%2520the%2520accumulated%2520errors%2520from%2520the%250Atracking%2520stage%252C%2520which%2520cannot%2520be%2520recovered%2520in%2520the%2520modeling%2520stage.%2520On%2520the%2520other%250Ahand%252C%2520ours%2520does%2520not%2520suffer%2520from%2520the%2520accumulated%2520errors%2520while%2520having%2520a%2520much%2520more%250Aconcise%2520overall%2520pipeline.%2520We%2520additionally%2520introduce%2520a%2520novel%2520image%2520matching%2520loss%250Afunction%2520to%2520address%2520a%2520skin%2520sliding%2520during%2520the%2520tracking%2520and%2520modeling%252C%2520while%250Aexisting%2520works%2520have%2520not%2520focused%2520on%2520it%2520much.%2520Finally%252C%2520using%2520learned%2520priors%2520from%250Aour%2520UHM%252C%2520we%2520effectively%2520adapt%2520our%2520UHM%2520to%2520each%2520person%2527s%2520short%2520phone%2520scan%2520for%2520the%250Aauthentic%2520hand%2520avatar.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07933v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Authentic%20Hand%20Avatar%20from%20a%20Phone%20Scan%20via%20Universal%20Hand%20Model&entry.906535625=Gyeongsik%20Moon%20and%20Weipeng%20Xu%20and%20Rohan%20Joshi%20and%20Chenglei%20Wu%20and%20Takaaki%20Shiratori&entry.1292438233=%20%20The%20authentic%203D%20hand%20avatar%20with%20every%20identifiable%20information%2C%20such%20as%0Ahand%20shapes%20and%20textures%2C%20is%20necessary%20for%20immersive%20experiences%20in%20AR/VR.%20In%0Athis%20paper%2C%20we%20present%20a%20universal%20hand%20model%20%28UHM%29%2C%20which%201%29%20can%20universally%0Arepresent%20high-fidelity%203D%20hand%20meshes%20of%20arbitrary%20identities%20%28IDs%29%20and%202%29%20can%0Abe%20adapted%20to%20each%20person%20with%20a%20short%20phone%20scan%20for%20the%20authentic%20hand%0Aavatar.%20For%20effective%20universal%20hand%20modeling%2C%20we%20perform%20tracking%20and%20modeling%0Aat%20the%20same%20time%2C%20while%20previous%203D%20hand%20models%20perform%20them%20separately.%20The%0Aconventional%20separate%20pipeline%20suffers%20from%20the%20accumulated%20errors%20from%20the%0Atracking%20stage%2C%20which%20cannot%20be%20recovered%20in%20the%20modeling%20stage.%20On%20the%20other%0Ahand%2C%20ours%20does%20not%20suffer%20from%20the%20accumulated%20errors%20while%20having%20a%20much%20more%0Aconcise%20overall%20pipeline.%20We%20additionally%20introduce%20a%20novel%20image%20matching%20loss%0Afunction%20to%20address%20a%20skin%20sliding%20during%20the%20tracking%20and%20modeling%2C%20while%0Aexisting%20works%20have%20not%20focused%20on%20it%20much.%20Finally%2C%20using%20learned%20priors%20from%0Aour%20UHM%2C%20we%20effectively%20adapt%20our%20UHM%20to%20each%20person%27s%20short%20phone%20scan%20for%20the%0Aauthentic%20hand%20avatar.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07933v1&entry.124074799=Read"},
{"title": "Sign Stitching: A Novel Approach to Sign Language Production", "author": "Harry Walsh and Ben Saunders and Richard Bowden", "abstract": "  Sign Language Production (SLP) is a challenging task, given the limited\nresources available and the inherent diversity within sign data. As a result,\nprevious works have suffered from the problem of regression to the mean,\nleading to under-articulated and incomprehensible signing. In this paper, we\npropose using dictionary examples and a learnt codebook of facial expressions\nto create expressive sign language sequences. However, simply concatenating\nsigns and adding the face creates robotic and unnatural sequences. To address\nthis we present a 7-step approach to effectively stitch sequences together.\nFirst, by normalizing each sign into a canonical pose, cropping, and stitching\nwe create a continuous sequence. Then, by applying filtering in the frequency\ndomain and resampling each sign, we create cohesive natural sequences that\nmimic the prosody found in the original data. We leverage a SignGAN model to\nmap the output to a photo-realistic signer and present a complete Text-to-Sign\n(T2S) SLP pipeline. Our evaluation demonstrates the effectiveness of the\napproach, showcasing state-of-the-art performance across all datasets. Finally,\na user evaluation shows our approach outperforms the baseline model and is\ncapable of producing realistic sign language sequences.\n", "link": "http://arxiv.org/abs/2405.07663v1", "date": "2024-05-13", "relevancy": 2.6325, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5607}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5112}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sign%20Stitching%3A%20A%20Novel%20Approach%20to%20Sign%20Language%20Production&body=Title%3A%20Sign%20Stitching%3A%20A%20Novel%20Approach%20to%20Sign%20Language%20Production%0AAuthor%3A%20Harry%20Walsh%20and%20Ben%20Saunders%20and%20Richard%20Bowden%0AAbstract%3A%20%20%20Sign%20Language%20Production%20%28SLP%29%20is%20a%20challenging%20task%2C%20given%20the%20limited%0Aresources%20available%20and%20the%20inherent%20diversity%20within%20sign%20data.%20As%20a%20result%2C%0Aprevious%20works%20have%20suffered%20from%20the%20problem%20of%20regression%20to%20the%20mean%2C%0Aleading%20to%20under-articulated%20and%20incomprehensible%20signing.%20In%20this%20paper%2C%20we%0Apropose%20using%20dictionary%20examples%20and%20a%20learnt%20codebook%20of%20facial%20expressions%0Ato%20create%20expressive%20sign%20language%20sequences.%20However%2C%20simply%20concatenating%0Asigns%20and%20adding%20the%20face%20creates%20robotic%20and%20unnatural%20sequences.%20To%20address%0Athis%20we%20present%20a%207-step%20approach%20to%20effectively%20stitch%20sequences%20together.%0AFirst%2C%20by%20normalizing%20each%20sign%20into%20a%20canonical%20pose%2C%20cropping%2C%20and%20stitching%0Awe%20create%20a%20continuous%20sequence.%20Then%2C%20by%20applying%20filtering%20in%20the%20frequency%0Adomain%20and%20resampling%20each%20sign%2C%20we%20create%20cohesive%20natural%20sequences%20that%0Amimic%20the%20prosody%20found%20in%20the%20original%20data.%20We%20leverage%20a%20SignGAN%20model%20to%0Amap%20the%20output%20to%20a%20photo-realistic%20signer%20and%20present%20a%20complete%20Text-to-Sign%0A%28T2S%29%20SLP%20pipeline.%20Our%20evaluation%20demonstrates%20the%20effectiveness%20of%20the%0Aapproach%2C%20showcasing%20state-of-the-art%20performance%20across%20all%20datasets.%20Finally%2C%0Aa%20user%20evaluation%20shows%20our%20approach%20outperforms%20the%20baseline%20model%20and%20is%0Acapable%20of%20producing%20realistic%20sign%20language%20sequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSign%2520Stitching%253A%2520A%2520Novel%2520Approach%2520to%2520Sign%2520Language%2520Production%26entry.906535625%3DHarry%2520Walsh%2520and%2520Ben%2520Saunders%2520and%2520Richard%2520Bowden%26entry.1292438233%3D%2520%2520Sign%2520Language%2520Production%2520%2528SLP%2529%2520is%2520a%2520challenging%2520task%252C%2520given%2520the%2520limited%250Aresources%2520available%2520and%2520the%2520inherent%2520diversity%2520within%2520sign%2520data.%2520As%2520a%2520result%252C%250Aprevious%2520works%2520have%2520suffered%2520from%2520the%2520problem%2520of%2520regression%2520to%2520the%2520mean%252C%250Aleading%2520to%2520under-articulated%2520and%2520incomprehensible%2520signing.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520using%2520dictionary%2520examples%2520and%2520a%2520learnt%2520codebook%2520of%2520facial%2520expressions%250Ato%2520create%2520expressive%2520sign%2520language%2520sequences.%2520However%252C%2520simply%2520concatenating%250Asigns%2520and%2520adding%2520the%2520face%2520creates%2520robotic%2520and%2520unnatural%2520sequences.%2520To%2520address%250Athis%2520we%2520present%2520a%25207-step%2520approach%2520to%2520effectively%2520stitch%2520sequences%2520together.%250AFirst%252C%2520by%2520normalizing%2520each%2520sign%2520into%2520a%2520canonical%2520pose%252C%2520cropping%252C%2520and%2520stitching%250Awe%2520create%2520a%2520continuous%2520sequence.%2520Then%252C%2520by%2520applying%2520filtering%2520in%2520the%2520frequency%250Adomain%2520and%2520resampling%2520each%2520sign%252C%2520we%2520create%2520cohesive%2520natural%2520sequences%2520that%250Amimic%2520the%2520prosody%2520found%2520in%2520the%2520original%2520data.%2520We%2520leverage%2520a%2520SignGAN%2520model%2520to%250Amap%2520the%2520output%2520to%2520a%2520photo-realistic%2520signer%2520and%2520present%2520a%2520complete%2520Text-to-Sign%250A%2528T2S%2529%2520SLP%2520pipeline.%2520Our%2520evaluation%2520demonstrates%2520the%2520effectiveness%2520of%2520the%250Aapproach%252C%2520showcasing%2520state-of-the-art%2520performance%2520across%2520all%2520datasets.%2520Finally%252C%250Aa%2520user%2520evaluation%2520shows%2520our%2520approach%2520outperforms%2520the%2520baseline%2520model%2520and%2520is%250Acapable%2520of%2520producing%2520realistic%2520sign%2520language%2520sequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sign%20Stitching%3A%20A%20Novel%20Approach%20to%20Sign%20Language%20Production&entry.906535625=Harry%20Walsh%20and%20Ben%20Saunders%20and%20Richard%20Bowden&entry.1292438233=%20%20Sign%20Language%20Production%20%28SLP%29%20is%20a%20challenging%20task%2C%20given%20the%20limited%0Aresources%20available%20and%20the%20inherent%20diversity%20within%20sign%20data.%20As%20a%20result%2C%0Aprevious%20works%20have%20suffered%20from%20the%20problem%20of%20regression%20to%20the%20mean%2C%0Aleading%20to%20under-articulated%20and%20incomprehensible%20signing.%20In%20this%20paper%2C%20we%0Apropose%20using%20dictionary%20examples%20and%20a%20learnt%20codebook%20of%20facial%20expressions%0Ato%20create%20expressive%20sign%20language%20sequences.%20However%2C%20simply%20concatenating%0Asigns%20and%20adding%20the%20face%20creates%20robotic%20and%20unnatural%20sequences.%20To%20address%0Athis%20we%20present%20a%207-step%20approach%20to%20effectively%20stitch%20sequences%20together.%0AFirst%2C%20by%20normalizing%20each%20sign%20into%20a%20canonical%20pose%2C%20cropping%2C%20and%20stitching%0Awe%20create%20a%20continuous%20sequence.%20Then%2C%20by%20applying%20filtering%20in%20the%20frequency%0Adomain%20and%20resampling%20each%20sign%2C%20we%20create%20cohesive%20natural%20sequences%20that%0Amimic%20the%20prosody%20found%20in%20the%20original%20data.%20We%20leverage%20a%20SignGAN%20model%20to%0Amap%20the%20output%20to%20a%20photo-realistic%20signer%20and%20present%20a%20complete%20Text-to-Sign%0A%28T2S%29%20SLP%20pipeline.%20Our%20evaluation%20demonstrates%20the%20effectiveness%20of%20the%0Aapproach%2C%20showcasing%20state-of-the-art%20performance%20across%20all%20datasets.%20Finally%2C%0Aa%20user%20evaluation%20shows%20our%20approach%20outperforms%20the%20baseline%20model%20and%20is%0Acapable%20of%20producing%20realistic%20sign%20language%20sequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07663v1&entry.124074799=Read"},
{"title": "Localizing Task Information for Improved Model Merging and Compression", "author": "Ke Wang and Nikolaos Dimitriadis and Guillermo Ortiz-Jimenez and Fran\u00e7ois Fleuret and Pascal Frossard", "abstract": "  Model merging and task arithmetic have emerged as promising scalable\napproaches to merge multiple single-task checkpoints to one multi-task model,\nbut their applicability is reduced by significant performance loss. Previous\nworks have linked these drops to interference in the weight space and erasure\nof important task-specific features. Instead, in this work we show that the\ninformation required to solve each task is still preserved after merging as\ndifferent tasks mostly use non-overlapping sets of weights. We propose\nTALL-masks, a method to identify these task supports given a collection of task\nvectors and show that one can retrieve >99% of the single task accuracy by\napplying our masks to the multi-task vector, effectively compressing the\nindividual checkpoints. We study the statistics of intersections among\nconstructed masks and reveal the existence of selfish and catastrophic weights,\ni.e., parameters that are important exclusively to one task and irrelevant to\nall tasks but detrimental to multi-task fusion. For this reason, we propose\nConsensus Merging, an algorithm that eliminates such weights and improves the\ngeneral performance of existing model merging approaches. Our experiments in\nvision and NLP benchmarks with up to 20 tasks, show that Consensus Merging\nconsistently improves existing approaches. Furthermore, our proposed\ncompression scheme reduces storage from 57Gb to 8.2Gb while retaining 99.7% of\noriginal performance.\n", "link": "http://arxiv.org/abs/2405.07813v1", "date": "2024-05-13", "relevancy": 2.6304, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5351}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5237}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localizing%20Task%20Information%20for%20Improved%20Model%20Merging%20and%20Compression&body=Title%3A%20Localizing%20Task%20Information%20for%20Improved%20Model%20Merging%20and%20Compression%0AAuthor%3A%20Ke%20Wang%20and%20Nikolaos%20Dimitriadis%20and%20Guillermo%20Ortiz-Jimenez%20and%20Fran%C3%A7ois%20Fleuret%20and%20Pascal%20Frossard%0AAbstract%3A%20%20%20Model%20merging%20and%20task%20arithmetic%20have%20emerged%20as%20promising%20scalable%0Aapproaches%20to%20merge%20multiple%20single-task%20checkpoints%20to%20one%20multi-task%20model%2C%0Abut%20their%20applicability%20is%20reduced%20by%20significant%20performance%20loss.%20Previous%0Aworks%20have%20linked%20these%20drops%20to%20interference%20in%20the%20weight%20space%20and%20erasure%0Aof%20important%20task-specific%20features.%20Instead%2C%20in%20this%20work%20we%20show%20that%20the%0Ainformation%20required%20to%20solve%20each%20task%20is%20still%20preserved%20after%20merging%20as%0Adifferent%20tasks%20mostly%20use%20non-overlapping%20sets%20of%20weights.%20We%20propose%0ATALL-masks%2C%20a%20method%20to%20identify%20these%20task%20supports%20given%20a%20collection%20of%20task%0Avectors%20and%20show%20that%20one%20can%20retrieve%20%3E99%25%20of%20the%20single%20task%20accuracy%20by%0Aapplying%20our%20masks%20to%20the%20multi-task%20vector%2C%20effectively%20compressing%20the%0Aindividual%20checkpoints.%20We%20study%20the%20statistics%20of%20intersections%20among%0Aconstructed%20masks%20and%20reveal%20the%20existence%20of%20selfish%20and%20catastrophic%20weights%2C%0Ai.e.%2C%20parameters%20that%20are%20important%20exclusively%20to%20one%20task%20and%20irrelevant%20to%0Aall%20tasks%20but%20detrimental%20to%20multi-task%20fusion.%20For%20this%20reason%2C%20we%20propose%0AConsensus%20Merging%2C%20an%20algorithm%20that%20eliminates%20such%20weights%20and%20improves%20the%0Ageneral%20performance%20of%20existing%20model%20merging%20approaches.%20Our%20experiments%20in%0Avision%20and%20NLP%20benchmarks%20with%20up%20to%2020%20tasks%2C%20show%20that%20Consensus%20Merging%0Aconsistently%20improves%20existing%20approaches.%20Furthermore%2C%20our%20proposed%0Acompression%20scheme%20reduces%20storage%20from%2057Gb%20to%208.2Gb%20while%20retaining%2099.7%25%20of%0Aoriginal%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalizing%2520Task%2520Information%2520for%2520Improved%2520Model%2520Merging%2520and%2520Compression%26entry.906535625%3DKe%2520Wang%2520and%2520Nikolaos%2520Dimitriadis%2520and%2520Guillermo%2520Ortiz-Jimenez%2520and%2520Fran%25C3%25A7ois%2520Fleuret%2520and%2520Pascal%2520Frossard%26entry.1292438233%3D%2520%2520Model%2520merging%2520and%2520task%2520arithmetic%2520have%2520emerged%2520as%2520promising%2520scalable%250Aapproaches%2520to%2520merge%2520multiple%2520single-task%2520checkpoints%2520to%2520one%2520multi-task%2520model%252C%250Abut%2520their%2520applicability%2520is%2520reduced%2520by%2520significant%2520performance%2520loss.%2520Previous%250Aworks%2520have%2520linked%2520these%2520drops%2520to%2520interference%2520in%2520the%2520weight%2520space%2520and%2520erasure%250Aof%2520important%2520task-specific%2520features.%2520Instead%252C%2520in%2520this%2520work%2520we%2520show%2520that%2520the%250Ainformation%2520required%2520to%2520solve%2520each%2520task%2520is%2520still%2520preserved%2520after%2520merging%2520as%250Adifferent%2520tasks%2520mostly%2520use%2520non-overlapping%2520sets%2520of%2520weights.%2520We%2520propose%250ATALL-masks%252C%2520a%2520method%2520to%2520identify%2520these%2520task%2520supports%2520given%2520a%2520collection%2520of%2520task%250Avectors%2520and%2520show%2520that%2520one%2520can%2520retrieve%2520%253E99%2525%2520of%2520the%2520single%2520task%2520accuracy%2520by%250Aapplying%2520our%2520masks%2520to%2520the%2520multi-task%2520vector%252C%2520effectively%2520compressing%2520the%250Aindividual%2520checkpoints.%2520We%2520study%2520the%2520statistics%2520of%2520intersections%2520among%250Aconstructed%2520masks%2520and%2520reveal%2520the%2520existence%2520of%2520selfish%2520and%2520catastrophic%2520weights%252C%250Ai.e.%252C%2520parameters%2520that%2520are%2520important%2520exclusively%2520to%2520one%2520task%2520and%2520irrelevant%2520to%250Aall%2520tasks%2520but%2520detrimental%2520to%2520multi-task%2520fusion.%2520For%2520this%2520reason%252C%2520we%2520propose%250AConsensus%2520Merging%252C%2520an%2520algorithm%2520that%2520eliminates%2520such%2520weights%2520and%2520improves%2520the%250Ageneral%2520performance%2520of%2520existing%2520model%2520merging%2520approaches.%2520Our%2520experiments%2520in%250Avision%2520and%2520NLP%2520benchmarks%2520with%2520up%2520to%252020%2520tasks%252C%2520show%2520that%2520Consensus%2520Merging%250Aconsistently%2520improves%2520existing%2520approaches.%2520Furthermore%252C%2520our%2520proposed%250Acompression%2520scheme%2520reduces%2520storage%2520from%252057Gb%2520to%25208.2Gb%2520while%2520retaining%252099.7%2525%2520of%250Aoriginal%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localizing%20Task%20Information%20for%20Improved%20Model%20Merging%20and%20Compression&entry.906535625=Ke%20Wang%20and%20Nikolaos%20Dimitriadis%20and%20Guillermo%20Ortiz-Jimenez%20and%20Fran%C3%A7ois%20Fleuret%20and%20Pascal%20Frossard&entry.1292438233=%20%20Model%20merging%20and%20task%20arithmetic%20have%20emerged%20as%20promising%20scalable%0Aapproaches%20to%20merge%20multiple%20single-task%20checkpoints%20to%20one%20multi-task%20model%2C%0Abut%20their%20applicability%20is%20reduced%20by%20significant%20performance%20loss.%20Previous%0Aworks%20have%20linked%20these%20drops%20to%20interference%20in%20the%20weight%20space%20and%20erasure%0Aof%20important%20task-specific%20features.%20Instead%2C%20in%20this%20work%20we%20show%20that%20the%0Ainformation%20required%20to%20solve%20each%20task%20is%20still%20preserved%20after%20merging%20as%0Adifferent%20tasks%20mostly%20use%20non-overlapping%20sets%20of%20weights.%20We%20propose%0ATALL-masks%2C%20a%20method%20to%20identify%20these%20task%20supports%20given%20a%20collection%20of%20task%0Avectors%20and%20show%20that%20one%20can%20retrieve%20%3E99%25%20of%20the%20single%20task%20accuracy%20by%0Aapplying%20our%20masks%20to%20the%20multi-task%20vector%2C%20effectively%20compressing%20the%0Aindividual%20checkpoints.%20We%20study%20the%20statistics%20of%20intersections%20among%0Aconstructed%20masks%20and%20reveal%20the%20existence%20of%20selfish%20and%20catastrophic%20weights%2C%0Ai.e.%2C%20parameters%20that%20are%20important%20exclusively%20to%20one%20task%20and%20irrelevant%20to%0Aall%20tasks%20but%20detrimental%20to%20multi-task%20fusion.%20For%20this%20reason%2C%20we%20propose%0AConsensus%20Merging%2C%20an%20algorithm%20that%20eliminates%20such%20weights%20and%20improves%20the%0Ageneral%20performance%20of%20existing%20model%20merging%20approaches.%20Our%20experiments%20in%0Avision%20and%20NLP%20benchmarks%20with%20up%20to%2020%20tasks%2C%20show%20that%20Consensus%20Merging%0Aconsistently%20improves%20existing%20approaches.%20Furthermore%2C%20our%20proposed%0Acompression%20scheme%20reduces%20storage%20from%2057Gb%20to%208.2Gb%20while%20retaining%2099.7%25%20of%0Aoriginal%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07813v1&entry.124074799=Read"},
{"title": "FRACTAL: An Ultra-Large-Scale Aerial Lidar Dataset for 3D Semantic\n  Segmentation of Diverse Landscapes", "author": "Charles Gaydon and Michel Daab and Floryne Roche", "abstract": "  Mapping agencies are increasingly adopting Aerial Lidar Scanning (ALS) as a\nnew tool to monitor territory and support public policies. Processing ALS data\nat scale requires efficient point classification methods that perform well over\nhighly diverse territories. To evaluate them, researchers need large annotated\nLidar datasets, however, current Lidar benchmark datasets have restricted scope\nand often cover a single urban area. To bridge this data gap, we present the\nFRench ALS Clouds from TArgeted Landscapes (FRACTAL) dataset: an\nultra-large-scale aerial Lidar dataset made of 100,000 dense point clouds with\nhigh-quality labels for 7 semantic classes and spanning 250 km$^2$. FRACTAL is\nbuilt upon France's nationwide open Lidar data. It achieves spatial and\nsemantic diversity via a sampling scheme that explicitly concentrates rare\nclasses and challenging landscapes from five French regions. It should support\nthe development of 3D deep learning approaches for large-scale land monitoring.\nWe describe the nature of the source data, the sampling workflow, the content\nof the resulting dataset, and provide an initial evaluation of segmentation\nperformance using a performant 3D neural architecture.\n", "link": "http://arxiv.org/abs/2405.04634v2", "date": "2024-05-13", "relevancy": 2.6161, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.539}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5336}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FRACTAL%3A%20An%20Ultra-Large-Scale%20Aerial%20Lidar%20Dataset%20for%203D%20Semantic%0A%20%20Segmentation%20of%20Diverse%20Landscapes&body=Title%3A%20FRACTAL%3A%20An%20Ultra-Large-Scale%20Aerial%20Lidar%20Dataset%20for%203D%20Semantic%0A%20%20Segmentation%20of%20Diverse%20Landscapes%0AAuthor%3A%20Charles%20Gaydon%20and%20Michel%20Daab%20and%20Floryne%20Roche%0AAbstract%3A%20%20%20Mapping%20agencies%20are%20increasingly%20adopting%20Aerial%20Lidar%20Scanning%20%28ALS%29%20as%20a%0Anew%20tool%20to%20monitor%20territory%20and%20support%20public%20policies.%20Processing%20ALS%20data%0Aat%20scale%20requires%20efficient%20point%20classification%20methods%20that%20perform%20well%20over%0Ahighly%20diverse%20territories.%20To%20evaluate%20them%2C%20researchers%20need%20large%20annotated%0ALidar%20datasets%2C%20however%2C%20current%20Lidar%20benchmark%20datasets%20have%20restricted%20scope%0Aand%20often%20cover%20a%20single%20urban%20area.%20To%20bridge%20this%20data%20gap%2C%20we%20present%20the%0AFRench%20ALS%20Clouds%20from%20TArgeted%20Landscapes%20%28FRACTAL%29%20dataset%3A%20an%0Aultra-large-scale%20aerial%20Lidar%20dataset%20made%20of%20100%2C000%20dense%20point%20clouds%20with%0Ahigh-quality%20labels%20for%207%20semantic%20classes%20and%20spanning%20250%20km%24%5E2%24.%20FRACTAL%20is%0Abuilt%20upon%20France%27s%20nationwide%20open%20Lidar%20data.%20It%20achieves%20spatial%20and%0Asemantic%20diversity%20via%20a%20sampling%20scheme%20that%20explicitly%20concentrates%20rare%0Aclasses%20and%20challenging%20landscapes%20from%20five%20French%20regions.%20It%20should%20support%0Athe%20development%20of%203D%20deep%20learning%20approaches%20for%20large-scale%20land%20monitoring.%0AWe%20describe%20the%20nature%20of%20the%20source%20data%2C%20the%20sampling%20workflow%2C%20the%20content%0Aof%20the%20resulting%20dataset%2C%20and%20provide%20an%20initial%20evaluation%20of%20segmentation%0Aperformance%20using%20a%20performant%203D%20neural%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04634v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFRACTAL%253A%2520An%2520Ultra-Large-Scale%2520Aerial%2520Lidar%2520Dataset%2520for%25203D%2520Semantic%250A%2520%2520Segmentation%2520of%2520Diverse%2520Landscapes%26entry.906535625%3DCharles%2520Gaydon%2520and%2520Michel%2520Daab%2520and%2520Floryne%2520Roche%26entry.1292438233%3D%2520%2520Mapping%2520agencies%2520are%2520increasingly%2520adopting%2520Aerial%2520Lidar%2520Scanning%2520%2528ALS%2529%2520as%2520a%250Anew%2520tool%2520to%2520monitor%2520territory%2520and%2520support%2520public%2520policies.%2520Processing%2520ALS%2520data%250Aat%2520scale%2520requires%2520efficient%2520point%2520classification%2520methods%2520that%2520perform%2520well%2520over%250Ahighly%2520diverse%2520territories.%2520To%2520evaluate%2520them%252C%2520researchers%2520need%2520large%2520annotated%250ALidar%2520datasets%252C%2520however%252C%2520current%2520Lidar%2520benchmark%2520datasets%2520have%2520restricted%2520scope%250Aand%2520often%2520cover%2520a%2520single%2520urban%2520area.%2520To%2520bridge%2520this%2520data%2520gap%252C%2520we%2520present%2520the%250AFRench%2520ALS%2520Clouds%2520from%2520TArgeted%2520Landscapes%2520%2528FRACTAL%2529%2520dataset%253A%2520an%250Aultra-large-scale%2520aerial%2520Lidar%2520dataset%2520made%2520of%2520100%252C000%2520dense%2520point%2520clouds%2520with%250Ahigh-quality%2520labels%2520for%25207%2520semantic%2520classes%2520and%2520spanning%2520250%2520km%2524%255E2%2524.%2520FRACTAL%2520is%250Abuilt%2520upon%2520France%2527s%2520nationwide%2520open%2520Lidar%2520data.%2520It%2520achieves%2520spatial%2520and%250Asemantic%2520diversity%2520via%2520a%2520sampling%2520scheme%2520that%2520explicitly%2520concentrates%2520rare%250Aclasses%2520and%2520challenging%2520landscapes%2520from%2520five%2520French%2520regions.%2520It%2520should%2520support%250Athe%2520development%2520of%25203D%2520deep%2520learning%2520approaches%2520for%2520large-scale%2520land%2520monitoring.%250AWe%2520describe%2520the%2520nature%2520of%2520the%2520source%2520data%252C%2520the%2520sampling%2520workflow%252C%2520the%2520content%250Aof%2520the%2520resulting%2520dataset%252C%2520and%2520provide%2520an%2520initial%2520evaluation%2520of%2520segmentation%250Aperformance%2520using%2520a%2520performant%25203D%2520neural%2520architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04634v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRACTAL%3A%20An%20Ultra-Large-Scale%20Aerial%20Lidar%20Dataset%20for%203D%20Semantic%0A%20%20Segmentation%20of%20Diverse%20Landscapes&entry.906535625=Charles%20Gaydon%20and%20Michel%20Daab%20and%20Floryne%20Roche&entry.1292438233=%20%20Mapping%20agencies%20are%20increasingly%20adopting%20Aerial%20Lidar%20Scanning%20%28ALS%29%20as%20a%0Anew%20tool%20to%20monitor%20territory%20and%20support%20public%20policies.%20Processing%20ALS%20data%0Aat%20scale%20requires%20efficient%20point%20classification%20methods%20that%20perform%20well%20over%0Ahighly%20diverse%20territories.%20To%20evaluate%20them%2C%20researchers%20need%20large%20annotated%0ALidar%20datasets%2C%20however%2C%20current%20Lidar%20benchmark%20datasets%20have%20restricted%20scope%0Aand%20often%20cover%20a%20single%20urban%20area.%20To%20bridge%20this%20data%20gap%2C%20we%20present%20the%0AFRench%20ALS%20Clouds%20from%20TArgeted%20Landscapes%20%28FRACTAL%29%20dataset%3A%20an%0Aultra-large-scale%20aerial%20Lidar%20dataset%20made%20of%20100%2C000%20dense%20point%20clouds%20with%0Ahigh-quality%20labels%20for%207%20semantic%20classes%20and%20spanning%20250%20km%24%5E2%24.%20FRACTAL%20is%0Abuilt%20upon%20France%27s%20nationwide%20open%20Lidar%20data.%20It%20achieves%20spatial%20and%0Asemantic%20diversity%20via%20a%20sampling%20scheme%20that%20explicitly%20concentrates%20rare%0Aclasses%20and%20challenging%20landscapes%20from%20five%20French%20regions.%20It%20should%20support%0Athe%20development%20of%203D%20deep%20learning%20approaches%20for%20large-scale%20land%20monitoring.%0AWe%20describe%20the%20nature%20of%20the%20source%20data%2C%20the%20sampling%20workflow%2C%20the%20content%0Aof%20the%20resulting%20dataset%2C%20and%20provide%20an%20initial%20evaluation%20of%20segmentation%0Aperformance%20using%20a%20performant%203D%20neural%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04634v2&entry.124074799=Read"},
{"title": "NeuroNet: A Novel Hybrid Self-Supervised Learning Framework for Sleep\n  Stage Classification Using Single-Channel EEG", "author": "Cheol-Hui Lee and Hakseung Kim and Hyun-jee Han and Min-Kyung Jung and Byung C. Yoon and Dong-Joo Kim", "abstract": "  The classification of sleep stages is a pivotal aspect of diagnosing sleep\ndisorders and evaluating sleep quality. However, the conventional manual\nscoring process, conducted by clinicians, is time-consuming and prone to human\nbias. Recent advancements in deep learning have substantially propelled the\nautomation of sleep stage classification. Nevertheless, challenges persist,\nincluding the need for large datasets with labels and the inherent biases in\nhuman-generated annotations. This paper introduces NeuroNet, a self-supervised\nlearning (SSL) framework designed to effectively harness unlabeled\nsingle-channel sleep electroencephalogram (EEG) signals by integrating\ncontrastive learning tasks and masked prediction tasks. NeuroNet demonstrates\nsuperior performance over existing SSL methodologies through extensive\nexperimentation conducted across three polysomnography (PSG) datasets.\nAdditionally, this study proposes a Mamba-based temporal context module to\ncapture the relationships among diverse EEG epochs. Combining NeuroNet with the\nMamba-based temporal context module has demonstrated the capability to achieve,\nor even surpass, the performance of the latest supervised learning\nmethodologies, even with a limited amount of labeled data. This study is\nexpected to establish a new benchmark in sleep stage classification, promising\nto guide future research and applications in the field of sleep analysis.\n", "link": "http://arxiv.org/abs/2404.17585v2", "date": "2024-05-13", "relevancy": 2.5752, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5179}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5138}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuroNet%3A%20A%20Novel%20Hybrid%20Self-Supervised%20Learning%20Framework%20for%20Sleep%0A%20%20Stage%20Classification%20Using%20Single-Channel%20EEG&body=Title%3A%20NeuroNet%3A%20A%20Novel%20Hybrid%20Self-Supervised%20Learning%20Framework%20for%20Sleep%0A%20%20Stage%20Classification%20Using%20Single-Channel%20EEG%0AAuthor%3A%20Cheol-Hui%20Lee%20and%20Hakseung%20Kim%20and%20Hyun-jee%20Han%20and%20Min-Kyung%20Jung%20and%20Byung%20C.%20Yoon%20and%20Dong-Joo%20Kim%0AAbstract%3A%20%20%20The%20classification%20of%20sleep%20stages%20is%20a%20pivotal%20aspect%20of%20diagnosing%20sleep%0Adisorders%20and%20evaluating%20sleep%20quality.%20However%2C%20the%20conventional%20manual%0Ascoring%20process%2C%20conducted%20by%20clinicians%2C%20is%20time-consuming%20and%20prone%20to%20human%0Abias.%20Recent%20advancements%20in%20deep%20learning%20have%20substantially%20propelled%20the%0Aautomation%20of%20sleep%20stage%20classification.%20Nevertheless%2C%20challenges%20persist%2C%0Aincluding%20the%20need%20for%20large%20datasets%20with%20labels%20and%20the%20inherent%20biases%20in%0Ahuman-generated%20annotations.%20This%20paper%20introduces%20NeuroNet%2C%20a%20self-supervised%0Alearning%20%28SSL%29%20framework%20designed%20to%20effectively%20harness%20unlabeled%0Asingle-channel%20sleep%20electroencephalogram%20%28EEG%29%20signals%20by%20integrating%0Acontrastive%20learning%20tasks%20and%20masked%20prediction%20tasks.%20NeuroNet%20demonstrates%0Asuperior%20performance%20over%20existing%20SSL%20methodologies%20through%20extensive%0Aexperimentation%20conducted%20across%20three%20polysomnography%20%28PSG%29%20datasets.%0AAdditionally%2C%20this%20study%20proposes%20a%20Mamba-based%20temporal%20context%20module%20to%0Acapture%20the%20relationships%20among%20diverse%20EEG%20epochs.%20Combining%20NeuroNet%20with%20the%0AMamba-based%20temporal%20context%20module%20has%20demonstrated%20the%20capability%20to%20achieve%2C%0Aor%20even%20surpass%2C%20the%20performance%20of%20the%20latest%20supervised%20learning%0Amethodologies%2C%20even%20with%20a%20limited%20amount%20of%20labeled%20data.%20This%20study%20is%0Aexpected%20to%20establish%20a%20new%20benchmark%20in%20sleep%20stage%20classification%2C%20promising%0Ato%20guide%20future%20research%20and%20applications%20in%20the%20field%20of%20sleep%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17585v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuroNet%253A%2520A%2520Novel%2520Hybrid%2520Self-Supervised%2520Learning%2520Framework%2520for%2520Sleep%250A%2520%2520Stage%2520Classification%2520Using%2520Single-Channel%2520EEG%26entry.906535625%3DCheol-Hui%2520Lee%2520and%2520Hakseung%2520Kim%2520and%2520Hyun-jee%2520Han%2520and%2520Min-Kyung%2520Jung%2520and%2520Byung%2520C.%2520Yoon%2520and%2520Dong-Joo%2520Kim%26entry.1292438233%3D%2520%2520The%2520classification%2520of%2520sleep%2520stages%2520is%2520a%2520pivotal%2520aspect%2520of%2520diagnosing%2520sleep%250Adisorders%2520and%2520evaluating%2520sleep%2520quality.%2520However%252C%2520the%2520conventional%2520manual%250Ascoring%2520process%252C%2520conducted%2520by%2520clinicians%252C%2520is%2520time-consuming%2520and%2520prone%2520to%2520human%250Abias.%2520Recent%2520advancements%2520in%2520deep%2520learning%2520have%2520substantially%2520propelled%2520the%250Aautomation%2520of%2520sleep%2520stage%2520classification.%2520Nevertheless%252C%2520challenges%2520persist%252C%250Aincluding%2520the%2520need%2520for%2520large%2520datasets%2520with%2520labels%2520and%2520the%2520inherent%2520biases%2520in%250Ahuman-generated%2520annotations.%2520This%2520paper%2520introduces%2520NeuroNet%252C%2520a%2520self-supervised%250Alearning%2520%2528SSL%2529%2520framework%2520designed%2520to%2520effectively%2520harness%2520unlabeled%250Asingle-channel%2520sleep%2520electroencephalogram%2520%2528EEG%2529%2520signals%2520by%2520integrating%250Acontrastive%2520learning%2520tasks%2520and%2520masked%2520prediction%2520tasks.%2520NeuroNet%2520demonstrates%250Asuperior%2520performance%2520over%2520existing%2520SSL%2520methodologies%2520through%2520extensive%250Aexperimentation%2520conducted%2520across%2520three%2520polysomnography%2520%2528PSG%2529%2520datasets.%250AAdditionally%252C%2520this%2520study%2520proposes%2520a%2520Mamba-based%2520temporal%2520context%2520module%2520to%250Acapture%2520the%2520relationships%2520among%2520diverse%2520EEG%2520epochs.%2520Combining%2520NeuroNet%2520with%2520the%250AMamba-based%2520temporal%2520context%2520module%2520has%2520demonstrated%2520the%2520capability%2520to%2520achieve%252C%250Aor%2520even%2520surpass%252C%2520the%2520performance%2520of%2520the%2520latest%2520supervised%2520learning%250Amethodologies%252C%2520even%2520with%2520a%2520limited%2520amount%2520of%2520labeled%2520data.%2520This%2520study%2520is%250Aexpected%2520to%2520establish%2520a%2520new%2520benchmark%2520in%2520sleep%2520stage%2520classification%252C%2520promising%250Ato%2520guide%2520future%2520research%2520and%2520applications%2520in%2520the%2520field%2520of%2520sleep%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17585v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroNet%3A%20A%20Novel%20Hybrid%20Self-Supervised%20Learning%20Framework%20for%20Sleep%0A%20%20Stage%20Classification%20Using%20Single-Channel%20EEG&entry.906535625=Cheol-Hui%20Lee%20and%20Hakseung%20Kim%20and%20Hyun-jee%20Han%20and%20Min-Kyung%20Jung%20and%20Byung%20C.%20Yoon%20and%20Dong-Joo%20Kim&entry.1292438233=%20%20The%20classification%20of%20sleep%20stages%20is%20a%20pivotal%20aspect%20of%20diagnosing%20sleep%0Adisorders%20and%20evaluating%20sleep%20quality.%20However%2C%20the%20conventional%20manual%0Ascoring%20process%2C%20conducted%20by%20clinicians%2C%20is%20time-consuming%20and%20prone%20to%20human%0Abias.%20Recent%20advancements%20in%20deep%20learning%20have%20substantially%20propelled%20the%0Aautomation%20of%20sleep%20stage%20classification.%20Nevertheless%2C%20challenges%20persist%2C%0Aincluding%20the%20need%20for%20large%20datasets%20with%20labels%20and%20the%20inherent%20biases%20in%0Ahuman-generated%20annotations.%20This%20paper%20introduces%20NeuroNet%2C%20a%20self-supervised%0Alearning%20%28SSL%29%20framework%20designed%20to%20effectively%20harness%20unlabeled%0Asingle-channel%20sleep%20electroencephalogram%20%28EEG%29%20signals%20by%20integrating%0Acontrastive%20learning%20tasks%20and%20masked%20prediction%20tasks.%20NeuroNet%20demonstrates%0Asuperior%20performance%20over%20existing%20SSL%20methodologies%20through%20extensive%0Aexperimentation%20conducted%20across%20three%20polysomnography%20%28PSG%29%20datasets.%0AAdditionally%2C%20this%20study%20proposes%20a%20Mamba-based%20temporal%20context%20module%20to%0Acapture%20the%20relationships%20among%20diverse%20EEG%20epochs.%20Combining%20NeuroNet%20with%20the%0AMamba-based%20temporal%20context%20module%20has%20demonstrated%20the%20capability%20to%20achieve%2C%0Aor%20even%20surpass%2C%20the%20performance%20of%20the%20latest%20supervised%20learning%0Amethodologies%2C%20even%20with%20a%20limited%20amount%20of%20labeled%20data.%20This%20study%20is%0Aexpected%20to%20establish%20a%20new%20benchmark%20in%20sleep%20stage%20classification%2C%20promising%0Ato%20guide%20future%20research%20and%20applications%20in%20the%20field%20of%20sleep%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17585v2&entry.124074799=Read"},
{"title": "Coarse or Fine? Recognising Action End States without Labels", "author": "Davide Moltisanti and Hakan Bilen and Laura Sevilla-Lara and Frank Keller", "abstract": "  We focus on the problem of recognising the end state of an action in an\nimage, which is critical for understanding what action is performed and in\nwhich manner. We study this focusing on the task of predicting the coarseness\nof a cut, i.e., deciding whether an object was cut \"coarsely\" or \"finely\". No\ndataset with these annotated end states is available, so we propose an\naugmentation method to synthesise training data. We apply this method to\ncutting actions extracted from an existing action recognition dataset. Our\nmethod is object agnostic, i.e., it presupposes the location of the object but\nnot its identity. Starting from less than a hundred images of a whole object,\nwe can generate several thousands images simulating visually diverse cuts of\ndifferent coarseness. We use our synthetic data to train a model based on UNet\nand test it on real images showing coarsely/finely cut objects. Results\ndemonstrate that the model successfully recognises the end state of the cutting\naction despite the domain gap between training and testing, and that the model\ngeneralises well to unseen objects.\n", "link": "http://arxiv.org/abs/2405.07723v1", "date": "2024-05-13", "relevancy": 2.5151, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5069}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5036}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coarse%20or%20Fine%3F%20Recognising%20Action%20End%20States%20without%20Labels&body=Title%3A%20Coarse%20or%20Fine%3F%20Recognising%20Action%20End%20States%20without%20Labels%0AAuthor%3A%20Davide%20Moltisanti%20and%20Hakan%20Bilen%20and%20Laura%20Sevilla-Lara%20and%20Frank%20Keller%0AAbstract%3A%20%20%20We%20focus%20on%20the%20problem%20of%20recognising%20the%20end%20state%20of%20an%20action%20in%20an%0Aimage%2C%20which%20is%20critical%20for%20understanding%20what%20action%20is%20performed%20and%20in%0Awhich%20manner.%20We%20study%20this%20focusing%20on%20the%20task%20of%20predicting%20the%20coarseness%0Aof%20a%20cut%2C%20i.e.%2C%20deciding%20whether%20an%20object%20was%20cut%20%22coarsely%22%20or%20%22finely%22.%20No%0Adataset%20with%20these%20annotated%20end%20states%20is%20available%2C%20so%20we%20propose%20an%0Aaugmentation%20method%20to%20synthesise%20training%20data.%20We%20apply%20this%20method%20to%0Acutting%20actions%20extracted%20from%20an%20existing%20action%20recognition%20dataset.%20Our%0Amethod%20is%20object%20agnostic%2C%20i.e.%2C%20it%20presupposes%20the%20location%20of%20the%20object%20but%0Anot%20its%20identity.%20Starting%20from%20less%20than%20a%20hundred%20images%20of%20a%20whole%20object%2C%0Awe%20can%20generate%20several%20thousands%20images%20simulating%20visually%20diverse%20cuts%20of%0Adifferent%20coarseness.%20We%20use%20our%20synthetic%20data%20to%20train%20a%20model%20based%20on%20UNet%0Aand%20test%20it%20on%20real%20images%20showing%20coarsely/finely%20cut%20objects.%20Results%0Ademonstrate%20that%20the%20model%20successfully%20recognises%20the%20end%20state%20of%20the%20cutting%0Aaction%20despite%20the%20domain%20gap%20between%20training%20and%20testing%2C%20and%20that%20the%20model%0Ageneralises%20well%20to%20unseen%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07723v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoarse%2520or%2520Fine%253F%2520Recognising%2520Action%2520End%2520States%2520without%2520Labels%26entry.906535625%3DDavide%2520Moltisanti%2520and%2520Hakan%2520Bilen%2520and%2520Laura%2520Sevilla-Lara%2520and%2520Frank%2520Keller%26entry.1292438233%3D%2520%2520We%2520focus%2520on%2520the%2520problem%2520of%2520recognising%2520the%2520end%2520state%2520of%2520an%2520action%2520in%2520an%250Aimage%252C%2520which%2520is%2520critical%2520for%2520understanding%2520what%2520action%2520is%2520performed%2520and%2520in%250Awhich%2520manner.%2520We%2520study%2520this%2520focusing%2520on%2520the%2520task%2520of%2520predicting%2520the%2520coarseness%250Aof%2520a%2520cut%252C%2520i.e.%252C%2520deciding%2520whether%2520an%2520object%2520was%2520cut%2520%2522coarsely%2522%2520or%2520%2522finely%2522.%2520No%250Adataset%2520with%2520these%2520annotated%2520end%2520states%2520is%2520available%252C%2520so%2520we%2520propose%2520an%250Aaugmentation%2520method%2520to%2520synthesise%2520training%2520data.%2520We%2520apply%2520this%2520method%2520to%250Acutting%2520actions%2520extracted%2520from%2520an%2520existing%2520action%2520recognition%2520dataset.%2520Our%250Amethod%2520is%2520object%2520agnostic%252C%2520i.e.%252C%2520it%2520presupposes%2520the%2520location%2520of%2520the%2520object%2520but%250Anot%2520its%2520identity.%2520Starting%2520from%2520less%2520than%2520a%2520hundred%2520images%2520of%2520a%2520whole%2520object%252C%250Awe%2520can%2520generate%2520several%2520thousands%2520images%2520simulating%2520visually%2520diverse%2520cuts%2520of%250Adifferent%2520coarseness.%2520We%2520use%2520our%2520synthetic%2520data%2520to%2520train%2520a%2520model%2520based%2520on%2520UNet%250Aand%2520test%2520it%2520on%2520real%2520images%2520showing%2520coarsely/finely%2520cut%2520objects.%2520Results%250Ademonstrate%2520that%2520the%2520model%2520successfully%2520recognises%2520the%2520end%2520state%2520of%2520the%2520cutting%250Aaction%2520despite%2520the%2520domain%2520gap%2520between%2520training%2520and%2520testing%252C%2520and%2520that%2520the%2520model%250Ageneralises%2520well%2520to%2520unseen%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07723v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coarse%20or%20Fine%3F%20Recognising%20Action%20End%20States%20without%20Labels&entry.906535625=Davide%20Moltisanti%20and%20Hakan%20Bilen%20and%20Laura%20Sevilla-Lara%20and%20Frank%20Keller&entry.1292438233=%20%20We%20focus%20on%20the%20problem%20of%20recognising%20the%20end%20state%20of%20an%20action%20in%20an%0Aimage%2C%20which%20is%20critical%20for%20understanding%20what%20action%20is%20performed%20and%20in%0Awhich%20manner.%20We%20study%20this%20focusing%20on%20the%20task%20of%20predicting%20the%20coarseness%0Aof%20a%20cut%2C%20i.e.%2C%20deciding%20whether%20an%20object%20was%20cut%20%22coarsely%22%20or%20%22finely%22.%20No%0Adataset%20with%20these%20annotated%20end%20states%20is%20available%2C%20so%20we%20propose%20an%0Aaugmentation%20method%20to%20synthesise%20training%20data.%20We%20apply%20this%20method%20to%0Acutting%20actions%20extracted%20from%20an%20existing%20action%20recognition%20dataset.%20Our%0Amethod%20is%20object%20agnostic%2C%20i.e.%2C%20it%20presupposes%20the%20location%20of%20the%20object%20but%0Anot%20its%20identity.%20Starting%20from%20less%20than%20a%20hundred%20images%20of%20a%20whole%20object%2C%0Awe%20can%20generate%20several%20thousands%20images%20simulating%20visually%20diverse%20cuts%20of%0Adifferent%20coarseness.%20We%20use%20our%20synthetic%20data%20to%20train%20a%20model%20based%20on%20UNet%0Aand%20test%20it%20on%20real%20images%20showing%20coarsely/finely%20cut%20objects.%20Results%0Ademonstrate%20that%20the%20model%20successfully%20recognises%20the%20end%20state%20of%20the%20cutting%0Aaction%20despite%20the%20domain%20gap%20between%20training%20and%20testing%2C%20and%20that%20the%20model%0Ageneralises%20well%20to%20unseen%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07723v1&entry.124074799=Read"},
{"title": "Parallax-Tolerant Image Stitching with Epipolar Displacement Field", "author": "Jian Yu and Feipeng Da", "abstract": "  Image stitching with parallax is still a challenging task. Existing methods\noften struggle to maintain both the local and global structures of the image\nwhile reducing alignment artifacts and warping distortions. In this paper, we\npropose a novel approach that utilizes epipolar geometry to establish a warping\ntechnique based on the epipolar displacement field. Initially, the warping rule\nfor pixels in the epipolar geometry is established through the infinite\nhomography. Subsequently, the epipolar displacement field, which represents the\nsliding distance of the warped pixel along the epipolar line, is formulated by\nthin-plate splines based on the principle of local elastic deformation. The\nstitching result can be generated by inversely warping the pixels according to\nthe epipolar displacement field. This method incorporates the epipolar\nconstraints in the warping rule, which ensures high-quality alignment and\nmaintains the projectivity of the panorama. Qualitative and quantitative\ncomparative experiments demonstrate the competitiveness of the proposed method\nfor stitching images with large parallax.\n", "link": "http://arxiv.org/abs/2311.16637v2", "date": "2024-05-13", "relevancy": 2.4364, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4966}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4956}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parallax-Tolerant%20Image%20Stitching%20with%20Epipolar%20Displacement%20Field&body=Title%3A%20Parallax-Tolerant%20Image%20Stitching%20with%20Epipolar%20Displacement%20Field%0AAuthor%3A%20Jian%20Yu%20and%20Feipeng%20Da%0AAbstract%3A%20%20%20Image%20stitching%20with%20parallax%20is%20still%20a%20challenging%20task.%20Existing%20methods%0Aoften%20struggle%20to%20maintain%20both%20the%20local%20and%20global%20structures%20of%20the%20image%0Awhile%20reducing%20alignment%20artifacts%20and%20warping%20distortions.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20approach%20that%20utilizes%20epipolar%20geometry%20to%20establish%20a%20warping%0Atechnique%20based%20on%20the%20epipolar%20displacement%20field.%20Initially%2C%20the%20warping%20rule%0Afor%20pixels%20in%20the%20epipolar%20geometry%20is%20established%20through%20the%20infinite%0Ahomography.%20Subsequently%2C%20the%20epipolar%20displacement%20field%2C%20which%20represents%20the%0Asliding%20distance%20of%20the%20warped%20pixel%20along%20the%20epipolar%20line%2C%20is%20formulated%20by%0Athin-plate%20splines%20based%20on%20the%20principle%20of%20local%20elastic%20deformation.%20The%0Astitching%20result%20can%20be%20generated%20by%20inversely%20warping%20the%20pixels%20according%20to%0Athe%20epipolar%20displacement%20field.%20This%20method%20incorporates%20the%20epipolar%0Aconstraints%20in%20the%20warping%20rule%2C%20which%20ensures%20high-quality%20alignment%20and%0Amaintains%20the%20projectivity%20of%20the%20panorama.%20Qualitative%20and%20quantitative%0Acomparative%20experiments%20demonstrate%20the%20competitiveness%20of%20the%20proposed%20method%0Afor%20stitching%20images%20with%20large%20parallax.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.16637v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParallax-Tolerant%2520Image%2520Stitching%2520with%2520Epipolar%2520Displacement%2520Field%26entry.906535625%3DJian%2520Yu%2520and%2520Feipeng%2520Da%26entry.1292438233%3D%2520%2520Image%2520stitching%2520with%2520parallax%2520is%2520still%2520a%2520challenging%2520task.%2520Existing%2520methods%250Aoften%2520struggle%2520to%2520maintain%2520both%2520the%2520local%2520and%2520global%2520structures%2520of%2520the%2520image%250Awhile%2520reducing%2520alignment%2520artifacts%2520and%2520warping%2520distortions.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520approach%2520that%2520utilizes%2520epipolar%2520geometry%2520to%2520establish%2520a%2520warping%250Atechnique%2520based%2520on%2520the%2520epipolar%2520displacement%2520field.%2520Initially%252C%2520the%2520warping%2520rule%250Afor%2520pixels%2520in%2520the%2520epipolar%2520geometry%2520is%2520established%2520through%2520the%2520infinite%250Ahomography.%2520Subsequently%252C%2520the%2520epipolar%2520displacement%2520field%252C%2520which%2520represents%2520the%250Asliding%2520distance%2520of%2520the%2520warped%2520pixel%2520along%2520the%2520epipolar%2520line%252C%2520is%2520formulated%2520by%250Athin-plate%2520splines%2520based%2520on%2520the%2520principle%2520of%2520local%2520elastic%2520deformation.%2520The%250Astitching%2520result%2520can%2520be%2520generated%2520by%2520inversely%2520warping%2520the%2520pixels%2520according%2520to%250Athe%2520epipolar%2520displacement%2520field.%2520This%2520method%2520incorporates%2520the%2520epipolar%250Aconstraints%2520in%2520the%2520warping%2520rule%252C%2520which%2520ensures%2520high-quality%2520alignment%2520and%250Amaintains%2520the%2520projectivity%2520of%2520the%2520panorama.%2520Qualitative%2520and%2520quantitative%250Acomparative%2520experiments%2520demonstrate%2520the%2520competitiveness%2520of%2520the%2520proposed%2520method%250Afor%2520stitching%2520images%2520with%2520large%2520parallax.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.16637v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parallax-Tolerant%20Image%20Stitching%20with%20Epipolar%20Displacement%20Field&entry.906535625=Jian%20Yu%20and%20Feipeng%20Da&entry.1292438233=%20%20Image%20stitching%20with%20parallax%20is%20still%20a%20challenging%20task.%20Existing%20methods%0Aoften%20struggle%20to%20maintain%20both%20the%20local%20and%20global%20structures%20of%20the%20image%0Awhile%20reducing%20alignment%20artifacts%20and%20warping%20distortions.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20approach%20that%20utilizes%20epipolar%20geometry%20to%20establish%20a%20warping%0Atechnique%20based%20on%20the%20epipolar%20displacement%20field.%20Initially%2C%20the%20warping%20rule%0Afor%20pixels%20in%20the%20epipolar%20geometry%20is%20established%20through%20the%20infinite%0Ahomography.%20Subsequently%2C%20the%20epipolar%20displacement%20field%2C%20which%20represents%20the%0Asliding%20distance%20of%20the%20warped%20pixel%20along%20the%20epipolar%20line%2C%20is%20formulated%20by%0Athin-plate%20splines%20based%20on%20the%20principle%20of%20local%20elastic%20deformation.%20The%0Astitching%20result%20can%20be%20generated%20by%20inversely%20warping%20the%20pixels%20according%20to%0Athe%20epipolar%20displacement%20field.%20This%20method%20incorporates%20the%20epipolar%0Aconstraints%20in%20the%20warping%20rule%2C%20which%20ensures%20high-quality%20alignment%20and%0Amaintains%20the%20projectivity%20of%20the%20panorama.%20Qualitative%20and%20quantitative%0Acomparative%20experiments%20demonstrate%20the%20competitiveness%20of%20the%20proposed%20method%0Afor%20stitching%20images%20with%20large%20parallax.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.16637v2&entry.124074799=Read"},
{"title": "Lai Loss: A Novel Loss Integrating Regularization", "author": "YuFei Lai", "abstract": "  In the field of machine learning, traditional regularization methods\ngenerally tend to directly add regularization terms to the loss function. This\npaper introduces the \"Lai loss\", a novel loss design that integrates the\nregularization terms (gradient component) into the traditional loss function\nthrough a straightforward geometric ideation. This design innovatively\npenalizes the gradient vectors through the loss, effectively controlling the\nmodel's smoothness and offering the dual benefits of reducing overfitting and\navoiding underfitting. Subsequently, we proposed a random sampling method that\nsuccessfully addresses the challenges associated with its application under\nlarge sample conditions. We conducted preliminary experiments using publicly\navailable datasets from Kaggle, demonstrating that the design of Lai loss can\ncontrol the model's smoothness while ensuring maximum accuracy.\n", "link": "http://arxiv.org/abs/2405.07884v1", "date": "2024-05-13", "relevancy": 2.4185, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5076}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4861}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lai%20Loss%3A%20A%20Novel%20Loss%20Integrating%20Regularization&body=Title%3A%20Lai%20Loss%3A%20A%20Novel%20Loss%20Integrating%20Regularization%0AAuthor%3A%20YuFei%20Lai%0AAbstract%3A%20%20%20In%20the%20field%20of%20machine%20learning%2C%20traditional%20regularization%20methods%0Agenerally%20tend%20to%20directly%20add%20regularization%20terms%20to%20the%20loss%20function.%20This%0Apaper%20introduces%20the%20%22Lai%20loss%22%2C%20a%20novel%20loss%20design%20that%20integrates%20the%0Aregularization%20terms%20%28gradient%20component%29%20into%20the%20traditional%20loss%20function%0Athrough%20a%20straightforward%20geometric%20ideation.%20This%20design%20innovatively%0Apenalizes%20the%20gradient%20vectors%20through%20the%20loss%2C%20effectively%20controlling%20the%0Amodel%27s%20smoothness%20and%20offering%20the%20dual%20benefits%20of%20reducing%20overfitting%20and%0Aavoiding%20underfitting.%20Subsequently%2C%20we%20proposed%20a%20random%20sampling%20method%20that%0Asuccessfully%20addresses%20the%20challenges%20associated%20with%20its%20application%20under%0Alarge%20sample%20conditions.%20We%20conducted%20preliminary%20experiments%20using%20publicly%0Aavailable%20datasets%20from%20Kaggle%2C%20demonstrating%20that%20the%20design%20of%20Lai%20loss%20can%0Acontrol%20the%20model%27s%20smoothness%20while%20ensuring%20maximum%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07884v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLai%2520Loss%253A%2520A%2520Novel%2520Loss%2520Integrating%2520Regularization%26entry.906535625%3DYuFei%2520Lai%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520machine%2520learning%252C%2520traditional%2520regularization%2520methods%250Agenerally%2520tend%2520to%2520directly%2520add%2520regularization%2520terms%2520to%2520the%2520loss%2520function.%2520This%250Apaper%2520introduces%2520the%2520%2522Lai%2520loss%2522%252C%2520a%2520novel%2520loss%2520design%2520that%2520integrates%2520the%250Aregularization%2520terms%2520%2528gradient%2520component%2529%2520into%2520the%2520traditional%2520loss%2520function%250Athrough%2520a%2520straightforward%2520geometric%2520ideation.%2520This%2520design%2520innovatively%250Apenalizes%2520the%2520gradient%2520vectors%2520through%2520the%2520loss%252C%2520effectively%2520controlling%2520the%250Amodel%2527s%2520smoothness%2520and%2520offering%2520the%2520dual%2520benefits%2520of%2520reducing%2520overfitting%2520and%250Aavoiding%2520underfitting.%2520Subsequently%252C%2520we%2520proposed%2520a%2520random%2520sampling%2520method%2520that%250Asuccessfully%2520addresses%2520the%2520challenges%2520associated%2520with%2520its%2520application%2520under%250Alarge%2520sample%2520conditions.%2520We%2520conducted%2520preliminary%2520experiments%2520using%2520publicly%250Aavailable%2520datasets%2520from%2520Kaggle%252C%2520demonstrating%2520that%2520the%2520design%2520of%2520Lai%2520loss%2520can%250Acontrol%2520the%2520model%2527s%2520smoothness%2520while%2520ensuring%2520maximum%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07884v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lai%20Loss%3A%20A%20Novel%20Loss%20Integrating%20Regularization&entry.906535625=YuFei%20Lai&entry.1292438233=%20%20In%20the%20field%20of%20machine%20learning%2C%20traditional%20regularization%20methods%0Agenerally%20tend%20to%20directly%20add%20regularization%20terms%20to%20the%20loss%20function.%20This%0Apaper%20introduces%20the%20%22Lai%20loss%22%2C%20a%20novel%20loss%20design%20that%20integrates%20the%0Aregularization%20terms%20%28gradient%20component%29%20into%20the%20traditional%20loss%20function%0Athrough%20a%20straightforward%20geometric%20ideation.%20This%20design%20innovatively%0Apenalizes%20the%20gradient%20vectors%20through%20the%20loss%2C%20effectively%20controlling%20the%0Amodel%27s%20smoothness%20and%20offering%20the%20dual%20benefits%20of%20reducing%20overfitting%20and%0Aavoiding%20underfitting.%20Subsequently%2C%20we%20proposed%20a%20random%20sampling%20method%20that%0Asuccessfully%20addresses%20the%20challenges%20associated%20with%20its%20application%20under%0Alarge%20sample%20conditions.%20We%20conducted%20preliminary%20experiments%20using%20publicly%0Aavailable%20datasets%20from%20Kaggle%2C%20demonstrating%20that%20the%20design%20of%20Lai%20loss%20can%0Acontrol%20the%20model%27s%20smoothness%20while%20ensuring%20maximum%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07884v1&entry.124074799=Read"},
{"title": "Active Learning with Simple Questions", "author": "Vasilis Kontonis and Mingchen Ma and Christos Tzamos", "abstract": "  We consider an active learning setting where a learner is presented with a\npool S of n unlabeled examples belonging to a domain X and asks queries to find\nthe underlying labeling that agrees with a target concept h^* \\in H.\n  In contrast to traditional active learning that queries a single example for\nits label, we study more general region queries that allow the learner to pick\na subset of the domain T \\subset X and a target label y and ask a labeler\nwhether h^*(x) = y for every example in the set T \\cap S.\n  Such more powerful queries allow us to bypass the limitations of traditional\nactive learning and use significantly fewer rounds of interactions to learn but\ncan potentially lead to a significantly more complex query language. Our main\ncontribution is quantifying the trade-off between the number of queries and the\ncomplexity of the query language used by the learner.\n  We measure the complexity of the region queries via the VC dimension of the\nfamily of regions. We show that given any hypothesis class H with VC dimension\nd, one can design a region query family Q with VC dimension O(d) such that for\nevery set of n examples S \\subset X and every h^* \\in H, a learner can submit\nO(d log n) queries from Q to a labeler and perfectly label S. We show a\nmatching lower bound by designing a hypothesis class H with VC dimension d and\na dataset S \\subset X of size n such that any learning algorithm using any\nquery class with VC dimension O(d) must make poly(n) queries to label S\nperfectly.\n  Finally, we focus on well-studied hypothesis classes including unions of\nintervals, high-dimensional boxes, and d-dimensional halfspaces, and obtain\nstronger results. In particular, we design learning algorithms that (i) are\ncomputationally efficient and (ii) work even when the queries are not answered\nbased on the learner's pool of examples S but on some unknown superset L of S\n", "link": "http://arxiv.org/abs/2405.07937v1", "date": "2024-05-13", "relevancy": 2.4007, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4934}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4758}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Learning%20with%20Simple%20Questions&body=Title%3A%20Active%20Learning%20with%20Simple%20Questions%0AAuthor%3A%20Vasilis%20Kontonis%20and%20Mingchen%20Ma%20and%20Christos%20Tzamos%0AAbstract%3A%20%20%20We%20consider%20an%20active%20learning%20setting%20where%20a%20learner%20is%20presented%20with%20a%0Apool%20S%20of%20n%20unlabeled%20examples%20belonging%20to%20a%20domain%20X%20and%20asks%20queries%20to%20find%0Athe%20underlying%20labeling%20that%20agrees%20with%20a%20target%20concept%20h%5E%2A%20%5Cin%20H.%0A%20%20In%20contrast%20to%20traditional%20active%20learning%20that%20queries%20a%20single%20example%20for%0Aits%20label%2C%20we%20study%20more%20general%20region%20queries%20that%20allow%20the%20learner%20to%20pick%0Aa%20subset%20of%20the%20domain%20T%20%5Csubset%20X%20and%20a%20target%20label%20y%20and%20ask%20a%20labeler%0Awhether%20h%5E%2A%28x%29%20%3D%20y%20for%20every%20example%20in%20the%20set%20T%20%5Ccap%20S.%0A%20%20Such%20more%20powerful%20queries%20allow%20us%20to%20bypass%20the%20limitations%20of%20traditional%0Aactive%20learning%20and%20use%20significantly%20fewer%20rounds%20of%20interactions%20to%20learn%20but%0Acan%20potentially%20lead%20to%20a%20significantly%20more%20complex%20query%20language.%20Our%20main%0Acontribution%20is%20quantifying%20the%20trade-off%20between%20the%20number%20of%20queries%20and%20the%0Acomplexity%20of%20the%20query%20language%20used%20by%20the%20learner.%0A%20%20We%20measure%20the%20complexity%20of%20the%20region%20queries%20via%20the%20VC%20dimension%20of%20the%0Afamily%20of%20regions.%20We%20show%20that%20given%20any%20hypothesis%20class%20H%20with%20VC%20dimension%0Ad%2C%20one%20can%20design%20a%20region%20query%20family%20Q%20with%20VC%20dimension%20O%28d%29%20such%20that%20for%0Aevery%20set%20of%20n%20examples%20S%20%5Csubset%20X%20and%20every%20h%5E%2A%20%5Cin%20H%2C%20a%20learner%20can%20submit%0AO%28d%20log%20n%29%20queries%20from%20Q%20to%20a%20labeler%20and%20perfectly%20label%20S.%20We%20show%20a%0Amatching%20lower%20bound%20by%20designing%20a%20hypothesis%20class%20H%20with%20VC%20dimension%20d%20and%0Aa%20dataset%20S%20%5Csubset%20X%20of%20size%20n%20such%20that%20any%20learning%20algorithm%20using%20any%0Aquery%20class%20with%20VC%20dimension%20O%28d%29%20must%20make%20poly%28n%29%20queries%20to%20label%20S%0Aperfectly.%0A%20%20Finally%2C%20we%20focus%20on%20well-studied%20hypothesis%20classes%20including%20unions%20of%0Aintervals%2C%20high-dimensional%20boxes%2C%20and%20d-dimensional%20halfspaces%2C%20and%20obtain%0Astronger%20results.%20In%20particular%2C%20we%20design%20learning%20algorithms%20that%20%28i%29%20are%0Acomputationally%20efficient%20and%20%28ii%29%20work%20even%20when%20the%20queries%20are%20not%20answered%0Abased%20on%20the%20learner%27s%20pool%20of%20examples%20S%20but%20on%20some%20unknown%20superset%20L%20of%20S%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07937v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Learning%2520with%2520Simple%2520Questions%26entry.906535625%3DVasilis%2520Kontonis%2520and%2520Mingchen%2520Ma%2520and%2520Christos%2520Tzamos%26entry.1292438233%3D%2520%2520We%2520consider%2520an%2520active%2520learning%2520setting%2520where%2520a%2520learner%2520is%2520presented%2520with%2520a%250Apool%2520S%2520of%2520n%2520unlabeled%2520examples%2520belonging%2520to%2520a%2520domain%2520X%2520and%2520asks%2520queries%2520to%2520find%250Athe%2520underlying%2520labeling%2520that%2520agrees%2520with%2520a%2520target%2520concept%2520h%255E%252A%2520%255Cin%2520H.%250A%2520%2520In%2520contrast%2520to%2520traditional%2520active%2520learning%2520that%2520queries%2520a%2520single%2520example%2520for%250Aits%2520label%252C%2520we%2520study%2520more%2520general%2520region%2520queries%2520that%2520allow%2520the%2520learner%2520to%2520pick%250Aa%2520subset%2520of%2520the%2520domain%2520T%2520%255Csubset%2520X%2520and%2520a%2520target%2520label%2520y%2520and%2520ask%2520a%2520labeler%250Awhether%2520h%255E%252A%2528x%2529%2520%253D%2520y%2520for%2520every%2520example%2520in%2520the%2520set%2520T%2520%255Ccap%2520S.%250A%2520%2520Such%2520more%2520powerful%2520queries%2520allow%2520us%2520to%2520bypass%2520the%2520limitations%2520of%2520traditional%250Aactive%2520learning%2520and%2520use%2520significantly%2520fewer%2520rounds%2520of%2520interactions%2520to%2520learn%2520but%250Acan%2520potentially%2520lead%2520to%2520a%2520significantly%2520more%2520complex%2520query%2520language.%2520Our%2520main%250Acontribution%2520is%2520quantifying%2520the%2520trade-off%2520between%2520the%2520number%2520of%2520queries%2520and%2520the%250Acomplexity%2520of%2520the%2520query%2520language%2520used%2520by%2520the%2520learner.%250A%2520%2520We%2520measure%2520the%2520complexity%2520of%2520the%2520region%2520queries%2520via%2520the%2520VC%2520dimension%2520of%2520the%250Afamily%2520of%2520regions.%2520We%2520show%2520that%2520given%2520any%2520hypothesis%2520class%2520H%2520with%2520VC%2520dimension%250Ad%252C%2520one%2520can%2520design%2520a%2520region%2520query%2520family%2520Q%2520with%2520VC%2520dimension%2520O%2528d%2529%2520such%2520that%2520for%250Aevery%2520set%2520of%2520n%2520examples%2520S%2520%255Csubset%2520X%2520and%2520every%2520h%255E%252A%2520%255Cin%2520H%252C%2520a%2520learner%2520can%2520submit%250AO%2528d%2520log%2520n%2529%2520queries%2520from%2520Q%2520to%2520a%2520labeler%2520and%2520perfectly%2520label%2520S.%2520We%2520show%2520a%250Amatching%2520lower%2520bound%2520by%2520designing%2520a%2520hypothesis%2520class%2520H%2520with%2520VC%2520dimension%2520d%2520and%250Aa%2520dataset%2520S%2520%255Csubset%2520X%2520of%2520size%2520n%2520such%2520that%2520any%2520learning%2520algorithm%2520using%2520any%250Aquery%2520class%2520with%2520VC%2520dimension%2520O%2528d%2529%2520must%2520make%2520poly%2528n%2529%2520queries%2520to%2520label%2520S%250Aperfectly.%250A%2520%2520Finally%252C%2520we%2520focus%2520on%2520well-studied%2520hypothesis%2520classes%2520including%2520unions%2520of%250Aintervals%252C%2520high-dimensional%2520boxes%252C%2520and%2520d-dimensional%2520halfspaces%252C%2520and%2520obtain%250Astronger%2520results.%2520In%2520particular%252C%2520we%2520design%2520learning%2520algorithms%2520that%2520%2528i%2529%2520are%250Acomputationally%2520efficient%2520and%2520%2528ii%2529%2520work%2520even%2520when%2520the%2520queries%2520are%2520not%2520answered%250Abased%2520on%2520the%2520learner%2527s%2520pool%2520of%2520examples%2520S%2520but%2520on%2520some%2520unknown%2520superset%2520L%2520of%2520S%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07937v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Learning%20with%20Simple%20Questions&entry.906535625=Vasilis%20Kontonis%20and%20Mingchen%20Ma%20and%20Christos%20Tzamos&entry.1292438233=%20%20We%20consider%20an%20active%20learning%20setting%20where%20a%20learner%20is%20presented%20with%20a%0Apool%20S%20of%20n%20unlabeled%20examples%20belonging%20to%20a%20domain%20X%20and%20asks%20queries%20to%20find%0Athe%20underlying%20labeling%20that%20agrees%20with%20a%20target%20concept%20h%5E%2A%20%5Cin%20H.%0A%20%20In%20contrast%20to%20traditional%20active%20learning%20that%20queries%20a%20single%20example%20for%0Aits%20label%2C%20we%20study%20more%20general%20region%20queries%20that%20allow%20the%20learner%20to%20pick%0Aa%20subset%20of%20the%20domain%20T%20%5Csubset%20X%20and%20a%20target%20label%20y%20and%20ask%20a%20labeler%0Awhether%20h%5E%2A%28x%29%20%3D%20y%20for%20every%20example%20in%20the%20set%20T%20%5Ccap%20S.%0A%20%20Such%20more%20powerful%20queries%20allow%20us%20to%20bypass%20the%20limitations%20of%20traditional%0Aactive%20learning%20and%20use%20significantly%20fewer%20rounds%20of%20interactions%20to%20learn%20but%0Acan%20potentially%20lead%20to%20a%20significantly%20more%20complex%20query%20language.%20Our%20main%0Acontribution%20is%20quantifying%20the%20trade-off%20between%20the%20number%20of%20queries%20and%20the%0Acomplexity%20of%20the%20query%20language%20used%20by%20the%20learner.%0A%20%20We%20measure%20the%20complexity%20of%20the%20region%20queries%20via%20the%20VC%20dimension%20of%20the%0Afamily%20of%20regions.%20We%20show%20that%20given%20any%20hypothesis%20class%20H%20with%20VC%20dimension%0Ad%2C%20one%20can%20design%20a%20region%20query%20family%20Q%20with%20VC%20dimension%20O%28d%29%20such%20that%20for%0Aevery%20set%20of%20n%20examples%20S%20%5Csubset%20X%20and%20every%20h%5E%2A%20%5Cin%20H%2C%20a%20learner%20can%20submit%0AO%28d%20log%20n%29%20queries%20from%20Q%20to%20a%20labeler%20and%20perfectly%20label%20S.%20We%20show%20a%0Amatching%20lower%20bound%20by%20designing%20a%20hypothesis%20class%20H%20with%20VC%20dimension%20d%20and%0Aa%20dataset%20S%20%5Csubset%20X%20of%20size%20n%20such%20that%20any%20learning%20algorithm%20using%20any%0Aquery%20class%20with%20VC%20dimension%20O%28d%29%20must%20make%20poly%28n%29%20queries%20to%20label%20S%0Aperfectly.%0A%20%20Finally%2C%20we%20focus%20on%20well-studied%20hypothesis%20classes%20including%20unions%20of%0Aintervals%2C%20high-dimensional%20boxes%2C%20and%20d-dimensional%20halfspaces%2C%20and%20obtain%0Astronger%20results.%20In%20particular%2C%20we%20design%20learning%20algorithms%20that%20%28i%29%20are%0Acomputationally%20efficient%20and%20%28ii%29%20work%20even%20when%20the%20queries%20are%20not%20answered%0Abased%20on%20the%20learner%27s%20pool%20of%20examples%20S%20but%20on%20some%20unknown%20superset%20L%20of%20S%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07937v1&entry.124074799=Read"},
{"title": "MonoMAE: Enhancing Monocular 3D Detection through Depth-Aware Masked\n  Autoencoders", "author": "Xueying Jiang and Sheng Jin and Xiaoqin Zhang and Ling Shao and Shijian Lu", "abstract": "  Monocular 3D object detection aims for precise 3D localization and\nidentification of objects from a single-view image. Despite its recent\nprogress, it often struggles while handling pervasive object occlusions that\ntend to complicate and degrade the prediction of object dimensions, depths, and\norientations. We design MonoMAE, a monocular 3D detector inspired by Masked\nAutoencoders that addresses the object occlusion issue by masking and\nreconstructing objects in the feature space. MonoMAE consists of two novel\ndesigns. The first is depth-aware masking that selectively masks certain parts\nof non-occluded object queries in the feature space for simulating occluded\nobject queries for network training. It masks non-occluded object queries by\nbalancing the masked and preserved query portions adaptively according to the\ndepth information. The second is lightweight query completion that works with\nthe depth-aware masking to learn to reconstruct and complete the masked object\nqueries. With the proposed object occlusion and completion, MonoMAE learns\nenriched 3D representations that achieve superior monocular 3D detection\nperformance qualitatively and quantitatively for both occluded and non-occluded\nobjects. Additionally, MonoMAE learns generalizable representations that can\nwork well in new domains.\n", "link": "http://arxiv.org/abs/2405.07696v1", "date": "2024-05-13", "relevancy": 2.383, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6304}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5968}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MonoMAE%3A%20Enhancing%20Monocular%203D%20Detection%20through%20Depth-Aware%20Masked%0A%20%20Autoencoders&body=Title%3A%20MonoMAE%3A%20Enhancing%20Monocular%203D%20Detection%20through%20Depth-Aware%20Masked%0A%20%20Autoencoders%0AAuthor%3A%20Xueying%20Jiang%20and%20Sheng%20Jin%20and%20Xiaoqin%20Zhang%20and%20Ling%20Shao%20and%20Shijian%20Lu%0AAbstract%3A%20%20%20Monocular%203D%20object%20detection%20aims%20for%20precise%203D%20localization%20and%0Aidentification%20of%20objects%20from%20a%20single-view%20image.%20Despite%20its%20recent%0Aprogress%2C%20it%20often%20struggles%20while%20handling%20pervasive%20object%20occlusions%20that%0Atend%20to%20complicate%20and%20degrade%20the%20prediction%20of%20object%20dimensions%2C%20depths%2C%20and%0Aorientations.%20We%20design%20MonoMAE%2C%20a%20monocular%203D%20detector%20inspired%20by%20Masked%0AAutoencoders%20that%20addresses%20the%20object%20occlusion%20issue%20by%20masking%20and%0Areconstructing%20objects%20in%20the%20feature%20space.%20MonoMAE%20consists%20of%20two%20novel%0Adesigns.%20The%20first%20is%20depth-aware%20masking%20that%20selectively%20masks%20certain%20parts%0Aof%20non-occluded%20object%20queries%20in%20the%20feature%20space%20for%20simulating%20occluded%0Aobject%20queries%20for%20network%20training.%20It%20masks%20non-occluded%20object%20queries%20by%0Abalancing%20the%20masked%20and%20preserved%20query%20portions%20adaptively%20according%20to%20the%0Adepth%20information.%20The%20second%20is%20lightweight%20query%20completion%20that%20works%20with%0Athe%20depth-aware%20masking%20to%20learn%20to%20reconstruct%20and%20complete%20the%20masked%20object%0Aqueries.%20With%20the%20proposed%20object%20occlusion%20and%20completion%2C%20MonoMAE%20learns%0Aenriched%203D%20representations%20that%20achieve%20superior%20monocular%203D%20detection%0Aperformance%20qualitatively%20and%20quantitatively%20for%20both%20occluded%20and%20non-occluded%0Aobjects.%20Additionally%2C%20MonoMAE%20learns%20generalizable%20representations%20that%20can%0Awork%20well%20in%20new%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMonoMAE%253A%2520Enhancing%2520Monocular%25203D%2520Detection%2520through%2520Depth-Aware%2520Masked%250A%2520%2520Autoencoders%26entry.906535625%3DXueying%2520Jiang%2520and%2520Sheng%2520Jin%2520and%2520Xiaoqin%2520Zhang%2520and%2520Ling%2520Shao%2520and%2520Shijian%2520Lu%26entry.1292438233%3D%2520%2520Monocular%25203D%2520object%2520detection%2520aims%2520for%2520precise%25203D%2520localization%2520and%250Aidentification%2520of%2520objects%2520from%2520a%2520single-view%2520image.%2520Despite%2520its%2520recent%250Aprogress%252C%2520it%2520often%2520struggles%2520while%2520handling%2520pervasive%2520object%2520occlusions%2520that%250Atend%2520to%2520complicate%2520and%2520degrade%2520the%2520prediction%2520of%2520object%2520dimensions%252C%2520depths%252C%2520and%250Aorientations.%2520We%2520design%2520MonoMAE%252C%2520a%2520monocular%25203D%2520detector%2520inspired%2520by%2520Masked%250AAutoencoders%2520that%2520addresses%2520the%2520object%2520occlusion%2520issue%2520by%2520masking%2520and%250Areconstructing%2520objects%2520in%2520the%2520feature%2520space.%2520MonoMAE%2520consists%2520of%2520two%2520novel%250Adesigns.%2520The%2520first%2520is%2520depth-aware%2520masking%2520that%2520selectively%2520masks%2520certain%2520parts%250Aof%2520non-occluded%2520object%2520queries%2520in%2520the%2520feature%2520space%2520for%2520simulating%2520occluded%250Aobject%2520queries%2520for%2520network%2520training.%2520It%2520masks%2520non-occluded%2520object%2520queries%2520by%250Abalancing%2520the%2520masked%2520and%2520preserved%2520query%2520portions%2520adaptively%2520according%2520to%2520the%250Adepth%2520information.%2520The%2520second%2520is%2520lightweight%2520query%2520completion%2520that%2520works%2520with%250Athe%2520depth-aware%2520masking%2520to%2520learn%2520to%2520reconstruct%2520and%2520complete%2520the%2520masked%2520object%250Aqueries.%2520With%2520the%2520proposed%2520object%2520occlusion%2520and%2520completion%252C%2520MonoMAE%2520learns%250Aenriched%25203D%2520representations%2520that%2520achieve%2520superior%2520monocular%25203D%2520detection%250Aperformance%2520qualitatively%2520and%2520quantitatively%2520for%2520both%2520occluded%2520and%2520non-occluded%250Aobjects.%2520Additionally%252C%2520MonoMAE%2520learns%2520generalizable%2520representations%2520that%2520can%250Awork%2520well%2520in%2520new%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MonoMAE%3A%20Enhancing%20Monocular%203D%20Detection%20through%20Depth-Aware%20Masked%0A%20%20Autoencoders&entry.906535625=Xueying%20Jiang%20and%20Sheng%20Jin%20and%20Xiaoqin%20Zhang%20and%20Ling%20Shao%20and%20Shijian%20Lu&entry.1292438233=%20%20Monocular%203D%20object%20detection%20aims%20for%20precise%203D%20localization%20and%0Aidentification%20of%20objects%20from%20a%20single-view%20image.%20Despite%20its%20recent%0Aprogress%2C%20it%20often%20struggles%20while%20handling%20pervasive%20object%20occlusions%20that%0Atend%20to%20complicate%20and%20degrade%20the%20prediction%20of%20object%20dimensions%2C%20depths%2C%20and%0Aorientations.%20We%20design%20MonoMAE%2C%20a%20monocular%203D%20detector%20inspired%20by%20Masked%0AAutoencoders%20that%20addresses%20the%20object%20occlusion%20issue%20by%20masking%20and%0Areconstructing%20objects%20in%20the%20feature%20space.%20MonoMAE%20consists%20of%20two%20novel%0Adesigns.%20The%20first%20is%20depth-aware%20masking%20that%20selectively%20masks%20certain%20parts%0Aof%20non-occluded%20object%20queries%20in%20the%20feature%20space%20for%20simulating%20occluded%0Aobject%20queries%20for%20network%20training.%20It%20masks%20non-occluded%20object%20queries%20by%0Abalancing%20the%20masked%20and%20preserved%20query%20portions%20adaptively%20according%20to%20the%0Adepth%20information.%20The%20second%20is%20lightweight%20query%20completion%20that%20works%20with%0Athe%20depth-aware%20masking%20to%20learn%20to%20reconstruct%20and%20complete%20the%20masked%20object%0Aqueries.%20With%20the%20proposed%20object%20occlusion%20and%20completion%2C%20MonoMAE%20learns%0Aenriched%203D%20representations%20that%20achieve%20superior%20monocular%203D%20detection%0Aperformance%20qualitatively%20and%20quantitatively%20for%20both%20occluded%20and%20non-occluded%0Aobjects.%20Additionally%2C%20MonoMAE%20learns%20generalizable%20representations%20that%20can%0Awork%20well%20in%20new%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07696v1&entry.124074799=Read"},
{"title": "PeFLL: Personalized Federated Learning by Learning to Learn", "author": "Jonathan Scott and Hossein Zakerinia and Christoph H. Lampert", "abstract": "  We present PeFLL, a new personalized federated learning algorithm that\nimproves over the state-of-the-art in three aspects: 1) it produces more\naccurate models, especially in the low-data regime, and not only for clients\npresent during its training phase, but also for any that may emerge in the\nfuture; 2) it reduces the amount of on-client computation and client-server\ncommunication by providing future clients with ready-to-use personalized models\nthat require no additional finetuning or optimization; 3) it comes with\ntheoretical guarantees that establish generalization from the observed clients\nto future ones. At the core of PeFLL lies a learning-to-learn approach that\njointly trains an embedding network and a hypernetwork. The embedding network\nis used to represent clients in a latent descriptor space in a way that\nreflects their similarity to each other. The hypernetwork takes as input such\ndescriptors and outputs the parameters of fully personalized client models. In\ncombination, both networks constitute a learning algorithm that achieves\nstate-of-the-art performance in several personalized federated learning\nbenchmarks.\n", "link": "http://arxiv.org/abs/2306.05515v3", "date": "2024-05-13", "relevancy": 2.3803, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4774}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4758}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PeFLL%3A%20Personalized%20Federated%20Learning%20by%20Learning%20to%20Learn&body=Title%3A%20PeFLL%3A%20Personalized%20Federated%20Learning%20by%20Learning%20to%20Learn%0AAuthor%3A%20Jonathan%20Scott%20and%20Hossein%20Zakerinia%20and%20Christoph%20H.%20Lampert%0AAbstract%3A%20%20%20We%20present%20PeFLL%2C%20a%20new%20personalized%20federated%20learning%20algorithm%20that%0Aimproves%20over%20the%20state-of-the-art%20in%20three%20aspects%3A%201%29%20it%20produces%20more%0Aaccurate%20models%2C%20especially%20in%20the%20low-data%20regime%2C%20and%20not%20only%20for%20clients%0Apresent%20during%20its%20training%20phase%2C%20but%20also%20for%20any%20that%20may%20emerge%20in%20the%0Afuture%3B%202%29%20it%20reduces%20the%20amount%20of%20on-client%20computation%20and%20client-server%0Acommunication%20by%20providing%20future%20clients%20with%20ready-to-use%20personalized%20models%0Athat%20require%20no%20additional%20finetuning%20or%20optimization%3B%203%29%20it%20comes%20with%0Atheoretical%20guarantees%20that%20establish%20generalization%20from%20the%20observed%20clients%0Ato%20future%20ones.%20At%20the%20core%20of%20PeFLL%20lies%20a%20learning-to-learn%20approach%20that%0Ajointly%20trains%20an%20embedding%20network%20and%20a%20hypernetwork.%20The%20embedding%20network%0Ais%20used%20to%20represent%20clients%20in%20a%20latent%20descriptor%20space%20in%20a%20way%20that%0Areflects%20their%20similarity%20to%20each%20other.%20The%20hypernetwork%20takes%20as%20input%20such%0Adescriptors%20and%20outputs%20the%20parameters%20of%20fully%20personalized%20client%20models.%20In%0Acombination%2C%20both%20networks%20constitute%20a%20learning%20algorithm%20that%20achieves%0Astate-of-the-art%20performance%20in%20several%20personalized%20federated%20learning%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.05515v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPeFLL%253A%2520Personalized%2520Federated%2520Learning%2520by%2520Learning%2520to%2520Learn%26entry.906535625%3DJonathan%2520Scott%2520and%2520Hossein%2520Zakerinia%2520and%2520Christoph%2520H.%2520Lampert%26entry.1292438233%3D%2520%2520We%2520present%2520PeFLL%252C%2520a%2520new%2520personalized%2520federated%2520learning%2520algorithm%2520that%250Aimproves%2520over%2520the%2520state-of-the-art%2520in%2520three%2520aspects%253A%25201%2529%2520it%2520produces%2520more%250Aaccurate%2520models%252C%2520especially%2520in%2520the%2520low-data%2520regime%252C%2520and%2520not%2520only%2520for%2520clients%250Apresent%2520during%2520its%2520training%2520phase%252C%2520but%2520also%2520for%2520any%2520that%2520may%2520emerge%2520in%2520the%250Afuture%253B%25202%2529%2520it%2520reduces%2520the%2520amount%2520of%2520on-client%2520computation%2520and%2520client-server%250Acommunication%2520by%2520providing%2520future%2520clients%2520with%2520ready-to-use%2520personalized%2520models%250Athat%2520require%2520no%2520additional%2520finetuning%2520or%2520optimization%253B%25203%2529%2520it%2520comes%2520with%250Atheoretical%2520guarantees%2520that%2520establish%2520generalization%2520from%2520the%2520observed%2520clients%250Ato%2520future%2520ones.%2520At%2520the%2520core%2520of%2520PeFLL%2520lies%2520a%2520learning-to-learn%2520approach%2520that%250Ajointly%2520trains%2520an%2520embedding%2520network%2520and%2520a%2520hypernetwork.%2520The%2520embedding%2520network%250Ais%2520used%2520to%2520represent%2520clients%2520in%2520a%2520latent%2520descriptor%2520space%2520in%2520a%2520way%2520that%250Areflects%2520their%2520similarity%2520to%2520each%2520other.%2520The%2520hypernetwork%2520takes%2520as%2520input%2520such%250Adescriptors%2520and%2520outputs%2520the%2520parameters%2520of%2520fully%2520personalized%2520client%2520models.%2520In%250Acombination%252C%2520both%2520networks%2520constitute%2520a%2520learning%2520algorithm%2520that%2520achieves%250Astate-of-the-art%2520performance%2520in%2520several%2520personalized%2520federated%2520learning%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.05515v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PeFLL%3A%20Personalized%20Federated%20Learning%20by%20Learning%20to%20Learn&entry.906535625=Jonathan%20Scott%20and%20Hossein%20Zakerinia%20and%20Christoph%20H.%20Lampert&entry.1292438233=%20%20We%20present%20PeFLL%2C%20a%20new%20personalized%20federated%20learning%20algorithm%20that%0Aimproves%20over%20the%20state-of-the-art%20in%20three%20aspects%3A%201%29%20it%20produces%20more%0Aaccurate%20models%2C%20especially%20in%20the%20low-data%20regime%2C%20and%20not%20only%20for%20clients%0Apresent%20during%20its%20training%20phase%2C%20but%20also%20for%20any%20that%20may%20emerge%20in%20the%0Afuture%3B%202%29%20it%20reduces%20the%20amount%20of%20on-client%20computation%20and%20client-server%0Acommunication%20by%20providing%20future%20clients%20with%20ready-to-use%20personalized%20models%0Athat%20require%20no%20additional%20finetuning%20or%20optimization%3B%203%29%20it%20comes%20with%0Atheoretical%20guarantees%20that%20establish%20generalization%20from%20the%20observed%20clients%0Ato%20future%20ones.%20At%20the%20core%20of%20PeFLL%20lies%20a%20learning-to-learn%20approach%20that%0Ajointly%20trains%20an%20embedding%20network%20and%20a%20hypernetwork.%20The%20embedding%20network%0Ais%20used%20to%20represent%20clients%20in%20a%20latent%20descriptor%20space%20in%20a%20way%20that%0Areflects%20their%20similarity%20to%20each%20other.%20The%20hypernetwork%20takes%20as%20input%20such%0Adescriptors%20and%20outputs%20the%20parameters%20of%20fully%20personalized%20client%20models.%20In%0Acombination%2C%20both%20networks%20constitute%20a%20learning%20algorithm%20that%20achieves%0Astate-of-the-art%20performance%20in%20several%20personalized%20federated%20learning%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.05515v3&entry.124074799=Read"},
{"title": "SceneFactory: A Workflow-centric and Unified Framework for Incremental\n  Scene Modeling", "author": "Yijun Yuan and Michael Bleier and Andreas N\u00fcchter", "abstract": "  We present SceneFactory, a workflow-centric and unified framework for\nincremental scene modeling, that supports conveniently a wide range of\napplications, such as (unposed and/or uncalibrated) multi-view depth\nestimation, LiDAR completion, (dense) RGB-D/RGB-L/Mono//Depth-only\nreconstruction and SLAM. The workflow-centric design uses multiple blocks as\nthe basis for building different production lines. The supported applications,\ni.e., productions avoid redundancy in their designs. Thus, the focus is on each\nblock itself for independent expansion. To support all input combinations, our\nimplementation consists of four building blocks in SceneFactory: (1) Mono-SLAM,\n(2) depth estimation, (3) flexion and (4) scene reconstruction. Furthermore, we\npropose an unposed & uncalibrated multi-view depth estimation model (U2-MVD) to\nestimate dense geometry. U2-MVD exploits dense bundle adjustment for solving\nfor poses, intrinsics, and inverse depth. Then a semantic-awared ScaleCov step\nis introduced to complete the multi-view depth. Relying on U2-MVD, SceneFactory\nboth supports user-friendly 3D creation (with just images) and bridges the\napplications of Dense RGB-D and Dense Mono. For high quality surface and color\nreconstruction, we propose due-purpose Multi-resolutional Neural Points\n(DM-NPs) for the first surface accessible Surface Color Field design, where we\nintroduce Improved Point Rasterization (IPR) for point cloud based surface\nquery.\n  We implement and experiment with SceneFactory to demonstrate its broad\npracticability and high flexibility. Its quality also competes or exceeds the\ntightly-coupled state of the art approaches in all tasks. We contribute the\ncode to the community (https://jarrome.github.io/).\n", "link": "http://arxiv.org/abs/2405.07847v1", "date": "2024-05-13", "relevancy": 2.3341, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6086}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5966}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SceneFactory%3A%20A%20Workflow-centric%20and%20Unified%20Framework%20for%20Incremental%0A%20%20Scene%20Modeling&body=Title%3A%20SceneFactory%3A%20A%20Workflow-centric%20and%20Unified%20Framework%20for%20Incremental%0A%20%20Scene%20Modeling%0AAuthor%3A%20Yijun%20Yuan%20and%20Michael%20Bleier%20and%20Andreas%20N%C3%BCchter%0AAbstract%3A%20%20%20We%20present%20SceneFactory%2C%20a%20workflow-centric%20and%20unified%20framework%20for%0Aincremental%20scene%20modeling%2C%20that%20supports%20conveniently%20a%20wide%20range%20of%0Aapplications%2C%20such%20as%20%28unposed%20and/or%20uncalibrated%29%20multi-view%20depth%0Aestimation%2C%20LiDAR%20completion%2C%20%28dense%29%20RGB-D/RGB-L/Mono//Depth-only%0Areconstruction%20and%20SLAM.%20The%20workflow-centric%20design%20uses%20multiple%20blocks%20as%0Athe%20basis%20for%20building%20different%20production%20lines.%20The%20supported%20applications%2C%0Ai.e.%2C%20productions%20avoid%20redundancy%20in%20their%20designs.%20Thus%2C%20the%20focus%20is%20on%20each%0Ablock%20itself%20for%20independent%20expansion.%20To%20support%20all%20input%20combinations%2C%20our%0Aimplementation%20consists%20of%20four%20building%20blocks%20in%20SceneFactory%3A%20%281%29%20Mono-SLAM%2C%0A%282%29%20depth%20estimation%2C%20%283%29%20flexion%20and%20%284%29%20scene%20reconstruction.%20Furthermore%2C%20we%0Apropose%20an%20unposed%20%26%20uncalibrated%20multi-view%20depth%20estimation%20model%20%28U2-MVD%29%20to%0Aestimate%20dense%20geometry.%20U2-MVD%20exploits%20dense%20bundle%20adjustment%20for%20solving%0Afor%20poses%2C%20intrinsics%2C%20and%20inverse%20depth.%20Then%20a%20semantic-awared%20ScaleCov%20step%0Ais%20introduced%20to%20complete%20the%20multi-view%20depth.%20Relying%20on%20U2-MVD%2C%20SceneFactory%0Aboth%20supports%20user-friendly%203D%20creation%20%28with%20just%20images%29%20and%20bridges%20the%0Aapplications%20of%20Dense%20RGB-D%20and%20Dense%20Mono.%20For%20high%20quality%20surface%20and%20color%0Areconstruction%2C%20we%20propose%20due-purpose%20Multi-resolutional%20Neural%20Points%0A%28DM-NPs%29%20for%20the%20first%20surface%20accessible%20Surface%20Color%20Field%20design%2C%20where%20we%0Aintroduce%20Improved%20Point%20Rasterization%20%28IPR%29%20for%20point%20cloud%20based%20surface%0Aquery.%0A%20%20We%20implement%20and%20experiment%20with%20SceneFactory%20to%20demonstrate%20its%20broad%0Apracticability%20and%20high%20flexibility.%20Its%20quality%20also%20competes%20or%20exceeds%20the%0Atightly-coupled%20state%20of%20the%20art%20approaches%20in%20all%20tasks.%20We%20contribute%20the%0Acode%20to%20the%20community%20%28https%3A//jarrome.github.io/%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSceneFactory%253A%2520A%2520Workflow-centric%2520and%2520Unified%2520Framework%2520for%2520Incremental%250A%2520%2520Scene%2520Modeling%26entry.906535625%3DYijun%2520Yuan%2520and%2520Michael%2520Bleier%2520and%2520Andreas%2520N%25C3%25BCchter%26entry.1292438233%3D%2520%2520We%2520present%2520SceneFactory%252C%2520a%2520workflow-centric%2520and%2520unified%2520framework%2520for%250Aincremental%2520scene%2520modeling%252C%2520that%2520supports%2520conveniently%2520a%2520wide%2520range%2520of%250Aapplications%252C%2520such%2520as%2520%2528unposed%2520and/or%2520uncalibrated%2529%2520multi-view%2520depth%250Aestimation%252C%2520LiDAR%2520completion%252C%2520%2528dense%2529%2520RGB-D/RGB-L/Mono//Depth-only%250Areconstruction%2520and%2520SLAM.%2520The%2520workflow-centric%2520design%2520uses%2520multiple%2520blocks%2520as%250Athe%2520basis%2520for%2520building%2520different%2520production%2520lines.%2520The%2520supported%2520applications%252C%250Ai.e.%252C%2520productions%2520avoid%2520redundancy%2520in%2520their%2520designs.%2520Thus%252C%2520the%2520focus%2520is%2520on%2520each%250Ablock%2520itself%2520for%2520independent%2520expansion.%2520To%2520support%2520all%2520input%2520combinations%252C%2520our%250Aimplementation%2520consists%2520of%2520four%2520building%2520blocks%2520in%2520SceneFactory%253A%2520%25281%2529%2520Mono-SLAM%252C%250A%25282%2529%2520depth%2520estimation%252C%2520%25283%2529%2520flexion%2520and%2520%25284%2529%2520scene%2520reconstruction.%2520Furthermore%252C%2520we%250Apropose%2520an%2520unposed%2520%2526%2520uncalibrated%2520multi-view%2520depth%2520estimation%2520model%2520%2528U2-MVD%2529%2520to%250Aestimate%2520dense%2520geometry.%2520U2-MVD%2520exploits%2520dense%2520bundle%2520adjustment%2520for%2520solving%250Afor%2520poses%252C%2520intrinsics%252C%2520and%2520inverse%2520depth.%2520Then%2520a%2520semantic-awared%2520ScaleCov%2520step%250Ais%2520introduced%2520to%2520complete%2520the%2520multi-view%2520depth.%2520Relying%2520on%2520U2-MVD%252C%2520SceneFactory%250Aboth%2520supports%2520user-friendly%25203D%2520creation%2520%2528with%2520just%2520images%2529%2520and%2520bridges%2520the%250Aapplications%2520of%2520Dense%2520RGB-D%2520and%2520Dense%2520Mono.%2520For%2520high%2520quality%2520surface%2520and%2520color%250Areconstruction%252C%2520we%2520propose%2520due-purpose%2520Multi-resolutional%2520Neural%2520Points%250A%2528DM-NPs%2529%2520for%2520the%2520first%2520surface%2520accessible%2520Surface%2520Color%2520Field%2520design%252C%2520where%2520we%250Aintroduce%2520Improved%2520Point%2520Rasterization%2520%2528IPR%2529%2520for%2520point%2520cloud%2520based%2520surface%250Aquery.%250A%2520%2520We%2520implement%2520and%2520experiment%2520with%2520SceneFactory%2520to%2520demonstrate%2520its%2520broad%250Apracticability%2520and%2520high%2520flexibility.%2520Its%2520quality%2520also%2520competes%2520or%2520exceeds%2520the%250Atightly-coupled%2520state%2520of%2520the%2520art%2520approaches%2520in%2520all%2520tasks.%2520We%2520contribute%2520the%250Acode%2520to%2520the%2520community%2520%2528https%253A//jarrome.github.io/%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SceneFactory%3A%20A%20Workflow-centric%20and%20Unified%20Framework%20for%20Incremental%0A%20%20Scene%20Modeling&entry.906535625=Yijun%20Yuan%20and%20Michael%20Bleier%20and%20Andreas%20N%C3%BCchter&entry.1292438233=%20%20We%20present%20SceneFactory%2C%20a%20workflow-centric%20and%20unified%20framework%20for%0Aincremental%20scene%20modeling%2C%20that%20supports%20conveniently%20a%20wide%20range%20of%0Aapplications%2C%20such%20as%20%28unposed%20and/or%20uncalibrated%29%20multi-view%20depth%0Aestimation%2C%20LiDAR%20completion%2C%20%28dense%29%20RGB-D/RGB-L/Mono//Depth-only%0Areconstruction%20and%20SLAM.%20The%20workflow-centric%20design%20uses%20multiple%20blocks%20as%0Athe%20basis%20for%20building%20different%20production%20lines.%20The%20supported%20applications%2C%0Ai.e.%2C%20productions%20avoid%20redundancy%20in%20their%20designs.%20Thus%2C%20the%20focus%20is%20on%20each%0Ablock%20itself%20for%20independent%20expansion.%20To%20support%20all%20input%20combinations%2C%20our%0Aimplementation%20consists%20of%20four%20building%20blocks%20in%20SceneFactory%3A%20%281%29%20Mono-SLAM%2C%0A%282%29%20depth%20estimation%2C%20%283%29%20flexion%20and%20%284%29%20scene%20reconstruction.%20Furthermore%2C%20we%0Apropose%20an%20unposed%20%26%20uncalibrated%20multi-view%20depth%20estimation%20model%20%28U2-MVD%29%20to%0Aestimate%20dense%20geometry.%20U2-MVD%20exploits%20dense%20bundle%20adjustment%20for%20solving%0Afor%20poses%2C%20intrinsics%2C%20and%20inverse%20depth.%20Then%20a%20semantic-awared%20ScaleCov%20step%0Ais%20introduced%20to%20complete%20the%20multi-view%20depth.%20Relying%20on%20U2-MVD%2C%20SceneFactory%0Aboth%20supports%20user-friendly%203D%20creation%20%28with%20just%20images%29%20and%20bridges%20the%0Aapplications%20of%20Dense%20RGB-D%20and%20Dense%20Mono.%20For%20high%20quality%20surface%20and%20color%0Areconstruction%2C%20we%20propose%20due-purpose%20Multi-resolutional%20Neural%20Points%0A%28DM-NPs%29%20for%20the%20first%20surface%20accessible%20Surface%20Color%20Field%20design%2C%20where%20we%0Aintroduce%20Improved%20Point%20Rasterization%20%28IPR%29%20for%20point%20cloud%20based%20surface%0Aquery.%0A%20%20We%20implement%20and%20experiment%20with%20SceneFactory%20to%20demonstrate%20its%20broad%0Apracticability%20and%20high%20flexibility.%20Its%20quality%20also%20competes%20or%20exceeds%20the%0Atightly-coupled%20state%20of%20the%20art%20approaches%20in%20all%20tasks.%20We%20contribute%20the%0Acode%20to%20the%20community%20%28https%3A//jarrome.github.io/%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07847v1&entry.124074799=Read"},
{"title": "OverlapMamba: Novel Shift State Space Model for LiDAR-based Place\n  Recognition", "author": "Qiuchi Xiang and Jintao Cheng and Jiehao Luo and Jin Wu and Rui Fan and Xieyuanli Chen and Xiaoyu Tang", "abstract": "  Place recognition is the foundation for enabling autonomous systems to\nachieve independent decision-making and safe operations. It is also crucial in\ntasks such as loop closure detection and global localization within SLAM.\nPrevious methods utilize mundane point cloud representations as input and deep\nlearning-based LiDAR-based Place Recognition (LPR) approaches employing\ndifferent point cloud image inputs with convolutional neural networks (CNNs) or\ntransformer architectures. However, the recently proposed Mamba deep learning\nmodel, combined with state space models (SSMs), holds great potential for long\nsequence modeling. Therefore, we developed OverlapMamba, a novel network for\nplace recognition, which represents input range views (RVs) as sequences. In a\nnovel way, we employ a stochastic reconstruction approach to build shift state\nspace models, compressing the visual representation. Evaluated on three\ndifferent public datasets, our method effectively detects loop closures,\nshowing robustness even when traversing previously visited locations from\ndifferent directions. Relying on raw range view inputs, it outperforms typical\nLiDAR and multi-view combination methods in time complexity and speed,\nindicating strong place recognition capabilities and real-time efficiency.\n", "link": "http://arxiv.org/abs/2405.07966v1", "date": "2024-05-13", "relevancy": 2.3249, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6273}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5492}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OverlapMamba%3A%20Novel%20Shift%20State%20Space%20Model%20for%20LiDAR-based%20Place%0A%20%20Recognition&body=Title%3A%20OverlapMamba%3A%20Novel%20Shift%20State%20Space%20Model%20for%20LiDAR-based%20Place%0A%20%20Recognition%0AAuthor%3A%20Qiuchi%20Xiang%20and%20Jintao%20Cheng%20and%20Jiehao%20Luo%20and%20Jin%20Wu%20and%20Rui%20Fan%20and%20Xieyuanli%20Chen%20and%20Xiaoyu%20Tang%0AAbstract%3A%20%20%20Place%20recognition%20is%20the%20foundation%20for%20enabling%20autonomous%20systems%20to%0Aachieve%20independent%20decision-making%20and%20safe%20operations.%20It%20is%20also%20crucial%20in%0Atasks%20such%20as%20loop%20closure%20detection%20and%20global%20localization%20within%20SLAM.%0APrevious%20methods%20utilize%20mundane%20point%20cloud%20representations%20as%20input%20and%20deep%0Alearning-based%20LiDAR-based%20Place%20Recognition%20%28LPR%29%20approaches%20employing%0Adifferent%20point%20cloud%20image%20inputs%20with%20convolutional%20neural%20networks%20%28CNNs%29%20or%0Atransformer%20architectures.%20However%2C%20the%20recently%20proposed%20Mamba%20deep%20learning%0Amodel%2C%20combined%20with%20state%20space%20models%20%28SSMs%29%2C%20holds%20great%20potential%20for%20long%0Asequence%20modeling.%20Therefore%2C%20we%20developed%20OverlapMamba%2C%20a%20novel%20network%20for%0Aplace%20recognition%2C%20which%20represents%20input%20range%20views%20%28RVs%29%20as%20sequences.%20In%20a%0Anovel%20way%2C%20we%20employ%20a%20stochastic%20reconstruction%20approach%20to%20build%20shift%20state%0Aspace%20models%2C%20compressing%20the%20visual%20representation.%20Evaluated%20on%20three%0Adifferent%20public%20datasets%2C%20our%20method%20effectively%20detects%20loop%20closures%2C%0Ashowing%20robustness%20even%20when%20traversing%20previously%20visited%20locations%20from%0Adifferent%20directions.%20Relying%20on%20raw%20range%20view%20inputs%2C%20it%20outperforms%20typical%0ALiDAR%20and%20multi-view%20combination%20methods%20in%20time%20complexity%20and%20speed%2C%0Aindicating%20strong%20place%20recognition%20capabilities%20and%20real-time%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOverlapMamba%253A%2520Novel%2520Shift%2520State%2520Space%2520Model%2520for%2520LiDAR-based%2520Place%250A%2520%2520Recognition%26entry.906535625%3DQiuchi%2520Xiang%2520and%2520Jintao%2520Cheng%2520and%2520Jiehao%2520Luo%2520and%2520Jin%2520Wu%2520and%2520Rui%2520Fan%2520and%2520Xieyuanli%2520Chen%2520and%2520Xiaoyu%2520Tang%26entry.1292438233%3D%2520%2520Place%2520recognition%2520is%2520the%2520foundation%2520for%2520enabling%2520autonomous%2520systems%2520to%250Aachieve%2520independent%2520decision-making%2520and%2520safe%2520operations.%2520It%2520is%2520also%2520crucial%2520in%250Atasks%2520such%2520as%2520loop%2520closure%2520detection%2520and%2520global%2520localization%2520within%2520SLAM.%250APrevious%2520methods%2520utilize%2520mundane%2520point%2520cloud%2520representations%2520as%2520input%2520and%2520deep%250Alearning-based%2520LiDAR-based%2520Place%2520Recognition%2520%2528LPR%2529%2520approaches%2520employing%250Adifferent%2520point%2520cloud%2520image%2520inputs%2520with%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520or%250Atransformer%2520architectures.%2520However%252C%2520the%2520recently%2520proposed%2520Mamba%2520deep%2520learning%250Amodel%252C%2520combined%2520with%2520state%2520space%2520models%2520%2528SSMs%2529%252C%2520holds%2520great%2520potential%2520for%2520long%250Asequence%2520modeling.%2520Therefore%252C%2520we%2520developed%2520OverlapMamba%252C%2520a%2520novel%2520network%2520for%250Aplace%2520recognition%252C%2520which%2520represents%2520input%2520range%2520views%2520%2528RVs%2529%2520as%2520sequences.%2520In%2520a%250Anovel%2520way%252C%2520we%2520employ%2520a%2520stochastic%2520reconstruction%2520approach%2520to%2520build%2520shift%2520state%250Aspace%2520models%252C%2520compressing%2520the%2520visual%2520representation.%2520Evaluated%2520on%2520three%250Adifferent%2520public%2520datasets%252C%2520our%2520method%2520effectively%2520detects%2520loop%2520closures%252C%250Ashowing%2520robustness%2520even%2520when%2520traversing%2520previously%2520visited%2520locations%2520from%250Adifferent%2520directions.%2520Relying%2520on%2520raw%2520range%2520view%2520inputs%252C%2520it%2520outperforms%2520typical%250ALiDAR%2520and%2520multi-view%2520combination%2520methods%2520in%2520time%2520complexity%2520and%2520speed%252C%250Aindicating%2520strong%2520place%2520recognition%2520capabilities%2520and%2520real-time%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OverlapMamba%3A%20Novel%20Shift%20State%20Space%20Model%20for%20LiDAR-based%20Place%0A%20%20Recognition&entry.906535625=Qiuchi%20Xiang%20and%20Jintao%20Cheng%20and%20Jiehao%20Luo%20and%20Jin%20Wu%20and%20Rui%20Fan%20and%20Xieyuanli%20Chen%20and%20Xiaoyu%20Tang&entry.1292438233=%20%20Place%20recognition%20is%20the%20foundation%20for%20enabling%20autonomous%20systems%20to%0Aachieve%20independent%20decision-making%20and%20safe%20operations.%20It%20is%20also%20crucial%20in%0Atasks%20such%20as%20loop%20closure%20detection%20and%20global%20localization%20within%20SLAM.%0APrevious%20methods%20utilize%20mundane%20point%20cloud%20representations%20as%20input%20and%20deep%0Alearning-based%20LiDAR-based%20Place%20Recognition%20%28LPR%29%20approaches%20employing%0Adifferent%20point%20cloud%20image%20inputs%20with%20convolutional%20neural%20networks%20%28CNNs%29%20or%0Atransformer%20architectures.%20However%2C%20the%20recently%20proposed%20Mamba%20deep%20learning%0Amodel%2C%20combined%20with%20state%20space%20models%20%28SSMs%29%2C%20holds%20great%20potential%20for%20long%0Asequence%20modeling.%20Therefore%2C%20we%20developed%20OverlapMamba%2C%20a%20novel%20network%20for%0Aplace%20recognition%2C%20which%20represents%20input%20range%20views%20%28RVs%29%20as%20sequences.%20In%20a%0Anovel%20way%2C%20we%20employ%20a%20stochastic%20reconstruction%20approach%20to%20build%20shift%20state%0Aspace%20models%2C%20compressing%20the%20visual%20representation.%20Evaluated%20on%20three%0Adifferent%20public%20datasets%2C%20our%20method%20effectively%20detects%20loop%20closures%2C%0Ashowing%20robustness%20even%20when%20traversing%20previously%20visited%20locations%20from%0Adifferent%20directions.%20Relying%20on%20raw%20range%20view%20inputs%2C%20it%20outperforms%20typical%0ALiDAR%20and%20multi-view%20combination%20methods%20in%20time%20complexity%20and%20speed%2C%0Aindicating%20strong%20place%20recognition%20capabilities%20and%20real-time%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07966v1&entry.124074799=Read"},
{"title": "DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning", "author": "Jonathan Lebensold and Maziar Sanjabi and Pietro Astolfi and Adriana Romero-Soriano and Kamalika Chaudhuri and Mike Rabbat and Chuan Guo", "abstract": "  Text-to-image diffusion models have been shown to suffer from sample-level\nmemorization, possibly reproducing near-perfect replica of images that they are\ntrained on, which may be undesirable. To remedy this issue, we develop the\nfirst differentially private (DP) retrieval-augmented generation algorithm that\nis capable of generating high-quality image samples while providing provable\nprivacy guarantees. Specifically, we assume access to a text-to-image diffusion\nmodel trained on a small amount of public data, and design a DP retrieval\nmechanism to augment the text prompt with samples retrieved from a private\nretrieval dataset. Our \\emph{differentially private retrieval-augmented\ndiffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to\nadapt to another domain, and can use state-of-the-art generative models to\ngenerate high-quality image samples while satisfying rigorous DP guarantees.\nFor instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a\nprivacy budget of $\\epsilon=10$, while providing a $3.5$ point improvement in\nFID compared to public-only retrieval for up to $10,000$ queries.\n", "link": "http://arxiv.org/abs/2403.14421v3", "date": "2024-05-13", "relevancy": 2.3218, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6104}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5788}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DP-RDM%3A%20Adapting%20Diffusion%20Models%20to%20Private%20Domains%20Without%20Fine-Tuning&body=Title%3A%20DP-RDM%3A%20Adapting%20Diffusion%20Models%20to%20Private%20Domains%20Without%20Fine-Tuning%0AAuthor%3A%20Jonathan%20Lebensold%20and%20Maziar%20Sanjabi%20and%20Pietro%20Astolfi%20and%20Adriana%20Romero-Soriano%20and%20Kamalika%20Chaudhuri%20and%20Mike%20Rabbat%20and%20Chuan%20Guo%0AAbstract%3A%20%20%20Text-to-image%20diffusion%20models%20have%20been%20shown%20to%20suffer%20from%20sample-level%0Amemorization%2C%20possibly%20reproducing%20near-perfect%20replica%20of%20images%20that%20they%20are%0Atrained%20on%2C%20which%20may%20be%20undesirable.%20To%20remedy%20this%20issue%2C%20we%20develop%20the%0Afirst%20differentially%20private%20%28DP%29%20retrieval-augmented%20generation%20algorithm%20that%0Ais%20capable%20of%20generating%20high-quality%20image%20samples%20while%20providing%20provable%0Aprivacy%20guarantees.%20Specifically%2C%20we%20assume%20access%20to%20a%20text-to-image%20diffusion%0Amodel%20trained%20on%20a%20small%20amount%20of%20public%20data%2C%20and%20design%20a%20DP%20retrieval%0Amechanism%20to%20augment%20the%20text%20prompt%20with%20samples%20retrieved%20from%20a%20private%0Aretrieval%20dataset.%20Our%20%5Cemph%7Bdifferentially%20private%20retrieval-augmented%0Adiffusion%20model%7D%20%28DP-RDM%29%20requires%20no%20fine-tuning%20on%20the%20retrieval%20dataset%20to%0Aadapt%20to%20another%20domain%2C%20and%20can%20use%20state-of-the-art%20generative%20models%20to%0Agenerate%20high-quality%20image%20samples%20while%20satisfying%20rigorous%20DP%20guarantees.%0AFor%20instance%2C%20when%20evaluated%20on%20MS-COCO%2C%20our%20DP-RDM%20can%20generate%20samples%20with%20a%0Aprivacy%20budget%20of%20%24%5Cepsilon%3D10%24%2C%20while%20providing%20a%20%243.5%24%20point%20improvement%20in%0AFID%20compared%20to%20public-only%20retrieval%20for%20up%20to%20%2410%2C000%24%20queries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14421v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDP-RDM%253A%2520Adapting%2520Diffusion%2520Models%2520to%2520Private%2520Domains%2520Without%2520Fine-Tuning%26entry.906535625%3DJonathan%2520Lebensold%2520and%2520Maziar%2520Sanjabi%2520and%2520Pietro%2520Astolfi%2520and%2520Adriana%2520Romero-Soriano%2520and%2520Kamalika%2520Chaudhuri%2520and%2520Mike%2520Rabbat%2520and%2520Chuan%2520Guo%26entry.1292438233%3D%2520%2520Text-to-image%2520diffusion%2520models%2520have%2520been%2520shown%2520to%2520suffer%2520from%2520sample-level%250Amemorization%252C%2520possibly%2520reproducing%2520near-perfect%2520replica%2520of%2520images%2520that%2520they%2520are%250Atrained%2520on%252C%2520which%2520may%2520be%2520undesirable.%2520To%2520remedy%2520this%2520issue%252C%2520we%2520develop%2520the%250Afirst%2520differentially%2520private%2520%2528DP%2529%2520retrieval-augmented%2520generation%2520algorithm%2520that%250Ais%2520capable%2520of%2520generating%2520high-quality%2520image%2520samples%2520while%2520providing%2520provable%250Aprivacy%2520guarantees.%2520Specifically%252C%2520we%2520assume%2520access%2520to%2520a%2520text-to-image%2520diffusion%250Amodel%2520trained%2520on%2520a%2520small%2520amount%2520of%2520public%2520data%252C%2520and%2520design%2520a%2520DP%2520retrieval%250Amechanism%2520to%2520augment%2520the%2520text%2520prompt%2520with%2520samples%2520retrieved%2520from%2520a%2520private%250Aretrieval%2520dataset.%2520Our%2520%255Cemph%257Bdifferentially%2520private%2520retrieval-augmented%250Adiffusion%2520model%257D%2520%2528DP-RDM%2529%2520requires%2520no%2520fine-tuning%2520on%2520the%2520retrieval%2520dataset%2520to%250Aadapt%2520to%2520another%2520domain%252C%2520and%2520can%2520use%2520state-of-the-art%2520generative%2520models%2520to%250Agenerate%2520high-quality%2520image%2520samples%2520while%2520satisfying%2520rigorous%2520DP%2520guarantees.%250AFor%2520instance%252C%2520when%2520evaluated%2520on%2520MS-COCO%252C%2520our%2520DP-RDM%2520can%2520generate%2520samples%2520with%2520a%250Aprivacy%2520budget%2520of%2520%2524%255Cepsilon%253D10%2524%252C%2520while%2520providing%2520a%2520%25243.5%2524%2520point%2520improvement%2520in%250AFID%2520compared%2520to%2520public-only%2520retrieval%2520for%2520up%2520to%2520%252410%252C000%2524%2520queries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14421v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DP-RDM%3A%20Adapting%20Diffusion%20Models%20to%20Private%20Domains%20Without%20Fine-Tuning&entry.906535625=Jonathan%20Lebensold%20and%20Maziar%20Sanjabi%20and%20Pietro%20Astolfi%20and%20Adriana%20Romero-Soriano%20and%20Kamalika%20Chaudhuri%20and%20Mike%20Rabbat%20and%20Chuan%20Guo&entry.1292438233=%20%20Text-to-image%20diffusion%20models%20have%20been%20shown%20to%20suffer%20from%20sample-level%0Amemorization%2C%20possibly%20reproducing%20near-perfect%20replica%20of%20images%20that%20they%20are%0Atrained%20on%2C%20which%20may%20be%20undesirable.%20To%20remedy%20this%20issue%2C%20we%20develop%20the%0Afirst%20differentially%20private%20%28DP%29%20retrieval-augmented%20generation%20algorithm%20that%0Ais%20capable%20of%20generating%20high-quality%20image%20samples%20while%20providing%20provable%0Aprivacy%20guarantees.%20Specifically%2C%20we%20assume%20access%20to%20a%20text-to-image%20diffusion%0Amodel%20trained%20on%20a%20small%20amount%20of%20public%20data%2C%20and%20design%20a%20DP%20retrieval%0Amechanism%20to%20augment%20the%20text%20prompt%20with%20samples%20retrieved%20from%20a%20private%0Aretrieval%20dataset.%20Our%20%5Cemph%7Bdifferentially%20private%20retrieval-augmented%0Adiffusion%20model%7D%20%28DP-RDM%29%20requires%20no%20fine-tuning%20on%20the%20retrieval%20dataset%20to%0Aadapt%20to%20another%20domain%2C%20and%20can%20use%20state-of-the-art%20generative%20models%20to%0Agenerate%20high-quality%20image%20samples%20while%20satisfying%20rigorous%20DP%20guarantees.%0AFor%20instance%2C%20when%20evaluated%20on%20MS-COCO%2C%20our%20DP-RDM%20can%20generate%20samples%20with%20a%0Aprivacy%20budget%20of%20%24%5Cepsilon%3D10%24%2C%20while%20providing%20a%20%243.5%24%20point%20improvement%20in%0AFID%20compared%20to%20public-only%20retrieval%20for%20up%20to%20%2410%2C000%24%20queries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14421v3&entry.124074799=Read"},
{"title": "GMSR:Gradient-Guided Mamba for Spectral Reconstruction from RGB Images", "author": "Xinying Wang and Zhixiong Huang and Sifan Zhang and Jiawen Zhu and Lin Feng", "abstract": "  Mainstream approaches to spectral reconstruction (SR) primarily focus on\ndesigning Convolution- and Transformer-based architectures. However, CNN\nmethods often face challenges in handling long-range dependencies, whereas\nTransformers are constrained by computational efficiency limitations. Recent\nbreakthroughs in state-space model (e.g., Mamba) has attracted significant\nattention due to its near-linear computational efficiency and superior\nperformance, prompting our investigation into its potential for SR problem. To\nthis end, we propose the Gradient-guided Mamba for Spectral Reconstruction from\nRGB Images, dubbed GMSR-Net. GMSR-Net is a lightweight model characterized by a\nglobal receptive field and linear computational complexity. Its core comprises\nmultiple stacked Gradient Mamba (GM) blocks, each featuring a tri-branch\nstructure. In addition to benefiting from efficient global feature\nrepresentation by Mamba block, we further innovatively introduce spatial\ngradient attention and spectral gradient attention to guide the reconstruction\nof spatial and spectral cues. GMSR-Net demonstrates a significant\naccuracy-efficiency trade-off, achieving state-of-the-art performance while\nmarkedly reducing the number of parameters and computational burdens. Compared\nto existing approaches, GMSR-Net slashes parameters and FLOPS by substantial\nmargins of 10 times and 20 times, respectively. Code is available at\nhttps://github.com/wxy11-27/GMSR.\n", "link": "http://arxiv.org/abs/2405.07777v1", "date": "2024-05-13", "relevancy": 2.2797, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6295}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.536}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GMSR%3AGradient-Guided%20Mamba%20for%20Spectral%20Reconstruction%20from%20RGB%20Images&body=Title%3A%20GMSR%3AGradient-Guided%20Mamba%20for%20Spectral%20Reconstruction%20from%20RGB%20Images%0AAuthor%3A%20Xinying%20Wang%20and%20Zhixiong%20Huang%20and%20Sifan%20Zhang%20and%20Jiawen%20Zhu%20and%20Lin%20Feng%0AAbstract%3A%20%20%20Mainstream%20approaches%20to%20spectral%20reconstruction%20%28SR%29%20primarily%20focus%20on%0Adesigning%20Convolution-%20and%20Transformer-based%20architectures.%20However%2C%20CNN%0Amethods%20often%20face%20challenges%20in%20handling%20long-range%20dependencies%2C%20whereas%0ATransformers%20are%20constrained%20by%20computational%20efficiency%20limitations.%20Recent%0Abreakthroughs%20in%20state-space%20model%20%28e.g.%2C%20Mamba%29%20has%20attracted%20significant%0Aattention%20due%20to%20its%20near-linear%20computational%20efficiency%20and%20superior%0Aperformance%2C%20prompting%20our%20investigation%20into%20its%20potential%20for%20SR%20problem.%20To%0Athis%20end%2C%20we%20propose%20the%20Gradient-guided%20Mamba%20for%20Spectral%20Reconstruction%20from%0ARGB%20Images%2C%20dubbed%20GMSR-Net.%20GMSR-Net%20is%20a%20lightweight%20model%20characterized%20by%20a%0Aglobal%20receptive%20field%20and%20linear%20computational%20complexity.%20Its%20core%20comprises%0Amultiple%20stacked%20Gradient%20Mamba%20%28GM%29%20blocks%2C%20each%20featuring%20a%20tri-branch%0Astructure.%20In%20addition%20to%20benefiting%20from%20efficient%20global%20feature%0Arepresentation%20by%20Mamba%20block%2C%20we%20further%20innovatively%20introduce%20spatial%0Agradient%20attention%20and%20spectral%20gradient%20attention%20to%20guide%20the%20reconstruction%0Aof%20spatial%20and%20spectral%20cues.%20GMSR-Net%20demonstrates%20a%20significant%0Aaccuracy-efficiency%20trade-off%2C%20achieving%20state-of-the-art%20performance%20while%0Amarkedly%20reducing%20the%20number%20of%20parameters%20and%20computational%20burdens.%20Compared%0Ato%20existing%20approaches%2C%20GMSR-Net%20slashes%20parameters%20and%20FLOPS%20by%20substantial%0Amargins%20of%2010%20times%20and%2020%20times%2C%20respectively.%20Code%20is%20available%20at%0Ahttps%3A//github.com/wxy11-27/GMSR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGMSR%253AGradient-Guided%2520Mamba%2520for%2520Spectral%2520Reconstruction%2520from%2520RGB%2520Images%26entry.906535625%3DXinying%2520Wang%2520and%2520Zhixiong%2520Huang%2520and%2520Sifan%2520Zhang%2520and%2520Jiawen%2520Zhu%2520and%2520Lin%2520Feng%26entry.1292438233%3D%2520%2520Mainstream%2520approaches%2520to%2520spectral%2520reconstruction%2520%2528SR%2529%2520primarily%2520focus%2520on%250Adesigning%2520Convolution-%2520and%2520Transformer-based%2520architectures.%2520However%252C%2520CNN%250Amethods%2520often%2520face%2520challenges%2520in%2520handling%2520long-range%2520dependencies%252C%2520whereas%250ATransformers%2520are%2520constrained%2520by%2520computational%2520efficiency%2520limitations.%2520Recent%250Abreakthroughs%2520in%2520state-space%2520model%2520%2528e.g.%252C%2520Mamba%2529%2520has%2520attracted%2520significant%250Aattention%2520due%2520to%2520its%2520near-linear%2520computational%2520efficiency%2520and%2520superior%250Aperformance%252C%2520prompting%2520our%2520investigation%2520into%2520its%2520potential%2520for%2520SR%2520problem.%2520To%250Athis%2520end%252C%2520we%2520propose%2520the%2520Gradient-guided%2520Mamba%2520for%2520Spectral%2520Reconstruction%2520from%250ARGB%2520Images%252C%2520dubbed%2520GMSR-Net.%2520GMSR-Net%2520is%2520a%2520lightweight%2520model%2520characterized%2520by%2520a%250Aglobal%2520receptive%2520field%2520and%2520linear%2520computational%2520complexity.%2520Its%2520core%2520comprises%250Amultiple%2520stacked%2520Gradient%2520Mamba%2520%2528GM%2529%2520blocks%252C%2520each%2520featuring%2520a%2520tri-branch%250Astructure.%2520In%2520addition%2520to%2520benefiting%2520from%2520efficient%2520global%2520feature%250Arepresentation%2520by%2520Mamba%2520block%252C%2520we%2520further%2520innovatively%2520introduce%2520spatial%250Agradient%2520attention%2520and%2520spectral%2520gradient%2520attention%2520to%2520guide%2520the%2520reconstruction%250Aof%2520spatial%2520and%2520spectral%2520cues.%2520GMSR-Net%2520demonstrates%2520a%2520significant%250Aaccuracy-efficiency%2520trade-off%252C%2520achieving%2520state-of-the-art%2520performance%2520while%250Amarkedly%2520reducing%2520the%2520number%2520of%2520parameters%2520and%2520computational%2520burdens.%2520Compared%250Ato%2520existing%2520approaches%252C%2520GMSR-Net%2520slashes%2520parameters%2520and%2520FLOPS%2520by%2520substantial%250Amargins%2520of%252010%2520times%2520and%252020%2520times%252C%2520respectively.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/wxy11-27/GMSR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GMSR%3AGradient-Guided%20Mamba%20for%20Spectral%20Reconstruction%20from%20RGB%20Images&entry.906535625=Xinying%20Wang%20and%20Zhixiong%20Huang%20and%20Sifan%20Zhang%20and%20Jiawen%20Zhu%20and%20Lin%20Feng&entry.1292438233=%20%20Mainstream%20approaches%20to%20spectral%20reconstruction%20%28SR%29%20primarily%20focus%20on%0Adesigning%20Convolution-%20and%20Transformer-based%20architectures.%20However%2C%20CNN%0Amethods%20often%20face%20challenges%20in%20handling%20long-range%20dependencies%2C%20whereas%0ATransformers%20are%20constrained%20by%20computational%20efficiency%20limitations.%20Recent%0Abreakthroughs%20in%20state-space%20model%20%28e.g.%2C%20Mamba%29%20has%20attracted%20significant%0Aattention%20due%20to%20its%20near-linear%20computational%20efficiency%20and%20superior%0Aperformance%2C%20prompting%20our%20investigation%20into%20its%20potential%20for%20SR%20problem.%20To%0Athis%20end%2C%20we%20propose%20the%20Gradient-guided%20Mamba%20for%20Spectral%20Reconstruction%20from%0ARGB%20Images%2C%20dubbed%20GMSR-Net.%20GMSR-Net%20is%20a%20lightweight%20model%20characterized%20by%20a%0Aglobal%20receptive%20field%20and%20linear%20computational%20complexity.%20Its%20core%20comprises%0Amultiple%20stacked%20Gradient%20Mamba%20%28GM%29%20blocks%2C%20each%20featuring%20a%20tri-branch%0Astructure.%20In%20addition%20to%20benefiting%20from%20efficient%20global%20feature%0Arepresentation%20by%20Mamba%20block%2C%20we%20further%20innovatively%20introduce%20spatial%0Agradient%20attention%20and%20spectral%20gradient%20attention%20to%20guide%20the%20reconstruction%0Aof%20spatial%20and%20spectral%20cues.%20GMSR-Net%20demonstrates%20a%20significant%0Aaccuracy-efficiency%20trade-off%2C%20achieving%20state-of-the-art%20performance%20while%0Amarkedly%20reducing%20the%20number%20of%20parameters%20and%20computational%20burdens.%20Compared%0Ato%20existing%20approaches%2C%20GMSR-Net%20slashes%20parameters%20and%20FLOPS%20by%20substantial%0Amargins%20of%2010%20times%20and%2020%20times%2C%20respectively.%20Code%20is%20available%20at%0Ahttps%3A//github.com/wxy11-27/GMSR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07777v1&entry.124074799=Read"},
{"title": "Learning to Plan Maneuverable and Agile Flight Trajectory with\n  Optimization Embedded Networks", "author": "Zhichao Han and Long Xu and Fei Gao", "abstract": "  In recent times, an increasing number of researchers have been devoted to\nutilizing deep neural networks for end-to-end flight navigation. This approach\nhas gained traction due to its ability to bridge the gap between perception and\nplanning that exists in traditional methods, thereby eliminating delays between\nmodules. However, the practice of replacing original modules with neural\nnetworks in a black-box manner diminishes the overall system's robustness and\nstability. It lacks principled explanations and often fails to consistently\ngenerate high-quality motion trajectories. Furthermore, such methods often\nstruggle to rigorously account for the robot's kinematic constraints, resulting\nin the generation of trajectories that cannot be executed satisfactorily. In\nthis work, we combine the advantages of traditional methods and neural networks\nby proposing an optimization-embedded neural network. This network can learn\nhigh-quality trajectories directly from visual inputs without the need of\nmapping, while ensuring dynamic feasibility. Here, the deep neural network is\nemployed to directly extract environment safety regions from depth images.\nSubsequently, we employ a model-based approach to represent these regions as\nsafety constraints in trajectory optimization. Leveraging the availability of\nhighly efficient optimization algorithms, our method robustly converges to\nfeasible and optimal solutions that satisfy various user-defined constraints.\nMoreover, we differentiate the optimization process, allowing it to be trained\nas a layer within the neural network. This approach facilitates the direct\ninteraction between perception and planning, enabling the network to focus more\non the spatial regions where optimal solutions exist. As a result, it further\nenhances the quality and stability of the generated trajectories.\n", "link": "http://arxiv.org/abs/2405.07736v1", "date": "2024-05-13", "relevancy": 2.2435, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5783}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5583}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Plan%20Maneuverable%20and%20Agile%20Flight%20Trajectory%20with%0A%20%20Optimization%20Embedded%20Networks&body=Title%3A%20Learning%20to%20Plan%20Maneuverable%20and%20Agile%20Flight%20Trajectory%20with%0A%20%20Optimization%20Embedded%20Networks%0AAuthor%3A%20Zhichao%20Han%20and%20Long%20Xu%20and%20Fei%20Gao%0AAbstract%3A%20%20%20In%20recent%20times%2C%20an%20increasing%20number%20of%20researchers%20have%20been%20devoted%20to%0Autilizing%20deep%20neural%20networks%20for%20end-to-end%20flight%20navigation.%20This%20approach%0Ahas%20gained%20traction%20due%20to%20its%20ability%20to%20bridge%20the%20gap%20between%20perception%20and%0Aplanning%20that%20exists%20in%20traditional%20methods%2C%20thereby%20eliminating%20delays%20between%0Amodules.%20However%2C%20the%20practice%20of%20replacing%20original%20modules%20with%20neural%0Anetworks%20in%20a%20black-box%20manner%20diminishes%20the%20overall%20system%27s%20robustness%20and%0Astability.%20It%20lacks%20principled%20explanations%20and%20often%20fails%20to%20consistently%0Agenerate%20high-quality%20motion%20trajectories.%20Furthermore%2C%20such%20methods%20often%0Astruggle%20to%20rigorously%20account%20for%20the%20robot%27s%20kinematic%20constraints%2C%20resulting%0Ain%20the%20generation%20of%20trajectories%20that%20cannot%20be%20executed%20satisfactorily.%20In%0Athis%20work%2C%20we%20combine%20the%20advantages%20of%20traditional%20methods%20and%20neural%20networks%0Aby%20proposing%20an%20optimization-embedded%20neural%20network.%20This%20network%20can%20learn%0Ahigh-quality%20trajectories%20directly%20from%20visual%20inputs%20without%20the%20need%20of%0Amapping%2C%20while%20ensuring%20dynamic%20feasibility.%20Here%2C%20the%20deep%20neural%20network%20is%0Aemployed%20to%20directly%20extract%20environment%20safety%20regions%20from%20depth%20images.%0ASubsequently%2C%20we%20employ%20a%20model-based%20approach%20to%20represent%20these%20regions%20as%0Asafety%20constraints%20in%20trajectory%20optimization.%20Leveraging%20the%20availability%20of%0Ahighly%20efficient%20optimization%20algorithms%2C%20our%20method%20robustly%20converges%20to%0Afeasible%20and%20optimal%20solutions%20that%20satisfy%20various%20user-defined%20constraints.%0AMoreover%2C%20we%20differentiate%20the%20optimization%20process%2C%20allowing%20it%20to%20be%20trained%0Aas%20a%20layer%20within%20the%20neural%20network.%20This%20approach%20facilitates%20the%20direct%0Ainteraction%20between%20perception%20and%20planning%2C%20enabling%20the%20network%20to%20focus%20more%0Aon%20the%20spatial%20regions%20where%20optimal%20solutions%20exist.%20As%20a%20result%2C%20it%20further%0Aenhances%20the%20quality%20and%20stability%20of%20the%20generated%20trajectories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Plan%2520Maneuverable%2520and%2520Agile%2520Flight%2520Trajectory%2520with%250A%2520%2520Optimization%2520Embedded%2520Networks%26entry.906535625%3DZhichao%2520Han%2520and%2520Long%2520Xu%2520and%2520Fei%2520Gao%26entry.1292438233%3D%2520%2520In%2520recent%2520times%252C%2520an%2520increasing%2520number%2520of%2520researchers%2520have%2520been%2520devoted%2520to%250Autilizing%2520deep%2520neural%2520networks%2520for%2520end-to-end%2520flight%2520navigation.%2520This%2520approach%250Ahas%2520gained%2520traction%2520due%2520to%2520its%2520ability%2520to%2520bridge%2520the%2520gap%2520between%2520perception%2520and%250Aplanning%2520that%2520exists%2520in%2520traditional%2520methods%252C%2520thereby%2520eliminating%2520delays%2520between%250Amodules.%2520However%252C%2520the%2520practice%2520of%2520replacing%2520original%2520modules%2520with%2520neural%250Anetworks%2520in%2520a%2520black-box%2520manner%2520diminishes%2520the%2520overall%2520system%2527s%2520robustness%2520and%250Astability.%2520It%2520lacks%2520principled%2520explanations%2520and%2520often%2520fails%2520to%2520consistently%250Agenerate%2520high-quality%2520motion%2520trajectories.%2520Furthermore%252C%2520such%2520methods%2520often%250Astruggle%2520to%2520rigorously%2520account%2520for%2520the%2520robot%2527s%2520kinematic%2520constraints%252C%2520resulting%250Ain%2520the%2520generation%2520of%2520trajectories%2520that%2520cannot%2520be%2520executed%2520satisfactorily.%2520In%250Athis%2520work%252C%2520we%2520combine%2520the%2520advantages%2520of%2520traditional%2520methods%2520and%2520neural%2520networks%250Aby%2520proposing%2520an%2520optimization-embedded%2520neural%2520network.%2520This%2520network%2520can%2520learn%250Ahigh-quality%2520trajectories%2520directly%2520from%2520visual%2520inputs%2520without%2520the%2520need%2520of%250Amapping%252C%2520while%2520ensuring%2520dynamic%2520feasibility.%2520Here%252C%2520the%2520deep%2520neural%2520network%2520is%250Aemployed%2520to%2520directly%2520extract%2520environment%2520safety%2520regions%2520from%2520depth%2520images.%250ASubsequently%252C%2520we%2520employ%2520a%2520model-based%2520approach%2520to%2520represent%2520these%2520regions%2520as%250Asafety%2520constraints%2520in%2520trajectory%2520optimization.%2520Leveraging%2520the%2520availability%2520of%250Ahighly%2520efficient%2520optimization%2520algorithms%252C%2520our%2520method%2520robustly%2520converges%2520to%250Afeasible%2520and%2520optimal%2520solutions%2520that%2520satisfy%2520various%2520user-defined%2520constraints.%250AMoreover%252C%2520we%2520differentiate%2520the%2520optimization%2520process%252C%2520allowing%2520it%2520to%2520be%2520trained%250Aas%2520a%2520layer%2520within%2520the%2520neural%2520network.%2520This%2520approach%2520facilitates%2520the%2520direct%250Ainteraction%2520between%2520perception%2520and%2520planning%252C%2520enabling%2520the%2520network%2520to%2520focus%2520more%250Aon%2520the%2520spatial%2520regions%2520where%2520optimal%2520solutions%2520exist.%2520As%2520a%2520result%252C%2520it%2520further%250Aenhances%2520the%2520quality%2520and%2520stability%2520of%2520the%2520generated%2520trajectories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Plan%20Maneuverable%20and%20Agile%20Flight%20Trajectory%20with%0A%20%20Optimization%20Embedded%20Networks&entry.906535625=Zhichao%20Han%20and%20Long%20Xu%20and%20Fei%20Gao&entry.1292438233=%20%20In%20recent%20times%2C%20an%20increasing%20number%20of%20researchers%20have%20been%20devoted%20to%0Autilizing%20deep%20neural%20networks%20for%20end-to-end%20flight%20navigation.%20This%20approach%0Ahas%20gained%20traction%20due%20to%20its%20ability%20to%20bridge%20the%20gap%20between%20perception%20and%0Aplanning%20that%20exists%20in%20traditional%20methods%2C%20thereby%20eliminating%20delays%20between%0Amodules.%20However%2C%20the%20practice%20of%20replacing%20original%20modules%20with%20neural%0Anetworks%20in%20a%20black-box%20manner%20diminishes%20the%20overall%20system%27s%20robustness%20and%0Astability.%20It%20lacks%20principled%20explanations%20and%20often%20fails%20to%20consistently%0Agenerate%20high-quality%20motion%20trajectories.%20Furthermore%2C%20such%20methods%20often%0Astruggle%20to%20rigorously%20account%20for%20the%20robot%27s%20kinematic%20constraints%2C%20resulting%0Ain%20the%20generation%20of%20trajectories%20that%20cannot%20be%20executed%20satisfactorily.%20In%0Athis%20work%2C%20we%20combine%20the%20advantages%20of%20traditional%20methods%20and%20neural%20networks%0Aby%20proposing%20an%20optimization-embedded%20neural%20network.%20This%20network%20can%20learn%0Ahigh-quality%20trajectories%20directly%20from%20visual%20inputs%20without%20the%20need%20of%0Amapping%2C%20while%20ensuring%20dynamic%20feasibility.%20Here%2C%20the%20deep%20neural%20network%20is%0Aemployed%20to%20directly%20extract%20environment%20safety%20regions%20from%20depth%20images.%0ASubsequently%2C%20we%20employ%20a%20model-based%20approach%20to%20represent%20these%20regions%20as%0Asafety%20constraints%20in%20trajectory%20optimization.%20Leveraging%20the%20availability%20of%0Ahighly%20efficient%20optimization%20algorithms%2C%20our%20method%20robustly%20converges%20to%0Afeasible%20and%20optimal%20solutions%20that%20satisfy%20various%20user-defined%20constraints.%0AMoreover%2C%20we%20differentiate%20the%20optimization%20process%2C%20allowing%20it%20to%20be%20trained%0Aas%20a%20layer%20within%20the%20neural%20network.%20This%20approach%20facilitates%20the%20direct%0Ainteraction%20between%20perception%20and%20planning%2C%20enabling%20the%20network%20to%20focus%20more%0Aon%20the%20spatial%20regions%20where%20optimal%20solutions%20exist.%20As%20a%20result%2C%20it%20further%0Aenhances%20the%20quality%20and%20stability%20of%20the%20generated%20trajectories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07736v1&entry.124074799=Read"},
{"title": "Integrity Monitoring of 3D Object Detection in Automated Driving Systems\n  using Raw Activation Patterns and Spatial Filtering", "author": "Hakan Yekta Yatbaz and Mehrdad Dianati and Konstantinos Koufos and Roger Woodman", "abstract": "  The deep neural network (DNN) models are widely used for object detection in\nautomated driving systems (ADS). Yet, such models are prone to errors which can\nhave serious safety implications. Introspection and self-assessment models that\naim to detect such errors are therefore of paramount importance for the safe\ndeployment of ADS. Current research on this topic has focused on techniques to\nmonitor the integrity of the perception mechanism in ADS. Existing\nintrospection models in the literature, however, largely concentrate on\ndetecting perception errors by assigning equal importance to all parts of the\ninput data frame to the perception module. This generic approach overlooks the\nvarying safety significance of different objects within a scene, which obscures\nthe recognition of safety-critical errors, posing challenges in assessing the\nreliability of perception in specific, crucial instances. Motivated by this\nshortcoming of state of the art, this paper proposes a novel method integrating\nraw activation patterns of the underlying DNNs, employed by the perception\nmodule, analysis with spatial filtering techniques. This novel approach\nenhances the accuracy of runtime introspection of the DNN-based 3D object\ndetections by selectively focusing on an area of interest in the data, thereby\ncontributing to the safety and efficacy of ADS perception self-assessment\nprocesses.\n", "link": "http://arxiv.org/abs/2405.07600v1", "date": "2024-05-13", "relevancy": 2.2158, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.574}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.54}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrity%20Monitoring%20of%203D%20Object%20Detection%20in%20Automated%20Driving%20Systems%0A%20%20using%20Raw%20Activation%20Patterns%20and%20Spatial%20Filtering&body=Title%3A%20Integrity%20Monitoring%20of%203D%20Object%20Detection%20in%20Automated%20Driving%20Systems%0A%20%20using%20Raw%20Activation%20Patterns%20and%20Spatial%20Filtering%0AAuthor%3A%20Hakan%20Yekta%20Yatbaz%20and%20Mehrdad%20Dianati%20and%20Konstantinos%20Koufos%20and%20Roger%20Woodman%0AAbstract%3A%20%20%20The%20deep%20neural%20network%20%28DNN%29%20models%20are%20widely%20used%20for%20object%20detection%20in%0Aautomated%20driving%20systems%20%28ADS%29.%20Yet%2C%20such%20models%20are%20prone%20to%20errors%20which%20can%0Ahave%20serious%20safety%20implications.%20Introspection%20and%20self-assessment%20models%20that%0Aaim%20to%20detect%20such%20errors%20are%20therefore%20of%20paramount%20importance%20for%20the%20safe%0Adeployment%20of%20ADS.%20Current%20research%20on%20this%20topic%20has%20focused%20on%20techniques%20to%0Amonitor%20the%20integrity%20of%20the%20perception%20mechanism%20in%20ADS.%20Existing%0Aintrospection%20models%20in%20the%20literature%2C%20however%2C%20largely%20concentrate%20on%0Adetecting%20perception%20errors%20by%20assigning%20equal%20importance%20to%20all%20parts%20of%20the%0Ainput%20data%20frame%20to%20the%20perception%20module.%20This%20generic%20approach%20overlooks%20the%0Avarying%20safety%20significance%20of%20different%20objects%20within%20a%20scene%2C%20which%20obscures%0Athe%20recognition%20of%20safety-critical%20errors%2C%20posing%20challenges%20in%20assessing%20the%0Areliability%20of%20perception%20in%20specific%2C%20crucial%20instances.%20Motivated%20by%20this%0Ashortcoming%20of%20state%20of%20the%20art%2C%20this%20paper%20proposes%20a%20novel%20method%20integrating%0Araw%20activation%20patterns%20of%20the%20underlying%20DNNs%2C%20employed%20by%20the%20perception%0Amodule%2C%20analysis%20with%20spatial%20filtering%20techniques.%20This%20novel%20approach%0Aenhances%20the%20accuracy%20of%20runtime%20introspection%20of%20the%20DNN-based%203D%20object%0Adetections%20by%20selectively%20focusing%20on%20an%20area%20of%20interest%20in%20the%20data%2C%20thereby%0Acontributing%20to%20the%20safety%20and%20efficacy%20of%20ADS%20perception%20self-assessment%0Aprocesses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrity%2520Monitoring%2520of%25203D%2520Object%2520Detection%2520in%2520Automated%2520Driving%2520Systems%250A%2520%2520using%2520Raw%2520Activation%2520Patterns%2520and%2520Spatial%2520Filtering%26entry.906535625%3DHakan%2520Yekta%2520Yatbaz%2520and%2520Mehrdad%2520Dianati%2520and%2520Konstantinos%2520Koufos%2520and%2520Roger%2520Woodman%26entry.1292438233%3D%2520%2520The%2520deep%2520neural%2520network%2520%2528DNN%2529%2520models%2520are%2520widely%2520used%2520for%2520object%2520detection%2520in%250Aautomated%2520driving%2520systems%2520%2528ADS%2529.%2520Yet%252C%2520such%2520models%2520are%2520prone%2520to%2520errors%2520which%2520can%250Ahave%2520serious%2520safety%2520implications.%2520Introspection%2520and%2520self-assessment%2520models%2520that%250Aaim%2520to%2520detect%2520such%2520errors%2520are%2520therefore%2520of%2520paramount%2520importance%2520for%2520the%2520safe%250Adeployment%2520of%2520ADS.%2520Current%2520research%2520on%2520this%2520topic%2520has%2520focused%2520on%2520techniques%2520to%250Amonitor%2520the%2520integrity%2520of%2520the%2520perception%2520mechanism%2520in%2520ADS.%2520Existing%250Aintrospection%2520models%2520in%2520the%2520literature%252C%2520however%252C%2520largely%2520concentrate%2520on%250Adetecting%2520perception%2520errors%2520by%2520assigning%2520equal%2520importance%2520to%2520all%2520parts%2520of%2520the%250Ainput%2520data%2520frame%2520to%2520the%2520perception%2520module.%2520This%2520generic%2520approach%2520overlooks%2520the%250Avarying%2520safety%2520significance%2520of%2520different%2520objects%2520within%2520a%2520scene%252C%2520which%2520obscures%250Athe%2520recognition%2520of%2520safety-critical%2520errors%252C%2520posing%2520challenges%2520in%2520assessing%2520the%250Areliability%2520of%2520perception%2520in%2520specific%252C%2520crucial%2520instances.%2520Motivated%2520by%2520this%250Ashortcoming%2520of%2520state%2520of%2520the%2520art%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520method%2520integrating%250Araw%2520activation%2520patterns%2520of%2520the%2520underlying%2520DNNs%252C%2520employed%2520by%2520the%2520perception%250Amodule%252C%2520analysis%2520with%2520spatial%2520filtering%2520techniques.%2520This%2520novel%2520approach%250Aenhances%2520the%2520accuracy%2520of%2520runtime%2520introspection%2520of%2520the%2520DNN-based%25203D%2520object%250Adetections%2520by%2520selectively%2520focusing%2520on%2520an%2520area%2520of%2520interest%2520in%2520the%2520data%252C%2520thereby%250Acontributing%2520to%2520the%2520safety%2520and%2520efficacy%2520of%2520ADS%2520perception%2520self-assessment%250Aprocesses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrity%20Monitoring%20of%203D%20Object%20Detection%20in%20Automated%20Driving%20Systems%0A%20%20using%20Raw%20Activation%20Patterns%20and%20Spatial%20Filtering&entry.906535625=Hakan%20Yekta%20Yatbaz%20and%20Mehrdad%20Dianati%20and%20Konstantinos%20Koufos%20and%20Roger%20Woodman&entry.1292438233=%20%20The%20deep%20neural%20network%20%28DNN%29%20models%20are%20widely%20used%20for%20object%20detection%20in%0Aautomated%20driving%20systems%20%28ADS%29.%20Yet%2C%20such%20models%20are%20prone%20to%20errors%20which%20can%0Ahave%20serious%20safety%20implications.%20Introspection%20and%20self-assessment%20models%20that%0Aaim%20to%20detect%20such%20errors%20are%20therefore%20of%20paramount%20importance%20for%20the%20safe%0Adeployment%20of%20ADS.%20Current%20research%20on%20this%20topic%20has%20focused%20on%20techniques%20to%0Amonitor%20the%20integrity%20of%20the%20perception%20mechanism%20in%20ADS.%20Existing%0Aintrospection%20models%20in%20the%20literature%2C%20however%2C%20largely%20concentrate%20on%0Adetecting%20perception%20errors%20by%20assigning%20equal%20importance%20to%20all%20parts%20of%20the%0Ainput%20data%20frame%20to%20the%20perception%20module.%20This%20generic%20approach%20overlooks%20the%0Avarying%20safety%20significance%20of%20different%20objects%20within%20a%20scene%2C%20which%20obscures%0Athe%20recognition%20of%20safety-critical%20errors%2C%20posing%20challenges%20in%20assessing%20the%0Areliability%20of%20perception%20in%20specific%2C%20crucial%20instances.%20Motivated%20by%20this%0Ashortcoming%20of%20state%20of%20the%20art%2C%20this%20paper%20proposes%20a%20novel%20method%20integrating%0Araw%20activation%20patterns%20of%20the%20underlying%20DNNs%2C%20employed%20by%20the%20perception%0Amodule%2C%20analysis%20with%20spatial%20filtering%20techniques.%20This%20novel%20approach%0Aenhances%20the%20accuracy%20of%20runtime%20introspection%20of%20the%20DNN-based%203D%20object%0Adetections%20by%20selectively%20focusing%20on%20an%20area%20of%20interest%20in%20the%20data%2C%20thereby%0Acontributing%20to%20the%20safety%20and%20efficacy%20of%20ADS%20perception%20self-assessment%0Aprocesses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07600v1&entry.124074799=Read"},
{"title": "3DTINC: Time-Equivariant Non-Contrastive Learning for Predicting Disease\n  Progression from Longitudinal OCTs", "author": "Taha Emre and Arunava Chakravarty and Antoine Rivail and Dmitrii Lachinov and Oliver Leingang and Sophie Riedl and Julia Mai and Hendrik P. N. Scholl and Sobha Sivaprasad and Daniel Rueckert and Andrew Lotery and Ursula Schmidt-Erfurth and Hrvoje Bogunovi\u0107", "abstract": "  Self-supervised learning (SSL) has emerged as a powerful technique for\nimproving the efficiency and effectiveness of deep learning models. Contrastive\nmethods are a prominent family of SSL that extract similar representations of\ntwo augmented views of an image while pushing away others in the representation\nspace as negatives. However, the state-of-the-art contrastive methods require\nlarge batch sizes and augmentations designed for natural images that are\nimpractical for 3D medical images. To address these limitations, we propose a\nnew longitudinal SSL method, 3DTINC, based on non-contrastive learning. It is\ndesigned to learn perturbation-invariant features for 3D optical coherence\ntomography (OCT) volumes, using augmentations specifically designed for OCT. We\nintroduce a new non-contrastive similarity loss term that learns temporal\ninformation implicitly from intra-patient scans acquired at different times.\nOur experiments show that this temporal information is crucial for predicting\nprogression of retinal diseases, such as age-related macular degeneration\n(AMD). After pretraining with 3DTINC, we evaluated the learned representations\nand the prognostic models on two large-scale longitudinal datasets of retinal\nOCTs where we predict the conversion to wet-AMD within a six months interval.\nOur results demonstrate that each component of our contributions is crucial for\nlearning meaningful representations useful in predicting disease progression\nfrom longitudinal volumetric scans.\n", "link": "http://arxiv.org/abs/2312.16980v2", "date": "2024-05-13", "relevancy": 2.2144, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5698}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5421}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DTINC%3A%20Time-Equivariant%20Non-Contrastive%20Learning%20for%20Predicting%20Disease%0A%20%20Progression%20from%20Longitudinal%20OCTs&body=Title%3A%203DTINC%3A%20Time-Equivariant%20Non-Contrastive%20Learning%20for%20Predicting%20Disease%0A%20%20Progression%20from%20Longitudinal%20OCTs%0AAuthor%3A%20Taha%20Emre%20and%20Arunava%20Chakravarty%20and%20Antoine%20Rivail%20and%20Dmitrii%20Lachinov%20and%20Oliver%20Leingang%20and%20Sophie%20Riedl%20and%20Julia%20Mai%20and%20Hendrik%20P.%20N.%20Scholl%20and%20Sobha%20Sivaprasad%20and%20Daniel%20Rueckert%20and%20Andrew%20Lotery%20and%20Ursula%20Schmidt-Erfurth%20and%20Hrvoje%20Bogunovi%C4%87%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20has%20emerged%20as%20a%20powerful%20technique%20for%0Aimproving%20the%20efficiency%20and%20effectiveness%20of%20deep%20learning%20models.%20Contrastive%0Amethods%20are%20a%20prominent%20family%20of%20SSL%20that%20extract%20similar%20representations%20of%0Atwo%20augmented%20views%20of%20an%20image%20while%20pushing%20away%20others%20in%20the%20representation%0Aspace%20as%20negatives.%20However%2C%20the%20state-of-the-art%20contrastive%20methods%20require%0Alarge%20batch%20sizes%20and%20augmentations%20designed%20for%20natural%20images%20that%20are%0Aimpractical%20for%203D%20medical%20images.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Anew%20longitudinal%20SSL%20method%2C%203DTINC%2C%20based%20on%20non-contrastive%20learning.%20It%20is%0Adesigned%20to%20learn%20perturbation-invariant%20features%20for%203D%20optical%20coherence%0Atomography%20%28OCT%29%20volumes%2C%20using%20augmentations%20specifically%20designed%20for%20OCT.%20We%0Aintroduce%20a%20new%20non-contrastive%20similarity%20loss%20term%20that%20learns%20temporal%0Ainformation%20implicitly%20from%20intra-patient%20scans%20acquired%20at%20different%20times.%0AOur%20experiments%20show%20that%20this%20temporal%20information%20is%20crucial%20for%20predicting%0Aprogression%20of%20retinal%20diseases%2C%20such%20as%20age-related%20macular%20degeneration%0A%28AMD%29.%20After%20pretraining%20with%203DTINC%2C%20we%20evaluated%20the%20learned%20representations%0Aand%20the%20prognostic%20models%20on%20two%20large-scale%20longitudinal%20datasets%20of%20retinal%0AOCTs%20where%20we%20predict%20the%20conversion%20to%20wet-AMD%20within%20a%20six%20months%20interval.%0AOur%20results%20demonstrate%20that%20each%20component%20of%20our%20contributions%20is%20crucial%20for%0Alearning%20meaningful%20representations%20useful%20in%20predicting%20disease%20progression%0Afrom%20longitudinal%20volumetric%20scans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.16980v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DTINC%253A%2520Time-Equivariant%2520Non-Contrastive%2520Learning%2520for%2520Predicting%2520Disease%250A%2520%2520Progression%2520from%2520Longitudinal%2520OCTs%26entry.906535625%3DTaha%2520Emre%2520and%2520Arunava%2520Chakravarty%2520and%2520Antoine%2520Rivail%2520and%2520Dmitrii%2520Lachinov%2520and%2520Oliver%2520Leingang%2520and%2520Sophie%2520Riedl%2520and%2520Julia%2520Mai%2520and%2520Hendrik%2520P.%2520N.%2520Scholl%2520and%2520Sobha%2520Sivaprasad%2520and%2520Daniel%2520Rueckert%2520and%2520Andrew%2520Lotery%2520and%2520Ursula%2520Schmidt-Erfurth%2520and%2520Hrvoje%2520Bogunovi%25C4%2587%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520technique%2520for%250Aimproving%2520the%2520efficiency%2520and%2520effectiveness%2520of%2520deep%2520learning%2520models.%2520Contrastive%250Amethods%2520are%2520a%2520prominent%2520family%2520of%2520SSL%2520that%2520extract%2520similar%2520representations%2520of%250Atwo%2520augmented%2520views%2520of%2520an%2520image%2520while%2520pushing%2520away%2520others%2520in%2520the%2520representation%250Aspace%2520as%2520negatives.%2520However%252C%2520the%2520state-of-the-art%2520contrastive%2520methods%2520require%250Alarge%2520batch%2520sizes%2520and%2520augmentations%2520designed%2520for%2520natural%2520images%2520that%2520are%250Aimpractical%2520for%25203D%2520medical%2520images.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%250Anew%2520longitudinal%2520SSL%2520method%252C%25203DTINC%252C%2520based%2520on%2520non-contrastive%2520learning.%2520It%2520is%250Adesigned%2520to%2520learn%2520perturbation-invariant%2520features%2520for%25203D%2520optical%2520coherence%250Atomography%2520%2528OCT%2529%2520volumes%252C%2520using%2520augmentations%2520specifically%2520designed%2520for%2520OCT.%2520We%250Aintroduce%2520a%2520new%2520non-contrastive%2520similarity%2520loss%2520term%2520that%2520learns%2520temporal%250Ainformation%2520implicitly%2520from%2520intra-patient%2520scans%2520acquired%2520at%2520different%2520times.%250AOur%2520experiments%2520show%2520that%2520this%2520temporal%2520information%2520is%2520crucial%2520for%2520predicting%250Aprogression%2520of%2520retinal%2520diseases%252C%2520such%2520as%2520age-related%2520macular%2520degeneration%250A%2528AMD%2529.%2520After%2520pretraining%2520with%25203DTINC%252C%2520we%2520evaluated%2520the%2520learned%2520representations%250Aand%2520the%2520prognostic%2520models%2520on%2520two%2520large-scale%2520longitudinal%2520datasets%2520of%2520retinal%250AOCTs%2520where%2520we%2520predict%2520the%2520conversion%2520to%2520wet-AMD%2520within%2520a%2520six%2520months%2520interval.%250AOur%2520results%2520demonstrate%2520that%2520each%2520component%2520of%2520our%2520contributions%2520is%2520crucial%2520for%250Alearning%2520meaningful%2520representations%2520useful%2520in%2520predicting%2520disease%2520progression%250Afrom%2520longitudinal%2520volumetric%2520scans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.16980v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DTINC%3A%20Time-Equivariant%20Non-Contrastive%20Learning%20for%20Predicting%20Disease%0A%20%20Progression%20from%20Longitudinal%20OCTs&entry.906535625=Taha%20Emre%20and%20Arunava%20Chakravarty%20and%20Antoine%20Rivail%20and%20Dmitrii%20Lachinov%20and%20Oliver%20Leingang%20and%20Sophie%20Riedl%20and%20Julia%20Mai%20and%20Hendrik%20P.%20N.%20Scholl%20and%20Sobha%20Sivaprasad%20and%20Daniel%20Rueckert%20and%20Andrew%20Lotery%20and%20Ursula%20Schmidt-Erfurth%20and%20Hrvoje%20Bogunovi%C4%87&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20has%20emerged%20as%20a%20powerful%20technique%20for%0Aimproving%20the%20efficiency%20and%20effectiveness%20of%20deep%20learning%20models.%20Contrastive%0Amethods%20are%20a%20prominent%20family%20of%20SSL%20that%20extract%20similar%20representations%20of%0Atwo%20augmented%20views%20of%20an%20image%20while%20pushing%20away%20others%20in%20the%20representation%0Aspace%20as%20negatives.%20However%2C%20the%20state-of-the-art%20contrastive%20methods%20require%0Alarge%20batch%20sizes%20and%20augmentations%20designed%20for%20natural%20images%20that%20are%0Aimpractical%20for%203D%20medical%20images.%20To%20address%20these%20limitations%2C%20we%20propose%20a%0Anew%20longitudinal%20SSL%20method%2C%203DTINC%2C%20based%20on%20non-contrastive%20learning.%20It%20is%0Adesigned%20to%20learn%20perturbation-invariant%20features%20for%203D%20optical%20coherence%0Atomography%20%28OCT%29%20volumes%2C%20using%20augmentations%20specifically%20designed%20for%20OCT.%20We%0Aintroduce%20a%20new%20non-contrastive%20similarity%20loss%20term%20that%20learns%20temporal%0Ainformation%20implicitly%20from%20intra-patient%20scans%20acquired%20at%20different%20times.%0AOur%20experiments%20show%20that%20this%20temporal%20information%20is%20crucial%20for%20predicting%0Aprogression%20of%20retinal%20diseases%2C%20such%20as%20age-related%20macular%20degeneration%0A%28AMD%29.%20After%20pretraining%20with%203DTINC%2C%20we%20evaluated%20the%20learned%20representations%0Aand%20the%20prognostic%20models%20on%20two%20large-scale%20longitudinal%20datasets%20of%20retinal%0AOCTs%20where%20we%20predict%20the%20conversion%20to%20wet-AMD%20within%20a%20six%20months%20interval.%0AOur%20results%20demonstrate%20that%20each%20component%20of%20our%20contributions%20is%20crucial%20for%0Alearning%20meaningful%20representations%20useful%20in%20predicting%20disease%20progression%0Afrom%20longitudinal%20volumetric%20scans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.16980v2&entry.124074799=Read"},
{"title": "Diagnosing and Predicting Autonomous Vehicle Operational Safety Using\n  Multiple Simulation Modalities and a Virtual Environment", "author": "Joe Beck and Shean Huff and Subhadeep Chakraborty", "abstract": "  Even as technology and performance gains are made in the sphere of automated\ndriving, safety concerns remain. Vehicle simulation has long been seen as a\ntool to overcome the cost associated with a massive amount of on-road testing\nfor development and discovery of safety critical \"edge-cases\". However, purely\nsoftware-based vehicle models may leave a large realism gap between their\nreal-world counterparts in terms of dynamic response, and highly realistic\nvehicle-in-the-loop (VIL) simulations that encapsulate a virtual world around a\nphysical vehicle may still be quite expensive to produce and similarly time\nintensive as on-road testing. In this work, we demonstrate an AV simulation\ntest bed that combines the realism of vehicle-in-the-loop (VIL) simulation with\nthe ease of implementation of model-in-the-loop (MIL) simulation. The setup\ndemonstrated in this work allows for response diagnosis for the VIL\nsimulations. By observing causal links between virtual weather and lighting\nconditions that surround the virtual depiction of our vehicle, the vision-based\nperception model and controller of Openpilot, and the dynamic response of our\nphysical vehicle under test, we can draw conclusions regarding how the\nperceived environment contributed to vehicle response. Conversely, we also\ndemonstrate response prediction for the MIL setup, where the need for a\nphysical vehicle is not required to draw richer conclusions around the impact\nof environmental conditions on AV performance than could be obtained with VIL\nsimulation alone. These combine for a simulation setup with accurate real-world\nimplications for edge-case discovery that is both cost effective and time\nefficient to implement.\n", "link": "http://arxiv.org/abs/2405.07981v1", "date": "2024-05-13", "relevancy": 2.1877, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5989}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5655}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diagnosing%20and%20Predicting%20Autonomous%20Vehicle%20Operational%20Safety%20Using%0A%20%20Multiple%20Simulation%20Modalities%20and%20a%20Virtual%20Environment&body=Title%3A%20Diagnosing%20and%20Predicting%20Autonomous%20Vehicle%20Operational%20Safety%20Using%0A%20%20Multiple%20Simulation%20Modalities%20and%20a%20Virtual%20Environment%0AAuthor%3A%20Joe%20Beck%20and%20Shean%20Huff%20and%20Subhadeep%20Chakraborty%0AAbstract%3A%20%20%20Even%20as%20technology%20and%20performance%20gains%20are%20made%20in%20the%20sphere%20of%20automated%0Adriving%2C%20safety%20concerns%20remain.%20Vehicle%20simulation%20has%20long%20been%20seen%20as%20a%0Atool%20to%20overcome%20the%20cost%20associated%20with%20a%20massive%20amount%20of%20on-road%20testing%0Afor%20development%20and%20discovery%20of%20safety%20critical%20%22edge-cases%22.%20However%2C%20purely%0Asoftware-based%20vehicle%20models%20may%20leave%20a%20large%20realism%20gap%20between%20their%0Areal-world%20counterparts%20in%20terms%20of%20dynamic%20response%2C%20and%20highly%20realistic%0Avehicle-in-the-loop%20%28VIL%29%20simulations%20that%20encapsulate%20a%20virtual%20world%20around%20a%0Aphysical%20vehicle%20may%20still%20be%20quite%20expensive%20to%20produce%20and%20similarly%20time%0Aintensive%20as%20on-road%20testing.%20In%20this%20work%2C%20we%20demonstrate%20an%20AV%20simulation%0Atest%20bed%20that%20combines%20the%20realism%20of%20vehicle-in-the-loop%20%28VIL%29%20simulation%20with%0Athe%20ease%20of%20implementation%20of%20model-in-the-loop%20%28MIL%29%20simulation.%20The%20setup%0Ademonstrated%20in%20this%20work%20allows%20for%20response%20diagnosis%20for%20the%20VIL%0Asimulations.%20By%20observing%20causal%20links%20between%20virtual%20weather%20and%20lighting%0Aconditions%20that%20surround%20the%20virtual%20depiction%20of%20our%20vehicle%2C%20the%20vision-based%0Aperception%20model%20and%20controller%20of%20Openpilot%2C%20and%20the%20dynamic%20response%20of%20our%0Aphysical%20vehicle%20under%20test%2C%20we%20can%20draw%20conclusions%20regarding%20how%20the%0Aperceived%20environment%20contributed%20to%20vehicle%20response.%20Conversely%2C%20we%20also%0Ademonstrate%20response%20prediction%20for%20the%20MIL%20setup%2C%20where%20the%20need%20for%20a%0Aphysical%20vehicle%20is%20not%20required%20to%20draw%20richer%20conclusions%20around%20the%20impact%0Aof%20environmental%20conditions%20on%20AV%20performance%20than%20could%20be%20obtained%20with%20VIL%0Asimulation%20alone.%20These%20combine%20for%20a%20simulation%20setup%20with%20accurate%20real-world%0Aimplications%20for%20edge-case%20discovery%20that%20is%20both%20cost%20effective%20and%20time%0Aefficient%20to%20implement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiagnosing%2520and%2520Predicting%2520Autonomous%2520Vehicle%2520Operational%2520Safety%2520Using%250A%2520%2520Multiple%2520Simulation%2520Modalities%2520and%2520a%2520Virtual%2520Environment%26entry.906535625%3DJoe%2520Beck%2520and%2520Shean%2520Huff%2520and%2520Subhadeep%2520Chakraborty%26entry.1292438233%3D%2520%2520Even%2520as%2520technology%2520and%2520performance%2520gains%2520are%2520made%2520in%2520the%2520sphere%2520of%2520automated%250Adriving%252C%2520safety%2520concerns%2520remain.%2520Vehicle%2520simulation%2520has%2520long%2520been%2520seen%2520as%2520a%250Atool%2520to%2520overcome%2520the%2520cost%2520associated%2520with%2520a%2520massive%2520amount%2520of%2520on-road%2520testing%250Afor%2520development%2520and%2520discovery%2520of%2520safety%2520critical%2520%2522edge-cases%2522.%2520However%252C%2520purely%250Asoftware-based%2520vehicle%2520models%2520may%2520leave%2520a%2520large%2520realism%2520gap%2520between%2520their%250Areal-world%2520counterparts%2520in%2520terms%2520of%2520dynamic%2520response%252C%2520and%2520highly%2520realistic%250Avehicle-in-the-loop%2520%2528VIL%2529%2520simulations%2520that%2520encapsulate%2520a%2520virtual%2520world%2520around%2520a%250Aphysical%2520vehicle%2520may%2520still%2520be%2520quite%2520expensive%2520to%2520produce%2520and%2520similarly%2520time%250Aintensive%2520as%2520on-road%2520testing.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520an%2520AV%2520simulation%250Atest%2520bed%2520that%2520combines%2520the%2520realism%2520of%2520vehicle-in-the-loop%2520%2528VIL%2529%2520simulation%2520with%250Athe%2520ease%2520of%2520implementation%2520of%2520model-in-the-loop%2520%2528MIL%2529%2520simulation.%2520The%2520setup%250Ademonstrated%2520in%2520this%2520work%2520allows%2520for%2520response%2520diagnosis%2520for%2520the%2520VIL%250Asimulations.%2520By%2520observing%2520causal%2520links%2520between%2520virtual%2520weather%2520and%2520lighting%250Aconditions%2520that%2520surround%2520the%2520virtual%2520depiction%2520of%2520our%2520vehicle%252C%2520the%2520vision-based%250Aperception%2520model%2520and%2520controller%2520of%2520Openpilot%252C%2520and%2520the%2520dynamic%2520response%2520of%2520our%250Aphysical%2520vehicle%2520under%2520test%252C%2520we%2520can%2520draw%2520conclusions%2520regarding%2520how%2520the%250Aperceived%2520environment%2520contributed%2520to%2520vehicle%2520response.%2520Conversely%252C%2520we%2520also%250Ademonstrate%2520response%2520prediction%2520for%2520the%2520MIL%2520setup%252C%2520where%2520the%2520need%2520for%2520a%250Aphysical%2520vehicle%2520is%2520not%2520required%2520to%2520draw%2520richer%2520conclusions%2520around%2520the%2520impact%250Aof%2520environmental%2520conditions%2520on%2520AV%2520performance%2520than%2520could%2520be%2520obtained%2520with%2520VIL%250Asimulation%2520alone.%2520These%2520combine%2520for%2520a%2520simulation%2520setup%2520with%2520accurate%2520real-world%250Aimplications%2520for%2520edge-case%2520discovery%2520that%2520is%2520both%2520cost%2520effective%2520and%2520time%250Aefficient%2520to%2520implement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diagnosing%20and%20Predicting%20Autonomous%20Vehicle%20Operational%20Safety%20Using%0A%20%20Multiple%20Simulation%20Modalities%20and%20a%20Virtual%20Environment&entry.906535625=Joe%20Beck%20and%20Shean%20Huff%20and%20Subhadeep%20Chakraborty&entry.1292438233=%20%20Even%20as%20technology%20and%20performance%20gains%20are%20made%20in%20the%20sphere%20of%20automated%0Adriving%2C%20safety%20concerns%20remain.%20Vehicle%20simulation%20has%20long%20been%20seen%20as%20a%0Atool%20to%20overcome%20the%20cost%20associated%20with%20a%20massive%20amount%20of%20on-road%20testing%0Afor%20development%20and%20discovery%20of%20safety%20critical%20%22edge-cases%22.%20However%2C%20purely%0Asoftware-based%20vehicle%20models%20may%20leave%20a%20large%20realism%20gap%20between%20their%0Areal-world%20counterparts%20in%20terms%20of%20dynamic%20response%2C%20and%20highly%20realistic%0Avehicle-in-the-loop%20%28VIL%29%20simulations%20that%20encapsulate%20a%20virtual%20world%20around%20a%0Aphysical%20vehicle%20may%20still%20be%20quite%20expensive%20to%20produce%20and%20similarly%20time%0Aintensive%20as%20on-road%20testing.%20In%20this%20work%2C%20we%20demonstrate%20an%20AV%20simulation%0Atest%20bed%20that%20combines%20the%20realism%20of%20vehicle-in-the-loop%20%28VIL%29%20simulation%20with%0Athe%20ease%20of%20implementation%20of%20model-in-the-loop%20%28MIL%29%20simulation.%20The%20setup%0Ademonstrated%20in%20this%20work%20allows%20for%20response%20diagnosis%20for%20the%20VIL%0Asimulations.%20By%20observing%20causal%20links%20between%20virtual%20weather%20and%20lighting%0Aconditions%20that%20surround%20the%20virtual%20depiction%20of%20our%20vehicle%2C%20the%20vision-based%0Aperception%20model%20and%20controller%20of%20Openpilot%2C%20and%20the%20dynamic%20response%20of%20our%0Aphysical%20vehicle%20under%20test%2C%20we%20can%20draw%20conclusions%20regarding%20how%20the%0Aperceived%20environment%20contributed%20to%20vehicle%20response.%20Conversely%2C%20we%20also%0Ademonstrate%20response%20prediction%20for%20the%20MIL%20setup%2C%20where%20the%20need%20for%20a%0Aphysical%20vehicle%20is%20not%20required%20to%20draw%20richer%20conclusions%20around%20the%20impact%0Aof%20environmental%20conditions%20on%20AV%20performance%20than%20could%20be%20obtained%20with%20VIL%0Asimulation%20alone.%20These%20combine%20for%20a%20simulation%20setup%20with%20accurate%20real-world%0Aimplications%20for%20edge-case%20discovery%20that%20is%20both%20cost%20effective%20and%20time%0Aefficient%20to%20implement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07981v1&entry.124074799=Read"},
{"title": "Decentralized Kernel Ridge Regression Based on Data-dependent Random\n  Feature", "author": "Ruikai Yang and Fan He and Mingzhen He and Jie Yang and Xiaolin Huang", "abstract": "  Random feature (RF) has been widely used for node consistency in\ndecentralized kernel ridge regression (KRR). Currently, the consistency is\nguaranteed by imposing constraints on coefficients of features, necessitating\nthat the random features on different nodes are identical. However, in many\napplications, data on different nodes varies significantly on the number or\ndistribution, which calls for adaptive and data-dependent methods that generate\ndifferent RFs. To tackle the essential difficulty, we propose a new\ndecentralized KRR algorithm that pursues consensus on decision functions, which\nallows great flexibility and well adapts data on nodes. The convergence is\nrigorously given and the effectiveness is numerically verified: by capturing\nthe characteristics of the data on each node, while maintaining the same\ncommunication costs as other methods, we achieved an average regression\naccuracy improvement of 25.5\\% across six real-world data sets.\n", "link": "http://arxiv.org/abs/2405.07791v1", "date": "2024-05-13", "relevancy": 2.1741, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.444}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4304}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.43}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decentralized%20Kernel%20Ridge%20Regression%20Based%20on%20Data-dependent%20Random%0A%20%20Feature&body=Title%3A%20Decentralized%20Kernel%20Ridge%20Regression%20Based%20on%20Data-dependent%20Random%0A%20%20Feature%0AAuthor%3A%20Ruikai%20Yang%20and%20Fan%20He%20and%20Mingzhen%20He%20and%20Jie%20Yang%20and%20Xiaolin%20Huang%0AAbstract%3A%20%20%20Random%20feature%20%28RF%29%20has%20been%20widely%20used%20for%20node%20consistency%20in%0Adecentralized%20kernel%20ridge%20regression%20%28KRR%29.%20Currently%2C%20the%20consistency%20is%0Aguaranteed%20by%20imposing%20constraints%20on%20coefficients%20of%20features%2C%20necessitating%0Athat%20the%20random%20features%20on%20different%20nodes%20are%20identical.%20However%2C%20in%20many%0Aapplications%2C%20data%20on%20different%20nodes%20varies%20significantly%20on%20the%20number%20or%0Adistribution%2C%20which%20calls%20for%20adaptive%20and%20data-dependent%20methods%20that%20generate%0Adifferent%20RFs.%20To%20tackle%20the%20essential%20difficulty%2C%20we%20propose%20a%20new%0Adecentralized%20KRR%20algorithm%20that%20pursues%20consensus%20on%20decision%20functions%2C%20which%0Aallows%20great%20flexibility%20and%20well%20adapts%20data%20on%20nodes.%20The%20convergence%20is%0Arigorously%20given%20and%20the%20effectiveness%20is%20numerically%20verified%3A%20by%20capturing%0Athe%20characteristics%20of%20the%20data%20on%20each%20node%2C%20while%20maintaining%20the%20same%0Acommunication%20costs%20as%20other%20methods%2C%20we%20achieved%20an%20average%20regression%0Aaccuracy%20improvement%20of%2025.5%5C%25%20across%20six%20real-world%20data%20sets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecentralized%2520Kernel%2520Ridge%2520Regression%2520Based%2520on%2520Data-dependent%2520Random%250A%2520%2520Feature%26entry.906535625%3DRuikai%2520Yang%2520and%2520Fan%2520He%2520and%2520Mingzhen%2520He%2520and%2520Jie%2520Yang%2520and%2520Xiaolin%2520Huang%26entry.1292438233%3D%2520%2520Random%2520feature%2520%2528RF%2529%2520has%2520been%2520widely%2520used%2520for%2520node%2520consistency%2520in%250Adecentralized%2520kernel%2520ridge%2520regression%2520%2528KRR%2529.%2520Currently%252C%2520the%2520consistency%2520is%250Aguaranteed%2520by%2520imposing%2520constraints%2520on%2520coefficients%2520of%2520features%252C%2520necessitating%250Athat%2520the%2520random%2520features%2520on%2520different%2520nodes%2520are%2520identical.%2520However%252C%2520in%2520many%250Aapplications%252C%2520data%2520on%2520different%2520nodes%2520varies%2520significantly%2520on%2520the%2520number%2520or%250Adistribution%252C%2520which%2520calls%2520for%2520adaptive%2520and%2520data-dependent%2520methods%2520that%2520generate%250Adifferent%2520RFs.%2520To%2520tackle%2520the%2520essential%2520difficulty%252C%2520we%2520propose%2520a%2520new%250Adecentralized%2520KRR%2520algorithm%2520that%2520pursues%2520consensus%2520on%2520decision%2520functions%252C%2520which%250Aallows%2520great%2520flexibility%2520and%2520well%2520adapts%2520data%2520on%2520nodes.%2520The%2520convergence%2520is%250Arigorously%2520given%2520and%2520the%2520effectiveness%2520is%2520numerically%2520verified%253A%2520by%2520capturing%250Athe%2520characteristics%2520of%2520the%2520data%2520on%2520each%2520node%252C%2520while%2520maintaining%2520the%2520same%250Acommunication%2520costs%2520as%2520other%2520methods%252C%2520we%2520achieved%2520an%2520average%2520regression%250Aaccuracy%2520improvement%2520of%252025.5%255C%2525%2520across%2520six%2520real-world%2520data%2520sets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decentralized%20Kernel%20Ridge%20Regression%20Based%20on%20Data-dependent%20Random%0A%20%20Feature&entry.906535625=Ruikai%20Yang%20and%20Fan%20He%20and%20Mingzhen%20He%20and%20Jie%20Yang%20and%20Xiaolin%20Huang&entry.1292438233=%20%20Random%20feature%20%28RF%29%20has%20been%20widely%20used%20for%20node%20consistency%20in%0Adecentralized%20kernel%20ridge%20regression%20%28KRR%29.%20Currently%2C%20the%20consistency%20is%0Aguaranteed%20by%20imposing%20constraints%20on%20coefficients%20of%20features%2C%20necessitating%0Athat%20the%20random%20features%20on%20different%20nodes%20are%20identical.%20However%2C%20in%20many%0Aapplications%2C%20data%20on%20different%20nodes%20varies%20significantly%20on%20the%20number%20or%0Adistribution%2C%20which%20calls%20for%20adaptive%20and%20data-dependent%20methods%20that%20generate%0Adifferent%20RFs.%20To%20tackle%20the%20essential%20difficulty%2C%20we%20propose%20a%20new%0Adecentralized%20KRR%20algorithm%20that%20pursues%20consensus%20on%20decision%20functions%2C%20which%0Aallows%20great%20flexibility%20and%20well%20adapts%20data%20on%20nodes.%20The%20convergence%20is%0Arigorously%20given%20and%20the%20effectiveness%20is%20numerically%20verified%3A%20by%20capturing%0Athe%20characteristics%20of%20the%20data%20on%20each%20node%2C%20while%20maintaining%20the%20same%0Acommunication%20costs%20as%20other%20methods%2C%20we%20achieved%20an%20average%20regression%0Aaccuracy%20improvement%20of%2025.5%5C%25%20across%20six%20real-world%20data%20sets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07791v1&entry.124074799=Read"},
{"title": "Quality-aware Selective Fusion Network for V-D-T Salient Object\n  Detection", "author": "Liuxin Bao and Xiaofei Zhou and Xiankai Lu and Yaoqi Sun and Haibing Yin and Zhenghui Hu and Jiyong Zhang and Chenggang Yan", "abstract": "  Depth images and thermal images contain the spatial geometry information and\nsurface temperature information, which can act as complementary information for\nthe RGB modality. However, the quality of the depth and thermal images is often\nunreliable in some challenging scenarios, which will result in the performance\ndegradation of the two-modal based salient object detection (SOD). Meanwhile,\nsome researchers pay attention to the triple-modal SOD task, where they attempt\nto explore the complementarity of the RGB image, the depth image, and the\nthermal image. However, existing triple-modal SOD methods fail to perceive the\nquality of depth maps and thermal images, which leads to performance\ndegradation when dealing with scenes with low-quality depth and thermal images.\nTherefore, we propose a quality-aware selective fusion network (QSF-Net) to\nconduct VDT salient object detection, which contains three subnets including\nthe initial feature extraction subnet, the quality-aware region selection\nsubnet, and the region-guided selective fusion subnet. Firstly, except for\nextracting features, the initial feature extraction subnet can generate a\npreliminary prediction map from each modality via a shrinkage pyramid\narchitecture. Then, we design the weakly-supervised quality-aware region\nselection subnet to generate the quality-aware maps. Concretely, we first find\nthe high-quality and low-quality regions by using the preliminary predictions,\nwhich further constitute the pseudo label that can be used to train this\nsubnet. Finally, the region-guided selective fusion subnet purifies the initial\nfeatures under the guidance of the quality-aware maps, and then fuses the\ntriple-modal features and refines the edge details of prediction maps through\nthe intra-modality and inter-modality attention (IIA) module and the edge\nrefinement (ER) module, respectively. Extensive experiments are performed on\nVDT-2048\n", "link": "http://arxiv.org/abs/2405.07655v1", "date": "2024-05-13", "relevancy": 2.1736, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.562}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5331}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quality-aware%20Selective%20Fusion%20Network%20for%20V-D-T%20Salient%20Object%0A%20%20Detection&body=Title%3A%20Quality-aware%20Selective%20Fusion%20Network%20for%20V-D-T%20Salient%20Object%0A%20%20Detection%0AAuthor%3A%20Liuxin%20Bao%20and%20Xiaofei%20Zhou%20and%20Xiankai%20Lu%20and%20Yaoqi%20Sun%20and%20Haibing%20Yin%20and%20Zhenghui%20Hu%20and%20Jiyong%20Zhang%20and%20Chenggang%20Yan%0AAbstract%3A%20%20%20Depth%20images%20and%20thermal%20images%20contain%20the%20spatial%20geometry%20information%20and%0Asurface%20temperature%20information%2C%20which%20can%20act%20as%20complementary%20information%20for%0Athe%20RGB%20modality.%20However%2C%20the%20quality%20of%20the%20depth%20and%20thermal%20images%20is%20often%0Aunreliable%20in%20some%20challenging%20scenarios%2C%20which%20will%20result%20in%20the%20performance%0Adegradation%20of%20the%20two-modal%20based%20salient%20object%20detection%20%28SOD%29.%20Meanwhile%2C%0Asome%20researchers%20pay%20attention%20to%20the%20triple-modal%20SOD%20task%2C%20where%20they%20attempt%0Ato%20explore%20the%20complementarity%20of%20the%20RGB%20image%2C%20the%20depth%20image%2C%20and%20the%0Athermal%20image.%20However%2C%20existing%20triple-modal%20SOD%20methods%20fail%20to%20perceive%20the%0Aquality%20of%20depth%20maps%20and%20thermal%20images%2C%20which%20leads%20to%20performance%0Adegradation%20when%20dealing%20with%20scenes%20with%20low-quality%20depth%20and%20thermal%20images.%0ATherefore%2C%20we%20propose%20a%20quality-aware%20selective%20fusion%20network%20%28QSF-Net%29%20to%0Aconduct%20VDT%20salient%20object%20detection%2C%20which%20contains%20three%20subnets%20including%0Athe%20initial%20feature%20extraction%20subnet%2C%20the%20quality-aware%20region%20selection%0Asubnet%2C%20and%20the%20region-guided%20selective%20fusion%20subnet.%20Firstly%2C%20except%20for%0Aextracting%20features%2C%20the%20initial%20feature%20extraction%20subnet%20can%20generate%20a%0Apreliminary%20prediction%20map%20from%20each%20modality%20via%20a%20shrinkage%20pyramid%0Aarchitecture.%20Then%2C%20we%20design%20the%20weakly-supervised%20quality-aware%20region%0Aselection%20subnet%20to%20generate%20the%20quality-aware%20maps.%20Concretely%2C%20we%20first%20find%0Athe%20high-quality%20and%20low-quality%20regions%20by%20using%20the%20preliminary%20predictions%2C%0Awhich%20further%20constitute%20the%20pseudo%20label%20that%20can%20be%20used%20to%20train%20this%0Asubnet.%20Finally%2C%20the%20region-guided%20selective%20fusion%20subnet%20purifies%20the%20initial%0Afeatures%20under%20the%20guidance%20of%20the%20quality-aware%20maps%2C%20and%20then%20fuses%20the%0Atriple-modal%20features%20and%20refines%20the%20edge%20details%20of%20prediction%20maps%20through%0Athe%20intra-modality%20and%20inter-modality%20attention%20%28IIA%29%20module%20and%20the%20edge%0Arefinement%20%28ER%29%20module%2C%20respectively.%20Extensive%20experiments%20are%20performed%20on%0AVDT-2048%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07655v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuality-aware%2520Selective%2520Fusion%2520Network%2520for%2520V-D-T%2520Salient%2520Object%250A%2520%2520Detection%26entry.906535625%3DLiuxin%2520Bao%2520and%2520Xiaofei%2520Zhou%2520and%2520Xiankai%2520Lu%2520and%2520Yaoqi%2520Sun%2520and%2520Haibing%2520Yin%2520and%2520Zhenghui%2520Hu%2520and%2520Jiyong%2520Zhang%2520and%2520Chenggang%2520Yan%26entry.1292438233%3D%2520%2520Depth%2520images%2520and%2520thermal%2520images%2520contain%2520the%2520spatial%2520geometry%2520information%2520and%250Asurface%2520temperature%2520information%252C%2520which%2520can%2520act%2520as%2520complementary%2520information%2520for%250Athe%2520RGB%2520modality.%2520However%252C%2520the%2520quality%2520of%2520the%2520depth%2520and%2520thermal%2520images%2520is%2520often%250Aunreliable%2520in%2520some%2520challenging%2520scenarios%252C%2520which%2520will%2520result%2520in%2520the%2520performance%250Adegradation%2520of%2520the%2520two-modal%2520based%2520salient%2520object%2520detection%2520%2528SOD%2529.%2520Meanwhile%252C%250Asome%2520researchers%2520pay%2520attention%2520to%2520the%2520triple-modal%2520SOD%2520task%252C%2520where%2520they%2520attempt%250Ato%2520explore%2520the%2520complementarity%2520of%2520the%2520RGB%2520image%252C%2520the%2520depth%2520image%252C%2520and%2520the%250Athermal%2520image.%2520However%252C%2520existing%2520triple-modal%2520SOD%2520methods%2520fail%2520to%2520perceive%2520the%250Aquality%2520of%2520depth%2520maps%2520and%2520thermal%2520images%252C%2520which%2520leads%2520to%2520performance%250Adegradation%2520when%2520dealing%2520with%2520scenes%2520with%2520low-quality%2520depth%2520and%2520thermal%2520images.%250ATherefore%252C%2520we%2520propose%2520a%2520quality-aware%2520selective%2520fusion%2520network%2520%2528QSF-Net%2529%2520to%250Aconduct%2520VDT%2520salient%2520object%2520detection%252C%2520which%2520contains%2520three%2520subnets%2520including%250Athe%2520initial%2520feature%2520extraction%2520subnet%252C%2520the%2520quality-aware%2520region%2520selection%250Asubnet%252C%2520and%2520the%2520region-guided%2520selective%2520fusion%2520subnet.%2520Firstly%252C%2520except%2520for%250Aextracting%2520features%252C%2520the%2520initial%2520feature%2520extraction%2520subnet%2520can%2520generate%2520a%250Apreliminary%2520prediction%2520map%2520from%2520each%2520modality%2520via%2520a%2520shrinkage%2520pyramid%250Aarchitecture.%2520Then%252C%2520we%2520design%2520the%2520weakly-supervised%2520quality-aware%2520region%250Aselection%2520subnet%2520to%2520generate%2520the%2520quality-aware%2520maps.%2520Concretely%252C%2520we%2520first%2520find%250Athe%2520high-quality%2520and%2520low-quality%2520regions%2520by%2520using%2520the%2520preliminary%2520predictions%252C%250Awhich%2520further%2520constitute%2520the%2520pseudo%2520label%2520that%2520can%2520be%2520used%2520to%2520train%2520this%250Asubnet.%2520Finally%252C%2520the%2520region-guided%2520selective%2520fusion%2520subnet%2520purifies%2520the%2520initial%250Afeatures%2520under%2520the%2520guidance%2520of%2520the%2520quality-aware%2520maps%252C%2520and%2520then%2520fuses%2520the%250Atriple-modal%2520features%2520and%2520refines%2520the%2520edge%2520details%2520of%2520prediction%2520maps%2520through%250Athe%2520intra-modality%2520and%2520inter-modality%2520attention%2520%2528IIA%2529%2520module%2520and%2520the%2520edge%250Arefinement%2520%2528ER%2529%2520module%252C%2520respectively.%2520Extensive%2520experiments%2520are%2520performed%2520on%250AVDT-2048%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07655v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quality-aware%20Selective%20Fusion%20Network%20for%20V-D-T%20Salient%20Object%0A%20%20Detection&entry.906535625=Liuxin%20Bao%20and%20Xiaofei%20Zhou%20and%20Xiankai%20Lu%20and%20Yaoqi%20Sun%20and%20Haibing%20Yin%20and%20Zhenghui%20Hu%20and%20Jiyong%20Zhang%20and%20Chenggang%20Yan&entry.1292438233=%20%20Depth%20images%20and%20thermal%20images%20contain%20the%20spatial%20geometry%20information%20and%0Asurface%20temperature%20information%2C%20which%20can%20act%20as%20complementary%20information%20for%0Athe%20RGB%20modality.%20However%2C%20the%20quality%20of%20the%20depth%20and%20thermal%20images%20is%20often%0Aunreliable%20in%20some%20challenging%20scenarios%2C%20which%20will%20result%20in%20the%20performance%0Adegradation%20of%20the%20two-modal%20based%20salient%20object%20detection%20%28SOD%29.%20Meanwhile%2C%0Asome%20researchers%20pay%20attention%20to%20the%20triple-modal%20SOD%20task%2C%20where%20they%20attempt%0Ato%20explore%20the%20complementarity%20of%20the%20RGB%20image%2C%20the%20depth%20image%2C%20and%20the%0Athermal%20image.%20However%2C%20existing%20triple-modal%20SOD%20methods%20fail%20to%20perceive%20the%0Aquality%20of%20depth%20maps%20and%20thermal%20images%2C%20which%20leads%20to%20performance%0Adegradation%20when%20dealing%20with%20scenes%20with%20low-quality%20depth%20and%20thermal%20images.%0ATherefore%2C%20we%20propose%20a%20quality-aware%20selective%20fusion%20network%20%28QSF-Net%29%20to%0Aconduct%20VDT%20salient%20object%20detection%2C%20which%20contains%20three%20subnets%20including%0Athe%20initial%20feature%20extraction%20subnet%2C%20the%20quality-aware%20region%20selection%0Asubnet%2C%20and%20the%20region-guided%20selective%20fusion%20subnet.%20Firstly%2C%20except%20for%0Aextracting%20features%2C%20the%20initial%20feature%20extraction%20subnet%20can%20generate%20a%0Apreliminary%20prediction%20map%20from%20each%20modality%20via%20a%20shrinkage%20pyramid%0Aarchitecture.%20Then%2C%20we%20design%20the%20weakly-supervised%20quality-aware%20region%0Aselection%20subnet%20to%20generate%20the%20quality-aware%20maps.%20Concretely%2C%20we%20first%20find%0Athe%20high-quality%20and%20low-quality%20regions%20by%20using%20the%20preliminary%20predictions%2C%0Awhich%20further%20constitute%20the%20pseudo%20label%20that%20can%20be%20used%20to%20train%20this%0Asubnet.%20Finally%2C%20the%20region-guided%20selective%20fusion%20subnet%20purifies%20the%20initial%0Afeatures%20under%20the%20guidance%20of%20the%20quality-aware%20maps%2C%20and%20then%20fuses%20the%0Atriple-modal%20features%20and%20refines%20the%20edge%20details%20of%20prediction%20maps%20through%0Athe%20intra-modality%20and%20inter-modality%20attention%20%28IIA%29%20module%20and%20the%20edge%0Arefinement%20%28ER%29%20module%2C%20respectively.%20Extensive%20experiments%20are%20performed%20on%0AVDT-2048%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07655v1&entry.124074799=Read"},
{"title": "oTTC: Object Time-to-Contact for Motion Estimation in Autonomous Driving", "author": "Abdul Hannan Khan and Syed Tahseen Raza Rizvi and Dheeraj Varma Chittari Macharavtu and Andreas Dengel", "abstract": "  Autonomous driving systems require a quick and robust perception of the\nnearby environment to carry out their routines effectively. With the aim to\navoid collisions and drive safely, autonomous driving systems rely heavily on\nobject detection. However, 2D object detections alone are insufficient; more\ninformation, such as relative velocity and distance, is required for safer\nplanning. Monocular 3D object detectors try to solve this problem by directly\npredicting 3D bounding boxes and object velocities given a camera image. Recent\nresearch estimates time-to-contact in a per-pixel manner and suggests that it\nis more effective measure than velocity and depth combined. However, per-pixel\ntime-to-contact requires object detection to serve its purpose effectively and\nhence increases overall computational requirements as two different models need\nto run. To address this issue, we propose per-object time-to-contact estimation\nby extending object detection models to additionally predict the\ntime-to-contact attribute for each object. We compare our proposed approach\nwith existing time-to-contact methods and provide benchmarking results on\nwell-known datasets. Our proposed approach achieves higher precision compared\nto prior art while using a single image.\n", "link": "http://arxiv.org/abs/2405.07698v1", "date": "2024-05-13", "relevancy": 2.1582, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5498}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5387}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20oTTC%3A%20Object%20Time-to-Contact%20for%20Motion%20Estimation%20in%20Autonomous%20Driving&body=Title%3A%20oTTC%3A%20Object%20Time-to-Contact%20for%20Motion%20Estimation%20in%20Autonomous%20Driving%0AAuthor%3A%20Abdul%20Hannan%20Khan%20and%20Syed%20Tahseen%20Raza%20Rizvi%20and%20Dheeraj%20Varma%20Chittari%20Macharavtu%20and%20Andreas%20Dengel%0AAbstract%3A%20%20%20Autonomous%20driving%20systems%20require%20a%20quick%20and%20robust%20perception%20of%20the%0Anearby%20environment%20to%20carry%20out%20their%20routines%20effectively.%20With%20the%20aim%20to%0Aavoid%20collisions%20and%20drive%20safely%2C%20autonomous%20driving%20systems%20rely%20heavily%20on%0Aobject%20detection.%20However%2C%202D%20object%20detections%20alone%20are%20insufficient%3B%20more%0Ainformation%2C%20such%20as%20relative%20velocity%20and%20distance%2C%20is%20required%20for%20safer%0Aplanning.%20Monocular%203D%20object%20detectors%20try%20to%20solve%20this%20problem%20by%20directly%0Apredicting%203D%20bounding%20boxes%20and%20object%20velocities%20given%20a%20camera%20image.%20Recent%0Aresearch%20estimates%20time-to-contact%20in%20a%20per-pixel%20manner%20and%20suggests%20that%20it%0Ais%20more%20effective%20measure%20than%20velocity%20and%20depth%20combined.%20However%2C%20per-pixel%0Atime-to-contact%20requires%20object%20detection%20to%20serve%20its%20purpose%20effectively%20and%0Ahence%20increases%20overall%20computational%20requirements%20as%20two%20different%20models%20need%0Ato%20run.%20To%20address%20this%20issue%2C%20we%20propose%20per-object%20time-to-contact%20estimation%0Aby%20extending%20object%20detection%20models%20to%20additionally%20predict%20the%0Atime-to-contact%20attribute%20for%20each%20object.%20We%20compare%20our%20proposed%20approach%0Awith%20existing%20time-to-contact%20methods%20and%20provide%20benchmarking%20results%20on%0Awell-known%20datasets.%20Our%20proposed%20approach%20achieves%20higher%20precision%20compared%0Ato%20prior%20art%20while%20using%20a%20single%20image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DoTTC%253A%2520Object%2520Time-to-Contact%2520for%2520Motion%2520Estimation%2520in%2520Autonomous%2520Driving%26entry.906535625%3DAbdul%2520Hannan%2520Khan%2520and%2520Syed%2520Tahseen%2520Raza%2520Rizvi%2520and%2520Dheeraj%2520Varma%2520Chittari%2520Macharavtu%2520and%2520Andreas%2520Dengel%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520systems%2520require%2520a%2520quick%2520and%2520robust%2520perception%2520of%2520the%250Anearby%2520environment%2520to%2520carry%2520out%2520their%2520routines%2520effectively.%2520With%2520the%2520aim%2520to%250Aavoid%2520collisions%2520and%2520drive%2520safely%252C%2520autonomous%2520driving%2520systems%2520rely%2520heavily%2520on%250Aobject%2520detection.%2520However%252C%25202D%2520object%2520detections%2520alone%2520are%2520insufficient%253B%2520more%250Ainformation%252C%2520such%2520as%2520relative%2520velocity%2520and%2520distance%252C%2520is%2520required%2520for%2520safer%250Aplanning.%2520Monocular%25203D%2520object%2520detectors%2520try%2520to%2520solve%2520this%2520problem%2520by%2520directly%250Apredicting%25203D%2520bounding%2520boxes%2520and%2520object%2520velocities%2520given%2520a%2520camera%2520image.%2520Recent%250Aresearch%2520estimates%2520time-to-contact%2520in%2520a%2520per-pixel%2520manner%2520and%2520suggests%2520that%2520it%250Ais%2520more%2520effective%2520measure%2520than%2520velocity%2520and%2520depth%2520combined.%2520However%252C%2520per-pixel%250Atime-to-contact%2520requires%2520object%2520detection%2520to%2520serve%2520its%2520purpose%2520effectively%2520and%250Ahence%2520increases%2520overall%2520computational%2520requirements%2520as%2520two%2520different%2520models%2520need%250Ato%2520run.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520per-object%2520time-to-contact%2520estimation%250Aby%2520extending%2520object%2520detection%2520models%2520to%2520additionally%2520predict%2520the%250Atime-to-contact%2520attribute%2520for%2520each%2520object.%2520We%2520compare%2520our%2520proposed%2520approach%250Awith%2520existing%2520time-to-contact%2520methods%2520and%2520provide%2520benchmarking%2520results%2520on%250Awell-known%2520datasets.%2520Our%2520proposed%2520approach%2520achieves%2520higher%2520precision%2520compared%250Ato%2520prior%2520art%2520while%2520using%2520a%2520single%2520image.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=oTTC%3A%20Object%20Time-to-Contact%20for%20Motion%20Estimation%20in%20Autonomous%20Driving&entry.906535625=Abdul%20Hannan%20Khan%20and%20Syed%20Tahseen%20Raza%20Rizvi%20and%20Dheeraj%20Varma%20Chittari%20Macharavtu%20and%20Andreas%20Dengel&entry.1292438233=%20%20Autonomous%20driving%20systems%20require%20a%20quick%20and%20robust%20perception%20of%20the%0Anearby%20environment%20to%20carry%20out%20their%20routines%20effectively.%20With%20the%20aim%20to%0Aavoid%20collisions%20and%20drive%20safely%2C%20autonomous%20driving%20systems%20rely%20heavily%20on%0Aobject%20detection.%20However%2C%202D%20object%20detections%20alone%20are%20insufficient%3B%20more%0Ainformation%2C%20such%20as%20relative%20velocity%20and%20distance%2C%20is%20required%20for%20safer%0Aplanning.%20Monocular%203D%20object%20detectors%20try%20to%20solve%20this%20problem%20by%20directly%0Apredicting%203D%20bounding%20boxes%20and%20object%20velocities%20given%20a%20camera%20image.%20Recent%0Aresearch%20estimates%20time-to-contact%20in%20a%20per-pixel%20manner%20and%20suggests%20that%20it%0Ais%20more%20effective%20measure%20than%20velocity%20and%20depth%20combined.%20However%2C%20per-pixel%0Atime-to-contact%20requires%20object%20detection%20to%20serve%20its%20purpose%20effectively%20and%0Ahence%20increases%20overall%20computational%20requirements%20as%20two%20different%20models%20need%0Ato%20run.%20To%20address%20this%20issue%2C%20we%20propose%20per-object%20time-to-contact%20estimation%0Aby%20extending%20object%20detection%20models%20to%20additionally%20predict%20the%0Atime-to-contact%20attribute%20for%20each%20object.%20We%20compare%20our%20proposed%20approach%0Awith%20existing%20time-to-contact%20methods%20and%20provide%20benchmarking%20results%20on%0Awell-known%20datasets.%20Our%20proposed%20approach%20achieves%20higher%20precision%20compared%0Ato%20prior%20art%20while%20using%20a%20single%20image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07698v1&entry.124074799=Read"},
{"title": "Fast Training Data Acquisition for Object Detection and Segmentation\n  using Black Screen Luminance Keying", "author": "Thomas P\u00f6llabauer and Volker Knauthe and Andr\u00e9 Boller and Arjan Kuijper and Dieter Fellner", "abstract": "  Deep Neural Networks (DNNs) require large amounts of annotated training data\nfor a good performance. Often this data is generated using manual labeling\n(error-prone and time-consuming) or rendering (requiring geometry and material\ninformation). Both approaches make it difficult or uneconomic to apply them to\nmany small-scale applications. A fast and straightforward approach of acquiring\nthe necessary training data would allow the adoption of deep learning to even\nthe smallest of applications. Chroma keying is the process of replacing a color\n(usually blue or green) with another background. Instead of chroma keying, we\npropose luminance keying for fast and straightforward training image\nacquisition. We deploy a black screen with high light absorption (99.99\\%) to\nrecord roughly 1-minute long videos of our target objects, circumventing\ntypical problems of chroma keying, such as color bleeding or color overlap\nbetween background color and object color. Next we automatically mask our\nobjects using simple brightness thresholding, saving the need for manual\nannotation. Finally, we automatically place the objects on random backgrounds\nand train a 2D object detector. We do extensive evaluation of the performance\non the widely-used YCB-V object set and compare favourably to other\nconventional techniques such as rendering, without needing 3D meshes, materials\nor any other information of our target objects and in a fraction of the time\nneeded for other approaches. Our work demonstrates highly accurate training\ndata acquisition allowing to start training state-of-the-art networks within\nminutes.\n", "link": "http://arxiv.org/abs/2405.07653v1", "date": "2024-05-13", "relevancy": 2.1533, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5623}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5425}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Training%20Data%20Acquisition%20for%20Object%20Detection%20and%20Segmentation%0A%20%20using%20Black%20Screen%20Luminance%20Keying&body=Title%3A%20Fast%20Training%20Data%20Acquisition%20for%20Object%20Detection%20and%20Segmentation%0A%20%20using%20Black%20Screen%20Luminance%20Keying%0AAuthor%3A%20Thomas%20P%C3%B6llabauer%20and%20Volker%20Knauthe%20and%20Andr%C3%A9%20Boller%20and%20Arjan%20Kuijper%20and%20Dieter%20Fellner%0AAbstract%3A%20%20%20Deep%20Neural%20Networks%20%28DNNs%29%20require%20large%20amounts%20of%20annotated%20training%20data%0Afor%20a%20good%20performance.%20Often%20this%20data%20is%20generated%20using%20manual%20labeling%0A%28error-prone%20and%20time-consuming%29%20or%20rendering%20%28requiring%20geometry%20and%20material%0Ainformation%29.%20Both%20approaches%20make%20it%20difficult%20or%20uneconomic%20to%20apply%20them%20to%0Amany%20small-scale%20applications.%20A%20fast%20and%20straightforward%20approach%20of%20acquiring%0Athe%20necessary%20training%20data%20would%20allow%20the%20adoption%20of%20deep%20learning%20to%20even%0Athe%20smallest%20of%20applications.%20Chroma%20keying%20is%20the%20process%20of%20replacing%20a%20color%0A%28usually%20blue%20or%20green%29%20with%20another%20background.%20Instead%20of%20chroma%20keying%2C%20we%0Apropose%20luminance%20keying%20for%20fast%20and%20straightforward%20training%20image%0Aacquisition.%20We%20deploy%20a%20black%20screen%20with%20high%20light%20absorption%20%2899.99%5C%25%29%20to%0Arecord%20roughly%201-minute%20long%20videos%20of%20our%20target%20objects%2C%20circumventing%0Atypical%20problems%20of%20chroma%20keying%2C%20such%20as%20color%20bleeding%20or%20color%20overlap%0Abetween%20background%20color%20and%20object%20color.%20Next%20we%20automatically%20mask%20our%0Aobjects%20using%20simple%20brightness%20thresholding%2C%20saving%20the%20need%20for%20manual%0Aannotation.%20Finally%2C%20we%20automatically%20place%20the%20objects%20on%20random%20backgrounds%0Aand%20train%20a%202D%20object%20detector.%20We%20do%20extensive%20evaluation%20of%20the%20performance%0Aon%20the%20widely-used%20YCB-V%20object%20set%20and%20compare%20favourably%20to%20other%0Aconventional%20techniques%20such%20as%20rendering%2C%20without%20needing%203D%20meshes%2C%20materials%0Aor%20any%20other%20information%20of%20our%20target%20objects%20and%20in%20a%20fraction%20of%20the%20time%0Aneeded%20for%20other%20approaches.%20Our%20work%20demonstrates%20highly%20accurate%20training%0Adata%20acquisition%20allowing%20to%20start%20training%20state-of-the-art%20networks%20within%0Aminutes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Training%2520Data%2520Acquisition%2520for%2520Object%2520Detection%2520and%2520Segmentation%250A%2520%2520using%2520Black%2520Screen%2520Luminance%2520Keying%26entry.906535625%3DThomas%2520P%25C3%25B6llabauer%2520and%2520Volker%2520Knauthe%2520and%2520Andr%25C3%25A9%2520Boller%2520and%2520Arjan%2520Kuijper%2520and%2520Dieter%2520Fellner%26entry.1292438233%3D%2520%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520require%2520large%2520amounts%2520of%2520annotated%2520training%2520data%250Afor%2520a%2520good%2520performance.%2520Often%2520this%2520data%2520is%2520generated%2520using%2520manual%2520labeling%250A%2528error-prone%2520and%2520time-consuming%2529%2520or%2520rendering%2520%2528requiring%2520geometry%2520and%2520material%250Ainformation%2529.%2520Both%2520approaches%2520make%2520it%2520difficult%2520or%2520uneconomic%2520to%2520apply%2520them%2520to%250Amany%2520small-scale%2520applications.%2520A%2520fast%2520and%2520straightforward%2520approach%2520of%2520acquiring%250Athe%2520necessary%2520training%2520data%2520would%2520allow%2520the%2520adoption%2520of%2520deep%2520learning%2520to%2520even%250Athe%2520smallest%2520of%2520applications.%2520Chroma%2520keying%2520is%2520the%2520process%2520of%2520replacing%2520a%2520color%250A%2528usually%2520blue%2520or%2520green%2529%2520with%2520another%2520background.%2520Instead%2520of%2520chroma%2520keying%252C%2520we%250Apropose%2520luminance%2520keying%2520for%2520fast%2520and%2520straightforward%2520training%2520image%250Aacquisition.%2520We%2520deploy%2520a%2520black%2520screen%2520with%2520high%2520light%2520absorption%2520%252899.99%255C%2525%2529%2520to%250Arecord%2520roughly%25201-minute%2520long%2520videos%2520of%2520our%2520target%2520objects%252C%2520circumventing%250Atypical%2520problems%2520of%2520chroma%2520keying%252C%2520such%2520as%2520color%2520bleeding%2520or%2520color%2520overlap%250Abetween%2520background%2520color%2520and%2520object%2520color.%2520Next%2520we%2520automatically%2520mask%2520our%250Aobjects%2520using%2520simple%2520brightness%2520thresholding%252C%2520saving%2520the%2520need%2520for%2520manual%250Aannotation.%2520Finally%252C%2520we%2520automatically%2520place%2520the%2520objects%2520on%2520random%2520backgrounds%250Aand%2520train%2520a%25202D%2520object%2520detector.%2520We%2520do%2520extensive%2520evaluation%2520of%2520the%2520performance%250Aon%2520the%2520widely-used%2520YCB-V%2520object%2520set%2520and%2520compare%2520favourably%2520to%2520other%250Aconventional%2520techniques%2520such%2520as%2520rendering%252C%2520without%2520needing%25203D%2520meshes%252C%2520materials%250Aor%2520any%2520other%2520information%2520of%2520our%2520target%2520objects%2520and%2520in%2520a%2520fraction%2520of%2520the%2520time%250Aneeded%2520for%2520other%2520approaches.%2520Our%2520work%2520demonstrates%2520highly%2520accurate%2520training%250Adata%2520acquisition%2520allowing%2520to%2520start%2520training%2520state-of-the-art%2520networks%2520within%250Aminutes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Training%20Data%20Acquisition%20for%20Object%20Detection%20and%20Segmentation%0A%20%20using%20Black%20Screen%20Luminance%20Keying&entry.906535625=Thomas%20P%C3%B6llabauer%20and%20Volker%20Knauthe%20and%20Andr%C3%A9%20Boller%20and%20Arjan%20Kuijper%20and%20Dieter%20Fellner&entry.1292438233=%20%20Deep%20Neural%20Networks%20%28DNNs%29%20require%20large%20amounts%20of%20annotated%20training%20data%0Afor%20a%20good%20performance.%20Often%20this%20data%20is%20generated%20using%20manual%20labeling%0A%28error-prone%20and%20time-consuming%29%20or%20rendering%20%28requiring%20geometry%20and%20material%0Ainformation%29.%20Both%20approaches%20make%20it%20difficult%20or%20uneconomic%20to%20apply%20them%20to%0Amany%20small-scale%20applications.%20A%20fast%20and%20straightforward%20approach%20of%20acquiring%0Athe%20necessary%20training%20data%20would%20allow%20the%20adoption%20of%20deep%20learning%20to%20even%0Athe%20smallest%20of%20applications.%20Chroma%20keying%20is%20the%20process%20of%20replacing%20a%20color%0A%28usually%20blue%20or%20green%29%20with%20another%20background.%20Instead%20of%20chroma%20keying%2C%20we%0Apropose%20luminance%20keying%20for%20fast%20and%20straightforward%20training%20image%0Aacquisition.%20We%20deploy%20a%20black%20screen%20with%20high%20light%20absorption%20%2899.99%5C%25%29%20to%0Arecord%20roughly%201-minute%20long%20videos%20of%20our%20target%20objects%2C%20circumventing%0Atypical%20problems%20of%20chroma%20keying%2C%20such%20as%20color%20bleeding%20or%20color%20overlap%0Abetween%20background%20color%20and%20object%20color.%20Next%20we%20automatically%20mask%20our%0Aobjects%20using%20simple%20brightness%20thresholding%2C%20saving%20the%20need%20for%20manual%0Aannotation.%20Finally%2C%20we%20automatically%20place%20the%20objects%20on%20random%20backgrounds%0Aand%20train%20a%202D%20object%20detector.%20We%20do%20extensive%20evaluation%20of%20the%20performance%0Aon%20the%20widely-used%20YCB-V%20object%20set%20and%20compare%20favourably%20to%20other%0Aconventional%20techniques%20such%20as%20rendering%2C%20without%20needing%203D%20meshes%2C%20materials%0Aor%20any%20other%20information%20of%20our%20target%20objects%20and%20in%20a%20fraction%20of%20the%20time%0Aneeded%20for%20other%20approaches.%20Our%20work%20demonstrates%20highly%20accurate%20training%0Adata%20acquisition%20allowing%20to%20start%20training%20state-of-the-art%20networks%20within%0Aminutes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07653v1&entry.124074799=Read"},
{"title": "A method for supervoxel-wise association studies of age and other\n  non-imaging variables from coronary computed tomography angiograms", "author": "Johan \u00d6fverstedt and Elin Lundstr\u00f6m and G\u00f6ran Bergstr\u00f6m and Joel Kullberg and H\u00e5kan Ahlstr\u00f6m", "abstract": "  The study of associations between an individual's age and imaging and\nnon-imaging data is an active research area that attempts to aid understanding\nof the effects and patterns of aging. In this work we have conducted a\nsupervoxel-wise association study between both volumetric and tissue density\nfeatures in coronary computed tomography angiograms and the chronological age\nof a subject, to understand the localized changes in morphology and tissue\ndensity with age. To enable a supervoxel-wise study of volume and tissue\ndensity, we developed a novel method based on image segmentation, inter-subject\nimage registration, and robust supervoxel-based correlation analysis, to\nachieve a statistical association study between the images and age. We evaluate\nthe registration methodology in terms of the Dice coefficient for the heart\nchambers and myocardium, and the inverse consistency of the transformations,\nshowing that the method works well in most cases with high overlap and inverse\nconsistency. In a sex-stratified study conducted on a subset of $n=1388$ images\nfrom the SCAPIS study, the supervoxel-wise analysis was able to find localized\nassociations with age outside of the commonly segmented and analyzed\nsub-regions, and several substantial differences between the sexes in\nassociation of age and volume.\n", "link": "http://arxiv.org/abs/2405.07762v1", "date": "2024-05-13", "relevancy": 2.1482, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.441}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4312}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20method%20for%20supervoxel-wise%20association%20studies%20of%20age%20and%20other%0A%20%20non-imaging%20variables%20from%20coronary%20computed%20tomography%20angiograms&body=Title%3A%20A%20method%20for%20supervoxel-wise%20association%20studies%20of%20age%20and%20other%0A%20%20non-imaging%20variables%20from%20coronary%20computed%20tomography%20angiograms%0AAuthor%3A%20Johan%20%C3%96fverstedt%20and%20Elin%20Lundstr%C3%B6m%20and%20G%C3%B6ran%20Bergstr%C3%B6m%20and%20Joel%20Kullberg%20and%20H%C3%A5kan%20Ahlstr%C3%B6m%0AAbstract%3A%20%20%20The%20study%20of%20associations%20between%20an%20individual%27s%20age%20and%20imaging%20and%0Anon-imaging%20data%20is%20an%20active%20research%20area%20that%20attempts%20to%20aid%20understanding%0Aof%20the%20effects%20and%20patterns%20of%20aging.%20In%20this%20work%20we%20have%20conducted%20a%0Asupervoxel-wise%20association%20study%20between%20both%20volumetric%20and%20tissue%20density%0Afeatures%20in%20coronary%20computed%20tomography%20angiograms%20and%20the%20chronological%20age%0Aof%20a%20subject%2C%20to%20understand%20the%20localized%20changes%20in%20morphology%20and%20tissue%0Adensity%20with%20age.%20To%20enable%20a%20supervoxel-wise%20study%20of%20volume%20and%20tissue%0Adensity%2C%20we%20developed%20a%20novel%20method%20based%20on%20image%20segmentation%2C%20inter-subject%0Aimage%20registration%2C%20and%20robust%20supervoxel-based%20correlation%20analysis%2C%20to%0Aachieve%20a%20statistical%20association%20study%20between%20the%20images%20and%20age.%20We%20evaluate%0Athe%20registration%20methodology%20in%20terms%20of%20the%20Dice%20coefficient%20for%20the%20heart%0Achambers%20and%20myocardium%2C%20and%20the%20inverse%20consistency%20of%20the%20transformations%2C%0Ashowing%20that%20the%20method%20works%20well%20in%20most%20cases%20with%20high%20overlap%20and%20inverse%0Aconsistency.%20In%20a%20sex-stratified%20study%20conducted%20on%20a%20subset%20of%20%24n%3D1388%24%20images%0Afrom%20the%20SCAPIS%20study%2C%20the%20supervoxel-wise%20analysis%20was%20able%20to%20find%20localized%0Aassociations%20with%20age%20outside%20of%20the%20commonly%20segmented%20and%20analyzed%0Asub-regions%2C%20and%20several%20substantial%20differences%20between%20the%20sexes%20in%0Aassociation%20of%20age%20and%20volume.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07762v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520method%2520for%2520supervoxel-wise%2520association%2520studies%2520of%2520age%2520and%2520other%250A%2520%2520non-imaging%2520variables%2520from%2520coronary%2520computed%2520tomography%2520angiograms%26entry.906535625%3DJohan%2520%25C3%2596fverstedt%2520and%2520Elin%2520Lundstr%25C3%25B6m%2520and%2520G%25C3%25B6ran%2520Bergstr%25C3%25B6m%2520and%2520Joel%2520Kullberg%2520and%2520H%25C3%25A5kan%2520Ahlstr%25C3%25B6m%26entry.1292438233%3D%2520%2520The%2520study%2520of%2520associations%2520between%2520an%2520individual%2527s%2520age%2520and%2520imaging%2520and%250Anon-imaging%2520data%2520is%2520an%2520active%2520research%2520area%2520that%2520attempts%2520to%2520aid%2520understanding%250Aof%2520the%2520effects%2520and%2520patterns%2520of%2520aging.%2520In%2520this%2520work%2520we%2520have%2520conducted%2520a%250Asupervoxel-wise%2520association%2520study%2520between%2520both%2520volumetric%2520and%2520tissue%2520density%250Afeatures%2520in%2520coronary%2520computed%2520tomography%2520angiograms%2520and%2520the%2520chronological%2520age%250Aof%2520a%2520subject%252C%2520to%2520understand%2520the%2520localized%2520changes%2520in%2520morphology%2520and%2520tissue%250Adensity%2520with%2520age.%2520To%2520enable%2520a%2520supervoxel-wise%2520study%2520of%2520volume%2520and%2520tissue%250Adensity%252C%2520we%2520developed%2520a%2520novel%2520method%2520based%2520on%2520image%2520segmentation%252C%2520inter-subject%250Aimage%2520registration%252C%2520and%2520robust%2520supervoxel-based%2520correlation%2520analysis%252C%2520to%250Aachieve%2520a%2520statistical%2520association%2520study%2520between%2520the%2520images%2520and%2520age.%2520We%2520evaluate%250Athe%2520registration%2520methodology%2520in%2520terms%2520of%2520the%2520Dice%2520coefficient%2520for%2520the%2520heart%250Achambers%2520and%2520myocardium%252C%2520and%2520the%2520inverse%2520consistency%2520of%2520the%2520transformations%252C%250Ashowing%2520that%2520the%2520method%2520works%2520well%2520in%2520most%2520cases%2520with%2520high%2520overlap%2520and%2520inverse%250Aconsistency.%2520In%2520a%2520sex-stratified%2520study%2520conducted%2520on%2520a%2520subset%2520of%2520%2524n%253D1388%2524%2520images%250Afrom%2520the%2520SCAPIS%2520study%252C%2520the%2520supervoxel-wise%2520analysis%2520was%2520able%2520to%2520find%2520localized%250Aassociations%2520with%2520age%2520outside%2520of%2520the%2520commonly%2520segmented%2520and%2520analyzed%250Asub-regions%252C%2520and%2520several%2520substantial%2520differences%2520between%2520the%2520sexes%2520in%250Aassociation%2520of%2520age%2520and%2520volume.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07762v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20method%20for%20supervoxel-wise%20association%20studies%20of%20age%20and%20other%0A%20%20non-imaging%20variables%20from%20coronary%20computed%20tomography%20angiograms&entry.906535625=Johan%20%C3%96fverstedt%20and%20Elin%20Lundstr%C3%B6m%20and%20G%C3%B6ran%20Bergstr%C3%B6m%20and%20Joel%20Kullberg%20and%20H%C3%A5kan%20Ahlstr%C3%B6m&entry.1292438233=%20%20The%20study%20of%20associations%20between%20an%20individual%27s%20age%20and%20imaging%20and%0Anon-imaging%20data%20is%20an%20active%20research%20area%20that%20attempts%20to%20aid%20understanding%0Aof%20the%20effects%20and%20patterns%20of%20aging.%20In%20this%20work%20we%20have%20conducted%20a%0Asupervoxel-wise%20association%20study%20between%20both%20volumetric%20and%20tissue%20density%0Afeatures%20in%20coronary%20computed%20tomography%20angiograms%20and%20the%20chronological%20age%0Aof%20a%20subject%2C%20to%20understand%20the%20localized%20changes%20in%20morphology%20and%20tissue%0Adensity%20with%20age.%20To%20enable%20a%20supervoxel-wise%20study%20of%20volume%20and%20tissue%0Adensity%2C%20we%20developed%20a%20novel%20method%20based%20on%20image%20segmentation%2C%20inter-subject%0Aimage%20registration%2C%20and%20robust%20supervoxel-based%20correlation%20analysis%2C%20to%0Aachieve%20a%20statistical%20association%20study%20between%20the%20images%20and%20age.%20We%20evaluate%0Athe%20registration%20methodology%20in%20terms%20of%20the%20Dice%20coefficient%20for%20the%20heart%0Achambers%20and%20myocardium%2C%20and%20the%20inverse%20consistency%20of%20the%20transformations%2C%0Ashowing%20that%20the%20method%20works%20well%20in%20most%20cases%20with%20high%20overlap%20and%20inverse%0Aconsistency.%20In%20a%20sex-stratified%20study%20conducted%20on%20a%20subset%20of%20%24n%3D1388%24%20images%0Afrom%20the%20SCAPIS%20study%2C%20the%20supervoxel-wise%20analysis%20was%20able%20to%20find%20localized%0Aassociations%20with%20age%20outside%20of%20the%20commonly%20segmented%20and%20analyzed%0Asub-regions%2C%20and%20several%20substantial%20differences%20between%20the%20sexes%20in%0Aassociation%20of%20age%20and%20volume.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07762v1&entry.124074799=Read"},
{"title": "Harnessing Hierarchical Label Distribution Variations in Test Agnostic\n  Long-tail Recognition", "author": "Zhiyong Yang and Qianqian Xu and Zitai Wang and Sicong Li and Boyu Han and Shilong Bao and Xiaochun Cao and Qingming Huang", "abstract": "  This paper explores test-agnostic long-tail recognition, a challenging\nlong-tail task where the test label distributions are unknown and arbitrarily\nimbalanced. We argue that the variation in these distributions can be broken\ndown hierarchically into global and local levels. The global ones reflect a\nbroad range of diversity, while the local ones typically arise from milder\nchanges, often focused on a particular neighbor. Traditional methods\npredominantly use a Mixture-of-Expert (MoE) approach, targeting a few fixed\ntest label distributions that exhibit substantial global variations. However,\nthe local variations are left unconsidered. To address this issue, we propose a\nnew MoE strategy, $\\mathsf{DirMixE}$, which assigns experts to different\nDirichlet meta-distributions of the label distribution, each targeting a\nspecific aspect of local variations. Additionally, the diversity among these\nDirichlet meta-distributions inherently captures global variations. This\ndual-level approach also leads to a more stable objective function, allowing us\nto sample different test distributions better to quantify the mean and variance\nof performance outcomes. Theoretically, we show that our proposed objective\nbenefits from enhanced generalization by virtue of the variance-based\nregularization. Comprehensive experiments across multiple benchmarks confirm\nthe effectiveness of $\\mathsf{DirMixE}$. The code is available at\n\\url{https://github.com/scongl/DirMixE}.\n", "link": "http://arxiv.org/abs/2405.07780v1", "date": "2024-05-13", "relevancy": 2.1412, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5928}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5185}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Hierarchical%20Label%20Distribution%20Variations%20in%20Test%20Agnostic%0A%20%20Long-tail%20Recognition&body=Title%3A%20Harnessing%20Hierarchical%20Label%20Distribution%20Variations%20in%20Test%20Agnostic%0A%20%20Long-tail%20Recognition%0AAuthor%3A%20Zhiyong%20Yang%20and%20Qianqian%20Xu%20and%20Zitai%20Wang%20and%20Sicong%20Li%20and%20Boyu%20Han%20and%20Shilong%20Bao%20and%20Xiaochun%20Cao%20and%20Qingming%20Huang%0AAbstract%3A%20%20%20This%20paper%20explores%20test-agnostic%20long-tail%20recognition%2C%20a%20challenging%0Along-tail%20task%20where%20the%20test%20label%20distributions%20are%20unknown%20and%20arbitrarily%0Aimbalanced.%20We%20argue%20that%20the%20variation%20in%20these%20distributions%20can%20be%20broken%0Adown%20hierarchically%20into%20global%20and%20local%20levels.%20The%20global%20ones%20reflect%20a%0Abroad%20range%20of%20diversity%2C%20while%20the%20local%20ones%20typically%20arise%20from%20milder%0Achanges%2C%20often%20focused%20on%20a%20particular%20neighbor.%20Traditional%20methods%0Apredominantly%20use%20a%20Mixture-of-Expert%20%28MoE%29%20approach%2C%20targeting%20a%20few%20fixed%0Atest%20label%20distributions%20that%20exhibit%20substantial%20global%20variations.%20However%2C%0Athe%20local%20variations%20are%20left%20unconsidered.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Anew%20MoE%20strategy%2C%20%24%5Cmathsf%7BDirMixE%7D%24%2C%20which%20assigns%20experts%20to%20different%0ADirichlet%20meta-distributions%20of%20the%20label%20distribution%2C%20each%20targeting%20a%0Aspecific%20aspect%20of%20local%20variations.%20Additionally%2C%20the%20diversity%20among%20these%0ADirichlet%20meta-distributions%20inherently%20captures%20global%20variations.%20This%0Adual-level%20approach%20also%20leads%20to%20a%20more%20stable%20objective%20function%2C%20allowing%20us%0Ato%20sample%20different%20test%20distributions%20better%20to%20quantify%20the%20mean%20and%20variance%0Aof%20performance%20outcomes.%20Theoretically%2C%20we%20show%20that%20our%20proposed%20objective%0Abenefits%20from%20enhanced%20generalization%20by%20virtue%20of%20the%20variance-based%0Aregularization.%20Comprehensive%20experiments%20across%20multiple%20benchmarks%20confirm%0Athe%20effectiveness%20of%20%24%5Cmathsf%7BDirMixE%7D%24.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/scongl/DirMixE%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07780v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Hierarchical%2520Label%2520Distribution%2520Variations%2520in%2520Test%2520Agnostic%250A%2520%2520Long-tail%2520Recognition%26entry.906535625%3DZhiyong%2520Yang%2520and%2520Qianqian%2520Xu%2520and%2520Zitai%2520Wang%2520and%2520Sicong%2520Li%2520and%2520Boyu%2520Han%2520and%2520Shilong%2520Bao%2520and%2520Xiaochun%2520Cao%2520and%2520Qingming%2520Huang%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520test-agnostic%2520long-tail%2520recognition%252C%2520a%2520challenging%250Along-tail%2520task%2520where%2520the%2520test%2520label%2520distributions%2520are%2520unknown%2520and%2520arbitrarily%250Aimbalanced.%2520We%2520argue%2520that%2520the%2520variation%2520in%2520these%2520distributions%2520can%2520be%2520broken%250Adown%2520hierarchically%2520into%2520global%2520and%2520local%2520levels.%2520The%2520global%2520ones%2520reflect%2520a%250Abroad%2520range%2520of%2520diversity%252C%2520while%2520the%2520local%2520ones%2520typically%2520arise%2520from%2520milder%250Achanges%252C%2520often%2520focused%2520on%2520a%2520particular%2520neighbor.%2520Traditional%2520methods%250Apredominantly%2520use%2520a%2520Mixture-of-Expert%2520%2528MoE%2529%2520approach%252C%2520targeting%2520a%2520few%2520fixed%250Atest%2520label%2520distributions%2520that%2520exhibit%2520substantial%2520global%2520variations.%2520However%252C%250Athe%2520local%2520variations%2520are%2520left%2520unconsidered.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%250Anew%2520MoE%2520strategy%252C%2520%2524%255Cmathsf%257BDirMixE%257D%2524%252C%2520which%2520assigns%2520experts%2520to%2520different%250ADirichlet%2520meta-distributions%2520of%2520the%2520label%2520distribution%252C%2520each%2520targeting%2520a%250Aspecific%2520aspect%2520of%2520local%2520variations.%2520Additionally%252C%2520the%2520diversity%2520among%2520these%250ADirichlet%2520meta-distributions%2520inherently%2520captures%2520global%2520variations.%2520This%250Adual-level%2520approach%2520also%2520leads%2520to%2520a%2520more%2520stable%2520objective%2520function%252C%2520allowing%2520us%250Ato%2520sample%2520different%2520test%2520distributions%2520better%2520to%2520quantify%2520the%2520mean%2520and%2520variance%250Aof%2520performance%2520outcomes.%2520Theoretically%252C%2520we%2520show%2520that%2520our%2520proposed%2520objective%250Abenefits%2520from%2520enhanced%2520generalization%2520by%2520virtue%2520of%2520the%2520variance-based%250Aregularization.%2520Comprehensive%2520experiments%2520across%2520multiple%2520benchmarks%2520confirm%250Athe%2520effectiveness%2520of%2520%2524%255Cmathsf%257BDirMixE%257D%2524.%2520The%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/scongl/DirMixE%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07780v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Hierarchical%20Label%20Distribution%20Variations%20in%20Test%20Agnostic%0A%20%20Long-tail%20Recognition&entry.906535625=Zhiyong%20Yang%20and%20Qianqian%20Xu%20and%20Zitai%20Wang%20and%20Sicong%20Li%20and%20Boyu%20Han%20and%20Shilong%20Bao%20and%20Xiaochun%20Cao%20and%20Qingming%20Huang&entry.1292438233=%20%20This%20paper%20explores%20test-agnostic%20long-tail%20recognition%2C%20a%20challenging%0Along-tail%20task%20where%20the%20test%20label%20distributions%20are%20unknown%20and%20arbitrarily%0Aimbalanced.%20We%20argue%20that%20the%20variation%20in%20these%20distributions%20can%20be%20broken%0Adown%20hierarchically%20into%20global%20and%20local%20levels.%20The%20global%20ones%20reflect%20a%0Abroad%20range%20of%20diversity%2C%20while%20the%20local%20ones%20typically%20arise%20from%20milder%0Achanges%2C%20often%20focused%20on%20a%20particular%20neighbor.%20Traditional%20methods%0Apredominantly%20use%20a%20Mixture-of-Expert%20%28MoE%29%20approach%2C%20targeting%20a%20few%20fixed%0Atest%20label%20distributions%20that%20exhibit%20substantial%20global%20variations.%20However%2C%0Athe%20local%20variations%20are%20left%20unconsidered.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Anew%20MoE%20strategy%2C%20%24%5Cmathsf%7BDirMixE%7D%24%2C%20which%20assigns%20experts%20to%20different%0ADirichlet%20meta-distributions%20of%20the%20label%20distribution%2C%20each%20targeting%20a%0Aspecific%20aspect%20of%20local%20variations.%20Additionally%2C%20the%20diversity%20among%20these%0ADirichlet%20meta-distributions%20inherently%20captures%20global%20variations.%20This%0Adual-level%20approach%20also%20leads%20to%20a%20more%20stable%20objective%20function%2C%20allowing%20us%0Ato%20sample%20different%20test%20distributions%20better%20to%20quantify%20the%20mean%20and%20variance%0Aof%20performance%20outcomes.%20Theoretically%2C%20we%20show%20that%20our%20proposed%20objective%0Abenefits%20from%20enhanced%20generalization%20by%20virtue%20of%20the%20variance-based%0Aregularization.%20Comprehensive%20experiments%20across%20multiple%20benchmarks%20confirm%0Athe%20effectiveness%20of%20%24%5Cmathsf%7BDirMixE%7D%24.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/scongl/DirMixE%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07780v1&entry.124074799=Read"},
{"title": "Constrained Exploration via Reflected Replica Exchange Stochastic\n  Gradient Langevin Dynamics", "author": "Haoyang Zheng and Hengrong Du and Qi Feng and Wei Deng and Guang Lin", "abstract": "  Replica exchange stochastic gradient Langevin dynamics (reSGLD) is an\neffective sampler for non-convex learning in large-scale datasets. However, the\nsimulation may encounter stagnation issues when the high-temperature chain\ndelves too deeply into the distribution tails. To tackle this issue, we propose\nreflected reSGLD (r2SGLD): an algorithm tailored for constrained non-convex\nexploration by utilizing reflection steps within a bounded domain.\nTheoretically, we observe that reducing the diameter of the domain enhances\nmixing rates, exhibiting a \\emph{quadratic} behavior. Empirically, we test its\nperformance through extensive experiments, including identifying dynamical\nsystems with physical constraints, simulations of constrained multi-modal\ndistributions, and image classification tasks. The theoretical and empirical\nfindings highlight the crucial role of constrained exploration in improving the\nsimulation efficiency.\n", "link": "http://arxiv.org/abs/2405.07839v1", "date": "2024-05-13", "relevancy": 2.1303, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5563}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5169}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constrained%20Exploration%20via%20Reflected%20Replica%20Exchange%20Stochastic%0A%20%20Gradient%20Langevin%20Dynamics&body=Title%3A%20Constrained%20Exploration%20via%20Reflected%20Replica%20Exchange%20Stochastic%0A%20%20Gradient%20Langevin%20Dynamics%0AAuthor%3A%20Haoyang%20Zheng%20and%20Hengrong%20Du%20and%20Qi%20Feng%20and%20Wei%20Deng%20and%20Guang%20Lin%0AAbstract%3A%20%20%20Replica%20exchange%20stochastic%20gradient%20Langevin%20dynamics%20%28reSGLD%29%20is%20an%0Aeffective%20sampler%20for%20non-convex%20learning%20in%20large-scale%20datasets.%20However%2C%20the%0Asimulation%20may%20encounter%20stagnation%20issues%20when%20the%20high-temperature%20chain%0Adelves%20too%20deeply%20into%20the%20distribution%20tails.%20To%20tackle%20this%20issue%2C%20we%20propose%0Areflected%20reSGLD%20%28r2SGLD%29%3A%20an%20algorithm%20tailored%20for%20constrained%20non-convex%0Aexploration%20by%20utilizing%20reflection%20steps%20within%20a%20bounded%20domain.%0ATheoretically%2C%20we%20observe%20that%20reducing%20the%20diameter%20of%20the%20domain%20enhances%0Amixing%20rates%2C%20exhibiting%20a%20%5Cemph%7Bquadratic%7D%20behavior.%20Empirically%2C%20we%20test%20its%0Aperformance%20through%20extensive%20experiments%2C%20including%20identifying%20dynamical%0Asystems%20with%20physical%20constraints%2C%20simulations%20of%20constrained%20multi-modal%0Adistributions%2C%20and%20image%20classification%20tasks.%20The%20theoretical%20and%20empirical%0Afindings%20highlight%20the%20crucial%20role%20of%20constrained%20exploration%20in%20improving%20the%0Asimulation%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07839v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstrained%2520Exploration%2520via%2520Reflected%2520Replica%2520Exchange%2520Stochastic%250A%2520%2520Gradient%2520Langevin%2520Dynamics%26entry.906535625%3DHaoyang%2520Zheng%2520and%2520Hengrong%2520Du%2520and%2520Qi%2520Feng%2520and%2520Wei%2520Deng%2520and%2520Guang%2520Lin%26entry.1292438233%3D%2520%2520Replica%2520exchange%2520stochastic%2520gradient%2520Langevin%2520dynamics%2520%2528reSGLD%2529%2520is%2520an%250Aeffective%2520sampler%2520for%2520non-convex%2520learning%2520in%2520large-scale%2520datasets.%2520However%252C%2520the%250Asimulation%2520may%2520encounter%2520stagnation%2520issues%2520when%2520the%2520high-temperature%2520chain%250Adelves%2520too%2520deeply%2520into%2520the%2520distribution%2520tails.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520propose%250Areflected%2520reSGLD%2520%2528r2SGLD%2529%253A%2520an%2520algorithm%2520tailored%2520for%2520constrained%2520non-convex%250Aexploration%2520by%2520utilizing%2520reflection%2520steps%2520within%2520a%2520bounded%2520domain.%250ATheoretically%252C%2520we%2520observe%2520that%2520reducing%2520the%2520diameter%2520of%2520the%2520domain%2520enhances%250Amixing%2520rates%252C%2520exhibiting%2520a%2520%255Cemph%257Bquadratic%257D%2520behavior.%2520Empirically%252C%2520we%2520test%2520its%250Aperformance%2520through%2520extensive%2520experiments%252C%2520including%2520identifying%2520dynamical%250Asystems%2520with%2520physical%2520constraints%252C%2520simulations%2520of%2520constrained%2520multi-modal%250Adistributions%252C%2520and%2520image%2520classification%2520tasks.%2520The%2520theoretical%2520and%2520empirical%250Afindings%2520highlight%2520the%2520crucial%2520role%2520of%2520constrained%2520exploration%2520in%2520improving%2520the%250Asimulation%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07839v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constrained%20Exploration%20via%20Reflected%20Replica%20Exchange%20Stochastic%0A%20%20Gradient%20Langevin%20Dynamics&entry.906535625=Haoyang%20Zheng%20and%20Hengrong%20Du%20and%20Qi%20Feng%20and%20Wei%20Deng%20and%20Guang%20Lin&entry.1292438233=%20%20Replica%20exchange%20stochastic%20gradient%20Langevin%20dynamics%20%28reSGLD%29%20is%20an%0Aeffective%20sampler%20for%20non-convex%20learning%20in%20large-scale%20datasets.%20However%2C%20the%0Asimulation%20may%20encounter%20stagnation%20issues%20when%20the%20high-temperature%20chain%0Adelves%20too%20deeply%20into%20the%20distribution%20tails.%20To%20tackle%20this%20issue%2C%20we%20propose%0Areflected%20reSGLD%20%28r2SGLD%29%3A%20an%20algorithm%20tailored%20for%20constrained%20non-convex%0Aexploration%20by%20utilizing%20reflection%20steps%20within%20a%20bounded%20domain.%0ATheoretically%2C%20we%20observe%20that%20reducing%20the%20diameter%20of%20the%20domain%20enhances%0Amixing%20rates%2C%20exhibiting%20a%20%5Cemph%7Bquadratic%7D%20behavior.%20Empirically%2C%20we%20test%20its%0Aperformance%20through%20extensive%20experiments%2C%20including%20identifying%20dynamical%0Asystems%20with%20physical%20constraints%2C%20simulations%20of%20constrained%20multi-modal%0Adistributions%2C%20and%20image%20classification%20tasks.%20The%20theoretical%20and%20empirical%0Afindings%20highlight%20the%20crucial%20role%20of%20constrained%20exploration%20in%20improving%20the%0Asimulation%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07839v1&entry.124074799=Read"},
{"title": "How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English\n  ChatGPT Responses", "author": "Stefanie Urchs and Veronika Thurner and Matthias A\u00dfenmacher and Christian Heumann and Stephanie Thiemichen", "abstract": "  With the introduction of ChatGPT, OpenAI made large language models (LLM)\naccessible to users with limited IT expertise. However, users with no\nbackground in natural language processing (NLP) might lack a proper\nunderstanding of LLMs. Thus the awareness of their inherent limitations, and\ntherefore will take the systems' output at face value. In this paper, we\nsystematically analyse prompts and the generated responses to identify possible\nproblematic issues with a special focus on gender biases, which users need to\nbe aware of when processing the system's output. We explore how ChatGPT reacts\nin English and German if prompted to answer from a female, male, or neutral\nperspective. In an in-depth investigation, we examine selected prompts and\nanalyse to what extent responses differ if the system is prompted several times\nin an identical way. On this basis, we show that ChatGPT is indeed useful for\nhelping non-IT users draft texts for their daily work. However, it is\nabsolutely crucial to thoroughly check the system's responses for biases as\nwell as for syntactic and grammatical mistakes.\n", "link": "http://arxiv.org/abs/2310.03031v3", "date": "2024-05-13", "relevancy": 2.1284, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4837}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4117}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Prevalent%20is%20Gender%20Bias%20in%20ChatGPT%3F%20--%20Exploring%20German%20and%20English%0A%20%20ChatGPT%20Responses&body=Title%3A%20How%20Prevalent%20is%20Gender%20Bias%20in%20ChatGPT%3F%20--%20Exploring%20German%20and%20English%0A%20%20ChatGPT%20Responses%0AAuthor%3A%20Stefanie%20Urchs%20and%20Veronika%20Thurner%20and%20Matthias%20A%C3%9Fenmacher%20and%20Christian%20Heumann%20and%20Stephanie%20Thiemichen%0AAbstract%3A%20%20%20With%20the%20introduction%20of%20ChatGPT%2C%20OpenAI%20made%20large%20language%20models%20%28LLM%29%0Aaccessible%20to%20users%20with%20limited%20IT%20expertise.%20However%2C%20users%20with%20no%0Abackground%20in%20natural%20language%20processing%20%28NLP%29%20might%20lack%20a%20proper%0Aunderstanding%20of%20LLMs.%20Thus%20the%20awareness%20of%20their%20inherent%20limitations%2C%20and%0Atherefore%20will%20take%20the%20systems%27%20output%20at%20face%20value.%20In%20this%20paper%2C%20we%0Asystematically%20analyse%20prompts%20and%20the%20generated%20responses%20to%20identify%20possible%0Aproblematic%20issues%20with%20a%20special%20focus%20on%20gender%20biases%2C%20which%20users%20need%20to%0Abe%20aware%20of%20when%20processing%20the%20system%27s%20output.%20We%20explore%20how%20ChatGPT%20reacts%0Ain%20English%20and%20German%20if%20prompted%20to%20answer%20from%20a%20female%2C%20male%2C%20or%20neutral%0Aperspective.%20In%20an%20in-depth%20investigation%2C%20we%20examine%20selected%20prompts%20and%0Aanalyse%20to%20what%20extent%20responses%20differ%20if%20the%20system%20is%20prompted%20several%20times%0Ain%20an%20identical%20way.%20On%20this%20basis%2C%20we%20show%20that%20ChatGPT%20is%20indeed%20useful%20for%0Ahelping%20non-IT%20users%20draft%20texts%20for%20their%20daily%20work.%20However%2C%20it%20is%0Aabsolutely%20crucial%20to%20thoroughly%20check%20the%20system%27s%20responses%20for%20biases%20as%0Awell%20as%20for%20syntactic%20and%20grammatical%20mistakes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03031v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Prevalent%2520is%2520Gender%2520Bias%2520in%2520ChatGPT%253F%2520--%2520Exploring%2520German%2520and%2520English%250A%2520%2520ChatGPT%2520Responses%26entry.906535625%3DStefanie%2520Urchs%2520and%2520Veronika%2520Thurner%2520and%2520Matthias%2520A%25C3%259Fenmacher%2520and%2520Christian%2520Heumann%2520and%2520Stephanie%2520Thiemichen%26entry.1292438233%3D%2520%2520With%2520the%2520introduction%2520of%2520ChatGPT%252C%2520OpenAI%2520made%2520large%2520language%2520models%2520%2528LLM%2529%250Aaccessible%2520to%2520users%2520with%2520limited%2520IT%2520expertise.%2520However%252C%2520users%2520with%2520no%250Abackground%2520in%2520natural%2520language%2520processing%2520%2528NLP%2529%2520might%2520lack%2520a%2520proper%250Aunderstanding%2520of%2520LLMs.%2520Thus%2520the%2520awareness%2520of%2520their%2520inherent%2520limitations%252C%2520and%250Atherefore%2520will%2520take%2520the%2520systems%2527%2520output%2520at%2520face%2520value.%2520In%2520this%2520paper%252C%2520we%250Asystematically%2520analyse%2520prompts%2520and%2520the%2520generated%2520responses%2520to%2520identify%2520possible%250Aproblematic%2520issues%2520with%2520a%2520special%2520focus%2520on%2520gender%2520biases%252C%2520which%2520users%2520need%2520to%250Abe%2520aware%2520of%2520when%2520processing%2520the%2520system%2527s%2520output.%2520We%2520explore%2520how%2520ChatGPT%2520reacts%250Ain%2520English%2520and%2520German%2520if%2520prompted%2520to%2520answer%2520from%2520a%2520female%252C%2520male%252C%2520or%2520neutral%250Aperspective.%2520In%2520an%2520in-depth%2520investigation%252C%2520we%2520examine%2520selected%2520prompts%2520and%250Aanalyse%2520to%2520what%2520extent%2520responses%2520differ%2520if%2520the%2520system%2520is%2520prompted%2520several%2520times%250Ain%2520an%2520identical%2520way.%2520On%2520this%2520basis%252C%2520we%2520show%2520that%2520ChatGPT%2520is%2520indeed%2520useful%2520for%250Ahelping%2520non-IT%2520users%2520draft%2520texts%2520for%2520their%2520daily%2520work.%2520However%252C%2520it%2520is%250Aabsolutely%2520crucial%2520to%2520thoroughly%2520check%2520the%2520system%2527s%2520responses%2520for%2520biases%2520as%250Awell%2520as%2520for%2520syntactic%2520and%2520grammatical%2520mistakes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.03031v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Prevalent%20is%20Gender%20Bias%20in%20ChatGPT%3F%20--%20Exploring%20German%20and%20English%0A%20%20ChatGPT%20Responses&entry.906535625=Stefanie%20Urchs%20and%20Veronika%20Thurner%20and%20Matthias%20A%C3%9Fenmacher%20and%20Christian%20Heumann%20and%20Stephanie%20Thiemichen&entry.1292438233=%20%20With%20the%20introduction%20of%20ChatGPT%2C%20OpenAI%20made%20large%20language%20models%20%28LLM%29%0Aaccessible%20to%20users%20with%20limited%20IT%20expertise.%20However%2C%20users%20with%20no%0Abackground%20in%20natural%20language%20processing%20%28NLP%29%20might%20lack%20a%20proper%0Aunderstanding%20of%20LLMs.%20Thus%20the%20awareness%20of%20their%20inherent%20limitations%2C%20and%0Atherefore%20will%20take%20the%20systems%27%20output%20at%20face%20value.%20In%20this%20paper%2C%20we%0Asystematically%20analyse%20prompts%20and%20the%20generated%20responses%20to%20identify%20possible%0Aproblematic%20issues%20with%20a%20special%20focus%20on%20gender%20biases%2C%20which%20users%20need%20to%0Abe%20aware%20of%20when%20processing%20the%20system%27s%20output.%20We%20explore%20how%20ChatGPT%20reacts%0Ain%20English%20and%20German%20if%20prompted%20to%20answer%20from%20a%20female%2C%20male%2C%20or%20neutral%0Aperspective.%20In%20an%20in-depth%20investigation%2C%20we%20examine%20selected%20prompts%20and%0Aanalyse%20to%20what%20extent%20responses%20differ%20if%20the%20system%20is%20prompted%20several%20times%0Ain%20an%20identical%20way.%20On%20this%20basis%2C%20we%20show%20that%20ChatGPT%20is%20indeed%20useful%20for%0Ahelping%20non-IT%20users%20draft%20texts%20for%20their%20daily%20work.%20However%2C%20it%20is%0Aabsolutely%20crucial%20to%20thoroughly%20check%20the%20system%27s%20responses%20for%20biases%20as%0Awell%20as%20for%20syntactic%20and%20grammatical%20mistakes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03031v3&entry.124074799=Read"},
{"title": "TUNeS: A Temporal U-Net with Self-Attention for Video-based Surgical\n  Phase Recognition", "author": "Isabel Funke and Dominik Rivoir and Stefanie Krell and Stefanie Speidel", "abstract": "  To enable context-aware computer assistance in the operating room of the\nfuture, cognitive systems need to understand automatically which surgical phase\nis being performed by the medical team. The primary source of information for\nsurgical phase recognition is typically video, which presents two challenges:\nextracting meaningful features from the video stream and effectively modeling\ntemporal information in the sequence of visual features. For temporal modeling,\nattention mechanisms have gained popularity due to their ability to capture\nlong-range dependencies. In this paper, we explore design choices for attention\nin existing temporal models for surgical phase recognition and propose a novel\napproach that uses attention more effectively and does not require hand-crafted\nconstraints: TUNeS, an efficient and simple temporal model that incorporates\nself-attention at the core of a convolutional U-Net structure. In addition, we\npropose to train the feature extractor, a standard CNN, together with an LSTM\non preferably long video segments, i.e., with long temporal context. In our\nexperiments, almost all temporal models performed better on top of feature\nextractors that were trained with longer temporal context. On these\ncontextualized features, TUNeS achieves state-of-the-art results on the\nCholec80 dataset. This study offers new insights on how to use attention\nmechanisms to build accurate and efficient temporal models for surgical phase\nrecognition. Implementing automatic surgical phase recognition is essential to\nautomate the analysis and optimization of surgical workflows and to enable\ncontext-aware computer assistance during surgery, thus ultimately improving\npatient care.\n", "link": "http://arxiv.org/abs/2307.09997v4", "date": "2024-05-13", "relevancy": 2.1282, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5515}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5197}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TUNeS%3A%20A%20Temporal%20U-Net%20with%20Self-Attention%20for%20Video-based%20Surgical%0A%20%20Phase%20Recognition&body=Title%3A%20TUNeS%3A%20A%20Temporal%20U-Net%20with%20Self-Attention%20for%20Video-based%20Surgical%0A%20%20Phase%20Recognition%0AAuthor%3A%20Isabel%20Funke%20and%20Dominik%20Rivoir%20and%20Stefanie%20Krell%20and%20Stefanie%20Speidel%0AAbstract%3A%20%20%20To%20enable%20context-aware%20computer%20assistance%20in%20the%20operating%20room%20of%20the%0Afuture%2C%20cognitive%20systems%20need%20to%20understand%20automatically%20which%20surgical%20phase%0Ais%20being%20performed%20by%20the%20medical%20team.%20The%20primary%20source%20of%20information%20for%0Asurgical%20phase%20recognition%20is%20typically%20video%2C%20which%20presents%20two%20challenges%3A%0Aextracting%20meaningful%20features%20from%20the%20video%20stream%20and%20effectively%20modeling%0Atemporal%20information%20in%20the%20sequence%20of%20visual%20features.%20For%20temporal%20modeling%2C%0Aattention%20mechanisms%20have%20gained%20popularity%20due%20to%20their%20ability%20to%20capture%0Along-range%20dependencies.%20In%20this%20paper%2C%20we%20explore%20design%20choices%20for%20attention%0Ain%20existing%20temporal%20models%20for%20surgical%20phase%20recognition%20and%20propose%20a%20novel%0Aapproach%20that%20uses%20attention%20more%20effectively%20and%20does%20not%20require%20hand-crafted%0Aconstraints%3A%20TUNeS%2C%20an%20efficient%20and%20simple%20temporal%20model%20that%20incorporates%0Aself-attention%20at%20the%20core%20of%20a%20convolutional%20U-Net%20structure.%20In%20addition%2C%20we%0Apropose%20to%20train%20the%20feature%20extractor%2C%20a%20standard%20CNN%2C%20together%20with%20an%20LSTM%0Aon%20preferably%20long%20video%20segments%2C%20i.e.%2C%20with%20long%20temporal%20context.%20In%20our%0Aexperiments%2C%20almost%20all%20temporal%20models%20performed%20better%20on%20top%20of%20feature%0Aextractors%20that%20were%20trained%20with%20longer%20temporal%20context.%20On%20these%0Acontextualized%20features%2C%20TUNeS%20achieves%20state-of-the-art%20results%20on%20the%0ACholec80%20dataset.%20This%20study%20offers%20new%20insights%20on%20how%20to%20use%20attention%0Amechanisms%20to%20build%20accurate%20and%20efficient%20temporal%20models%20for%20surgical%20phase%0Arecognition.%20Implementing%20automatic%20surgical%20phase%20recognition%20is%20essential%20to%0Aautomate%20the%20analysis%20and%20optimization%20of%20surgical%20workflows%20and%20to%20enable%0Acontext-aware%20computer%20assistance%20during%20surgery%2C%20thus%20ultimately%20improving%0Apatient%20care.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.09997v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTUNeS%253A%2520A%2520Temporal%2520U-Net%2520with%2520Self-Attention%2520for%2520Video-based%2520Surgical%250A%2520%2520Phase%2520Recognition%26entry.906535625%3DIsabel%2520Funke%2520and%2520Dominik%2520Rivoir%2520and%2520Stefanie%2520Krell%2520and%2520Stefanie%2520Speidel%26entry.1292438233%3D%2520%2520To%2520enable%2520context-aware%2520computer%2520assistance%2520in%2520the%2520operating%2520room%2520of%2520the%250Afuture%252C%2520cognitive%2520systems%2520need%2520to%2520understand%2520automatically%2520which%2520surgical%2520phase%250Ais%2520being%2520performed%2520by%2520the%2520medical%2520team.%2520The%2520primary%2520source%2520of%2520information%2520for%250Asurgical%2520phase%2520recognition%2520is%2520typically%2520video%252C%2520which%2520presents%2520two%2520challenges%253A%250Aextracting%2520meaningful%2520features%2520from%2520the%2520video%2520stream%2520and%2520effectively%2520modeling%250Atemporal%2520information%2520in%2520the%2520sequence%2520of%2520visual%2520features.%2520For%2520temporal%2520modeling%252C%250Aattention%2520mechanisms%2520have%2520gained%2520popularity%2520due%2520to%2520their%2520ability%2520to%2520capture%250Along-range%2520dependencies.%2520In%2520this%2520paper%252C%2520we%2520explore%2520design%2520choices%2520for%2520attention%250Ain%2520existing%2520temporal%2520models%2520for%2520surgical%2520phase%2520recognition%2520and%2520propose%2520a%2520novel%250Aapproach%2520that%2520uses%2520attention%2520more%2520effectively%2520and%2520does%2520not%2520require%2520hand-crafted%250Aconstraints%253A%2520TUNeS%252C%2520an%2520efficient%2520and%2520simple%2520temporal%2520model%2520that%2520incorporates%250Aself-attention%2520at%2520the%2520core%2520of%2520a%2520convolutional%2520U-Net%2520structure.%2520In%2520addition%252C%2520we%250Apropose%2520to%2520train%2520the%2520feature%2520extractor%252C%2520a%2520standard%2520CNN%252C%2520together%2520with%2520an%2520LSTM%250Aon%2520preferably%2520long%2520video%2520segments%252C%2520i.e.%252C%2520with%2520long%2520temporal%2520context.%2520In%2520our%250Aexperiments%252C%2520almost%2520all%2520temporal%2520models%2520performed%2520better%2520on%2520top%2520of%2520feature%250Aextractors%2520that%2520were%2520trained%2520with%2520longer%2520temporal%2520context.%2520On%2520these%250Acontextualized%2520features%252C%2520TUNeS%2520achieves%2520state-of-the-art%2520results%2520on%2520the%250ACholec80%2520dataset.%2520This%2520study%2520offers%2520new%2520insights%2520on%2520how%2520to%2520use%2520attention%250Amechanisms%2520to%2520build%2520accurate%2520and%2520efficient%2520temporal%2520models%2520for%2520surgical%2520phase%250Arecognition.%2520Implementing%2520automatic%2520surgical%2520phase%2520recognition%2520is%2520essential%2520to%250Aautomate%2520the%2520analysis%2520and%2520optimization%2520of%2520surgical%2520workflows%2520and%2520to%2520enable%250Acontext-aware%2520computer%2520assistance%2520during%2520surgery%252C%2520thus%2520ultimately%2520improving%250Apatient%2520care.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.09997v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TUNeS%3A%20A%20Temporal%20U-Net%20with%20Self-Attention%20for%20Video-based%20Surgical%0A%20%20Phase%20Recognition&entry.906535625=Isabel%20Funke%20and%20Dominik%20Rivoir%20and%20Stefanie%20Krell%20and%20Stefanie%20Speidel&entry.1292438233=%20%20To%20enable%20context-aware%20computer%20assistance%20in%20the%20operating%20room%20of%20the%0Afuture%2C%20cognitive%20systems%20need%20to%20understand%20automatically%20which%20surgical%20phase%0Ais%20being%20performed%20by%20the%20medical%20team.%20The%20primary%20source%20of%20information%20for%0Asurgical%20phase%20recognition%20is%20typically%20video%2C%20which%20presents%20two%20challenges%3A%0Aextracting%20meaningful%20features%20from%20the%20video%20stream%20and%20effectively%20modeling%0Atemporal%20information%20in%20the%20sequence%20of%20visual%20features.%20For%20temporal%20modeling%2C%0Aattention%20mechanisms%20have%20gained%20popularity%20due%20to%20their%20ability%20to%20capture%0Along-range%20dependencies.%20In%20this%20paper%2C%20we%20explore%20design%20choices%20for%20attention%0Ain%20existing%20temporal%20models%20for%20surgical%20phase%20recognition%20and%20propose%20a%20novel%0Aapproach%20that%20uses%20attention%20more%20effectively%20and%20does%20not%20require%20hand-crafted%0Aconstraints%3A%20TUNeS%2C%20an%20efficient%20and%20simple%20temporal%20model%20that%20incorporates%0Aself-attention%20at%20the%20core%20of%20a%20convolutional%20U-Net%20structure.%20In%20addition%2C%20we%0Apropose%20to%20train%20the%20feature%20extractor%2C%20a%20standard%20CNN%2C%20together%20with%20an%20LSTM%0Aon%20preferably%20long%20video%20segments%2C%20i.e.%2C%20with%20long%20temporal%20context.%20In%20our%0Aexperiments%2C%20almost%20all%20temporal%20models%20performed%20better%20on%20top%20of%20feature%0Aextractors%20that%20were%20trained%20with%20longer%20temporal%20context.%20On%20these%0Acontextualized%20features%2C%20TUNeS%20achieves%20state-of-the-art%20results%20on%20the%0ACholec80%20dataset.%20This%20study%20offers%20new%20insights%20on%20how%20to%20use%20attention%0Amechanisms%20to%20build%20accurate%20and%20efficient%20temporal%20models%20for%20surgical%20phase%0Arecognition.%20Implementing%20automatic%20surgical%20phase%20recognition%20is%20essential%20to%0Aautomate%20the%20analysis%20and%20optimization%20of%20surgical%20workflows%20and%20to%20enable%0Acontext-aware%20computer%20assistance%20during%20surgery%2C%20thus%20ultimately%20improving%0Apatient%20care.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.09997v4&entry.124074799=Read"},
{"title": "FORESEE: Multimodal and Multi-view Representation Learning for Robust\n  Prediction of Cancer Survival", "author": "Liangrui Pan and Yijun Peng and Yan Li and Yiyi Liang and Liwen Xu and Qingchun Liang and Shaoliang Peng", "abstract": "  Integrating the different data modalities of cancer patients can\nsignificantly improve the predictive performance of patient survival. However,\nmost existing methods ignore the simultaneous utilization of rich semantic\nfeatures at different scales in pathology images. When collecting multimodal\ndata and extracting features, there is a likelihood of encountering\nintra-modality missing data, introducing noise into the multimodal data. To\naddress these challenges, this paper proposes a new end-to-end framework,\nFORESEE, for robustly predicting patient survival by mining multimodal\ninformation. Specifically, the cross-fusion transformer effectively utilizes\nfeatures at the cellular level, tissue level, and tumor heterogeneity level to\ncorrelate prognosis through a cross-scale feature cross-fusion method. This\nenhances the ability of pathological image feature representation. Secondly,\nthe hybrid attention encoder (HAE) uses the denoising contextual attention\nmodule to obtain the contextual relationship features and local detail features\nof the molecular data. HAE's channel attention module obtains global features\nof molecular data. Furthermore, to address the issue of missing information\nwithin modalities, we propose an asymmetrically masked triplet masked\nautoencoder to reconstruct lost information within modalities. Extensive\nexperiments demonstrate the superiority of our method over state-of-the-art\nmethods on four benchmark datasets in both complete and missing settings.\n", "link": "http://arxiv.org/abs/2405.07702v1", "date": "2024-05-13", "relevancy": 2.1181, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5624}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5343}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FORESEE%3A%20Multimodal%20and%20Multi-view%20Representation%20Learning%20for%20Robust%0A%20%20Prediction%20of%20Cancer%20Survival&body=Title%3A%20FORESEE%3A%20Multimodal%20and%20Multi-view%20Representation%20Learning%20for%20Robust%0A%20%20Prediction%20of%20Cancer%20Survival%0AAuthor%3A%20Liangrui%20Pan%20and%20Yijun%20Peng%20and%20Yan%20Li%20and%20Yiyi%20Liang%20and%20Liwen%20Xu%20and%20Qingchun%20Liang%20and%20Shaoliang%20Peng%0AAbstract%3A%20%20%20Integrating%20the%20different%20data%20modalities%20of%20cancer%20patients%20can%0Asignificantly%20improve%20the%20predictive%20performance%20of%20patient%20survival.%20However%2C%0Amost%20existing%20methods%20ignore%20the%20simultaneous%20utilization%20of%20rich%20semantic%0Afeatures%20at%20different%20scales%20in%20pathology%20images.%20When%20collecting%20multimodal%0Adata%20and%20extracting%20features%2C%20there%20is%20a%20likelihood%20of%20encountering%0Aintra-modality%20missing%20data%2C%20introducing%20noise%20into%20the%20multimodal%20data.%20To%0Aaddress%20these%20challenges%2C%20this%20paper%20proposes%20a%20new%20end-to-end%20framework%2C%0AFORESEE%2C%20for%20robustly%20predicting%20patient%20survival%20by%20mining%20multimodal%0Ainformation.%20Specifically%2C%20the%20cross-fusion%20transformer%20effectively%20utilizes%0Afeatures%20at%20the%20cellular%20level%2C%20tissue%20level%2C%20and%20tumor%20heterogeneity%20level%20to%0Acorrelate%20prognosis%20through%20a%20cross-scale%20feature%20cross-fusion%20method.%20This%0Aenhances%20the%20ability%20of%20pathological%20image%20feature%20representation.%20Secondly%2C%0Athe%20hybrid%20attention%20encoder%20%28HAE%29%20uses%20the%20denoising%20contextual%20attention%0Amodule%20to%20obtain%20the%20contextual%20relationship%20features%20and%20local%20detail%20features%0Aof%20the%20molecular%20data.%20HAE%27s%20channel%20attention%20module%20obtains%20global%20features%0Aof%20molecular%20data.%20Furthermore%2C%20to%20address%20the%20issue%20of%20missing%20information%0Awithin%20modalities%2C%20we%20propose%20an%20asymmetrically%20masked%20triplet%20masked%0Aautoencoder%20to%20reconstruct%20lost%20information%20within%20modalities.%20Extensive%0Aexperiments%20demonstrate%20the%20superiority%20of%20our%20method%20over%20state-of-the-art%0Amethods%20on%20four%20benchmark%20datasets%20in%20both%20complete%20and%20missing%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFORESEE%253A%2520Multimodal%2520and%2520Multi-view%2520Representation%2520Learning%2520for%2520Robust%250A%2520%2520Prediction%2520of%2520Cancer%2520Survival%26entry.906535625%3DLiangrui%2520Pan%2520and%2520Yijun%2520Peng%2520and%2520Yan%2520Li%2520and%2520Yiyi%2520Liang%2520and%2520Liwen%2520Xu%2520and%2520Qingchun%2520Liang%2520and%2520Shaoliang%2520Peng%26entry.1292438233%3D%2520%2520Integrating%2520the%2520different%2520data%2520modalities%2520of%2520cancer%2520patients%2520can%250Asignificantly%2520improve%2520the%2520predictive%2520performance%2520of%2520patient%2520survival.%2520However%252C%250Amost%2520existing%2520methods%2520ignore%2520the%2520simultaneous%2520utilization%2520of%2520rich%2520semantic%250Afeatures%2520at%2520different%2520scales%2520in%2520pathology%2520images.%2520When%2520collecting%2520multimodal%250Adata%2520and%2520extracting%2520features%252C%2520there%2520is%2520a%2520likelihood%2520of%2520encountering%250Aintra-modality%2520missing%2520data%252C%2520introducing%2520noise%2520into%2520the%2520multimodal%2520data.%2520To%250Aaddress%2520these%2520challenges%252C%2520this%2520paper%2520proposes%2520a%2520new%2520end-to-end%2520framework%252C%250AFORESEE%252C%2520for%2520robustly%2520predicting%2520patient%2520survival%2520by%2520mining%2520multimodal%250Ainformation.%2520Specifically%252C%2520the%2520cross-fusion%2520transformer%2520effectively%2520utilizes%250Afeatures%2520at%2520the%2520cellular%2520level%252C%2520tissue%2520level%252C%2520and%2520tumor%2520heterogeneity%2520level%2520to%250Acorrelate%2520prognosis%2520through%2520a%2520cross-scale%2520feature%2520cross-fusion%2520method.%2520This%250Aenhances%2520the%2520ability%2520of%2520pathological%2520image%2520feature%2520representation.%2520Secondly%252C%250Athe%2520hybrid%2520attention%2520encoder%2520%2528HAE%2529%2520uses%2520the%2520denoising%2520contextual%2520attention%250Amodule%2520to%2520obtain%2520the%2520contextual%2520relationship%2520features%2520and%2520local%2520detail%2520features%250Aof%2520the%2520molecular%2520data.%2520HAE%2527s%2520channel%2520attention%2520module%2520obtains%2520global%2520features%250Aof%2520molecular%2520data.%2520Furthermore%252C%2520to%2520address%2520the%2520issue%2520of%2520missing%2520information%250Awithin%2520modalities%252C%2520we%2520propose%2520an%2520asymmetrically%2520masked%2520triplet%2520masked%250Aautoencoder%2520to%2520reconstruct%2520lost%2520information%2520within%2520modalities.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method%2520over%2520state-of-the-art%250Amethods%2520on%2520four%2520benchmark%2520datasets%2520in%2520both%2520complete%2520and%2520missing%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FORESEE%3A%20Multimodal%20and%20Multi-view%20Representation%20Learning%20for%20Robust%0A%20%20Prediction%20of%20Cancer%20Survival&entry.906535625=Liangrui%20Pan%20and%20Yijun%20Peng%20and%20Yan%20Li%20and%20Yiyi%20Liang%20and%20Liwen%20Xu%20and%20Qingchun%20Liang%20and%20Shaoliang%20Peng&entry.1292438233=%20%20Integrating%20the%20different%20data%20modalities%20of%20cancer%20patients%20can%0Asignificantly%20improve%20the%20predictive%20performance%20of%20patient%20survival.%20However%2C%0Amost%20existing%20methods%20ignore%20the%20simultaneous%20utilization%20of%20rich%20semantic%0Afeatures%20at%20different%20scales%20in%20pathology%20images.%20When%20collecting%20multimodal%0Adata%20and%20extracting%20features%2C%20there%20is%20a%20likelihood%20of%20encountering%0Aintra-modality%20missing%20data%2C%20introducing%20noise%20into%20the%20multimodal%20data.%20To%0Aaddress%20these%20challenges%2C%20this%20paper%20proposes%20a%20new%20end-to-end%20framework%2C%0AFORESEE%2C%20for%20robustly%20predicting%20patient%20survival%20by%20mining%20multimodal%0Ainformation.%20Specifically%2C%20the%20cross-fusion%20transformer%20effectively%20utilizes%0Afeatures%20at%20the%20cellular%20level%2C%20tissue%20level%2C%20and%20tumor%20heterogeneity%20level%20to%0Acorrelate%20prognosis%20through%20a%20cross-scale%20feature%20cross-fusion%20method.%20This%0Aenhances%20the%20ability%20of%20pathological%20image%20feature%20representation.%20Secondly%2C%0Athe%20hybrid%20attention%20encoder%20%28HAE%29%20uses%20the%20denoising%20contextual%20attention%0Amodule%20to%20obtain%20the%20contextual%20relationship%20features%20and%20local%20detail%20features%0Aof%20the%20molecular%20data.%20HAE%27s%20channel%20attention%20module%20obtains%20global%20features%0Aof%20molecular%20data.%20Furthermore%2C%20to%20address%20the%20issue%20of%20missing%20information%0Awithin%20modalities%2C%20we%20propose%20an%20asymmetrically%20masked%20triplet%20masked%0Aautoencoder%20to%20reconstruct%20lost%20information%20within%20modalities.%20Extensive%0Aexperiments%20demonstrate%20the%20superiority%20of%20our%20method%20over%20state-of-the-art%0Amethods%20on%20four%20benchmark%20datasets%20in%20both%20complete%20and%20missing%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07702v1&entry.124074799=Read"},
{"title": "Discrete approximations of Gaussian smoothing and Gaussian derivatives", "author": "Tony Lindeberg", "abstract": "  This paper develops an in-depth treatment concerning the problem of\napproximating the Gaussian smoothing and Gaussian derivative computations in\nscale-space theory for application on discrete data. With close connections to\nprevious axiomatic treatments of continuous and discrete scale-space theory, we\nconsider three main ways discretizing these scale-space operations in terms of\nexplicit discrete convolutions, based on either (i) sampling the Gaussian\nkernels and the Gaussian derivative kernels, (ii) locally integrating the\nGaussian kernels and the Gaussian derivative kernels over each pixel support\nregion and (iii) basing the scale-space analysis on the discrete analogue of\nthe Gaussian kernel, and then computing derivative approximations by applying\nsmall-support central difference operators to the spatially smoothed image\ndata.\n  We study the properties of these three main discretization methods both\ntheoretically and experimentally, and characterize their performance by\nquantitative measures, including the results they give rise to with respect to\nthe task of scale selection, investigated for four different use cases, and\nwith emphasis on the behaviour at fine scales. The results show that the\nsampled Gaussian kernels and derivatives as well as the integrated Gaussian\nkernels and derivatives perform very poorly at very fine scales. At very fine\nscales, the discrete analogue of the Gaussian kernel with its corresponding\ndiscrete derivative approximations performs substantially better. The sampled\nGaussian kernel and the sampled Gaussian derivatives do, on the other hand,\nlead to numerically very good approximations of the corresponding continuous\nresults, when the scale parameter is sufficiently large, in the experiments\npresented in the paper, when the scale parameter is greater than a value of\nabout 1, in units of the grid spacing.\n", "link": "http://arxiv.org/abs/2311.11317v7", "date": "2024-05-13", "relevancy": 2.1157, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5376}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5357}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discrete%20approximations%20of%20Gaussian%20smoothing%20and%20Gaussian%20derivatives&body=Title%3A%20Discrete%20approximations%20of%20Gaussian%20smoothing%20and%20Gaussian%20derivatives%0AAuthor%3A%20Tony%20Lindeberg%0AAbstract%3A%20%20%20This%20paper%20develops%20an%20in-depth%20treatment%20concerning%20the%20problem%20of%0Aapproximating%20the%20Gaussian%20smoothing%20and%20Gaussian%20derivative%20computations%20in%0Ascale-space%20theory%20for%20application%20on%20discrete%20data.%20With%20close%20connections%20to%0Aprevious%20axiomatic%20treatments%20of%20continuous%20and%20discrete%20scale-space%20theory%2C%20we%0Aconsider%20three%20main%20ways%20discretizing%20these%20scale-space%20operations%20in%20terms%20of%0Aexplicit%20discrete%20convolutions%2C%20based%20on%20either%20%28i%29%20sampling%20the%20Gaussian%0Akernels%20and%20the%20Gaussian%20derivative%20kernels%2C%20%28ii%29%20locally%20integrating%20the%0AGaussian%20kernels%20and%20the%20Gaussian%20derivative%20kernels%20over%20each%20pixel%20support%0Aregion%20and%20%28iii%29%20basing%20the%20scale-space%20analysis%20on%20the%20discrete%20analogue%20of%0Athe%20Gaussian%20kernel%2C%20and%20then%20computing%20derivative%20approximations%20by%20applying%0Asmall-support%20central%20difference%20operators%20to%20the%20spatially%20smoothed%20image%0Adata.%0A%20%20We%20study%20the%20properties%20of%20these%20three%20main%20discretization%20methods%20both%0Atheoretically%20and%20experimentally%2C%20and%20characterize%20their%20performance%20by%0Aquantitative%20measures%2C%20including%20the%20results%20they%20give%20rise%20to%20with%20respect%20to%0Athe%20task%20of%20scale%20selection%2C%20investigated%20for%20four%20different%20use%20cases%2C%20and%0Awith%20emphasis%20on%20the%20behaviour%20at%20fine%20scales.%20The%20results%20show%20that%20the%0Asampled%20Gaussian%20kernels%20and%20derivatives%20as%20well%20as%20the%20integrated%20Gaussian%0Akernels%20and%20derivatives%20perform%20very%20poorly%20at%20very%20fine%20scales.%20At%20very%20fine%0Ascales%2C%20the%20discrete%20analogue%20of%20the%20Gaussian%20kernel%20with%20its%20corresponding%0Adiscrete%20derivative%20approximations%20performs%20substantially%20better.%20The%20sampled%0AGaussian%20kernel%20and%20the%20sampled%20Gaussian%20derivatives%20do%2C%20on%20the%20other%20hand%2C%0Alead%20to%20numerically%20very%20good%20approximations%20of%20the%20corresponding%20continuous%0Aresults%2C%20when%20the%20scale%20parameter%20is%20sufficiently%20large%2C%20in%20the%20experiments%0Apresented%20in%20the%20paper%2C%20when%20the%20scale%20parameter%20is%20greater%20than%20a%20value%20of%0Aabout%201%2C%20in%20units%20of%20the%20grid%20spacing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.11317v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscrete%2520approximations%2520of%2520Gaussian%2520smoothing%2520and%2520Gaussian%2520derivatives%26entry.906535625%3DTony%2520Lindeberg%26entry.1292438233%3D%2520%2520This%2520paper%2520develops%2520an%2520in-depth%2520treatment%2520concerning%2520the%2520problem%2520of%250Aapproximating%2520the%2520Gaussian%2520smoothing%2520and%2520Gaussian%2520derivative%2520computations%2520in%250Ascale-space%2520theory%2520for%2520application%2520on%2520discrete%2520data.%2520With%2520close%2520connections%2520to%250Aprevious%2520axiomatic%2520treatments%2520of%2520continuous%2520and%2520discrete%2520scale-space%2520theory%252C%2520we%250Aconsider%2520three%2520main%2520ways%2520discretizing%2520these%2520scale-space%2520operations%2520in%2520terms%2520of%250Aexplicit%2520discrete%2520convolutions%252C%2520based%2520on%2520either%2520%2528i%2529%2520sampling%2520the%2520Gaussian%250Akernels%2520and%2520the%2520Gaussian%2520derivative%2520kernels%252C%2520%2528ii%2529%2520locally%2520integrating%2520the%250AGaussian%2520kernels%2520and%2520the%2520Gaussian%2520derivative%2520kernels%2520over%2520each%2520pixel%2520support%250Aregion%2520and%2520%2528iii%2529%2520basing%2520the%2520scale-space%2520analysis%2520on%2520the%2520discrete%2520analogue%2520of%250Athe%2520Gaussian%2520kernel%252C%2520and%2520then%2520computing%2520derivative%2520approximations%2520by%2520applying%250Asmall-support%2520central%2520difference%2520operators%2520to%2520the%2520spatially%2520smoothed%2520image%250Adata.%250A%2520%2520We%2520study%2520the%2520properties%2520of%2520these%2520three%2520main%2520discretization%2520methods%2520both%250Atheoretically%2520and%2520experimentally%252C%2520and%2520characterize%2520their%2520performance%2520by%250Aquantitative%2520measures%252C%2520including%2520the%2520results%2520they%2520give%2520rise%2520to%2520with%2520respect%2520to%250Athe%2520task%2520of%2520scale%2520selection%252C%2520investigated%2520for%2520four%2520different%2520use%2520cases%252C%2520and%250Awith%2520emphasis%2520on%2520the%2520behaviour%2520at%2520fine%2520scales.%2520The%2520results%2520show%2520that%2520the%250Asampled%2520Gaussian%2520kernels%2520and%2520derivatives%2520as%2520well%2520as%2520the%2520integrated%2520Gaussian%250Akernels%2520and%2520derivatives%2520perform%2520very%2520poorly%2520at%2520very%2520fine%2520scales.%2520At%2520very%2520fine%250Ascales%252C%2520the%2520discrete%2520analogue%2520of%2520the%2520Gaussian%2520kernel%2520with%2520its%2520corresponding%250Adiscrete%2520derivative%2520approximations%2520performs%2520substantially%2520better.%2520The%2520sampled%250AGaussian%2520kernel%2520and%2520the%2520sampled%2520Gaussian%2520derivatives%2520do%252C%2520on%2520the%2520other%2520hand%252C%250Alead%2520to%2520numerically%2520very%2520good%2520approximations%2520of%2520the%2520corresponding%2520continuous%250Aresults%252C%2520when%2520the%2520scale%2520parameter%2520is%2520sufficiently%2520large%252C%2520in%2520the%2520experiments%250Apresented%2520in%2520the%2520paper%252C%2520when%2520the%2520scale%2520parameter%2520is%2520greater%2520than%2520a%2520value%2520of%250Aabout%25201%252C%2520in%2520units%2520of%2520the%2520grid%2520spacing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.11317v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discrete%20approximations%20of%20Gaussian%20smoothing%20and%20Gaussian%20derivatives&entry.906535625=Tony%20Lindeberg&entry.1292438233=%20%20This%20paper%20develops%20an%20in-depth%20treatment%20concerning%20the%20problem%20of%0Aapproximating%20the%20Gaussian%20smoothing%20and%20Gaussian%20derivative%20computations%20in%0Ascale-space%20theory%20for%20application%20on%20discrete%20data.%20With%20close%20connections%20to%0Aprevious%20axiomatic%20treatments%20of%20continuous%20and%20discrete%20scale-space%20theory%2C%20we%0Aconsider%20three%20main%20ways%20discretizing%20these%20scale-space%20operations%20in%20terms%20of%0Aexplicit%20discrete%20convolutions%2C%20based%20on%20either%20%28i%29%20sampling%20the%20Gaussian%0Akernels%20and%20the%20Gaussian%20derivative%20kernels%2C%20%28ii%29%20locally%20integrating%20the%0AGaussian%20kernels%20and%20the%20Gaussian%20derivative%20kernels%20over%20each%20pixel%20support%0Aregion%20and%20%28iii%29%20basing%20the%20scale-space%20analysis%20on%20the%20discrete%20analogue%20of%0Athe%20Gaussian%20kernel%2C%20and%20then%20computing%20derivative%20approximations%20by%20applying%0Asmall-support%20central%20difference%20operators%20to%20the%20spatially%20smoothed%20image%0Adata.%0A%20%20We%20study%20the%20properties%20of%20these%20three%20main%20discretization%20methods%20both%0Atheoretically%20and%20experimentally%2C%20and%20characterize%20their%20performance%20by%0Aquantitative%20measures%2C%20including%20the%20results%20they%20give%20rise%20to%20with%20respect%20to%0Athe%20task%20of%20scale%20selection%2C%20investigated%20for%20four%20different%20use%20cases%2C%20and%0Awith%20emphasis%20on%20the%20behaviour%20at%20fine%20scales.%20The%20results%20show%20that%20the%0Asampled%20Gaussian%20kernels%20and%20derivatives%20as%20well%20as%20the%20integrated%20Gaussian%0Akernels%20and%20derivatives%20perform%20very%20poorly%20at%20very%20fine%20scales.%20At%20very%20fine%0Ascales%2C%20the%20discrete%20analogue%20of%20the%20Gaussian%20kernel%20with%20its%20corresponding%0Adiscrete%20derivative%20approximations%20performs%20substantially%20better.%20The%20sampled%0AGaussian%20kernel%20and%20the%20sampled%20Gaussian%20derivatives%20do%2C%20on%20the%20other%20hand%2C%0Alead%20to%20numerically%20very%20good%20approximations%20of%20the%20corresponding%20continuous%0Aresults%2C%20when%20the%20scale%20parameter%20is%20sufficiently%20large%2C%20in%20the%20experiments%0Apresented%20in%20the%20paper%2C%20when%20the%20scale%20parameter%20is%20greater%20than%20a%20value%20of%0Aabout%201%2C%20in%20units%20of%20the%20grid%20spacing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11317v7&entry.124074799=Read"},
{"title": "AnomalyLLM: Few-shot Anomaly Edge Detection for Dynamic Graphs using\n  Large Language Models", "author": "Shuo Liu and Di Yao and Lanting Fang and Zhetao Li and Wenbin Li and Kaiyu Feng and XiaoWen Ji and Jingping Bi", "abstract": "  Detecting anomaly edges for dynamic graphs aims to identify edges\nsignificantly deviating from the normal pattern and can be applied in various\ndomains, such as cybersecurity, financial transactions and AIOps. With the\nevolving of time, the types of anomaly edges are emerging and the labeled\nanomaly samples are few for each type. Current methods are either designed to\ndetect randomly inserted edges or require sufficient labeled data for model\ntraining, which harms their applicability for real-world applications. In this\npaper, we study this problem by cooperating with the rich knowledge encoded in\nlarge language models(LLMs) and propose a method, namely AnomalyLLM. To align\nthe dynamic graph with LLMs, AnomalyLLM pre-trains a dynamic-aware encoder to\ngenerate the representations of edges and reprograms the edges using the\nprototypes of word embeddings. Along with the encoder, we design an in-context\nlearning framework that integrates the information of a few labeled samples to\nachieve few-shot anomaly detection. Experiments on four datasets reveal that\nAnomalyLLM can not only significantly improve the performance of few-shot\nanomaly detection, but also achieve superior results on new anomalies without\nany update of model parameters.\n", "link": "http://arxiv.org/abs/2405.07626v1", "date": "2024-05-13", "relevancy": 2.1112, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5451}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5301}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnomalyLLM%3A%20Few-shot%20Anomaly%20Edge%20Detection%20for%20Dynamic%20Graphs%20using%0A%20%20Large%20Language%20Models&body=Title%3A%20AnomalyLLM%3A%20Few-shot%20Anomaly%20Edge%20Detection%20for%20Dynamic%20Graphs%20using%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Shuo%20Liu%20and%20Di%20Yao%20and%20Lanting%20Fang%20and%20Zhetao%20Li%20and%20Wenbin%20Li%20and%20Kaiyu%20Feng%20and%20XiaoWen%20Ji%20and%20Jingping%20Bi%0AAbstract%3A%20%20%20Detecting%20anomaly%20edges%20for%20dynamic%20graphs%20aims%20to%20identify%20edges%0Asignificantly%20deviating%20from%20the%20normal%20pattern%20and%20can%20be%20applied%20in%20various%0Adomains%2C%20such%20as%20cybersecurity%2C%20financial%20transactions%20and%20AIOps.%20With%20the%0Aevolving%20of%20time%2C%20the%20types%20of%20anomaly%20edges%20are%20emerging%20and%20the%20labeled%0Aanomaly%20samples%20are%20few%20for%20each%20type.%20Current%20methods%20are%20either%20designed%20to%0Adetect%20randomly%20inserted%20edges%20or%20require%20sufficient%20labeled%20data%20for%20model%0Atraining%2C%20which%20harms%20their%20applicability%20for%20real-world%20applications.%20In%20this%0Apaper%2C%20we%20study%20this%20problem%20by%20cooperating%20with%20the%20rich%20knowledge%20encoded%20in%0Alarge%20language%20models%28LLMs%29%20and%20propose%20a%20method%2C%20namely%20AnomalyLLM.%20To%20align%0Athe%20dynamic%20graph%20with%20LLMs%2C%20AnomalyLLM%20pre-trains%20a%20dynamic-aware%20encoder%20to%0Agenerate%20the%20representations%20of%20edges%20and%20reprograms%20the%20edges%20using%20the%0Aprototypes%20of%20word%20embeddings.%20Along%20with%20the%20encoder%2C%20we%20design%20an%20in-context%0Alearning%20framework%20that%20integrates%20the%20information%20of%20a%20few%20labeled%20samples%20to%0Aachieve%20few-shot%20anomaly%20detection.%20Experiments%20on%20four%20datasets%20reveal%20that%0AAnomalyLLM%20can%20not%20only%20significantly%20improve%20the%20performance%20of%20few-shot%0Aanomaly%20detection%2C%20but%20also%20achieve%20superior%20results%20on%20new%20anomalies%20without%0Aany%20update%20of%20model%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07626v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnomalyLLM%253A%2520Few-shot%2520Anomaly%2520Edge%2520Detection%2520for%2520Dynamic%2520Graphs%2520using%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DShuo%2520Liu%2520and%2520Di%2520Yao%2520and%2520Lanting%2520Fang%2520and%2520Zhetao%2520Li%2520and%2520Wenbin%2520Li%2520and%2520Kaiyu%2520Feng%2520and%2520XiaoWen%2520Ji%2520and%2520Jingping%2520Bi%26entry.1292438233%3D%2520%2520Detecting%2520anomaly%2520edges%2520for%2520dynamic%2520graphs%2520aims%2520to%2520identify%2520edges%250Asignificantly%2520deviating%2520from%2520the%2520normal%2520pattern%2520and%2520can%2520be%2520applied%2520in%2520various%250Adomains%252C%2520such%2520as%2520cybersecurity%252C%2520financial%2520transactions%2520and%2520AIOps.%2520With%2520the%250Aevolving%2520of%2520time%252C%2520the%2520types%2520of%2520anomaly%2520edges%2520are%2520emerging%2520and%2520the%2520labeled%250Aanomaly%2520samples%2520are%2520few%2520for%2520each%2520type.%2520Current%2520methods%2520are%2520either%2520designed%2520to%250Adetect%2520randomly%2520inserted%2520edges%2520or%2520require%2520sufficient%2520labeled%2520data%2520for%2520model%250Atraining%252C%2520which%2520harms%2520their%2520applicability%2520for%2520real-world%2520applications.%2520In%2520this%250Apaper%252C%2520we%2520study%2520this%2520problem%2520by%2520cooperating%2520with%2520the%2520rich%2520knowledge%2520encoded%2520in%250Alarge%2520language%2520models%2528LLMs%2529%2520and%2520propose%2520a%2520method%252C%2520namely%2520AnomalyLLM.%2520To%2520align%250Athe%2520dynamic%2520graph%2520with%2520LLMs%252C%2520AnomalyLLM%2520pre-trains%2520a%2520dynamic-aware%2520encoder%2520to%250Agenerate%2520the%2520representations%2520of%2520edges%2520and%2520reprograms%2520the%2520edges%2520using%2520the%250Aprototypes%2520of%2520word%2520embeddings.%2520Along%2520with%2520the%2520encoder%252C%2520we%2520design%2520an%2520in-context%250Alearning%2520framework%2520that%2520integrates%2520the%2520information%2520of%2520a%2520few%2520labeled%2520samples%2520to%250Aachieve%2520few-shot%2520anomaly%2520detection.%2520Experiments%2520on%2520four%2520datasets%2520reveal%2520that%250AAnomalyLLM%2520can%2520not%2520only%2520significantly%2520improve%2520the%2520performance%2520of%2520few-shot%250Aanomaly%2520detection%252C%2520but%2520also%2520achieve%2520superior%2520results%2520on%2520new%2520anomalies%2520without%250Aany%2520update%2520of%2520model%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07626v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnomalyLLM%3A%20Few-shot%20Anomaly%20Edge%20Detection%20for%20Dynamic%20Graphs%20using%0A%20%20Large%20Language%20Models&entry.906535625=Shuo%20Liu%20and%20Di%20Yao%20and%20Lanting%20Fang%20and%20Zhetao%20Li%20and%20Wenbin%20Li%20and%20Kaiyu%20Feng%20and%20XiaoWen%20Ji%20and%20Jingping%20Bi&entry.1292438233=%20%20Detecting%20anomaly%20edges%20for%20dynamic%20graphs%20aims%20to%20identify%20edges%0Asignificantly%20deviating%20from%20the%20normal%20pattern%20and%20can%20be%20applied%20in%20various%0Adomains%2C%20such%20as%20cybersecurity%2C%20financial%20transactions%20and%20AIOps.%20With%20the%0Aevolving%20of%20time%2C%20the%20types%20of%20anomaly%20edges%20are%20emerging%20and%20the%20labeled%0Aanomaly%20samples%20are%20few%20for%20each%20type.%20Current%20methods%20are%20either%20designed%20to%0Adetect%20randomly%20inserted%20edges%20or%20require%20sufficient%20labeled%20data%20for%20model%0Atraining%2C%20which%20harms%20their%20applicability%20for%20real-world%20applications.%20In%20this%0Apaper%2C%20we%20study%20this%20problem%20by%20cooperating%20with%20the%20rich%20knowledge%20encoded%20in%0Alarge%20language%20models%28LLMs%29%20and%20propose%20a%20method%2C%20namely%20AnomalyLLM.%20To%20align%0Athe%20dynamic%20graph%20with%20LLMs%2C%20AnomalyLLM%20pre-trains%20a%20dynamic-aware%20encoder%20to%0Agenerate%20the%20representations%20of%20edges%20and%20reprograms%20the%20edges%20using%20the%0Aprototypes%20of%20word%20embeddings.%20Along%20with%20the%20encoder%2C%20we%20design%20an%20in-context%0Alearning%20framework%20that%20integrates%20the%20information%20of%20a%20few%20labeled%20samples%20to%0Aachieve%20few-shot%20anomaly%20detection.%20Experiments%20on%20four%20datasets%20reveal%20that%0AAnomalyLLM%20can%20not%20only%20significantly%20improve%20the%20performance%20of%20few-shot%0Aanomaly%20detection%2C%20but%20also%20achieve%20superior%20results%20on%20new%20anomalies%20without%0Aany%20update%20of%20model%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07626v1&entry.124074799=Read"},
{"title": "Delta Tensor: Efficient Vector and Tensor Storage in Delta Lake", "author": "Zhiwei Bao and Liu Liao-Liao and Zhiyu Wu and Yifan Zhou and Dan Fan and Michal Aibin and Yvonne Coady and Andrew Brownsword", "abstract": "  The exponential growth of artificial intelligence (AI) and machine learning\n(ML) applications has necessitated the development of efficient storage\nsolutions for vector and tensor data. This paper presents a novel approach for\ntensor storage in a Lakehouse architecture using Delta Lake. By adopting the\nmultidimensional array storage strategy from array databases and sparse\nencoding methods to Delta Lake tables, experiments show that this approach has\ndemonstrated notable improvements in both space and time efficiencies when\ncompared to traditional serialization of tensors. These results provide\nvaluable insights for the development and implementation of optimized vector\nand tensor storage solutions in data-intensive applications, contributing to\nthe evolution of efficient data management practices in AI and ML domains in\ncloud-native environments\n", "link": "http://arxiv.org/abs/2405.03708v3", "date": "2024-05-13", "relevancy": 2.1072, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4248}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4247}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Delta%20Tensor%3A%20Efficient%20Vector%20and%20Tensor%20Storage%20in%20Delta%20Lake&body=Title%3A%20Delta%20Tensor%3A%20Efficient%20Vector%20and%20Tensor%20Storage%20in%20Delta%20Lake%0AAuthor%3A%20Zhiwei%20Bao%20and%20Liu%20Liao-Liao%20and%20Zhiyu%20Wu%20and%20Yifan%20Zhou%20and%20Dan%20Fan%20and%20Michal%20Aibin%20and%20Yvonne%20Coady%20and%20Andrew%20Brownsword%0AAbstract%3A%20%20%20The%20exponential%20growth%20of%20artificial%20intelligence%20%28AI%29%20and%20machine%20learning%0A%28ML%29%20applications%20has%20necessitated%20the%20development%20of%20efficient%20storage%0Asolutions%20for%20vector%20and%20tensor%20data.%20This%20paper%20presents%20a%20novel%20approach%20for%0Atensor%20storage%20in%20a%20Lakehouse%20architecture%20using%20Delta%20Lake.%20By%20adopting%20the%0Amultidimensional%20array%20storage%20strategy%20from%20array%20databases%20and%20sparse%0Aencoding%20methods%20to%20Delta%20Lake%20tables%2C%20experiments%20show%20that%20this%20approach%20has%0Ademonstrated%20notable%20improvements%20in%20both%20space%20and%20time%20efficiencies%20when%0Acompared%20to%20traditional%20serialization%20of%20tensors.%20These%20results%20provide%0Avaluable%20insights%20for%20the%20development%20and%20implementation%20of%20optimized%20vector%0Aand%20tensor%20storage%20solutions%20in%20data-intensive%20applications%2C%20contributing%20to%0Athe%20evolution%20of%20efficient%20data%20management%20practices%20in%20AI%20and%20ML%20domains%20in%0Acloud-native%20environments%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03708v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDelta%2520Tensor%253A%2520Efficient%2520Vector%2520and%2520Tensor%2520Storage%2520in%2520Delta%2520Lake%26entry.906535625%3DZhiwei%2520Bao%2520and%2520Liu%2520Liao-Liao%2520and%2520Zhiyu%2520Wu%2520and%2520Yifan%2520Zhou%2520and%2520Dan%2520Fan%2520and%2520Michal%2520Aibin%2520and%2520Yvonne%2520Coady%2520and%2520Andrew%2520Brownsword%26entry.1292438233%3D%2520%2520The%2520exponential%2520growth%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520and%2520machine%2520learning%250A%2528ML%2529%2520applications%2520has%2520necessitated%2520the%2520development%2520of%2520efficient%2520storage%250Asolutions%2520for%2520vector%2520and%2520tensor%2520data.%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520for%250Atensor%2520storage%2520in%2520a%2520Lakehouse%2520architecture%2520using%2520Delta%2520Lake.%2520By%2520adopting%2520the%250Amultidimensional%2520array%2520storage%2520strategy%2520from%2520array%2520databases%2520and%2520sparse%250Aencoding%2520methods%2520to%2520Delta%2520Lake%2520tables%252C%2520experiments%2520show%2520that%2520this%2520approach%2520has%250Ademonstrated%2520notable%2520improvements%2520in%2520both%2520space%2520and%2520time%2520efficiencies%2520when%250Acompared%2520to%2520traditional%2520serialization%2520of%2520tensors.%2520These%2520results%2520provide%250Avaluable%2520insights%2520for%2520the%2520development%2520and%2520implementation%2520of%2520optimized%2520vector%250Aand%2520tensor%2520storage%2520solutions%2520in%2520data-intensive%2520applications%252C%2520contributing%2520to%250Athe%2520evolution%2520of%2520efficient%2520data%2520management%2520practices%2520in%2520AI%2520and%2520ML%2520domains%2520in%250Acloud-native%2520environments%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03708v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Delta%20Tensor%3A%20Efficient%20Vector%20and%20Tensor%20Storage%20in%20Delta%20Lake&entry.906535625=Zhiwei%20Bao%20and%20Liu%20Liao-Liao%20and%20Zhiyu%20Wu%20and%20Yifan%20Zhou%20and%20Dan%20Fan%20and%20Michal%20Aibin%20and%20Yvonne%20Coady%20and%20Andrew%20Brownsword&entry.1292438233=%20%20The%20exponential%20growth%20of%20artificial%20intelligence%20%28AI%29%20and%20machine%20learning%0A%28ML%29%20applications%20has%20necessitated%20the%20development%20of%20efficient%20storage%0Asolutions%20for%20vector%20and%20tensor%20data.%20This%20paper%20presents%20a%20novel%20approach%20for%0Atensor%20storage%20in%20a%20Lakehouse%20architecture%20using%20Delta%20Lake.%20By%20adopting%20the%0Amultidimensional%20array%20storage%20strategy%20from%20array%20databases%20and%20sparse%0Aencoding%20methods%20to%20Delta%20Lake%20tables%2C%20experiments%20show%20that%20this%20approach%20has%0Ademonstrated%20notable%20improvements%20in%20both%20space%20and%20time%20efficiencies%20when%0Acompared%20to%20traditional%20serialization%20of%20tensors.%20These%20results%20provide%0Avaluable%20insights%20for%20the%20development%20and%20implementation%20of%20optimized%20vector%0Aand%20tensor%20storage%20solutions%20in%20data-intensive%20applications%2C%20contributing%20to%0Athe%20evolution%20of%20efficient%20data%20management%20practices%20in%20AI%20and%20ML%20domains%20in%0Acloud-native%20environments%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03708v3&entry.124074799=Read"},
{"title": "Multi-Task Learning for Fatigue Detection and Face Recognition of\n  Drivers via Tree-Style Space-Channel Attention Fusion Network", "author": "Shulei Qu and Zhenguo Gao and Xiaowei Chen and Na Li and Yakai Wang and Xiaoxiao Wu", "abstract": "  In driving scenarios, automobile active safety systems are increasingly\nincorporating deep learning technology. These systems typically need to handle\nmultiple tasks simultaneously, such as detecting fatigue driving and\nrecognizing the driver's identity. However, the traditional parallel-style\napproach of combining multiple single-task models tends to waste resources when\ndealing with similar tasks. Therefore, we propose a novel tree-style multi-task\nmodeling approach for multi-task learning, which rooted at a shared backbone,\nmore dedicated separate module branches are appended as the model pipeline goes\ndeeper. Following the tree-style approach, we propose a multi-task learning\nmodel for simultaneously performing driver fatigue detection and face\nrecognition for identifying a driver. This model shares a common feature\nextraction backbone module, with further separated feature extraction and\nclassification module branches. The dedicated branches exploit and combine\nspatial and channel attention mechanisms to generate space-channel\nfused-attention enhanced features, leading to improved detection performance.\nAs only single-task datasets are available, we introduce techniques including\nalternating updation and gradient accumulation for training our multi-task\nmodel using only the single-task datasets. The effectiveness of our tree-style\nmulti-task learning model is verified through extensive validations.\n", "link": "http://arxiv.org/abs/2405.07845v1", "date": "2024-05-13", "relevancy": 2.1025, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5507}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5135}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Task%20Learning%20for%20Fatigue%20Detection%20and%20Face%20Recognition%20of%0A%20%20Drivers%20via%20Tree-Style%20Space-Channel%20Attention%20Fusion%20Network&body=Title%3A%20Multi-Task%20Learning%20for%20Fatigue%20Detection%20and%20Face%20Recognition%20of%0A%20%20Drivers%20via%20Tree-Style%20Space-Channel%20Attention%20Fusion%20Network%0AAuthor%3A%20Shulei%20Qu%20and%20Zhenguo%20Gao%20and%20Xiaowei%20Chen%20and%20Na%20Li%20and%20Yakai%20Wang%20and%20Xiaoxiao%20Wu%0AAbstract%3A%20%20%20In%20driving%20scenarios%2C%20automobile%20active%20safety%20systems%20are%20increasingly%0Aincorporating%20deep%20learning%20technology.%20These%20systems%20typically%20need%20to%20handle%0Amultiple%20tasks%20simultaneously%2C%20such%20as%20detecting%20fatigue%20driving%20and%0Arecognizing%20the%20driver%27s%20identity.%20However%2C%20the%20traditional%20parallel-style%0Aapproach%20of%20combining%20multiple%20single-task%20models%20tends%20to%20waste%20resources%20when%0Adealing%20with%20similar%20tasks.%20Therefore%2C%20we%20propose%20a%20novel%20tree-style%20multi-task%0Amodeling%20approach%20for%20multi-task%20learning%2C%20which%20rooted%20at%20a%20shared%20backbone%2C%0Amore%20dedicated%20separate%20module%20branches%20are%20appended%20as%20the%20model%20pipeline%20goes%0Adeeper.%20Following%20the%20tree-style%20approach%2C%20we%20propose%20a%20multi-task%20learning%0Amodel%20for%20simultaneously%20performing%20driver%20fatigue%20detection%20and%20face%0Arecognition%20for%20identifying%20a%20driver.%20This%20model%20shares%20a%20common%20feature%0Aextraction%20backbone%20module%2C%20with%20further%20separated%20feature%20extraction%20and%0Aclassification%20module%20branches.%20The%20dedicated%20branches%20exploit%20and%20combine%0Aspatial%20and%20channel%20attention%20mechanisms%20to%20generate%20space-channel%0Afused-attention%20enhanced%20features%2C%20leading%20to%20improved%20detection%20performance.%0AAs%20only%20single-task%20datasets%20are%20available%2C%20we%20introduce%20techniques%20including%0Aalternating%20updation%20and%20gradient%20accumulation%20for%20training%20our%20multi-task%0Amodel%20using%20only%20the%20single-task%20datasets.%20The%20effectiveness%20of%20our%20tree-style%0Amulti-task%20learning%20model%20is%20verified%20through%20extensive%20validations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07845v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Task%2520Learning%2520for%2520Fatigue%2520Detection%2520and%2520Face%2520Recognition%2520of%250A%2520%2520Drivers%2520via%2520Tree-Style%2520Space-Channel%2520Attention%2520Fusion%2520Network%26entry.906535625%3DShulei%2520Qu%2520and%2520Zhenguo%2520Gao%2520and%2520Xiaowei%2520Chen%2520and%2520Na%2520Li%2520and%2520Yakai%2520Wang%2520and%2520Xiaoxiao%2520Wu%26entry.1292438233%3D%2520%2520In%2520driving%2520scenarios%252C%2520automobile%2520active%2520safety%2520systems%2520are%2520increasingly%250Aincorporating%2520deep%2520learning%2520technology.%2520These%2520systems%2520typically%2520need%2520to%2520handle%250Amultiple%2520tasks%2520simultaneously%252C%2520such%2520as%2520detecting%2520fatigue%2520driving%2520and%250Arecognizing%2520the%2520driver%2527s%2520identity.%2520However%252C%2520the%2520traditional%2520parallel-style%250Aapproach%2520of%2520combining%2520multiple%2520single-task%2520models%2520tends%2520to%2520waste%2520resources%2520when%250Adealing%2520with%2520similar%2520tasks.%2520Therefore%252C%2520we%2520propose%2520a%2520novel%2520tree-style%2520multi-task%250Amodeling%2520approach%2520for%2520multi-task%2520learning%252C%2520which%2520rooted%2520at%2520a%2520shared%2520backbone%252C%250Amore%2520dedicated%2520separate%2520module%2520branches%2520are%2520appended%2520as%2520the%2520model%2520pipeline%2520goes%250Adeeper.%2520Following%2520the%2520tree-style%2520approach%252C%2520we%2520propose%2520a%2520multi-task%2520learning%250Amodel%2520for%2520simultaneously%2520performing%2520driver%2520fatigue%2520detection%2520and%2520face%250Arecognition%2520for%2520identifying%2520a%2520driver.%2520This%2520model%2520shares%2520a%2520common%2520feature%250Aextraction%2520backbone%2520module%252C%2520with%2520further%2520separated%2520feature%2520extraction%2520and%250Aclassification%2520module%2520branches.%2520The%2520dedicated%2520branches%2520exploit%2520and%2520combine%250Aspatial%2520and%2520channel%2520attention%2520mechanisms%2520to%2520generate%2520space-channel%250Afused-attention%2520enhanced%2520features%252C%2520leading%2520to%2520improved%2520detection%2520performance.%250AAs%2520only%2520single-task%2520datasets%2520are%2520available%252C%2520we%2520introduce%2520techniques%2520including%250Aalternating%2520updation%2520and%2520gradient%2520accumulation%2520for%2520training%2520our%2520multi-task%250Amodel%2520using%2520only%2520the%2520single-task%2520datasets.%2520The%2520effectiveness%2520of%2520our%2520tree-style%250Amulti-task%2520learning%2520model%2520is%2520verified%2520through%2520extensive%2520validations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07845v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Task%20Learning%20for%20Fatigue%20Detection%20and%20Face%20Recognition%20of%0A%20%20Drivers%20via%20Tree-Style%20Space-Channel%20Attention%20Fusion%20Network&entry.906535625=Shulei%20Qu%20and%20Zhenguo%20Gao%20and%20Xiaowei%20Chen%20and%20Na%20Li%20and%20Yakai%20Wang%20and%20Xiaoxiao%20Wu&entry.1292438233=%20%20In%20driving%20scenarios%2C%20automobile%20active%20safety%20systems%20are%20increasingly%0Aincorporating%20deep%20learning%20technology.%20These%20systems%20typically%20need%20to%20handle%0Amultiple%20tasks%20simultaneously%2C%20such%20as%20detecting%20fatigue%20driving%20and%0Arecognizing%20the%20driver%27s%20identity.%20However%2C%20the%20traditional%20parallel-style%0Aapproach%20of%20combining%20multiple%20single-task%20models%20tends%20to%20waste%20resources%20when%0Adealing%20with%20similar%20tasks.%20Therefore%2C%20we%20propose%20a%20novel%20tree-style%20multi-task%0Amodeling%20approach%20for%20multi-task%20learning%2C%20which%20rooted%20at%20a%20shared%20backbone%2C%0Amore%20dedicated%20separate%20module%20branches%20are%20appended%20as%20the%20model%20pipeline%20goes%0Adeeper.%20Following%20the%20tree-style%20approach%2C%20we%20propose%20a%20multi-task%20learning%0Amodel%20for%20simultaneously%20performing%20driver%20fatigue%20detection%20and%20face%0Arecognition%20for%20identifying%20a%20driver.%20This%20model%20shares%20a%20common%20feature%0Aextraction%20backbone%20module%2C%20with%20further%20separated%20feature%20extraction%20and%0Aclassification%20module%20branches.%20The%20dedicated%20branches%20exploit%20and%20combine%0Aspatial%20and%20channel%20attention%20mechanisms%20to%20generate%20space-channel%0Afused-attention%20enhanced%20features%2C%20leading%20to%20improved%20detection%20performance.%0AAs%20only%20single-task%20datasets%20are%20available%2C%20we%20introduce%20techniques%20including%0Aalternating%20updation%20and%20gradient%20accumulation%20for%20training%20our%20multi-task%0Amodel%20using%20only%20the%20single-task%20datasets.%20The%20effectiveness%20of%20our%20tree-style%0Amulti-task%20learning%20model%20is%20verified%20through%20extensive%20validations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07845v1&entry.124074799=Read"},
{"title": "FreeVA: Offline MLLM as Training-Free Video Assistant", "author": "Wenhao Wu", "abstract": "  This paper undertakes an empirical study to revisit the latest advancements\nin Multimodal Large Language Models (MLLMs): Video Assistant. This study,\nnamely FreeVA, aims to extend existing image-based MLLM to the video domain in\na training-free manner. The study provides an essential, yet must-know\nbaseline, and reveals several surprising findings: 1) FreeVA, leveraging only\noffline image-based MLLM without additional training, excels in zero-shot video\nquestion-answering (e.g., MSVD-QA, ActivityNet-QA, and MSRVTT-QA), even\nsurpassing state-of-the-art methods that involve video instruction tuning. 2)\nWhile mainstream video-based MLLMs typically initialize with an image-based\nMLLM (e.g., LLaVA) and then fine-tune using video instruction tuning, the study\nindicates that utilizing the widely adopted VideoInstruct-100K for video\ninstruction tuning doesn't actually lead to better performance compared to not\ntraining at all. 3) The commonly used evaluation metrics in existing works are\nsignificantly influenced by changes in the GPT API version over time. If\nignored, this could affect the fairness and uniformity of comparisons between\ndifferent methods and impact the analysis and judgment of researchers in the\nfield. The advancement of MLLMs is currently thriving, drawing numerous\nresearchers into the field. We aim for this work to serve as a plug-and-play,\nsimple yet effective baseline, encouraging the direct evaluation of existing\nMLLMs in video domain while also standardizing the field of video\nconversational models to a certain extent. Also, we encourage researchers to\nreconsider: Have current video MLLM methods truly acquired knowledge beyond\nimage MLLM? Code is available at https://github.com/whwu95/FreeVA\n", "link": "http://arxiv.org/abs/2405.07798v1", "date": "2024-05-13", "relevancy": 2.097, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5326}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5246}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeVA%3A%20Offline%20MLLM%20as%20Training-Free%20Video%20Assistant&body=Title%3A%20FreeVA%3A%20Offline%20MLLM%20as%20Training-Free%20Video%20Assistant%0AAuthor%3A%20Wenhao%20Wu%0AAbstract%3A%20%20%20This%20paper%20undertakes%20an%20empirical%20study%20to%20revisit%20the%20latest%20advancements%0Ain%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%3A%20Video%20Assistant.%20This%20study%2C%0Anamely%20FreeVA%2C%20aims%20to%20extend%20existing%20image-based%20MLLM%20to%20the%20video%20domain%20in%0Aa%20training-free%20manner.%20The%20study%20provides%20an%20essential%2C%20yet%20must-know%0Abaseline%2C%20and%20reveals%20several%20surprising%20findings%3A%201%29%20FreeVA%2C%20leveraging%20only%0Aoffline%20image-based%20MLLM%20without%20additional%20training%2C%20excels%20in%20zero-shot%20video%0Aquestion-answering%20%28e.g.%2C%20MSVD-QA%2C%20ActivityNet-QA%2C%20and%20MSRVTT-QA%29%2C%20even%0Asurpassing%20state-of-the-art%20methods%20that%20involve%20video%20instruction%20tuning.%202%29%0AWhile%20mainstream%20video-based%20MLLMs%20typically%20initialize%20with%20an%20image-based%0AMLLM%20%28e.g.%2C%20LLaVA%29%20and%20then%20fine-tune%20using%20video%20instruction%20tuning%2C%20the%20study%0Aindicates%20that%20utilizing%20the%20widely%20adopted%20VideoInstruct-100K%20for%20video%0Ainstruction%20tuning%20doesn%27t%20actually%20lead%20to%20better%20performance%20compared%20to%20not%0Atraining%20at%20all.%203%29%20The%20commonly%20used%20evaluation%20metrics%20in%20existing%20works%20are%0Asignificantly%20influenced%20by%20changes%20in%20the%20GPT%20API%20version%20over%20time.%20If%0Aignored%2C%20this%20could%20affect%20the%20fairness%20and%20uniformity%20of%20comparisons%20between%0Adifferent%20methods%20and%20impact%20the%20analysis%20and%20judgment%20of%20researchers%20in%20the%0Afield.%20The%20advancement%20of%20MLLMs%20is%20currently%20thriving%2C%20drawing%20numerous%0Aresearchers%20into%20the%20field.%20We%20aim%20for%20this%20work%20to%20serve%20as%20a%20plug-and-play%2C%0Asimple%20yet%20effective%20baseline%2C%20encouraging%20the%20direct%20evaluation%20of%20existing%0AMLLMs%20in%20video%20domain%20while%20also%20standardizing%20the%20field%20of%20video%0Aconversational%20models%20to%20a%20certain%20extent.%20Also%2C%20we%20encourage%20researchers%20to%0Areconsider%3A%20Have%20current%20video%20MLLM%20methods%20truly%20acquired%20knowledge%20beyond%0Aimage%20MLLM%3F%20Code%20is%20available%20at%20https%3A//github.com/whwu95/FreeVA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeVA%253A%2520Offline%2520MLLM%2520as%2520Training-Free%2520Video%2520Assistant%26entry.906535625%3DWenhao%2520Wu%26entry.1292438233%3D%2520%2520This%2520paper%2520undertakes%2520an%2520empirical%2520study%2520to%2520revisit%2520the%2520latest%2520advancements%250Ain%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%253A%2520Video%2520Assistant.%2520This%2520study%252C%250Anamely%2520FreeVA%252C%2520aims%2520to%2520extend%2520existing%2520image-based%2520MLLM%2520to%2520the%2520video%2520domain%2520in%250Aa%2520training-free%2520manner.%2520The%2520study%2520provides%2520an%2520essential%252C%2520yet%2520must-know%250Abaseline%252C%2520and%2520reveals%2520several%2520surprising%2520findings%253A%25201%2529%2520FreeVA%252C%2520leveraging%2520only%250Aoffline%2520image-based%2520MLLM%2520without%2520additional%2520training%252C%2520excels%2520in%2520zero-shot%2520video%250Aquestion-answering%2520%2528e.g.%252C%2520MSVD-QA%252C%2520ActivityNet-QA%252C%2520and%2520MSRVTT-QA%2529%252C%2520even%250Asurpassing%2520state-of-the-art%2520methods%2520that%2520involve%2520video%2520instruction%2520tuning.%25202%2529%250AWhile%2520mainstream%2520video-based%2520MLLMs%2520typically%2520initialize%2520with%2520an%2520image-based%250AMLLM%2520%2528e.g.%252C%2520LLaVA%2529%2520and%2520then%2520fine-tune%2520using%2520video%2520instruction%2520tuning%252C%2520the%2520study%250Aindicates%2520that%2520utilizing%2520the%2520widely%2520adopted%2520VideoInstruct-100K%2520for%2520video%250Ainstruction%2520tuning%2520doesn%2527t%2520actually%2520lead%2520to%2520better%2520performance%2520compared%2520to%2520not%250Atraining%2520at%2520all.%25203%2529%2520The%2520commonly%2520used%2520evaluation%2520metrics%2520in%2520existing%2520works%2520are%250Asignificantly%2520influenced%2520by%2520changes%2520in%2520the%2520GPT%2520API%2520version%2520over%2520time.%2520If%250Aignored%252C%2520this%2520could%2520affect%2520the%2520fairness%2520and%2520uniformity%2520of%2520comparisons%2520between%250Adifferent%2520methods%2520and%2520impact%2520the%2520analysis%2520and%2520judgment%2520of%2520researchers%2520in%2520the%250Afield.%2520The%2520advancement%2520of%2520MLLMs%2520is%2520currently%2520thriving%252C%2520drawing%2520numerous%250Aresearchers%2520into%2520the%2520field.%2520We%2520aim%2520for%2520this%2520work%2520to%2520serve%2520as%2520a%2520plug-and-play%252C%250Asimple%2520yet%2520effective%2520baseline%252C%2520encouraging%2520the%2520direct%2520evaluation%2520of%2520existing%250AMLLMs%2520in%2520video%2520domain%2520while%2520also%2520standardizing%2520the%2520field%2520of%2520video%250Aconversational%2520models%2520to%2520a%2520certain%2520extent.%2520Also%252C%2520we%2520encourage%2520researchers%2520to%250Areconsider%253A%2520Have%2520current%2520video%2520MLLM%2520methods%2520truly%2520acquired%2520knowledge%2520beyond%250Aimage%2520MLLM%253F%2520Code%2520is%2520available%2520at%2520https%253A//github.com/whwu95/FreeVA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeVA%3A%20Offline%20MLLM%20as%20Training-Free%20Video%20Assistant&entry.906535625=Wenhao%20Wu&entry.1292438233=%20%20This%20paper%20undertakes%20an%20empirical%20study%20to%20revisit%20the%20latest%20advancements%0Ain%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%3A%20Video%20Assistant.%20This%20study%2C%0Anamely%20FreeVA%2C%20aims%20to%20extend%20existing%20image-based%20MLLM%20to%20the%20video%20domain%20in%0Aa%20training-free%20manner.%20The%20study%20provides%20an%20essential%2C%20yet%20must-know%0Abaseline%2C%20and%20reveals%20several%20surprising%20findings%3A%201%29%20FreeVA%2C%20leveraging%20only%0Aoffline%20image-based%20MLLM%20without%20additional%20training%2C%20excels%20in%20zero-shot%20video%0Aquestion-answering%20%28e.g.%2C%20MSVD-QA%2C%20ActivityNet-QA%2C%20and%20MSRVTT-QA%29%2C%20even%0Asurpassing%20state-of-the-art%20methods%20that%20involve%20video%20instruction%20tuning.%202%29%0AWhile%20mainstream%20video-based%20MLLMs%20typically%20initialize%20with%20an%20image-based%0AMLLM%20%28e.g.%2C%20LLaVA%29%20and%20then%20fine-tune%20using%20video%20instruction%20tuning%2C%20the%20study%0Aindicates%20that%20utilizing%20the%20widely%20adopted%20VideoInstruct-100K%20for%20video%0Ainstruction%20tuning%20doesn%27t%20actually%20lead%20to%20better%20performance%20compared%20to%20not%0Atraining%20at%20all.%203%29%20The%20commonly%20used%20evaluation%20metrics%20in%20existing%20works%20are%0Asignificantly%20influenced%20by%20changes%20in%20the%20GPT%20API%20version%20over%20time.%20If%0Aignored%2C%20this%20could%20affect%20the%20fairness%20and%20uniformity%20of%20comparisons%20between%0Adifferent%20methods%20and%20impact%20the%20analysis%20and%20judgment%20of%20researchers%20in%20the%0Afield.%20The%20advancement%20of%20MLLMs%20is%20currently%20thriving%2C%20drawing%20numerous%0Aresearchers%20into%20the%20field.%20We%20aim%20for%20this%20work%20to%20serve%20as%20a%20plug-and-play%2C%0Asimple%20yet%20effective%20baseline%2C%20encouraging%20the%20direct%20evaluation%20of%20existing%0AMLLMs%20in%20video%20domain%20while%20also%20standardizing%20the%20field%20of%20video%0Aconversational%20models%20to%20a%20certain%20extent.%20Also%2C%20we%20encourage%20researchers%20to%0Areconsider%3A%20Have%20current%20video%20MLLM%20methods%20truly%20acquired%20knowledge%20beyond%0Aimage%20MLLM%3F%20Code%20is%20available%20at%20https%3A//github.com/whwu95/FreeVA%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07798v1&entry.124074799=Read"},
{"title": "Investigating the Semantic Robustness of CLIP-based Zero-Shot Anomaly\n  Segmentation", "author": "Kevin Stangl and Marius Arvinte and Weilin Xu and Cory Cornelius", "abstract": "  Zero-shot anomaly segmentation using pre-trained foundation models is a\npromising approach that enables effective algorithms without expensive,\ndomain-specific training or fine-tuning. Ensuring that these methods work\nacross various environmental conditions and are robust to distribution shifts\nis an open problem. We investigate the performance of WinCLIP [14] zero-shot\nanomaly segmentation algorithm by perturbing test data using three semantic\ntransformations: bounded angular rotations, bounded saturation shifts, and hue\nshifts. We empirically measure a lower performance bound by aggregating across\nper-sample worst-case perturbations and find that average performance drops by\nup to 20% in area under the ROC curve and 40% in area under the per-region\noverlap curve. We find that performance is consistently lowered on three CLIP\nbackbones, regardless of model architecture or learning objective,\ndemonstrating a need for careful performance evaluation.\n", "link": "http://arxiv.org/abs/2405.07969v1", "date": "2024-05-13", "relevancy": 2.0946, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5576}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5001}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20the%20Semantic%20Robustness%20of%20CLIP-based%20Zero-Shot%20Anomaly%0A%20%20Segmentation&body=Title%3A%20Investigating%20the%20Semantic%20Robustness%20of%20CLIP-based%20Zero-Shot%20Anomaly%0A%20%20Segmentation%0AAuthor%3A%20Kevin%20Stangl%20and%20Marius%20Arvinte%20and%20Weilin%20Xu%20and%20Cory%20Cornelius%0AAbstract%3A%20%20%20Zero-shot%20anomaly%20segmentation%20using%20pre-trained%20foundation%20models%20is%20a%0Apromising%20approach%20that%20enables%20effective%20algorithms%20without%20expensive%2C%0Adomain-specific%20training%20or%20fine-tuning.%20Ensuring%20that%20these%20methods%20work%0Aacross%20various%20environmental%20conditions%20and%20are%20robust%20to%20distribution%20shifts%0Ais%20an%20open%20problem.%20We%20investigate%20the%20performance%20of%20WinCLIP%20%5B14%5D%20zero-shot%0Aanomaly%20segmentation%20algorithm%20by%20perturbing%20test%20data%20using%20three%20semantic%0Atransformations%3A%20bounded%20angular%20rotations%2C%20bounded%20saturation%20shifts%2C%20and%20hue%0Ashifts.%20We%20empirically%20measure%20a%20lower%20performance%20bound%20by%20aggregating%20across%0Aper-sample%20worst-case%20perturbations%20and%20find%20that%20average%20performance%20drops%20by%0Aup%20to%2020%25%20in%20area%20under%20the%20ROC%20curve%20and%2040%25%20in%20area%20under%20the%20per-region%0Aoverlap%20curve.%20We%20find%20that%20performance%20is%20consistently%20lowered%20on%20three%20CLIP%0Abackbones%2C%20regardless%20of%20model%20architecture%20or%20learning%20objective%2C%0Ademonstrating%20a%20need%20for%20careful%20performance%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520the%2520Semantic%2520Robustness%2520of%2520CLIP-based%2520Zero-Shot%2520Anomaly%250A%2520%2520Segmentation%26entry.906535625%3DKevin%2520Stangl%2520and%2520Marius%2520Arvinte%2520and%2520Weilin%2520Xu%2520and%2520Cory%2520Cornelius%26entry.1292438233%3D%2520%2520Zero-shot%2520anomaly%2520segmentation%2520using%2520pre-trained%2520foundation%2520models%2520is%2520a%250Apromising%2520approach%2520that%2520enables%2520effective%2520algorithms%2520without%2520expensive%252C%250Adomain-specific%2520training%2520or%2520fine-tuning.%2520Ensuring%2520that%2520these%2520methods%2520work%250Aacross%2520various%2520environmental%2520conditions%2520and%2520are%2520robust%2520to%2520distribution%2520shifts%250Ais%2520an%2520open%2520problem.%2520We%2520investigate%2520the%2520performance%2520of%2520WinCLIP%2520%255B14%255D%2520zero-shot%250Aanomaly%2520segmentation%2520algorithm%2520by%2520perturbing%2520test%2520data%2520using%2520three%2520semantic%250Atransformations%253A%2520bounded%2520angular%2520rotations%252C%2520bounded%2520saturation%2520shifts%252C%2520and%2520hue%250Ashifts.%2520We%2520empirically%2520measure%2520a%2520lower%2520performance%2520bound%2520by%2520aggregating%2520across%250Aper-sample%2520worst-case%2520perturbations%2520and%2520find%2520that%2520average%2520performance%2520drops%2520by%250Aup%2520to%252020%2525%2520in%2520area%2520under%2520the%2520ROC%2520curve%2520and%252040%2525%2520in%2520area%2520under%2520the%2520per-region%250Aoverlap%2520curve.%2520We%2520find%2520that%2520performance%2520is%2520consistently%2520lowered%2520on%2520three%2520CLIP%250Abackbones%252C%2520regardless%2520of%2520model%2520architecture%2520or%2520learning%2520objective%252C%250Ademonstrating%2520a%2520need%2520for%2520careful%2520performance%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20the%20Semantic%20Robustness%20of%20CLIP-based%20Zero-Shot%20Anomaly%0A%20%20Segmentation&entry.906535625=Kevin%20Stangl%20and%20Marius%20Arvinte%20and%20Weilin%20Xu%20and%20Cory%20Cornelius&entry.1292438233=%20%20Zero-shot%20anomaly%20segmentation%20using%20pre-trained%20foundation%20models%20is%20a%0Apromising%20approach%20that%20enables%20effective%20algorithms%20without%20expensive%2C%0Adomain-specific%20training%20or%20fine-tuning.%20Ensuring%20that%20these%20methods%20work%0Aacross%20various%20environmental%20conditions%20and%20are%20robust%20to%20distribution%20shifts%0Ais%20an%20open%20problem.%20We%20investigate%20the%20performance%20of%20WinCLIP%20%5B14%5D%20zero-shot%0Aanomaly%20segmentation%20algorithm%20by%20perturbing%20test%20data%20using%20three%20semantic%0Atransformations%3A%20bounded%20angular%20rotations%2C%20bounded%20saturation%20shifts%2C%20and%20hue%0Ashifts.%20We%20empirically%20measure%20a%20lower%20performance%20bound%20by%20aggregating%20across%0Aper-sample%20worst-case%20perturbations%20and%20find%20that%20average%20performance%20drops%20by%0Aup%20to%2020%25%20in%20area%20under%20the%20ROC%20curve%20and%2040%25%20in%20area%20under%20the%20per-region%0Aoverlap%20curve.%20We%20find%20that%20performance%20is%20consistently%20lowered%20on%20three%20CLIP%0Abackbones%2C%20regardless%20of%20model%20architecture%20or%20learning%20objective%2C%0Ademonstrating%20a%20need%20for%20careful%20performance%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07969v1&entry.124074799=Read"},
{"title": "Establishing a Unified Evaluation Framework for Human Motion Generation:\n  A Comparative Analysis of Metrics", "author": "Ali Ismail-Fawaz and Maxime Devanne and Stefano Berretti and Jonathan Weber and Germain Forestier", "abstract": "  The development of generative artificial intelligence for human motion\ngeneration has expanded rapidly, necessitating a unified evaluation framework.\nThis paper presents a detailed review of eight evaluation metrics for human\nmotion generation, highlighting their unique features and shortcomings. We\npropose standardized practices through a unified evaluation setup to facilitate\nconsistent model comparisons. Additionally, we introduce a novel metric that\nassesses diversity in temporal distortion by analyzing warping diversity,\nthereby enhancing the evaluation of temporal data. We also conduct experimental\nanalyses of three generative models using a publicly available dataset,\noffering insights into the interpretation of each metric in specific case\nscenarios. Our goal is to offer a clear, user-friendly evaluation framework for\nnewcomers, complemented by publicly accessible code.\n", "link": "http://arxiv.org/abs/2405.07680v1", "date": "2024-05-13", "relevancy": 2.0864, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5258}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5223}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Establishing%20a%20Unified%20Evaluation%20Framework%20for%20Human%20Motion%20Generation%3A%0A%20%20A%20Comparative%20Analysis%20of%20Metrics&body=Title%3A%20Establishing%20a%20Unified%20Evaluation%20Framework%20for%20Human%20Motion%20Generation%3A%0A%20%20A%20Comparative%20Analysis%20of%20Metrics%0AAuthor%3A%20Ali%20Ismail-Fawaz%20and%20Maxime%20Devanne%20and%20Stefano%20Berretti%20and%20Jonathan%20Weber%20and%20Germain%20Forestier%0AAbstract%3A%20%20%20The%20development%20of%20generative%20artificial%20intelligence%20for%20human%20motion%0Ageneration%20has%20expanded%20rapidly%2C%20necessitating%20a%20unified%20evaluation%20framework.%0AThis%20paper%20presents%20a%20detailed%20review%20of%20eight%20evaluation%20metrics%20for%20human%0Amotion%20generation%2C%20highlighting%20their%20unique%20features%20and%20shortcomings.%20We%0Apropose%20standardized%20practices%20through%20a%20unified%20evaluation%20setup%20to%20facilitate%0Aconsistent%20model%20comparisons.%20Additionally%2C%20we%20introduce%20a%20novel%20metric%20that%0Aassesses%20diversity%20in%20temporal%20distortion%20by%20analyzing%20warping%20diversity%2C%0Athereby%20enhancing%20the%20evaluation%20of%20temporal%20data.%20We%20also%20conduct%20experimental%0Aanalyses%20of%20three%20generative%20models%20using%20a%20publicly%20available%20dataset%2C%0Aoffering%20insights%20into%20the%20interpretation%20of%20each%20metric%20in%20specific%20case%0Ascenarios.%20Our%20goal%20is%20to%20offer%20a%20clear%2C%20user-friendly%20evaluation%20framework%20for%0Anewcomers%2C%20complemented%20by%20publicly%20accessible%20code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstablishing%2520a%2520Unified%2520Evaluation%2520Framework%2520for%2520Human%2520Motion%2520Generation%253A%250A%2520%2520A%2520Comparative%2520Analysis%2520of%2520Metrics%26entry.906535625%3DAli%2520Ismail-Fawaz%2520and%2520Maxime%2520Devanne%2520and%2520Stefano%2520Berretti%2520and%2520Jonathan%2520Weber%2520and%2520Germain%2520Forestier%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520generative%2520artificial%2520intelligence%2520for%2520human%2520motion%250Ageneration%2520has%2520expanded%2520rapidly%252C%2520necessitating%2520a%2520unified%2520evaluation%2520framework.%250AThis%2520paper%2520presents%2520a%2520detailed%2520review%2520of%2520eight%2520evaluation%2520metrics%2520for%2520human%250Amotion%2520generation%252C%2520highlighting%2520their%2520unique%2520features%2520and%2520shortcomings.%2520We%250Apropose%2520standardized%2520practices%2520through%2520a%2520unified%2520evaluation%2520setup%2520to%2520facilitate%250Aconsistent%2520model%2520comparisons.%2520Additionally%252C%2520we%2520introduce%2520a%2520novel%2520metric%2520that%250Aassesses%2520diversity%2520in%2520temporal%2520distortion%2520by%2520analyzing%2520warping%2520diversity%252C%250Athereby%2520enhancing%2520the%2520evaluation%2520of%2520temporal%2520data.%2520We%2520also%2520conduct%2520experimental%250Aanalyses%2520of%2520three%2520generative%2520models%2520using%2520a%2520publicly%2520available%2520dataset%252C%250Aoffering%2520insights%2520into%2520the%2520interpretation%2520of%2520each%2520metric%2520in%2520specific%2520case%250Ascenarios.%2520Our%2520goal%2520is%2520to%2520offer%2520a%2520clear%252C%2520user-friendly%2520evaluation%2520framework%2520for%250Anewcomers%252C%2520complemented%2520by%2520publicly%2520accessible%2520code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Establishing%20a%20Unified%20Evaluation%20Framework%20for%20Human%20Motion%20Generation%3A%0A%20%20A%20Comparative%20Analysis%20of%20Metrics&entry.906535625=Ali%20Ismail-Fawaz%20and%20Maxime%20Devanne%20and%20Stefano%20Berretti%20and%20Jonathan%20Weber%20and%20Germain%20Forestier&entry.1292438233=%20%20The%20development%20of%20generative%20artificial%20intelligence%20for%20human%20motion%0Ageneration%20has%20expanded%20rapidly%2C%20necessitating%20a%20unified%20evaluation%20framework.%0AThis%20paper%20presents%20a%20detailed%20review%20of%20eight%20evaluation%20metrics%20for%20human%0Amotion%20generation%2C%20highlighting%20their%20unique%20features%20and%20shortcomings.%20We%0Apropose%20standardized%20practices%20through%20a%20unified%20evaluation%20setup%20to%20facilitate%0Aconsistent%20model%20comparisons.%20Additionally%2C%20we%20introduce%20a%20novel%20metric%20that%0Aassesses%20diversity%20in%20temporal%20distortion%20by%20analyzing%20warping%20diversity%2C%0Athereby%20enhancing%20the%20evaluation%20of%20temporal%20data.%20We%20also%20conduct%20experimental%0Aanalyses%20of%20three%20generative%20models%20using%20a%20publicly%20available%20dataset%2C%0Aoffering%20insights%20into%20the%20interpretation%20of%20each%20metric%20in%20specific%20case%0Ascenarios.%20Our%20goal%20is%20to%20offer%20a%20clear%2C%20user-friendly%20evaluation%20framework%20for%0Anewcomers%2C%20complemented%20by%20publicly%20accessible%20code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07680v1&entry.124074799=Read"},
{"title": "Domain Generalisation for Object Detection under Covariate and Concept\n  Shift", "author": "Karthik Seemakurthy and Erchan Aptoula and Charles Fox and Petra Bosilj", "abstract": "  Domain generalisation aims to promote the learning of domain-invariant\nfeatures while suppressing domain-specific features, so that a model can\ngeneralise better to previously unseen target domains. An approach to domain\ngeneralisation for object detection is proposed, the first such approach\napplicable to any object detection architecture. Based on a rigorous\nmathematical analysis, we extend approaches based on feature alignment with a\nnovel component for performing class conditional alignment at the instance\nlevel, in addition to aligning the marginal feature distributions across\ndomains at the image level. This allows us to fully address both components of\ndomain shift, i.e. covariate and concept shift, and learn a domain agnostic\nfeature representation. We perform extensive evaluation with both one-stage\n(FCOS, YOLO) and two-stage (FRCNN) detectors, on a newly proposed benchmark\ncomprising several different datasets for autonomous driving applications\n(Cityscapes, BDD10K, ACDC, IDD) as well as the GWHD dataset for precision\nagriculture, and show consistent improvements to the generalisation and\nlocalisation performance over baselines and state-of-the-art.\n", "link": "http://arxiv.org/abs/2203.05294v3", "date": "2024-05-13", "relevancy": 2.069, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5332}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5092}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain%20Generalisation%20for%20Object%20Detection%20under%20Covariate%20and%20Concept%0A%20%20Shift&body=Title%3A%20Domain%20Generalisation%20for%20Object%20Detection%20under%20Covariate%20and%20Concept%0A%20%20Shift%0AAuthor%3A%20Karthik%20Seemakurthy%20and%20Erchan%20Aptoula%20and%20Charles%20Fox%20and%20Petra%20Bosilj%0AAbstract%3A%20%20%20Domain%20generalisation%20aims%20to%20promote%20the%20learning%20of%20domain-invariant%0Afeatures%20while%20suppressing%20domain-specific%20features%2C%20so%20that%20a%20model%20can%0Ageneralise%20better%20to%20previously%20unseen%20target%20domains.%20An%20approach%20to%20domain%0Ageneralisation%20for%20object%20detection%20is%20proposed%2C%20the%20first%20such%20approach%0Aapplicable%20to%20any%20object%20detection%20architecture.%20Based%20on%20a%20rigorous%0Amathematical%20analysis%2C%20we%20extend%20approaches%20based%20on%20feature%20alignment%20with%20a%0Anovel%20component%20for%20performing%20class%20conditional%20alignment%20at%20the%20instance%0Alevel%2C%20in%20addition%20to%20aligning%20the%20marginal%20feature%20distributions%20across%0Adomains%20at%20the%20image%20level.%20This%20allows%20us%20to%20fully%20address%20both%20components%20of%0Adomain%20shift%2C%20i.e.%20covariate%20and%20concept%20shift%2C%20and%20learn%20a%20domain%20agnostic%0Afeature%20representation.%20We%20perform%20extensive%20evaluation%20with%20both%20one-stage%0A%28FCOS%2C%20YOLO%29%20and%20two-stage%20%28FRCNN%29%20detectors%2C%20on%20a%20newly%20proposed%20benchmark%0Acomprising%20several%20different%20datasets%20for%20autonomous%20driving%20applications%0A%28Cityscapes%2C%20BDD10K%2C%20ACDC%2C%20IDD%29%20as%20well%20as%20the%20GWHD%20dataset%20for%20precision%0Aagriculture%2C%20and%20show%20consistent%20improvements%20to%20the%20generalisation%20and%0Alocalisation%20performance%20over%20baselines%20and%20state-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2203.05294v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain%2520Generalisation%2520for%2520Object%2520Detection%2520under%2520Covariate%2520and%2520Concept%250A%2520%2520Shift%26entry.906535625%3DKarthik%2520Seemakurthy%2520and%2520Erchan%2520Aptoula%2520and%2520Charles%2520Fox%2520and%2520Petra%2520Bosilj%26entry.1292438233%3D%2520%2520Domain%2520generalisation%2520aims%2520to%2520promote%2520the%2520learning%2520of%2520domain-invariant%250Afeatures%2520while%2520suppressing%2520domain-specific%2520features%252C%2520so%2520that%2520a%2520model%2520can%250Ageneralise%2520better%2520to%2520previously%2520unseen%2520target%2520domains.%2520An%2520approach%2520to%2520domain%250Ageneralisation%2520for%2520object%2520detection%2520is%2520proposed%252C%2520the%2520first%2520such%2520approach%250Aapplicable%2520to%2520any%2520object%2520detection%2520architecture.%2520Based%2520on%2520a%2520rigorous%250Amathematical%2520analysis%252C%2520we%2520extend%2520approaches%2520based%2520on%2520feature%2520alignment%2520with%2520a%250Anovel%2520component%2520for%2520performing%2520class%2520conditional%2520alignment%2520at%2520the%2520instance%250Alevel%252C%2520in%2520addition%2520to%2520aligning%2520the%2520marginal%2520feature%2520distributions%2520across%250Adomains%2520at%2520the%2520image%2520level.%2520This%2520allows%2520us%2520to%2520fully%2520address%2520both%2520components%2520of%250Adomain%2520shift%252C%2520i.e.%2520covariate%2520and%2520concept%2520shift%252C%2520and%2520learn%2520a%2520domain%2520agnostic%250Afeature%2520representation.%2520We%2520perform%2520extensive%2520evaluation%2520with%2520both%2520one-stage%250A%2528FCOS%252C%2520YOLO%2529%2520and%2520two-stage%2520%2528FRCNN%2529%2520detectors%252C%2520on%2520a%2520newly%2520proposed%2520benchmark%250Acomprising%2520several%2520different%2520datasets%2520for%2520autonomous%2520driving%2520applications%250A%2528Cityscapes%252C%2520BDD10K%252C%2520ACDC%252C%2520IDD%2529%2520as%2520well%2520as%2520the%2520GWHD%2520dataset%2520for%2520precision%250Aagriculture%252C%2520and%2520show%2520consistent%2520improvements%2520to%2520the%2520generalisation%2520and%250Alocalisation%2520performance%2520over%2520baselines%2520and%2520state-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2203.05294v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Generalisation%20for%20Object%20Detection%20under%20Covariate%20and%20Concept%0A%20%20Shift&entry.906535625=Karthik%20Seemakurthy%20and%20Erchan%20Aptoula%20and%20Charles%20Fox%20and%20Petra%20Bosilj&entry.1292438233=%20%20Domain%20generalisation%20aims%20to%20promote%20the%20learning%20of%20domain-invariant%0Afeatures%20while%20suppressing%20domain-specific%20features%2C%20so%20that%20a%20model%20can%0Ageneralise%20better%20to%20previously%20unseen%20target%20domains.%20An%20approach%20to%20domain%0Ageneralisation%20for%20object%20detection%20is%20proposed%2C%20the%20first%20such%20approach%0Aapplicable%20to%20any%20object%20detection%20architecture.%20Based%20on%20a%20rigorous%0Amathematical%20analysis%2C%20we%20extend%20approaches%20based%20on%20feature%20alignment%20with%20a%0Anovel%20component%20for%20performing%20class%20conditional%20alignment%20at%20the%20instance%0Alevel%2C%20in%20addition%20to%20aligning%20the%20marginal%20feature%20distributions%20across%0Adomains%20at%20the%20image%20level.%20This%20allows%20us%20to%20fully%20address%20both%20components%20of%0Adomain%20shift%2C%20i.e.%20covariate%20and%20concept%20shift%2C%20and%20learn%20a%20domain%20agnostic%0Afeature%20representation.%20We%20perform%20extensive%20evaluation%20with%20both%20one-stage%0A%28FCOS%2C%20YOLO%29%20and%20two-stage%20%28FRCNN%29%20detectors%2C%20on%20a%20newly%20proposed%20benchmark%0Acomprising%20several%20different%20datasets%20for%20autonomous%20driving%20applications%0A%28Cityscapes%2C%20BDD10K%2C%20ACDC%2C%20IDD%29%20as%20well%20as%20the%20GWHD%20dataset%20for%20precision%0Aagriculture%2C%20and%20show%20consistent%20improvements%20to%20the%20generalisation%20and%0Alocalisation%20performance%20over%20baselines%20and%20state-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2203.05294v3&entry.124074799=Read"},
{"title": "Fully Embedded Time-Series Generative Adversarial Networks", "author": "Joe Beck and Subhadeep Chakraborty", "abstract": "  Generative Adversarial Networks (GANs) should produce synthetic data that\nfits the underlying distribution of the data being modeled. For real valued\ntime-series data, this implies the need to simultaneously capture the static\ndistribution of the data, but also the full temporal distribution of the data\nfor any potential time horizon. This temporal element produces a more complex\nproblem that can potentially leave current solutions under-constrained,\nunstable during training, or prone to varying degrees of mode collapse. In\nFETSGAN, entire sequences are translated directly to the generator's sampling\nspace using a seq2seq style adversarial auto encoder (AAE), where adversarial\ntraining is used to match the training distribution in both the feature space\nand the lower dimensional sampling space. This additional constraint provides a\nloose assurance that the temporal distribution of the synthetic samples will\nnot collapse. In addition, the First Above Threshold (FAT) operator is\nintroduced to supplement the reconstruction of encoded sequences, which\nimproves training stability and the overall quality of the synthetic data being\ngenerated. These novel contributions demonstrate a significant improvement to\nthe current state of the art for adversarial learners in qualitative measures\nof temporal similarity and quantitative predictive ability of data generated\nthrough FETSGAN.\n", "link": "http://arxiv.org/abs/2308.15730v2", "date": "2024-05-13", "relevancy": 2.0655, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5512}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.502}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fully%20Embedded%20Time-Series%20Generative%20Adversarial%20Networks&body=Title%3A%20Fully%20Embedded%20Time-Series%20Generative%20Adversarial%20Networks%0AAuthor%3A%20Joe%20Beck%20and%20Subhadeep%20Chakraborty%0AAbstract%3A%20%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20should%20produce%20synthetic%20data%20that%0Afits%20the%20underlying%20distribution%20of%20the%20data%20being%20modeled.%20For%20real%20valued%0Atime-series%20data%2C%20this%20implies%20the%20need%20to%20simultaneously%20capture%20the%20static%0Adistribution%20of%20the%20data%2C%20but%20also%20the%20full%20temporal%20distribution%20of%20the%20data%0Afor%20any%20potential%20time%20horizon.%20This%20temporal%20element%20produces%20a%20more%20complex%0Aproblem%20that%20can%20potentially%20leave%20current%20solutions%20under-constrained%2C%0Aunstable%20during%20training%2C%20or%20prone%20to%20varying%20degrees%20of%20mode%20collapse.%20In%0AFETSGAN%2C%20entire%20sequences%20are%20translated%20directly%20to%20the%20generator%27s%20sampling%0Aspace%20using%20a%20seq2seq%20style%20adversarial%20auto%20encoder%20%28AAE%29%2C%20where%20adversarial%0Atraining%20is%20used%20to%20match%20the%20training%20distribution%20in%20both%20the%20feature%20space%0Aand%20the%20lower%20dimensional%20sampling%20space.%20This%20additional%20constraint%20provides%20a%0Aloose%20assurance%20that%20the%20temporal%20distribution%20of%20the%20synthetic%20samples%20will%0Anot%20collapse.%20In%20addition%2C%20the%20First%20Above%20Threshold%20%28FAT%29%20operator%20is%0Aintroduced%20to%20supplement%20the%20reconstruction%20of%20encoded%20sequences%2C%20which%0Aimproves%20training%20stability%20and%20the%20overall%20quality%20of%20the%20synthetic%20data%20being%0Agenerated.%20These%20novel%20contributions%20demonstrate%20a%20significant%20improvement%20to%0Athe%20current%20state%20of%20the%20art%20for%20adversarial%20learners%20in%20qualitative%20measures%0Aof%20temporal%20similarity%20and%20quantitative%20predictive%20ability%20of%20data%20generated%0Athrough%20FETSGAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.15730v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFully%2520Embedded%2520Time-Series%2520Generative%2520Adversarial%2520Networks%26entry.906535625%3DJoe%2520Beck%2520and%2520Subhadeep%2520Chakraborty%26entry.1292438233%3D%2520%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520should%2520produce%2520synthetic%2520data%2520that%250Afits%2520the%2520underlying%2520distribution%2520of%2520the%2520data%2520being%2520modeled.%2520For%2520real%2520valued%250Atime-series%2520data%252C%2520this%2520implies%2520the%2520need%2520to%2520simultaneously%2520capture%2520the%2520static%250Adistribution%2520of%2520the%2520data%252C%2520but%2520also%2520the%2520full%2520temporal%2520distribution%2520of%2520the%2520data%250Afor%2520any%2520potential%2520time%2520horizon.%2520This%2520temporal%2520element%2520produces%2520a%2520more%2520complex%250Aproblem%2520that%2520can%2520potentially%2520leave%2520current%2520solutions%2520under-constrained%252C%250Aunstable%2520during%2520training%252C%2520or%2520prone%2520to%2520varying%2520degrees%2520of%2520mode%2520collapse.%2520In%250AFETSGAN%252C%2520entire%2520sequences%2520are%2520translated%2520directly%2520to%2520the%2520generator%2527s%2520sampling%250Aspace%2520using%2520a%2520seq2seq%2520style%2520adversarial%2520auto%2520encoder%2520%2528AAE%2529%252C%2520where%2520adversarial%250Atraining%2520is%2520used%2520to%2520match%2520the%2520training%2520distribution%2520in%2520both%2520the%2520feature%2520space%250Aand%2520the%2520lower%2520dimensional%2520sampling%2520space.%2520This%2520additional%2520constraint%2520provides%2520a%250Aloose%2520assurance%2520that%2520the%2520temporal%2520distribution%2520of%2520the%2520synthetic%2520samples%2520will%250Anot%2520collapse.%2520In%2520addition%252C%2520the%2520First%2520Above%2520Threshold%2520%2528FAT%2529%2520operator%2520is%250Aintroduced%2520to%2520supplement%2520the%2520reconstruction%2520of%2520encoded%2520sequences%252C%2520which%250Aimproves%2520training%2520stability%2520and%2520the%2520overall%2520quality%2520of%2520the%2520synthetic%2520data%2520being%250Agenerated.%2520These%2520novel%2520contributions%2520demonstrate%2520a%2520significant%2520improvement%2520to%250Athe%2520current%2520state%2520of%2520the%2520art%2520for%2520adversarial%2520learners%2520in%2520qualitative%2520measures%250Aof%2520temporal%2520similarity%2520and%2520quantitative%2520predictive%2520ability%2520of%2520data%2520generated%250Athrough%2520FETSGAN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.15730v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fully%20Embedded%20Time-Series%20Generative%20Adversarial%20Networks&entry.906535625=Joe%20Beck%20and%20Subhadeep%20Chakraborty&entry.1292438233=%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20should%20produce%20synthetic%20data%20that%0Afits%20the%20underlying%20distribution%20of%20the%20data%20being%20modeled.%20For%20real%20valued%0Atime-series%20data%2C%20this%20implies%20the%20need%20to%20simultaneously%20capture%20the%20static%0Adistribution%20of%20the%20data%2C%20but%20also%20the%20full%20temporal%20distribution%20of%20the%20data%0Afor%20any%20potential%20time%20horizon.%20This%20temporal%20element%20produces%20a%20more%20complex%0Aproblem%20that%20can%20potentially%20leave%20current%20solutions%20under-constrained%2C%0Aunstable%20during%20training%2C%20or%20prone%20to%20varying%20degrees%20of%20mode%20collapse.%20In%0AFETSGAN%2C%20entire%20sequences%20are%20translated%20directly%20to%20the%20generator%27s%20sampling%0Aspace%20using%20a%20seq2seq%20style%20adversarial%20auto%20encoder%20%28AAE%29%2C%20where%20adversarial%0Atraining%20is%20used%20to%20match%20the%20training%20distribution%20in%20both%20the%20feature%20space%0Aand%20the%20lower%20dimensional%20sampling%20space.%20This%20additional%20constraint%20provides%20a%0Aloose%20assurance%20that%20the%20temporal%20distribution%20of%20the%20synthetic%20samples%20will%0Anot%20collapse.%20In%20addition%2C%20the%20First%20Above%20Threshold%20%28FAT%29%20operator%20is%0Aintroduced%20to%20supplement%20the%20reconstruction%20of%20encoded%20sequences%2C%20which%0Aimproves%20training%20stability%20and%20the%20overall%20quality%20of%20the%20synthetic%20data%20being%0Agenerated.%20These%20novel%20contributions%20demonstrate%20a%20significant%20improvement%20to%0Athe%20current%20state%20of%20the%20art%20for%20adversarial%20learners%20in%20qualitative%20measures%0Aof%20temporal%20similarity%20and%20quantitative%20predictive%20ability%20of%20data%20generated%0Athrough%20FETSGAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.15730v2&entry.124074799=Read"},
{"title": "Synergistic Integration of Coordinate Network and Tensorial Feature for\n  Improving Neural Radiance Fields from Sparse Inputs", "author": "Mingyu Kim and Jun-Seong Kim and Se-Young Yun and Jin-Hwa Kim", "abstract": "  The multi-plane representation has been highlighted for its fast training and\ninference across static and dynamic neural radiance fields. This approach\nconstructs relevant features via projection onto learnable grids and\ninterpolating adjacent vertices. However, it has limitations in capturing\nlow-frequency details and tends to overuse parameters for low-frequency\nfeatures due to its bias toward fine details, despite its multi-resolution\nconcept. This phenomenon leads to instability and inefficiency when training\nposes are sparse. In this work, we propose a method that synergistically\nintegrates multi-plane representation with a coordinate-based network known for\nstrong bias toward low-frequency signals. The coordinate-based network is\nresponsible for capturing low-frequency details, while the multi-plane\nrepresentation focuses on capturing fine-grained details. We demonstrate that\nusing residual connections between them seamlessly preserves their own inherent\nproperties. Additionally, the proposed progressive training scheme accelerates\nthe disentanglement of these two features. We empirically show that the\nproposed method achieves comparable results to explicit encoding with fewer\nparameters, and particularly, it outperforms others for the static and dynamic\nNeRFs under sparse inputs.\n", "link": "http://arxiv.org/abs/2405.07857v1", "date": "2024-05-13", "relevancy": 2.0612, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5183}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5134}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synergistic%20Integration%20of%20Coordinate%20Network%20and%20Tensorial%20Feature%20for%0A%20%20Improving%20Neural%20Radiance%20Fields%20from%20Sparse%20Inputs&body=Title%3A%20Synergistic%20Integration%20of%20Coordinate%20Network%20and%20Tensorial%20Feature%20for%0A%20%20Improving%20Neural%20Radiance%20Fields%20from%20Sparse%20Inputs%0AAuthor%3A%20Mingyu%20Kim%20and%20Jun-Seong%20Kim%20and%20Se-Young%20Yun%20and%20Jin-Hwa%20Kim%0AAbstract%3A%20%20%20The%20multi-plane%20representation%20has%20been%20highlighted%20for%20its%20fast%20training%20and%0Ainference%20across%20static%20and%20dynamic%20neural%20radiance%20fields.%20This%20approach%0Aconstructs%20relevant%20features%20via%20projection%20onto%20learnable%20grids%20and%0Ainterpolating%20adjacent%20vertices.%20However%2C%20it%20has%20limitations%20in%20capturing%0Alow-frequency%20details%20and%20tends%20to%20overuse%20parameters%20for%20low-frequency%0Afeatures%20due%20to%20its%20bias%20toward%20fine%20details%2C%20despite%20its%20multi-resolution%0Aconcept.%20This%20phenomenon%20leads%20to%20instability%20and%20inefficiency%20when%20training%0Aposes%20are%20sparse.%20In%20this%20work%2C%20we%20propose%20a%20method%20that%20synergistically%0Aintegrates%20multi-plane%20representation%20with%20a%20coordinate-based%20network%20known%20for%0Astrong%20bias%20toward%20low-frequency%20signals.%20The%20coordinate-based%20network%20is%0Aresponsible%20for%20capturing%20low-frequency%20details%2C%20while%20the%20multi-plane%0Arepresentation%20focuses%20on%20capturing%20fine-grained%20details.%20We%20demonstrate%20that%0Ausing%20residual%20connections%20between%20them%20seamlessly%20preserves%20their%20own%20inherent%0Aproperties.%20Additionally%2C%20the%20proposed%20progressive%20training%20scheme%20accelerates%0Athe%20disentanglement%20of%20these%20two%20features.%20We%20empirically%20show%20that%20the%0Aproposed%20method%20achieves%20comparable%20results%20to%20explicit%20encoding%20with%20fewer%0Aparameters%2C%20and%20particularly%2C%20it%20outperforms%20others%20for%20the%20static%20and%20dynamic%0ANeRFs%20under%20sparse%20inputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07857v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynergistic%2520Integration%2520of%2520Coordinate%2520Network%2520and%2520Tensorial%2520Feature%2520for%250A%2520%2520Improving%2520Neural%2520Radiance%2520Fields%2520from%2520Sparse%2520Inputs%26entry.906535625%3DMingyu%2520Kim%2520and%2520Jun-Seong%2520Kim%2520and%2520Se-Young%2520Yun%2520and%2520Jin-Hwa%2520Kim%26entry.1292438233%3D%2520%2520The%2520multi-plane%2520representation%2520has%2520been%2520highlighted%2520for%2520its%2520fast%2520training%2520and%250Ainference%2520across%2520static%2520and%2520dynamic%2520neural%2520radiance%2520fields.%2520This%2520approach%250Aconstructs%2520relevant%2520features%2520via%2520projection%2520onto%2520learnable%2520grids%2520and%250Ainterpolating%2520adjacent%2520vertices.%2520However%252C%2520it%2520has%2520limitations%2520in%2520capturing%250Alow-frequency%2520details%2520and%2520tends%2520to%2520overuse%2520parameters%2520for%2520low-frequency%250Afeatures%2520due%2520to%2520its%2520bias%2520toward%2520fine%2520details%252C%2520despite%2520its%2520multi-resolution%250Aconcept.%2520This%2520phenomenon%2520leads%2520to%2520instability%2520and%2520inefficiency%2520when%2520training%250Aposes%2520are%2520sparse.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520method%2520that%2520synergistically%250Aintegrates%2520multi-plane%2520representation%2520with%2520a%2520coordinate-based%2520network%2520known%2520for%250Astrong%2520bias%2520toward%2520low-frequency%2520signals.%2520The%2520coordinate-based%2520network%2520is%250Aresponsible%2520for%2520capturing%2520low-frequency%2520details%252C%2520while%2520the%2520multi-plane%250Arepresentation%2520focuses%2520on%2520capturing%2520fine-grained%2520details.%2520We%2520demonstrate%2520that%250Ausing%2520residual%2520connections%2520between%2520them%2520seamlessly%2520preserves%2520their%2520own%2520inherent%250Aproperties.%2520Additionally%252C%2520the%2520proposed%2520progressive%2520training%2520scheme%2520accelerates%250Athe%2520disentanglement%2520of%2520these%2520two%2520features.%2520We%2520empirically%2520show%2520that%2520the%250Aproposed%2520method%2520achieves%2520comparable%2520results%2520to%2520explicit%2520encoding%2520with%2520fewer%250Aparameters%252C%2520and%2520particularly%252C%2520it%2520outperforms%2520others%2520for%2520the%2520static%2520and%2520dynamic%250ANeRFs%2520under%2520sparse%2520inputs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07857v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synergistic%20Integration%20of%20Coordinate%20Network%20and%20Tensorial%20Feature%20for%0A%20%20Improving%20Neural%20Radiance%20Fields%20from%20Sparse%20Inputs&entry.906535625=Mingyu%20Kim%20and%20Jun-Seong%20Kim%20and%20Se-Young%20Yun%20and%20Jin-Hwa%20Kim&entry.1292438233=%20%20The%20multi-plane%20representation%20has%20been%20highlighted%20for%20its%20fast%20training%20and%0Ainference%20across%20static%20and%20dynamic%20neural%20radiance%20fields.%20This%20approach%0Aconstructs%20relevant%20features%20via%20projection%20onto%20learnable%20grids%20and%0Ainterpolating%20adjacent%20vertices.%20However%2C%20it%20has%20limitations%20in%20capturing%0Alow-frequency%20details%20and%20tends%20to%20overuse%20parameters%20for%20low-frequency%0Afeatures%20due%20to%20its%20bias%20toward%20fine%20details%2C%20despite%20its%20multi-resolution%0Aconcept.%20This%20phenomenon%20leads%20to%20instability%20and%20inefficiency%20when%20training%0Aposes%20are%20sparse.%20In%20this%20work%2C%20we%20propose%20a%20method%20that%20synergistically%0Aintegrates%20multi-plane%20representation%20with%20a%20coordinate-based%20network%20known%20for%0Astrong%20bias%20toward%20low-frequency%20signals.%20The%20coordinate-based%20network%20is%0Aresponsible%20for%20capturing%20low-frequency%20details%2C%20while%20the%20multi-plane%0Arepresentation%20focuses%20on%20capturing%20fine-grained%20details.%20We%20demonstrate%20that%0Ausing%20residual%20connections%20between%20them%20seamlessly%20preserves%20their%20own%20inherent%0Aproperties.%20Additionally%2C%20the%20proposed%20progressive%20training%20scheme%20accelerates%0Athe%20disentanglement%20of%20these%20two%20features.%20We%20empirically%20show%20that%20the%0Aproposed%20method%20achieves%20comparable%20results%20to%20explicit%20encoding%20with%20fewer%0Aparameters%2C%20and%20particularly%2C%20it%20outperforms%20others%20for%20the%20static%20and%20dynamic%0ANeRFs%20under%20sparse%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07857v1&entry.124074799=Read"},
{"title": "Hijacking Context in Large Multi-modal Models", "author": "Joonhyun Jeong", "abstract": "  Recently, Large Multi-modal Models (LMMs) have demonstrated their ability to\nunderstand the visual contents of images given the instructions regarding the\nimages. Built upon the Large Language Models (LLMs), LMMs also inherit their\nabilities and characteristics such as in-context learning where a coherent\nsequence of images and texts are given as the input prompt. However, we\nidentify a new limitation of off-the-shelf LMMs where a small fraction of\nincoherent images or text descriptions mislead LMMs to only generate biased\noutput about the hijacked context, not the originally intended context. To\naddress this, we propose a pre-filtering method that removes irrelevant\ncontexts via GPT-4V, based on its robustness towards distribution shift within\nthe contexts. We further investigate whether replacing the hijacked visual and\ntextual contexts with the correlated ones via GPT-4V and text-to-image models\ncan help yield coherent responses.\n", "link": "http://arxiv.org/abs/2312.07553v2", "date": "2024-05-13", "relevancy": 2.0323, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5277}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.517}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hijacking%20Context%20in%20Large%20Multi-modal%20Models&body=Title%3A%20Hijacking%20Context%20in%20Large%20Multi-modal%20Models%0AAuthor%3A%20Joonhyun%20Jeong%0AAbstract%3A%20%20%20Recently%2C%20Large%20Multi-modal%20Models%20%28LMMs%29%20have%20demonstrated%20their%20ability%20to%0Aunderstand%20the%20visual%20contents%20of%20images%20given%20the%20instructions%20regarding%20the%0Aimages.%20Built%20upon%20the%20Large%20Language%20Models%20%28LLMs%29%2C%20LMMs%20also%20inherit%20their%0Aabilities%20and%20characteristics%20such%20as%20in-context%20learning%20where%20a%20coherent%0Asequence%20of%20images%20and%20texts%20are%20given%20as%20the%20input%20prompt.%20However%2C%20we%0Aidentify%20a%20new%20limitation%20of%20off-the-shelf%20LMMs%20where%20a%20small%20fraction%20of%0Aincoherent%20images%20or%20text%20descriptions%20mislead%20LMMs%20to%20only%20generate%20biased%0Aoutput%20about%20the%20hijacked%20context%2C%20not%20the%20originally%20intended%20context.%20To%0Aaddress%20this%2C%20we%20propose%20a%20pre-filtering%20method%20that%20removes%20irrelevant%0Acontexts%20via%20GPT-4V%2C%20based%20on%20its%20robustness%20towards%20distribution%20shift%20within%0Athe%20contexts.%20We%20further%20investigate%20whether%20replacing%20the%20hijacked%20visual%20and%0Atextual%20contexts%20with%20the%20correlated%20ones%20via%20GPT-4V%20and%20text-to-image%20models%0Acan%20help%20yield%20coherent%20responses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07553v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHijacking%2520Context%2520in%2520Large%2520Multi-modal%2520Models%26entry.906535625%3DJoonhyun%2520Jeong%26entry.1292438233%3D%2520%2520Recently%252C%2520Large%2520Multi-modal%2520Models%2520%2528LMMs%2529%2520have%2520demonstrated%2520their%2520ability%2520to%250Aunderstand%2520the%2520visual%2520contents%2520of%2520images%2520given%2520the%2520instructions%2520regarding%2520the%250Aimages.%2520Built%2520upon%2520the%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520LMMs%2520also%2520inherit%2520their%250Aabilities%2520and%2520characteristics%2520such%2520as%2520in-context%2520learning%2520where%2520a%2520coherent%250Asequence%2520of%2520images%2520and%2520texts%2520are%2520given%2520as%2520the%2520input%2520prompt.%2520However%252C%2520we%250Aidentify%2520a%2520new%2520limitation%2520of%2520off-the-shelf%2520LMMs%2520where%2520a%2520small%2520fraction%2520of%250Aincoherent%2520images%2520or%2520text%2520descriptions%2520mislead%2520LMMs%2520to%2520only%2520generate%2520biased%250Aoutput%2520about%2520the%2520hijacked%2520context%252C%2520not%2520the%2520originally%2520intended%2520context.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520a%2520pre-filtering%2520method%2520that%2520removes%2520irrelevant%250Acontexts%2520via%2520GPT-4V%252C%2520based%2520on%2520its%2520robustness%2520towards%2520distribution%2520shift%2520within%250Athe%2520contexts.%2520We%2520further%2520investigate%2520whether%2520replacing%2520the%2520hijacked%2520visual%2520and%250Atextual%2520contexts%2520with%2520the%2520correlated%2520ones%2520via%2520GPT-4V%2520and%2520text-to-image%2520models%250Acan%2520help%2520yield%2520coherent%2520responses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07553v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hijacking%20Context%20in%20Large%20Multi-modal%20Models&entry.906535625=Joonhyun%20Jeong&entry.1292438233=%20%20Recently%2C%20Large%20Multi-modal%20Models%20%28LMMs%29%20have%20demonstrated%20their%20ability%20to%0Aunderstand%20the%20visual%20contents%20of%20images%20given%20the%20instructions%20regarding%20the%0Aimages.%20Built%20upon%20the%20Large%20Language%20Models%20%28LLMs%29%2C%20LMMs%20also%20inherit%20their%0Aabilities%20and%20characteristics%20such%20as%20in-context%20learning%20where%20a%20coherent%0Asequence%20of%20images%20and%20texts%20are%20given%20as%20the%20input%20prompt.%20However%2C%20we%0Aidentify%20a%20new%20limitation%20of%20off-the-shelf%20LMMs%20where%20a%20small%20fraction%20of%0Aincoherent%20images%20or%20text%20descriptions%20mislead%20LMMs%20to%20only%20generate%20biased%0Aoutput%20about%20the%20hijacked%20context%2C%20not%20the%20originally%20intended%20context.%20To%0Aaddress%20this%2C%20we%20propose%20a%20pre-filtering%20method%20that%20removes%20irrelevant%0Acontexts%20via%20GPT-4V%2C%20based%20on%20its%20robustness%20towards%20distribution%20shift%20within%0Athe%20contexts.%20We%20further%20investigate%20whether%20replacing%20the%20hijacked%20visual%20and%0Atextual%20contexts%20with%20the%20correlated%20ones%20via%20GPT-4V%20and%20text-to-image%20models%0Acan%20help%20yield%20coherent%20responses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07553v2&entry.124074799=Read"},
{"title": "LlamaTurk: Adapting Open-Source Generative Large Language Models for\n  Low-Resource Language", "author": "Cagri Toraman", "abstract": "  Despite advancements in English-dominant generative large language models,\nfurther development is needed for low-resource languages to enhance global\naccessibility. The primary methods for representing these languages are\nmonolingual and multilingual pretraining. Monolingual pretraining is expensive\ndue to hardware requirements, and multilingual models often have uneven\nperformance across languages. This study explores an alternative solution by\nadapting large language models, primarily trained on English, to low-resource\nlanguages. We assess various strategies, including continual training,\ninstruction fine-tuning, task-specific fine-tuning, and vocabulary extension.\nThe results show that continual training improves language comprehension, as\nreflected in perplexity scores, and task-specific tuning generally enhances\nperformance of downstream tasks. However, extending the vocabulary shows no\nsubstantial benefits. Additionally, while larger models improve task\nperformance with few-shot tuning, multilingual models perform worse than their\nmonolingual counterparts when adapted.\n", "link": "http://arxiv.org/abs/2405.07745v1", "date": "2024-05-13", "relevancy": 2.0169, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5184}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4978}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LlamaTurk%3A%20Adapting%20Open-Source%20Generative%20Large%20Language%20Models%20for%0A%20%20Low-Resource%20Language&body=Title%3A%20LlamaTurk%3A%20Adapting%20Open-Source%20Generative%20Large%20Language%20Models%20for%0A%20%20Low-Resource%20Language%0AAuthor%3A%20Cagri%20Toraman%0AAbstract%3A%20%20%20Despite%20advancements%20in%20English-dominant%20generative%20large%20language%20models%2C%0Afurther%20development%20is%20needed%20for%20low-resource%20languages%20to%20enhance%20global%0Aaccessibility.%20The%20primary%20methods%20for%20representing%20these%20languages%20are%0Amonolingual%20and%20multilingual%20pretraining.%20Monolingual%20pretraining%20is%20expensive%0Adue%20to%20hardware%20requirements%2C%20and%20multilingual%20models%20often%20have%20uneven%0Aperformance%20across%20languages.%20This%20study%20explores%20an%20alternative%20solution%20by%0Aadapting%20large%20language%20models%2C%20primarily%20trained%20on%20English%2C%20to%20low-resource%0Alanguages.%20We%20assess%20various%20strategies%2C%20including%20continual%20training%2C%0Ainstruction%20fine-tuning%2C%20task-specific%20fine-tuning%2C%20and%20vocabulary%20extension.%0AThe%20results%20show%20that%20continual%20training%20improves%20language%20comprehension%2C%20as%0Areflected%20in%20perplexity%20scores%2C%20and%20task-specific%20tuning%20generally%20enhances%0Aperformance%20of%20downstream%20tasks.%20However%2C%20extending%20the%20vocabulary%20shows%20no%0Asubstantial%20benefits.%20Additionally%2C%20while%20larger%20models%20improve%20task%0Aperformance%20with%20few-shot%20tuning%2C%20multilingual%20models%20perform%20worse%20than%20their%0Amonolingual%20counterparts%20when%20adapted.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLlamaTurk%253A%2520Adapting%2520Open-Source%2520Generative%2520Large%2520Language%2520Models%2520for%250A%2520%2520Low-Resource%2520Language%26entry.906535625%3DCagri%2520Toraman%26entry.1292438233%3D%2520%2520Despite%2520advancements%2520in%2520English-dominant%2520generative%2520large%2520language%2520models%252C%250Afurther%2520development%2520is%2520needed%2520for%2520low-resource%2520languages%2520to%2520enhance%2520global%250Aaccessibility.%2520The%2520primary%2520methods%2520for%2520representing%2520these%2520languages%2520are%250Amonolingual%2520and%2520multilingual%2520pretraining.%2520Monolingual%2520pretraining%2520is%2520expensive%250Adue%2520to%2520hardware%2520requirements%252C%2520and%2520multilingual%2520models%2520often%2520have%2520uneven%250Aperformance%2520across%2520languages.%2520This%2520study%2520explores%2520an%2520alternative%2520solution%2520by%250Aadapting%2520large%2520language%2520models%252C%2520primarily%2520trained%2520on%2520English%252C%2520to%2520low-resource%250Alanguages.%2520We%2520assess%2520various%2520strategies%252C%2520including%2520continual%2520training%252C%250Ainstruction%2520fine-tuning%252C%2520task-specific%2520fine-tuning%252C%2520and%2520vocabulary%2520extension.%250AThe%2520results%2520show%2520that%2520continual%2520training%2520improves%2520language%2520comprehension%252C%2520as%250Areflected%2520in%2520perplexity%2520scores%252C%2520and%2520task-specific%2520tuning%2520generally%2520enhances%250Aperformance%2520of%2520downstream%2520tasks.%2520However%252C%2520extending%2520the%2520vocabulary%2520shows%2520no%250Asubstantial%2520benefits.%2520Additionally%252C%2520while%2520larger%2520models%2520improve%2520task%250Aperformance%2520with%2520few-shot%2520tuning%252C%2520multilingual%2520models%2520perform%2520worse%2520than%2520their%250Amonolingual%2520counterparts%2520when%2520adapted.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LlamaTurk%3A%20Adapting%20Open-Source%20Generative%20Large%20Language%20Models%20for%0A%20%20Low-Resource%20Language&entry.906535625=Cagri%20Toraman&entry.1292438233=%20%20Despite%20advancements%20in%20English-dominant%20generative%20large%20language%20models%2C%0Afurther%20development%20is%20needed%20for%20low-resource%20languages%20to%20enhance%20global%0Aaccessibility.%20The%20primary%20methods%20for%20representing%20these%20languages%20are%0Amonolingual%20and%20multilingual%20pretraining.%20Monolingual%20pretraining%20is%20expensive%0Adue%20to%20hardware%20requirements%2C%20and%20multilingual%20models%20often%20have%20uneven%0Aperformance%20across%20languages.%20This%20study%20explores%20an%20alternative%20solution%20by%0Aadapting%20large%20language%20models%2C%20primarily%20trained%20on%20English%2C%20to%20low-resource%0Alanguages.%20We%20assess%20various%20strategies%2C%20including%20continual%20training%2C%0Ainstruction%20fine-tuning%2C%20task-specific%20fine-tuning%2C%20and%20vocabulary%20extension.%0AThe%20results%20show%20that%20continual%20training%20improves%20language%20comprehension%2C%20as%0Areflected%20in%20perplexity%20scores%2C%20and%20task-specific%20tuning%20generally%20enhances%0Aperformance%20of%20downstream%20tasks.%20However%2C%20extending%20the%20vocabulary%20shows%20no%0Asubstantial%20benefits.%20Additionally%2C%20while%20larger%20models%20improve%20task%0Aperformance%20with%20few-shot%20tuning%2C%20multilingual%20models%20perform%20worse%20than%20their%0Amonolingual%20counterparts%20when%20adapted.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07745v1&entry.124074799=Read"},
{"title": "Can Better Text Semantics in Prompt Tuning Improve VLM Generalization?", "author": "Hari Chandana Kuchibhotla and Sai Srinivas Kancheti and Abbavaram Gowtham Reddy and Vineeth N Balasubramanian", "abstract": "  Going beyond mere fine-tuning of vision-language models (VLMs), learnable\nprompt tuning has emerged as a promising, resource-efficient alternative.\nDespite their potential, effectively learning prompts faces the following\nchallenges: (i) training in a low-shot scenario results in overfitting,\nlimiting adaptability and yielding weaker performance on newer classes or\ndatasets; (ii) prompt-tuning's efficacy heavily relies on the label space, with\ndecreased performance in large class spaces, signaling potential gaps in\nbridging image and class concepts. In this work, we ask the question if better\ntext semantics can help address these concerns. In particular, we introduce a\nprompt-tuning method that leverages class descriptions obtained from large\nlanguage models (LLMs). Our approach constructs part-level description-guided\nviews of both image and text features, which are subsequently aligned to learn\nmore generalizable prompts. Our comprehensive experiments, conducted across 11\nbenchmark datasets, outperform established methods, demonstrating substantial\nimprovements.\n", "link": "http://arxiv.org/abs/2405.07921v1", "date": "2024-05-13", "relevancy": 2.0166, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5218}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5025}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Better%20Text%20Semantics%20in%20Prompt%20Tuning%20Improve%20VLM%20Generalization%3F&body=Title%3A%20Can%20Better%20Text%20Semantics%20in%20Prompt%20Tuning%20Improve%20VLM%20Generalization%3F%0AAuthor%3A%20Hari%20Chandana%20Kuchibhotla%20and%20Sai%20Srinivas%20Kancheti%20and%20Abbavaram%20Gowtham%20Reddy%20and%20Vineeth%20N%20Balasubramanian%0AAbstract%3A%20%20%20Going%20beyond%20mere%20fine-tuning%20of%20vision-language%20models%20%28VLMs%29%2C%20learnable%0Aprompt%20tuning%20has%20emerged%20as%20a%20promising%2C%20resource-efficient%20alternative.%0ADespite%20their%20potential%2C%20effectively%20learning%20prompts%20faces%20the%20following%0Achallenges%3A%20%28i%29%20training%20in%20a%20low-shot%20scenario%20results%20in%20overfitting%2C%0Alimiting%20adaptability%20and%20yielding%20weaker%20performance%20on%20newer%20classes%20or%0Adatasets%3B%20%28ii%29%20prompt-tuning%27s%20efficacy%20heavily%20relies%20on%20the%20label%20space%2C%20with%0Adecreased%20performance%20in%20large%20class%20spaces%2C%20signaling%20potential%20gaps%20in%0Abridging%20image%20and%20class%20concepts.%20In%20this%20work%2C%20we%20ask%20the%20question%20if%20better%0Atext%20semantics%20can%20help%20address%20these%20concerns.%20In%20particular%2C%20we%20introduce%20a%0Aprompt-tuning%20method%20that%20leverages%20class%20descriptions%20obtained%20from%20large%0Alanguage%20models%20%28LLMs%29.%20Our%20approach%20constructs%20part-level%20description-guided%0Aviews%20of%20both%20image%20and%20text%20features%2C%20which%20are%20subsequently%20aligned%20to%20learn%0Amore%20generalizable%20prompts.%20Our%20comprehensive%20experiments%2C%20conducted%20across%2011%0Abenchmark%20datasets%2C%20outperform%20established%20methods%2C%20demonstrating%20substantial%0Aimprovements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Better%2520Text%2520Semantics%2520in%2520Prompt%2520Tuning%2520Improve%2520VLM%2520Generalization%253F%26entry.906535625%3DHari%2520Chandana%2520Kuchibhotla%2520and%2520Sai%2520Srinivas%2520Kancheti%2520and%2520Abbavaram%2520Gowtham%2520Reddy%2520and%2520Vineeth%2520N%2520Balasubramanian%26entry.1292438233%3D%2520%2520Going%2520beyond%2520mere%2520fine-tuning%2520of%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520learnable%250Aprompt%2520tuning%2520has%2520emerged%2520as%2520a%2520promising%252C%2520resource-efficient%2520alternative.%250ADespite%2520their%2520potential%252C%2520effectively%2520learning%2520prompts%2520faces%2520the%2520following%250Achallenges%253A%2520%2528i%2529%2520training%2520in%2520a%2520low-shot%2520scenario%2520results%2520in%2520overfitting%252C%250Alimiting%2520adaptability%2520and%2520yielding%2520weaker%2520performance%2520on%2520newer%2520classes%2520or%250Adatasets%253B%2520%2528ii%2529%2520prompt-tuning%2527s%2520efficacy%2520heavily%2520relies%2520on%2520the%2520label%2520space%252C%2520with%250Adecreased%2520performance%2520in%2520large%2520class%2520spaces%252C%2520signaling%2520potential%2520gaps%2520in%250Abridging%2520image%2520and%2520class%2520concepts.%2520In%2520this%2520work%252C%2520we%2520ask%2520the%2520question%2520if%2520better%250Atext%2520semantics%2520can%2520help%2520address%2520these%2520concerns.%2520In%2520particular%252C%2520we%2520introduce%2520a%250Aprompt-tuning%2520method%2520that%2520leverages%2520class%2520descriptions%2520obtained%2520from%2520large%250Alanguage%2520models%2520%2528LLMs%2529.%2520Our%2520approach%2520constructs%2520part-level%2520description-guided%250Aviews%2520of%2520both%2520image%2520and%2520text%2520features%252C%2520which%2520are%2520subsequently%2520aligned%2520to%2520learn%250Amore%2520generalizable%2520prompts.%2520Our%2520comprehensive%2520experiments%252C%2520conducted%2520across%252011%250Abenchmark%2520datasets%252C%2520outperform%2520established%2520methods%252C%2520demonstrating%2520substantial%250Aimprovements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Better%20Text%20Semantics%20in%20Prompt%20Tuning%20Improve%20VLM%20Generalization%3F&entry.906535625=Hari%20Chandana%20Kuchibhotla%20and%20Sai%20Srinivas%20Kancheti%20and%20Abbavaram%20Gowtham%20Reddy%20and%20Vineeth%20N%20Balasubramanian&entry.1292438233=%20%20Going%20beyond%20mere%20fine-tuning%20of%20vision-language%20models%20%28VLMs%29%2C%20learnable%0Aprompt%20tuning%20has%20emerged%20as%20a%20promising%2C%20resource-efficient%20alternative.%0ADespite%20their%20potential%2C%20effectively%20learning%20prompts%20faces%20the%20following%0Achallenges%3A%20%28i%29%20training%20in%20a%20low-shot%20scenario%20results%20in%20overfitting%2C%0Alimiting%20adaptability%20and%20yielding%20weaker%20performance%20on%20newer%20classes%20or%0Adatasets%3B%20%28ii%29%20prompt-tuning%27s%20efficacy%20heavily%20relies%20on%20the%20label%20space%2C%20with%0Adecreased%20performance%20in%20large%20class%20spaces%2C%20signaling%20potential%20gaps%20in%0Abridging%20image%20and%20class%20concepts.%20In%20this%20work%2C%20we%20ask%20the%20question%20if%20better%0Atext%20semantics%20can%20help%20address%20these%20concerns.%20In%20particular%2C%20we%20introduce%20a%0Aprompt-tuning%20method%20that%20leverages%20class%20descriptions%20obtained%20from%20large%0Alanguage%20models%20%28LLMs%29.%20Our%20approach%20constructs%20part-level%20description-guided%0Aviews%20of%20both%20image%20and%20text%20features%2C%20which%20are%20subsequently%20aligned%20to%20learn%0Amore%20generalizable%20prompts.%20Our%20comprehensive%20experiments%2C%20conducted%20across%2011%0Abenchmark%20datasets%2C%20outperform%20established%20methods%2C%20demonstrating%20substantial%0Aimprovements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07921v1&entry.124074799=Read"},
{"title": "All Nodes are created Not Equal: Node-Specific Layer Aggregation and\n  Filtration for GNN", "author": "Shilong Wang and Hao Wu and Yifan Duan and Guibin Zhang and Guohao Li and Yuxuan Liang and Shirui Pan and Kun Wang and Yang Wang", "abstract": "  The ever-designed Graph Neural Networks, though opening a promising path for\nthe modeling of the graph-structure data, unfortunately introduce two daunting\nobstacles to their deployment on devices. (I) Most of existing GNNs are\nshallow, due mostly to the over-smoothing and gradient-vanish problem as they\ngo deeper as convolutional architectures. (II) The vast majority of GNNs adhere\nto the homophily assumption, where the central node and its adjacent nodes\nshare the same label. This assumption often poses challenges for many GNNs\nworking with heterophilic graphs. Addressing the aforementioned issue has\nbecome a looming challenge in enhancing the robustness and scalability of GNN\napplications. In this paper, we take a comprehensive and systematic approach to\novercoming the two aforementioned challenges for the first time. We propose a\nNode-Specific Layer Aggregation and Filtration architecture, termed NoSAF, a\nframework capable of filtering and processing information from each individual\nnodes. NoSAF introduces the concept of \"All Nodes are Created Not Equal\" into\nevery layer of deep networks, aiming to provide a reliable information filter\nfor each layer's nodes to sieve out information beneficial for the subsequent\nlayer. By incorporating a dynamically updated codebank, NoSAF dynamically\noptimizes the optimal information outputted downwards at each layer. This\neffectively overcomes heterophilic issues and aids in deepening the network. To\ncompensate for the information loss caused by the continuous filtering in\nNoSAF, we also propose NoSAF-D (Deep), which incorporates a compensation\nmechanism that replenishes information in every layer of the model, allowing\nNoSAF to perform meaningful computations even in very deep layers.\n", "link": "http://arxiv.org/abs/2405.07892v1", "date": "2024-05-13", "relevancy": 2.0156, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5422}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4795}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20All%20Nodes%20are%20created%20Not%20Equal%3A%20Node-Specific%20Layer%20Aggregation%20and%0A%20%20Filtration%20for%20GNN&body=Title%3A%20All%20Nodes%20are%20created%20Not%20Equal%3A%20Node-Specific%20Layer%20Aggregation%20and%0A%20%20Filtration%20for%20GNN%0AAuthor%3A%20Shilong%20Wang%20and%20Hao%20Wu%20and%20Yifan%20Duan%20and%20Guibin%20Zhang%20and%20Guohao%20Li%20and%20Yuxuan%20Liang%20and%20Shirui%20Pan%20and%20Kun%20Wang%20and%20Yang%20Wang%0AAbstract%3A%20%20%20The%20ever-designed%20Graph%20Neural%20Networks%2C%20though%20opening%20a%20promising%20path%20for%0Athe%20modeling%20of%20the%20graph-structure%20data%2C%20unfortunately%20introduce%20two%20daunting%0Aobstacles%20to%20their%20deployment%20on%20devices.%20%28I%29%20Most%20of%20existing%20GNNs%20are%0Ashallow%2C%20due%20mostly%20to%20the%20over-smoothing%20and%20gradient-vanish%20problem%20as%20they%0Ago%20deeper%20as%20convolutional%20architectures.%20%28II%29%20The%20vast%20majority%20of%20GNNs%20adhere%0Ato%20the%20homophily%20assumption%2C%20where%20the%20central%20node%20and%20its%20adjacent%20nodes%0Ashare%20the%20same%20label.%20This%20assumption%20often%20poses%20challenges%20for%20many%20GNNs%0Aworking%20with%20heterophilic%20graphs.%20Addressing%20the%20aforementioned%20issue%20has%0Abecome%20a%20looming%20challenge%20in%20enhancing%20the%20robustness%20and%20scalability%20of%20GNN%0Aapplications.%20In%20this%20paper%2C%20we%20take%20a%20comprehensive%20and%20systematic%20approach%20to%0Aovercoming%20the%20two%20aforementioned%20challenges%20for%20the%20first%20time.%20We%20propose%20a%0ANode-Specific%20Layer%20Aggregation%20and%20Filtration%20architecture%2C%20termed%20NoSAF%2C%20a%0Aframework%20capable%20of%20filtering%20and%20processing%20information%20from%20each%20individual%0Anodes.%20NoSAF%20introduces%20the%20concept%20of%20%22All%20Nodes%20are%20Created%20Not%20Equal%22%20into%0Aevery%20layer%20of%20deep%20networks%2C%20aiming%20to%20provide%20a%20reliable%20information%20filter%0Afor%20each%20layer%27s%20nodes%20to%20sieve%20out%20information%20beneficial%20for%20the%20subsequent%0Alayer.%20By%20incorporating%20a%20dynamically%20updated%20codebank%2C%20NoSAF%20dynamically%0Aoptimizes%20the%20optimal%20information%20outputted%20downwards%20at%20each%20layer.%20This%0Aeffectively%20overcomes%20heterophilic%20issues%20and%20aids%20in%20deepening%20the%20network.%20To%0Acompensate%20for%20the%20information%20loss%20caused%20by%20the%20continuous%20filtering%20in%0ANoSAF%2C%20we%20also%20propose%20NoSAF-D%20%28Deep%29%2C%20which%20incorporates%20a%20compensation%0Amechanism%20that%20replenishes%20information%20in%20every%20layer%20of%20the%20model%2C%20allowing%0ANoSAF%20to%20perform%20meaningful%20computations%20even%20in%20very%20deep%20layers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAll%2520Nodes%2520are%2520created%2520Not%2520Equal%253A%2520Node-Specific%2520Layer%2520Aggregation%2520and%250A%2520%2520Filtration%2520for%2520GNN%26entry.906535625%3DShilong%2520Wang%2520and%2520Hao%2520Wu%2520and%2520Yifan%2520Duan%2520and%2520Guibin%2520Zhang%2520and%2520Guohao%2520Li%2520and%2520Yuxuan%2520Liang%2520and%2520Shirui%2520Pan%2520and%2520Kun%2520Wang%2520and%2520Yang%2520Wang%26entry.1292438233%3D%2520%2520The%2520ever-designed%2520Graph%2520Neural%2520Networks%252C%2520though%2520opening%2520a%2520promising%2520path%2520for%250Athe%2520modeling%2520of%2520the%2520graph-structure%2520data%252C%2520unfortunately%2520introduce%2520two%2520daunting%250Aobstacles%2520to%2520their%2520deployment%2520on%2520devices.%2520%2528I%2529%2520Most%2520of%2520existing%2520GNNs%2520are%250Ashallow%252C%2520due%2520mostly%2520to%2520the%2520over-smoothing%2520and%2520gradient-vanish%2520problem%2520as%2520they%250Ago%2520deeper%2520as%2520convolutional%2520architectures.%2520%2528II%2529%2520The%2520vast%2520majority%2520of%2520GNNs%2520adhere%250Ato%2520the%2520homophily%2520assumption%252C%2520where%2520the%2520central%2520node%2520and%2520its%2520adjacent%2520nodes%250Ashare%2520the%2520same%2520label.%2520This%2520assumption%2520often%2520poses%2520challenges%2520for%2520many%2520GNNs%250Aworking%2520with%2520heterophilic%2520graphs.%2520Addressing%2520the%2520aforementioned%2520issue%2520has%250Abecome%2520a%2520looming%2520challenge%2520in%2520enhancing%2520the%2520robustness%2520and%2520scalability%2520of%2520GNN%250Aapplications.%2520In%2520this%2520paper%252C%2520we%2520take%2520a%2520comprehensive%2520and%2520systematic%2520approach%2520to%250Aovercoming%2520the%2520two%2520aforementioned%2520challenges%2520for%2520the%2520first%2520time.%2520We%2520propose%2520a%250ANode-Specific%2520Layer%2520Aggregation%2520and%2520Filtration%2520architecture%252C%2520termed%2520NoSAF%252C%2520a%250Aframework%2520capable%2520of%2520filtering%2520and%2520processing%2520information%2520from%2520each%2520individual%250Anodes.%2520NoSAF%2520introduces%2520the%2520concept%2520of%2520%2522All%2520Nodes%2520are%2520Created%2520Not%2520Equal%2522%2520into%250Aevery%2520layer%2520of%2520deep%2520networks%252C%2520aiming%2520to%2520provide%2520a%2520reliable%2520information%2520filter%250Afor%2520each%2520layer%2527s%2520nodes%2520to%2520sieve%2520out%2520information%2520beneficial%2520for%2520the%2520subsequent%250Alayer.%2520By%2520incorporating%2520a%2520dynamically%2520updated%2520codebank%252C%2520NoSAF%2520dynamically%250Aoptimizes%2520the%2520optimal%2520information%2520outputted%2520downwards%2520at%2520each%2520layer.%2520This%250Aeffectively%2520overcomes%2520heterophilic%2520issues%2520and%2520aids%2520in%2520deepening%2520the%2520network.%2520To%250Acompensate%2520for%2520the%2520information%2520loss%2520caused%2520by%2520the%2520continuous%2520filtering%2520in%250ANoSAF%252C%2520we%2520also%2520propose%2520NoSAF-D%2520%2528Deep%2529%252C%2520which%2520incorporates%2520a%2520compensation%250Amechanism%2520that%2520replenishes%2520information%2520in%2520every%2520layer%2520of%2520the%2520model%252C%2520allowing%250ANoSAF%2520to%2520perform%2520meaningful%2520computations%2520even%2520in%2520very%2520deep%2520layers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=All%20Nodes%20are%20created%20Not%20Equal%3A%20Node-Specific%20Layer%20Aggregation%20and%0A%20%20Filtration%20for%20GNN&entry.906535625=Shilong%20Wang%20and%20Hao%20Wu%20and%20Yifan%20Duan%20and%20Guibin%20Zhang%20and%20Guohao%20Li%20and%20Yuxuan%20Liang%20and%20Shirui%20Pan%20and%20Kun%20Wang%20and%20Yang%20Wang&entry.1292438233=%20%20The%20ever-designed%20Graph%20Neural%20Networks%2C%20though%20opening%20a%20promising%20path%20for%0Athe%20modeling%20of%20the%20graph-structure%20data%2C%20unfortunately%20introduce%20two%20daunting%0Aobstacles%20to%20their%20deployment%20on%20devices.%20%28I%29%20Most%20of%20existing%20GNNs%20are%0Ashallow%2C%20due%20mostly%20to%20the%20over-smoothing%20and%20gradient-vanish%20problem%20as%20they%0Ago%20deeper%20as%20convolutional%20architectures.%20%28II%29%20The%20vast%20majority%20of%20GNNs%20adhere%0Ato%20the%20homophily%20assumption%2C%20where%20the%20central%20node%20and%20its%20adjacent%20nodes%0Ashare%20the%20same%20label.%20This%20assumption%20often%20poses%20challenges%20for%20many%20GNNs%0Aworking%20with%20heterophilic%20graphs.%20Addressing%20the%20aforementioned%20issue%20has%0Abecome%20a%20looming%20challenge%20in%20enhancing%20the%20robustness%20and%20scalability%20of%20GNN%0Aapplications.%20In%20this%20paper%2C%20we%20take%20a%20comprehensive%20and%20systematic%20approach%20to%0Aovercoming%20the%20two%20aforementioned%20challenges%20for%20the%20first%20time.%20We%20propose%20a%0ANode-Specific%20Layer%20Aggregation%20and%20Filtration%20architecture%2C%20termed%20NoSAF%2C%20a%0Aframework%20capable%20of%20filtering%20and%20processing%20information%20from%20each%20individual%0Anodes.%20NoSAF%20introduces%20the%20concept%20of%20%22All%20Nodes%20are%20Created%20Not%20Equal%22%20into%0Aevery%20layer%20of%20deep%20networks%2C%20aiming%20to%20provide%20a%20reliable%20information%20filter%0Afor%20each%20layer%27s%20nodes%20to%20sieve%20out%20information%20beneficial%20for%20the%20subsequent%0Alayer.%20By%20incorporating%20a%20dynamically%20updated%20codebank%2C%20NoSAF%20dynamically%0Aoptimizes%20the%20optimal%20information%20outputted%20downwards%20at%20each%20layer.%20This%0Aeffectively%20overcomes%20heterophilic%20issues%20and%20aids%20in%20deepening%20the%20network.%20To%0Acompensate%20for%20the%20information%20loss%20caused%20by%20the%20continuous%20filtering%20in%0ANoSAF%2C%20we%20also%20propose%20NoSAF-D%20%28Deep%29%2C%20which%20incorporates%20a%20compensation%0Amechanism%20that%20replenishes%20information%20in%20every%20layer%20of%20the%20model%2C%20allowing%0ANoSAF%20to%20perform%20meaningful%20computations%20even%20in%20very%20deep%20layers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07892v1&entry.124074799=Read"},
{"title": "Class-wise Activation Unravelling the Engima of Deep Double Descent", "author": "Yufei Gu", "abstract": "  Double descent presents a counter-intuitive aspect within the machine\nlearning domain, and researchers have observed its manifestation in various\nmodels and tasks. While some theoretical explanations have been proposed for\nthis phenomenon in specific contexts, an accepted theory for its occurring\nmechanism in deep learning remains yet to be established. In this study, we\nrevisited the phenomenon of double descent and discussed the conditions of its\noccurrence. This paper introduces the concept of class-activation matrices and\na methodology for estimating the effective complexity of functions, on which we\nunveil that over-parameterized models exhibit more distinct and simpler class\npatterns in hidden activations compared to under-parameterized ones. We further\nlooked into the interpolation of noisy labelled data among clean\nrepresentations and demonstrated overfitting w.r.t. expressive capacity. By\ncomprehensively analysing hypotheses and presenting corresponding empirical\nevidence that either validates or contradicts these hypotheses, we aim to\nprovide fresh insights into the phenomenon of double descent and benign\nover-parameterization and facilitate future explorations. By comprehensively\nstudying different hypotheses and the corresponding empirical evidence either\nsupports or challenges these hypotheses, our goal is to offer new insights into\nthe phenomena of double descent and benign over-parameterization, thereby\nenabling further explorations in the field. The source code is available at\nhttps://github.com/Yufei-Gu-451/sparse-generalization.git.\n", "link": "http://arxiv.org/abs/2405.07679v1", "date": "2024-05-13", "relevancy": 2.0112, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5213}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4901}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4882}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Class-wise%20Activation%20Unravelling%20the%20Engima%20of%20Deep%20Double%20Descent&body=Title%3A%20Class-wise%20Activation%20Unravelling%20the%20Engima%20of%20Deep%20Double%20Descent%0AAuthor%3A%20Yufei%20Gu%0AAbstract%3A%20%20%20Double%20descent%20presents%20a%20counter-intuitive%20aspect%20within%20the%20machine%0Alearning%20domain%2C%20and%20researchers%20have%20observed%20its%20manifestation%20in%20various%0Amodels%20and%20tasks.%20While%20some%20theoretical%20explanations%20have%20been%20proposed%20for%0Athis%20phenomenon%20in%20specific%20contexts%2C%20an%20accepted%20theory%20for%20its%20occurring%0Amechanism%20in%20deep%20learning%20remains%20yet%20to%20be%20established.%20In%20this%20study%2C%20we%0Arevisited%20the%20phenomenon%20of%20double%20descent%20and%20discussed%20the%20conditions%20of%20its%0Aoccurrence.%20This%20paper%20introduces%20the%20concept%20of%20class-activation%20matrices%20and%0Aa%20methodology%20for%20estimating%20the%20effective%20complexity%20of%20functions%2C%20on%20which%20we%0Aunveil%20that%20over-parameterized%20models%20exhibit%20more%20distinct%20and%20simpler%20class%0Apatterns%20in%20hidden%20activations%20compared%20to%20under-parameterized%20ones.%20We%20further%0Alooked%20into%20the%20interpolation%20of%20noisy%20labelled%20data%20among%20clean%0Arepresentations%20and%20demonstrated%20overfitting%20w.r.t.%20expressive%20capacity.%20By%0Acomprehensively%20analysing%20hypotheses%20and%20presenting%20corresponding%20empirical%0Aevidence%20that%20either%20validates%20or%20contradicts%20these%20hypotheses%2C%20we%20aim%20to%0Aprovide%20fresh%20insights%20into%20the%20phenomenon%20of%20double%20descent%20and%20benign%0Aover-parameterization%20and%20facilitate%20future%20explorations.%20By%20comprehensively%0Astudying%20different%20hypotheses%20and%20the%20corresponding%20empirical%20evidence%20either%0Asupports%20or%20challenges%20these%20hypotheses%2C%20our%20goal%20is%20to%20offer%20new%20insights%20into%0Athe%20phenomena%20of%20double%20descent%20and%20benign%20over-parameterization%2C%20thereby%0Aenabling%20further%20explorations%20in%20the%20field.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/Yufei-Gu-451/sparse-generalization.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClass-wise%2520Activation%2520Unravelling%2520the%2520Engima%2520of%2520Deep%2520Double%2520Descent%26entry.906535625%3DYufei%2520Gu%26entry.1292438233%3D%2520%2520Double%2520descent%2520presents%2520a%2520counter-intuitive%2520aspect%2520within%2520the%2520machine%250Alearning%2520domain%252C%2520and%2520researchers%2520have%2520observed%2520its%2520manifestation%2520in%2520various%250Amodels%2520and%2520tasks.%2520While%2520some%2520theoretical%2520explanations%2520have%2520been%2520proposed%2520for%250Athis%2520phenomenon%2520in%2520specific%2520contexts%252C%2520an%2520accepted%2520theory%2520for%2520its%2520occurring%250Amechanism%2520in%2520deep%2520learning%2520remains%2520yet%2520to%2520be%2520established.%2520In%2520this%2520study%252C%2520we%250Arevisited%2520the%2520phenomenon%2520of%2520double%2520descent%2520and%2520discussed%2520the%2520conditions%2520of%2520its%250Aoccurrence.%2520This%2520paper%2520introduces%2520the%2520concept%2520of%2520class-activation%2520matrices%2520and%250Aa%2520methodology%2520for%2520estimating%2520the%2520effective%2520complexity%2520of%2520functions%252C%2520on%2520which%2520we%250Aunveil%2520that%2520over-parameterized%2520models%2520exhibit%2520more%2520distinct%2520and%2520simpler%2520class%250Apatterns%2520in%2520hidden%2520activations%2520compared%2520to%2520under-parameterized%2520ones.%2520We%2520further%250Alooked%2520into%2520the%2520interpolation%2520of%2520noisy%2520labelled%2520data%2520among%2520clean%250Arepresentations%2520and%2520demonstrated%2520overfitting%2520w.r.t.%2520expressive%2520capacity.%2520By%250Acomprehensively%2520analysing%2520hypotheses%2520and%2520presenting%2520corresponding%2520empirical%250Aevidence%2520that%2520either%2520validates%2520or%2520contradicts%2520these%2520hypotheses%252C%2520we%2520aim%2520to%250Aprovide%2520fresh%2520insights%2520into%2520the%2520phenomenon%2520of%2520double%2520descent%2520and%2520benign%250Aover-parameterization%2520and%2520facilitate%2520future%2520explorations.%2520By%2520comprehensively%250Astudying%2520different%2520hypotheses%2520and%2520the%2520corresponding%2520empirical%2520evidence%2520either%250Asupports%2520or%2520challenges%2520these%2520hypotheses%252C%2520our%2520goal%2520is%2520to%2520offer%2520new%2520insights%2520into%250Athe%2520phenomena%2520of%2520double%2520descent%2520and%2520benign%2520over-parameterization%252C%2520thereby%250Aenabling%2520further%2520explorations%2520in%2520the%2520field.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Yufei-Gu-451/sparse-generalization.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Class-wise%20Activation%20Unravelling%20the%20Engima%20of%20Deep%20Double%20Descent&entry.906535625=Yufei%20Gu&entry.1292438233=%20%20Double%20descent%20presents%20a%20counter-intuitive%20aspect%20within%20the%20machine%0Alearning%20domain%2C%20and%20researchers%20have%20observed%20its%20manifestation%20in%20various%0Amodels%20and%20tasks.%20While%20some%20theoretical%20explanations%20have%20been%20proposed%20for%0Athis%20phenomenon%20in%20specific%20contexts%2C%20an%20accepted%20theory%20for%20its%20occurring%0Amechanism%20in%20deep%20learning%20remains%20yet%20to%20be%20established.%20In%20this%20study%2C%20we%0Arevisited%20the%20phenomenon%20of%20double%20descent%20and%20discussed%20the%20conditions%20of%20its%0Aoccurrence.%20This%20paper%20introduces%20the%20concept%20of%20class-activation%20matrices%20and%0Aa%20methodology%20for%20estimating%20the%20effective%20complexity%20of%20functions%2C%20on%20which%20we%0Aunveil%20that%20over-parameterized%20models%20exhibit%20more%20distinct%20and%20simpler%20class%0Apatterns%20in%20hidden%20activations%20compared%20to%20under-parameterized%20ones.%20We%20further%0Alooked%20into%20the%20interpolation%20of%20noisy%20labelled%20data%20among%20clean%0Arepresentations%20and%20demonstrated%20overfitting%20w.r.t.%20expressive%20capacity.%20By%0Acomprehensively%20analysing%20hypotheses%20and%20presenting%20corresponding%20empirical%0Aevidence%20that%20either%20validates%20or%20contradicts%20these%20hypotheses%2C%20we%20aim%20to%0Aprovide%20fresh%20insights%20into%20the%20phenomenon%20of%20double%20descent%20and%20benign%0Aover-parameterization%20and%20facilitate%20future%20explorations.%20By%20comprehensively%0Astudying%20different%20hypotheses%20and%20the%20corresponding%20empirical%20evidence%20either%0Asupports%20or%20challenges%20these%20hypotheses%2C%20our%20goal%20is%20to%20offer%20new%20insights%20into%0Athe%20phenomena%20of%20double%20descent%20and%20benign%20over-parameterization%2C%20thereby%0Aenabling%20further%20explorations%20in%20the%20field.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/Yufei-Gu-451/sparse-generalization.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07679v1&entry.124074799=Read"},
{"title": "Distribution Learning Meets Graph Structure Sampling", "author": "Arnab Bhattacharyya and Sutanu Gayen and Philips George John and Sayantan Sen and N. V. Vinodchandran", "abstract": "  This work establishes a novel link between the problem of PAC-learning\nhigh-dimensional graphical models and the task of (efficient) counting and\nsampling of graph structures, using an online learning framework.\n  We observe that if we apply the exponentially weighted average (EWA) or\nrandomized weighted majority (RWM) forecasters on a sequence of samples from a\ndistribution P using the log loss function, the average regret incurred by the\nforecaster's predictions can be used to bound the expected KL divergence\nbetween P and the predictions. Known regret bounds for EWA and RWM then yield\nnew sample complexity bounds for learning Bayes nets. Moreover, these\nalgorithms can be made computationally efficient for several interesting\nclasses of Bayes nets. Specifically, we give a new sample-optimal and\npolynomial time learning algorithm with respect to trees of unknown structure\nand the first polynomial sample and time algorithm for learning with respect to\nBayes nets over a given chordal skeleton.\n", "link": "http://arxiv.org/abs/2405.07914v1", "date": "2024-05-13", "relevancy": 2.0066, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5131}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.508}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distribution%20Learning%20Meets%20Graph%20Structure%20Sampling&body=Title%3A%20Distribution%20Learning%20Meets%20Graph%20Structure%20Sampling%0AAuthor%3A%20Arnab%20Bhattacharyya%20and%20Sutanu%20Gayen%20and%20Philips%20George%20John%20and%20Sayantan%20Sen%20and%20N.%20V.%20Vinodchandran%0AAbstract%3A%20%20%20This%20work%20establishes%20a%20novel%20link%20between%20the%20problem%20of%20PAC-learning%0Ahigh-dimensional%20graphical%20models%20and%20the%20task%20of%20%28efficient%29%20counting%20and%0Asampling%20of%20graph%20structures%2C%20using%20an%20online%20learning%20framework.%0A%20%20We%20observe%20that%20if%20we%20apply%20the%20exponentially%20weighted%20average%20%28EWA%29%20or%0Arandomized%20weighted%20majority%20%28RWM%29%20forecasters%20on%20a%20sequence%20of%20samples%20from%20a%0Adistribution%20P%20using%20the%20log%20loss%20function%2C%20the%20average%20regret%20incurred%20by%20the%0Aforecaster%27s%20predictions%20can%20be%20used%20to%20bound%20the%20expected%20KL%20divergence%0Abetween%20P%20and%20the%20predictions.%20Known%20regret%20bounds%20for%20EWA%20and%20RWM%20then%20yield%0Anew%20sample%20complexity%20bounds%20for%20learning%20Bayes%20nets.%20Moreover%2C%20these%0Aalgorithms%20can%20be%20made%20computationally%20efficient%20for%20several%20interesting%0Aclasses%20of%20Bayes%20nets.%20Specifically%2C%20we%20give%20a%20new%20sample-optimal%20and%0Apolynomial%20time%20learning%20algorithm%20with%20respect%20to%20trees%20of%20unknown%20structure%0Aand%20the%20first%20polynomial%20sample%20and%20time%20algorithm%20for%20learning%20with%20respect%20to%0ABayes%20nets%20over%20a%20given%20chordal%20skeleton.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07914v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistribution%2520Learning%2520Meets%2520Graph%2520Structure%2520Sampling%26entry.906535625%3DArnab%2520Bhattacharyya%2520and%2520Sutanu%2520Gayen%2520and%2520Philips%2520George%2520John%2520and%2520Sayantan%2520Sen%2520and%2520N.%2520V.%2520Vinodchandran%26entry.1292438233%3D%2520%2520This%2520work%2520establishes%2520a%2520novel%2520link%2520between%2520the%2520problem%2520of%2520PAC-learning%250Ahigh-dimensional%2520graphical%2520models%2520and%2520the%2520task%2520of%2520%2528efficient%2529%2520counting%2520and%250Asampling%2520of%2520graph%2520structures%252C%2520using%2520an%2520online%2520learning%2520framework.%250A%2520%2520We%2520observe%2520that%2520if%2520we%2520apply%2520the%2520exponentially%2520weighted%2520average%2520%2528EWA%2529%2520or%250Arandomized%2520weighted%2520majority%2520%2528RWM%2529%2520forecasters%2520on%2520a%2520sequence%2520of%2520samples%2520from%2520a%250Adistribution%2520P%2520using%2520the%2520log%2520loss%2520function%252C%2520the%2520average%2520regret%2520incurred%2520by%2520the%250Aforecaster%2527s%2520predictions%2520can%2520be%2520used%2520to%2520bound%2520the%2520expected%2520KL%2520divergence%250Abetween%2520P%2520and%2520the%2520predictions.%2520Known%2520regret%2520bounds%2520for%2520EWA%2520and%2520RWM%2520then%2520yield%250Anew%2520sample%2520complexity%2520bounds%2520for%2520learning%2520Bayes%2520nets.%2520Moreover%252C%2520these%250Aalgorithms%2520can%2520be%2520made%2520computationally%2520efficient%2520for%2520several%2520interesting%250Aclasses%2520of%2520Bayes%2520nets.%2520Specifically%252C%2520we%2520give%2520a%2520new%2520sample-optimal%2520and%250Apolynomial%2520time%2520learning%2520algorithm%2520with%2520respect%2520to%2520trees%2520of%2520unknown%2520structure%250Aand%2520the%2520first%2520polynomial%2520sample%2520and%2520time%2520algorithm%2520for%2520learning%2520with%2520respect%2520to%250ABayes%2520nets%2520over%2520a%2520given%2520chordal%2520skeleton.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07914v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distribution%20Learning%20Meets%20Graph%20Structure%20Sampling&entry.906535625=Arnab%20Bhattacharyya%20and%20Sutanu%20Gayen%20and%20Philips%20George%20John%20and%20Sayantan%20Sen%20and%20N.%20V.%20Vinodchandran&entry.1292438233=%20%20This%20work%20establishes%20a%20novel%20link%20between%20the%20problem%20of%20PAC-learning%0Ahigh-dimensional%20graphical%20models%20and%20the%20task%20of%20%28efficient%29%20counting%20and%0Asampling%20of%20graph%20structures%2C%20using%20an%20online%20learning%20framework.%0A%20%20We%20observe%20that%20if%20we%20apply%20the%20exponentially%20weighted%20average%20%28EWA%29%20or%0Arandomized%20weighted%20majority%20%28RWM%29%20forecasters%20on%20a%20sequence%20of%20samples%20from%20a%0Adistribution%20P%20using%20the%20log%20loss%20function%2C%20the%20average%20regret%20incurred%20by%20the%0Aforecaster%27s%20predictions%20can%20be%20used%20to%20bound%20the%20expected%20KL%20divergence%0Abetween%20P%20and%20the%20predictions.%20Known%20regret%20bounds%20for%20EWA%20and%20RWM%20then%20yield%0Anew%20sample%20complexity%20bounds%20for%20learning%20Bayes%20nets.%20Moreover%2C%20these%0Aalgorithms%20can%20be%20made%20computationally%20efficient%20for%20several%20interesting%0Aclasses%20of%20Bayes%20nets.%20Specifically%2C%20we%20give%20a%20new%20sample-optimal%20and%0Apolynomial%20time%20learning%20algorithm%20with%20respect%20to%20trees%20of%20unknown%20structure%0Aand%20the%20first%20polynomial%20sample%20and%20time%20algorithm%20for%20learning%20with%20respect%20to%0ABayes%20nets%20over%20a%20given%20chordal%20skeleton.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07914v1&entry.124074799=Read"},
{"title": "Environmental Matching Attack Against Unmanned Aerial Vehicles Object\n  Detection", "author": "Dehong Kong and Siyuan Liang and Wenqi Ren", "abstract": "  Object detection techniques for Unmanned Aerial Vehicles (UAVs) rely on Deep\nNeural Networks (DNNs), which are vulnerable to adversarial attacks.\nNonetheless, adversarial patches generated by existing algorithms in the UAV\ndomain pay very little attention to the naturalness of adversarial patches.\nMoreover, imposing constraints directly on adversarial patches makes it\ndifficult to generate patches that appear natural to the human eye while\nensuring a high attack success rate. We notice that patches are natural looking\nwhen their overall color is consistent with the environment. Therefore, we\npropose a new method named Environmental Matching Attack(EMA) to address the\nissue of optimizing the adversarial patch under the constraints of color. To\nthe best of our knowledge, this paper is the first to consider natural patches\nin the domain of UAVs. The EMA method exploits strong prior knowledge of a\npretrained stable diffusion to guide the optimization direction of the\nadversarial patch, where the text guidance can restrict the color of the patch.\nTo better match the environment, the contrast and brightness of the patch are\nappropriately adjusted. Instead of optimizing the adversarial patch itself, we\noptimize an adversarial perturbation patch which initializes to zero so that\nthe model can better trade off attacking performance and naturalness.\nExperiments conducted on the DroneVehicle and Carpk datasets have shown that\nour work can reach nearly the same attack performance in the digital attack(no\ngreater than 2 in mAP$\\%$), surpass the baseline method in the physical\nspecific scenarios, and exhibit a significant advantage in terms of naturalness\nin visualization and color difference with the environment.\n", "link": "http://arxiv.org/abs/2405.07595v1", "date": "2024-05-13", "relevancy": 2.0047, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.521}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5052}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Environmental%20Matching%20Attack%20Against%20Unmanned%20Aerial%20Vehicles%20Object%0A%20%20Detection&body=Title%3A%20Environmental%20Matching%20Attack%20Against%20Unmanned%20Aerial%20Vehicles%20Object%0A%20%20Detection%0AAuthor%3A%20Dehong%20Kong%20and%20Siyuan%20Liang%20and%20Wenqi%20Ren%0AAbstract%3A%20%20%20Object%20detection%20techniques%20for%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20rely%20on%20Deep%0ANeural%20Networks%20%28DNNs%29%2C%20which%20are%20vulnerable%20to%20adversarial%20attacks.%0ANonetheless%2C%20adversarial%20patches%20generated%20by%20existing%20algorithms%20in%20the%20UAV%0Adomain%20pay%20very%20little%20attention%20to%20the%20naturalness%20of%20adversarial%20patches.%0AMoreover%2C%20imposing%20constraints%20directly%20on%20adversarial%20patches%20makes%20it%0Adifficult%20to%20generate%20patches%20that%20appear%20natural%20to%20the%20human%20eye%20while%0Aensuring%20a%20high%20attack%20success%20rate.%20We%20notice%20that%20patches%20are%20natural%20looking%0Awhen%20their%20overall%20color%20is%20consistent%20with%20the%20environment.%20Therefore%2C%20we%0Apropose%20a%20new%20method%20named%20Environmental%20Matching%20Attack%28EMA%29%20to%20address%20the%0Aissue%20of%20optimizing%20the%20adversarial%20patch%20under%20the%20constraints%20of%20color.%20To%0Athe%20best%20of%20our%20knowledge%2C%20this%20paper%20is%20the%20first%20to%20consider%20natural%20patches%0Ain%20the%20domain%20of%20UAVs.%20The%20EMA%20method%20exploits%20strong%20prior%20knowledge%20of%20a%0Apretrained%20stable%20diffusion%20to%20guide%20the%20optimization%20direction%20of%20the%0Aadversarial%20patch%2C%20where%20the%20text%20guidance%20can%20restrict%20the%20color%20of%20the%20patch.%0ATo%20better%20match%20the%20environment%2C%20the%20contrast%20and%20brightness%20of%20the%20patch%20are%0Aappropriately%20adjusted.%20Instead%20of%20optimizing%20the%20adversarial%20patch%20itself%2C%20we%0Aoptimize%20an%20adversarial%20perturbation%20patch%20which%20initializes%20to%20zero%20so%20that%0Athe%20model%20can%20better%20trade%20off%20attacking%20performance%20and%20naturalness.%0AExperiments%20conducted%20on%20the%20DroneVehicle%20and%20Carpk%20datasets%20have%20shown%20that%0Aour%20work%20can%20reach%20nearly%20the%20same%20attack%20performance%20in%20the%20digital%20attack%28no%0Agreater%20than%202%20in%20mAP%24%5C%25%24%29%2C%20surpass%20the%20baseline%20method%20in%20the%20physical%0Aspecific%20scenarios%2C%20and%20exhibit%20a%20significant%20advantage%20in%20terms%20of%20naturalness%0Ain%20visualization%20and%20color%20difference%20with%20the%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnvironmental%2520Matching%2520Attack%2520Against%2520Unmanned%2520Aerial%2520Vehicles%2520Object%250A%2520%2520Detection%26entry.906535625%3DDehong%2520Kong%2520and%2520Siyuan%2520Liang%2520and%2520Wenqi%2520Ren%26entry.1292438233%3D%2520%2520Object%2520detection%2520techniques%2520for%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520rely%2520on%2520Deep%250ANeural%2520Networks%2520%2528DNNs%2529%252C%2520which%2520are%2520vulnerable%2520to%2520adversarial%2520attacks.%250ANonetheless%252C%2520adversarial%2520patches%2520generated%2520by%2520existing%2520algorithms%2520in%2520the%2520UAV%250Adomain%2520pay%2520very%2520little%2520attention%2520to%2520the%2520naturalness%2520of%2520adversarial%2520patches.%250AMoreover%252C%2520imposing%2520constraints%2520directly%2520on%2520adversarial%2520patches%2520makes%2520it%250Adifficult%2520to%2520generate%2520patches%2520that%2520appear%2520natural%2520to%2520the%2520human%2520eye%2520while%250Aensuring%2520a%2520high%2520attack%2520success%2520rate.%2520We%2520notice%2520that%2520patches%2520are%2520natural%2520looking%250Awhen%2520their%2520overall%2520color%2520is%2520consistent%2520with%2520the%2520environment.%2520Therefore%252C%2520we%250Apropose%2520a%2520new%2520method%2520named%2520Environmental%2520Matching%2520Attack%2528EMA%2529%2520to%2520address%2520the%250Aissue%2520of%2520optimizing%2520the%2520adversarial%2520patch%2520under%2520the%2520constraints%2520of%2520color.%2520To%250Athe%2520best%2520of%2520our%2520knowledge%252C%2520this%2520paper%2520is%2520the%2520first%2520to%2520consider%2520natural%2520patches%250Ain%2520the%2520domain%2520of%2520UAVs.%2520The%2520EMA%2520method%2520exploits%2520strong%2520prior%2520knowledge%2520of%2520a%250Apretrained%2520stable%2520diffusion%2520to%2520guide%2520the%2520optimization%2520direction%2520of%2520the%250Aadversarial%2520patch%252C%2520where%2520the%2520text%2520guidance%2520can%2520restrict%2520the%2520color%2520of%2520the%2520patch.%250ATo%2520better%2520match%2520the%2520environment%252C%2520the%2520contrast%2520and%2520brightness%2520of%2520the%2520patch%2520are%250Aappropriately%2520adjusted.%2520Instead%2520of%2520optimizing%2520the%2520adversarial%2520patch%2520itself%252C%2520we%250Aoptimize%2520an%2520adversarial%2520perturbation%2520patch%2520which%2520initializes%2520to%2520zero%2520so%2520that%250Athe%2520model%2520can%2520better%2520trade%2520off%2520attacking%2520performance%2520and%2520naturalness.%250AExperiments%2520conducted%2520on%2520the%2520DroneVehicle%2520and%2520Carpk%2520datasets%2520have%2520shown%2520that%250Aour%2520work%2520can%2520reach%2520nearly%2520the%2520same%2520attack%2520performance%2520in%2520the%2520digital%2520attack%2528no%250Agreater%2520than%25202%2520in%2520mAP%2524%255C%2525%2524%2529%252C%2520surpass%2520the%2520baseline%2520method%2520in%2520the%2520physical%250Aspecific%2520scenarios%252C%2520and%2520exhibit%2520a%2520significant%2520advantage%2520in%2520terms%2520of%2520naturalness%250Ain%2520visualization%2520and%2520color%2520difference%2520with%2520the%2520environment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Environmental%20Matching%20Attack%20Against%20Unmanned%20Aerial%20Vehicles%20Object%0A%20%20Detection&entry.906535625=Dehong%20Kong%20and%20Siyuan%20Liang%20and%20Wenqi%20Ren&entry.1292438233=%20%20Object%20detection%20techniques%20for%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20rely%20on%20Deep%0ANeural%20Networks%20%28DNNs%29%2C%20which%20are%20vulnerable%20to%20adversarial%20attacks.%0ANonetheless%2C%20adversarial%20patches%20generated%20by%20existing%20algorithms%20in%20the%20UAV%0Adomain%20pay%20very%20little%20attention%20to%20the%20naturalness%20of%20adversarial%20patches.%0AMoreover%2C%20imposing%20constraints%20directly%20on%20adversarial%20patches%20makes%20it%0Adifficult%20to%20generate%20patches%20that%20appear%20natural%20to%20the%20human%20eye%20while%0Aensuring%20a%20high%20attack%20success%20rate.%20We%20notice%20that%20patches%20are%20natural%20looking%0Awhen%20their%20overall%20color%20is%20consistent%20with%20the%20environment.%20Therefore%2C%20we%0Apropose%20a%20new%20method%20named%20Environmental%20Matching%20Attack%28EMA%29%20to%20address%20the%0Aissue%20of%20optimizing%20the%20adversarial%20patch%20under%20the%20constraints%20of%20color.%20To%0Athe%20best%20of%20our%20knowledge%2C%20this%20paper%20is%20the%20first%20to%20consider%20natural%20patches%0Ain%20the%20domain%20of%20UAVs.%20The%20EMA%20method%20exploits%20strong%20prior%20knowledge%20of%20a%0Apretrained%20stable%20diffusion%20to%20guide%20the%20optimization%20direction%20of%20the%0Aadversarial%20patch%2C%20where%20the%20text%20guidance%20can%20restrict%20the%20color%20of%20the%20patch.%0ATo%20better%20match%20the%20environment%2C%20the%20contrast%20and%20brightness%20of%20the%20patch%20are%0Aappropriately%20adjusted.%20Instead%20of%20optimizing%20the%20adversarial%20patch%20itself%2C%20we%0Aoptimize%20an%20adversarial%20perturbation%20patch%20which%20initializes%20to%20zero%20so%20that%0Athe%20model%20can%20better%20trade%20off%20attacking%20performance%20and%20naturalness.%0AExperiments%20conducted%20on%20the%20DroneVehicle%20and%20Carpk%20datasets%20have%20shown%20that%0Aour%20work%20can%20reach%20nearly%20the%20same%20attack%20performance%20in%20the%20digital%20attack%28no%0Agreater%20than%202%20in%20mAP%24%5C%25%24%29%2C%20surpass%20the%20baseline%20method%20in%20the%20physical%0Aspecific%20scenarios%2C%20and%20exhibit%20a%20significant%20advantage%20in%20terms%20of%20naturalness%0Ain%20visualization%20and%20color%20difference%20with%20the%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07595v1&entry.124074799=Read"},
{"title": "Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large\n  Language Models in Code Generation from Scientific Plots", "author": "Chengyue Wu and Yixiao Ge and Qiushan Guo and Jiahao Wang and Zhixuan Liang and Zeyu Lu and Ying Shan and Ping Luo", "abstract": "  The remarkable progress of Multi-modal Large Language Models (MLLMs) has\nattracted significant attention due to their superior performance in visual\ncontexts. However, their capabilities in turning visual figure to executable\ncode, have not been evaluated thoroughly. To address this, we introduce\nPlot2Code, a comprehensive visual coding benchmark designed for a fair and\nin-depth assessment of MLLMs. We carefully collect 132 manually selected\nhigh-quality matplotlib plots across six plot types from publicly available\nmatplotlib galleries. For each plot, we carefully offer its source code, and an\ndescriptive instruction summarized by GPT-4. This approach enables Plot2Code to\nextensively evaluate MLLMs' code capabilities across various input modalities.\nFurthermore, we propose three automatic evaluation metrics, including code pass\nrate, text-match ratio, and GPT-4V overall rating, for a fine-grained\nassessment of the output code and rendered images. Instead of simply judging\npass or fail, we employ GPT-4V to make an overall judgement between the\ngenerated and reference images, which has been shown to be consistent with\nhuman evaluation. The evaluation results, which include analyses of 14 MLLMs\nsuch as the proprietary GPT-4V, Gemini-Pro, and the open-sourced Mini-Gemini,\nhighlight the substantial challenges presented by Plot2Code. With Plot2Code, we\nreveal that most existing MLLMs struggle with visual coding for text-dense\nplots, heavily relying on textual instruction. We hope that the evaluation\nresults from Plot2Code on visual coding will guide the future development of\nMLLMs. All data involved with Plot2Code are available at\nhttps://huggingface.co/datasets/TencentARC/Plot2Code.\n", "link": "http://arxiv.org/abs/2405.07990v1", "date": "2024-05-13", "relevancy": 1.9873, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5074}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4898}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Plot2Code%3A%20A%20Comprehensive%20Benchmark%20for%20Evaluating%20Multi-modal%20Large%0A%20%20Language%20Models%20in%20Code%20Generation%20from%20Scientific%20Plots&body=Title%3A%20Plot2Code%3A%20A%20Comprehensive%20Benchmark%20for%20Evaluating%20Multi-modal%20Large%0A%20%20Language%20Models%20in%20Code%20Generation%20from%20Scientific%20Plots%0AAuthor%3A%20Chengyue%20Wu%20and%20Yixiao%20Ge%20and%20Qiushan%20Guo%20and%20Jiahao%20Wang%20and%20Zhixuan%20Liang%20and%20Zeyu%20Lu%20and%20Ying%20Shan%20and%20Ping%20Luo%0AAbstract%3A%20%20%20The%20remarkable%20progress%20of%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20has%0Aattracted%20significant%20attention%20due%20to%20their%20superior%20performance%20in%20visual%0Acontexts.%20However%2C%20their%20capabilities%20in%20turning%20visual%20figure%20to%20executable%0Acode%2C%20have%20not%20been%20evaluated%20thoroughly.%20To%20address%20this%2C%20we%20introduce%0APlot2Code%2C%20a%20comprehensive%20visual%20coding%20benchmark%20designed%20for%20a%20fair%20and%0Ain-depth%20assessment%20of%20MLLMs.%20We%20carefully%20collect%20132%20manually%20selected%0Ahigh-quality%20matplotlib%20plots%20across%20six%20plot%20types%20from%20publicly%20available%0Amatplotlib%20galleries.%20For%20each%20plot%2C%20we%20carefully%20offer%20its%20source%20code%2C%20and%20an%0Adescriptive%20instruction%20summarized%20by%20GPT-4.%20This%20approach%20enables%20Plot2Code%20to%0Aextensively%20evaluate%20MLLMs%27%20code%20capabilities%20across%20various%20input%20modalities.%0AFurthermore%2C%20we%20propose%20three%20automatic%20evaluation%20metrics%2C%20including%20code%20pass%0Arate%2C%20text-match%20ratio%2C%20and%20GPT-4V%20overall%20rating%2C%20for%20a%20fine-grained%0Aassessment%20of%20the%20output%20code%20and%20rendered%20images.%20Instead%20of%20simply%20judging%0Apass%20or%20fail%2C%20we%20employ%20GPT-4V%20to%20make%20an%20overall%20judgement%20between%20the%0Agenerated%20and%20reference%20images%2C%20which%20has%20been%20shown%20to%20be%20consistent%20with%0Ahuman%20evaluation.%20The%20evaluation%20results%2C%20which%20include%20analyses%20of%2014%20MLLMs%0Asuch%20as%20the%20proprietary%20GPT-4V%2C%20Gemini-Pro%2C%20and%20the%20open-sourced%20Mini-Gemini%2C%0Ahighlight%20the%20substantial%20challenges%20presented%20by%20Plot2Code.%20With%20Plot2Code%2C%20we%0Areveal%20that%20most%20existing%20MLLMs%20struggle%20with%20visual%20coding%20for%20text-dense%0Aplots%2C%20heavily%20relying%20on%20textual%20instruction.%20We%20hope%20that%20the%20evaluation%0Aresults%20from%20Plot2Code%20on%20visual%20coding%20will%20guide%20the%20future%20development%20of%0AMLLMs.%20All%20data%20involved%20with%20Plot2Code%20are%20available%20at%0Ahttps%3A//huggingface.co/datasets/TencentARC/Plot2Code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07990v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlot2Code%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Evaluating%2520Multi-modal%2520Large%250A%2520%2520Language%2520Models%2520in%2520Code%2520Generation%2520from%2520Scientific%2520Plots%26entry.906535625%3DChengyue%2520Wu%2520and%2520Yixiao%2520Ge%2520and%2520Qiushan%2520Guo%2520and%2520Jiahao%2520Wang%2520and%2520Zhixuan%2520Liang%2520and%2520Zeyu%2520Lu%2520and%2520Ying%2520Shan%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520The%2520remarkable%2520progress%2520of%2520Multi-modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520has%250Aattracted%2520significant%2520attention%2520due%2520to%2520their%2520superior%2520performance%2520in%2520visual%250Acontexts.%2520However%252C%2520their%2520capabilities%2520in%2520turning%2520visual%2520figure%2520to%2520executable%250Acode%252C%2520have%2520not%2520been%2520evaluated%2520thoroughly.%2520To%2520address%2520this%252C%2520we%2520introduce%250APlot2Code%252C%2520a%2520comprehensive%2520visual%2520coding%2520benchmark%2520designed%2520for%2520a%2520fair%2520and%250Ain-depth%2520assessment%2520of%2520MLLMs.%2520We%2520carefully%2520collect%2520132%2520manually%2520selected%250Ahigh-quality%2520matplotlib%2520plots%2520across%2520six%2520plot%2520types%2520from%2520publicly%2520available%250Amatplotlib%2520galleries.%2520For%2520each%2520plot%252C%2520we%2520carefully%2520offer%2520its%2520source%2520code%252C%2520and%2520an%250Adescriptive%2520instruction%2520summarized%2520by%2520GPT-4.%2520This%2520approach%2520enables%2520Plot2Code%2520to%250Aextensively%2520evaluate%2520MLLMs%2527%2520code%2520capabilities%2520across%2520various%2520input%2520modalities.%250AFurthermore%252C%2520we%2520propose%2520three%2520automatic%2520evaluation%2520metrics%252C%2520including%2520code%2520pass%250Arate%252C%2520text-match%2520ratio%252C%2520and%2520GPT-4V%2520overall%2520rating%252C%2520for%2520a%2520fine-grained%250Aassessment%2520of%2520the%2520output%2520code%2520and%2520rendered%2520images.%2520Instead%2520of%2520simply%2520judging%250Apass%2520or%2520fail%252C%2520we%2520employ%2520GPT-4V%2520to%2520make%2520an%2520overall%2520judgement%2520between%2520the%250Agenerated%2520and%2520reference%2520images%252C%2520which%2520has%2520been%2520shown%2520to%2520be%2520consistent%2520with%250Ahuman%2520evaluation.%2520The%2520evaluation%2520results%252C%2520which%2520include%2520analyses%2520of%252014%2520MLLMs%250Asuch%2520as%2520the%2520proprietary%2520GPT-4V%252C%2520Gemini-Pro%252C%2520and%2520the%2520open-sourced%2520Mini-Gemini%252C%250Ahighlight%2520the%2520substantial%2520challenges%2520presented%2520by%2520Plot2Code.%2520With%2520Plot2Code%252C%2520we%250Areveal%2520that%2520most%2520existing%2520MLLMs%2520struggle%2520with%2520visual%2520coding%2520for%2520text-dense%250Aplots%252C%2520heavily%2520relying%2520on%2520textual%2520instruction.%2520We%2520hope%2520that%2520the%2520evaluation%250Aresults%2520from%2520Plot2Code%2520on%2520visual%2520coding%2520will%2520guide%2520the%2520future%2520development%2520of%250AMLLMs.%2520All%2520data%2520involved%2520with%2520Plot2Code%2520are%2520available%2520at%250Ahttps%253A//huggingface.co/datasets/TencentARC/Plot2Code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07990v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Plot2Code%3A%20A%20Comprehensive%20Benchmark%20for%20Evaluating%20Multi-modal%20Large%0A%20%20Language%20Models%20in%20Code%20Generation%20from%20Scientific%20Plots&entry.906535625=Chengyue%20Wu%20and%20Yixiao%20Ge%20and%20Qiushan%20Guo%20and%20Jiahao%20Wang%20and%20Zhixuan%20Liang%20and%20Zeyu%20Lu%20and%20Ying%20Shan%20and%20Ping%20Luo&entry.1292438233=%20%20The%20remarkable%20progress%20of%20Multi-modal%20Large%20Language%20Models%20%28MLLMs%29%20has%0Aattracted%20significant%20attention%20due%20to%20their%20superior%20performance%20in%20visual%0Acontexts.%20However%2C%20their%20capabilities%20in%20turning%20visual%20figure%20to%20executable%0Acode%2C%20have%20not%20been%20evaluated%20thoroughly.%20To%20address%20this%2C%20we%20introduce%0APlot2Code%2C%20a%20comprehensive%20visual%20coding%20benchmark%20designed%20for%20a%20fair%20and%0Ain-depth%20assessment%20of%20MLLMs.%20We%20carefully%20collect%20132%20manually%20selected%0Ahigh-quality%20matplotlib%20plots%20across%20six%20plot%20types%20from%20publicly%20available%0Amatplotlib%20galleries.%20For%20each%20plot%2C%20we%20carefully%20offer%20its%20source%20code%2C%20and%20an%0Adescriptive%20instruction%20summarized%20by%20GPT-4.%20This%20approach%20enables%20Plot2Code%20to%0Aextensively%20evaluate%20MLLMs%27%20code%20capabilities%20across%20various%20input%20modalities.%0AFurthermore%2C%20we%20propose%20three%20automatic%20evaluation%20metrics%2C%20including%20code%20pass%0Arate%2C%20text-match%20ratio%2C%20and%20GPT-4V%20overall%20rating%2C%20for%20a%20fine-grained%0Aassessment%20of%20the%20output%20code%20and%20rendered%20images.%20Instead%20of%20simply%20judging%0Apass%20or%20fail%2C%20we%20employ%20GPT-4V%20to%20make%20an%20overall%20judgement%20between%20the%0Agenerated%20and%20reference%20images%2C%20which%20has%20been%20shown%20to%20be%20consistent%20with%0Ahuman%20evaluation.%20The%20evaluation%20results%2C%20which%20include%20analyses%20of%2014%20MLLMs%0Asuch%20as%20the%20proprietary%20GPT-4V%2C%20Gemini-Pro%2C%20and%20the%20open-sourced%20Mini-Gemini%2C%0Ahighlight%20the%20substantial%20challenges%20presented%20by%20Plot2Code.%20With%20Plot2Code%2C%20we%0Areveal%20that%20most%20existing%20MLLMs%20struggle%20with%20visual%20coding%20for%20text-dense%0Aplots%2C%20heavily%20relying%20on%20textual%20instruction.%20We%20hope%20that%20the%20evaluation%0Aresults%20from%20Plot2Code%20on%20visual%20coding%20will%20guide%20the%20future%20development%20of%0AMLLMs.%20All%20data%20involved%20with%20Plot2Code%20are%20available%20at%0Ahttps%3A//huggingface.co/datasets/TencentARC/Plot2Code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07990v1&entry.124074799=Read"},
{"title": "Localized Adaptive Risk Control", "author": "Matteo Zecchin and Osvaldo Simeone", "abstract": "  Adaptive Risk Control (ARC) is an online calibration strategy based on set\nprediction that offers worst-case deterministic long-term risk control, as well\nas statistical marginal coverage guarantees. ARC adjusts the size of the\nprediction set by varying a single scalar threshold based on feedback from past\ndecisions. In this work, we introduce Localized Adaptive Risk Control (L-ARC),\nan online calibration scheme that targets statistical localized risk guarantees\nranging from conditional risk to marginal risk, while preserving the worst-case\nperformance of ARC. L-ARC updates a threshold function within a reproducing\nkernel Hilbert space (RKHS), with the kernel determining the level of\nlocalization of the statistical risk guarantee. The theoretical results\nhighlight a trade-off between localization of the statistical risk and\nconvergence speed to the long-term risk target. Thanks to localization, L-ARC\nis demonstrated via experiments to produce prediction sets with risk guarantees\nacross different data subpopulations, significantly improving the fairness of\nthe calibrated model for tasks such as image segmentation and beam selection in\nwireless networks.\n", "link": "http://arxiv.org/abs/2405.07976v1", "date": "2024-05-13", "relevancy": 1.982, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5194}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4844}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localized%20Adaptive%20Risk%20Control&body=Title%3A%20Localized%20Adaptive%20Risk%20Control%0AAuthor%3A%20Matteo%20Zecchin%20and%20Osvaldo%20Simeone%0AAbstract%3A%20%20%20Adaptive%20Risk%20Control%20%28ARC%29%20is%20an%20online%20calibration%20strategy%20based%20on%20set%0Aprediction%20that%20offers%20worst-case%20deterministic%20long-term%20risk%20control%2C%20as%20well%0Aas%20statistical%20marginal%20coverage%20guarantees.%20ARC%20adjusts%20the%20size%20of%20the%0Aprediction%20set%20by%20varying%20a%20single%20scalar%20threshold%20based%20on%20feedback%20from%20past%0Adecisions.%20In%20this%20work%2C%20we%20introduce%20Localized%20Adaptive%20Risk%20Control%20%28L-ARC%29%2C%0Aan%20online%20calibration%20scheme%20that%20targets%20statistical%20localized%20risk%20guarantees%0Aranging%20from%20conditional%20risk%20to%20marginal%20risk%2C%20while%20preserving%20the%20worst-case%0Aperformance%20of%20ARC.%20L-ARC%20updates%20a%20threshold%20function%20within%20a%20reproducing%0Akernel%20Hilbert%20space%20%28RKHS%29%2C%20with%20the%20kernel%20determining%20the%20level%20of%0Alocalization%20of%20the%20statistical%20risk%20guarantee.%20The%20theoretical%20results%0Ahighlight%20a%20trade-off%20between%20localization%20of%20the%20statistical%20risk%20and%0Aconvergence%20speed%20to%20the%20long-term%20risk%20target.%20Thanks%20to%20localization%2C%20L-ARC%0Ais%20demonstrated%20via%20experiments%20to%20produce%20prediction%20sets%20with%20risk%20guarantees%0Aacross%20different%20data%20subpopulations%2C%20significantly%20improving%20the%20fairness%20of%0Athe%20calibrated%20model%20for%20tasks%20such%20as%20image%20segmentation%20and%20beam%20selection%20in%0Awireless%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalized%2520Adaptive%2520Risk%2520Control%26entry.906535625%3DMatteo%2520Zecchin%2520and%2520Osvaldo%2520Simeone%26entry.1292438233%3D%2520%2520Adaptive%2520Risk%2520Control%2520%2528ARC%2529%2520is%2520an%2520online%2520calibration%2520strategy%2520based%2520on%2520set%250Aprediction%2520that%2520offers%2520worst-case%2520deterministic%2520long-term%2520risk%2520control%252C%2520as%2520well%250Aas%2520statistical%2520marginal%2520coverage%2520guarantees.%2520ARC%2520adjusts%2520the%2520size%2520of%2520the%250Aprediction%2520set%2520by%2520varying%2520a%2520single%2520scalar%2520threshold%2520based%2520on%2520feedback%2520from%2520past%250Adecisions.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Localized%2520Adaptive%2520Risk%2520Control%2520%2528L-ARC%2529%252C%250Aan%2520online%2520calibration%2520scheme%2520that%2520targets%2520statistical%2520localized%2520risk%2520guarantees%250Aranging%2520from%2520conditional%2520risk%2520to%2520marginal%2520risk%252C%2520while%2520preserving%2520the%2520worst-case%250Aperformance%2520of%2520ARC.%2520L-ARC%2520updates%2520a%2520threshold%2520function%2520within%2520a%2520reproducing%250Akernel%2520Hilbert%2520space%2520%2528RKHS%2529%252C%2520with%2520the%2520kernel%2520determining%2520the%2520level%2520of%250Alocalization%2520of%2520the%2520statistical%2520risk%2520guarantee.%2520The%2520theoretical%2520results%250Ahighlight%2520a%2520trade-off%2520between%2520localization%2520of%2520the%2520statistical%2520risk%2520and%250Aconvergence%2520speed%2520to%2520the%2520long-term%2520risk%2520target.%2520Thanks%2520to%2520localization%252C%2520L-ARC%250Ais%2520demonstrated%2520via%2520experiments%2520to%2520produce%2520prediction%2520sets%2520with%2520risk%2520guarantees%250Aacross%2520different%2520data%2520subpopulations%252C%2520significantly%2520improving%2520the%2520fairness%2520of%250Athe%2520calibrated%2520model%2520for%2520tasks%2520such%2520as%2520image%2520segmentation%2520and%2520beam%2520selection%2520in%250Awireless%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localized%20Adaptive%20Risk%20Control&entry.906535625=Matteo%20Zecchin%20and%20Osvaldo%20Simeone&entry.1292438233=%20%20Adaptive%20Risk%20Control%20%28ARC%29%20is%20an%20online%20calibration%20strategy%20based%20on%20set%0Aprediction%20that%20offers%20worst-case%20deterministic%20long-term%20risk%20control%2C%20as%20well%0Aas%20statistical%20marginal%20coverage%20guarantees.%20ARC%20adjusts%20the%20size%20of%20the%0Aprediction%20set%20by%20varying%20a%20single%20scalar%20threshold%20based%20on%20feedback%20from%20past%0Adecisions.%20In%20this%20work%2C%20we%20introduce%20Localized%20Adaptive%20Risk%20Control%20%28L-ARC%29%2C%0Aan%20online%20calibration%20scheme%20that%20targets%20statistical%20localized%20risk%20guarantees%0Aranging%20from%20conditional%20risk%20to%20marginal%20risk%2C%20while%20preserving%20the%20worst-case%0Aperformance%20of%20ARC.%20L-ARC%20updates%20a%20threshold%20function%20within%20a%20reproducing%0Akernel%20Hilbert%20space%20%28RKHS%29%2C%20with%20the%20kernel%20determining%20the%20level%20of%0Alocalization%20of%20the%20statistical%20risk%20guarantee.%20The%20theoretical%20results%0Ahighlight%20a%20trade-off%20between%20localization%20of%20the%20statistical%20risk%20and%0Aconvergence%20speed%20to%20the%20long-term%20risk%20target.%20Thanks%20to%20localization%2C%20L-ARC%0Ais%20demonstrated%20via%20experiments%20to%20produce%20prediction%20sets%20with%20risk%20guarantees%0Aacross%20different%20data%20subpopulations%2C%20significantly%20improving%20the%20fairness%20of%0Athe%20calibrated%20model%20for%20tasks%20such%20as%20image%20segmentation%20and%20beam%20selection%20in%0Awireless%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07976v1&entry.124074799=Read"},
{"title": "MambaOut: Do We Really Need Mamba for Vision?", "author": "Weihao Yu and Xinchao Wang", "abstract": "  Mamba, an architecture with RNN-like token mixer of state space model (SSM),\nwas recently introduced to address the quadratic complexity of the attention\nmechanism and subsequently applied to vision tasks. Nevertheless, the\nperformance of Mamba for vision is often underwhelming when compared with\nconvolutional and attention-based models. In this paper, we delve into the\nessence of Mamba, and conceptually conclude that Mamba is ideally suited for\ntasks with long-sequence and autoregressive characteristics. For vision tasks,\nas image classification does not align with either characteristic, we\nhypothesize that Mamba is not necessary for this task; Detection and\nsegmentation tasks are also not autoregressive, yet they adhere to the\nlong-sequence characteristic, so we believe it is still worthwhile to explore\nMamba's potential for these tasks. To empirically verify our hypotheses, we\nconstruct a series of models named \\emph{MambaOut} through stacking Mamba\nblocks while removing their core token mixer, SSM. Experimental results\nstrongly support our hypotheses. Specifically, our MambaOut model surpasses all\nvisual Mamba models on ImageNet image classification, indicating that Mamba is\nindeed unnecessary for this task. As for detection and segmentation, MambaOut\ncannot match the performance of state-of-the-art visual Mamba models,\ndemonstrating the potential of Mamba for long-sequence visual tasks. The code\nis available at https://github.com/yuweihao/MambaOut\n", "link": "http://arxiv.org/abs/2405.07992v1", "date": "2024-05-13", "relevancy": 1.9818, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5188}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4919}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambaOut%3A%20Do%20We%20Really%20Need%20Mamba%20for%20Vision%3F&body=Title%3A%20MambaOut%3A%20Do%20We%20Really%20Need%20Mamba%20for%20Vision%3F%0AAuthor%3A%20Weihao%20Yu%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Mamba%2C%20an%20architecture%20with%20RNN-like%20token%20mixer%20of%20state%20space%20model%20%28SSM%29%2C%0Awas%20recently%20introduced%20to%20address%20the%20quadratic%20complexity%20of%20the%20attention%0Amechanism%20and%20subsequently%20applied%20to%20vision%20tasks.%20Nevertheless%2C%20the%0Aperformance%20of%20Mamba%20for%20vision%20is%20often%20underwhelming%20when%20compared%20with%0Aconvolutional%20and%20attention-based%20models.%20In%20this%20paper%2C%20we%20delve%20into%20the%0Aessence%20of%20Mamba%2C%20and%20conceptually%20conclude%20that%20Mamba%20is%20ideally%20suited%20for%0Atasks%20with%20long-sequence%20and%20autoregressive%20characteristics.%20For%20vision%20tasks%2C%0Aas%20image%20classification%20does%20not%20align%20with%20either%20characteristic%2C%20we%0Ahypothesize%20that%20Mamba%20is%20not%20necessary%20for%20this%20task%3B%20Detection%20and%0Asegmentation%20tasks%20are%20also%20not%20autoregressive%2C%20yet%20they%20adhere%20to%20the%0Along-sequence%20characteristic%2C%20so%20we%20believe%20it%20is%20still%20worthwhile%20to%20explore%0AMamba%27s%20potential%20for%20these%20tasks.%20To%20empirically%20verify%20our%20hypotheses%2C%20we%0Aconstruct%20a%20series%20of%20models%20named%20%5Cemph%7BMambaOut%7D%20through%20stacking%20Mamba%0Ablocks%20while%20removing%20their%20core%20token%20mixer%2C%20SSM.%20Experimental%20results%0Astrongly%20support%20our%20hypotheses.%20Specifically%2C%20our%20MambaOut%20model%20surpasses%20all%0Avisual%20Mamba%20models%20on%20ImageNet%20image%20classification%2C%20indicating%20that%20Mamba%20is%0Aindeed%20unnecessary%20for%20this%20task.%20As%20for%20detection%20and%20segmentation%2C%20MambaOut%0Acannot%20match%20the%20performance%20of%20state-of-the-art%20visual%20Mamba%20models%2C%0Ademonstrating%20the%20potential%20of%20Mamba%20for%20long-sequence%20visual%20tasks.%20The%20code%0Ais%20available%20at%20https%3A//github.com/yuweihao/MambaOut%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07992v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambaOut%253A%2520Do%2520We%2520Really%2520Need%2520Mamba%2520for%2520Vision%253F%26entry.906535625%3DWeihao%2520Yu%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Mamba%252C%2520an%2520architecture%2520with%2520RNN-like%2520token%2520mixer%2520of%2520state%2520space%2520model%2520%2528SSM%2529%252C%250Awas%2520recently%2520introduced%2520to%2520address%2520the%2520quadratic%2520complexity%2520of%2520the%2520attention%250Amechanism%2520and%2520subsequently%2520applied%2520to%2520vision%2520tasks.%2520Nevertheless%252C%2520the%250Aperformance%2520of%2520Mamba%2520for%2520vision%2520is%2520often%2520underwhelming%2520when%2520compared%2520with%250Aconvolutional%2520and%2520attention-based%2520models.%2520In%2520this%2520paper%252C%2520we%2520delve%2520into%2520the%250Aessence%2520of%2520Mamba%252C%2520and%2520conceptually%2520conclude%2520that%2520Mamba%2520is%2520ideally%2520suited%2520for%250Atasks%2520with%2520long-sequence%2520and%2520autoregressive%2520characteristics.%2520For%2520vision%2520tasks%252C%250Aas%2520image%2520classification%2520does%2520not%2520align%2520with%2520either%2520characteristic%252C%2520we%250Ahypothesize%2520that%2520Mamba%2520is%2520not%2520necessary%2520for%2520this%2520task%253B%2520Detection%2520and%250Asegmentation%2520tasks%2520are%2520also%2520not%2520autoregressive%252C%2520yet%2520they%2520adhere%2520to%2520the%250Along-sequence%2520characteristic%252C%2520so%2520we%2520believe%2520it%2520is%2520still%2520worthwhile%2520to%2520explore%250AMamba%2527s%2520potential%2520for%2520these%2520tasks.%2520To%2520empirically%2520verify%2520our%2520hypotheses%252C%2520we%250Aconstruct%2520a%2520series%2520of%2520models%2520named%2520%255Cemph%257BMambaOut%257D%2520through%2520stacking%2520Mamba%250Ablocks%2520while%2520removing%2520their%2520core%2520token%2520mixer%252C%2520SSM.%2520Experimental%2520results%250Astrongly%2520support%2520our%2520hypotheses.%2520Specifically%252C%2520our%2520MambaOut%2520model%2520surpasses%2520all%250Avisual%2520Mamba%2520models%2520on%2520ImageNet%2520image%2520classification%252C%2520indicating%2520that%2520Mamba%2520is%250Aindeed%2520unnecessary%2520for%2520this%2520task.%2520As%2520for%2520detection%2520and%2520segmentation%252C%2520MambaOut%250Acannot%2520match%2520the%2520performance%2520of%2520state-of-the-art%2520visual%2520Mamba%2520models%252C%250Ademonstrating%2520the%2520potential%2520of%2520Mamba%2520for%2520long-sequence%2520visual%2520tasks.%2520The%2520code%250Ais%2520available%2520at%2520https%253A//github.com/yuweihao/MambaOut%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07992v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaOut%3A%20Do%20We%20Really%20Need%20Mamba%20for%20Vision%3F&entry.906535625=Weihao%20Yu%20and%20Xinchao%20Wang&entry.1292438233=%20%20Mamba%2C%20an%20architecture%20with%20RNN-like%20token%20mixer%20of%20state%20space%20model%20%28SSM%29%2C%0Awas%20recently%20introduced%20to%20address%20the%20quadratic%20complexity%20of%20the%20attention%0Amechanism%20and%20subsequently%20applied%20to%20vision%20tasks.%20Nevertheless%2C%20the%0Aperformance%20of%20Mamba%20for%20vision%20is%20often%20underwhelming%20when%20compared%20with%0Aconvolutional%20and%20attention-based%20models.%20In%20this%20paper%2C%20we%20delve%20into%20the%0Aessence%20of%20Mamba%2C%20and%20conceptually%20conclude%20that%20Mamba%20is%20ideally%20suited%20for%0Atasks%20with%20long-sequence%20and%20autoregressive%20characteristics.%20For%20vision%20tasks%2C%0Aas%20image%20classification%20does%20not%20align%20with%20either%20characteristic%2C%20we%0Ahypothesize%20that%20Mamba%20is%20not%20necessary%20for%20this%20task%3B%20Detection%20and%0Asegmentation%20tasks%20are%20also%20not%20autoregressive%2C%20yet%20they%20adhere%20to%20the%0Along-sequence%20characteristic%2C%20so%20we%20believe%20it%20is%20still%20worthwhile%20to%20explore%0AMamba%27s%20potential%20for%20these%20tasks.%20To%20empirically%20verify%20our%20hypotheses%2C%20we%0Aconstruct%20a%20series%20of%20models%20named%20%5Cemph%7BMambaOut%7D%20through%20stacking%20Mamba%0Ablocks%20while%20removing%20their%20core%20token%20mixer%2C%20SSM.%20Experimental%20results%0Astrongly%20support%20our%20hypotheses.%20Specifically%2C%20our%20MambaOut%20model%20surpasses%20all%0Avisual%20Mamba%20models%20on%20ImageNet%20image%20classification%2C%20indicating%20that%20Mamba%20is%0Aindeed%20unnecessary%20for%20this%20task.%20As%20for%20detection%20and%20segmentation%2C%20MambaOut%0Acannot%20match%20the%20performance%20of%20state-of-the-art%20visual%20Mamba%20models%2C%0Ademonstrating%20the%20potential%20of%20Mamba%20for%20long-sequence%20visual%20tasks.%20The%20code%0Ais%20available%20at%20https%3A//github.com/yuweihao/MambaOut%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07992v1&entry.124074799=Read"},
{"title": "Enhancing Clinically Significant Prostate Cancer Prediction in\n  T2-weighted Images through Transfer Learning from Breast Cancer", "author": "Chi-en Amy Tai and Alexander Wong", "abstract": "  In 2020, prostate cancer saw a staggering 1.4 million new cases, resulting in\nover 375,000 deaths. The accurate identification of clinically significant\nprostate cancer is crucial for delivering effective treatment to patients.\nConsequently, there has been a surge in research exploring the application of\ndeep neural networks to predict clinical significance based on magnetic\nresonance images. However, these networks demand extensive datasets to attain\noptimal performance. Recently, transfer learning emerged as a technique that\nleverages acquired features from a domain with richer data to enhance the\nperformance of a domain with limited data. In this paper, we investigate the\nimprovement of clinically significant prostate cancer prediction in T2-weighted\nimages through transfer learning from breast cancer. The results demonstrate a\nremarkable improvement of over 30% in leave-one-out cross-validation accuracy.\n", "link": "http://arxiv.org/abs/2405.07869v1", "date": "2024-05-13", "relevancy": 1.9727, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5007}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4912}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Clinically%20Significant%20Prostate%20Cancer%20Prediction%20in%0A%20%20T2-weighted%20Images%20through%20Transfer%20Learning%20from%20Breast%20Cancer&body=Title%3A%20Enhancing%20Clinically%20Significant%20Prostate%20Cancer%20Prediction%20in%0A%20%20T2-weighted%20Images%20through%20Transfer%20Learning%20from%20Breast%20Cancer%0AAuthor%3A%20Chi-en%20Amy%20Tai%20and%20Alexander%20Wong%0AAbstract%3A%20%20%20In%202020%2C%20prostate%20cancer%20saw%20a%20staggering%201.4%20million%20new%20cases%2C%20resulting%20in%0Aover%20375%2C000%20deaths.%20The%20accurate%20identification%20of%20clinically%20significant%0Aprostate%20cancer%20is%20crucial%20for%20delivering%20effective%20treatment%20to%20patients.%0AConsequently%2C%20there%20has%20been%20a%20surge%20in%20research%20exploring%20the%20application%20of%0Adeep%20neural%20networks%20to%20predict%20clinical%20significance%20based%20on%20magnetic%0Aresonance%20images.%20However%2C%20these%20networks%20demand%20extensive%20datasets%20to%20attain%0Aoptimal%20performance.%20Recently%2C%20transfer%20learning%20emerged%20as%20a%20technique%20that%0Aleverages%20acquired%20features%20from%20a%20domain%20with%20richer%20data%20to%20enhance%20the%0Aperformance%20of%20a%20domain%20with%20limited%20data.%20In%20this%20paper%2C%20we%20investigate%20the%0Aimprovement%20of%20clinically%20significant%20prostate%20cancer%20prediction%20in%20T2-weighted%0Aimages%20through%20transfer%20learning%20from%20breast%20cancer.%20The%20results%20demonstrate%20a%0Aremarkable%20improvement%20of%20over%2030%25%20in%20leave-one-out%20cross-validation%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07869v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Clinically%2520Significant%2520Prostate%2520Cancer%2520Prediction%2520in%250A%2520%2520T2-weighted%2520Images%2520through%2520Transfer%2520Learning%2520from%2520Breast%2520Cancer%26entry.906535625%3DChi-en%2520Amy%2520Tai%2520and%2520Alexander%2520Wong%26entry.1292438233%3D%2520%2520In%25202020%252C%2520prostate%2520cancer%2520saw%2520a%2520staggering%25201.4%2520million%2520new%2520cases%252C%2520resulting%2520in%250Aover%2520375%252C000%2520deaths.%2520The%2520accurate%2520identification%2520of%2520clinically%2520significant%250Aprostate%2520cancer%2520is%2520crucial%2520for%2520delivering%2520effective%2520treatment%2520to%2520patients.%250AConsequently%252C%2520there%2520has%2520been%2520a%2520surge%2520in%2520research%2520exploring%2520the%2520application%2520of%250Adeep%2520neural%2520networks%2520to%2520predict%2520clinical%2520significance%2520based%2520on%2520magnetic%250Aresonance%2520images.%2520However%252C%2520these%2520networks%2520demand%2520extensive%2520datasets%2520to%2520attain%250Aoptimal%2520performance.%2520Recently%252C%2520transfer%2520learning%2520emerged%2520as%2520a%2520technique%2520that%250Aleverages%2520acquired%2520features%2520from%2520a%2520domain%2520with%2520richer%2520data%2520to%2520enhance%2520the%250Aperformance%2520of%2520a%2520domain%2520with%2520limited%2520data.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%250Aimprovement%2520of%2520clinically%2520significant%2520prostate%2520cancer%2520prediction%2520in%2520T2-weighted%250Aimages%2520through%2520transfer%2520learning%2520from%2520breast%2520cancer.%2520The%2520results%2520demonstrate%2520a%250Aremarkable%2520improvement%2520of%2520over%252030%2525%2520in%2520leave-one-out%2520cross-validation%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07869v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Clinically%20Significant%20Prostate%20Cancer%20Prediction%20in%0A%20%20T2-weighted%20Images%20through%20Transfer%20Learning%20from%20Breast%20Cancer&entry.906535625=Chi-en%20Amy%20Tai%20and%20Alexander%20Wong&entry.1292438233=%20%20In%202020%2C%20prostate%20cancer%20saw%20a%20staggering%201.4%20million%20new%20cases%2C%20resulting%20in%0Aover%20375%2C000%20deaths.%20The%20accurate%20identification%20of%20clinically%20significant%0Aprostate%20cancer%20is%20crucial%20for%20delivering%20effective%20treatment%20to%20patients.%0AConsequently%2C%20there%20has%20been%20a%20surge%20in%20research%20exploring%20the%20application%20of%0Adeep%20neural%20networks%20to%20predict%20clinical%20significance%20based%20on%20magnetic%0Aresonance%20images.%20However%2C%20these%20networks%20demand%20extensive%20datasets%20to%20attain%0Aoptimal%20performance.%20Recently%2C%20transfer%20learning%20emerged%20as%20a%20technique%20that%0Aleverages%20acquired%20features%20from%20a%20domain%20with%20richer%20data%20to%20enhance%20the%0Aperformance%20of%20a%20domain%20with%20limited%20data.%20In%20this%20paper%2C%20we%20investigate%20the%0Aimprovement%20of%20clinically%20significant%20prostate%20cancer%20prediction%20in%20T2-weighted%0Aimages%20through%20transfer%20learning%20from%20breast%20cancer.%20The%20results%20demonstrate%20a%0Aremarkable%20improvement%20of%20over%2030%25%20in%20leave-one-out%20cross-validation%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07869v1&entry.124074799=Read"},
{"title": "Temporal Interest Network for User Response Prediction", "author": "Haolin Zhou and Junwei Pan and Xinyi Zhou and Xihua Chen and Jie Jiang and Xiaofeng Gao and Guihai Chen", "abstract": "  User response prediction is essential in industrial recommendation systems,\nsuch as online display advertising. Among all the features in recommendation\nmodels, user behaviors are among the most critical. Many works have revealed\nthat a user's behavior reflects her interest in the candidate item, owing to\nthe semantic or temporal correlation between behaviors and the candidate. While\nthe literature has individually examined each of these correlations,\nresearchers have yet to analyze them in combination, that is, the\nsemantic-temporal correlation. We empirically measure this correlation and\nobserve intuitive yet robust patterns. We then examine several popular user\ninterest models and find that, surprisingly, none of them learn such\ncorrelation well.\n  To fill this gap, we propose a Temporal Interest Network (TIN) to capture the\nsemantic-temporal correlation simultaneously between behaviors and the target.\nWe achieve this by incorporating target-aware temporal encoding, in addition to\nsemantic encoding, to represent behaviors and the target. Furthermore, we\nconduct explicit 4-way interaction by deploying target-aware attention and\ntarget-aware representation to capture both semantic and temporal correlation.\nWe conduct comprehensive evaluations on two popular public datasets, and our\nproposed TIN outperforms the best-performing baselines by 0.43% and 0.29% on\nGAUC, respectively. During online A/B testing in Tencent's advertising\nplatform, TIN achieves 1.65% cost lift and 1.93% GMV lift over the base model.\nIt has been successfully deployed in production since October 2023, serving the\nWeChat Moments traffic. We have released our code at\nhttps://github.com/zhouxy1003/TIN.\n", "link": "http://arxiv.org/abs/2308.08487v3", "date": "2024-05-13", "relevancy": 1.969, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5045}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4854}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Interest%20Network%20for%20User%20Response%20Prediction&body=Title%3A%20Temporal%20Interest%20Network%20for%20User%20Response%20Prediction%0AAuthor%3A%20Haolin%20Zhou%20and%20Junwei%20Pan%20and%20Xinyi%20Zhou%20and%20Xihua%20Chen%20and%20Jie%20Jiang%20and%20Xiaofeng%20Gao%20and%20Guihai%20Chen%0AAbstract%3A%20%20%20User%20response%20prediction%20is%20essential%20in%20industrial%20recommendation%20systems%2C%0Asuch%20as%20online%20display%20advertising.%20Among%20all%20the%20features%20in%20recommendation%0Amodels%2C%20user%20behaviors%20are%20among%20the%20most%20critical.%20Many%20works%20have%20revealed%0Athat%20a%20user%27s%20behavior%20reflects%20her%20interest%20in%20the%20candidate%20item%2C%20owing%20to%0Athe%20semantic%20or%20temporal%20correlation%20between%20behaviors%20and%20the%20candidate.%20While%0Athe%20literature%20has%20individually%20examined%20each%20of%20these%20correlations%2C%0Aresearchers%20have%20yet%20to%20analyze%20them%20in%20combination%2C%20that%20is%2C%20the%0Asemantic-temporal%20correlation.%20We%20empirically%20measure%20this%20correlation%20and%0Aobserve%20intuitive%20yet%20robust%20patterns.%20We%20then%20examine%20several%20popular%20user%0Ainterest%20models%20and%20find%20that%2C%20surprisingly%2C%20none%20of%20them%20learn%20such%0Acorrelation%20well.%0A%20%20To%20fill%20this%20gap%2C%20we%20propose%20a%20Temporal%20Interest%20Network%20%28TIN%29%20to%20capture%20the%0Asemantic-temporal%20correlation%20simultaneously%20between%20behaviors%20and%20the%20target.%0AWe%20achieve%20this%20by%20incorporating%20target-aware%20temporal%20encoding%2C%20in%20addition%20to%0Asemantic%20encoding%2C%20to%20represent%20behaviors%20and%20the%20target.%20Furthermore%2C%20we%0Aconduct%20explicit%204-way%20interaction%20by%20deploying%20target-aware%20attention%20and%0Atarget-aware%20representation%20to%20capture%20both%20semantic%20and%20temporal%20correlation.%0AWe%20conduct%20comprehensive%20evaluations%20on%20two%20popular%20public%20datasets%2C%20and%20our%0Aproposed%20TIN%20outperforms%20the%20best-performing%20baselines%20by%200.43%25%20and%200.29%25%20on%0AGAUC%2C%20respectively.%20During%20online%20A/B%20testing%20in%20Tencent%27s%20advertising%0Aplatform%2C%20TIN%20achieves%201.65%25%20cost%20lift%20and%201.93%25%20GMV%20lift%20over%20the%20base%20model.%0AIt%20has%20been%20successfully%20deployed%20in%20production%20since%20October%202023%2C%20serving%20the%0AWeChat%20Moments%20traffic.%20We%20have%20released%20our%20code%20at%0Ahttps%3A//github.com/zhouxy1003/TIN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.08487v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Interest%2520Network%2520for%2520User%2520Response%2520Prediction%26entry.906535625%3DHaolin%2520Zhou%2520and%2520Junwei%2520Pan%2520and%2520Xinyi%2520Zhou%2520and%2520Xihua%2520Chen%2520and%2520Jie%2520Jiang%2520and%2520Xiaofeng%2520Gao%2520and%2520Guihai%2520Chen%26entry.1292438233%3D%2520%2520User%2520response%2520prediction%2520is%2520essential%2520in%2520industrial%2520recommendation%2520systems%252C%250Asuch%2520as%2520online%2520display%2520advertising.%2520Among%2520all%2520the%2520features%2520in%2520recommendation%250Amodels%252C%2520user%2520behaviors%2520are%2520among%2520the%2520most%2520critical.%2520Many%2520works%2520have%2520revealed%250Athat%2520a%2520user%2527s%2520behavior%2520reflects%2520her%2520interest%2520in%2520the%2520candidate%2520item%252C%2520owing%2520to%250Athe%2520semantic%2520or%2520temporal%2520correlation%2520between%2520behaviors%2520and%2520the%2520candidate.%2520While%250Athe%2520literature%2520has%2520individually%2520examined%2520each%2520of%2520these%2520correlations%252C%250Aresearchers%2520have%2520yet%2520to%2520analyze%2520them%2520in%2520combination%252C%2520that%2520is%252C%2520the%250Asemantic-temporal%2520correlation.%2520We%2520empirically%2520measure%2520this%2520correlation%2520and%250Aobserve%2520intuitive%2520yet%2520robust%2520patterns.%2520We%2520then%2520examine%2520several%2520popular%2520user%250Ainterest%2520models%2520and%2520find%2520that%252C%2520surprisingly%252C%2520none%2520of%2520them%2520learn%2520such%250Acorrelation%2520well.%250A%2520%2520To%2520fill%2520this%2520gap%252C%2520we%2520propose%2520a%2520Temporal%2520Interest%2520Network%2520%2528TIN%2529%2520to%2520capture%2520the%250Asemantic-temporal%2520correlation%2520simultaneously%2520between%2520behaviors%2520and%2520the%2520target.%250AWe%2520achieve%2520this%2520by%2520incorporating%2520target-aware%2520temporal%2520encoding%252C%2520in%2520addition%2520to%250Asemantic%2520encoding%252C%2520to%2520represent%2520behaviors%2520and%2520the%2520target.%2520Furthermore%252C%2520we%250Aconduct%2520explicit%25204-way%2520interaction%2520by%2520deploying%2520target-aware%2520attention%2520and%250Atarget-aware%2520representation%2520to%2520capture%2520both%2520semantic%2520and%2520temporal%2520correlation.%250AWe%2520conduct%2520comprehensive%2520evaluations%2520on%2520two%2520popular%2520public%2520datasets%252C%2520and%2520our%250Aproposed%2520TIN%2520outperforms%2520the%2520best-performing%2520baselines%2520by%25200.43%2525%2520and%25200.29%2525%2520on%250AGAUC%252C%2520respectively.%2520During%2520online%2520A/B%2520testing%2520in%2520Tencent%2527s%2520advertising%250Aplatform%252C%2520TIN%2520achieves%25201.65%2525%2520cost%2520lift%2520and%25201.93%2525%2520GMV%2520lift%2520over%2520the%2520base%2520model.%250AIt%2520has%2520been%2520successfully%2520deployed%2520in%2520production%2520since%2520October%25202023%252C%2520serving%2520the%250AWeChat%2520Moments%2520traffic.%2520We%2520have%2520released%2520our%2520code%2520at%250Ahttps%253A//github.com/zhouxy1003/TIN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.08487v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Interest%20Network%20for%20User%20Response%20Prediction&entry.906535625=Haolin%20Zhou%20and%20Junwei%20Pan%20and%20Xinyi%20Zhou%20and%20Xihua%20Chen%20and%20Jie%20Jiang%20and%20Xiaofeng%20Gao%20and%20Guihai%20Chen&entry.1292438233=%20%20User%20response%20prediction%20is%20essential%20in%20industrial%20recommendation%20systems%2C%0Asuch%20as%20online%20display%20advertising.%20Among%20all%20the%20features%20in%20recommendation%0Amodels%2C%20user%20behaviors%20are%20among%20the%20most%20critical.%20Many%20works%20have%20revealed%0Athat%20a%20user%27s%20behavior%20reflects%20her%20interest%20in%20the%20candidate%20item%2C%20owing%20to%0Athe%20semantic%20or%20temporal%20correlation%20between%20behaviors%20and%20the%20candidate.%20While%0Athe%20literature%20has%20individually%20examined%20each%20of%20these%20correlations%2C%0Aresearchers%20have%20yet%20to%20analyze%20them%20in%20combination%2C%20that%20is%2C%20the%0Asemantic-temporal%20correlation.%20We%20empirically%20measure%20this%20correlation%20and%0Aobserve%20intuitive%20yet%20robust%20patterns.%20We%20then%20examine%20several%20popular%20user%0Ainterest%20models%20and%20find%20that%2C%20surprisingly%2C%20none%20of%20them%20learn%20such%0Acorrelation%20well.%0A%20%20To%20fill%20this%20gap%2C%20we%20propose%20a%20Temporal%20Interest%20Network%20%28TIN%29%20to%20capture%20the%0Asemantic-temporal%20correlation%20simultaneously%20between%20behaviors%20and%20the%20target.%0AWe%20achieve%20this%20by%20incorporating%20target-aware%20temporal%20encoding%2C%20in%20addition%20to%0Asemantic%20encoding%2C%20to%20represent%20behaviors%20and%20the%20target.%20Furthermore%2C%20we%0Aconduct%20explicit%204-way%20interaction%20by%20deploying%20target-aware%20attention%20and%0Atarget-aware%20representation%20to%20capture%20both%20semantic%20and%20temporal%20correlation.%0AWe%20conduct%20comprehensive%20evaluations%20on%20two%20popular%20public%20datasets%2C%20and%20our%0Aproposed%20TIN%20outperforms%20the%20best-performing%20baselines%20by%200.43%25%20and%200.29%25%20on%0AGAUC%2C%20respectively.%20During%20online%20A/B%20testing%20in%20Tencent%27s%20advertising%0Aplatform%2C%20TIN%20achieves%201.65%25%20cost%20lift%20and%201.93%25%20GMV%20lift%20over%20the%20base%20model.%0AIt%20has%20been%20successfully%20deployed%20in%20production%20since%20October%202023%2C%20serving%20the%0AWeChat%20Moments%20traffic.%20We%20have%20released%20our%20code%20at%0Ahttps%3A//github.com/zhouxy1003/TIN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.08487v3&entry.124074799=Read"},
{"title": "DeepHYDRA: Resource-Efficient Time-Series Anomaly Detection in\n  Dynamically-Configured Systems", "author": "Franz Kevin Stehle and Wainer Vandelli and Giuseppe Avolio and Felix Zahn and Holger Fr\u00f6ning", "abstract": "  Anomaly detection in distributed systems such as High-Performance Computing\n(HPC) clusters is vital for early fault detection, performance optimisation,\nsecurity monitoring, reliability in general but also operational insights. Deep\nNeural Networks have seen successful use in detecting long-term anomalies in\nmultidimensional data, originating for instance from industrial or medical\nsystems, or weather prediction. A downside of such methods is that they require\na static input size, or lose data through cropping, sampling, or other\ndimensionality reduction methods, making deployment on systems with variability\non monitored data channels, such as computing clusters difficult. To address\nthese problems, we present DeepHYDRA (Deep Hybrid DBSCAN/Reduction-Based\nAnomaly Detection) which combines DBSCAN and learning-based anomaly detection.\nDBSCAN clustering is used to find point anomalies in time-series data,\nmitigating the risk of missing outliers through loss of information when\nreducing input data to a fixed number of channels. A deep learning-based\ntime-series anomaly detection method is then applied to the reduced data in\norder to identify long-term outliers. This hybrid approach reduces the chances\nof missing anomalies that might be made indistinguishable from normal data by\nthe reduction process, and likewise enables the algorithm to be scalable and\ntolerate partial system failures while retaining its detection capabilities.\nUsing a subset of the well-known SMD dataset family, a modified variant of the\nEclipse dataset, as well as an in-house dataset with a large variability in\nactive data channels, made publicly available with this work, we furthermore\nanalyse computational intensity, memory footprint, and activation counts.\nDeepHYDRA is shown to reliably detect different types of anomalies in both\nlarge and complex datasets.\n", "link": "http://arxiv.org/abs/2405.07749v1", "date": "2024-05-13", "relevancy": 1.9616, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4964}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4903}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepHYDRA%3A%20Resource-Efficient%20Time-Series%20Anomaly%20Detection%20in%0A%20%20Dynamically-Configured%20Systems&body=Title%3A%20DeepHYDRA%3A%20Resource-Efficient%20Time-Series%20Anomaly%20Detection%20in%0A%20%20Dynamically-Configured%20Systems%0AAuthor%3A%20Franz%20Kevin%20Stehle%20and%20Wainer%20Vandelli%20and%20Giuseppe%20Avolio%20and%20Felix%20Zahn%20and%20Holger%20Fr%C3%B6ning%0AAbstract%3A%20%20%20Anomaly%20detection%20in%20distributed%20systems%20such%20as%20High-Performance%20Computing%0A%28HPC%29%20clusters%20is%20vital%20for%20early%20fault%20detection%2C%20performance%20optimisation%2C%0Asecurity%20monitoring%2C%20reliability%20in%20general%20but%20also%20operational%20insights.%20Deep%0ANeural%20Networks%20have%20seen%20successful%20use%20in%20detecting%20long-term%20anomalies%20in%0Amultidimensional%20data%2C%20originating%20for%20instance%20from%20industrial%20or%20medical%0Asystems%2C%20or%20weather%20prediction.%20A%20downside%20of%20such%20methods%20is%20that%20they%20require%0Aa%20static%20input%20size%2C%20or%20lose%20data%20through%20cropping%2C%20sampling%2C%20or%20other%0Adimensionality%20reduction%20methods%2C%20making%20deployment%20on%20systems%20with%20variability%0Aon%20monitored%20data%20channels%2C%20such%20as%20computing%20clusters%20difficult.%20To%20address%0Athese%20problems%2C%20we%20present%20DeepHYDRA%20%28Deep%20Hybrid%20DBSCAN/Reduction-Based%0AAnomaly%20Detection%29%20which%20combines%20DBSCAN%20and%20learning-based%20anomaly%20detection.%0ADBSCAN%20clustering%20is%20used%20to%20find%20point%20anomalies%20in%20time-series%20data%2C%0Amitigating%20the%20risk%20of%20missing%20outliers%20through%20loss%20of%20information%20when%0Areducing%20input%20data%20to%20a%20fixed%20number%20of%20channels.%20A%20deep%20learning-based%0Atime-series%20anomaly%20detection%20method%20is%20then%20applied%20to%20the%20reduced%20data%20in%0Aorder%20to%20identify%20long-term%20outliers.%20This%20hybrid%20approach%20reduces%20the%20chances%0Aof%20missing%20anomalies%20that%20might%20be%20made%20indistinguishable%20from%20normal%20data%20by%0Athe%20reduction%20process%2C%20and%20likewise%20enables%20the%20algorithm%20to%20be%20scalable%20and%0Atolerate%20partial%20system%20failures%20while%20retaining%20its%20detection%20capabilities.%0AUsing%20a%20subset%20of%20the%20well-known%20SMD%20dataset%20family%2C%20a%20modified%20variant%20of%20the%0AEclipse%20dataset%2C%20as%20well%20as%20an%20in-house%20dataset%20with%20a%20large%20variability%20in%0Aactive%20data%20channels%2C%20made%20publicly%20available%20with%20this%20work%2C%20we%20furthermore%0Aanalyse%20computational%20intensity%2C%20memory%20footprint%2C%20and%20activation%20counts.%0ADeepHYDRA%20is%20shown%20to%20reliably%20detect%20different%20types%20of%20anomalies%20in%20both%0Alarge%20and%20complex%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepHYDRA%253A%2520Resource-Efficient%2520Time-Series%2520Anomaly%2520Detection%2520in%250A%2520%2520Dynamically-Configured%2520Systems%26entry.906535625%3DFranz%2520Kevin%2520Stehle%2520and%2520Wainer%2520Vandelli%2520and%2520Giuseppe%2520Avolio%2520and%2520Felix%2520Zahn%2520and%2520Holger%2520Fr%25C3%25B6ning%26entry.1292438233%3D%2520%2520Anomaly%2520detection%2520in%2520distributed%2520systems%2520such%2520as%2520High-Performance%2520Computing%250A%2528HPC%2529%2520clusters%2520is%2520vital%2520for%2520early%2520fault%2520detection%252C%2520performance%2520optimisation%252C%250Asecurity%2520monitoring%252C%2520reliability%2520in%2520general%2520but%2520also%2520operational%2520insights.%2520Deep%250ANeural%2520Networks%2520have%2520seen%2520successful%2520use%2520in%2520detecting%2520long-term%2520anomalies%2520in%250Amultidimensional%2520data%252C%2520originating%2520for%2520instance%2520from%2520industrial%2520or%2520medical%250Asystems%252C%2520or%2520weather%2520prediction.%2520A%2520downside%2520of%2520such%2520methods%2520is%2520that%2520they%2520require%250Aa%2520static%2520input%2520size%252C%2520or%2520lose%2520data%2520through%2520cropping%252C%2520sampling%252C%2520or%2520other%250Adimensionality%2520reduction%2520methods%252C%2520making%2520deployment%2520on%2520systems%2520with%2520variability%250Aon%2520monitored%2520data%2520channels%252C%2520such%2520as%2520computing%2520clusters%2520difficult.%2520To%2520address%250Athese%2520problems%252C%2520we%2520present%2520DeepHYDRA%2520%2528Deep%2520Hybrid%2520DBSCAN/Reduction-Based%250AAnomaly%2520Detection%2529%2520which%2520combines%2520DBSCAN%2520and%2520learning-based%2520anomaly%2520detection.%250ADBSCAN%2520clustering%2520is%2520used%2520to%2520find%2520point%2520anomalies%2520in%2520time-series%2520data%252C%250Amitigating%2520the%2520risk%2520of%2520missing%2520outliers%2520through%2520loss%2520of%2520information%2520when%250Areducing%2520input%2520data%2520to%2520a%2520fixed%2520number%2520of%2520channels.%2520A%2520deep%2520learning-based%250Atime-series%2520anomaly%2520detection%2520method%2520is%2520then%2520applied%2520to%2520the%2520reduced%2520data%2520in%250Aorder%2520to%2520identify%2520long-term%2520outliers.%2520This%2520hybrid%2520approach%2520reduces%2520the%2520chances%250Aof%2520missing%2520anomalies%2520that%2520might%2520be%2520made%2520indistinguishable%2520from%2520normal%2520data%2520by%250Athe%2520reduction%2520process%252C%2520and%2520likewise%2520enables%2520the%2520algorithm%2520to%2520be%2520scalable%2520and%250Atolerate%2520partial%2520system%2520failures%2520while%2520retaining%2520its%2520detection%2520capabilities.%250AUsing%2520a%2520subset%2520of%2520the%2520well-known%2520SMD%2520dataset%2520family%252C%2520a%2520modified%2520variant%2520of%2520the%250AEclipse%2520dataset%252C%2520as%2520well%2520as%2520an%2520in-house%2520dataset%2520with%2520a%2520large%2520variability%2520in%250Aactive%2520data%2520channels%252C%2520made%2520publicly%2520available%2520with%2520this%2520work%252C%2520we%2520furthermore%250Aanalyse%2520computational%2520intensity%252C%2520memory%2520footprint%252C%2520and%2520activation%2520counts.%250ADeepHYDRA%2520is%2520shown%2520to%2520reliably%2520detect%2520different%2520types%2520of%2520anomalies%2520in%2520both%250Alarge%2520and%2520complex%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepHYDRA%3A%20Resource-Efficient%20Time-Series%20Anomaly%20Detection%20in%0A%20%20Dynamically-Configured%20Systems&entry.906535625=Franz%20Kevin%20Stehle%20and%20Wainer%20Vandelli%20and%20Giuseppe%20Avolio%20and%20Felix%20Zahn%20and%20Holger%20Fr%C3%B6ning&entry.1292438233=%20%20Anomaly%20detection%20in%20distributed%20systems%20such%20as%20High-Performance%20Computing%0A%28HPC%29%20clusters%20is%20vital%20for%20early%20fault%20detection%2C%20performance%20optimisation%2C%0Asecurity%20monitoring%2C%20reliability%20in%20general%20but%20also%20operational%20insights.%20Deep%0ANeural%20Networks%20have%20seen%20successful%20use%20in%20detecting%20long-term%20anomalies%20in%0Amultidimensional%20data%2C%20originating%20for%20instance%20from%20industrial%20or%20medical%0Asystems%2C%20or%20weather%20prediction.%20A%20downside%20of%20such%20methods%20is%20that%20they%20require%0Aa%20static%20input%20size%2C%20or%20lose%20data%20through%20cropping%2C%20sampling%2C%20or%20other%0Adimensionality%20reduction%20methods%2C%20making%20deployment%20on%20systems%20with%20variability%0Aon%20monitored%20data%20channels%2C%20such%20as%20computing%20clusters%20difficult.%20To%20address%0Athese%20problems%2C%20we%20present%20DeepHYDRA%20%28Deep%20Hybrid%20DBSCAN/Reduction-Based%0AAnomaly%20Detection%29%20which%20combines%20DBSCAN%20and%20learning-based%20anomaly%20detection.%0ADBSCAN%20clustering%20is%20used%20to%20find%20point%20anomalies%20in%20time-series%20data%2C%0Amitigating%20the%20risk%20of%20missing%20outliers%20through%20loss%20of%20information%20when%0Areducing%20input%20data%20to%20a%20fixed%20number%20of%20channels.%20A%20deep%20learning-based%0Atime-series%20anomaly%20detection%20method%20is%20then%20applied%20to%20the%20reduced%20data%20in%0Aorder%20to%20identify%20long-term%20outliers.%20This%20hybrid%20approach%20reduces%20the%20chances%0Aof%20missing%20anomalies%20that%20might%20be%20made%20indistinguishable%20from%20normal%20data%20by%0Athe%20reduction%20process%2C%20and%20likewise%20enables%20the%20algorithm%20to%20be%20scalable%20and%0Atolerate%20partial%20system%20failures%20while%20retaining%20its%20detection%20capabilities.%0AUsing%20a%20subset%20of%20the%20well-known%20SMD%20dataset%20family%2C%20a%20modified%20variant%20of%20the%0AEclipse%20dataset%2C%20as%20well%20as%20an%20in-house%20dataset%20with%20a%20large%20variability%20in%0Aactive%20data%20channels%2C%20made%20publicly%20available%20with%20this%20work%2C%20we%20furthermore%0Aanalyse%20computational%20intensity%2C%20memory%20footprint%2C%20and%20activation%20counts.%0ADeepHYDRA%20is%20shown%20to%20reliably%20detect%20different%20types%20of%20anomalies%20in%20both%0Alarge%20and%20complex%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07749v1&entry.124074799=Read"},
{"title": "PLUTO: Pathology-Universal Transformer", "author": "Dinkar Juyal and Harshith Padigela and Chintan Shah and Daniel Shenker and Natalia Harguindeguy and Yi Liu and Blake Martin and Yibo Zhang and Michael Nercessian and Miles Markey and Isaac Finberg and Kelsey Luu and Daniel Borders and Syed Ashar Javed and Emma Krause and Raymond Biju and Aashish Sood and Allen Ma and Jackson Nyman and John Shamshoian and Guillaume Chhor and Darpan Sanghavi and Marc Thibault and Limin Yu and Fedaa Najdawi and Jennifer A. Hipp and Darren Fahy and Benjamin Glass and Eric Walk and John Abel and Harsha Pokkalla and Andrew H. Beck and Sean Grullon", "abstract": "  Pathology is the study of microscopic inspection of tissue, and a pathology\ndiagnosis is often the medical gold standard to diagnose disease. Pathology\nimages provide a unique challenge for computer-vision-based analysis: a single\npathology Whole Slide Image (WSI) is gigapixel-sized and often contains\nhundreds of thousands to millions of objects of interest across multiple\nresolutions. In this work, we propose PathoLogy Universal TransfOrmer (PLUTO):\na light-weight pathology FM that is pre-trained on a diverse dataset of 195\nmillion image tiles collected from multiple sites and extracts meaningful\nrepresentations across multiple WSI scales that enable a large variety of\ndownstream pathology tasks. In particular, we design task-specific adaptation\nheads that utilize PLUTO's output embeddings for tasks which span pathology\nscales ranging from subcellular to slide-scale, including instance\nsegmentation, tile classification, and slide-level prediction. We compare\nPLUTO's performance to other state-of-the-art methods on a diverse set of\nexternal and internal benchmarks covering multiple biologically relevant tasks,\ntissue types, resolutions, stains, and scanners. We find that PLUTO matches or\noutperforms existing task-specific baselines and pathology-specific foundation\nmodels, some of which use orders-of-magnitude larger datasets and model sizes\nwhen compared to PLUTO. Our findings present a path towards a universal\nembedding to power pathology image analysis, and motivate further exploration\naround pathology foundation models in terms of data diversity, architectural\nimprovements, sample efficiency, and practical deployability in real-world\napplications.\n", "link": "http://arxiv.org/abs/2405.07905v1", "date": "2024-05-13", "relevancy": 1.9567, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5044}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4875}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PLUTO%3A%20Pathology-Universal%20Transformer&body=Title%3A%20PLUTO%3A%20Pathology-Universal%20Transformer%0AAuthor%3A%20Dinkar%20Juyal%20and%20Harshith%20Padigela%20and%20Chintan%20Shah%20and%20Daniel%20Shenker%20and%20Natalia%20Harguindeguy%20and%20Yi%20Liu%20and%20Blake%20Martin%20and%20Yibo%20Zhang%20and%20Michael%20Nercessian%20and%20Miles%20Markey%20and%20Isaac%20Finberg%20and%20Kelsey%20Luu%20and%20Daniel%20Borders%20and%20Syed%20Ashar%20Javed%20and%20Emma%20Krause%20and%20Raymond%20Biju%20and%20Aashish%20Sood%20and%20Allen%20Ma%20and%20Jackson%20Nyman%20and%20John%20Shamshoian%20and%20Guillaume%20Chhor%20and%20Darpan%20Sanghavi%20and%20Marc%20Thibault%20and%20Limin%20Yu%20and%20Fedaa%20Najdawi%20and%20Jennifer%20A.%20Hipp%20and%20Darren%20Fahy%20and%20Benjamin%20Glass%20and%20Eric%20Walk%20and%20John%20Abel%20and%20Harsha%20Pokkalla%20and%20Andrew%20H.%20Beck%20and%20Sean%20Grullon%0AAbstract%3A%20%20%20Pathology%20is%20the%20study%20of%20microscopic%20inspection%20of%20tissue%2C%20and%20a%20pathology%0Adiagnosis%20is%20often%20the%20medical%20gold%20standard%20to%20diagnose%20disease.%20Pathology%0Aimages%20provide%20a%20unique%20challenge%20for%20computer-vision-based%20analysis%3A%20a%20single%0Apathology%20Whole%20Slide%20Image%20%28WSI%29%20is%20gigapixel-sized%20and%20often%20contains%0Ahundreds%20of%20thousands%20to%20millions%20of%20objects%20of%20interest%20across%20multiple%0Aresolutions.%20In%20this%20work%2C%20we%20propose%20PathoLogy%20Universal%20TransfOrmer%20%28PLUTO%29%3A%0Aa%20light-weight%20pathology%20FM%20that%20is%20pre-trained%20on%20a%20diverse%20dataset%20of%20195%0Amillion%20image%20tiles%20collected%20from%20multiple%20sites%20and%20extracts%20meaningful%0Arepresentations%20across%20multiple%20WSI%20scales%20that%20enable%20a%20large%20variety%20of%0Adownstream%20pathology%20tasks.%20In%20particular%2C%20we%20design%20task-specific%20adaptation%0Aheads%20that%20utilize%20PLUTO%27s%20output%20embeddings%20for%20tasks%20which%20span%20pathology%0Ascales%20ranging%20from%20subcellular%20to%20slide-scale%2C%20including%20instance%0Asegmentation%2C%20tile%20classification%2C%20and%20slide-level%20prediction.%20We%20compare%0APLUTO%27s%20performance%20to%20other%20state-of-the-art%20methods%20on%20a%20diverse%20set%20of%0Aexternal%20and%20internal%20benchmarks%20covering%20multiple%20biologically%20relevant%20tasks%2C%0Atissue%20types%2C%20resolutions%2C%20stains%2C%20and%20scanners.%20We%20find%20that%20PLUTO%20matches%20or%0Aoutperforms%20existing%20task-specific%20baselines%20and%20pathology-specific%20foundation%0Amodels%2C%20some%20of%20which%20use%20orders-of-magnitude%20larger%20datasets%20and%20model%20sizes%0Awhen%20compared%20to%20PLUTO.%20Our%20findings%20present%20a%20path%20towards%20a%20universal%0Aembedding%20to%20power%20pathology%20image%20analysis%2C%20and%20motivate%20further%20exploration%0Aaround%20pathology%20foundation%20models%20in%20terms%20of%20data%20diversity%2C%20architectural%0Aimprovements%2C%20sample%20efficiency%2C%20and%20practical%20deployability%20in%20real-world%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPLUTO%253A%2520Pathology-Universal%2520Transformer%26entry.906535625%3DDinkar%2520Juyal%2520and%2520Harshith%2520Padigela%2520and%2520Chintan%2520Shah%2520and%2520Daniel%2520Shenker%2520and%2520Natalia%2520Harguindeguy%2520and%2520Yi%2520Liu%2520and%2520Blake%2520Martin%2520and%2520Yibo%2520Zhang%2520and%2520Michael%2520Nercessian%2520and%2520Miles%2520Markey%2520and%2520Isaac%2520Finberg%2520and%2520Kelsey%2520Luu%2520and%2520Daniel%2520Borders%2520and%2520Syed%2520Ashar%2520Javed%2520and%2520Emma%2520Krause%2520and%2520Raymond%2520Biju%2520and%2520Aashish%2520Sood%2520and%2520Allen%2520Ma%2520and%2520Jackson%2520Nyman%2520and%2520John%2520Shamshoian%2520and%2520Guillaume%2520Chhor%2520and%2520Darpan%2520Sanghavi%2520and%2520Marc%2520Thibault%2520and%2520Limin%2520Yu%2520and%2520Fedaa%2520Najdawi%2520and%2520Jennifer%2520A.%2520Hipp%2520and%2520Darren%2520Fahy%2520and%2520Benjamin%2520Glass%2520and%2520Eric%2520Walk%2520and%2520John%2520Abel%2520and%2520Harsha%2520Pokkalla%2520and%2520Andrew%2520H.%2520Beck%2520and%2520Sean%2520Grullon%26entry.1292438233%3D%2520%2520Pathology%2520is%2520the%2520study%2520of%2520microscopic%2520inspection%2520of%2520tissue%252C%2520and%2520a%2520pathology%250Adiagnosis%2520is%2520often%2520the%2520medical%2520gold%2520standard%2520to%2520diagnose%2520disease.%2520Pathology%250Aimages%2520provide%2520a%2520unique%2520challenge%2520for%2520computer-vision-based%2520analysis%253A%2520a%2520single%250Apathology%2520Whole%2520Slide%2520Image%2520%2528WSI%2529%2520is%2520gigapixel-sized%2520and%2520often%2520contains%250Ahundreds%2520of%2520thousands%2520to%2520millions%2520of%2520objects%2520of%2520interest%2520across%2520multiple%250Aresolutions.%2520In%2520this%2520work%252C%2520we%2520propose%2520PathoLogy%2520Universal%2520TransfOrmer%2520%2528PLUTO%2529%253A%250Aa%2520light-weight%2520pathology%2520FM%2520that%2520is%2520pre-trained%2520on%2520a%2520diverse%2520dataset%2520of%2520195%250Amillion%2520image%2520tiles%2520collected%2520from%2520multiple%2520sites%2520and%2520extracts%2520meaningful%250Arepresentations%2520across%2520multiple%2520WSI%2520scales%2520that%2520enable%2520a%2520large%2520variety%2520of%250Adownstream%2520pathology%2520tasks.%2520In%2520particular%252C%2520we%2520design%2520task-specific%2520adaptation%250Aheads%2520that%2520utilize%2520PLUTO%2527s%2520output%2520embeddings%2520for%2520tasks%2520which%2520span%2520pathology%250Ascales%2520ranging%2520from%2520subcellular%2520to%2520slide-scale%252C%2520including%2520instance%250Asegmentation%252C%2520tile%2520classification%252C%2520and%2520slide-level%2520prediction.%2520We%2520compare%250APLUTO%2527s%2520performance%2520to%2520other%2520state-of-the-art%2520methods%2520on%2520a%2520diverse%2520set%2520of%250Aexternal%2520and%2520internal%2520benchmarks%2520covering%2520multiple%2520biologically%2520relevant%2520tasks%252C%250Atissue%2520types%252C%2520resolutions%252C%2520stains%252C%2520and%2520scanners.%2520We%2520find%2520that%2520PLUTO%2520matches%2520or%250Aoutperforms%2520existing%2520task-specific%2520baselines%2520and%2520pathology-specific%2520foundation%250Amodels%252C%2520some%2520of%2520which%2520use%2520orders-of-magnitude%2520larger%2520datasets%2520and%2520model%2520sizes%250Awhen%2520compared%2520to%2520PLUTO.%2520Our%2520findings%2520present%2520a%2520path%2520towards%2520a%2520universal%250Aembedding%2520to%2520power%2520pathology%2520image%2520analysis%252C%2520and%2520motivate%2520further%2520exploration%250Aaround%2520pathology%2520foundation%2520models%2520in%2520terms%2520of%2520data%2520diversity%252C%2520architectural%250Aimprovements%252C%2520sample%2520efficiency%252C%2520and%2520practical%2520deployability%2520in%2520real-world%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PLUTO%3A%20Pathology-Universal%20Transformer&entry.906535625=Dinkar%20Juyal%20and%20Harshith%20Padigela%20and%20Chintan%20Shah%20and%20Daniel%20Shenker%20and%20Natalia%20Harguindeguy%20and%20Yi%20Liu%20and%20Blake%20Martin%20and%20Yibo%20Zhang%20and%20Michael%20Nercessian%20and%20Miles%20Markey%20and%20Isaac%20Finberg%20and%20Kelsey%20Luu%20and%20Daniel%20Borders%20and%20Syed%20Ashar%20Javed%20and%20Emma%20Krause%20and%20Raymond%20Biju%20and%20Aashish%20Sood%20and%20Allen%20Ma%20and%20Jackson%20Nyman%20and%20John%20Shamshoian%20and%20Guillaume%20Chhor%20and%20Darpan%20Sanghavi%20and%20Marc%20Thibault%20and%20Limin%20Yu%20and%20Fedaa%20Najdawi%20and%20Jennifer%20A.%20Hipp%20and%20Darren%20Fahy%20and%20Benjamin%20Glass%20and%20Eric%20Walk%20and%20John%20Abel%20and%20Harsha%20Pokkalla%20and%20Andrew%20H.%20Beck%20and%20Sean%20Grullon&entry.1292438233=%20%20Pathology%20is%20the%20study%20of%20microscopic%20inspection%20of%20tissue%2C%20and%20a%20pathology%0Adiagnosis%20is%20often%20the%20medical%20gold%20standard%20to%20diagnose%20disease.%20Pathology%0Aimages%20provide%20a%20unique%20challenge%20for%20computer-vision-based%20analysis%3A%20a%20single%0Apathology%20Whole%20Slide%20Image%20%28WSI%29%20is%20gigapixel-sized%20and%20often%20contains%0Ahundreds%20of%20thousands%20to%20millions%20of%20objects%20of%20interest%20across%20multiple%0Aresolutions.%20In%20this%20work%2C%20we%20propose%20PathoLogy%20Universal%20TransfOrmer%20%28PLUTO%29%3A%0Aa%20light-weight%20pathology%20FM%20that%20is%20pre-trained%20on%20a%20diverse%20dataset%20of%20195%0Amillion%20image%20tiles%20collected%20from%20multiple%20sites%20and%20extracts%20meaningful%0Arepresentations%20across%20multiple%20WSI%20scales%20that%20enable%20a%20large%20variety%20of%0Adownstream%20pathology%20tasks.%20In%20particular%2C%20we%20design%20task-specific%20adaptation%0Aheads%20that%20utilize%20PLUTO%27s%20output%20embeddings%20for%20tasks%20which%20span%20pathology%0Ascales%20ranging%20from%20subcellular%20to%20slide-scale%2C%20including%20instance%0Asegmentation%2C%20tile%20classification%2C%20and%20slide-level%20prediction.%20We%20compare%0APLUTO%27s%20performance%20to%20other%20state-of-the-art%20methods%20on%20a%20diverse%20set%20of%0Aexternal%20and%20internal%20benchmarks%20covering%20multiple%20biologically%20relevant%20tasks%2C%0Atissue%20types%2C%20resolutions%2C%20stains%2C%20and%20scanners.%20We%20find%20that%20PLUTO%20matches%20or%0Aoutperforms%20existing%20task-specific%20baselines%20and%20pathology-specific%20foundation%0Amodels%2C%20some%20of%20which%20use%20orders-of-magnitude%20larger%20datasets%20and%20model%20sizes%0Awhen%20compared%20to%20PLUTO.%20Our%20findings%20present%20a%20path%20towards%20a%20universal%0Aembedding%20to%20power%20pathology%20image%20analysis%2C%20and%20motivate%20further%20exploration%0Aaround%20pathology%20foundation%20models%20in%20terms%20of%20data%20diversity%2C%20architectural%0Aimprovements%2C%20sample%20efficiency%2C%20and%20practical%20deployability%20in%20real-world%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07905v1&entry.124074799=Read"},
{"title": "Neural Network Compression for Reinforcement Learning Tasks", "author": "Dmitry A. Ivanov and Denis A. Larionov and Oleg V. Maslennikov and Vladimir V. Voevodin", "abstract": "  In real applications of Reinforcement Learning (RL), such as robotics, low\nlatency and energy efficient inference is very desired. The use of sparsity and\npruning for optimizing Neural Network inference, and particularly to improve\nenergy and latency efficiency, is a standard technique. In this work, we\nperform a systematic investigation of applying these optimization techniques\nfor different RL algorithms in different RL environments, yielding up to a\n400-fold reduction in the size of neural networks.\n", "link": "http://arxiv.org/abs/2405.07748v1", "date": "2024-05-13", "relevancy": 1.947, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.499}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4908}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Network%20Compression%20for%20Reinforcement%20Learning%20Tasks&body=Title%3A%20Neural%20Network%20Compression%20for%20Reinforcement%20Learning%20Tasks%0AAuthor%3A%20Dmitry%20A.%20Ivanov%20and%20Denis%20A.%20Larionov%20and%20Oleg%20V.%20Maslennikov%20and%20Vladimir%20V.%20Voevodin%0AAbstract%3A%20%20%20In%20real%20applications%20of%20Reinforcement%20Learning%20%28RL%29%2C%20such%20as%20robotics%2C%20low%0Alatency%20and%20energy%20efficient%20inference%20is%20very%20desired.%20The%20use%20of%20sparsity%20and%0Apruning%20for%20optimizing%20Neural%20Network%20inference%2C%20and%20particularly%20to%20improve%0Aenergy%20and%20latency%20efficiency%2C%20is%20a%20standard%20technique.%20In%20this%20work%2C%20we%0Aperform%20a%20systematic%20investigation%20of%20applying%20these%20optimization%20techniques%0Afor%20different%20RL%20algorithms%20in%20different%20RL%20environments%2C%20yielding%20up%20to%20a%0A400-fold%20reduction%20in%20the%20size%20of%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07748v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Network%2520Compression%2520for%2520Reinforcement%2520Learning%2520Tasks%26entry.906535625%3DDmitry%2520A.%2520Ivanov%2520and%2520Denis%2520A.%2520Larionov%2520and%2520Oleg%2520V.%2520Maslennikov%2520and%2520Vladimir%2520V.%2520Voevodin%26entry.1292438233%3D%2520%2520In%2520real%2520applications%2520of%2520Reinforcement%2520Learning%2520%2528RL%2529%252C%2520such%2520as%2520robotics%252C%2520low%250Alatency%2520and%2520energy%2520efficient%2520inference%2520is%2520very%2520desired.%2520The%2520use%2520of%2520sparsity%2520and%250Apruning%2520for%2520optimizing%2520Neural%2520Network%2520inference%252C%2520and%2520particularly%2520to%2520improve%250Aenergy%2520and%2520latency%2520efficiency%252C%2520is%2520a%2520standard%2520technique.%2520In%2520this%2520work%252C%2520we%250Aperform%2520a%2520systematic%2520investigation%2520of%2520applying%2520these%2520optimization%2520techniques%250Afor%2520different%2520RL%2520algorithms%2520in%2520different%2520RL%2520environments%252C%2520yielding%2520up%2520to%2520a%250A400-fold%2520reduction%2520in%2520the%2520size%2520of%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07748v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Network%20Compression%20for%20Reinforcement%20Learning%20Tasks&entry.906535625=Dmitry%20A.%20Ivanov%20and%20Denis%20A.%20Larionov%20and%20Oleg%20V.%20Maslennikov%20and%20Vladimir%20V.%20Voevodin&entry.1292438233=%20%20In%20real%20applications%20of%20Reinforcement%20Learning%20%28RL%29%2C%20such%20as%20robotics%2C%20low%0Alatency%20and%20energy%20efficient%20inference%20is%20very%20desired.%20The%20use%20of%20sparsity%20and%0Apruning%20for%20optimizing%20Neural%20Network%20inference%2C%20and%20particularly%20to%20improve%0Aenergy%20and%20latency%20efficiency%2C%20is%20a%20standard%20technique.%20In%20this%20work%2C%20we%0Aperform%20a%20systematic%20investigation%20of%20applying%20these%20optimization%20techniques%0Afor%20different%20RL%20algorithms%20in%20different%20RL%20environments%2C%20yielding%20up%20to%20a%0A400-fold%20reduction%20in%20the%20size%20of%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07748v1&entry.124074799=Read"},
{"title": "Squeezing Lemons with Hammers: An Evaluation of AutoML and Tabular Deep\n  Learning for Data-Scarce Classification Applications", "author": "Ricardo Knauer and Erik Rodner", "abstract": "  Many industry verticals are confronted with small-sized tabular data. In this\nlow-data regime, it is currently unclear whether the best performance can be\nexpected from simple baselines, or more complex machine learning approaches\nthat leverage meta-learning and ensembling. On 44 tabular classification\ndatasets with sample sizes $\\leq$ 500, we find that L2-regularized logistic\nregression performs similar to state-of-the-art automated machine learning\n(AutoML) frameworks (AutoPrognosis, AutoGluon) and off-the-shelf deep neural\nnetworks (TabPFN, HyperFast) on the majority of the benchmark datasets. We\ntherefore recommend to consider logistic regression as the first choice for\ndata-scarce applications with tabular data and provide practitioners with best\npractices for further method selection.\n", "link": "http://arxiv.org/abs/2405.07662v1", "date": "2024-05-13", "relevancy": 1.9469, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4943}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4835}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Squeezing%20Lemons%20with%20Hammers%3A%20An%20Evaluation%20of%20AutoML%20and%20Tabular%20Deep%0A%20%20Learning%20for%20Data-Scarce%20Classification%20Applications&body=Title%3A%20Squeezing%20Lemons%20with%20Hammers%3A%20An%20Evaluation%20of%20AutoML%20and%20Tabular%20Deep%0A%20%20Learning%20for%20Data-Scarce%20Classification%20Applications%0AAuthor%3A%20Ricardo%20Knauer%20and%20Erik%20Rodner%0AAbstract%3A%20%20%20Many%20industry%20verticals%20are%20confronted%20with%20small-sized%20tabular%20data.%20In%20this%0Alow-data%20regime%2C%20it%20is%20currently%20unclear%20whether%20the%20best%20performance%20can%20be%0Aexpected%20from%20simple%20baselines%2C%20or%20more%20complex%20machine%20learning%20approaches%0Athat%20leverage%20meta-learning%20and%20ensembling.%20On%2044%20tabular%20classification%0Adatasets%20with%20sample%20sizes%20%24%5Cleq%24%20500%2C%20we%20find%20that%20L2-regularized%20logistic%0Aregression%20performs%20similar%20to%20state-of-the-art%20automated%20machine%20learning%0A%28AutoML%29%20frameworks%20%28AutoPrognosis%2C%20AutoGluon%29%20and%20off-the-shelf%20deep%20neural%0Anetworks%20%28TabPFN%2C%20HyperFast%29%20on%20the%20majority%20of%20the%20benchmark%20datasets.%20We%0Atherefore%20recommend%20to%20consider%20logistic%20regression%20as%20the%20first%20choice%20for%0Adata-scarce%20applications%20with%20tabular%20data%20and%20provide%20practitioners%20with%20best%0Apractices%20for%20further%20method%20selection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07662v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSqueezing%2520Lemons%2520with%2520Hammers%253A%2520An%2520Evaluation%2520of%2520AutoML%2520and%2520Tabular%2520Deep%250A%2520%2520Learning%2520for%2520Data-Scarce%2520Classification%2520Applications%26entry.906535625%3DRicardo%2520Knauer%2520and%2520Erik%2520Rodner%26entry.1292438233%3D%2520%2520Many%2520industry%2520verticals%2520are%2520confronted%2520with%2520small-sized%2520tabular%2520data.%2520In%2520this%250Alow-data%2520regime%252C%2520it%2520is%2520currently%2520unclear%2520whether%2520the%2520best%2520performance%2520can%2520be%250Aexpected%2520from%2520simple%2520baselines%252C%2520or%2520more%2520complex%2520machine%2520learning%2520approaches%250Athat%2520leverage%2520meta-learning%2520and%2520ensembling.%2520On%252044%2520tabular%2520classification%250Adatasets%2520with%2520sample%2520sizes%2520%2524%255Cleq%2524%2520500%252C%2520we%2520find%2520that%2520L2-regularized%2520logistic%250Aregression%2520performs%2520similar%2520to%2520state-of-the-art%2520automated%2520machine%2520learning%250A%2528AutoML%2529%2520frameworks%2520%2528AutoPrognosis%252C%2520AutoGluon%2529%2520and%2520off-the-shelf%2520deep%2520neural%250Anetworks%2520%2528TabPFN%252C%2520HyperFast%2529%2520on%2520the%2520majority%2520of%2520the%2520benchmark%2520datasets.%2520We%250Atherefore%2520recommend%2520to%2520consider%2520logistic%2520regression%2520as%2520the%2520first%2520choice%2520for%250Adata-scarce%2520applications%2520with%2520tabular%2520data%2520and%2520provide%2520practitioners%2520with%2520best%250Apractices%2520for%2520further%2520method%2520selection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07662v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Squeezing%20Lemons%20with%20Hammers%3A%20An%20Evaluation%20of%20AutoML%20and%20Tabular%20Deep%0A%20%20Learning%20for%20Data-Scarce%20Classification%20Applications&entry.906535625=Ricardo%20Knauer%20and%20Erik%20Rodner&entry.1292438233=%20%20Many%20industry%20verticals%20are%20confronted%20with%20small-sized%20tabular%20data.%20In%20this%0Alow-data%20regime%2C%20it%20is%20currently%20unclear%20whether%20the%20best%20performance%20can%20be%0Aexpected%20from%20simple%20baselines%2C%20or%20more%20complex%20machine%20learning%20approaches%0Athat%20leverage%20meta-learning%20and%20ensembling.%20On%2044%20tabular%20classification%0Adatasets%20with%20sample%20sizes%20%24%5Cleq%24%20500%2C%20we%20find%20that%20L2-regularized%20logistic%0Aregression%20performs%20similar%20to%20state-of-the-art%20automated%20machine%20learning%0A%28AutoML%29%20frameworks%20%28AutoPrognosis%2C%20AutoGluon%29%20and%20off-the-shelf%20deep%20neural%0Anetworks%20%28TabPFN%2C%20HyperFast%29%20on%20the%20majority%20of%20the%20benchmark%20datasets.%20We%0Atherefore%20recommend%20to%20consider%20logistic%20regression%20as%20the%20first%20choice%20for%0Adata-scarce%20applications%20with%20tabular%20data%20and%20provide%20practitioners%20with%20best%0Apractices%20for%20further%20method%20selection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07662v1&entry.124074799=Read"},
{"title": "Sampling-Based Motion Planning with Online Racing Line Generation for\n  Autonomous Driving on Three-Dimensional Race Tracks", "author": "Levent \u00d6gretmen and Matthias Rowold and Alexander Langmann and Boris Lohmann", "abstract": "  Existing approaches to trajectory planning for autonomous racing employ\nsampling-based methods, generating numerous jerk-optimal trajectories and\nselecting the most favorable feasible trajectory based on a cost function\npenalizing deviations from an offline-calculated racing line. While successful\non oval tracks, these methods face limitations on complex circuits due to the\nsimplistic geometry of jerk-optimal edges failing to capture the complexity of\nthe racing line. Additionally, they only consider two-dimensional tracks,\npotentially neglecting or surpassing the actual dynamic potential. In this\npaper, we present a sampling-based local trajectory planning approach for\nautonomous racing that can maintain the lap time of the racing line even on\ncomplex race tracks and consider the race track's three-dimensional effects. In\nsimulative experiments, we demonstrate that our approach achieves lower lap\ntimes and improved utilization of dynamic limits compared to existing\napproaches. We also investigate the impact of online racing line generation, in\nwhich the time-optimal solution is planned from the current vehicle state for a\nlimited spatial horizon, in contrast to a closed racing line calculated\noffline. We show that combining the sampling-based planner with the online\nracing line generation can significantly reduce lap times in multi-vehicle\nscenarios.\n", "link": "http://arxiv.org/abs/2403.18643v3", "date": "2024-05-13", "relevancy": 1.9403, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5024}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4866}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sampling-Based%20Motion%20Planning%20with%20Online%20Racing%20Line%20Generation%20for%0A%20%20Autonomous%20Driving%20on%20Three-Dimensional%20Race%20Tracks&body=Title%3A%20Sampling-Based%20Motion%20Planning%20with%20Online%20Racing%20Line%20Generation%20for%0A%20%20Autonomous%20Driving%20on%20Three-Dimensional%20Race%20Tracks%0AAuthor%3A%20Levent%20%C3%96gretmen%20and%20Matthias%20Rowold%20and%20Alexander%20Langmann%20and%20Boris%20Lohmann%0AAbstract%3A%20%20%20Existing%20approaches%20to%20trajectory%20planning%20for%20autonomous%20racing%20employ%0Asampling-based%20methods%2C%20generating%20numerous%20jerk-optimal%20trajectories%20and%0Aselecting%20the%20most%20favorable%20feasible%20trajectory%20based%20on%20a%20cost%20function%0Apenalizing%20deviations%20from%20an%20offline-calculated%20racing%20line.%20While%20successful%0Aon%20oval%20tracks%2C%20these%20methods%20face%20limitations%20on%20complex%20circuits%20due%20to%20the%0Asimplistic%20geometry%20of%20jerk-optimal%20edges%20failing%20to%20capture%20the%20complexity%20of%0Athe%20racing%20line.%20Additionally%2C%20they%20only%20consider%20two-dimensional%20tracks%2C%0Apotentially%20neglecting%20or%20surpassing%20the%20actual%20dynamic%20potential.%20In%20this%0Apaper%2C%20we%20present%20a%20sampling-based%20local%20trajectory%20planning%20approach%20for%0Aautonomous%20racing%20that%20can%20maintain%20the%20lap%20time%20of%20the%20racing%20line%20even%20on%0Acomplex%20race%20tracks%20and%20consider%20the%20race%20track%27s%20three-dimensional%20effects.%20In%0Asimulative%20experiments%2C%20we%20demonstrate%20that%20our%20approach%20achieves%20lower%20lap%0Atimes%20and%20improved%20utilization%20of%20dynamic%20limits%20compared%20to%20existing%0Aapproaches.%20We%20also%20investigate%20the%20impact%20of%20online%20racing%20line%20generation%2C%20in%0Awhich%20the%20time-optimal%20solution%20is%20planned%20from%20the%20current%20vehicle%20state%20for%20a%0Alimited%20spatial%20horizon%2C%20in%20contrast%20to%20a%20closed%20racing%20line%20calculated%0Aoffline.%20We%20show%20that%20combining%20the%20sampling-based%20planner%20with%20the%20online%0Aracing%20line%20generation%20can%20significantly%20reduce%20lap%20times%20in%20multi-vehicle%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18643v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSampling-Based%2520Motion%2520Planning%2520with%2520Online%2520Racing%2520Line%2520Generation%2520for%250A%2520%2520Autonomous%2520Driving%2520on%2520Three-Dimensional%2520Race%2520Tracks%26entry.906535625%3DLevent%2520%25C3%2596gretmen%2520and%2520Matthias%2520Rowold%2520and%2520Alexander%2520Langmann%2520and%2520Boris%2520Lohmann%26entry.1292438233%3D%2520%2520Existing%2520approaches%2520to%2520trajectory%2520planning%2520for%2520autonomous%2520racing%2520employ%250Asampling-based%2520methods%252C%2520generating%2520numerous%2520jerk-optimal%2520trajectories%2520and%250Aselecting%2520the%2520most%2520favorable%2520feasible%2520trajectory%2520based%2520on%2520a%2520cost%2520function%250Apenalizing%2520deviations%2520from%2520an%2520offline-calculated%2520racing%2520line.%2520While%2520successful%250Aon%2520oval%2520tracks%252C%2520these%2520methods%2520face%2520limitations%2520on%2520complex%2520circuits%2520due%2520to%2520the%250Asimplistic%2520geometry%2520of%2520jerk-optimal%2520edges%2520failing%2520to%2520capture%2520the%2520complexity%2520of%250Athe%2520racing%2520line.%2520Additionally%252C%2520they%2520only%2520consider%2520two-dimensional%2520tracks%252C%250Apotentially%2520neglecting%2520or%2520surpassing%2520the%2520actual%2520dynamic%2520potential.%2520In%2520this%250Apaper%252C%2520we%2520present%2520a%2520sampling-based%2520local%2520trajectory%2520planning%2520approach%2520for%250Aautonomous%2520racing%2520that%2520can%2520maintain%2520the%2520lap%2520time%2520of%2520the%2520racing%2520line%2520even%2520on%250Acomplex%2520race%2520tracks%2520and%2520consider%2520the%2520race%2520track%2527s%2520three-dimensional%2520effects.%2520In%250Asimulative%2520experiments%252C%2520we%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520lower%2520lap%250Atimes%2520and%2520improved%2520utilization%2520of%2520dynamic%2520limits%2520compared%2520to%2520existing%250Aapproaches.%2520We%2520also%2520investigate%2520the%2520impact%2520of%2520online%2520racing%2520line%2520generation%252C%2520in%250Awhich%2520the%2520time-optimal%2520solution%2520is%2520planned%2520from%2520the%2520current%2520vehicle%2520state%2520for%2520a%250Alimited%2520spatial%2520horizon%252C%2520in%2520contrast%2520to%2520a%2520closed%2520racing%2520line%2520calculated%250Aoffline.%2520We%2520show%2520that%2520combining%2520the%2520sampling-based%2520planner%2520with%2520the%2520online%250Aracing%2520line%2520generation%2520can%2520significantly%2520reduce%2520lap%2520times%2520in%2520multi-vehicle%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.18643v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sampling-Based%20Motion%20Planning%20with%20Online%20Racing%20Line%20Generation%20for%0A%20%20Autonomous%20Driving%20on%20Three-Dimensional%20Race%20Tracks&entry.906535625=Levent%20%C3%96gretmen%20and%20Matthias%20Rowold%20and%20Alexander%20Langmann%20and%20Boris%20Lohmann&entry.1292438233=%20%20Existing%20approaches%20to%20trajectory%20planning%20for%20autonomous%20racing%20employ%0Asampling-based%20methods%2C%20generating%20numerous%20jerk-optimal%20trajectories%20and%0Aselecting%20the%20most%20favorable%20feasible%20trajectory%20based%20on%20a%20cost%20function%0Apenalizing%20deviations%20from%20an%20offline-calculated%20racing%20line.%20While%20successful%0Aon%20oval%20tracks%2C%20these%20methods%20face%20limitations%20on%20complex%20circuits%20due%20to%20the%0Asimplistic%20geometry%20of%20jerk-optimal%20edges%20failing%20to%20capture%20the%20complexity%20of%0Athe%20racing%20line.%20Additionally%2C%20they%20only%20consider%20two-dimensional%20tracks%2C%0Apotentially%20neglecting%20or%20surpassing%20the%20actual%20dynamic%20potential.%20In%20this%0Apaper%2C%20we%20present%20a%20sampling-based%20local%20trajectory%20planning%20approach%20for%0Aautonomous%20racing%20that%20can%20maintain%20the%20lap%20time%20of%20the%20racing%20line%20even%20on%0Acomplex%20race%20tracks%20and%20consider%20the%20race%20track%27s%20three-dimensional%20effects.%20In%0Asimulative%20experiments%2C%20we%20demonstrate%20that%20our%20approach%20achieves%20lower%20lap%0Atimes%20and%20improved%20utilization%20of%20dynamic%20limits%20compared%20to%20existing%0Aapproaches.%20We%20also%20investigate%20the%20impact%20of%20online%20racing%20line%20generation%2C%20in%0Awhich%20the%20time-optimal%20solution%20is%20planned%20from%20the%20current%20vehicle%20state%20for%20a%0Alimited%20spatial%20horizon%2C%20in%20contrast%20to%20a%20closed%20racing%20line%20calculated%0Aoffline.%20We%20show%20that%20combining%20the%20sampling-based%20planner%20with%20the%20online%0Aracing%20line%20generation%20can%20significantly%20reduce%20lap%20times%20in%20multi-vehicle%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18643v3&entry.124074799=Read"},
{"title": "Approximation properties relative to continuous scale space for hybrid\n  discretizations of Gaussian derivative operators", "author": "Tony Lindeberg", "abstract": "  This paper presents an analysis of properties of two hybrid discretization\nmethods for Gaussian derivatives, based on convolutions with either the\nnormalized sampled Gaussian kernel or the integrated Gaussian kernel followed\nby central differences. The motivation for studying these discretization\nmethods is that in situations when multiple spatial derivatives of different\norder are needed at the same scale level, they can be computed significantly\nmore efficiently compared to more direct derivative approximations based on\nexplicit convolutions with either sampled Gaussian kernels or integrated\nGaussian kernels.\n  While these computational benefits do also hold for the genuinely discrete\napproach for computing discrete analogues of Gaussian derivatives, based on\nconvolution with the discrete analogue of the Gaussian kernel followed by\ncentral differences, the underlying mathematical primitives for the discrete\nanalogue of the Gaussian kernel, in terms of modified Bessel functions of\ninteger order, may not be available in certain frameworks for image processing,\nsuch as when performing deep learning based on scale-parameterized filters in\nterms of Gaussian derivatives, with learning of the scale levels.\n  In this paper, we present a characterization of the properties of these\nhybrid discretization methods, in terms of quantitative performance measures\nconcerning the amount of spatial smoothing that they imply, as well as the\nrelative consistency of scale estimates obtained from scale-invariant feature\ndetectors with automatic scale selection, with an emphasis on the behaviour for\nvery small values of the scale parameter, which may differ significantly from\ncorresponding results obtained from the fully continuous scale-space theory, as\nwell as between different types of discretization methods.\n", "link": "http://arxiv.org/abs/2405.05095v2", "date": "2024-05-13", "relevancy": 1.9377, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4966}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4833}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Approximation%20properties%20relative%20to%20continuous%20scale%20space%20for%20hybrid%0A%20%20discretizations%20of%20Gaussian%20derivative%20operators&body=Title%3A%20Approximation%20properties%20relative%20to%20continuous%20scale%20space%20for%20hybrid%0A%20%20discretizations%20of%20Gaussian%20derivative%20operators%0AAuthor%3A%20Tony%20Lindeberg%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20analysis%20of%20properties%20of%20two%20hybrid%20discretization%0Amethods%20for%20Gaussian%20derivatives%2C%20based%20on%20convolutions%20with%20either%20the%0Anormalized%20sampled%20Gaussian%20kernel%20or%20the%20integrated%20Gaussian%20kernel%20followed%0Aby%20central%20differences.%20The%20motivation%20for%20studying%20these%20discretization%0Amethods%20is%20that%20in%20situations%20when%20multiple%20spatial%20derivatives%20of%20different%0Aorder%20are%20needed%20at%20the%20same%20scale%20level%2C%20they%20can%20be%20computed%20significantly%0Amore%20efficiently%20compared%20to%20more%20direct%20derivative%20approximations%20based%20on%0Aexplicit%20convolutions%20with%20either%20sampled%20Gaussian%20kernels%20or%20integrated%0AGaussian%20kernels.%0A%20%20While%20these%20computational%20benefits%20do%20also%20hold%20for%20the%20genuinely%20discrete%0Aapproach%20for%20computing%20discrete%20analogues%20of%20Gaussian%20derivatives%2C%20based%20on%0Aconvolution%20with%20the%20discrete%20analogue%20of%20the%20Gaussian%20kernel%20followed%20by%0Acentral%20differences%2C%20the%20underlying%20mathematical%20primitives%20for%20the%20discrete%0Aanalogue%20of%20the%20Gaussian%20kernel%2C%20in%20terms%20of%20modified%20Bessel%20functions%20of%0Ainteger%20order%2C%20may%20not%20be%20available%20in%20certain%20frameworks%20for%20image%20processing%2C%0Asuch%20as%20when%20performing%20deep%20learning%20based%20on%20scale-parameterized%20filters%20in%0Aterms%20of%20Gaussian%20derivatives%2C%20with%20learning%20of%20the%20scale%20levels.%0A%20%20In%20this%20paper%2C%20we%20present%20a%20characterization%20of%20the%20properties%20of%20these%0Ahybrid%20discretization%20methods%2C%20in%20terms%20of%20quantitative%20performance%20measures%0Aconcerning%20the%20amount%20of%20spatial%20smoothing%20that%20they%20imply%2C%20as%20well%20as%20the%0Arelative%20consistency%20of%20scale%20estimates%20obtained%20from%20scale-invariant%20feature%0Adetectors%20with%20automatic%20scale%20selection%2C%20with%20an%20emphasis%20on%20the%20behaviour%20for%0Avery%20small%20values%20of%20the%20scale%20parameter%2C%20which%20may%20differ%20significantly%20from%0Acorresponding%20results%20obtained%20from%20the%20fully%20continuous%20scale-space%20theory%2C%20as%0Awell%20as%20between%20different%20types%20of%20discretization%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05095v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApproximation%2520properties%2520relative%2520to%2520continuous%2520scale%2520space%2520for%2520hybrid%250A%2520%2520discretizations%2520of%2520Gaussian%2520derivative%2520operators%26entry.906535625%3DTony%2520Lindeberg%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520analysis%2520of%2520properties%2520of%2520two%2520hybrid%2520discretization%250Amethods%2520for%2520Gaussian%2520derivatives%252C%2520based%2520on%2520convolutions%2520with%2520either%2520the%250Anormalized%2520sampled%2520Gaussian%2520kernel%2520or%2520the%2520integrated%2520Gaussian%2520kernel%2520followed%250Aby%2520central%2520differences.%2520The%2520motivation%2520for%2520studying%2520these%2520discretization%250Amethods%2520is%2520that%2520in%2520situations%2520when%2520multiple%2520spatial%2520derivatives%2520of%2520different%250Aorder%2520are%2520needed%2520at%2520the%2520same%2520scale%2520level%252C%2520they%2520can%2520be%2520computed%2520significantly%250Amore%2520efficiently%2520compared%2520to%2520more%2520direct%2520derivative%2520approximations%2520based%2520on%250Aexplicit%2520convolutions%2520with%2520either%2520sampled%2520Gaussian%2520kernels%2520or%2520integrated%250AGaussian%2520kernels.%250A%2520%2520While%2520these%2520computational%2520benefits%2520do%2520also%2520hold%2520for%2520the%2520genuinely%2520discrete%250Aapproach%2520for%2520computing%2520discrete%2520analogues%2520of%2520Gaussian%2520derivatives%252C%2520based%2520on%250Aconvolution%2520with%2520the%2520discrete%2520analogue%2520of%2520the%2520Gaussian%2520kernel%2520followed%2520by%250Acentral%2520differences%252C%2520the%2520underlying%2520mathematical%2520primitives%2520for%2520the%2520discrete%250Aanalogue%2520of%2520the%2520Gaussian%2520kernel%252C%2520in%2520terms%2520of%2520modified%2520Bessel%2520functions%2520of%250Ainteger%2520order%252C%2520may%2520not%2520be%2520available%2520in%2520certain%2520frameworks%2520for%2520image%2520processing%252C%250Asuch%2520as%2520when%2520performing%2520deep%2520learning%2520based%2520on%2520scale-parameterized%2520filters%2520in%250Aterms%2520of%2520Gaussian%2520derivatives%252C%2520with%2520learning%2520of%2520the%2520scale%2520levels.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520characterization%2520of%2520the%2520properties%2520of%2520these%250Ahybrid%2520discretization%2520methods%252C%2520in%2520terms%2520of%2520quantitative%2520performance%2520measures%250Aconcerning%2520the%2520amount%2520of%2520spatial%2520smoothing%2520that%2520they%2520imply%252C%2520as%2520well%2520as%2520the%250Arelative%2520consistency%2520of%2520scale%2520estimates%2520obtained%2520from%2520scale-invariant%2520feature%250Adetectors%2520with%2520automatic%2520scale%2520selection%252C%2520with%2520an%2520emphasis%2520on%2520the%2520behaviour%2520for%250Avery%2520small%2520values%2520of%2520the%2520scale%2520parameter%252C%2520which%2520may%2520differ%2520significantly%2520from%250Acorresponding%2520results%2520obtained%2520from%2520the%2520fully%2520continuous%2520scale-space%2520theory%252C%2520as%250Awell%2520as%2520between%2520different%2520types%2520of%2520discretization%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05095v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Approximation%20properties%20relative%20to%20continuous%20scale%20space%20for%20hybrid%0A%20%20discretizations%20of%20Gaussian%20derivative%20operators&entry.906535625=Tony%20Lindeberg&entry.1292438233=%20%20This%20paper%20presents%20an%20analysis%20of%20properties%20of%20two%20hybrid%20discretization%0Amethods%20for%20Gaussian%20derivatives%2C%20based%20on%20convolutions%20with%20either%20the%0Anormalized%20sampled%20Gaussian%20kernel%20or%20the%20integrated%20Gaussian%20kernel%20followed%0Aby%20central%20differences.%20The%20motivation%20for%20studying%20these%20discretization%0Amethods%20is%20that%20in%20situations%20when%20multiple%20spatial%20derivatives%20of%20different%0Aorder%20are%20needed%20at%20the%20same%20scale%20level%2C%20they%20can%20be%20computed%20significantly%0Amore%20efficiently%20compared%20to%20more%20direct%20derivative%20approximations%20based%20on%0Aexplicit%20convolutions%20with%20either%20sampled%20Gaussian%20kernels%20or%20integrated%0AGaussian%20kernels.%0A%20%20While%20these%20computational%20benefits%20do%20also%20hold%20for%20the%20genuinely%20discrete%0Aapproach%20for%20computing%20discrete%20analogues%20of%20Gaussian%20derivatives%2C%20based%20on%0Aconvolution%20with%20the%20discrete%20analogue%20of%20the%20Gaussian%20kernel%20followed%20by%0Acentral%20differences%2C%20the%20underlying%20mathematical%20primitives%20for%20the%20discrete%0Aanalogue%20of%20the%20Gaussian%20kernel%2C%20in%20terms%20of%20modified%20Bessel%20functions%20of%0Ainteger%20order%2C%20may%20not%20be%20available%20in%20certain%20frameworks%20for%20image%20processing%2C%0Asuch%20as%20when%20performing%20deep%20learning%20based%20on%20scale-parameterized%20filters%20in%0Aterms%20of%20Gaussian%20derivatives%2C%20with%20learning%20of%20the%20scale%20levels.%0A%20%20In%20this%20paper%2C%20we%20present%20a%20characterization%20of%20the%20properties%20of%20these%0Ahybrid%20discretization%20methods%2C%20in%20terms%20of%20quantitative%20performance%20measures%0Aconcerning%20the%20amount%20of%20spatial%20smoothing%20that%20they%20imply%2C%20as%20well%20as%20the%0Arelative%20consistency%20of%20scale%20estimates%20obtained%20from%20scale-invariant%20feature%0Adetectors%20with%20automatic%20scale%20selection%2C%20with%20an%20emphasis%20on%20the%20behaviour%20for%0Avery%20small%20values%20of%20the%20scale%20parameter%2C%20which%20may%20differ%20significantly%20from%0Acorresponding%20results%20obtained%20from%20the%20fully%20continuous%20scale-space%20theory%2C%20as%0Awell%20as%20between%20different%20types%20of%20discretization%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05095v2&entry.124074799=Read"},
{"title": "OMPGPT: A Generative Pre-trained Transformer Model for OpenMP", "author": "Le Chen and Arijit Bhattacharjee and Nesreen Ahmed and Niranjan Hasabnis and Gal Oren and Vy Vo and Ali Jannesari", "abstract": "  Large language models (LLMs)such as ChatGPT have significantly advanced the\nfield of Natural Language Processing (NLP). This trend led to the development\nof code-based large language models such as StarCoder, WizardCoder, and\nCodeLlama, which are trained extensively on vast repositories of code and\nprogramming languages. While the generic abilities of these code LLMs are\nuseful for many programmers in tasks like code generation, the area of\nhigh-performance computing (HPC) has a narrower set of requirements that make a\nsmaller and more domain-specific model a smarter choice. This paper presents\nOMPGPT, a novel domain-specific model meticulously designed to harness the\ninherent strengths of language models for OpenMP pragma generation.\nFurthermore, we leverage prompt engineering techniques from the NLP domain to\ncreate Chain-of-OMP, an innovative strategy designed to enhance OMPGPT's\neffectiveness. Our extensive evaluations demonstrate that OMPGPT outperforms\nexisting large language models specialized in OpenMP tasks and maintains a\nnotably smaller size, aligning it more closely with the typical hardware\nconstraints of HPC environments. We consider our contribution as a pivotal\nbridge, connecting the advantage of language models with the specific demands\nof HPC tasks.\n", "link": "http://arxiv.org/abs/2401.16445v2", "date": "2024-05-13", "relevancy": 1.9338, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4897}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4884}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OMPGPT%3A%20A%20Generative%20Pre-trained%20Transformer%20Model%20for%20OpenMP&body=Title%3A%20OMPGPT%3A%20A%20Generative%20Pre-trained%20Transformer%20Model%20for%20OpenMP%0AAuthor%3A%20Le%20Chen%20and%20Arijit%20Bhattacharjee%20and%20Nesreen%20Ahmed%20and%20Niranjan%20Hasabnis%20and%20Gal%20Oren%20and%20Vy%20Vo%20and%20Ali%20Jannesari%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29such%20as%20ChatGPT%20have%20significantly%20advanced%20the%0Afield%20of%20Natural%20Language%20Processing%20%28NLP%29.%20This%20trend%20led%20to%20the%20development%0Aof%20code-based%20large%20language%20models%20such%20as%20StarCoder%2C%20WizardCoder%2C%20and%0ACodeLlama%2C%20which%20are%20trained%20extensively%20on%20vast%20repositories%20of%20code%20and%0Aprogramming%20languages.%20While%20the%20generic%20abilities%20of%20these%20code%20LLMs%20are%0Auseful%20for%20many%20programmers%20in%20tasks%20like%20code%20generation%2C%20the%20area%20of%0Ahigh-performance%20computing%20%28HPC%29%20has%20a%20narrower%20set%20of%20requirements%20that%20make%20a%0Asmaller%20and%20more%20domain-specific%20model%20a%20smarter%20choice.%20This%20paper%20presents%0AOMPGPT%2C%20a%20novel%20domain-specific%20model%20meticulously%20designed%20to%20harness%20the%0Ainherent%20strengths%20of%20language%20models%20for%20OpenMP%20pragma%20generation.%0AFurthermore%2C%20we%20leverage%20prompt%20engineering%20techniques%20from%20the%20NLP%20domain%20to%0Acreate%20Chain-of-OMP%2C%20an%20innovative%20strategy%20designed%20to%20enhance%20OMPGPT%27s%0Aeffectiveness.%20Our%20extensive%20evaluations%20demonstrate%20that%20OMPGPT%20outperforms%0Aexisting%20large%20language%20models%20specialized%20in%20OpenMP%20tasks%20and%20maintains%20a%0Anotably%20smaller%20size%2C%20aligning%20it%20more%20closely%20with%20the%20typical%20hardware%0Aconstraints%20of%20HPC%20environments.%20We%20consider%20our%20contribution%20as%20a%20pivotal%0Abridge%2C%20connecting%20the%20advantage%20of%20language%20models%20with%20the%20specific%20demands%0Aof%20HPC%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.16445v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOMPGPT%253A%2520A%2520Generative%2520Pre-trained%2520Transformer%2520Model%2520for%2520OpenMP%26entry.906535625%3DLe%2520Chen%2520and%2520Arijit%2520Bhattacharjee%2520and%2520Nesreen%2520Ahmed%2520and%2520Niranjan%2520Hasabnis%2520and%2520Gal%2520Oren%2520and%2520Vy%2520Vo%2520and%2520Ali%2520Jannesari%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529such%2520as%2520ChatGPT%2520have%2520significantly%2520advanced%2520the%250Afield%2520of%2520Natural%2520Language%2520Processing%2520%2528NLP%2529.%2520This%2520trend%2520led%2520to%2520the%2520development%250Aof%2520code-based%2520large%2520language%2520models%2520such%2520as%2520StarCoder%252C%2520WizardCoder%252C%2520and%250ACodeLlama%252C%2520which%2520are%2520trained%2520extensively%2520on%2520vast%2520repositories%2520of%2520code%2520and%250Aprogramming%2520languages.%2520While%2520the%2520generic%2520abilities%2520of%2520these%2520code%2520LLMs%2520are%250Auseful%2520for%2520many%2520programmers%2520in%2520tasks%2520like%2520code%2520generation%252C%2520the%2520area%2520of%250Ahigh-performance%2520computing%2520%2528HPC%2529%2520has%2520a%2520narrower%2520set%2520of%2520requirements%2520that%2520make%2520a%250Asmaller%2520and%2520more%2520domain-specific%2520model%2520a%2520smarter%2520choice.%2520This%2520paper%2520presents%250AOMPGPT%252C%2520a%2520novel%2520domain-specific%2520model%2520meticulously%2520designed%2520to%2520harness%2520the%250Ainherent%2520strengths%2520of%2520language%2520models%2520for%2520OpenMP%2520pragma%2520generation.%250AFurthermore%252C%2520we%2520leverage%2520prompt%2520engineering%2520techniques%2520from%2520the%2520NLP%2520domain%2520to%250Acreate%2520Chain-of-OMP%252C%2520an%2520innovative%2520strategy%2520designed%2520to%2520enhance%2520OMPGPT%2527s%250Aeffectiveness.%2520Our%2520extensive%2520evaluations%2520demonstrate%2520that%2520OMPGPT%2520outperforms%250Aexisting%2520large%2520language%2520models%2520specialized%2520in%2520OpenMP%2520tasks%2520and%2520maintains%2520a%250Anotably%2520smaller%2520size%252C%2520aligning%2520it%2520more%2520closely%2520with%2520the%2520typical%2520hardware%250Aconstraints%2520of%2520HPC%2520environments.%2520We%2520consider%2520our%2520contribution%2520as%2520a%2520pivotal%250Abridge%252C%2520connecting%2520the%2520advantage%2520of%2520language%2520models%2520with%2520the%2520specific%2520demands%250Aof%2520HPC%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.16445v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OMPGPT%3A%20A%20Generative%20Pre-trained%20Transformer%20Model%20for%20OpenMP&entry.906535625=Le%20Chen%20and%20Arijit%20Bhattacharjee%20and%20Nesreen%20Ahmed%20and%20Niranjan%20Hasabnis%20and%20Gal%20Oren%20and%20Vy%20Vo%20and%20Ali%20Jannesari&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29such%20as%20ChatGPT%20have%20significantly%20advanced%20the%0Afield%20of%20Natural%20Language%20Processing%20%28NLP%29.%20This%20trend%20led%20to%20the%20development%0Aof%20code-based%20large%20language%20models%20such%20as%20StarCoder%2C%20WizardCoder%2C%20and%0ACodeLlama%2C%20which%20are%20trained%20extensively%20on%20vast%20repositories%20of%20code%20and%0Aprogramming%20languages.%20While%20the%20generic%20abilities%20of%20these%20code%20LLMs%20are%0Auseful%20for%20many%20programmers%20in%20tasks%20like%20code%20generation%2C%20the%20area%20of%0Ahigh-performance%20computing%20%28HPC%29%20has%20a%20narrower%20set%20of%20requirements%20that%20make%20a%0Asmaller%20and%20more%20domain-specific%20model%20a%20smarter%20choice.%20This%20paper%20presents%0AOMPGPT%2C%20a%20novel%20domain-specific%20model%20meticulously%20designed%20to%20harness%20the%0Ainherent%20strengths%20of%20language%20models%20for%20OpenMP%20pragma%20generation.%0AFurthermore%2C%20we%20leverage%20prompt%20engineering%20techniques%20from%20the%20NLP%20domain%20to%0Acreate%20Chain-of-OMP%2C%20an%20innovative%20strategy%20designed%20to%20enhance%20OMPGPT%27s%0Aeffectiveness.%20Our%20extensive%20evaluations%20demonstrate%20that%20OMPGPT%20outperforms%0Aexisting%20large%20language%20models%20specialized%20in%20OpenMP%20tasks%20and%20maintains%20a%0Anotably%20smaller%20size%2C%20aligning%20it%20more%20closely%20with%20the%20typical%20hardware%0Aconstraints%20of%20HPC%20environments.%20We%20consider%20our%20contribution%20as%20a%20pivotal%0Abridge%2C%20connecting%20the%20advantage%20of%20language%20models%20with%20the%20specific%20demands%0Aof%20HPC%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16445v2&entry.124074799=Read"},
{"title": "Bayesian Optimization with Formal Safety Guarantees via Online Conformal\n  Prediction", "author": "Yunchuan Zhang and Sangwoo Park and Osvaldo Simeone", "abstract": "  Black-box zero-th order optimization is a central primitive for applications\nin fields as diverse as finance, physics, and engineering. In a common\nformulation of this problem, a designer sequentially attempts candidate\nsolutions, receiving noisy feedback on the value of each attempt from the\nsystem. In this paper, we study scenarios in which feedback is also provided on\nthe safety of the attempted solution, and the optimizer is constrained to limit\nthe number of unsafe solutions that are tried throughout the optimization\nprocess. Focusing on methods based on Bayesian optimization (BO), prior art has\nintroduced an optimization scheme -- referred to as SAFEOPT -- that is\nguaranteed not to select any unsafe solution with a controllable probability\nover feedback noise as long as strict assumptions on the safety constraint\nfunction are met. In this paper, a novel BO-based approach is introduced that\nsatisfies safety requirements irrespective of properties of the constraint\nfunction. This strong theoretical guarantee is obtained at the cost of allowing\nfor an arbitrary, controllable but non-zero, rate of violation of the safety\nconstraint. The proposed method, referred to as SAFE-BOCP, builds on online\nconformal prediction (CP) and is specialized to the cases in which feedback on\nthe safety constraint is either noiseless or noisy. Experimental results on\nsynthetic and real-world data validate the advantages and flexibility of the\nproposed SAFE-BOCP.\n", "link": "http://arxiv.org/abs/2306.17815v2", "date": "2024-05-13", "relevancy": 1.9292, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.563}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4821}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Optimization%20with%20Formal%20Safety%20Guarantees%20via%20Online%20Conformal%0A%20%20Prediction&body=Title%3A%20Bayesian%20Optimization%20with%20Formal%20Safety%20Guarantees%20via%20Online%20Conformal%0A%20%20Prediction%0AAuthor%3A%20Yunchuan%20Zhang%20and%20Sangwoo%20Park%20and%20Osvaldo%20Simeone%0AAbstract%3A%20%20%20Black-box%20zero-th%20order%20optimization%20is%20a%20central%20primitive%20for%20applications%0Ain%20fields%20as%20diverse%20as%20finance%2C%20physics%2C%20and%20engineering.%20In%20a%20common%0Aformulation%20of%20this%20problem%2C%20a%20designer%20sequentially%20attempts%20candidate%0Asolutions%2C%20receiving%20noisy%20feedback%20on%20the%20value%20of%20each%20attempt%20from%20the%0Asystem.%20In%20this%20paper%2C%20we%20study%20scenarios%20in%20which%20feedback%20is%20also%20provided%20on%0Athe%20safety%20of%20the%20attempted%20solution%2C%20and%20the%20optimizer%20is%20constrained%20to%20limit%0Athe%20number%20of%20unsafe%20solutions%20that%20are%20tried%20throughout%20the%20optimization%0Aprocess.%20Focusing%20on%20methods%20based%20on%20Bayesian%20optimization%20%28BO%29%2C%20prior%20art%20has%0Aintroduced%20an%20optimization%20scheme%20--%20referred%20to%20as%20SAFEOPT%20--%20that%20is%0Aguaranteed%20not%20to%20select%20any%20unsafe%20solution%20with%20a%20controllable%20probability%0Aover%20feedback%20noise%20as%20long%20as%20strict%20assumptions%20on%20the%20safety%20constraint%0Afunction%20are%20met.%20In%20this%20paper%2C%20a%20novel%20BO-based%20approach%20is%20introduced%20that%0Asatisfies%20safety%20requirements%20irrespective%20of%20properties%20of%20the%20constraint%0Afunction.%20This%20strong%20theoretical%20guarantee%20is%20obtained%20at%20the%20cost%20of%20allowing%0Afor%20an%20arbitrary%2C%20controllable%20but%20non-zero%2C%20rate%20of%20violation%20of%20the%20safety%0Aconstraint.%20The%20proposed%20method%2C%20referred%20to%20as%20SAFE-BOCP%2C%20builds%20on%20online%0Aconformal%20prediction%20%28CP%29%20and%20is%20specialized%20to%20the%20cases%20in%20which%20feedback%20on%0Athe%20safety%20constraint%20is%20either%20noiseless%20or%20noisy.%20Experimental%20results%20on%0Asynthetic%20and%20real-world%20data%20validate%20the%20advantages%20and%20flexibility%20of%20the%0Aproposed%20SAFE-BOCP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.17815v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Optimization%2520with%2520Formal%2520Safety%2520Guarantees%2520via%2520Online%2520Conformal%250A%2520%2520Prediction%26entry.906535625%3DYunchuan%2520Zhang%2520and%2520Sangwoo%2520Park%2520and%2520Osvaldo%2520Simeone%26entry.1292438233%3D%2520%2520Black-box%2520zero-th%2520order%2520optimization%2520is%2520a%2520central%2520primitive%2520for%2520applications%250Ain%2520fields%2520as%2520diverse%2520as%2520finance%252C%2520physics%252C%2520and%2520engineering.%2520In%2520a%2520common%250Aformulation%2520of%2520this%2520problem%252C%2520a%2520designer%2520sequentially%2520attempts%2520candidate%250Asolutions%252C%2520receiving%2520noisy%2520feedback%2520on%2520the%2520value%2520of%2520each%2520attempt%2520from%2520the%250Asystem.%2520In%2520this%2520paper%252C%2520we%2520study%2520scenarios%2520in%2520which%2520feedback%2520is%2520also%2520provided%2520on%250Athe%2520safety%2520of%2520the%2520attempted%2520solution%252C%2520and%2520the%2520optimizer%2520is%2520constrained%2520to%2520limit%250Athe%2520number%2520of%2520unsafe%2520solutions%2520that%2520are%2520tried%2520throughout%2520the%2520optimization%250Aprocess.%2520Focusing%2520on%2520methods%2520based%2520on%2520Bayesian%2520optimization%2520%2528BO%2529%252C%2520prior%2520art%2520has%250Aintroduced%2520an%2520optimization%2520scheme%2520--%2520referred%2520to%2520as%2520SAFEOPT%2520--%2520that%2520is%250Aguaranteed%2520not%2520to%2520select%2520any%2520unsafe%2520solution%2520with%2520a%2520controllable%2520probability%250Aover%2520feedback%2520noise%2520as%2520long%2520as%2520strict%2520assumptions%2520on%2520the%2520safety%2520constraint%250Afunction%2520are%2520met.%2520In%2520this%2520paper%252C%2520a%2520novel%2520BO-based%2520approach%2520is%2520introduced%2520that%250Asatisfies%2520safety%2520requirements%2520irrespective%2520of%2520properties%2520of%2520the%2520constraint%250Afunction.%2520This%2520strong%2520theoretical%2520guarantee%2520is%2520obtained%2520at%2520the%2520cost%2520of%2520allowing%250Afor%2520an%2520arbitrary%252C%2520controllable%2520but%2520non-zero%252C%2520rate%2520of%2520violation%2520of%2520the%2520safety%250Aconstraint.%2520The%2520proposed%2520method%252C%2520referred%2520to%2520as%2520SAFE-BOCP%252C%2520builds%2520on%2520online%250Aconformal%2520prediction%2520%2528CP%2529%2520and%2520is%2520specialized%2520to%2520the%2520cases%2520in%2520which%2520feedback%2520on%250Athe%2520safety%2520constraint%2520is%2520either%2520noiseless%2520or%2520noisy.%2520Experimental%2520results%2520on%250Asynthetic%2520and%2520real-world%2520data%2520validate%2520the%2520advantages%2520and%2520flexibility%2520of%2520the%250Aproposed%2520SAFE-BOCP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.17815v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Optimization%20with%20Formal%20Safety%20Guarantees%20via%20Online%20Conformal%0A%20%20Prediction&entry.906535625=Yunchuan%20Zhang%20and%20Sangwoo%20Park%20and%20Osvaldo%20Simeone&entry.1292438233=%20%20Black-box%20zero-th%20order%20optimization%20is%20a%20central%20primitive%20for%20applications%0Ain%20fields%20as%20diverse%20as%20finance%2C%20physics%2C%20and%20engineering.%20In%20a%20common%0Aformulation%20of%20this%20problem%2C%20a%20designer%20sequentially%20attempts%20candidate%0Asolutions%2C%20receiving%20noisy%20feedback%20on%20the%20value%20of%20each%20attempt%20from%20the%0Asystem.%20In%20this%20paper%2C%20we%20study%20scenarios%20in%20which%20feedback%20is%20also%20provided%20on%0Athe%20safety%20of%20the%20attempted%20solution%2C%20and%20the%20optimizer%20is%20constrained%20to%20limit%0Athe%20number%20of%20unsafe%20solutions%20that%20are%20tried%20throughout%20the%20optimization%0Aprocess.%20Focusing%20on%20methods%20based%20on%20Bayesian%20optimization%20%28BO%29%2C%20prior%20art%20has%0Aintroduced%20an%20optimization%20scheme%20--%20referred%20to%20as%20SAFEOPT%20--%20that%20is%0Aguaranteed%20not%20to%20select%20any%20unsafe%20solution%20with%20a%20controllable%20probability%0Aover%20feedback%20noise%20as%20long%20as%20strict%20assumptions%20on%20the%20safety%20constraint%0Afunction%20are%20met.%20In%20this%20paper%2C%20a%20novel%20BO-based%20approach%20is%20introduced%20that%0Asatisfies%20safety%20requirements%20irrespective%20of%20properties%20of%20the%20constraint%0Afunction.%20This%20strong%20theoretical%20guarantee%20is%20obtained%20at%20the%20cost%20of%20allowing%0Afor%20an%20arbitrary%2C%20controllable%20but%20non-zero%2C%20rate%20of%20violation%20of%20the%20safety%0Aconstraint.%20The%20proposed%20method%2C%20referred%20to%20as%20SAFE-BOCP%2C%20builds%20on%20online%0Aconformal%20prediction%20%28CP%29%20and%20is%20specialized%20to%20the%20cases%20in%20which%20feedback%20on%0Athe%20safety%20constraint%20is%20either%20noiseless%20or%20noisy.%20Experimental%20results%20on%0Asynthetic%20and%20real-world%20data%20validate%20the%20advantages%20and%20flexibility%20of%20the%0Aproposed%20SAFE-BOCP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.17815v2&entry.124074799=Read"},
{"title": "G-VOILA: Gaze-Facilitated Information Querying in Daily Scenarios", "author": "Zeyu Wang and Yuanchun Shi and Yuntao Wang and Yuchen Yao and Kun Yan and Yuhan Wang and Lei Ji and Xuhai Xu and Chun Yu", "abstract": "  Modern information querying systems are progressively incorporating\nmultimodal inputs like vision and audio. However, the integration of gaze -- a\nmodality deeply linked to user intent and increasingly accessible via\ngaze-tracking wearables -- remains underexplored. This paper introduces a novel\ngaze-facilitated information querying paradigm, named G-VOILA, which synergizes\nusers' gaze, visual field, and voice-based natural language queries to\nfacilitate a more intuitive querying process. In a user-enactment study\ninvolving 21 participants in 3 daily scenarios (p = 21, scene = 3), we revealed\nthe ambiguity in users' query language and a gaze-voice coordination pattern in\nusers' natural query behaviors with G-VOILA. Based on the quantitative and\nqualitative findings, we developed a design framework for the G-VOILA paradigm,\nwhich effectively integrates the gaze data with the in-situ querying context.\nThen we implemented a G-VOILA proof-of-concept using cutting-edge deep learning\ntechniques. A follow-up user study (p = 16, scene = 2) demonstrates its\neffectiveness by achieving both higher objective score and subjective score,\ncompared to a baseline without gaze data. We further conducted interviews and\nprovided insights for future gaze-facilitated information querying systems.\n", "link": "http://arxiv.org/abs/2405.07652v1", "date": "2024-05-13", "relevancy": 1.9288, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5013}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.481}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G-VOILA%3A%20Gaze-Facilitated%20Information%20Querying%20in%20Daily%20Scenarios&body=Title%3A%20G-VOILA%3A%20Gaze-Facilitated%20Information%20Querying%20in%20Daily%20Scenarios%0AAuthor%3A%20Zeyu%20Wang%20and%20Yuanchun%20Shi%20and%20Yuntao%20Wang%20and%20Yuchen%20Yao%20and%20Kun%20Yan%20and%20Yuhan%20Wang%20and%20Lei%20Ji%20and%20Xuhai%20Xu%20and%20Chun%20Yu%0AAbstract%3A%20%20%20Modern%20information%20querying%20systems%20are%20progressively%20incorporating%0Amultimodal%20inputs%20like%20vision%20and%20audio.%20However%2C%20the%20integration%20of%20gaze%20--%20a%0Amodality%20deeply%20linked%20to%20user%20intent%20and%20increasingly%20accessible%20via%0Agaze-tracking%20wearables%20--%20remains%20underexplored.%20This%20paper%20introduces%20a%20novel%0Agaze-facilitated%20information%20querying%20paradigm%2C%20named%20G-VOILA%2C%20which%20synergizes%0Ausers%27%20gaze%2C%20visual%20field%2C%20and%20voice-based%20natural%20language%20queries%20to%0Afacilitate%20a%20more%20intuitive%20querying%20process.%20In%20a%20user-enactment%20study%0Ainvolving%2021%20participants%20in%203%20daily%20scenarios%20%28p%20%3D%2021%2C%20scene%20%3D%203%29%2C%20we%20revealed%0Athe%20ambiguity%20in%20users%27%20query%20language%20and%20a%20gaze-voice%20coordination%20pattern%20in%0Ausers%27%20natural%20query%20behaviors%20with%20G-VOILA.%20Based%20on%20the%20quantitative%20and%0Aqualitative%20findings%2C%20we%20developed%20a%20design%20framework%20for%20the%20G-VOILA%20paradigm%2C%0Awhich%20effectively%20integrates%20the%20gaze%20data%20with%20the%20in-situ%20querying%20context.%0AThen%20we%20implemented%20a%20G-VOILA%20proof-of-concept%20using%20cutting-edge%20deep%20learning%0Atechniques.%20A%20follow-up%20user%20study%20%28p%20%3D%2016%2C%20scene%20%3D%202%29%20demonstrates%20its%0Aeffectiveness%20by%20achieving%20both%20higher%20objective%20score%20and%20subjective%20score%2C%0Acompared%20to%20a%20baseline%20without%20gaze%20data.%20We%20further%20conducted%20interviews%20and%0Aprovided%20insights%20for%20future%20gaze-facilitated%20information%20querying%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07652v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG-VOILA%253A%2520Gaze-Facilitated%2520Information%2520Querying%2520in%2520Daily%2520Scenarios%26entry.906535625%3DZeyu%2520Wang%2520and%2520Yuanchun%2520Shi%2520and%2520Yuntao%2520Wang%2520and%2520Yuchen%2520Yao%2520and%2520Kun%2520Yan%2520and%2520Yuhan%2520Wang%2520and%2520Lei%2520Ji%2520and%2520Xuhai%2520Xu%2520and%2520Chun%2520Yu%26entry.1292438233%3D%2520%2520Modern%2520information%2520querying%2520systems%2520are%2520progressively%2520incorporating%250Amultimodal%2520inputs%2520like%2520vision%2520and%2520audio.%2520However%252C%2520the%2520integration%2520of%2520gaze%2520--%2520a%250Amodality%2520deeply%2520linked%2520to%2520user%2520intent%2520and%2520increasingly%2520accessible%2520via%250Agaze-tracking%2520wearables%2520--%2520remains%2520underexplored.%2520This%2520paper%2520introduces%2520a%2520novel%250Agaze-facilitated%2520information%2520querying%2520paradigm%252C%2520named%2520G-VOILA%252C%2520which%2520synergizes%250Ausers%2527%2520gaze%252C%2520visual%2520field%252C%2520and%2520voice-based%2520natural%2520language%2520queries%2520to%250Afacilitate%2520a%2520more%2520intuitive%2520querying%2520process.%2520In%2520a%2520user-enactment%2520study%250Ainvolving%252021%2520participants%2520in%25203%2520daily%2520scenarios%2520%2528p%2520%253D%252021%252C%2520scene%2520%253D%25203%2529%252C%2520we%2520revealed%250Athe%2520ambiguity%2520in%2520users%2527%2520query%2520language%2520and%2520a%2520gaze-voice%2520coordination%2520pattern%2520in%250Ausers%2527%2520natural%2520query%2520behaviors%2520with%2520G-VOILA.%2520Based%2520on%2520the%2520quantitative%2520and%250Aqualitative%2520findings%252C%2520we%2520developed%2520a%2520design%2520framework%2520for%2520the%2520G-VOILA%2520paradigm%252C%250Awhich%2520effectively%2520integrates%2520the%2520gaze%2520data%2520with%2520the%2520in-situ%2520querying%2520context.%250AThen%2520we%2520implemented%2520a%2520G-VOILA%2520proof-of-concept%2520using%2520cutting-edge%2520deep%2520learning%250Atechniques.%2520A%2520follow-up%2520user%2520study%2520%2528p%2520%253D%252016%252C%2520scene%2520%253D%25202%2529%2520demonstrates%2520its%250Aeffectiveness%2520by%2520achieving%2520both%2520higher%2520objective%2520score%2520and%2520subjective%2520score%252C%250Acompared%2520to%2520a%2520baseline%2520without%2520gaze%2520data.%2520We%2520further%2520conducted%2520interviews%2520and%250Aprovided%2520insights%2520for%2520future%2520gaze-facilitated%2520information%2520querying%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07652v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G-VOILA%3A%20Gaze-Facilitated%20Information%20Querying%20in%20Daily%20Scenarios&entry.906535625=Zeyu%20Wang%20and%20Yuanchun%20Shi%20and%20Yuntao%20Wang%20and%20Yuchen%20Yao%20and%20Kun%20Yan%20and%20Yuhan%20Wang%20and%20Lei%20Ji%20and%20Xuhai%20Xu%20and%20Chun%20Yu&entry.1292438233=%20%20Modern%20information%20querying%20systems%20are%20progressively%20incorporating%0Amultimodal%20inputs%20like%20vision%20and%20audio.%20However%2C%20the%20integration%20of%20gaze%20--%20a%0Amodality%20deeply%20linked%20to%20user%20intent%20and%20increasingly%20accessible%20via%0Agaze-tracking%20wearables%20--%20remains%20underexplored.%20This%20paper%20introduces%20a%20novel%0Agaze-facilitated%20information%20querying%20paradigm%2C%20named%20G-VOILA%2C%20which%20synergizes%0Ausers%27%20gaze%2C%20visual%20field%2C%20and%20voice-based%20natural%20language%20queries%20to%0Afacilitate%20a%20more%20intuitive%20querying%20process.%20In%20a%20user-enactment%20study%0Ainvolving%2021%20participants%20in%203%20daily%20scenarios%20%28p%20%3D%2021%2C%20scene%20%3D%203%29%2C%20we%20revealed%0Athe%20ambiguity%20in%20users%27%20query%20language%20and%20a%20gaze-voice%20coordination%20pattern%20in%0Ausers%27%20natural%20query%20behaviors%20with%20G-VOILA.%20Based%20on%20the%20quantitative%20and%0Aqualitative%20findings%2C%20we%20developed%20a%20design%20framework%20for%20the%20G-VOILA%20paradigm%2C%0Awhich%20effectively%20integrates%20the%20gaze%20data%20with%20the%20in-situ%20querying%20context.%0AThen%20we%20implemented%20a%20G-VOILA%20proof-of-concept%20using%20cutting-edge%20deep%20learning%0Atechniques.%20A%20follow-up%20user%20study%20%28p%20%3D%2016%2C%20scene%20%3D%202%29%20demonstrates%20its%0Aeffectiveness%20by%20achieving%20both%20higher%20objective%20score%20and%20subjective%20score%2C%0Acompared%20to%20a%20baseline%20without%20gaze%20data.%20We%20further%20conducted%20interviews%20and%0Aprovided%20insights%20for%20future%20gaze-facilitated%20information%20querying%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07652v1&entry.124074799=Read"},
{"title": "A Unified Sequence Parallelism Approach for Long Context Generative AI", "author": "Jiarui Fang and Shangchun Zhao", "abstract": "  Sequence parallelism (SP), which divides the sequence dimension of input\ntensors across multiple computational devices, is becoming key to unlocking the\nlong-context capabilities of generative AI models. This paper investigates the\nstate-of-the-art SP approaches, i.e. DeepSpeed-Ulysses and Ring-Attention, and\nproposes a unified SP approach, which is more robust to transformer model\narchitectures and network hardware topology. This paper compares the\ncommunication and memory cost of SP and existing parallelism, including\ndata/tensor/zero/expert/pipeline parallelism, and discusses the best practices\nfor designing hybrid 4D parallelism involving SP. We achieved 86\\% MFU on two\n8xA800 nodes using SP for sequence length 208K for the LLAMA3-8B model. Our\ncode is publicly available on\n\\url{https://github.com/feifeibear/long-context-attention}.\n", "link": "http://arxiv.org/abs/2405.07719v1", "date": "2024-05-13", "relevancy": 1.9268, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4847}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4818}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Sequence%20Parallelism%20Approach%20for%20Long%20Context%20Generative%20AI&body=Title%3A%20A%20Unified%20Sequence%20Parallelism%20Approach%20for%20Long%20Context%20Generative%20AI%0AAuthor%3A%20Jiarui%20Fang%20and%20Shangchun%20Zhao%0AAbstract%3A%20%20%20Sequence%20parallelism%20%28SP%29%2C%20which%20divides%20the%20sequence%20dimension%20of%20input%0Atensors%20across%20multiple%20computational%20devices%2C%20is%20becoming%20key%20to%20unlocking%20the%0Along-context%20capabilities%20of%20generative%20AI%20models.%20This%20paper%20investigates%20the%0Astate-of-the-art%20SP%20approaches%2C%20i.e.%20DeepSpeed-Ulysses%20and%20Ring-Attention%2C%20and%0Aproposes%20a%20unified%20SP%20approach%2C%20which%20is%20more%20robust%20to%20transformer%20model%0Aarchitectures%20and%20network%20hardware%20topology.%20This%20paper%20compares%20the%0Acommunication%20and%20memory%20cost%20of%20SP%20and%20existing%20parallelism%2C%20including%0Adata/tensor/zero/expert/pipeline%20parallelism%2C%20and%20discusses%20the%20best%20practices%0Afor%20designing%20hybrid%204D%20parallelism%20involving%20SP.%20We%20achieved%2086%5C%25%20MFU%20on%20two%0A8xA800%20nodes%20using%20SP%20for%20sequence%20length%20208K%20for%20the%20LLAMA3-8B%20model.%20Our%0Acode%20is%20publicly%20available%20on%0A%5Curl%7Bhttps%3A//github.com/feifeibear/long-context-attention%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Sequence%2520Parallelism%2520Approach%2520for%2520Long%2520Context%2520Generative%2520AI%26entry.906535625%3DJiarui%2520Fang%2520and%2520Shangchun%2520Zhao%26entry.1292438233%3D%2520%2520Sequence%2520parallelism%2520%2528SP%2529%252C%2520which%2520divides%2520the%2520sequence%2520dimension%2520of%2520input%250Atensors%2520across%2520multiple%2520computational%2520devices%252C%2520is%2520becoming%2520key%2520to%2520unlocking%2520the%250Along-context%2520capabilities%2520of%2520generative%2520AI%2520models.%2520This%2520paper%2520investigates%2520the%250Astate-of-the-art%2520SP%2520approaches%252C%2520i.e.%2520DeepSpeed-Ulysses%2520and%2520Ring-Attention%252C%2520and%250Aproposes%2520a%2520unified%2520SP%2520approach%252C%2520which%2520is%2520more%2520robust%2520to%2520transformer%2520model%250Aarchitectures%2520and%2520network%2520hardware%2520topology.%2520This%2520paper%2520compares%2520the%250Acommunication%2520and%2520memory%2520cost%2520of%2520SP%2520and%2520existing%2520parallelism%252C%2520including%250Adata/tensor/zero/expert/pipeline%2520parallelism%252C%2520and%2520discusses%2520the%2520best%2520practices%250Afor%2520designing%2520hybrid%25204D%2520parallelism%2520involving%2520SP.%2520We%2520achieved%252086%255C%2525%2520MFU%2520on%2520two%250A8xA800%2520nodes%2520using%2520SP%2520for%2520sequence%2520length%2520208K%2520for%2520the%2520LLAMA3-8B%2520model.%2520Our%250Acode%2520is%2520publicly%2520available%2520on%250A%255Curl%257Bhttps%253A//github.com/feifeibear/long-context-attention%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Sequence%20Parallelism%20Approach%20for%20Long%20Context%20Generative%20AI&entry.906535625=Jiarui%20Fang%20and%20Shangchun%20Zhao&entry.1292438233=%20%20Sequence%20parallelism%20%28SP%29%2C%20which%20divides%20the%20sequence%20dimension%20of%20input%0Atensors%20across%20multiple%20computational%20devices%2C%20is%20becoming%20key%20to%20unlocking%20the%0Along-context%20capabilities%20of%20generative%20AI%20models.%20This%20paper%20investigates%20the%0Astate-of-the-art%20SP%20approaches%2C%20i.e.%20DeepSpeed-Ulysses%20and%20Ring-Attention%2C%20and%0Aproposes%20a%20unified%20SP%20approach%2C%20which%20is%20more%20robust%20to%20transformer%20model%0Aarchitectures%20and%20network%20hardware%20topology.%20This%20paper%20compares%20the%0Acommunication%20and%20memory%20cost%20of%20SP%20and%20existing%20parallelism%2C%20including%0Adata/tensor/zero/expert/pipeline%20parallelism%2C%20and%20discusses%20the%20best%20practices%0Afor%20designing%20hybrid%204D%20parallelism%20involving%20SP.%20We%20achieved%2086%5C%25%20MFU%20on%20two%0A8xA800%20nodes%20using%20SP%20for%20sequence%20length%20208K%20for%20the%20LLAMA3-8B%20model.%20Our%0Acode%20is%20publicly%20available%20on%0A%5Curl%7Bhttps%3A//github.com/feifeibear/long-context-attention%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07719v1&entry.124074799=Read"},
{"title": "NoiseBench: Benchmarking the Impact of Real Label Noise on Named Entity\n  Recognition", "author": "Elena Merdjanovska and Ansar Aynetdinov and Alan Akbik", "abstract": "  Available training data for named entity recognition (NER) often contains a\nsignificant percentage of incorrect labels for entity types and entity\nboundaries. Such label noise poses challenges for supervised learning and may\nsignificantly deteriorate model quality. To address this, prior work proposed\nvarious noise-robust learning approaches capable of learning from data with\npartially incorrect labels. These approaches are typically evaluated using\nsimulated noise where the labels in a clean dataset are automatically\ncorrupted. However, as we show in this paper, this leads to unrealistic noise\nthat is far easier to handle than real noise caused by human error or\nsemi-automatic annotation. To enable the study of the impact of various types\nof real noise, we introduce NoiseBench, an NER benchmark consisting of clean\ntraining data corrupted with 6 types of real noise, including expert errors,\ncrowdsourcing errors, automatic annotation errors and LLM errors. We present an\nanalysis that shows that real noise is significantly more challenging than\nsimulated noise, and show that current state-of-the-art models for noise-robust\nlearning fall far short of their theoretically achievable upper bound. We\nrelease NoiseBench to the research community.\n", "link": "http://arxiv.org/abs/2405.07609v1", "date": "2024-05-13", "relevancy": 1.9251, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.513}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4806}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NoiseBench%3A%20Benchmarking%20the%20Impact%20of%20Real%20Label%20Noise%20on%20Named%20Entity%0A%20%20Recognition&body=Title%3A%20NoiseBench%3A%20Benchmarking%20the%20Impact%20of%20Real%20Label%20Noise%20on%20Named%20Entity%0A%20%20Recognition%0AAuthor%3A%20Elena%20Merdjanovska%20and%20Ansar%20Aynetdinov%20and%20Alan%20Akbik%0AAbstract%3A%20%20%20Available%20training%20data%20for%20named%20entity%20recognition%20%28NER%29%20often%20contains%20a%0Asignificant%20percentage%20of%20incorrect%20labels%20for%20entity%20types%20and%20entity%0Aboundaries.%20Such%20label%20noise%20poses%20challenges%20for%20supervised%20learning%20and%20may%0Asignificantly%20deteriorate%20model%20quality.%20To%20address%20this%2C%20prior%20work%20proposed%0Avarious%20noise-robust%20learning%20approaches%20capable%20of%20learning%20from%20data%20with%0Apartially%20incorrect%20labels.%20These%20approaches%20are%20typically%20evaluated%20using%0Asimulated%20noise%20where%20the%20labels%20in%20a%20clean%20dataset%20are%20automatically%0Acorrupted.%20However%2C%20as%20we%20show%20in%20this%20paper%2C%20this%20leads%20to%20unrealistic%20noise%0Athat%20is%20far%20easier%20to%20handle%20than%20real%20noise%20caused%20by%20human%20error%20or%0Asemi-automatic%20annotation.%20To%20enable%20the%20study%20of%20the%20impact%20of%20various%20types%0Aof%20real%20noise%2C%20we%20introduce%20NoiseBench%2C%20an%20NER%20benchmark%20consisting%20of%20clean%0Atraining%20data%20corrupted%20with%206%20types%20of%20real%20noise%2C%20including%20expert%20errors%2C%0Acrowdsourcing%20errors%2C%20automatic%20annotation%20errors%20and%20LLM%20errors.%20We%20present%20an%0Aanalysis%20that%20shows%20that%20real%20noise%20is%20significantly%20more%20challenging%20than%0Asimulated%20noise%2C%20and%20show%20that%20current%20state-of-the-art%20models%20for%20noise-robust%0Alearning%20fall%20far%20short%20of%20their%20theoretically%20achievable%20upper%20bound.%20We%0Arelease%20NoiseBench%20to%20the%20research%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoiseBench%253A%2520Benchmarking%2520the%2520Impact%2520of%2520Real%2520Label%2520Noise%2520on%2520Named%2520Entity%250A%2520%2520Recognition%26entry.906535625%3DElena%2520Merdjanovska%2520and%2520Ansar%2520Aynetdinov%2520and%2520Alan%2520Akbik%26entry.1292438233%3D%2520%2520Available%2520training%2520data%2520for%2520named%2520entity%2520recognition%2520%2528NER%2529%2520often%2520contains%2520a%250Asignificant%2520percentage%2520of%2520incorrect%2520labels%2520for%2520entity%2520types%2520and%2520entity%250Aboundaries.%2520Such%2520label%2520noise%2520poses%2520challenges%2520for%2520supervised%2520learning%2520and%2520may%250Asignificantly%2520deteriorate%2520model%2520quality.%2520To%2520address%2520this%252C%2520prior%2520work%2520proposed%250Avarious%2520noise-robust%2520learning%2520approaches%2520capable%2520of%2520learning%2520from%2520data%2520with%250Apartially%2520incorrect%2520labels.%2520These%2520approaches%2520are%2520typically%2520evaluated%2520using%250Asimulated%2520noise%2520where%2520the%2520labels%2520in%2520a%2520clean%2520dataset%2520are%2520automatically%250Acorrupted.%2520However%252C%2520as%2520we%2520show%2520in%2520this%2520paper%252C%2520this%2520leads%2520to%2520unrealistic%2520noise%250Athat%2520is%2520far%2520easier%2520to%2520handle%2520than%2520real%2520noise%2520caused%2520by%2520human%2520error%2520or%250Asemi-automatic%2520annotation.%2520To%2520enable%2520the%2520study%2520of%2520the%2520impact%2520of%2520various%2520types%250Aof%2520real%2520noise%252C%2520we%2520introduce%2520NoiseBench%252C%2520an%2520NER%2520benchmark%2520consisting%2520of%2520clean%250Atraining%2520data%2520corrupted%2520with%25206%2520types%2520of%2520real%2520noise%252C%2520including%2520expert%2520errors%252C%250Acrowdsourcing%2520errors%252C%2520automatic%2520annotation%2520errors%2520and%2520LLM%2520errors.%2520We%2520present%2520an%250Aanalysis%2520that%2520shows%2520that%2520real%2520noise%2520is%2520significantly%2520more%2520challenging%2520than%250Asimulated%2520noise%252C%2520and%2520show%2520that%2520current%2520state-of-the-art%2520models%2520for%2520noise-robust%250Alearning%2520fall%2520far%2520short%2520of%2520their%2520theoretically%2520achievable%2520upper%2520bound.%2520We%250Arelease%2520NoiseBench%2520to%2520the%2520research%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NoiseBench%3A%20Benchmarking%20the%20Impact%20of%20Real%20Label%20Noise%20on%20Named%20Entity%0A%20%20Recognition&entry.906535625=Elena%20Merdjanovska%20and%20Ansar%20Aynetdinov%20and%20Alan%20Akbik&entry.1292438233=%20%20Available%20training%20data%20for%20named%20entity%20recognition%20%28NER%29%20often%20contains%20a%0Asignificant%20percentage%20of%20incorrect%20labels%20for%20entity%20types%20and%20entity%0Aboundaries.%20Such%20label%20noise%20poses%20challenges%20for%20supervised%20learning%20and%20may%0Asignificantly%20deteriorate%20model%20quality.%20To%20address%20this%2C%20prior%20work%20proposed%0Avarious%20noise-robust%20learning%20approaches%20capable%20of%20learning%20from%20data%20with%0Apartially%20incorrect%20labels.%20These%20approaches%20are%20typically%20evaluated%20using%0Asimulated%20noise%20where%20the%20labels%20in%20a%20clean%20dataset%20are%20automatically%0Acorrupted.%20However%2C%20as%20we%20show%20in%20this%20paper%2C%20this%20leads%20to%20unrealistic%20noise%0Athat%20is%20far%20easier%20to%20handle%20than%20real%20noise%20caused%20by%20human%20error%20or%0Asemi-automatic%20annotation.%20To%20enable%20the%20study%20of%20the%20impact%20of%20various%20types%0Aof%20real%20noise%2C%20we%20introduce%20NoiseBench%2C%20an%20NER%20benchmark%20consisting%20of%20clean%0Atraining%20data%20corrupted%20with%206%20types%20of%20real%20noise%2C%20including%20expert%20errors%2C%0Acrowdsourcing%20errors%2C%20automatic%20annotation%20errors%20and%20LLM%20errors.%20We%20present%20an%0Aanalysis%20that%20shows%20that%20real%20noise%20is%20significantly%20more%20challenging%20than%0Asimulated%20noise%2C%20and%20show%20that%20current%20state-of-the-art%20models%20for%20noise-robust%0Alearning%20fall%20far%20short%20of%20their%20theoretically%20achievable%20upper%20bound.%20We%0Arelease%20NoiseBench%20to%20the%20research%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07609v1&entry.124074799=Read"},
{"title": "DP-DCAN: Differentially Private Deep Contrastive Autoencoder Network for\n  Single-cell Clustering", "author": "Huifa Li and Jie Fu and Zhili Chen and Xiaomin Yang and Haitao Liu and Xinpeng Ling", "abstract": "  Single-cell RNA sequencing (scRNA-seq) is important to transcriptomic\nanalysis of gene expression. Recently, deep learning has facilitated the\nanalysis of high-dimensional single-cell data. Unfortunately, deep learning\nmodels may leak sensitive information about users. As a result, Differential\nPrivacy (DP) is increasingly used to protect privacy. However, existing DP\nmethods usually perturb whole neural networks to achieve differential privacy,\nand hence result in great performance overheads. To address this challenge, in\nthis paper, we take advantage of the uniqueness of the autoencoder that it\noutputs only the dimension-reduced vector in the middle of the network, and\ndesign a Differentially Private Deep Contrastive Autoencoder Network (DP-DCAN)\nby partial network perturbation for single-cell clustering. Since only partial\nnetwork is added with noise, the performance improvement is obvious and\ntwofold: one part of network is trained with less noise due to a bigger privacy\nbudget, and the other part is trained without any noise. Experimental results\nof six datasets have verified that DP-DCAN is superior to the traditional DP\nscheme with whole network perturbation. Moreover, DP-DCAN demonstrates strong\nrobustness to adversarial attacks.\n", "link": "http://arxiv.org/abs/2311.03410v2", "date": "2024-05-13", "relevancy": 1.9218, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4853}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4772}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DP-DCAN%3A%20Differentially%20Private%20Deep%20Contrastive%20Autoencoder%20Network%20for%0A%20%20Single-cell%20Clustering&body=Title%3A%20DP-DCAN%3A%20Differentially%20Private%20Deep%20Contrastive%20Autoencoder%20Network%20for%0A%20%20Single-cell%20Clustering%0AAuthor%3A%20Huifa%20Li%20and%20Jie%20Fu%20and%20Zhili%20Chen%20and%20Xiaomin%20Yang%20and%20Haitao%20Liu%20and%20Xinpeng%20Ling%0AAbstract%3A%20%20%20Single-cell%20RNA%20sequencing%20%28scRNA-seq%29%20is%20important%20to%20transcriptomic%0Aanalysis%20of%20gene%20expression.%20Recently%2C%20deep%20learning%20has%20facilitated%20the%0Aanalysis%20of%20high-dimensional%20single-cell%20data.%20Unfortunately%2C%20deep%20learning%0Amodels%20may%20leak%20sensitive%20information%20about%20users.%20As%20a%20result%2C%20Differential%0APrivacy%20%28DP%29%20is%20increasingly%20used%20to%20protect%20privacy.%20However%2C%20existing%20DP%0Amethods%20usually%20perturb%20whole%20neural%20networks%20to%20achieve%20differential%20privacy%2C%0Aand%20hence%20result%20in%20great%20performance%20overheads.%20To%20address%20this%20challenge%2C%20in%0Athis%20paper%2C%20we%20take%20advantage%20of%20the%20uniqueness%20of%20the%20autoencoder%20that%20it%0Aoutputs%20only%20the%20dimension-reduced%20vector%20in%20the%20middle%20of%20the%20network%2C%20and%0Adesign%20a%20Differentially%20Private%20Deep%20Contrastive%20Autoencoder%20Network%20%28DP-DCAN%29%0Aby%20partial%20network%20perturbation%20for%20single-cell%20clustering.%20Since%20only%20partial%0Anetwork%20is%20added%20with%20noise%2C%20the%20performance%20improvement%20is%20obvious%20and%0Atwofold%3A%20one%20part%20of%20network%20is%20trained%20with%20less%20noise%20due%20to%20a%20bigger%20privacy%0Abudget%2C%20and%20the%20other%20part%20is%20trained%20without%20any%20noise.%20Experimental%20results%0Aof%20six%20datasets%20have%20verified%20that%20DP-DCAN%20is%20superior%20to%20the%20traditional%20DP%0Ascheme%20with%20whole%20network%20perturbation.%20Moreover%2C%20DP-DCAN%20demonstrates%20strong%0Arobustness%20to%20adversarial%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.03410v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDP-DCAN%253A%2520Differentially%2520Private%2520Deep%2520Contrastive%2520Autoencoder%2520Network%2520for%250A%2520%2520Single-cell%2520Clustering%26entry.906535625%3DHuifa%2520Li%2520and%2520Jie%2520Fu%2520and%2520Zhili%2520Chen%2520and%2520Xiaomin%2520Yang%2520and%2520Haitao%2520Liu%2520and%2520Xinpeng%2520Ling%26entry.1292438233%3D%2520%2520Single-cell%2520RNA%2520sequencing%2520%2528scRNA-seq%2529%2520is%2520important%2520to%2520transcriptomic%250Aanalysis%2520of%2520gene%2520expression.%2520Recently%252C%2520deep%2520learning%2520has%2520facilitated%2520the%250Aanalysis%2520of%2520high-dimensional%2520single-cell%2520data.%2520Unfortunately%252C%2520deep%2520learning%250Amodels%2520may%2520leak%2520sensitive%2520information%2520about%2520users.%2520As%2520a%2520result%252C%2520Differential%250APrivacy%2520%2528DP%2529%2520is%2520increasingly%2520used%2520to%2520protect%2520privacy.%2520However%252C%2520existing%2520DP%250Amethods%2520usually%2520perturb%2520whole%2520neural%2520networks%2520to%2520achieve%2520differential%2520privacy%252C%250Aand%2520hence%2520result%2520in%2520great%2520performance%2520overheads.%2520To%2520address%2520this%2520challenge%252C%2520in%250Athis%2520paper%252C%2520we%2520take%2520advantage%2520of%2520the%2520uniqueness%2520of%2520the%2520autoencoder%2520that%2520it%250Aoutputs%2520only%2520the%2520dimension-reduced%2520vector%2520in%2520the%2520middle%2520of%2520the%2520network%252C%2520and%250Adesign%2520a%2520Differentially%2520Private%2520Deep%2520Contrastive%2520Autoencoder%2520Network%2520%2528DP-DCAN%2529%250Aby%2520partial%2520network%2520perturbation%2520for%2520single-cell%2520clustering.%2520Since%2520only%2520partial%250Anetwork%2520is%2520added%2520with%2520noise%252C%2520the%2520performance%2520improvement%2520is%2520obvious%2520and%250Atwofold%253A%2520one%2520part%2520of%2520network%2520is%2520trained%2520with%2520less%2520noise%2520due%2520to%2520a%2520bigger%2520privacy%250Abudget%252C%2520and%2520the%2520other%2520part%2520is%2520trained%2520without%2520any%2520noise.%2520Experimental%2520results%250Aof%2520six%2520datasets%2520have%2520verified%2520that%2520DP-DCAN%2520is%2520superior%2520to%2520the%2520traditional%2520DP%250Ascheme%2520with%2520whole%2520network%2520perturbation.%2520Moreover%252C%2520DP-DCAN%2520demonstrates%2520strong%250Arobustness%2520to%2520adversarial%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.03410v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DP-DCAN%3A%20Differentially%20Private%20Deep%20Contrastive%20Autoencoder%20Network%20for%0A%20%20Single-cell%20Clustering&entry.906535625=Huifa%20Li%20and%20Jie%20Fu%20and%20Zhili%20Chen%20and%20Xiaomin%20Yang%20and%20Haitao%20Liu%20and%20Xinpeng%20Ling&entry.1292438233=%20%20Single-cell%20RNA%20sequencing%20%28scRNA-seq%29%20is%20important%20to%20transcriptomic%0Aanalysis%20of%20gene%20expression.%20Recently%2C%20deep%20learning%20has%20facilitated%20the%0Aanalysis%20of%20high-dimensional%20single-cell%20data.%20Unfortunately%2C%20deep%20learning%0Amodels%20may%20leak%20sensitive%20information%20about%20users.%20As%20a%20result%2C%20Differential%0APrivacy%20%28DP%29%20is%20increasingly%20used%20to%20protect%20privacy.%20However%2C%20existing%20DP%0Amethods%20usually%20perturb%20whole%20neural%20networks%20to%20achieve%20differential%20privacy%2C%0Aand%20hence%20result%20in%20great%20performance%20overheads.%20To%20address%20this%20challenge%2C%20in%0Athis%20paper%2C%20we%20take%20advantage%20of%20the%20uniqueness%20of%20the%20autoencoder%20that%20it%0Aoutputs%20only%20the%20dimension-reduced%20vector%20in%20the%20middle%20of%20the%20network%2C%20and%0Adesign%20a%20Differentially%20Private%20Deep%20Contrastive%20Autoencoder%20Network%20%28DP-DCAN%29%0Aby%20partial%20network%20perturbation%20for%20single-cell%20clustering.%20Since%20only%20partial%0Anetwork%20is%20added%20with%20noise%2C%20the%20performance%20improvement%20is%20obvious%20and%0Atwofold%3A%20one%20part%20of%20network%20is%20trained%20with%20less%20noise%20due%20to%20a%20bigger%20privacy%0Abudget%2C%20and%20the%20other%20part%20is%20trained%20without%20any%20noise.%20Experimental%20results%0Aof%20six%20datasets%20have%20verified%20that%20DP-DCAN%20is%20superior%20to%20the%20traditional%20DP%0Ascheme%20with%20whole%20network%20perturbation.%20Moreover%2C%20DP-DCAN%20demonstrates%20strong%0Arobustness%20to%20adversarial%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.03410v2&entry.124074799=Read"},
{"title": "Revisiting the Power of Prompt for Visual Tuning", "author": "Yuzhu Wang and Lechao Cheng and Chaowei Fang and Dingwen Zhang and Manni Duan and Meng Wang", "abstract": "  Visual prompt tuning (VPT) is a promising solution incorporating learnable\nprompt tokens to customize pre-trained models for downstream tasks. However,\nVPT and its variants often encounter challenges like prompt initialization,\nprompt length, and subpar performance in self-supervised pretraining, hindering\nsuccessful contextual adaptation. This study commences by exploring the\ncorrelation evolvement between prompts and patch tokens during proficient\ntraining. Inspired by the observation that the prompt tokens tend to share high\nmutual information with patch tokens, we propose initializing prompts with\ndownstream token prototypes. The strategic initialization, a stand-in for the\nprevious initialization, substantially improves performance in fine-tuning. To\nrefine further, we optimize token construction with a streamlined pipeline that\nmaintains excellent performance with almost no increase in computational\nexpenses compared to VPT. Exhaustive experiments show our proposed approach\noutperforms existing methods by a remarkable margin. For instance, it surpasses\nfull fine-tuning in 19 out of 24 tasks, using less than 0.4% of learnable\nparameters on the FGVC and VTAB-1K benchmarks. Notably, our method\nsignificantly advances the adaptation for self-supervised pretraining,\nachieving impressive task performance gains of at least 10% to 30%. Besides,\nthe experimental results demonstrate the proposed SPT is robust to prompt\nlengths and scales well with model capacity and training data size. We finally\nprovide an insightful exploration into the amount of target data facilitating\nthe adaptation of pre-trained models to downstream tasks. The code is available\nat https://github.com/WangYZ1608/Self-Prompt-Tuning.\n", "link": "http://arxiv.org/abs/2402.02382v2", "date": "2024-05-13", "relevancy": 1.919, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.488}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4769}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20the%20Power%20of%20Prompt%20for%20Visual%20Tuning&body=Title%3A%20Revisiting%20the%20Power%20of%20Prompt%20for%20Visual%20Tuning%0AAuthor%3A%20Yuzhu%20Wang%20and%20Lechao%20Cheng%20and%20Chaowei%20Fang%20and%20Dingwen%20Zhang%20and%20Manni%20Duan%20and%20Meng%20Wang%0AAbstract%3A%20%20%20Visual%20prompt%20tuning%20%28VPT%29%20is%20a%20promising%20solution%20incorporating%20learnable%0Aprompt%20tokens%20to%20customize%20pre-trained%20models%20for%20downstream%20tasks.%20However%2C%0AVPT%20and%20its%20variants%20often%20encounter%20challenges%20like%20prompt%20initialization%2C%0Aprompt%20length%2C%20and%20subpar%20performance%20in%20self-supervised%20pretraining%2C%20hindering%0Asuccessful%20contextual%20adaptation.%20This%20study%20commences%20by%20exploring%20the%0Acorrelation%20evolvement%20between%20prompts%20and%20patch%20tokens%20during%20proficient%0Atraining.%20Inspired%20by%20the%20observation%20that%20the%20prompt%20tokens%20tend%20to%20share%20high%0Amutual%20information%20with%20patch%20tokens%2C%20we%20propose%20initializing%20prompts%20with%0Adownstream%20token%20prototypes.%20The%20strategic%20initialization%2C%20a%20stand-in%20for%20the%0Aprevious%20initialization%2C%20substantially%20improves%20performance%20in%20fine-tuning.%20To%0Arefine%20further%2C%20we%20optimize%20token%20construction%20with%20a%20streamlined%20pipeline%20that%0Amaintains%20excellent%20performance%20with%20almost%20no%20increase%20in%20computational%0Aexpenses%20compared%20to%20VPT.%20Exhaustive%20experiments%20show%20our%20proposed%20approach%0Aoutperforms%20existing%20methods%20by%20a%20remarkable%20margin.%20For%20instance%2C%20it%20surpasses%0Afull%20fine-tuning%20in%2019%20out%20of%2024%20tasks%2C%20using%20less%20than%200.4%25%20of%20learnable%0Aparameters%20on%20the%20FGVC%20and%20VTAB-1K%20benchmarks.%20Notably%2C%20our%20method%0Asignificantly%20advances%20the%20adaptation%20for%20self-supervised%20pretraining%2C%0Aachieving%20impressive%20task%20performance%20gains%20of%20at%20least%2010%25%20to%2030%25.%20Besides%2C%0Athe%20experimental%20results%20demonstrate%20the%20proposed%20SPT%20is%20robust%20to%20prompt%0Alengths%20and%20scales%20well%20with%20model%20capacity%20and%20training%20data%20size.%20We%20finally%0Aprovide%20an%20insightful%20exploration%20into%20the%20amount%20of%20target%20data%20facilitating%0Athe%20adaptation%20of%20pre-trained%20models%20to%20downstream%20tasks.%20The%20code%20is%20available%0Aat%20https%3A//github.com/WangYZ1608/Self-Prompt-Tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02382v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520the%2520Power%2520of%2520Prompt%2520for%2520Visual%2520Tuning%26entry.906535625%3DYuzhu%2520Wang%2520and%2520Lechao%2520Cheng%2520and%2520Chaowei%2520Fang%2520and%2520Dingwen%2520Zhang%2520and%2520Manni%2520Duan%2520and%2520Meng%2520Wang%26entry.1292438233%3D%2520%2520Visual%2520prompt%2520tuning%2520%2528VPT%2529%2520is%2520a%2520promising%2520solution%2520incorporating%2520learnable%250Aprompt%2520tokens%2520to%2520customize%2520pre-trained%2520models%2520for%2520downstream%2520tasks.%2520However%252C%250AVPT%2520and%2520its%2520variants%2520often%2520encounter%2520challenges%2520like%2520prompt%2520initialization%252C%250Aprompt%2520length%252C%2520and%2520subpar%2520performance%2520in%2520self-supervised%2520pretraining%252C%2520hindering%250Asuccessful%2520contextual%2520adaptation.%2520This%2520study%2520commences%2520by%2520exploring%2520the%250Acorrelation%2520evolvement%2520between%2520prompts%2520and%2520patch%2520tokens%2520during%2520proficient%250Atraining.%2520Inspired%2520by%2520the%2520observation%2520that%2520the%2520prompt%2520tokens%2520tend%2520to%2520share%2520high%250Amutual%2520information%2520with%2520patch%2520tokens%252C%2520we%2520propose%2520initializing%2520prompts%2520with%250Adownstream%2520token%2520prototypes.%2520The%2520strategic%2520initialization%252C%2520a%2520stand-in%2520for%2520the%250Aprevious%2520initialization%252C%2520substantially%2520improves%2520performance%2520in%2520fine-tuning.%2520To%250Arefine%2520further%252C%2520we%2520optimize%2520token%2520construction%2520with%2520a%2520streamlined%2520pipeline%2520that%250Amaintains%2520excellent%2520performance%2520with%2520almost%2520no%2520increase%2520in%2520computational%250Aexpenses%2520compared%2520to%2520VPT.%2520Exhaustive%2520experiments%2520show%2520our%2520proposed%2520approach%250Aoutperforms%2520existing%2520methods%2520by%2520a%2520remarkable%2520margin.%2520For%2520instance%252C%2520it%2520surpasses%250Afull%2520fine-tuning%2520in%252019%2520out%2520of%252024%2520tasks%252C%2520using%2520less%2520than%25200.4%2525%2520of%2520learnable%250Aparameters%2520on%2520the%2520FGVC%2520and%2520VTAB-1K%2520benchmarks.%2520Notably%252C%2520our%2520method%250Asignificantly%2520advances%2520the%2520adaptation%2520for%2520self-supervised%2520pretraining%252C%250Aachieving%2520impressive%2520task%2520performance%2520gains%2520of%2520at%2520least%252010%2525%2520to%252030%2525.%2520Besides%252C%250Athe%2520experimental%2520results%2520demonstrate%2520the%2520proposed%2520SPT%2520is%2520robust%2520to%2520prompt%250Alengths%2520and%2520scales%2520well%2520with%2520model%2520capacity%2520and%2520training%2520data%2520size.%2520We%2520finally%250Aprovide%2520an%2520insightful%2520exploration%2520into%2520the%2520amount%2520of%2520target%2520data%2520facilitating%250Athe%2520adaptation%2520of%2520pre-trained%2520models%2520to%2520downstream%2520tasks.%2520The%2520code%2520is%2520available%250Aat%2520https%253A//github.com/WangYZ1608/Self-Prompt-Tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02382v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20the%20Power%20of%20Prompt%20for%20Visual%20Tuning&entry.906535625=Yuzhu%20Wang%20and%20Lechao%20Cheng%20and%20Chaowei%20Fang%20and%20Dingwen%20Zhang%20and%20Manni%20Duan%20and%20Meng%20Wang&entry.1292438233=%20%20Visual%20prompt%20tuning%20%28VPT%29%20is%20a%20promising%20solution%20incorporating%20learnable%0Aprompt%20tokens%20to%20customize%20pre-trained%20models%20for%20downstream%20tasks.%20However%2C%0AVPT%20and%20its%20variants%20often%20encounter%20challenges%20like%20prompt%20initialization%2C%0Aprompt%20length%2C%20and%20subpar%20performance%20in%20self-supervised%20pretraining%2C%20hindering%0Asuccessful%20contextual%20adaptation.%20This%20study%20commences%20by%20exploring%20the%0Acorrelation%20evolvement%20between%20prompts%20and%20patch%20tokens%20during%20proficient%0Atraining.%20Inspired%20by%20the%20observation%20that%20the%20prompt%20tokens%20tend%20to%20share%20high%0Amutual%20information%20with%20patch%20tokens%2C%20we%20propose%20initializing%20prompts%20with%0Adownstream%20token%20prototypes.%20The%20strategic%20initialization%2C%20a%20stand-in%20for%20the%0Aprevious%20initialization%2C%20substantially%20improves%20performance%20in%20fine-tuning.%20To%0Arefine%20further%2C%20we%20optimize%20token%20construction%20with%20a%20streamlined%20pipeline%20that%0Amaintains%20excellent%20performance%20with%20almost%20no%20increase%20in%20computational%0Aexpenses%20compared%20to%20VPT.%20Exhaustive%20experiments%20show%20our%20proposed%20approach%0Aoutperforms%20existing%20methods%20by%20a%20remarkable%20margin.%20For%20instance%2C%20it%20surpasses%0Afull%20fine-tuning%20in%2019%20out%20of%2024%20tasks%2C%20using%20less%20than%200.4%25%20of%20learnable%0Aparameters%20on%20the%20FGVC%20and%20VTAB-1K%20benchmarks.%20Notably%2C%20our%20method%0Asignificantly%20advances%20the%20adaptation%20for%20self-supervised%20pretraining%2C%0Aachieving%20impressive%20task%20performance%20gains%20of%20at%20least%2010%25%20to%2030%25.%20Besides%2C%0Athe%20experimental%20results%20demonstrate%20the%20proposed%20SPT%20is%20robust%20to%20prompt%0Alengths%20and%20scales%20well%20with%20model%20capacity%20and%20training%20data%20size.%20We%20finally%0Aprovide%20an%20insightful%20exploration%20into%20the%20amount%20of%20target%20data%20facilitating%0Athe%20adaptation%20of%20pre-trained%20models%20to%20downstream%20tasks.%20The%20code%20is%20available%0Aat%20https%3A//github.com/WangYZ1608/Self-Prompt-Tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02382v2&entry.124074799=Read"},
{"title": "SPIN: Simultaneous Perception, Interaction and Navigation", "author": "Shagun Uppal and Ananye Agarwal and Haoyu Xiong and Kenneth Shaw and Deepak Pathak", "abstract": "  While there has been remarkable progress recently in the fields of\nmanipulation and locomotion, mobile manipulation remains a long-standing\nchallenge. Compared to locomotion or static manipulation, a mobile system must\nmake a diverse range of long-horizon tasks feasible in unstructured and dynamic\nenvironments. While the applications are broad and interesting, there are a\nplethora of challenges in developing these systems such as coordination between\nthe base and arm, reliance on onboard perception for perceiving and interacting\nwith the environment, and most importantly, simultaneously integrating all\nthese parts together. Prior works approach the problem using disentangled\nmodular skills for mobility and manipulation that are trivially tied together.\nThis causes several limitations such as compounding errors, delays in\ndecision-making, and no whole-body coordination. In this work, we present a\nreactive mobile manipulation framework that uses an active visual system to\nconsciously perceive and react to its environment. Similar to how humans\nleverage whole-body and hand-eye coordination, we develop a mobile manipulator\nthat exploits its ability to move and see, more specifically -- to move in\norder to see and to see in order to move. This allows it to not only move\naround and interact with its environment but also, choose \"when\" to perceive\n\"what\" using an active visual system. We observe that such an agent learns to\nnavigate around complex cluttered scenarios while displaying agile whole-body\ncoordination using only ego-vision without needing to create environment maps.\nResults visualizations and videos at https://spin-robot.github.io/\n", "link": "http://arxiv.org/abs/2405.07991v1", "date": "2024-05-13", "relevancy": 1.9048, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6454}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6406}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPIN%3A%20Simultaneous%20Perception%2C%20Interaction%20and%20Navigation&body=Title%3A%20SPIN%3A%20Simultaneous%20Perception%2C%20Interaction%20and%20Navigation%0AAuthor%3A%20Shagun%20Uppal%20and%20Ananye%20Agarwal%20and%20Haoyu%20Xiong%20and%20Kenneth%20Shaw%20and%20Deepak%20Pathak%0AAbstract%3A%20%20%20While%20there%20has%20been%20remarkable%20progress%20recently%20in%20the%20fields%20of%0Amanipulation%20and%20locomotion%2C%20mobile%20manipulation%20remains%20a%20long-standing%0Achallenge.%20Compared%20to%20locomotion%20or%20static%20manipulation%2C%20a%20mobile%20system%20must%0Amake%20a%20diverse%20range%20of%20long-horizon%20tasks%20feasible%20in%20unstructured%20and%20dynamic%0Aenvironments.%20While%20the%20applications%20are%20broad%20and%20interesting%2C%20there%20are%20a%0Aplethora%20of%20challenges%20in%20developing%20these%20systems%20such%20as%20coordination%20between%0Athe%20base%20and%20arm%2C%20reliance%20on%20onboard%20perception%20for%20perceiving%20and%20interacting%0Awith%20the%20environment%2C%20and%20most%20importantly%2C%20simultaneously%20integrating%20all%0Athese%20parts%20together.%20Prior%20works%20approach%20the%20problem%20using%20disentangled%0Amodular%20skills%20for%20mobility%20and%20manipulation%20that%20are%20trivially%20tied%20together.%0AThis%20causes%20several%20limitations%20such%20as%20compounding%20errors%2C%20delays%20in%0Adecision-making%2C%20and%20no%20whole-body%20coordination.%20In%20this%20work%2C%20we%20present%20a%0Areactive%20mobile%20manipulation%20framework%20that%20uses%20an%20active%20visual%20system%20to%0Aconsciously%20perceive%20and%20react%20to%20its%20environment.%20Similar%20to%20how%20humans%0Aleverage%20whole-body%20and%20hand-eye%20coordination%2C%20we%20develop%20a%20mobile%20manipulator%0Athat%20exploits%20its%20ability%20to%20move%20and%20see%2C%20more%20specifically%20--%20to%20move%20in%0Aorder%20to%20see%20and%20to%20see%20in%20order%20to%20move.%20This%20allows%20it%20to%20not%20only%20move%0Aaround%20and%20interact%20with%20its%20environment%20but%20also%2C%20choose%20%22when%22%20to%20perceive%0A%22what%22%20using%20an%20active%20visual%20system.%20We%20observe%20that%20such%20an%20agent%20learns%20to%0Anavigate%20around%20complex%20cluttered%20scenarios%20while%20displaying%20agile%20whole-body%0Acoordination%20using%20only%20ego-vision%20without%20needing%20to%20create%20environment%20maps.%0AResults%20visualizations%20and%20videos%20at%20https%3A//spin-robot.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPIN%253A%2520Simultaneous%2520Perception%252C%2520Interaction%2520and%2520Navigation%26entry.906535625%3DShagun%2520Uppal%2520and%2520Ananye%2520Agarwal%2520and%2520Haoyu%2520Xiong%2520and%2520Kenneth%2520Shaw%2520and%2520Deepak%2520Pathak%26entry.1292438233%3D%2520%2520While%2520there%2520has%2520been%2520remarkable%2520progress%2520recently%2520in%2520the%2520fields%2520of%250Amanipulation%2520and%2520locomotion%252C%2520mobile%2520manipulation%2520remains%2520a%2520long-standing%250Achallenge.%2520Compared%2520to%2520locomotion%2520or%2520static%2520manipulation%252C%2520a%2520mobile%2520system%2520must%250Amake%2520a%2520diverse%2520range%2520of%2520long-horizon%2520tasks%2520feasible%2520in%2520unstructured%2520and%2520dynamic%250Aenvironments.%2520While%2520the%2520applications%2520are%2520broad%2520and%2520interesting%252C%2520there%2520are%2520a%250Aplethora%2520of%2520challenges%2520in%2520developing%2520these%2520systems%2520such%2520as%2520coordination%2520between%250Athe%2520base%2520and%2520arm%252C%2520reliance%2520on%2520onboard%2520perception%2520for%2520perceiving%2520and%2520interacting%250Awith%2520the%2520environment%252C%2520and%2520most%2520importantly%252C%2520simultaneously%2520integrating%2520all%250Athese%2520parts%2520together.%2520Prior%2520works%2520approach%2520the%2520problem%2520using%2520disentangled%250Amodular%2520skills%2520for%2520mobility%2520and%2520manipulation%2520that%2520are%2520trivially%2520tied%2520together.%250AThis%2520causes%2520several%2520limitations%2520such%2520as%2520compounding%2520errors%252C%2520delays%2520in%250Adecision-making%252C%2520and%2520no%2520whole-body%2520coordination.%2520In%2520this%2520work%252C%2520we%2520present%2520a%250Areactive%2520mobile%2520manipulation%2520framework%2520that%2520uses%2520an%2520active%2520visual%2520system%2520to%250Aconsciously%2520perceive%2520and%2520react%2520to%2520its%2520environment.%2520Similar%2520to%2520how%2520humans%250Aleverage%2520whole-body%2520and%2520hand-eye%2520coordination%252C%2520we%2520develop%2520a%2520mobile%2520manipulator%250Athat%2520exploits%2520its%2520ability%2520to%2520move%2520and%2520see%252C%2520more%2520specifically%2520--%2520to%2520move%2520in%250Aorder%2520to%2520see%2520and%2520to%2520see%2520in%2520order%2520to%2520move.%2520This%2520allows%2520it%2520to%2520not%2520only%2520move%250Aaround%2520and%2520interact%2520with%2520its%2520environment%2520but%2520also%252C%2520choose%2520%2522when%2522%2520to%2520perceive%250A%2522what%2522%2520using%2520an%2520active%2520visual%2520system.%2520We%2520observe%2520that%2520such%2520an%2520agent%2520learns%2520to%250Anavigate%2520around%2520complex%2520cluttered%2520scenarios%2520while%2520displaying%2520agile%2520whole-body%250Acoordination%2520using%2520only%2520ego-vision%2520without%2520needing%2520to%2520create%2520environment%2520maps.%250AResults%2520visualizations%2520and%2520videos%2520at%2520https%253A//spin-robot.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPIN%3A%20Simultaneous%20Perception%2C%20Interaction%20and%20Navigation&entry.906535625=Shagun%20Uppal%20and%20Ananye%20Agarwal%20and%20Haoyu%20Xiong%20and%20Kenneth%20Shaw%20and%20Deepak%20Pathak&entry.1292438233=%20%20While%20there%20has%20been%20remarkable%20progress%20recently%20in%20the%20fields%20of%0Amanipulation%20and%20locomotion%2C%20mobile%20manipulation%20remains%20a%20long-standing%0Achallenge.%20Compared%20to%20locomotion%20or%20static%20manipulation%2C%20a%20mobile%20system%20must%0Amake%20a%20diverse%20range%20of%20long-horizon%20tasks%20feasible%20in%20unstructured%20and%20dynamic%0Aenvironments.%20While%20the%20applications%20are%20broad%20and%20interesting%2C%20there%20are%20a%0Aplethora%20of%20challenges%20in%20developing%20these%20systems%20such%20as%20coordination%20between%0Athe%20base%20and%20arm%2C%20reliance%20on%20onboard%20perception%20for%20perceiving%20and%20interacting%0Awith%20the%20environment%2C%20and%20most%20importantly%2C%20simultaneously%20integrating%20all%0Athese%20parts%20together.%20Prior%20works%20approach%20the%20problem%20using%20disentangled%0Amodular%20skills%20for%20mobility%20and%20manipulation%20that%20are%20trivially%20tied%20together.%0AThis%20causes%20several%20limitations%20such%20as%20compounding%20errors%2C%20delays%20in%0Adecision-making%2C%20and%20no%20whole-body%20coordination.%20In%20this%20work%2C%20we%20present%20a%0Areactive%20mobile%20manipulation%20framework%20that%20uses%20an%20active%20visual%20system%20to%0Aconsciously%20perceive%20and%20react%20to%20its%20environment.%20Similar%20to%20how%20humans%0Aleverage%20whole-body%20and%20hand-eye%20coordination%2C%20we%20develop%20a%20mobile%20manipulator%0Athat%20exploits%20its%20ability%20to%20move%20and%20see%2C%20more%20specifically%20--%20to%20move%20in%0Aorder%20to%20see%20and%20to%20see%20in%20order%20to%20move.%20This%20allows%20it%20to%20not%20only%20move%0Aaround%20and%20interact%20with%20its%20environment%20but%20also%2C%20choose%20%22when%22%20to%20perceive%0A%22what%22%20using%20an%20active%20visual%20system.%20We%20observe%20that%20such%20an%20agent%20learns%20to%0Anavigate%20around%20complex%20cluttered%20scenarios%20while%20displaying%20agile%20whole-body%0Acoordination%20using%20only%20ego-vision%20without%20needing%20to%20create%20environment%20maps.%0AResults%20visualizations%20and%20videos%20at%20https%3A//spin-robot.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07991v1&entry.124074799=Read"},
{"title": "Generative Modeling of Regular and Irregular Time Series Data via\n  Koopman VAEs", "author": "Ilan Naiman and N. Benjamin Erichson and Pu Ren and Michael W. Mahoney and Omri Azencot", "abstract": "  Generating realistic time series data is important for many engineering and\nscientific applications. Existing work tackles this problem using generative\nadversarial networks (GANs). However, GANs are unstable during training, and\nthey can suffer from mode collapse. While variational autoencoders (VAEs) are\nknown to be more robust to the these issues, they are (surprisingly) less\nconsidered for time series generation. In this work, we introduce Koopman VAE\n(KoVAE), a new generative framework that is based on a novel design for the\nmodel prior, and that can be optimized for either regular and irregular\ntraining data. Inspired by Koopman theory, we represent the latent conditional\nprior dynamics using a linear map. Our approach enhances generative modeling\nwith two desired features: (i) incorporating domain knowledge can be achieved\nby leveraging spectral tools that prescribe constraints on the eigenvalues of\nthe linear map; and (ii) studying the qualitative behavior and stability of the\nsystem can be performed using tools from dynamical systems theory. Our results\nshow that KoVAE outperforms state-of-the-art GAN and VAE methods across several\nchallenging synthetic and real-world time series generation benchmarks. Whether\ntrained on regular or irregular data, KoVAE generates time series that improve\nboth discriminative and predictive metrics. We also present visual evidence\nsuggesting that KoVAE learns probability density functions that better\napproximate the empirical ground truth distribution.\n", "link": "http://arxiv.org/abs/2310.02619v2", "date": "2024-05-13", "relevancy": 1.9016, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4943}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4649}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Modeling%20of%20Regular%20and%20Irregular%20Time%20Series%20Data%20via%0A%20%20Koopman%20VAEs&body=Title%3A%20Generative%20Modeling%20of%20Regular%20and%20Irregular%20Time%20Series%20Data%20via%0A%20%20Koopman%20VAEs%0AAuthor%3A%20Ilan%20Naiman%20and%20N.%20Benjamin%20Erichson%20and%20Pu%20Ren%20and%20Michael%20W.%20Mahoney%20and%20Omri%20Azencot%0AAbstract%3A%20%20%20Generating%20realistic%20time%20series%20data%20is%20important%20for%20many%20engineering%20and%0Ascientific%20applications.%20Existing%20work%20tackles%20this%20problem%20using%20generative%0Aadversarial%20networks%20%28GANs%29.%20However%2C%20GANs%20are%20unstable%20during%20training%2C%20and%0Athey%20can%20suffer%20from%20mode%20collapse.%20While%20variational%20autoencoders%20%28VAEs%29%20are%0Aknown%20to%20be%20more%20robust%20to%20the%20these%20issues%2C%20they%20are%20%28surprisingly%29%20less%0Aconsidered%20for%20time%20series%20generation.%20In%20this%20work%2C%20we%20introduce%20Koopman%20VAE%0A%28KoVAE%29%2C%20a%20new%20generative%20framework%20that%20is%20based%20on%20a%20novel%20design%20for%20the%0Amodel%20prior%2C%20and%20that%20can%20be%20optimized%20for%20either%20regular%20and%20irregular%0Atraining%20data.%20Inspired%20by%20Koopman%20theory%2C%20we%20represent%20the%20latent%20conditional%0Aprior%20dynamics%20using%20a%20linear%20map.%20Our%20approach%20enhances%20generative%20modeling%0Awith%20two%20desired%20features%3A%20%28i%29%20incorporating%20domain%20knowledge%20can%20be%20achieved%0Aby%20leveraging%20spectral%20tools%20that%20prescribe%20constraints%20on%20the%20eigenvalues%20of%0Athe%20linear%20map%3B%20and%20%28ii%29%20studying%20the%20qualitative%20behavior%20and%20stability%20of%20the%0Asystem%20can%20be%20performed%20using%20tools%20from%20dynamical%20systems%20theory.%20Our%20results%0Ashow%20that%20KoVAE%20outperforms%20state-of-the-art%20GAN%20and%20VAE%20methods%20across%20several%0Achallenging%20synthetic%20and%20real-world%20time%20series%20generation%20benchmarks.%20Whether%0Atrained%20on%20regular%20or%20irregular%20data%2C%20KoVAE%20generates%20time%20series%20that%20improve%0Aboth%20discriminative%20and%20predictive%20metrics.%20We%20also%20present%20visual%20evidence%0Asuggesting%20that%20KoVAE%20learns%20probability%20density%20functions%20that%20better%0Aapproximate%20the%20empirical%20ground%20truth%20distribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02619v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Modeling%2520of%2520Regular%2520and%2520Irregular%2520Time%2520Series%2520Data%2520via%250A%2520%2520Koopman%2520VAEs%26entry.906535625%3DIlan%2520Naiman%2520and%2520N.%2520Benjamin%2520Erichson%2520and%2520Pu%2520Ren%2520and%2520Michael%2520W.%2520Mahoney%2520and%2520Omri%2520Azencot%26entry.1292438233%3D%2520%2520Generating%2520realistic%2520time%2520series%2520data%2520is%2520important%2520for%2520many%2520engineering%2520and%250Ascientific%2520applications.%2520Existing%2520work%2520tackles%2520this%2520problem%2520using%2520generative%250Aadversarial%2520networks%2520%2528GANs%2529.%2520However%252C%2520GANs%2520are%2520unstable%2520during%2520training%252C%2520and%250Athey%2520can%2520suffer%2520from%2520mode%2520collapse.%2520While%2520variational%2520autoencoders%2520%2528VAEs%2529%2520are%250Aknown%2520to%2520be%2520more%2520robust%2520to%2520the%2520these%2520issues%252C%2520they%2520are%2520%2528surprisingly%2529%2520less%250Aconsidered%2520for%2520time%2520series%2520generation.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Koopman%2520VAE%250A%2528KoVAE%2529%252C%2520a%2520new%2520generative%2520framework%2520that%2520is%2520based%2520on%2520a%2520novel%2520design%2520for%2520the%250Amodel%2520prior%252C%2520and%2520that%2520can%2520be%2520optimized%2520for%2520either%2520regular%2520and%2520irregular%250Atraining%2520data.%2520Inspired%2520by%2520Koopman%2520theory%252C%2520we%2520represent%2520the%2520latent%2520conditional%250Aprior%2520dynamics%2520using%2520a%2520linear%2520map.%2520Our%2520approach%2520enhances%2520generative%2520modeling%250Awith%2520two%2520desired%2520features%253A%2520%2528i%2529%2520incorporating%2520domain%2520knowledge%2520can%2520be%2520achieved%250Aby%2520leveraging%2520spectral%2520tools%2520that%2520prescribe%2520constraints%2520on%2520the%2520eigenvalues%2520of%250Athe%2520linear%2520map%253B%2520and%2520%2528ii%2529%2520studying%2520the%2520qualitative%2520behavior%2520and%2520stability%2520of%2520the%250Asystem%2520can%2520be%2520performed%2520using%2520tools%2520from%2520dynamical%2520systems%2520theory.%2520Our%2520results%250Ashow%2520that%2520KoVAE%2520outperforms%2520state-of-the-art%2520GAN%2520and%2520VAE%2520methods%2520across%2520several%250Achallenging%2520synthetic%2520and%2520real-world%2520time%2520series%2520generation%2520benchmarks.%2520Whether%250Atrained%2520on%2520regular%2520or%2520irregular%2520data%252C%2520KoVAE%2520generates%2520time%2520series%2520that%2520improve%250Aboth%2520discriminative%2520and%2520predictive%2520metrics.%2520We%2520also%2520present%2520visual%2520evidence%250Asuggesting%2520that%2520KoVAE%2520learns%2520probability%2520density%2520functions%2520that%2520better%250Aapproximate%2520the%2520empirical%2520ground%2520truth%2520distribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02619v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Modeling%20of%20Regular%20and%20Irregular%20Time%20Series%20Data%20via%0A%20%20Koopman%20VAEs&entry.906535625=Ilan%20Naiman%20and%20N.%20Benjamin%20Erichson%20and%20Pu%20Ren%20and%20Michael%20W.%20Mahoney%20and%20Omri%20Azencot&entry.1292438233=%20%20Generating%20realistic%20time%20series%20data%20is%20important%20for%20many%20engineering%20and%0Ascientific%20applications.%20Existing%20work%20tackles%20this%20problem%20using%20generative%0Aadversarial%20networks%20%28GANs%29.%20However%2C%20GANs%20are%20unstable%20during%20training%2C%20and%0Athey%20can%20suffer%20from%20mode%20collapse.%20While%20variational%20autoencoders%20%28VAEs%29%20are%0Aknown%20to%20be%20more%20robust%20to%20the%20these%20issues%2C%20they%20are%20%28surprisingly%29%20less%0Aconsidered%20for%20time%20series%20generation.%20In%20this%20work%2C%20we%20introduce%20Koopman%20VAE%0A%28KoVAE%29%2C%20a%20new%20generative%20framework%20that%20is%20based%20on%20a%20novel%20design%20for%20the%0Amodel%20prior%2C%20and%20that%20can%20be%20optimized%20for%20either%20regular%20and%20irregular%0Atraining%20data.%20Inspired%20by%20Koopman%20theory%2C%20we%20represent%20the%20latent%20conditional%0Aprior%20dynamics%20using%20a%20linear%20map.%20Our%20approach%20enhances%20generative%20modeling%0Awith%20two%20desired%20features%3A%20%28i%29%20incorporating%20domain%20knowledge%20can%20be%20achieved%0Aby%20leveraging%20spectral%20tools%20that%20prescribe%20constraints%20on%20the%20eigenvalues%20of%0Athe%20linear%20map%3B%20and%20%28ii%29%20studying%20the%20qualitative%20behavior%20and%20stability%20of%20the%0Asystem%20can%20be%20performed%20using%20tools%20from%20dynamical%20systems%20theory.%20Our%20results%0Ashow%20that%20KoVAE%20outperforms%20state-of-the-art%20GAN%20and%20VAE%20methods%20across%20several%0Achallenging%20synthetic%20and%20real-world%20time%20series%20generation%20benchmarks.%20Whether%0Atrained%20on%20regular%20or%20irregular%20data%2C%20KoVAE%20generates%20time%20series%20that%20improve%0Aboth%20discriminative%20and%20predictive%20metrics.%20We%20also%20present%20visual%20evidence%0Asuggesting%20that%20KoVAE%20learns%20probability%20density%20functions%20that%20better%0Aapproximate%20the%20empirical%20ground%20truth%20distribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02619v2&entry.124074799=Read"},
{"title": "A Demographic-Conditioned Variational Autoencoder for fMRI Distribution\n  Sampling and Removal of Confounds", "author": "Anton Orlichenko and Gang Qu and Ziyu Zhou and Anqi Liu and Hong-Wen Deng and Zhengming Ding and Julia M. Stephen and Tony W. Wilson and Vince D. Calhoun and Yu-Ping Wang", "abstract": "  Objective: fMRI and derived measures such as functional connectivity (FC)\nhave been used to predict brain age, general fluid intelligence, psychiatric\ndisease status, and preclinical neurodegenerative disease. However, it is not\nalways clear that all demographic confounds, such as age, sex, and race, have\nbeen removed from fMRI data. Additionally, many fMRI datasets are restricted to\nauthorized researchers, making dissemination of these valuable data sources\nchallenging. Methods: We create a variational autoencoder (VAE)-based model,\nDemoVAE, to decorrelate fMRI features from demographics and generate\nhigh-quality synthetic fMRI data based on user-supplied demographics. We train\nand validate our model using two large, widely used datasets, the Philadelphia\nNeurodevelopmental Cohort (PNC) and Bipolar and Schizophrenia Network for\nIntermediate Phenotypes (BSNIP). Results: We find that DemoVAE recapitulates\ngroup differences in fMRI data while capturing the full breadth of individual\nvariations. Significantly, we also find that most clinical and computerized\nbattery fields that are correlated with fMRI data are not correlated with\nDemoVAE latents. An exception are several fields related to schizophrenia\nmedication and symptom severity. Conclusion: Our model generates fMRI data that\ncaptures the full distribution of FC better than traditional VAE or GAN models.\nWe also find that most prediction using fMRI data is dependent on correlation\nwith, and prediction of, demographics. Significance: Our DemoVAE model allows\nfor generation of high quality synthetic data conditioned on subject\ndemographics as well as the removal of the confounding effects of demographics.\nWe identify that FC-based prediction tasks are highly influenced by demographic\nconfounds.\n", "link": "http://arxiv.org/abs/2405.07977v1", "date": "2024-05-13", "relevancy": 1.9011, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4888}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4732}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Demographic-Conditioned%20Variational%20Autoencoder%20for%20fMRI%20Distribution%0A%20%20Sampling%20and%20Removal%20of%20Confounds&body=Title%3A%20A%20Demographic-Conditioned%20Variational%20Autoencoder%20for%20fMRI%20Distribution%0A%20%20Sampling%20and%20Removal%20of%20Confounds%0AAuthor%3A%20Anton%20Orlichenko%20and%20Gang%20Qu%20and%20Ziyu%20Zhou%20and%20Anqi%20Liu%20and%20Hong-Wen%20Deng%20and%20Zhengming%20Ding%20and%20Julia%20M.%20Stephen%20and%20Tony%20W.%20Wilson%20and%20Vince%20D.%20Calhoun%20and%20Yu-Ping%20Wang%0AAbstract%3A%20%20%20Objective%3A%20fMRI%20and%20derived%20measures%20such%20as%20functional%20connectivity%20%28FC%29%0Ahave%20been%20used%20to%20predict%20brain%20age%2C%20general%20fluid%20intelligence%2C%20psychiatric%0Adisease%20status%2C%20and%20preclinical%20neurodegenerative%20disease.%20However%2C%20it%20is%20not%0Aalways%20clear%20that%20all%20demographic%20confounds%2C%20such%20as%20age%2C%20sex%2C%20and%20race%2C%20have%0Abeen%20removed%20from%20fMRI%20data.%20Additionally%2C%20many%20fMRI%20datasets%20are%20restricted%20to%0Aauthorized%20researchers%2C%20making%20dissemination%20of%20these%20valuable%20data%20sources%0Achallenging.%20Methods%3A%20We%20create%20a%20variational%20autoencoder%20%28VAE%29-based%20model%2C%0ADemoVAE%2C%20to%20decorrelate%20fMRI%20features%20from%20demographics%20and%20generate%0Ahigh-quality%20synthetic%20fMRI%20data%20based%20on%20user-supplied%20demographics.%20We%20train%0Aand%20validate%20our%20model%20using%20two%20large%2C%20widely%20used%20datasets%2C%20the%20Philadelphia%0ANeurodevelopmental%20Cohort%20%28PNC%29%20and%20Bipolar%20and%20Schizophrenia%20Network%20for%0AIntermediate%20Phenotypes%20%28BSNIP%29.%20Results%3A%20We%20find%20that%20DemoVAE%20recapitulates%0Agroup%20differences%20in%20fMRI%20data%20while%20capturing%20the%20full%20breadth%20of%20individual%0Avariations.%20Significantly%2C%20we%20also%20find%20that%20most%20clinical%20and%20computerized%0Abattery%20fields%20that%20are%20correlated%20with%20fMRI%20data%20are%20not%20correlated%20with%0ADemoVAE%20latents.%20An%20exception%20are%20several%20fields%20related%20to%20schizophrenia%0Amedication%20and%20symptom%20severity.%20Conclusion%3A%20Our%20model%20generates%20fMRI%20data%20that%0Acaptures%20the%20full%20distribution%20of%20FC%20better%20than%20traditional%20VAE%20or%20GAN%20models.%0AWe%20also%20find%20that%20most%20prediction%20using%20fMRI%20data%20is%20dependent%20on%20correlation%0Awith%2C%20and%20prediction%20of%2C%20demographics.%20Significance%3A%20Our%20DemoVAE%20model%20allows%0Afor%20generation%20of%20high%20quality%20synthetic%20data%20conditioned%20on%20subject%0Ademographics%20as%20well%20as%20the%20removal%20of%20the%20confounding%20effects%20of%20demographics.%0AWe%20identify%20that%20FC-based%20prediction%20tasks%20are%20highly%20influenced%20by%20demographic%0Aconfounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07977v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Demographic-Conditioned%2520Variational%2520Autoencoder%2520for%2520fMRI%2520Distribution%250A%2520%2520Sampling%2520and%2520Removal%2520of%2520Confounds%26entry.906535625%3DAnton%2520Orlichenko%2520and%2520Gang%2520Qu%2520and%2520Ziyu%2520Zhou%2520and%2520Anqi%2520Liu%2520and%2520Hong-Wen%2520Deng%2520and%2520Zhengming%2520Ding%2520and%2520Julia%2520M.%2520Stephen%2520and%2520Tony%2520W.%2520Wilson%2520and%2520Vince%2520D.%2520Calhoun%2520and%2520Yu-Ping%2520Wang%26entry.1292438233%3D%2520%2520Objective%253A%2520fMRI%2520and%2520derived%2520measures%2520such%2520as%2520functional%2520connectivity%2520%2528FC%2529%250Ahave%2520been%2520used%2520to%2520predict%2520brain%2520age%252C%2520general%2520fluid%2520intelligence%252C%2520psychiatric%250Adisease%2520status%252C%2520and%2520preclinical%2520neurodegenerative%2520disease.%2520However%252C%2520it%2520is%2520not%250Aalways%2520clear%2520that%2520all%2520demographic%2520confounds%252C%2520such%2520as%2520age%252C%2520sex%252C%2520and%2520race%252C%2520have%250Abeen%2520removed%2520from%2520fMRI%2520data.%2520Additionally%252C%2520many%2520fMRI%2520datasets%2520are%2520restricted%2520to%250Aauthorized%2520researchers%252C%2520making%2520dissemination%2520of%2520these%2520valuable%2520data%2520sources%250Achallenging.%2520Methods%253A%2520We%2520create%2520a%2520variational%2520autoencoder%2520%2528VAE%2529-based%2520model%252C%250ADemoVAE%252C%2520to%2520decorrelate%2520fMRI%2520features%2520from%2520demographics%2520and%2520generate%250Ahigh-quality%2520synthetic%2520fMRI%2520data%2520based%2520on%2520user-supplied%2520demographics.%2520We%2520train%250Aand%2520validate%2520our%2520model%2520using%2520two%2520large%252C%2520widely%2520used%2520datasets%252C%2520the%2520Philadelphia%250ANeurodevelopmental%2520Cohort%2520%2528PNC%2529%2520and%2520Bipolar%2520and%2520Schizophrenia%2520Network%2520for%250AIntermediate%2520Phenotypes%2520%2528BSNIP%2529.%2520Results%253A%2520We%2520find%2520that%2520DemoVAE%2520recapitulates%250Agroup%2520differences%2520in%2520fMRI%2520data%2520while%2520capturing%2520the%2520full%2520breadth%2520of%2520individual%250Avariations.%2520Significantly%252C%2520we%2520also%2520find%2520that%2520most%2520clinical%2520and%2520computerized%250Abattery%2520fields%2520that%2520are%2520correlated%2520with%2520fMRI%2520data%2520are%2520not%2520correlated%2520with%250ADemoVAE%2520latents.%2520An%2520exception%2520are%2520several%2520fields%2520related%2520to%2520schizophrenia%250Amedication%2520and%2520symptom%2520severity.%2520Conclusion%253A%2520Our%2520model%2520generates%2520fMRI%2520data%2520that%250Acaptures%2520the%2520full%2520distribution%2520of%2520FC%2520better%2520than%2520traditional%2520VAE%2520or%2520GAN%2520models.%250AWe%2520also%2520find%2520that%2520most%2520prediction%2520using%2520fMRI%2520data%2520is%2520dependent%2520on%2520correlation%250Awith%252C%2520and%2520prediction%2520of%252C%2520demographics.%2520Significance%253A%2520Our%2520DemoVAE%2520model%2520allows%250Afor%2520generation%2520of%2520high%2520quality%2520synthetic%2520data%2520conditioned%2520on%2520subject%250Ademographics%2520as%2520well%2520as%2520the%2520removal%2520of%2520the%2520confounding%2520effects%2520of%2520demographics.%250AWe%2520identify%2520that%2520FC-based%2520prediction%2520tasks%2520are%2520highly%2520influenced%2520by%2520demographic%250Aconfounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07977v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Demographic-Conditioned%20Variational%20Autoencoder%20for%20fMRI%20Distribution%0A%20%20Sampling%20and%20Removal%20of%20Confounds&entry.906535625=Anton%20Orlichenko%20and%20Gang%20Qu%20and%20Ziyu%20Zhou%20and%20Anqi%20Liu%20and%20Hong-Wen%20Deng%20and%20Zhengming%20Ding%20and%20Julia%20M.%20Stephen%20and%20Tony%20W.%20Wilson%20and%20Vince%20D.%20Calhoun%20and%20Yu-Ping%20Wang&entry.1292438233=%20%20Objective%3A%20fMRI%20and%20derived%20measures%20such%20as%20functional%20connectivity%20%28FC%29%0Ahave%20been%20used%20to%20predict%20brain%20age%2C%20general%20fluid%20intelligence%2C%20psychiatric%0Adisease%20status%2C%20and%20preclinical%20neurodegenerative%20disease.%20However%2C%20it%20is%20not%0Aalways%20clear%20that%20all%20demographic%20confounds%2C%20such%20as%20age%2C%20sex%2C%20and%20race%2C%20have%0Abeen%20removed%20from%20fMRI%20data.%20Additionally%2C%20many%20fMRI%20datasets%20are%20restricted%20to%0Aauthorized%20researchers%2C%20making%20dissemination%20of%20these%20valuable%20data%20sources%0Achallenging.%20Methods%3A%20We%20create%20a%20variational%20autoencoder%20%28VAE%29-based%20model%2C%0ADemoVAE%2C%20to%20decorrelate%20fMRI%20features%20from%20demographics%20and%20generate%0Ahigh-quality%20synthetic%20fMRI%20data%20based%20on%20user-supplied%20demographics.%20We%20train%0Aand%20validate%20our%20model%20using%20two%20large%2C%20widely%20used%20datasets%2C%20the%20Philadelphia%0ANeurodevelopmental%20Cohort%20%28PNC%29%20and%20Bipolar%20and%20Schizophrenia%20Network%20for%0AIntermediate%20Phenotypes%20%28BSNIP%29.%20Results%3A%20We%20find%20that%20DemoVAE%20recapitulates%0Agroup%20differences%20in%20fMRI%20data%20while%20capturing%20the%20full%20breadth%20of%20individual%0Avariations.%20Significantly%2C%20we%20also%20find%20that%20most%20clinical%20and%20computerized%0Abattery%20fields%20that%20are%20correlated%20with%20fMRI%20data%20are%20not%20correlated%20with%0ADemoVAE%20latents.%20An%20exception%20are%20several%20fields%20related%20to%20schizophrenia%0Amedication%20and%20symptom%20severity.%20Conclusion%3A%20Our%20model%20generates%20fMRI%20data%20that%0Acaptures%20the%20full%20distribution%20of%20FC%20better%20than%20traditional%20VAE%20or%20GAN%20models.%0AWe%20also%20find%20that%20most%20prediction%20using%20fMRI%20data%20is%20dependent%20on%20correlation%0Awith%2C%20and%20prediction%20of%2C%20demographics.%20Significance%3A%20Our%20DemoVAE%20model%20allows%0Afor%20generation%20of%20high%20quality%20synthetic%20data%20conditioned%20on%20subject%0Ademographics%20as%20well%20as%20the%20removal%20of%20the%20confounding%20effects%20of%20demographics.%0AWe%20identify%20that%20FC-based%20prediction%20tasks%20are%20highly%20influenced%20by%20demographic%0Aconfounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07977v1&entry.124074799=Read"},
{"title": "Forecasting with Hyper-Trees", "author": "Alexander M\u00e4rz and Kashif Rasul", "abstract": "  This paper introduces the concept of Hyper-Trees and offers a new direction\nin applying tree-based models to time series data. Unlike conventional\napplications of decision trees that forecast time series directly, Hyper-Trees\nare designed to learn the parameters of a target time series model. Our\nframework leverages the gradient-based nature of boosted trees, which allows us\nto extend the concept of Hyper-Networks to Hyper-Trees and to induce a\ntime-series inductive bias to tree models. By relating the parameters of a\ntarget time series model to features, Hyper-Trees address the challenge of\nparameter non-stationarity and enable tree-based forecasts to extend beyond\ntheir initial training range. With our research, we aim to explore the\neffectiveness of Hyper-Trees across various forecasting scenarios and to expand\nthe application of gradient boosted decision trees past their conventional use\nin time series forecasting.\n", "link": "http://arxiv.org/abs/2405.07836v1", "date": "2024-05-13", "relevancy": 1.892, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5372}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4441}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forecasting%20with%20Hyper-Trees&body=Title%3A%20Forecasting%20with%20Hyper-Trees%0AAuthor%3A%20Alexander%20M%C3%A4rz%20and%20Kashif%20Rasul%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20concept%20of%20Hyper-Trees%20and%20offers%20a%20new%20direction%0Ain%20applying%20tree-based%20models%20to%20time%20series%20data.%20Unlike%20conventional%0Aapplications%20of%20decision%20trees%20that%20forecast%20time%20series%20directly%2C%20Hyper-Trees%0Aare%20designed%20to%20learn%20the%20parameters%20of%20a%20target%20time%20series%20model.%20Our%0Aframework%20leverages%20the%20gradient-based%20nature%20of%20boosted%20trees%2C%20which%20allows%20us%0Ato%20extend%20the%20concept%20of%20Hyper-Networks%20to%20Hyper-Trees%20and%20to%20induce%20a%0Atime-series%20inductive%20bias%20to%20tree%20models.%20By%20relating%20the%20parameters%20of%20a%0Atarget%20time%20series%20model%20to%20features%2C%20Hyper-Trees%20address%20the%20challenge%20of%0Aparameter%20non-stationarity%20and%20enable%20tree-based%20forecasts%20to%20extend%20beyond%0Atheir%20initial%20training%20range.%20With%20our%20research%2C%20we%20aim%20to%20explore%20the%0Aeffectiveness%20of%20Hyper-Trees%20across%20various%20forecasting%20scenarios%20and%20to%20expand%0Athe%20application%20of%20gradient%20boosted%20decision%20trees%20past%20their%20conventional%20use%0Ain%20time%20series%20forecasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForecasting%2520with%2520Hyper-Trees%26entry.906535625%3DAlexander%2520M%25C3%25A4rz%2520and%2520Kashif%2520Rasul%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520concept%2520of%2520Hyper-Trees%2520and%2520offers%2520a%2520new%2520direction%250Ain%2520applying%2520tree-based%2520models%2520to%2520time%2520series%2520data.%2520Unlike%2520conventional%250Aapplications%2520of%2520decision%2520trees%2520that%2520forecast%2520time%2520series%2520directly%252C%2520Hyper-Trees%250Aare%2520designed%2520to%2520learn%2520the%2520parameters%2520of%2520a%2520target%2520time%2520series%2520model.%2520Our%250Aframework%2520leverages%2520the%2520gradient-based%2520nature%2520of%2520boosted%2520trees%252C%2520which%2520allows%2520us%250Ato%2520extend%2520the%2520concept%2520of%2520Hyper-Networks%2520to%2520Hyper-Trees%2520and%2520to%2520induce%2520a%250Atime-series%2520inductive%2520bias%2520to%2520tree%2520models.%2520By%2520relating%2520the%2520parameters%2520of%2520a%250Atarget%2520time%2520series%2520model%2520to%2520features%252C%2520Hyper-Trees%2520address%2520the%2520challenge%2520of%250Aparameter%2520non-stationarity%2520and%2520enable%2520tree-based%2520forecasts%2520to%2520extend%2520beyond%250Atheir%2520initial%2520training%2520range.%2520With%2520our%2520research%252C%2520we%2520aim%2520to%2520explore%2520the%250Aeffectiveness%2520of%2520Hyper-Trees%2520across%2520various%2520forecasting%2520scenarios%2520and%2520to%2520expand%250Athe%2520application%2520of%2520gradient%2520boosted%2520decision%2520trees%2520past%2520their%2520conventional%2520use%250Ain%2520time%2520series%2520forecasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forecasting%20with%20Hyper-Trees&entry.906535625=Alexander%20M%C3%A4rz%20and%20Kashif%20Rasul&entry.1292438233=%20%20This%20paper%20introduces%20the%20concept%20of%20Hyper-Trees%20and%20offers%20a%20new%20direction%0Ain%20applying%20tree-based%20models%20to%20time%20series%20data.%20Unlike%20conventional%0Aapplications%20of%20decision%20trees%20that%20forecast%20time%20series%20directly%2C%20Hyper-Trees%0Aare%20designed%20to%20learn%20the%20parameters%20of%20a%20target%20time%20series%20model.%20Our%0Aframework%20leverages%20the%20gradient-based%20nature%20of%20boosted%20trees%2C%20which%20allows%20us%0Ato%20extend%20the%20concept%20of%20Hyper-Networks%20to%20Hyper-Trees%20and%20to%20induce%20a%0Atime-series%20inductive%20bias%20to%20tree%20models.%20By%20relating%20the%20parameters%20of%20a%0Atarget%20time%20series%20model%20to%20features%2C%20Hyper-Trees%20address%20the%20challenge%20of%0Aparameter%20non-stationarity%20and%20enable%20tree-based%20forecasts%20to%20extend%20beyond%0Atheir%20initial%20training%20range.%20With%20our%20research%2C%20we%20aim%20to%20explore%20the%0Aeffectiveness%20of%20Hyper-Trees%20across%20various%20forecasting%20scenarios%20and%20to%20expand%0Athe%20application%20of%20gradient%20boosted%20decision%20trees%20past%20their%20conventional%20use%0Ain%20time%20series%20forecasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07836v1&entry.124074799=Read"},
{"title": "FastSAG: Towards Fast Non-Autoregressive Singing Accompaniment\n  Generation", "author": "Jianyi Chen and Wei Xue and Xu Tan and Zhen Ye and Qifeng Liu and Yike Guo", "abstract": "  Singing Accompaniment Generation (SAG), which generates instrumental music to\naccompany input vocals, is crucial to developing human-AI symbiotic art\ncreation systems. The state-of-the-art method, SingSong, utilizes a multi-stage\nautoregressive (AR) model for SAG, however, this method is extremely slow as it\ngenerates semantic and acoustic tokens recursively, and this makes it\nimpossible for real-time applications. In this paper, we aim to develop a Fast\nSAG method that can create high-quality and coherent accompaniments. A non-AR\ndiffusion-based framework is developed, which by carefully designing the\nconditions inferred from the vocal signals, generates the Mel spectrogram of\nthe target accompaniment directly. With diffusion and Mel spectrogram modeling,\nthe proposed method significantly simplifies the AR token-based SingSong\nframework, and largely accelerates the generation. We also design semantic\nprojection, prior projection blocks as well as a set of loss functions, to\nensure the generated accompaniment has semantic and rhythm coherence with the\nvocal signal. By intensive experimental studies, we demonstrate that the\nproposed method can generate better samples than SingSong, and accelerate the\ngeneration by at least 30 times. Audio samples and code are available at\nhttps://fastsag.github.io/.\n", "link": "http://arxiv.org/abs/2405.07682v1", "date": "2024-05-13", "relevancy": 1.8787, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4782}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.467}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastSAG%3A%20Towards%20Fast%20Non-Autoregressive%20Singing%20Accompaniment%0A%20%20Generation&body=Title%3A%20FastSAG%3A%20Towards%20Fast%20Non-Autoregressive%20Singing%20Accompaniment%0A%20%20Generation%0AAuthor%3A%20Jianyi%20Chen%20and%20Wei%20Xue%20and%20Xu%20Tan%20and%20Zhen%20Ye%20and%20Qifeng%20Liu%20and%20Yike%20Guo%0AAbstract%3A%20%20%20Singing%20Accompaniment%20Generation%20%28SAG%29%2C%20which%20generates%20instrumental%20music%20to%0Aaccompany%20input%20vocals%2C%20is%20crucial%20to%20developing%20human-AI%20symbiotic%20art%0Acreation%20systems.%20The%20state-of-the-art%20method%2C%20SingSong%2C%20utilizes%20a%20multi-stage%0Aautoregressive%20%28AR%29%20model%20for%20SAG%2C%20however%2C%20this%20method%20is%20extremely%20slow%20as%20it%0Agenerates%20semantic%20and%20acoustic%20tokens%20recursively%2C%20and%20this%20makes%20it%0Aimpossible%20for%20real-time%20applications.%20In%20this%20paper%2C%20we%20aim%20to%20develop%20a%20Fast%0ASAG%20method%20that%20can%20create%20high-quality%20and%20coherent%20accompaniments.%20A%20non-AR%0Adiffusion-based%20framework%20is%20developed%2C%20which%20by%20carefully%20designing%20the%0Aconditions%20inferred%20from%20the%20vocal%20signals%2C%20generates%20the%20Mel%20spectrogram%20of%0Athe%20target%20accompaniment%20directly.%20With%20diffusion%20and%20Mel%20spectrogram%20modeling%2C%0Athe%20proposed%20method%20significantly%20simplifies%20the%20AR%20token-based%20SingSong%0Aframework%2C%20and%20largely%20accelerates%20the%20generation.%20We%20also%20design%20semantic%0Aprojection%2C%20prior%20projection%20blocks%20as%20well%20as%20a%20set%20of%20loss%20functions%2C%20to%0Aensure%20the%20generated%20accompaniment%20has%20semantic%20and%20rhythm%20coherence%20with%20the%0Avocal%20signal.%20By%20intensive%20experimental%20studies%2C%20we%20demonstrate%20that%20the%0Aproposed%20method%20can%20generate%20better%20samples%20than%20SingSong%2C%20and%20accelerate%20the%0Ageneration%20by%20at%20least%2030%20times.%20Audio%20samples%20and%20code%20are%20available%20at%0Ahttps%3A//fastsag.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastSAG%253A%2520Towards%2520Fast%2520Non-Autoregressive%2520Singing%2520Accompaniment%250A%2520%2520Generation%26entry.906535625%3DJianyi%2520Chen%2520and%2520Wei%2520Xue%2520and%2520Xu%2520Tan%2520and%2520Zhen%2520Ye%2520and%2520Qifeng%2520Liu%2520and%2520Yike%2520Guo%26entry.1292438233%3D%2520%2520Singing%2520Accompaniment%2520Generation%2520%2528SAG%2529%252C%2520which%2520generates%2520instrumental%2520music%2520to%250Aaccompany%2520input%2520vocals%252C%2520is%2520crucial%2520to%2520developing%2520human-AI%2520symbiotic%2520art%250Acreation%2520systems.%2520The%2520state-of-the-art%2520method%252C%2520SingSong%252C%2520utilizes%2520a%2520multi-stage%250Aautoregressive%2520%2528AR%2529%2520model%2520for%2520SAG%252C%2520however%252C%2520this%2520method%2520is%2520extremely%2520slow%2520as%2520it%250Agenerates%2520semantic%2520and%2520acoustic%2520tokens%2520recursively%252C%2520and%2520this%2520makes%2520it%250Aimpossible%2520for%2520real-time%2520applications.%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520develop%2520a%2520Fast%250ASAG%2520method%2520that%2520can%2520create%2520high-quality%2520and%2520coherent%2520accompaniments.%2520A%2520non-AR%250Adiffusion-based%2520framework%2520is%2520developed%252C%2520which%2520by%2520carefully%2520designing%2520the%250Aconditions%2520inferred%2520from%2520the%2520vocal%2520signals%252C%2520generates%2520the%2520Mel%2520spectrogram%2520of%250Athe%2520target%2520accompaniment%2520directly.%2520With%2520diffusion%2520and%2520Mel%2520spectrogram%2520modeling%252C%250Athe%2520proposed%2520method%2520significantly%2520simplifies%2520the%2520AR%2520token-based%2520SingSong%250Aframework%252C%2520and%2520largely%2520accelerates%2520the%2520generation.%2520We%2520also%2520design%2520semantic%250Aprojection%252C%2520prior%2520projection%2520blocks%2520as%2520well%2520as%2520a%2520set%2520of%2520loss%2520functions%252C%2520to%250Aensure%2520the%2520generated%2520accompaniment%2520has%2520semantic%2520and%2520rhythm%2520coherence%2520with%2520the%250Avocal%2520signal.%2520By%2520intensive%2520experimental%2520studies%252C%2520we%2520demonstrate%2520that%2520the%250Aproposed%2520method%2520can%2520generate%2520better%2520samples%2520than%2520SingSong%252C%2520and%2520accelerate%2520the%250Ageneration%2520by%2520at%2520least%252030%2520times.%2520Audio%2520samples%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//fastsag.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastSAG%3A%20Towards%20Fast%20Non-Autoregressive%20Singing%20Accompaniment%0A%20%20Generation&entry.906535625=Jianyi%20Chen%20and%20Wei%20Xue%20and%20Xu%20Tan%20and%20Zhen%20Ye%20and%20Qifeng%20Liu%20and%20Yike%20Guo&entry.1292438233=%20%20Singing%20Accompaniment%20Generation%20%28SAG%29%2C%20which%20generates%20instrumental%20music%20to%0Aaccompany%20input%20vocals%2C%20is%20crucial%20to%20developing%20human-AI%20symbiotic%20art%0Acreation%20systems.%20The%20state-of-the-art%20method%2C%20SingSong%2C%20utilizes%20a%20multi-stage%0Aautoregressive%20%28AR%29%20model%20for%20SAG%2C%20however%2C%20this%20method%20is%20extremely%20slow%20as%20it%0Agenerates%20semantic%20and%20acoustic%20tokens%20recursively%2C%20and%20this%20makes%20it%0Aimpossible%20for%20real-time%20applications.%20In%20this%20paper%2C%20we%20aim%20to%20develop%20a%20Fast%0ASAG%20method%20that%20can%20create%20high-quality%20and%20coherent%20accompaniments.%20A%20non-AR%0Adiffusion-based%20framework%20is%20developed%2C%20which%20by%20carefully%20designing%20the%0Aconditions%20inferred%20from%20the%20vocal%20signals%2C%20generates%20the%20Mel%20spectrogram%20of%0Athe%20target%20accompaniment%20directly.%20With%20diffusion%20and%20Mel%20spectrogram%20modeling%2C%0Athe%20proposed%20method%20significantly%20simplifies%20the%20AR%20token-based%20SingSong%0Aframework%2C%20and%20largely%20accelerates%20the%20generation.%20We%20also%20design%20semantic%0Aprojection%2C%20prior%20projection%20blocks%20as%20well%20as%20a%20set%20of%20loss%20functions%2C%20to%0Aensure%20the%20generated%20accompaniment%20has%20semantic%20and%20rhythm%20coherence%20with%20the%0Avocal%20signal.%20By%20intensive%20experimental%20studies%2C%20we%20demonstrate%20that%20the%0Aproposed%20method%20can%20generate%20better%20samples%20than%20SingSong%2C%20and%20accelerate%20the%0Ageneration%20by%20at%20least%2030%20times.%20Audio%20samples%20and%20code%20are%20available%20at%0Ahttps%3A//fastsag.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07682v1&entry.124074799=Read"},
{"title": "Data Imputation by Pursuing Better Classification: A Supervised\n  Kernel-Based Method", "author": "Ruikai Yang and Fan He and Mingzhen He and Kaijie Wang and Xiaolin Huang", "abstract": "  Data imputation, the process of filling in missing feature elements for\nincomplete data sets, plays a crucial role in data-driven learning. A\nfundamental belief is that data imputation is helpful for learning performance,\nand it follows that the pursuit of better classification can guide the data\nimputation process. While some works consider using label information to assist\nin this task, their simplistic utilization of labels lacks flexibility and may\nrely on strict assumptions. In this paper, we propose a new framework that\neffectively leverages supervision information to complete missing data in a\nmanner conducive to classification. Specifically, this framework operates in\ntwo stages. Firstly, it leverages labels to supervise the optimization of\nsimilarity relationships among data, represented by the kernel matrix, with the\ngoal of enhancing classification accuracy. To mitigate overfitting that may\noccur during this process, a perturbation variable is introduced to improve the\nrobustness of the framework. Secondly, the learned kernel matrix serves as\nadditional supervision information to guide data imputation through regression,\nutilizing the block coordinate descent method. The superiority of the proposed\nmethod is evaluated on four real-world data sets by comparing it with\nstate-of-the-art imputation methods. Remarkably, our algorithm significantly\noutperforms other methods when the data is missing more than 60\\% of the\nfeatures\n", "link": "http://arxiv.org/abs/2405.07800v1", "date": "2024-05-13", "relevancy": 1.8754, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4779}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4706}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Imputation%20by%20Pursuing%20Better%20Classification%3A%20A%20Supervised%0A%20%20Kernel-Based%20Method&body=Title%3A%20Data%20Imputation%20by%20Pursuing%20Better%20Classification%3A%20A%20Supervised%0A%20%20Kernel-Based%20Method%0AAuthor%3A%20Ruikai%20Yang%20and%20Fan%20He%20and%20Mingzhen%20He%20and%20Kaijie%20Wang%20and%20Xiaolin%20Huang%0AAbstract%3A%20%20%20Data%20imputation%2C%20the%20process%20of%20filling%20in%20missing%20feature%20elements%20for%0Aincomplete%20data%20sets%2C%20plays%20a%20crucial%20role%20in%20data-driven%20learning.%20A%0Afundamental%20belief%20is%20that%20data%20imputation%20is%20helpful%20for%20learning%20performance%2C%0Aand%20it%20follows%20that%20the%20pursuit%20of%20better%20classification%20can%20guide%20the%20data%0Aimputation%20process.%20While%20some%20works%20consider%20using%20label%20information%20to%20assist%0Ain%20this%20task%2C%20their%20simplistic%20utilization%20of%20labels%20lacks%20flexibility%20and%20may%0Arely%20on%20strict%20assumptions.%20In%20this%20paper%2C%20we%20propose%20a%20new%20framework%20that%0Aeffectively%20leverages%20supervision%20information%20to%20complete%20missing%20data%20in%20a%0Amanner%20conducive%20to%20classification.%20Specifically%2C%20this%20framework%20operates%20in%0Atwo%20stages.%20Firstly%2C%20it%20leverages%20labels%20to%20supervise%20the%20optimization%20of%0Asimilarity%20relationships%20among%20data%2C%20represented%20by%20the%20kernel%20matrix%2C%20with%20the%0Agoal%20of%20enhancing%20classification%20accuracy.%20To%20mitigate%20overfitting%20that%20may%0Aoccur%20during%20this%20process%2C%20a%20perturbation%20variable%20is%20introduced%20to%20improve%20the%0Arobustness%20of%20the%20framework.%20Secondly%2C%20the%20learned%20kernel%20matrix%20serves%20as%0Aadditional%20supervision%20information%20to%20guide%20data%20imputation%20through%20regression%2C%0Autilizing%20the%20block%20coordinate%20descent%20method.%20The%20superiority%20of%20the%20proposed%0Amethod%20is%20evaluated%20on%20four%20real-world%20data%20sets%20by%20comparing%20it%20with%0Astate-of-the-art%20imputation%20methods.%20Remarkably%2C%20our%20algorithm%20significantly%0Aoutperforms%20other%20methods%20when%20the%20data%20is%20missing%20more%20than%2060%5C%25%20of%20the%0Afeatures%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Imputation%2520by%2520Pursuing%2520Better%2520Classification%253A%2520A%2520Supervised%250A%2520%2520Kernel-Based%2520Method%26entry.906535625%3DRuikai%2520Yang%2520and%2520Fan%2520He%2520and%2520Mingzhen%2520He%2520and%2520Kaijie%2520Wang%2520and%2520Xiaolin%2520Huang%26entry.1292438233%3D%2520%2520Data%2520imputation%252C%2520the%2520process%2520of%2520filling%2520in%2520missing%2520feature%2520elements%2520for%250Aincomplete%2520data%2520sets%252C%2520plays%2520a%2520crucial%2520role%2520in%2520data-driven%2520learning.%2520A%250Afundamental%2520belief%2520is%2520that%2520data%2520imputation%2520is%2520helpful%2520for%2520learning%2520performance%252C%250Aand%2520it%2520follows%2520that%2520the%2520pursuit%2520of%2520better%2520classification%2520can%2520guide%2520the%2520data%250Aimputation%2520process.%2520While%2520some%2520works%2520consider%2520using%2520label%2520information%2520to%2520assist%250Ain%2520this%2520task%252C%2520their%2520simplistic%2520utilization%2520of%2520labels%2520lacks%2520flexibility%2520and%2520may%250Arely%2520on%2520strict%2520assumptions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520framework%2520that%250Aeffectively%2520leverages%2520supervision%2520information%2520to%2520complete%2520missing%2520data%2520in%2520a%250Amanner%2520conducive%2520to%2520classification.%2520Specifically%252C%2520this%2520framework%2520operates%2520in%250Atwo%2520stages.%2520Firstly%252C%2520it%2520leverages%2520labels%2520to%2520supervise%2520the%2520optimization%2520of%250Asimilarity%2520relationships%2520among%2520data%252C%2520represented%2520by%2520the%2520kernel%2520matrix%252C%2520with%2520the%250Agoal%2520of%2520enhancing%2520classification%2520accuracy.%2520To%2520mitigate%2520overfitting%2520that%2520may%250Aoccur%2520during%2520this%2520process%252C%2520a%2520perturbation%2520variable%2520is%2520introduced%2520to%2520improve%2520the%250Arobustness%2520of%2520the%2520framework.%2520Secondly%252C%2520the%2520learned%2520kernel%2520matrix%2520serves%2520as%250Aadditional%2520supervision%2520information%2520to%2520guide%2520data%2520imputation%2520through%2520regression%252C%250Autilizing%2520the%2520block%2520coordinate%2520descent%2520method.%2520The%2520superiority%2520of%2520the%2520proposed%250Amethod%2520is%2520evaluated%2520on%2520four%2520real-world%2520data%2520sets%2520by%2520comparing%2520it%2520with%250Astate-of-the-art%2520imputation%2520methods.%2520Remarkably%252C%2520our%2520algorithm%2520significantly%250Aoutperforms%2520other%2520methods%2520when%2520the%2520data%2520is%2520missing%2520more%2520than%252060%255C%2525%2520of%2520the%250Afeatures%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Imputation%20by%20Pursuing%20Better%20Classification%3A%20A%20Supervised%0A%20%20Kernel-Based%20Method&entry.906535625=Ruikai%20Yang%20and%20Fan%20He%20and%20Mingzhen%20He%20and%20Kaijie%20Wang%20and%20Xiaolin%20Huang&entry.1292438233=%20%20Data%20imputation%2C%20the%20process%20of%20filling%20in%20missing%20feature%20elements%20for%0Aincomplete%20data%20sets%2C%20plays%20a%20crucial%20role%20in%20data-driven%20learning.%20A%0Afundamental%20belief%20is%20that%20data%20imputation%20is%20helpful%20for%20learning%20performance%2C%0Aand%20it%20follows%20that%20the%20pursuit%20of%20better%20classification%20can%20guide%20the%20data%0Aimputation%20process.%20While%20some%20works%20consider%20using%20label%20information%20to%20assist%0Ain%20this%20task%2C%20their%20simplistic%20utilization%20of%20labels%20lacks%20flexibility%20and%20may%0Arely%20on%20strict%20assumptions.%20In%20this%20paper%2C%20we%20propose%20a%20new%20framework%20that%0Aeffectively%20leverages%20supervision%20information%20to%20complete%20missing%20data%20in%20a%0Amanner%20conducive%20to%20classification.%20Specifically%2C%20this%20framework%20operates%20in%0Atwo%20stages.%20Firstly%2C%20it%20leverages%20labels%20to%20supervise%20the%20optimization%20of%0Asimilarity%20relationships%20among%20data%2C%20represented%20by%20the%20kernel%20matrix%2C%20with%20the%0Agoal%20of%20enhancing%20classification%20accuracy.%20To%20mitigate%20overfitting%20that%20may%0Aoccur%20during%20this%20process%2C%20a%20perturbation%20variable%20is%20introduced%20to%20improve%20the%0Arobustness%20of%20the%20framework.%20Secondly%2C%20the%20learned%20kernel%20matrix%20serves%20as%0Aadditional%20supervision%20information%20to%20guide%20data%20imputation%20through%20regression%2C%0Autilizing%20the%20block%20coordinate%20descent%20method.%20The%20superiority%20of%20the%20proposed%0Amethod%20is%20evaluated%20on%20four%20real-world%20data%20sets%20by%20comparing%20it%20with%0Astate-of-the-art%20imputation%20methods.%20Remarkably%2C%20our%20algorithm%20significantly%0Aoutperforms%20other%20methods%20when%20the%20data%20is%20missing%20more%20than%2060%5C%25%20of%20the%0Afeatures%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07800v1&entry.124074799=Read"},
{"title": "Optimal Matrix Sketching over Sliding Windows", "author": "Hanyan Yin and Dongxie Wen and Jiajun Li and Zhewei Wei and Xiao Zhang and Zengfeng Huang and Feifei Li", "abstract": "  Matrix sketching, aimed at approximating a matrix $\\boldsymbol{A} \\in\n\\mathbb{R}^{N\\times d}$ consisting of vector streams of length $N$ with a\nsmaller sketching matrix $\\boldsymbol{B} \\in \\mathbb{R}^{\\ell\\times d}, \\ell\n\\ll N$, has garnered increasing attention in fields such as large-scale data\nanalytics and machine learning. A well-known deterministic matrix sketching\nmethod is the Frequent Directions algorithm, which achieves the optimal\n$O\\left(\\frac{d}{\\varepsilon}\\right)$ space bound and provides a covariance\nerror guarantee of $\\varepsilon = \\lVert \\boldsymbol{A}^\\top \\boldsymbol{A} -\n\\boldsymbol{B}^\\top \\boldsymbol{B} \\rVert_2/\\lVert \\boldsymbol{A} \\rVert_F^2$.\nThe matrix sketching problem becomes particularly interesting in the context of\nsliding windows, where the goal is to approximate the matrix\n$\\boldsymbol{A}_W$, formed by input vectors over the most recent $N$ time\nunits. However, despite recent efforts, whether achieving the optimal\n$O\\left(\\frac{d}{\\varepsilon}\\right)$ space bound on sliding windows is\npossible has remained an open question.\n  In this paper, we introduce the DS-FD algorithm, which achieves the optimal\n$O\\left(\\frac{d}{\\varepsilon}\\right)$ space bound for matrix sketching over\nrow-normalized, sequence-based sliding windows. We also present matching upper\nand lower space bounds for time-based and unnormalized sliding windows,\ndemonstrating the generality and optimality of \\dsfd across various sliding\nwindow models. This conclusively answers the open question regarding the\noptimal space bound for matrix sketching over sliding windows. Furthermore, we\nconduct extensive experiments with both synthetic and real-world datasets,\nvalidating our theoretical claims and thus confirming the correctness and\neffectiveness of our algorithm, both theoretically and empirically.\n", "link": "http://arxiv.org/abs/2405.07792v1", "date": "2024-05-13", "relevancy": 1.8626, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4779}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4662}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Matrix%20Sketching%20over%20Sliding%20Windows&body=Title%3A%20Optimal%20Matrix%20Sketching%20over%20Sliding%20Windows%0AAuthor%3A%20Hanyan%20Yin%20and%20Dongxie%20Wen%20and%20Jiajun%20Li%20and%20Zhewei%20Wei%20and%20Xiao%20Zhang%20and%20Zengfeng%20Huang%20and%20Feifei%20Li%0AAbstract%3A%20%20%20Matrix%20sketching%2C%20aimed%20at%20approximating%20a%20matrix%20%24%5Cboldsymbol%7BA%7D%20%5Cin%0A%5Cmathbb%7BR%7D%5E%7BN%5Ctimes%20d%7D%24%20consisting%20of%20vector%20streams%20of%20length%20%24N%24%20with%20a%0Asmaller%20sketching%20matrix%20%24%5Cboldsymbol%7BB%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7B%5Cell%5Ctimes%20d%7D%2C%20%5Cell%0A%5Cll%20N%24%2C%20has%20garnered%20increasing%20attention%20in%20fields%20such%20as%20large-scale%20data%0Aanalytics%20and%20machine%20learning.%20A%20well-known%20deterministic%20matrix%20sketching%0Amethod%20is%20the%20Frequent%20Directions%20algorithm%2C%20which%20achieves%20the%20optimal%0A%24O%5Cleft%28%5Cfrac%7Bd%7D%7B%5Cvarepsilon%7D%5Cright%29%24%20space%20bound%20and%20provides%20a%20covariance%0Aerror%20guarantee%20of%20%24%5Cvarepsilon%20%3D%20%5ClVert%20%5Cboldsymbol%7BA%7D%5E%5Ctop%20%5Cboldsymbol%7BA%7D%20-%0A%5Cboldsymbol%7BB%7D%5E%5Ctop%20%5Cboldsymbol%7BB%7D%20%5CrVert_2/%5ClVert%20%5Cboldsymbol%7BA%7D%20%5CrVert_F%5E2%24.%0AThe%20matrix%20sketching%20problem%20becomes%20particularly%20interesting%20in%20the%20context%20of%0Asliding%20windows%2C%20where%20the%20goal%20is%20to%20approximate%20the%20matrix%0A%24%5Cboldsymbol%7BA%7D_W%24%2C%20formed%20by%20input%20vectors%20over%20the%20most%20recent%20%24N%24%20time%0Aunits.%20However%2C%20despite%20recent%20efforts%2C%20whether%20achieving%20the%20optimal%0A%24O%5Cleft%28%5Cfrac%7Bd%7D%7B%5Cvarepsilon%7D%5Cright%29%24%20space%20bound%20on%20sliding%20windows%20is%0Apossible%20has%20remained%20an%20open%20question.%0A%20%20In%20this%20paper%2C%20we%20introduce%20the%20DS-FD%20algorithm%2C%20which%20achieves%20the%20optimal%0A%24O%5Cleft%28%5Cfrac%7Bd%7D%7B%5Cvarepsilon%7D%5Cright%29%24%20space%20bound%20for%20matrix%20sketching%20over%0Arow-normalized%2C%20sequence-based%20sliding%20windows.%20We%20also%20present%20matching%20upper%0Aand%20lower%20space%20bounds%20for%20time-based%20and%20unnormalized%20sliding%20windows%2C%0Ademonstrating%20the%20generality%20and%20optimality%20of%20%5Cdsfd%20across%20various%20sliding%0Awindow%20models.%20This%20conclusively%20answers%20the%20open%20question%20regarding%20the%0Aoptimal%20space%20bound%20for%20matrix%20sketching%20over%20sliding%20windows.%20Furthermore%2C%20we%0Aconduct%20extensive%20experiments%20with%20both%20synthetic%20and%20real-world%20datasets%2C%0Avalidating%20our%20theoretical%20claims%20and%20thus%20confirming%20the%20correctness%20and%0Aeffectiveness%20of%20our%20algorithm%2C%20both%20theoretically%20and%20empirically.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Matrix%2520Sketching%2520over%2520Sliding%2520Windows%26entry.906535625%3DHanyan%2520Yin%2520and%2520Dongxie%2520Wen%2520and%2520Jiajun%2520Li%2520and%2520Zhewei%2520Wei%2520and%2520Xiao%2520Zhang%2520and%2520Zengfeng%2520Huang%2520and%2520Feifei%2520Li%26entry.1292438233%3D%2520%2520Matrix%2520sketching%252C%2520aimed%2520at%2520approximating%2520a%2520matrix%2520%2524%255Cboldsymbol%257BA%257D%2520%255Cin%250A%255Cmathbb%257BR%257D%255E%257BN%255Ctimes%2520d%257D%2524%2520consisting%2520of%2520vector%2520streams%2520of%2520length%2520%2524N%2524%2520with%2520a%250Asmaller%2520sketching%2520matrix%2520%2524%255Cboldsymbol%257BB%257D%2520%255Cin%2520%255Cmathbb%257BR%257D%255E%257B%255Cell%255Ctimes%2520d%257D%252C%2520%255Cell%250A%255Cll%2520N%2524%252C%2520has%2520garnered%2520increasing%2520attention%2520in%2520fields%2520such%2520as%2520large-scale%2520data%250Aanalytics%2520and%2520machine%2520learning.%2520A%2520well-known%2520deterministic%2520matrix%2520sketching%250Amethod%2520is%2520the%2520Frequent%2520Directions%2520algorithm%252C%2520which%2520achieves%2520the%2520optimal%250A%2524O%255Cleft%2528%255Cfrac%257Bd%257D%257B%255Cvarepsilon%257D%255Cright%2529%2524%2520space%2520bound%2520and%2520provides%2520a%2520covariance%250Aerror%2520guarantee%2520of%2520%2524%255Cvarepsilon%2520%253D%2520%255ClVert%2520%255Cboldsymbol%257BA%257D%255E%255Ctop%2520%255Cboldsymbol%257BA%257D%2520-%250A%255Cboldsymbol%257BB%257D%255E%255Ctop%2520%255Cboldsymbol%257BB%257D%2520%255CrVert_2/%255ClVert%2520%255Cboldsymbol%257BA%257D%2520%255CrVert_F%255E2%2524.%250AThe%2520matrix%2520sketching%2520problem%2520becomes%2520particularly%2520interesting%2520in%2520the%2520context%2520of%250Asliding%2520windows%252C%2520where%2520the%2520goal%2520is%2520to%2520approximate%2520the%2520matrix%250A%2524%255Cboldsymbol%257BA%257D_W%2524%252C%2520formed%2520by%2520input%2520vectors%2520over%2520the%2520most%2520recent%2520%2524N%2524%2520time%250Aunits.%2520However%252C%2520despite%2520recent%2520efforts%252C%2520whether%2520achieving%2520the%2520optimal%250A%2524O%255Cleft%2528%255Cfrac%257Bd%257D%257B%255Cvarepsilon%257D%255Cright%2529%2524%2520space%2520bound%2520on%2520sliding%2520windows%2520is%250Apossible%2520has%2520remained%2520an%2520open%2520question.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520DS-FD%2520algorithm%252C%2520which%2520achieves%2520the%2520optimal%250A%2524O%255Cleft%2528%255Cfrac%257Bd%257D%257B%255Cvarepsilon%257D%255Cright%2529%2524%2520space%2520bound%2520for%2520matrix%2520sketching%2520over%250Arow-normalized%252C%2520sequence-based%2520sliding%2520windows.%2520We%2520also%2520present%2520matching%2520upper%250Aand%2520lower%2520space%2520bounds%2520for%2520time-based%2520and%2520unnormalized%2520sliding%2520windows%252C%250Ademonstrating%2520the%2520generality%2520and%2520optimality%2520of%2520%255Cdsfd%2520across%2520various%2520sliding%250Awindow%2520models.%2520This%2520conclusively%2520answers%2520the%2520open%2520question%2520regarding%2520the%250Aoptimal%2520space%2520bound%2520for%2520matrix%2520sketching%2520over%2520sliding%2520windows.%2520Furthermore%252C%2520we%250Aconduct%2520extensive%2520experiments%2520with%2520both%2520synthetic%2520and%2520real-world%2520datasets%252C%250Avalidating%2520our%2520theoretical%2520claims%2520and%2520thus%2520confirming%2520the%2520correctness%2520and%250Aeffectiveness%2520of%2520our%2520algorithm%252C%2520both%2520theoretically%2520and%2520empirically.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Matrix%20Sketching%20over%20Sliding%20Windows&entry.906535625=Hanyan%20Yin%20and%20Dongxie%20Wen%20and%20Jiajun%20Li%20and%20Zhewei%20Wei%20and%20Xiao%20Zhang%20and%20Zengfeng%20Huang%20and%20Feifei%20Li&entry.1292438233=%20%20Matrix%20sketching%2C%20aimed%20at%20approximating%20a%20matrix%20%24%5Cboldsymbol%7BA%7D%20%5Cin%0A%5Cmathbb%7BR%7D%5E%7BN%5Ctimes%20d%7D%24%20consisting%20of%20vector%20streams%20of%20length%20%24N%24%20with%20a%0Asmaller%20sketching%20matrix%20%24%5Cboldsymbol%7BB%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7B%5Cell%5Ctimes%20d%7D%2C%20%5Cell%0A%5Cll%20N%24%2C%20has%20garnered%20increasing%20attention%20in%20fields%20such%20as%20large-scale%20data%0Aanalytics%20and%20machine%20learning.%20A%20well-known%20deterministic%20matrix%20sketching%0Amethod%20is%20the%20Frequent%20Directions%20algorithm%2C%20which%20achieves%20the%20optimal%0A%24O%5Cleft%28%5Cfrac%7Bd%7D%7B%5Cvarepsilon%7D%5Cright%29%24%20space%20bound%20and%20provides%20a%20covariance%0Aerror%20guarantee%20of%20%24%5Cvarepsilon%20%3D%20%5ClVert%20%5Cboldsymbol%7BA%7D%5E%5Ctop%20%5Cboldsymbol%7BA%7D%20-%0A%5Cboldsymbol%7BB%7D%5E%5Ctop%20%5Cboldsymbol%7BB%7D%20%5CrVert_2/%5ClVert%20%5Cboldsymbol%7BA%7D%20%5CrVert_F%5E2%24.%0AThe%20matrix%20sketching%20problem%20becomes%20particularly%20interesting%20in%20the%20context%20of%0Asliding%20windows%2C%20where%20the%20goal%20is%20to%20approximate%20the%20matrix%0A%24%5Cboldsymbol%7BA%7D_W%24%2C%20formed%20by%20input%20vectors%20over%20the%20most%20recent%20%24N%24%20time%0Aunits.%20However%2C%20despite%20recent%20efforts%2C%20whether%20achieving%20the%20optimal%0A%24O%5Cleft%28%5Cfrac%7Bd%7D%7B%5Cvarepsilon%7D%5Cright%29%24%20space%20bound%20on%20sliding%20windows%20is%0Apossible%20has%20remained%20an%20open%20question.%0A%20%20In%20this%20paper%2C%20we%20introduce%20the%20DS-FD%20algorithm%2C%20which%20achieves%20the%20optimal%0A%24O%5Cleft%28%5Cfrac%7Bd%7D%7B%5Cvarepsilon%7D%5Cright%29%24%20space%20bound%20for%20matrix%20sketching%20over%0Arow-normalized%2C%20sequence-based%20sliding%20windows.%20We%20also%20present%20matching%20upper%0Aand%20lower%20space%20bounds%20for%20time-based%20and%20unnormalized%20sliding%20windows%2C%0Ademonstrating%20the%20generality%20and%20optimality%20of%20%5Cdsfd%20across%20various%20sliding%0Awindow%20models.%20This%20conclusively%20answers%20the%20open%20question%20regarding%20the%0Aoptimal%20space%20bound%20for%20matrix%20sketching%20over%20sliding%20windows.%20Furthermore%2C%20we%0Aconduct%20extensive%20experiments%20with%20both%20synthetic%20and%20real-world%20datasets%2C%0Avalidating%20our%20theoretical%20claims%20and%20thus%20confirming%20the%20correctness%20and%0Aeffectiveness%20of%20our%20algorithm%2C%20both%20theoretically%20and%20empirically.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07792v1&entry.124074799=Read"},
{"title": "Look Once to Hear: Target Speech Hearing with Noisy Examples", "author": "Bandhav Veluri and Malek Itani and Tuochao Chen and Takuya Yoshioka and Shyamnath Gollakota", "abstract": "  In crowded settings, the human brain can focus on speech from a target\nspeaker, given prior knowledge of how they sound. We introduce a novel\nintelligent hearable system that achieves this capability, enabling target\nspeech hearing to ignore all interfering speech and noise, but the target\nspeaker. A naive approach is to require a clean speech example to enroll the\ntarget speaker. This is however not well aligned with the hearable application\ndomain since obtaining a clean example is challenging in real world scenarios,\ncreating a unique user interface problem. We present the first enrollment\ninterface where the wearer looks at the target speaker for a few seconds to\ncapture a single, short, highly noisy, binaural example of the target speaker.\nThis noisy example is used for enrollment and subsequent speech extraction in\nthe presence of interfering speakers and noise. Our system achieves a signal\nquality improvement of 7.01 dB using less than 5 seconds of noisy enrollment\naudio and can process 8 ms of audio chunks in 6.24 ms on an embedded CPU. Our\nuser studies demonstrate generalization to real-world static and mobile\nspeakers in previously unseen indoor and outdoor multipath environments.\nFinally, our enrollment interface for noisy examples does not cause performance\ndegradation compared to clean examples, while being convenient and\nuser-friendly. Taking a step back, this paper takes an important step towards\nenhancing the human auditory perception with artificial intelligence. We\nprovide code and data at: https://github.com/vb000/LookOnceToHear.\n", "link": "http://arxiv.org/abs/2405.06289v2", "date": "2024-05-13", "relevancy": 1.8616, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4738}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4608}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Look%20Once%20to%20Hear%3A%20Target%20Speech%20Hearing%20with%20Noisy%20Examples&body=Title%3A%20Look%20Once%20to%20Hear%3A%20Target%20Speech%20Hearing%20with%20Noisy%20Examples%0AAuthor%3A%20Bandhav%20Veluri%20and%20Malek%20Itani%20and%20Tuochao%20Chen%20and%20Takuya%20Yoshioka%20and%20Shyamnath%20Gollakota%0AAbstract%3A%20%20%20In%20crowded%20settings%2C%20the%20human%20brain%20can%20focus%20on%20speech%20from%20a%20target%0Aspeaker%2C%20given%20prior%20knowledge%20of%20how%20they%20sound.%20We%20introduce%20a%20novel%0Aintelligent%20hearable%20system%20that%20achieves%20this%20capability%2C%20enabling%20target%0Aspeech%20hearing%20to%20ignore%20all%20interfering%20speech%20and%20noise%2C%20but%20the%20target%0Aspeaker.%20A%20naive%20approach%20is%20to%20require%20a%20clean%20speech%20example%20to%20enroll%20the%0Atarget%20speaker.%20This%20is%20however%20not%20well%20aligned%20with%20the%20hearable%20application%0Adomain%20since%20obtaining%20a%20clean%20example%20is%20challenging%20in%20real%20world%20scenarios%2C%0Acreating%20a%20unique%20user%20interface%20problem.%20We%20present%20the%20first%20enrollment%0Ainterface%20where%20the%20wearer%20looks%20at%20the%20target%20speaker%20for%20a%20few%20seconds%20to%0Acapture%20a%20single%2C%20short%2C%20highly%20noisy%2C%20binaural%20example%20of%20the%20target%20speaker.%0AThis%20noisy%20example%20is%20used%20for%20enrollment%20and%20subsequent%20speech%20extraction%20in%0Athe%20presence%20of%20interfering%20speakers%20and%20noise.%20Our%20system%20achieves%20a%20signal%0Aquality%20improvement%20of%207.01%20dB%20using%20less%20than%205%20seconds%20of%20noisy%20enrollment%0Aaudio%20and%20can%20process%208%20ms%20of%20audio%20chunks%20in%206.24%20ms%20on%20an%20embedded%20CPU.%20Our%0Auser%20studies%20demonstrate%20generalization%20to%20real-world%20static%20and%20mobile%0Aspeakers%20in%20previously%20unseen%20indoor%20and%20outdoor%20multipath%20environments.%0AFinally%2C%20our%20enrollment%20interface%20for%20noisy%20examples%20does%20not%20cause%20performance%0Adegradation%20compared%20to%20clean%20examples%2C%20while%20being%20convenient%20and%0Auser-friendly.%20Taking%20a%20step%20back%2C%20this%20paper%20takes%20an%20important%20step%20towards%0Aenhancing%20the%20human%20auditory%20perception%20with%20artificial%20intelligence.%20We%0Aprovide%20code%20and%20data%20at%3A%20https%3A//github.com/vb000/LookOnceToHear.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06289v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLook%2520Once%2520to%2520Hear%253A%2520Target%2520Speech%2520Hearing%2520with%2520Noisy%2520Examples%26entry.906535625%3DBandhav%2520Veluri%2520and%2520Malek%2520Itani%2520and%2520Tuochao%2520Chen%2520and%2520Takuya%2520Yoshioka%2520and%2520Shyamnath%2520Gollakota%26entry.1292438233%3D%2520%2520In%2520crowded%2520settings%252C%2520the%2520human%2520brain%2520can%2520focus%2520on%2520speech%2520from%2520a%2520target%250Aspeaker%252C%2520given%2520prior%2520knowledge%2520of%2520how%2520they%2520sound.%2520We%2520introduce%2520a%2520novel%250Aintelligent%2520hearable%2520system%2520that%2520achieves%2520this%2520capability%252C%2520enabling%2520target%250Aspeech%2520hearing%2520to%2520ignore%2520all%2520interfering%2520speech%2520and%2520noise%252C%2520but%2520the%2520target%250Aspeaker.%2520A%2520naive%2520approach%2520is%2520to%2520require%2520a%2520clean%2520speech%2520example%2520to%2520enroll%2520the%250Atarget%2520speaker.%2520This%2520is%2520however%2520not%2520well%2520aligned%2520with%2520the%2520hearable%2520application%250Adomain%2520since%2520obtaining%2520a%2520clean%2520example%2520is%2520challenging%2520in%2520real%2520world%2520scenarios%252C%250Acreating%2520a%2520unique%2520user%2520interface%2520problem.%2520We%2520present%2520the%2520first%2520enrollment%250Ainterface%2520where%2520the%2520wearer%2520looks%2520at%2520the%2520target%2520speaker%2520for%2520a%2520few%2520seconds%2520to%250Acapture%2520a%2520single%252C%2520short%252C%2520highly%2520noisy%252C%2520binaural%2520example%2520of%2520the%2520target%2520speaker.%250AThis%2520noisy%2520example%2520is%2520used%2520for%2520enrollment%2520and%2520subsequent%2520speech%2520extraction%2520in%250Athe%2520presence%2520of%2520interfering%2520speakers%2520and%2520noise.%2520Our%2520system%2520achieves%2520a%2520signal%250Aquality%2520improvement%2520of%25207.01%2520dB%2520using%2520less%2520than%25205%2520seconds%2520of%2520noisy%2520enrollment%250Aaudio%2520and%2520can%2520process%25208%2520ms%2520of%2520audio%2520chunks%2520in%25206.24%2520ms%2520on%2520an%2520embedded%2520CPU.%2520Our%250Auser%2520studies%2520demonstrate%2520generalization%2520to%2520real-world%2520static%2520and%2520mobile%250Aspeakers%2520in%2520previously%2520unseen%2520indoor%2520and%2520outdoor%2520multipath%2520environments.%250AFinally%252C%2520our%2520enrollment%2520interface%2520for%2520noisy%2520examples%2520does%2520not%2520cause%2520performance%250Adegradation%2520compared%2520to%2520clean%2520examples%252C%2520while%2520being%2520convenient%2520and%250Auser-friendly.%2520Taking%2520a%2520step%2520back%252C%2520this%2520paper%2520takes%2520an%2520important%2520step%2520towards%250Aenhancing%2520the%2520human%2520auditory%2520perception%2520with%2520artificial%2520intelligence.%2520We%250Aprovide%2520code%2520and%2520data%2520at%253A%2520https%253A//github.com/vb000/LookOnceToHear.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06289v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Look%20Once%20to%20Hear%3A%20Target%20Speech%20Hearing%20with%20Noisy%20Examples&entry.906535625=Bandhav%20Veluri%20and%20Malek%20Itani%20and%20Tuochao%20Chen%20and%20Takuya%20Yoshioka%20and%20Shyamnath%20Gollakota&entry.1292438233=%20%20In%20crowded%20settings%2C%20the%20human%20brain%20can%20focus%20on%20speech%20from%20a%20target%0Aspeaker%2C%20given%20prior%20knowledge%20of%20how%20they%20sound.%20We%20introduce%20a%20novel%0Aintelligent%20hearable%20system%20that%20achieves%20this%20capability%2C%20enabling%20target%0Aspeech%20hearing%20to%20ignore%20all%20interfering%20speech%20and%20noise%2C%20but%20the%20target%0Aspeaker.%20A%20naive%20approach%20is%20to%20require%20a%20clean%20speech%20example%20to%20enroll%20the%0Atarget%20speaker.%20This%20is%20however%20not%20well%20aligned%20with%20the%20hearable%20application%0Adomain%20since%20obtaining%20a%20clean%20example%20is%20challenging%20in%20real%20world%20scenarios%2C%0Acreating%20a%20unique%20user%20interface%20problem.%20We%20present%20the%20first%20enrollment%0Ainterface%20where%20the%20wearer%20looks%20at%20the%20target%20speaker%20for%20a%20few%20seconds%20to%0Acapture%20a%20single%2C%20short%2C%20highly%20noisy%2C%20binaural%20example%20of%20the%20target%20speaker.%0AThis%20noisy%20example%20is%20used%20for%20enrollment%20and%20subsequent%20speech%20extraction%20in%0Athe%20presence%20of%20interfering%20speakers%20and%20noise.%20Our%20system%20achieves%20a%20signal%0Aquality%20improvement%20of%207.01%20dB%20using%20less%20than%205%20seconds%20of%20noisy%20enrollment%0Aaudio%20and%20can%20process%208%20ms%20of%20audio%20chunks%20in%206.24%20ms%20on%20an%20embedded%20CPU.%20Our%0Auser%20studies%20demonstrate%20generalization%20to%20real-world%20static%20and%20mobile%0Aspeakers%20in%20previously%20unseen%20indoor%20and%20outdoor%20multipath%20environments.%0AFinally%2C%20our%20enrollment%20interface%20for%20noisy%20examples%20does%20not%20cause%20performance%0Adegradation%20compared%20to%20clean%20examples%2C%20while%20being%20convenient%20and%0Auser-friendly.%20Taking%20a%20step%20back%2C%20this%20paper%20takes%20an%20important%20step%20towards%0Aenhancing%20the%20human%20auditory%20perception%20with%20artificial%20intelligence.%20We%0Aprovide%20code%20and%20data%20at%3A%20https%3A//github.com/vb000/LookOnceToHear.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06289v2&entry.124074799=Read"},
{"title": "Language Imbalance Can Boost Cross-lingual Generalisation", "author": "Anton Sch\u00e4fer and Shauli Ravfogel and Thomas Hofmann and Tiago Pimentel and Imanol Schlag", "abstract": "  Multilinguality is crucial for extending recent advancements in language\nmodelling to diverse linguistic communities. To maintain high performance while\nrepresenting multiple languages, multilingual models ideally align\nrepresentations, allowing what is learned in one language to generalise to\nothers. Prior research has emphasised the importance of parallel data and\nshared vocabulary elements as key factors for such alignment. In this study, we\ninvestigate an unintuitive novel driver of cross-lingual generalisation:\nlanguage imbalance. In controlled experiments on perfectly equivalent cloned\nlanguages, we observe that the existence of a predominant language during\ntraining boosts the performance of less frequent languages and leads to\nstronger alignment of model representations across languages. Furthermore, we\nfind that this trend is amplified with scale: with large enough models or long\nenough training, we observe that bilingual training data with a 90/10 language\nsplit yields better performance on both languages than a balanced 50/50 split.\nBuilding on these insights, we design training schemes that can improve\nperformance in all cloned languages, even without altering the training data.\nAs we extend our analysis to real languages, we find that infrequent languages\nstill benefit from frequent ones, yet whether language imbalance causes\ncross-lingual generalisation there is not conclusive.\n", "link": "http://arxiv.org/abs/2404.07982v3", "date": "2024-05-13", "relevancy": 1.8469, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4941}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4581}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Imbalance%20Can%20Boost%20Cross-lingual%20Generalisation&body=Title%3A%20Language%20Imbalance%20Can%20Boost%20Cross-lingual%20Generalisation%0AAuthor%3A%20Anton%20Sch%C3%A4fer%20and%20Shauli%20Ravfogel%20and%20Thomas%20Hofmann%20and%20Tiago%20Pimentel%20and%20Imanol%20Schlag%0AAbstract%3A%20%20%20Multilinguality%20is%20crucial%20for%20extending%20recent%20advancements%20in%20language%0Amodelling%20to%20diverse%20linguistic%20communities.%20To%20maintain%20high%20performance%20while%0Arepresenting%20multiple%20languages%2C%20multilingual%20models%20ideally%20align%0Arepresentations%2C%20allowing%20what%20is%20learned%20in%20one%20language%20to%20generalise%20to%0Aothers.%20Prior%20research%20has%20emphasised%20the%20importance%20of%20parallel%20data%20and%0Ashared%20vocabulary%20elements%20as%20key%20factors%20for%20such%20alignment.%20In%20this%20study%2C%20we%0Ainvestigate%20an%20unintuitive%20novel%20driver%20of%20cross-lingual%20generalisation%3A%0Alanguage%20imbalance.%20In%20controlled%20experiments%20on%20perfectly%20equivalent%20cloned%0Alanguages%2C%20we%20observe%20that%20the%20existence%20of%20a%20predominant%20language%20during%0Atraining%20boosts%20the%20performance%20of%20less%20frequent%20languages%20and%20leads%20to%0Astronger%20alignment%20of%20model%20representations%20across%20languages.%20Furthermore%2C%20we%0Afind%20that%20this%20trend%20is%20amplified%20with%20scale%3A%20with%20large%20enough%20models%20or%20long%0Aenough%20training%2C%20we%20observe%20that%20bilingual%20training%20data%20with%20a%2090/10%20language%0Asplit%20yields%20better%20performance%20on%20both%20languages%20than%20a%20balanced%2050/50%20split.%0ABuilding%20on%20these%20insights%2C%20we%20design%20training%20schemes%20that%20can%20improve%0Aperformance%20in%20all%20cloned%20languages%2C%20even%20without%20altering%20the%20training%20data.%0AAs%20we%20extend%20our%20analysis%20to%20real%20languages%2C%20we%20find%20that%20infrequent%20languages%0Astill%20benefit%20from%20frequent%20ones%2C%20yet%20whether%20language%20imbalance%20causes%0Across-lingual%20generalisation%20there%20is%20not%20conclusive.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07982v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Imbalance%2520Can%2520Boost%2520Cross-lingual%2520Generalisation%26entry.906535625%3DAnton%2520Sch%25C3%25A4fer%2520and%2520Shauli%2520Ravfogel%2520and%2520Thomas%2520Hofmann%2520and%2520Tiago%2520Pimentel%2520and%2520Imanol%2520Schlag%26entry.1292438233%3D%2520%2520Multilinguality%2520is%2520crucial%2520for%2520extending%2520recent%2520advancements%2520in%2520language%250Amodelling%2520to%2520diverse%2520linguistic%2520communities.%2520To%2520maintain%2520high%2520performance%2520while%250Arepresenting%2520multiple%2520languages%252C%2520multilingual%2520models%2520ideally%2520align%250Arepresentations%252C%2520allowing%2520what%2520is%2520learned%2520in%2520one%2520language%2520to%2520generalise%2520to%250Aothers.%2520Prior%2520research%2520has%2520emphasised%2520the%2520importance%2520of%2520parallel%2520data%2520and%250Ashared%2520vocabulary%2520elements%2520as%2520key%2520factors%2520for%2520such%2520alignment.%2520In%2520this%2520study%252C%2520we%250Ainvestigate%2520an%2520unintuitive%2520novel%2520driver%2520of%2520cross-lingual%2520generalisation%253A%250Alanguage%2520imbalance.%2520In%2520controlled%2520experiments%2520on%2520perfectly%2520equivalent%2520cloned%250Alanguages%252C%2520we%2520observe%2520that%2520the%2520existence%2520of%2520a%2520predominant%2520language%2520during%250Atraining%2520boosts%2520the%2520performance%2520of%2520less%2520frequent%2520languages%2520and%2520leads%2520to%250Astronger%2520alignment%2520of%2520model%2520representations%2520across%2520languages.%2520Furthermore%252C%2520we%250Afind%2520that%2520this%2520trend%2520is%2520amplified%2520with%2520scale%253A%2520with%2520large%2520enough%2520models%2520or%2520long%250Aenough%2520training%252C%2520we%2520observe%2520that%2520bilingual%2520training%2520data%2520with%2520a%252090/10%2520language%250Asplit%2520yields%2520better%2520performance%2520on%2520both%2520languages%2520than%2520a%2520balanced%252050/50%2520split.%250ABuilding%2520on%2520these%2520insights%252C%2520we%2520design%2520training%2520schemes%2520that%2520can%2520improve%250Aperformance%2520in%2520all%2520cloned%2520languages%252C%2520even%2520without%2520altering%2520the%2520training%2520data.%250AAs%2520we%2520extend%2520our%2520analysis%2520to%2520real%2520languages%252C%2520we%2520find%2520that%2520infrequent%2520languages%250Astill%2520benefit%2520from%2520frequent%2520ones%252C%2520yet%2520whether%2520language%2520imbalance%2520causes%250Across-lingual%2520generalisation%2520there%2520is%2520not%2520conclusive.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.07982v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Imbalance%20Can%20Boost%20Cross-lingual%20Generalisation&entry.906535625=Anton%20Sch%C3%A4fer%20and%20Shauli%20Ravfogel%20and%20Thomas%20Hofmann%20and%20Tiago%20Pimentel%20and%20Imanol%20Schlag&entry.1292438233=%20%20Multilinguality%20is%20crucial%20for%20extending%20recent%20advancements%20in%20language%0Amodelling%20to%20diverse%20linguistic%20communities.%20To%20maintain%20high%20performance%20while%0Arepresenting%20multiple%20languages%2C%20multilingual%20models%20ideally%20align%0Arepresentations%2C%20allowing%20what%20is%20learned%20in%20one%20language%20to%20generalise%20to%0Aothers.%20Prior%20research%20has%20emphasised%20the%20importance%20of%20parallel%20data%20and%0Ashared%20vocabulary%20elements%20as%20key%20factors%20for%20such%20alignment.%20In%20this%20study%2C%20we%0Ainvestigate%20an%20unintuitive%20novel%20driver%20of%20cross-lingual%20generalisation%3A%0Alanguage%20imbalance.%20In%20controlled%20experiments%20on%20perfectly%20equivalent%20cloned%0Alanguages%2C%20we%20observe%20that%20the%20existence%20of%20a%20predominant%20language%20during%0Atraining%20boosts%20the%20performance%20of%20less%20frequent%20languages%20and%20leads%20to%0Astronger%20alignment%20of%20model%20representations%20across%20languages.%20Furthermore%2C%20we%0Afind%20that%20this%20trend%20is%20amplified%20with%20scale%3A%20with%20large%20enough%20models%20or%20long%0Aenough%20training%2C%20we%20observe%20that%20bilingual%20training%20data%20with%20a%2090/10%20language%0Asplit%20yields%20better%20performance%20on%20both%20languages%20than%20a%20balanced%2050/50%20split.%0ABuilding%20on%20these%20insights%2C%20we%20design%20training%20schemes%20that%20can%20improve%0Aperformance%20in%20all%20cloned%20languages%2C%20even%20without%20altering%20the%20training%20data.%0AAs%20we%20extend%20our%20analysis%20to%20real%20languages%2C%20we%20find%20that%20infrequent%20languages%0Astill%20benefit%20from%20frequent%20ones%2C%20yet%20whether%20language%20imbalance%20causes%0Across-lingual%20generalisation%20there%20is%20not%20conclusive.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07982v3&entry.124074799=Read"},
{"title": "$\u03b1$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning", "author": "Rafael Kourdis and Gabriel Gordon-Hall and Philip John Gorinski", "abstract": "  Multitask Learning is a Machine Learning paradigm that aims to train a range\nof (usually related) tasks with the help of a shared model. While the goal is\noften to improve the joint performance of all training tasks, another approach\nis to focus on the performance of a specific target task, while treating the\nremaining ones as auxiliary data from which to possibly leverage positive\ntransfer towards the target during training. In such settings, it becomes\nimportant to estimate the positive or negative influence auxiliary tasks will\nhave on the target. While many ways have been proposed to estimate task weights\nbefore or during training they typically rely on heuristics or extensive search\nof the weighting space. We propose a novel method called $\\alpha$-Variable\nImportance Learning ($\\alpha$VIL) that is able to adjust task weights\ndynamically during model training, by making direct use of task-specific\nupdates of the underlying model's parameters between training epochs.\nExperiments indicate that $\\alpha$VIL is able to outperform other Multitask\nLearning approaches in a variety of settings. To our knowledge, this is the\nfirst attempt at making direct use of model updates for task weight estimation.\n", "link": "http://arxiv.org/abs/2405.07769v1", "date": "2024-05-13", "relevancy": 1.8412, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4678}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4569}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%CE%B1%24VIL%3A%20Learning%20to%20Leverage%20Auxiliary%20Tasks%20for%20Multitask%20Learning&body=Title%3A%20%24%CE%B1%24VIL%3A%20Learning%20to%20Leverage%20Auxiliary%20Tasks%20for%20Multitask%20Learning%0AAuthor%3A%20Rafael%20Kourdis%20and%20Gabriel%20Gordon-Hall%20and%20Philip%20John%20Gorinski%0AAbstract%3A%20%20%20Multitask%20Learning%20is%20a%20Machine%20Learning%20paradigm%20that%20aims%20to%20train%20a%20range%0Aof%20%28usually%20related%29%20tasks%20with%20the%20help%20of%20a%20shared%20model.%20While%20the%20goal%20is%0Aoften%20to%20improve%20the%20joint%20performance%20of%20all%20training%20tasks%2C%20another%20approach%0Ais%20to%20focus%20on%20the%20performance%20of%20a%20specific%20target%20task%2C%20while%20treating%20the%0Aremaining%20ones%20as%20auxiliary%20data%20from%20which%20to%20possibly%20leverage%20positive%0Atransfer%20towards%20the%20target%20during%20training.%20In%20such%20settings%2C%20it%20becomes%0Aimportant%20to%20estimate%20the%20positive%20or%20negative%20influence%20auxiliary%20tasks%20will%0Ahave%20on%20the%20target.%20While%20many%20ways%20have%20been%20proposed%20to%20estimate%20task%20weights%0Abefore%20or%20during%20training%20they%20typically%20rely%20on%20heuristics%20or%20extensive%20search%0Aof%20the%20weighting%20space.%20We%20propose%20a%20novel%20method%20called%20%24%5Calpha%24-Variable%0AImportance%20Learning%20%28%24%5Calpha%24VIL%29%20that%20is%20able%20to%20adjust%20task%20weights%0Adynamically%20during%20model%20training%2C%20by%20making%20direct%20use%20of%20task-specific%0Aupdates%20of%20the%20underlying%20model%27s%20parameters%20between%20training%20epochs.%0AExperiments%20indicate%20that%20%24%5Calpha%24VIL%20is%20able%20to%20outperform%20other%20Multitask%0ALearning%20approaches%20in%20a%20variety%20of%20settings.%20To%20our%20knowledge%2C%20this%20is%20the%0Afirst%20attempt%20at%20making%20direct%20use%20of%20model%20updates%20for%20task%20weight%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07769v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%25CE%25B1%2524VIL%253A%2520Learning%2520to%2520Leverage%2520Auxiliary%2520Tasks%2520for%2520Multitask%2520Learning%26entry.906535625%3DRafael%2520Kourdis%2520and%2520Gabriel%2520Gordon-Hall%2520and%2520Philip%2520John%2520Gorinski%26entry.1292438233%3D%2520%2520Multitask%2520Learning%2520is%2520a%2520Machine%2520Learning%2520paradigm%2520that%2520aims%2520to%2520train%2520a%2520range%250Aof%2520%2528usually%2520related%2529%2520tasks%2520with%2520the%2520help%2520of%2520a%2520shared%2520model.%2520While%2520the%2520goal%2520is%250Aoften%2520to%2520improve%2520the%2520joint%2520performance%2520of%2520all%2520training%2520tasks%252C%2520another%2520approach%250Ais%2520to%2520focus%2520on%2520the%2520performance%2520of%2520a%2520specific%2520target%2520task%252C%2520while%2520treating%2520the%250Aremaining%2520ones%2520as%2520auxiliary%2520data%2520from%2520which%2520to%2520possibly%2520leverage%2520positive%250Atransfer%2520towards%2520the%2520target%2520during%2520training.%2520In%2520such%2520settings%252C%2520it%2520becomes%250Aimportant%2520to%2520estimate%2520the%2520positive%2520or%2520negative%2520influence%2520auxiliary%2520tasks%2520will%250Ahave%2520on%2520the%2520target.%2520While%2520many%2520ways%2520have%2520been%2520proposed%2520to%2520estimate%2520task%2520weights%250Abefore%2520or%2520during%2520training%2520they%2520typically%2520rely%2520on%2520heuristics%2520or%2520extensive%2520search%250Aof%2520the%2520weighting%2520space.%2520We%2520propose%2520a%2520novel%2520method%2520called%2520%2524%255Calpha%2524-Variable%250AImportance%2520Learning%2520%2528%2524%255Calpha%2524VIL%2529%2520that%2520is%2520able%2520to%2520adjust%2520task%2520weights%250Adynamically%2520during%2520model%2520training%252C%2520by%2520making%2520direct%2520use%2520of%2520task-specific%250Aupdates%2520of%2520the%2520underlying%2520model%2527s%2520parameters%2520between%2520training%2520epochs.%250AExperiments%2520indicate%2520that%2520%2524%255Calpha%2524VIL%2520is%2520able%2520to%2520outperform%2520other%2520Multitask%250ALearning%2520approaches%2520in%2520a%2520variety%2520of%2520settings.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%250Afirst%2520attempt%2520at%2520making%2520direct%2520use%2520of%2520model%2520updates%2520for%2520task%2520weight%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07769v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%CE%B1%24VIL%3A%20Learning%20to%20Leverage%20Auxiliary%20Tasks%20for%20Multitask%20Learning&entry.906535625=Rafael%20Kourdis%20and%20Gabriel%20Gordon-Hall%20and%20Philip%20John%20Gorinski&entry.1292438233=%20%20Multitask%20Learning%20is%20a%20Machine%20Learning%20paradigm%20that%20aims%20to%20train%20a%20range%0Aof%20%28usually%20related%29%20tasks%20with%20the%20help%20of%20a%20shared%20model.%20While%20the%20goal%20is%0Aoften%20to%20improve%20the%20joint%20performance%20of%20all%20training%20tasks%2C%20another%20approach%0Ais%20to%20focus%20on%20the%20performance%20of%20a%20specific%20target%20task%2C%20while%20treating%20the%0Aremaining%20ones%20as%20auxiliary%20data%20from%20which%20to%20possibly%20leverage%20positive%0Atransfer%20towards%20the%20target%20during%20training.%20In%20such%20settings%2C%20it%20becomes%0Aimportant%20to%20estimate%20the%20positive%20or%20negative%20influence%20auxiliary%20tasks%20will%0Ahave%20on%20the%20target.%20While%20many%20ways%20have%20been%20proposed%20to%20estimate%20task%20weights%0Abefore%20or%20during%20training%20they%20typically%20rely%20on%20heuristics%20or%20extensive%20search%0Aof%20the%20weighting%20space.%20We%20propose%20a%20novel%20method%20called%20%24%5Calpha%24-Variable%0AImportance%20Learning%20%28%24%5Calpha%24VIL%29%20that%20is%20able%20to%20adjust%20task%20weights%0Adynamically%20during%20model%20training%2C%20by%20making%20direct%20use%20of%20task-specific%0Aupdates%20of%20the%20underlying%20model%27s%20parameters%20between%20training%20epochs.%0AExperiments%20indicate%20that%20%24%5Calpha%24VIL%20is%20able%20to%20outperform%20other%20Multitask%0ALearning%20approaches%20in%20a%20variety%20of%20settings.%20To%20our%20knowledge%2C%20this%20is%20the%0Afirst%20attempt%20at%20making%20direct%20use%20of%20model%20updates%20for%20task%20weight%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07769v1&entry.124074799=Read"},
{"title": "Integrating Multi-Physics Simulations and Machine Learning to Define the\n  Spatter Mechanism and Process Window in Laser Powder Bed Fusion", "author": "Olabode T. Ajenifujah and Francis Ogoke and Florian Wirth and Jack Beuth and Amir Barati Farimani", "abstract": "  Laser powder bed fusion (LPBF) has shown promise for wide range of\napplications due to its ability to fabricate freeform geometries and generate a\ncontrolled microstructure. However, components generated by LPBF still possess\nsub-optimal mechanical properties due to the defects that are created during\nlaser-material interactions. In this work, we investigate mechanism of spatter\nformation, using a high-fidelity modelling tool that was built to simulate the\nmulti-physics phenomena in LPBF. The modelling tool have the capability to\ncapture the 3D resolution of the meltpool and the spatter behavior. To\nunderstand spatter behavior and formation, we reveal its properties at ejection\nand evaluate its variation from the meltpool, the source where it is formed.\nThe dataset of the spatter and the meltpool collected consist of 50 % spatter\nand 50 % melt pool samples, with features that include position components,\nvelocity components, velocity magnitude, temperature, density and pressure. The\nrelationship between the spatter and the meltpool were evaluated via\ncorrelation analysis and machine learning (ML) algorithms for classification\ntasks. Upon screening different ML algorithms on the dataset, a high accuracy\nwas observed for all the ML models, with ExtraTrees having the highest at 96 %\nand KNN having the lowest at 94 %.\n", "link": "http://arxiv.org/abs/2405.07823v1", "date": "2024-05-13", "relevancy": 1.8354, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4735}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4577}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20Multi-Physics%20Simulations%20and%20Machine%20Learning%20to%20Define%20the%0A%20%20Spatter%20Mechanism%20and%20Process%20Window%20in%20Laser%20Powder%20Bed%20Fusion&body=Title%3A%20Integrating%20Multi-Physics%20Simulations%20and%20Machine%20Learning%20to%20Define%20the%0A%20%20Spatter%20Mechanism%20and%20Process%20Window%20in%20Laser%20Powder%20Bed%20Fusion%0AAuthor%3A%20Olabode%20T.%20Ajenifujah%20and%20Francis%20Ogoke%20and%20Florian%20Wirth%20and%20Jack%20Beuth%20and%20Amir%20Barati%20Farimani%0AAbstract%3A%20%20%20Laser%20powder%20bed%20fusion%20%28LPBF%29%20has%20shown%20promise%20for%20wide%20range%20of%0Aapplications%20due%20to%20its%20ability%20to%20fabricate%20freeform%20geometries%20and%20generate%20a%0Acontrolled%20microstructure.%20However%2C%20components%20generated%20by%20LPBF%20still%20possess%0Asub-optimal%20mechanical%20properties%20due%20to%20the%20defects%20that%20are%20created%20during%0Alaser-material%20interactions.%20In%20this%20work%2C%20we%20investigate%20mechanism%20of%20spatter%0Aformation%2C%20using%20a%20high-fidelity%20modelling%20tool%20that%20was%20built%20to%20simulate%20the%0Amulti-physics%20phenomena%20in%20LPBF.%20The%20modelling%20tool%20have%20the%20capability%20to%0Acapture%20the%203D%20resolution%20of%20the%20meltpool%20and%20the%20spatter%20behavior.%20To%0Aunderstand%20spatter%20behavior%20and%20formation%2C%20we%20reveal%20its%20properties%20at%20ejection%0Aand%20evaluate%20its%20variation%20from%20the%20meltpool%2C%20the%20source%20where%20it%20is%20formed.%0AThe%20dataset%20of%20the%20spatter%20and%20the%20meltpool%20collected%20consist%20of%2050%20%25%20spatter%0Aand%2050%20%25%20melt%20pool%20samples%2C%20with%20features%20that%20include%20position%20components%2C%0Avelocity%20components%2C%20velocity%20magnitude%2C%20temperature%2C%20density%20and%20pressure.%20The%0Arelationship%20between%20the%20spatter%20and%20the%20meltpool%20were%20evaluated%20via%0Acorrelation%20analysis%20and%20machine%20learning%20%28ML%29%20algorithms%20for%20classification%0Atasks.%20Upon%20screening%20different%20ML%20algorithms%20on%20the%20dataset%2C%20a%20high%20accuracy%0Awas%20observed%20for%20all%20the%20ML%20models%2C%20with%20ExtraTrees%20having%20the%20highest%20at%2096%20%25%0Aand%20KNN%20having%20the%20lowest%20at%2094%20%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07823v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520Multi-Physics%2520Simulations%2520and%2520Machine%2520Learning%2520to%2520Define%2520the%250A%2520%2520Spatter%2520Mechanism%2520and%2520Process%2520Window%2520in%2520Laser%2520Powder%2520Bed%2520Fusion%26entry.906535625%3DOlabode%2520T.%2520Ajenifujah%2520and%2520Francis%2520Ogoke%2520and%2520Florian%2520Wirth%2520and%2520Jack%2520Beuth%2520and%2520Amir%2520Barati%2520Farimani%26entry.1292438233%3D%2520%2520Laser%2520powder%2520bed%2520fusion%2520%2528LPBF%2529%2520has%2520shown%2520promise%2520for%2520wide%2520range%2520of%250Aapplications%2520due%2520to%2520its%2520ability%2520to%2520fabricate%2520freeform%2520geometries%2520and%2520generate%2520a%250Acontrolled%2520microstructure.%2520However%252C%2520components%2520generated%2520by%2520LPBF%2520still%2520possess%250Asub-optimal%2520mechanical%2520properties%2520due%2520to%2520the%2520defects%2520that%2520are%2520created%2520during%250Alaser-material%2520interactions.%2520In%2520this%2520work%252C%2520we%2520investigate%2520mechanism%2520of%2520spatter%250Aformation%252C%2520using%2520a%2520high-fidelity%2520modelling%2520tool%2520that%2520was%2520built%2520to%2520simulate%2520the%250Amulti-physics%2520phenomena%2520in%2520LPBF.%2520The%2520modelling%2520tool%2520have%2520the%2520capability%2520to%250Acapture%2520the%25203D%2520resolution%2520of%2520the%2520meltpool%2520and%2520the%2520spatter%2520behavior.%2520To%250Aunderstand%2520spatter%2520behavior%2520and%2520formation%252C%2520we%2520reveal%2520its%2520properties%2520at%2520ejection%250Aand%2520evaluate%2520its%2520variation%2520from%2520the%2520meltpool%252C%2520the%2520source%2520where%2520it%2520is%2520formed.%250AThe%2520dataset%2520of%2520the%2520spatter%2520and%2520the%2520meltpool%2520collected%2520consist%2520of%252050%2520%2525%2520spatter%250Aand%252050%2520%2525%2520melt%2520pool%2520samples%252C%2520with%2520features%2520that%2520include%2520position%2520components%252C%250Avelocity%2520components%252C%2520velocity%2520magnitude%252C%2520temperature%252C%2520density%2520and%2520pressure.%2520The%250Arelationship%2520between%2520the%2520spatter%2520and%2520the%2520meltpool%2520were%2520evaluated%2520via%250Acorrelation%2520analysis%2520and%2520machine%2520learning%2520%2528ML%2529%2520algorithms%2520for%2520classification%250Atasks.%2520Upon%2520screening%2520different%2520ML%2520algorithms%2520on%2520the%2520dataset%252C%2520a%2520high%2520accuracy%250Awas%2520observed%2520for%2520all%2520the%2520ML%2520models%252C%2520with%2520ExtraTrees%2520having%2520the%2520highest%2520at%252096%2520%2525%250Aand%2520KNN%2520having%2520the%2520lowest%2520at%252094%2520%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07823v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20Multi-Physics%20Simulations%20and%20Machine%20Learning%20to%20Define%20the%0A%20%20Spatter%20Mechanism%20and%20Process%20Window%20in%20Laser%20Powder%20Bed%20Fusion&entry.906535625=Olabode%20T.%20Ajenifujah%20and%20Francis%20Ogoke%20and%20Florian%20Wirth%20and%20Jack%20Beuth%20and%20Amir%20Barati%20Farimani&entry.1292438233=%20%20Laser%20powder%20bed%20fusion%20%28LPBF%29%20has%20shown%20promise%20for%20wide%20range%20of%0Aapplications%20due%20to%20its%20ability%20to%20fabricate%20freeform%20geometries%20and%20generate%20a%0Acontrolled%20microstructure.%20However%2C%20components%20generated%20by%20LPBF%20still%20possess%0Asub-optimal%20mechanical%20properties%20due%20to%20the%20defects%20that%20are%20created%20during%0Alaser-material%20interactions.%20In%20this%20work%2C%20we%20investigate%20mechanism%20of%20spatter%0Aformation%2C%20using%20a%20high-fidelity%20modelling%20tool%20that%20was%20built%20to%20simulate%20the%0Amulti-physics%20phenomena%20in%20LPBF.%20The%20modelling%20tool%20have%20the%20capability%20to%0Acapture%20the%203D%20resolution%20of%20the%20meltpool%20and%20the%20spatter%20behavior.%20To%0Aunderstand%20spatter%20behavior%20and%20formation%2C%20we%20reveal%20its%20properties%20at%20ejection%0Aand%20evaluate%20its%20variation%20from%20the%20meltpool%2C%20the%20source%20where%20it%20is%20formed.%0AThe%20dataset%20of%20the%20spatter%20and%20the%20meltpool%20collected%20consist%20of%2050%20%25%20spatter%0Aand%2050%20%25%20melt%20pool%20samples%2C%20with%20features%20that%20include%20position%20components%2C%0Avelocity%20components%2C%20velocity%20magnitude%2C%20temperature%2C%20density%20and%20pressure.%20The%0Arelationship%20between%20the%20spatter%20and%20the%20meltpool%20were%20evaluated%20via%0Acorrelation%20analysis%20and%20machine%20learning%20%28ML%29%20algorithms%20for%20classification%0Atasks.%20Upon%20screening%20different%20ML%20algorithms%20on%20the%20dataset%2C%20a%20high%20accuracy%0Awas%20observed%20for%20all%20the%20ML%20models%2C%20with%20ExtraTrees%20having%20the%20highest%20at%2096%20%25%0Aand%20KNN%20having%20the%20lowest%20at%2094%20%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07823v1&entry.124074799=Read"},
{"title": "PARDEN, Can You Repeat That? Defending against Jailbreaks via Repetition", "author": "Ziyang Zhang and Qizhen Zhang and Jakob Foerster", "abstract": "  Large language models (LLMs) have shown success in many natural language\nprocessing tasks. Despite rigorous safety alignment processes, supposedly\nsafety-aligned LLMs like Llama 2 and Claude 2 are still susceptible to\njailbreaks, leading to security risks and abuse of the models. One option to\nmitigate such risks is to augment the LLM with a dedicated \"safeguard\", which\nchecks the LLM's inputs or outputs for undesired behaviour. A promising\napproach is to use the LLM itself as the safeguard. Nonetheless, baseline\nmethods, such as prompting the LLM to self-classify toxic content, demonstrate\nlimited efficacy. We hypothesise that this is due to domain shift: the\nalignment training imparts a self-censoring behaviour to the model (\"Sorry I\ncan't do that\"), while the self-classify approach shifts it to a classification\nformat (\"Is this prompt malicious\"). In this work, we propose PARDEN, which\navoids this domain shift by simply asking the model to repeat its own outputs.\nPARDEN neither requires finetuning nor white box access to the model. We\nempirically verify the effectiveness of our method and show that PARDEN\nsignificantly outperforms existing jailbreak detection baselines for Llama-2\nand Claude-2. Code and data are available at https://github.com/Ed-Zh/PARDEN.\n  We find that PARDEN is particularly powerful in the relevant regime of high\nTrue Positive Rate (TPR) and low False Positive Rate (FPR). For instance, for\nLlama2-7B, at TPR equal to 90%, PARDEN accomplishes a roughly 11x reduction in\nthe FPR from 24.8% to 2.0% on the harmful behaviours dataset.\n", "link": "http://arxiv.org/abs/2405.07932v1", "date": "2024-05-13", "relevancy": 1.829, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4588}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4566}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PARDEN%2C%20Can%20You%20Repeat%20That%3F%20Defending%20against%20Jailbreaks%20via%20Repetition&body=Title%3A%20PARDEN%2C%20Can%20You%20Repeat%20That%3F%20Defending%20against%20Jailbreaks%20via%20Repetition%0AAuthor%3A%20Ziyang%20Zhang%20and%20Qizhen%20Zhang%20and%20Jakob%20Foerster%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20success%20in%20many%20natural%20language%0Aprocessing%20tasks.%20Despite%20rigorous%20safety%20alignment%20processes%2C%20supposedly%0Asafety-aligned%20LLMs%20like%20Llama%202%20and%20Claude%202%20are%20still%20susceptible%20to%0Ajailbreaks%2C%20leading%20to%20security%20risks%20and%20abuse%20of%20the%20models.%20One%20option%20to%0Amitigate%20such%20risks%20is%20to%20augment%20the%20LLM%20with%20a%20dedicated%20%22safeguard%22%2C%20which%0Achecks%20the%20LLM%27s%20inputs%20or%20outputs%20for%20undesired%20behaviour.%20A%20promising%0Aapproach%20is%20to%20use%20the%20LLM%20itself%20as%20the%20safeguard.%20Nonetheless%2C%20baseline%0Amethods%2C%20such%20as%20prompting%20the%20LLM%20to%20self-classify%20toxic%20content%2C%20demonstrate%0Alimited%20efficacy.%20We%20hypothesise%20that%20this%20is%20due%20to%20domain%20shift%3A%20the%0Aalignment%20training%20imparts%20a%20self-censoring%20behaviour%20to%20the%20model%20%28%22Sorry%20I%0Acan%27t%20do%20that%22%29%2C%20while%20the%20self-classify%20approach%20shifts%20it%20to%20a%20classification%0Aformat%20%28%22Is%20this%20prompt%20malicious%22%29.%20In%20this%20work%2C%20we%20propose%20PARDEN%2C%20which%0Aavoids%20this%20domain%20shift%20by%20simply%20asking%20the%20model%20to%20repeat%20its%20own%20outputs.%0APARDEN%20neither%20requires%20finetuning%20nor%20white%20box%20access%20to%20the%20model.%20We%0Aempirically%20verify%20the%20effectiveness%20of%20our%20method%20and%20show%20that%20PARDEN%0Asignificantly%20outperforms%20existing%20jailbreak%20detection%20baselines%20for%20Llama-2%0Aand%20Claude-2.%20Code%20and%20data%20are%20available%20at%20https%3A//github.com/Ed-Zh/PARDEN.%0A%20%20We%20find%20that%20PARDEN%20is%20particularly%20powerful%20in%20the%20relevant%20regime%20of%20high%0ATrue%20Positive%20Rate%20%28TPR%29%20and%20low%20False%20Positive%20Rate%20%28FPR%29.%20For%20instance%2C%20for%0ALlama2-7B%2C%20at%20TPR%20equal%20to%2090%25%2C%20PARDEN%20accomplishes%20a%20roughly%2011x%20reduction%20in%0Athe%20FPR%20from%2024.8%25%20to%202.0%25%20on%20the%20harmful%20behaviours%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPARDEN%252C%2520Can%2520You%2520Repeat%2520That%253F%2520Defending%2520against%2520Jailbreaks%2520via%2520Repetition%26entry.906535625%3DZiyang%2520Zhang%2520and%2520Qizhen%2520Zhang%2520and%2520Jakob%2520Foerster%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520success%2520in%2520many%2520natural%2520language%250Aprocessing%2520tasks.%2520Despite%2520rigorous%2520safety%2520alignment%2520processes%252C%2520supposedly%250Asafety-aligned%2520LLMs%2520like%2520Llama%25202%2520and%2520Claude%25202%2520are%2520still%2520susceptible%2520to%250Ajailbreaks%252C%2520leading%2520to%2520security%2520risks%2520and%2520abuse%2520of%2520the%2520models.%2520One%2520option%2520to%250Amitigate%2520such%2520risks%2520is%2520to%2520augment%2520the%2520LLM%2520with%2520a%2520dedicated%2520%2522safeguard%2522%252C%2520which%250Achecks%2520the%2520LLM%2527s%2520inputs%2520or%2520outputs%2520for%2520undesired%2520behaviour.%2520A%2520promising%250Aapproach%2520is%2520to%2520use%2520the%2520LLM%2520itself%2520as%2520the%2520safeguard.%2520Nonetheless%252C%2520baseline%250Amethods%252C%2520such%2520as%2520prompting%2520the%2520LLM%2520to%2520self-classify%2520toxic%2520content%252C%2520demonstrate%250Alimited%2520efficacy.%2520We%2520hypothesise%2520that%2520this%2520is%2520due%2520to%2520domain%2520shift%253A%2520the%250Aalignment%2520training%2520imparts%2520a%2520self-censoring%2520behaviour%2520to%2520the%2520model%2520%2528%2522Sorry%2520I%250Acan%2527t%2520do%2520that%2522%2529%252C%2520while%2520the%2520self-classify%2520approach%2520shifts%2520it%2520to%2520a%2520classification%250Aformat%2520%2528%2522Is%2520this%2520prompt%2520malicious%2522%2529.%2520In%2520this%2520work%252C%2520we%2520propose%2520PARDEN%252C%2520which%250Aavoids%2520this%2520domain%2520shift%2520by%2520simply%2520asking%2520the%2520model%2520to%2520repeat%2520its%2520own%2520outputs.%250APARDEN%2520neither%2520requires%2520finetuning%2520nor%2520white%2520box%2520access%2520to%2520the%2520model.%2520We%250Aempirically%2520verify%2520the%2520effectiveness%2520of%2520our%2520method%2520and%2520show%2520that%2520PARDEN%250Asignificantly%2520outperforms%2520existing%2520jailbreak%2520detection%2520baselines%2520for%2520Llama-2%250Aand%2520Claude-2.%2520Code%2520and%2520data%2520are%2520available%2520at%2520https%253A//github.com/Ed-Zh/PARDEN.%250A%2520%2520We%2520find%2520that%2520PARDEN%2520is%2520particularly%2520powerful%2520in%2520the%2520relevant%2520regime%2520of%2520high%250ATrue%2520Positive%2520Rate%2520%2528TPR%2529%2520and%2520low%2520False%2520Positive%2520Rate%2520%2528FPR%2529.%2520For%2520instance%252C%2520for%250ALlama2-7B%252C%2520at%2520TPR%2520equal%2520to%252090%2525%252C%2520PARDEN%2520accomplishes%2520a%2520roughly%252011x%2520reduction%2520in%250Athe%2520FPR%2520from%252024.8%2525%2520to%25202.0%2525%2520on%2520the%2520harmful%2520behaviours%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PARDEN%2C%20Can%20You%20Repeat%20That%3F%20Defending%20against%20Jailbreaks%20via%20Repetition&entry.906535625=Ziyang%20Zhang%20and%20Qizhen%20Zhang%20and%20Jakob%20Foerster&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20success%20in%20many%20natural%20language%0Aprocessing%20tasks.%20Despite%20rigorous%20safety%20alignment%20processes%2C%20supposedly%0Asafety-aligned%20LLMs%20like%20Llama%202%20and%20Claude%202%20are%20still%20susceptible%20to%0Ajailbreaks%2C%20leading%20to%20security%20risks%20and%20abuse%20of%20the%20models.%20One%20option%20to%0Amitigate%20such%20risks%20is%20to%20augment%20the%20LLM%20with%20a%20dedicated%20%22safeguard%22%2C%20which%0Achecks%20the%20LLM%27s%20inputs%20or%20outputs%20for%20undesired%20behaviour.%20A%20promising%0Aapproach%20is%20to%20use%20the%20LLM%20itself%20as%20the%20safeguard.%20Nonetheless%2C%20baseline%0Amethods%2C%20such%20as%20prompting%20the%20LLM%20to%20self-classify%20toxic%20content%2C%20demonstrate%0Alimited%20efficacy.%20We%20hypothesise%20that%20this%20is%20due%20to%20domain%20shift%3A%20the%0Aalignment%20training%20imparts%20a%20self-censoring%20behaviour%20to%20the%20model%20%28%22Sorry%20I%0Acan%27t%20do%20that%22%29%2C%20while%20the%20self-classify%20approach%20shifts%20it%20to%20a%20classification%0Aformat%20%28%22Is%20this%20prompt%20malicious%22%29.%20In%20this%20work%2C%20we%20propose%20PARDEN%2C%20which%0Aavoids%20this%20domain%20shift%20by%20simply%20asking%20the%20model%20to%20repeat%20its%20own%20outputs.%0APARDEN%20neither%20requires%20finetuning%20nor%20white%20box%20access%20to%20the%20model.%20We%0Aempirically%20verify%20the%20effectiveness%20of%20our%20method%20and%20show%20that%20PARDEN%0Asignificantly%20outperforms%20existing%20jailbreak%20detection%20baselines%20for%20Llama-2%0Aand%20Claude-2.%20Code%20and%20data%20are%20available%20at%20https%3A//github.com/Ed-Zh/PARDEN.%0A%20%20We%20find%20that%20PARDEN%20is%20particularly%20powerful%20in%20the%20relevant%20regime%20of%20high%0ATrue%20Positive%20Rate%20%28TPR%29%20and%20low%20False%20Positive%20Rate%20%28FPR%29.%20For%20instance%2C%20for%0ALlama2-7B%2C%20at%20TPR%20equal%20to%2090%25%2C%20PARDEN%20accomplishes%20a%20roughly%2011x%20reduction%20in%0Athe%20FPR%20from%2024.8%25%20to%202.0%25%20on%20the%20harmful%20behaviours%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07932v1&entry.124074799=Read"},
{"title": "Analysis of the rate of convergence of an over-parametrized\n  convolutional neural network image classifier learned by gradient descent", "author": "Michael Kohler and Adam Krzyzak and Benjamin Walter", "abstract": "  Image classification based on over-parametrized convolutional neural networks\nwith a global average-pooling layer is considered. The weights of the network\nare learned by gradient descent. A bound on the rate of convergence of the\ndifference between the misclassification risk of the newly introduced\nconvolutional neural network estimate and the minimal possible value is\nderived.\n", "link": "http://arxiv.org/abs/2405.07619v1", "date": "2024-05-13", "relevancy": 1.8175, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4718}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4489}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20of%20the%20rate%20of%20convergence%20of%20an%20over-parametrized%0A%20%20convolutional%20neural%20network%20image%20classifier%20learned%20by%20gradient%20descent&body=Title%3A%20Analysis%20of%20the%20rate%20of%20convergence%20of%20an%20over-parametrized%0A%20%20convolutional%20neural%20network%20image%20classifier%20learned%20by%20gradient%20descent%0AAuthor%3A%20Michael%20Kohler%20and%20Adam%20Krzyzak%20and%20Benjamin%20Walter%0AAbstract%3A%20%20%20Image%20classification%20based%20on%20over-parametrized%20convolutional%20neural%20networks%0Awith%20a%20global%20average-pooling%20layer%20is%20considered.%20The%20weights%20of%20the%20network%0Aare%20learned%20by%20gradient%20descent.%20A%20bound%20on%20the%20rate%20of%20convergence%20of%20the%0Adifference%20between%20the%20misclassification%20risk%20of%20the%20newly%20introduced%0Aconvolutional%20neural%20network%20estimate%20and%20the%20minimal%20possible%20value%20is%0Aderived.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520of%2520the%2520rate%2520of%2520convergence%2520of%2520an%2520over-parametrized%250A%2520%2520convolutional%2520neural%2520network%2520image%2520classifier%2520learned%2520by%2520gradient%2520descent%26entry.906535625%3DMichael%2520Kohler%2520and%2520Adam%2520Krzyzak%2520and%2520Benjamin%2520Walter%26entry.1292438233%3D%2520%2520Image%2520classification%2520based%2520on%2520over-parametrized%2520convolutional%2520neural%2520networks%250Awith%2520a%2520global%2520average-pooling%2520layer%2520is%2520considered.%2520The%2520weights%2520of%2520the%2520network%250Aare%2520learned%2520by%2520gradient%2520descent.%2520A%2520bound%2520on%2520the%2520rate%2520of%2520convergence%2520of%2520the%250Adifference%2520between%2520the%2520misclassification%2520risk%2520of%2520the%2520newly%2520introduced%250Aconvolutional%2520neural%2520network%2520estimate%2520and%2520the%2520minimal%2520possible%2520value%2520is%250Aderived.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20of%20the%20rate%20of%20convergence%20of%20an%20over-parametrized%0A%20%20convolutional%20neural%20network%20image%20classifier%20learned%20by%20gradient%20descent&entry.906535625=Michael%20Kohler%20and%20Adam%20Krzyzak%20and%20Benjamin%20Walter&entry.1292438233=%20%20Image%20classification%20based%20on%20over-parametrized%20convolutional%20neural%20networks%0Awith%20a%20global%20average-pooling%20layer%20is%20considered.%20The%20weights%20of%20the%20network%0Aare%20learned%20by%20gradient%20descent.%20A%20bound%20on%20the%20rate%20of%20convergence%20of%20the%0Adifference%20between%20the%20misclassification%20risk%20of%20the%20newly%20introduced%0Aconvolutional%20neural%20network%20estimate%20and%20the%20minimal%20possible%20value%20is%0Aderived.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07619v1&entry.124074799=Read"},
{"title": "CrossCert: A Cross-Checking Detection Approach to Patch Robustness\n  Certification for Deep Learning Models", "author": "Qilin Zhou and Zhengyuan Wei and Haipeng Wang and Bo Jiang and W. K. Chan", "abstract": "  Patch robustness certification is an emerging kind of defense technique\nagainst adversarial patch attacks with provable guarantees. There are two\nresearch lines: certified recovery and certified detection. They aim to label\nmalicious samples with provable guarantees correctly and issue warnings for\nmalicious samples predicted to non-benign labels with provable guarantees,\nrespectively. However, existing certified detection defenders suffer from\nprotecting labels subject to manipulation, and existing certified recovery\ndefenders cannot systematically warn samples about their labels. A certified\ndefense that simultaneously offers robust labels and systematic warning\nprotection against patch attacks is desirable. This paper proposes a novel\ncertified defense technique called CrossCert. CrossCert formulates a novel\napproach by cross-checking two certified recovery defenders to provide\nunwavering certification and detection certification. Unwavering certification\nensures that a certified sample, when subjected to a patched perturbation, will\nalways be returned with a benign label without triggering any warnings with a\nprovable guarantee. To our knowledge, CrossCert is the first certified\ndetection technique to offer this guarantee. Our experiments show that, with a\nslightly lower performance than ViP and comparable performance with PatchCensor\nin terms of detection certification, CrossCert certifies a significant\nproportion of samples with the guarantee of unwavering certification.\n", "link": "http://arxiv.org/abs/2405.07668v1", "date": "2024-05-13", "relevancy": 1.794, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4827}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4472}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CrossCert%3A%20A%20Cross-Checking%20Detection%20Approach%20to%20Patch%20Robustness%0A%20%20Certification%20for%20Deep%20Learning%20Models&body=Title%3A%20CrossCert%3A%20A%20Cross-Checking%20Detection%20Approach%20to%20Patch%20Robustness%0A%20%20Certification%20for%20Deep%20Learning%20Models%0AAuthor%3A%20Qilin%20Zhou%20and%20Zhengyuan%20Wei%20and%20Haipeng%20Wang%20and%20Bo%20Jiang%20and%20W.%20K.%20Chan%0AAbstract%3A%20%20%20Patch%20robustness%20certification%20is%20an%20emerging%20kind%20of%20defense%20technique%0Aagainst%20adversarial%20patch%20attacks%20with%20provable%20guarantees.%20There%20are%20two%0Aresearch%20lines%3A%20certified%20recovery%20and%20certified%20detection.%20They%20aim%20to%20label%0Amalicious%20samples%20with%20provable%20guarantees%20correctly%20and%20issue%20warnings%20for%0Amalicious%20samples%20predicted%20to%20non-benign%20labels%20with%20provable%20guarantees%2C%0Arespectively.%20However%2C%20existing%20certified%20detection%20defenders%20suffer%20from%0Aprotecting%20labels%20subject%20to%20manipulation%2C%20and%20existing%20certified%20recovery%0Adefenders%20cannot%20systematically%20warn%20samples%20about%20their%20labels.%20A%20certified%0Adefense%20that%20simultaneously%20offers%20robust%20labels%20and%20systematic%20warning%0Aprotection%20against%20patch%20attacks%20is%20desirable.%20This%20paper%20proposes%20a%20novel%0Acertified%20defense%20technique%20called%20CrossCert.%20CrossCert%20formulates%20a%20novel%0Aapproach%20by%20cross-checking%20two%20certified%20recovery%20defenders%20to%20provide%0Aunwavering%20certification%20and%20detection%20certification.%20Unwavering%20certification%0Aensures%20that%20a%20certified%20sample%2C%20when%20subjected%20to%20a%20patched%20perturbation%2C%20will%0Aalways%20be%20returned%20with%20a%20benign%20label%20without%20triggering%20any%20warnings%20with%20a%0Aprovable%20guarantee.%20To%20our%20knowledge%2C%20CrossCert%20is%20the%20first%20certified%0Adetection%20technique%20to%20offer%20this%20guarantee.%20Our%20experiments%20show%20that%2C%20with%20a%0Aslightly%20lower%20performance%20than%20ViP%20and%20comparable%20performance%20with%20PatchCensor%0Ain%20terms%20of%20detection%20certification%2C%20CrossCert%20certifies%20a%20significant%0Aproportion%20of%20samples%20with%20the%20guarantee%20of%20unwavering%20certification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrossCert%253A%2520A%2520Cross-Checking%2520Detection%2520Approach%2520to%2520Patch%2520Robustness%250A%2520%2520Certification%2520for%2520Deep%2520Learning%2520Models%26entry.906535625%3DQilin%2520Zhou%2520and%2520Zhengyuan%2520Wei%2520and%2520Haipeng%2520Wang%2520and%2520Bo%2520Jiang%2520and%2520W.%2520K.%2520Chan%26entry.1292438233%3D%2520%2520Patch%2520robustness%2520certification%2520is%2520an%2520emerging%2520kind%2520of%2520defense%2520technique%250Aagainst%2520adversarial%2520patch%2520attacks%2520with%2520provable%2520guarantees.%2520There%2520are%2520two%250Aresearch%2520lines%253A%2520certified%2520recovery%2520and%2520certified%2520detection.%2520They%2520aim%2520to%2520label%250Amalicious%2520samples%2520with%2520provable%2520guarantees%2520correctly%2520and%2520issue%2520warnings%2520for%250Amalicious%2520samples%2520predicted%2520to%2520non-benign%2520labels%2520with%2520provable%2520guarantees%252C%250Arespectively.%2520However%252C%2520existing%2520certified%2520detection%2520defenders%2520suffer%2520from%250Aprotecting%2520labels%2520subject%2520to%2520manipulation%252C%2520and%2520existing%2520certified%2520recovery%250Adefenders%2520cannot%2520systematically%2520warn%2520samples%2520about%2520their%2520labels.%2520A%2520certified%250Adefense%2520that%2520simultaneously%2520offers%2520robust%2520labels%2520and%2520systematic%2520warning%250Aprotection%2520against%2520patch%2520attacks%2520is%2520desirable.%2520This%2520paper%2520proposes%2520a%2520novel%250Acertified%2520defense%2520technique%2520called%2520CrossCert.%2520CrossCert%2520formulates%2520a%2520novel%250Aapproach%2520by%2520cross-checking%2520two%2520certified%2520recovery%2520defenders%2520to%2520provide%250Aunwavering%2520certification%2520and%2520detection%2520certification.%2520Unwavering%2520certification%250Aensures%2520that%2520a%2520certified%2520sample%252C%2520when%2520subjected%2520to%2520a%2520patched%2520perturbation%252C%2520will%250Aalways%2520be%2520returned%2520with%2520a%2520benign%2520label%2520without%2520triggering%2520any%2520warnings%2520with%2520a%250Aprovable%2520guarantee.%2520To%2520our%2520knowledge%252C%2520CrossCert%2520is%2520the%2520first%2520certified%250Adetection%2520technique%2520to%2520offer%2520this%2520guarantee.%2520Our%2520experiments%2520show%2520that%252C%2520with%2520a%250Aslightly%2520lower%2520performance%2520than%2520ViP%2520and%2520comparable%2520performance%2520with%2520PatchCensor%250Ain%2520terms%2520of%2520detection%2520certification%252C%2520CrossCert%2520certifies%2520a%2520significant%250Aproportion%2520of%2520samples%2520with%2520the%2520guarantee%2520of%2520unwavering%2520certification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CrossCert%3A%20A%20Cross-Checking%20Detection%20Approach%20to%20Patch%20Robustness%0A%20%20Certification%20for%20Deep%20Learning%20Models&entry.906535625=Qilin%20Zhou%20and%20Zhengyuan%20Wei%20and%20Haipeng%20Wang%20and%20Bo%20Jiang%20and%20W.%20K.%20Chan&entry.1292438233=%20%20Patch%20robustness%20certification%20is%20an%20emerging%20kind%20of%20defense%20technique%0Aagainst%20adversarial%20patch%20attacks%20with%20provable%20guarantees.%20There%20are%20two%0Aresearch%20lines%3A%20certified%20recovery%20and%20certified%20detection.%20They%20aim%20to%20label%0Amalicious%20samples%20with%20provable%20guarantees%20correctly%20and%20issue%20warnings%20for%0Amalicious%20samples%20predicted%20to%20non-benign%20labels%20with%20provable%20guarantees%2C%0Arespectively.%20However%2C%20existing%20certified%20detection%20defenders%20suffer%20from%0Aprotecting%20labels%20subject%20to%20manipulation%2C%20and%20existing%20certified%20recovery%0Adefenders%20cannot%20systematically%20warn%20samples%20about%20their%20labels.%20A%20certified%0Adefense%20that%20simultaneously%20offers%20robust%20labels%20and%20systematic%20warning%0Aprotection%20against%20patch%20attacks%20is%20desirable.%20This%20paper%20proposes%20a%20novel%0Acertified%20defense%20technique%20called%20CrossCert.%20CrossCert%20formulates%20a%20novel%0Aapproach%20by%20cross-checking%20two%20certified%20recovery%20defenders%20to%20provide%0Aunwavering%20certification%20and%20detection%20certification.%20Unwavering%20certification%0Aensures%20that%20a%20certified%20sample%2C%20when%20subjected%20to%20a%20patched%20perturbation%2C%20will%0Aalways%20be%20returned%20with%20a%20benign%20label%20without%20triggering%20any%20warnings%20with%20a%0Aprovable%20guarantee.%20To%20our%20knowledge%2C%20CrossCert%20is%20the%20first%20certified%0Adetection%20technique%20to%20offer%20this%20guarantee.%20Our%20experiments%20show%20that%2C%20with%20a%0Aslightly%20lower%20performance%20than%20ViP%20and%20comparable%20performance%20with%20PatchCensor%0Ain%20terms%20of%20detection%20certification%2C%20CrossCert%20certifies%20a%20significant%0Aproportion%20of%20samples%20with%20the%20guarantee%20of%20unwavering%20certification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07668v1&entry.124074799=Read"},
{"title": "Multi-scale Wasserstein Shortest-path Graph Kernels for Graph\n  Classification", "author": "Wei Ye and Hao Tian and Qijun Chen", "abstract": "  Graph kernels are conventional methods for computing graph similarities.\nHowever, the existing R-convolution graph kernels cannot resolve both of the\ntwo challenges: 1) Comparing graphs at multiple different scales, and 2)\nConsidering the distributions of substructures when computing the kernel\nmatrix. These two challenges limit their performances. To mitigate both of the\ntwo challenges, we propose a novel graph kernel called the Multi-scale\nWasserstein Shortest-Path graph kernel (MWSP), at the heart of which is the\nmulti-scale shortest-path node feature map, of which each element denotes the\nnumber of occurrences of the shortest path around a node. The shortest path is\nrepresented by the concatenation of all the labels of nodes in it. Since the\nshortest-path node feature map can only compare graphs at local scales, we\nincorporate into it the multiple different scales of the graph structure, which\nare captured by the truncated BFS trees of different depths rooted at each node\nin a graph. We use the Wasserstein distance to compute the similarity between\nthe multi-scale shortest-path node feature maps of two graphs, considering the\ndistributions of shortest paths. We empirically validate MWSP on various\nbenchmark graph datasets and demonstrate that it achieves state-of-the-art\nperformance on most datasets.\n", "link": "http://arxiv.org/abs/2206.00979v5", "date": "2024-05-13", "relevancy": 1.7877, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4482}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4478}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-scale%20Wasserstein%20Shortest-path%20Graph%20Kernels%20for%20Graph%0A%20%20Classification&body=Title%3A%20Multi-scale%20Wasserstein%20Shortest-path%20Graph%20Kernels%20for%20Graph%0A%20%20Classification%0AAuthor%3A%20Wei%20Ye%20and%20Hao%20Tian%20and%20Qijun%20Chen%0AAbstract%3A%20%20%20Graph%20kernels%20are%20conventional%20methods%20for%20computing%20graph%20similarities.%0AHowever%2C%20the%20existing%20R-convolution%20graph%20kernels%20cannot%20resolve%20both%20of%20the%0Atwo%20challenges%3A%201%29%20Comparing%20graphs%20at%20multiple%20different%20scales%2C%20and%202%29%0AConsidering%20the%20distributions%20of%20substructures%20when%20computing%20the%20kernel%0Amatrix.%20These%20two%20challenges%20limit%20their%20performances.%20To%20mitigate%20both%20of%20the%0Atwo%20challenges%2C%20we%20propose%20a%20novel%20graph%20kernel%20called%20the%20Multi-scale%0AWasserstein%20Shortest-Path%20graph%20kernel%20%28MWSP%29%2C%20at%20the%20heart%20of%20which%20is%20the%0Amulti-scale%20shortest-path%20node%20feature%20map%2C%20of%20which%20each%20element%20denotes%20the%0Anumber%20of%20occurrences%20of%20the%20shortest%20path%20around%20a%20node.%20The%20shortest%20path%20is%0Arepresented%20by%20the%20concatenation%20of%20all%20the%20labels%20of%20nodes%20in%20it.%20Since%20the%0Ashortest-path%20node%20feature%20map%20can%20only%20compare%20graphs%20at%20local%20scales%2C%20we%0Aincorporate%20into%20it%20the%20multiple%20different%20scales%20of%20the%20graph%20structure%2C%20which%0Aare%20captured%20by%20the%20truncated%20BFS%20trees%20of%20different%20depths%20rooted%20at%20each%20node%0Ain%20a%20graph.%20We%20use%20the%20Wasserstein%20distance%20to%20compute%20the%20similarity%20between%0Athe%20multi-scale%20shortest-path%20node%20feature%20maps%20of%20two%20graphs%2C%20considering%20the%0Adistributions%20of%20shortest%20paths.%20We%20empirically%20validate%20MWSP%20on%20various%0Abenchmark%20graph%20datasets%20and%20demonstrate%20that%20it%20achieves%20state-of-the-art%0Aperformance%20on%20most%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2206.00979v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-scale%2520Wasserstein%2520Shortest-path%2520Graph%2520Kernels%2520for%2520Graph%250A%2520%2520Classification%26entry.906535625%3DWei%2520Ye%2520and%2520Hao%2520Tian%2520and%2520Qijun%2520Chen%26entry.1292438233%3D%2520%2520Graph%2520kernels%2520are%2520conventional%2520methods%2520for%2520computing%2520graph%2520similarities.%250AHowever%252C%2520the%2520existing%2520R-convolution%2520graph%2520kernels%2520cannot%2520resolve%2520both%2520of%2520the%250Atwo%2520challenges%253A%25201%2529%2520Comparing%2520graphs%2520at%2520multiple%2520different%2520scales%252C%2520and%25202%2529%250AConsidering%2520the%2520distributions%2520of%2520substructures%2520when%2520computing%2520the%2520kernel%250Amatrix.%2520These%2520two%2520challenges%2520limit%2520their%2520performances.%2520To%2520mitigate%2520both%2520of%2520the%250Atwo%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520graph%2520kernel%2520called%2520the%2520Multi-scale%250AWasserstein%2520Shortest-Path%2520graph%2520kernel%2520%2528MWSP%2529%252C%2520at%2520the%2520heart%2520of%2520which%2520is%2520the%250Amulti-scale%2520shortest-path%2520node%2520feature%2520map%252C%2520of%2520which%2520each%2520element%2520denotes%2520the%250Anumber%2520of%2520occurrences%2520of%2520the%2520shortest%2520path%2520around%2520a%2520node.%2520The%2520shortest%2520path%2520is%250Arepresented%2520by%2520the%2520concatenation%2520of%2520all%2520the%2520labels%2520of%2520nodes%2520in%2520it.%2520Since%2520the%250Ashortest-path%2520node%2520feature%2520map%2520can%2520only%2520compare%2520graphs%2520at%2520local%2520scales%252C%2520we%250Aincorporate%2520into%2520it%2520the%2520multiple%2520different%2520scales%2520of%2520the%2520graph%2520structure%252C%2520which%250Aare%2520captured%2520by%2520the%2520truncated%2520BFS%2520trees%2520of%2520different%2520depths%2520rooted%2520at%2520each%2520node%250Ain%2520a%2520graph.%2520We%2520use%2520the%2520Wasserstein%2520distance%2520to%2520compute%2520the%2520similarity%2520between%250Athe%2520multi-scale%2520shortest-path%2520node%2520feature%2520maps%2520of%2520two%2520graphs%252C%2520considering%2520the%250Adistributions%2520of%2520shortest%2520paths.%2520We%2520empirically%2520validate%2520MWSP%2520on%2520various%250Abenchmark%2520graph%2520datasets%2520and%2520demonstrate%2520that%2520it%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520most%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2206.00979v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-scale%20Wasserstein%20Shortest-path%20Graph%20Kernels%20for%20Graph%0A%20%20Classification&entry.906535625=Wei%20Ye%20and%20Hao%20Tian%20and%20Qijun%20Chen&entry.1292438233=%20%20Graph%20kernels%20are%20conventional%20methods%20for%20computing%20graph%20similarities.%0AHowever%2C%20the%20existing%20R-convolution%20graph%20kernels%20cannot%20resolve%20both%20of%20the%0Atwo%20challenges%3A%201%29%20Comparing%20graphs%20at%20multiple%20different%20scales%2C%20and%202%29%0AConsidering%20the%20distributions%20of%20substructures%20when%20computing%20the%20kernel%0Amatrix.%20These%20two%20challenges%20limit%20their%20performances.%20To%20mitigate%20both%20of%20the%0Atwo%20challenges%2C%20we%20propose%20a%20novel%20graph%20kernel%20called%20the%20Multi-scale%0AWasserstein%20Shortest-Path%20graph%20kernel%20%28MWSP%29%2C%20at%20the%20heart%20of%20which%20is%20the%0Amulti-scale%20shortest-path%20node%20feature%20map%2C%20of%20which%20each%20element%20denotes%20the%0Anumber%20of%20occurrences%20of%20the%20shortest%20path%20around%20a%20node.%20The%20shortest%20path%20is%0Arepresented%20by%20the%20concatenation%20of%20all%20the%20labels%20of%20nodes%20in%20it.%20Since%20the%0Ashortest-path%20node%20feature%20map%20can%20only%20compare%20graphs%20at%20local%20scales%2C%20we%0Aincorporate%20into%20it%20the%20multiple%20different%20scales%20of%20the%20graph%20structure%2C%20which%0Aare%20captured%20by%20the%20truncated%20BFS%20trees%20of%20different%20depths%20rooted%20at%20each%20node%0Ain%20a%20graph.%20We%20use%20the%20Wasserstein%20distance%20to%20compute%20the%20similarity%20between%0Athe%20multi-scale%20shortest-path%20node%20feature%20maps%20of%20two%20graphs%2C%20considering%20the%0Adistributions%20of%20shortest%20paths.%20We%20empirically%20validate%20MWSP%20on%20various%0Abenchmark%20graph%20datasets%20and%20demonstrate%20that%20it%20achieves%20state-of-the-art%0Aperformance%20on%20most%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2206.00979v5&entry.124074799=Read"},
{"title": "Transferable Neural Wavefunctions for Solids", "author": "Leon Gerard and Michael Scherbela and Halvard Sutterud and Matthew Foulkes and Philipp Grohs", "abstract": "  Deep-Learning-based Variational Monte Carlo (DL-VMC) has recently emerged as\na highly accurate approach for finding approximate solutions to the\nmany-electron Schr\\\"odinger equation. Despite its favorable scaling with the\nnumber of electrons, $\\mathcal{O}(n_\\text{el}^{4})$, the practical value of\nDL-VMC is limited by the high cost of optimizing the neural network weights for\nevery system studied. To mitigate this problem, recent research has proposed\noptimizing a single neural network across multiple systems, reducing the cost\nper system. Here we extend this approach to solids, where similar but distinct\ncalculations using different geometries, boundary conditions, and supercell\nsizes are often required. We show how to optimize a single ansatz across all of\nthese variations, reducing the required number of optimization steps by an\norder of magnitude. Furthermore, we exploit the transfer capabilities of a\npre-trained network. We successfully transfer a network, pre-trained on 2x2x2\nsupercells of LiH, to 3x3x3 supercells. This reduces the number of optimization\nsteps required to simulate the large system by a factor of 50 compared to\nprevious work.\n", "link": "http://arxiv.org/abs/2405.07599v1", "date": "2024-05-13", "relevancy": 1.7875, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4633}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4462}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transferable%20Neural%20Wavefunctions%20for%20Solids&body=Title%3A%20Transferable%20Neural%20Wavefunctions%20for%20Solids%0AAuthor%3A%20Leon%20Gerard%20and%20Michael%20Scherbela%20and%20Halvard%20Sutterud%20and%20Matthew%20Foulkes%20and%20Philipp%20Grohs%0AAbstract%3A%20%20%20Deep-Learning-based%20Variational%20Monte%20Carlo%20%28DL-VMC%29%20has%20recently%20emerged%20as%0Aa%20highly%20accurate%20approach%20for%20finding%20approximate%20solutions%20to%20the%0Amany-electron%20Schr%5C%22odinger%20equation.%20Despite%20its%20favorable%20scaling%20with%20the%0Anumber%20of%20electrons%2C%20%24%5Cmathcal%7BO%7D%28n_%5Ctext%7Bel%7D%5E%7B4%7D%29%24%2C%20the%20practical%20value%20of%0ADL-VMC%20is%20limited%20by%20the%20high%20cost%20of%20optimizing%20the%20neural%20network%20weights%20for%0Aevery%20system%20studied.%20To%20mitigate%20this%20problem%2C%20recent%20research%20has%20proposed%0Aoptimizing%20a%20single%20neural%20network%20across%20multiple%20systems%2C%20reducing%20the%20cost%0Aper%20system.%20Here%20we%20extend%20this%20approach%20to%20solids%2C%20where%20similar%20but%20distinct%0Acalculations%20using%20different%20geometries%2C%20boundary%20conditions%2C%20and%20supercell%0Asizes%20are%20often%20required.%20We%20show%20how%20to%20optimize%20a%20single%20ansatz%20across%20all%20of%0Athese%20variations%2C%20reducing%20the%20required%20number%20of%20optimization%20steps%20by%20an%0Aorder%20of%20magnitude.%20Furthermore%2C%20we%20exploit%20the%20transfer%20capabilities%20of%20a%0Apre-trained%20network.%20We%20successfully%20transfer%20a%20network%2C%20pre-trained%20on%202x2x2%0Asupercells%20of%20LiH%2C%20to%203x3x3%20supercells.%20This%20reduces%20the%20number%20of%20optimization%0Asteps%20required%20to%20simulate%20the%20large%20system%20by%20a%20factor%20of%2050%20compared%20to%0Aprevious%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransferable%2520Neural%2520Wavefunctions%2520for%2520Solids%26entry.906535625%3DLeon%2520Gerard%2520and%2520Michael%2520Scherbela%2520and%2520Halvard%2520Sutterud%2520and%2520Matthew%2520Foulkes%2520and%2520Philipp%2520Grohs%26entry.1292438233%3D%2520%2520Deep-Learning-based%2520Variational%2520Monte%2520Carlo%2520%2528DL-VMC%2529%2520has%2520recently%2520emerged%2520as%250Aa%2520highly%2520accurate%2520approach%2520for%2520finding%2520approximate%2520solutions%2520to%2520the%250Amany-electron%2520Schr%255C%2522odinger%2520equation.%2520Despite%2520its%2520favorable%2520scaling%2520with%2520the%250Anumber%2520of%2520electrons%252C%2520%2524%255Cmathcal%257BO%257D%2528n_%255Ctext%257Bel%257D%255E%257B4%257D%2529%2524%252C%2520the%2520practical%2520value%2520of%250ADL-VMC%2520is%2520limited%2520by%2520the%2520high%2520cost%2520of%2520optimizing%2520the%2520neural%2520network%2520weights%2520for%250Aevery%2520system%2520studied.%2520To%2520mitigate%2520this%2520problem%252C%2520recent%2520research%2520has%2520proposed%250Aoptimizing%2520a%2520single%2520neural%2520network%2520across%2520multiple%2520systems%252C%2520reducing%2520the%2520cost%250Aper%2520system.%2520Here%2520we%2520extend%2520this%2520approach%2520to%2520solids%252C%2520where%2520similar%2520but%2520distinct%250Acalculations%2520using%2520different%2520geometries%252C%2520boundary%2520conditions%252C%2520and%2520supercell%250Asizes%2520are%2520often%2520required.%2520We%2520show%2520how%2520to%2520optimize%2520a%2520single%2520ansatz%2520across%2520all%2520of%250Athese%2520variations%252C%2520reducing%2520the%2520required%2520number%2520of%2520optimization%2520steps%2520by%2520an%250Aorder%2520of%2520magnitude.%2520Furthermore%252C%2520we%2520exploit%2520the%2520transfer%2520capabilities%2520of%2520a%250Apre-trained%2520network.%2520We%2520successfully%2520transfer%2520a%2520network%252C%2520pre-trained%2520on%25202x2x2%250Asupercells%2520of%2520LiH%252C%2520to%25203x3x3%2520supercells.%2520This%2520reduces%2520the%2520number%2520of%2520optimization%250Asteps%2520required%2520to%2520simulate%2520the%2520large%2520system%2520by%2520a%2520factor%2520of%252050%2520compared%2520to%250Aprevious%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transferable%20Neural%20Wavefunctions%20for%20Solids&entry.906535625=Leon%20Gerard%20and%20Michael%20Scherbela%20and%20Halvard%20Sutterud%20and%20Matthew%20Foulkes%20and%20Philipp%20Grohs&entry.1292438233=%20%20Deep-Learning-based%20Variational%20Monte%20Carlo%20%28DL-VMC%29%20has%20recently%20emerged%20as%0Aa%20highly%20accurate%20approach%20for%20finding%20approximate%20solutions%20to%20the%0Amany-electron%20Schr%5C%22odinger%20equation.%20Despite%20its%20favorable%20scaling%20with%20the%0Anumber%20of%20electrons%2C%20%24%5Cmathcal%7BO%7D%28n_%5Ctext%7Bel%7D%5E%7B4%7D%29%24%2C%20the%20practical%20value%20of%0ADL-VMC%20is%20limited%20by%20the%20high%20cost%20of%20optimizing%20the%20neural%20network%20weights%20for%0Aevery%20system%20studied.%20To%20mitigate%20this%20problem%2C%20recent%20research%20has%20proposed%0Aoptimizing%20a%20single%20neural%20network%20across%20multiple%20systems%2C%20reducing%20the%20cost%0Aper%20system.%20Here%20we%20extend%20this%20approach%20to%20solids%2C%20where%20similar%20but%20distinct%0Acalculations%20using%20different%20geometries%2C%20boundary%20conditions%2C%20and%20supercell%0Asizes%20are%20often%20required.%20We%20show%20how%20to%20optimize%20a%20single%20ansatz%20across%20all%20of%0Athese%20variations%2C%20reducing%20the%20required%20number%20of%20optimization%20steps%20by%20an%0Aorder%20of%20magnitude.%20Furthermore%2C%20we%20exploit%20the%20transfer%20capabilities%20of%20a%0Apre-trained%20network.%20We%20successfully%20transfer%20a%20network%2C%20pre-trained%20on%202x2x2%0Asupercells%20of%20LiH%2C%20to%203x3x3%20supercells.%20This%20reduces%20the%20number%20of%20optimization%0Asteps%20required%20to%20simulate%20the%20large%20system%20by%20a%20factor%20of%2050%20compared%20to%0Aprevious%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07599v1&entry.124074799=Read"},
{"title": "Prospects for AI-Enhanced ECG as a Unified Screening Tool for Cardiac\n  and Non-Cardiac Conditions -- An Explorative Study in Emergency Care", "author": "Nils Strodthoff and Juan Miguel Lopez Alcaraz and Wilhelm Haverkamp", "abstract": "  Current deep learning algorithms designed for automatic ECG analysis have\nexhibited notable accuracy. However, akin to traditional electrocardiography,\nthey tend to be narrowly focused and typically address a singular diagnostic\ncondition. In this exploratory study, we specifically investigate the\ncapability of a single model to predict a diverse range of both cardiac and\nnon-cardiac discharge diagnoses based on a sole ECG collected in the emergency\ndepartment. We find that 253, 81 cardiac, and 172 non-cardiac, ICD codes can be\nreliably predicted in the sense of exceeding an AUROC score of 0.8 in a\nstatistically significant manner. This underscores the model's proficiency in\nhandling a wide array of cardiac and non-cardiac diagnostic scenarios which\ndemonstrates potential as a screening tool for diverse medical encounters.\n", "link": "http://arxiv.org/abs/2312.11050v2", "date": "2024-05-13", "relevancy": 1.7791, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4691}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4478}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prospects%20for%20AI-Enhanced%20ECG%20as%20a%20Unified%20Screening%20Tool%20for%20Cardiac%0A%20%20and%20Non-Cardiac%20Conditions%20--%20An%20Explorative%20Study%20in%20Emergency%20Care&body=Title%3A%20Prospects%20for%20AI-Enhanced%20ECG%20as%20a%20Unified%20Screening%20Tool%20for%20Cardiac%0A%20%20and%20Non-Cardiac%20Conditions%20--%20An%20Explorative%20Study%20in%20Emergency%20Care%0AAuthor%3A%20Nils%20Strodthoff%20and%20Juan%20Miguel%20Lopez%20Alcaraz%20and%20Wilhelm%20Haverkamp%0AAbstract%3A%20%20%20Current%20deep%20learning%20algorithms%20designed%20for%20automatic%20ECG%20analysis%20have%0Aexhibited%20notable%20accuracy.%20However%2C%20akin%20to%20traditional%20electrocardiography%2C%0Athey%20tend%20to%20be%20narrowly%20focused%20and%20typically%20address%20a%20singular%20diagnostic%0Acondition.%20In%20this%20exploratory%20study%2C%20we%20specifically%20investigate%20the%0Acapability%20of%20a%20single%20model%20to%20predict%20a%20diverse%20range%20of%20both%20cardiac%20and%0Anon-cardiac%20discharge%20diagnoses%20based%20on%20a%20sole%20ECG%20collected%20in%20the%20emergency%0Adepartment.%20We%20find%20that%20253%2C%2081%20cardiac%2C%20and%20172%20non-cardiac%2C%20ICD%20codes%20can%20be%0Areliably%20predicted%20in%20the%20sense%20of%20exceeding%20an%20AUROC%20score%20of%200.8%20in%20a%0Astatistically%20significant%20manner.%20This%20underscores%20the%20model%27s%20proficiency%20in%0Ahandling%20a%20wide%20array%20of%20cardiac%20and%20non-cardiac%20diagnostic%20scenarios%20which%0Ademonstrates%20potential%20as%20a%20screening%20tool%20for%20diverse%20medical%20encounters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.11050v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProspects%2520for%2520AI-Enhanced%2520ECG%2520as%2520a%2520Unified%2520Screening%2520Tool%2520for%2520Cardiac%250A%2520%2520and%2520Non-Cardiac%2520Conditions%2520--%2520An%2520Explorative%2520Study%2520in%2520Emergency%2520Care%26entry.906535625%3DNils%2520Strodthoff%2520and%2520Juan%2520Miguel%2520Lopez%2520Alcaraz%2520and%2520Wilhelm%2520Haverkamp%26entry.1292438233%3D%2520%2520Current%2520deep%2520learning%2520algorithms%2520designed%2520for%2520automatic%2520ECG%2520analysis%2520have%250Aexhibited%2520notable%2520accuracy.%2520However%252C%2520akin%2520to%2520traditional%2520electrocardiography%252C%250Athey%2520tend%2520to%2520be%2520narrowly%2520focused%2520and%2520typically%2520address%2520a%2520singular%2520diagnostic%250Acondition.%2520In%2520this%2520exploratory%2520study%252C%2520we%2520specifically%2520investigate%2520the%250Acapability%2520of%2520a%2520single%2520model%2520to%2520predict%2520a%2520diverse%2520range%2520of%2520both%2520cardiac%2520and%250Anon-cardiac%2520discharge%2520diagnoses%2520based%2520on%2520a%2520sole%2520ECG%2520collected%2520in%2520the%2520emergency%250Adepartment.%2520We%2520find%2520that%2520253%252C%252081%2520cardiac%252C%2520and%2520172%2520non-cardiac%252C%2520ICD%2520codes%2520can%2520be%250Areliably%2520predicted%2520in%2520the%2520sense%2520of%2520exceeding%2520an%2520AUROC%2520score%2520of%25200.8%2520in%2520a%250Astatistically%2520significant%2520manner.%2520This%2520underscores%2520the%2520model%2527s%2520proficiency%2520in%250Ahandling%2520a%2520wide%2520array%2520of%2520cardiac%2520and%2520non-cardiac%2520diagnostic%2520scenarios%2520which%250Ademonstrates%2520potential%2520as%2520a%2520screening%2520tool%2520for%2520diverse%2520medical%2520encounters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.11050v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prospects%20for%20AI-Enhanced%20ECG%20as%20a%20Unified%20Screening%20Tool%20for%20Cardiac%0A%20%20and%20Non-Cardiac%20Conditions%20--%20An%20Explorative%20Study%20in%20Emergency%20Care&entry.906535625=Nils%20Strodthoff%20and%20Juan%20Miguel%20Lopez%20Alcaraz%20and%20Wilhelm%20Haverkamp&entry.1292438233=%20%20Current%20deep%20learning%20algorithms%20designed%20for%20automatic%20ECG%20analysis%20have%0Aexhibited%20notable%20accuracy.%20However%2C%20akin%20to%20traditional%20electrocardiography%2C%0Athey%20tend%20to%20be%20narrowly%20focused%20and%20typically%20address%20a%20singular%20diagnostic%0Acondition.%20In%20this%20exploratory%20study%2C%20we%20specifically%20investigate%20the%0Acapability%20of%20a%20single%20model%20to%20predict%20a%20diverse%20range%20of%20both%20cardiac%20and%0Anon-cardiac%20discharge%20diagnoses%20based%20on%20a%20sole%20ECG%20collected%20in%20the%20emergency%0Adepartment.%20We%20find%20that%20253%2C%2081%20cardiac%2C%20and%20172%20non-cardiac%2C%20ICD%20codes%20can%20be%0Areliably%20predicted%20in%20the%20sense%20of%20exceeding%20an%20AUROC%20score%20of%200.8%20in%20a%0Astatistically%20significant%20manner.%20This%20underscores%20the%20model%27s%20proficiency%20in%0Ahandling%20a%20wide%20array%20of%20cardiac%20and%20non-cardiac%20diagnostic%20scenarios%20which%0Ademonstrates%20potential%20as%20a%20screening%20tool%20for%20diverse%20medical%20encounters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.11050v2&entry.124074799=Read"},
{"title": "Physically Consistent Online Inertial Adaptation for Humanoid\n  Loco-manipulation", "author": "James Foster and Stephen McCrory and Christian DeBuys and Sylvain Bertrand and Robert Griffin", "abstract": "  The ability to accomplish manipulation and locomotion tasks in the presence\nof significant time-varying external loads is a remarkable skill of humans that\nhas yet to be replicated convincingly by humanoid robots. Such an ability will\nbe a key requirement in the environments we envision deploying our robots:\ndull, dirty, and dangerous. External loads constitute a large model bias, which\nis typically unaccounted for. In this work, we enable our humanoid robot to\nengage in loco-manipulation tasks in the presence of significant model bias due\nto external loads. We propose an online estimation and control framework\ninvolving the combination of a physically consistent extended Kalman filter for\ninertial parameter estimation coupled to a whole-body controller. We showcase\nour results both in simulation and in hardware, where weights are mounted on\nNadia's wrist links as a proxy for engaging in tasks where large external loads\nare applied to the robot.\n", "link": "http://arxiv.org/abs/2405.07901v1", "date": "2024-05-13", "relevancy": 1.7783, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6356}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5858}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physically%20Consistent%20Online%20Inertial%20Adaptation%20for%20Humanoid%0A%20%20Loco-manipulation&body=Title%3A%20Physically%20Consistent%20Online%20Inertial%20Adaptation%20for%20Humanoid%0A%20%20Loco-manipulation%0AAuthor%3A%20James%20Foster%20and%20Stephen%20McCrory%20and%20Christian%20DeBuys%20and%20Sylvain%20Bertrand%20and%20Robert%20Griffin%0AAbstract%3A%20%20%20The%20ability%20to%20accomplish%20manipulation%20and%20locomotion%20tasks%20in%20the%20presence%0Aof%20significant%20time-varying%20external%20loads%20is%20a%20remarkable%20skill%20of%20humans%20that%0Ahas%20yet%20to%20be%20replicated%20convincingly%20by%20humanoid%20robots.%20Such%20an%20ability%20will%0Abe%20a%20key%20requirement%20in%20the%20environments%20we%20envision%20deploying%20our%20robots%3A%0Adull%2C%20dirty%2C%20and%20dangerous.%20External%20loads%20constitute%20a%20large%20model%20bias%2C%20which%0Ais%20typically%20unaccounted%20for.%20In%20this%20work%2C%20we%20enable%20our%20humanoid%20robot%20to%0Aengage%20in%20loco-manipulation%20tasks%20in%20the%20presence%20of%20significant%20model%20bias%20due%0Ato%20external%20loads.%20We%20propose%20an%20online%20estimation%20and%20control%20framework%0Ainvolving%20the%20combination%20of%20a%20physically%20consistent%20extended%20Kalman%20filter%20for%0Ainertial%20parameter%20estimation%20coupled%20to%20a%20whole-body%20controller.%20We%20showcase%0Aour%20results%20both%20in%20simulation%20and%20in%20hardware%2C%20where%20weights%20are%20mounted%20on%0ANadia%27s%20wrist%20links%20as%20a%20proxy%20for%20engaging%20in%20tasks%20where%20large%20external%20loads%0Aare%20applied%20to%20the%20robot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysically%2520Consistent%2520Online%2520Inertial%2520Adaptation%2520for%2520Humanoid%250A%2520%2520Loco-manipulation%26entry.906535625%3DJames%2520Foster%2520and%2520Stephen%2520McCrory%2520and%2520Christian%2520DeBuys%2520and%2520Sylvain%2520Bertrand%2520and%2520Robert%2520Griffin%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520accomplish%2520manipulation%2520and%2520locomotion%2520tasks%2520in%2520the%2520presence%250Aof%2520significant%2520time-varying%2520external%2520loads%2520is%2520a%2520remarkable%2520skill%2520of%2520humans%2520that%250Ahas%2520yet%2520to%2520be%2520replicated%2520convincingly%2520by%2520humanoid%2520robots.%2520Such%2520an%2520ability%2520will%250Abe%2520a%2520key%2520requirement%2520in%2520the%2520environments%2520we%2520envision%2520deploying%2520our%2520robots%253A%250Adull%252C%2520dirty%252C%2520and%2520dangerous.%2520External%2520loads%2520constitute%2520a%2520large%2520model%2520bias%252C%2520which%250Ais%2520typically%2520unaccounted%2520for.%2520In%2520this%2520work%252C%2520we%2520enable%2520our%2520humanoid%2520robot%2520to%250Aengage%2520in%2520loco-manipulation%2520tasks%2520in%2520the%2520presence%2520of%2520significant%2520model%2520bias%2520due%250Ato%2520external%2520loads.%2520We%2520propose%2520an%2520online%2520estimation%2520and%2520control%2520framework%250Ainvolving%2520the%2520combination%2520of%2520a%2520physically%2520consistent%2520extended%2520Kalman%2520filter%2520for%250Ainertial%2520parameter%2520estimation%2520coupled%2520to%2520a%2520whole-body%2520controller.%2520We%2520showcase%250Aour%2520results%2520both%2520in%2520simulation%2520and%2520in%2520hardware%252C%2520where%2520weights%2520are%2520mounted%2520on%250ANadia%2527s%2520wrist%2520links%2520as%2520a%2520proxy%2520for%2520engaging%2520in%2520tasks%2520where%2520large%2520external%2520loads%250Aare%2520applied%2520to%2520the%2520robot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physically%20Consistent%20Online%20Inertial%20Adaptation%20for%20Humanoid%0A%20%20Loco-manipulation&entry.906535625=James%20Foster%20and%20Stephen%20McCrory%20and%20Christian%20DeBuys%20and%20Sylvain%20Bertrand%20and%20Robert%20Griffin&entry.1292438233=%20%20The%20ability%20to%20accomplish%20manipulation%20and%20locomotion%20tasks%20in%20the%20presence%0Aof%20significant%20time-varying%20external%20loads%20is%20a%20remarkable%20skill%20of%20humans%20that%0Ahas%20yet%20to%20be%20replicated%20convincingly%20by%20humanoid%20robots.%20Such%20an%20ability%20will%0Abe%20a%20key%20requirement%20in%20the%20environments%20we%20envision%20deploying%20our%20robots%3A%0Adull%2C%20dirty%2C%20and%20dangerous.%20External%20loads%20constitute%20a%20large%20model%20bias%2C%20which%0Ais%20typically%20unaccounted%20for.%20In%20this%20work%2C%20we%20enable%20our%20humanoid%20robot%20to%0Aengage%20in%20loco-manipulation%20tasks%20in%20the%20presence%20of%20significant%20model%20bias%20due%0Ato%20external%20loads.%20We%20propose%20an%20online%20estimation%20and%20control%20framework%0Ainvolving%20the%20combination%20of%20a%20physically%20consistent%20extended%20Kalman%20filter%20for%0Ainertial%20parameter%20estimation%20coupled%20to%20a%20whole-body%20controller.%20We%20showcase%0Aour%20results%20both%20in%20simulation%20and%20in%20hardware%2C%20where%20weights%20are%20mounted%20on%0ANadia%27s%20wrist%20links%20as%20a%20proxy%20for%20engaging%20in%20tasks%20where%20large%20external%20loads%0Aare%20applied%20to%20the%20robot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07901v1&entry.124074799=Read"},
{"title": "Frequency-Time Diffusion with Neural Cellular Automata", "author": "John Kalkhof and Arlene K\u00fchn and Yannik Frisch and Anirban Mukhopadhyay", "abstract": "  Despite considerable success, large Denoising Diffusion Models (DDMs) with\nUNet backbone pose practical challenges, particularly on limited hardware and\nin processing gigapixel images. To address these limitations, we introduce two\nNeural Cellular Automata (NCA)-based DDMs: Diff-NCA and FourierDiff-NCA.\nCapitalizing on the local communication capabilities of NCA, Diff-NCA\nsignificantly reduces the parameter counts of NCA-based DDMs. Integrating\nFourier-based diffusion enables global communication early in the diffusion\nprocess. This feature is particularly valuable in synthesizing complex images\nwith important global features, such as the CelebA dataset. We demonstrate that\neven a 331k parameter Diff-NCA can generate 512x512 pathology slices, while\nFourierDiff-NCA (1.1m parameters) reaches a three times lower FID score of\n43.86, compared to the four times bigger UNet (3.94m parameters) with a score\nof 128.2. Additionally, FourierDiff-NCA can perform diverse tasks such as\nsuper-resolution, out-of-distribution image synthesis, and inpainting without\nexplicit training.\n", "link": "http://arxiv.org/abs/2401.06291v2", "date": "2024-05-13", "relevancy": 1.7766, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6624}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5792}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frequency-Time%20Diffusion%20with%20Neural%20Cellular%20Automata&body=Title%3A%20Frequency-Time%20Diffusion%20with%20Neural%20Cellular%20Automata%0AAuthor%3A%20John%20Kalkhof%20and%20Arlene%20K%C3%BChn%20and%20Yannik%20Frisch%20and%20Anirban%20Mukhopadhyay%0AAbstract%3A%20%20%20Despite%20considerable%20success%2C%20large%20Denoising%20Diffusion%20Models%20%28DDMs%29%20with%0AUNet%20backbone%20pose%20practical%20challenges%2C%20particularly%20on%20limited%20hardware%20and%0Ain%20processing%20gigapixel%20images.%20To%20address%20these%20limitations%2C%20we%20introduce%20two%0ANeural%20Cellular%20Automata%20%28NCA%29-based%20DDMs%3A%20Diff-NCA%20and%20FourierDiff-NCA.%0ACapitalizing%20on%20the%20local%20communication%20capabilities%20of%20NCA%2C%20Diff-NCA%0Asignificantly%20reduces%20the%20parameter%20counts%20of%20NCA-based%20DDMs.%20Integrating%0AFourier-based%20diffusion%20enables%20global%20communication%20early%20in%20the%20diffusion%0Aprocess.%20This%20feature%20is%20particularly%20valuable%20in%20synthesizing%20complex%20images%0Awith%20important%20global%20features%2C%20such%20as%20the%20CelebA%20dataset.%20We%20demonstrate%20that%0Aeven%20a%20331k%20parameter%20Diff-NCA%20can%20generate%20512x512%20pathology%20slices%2C%20while%0AFourierDiff-NCA%20%281.1m%20parameters%29%20reaches%20a%20three%20times%20lower%20FID%20score%20of%0A43.86%2C%20compared%20to%20the%20four%20times%20bigger%20UNet%20%283.94m%20parameters%29%20with%20a%20score%0Aof%20128.2.%20Additionally%2C%20FourierDiff-NCA%20can%20perform%20diverse%20tasks%20such%20as%0Asuper-resolution%2C%20out-of-distribution%20image%20synthesis%2C%20and%20inpainting%20without%0Aexplicit%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.06291v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrequency-Time%2520Diffusion%2520with%2520Neural%2520Cellular%2520Automata%26entry.906535625%3DJohn%2520Kalkhof%2520and%2520Arlene%2520K%25C3%25BChn%2520and%2520Yannik%2520Frisch%2520and%2520Anirban%2520Mukhopadhyay%26entry.1292438233%3D%2520%2520Despite%2520considerable%2520success%252C%2520large%2520Denoising%2520Diffusion%2520Models%2520%2528DDMs%2529%2520with%250AUNet%2520backbone%2520pose%2520practical%2520challenges%252C%2520particularly%2520on%2520limited%2520hardware%2520and%250Ain%2520processing%2520gigapixel%2520images.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520two%250ANeural%2520Cellular%2520Automata%2520%2528NCA%2529-based%2520DDMs%253A%2520Diff-NCA%2520and%2520FourierDiff-NCA.%250ACapitalizing%2520on%2520the%2520local%2520communication%2520capabilities%2520of%2520NCA%252C%2520Diff-NCA%250Asignificantly%2520reduces%2520the%2520parameter%2520counts%2520of%2520NCA-based%2520DDMs.%2520Integrating%250AFourier-based%2520diffusion%2520enables%2520global%2520communication%2520early%2520in%2520the%2520diffusion%250Aprocess.%2520This%2520feature%2520is%2520particularly%2520valuable%2520in%2520synthesizing%2520complex%2520images%250Awith%2520important%2520global%2520features%252C%2520such%2520as%2520the%2520CelebA%2520dataset.%2520We%2520demonstrate%2520that%250Aeven%2520a%2520331k%2520parameter%2520Diff-NCA%2520can%2520generate%2520512x512%2520pathology%2520slices%252C%2520while%250AFourierDiff-NCA%2520%25281.1m%2520parameters%2529%2520reaches%2520a%2520three%2520times%2520lower%2520FID%2520score%2520of%250A43.86%252C%2520compared%2520to%2520the%2520four%2520times%2520bigger%2520UNet%2520%25283.94m%2520parameters%2529%2520with%2520a%2520score%250Aof%2520128.2.%2520Additionally%252C%2520FourierDiff-NCA%2520can%2520perform%2520diverse%2520tasks%2520such%2520as%250Asuper-resolution%252C%2520out-of-distribution%2520image%2520synthesis%252C%2520and%2520inpainting%2520without%250Aexplicit%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.06291v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frequency-Time%20Diffusion%20with%20Neural%20Cellular%20Automata&entry.906535625=John%20Kalkhof%20and%20Arlene%20K%C3%BChn%20and%20Yannik%20Frisch%20and%20Anirban%20Mukhopadhyay&entry.1292438233=%20%20Despite%20considerable%20success%2C%20large%20Denoising%20Diffusion%20Models%20%28DDMs%29%20with%0AUNet%20backbone%20pose%20practical%20challenges%2C%20particularly%20on%20limited%20hardware%20and%0Ain%20processing%20gigapixel%20images.%20To%20address%20these%20limitations%2C%20we%20introduce%20two%0ANeural%20Cellular%20Automata%20%28NCA%29-based%20DDMs%3A%20Diff-NCA%20and%20FourierDiff-NCA.%0ACapitalizing%20on%20the%20local%20communication%20capabilities%20of%20NCA%2C%20Diff-NCA%0Asignificantly%20reduces%20the%20parameter%20counts%20of%20NCA-based%20DDMs.%20Integrating%0AFourier-based%20diffusion%20enables%20global%20communication%20early%20in%20the%20diffusion%0Aprocess.%20This%20feature%20is%20particularly%20valuable%20in%20synthesizing%20complex%20images%0Awith%20important%20global%20features%2C%20such%20as%20the%20CelebA%20dataset.%20We%20demonstrate%20that%0Aeven%20a%20331k%20parameter%20Diff-NCA%20can%20generate%20512x512%20pathology%20slices%2C%20while%0AFourierDiff-NCA%20%281.1m%20parameters%29%20reaches%20a%20three%20times%20lower%20FID%20score%20of%0A43.86%2C%20compared%20to%20the%20four%20times%20bigger%20UNet%20%283.94m%20parameters%29%20with%20a%20score%0Aof%20128.2.%20Additionally%2C%20FourierDiff-NCA%20can%20perform%20diverse%20tasks%20such%20as%0Asuper-resolution%2C%20out-of-distribution%20image%20synthesis%2C%20and%20inpainting%20without%0Aexplicit%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06291v2&entry.124074799=Read"},
{"title": "On the Relation Between Autoencoders and Non-negative Matrix\n  Factorization, and Their Application for Mutational Signature Extraction", "author": "Ida Egendal and Rasmus Froberg Br\u00f8ndum and Marta Pelizzola and Asger Hobolth and Martin B\u00f8gsted", "abstract": "  The aim of this study is to provide a foundation to understand the\nrelationship between non-negative matrix factorization (NMF) and non-negative\nautoencoders enabling proper interpretation and understanding of\nautoencoder-based alternatives to NMF. Since its introduction, NMF has been a\npopular tool for extracting interpretable, low-dimensional representations of\nhigh-dimensional data. However, recently, several studies have proposed to\nreplace NMF with autoencoders. This increasing popularity of autoencoders\nwarrants an investigation on whether this replacement is in general valid and\nreasonable. Moreover, the exact relationship between non-negative autoencoders\nand NMF has not been thoroughly explored. Thus, a main aim of this study is to\ninvestigate in detail the relationship between non-negative autoencoders and\nNMF. We find that the connection between the two models can be established\nthrough convex NMF, which is a restricted case of NMF. In particular, convex\nNMF is a special case of an autoencoder. The performance of NMF and\nautoencoders is compared within the context of extraction of mutational\nsignatures from cancer genomics data. We find that the reconstructions based on\nNMF are more accurate compared to autoencoders, while the signatures extracted\nusing both methods show comparable consistencies and values when externally\nvalidated. These findings suggest that the non-negative autoencoders\ninvestigated in this article do not provide an improvement of NMF in the field\nof mutational signature extraction.\n", "link": "http://arxiv.org/abs/2405.07879v1", "date": "2024-05-13", "relevancy": 1.7726, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4542}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4401}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Relation%20Between%20Autoencoders%20and%20Non-negative%20Matrix%0A%20%20Factorization%2C%20and%20Their%20Application%20for%20Mutational%20Signature%20Extraction&body=Title%3A%20On%20the%20Relation%20Between%20Autoencoders%20and%20Non-negative%20Matrix%0A%20%20Factorization%2C%20and%20Their%20Application%20for%20Mutational%20Signature%20Extraction%0AAuthor%3A%20Ida%20Egendal%20and%20Rasmus%20Froberg%20Br%C3%B8ndum%20and%20Marta%20Pelizzola%20and%20Asger%20Hobolth%20and%20Martin%20B%C3%B8gsted%0AAbstract%3A%20%20%20The%20aim%20of%20this%20study%20is%20to%20provide%20a%20foundation%20to%20understand%20the%0Arelationship%20between%20non-negative%20matrix%20factorization%20%28NMF%29%20and%20non-negative%0Aautoencoders%20enabling%20proper%20interpretation%20and%20understanding%20of%0Aautoencoder-based%20alternatives%20to%20NMF.%20Since%20its%20introduction%2C%20NMF%20has%20been%20a%0Apopular%20tool%20for%20extracting%20interpretable%2C%20low-dimensional%20representations%20of%0Ahigh-dimensional%20data.%20However%2C%20recently%2C%20several%20studies%20have%20proposed%20to%0Areplace%20NMF%20with%20autoencoders.%20This%20increasing%20popularity%20of%20autoencoders%0Awarrants%20an%20investigation%20on%20whether%20this%20replacement%20is%20in%20general%20valid%20and%0Areasonable.%20Moreover%2C%20the%20exact%20relationship%20between%20non-negative%20autoencoders%0Aand%20NMF%20has%20not%20been%20thoroughly%20explored.%20Thus%2C%20a%20main%20aim%20of%20this%20study%20is%20to%0Ainvestigate%20in%20detail%20the%20relationship%20between%20non-negative%20autoencoders%20and%0ANMF.%20We%20find%20that%20the%20connection%20between%20the%20two%20models%20can%20be%20established%0Athrough%20convex%20NMF%2C%20which%20is%20a%20restricted%20case%20of%20NMF.%20In%20particular%2C%20convex%0ANMF%20is%20a%20special%20case%20of%20an%20autoencoder.%20The%20performance%20of%20NMF%20and%0Aautoencoders%20is%20compared%20within%20the%20context%20of%20extraction%20of%20mutational%0Asignatures%20from%20cancer%20genomics%20data.%20We%20find%20that%20the%20reconstructions%20based%20on%0ANMF%20are%20more%20accurate%20compared%20to%20autoencoders%2C%20while%20the%20signatures%20extracted%0Ausing%20both%20methods%20show%20comparable%20consistencies%20and%20values%20when%20externally%0Avalidated.%20These%20findings%20suggest%20that%20the%20non-negative%20autoencoders%0Ainvestigated%20in%20this%20article%20do%20not%20provide%20an%20improvement%20of%20NMF%20in%20the%20field%0Aof%20mutational%20signature%20extraction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Relation%2520Between%2520Autoencoders%2520and%2520Non-negative%2520Matrix%250A%2520%2520Factorization%252C%2520and%2520Their%2520Application%2520for%2520Mutational%2520Signature%2520Extraction%26entry.906535625%3DIda%2520Egendal%2520and%2520Rasmus%2520Froberg%2520Br%25C3%25B8ndum%2520and%2520Marta%2520Pelizzola%2520and%2520Asger%2520Hobolth%2520and%2520Martin%2520B%25C3%25B8gsted%26entry.1292438233%3D%2520%2520The%2520aim%2520of%2520this%2520study%2520is%2520to%2520provide%2520a%2520foundation%2520to%2520understand%2520the%250Arelationship%2520between%2520non-negative%2520matrix%2520factorization%2520%2528NMF%2529%2520and%2520non-negative%250Aautoencoders%2520enabling%2520proper%2520interpretation%2520and%2520understanding%2520of%250Aautoencoder-based%2520alternatives%2520to%2520NMF.%2520Since%2520its%2520introduction%252C%2520NMF%2520has%2520been%2520a%250Apopular%2520tool%2520for%2520extracting%2520interpretable%252C%2520low-dimensional%2520representations%2520of%250Ahigh-dimensional%2520data.%2520However%252C%2520recently%252C%2520several%2520studies%2520have%2520proposed%2520to%250Areplace%2520NMF%2520with%2520autoencoders.%2520This%2520increasing%2520popularity%2520of%2520autoencoders%250Awarrants%2520an%2520investigation%2520on%2520whether%2520this%2520replacement%2520is%2520in%2520general%2520valid%2520and%250Areasonable.%2520Moreover%252C%2520the%2520exact%2520relationship%2520between%2520non-negative%2520autoencoders%250Aand%2520NMF%2520has%2520not%2520been%2520thoroughly%2520explored.%2520Thus%252C%2520a%2520main%2520aim%2520of%2520this%2520study%2520is%2520to%250Ainvestigate%2520in%2520detail%2520the%2520relationship%2520between%2520non-negative%2520autoencoders%2520and%250ANMF.%2520We%2520find%2520that%2520the%2520connection%2520between%2520the%2520two%2520models%2520can%2520be%2520established%250Athrough%2520convex%2520NMF%252C%2520which%2520is%2520a%2520restricted%2520case%2520of%2520NMF.%2520In%2520particular%252C%2520convex%250ANMF%2520is%2520a%2520special%2520case%2520of%2520an%2520autoencoder.%2520The%2520performance%2520of%2520NMF%2520and%250Aautoencoders%2520is%2520compared%2520within%2520the%2520context%2520of%2520extraction%2520of%2520mutational%250Asignatures%2520from%2520cancer%2520genomics%2520data.%2520We%2520find%2520that%2520the%2520reconstructions%2520based%2520on%250ANMF%2520are%2520more%2520accurate%2520compared%2520to%2520autoencoders%252C%2520while%2520the%2520signatures%2520extracted%250Ausing%2520both%2520methods%2520show%2520comparable%2520consistencies%2520and%2520values%2520when%2520externally%250Avalidated.%2520These%2520findings%2520suggest%2520that%2520the%2520non-negative%2520autoencoders%250Ainvestigated%2520in%2520this%2520article%2520do%2520not%2520provide%2520an%2520improvement%2520of%2520NMF%2520in%2520the%2520field%250Aof%2520mutational%2520signature%2520extraction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Relation%20Between%20Autoencoders%20and%20Non-negative%20Matrix%0A%20%20Factorization%2C%20and%20Their%20Application%20for%20Mutational%20Signature%20Extraction&entry.906535625=Ida%20Egendal%20and%20Rasmus%20Froberg%20Br%C3%B8ndum%20and%20Marta%20Pelizzola%20and%20Asger%20Hobolth%20and%20Martin%20B%C3%B8gsted&entry.1292438233=%20%20The%20aim%20of%20this%20study%20is%20to%20provide%20a%20foundation%20to%20understand%20the%0Arelationship%20between%20non-negative%20matrix%20factorization%20%28NMF%29%20and%20non-negative%0Aautoencoders%20enabling%20proper%20interpretation%20and%20understanding%20of%0Aautoencoder-based%20alternatives%20to%20NMF.%20Since%20its%20introduction%2C%20NMF%20has%20been%20a%0Apopular%20tool%20for%20extracting%20interpretable%2C%20low-dimensional%20representations%20of%0Ahigh-dimensional%20data.%20However%2C%20recently%2C%20several%20studies%20have%20proposed%20to%0Areplace%20NMF%20with%20autoencoders.%20This%20increasing%20popularity%20of%20autoencoders%0Awarrants%20an%20investigation%20on%20whether%20this%20replacement%20is%20in%20general%20valid%20and%0Areasonable.%20Moreover%2C%20the%20exact%20relationship%20between%20non-negative%20autoencoders%0Aand%20NMF%20has%20not%20been%20thoroughly%20explored.%20Thus%2C%20a%20main%20aim%20of%20this%20study%20is%20to%0Ainvestigate%20in%20detail%20the%20relationship%20between%20non-negative%20autoencoders%20and%0ANMF.%20We%20find%20that%20the%20connection%20between%20the%20two%20models%20can%20be%20established%0Athrough%20convex%20NMF%2C%20which%20is%20a%20restricted%20case%20of%20NMF.%20In%20particular%2C%20convex%0ANMF%20is%20a%20special%20case%20of%20an%20autoencoder.%20The%20performance%20of%20NMF%20and%0Aautoencoders%20is%20compared%20within%20the%20context%20of%20extraction%20of%20mutational%0Asignatures%20from%20cancer%20genomics%20data.%20We%20find%20that%20the%20reconstructions%20based%20on%0ANMF%20are%20more%20accurate%20compared%20to%20autoencoders%2C%20while%20the%20signatures%20extracted%0Ausing%20both%20methods%20show%20comparable%20consistencies%20and%20values%20when%20externally%0Avalidated.%20These%20findings%20suggest%20that%20the%20non-negative%20autoencoders%0Ainvestigated%20in%20this%20article%20do%20not%20provide%20an%20improvement%20of%20NMF%20in%20the%20field%0Aof%20mutational%20signature%20extraction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07879v1&entry.124074799=Read"},
{"title": "CADS: Unleashing the Diversity of Diffusion Models through\n  Condition-Annealed Sampling", "author": "Seyedmorteza Sadat and Jakob Buhmann and Derek Bradley and Otmar Hilliges and Romann M. Weber", "abstract": "  While conditional diffusion models are known to have good coverage of the\ndata distribution, they still face limitations in output diversity,\nparticularly when sampled with a high classifier-free guidance scale for\noptimal image quality or when trained on small datasets. We attribute this\nproblem to the role of the conditioning signal in inference and offer an\nimproved sampling strategy for diffusion models that can increase generation\ndiversity, especially at high guidance scales, with minimal loss of sample\nquality. Our sampling strategy anneals the conditioning signal by adding\nscheduled, monotonically decreasing Gaussian noise to the conditioning vector\nduring inference to balance diversity and condition alignment. Our\nCondition-Annealed Diffusion Sampler (CADS) can be used with any pretrained\nmodel and sampling algorithm, and we show that it boosts the diversity of\ndiffusion models in various conditional generation tasks. Further, using an\nexisting pretrained diffusion model, CADS achieves a new state-of-the-art FID\nof 1.70 and 2.31 for class-conditional ImageNet generation at 256$\\times$256\nand 512$\\times$512 respectively.\n", "link": "http://arxiv.org/abs/2310.17347v4", "date": "2024-05-13", "relevancy": 1.7718, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6635}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5724}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CADS%3A%20Unleashing%20the%20Diversity%20of%20Diffusion%20Models%20through%0A%20%20Condition-Annealed%20Sampling&body=Title%3A%20CADS%3A%20Unleashing%20the%20Diversity%20of%20Diffusion%20Models%20through%0A%20%20Condition-Annealed%20Sampling%0AAuthor%3A%20Seyedmorteza%20Sadat%20and%20Jakob%20Buhmann%20and%20Derek%20Bradley%20and%20Otmar%20Hilliges%20and%20Romann%20M.%20Weber%0AAbstract%3A%20%20%20While%20conditional%20diffusion%20models%20are%20known%20to%20have%20good%20coverage%20of%20the%0Adata%20distribution%2C%20they%20still%20face%20limitations%20in%20output%20diversity%2C%0Aparticularly%20when%20sampled%20with%20a%20high%20classifier-free%20guidance%20scale%20for%0Aoptimal%20image%20quality%20or%20when%20trained%20on%20small%20datasets.%20We%20attribute%20this%0Aproblem%20to%20the%20role%20of%20the%20conditioning%20signal%20in%20inference%20and%20offer%20an%0Aimproved%20sampling%20strategy%20for%20diffusion%20models%20that%20can%20increase%20generation%0Adiversity%2C%20especially%20at%20high%20guidance%20scales%2C%20with%20minimal%20loss%20of%20sample%0Aquality.%20Our%20sampling%20strategy%20anneals%20the%20conditioning%20signal%20by%20adding%0Ascheduled%2C%20monotonically%20decreasing%20Gaussian%20noise%20to%20the%20conditioning%20vector%0Aduring%20inference%20to%20balance%20diversity%20and%20condition%20alignment.%20Our%0ACondition-Annealed%20Diffusion%20Sampler%20%28CADS%29%20can%20be%20used%20with%20any%20pretrained%0Amodel%20and%20sampling%20algorithm%2C%20and%20we%20show%20that%20it%20boosts%20the%20diversity%20of%0Adiffusion%20models%20in%20various%20conditional%20generation%20tasks.%20Further%2C%20using%20an%0Aexisting%20pretrained%20diffusion%20model%2C%20CADS%20achieves%20a%20new%20state-of-the-art%20FID%0Aof%201.70%20and%202.31%20for%20class-conditional%20ImageNet%20generation%20at%20256%24%5Ctimes%24256%0Aand%20512%24%5Ctimes%24512%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.17347v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCADS%253A%2520Unleashing%2520the%2520Diversity%2520of%2520Diffusion%2520Models%2520through%250A%2520%2520Condition-Annealed%2520Sampling%26entry.906535625%3DSeyedmorteza%2520Sadat%2520and%2520Jakob%2520Buhmann%2520and%2520Derek%2520Bradley%2520and%2520Otmar%2520Hilliges%2520and%2520Romann%2520M.%2520Weber%26entry.1292438233%3D%2520%2520While%2520conditional%2520diffusion%2520models%2520are%2520known%2520to%2520have%2520good%2520coverage%2520of%2520the%250Adata%2520distribution%252C%2520they%2520still%2520face%2520limitations%2520in%2520output%2520diversity%252C%250Aparticularly%2520when%2520sampled%2520with%2520a%2520high%2520classifier-free%2520guidance%2520scale%2520for%250Aoptimal%2520image%2520quality%2520or%2520when%2520trained%2520on%2520small%2520datasets.%2520We%2520attribute%2520this%250Aproblem%2520to%2520the%2520role%2520of%2520the%2520conditioning%2520signal%2520in%2520inference%2520and%2520offer%2520an%250Aimproved%2520sampling%2520strategy%2520for%2520diffusion%2520models%2520that%2520can%2520increase%2520generation%250Adiversity%252C%2520especially%2520at%2520high%2520guidance%2520scales%252C%2520with%2520minimal%2520loss%2520of%2520sample%250Aquality.%2520Our%2520sampling%2520strategy%2520anneals%2520the%2520conditioning%2520signal%2520by%2520adding%250Ascheduled%252C%2520monotonically%2520decreasing%2520Gaussian%2520noise%2520to%2520the%2520conditioning%2520vector%250Aduring%2520inference%2520to%2520balance%2520diversity%2520and%2520condition%2520alignment.%2520Our%250ACondition-Annealed%2520Diffusion%2520Sampler%2520%2528CADS%2529%2520can%2520be%2520used%2520with%2520any%2520pretrained%250Amodel%2520and%2520sampling%2520algorithm%252C%2520and%2520we%2520show%2520that%2520it%2520boosts%2520the%2520diversity%2520of%250Adiffusion%2520models%2520in%2520various%2520conditional%2520generation%2520tasks.%2520Further%252C%2520using%2520an%250Aexisting%2520pretrained%2520diffusion%2520model%252C%2520CADS%2520achieves%2520a%2520new%2520state-of-the-art%2520FID%250Aof%25201.70%2520and%25202.31%2520for%2520class-conditional%2520ImageNet%2520generation%2520at%2520256%2524%255Ctimes%2524256%250Aand%2520512%2524%255Ctimes%2524512%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.17347v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CADS%3A%20Unleashing%20the%20Diversity%20of%20Diffusion%20Models%20through%0A%20%20Condition-Annealed%20Sampling&entry.906535625=Seyedmorteza%20Sadat%20and%20Jakob%20Buhmann%20and%20Derek%20Bradley%20and%20Otmar%20Hilliges%20and%20Romann%20M.%20Weber&entry.1292438233=%20%20While%20conditional%20diffusion%20models%20are%20known%20to%20have%20good%20coverage%20of%20the%0Adata%20distribution%2C%20they%20still%20face%20limitations%20in%20output%20diversity%2C%0Aparticularly%20when%20sampled%20with%20a%20high%20classifier-free%20guidance%20scale%20for%0Aoptimal%20image%20quality%20or%20when%20trained%20on%20small%20datasets.%20We%20attribute%20this%0Aproblem%20to%20the%20role%20of%20the%20conditioning%20signal%20in%20inference%20and%20offer%20an%0Aimproved%20sampling%20strategy%20for%20diffusion%20models%20that%20can%20increase%20generation%0Adiversity%2C%20especially%20at%20high%20guidance%20scales%2C%20with%20minimal%20loss%20of%20sample%0Aquality.%20Our%20sampling%20strategy%20anneals%20the%20conditioning%20signal%20by%20adding%0Ascheduled%2C%20monotonically%20decreasing%20Gaussian%20noise%20to%20the%20conditioning%20vector%0Aduring%20inference%20to%20balance%20diversity%20and%20condition%20alignment.%20Our%0ACondition-Annealed%20Diffusion%20Sampler%20%28CADS%29%20can%20be%20used%20with%20any%20pretrained%0Amodel%20and%20sampling%20algorithm%2C%20and%20we%20show%20that%20it%20boosts%20the%20diversity%20of%0Adiffusion%20models%20in%20various%20conditional%20generation%20tasks.%20Further%2C%20using%20an%0Aexisting%20pretrained%20diffusion%20model%2C%20CADS%20achieves%20a%20new%20state-of-the-art%20FID%0Aof%201.70%20and%202.31%20for%20class-conditional%20ImageNet%20generation%20at%20256%24%5Ctimes%24256%0Aand%20512%24%5Ctimes%24512%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.17347v4&entry.124074799=Read"},
{"title": "Integrating supervised and unsupervised learning approaches to unveil\n  critical process inputs", "author": "Paris Papavasileiou and Dimitrios G. Giovanis and Gabriele Pozzetti and Martin Kathrein and Christoph Czettl and Ioannis G. Kevrekidis and Andreas G. Boudouvis and St\u00e9phane P. A. Bordas and Eleni D. Koronaki", "abstract": "  This study introduces a machine learning framework tailored to large-scale\nindustrial processes characterized by a plethora of numerical and categorical\ninputs. The framework aims to (i) discern critical parameters influencing the\noutput and (ii) generate accurate out-of-sample qualitative and quantitative\npredictions of production outcomes. Specifically, we address the pivotal\nquestion of the significance of each input in shaping the process outcome,\nusing an industrial Chemical Vapor Deposition (CVD) process as an example. The\ninitial objective involves merging subject matter expertise and clustering\ntechniques exclusively on the process output, here, coating thickness\nmeasurements at various positions in the reactor. This approach identifies\ngroups of production runs that share similar qualitative characteristics, such\nas film mean thickness and standard deviation. In particular, the differences\nof the outcomes represented by the different clusters can be attributed to\ndifferences in specific inputs, indicating that these inputs are critical for\nthe production outcome. Leveraging this insight, we subsequently implement\nsupervised classification and regression methods using the identified critical\nprocess inputs. The proposed methodology proves to be valuable in scenarios\nwith a multitude of inputs and insufficient data for the direct application of\ndeep learning techniques, providing meaningful insights into the underlying\nprocesses.\n", "link": "http://arxiv.org/abs/2405.07751v1", "date": "2024-05-13", "relevancy": 1.3962, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4781}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.45}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrating%20supervised%20and%20unsupervised%20learning%20approaches%20to%20unveil%0A%20%20critical%20process%20inputs&body=Title%3A%20Integrating%20supervised%20and%20unsupervised%20learning%20approaches%20to%20unveil%0A%20%20critical%20process%20inputs%0AAuthor%3A%20Paris%20Papavasileiou%20and%20Dimitrios%20G.%20Giovanis%20and%20Gabriele%20Pozzetti%20and%20Martin%20Kathrein%20and%20Christoph%20Czettl%20and%20Ioannis%20G.%20Kevrekidis%20and%20Andreas%20G.%20Boudouvis%20and%20St%C3%A9phane%20P.%20A.%20Bordas%20and%20Eleni%20D.%20Koronaki%0AAbstract%3A%20%20%20This%20study%20introduces%20a%20machine%20learning%20framework%20tailored%20to%20large-scale%0Aindustrial%20processes%20characterized%20by%20a%20plethora%20of%20numerical%20and%20categorical%0Ainputs.%20The%20framework%20aims%20to%20%28i%29%20discern%20critical%20parameters%20influencing%20the%0Aoutput%20and%20%28ii%29%20generate%20accurate%20out-of-sample%20qualitative%20and%20quantitative%0Apredictions%20of%20production%20outcomes.%20Specifically%2C%20we%20address%20the%20pivotal%0Aquestion%20of%20the%20significance%20of%20each%20input%20in%20shaping%20the%20process%20outcome%2C%0Ausing%20an%20industrial%20Chemical%20Vapor%20Deposition%20%28CVD%29%20process%20as%20an%20example.%20The%0Ainitial%20objective%20involves%20merging%20subject%20matter%20expertise%20and%20clustering%0Atechniques%20exclusively%20on%20the%20process%20output%2C%20here%2C%20coating%20thickness%0Ameasurements%20at%20various%20positions%20in%20the%20reactor.%20This%20approach%20identifies%0Agroups%20of%20production%20runs%20that%20share%20similar%20qualitative%20characteristics%2C%20such%0Aas%20film%20mean%20thickness%20and%20standard%20deviation.%20In%20particular%2C%20the%20differences%0Aof%20the%20outcomes%20represented%20by%20the%20different%20clusters%20can%20be%20attributed%20to%0Adifferences%20in%20specific%20inputs%2C%20indicating%20that%20these%20inputs%20are%20critical%20for%0Athe%20production%20outcome.%20Leveraging%20this%20insight%2C%20we%20subsequently%20implement%0Asupervised%20classification%20and%20regression%20methods%20using%20the%20identified%20critical%0Aprocess%20inputs.%20The%20proposed%20methodology%20proves%20to%20be%20valuable%20in%20scenarios%0Awith%20a%20multitude%20of%20inputs%20and%20insufficient%20data%20for%20the%20direct%20application%20of%0Adeep%20learning%20techniques%2C%20providing%20meaningful%20insights%20into%20the%20underlying%0Aprocesses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrating%2520supervised%2520and%2520unsupervised%2520learning%2520approaches%2520to%2520unveil%250A%2520%2520critical%2520process%2520inputs%26entry.906535625%3DParis%2520Papavasileiou%2520and%2520Dimitrios%2520G.%2520Giovanis%2520and%2520Gabriele%2520Pozzetti%2520and%2520Martin%2520Kathrein%2520and%2520Christoph%2520Czettl%2520and%2520Ioannis%2520G.%2520Kevrekidis%2520and%2520Andreas%2520G.%2520Boudouvis%2520and%2520St%25C3%25A9phane%2520P.%2520A.%2520Bordas%2520and%2520Eleni%2520D.%2520Koronaki%26entry.1292438233%3D%2520%2520This%2520study%2520introduces%2520a%2520machine%2520learning%2520framework%2520tailored%2520to%2520large-scale%250Aindustrial%2520processes%2520characterized%2520by%2520a%2520plethora%2520of%2520numerical%2520and%2520categorical%250Ainputs.%2520The%2520framework%2520aims%2520to%2520%2528i%2529%2520discern%2520critical%2520parameters%2520influencing%2520the%250Aoutput%2520and%2520%2528ii%2529%2520generate%2520accurate%2520out-of-sample%2520qualitative%2520and%2520quantitative%250Apredictions%2520of%2520production%2520outcomes.%2520Specifically%252C%2520we%2520address%2520the%2520pivotal%250Aquestion%2520of%2520the%2520significance%2520of%2520each%2520input%2520in%2520shaping%2520the%2520process%2520outcome%252C%250Ausing%2520an%2520industrial%2520Chemical%2520Vapor%2520Deposition%2520%2528CVD%2529%2520process%2520as%2520an%2520example.%2520The%250Ainitial%2520objective%2520involves%2520merging%2520subject%2520matter%2520expertise%2520and%2520clustering%250Atechniques%2520exclusively%2520on%2520the%2520process%2520output%252C%2520here%252C%2520coating%2520thickness%250Ameasurements%2520at%2520various%2520positions%2520in%2520the%2520reactor.%2520This%2520approach%2520identifies%250Agroups%2520of%2520production%2520runs%2520that%2520share%2520similar%2520qualitative%2520characteristics%252C%2520such%250Aas%2520film%2520mean%2520thickness%2520and%2520standard%2520deviation.%2520In%2520particular%252C%2520the%2520differences%250Aof%2520the%2520outcomes%2520represented%2520by%2520the%2520different%2520clusters%2520can%2520be%2520attributed%2520to%250Adifferences%2520in%2520specific%2520inputs%252C%2520indicating%2520that%2520these%2520inputs%2520are%2520critical%2520for%250Athe%2520production%2520outcome.%2520Leveraging%2520this%2520insight%252C%2520we%2520subsequently%2520implement%250Asupervised%2520classification%2520and%2520regression%2520methods%2520using%2520the%2520identified%2520critical%250Aprocess%2520inputs.%2520The%2520proposed%2520methodology%2520proves%2520to%2520be%2520valuable%2520in%2520scenarios%250Awith%2520a%2520multitude%2520of%2520inputs%2520and%2520insufficient%2520data%2520for%2520the%2520direct%2520application%2520of%250Adeep%2520learning%2520techniques%252C%2520providing%2520meaningful%2520insights%2520into%2520the%2520underlying%250Aprocesses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrating%20supervised%20and%20unsupervised%20learning%20approaches%20to%20unveil%0A%20%20critical%20process%20inputs&entry.906535625=Paris%20Papavasileiou%20and%20Dimitrios%20G.%20Giovanis%20and%20Gabriele%20Pozzetti%20and%20Martin%20Kathrein%20and%20Christoph%20Czettl%20and%20Ioannis%20G.%20Kevrekidis%20and%20Andreas%20G.%20Boudouvis%20and%20St%C3%A9phane%20P.%20A.%20Bordas%20and%20Eleni%20D.%20Koronaki&entry.1292438233=%20%20This%20study%20introduces%20a%20machine%20learning%20framework%20tailored%20to%20large-scale%0Aindustrial%20processes%20characterized%20by%20a%20plethora%20of%20numerical%20and%20categorical%0Ainputs.%20The%20framework%20aims%20to%20%28i%29%20discern%20critical%20parameters%20influencing%20the%0Aoutput%20and%20%28ii%29%20generate%20accurate%20out-of-sample%20qualitative%20and%20quantitative%0Apredictions%20of%20production%20outcomes.%20Specifically%2C%20we%20address%20the%20pivotal%0Aquestion%20of%20the%20significance%20of%20each%20input%20in%20shaping%20the%20process%20outcome%2C%0Ausing%20an%20industrial%20Chemical%20Vapor%20Deposition%20%28CVD%29%20process%20as%20an%20example.%20The%0Ainitial%20objective%20involves%20merging%20subject%20matter%20expertise%20and%20clustering%0Atechniques%20exclusively%20on%20the%20process%20output%2C%20here%2C%20coating%20thickness%0Ameasurements%20at%20various%20positions%20in%20the%20reactor.%20This%20approach%20identifies%0Agroups%20of%20production%20runs%20that%20share%20similar%20qualitative%20characteristics%2C%20such%0Aas%20film%20mean%20thickness%20and%20standard%20deviation.%20In%20particular%2C%20the%20differences%0Aof%20the%20outcomes%20represented%20by%20the%20different%20clusters%20can%20be%20attributed%20to%0Adifferences%20in%20specific%20inputs%2C%20indicating%20that%20these%20inputs%20are%20critical%20for%0Athe%20production%20outcome.%20Leveraging%20this%20insight%2C%20we%20subsequently%20implement%0Asupervised%20classification%20and%20regression%20methods%20using%20the%20identified%20critical%0Aprocess%20inputs.%20The%20proposed%20methodology%20proves%20to%20be%20valuable%20in%20scenarios%0Awith%20a%20multitude%20of%20inputs%20and%20insufficient%20data%20for%20the%20direct%20application%20of%0Adeep%20learning%20techniques%2C%20providing%20meaningful%20insights%20into%20the%20underlying%0Aprocesses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07751v1&entry.124074799=Read"},
{"title": "Do Large Language Models Solve ARC Visual Analogies Like People Do?", "author": "Gustaw Opie\u0142ka and Hannes Rosenbusch and Veerle Vijverberg and Claire E. Stevenson", "abstract": "  The Abstraction Reasoning Corpus (ARC) is a visual analogical reasoning test\ndesigned for humans and machines (Chollet, 2019). We compared human and large\nlanguage model (LLM) performance on a new child-friendly set of ARC items.\nResults show that both children and adults outperform most LLMs on these tasks.\nError analysis revealed a similar \"fallback\" solution strategy in LLMs and\nyoung children, where part of the analogy is simply copied. In addition, we\nfound two other error types, one based on seemingly grasping key concepts\n(e.g., Inside-Outside) and the other based on simple combinations of analogy\ninput matrices. On the whole, \"concept\" errors were more common in humans, and\n\"matrix\" errors were more common in LLMs. This study sheds new light on LLM\nreasoning ability and the extent to which we can use error analyses and\ncomparisons with human development to understand how LLMs solve visual\nanalogies.\n", "link": "http://arxiv.org/abs/2403.09734v2", "date": "2024-05-13", "relevancy": 1.3806, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4803}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4603}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Large%20Language%20Models%20Solve%20ARC%20Visual%20Analogies%20Like%20People%20Do%3F&body=Title%3A%20Do%20Large%20Language%20Models%20Solve%20ARC%20Visual%20Analogies%20Like%20People%20Do%3F%0AAuthor%3A%20Gustaw%20Opie%C5%82ka%20and%20Hannes%20Rosenbusch%20and%20Veerle%20Vijverberg%20and%20Claire%20E.%20Stevenson%0AAbstract%3A%20%20%20The%20Abstraction%20Reasoning%20Corpus%20%28ARC%29%20is%20a%20visual%20analogical%20reasoning%20test%0Adesigned%20for%20humans%20and%20machines%20%28Chollet%2C%202019%29.%20We%20compared%20human%20and%20large%0Alanguage%20model%20%28LLM%29%20performance%20on%20a%20new%20child-friendly%20set%20of%20ARC%20items.%0AResults%20show%20that%20both%20children%20and%20adults%20outperform%20most%20LLMs%20on%20these%20tasks.%0AError%20analysis%20revealed%20a%20similar%20%22fallback%22%20solution%20strategy%20in%20LLMs%20and%0Ayoung%20children%2C%20where%20part%20of%20the%20analogy%20is%20simply%20copied.%20In%20addition%2C%20we%0Afound%20two%20other%20error%20types%2C%20one%20based%20on%20seemingly%20grasping%20key%20concepts%0A%28e.g.%2C%20Inside-Outside%29%20and%20the%20other%20based%20on%20simple%20combinations%20of%20analogy%0Ainput%20matrices.%20On%20the%20whole%2C%20%22concept%22%20errors%20were%20more%20common%20in%20humans%2C%20and%0A%22matrix%22%20errors%20were%20more%20common%20in%20LLMs.%20This%20study%20sheds%20new%20light%20on%20LLM%0Areasoning%20ability%20and%20the%20extent%20to%20which%20we%20can%20use%20error%20analyses%20and%0Acomparisons%20with%20human%20development%20to%20understand%20how%20LLMs%20solve%20visual%0Aanalogies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09734v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Large%2520Language%2520Models%2520Solve%2520ARC%2520Visual%2520Analogies%2520Like%2520People%2520Do%253F%26entry.906535625%3DGustaw%2520Opie%25C5%2582ka%2520and%2520Hannes%2520Rosenbusch%2520and%2520Veerle%2520Vijverberg%2520and%2520Claire%2520E.%2520Stevenson%26entry.1292438233%3D%2520%2520The%2520Abstraction%2520Reasoning%2520Corpus%2520%2528ARC%2529%2520is%2520a%2520visual%2520analogical%2520reasoning%2520test%250Adesigned%2520for%2520humans%2520and%2520machines%2520%2528Chollet%252C%25202019%2529.%2520We%2520compared%2520human%2520and%2520large%250Alanguage%2520model%2520%2528LLM%2529%2520performance%2520on%2520a%2520new%2520child-friendly%2520set%2520of%2520ARC%2520items.%250AResults%2520show%2520that%2520both%2520children%2520and%2520adults%2520outperform%2520most%2520LLMs%2520on%2520these%2520tasks.%250AError%2520analysis%2520revealed%2520a%2520similar%2520%2522fallback%2522%2520solution%2520strategy%2520in%2520LLMs%2520and%250Ayoung%2520children%252C%2520where%2520part%2520of%2520the%2520analogy%2520is%2520simply%2520copied.%2520In%2520addition%252C%2520we%250Afound%2520two%2520other%2520error%2520types%252C%2520one%2520based%2520on%2520seemingly%2520grasping%2520key%2520concepts%250A%2528e.g.%252C%2520Inside-Outside%2529%2520and%2520the%2520other%2520based%2520on%2520simple%2520combinations%2520of%2520analogy%250Ainput%2520matrices.%2520On%2520the%2520whole%252C%2520%2522concept%2522%2520errors%2520were%2520more%2520common%2520in%2520humans%252C%2520and%250A%2522matrix%2522%2520errors%2520were%2520more%2520common%2520in%2520LLMs.%2520This%2520study%2520sheds%2520new%2520light%2520on%2520LLM%250Areasoning%2520ability%2520and%2520the%2520extent%2520to%2520which%2520we%2520can%2520use%2520error%2520analyses%2520and%250Acomparisons%2520with%2520human%2520development%2520to%2520understand%2520how%2520LLMs%2520solve%2520visual%250Aanalogies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09734v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Large%20Language%20Models%20Solve%20ARC%20Visual%20Analogies%20Like%20People%20Do%3F&entry.906535625=Gustaw%20Opie%C5%82ka%20and%20Hannes%20Rosenbusch%20and%20Veerle%20Vijverberg%20and%20Claire%20E.%20Stevenson&entry.1292438233=%20%20The%20Abstraction%20Reasoning%20Corpus%20%28ARC%29%20is%20a%20visual%20analogical%20reasoning%20test%0Adesigned%20for%20humans%20and%20machines%20%28Chollet%2C%202019%29.%20We%20compared%20human%20and%20large%0Alanguage%20model%20%28LLM%29%20performance%20on%20a%20new%20child-friendly%20set%20of%20ARC%20items.%0AResults%20show%20that%20both%20children%20and%20adults%20outperform%20most%20LLMs%20on%20these%20tasks.%0AError%20analysis%20revealed%20a%20similar%20%22fallback%22%20solution%20strategy%20in%20LLMs%20and%0Ayoung%20children%2C%20where%20part%20of%20the%20analogy%20is%20simply%20copied.%20In%20addition%2C%20we%0Afound%20two%20other%20error%20types%2C%20one%20based%20on%20seemingly%20grasping%20key%20concepts%0A%28e.g.%2C%20Inside-Outside%29%20and%20the%20other%20based%20on%20simple%20combinations%20of%20analogy%0Ainput%20matrices.%20On%20the%20whole%2C%20%22concept%22%20errors%20were%20more%20common%20in%20humans%2C%20and%0A%22matrix%22%20errors%20were%20more%20common%20in%20LLMs.%20This%20study%20sheds%20new%20light%20on%20LLM%0Areasoning%20ability%20and%20the%20extent%20to%20which%20we%20can%20use%20error%20analyses%20and%0Acomparisons%20with%20human%20development%20to%20understand%20how%20LLMs%20solve%20visual%0Aanalogies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09734v2&entry.124074799=Read"},
{"title": "CAGES: Cost-Aware Gradient Entropy Search for Efficient Local\n  Multi-Fidelity Bayesian Optimization", "author": "Wei-Ting Tang and Joel A. Paulson", "abstract": "  Bayesian optimization (BO) is a popular approach for optimizing\nexpensive-to-evaluate black-box objective functions. An important challenge in\nBO is its application to high-dimensional search spaces due in large part to\nthe curse of dimensionality. One way to overcome this challenge is to focus on\nlocal BO methods that aim to efficiently learn gradients, which have shown\nstrong empirical performance on a variety of high-dimensional problems\nincluding policy search in reinforcement learning (RL). However, current local\nBO methods assume access to only a single high-fidelity information source\nwhereas, in many engineering and control problems, one has access to multiple\ncheaper approximations of the objective. We propose a novel algorithm,\nCost-Aware Gradient Entropy Search (CAGES), for local BO of multi-fidelity\nblack-box functions. CAGES makes no assumption about the relationship between\ndifferent information sources, making it more flexible than other\nmulti-fidelity methods. It also employs a new type of information-theoretic\nacquisition function, which enables systematic identification of samples that\nmaximize the information gain about the unknown gradient per cost of the\nevaluation. We demonstrate CAGES can achieve significant performance\nimprovements compared to other state-of-the-art methods on a variety of\nsynthetic and benchmark RL problems.\n", "link": "http://arxiv.org/abs/2405.07760v1", "date": "2024-05-13", "relevancy": 1.4946, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5525}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4862}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAGES%3A%20Cost-Aware%20Gradient%20Entropy%20Search%20for%20Efficient%20Local%0A%20%20Multi-Fidelity%20Bayesian%20Optimization&body=Title%3A%20CAGES%3A%20Cost-Aware%20Gradient%20Entropy%20Search%20for%20Efficient%20Local%0A%20%20Multi-Fidelity%20Bayesian%20Optimization%0AAuthor%3A%20Wei-Ting%20Tang%20and%20Joel%20A.%20Paulson%0AAbstract%3A%20%20%20Bayesian%20optimization%20%28BO%29%20is%20a%20popular%20approach%20for%20optimizing%0Aexpensive-to-evaluate%20black-box%20objective%20functions.%20An%20important%20challenge%20in%0ABO%20is%20its%20application%20to%20high-dimensional%20search%20spaces%20due%20in%20large%20part%20to%0Athe%20curse%20of%20dimensionality.%20One%20way%20to%20overcome%20this%20challenge%20is%20to%20focus%20on%0Alocal%20BO%20methods%20that%20aim%20to%20efficiently%20learn%20gradients%2C%20which%20have%20shown%0Astrong%20empirical%20performance%20on%20a%20variety%20of%20high-dimensional%20problems%0Aincluding%20policy%20search%20in%20reinforcement%20learning%20%28RL%29.%20However%2C%20current%20local%0ABO%20methods%20assume%20access%20to%20only%20a%20single%20high-fidelity%20information%20source%0Awhereas%2C%20in%20many%20engineering%20and%20control%20problems%2C%20one%20has%20access%20to%20multiple%0Acheaper%20approximations%20of%20the%20objective.%20We%20propose%20a%20novel%20algorithm%2C%0ACost-Aware%20Gradient%20Entropy%20Search%20%28CAGES%29%2C%20for%20local%20BO%20of%20multi-fidelity%0Ablack-box%20functions.%20CAGES%20makes%20no%20assumption%20about%20the%20relationship%20between%0Adifferent%20information%20sources%2C%20making%20it%20more%20flexible%20than%20other%0Amulti-fidelity%20methods.%20It%20also%20employs%20a%20new%20type%20of%20information-theoretic%0Aacquisition%20function%2C%20which%20enables%20systematic%20identification%20of%20samples%20that%0Amaximize%20the%20information%20gain%20about%20the%20unknown%20gradient%20per%20cost%20of%20the%0Aevaluation.%20We%20demonstrate%20CAGES%20can%20achieve%20significant%20performance%0Aimprovements%20compared%20to%20other%20state-of-the-art%20methods%20on%20a%20variety%20of%0Asynthetic%20and%20benchmark%20RL%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAGES%253A%2520Cost-Aware%2520Gradient%2520Entropy%2520Search%2520for%2520Efficient%2520Local%250A%2520%2520Multi-Fidelity%2520Bayesian%2520Optimization%26entry.906535625%3DWei-Ting%2520Tang%2520and%2520Joel%2520A.%2520Paulson%26entry.1292438233%3D%2520%2520Bayesian%2520optimization%2520%2528BO%2529%2520is%2520a%2520popular%2520approach%2520for%2520optimizing%250Aexpensive-to-evaluate%2520black-box%2520objective%2520functions.%2520An%2520important%2520challenge%2520in%250ABO%2520is%2520its%2520application%2520to%2520high-dimensional%2520search%2520spaces%2520due%2520in%2520large%2520part%2520to%250Athe%2520curse%2520of%2520dimensionality.%2520One%2520way%2520to%2520overcome%2520this%2520challenge%2520is%2520to%2520focus%2520on%250Alocal%2520BO%2520methods%2520that%2520aim%2520to%2520efficiently%2520learn%2520gradients%252C%2520which%2520have%2520shown%250Astrong%2520empirical%2520performance%2520on%2520a%2520variety%2520of%2520high-dimensional%2520problems%250Aincluding%2520policy%2520search%2520in%2520reinforcement%2520learning%2520%2528RL%2529.%2520However%252C%2520current%2520local%250ABO%2520methods%2520assume%2520access%2520to%2520only%2520a%2520single%2520high-fidelity%2520information%2520source%250Awhereas%252C%2520in%2520many%2520engineering%2520and%2520control%2520problems%252C%2520one%2520has%2520access%2520to%2520multiple%250Acheaper%2520approximations%2520of%2520the%2520objective.%2520We%2520propose%2520a%2520novel%2520algorithm%252C%250ACost-Aware%2520Gradient%2520Entropy%2520Search%2520%2528CAGES%2529%252C%2520for%2520local%2520BO%2520of%2520multi-fidelity%250Ablack-box%2520functions.%2520CAGES%2520makes%2520no%2520assumption%2520about%2520the%2520relationship%2520between%250Adifferent%2520information%2520sources%252C%2520making%2520it%2520more%2520flexible%2520than%2520other%250Amulti-fidelity%2520methods.%2520It%2520also%2520employs%2520a%2520new%2520type%2520of%2520information-theoretic%250Aacquisition%2520function%252C%2520which%2520enables%2520systematic%2520identification%2520of%2520samples%2520that%250Amaximize%2520the%2520information%2520gain%2520about%2520the%2520unknown%2520gradient%2520per%2520cost%2520of%2520the%250Aevaluation.%2520We%2520demonstrate%2520CAGES%2520can%2520achieve%2520significant%2520performance%250Aimprovements%2520compared%2520to%2520other%2520state-of-the-art%2520methods%2520on%2520a%2520variety%2520of%250Asynthetic%2520and%2520benchmark%2520RL%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAGES%3A%20Cost-Aware%20Gradient%20Entropy%20Search%20for%20Efficient%20Local%0A%20%20Multi-Fidelity%20Bayesian%20Optimization&entry.906535625=Wei-Ting%20Tang%20and%20Joel%20A.%20Paulson&entry.1292438233=%20%20Bayesian%20optimization%20%28BO%29%20is%20a%20popular%20approach%20for%20optimizing%0Aexpensive-to-evaluate%20black-box%20objective%20functions.%20An%20important%20challenge%20in%0ABO%20is%20its%20application%20to%20high-dimensional%20search%20spaces%20due%20in%20large%20part%20to%0Athe%20curse%20of%20dimensionality.%20One%20way%20to%20overcome%20this%20challenge%20is%20to%20focus%20on%0Alocal%20BO%20methods%20that%20aim%20to%20efficiently%20learn%20gradients%2C%20which%20have%20shown%0Astrong%20empirical%20performance%20on%20a%20variety%20of%20high-dimensional%20problems%0Aincluding%20policy%20search%20in%20reinforcement%20learning%20%28RL%29.%20However%2C%20current%20local%0ABO%20methods%20assume%20access%20to%20only%20a%20single%20high-fidelity%20information%20source%0Awhereas%2C%20in%20many%20engineering%20and%20control%20problems%2C%20one%20has%20access%20to%20multiple%0Acheaper%20approximations%20of%20the%20objective.%20We%20propose%20a%20novel%20algorithm%2C%0ACost-Aware%20Gradient%20Entropy%20Search%20%28CAGES%29%2C%20for%20local%20BO%20of%20multi-fidelity%0Ablack-box%20functions.%20CAGES%20makes%20no%20assumption%20about%20the%20relationship%20between%0Adifferent%20information%20sources%2C%20making%20it%20more%20flexible%20than%20other%0Amulti-fidelity%20methods.%20It%20also%20employs%20a%20new%20type%20of%20information-theoretic%0Aacquisition%20function%2C%20which%20enables%20systematic%20identification%20of%20samples%20that%0Amaximize%20the%20information%20gain%20about%20the%20unknown%20gradient%20per%20cost%20of%20the%0Aevaluation.%20We%20demonstrate%20CAGES%20can%20achieve%20significant%20performance%0Aimprovements%20compared%20to%20other%20state-of-the-art%20methods%20on%20a%20variety%20of%0Asynthetic%20and%20benchmark%20RL%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07760v1&entry.124074799=Read"},
{"title": "Hype or Heuristic? Quantum Reinforcement Learning for Join Order\n  Optimisation", "author": "Maja Franz and Tobias Winker and Sven Groppe and Wolfgang Mauerer", "abstract": "  Identifying optimal join orders (JOs) stands out as a key challenge in\ndatabase research and engineering. Owing to the large search space, established\nclassical methods rely on approximations and heuristics. Recent efforts have\nsuccessfully explored reinforcement learning (RL) for JO. Likewise, quantum\nversions of RL have received considerable scientific attention. Yet, it is an\nopen question if they can achieve sustainable, overall practical advantages\nwith improved quantum processors.\n  In this paper, we present a novel approach that uses quantum reinforcement\nlearning (QRL) for JO based on a hybrid variational quantum ansatz. It is able\nto handle general bushy join trees instead of resorting to simpler left-deep\nvariants as compared to approaches based on quantum(-inspired) optimisation,\nyet requires multiple orders of magnitudes fewer qubits, which is a scarce\nresource even for post-NISQ systems.\n  Despite moderate circuit depth, the ansatz exceeds current NISQ capabilities,\nwhich requires an evaluation by numerical simulations. While QRL may not\nsignificantly outperform classical approaches in solving the JO problem with\nrespect to result quality (albeit we see parity), we find a drastic reduction\nin required trainable parameters. This benefits practically relevant aspects\nranging from shorter training times compared to classical RL, less involved\nclassical optimisation passes, or better use of available training data, and\nfits data-stream and low-latency processing scenarios. Our comprehensive\nevaluation and careful discussion delivers a balanced perspective on possible\npractical quantum advantage, provides insights for future systemic approaches,\nand allows for quantitatively assessing trade-offs of quantum approaches for\none of the most crucial problems of database management systems.\n", "link": "http://arxiv.org/abs/2405.07770v1", "date": "2024-05-13", "relevancy": 1.2798, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4603}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4201}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hype%20or%20Heuristic%3F%20Quantum%20Reinforcement%20Learning%20for%20Join%20Order%0A%20%20Optimisation&body=Title%3A%20Hype%20or%20Heuristic%3F%20Quantum%20Reinforcement%20Learning%20for%20Join%20Order%0A%20%20Optimisation%0AAuthor%3A%20Maja%20Franz%20and%20Tobias%20Winker%20and%20Sven%20Groppe%20and%20Wolfgang%20Mauerer%0AAbstract%3A%20%20%20Identifying%20optimal%20join%20orders%20%28JOs%29%20stands%20out%20as%20a%20key%20challenge%20in%0Adatabase%20research%20and%20engineering.%20Owing%20to%20the%20large%20search%20space%2C%20established%0Aclassical%20methods%20rely%20on%20approximations%20and%20heuristics.%20Recent%20efforts%20have%0Asuccessfully%20explored%20reinforcement%20learning%20%28RL%29%20for%20JO.%20Likewise%2C%20quantum%0Aversions%20of%20RL%20have%20received%20considerable%20scientific%20attention.%20Yet%2C%20it%20is%20an%0Aopen%20question%20if%20they%20can%20achieve%20sustainable%2C%20overall%20practical%20advantages%0Awith%20improved%20quantum%20processors.%0A%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20that%20uses%20quantum%20reinforcement%0Alearning%20%28QRL%29%20for%20JO%20based%20on%20a%20hybrid%20variational%20quantum%20ansatz.%20It%20is%20able%0Ato%20handle%20general%20bushy%20join%20trees%20instead%20of%20resorting%20to%20simpler%20left-deep%0Avariants%20as%20compared%20to%20approaches%20based%20on%20quantum%28-inspired%29%20optimisation%2C%0Ayet%20requires%20multiple%20orders%20of%20magnitudes%20fewer%20qubits%2C%20which%20is%20a%20scarce%0Aresource%20even%20for%20post-NISQ%20systems.%0A%20%20Despite%20moderate%20circuit%20depth%2C%20the%20ansatz%20exceeds%20current%20NISQ%20capabilities%2C%0Awhich%20requires%20an%20evaluation%20by%20numerical%20simulations.%20While%20QRL%20may%20not%0Asignificantly%20outperform%20classical%20approaches%20in%20solving%20the%20JO%20problem%20with%0Arespect%20to%20result%20quality%20%28albeit%20we%20see%20parity%29%2C%20we%20find%20a%20drastic%20reduction%0Ain%20required%20trainable%20parameters.%20This%20benefits%20practically%20relevant%20aspects%0Aranging%20from%20shorter%20training%20times%20compared%20to%20classical%20RL%2C%20less%20involved%0Aclassical%20optimisation%20passes%2C%20or%20better%20use%20of%20available%20training%20data%2C%20and%0Afits%20data-stream%20and%20low-latency%20processing%20scenarios.%20Our%20comprehensive%0Aevaluation%20and%20careful%20discussion%20delivers%20a%20balanced%20perspective%20on%20possible%0Apractical%20quantum%20advantage%2C%20provides%20insights%20for%20future%20systemic%20approaches%2C%0Aand%20allows%20for%20quantitatively%20assessing%20trade-offs%20of%20quantum%20approaches%20for%0Aone%20of%20the%20most%20crucial%20problems%20of%20database%20management%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07770v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHype%2520or%2520Heuristic%253F%2520Quantum%2520Reinforcement%2520Learning%2520for%2520Join%2520Order%250A%2520%2520Optimisation%26entry.906535625%3DMaja%2520Franz%2520and%2520Tobias%2520Winker%2520and%2520Sven%2520Groppe%2520and%2520Wolfgang%2520Mauerer%26entry.1292438233%3D%2520%2520Identifying%2520optimal%2520join%2520orders%2520%2528JOs%2529%2520stands%2520out%2520as%2520a%2520key%2520challenge%2520in%250Adatabase%2520research%2520and%2520engineering.%2520Owing%2520to%2520the%2520large%2520search%2520space%252C%2520established%250Aclassical%2520methods%2520rely%2520on%2520approximations%2520and%2520heuristics.%2520Recent%2520efforts%2520have%250Asuccessfully%2520explored%2520reinforcement%2520learning%2520%2528RL%2529%2520for%2520JO.%2520Likewise%252C%2520quantum%250Aversions%2520of%2520RL%2520have%2520received%2520considerable%2520scientific%2520attention.%2520Yet%252C%2520it%2520is%2520an%250Aopen%2520question%2520if%2520they%2520can%2520achieve%2520sustainable%252C%2520overall%2520practical%2520advantages%250Awith%2520improved%2520quantum%2520processors.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520approach%2520that%2520uses%2520quantum%2520reinforcement%250Alearning%2520%2528QRL%2529%2520for%2520JO%2520based%2520on%2520a%2520hybrid%2520variational%2520quantum%2520ansatz.%2520It%2520is%2520able%250Ato%2520handle%2520general%2520bushy%2520join%2520trees%2520instead%2520of%2520resorting%2520to%2520simpler%2520left-deep%250Avariants%2520as%2520compared%2520to%2520approaches%2520based%2520on%2520quantum%2528-inspired%2529%2520optimisation%252C%250Ayet%2520requires%2520multiple%2520orders%2520of%2520magnitudes%2520fewer%2520qubits%252C%2520which%2520is%2520a%2520scarce%250Aresource%2520even%2520for%2520post-NISQ%2520systems.%250A%2520%2520Despite%2520moderate%2520circuit%2520depth%252C%2520the%2520ansatz%2520exceeds%2520current%2520NISQ%2520capabilities%252C%250Awhich%2520requires%2520an%2520evaluation%2520by%2520numerical%2520simulations.%2520While%2520QRL%2520may%2520not%250Asignificantly%2520outperform%2520classical%2520approaches%2520in%2520solving%2520the%2520JO%2520problem%2520with%250Arespect%2520to%2520result%2520quality%2520%2528albeit%2520we%2520see%2520parity%2529%252C%2520we%2520find%2520a%2520drastic%2520reduction%250Ain%2520required%2520trainable%2520parameters.%2520This%2520benefits%2520practically%2520relevant%2520aspects%250Aranging%2520from%2520shorter%2520training%2520times%2520compared%2520to%2520classical%2520RL%252C%2520less%2520involved%250Aclassical%2520optimisation%2520passes%252C%2520or%2520better%2520use%2520of%2520available%2520training%2520data%252C%2520and%250Afits%2520data-stream%2520and%2520low-latency%2520processing%2520scenarios.%2520Our%2520comprehensive%250Aevaluation%2520and%2520careful%2520discussion%2520delivers%2520a%2520balanced%2520perspective%2520on%2520possible%250Apractical%2520quantum%2520advantage%252C%2520provides%2520insights%2520for%2520future%2520systemic%2520approaches%252C%250Aand%2520allows%2520for%2520quantitatively%2520assessing%2520trade-offs%2520of%2520quantum%2520approaches%2520for%250Aone%2520of%2520the%2520most%2520crucial%2520problems%2520of%2520database%2520management%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07770v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hype%20or%20Heuristic%3F%20Quantum%20Reinforcement%20Learning%20for%20Join%20Order%0A%20%20Optimisation&entry.906535625=Maja%20Franz%20and%20Tobias%20Winker%20and%20Sven%20Groppe%20and%20Wolfgang%20Mauerer&entry.1292438233=%20%20Identifying%20optimal%20join%20orders%20%28JOs%29%20stands%20out%20as%20a%20key%20challenge%20in%0Adatabase%20research%20and%20engineering.%20Owing%20to%20the%20large%20search%20space%2C%20established%0Aclassical%20methods%20rely%20on%20approximations%20and%20heuristics.%20Recent%20efforts%20have%0Asuccessfully%20explored%20reinforcement%20learning%20%28RL%29%20for%20JO.%20Likewise%2C%20quantum%0Aversions%20of%20RL%20have%20received%20considerable%20scientific%20attention.%20Yet%2C%20it%20is%20an%0Aopen%20question%20if%20they%20can%20achieve%20sustainable%2C%20overall%20practical%20advantages%0Awith%20improved%20quantum%20processors.%0A%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20approach%20that%20uses%20quantum%20reinforcement%0Alearning%20%28QRL%29%20for%20JO%20based%20on%20a%20hybrid%20variational%20quantum%20ansatz.%20It%20is%20able%0Ato%20handle%20general%20bushy%20join%20trees%20instead%20of%20resorting%20to%20simpler%20left-deep%0Avariants%20as%20compared%20to%20approaches%20based%20on%20quantum%28-inspired%29%20optimisation%2C%0Ayet%20requires%20multiple%20orders%20of%20magnitudes%20fewer%20qubits%2C%20which%20is%20a%20scarce%0Aresource%20even%20for%20post-NISQ%20systems.%0A%20%20Despite%20moderate%20circuit%20depth%2C%20the%20ansatz%20exceeds%20current%20NISQ%20capabilities%2C%0Awhich%20requires%20an%20evaluation%20by%20numerical%20simulations.%20While%20QRL%20may%20not%0Asignificantly%20outperform%20classical%20approaches%20in%20solving%20the%20JO%20problem%20with%0Arespect%20to%20result%20quality%20%28albeit%20we%20see%20parity%29%2C%20we%20find%20a%20drastic%20reduction%0Ain%20required%20trainable%20parameters.%20This%20benefits%20practically%20relevant%20aspects%0Aranging%20from%20shorter%20training%20times%20compared%20to%20classical%20RL%2C%20less%20involved%0Aclassical%20optimisation%20passes%2C%20or%20better%20use%20of%20available%20training%20data%2C%20and%0Afits%20data-stream%20and%20low-latency%20processing%20scenarios.%20Our%20comprehensive%0Aevaluation%20and%20careful%20discussion%20delivers%20a%20balanced%20perspective%20on%20possible%0Apractical%20quantum%20advantage%2C%20provides%20insights%20for%20future%20systemic%20approaches%2C%0Aand%20allows%20for%20quantitatively%20assessing%20trade-offs%20of%20quantum%20approaches%20for%0Aone%20of%20the%20most%20crucial%20problems%20of%20database%20management%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07770v1&entry.124074799=Read"},
{"title": "Adaptive Human-Swarm Interaction based on Workload Measurement using\n  Functional Near-Infrared Spectroscopy", "author": "Ayodeji O. Abioye and Aleksandra Landowska and William Hunt and Horia Maior and Sarvapali D. Ramchurn and Mohammad Naiseh and Alec Banks and Mohammad D. Soorati", "abstract": "  One of the challenges of human-swarm interaction (HSI) is how to manage the\noperator's workload. In order to do this, we propose a novel neurofeedback\ntechnique for the real-time measurement of workload using functional\nnear-infrared spectroscopy (fNIRS). The objective is to develop a baseline for\nworkload measurement in human-swarm interaction using fNIRS and to develop an\ninterface that dynamically adapts to the operator's workload. The proposed\nmethod consists of using fNIRS device to measure brain activity, process this\nthrough a machine learning algorithm, and pass it on to the HSI interface. By\ndynamically adapting the HSI interface, the swarm operator's workload could be\nreduced and the performance improved.\n", "link": "http://arxiv.org/abs/2405.07834v1", "date": "2024-05-13", "relevancy": 1.6711, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4412}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4203}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Human-Swarm%20Interaction%20based%20on%20Workload%20Measurement%20using%0A%20%20Functional%20Near-Infrared%20Spectroscopy&body=Title%3A%20Adaptive%20Human-Swarm%20Interaction%20based%20on%20Workload%20Measurement%20using%0A%20%20Functional%20Near-Infrared%20Spectroscopy%0AAuthor%3A%20Ayodeji%20O.%20Abioye%20and%20Aleksandra%20Landowska%20and%20William%20Hunt%20and%20Horia%20Maior%20and%20Sarvapali%20D.%20Ramchurn%20and%20Mohammad%20Naiseh%20and%20Alec%20Banks%20and%20Mohammad%20D.%20Soorati%0AAbstract%3A%20%20%20One%20of%20the%20challenges%20of%20human-swarm%20interaction%20%28HSI%29%20is%20how%20to%20manage%20the%0Aoperator%27s%20workload.%20In%20order%20to%20do%20this%2C%20we%20propose%20a%20novel%20neurofeedback%0Atechnique%20for%20the%20real-time%20measurement%20of%20workload%20using%20functional%0Anear-infrared%20spectroscopy%20%28fNIRS%29.%20The%20objective%20is%20to%20develop%20a%20baseline%20for%0Aworkload%20measurement%20in%20human-swarm%20interaction%20using%20fNIRS%20and%20to%20develop%20an%0Ainterface%20that%20dynamically%20adapts%20to%20the%20operator%27s%20workload.%20The%20proposed%0Amethod%20consists%20of%20using%20fNIRS%20device%20to%20measure%20brain%20activity%2C%20process%20this%0Athrough%20a%20machine%20learning%20algorithm%2C%20and%20pass%20it%20on%20to%20the%20HSI%20interface.%20By%0Adynamically%20adapting%20the%20HSI%20interface%2C%20the%20swarm%20operator%27s%20workload%20could%20be%0Areduced%20and%20the%20performance%20improved.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Human-Swarm%2520Interaction%2520based%2520on%2520Workload%2520Measurement%2520using%250A%2520%2520Functional%2520Near-Infrared%2520Spectroscopy%26entry.906535625%3DAyodeji%2520O.%2520Abioye%2520and%2520Aleksandra%2520Landowska%2520and%2520William%2520Hunt%2520and%2520Horia%2520Maior%2520and%2520Sarvapali%2520D.%2520Ramchurn%2520and%2520Mohammad%2520Naiseh%2520and%2520Alec%2520Banks%2520and%2520Mohammad%2520D.%2520Soorati%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520challenges%2520of%2520human-swarm%2520interaction%2520%2528HSI%2529%2520is%2520how%2520to%2520manage%2520the%250Aoperator%2527s%2520workload.%2520In%2520order%2520to%2520do%2520this%252C%2520we%2520propose%2520a%2520novel%2520neurofeedback%250Atechnique%2520for%2520the%2520real-time%2520measurement%2520of%2520workload%2520using%2520functional%250Anear-infrared%2520spectroscopy%2520%2528fNIRS%2529.%2520The%2520objective%2520is%2520to%2520develop%2520a%2520baseline%2520for%250Aworkload%2520measurement%2520in%2520human-swarm%2520interaction%2520using%2520fNIRS%2520and%2520to%2520develop%2520an%250Ainterface%2520that%2520dynamically%2520adapts%2520to%2520the%2520operator%2527s%2520workload.%2520The%2520proposed%250Amethod%2520consists%2520of%2520using%2520fNIRS%2520device%2520to%2520measure%2520brain%2520activity%252C%2520process%2520this%250Athrough%2520a%2520machine%2520learning%2520algorithm%252C%2520and%2520pass%2520it%2520on%2520to%2520the%2520HSI%2520interface.%2520By%250Adynamically%2520adapting%2520the%2520HSI%2520interface%252C%2520the%2520swarm%2520operator%2527s%2520workload%2520could%2520be%250Areduced%2520and%2520the%2520performance%2520improved.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Human-Swarm%20Interaction%20based%20on%20Workload%20Measurement%20using%0A%20%20Functional%20Near-Infrared%20Spectroscopy&entry.906535625=Ayodeji%20O.%20Abioye%20and%20Aleksandra%20Landowska%20and%20William%20Hunt%20and%20Horia%20Maior%20and%20Sarvapali%20D.%20Ramchurn%20and%20Mohammad%20Naiseh%20and%20Alec%20Banks%20and%20Mohammad%20D.%20Soorati&entry.1292438233=%20%20One%20of%20the%20challenges%20of%20human-swarm%20interaction%20%28HSI%29%20is%20how%20to%20manage%20the%0Aoperator%27s%20workload.%20In%20order%20to%20do%20this%2C%20we%20propose%20a%20novel%20neurofeedback%0Atechnique%20for%20the%20real-time%20measurement%20of%20workload%20using%20functional%0Anear-infrared%20spectroscopy%20%28fNIRS%29.%20The%20objective%20is%20to%20develop%20a%20baseline%20for%0Aworkload%20measurement%20in%20human-swarm%20interaction%20using%20fNIRS%20and%20to%20develop%20an%0Ainterface%20that%20dynamically%20adapts%20to%20the%20operator%27s%20workload.%20The%20proposed%0Amethod%20consists%20of%20using%20fNIRS%20device%20to%20measure%20brain%20activity%2C%20process%20this%0Athrough%20a%20machine%20learning%20algorithm%2C%20and%20pass%20it%20on%20to%20the%20HSI%20interface.%20By%0Adynamically%20adapting%20the%20HSI%20interface%2C%20the%20swarm%20operator%27s%20workload%20could%20be%0Areduced%20and%20the%20performance%20improved.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07834v1&entry.124074799=Read"},
{"title": "Regularized Q-learning", "author": "Han-Dong Lim and Donghwan Lee", "abstract": "  Q-learning is widely used algorithm in reinforcement learning community.\nUnder the lookup table setting, its convergence is well established. However,\nits behavior is known to be unstable with the linear function approximation\ncase. This paper develops a new Q-learning algorithm that converges when linear\nfunction approximation is used. We prove that simply adding an appropriate\nregularization term ensures convergence of the algorithm. We prove its\nstability using a recent analysis tool based on switching system models.\nMoreover, we experimentally show that it converges in environments where\nQ-learning with linear function approximation has known to diverge. We also\nprovide an error bound on the solution where the algorithm converges.\n", "link": "http://arxiv.org/abs/2202.05404v6", "date": "2024-05-13", "relevancy": 1.6974, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4436}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4223}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regularized%20Q-learning&body=Title%3A%20Regularized%20Q-learning%0AAuthor%3A%20Han-Dong%20Lim%20and%20Donghwan%20Lee%0AAbstract%3A%20%20%20Q-learning%20is%20widely%20used%20algorithm%20in%20reinforcement%20learning%20community.%0AUnder%20the%20lookup%20table%20setting%2C%20its%20convergence%20is%20well%20established.%20However%2C%0Aits%20behavior%20is%20known%20to%20be%20unstable%20with%20the%20linear%20function%20approximation%0Acase.%20This%20paper%20develops%20a%20new%20Q-learning%20algorithm%20that%20converges%20when%20linear%0Afunction%20approximation%20is%20used.%20We%20prove%20that%20simply%20adding%20an%20appropriate%0Aregularization%20term%20ensures%20convergence%20of%20the%20algorithm.%20We%20prove%20its%0Astability%20using%20a%20recent%20analysis%20tool%20based%20on%20switching%20system%20models.%0AMoreover%2C%20we%20experimentally%20show%20that%20it%20converges%20in%20environments%20where%0AQ-learning%20with%20linear%20function%20approximation%20has%20known%20to%20diverge.%20We%20also%0Aprovide%20an%20error%20bound%20on%20the%20solution%20where%20the%20algorithm%20converges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2202.05404v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegularized%2520Q-learning%26entry.906535625%3DHan-Dong%2520Lim%2520and%2520Donghwan%2520Lee%26entry.1292438233%3D%2520%2520Q-learning%2520is%2520widely%2520used%2520algorithm%2520in%2520reinforcement%2520learning%2520community.%250AUnder%2520the%2520lookup%2520table%2520setting%252C%2520its%2520convergence%2520is%2520well%2520established.%2520However%252C%250Aits%2520behavior%2520is%2520known%2520to%2520be%2520unstable%2520with%2520the%2520linear%2520function%2520approximation%250Acase.%2520This%2520paper%2520develops%2520a%2520new%2520Q-learning%2520algorithm%2520that%2520converges%2520when%2520linear%250Afunction%2520approximation%2520is%2520used.%2520We%2520prove%2520that%2520simply%2520adding%2520an%2520appropriate%250Aregularization%2520term%2520ensures%2520convergence%2520of%2520the%2520algorithm.%2520We%2520prove%2520its%250Astability%2520using%2520a%2520recent%2520analysis%2520tool%2520based%2520on%2520switching%2520system%2520models.%250AMoreover%252C%2520we%2520experimentally%2520show%2520that%2520it%2520converges%2520in%2520environments%2520where%250AQ-learning%2520with%2520linear%2520function%2520approximation%2520has%2520known%2520to%2520diverge.%2520We%2520also%250Aprovide%2520an%2520error%2520bound%2520on%2520the%2520solution%2520where%2520the%2520algorithm%2520converges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2202.05404v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regularized%20Q-learning&entry.906535625=Han-Dong%20Lim%20and%20Donghwan%20Lee&entry.1292438233=%20%20Q-learning%20is%20widely%20used%20algorithm%20in%20reinforcement%20learning%20community.%0AUnder%20the%20lookup%20table%20setting%2C%20its%20convergence%20is%20well%20established.%20However%2C%0Aits%20behavior%20is%20known%20to%20be%20unstable%20with%20the%20linear%20function%20approximation%0Acase.%20This%20paper%20develops%20a%20new%20Q-learning%20algorithm%20that%20converges%20when%20linear%0Afunction%20approximation%20is%20used.%20We%20prove%20that%20simply%20adding%20an%20appropriate%0Aregularization%20term%20ensures%20convergence%20of%20the%20algorithm.%20We%20prove%20its%0Astability%20using%20a%20recent%20analysis%20tool%20based%20on%20switching%20system%20models.%0AMoreover%2C%20we%20experimentally%20show%20that%20it%20converges%20in%20environments%20where%0AQ-learning%20with%20linear%20function%20approximation%20has%20known%20to%20diverge.%20We%20also%0Aprovide%20an%20error%20bound%20on%20the%20solution%20where%20the%20algorithm%20converges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2202.05404v6&entry.124074799=Read"},
{"title": "Challenges and Opportunities of NLP for HR Applications: A Discussion\n  Paper", "author": "Jochen L. Leidner and Mark Stevenson", "abstract": "  Over the course of the recent decade, tremendous progress has been made in\nthe areas of machine learning and natural language processing, which opened up\nvast areas of potential application use cases, including hiring and human\nresource management. We review the use cases for text analytics in the realm of\nhuman resources/personnel management, including actually realized as well as\npotential but not yet implemented ones, and we analyze the opportunities and\nrisks of these.\n", "link": "http://arxiv.org/abs/2405.07766v1", "date": "2024-05-13", "relevancy": 1.5734, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4111}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4055}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Challenges%20and%20Opportunities%20of%20NLP%20for%20HR%20Applications%3A%20A%20Discussion%0A%20%20Paper&body=Title%3A%20Challenges%20and%20Opportunities%20of%20NLP%20for%20HR%20Applications%3A%20A%20Discussion%0A%20%20Paper%0AAuthor%3A%20Jochen%20L.%20Leidner%20and%20Mark%20Stevenson%0AAbstract%3A%20%20%20Over%20the%20course%20of%20the%20recent%20decade%2C%20tremendous%20progress%20has%20been%20made%20in%0Athe%20areas%20of%20machine%20learning%20and%20natural%20language%20processing%2C%20which%20opened%20up%0Avast%20areas%20of%20potential%20application%20use%20cases%2C%20including%20hiring%20and%20human%0Aresource%20management.%20We%20review%20the%20use%20cases%20for%20text%20analytics%20in%20the%20realm%20of%0Ahuman%20resources/personnel%20management%2C%20including%20actually%20realized%20as%20well%20as%0Apotential%20but%20not%20yet%20implemented%20ones%2C%20and%20we%20analyze%20the%20opportunities%20and%0Arisks%20of%20these.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChallenges%2520and%2520Opportunities%2520of%2520NLP%2520for%2520HR%2520Applications%253A%2520A%2520Discussion%250A%2520%2520Paper%26entry.906535625%3DJochen%2520L.%2520Leidner%2520and%2520Mark%2520Stevenson%26entry.1292438233%3D%2520%2520Over%2520the%2520course%2520of%2520the%2520recent%2520decade%252C%2520tremendous%2520progress%2520has%2520been%2520made%2520in%250Athe%2520areas%2520of%2520machine%2520learning%2520and%2520natural%2520language%2520processing%252C%2520which%2520opened%2520up%250Avast%2520areas%2520of%2520potential%2520application%2520use%2520cases%252C%2520including%2520hiring%2520and%2520human%250Aresource%2520management.%2520We%2520review%2520the%2520use%2520cases%2520for%2520text%2520analytics%2520in%2520the%2520realm%2520of%250Ahuman%2520resources/personnel%2520management%252C%2520including%2520actually%2520realized%2520as%2520well%2520as%250Apotential%2520but%2520not%2520yet%2520implemented%2520ones%252C%2520and%2520we%2520analyze%2520the%2520opportunities%2520and%250Arisks%2520of%2520these.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Challenges%20and%20Opportunities%20of%20NLP%20for%20HR%20Applications%3A%20A%20Discussion%0A%20%20Paper&entry.906535625=Jochen%20L.%20Leidner%20and%20Mark%20Stevenson&entry.1292438233=%20%20Over%20the%20course%20of%20the%20recent%20decade%2C%20tremendous%20progress%20has%20been%20made%20in%0Athe%20areas%20of%20machine%20learning%20and%20natural%20language%20processing%2C%20which%20opened%20up%0Avast%20areas%20of%20potential%20application%20use%20cases%2C%20including%20hiring%20and%20human%0Aresource%20management.%20We%20review%20the%20use%20cases%20for%20text%20analytics%20in%20the%20realm%20of%0Ahuman%20resources/personnel%20management%2C%20including%20actually%20realized%20as%20well%20as%0Apotential%20but%20not%20yet%20implemented%20ones%2C%20and%20we%20analyze%20the%20opportunities%20and%0Arisks%20of%20these.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07766v1&entry.124074799=Read"},
{"title": "Stable Diffusion-based Data Augmentation for Federated Learning with\n  Non-IID Data", "author": "Mahdi Morafah and Matthias Reisser and Bill Lin and Christos Louizos", "abstract": "  The proliferation of edge devices has brought Federated Learning (FL) to the\nforefront as a promising paradigm for decentralized and collaborative model\ntraining while preserving the privacy of clients' data. However, FL struggles\nwith a significant performance reduction and poor convergence when confronted\nwith Non-Independent and Identically Distributed (Non-IID) data distributions\namong participating clients. While previous efforts, such as client drift\nmitigation and advanced server-side model fusion techniques, have shown some\nsuccess in addressing this challenge, they often overlook the root cause of the\nperformance reduction - the absence of identical data accurately mirroring the\nglobal data distribution among clients. In this paper, we introduce Gen-FedSD,\na novel approach that harnesses the powerful capability of state-of-the-art\ntext-to-image foundation models to bridge the significant Non-IID performance\ngaps in FL. In Gen-FedSD, each client constructs textual prompts for each class\nlabel and leverages an off-the-shelf state-of-the-art pre-trained Stable\nDiffusion model to synthesize high-quality data samples. The generated\nsynthetic data is tailored to each client's unique local data gaps and\ndistribution disparities, effectively making the final augmented local data\nIID. Through extensive experimentation, we demonstrate that Gen-FedSD achieves\nstate-of-the-art performance and significant communication cost savings across\nvarious datasets and Non-IID settings.\n", "link": "http://arxiv.org/abs/2405.07925v1", "date": "2024-05-13", "relevancy": 1.702, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5801}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5648}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20Diffusion-based%20Data%20Augmentation%20for%20Federated%20Learning%20with%0A%20%20Non-IID%20Data&body=Title%3A%20Stable%20Diffusion-based%20Data%20Augmentation%20for%20Federated%20Learning%20with%0A%20%20Non-IID%20Data%0AAuthor%3A%20Mahdi%20Morafah%20and%20Matthias%20Reisser%20and%20Bill%20Lin%20and%20Christos%20Louizos%0AAbstract%3A%20%20%20The%20proliferation%20of%20edge%20devices%20has%20brought%20Federated%20Learning%20%28FL%29%20to%20the%0Aforefront%20as%20a%20promising%20paradigm%20for%20decentralized%20and%20collaborative%20model%0Atraining%20while%20preserving%20the%20privacy%20of%20clients%27%20data.%20However%2C%20FL%20struggles%0Awith%20a%20significant%20performance%20reduction%20and%20poor%20convergence%20when%20confronted%0Awith%20Non-Independent%20and%20Identically%20Distributed%20%28Non-IID%29%20data%20distributions%0Aamong%20participating%20clients.%20While%20previous%20efforts%2C%20such%20as%20client%20drift%0Amitigation%20and%20advanced%20server-side%20model%20fusion%20techniques%2C%20have%20shown%20some%0Asuccess%20in%20addressing%20this%20challenge%2C%20they%20often%20overlook%20the%20root%20cause%20of%20the%0Aperformance%20reduction%20-%20the%20absence%20of%20identical%20data%20accurately%20mirroring%20the%0Aglobal%20data%20distribution%20among%20clients.%20In%20this%20paper%2C%20we%20introduce%20Gen-FedSD%2C%0Aa%20novel%20approach%20that%20harnesses%20the%20powerful%20capability%20of%20state-of-the-art%0Atext-to-image%20foundation%20models%20to%20bridge%20the%20significant%20Non-IID%20performance%0Agaps%20in%20FL.%20In%20Gen-FedSD%2C%20each%20client%20constructs%20textual%20prompts%20for%20each%20class%0Alabel%20and%20leverages%20an%20off-the-shelf%20state-of-the-art%20pre-trained%20Stable%0ADiffusion%20model%20to%20synthesize%20high-quality%20data%20samples.%20The%20generated%0Asynthetic%20data%20is%20tailored%20to%20each%20client%27s%20unique%20local%20data%20gaps%20and%0Adistribution%20disparities%2C%20effectively%20making%20the%20final%20augmented%20local%20data%0AIID.%20Through%20extensive%20experimentation%2C%20we%20demonstrate%20that%20Gen-FedSD%20achieves%0Astate-of-the-art%20performance%20and%20significant%20communication%20cost%20savings%20across%0Avarious%20datasets%20and%20Non-IID%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520Diffusion-based%2520Data%2520Augmentation%2520for%2520Federated%2520Learning%2520with%250A%2520%2520Non-IID%2520Data%26entry.906535625%3DMahdi%2520Morafah%2520and%2520Matthias%2520Reisser%2520and%2520Bill%2520Lin%2520and%2520Christos%2520Louizos%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520edge%2520devices%2520has%2520brought%2520Federated%2520Learning%2520%2528FL%2529%2520to%2520the%250Aforefront%2520as%2520a%2520promising%2520paradigm%2520for%2520decentralized%2520and%2520collaborative%2520model%250Atraining%2520while%2520preserving%2520the%2520privacy%2520of%2520clients%2527%2520data.%2520However%252C%2520FL%2520struggles%250Awith%2520a%2520significant%2520performance%2520reduction%2520and%2520poor%2520convergence%2520when%2520confronted%250Awith%2520Non-Independent%2520and%2520Identically%2520Distributed%2520%2528Non-IID%2529%2520data%2520distributions%250Aamong%2520participating%2520clients.%2520While%2520previous%2520efforts%252C%2520such%2520as%2520client%2520drift%250Amitigation%2520and%2520advanced%2520server-side%2520model%2520fusion%2520techniques%252C%2520have%2520shown%2520some%250Asuccess%2520in%2520addressing%2520this%2520challenge%252C%2520they%2520often%2520overlook%2520the%2520root%2520cause%2520of%2520the%250Aperformance%2520reduction%2520-%2520the%2520absence%2520of%2520identical%2520data%2520accurately%2520mirroring%2520the%250Aglobal%2520data%2520distribution%2520among%2520clients.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Gen-FedSD%252C%250Aa%2520novel%2520approach%2520that%2520harnesses%2520the%2520powerful%2520capability%2520of%2520state-of-the-art%250Atext-to-image%2520foundation%2520models%2520to%2520bridge%2520the%2520significant%2520Non-IID%2520performance%250Agaps%2520in%2520FL.%2520In%2520Gen-FedSD%252C%2520each%2520client%2520constructs%2520textual%2520prompts%2520for%2520each%2520class%250Alabel%2520and%2520leverages%2520an%2520off-the-shelf%2520state-of-the-art%2520pre-trained%2520Stable%250ADiffusion%2520model%2520to%2520synthesize%2520high-quality%2520data%2520samples.%2520The%2520generated%250Asynthetic%2520data%2520is%2520tailored%2520to%2520each%2520client%2527s%2520unique%2520local%2520data%2520gaps%2520and%250Adistribution%2520disparities%252C%2520effectively%2520making%2520the%2520final%2520augmented%2520local%2520data%250AIID.%2520Through%2520extensive%2520experimentation%252C%2520we%2520demonstrate%2520that%2520Gen-FedSD%2520achieves%250Astate-of-the-art%2520performance%2520and%2520significant%2520communication%2520cost%2520savings%2520across%250Avarious%2520datasets%2520and%2520Non-IID%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Diffusion-based%20Data%20Augmentation%20for%20Federated%20Learning%20with%0A%20%20Non-IID%20Data&entry.906535625=Mahdi%20Morafah%20and%20Matthias%20Reisser%20and%20Bill%20Lin%20and%20Christos%20Louizos&entry.1292438233=%20%20The%20proliferation%20of%20edge%20devices%20has%20brought%20Federated%20Learning%20%28FL%29%20to%20the%0Aforefront%20as%20a%20promising%20paradigm%20for%20decentralized%20and%20collaborative%20model%0Atraining%20while%20preserving%20the%20privacy%20of%20clients%27%20data.%20However%2C%20FL%20struggles%0Awith%20a%20significant%20performance%20reduction%20and%20poor%20convergence%20when%20confronted%0Awith%20Non-Independent%20and%20Identically%20Distributed%20%28Non-IID%29%20data%20distributions%0Aamong%20participating%20clients.%20While%20previous%20efforts%2C%20such%20as%20client%20drift%0Amitigation%20and%20advanced%20server-side%20model%20fusion%20techniques%2C%20have%20shown%20some%0Asuccess%20in%20addressing%20this%20challenge%2C%20they%20often%20overlook%20the%20root%20cause%20of%20the%0Aperformance%20reduction%20-%20the%20absence%20of%20identical%20data%20accurately%20mirroring%20the%0Aglobal%20data%20distribution%20among%20clients.%20In%20this%20paper%2C%20we%20introduce%20Gen-FedSD%2C%0Aa%20novel%20approach%20that%20harnesses%20the%20powerful%20capability%20of%20state-of-the-art%0Atext-to-image%20foundation%20models%20to%20bridge%20the%20significant%20Non-IID%20performance%0Agaps%20in%20FL.%20In%20Gen-FedSD%2C%20each%20client%20constructs%20textual%20prompts%20for%20each%20class%0Alabel%20and%20leverages%20an%20off-the-shelf%20state-of-the-art%20pre-trained%20Stable%0ADiffusion%20model%20to%20synthesize%20high-quality%20data%20samples.%20The%20generated%0Asynthetic%20data%20is%20tailored%20to%20each%20client%27s%20unique%20local%20data%20gaps%20and%0Adistribution%20disparities%2C%20effectively%20making%20the%20final%20augmented%20local%20data%0AIID.%20Through%20extensive%20experimentation%2C%20we%20demonstrate%20that%20Gen-FedSD%20achieves%0Astate-of-the-art%20performance%20and%20significant%20communication%20cost%20savings%20across%0Avarious%20datasets%20and%20Non-IID%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07925v1&entry.124074799=Read"},
{"title": "Sample Selection Bias in Machine Learning for Healthcare", "author": "Vinod Kumar Chauhan and Lei Clifton and Achille Sala\u00fcn and Huiqi Yvonne Lu and Kim Branson and Patrick Schwab and Gaurav Nigam and David A. Clifton", "abstract": "  While machine learning algorithms hold promise for personalised medicine,\ntheir clinical adoption remains limited. One critical factor contributing to\nthis restraint is sample selection bias (SSB) which refers to the study\npopulation being less representative of the target population, leading to\nbiased and potentially harmful decisions. Despite being well-known in the\nliterature, SSB remains scarcely studied in machine learning for healthcare.\nMoreover, the existing techniques try to correct the bias by balancing\ndistributions between the study and the target populations, which may result in\na loss of predictive performance. To address these problems, our study\nillustrates the potential risks associated with SSB by examining SSB's impact\non the performance of machine learning algorithms. Most importantly, we propose\na new research direction for addressing SSB, based on the target population\nidentification rather than the bias correction. Specifically, we propose two\nindependent networks (T-Net) and a multitasking network (MT-Net) for addressing\nSSB, where one network/task identifies the target subpopulation which is\nrepresentative of the study population and the second makes predictions for the\nidentified subpopulation. Our empirical results with synthetic and\nsemi-synthetic datasets highlight that SSB can lead to a large drop in the\nperformance of an algorithm for the target population as compared with the\nstudy population, as well as a substantial difference in the performance for\nthe target subpopulations that are representative of the selected and the\nnon-selected patients from the study population. Furthermore, our proposed\ntechniques demonstrate robustness across various settings, including different\ndataset sizes, event rates, and selection rates, outperforming the existing\nbias correction techniques.\n", "link": "http://arxiv.org/abs/2405.07841v1", "date": "2024-05-13", "relevancy": 1.749, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4418}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4376}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sample%20Selection%20Bias%20in%20Machine%20Learning%20for%20Healthcare&body=Title%3A%20Sample%20Selection%20Bias%20in%20Machine%20Learning%20for%20Healthcare%0AAuthor%3A%20Vinod%20Kumar%20Chauhan%20and%20Lei%20Clifton%20and%20Achille%20Sala%C3%BCn%20and%20Huiqi%20Yvonne%20Lu%20and%20Kim%20Branson%20and%20Patrick%20Schwab%20and%20Gaurav%20Nigam%20and%20David%20A.%20Clifton%0AAbstract%3A%20%20%20While%20machine%20learning%20algorithms%20hold%20promise%20for%20personalised%20medicine%2C%0Atheir%20clinical%20adoption%20remains%20limited.%20One%20critical%20factor%20contributing%20to%0Athis%20restraint%20is%20sample%20selection%20bias%20%28SSB%29%20which%20refers%20to%20the%20study%0Apopulation%20being%20less%20representative%20of%20the%20target%20population%2C%20leading%20to%0Abiased%20and%20potentially%20harmful%20decisions.%20Despite%20being%20well-known%20in%20the%0Aliterature%2C%20SSB%20remains%20scarcely%20studied%20in%20machine%20learning%20for%20healthcare.%0AMoreover%2C%20the%20existing%20techniques%20try%20to%20correct%20the%20bias%20by%20balancing%0Adistributions%20between%20the%20study%20and%20the%20target%20populations%2C%20which%20may%20result%20in%0Aa%20loss%20of%20predictive%20performance.%20To%20address%20these%20problems%2C%20our%20study%0Aillustrates%20the%20potential%20risks%20associated%20with%20SSB%20by%20examining%20SSB%27s%20impact%0Aon%20the%20performance%20of%20machine%20learning%20algorithms.%20Most%20importantly%2C%20we%20propose%0Aa%20new%20research%20direction%20for%20addressing%20SSB%2C%20based%20on%20the%20target%20population%0Aidentification%20rather%20than%20the%20bias%20correction.%20Specifically%2C%20we%20propose%20two%0Aindependent%20networks%20%28T-Net%29%20and%20a%20multitasking%20network%20%28MT-Net%29%20for%20addressing%0ASSB%2C%20where%20one%20network/task%20identifies%20the%20target%20subpopulation%20which%20is%0Arepresentative%20of%20the%20study%20population%20and%20the%20second%20makes%20predictions%20for%20the%0Aidentified%20subpopulation.%20Our%20empirical%20results%20with%20synthetic%20and%0Asemi-synthetic%20datasets%20highlight%20that%20SSB%20can%20lead%20to%20a%20large%20drop%20in%20the%0Aperformance%20of%20an%20algorithm%20for%20the%20target%20population%20as%20compared%20with%20the%0Astudy%20population%2C%20as%20well%20as%20a%20substantial%20difference%20in%20the%20performance%20for%0Athe%20target%20subpopulations%20that%20are%20representative%20of%20the%20selected%20and%20the%0Anon-selected%20patients%20from%20the%20study%20population.%20Furthermore%2C%20our%20proposed%0Atechniques%20demonstrate%20robustness%20across%20various%20settings%2C%20including%20different%0Adataset%20sizes%2C%20event%20rates%2C%20and%20selection%20rates%2C%20outperforming%20the%20existing%0Abias%20correction%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07841v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSample%2520Selection%2520Bias%2520in%2520Machine%2520Learning%2520for%2520Healthcare%26entry.906535625%3DVinod%2520Kumar%2520Chauhan%2520and%2520Lei%2520Clifton%2520and%2520Achille%2520Sala%25C3%25BCn%2520and%2520Huiqi%2520Yvonne%2520Lu%2520and%2520Kim%2520Branson%2520and%2520Patrick%2520Schwab%2520and%2520Gaurav%2520Nigam%2520and%2520David%2520A.%2520Clifton%26entry.1292438233%3D%2520%2520While%2520machine%2520learning%2520algorithms%2520hold%2520promise%2520for%2520personalised%2520medicine%252C%250Atheir%2520clinical%2520adoption%2520remains%2520limited.%2520One%2520critical%2520factor%2520contributing%2520to%250Athis%2520restraint%2520is%2520sample%2520selection%2520bias%2520%2528SSB%2529%2520which%2520refers%2520to%2520the%2520study%250Apopulation%2520being%2520less%2520representative%2520of%2520the%2520target%2520population%252C%2520leading%2520to%250Abiased%2520and%2520potentially%2520harmful%2520decisions.%2520Despite%2520being%2520well-known%2520in%2520the%250Aliterature%252C%2520SSB%2520remains%2520scarcely%2520studied%2520in%2520machine%2520learning%2520for%2520healthcare.%250AMoreover%252C%2520the%2520existing%2520techniques%2520try%2520to%2520correct%2520the%2520bias%2520by%2520balancing%250Adistributions%2520between%2520the%2520study%2520and%2520the%2520target%2520populations%252C%2520which%2520may%2520result%2520in%250Aa%2520loss%2520of%2520predictive%2520performance.%2520To%2520address%2520these%2520problems%252C%2520our%2520study%250Aillustrates%2520the%2520potential%2520risks%2520associated%2520with%2520SSB%2520by%2520examining%2520SSB%2527s%2520impact%250Aon%2520the%2520performance%2520of%2520machine%2520learning%2520algorithms.%2520Most%2520importantly%252C%2520we%2520propose%250Aa%2520new%2520research%2520direction%2520for%2520addressing%2520SSB%252C%2520based%2520on%2520the%2520target%2520population%250Aidentification%2520rather%2520than%2520the%2520bias%2520correction.%2520Specifically%252C%2520we%2520propose%2520two%250Aindependent%2520networks%2520%2528T-Net%2529%2520and%2520a%2520multitasking%2520network%2520%2528MT-Net%2529%2520for%2520addressing%250ASSB%252C%2520where%2520one%2520network/task%2520identifies%2520the%2520target%2520subpopulation%2520which%2520is%250Arepresentative%2520of%2520the%2520study%2520population%2520and%2520the%2520second%2520makes%2520predictions%2520for%2520the%250Aidentified%2520subpopulation.%2520Our%2520empirical%2520results%2520with%2520synthetic%2520and%250Asemi-synthetic%2520datasets%2520highlight%2520that%2520SSB%2520can%2520lead%2520to%2520a%2520large%2520drop%2520in%2520the%250Aperformance%2520of%2520an%2520algorithm%2520for%2520the%2520target%2520population%2520as%2520compared%2520with%2520the%250Astudy%2520population%252C%2520as%2520well%2520as%2520a%2520substantial%2520difference%2520in%2520the%2520performance%2520for%250Athe%2520target%2520subpopulations%2520that%2520are%2520representative%2520of%2520the%2520selected%2520and%2520the%250Anon-selected%2520patients%2520from%2520the%2520study%2520population.%2520Furthermore%252C%2520our%2520proposed%250Atechniques%2520demonstrate%2520robustness%2520across%2520various%2520settings%252C%2520including%2520different%250Adataset%2520sizes%252C%2520event%2520rates%252C%2520and%2520selection%2520rates%252C%2520outperforming%2520the%2520existing%250Abias%2520correction%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07841v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sample%20Selection%20Bias%20in%20Machine%20Learning%20for%20Healthcare&entry.906535625=Vinod%20Kumar%20Chauhan%20and%20Lei%20Clifton%20and%20Achille%20Sala%C3%BCn%20and%20Huiqi%20Yvonne%20Lu%20and%20Kim%20Branson%20and%20Patrick%20Schwab%20and%20Gaurav%20Nigam%20and%20David%20A.%20Clifton&entry.1292438233=%20%20While%20machine%20learning%20algorithms%20hold%20promise%20for%20personalised%20medicine%2C%0Atheir%20clinical%20adoption%20remains%20limited.%20One%20critical%20factor%20contributing%20to%0Athis%20restraint%20is%20sample%20selection%20bias%20%28SSB%29%20which%20refers%20to%20the%20study%0Apopulation%20being%20less%20representative%20of%20the%20target%20population%2C%20leading%20to%0Abiased%20and%20potentially%20harmful%20decisions.%20Despite%20being%20well-known%20in%20the%0Aliterature%2C%20SSB%20remains%20scarcely%20studied%20in%20machine%20learning%20for%20healthcare.%0AMoreover%2C%20the%20existing%20techniques%20try%20to%20correct%20the%20bias%20by%20balancing%0Adistributions%20between%20the%20study%20and%20the%20target%20populations%2C%20which%20may%20result%20in%0Aa%20loss%20of%20predictive%20performance.%20To%20address%20these%20problems%2C%20our%20study%0Aillustrates%20the%20potential%20risks%20associated%20with%20SSB%20by%20examining%20SSB%27s%20impact%0Aon%20the%20performance%20of%20machine%20learning%20algorithms.%20Most%20importantly%2C%20we%20propose%0Aa%20new%20research%20direction%20for%20addressing%20SSB%2C%20based%20on%20the%20target%20population%0Aidentification%20rather%20than%20the%20bias%20correction.%20Specifically%2C%20we%20propose%20two%0Aindependent%20networks%20%28T-Net%29%20and%20a%20multitasking%20network%20%28MT-Net%29%20for%20addressing%0ASSB%2C%20where%20one%20network/task%20identifies%20the%20target%20subpopulation%20which%20is%0Arepresentative%20of%20the%20study%20population%20and%20the%20second%20makes%20predictions%20for%20the%0Aidentified%20subpopulation.%20Our%20empirical%20results%20with%20synthetic%20and%0Asemi-synthetic%20datasets%20highlight%20that%20SSB%20can%20lead%20to%20a%20large%20drop%20in%20the%0Aperformance%20of%20an%20algorithm%20for%20the%20target%20population%20as%20compared%20with%20the%0Astudy%20population%2C%20as%20well%20as%20a%20substantial%20difference%20in%20the%20performance%20for%0Athe%20target%20subpopulations%20that%20are%20representative%20of%20the%20selected%20and%20the%0Anon-selected%20patients%20from%20the%20study%20population.%20Furthermore%2C%20our%20proposed%0Atechniques%20demonstrate%20robustness%20across%20various%20settings%2C%20including%20different%0Adataset%20sizes%2C%20event%20rates%2C%20and%20selection%20rates%2C%20outperforming%20the%20existing%0Abias%20correction%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07841v1&entry.124074799=Read"},
{"title": "Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of\n  Large Language Models", "author": "Loka Li and Zhenhao Chen and Guangyi Chen and Yixuan Zhang and Yusheng Su and Eric Xing and Kun Zhang", "abstract": "  The recent success of Large Language Models (LLMs) has catalyzed an\nincreasing interest in their self-correction capabilities. This paper presents\na comprehensive investigation into the intrinsic self-correction of LLMs,\nattempting to address the ongoing debate about its feasibility. Our research\nhas identified an important latent factor - the \"confidence\" of LLMs - during\nthe self-correction process. Overlooking this factor may cause the models to\nover-criticize themselves, resulting in unreliable conclusions regarding the\nefficacy of self-correction. We have experimentally observed that LLMs possess\nthe capability to understand the \"confidence\" in their own responses. It\nmotivates us to develop an \"If-or-Else\" (IoE) prompting framework, designed to\nguide LLMs in assessing their own \"confidence\", facilitating intrinsic\nself-corrections. We conduct extensive experiments and demonstrate that our\nIoE-based Prompt can achieve a consistent improvement regarding the accuracy of\nself-corrected responses over the initial answers. Our study not only sheds\nlight on the underlying factors affecting self-correction in LLMs, but also\nintroduces a practical framework that utilizes the IoE prompting principle to\nefficiently improve self-correction capabilities with \"confidence\". The code is\navailable at https://github.com/MBZUAI-CLeaR/IoE-Prompting.git.\n", "link": "http://arxiv.org/abs/2402.12563v3", "date": "2024-05-13", "relevancy": 1.4335, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4834}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4818}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Confidence%20Matters%3A%20Revisiting%20Intrinsic%20Self-Correction%20Capabilities%20of%0A%20%20Large%20Language%20Models&body=Title%3A%20Confidence%20Matters%3A%20Revisiting%20Intrinsic%20Self-Correction%20Capabilities%20of%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Loka%20Li%20and%20Zhenhao%20Chen%20and%20Guangyi%20Chen%20and%20Yixuan%20Zhang%20and%20Yusheng%20Su%20and%20Eric%20Xing%20and%20Kun%20Zhang%0AAbstract%3A%20%20%20The%20recent%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20catalyzed%20an%0Aincreasing%20interest%20in%20their%20self-correction%20capabilities.%20This%20paper%20presents%0Aa%20comprehensive%20investigation%20into%20the%20intrinsic%20self-correction%20of%20LLMs%2C%0Aattempting%20to%20address%20the%20ongoing%20debate%20about%20its%20feasibility.%20Our%20research%0Ahas%20identified%20an%20important%20latent%20factor%20-%20the%20%22confidence%22%20of%20LLMs%20-%20during%0Athe%20self-correction%20process.%20Overlooking%20this%20factor%20may%20cause%20the%20models%20to%0Aover-criticize%20themselves%2C%20resulting%20in%20unreliable%20conclusions%20regarding%20the%0Aefficacy%20of%20self-correction.%20We%20have%20experimentally%20observed%20that%20LLMs%20possess%0Athe%20capability%20to%20understand%20the%20%22confidence%22%20in%20their%20own%20responses.%20It%0Amotivates%20us%20to%20develop%20an%20%22If-or-Else%22%20%28IoE%29%20prompting%20framework%2C%20designed%20to%0Aguide%20LLMs%20in%20assessing%20their%20own%20%22confidence%22%2C%20facilitating%20intrinsic%0Aself-corrections.%20We%20conduct%20extensive%20experiments%20and%20demonstrate%20that%20our%0AIoE-based%20Prompt%20can%20achieve%20a%20consistent%20improvement%20regarding%20the%20accuracy%20of%0Aself-corrected%20responses%20over%20the%20initial%20answers.%20Our%20study%20not%20only%20sheds%0Alight%20on%20the%20underlying%20factors%20affecting%20self-correction%20in%20LLMs%2C%20but%20also%0Aintroduces%20a%20practical%20framework%20that%20utilizes%20the%20IoE%20prompting%20principle%20to%0Aefficiently%20improve%20self-correction%20capabilities%20with%20%22confidence%22.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/MBZUAI-CLeaR/IoE-Prompting.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12563v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConfidence%2520Matters%253A%2520Revisiting%2520Intrinsic%2520Self-Correction%2520Capabilities%2520of%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DLoka%2520Li%2520and%2520Zhenhao%2520Chen%2520and%2520Guangyi%2520Chen%2520and%2520Yixuan%2520Zhang%2520and%2520Yusheng%2520Su%2520and%2520Eric%2520Xing%2520and%2520Kun%2520Zhang%26entry.1292438233%3D%2520%2520The%2520recent%2520success%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520catalyzed%2520an%250Aincreasing%2520interest%2520in%2520their%2520self-correction%2520capabilities.%2520This%2520paper%2520presents%250Aa%2520comprehensive%2520investigation%2520into%2520the%2520intrinsic%2520self-correction%2520of%2520LLMs%252C%250Aattempting%2520to%2520address%2520the%2520ongoing%2520debate%2520about%2520its%2520feasibility.%2520Our%2520research%250Ahas%2520identified%2520an%2520important%2520latent%2520factor%2520-%2520the%2520%2522confidence%2522%2520of%2520LLMs%2520-%2520during%250Athe%2520self-correction%2520process.%2520Overlooking%2520this%2520factor%2520may%2520cause%2520the%2520models%2520to%250Aover-criticize%2520themselves%252C%2520resulting%2520in%2520unreliable%2520conclusions%2520regarding%2520the%250Aefficacy%2520of%2520self-correction.%2520We%2520have%2520experimentally%2520observed%2520that%2520LLMs%2520possess%250Athe%2520capability%2520to%2520understand%2520the%2520%2522confidence%2522%2520in%2520their%2520own%2520responses.%2520It%250Amotivates%2520us%2520to%2520develop%2520an%2520%2522If-or-Else%2522%2520%2528IoE%2529%2520prompting%2520framework%252C%2520designed%2520to%250Aguide%2520LLMs%2520in%2520assessing%2520their%2520own%2520%2522confidence%2522%252C%2520facilitating%2520intrinsic%250Aself-corrections.%2520We%2520conduct%2520extensive%2520experiments%2520and%2520demonstrate%2520that%2520our%250AIoE-based%2520Prompt%2520can%2520achieve%2520a%2520consistent%2520improvement%2520regarding%2520the%2520accuracy%2520of%250Aself-corrected%2520responses%2520over%2520the%2520initial%2520answers.%2520Our%2520study%2520not%2520only%2520sheds%250Alight%2520on%2520the%2520underlying%2520factors%2520affecting%2520self-correction%2520in%2520LLMs%252C%2520but%2520also%250Aintroduces%2520a%2520practical%2520framework%2520that%2520utilizes%2520the%2520IoE%2520prompting%2520principle%2520to%250Aefficiently%2520improve%2520self-correction%2520capabilities%2520with%2520%2522confidence%2522.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/MBZUAI-CLeaR/IoE-Prompting.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12563v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Confidence%20Matters%3A%20Revisiting%20Intrinsic%20Self-Correction%20Capabilities%20of%0A%20%20Large%20Language%20Models&entry.906535625=Loka%20Li%20and%20Zhenhao%20Chen%20and%20Guangyi%20Chen%20and%20Yixuan%20Zhang%20and%20Yusheng%20Su%20and%20Eric%20Xing%20and%20Kun%20Zhang&entry.1292438233=%20%20The%20recent%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20catalyzed%20an%0Aincreasing%20interest%20in%20their%20self-correction%20capabilities.%20This%20paper%20presents%0Aa%20comprehensive%20investigation%20into%20the%20intrinsic%20self-correction%20of%20LLMs%2C%0Aattempting%20to%20address%20the%20ongoing%20debate%20about%20its%20feasibility.%20Our%20research%0Ahas%20identified%20an%20important%20latent%20factor%20-%20the%20%22confidence%22%20of%20LLMs%20-%20during%0Athe%20self-correction%20process.%20Overlooking%20this%20factor%20may%20cause%20the%20models%20to%0Aover-criticize%20themselves%2C%20resulting%20in%20unreliable%20conclusions%20regarding%20the%0Aefficacy%20of%20self-correction.%20We%20have%20experimentally%20observed%20that%20LLMs%20possess%0Athe%20capability%20to%20understand%20the%20%22confidence%22%20in%20their%20own%20responses.%20It%0Amotivates%20us%20to%20develop%20an%20%22If-or-Else%22%20%28IoE%29%20prompting%20framework%2C%20designed%20to%0Aguide%20LLMs%20in%20assessing%20their%20own%20%22confidence%22%2C%20facilitating%20intrinsic%0Aself-corrections.%20We%20conduct%20extensive%20experiments%20and%20demonstrate%20that%20our%0AIoE-based%20Prompt%20can%20achieve%20a%20consistent%20improvement%20regarding%20the%20accuracy%20of%0Aself-corrected%20responses%20over%20the%20initial%20answers.%20Our%20study%20not%20only%20sheds%0Alight%20on%20the%20underlying%20factors%20affecting%20self-correction%20in%20LLMs%2C%20but%20also%0Aintroduces%20a%20practical%20framework%20that%20utilizes%20the%20IoE%20prompting%20principle%20to%0Aefficiently%20improve%20self-correction%20capabilities%20with%20%22confidence%22.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/MBZUAI-CLeaR/IoE-Prompting.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12563v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


