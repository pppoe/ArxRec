<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241110.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "PEP-GS: Perceptually-Enhanced Precise Structured 3D Gaussians for\n  View-Adaptive Rendering", "author": "Junxi Jin and Xiulai Li and Haiping Huang and Lianjun Liu and Yujie Sun", "abstract": "  Recent advances in structured 3D Gaussians for view-adaptive rendering,\nparticularly through methods like Scaffold-GS, have demonstrated promising\nresults in neural scene representation. However, existing approaches still face\nchallenges in perceptual consistency and precise view-dependent effects. We\npresent PEP-GS, a novel framework that enhances structured 3D Gaussians through\nthree key innovations: (1) a Local-Enhanced Multi-head Self-Attention (LEMSA)\nmechanism that replaces spherical harmonics for more accurate view-dependent\ncolor decoding, and (2) Kolmogorov-Arnold Networks (KAN) that optimize Gaussian\nopacity and covariance functions for enhanced interpretability and splatting\nprecision. (3) a Neural Laplacian Pyramid Decomposition (NLPD) that improves\nperceptual similarity across views. Our comprehensive evaluation across\nmultiple datasets indicates that, compared to the current state-of-the-art\nmethods, these improvements are particularly evident in challenging scenarios\nsuch as view-dependent effects, specular reflections, fine-scale details and\nfalse geometry generation.\n", "link": "http://arxiv.org/abs/2411.05731v1", "date": "2024-11-08", "relevancy": 3.2714, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6629}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6574}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PEP-GS%3A%20Perceptually-Enhanced%20Precise%20Structured%203D%20Gaussians%20for%0A%20%20View-Adaptive%20Rendering&body=Title%3A%20PEP-GS%3A%20Perceptually-Enhanced%20Precise%20Structured%203D%20Gaussians%20for%0A%20%20View-Adaptive%20Rendering%0AAuthor%3A%20Junxi%20Jin%20and%20Xiulai%20Li%20and%20Haiping%20Huang%20and%20Lianjun%20Liu%20and%20Yujie%20Sun%0AAbstract%3A%20%20%20Recent%20advances%20in%20structured%203D%20Gaussians%20for%20view-adaptive%20rendering%2C%0Aparticularly%20through%20methods%20like%20Scaffold-GS%2C%20have%20demonstrated%20promising%0Aresults%20in%20neural%20scene%20representation.%20However%2C%20existing%20approaches%20still%20face%0Achallenges%20in%20perceptual%20consistency%20and%20precise%20view-dependent%20effects.%20We%0Apresent%20PEP-GS%2C%20a%20novel%20framework%20that%20enhances%20structured%203D%20Gaussians%20through%0Athree%20key%20innovations%3A%20%281%29%20a%20Local-Enhanced%20Multi-head%20Self-Attention%20%28LEMSA%29%0Amechanism%20that%20replaces%20spherical%20harmonics%20for%20more%20accurate%20view-dependent%0Acolor%20decoding%2C%20and%20%282%29%20Kolmogorov-Arnold%20Networks%20%28KAN%29%20that%20optimize%20Gaussian%0Aopacity%20and%20covariance%20functions%20for%20enhanced%20interpretability%20and%20splatting%0Aprecision.%20%283%29%20a%20Neural%20Laplacian%20Pyramid%20Decomposition%20%28NLPD%29%20that%20improves%0Aperceptual%20similarity%20across%20views.%20Our%20comprehensive%20evaluation%20across%0Amultiple%20datasets%20indicates%20that%2C%20compared%20to%20the%20current%20state-of-the-art%0Amethods%2C%20these%20improvements%20are%20particularly%20evident%20in%20challenging%20scenarios%0Asuch%20as%20view-dependent%20effects%2C%20specular%20reflections%2C%20fine-scale%20details%20and%0Afalse%20geometry%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05731v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPEP-GS%253A%2520Perceptually-Enhanced%2520Precise%2520Structured%25203D%2520Gaussians%2520for%250A%2520%2520View-Adaptive%2520Rendering%26entry.906535625%3DJunxi%2520Jin%2520and%2520Xiulai%2520Li%2520and%2520Haiping%2520Huang%2520and%2520Lianjun%2520Liu%2520and%2520Yujie%2520Sun%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520structured%25203D%2520Gaussians%2520for%2520view-adaptive%2520rendering%252C%250Aparticularly%2520through%2520methods%2520like%2520Scaffold-GS%252C%2520have%2520demonstrated%2520promising%250Aresults%2520in%2520neural%2520scene%2520representation.%2520However%252C%2520existing%2520approaches%2520still%2520face%250Achallenges%2520in%2520perceptual%2520consistency%2520and%2520precise%2520view-dependent%2520effects.%2520We%250Apresent%2520PEP-GS%252C%2520a%2520novel%2520framework%2520that%2520enhances%2520structured%25203D%2520Gaussians%2520through%250Athree%2520key%2520innovations%253A%2520%25281%2529%2520a%2520Local-Enhanced%2520Multi-head%2520Self-Attention%2520%2528LEMSA%2529%250Amechanism%2520that%2520replaces%2520spherical%2520harmonics%2520for%2520more%2520accurate%2520view-dependent%250Acolor%2520decoding%252C%2520and%2520%25282%2529%2520Kolmogorov-Arnold%2520Networks%2520%2528KAN%2529%2520that%2520optimize%2520Gaussian%250Aopacity%2520and%2520covariance%2520functions%2520for%2520enhanced%2520interpretability%2520and%2520splatting%250Aprecision.%2520%25283%2529%2520a%2520Neural%2520Laplacian%2520Pyramid%2520Decomposition%2520%2528NLPD%2529%2520that%2520improves%250Aperceptual%2520similarity%2520across%2520views.%2520Our%2520comprehensive%2520evaluation%2520across%250Amultiple%2520datasets%2520indicates%2520that%252C%2520compared%2520to%2520the%2520current%2520state-of-the-art%250Amethods%252C%2520these%2520improvements%2520are%2520particularly%2520evident%2520in%2520challenging%2520scenarios%250Asuch%2520as%2520view-dependent%2520effects%252C%2520specular%2520reflections%252C%2520fine-scale%2520details%2520and%250Afalse%2520geometry%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05731v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PEP-GS%3A%20Perceptually-Enhanced%20Precise%20Structured%203D%20Gaussians%20for%0A%20%20View-Adaptive%20Rendering&entry.906535625=Junxi%20Jin%20and%20Xiulai%20Li%20and%20Haiping%20Huang%20and%20Lianjun%20Liu%20and%20Yujie%20Sun&entry.1292438233=%20%20Recent%20advances%20in%20structured%203D%20Gaussians%20for%20view-adaptive%20rendering%2C%0Aparticularly%20through%20methods%20like%20Scaffold-GS%2C%20have%20demonstrated%20promising%0Aresults%20in%20neural%20scene%20representation.%20However%2C%20existing%20approaches%20still%20face%0Achallenges%20in%20perceptual%20consistency%20and%20precise%20view-dependent%20effects.%20We%0Apresent%20PEP-GS%2C%20a%20novel%20framework%20that%20enhances%20structured%203D%20Gaussians%20through%0Athree%20key%20innovations%3A%20%281%29%20a%20Local-Enhanced%20Multi-head%20Self-Attention%20%28LEMSA%29%0Amechanism%20that%20replaces%20spherical%20harmonics%20for%20more%20accurate%20view-dependent%0Acolor%20decoding%2C%20and%20%282%29%20Kolmogorov-Arnold%20Networks%20%28KAN%29%20that%20optimize%20Gaussian%0Aopacity%20and%20covariance%20functions%20for%20enhanced%20interpretability%20and%20splatting%0Aprecision.%20%283%29%20a%20Neural%20Laplacian%20Pyramid%20Decomposition%20%28NLPD%29%20that%20improves%0Aperceptual%20similarity%20across%20views.%20Our%20comprehensive%20evaluation%20across%0Amultiple%20datasets%20indicates%20that%2C%20compared%20to%20the%20current%20state-of-the-art%0Amethods%2C%20these%20improvements%20are%20particularly%20evident%20in%20challenging%20scenarios%0Asuch%20as%20view-dependent%20effects%2C%20specular%20reflections%2C%20fine-scale%20details%20and%0Afalse%20geometry%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05731v1&entry.124074799=Read"},
{"title": "StdGEN: Semantic-Decomposed 3D Character Generation from Single Images", "author": "Yuze He and Yanning Zhou and Wang Zhao and Zhongkai Wu and Kaiwen Xiao and Wei Yang and Yong-Jin Liu and Xiao Han", "abstract": "  We present StdGEN, an innovative pipeline for generating semantically\ndecomposed high-quality 3D characters from single images, enabling broad\napplications in virtual reality, gaming, and filmmaking, etc. Unlike previous\nmethods which struggle with limited decomposability, unsatisfactory quality,\nand long optimization times, StdGEN features decomposability, effectiveness and\nefficiency; i.e., it generates intricately detailed 3D characters with\nseparated semantic components such as the body, clothes, and hair, in three\nminutes. At the core of StdGEN is our proposed Semantic-aware Large\nReconstruction Model (S-LRM), a transformer-based generalizable model that\njointly reconstructs geometry, color and semantics from multi-view images in a\nfeed-forward manner. A differentiable multi-layer semantic surface extraction\nscheme is introduced to acquire meshes from hybrid implicit fields\nreconstructed by our S-LRM. Additionally, a specialized efficient multi-view\ndiffusion model and an iterative multi-layer surface refinement module are\nintegrated into the pipeline to facilitate high-quality, decomposable 3D\ncharacter generation. Extensive experiments demonstrate our state-of-the-art\nperformance in 3D anime character generation, surpassing existing baselines by\na significant margin in geometry, texture and decomposability. StdGEN offers\nready-to-use semantic-decomposed 3D characters and enables flexible\ncustomization for a wide range of applications. Project page:\nhttps://stdgen.github.io\n", "link": "http://arxiv.org/abs/2411.05738v1", "date": "2024-11-08", "relevancy": 3.0686, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6255}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6223}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StdGEN%3A%20Semantic-Decomposed%203D%20Character%20Generation%20from%20Single%20Images&body=Title%3A%20StdGEN%3A%20Semantic-Decomposed%203D%20Character%20Generation%20from%20Single%20Images%0AAuthor%3A%20Yuze%20He%20and%20Yanning%20Zhou%20and%20Wang%20Zhao%20and%20Zhongkai%20Wu%20and%20Kaiwen%20Xiao%20and%20Wei%20Yang%20and%20Yong-Jin%20Liu%20and%20Xiao%20Han%0AAbstract%3A%20%20%20We%20present%20StdGEN%2C%20an%20innovative%20pipeline%20for%20generating%20semantically%0Adecomposed%20high-quality%203D%20characters%20from%20single%20images%2C%20enabling%20broad%0Aapplications%20in%20virtual%20reality%2C%20gaming%2C%20and%20filmmaking%2C%20etc.%20Unlike%20previous%0Amethods%20which%20struggle%20with%20limited%20decomposability%2C%20unsatisfactory%20quality%2C%0Aand%20long%20optimization%20times%2C%20StdGEN%20features%20decomposability%2C%20effectiveness%20and%0Aefficiency%3B%20i.e.%2C%20it%20generates%20intricately%20detailed%203D%20characters%20with%0Aseparated%20semantic%20components%20such%20as%20the%20body%2C%20clothes%2C%20and%20hair%2C%20in%20three%0Aminutes.%20At%20the%20core%20of%20StdGEN%20is%20our%20proposed%20Semantic-aware%20Large%0AReconstruction%20Model%20%28S-LRM%29%2C%20a%20transformer-based%20generalizable%20model%20that%0Ajointly%20reconstructs%20geometry%2C%20color%20and%20semantics%20from%20multi-view%20images%20in%20a%0Afeed-forward%20manner.%20A%20differentiable%20multi-layer%20semantic%20surface%20extraction%0Ascheme%20is%20introduced%20to%20acquire%20meshes%20from%20hybrid%20implicit%20fields%0Areconstructed%20by%20our%20S-LRM.%20Additionally%2C%20a%20specialized%20efficient%20multi-view%0Adiffusion%20model%20and%20an%20iterative%20multi-layer%20surface%20refinement%20module%20are%0Aintegrated%20into%20the%20pipeline%20to%20facilitate%20high-quality%2C%20decomposable%203D%0Acharacter%20generation.%20Extensive%20experiments%20demonstrate%20our%20state-of-the-art%0Aperformance%20in%203D%20anime%20character%20generation%2C%20surpassing%20existing%20baselines%20by%0Aa%20significant%20margin%20in%20geometry%2C%20texture%20and%20decomposability.%20StdGEN%20offers%0Aready-to-use%20semantic-decomposed%203D%20characters%20and%20enables%20flexible%0Acustomization%20for%20a%20wide%20range%20of%20applications.%20Project%20page%3A%0Ahttps%3A//stdgen.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStdGEN%253A%2520Semantic-Decomposed%25203D%2520Character%2520Generation%2520from%2520Single%2520Images%26entry.906535625%3DYuze%2520He%2520and%2520Yanning%2520Zhou%2520and%2520Wang%2520Zhao%2520and%2520Zhongkai%2520Wu%2520and%2520Kaiwen%2520Xiao%2520and%2520Wei%2520Yang%2520and%2520Yong-Jin%2520Liu%2520and%2520Xiao%2520Han%26entry.1292438233%3D%2520%2520We%2520present%2520StdGEN%252C%2520an%2520innovative%2520pipeline%2520for%2520generating%2520semantically%250Adecomposed%2520high-quality%25203D%2520characters%2520from%2520single%2520images%252C%2520enabling%2520broad%250Aapplications%2520in%2520virtual%2520reality%252C%2520gaming%252C%2520and%2520filmmaking%252C%2520etc.%2520Unlike%2520previous%250Amethods%2520which%2520struggle%2520with%2520limited%2520decomposability%252C%2520unsatisfactory%2520quality%252C%250Aand%2520long%2520optimization%2520times%252C%2520StdGEN%2520features%2520decomposability%252C%2520effectiveness%2520and%250Aefficiency%253B%2520i.e.%252C%2520it%2520generates%2520intricately%2520detailed%25203D%2520characters%2520with%250Aseparated%2520semantic%2520components%2520such%2520as%2520the%2520body%252C%2520clothes%252C%2520and%2520hair%252C%2520in%2520three%250Aminutes.%2520At%2520the%2520core%2520of%2520StdGEN%2520is%2520our%2520proposed%2520Semantic-aware%2520Large%250AReconstruction%2520Model%2520%2528S-LRM%2529%252C%2520a%2520transformer-based%2520generalizable%2520model%2520that%250Ajointly%2520reconstructs%2520geometry%252C%2520color%2520and%2520semantics%2520from%2520multi-view%2520images%2520in%2520a%250Afeed-forward%2520manner.%2520A%2520differentiable%2520multi-layer%2520semantic%2520surface%2520extraction%250Ascheme%2520is%2520introduced%2520to%2520acquire%2520meshes%2520from%2520hybrid%2520implicit%2520fields%250Areconstructed%2520by%2520our%2520S-LRM.%2520Additionally%252C%2520a%2520specialized%2520efficient%2520multi-view%250Adiffusion%2520model%2520and%2520an%2520iterative%2520multi-layer%2520surface%2520refinement%2520module%2520are%250Aintegrated%2520into%2520the%2520pipeline%2520to%2520facilitate%2520high-quality%252C%2520decomposable%25203D%250Acharacter%2520generation.%2520Extensive%2520experiments%2520demonstrate%2520our%2520state-of-the-art%250Aperformance%2520in%25203D%2520anime%2520character%2520generation%252C%2520surpassing%2520existing%2520baselines%2520by%250Aa%2520significant%2520margin%2520in%2520geometry%252C%2520texture%2520and%2520decomposability.%2520StdGEN%2520offers%250Aready-to-use%2520semantic-decomposed%25203D%2520characters%2520and%2520enables%2520flexible%250Acustomization%2520for%2520a%2520wide%2520range%2520of%2520applications.%2520Project%2520page%253A%250Ahttps%253A//stdgen.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StdGEN%3A%20Semantic-Decomposed%203D%20Character%20Generation%20from%20Single%20Images&entry.906535625=Yuze%20He%20and%20Yanning%20Zhou%20and%20Wang%20Zhao%20and%20Zhongkai%20Wu%20and%20Kaiwen%20Xiao%20and%20Wei%20Yang%20and%20Yong-Jin%20Liu%20and%20Xiao%20Han&entry.1292438233=%20%20We%20present%20StdGEN%2C%20an%20innovative%20pipeline%20for%20generating%20semantically%0Adecomposed%20high-quality%203D%20characters%20from%20single%20images%2C%20enabling%20broad%0Aapplications%20in%20virtual%20reality%2C%20gaming%2C%20and%20filmmaking%2C%20etc.%20Unlike%20previous%0Amethods%20which%20struggle%20with%20limited%20decomposability%2C%20unsatisfactory%20quality%2C%0Aand%20long%20optimization%20times%2C%20StdGEN%20features%20decomposability%2C%20effectiveness%20and%0Aefficiency%3B%20i.e.%2C%20it%20generates%20intricately%20detailed%203D%20characters%20with%0Aseparated%20semantic%20components%20such%20as%20the%20body%2C%20clothes%2C%20and%20hair%2C%20in%20three%0Aminutes.%20At%20the%20core%20of%20StdGEN%20is%20our%20proposed%20Semantic-aware%20Large%0AReconstruction%20Model%20%28S-LRM%29%2C%20a%20transformer-based%20generalizable%20model%20that%0Ajointly%20reconstructs%20geometry%2C%20color%20and%20semantics%20from%20multi-view%20images%20in%20a%0Afeed-forward%20manner.%20A%20differentiable%20multi-layer%20semantic%20surface%20extraction%0Ascheme%20is%20introduced%20to%20acquire%20meshes%20from%20hybrid%20implicit%20fields%0Areconstructed%20by%20our%20S-LRM.%20Additionally%2C%20a%20specialized%20efficient%20multi-view%0Adiffusion%20model%20and%20an%20iterative%20multi-layer%20surface%20refinement%20module%20are%0Aintegrated%20into%20the%20pipeline%20to%20facilitate%20high-quality%2C%20decomposable%203D%0Acharacter%20generation.%20Extensive%20experiments%20demonstrate%20our%20state-of-the-art%0Aperformance%20in%203D%20anime%20character%20generation%2C%20surpassing%20existing%20baselines%20by%0Aa%20significant%20margin%20in%20geometry%2C%20texture%20and%20decomposability.%20StdGEN%20offers%0Aready-to-use%20semantic-decomposed%203D%20characters%20and%20enables%20flexible%0Acustomization%20for%20a%20wide%20range%20of%20applications.%20Project%20page%3A%0Ahttps%3A//stdgen.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05738v1&entry.124074799=Read"},
{"title": "Tightly-Coupled, Speed-aided Monocular Visual-Inertial Localization in\n  Topological Map", "author": "Chanuk Yang and Hayeon O and Kunsoo Huh", "abstract": "  This paper proposes a novel algorithm for vehicle speed-aided monocular\nvisual-inertial localization using a topological map. The proposed system aims\nto address the limitations of existing methods that rely heavily on expensive\nsensors like GPS and LiDAR by leveraging relatively inexpensive camera-based\npose estimation. The topological map is generated offline from LiDAR point\nclouds and includes depth images, intensity images, and corresponding camera\nposes. This map is then used for real-time localization through correspondence\nmatching between current camera images and the stored topological images. The\nsystem employs an Iterated Error State Kalman Filter (IESKF) for optimized pose\nestimation, incorporating correspondence among images and vehicle speed\nmeasurements to enhance accuracy. Experimental results using both open dataset\nand our collected data in challenging scenario, such as tunnel, demonstrate the\nproposed algorithm's superior performance in topological map generation and\nlocalization tasks.\n", "link": "http://arxiv.org/abs/2411.05497v1", "date": "2024-11-08", "relevancy": 2.9239, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6316}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5818}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tightly-Coupled%2C%20Speed-aided%20Monocular%20Visual-Inertial%20Localization%20in%0A%20%20Topological%20Map&body=Title%3A%20Tightly-Coupled%2C%20Speed-aided%20Monocular%20Visual-Inertial%20Localization%20in%0A%20%20Topological%20Map%0AAuthor%3A%20Chanuk%20Yang%20and%20Hayeon%20O%20and%20Kunsoo%20Huh%0AAbstract%3A%20%20%20This%20paper%20proposes%20a%20novel%20algorithm%20for%20vehicle%20speed-aided%20monocular%0Avisual-inertial%20localization%20using%20a%20topological%20map.%20The%20proposed%20system%20aims%0Ato%20address%20the%20limitations%20of%20existing%20methods%20that%20rely%20heavily%20on%20expensive%0Asensors%20like%20GPS%20and%20LiDAR%20by%20leveraging%20relatively%20inexpensive%20camera-based%0Apose%20estimation.%20The%20topological%20map%20is%20generated%20offline%20from%20LiDAR%20point%0Aclouds%20and%20includes%20depth%20images%2C%20intensity%20images%2C%20and%20corresponding%20camera%0Aposes.%20This%20map%20is%20then%20used%20for%20real-time%20localization%20through%20correspondence%0Amatching%20between%20current%20camera%20images%20and%20the%20stored%20topological%20images.%20The%0Asystem%20employs%20an%20Iterated%20Error%20State%20Kalman%20Filter%20%28IESKF%29%20for%20optimized%20pose%0Aestimation%2C%20incorporating%20correspondence%20among%20images%20and%20vehicle%20speed%0Ameasurements%20to%20enhance%20accuracy.%20Experimental%20results%20using%20both%20open%20dataset%0Aand%20our%20collected%20data%20in%20challenging%20scenario%2C%20such%20as%20tunnel%2C%20demonstrate%20the%0Aproposed%20algorithm%27s%20superior%20performance%20in%20topological%20map%20generation%20and%0Alocalization%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTightly-Coupled%252C%2520Speed-aided%2520Monocular%2520Visual-Inertial%2520Localization%2520in%250A%2520%2520Topological%2520Map%26entry.906535625%3DChanuk%2520Yang%2520and%2520Hayeon%2520O%2520and%2520Kunsoo%2520Huh%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520a%2520novel%2520algorithm%2520for%2520vehicle%2520speed-aided%2520monocular%250Avisual-inertial%2520localization%2520using%2520a%2520topological%2520map.%2520The%2520proposed%2520system%2520aims%250Ato%2520address%2520the%2520limitations%2520of%2520existing%2520methods%2520that%2520rely%2520heavily%2520on%2520expensive%250Asensors%2520like%2520GPS%2520and%2520LiDAR%2520by%2520leveraging%2520relatively%2520inexpensive%2520camera-based%250Apose%2520estimation.%2520The%2520topological%2520map%2520is%2520generated%2520offline%2520from%2520LiDAR%2520point%250Aclouds%2520and%2520includes%2520depth%2520images%252C%2520intensity%2520images%252C%2520and%2520corresponding%2520camera%250Aposes.%2520This%2520map%2520is%2520then%2520used%2520for%2520real-time%2520localization%2520through%2520correspondence%250Amatching%2520between%2520current%2520camera%2520images%2520and%2520the%2520stored%2520topological%2520images.%2520The%250Asystem%2520employs%2520an%2520Iterated%2520Error%2520State%2520Kalman%2520Filter%2520%2528IESKF%2529%2520for%2520optimized%2520pose%250Aestimation%252C%2520incorporating%2520correspondence%2520among%2520images%2520and%2520vehicle%2520speed%250Ameasurements%2520to%2520enhance%2520accuracy.%2520Experimental%2520results%2520using%2520both%2520open%2520dataset%250Aand%2520our%2520collected%2520data%2520in%2520challenging%2520scenario%252C%2520such%2520as%2520tunnel%252C%2520demonstrate%2520the%250Aproposed%2520algorithm%2527s%2520superior%2520performance%2520in%2520topological%2520map%2520generation%2520and%250Alocalization%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tightly-Coupled%2C%20Speed-aided%20Monocular%20Visual-Inertial%20Localization%20in%0A%20%20Topological%20Map&entry.906535625=Chanuk%20Yang%20and%20Hayeon%20O%20and%20Kunsoo%20Huh&entry.1292438233=%20%20This%20paper%20proposes%20a%20novel%20algorithm%20for%20vehicle%20speed-aided%20monocular%0Avisual-inertial%20localization%20using%20a%20topological%20map.%20The%20proposed%20system%20aims%0Ato%20address%20the%20limitations%20of%20existing%20methods%20that%20rely%20heavily%20on%20expensive%0Asensors%20like%20GPS%20and%20LiDAR%20by%20leveraging%20relatively%20inexpensive%20camera-based%0Apose%20estimation.%20The%20topological%20map%20is%20generated%20offline%20from%20LiDAR%20point%0Aclouds%20and%20includes%20depth%20images%2C%20intensity%20images%2C%20and%20corresponding%20camera%0Aposes.%20This%20map%20is%20then%20used%20for%20real-time%20localization%20through%20correspondence%0Amatching%20between%20current%20camera%20images%20and%20the%20stored%20topological%20images.%20The%0Asystem%20employs%20an%20Iterated%20Error%20State%20Kalman%20Filter%20%28IESKF%29%20for%20optimized%20pose%0Aestimation%2C%20incorporating%20correspondence%20among%20images%20and%20vehicle%20speed%0Ameasurements%20to%20enhance%20accuracy.%20Experimental%20results%20using%20both%20open%20dataset%0Aand%20our%20collected%20data%20in%20challenging%20scenario%2C%20such%20as%20tunnel%2C%20demonstrate%20the%0Aproposed%20algorithm%27s%20superior%20performance%20in%20topological%20map%20generation%20and%0Alocalization%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05497v1&entry.124074799=Read"},
{"title": "Multimodal Structure-Aware Quantum Data Processing", "author": "Hala Hawashin and Mehrnoosh Sadrzadeh", "abstract": "  While large language models (LLMs) have advanced the field of natural\nlanguage processing (NLP), their \"black box\" nature obscures their\ndecision-making processes. To address this, researchers developed structured\napproaches using higher order tensors. These are able to model linguistic\nrelations, but stall when training on classical computers due to their\nexcessive size. Tensors are natural inhabitants of quantum systems and training\non quantum computers provides a solution by translating text to variational\nquantum circuits. In this paper, we develop MultiQ-NLP: a framework for\nstructure-aware data processing with multimodal text+image data. Here,\n\"structure\" refers to syntactic and grammatical relationships in language, as\nwell as the hierarchical organization of visual elements in images. We enrich\nthe translation with new types and type homomorphisms and develop novel\narchitectures to represent structure. When tested on a main stream image\nclassification task (SVO Probes), our best model showed a par performance with\nthe state of the art classical models; moreover the best model was fully\nstructured.\n", "link": "http://arxiv.org/abs/2411.04242v2", "date": "2024-11-08", "relevancy": 2.8867, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5863}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5729}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Structure-Aware%20Quantum%20Data%20Processing&body=Title%3A%20Multimodal%20Structure-Aware%20Quantum%20Data%20Processing%0AAuthor%3A%20Hala%20Hawashin%20and%20Mehrnoosh%20Sadrzadeh%0AAbstract%3A%20%20%20While%20large%20language%20models%20%28LLMs%29%20have%20advanced%20the%20field%20of%20natural%0Alanguage%20processing%20%28NLP%29%2C%20their%20%22black%20box%22%20nature%20obscures%20their%0Adecision-making%20processes.%20To%20address%20this%2C%20researchers%20developed%20structured%0Aapproaches%20using%20higher%20order%20tensors.%20These%20are%20able%20to%20model%20linguistic%0Arelations%2C%20but%20stall%20when%20training%20on%20classical%20computers%20due%20to%20their%0Aexcessive%20size.%20Tensors%20are%20natural%20inhabitants%20of%20quantum%20systems%20and%20training%0Aon%20quantum%20computers%20provides%20a%20solution%20by%20translating%20text%20to%20variational%0Aquantum%20circuits.%20In%20this%20paper%2C%20we%20develop%20MultiQ-NLP%3A%20a%20framework%20for%0Astructure-aware%20data%20processing%20with%20multimodal%20text%2Bimage%20data.%20Here%2C%0A%22structure%22%20refers%20to%20syntactic%20and%20grammatical%20relationships%20in%20language%2C%20as%0Awell%20as%20the%20hierarchical%20organization%20of%20visual%20elements%20in%20images.%20We%20enrich%0Athe%20translation%20with%20new%20types%20and%20type%20homomorphisms%20and%20develop%20novel%0Aarchitectures%20to%20represent%20structure.%20When%20tested%20on%20a%20main%20stream%20image%0Aclassification%20task%20%28SVO%20Probes%29%2C%20our%20best%20model%20showed%20a%20par%20performance%20with%0Athe%20state%20of%20the%20art%20classical%20models%3B%20moreover%20the%20best%20model%20was%20fully%0Astructured.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Structure-Aware%2520Quantum%2520Data%2520Processing%26entry.906535625%3DHala%2520Hawashin%2520and%2520Mehrnoosh%2520Sadrzadeh%26entry.1292438233%3D%2520%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520advanced%2520the%2520field%2520of%2520natural%250Alanguage%2520processing%2520%2528NLP%2529%252C%2520their%2520%2522black%2520box%2522%2520nature%2520obscures%2520their%250Adecision-making%2520processes.%2520To%2520address%2520this%252C%2520researchers%2520developed%2520structured%250Aapproaches%2520using%2520higher%2520order%2520tensors.%2520These%2520are%2520able%2520to%2520model%2520linguistic%250Arelations%252C%2520but%2520stall%2520when%2520training%2520on%2520classical%2520computers%2520due%2520to%2520their%250Aexcessive%2520size.%2520Tensors%2520are%2520natural%2520inhabitants%2520of%2520quantum%2520systems%2520and%2520training%250Aon%2520quantum%2520computers%2520provides%2520a%2520solution%2520by%2520translating%2520text%2520to%2520variational%250Aquantum%2520circuits.%2520In%2520this%2520paper%252C%2520we%2520develop%2520MultiQ-NLP%253A%2520a%2520framework%2520for%250Astructure-aware%2520data%2520processing%2520with%2520multimodal%2520text%252Bimage%2520data.%2520Here%252C%250A%2522structure%2522%2520refers%2520to%2520syntactic%2520and%2520grammatical%2520relationships%2520in%2520language%252C%2520as%250Awell%2520as%2520the%2520hierarchical%2520organization%2520of%2520visual%2520elements%2520in%2520images.%2520We%2520enrich%250Athe%2520translation%2520with%2520new%2520types%2520and%2520type%2520homomorphisms%2520and%2520develop%2520novel%250Aarchitectures%2520to%2520represent%2520structure.%2520When%2520tested%2520on%2520a%2520main%2520stream%2520image%250Aclassification%2520task%2520%2528SVO%2520Probes%2529%252C%2520our%2520best%2520model%2520showed%2520a%2520par%2520performance%2520with%250Athe%2520state%2520of%2520the%2520art%2520classical%2520models%253B%2520moreover%2520the%2520best%2520model%2520was%2520fully%250Astructured.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Structure-Aware%20Quantum%20Data%20Processing&entry.906535625=Hala%20Hawashin%20and%20Mehrnoosh%20Sadrzadeh&entry.1292438233=%20%20While%20large%20language%20models%20%28LLMs%29%20have%20advanced%20the%20field%20of%20natural%0Alanguage%20processing%20%28NLP%29%2C%20their%20%22black%20box%22%20nature%20obscures%20their%0Adecision-making%20processes.%20To%20address%20this%2C%20researchers%20developed%20structured%0Aapproaches%20using%20higher%20order%20tensors.%20These%20are%20able%20to%20model%20linguistic%0Arelations%2C%20but%20stall%20when%20training%20on%20classical%20computers%20due%20to%20their%0Aexcessive%20size.%20Tensors%20are%20natural%20inhabitants%20of%20quantum%20systems%20and%20training%0Aon%20quantum%20computers%20provides%20a%20solution%20by%20translating%20text%20to%20variational%0Aquantum%20circuits.%20In%20this%20paper%2C%20we%20develop%20MultiQ-NLP%3A%20a%20framework%20for%0Astructure-aware%20data%20processing%20with%20multimodal%20text%2Bimage%20data.%20Here%2C%0A%22structure%22%20refers%20to%20syntactic%20and%20grammatical%20relationships%20in%20language%2C%20as%0Awell%20as%20the%20hierarchical%20organization%20of%20visual%20elements%20in%20images.%20We%20enrich%0Athe%20translation%20with%20new%20types%20and%20type%20homomorphisms%20and%20develop%20novel%0Aarchitectures%20to%20represent%20structure.%20When%20tested%20on%20a%20main%20stream%20image%0Aclassification%20task%20%28SVO%20Probes%29%2C%20our%20best%20model%20showed%20a%20par%20performance%20with%0Athe%20state%20of%20the%20art%20classical%20models%3B%20moreover%20the%20best%20model%20was%20fully%0Astructured.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04242v2&entry.124074799=Read"},
{"title": "End-to-End Navigation with Vision Language Models: Transforming Spatial\n  Reasoning into Question-Answering", "author": "Dylan Goetting and Himanshu Gaurav Singh and Antonio Loquercio", "abstract": "  We present VLMnav, an embodied framework to transform a Vision-Language Model\n(VLM) into an end-to-end navigation policy. In contrast to prior work, we do\nnot rely on a separation between perception, planning, and control; instead, we\nuse a VLM to directly select actions in one step. Surprisingly, we find that a\nVLM can be used as an end-to-end policy zero-shot, i.e., without any\nfine-tuning or exposure to navigation data. This makes our approach open-ended\nand generalizable to any downstream navigation task. We run an extensive study\nto evaluate the performance of our approach in comparison to baseline prompting\nmethods. In addition, we perform a design analysis to understand the most\nimpactful design decisions. Visual examples and code for our project can be\nfound at https://jirl-upenn.github.io/VLMnav/\n", "link": "http://arxiv.org/abs/2411.05755v1", "date": "2024-11-08", "relevancy": 2.8295, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5851}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5563}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-End%20Navigation%20with%20Vision%20Language%20Models%3A%20Transforming%20Spatial%0A%20%20Reasoning%20into%20Question-Answering&body=Title%3A%20End-to-End%20Navigation%20with%20Vision%20Language%20Models%3A%20Transforming%20Spatial%0A%20%20Reasoning%20into%20Question-Answering%0AAuthor%3A%20Dylan%20Goetting%20and%20Himanshu%20Gaurav%20Singh%20and%20Antonio%20Loquercio%0AAbstract%3A%20%20%20We%20present%20VLMnav%2C%20an%20embodied%20framework%20to%20transform%20a%20Vision-Language%20Model%0A%28VLM%29%20into%20an%20end-to-end%20navigation%20policy.%20In%20contrast%20to%20prior%20work%2C%20we%20do%0Anot%20rely%20on%20a%20separation%20between%20perception%2C%20planning%2C%20and%20control%3B%20instead%2C%20we%0Ause%20a%20VLM%20to%20directly%20select%20actions%20in%20one%20step.%20Surprisingly%2C%20we%20find%20that%20a%0AVLM%20can%20be%20used%20as%20an%20end-to-end%20policy%20zero-shot%2C%20i.e.%2C%20without%20any%0Afine-tuning%20or%20exposure%20to%20navigation%20data.%20This%20makes%20our%20approach%20open-ended%0Aand%20generalizable%20to%20any%20downstream%20navigation%20task.%20We%20run%20an%20extensive%20study%0Ato%20evaluate%20the%20performance%20of%20our%20approach%20in%20comparison%20to%20baseline%20prompting%0Amethods.%20In%20addition%2C%20we%20perform%20a%20design%20analysis%20to%20understand%20the%20most%0Aimpactful%20design%20decisions.%20Visual%20examples%20and%20code%20for%20our%20project%20can%20be%0Afound%20at%20https%3A//jirl-upenn.github.io/VLMnav/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-End%2520Navigation%2520with%2520Vision%2520Language%2520Models%253A%2520Transforming%2520Spatial%250A%2520%2520Reasoning%2520into%2520Question-Answering%26entry.906535625%3DDylan%2520Goetting%2520and%2520Himanshu%2520Gaurav%2520Singh%2520and%2520Antonio%2520Loquercio%26entry.1292438233%3D%2520%2520We%2520present%2520VLMnav%252C%2520an%2520embodied%2520framework%2520to%2520transform%2520a%2520Vision-Language%2520Model%250A%2528VLM%2529%2520into%2520an%2520end-to-end%2520navigation%2520policy.%2520In%2520contrast%2520to%2520prior%2520work%252C%2520we%2520do%250Anot%2520rely%2520on%2520a%2520separation%2520between%2520perception%252C%2520planning%252C%2520and%2520control%253B%2520instead%252C%2520we%250Ause%2520a%2520VLM%2520to%2520directly%2520select%2520actions%2520in%2520one%2520step.%2520Surprisingly%252C%2520we%2520find%2520that%2520a%250AVLM%2520can%2520be%2520used%2520as%2520an%2520end-to-end%2520policy%2520zero-shot%252C%2520i.e.%252C%2520without%2520any%250Afine-tuning%2520or%2520exposure%2520to%2520navigation%2520data.%2520This%2520makes%2520our%2520approach%2520open-ended%250Aand%2520generalizable%2520to%2520any%2520downstream%2520navigation%2520task.%2520We%2520run%2520an%2520extensive%2520study%250Ato%2520evaluate%2520the%2520performance%2520of%2520our%2520approach%2520in%2520comparison%2520to%2520baseline%2520prompting%250Amethods.%2520In%2520addition%252C%2520we%2520perform%2520a%2520design%2520analysis%2520to%2520understand%2520the%2520most%250Aimpactful%2520design%2520decisions.%2520Visual%2520examples%2520and%2520code%2520for%2520our%2520project%2520can%2520be%250Afound%2520at%2520https%253A//jirl-upenn.github.io/VLMnav/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-End%20Navigation%20with%20Vision%20Language%20Models%3A%20Transforming%20Spatial%0A%20%20Reasoning%20into%20Question-Answering&entry.906535625=Dylan%20Goetting%20and%20Himanshu%20Gaurav%20Singh%20and%20Antonio%20Loquercio&entry.1292438233=%20%20We%20present%20VLMnav%2C%20an%20embodied%20framework%20to%20transform%20a%20Vision-Language%20Model%0A%28VLM%29%20into%20an%20end-to-end%20navigation%20policy.%20In%20contrast%20to%20prior%20work%2C%20we%20do%0Anot%20rely%20on%20a%20separation%20between%20perception%2C%20planning%2C%20and%20control%3B%20instead%2C%20we%0Ause%20a%20VLM%20to%20directly%20select%20actions%20in%20one%20step.%20Surprisingly%2C%20we%20find%20that%20a%0AVLM%20can%20be%20used%20as%20an%20end-to-end%20policy%20zero-shot%2C%20i.e.%2C%20without%20any%0Afine-tuning%20or%20exposure%20to%20navigation%20data.%20This%20makes%20our%20approach%20open-ended%0Aand%20generalizable%20to%20any%20downstream%20navigation%20task.%20We%20run%20an%20extensive%20study%0Ato%20evaluate%20the%20performance%20of%20our%20approach%20in%20comparison%20to%20baseline%20prompting%0Amethods.%20In%20addition%2C%20we%20perform%20a%20design%20analysis%20to%20understand%20the%20most%0Aimpactful%20design%20decisions.%20Visual%20examples%20and%20code%20for%20our%20project%20can%20be%0Afound%20at%20https%3A//jirl-upenn.github.io/VLMnav/%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05755v1&entry.124074799=Read"},
{"title": "HPE-CogVLM: Advancing Vision Language Models with a Head Pose Grounding\n  Task", "author": "Yu Tian and Tianqi Shao and Tsukasa Demizu and Xuyang Wu and Hsin-Tai Wu", "abstract": "  Head pose estimation (HPE) requires a sophisticated understanding of 3D\nspatial relationships to generate precise yaw, pitch, and roll angles. Previous\nHPE models, primarily CNN-based, rely on cropped close-up human head images as\ninputs and often lack robustness in real-world scenario. Vision Language Models\n(VLMs) can analyze entire images while focusing on specific objects through\ntheir attention mechanisms. In this paper, we propose a novel framework to\nimprove the HPE accuracy by leveraging the object detection grounding\ncapability of a VLM, referred to as CogVLM. We empirically find that directly\nLoRA fine-tuning of this VLM for the HPE task fails to achieve desirable HPE\naccuracy, while some model merging methods can improve accuracy but frequently\nproduce blended invalid response formats, struggling to handle both object\ndetection and HPE tasks simultaneously. To integrate HPE capability into CogVLM\neffectively, we develop a novel LoRA layer-based model merging method. This\nmerging approach applies a high cosine similarity threshold and a\nwinner-takes-all layer selection strategy, aligning attention to the HPE task\nwhile preserving original object detection knowledge. It successfully resolves\nissues with blended invalid response formats and improves accuracy. Results\nshow that our HPE-CogVLM achieves a 31.5\\% reduction in Mean Absolute Error\nover the current state-of-the-art CNN model, 6DRepNet, in cross-dataset\nevaluation. Furthermore, HPE-CogVLM outperforms both directly LoRA fine-tuned\nand task arithmetic-based merged VLMs across all HPE metrics.\n", "link": "http://arxiv.org/abs/2406.01914v2", "date": "2024-11-08", "relevancy": 2.7516, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5526}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HPE-CogVLM%3A%20Advancing%20Vision%20Language%20Models%20with%20a%20Head%20Pose%20Grounding%0A%20%20Task&body=Title%3A%20HPE-CogVLM%3A%20Advancing%20Vision%20Language%20Models%20with%20a%20Head%20Pose%20Grounding%0A%20%20Task%0AAuthor%3A%20Yu%20Tian%20and%20Tianqi%20Shao%20and%20Tsukasa%20Demizu%20and%20Xuyang%20Wu%20and%20Hsin-Tai%20Wu%0AAbstract%3A%20%20%20Head%20pose%20estimation%20%28HPE%29%20requires%20a%20sophisticated%20understanding%20of%203D%0Aspatial%20relationships%20to%20generate%20precise%20yaw%2C%20pitch%2C%20and%20roll%20angles.%20Previous%0AHPE%20models%2C%20primarily%20CNN-based%2C%20rely%20on%20cropped%20close-up%20human%20head%20images%20as%0Ainputs%20and%20often%20lack%20robustness%20in%20real-world%20scenario.%20Vision%20Language%20Models%0A%28VLMs%29%20can%20analyze%20entire%20images%20while%20focusing%20on%20specific%20objects%20through%0Atheir%20attention%20mechanisms.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20to%0Aimprove%20the%20HPE%20accuracy%20by%20leveraging%20the%20object%20detection%20grounding%0Acapability%20of%20a%20VLM%2C%20referred%20to%20as%20CogVLM.%20We%20empirically%20find%20that%20directly%0ALoRA%20fine-tuning%20of%20this%20VLM%20for%20the%20HPE%20task%20fails%20to%20achieve%20desirable%20HPE%0Aaccuracy%2C%20while%20some%20model%20merging%20methods%20can%20improve%20accuracy%20but%20frequently%0Aproduce%20blended%20invalid%20response%20formats%2C%20struggling%20to%20handle%20both%20object%0Adetection%20and%20HPE%20tasks%20simultaneously.%20To%20integrate%20HPE%20capability%20into%20CogVLM%0Aeffectively%2C%20we%20develop%20a%20novel%20LoRA%20layer-based%20model%20merging%20method.%20This%0Amerging%20approach%20applies%20a%20high%20cosine%20similarity%20threshold%20and%20a%0Awinner-takes-all%20layer%20selection%20strategy%2C%20aligning%20attention%20to%20the%20HPE%20task%0Awhile%20preserving%20original%20object%20detection%20knowledge.%20It%20successfully%20resolves%0Aissues%20with%20blended%20invalid%20response%20formats%20and%20improves%20accuracy.%20Results%0Ashow%20that%20our%20HPE-CogVLM%20achieves%20a%2031.5%5C%25%20reduction%20in%20Mean%20Absolute%20Error%0Aover%20the%20current%20state-of-the-art%20CNN%20model%2C%206DRepNet%2C%20in%20cross-dataset%0Aevaluation.%20Furthermore%2C%20HPE-CogVLM%20outperforms%20both%20directly%20LoRA%20fine-tuned%0Aand%20task%20arithmetic-based%20merged%20VLMs%20across%20all%20HPE%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01914v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHPE-CogVLM%253A%2520Advancing%2520Vision%2520Language%2520Models%2520with%2520a%2520Head%2520Pose%2520Grounding%250A%2520%2520Task%26entry.906535625%3DYu%2520Tian%2520and%2520Tianqi%2520Shao%2520and%2520Tsukasa%2520Demizu%2520and%2520Xuyang%2520Wu%2520and%2520Hsin-Tai%2520Wu%26entry.1292438233%3D%2520%2520Head%2520pose%2520estimation%2520%2528HPE%2529%2520requires%2520a%2520sophisticated%2520understanding%2520of%25203D%250Aspatial%2520relationships%2520to%2520generate%2520precise%2520yaw%252C%2520pitch%252C%2520and%2520roll%2520angles.%2520Previous%250AHPE%2520models%252C%2520primarily%2520CNN-based%252C%2520rely%2520on%2520cropped%2520close-up%2520human%2520head%2520images%2520as%250Ainputs%2520and%2520often%2520lack%2520robustness%2520in%2520real-world%2520scenario.%2520Vision%2520Language%2520Models%250A%2528VLMs%2529%2520can%2520analyze%2520entire%2520images%2520while%2520focusing%2520on%2520specific%2520objects%2520through%250Atheir%2520attention%2520mechanisms.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520framework%2520to%250Aimprove%2520the%2520HPE%2520accuracy%2520by%2520leveraging%2520the%2520object%2520detection%2520grounding%250Acapability%2520of%2520a%2520VLM%252C%2520referred%2520to%2520as%2520CogVLM.%2520We%2520empirically%2520find%2520that%2520directly%250ALoRA%2520fine-tuning%2520of%2520this%2520VLM%2520for%2520the%2520HPE%2520task%2520fails%2520to%2520achieve%2520desirable%2520HPE%250Aaccuracy%252C%2520while%2520some%2520model%2520merging%2520methods%2520can%2520improve%2520accuracy%2520but%2520frequently%250Aproduce%2520blended%2520invalid%2520response%2520formats%252C%2520struggling%2520to%2520handle%2520both%2520object%250Adetection%2520and%2520HPE%2520tasks%2520simultaneously.%2520To%2520integrate%2520HPE%2520capability%2520into%2520CogVLM%250Aeffectively%252C%2520we%2520develop%2520a%2520novel%2520LoRA%2520layer-based%2520model%2520merging%2520method.%2520This%250Amerging%2520approach%2520applies%2520a%2520high%2520cosine%2520similarity%2520threshold%2520and%2520a%250Awinner-takes-all%2520layer%2520selection%2520strategy%252C%2520aligning%2520attention%2520to%2520the%2520HPE%2520task%250Awhile%2520preserving%2520original%2520object%2520detection%2520knowledge.%2520It%2520successfully%2520resolves%250Aissues%2520with%2520blended%2520invalid%2520response%2520formats%2520and%2520improves%2520accuracy.%2520Results%250Ashow%2520that%2520our%2520HPE-CogVLM%2520achieves%2520a%252031.5%255C%2525%2520reduction%2520in%2520Mean%2520Absolute%2520Error%250Aover%2520the%2520current%2520state-of-the-art%2520CNN%2520model%252C%25206DRepNet%252C%2520in%2520cross-dataset%250Aevaluation.%2520Furthermore%252C%2520HPE-CogVLM%2520outperforms%2520both%2520directly%2520LoRA%2520fine-tuned%250Aand%2520task%2520arithmetic-based%2520merged%2520VLMs%2520across%2520all%2520HPE%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01914v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HPE-CogVLM%3A%20Advancing%20Vision%20Language%20Models%20with%20a%20Head%20Pose%20Grounding%0A%20%20Task&entry.906535625=Yu%20Tian%20and%20Tianqi%20Shao%20and%20Tsukasa%20Demizu%20and%20Xuyang%20Wu%20and%20Hsin-Tai%20Wu&entry.1292438233=%20%20Head%20pose%20estimation%20%28HPE%29%20requires%20a%20sophisticated%20understanding%20of%203D%0Aspatial%20relationships%20to%20generate%20precise%20yaw%2C%20pitch%2C%20and%20roll%20angles.%20Previous%0AHPE%20models%2C%20primarily%20CNN-based%2C%20rely%20on%20cropped%20close-up%20human%20head%20images%20as%0Ainputs%20and%20often%20lack%20robustness%20in%20real-world%20scenario.%20Vision%20Language%20Models%0A%28VLMs%29%20can%20analyze%20entire%20images%20while%20focusing%20on%20specific%20objects%20through%0Atheir%20attention%20mechanisms.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20framework%20to%0Aimprove%20the%20HPE%20accuracy%20by%20leveraging%20the%20object%20detection%20grounding%0Acapability%20of%20a%20VLM%2C%20referred%20to%20as%20CogVLM.%20We%20empirically%20find%20that%20directly%0ALoRA%20fine-tuning%20of%20this%20VLM%20for%20the%20HPE%20task%20fails%20to%20achieve%20desirable%20HPE%0Aaccuracy%2C%20while%20some%20model%20merging%20methods%20can%20improve%20accuracy%20but%20frequently%0Aproduce%20blended%20invalid%20response%20formats%2C%20struggling%20to%20handle%20both%20object%0Adetection%20and%20HPE%20tasks%20simultaneously.%20To%20integrate%20HPE%20capability%20into%20CogVLM%0Aeffectively%2C%20we%20develop%20a%20novel%20LoRA%20layer-based%20model%20merging%20method.%20This%0Amerging%20approach%20applies%20a%20high%20cosine%20similarity%20threshold%20and%20a%0Awinner-takes-all%20layer%20selection%20strategy%2C%20aligning%20attention%20to%20the%20HPE%20task%0Awhile%20preserving%20original%20object%20detection%20knowledge.%20It%20successfully%20resolves%0Aissues%20with%20blended%20invalid%20response%20formats%20and%20improves%20accuracy.%20Results%0Ashow%20that%20our%20HPE-CogVLM%20achieves%20a%2031.5%5C%25%20reduction%20in%20Mean%20Absolute%20Error%0Aover%20the%20current%20state-of-the-art%20CNN%20model%2C%206DRepNet%2C%20in%20cross-dataset%0Aevaluation.%20Furthermore%2C%20HPE-CogVLM%20outperforms%20both%20directly%20LoRA%20fine-tuned%0Aand%20task%20arithmetic-based%20merged%20VLMs%20across%20all%20HPE%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01914v2&entry.124074799=Read"},
{"title": "Poze: Sports Technique Feedback under Data Constraints", "author": "Agamdeep Singh and Sujit PB and Mayank Vatsa", "abstract": "  Access to expert coaching is essential for developing technique in sports,\nyet economic barriers often place it out of reach for many enthusiasts. To\nbridge this gap, we introduce Poze, an innovative video processing framework\nthat provides feedback on human motion, emulating the insights of a\nprofessional coach. Poze combines pose estimation with sequence comparison and\nis optimized to function effectively with minimal data. Poze surpasses\nstate-of-the-art vision-language models in video question-answering frameworks,\nachieving 70% and 196% increase in accuracy over GPT4V and LLaVAv1.6 7b,\nrespectively.\n", "link": "http://arxiv.org/abs/2411.05734v1", "date": "2024-11-08", "relevancy": 2.6861, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5393}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5366}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5358}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Poze%3A%20Sports%20Technique%20Feedback%20under%20Data%20Constraints&body=Title%3A%20Poze%3A%20Sports%20Technique%20Feedback%20under%20Data%20Constraints%0AAuthor%3A%20Agamdeep%20Singh%20and%20Sujit%20PB%20and%20Mayank%20Vatsa%0AAbstract%3A%20%20%20Access%20to%20expert%20coaching%20is%20essential%20for%20developing%20technique%20in%20sports%2C%0Ayet%20economic%20barriers%20often%20place%20it%20out%20of%20reach%20for%20many%20enthusiasts.%20To%0Abridge%20this%20gap%2C%20we%20introduce%20Poze%2C%20an%20innovative%20video%20processing%20framework%0Athat%20provides%20feedback%20on%20human%20motion%2C%20emulating%20the%20insights%20of%20a%0Aprofessional%20coach.%20Poze%20combines%20pose%20estimation%20with%20sequence%20comparison%20and%0Ais%20optimized%20to%20function%20effectively%20with%20minimal%20data.%20Poze%20surpasses%0Astate-of-the-art%20vision-language%20models%20in%20video%20question-answering%20frameworks%2C%0Aachieving%2070%25%20and%20196%25%20increase%20in%20accuracy%20over%20GPT4V%20and%20LLaVAv1.6%207b%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05734v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoze%253A%2520Sports%2520Technique%2520Feedback%2520under%2520Data%2520Constraints%26entry.906535625%3DAgamdeep%2520Singh%2520and%2520Sujit%2520PB%2520and%2520Mayank%2520Vatsa%26entry.1292438233%3D%2520%2520Access%2520to%2520expert%2520coaching%2520is%2520essential%2520for%2520developing%2520technique%2520in%2520sports%252C%250Ayet%2520economic%2520barriers%2520often%2520place%2520it%2520out%2520of%2520reach%2520for%2520many%2520enthusiasts.%2520To%250Abridge%2520this%2520gap%252C%2520we%2520introduce%2520Poze%252C%2520an%2520innovative%2520video%2520processing%2520framework%250Athat%2520provides%2520feedback%2520on%2520human%2520motion%252C%2520emulating%2520the%2520insights%2520of%2520a%250Aprofessional%2520coach.%2520Poze%2520combines%2520pose%2520estimation%2520with%2520sequence%2520comparison%2520and%250Ais%2520optimized%2520to%2520function%2520effectively%2520with%2520minimal%2520data.%2520Poze%2520surpasses%250Astate-of-the-art%2520vision-language%2520models%2520in%2520video%2520question-answering%2520frameworks%252C%250Aachieving%252070%2525%2520and%2520196%2525%2520increase%2520in%2520accuracy%2520over%2520GPT4V%2520and%2520LLaVAv1.6%25207b%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05734v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Poze%3A%20Sports%20Technique%20Feedback%20under%20Data%20Constraints&entry.906535625=Agamdeep%20Singh%20and%20Sujit%20PB%20and%20Mayank%20Vatsa&entry.1292438233=%20%20Access%20to%20expert%20coaching%20is%20essential%20for%20developing%20technique%20in%20sports%2C%0Ayet%20economic%20barriers%20often%20place%20it%20out%20of%20reach%20for%20many%20enthusiasts.%20To%0Abridge%20this%20gap%2C%20we%20introduce%20Poze%2C%20an%20innovative%20video%20processing%20framework%0Athat%20provides%20feedback%20on%20human%20motion%2C%20emulating%20the%20insights%20of%20a%0Aprofessional%20coach.%20Poze%20combines%20pose%20estimation%20with%20sequence%20comparison%20and%0Ais%20optimized%20to%20function%20effectively%20with%20minimal%20data.%20Poze%20surpasses%0Astate-of-the-art%20vision-language%20models%20in%20video%20question-answering%20frameworks%2C%0Aachieving%2070%25%20and%20196%25%20increase%20in%20accuracy%20over%20GPT4V%20and%20LLaVAv1.6%207b%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05734v1&entry.124074799=Read"},
{"title": "Enhancing Vision-Language Few-Shot Adaptation with Negative Learning", "author": "Ce Zhang and Simon Stepputtis and Katia Sycara and Yaqi Xie", "abstract": "  Large-scale pre-trained Vision-Language Models (VLMs) have exhibited\nimpressive zero-shot performance and transferability, allowing them to adapt to\ndownstream tasks in a data-efficient manner. However, when only a few labeled\nsamples are available, adapting VLMs to distinguish subtle differences between\nsimilar classes in specific downstream tasks remains challenging. In this work,\nwe propose a Simple yet effective Negative Learning approach, SimNL, to more\nefficiently exploit the task-specific knowledge from few-shot labeled samples.\nUnlike previous methods that focus on identifying a set of representative\npositive features defining \"what is a {CLASS}\", SimNL discovers a complementary\nset of negative features that define \"what is not a {CLASS}\", providing\nadditional insights that supplement the positive features to enhance\ntask-specific recognition capability. Further, we identify that current\nadaptation approaches are particularly vulnerable to potential noise in the\nfew-shot sample set. To mitigate this issue, we introduce a plug-and-play\nfew-shot instance reweighting technique to suppress noisy outliers and amplify\nclean samples for more stable adaptation. Our extensive experimental results\nacross 15 datasets validate that the proposed SimNL outperforms existing\nstate-of-the-art methods on both few-shot learning and domain generalization\ntasks while achieving competitive computational efficiency. Code is available\nat https://github.com/zhangce01/SimNL.\n", "link": "http://arxiv.org/abs/2403.12964v2", "date": "2024-11-08", "relevancy": 2.6644, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5638}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5332}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Vision-Language%20Few-Shot%20Adaptation%20with%20Negative%20Learning&body=Title%3A%20Enhancing%20Vision-Language%20Few-Shot%20Adaptation%20with%20Negative%20Learning%0AAuthor%3A%20Ce%20Zhang%20and%20Simon%20Stepputtis%20and%20Katia%20Sycara%20and%20Yaqi%20Xie%0AAbstract%3A%20%20%20Large-scale%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20have%20exhibited%0Aimpressive%20zero-shot%20performance%20and%20transferability%2C%20allowing%20them%20to%20adapt%20to%0Adownstream%20tasks%20in%20a%20data-efficient%20manner.%20However%2C%20when%20only%20a%20few%20labeled%0Asamples%20are%20available%2C%20adapting%20VLMs%20to%20distinguish%20subtle%20differences%20between%0Asimilar%20classes%20in%20specific%20downstream%20tasks%20remains%20challenging.%20In%20this%20work%2C%0Awe%20propose%20a%20Simple%20yet%20effective%20Negative%20Learning%20approach%2C%20SimNL%2C%20to%20more%0Aefficiently%20exploit%20the%20task-specific%20knowledge%20from%20few-shot%20labeled%20samples.%0AUnlike%20previous%20methods%20that%20focus%20on%20identifying%20a%20set%20of%20representative%0Apositive%20features%20defining%20%22what%20is%20a%20%7BCLASS%7D%22%2C%20SimNL%20discovers%20a%20complementary%0Aset%20of%20negative%20features%20that%20define%20%22what%20is%20not%20a%20%7BCLASS%7D%22%2C%20providing%0Aadditional%20insights%20that%20supplement%20the%20positive%20features%20to%20enhance%0Atask-specific%20recognition%20capability.%20Further%2C%20we%20identify%20that%20current%0Aadaptation%20approaches%20are%20particularly%20vulnerable%20to%20potential%20noise%20in%20the%0Afew-shot%20sample%20set.%20To%20mitigate%20this%20issue%2C%20we%20introduce%20a%20plug-and-play%0Afew-shot%20instance%20reweighting%20technique%20to%20suppress%20noisy%20outliers%20and%20amplify%0Aclean%20samples%20for%20more%20stable%20adaptation.%20Our%20extensive%20experimental%20results%0Aacross%2015%20datasets%20validate%20that%20the%20proposed%20SimNL%20outperforms%20existing%0Astate-of-the-art%20methods%20on%20both%20few-shot%20learning%20and%20domain%20generalization%0Atasks%20while%20achieving%20competitive%20computational%20efficiency.%20Code%20is%20available%0Aat%20https%3A//github.com/zhangce01/SimNL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12964v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Vision-Language%2520Few-Shot%2520Adaptation%2520with%2520Negative%2520Learning%26entry.906535625%3DCe%2520Zhang%2520and%2520Simon%2520Stepputtis%2520and%2520Katia%2520Sycara%2520and%2520Yaqi%2520Xie%26entry.1292438233%3D%2520%2520Large-scale%2520pre-trained%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520exhibited%250Aimpressive%2520zero-shot%2520performance%2520and%2520transferability%252C%2520allowing%2520them%2520to%2520adapt%2520to%250Adownstream%2520tasks%2520in%2520a%2520data-efficient%2520manner.%2520However%252C%2520when%2520only%2520a%2520few%2520labeled%250Asamples%2520are%2520available%252C%2520adapting%2520VLMs%2520to%2520distinguish%2520subtle%2520differences%2520between%250Asimilar%2520classes%2520in%2520specific%2520downstream%2520tasks%2520remains%2520challenging.%2520In%2520this%2520work%252C%250Awe%2520propose%2520a%2520Simple%2520yet%2520effective%2520Negative%2520Learning%2520approach%252C%2520SimNL%252C%2520to%2520more%250Aefficiently%2520exploit%2520the%2520task-specific%2520knowledge%2520from%2520few-shot%2520labeled%2520samples.%250AUnlike%2520previous%2520methods%2520that%2520focus%2520on%2520identifying%2520a%2520set%2520of%2520representative%250Apositive%2520features%2520defining%2520%2522what%2520is%2520a%2520%257BCLASS%257D%2522%252C%2520SimNL%2520discovers%2520a%2520complementary%250Aset%2520of%2520negative%2520features%2520that%2520define%2520%2522what%2520is%2520not%2520a%2520%257BCLASS%257D%2522%252C%2520providing%250Aadditional%2520insights%2520that%2520supplement%2520the%2520positive%2520features%2520to%2520enhance%250Atask-specific%2520recognition%2520capability.%2520Further%252C%2520we%2520identify%2520that%2520current%250Aadaptation%2520approaches%2520are%2520particularly%2520vulnerable%2520to%2520potential%2520noise%2520in%2520the%250Afew-shot%2520sample%2520set.%2520To%2520mitigate%2520this%2520issue%252C%2520we%2520introduce%2520a%2520plug-and-play%250Afew-shot%2520instance%2520reweighting%2520technique%2520to%2520suppress%2520noisy%2520outliers%2520and%2520amplify%250Aclean%2520samples%2520for%2520more%2520stable%2520adaptation.%2520Our%2520extensive%2520experimental%2520results%250Aacross%252015%2520datasets%2520validate%2520that%2520the%2520proposed%2520SimNL%2520outperforms%2520existing%250Astate-of-the-art%2520methods%2520on%2520both%2520few-shot%2520learning%2520and%2520domain%2520generalization%250Atasks%2520while%2520achieving%2520competitive%2520computational%2520efficiency.%2520Code%2520is%2520available%250Aat%2520https%253A//github.com/zhangce01/SimNL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12964v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Vision-Language%20Few-Shot%20Adaptation%20with%20Negative%20Learning&entry.906535625=Ce%20Zhang%20and%20Simon%20Stepputtis%20and%20Katia%20Sycara%20and%20Yaqi%20Xie&entry.1292438233=%20%20Large-scale%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20have%20exhibited%0Aimpressive%20zero-shot%20performance%20and%20transferability%2C%20allowing%20them%20to%20adapt%20to%0Adownstream%20tasks%20in%20a%20data-efficient%20manner.%20However%2C%20when%20only%20a%20few%20labeled%0Asamples%20are%20available%2C%20adapting%20VLMs%20to%20distinguish%20subtle%20differences%20between%0Asimilar%20classes%20in%20specific%20downstream%20tasks%20remains%20challenging.%20In%20this%20work%2C%0Awe%20propose%20a%20Simple%20yet%20effective%20Negative%20Learning%20approach%2C%20SimNL%2C%20to%20more%0Aefficiently%20exploit%20the%20task-specific%20knowledge%20from%20few-shot%20labeled%20samples.%0AUnlike%20previous%20methods%20that%20focus%20on%20identifying%20a%20set%20of%20representative%0Apositive%20features%20defining%20%22what%20is%20a%20%7BCLASS%7D%22%2C%20SimNL%20discovers%20a%20complementary%0Aset%20of%20negative%20features%20that%20define%20%22what%20is%20not%20a%20%7BCLASS%7D%22%2C%20providing%0Aadditional%20insights%20that%20supplement%20the%20positive%20features%20to%20enhance%0Atask-specific%20recognition%20capability.%20Further%2C%20we%20identify%20that%20current%0Aadaptation%20approaches%20are%20particularly%20vulnerable%20to%20potential%20noise%20in%20the%0Afew-shot%20sample%20set.%20To%20mitigate%20this%20issue%2C%20we%20introduce%20a%20plug-and-play%0Afew-shot%20instance%20reweighting%20technique%20to%20suppress%20noisy%20outliers%20and%20amplify%0Aclean%20samples%20for%20more%20stable%20adaptation.%20Our%20extensive%20experimental%20results%0Aacross%2015%20datasets%20validate%20that%20the%20proposed%20SimNL%20outperforms%20existing%0Astate-of-the-art%20methods%20on%20both%20few-shot%20learning%20and%20domain%20generalization%0Atasks%20while%20achieving%20competitive%20computational%20efficiency.%20Code%20is%20available%0Aat%20https%3A//github.com/zhangce01/SimNL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12964v2&entry.124074799=Read"},
{"title": "Online-LoRA: Task-free Online Continual Learning via Low Rank Adaptation", "author": "Xiwen Wei and Guihong Li and Radu Marculescu", "abstract": "  Catastrophic forgetting is a significant challenge in online continual\nlearning (OCL), especially for non-stationary data streams that do not have\nwell-defined task boundaries. This challenge is exacerbated by the memory\nconstraints and privacy concerns inherent in rehearsal buffers. To tackle\ncatastrophic forgetting, in this paper, we introduce Online-LoRA, a novel\nframework for task-free OCL. Online-LoRA allows to finetune pre-trained Vision\nTransformer (ViT) models in real-time to address the limitations of rehearsal\nbuffers and leverage pre-trained models' performance benefits. As the main\ncontribution, our approach features a novel online weight regularization\nstrategy to identify and consolidate important model parameters. Moreover,\nOnline-LoRA leverages the training dynamics of loss values to enable the\nautomatic recognition of the data distribution shifts. Extensive experiments\nacross many task-free OCL scenarios and benchmark datasets (including\nCIFAR-100, ImageNet-R, ImageNet-S, CUB-200 and CORe50) demonstrate that\nOnline-LoRA can be robustly adapted to various ViT architectures, while\nachieving better performance compared to SOTA methods. Our code will be\npublicly available at:\nhttps://github.com/Christina200/Online-LoRA-official.git.\n", "link": "http://arxiv.org/abs/2411.05663v1", "date": "2024-11-08", "relevancy": 2.6538, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5372}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5279}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online-LoRA%3A%20Task-free%20Online%20Continual%20Learning%20via%20Low%20Rank%20Adaptation&body=Title%3A%20Online-LoRA%3A%20Task-free%20Online%20Continual%20Learning%20via%20Low%20Rank%20Adaptation%0AAuthor%3A%20Xiwen%20Wei%20and%20Guihong%20Li%20and%20Radu%20Marculescu%0AAbstract%3A%20%20%20Catastrophic%20forgetting%20is%20a%20significant%20challenge%20in%20online%20continual%0Alearning%20%28OCL%29%2C%20especially%20for%20non-stationary%20data%20streams%20that%20do%20not%20have%0Awell-defined%20task%20boundaries.%20This%20challenge%20is%20exacerbated%20by%20the%20memory%0Aconstraints%20and%20privacy%20concerns%20inherent%20in%20rehearsal%20buffers.%20To%20tackle%0Acatastrophic%20forgetting%2C%20in%20this%20paper%2C%20we%20introduce%20Online-LoRA%2C%20a%20novel%0Aframework%20for%20task-free%20OCL.%20Online-LoRA%20allows%20to%20finetune%20pre-trained%20Vision%0ATransformer%20%28ViT%29%20models%20in%20real-time%20to%20address%20the%20limitations%20of%20rehearsal%0Abuffers%20and%20leverage%20pre-trained%20models%27%20performance%20benefits.%20As%20the%20main%0Acontribution%2C%20our%20approach%20features%20a%20novel%20online%20weight%20regularization%0Astrategy%20to%20identify%20and%20consolidate%20important%20model%20parameters.%20Moreover%2C%0AOnline-LoRA%20leverages%20the%20training%20dynamics%20of%20loss%20values%20to%20enable%20the%0Aautomatic%20recognition%20of%20the%20data%20distribution%20shifts.%20Extensive%20experiments%0Aacross%20many%20task-free%20OCL%20scenarios%20and%20benchmark%20datasets%20%28including%0ACIFAR-100%2C%20ImageNet-R%2C%20ImageNet-S%2C%20CUB-200%20and%20CORe50%29%20demonstrate%20that%0AOnline-LoRA%20can%20be%20robustly%20adapted%20to%20various%20ViT%20architectures%2C%20while%0Aachieving%20better%20performance%20compared%20to%20SOTA%20methods.%20Our%20code%20will%20be%0Apublicly%20available%20at%3A%0Ahttps%3A//github.com/Christina200/Online-LoRA-official.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline-LoRA%253A%2520Task-free%2520Online%2520Continual%2520Learning%2520via%2520Low%2520Rank%2520Adaptation%26entry.906535625%3DXiwen%2520Wei%2520and%2520Guihong%2520Li%2520and%2520Radu%2520Marculescu%26entry.1292438233%3D%2520%2520Catastrophic%2520forgetting%2520is%2520a%2520significant%2520challenge%2520in%2520online%2520continual%250Alearning%2520%2528OCL%2529%252C%2520especially%2520for%2520non-stationary%2520data%2520streams%2520that%2520do%2520not%2520have%250Awell-defined%2520task%2520boundaries.%2520This%2520challenge%2520is%2520exacerbated%2520by%2520the%2520memory%250Aconstraints%2520and%2520privacy%2520concerns%2520inherent%2520in%2520rehearsal%2520buffers.%2520To%2520tackle%250Acatastrophic%2520forgetting%252C%2520in%2520this%2520paper%252C%2520we%2520introduce%2520Online-LoRA%252C%2520a%2520novel%250Aframework%2520for%2520task-free%2520OCL.%2520Online-LoRA%2520allows%2520to%2520finetune%2520pre-trained%2520Vision%250ATransformer%2520%2528ViT%2529%2520models%2520in%2520real-time%2520to%2520address%2520the%2520limitations%2520of%2520rehearsal%250Abuffers%2520and%2520leverage%2520pre-trained%2520models%2527%2520performance%2520benefits.%2520As%2520the%2520main%250Acontribution%252C%2520our%2520approach%2520features%2520a%2520novel%2520online%2520weight%2520regularization%250Astrategy%2520to%2520identify%2520and%2520consolidate%2520important%2520model%2520parameters.%2520Moreover%252C%250AOnline-LoRA%2520leverages%2520the%2520training%2520dynamics%2520of%2520loss%2520values%2520to%2520enable%2520the%250Aautomatic%2520recognition%2520of%2520the%2520data%2520distribution%2520shifts.%2520Extensive%2520experiments%250Aacross%2520many%2520task-free%2520OCL%2520scenarios%2520and%2520benchmark%2520datasets%2520%2528including%250ACIFAR-100%252C%2520ImageNet-R%252C%2520ImageNet-S%252C%2520CUB-200%2520and%2520CORe50%2529%2520demonstrate%2520that%250AOnline-LoRA%2520can%2520be%2520robustly%2520adapted%2520to%2520various%2520ViT%2520architectures%252C%2520while%250Aachieving%2520better%2520performance%2520compared%2520to%2520SOTA%2520methods.%2520Our%2520code%2520will%2520be%250Apublicly%2520available%2520at%253A%250Ahttps%253A//github.com/Christina200/Online-LoRA-official.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online-LoRA%3A%20Task-free%20Online%20Continual%20Learning%20via%20Low%20Rank%20Adaptation&entry.906535625=Xiwen%20Wei%20and%20Guihong%20Li%20and%20Radu%20Marculescu&entry.1292438233=%20%20Catastrophic%20forgetting%20is%20a%20significant%20challenge%20in%20online%20continual%0Alearning%20%28OCL%29%2C%20especially%20for%20non-stationary%20data%20streams%20that%20do%20not%20have%0Awell-defined%20task%20boundaries.%20This%20challenge%20is%20exacerbated%20by%20the%20memory%0Aconstraints%20and%20privacy%20concerns%20inherent%20in%20rehearsal%20buffers.%20To%20tackle%0Acatastrophic%20forgetting%2C%20in%20this%20paper%2C%20we%20introduce%20Online-LoRA%2C%20a%20novel%0Aframework%20for%20task-free%20OCL.%20Online-LoRA%20allows%20to%20finetune%20pre-trained%20Vision%0ATransformer%20%28ViT%29%20models%20in%20real-time%20to%20address%20the%20limitations%20of%20rehearsal%0Abuffers%20and%20leverage%20pre-trained%20models%27%20performance%20benefits.%20As%20the%20main%0Acontribution%2C%20our%20approach%20features%20a%20novel%20online%20weight%20regularization%0Astrategy%20to%20identify%20and%20consolidate%20important%20model%20parameters.%20Moreover%2C%0AOnline-LoRA%20leverages%20the%20training%20dynamics%20of%20loss%20values%20to%20enable%20the%0Aautomatic%20recognition%20of%20the%20data%20distribution%20shifts.%20Extensive%20experiments%0Aacross%20many%20task-free%20OCL%20scenarios%20and%20benchmark%20datasets%20%28including%0ACIFAR-100%2C%20ImageNet-R%2C%20ImageNet-S%2C%20CUB-200%20and%20CORe50%29%20demonstrate%20that%0AOnline-LoRA%20can%20be%20robustly%20adapted%20to%20various%20ViT%20architectures%2C%20while%0Aachieving%20better%20performance%20compared%20to%20SOTA%20methods.%20Our%20code%20will%20be%0Apublicly%20available%20at%3A%0Ahttps%3A//github.com/Christina200/Online-LoRA-official.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05663v1&entry.124074799=Read"},
{"title": "How to Connect Speech Foundation Models and Large Language Models? What\n  Matters and What Does Not", "author": "Francesco Verdini and Pierfrancesco Melucci and Stefano Perna and Francesco Cariaggi and Marco Gaido and Sara Papi and Szymon Mazurek and Marek Kasztelnik and Luisa Bentivogli and S\u00e9bastien Brati\u00e8res and Paolo Merialdo and Simone Scardapane", "abstract": "  The remarkable performance achieved by Large Language Models (LLM) has driven\nresearch efforts to leverage them for a wide range of tasks and input\nmodalities. In speech-to-text (S2T) tasks, the emerging solution consists of\nprojecting the output of the encoder of a Speech Foundational Model (SFM) into\nthe LLM embedding space through an adapter module. However, no work has yet\ninvestigated how much the downstream-task performance depends on each component\n(SFM, adapter, LLM) nor whether the best design of the adapter depends on the\nchosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter\nmodules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on\ntwo widespread S2T tasks, namely Automatic Speech Recognition and Speech\nTranslation. Our results demonstrate that the SFM plays a pivotal role in\ndownstream performance, while the adapter choice has moderate impact and\ndepends on the SFM and LLM.\n", "link": "http://arxiv.org/abs/2409.17044v2", "date": "2024-11-08", "relevancy": 2.6511, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5347}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5347}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Connect%20Speech%20Foundation%20Models%20and%20Large%20Language%20Models%3F%20What%0A%20%20Matters%20and%20What%20Does%20Not&body=Title%3A%20How%20to%20Connect%20Speech%20Foundation%20Models%20and%20Large%20Language%20Models%3F%20What%0A%20%20Matters%20and%20What%20Does%20Not%0AAuthor%3A%20Francesco%20Verdini%20and%20Pierfrancesco%20Melucci%20and%20Stefano%20Perna%20and%20Francesco%20Cariaggi%20and%20Marco%20Gaido%20and%20Sara%20Papi%20and%20Szymon%20Mazurek%20and%20Marek%20Kasztelnik%20and%20Luisa%20Bentivogli%20and%20S%C3%A9bastien%20Brati%C3%A8res%20and%20Paolo%20Merialdo%20and%20Simone%20Scardapane%0AAbstract%3A%20%20%20The%20remarkable%20performance%20achieved%20by%20Large%20Language%20Models%20%28LLM%29%20has%20driven%0Aresearch%20efforts%20to%20leverage%20them%20for%20a%20wide%20range%20of%20tasks%20and%20input%0Amodalities.%20In%20speech-to-text%20%28S2T%29%20tasks%2C%20the%20emerging%20solution%20consists%20of%0Aprojecting%20the%20output%20of%20the%20encoder%20of%20a%20Speech%20Foundational%20Model%20%28SFM%29%20into%0Athe%20LLM%20embedding%20space%20through%20an%20adapter%20module.%20However%2C%20no%20work%20has%20yet%0Ainvestigated%20how%20much%20the%20downstream-task%20performance%20depends%20on%20each%20component%0A%28SFM%2C%20adapter%2C%20LLM%29%20nor%20whether%20the%20best%20design%20of%20the%20adapter%20depends%20on%20the%0Achosen%20SFM%20and%20LLM.%20To%20fill%20this%20gap%2C%20we%20evaluate%20the%20combination%20of%205%20adapter%0Amodules%2C%202%20LLMs%20%28Mistral%20and%20Llama%29%2C%20and%202%20SFMs%20%28Whisper%20and%20SeamlessM4T%29%20on%0Atwo%20widespread%20S2T%20tasks%2C%20namely%20Automatic%20Speech%20Recognition%20and%20Speech%0ATranslation.%20Our%20results%20demonstrate%20that%20the%20SFM%20plays%20a%20pivotal%20role%20in%0Adownstream%20performance%2C%20while%20the%20adapter%20choice%20has%20moderate%20impact%20and%0Adepends%20on%20the%20SFM%20and%20LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17044v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Connect%2520Speech%2520Foundation%2520Models%2520and%2520Large%2520Language%2520Models%253F%2520What%250A%2520%2520Matters%2520and%2520What%2520Does%2520Not%26entry.906535625%3DFrancesco%2520Verdini%2520and%2520Pierfrancesco%2520Melucci%2520and%2520Stefano%2520Perna%2520and%2520Francesco%2520Cariaggi%2520and%2520Marco%2520Gaido%2520and%2520Sara%2520Papi%2520and%2520Szymon%2520Mazurek%2520and%2520Marek%2520Kasztelnik%2520and%2520Luisa%2520Bentivogli%2520and%2520S%25C3%25A9bastien%2520Brati%25C3%25A8res%2520and%2520Paolo%2520Merialdo%2520and%2520Simone%2520Scardapane%26entry.1292438233%3D%2520%2520The%2520remarkable%2520performance%2520achieved%2520by%2520Large%2520Language%2520Models%2520%2528LLM%2529%2520has%2520driven%250Aresearch%2520efforts%2520to%2520leverage%2520them%2520for%2520a%2520wide%2520range%2520of%2520tasks%2520and%2520input%250Amodalities.%2520In%2520speech-to-text%2520%2528S2T%2529%2520tasks%252C%2520the%2520emerging%2520solution%2520consists%2520of%250Aprojecting%2520the%2520output%2520of%2520the%2520encoder%2520of%2520a%2520Speech%2520Foundational%2520Model%2520%2528SFM%2529%2520into%250Athe%2520LLM%2520embedding%2520space%2520through%2520an%2520adapter%2520module.%2520However%252C%2520no%2520work%2520has%2520yet%250Ainvestigated%2520how%2520much%2520the%2520downstream-task%2520performance%2520depends%2520on%2520each%2520component%250A%2528SFM%252C%2520adapter%252C%2520LLM%2529%2520nor%2520whether%2520the%2520best%2520design%2520of%2520the%2520adapter%2520depends%2520on%2520the%250Achosen%2520SFM%2520and%2520LLM.%2520To%2520fill%2520this%2520gap%252C%2520we%2520evaluate%2520the%2520combination%2520of%25205%2520adapter%250Amodules%252C%25202%2520LLMs%2520%2528Mistral%2520and%2520Llama%2529%252C%2520and%25202%2520SFMs%2520%2528Whisper%2520and%2520SeamlessM4T%2529%2520on%250Atwo%2520widespread%2520S2T%2520tasks%252C%2520namely%2520Automatic%2520Speech%2520Recognition%2520and%2520Speech%250ATranslation.%2520Our%2520results%2520demonstrate%2520that%2520the%2520SFM%2520plays%2520a%2520pivotal%2520role%2520in%250Adownstream%2520performance%252C%2520while%2520the%2520adapter%2520choice%2520has%2520moderate%2520impact%2520and%250Adepends%2520on%2520the%2520SFM%2520and%2520LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17044v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Connect%20Speech%20Foundation%20Models%20and%20Large%20Language%20Models%3F%20What%0A%20%20Matters%20and%20What%20Does%20Not&entry.906535625=Francesco%20Verdini%20and%20Pierfrancesco%20Melucci%20and%20Stefano%20Perna%20and%20Francesco%20Cariaggi%20and%20Marco%20Gaido%20and%20Sara%20Papi%20and%20Szymon%20Mazurek%20and%20Marek%20Kasztelnik%20and%20Luisa%20Bentivogli%20and%20S%C3%A9bastien%20Brati%C3%A8res%20and%20Paolo%20Merialdo%20and%20Simone%20Scardapane&entry.1292438233=%20%20The%20remarkable%20performance%20achieved%20by%20Large%20Language%20Models%20%28LLM%29%20has%20driven%0Aresearch%20efforts%20to%20leverage%20them%20for%20a%20wide%20range%20of%20tasks%20and%20input%0Amodalities.%20In%20speech-to-text%20%28S2T%29%20tasks%2C%20the%20emerging%20solution%20consists%20of%0Aprojecting%20the%20output%20of%20the%20encoder%20of%20a%20Speech%20Foundational%20Model%20%28SFM%29%20into%0Athe%20LLM%20embedding%20space%20through%20an%20adapter%20module.%20However%2C%20no%20work%20has%20yet%0Ainvestigated%20how%20much%20the%20downstream-task%20performance%20depends%20on%20each%20component%0A%28SFM%2C%20adapter%2C%20LLM%29%20nor%20whether%20the%20best%20design%20of%20the%20adapter%20depends%20on%20the%0Achosen%20SFM%20and%20LLM.%20To%20fill%20this%20gap%2C%20we%20evaluate%20the%20combination%20of%205%20adapter%0Amodules%2C%202%20LLMs%20%28Mistral%20and%20Llama%29%2C%20and%202%20SFMs%20%28Whisper%20and%20SeamlessM4T%29%20on%0Atwo%20widespread%20S2T%20tasks%2C%20namely%20Automatic%20Speech%20Recognition%20and%20Speech%0ATranslation.%20Our%20results%20demonstrate%20that%20the%20SFM%20plays%20a%20pivotal%20role%20in%0Adownstream%20performance%2C%20while%20the%20adapter%20choice%20has%20moderate%20impact%20and%0Adepends%20on%20the%20SFM%20and%20LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17044v2&entry.124074799=Read"},
{"title": "Curriculum Learning for Few-Shot Domain Adaptation in CT-based Airway\n  Tree Segmentation", "author": "Maxime Jacovella and Ali Keshavarzi and Elsa Angelini", "abstract": "  Despite advances with deep learning (DL), automated airway segmentation from\nchest CT scans continues to face challenges in segmentation quality and\ngeneralization across cohorts. To address these, we propose integrating\nCurriculum Learning (CL) into airway segmentation networks, distributing the\ntraining set into batches according to ad-hoc complexity scores derived from CT\nscans and corresponding ground-truth tree features. We specifically investigate\nfew-shot domain adaptation, targeting scenarios where manual annotation of a\nfull fine-tuning dataset is prohibitively expensive. Results are reported on\ntwo large open-cohorts (ATM22 and AIIB23) with high performance using CL for\nfull training (Source domain) and few-shot fine-tuning (Target domain), but\nwith also some insights on potential detrimental effects if using a classic\nBootstrapping scoring function or if not using proper scan sequencing.\n", "link": "http://arxiv.org/abs/2411.05779v1", "date": "2024-11-08", "relevancy": 2.6421, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5874}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5045}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Curriculum%20Learning%20for%20Few-Shot%20Domain%20Adaptation%20in%20CT-based%20Airway%0A%20%20Tree%20Segmentation&body=Title%3A%20Curriculum%20Learning%20for%20Few-Shot%20Domain%20Adaptation%20in%20CT-based%20Airway%0A%20%20Tree%20Segmentation%0AAuthor%3A%20Maxime%20Jacovella%20and%20Ali%20Keshavarzi%20and%20Elsa%20Angelini%0AAbstract%3A%20%20%20Despite%20advances%20with%20deep%20learning%20%28DL%29%2C%20automated%20airway%20segmentation%20from%0Achest%20CT%20scans%20continues%20to%20face%20challenges%20in%20segmentation%20quality%20and%0Ageneralization%20across%20cohorts.%20To%20address%20these%2C%20we%20propose%20integrating%0ACurriculum%20Learning%20%28CL%29%20into%20airway%20segmentation%20networks%2C%20distributing%20the%0Atraining%20set%20into%20batches%20according%20to%20ad-hoc%20complexity%20scores%20derived%20from%20CT%0Ascans%20and%20corresponding%20ground-truth%20tree%20features.%20We%20specifically%20investigate%0Afew-shot%20domain%20adaptation%2C%20targeting%20scenarios%20where%20manual%20annotation%20of%20a%0Afull%20fine-tuning%20dataset%20is%20prohibitively%20expensive.%20Results%20are%20reported%20on%0Atwo%20large%20open-cohorts%20%28ATM22%20and%20AIIB23%29%20with%20high%20performance%20using%20CL%20for%0Afull%20training%20%28Source%20domain%29%20and%20few-shot%20fine-tuning%20%28Target%20domain%29%2C%20but%0Awith%20also%20some%20insights%20on%20potential%20detrimental%20effects%20if%20using%20a%20classic%0ABootstrapping%20scoring%20function%20or%20if%20not%20using%20proper%20scan%20sequencing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurriculum%2520Learning%2520for%2520Few-Shot%2520Domain%2520Adaptation%2520in%2520CT-based%2520Airway%250A%2520%2520Tree%2520Segmentation%26entry.906535625%3DMaxime%2520Jacovella%2520and%2520Ali%2520Keshavarzi%2520and%2520Elsa%2520Angelini%26entry.1292438233%3D%2520%2520Despite%2520advances%2520with%2520deep%2520learning%2520%2528DL%2529%252C%2520automated%2520airway%2520segmentation%2520from%250Achest%2520CT%2520scans%2520continues%2520to%2520face%2520challenges%2520in%2520segmentation%2520quality%2520and%250Ageneralization%2520across%2520cohorts.%2520To%2520address%2520these%252C%2520we%2520propose%2520integrating%250ACurriculum%2520Learning%2520%2528CL%2529%2520into%2520airway%2520segmentation%2520networks%252C%2520distributing%2520the%250Atraining%2520set%2520into%2520batches%2520according%2520to%2520ad-hoc%2520complexity%2520scores%2520derived%2520from%2520CT%250Ascans%2520and%2520corresponding%2520ground-truth%2520tree%2520features.%2520We%2520specifically%2520investigate%250Afew-shot%2520domain%2520adaptation%252C%2520targeting%2520scenarios%2520where%2520manual%2520annotation%2520of%2520a%250Afull%2520fine-tuning%2520dataset%2520is%2520prohibitively%2520expensive.%2520Results%2520are%2520reported%2520on%250Atwo%2520large%2520open-cohorts%2520%2528ATM22%2520and%2520AIIB23%2529%2520with%2520high%2520performance%2520using%2520CL%2520for%250Afull%2520training%2520%2528Source%2520domain%2529%2520and%2520few-shot%2520fine-tuning%2520%2528Target%2520domain%2529%252C%2520but%250Awith%2520also%2520some%2520insights%2520on%2520potential%2520detrimental%2520effects%2520if%2520using%2520a%2520classic%250ABootstrapping%2520scoring%2520function%2520or%2520if%2520not%2520using%2520proper%2520scan%2520sequencing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Curriculum%20Learning%20for%20Few-Shot%20Domain%20Adaptation%20in%20CT-based%20Airway%0A%20%20Tree%20Segmentation&entry.906535625=Maxime%20Jacovella%20and%20Ali%20Keshavarzi%20and%20Elsa%20Angelini&entry.1292438233=%20%20Despite%20advances%20with%20deep%20learning%20%28DL%29%2C%20automated%20airway%20segmentation%20from%0Achest%20CT%20scans%20continues%20to%20face%20challenges%20in%20segmentation%20quality%20and%0Ageneralization%20across%20cohorts.%20To%20address%20these%2C%20we%20propose%20integrating%0ACurriculum%20Learning%20%28CL%29%20into%20airway%20segmentation%20networks%2C%20distributing%20the%0Atraining%20set%20into%20batches%20according%20to%20ad-hoc%20complexity%20scores%20derived%20from%20CT%0Ascans%20and%20corresponding%20ground-truth%20tree%20features.%20We%20specifically%20investigate%0Afew-shot%20domain%20adaptation%2C%20targeting%20scenarios%20where%20manual%20annotation%20of%20a%0Afull%20fine-tuning%20dataset%20is%20prohibitively%20expensive.%20Results%20are%20reported%20on%0Atwo%20large%20open-cohorts%20%28ATM22%20and%20AIIB23%29%20with%20high%20performance%20using%20CL%20for%0Afull%20training%20%28Source%20domain%29%20and%20few-shot%20fine-tuning%20%28Target%20domain%29%2C%20but%0Awith%20also%20some%20insights%20on%20potential%20detrimental%20effects%20if%20using%20a%20classic%0ABootstrapping%20scoring%20function%20or%20if%20not%20using%20proper%20scan%20sequencing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05779v1&entry.124074799=Read"},
{"title": "CodeGRAG: Bridging the Gap between Natural Language and Programming\n  Language via Graphical Retrieval Augmented Generation", "author": "Kounianhua Du and Jizheng Chen and Renting Rui and Huacan Chai and Lingyue Fu and Wei Xia and Yasheng Wang and Ruiming Tang and Yong Yu and Weinan Zhang", "abstract": "  Utilizing large language models to generate codes has shown promising meaning\nin software development revolution. Despite the intelligence shown by the\ngeneral large language models, their specificity in code generation can still\nbe improved due to the syntactic gap and mismatched vocabulary existing among\nnatural language and different programming languages. In this paper, we propose\nCodeGRAG, a Graphical Retrieval Augmented Code Generation framework to enhance\nthe performance of LLMs. CodeGRAG builds the graphical view of code blocks\nbased on the control flow and data flow of them to fill the gap between\nprogramming languages and natural language, which can facilitate natural\nlanguage based LLMs for better understanding of code syntax and serve as a\nbridge among different programming languages. To take the extracted structural\nknowledge into the foundation models, we propose 1) a hard meta-graph prompt\ntemplate to transform the challenging graphical representation into informative\nknowledge for tuning-free models and 2) a soft prompting technique that injects\nthe domain knowledge of programming languages into the model parameters via\nfinetuning the models with the help of a pretrained GNN expert model. Various\nexperiments and ablations are done on four datasets including both the C++ and\npython languages to validate the hard meta-graph prompt, the soft prompting\ntechnique, and the effectiveness of the objectives for pretrained GNN expert.\nCodeGRAG improves the code generation ability of LLMs and can even offer\nperformance gain for cross-lingual code generation. Code is available at\nhttps://anonymous.4open.science/r/Code-5970/.\n", "link": "http://arxiv.org/abs/2405.02355v3", "date": "2024-11-08", "relevancy": 2.637, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5456}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5284}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CodeGRAG%3A%20Bridging%20the%20Gap%20between%20Natural%20Language%20and%20Programming%0A%20%20Language%20via%20Graphical%20Retrieval%20Augmented%20Generation&body=Title%3A%20CodeGRAG%3A%20Bridging%20the%20Gap%20between%20Natural%20Language%20and%20Programming%0A%20%20Language%20via%20Graphical%20Retrieval%20Augmented%20Generation%0AAuthor%3A%20Kounianhua%20Du%20and%20Jizheng%20Chen%20and%20Renting%20Rui%20and%20Huacan%20Chai%20and%20Lingyue%20Fu%20and%20Wei%20Xia%20and%20Yasheng%20Wang%20and%20Ruiming%20Tang%20and%20Yong%20Yu%20and%20Weinan%20Zhang%0AAbstract%3A%20%20%20Utilizing%20large%20language%20models%20to%20generate%20codes%20has%20shown%20promising%20meaning%0Ain%20software%20development%20revolution.%20Despite%20the%20intelligence%20shown%20by%20the%0Ageneral%20large%20language%20models%2C%20their%20specificity%20in%20code%20generation%20can%20still%0Abe%20improved%20due%20to%20the%20syntactic%20gap%20and%20mismatched%20vocabulary%20existing%20among%0Anatural%20language%20and%20different%20programming%20languages.%20In%20this%20paper%2C%20we%20propose%0ACodeGRAG%2C%20a%20Graphical%20Retrieval%20Augmented%20Code%20Generation%20framework%20to%20enhance%0Athe%20performance%20of%20LLMs.%20CodeGRAG%20builds%20the%20graphical%20view%20of%20code%20blocks%0Abased%20on%20the%20control%20flow%20and%20data%20flow%20of%20them%20to%20fill%20the%20gap%20between%0Aprogramming%20languages%20and%20natural%20language%2C%20which%20can%20facilitate%20natural%0Alanguage%20based%20LLMs%20for%20better%20understanding%20of%20code%20syntax%20and%20serve%20as%20a%0Abridge%20among%20different%20programming%20languages.%20To%20take%20the%20extracted%20structural%0Aknowledge%20into%20the%20foundation%20models%2C%20we%20propose%201%29%20a%20hard%20meta-graph%20prompt%0Atemplate%20to%20transform%20the%20challenging%20graphical%20representation%20into%20informative%0Aknowledge%20for%20tuning-free%20models%20and%202%29%20a%20soft%20prompting%20technique%20that%20injects%0Athe%20domain%20knowledge%20of%20programming%20languages%20into%20the%20model%20parameters%20via%0Afinetuning%20the%20models%20with%20the%20help%20of%20a%20pretrained%20GNN%20expert%20model.%20Various%0Aexperiments%20and%20ablations%20are%20done%20on%20four%20datasets%20including%20both%20the%20C%2B%2B%20and%0Apython%20languages%20to%20validate%20the%20hard%20meta-graph%20prompt%2C%20the%20soft%20prompting%0Atechnique%2C%20and%20the%20effectiveness%20of%20the%20objectives%20for%20pretrained%20GNN%20expert.%0ACodeGRAG%20improves%20the%20code%20generation%20ability%20of%20LLMs%20and%20can%20even%20offer%0Aperformance%20gain%20for%20cross-lingual%20code%20generation.%20Code%20is%20available%20at%0Ahttps%3A//anonymous.4open.science/r/Code-5970/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.02355v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCodeGRAG%253A%2520Bridging%2520the%2520Gap%2520between%2520Natural%2520Language%2520and%2520Programming%250A%2520%2520Language%2520via%2520Graphical%2520Retrieval%2520Augmented%2520Generation%26entry.906535625%3DKounianhua%2520Du%2520and%2520Jizheng%2520Chen%2520and%2520Renting%2520Rui%2520and%2520Huacan%2520Chai%2520and%2520Lingyue%2520Fu%2520and%2520Wei%2520Xia%2520and%2520Yasheng%2520Wang%2520and%2520Ruiming%2520Tang%2520and%2520Yong%2520Yu%2520and%2520Weinan%2520Zhang%26entry.1292438233%3D%2520%2520Utilizing%2520large%2520language%2520models%2520to%2520generate%2520codes%2520has%2520shown%2520promising%2520meaning%250Ain%2520software%2520development%2520revolution.%2520Despite%2520the%2520intelligence%2520shown%2520by%2520the%250Ageneral%2520large%2520language%2520models%252C%2520their%2520specificity%2520in%2520code%2520generation%2520can%2520still%250Abe%2520improved%2520due%2520to%2520the%2520syntactic%2520gap%2520and%2520mismatched%2520vocabulary%2520existing%2520among%250Anatural%2520language%2520and%2520different%2520programming%2520languages.%2520In%2520this%2520paper%252C%2520we%2520propose%250ACodeGRAG%252C%2520a%2520Graphical%2520Retrieval%2520Augmented%2520Code%2520Generation%2520framework%2520to%2520enhance%250Athe%2520performance%2520of%2520LLMs.%2520CodeGRAG%2520builds%2520the%2520graphical%2520view%2520of%2520code%2520blocks%250Abased%2520on%2520the%2520control%2520flow%2520and%2520data%2520flow%2520of%2520them%2520to%2520fill%2520the%2520gap%2520between%250Aprogramming%2520languages%2520and%2520natural%2520language%252C%2520which%2520can%2520facilitate%2520natural%250Alanguage%2520based%2520LLMs%2520for%2520better%2520understanding%2520of%2520code%2520syntax%2520and%2520serve%2520as%2520a%250Abridge%2520among%2520different%2520programming%2520languages.%2520To%2520take%2520the%2520extracted%2520structural%250Aknowledge%2520into%2520the%2520foundation%2520models%252C%2520we%2520propose%25201%2529%2520a%2520hard%2520meta-graph%2520prompt%250Atemplate%2520to%2520transform%2520the%2520challenging%2520graphical%2520representation%2520into%2520informative%250Aknowledge%2520for%2520tuning-free%2520models%2520and%25202%2529%2520a%2520soft%2520prompting%2520technique%2520that%2520injects%250Athe%2520domain%2520knowledge%2520of%2520programming%2520languages%2520into%2520the%2520model%2520parameters%2520via%250Afinetuning%2520the%2520models%2520with%2520the%2520help%2520of%2520a%2520pretrained%2520GNN%2520expert%2520model.%2520Various%250Aexperiments%2520and%2520ablations%2520are%2520done%2520on%2520four%2520datasets%2520including%2520both%2520the%2520C%252B%252B%2520and%250Apython%2520languages%2520to%2520validate%2520the%2520hard%2520meta-graph%2520prompt%252C%2520the%2520soft%2520prompting%250Atechnique%252C%2520and%2520the%2520effectiveness%2520of%2520the%2520objectives%2520for%2520pretrained%2520GNN%2520expert.%250ACodeGRAG%2520improves%2520the%2520code%2520generation%2520ability%2520of%2520LLMs%2520and%2520can%2520even%2520offer%250Aperformance%2520gain%2520for%2520cross-lingual%2520code%2520generation.%2520Code%2520is%2520available%2520at%250Ahttps%253A//anonymous.4open.science/r/Code-5970/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.02355v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CodeGRAG%3A%20Bridging%20the%20Gap%20between%20Natural%20Language%20and%20Programming%0A%20%20Language%20via%20Graphical%20Retrieval%20Augmented%20Generation&entry.906535625=Kounianhua%20Du%20and%20Jizheng%20Chen%20and%20Renting%20Rui%20and%20Huacan%20Chai%20and%20Lingyue%20Fu%20and%20Wei%20Xia%20and%20Yasheng%20Wang%20and%20Ruiming%20Tang%20and%20Yong%20Yu%20and%20Weinan%20Zhang&entry.1292438233=%20%20Utilizing%20large%20language%20models%20to%20generate%20codes%20has%20shown%20promising%20meaning%0Ain%20software%20development%20revolution.%20Despite%20the%20intelligence%20shown%20by%20the%0Ageneral%20large%20language%20models%2C%20their%20specificity%20in%20code%20generation%20can%20still%0Abe%20improved%20due%20to%20the%20syntactic%20gap%20and%20mismatched%20vocabulary%20existing%20among%0Anatural%20language%20and%20different%20programming%20languages.%20In%20this%20paper%2C%20we%20propose%0ACodeGRAG%2C%20a%20Graphical%20Retrieval%20Augmented%20Code%20Generation%20framework%20to%20enhance%0Athe%20performance%20of%20LLMs.%20CodeGRAG%20builds%20the%20graphical%20view%20of%20code%20blocks%0Abased%20on%20the%20control%20flow%20and%20data%20flow%20of%20them%20to%20fill%20the%20gap%20between%0Aprogramming%20languages%20and%20natural%20language%2C%20which%20can%20facilitate%20natural%0Alanguage%20based%20LLMs%20for%20better%20understanding%20of%20code%20syntax%20and%20serve%20as%20a%0Abridge%20among%20different%20programming%20languages.%20To%20take%20the%20extracted%20structural%0Aknowledge%20into%20the%20foundation%20models%2C%20we%20propose%201%29%20a%20hard%20meta-graph%20prompt%0Atemplate%20to%20transform%20the%20challenging%20graphical%20representation%20into%20informative%0Aknowledge%20for%20tuning-free%20models%20and%202%29%20a%20soft%20prompting%20technique%20that%20injects%0Athe%20domain%20knowledge%20of%20programming%20languages%20into%20the%20model%20parameters%20via%0Afinetuning%20the%20models%20with%20the%20help%20of%20a%20pretrained%20GNN%20expert%20model.%20Various%0Aexperiments%20and%20ablations%20are%20done%20on%20four%20datasets%20including%20both%20the%20C%2B%2B%20and%0Apython%20languages%20to%20validate%20the%20hard%20meta-graph%20prompt%2C%20the%20soft%20prompting%0Atechnique%2C%20and%20the%20effectiveness%20of%20the%20objectives%20for%20pretrained%20GNN%20expert.%0ACodeGRAG%20improves%20the%20code%20generation%20ability%20of%20LLMs%20and%20can%20even%20offer%0Aperformance%20gain%20for%20cross-lingual%20code%20generation.%20Code%20is%20available%20at%0Ahttps%3A//anonymous.4open.science/r/Code-5970/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.02355v3&entry.124074799=Read"},
{"title": "Training objective drives the consistency of representational similarity\n  across datasets", "author": "Laure Ciernik and Lorenz Linhardt and Marco Morik and Jonas Dippel and Simon Kornblith and Lukas Muttenthaler", "abstract": "  The Platonic Representation Hypothesis claims that recent foundation models\nare converging to a shared representation space as a function of their\ndownstream task performance, irrespective of the objectives and data modalities\nused to train these models. Representational similarity is generally measured\nfor individual datasets and is not necessarily consistent across datasets.\nThus, one may wonder whether this convergence of model representations is\nconfounded by the datasets commonly used in machine learning. Here, we propose\na systematic way to measure how representational similarity between models\nvaries with the set of stimuli used to construct the representations. We find\nthat the objective function is the most crucial factor in determining the\nconsistency of representational similarities across datasets. Specifically,\nself-supervised vision models learn representations whose relative pairwise\nsimilarities generalize better from one dataset to another compared to those of\nimage classification or image-text models. Moreover, the correspondence between\nrepresentational similarities and the models' task behavior is\ndataset-dependent, being most strongly pronounced for single-domain datasets.\nOur work provides a framework for systematically measuring similarities of\nmodel representations across datasets and linking those similarities to\ndifferences in task behavior.\n", "link": "http://arxiv.org/abs/2411.05561v1", "date": "2024-11-08", "relevancy": 2.6257, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5281}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5281}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20objective%20drives%20the%20consistency%20of%20representational%20similarity%0A%20%20across%20datasets&body=Title%3A%20Training%20objective%20drives%20the%20consistency%20of%20representational%20similarity%0A%20%20across%20datasets%0AAuthor%3A%20Laure%20Ciernik%20and%20Lorenz%20Linhardt%20and%20Marco%20Morik%20and%20Jonas%20Dippel%20and%20Simon%20Kornblith%20and%20Lukas%20Muttenthaler%0AAbstract%3A%20%20%20The%20Platonic%20Representation%20Hypothesis%20claims%20that%20recent%20foundation%20models%0Aare%20converging%20to%20a%20shared%20representation%20space%20as%20a%20function%20of%20their%0Adownstream%20task%20performance%2C%20irrespective%20of%20the%20objectives%20and%20data%20modalities%0Aused%20to%20train%20these%20models.%20Representational%20similarity%20is%20generally%20measured%0Afor%20individual%20datasets%20and%20is%20not%20necessarily%20consistent%20across%20datasets.%0AThus%2C%20one%20may%20wonder%20whether%20this%20convergence%20of%20model%20representations%20is%0Aconfounded%20by%20the%20datasets%20commonly%20used%20in%20machine%20learning.%20Here%2C%20we%20propose%0Aa%20systematic%20way%20to%20measure%20how%20representational%20similarity%20between%20models%0Avaries%20with%20the%20set%20of%20stimuli%20used%20to%20construct%20the%20representations.%20We%20find%0Athat%20the%20objective%20function%20is%20the%20most%20crucial%20factor%20in%20determining%20the%0Aconsistency%20of%20representational%20similarities%20across%20datasets.%20Specifically%2C%0Aself-supervised%20vision%20models%20learn%20representations%20whose%20relative%20pairwise%0Asimilarities%20generalize%20better%20from%20one%20dataset%20to%20another%20compared%20to%20those%20of%0Aimage%20classification%20or%20image-text%20models.%20Moreover%2C%20the%20correspondence%20between%0Arepresentational%20similarities%20and%20the%20models%27%20task%20behavior%20is%0Adataset-dependent%2C%20being%20most%20strongly%20pronounced%20for%20single-domain%20datasets.%0AOur%20work%20provides%20a%20framework%20for%20systematically%20measuring%20similarities%20of%0Amodel%20representations%20across%20datasets%20and%20linking%20those%20similarities%20to%0Adifferences%20in%20task%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520objective%2520drives%2520the%2520consistency%2520of%2520representational%2520similarity%250A%2520%2520across%2520datasets%26entry.906535625%3DLaure%2520Ciernik%2520and%2520Lorenz%2520Linhardt%2520and%2520Marco%2520Morik%2520and%2520Jonas%2520Dippel%2520and%2520Simon%2520Kornblith%2520and%2520Lukas%2520Muttenthaler%26entry.1292438233%3D%2520%2520The%2520Platonic%2520Representation%2520Hypothesis%2520claims%2520that%2520recent%2520foundation%2520models%250Aare%2520converging%2520to%2520a%2520shared%2520representation%2520space%2520as%2520a%2520function%2520of%2520their%250Adownstream%2520task%2520performance%252C%2520irrespective%2520of%2520the%2520objectives%2520and%2520data%2520modalities%250Aused%2520to%2520train%2520these%2520models.%2520Representational%2520similarity%2520is%2520generally%2520measured%250Afor%2520individual%2520datasets%2520and%2520is%2520not%2520necessarily%2520consistent%2520across%2520datasets.%250AThus%252C%2520one%2520may%2520wonder%2520whether%2520this%2520convergence%2520of%2520model%2520representations%2520is%250Aconfounded%2520by%2520the%2520datasets%2520commonly%2520used%2520in%2520machine%2520learning.%2520Here%252C%2520we%2520propose%250Aa%2520systematic%2520way%2520to%2520measure%2520how%2520representational%2520similarity%2520between%2520models%250Avaries%2520with%2520the%2520set%2520of%2520stimuli%2520used%2520to%2520construct%2520the%2520representations.%2520We%2520find%250Athat%2520the%2520objective%2520function%2520is%2520the%2520most%2520crucial%2520factor%2520in%2520determining%2520the%250Aconsistency%2520of%2520representational%2520similarities%2520across%2520datasets.%2520Specifically%252C%250Aself-supervised%2520vision%2520models%2520learn%2520representations%2520whose%2520relative%2520pairwise%250Asimilarities%2520generalize%2520better%2520from%2520one%2520dataset%2520to%2520another%2520compared%2520to%2520those%2520of%250Aimage%2520classification%2520or%2520image-text%2520models.%2520Moreover%252C%2520the%2520correspondence%2520between%250Arepresentational%2520similarities%2520and%2520the%2520models%2527%2520task%2520behavior%2520is%250Adataset-dependent%252C%2520being%2520most%2520strongly%2520pronounced%2520for%2520single-domain%2520datasets.%250AOur%2520work%2520provides%2520a%2520framework%2520for%2520systematically%2520measuring%2520similarities%2520of%250Amodel%2520representations%2520across%2520datasets%2520and%2520linking%2520those%2520similarities%2520to%250Adifferences%2520in%2520task%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20objective%20drives%20the%20consistency%20of%20representational%20similarity%0A%20%20across%20datasets&entry.906535625=Laure%20Ciernik%20and%20Lorenz%20Linhardt%20and%20Marco%20Morik%20and%20Jonas%20Dippel%20and%20Simon%20Kornblith%20and%20Lukas%20Muttenthaler&entry.1292438233=%20%20The%20Platonic%20Representation%20Hypothesis%20claims%20that%20recent%20foundation%20models%0Aare%20converging%20to%20a%20shared%20representation%20space%20as%20a%20function%20of%20their%0Adownstream%20task%20performance%2C%20irrespective%20of%20the%20objectives%20and%20data%20modalities%0Aused%20to%20train%20these%20models.%20Representational%20similarity%20is%20generally%20measured%0Afor%20individual%20datasets%20and%20is%20not%20necessarily%20consistent%20across%20datasets.%0AThus%2C%20one%20may%20wonder%20whether%20this%20convergence%20of%20model%20representations%20is%0Aconfounded%20by%20the%20datasets%20commonly%20used%20in%20machine%20learning.%20Here%2C%20we%20propose%0Aa%20systematic%20way%20to%20measure%20how%20representational%20similarity%20between%20models%0Avaries%20with%20the%20set%20of%20stimuli%20used%20to%20construct%20the%20representations.%20We%20find%0Athat%20the%20objective%20function%20is%20the%20most%20crucial%20factor%20in%20determining%20the%0Aconsistency%20of%20representational%20similarities%20across%20datasets.%20Specifically%2C%0Aself-supervised%20vision%20models%20learn%20representations%20whose%20relative%20pairwise%0Asimilarities%20generalize%20better%20from%20one%20dataset%20to%20another%20compared%20to%20those%20of%0Aimage%20classification%20or%20image-text%20models.%20Moreover%2C%20the%20correspondence%20between%0Arepresentational%20similarities%20and%20the%20models%27%20task%20behavior%20is%0Adataset-dependent%2C%20being%20most%20strongly%20pronounced%20for%20single-domain%20datasets.%0AOur%20work%20provides%20a%20framework%20for%20systematically%20measuring%20similarities%20of%0Amodel%20representations%20across%20datasets%20and%20linking%20those%20similarities%20to%0Adifferences%20in%20task%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05561v1&entry.124074799=Read"},
{"title": "CFPNet: Improving Lightweight ToF Depth Completion via Cross-zone\n  Feature Propagation", "author": "Laiyan Ding and Hualie Jiang and Rui Xu and Rui Huang", "abstract": "  Depth completion using lightweight time-of-flight (ToF) depth sensors is\nattractive due to their low cost. However, lightweight ToF sensors usually have\na limited field of view (FOV) compared with cameras. Thus, only pixels in the\nzone area of the image can be associated with depth signals. Previous methods\nfail to propagate depth features from the zone area to the outside-zone area\neffectively, thus suffering from degraded depth completion performance outside\nthe zone. To this end, this paper proposes the CFPNet to achieve cross-zone\nfeature propagation from the zone area to the outside-zone area with two novel\nmodules. The first is a direct-attention-based propagation module (DAPM), which\nenforces direct cross-zone feature acquisition. The second is a\nlarge-kernel-based propagation module (LKPM), which realizes cross-zone feature\npropagation by utilizing convolution layers with kernel sizes up to 31. CFPNet\nachieves state-of-the-art (SOTA) depth completion performance by combining\nthese two modules properly, as verified by extensive experimental results on\nthe ZJU-L5 dataset. The code will be made public.\n", "link": "http://arxiv.org/abs/2411.04480v2", "date": "2024-11-08", "relevancy": 2.6257, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5288}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5281}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CFPNet%3A%20Improving%20Lightweight%20ToF%20Depth%20Completion%20via%20Cross-zone%0A%20%20Feature%20Propagation&body=Title%3A%20CFPNet%3A%20Improving%20Lightweight%20ToF%20Depth%20Completion%20via%20Cross-zone%0A%20%20Feature%20Propagation%0AAuthor%3A%20Laiyan%20Ding%20and%20Hualie%20Jiang%20and%20Rui%20Xu%20and%20Rui%20Huang%0AAbstract%3A%20%20%20Depth%20completion%20using%20lightweight%20time-of-flight%20%28ToF%29%20depth%20sensors%20is%0Aattractive%20due%20to%20their%20low%20cost.%20However%2C%20lightweight%20ToF%20sensors%20usually%20have%0Aa%20limited%20field%20of%20view%20%28FOV%29%20compared%20with%20cameras.%20Thus%2C%20only%20pixels%20in%20the%0Azone%20area%20of%20the%20image%20can%20be%20associated%20with%20depth%20signals.%20Previous%20methods%0Afail%20to%20propagate%20depth%20features%20from%20the%20zone%20area%20to%20the%20outside-zone%20area%0Aeffectively%2C%20thus%20suffering%20from%20degraded%20depth%20completion%20performance%20outside%0Athe%20zone.%20To%20this%20end%2C%20this%20paper%20proposes%20the%20CFPNet%20to%20achieve%20cross-zone%0Afeature%20propagation%20from%20the%20zone%20area%20to%20the%20outside-zone%20area%20with%20two%20novel%0Amodules.%20The%20first%20is%20a%20direct-attention-based%20propagation%20module%20%28DAPM%29%2C%20which%0Aenforces%20direct%20cross-zone%20feature%20acquisition.%20The%20second%20is%20a%0Alarge-kernel-based%20propagation%20module%20%28LKPM%29%2C%20which%20realizes%20cross-zone%20feature%0Apropagation%20by%20utilizing%20convolution%20layers%20with%20kernel%20sizes%20up%20to%2031.%20CFPNet%0Aachieves%20state-of-the-art%20%28SOTA%29%20depth%20completion%20performance%20by%20combining%0Athese%20two%20modules%20properly%2C%20as%20verified%20by%20extensive%20experimental%20results%20on%0Athe%20ZJU-L5%20dataset.%20The%20code%20will%20be%20made%20public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04480v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCFPNet%253A%2520Improving%2520Lightweight%2520ToF%2520Depth%2520Completion%2520via%2520Cross-zone%250A%2520%2520Feature%2520Propagation%26entry.906535625%3DLaiyan%2520Ding%2520and%2520Hualie%2520Jiang%2520and%2520Rui%2520Xu%2520and%2520Rui%2520Huang%26entry.1292438233%3D%2520%2520Depth%2520completion%2520using%2520lightweight%2520time-of-flight%2520%2528ToF%2529%2520depth%2520sensors%2520is%250Aattractive%2520due%2520to%2520their%2520low%2520cost.%2520However%252C%2520lightweight%2520ToF%2520sensors%2520usually%2520have%250Aa%2520limited%2520field%2520of%2520view%2520%2528FOV%2529%2520compared%2520with%2520cameras.%2520Thus%252C%2520only%2520pixels%2520in%2520the%250Azone%2520area%2520of%2520the%2520image%2520can%2520be%2520associated%2520with%2520depth%2520signals.%2520Previous%2520methods%250Afail%2520to%2520propagate%2520depth%2520features%2520from%2520the%2520zone%2520area%2520to%2520the%2520outside-zone%2520area%250Aeffectively%252C%2520thus%2520suffering%2520from%2520degraded%2520depth%2520completion%2520performance%2520outside%250Athe%2520zone.%2520To%2520this%2520end%252C%2520this%2520paper%2520proposes%2520the%2520CFPNet%2520to%2520achieve%2520cross-zone%250Afeature%2520propagation%2520from%2520the%2520zone%2520area%2520to%2520the%2520outside-zone%2520area%2520with%2520two%2520novel%250Amodules.%2520The%2520first%2520is%2520a%2520direct-attention-based%2520propagation%2520module%2520%2528DAPM%2529%252C%2520which%250Aenforces%2520direct%2520cross-zone%2520feature%2520acquisition.%2520The%2520second%2520is%2520a%250Alarge-kernel-based%2520propagation%2520module%2520%2528LKPM%2529%252C%2520which%2520realizes%2520cross-zone%2520feature%250Apropagation%2520by%2520utilizing%2520convolution%2520layers%2520with%2520kernel%2520sizes%2520up%2520to%252031.%2520CFPNet%250Aachieves%2520state-of-the-art%2520%2528SOTA%2529%2520depth%2520completion%2520performance%2520by%2520combining%250Athese%2520two%2520modules%2520properly%252C%2520as%2520verified%2520by%2520extensive%2520experimental%2520results%2520on%250Athe%2520ZJU-L5%2520dataset.%2520The%2520code%2520will%2520be%2520made%2520public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04480v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CFPNet%3A%20Improving%20Lightweight%20ToF%20Depth%20Completion%20via%20Cross-zone%0A%20%20Feature%20Propagation&entry.906535625=Laiyan%20Ding%20and%20Hualie%20Jiang%20and%20Rui%20Xu%20and%20Rui%20Huang&entry.1292438233=%20%20Depth%20completion%20using%20lightweight%20time-of-flight%20%28ToF%29%20depth%20sensors%20is%0Aattractive%20due%20to%20their%20low%20cost.%20However%2C%20lightweight%20ToF%20sensors%20usually%20have%0Aa%20limited%20field%20of%20view%20%28FOV%29%20compared%20with%20cameras.%20Thus%2C%20only%20pixels%20in%20the%0Azone%20area%20of%20the%20image%20can%20be%20associated%20with%20depth%20signals.%20Previous%20methods%0Afail%20to%20propagate%20depth%20features%20from%20the%20zone%20area%20to%20the%20outside-zone%20area%0Aeffectively%2C%20thus%20suffering%20from%20degraded%20depth%20completion%20performance%20outside%0Athe%20zone.%20To%20this%20end%2C%20this%20paper%20proposes%20the%20CFPNet%20to%20achieve%20cross-zone%0Afeature%20propagation%20from%20the%20zone%20area%20to%20the%20outside-zone%20area%20with%20two%20novel%0Amodules.%20The%20first%20is%20a%20direct-attention-based%20propagation%20module%20%28DAPM%29%2C%20which%0Aenforces%20direct%20cross-zone%20feature%20acquisition.%20The%20second%20is%20a%0Alarge-kernel-based%20propagation%20module%20%28LKPM%29%2C%20which%20realizes%20cross-zone%20feature%0Apropagation%20by%20utilizing%20convolution%20layers%20with%20kernel%20sizes%20up%20to%2031.%20CFPNet%0Aachieves%20state-of-the-art%20%28SOTA%29%20depth%20completion%20performance%20by%20combining%0Athese%20two%20modules%20properly%2C%20as%20verified%20by%20extensive%20experimental%20results%20on%0Athe%20ZJU-L5%20dataset.%20The%20code%20will%20be%20made%20public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04480v2&entry.124074799=Read"},
{"title": "Using Language Models to Disambiguate Lexical Choices in Translation", "author": "Josh Barua and Sanjay Subramanian and Kayo Yin and Alane Suhr", "abstract": "  In translation, a concept represented by a single word in a source language\ncan have multiple variations in a target language. The task of lexical\nselection requires using context to identify which variation is most\nappropriate for a source text. We work with native speakers of nine languages\nto create DTAiLS, a dataset of 1,377 sentence pairs that exhibit cross-lingual\nconcept variation when translating from English. We evaluate recent LLMs and\nneural machine translation systems on DTAiLS, with the best-performing model,\nGPT-4, achieving from 67 to 85% accuracy across languages. Finally, we use\nlanguage models to generate English rules describing target-language concept\nvariations. Providing weaker models with high-quality lexical rules improves\naccuracy substantially, in some cases reaching or outperforming GPT-4.\n", "link": "http://arxiv.org/abs/2411.05781v1", "date": "2024-11-08", "relevancy": 2.5953, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5266}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Language%20Models%20to%20Disambiguate%20Lexical%20Choices%20in%20Translation&body=Title%3A%20Using%20Language%20Models%20to%20Disambiguate%20Lexical%20Choices%20in%20Translation%0AAuthor%3A%20Josh%20Barua%20and%20Sanjay%20Subramanian%20and%20Kayo%20Yin%20and%20Alane%20Suhr%0AAbstract%3A%20%20%20In%20translation%2C%20a%20concept%20represented%20by%20a%20single%20word%20in%20a%20source%20language%0Acan%20have%20multiple%20variations%20in%20a%20target%20language.%20The%20task%20of%20lexical%0Aselection%20requires%20using%20context%20to%20identify%20which%20variation%20is%20most%0Aappropriate%20for%20a%20source%20text.%20We%20work%20with%20native%20speakers%20of%20nine%20languages%0Ato%20create%20DTAiLS%2C%20a%20dataset%20of%201%2C377%20sentence%20pairs%20that%20exhibit%20cross-lingual%0Aconcept%20variation%20when%20translating%20from%20English.%20We%20evaluate%20recent%20LLMs%20and%0Aneural%20machine%20translation%20systems%20on%20DTAiLS%2C%20with%20the%20best-performing%20model%2C%0AGPT-4%2C%20achieving%20from%2067%20to%2085%25%20accuracy%20across%20languages.%20Finally%2C%20we%20use%0Alanguage%20models%20to%20generate%20English%20rules%20describing%20target-language%20concept%0Avariations.%20Providing%20weaker%20models%20with%20high-quality%20lexical%20rules%20improves%0Aaccuracy%20substantially%2C%20in%20some%20cases%20reaching%20or%20outperforming%20GPT-4.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Language%2520Models%2520to%2520Disambiguate%2520Lexical%2520Choices%2520in%2520Translation%26entry.906535625%3DJosh%2520Barua%2520and%2520Sanjay%2520Subramanian%2520and%2520Kayo%2520Yin%2520and%2520Alane%2520Suhr%26entry.1292438233%3D%2520%2520In%2520translation%252C%2520a%2520concept%2520represented%2520by%2520a%2520single%2520word%2520in%2520a%2520source%2520language%250Acan%2520have%2520multiple%2520variations%2520in%2520a%2520target%2520language.%2520The%2520task%2520of%2520lexical%250Aselection%2520requires%2520using%2520context%2520to%2520identify%2520which%2520variation%2520is%2520most%250Aappropriate%2520for%2520a%2520source%2520text.%2520We%2520work%2520with%2520native%2520speakers%2520of%2520nine%2520languages%250Ato%2520create%2520DTAiLS%252C%2520a%2520dataset%2520of%25201%252C377%2520sentence%2520pairs%2520that%2520exhibit%2520cross-lingual%250Aconcept%2520variation%2520when%2520translating%2520from%2520English.%2520We%2520evaluate%2520recent%2520LLMs%2520and%250Aneural%2520machine%2520translation%2520systems%2520on%2520DTAiLS%252C%2520with%2520the%2520best-performing%2520model%252C%250AGPT-4%252C%2520achieving%2520from%252067%2520to%252085%2525%2520accuracy%2520across%2520languages.%2520Finally%252C%2520we%2520use%250Alanguage%2520models%2520to%2520generate%2520English%2520rules%2520describing%2520target-language%2520concept%250Avariations.%2520Providing%2520weaker%2520models%2520with%2520high-quality%2520lexical%2520rules%2520improves%250Aaccuracy%2520substantially%252C%2520in%2520some%2520cases%2520reaching%2520or%2520outperforming%2520GPT-4.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Language%20Models%20to%20Disambiguate%20Lexical%20Choices%20in%20Translation&entry.906535625=Josh%20Barua%20and%20Sanjay%20Subramanian%20and%20Kayo%20Yin%20and%20Alane%20Suhr&entry.1292438233=%20%20In%20translation%2C%20a%20concept%20represented%20by%20a%20single%20word%20in%20a%20source%20language%0Acan%20have%20multiple%20variations%20in%20a%20target%20language.%20The%20task%20of%20lexical%0Aselection%20requires%20using%20context%20to%20identify%20which%20variation%20is%20most%0Aappropriate%20for%20a%20source%20text.%20We%20work%20with%20native%20speakers%20of%20nine%20languages%0Ato%20create%20DTAiLS%2C%20a%20dataset%20of%201%2C377%20sentence%20pairs%20that%20exhibit%20cross-lingual%0Aconcept%20variation%20when%20translating%20from%20English.%20We%20evaluate%20recent%20LLMs%20and%0Aneural%20machine%20translation%20systems%20on%20DTAiLS%2C%20with%20the%20best-performing%20model%2C%0AGPT-4%2C%20achieving%20from%2067%20to%2085%25%20accuracy%20across%20languages.%20Finally%2C%20we%20use%0Alanguage%20models%20to%20generate%20English%20rules%20describing%20target-language%20concept%0Avariations.%20Providing%20weaker%20models%20with%20high-quality%20lexical%20rules%20improves%0Aaccuracy%20substantially%2C%20in%20some%20cases%20reaching%20or%20outperforming%20GPT-4.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05781v1&entry.124074799=Read"},
{"title": "Leveraging Bi-Focal Perspectives and Granular Feature Integration for\n  Accurate Reliable Early Alzheimer's Detection", "author": "Pandiyaraju V and Shravan Venkatraman and Abeshek A and Aravintakshan S A and Pavan Kumar S and Kannan A", "abstract": "  Alzheimer's disease (AD) is the most common form of neurodegeneration, which\nimpacts millions of people each year. Diagnosing and classifying AD accurately\nwith neuroimaging data is an ongoing challenge in the field of medicine.\nTraditional Convolutional Neural Networks (CNNs) are good at capturing\nlow-level information from images, but their capability to extract high-level\nminuscule particles is suboptimal, which is a significant challenge in\ndetecting AD from MRI scans. To overcome this, we propose a novel Granular\nFeature Integration method to combine information extraction at different\nscales combined with an efficient information flow. We also propose a Bi-Focal\nPerspective mechanism to highlight focus on subtle neurofibrillary tangles and\namyloid plaques in MRI scans. Our model yielded an F1-Score of 99.31%, a\nprecision of 99.24%, and a recall of 99.51%, which shows a major improvement in\ncomparison to existing state-of-the-art (SOTA) CNNs.\n", "link": "http://arxiv.org/abs/2407.10921v2", "date": "2024-11-08", "relevancy": 2.5479, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5303}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5011}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Bi-Focal%20Perspectives%20and%20Granular%20Feature%20Integration%20for%0A%20%20Accurate%20Reliable%20Early%20Alzheimer%27s%20Detection&body=Title%3A%20Leveraging%20Bi-Focal%20Perspectives%20and%20Granular%20Feature%20Integration%20for%0A%20%20Accurate%20Reliable%20Early%20Alzheimer%27s%20Detection%0AAuthor%3A%20Pandiyaraju%20V%20and%20Shravan%20Venkatraman%20and%20Abeshek%20A%20and%20Aravintakshan%20S%20A%20and%20Pavan%20Kumar%20S%20and%20Kannan%20A%0AAbstract%3A%20%20%20Alzheimer%27s%20disease%20%28AD%29%20is%20the%20most%20common%20form%20of%20neurodegeneration%2C%20which%0Aimpacts%20millions%20of%20people%20each%20year.%20Diagnosing%20and%20classifying%20AD%20accurately%0Awith%20neuroimaging%20data%20is%20an%20ongoing%20challenge%20in%20the%20field%20of%20medicine.%0ATraditional%20Convolutional%20Neural%20Networks%20%28CNNs%29%20are%20good%20at%20capturing%0Alow-level%20information%20from%20images%2C%20but%20their%20capability%20to%20extract%20high-level%0Aminuscule%20particles%20is%20suboptimal%2C%20which%20is%20a%20significant%20challenge%20in%0Adetecting%20AD%20from%20MRI%20scans.%20To%20overcome%20this%2C%20we%20propose%20a%20novel%20Granular%0AFeature%20Integration%20method%20to%20combine%20information%20extraction%20at%20different%0Ascales%20combined%20with%20an%20efficient%20information%20flow.%20We%20also%20propose%20a%20Bi-Focal%0APerspective%20mechanism%20to%20highlight%20focus%20on%20subtle%20neurofibrillary%20tangles%20and%0Aamyloid%20plaques%20in%20MRI%20scans.%20Our%20model%20yielded%20an%20F1-Score%20of%2099.31%25%2C%20a%0Aprecision%20of%2099.24%25%2C%20and%20a%20recall%20of%2099.51%25%2C%20which%20shows%20a%20major%20improvement%20in%0Acomparison%20to%20existing%20state-of-the-art%20%28SOTA%29%20CNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10921v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Bi-Focal%2520Perspectives%2520and%2520Granular%2520Feature%2520Integration%2520for%250A%2520%2520Accurate%2520Reliable%2520Early%2520Alzheimer%2527s%2520Detection%26entry.906535625%3DPandiyaraju%2520V%2520and%2520Shravan%2520Venkatraman%2520and%2520Abeshek%2520A%2520and%2520Aravintakshan%2520S%2520A%2520and%2520Pavan%2520Kumar%2520S%2520and%2520Kannan%2520A%26entry.1292438233%3D%2520%2520Alzheimer%2527s%2520disease%2520%2528AD%2529%2520is%2520the%2520most%2520common%2520form%2520of%2520neurodegeneration%252C%2520which%250Aimpacts%2520millions%2520of%2520people%2520each%2520year.%2520Diagnosing%2520and%2520classifying%2520AD%2520accurately%250Awith%2520neuroimaging%2520data%2520is%2520an%2520ongoing%2520challenge%2520in%2520the%2520field%2520of%2520medicine.%250ATraditional%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520are%2520good%2520at%2520capturing%250Alow-level%2520information%2520from%2520images%252C%2520but%2520their%2520capability%2520to%2520extract%2520high-level%250Aminuscule%2520particles%2520is%2520suboptimal%252C%2520which%2520is%2520a%2520significant%2520challenge%2520in%250Adetecting%2520AD%2520from%2520MRI%2520scans.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520a%2520novel%2520Granular%250AFeature%2520Integration%2520method%2520to%2520combine%2520information%2520extraction%2520at%2520different%250Ascales%2520combined%2520with%2520an%2520efficient%2520information%2520flow.%2520We%2520also%2520propose%2520a%2520Bi-Focal%250APerspective%2520mechanism%2520to%2520highlight%2520focus%2520on%2520subtle%2520neurofibrillary%2520tangles%2520and%250Aamyloid%2520plaques%2520in%2520MRI%2520scans.%2520Our%2520model%2520yielded%2520an%2520F1-Score%2520of%252099.31%2525%252C%2520a%250Aprecision%2520of%252099.24%2525%252C%2520and%2520a%2520recall%2520of%252099.51%2525%252C%2520which%2520shows%2520a%2520major%2520improvement%2520in%250Acomparison%2520to%2520existing%2520state-of-the-art%2520%2528SOTA%2529%2520CNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10921v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Bi-Focal%20Perspectives%20and%20Granular%20Feature%20Integration%20for%0A%20%20Accurate%20Reliable%20Early%20Alzheimer%27s%20Detection&entry.906535625=Pandiyaraju%20V%20and%20Shravan%20Venkatraman%20and%20Abeshek%20A%20and%20Aravintakshan%20S%20A%20and%20Pavan%20Kumar%20S%20and%20Kannan%20A&entry.1292438233=%20%20Alzheimer%27s%20disease%20%28AD%29%20is%20the%20most%20common%20form%20of%20neurodegeneration%2C%20which%0Aimpacts%20millions%20of%20people%20each%20year.%20Diagnosing%20and%20classifying%20AD%20accurately%0Awith%20neuroimaging%20data%20is%20an%20ongoing%20challenge%20in%20the%20field%20of%20medicine.%0ATraditional%20Convolutional%20Neural%20Networks%20%28CNNs%29%20are%20good%20at%20capturing%0Alow-level%20information%20from%20images%2C%20but%20their%20capability%20to%20extract%20high-level%0Aminuscule%20particles%20is%20suboptimal%2C%20which%20is%20a%20significant%20challenge%20in%0Adetecting%20AD%20from%20MRI%20scans.%20To%20overcome%20this%2C%20we%20propose%20a%20novel%20Granular%0AFeature%20Integration%20method%20to%20combine%20information%20extraction%20at%20different%0Ascales%20combined%20with%20an%20efficient%20information%20flow.%20We%20also%20propose%20a%20Bi-Focal%0APerspective%20mechanism%20to%20highlight%20focus%20on%20subtle%20neurofibrillary%20tangles%20and%0Aamyloid%20plaques%20in%20MRI%20scans.%20Our%20model%20yielded%20an%20F1-Score%20of%2099.31%25%2C%20a%0Aprecision%20of%2099.24%25%2C%20and%20a%20recall%20of%2099.51%25%2C%20which%20shows%20a%20major%20improvement%20in%0Acomparison%20to%20existing%20state-of-the-art%20%28SOTA%29%20CNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10921v2&entry.124074799=Read"},
{"title": "FGGP: Fixed-Rate Gradient-First Gradual Pruning", "author": "Lingkai Zhu and Can Deniz Bezek and Orcun Goksel", "abstract": "  In recent years, the increasing size of deep learning models and their\ngrowing demand for computational resources have drawn significant attention to\nthe practice of pruning neural networks, while aiming to preserve their\naccuracy. In unstructured gradual pruning, which sparsifies a network by\ngradually removing individual network parameters until a targeted network\nsparsity is reached, recent works show that both gradient and weight magnitudes\nshould be considered. In this work, we show that such mechanism, e.g., the\norder of prioritization and selection criteria, is essential. We introduce a\ngradient-first magnitude-next strategy for choosing the parameters to prune,\nand show that a fixed-rate subselection criterion between these steps works\nbetter, in contrast to the annealing approach in the literature. We validate\nthis on CIFAR-10 dataset, with multiple randomized initializations on both\nVGG-19 and ResNet-50 network backbones, for pruning targets of 90, 95, and 98%\nsparsity and for both initially dense and 50% sparse networks. Our proposed\nfixed-rate gradient-first gradual pruning (FGGP) approach outperforms its\nstate-of-the-art alternatives in most of the above experimental settings, even\noccasionally surpassing the upperbound of corresponding dense network results,\nand having the highest ranking across the considered experimental settings.\n", "link": "http://arxiv.org/abs/2411.05500v1", "date": "2024-11-08", "relevancy": 2.5339, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5088}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5082}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FGGP%3A%20Fixed-Rate%20Gradient-First%20Gradual%20Pruning&body=Title%3A%20FGGP%3A%20Fixed-Rate%20Gradient-First%20Gradual%20Pruning%0AAuthor%3A%20Lingkai%20Zhu%20and%20Can%20Deniz%20Bezek%20and%20Orcun%20Goksel%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20increasing%20size%20of%20deep%20learning%20models%20and%20their%0Agrowing%20demand%20for%20computational%20resources%20have%20drawn%20significant%20attention%20to%0Athe%20practice%20of%20pruning%20neural%20networks%2C%20while%20aiming%20to%20preserve%20their%0Aaccuracy.%20In%20unstructured%20gradual%20pruning%2C%20which%20sparsifies%20a%20network%20by%0Agradually%20removing%20individual%20network%20parameters%20until%20a%20targeted%20network%0Asparsity%20is%20reached%2C%20recent%20works%20show%20that%20both%20gradient%20and%20weight%20magnitudes%0Ashould%20be%20considered.%20In%20this%20work%2C%20we%20show%20that%20such%20mechanism%2C%20e.g.%2C%20the%0Aorder%20of%20prioritization%20and%20selection%20criteria%2C%20is%20essential.%20We%20introduce%20a%0Agradient-first%20magnitude-next%20strategy%20for%20choosing%20the%20parameters%20to%20prune%2C%0Aand%20show%20that%20a%20fixed-rate%20subselection%20criterion%20between%20these%20steps%20works%0Abetter%2C%20in%20contrast%20to%20the%20annealing%20approach%20in%20the%20literature.%20We%20validate%0Athis%20on%20CIFAR-10%20dataset%2C%20with%20multiple%20randomized%20initializations%20on%20both%0AVGG-19%20and%20ResNet-50%20network%20backbones%2C%20for%20pruning%20targets%20of%2090%2C%2095%2C%20and%2098%25%0Asparsity%20and%20for%20both%20initially%20dense%20and%2050%25%20sparse%20networks.%20Our%20proposed%0Afixed-rate%20gradient-first%20gradual%20pruning%20%28FGGP%29%20approach%20outperforms%20its%0Astate-of-the-art%20alternatives%20in%20most%20of%20the%20above%20experimental%20settings%2C%20even%0Aoccasionally%20surpassing%20the%20upperbound%20of%20corresponding%20dense%20network%20results%2C%0Aand%20having%20the%20highest%20ranking%20across%20the%20considered%20experimental%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFGGP%253A%2520Fixed-Rate%2520Gradient-First%2520Gradual%2520Pruning%26entry.906535625%3DLingkai%2520Zhu%2520and%2520Can%2520Deniz%2520Bezek%2520and%2520Orcun%2520Goksel%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520increasing%2520size%2520of%2520deep%2520learning%2520models%2520and%2520their%250Agrowing%2520demand%2520for%2520computational%2520resources%2520have%2520drawn%2520significant%2520attention%2520to%250Athe%2520practice%2520of%2520pruning%2520neural%2520networks%252C%2520while%2520aiming%2520to%2520preserve%2520their%250Aaccuracy.%2520In%2520unstructured%2520gradual%2520pruning%252C%2520which%2520sparsifies%2520a%2520network%2520by%250Agradually%2520removing%2520individual%2520network%2520parameters%2520until%2520a%2520targeted%2520network%250Asparsity%2520is%2520reached%252C%2520recent%2520works%2520show%2520that%2520both%2520gradient%2520and%2520weight%2520magnitudes%250Ashould%2520be%2520considered.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520such%2520mechanism%252C%2520e.g.%252C%2520the%250Aorder%2520of%2520prioritization%2520and%2520selection%2520criteria%252C%2520is%2520essential.%2520We%2520introduce%2520a%250Agradient-first%2520magnitude-next%2520strategy%2520for%2520choosing%2520the%2520parameters%2520to%2520prune%252C%250Aand%2520show%2520that%2520a%2520fixed-rate%2520subselection%2520criterion%2520between%2520these%2520steps%2520works%250Abetter%252C%2520in%2520contrast%2520to%2520the%2520annealing%2520approach%2520in%2520the%2520literature.%2520We%2520validate%250Athis%2520on%2520CIFAR-10%2520dataset%252C%2520with%2520multiple%2520randomized%2520initializations%2520on%2520both%250AVGG-19%2520and%2520ResNet-50%2520network%2520backbones%252C%2520for%2520pruning%2520targets%2520of%252090%252C%252095%252C%2520and%252098%2525%250Asparsity%2520and%2520for%2520both%2520initially%2520dense%2520and%252050%2525%2520sparse%2520networks.%2520Our%2520proposed%250Afixed-rate%2520gradient-first%2520gradual%2520pruning%2520%2528FGGP%2529%2520approach%2520outperforms%2520its%250Astate-of-the-art%2520alternatives%2520in%2520most%2520of%2520the%2520above%2520experimental%2520settings%252C%2520even%250Aoccasionally%2520surpassing%2520the%2520upperbound%2520of%2520corresponding%2520dense%2520network%2520results%252C%250Aand%2520having%2520the%2520highest%2520ranking%2520across%2520the%2520considered%2520experimental%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FGGP%3A%20Fixed-Rate%20Gradient-First%20Gradual%20Pruning&entry.906535625=Lingkai%20Zhu%20and%20Can%20Deniz%20Bezek%20and%20Orcun%20Goksel&entry.1292438233=%20%20In%20recent%20years%2C%20the%20increasing%20size%20of%20deep%20learning%20models%20and%20their%0Agrowing%20demand%20for%20computational%20resources%20have%20drawn%20significant%20attention%20to%0Athe%20practice%20of%20pruning%20neural%20networks%2C%20while%20aiming%20to%20preserve%20their%0Aaccuracy.%20In%20unstructured%20gradual%20pruning%2C%20which%20sparsifies%20a%20network%20by%0Agradually%20removing%20individual%20network%20parameters%20until%20a%20targeted%20network%0Asparsity%20is%20reached%2C%20recent%20works%20show%20that%20both%20gradient%20and%20weight%20magnitudes%0Ashould%20be%20considered.%20In%20this%20work%2C%20we%20show%20that%20such%20mechanism%2C%20e.g.%2C%20the%0Aorder%20of%20prioritization%20and%20selection%20criteria%2C%20is%20essential.%20We%20introduce%20a%0Agradient-first%20magnitude-next%20strategy%20for%20choosing%20the%20parameters%20to%20prune%2C%0Aand%20show%20that%20a%20fixed-rate%20subselection%20criterion%20between%20these%20steps%20works%0Abetter%2C%20in%20contrast%20to%20the%20annealing%20approach%20in%20the%20literature.%20We%20validate%0Athis%20on%20CIFAR-10%20dataset%2C%20with%20multiple%20randomized%20initializations%20on%20both%0AVGG-19%20and%20ResNet-50%20network%20backbones%2C%20for%20pruning%20targets%20of%2090%2C%2095%2C%20and%2098%25%0Asparsity%20and%20for%20both%20initially%20dense%20and%2050%25%20sparse%20networks.%20Our%20proposed%0Afixed-rate%20gradient-first%20gradual%20pruning%20%28FGGP%29%20approach%20outperforms%20its%0Astate-of-the-art%20alternatives%20in%20most%20of%20the%20above%20experimental%20settings%2C%20even%0Aoccasionally%20surpassing%20the%20upperbound%20of%20corresponding%20dense%20network%20results%2C%0Aand%20having%20the%20highest%20ranking%20across%20the%20considered%20experimental%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05500v1&entry.124074799=Read"},
{"title": "Contextual Document Embeddings", "author": "John X. Morris and Alexander M. Rush", "abstract": "  Dense document embeddings are central to neural retrieval. The dominant\nparadigm is to train and construct embeddings by running encoders directly on\nindividual documents. In this work, we argue that these embeddings, while\neffective, are implicitly out-of-context for targeted use cases of retrieval,\nand that a contextualized document embedding should take into account both the\ndocument and neighboring documents in context - analogous to contextualized\nword embeddings. We propose two complementary methods for contextualized\ndocument embeddings: first, an alternative contrastive learning objective that\nexplicitly incorporates the document neighbors into the intra-batch contextual\nloss; second, a new contextual architecture that explicitly encodes neighbor\ndocument information into the encoded representation. Results show that both\nmethods achieve better performance than biencoders in several settings, with\ndifferences especially pronounced out-of-domain. We achieve state-of-the-art\nresults on the MTEB benchmark with no hard negative mining, score distillation,\ndataset-specific instructions, intra-GPU example-sharing, or extremely large\nbatch sizes. Our method can be applied to improve performance on any\ncontrastive learning dataset and any biencoder.\n", "link": "http://arxiv.org/abs/2410.02525v4", "date": "2024-11-08", "relevancy": 2.5282, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5211}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5211}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4746}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20Document%20Embeddings&body=Title%3A%20Contextual%20Document%20Embeddings%0AAuthor%3A%20John%20X.%20Morris%20and%20Alexander%20M.%20Rush%0AAbstract%3A%20%20%20Dense%20document%20embeddings%20are%20central%20to%20neural%20retrieval.%20The%20dominant%0Aparadigm%20is%20to%20train%20and%20construct%20embeddings%20by%20running%20encoders%20directly%20on%0Aindividual%20documents.%20In%20this%20work%2C%20we%20argue%20that%20these%20embeddings%2C%20while%0Aeffective%2C%20are%20implicitly%20out-of-context%20for%20targeted%20use%20cases%20of%20retrieval%2C%0Aand%20that%20a%20contextualized%20document%20embedding%20should%20take%20into%20account%20both%20the%0Adocument%20and%20neighboring%20documents%20in%20context%20-%20analogous%20to%20contextualized%0Aword%20embeddings.%20We%20propose%20two%20complementary%20methods%20for%20contextualized%0Adocument%20embeddings%3A%20first%2C%20an%20alternative%20contrastive%20learning%20objective%20that%0Aexplicitly%20incorporates%20the%20document%20neighbors%20into%20the%20intra-batch%20contextual%0Aloss%3B%20second%2C%20a%20new%20contextual%20architecture%20that%20explicitly%20encodes%20neighbor%0Adocument%20information%20into%20the%20encoded%20representation.%20Results%20show%20that%20both%0Amethods%20achieve%20better%20performance%20than%20biencoders%20in%20several%20settings%2C%20with%0Adifferences%20especially%20pronounced%20out-of-domain.%20We%20achieve%20state-of-the-art%0Aresults%20on%20the%20MTEB%20benchmark%20with%20no%20hard%20negative%20mining%2C%20score%20distillation%2C%0Adataset-specific%20instructions%2C%20intra-GPU%20example-sharing%2C%20or%20extremely%20large%0Abatch%20sizes.%20Our%20method%20can%20be%20applied%20to%20improve%20performance%20on%20any%0Acontrastive%20learning%20dataset%20and%20any%20biencoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.02525v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520Document%2520Embeddings%26entry.906535625%3DJohn%2520X.%2520Morris%2520and%2520Alexander%2520M.%2520Rush%26entry.1292438233%3D%2520%2520Dense%2520document%2520embeddings%2520are%2520central%2520to%2520neural%2520retrieval.%2520The%2520dominant%250Aparadigm%2520is%2520to%2520train%2520and%2520construct%2520embeddings%2520by%2520running%2520encoders%2520directly%2520on%250Aindividual%2520documents.%2520In%2520this%2520work%252C%2520we%2520argue%2520that%2520these%2520embeddings%252C%2520while%250Aeffective%252C%2520are%2520implicitly%2520out-of-context%2520for%2520targeted%2520use%2520cases%2520of%2520retrieval%252C%250Aand%2520that%2520a%2520contextualized%2520document%2520embedding%2520should%2520take%2520into%2520account%2520both%2520the%250Adocument%2520and%2520neighboring%2520documents%2520in%2520context%2520-%2520analogous%2520to%2520contextualized%250Aword%2520embeddings.%2520We%2520propose%2520two%2520complementary%2520methods%2520for%2520contextualized%250Adocument%2520embeddings%253A%2520first%252C%2520an%2520alternative%2520contrastive%2520learning%2520objective%2520that%250Aexplicitly%2520incorporates%2520the%2520document%2520neighbors%2520into%2520the%2520intra-batch%2520contextual%250Aloss%253B%2520second%252C%2520a%2520new%2520contextual%2520architecture%2520that%2520explicitly%2520encodes%2520neighbor%250Adocument%2520information%2520into%2520the%2520encoded%2520representation.%2520Results%2520show%2520that%2520both%250Amethods%2520achieve%2520better%2520performance%2520than%2520biencoders%2520in%2520several%2520settings%252C%2520with%250Adifferences%2520especially%2520pronounced%2520out-of-domain.%2520We%2520achieve%2520state-of-the-art%250Aresults%2520on%2520the%2520MTEB%2520benchmark%2520with%2520no%2520hard%2520negative%2520mining%252C%2520score%2520distillation%252C%250Adataset-specific%2520instructions%252C%2520intra-GPU%2520example-sharing%252C%2520or%2520extremely%2520large%250Abatch%2520sizes.%2520Our%2520method%2520can%2520be%2520applied%2520to%2520improve%2520performance%2520on%2520any%250Acontrastive%2520learning%2520dataset%2520and%2520any%2520biencoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.02525v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Document%20Embeddings&entry.906535625=John%20X.%20Morris%20and%20Alexander%20M.%20Rush&entry.1292438233=%20%20Dense%20document%20embeddings%20are%20central%20to%20neural%20retrieval.%20The%20dominant%0Aparadigm%20is%20to%20train%20and%20construct%20embeddings%20by%20running%20encoders%20directly%20on%0Aindividual%20documents.%20In%20this%20work%2C%20we%20argue%20that%20these%20embeddings%2C%20while%0Aeffective%2C%20are%20implicitly%20out-of-context%20for%20targeted%20use%20cases%20of%20retrieval%2C%0Aand%20that%20a%20contextualized%20document%20embedding%20should%20take%20into%20account%20both%20the%0Adocument%20and%20neighboring%20documents%20in%20context%20-%20analogous%20to%20contextualized%0Aword%20embeddings.%20We%20propose%20two%20complementary%20methods%20for%20contextualized%0Adocument%20embeddings%3A%20first%2C%20an%20alternative%20contrastive%20learning%20objective%20that%0Aexplicitly%20incorporates%20the%20document%20neighbors%20into%20the%20intra-batch%20contextual%0Aloss%3B%20second%2C%20a%20new%20contextual%20architecture%20that%20explicitly%20encodes%20neighbor%0Adocument%20information%20into%20the%20encoded%20representation.%20Results%20show%20that%20both%0Amethods%20achieve%20better%20performance%20than%20biencoders%20in%20several%20settings%2C%20with%0Adifferences%20especially%20pronounced%20out-of-domain.%20We%20achieve%20state-of-the-art%0Aresults%20on%20the%20MTEB%20benchmark%20with%20no%20hard%20negative%20mining%2C%20score%20distillation%2C%0Adataset-specific%20instructions%2C%20intra-GPU%20example-sharing%2C%20or%20extremely%20large%0Abatch%20sizes.%20Our%20method%20can%20be%20applied%20to%20improve%20performance%20on%20any%0Acontrastive%20learning%20dataset%20and%20any%20biencoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.02525v4&entry.124074799=Read"},
{"title": "Visual-TCAV: Concept-based Attribution and Saliency Maps for Post-hoc\n  Explainability in Image Classification", "author": "Antonio De Santis and Riccardo Campi and Matteo Bianchi and Marco Brambilla", "abstract": "  Convolutional Neural Networks (CNNs) have seen significant performance\nimprovements in recent years. However, due to their size and complexity, they\nfunction as black-boxes, leading to transparency concerns. State-of-the-art\nsaliency methods generate local explanations that highlight the area in the\ninput image where a class is identified but cannot explain how a concept of\ninterest contributes to the prediction, which is essential for bias mitigation.\nOn the other hand, concept-based methods, such as TCAV (Testing with Concept\nActivation Vectors), provide insights into how sensitive is the network to a\nconcept, but cannot compute its attribution in a specific prediction nor show\nits location within the input image. This paper introduces a novel post-hoc\nexplainability framework, Visual-TCAV, which aims to bridge the gap between\nthese methods by providing both local and global explanations for CNN-based\nimage classification. Visual-TCAV uses Concept Activation Vectors (CAVs) to\ngenerate saliency maps that show where concepts are recognized by the network.\nMoreover, it can estimate the attribution of these concepts to the output of\nany class using a generalization of Integrated Gradients. This framework is\nevaluated on popular CNN architectures, with its validity further confirmed via\nexperiments where ground truth for explanations is known, and a comparison with\nTCAV. Our code will be made available soon.\n", "link": "http://arxiv.org/abs/2411.05698v1", "date": "2024-11-08", "relevancy": 2.5185, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5087}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5025}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual-TCAV%3A%20Concept-based%20Attribution%20and%20Saliency%20Maps%20for%20Post-hoc%0A%20%20Explainability%20in%20Image%20Classification&body=Title%3A%20Visual-TCAV%3A%20Concept-based%20Attribution%20and%20Saliency%20Maps%20for%20Post-hoc%0A%20%20Explainability%20in%20Image%20Classification%0AAuthor%3A%20Antonio%20De%20Santis%20and%20Riccardo%20Campi%20and%20Matteo%20Bianchi%20and%20Marco%20Brambilla%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20seen%20significant%20performance%0Aimprovements%20in%20recent%20years.%20However%2C%20due%20to%20their%20size%20and%20complexity%2C%20they%0Afunction%20as%20black-boxes%2C%20leading%20to%20transparency%20concerns.%20State-of-the-art%0Asaliency%20methods%20generate%20local%20explanations%20that%20highlight%20the%20area%20in%20the%0Ainput%20image%20where%20a%20class%20is%20identified%20but%20cannot%20explain%20how%20a%20concept%20of%0Ainterest%20contributes%20to%20the%20prediction%2C%20which%20is%20essential%20for%20bias%20mitigation.%0AOn%20the%20other%20hand%2C%20concept-based%20methods%2C%20such%20as%20TCAV%20%28Testing%20with%20Concept%0AActivation%20Vectors%29%2C%20provide%20insights%20into%20how%20sensitive%20is%20the%20network%20to%20a%0Aconcept%2C%20but%20cannot%20compute%20its%20attribution%20in%20a%20specific%20prediction%20nor%20show%0Aits%20location%20within%20the%20input%20image.%20This%20paper%20introduces%20a%20novel%20post-hoc%0Aexplainability%20framework%2C%20Visual-TCAV%2C%20which%20aims%20to%20bridge%20the%20gap%20between%0Athese%20methods%20by%20providing%20both%20local%20and%20global%20explanations%20for%20CNN-based%0Aimage%20classification.%20Visual-TCAV%20uses%20Concept%20Activation%20Vectors%20%28CAVs%29%20to%0Agenerate%20saliency%20maps%20that%20show%20where%20concepts%20are%20recognized%20by%20the%20network.%0AMoreover%2C%20it%20can%20estimate%20the%20attribution%20of%20these%20concepts%20to%20the%20output%20of%0Aany%20class%20using%20a%20generalization%20of%20Integrated%20Gradients.%20This%20framework%20is%0Aevaluated%20on%20popular%20CNN%20architectures%2C%20with%20its%20validity%20further%20confirmed%20via%0Aexperiments%20where%20ground%20truth%20for%20explanations%20is%20known%2C%20and%20a%20comparison%20with%0ATCAV.%20Our%20code%20will%20be%20made%20available%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05698v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual-TCAV%253A%2520Concept-based%2520Attribution%2520and%2520Saliency%2520Maps%2520for%2520Post-hoc%250A%2520%2520Explainability%2520in%2520Image%2520Classification%26entry.906535625%3DAntonio%2520De%2520Santis%2520and%2520Riccardo%2520Campi%2520and%2520Matteo%2520Bianchi%2520and%2520Marco%2520Brambilla%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520have%2520seen%2520significant%2520performance%250Aimprovements%2520in%2520recent%2520years.%2520However%252C%2520due%2520to%2520their%2520size%2520and%2520complexity%252C%2520they%250Afunction%2520as%2520black-boxes%252C%2520leading%2520to%2520transparency%2520concerns.%2520State-of-the-art%250Asaliency%2520methods%2520generate%2520local%2520explanations%2520that%2520highlight%2520the%2520area%2520in%2520the%250Ainput%2520image%2520where%2520a%2520class%2520is%2520identified%2520but%2520cannot%2520explain%2520how%2520a%2520concept%2520of%250Ainterest%2520contributes%2520to%2520the%2520prediction%252C%2520which%2520is%2520essential%2520for%2520bias%2520mitigation.%250AOn%2520the%2520other%2520hand%252C%2520concept-based%2520methods%252C%2520such%2520as%2520TCAV%2520%2528Testing%2520with%2520Concept%250AActivation%2520Vectors%2529%252C%2520provide%2520insights%2520into%2520how%2520sensitive%2520is%2520the%2520network%2520to%2520a%250Aconcept%252C%2520but%2520cannot%2520compute%2520its%2520attribution%2520in%2520a%2520specific%2520prediction%2520nor%2520show%250Aits%2520location%2520within%2520the%2520input%2520image.%2520This%2520paper%2520introduces%2520a%2520novel%2520post-hoc%250Aexplainability%2520framework%252C%2520Visual-TCAV%252C%2520which%2520aims%2520to%2520bridge%2520the%2520gap%2520between%250Athese%2520methods%2520by%2520providing%2520both%2520local%2520and%2520global%2520explanations%2520for%2520CNN-based%250Aimage%2520classification.%2520Visual-TCAV%2520uses%2520Concept%2520Activation%2520Vectors%2520%2528CAVs%2529%2520to%250Agenerate%2520saliency%2520maps%2520that%2520show%2520where%2520concepts%2520are%2520recognized%2520by%2520the%2520network.%250AMoreover%252C%2520it%2520can%2520estimate%2520the%2520attribution%2520of%2520these%2520concepts%2520to%2520the%2520output%2520of%250Aany%2520class%2520using%2520a%2520generalization%2520of%2520Integrated%2520Gradients.%2520This%2520framework%2520is%250Aevaluated%2520on%2520popular%2520CNN%2520architectures%252C%2520with%2520its%2520validity%2520further%2520confirmed%2520via%250Aexperiments%2520where%2520ground%2520truth%2520for%2520explanations%2520is%2520known%252C%2520and%2520a%2520comparison%2520with%250ATCAV.%2520Our%2520code%2520will%2520be%2520made%2520available%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05698v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual-TCAV%3A%20Concept-based%20Attribution%20and%20Saliency%20Maps%20for%20Post-hoc%0A%20%20Explainability%20in%20Image%20Classification&entry.906535625=Antonio%20De%20Santis%20and%20Riccardo%20Campi%20and%20Matteo%20Bianchi%20and%20Marco%20Brambilla&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20seen%20significant%20performance%0Aimprovements%20in%20recent%20years.%20However%2C%20due%20to%20their%20size%20and%20complexity%2C%20they%0Afunction%20as%20black-boxes%2C%20leading%20to%20transparency%20concerns.%20State-of-the-art%0Asaliency%20methods%20generate%20local%20explanations%20that%20highlight%20the%20area%20in%20the%0Ainput%20image%20where%20a%20class%20is%20identified%20but%20cannot%20explain%20how%20a%20concept%20of%0Ainterest%20contributes%20to%20the%20prediction%2C%20which%20is%20essential%20for%20bias%20mitigation.%0AOn%20the%20other%20hand%2C%20concept-based%20methods%2C%20such%20as%20TCAV%20%28Testing%20with%20Concept%0AActivation%20Vectors%29%2C%20provide%20insights%20into%20how%20sensitive%20is%20the%20network%20to%20a%0Aconcept%2C%20but%20cannot%20compute%20its%20attribution%20in%20a%20specific%20prediction%20nor%20show%0Aits%20location%20within%20the%20input%20image.%20This%20paper%20introduces%20a%20novel%20post-hoc%0Aexplainability%20framework%2C%20Visual-TCAV%2C%20which%20aims%20to%20bridge%20the%20gap%20between%0Athese%20methods%20by%20providing%20both%20local%20and%20global%20explanations%20for%20CNN-based%0Aimage%20classification.%20Visual-TCAV%20uses%20Concept%20Activation%20Vectors%20%28CAVs%29%20to%0Agenerate%20saliency%20maps%20that%20show%20where%20concepts%20are%20recognized%20by%20the%20network.%0AMoreover%2C%20it%20can%20estimate%20the%20attribution%20of%20these%20concepts%20to%20the%20output%20of%0Aany%20class%20using%20a%20generalization%20of%20Integrated%20Gradients.%20This%20framework%20is%0Aevaluated%20on%20popular%20CNN%20architectures%2C%20with%20its%20validity%20further%20confirmed%20via%0Aexperiments%20where%20ground%20truth%20for%20explanations%20is%20known%2C%20and%20a%20comparison%20with%0ATCAV.%20Our%20code%20will%20be%20made%20available%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05698v1&entry.124074799=Read"},
{"title": "Alignment of 3D woodblock geometrical models and 2D orthographic\n  projection image", "author": "Minh DUc Nguyen and Cong Thuong Le and Trong Lam Nguyen", "abstract": "  The accurate alignment of 3D woodblock geometrical models with 2D\northographic projection images presents a significant challenge in the digital\npreservation of Vietnamese cultural heritage. This paper proposes a unified\nimage processing algorithm to address this issue, enhancing the registration\nquality between 3D woodblock models and their 2D representations. The method\nincludes determining the plane of the 3D character model, establishing a\ntransformation matrix to align this plane with the 2D printed image plane, and\ncreating a parallel-projected depth map for precise alignment. This process\nminimizes disocclusions and ensures that character shapes and strokes are\ncorrectly positioned. Experimental results highlight the importance of\nstructure-based comparisons to optimize alignment for large-scale Han-Nom\ncharacter datasets. The proposed approach, combining density-based and\nstructure-based methods, demonstrates improved registration performance,\noffering an effective normalization scheme for digital heritage preservation.\n", "link": "http://arxiv.org/abs/2411.05524v1", "date": "2024-11-08", "relevancy": 2.5134, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5193}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4944}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alignment%20of%203D%20woodblock%20geometrical%20models%20and%202D%20orthographic%0A%20%20projection%20image&body=Title%3A%20Alignment%20of%203D%20woodblock%20geometrical%20models%20and%202D%20orthographic%0A%20%20projection%20image%0AAuthor%3A%20Minh%20DUc%20Nguyen%20and%20Cong%20Thuong%20Le%20and%20Trong%20Lam%20Nguyen%0AAbstract%3A%20%20%20The%20accurate%20alignment%20of%203D%20woodblock%20geometrical%20models%20with%202D%0Aorthographic%20projection%20images%20presents%20a%20significant%20challenge%20in%20the%20digital%0Apreservation%20of%20Vietnamese%20cultural%20heritage.%20This%20paper%20proposes%20a%20unified%0Aimage%20processing%20algorithm%20to%20address%20this%20issue%2C%20enhancing%20the%20registration%0Aquality%20between%203D%20woodblock%20models%20and%20their%202D%20representations.%20The%20method%0Aincludes%20determining%20the%20plane%20of%20the%203D%20character%20model%2C%20establishing%20a%0Atransformation%20matrix%20to%20align%20this%20plane%20with%20the%202D%20printed%20image%20plane%2C%20and%0Acreating%20a%20parallel-projected%20depth%20map%20for%20precise%20alignment.%20This%20process%0Aminimizes%20disocclusions%20and%20ensures%20that%20character%20shapes%20and%20strokes%20are%0Acorrectly%20positioned.%20Experimental%20results%20highlight%20the%20importance%20of%0Astructure-based%20comparisons%20to%20optimize%20alignment%20for%20large-scale%20Han-Nom%0Acharacter%20datasets.%20The%20proposed%20approach%2C%20combining%20density-based%20and%0Astructure-based%20methods%2C%20demonstrates%20improved%20registration%20performance%2C%0Aoffering%20an%20effective%20normalization%20scheme%20for%20digital%20heritage%20preservation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignment%2520of%25203D%2520woodblock%2520geometrical%2520models%2520and%25202D%2520orthographic%250A%2520%2520projection%2520image%26entry.906535625%3DMinh%2520DUc%2520Nguyen%2520and%2520Cong%2520Thuong%2520Le%2520and%2520Trong%2520Lam%2520Nguyen%26entry.1292438233%3D%2520%2520The%2520accurate%2520alignment%2520of%25203D%2520woodblock%2520geometrical%2520models%2520with%25202D%250Aorthographic%2520projection%2520images%2520presents%2520a%2520significant%2520challenge%2520in%2520the%2520digital%250Apreservation%2520of%2520Vietnamese%2520cultural%2520heritage.%2520This%2520paper%2520proposes%2520a%2520unified%250Aimage%2520processing%2520algorithm%2520to%2520address%2520this%2520issue%252C%2520enhancing%2520the%2520registration%250Aquality%2520between%25203D%2520woodblock%2520models%2520and%2520their%25202D%2520representations.%2520The%2520method%250Aincludes%2520determining%2520the%2520plane%2520of%2520the%25203D%2520character%2520model%252C%2520establishing%2520a%250Atransformation%2520matrix%2520to%2520align%2520this%2520plane%2520with%2520the%25202D%2520printed%2520image%2520plane%252C%2520and%250Acreating%2520a%2520parallel-projected%2520depth%2520map%2520for%2520precise%2520alignment.%2520This%2520process%250Aminimizes%2520disocclusions%2520and%2520ensures%2520that%2520character%2520shapes%2520and%2520strokes%2520are%250Acorrectly%2520positioned.%2520Experimental%2520results%2520highlight%2520the%2520importance%2520of%250Astructure-based%2520comparisons%2520to%2520optimize%2520alignment%2520for%2520large-scale%2520Han-Nom%250Acharacter%2520datasets.%2520The%2520proposed%2520approach%252C%2520combining%2520density-based%2520and%250Astructure-based%2520methods%252C%2520demonstrates%2520improved%2520registration%2520performance%252C%250Aoffering%2520an%2520effective%2520normalization%2520scheme%2520for%2520digital%2520heritage%2520preservation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alignment%20of%203D%20woodblock%20geometrical%20models%20and%202D%20orthographic%0A%20%20projection%20image&entry.906535625=Minh%20DUc%20Nguyen%20and%20Cong%20Thuong%20Le%20and%20Trong%20Lam%20Nguyen&entry.1292438233=%20%20The%20accurate%20alignment%20of%203D%20woodblock%20geometrical%20models%20with%202D%0Aorthographic%20projection%20images%20presents%20a%20significant%20challenge%20in%20the%20digital%0Apreservation%20of%20Vietnamese%20cultural%20heritage.%20This%20paper%20proposes%20a%20unified%0Aimage%20processing%20algorithm%20to%20address%20this%20issue%2C%20enhancing%20the%20registration%0Aquality%20between%203D%20woodblock%20models%20and%20their%202D%20representations.%20The%20method%0Aincludes%20determining%20the%20plane%20of%20the%203D%20character%20model%2C%20establishing%20a%0Atransformation%20matrix%20to%20align%20this%20plane%20with%20the%202D%20printed%20image%20plane%2C%20and%0Acreating%20a%20parallel-projected%20depth%20map%20for%20precise%20alignment.%20This%20process%0Aminimizes%20disocclusions%20and%20ensures%20that%20character%20shapes%20and%20strokes%20are%0Acorrectly%20positioned.%20Experimental%20results%20highlight%20the%20importance%20of%0Astructure-based%20comparisons%20to%20optimize%20alignment%20for%20large-scale%20Han-Nom%0Acharacter%20datasets.%20The%20proposed%20approach%2C%20combining%20density-based%20and%0Astructure-based%20methods%2C%20demonstrates%20improved%20registration%20performance%2C%0Aoffering%20an%20effective%20normalization%20scheme%20for%20digital%20heritage%20preservation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05524v1&entry.124074799=Read"},
{"title": "Triple Component Matrix Factorization: Untangling Global, Local, and\n  Noisy Components", "author": "Naichen Shi and Salar Fattahi and Raed Al Kontar", "abstract": "  In this work, we study the problem of common and unique feature extraction\nfrom noisy data. When we have N observation matrices from N different and\nassociated sources corrupted by sparse and potentially gross noise, can we\nrecover the common and unique components from these noisy observations? This is\na challenging task as the number of parameters to estimate is approximately\nthrice the number of observations. Despite the difficulty, we propose an\nintuitive alternating minimization algorithm called triple component matrix\nfactorization (TCMF) to recover the three components exactly. TCMF is\ndistinguished from existing works in literature thanks to two salient features.\nFirst, TCMF is a principled method to separate the three components given noisy\nobservations provably. Second, the bulk of the computation in TCMF can be\ndistributed. On the technical side, we formulate the problem as a constrained\nnonconvex nonsmooth optimization problem. Despite the intricate nature of the\nproblem, we provide a Taylor series characterization of its solution by solving\nthe corresponding Karush-Kuhn-Tucker conditions. Using this characterization,\nwe can show that the alternating minimization algorithm makes significant\nprogress at each iteration and converges into the ground truth at a linear\nrate. Numerical experiments in video segmentation and anomaly detection\nhighlight the superior feature extraction abilities of TCMF.\n", "link": "http://arxiv.org/abs/2404.07955v2", "date": "2024-11-08", "relevancy": 2.506, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5048}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5007}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Triple%20Component%20Matrix%20Factorization%3A%20Untangling%20Global%2C%20Local%2C%20and%0A%20%20Noisy%20Components&body=Title%3A%20Triple%20Component%20Matrix%20Factorization%3A%20Untangling%20Global%2C%20Local%2C%20and%0A%20%20Noisy%20Components%0AAuthor%3A%20Naichen%20Shi%20and%20Salar%20Fattahi%20and%20Raed%20Al%20Kontar%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20study%20the%20problem%20of%20common%20and%20unique%20feature%20extraction%0Afrom%20noisy%20data.%20When%20we%20have%20N%20observation%20matrices%20from%20N%20different%20and%0Aassociated%20sources%20corrupted%20by%20sparse%20and%20potentially%20gross%20noise%2C%20can%20we%0Arecover%20the%20common%20and%20unique%20components%20from%20these%20noisy%20observations%3F%20This%20is%0Aa%20challenging%20task%20as%20the%20number%20of%20parameters%20to%20estimate%20is%20approximately%0Athrice%20the%20number%20of%20observations.%20Despite%20the%20difficulty%2C%20we%20propose%20an%0Aintuitive%20alternating%20minimization%20algorithm%20called%20triple%20component%20matrix%0Afactorization%20%28TCMF%29%20to%20recover%20the%20three%20components%20exactly.%20TCMF%20is%0Adistinguished%20from%20existing%20works%20in%20literature%20thanks%20to%20two%20salient%20features.%0AFirst%2C%20TCMF%20is%20a%20principled%20method%20to%20separate%20the%20three%20components%20given%20noisy%0Aobservations%20provably.%20Second%2C%20the%20bulk%20of%20the%20computation%20in%20TCMF%20can%20be%0Adistributed.%20On%20the%20technical%20side%2C%20we%20formulate%20the%20problem%20as%20a%20constrained%0Anonconvex%20nonsmooth%20optimization%20problem.%20Despite%20the%20intricate%20nature%20of%20the%0Aproblem%2C%20we%20provide%20a%20Taylor%20series%20characterization%20of%20its%20solution%20by%20solving%0Athe%20corresponding%20Karush-Kuhn-Tucker%20conditions.%20Using%20this%20characterization%2C%0Awe%20can%20show%20that%20the%20alternating%20minimization%20algorithm%20makes%20significant%0Aprogress%20at%20each%20iteration%20and%20converges%20into%20the%20ground%20truth%20at%20a%20linear%0Arate.%20Numerical%20experiments%20in%20video%20segmentation%20and%20anomaly%20detection%0Ahighlight%20the%20superior%20feature%20extraction%20abilities%20of%20TCMF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07955v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTriple%2520Component%2520Matrix%2520Factorization%253A%2520Untangling%2520Global%252C%2520Local%252C%2520and%250A%2520%2520Noisy%2520Components%26entry.906535625%3DNaichen%2520Shi%2520and%2520Salar%2520Fattahi%2520and%2520Raed%2520Al%2520Kontar%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520study%2520the%2520problem%2520of%2520common%2520and%2520unique%2520feature%2520extraction%250Afrom%2520noisy%2520data.%2520When%2520we%2520have%2520N%2520observation%2520matrices%2520from%2520N%2520different%2520and%250Aassociated%2520sources%2520corrupted%2520by%2520sparse%2520and%2520potentially%2520gross%2520noise%252C%2520can%2520we%250Arecover%2520the%2520common%2520and%2520unique%2520components%2520from%2520these%2520noisy%2520observations%253F%2520This%2520is%250Aa%2520challenging%2520task%2520as%2520the%2520number%2520of%2520parameters%2520to%2520estimate%2520is%2520approximately%250Athrice%2520the%2520number%2520of%2520observations.%2520Despite%2520the%2520difficulty%252C%2520we%2520propose%2520an%250Aintuitive%2520alternating%2520minimization%2520algorithm%2520called%2520triple%2520component%2520matrix%250Afactorization%2520%2528TCMF%2529%2520to%2520recover%2520the%2520three%2520components%2520exactly.%2520TCMF%2520is%250Adistinguished%2520from%2520existing%2520works%2520in%2520literature%2520thanks%2520to%2520two%2520salient%2520features.%250AFirst%252C%2520TCMF%2520is%2520a%2520principled%2520method%2520to%2520separate%2520the%2520three%2520components%2520given%2520noisy%250Aobservations%2520provably.%2520Second%252C%2520the%2520bulk%2520of%2520the%2520computation%2520in%2520TCMF%2520can%2520be%250Adistributed.%2520On%2520the%2520technical%2520side%252C%2520we%2520formulate%2520the%2520problem%2520as%2520a%2520constrained%250Anonconvex%2520nonsmooth%2520optimization%2520problem.%2520Despite%2520the%2520intricate%2520nature%2520of%2520the%250Aproblem%252C%2520we%2520provide%2520a%2520Taylor%2520series%2520characterization%2520of%2520its%2520solution%2520by%2520solving%250Athe%2520corresponding%2520Karush-Kuhn-Tucker%2520conditions.%2520Using%2520this%2520characterization%252C%250Awe%2520can%2520show%2520that%2520the%2520alternating%2520minimization%2520algorithm%2520makes%2520significant%250Aprogress%2520at%2520each%2520iteration%2520and%2520converges%2520into%2520the%2520ground%2520truth%2520at%2520a%2520linear%250Arate.%2520Numerical%2520experiments%2520in%2520video%2520segmentation%2520and%2520anomaly%2520detection%250Ahighlight%2520the%2520superior%2520feature%2520extraction%2520abilities%2520of%2520TCMF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.07955v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Triple%20Component%20Matrix%20Factorization%3A%20Untangling%20Global%2C%20Local%2C%20and%0A%20%20Noisy%20Components&entry.906535625=Naichen%20Shi%20and%20Salar%20Fattahi%20and%20Raed%20Al%20Kontar&entry.1292438233=%20%20In%20this%20work%2C%20we%20study%20the%20problem%20of%20common%20and%20unique%20feature%20extraction%0Afrom%20noisy%20data.%20When%20we%20have%20N%20observation%20matrices%20from%20N%20different%20and%0Aassociated%20sources%20corrupted%20by%20sparse%20and%20potentially%20gross%20noise%2C%20can%20we%0Arecover%20the%20common%20and%20unique%20components%20from%20these%20noisy%20observations%3F%20This%20is%0Aa%20challenging%20task%20as%20the%20number%20of%20parameters%20to%20estimate%20is%20approximately%0Athrice%20the%20number%20of%20observations.%20Despite%20the%20difficulty%2C%20we%20propose%20an%0Aintuitive%20alternating%20minimization%20algorithm%20called%20triple%20component%20matrix%0Afactorization%20%28TCMF%29%20to%20recover%20the%20three%20components%20exactly.%20TCMF%20is%0Adistinguished%20from%20existing%20works%20in%20literature%20thanks%20to%20two%20salient%20features.%0AFirst%2C%20TCMF%20is%20a%20principled%20method%20to%20separate%20the%20three%20components%20given%20noisy%0Aobservations%20provably.%20Second%2C%20the%20bulk%20of%20the%20computation%20in%20TCMF%20can%20be%0Adistributed.%20On%20the%20technical%20side%2C%20we%20formulate%20the%20problem%20as%20a%20constrained%0Anonconvex%20nonsmooth%20optimization%20problem.%20Despite%20the%20intricate%20nature%20of%20the%0Aproblem%2C%20we%20provide%20a%20Taylor%20series%20characterization%20of%20its%20solution%20by%20solving%0Athe%20corresponding%20Karush-Kuhn-Tucker%20conditions.%20Using%20this%20characterization%2C%0Awe%20can%20show%20that%20the%20alternating%20minimization%20algorithm%20makes%20significant%0Aprogress%20at%20each%20iteration%20and%20converges%20into%20the%20ground%20truth%20at%20a%20linear%0Arate.%20Numerical%20experiments%20in%20video%20segmentation%20and%20anomaly%20detection%0Ahighlight%20the%20superior%20feature%20extraction%20abilities%20of%20TCMF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07955v2&entry.124074799=Read"},
{"title": "Network EM Algorithm for Gaussian Mixture Model in Decentralized\n  Federated Learning", "author": "Shuyuan Wu and Bin Du and Xuetong Li and Hansheng Wang", "abstract": "  We systematically study various network Expectation-Maximization (EM)\nalgorithms for the Gaussian mixture model within the framework of decentralized\nfederated learning. Our theoretical investigation reveals that directly\nextending the classical decentralized supervised learning method to the EM\nalgorithm exhibits poor estimation accuracy with heterogeneous data across\nclients and struggles to converge numerically when Gaussian components are\npoorly-separated. To address these issues, we propose two novel solutions.\nFirst, to handle heterogeneous data, we introduce a momentum network EM (MNEM)\nalgorithm, which uses a momentum parameter to combine information from both the\ncurrent and historical estimators. Second, to tackle the challenge of\npoorly-separated Gaussian components, we develop a semi-supervised MNEM\n(semi-MNEM) algorithm, which leverages partially labeled data. Rigorous\ntheoretical analysis demonstrates that MNEM can achieve statistical efficiency\ncomparable to that of the whole sample estimator when the mixture components\nsatisfy certain separation conditions, even in heterogeneous scenarios.\nMoreover, the semi-MNEM estimator enhances the convergence speed of the MNEM\nalgorithm, effectively addressing the numerical convergence challenges in\npoorly-separated scenarios. Extensive simulation and real data analyses are\nconducted to justify our theoretical findings.\n", "link": "http://arxiv.org/abs/2411.05591v1", "date": "2024-11-08", "relevancy": 2.4785, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5057}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4917}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Network%20EM%20Algorithm%20for%20Gaussian%20Mixture%20Model%20in%20Decentralized%0A%20%20Federated%20Learning&body=Title%3A%20Network%20EM%20Algorithm%20for%20Gaussian%20Mixture%20Model%20in%20Decentralized%0A%20%20Federated%20Learning%0AAuthor%3A%20Shuyuan%20Wu%20and%20Bin%20Du%20and%20Xuetong%20Li%20and%20Hansheng%20Wang%0AAbstract%3A%20%20%20We%20systematically%20study%20various%20network%20Expectation-Maximization%20%28EM%29%0Aalgorithms%20for%20the%20Gaussian%20mixture%20model%20within%20the%20framework%20of%20decentralized%0Afederated%20learning.%20Our%20theoretical%20investigation%20reveals%20that%20directly%0Aextending%20the%20classical%20decentralized%20supervised%20learning%20method%20to%20the%20EM%0Aalgorithm%20exhibits%20poor%20estimation%20accuracy%20with%20heterogeneous%20data%20across%0Aclients%20and%20struggles%20to%20converge%20numerically%20when%20Gaussian%20components%20are%0Apoorly-separated.%20To%20address%20these%20issues%2C%20we%20propose%20two%20novel%20solutions.%0AFirst%2C%20to%20handle%20heterogeneous%20data%2C%20we%20introduce%20a%20momentum%20network%20EM%20%28MNEM%29%0Aalgorithm%2C%20which%20uses%20a%20momentum%20parameter%20to%20combine%20information%20from%20both%20the%0Acurrent%20and%20historical%20estimators.%20Second%2C%20to%20tackle%20the%20challenge%20of%0Apoorly-separated%20Gaussian%20components%2C%20we%20develop%20a%20semi-supervised%20MNEM%0A%28semi-MNEM%29%20algorithm%2C%20which%20leverages%20partially%20labeled%20data.%20Rigorous%0Atheoretical%20analysis%20demonstrates%20that%20MNEM%20can%20achieve%20statistical%20efficiency%0Acomparable%20to%20that%20of%20the%20whole%20sample%20estimator%20when%20the%20mixture%20components%0Asatisfy%20certain%20separation%20conditions%2C%20even%20in%20heterogeneous%20scenarios.%0AMoreover%2C%20the%20semi-MNEM%20estimator%20enhances%20the%20convergence%20speed%20of%20the%20MNEM%0Aalgorithm%2C%20effectively%20addressing%20the%20numerical%20convergence%20challenges%20in%0Apoorly-separated%20scenarios.%20Extensive%20simulation%20and%20real%20data%20analyses%20are%0Aconducted%20to%20justify%20our%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05591v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNetwork%2520EM%2520Algorithm%2520for%2520Gaussian%2520Mixture%2520Model%2520in%2520Decentralized%250A%2520%2520Federated%2520Learning%26entry.906535625%3DShuyuan%2520Wu%2520and%2520Bin%2520Du%2520and%2520Xuetong%2520Li%2520and%2520Hansheng%2520Wang%26entry.1292438233%3D%2520%2520We%2520systematically%2520study%2520various%2520network%2520Expectation-Maximization%2520%2528EM%2529%250Aalgorithms%2520for%2520the%2520Gaussian%2520mixture%2520model%2520within%2520the%2520framework%2520of%2520decentralized%250Afederated%2520learning.%2520Our%2520theoretical%2520investigation%2520reveals%2520that%2520directly%250Aextending%2520the%2520classical%2520decentralized%2520supervised%2520learning%2520method%2520to%2520the%2520EM%250Aalgorithm%2520exhibits%2520poor%2520estimation%2520accuracy%2520with%2520heterogeneous%2520data%2520across%250Aclients%2520and%2520struggles%2520to%2520converge%2520numerically%2520when%2520Gaussian%2520components%2520are%250Apoorly-separated.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520two%2520novel%2520solutions.%250AFirst%252C%2520to%2520handle%2520heterogeneous%2520data%252C%2520we%2520introduce%2520a%2520momentum%2520network%2520EM%2520%2528MNEM%2529%250Aalgorithm%252C%2520which%2520uses%2520a%2520momentum%2520parameter%2520to%2520combine%2520information%2520from%2520both%2520the%250Acurrent%2520and%2520historical%2520estimators.%2520Second%252C%2520to%2520tackle%2520the%2520challenge%2520of%250Apoorly-separated%2520Gaussian%2520components%252C%2520we%2520develop%2520a%2520semi-supervised%2520MNEM%250A%2528semi-MNEM%2529%2520algorithm%252C%2520which%2520leverages%2520partially%2520labeled%2520data.%2520Rigorous%250Atheoretical%2520analysis%2520demonstrates%2520that%2520MNEM%2520can%2520achieve%2520statistical%2520efficiency%250Acomparable%2520to%2520that%2520of%2520the%2520whole%2520sample%2520estimator%2520when%2520the%2520mixture%2520components%250Asatisfy%2520certain%2520separation%2520conditions%252C%2520even%2520in%2520heterogeneous%2520scenarios.%250AMoreover%252C%2520the%2520semi-MNEM%2520estimator%2520enhances%2520the%2520convergence%2520speed%2520of%2520the%2520MNEM%250Aalgorithm%252C%2520effectively%2520addressing%2520the%2520numerical%2520convergence%2520challenges%2520in%250Apoorly-separated%2520scenarios.%2520Extensive%2520simulation%2520and%2520real%2520data%2520analyses%2520are%250Aconducted%2520to%2520justify%2520our%2520theoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05591v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Network%20EM%20Algorithm%20for%20Gaussian%20Mixture%20Model%20in%20Decentralized%0A%20%20Federated%20Learning&entry.906535625=Shuyuan%20Wu%20and%20Bin%20Du%20and%20Xuetong%20Li%20and%20Hansheng%20Wang&entry.1292438233=%20%20We%20systematically%20study%20various%20network%20Expectation-Maximization%20%28EM%29%0Aalgorithms%20for%20the%20Gaussian%20mixture%20model%20within%20the%20framework%20of%20decentralized%0Afederated%20learning.%20Our%20theoretical%20investigation%20reveals%20that%20directly%0Aextending%20the%20classical%20decentralized%20supervised%20learning%20method%20to%20the%20EM%0Aalgorithm%20exhibits%20poor%20estimation%20accuracy%20with%20heterogeneous%20data%20across%0Aclients%20and%20struggles%20to%20converge%20numerically%20when%20Gaussian%20components%20are%0Apoorly-separated.%20To%20address%20these%20issues%2C%20we%20propose%20two%20novel%20solutions.%0AFirst%2C%20to%20handle%20heterogeneous%20data%2C%20we%20introduce%20a%20momentum%20network%20EM%20%28MNEM%29%0Aalgorithm%2C%20which%20uses%20a%20momentum%20parameter%20to%20combine%20information%20from%20both%20the%0Acurrent%20and%20historical%20estimators.%20Second%2C%20to%20tackle%20the%20challenge%20of%0Apoorly-separated%20Gaussian%20components%2C%20we%20develop%20a%20semi-supervised%20MNEM%0A%28semi-MNEM%29%20algorithm%2C%20which%20leverages%20partially%20labeled%20data.%20Rigorous%0Atheoretical%20analysis%20demonstrates%20that%20MNEM%20can%20achieve%20statistical%20efficiency%0Acomparable%20to%20that%20of%20the%20whole%20sample%20estimator%20when%20the%20mixture%20components%0Asatisfy%20certain%20separation%20conditions%2C%20even%20in%20heterogeneous%20scenarios.%0AMoreover%2C%20the%20semi-MNEM%20estimator%20enhances%20the%20convergence%20speed%20of%20the%20MNEM%0Aalgorithm%2C%20effectively%20addressing%20the%20numerical%20convergence%20challenges%20in%0Apoorly-separated%20scenarios.%20Extensive%20simulation%20and%20real%20data%20analyses%20are%0Aconducted%20to%20justify%20our%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05591v1&entry.124074799=Read"},
{"title": "FALCON: Feedback-driven Adaptive Long/short-term memory reinforced\n  Coding Optimization system", "author": "Zeyuan Li and Yangfan He and Lewei He and Jianhui Wang and Tianyu Shi and Bin Lei and Yuchen Li and Qiuwu Chen", "abstract": "  Recently, large language models (LLMs) have achieved significant progress in\nautomated code generation. Despite their strong instruction-following\ncapabilities, these models frequently struggled to align with user intent in\ncoding scenarios. In particular, they were hampered by datasets that lacked\ndiversity and failed to address specialized tasks or edge cases. Furthermore,\nchallenges in supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) led to failures in generating precise,\nhuman-intent-aligned code. To tackle these challenges and improve the code\ngeneration performance for automated programming systems, we propose\nFeedback-driven Adaptive Long/short-term memory reinforced Coding Optimization\n(i.e., FALCON). FALCON is structured into two hierarchical levels. From the\nglobal level, long-term memory improves code quality by retaining and applying\nlearned knowledge. At the local level, short-term memory allows for the\nincorporation of immediate feedback from compilers and AI systems.\nAdditionally, we introduce meta-reinforcement learning with feedback rewards to\nsolve the global-local bi-level optimization problem and enhance the model's\nadaptability across diverse code generation tasks. Extensive experiments\ndemonstrate that our technique achieves state-of-the-art performance, leading\nother reinforcement learning methods by more than 4.5 percentage points on the\nMBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The\nopen-sourced code is publicly available at https://github.com/titurte/FALCON.\n", "link": "http://arxiv.org/abs/2410.21349v2", "date": "2024-11-08", "relevancy": 2.4758, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5009}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4951}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FALCON%3A%20Feedback-driven%20Adaptive%20Long/short-term%20memory%20reinforced%0A%20%20Coding%20Optimization%20system&body=Title%3A%20FALCON%3A%20Feedback-driven%20Adaptive%20Long/short-term%20memory%20reinforced%0A%20%20Coding%20Optimization%20system%0AAuthor%3A%20Zeyuan%20Li%20and%20Yangfan%20He%20and%20Lewei%20He%20and%20Jianhui%20Wang%20and%20Tianyu%20Shi%20and%20Bin%20Lei%20and%20Yuchen%20Li%20and%20Qiuwu%20Chen%0AAbstract%3A%20%20%20Recently%2C%20large%20language%20models%20%28LLMs%29%20have%20achieved%20significant%20progress%20in%0Aautomated%20code%20generation.%20Despite%20their%20strong%20instruction-following%0Acapabilities%2C%20these%20models%20frequently%20struggled%20to%20align%20with%20user%20intent%20in%0Acoding%20scenarios.%20In%20particular%2C%20they%20were%20hampered%20by%20datasets%20that%20lacked%0Adiversity%20and%20failed%20to%20address%20specialized%20tasks%20or%20edge%20cases.%20Furthermore%2C%0Achallenges%20in%20supervised%20fine-tuning%20%28SFT%29%20and%20reinforcement%20learning%20from%0Ahuman%20feedback%20%28RLHF%29%20led%20to%20failures%20in%20generating%20precise%2C%0Ahuman-intent-aligned%20code.%20To%20tackle%20these%20challenges%20and%20improve%20the%20code%0Ageneration%20performance%20for%20automated%20programming%20systems%2C%20we%20propose%0AFeedback-driven%20Adaptive%20Long/short-term%20memory%20reinforced%20Coding%20Optimization%0A%28i.e.%2C%20FALCON%29.%20FALCON%20is%20structured%20into%20two%20hierarchical%20levels.%20From%20the%0Aglobal%20level%2C%20long-term%20memory%20improves%20code%20quality%20by%20retaining%20and%20applying%0Alearned%20knowledge.%20At%20the%20local%20level%2C%20short-term%20memory%20allows%20for%20the%0Aincorporation%20of%20immediate%20feedback%20from%20compilers%20and%20AI%20systems.%0AAdditionally%2C%20we%20introduce%20meta-reinforcement%20learning%20with%20feedback%20rewards%20to%0Asolve%20the%20global-local%20bi-level%20optimization%20problem%20and%20enhance%20the%20model%27s%0Aadaptability%20across%20diverse%20code%20generation%20tasks.%20Extensive%20experiments%0Ademonstrate%20that%20our%20technique%20achieves%20state-of-the-art%20performance%2C%20leading%0Aother%20reinforcement%20learning%20methods%20by%20more%20than%204.5%20percentage%20points%20on%20the%0AMBPP%20benchmark%20and%206.1%20percentage%20points%20on%20the%20Humaneval%20benchmark.%20The%0Aopen-sourced%20code%20is%20publicly%20available%20at%20https%3A//github.com/titurte/FALCON.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21349v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFALCON%253A%2520Feedback-driven%2520Adaptive%2520Long/short-term%2520memory%2520reinforced%250A%2520%2520Coding%2520Optimization%2520system%26entry.906535625%3DZeyuan%2520Li%2520and%2520Yangfan%2520He%2520and%2520Lewei%2520He%2520and%2520Jianhui%2520Wang%2520and%2520Tianyu%2520Shi%2520and%2520Bin%2520Lei%2520and%2520Yuchen%2520Li%2520and%2520Qiuwu%2520Chen%26entry.1292438233%3D%2520%2520Recently%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520significant%2520progress%2520in%250Aautomated%2520code%2520generation.%2520Despite%2520their%2520strong%2520instruction-following%250Acapabilities%252C%2520these%2520models%2520frequently%2520struggled%2520to%2520align%2520with%2520user%2520intent%2520in%250Acoding%2520scenarios.%2520In%2520particular%252C%2520they%2520were%2520hampered%2520by%2520datasets%2520that%2520lacked%250Adiversity%2520and%2520failed%2520to%2520address%2520specialized%2520tasks%2520or%2520edge%2520cases.%2520Furthermore%252C%250Achallenges%2520in%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520reinforcement%2520learning%2520from%250Ahuman%2520feedback%2520%2528RLHF%2529%2520led%2520to%2520failures%2520in%2520generating%2520precise%252C%250Ahuman-intent-aligned%2520code.%2520To%2520tackle%2520these%2520challenges%2520and%2520improve%2520the%2520code%250Ageneration%2520performance%2520for%2520automated%2520programming%2520systems%252C%2520we%2520propose%250AFeedback-driven%2520Adaptive%2520Long/short-term%2520memory%2520reinforced%2520Coding%2520Optimization%250A%2528i.e.%252C%2520FALCON%2529.%2520FALCON%2520is%2520structured%2520into%2520two%2520hierarchical%2520levels.%2520From%2520the%250Aglobal%2520level%252C%2520long-term%2520memory%2520improves%2520code%2520quality%2520by%2520retaining%2520and%2520applying%250Alearned%2520knowledge.%2520At%2520the%2520local%2520level%252C%2520short-term%2520memory%2520allows%2520for%2520the%250Aincorporation%2520of%2520immediate%2520feedback%2520from%2520compilers%2520and%2520AI%2520systems.%250AAdditionally%252C%2520we%2520introduce%2520meta-reinforcement%2520learning%2520with%2520feedback%2520rewards%2520to%250Asolve%2520the%2520global-local%2520bi-level%2520optimization%2520problem%2520and%2520enhance%2520the%2520model%2527s%250Aadaptability%2520across%2520diverse%2520code%2520generation%2520tasks.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520our%2520technique%2520achieves%2520state-of-the-art%2520performance%252C%2520leading%250Aother%2520reinforcement%2520learning%2520methods%2520by%2520more%2520than%25204.5%2520percentage%2520points%2520on%2520the%250AMBPP%2520benchmark%2520and%25206.1%2520percentage%2520points%2520on%2520the%2520Humaneval%2520benchmark.%2520The%250Aopen-sourced%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/titurte/FALCON.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21349v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FALCON%3A%20Feedback-driven%20Adaptive%20Long/short-term%20memory%20reinforced%0A%20%20Coding%20Optimization%20system&entry.906535625=Zeyuan%20Li%20and%20Yangfan%20He%20and%20Lewei%20He%20and%20Jianhui%20Wang%20and%20Tianyu%20Shi%20and%20Bin%20Lei%20and%20Yuchen%20Li%20and%20Qiuwu%20Chen&entry.1292438233=%20%20Recently%2C%20large%20language%20models%20%28LLMs%29%20have%20achieved%20significant%20progress%20in%0Aautomated%20code%20generation.%20Despite%20their%20strong%20instruction-following%0Acapabilities%2C%20these%20models%20frequently%20struggled%20to%20align%20with%20user%20intent%20in%0Acoding%20scenarios.%20In%20particular%2C%20they%20were%20hampered%20by%20datasets%20that%20lacked%0Adiversity%20and%20failed%20to%20address%20specialized%20tasks%20or%20edge%20cases.%20Furthermore%2C%0Achallenges%20in%20supervised%20fine-tuning%20%28SFT%29%20and%20reinforcement%20learning%20from%0Ahuman%20feedback%20%28RLHF%29%20led%20to%20failures%20in%20generating%20precise%2C%0Ahuman-intent-aligned%20code.%20To%20tackle%20these%20challenges%20and%20improve%20the%20code%0Ageneration%20performance%20for%20automated%20programming%20systems%2C%20we%20propose%0AFeedback-driven%20Adaptive%20Long/short-term%20memory%20reinforced%20Coding%20Optimization%0A%28i.e.%2C%20FALCON%29.%20FALCON%20is%20structured%20into%20two%20hierarchical%20levels.%20From%20the%0Aglobal%20level%2C%20long-term%20memory%20improves%20code%20quality%20by%20retaining%20and%20applying%0Alearned%20knowledge.%20At%20the%20local%20level%2C%20short-term%20memory%20allows%20for%20the%0Aincorporation%20of%20immediate%20feedback%20from%20compilers%20and%20AI%20systems.%0AAdditionally%2C%20we%20introduce%20meta-reinforcement%20learning%20with%20feedback%20rewards%20to%0Asolve%20the%20global-local%20bi-level%20optimization%20problem%20and%20enhance%20the%20model%27s%0Aadaptability%20across%20diverse%20code%20generation%20tasks.%20Extensive%20experiments%0Ademonstrate%20that%20our%20technique%20achieves%20state-of-the-art%20performance%2C%20leading%0Aother%20reinforcement%20learning%20methods%20by%20more%20than%204.5%20percentage%20points%20on%20the%0AMBPP%20benchmark%20and%206.1%20percentage%20points%20on%20the%20Humaneval%20benchmark.%20The%0Aopen-sourced%20code%20is%20publicly%20available%20at%20https%3A//github.com/titurte/FALCON.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21349v2&entry.124074799=Read"},
{"title": "Continuous-Time Analysis of Adaptive Optimization and Normalization", "author": "Rhys Gould and Hidenori Tanaka", "abstract": "  Adaptive optimization algorithms, particularly Adam and its variant AdamW,\nare fundamental components of modern deep learning. However, their training\ndynamics lack comprehensive theoretical understanding, with limited insight\ninto why common practices - such as specific hyperparameter choices and\nnormalization layers - contribute to successful generalization. This work\npresents a continuous-time formulation of Adam and AdamW, facilitating a\ntractable analysis of training dynamics that can shed light on such practical\nquestions. We theoretically derive a stable region for Adam's hyperparameters\n$(\\beta, \\gamma)$ that ensures bounded updates, empirically verifying these\npredictions by observing unstable exponential growth of parameter updates\noutside this region. Furthermore, we theoretically justify the success of\nnormalization layers by uncovering an implicit meta-adaptive effect of\nscale-invariant architectural components. This insight leads to an explicit\noptimizer, $2$-Adam, which we generalize to $k$-Adam - an optimizer that\napplies an adaptive normalization procedure $k$ times, encompassing Adam\n(corresponding to $k=1$) and Adam with a normalization layer (corresponding to\n$k=2$). Overall, our continuous-time formulation of Adam facilitates a\nprincipled analysis, offering deeper understanding of optimal hyperparameter\nchoices and architectural decisions in modern deep learning.\n", "link": "http://arxiv.org/abs/2411.05746v1", "date": "2024-11-08", "relevancy": 2.4706, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5129}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4853}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous-Time%20Analysis%20of%20Adaptive%20Optimization%20and%20Normalization&body=Title%3A%20Continuous-Time%20Analysis%20of%20Adaptive%20Optimization%20and%20Normalization%0AAuthor%3A%20Rhys%20Gould%20and%20Hidenori%20Tanaka%0AAbstract%3A%20%20%20Adaptive%20optimization%20algorithms%2C%20particularly%20Adam%20and%20its%20variant%20AdamW%2C%0Aare%20fundamental%20components%20of%20modern%20deep%20learning.%20However%2C%20their%20training%0Adynamics%20lack%20comprehensive%20theoretical%20understanding%2C%20with%20limited%20insight%0Ainto%20why%20common%20practices%20-%20such%20as%20specific%20hyperparameter%20choices%20and%0Anormalization%20layers%20-%20contribute%20to%20successful%20generalization.%20This%20work%0Apresents%20a%20continuous-time%20formulation%20of%20Adam%20and%20AdamW%2C%20facilitating%20a%0Atractable%20analysis%20of%20training%20dynamics%20that%20can%20shed%20light%20on%20such%20practical%0Aquestions.%20We%20theoretically%20derive%20a%20stable%20region%20for%20Adam%27s%20hyperparameters%0A%24%28%5Cbeta%2C%20%5Cgamma%29%24%20that%20ensures%20bounded%20updates%2C%20empirically%20verifying%20these%0Apredictions%20by%20observing%20unstable%20exponential%20growth%20of%20parameter%20updates%0Aoutside%20this%20region.%20Furthermore%2C%20we%20theoretically%20justify%20the%20success%20of%0Anormalization%20layers%20by%20uncovering%20an%20implicit%20meta-adaptive%20effect%20of%0Ascale-invariant%20architectural%20components.%20This%20insight%20leads%20to%20an%20explicit%0Aoptimizer%2C%20%242%24-Adam%2C%20which%20we%20generalize%20to%20%24k%24-Adam%20-%20an%20optimizer%20that%0Aapplies%20an%20adaptive%20normalization%20procedure%20%24k%24%20times%2C%20encompassing%20Adam%0A%28corresponding%20to%20%24k%3D1%24%29%20and%20Adam%20with%20a%20normalization%20layer%20%28corresponding%20to%0A%24k%3D2%24%29.%20Overall%2C%20our%20continuous-time%20formulation%20of%20Adam%20facilitates%20a%0Aprincipled%20analysis%2C%20offering%20deeper%20understanding%20of%20optimal%20hyperparameter%0Achoices%20and%20architectural%20decisions%20in%20modern%20deep%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05746v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous-Time%2520Analysis%2520of%2520Adaptive%2520Optimization%2520and%2520Normalization%26entry.906535625%3DRhys%2520Gould%2520and%2520Hidenori%2520Tanaka%26entry.1292438233%3D%2520%2520Adaptive%2520optimization%2520algorithms%252C%2520particularly%2520Adam%2520and%2520its%2520variant%2520AdamW%252C%250Aare%2520fundamental%2520components%2520of%2520modern%2520deep%2520learning.%2520However%252C%2520their%2520training%250Adynamics%2520lack%2520comprehensive%2520theoretical%2520understanding%252C%2520with%2520limited%2520insight%250Ainto%2520why%2520common%2520practices%2520-%2520such%2520as%2520specific%2520hyperparameter%2520choices%2520and%250Anormalization%2520layers%2520-%2520contribute%2520to%2520successful%2520generalization.%2520This%2520work%250Apresents%2520a%2520continuous-time%2520formulation%2520of%2520Adam%2520and%2520AdamW%252C%2520facilitating%2520a%250Atractable%2520analysis%2520of%2520training%2520dynamics%2520that%2520can%2520shed%2520light%2520on%2520such%2520practical%250Aquestions.%2520We%2520theoretically%2520derive%2520a%2520stable%2520region%2520for%2520Adam%2527s%2520hyperparameters%250A%2524%2528%255Cbeta%252C%2520%255Cgamma%2529%2524%2520that%2520ensures%2520bounded%2520updates%252C%2520empirically%2520verifying%2520these%250Apredictions%2520by%2520observing%2520unstable%2520exponential%2520growth%2520of%2520parameter%2520updates%250Aoutside%2520this%2520region.%2520Furthermore%252C%2520we%2520theoretically%2520justify%2520the%2520success%2520of%250Anormalization%2520layers%2520by%2520uncovering%2520an%2520implicit%2520meta-adaptive%2520effect%2520of%250Ascale-invariant%2520architectural%2520components.%2520This%2520insight%2520leads%2520to%2520an%2520explicit%250Aoptimizer%252C%2520%25242%2524-Adam%252C%2520which%2520we%2520generalize%2520to%2520%2524k%2524-Adam%2520-%2520an%2520optimizer%2520that%250Aapplies%2520an%2520adaptive%2520normalization%2520procedure%2520%2524k%2524%2520times%252C%2520encompassing%2520Adam%250A%2528corresponding%2520to%2520%2524k%253D1%2524%2529%2520and%2520Adam%2520with%2520a%2520normalization%2520layer%2520%2528corresponding%2520to%250A%2524k%253D2%2524%2529.%2520Overall%252C%2520our%2520continuous-time%2520formulation%2520of%2520Adam%2520facilitates%2520a%250Aprincipled%2520analysis%252C%2520offering%2520deeper%2520understanding%2520of%2520optimal%2520hyperparameter%250Achoices%2520and%2520architectural%2520decisions%2520in%2520modern%2520deep%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05746v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous-Time%20Analysis%20of%20Adaptive%20Optimization%20and%20Normalization&entry.906535625=Rhys%20Gould%20and%20Hidenori%20Tanaka&entry.1292438233=%20%20Adaptive%20optimization%20algorithms%2C%20particularly%20Adam%20and%20its%20variant%20AdamW%2C%0Aare%20fundamental%20components%20of%20modern%20deep%20learning.%20However%2C%20their%20training%0Adynamics%20lack%20comprehensive%20theoretical%20understanding%2C%20with%20limited%20insight%0Ainto%20why%20common%20practices%20-%20such%20as%20specific%20hyperparameter%20choices%20and%0Anormalization%20layers%20-%20contribute%20to%20successful%20generalization.%20This%20work%0Apresents%20a%20continuous-time%20formulation%20of%20Adam%20and%20AdamW%2C%20facilitating%20a%0Atractable%20analysis%20of%20training%20dynamics%20that%20can%20shed%20light%20on%20such%20practical%0Aquestions.%20We%20theoretically%20derive%20a%20stable%20region%20for%20Adam%27s%20hyperparameters%0A%24%28%5Cbeta%2C%20%5Cgamma%29%24%20that%20ensures%20bounded%20updates%2C%20empirically%20verifying%20these%0Apredictions%20by%20observing%20unstable%20exponential%20growth%20of%20parameter%20updates%0Aoutside%20this%20region.%20Furthermore%2C%20we%20theoretically%20justify%20the%20success%20of%0Anormalization%20layers%20by%20uncovering%20an%20implicit%20meta-adaptive%20effect%20of%0Ascale-invariant%20architectural%20components.%20This%20insight%20leads%20to%20an%20explicit%0Aoptimizer%2C%20%242%24-Adam%2C%20which%20we%20generalize%20to%20%24k%24-Adam%20-%20an%20optimizer%20that%0Aapplies%20an%20adaptive%20normalization%20procedure%20%24k%24%20times%2C%20encompassing%20Adam%0A%28corresponding%20to%20%24k%3D1%24%29%20and%20Adam%20with%20a%20normalization%20layer%20%28corresponding%20to%0A%24k%3D2%24%29.%20Overall%2C%20our%20continuous-time%20formulation%20of%20Adam%20facilitates%20a%0Aprincipled%20analysis%2C%20offering%20deeper%20understanding%20of%20optimal%20hyperparameter%0Achoices%20and%20architectural%20decisions%20in%20modern%20deep%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05746v1&entry.124074799=Read"},
{"title": "DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection", "author": "Hongyu Shen and Yici Yan and Zhizhen Zhao", "abstract": "  Model-X knockoff has garnered significant attention among various feature\nselection methods due to its guarantees for controlling the false discovery\nrate (FDR). Since its introduction in parametric design, knockoff techniques\nhave evolved to handle arbitrary data distributions using deep learning-based\ngenerative models. However, we have observed limitations in the current\nimplementations of the deep Model-X knockoff framework. Notably, the \"swap\nproperty\" that knockoffs require often faces challenges at the sample level,\nresulting in diminished selection power. To address these issues, we develop\n\"Deep Dependency Regularized Knockoff (DeepDRK),\" a distribution-free deep\nlearning method that effectively balances FDR and power. In DeepDRK, we\nintroduce a novel formulation of the knockoff model as a learning problem under\nmulti-source adversarial attacks. By employing an innovative perturbation\ntechnique, we achieve lower FDR and higher power. Our model outperforms\nexisting benchmarks across synthetic, semi-synthetic, and real-world datasets,\nparticularly when sample sizes are small and data distributions are\nnon-Gaussian.\n", "link": "http://arxiv.org/abs/2402.17176v2", "date": "2024-11-08", "relevancy": 2.4512, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5003}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4855}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepDRK%3A%20Deep%20Dependency%20Regularized%20Knockoff%20for%20Feature%20Selection&body=Title%3A%20DeepDRK%3A%20Deep%20Dependency%20Regularized%20Knockoff%20for%20Feature%20Selection%0AAuthor%3A%20Hongyu%20Shen%20and%20Yici%20Yan%20and%20Zhizhen%20Zhao%0AAbstract%3A%20%20%20Model-X%20knockoff%20has%20garnered%20significant%20attention%20among%20various%20feature%0Aselection%20methods%20due%20to%20its%20guarantees%20for%20controlling%20the%20false%20discovery%0Arate%20%28FDR%29.%20Since%20its%20introduction%20in%20parametric%20design%2C%20knockoff%20techniques%0Ahave%20evolved%20to%20handle%20arbitrary%20data%20distributions%20using%20deep%20learning-based%0Agenerative%20models.%20However%2C%20we%20have%20observed%20limitations%20in%20the%20current%0Aimplementations%20of%20the%20deep%20Model-X%20knockoff%20framework.%20Notably%2C%20the%20%22swap%0Aproperty%22%20that%20knockoffs%20require%20often%20faces%20challenges%20at%20the%20sample%20level%2C%0Aresulting%20in%20diminished%20selection%20power.%20To%20address%20these%20issues%2C%20we%20develop%0A%22Deep%20Dependency%20Regularized%20Knockoff%20%28DeepDRK%29%2C%22%20a%20distribution-free%20deep%0Alearning%20method%20that%20effectively%20balances%20FDR%20and%20power.%20In%20DeepDRK%2C%20we%0Aintroduce%20a%20novel%20formulation%20of%20the%20knockoff%20model%20as%20a%20learning%20problem%20under%0Amulti-source%20adversarial%20attacks.%20By%20employing%20an%20innovative%20perturbation%0Atechnique%2C%20we%20achieve%20lower%20FDR%20and%20higher%20power.%20Our%20model%20outperforms%0Aexisting%20benchmarks%20across%20synthetic%2C%20semi-synthetic%2C%20and%20real-world%20datasets%2C%0Aparticularly%20when%20sample%20sizes%20are%20small%20and%20data%20distributions%20are%0Anon-Gaussian.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17176v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepDRK%253A%2520Deep%2520Dependency%2520Regularized%2520Knockoff%2520for%2520Feature%2520Selection%26entry.906535625%3DHongyu%2520Shen%2520and%2520Yici%2520Yan%2520and%2520Zhizhen%2520Zhao%26entry.1292438233%3D%2520%2520Model-X%2520knockoff%2520has%2520garnered%2520significant%2520attention%2520among%2520various%2520feature%250Aselection%2520methods%2520due%2520to%2520its%2520guarantees%2520for%2520controlling%2520the%2520false%2520discovery%250Arate%2520%2528FDR%2529.%2520Since%2520its%2520introduction%2520in%2520parametric%2520design%252C%2520knockoff%2520techniques%250Ahave%2520evolved%2520to%2520handle%2520arbitrary%2520data%2520distributions%2520using%2520deep%2520learning-based%250Agenerative%2520models.%2520However%252C%2520we%2520have%2520observed%2520limitations%2520in%2520the%2520current%250Aimplementations%2520of%2520the%2520deep%2520Model-X%2520knockoff%2520framework.%2520Notably%252C%2520the%2520%2522swap%250Aproperty%2522%2520that%2520knockoffs%2520require%2520often%2520faces%2520challenges%2520at%2520the%2520sample%2520level%252C%250Aresulting%2520in%2520diminished%2520selection%2520power.%2520To%2520address%2520these%2520issues%252C%2520we%2520develop%250A%2522Deep%2520Dependency%2520Regularized%2520Knockoff%2520%2528DeepDRK%2529%252C%2522%2520a%2520distribution-free%2520deep%250Alearning%2520method%2520that%2520effectively%2520balances%2520FDR%2520and%2520power.%2520In%2520DeepDRK%252C%2520we%250Aintroduce%2520a%2520novel%2520formulation%2520of%2520the%2520knockoff%2520model%2520as%2520a%2520learning%2520problem%2520under%250Amulti-source%2520adversarial%2520attacks.%2520By%2520employing%2520an%2520innovative%2520perturbation%250Atechnique%252C%2520we%2520achieve%2520lower%2520FDR%2520and%2520higher%2520power.%2520Our%2520model%2520outperforms%250Aexisting%2520benchmarks%2520across%2520synthetic%252C%2520semi-synthetic%252C%2520and%2520real-world%2520datasets%252C%250Aparticularly%2520when%2520sample%2520sizes%2520are%2520small%2520and%2520data%2520distributions%2520are%250Anon-Gaussian.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17176v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepDRK%3A%20Deep%20Dependency%20Regularized%20Knockoff%20for%20Feature%20Selection&entry.906535625=Hongyu%20Shen%20and%20Yici%20Yan%20and%20Zhizhen%20Zhao&entry.1292438233=%20%20Model-X%20knockoff%20has%20garnered%20significant%20attention%20among%20various%20feature%0Aselection%20methods%20due%20to%20its%20guarantees%20for%20controlling%20the%20false%20discovery%0Arate%20%28FDR%29.%20Since%20its%20introduction%20in%20parametric%20design%2C%20knockoff%20techniques%0Ahave%20evolved%20to%20handle%20arbitrary%20data%20distributions%20using%20deep%20learning-based%0Agenerative%20models.%20However%2C%20we%20have%20observed%20limitations%20in%20the%20current%0Aimplementations%20of%20the%20deep%20Model-X%20knockoff%20framework.%20Notably%2C%20the%20%22swap%0Aproperty%22%20that%20knockoffs%20require%20often%20faces%20challenges%20at%20the%20sample%20level%2C%0Aresulting%20in%20diminished%20selection%20power.%20To%20address%20these%20issues%2C%20we%20develop%0A%22Deep%20Dependency%20Regularized%20Knockoff%20%28DeepDRK%29%2C%22%20a%20distribution-free%20deep%0Alearning%20method%20that%20effectively%20balances%20FDR%20and%20power.%20In%20DeepDRK%2C%20we%0Aintroduce%20a%20novel%20formulation%20of%20the%20knockoff%20model%20as%20a%20learning%20problem%20under%0Amulti-source%20adversarial%20attacks.%20By%20employing%20an%20innovative%20perturbation%0Atechnique%2C%20we%20achieve%20lower%20FDR%20and%20higher%20power.%20Our%20model%20outperforms%0Aexisting%20benchmarks%20across%20synthetic%2C%20semi-synthetic%2C%20and%20real-world%20datasets%2C%0Aparticularly%20when%20sample%20sizes%20are%20small%20and%20data%20distributions%20are%0Anon-Gaussian.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17176v2&entry.124074799=Read"},
{"title": "Using Time-Aware Graph Neural Networks to Predict Temporal Centralities\n  in Dynamic Graphs", "author": "Franziska Heeg and Ingo Scholtes", "abstract": "  Node centralities play a pivotal role in network science, social network\nanalysis, and recommender systems. In temporal data, static path-based\ncentralities like closeness or betweenness can give misleading results about\nthe true importance of nodes in a temporal graph. To address this issue,\ntemporal generalizations of betweenness and closeness have been defined that\nare based on the shortest time-respecting paths between pairs of nodes.\nHowever, a major issue of those generalizations is that the calculation of such\npaths is computationally expensive. Addressing this issue, we study the\napplication of De Bruijn Graph Neural Networks (DBGNN), a time-aware graph\nneural network architecture, to predict temporal path-based centralities in\ntime series data. We experimentally evaluate our approach in 13 temporal graphs\nfrom biological and social systems and show that it considerably improves the\nprediction of betweenness and closeness centrality compared to (i) a static\nGraph Convolutional Neural Network, (ii) an efficient sampling-based\napproximation technique for temporal betweenness, and (iii) two\nstate-of-the-art time-aware graph learning techniques for dynamic graphs.\n", "link": "http://arxiv.org/abs/2310.15865v2", "date": "2024-11-08", "relevancy": 2.4239, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5316}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4729}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Time-Aware%20Graph%20Neural%20Networks%20to%20Predict%20Temporal%20Centralities%0A%20%20in%20Dynamic%20Graphs&body=Title%3A%20Using%20Time-Aware%20Graph%20Neural%20Networks%20to%20Predict%20Temporal%20Centralities%0A%20%20in%20Dynamic%20Graphs%0AAuthor%3A%20Franziska%20Heeg%20and%20Ingo%20Scholtes%0AAbstract%3A%20%20%20Node%20centralities%20play%20a%20pivotal%20role%20in%20network%20science%2C%20social%20network%0Aanalysis%2C%20and%20recommender%20systems.%20In%20temporal%20data%2C%20static%20path-based%0Acentralities%20like%20closeness%20or%20betweenness%20can%20give%20misleading%20results%20about%0Athe%20true%20importance%20of%20nodes%20in%20a%20temporal%20graph.%20To%20address%20this%20issue%2C%0Atemporal%20generalizations%20of%20betweenness%20and%20closeness%20have%20been%20defined%20that%0Aare%20based%20on%20the%20shortest%20time-respecting%20paths%20between%20pairs%20of%20nodes.%0AHowever%2C%20a%20major%20issue%20of%20those%20generalizations%20is%20that%20the%20calculation%20of%20such%0Apaths%20is%20computationally%20expensive.%20Addressing%20this%20issue%2C%20we%20study%20the%0Aapplication%20of%20De%20Bruijn%20Graph%20Neural%20Networks%20%28DBGNN%29%2C%20a%20time-aware%20graph%0Aneural%20network%20architecture%2C%20to%20predict%20temporal%20path-based%20centralities%20in%0Atime%20series%20data.%20We%20experimentally%20evaluate%20our%20approach%20in%2013%20temporal%20graphs%0Afrom%20biological%20and%20social%20systems%20and%20show%20that%20it%20considerably%20improves%20the%0Aprediction%20of%20betweenness%20and%20closeness%20centrality%20compared%20to%20%28i%29%20a%20static%0AGraph%20Convolutional%20Neural%20Network%2C%20%28ii%29%20an%20efficient%20sampling-based%0Aapproximation%20technique%20for%20temporal%20betweenness%2C%20and%20%28iii%29%20two%0Astate-of-the-art%20time-aware%20graph%20learning%20techniques%20for%20dynamic%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.15865v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Time-Aware%2520Graph%2520Neural%2520Networks%2520to%2520Predict%2520Temporal%2520Centralities%250A%2520%2520in%2520Dynamic%2520Graphs%26entry.906535625%3DFranziska%2520Heeg%2520and%2520Ingo%2520Scholtes%26entry.1292438233%3D%2520%2520Node%2520centralities%2520play%2520a%2520pivotal%2520role%2520in%2520network%2520science%252C%2520social%2520network%250Aanalysis%252C%2520and%2520recommender%2520systems.%2520In%2520temporal%2520data%252C%2520static%2520path-based%250Acentralities%2520like%2520closeness%2520or%2520betweenness%2520can%2520give%2520misleading%2520results%2520about%250Athe%2520true%2520importance%2520of%2520nodes%2520in%2520a%2520temporal%2520graph.%2520To%2520address%2520this%2520issue%252C%250Atemporal%2520generalizations%2520of%2520betweenness%2520and%2520closeness%2520have%2520been%2520defined%2520that%250Aare%2520based%2520on%2520the%2520shortest%2520time-respecting%2520paths%2520between%2520pairs%2520of%2520nodes.%250AHowever%252C%2520a%2520major%2520issue%2520of%2520those%2520generalizations%2520is%2520that%2520the%2520calculation%2520of%2520such%250Apaths%2520is%2520computationally%2520expensive.%2520Addressing%2520this%2520issue%252C%2520we%2520study%2520the%250Aapplication%2520of%2520De%2520Bruijn%2520Graph%2520Neural%2520Networks%2520%2528DBGNN%2529%252C%2520a%2520time-aware%2520graph%250Aneural%2520network%2520architecture%252C%2520to%2520predict%2520temporal%2520path-based%2520centralities%2520in%250Atime%2520series%2520data.%2520We%2520experimentally%2520evaluate%2520our%2520approach%2520in%252013%2520temporal%2520graphs%250Afrom%2520biological%2520and%2520social%2520systems%2520and%2520show%2520that%2520it%2520considerably%2520improves%2520the%250Aprediction%2520of%2520betweenness%2520and%2520closeness%2520centrality%2520compared%2520to%2520%2528i%2529%2520a%2520static%250AGraph%2520Convolutional%2520Neural%2520Network%252C%2520%2528ii%2529%2520an%2520efficient%2520sampling-based%250Aapproximation%2520technique%2520for%2520temporal%2520betweenness%252C%2520and%2520%2528iii%2529%2520two%250Astate-of-the-art%2520time-aware%2520graph%2520learning%2520techniques%2520for%2520dynamic%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.15865v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Time-Aware%20Graph%20Neural%20Networks%20to%20Predict%20Temporal%20Centralities%0A%20%20in%20Dynamic%20Graphs&entry.906535625=Franziska%20Heeg%20and%20Ingo%20Scholtes&entry.1292438233=%20%20Node%20centralities%20play%20a%20pivotal%20role%20in%20network%20science%2C%20social%20network%0Aanalysis%2C%20and%20recommender%20systems.%20In%20temporal%20data%2C%20static%20path-based%0Acentralities%20like%20closeness%20or%20betweenness%20can%20give%20misleading%20results%20about%0Athe%20true%20importance%20of%20nodes%20in%20a%20temporal%20graph.%20To%20address%20this%20issue%2C%0Atemporal%20generalizations%20of%20betweenness%20and%20closeness%20have%20been%20defined%20that%0Aare%20based%20on%20the%20shortest%20time-respecting%20paths%20between%20pairs%20of%20nodes.%0AHowever%2C%20a%20major%20issue%20of%20those%20generalizations%20is%20that%20the%20calculation%20of%20such%0Apaths%20is%20computationally%20expensive.%20Addressing%20this%20issue%2C%20we%20study%20the%0Aapplication%20of%20De%20Bruijn%20Graph%20Neural%20Networks%20%28DBGNN%29%2C%20a%20time-aware%20graph%0Aneural%20network%20architecture%2C%20to%20predict%20temporal%20path-based%20centralities%20in%0Atime%20series%20data.%20We%20experimentally%20evaluate%20our%20approach%20in%2013%20temporal%20graphs%0Afrom%20biological%20and%20social%20systems%20and%20show%20that%20it%20considerably%20improves%20the%0Aprediction%20of%20betweenness%20and%20closeness%20centrality%20compared%20to%20%28i%29%20a%20static%0AGraph%20Convolutional%20Neural%20Network%2C%20%28ii%29%20an%20efficient%20sampling-based%0Aapproximation%20technique%20for%20temporal%20betweenness%2C%20and%20%28iii%29%20two%0Astate-of-the-art%20time-aware%20graph%20learning%20techniques%20for%20dynamic%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.15865v2&entry.124074799=Read"},
{"title": "Boosting 3D Object Detection with Semantic-Aware Multi-Branch Framework", "author": "Hao Jing and Anhong Wang and Lijun Zhao and Yakun Yang and Donghan Bu and Jing Zhang and Yifan Zhang and Junhui Hou", "abstract": "  In autonomous driving, LiDAR sensors are vital for acquiring 3D point clouds,\nproviding reliable geometric information. However, traditional sampling methods\nof preprocessing often ignore semantic features, leading to detail loss and\nground point interference in 3D object detection. To address this, we propose a\nmulti-branch two-stage 3D object detection framework using a Semantic-aware\nMulti-branch Sampling (SMS) module and multi-view consistency constraints. The\nSMS module includes random sampling, Density Equalization Sampling (DES) for\nenhancing distant objects, and Ground Abandonment Sampling (GAS) to focus on\nnon-ground points. The sampled multi-view points are processed through a\nConsistent KeyPoint Selection (CKPS) module to generate consistent keypoint\nmasks for efficient proposal sampling. The first-stage detector uses\nmulti-branch parallel learning with multi-view consistency loss for feature\naggregation, while the second-stage detector fuses multi-view data through a\nMulti-View Fusion Pooling (MVFP) module to precisely predict 3D objects. The\nexperimental results on the KITTI dataset and Waymo Open Dataset show that our\nmethod achieves excellent detection performance improvement for a variety of\nbackbones, especially for low-performance backbones with the simple network\nstructures.\n", "link": "http://arxiv.org/abs/2407.05769v2", "date": "2024-11-08", "relevancy": 2.4238, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6197}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.612}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%203D%20Object%20Detection%20with%20Semantic-Aware%20Multi-Branch%20Framework&body=Title%3A%20Boosting%203D%20Object%20Detection%20with%20Semantic-Aware%20Multi-Branch%20Framework%0AAuthor%3A%20Hao%20Jing%20and%20Anhong%20Wang%20and%20Lijun%20Zhao%20and%20Yakun%20Yang%20and%20Donghan%20Bu%20and%20Jing%20Zhang%20and%20Yifan%20Zhang%20and%20Junhui%20Hou%0AAbstract%3A%20%20%20In%20autonomous%20driving%2C%20LiDAR%20sensors%20are%20vital%20for%20acquiring%203D%20point%20clouds%2C%0Aproviding%20reliable%20geometric%20information.%20However%2C%20traditional%20sampling%20methods%0Aof%20preprocessing%20often%20ignore%20semantic%20features%2C%20leading%20to%20detail%20loss%20and%0Aground%20point%20interference%20in%203D%20object%20detection.%20To%20address%20this%2C%20we%20propose%20a%0Amulti-branch%20two-stage%203D%20object%20detection%20framework%20using%20a%20Semantic-aware%0AMulti-branch%20Sampling%20%28SMS%29%20module%20and%20multi-view%20consistency%20constraints.%20The%0ASMS%20module%20includes%20random%20sampling%2C%20Density%20Equalization%20Sampling%20%28DES%29%20for%0Aenhancing%20distant%20objects%2C%20and%20Ground%20Abandonment%20Sampling%20%28GAS%29%20to%20focus%20on%0Anon-ground%20points.%20The%20sampled%20multi-view%20points%20are%20processed%20through%20a%0AConsistent%20KeyPoint%20Selection%20%28CKPS%29%20module%20to%20generate%20consistent%20keypoint%0Amasks%20for%20efficient%20proposal%20sampling.%20The%20first-stage%20detector%20uses%0Amulti-branch%20parallel%20learning%20with%20multi-view%20consistency%20loss%20for%20feature%0Aaggregation%2C%20while%20the%20second-stage%20detector%20fuses%20multi-view%20data%20through%20a%0AMulti-View%20Fusion%20Pooling%20%28MVFP%29%20module%20to%20precisely%20predict%203D%20objects.%20The%0Aexperimental%20results%20on%20the%20KITTI%20dataset%20and%20Waymo%20Open%20Dataset%20show%20that%20our%0Amethod%20achieves%20excellent%20detection%20performance%20improvement%20for%20a%20variety%20of%0Abackbones%2C%20especially%20for%20low-performance%20backbones%20with%20the%20simple%20network%0Astructures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05769v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%25203D%2520Object%2520Detection%2520with%2520Semantic-Aware%2520Multi-Branch%2520Framework%26entry.906535625%3DHao%2520Jing%2520and%2520Anhong%2520Wang%2520and%2520Lijun%2520Zhao%2520and%2520Yakun%2520Yang%2520and%2520Donghan%2520Bu%2520and%2520Jing%2520Zhang%2520and%2520Yifan%2520Zhang%2520and%2520Junhui%2520Hou%26entry.1292438233%3D%2520%2520In%2520autonomous%2520driving%252C%2520LiDAR%2520sensors%2520are%2520vital%2520for%2520acquiring%25203D%2520point%2520clouds%252C%250Aproviding%2520reliable%2520geometric%2520information.%2520However%252C%2520traditional%2520sampling%2520methods%250Aof%2520preprocessing%2520often%2520ignore%2520semantic%2520features%252C%2520leading%2520to%2520detail%2520loss%2520and%250Aground%2520point%2520interference%2520in%25203D%2520object%2520detection.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250Amulti-branch%2520two-stage%25203D%2520object%2520detection%2520framework%2520using%2520a%2520Semantic-aware%250AMulti-branch%2520Sampling%2520%2528SMS%2529%2520module%2520and%2520multi-view%2520consistency%2520constraints.%2520The%250ASMS%2520module%2520includes%2520random%2520sampling%252C%2520Density%2520Equalization%2520Sampling%2520%2528DES%2529%2520for%250Aenhancing%2520distant%2520objects%252C%2520and%2520Ground%2520Abandonment%2520Sampling%2520%2528GAS%2529%2520to%2520focus%2520on%250Anon-ground%2520points.%2520The%2520sampled%2520multi-view%2520points%2520are%2520processed%2520through%2520a%250AConsistent%2520KeyPoint%2520Selection%2520%2528CKPS%2529%2520module%2520to%2520generate%2520consistent%2520keypoint%250Amasks%2520for%2520efficient%2520proposal%2520sampling.%2520The%2520first-stage%2520detector%2520uses%250Amulti-branch%2520parallel%2520learning%2520with%2520multi-view%2520consistency%2520loss%2520for%2520feature%250Aaggregation%252C%2520while%2520the%2520second-stage%2520detector%2520fuses%2520multi-view%2520data%2520through%2520a%250AMulti-View%2520Fusion%2520Pooling%2520%2528MVFP%2529%2520module%2520to%2520precisely%2520predict%25203D%2520objects.%2520The%250Aexperimental%2520results%2520on%2520the%2520KITTI%2520dataset%2520and%2520Waymo%2520Open%2520Dataset%2520show%2520that%2520our%250Amethod%2520achieves%2520excellent%2520detection%2520performance%2520improvement%2520for%2520a%2520variety%2520of%250Abackbones%252C%2520especially%2520for%2520low-performance%2520backbones%2520with%2520the%2520simple%2520network%250Astructures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05769v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%203D%20Object%20Detection%20with%20Semantic-Aware%20Multi-Branch%20Framework&entry.906535625=Hao%20Jing%20and%20Anhong%20Wang%20and%20Lijun%20Zhao%20and%20Yakun%20Yang%20and%20Donghan%20Bu%20and%20Jing%20Zhang%20and%20Yifan%20Zhang%20and%20Junhui%20Hou&entry.1292438233=%20%20In%20autonomous%20driving%2C%20LiDAR%20sensors%20are%20vital%20for%20acquiring%203D%20point%20clouds%2C%0Aproviding%20reliable%20geometric%20information.%20However%2C%20traditional%20sampling%20methods%0Aof%20preprocessing%20often%20ignore%20semantic%20features%2C%20leading%20to%20detail%20loss%20and%0Aground%20point%20interference%20in%203D%20object%20detection.%20To%20address%20this%2C%20we%20propose%20a%0Amulti-branch%20two-stage%203D%20object%20detection%20framework%20using%20a%20Semantic-aware%0AMulti-branch%20Sampling%20%28SMS%29%20module%20and%20multi-view%20consistency%20constraints.%20The%0ASMS%20module%20includes%20random%20sampling%2C%20Density%20Equalization%20Sampling%20%28DES%29%20for%0Aenhancing%20distant%20objects%2C%20and%20Ground%20Abandonment%20Sampling%20%28GAS%29%20to%20focus%20on%0Anon-ground%20points.%20The%20sampled%20multi-view%20points%20are%20processed%20through%20a%0AConsistent%20KeyPoint%20Selection%20%28CKPS%29%20module%20to%20generate%20consistent%20keypoint%0Amasks%20for%20efficient%20proposal%20sampling.%20The%20first-stage%20detector%20uses%0Amulti-branch%20parallel%20learning%20with%20multi-view%20consistency%20loss%20for%20feature%0Aaggregation%2C%20while%20the%20second-stage%20detector%20fuses%20multi-view%20data%20through%20a%0AMulti-View%20Fusion%20Pooling%20%28MVFP%29%20module%20to%20precisely%20predict%203D%20objects.%20The%0Aexperimental%20results%20on%20the%20KITTI%20dataset%20and%20Waymo%20Open%20Dataset%20show%20that%20our%0Amethod%20achieves%20excellent%20detection%20performance%20improvement%20for%20a%20variety%20of%0Abackbones%2C%20especially%20for%20low-performance%20backbones%20with%20the%20simple%20network%0Astructures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05769v2&entry.124074799=Read"},
{"title": "\"Where am I?\" Scene Retrieval with Language", "author": "Jiaqi Chen and Daniel Barath and Iro Armeni and Marc Pollefeys and Hermann Blum", "abstract": "  Natural language interfaces to embodied AI are becoming more ubiquitous in\nour daily lives. This opens up further opportunities for language-based\ninteraction with embodied agents, such as a user verbally instructing an agent\nto execute some task in a specific location. For example, \"put the bowls back\nin the cupboard next to the fridge\" or \"meet me at the intersection under the\nred sign.\" As such, we need methods that interface between natural language and\nmap representations of the environment. To this end, we explore the question of\nwhether we can use an open-set natural language query to identify a scene\nrepresented by a 3D scene graph. We define this task as \"language-based\nscene-retrieval\" and it is closely related to \"coarse-localization,\" but we are\ninstead searching for a match from a collection of disjoint scenes and not\nnecessarily a large-scale continuous map. We present Text2SceneGraphMatcher, a\n\"scene-retrieval\" pipeline that learns joint embeddings between text\ndescriptions and scene graphs to determine if they are a match. The code,\ntrained models, and datasets will be made public.\n", "link": "http://arxiv.org/abs/2404.14565v2", "date": "2024-11-08", "relevancy": 2.4021, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6025}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6025}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%22Where%20am%20I%3F%22%20Scene%20Retrieval%20with%20Language&body=Title%3A%20%22Where%20am%20I%3F%22%20Scene%20Retrieval%20with%20Language%0AAuthor%3A%20Jiaqi%20Chen%20and%20Daniel%20Barath%20and%20Iro%20Armeni%20and%20Marc%20Pollefeys%20and%20Hermann%20Blum%0AAbstract%3A%20%20%20Natural%20language%20interfaces%20to%20embodied%20AI%20are%20becoming%20more%20ubiquitous%20in%0Aour%20daily%20lives.%20This%20opens%20up%20further%20opportunities%20for%20language-based%0Ainteraction%20with%20embodied%20agents%2C%20such%20as%20a%20user%20verbally%20instructing%20an%20agent%0Ato%20execute%20some%20task%20in%20a%20specific%20location.%20For%20example%2C%20%22put%20the%20bowls%20back%0Ain%20the%20cupboard%20next%20to%20the%20fridge%22%20or%20%22meet%20me%20at%20the%20intersection%20under%20the%0Ared%20sign.%22%20As%20such%2C%20we%20need%20methods%20that%20interface%20between%20natural%20language%20and%0Amap%20representations%20of%20the%20environment.%20To%20this%20end%2C%20we%20explore%20the%20question%20of%0Awhether%20we%20can%20use%20an%20open-set%20natural%20language%20query%20to%20identify%20a%20scene%0Arepresented%20by%20a%203D%20scene%20graph.%20We%20define%20this%20task%20as%20%22language-based%0Ascene-retrieval%22%20and%20it%20is%20closely%20related%20to%20%22coarse-localization%2C%22%20but%20we%20are%0Ainstead%20searching%20for%20a%20match%20from%20a%20collection%20of%20disjoint%20scenes%20and%20not%0Anecessarily%20a%20large-scale%20continuous%20map.%20We%20present%20Text2SceneGraphMatcher%2C%20a%0A%22scene-retrieval%22%20pipeline%20that%20learns%20joint%20embeddings%20between%20text%0Adescriptions%20and%20scene%20graphs%20to%20determine%20if%20they%20are%20a%20match.%20The%20code%2C%0Atrained%20models%2C%20and%20datasets%20will%20be%20made%20public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14565v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2522Where%2520am%2520I%253F%2522%2520Scene%2520Retrieval%2520with%2520Language%26entry.906535625%3DJiaqi%2520Chen%2520and%2520Daniel%2520Barath%2520and%2520Iro%2520Armeni%2520and%2520Marc%2520Pollefeys%2520and%2520Hermann%2520Blum%26entry.1292438233%3D%2520%2520Natural%2520language%2520interfaces%2520to%2520embodied%2520AI%2520are%2520becoming%2520more%2520ubiquitous%2520in%250Aour%2520daily%2520lives.%2520This%2520opens%2520up%2520further%2520opportunities%2520for%2520language-based%250Ainteraction%2520with%2520embodied%2520agents%252C%2520such%2520as%2520a%2520user%2520verbally%2520instructing%2520an%2520agent%250Ato%2520execute%2520some%2520task%2520in%2520a%2520specific%2520location.%2520For%2520example%252C%2520%2522put%2520the%2520bowls%2520back%250Ain%2520the%2520cupboard%2520next%2520to%2520the%2520fridge%2522%2520or%2520%2522meet%2520me%2520at%2520the%2520intersection%2520under%2520the%250Ared%2520sign.%2522%2520As%2520such%252C%2520we%2520need%2520methods%2520that%2520interface%2520between%2520natural%2520language%2520and%250Amap%2520representations%2520of%2520the%2520environment.%2520To%2520this%2520end%252C%2520we%2520explore%2520the%2520question%2520of%250Awhether%2520we%2520can%2520use%2520an%2520open-set%2520natural%2520language%2520query%2520to%2520identify%2520a%2520scene%250Arepresented%2520by%2520a%25203D%2520scene%2520graph.%2520We%2520define%2520this%2520task%2520as%2520%2522language-based%250Ascene-retrieval%2522%2520and%2520it%2520is%2520closely%2520related%2520to%2520%2522coarse-localization%252C%2522%2520but%2520we%2520are%250Ainstead%2520searching%2520for%2520a%2520match%2520from%2520a%2520collection%2520of%2520disjoint%2520scenes%2520and%2520not%250Anecessarily%2520a%2520large-scale%2520continuous%2520map.%2520We%2520present%2520Text2SceneGraphMatcher%252C%2520a%250A%2522scene-retrieval%2522%2520pipeline%2520that%2520learns%2520joint%2520embeddings%2520between%2520text%250Adescriptions%2520and%2520scene%2520graphs%2520to%2520determine%2520if%2520they%2520are%2520a%2520match.%2520The%2520code%252C%250Atrained%2520models%252C%2520and%2520datasets%2520will%2520be%2520made%2520public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14565v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22Where%20am%20I%3F%22%20Scene%20Retrieval%20with%20Language&entry.906535625=Jiaqi%20Chen%20and%20Daniel%20Barath%20and%20Iro%20Armeni%20and%20Marc%20Pollefeys%20and%20Hermann%20Blum&entry.1292438233=%20%20Natural%20language%20interfaces%20to%20embodied%20AI%20are%20becoming%20more%20ubiquitous%20in%0Aour%20daily%20lives.%20This%20opens%20up%20further%20opportunities%20for%20language-based%0Ainteraction%20with%20embodied%20agents%2C%20such%20as%20a%20user%20verbally%20instructing%20an%20agent%0Ato%20execute%20some%20task%20in%20a%20specific%20location.%20For%20example%2C%20%22put%20the%20bowls%20back%0Ain%20the%20cupboard%20next%20to%20the%20fridge%22%20or%20%22meet%20me%20at%20the%20intersection%20under%20the%0Ared%20sign.%22%20As%20such%2C%20we%20need%20methods%20that%20interface%20between%20natural%20language%20and%0Amap%20representations%20of%20the%20environment.%20To%20this%20end%2C%20we%20explore%20the%20question%20of%0Awhether%20we%20can%20use%20an%20open-set%20natural%20language%20query%20to%20identify%20a%20scene%0Arepresented%20by%20a%203D%20scene%20graph.%20We%20define%20this%20task%20as%20%22language-based%0Ascene-retrieval%22%20and%20it%20is%20closely%20related%20to%20%22coarse-localization%2C%22%20but%20we%20are%0Ainstead%20searching%20for%20a%20match%20from%20a%20collection%20of%20disjoint%20scenes%20and%20not%0Anecessarily%20a%20large-scale%20continuous%20map.%20We%20present%20Text2SceneGraphMatcher%2C%20a%0A%22scene-retrieval%22%20pipeline%20that%20learns%20joint%20embeddings%20between%20text%0Adescriptions%20and%20scene%20graphs%20to%20determine%20if%20they%20are%20a%20match.%20The%20code%2C%0Atrained%20models%2C%20and%20datasets%20will%20be%20made%20public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14565v2&entry.124074799=Read"},
{"title": "GazeSearch: Radiology Findings Search Benchmark", "author": "Trong Thang Pham and Tien-Phat Nguyen and Yuki Ikebe and Akash Awasthi and Zhigang Deng and Carol C. Wu and Hien Nguyen and Ngan Le", "abstract": "  Medical eye-tracking data is an important information source for\nunderstanding how radiologists visually interpret medical images. This\ninformation not only improves the accuracy of deep learning models for X-ray\nanalysis but also their interpretability, enhancing transparency in\ndecision-making. However, the current eye-tracking data is dispersed,\nunprocessed, and ambiguous, making it difficult to derive meaningful insights.\nTherefore, there is a need to create a new dataset with more focus and\npurposeful eyetracking data, improving its utility for diagnostic applications.\nIn this work, we propose a refinement method inspired by the target-present\nvisual search challenge: there is a specific finding and fixations are guided\nto locate it. After refining the existing eye-tracking datasets, we transform\nthem into a curated visual search dataset, called GazeSearch, specifically for\nradiology findings, where each fixation sequence is purposefully aligned to the\ntask of locating a particular finding. Subsequently, we introduce a scan path\nprediction baseline, called ChestSearch, specifically tailored to GazeSearch.\nFinally, we employ the newly introduced GazeSearch as a benchmark to evaluate\nthe performance of current state-of-the-art methods, offering a comprehensive\nassessment for visual search in the medical imaging domain.\n", "link": "http://arxiv.org/abs/2411.05780v1", "date": "2024-11-08", "relevancy": 2.3964, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4848}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4848}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GazeSearch%3A%20Radiology%20Findings%20Search%20Benchmark&body=Title%3A%20GazeSearch%3A%20Radiology%20Findings%20Search%20Benchmark%0AAuthor%3A%20Trong%20Thang%20Pham%20and%20Tien-Phat%20Nguyen%20and%20Yuki%20Ikebe%20and%20Akash%20Awasthi%20and%20Zhigang%20Deng%20and%20Carol%20C.%20Wu%20and%20Hien%20Nguyen%20and%20Ngan%20Le%0AAbstract%3A%20%20%20Medical%20eye-tracking%20data%20is%20an%20important%20information%20source%20for%0Aunderstanding%20how%20radiologists%20visually%20interpret%20medical%20images.%20This%0Ainformation%20not%20only%20improves%20the%20accuracy%20of%20deep%20learning%20models%20for%20X-ray%0Aanalysis%20but%20also%20their%20interpretability%2C%20enhancing%20transparency%20in%0Adecision-making.%20However%2C%20the%20current%20eye-tracking%20data%20is%20dispersed%2C%0Aunprocessed%2C%20and%20ambiguous%2C%20making%20it%20difficult%20to%20derive%20meaningful%20insights.%0ATherefore%2C%20there%20is%20a%20need%20to%20create%20a%20new%20dataset%20with%20more%20focus%20and%0Apurposeful%20eyetracking%20data%2C%20improving%20its%20utility%20for%20diagnostic%20applications.%0AIn%20this%20work%2C%20we%20propose%20a%20refinement%20method%20inspired%20by%20the%20target-present%0Avisual%20search%20challenge%3A%20there%20is%20a%20specific%20finding%20and%20fixations%20are%20guided%0Ato%20locate%20it.%20After%20refining%20the%20existing%20eye-tracking%20datasets%2C%20we%20transform%0Athem%20into%20a%20curated%20visual%20search%20dataset%2C%20called%20GazeSearch%2C%20specifically%20for%0Aradiology%20findings%2C%20where%20each%20fixation%20sequence%20is%20purposefully%20aligned%20to%20the%0Atask%20of%20locating%20a%20particular%20finding.%20Subsequently%2C%20we%20introduce%20a%20scan%20path%0Aprediction%20baseline%2C%20called%20ChestSearch%2C%20specifically%20tailored%20to%20GazeSearch.%0AFinally%2C%20we%20employ%20the%20newly%20introduced%20GazeSearch%20as%20a%20benchmark%20to%20evaluate%0Athe%20performance%20of%20current%20state-of-the-art%20methods%2C%20offering%20a%20comprehensive%0Aassessment%20for%20visual%20search%20in%20the%20medical%20imaging%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05780v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGazeSearch%253A%2520Radiology%2520Findings%2520Search%2520Benchmark%26entry.906535625%3DTrong%2520Thang%2520Pham%2520and%2520Tien-Phat%2520Nguyen%2520and%2520Yuki%2520Ikebe%2520and%2520Akash%2520Awasthi%2520and%2520Zhigang%2520Deng%2520and%2520Carol%2520C.%2520Wu%2520and%2520Hien%2520Nguyen%2520and%2520Ngan%2520Le%26entry.1292438233%3D%2520%2520Medical%2520eye-tracking%2520data%2520is%2520an%2520important%2520information%2520source%2520for%250Aunderstanding%2520how%2520radiologists%2520visually%2520interpret%2520medical%2520images.%2520This%250Ainformation%2520not%2520only%2520improves%2520the%2520accuracy%2520of%2520deep%2520learning%2520models%2520for%2520X-ray%250Aanalysis%2520but%2520also%2520their%2520interpretability%252C%2520enhancing%2520transparency%2520in%250Adecision-making.%2520However%252C%2520the%2520current%2520eye-tracking%2520data%2520is%2520dispersed%252C%250Aunprocessed%252C%2520and%2520ambiguous%252C%2520making%2520it%2520difficult%2520to%2520derive%2520meaningful%2520insights.%250ATherefore%252C%2520there%2520is%2520a%2520need%2520to%2520create%2520a%2520new%2520dataset%2520with%2520more%2520focus%2520and%250Apurposeful%2520eyetracking%2520data%252C%2520improving%2520its%2520utility%2520for%2520diagnostic%2520applications.%250AIn%2520this%2520work%252C%2520we%2520propose%2520a%2520refinement%2520method%2520inspired%2520by%2520the%2520target-present%250Avisual%2520search%2520challenge%253A%2520there%2520is%2520a%2520specific%2520finding%2520and%2520fixations%2520are%2520guided%250Ato%2520locate%2520it.%2520After%2520refining%2520the%2520existing%2520eye-tracking%2520datasets%252C%2520we%2520transform%250Athem%2520into%2520a%2520curated%2520visual%2520search%2520dataset%252C%2520called%2520GazeSearch%252C%2520specifically%2520for%250Aradiology%2520findings%252C%2520where%2520each%2520fixation%2520sequence%2520is%2520purposefully%2520aligned%2520to%2520the%250Atask%2520of%2520locating%2520a%2520particular%2520finding.%2520Subsequently%252C%2520we%2520introduce%2520a%2520scan%2520path%250Aprediction%2520baseline%252C%2520called%2520ChestSearch%252C%2520specifically%2520tailored%2520to%2520GazeSearch.%250AFinally%252C%2520we%2520employ%2520the%2520newly%2520introduced%2520GazeSearch%2520as%2520a%2520benchmark%2520to%2520evaluate%250Athe%2520performance%2520of%2520current%2520state-of-the-art%2520methods%252C%2520offering%2520a%2520comprehensive%250Aassessment%2520for%2520visual%2520search%2520in%2520the%2520medical%2520imaging%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05780v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GazeSearch%3A%20Radiology%20Findings%20Search%20Benchmark&entry.906535625=Trong%20Thang%20Pham%20and%20Tien-Phat%20Nguyen%20and%20Yuki%20Ikebe%20and%20Akash%20Awasthi%20and%20Zhigang%20Deng%20and%20Carol%20C.%20Wu%20and%20Hien%20Nguyen%20and%20Ngan%20Le&entry.1292438233=%20%20Medical%20eye-tracking%20data%20is%20an%20important%20information%20source%20for%0Aunderstanding%20how%20radiologists%20visually%20interpret%20medical%20images.%20This%0Ainformation%20not%20only%20improves%20the%20accuracy%20of%20deep%20learning%20models%20for%20X-ray%0Aanalysis%20but%20also%20their%20interpretability%2C%20enhancing%20transparency%20in%0Adecision-making.%20However%2C%20the%20current%20eye-tracking%20data%20is%20dispersed%2C%0Aunprocessed%2C%20and%20ambiguous%2C%20making%20it%20difficult%20to%20derive%20meaningful%20insights.%0ATherefore%2C%20there%20is%20a%20need%20to%20create%20a%20new%20dataset%20with%20more%20focus%20and%0Apurposeful%20eyetracking%20data%2C%20improving%20its%20utility%20for%20diagnostic%20applications.%0AIn%20this%20work%2C%20we%20propose%20a%20refinement%20method%20inspired%20by%20the%20target-present%0Avisual%20search%20challenge%3A%20there%20is%20a%20specific%20finding%20and%20fixations%20are%20guided%0Ato%20locate%20it.%20After%20refining%20the%20existing%20eye-tracking%20datasets%2C%20we%20transform%0Athem%20into%20a%20curated%20visual%20search%20dataset%2C%20called%20GazeSearch%2C%20specifically%20for%0Aradiology%20findings%2C%20where%20each%20fixation%20sequence%20is%20purposefully%20aligned%20to%20the%0Atask%20of%20locating%20a%20particular%20finding.%20Subsequently%2C%20we%20introduce%20a%20scan%20path%0Aprediction%20baseline%2C%20called%20ChestSearch%2C%20specifically%20tailored%20to%20GazeSearch.%0AFinally%2C%20we%20employ%20the%20newly%20introduced%20GazeSearch%20as%20a%20benchmark%20to%20evaluate%0Athe%20performance%20of%20current%20state-of-the-art%20methods%2C%20offering%20a%20comprehensive%0Aassessment%20for%20visual%20search%20in%20the%20medical%20imaging%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05780v1&entry.124074799=Read"},
{"title": "Confidence Regulation Neurons in Language Models", "author": "Alessandro Stolfo and Ben Wu and Wes Gurnee and Yonatan Belinkov and Xingyi Song and Mrinmaya Sachan and Neel Nanda", "abstract": "  Despite their widespread use, the mechanisms by which large language models\n(LLMs) represent and regulate uncertainty in next-token predictions remain\nlargely unexplored. This study investigates two critical components believed to\ninfluence this uncertainty: the recently discovered entropy neurons and a new\nset of components that we term token frequency neurons. Entropy neurons are\ncharacterized by an unusually high weight norm and influence the final layer\nnormalization (LayerNorm) scale to effectively scale down the logits. Our work\nshows that entropy neurons operate by writing onto an unembedding null space,\nallowing them to impact the residual stream norm with minimal direct effect on\nthe logits themselves. We observe the presence of entropy neurons across a\nrange of models, up to 7 billion parameters. On the other hand, token frequency\nneurons, which we discover and describe here for the first time, boost or\nsuppress each token's logit proportionally to its log frequency, thereby\nshifting the output distribution towards or away from the unigram distribution.\nFinally, we present a detailed case study where entropy neurons actively manage\nconfidence in the setting of induction, i.e. detecting and continuing repeated\nsubsequences.\n", "link": "http://arxiv.org/abs/2406.16254v2", "date": "2024-11-08", "relevancy": 2.3899, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4787}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4777}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Confidence%20Regulation%20Neurons%20in%20Language%20Models&body=Title%3A%20Confidence%20Regulation%20Neurons%20in%20Language%20Models%0AAuthor%3A%20Alessandro%20Stolfo%20and%20Ben%20Wu%20and%20Wes%20Gurnee%20and%20Yonatan%20Belinkov%20and%20Xingyi%20Song%20and%20Mrinmaya%20Sachan%20and%20Neel%20Nanda%0AAbstract%3A%20%20%20Despite%20their%20widespread%20use%2C%20the%20mechanisms%20by%20which%20large%20language%20models%0A%28LLMs%29%20represent%20and%20regulate%20uncertainty%20in%20next-token%20predictions%20remain%0Alargely%20unexplored.%20This%20study%20investigates%20two%20critical%20components%20believed%20to%0Ainfluence%20this%20uncertainty%3A%20the%20recently%20discovered%20entropy%20neurons%20and%20a%20new%0Aset%20of%20components%20that%20we%20term%20token%20frequency%20neurons.%20Entropy%20neurons%20are%0Acharacterized%20by%20an%20unusually%20high%20weight%20norm%20and%20influence%20the%20final%20layer%0Anormalization%20%28LayerNorm%29%20scale%20to%20effectively%20scale%20down%20the%20logits.%20Our%20work%0Ashows%20that%20entropy%20neurons%20operate%20by%20writing%20onto%20an%20unembedding%20null%20space%2C%0Aallowing%20them%20to%20impact%20the%20residual%20stream%20norm%20with%20minimal%20direct%20effect%20on%0Athe%20logits%20themselves.%20We%20observe%20the%20presence%20of%20entropy%20neurons%20across%20a%0Arange%20of%20models%2C%20up%20to%207%20billion%20parameters.%20On%20the%20other%20hand%2C%20token%20frequency%0Aneurons%2C%20which%20we%20discover%20and%20describe%20here%20for%20the%20first%20time%2C%20boost%20or%0Asuppress%20each%20token%27s%20logit%20proportionally%20to%20its%20log%20frequency%2C%20thereby%0Ashifting%20the%20output%20distribution%20towards%20or%20away%20from%20the%20unigram%20distribution.%0AFinally%2C%20we%20present%20a%20detailed%20case%20study%20where%20entropy%20neurons%20actively%20manage%0Aconfidence%20in%20the%20setting%20of%20induction%2C%20i.e.%20detecting%20and%20continuing%20repeated%0Asubsequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16254v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConfidence%2520Regulation%2520Neurons%2520in%2520Language%2520Models%26entry.906535625%3DAlessandro%2520Stolfo%2520and%2520Ben%2520Wu%2520and%2520Wes%2520Gurnee%2520and%2520Yonatan%2520Belinkov%2520and%2520Xingyi%2520Song%2520and%2520Mrinmaya%2520Sachan%2520and%2520Neel%2520Nanda%26entry.1292438233%3D%2520%2520Despite%2520their%2520widespread%2520use%252C%2520the%2520mechanisms%2520by%2520which%2520large%2520language%2520models%250A%2528LLMs%2529%2520represent%2520and%2520regulate%2520uncertainty%2520in%2520next-token%2520predictions%2520remain%250Alargely%2520unexplored.%2520This%2520study%2520investigates%2520two%2520critical%2520components%2520believed%2520to%250Ainfluence%2520this%2520uncertainty%253A%2520the%2520recently%2520discovered%2520entropy%2520neurons%2520and%2520a%2520new%250Aset%2520of%2520components%2520that%2520we%2520term%2520token%2520frequency%2520neurons.%2520Entropy%2520neurons%2520are%250Acharacterized%2520by%2520an%2520unusually%2520high%2520weight%2520norm%2520and%2520influence%2520the%2520final%2520layer%250Anormalization%2520%2528LayerNorm%2529%2520scale%2520to%2520effectively%2520scale%2520down%2520the%2520logits.%2520Our%2520work%250Ashows%2520that%2520entropy%2520neurons%2520operate%2520by%2520writing%2520onto%2520an%2520unembedding%2520null%2520space%252C%250Aallowing%2520them%2520to%2520impact%2520the%2520residual%2520stream%2520norm%2520with%2520minimal%2520direct%2520effect%2520on%250Athe%2520logits%2520themselves.%2520We%2520observe%2520the%2520presence%2520of%2520entropy%2520neurons%2520across%2520a%250Arange%2520of%2520models%252C%2520up%2520to%25207%2520billion%2520parameters.%2520On%2520the%2520other%2520hand%252C%2520token%2520frequency%250Aneurons%252C%2520which%2520we%2520discover%2520and%2520describe%2520here%2520for%2520the%2520first%2520time%252C%2520boost%2520or%250Asuppress%2520each%2520token%2527s%2520logit%2520proportionally%2520to%2520its%2520log%2520frequency%252C%2520thereby%250Ashifting%2520the%2520output%2520distribution%2520towards%2520or%2520away%2520from%2520the%2520unigram%2520distribution.%250AFinally%252C%2520we%2520present%2520a%2520detailed%2520case%2520study%2520where%2520entropy%2520neurons%2520actively%2520manage%250Aconfidence%2520in%2520the%2520setting%2520of%2520induction%252C%2520i.e.%2520detecting%2520and%2520continuing%2520repeated%250Asubsequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16254v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Confidence%20Regulation%20Neurons%20in%20Language%20Models&entry.906535625=Alessandro%20Stolfo%20and%20Ben%20Wu%20and%20Wes%20Gurnee%20and%20Yonatan%20Belinkov%20and%20Xingyi%20Song%20and%20Mrinmaya%20Sachan%20and%20Neel%20Nanda&entry.1292438233=%20%20Despite%20their%20widespread%20use%2C%20the%20mechanisms%20by%20which%20large%20language%20models%0A%28LLMs%29%20represent%20and%20regulate%20uncertainty%20in%20next-token%20predictions%20remain%0Alargely%20unexplored.%20This%20study%20investigates%20two%20critical%20components%20believed%20to%0Ainfluence%20this%20uncertainty%3A%20the%20recently%20discovered%20entropy%20neurons%20and%20a%20new%0Aset%20of%20components%20that%20we%20term%20token%20frequency%20neurons.%20Entropy%20neurons%20are%0Acharacterized%20by%20an%20unusually%20high%20weight%20norm%20and%20influence%20the%20final%20layer%0Anormalization%20%28LayerNorm%29%20scale%20to%20effectively%20scale%20down%20the%20logits.%20Our%20work%0Ashows%20that%20entropy%20neurons%20operate%20by%20writing%20onto%20an%20unembedding%20null%20space%2C%0Aallowing%20them%20to%20impact%20the%20residual%20stream%20norm%20with%20minimal%20direct%20effect%20on%0Athe%20logits%20themselves.%20We%20observe%20the%20presence%20of%20entropy%20neurons%20across%20a%0Arange%20of%20models%2C%20up%20to%207%20billion%20parameters.%20On%20the%20other%20hand%2C%20token%20frequency%0Aneurons%2C%20which%20we%20discover%20and%20describe%20here%20for%20the%20first%20time%2C%20boost%20or%0Asuppress%20each%20token%27s%20logit%20proportionally%20to%20its%20log%20frequency%2C%20thereby%0Ashifting%20the%20output%20distribution%20towards%20or%20away%20from%20the%20unigram%20distribution.%0AFinally%2C%20we%20present%20a%20detailed%20case%20study%20where%20entropy%20neurons%20actively%20manage%0Aconfidence%20in%20the%20setting%20of%20induction%2C%20i.e.%20detecting%20and%20continuing%20repeated%0Asubsequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16254v2&entry.124074799=Read"},
{"title": "YOSO: You-Only-Sample-Once via Compressed Sensing for Graph Neural\n  Network Training", "author": "Yi Li and Zhichun Guo and Guanpeng Li and Bingzhe Li", "abstract": "  Graph neural networks (GNNs) have become essential tools for analyzing\nnon-Euclidean data across various domains. During training stage, sampling\nplays an important role in reducing latency by limiting the number of nodes\nprocessed, particularly in large-scale applications. However, as the demand for\nbetter prediction performance grows, existing sampling algorithms become\nincreasingly complex, leading to significant overhead. To mitigate this, we\npropose YOSO (You-Only-Sample-Once), an algorithm designed to achieve efficient\ntraining while preserving prediction accuracy. YOSO introduces a compressed\nsensing (CS)-based sampling and reconstruction framework, where nodes are\nsampled once at input layer, followed by a lossless reconstruction at the\noutput layer per epoch. By integrating the reconstruction process with the loss\nfunction of specific learning tasks, YOSO not only avoids costly computations\nin traditional compressed sensing (CS) methods, such as orthonormal basis\ncalculations, but also ensures high-probability accuracy retention which\nequivalent to full node participation. Experimental results on node\nclassification and link prediction demonstrate the effectiveness and efficiency\nof YOSO, reducing GNN training by an average of 75\\% compared to\nstate-of-the-art methods, while maintaining accuracy on par with top-performing\nbaselines.\n", "link": "http://arxiv.org/abs/2411.05693v1", "date": "2024-11-08", "relevancy": 2.3714, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4776}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4731}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YOSO%3A%20You-Only-Sample-Once%20via%20Compressed%20Sensing%20for%20Graph%20Neural%0A%20%20Network%20Training&body=Title%3A%20YOSO%3A%20You-Only-Sample-Once%20via%20Compressed%20Sensing%20for%20Graph%20Neural%0A%20%20Network%20Training%0AAuthor%3A%20Yi%20Li%20and%20Zhichun%20Guo%20and%20Guanpeng%20Li%20and%20Bingzhe%20Li%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20become%20essential%20tools%20for%20analyzing%0Anon-Euclidean%20data%20across%20various%20domains.%20During%20training%20stage%2C%20sampling%0Aplays%20an%20important%20role%20in%20reducing%20latency%20by%20limiting%20the%20number%20of%20nodes%0Aprocessed%2C%20particularly%20in%20large-scale%20applications.%20However%2C%20as%20the%20demand%20for%0Abetter%20prediction%20performance%20grows%2C%20existing%20sampling%20algorithms%20become%0Aincreasingly%20complex%2C%20leading%20to%20significant%20overhead.%20To%20mitigate%20this%2C%20we%0Apropose%20YOSO%20%28You-Only-Sample-Once%29%2C%20an%20algorithm%20designed%20to%20achieve%20efficient%0Atraining%20while%20preserving%20prediction%20accuracy.%20YOSO%20introduces%20a%20compressed%0Asensing%20%28CS%29-based%20sampling%20and%20reconstruction%20framework%2C%20where%20nodes%20are%0Asampled%20once%20at%20input%20layer%2C%20followed%20by%20a%20lossless%20reconstruction%20at%20the%0Aoutput%20layer%20per%20epoch.%20By%20integrating%20the%20reconstruction%20process%20with%20the%20loss%0Afunction%20of%20specific%20learning%20tasks%2C%20YOSO%20not%20only%20avoids%20costly%20computations%0Ain%20traditional%20compressed%20sensing%20%28CS%29%20methods%2C%20such%20as%20orthonormal%20basis%0Acalculations%2C%20but%20also%20ensures%20high-probability%20accuracy%20retention%20which%0Aequivalent%20to%20full%20node%20participation.%20Experimental%20results%20on%20node%0Aclassification%20and%20link%20prediction%20demonstrate%20the%20effectiveness%20and%20efficiency%0Aof%20YOSO%2C%20reducing%20GNN%20training%20by%20an%20average%20of%2075%5C%25%20compared%20to%0Astate-of-the-art%20methods%2C%20while%20maintaining%20accuracy%20on%20par%20with%20top-performing%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYOSO%253A%2520You-Only-Sample-Once%2520via%2520Compressed%2520Sensing%2520for%2520Graph%2520Neural%250A%2520%2520Network%2520Training%26entry.906535625%3DYi%2520Li%2520and%2520Zhichun%2520Guo%2520and%2520Guanpeng%2520Li%2520and%2520Bingzhe%2520Li%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520become%2520essential%2520tools%2520for%2520analyzing%250Anon-Euclidean%2520data%2520across%2520various%2520domains.%2520During%2520training%2520stage%252C%2520sampling%250Aplays%2520an%2520important%2520role%2520in%2520reducing%2520latency%2520by%2520limiting%2520the%2520number%2520of%2520nodes%250Aprocessed%252C%2520particularly%2520in%2520large-scale%2520applications.%2520However%252C%2520as%2520the%2520demand%2520for%250Abetter%2520prediction%2520performance%2520grows%252C%2520existing%2520sampling%2520algorithms%2520become%250Aincreasingly%2520complex%252C%2520leading%2520to%2520significant%2520overhead.%2520To%2520mitigate%2520this%252C%2520we%250Apropose%2520YOSO%2520%2528You-Only-Sample-Once%2529%252C%2520an%2520algorithm%2520designed%2520to%2520achieve%2520efficient%250Atraining%2520while%2520preserving%2520prediction%2520accuracy.%2520YOSO%2520introduces%2520a%2520compressed%250Asensing%2520%2528CS%2529-based%2520sampling%2520and%2520reconstruction%2520framework%252C%2520where%2520nodes%2520are%250Asampled%2520once%2520at%2520input%2520layer%252C%2520followed%2520by%2520a%2520lossless%2520reconstruction%2520at%2520the%250Aoutput%2520layer%2520per%2520epoch.%2520By%2520integrating%2520the%2520reconstruction%2520process%2520with%2520the%2520loss%250Afunction%2520of%2520specific%2520learning%2520tasks%252C%2520YOSO%2520not%2520only%2520avoids%2520costly%2520computations%250Ain%2520traditional%2520compressed%2520sensing%2520%2528CS%2529%2520methods%252C%2520such%2520as%2520orthonormal%2520basis%250Acalculations%252C%2520but%2520also%2520ensures%2520high-probability%2520accuracy%2520retention%2520which%250Aequivalent%2520to%2520full%2520node%2520participation.%2520Experimental%2520results%2520on%2520node%250Aclassification%2520and%2520link%2520prediction%2520demonstrate%2520the%2520effectiveness%2520and%2520efficiency%250Aof%2520YOSO%252C%2520reducing%2520GNN%2520training%2520by%2520an%2520average%2520of%252075%255C%2525%2520compared%2520to%250Astate-of-the-art%2520methods%252C%2520while%2520maintaining%2520accuracy%2520on%2520par%2520with%2520top-performing%250Abaselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YOSO%3A%20You-Only-Sample-Once%20via%20Compressed%20Sensing%20for%20Graph%20Neural%0A%20%20Network%20Training&entry.906535625=Yi%20Li%20and%20Zhichun%20Guo%20and%20Guanpeng%20Li%20and%20Bingzhe%20Li&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20become%20essential%20tools%20for%20analyzing%0Anon-Euclidean%20data%20across%20various%20domains.%20During%20training%20stage%2C%20sampling%0Aplays%20an%20important%20role%20in%20reducing%20latency%20by%20limiting%20the%20number%20of%20nodes%0Aprocessed%2C%20particularly%20in%20large-scale%20applications.%20However%2C%20as%20the%20demand%20for%0Abetter%20prediction%20performance%20grows%2C%20existing%20sampling%20algorithms%20become%0Aincreasingly%20complex%2C%20leading%20to%20significant%20overhead.%20To%20mitigate%20this%2C%20we%0Apropose%20YOSO%20%28You-Only-Sample-Once%29%2C%20an%20algorithm%20designed%20to%20achieve%20efficient%0Atraining%20while%20preserving%20prediction%20accuracy.%20YOSO%20introduces%20a%20compressed%0Asensing%20%28CS%29-based%20sampling%20and%20reconstruction%20framework%2C%20where%20nodes%20are%0Asampled%20once%20at%20input%20layer%2C%20followed%20by%20a%20lossless%20reconstruction%20at%20the%0Aoutput%20layer%20per%20epoch.%20By%20integrating%20the%20reconstruction%20process%20with%20the%20loss%0Afunction%20of%20specific%20learning%20tasks%2C%20YOSO%20not%20only%20avoids%20costly%20computations%0Ain%20traditional%20compressed%20sensing%20%28CS%29%20methods%2C%20such%20as%20orthonormal%20basis%0Acalculations%2C%20but%20also%20ensures%20high-probability%20accuracy%20retention%20which%0Aequivalent%20to%20full%20node%20participation.%20Experimental%20results%20on%20node%0Aclassification%20and%20link%20prediction%20demonstrate%20the%20effectiveness%20and%20efficiency%0Aof%20YOSO%2C%20reducing%20GNN%20training%20by%20an%20average%20of%2075%5C%25%20compared%20to%0Astate-of-the-art%20methods%2C%20while%20maintaining%20accuracy%20on%20par%20with%20top-performing%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05693v1&entry.124074799=Read"},
{"title": "Tell What You Hear From What You See -- Video to Audio Generation\n  Through Text", "author": "Xiulong Liu and Kun Su and Eli Shlizerman", "abstract": "  The content of visual and audio scenes is multi-faceted such that a video can\nbe paired with various audio and vice-versa. Thereby, in video-to-audio\ngeneration task, it is imperative to introduce steering approaches for\ncontrolling the generated audio. While Video-to-Audio generation is a\nwell-established generative task, existing methods lack such controllability.\nIn this work, we propose VATT, a multi-modal generative framework that takes a\nvideo and an optional text prompt as input, and generates audio and optional\ntextual description of the audio. Such a framework has two advantages: i)\nVideo-to-Audio generation process can be refined and controlled via text which\ncomplements the context of visual information, and ii) The model can suggest\nwhat audio to generate for the video by generating audio captions. VATT\nconsists of two key modules: VATT Converter, a LLM that is fine-tuned for\ninstructions and includes a projection layer that maps video features to the\nLLM vector space; and VATT Audio, a transformer that generates audio tokens\nfrom visual frames and from optional text prompt using iterative parallel\ndecoding. The audio tokens are converted to a waveform by pretrained neural\ncodec. Experiments show that when VATT is compared to existing video-to-audio\ngeneration methods in objective metrics, it achieves competitive performance\nwhen the audio caption is not provided. When the audio caption is provided as a\nprompt, VATT achieves even more refined performance (lowest KLD score of 1.41).\nFurthermore, subjective studies show that VATT Audio has been chosen as\npreferred generated audio than audio generated by existing methods. VATT\nenables controllable video-to-audio generation through text as well as\nsuggesting text prompts for videos through audio captions, unlocking novel\napplications such as text-guided video-to-audio generation and video-to-audio\ncaptioning.\n", "link": "http://arxiv.org/abs/2411.05679v1", "date": "2024-11-08", "relevancy": 2.3652, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6218}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6138}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tell%20What%20You%20Hear%20From%20What%20You%20See%20--%20Video%20to%20Audio%20Generation%0A%20%20Through%20Text&body=Title%3A%20Tell%20What%20You%20Hear%20From%20What%20You%20See%20--%20Video%20to%20Audio%20Generation%0A%20%20Through%20Text%0AAuthor%3A%20Xiulong%20Liu%20and%20Kun%20Su%20and%20Eli%20Shlizerman%0AAbstract%3A%20%20%20The%20content%20of%20visual%20and%20audio%20scenes%20is%20multi-faceted%20such%20that%20a%20video%20can%0Abe%20paired%20with%20various%20audio%20and%20vice-versa.%20Thereby%2C%20in%20video-to-audio%0Ageneration%20task%2C%20it%20is%20imperative%20to%20introduce%20steering%20approaches%20for%0Acontrolling%20the%20generated%20audio.%20While%20Video-to-Audio%20generation%20is%20a%0Awell-established%20generative%20task%2C%20existing%20methods%20lack%20such%20controllability.%0AIn%20this%20work%2C%20we%20propose%20VATT%2C%20a%20multi-modal%20generative%20framework%20that%20takes%20a%0Avideo%20and%20an%20optional%20text%20prompt%20as%20input%2C%20and%20generates%20audio%20and%20optional%0Atextual%20description%20of%20the%20audio.%20Such%20a%20framework%20has%20two%20advantages%3A%20i%29%0AVideo-to-Audio%20generation%20process%20can%20be%20refined%20and%20controlled%20via%20text%20which%0Acomplements%20the%20context%20of%20visual%20information%2C%20and%20ii%29%20The%20model%20can%20suggest%0Awhat%20audio%20to%20generate%20for%20the%20video%20by%20generating%20audio%20captions.%20VATT%0Aconsists%20of%20two%20key%20modules%3A%20VATT%20Converter%2C%20a%20LLM%20that%20is%20fine-tuned%20for%0Ainstructions%20and%20includes%20a%20projection%20layer%20that%20maps%20video%20features%20to%20the%0ALLM%20vector%20space%3B%20and%20VATT%20Audio%2C%20a%20transformer%20that%20generates%20audio%20tokens%0Afrom%20visual%20frames%20and%20from%20optional%20text%20prompt%20using%20iterative%20parallel%0Adecoding.%20The%20audio%20tokens%20are%20converted%20to%20a%20waveform%20by%20pretrained%20neural%0Acodec.%20Experiments%20show%20that%20when%20VATT%20is%20compared%20to%20existing%20video-to-audio%0Ageneration%20methods%20in%20objective%20metrics%2C%20it%20achieves%20competitive%20performance%0Awhen%20the%20audio%20caption%20is%20not%20provided.%20When%20the%20audio%20caption%20is%20provided%20as%20a%0Aprompt%2C%20VATT%20achieves%20even%20more%20refined%20performance%20%28lowest%20KLD%20score%20of%201.41%29.%0AFurthermore%2C%20subjective%20studies%20show%20that%20VATT%20Audio%20has%20been%20chosen%20as%0Apreferred%20generated%20audio%20than%20audio%20generated%20by%20existing%20methods.%20VATT%0Aenables%20controllable%20video-to-audio%20generation%20through%20text%20as%20well%20as%0Asuggesting%20text%20prompts%20for%20videos%20through%20audio%20captions%2C%20unlocking%20novel%0Aapplications%20such%20as%20text-guided%20video-to-audio%20generation%20and%20video-to-audio%0Acaptioning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTell%2520What%2520You%2520Hear%2520From%2520What%2520You%2520See%2520--%2520Video%2520to%2520Audio%2520Generation%250A%2520%2520Through%2520Text%26entry.906535625%3DXiulong%2520Liu%2520and%2520Kun%2520Su%2520and%2520Eli%2520Shlizerman%26entry.1292438233%3D%2520%2520The%2520content%2520of%2520visual%2520and%2520audio%2520scenes%2520is%2520multi-faceted%2520such%2520that%2520a%2520video%2520can%250Abe%2520paired%2520with%2520various%2520audio%2520and%2520vice-versa.%2520Thereby%252C%2520in%2520video-to-audio%250Ageneration%2520task%252C%2520it%2520is%2520imperative%2520to%2520introduce%2520steering%2520approaches%2520for%250Acontrolling%2520the%2520generated%2520audio.%2520While%2520Video-to-Audio%2520generation%2520is%2520a%250Awell-established%2520generative%2520task%252C%2520existing%2520methods%2520lack%2520such%2520controllability.%250AIn%2520this%2520work%252C%2520we%2520propose%2520VATT%252C%2520a%2520multi-modal%2520generative%2520framework%2520that%2520takes%2520a%250Avideo%2520and%2520an%2520optional%2520text%2520prompt%2520as%2520input%252C%2520and%2520generates%2520audio%2520and%2520optional%250Atextual%2520description%2520of%2520the%2520audio.%2520Such%2520a%2520framework%2520has%2520two%2520advantages%253A%2520i%2529%250AVideo-to-Audio%2520generation%2520process%2520can%2520be%2520refined%2520and%2520controlled%2520via%2520text%2520which%250Acomplements%2520the%2520context%2520of%2520visual%2520information%252C%2520and%2520ii%2529%2520The%2520model%2520can%2520suggest%250Awhat%2520audio%2520to%2520generate%2520for%2520the%2520video%2520by%2520generating%2520audio%2520captions.%2520VATT%250Aconsists%2520of%2520two%2520key%2520modules%253A%2520VATT%2520Converter%252C%2520a%2520LLM%2520that%2520is%2520fine-tuned%2520for%250Ainstructions%2520and%2520includes%2520a%2520projection%2520layer%2520that%2520maps%2520video%2520features%2520to%2520the%250ALLM%2520vector%2520space%253B%2520and%2520VATT%2520Audio%252C%2520a%2520transformer%2520that%2520generates%2520audio%2520tokens%250Afrom%2520visual%2520frames%2520and%2520from%2520optional%2520text%2520prompt%2520using%2520iterative%2520parallel%250Adecoding.%2520The%2520audio%2520tokens%2520are%2520converted%2520to%2520a%2520waveform%2520by%2520pretrained%2520neural%250Acodec.%2520Experiments%2520show%2520that%2520when%2520VATT%2520is%2520compared%2520to%2520existing%2520video-to-audio%250Ageneration%2520methods%2520in%2520objective%2520metrics%252C%2520it%2520achieves%2520competitive%2520performance%250Awhen%2520the%2520audio%2520caption%2520is%2520not%2520provided.%2520When%2520the%2520audio%2520caption%2520is%2520provided%2520as%2520a%250Aprompt%252C%2520VATT%2520achieves%2520even%2520more%2520refined%2520performance%2520%2528lowest%2520KLD%2520score%2520of%25201.41%2529.%250AFurthermore%252C%2520subjective%2520studies%2520show%2520that%2520VATT%2520Audio%2520has%2520been%2520chosen%2520as%250Apreferred%2520generated%2520audio%2520than%2520audio%2520generated%2520by%2520existing%2520methods.%2520VATT%250Aenables%2520controllable%2520video-to-audio%2520generation%2520through%2520text%2520as%2520well%2520as%250Asuggesting%2520text%2520prompts%2520for%2520videos%2520through%2520audio%2520captions%252C%2520unlocking%2520novel%250Aapplications%2520such%2520as%2520text-guided%2520video-to-audio%2520generation%2520and%2520video-to-audio%250Acaptioning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tell%20What%20You%20Hear%20From%20What%20You%20See%20--%20Video%20to%20Audio%20Generation%0A%20%20Through%20Text&entry.906535625=Xiulong%20Liu%20and%20Kun%20Su%20and%20Eli%20Shlizerman&entry.1292438233=%20%20The%20content%20of%20visual%20and%20audio%20scenes%20is%20multi-faceted%20such%20that%20a%20video%20can%0Abe%20paired%20with%20various%20audio%20and%20vice-versa.%20Thereby%2C%20in%20video-to-audio%0Ageneration%20task%2C%20it%20is%20imperative%20to%20introduce%20steering%20approaches%20for%0Acontrolling%20the%20generated%20audio.%20While%20Video-to-Audio%20generation%20is%20a%0Awell-established%20generative%20task%2C%20existing%20methods%20lack%20such%20controllability.%0AIn%20this%20work%2C%20we%20propose%20VATT%2C%20a%20multi-modal%20generative%20framework%20that%20takes%20a%0Avideo%20and%20an%20optional%20text%20prompt%20as%20input%2C%20and%20generates%20audio%20and%20optional%0Atextual%20description%20of%20the%20audio.%20Such%20a%20framework%20has%20two%20advantages%3A%20i%29%0AVideo-to-Audio%20generation%20process%20can%20be%20refined%20and%20controlled%20via%20text%20which%0Acomplements%20the%20context%20of%20visual%20information%2C%20and%20ii%29%20The%20model%20can%20suggest%0Awhat%20audio%20to%20generate%20for%20the%20video%20by%20generating%20audio%20captions.%20VATT%0Aconsists%20of%20two%20key%20modules%3A%20VATT%20Converter%2C%20a%20LLM%20that%20is%20fine-tuned%20for%0Ainstructions%20and%20includes%20a%20projection%20layer%20that%20maps%20video%20features%20to%20the%0ALLM%20vector%20space%3B%20and%20VATT%20Audio%2C%20a%20transformer%20that%20generates%20audio%20tokens%0Afrom%20visual%20frames%20and%20from%20optional%20text%20prompt%20using%20iterative%20parallel%0Adecoding.%20The%20audio%20tokens%20are%20converted%20to%20a%20waveform%20by%20pretrained%20neural%0Acodec.%20Experiments%20show%20that%20when%20VATT%20is%20compared%20to%20existing%20video-to-audio%0Ageneration%20methods%20in%20objective%20metrics%2C%20it%20achieves%20competitive%20performance%0Awhen%20the%20audio%20caption%20is%20not%20provided.%20When%20the%20audio%20caption%20is%20provided%20as%20a%0Aprompt%2C%20VATT%20achieves%20even%20more%20refined%20performance%20%28lowest%20KLD%20score%20of%201.41%29.%0AFurthermore%2C%20subjective%20studies%20show%20that%20VATT%20Audio%20has%20been%20chosen%20as%0Apreferred%20generated%20audio%20than%20audio%20generated%20by%20existing%20methods.%20VATT%0Aenables%20controllable%20video-to-audio%20generation%20through%20text%20as%20well%20as%0Asuggesting%20text%20prompts%20for%20videos%20through%20audio%20captions%2C%20unlocking%20novel%0Aapplications%20such%20as%20text-guided%20video-to-audio%20generation%20and%20video-to-audio%0Acaptioning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05679v1&entry.124074799=Read"},
{"title": "SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark", "author": "Sithursan Sivasubramaniam and Cedric Osei-Akoto and Yi Zhang and Kurt Stockinger and Jonathan Fuerst", "abstract": "  Electronic health records (EHRs) are stored in various database systems with\ndifferent database models on heterogeneous storage architectures, such as\nrelational databases, document stores, or graph databases. These different\ndatabase models have a big impact on query complexity and performance. While\nthis has been a known fact in database research, its implications for the\ngrowing number of Text-to-Query systems have surprisingly not been investigated\nso far. In this paper, we present SM3-Text-to-Query, the first multi-model\nmedical Text-to-Query benchmark based on synthetic patient data from Synthea,\nfollowing the SNOMED-CT taxonomy -- a widely used knowledge graph ontology\ncovering medical terminology. SM3-Text-to-Query provides data representations\nfor relational databases (PostgreSQL), document stores (MongoDB), and graph\ndatabases (Neo4j and GraphDB (RDF)), allowing the evaluation across four\npopular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically\nand manually develop 408 template questions, which we augment to construct a\nbenchmark of 10K diverse natural language question/query pairs for these four\nquery languages (40K pairs overall). On our dataset, we evaluate several common\nin-context-learning (ICL) approaches for a set of representative closed and\nopen-source LLMs. Our evaluation sheds light on the trade-offs between database\nmodels and query languages for different ICL strategies and LLMs. Last,\nSM3-Text-to-Query is easily extendable to additional query languages or real,\nstandard-based patient databases.\n", "link": "http://arxiv.org/abs/2411.05521v1", "date": "2024-11-08", "relevancy": 2.3203, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4795}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4795}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SM3-Text-to-Query%3A%20Synthetic%20Multi-Model%20Medical%20Text-to-Query%20Benchmark&body=Title%3A%20SM3-Text-to-Query%3A%20Synthetic%20Multi-Model%20Medical%20Text-to-Query%20Benchmark%0AAuthor%3A%20Sithursan%20Sivasubramaniam%20and%20Cedric%20Osei-Akoto%20and%20Yi%20Zhang%20and%20Kurt%20Stockinger%20and%20Jonathan%20Fuerst%0AAbstract%3A%20%20%20Electronic%20health%20records%20%28EHRs%29%20are%20stored%20in%20various%20database%20systems%20with%0Adifferent%20database%20models%20on%20heterogeneous%20storage%20architectures%2C%20such%20as%0Arelational%20databases%2C%20document%20stores%2C%20or%20graph%20databases.%20These%20different%0Adatabase%20models%20have%20a%20big%20impact%20on%20query%20complexity%20and%20performance.%20While%0Athis%20has%20been%20a%20known%20fact%20in%20database%20research%2C%20its%20implications%20for%20the%0Agrowing%20number%20of%20Text-to-Query%20systems%20have%20surprisingly%20not%20been%20investigated%0Aso%20far.%20In%20this%20paper%2C%20we%20present%20SM3-Text-to-Query%2C%20the%20first%20multi-model%0Amedical%20Text-to-Query%20benchmark%20based%20on%20synthetic%20patient%20data%20from%20Synthea%2C%0Afollowing%20the%20SNOMED-CT%20taxonomy%20--%20a%20widely%20used%20knowledge%20graph%20ontology%0Acovering%20medical%20terminology.%20SM3-Text-to-Query%20provides%20data%20representations%0Afor%20relational%20databases%20%28PostgreSQL%29%2C%20document%20stores%20%28MongoDB%29%2C%20and%20graph%0Adatabases%20%28Neo4j%20and%20GraphDB%20%28RDF%29%29%2C%20allowing%20the%20evaluation%20across%20four%0Apopular%20query%20languages%2C%20namely%20SQL%2C%20MQL%2C%20Cypher%2C%20and%20SPARQL.%20We%20systematically%0Aand%20manually%20develop%20408%20template%20questions%2C%20which%20we%20augment%20to%20construct%20a%0Abenchmark%20of%2010K%20diverse%20natural%20language%20question/query%20pairs%20for%20these%20four%0Aquery%20languages%20%2840K%20pairs%20overall%29.%20On%20our%20dataset%2C%20we%20evaluate%20several%20common%0Ain-context-learning%20%28ICL%29%20approaches%20for%20a%20set%20of%20representative%20closed%20and%0Aopen-source%20LLMs.%20Our%20evaluation%20sheds%20light%20on%20the%20trade-offs%20between%20database%0Amodels%20and%20query%20languages%20for%20different%20ICL%20strategies%20and%20LLMs.%20Last%2C%0ASM3-Text-to-Query%20is%20easily%20extendable%20to%20additional%20query%20languages%20or%20real%2C%0Astandard-based%20patient%20databases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSM3-Text-to-Query%253A%2520Synthetic%2520Multi-Model%2520Medical%2520Text-to-Query%2520Benchmark%26entry.906535625%3DSithursan%2520Sivasubramaniam%2520and%2520Cedric%2520Osei-Akoto%2520and%2520Yi%2520Zhang%2520and%2520Kurt%2520Stockinger%2520and%2520Jonathan%2520Fuerst%26entry.1292438233%3D%2520%2520Electronic%2520health%2520records%2520%2528EHRs%2529%2520are%2520stored%2520in%2520various%2520database%2520systems%2520with%250Adifferent%2520database%2520models%2520on%2520heterogeneous%2520storage%2520architectures%252C%2520such%2520as%250Arelational%2520databases%252C%2520document%2520stores%252C%2520or%2520graph%2520databases.%2520These%2520different%250Adatabase%2520models%2520have%2520a%2520big%2520impact%2520on%2520query%2520complexity%2520and%2520performance.%2520While%250Athis%2520has%2520been%2520a%2520known%2520fact%2520in%2520database%2520research%252C%2520its%2520implications%2520for%2520the%250Agrowing%2520number%2520of%2520Text-to-Query%2520systems%2520have%2520surprisingly%2520not%2520been%2520investigated%250Aso%2520far.%2520In%2520this%2520paper%252C%2520we%2520present%2520SM3-Text-to-Query%252C%2520the%2520first%2520multi-model%250Amedical%2520Text-to-Query%2520benchmark%2520based%2520on%2520synthetic%2520patient%2520data%2520from%2520Synthea%252C%250Afollowing%2520the%2520SNOMED-CT%2520taxonomy%2520--%2520a%2520widely%2520used%2520knowledge%2520graph%2520ontology%250Acovering%2520medical%2520terminology.%2520SM3-Text-to-Query%2520provides%2520data%2520representations%250Afor%2520relational%2520databases%2520%2528PostgreSQL%2529%252C%2520document%2520stores%2520%2528MongoDB%2529%252C%2520and%2520graph%250Adatabases%2520%2528Neo4j%2520and%2520GraphDB%2520%2528RDF%2529%2529%252C%2520allowing%2520the%2520evaluation%2520across%2520four%250Apopular%2520query%2520languages%252C%2520namely%2520SQL%252C%2520MQL%252C%2520Cypher%252C%2520and%2520SPARQL.%2520We%2520systematically%250Aand%2520manually%2520develop%2520408%2520template%2520questions%252C%2520which%2520we%2520augment%2520to%2520construct%2520a%250Abenchmark%2520of%252010K%2520diverse%2520natural%2520language%2520question/query%2520pairs%2520for%2520these%2520four%250Aquery%2520languages%2520%252840K%2520pairs%2520overall%2529.%2520On%2520our%2520dataset%252C%2520we%2520evaluate%2520several%2520common%250Ain-context-learning%2520%2528ICL%2529%2520approaches%2520for%2520a%2520set%2520of%2520representative%2520closed%2520and%250Aopen-source%2520LLMs.%2520Our%2520evaluation%2520sheds%2520light%2520on%2520the%2520trade-offs%2520between%2520database%250Amodels%2520and%2520query%2520languages%2520for%2520different%2520ICL%2520strategies%2520and%2520LLMs.%2520Last%252C%250ASM3-Text-to-Query%2520is%2520easily%2520extendable%2520to%2520additional%2520query%2520languages%2520or%2520real%252C%250Astandard-based%2520patient%2520databases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SM3-Text-to-Query%3A%20Synthetic%20Multi-Model%20Medical%20Text-to-Query%20Benchmark&entry.906535625=Sithursan%20Sivasubramaniam%20and%20Cedric%20Osei-Akoto%20and%20Yi%20Zhang%20and%20Kurt%20Stockinger%20and%20Jonathan%20Fuerst&entry.1292438233=%20%20Electronic%20health%20records%20%28EHRs%29%20are%20stored%20in%20various%20database%20systems%20with%0Adifferent%20database%20models%20on%20heterogeneous%20storage%20architectures%2C%20such%20as%0Arelational%20databases%2C%20document%20stores%2C%20or%20graph%20databases.%20These%20different%0Adatabase%20models%20have%20a%20big%20impact%20on%20query%20complexity%20and%20performance.%20While%0Athis%20has%20been%20a%20known%20fact%20in%20database%20research%2C%20its%20implications%20for%20the%0Agrowing%20number%20of%20Text-to-Query%20systems%20have%20surprisingly%20not%20been%20investigated%0Aso%20far.%20In%20this%20paper%2C%20we%20present%20SM3-Text-to-Query%2C%20the%20first%20multi-model%0Amedical%20Text-to-Query%20benchmark%20based%20on%20synthetic%20patient%20data%20from%20Synthea%2C%0Afollowing%20the%20SNOMED-CT%20taxonomy%20--%20a%20widely%20used%20knowledge%20graph%20ontology%0Acovering%20medical%20terminology.%20SM3-Text-to-Query%20provides%20data%20representations%0Afor%20relational%20databases%20%28PostgreSQL%29%2C%20document%20stores%20%28MongoDB%29%2C%20and%20graph%0Adatabases%20%28Neo4j%20and%20GraphDB%20%28RDF%29%29%2C%20allowing%20the%20evaluation%20across%20four%0Apopular%20query%20languages%2C%20namely%20SQL%2C%20MQL%2C%20Cypher%2C%20and%20SPARQL.%20We%20systematically%0Aand%20manually%20develop%20408%20template%20questions%2C%20which%20we%20augment%20to%20construct%20a%0Abenchmark%20of%2010K%20diverse%20natural%20language%20question/query%20pairs%20for%20these%20four%0Aquery%20languages%20%2840K%20pairs%20overall%29.%20On%20our%20dataset%2C%20we%20evaluate%20several%20common%0Ain-context-learning%20%28ICL%29%20approaches%20for%20a%20set%20of%20representative%20closed%20and%0Aopen-source%20LLMs.%20Our%20evaluation%20sheds%20light%20on%20the%20trade-offs%20between%20database%0Amodels%20and%20query%20languages%20for%20different%20ICL%20strategies%20and%20LLMs.%20Last%2C%0ASM3-Text-to-Query%20is%20easily%20extendable%20to%20additional%20query%20languages%20or%20real%2C%0Astandard-based%20patient%20databases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05521v1&entry.124074799=Read"},
{"title": "Sample and Computationally Efficient Robust Learning of Gaussian\n  Single-Index Models", "author": "Puqian Wang and Nikos Zarifis and Ilias Diakonikolas and Jelena Diakonikolas", "abstract": "  A single-index model (SIM) is a function of the form\n$\\sigma(\\mathbf{w}^{\\ast} \\cdot \\mathbf{x})$, where $\\sigma: \\mathbb{R} \\to\n\\mathbb{R}$ is a known link function and $\\mathbf{w}^{\\ast}$ is a hidden unit\nvector. We study the task of learning SIMs in the agnostic (a.k.a. adversarial\nlabel noise) model with respect to the $L^2_2$-loss under the Gaussian\ndistribution. Our main result is a sample and computationally efficient\nagnostic proper learner that attains $L^2_2$-error of\n$O(\\mathrm{OPT})+\\epsilon$, where $\\mathrm{OPT}$ is the optimal loss. The\nsample complexity of our algorithm is $\\tilde{O}(d^{\\lceil\nk^{\\ast}/2\\rceil}+d/\\epsilon)$, where $k^{\\ast}$ is the information-exponent of\n$\\sigma$ corresponding to the degree of its first non-zero Hermite coefficient.\nThis sample bound nearly matches known CSQ lower bounds, even in the realizable\nsetting. Prior algorithmic work in this setting had focused on learning in the\nrealizable case or in the presence of semi-random noise. Prior computationally\nefficient robust learners required significantly stronger assumptions on the\nlink function.\n", "link": "http://arxiv.org/abs/2411.05708v1", "date": "2024-11-08", "relevancy": 2.3192, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4679}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4679}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sample%20and%20Computationally%20Efficient%20Robust%20Learning%20of%20Gaussian%0A%20%20Single-Index%20Models&body=Title%3A%20Sample%20and%20Computationally%20Efficient%20Robust%20Learning%20of%20Gaussian%0A%20%20Single-Index%20Models%0AAuthor%3A%20Puqian%20Wang%20and%20Nikos%20Zarifis%20and%20Ilias%20Diakonikolas%20and%20Jelena%20Diakonikolas%0AAbstract%3A%20%20%20A%20single-index%20model%20%28SIM%29%20is%20a%20function%20of%20the%20form%0A%24%5Csigma%28%5Cmathbf%7Bw%7D%5E%7B%5Cast%7D%20%5Ccdot%20%5Cmathbf%7Bx%7D%29%24%2C%20where%20%24%5Csigma%3A%20%5Cmathbb%7BR%7D%20%5Cto%0A%5Cmathbb%7BR%7D%24%20is%20a%20known%20link%20function%20and%20%24%5Cmathbf%7Bw%7D%5E%7B%5Cast%7D%24%20is%20a%20hidden%20unit%0Avector.%20We%20study%20the%20task%20of%20learning%20SIMs%20in%20the%20agnostic%20%28a.k.a.%20adversarial%0Alabel%20noise%29%20model%20with%20respect%20to%20the%20%24L%5E2_2%24-loss%20under%20the%20Gaussian%0Adistribution.%20Our%20main%20result%20is%20a%20sample%20and%20computationally%20efficient%0Aagnostic%20proper%20learner%20that%20attains%20%24L%5E2_2%24-error%20of%0A%24O%28%5Cmathrm%7BOPT%7D%29%2B%5Cepsilon%24%2C%20where%20%24%5Cmathrm%7BOPT%7D%24%20is%20the%20optimal%20loss.%20The%0Asample%20complexity%20of%20our%20algorithm%20is%20%24%5Ctilde%7BO%7D%28d%5E%7B%5Clceil%0Ak%5E%7B%5Cast%7D/2%5Crceil%7D%2Bd/%5Cepsilon%29%24%2C%20where%20%24k%5E%7B%5Cast%7D%24%20is%20the%20information-exponent%20of%0A%24%5Csigma%24%20corresponding%20to%20the%20degree%20of%20its%20first%20non-zero%20Hermite%20coefficient.%0AThis%20sample%20bound%20nearly%20matches%20known%20CSQ%20lower%20bounds%2C%20even%20in%20the%20realizable%0Asetting.%20Prior%20algorithmic%20work%20in%20this%20setting%20had%20focused%20on%20learning%20in%20the%0Arealizable%20case%20or%20in%20the%20presence%20of%20semi-random%20noise.%20Prior%20computationally%0Aefficient%20robust%20learners%20required%20significantly%20stronger%20assumptions%20on%20the%0Alink%20function.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05708v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSample%2520and%2520Computationally%2520Efficient%2520Robust%2520Learning%2520of%2520Gaussian%250A%2520%2520Single-Index%2520Models%26entry.906535625%3DPuqian%2520Wang%2520and%2520Nikos%2520Zarifis%2520and%2520Ilias%2520Diakonikolas%2520and%2520Jelena%2520Diakonikolas%26entry.1292438233%3D%2520%2520A%2520single-index%2520model%2520%2528SIM%2529%2520is%2520a%2520function%2520of%2520the%2520form%250A%2524%255Csigma%2528%255Cmathbf%257Bw%257D%255E%257B%255Cast%257D%2520%255Ccdot%2520%255Cmathbf%257Bx%257D%2529%2524%252C%2520where%2520%2524%255Csigma%253A%2520%255Cmathbb%257BR%257D%2520%255Cto%250A%255Cmathbb%257BR%257D%2524%2520is%2520a%2520known%2520link%2520function%2520and%2520%2524%255Cmathbf%257Bw%257D%255E%257B%255Cast%257D%2524%2520is%2520a%2520hidden%2520unit%250Avector.%2520We%2520study%2520the%2520task%2520of%2520learning%2520SIMs%2520in%2520the%2520agnostic%2520%2528a.k.a.%2520adversarial%250Alabel%2520noise%2529%2520model%2520with%2520respect%2520to%2520the%2520%2524L%255E2_2%2524-loss%2520under%2520the%2520Gaussian%250Adistribution.%2520Our%2520main%2520result%2520is%2520a%2520sample%2520and%2520computationally%2520efficient%250Aagnostic%2520proper%2520learner%2520that%2520attains%2520%2524L%255E2_2%2524-error%2520of%250A%2524O%2528%255Cmathrm%257BOPT%257D%2529%252B%255Cepsilon%2524%252C%2520where%2520%2524%255Cmathrm%257BOPT%257D%2524%2520is%2520the%2520optimal%2520loss.%2520The%250Asample%2520complexity%2520of%2520our%2520algorithm%2520is%2520%2524%255Ctilde%257BO%257D%2528d%255E%257B%255Clceil%250Ak%255E%257B%255Cast%257D/2%255Crceil%257D%252Bd/%255Cepsilon%2529%2524%252C%2520where%2520%2524k%255E%257B%255Cast%257D%2524%2520is%2520the%2520information-exponent%2520of%250A%2524%255Csigma%2524%2520corresponding%2520to%2520the%2520degree%2520of%2520its%2520first%2520non-zero%2520Hermite%2520coefficient.%250AThis%2520sample%2520bound%2520nearly%2520matches%2520known%2520CSQ%2520lower%2520bounds%252C%2520even%2520in%2520the%2520realizable%250Asetting.%2520Prior%2520algorithmic%2520work%2520in%2520this%2520setting%2520had%2520focused%2520on%2520learning%2520in%2520the%250Arealizable%2520case%2520or%2520in%2520the%2520presence%2520of%2520semi-random%2520noise.%2520Prior%2520computationally%250Aefficient%2520robust%2520learners%2520required%2520significantly%2520stronger%2520assumptions%2520on%2520the%250Alink%2520function.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05708v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sample%20and%20Computationally%20Efficient%20Robust%20Learning%20of%20Gaussian%0A%20%20Single-Index%20Models&entry.906535625=Puqian%20Wang%20and%20Nikos%20Zarifis%20and%20Ilias%20Diakonikolas%20and%20Jelena%20Diakonikolas&entry.1292438233=%20%20A%20single-index%20model%20%28SIM%29%20is%20a%20function%20of%20the%20form%0A%24%5Csigma%28%5Cmathbf%7Bw%7D%5E%7B%5Cast%7D%20%5Ccdot%20%5Cmathbf%7Bx%7D%29%24%2C%20where%20%24%5Csigma%3A%20%5Cmathbb%7BR%7D%20%5Cto%0A%5Cmathbb%7BR%7D%24%20is%20a%20known%20link%20function%20and%20%24%5Cmathbf%7Bw%7D%5E%7B%5Cast%7D%24%20is%20a%20hidden%20unit%0Avector.%20We%20study%20the%20task%20of%20learning%20SIMs%20in%20the%20agnostic%20%28a.k.a.%20adversarial%0Alabel%20noise%29%20model%20with%20respect%20to%20the%20%24L%5E2_2%24-loss%20under%20the%20Gaussian%0Adistribution.%20Our%20main%20result%20is%20a%20sample%20and%20computationally%20efficient%0Aagnostic%20proper%20learner%20that%20attains%20%24L%5E2_2%24-error%20of%0A%24O%28%5Cmathrm%7BOPT%7D%29%2B%5Cepsilon%24%2C%20where%20%24%5Cmathrm%7BOPT%7D%24%20is%20the%20optimal%20loss.%20The%0Asample%20complexity%20of%20our%20algorithm%20is%20%24%5Ctilde%7BO%7D%28d%5E%7B%5Clceil%0Ak%5E%7B%5Cast%7D/2%5Crceil%7D%2Bd/%5Cepsilon%29%24%2C%20where%20%24k%5E%7B%5Cast%7D%24%20is%20the%20information-exponent%20of%0A%24%5Csigma%24%20corresponding%20to%20the%20degree%20of%20its%20first%20non-zero%20Hermite%20coefficient.%0AThis%20sample%20bound%20nearly%20matches%20known%20CSQ%20lower%20bounds%2C%20even%20in%20the%20realizable%0Asetting.%20Prior%20algorithmic%20work%20in%20this%20setting%20had%20focused%20on%20learning%20in%20the%0Arealizable%20case%20or%20in%20the%20presence%20of%20semi-random%20noise.%20Prior%20computationally%0Aefficient%20robust%20learners%20required%20significantly%20stronger%20assumptions%20on%20the%0Alink%20function.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05708v1&entry.124074799=Read"},
{"title": "Logits of API-Protected LLMs Leak Proprietary Information", "author": "Matthew Finlayson and Xiang Ren and Swabha Swayamdipta", "abstract": "  Large language model (LLM) providers often hide the architectural details and\nparameters of their proprietary models by restricting public access to a\nlimited API. In this work we show that, with only a conservative assumption\nabout the model architecture, it is possible to learn a surprisingly large\namount of non-public information about an API-protected LLM from a relatively\nsmall number of API queries (e.g., costing under $1000 USD for OpenAI's\ngpt-3.5-turbo). Our findings are centered on one key observation: most modern\nLLMs suffer from a softmax bottleneck, which restricts the model outputs to a\nlinear subspace of the full output space. We exploit this fact to unlock\nseveral capabilities, including (but not limited to) obtaining cheap\nfull-vocabulary outputs, auditing for specific types of model updates,\nidentifying the source LLM given a single full LLM output, and even efficiently\ndiscovering the LLM's hidden size. Our empirical investigations show the\neffectiveness of our methods, which allow us to estimate the embedding size of\nOpenAI's gpt-3.5-turbo to be about 4096. Lastly, we discuss ways that LLM\nproviders can guard against these attacks, as well as how these capabilities\ncan be viewed as a feature (rather than a bug) by allowing for greater\ntransparency and accountability.\n", "link": "http://arxiv.org/abs/2403.09539v3", "date": "2024-11-08", "relevancy": 2.3134, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4677}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4602}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Logits%20of%20API-Protected%20LLMs%20Leak%20Proprietary%20Information&body=Title%3A%20Logits%20of%20API-Protected%20LLMs%20Leak%20Proprietary%20Information%0AAuthor%3A%20Matthew%20Finlayson%20and%20Xiang%20Ren%20and%20Swabha%20Swayamdipta%0AAbstract%3A%20%20%20Large%20language%20model%20%28LLM%29%20providers%20often%20hide%20the%20architectural%20details%20and%0Aparameters%20of%20their%20proprietary%20models%20by%20restricting%20public%20access%20to%20a%0Alimited%20API.%20In%20this%20work%20we%20show%20that%2C%20with%20only%20a%20conservative%20assumption%0Aabout%20the%20model%20architecture%2C%20it%20is%20possible%20to%20learn%20a%20surprisingly%20large%0Aamount%20of%20non-public%20information%20about%20an%20API-protected%20LLM%20from%20a%20relatively%0Asmall%20number%20of%20API%20queries%20%28e.g.%2C%20costing%20under%20%241000%20USD%20for%20OpenAI%27s%0Agpt-3.5-turbo%29.%20Our%20findings%20are%20centered%20on%20one%20key%20observation%3A%20most%20modern%0ALLMs%20suffer%20from%20a%20softmax%20bottleneck%2C%20which%20restricts%20the%20model%20outputs%20to%20a%0Alinear%20subspace%20of%20the%20full%20output%20space.%20We%20exploit%20this%20fact%20to%20unlock%0Aseveral%20capabilities%2C%20including%20%28but%20not%20limited%20to%29%20obtaining%20cheap%0Afull-vocabulary%20outputs%2C%20auditing%20for%20specific%20types%20of%20model%20updates%2C%0Aidentifying%20the%20source%20LLM%20given%20a%20single%20full%20LLM%20output%2C%20and%20even%20efficiently%0Adiscovering%20the%20LLM%27s%20hidden%20size.%20Our%20empirical%20investigations%20show%20the%0Aeffectiveness%20of%20our%20methods%2C%20which%20allow%20us%20to%20estimate%20the%20embedding%20size%20of%0AOpenAI%27s%20gpt-3.5-turbo%20to%20be%20about%204096.%20Lastly%2C%20we%20discuss%20ways%20that%20LLM%0Aproviders%20can%20guard%20against%20these%20attacks%2C%20as%20well%20as%20how%20these%20capabilities%0Acan%20be%20viewed%20as%20a%20feature%20%28rather%20than%20a%20bug%29%20by%20allowing%20for%20greater%0Atransparency%20and%20accountability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09539v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogits%2520of%2520API-Protected%2520LLMs%2520Leak%2520Proprietary%2520Information%26entry.906535625%3DMatthew%2520Finlayson%2520and%2520Xiang%2520Ren%2520and%2520Swabha%2520Swayamdipta%26entry.1292438233%3D%2520%2520Large%2520language%2520model%2520%2528LLM%2529%2520providers%2520often%2520hide%2520the%2520architectural%2520details%2520and%250Aparameters%2520of%2520their%2520proprietary%2520models%2520by%2520restricting%2520public%2520access%2520to%2520a%250Alimited%2520API.%2520In%2520this%2520work%2520we%2520show%2520that%252C%2520with%2520only%2520a%2520conservative%2520assumption%250Aabout%2520the%2520model%2520architecture%252C%2520it%2520is%2520possible%2520to%2520learn%2520a%2520surprisingly%2520large%250Aamount%2520of%2520non-public%2520information%2520about%2520an%2520API-protected%2520LLM%2520from%2520a%2520relatively%250Asmall%2520number%2520of%2520API%2520queries%2520%2528e.g.%252C%2520costing%2520under%2520%25241000%2520USD%2520for%2520OpenAI%2527s%250Agpt-3.5-turbo%2529.%2520Our%2520findings%2520are%2520centered%2520on%2520one%2520key%2520observation%253A%2520most%2520modern%250ALLMs%2520suffer%2520from%2520a%2520softmax%2520bottleneck%252C%2520which%2520restricts%2520the%2520model%2520outputs%2520to%2520a%250Alinear%2520subspace%2520of%2520the%2520full%2520output%2520space.%2520We%2520exploit%2520this%2520fact%2520to%2520unlock%250Aseveral%2520capabilities%252C%2520including%2520%2528but%2520not%2520limited%2520to%2529%2520obtaining%2520cheap%250Afull-vocabulary%2520outputs%252C%2520auditing%2520for%2520specific%2520types%2520of%2520model%2520updates%252C%250Aidentifying%2520the%2520source%2520LLM%2520given%2520a%2520single%2520full%2520LLM%2520output%252C%2520and%2520even%2520efficiently%250Adiscovering%2520the%2520LLM%2527s%2520hidden%2520size.%2520Our%2520empirical%2520investigations%2520show%2520the%250Aeffectiveness%2520of%2520our%2520methods%252C%2520which%2520allow%2520us%2520to%2520estimate%2520the%2520embedding%2520size%2520of%250AOpenAI%2527s%2520gpt-3.5-turbo%2520to%2520be%2520about%25204096.%2520Lastly%252C%2520we%2520discuss%2520ways%2520that%2520LLM%250Aproviders%2520can%2520guard%2520against%2520these%2520attacks%252C%2520as%2520well%2520as%2520how%2520these%2520capabilities%250Acan%2520be%2520viewed%2520as%2520a%2520feature%2520%2528rather%2520than%2520a%2520bug%2529%2520by%2520allowing%2520for%2520greater%250Atransparency%2520and%2520accountability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09539v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Logits%20of%20API-Protected%20LLMs%20Leak%20Proprietary%20Information&entry.906535625=Matthew%20Finlayson%20and%20Xiang%20Ren%20and%20Swabha%20Swayamdipta&entry.1292438233=%20%20Large%20language%20model%20%28LLM%29%20providers%20often%20hide%20the%20architectural%20details%20and%0Aparameters%20of%20their%20proprietary%20models%20by%20restricting%20public%20access%20to%20a%0Alimited%20API.%20In%20this%20work%20we%20show%20that%2C%20with%20only%20a%20conservative%20assumption%0Aabout%20the%20model%20architecture%2C%20it%20is%20possible%20to%20learn%20a%20surprisingly%20large%0Aamount%20of%20non-public%20information%20about%20an%20API-protected%20LLM%20from%20a%20relatively%0Asmall%20number%20of%20API%20queries%20%28e.g.%2C%20costing%20under%20%241000%20USD%20for%20OpenAI%27s%0Agpt-3.5-turbo%29.%20Our%20findings%20are%20centered%20on%20one%20key%20observation%3A%20most%20modern%0ALLMs%20suffer%20from%20a%20softmax%20bottleneck%2C%20which%20restricts%20the%20model%20outputs%20to%20a%0Alinear%20subspace%20of%20the%20full%20output%20space.%20We%20exploit%20this%20fact%20to%20unlock%0Aseveral%20capabilities%2C%20including%20%28but%20not%20limited%20to%29%20obtaining%20cheap%0Afull-vocabulary%20outputs%2C%20auditing%20for%20specific%20types%20of%20model%20updates%2C%0Aidentifying%20the%20source%20LLM%20given%20a%20single%20full%20LLM%20output%2C%20and%20even%20efficiently%0Adiscovering%20the%20LLM%27s%20hidden%20size.%20Our%20empirical%20investigations%20show%20the%0Aeffectiveness%20of%20our%20methods%2C%20which%20allow%20us%20to%20estimate%20the%20embedding%20size%20of%0AOpenAI%27s%20gpt-3.5-turbo%20to%20be%20about%204096.%20Lastly%2C%20we%20discuss%20ways%20that%20LLM%0Aproviders%20can%20guard%20against%20these%20attacks%2C%20as%20well%20as%20how%20these%20capabilities%0Acan%20be%20viewed%20as%20a%20feature%20%28rather%20than%20a%20bug%29%20by%20allowing%20for%20greater%0Atransparency%20and%20accountability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09539v3&entry.124074799=Read"},
{"title": "Scaling Laws for Task-Optimized Models of the Primate Visual Ventral\n  Stream", "author": "Abdulkadir Gokce and Martin Schrimpf", "abstract": "  When trained on large-scale object classification datasets, certain\nartificial neural network models begin to approximate core object recognition\n(COR) behaviors and neural response patterns in the primate visual ventral\nstream (VVS). While recent machine learning advances suggest that scaling model\nsize, dataset size, and compute resources improve task performance, the impact\nof scaling on brain alignment remains unclear. In this study, we explore\nscaling laws for modeling the primate VVS by systematically evaluating over 600\nmodels trained under controlled conditions on benchmarks spanning V1, V2, V4,\nIT and COR behaviors. We observe that while behavioral alignment continues to\nscale with larger models, neural alignment saturates. This observation remains\ntrue across model architectures and training datasets, even though models with\nstronger inductive bias and datasets with higher-quality images are more\ncompute-efficient. Increased scaling is especially beneficial for higher-level\nvisual areas, where small models trained on few samples exhibit only poor\nalignment. Finally, we develop a scaling recipe, indicating that a greater\nproportion of compute should be allocated to data samples over model size. Our\nresults suggest that while scaling alone might suffice for alignment with human\ncore object recognition behavior, it will not yield improved models of the\nbrain's visual ventral stream with current architectures and datasets,\nhighlighting the need for novel strategies in building brain-like models.\n", "link": "http://arxiv.org/abs/2411.05712v1", "date": "2024-11-08", "relevancy": 2.2792, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5777}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5682}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Laws%20for%20Task-Optimized%20Models%20of%20the%20Primate%20Visual%20Ventral%0A%20%20Stream&body=Title%3A%20Scaling%20Laws%20for%20Task-Optimized%20Models%20of%20the%20Primate%20Visual%20Ventral%0A%20%20Stream%0AAuthor%3A%20Abdulkadir%20Gokce%20and%20Martin%20Schrimpf%0AAbstract%3A%20%20%20When%20trained%20on%20large-scale%20object%20classification%20datasets%2C%20certain%0Aartificial%20neural%20network%20models%20begin%20to%20approximate%20core%20object%20recognition%0A%28COR%29%20behaviors%20and%20neural%20response%20patterns%20in%20the%20primate%20visual%20ventral%0Astream%20%28VVS%29.%20While%20recent%20machine%20learning%20advances%20suggest%20that%20scaling%20model%0Asize%2C%20dataset%20size%2C%20and%20compute%20resources%20improve%20task%20performance%2C%20the%20impact%0Aof%20scaling%20on%20brain%20alignment%20remains%20unclear.%20In%20this%20study%2C%20we%20explore%0Ascaling%20laws%20for%20modeling%20the%20primate%20VVS%20by%20systematically%20evaluating%20over%20600%0Amodels%20trained%20under%20controlled%20conditions%20on%20benchmarks%20spanning%20V1%2C%20V2%2C%20V4%2C%0AIT%20and%20COR%20behaviors.%20We%20observe%20that%20while%20behavioral%20alignment%20continues%20to%0Ascale%20with%20larger%20models%2C%20neural%20alignment%20saturates.%20This%20observation%20remains%0Atrue%20across%20model%20architectures%20and%20training%20datasets%2C%20even%20though%20models%20with%0Astronger%20inductive%20bias%20and%20datasets%20with%20higher-quality%20images%20are%20more%0Acompute-efficient.%20Increased%20scaling%20is%20especially%20beneficial%20for%20higher-level%0Avisual%20areas%2C%20where%20small%20models%20trained%20on%20few%20samples%20exhibit%20only%20poor%0Aalignment.%20Finally%2C%20we%20develop%20a%20scaling%20recipe%2C%20indicating%20that%20a%20greater%0Aproportion%20of%20compute%20should%20be%20allocated%20to%20data%20samples%20over%20model%20size.%20Our%0Aresults%20suggest%20that%20while%20scaling%20alone%20might%20suffice%20for%20alignment%20with%20human%0Acore%20object%20recognition%20behavior%2C%20it%20will%20not%20yield%20improved%20models%20of%20the%0Abrain%27s%20visual%20ventral%20stream%20with%20current%20architectures%20and%20datasets%2C%0Ahighlighting%20the%20need%20for%20novel%20strategies%20in%20building%20brain-like%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Laws%2520for%2520Task-Optimized%2520Models%2520of%2520the%2520Primate%2520Visual%2520Ventral%250A%2520%2520Stream%26entry.906535625%3DAbdulkadir%2520Gokce%2520and%2520Martin%2520Schrimpf%26entry.1292438233%3D%2520%2520When%2520trained%2520on%2520large-scale%2520object%2520classification%2520datasets%252C%2520certain%250Aartificial%2520neural%2520network%2520models%2520begin%2520to%2520approximate%2520core%2520object%2520recognition%250A%2528COR%2529%2520behaviors%2520and%2520neural%2520response%2520patterns%2520in%2520the%2520primate%2520visual%2520ventral%250Astream%2520%2528VVS%2529.%2520While%2520recent%2520machine%2520learning%2520advances%2520suggest%2520that%2520scaling%2520model%250Asize%252C%2520dataset%2520size%252C%2520and%2520compute%2520resources%2520improve%2520task%2520performance%252C%2520the%2520impact%250Aof%2520scaling%2520on%2520brain%2520alignment%2520remains%2520unclear.%2520In%2520this%2520study%252C%2520we%2520explore%250Ascaling%2520laws%2520for%2520modeling%2520the%2520primate%2520VVS%2520by%2520systematically%2520evaluating%2520over%2520600%250Amodels%2520trained%2520under%2520controlled%2520conditions%2520on%2520benchmarks%2520spanning%2520V1%252C%2520V2%252C%2520V4%252C%250AIT%2520and%2520COR%2520behaviors.%2520We%2520observe%2520that%2520while%2520behavioral%2520alignment%2520continues%2520to%250Ascale%2520with%2520larger%2520models%252C%2520neural%2520alignment%2520saturates.%2520This%2520observation%2520remains%250Atrue%2520across%2520model%2520architectures%2520and%2520training%2520datasets%252C%2520even%2520though%2520models%2520with%250Astronger%2520inductive%2520bias%2520and%2520datasets%2520with%2520higher-quality%2520images%2520are%2520more%250Acompute-efficient.%2520Increased%2520scaling%2520is%2520especially%2520beneficial%2520for%2520higher-level%250Avisual%2520areas%252C%2520where%2520small%2520models%2520trained%2520on%2520few%2520samples%2520exhibit%2520only%2520poor%250Aalignment.%2520Finally%252C%2520we%2520develop%2520a%2520scaling%2520recipe%252C%2520indicating%2520that%2520a%2520greater%250Aproportion%2520of%2520compute%2520should%2520be%2520allocated%2520to%2520data%2520samples%2520over%2520model%2520size.%2520Our%250Aresults%2520suggest%2520that%2520while%2520scaling%2520alone%2520might%2520suffice%2520for%2520alignment%2520with%2520human%250Acore%2520object%2520recognition%2520behavior%252C%2520it%2520will%2520not%2520yield%2520improved%2520models%2520of%2520the%250Abrain%2527s%2520visual%2520ventral%2520stream%2520with%2520current%2520architectures%2520and%2520datasets%252C%250Ahighlighting%2520the%2520need%2520for%2520novel%2520strategies%2520in%2520building%2520brain-like%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Laws%20for%20Task-Optimized%20Models%20of%20the%20Primate%20Visual%20Ventral%0A%20%20Stream&entry.906535625=Abdulkadir%20Gokce%20and%20Martin%20Schrimpf&entry.1292438233=%20%20When%20trained%20on%20large-scale%20object%20classification%20datasets%2C%20certain%0Aartificial%20neural%20network%20models%20begin%20to%20approximate%20core%20object%20recognition%0A%28COR%29%20behaviors%20and%20neural%20response%20patterns%20in%20the%20primate%20visual%20ventral%0Astream%20%28VVS%29.%20While%20recent%20machine%20learning%20advances%20suggest%20that%20scaling%20model%0Asize%2C%20dataset%20size%2C%20and%20compute%20resources%20improve%20task%20performance%2C%20the%20impact%0Aof%20scaling%20on%20brain%20alignment%20remains%20unclear.%20In%20this%20study%2C%20we%20explore%0Ascaling%20laws%20for%20modeling%20the%20primate%20VVS%20by%20systematically%20evaluating%20over%20600%0Amodels%20trained%20under%20controlled%20conditions%20on%20benchmarks%20spanning%20V1%2C%20V2%2C%20V4%2C%0AIT%20and%20COR%20behaviors.%20We%20observe%20that%20while%20behavioral%20alignment%20continues%20to%0Ascale%20with%20larger%20models%2C%20neural%20alignment%20saturates.%20This%20observation%20remains%0Atrue%20across%20model%20architectures%20and%20training%20datasets%2C%20even%20though%20models%20with%0Astronger%20inductive%20bias%20and%20datasets%20with%20higher-quality%20images%20are%20more%0Acompute-efficient.%20Increased%20scaling%20is%20especially%20beneficial%20for%20higher-level%0Avisual%20areas%2C%20where%20small%20models%20trained%20on%20few%20samples%20exhibit%20only%20poor%0Aalignment.%20Finally%2C%20we%20develop%20a%20scaling%20recipe%2C%20indicating%20that%20a%20greater%0Aproportion%20of%20compute%20should%20be%20allocated%20to%20data%20samples%20over%20model%20size.%20Our%0Aresults%20suggest%20that%20while%20scaling%20alone%20might%20suffice%20for%20alignment%20with%20human%0Acore%20object%20recognition%20behavior%2C%20it%20will%20not%20yield%20improved%20models%20of%20the%0Abrain%27s%20visual%20ventral%20stream%20with%20current%20architectures%20and%20datasets%2C%0Ahighlighting%20the%20need%20for%20novel%20strategies%20in%20building%20brain-like%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05712v1&entry.124074799=Read"},
{"title": "Predicting Stroke through Retinal Graphs and Multimodal Self-supervised\n  Learning", "author": "Yuqing Huang and Bastian Wittmann and Olga Demler and Bjoern Menze and Neda Davoudi", "abstract": "  Early identification of stroke is crucial for intervention, requiring\nreliable models. We proposed an efficient retinal image representation together\nwith clinical information to capture a comprehensive overview of cardiovascular\nhealth, leveraging large multimodal datasets for new medical insights. Our\napproach is one of the first contrastive frameworks that integrates graph and\ntabular data, using vessel graphs derived from retinal images for efficient\nrepresentation. This method, combined with multimodal contrastive learning,\nsignificantly enhances stroke prediction accuracy by integrating data from\nmultiple sources and using contrastive learning for transfer learning. The\nself-supervised learning techniques employed allow the model to learn\neffectively from unlabeled data, reducing the dependency on large annotated\ndatasets. Our framework showed an AUROC improvement of 3.78% from supervised to\nself-supervised approaches. Additionally, the graph-level representation\napproach achieved superior performance to image encoders while significantly\nreducing pre-training and fine-tuning runtimes. These findings indicate that\nretinal images are a cost-effective method for improving cardiovascular disease\npredictions and pave the way for future research into retinal and cerebral\nvessel connections and the use of graph-based retinal vessel representations.\n", "link": "http://arxiv.org/abs/2411.05597v1", "date": "2024-11-08", "relevancy": 2.2479, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5994}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5399}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Stroke%20through%20Retinal%20Graphs%20and%20Multimodal%20Self-supervised%0A%20%20Learning&body=Title%3A%20Predicting%20Stroke%20through%20Retinal%20Graphs%20and%20Multimodal%20Self-supervised%0A%20%20Learning%0AAuthor%3A%20Yuqing%20Huang%20and%20Bastian%20Wittmann%20and%20Olga%20Demler%20and%20Bjoern%20Menze%20and%20Neda%20Davoudi%0AAbstract%3A%20%20%20Early%20identification%20of%20stroke%20is%20crucial%20for%20intervention%2C%20requiring%0Areliable%20models.%20We%20proposed%20an%20efficient%20retinal%20image%20representation%20together%0Awith%20clinical%20information%20to%20capture%20a%20comprehensive%20overview%20of%20cardiovascular%0Ahealth%2C%20leveraging%20large%20multimodal%20datasets%20for%20new%20medical%20insights.%20Our%0Aapproach%20is%20one%20of%20the%20first%20contrastive%20frameworks%20that%20integrates%20graph%20and%0Atabular%20data%2C%20using%20vessel%20graphs%20derived%20from%20retinal%20images%20for%20efficient%0Arepresentation.%20This%20method%2C%20combined%20with%20multimodal%20contrastive%20learning%2C%0Asignificantly%20enhances%20stroke%20prediction%20accuracy%20by%20integrating%20data%20from%0Amultiple%20sources%20and%20using%20contrastive%20learning%20for%20transfer%20learning.%20The%0Aself-supervised%20learning%20techniques%20employed%20allow%20the%20model%20to%20learn%0Aeffectively%20from%20unlabeled%20data%2C%20reducing%20the%20dependency%20on%20large%20annotated%0Adatasets.%20Our%20framework%20showed%20an%20AUROC%20improvement%20of%203.78%25%20from%20supervised%20to%0Aself-supervised%20approaches.%20Additionally%2C%20the%20graph-level%20representation%0Aapproach%20achieved%20superior%20performance%20to%20image%20encoders%20while%20significantly%0Areducing%20pre-training%20and%20fine-tuning%20runtimes.%20These%20findings%20indicate%20that%0Aretinal%20images%20are%20a%20cost-effective%20method%20for%20improving%20cardiovascular%20disease%0Apredictions%20and%20pave%20the%20way%20for%20future%20research%20into%20retinal%20and%20cerebral%0Avessel%20connections%20and%20the%20use%20of%20graph-based%20retinal%20vessel%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Stroke%2520through%2520Retinal%2520Graphs%2520and%2520Multimodal%2520Self-supervised%250A%2520%2520Learning%26entry.906535625%3DYuqing%2520Huang%2520and%2520Bastian%2520Wittmann%2520and%2520Olga%2520Demler%2520and%2520Bjoern%2520Menze%2520and%2520Neda%2520Davoudi%26entry.1292438233%3D%2520%2520Early%2520identification%2520of%2520stroke%2520is%2520crucial%2520for%2520intervention%252C%2520requiring%250Areliable%2520models.%2520We%2520proposed%2520an%2520efficient%2520retinal%2520image%2520representation%2520together%250Awith%2520clinical%2520information%2520to%2520capture%2520a%2520comprehensive%2520overview%2520of%2520cardiovascular%250Ahealth%252C%2520leveraging%2520large%2520multimodal%2520datasets%2520for%2520new%2520medical%2520insights.%2520Our%250Aapproach%2520is%2520one%2520of%2520the%2520first%2520contrastive%2520frameworks%2520that%2520integrates%2520graph%2520and%250Atabular%2520data%252C%2520using%2520vessel%2520graphs%2520derived%2520from%2520retinal%2520images%2520for%2520efficient%250Arepresentation.%2520This%2520method%252C%2520combined%2520with%2520multimodal%2520contrastive%2520learning%252C%250Asignificantly%2520enhances%2520stroke%2520prediction%2520accuracy%2520by%2520integrating%2520data%2520from%250Amultiple%2520sources%2520and%2520using%2520contrastive%2520learning%2520for%2520transfer%2520learning.%2520The%250Aself-supervised%2520learning%2520techniques%2520employed%2520allow%2520the%2520model%2520to%2520learn%250Aeffectively%2520from%2520unlabeled%2520data%252C%2520reducing%2520the%2520dependency%2520on%2520large%2520annotated%250Adatasets.%2520Our%2520framework%2520showed%2520an%2520AUROC%2520improvement%2520of%25203.78%2525%2520from%2520supervised%2520to%250Aself-supervised%2520approaches.%2520Additionally%252C%2520the%2520graph-level%2520representation%250Aapproach%2520achieved%2520superior%2520performance%2520to%2520image%2520encoders%2520while%2520significantly%250Areducing%2520pre-training%2520and%2520fine-tuning%2520runtimes.%2520These%2520findings%2520indicate%2520that%250Aretinal%2520images%2520are%2520a%2520cost-effective%2520method%2520for%2520improving%2520cardiovascular%2520disease%250Apredictions%2520and%2520pave%2520the%2520way%2520for%2520future%2520research%2520into%2520retinal%2520and%2520cerebral%250Avessel%2520connections%2520and%2520the%2520use%2520of%2520graph-based%2520retinal%2520vessel%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Stroke%20through%20Retinal%20Graphs%20and%20Multimodal%20Self-supervised%0A%20%20Learning&entry.906535625=Yuqing%20Huang%20and%20Bastian%20Wittmann%20and%20Olga%20Demler%20and%20Bjoern%20Menze%20and%20Neda%20Davoudi&entry.1292438233=%20%20Early%20identification%20of%20stroke%20is%20crucial%20for%20intervention%2C%20requiring%0Areliable%20models.%20We%20proposed%20an%20efficient%20retinal%20image%20representation%20together%0Awith%20clinical%20information%20to%20capture%20a%20comprehensive%20overview%20of%20cardiovascular%0Ahealth%2C%20leveraging%20large%20multimodal%20datasets%20for%20new%20medical%20insights.%20Our%0Aapproach%20is%20one%20of%20the%20first%20contrastive%20frameworks%20that%20integrates%20graph%20and%0Atabular%20data%2C%20using%20vessel%20graphs%20derived%20from%20retinal%20images%20for%20efficient%0Arepresentation.%20This%20method%2C%20combined%20with%20multimodal%20contrastive%20learning%2C%0Asignificantly%20enhances%20stroke%20prediction%20accuracy%20by%20integrating%20data%20from%0Amultiple%20sources%20and%20using%20contrastive%20learning%20for%20transfer%20learning.%20The%0Aself-supervised%20learning%20techniques%20employed%20allow%20the%20model%20to%20learn%0Aeffectively%20from%20unlabeled%20data%2C%20reducing%20the%20dependency%20on%20large%20annotated%0Adatasets.%20Our%20framework%20showed%20an%20AUROC%20improvement%20of%203.78%25%20from%20supervised%20to%0Aself-supervised%20approaches.%20Additionally%2C%20the%20graph-level%20representation%0Aapproach%20achieved%20superior%20performance%20to%20image%20encoders%20while%20significantly%0Areducing%20pre-training%20and%20fine-tuning%20runtimes.%20These%20findings%20indicate%20that%0Aretinal%20images%20are%20a%20cost-effective%20method%20for%20improving%20cardiovascular%20disease%0Apredictions%20and%20pave%20the%20way%20for%20future%20research%20into%20retinal%20and%20cerebral%0Avessel%20connections%20and%20the%20use%20of%20graph-based%20retinal%20vessel%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05597v1&entry.124074799=Read"},
{"title": "Autoregressive Adaptive Hypergraph Transformer for Skeleton-based\n  Activity Recognition", "author": "Abhisek Ray and Ayush Raj and Maheshkumar H. Kolekar", "abstract": "  Extracting multiscale contextual information and higher-order correlations\namong skeleton sequences using Graph Convolutional Networks (GCNs) alone is\ninadequate for effective action classification. Hypergraph convolution\naddresses the above issues but cannot harness the long-range dependencies.\nTransformer proves to be effective in capturing these dependencies and making\ncomplex contextual features accessible. We propose an Autoregressive Adaptive\nHyperGraph Transformer (AutoregAd-HGformer) model for in-phase (autoregressive\nand discrete) and out-phase (adaptive) hypergraph generation. The vector\nquantized in-phase hypergraph equipped with powerful autoregressive learned\npriors produces a more robust and informative representation suitable for\nhyperedge formation. The out-phase hypergraph generator provides a\nmodel-agnostic hyperedge learning technique to align the attributes with input\nskeleton embedding. The hybrid (supervised and unsupervised) learning in\nAutoregAd-HGformer explores the action-dependent feature along spatial,\ntemporal, and channel dimensions. The extensive experimental results and\nablation study indicate the superiority of our model over state-of-the-art\nhypergraph architectures on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.\n", "link": "http://arxiv.org/abs/2411.05692v1", "date": "2024-11-08", "relevancy": 2.2435, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5638}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5611}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Autoregressive%20Adaptive%20Hypergraph%20Transformer%20for%20Skeleton-based%0A%20%20Activity%20Recognition&body=Title%3A%20Autoregressive%20Adaptive%20Hypergraph%20Transformer%20for%20Skeleton-based%0A%20%20Activity%20Recognition%0AAuthor%3A%20Abhisek%20Ray%20and%20Ayush%20Raj%20and%20Maheshkumar%20H.%20Kolekar%0AAbstract%3A%20%20%20Extracting%20multiscale%20contextual%20information%20and%20higher-order%20correlations%0Aamong%20skeleton%20sequences%20using%20Graph%20Convolutional%20Networks%20%28GCNs%29%20alone%20is%0Ainadequate%20for%20effective%20action%20classification.%20Hypergraph%20convolution%0Aaddresses%20the%20above%20issues%20but%20cannot%20harness%20the%20long-range%20dependencies.%0ATransformer%20proves%20to%20be%20effective%20in%20capturing%20these%20dependencies%20and%20making%0Acomplex%20contextual%20features%20accessible.%20We%20propose%20an%20Autoregressive%20Adaptive%0AHyperGraph%20Transformer%20%28AutoregAd-HGformer%29%20model%20for%20in-phase%20%28autoregressive%0Aand%20discrete%29%20and%20out-phase%20%28adaptive%29%20hypergraph%20generation.%20The%20vector%0Aquantized%20in-phase%20hypergraph%20equipped%20with%20powerful%20autoregressive%20learned%0Apriors%20produces%20a%20more%20robust%20and%20informative%20representation%20suitable%20for%0Ahyperedge%20formation.%20The%20out-phase%20hypergraph%20generator%20provides%20a%0Amodel-agnostic%20hyperedge%20learning%20technique%20to%20align%20the%20attributes%20with%20input%0Askeleton%20embedding.%20The%20hybrid%20%28supervised%20and%20unsupervised%29%20learning%20in%0AAutoregAd-HGformer%20explores%20the%20action-dependent%20feature%20along%20spatial%2C%0Atemporal%2C%20and%20channel%20dimensions.%20The%20extensive%20experimental%20results%20and%0Aablation%20study%20indicate%20the%20superiority%20of%20our%20model%20over%20state-of-the-art%0Ahypergraph%20architectures%20on%20NTU%20RGB%2BD%2C%20NTU%20RGB%2BD%20120%2C%20and%20NW-UCLA%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoregressive%2520Adaptive%2520Hypergraph%2520Transformer%2520for%2520Skeleton-based%250A%2520%2520Activity%2520Recognition%26entry.906535625%3DAbhisek%2520Ray%2520and%2520Ayush%2520Raj%2520and%2520Maheshkumar%2520H.%2520Kolekar%26entry.1292438233%3D%2520%2520Extracting%2520multiscale%2520contextual%2520information%2520and%2520higher-order%2520correlations%250Aamong%2520skeleton%2520sequences%2520using%2520Graph%2520Convolutional%2520Networks%2520%2528GCNs%2529%2520alone%2520is%250Ainadequate%2520for%2520effective%2520action%2520classification.%2520Hypergraph%2520convolution%250Aaddresses%2520the%2520above%2520issues%2520but%2520cannot%2520harness%2520the%2520long-range%2520dependencies.%250ATransformer%2520proves%2520to%2520be%2520effective%2520in%2520capturing%2520these%2520dependencies%2520and%2520making%250Acomplex%2520contextual%2520features%2520accessible.%2520We%2520propose%2520an%2520Autoregressive%2520Adaptive%250AHyperGraph%2520Transformer%2520%2528AutoregAd-HGformer%2529%2520model%2520for%2520in-phase%2520%2528autoregressive%250Aand%2520discrete%2529%2520and%2520out-phase%2520%2528adaptive%2529%2520hypergraph%2520generation.%2520The%2520vector%250Aquantized%2520in-phase%2520hypergraph%2520equipped%2520with%2520powerful%2520autoregressive%2520learned%250Apriors%2520produces%2520a%2520more%2520robust%2520and%2520informative%2520representation%2520suitable%2520for%250Ahyperedge%2520formation.%2520The%2520out-phase%2520hypergraph%2520generator%2520provides%2520a%250Amodel-agnostic%2520hyperedge%2520learning%2520technique%2520to%2520align%2520the%2520attributes%2520with%2520input%250Askeleton%2520embedding.%2520The%2520hybrid%2520%2528supervised%2520and%2520unsupervised%2529%2520learning%2520in%250AAutoregAd-HGformer%2520explores%2520the%2520action-dependent%2520feature%2520along%2520spatial%252C%250Atemporal%252C%2520and%2520channel%2520dimensions.%2520The%2520extensive%2520experimental%2520results%2520and%250Aablation%2520study%2520indicate%2520the%2520superiority%2520of%2520our%2520model%2520over%2520state-of-the-art%250Ahypergraph%2520architectures%2520on%2520NTU%2520RGB%252BD%252C%2520NTU%2520RGB%252BD%2520120%252C%2520and%2520NW-UCLA%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Autoregressive%20Adaptive%20Hypergraph%20Transformer%20for%20Skeleton-based%0A%20%20Activity%20Recognition&entry.906535625=Abhisek%20Ray%20and%20Ayush%20Raj%20and%20Maheshkumar%20H.%20Kolekar&entry.1292438233=%20%20Extracting%20multiscale%20contextual%20information%20and%20higher-order%20correlations%0Aamong%20skeleton%20sequences%20using%20Graph%20Convolutional%20Networks%20%28GCNs%29%20alone%20is%0Ainadequate%20for%20effective%20action%20classification.%20Hypergraph%20convolution%0Aaddresses%20the%20above%20issues%20but%20cannot%20harness%20the%20long-range%20dependencies.%0ATransformer%20proves%20to%20be%20effective%20in%20capturing%20these%20dependencies%20and%20making%0Acomplex%20contextual%20features%20accessible.%20We%20propose%20an%20Autoregressive%20Adaptive%0AHyperGraph%20Transformer%20%28AutoregAd-HGformer%29%20model%20for%20in-phase%20%28autoregressive%0Aand%20discrete%29%20and%20out-phase%20%28adaptive%29%20hypergraph%20generation.%20The%20vector%0Aquantized%20in-phase%20hypergraph%20equipped%20with%20powerful%20autoregressive%20learned%0Apriors%20produces%20a%20more%20robust%20and%20informative%20representation%20suitable%20for%0Ahyperedge%20formation.%20The%20out-phase%20hypergraph%20generator%20provides%20a%0Amodel-agnostic%20hyperedge%20learning%20technique%20to%20align%20the%20attributes%20with%20input%0Askeleton%20embedding.%20The%20hybrid%20%28supervised%20and%20unsupervised%29%20learning%20in%0AAutoregAd-HGformer%20explores%20the%20action-dependent%20feature%20along%20spatial%2C%0Atemporal%2C%20and%20channel%20dimensions.%20The%20extensive%20experimental%20results%20and%0Aablation%20study%20indicate%20the%20superiority%20of%20our%20model%20over%20state-of-the-art%0Ahypergraph%20architectures%20on%20NTU%20RGB%2BD%2C%20NTU%20RGB%2BD%20120%2C%20and%20NW-UCLA%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05692v1&entry.124074799=Read"},
{"title": "Open-set object detection: towards unified problem formulation and\n  benchmarking", "author": "Hejer Ammar and Nikita Kiselov and Guillaume Lapouge and Romaric Audigier", "abstract": "  In real-world applications where confidence is key, like autonomous driving,\nthe accurate detection and appropriate handling of classes differing from those\nused during training are crucial. Despite the proposal of various unknown\nobject detection approaches, we have observed widespread inconsistencies among\nthem regarding the datasets, metrics, and scenarios used, alongside a notable\nabsence of a clear definition for unknown objects, which hampers meaningful\nevaluation. To counter these issues, we introduce two benchmarks: a unified\nVOC-COCO evaluation, and the new OpenImagesRoad benchmark which provides clear\nhierarchical object definition besides new evaluation metrics. Complementing\nthe benchmark, we exploit recent self-supervised Vision Transformers\nperformance, to improve pseudo-labeling-based OpenSet Object Detection (OSOD),\nthrough OW-DETR++. State-of-the-art methods are extensively evaluated on the\nproposed benchmarks. This study provides a clear problem definition, ensures\nconsistent evaluations, and draws new conclusions about effectiveness of OSOD\nstrategies.\n", "link": "http://arxiv.org/abs/2411.05564v1", "date": "2024-11-08", "relevancy": 2.2357, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5592}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-set%20object%20detection%3A%20towards%20unified%20problem%20formulation%20and%0A%20%20benchmarking&body=Title%3A%20Open-set%20object%20detection%3A%20towards%20unified%20problem%20formulation%20and%0A%20%20benchmarking%0AAuthor%3A%20Hejer%20Ammar%20and%20Nikita%20Kiselov%20and%20Guillaume%20Lapouge%20and%20Romaric%20Audigier%0AAbstract%3A%20%20%20In%20real-world%20applications%20where%20confidence%20is%20key%2C%20like%20autonomous%20driving%2C%0Athe%20accurate%20detection%20and%20appropriate%20handling%20of%20classes%20differing%20from%20those%0Aused%20during%20training%20are%20crucial.%20Despite%20the%20proposal%20of%20various%20unknown%0Aobject%20detection%20approaches%2C%20we%20have%20observed%20widespread%20inconsistencies%20among%0Athem%20regarding%20the%20datasets%2C%20metrics%2C%20and%20scenarios%20used%2C%20alongside%20a%20notable%0Aabsence%20of%20a%20clear%20definition%20for%20unknown%20objects%2C%20which%20hampers%20meaningful%0Aevaluation.%20To%20counter%20these%20issues%2C%20we%20introduce%20two%20benchmarks%3A%20a%20unified%0AVOC-COCO%20evaluation%2C%20and%20the%20new%20OpenImagesRoad%20benchmark%20which%20provides%20clear%0Ahierarchical%20object%20definition%20besides%20new%20evaluation%20metrics.%20Complementing%0Athe%20benchmark%2C%20we%20exploit%20recent%20self-supervised%20Vision%20Transformers%0Aperformance%2C%20to%20improve%20pseudo-labeling-based%20OpenSet%20Object%20Detection%20%28OSOD%29%2C%0Athrough%20OW-DETR%2B%2B.%20State-of-the-art%20methods%20are%20extensively%20evaluated%20on%20the%0Aproposed%20benchmarks.%20This%20study%20provides%20a%20clear%20problem%20definition%2C%20ensures%0Aconsistent%20evaluations%2C%20and%20draws%20new%20conclusions%20about%20effectiveness%20of%20OSOD%0Astrategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-set%2520object%2520detection%253A%2520towards%2520unified%2520problem%2520formulation%2520and%250A%2520%2520benchmarking%26entry.906535625%3DHejer%2520Ammar%2520and%2520Nikita%2520Kiselov%2520and%2520Guillaume%2520Lapouge%2520and%2520Romaric%2520Audigier%26entry.1292438233%3D%2520%2520In%2520real-world%2520applications%2520where%2520confidence%2520is%2520key%252C%2520like%2520autonomous%2520driving%252C%250Athe%2520accurate%2520detection%2520and%2520appropriate%2520handling%2520of%2520classes%2520differing%2520from%2520those%250Aused%2520during%2520training%2520are%2520crucial.%2520Despite%2520the%2520proposal%2520of%2520various%2520unknown%250Aobject%2520detection%2520approaches%252C%2520we%2520have%2520observed%2520widespread%2520inconsistencies%2520among%250Athem%2520regarding%2520the%2520datasets%252C%2520metrics%252C%2520and%2520scenarios%2520used%252C%2520alongside%2520a%2520notable%250Aabsence%2520of%2520a%2520clear%2520definition%2520for%2520unknown%2520objects%252C%2520which%2520hampers%2520meaningful%250Aevaluation.%2520To%2520counter%2520these%2520issues%252C%2520we%2520introduce%2520two%2520benchmarks%253A%2520a%2520unified%250AVOC-COCO%2520evaluation%252C%2520and%2520the%2520new%2520OpenImagesRoad%2520benchmark%2520which%2520provides%2520clear%250Ahierarchical%2520object%2520definition%2520besides%2520new%2520evaluation%2520metrics.%2520Complementing%250Athe%2520benchmark%252C%2520we%2520exploit%2520recent%2520self-supervised%2520Vision%2520Transformers%250Aperformance%252C%2520to%2520improve%2520pseudo-labeling-based%2520OpenSet%2520Object%2520Detection%2520%2528OSOD%2529%252C%250Athrough%2520OW-DETR%252B%252B.%2520State-of-the-art%2520methods%2520are%2520extensively%2520evaluated%2520on%2520the%250Aproposed%2520benchmarks.%2520This%2520study%2520provides%2520a%2520clear%2520problem%2520definition%252C%2520ensures%250Aconsistent%2520evaluations%252C%2520and%2520draws%2520new%2520conclusions%2520about%2520effectiveness%2520of%2520OSOD%250Astrategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-set%20object%20detection%3A%20towards%20unified%20problem%20formulation%20and%0A%20%20benchmarking&entry.906535625=Hejer%20Ammar%20and%20Nikita%20Kiselov%20and%20Guillaume%20Lapouge%20and%20Romaric%20Audigier&entry.1292438233=%20%20In%20real-world%20applications%20where%20confidence%20is%20key%2C%20like%20autonomous%20driving%2C%0Athe%20accurate%20detection%20and%20appropriate%20handling%20of%20classes%20differing%20from%20those%0Aused%20during%20training%20are%20crucial.%20Despite%20the%20proposal%20of%20various%20unknown%0Aobject%20detection%20approaches%2C%20we%20have%20observed%20widespread%20inconsistencies%20among%0Athem%20regarding%20the%20datasets%2C%20metrics%2C%20and%20scenarios%20used%2C%20alongside%20a%20notable%0Aabsence%20of%20a%20clear%20definition%20for%20unknown%20objects%2C%20which%20hampers%20meaningful%0Aevaluation.%20To%20counter%20these%20issues%2C%20we%20introduce%20two%20benchmarks%3A%20a%20unified%0AVOC-COCO%20evaluation%2C%20and%20the%20new%20OpenImagesRoad%20benchmark%20which%20provides%20clear%0Ahierarchical%20object%20definition%20besides%20new%20evaluation%20metrics.%20Complementing%0Athe%20benchmark%2C%20we%20exploit%20recent%20self-supervised%20Vision%20Transformers%0Aperformance%2C%20to%20improve%20pseudo-labeling-based%20OpenSet%20Object%20Detection%20%28OSOD%29%2C%0Athrough%20OW-DETR%2B%2B.%20State-of-the-art%20methods%20are%20extensively%20evaluated%20on%20the%0Aproposed%20benchmarks.%20This%20study%20provides%20a%20clear%20problem%20definition%2C%20ensures%0Aconsistent%20evaluations%2C%20and%20draws%20new%20conclusions%20about%20effectiveness%20of%20OSOD%0Astrategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05564v1&entry.124074799=Read"},
{"title": "Asterisk*: Keep it Simple", "author": "Andrew Semenov", "abstract": "  This paper describes Asterisk, a compact GPT-based model for generating text\nembeddings. The model uses a minimalist architecture with two layers, two\nattention heads, and 256 embedding dimensions. By applying knowledge\ndistillation from larger pretrained models, we explore the trade-offs between\nmodel size and performance while minimizing computational and memory\nrequirements. The model is primarily evaluated and optimized for classification\ntasks, with experimental results showing its moderate performance in zero-shot\nclassification across various downstream applications. With additional\nconfiguration, the model performance can approach or even surpass that of\nlarger architectures on specific classification tasks.\n", "link": "http://arxiv.org/abs/2411.05691v1", "date": "2024-11-08", "relevancy": 2.2221, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4863}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4292}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asterisk%2A%3A%20Keep%20it%20Simple&body=Title%3A%20Asterisk%2A%3A%20Keep%20it%20Simple%0AAuthor%3A%20Andrew%20Semenov%0AAbstract%3A%20%20%20This%20paper%20describes%20Asterisk%2C%20a%20compact%20GPT-based%20model%20for%20generating%20text%0Aembeddings.%20The%20model%20uses%20a%20minimalist%20architecture%20with%20two%20layers%2C%20two%0Aattention%20heads%2C%20and%20256%20embedding%20dimensions.%20By%20applying%20knowledge%0Adistillation%20from%20larger%20pretrained%20models%2C%20we%20explore%20the%20trade-offs%20between%0Amodel%20size%20and%20performance%20while%20minimizing%20computational%20and%20memory%0Arequirements.%20The%20model%20is%20primarily%20evaluated%20and%20optimized%20for%20classification%0Atasks%2C%20with%20experimental%20results%20showing%20its%20moderate%20performance%20in%20zero-shot%0Aclassification%20across%20various%20downstream%20applications.%20With%20additional%0Aconfiguration%2C%20the%20model%20performance%20can%20approach%20or%20even%20surpass%20that%20of%0Alarger%20architectures%20on%20specific%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05691v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsterisk%252A%253A%2520Keep%2520it%2520Simple%26entry.906535625%3DAndrew%2520Semenov%26entry.1292438233%3D%2520%2520This%2520paper%2520describes%2520Asterisk%252C%2520a%2520compact%2520GPT-based%2520model%2520for%2520generating%2520text%250Aembeddings.%2520The%2520model%2520uses%2520a%2520minimalist%2520architecture%2520with%2520two%2520layers%252C%2520two%250Aattention%2520heads%252C%2520and%2520256%2520embedding%2520dimensions.%2520By%2520applying%2520knowledge%250Adistillation%2520from%2520larger%2520pretrained%2520models%252C%2520we%2520explore%2520the%2520trade-offs%2520between%250Amodel%2520size%2520and%2520performance%2520while%2520minimizing%2520computational%2520and%2520memory%250Arequirements.%2520The%2520model%2520is%2520primarily%2520evaluated%2520and%2520optimized%2520for%2520classification%250Atasks%252C%2520with%2520experimental%2520results%2520showing%2520its%2520moderate%2520performance%2520in%2520zero-shot%250Aclassification%2520across%2520various%2520downstream%2520applications.%2520With%2520additional%250Aconfiguration%252C%2520the%2520model%2520performance%2520can%2520approach%2520or%2520even%2520surpass%2520that%2520of%250Alarger%2520architectures%2520on%2520specific%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05691v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asterisk%2A%3A%20Keep%20it%20Simple&entry.906535625=Andrew%20Semenov&entry.1292438233=%20%20This%20paper%20describes%20Asterisk%2C%20a%20compact%20GPT-based%20model%20for%20generating%20text%0Aembeddings.%20The%20model%20uses%20a%20minimalist%20architecture%20with%20two%20layers%2C%20two%0Aattention%20heads%2C%20and%20256%20embedding%20dimensions.%20By%20applying%20knowledge%0Adistillation%20from%20larger%20pretrained%20models%2C%20we%20explore%20the%20trade-offs%20between%0Amodel%20size%20and%20performance%20while%20minimizing%20computational%20and%20memory%0Arequirements.%20The%20model%20is%20primarily%20evaluated%20and%20optimized%20for%20classification%0Atasks%2C%20with%20experimental%20results%20showing%20its%20moderate%20performance%20in%20zero-shot%0Aclassification%20across%20various%20downstream%20applications.%20With%20additional%0Aconfiguration%2C%20the%20model%20performance%20can%20approach%20or%20even%20surpass%20that%20of%0Alarger%20architectures%20on%20specific%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05691v1&entry.124074799=Read"},
{"title": "IMDL-BenCo: A Comprehensive Benchmark and Codebase for Image\n  Manipulation Detection & Localization", "author": "Xiaochen Ma and Xuekang Zhu and Lei Su and Bo Du and Zhuohang Jiang and Bingkui Tong and Zeyu Lei and Xinyu Yang and Chi-Man Pun and Jiancheng Lv and Jizhe Zhou", "abstract": "  A comprehensive benchmark is yet to be established in the Image Manipulation\nDetection & Localization (IMDL) field. The absence of such a benchmark leads to\ninsufficient and misleading model evaluations, severely undermining the\ndevelopment of this field. However, the scarcity of open-sourced baseline\nmodels and inconsistent training and evaluation protocols make conducting\nrigorous experiments and faithful comparisons among IMDL models challenging. To\naddress these challenges, we introduce IMDL-BenCo, the first comprehensive IMDL\nbenchmark and modular codebase. IMDL-BenCo: i) decomposes the IMDL framework\ninto standardized, reusable components and revises the model construction\npipeline, improving coding efficiency and customization flexibility; ii) fully\nimplements or incorporates training code for state-of-the-art models to\nestablish a comprehensive IMDL benchmark; and iii) conducts deep analysis based\non the established benchmark and codebase, offering new insights into IMDL\nmodel architecture, dataset characteristics, and evaluation standards.\nSpecifically, IMDL-BenCo includes common processing algorithms, 8\nstate-of-the-art IMDL models (1 of which are reproduced from scratch), 2 sets\nof standard training and evaluation protocols, 15 GPU-accelerated evaluation\nmetrics, and 3 kinds of robustness evaluation. This benchmark and codebase\nrepresent a significant leap forward in calibrating the current progress in the\nIMDL field and inspiring future breakthroughs. Code is available at:\nhttps://github.com/scu-zjz/IMDLBenCo.\n", "link": "http://arxiv.org/abs/2406.10580v2", "date": "2024-11-08", "relevancy": 2.2114, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5546}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IMDL-BenCo%3A%20A%20Comprehensive%20Benchmark%20and%20Codebase%20for%20Image%0A%20%20Manipulation%20Detection%20%26%20Localization&body=Title%3A%20IMDL-BenCo%3A%20A%20Comprehensive%20Benchmark%20and%20Codebase%20for%20Image%0A%20%20Manipulation%20Detection%20%26%20Localization%0AAuthor%3A%20Xiaochen%20Ma%20and%20Xuekang%20Zhu%20and%20Lei%20Su%20and%20Bo%20Du%20and%20Zhuohang%20Jiang%20and%20Bingkui%20Tong%20and%20Zeyu%20Lei%20and%20Xinyu%20Yang%20and%20Chi-Man%20Pun%20and%20Jiancheng%20Lv%20and%20Jizhe%20Zhou%0AAbstract%3A%20%20%20A%20comprehensive%20benchmark%20is%20yet%20to%20be%20established%20in%20the%20Image%20Manipulation%0ADetection%20%26%20Localization%20%28IMDL%29%20field.%20The%20absence%20of%20such%20a%20benchmark%20leads%20to%0Ainsufficient%20and%20misleading%20model%20evaluations%2C%20severely%20undermining%20the%0Adevelopment%20of%20this%20field.%20However%2C%20the%20scarcity%20of%20open-sourced%20baseline%0Amodels%20and%20inconsistent%20training%20and%20evaluation%20protocols%20make%20conducting%0Arigorous%20experiments%20and%20faithful%20comparisons%20among%20IMDL%20models%20challenging.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20IMDL-BenCo%2C%20the%20first%20comprehensive%20IMDL%0Abenchmark%20and%20modular%20codebase.%20IMDL-BenCo%3A%20i%29%20decomposes%20the%20IMDL%20framework%0Ainto%20standardized%2C%20reusable%20components%20and%20revises%20the%20model%20construction%0Apipeline%2C%20improving%20coding%20efficiency%20and%20customization%20flexibility%3B%20ii%29%20fully%0Aimplements%20or%20incorporates%20training%20code%20for%20state-of-the-art%20models%20to%0Aestablish%20a%20comprehensive%20IMDL%20benchmark%3B%20and%20iii%29%20conducts%20deep%20analysis%20based%0Aon%20the%20established%20benchmark%20and%20codebase%2C%20offering%20new%20insights%20into%20IMDL%0Amodel%20architecture%2C%20dataset%20characteristics%2C%20and%20evaluation%20standards.%0ASpecifically%2C%20IMDL-BenCo%20includes%20common%20processing%20algorithms%2C%208%0Astate-of-the-art%20IMDL%20models%20%281%20of%20which%20are%20reproduced%20from%20scratch%29%2C%202%20sets%0Aof%20standard%20training%20and%20evaluation%20protocols%2C%2015%20GPU-accelerated%20evaluation%0Ametrics%2C%20and%203%20kinds%20of%20robustness%20evaluation.%20This%20benchmark%20and%20codebase%0Arepresent%20a%20significant%20leap%20forward%20in%20calibrating%20the%20current%20progress%20in%20the%0AIMDL%20field%20and%20inspiring%20future%20breakthroughs.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/scu-zjz/IMDLBenCo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.10580v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIMDL-BenCo%253A%2520A%2520Comprehensive%2520Benchmark%2520and%2520Codebase%2520for%2520Image%250A%2520%2520Manipulation%2520Detection%2520%2526%2520Localization%26entry.906535625%3DXiaochen%2520Ma%2520and%2520Xuekang%2520Zhu%2520and%2520Lei%2520Su%2520and%2520Bo%2520Du%2520and%2520Zhuohang%2520Jiang%2520and%2520Bingkui%2520Tong%2520and%2520Zeyu%2520Lei%2520and%2520Xinyu%2520Yang%2520and%2520Chi-Man%2520Pun%2520and%2520Jiancheng%2520Lv%2520and%2520Jizhe%2520Zhou%26entry.1292438233%3D%2520%2520A%2520comprehensive%2520benchmark%2520is%2520yet%2520to%2520be%2520established%2520in%2520the%2520Image%2520Manipulation%250ADetection%2520%2526%2520Localization%2520%2528IMDL%2529%2520field.%2520The%2520absence%2520of%2520such%2520a%2520benchmark%2520leads%2520to%250Ainsufficient%2520and%2520misleading%2520model%2520evaluations%252C%2520severely%2520undermining%2520the%250Adevelopment%2520of%2520this%2520field.%2520However%252C%2520the%2520scarcity%2520of%2520open-sourced%2520baseline%250Amodels%2520and%2520inconsistent%2520training%2520and%2520evaluation%2520protocols%2520make%2520conducting%250Arigorous%2520experiments%2520and%2520faithful%2520comparisons%2520among%2520IMDL%2520models%2520challenging.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520introduce%2520IMDL-BenCo%252C%2520the%2520first%2520comprehensive%2520IMDL%250Abenchmark%2520and%2520modular%2520codebase.%2520IMDL-BenCo%253A%2520i%2529%2520decomposes%2520the%2520IMDL%2520framework%250Ainto%2520standardized%252C%2520reusable%2520components%2520and%2520revises%2520the%2520model%2520construction%250Apipeline%252C%2520improving%2520coding%2520efficiency%2520and%2520customization%2520flexibility%253B%2520ii%2529%2520fully%250Aimplements%2520or%2520incorporates%2520training%2520code%2520for%2520state-of-the-art%2520models%2520to%250Aestablish%2520a%2520comprehensive%2520IMDL%2520benchmark%253B%2520and%2520iii%2529%2520conducts%2520deep%2520analysis%2520based%250Aon%2520the%2520established%2520benchmark%2520and%2520codebase%252C%2520offering%2520new%2520insights%2520into%2520IMDL%250Amodel%2520architecture%252C%2520dataset%2520characteristics%252C%2520and%2520evaluation%2520standards.%250ASpecifically%252C%2520IMDL-BenCo%2520includes%2520common%2520processing%2520algorithms%252C%25208%250Astate-of-the-art%2520IMDL%2520models%2520%25281%2520of%2520which%2520are%2520reproduced%2520from%2520scratch%2529%252C%25202%2520sets%250Aof%2520standard%2520training%2520and%2520evaluation%2520protocols%252C%252015%2520GPU-accelerated%2520evaluation%250Ametrics%252C%2520and%25203%2520kinds%2520of%2520robustness%2520evaluation.%2520This%2520benchmark%2520and%2520codebase%250Arepresent%2520a%2520significant%2520leap%2520forward%2520in%2520calibrating%2520the%2520current%2520progress%2520in%2520the%250AIMDL%2520field%2520and%2520inspiring%2520future%2520breakthroughs.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/scu-zjz/IMDLBenCo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.10580v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IMDL-BenCo%3A%20A%20Comprehensive%20Benchmark%20and%20Codebase%20for%20Image%0A%20%20Manipulation%20Detection%20%26%20Localization&entry.906535625=Xiaochen%20Ma%20and%20Xuekang%20Zhu%20and%20Lei%20Su%20and%20Bo%20Du%20and%20Zhuohang%20Jiang%20and%20Bingkui%20Tong%20and%20Zeyu%20Lei%20and%20Xinyu%20Yang%20and%20Chi-Man%20Pun%20and%20Jiancheng%20Lv%20and%20Jizhe%20Zhou&entry.1292438233=%20%20A%20comprehensive%20benchmark%20is%20yet%20to%20be%20established%20in%20the%20Image%20Manipulation%0ADetection%20%26%20Localization%20%28IMDL%29%20field.%20The%20absence%20of%20such%20a%20benchmark%20leads%20to%0Ainsufficient%20and%20misleading%20model%20evaluations%2C%20severely%20undermining%20the%0Adevelopment%20of%20this%20field.%20However%2C%20the%20scarcity%20of%20open-sourced%20baseline%0Amodels%20and%20inconsistent%20training%20and%20evaluation%20protocols%20make%20conducting%0Arigorous%20experiments%20and%20faithful%20comparisons%20among%20IMDL%20models%20challenging.%20To%0Aaddress%20these%20challenges%2C%20we%20introduce%20IMDL-BenCo%2C%20the%20first%20comprehensive%20IMDL%0Abenchmark%20and%20modular%20codebase.%20IMDL-BenCo%3A%20i%29%20decomposes%20the%20IMDL%20framework%0Ainto%20standardized%2C%20reusable%20components%20and%20revises%20the%20model%20construction%0Apipeline%2C%20improving%20coding%20efficiency%20and%20customization%20flexibility%3B%20ii%29%20fully%0Aimplements%20or%20incorporates%20training%20code%20for%20state-of-the-art%20models%20to%0Aestablish%20a%20comprehensive%20IMDL%20benchmark%3B%20and%20iii%29%20conducts%20deep%20analysis%20based%0Aon%20the%20established%20benchmark%20and%20codebase%2C%20offering%20new%20insights%20into%20IMDL%0Amodel%20architecture%2C%20dataset%20characteristics%2C%20and%20evaluation%20standards.%0ASpecifically%2C%20IMDL-BenCo%20includes%20common%20processing%20algorithms%2C%208%0Astate-of-the-art%20IMDL%20models%20%281%20of%20which%20are%20reproduced%20from%20scratch%29%2C%202%20sets%0Aof%20standard%20training%20and%20evaluation%20protocols%2C%2015%20GPU-accelerated%20evaluation%0Ametrics%2C%20and%203%20kinds%20of%20robustness%20evaluation.%20This%20benchmark%20and%20codebase%0Arepresent%20a%20significant%20leap%20forward%20in%20calibrating%20the%20current%20progress%20in%20the%0AIMDL%20field%20and%20inspiring%20future%20breakthroughs.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/scu-zjz/IMDLBenCo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.10580v2&entry.124074799=Read"},
{"title": "Data-Driven Distributed Common Operational Picture from Heterogeneous\n  Platforms using Multi-Agent Reinforcement Learning", "author": "Indranil Sur and Aswin Raghavan and Abrar Rahman and James Z Hare and Daniel Cassenti and Carl Busart", "abstract": "  The integration of unmanned platforms equipped with advanced sensors promises\nto enhance situational awareness and mitigate the \"fog of war\" in military\noperations. However, managing the vast influx of data from these platforms\nposes a significant challenge for Command and Control (C2) systems. This study\npresents a novel multi-agent learning framework to address this challenge. Our\nmethod enables autonomous and secure communication between agents and humans,\nwhich in turn enables real-time formation of an interpretable Common\nOperational Picture (COP). Each agent encodes its perceptions and actions into\ncompact vectors, which are then transmitted, received and decoded to form a COP\nencompassing the current state of all agents (friendly and enemy) on the\nbattlefield. Using Deep Reinforcement Learning (DRL), we jointly train COP\nmodels and agent's action selection policies. We demonstrate resilience to\ndegraded conditions such as denied GPS and disrupted communications.\nExperimental validation is performed in the Starcraft-2 simulation environment\nto evaluate the precision of the COPs and robustness of policies. We report\nless than 5% error in COPs and policies resilient to various adversarial\nconditions. In summary, our contributions include a method for autonomous COP\nformation, increased resilience through distributed prediction, and joint\ntraining of COP models and multi-agent RL policies. This research advances\nadaptive and resilient C2, facilitating effective control of heterogeneous\nunmanned platforms.\n", "link": "http://arxiv.org/abs/2411.05683v1", "date": "2024-11-08", "relevancy": 2.2042, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5608}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5597}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5385}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Driven%20Distributed%20Common%20Operational%20Picture%20from%20Heterogeneous%0A%20%20Platforms%20using%20Multi-Agent%20Reinforcement%20Learning&body=Title%3A%20Data-Driven%20Distributed%20Common%20Operational%20Picture%20from%20Heterogeneous%0A%20%20Platforms%20using%20Multi-Agent%20Reinforcement%20Learning%0AAuthor%3A%20Indranil%20Sur%20and%20Aswin%20Raghavan%20and%20Abrar%20Rahman%20and%20James%20Z%20Hare%20and%20Daniel%20Cassenti%20and%20Carl%20Busart%0AAbstract%3A%20%20%20The%20integration%20of%20unmanned%20platforms%20equipped%20with%20advanced%20sensors%20promises%0Ato%20enhance%20situational%20awareness%20and%20mitigate%20the%20%22fog%20of%20war%22%20in%20military%0Aoperations.%20However%2C%20managing%20the%20vast%20influx%20of%20data%20from%20these%20platforms%0Aposes%20a%20significant%20challenge%20for%20Command%20and%20Control%20%28C2%29%20systems.%20This%20study%0Apresents%20a%20novel%20multi-agent%20learning%20framework%20to%20address%20this%20challenge.%20Our%0Amethod%20enables%20autonomous%20and%20secure%20communication%20between%20agents%20and%20humans%2C%0Awhich%20in%20turn%20enables%20real-time%20formation%20of%20an%20interpretable%20Common%0AOperational%20Picture%20%28COP%29.%20Each%20agent%20encodes%20its%20perceptions%20and%20actions%20into%0Acompact%20vectors%2C%20which%20are%20then%20transmitted%2C%20received%20and%20decoded%20to%20form%20a%20COP%0Aencompassing%20the%20current%20state%20of%20all%20agents%20%28friendly%20and%20enemy%29%20on%20the%0Abattlefield.%20Using%20Deep%20Reinforcement%20Learning%20%28DRL%29%2C%20we%20jointly%20train%20COP%0Amodels%20and%20agent%27s%20action%20selection%20policies.%20We%20demonstrate%20resilience%20to%0Adegraded%20conditions%20such%20as%20denied%20GPS%20and%20disrupted%20communications.%0AExperimental%20validation%20is%20performed%20in%20the%20Starcraft-2%20simulation%20environment%0Ato%20evaluate%20the%20precision%20of%20the%20COPs%20and%20robustness%20of%20policies.%20We%20report%0Aless%20than%205%25%20error%20in%20COPs%20and%20policies%20resilient%20to%20various%20adversarial%0Aconditions.%20In%20summary%2C%20our%20contributions%20include%20a%20method%20for%20autonomous%20COP%0Aformation%2C%20increased%20resilience%20through%20distributed%20prediction%2C%20and%20joint%0Atraining%20of%20COP%20models%20and%20multi-agent%20RL%20policies.%20This%20research%20advances%0Aadaptive%20and%20resilient%20C2%2C%20facilitating%20effective%20control%20of%20heterogeneous%0Aunmanned%20platforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Driven%2520Distributed%2520Common%2520Operational%2520Picture%2520from%2520Heterogeneous%250A%2520%2520Platforms%2520using%2520Multi-Agent%2520Reinforcement%2520Learning%26entry.906535625%3DIndranil%2520Sur%2520and%2520Aswin%2520Raghavan%2520and%2520Abrar%2520Rahman%2520and%2520James%2520Z%2520Hare%2520and%2520Daniel%2520Cassenti%2520and%2520Carl%2520Busart%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520unmanned%2520platforms%2520equipped%2520with%2520advanced%2520sensors%2520promises%250Ato%2520enhance%2520situational%2520awareness%2520and%2520mitigate%2520the%2520%2522fog%2520of%2520war%2522%2520in%2520military%250Aoperations.%2520However%252C%2520managing%2520the%2520vast%2520influx%2520of%2520data%2520from%2520these%2520platforms%250Aposes%2520a%2520significant%2520challenge%2520for%2520Command%2520and%2520Control%2520%2528C2%2529%2520systems.%2520This%2520study%250Apresents%2520a%2520novel%2520multi-agent%2520learning%2520framework%2520to%2520address%2520this%2520challenge.%2520Our%250Amethod%2520enables%2520autonomous%2520and%2520secure%2520communication%2520between%2520agents%2520and%2520humans%252C%250Awhich%2520in%2520turn%2520enables%2520real-time%2520formation%2520of%2520an%2520interpretable%2520Common%250AOperational%2520Picture%2520%2528COP%2529.%2520Each%2520agent%2520encodes%2520its%2520perceptions%2520and%2520actions%2520into%250Acompact%2520vectors%252C%2520which%2520are%2520then%2520transmitted%252C%2520received%2520and%2520decoded%2520to%2520form%2520a%2520COP%250Aencompassing%2520the%2520current%2520state%2520of%2520all%2520agents%2520%2528friendly%2520and%2520enemy%2529%2520on%2520the%250Abattlefield.%2520Using%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%252C%2520we%2520jointly%2520train%2520COP%250Amodels%2520and%2520agent%2527s%2520action%2520selection%2520policies.%2520We%2520demonstrate%2520resilience%2520to%250Adegraded%2520conditions%2520such%2520as%2520denied%2520GPS%2520and%2520disrupted%2520communications.%250AExperimental%2520validation%2520is%2520performed%2520in%2520the%2520Starcraft-2%2520simulation%2520environment%250Ato%2520evaluate%2520the%2520precision%2520of%2520the%2520COPs%2520and%2520robustness%2520of%2520policies.%2520We%2520report%250Aless%2520than%25205%2525%2520error%2520in%2520COPs%2520and%2520policies%2520resilient%2520to%2520various%2520adversarial%250Aconditions.%2520In%2520summary%252C%2520our%2520contributions%2520include%2520a%2520method%2520for%2520autonomous%2520COP%250Aformation%252C%2520increased%2520resilience%2520through%2520distributed%2520prediction%252C%2520and%2520joint%250Atraining%2520of%2520COP%2520models%2520and%2520multi-agent%2520RL%2520policies.%2520This%2520research%2520advances%250Aadaptive%2520and%2520resilient%2520C2%252C%2520facilitating%2520effective%2520control%2520of%2520heterogeneous%250Aunmanned%2520platforms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Driven%20Distributed%20Common%20Operational%20Picture%20from%20Heterogeneous%0A%20%20Platforms%20using%20Multi-Agent%20Reinforcement%20Learning&entry.906535625=Indranil%20Sur%20and%20Aswin%20Raghavan%20and%20Abrar%20Rahman%20and%20James%20Z%20Hare%20and%20Daniel%20Cassenti%20and%20Carl%20Busart&entry.1292438233=%20%20The%20integration%20of%20unmanned%20platforms%20equipped%20with%20advanced%20sensors%20promises%0Ato%20enhance%20situational%20awareness%20and%20mitigate%20the%20%22fog%20of%20war%22%20in%20military%0Aoperations.%20However%2C%20managing%20the%20vast%20influx%20of%20data%20from%20these%20platforms%0Aposes%20a%20significant%20challenge%20for%20Command%20and%20Control%20%28C2%29%20systems.%20This%20study%0Apresents%20a%20novel%20multi-agent%20learning%20framework%20to%20address%20this%20challenge.%20Our%0Amethod%20enables%20autonomous%20and%20secure%20communication%20between%20agents%20and%20humans%2C%0Awhich%20in%20turn%20enables%20real-time%20formation%20of%20an%20interpretable%20Common%0AOperational%20Picture%20%28COP%29.%20Each%20agent%20encodes%20its%20perceptions%20and%20actions%20into%0Acompact%20vectors%2C%20which%20are%20then%20transmitted%2C%20received%20and%20decoded%20to%20form%20a%20COP%0Aencompassing%20the%20current%20state%20of%20all%20agents%20%28friendly%20and%20enemy%29%20on%20the%0Abattlefield.%20Using%20Deep%20Reinforcement%20Learning%20%28DRL%29%2C%20we%20jointly%20train%20COP%0Amodels%20and%20agent%27s%20action%20selection%20policies.%20We%20demonstrate%20resilience%20to%0Adegraded%20conditions%20such%20as%20denied%20GPS%20and%20disrupted%20communications.%0AExperimental%20validation%20is%20performed%20in%20the%20Starcraft-2%20simulation%20environment%0Ato%20evaluate%20the%20precision%20of%20the%20COPs%20and%20robustness%20of%20policies.%20We%20report%0Aless%20than%205%25%20error%20in%20COPs%20and%20policies%20resilient%20to%20various%20adversarial%0Aconditions.%20In%20summary%2C%20our%20contributions%20include%20a%20method%20for%20autonomous%20COP%0Aformation%2C%20increased%20resilience%20through%20distributed%20prediction%2C%20and%20joint%0Atraining%20of%20COP%20models%20and%20multi-agent%20RL%20policies.%20This%20research%20advances%0Aadaptive%20and%20resilient%20C2%2C%20facilitating%20effective%20control%20of%20heterogeneous%0Aunmanned%20platforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05683v1&entry.124074799=Read"},
{"title": "A Two-Step Concept-Based Approach for Enhanced Interpretability and\n  Trust in Skin Lesion Diagnosis", "author": "Cristiano Patr\u00edcio and Lu\u00eds F. Teixeira and Jo\u00e3o C. Neves", "abstract": "  The main challenges hindering the adoption of deep learning-based systems in\nclinical settings are the scarcity of annotated data and the lack of\ninterpretability and trust in these systems. Concept Bottleneck Models (CBMs)\noffer inherent interpretability by constraining the final disease prediction on\na set of human-understandable concepts. However, this inherent interpretability\ncomes at the cost of greater annotation burden. Additionally, adding new\nconcepts requires retraining the entire system. In this work, we introduce a\nnovel two-step methodology that addresses both of these challenges. By\nsimulating the two stages of a CBM, we utilize a pretrained Vision Language\nModel (VLM) to automatically predict clinical concepts, and a Large Language\nModel (LLM) to generate disease diagnoses based on the predicted concepts. We\nvalidate our approach on three skin lesion datasets, demonstrating that it\noutperforms traditional CBMs and state-of-the-art explainable methods, all\nwithout requiring any training and utilizing only a few annotated examples. The\ncode is available at\nhttps://github.com/CristianoPatricio/2-step-concept-based-skin-diagnosis.\n", "link": "http://arxiv.org/abs/2411.05609v1", "date": "2024-11-08", "relevancy": 2.1919, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5491}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Two-Step%20Concept-Based%20Approach%20for%20Enhanced%20Interpretability%20and%0A%20%20Trust%20in%20Skin%20Lesion%20Diagnosis&body=Title%3A%20A%20Two-Step%20Concept-Based%20Approach%20for%20Enhanced%20Interpretability%20and%0A%20%20Trust%20in%20Skin%20Lesion%20Diagnosis%0AAuthor%3A%20Cristiano%20Patr%C3%ADcio%20and%20Lu%C3%ADs%20F.%20Teixeira%20and%20Jo%C3%A3o%20C.%20Neves%0AAbstract%3A%20%20%20The%20main%20challenges%20hindering%20the%20adoption%20of%20deep%20learning-based%20systems%20in%0Aclinical%20settings%20are%20the%20scarcity%20of%20annotated%20data%20and%20the%20lack%20of%0Ainterpretability%20and%20trust%20in%20these%20systems.%20Concept%20Bottleneck%20Models%20%28CBMs%29%0Aoffer%20inherent%20interpretability%20by%20constraining%20the%20final%20disease%20prediction%20on%0Aa%20set%20of%20human-understandable%20concepts.%20However%2C%20this%20inherent%20interpretability%0Acomes%20at%20the%20cost%20of%20greater%20annotation%20burden.%20Additionally%2C%20adding%20new%0Aconcepts%20requires%20retraining%20the%20entire%20system.%20In%20this%20work%2C%20we%20introduce%20a%0Anovel%20two-step%20methodology%20that%20addresses%20both%20of%20these%20challenges.%20By%0Asimulating%20the%20two%20stages%20of%20a%20CBM%2C%20we%20utilize%20a%20pretrained%20Vision%20Language%0AModel%20%28VLM%29%20to%20automatically%20predict%20clinical%20concepts%2C%20and%20a%20Large%20Language%0AModel%20%28LLM%29%20to%20generate%20disease%20diagnoses%20based%20on%20the%20predicted%20concepts.%20We%0Avalidate%20our%20approach%20on%20three%20skin%20lesion%20datasets%2C%20demonstrating%20that%20it%0Aoutperforms%20traditional%20CBMs%20and%20state-of-the-art%20explainable%20methods%2C%20all%0Awithout%20requiring%20any%20training%20and%20utilizing%20only%20a%20few%20annotated%20examples.%20The%0Acode%20is%20available%20at%0Ahttps%3A//github.com/CristianoPatricio/2-step-concept-based-skin-diagnosis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Two-Step%2520Concept-Based%2520Approach%2520for%2520Enhanced%2520Interpretability%2520and%250A%2520%2520Trust%2520in%2520Skin%2520Lesion%2520Diagnosis%26entry.906535625%3DCristiano%2520Patr%25C3%25ADcio%2520and%2520Lu%25C3%25ADs%2520F.%2520Teixeira%2520and%2520Jo%25C3%25A3o%2520C.%2520Neves%26entry.1292438233%3D%2520%2520The%2520main%2520challenges%2520hindering%2520the%2520adoption%2520of%2520deep%2520learning-based%2520systems%2520in%250Aclinical%2520settings%2520are%2520the%2520scarcity%2520of%2520annotated%2520data%2520and%2520the%2520lack%2520of%250Ainterpretability%2520and%2520trust%2520in%2520these%2520systems.%2520Concept%2520Bottleneck%2520Models%2520%2528CBMs%2529%250Aoffer%2520inherent%2520interpretability%2520by%2520constraining%2520the%2520final%2520disease%2520prediction%2520on%250Aa%2520set%2520of%2520human-understandable%2520concepts.%2520However%252C%2520this%2520inherent%2520interpretability%250Acomes%2520at%2520the%2520cost%2520of%2520greater%2520annotation%2520burden.%2520Additionally%252C%2520adding%2520new%250Aconcepts%2520requires%2520retraining%2520the%2520entire%2520system.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%250Anovel%2520two-step%2520methodology%2520that%2520addresses%2520both%2520of%2520these%2520challenges.%2520By%250Asimulating%2520the%2520two%2520stages%2520of%2520a%2520CBM%252C%2520we%2520utilize%2520a%2520pretrained%2520Vision%2520Language%250AModel%2520%2528VLM%2529%2520to%2520automatically%2520predict%2520clinical%2520concepts%252C%2520and%2520a%2520Large%2520Language%250AModel%2520%2528LLM%2529%2520to%2520generate%2520disease%2520diagnoses%2520based%2520on%2520the%2520predicted%2520concepts.%2520We%250Avalidate%2520our%2520approach%2520on%2520three%2520skin%2520lesion%2520datasets%252C%2520demonstrating%2520that%2520it%250Aoutperforms%2520traditional%2520CBMs%2520and%2520state-of-the-art%2520explainable%2520methods%252C%2520all%250Awithout%2520requiring%2520any%2520training%2520and%2520utilizing%2520only%2520a%2520few%2520annotated%2520examples.%2520The%250Acode%2520is%2520available%2520at%250Ahttps%253A//github.com/CristianoPatricio/2-step-concept-based-skin-diagnosis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Two-Step%20Concept-Based%20Approach%20for%20Enhanced%20Interpretability%20and%0A%20%20Trust%20in%20Skin%20Lesion%20Diagnosis&entry.906535625=Cristiano%20Patr%C3%ADcio%20and%20Lu%C3%ADs%20F.%20Teixeira%20and%20Jo%C3%A3o%20C.%20Neves&entry.1292438233=%20%20The%20main%20challenges%20hindering%20the%20adoption%20of%20deep%20learning-based%20systems%20in%0Aclinical%20settings%20are%20the%20scarcity%20of%20annotated%20data%20and%20the%20lack%20of%0Ainterpretability%20and%20trust%20in%20these%20systems.%20Concept%20Bottleneck%20Models%20%28CBMs%29%0Aoffer%20inherent%20interpretability%20by%20constraining%20the%20final%20disease%20prediction%20on%0Aa%20set%20of%20human-understandable%20concepts.%20However%2C%20this%20inherent%20interpretability%0Acomes%20at%20the%20cost%20of%20greater%20annotation%20burden.%20Additionally%2C%20adding%20new%0Aconcepts%20requires%20retraining%20the%20entire%20system.%20In%20this%20work%2C%20we%20introduce%20a%0Anovel%20two-step%20methodology%20that%20addresses%20both%20of%20these%20challenges.%20By%0Asimulating%20the%20two%20stages%20of%20a%20CBM%2C%20we%20utilize%20a%20pretrained%20Vision%20Language%0AModel%20%28VLM%29%20to%20automatically%20predict%20clinical%20concepts%2C%20and%20a%20Large%20Language%0AModel%20%28LLM%29%20to%20generate%20disease%20diagnoses%20based%20on%20the%20predicted%20concepts.%20We%0Avalidate%20our%20approach%20on%20three%20skin%20lesion%20datasets%2C%20demonstrating%20that%20it%0Aoutperforms%20traditional%20CBMs%20and%20state-of-the-art%20explainable%20methods%2C%20all%0Awithout%20requiring%20any%20training%20and%20utilizing%20only%20a%20few%20annotated%20examples.%20The%0Acode%20is%20available%20at%0Ahttps%3A//github.com/CristianoPatricio/2-step-concept-based-skin-diagnosis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05609v1&entry.124074799=Read"},
{"title": "DeepArUco++: Improved detection of square fiducial markers in\n  challenging lighting conditions", "author": "Rafael Berral-Soler and Rafael Mu\u00f1oz-Salinas and Rafael Medina-Carnicer and Manuel J. Mar\u00edn-Jim\u00e9nez", "abstract": "  Fiducial markers are a computer vision tool used for object pose estimation\nand detection. These markers are highly useful in fields such as industry,\nmedicine and logistics. However, optimal lighting conditions are not always\navailable,and other factors such as blur or sensor noise can affect image\nquality. Classical computer vision techniques that precisely locate and decode\nfiducial markers often fail under difficult illumination conditions (e.g.\nextreme variations of lighting within the same frame). Hence, we propose\nDeepArUco++, a deep learning-based framework that leverages the robustness of\nConvolutional Neural Networks to perform marker detection and decoding in\nchallenging lighting conditions. The framework is based on a pipeline using\ndifferent Neural Network models at each step, namely marker detection, corner\nrefinement and marker decoding. Additionally, we propose a simple method for\ngenerating synthetic data for training the different models that compose the\nproposed pipeline, and we present a second, real-life dataset of ArUco markers\nin challenging lighting conditions used to evaluate our system. The developed\nmethod outperforms other state-of-the-art methods in such tasks and remains\ncompetitive even when testing on the datasets used to develop those methods.\nCode available in GitHub: https://github.com/AVAuco/deeparuco/\n", "link": "http://arxiv.org/abs/2411.05552v1", "date": "2024-11-08", "relevancy": 2.1829, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5565}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepArUco%2B%2B%3A%20Improved%20detection%20of%20square%20fiducial%20markers%20in%0A%20%20challenging%20lighting%20conditions&body=Title%3A%20DeepArUco%2B%2B%3A%20Improved%20detection%20of%20square%20fiducial%20markers%20in%0A%20%20challenging%20lighting%20conditions%0AAuthor%3A%20Rafael%20Berral-Soler%20and%20Rafael%20Mu%C3%B1oz-Salinas%20and%20Rafael%20Medina-Carnicer%20and%20Manuel%20J.%20Mar%C3%ADn-Jim%C3%A9nez%0AAbstract%3A%20%20%20Fiducial%20markers%20are%20a%20computer%20vision%20tool%20used%20for%20object%20pose%20estimation%0Aand%20detection.%20These%20markers%20are%20highly%20useful%20in%20fields%20such%20as%20industry%2C%0Amedicine%20and%20logistics.%20However%2C%20optimal%20lighting%20conditions%20are%20not%20always%0Aavailable%2Cand%20other%20factors%20such%20as%20blur%20or%20sensor%20noise%20can%20affect%20image%0Aquality.%20Classical%20computer%20vision%20techniques%20that%20precisely%20locate%20and%20decode%0Afiducial%20markers%20often%20fail%20under%20difficult%20illumination%20conditions%20%28e.g.%0Aextreme%20variations%20of%20lighting%20within%20the%20same%20frame%29.%20Hence%2C%20we%20propose%0ADeepArUco%2B%2B%2C%20a%20deep%20learning-based%20framework%20that%20leverages%20the%20robustness%20of%0AConvolutional%20Neural%20Networks%20to%20perform%20marker%20detection%20and%20decoding%20in%0Achallenging%20lighting%20conditions.%20The%20framework%20is%20based%20on%20a%20pipeline%20using%0Adifferent%20Neural%20Network%20models%20at%20each%20step%2C%20namely%20marker%20detection%2C%20corner%0Arefinement%20and%20marker%20decoding.%20Additionally%2C%20we%20propose%20a%20simple%20method%20for%0Agenerating%20synthetic%20data%20for%20training%20the%20different%20models%20that%20compose%20the%0Aproposed%20pipeline%2C%20and%20we%20present%20a%20second%2C%20real-life%20dataset%20of%20ArUco%20markers%0Ain%20challenging%20lighting%20conditions%20used%20to%20evaluate%20our%20system.%20The%20developed%0Amethod%20outperforms%20other%20state-of-the-art%20methods%20in%20such%20tasks%20and%20remains%0Acompetitive%20even%20when%20testing%20on%20the%20datasets%20used%20to%20develop%20those%20methods.%0ACode%20available%20in%20GitHub%3A%20https%3A//github.com/AVAuco/deeparuco/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepArUco%252B%252B%253A%2520Improved%2520detection%2520of%2520square%2520fiducial%2520markers%2520in%250A%2520%2520challenging%2520lighting%2520conditions%26entry.906535625%3DRafael%2520Berral-Soler%2520and%2520Rafael%2520Mu%25C3%25B1oz-Salinas%2520and%2520Rafael%2520Medina-Carnicer%2520and%2520Manuel%2520J.%2520Mar%25C3%25ADn-Jim%25C3%25A9nez%26entry.1292438233%3D%2520%2520Fiducial%2520markers%2520are%2520a%2520computer%2520vision%2520tool%2520used%2520for%2520object%2520pose%2520estimation%250Aand%2520detection.%2520These%2520markers%2520are%2520highly%2520useful%2520in%2520fields%2520such%2520as%2520industry%252C%250Amedicine%2520and%2520logistics.%2520However%252C%2520optimal%2520lighting%2520conditions%2520are%2520not%2520always%250Aavailable%252Cand%2520other%2520factors%2520such%2520as%2520blur%2520or%2520sensor%2520noise%2520can%2520affect%2520image%250Aquality.%2520Classical%2520computer%2520vision%2520techniques%2520that%2520precisely%2520locate%2520and%2520decode%250Afiducial%2520markers%2520often%2520fail%2520under%2520difficult%2520illumination%2520conditions%2520%2528e.g.%250Aextreme%2520variations%2520of%2520lighting%2520within%2520the%2520same%2520frame%2529.%2520Hence%252C%2520we%2520propose%250ADeepArUco%252B%252B%252C%2520a%2520deep%2520learning-based%2520framework%2520that%2520leverages%2520the%2520robustness%2520of%250AConvolutional%2520Neural%2520Networks%2520to%2520perform%2520marker%2520detection%2520and%2520decoding%2520in%250Achallenging%2520lighting%2520conditions.%2520The%2520framework%2520is%2520based%2520on%2520a%2520pipeline%2520using%250Adifferent%2520Neural%2520Network%2520models%2520at%2520each%2520step%252C%2520namely%2520marker%2520detection%252C%2520corner%250Arefinement%2520and%2520marker%2520decoding.%2520Additionally%252C%2520we%2520propose%2520a%2520simple%2520method%2520for%250Agenerating%2520synthetic%2520data%2520for%2520training%2520the%2520different%2520models%2520that%2520compose%2520the%250Aproposed%2520pipeline%252C%2520and%2520we%2520present%2520a%2520second%252C%2520real-life%2520dataset%2520of%2520ArUco%2520markers%250Ain%2520challenging%2520lighting%2520conditions%2520used%2520to%2520evaluate%2520our%2520system.%2520The%2520developed%250Amethod%2520outperforms%2520other%2520state-of-the-art%2520methods%2520in%2520such%2520tasks%2520and%2520remains%250Acompetitive%2520even%2520when%2520testing%2520on%2520the%2520datasets%2520used%2520to%2520develop%2520those%2520methods.%250ACode%2520available%2520in%2520GitHub%253A%2520https%253A//github.com/AVAuco/deeparuco/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepArUco%2B%2B%3A%20Improved%20detection%20of%20square%20fiducial%20markers%20in%0A%20%20challenging%20lighting%20conditions&entry.906535625=Rafael%20Berral-Soler%20and%20Rafael%20Mu%C3%B1oz-Salinas%20and%20Rafael%20Medina-Carnicer%20and%20Manuel%20J.%20Mar%C3%ADn-Jim%C3%A9nez&entry.1292438233=%20%20Fiducial%20markers%20are%20a%20computer%20vision%20tool%20used%20for%20object%20pose%20estimation%0Aand%20detection.%20These%20markers%20are%20highly%20useful%20in%20fields%20such%20as%20industry%2C%0Amedicine%20and%20logistics.%20However%2C%20optimal%20lighting%20conditions%20are%20not%20always%0Aavailable%2Cand%20other%20factors%20such%20as%20blur%20or%20sensor%20noise%20can%20affect%20image%0Aquality.%20Classical%20computer%20vision%20techniques%20that%20precisely%20locate%20and%20decode%0Afiducial%20markers%20often%20fail%20under%20difficult%20illumination%20conditions%20%28e.g.%0Aextreme%20variations%20of%20lighting%20within%20the%20same%20frame%29.%20Hence%2C%20we%20propose%0ADeepArUco%2B%2B%2C%20a%20deep%20learning-based%20framework%20that%20leverages%20the%20robustness%20of%0AConvolutional%20Neural%20Networks%20to%20perform%20marker%20detection%20and%20decoding%20in%0Achallenging%20lighting%20conditions.%20The%20framework%20is%20based%20on%20a%20pipeline%20using%0Adifferent%20Neural%20Network%20models%20at%20each%20step%2C%20namely%20marker%20detection%2C%20corner%0Arefinement%20and%20marker%20decoding.%20Additionally%2C%20we%20propose%20a%20simple%20method%20for%0Agenerating%20synthetic%20data%20for%20training%20the%20different%20models%20that%20compose%20the%0Aproposed%20pipeline%2C%20and%20we%20present%20a%20second%2C%20real-life%20dataset%20of%20ArUco%20markers%0Ain%20challenging%20lighting%20conditions%20used%20to%20evaluate%20our%20system.%20The%20developed%0Amethod%20outperforms%20other%20state-of-the-art%20methods%20in%20such%20tasks%20and%20remains%0Acompetitive%20even%20when%20testing%20on%20the%20datasets%20used%20to%20develop%20those%20methods.%0ACode%20available%20in%20GitHub%3A%20https%3A//github.com/AVAuco/deeparuco/%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05552v1&entry.124074799=Read"},
{"title": "Handling geometrical variability in nonlinear reduced order modeling\n  through Continuous Geometry-Aware DL-ROMs", "author": "Simone Brivio and Stefania Fresca and Andrea Manzoni", "abstract": "  Deep Learning-based Reduced Order Models (DL-ROMs) provide nowadays a\nwell-established class of accurate surrogate models for complex physical\nsystems described by parametrized PDEs, by nonlinearly compressing the solution\nmanifold into a handful of latent coordinates. Until now, design and\napplication of DL-ROMs mainly focused on physically parameterized problems.\nWithin this work, we provide a novel extension of these architectures to\nproblems featuring geometrical variability and parametrized domains, namely, we\npropose Continuous Geometry-Aware DL-ROMs (CGA-DL-ROMs). In particular, the\nspace-continuous nature of the proposed architecture matches the need to deal\nwith multi-resolution datasets, which are quite common in the case of\ngeometrically parametrized problems. Moreover, CGA-DL-ROMs are endowed with a\nstrong inductive bias that makes them aware of geometrical parametrizations,\nthus enhancing both the compression capability and the overall performance of\nthe architecture. Within this work, we justify our findings through a thorough\ntheoretical analysis, and we practically validate our claims by means of a\nseries of numerical tests encompassing physically-and-geometrically\nparametrized PDEs, ranging from the unsteady Navier-Stokes equations for fluid\ndynamics to advection-diffusion-reaction equations for mathematical biology.\n", "link": "http://arxiv.org/abs/2411.05486v1", "date": "2024-11-08", "relevancy": 2.1615, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5658}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5265}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Handling%20geometrical%20variability%20in%20nonlinear%20reduced%20order%20modeling%0A%20%20through%20Continuous%20Geometry-Aware%20DL-ROMs&body=Title%3A%20Handling%20geometrical%20variability%20in%20nonlinear%20reduced%20order%20modeling%0A%20%20through%20Continuous%20Geometry-Aware%20DL-ROMs%0AAuthor%3A%20Simone%20Brivio%20and%20Stefania%20Fresca%20and%20Andrea%20Manzoni%0AAbstract%3A%20%20%20Deep%20Learning-based%20Reduced%20Order%20Models%20%28DL-ROMs%29%20provide%20nowadays%20a%0Awell-established%20class%20of%20accurate%20surrogate%20models%20for%20complex%20physical%0Asystems%20described%20by%20parametrized%20PDEs%2C%20by%20nonlinearly%20compressing%20the%20solution%0Amanifold%20into%20a%20handful%20of%20latent%20coordinates.%20Until%20now%2C%20design%20and%0Aapplication%20of%20DL-ROMs%20mainly%20focused%20on%20physically%20parameterized%20problems.%0AWithin%20this%20work%2C%20we%20provide%20a%20novel%20extension%20of%20these%20architectures%20to%0Aproblems%20featuring%20geometrical%20variability%20and%20parametrized%20domains%2C%20namely%2C%20we%0Apropose%20Continuous%20Geometry-Aware%20DL-ROMs%20%28CGA-DL-ROMs%29.%20In%20particular%2C%20the%0Aspace-continuous%20nature%20of%20the%20proposed%20architecture%20matches%20the%20need%20to%20deal%0Awith%20multi-resolution%20datasets%2C%20which%20are%20quite%20common%20in%20the%20case%20of%0Ageometrically%20parametrized%20problems.%20Moreover%2C%20CGA-DL-ROMs%20are%20endowed%20with%20a%0Astrong%20inductive%20bias%20that%20makes%20them%20aware%20of%20geometrical%20parametrizations%2C%0Athus%20enhancing%20both%20the%20compression%20capability%20and%20the%20overall%20performance%20of%0Athe%20architecture.%20Within%20this%20work%2C%20we%20justify%20our%20findings%20through%20a%20thorough%0Atheoretical%20analysis%2C%20and%20we%20practically%20validate%20our%20claims%20by%20means%20of%20a%0Aseries%20of%20numerical%20tests%20encompassing%20physically-and-geometrically%0Aparametrized%20PDEs%2C%20ranging%20from%20the%20unsteady%20Navier-Stokes%20equations%20for%20fluid%0Adynamics%20to%20advection-diffusion-reaction%20equations%20for%20mathematical%20biology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05486v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHandling%2520geometrical%2520variability%2520in%2520nonlinear%2520reduced%2520order%2520modeling%250A%2520%2520through%2520Continuous%2520Geometry-Aware%2520DL-ROMs%26entry.906535625%3DSimone%2520Brivio%2520and%2520Stefania%2520Fresca%2520and%2520Andrea%2520Manzoni%26entry.1292438233%3D%2520%2520Deep%2520Learning-based%2520Reduced%2520Order%2520Models%2520%2528DL-ROMs%2529%2520provide%2520nowadays%2520a%250Awell-established%2520class%2520of%2520accurate%2520surrogate%2520models%2520for%2520complex%2520physical%250Asystems%2520described%2520by%2520parametrized%2520PDEs%252C%2520by%2520nonlinearly%2520compressing%2520the%2520solution%250Amanifold%2520into%2520a%2520handful%2520of%2520latent%2520coordinates.%2520Until%2520now%252C%2520design%2520and%250Aapplication%2520of%2520DL-ROMs%2520mainly%2520focused%2520on%2520physically%2520parameterized%2520problems.%250AWithin%2520this%2520work%252C%2520we%2520provide%2520a%2520novel%2520extension%2520of%2520these%2520architectures%2520to%250Aproblems%2520featuring%2520geometrical%2520variability%2520and%2520parametrized%2520domains%252C%2520namely%252C%2520we%250Apropose%2520Continuous%2520Geometry-Aware%2520DL-ROMs%2520%2528CGA-DL-ROMs%2529.%2520In%2520particular%252C%2520the%250Aspace-continuous%2520nature%2520of%2520the%2520proposed%2520architecture%2520matches%2520the%2520need%2520to%2520deal%250Awith%2520multi-resolution%2520datasets%252C%2520which%2520are%2520quite%2520common%2520in%2520the%2520case%2520of%250Ageometrically%2520parametrized%2520problems.%2520Moreover%252C%2520CGA-DL-ROMs%2520are%2520endowed%2520with%2520a%250Astrong%2520inductive%2520bias%2520that%2520makes%2520them%2520aware%2520of%2520geometrical%2520parametrizations%252C%250Athus%2520enhancing%2520both%2520the%2520compression%2520capability%2520and%2520the%2520overall%2520performance%2520of%250Athe%2520architecture.%2520Within%2520this%2520work%252C%2520we%2520justify%2520our%2520findings%2520through%2520a%2520thorough%250Atheoretical%2520analysis%252C%2520and%2520we%2520practically%2520validate%2520our%2520claims%2520by%2520means%2520of%2520a%250Aseries%2520of%2520numerical%2520tests%2520encompassing%2520physically-and-geometrically%250Aparametrized%2520PDEs%252C%2520ranging%2520from%2520the%2520unsteady%2520Navier-Stokes%2520equations%2520for%2520fluid%250Adynamics%2520to%2520advection-diffusion-reaction%2520equations%2520for%2520mathematical%2520biology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05486v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Handling%20geometrical%20variability%20in%20nonlinear%20reduced%20order%20modeling%0A%20%20through%20Continuous%20Geometry-Aware%20DL-ROMs&entry.906535625=Simone%20Brivio%20and%20Stefania%20Fresca%20and%20Andrea%20Manzoni&entry.1292438233=%20%20Deep%20Learning-based%20Reduced%20Order%20Models%20%28DL-ROMs%29%20provide%20nowadays%20a%0Awell-established%20class%20of%20accurate%20surrogate%20models%20for%20complex%20physical%0Asystems%20described%20by%20parametrized%20PDEs%2C%20by%20nonlinearly%20compressing%20the%20solution%0Amanifold%20into%20a%20handful%20of%20latent%20coordinates.%20Until%20now%2C%20design%20and%0Aapplication%20of%20DL-ROMs%20mainly%20focused%20on%20physically%20parameterized%20problems.%0AWithin%20this%20work%2C%20we%20provide%20a%20novel%20extension%20of%20these%20architectures%20to%0Aproblems%20featuring%20geometrical%20variability%20and%20parametrized%20domains%2C%20namely%2C%20we%0Apropose%20Continuous%20Geometry-Aware%20DL-ROMs%20%28CGA-DL-ROMs%29.%20In%20particular%2C%20the%0Aspace-continuous%20nature%20of%20the%20proposed%20architecture%20matches%20the%20need%20to%20deal%0Awith%20multi-resolution%20datasets%2C%20which%20are%20quite%20common%20in%20the%20case%20of%0Ageometrically%20parametrized%20problems.%20Moreover%2C%20CGA-DL-ROMs%20are%20endowed%20with%20a%0Astrong%20inductive%20bias%20that%20makes%20them%20aware%20of%20geometrical%20parametrizations%2C%0Athus%20enhancing%20both%20the%20compression%20capability%20and%20the%20overall%20performance%20of%0Athe%20architecture.%20Within%20this%20work%2C%20we%20justify%20our%20findings%20through%20a%20thorough%0Atheoretical%20analysis%2C%20and%20we%20practically%20validate%20our%20claims%20by%20means%20of%20a%0Aseries%20of%20numerical%20tests%20encompassing%20physically-and-geometrically%0Aparametrized%20PDEs%2C%20ranging%20from%20the%20unsteady%20Navier-Stokes%20equations%20for%20fluid%0Adynamics%20to%20advection-diffusion-reaction%20equations%20for%20mathematical%20biology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05486v1&entry.124074799=Read"},
{"title": "Sketched Equivariant Imaging Regularization and Deep Internal Learning\n  for Inverse Problems", "author": "Guixian Xu and Jinglai Li and Junqi Tang", "abstract": "  Equivariant Imaging (EI) regularization has become the de-facto technique for\nunsupervised training of deep imaging networks, without any need of\nground-truth data. Observing that the EI-based unsupervised training paradigm\ncurrently has significant computational redundancy leading to inefficiency in\nhigh-dimensional applications, we propose a sketched EI regularization which\nleverages the randomized sketching techniques for acceleration. We then extend\nour sketched EI regularization to develop an accelerated deep internal learning\nframework -- Sketched Equivariant Deep Image Prior (Sk.EI-DIP), which can be\nefficiently applied for single-image and task-adapted reconstruction. Our\nnumerical study on X-ray CT image reconstruction tasks demonstrate that our\napproach can achieve order-of-magnitude computational acceleration over\nstandard EI-based counterpart in single-input setting, and network adaptation\nat test time.\n", "link": "http://arxiv.org/abs/2411.05771v1", "date": "2024-11-08", "relevancy": 2.1596, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5519}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5431}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sketched%20Equivariant%20Imaging%20Regularization%20and%20Deep%20Internal%20Learning%0A%20%20for%20Inverse%20Problems&body=Title%3A%20Sketched%20Equivariant%20Imaging%20Regularization%20and%20Deep%20Internal%20Learning%0A%20%20for%20Inverse%20Problems%0AAuthor%3A%20Guixian%20Xu%20and%20Jinglai%20Li%20and%20Junqi%20Tang%0AAbstract%3A%20%20%20Equivariant%20Imaging%20%28EI%29%20regularization%20has%20become%20the%20de-facto%20technique%20for%0Aunsupervised%20training%20of%20deep%20imaging%20networks%2C%20without%20any%20need%20of%0Aground-truth%20data.%20Observing%20that%20the%20EI-based%20unsupervised%20training%20paradigm%0Acurrently%20has%20significant%20computational%20redundancy%20leading%20to%20inefficiency%20in%0Ahigh-dimensional%20applications%2C%20we%20propose%20a%20sketched%20EI%20regularization%20which%0Aleverages%20the%20randomized%20sketching%20techniques%20for%20acceleration.%20We%20then%20extend%0Aour%20sketched%20EI%20regularization%20to%20develop%20an%20accelerated%20deep%20internal%20learning%0Aframework%20--%20Sketched%20Equivariant%20Deep%20Image%20Prior%20%28Sk.EI-DIP%29%2C%20which%20can%20be%0Aefficiently%20applied%20for%20single-image%20and%20task-adapted%20reconstruction.%20Our%0Anumerical%20study%20on%20X-ray%20CT%20image%20reconstruction%20tasks%20demonstrate%20that%20our%0Aapproach%20can%20achieve%20order-of-magnitude%20computational%20acceleration%20over%0Astandard%20EI-based%20counterpart%20in%20single-input%20setting%2C%20and%20network%20adaptation%0Aat%20test%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSketched%2520Equivariant%2520Imaging%2520Regularization%2520and%2520Deep%2520Internal%2520Learning%250A%2520%2520for%2520Inverse%2520Problems%26entry.906535625%3DGuixian%2520Xu%2520and%2520Jinglai%2520Li%2520and%2520Junqi%2520Tang%26entry.1292438233%3D%2520%2520Equivariant%2520Imaging%2520%2528EI%2529%2520regularization%2520has%2520become%2520the%2520de-facto%2520technique%2520for%250Aunsupervised%2520training%2520of%2520deep%2520imaging%2520networks%252C%2520without%2520any%2520need%2520of%250Aground-truth%2520data.%2520Observing%2520that%2520the%2520EI-based%2520unsupervised%2520training%2520paradigm%250Acurrently%2520has%2520significant%2520computational%2520redundancy%2520leading%2520to%2520inefficiency%2520in%250Ahigh-dimensional%2520applications%252C%2520we%2520propose%2520a%2520sketched%2520EI%2520regularization%2520which%250Aleverages%2520the%2520randomized%2520sketching%2520techniques%2520for%2520acceleration.%2520We%2520then%2520extend%250Aour%2520sketched%2520EI%2520regularization%2520to%2520develop%2520an%2520accelerated%2520deep%2520internal%2520learning%250Aframework%2520--%2520Sketched%2520Equivariant%2520Deep%2520Image%2520Prior%2520%2528Sk.EI-DIP%2529%252C%2520which%2520can%2520be%250Aefficiently%2520applied%2520for%2520single-image%2520and%2520task-adapted%2520reconstruction.%2520Our%250Anumerical%2520study%2520on%2520X-ray%2520CT%2520image%2520reconstruction%2520tasks%2520demonstrate%2520that%2520our%250Aapproach%2520can%2520achieve%2520order-of-magnitude%2520computational%2520acceleration%2520over%250Astandard%2520EI-based%2520counterpart%2520in%2520single-input%2520setting%252C%2520and%2520network%2520adaptation%250Aat%2520test%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sketched%20Equivariant%20Imaging%20Regularization%20and%20Deep%20Internal%20Learning%0A%20%20for%20Inverse%20Problems&entry.906535625=Guixian%20Xu%20and%20Jinglai%20Li%20and%20Junqi%20Tang&entry.1292438233=%20%20Equivariant%20Imaging%20%28EI%29%20regularization%20has%20become%20the%20de-facto%20technique%20for%0Aunsupervised%20training%20of%20deep%20imaging%20networks%2C%20without%20any%20need%20of%0Aground-truth%20data.%20Observing%20that%20the%20EI-based%20unsupervised%20training%20paradigm%0Acurrently%20has%20significant%20computational%20redundancy%20leading%20to%20inefficiency%20in%0Ahigh-dimensional%20applications%2C%20we%20propose%20a%20sketched%20EI%20regularization%20which%0Aleverages%20the%20randomized%20sketching%20techniques%20for%20acceleration.%20We%20then%20extend%0Aour%20sketched%20EI%20regularization%20to%20develop%20an%20accelerated%20deep%20internal%20learning%0Aframework%20--%20Sketched%20Equivariant%20Deep%20Image%20Prior%20%28Sk.EI-DIP%29%2C%20which%20can%20be%0Aefficiently%20applied%20for%20single-image%20and%20task-adapted%20reconstruction.%20Our%0Anumerical%20study%20on%20X-ray%20CT%20image%20reconstruction%20tasks%20demonstrate%20that%20our%0Aapproach%20can%20achieve%20order-of-magnitude%20computational%20acceleration%20over%0Astandard%20EI-based%20counterpart%20in%20single-input%20setting%2C%20and%20network%20adaptation%0Aat%20test%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05771v1&entry.124074799=Read"},
{"title": "EROAS: 3D Efficient Reactive Obstacle Avoidance System for Autonomous\n  Underwater Vehicles using 2.5D Forward-Looking Sonar", "author": "Pruthviraj Mane and Allen Jacob George and Rajini Makam and Rudrashis Majumder and Suresh Sundaram", "abstract": "  Advances in Autonomous Underwater Vehicles (AUVs) have evolved vastly in\nshort period of time. While advancements in sonar and camera technology with\ndeep learning aid the obstacle detection and path planning to a great extent,\nachieving the right balance between computational resources , precision and\nsafety maintained remains a challenge. Finding optimal solutions for real-time\nnavigation in cluttered environments becomes pivotal as systems have to process\nlarge amounts of data efficiently. In this work, we propose a novel obstacle\navoidance method for navigating 3D underwater environments. This approach\nutilizes a standard multibeam forward-looking sonar to detect and map obstacle\nin 3D environment. Instead of using computationally expensive 3D sensors, we\npivot the 2D sonar to get 3D heuristic data effectively transforming the sensor\ninto a 2.5D sonar for real-time 3D navigation decisions. This approach enhances\nobstacle detection and navigation by leveraging the simplicity of 2D sonar with\nthe depth perception typically associated with 3D systems. We have further\nincorporated Control Barrier Function (CBF) as a filter to ensure safety of the\nAUV. The effectiveness of this algorithm was tested on a six degrees of freedom\n(DOF) rover in various simulation scenarios. The results demonstrate that the\nsystem successfully avoids obstacles and navigates toward predefined goals,\nshowcasing its capability to manage complex underwater environments with\nprecision. This paper highlights the potential of 2.5D sonar for improving AUV\nnavigation and offers insights into future enhancements and applications of\nthis technology in underwater autonomous systems.\n\\url{https://github.com/AIRLabIISc/EROAS}\n", "link": "http://arxiv.org/abs/2411.05516v1", "date": "2024-11-08", "relevancy": 2.1524, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5452}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5363}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EROAS%3A%203D%20Efficient%20Reactive%20Obstacle%20Avoidance%20System%20for%20Autonomous%0A%20%20Underwater%20Vehicles%20using%202.5D%20Forward-Looking%20Sonar&body=Title%3A%20EROAS%3A%203D%20Efficient%20Reactive%20Obstacle%20Avoidance%20System%20for%20Autonomous%0A%20%20Underwater%20Vehicles%20using%202.5D%20Forward-Looking%20Sonar%0AAuthor%3A%20Pruthviraj%20Mane%20and%20Allen%20Jacob%20George%20and%20Rajini%20Makam%20and%20Rudrashis%20Majumder%20and%20Suresh%20Sundaram%0AAbstract%3A%20%20%20Advances%20in%20Autonomous%20Underwater%20Vehicles%20%28AUVs%29%20have%20evolved%20vastly%20in%0Ashort%20period%20of%20time.%20While%20advancements%20in%20sonar%20and%20camera%20technology%20with%0Adeep%20learning%20aid%20the%20obstacle%20detection%20and%20path%20planning%20to%20a%20great%20extent%2C%0Aachieving%20the%20right%20balance%20between%20computational%20resources%20%2C%20precision%20and%0Asafety%20maintained%20remains%20a%20challenge.%20Finding%20optimal%20solutions%20for%20real-time%0Anavigation%20in%20cluttered%20environments%20becomes%20pivotal%20as%20systems%20have%20to%20process%0Alarge%20amounts%20of%20data%20efficiently.%20In%20this%20work%2C%20we%20propose%20a%20novel%20obstacle%0Aavoidance%20method%20for%20navigating%203D%20underwater%20environments.%20This%20approach%0Autilizes%20a%20standard%20multibeam%20forward-looking%20sonar%20to%20detect%20and%20map%20obstacle%0Ain%203D%20environment.%20Instead%20of%20using%20computationally%20expensive%203D%20sensors%2C%20we%0Apivot%20the%202D%20sonar%20to%20get%203D%20heuristic%20data%20effectively%20transforming%20the%20sensor%0Ainto%20a%202.5D%20sonar%20for%20real-time%203D%20navigation%20decisions.%20This%20approach%20enhances%0Aobstacle%20detection%20and%20navigation%20by%20leveraging%20the%20simplicity%20of%202D%20sonar%20with%0Athe%20depth%20perception%20typically%20associated%20with%203D%20systems.%20We%20have%20further%0Aincorporated%20Control%20Barrier%20Function%20%28CBF%29%20as%20a%20filter%20to%20ensure%20safety%20of%20the%0AAUV.%20The%20effectiveness%20of%20this%20algorithm%20was%20tested%20on%20a%20six%20degrees%20of%20freedom%0A%28DOF%29%20rover%20in%20various%20simulation%20scenarios.%20The%20results%20demonstrate%20that%20the%0Asystem%20successfully%20avoids%20obstacles%20and%20navigates%20toward%20predefined%20goals%2C%0Ashowcasing%20its%20capability%20to%20manage%20complex%20underwater%20environments%20with%0Aprecision.%20This%20paper%20highlights%20the%20potential%20of%202.5D%20sonar%20for%20improving%20AUV%0Anavigation%20and%20offers%20insights%20into%20future%20enhancements%20and%20applications%20of%0Athis%20technology%20in%20underwater%20autonomous%20systems.%0A%5Curl%7Bhttps%3A//github.com/AIRLabIISc/EROAS%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEROAS%253A%25203D%2520Efficient%2520Reactive%2520Obstacle%2520Avoidance%2520System%2520for%2520Autonomous%250A%2520%2520Underwater%2520Vehicles%2520using%25202.5D%2520Forward-Looking%2520Sonar%26entry.906535625%3DPruthviraj%2520Mane%2520and%2520Allen%2520Jacob%2520George%2520and%2520Rajini%2520Makam%2520and%2520Rudrashis%2520Majumder%2520and%2520Suresh%2520Sundaram%26entry.1292438233%3D%2520%2520Advances%2520in%2520Autonomous%2520Underwater%2520Vehicles%2520%2528AUVs%2529%2520have%2520evolved%2520vastly%2520in%250Ashort%2520period%2520of%2520time.%2520While%2520advancements%2520in%2520sonar%2520and%2520camera%2520technology%2520with%250Adeep%2520learning%2520aid%2520the%2520obstacle%2520detection%2520and%2520path%2520planning%2520to%2520a%2520great%2520extent%252C%250Aachieving%2520the%2520right%2520balance%2520between%2520computational%2520resources%2520%252C%2520precision%2520and%250Asafety%2520maintained%2520remains%2520a%2520challenge.%2520Finding%2520optimal%2520solutions%2520for%2520real-time%250Anavigation%2520in%2520cluttered%2520environments%2520becomes%2520pivotal%2520as%2520systems%2520have%2520to%2520process%250Alarge%2520amounts%2520of%2520data%2520efficiently.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520obstacle%250Aavoidance%2520method%2520for%2520navigating%25203D%2520underwater%2520environments.%2520This%2520approach%250Autilizes%2520a%2520standard%2520multibeam%2520forward-looking%2520sonar%2520to%2520detect%2520and%2520map%2520obstacle%250Ain%25203D%2520environment.%2520Instead%2520of%2520using%2520computationally%2520expensive%25203D%2520sensors%252C%2520we%250Apivot%2520the%25202D%2520sonar%2520to%2520get%25203D%2520heuristic%2520data%2520effectively%2520transforming%2520the%2520sensor%250Ainto%2520a%25202.5D%2520sonar%2520for%2520real-time%25203D%2520navigation%2520decisions.%2520This%2520approach%2520enhances%250Aobstacle%2520detection%2520and%2520navigation%2520by%2520leveraging%2520the%2520simplicity%2520of%25202D%2520sonar%2520with%250Athe%2520depth%2520perception%2520typically%2520associated%2520with%25203D%2520systems.%2520We%2520have%2520further%250Aincorporated%2520Control%2520Barrier%2520Function%2520%2528CBF%2529%2520as%2520a%2520filter%2520to%2520ensure%2520safety%2520of%2520the%250AAUV.%2520The%2520effectiveness%2520of%2520this%2520algorithm%2520was%2520tested%2520on%2520a%2520six%2520degrees%2520of%2520freedom%250A%2528DOF%2529%2520rover%2520in%2520various%2520simulation%2520scenarios.%2520The%2520results%2520demonstrate%2520that%2520the%250Asystem%2520successfully%2520avoids%2520obstacles%2520and%2520navigates%2520toward%2520predefined%2520goals%252C%250Ashowcasing%2520its%2520capability%2520to%2520manage%2520complex%2520underwater%2520environments%2520with%250Aprecision.%2520This%2520paper%2520highlights%2520the%2520potential%2520of%25202.5D%2520sonar%2520for%2520improving%2520AUV%250Anavigation%2520and%2520offers%2520insights%2520into%2520future%2520enhancements%2520and%2520applications%2520of%250Athis%2520technology%2520in%2520underwater%2520autonomous%2520systems.%250A%255Curl%257Bhttps%253A//github.com/AIRLabIISc/EROAS%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EROAS%3A%203D%20Efficient%20Reactive%20Obstacle%20Avoidance%20System%20for%20Autonomous%0A%20%20Underwater%20Vehicles%20using%202.5D%20Forward-Looking%20Sonar&entry.906535625=Pruthviraj%20Mane%20and%20Allen%20Jacob%20George%20and%20Rajini%20Makam%20and%20Rudrashis%20Majumder%20and%20Suresh%20Sundaram&entry.1292438233=%20%20Advances%20in%20Autonomous%20Underwater%20Vehicles%20%28AUVs%29%20have%20evolved%20vastly%20in%0Ashort%20period%20of%20time.%20While%20advancements%20in%20sonar%20and%20camera%20technology%20with%0Adeep%20learning%20aid%20the%20obstacle%20detection%20and%20path%20planning%20to%20a%20great%20extent%2C%0Aachieving%20the%20right%20balance%20between%20computational%20resources%20%2C%20precision%20and%0Asafety%20maintained%20remains%20a%20challenge.%20Finding%20optimal%20solutions%20for%20real-time%0Anavigation%20in%20cluttered%20environments%20becomes%20pivotal%20as%20systems%20have%20to%20process%0Alarge%20amounts%20of%20data%20efficiently.%20In%20this%20work%2C%20we%20propose%20a%20novel%20obstacle%0Aavoidance%20method%20for%20navigating%203D%20underwater%20environments.%20This%20approach%0Autilizes%20a%20standard%20multibeam%20forward-looking%20sonar%20to%20detect%20and%20map%20obstacle%0Ain%203D%20environment.%20Instead%20of%20using%20computationally%20expensive%203D%20sensors%2C%20we%0Apivot%20the%202D%20sonar%20to%20get%203D%20heuristic%20data%20effectively%20transforming%20the%20sensor%0Ainto%20a%202.5D%20sonar%20for%20real-time%203D%20navigation%20decisions.%20This%20approach%20enhances%0Aobstacle%20detection%20and%20navigation%20by%20leveraging%20the%20simplicity%20of%202D%20sonar%20with%0Athe%20depth%20perception%20typically%20associated%20with%203D%20systems.%20We%20have%20further%0Aincorporated%20Control%20Barrier%20Function%20%28CBF%29%20as%20a%20filter%20to%20ensure%20safety%20of%20the%0AAUV.%20The%20effectiveness%20of%20this%20algorithm%20was%20tested%20on%20a%20six%20degrees%20of%20freedom%0A%28DOF%29%20rover%20in%20various%20simulation%20scenarios.%20The%20results%20demonstrate%20that%20the%0Asystem%20successfully%20avoids%20obstacles%20and%20navigates%20toward%20predefined%20goals%2C%0Ashowcasing%20its%20capability%20to%20manage%20complex%20underwater%20environments%20with%0Aprecision.%20This%20paper%20highlights%20the%20potential%20of%202.5D%20sonar%20for%20improving%20AUV%0Anavigation%20and%20offers%20insights%20into%20future%20enhancements%20and%20applications%20of%0Athis%20technology%20in%20underwater%20autonomous%20systems.%0A%5Curl%7Bhttps%3A//github.com/AIRLabIISc/EROAS%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05516v1&entry.124074799=Read"},
{"title": "StoX-Net: Stochastic Processing of Partial Sums for Efficient In-Memory\n  Computing DNN Accelerators", "author": "Ethan G Rogers and Sohan Salahuddin Mugdho and Kshemal Kshemendra Gupte and Cheng Wang", "abstract": "  Crossbar-based in-memory computing (IMC) has emerged as a promising platform\nfor hardware acceleration of deep neural networks (DNNs). However, the energy\nand latency of IMC systems are dominated by the large overhead of the\nperipheral analog-to-digital converters (ADCs). To address such ADC bottleneck,\nhere we propose to implement stochastic processing of array-level partial sums\n(PS) for efficient IMC. Leveraging the probabilistic switching of spin-orbit\ntorque magnetic tunnel junctions, the proposed PS processing eliminates the\ncostly ADC, achieving significant improvement in energy and area efficiency. To\nmitigate accuracy loss, we develop PS-quantization-aware training that enables\nbackward propagation across stochastic PS. Furthermore, a novel scheme with an\ninhomogeneous sampling length of the stochastic conversion is proposed. When\nrunning ResNet20 on the CIFAR-10 dataset, our architecture-to-algorithm\nco-design demonstrates up to 16x, 8x, and 10x improvement in energy, latency,\nand area, respectively, compared to IMC with standard ADC. Our optimized design\nconfiguration using stochastic PS achieved 130x (24x) improvement in\nEnergy-Delay-Product compared to IMC with full precision ADC (sparse low-bit\nADC), while maintaining near-software accuracy at various benchmark\nclassification tasks.\n", "link": "http://arxiv.org/abs/2407.12378v2", "date": "2024-11-08", "relevancy": 2.1403, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5958}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.537}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StoX-Net%3A%20Stochastic%20Processing%20of%20Partial%20Sums%20for%20Efficient%20In-Memory%0A%20%20Computing%20DNN%20Accelerators&body=Title%3A%20StoX-Net%3A%20Stochastic%20Processing%20of%20Partial%20Sums%20for%20Efficient%20In-Memory%0A%20%20Computing%20DNN%20Accelerators%0AAuthor%3A%20Ethan%20G%20Rogers%20and%20Sohan%20Salahuddin%20Mugdho%20and%20Kshemal%20Kshemendra%20Gupte%20and%20Cheng%20Wang%0AAbstract%3A%20%20%20Crossbar-based%20in-memory%20computing%20%28IMC%29%20has%20emerged%20as%20a%20promising%20platform%0Afor%20hardware%20acceleration%20of%20deep%20neural%20networks%20%28DNNs%29.%20However%2C%20the%20energy%0Aand%20latency%20of%20IMC%20systems%20are%20dominated%20by%20the%20large%20overhead%20of%20the%0Aperipheral%20analog-to-digital%20converters%20%28ADCs%29.%20To%20address%20such%20ADC%20bottleneck%2C%0Ahere%20we%20propose%20to%20implement%20stochastic%20processing%20of%20array-level%20partial%20sums%0A%28PS%29%20for%20efficient%20IMC.%20Leveraging%20the%20probabilistic%20switching%20of%20spin-orbit%0Atorque%20magnetic%20tunnel%20junctions%2C%20the%20proposed%20PS%20processing%20eliminates%20the%0Acostly%20ADC%2C%20achieving%20significant%20improvement%20in%20energy%20and%20area%20efficiency.%20To%0Amitigate%20accuracy%20loss%2C%20we%20develop%20PS-quantization-aware%20training%20that%20enables%0Abackward%20propagation%20across%20stochastic%20PS.%20Furthermore%2C%20a%20novel%20scheme%20with%20an%0Ainhomogeneous%20sampling%20length%20of%20the%20stochastic%20conversion%20is%20proposed.%20When%0Arunning%20ResNet20%20on%20the%20CIFAR-10%20dataset%2C%20our%20architecture-to-algorithm%0Aco-design%20demonstrates%20up%20to%2016x%2C%208x%2C%20and%2010x%20improvement%20in%20energy%2C%20latency%2C%0Aand%20area%2C%20respectively%2C%20compared%20to%20IMC%20with%20standard%20ADC.%20Our%20optimized%20design%0Aconfiguration%20using%20stochastic%20PS%20achieved%20130x%20%2824x%29%20improvement%20in%0AEnergy-Delay-Product%20compared%20to%20IMC%20with%20full%20precision%20ADC%20%28sparse%20low-bit%0AADC%29%2C%20while%20maintaining%20near-software%20accuracy%20at%20various%20benchmark%0Aclassification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12378v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStoX-Net%253A%2520Stochastic%2520Processing%2520of%2520Partial%2520Sums%2520for%2520Efficient%2520In-Memory%250A%2520%2520Computing%2520DNN%2520Accelerators%26entry.906535625%3DEthan%2520G%2520Rogers%2520and%2520Sohan%2520Salahuddin%2520Mugdho%2520and%2520Kshemal%2520Kshemendra%2520Gupte%2520and%2520Cheng%2520Wang%26entry.1292438233%3D%2520%2520Crossbar-based%2520in-memory%2520computing%2520%2528IMC%2529%2520has%2520emerged%2520as%2520a%2520promising%2520platform%250Afor%2520hardware%2520acceleration%2520of%2520deep%2520neural%2520networks%2520%2528DNNs%2529.%2520However%252C%2520the%2520energy%250Aand%2520latency%2520of%2520IMC%2520systems%2520are%2520dominated%2520by%2520the%2520large%2520overhead%2520of%2520the%250Aperipheral%2520analog-to-digital%2520converters%2520%2528ADCs%2529.%2520To%2520address%2520such%2520ADC%2520bottleneck%252C%250Ahere%2520we%2520propose%2520to%2520implement%2520stochastic%2520processing%2520of%2520array-level%2520partial%2520sums%250A%2528PS%2529%2520for%2520efficient%2520IMC.%2520Leveraging%2520the%2520probabilistic%2520switching%2520of%2520spin-orbit%250Atorque%2520magnetic%2520tunnel%2520junctions%252C%2520the%2520proposed%2520PS%2520processing%2520eliminates%2520the%250Acostly%2520ADC%252C%2520achieving%2520significant%2520improvement%2520in%2520energy%2520and%2520area%2520efficiency.%2520To%250Amitigate%2520accuracy%2520loss%252C%2520we%2520develop%2520PS-quantization-aware%2520training%2520that%2520enables%250Abackward%2520propagation%2520across%2520stochastic%2520PS.%2520Furthermore%252C%2520a%2520novel%2520scheme%2520with%2520an%250Ainhomogeneous%2520sampling%2520length%2520of%2520the%2520stochastic%2520conversion%2520is%2520proposed.%2520When%250Arunning%2520ResNet20%2520on%2520the%2520CIFAR-10%2520dataset%252C%2520our%2520architecture-to-algorithm%250Aco-design%2520demonstrates%2520up%2520to%252016x%252C%25208x%252C%2520and%252010x%2520improvement%2520in%2520energy%252C%2520latency%252C%250Aand%2520area%252C%2520respectively%252C%2520compared%2520to%2520IMC%2520with%2520standard%2520ADC.%2520Our%2520optimized%2520design%250Aconfiguration%2520using%2520stochastic%2520PS%2520achieved%2520130x%2520%252824x%2529%2520improvement%2520in%250AEnergy-Delay-Product%2520compared%2520to%2520IMC%2520with%2520full%2520precision%2520ADC%2520%2528sparse%2520low-bit%250AADC%2529%252C%2520while%2520maintaining%2520near-software%2520accuracy%2520at%2520various%2520benchmark%250Aclassification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12378v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StoX-Net%3A%20Stochastic%20Processing%20of%20Partial%20Sums%20for%20Efficient%20In-Memory%0A%20%20Computing%20DNN%20Accelerators&entry.906535625=Ethan%20G%20Rogers%20and%20Sohan%20Salahuddin%20Mugdho%20and%20Kshemal%20Kshemendra%20Gupte%20and%20Cheng%20Wang&entry.1292438233=%20%20Crossbar-based%20in-memory%20computing%20%28IMC%29%20has%20emerged%20as%20a%20promising%20platform%0Afor%20hardware%20acceleration%20of%20deep%20neural%20networks%20%28DNNs%29.%20However%2C%20the%20energy%0Aand%20latency%20of%20IMC%20systems%20are%20dominated%20by%20the%20large%20overhead%20of%20the%0Aperipheral%20analog-to-digital%20converters%20%28ADCs%29.%20To%20address%20such%20ADC%20bottleneck%2C%0Ahere%20we%20propose%20to%20implement%20stochastic%20processing%20of%20array-level%20partial%20sums%0A%28PS%29%20for%20efficient%20IMC.%20Leveraging%20the%20probabilistic%20switching%20of%20spin-orbit%0Atorque%20magnetic%20tunnel%20junctions%2C%20the%20proposed%20PS%20processing%20eliminates%20the%0Acostly%20ADC%2C%20achieving%20significant%20improvement%20in%20energy%20and%20area%20efficiency.%20To%0Amitigate%20accuracy%20loss%2C%20we%20develop%20PS-quantization-aware%20training%20that%20enables%0Abackward%20propagation%20across%20stochastic%20PS.%20Furthermore%2C%20a%20novel%20scheme%20with%20an%0Ainhomogeneous%20sampling%20length%20of%20the%20stochastic%20conversion%20is%20proposed.%20When%0Arunning%20ResNet20%20on%20the%20CIFAR-10%20dataset%2C%20our%20architecture-to-algorithm%0Aco-design%20demonstrates%20up%20to%2016x%2C%208x%2C%20and%2010x%20improvement%20in%20energy%2C%20latency%2C%0Aand%20area%2C%20respectively%2C%20compared%20to%20IMC%20with%20standard%20ADC.%20Our%20optimized%20design%0Aconfiguration%20using%20stochastic%20PS%20achieved%20130x%20%2824x%29%20improvement%20in%0AEnergy-Delay-Product%20compared%20to%20IMC%20with%20full%20precision%20ADC%20%28sparse%20low-bit%0AADC%29%2C%20while%20maintaining%20near-software%20accuracy%20at%20various%20benchmark%0Aclassification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12378v2&entry.124074799=Read"},
{"title": "Relative Pose Estimation for Nonholonomic Robot Formation with UWB-IO\n  Measurements", "author": "Kunrui Ze and Wei Wang and Shuoyu Yue and Guibin Sun and Kexin Liu and Jinhu L\u00fc", "abstract": "  This article studies the problem of distributed formation control for\nmultiple robots by using onboard ultra wide band (UWB) ranging and inertial\nodometer (IO) measurements. Although this problem has been widely studied, a\nfundamental limitation of most works is that they require each robot's pose and\nsensor measurements are expressed in a common reference frame. However, it is\ninapplicable for nonholonomic robot formations due to the practical difficulty\nof aligning IO measurements of individual robot in a common frame. To address\nthis problem, firstly, a concurrent-learning based estimator is firstly\nproposed to achieve relative localization between neighboring robots in a local\nframe. Different from most relative localization methods in a global frame,\nboth relative position and orientation in a local frame are estimated with only\nUWB ranging and IO measurements. Secondly, to deal with information loss caused\nby directed communication topology, a cooperative localization algorithm is\nintroduced to estimate the relative pose to the leader robot. Thirdly, based on\nthe theoretical results on relative pose estimation, a distributed formation\ntracking controller is proposed for nonholonomic robots. Both gazebo physical\nsimulation and real-world experiments conducted on networked TurtleBot3\nnonholonomic robots are provided to demonstrate the effectiveness of the\nproposed method.\n", "link": "http://arxiv.org/abs/2411.05481v1", "date": "2024-11-08", "relevancy": 2.136, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5497}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5413}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relative%20Pose%20Estimation%20for%20Nonholonomic%20Robot%20Formation%20with%20UWB-IO%0A%20%20Measurements&body=Title%3A%20Relative%20Pose%20Estimation%20for%20Nonholonomic%20Robot%20Formation%20with%20UWB-IO%0A%20%20Measurements%0AAuthor%3A%20Kunrui%20Ze%20and%20Wei%20Wang%20and%20Shuoyu%20Yue%20and%20Guibin%20Sun%20and%20Kexin%20Liu%20and%20Jinhu%20L%C3%BC%0AAbstract%3A%20%20%20This%20article%20studies%20the%20problem%20of%20distributed%20formation%20control%20for%0Amultiple%20robots%20by%20using%20onboard%20ultra%20wide%20band%20%28UWB%29%20ranging%20and%20inertial%0Aodometer%20%28IO%29%20measurements.%20Although%20this%20problem%20has%20been%20widely%20studied%2C%20a%0Afundamental%20limitation%20of%20most%20works%20is%20that%20they%20require%20each%20robot%27s%20pose%20and%0Asensor%20measurements%20are%20expressed%20in%20a%20common%20reference%20frame.%20However%2C%20it%20is%0Ainapplicable%20for%20nonholonomic%20robot%20formations%20due%20to%20the%20practical%20difficulty%0Aof%20aligning%20IO%20measurements%20of%20individual%20robot%20in%20a%20common%20frame.%20To%20address%0Athis%20problem%2C%20firstly%2C%20a%20concurrent-learning%20based%20estimator%20is%20firstly%0Aproposed%20to%20achieve%20relative%20localization%20between%20neighboring%20robots%20in%20a%20local%0Aframe.%20Different%20from%20most%20relative%20localization%20methods%20in%20a%20global%20frame%2C%0Aboth%20relative%20position%20and%20orientation%20in%20a%20local%20frame%20are%20estimated%20with%20only%0AUWB%20ranging%20and%20IO%20measurements.%20Secondly%2C%20to%20deal%20with%20information%20loss%20caused%0Aby%20directed%20communication%20topology%2C%20a%20cooperative%20localization%20algorithm%20is%0Aintroduced%20to%20estimate%20the%20relative%20pose%20to%20the%20leader%20robot.%20Thirdly%2C%20based%20on%0Athe%20theoretical%20results%20on%20relative%20pose%20estimation%2C%20a%20distributed%20formation%0Atracking%20controller%20is%20proposed%20for%20nonholonomic%20robots.%20Both%20gazebo%20physical%0Asimulation%20and%20real-world%20experiments%20conducted%20on%20networked%20TurtleBot3%0Anonholonomic%20robots%20are%20provided%20to%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelative%2520Pose%2520Estimation%2520for%2520Nonholonomic%2520Robot%2520Formation%2520with%2520UWB-IO%250A%2520%2520Measurements%26entry.906535625%3DKunrui%2520Ze%2520and%2520Wei%2520Wang%2520and%2520Shuoyu%2520Yue%2520and%2520Guibin%2520Sun%2520and%2520Kexin%2520Liu%2520and%2520Jinhu%2520L%25C3%25BC%26entry.1292438233%3D%2520%2520This%2520article%2520studies%2520the%2520problem%2520of%2520distributed%2520formation%2520control%2520for%250Amultiple%2520robots%2520by%2520using%2520onboard%2520ultra%2520wide%2520band%2520%2528UWB%2529%2520ranging%2520and%2520inertial%250Aodometer%2520%2528IO%2529%2520measurements.%2520Although%2520this%2520problem%2520has%2520been%2520widely%2520studied%252C%2520a%250Afundamental%2520limitation%2520of%2520most%2520works%2520is%2520that%2520they%2520require%2520each%2520robot%2527s%2520pose%2520and%250Asensor%2520measurements%2520are%2520expressed%2520in%2520a%2520common%2520reference%2520frame.%2520However%252C%2520it%2520is%250Ainapplicable%2520for%2520nonholonomic%2520robot%2520formations%2520due%2520to%2520the%2520practical%2520difficulty%250Aof%2520aligning%2520IO%2520measurements%2520of%2520individual%2520robot%2520in%2520a%2520common%2520frame.%2520To%2520address%250Athis%2520problem%252C%2520firstly%252C%2520a%2520concurrent-learning%2520based%2520estimator%2520is%2520firstly%250Aproposed%2520to%2520achieve%2520relative%2520localization%2520between%2520neighboring%2520robots%2520in%2520a%2520local%250Aframe.%2520Different%2520from%2520most%2520relative%2520localization%2520methods%2520in%2520a%2520global%2520frame%252C%250Aboth%2520relative%2520position%2520and%2520orientation%2520in%2520a%2520local%2520frame%2520are%2520estimated%2520with%2520only%250AUWB%2520ranging%2520and%2520IO%2520measurements.%2520Secondly%252C%2520to%2520deal%2520with%2520information%2520loss%2520caused%250Aby%2520directed%2520communication%2520topology%252C%2520a%2520cooperative%2520localization%2520algorithm%2520is%250Aintroduced%2520to%2520estimate%2520the%2520relative%2520pose%2520to%2520the%2520leader%2520robot.%2520Thirdly%252C%2520based%2520on%250Athe%2520theoretical%2520results%2520on%2520relative%2520pose%2520estimation%252C%2520a%2520distributed%2520formation%250Atracking%2520controller%2520is%2520proposed%2520for%2520nonholonomic%2520robots.%2520Both%2520gazebo%2520physical%250Asimulation%2520and%2520real-world%2520experiments%2520conducted%2520on%2520networked%2520TurtleBot3%250Anonholonomic%2520robots%2520are%2520provided%2520to%2520demonstrate%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relative%20Pose%20Estimation%20for%20Nonholonomic%20Robot%20Formation%20with%20UWB-IO%0A%20%20Measurements&entry.906535625=Kunrui%20Ze%20and%20Wei%20Wang%20and%20Shuoyu%20Yue%20and%20Guibin%20Sun%20and%20Kexin%20Liu%20and%20Jinhu%20L%C3%BC&entry.1292438233=%20%20This%20article%20studies%20the%20problem%20of%20distributed%20formation%20control%20for%0Amultiple%20robots%20by%20using%20onboard%20ultra%20wide%20band%20%28UWB%29%20ranging%20and%20inertial%0Aodometer%20%28IO%29%20measurements.%20Although%20this%20problem%20has%20been%20widely%20studied%2C%20a%0Afundamental%20limitation%20of%20most%20works%20is%20that%20they%20require%20each%20robot%27s%20pose%20and%0Asensor%20measurements%20are%20expressed%20in%20a%20common%20reference%20frame.%20However%2C%20it%20is%0Ainapplicable%20for%20nonholonomic%20robot%20formations%20due%20to%20the%20practical%20difficulty%0Aof%20aligning%20IO%20measurements%20of%20individual%20robot%20in%20a%20common%20frame.%20To%20address%0Athis%20problem%2C%20firstly%2C%20a%20concurrent-learning%20based%20estimator%20is%20firstly%0Aproposed%20to%20achieve%20relative%20localization%20between%20neighboring%20robots%20in%20a%20local%0Aframe.%20Different%20from%20most%20relative%20localization%20methods%20in%20a%20global%20frame%2C%0Aboth%20relative%20position%20and%20orientation%20in%20a%20local%20frame%20are%20estimated%20with%20only%0AUWB%20ranging%20and%20IO%20measurements.%20Secondly%2C%20to%20deal%20with%20information%20loss%20caused%0Aby%20directed%20communication%20topology%2C%20a%20cooperative%20localization%20algorithm%20is%0Aintroduced%20to%20estimate%20the%20relative%20pose%20to%20the%20leader%20robot.%20Thirdly%2C%20based%20on%0Athe%20theoretical%20results%20on%20relative%20pose%20estimation%2C%20a%20distributed%20formation%0Atracking%20controller%20is%20proposed%20for%20nonholonomic%20robots.%20Both%20gazebo%20physical%0Asimulation%20and%20real-world%20experiments%20conducted%20on%20networked%20TurtleBot3%0Anonholonomic%20robots%20are%20provided%20to%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05481v1&entry.124074799=Read"},
{"title": "Image inpainting enhancement by replacing the original mask with a\n  self-attended region from the input image", "author": "Kourosh Kiani and Razieh Rastgoo and Alireza Chaji and Sergio Escalera", "abstract": "  Image inpainting, the process of restoring missing or corrupted regions of an\nimage by reconstructing pixel information, has recently seen considerable\nadvancements through deep learning-based approaches. In this paper, we\nintroduce a novel deep learning-based pre-processing methodology for image\ninpainting utilizing the Vision Transformer (ViT). Our approach involves\nreplacing masked pixel values with those generated by the ViT, leveraging\ndiverse visual patches within the attention matrix to capture discriminative\nspatial features. To the best of our knowledge, this is the first instance of\nsuch a pre-processing model being proposed for image inpainting tasks.\nFurthermore, we show that our methodology can be effectively applied using the\npre-trained ViT model with pre-defined patch size. To evaluate the\ngeneralization capability of the proposed methodology, we provide experimental\nresults comparing our approach with four standard models across four public\ndatasets, demonstrating the efficacy of our pre-processing technique in\nenhancing inpainting performance.\n", "link": "http://arxiv.org/abs/2411.05705v1", "date": "2024-11-08", "relevancy": 2.1285, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5547}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5223}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20inpainting%20enhancement%20by%20replacing%20the%20original%20mask%20with%20a%0A%20%20self-attended%20region%20from%20the%20input%20image&body=Title%3A%20Image%20inpainting%20enhancement%20by%20replacing%20the%20original%20mask%20with%20a%0A%20%20self-attended%20region%20from%20the%20input%20image%0AAuthor%3A%20Kourosh%20Kiani%20and%20Razieh%20Rastgoo%20and%20Alireza%20Chaji%20and%20Sergio%20Escalera%0AAbstract%3A%20%20%20Image%20inpainting%2C%20the%20process%20of%20restoring%20missing%20or%20corrupted%20regions%20of%20an%0Aimage%20by%20reconstructing%20pixel%20information%2C%20has%20recently%20seen%20considerable%0Aadvancements%20through%20deep%20learning-based%20approaches.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20deep%20learning-based%20pre-processing%20methodology%20for%20image%0Ainpainting%20utilizing%20the%20Vision%20Transformer%20%28ViT%29.%20Our%20approach%20involves%0Areplacing%20masked%20pixel%20values%20with%20those%20generated%20by%20the%20ViT%2C%20leveraging%0Adiverse%20visual%20patches%20within%20the%20attention%20matrix%20to%20capture%20discriminative%0Aspatial%20features.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20instance%20of%0Asuch%20a%20pre-processing%20model%20being%20proposed%20for%20image%20inpainting%20tasks.%0AFurthermore%2C%20we%20show%20that%20our%20methodology%20can%20be%20effectively%20applied%20using%20the%0Apre-trained%20ViT%20model%20with%20pre-defined%20patch%20size.%20To%20evaluate%20the%0Ageneralization%20capability%20of%20the%20proposed%20methodology%2C%20we%20provide%20experimental%0Aresults%20comparing%20our%20approach%20with%20four%20standard%20models%20across%20four%20public%0Adatasets%2C%20demonstrating%20the%20efficacy%20of%20our%20pre-processing%20technique%20in%0Aenhancing%20inpainting%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05705v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520inpainting%2520enhancement%2520by%2520replacing%2520the%2520original%2520mask%2520with%2520a%250A%2520%2520self-attended%2520region%2520from%2520the%2520input%2520image%26entry.906535625%3DKourosh%2520Kiani%2520and%2520Razieh%2520Rastgoo%2520and%2520Alireza%2520Chaji%2520and%2520Sergio%2520Escalera%26entry.1292438233%3D%2520%2520Image%2520inpainting%252C%2520the%2520process%2520of%2520restoring%2520missing%2520or%2520corrupted%2520regions%2520of%2520an%250Aimage%2520by%2520reconstructing%2520pixel%2520information%252C%2520has%2520recently%2520seen%2520considerable%250Aadvancements%2520through%2520deep%2520learning-based%2520approaches.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520novel%2520deep%2520learning-based%2520pre-processing%2520methodology%2520for%2520image%250Ainpainting%2520utilizing%2520the%2520Vision%2520Transformer%2520%2528ViT%2529.%2520Our%2520approach%2520involves%250Areplacing%2520masked%2520pixel%2520values%2520with%2520those%2520generated%2520by%2520the%2520ViT%252C%2520leveraging%250Adiverse%2520visual%2520patches%2520within%2520the%2520attention%2520matrix%2520to%2520capture%2520discriminative%250Aspatial%2520features.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520instance%2520of%250Asuch%2520a%2520pre-processing%2520model%2520being%2520proposed%2520for%2520image%2520inpainting%2520tasks.%250AFurthermore%252C%2520we%2520show%2520that%2520our%2520methodology%2520can%2520be%2520effectively%2520applied%2520using%2520the%250Apre-trained%2520ViT%2520model%2520with%2520pre-defined%2520patch%2520size.%2520To%2520evaluate%2520the%250Ageneralization%2520capability%2520of%2520the%2520proposed%2520methodology%252C%2520we%2520provide%2520experimental%250Aresults%2520comparing%2520our%2520approach%2520with%2520four%2520standard%2520models%2520across%2520four%2520public%250Adatasets%252C%2520demonstrating%2520the%2520efficacy%2520of%2520our%2520pre-processing%2520technique%2520in%250Aenhancing%2520inpainting%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05705v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20inpainting%20enhancement%20by%20replacing%20the%20original%20mask%20with%20a%0A%20%20self-attended%20region%20from%20the%20input%20image&entry.906535625=Kourosh%20Kiani%20and%20Razieh%20Rastgoo%20and%20Alireza%20Chaji%20and%20Sergio%20Escalera&entry.1292438233=%20%20Image%20inpainting%2C%20the%20process%20of%20restoring%20missing%20or%20corrupted%20regions%20of%20an%0Aimage%20by%20reconstructing%20pixel%20information%2C%20has%20recently%20seen%20considerable%0Aadvancements%20through%20deep%20learning-based%20approaches.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20deep%20learning-based%20pre-processing%20methodology%20for%20image%0Ainpainting%20utilizing%20the%20Vision%20Transformer%20%28ViT%29.%20Our%20approach%20involves%0Areplacing%20masked%20pixel%20values%20with%20those%20generated%20by%20the%20ViT%2C%20leveraging%0Adiverse%20visual%20patches%20within%20the%20attention%20matrix%20to%20capture%20discriminative%0Aspatial%20features.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20instance%20of%0Asuch%20a%20pre-processing%20model%20being%20proposed%20for%20image%20inpainting%20tasks.%0AFurthermore%2C%20we%20show%20that%20our%20methodology%20can%20be%20effectively%20applied%20using%20the%0Apre-trained%20ViT%20model%20with%20pre-defined%20patch%20size.%20To%20evaluate%20the%0Ageneralization%20capability%20of%20the%20proposed%20methodology%2C%20we%20provide%20experimental%0Aresults%20comparing%20our%20approach%20with%20four%20standard%20models%20across%20four%20public%0Adatasets%2C%20demonstrating%20the%20efficacy%20of%20our%20pre-processing%20technique%20in%0Aenhancing%20inpainting%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05705v1&entry.124074799=Read"},
{"title": "VM-UNet: Vision Mamba UNet for Medical Image Segmentation", "author": "Jiacheng Ruan and Jincheng Li and Suncheng Xiang", "abstract": "  In the realm of medical image segmentation, both CNN-based and\nTransformer-based models have been extensively explored. However, CNNs exhibit\nlimitations in long-range modeling capabilities, whereas Transformers are\nhampered by their quadratic computational complexity. Recently, State Space\nModels (SSMs), exemplified by Mamba, have emerged as a promising approach. They\nnot only excel in modeling long-range interactions but also maintain a linear\ncomputational complexity. In this paper, leveraging state space models, we\npropose a U-shape architecture model for medical image segmentation, named\nVision Mamba UNet (VM-UNet). Specifically, the Visual State Space (VSS) block\nis introduced as the foundation block to capture extensive contextual\ninformation, and an asymmetrical encoder-decoder structure is constructed with\nfewer convolution layers to save calculation cost. We conduct comprehensive\nexperiments on the ISIC17, ISIC18, and Synapse datasets, and the results\nindicate that VM-UNet performs competitively in medical image segmentation\ntasks. To our best knowledge, this is the first medical image segmentation\nmodel constructed based on the pure SSM-based model. We aim to establish a\nbaseline and provide valuable insights for the future development of more\nefficient and effective SSM-based segmentation systems. Our code is available\nat https://github.com/JCruan519/VM-UNet.\n", "link": "http://arxiv.org/abs/2402.02491v2", "date": "2024-11-08", "relevancy": 2.1166, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5481}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5353}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VM-UNet%3A%20Vision%20Mamba%20UNet%20for%20Medical%20Image%20Segmentation&body=Title%3A%20VM-UNet%3A%20Vision%20Mamba%20UNet%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Jiacheng%20Ruan%20and%20Jincheng%20Li%20and%20Suncheng%20Xiang%0AAbstract%3A%20%20%20In%20the%20realm%20of%20medical%20image%20segmentation%2C%20both%20CNN-based%20and%0ATransformer-based%20models%20have%20been%20extensively%20explored.%20However%2C%20CNNs%20exhibit%0Alimitations%20in%20long-range%20modeling%20capabilities%2C%20whereas%20Transformers%20are%0Ahampered%20by%20their%20quadratic%20computational%20complexity.%20Recently%2C%20State%20Space%0AModels%20%28SSMs%29%2C%20exemplified%20by%20Mamba%2C%20have%20emerged%20as%20a%20promising%20approach.%20They%0Anot%20only%20excel%20in%20modeling%20long-range%20interactions%20but%20also%20maintain%20a%20linear%0Acomputational%20complexity.%20In%20this%20paper%2C%20leveraging%20state%20space%20models%2C%20we%0Apropose%20a%20U-shape%20architecture%20model%20for%20medical%20image%20segmentation%2C%20named%0AVision%20Mamba%20UNet%20%28VM-UNet%29.%20Specifically%2C%20the%20Visual%20State%20Space%20%28VSS%29%20block%0Ais%20introduced%20as%20the%20foundation%20block%20to%20capture%20extensive%20contextual%0Ainformation%2C%20and%20an%20asymmetrical%20encoder-decoder%20structure%20is%20constructed%20with%0Afewer%20convolution%20layers%20to%20save%20calculation%20cost.%20We%20conduct%20comprehensive%0Aexperiments%20on%20the%20ISIC17%2C%20ISIC18%2C%20and%20Synapse%20datasets%2C%20and%20the%20results%0Aindicate%20that%20VM-UNet%20performs%20competitively%20in%20medical%20image%20segmentation%0Atasks.%20To%20our%20best%20knowledge%2C%20this%20is%20the%20first%20medical%20image%20segmentation%0Amodel%20constructed%20based%20on%20the%20pure%20SSM-based%20model.%20We%20aim%20to%20establish%20a%0Abaseline%20and%20provide%20valuable%20insights%20for%20the%20future%20development%20of%20more%0Aefficient%20and%20effective%20SSM-based%20segmentation%20systems.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/JCruan519/VM-UNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02491v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVM-UNet%253A%2520Vision%2520Mamba%2520UNet%2520for%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DJiacheng%2520Ruan%2520and%2520Jincheng%2520Li%2520and%2520Suncheng%2520Xiang%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520medical%2520image%2520segmentation%252C%2520both%2520CNN-based%2520and%250ATransformer-based%2520models%2520have%2520been%2520extensively%2520explored.%2520However%252C%2520CNNs%2520exhibit%250Alimitations%2520in%2520long-range%2520modeling%2520capabilities%252C%2520whereas%2520Transformers%2520are%250Ahampered%2520by%2520their%2520quadratic%2520computational%2520complexity.%2520Recently%252C%2520State%2520Space%250AModels%2520%2528SSMs%2529%252C%2520exemplified%2520by%2520Mamba%252C%2520have%2520emerged%2520as%2520a%2520promising%2520approach.%2520They%250Anot%2520only%2520excel%2520in%2520modeling%2520long-range%2520interactions%2520but%2520also%2520maintain%2520a%2520linear%250Acomputational%2520complexity.%2520In%2520this%2520paper%252C%2520leveraging%2520state%2520space%2520models%252C%2520we%250Apropose%2520a%2520U-shape%2520architecture%2520model%2520for%2520medical%2520image%2520segmentation%252C%2520named%250AVision%2520Mamba%2520UNet%2520%2528VM-UNet%2529.%2520Specifically%252C%2520the%2520Visual%2520State%2520Space%2520%2528VSS%2529%2520block%250Ais%2520introduced%2520as%2520the%2520foundation%2520block%2520to%2520capture%2520extensive%2520contextual%250Ainformation%252C%2520and%2520an%2520asymmetrical%2520encoder-decoder%2520structure%2520is%2520constructed%2520with%250Afewer%2520convolution%2520layers%2520to%2520save%2520calculation%2520cost.%2520We%2520conduct%2520comprehensive%250Aexperiments%2520on%2520the%2520ISIC17%252C%2520ISIC18%252C%2520and%2520Synapse%2520datasets%252C%2520and%2520the%2520results%250Aindicate%2520that%2520VM-UNet%2520performs%2520competitively%2520in%2520medical%2520image%2520segmentation%250Atasks.%2520To%2520our%2520best%2520knowledge%252C%2520this%2520is%2520the%2520first%2520medical%2520image%2520segmentation%250Amodel%2520constructed%2520based%2520on%2520the%2520pure%2520SSM-based%2520model.%2520We%2520aim%2520to%2520establish%2520a%250Abaseline%2520and%2520provide%2520valuable%2520insights%2520for%2520the%2520future%2520development%2520of%2520more%250Aefficient%2520and%2520effective%2520SSM-based%2520segmentation%2520systems.%2520Our%2520code%2520is%2520available%250Aat%2520https%253A//github.com/JCruan519/VM-UNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02491v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VM-UNet%3A%20Vision%20Mamba%20UNet%20for%20Medical%20Image%20Segmentation&entry.906535625=Jiacheng%20Ruan%20and%20Jincheng%20Li%20and%20Suncheng%20Xiang&entry.1292438233=%20%20In%20the%20realm%20of%20medical%20image%20segmentation%2C%20both%20CNN-based%20and%0ATransformer-based%20models%20have%20been%20extensively%20explored.%20However%2C%20CNNs%20exhibit%0Alimitations%20in%20long-range%20modeling%20capabilities%2C%20whereas%20Transformers%20are%0Ahampered%20by%20their%20quadratic%20computational%20complexity.%20Recently%2C%20State%20Space%0AModels%20%28SSMs%29%2C%20exemplified%20by%20Mamba%2C%20have%20emerged%20as%20a%20promising%20approach.%20They%0Anot%20only%20excel%20in%20modeling%20long-range%20interactions%20but%20also%20maintain%20a%20linear%0Acomputational%20complexity.%20In%20this%20paper%2C%20leveraging%20state%20space%20models%2C%20we%0Apropose%20a%20U-shape%20architecture%20model%20for%20medical%20image%20segmentation%2C%20named%0AVision%20Mamba%20UNet%20%28VM-UNet%29.%20Specifically%2C%20the%20Visual%20State%20Space%20%28VSS%29%20block%0Ais%20introduced%20as%20the%20foundation%20block%20to%20capture%20extensive%20contextual%0Ainformation%2C%20and%20an%20asymmetrical%20encoder-decoder%20structure%20is%20constructed%20with%0Afewer%20convolution%20layers%20to%20save%20calculation%20cost.%20We%20conduct%20comprehensive%0Aexperiments%20on%20the%20ISIC17%2C%20ISIC18%2C%20and%20Synapse%20datasets%2C%20and%20the%20results%0Aindicate%20that%20VM-UNet%20performs%20competitively%20in%20medical%20image%20segmentation%0Atasks.%20To%20our%20best%20knowledge%2C%20this%20is%20the%20first%20medical%20image%20segmentation%0Amodel%20constructed%20based%20on%20the%20pure%20SSM-based%20model.%20We%20aim%20to%20establish%20a%0Abaseline%20and%20provide%20valuable%20insights%20for%20the%20future%20development%20of%20more%0Aefficient%20and%20effective%20SSM-based%20segmentation%20systems.%20Our%20code%20is%20available%0Aat%20https%3A//github.com/JCruan519/VM-UNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02491v2&entry.124074799=Read"},
{"title": "Meta-models for transfer learning in source localisation", "author": "Lawrence A. Bull and Matthew R. Jones and Elizabeth J. Cross and Andrew Duncan and Mark Girolami", "abstract": "  In practice, non-destructive testing (NDT) procedures tend to consider\nexperiments (and their respective models) as distinct, conducted in isolation\nand associated with independent data. In contrast, this work looks to capture\nthe interdependencies between acoustic emission (AE) experiments (as\nmeta-models) and then use the resulting functions to predict the model\nhyperparameters for previously unobserved systems. We utilise a Bayesian\nmultilevel approach (similar to deep Gaussian Processes) where a higher level\nmeta-model captures the inter-task relationships. Our key contribution is how\nknowledge of the experimental campaign can be encoded between tasks as well as\nwithin tasks. We present an example of AE time-of-arrival mapping for source\nlocalisation, to illustrate how multilevel models naturally lend themselves to\nrepresenting aggregate systems in engineering. We constrain the meta-model\nbased on domain knowledge, then use the inter-task functions for transfer\nlearning, predicting hyperparameters for models of previously unobserved\nexperiments (for a specific design).\n", "link": "http://arxiv.org/abs/2305.08657v2", "date": "2024-11-08", "relevancy": 2.1152, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5387}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5336}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta-models%20for%20transfer%20learning%20in%20source%20localisation&body=Title%3A%20Meta-models%20for%20transfer%20learning%20in%20source%20localisation%0AAuthor%3A%20Lawrence%20A.%20Bull%20and%20Matthew%20R.%20Jones%20and%20Elizabeth%20J.%20Cross%20and%20Andrew%20Duncan%20and%20Mark%20Girolami%0AAbstract%3A%20%20%20In%20practice%2C%20non-destructive%20testing%20%28NDT%29%20procedures%20tend%20to%20consider%0Aexperiments%20%28and%20their%20respective%20models%29%20as%20distinct%2C%20conducted%20in%20isolation%0Aand%20associated%20with%20independent%20data.%20In%20contrast%2C%20this%20work%20looks%20to%20capture%0Athe%20interdependencies%20between%20acoustic%20emission%20%28AE%29%20experiments%20%28as%0Ameta-models%29%20and%20then%20use%20the%20resulting%20functions%20to%20predict%20the%20model%0Ahyperparameters%20for%20previously%20unobserved%20systems.%20We%20utilise%20a%20Bayesian%0Amultilevel%20approach%20%28similar%20to%20deep%20Gaussian%20Processes%29%20where%20a%20higher%20level%0Ameta-model%20captures%20the%20inter-task%20relationships.%20Our%20key%20contribution%20is%20how%0Aknowledge%20of%20the%20experimental%20campaign%20can%20be%20encoded%20between%20tasks%20as%20well%20as%0Awithin%20tasks.%20We%20present%20an%20example%20of%20AE%20time-of-arrival%20mapping%20for%20source%0Alocalisation%2C%20to%20illustrate%20how%20multilevel%20models%20naturally%20lend%20themselves%20to%0Arepresenting%20aggregate%20systems%20in%20engineering.%20We%20constrain%20the%20meta-model%0Abased%20on%20domain%20knowledge%2C%20then%20use%20the%20inter-task%20functions%20for%20transfer%0Alearning%2C%20predicting%20hyperparameters%20for%20models%20of%20previously%20unobserved%0Aexperiments%20%28for%20a%20specific%20design%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.08657v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta-models%2520for%2520transfer%2520learning%2520in%2520source%2520localisation%26entry.906535625%3DLawrence%2520A.%2520Bull%2520and%2520Matthew%2520R.%2520Jones%2520and%2520Elizabeth%2520J.%2520Cross%2520and%2520Andrew%2520Duncan%2520and%2520Mark%2520Girolami%26entry.1292438233%3D%2520%2520In%2520practice%252C%2520non-destructive%2520testing%2520%2528NDT%2529%2520procedures%2520tend%2520to%2520consider%250Aexperiments%2520%2528and%2520their%2520respective%2520models%2529%2520as%2520distinct%252C%2520conducted%2520in%2520isolation%250Aand%2520associated%2520with%2520independent%2520data.%2520In%2520contrast%252C%2520this%2520work%2520looks%2520to%2520capture%250Athe%2520interdependencies%2520between%2520acoustic%2520emission%2520%2528AE%2529%2520experiments%2520%2528as%250Ameta-models%2529%2520and%2520then%2520use%2520the%2520resulting%2520functions%2520to%2520predict%2520the%2520model%250Ahyperparameters%2520for%2520previously%2520unobserved%2520systems.%2520We%2520utilise%2520a%2520Bayesian%250Amultilevel%2520approach%2520%2528similar%2520to%2520deep%2520Gaussian%2520Processes%2529%2520where%2520a%2520higher%2520level%250Ameta-model%2520captures%2520the%2520inter-task%2520relationships.%2520Our%2520key%2520contribution%2520is%2520how%250Aknowledge%2520of%2520the%2520experimental%2520campaign%2520can%2520be%2520encoded%2520between%2520tasks%2520as%2520well%2520as%250Awithin%2520tasks.%2520We%2520present%2520an%2520example%2520of%2520AE%2520time-of-arrival%2520mapping%2520for%2520source%250Alocalisation%252C%2520to%2520illustrate%2520how%2520multilevel%2520models%2520naturally%2520lend%2520themselves%2520to%250Arepresenting%2520aggregate%2520systems%2520in%2520engineering.%2520We%2520constrain%2520the%2520meta-model%250Abased%2520on%2520domain%2520knowledge%252C%2520then%2520use%2520the%2520inter-task%2520functions%2520for%2520transfer%250Alearning%252C%2520predicting%2520hyperparameters%2520for%2520models%2520of%2520previously%2520unobserved%250Aexperiments%2520%2528for%2520a%2520specific%2520design%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.08657v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-models%20for%20transfer%20learning%20in%20source%20localisation&entry.906535625=Lawrence%20A.%20Bull%20and%20Matthew%20R.%20Jones%20and%20Elizabeth%20J.%20Cross%20and%20Andrew%20Duncan%20and%20Mark%20Girolami&entry.1292438233=%20%20In%20practice%2C%20non-destructive%20testing%20%28NDT%29%20procedures%20tend%20to%20consider%0Aexperiments%20%28and%20their%20respective%20models%29%20as%20distinct%2C%20conducted%20in%20isolation%0Aand%20associated%20with%20independent%20data.%20In%20contrast%2C%20this%20work%20looks%20to%20capture%0Athe%20interdependencies%20between%20acoustic%20emission%20%28AE%29%20experiments%20%28as%0Ameta-models%29%20and%20then%20use%20the%20resulting%20functions%20to%20predict%20the%20model%0Ahyperparameters%20for%20previously%20unobserved%20systems.%20We%20utilise%20a%20Bayesian%0Amultilevel%20approach%20%28similar%20to%20deep%20Gaussian%20Processes%29%20where%20a%20higher%20level%0Ameta-model%20captures%20the%20inter-task%20relationships.%20Our%20key%20contribution%20is%20how%0Aknowledge%20of%20the%20experimental%20campaign%20can%20be%20encoded%20between%20tasks%20as%20well%20as%0Awithin%20tasks.%20We%20present%20an%20example%20of%20AE%20time-of-arrival%20mapping%20for%20source%0Alocalisation%2C%20to%20illustrate%20how%20multilevel%20models%20naturally%20lend%20themselves%20to%0Arepresenting%20aggregate%20systems%20in%20engineering.%20We%20constrain%20the%20meta-model%0Abased%20on%20domain%20knowledge%2C%20then%20use%20the%20inter-task%20functions%20for%20transfer%0Alearning%2C%20predicting%20hyperparameters%20for%20models%20of%20previously%20unobserved%0Aexperiments%20%28for%20a%20specific%20design%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.08657v2&entry.124074799=Read"},
{"title": "Video RWKV:Video Action Recognition Based RWKV", "author": "Zhuowen Yin and Chengru Li and Xingbo Dong", "abstract": "  To address the challenges of high computational costs and long-distance\ndependencies in exist ing video understanding methods, such as CNNs and\nTransformers, this work introduces RWKV to the video domain in a novel way. We\npropose a LSTM CrossRWKV (LCR) framework, designed for spatiotemporal\nrepresentation learning to tackle the video understanding task. Specifically,\nthe proposed linear complexity LCR incorporates a novel Cross RWKV gate to\nfacilitate interaction be tween current frame edge information and past\nfeatures, enhancing the focus on the subject through edge features and globally\naggregating inter-frame features over time. LCR stores long-term mem ory for\nvideo processing through an enhanced LSTM recurrent execution mechanism. By\nleveraging the Cross RWKV gate and recurrent execution, LCR effectively\ncaptures both spatial and temporal features. Additionally, the edge information\nserves as a forgetting gate for LSTM, guiding long-term memory management.Tube\nmasking strategy reduces redundant information in food and reduces\noverfitting.These advantages enable LSTM CrossRWKV to set a new benchmark in\nvideo under standing, offering a scalable and efficient solution for\ncomprehensive video analysis. All code and models are publicly available.\n", "link": "http://arxiv.org/abs/2411.05636v1", "date": "2024-11-08", "relevancy": 2.0994, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5734}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5254}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20RWKV%3AVideo%20Action%20Recognition%20Based%20RWKV&body=Title%3A%20Video%20RWKV%3AVideo%20Action%20Recognition%20Based%20RWKV%0AAuthor%3A%20Zhuowen%20Yin%20and%20Chengru%20Li%20and%20Xingbo%20Dong%0AAbstract%3A%20%20%20To%20address%20the%20challenges%20of%20high%20computational%20costs%20and%20long-distance%0Adependencies%20in%20exist%20ing%20video%20understanding%20methods%2C%20such%20as%20CNNs%20and%0ATransformers%2C%20this%20work%20introduces%20RWKV%20to%20the%20video%20domain%20in%20a%20novel%20way.%20We%0Apropose%20a%20LSTM%20CrossRWKV%20%28LCR%29%20framework%2C%20designed%20for%20spatiotemporal%0Arepresentation%20learning%20to%20tackle%20the%20video%20understanding%20task.%20Specifically%2C%0Athe%20proposed%20linear%20complexity%20LCR%20incorporates%20a%20novel%20Cross%20RWKV%20gate%20to%0Afacilitate%20interaction%20be%20tween%20current%20frame%20edge%20information%20and%20past%0Afeatures%2C%20enhancing%20the%20focus%20on%20the%20subject%20through%20edge%20features%20and%20globally%0Aaggregating%20inter-frame%20features%20over%20time.%20LCR%20stores%20long-term%20mem%20ory%20for%0Avideo%20processing%20through%20an%20enhanced%20LSTM%20recurrent%20execution%20mechanism.%20By%0Aleveraging%20the%20Cross%20RWKV%20gate%20and%20recurrent%20execution%2C%20LCR%20effectively%0Acaptures%20both%20spatial%20and%20temporal%20features.%20Additionally%2C%20the%20edge%20information%0Aserves%20as%20a%20forgetting%20gate%20for%20LSTM%2C%20guiding%20long-term%20memory%20management.Tube%0Amasking%20strategy%20reduces%20redundant%20information%20in%20food%20and%20reduces%0Aoverfitting.These%20advantages%20enable%20LSTM%20CrossRWKV%20to%20set%20a%20new%20benchmark%20in%0Avideo%20under%20standing%2C%20offering%20a%20scalable%20and%20efficient%20solution%20for%0Acomprehensive%20video%20analysis.%20All%20code%20and%20models%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05636v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520RWKV%253AVideo%2520Action%2520Recognition%2520Based%2520RWKV%26entry.906535625%3DZhuowen%2520Yin%2520and%2520Chengru%2520Li%2520and%2520Xingbo%2520Dong%26entry.1292438233%3D%2520%2520To%2520address%2520the%2520challenges%2520of%2520high%2520computational%2520costs%2520and%2520long-distance%250Adependencies%2520in%2520exist%2520ing%2520video%2520understanding%2520methods%252C%2520such%2520as%2520CNNs%2520and%250ATransformers%252C%2520this%2520work%2520introduces%2520RWKV%2520to%2520the%2520video%2520domain%2520in%2520a%2520novel%2520way.%2520We%250Apropose%2520a%2520LSTM%2520CrossRWKV%2520%2528LCR%2529%2520framework%252C%2520designed%2520for%2520spatiotemporal%250Arepresentation%2520learning%2520to%2520tackle%2520the%2520video%2520understanding%2520task.%2520Specifically%252C%250Athe%2520proposed%2520linear%2520complexity%2520LCR%2520incorporates%2520a%2520novel%2520Cross%2520RWKV%2520gate%2520to%250Afacilitate%2520interaction%2520be%2520tween%2520current%2520frame%2520edge%2520information%2520and%2520past%250Afeatures%252C%2520enhancing%2520the%2520focus%2520on%2520the%2520subject%2520through%2520edge%2520features%2520and%2520globally%250Aaggregating%2520inter-frame%2520features%2520over%2520time.%2520LCR%2520stores%2520long-term%2520mem%2520ory%2520for%250Avideo%2520processing%2520through%2520an%2520enhanced%2520LSTM%2520recurrent%2520execution%2520mechanism.%2520By%250Aleveraging%2520the%2520Cross%2520RWKV%2520gate%2520and%2520recurrent%2520execution%252C%2520LCR%2520effectively%250Acaptures%2520both%2520spatial%2520and%2520temporal%2520features.%2520Additionally%252C%2520the%2520edge%2520information%250Aserves%2520as%2520a%2520forgetting%2520gate%2520for%2520LSTM%252C%2520guiding%2520long-term%2520memory%2520management.Tube%250Amasking%2520strategy%2520reduces%2520redundant%2520information%2520in%2520food%2520and%2520reduces%250Aoverfitting.These%2520advantages%2520enable%2520LSTM%2520CrossRWKV%2520to%2520set%2520a%2520new%2520benchmark%2520in%250Avideo%2520under%2520standing%252C%2520offering%2520a%2520scalable%2520and%2520efficient%2520solution%2520for%250Acomprehensive%2520video%2520analysis.%2520All%2520code%2520and%2520models%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05636v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20RWKV%3AVideo%20Action%20Recognition%20Based%20RWKV&entry.906535625=Zhuowen%20Yin%20and%20Chengru%20Li%20and%20Xingbo%20Dong&entry.1292438233=%20%20To%20address%20the%20challenges%20of%20high%20computational%20costs%20and%20long-distance%0Adependencies%20in%20exist%20ing%20video%20understanding%20methods%2C%20such%20as%20CNNs%20and%0ATransformers%2C%20this%20work%20introduces%20RWKV%20to%20the%20video%20domain%20in%20a%20novel%20way.%20We%0Apropose%20a%20LSTM%20CrossRWKV%20%28LCR%29%20framework%2C%20designed%20for%20spatiotemporal%0Arepresentation%20learning%20to%20tackle%20the%20video%20understanding%20task.%20Specifically%2C%0Athe%20proposed%20linear%20complexity%20LCR%20incorporates%20a%20novel%20Cross%20RWKV%20gate%20to%0Afacilitate%20interaction%20be%20tween%20current%20frame%20edge%20information%20and%20past%0Afeatures%2C%20enhancing%20the%20focus%20on%20the%20subject%20through%20edge%20features%20and%20globally%0Aaggregating%20inter-frame%20features%20over%20time.%20LCR%20stores%20long-term%20mem%20ory%20for%0Avideo%20processing%20through%20an%20enhanced%20LSTM%20recurrent%20execution%20mechanism.%20By%0Aleveraging%20the%20Cross%20RWKV%20gate%20and%20recurrent%20execution%2C%20LCR%20effectively%0Acaptures%20both%20spatial%20and%20temporal%20features.%20Additionally%2C%20the%20edge%20information%0Aserves%20as%20a%20forgetting%20gate%20for%20LSTM%2C%20guiding%20long-term%20memory%20management.Tube%0Amasking%20strategy%20reduces%20redundant%20information%20in%20food%20and%20reduces%0Aoverfitting.These%20advantages%20enable%20LSTM%20CrossRWKV%20to%20set%20a%20new%20benchmark%20in%0Avideo%20under%20standing%2C%20offering%20a%20scalable%20and%20efficient%20solution%20for%0Acomprehensive%20video%20analysis.%20All%20code%20and%20models%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05636v1&entry.124074799=Read"},
{"title": "Towards Scalable Foundation Models for Digital Dermatology", "author": "Fabian Gr\u00f6ger and Philippe Gottfrois and Ludovic Amruthalingam and Alvaro Gonzalez-Jimenez and Simone Lionetti and Luis R. Soenksen-Martinez and Alexander A. Navarini and Marc Pouly", "abstract": "  The growing demand for accurate and equitable AI models in digital\ndermatology faces a significant challenge: the lack of diverse, high-quality\nlabeled data. In this work, we investigate the potential of domain-specific\nfoundation models for dermatology in addressing this challenge. We utilize\nself-supervised learning (SSL) techniques to pre-train models on a dataset of\nover 240,000 dermatological images from public and private collections. Our\nstudy considers several SSL methods and compares the resulting foundation\nmodels against domain-agnostic models like those pre-trained on ImageNet and\nstate-of-the-art models such as MONET across 12 downstream tasks. Unlike\nprevious research, we emphasize the development of smaller models that are more\nsuitable for resource-limited clinical settings, facilitating easier adaptation\nto a broad range of use cases. Results show that models pre-trained in this\nwork not only outperform general-purpose models but also approach the\nperformance of models 50 times larger on clinically relevant diagnostic tasks.\nTo promote further research in this direction, we publicly release both the\ntraining code and the foundation models, which can benefit clinicians in\ndermatological applications.\n", "link": "http://arxiv.org/abs/2411.05514v1", "date": "2024-11-08", "relevancy": 2.0941, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5307}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5221}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5221}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Scalable%20Foundation%20Models%20for%20Digital%20Dermatology&body=Title%3A%20Towards%20Scalable%20Foundation%20Models%20for%20Digital%20Dermatology%0AAuthor%3A%20Fabian%20Gr%C3%B6ger%20and%20Philippe%20Gottfrois%20and%20Ludovic%20Amruthalingam%20and%20Alvaro%20Gonzalez-Jimenez%20and%20Simone%20Lionetti%20and%20Luis%20R.%20Soenksen-Martinez%20and%20Alexander%20A.%20Navarini%20and%20Marc%20Pouly%0AAbstract%3A%20%20%20The%20growing%20demand%20for%20accurate%20and%20equitable%20AI%20models%20in%20digital%0Adermatology%20faces%20a%20significant%20challenge%3A%20the%20lack%20of%20diverse%2C%20high-quality%0Alabeled%20data.%20In%20this%20work%2C%20we%20investigate%20the%20potential%20of%20domain-specific%0Afoundation%20models%20for%20dermatology%20in%20addressing%20this%20challenge.%20We%20utilize%0Aself-supervised%20learning%20%28SSL%29%20techniques%20to%20pre-train%20models%20on%20a%20dataset%20of%0Aover%20240%2C000%20dermatological%20images%20from%20public%20and%20private%20collections.%20Our%0Astudy%20considers%20several%20SSL%20methods%20and%20compares%20the%20resulting%20foundation%0Amodels%20against%20domain-agnostic%20models%20like%20those%20pre-trained%20on%20ImageNet%20and%0Astate-of-the-art%20models%20such%20as%20MONET%20across%2012%20downstream%20tasks.%20Unlike%0Aprevious%20research%2C%20we%20emphasize%20the%20development%20of%20smaller%20models%20that%20are%20more%0Asuitable%20for%20resource-limited%20clinical%20settings%2C%20facilitating%20easier%20adaptation%0Ato%20a%20broad%20range%20of%20use%20cases.%20Results%20show%20that%20models%20pre-trained%20in%20this%0Awork%20not%20only%20outperform%20general-purpose%20models%20but%20also%20approach%20the%0Aperformance%20of%20models%2050%20times%20larger%20on%20clinically%20relevant%20diagnostic%20tasks.%0ATo%20promote%20further%20research%20in%20this%20direction%2C%20we%20publicly%20release%20both%20the%0Atraining%20code%20and%20the%20foundation%20models%2C%20which%20can%20benefit%20clinicians%20in%0Adermatological%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05514v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Scalable%2520Foundation%2520Models%2520for%2520Digital%2520Dermatology%26entry.906535625%3DFabian%2520Gr%25C3%25B6ger%2520and%2520Philippe%2520Gottfrois%2520and%2520Ludovic%2520Amruthalingam%2520and%2520Alvaro%2520Gonzalez-Jimenez%2520and%2520Simone%2520Lionetti%2520and%2520Luis%2520R.%2520Soenksen-Martinez%2520and%2520Alexander%2520A.%2520Navarini%2520and%2520Marc%2520Pouly%26entry.1292438233%3D%2520%2520The%2520growing%2520demand%2520for%2520accurate%2520and%2520equitable%2520AI%2520models%2520in%2520digital%250Adermatology%2520faces%2520a%2520significant%2520challenge%253A%2520the%2520lack%2520of%2520diverse%252C%2520high-quality%250Alabeled%2520data.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520potential%2520of%2520domain-specific%250Afoundation%2520models%2520for%2520dermatology%2520in%2520addressing%2520this%2520challenge.%2520We%2520utilize%250Aself-supervised%2520learning%2520%2528SSL%2529%2520techniques%2520to%2520pre-train%2520models%2520on%2520a%2520dataset%2520of%250Aover%2520240%252C000%2520dermatological%2520images%2520from%2520public%2520and%2520private%2520collections.%2520Our%250Astudy%2520considers%2520several%2520SSL%2520methods%2520and%2520compares%2520the%2520resulting%2520foundation%250Amodels%2520against%2520domain-agnostic%2520models%2520like%2520those%2520pre-trained%2520on%2520ImageNet%2520and%250Astate-of-the-art%2520models%2520such%2520as%2520MONET%2520across%252012%2520downstream%2520tasks.%2520Unlike%250Aprevious%2520research%252C%2520we%2520emphasize%2520the%2520development%2520of%2520smaller%2520models%2520that%2520are%2520more%250Asuitable%2520for%2520resource-limited%2520clinical%2520settings%252C%2520facilitating%2520easier%2520adaptation%250Ato%2520a%2520broad%2520range%2520of%2520use%2520cases.%2520Results%2520show%2520that%2520models%2520pre-trained%2520in%2520this%250Awork%2520not%2520only%2520outperform%2520general-purpose%2520models%2520but%2520also%2520approach%2520the%250Aperformance%2520of%2520models%252050%2520times%2520larger%2520on%2520clinically%2520relevant%2520diagnostic%2520tasks.%250ATo%2520promote%2520further%2520research%2520in%2520this%2520direction%252C%2520we%2520publicly%2520release%2520both%2520the%250Atraining%2520code%2520and%2520the%2520foundation%2520models%252C%2520which%2520can%2520benefit%2520clinicians%2520in%250Adermatological%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05514v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Scalable%20Foundation%20Models%20for%20Digital%20Dermatology&entry.906535625=Fabian%20Gr%C3%B6ger%20and%20Philippe%20Gottfrois%20and%20Ludovic%20Amruthalingam%20and%20Alvaro%20Gonzalez-Jimenez%20and%20Simone%20Lionetti%20and%20Luis%20R.%20Soenksen-Martinez%20and%20Alexander%20A.%20Navarini%20and%20Marc%20Pouly&entry.1292438233=%20%20The%20growing%20demand%20for%20accurate%20and%20equitable%20AI%20models%20in%20digital%0Adermatology%20faces%20a%20significant%20challenge%3A%20the%20lack%20of%20diverse%2C%20high-quality%0Alabeled%20data.%20In%20this%20work%2C%20we%20investigate%20the%20potential%20of%20domain-specific%0Afoundation%20models%20for%20dermatology%20in%20addressing%20this%20challenge.%20We%20utilize%0Aself-supervised%20learning%20%28SSL%29%20techniques%20to%20pre-train%20models%20on%20a%20dataset%20of%0Aover%20240%2C000%20dermatological%20images%20from%20public%20and%20private%20collections.%20Our%0Astudy%20considers%20several%20SSL%20methods%20and%20compares%20the%20resulting%20foundation%0Amodels%20against%20domain-agnostic%20models%20like%20those%20pre-trained%20on%20ImageNet%20and%0Astate-of-the-art%20models%20such%20as%20MONET%20across%2012%20downstream%20tasks.%20Unlike%0Aprevious%20research%2C%20we%20emphasize%20the%20development%20of%20smaller%20models%20that%20are%20more%0Asuitable%20for%20resource-limited%20clinical%20settings%2C%20facilitating%20easier%20adaptation%0Ato%20a%20broad%20range%20of%20use%20cases.%20Results%20show%20that%20models%20pre-trained%20in%20this%0Awork%20not%20only%20outperform%20general-purpose%20models%20but%20also%20approach%20the%0Aperformance%20of%20models%2050%20times%20larger%20on%20clinically%20relevant%20diagnostic%20tasks.%0ATo%20promote%20further%20research%20in%20this%20direction%2C%20we%20publicly%20release%20both%20the%0Atraining%20code%20and%20the%20foundation%20models%2C%20which%20can%20benefit%20clinicians%20in%0Adermatological%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05514v1&entry.124074799=Read"},
{"title": "Graph-Dictionary Signal Model for Sparse Representations of Multivariate\n  Data", "author": "William Cappelletti and Pascal Frossard", "abstract": "  Representing and exploiting multivariate signals require capturing complex\nrelations between variables. We define a novel Graph-Dictionary signal model,\nwhere a finite set of graphs characterizes relationships in data distribution\nthrough a weighted sum of their Laplacians. We propose a framework to infer the\ngraph dictionary representation from observed data, along with a bilinear\ngeneralization of the primal-dual splitting algorithm to solve the learning\nproblem. Our new formulation allows to include a priori knowledge on signal\nproperties, as well as on underlying graphs and their coefficients. We show the\ncapability of our method to reconstruct graphs from signals in multiple\nsynthetic settings, where our model outperforms previous baselines. Then, we\nexploit graph-dictionary representations in a motor imagery decoding task on\nbrain activity data, where we classify imagined motion better than standard\nmethods relying on many more features.\n", "link": "http://arxiv.org/abs/2411.05729v1", "date": "2024-11-08", "relevancy": 2.0893, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5492}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5059}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-Dictionary%20Signal%20Model%20for%20Sparse%20Representations%20of%20Multivariate%0A%20%20Data&body=Title%3A%20Graph-Dictionary%20Signal%20Model%20for%20Sparse%20Representations%20of%20Multivariate%0A%20%20Data%0AAuthor%3A%20William%20Cappelletti%20and%20Pascal%20Frossard%0AAbstract%3A%20%20%20Representing%20and%20exploiting%20multivariate%20signals%20require%20capturing%20complex%0Arelations%20between%20variables.%20We%20define%20a%20novel%20Graph-Dictionary%20signal%20model%2C%0Awhere%20a%20finite%20set%20of%20graphs%20characterizes%20relationships%20in%20data%20distribution%0Athrough%20a%20weighted%20sum%20of%20their%20Laplacians.%20We%20propose%20a%20framework%20to%20infer%20the%0Agraph%20dictionary%20representation%20from%20observed%20data%2C%20along%20with%20a%20bilinear%0Ageneralization%20of%20the%20primal-dual%20splitting%20algorithm%20to%20solve%20the%20learning%0Aproblem.%20Our%20new%20formulation%20allows%20to%20include%20a%20priori%20knowledge%20on%20signal%0Aproperties%2C%20as%20well%20as%20on%20underlying%20graphs%20and%20their%20coefficients.%20We%20show%20the%0Acapability%20of%20our%20method%20to%20reconstruct%20graphs%20from%20signals%20in%20multiple%0Asynthetic%20settings%2C%20where%20our%20model%20outperforms%20previous%20baselines.%20Then%2C%20we%0Aexploit%20graph-dictionary%20representations%20in%20a%20motor%20imagery%20decoding%20task%20on%0Abrain%20activity%20data%2C%20where%20we%20classify%20imagined%20motion%20better%20than%20standard%0Amethods%20relying%20on%20many%20more%20features.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05729v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-Dictionary%2520Signal%2520Model%2520for%2520Sparse%2520Representations%2520of%2520Multivariate%250A%2520%2520Data%26entry.906535625%3DWilliam%2520Cappelletti%2520and%2520Pascal%2520Frossard%26entry.1292438233%3D%2520%2520Representing%2520and%2520exploiting%2520multivariate%2520signals%2520require%2520capturing%2520complex%250Arelations%2520between%2520variables.%2520We%2520define%2520a%2520novel%2520Graph-Dictionary%2520signal%2520model%252C%250Awhere%2520a%2520finite%2520set%2520of%2520graphs%2520characterizes%2520relationships%2520in%2520data%2520distribution%250Athrough%2520a%2520weighted%2520sum%2520of%2520their%2520Laplacians.%2520We%2520propose%2520a%2520framework%2520to%2520infer%2520the%250Agraph%2520dictionary%2520representation%2520from%2520observed%2520data%252C%2520along%2520with%2520a%2520bilinear%250Ageneralization%2520of%2520the%2520primal-dual%2520splitting%2520algorithm%2520to%2520solve%2520the%2520learning%250Aproblem.%2520Our%2520new%2520formulation%2520allows%2520to%2520include%2520a%2520priori%2520knowledge%2520on%2520signal%250Aproperties%252C%2520as%2520well%2520as%2520on%2520underlying%2520graphs%2520and%2520their%2520coefficients.%2520We%2520show%2520the%250Acapability%2520of%2520our%2520method%2520to%2520reconstruct%2520graphs%2520from%2520signals%2520in%2520multiple%250Asynthetic%2520settings%252C%2520where%2520our%2520model%2520outperforms%2520previous%2520baselines.%2520Then%252C%2520we%250Aexploit%2520graph-dictionary%2520representations%2520in%2520a%2520motor%2520imagery%2520decoding%2520task%2520on%250Abrain%2520activity%2520data%252C%2520where%2520we%2520classify%2520imagined%2520motion%2520better%2520than%2520standard%250Amethods%2520relying%2520on%2520many%2520more%2520features.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05729v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-Dictionary%20Signal%20Model%20for%20Sparse%20Representations%20of%20Multivariate%0A%20%20Data&entry.906535625=William%20Cappelletti%20and%20Pascal%20Frossard&entry.1292438233=%20%20Representing%20and%20exploiting%20multivariate%20signals%20require%20capturing%20complex%0Arelations%20between%20variables.%20We%20define%20a%20novel%20Graph-Dictionary%20signal%20model%2C%0Awhere%20a%20finite%20set%20of%20graphs%20characterizes%20relationships%20in%20data%20distribution%0Athrough%20a%20weighted%20sum%20of%20their%20Laplacians.%20We%20propose%20a%20framework%20to%20infer%20the%0Agraph%20dictionary%20representation%20from%20observed%20data%2C%20along%20with%20a%20bilinear%0Ageneralization%20of%20the%20primal-dual%20splitting%20algorithm%20to%20solve%20the%20learning%0Aproblem.%20Our%20new%20formulation%20allows%20to%20include%20a%20priori%20knowledge%20on%20signal%0Aproperties%2C%20as%20well%20as%20on%20underlying%20graphs%20and%20their%20coefficients.%20We%20show%20the%0Acapability%20of%20our%20method%20to%20reconstruct%20graphs%20from%20signals%20in%20multiple%0Asynthetic%20settings%2C%20where%20our%20model%20outperforms%20previous%20baselines.%20Then%2C%20we%0Aexploit%20graph-dictionary%20representations%20in%20a%20motor%20imagery%20decoding%20task%20on%0Abrain%20activity%20data%2C%20where%20we%20classify%20imagined%20motion%20better%20than%20standard%0Amethods%20relying%20on%20many%20more%20features.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05729v1&entry.124074799=Read"},
{"title": "A Nerf-Based Color Consistency Method for Remote Sensing Images", "author": "Zongcheng Zuo and Yuanxiang Li and Tongtong Zhang", "abstract": "  Due to different seasons, illumination, and atmospheric conditions, the\nphotometric of the acquired image varies greatly, which leads to obvious\nstitching seams at the edges of the mosaic image. Traditional methods can be\ndivided into two categories, one is absolute radiation correction and the other\nis relative radiation normalization. We propose a NeRF-based method of color\nconsistency correction for multi-view images, which weaves image features\ntogether using implicit expressions, and then re-illuminates feature space to\ngenerate a fusion image with a new perspective. We chose Superview-1 satellite\nimages and UAV images with large range and time difference for the experiment.\nExperimental results show that the synthesize image generated by our method has\nexcellent visual effect and smooth color transition at the edges.\n", "link": "http://arxiv.org/abs/2411.05557v1", "date": "2024-11-08", "relevancy": 2.0891, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5331}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5173}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Nerf-Based%20Color%20Consistency%20Method%20for%20Remote%20Sensing%20Images&body=Title%3A%20A%20Nerf-Based%20Color%20Consistency%20Method%20for%20Remote%20Sensing%20Images%0AAuthor%3A%20Zongcheng%20Zuo%20and%20Yuanxiang%20Li%20and%20Tongtong%20Zhang%0AAbstract%3A%20%20%20Due%20to%20different%20seasons%2C%20illumination%2C%20and%20atmospheric%20conditions%2C%20the%0Aphotometric%20of%20the%20acquired%20image%20varies%20greatly%2C%20which%20leads%20to%20obvious%0Astitching%20seams%20at%20the%20edges%20of%20the%20mosaic%20image.%20Traditional%20methods%20can%20be%0Adivided%20into%20two%20categories%2C%20one%20is%20absolute%20radiation%20correction%20and%20the%20other%0Ais%20relative%20radiation%20normalization.%20We%20propose%20a%20NeRF-based%20method%20of%20color%0Aconsistency%20correction%20for%20multi-view%20images%2C%20which%20weaves%20image%20features%0Atogether%20using%20implicit%20expressions%2C%20and%20then%20re-illuminates%20feature%20space%20to%0Agenerate%20a%20fusion%20image%20with%20a%20new%20perspective.%20We%20chose%20Superview-1%20satellite%0Aimages%20and%20UAV%20images%20with%20large%20range%20and%20time%20difference%20for%20the%20experiment.%0AExperimental%20results%20show%20that%20the%20synthesize%20image%20generated%20by%20our%20method%20has%0Aexcellent%20visual%20effect%20and%20smooth%20color%20transition%20at%20the%20edges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Nerf-Based%2520Color%2520Consistency%2520Method%2520for%2520Remote%2520Sensing%2520Images%26entry.906535625%3DZongcheng%2520Zuo%2520and%2520Yuanxiang%2520Li%2520and%2520Tongtong%2520Zhang%26entry.1292438233%3D%2520%2520Due%2520to%2520different%2520seasons%252C%2520illumination%252C%2520and%2520atmospheric%2520conditions%252C%2520the%250Aphotometric%2520of%2520the%2520acquired%2520image%2520varies%2520greatly%252C%2520which%2520leads%2520to%2520obvious%250Astitching%2520seams%2520at%2520the%2520edges%2520of%2520the%2520mosaic%2520image.%2520Traditional%2520methods%2520can%2520be%250Adivided%2520into%2520two%2520categories%252C%2520one%2520is%2520absolute%2520radiation%2520correction%2520and%2520the%2520other%250Ais%2520relative%2520radiation%2520normalization.%2520We%2520propose%2520a%2520NeRF-based%2520method%2520of%2520color%250Aconsistency%2520correction%2520for%2520multi-view%2520images%252C%2520which%2520weaves%2520image%2520features%250Atogether%2520using%2520implicit%2520expressions%252C%2520and%2520then%2520re-illuminates%2520feature%2520space%2520to%250Agenerate%2520a%2520fusion%2520image%2520with%2520a%2520new%2520perspective.%2520We%2520chose%2520Superview-1%2520satellite%250Aimages%2520and%2520UAV%2520images%2520with%2520large%2520range%2520and%2520time%2520difference%2520for%2520the%2520experiment.%250AExperimental%2520results%2520show%2520that%2520the%2520synthesize%2520image%2520generated%2520by%2520our%2520method%2520has%250Aexcellent%2520visual%2520effect%2520and%2520smooth%2520color%2520transition%2520at%2520the%2520edges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Nerf-Based%20Color%20Consistency%20Method%20for%20Remote%20Sensing%20Images&entry.906535625=Zongcheng%20Zuo%20and%20Yuanxiang%20Li%20and%20Tongtong%20Zhang&entry.1292438233=%20%20Due%20to%20different%20seasons%2C%20illumination%2C%20and%20atmospheric%20conditions%2C%20the%0Aphotometric%20of%20the%20acquired%20image%20varies%20greatly%2C%20which%20leads%20to%20obvious%0Astitching%20seams%20at%20the%20edges%20of%20the%20mosaic%20image.%20Traditional%20methods%20can%20be%0Adivided%20into%20two%20categories%2C%20one%20is%20absolute%20radiation%20correction%20and%20the%20other%0Ais%20relative%20radiation%20normalization.%20We%20propose%20a%20NeRF-based%20method%20of%20color%0Aconsistency%20correction%20for%20multi-view%20images%2C%20which%20weaves%20image%20features%0Atogether%20using%20implicit%20expressions%2C%20and%20then%20re-illuminates%20feature%20space%20to%0Agenerate%20a%20fusion%20image%20with%20a%20new%20perspective.%20We%20chose%20Superview-1%20satellite%0Aimages%20and%20UAV%20images%20with%20large%20range%20and%20time%20difference%20for%20the%20experiment.%0AExperimental%20results%20show%20that%20the%20synthesize%20image%20generated%20by%20our%20method%20has%0Aexcellent%20visual%20effect%20and%20smooth%20color%20transition%20at%20the%20edges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05557v1&entry.124074799=Read"},
{"title": "ROAD-Waymo: Action Awareness at Scale for Autonomous Driving", "author": "Salman Khan and Izzeddin Teeti and Reza Javanmard Alitappeh and Mihaela C. Stoian and Eleonora Giunchiglia and Gurkirt Singh and Andrew Bradley and Fabio Cuzzolin", "abstract": "  Autonomous Vehicle (AV) perception systems require more than simply seeing,\nvia e.g., object detection or scene segmentation. They need a holistic\nunderstanding of what is happening within the scene for safe interaction with\nother road users. Few datasets exist for the purpose of developing and training\nalgorithms to comprehend the actions of other road users. This paper presents\nROAD-Waymo, an extensive dataset for the development and benchmarking of\ntechniques for agent, action, location and event detection in road scenes,\nprovided as a layer upon the (US) Waymo Open dataset. Considerably larger and\nmore challenging than any existing dataset (and encompassing multiple cities),\nit comes with 198k annotated video frames, 54k agent tubes, 3.9M bounding boxes\nand a total of 12.4M labels. The integrity of the dataset has been confirmed\nand enhanced via a novel annotation pipeline designed for automatically\nidentifying violations of requirements specifically designed for this dataset.\nAs ROAD-Waymo is compatible with the original (UK) ROAD dataset, it provides\nthe opportunity to tackle domain adaptation between real-world road scenarios\nin different countries within a novel benchmark: ROAD++.\n", "link": "http://arxiv.org/abs/2411.01683v2", "date": "2024-11-08", "relevancy": 2.0845, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5339}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5207}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ROAD-Waymo%3A%20Action%20Awareness%20at%20Scale%20for%20Autonomous%20Driving&body=Title%3A%20ROAD-Waymo%3A%20Action%20Awareness%20at%20Scale%20for%20Autonomous%20Driving%0AAuthor%3A%20Salman%20Khan%20and%20Izzeddin%20Teeti%20and%20Reza%20Javanmard%20Alitappeh%20and%20Mihaela%20C.%20Stoian%20and%20Eleonora%20Giunchiglia%20and%20Gurkirt%20Singh%20and%20Andrew%20Bradley%20and%20Fabio%20Cuzzolin%0AAbstract%3A%20%20%20Autonomous%20Vehicle%20%28AV%29%20perception%20systems%20require%20more%20than%20simply%20seeing%2C%0Avia%20e.g.%2C%20object%20detection%20or%20scene%20segmentation.%20They%20need%20a%20holistic%0Aunderstanding%20of%20what%20is%20happening%20within%20the%20scene%20for%20safe%20interaction%20with%0Aother%20road%20users.%20Few%20datasets%20exist%20for%20the%20purpose%20of%20developing%20and%20training%0Aalgorithms%20to%20comprehend%20the%20actions%20of%20other%20road%20users.%20This%20paper%20presents%0AROAD-Waymo%2C%20an%20extensive%20dataset%20for%20the%20development%20and%20benchmarking%20of%0Atechniques%20for%20agent%2C%20action%2C%20location%20and%20event%20detection%20in%20road%20scenes%2C%0Aprovided%20as%20a%20layer%20upon%20the%20%28US%29%20Waymo%20Open%20dataset.%20Considerably%20larger%20and%0Amore%20challenging%20than%20any%20existing%20dataset%20%28and%20encompassing%20multiple%20cities%29%2C%0Ait%20comes%20with%20198k%20annotated%20video%20frames%2C%2054k%20agent%20tubes%2C%203.9M%20bounding%20boxes%0Aand%20a%20total%20of%2012.4M%20labels.%20The%20integrity%20of%20the%20dataset%20has%20been%20confirmed%0Aand%20enhanced%20via%20a%20novel%20annotation%20pipeline%20designed%20for%20automatically%0Aidentifying%20violations%20of%20requirements%20specifically%20designed%20for%20this%20dataset.%0AAs%20ROAD-Waymo%20is%20compatible%20with%20the%20original%20%28UK%29%20ROAD%20dataset%2C%20it%20provides%0Athe%20opportunity%20to%20tackle%20domain%20adaptation%20between%20real-world%20road%20scenarios%0Ain%20different%20countries%20within%20a%20novel%20benchmark%3A%20ROAD%2B%2B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.01683v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DROAD-Waymo%253A%2520Action%2520Awareness%2520at%2520Scale%2520for%2520Autonomous%2520Driving%26entry.906535625%3DSalman%2520Khan%2520and%2520Izzeddin%2520Teeti%2520and%2520Reza%2520Javanmard%2520Alitappeh%2520and%2520Mihaela%2520C.%2520Stoian%2520and%2520Eleonora%2520Giunchiglia%2520and%2520Gurkirt%2520Singh%2520and%2520Andrew%2520Bradley%2520and%2520Fabio%2520Cuzzolin%26entry.1292438233%3D%2520%2520Autonomous%2520Vehicle%2520%2528AV%2529%2520perception%2520systems%2520require%2520more%2520than%2520simply%2520seeing%252C%250Avia%2520e.g.%252C%2520object%2520detection%2520or%2520scene%2520segmentation.%2520They%2520need%2520a%2520holistic%250Aunderstanding%2520of%2520what%2520is%2520happening%2520within%2520the%2520scene%2520for%2520safe%2520interaction%2520with%250Aother%2520road%2520users.%2520Few%2520datasets%2520exist%2520for%2520the%2520purpose%2520of%2520developing%2520and%2520training%250Aalgorithms%2520to%2520comprehend%2520the%2520actions%2520of%2520other%2520road%2520users.%2520This%2520paper%2520presents%250AROAD-Waymo%252C%2520an%2520extensive%2520dataset%2520for%2520the%2520development%2520and%2520benchmarking%2520of%250Atechniques%2520for%2520agent%252C%2520action%252C%2520location%2520and%2520event%2520detection%2520in%2520road%2520scenes%252C%250Aprovided%2520as%2520a%2520layer%2520upon%2520the%2520%2528US%2529%2520Waymo%2520Open%2520dataset.%2520Considerably%2520larger%2520and%250Amore%2520challenging%2520than%2520any%2520existing%2520dataset%2520%2528and%2520encompassing%2520multiple%2520cities%2529%252C%250Ait%2520comes%2520with%2520198k%2520annotated%2520video%2520frames%252C%252054k%2520agent%2520tubes%252C%25203.9M%2520bounding%2520boxes%250Aand%2520a%2520total%2520of%252012.4M%2520labels.%2520The%2520integrity%2520of%2520the%2520dataset%2520has%2520been%2520confirmed%250Aand%2520enhanced%2520via%2520a%2520novel%2520annotation%2520pipeline%2520designed%2520for%2520automatically%250Aidentifying%2520violations%2520of%2520requirements%2520specifically%2520designed%2520for%2520this%2520dataset.%250AAs%2520ROAD-Waymo%2520is%2520compatible%2520with%2520the%2520original%2520%2528UK%2529%2520ROAD%2520dataset%252C%2520it%2520provides%250Athe%2520opportunity%2520to%2520tackle%2520domain%2520adaptation%2520between%2520real-world%2520road%2520scenarios%250Ain%2520different%2520countries%2520within%2520a%2520novel%2520benchmark%253A%2520ROAD%252B%252B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.01683v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ROAD-Waymo%3A%20Action%20Awareness%20at%20Scale%20for%20Autonomous%20Driving&entry.906535625=Salman%20Khan%20and%20Izzeddin%20Teeti%20and%20Reza%20Javanmard%20Alitappeh%20and%20Mihaela%20C.%20Stoian%20and%20Eleonora%20Giunchiglia%20and%20Gurkirt%20Singh%20and%20Andrew%20Bradley%20and%20Fabio%20Cuzzolin&entry.1292438233=%20%20Autonomous%20Vehicle%20%28AV%29%20perception%20systems%20require%20more%20than%20simply%20seeing%2C%0Avia%20e.g.%2C%20object%20detection%20or%20scene%20segmentation.%20They%20need%20a%20holistic%0Aunderstanding%20of%20what%20is%20happening%20within%20the%20scene%20for%20safe%20interaction%20with%0Aother%20road%20users.%20Few%20datasets%20exist%20for%20the%20purpose%20of%20developing%20and%20training%0Aalgorithms%20to%20comprehend%20the%20actions%20of%20other%20road%20users.%20This%20paper%20presents%0AROAD-Waymo%2C%20an%20extensive%20dataset%20for%20the%20development%20and%20benchmarking%20of%0Atechniques%20for%20agent%2C%20action%2C%20location%20and%20event%20detection%20in%20road%20scenes%2C%0Aprovided%20as%20a%20layer%20upon%20the%20%28US%29%20Waymo%20Open%20dataset.%20Considerably%20larger%20and%0Amore%20challenging%20than%20any%20existing%20dataset%20%28and%20encompassing%20multiple%20cities%29%2C%0Ait%20comes%20with%20198k%20annotated%20video%20frames%2C%2054k%20agent%20tubes%2C%203.9M%20bounding%20boxes%0Aand%20a%20total%20of%2012.4M%20labels.%20The%20integrity%20of%20the%20dataset%20has%20been%20confirmed%0Aand%20enhanced%20via%20a%20novel%20annotation%20pipeline%20designed%20for%20automatically%0Aidentifying%20violations%20of%20requirements%20specifically%20designed%20for%20this%20dataset.%0AAs%20ROAD-Waymo%20is%20compatible%20with%20the%20original%20%28UK%29%20ROAD%20dataset%2C%20it%20provides%0Athe%20opportunity%20to%20tackle%20domain%20adaptation%20between%20real-world%20road%20scenarios%0Ain%20different%20countries%20within%20a%20novel%20benchmark%3A%20ROAD%2B%2B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.01683v2&entry.124074799=Read"},
{"title": "FinDVer: Explainable Claim Verification over Long and Hybrid-Content\n  Financial Documents", "author": "Yilun Zhao and Yitao Long and Yuru Jiang and Chengye Wang and Weiyuan Chen and Hongjun Liu and Yiming Zhang and Xiangru Tang and Chen Zhao and Arman Cohan", "abstract": "  We introduce FinDVer, a comprehensive benchmark specifically designed to\nevaluate the explainable claim verification capabilities of LLMs in the context\nof understanding and analyzing long, hybrid-content financial documents.\nFinDVer contains 2,400 expert-annotated examples, divided into three subsets:\ninformation extraction, numerical reasoning, and knowledge-intensive reasoning,\neach addressing common scenarios encountered in real-world financial contexts.\nWe assess a broad spectrum of LLMs under long-context and RAG settings. Our\nresults show that even the current best-performing system, GPT-4o, still lags\nbehind human experts. We further provide in-depth analysis on long-context and\nRAG setting, Chain-of-Thought reasoning, and model reasoning errors, offering\ninsights to drive future advancements. We believe that FinDVer can serve as a\nvaluable benchmark for evaluating LLMs in claim verification over complex,\nexpert-domain documents.\n", "link": "http://arxiv.org/abs/2411.05764v1", "date": "2024-11-08", "relevancy": 2.082, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5227}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5227}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FinDVer%3A%20Explainable%20Claim%20Verification%20over%20Long%20and%20Hybrid-Content%0A%20%20Financial%20Documents&body=Title%3A%20FinDVer%3A%20Explainable%20Claim%20Verification%20over%20Long%20and%20Hybrid-Content%0A%20%20Financial%20Documents%0AAuthor%3A%20Yilun%20Zhao%20and%20Yitao%20Long%20and%20Yuru%20Jiang%20and%20Chengye%20Wang%20and%20Weiyuan%20Chen%20and%20Hongjun%20Liu%20and%20Yiming%20Zhang%20and%20Xiangru%20Tang%20and%20Chen%20Zhao%20and%20Arman%20Cohan%0AAbstract%3A%20%20%20We%20introduce%20FinDVer%2C%20a%20comprehensive%20benchmark%20specifically%20designed%20to%0Aevaluate%20the%20explainable%20claim%20verification%20capabilities%20of%20LLMs%20in%20the%20context%0Aof%20understanding%20and%20analyzing%20long%2C%20hybrid-content%20financial%20documents.%0AFinDVer%20contains%202%2C400%20expert-annotated%20examples%2C%20divided%20into%20three%20subsets%3A%0Ainformation%20extraction%2C%20numerical%20reasoning%2C%20and%20knowledge-intensive%20reasoning%2C%0Aeach%20addressing%20common%20scenarios%20encountered%20in%20real-world%20financial%20contexts.%0AWe%20assess%20a%20broad%20spectrum%20of%20LLMs%20under%20long-context%20and%20RAG%20settings.%20Our%0Aresults%20show%20that%20even%20the%20current%20best-performing%20system%2C%20GPT-4o%2C%20still%20lags%0Abehind%20human%20experts.%20We%20further%20provide%20in-depth%20analysis%20on%20long-context%20and%0ARAG%20setting%2C%20Chain-of-Thought%20reasoning%2C%20and%20model%20reasoning%20errors%2C%20offering%0Ainsights%20to%20drive%20future%20advancements.%20We%20believe%20that%20FinDVer%20can%20serve%20as%20a%0Avaluable%20benchmark%20for%20evaluating%20LLMs%20in%20claim%20verification%20over%20complex%2C%0Aexpert-domain%20documents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05764v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinDVer%253A%2520Explainable%2520Claim%2520Verification%2520over%2520Long%2520and%2520Hybrid-Content%250A%2520%2520Financial%2520Documents%26entry.906535625%3DYilun%2520Zhao%2520and%2520Yitao%2520Long%2520and%2520Yuru%2520Jiang%2520and%2520Chengye%2520Wang%2520and%2520Weiyuan%2520Chen%2520and%2520Hongjun%2520Liu%2520and%2520Yiming%2520Zhang%2520and%2520Xiangru%2520Tang%2520and%2520Chen%2520Zhao%2520and%2520Arman%2520Cohan%26entry.1292438233%3D%2520%2520We%2520introduce%2520FinDVer%252C%2520a%2520comprehensive%2520benchmark%2520specifically%2520designed%2520to%250Aevaluate%2520the%2520explainable%2520claim%2520verification%2520capabilities%2520of%2520LLMs%2520in%2520the%2520context%250Aof%2520understanding%2520and%2520analyzing%2520long%252C%2520hybrid-content%2520financial%2520documents.%250AFinDVer%2520contains%25202%252C400%2520expert-annotated%2520examples%252C%2520divided%2520into%2520three%2520subsets%253A%250Ainformation%2520extraction%252C%2520numerical%2520reasoning%252C%2520and%2520knowledge-intensive%2520reasoning%252C%250Aeach%2520addressing%2520common%2520scenarios%2520encountered%2520in%2520real-world%2520financial%2520contexts.%250AWe%2520assess%2520a%2520broad%2520spectrum%2520of%2520LLMs%2520under%2520long-context%2520and%2520RAG%2520settings.%2520Our%250Aresults%2520show%2520that%2520even%2520the%2520current%2520best-performing%2520system%252C%2520GPT-4o%252C%2520still%2520lags%250Abehind%2520human%2520experts.%2520We%2520further%2520provide%2520in-depth%2520analysis%2520on%2520long-context%2520and%250ARAG%2520setting%252C%2520Chain-of-Thought%2520reasoning%252C%2520and%2520model%2520reasoning%2520errors%252C%2520offering%250Ainsights%2520to%2520drive%2520future%2520advancements.%2520We%2520believe%2520that%2520FinDVer%2520can%2520serve%2520as%2520a%250Avaluable%2520benchmark%2520for%2520evaluating%2520LLMs%2520in%2520claim%2520verification%2520over%2520complex%252C%250Aexpert-domain%2520documents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05764v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FinDVer%3A%20Explainable%20Claim%20Verification%20over%20Long%20and%20Hybrid-Content%0A%20%20Financial%20Documents&entry.906535625=Yilun%20Zhao%20and%20Yitao%20Long%20and%20Yuru%20Jiang%20and%20Chengye%20Wang%20and%20Weiyuan%20Chen%20and%20Hongjun%20Liu%20and%20Yiming%20Zhang%20and%20Xiangru%20Tang%20and%20Chen%20Zhao%20and%20Arman%20Cohan&entry.1292438233=%20%20We%20introduce%20FinDVer%2C%20a%20comprehensive%20benchmark%20specifically%20designed%20to%0Aevaluate%20the%20explainable%20claim%20verification%20capabilities%20of%20LLMs%20in%20the%20context%0Aof%20understanding%20and%20analyzing%20long%2C%20hybrid-content%20financial%20documents.%0AFinDVer%20contains%202%2C400%20expert-annotated%20examples%2C%20divided%20into%20three%20subsets%3A%0Ainformation%20extraction%2C%20numerical%20reasoning%2C%20and%20knowledge-intensive%20reasoning%2C%0Aeach%20addressing%20common%20scenarios%20encountered%20in%20real-world%20financial%20contexts.%0AWe%20assess%20a%20broad%20spectrum%20of%20LLMs%20under%20long-context%20and%20RAG%20settings.%20Our%0Aresults%20show%20that%20even%20the%20current%20best-performing%20system%2C%20GPT-4o%2C%20still%20lags%0Abehind%20human%20experts.%20We%20further%20provide%20in-depth%20analysis%20on%20long-context%20and%0ARAG%20setting%2C%20Chain-of-Thought%20reasoning%2C%20and%20model%20reasoning%20errors%2C%20offering%0Ainsights%20to%20drive%20future%20advancements.%20We%20believe%20that%20FinDVer%20can%20serve%20as%20a%0Avaluable%20benchmark%20for%20evaluating%20LLMs%20in%20claim%20verification%20over%20complex%2C%0Aexpert-domain%20documents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05764v1&entry.124074799=Read"},
{"title": "Topology-aware Reinforcement Feature Space Reconstruction for Graph Data", "author": "Wangyang Ying and Haoyue Bai and Kunpeng Liu and Yanjie Fu", "abstract": "  Feature space is an environment where data points are vectorized to represent\nthe original dataset. Reconstructing a good feature space is essential to\naugment the AI power of data, improve model generalization, and increase the\navailability of downstream ML models. Existing literature, such as feature\ntransformation and feature selection, is labor-intensive (e.g., heavy reliance\non empirical experience) and mostly designed for tabular data. Moreover, these\nmethods regard data samples as independent, which ignores the unique\ntopological structure when applied to graph data, thus resulting in a\nsuboptimal reconstruction feature space. Can we consider the topological\ninformation to automatically reconstruct feature space for graph data without\nheavy experiential knowledge? To fill this gap, we leverage topology-aware\nreinforcement learning to automate and optimize feature space reconstruction\nfor graph data. Our approach combines the extraction of core subgraphs to\ncapture essential structural information with a graph neural network (GNN) to\nencode topological features and reduce computing complexity. Then we introduce\nthree reinforcement agents within a hierarchical structure to systematically\ngenerate meaningful features through an iterative process, effectively\nreconstructing the feature space. This framework provides a principled solution\nfor attributed graph feature space reconstruction. The extensive experiments\ndemonstrate the effectiveness and efficiency of including topological\nawareness.\n", "link": "http://arxiv.org/abs/2411.05742v1", "date": "2024-11-08", "relevancy": 2.0745, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5322}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5093}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topology-aware%20Reinforcement%20Feature%20Space%20Reconstruction%20for%20Graph%20Data&body=Title%3A%20Topology-aware%20Reinforcement%20Feature%20Space%20Reconstruction%20for%20Graph%20Data%0AAuthor%3A%20Wangyang%20Ying%20and%20Haoyue%20Bai%20and%20Kunpeng%20Liu%20and%20Yanjie%20Fu%0AAbstract%3A%20%20%20Feature%20space%20is%20an%20environment%20where%20data%20points%20are%20vectorized%20to%20represent%0Athe%20original%20dataset.%20Reconstructing%20a%20good%20feature%20space%20is%20essential%20to%0Aaugment%20the%20AI%20power%20of%20data%2C%20improve%20model%20generalization%2C%20and%20increase%20the%0Aavailability%20of%20downstream%20ML%20models.%20Existing%20literature%2C%20such%20as%20feature%0Atransformation%20and%20feature%20selection%2C%20is%20labor-intensive%20%28e.g.%2C%20heavy%20reliance%0Aon%20empirical%20experience%29%20and%20mostly%20designed%20for%20tabular%20data.%20Moreover%2C%20these%0Amethods%20regard%20data%20samples%20as%20independent%2C%20which%20ignores%20the%20unique%0Atopological%20structure%20when%20applied%20to%20graph%20data%2C%20thus%20resulting%20in%20a%0Asuboptimal%20reconstruction%20feature%20space.%20Can%20we%20consider%20the%20topological%0Ainformation%20to%20automatically%20reconstruct%20feature%20space%20for%20graph%20data%20without%0Aheavy%20experiential%20knowledge%3F%20To%20fill%20this%20gap%2C%20we%20leverage%20topology-aware%0Areinforcement%20learning%20to%20automate%20and%20optimize%20feature%20space%20reconstruction%0Afor%20graph%20data.%20Our%20approach%20combines%20the%20extraction%20of%20core%20subgraphs%20to%0Acapture%20essential%20structural%20information%20with%20a%20graph%20neural%20network%20%28GNN%29%20to%0Aencode%20topological%20features%20and%20reduce%20computing%20complexity.%20Then%20we%20introduce%0Athree%20reinforcement%20agents%20within%20a%20hierarchical%20structure%20to%20systematically%0Agenerate%20meaningful%20features%20through%20an%20iterative%20process%2C%20effectively%0Areconstructing%20the%20feature%20space.%20This%20framework%20provides%20a%20principled%20solution%0Afor%20attributed%20graph%20feature%20space%20reconstruction.%20The%20extensive%20experiments%0Ademonstrate%20the%20effectiveness%20and%20efficiency%20of%20including%20topological%0Aawareness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopology-aware%2520Reinforcement%2520Feature%2520Space%2520Reconstruction%2520for%2520Graph%2520Data%26entry.906535625%3DWangyang%2520Ying%2520and%2520Haoyue%2520Bai%2520and%2520Kunpeng%2520Liu%2520and%2520Yanjie%2520Fu%26entry.1292438233%3D%2520%2520Feature%2520space%2520is%2520an%2520environment%2520where%2520data%2520points%2520are%2520vectorized%2520to%2520represent%250Athe%2520original%2520dataset.%2520Reconstructing%2520a%2520good%2520feature%2520space%2520is%2520essential%2520to%250Aaugment%2520the%2520AI%2520power%2520of%2520data%252C%2520improve%2520model%2520generalization%252C%2520and%2520increase%2520the%250Aavailability%2520of%2520downstream%2520ML%2520models.%2520Existing%2520literature%252C%2520such%2520as%2520feature%250Atransformation%2520and%2520feature%2520selection%252C%2520is%2520labor-intensive%2520%2528e.g.%252C%2520heavy%2520reliance%250Aon%2520empirical%2520experience%2529%2520and%2520mostly%2520designed%2520for%2520tabular%2520data.%2520Moreover%252C%2520these%250Amethods%2520regard%2520data%2520samples%2520as%2520independent%252C%2520which%2520ignores%2520the%2520unique%250Atopological%2520structure%2520when%2520applied%2520to%2520graph%2520data%252C%2520thus%2520resulting%2520in%2520a%250Asuboptimal%2520reconstruction%2520feature%2520space.%2520Can%2520we%2520consider%2520the%2520topological%250Ainformation%2520to%2520automatically%2520reconstruct%2520feature%2520space%2520for%2520graph%2520data%2520without%250Aheavy%2520experiential%2520knowledge%253F%2520To%2520fill%2520this%2520gap%252C%2520we%2520leverage%2520topology-aware%250Areinforcement%2520learning%2520to%2520automate%2520and%2520optimize%2520feature%2520space%2520reconstruction%250Afor%2520graph%2520data.%2520Our%2520approach%2520combines%2520the%2520extraction%2520of%2520core%2520subgraphs%2520to%250Acapture%2520essential%2520structural%2520information%2520with%2520a%2520graph%2520neural%2520network%2520%2528GNN%2529%2520to%250Aencode%2520topological%2520features%2520and%2520reduce%2520computing%2520complexity.%2520Then%2520we%2520introduce%250Athree%2520reinforcement%2520agents%2520within%2520a%2520hierarchical%2520structure%2520to%2520systematically%250Agenerate%2520meaningful%2520features%2520through%2520an%2520iterative%2520process%252C%2520effectively%250Areconstructing%2520the%2520feature%2520space.%2520This%2520framework%2520provides%2520a%2520principled%2520solution%250Afor%2520attributed%2520graph%2520feature%2520space%2520reconstruction.%2520The%2520extensive%2520experiments%250Ademonstrate%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520including%2520topological%250Aawareness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topology-aware%20Reinforcement%20Feature%20Space%20Reconstruction%20for%20Graph%20Data&entry.906535625=Wangyang%20Ying%20and%20Haoyue%20Bai%20and%20Kunpeng%20Liu%20and%20Yanjie%20Fu&entry.1292438233=%20%20Feature%20space%20is%20an%20environment%20where%20data%20points%20are%20vectorized%20to%20represent%0Athe%20original%20dataset.%20Reconstructing%20a%20good%20feature%20space%20is%20essential%20to%0Aaugment%20the%20AI%20power%20of%20data%2C%20improve%20model%20generalization%2C%20and%20increase%20the%0Aavailability%20of%20downstream%20ML%20models.%20Existing%20literature%2C%20such%20as%20feature%0Atransformation%20and%20feature%20selection%2C%20is%20labor-intensive%20%28e.g.%2C%20heavy%20reliance%0Aon%20empirical%20experience%29%20and%20mostly%20designed%20for%20tabular%20data.%20Moreover%2C%20these%0Amethods%20regard%20data%20samples%20as%20independent%2C%20which%20ignores%20the%20unique%0Atopological%20structure%20when%20applied%20to%20graph%20data%2C%20thus%20resulting%20in%20a%0Asuboptimal%20reconstruction%20feature%20space.%20Can%20we%20consider%20the%20topological%0Ainformation%20to%20automatically%20reconstruct%20feature%20space%20for%20graph%20data%20without%0Aheavy%20experiential%20knowledge%3F%20To%20fill%20this%20gap%2C%20we%20leverage%20topology-aware%0Areinforcement%20learning%20to%20automate%20and%20optimize%20feature%20space%20reconstruction%0Afor%20graph%20data.%20Our%20approach%20combines%20the%20extraction%20of%20core%20subgraphs%20to%0Acapture%20essential%20structural%20information%20with%20a%20graph%20neural%20network%20%28GNN%29%20to%0Aencode%20topological%20features%20and%20reduce%20computing%20complexity.%20Then%20we%20introduce%0Athree%20reinforcement%20agents%20within%20a%20hierarchical%20structure%20to%20systematically%0Agenerate%20meaningful%20features%20through%20an%20iterative%20process%2C%20effectively%0Areconstructing%20the%20feature%20space.%20This%20framework%20provides%20a%20principled%20solution%0Afor%20attributed%20graph%20feature%20space%20reconstruction.%20The%20extensive%20experiments%0Ademonstrate%20the%20effectiveness%20and%20efficiency%20of%20including%20topological%0Aawareness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05742v1&entry.124074799=Read"},
{"title": "GEPS: Boosting Generalization in Parametric PDE Neural Solvers through\n  Adaptive Conditioning", "author": "Armand Kassa\u00ef Koupa\u00ef and Jorge Mifsut Benet and Yuan Yin and Jean-No\u00ebl Vittaut and Patrick Gallinari", "abstract": "  Solving parametric partial differential equations (PDEs) presents significant\nchallenges for data-driven methods due to the sensitivity of spatio-temporal\ndynamics to variations in PDE parameters. Machine learning approaches often\nstruggle to capture this variability. To address this, data-driven approaches\nlearn parametric PDEs by sampling a very large variety of trajectories with\nvarying PDE parameters. We first show that incorporating conditioning\nmechanisms for learning parametric PDEs is essential and that among them,\n$\\textit{adaptive conditioning}$, allows stronger generalization. As existing\nadaptive conditioning methods do not scale well with respect to the number of\nparameters to adapt in the neural solver, we propose GEPS, a simple adaptation\nmechanism to boost GEneralization in Pde Solvers via a first-order optimization\nand low-rank rapid adaptation of a small set of context parameters. We\ndemonstrate the versatility of our approach for both fully data-driven and for\nphysics-aware neural solvers. Validation performed on a whole range of\nspatio-temporal forecasting problems demonstrates excellent performance for\ngeneralizing to unseen conditions including initial conditions, PDE\ncoefficients, forcing terms and solution domain. $\\textit{Project page}$:\nhttps://geps-project.github.io\n", "link": "http://arxiv.org/abs/2410.23889v2", "date": "2024-11-08", "relevancy": 2.074, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5242}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5224}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GEPS%3A%20Boosting%20Generalization%20in%20Parametric%20PDE%20Neural%20Solvers%20through%0A%20%20Adaptive%20Conditioning&body=Title%3A%20GEPS%3A%20Boosting%20Generalization%20in%20Parametric%20PDE%20Neural%20Solvers%20through%0A%20%20Adaptive%20Conditioning%0AAuthor%3A%20Armand%20Kassa%C3%AF%20Koupa%C3%AF%20and%20Jorge%20Mifsut%20Benet%20and%20Yuan%20Yin%20and%20Jean-No%C3%ABl%20Vittaut%20and%20Patrick%20Gallinari%0AAbstract%3A%20%20%20Solving%20parametric%20partial%20differential%20equations%20%28PDEs%29%20presents%20significant%0Achallenges%20for%20data-driven%20methods%20due%20to%20the%20sensitivity%20of%20spatio-temporal%0Adynamics%20to%20variations%20in%20PDE%20parameters.%20Machine%20learning%20approaches%20often%0Astruggle%20to%20capture%20this%20variability.%20To%20address%20this%2C%20data-driven%20approaches%0Alearn%20parametric%20PDEs%20by%20sampling%20a%20very%20large%20variety%20of%20trajectories%20with%0Avarying%20PDE%20parameters.%20We%20first%20show%20that%20incorporating%20conditioning%0Amechanisms%20for%20learning%20parametric%20PDEs%20is%20essential%20and%20that%20among%20them%2C%0A%24%5Ctextit%7Badaptive%20conditioning%7D%24%2C%20allows%20stronger%20generalization.%20As%20existing%0Aadaptive%20conditioning%20methods%20do%20not%20scale%20well%20with%20respect%20to%20the%20number%20of%0Aparameters%20to%20adapt%20in%20the%20neural%20solver%2C%20we%20propose%20GEPS%2C%20a%20simple%20adaptation%0Amechanism%20to%20boost%20GEneralization%20in%20Pde%20Solvers%20via%20a%20first-order%20optimization%0Aand%20low-rank%20rapid%20adaptation%20of%20a%20small%20set%20of%20context%20parameters.%20We%0Ademonstrate%20the%20versatility%20of%20our%20approach%20for%20both%20fully%20data-driven%20and%20for%0Aphysics-aware%20neural%20solvers.%20Validation%20performed%20on%20a%20whole%20range%20of%0Aspatio-temporal%20forecasting%20problems%20demonstrates%20excellent%20performance%20for%0Ageneralizing%20to%20unseen%20conditions%20including%20initial%20conditions%2C%20PDE%0Acoefficients%2C%20forcing%20terms%20and%20solution%20domain.%20%24%5Ctextit%7BProject%20page%7D%24%3A%0Ahttps%3A//geps-project.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23889v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGEPS%253A%2520Boosting%2520Generalization%2520in%2520Parametric%2520PDE%2520Neural%2520Solvers%2520through%250A%2520%2520Adaptive%2520Conditioning%26entry.906535625%3DArmand%2520Kassa%25C3%25AF%2520Koupa%25C3%25AF%2520and%2520Jorge%2520Mifsut%2520Benet%2520and%2520Yuan%2520Yin%2520and%2520Jean-No%25C3%25ABl%2520Vittaut%2520and%2520Patrick%2520Gallinari%26entry.1292438233%3D%2520%2520Solving%2520parametric%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520presents%2520significant%250Achallenges%2520for%2520data-driven%2520methods%2520due%2520to%2520the%2520sensitivity%2520of%2520spatio-temporal%250Adynamics%2520to%2520variations%2520in%2520PDE%2520parameters.%2520Machine%2520learning%2520approaches%2520often%250Astruggle%2520to%2520capture%2520this%2520variability.%2520To%2520address%2520this%252C%2520data-driven%2520approaches%250Alearn%2520parametric%2520PDEs%2520by%2520sampling%2520a%2520very%2520large%2520variety%2520of%2520trajectories%2520with%250Avarying%2520PDE%2520parameters.%2520We%2520first%2520show%2520that%2520incorporating%2520conditioning%250Amechanisms%2520for%2520learning%2520parametric%2520PDEs%2520is%2520essential%2520and%2520that%2520among%2520them%252C%250A%2524%255Ctextit%257Badaptive%2520conditioning%257D%2524%252C%2520allows%2520stronger%2520generalization.%2520As%2520existing%250Aadaptive%2520conditioning%2520methods%2520do%2520not%2520scale%2520well%2520with%2520respect%2520to%2520the%2520number%2520of%250Aparameters%2520to%2520adapt%2520in%2520the%2520neural%2520solver%252C%2520we%2520propose%2520GEPS%252C%2520a%2520simple%2520adaptation%250Amechanism%2520to%2520boost%2520GEneralization%2520in%2520Pde%2520Solvers%2520via%2520a%2520first-order%2520optimization%250Aand%2520low-rank%2520rapid%2520adaptation%2520of%2520a%2520small%2520set%2520of%2520context%2520parameters.%2520We%250Ademonstrate%2520the%2520versatility%2520of%2520our%2520approach%2520for%2520both%2520fully%2520data-driven%2520and%2520for%250Aphysics-aware%2520neural%2520solvers.%2520Validation%2520performed%2520on%2520a%2520whole%2520range%2520of%250Aspatio-temporal%2520forecasting%2520problems%2520demonstrates%2520excellent%2520performance%2520for%250Ageneralizing%2520to%2520unseen%2520conditions%2520including%2520initial%2520conditions%252C%2520PDE%250Acoefficients%252C%2520forcing%2520terms%2520and%2520solution%2520domain.%2520%2524%255Ctextit%257BProject%2520page%257D%2524%253A%250Ahttps%253A//geps-project.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23889v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GEPS%3A%20Boosting%20Generalization%20in%20Parametric%20PDE%20Neural%20Solvers%20through%0A%20%20Adaptive%20Conditioning&entry.906535625=Armand%20Kassa%C3%AF%20Koupa%C3%AF%20and%20Jorge%20Mifsut%20Benet%20and%20Yuan%20Yin%20and%20Jean-No%C3%ABl%20Vittaut%20and%20Patrick%20Gallinari&entry.1292438233=%20%20Solving%20parametric%20partial%20differential%20equations%20%28PDEs%29%20presents%20significant%0Achallenges%20for%20data-driven%20methods%20due%20to%20the%20sensitivity%20of%20spatio-temporal%0Adynamics%20to%20variations%20in%20PDE%20parameters.%20Machine%20learning%20approaches%20often%0Astruggle%20to%20capture%20this%20variability.%20To%20address%20this%2C%20data-driven%20approaches%0Alearn%20parametric%20PDEs%20by%20sampling%20a%20very%20large%20variety%20of%20trajectories%20with%0Avarying%20PDE%20parameters.%20We%20first%20show%20that%20incorporating%20conditioning%0Amechanisms%20for%20learning%20parametric%20PDEs%20is%20essential%20and%20that%20among%20them%2C%0A%24%5Ctextit%7Badaptive%20conditioning%7D%24%2C%20allows%20stronger%20generalization.%20As%20existing%0Aadaptive%20conditioning%20methods%20do%20not%20scale%20well%20with%20respect%20to%20the%20number%20of%0Aparameters%20to%20adapt%20in%20the%20neural%20solver%2C%20we%20propose%20GEPS%2C%20a%20simple%20adaptation%0Amechanism%20to%20boost%20GEneralization%20in%20Pde%20Solvers%20via%20a%20first-order%20optimization%0Aand%20low-rank%20rapid%20adaptation%20of%20a%20small%20set%20of%20context%20parameters.%20We%0Ademonstrate%20the%20versatility%20of%20our%20approach%20for%20both%20fully%20data-driven%20and%20for%0Aphysics-aware%20neural%20solvers.%20Validation%20performed%20on%20a%20whole%20range%20of%0Aspatio-temporal%20forecasting%20problems%20demonstrates%20excellent%20performance%20for%0Ageneralizing%20to%20unseen%20conditions%20including%20initial%20conditions%2C%20PDE%0Acoefficients%2C%20forcing%20terms%20and%20solution%20domain.%20%24%5Ctextit%7BProject%20page%7D%24%3A%0Ahttps%3A//geps-project.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23889v2&entry.124074799=Read"},
{"title": "ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles", "author": "Kayo Yin and Chinmay Singh and Fyodor O. Minakov and Vanessa Milan and Hal Daum\u00e9 III and Cyril Zhang and Alex X. Lu and Danielle Bragg", "abstract": "  Deaf and hard-of-hearing (DHH) students face significant barriers in\naccessing science, technology, engineering, and mathematics (STEM) education,\nnotably due to the scarcity of STEM resources in signed languages. To help\naddress this, we introduce ASL STEM Wiki: a parallel corpus of 254 Wikipedia\narticles on STEM topics in English, interpreted into over 300 hours of American\nSign Language (ASL). ASL STEM Wiki is the first continuous signing dataset\nfocused on STEM, facilitating the development of AI resources for STEM\neducation in ASL. We identify several use cases of ASL STEM Wiki with\nhuman-centered applications. For example, because this dataset highlights the\nfrequent use of fingerspelling for technical concepts, which inhibits DHH\nstudents' ability to learn, we develop models to identify fingerspelled words\n-- which can later be used to query for appropriate ASL signs to suggest to\ninterpreters.\n", "link": "http://arxiv.org/abs/2411.05783v1", "date": "2024-11-08", "relevancy": 2.0672, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4203}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4203}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ASL%20STEM%20Wiki%3A%20Dataset%20and%20Benchmark%20for%20Interpreting%20STEM%20Articles&body=Title%3A%20ASL%20STEM%20Wiki%3A%20Dataset%20and%20Benchmark%20for%20Interpreting%20STEM%20Articles%0AAuthor%3A%20Kayo%20Yin%20and%20Chinmay%20Singh%20and%20Fyodor%20O.%20Minakov%20and%20Vanessa%20Milan%20and%20Hal%20Daum%C3%A9%20III%20and%20Cyril%20Zhang%20and%20Alex%20X.%20Lu%20and%20Danielle%20Bragg%0AAbstract%3A%20%20%20Deaf%20and%20hard-of-hearing%20%28DHH%29%20students%20face%20significant%20barriers%20in%0Aaccessing%20science%2C%20technology%2C%20engineering%2C%20and%20mathematics%20%28STEM%29%20education%2C%0Anotably%20due%20to%20the%20scarcity%20of%20STEM%20resources%20in%20signed%20languages.%20To%20help%0Aaddress%20this%2C%20we%20introduce%20ASL%20STEM%20Wiki%3A%20a%20parallel%20corpus%20of%20254%20Wikipedia%0Aarticles%20on%20STEM%20topics%20in%20English%2C%20interpreted%20into%20over%20300%20hours%20of%20American%0ASign%20Language%20%28ASL%29.%20ASL%20STEM%20Wiki%20is%20the%20first%20continuous%20signing%20dataset%0Afocused%20on%20STEM%2C%20facilitating%20the%20development%20of%20AI%20resources%20for%20STEM%0Aeducation%20in%20ASL.%20We%20identify%20several%20use%20cases%20of%20ASL%20STEM%20Wiki%20with%0Ahuman-centered%20applications.%20For%20example%2C%20because%20this%20dataset%20highlights%20the%0Afrequent%20use%20of%20fingerspelling%20for%20technical%20concepts%2C%20which%20inhibits%20DHH%0Astudents%27%20ability%20to%20learn%2C%20we%20develop%20models%20to%20identify%20fingerspelled%20words%0A--%20which%20can%20later%20be%20used%20to%20query%20for%20appropriate%20ASL%20signs%20to%20suggest%20to%0Ainterpreters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DASL%2520STEM%2520Wiki%253A%2520Dataset%2520and%2520Benchmark%2520for%2520Interpreting%2520STEM%2520Articles%26entry.906535625%3DKayo%2520Yin%2520and%2520Chinmay%2520Singh%2520and%2520Fyodor%2520O.%2520Minakov%2520and%2520Vanessa%2520Milan%2520and%2520Hal%2520Daum%25C3%25A9%2520III%2520and%2520Cyril%2520Zhang%2520and%2520Alex%2520X.%2520Lu%2520and%2520Danielle%2520Bragg%26entry.1292438233%3D%2520%2520Deaf%2520and%2520hard-of-hearing%2520%2528DHH%2529%2520students%2520face%2520significant%2520barriers%2520in%250Aaccessing%2520science%252C%2520technology%252C%2520engineering%252C%2520and%2520mathematics%2520%2528STEM%2529%2520education%252C%250Anotably%2520due%2520to%2520the%2520scarcity%2520of%2520STEM%2520resources%2520in%2520signed%2520languages.%2520To%2520help%250Aaddress%2520this%252C%2520we%2520introduce%2520ASL%2520STEM%2520Wiki%253A%2520a%2520parallel%2520corpus%2520of%2520254%2520Wikipedia%250Aarticles%2520on%2520STEM%2520topics%2520in%2520English%252C%2520interpreted%2520into%2520over%2520300%2520hours%2520of%2520American%250ASign%2520Language%2520%2528ASL%2529.%2520ASL%2520STEM%2520Wiki%2520is%2520the%2520first%2520continuous%2520signing%2520dataset%250Afocused%2520on%2520STEM%252C%2520facilitating%2520the%2520development%2520of%2520AI%2520resources%2520for%2520STEM%250Aeducation%2520in%2520ASL.%2520We%2520identify%2520several%2520use%2520cases%2520of%2520ASL%2520STEM%2520Wiki%2520with%250Ahuman-centered%2520applications.%2520For%2520example%252C%2520because%2520this%2520dataset%2520highlights%2520the%250Afrequent%2520use%2520of%2520fingerspelling%2520for%2520technical%2520concepts%252C%2520which%2520inhibits%2520DHH%250Astudents%2527%2520ability%2520to%2520learn%252C%2520we%2520develop%2520models%2520to%2520identify%2520fingerspelled%2520words%250A--%2520which%2520can%2520later%2520be%2520used%2520to%2520query%2520for%2520appropriate%2520ASL%2520signs%2520to%2520suggest%2520to%250Ainterpreters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ASL%20STEM%20Wiki%3A%20Dataset%20and%20Benchmark%20for%20Interpreting%20STEM%20Articles&entry.906535625=Kayo%20Yin%20and%20Chinmay%20Singh%20and%20Fyodor%20O.%20Minakov%20and%20Vanessa%20Milan%20and%20Hal%20Daum%C3%A9%20III%20and%20Cyril%20Zhang%20and%20Alex%20X.%20Lu%20and%20Danielle%20Bragg&entry.1292438233=%20%20Deaf%20and%20hard-of-hearing%20%28DHH%29%20students%20face%20significant%20barriers%20in%0Aaccessing%20science%2C%20technology%2C%20engineering%2C%20and%20mathematics%20%28STEM%29%20education%2C%0Anotably%20due%20to%20the%20scarcity%20of%20STEM%20resources%20in%20signed%20languages.%20To%20help%0Aaddress%20this%2C%20we%20introduce%20ASL%20STEM%20Wiki%3A%20a%20parallel%20corpus%20of%20254%20Wikipedia%0Aarticles%20on%20STEM%20topics%20in%20English%2C%20interpreted%20into%20over%20300%20hours%20of%20American%0ASign%20Language%20%28ASL%29.%20ASL%20STEM%20Wiki%20is%20the%20first%20continuous%20signing%20dataset%0Afocused%20on%20STEM%2C%20facilitating%20the%20development%20of%20AI%20resources%20for%20STEM%0Aeducation%20in%20ASL.%20We%20identify%20several%20use%20cases%20of%20ASL%20STEM%20Wiki%20with%0Ahuman-centered%20applications.%20For%20example%2C%20because%20this%20dataset%20highlights%20the%0Afrequent%20use%20of%20fingerspelling%20for%20technical%20concepts%2C%20which%20inhibits%20DHH%0Astudents%27%20ability%20to%20learn%2C%20we%20develop%20models%20to%20identify%20fingerspelled%20words%0A--%20which%20can%20later%20be%20used%20to%20query%20for%20appropriate%20ASL%20signs%20to%20suggest%20to%0Ainterpreters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05783v1&entry.124074799=Read"},
{"title": "SynDroneVision: A Synthetic Dataset for Image-Based Drone Detection", "author": "Tamara R. Lenhard and Andreas Weinmann and Kai Franke and Tobias Koch", "abstract": "  Developing robust drone detection systems is often constrained by the limited\navailability of large-scale annotated training data and the high costs\nassociated with real-world data collection. However, leveraging synthetic data\ngenerated via game engine-based simulations provides a promising and\ncost-effective solution to overcome this issue. Therefore, we present\nSynDroneVision, a synthetic dataset specifically designed for RGB-based drone\ndetection in surveillance applications. Featuring diverse backgrounds, lighting\nconditions, and drone models, SynDroneVision offers a comprehensive training\nfoundation for deep learning algorithms. To evaluate the dataset's\neffectiveness, we perform a comparative analysis across a selection of recent\nYOLO detection models. Our findings demonstrate that SynDroneVision is a\nvaluable resource for real-world data enrichment, achieving notable\nenhancements in model performance and robustness, while significantly reducing\nthe time and costs of real-world data acquisition. SynDroneVision will be\npublicly released upon paper acceptance.\n", "link": "http://arxiv.org/abs/2411.05633v1", "date": "2024-11-08", "relevancy": 2.0627, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5185}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5185}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynDroneVision%3A%20A%20Synthetic%20Dataset%20for%20Image-Based%20Drone%20Detection&body=Title%3A%20SynDroneVision%3A%20A%20Synthetic%20Dataset%20for%20Image-Based%20Drone%20Detection%0AAuthor%3A%20Tamara%20R.%20Lenhard%20and%20Andreas%20Weinmann%20and%20Kai%20Franke%20and%20Tobias%20Koch%0AAbstract%3A%20%20%20Developing%20robust%20drone%20detection%20systems%20is%20often%20constrained%20by%20the%20limited%0Aavailability%20of%20large-scale%20annotated%20training%20data%20and%20the%20high%20costs%0Aassociated%20with%20real-world%20data%20collection.%20However%2C%20leveraging%20synthetic%20data%0Agenerated%20via%20game%20engine-based%20simulations%20provides%20a%20promising%20and%0Acost-effective%20solution%20to%20overcome%20this%20issue.%20Therefore%2C%20we%20present%0ASynDroneVision%2C%20a%20synthetic%20dataset%20specifically%20designed%20for%20RGB-based%20drone%0Adetection%20in%20surveillance%20applications.%20Featuring%20diverse%20backgrounds%2C%20lighting%0Aconditions%2C%20and%20drone%20models%2C%20SynDroneVision%20offers%20a%20comprehensive%20training%0Afoundation%20for%20deep%20learning%20algorithms.%20To%20evaluate%20the%20dataset%27s%0Aeffectiveness%2C%20we%20perform%20a%20comparative%20analysis%20across%20a%20selection%20of%20recent%0AYOLO%20detection%20models.%20Our%20findings%20demonstrate%20that%20SynDroneVision%20is%20a%0Avaluable%20resource%20for%20real-world%20data%20enrichment%2C%20achieving%20notable%0Aenhancements%20in%20model%20performance%20and%20robustness%2C%20while%20significantly%20reducing%0Athe%20time%20and%20costs%20of%20real-world%20data%20acquisition.%20SynDroneVision%20will%20be%0Apublicly%20released%20upon%20paper%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynDroneVision%253A%2520A%2520Synthetic%2520Dataset%2520for%2520Image-Based%2520Drone%2520Detection%26entry.906535625%3DTamara%2520R.%2520Lenhard%2520and%2520Andreas%2520Weinmann%2520and%2520Kai%2520Franke%2520and%2520Tobias%2520Koch%26entry.1292438233%3D%2520%2520Developing%2520robust%2520drone%2520detection%2520systems%2520is%2520often%2520constrained%2520by%2520the%2520limited%250Aavailability%2520of%2520large-scale%2520annotated%2520training%2520data%2520and%2520the%2520high%2520costs%250Aassociated%2520with%2520real-world%2520data%2520collection.%2520However%252C%2520leveraging%2520synthetic%2520data%250Agenerated%2520via%2520game%2520engine-based%2520simulations%2520provides%2520a%2520promising%2520and%250Acost-effective%2520solution%2520to%2520overcome%2520this%2520issue.%2520Therefore%252C%2520we%2520present%250ASynDroneVision%252C%2520a%2520synthetic%2520dataset%2520specifically%2520designed%2520for%2520RGB-based%2520drone%250Adetection%2520in%2520surveillance%2520applications.%2520Featuring%2520diverse%2520backgrounds%252C%2520lighting%250Aconditions%252C%2520and%2520drone%2520models%252C%2520SynDroneVision%2520offers%2520a%2520comprehensive%2520training%250Afoundation%2520for%2520deep%2520learning%2520algorithms.%2520To%2520evaluate%2520the%2520dataset%2527s%250Aeffectiveness%252C%2520we%2520perform%2520a%2520comparative%2520analysis%2520across%2520a%2520selection%2520of%2520recent%250AYOLO%2520detection%2520models.%2520Our%2520findings%2520demonstrate%2520that%2520SynDroneVision%2520is%2520a%250Avaluable%2520resource%2520for%2520real-world%2520data%2520enrichment%252C%2520achieving%2520notable%250Aenhancements%2520in%2520model%2520performance%2520and%2520robustness%252C%2520while%2520significantly%2520reducing%250Athe%2520time%2520and%2520costs%2520of%2520real-world%2520data%2520acquisition.%2520SynDroneVision%2520will%2520be%250Apublicly%2520released%2520upon%2520paper%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynDroneVision%3A%20A%20Synthetic%20Dataset%20for%20Image-Based%20Drone%20Detection&entry.906535625=Tamara%20R.%20Lenhard%20and%20Andreas%20Weinmann%20and%20Kai%20Franke%20and%20Tobias%20Koch&entry.1292438233=%20%20Developing%20robust%20drone%20detection%20systems%20is%20often%20constrained%20by%20the%20limited%0Aavailability%20of%20large-scale%20annotated%20training%20data%20and%20the%20high%20costs%0Aassociated%20with%20real-world%20data%20collection.%20However%2C%20leveraging%20synthetic%20data%0Agenerated%20via%20game%20engine-based%20simulations%20provides%20a%20promising%20and%0Acost-effective%20solution%20to%20overcome%20this%20issue.%20Therefore%2C%20we%20present%0ASynDroneVision%2C%20a%20synthetic%20dataset%20specifically%20designed%20for%20RGB-based%20drone%0Adetection%20in%20surveillance%20applications.%20Featuring%20diverse%20backgrounds%2C%20lighting%0Aconditions%2C%20and%20drone%20models%2C%20SynDroneVision%20offers%20a%20comprehensive%20training%0Afoundation%20for%20deep%20learning%20algorithms.%20To%20evaluate%20the%20dataset%27s%0Aeffectiveness%2C%20we%20perform%20a%20comparative%20analysis%20across%20a%20selection%20of%20recent%0AYOLO%20detection%20models.%20Our%20findings%20demonstrate%20that%20SynDroneVision%20is%20a%0Avaluable%20resource%20for%20real-world%20data%20enrichment%2C%20achieving%20notable%0Aenhancements%20in%20model%20performance%20and%20robustness%2C%20while%20significantly%20reducing%0Athe%20time%20and%20costs%20of%20real-world%20data%20acquisition.%20SynDroneVision%20will%20be%0Apublicly%20released%20upon%20paper%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05633v1&entry.124074799=Read"},
{"title": "LLMs as Method Actors: A Model for Prompt Engineering and Architecture", "author": "Colin Doyle", "abstract": "  We introduce \"Method Actors\" as a mental model for guiding LLM prompt\nengineering and prompt architecture. Under this mental model, LLMs should be\nthought of as actors; prompts as scripts and cues; and LLM responses as\nperformances. We apply this mental model to the task of improving LLM\nperformance at playing Connections, a New York Times word puzzle game that\nprior research identified as a challenging benchmark for evaluating LLM\nreasoning. Our experiments with GPT-4o show that a \"Method Actors\" approach can\nsignificantly improve LLM performance over both a vanilla and \"Chain of\nThoughts\" approach. A vanilla approach solves 27% of Connections puzzles in our\ndataset and a \"Chain of Thoughts\" approach solves 41% of puzzles, whereas our\nstrongest \"Method Actor\" approach solves 86% of puzzles. We also test OpenAI's\nnewest model designed specifically for complex reasoning tasks, o1-preview.\nWhen asked to solve a puzzle all at once, o1-preview solves 79% of Connections\npuzzles in our dataset, and when allowed to build puzzle solutions one guess at\na time over multiple API calls, o1-preview solves 100% of the puzzles.\nIncorporating a \"Method Actor\" prompt architecture increases the percentage of\npuzzles that o1-preview solves perfectly from 76% to 87%.\n", "link": "http://arxiv.org/abs/2411.05778v1", "date": "2024-11-08", "relevancy": 2.0551, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5184}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5184}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMs%20as%20Method%20Actors%3A%20A%20Model%20for%20Prompt%20Engineering%20and%20Architecture&body=Title%3A%20LLMs%20as%20Method%20Actors%3A%20A%20Model%20for%20Prompt%20Engineering%20and%20Architecture%0AAuthor%3A%20Colin%20Doyle%0AAbstract%3A%20%20%20We%20introduce%20%22Method%20Actors%22%20as%20a%20mental%20model%20for%20guiding%20LLM%20prompt%0Aengineering%20and%20prompt%20architecture.%20Under%20this%20mental%20model%2C%20LLMs%20should%20be%0Athought%20of%20as%20actors%3B%20prompts%20as%20scripts%20and%20cues%3B%20and%20LLM%20responses%20as%0Aperformances.%20We%20apply%20this%20mental%20model%20to%20the%20task%20of%20improving%20LLM%0Aperformance%20at%20playing%20Connections%2C%20a%20New%20York%20Times%20word%20puzzle%20game%20that%0Aprior%20research%20identified%20as%20a%20challenging%20benchmark%20for%20evaluating%20LLM%0Areasoning.%20Our%20experiments%20with%20GPT-4o%20show%20that%20a%20%22Method%20Actors%22%20approach%20can%0Asignificantly%20improve%20LLM%20performance%20over%20both%20a%20vanilla%20and%20%22Chain%20of%0AThoughts%22%20approach.%20A%20vanilla%20approach%20solves%2027%25%20of%20Connections%20puzzles%20in%20our%0Adataset%20and%20a%20%22Chain%20of%20Thoughts%22%20approach%20solves%2041%25%20of%20puzzles%2C%20whereas%20our%0Astrongest%20%22Method%20Actor%22%20approach%20solves%2086%25%20of%20puzzles.%20We%20also%20test%20OpenAI%27s%0Anewest%20model%20designed%20specifically%20for%20complex%20reasoning%20tasks%2C%20o1-preview.%0AWhen%20asked%20to%20solve%20a%20puzzle%20all%20at%20once%2C%20o1-preview%20solves%2079%25%20of%20Connections%0Apuzzles%20in%20our%20dataset%2C%20and%20when%20allowed%20to%20build%20puzzle%20solutions%20one%20guess%20at%0Aa%20time%20over%20multiple%20API%20calls%2C%20o1-preview%20solves%20100%25%20of%20the%20puzzles.%0AIncorporating%20a%20%22Method%20Actor%22%20prompt%20architecture%20increases%20the%20percentage%20of%0Apuzzles%20that%20o1-preview%20solves%20perfectly%20from%2076%25%20to%2087%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMs%2520as%2520Method%2520Actors%253A%2520A%2520Model%2520for%2520Prompt%2520Engineering%2520and%2520Architecture%26entry.906535625%3DColin%2520Doyle%26entry.1292438233%3D%2520%2520We%2520introduce%2520%2522Method%2520Actors%2522%2520as%2520a%2520mental%2520model%2520for%2520guiding%2520LLM%2520prompt%250Aengineering%2520and%2520prompt%2520architecture.%2520Under%2520this%2520mental%2520model%252C%2520LLMs%2520should%2520be%250Athought%2520of%2520as%2520actors%253B%2520prompts%2520as%2520scripts%2520and%2520cues%253B%2520and%2520LLM%2520responses%2520as%250Aperformances.%2520We%2520apply%2520this%2520mental%2520model%2520to%2520the%2520task%2520of%2520improving%2520LLM%250Aperformance%2520at%2520playing%2520Connections%252C%2520a%2520New%2520York%2520Times%2520word%2520puzzle%2520game%2520that%250Aprior%2520research%2520identified%2520as%2520a%2520challenging%2520benchmark%2520for%2520evaluating%2520LLM%250Areasoning.%2520Our%2520experiments%2520with%2520GPT-4o%2520show%2520that%2520a%2520%2522Method%2520Actors%2522%2520approach%2520can%250Asignificantly%2520improve%2520LLM%2520performance%2520over%2520both%2520a%2520vanilla%2520and%2520%2522Chain%2520of%250AThoughts%2522%2520approach.%2520A%2520vanilla%2520approach%2520solves%252027%2525%2520of%2520Connections%2520puzzles%2520in%2520our%250Adataset%2520and%2520a%2520%2522Chain%2520of%2520Thoughts%2522%2520approach%2520solves%252041%2525%2520of%2520puzzles%252C%2520whereas%2520our%250Astrongest%2520%2522Method%2520Actor%2522%2520approach%2520solves%252086%2525%2520of%2520puzzles.%2520We%2520also%2520test%2520OpenAI%2527s%250Anewest%2520model%2520designed%2520specifically%2520for%2520complex%2520reasoning%2520tasks%252C%2520o1-preview.%250AWhen%2520asked%2520to%2520solve%2520a%2520puzzle%2520all%2520at%2520once%252C%2520o1-preview%2520solves%252079%2525%2520of%2520Connections%250Apuzzles%2520in%2520our%2520dataset%252C%2520and%2520when%2520allowed%2520to%2520build%2520puzzle%2520solutions%2520one%2520guess%2520at%250Aa%2520time%2520over%2520multiple%2520API%2520calls%252C%2520o1-preview%2520solves%2520100%2525%2520of%2520the%2520puzzles.%250AIncorporating%2520a%2520%2522Method%2520Actor%2522%2520prompt%2520architecture%2520increases%2520the%2520percentage%2520of%250Apuzzles%2520that%2520o1-preview%2520solves%2520perfectly%2520from%252076%2525%2520to%252087%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMs%20as%20Method%20Actors%3A%20A%20Model%20for%20Prompt%20Engineering%20and%20Architecture&entry.906535625=Colin%20Doyle&entry.1292438233=%20%20We%20introduce%20%22Method%20Actors%22%20as%20a%20mental%20model%20for%20guiding%20LLM%20prompt%0Aengineering%20and%20prompt%20architecture.%20Under%20this%20mental%20model%2C%20LLMs%20should%20be%0Athought%20of%20as%20actors%3B%20prompts%20as%20scripts%20and%20cues%3B%20and%20LLM%20responses%20as%0Aperformances.%20We%20apply%20this%20mental%20model%20to%20the%20task%20of%20improving%20LLM%0Aperformance%20at%20playing%20Connections%2C%20a%20New%20York%20Times%20word%20puzzle%20game%20that%0Aprior%20research%20identified%20as%20a%20challenging%20benchmark%20for%20evaluating%20LLM%0Areasoning.%20Our%20experiments%20with%20GPT-4o%20show%20that%20a%20%22Method%20Actors%22%20approach%20can%0Asignificantly%20improve%20LLM%20performance%20over%20both%20a%20vanilla%20and%20%22Chain%20of%0AThoughts%22%20approach.%20A%20vanilla%20approach%20solves%2027%25%20of%20Connections%20puzzles%20in%20our%0Adataset%20and%20a%20%22Chain%20of%20Thoughts%22%20approach%20solves%2041%25%20of%20puzzles%2C%20whereas%20our%0Astrongest%20%22Method%20Actor%22%20approach%20solves%2086%25%20of%20puzzles.%20We%20also%20test%20OpenAI%27s%0Anewest%20model%20designed%20specifically%20for%20complex%20reasoning%20tasks%2C%20o1-preview.%0AWhen%20asked%20to%20solve%20a%20puzzle%20all%20at%20once%2C%20o1-preview%20solves%2079%25%20of%20Connections%0Apuzzles%20in%20our%20dataset%2C%20and%20when%20allowed%20to%20build%20puzzle%20solutions%20one%20guess%20at%0Aa%20time%20over%20multiple%20API%20calls%2C%20o1-preview%20solves%20100%25%20of%20the%20puzzles.%0AIncorporating%20a%20%22Method%20Actor%22%20prompt%20architecture%20increases%20the%20percentage%20of%0Apuzzles%20that%20o1-preview%20solves%20perfectly%20from%2076%25%20to%2087%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05778v1&entry.124074799=Read"},
{"title": "Fast training and sampling of Restricted Boltzmann Machines", "author": "Nicolas B\u00e9reux and Aur\u00e9lien Decelle and Cyril Furtlehner and Lorenzo Rosset and Beatriz Seoane", "abstract": "  Restricted Boltzmann Machines (RBMs) are effective tools for modeling complex\nsystems and deriving insights from data. However, training these models with\nhighly structured data presents significant challenges due to the slow mixing\ncharacteristics of Markov Chain Monte Carlo processes. In this study, we build\nupon recent theoretical advancements in RBM training, to significantly reduce\nthe computational cost of training (in very clustered datasets), evaluating and\nsampling in RBMs in general. The learning process is analogous to thermodynamic\ncontinuous phase transitions observed in ferromagnetic models, where new modes\nin the probability measure emerge in a continuous manner. Such continuous\ntransitions are associated with the critical slowdown effect, which adversely\naffects the accuracy of gradient estimates, particularly during the initial\nstages of training with clustered data. To mitigate this issue, we propose a\npre-training phase that encodes the principal components into a low-rank RBM\nthrough a convex optimization process. This approach enables efficient static\nMonte Carlo sampling and accurate computation of the partition function. We\nexploit the continuous and smooth nature of the parameter annealing trajectory\nto achieve reliable and computationally efficient log-likelihood estimations,\nenabling online assessment during the training, and propose a novel sampling\nstrategy named parallel trajectory tempering (PTT) which outperforms previously\noptimized MCMC methods. Our results show that this training strategy enables\nRBMs to effectively address highly structured datasets that conventional\nmethods struggle with. We also provide evidence that our log-likelihood\nestimation is more accurate than traditional, more computationally intensive\napproaches in controlled scenarios. The PTT algorithm significantly accelerates\nMCMC processes compared to existing and conventional methods.\n", "link": "http://arxiv.org/abs/2405.15376v2", "date": "2024-11-08", "relevancy": 2.0359, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5331}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5134}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4831}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20training%20and%20sampling%20of%20Restricted%20Boltzmann%20Machines&body=Title%3A%20Fast%20training%20and%20sampling%20of%20Restricted%20Boltzmann%20Machines%0AAuthor%3A%20Nicolas%20B%C3%A9reux%20and%20Aur%C3%A9lien%20Decelle%20and%20Cyril%20Furtlehner%20and%20Lorenzo%20Rosset%20and%20Beatriz%20Seoane%0AAbstract%3A%20%20%20Restricted%20Boltzmann%20Machines%20%28RBMs%29%20are%20effective%20tools%20for%20modeling%20complex%0Asystems%20and%20deriving%20insights%20from%20data.%20However%2C%20training%20these%20models%20with%0Ahighly%20structured%20data%20presents%20significant%20challenges%20due%20to%20the%20slow%20mixing%0Acharacteristics%20of%20Markov%20Chain%20Monte%20Carlo%20processes.%20In%20this%20study%2C%20we%20build%0Aupon%20recent%20theoretical%20advancements%20in%20RBM%20training%2C%20to%20significantly%20reduce%0Athe%20computational%20cost%20of%20training%20%28in%20very%20clustered%20datasets%29%2C%20evaluating%20and%0Asampling%20in%20RBMs%20in%20general.%20The%20learning%20process%20is%20analogous%20to%20thermodynamic%0Acontinuous%20phase%20transitions%20observed%20in%20ferromagnetic%20models%2C%20where%20new%20modes%0Ain%20the%20probability%20measure%20emerge%20in%20a%20continuous%20manner.%20Such%20continuous%0Atransitions%20are%20associated%20with%20the%20critical%20slowdown%20effect%2C%20which%20adversely%0Aaffects%20the%20accuracy%20of%20gradient%20estimates%2C%20particularly%20during%20the%20initial%0Astages%20of%20training%20with%20clustered%20data.%20To%20mitigate%20this%20issue%2C%20we%20propose%20a%0Apre-training%20phase%20that%20encodes%20the%20principal%20components%20into%20a%20low-rank%20RBM%0Athrough%20a%20convex%20optimization%20process.%20This%20approach%20enables%20efficient%20static%0AMonte%20Carlo%20sampling%20and%20accurate%20computation%20of%20the%20partition%20function.%20We%0Aexploit%20the%20continuous%20and%20smooth%20nature%20of%20the%20parameter%20annealing%20trajectory%0Ato%20achieve%20reliable%20and%20computationally%20efficient%20log-likelihood%20estimations%2C%0Aenabling%20online%20assessment%20during%20the%20training%2C%20and%20propose%20a%20novel%20sampling%0Astrategy%20named%20parallel%20trajectory%20tempering%20%28PTT%29%20which%20outperforms%20previously%0Aoptimized%20MCMC%20methods.%20Our%20results%20show%20that%20this%20training%20strategy%20enables%0ARBMs%20to%20effectively%20address%20highly%20structured%20datasets%20that%20conventional%0Amethods%20struggle%20with.%20We%20also%20provide%20evidence%20that%20our%20log-likelihood%0Aestimation%20is%20more%20accurate%20than%20traditional%2C%20more%20computationally%20intensive%0Aapproaches%20in%20controlled%20scenarios.%20The%20PTT%20algorithm%20significantly%20accelerates%0AMCMC%20processes%20compared%20to%20existing%20and%20conventional%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15376v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520training%2520and%2520sampling%2520of%2520Restricted%2520Boltzmann%2520Machines%26entry.906535625%3DNicolas%2520B%25C3%25A9reux%2520and%2520Aur%25C3%25A9lien%2520Decelle%2520and%2520Cyril%2520Furtlehner%2520and%2520Lorenzo%2520Rosset%2520and%2520Beatriz%2520Seoane%26entry.1292438233%3D%2520%2520Restricted%2520Boltzmann%2520Machines%2520%2528RBMs%2529%2520are%2520effective%2520tools%2520for%2520modeling%2520complex%250Asystems%2520and%2520deriving%2520insights%2520from%2520data.%2520However%252C%2520training%2520these%2520models%2520with%250Ahighly%2520structured%2520data%2520presents%2520significant%2520challenges%2520due%2520to%2520the%2520slow%2520mixing%250Acharacteristics%2520of%2520Markov%2520Chain%2520Monte%2520Carlo%2520processes.%2520In%2520this%2520study%252C%2520we%2520build%250Aupon%2520recent%2520theoretical%2520advancements%2520in%2520RBM%2520training%252C%2520to%2520significantly%2520reduce%250Athe%2520computational%2520cost%2520of%2520training%2520%2528in%2520very%2520clustered%2520datasets%2529%252C%2520evaluating%2520and%250Asampling%2520in%2520RBMs%2520in%2520general.%2520The%2520learning%2520process%2520is%2520analogous%2520to%2520thermodynamic%250Acontinuous%2520phase%2520transitions%2520observed%2520in%2520ferromagnetic%2520models%252C%2520where%2520new%2520modes%250Ain%2520the%2520probability%2520measure%2520emerge%2520in%2520a%2520continuous%2520manner.%2520Such%2520continuous%250Atransitions%2520are%2520associated%2520with%2520the%2520critical%2520slowdown%2520effect%252C%2520which%2520adversely%250Aaffects%2520the%2520accuracy%2520of%2520gradient%2520estimates%252C%2520particularly%2520during%2520the%2520initial%250Astages%2520of%2520training%2520with%2520clustered%2520data.%2520To%2520mitigate%2520this%2520issue%252C%2520we%2520propose%2520a%250Apre-training%2520phase%2520that%2520encodes%2520the%2520principal%2520components%2520into%2520a%2520low-rank%2520RBM%250Athrough%2520a%2520convex%2520optimization%2520process.%2520This%2520approach%2520enables%2520efficient%2520static%250AMonte%2520Carlo%2520sampling%2520and%2520accurate%2520computation%2520of%2520the%2520partition%2520function.%2520We%250Aexploit%2520the%2520continuous%2520and%2520smooth%2520nature%2520of%2520the%2520parameter%2520annealing%2520trajectory%250Ato%2520achieve%2520reliable%2520and%2520computationally%2520efficient%2520log-likelihood%2520estimations%252C%250Aenabling%2520online%2520assessment%2520during%2520the%2520training%252C%2520and%2520propose%2520a%2520novel%2520sampling%250Astrategy%2520named%2520parallel%2520trajectory%2520tempering%2520%2528PTT%2529%2520which%2520outperforms%2520previously%250Aoptimized%2520MCMC%2520methods.%2520Our%2520results%2520show%2520that%2520this%2520training%2520strategy%2520enables%250ARBMs%2520to%2520effectively%2520address%2520highly%2520structured%2520datasets%2520that%2520conventional%250Amethods%2520struggle%2520with.%2520We%2520also%2520provide%2520evidence%2520that%2520our%2520log-likelihood%250Aestimation%2520is%2520more%2520accurate%2520than%2520traditional%252C%2520more%2520computationally%2520intensive%250Aapproaches%2520in%2520controlled%2520scenarios.%2520The%2520PTT%2520algorithm%2520significantly%2520accelerates%250AMCMC%2520processes%2520compared%2520to%2520existing%2520and%2520conventional%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15376v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20training%20and%20sampling%20of%20Restricted%20Boltzmann%20Machines&entry.906535625=Nicolas%20B%C3%A9reux%20and%20Aur%C3%A9lien%20Decelle%20and%20Cyril%20Furtlehner%20and%20Lorenzo%20Rosset%20and%20Beatriz%20Seoane&entry.1292438233=%20%20Restricted%20Boltzmann%20Machines%20%28RBMs%29%20are%20effective%20tools%20for%20modeling%20complex%0Asystems%20and%20deriving%20insights%20from%20data.%20However%2C%20training%20these%20models%20with%0Ahighly%20structured%20data%20presents%20significant%20challenges%20due%20to%20the%20slow%20mixing%0Acharacteristics%20of%20Markov%20Chain%20Monte%20Carlo%20processes.%20In%20this%20study%2C%20we%20build%0Aupon%20recent%20theoretical%20advancements%20in%20RBM%20training%2C%20to%20significantly%20reduce%0Athe%20computational%20cost%20of%20training%20%28in%20very%20clustered%20datasets%29%2C%20evaluating%20and%0Asampling%20in%20RBMs%20in%20general.%20The%20learning%20process%20is%20analogous%20to%20thermodynamic%0Acontinuous%20phase%20transitions%20observed%20in%20ferromagnetic%20models%2C%20where%20new%20modes%0Ain%20the%20probability%20measure%20emerge%20in%20a%20continuous%20manner.%20Such%20continuous%0Atransitions%20are%20associated%20with%20the%20critical%20slowdown%20effect%2C%20which%20adversely%0Aaffects%20the%20accuracy%20of%20gradient%20estimates%2C%20particularly%20during%20the%20initial%0Astages%20of%20training%20with%20clustered%20data.%20To%20mitigate%20this%20issue%2C%20we%20propose%20a%0Apre-training%20phase%20that%20encodes%20the%20principal%20components%20into%20a%20low-rank%20RBM%0Athrough%20a%20convex%20optimization%20process.%20This%20approach%20enables%20efficient%20static%0AMonte%20Carlo%20sampling%20and%20accurate%20computation%20of%20the%20partition%20function.%20We%0Aexploit%20the%20continuous%20and%20smooth%20nature%20of%20the%20parameter%20annealing%20trajectory%0Ato%20achieve%20reliable%20and%20computationally%20efficient%20log-likelihood%20estimations%2C%0Aenabling%20online%20assessment%20during%20the%20training%2C%20and%20propose%20a%20novel%20sampling%0Astrategy%20named%20parallel%20trajectory%20tempering%20%28PTT%29%20which%20outperforms%20previously%0Aoptimized%20MCMC%20methods.%20Our%20results%20show%20that%20this%20training%20strategy%20enables%0ARBMs%20to%20effectively%20address%20highly%20structured%20datasets%20that%20conventional%0Amethods%20struggle%20with.%20We%20also%20provide%20evidence%20that%20our%20log-likelihood%0Aestimation%20is%20more%20accurate%20than%20traditional%2C%20more%20computationally%20intensive%0Aapproaches%20in%20controlled%20scenarios.%20The%20PTT%20algorithm%20significantly%20accelerates%0AMCMC%20processes%20compared%20to%20existing%20and%20conventional%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15376v2&entry.124074799=Read"},
{"title": "Lung tumor segmentation in MRI mice scans using 3D nnU-Net with minimum\n  annotations", "author": "Piotr Kaniewski and Fariba Yousefi and Yeman Brhane Hagos and Talha Qaiser and Nikolay Burlutskiy", "abstract": "  In drug discovery, accurate lung tumor segmentation is an important step for\nassessing tumor size and its progression using \\textit{in-vivo} imaging such as\nMRI. While deep learning models have been developed to automate this process,\nthe focus has predominantly been on human subjects, neglecting the pivotal role\nof animal models in pre-clinical drug development. In this work, we focus on\noptimizing lung tumor segmentation in mice. First, we demonstrate that the\nnnU-Net model outperforms the U-Net, U-Net3+, and DeepMeta models. Most\nimportantly, we achieve better results with nnU-Net 3D models than 2D models,\nindicating the importance of spatial context for segmentation tasks in MRI mice\nscans. This study demonstrates the importance of 3D input over 2D input images\nfor lung tumor segmentation in MRI scans. Finally, we outperform the prior\nstate-of-the-art approach that involves the combined segmentation of lungs and\ntumors within the lungs. Our work achieves comparable results using only lung\ntumor annotations requiring fewer annotations, saving time and annotation\nefforts. This work\n(https://anonymous.4open.science/r/lung-tumour-mice-mri-64BB) is an important\nstep in automating pre-clinical animal studies to quantify the efficacy of\nexperimental drugs, particularly in assessing tumor changes.\n", "link": "http://arxiv.org/abs/2411.00922v2", "date": "2024-11-08", "relevancy": 2.0341, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5783}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4983}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lung%20tumor%20segmentation%20in%20MRI%20mice%20scans%20using%203D%20nnU-Net%20with%20minimum%0A%20%20annotations&body=Title%3A%20Lung%20tumor%20segmentation%20in%20MRI%20mice%20scans%20using%203D%20nnU-Net%20with%20minimum%0A%20%20annotations%0AAuthor%3A%20Piotr%20Kaniewski%20and%20Fariba%20Yousefi%20and%20Yeman%20Brhane%20Hagos%20and%20Talha%20Qaiser%20and%20Nikolay%20Burlutskiy%0AAbstract%3A%20%20%20In%20drug%20discovery%2C%20accurate%20lung%20tumor%20segmentation%20is%20an%20important%20step%20for%0Aassessing%20tumor%20size%20and%20its%20progression%20using%20%5Ctextit%7Bin-vivo%7D%20imaging%20such%20as%0AMRI.%20While%20deep%20learning%20models%20have%20been%20developed%20to%20automate%20this%20process%2C%0Athe%20focus%20has%20predominantly%20been%20on%20human%20subjects%2C%20neglecting%20the%20pivotal%20role%0Aof%20animal%20models%20in%20pre-clinical%20drug%20development.%20In%20this%20work%2C%20we%20focus%20on%0Aoptimizing%20lung%20tumor%20segmentation%20in%20mice.%20First%2C%20we%20demonstrate%20that%20the%0AnnU-Net%20model%20outperforms%20the%20U-Net%2C%20U-Net3%2B%2C%20and%20DeepMeta%20models.%20Most%0Aimportantly%2C%20we%20achieve%20better%20results%20with%20nnU-Net%203D%20models%20than%202D%20models%2C%0Aindicating%20the%20importance%20of%20spatial%20context%20for%20segmentation%20tasks%20in%20MRI%20mice%0Ascans.%20This%20study%20demonstrates%20the%20importance%20of%203D%20input%20over%202D%20input%20images%0Afor%20lung%20tumor%20segmentation%20in%20MRI%20scans.%20Finally%2C%20we%20outperform%20the%20prior%0Astate-of-the-art%20approach%20that%20involves%20the%20combined%20segmentation%20of%20lungs%20and%0Atumors%20within%20the%20lungs.%20Our%20work%20achieves%20comparable%20results%20using%20only%20lung%0Atumor%20annotations%20requiring%20fewer%20annotations%2C%20saving%20time%20and%20annotation%0Aefforts.%20This%20work%0A%28https%3A//anonymous.4open.science/r/lung-tumour-mice-mri-64BB%29%20is%20an%20important%0Astep%20in%20automating%20pre-clinical%20animal%20studies%20to%20quantify%20the%20efficacy%20of%0Aexperimental%20drugs%2C%20particularly%20in%20assessing%20tumor%20changes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.00922v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLung%2520tumor%2520segmentation%2520in%2520MRI%2520mice%2520scans%2520using%25203D%2520nnU-Net%2520with%2520minimum%250A%2520%2520annotations%26entry.906535625%3DPiotr%2520Kaniewski%2520and%2520Fariba%2520Yousefi%2520and%2520Yeman%2520Brhane%2520Hagos%2520and%2520Talha%2520Qaiser%2520and%2520Nikolay%2520Burlutskiy%26entry.1292438233%3D%2520%2520In%2520drug%2520discovery%252C%2520accurate%2520lung%2520tumor%2520segmentation%2520is%2520an%2520important%2520step%2520for%250Aassessing%2520tumor%2520size%2520and%2520its%2520progression%2520using%2520%255Ctextit%257Bin-vivo%257D%2520imaging%2520such%2520as%250AMRI.%2520While%2520deep%2520learning%2520models%2520have%2520been%2520developed%2520to%2520automate%2520this%2520process%252C%250Athe%2520focus%2520has%2520predominantly%2520been%2520on%2520human%2520subjects%252C%2520neglecting%2520the%2520pivotal%2520role%250Aof%2520animal%2520models%2520in%2520pre-clinical%2520drug%2520development.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%250Aoptimizing%2520lung%2520tumor%2520segmentation%2520in%2520mice.%2520First%252C%2520we%2520demonstrate%2520that%2520the%250AnnU-Net%2520model%2520outperforms%2520the%2520U-Net%252C%2520U-Net3%252B%252C%2520and%2520DeepMeta%2520models.%2520Most%250Aimportantly%252C%2520we%2520achieve%2520better%2520results%2520with%2520nnU-Net%25203D%2520models%2520than%25202D%2520models%252C%250Aindicating%2520the%2520importance%2520of%2520spatial%2520context%2520for%2520segmentation%2520tasks%2520in%2520MRI%2520mice%250Ascans.%2520This%2520study%2520demonstrates%2520the%2520importance%2520of%25203D%2520input%2520over%25202D%2520input%2520images%250Afor%2520lung%2520tumor%2520segmentation%2520in%2520MRI%2520scans.%2520Finally%252C%2520we%2520outperform%2520the%2520prior%250Astate-of-the-art%2520approach%2520that%2520involves%2520the%2520combined%2520segmentation%2520of%2520lungs%2520and%250Atumors%2520within%2520the%2520lungs.%2520Our%2520work%2520achieves%2520comparable%2520results%2520using%2520only%2520lung%250Atumor%2520annotations%2520requiring%2520fewer%2520annotations%252C%2520saving%2520time%2520and%2520annotation%250Aefforts.%2520This%2520work%250A%2528https%253A//anonymous.4open.science/r/lung-tumour-mice-mri-64BB%2529%2520is%2520an%2520important%250Astep%2520in%2520automating%2520pre-clinical%2520animal%2520studies%2520to%2520quantify%2520the%2520efficacy%2520of%250Aexperimental%2520drugs%252C%2520particularly%2520in%2520assessing%2520tumor%2520changes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.00922v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lung%20tumor%20segmentation%20in%20MRI%20mice%20scans%20using%203D%20nnU-Net%20with%20minimum%0A%20%20annotations&entry.906535625=Piotr%20Kaniewski%20and%20Fariba%20Yousefi%20and%20Yeman%20Brhane%20Hagos%20and%20Talha%20Qaiser%20and%20Nikolay%20Burlutskiy&entry.1292438233=%20%20In%20drug%20discovery%2C%20accurate%20lung%20tumor%20segmentation%20is%20an%20important%20step%20for%0Aassessing%20tumor%20size%20and%20its%20progression%20using%20%5Ctextit%7Bin-vivo%7D%20imaging%20such%20as%0AMRI.%20While%20deep%20learning%20models%20have%20been%20developed%20to%20automate%20this%20process%2C%0Athe%20focus%20has%20predominantly%20been%20on%20human%20subjects%2C%20neglecting%20the%20pivotal%20role%0Aof%20animal%20models%20in%20pre-clinical%20drug%20development.%20In%20this%20work%2C%20we%20focus%20on%0Aoptimizing%20lung%20tumor%20segmentation%20in%20mice.%20First%2C%20we%20demonstrate%20that%20the%0AnnU-Net%20model%20outperforms%20the%20U-Net%2C%20U-Net3%2B%2C%20and%20DeepMeta%20models.%20Most%0Aimportantly%2C%20we%20achieve%20better%20results%20with%20nnU-Net%203D%20models%20than%202D%20models%2C%0Aindicating%20the%20importance%20of%20spatial%20context%20for%20segmentation%20tasks%20in%20MRI%20mice%0Ascans.%20This%20study%20demonstrates%20the%20importance%20of%203D%20input%20over%202D%20input%20images%0Afor%20lung%20tumor%20segmentation%20in%20MRI%20scans.%20Finally%2C%20we%20outperform%20the%20prior%0Astate-of-the-art%20approach%20that%20involves%20the%20combined%20segmentation%20of%20lungs%20and%0Atumors%20within%20the%20lungs.%20Our%20work%20achieves%20comparable%20results%20using%20only%20lung%0Atumor%20annotations%20requiring%20fewer%20annotations%2C%20saving%20time%20and%20annotation%0Aefforts.%20This%20work%0A%28https%3A//anonymous.4open.science/r/lung-tumour-mice-mri-64BB%29%20is%20an%20important%0Astep%20in%20automating%20pre-clinical%20animal%20studies%20to%20quantify%20the%20efficacy%20of%0Aexperimental%20drugs%2C%20particularly%20in%20assessing%20tumor%20changes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.00922v2&entry.124074799=Read"},
{"title": "Aioli: A Unified Optimization Framework for Language Model Data Mixing", "author": "Mayee F. Chen and Michael Y. Hu and Nicholas Lourie and Kyunghyun Cho and Christopher R\u00e9", "abstract": "  Language model performance depends on identifying the optimal mixture of data\ngroups to train on (e.g., law, code, math). Prior work has proposed a diverse\nset of methods to efficiently learn mixture proportions, ranging from fitting\nregression models over training runs to dynamically updating proportions\nthroughout training. Surprisingly, we find that no existing method consistently\noutperforms a simple stratified sampling baseline in terms of average test\nperplexity per group. In this paper, we study the cause of this inconsistency\nby unifying existing methods into a standard optimization framework. We show\nthat all methods set proportions to minimize total loss, subject to a\nmethod-specific mixing law -- an assumption on how loss is a function of\nmixture proportions. We find that existing parameterizations of mixing laws can\nexpress the true loss-proportion relationship empirically, but the methods\nthemselves often set the mixing law parameters inaccurately, resulting in poor\nand inconsistent performance. Finally, we leverage the insights from our\nframework to derive a new online method named Aioli, which directly estimates\nthe mixing law parameters throughout training and uses them to dynamically\nadjust proportions. Empirically, Aioli outperforms stratified sampling on 6 out\nof 6 datasets by an average of 0.28 test perplexity points, whereas existing\nmethods fail to consistently beat stratified sampling, doing up to 6.9 points\nworse. Moreover, in a practical setting where proportions are learned on\nshorter runs due to computational constraints, Aioli can dynamically adjust\nthese proportions over the full training run, consistently improving\nperformance over existing methods by up to 12.01 test perplexity points.\n", "link": "http://arxiv.org/abs/2411.05735v1", "date": "2024-11-08", "relevancy": 2.0267, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5164}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5047}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aioli%3A%20A%20Unified%20Optimization%20Framework%20for%20Language%20Model%20Data%20Mixing&body=Title%3A%20Aioli%3A%20A%20Unified%20Optimization%20Framework%20for%20Language%20Model%20Data%20Mixing%0AAuthor%3A%20Mayee%20F.%20Chen%20and%20Michael%20Y.%20Hu%20and%20Nicholas%20Lourie%20and%20Kyunghyun%20Cho%20and%20Christopher%20R%C3%A9%0AAbstract%3A%20%20%20Language%20model%20performance%20depends%20on%20identifying%20the%20optimal%20mixture%20of%20data%0Agroups%20to%20train%20on%20%28e.g.%2C%20law%2C%20code%2C%20math%29.%20Prior%20work%20has%20proposed%20a%20diverse%0Aset%20of%20methods%20to%20efficiently%20learn%20mixture%20proportions%2C%20ranging%20from%20fitting%0Aregression%20models%20over%20training%20runs%20to%20dynamically%20updating%20proportions%0Athroughout%20training.%20Surprisingly%2C%20we%20find%20that%20no%20existing%20method%20consistently%0Aoutperforms%20a%20simple%20stratified%20sampling%20baseline%20in%20terms%20of%20average%20test%0Aperplexity%20per%20group.%20In%20this%20paper%2C%20we%20study%20the%20cause%20of%20this%20inconsistency%0Aby%20unifying%20existing%20methods%20into%20a%20standard%20optimization%20framework.%20We%20show%0Athat%20all%20methods%20set%20proportions%20to%20minimize%20total%20loss%2C%20subject%20to%20a%0Amethod-specific%20mixing%20law%20--%20an%20assumption%20on%20how%20loss%20is%20a%20function%20of%0Amixture%20proportions.%20We%20find%20that%20existing%20parameterizations%20of%20mixing%20laws%20can%0Aexpress%20the%20true%20loss-proportion%20relationship%20empirically%2C%20but%20the%20methods%0Athemselves%20often%20set%20the%20mixing%20law%20parameters%20inaccurately%2C%20resulting%20in%20poor%0Aand%20inconsistent%20performance.%20Finally%2C%20we%20leverage%20the%20insights%20from%20our%0Aframework%20to%20derive%20a%20new%20online%20method%20named%20Aioli%2C%20which%20directly%20estimates%0Athe%20mixing%20law%20parameters%20throughout%20training%20and%20uses%20them%20to%20dynamically%0Aadjust%20proportions.%20Empirically%2C%20Aioli%20outperforms%20stratified%20sampling%20on%206%20out%0Aof%206%20datasets%20by%20an%20average%20of%200.28%20test%20perplexity%20points%2C%20whereas%20existing%0Amethods%20fail%20to%20consistently%20beat%20stratified%20sampling%2C%20doing%20up%20to%206.9%20points%0Aworse.%20Moreover%2C%20in%20a%20practical%20setting%20where%20proportions%20are%20learned%20on%0Ashorter%20runs%20due%20to%20computational%20constraints%2C%20Aioli%20can%20dynamically%20adjust%0Athese%20proportions%20over%20the%20full%20training%20run%2C%20consistently%20improving%0Aperformance%20over%20existing%20methods%20by%20up%20to%2012.01%20test%20perplexity%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAioli%253A%2520A%2520Unified%2520Optimization%2520Framework%2520for%2520Language%2520Model%2520Data%2520Mixing%26entry.906535625%3DMayee%2520F.%2520Chen%2520and%2520Michael%2520Y.%2520Hu%2520and%2520Nicholas%2520Lourie%2520and%2520Kyunghyun%2520Cho%2520and%2520Christopher%2520R%25C3%25A9%26entry.1292438233%3D%2520%2520Language%2520model%2520performance%2520depends%2520on%2520identifying%2520the%2520optimal%2520mixture%2520of%2520data%250Agroups%2520to%2520train%2520on%2520%2528e.g.%252C%2520law%252C%2520code%252C%2520math%2529.%2520Prior%2520work%2520has%2520proposed%2520a%2520diverse%250Aset%2520of%2520methods%2520to%2520efficiently%2520learn%2520mixture%2520proportions%252C%2520ranging%2520from%2520fitting%250Aregression%2520models%2520over%2520training%2520runs%2520to%2520dynamically%2520updating%2520proportions%250Athroughout%2520training.%2520Surprisingly%252C%2520we%2520find%2520that%2520no%2520existing%2520method%2520consistently%250Aoutperforms%2520a%2520simple%2520stratified%2520sampling%2520baseline%2520in%2520terms%2520of%2520average%2520test%250Aperplexity%2520per%2520group.%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520cause%2520of%2520this%2520inconsistency%250Aby%2520unifying%2520existing%2520methods%2520into%2520a%2520standard%2520optimization%2520framework.%2520We%2520show%250Athat%2520all%2520methods%2520set%2520proportions%2520to%2520minimize%2520total%2520loss%252C%2520subject%2520to%2520a%250Amethod-specific%2520mixing%2520law%2520--%2520an%2520assumption%2520on%2520how%2520loss%2520is%2520a%2520function%2520of%250Amixture%2520proportions.%2520We%2520find%2520that%2520existing%2520parameterizations%2520of%2520mixing%2520laws%2520can%250Aexpress%2520the%2520true%2520loss-proportion%2520relationship%2520empirically%252C%2520but%2520the%2520methods%250Athemselves%2520often%2520set%2520the%2520mixing%2520law%2520parameters%2520inaccurately%252C%2520resulting%2520in%2520poor%250Aand%2520inconsistent%2520performance.%2520Finally%252C%2520we%2520leverage%2520the%2520insights%2520from%2520our%250Aframework%2520to%2520derive%2520a%2520new%2520online%2520method%2520named%2520Aioli%252C%2520which%2520directly%2520estimates%250Athe%2520mixing%2520law%2520parameters%2520throughout%2520training%2520and%2520uses%2520them%2520to%2520dynamically%250Aadjust%2520proportions.%2520Empirically%252C%2520Aioli%2520outperforms%2520stratified%2520sampling%2520on%25206%2520out%250Aof%25206%2520datasets%2520by%2520an%2520average%2520of%25200.28%2520test%2520perplexity%2520points%252C%2520whereas%2520existing%250Amethods%2520fail%2520to%2520consistently%2520beat%2520stratified%2520sampling%252C%2520doing%2520up%2520to%25206.9%2520points%250Aworse.%2520Moreover%252C%2520in%2520a%2520practical%2520setting%2520where%2520proportions%2520are%2520learned%2520on%250Ashorter%2520runs%2520due%2520to%2520computational%2520constraints%252C%2520Aioli%2520can%2520dynamically%2520adjust%250Athese%2520proportions%2520over%2520the%2520full%2520training%2520run%252C%2520consistently%2520improving%250Aperformance%2520over%2520existing%2520methods%2520by%2520up%2520to%252012.01%2520test%2520perplexity%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aioli%3A%20A%20Unified%20Optimization%20Framework%20for%20Language%20Model%20Data%20Mixing&entry.906535625=Mayee%20F.%20Chen%20and%20Michael%20Y.%20Hu%20and%20Nicholas%20Lourie%20and%20Kyunghyun%20Cho%20and%20Christopher%20R%C3%A9&entry.1292438233=%20%20Language%20model%20performance%20depends%20on%20identifying%20the%20optimal%20mixture%20of%20data%0Agroups%20to%20train%20on%20%28e.g.%2C%20law%2C%20code%2C%20math%29.%20Prior%20work%20has%20proposed%20a%20diverse%0Aset%20of%20methods%20to%20efficiently%20learn%20mixture%20proportions%2C%20ranging%20from%20fitting%0Aregression%20models%20over%20training%20runs%20to%20dynamically%20updating%20proportions%0Athroughout%20training.%20Surprisingly%2C%20we%20find%20that%20no%20existing%20method%20consistently%0Aoutperforms%20a%20simple%20stratified%20sampling%20baseline%20in%20terms%20of%20average%20test%0Aperplexity%20per%20group.%20In%20this%20paper%2C%20we%20study%20the%20cause%20of%20this%20inconsistency%0Aby%20unifying%20existing%20methods%20into%20a%20standard%20optimization%20framework.%20We%20show%0Athat%20all%20methods%20set%20proportions%20to%20minimize%20total%20loss%2C%20subject%20to%20a%0Amethod-specific%20mixing%20law%20--%20an%20assumption%20on%20how%20loss%20is%20a%20function%20of%0Amixture%20proportions.%20We%20find%20that%20existing%20parameterizations%20of%20mixing%20laws%20can%0Aexpress%20the%20true%20loss-proportion%20relationship%20empirically%2C%20but%20the%20methods%0Athemselves%20often%20set%20the%20mixing%20law%20parameters%20inaccurately%2C%20resulting%20in%20poor%0Aand%20inconsistent%20performance.%20Finally%2C%20we%20leverage%20the%20insights%20from%20our%0Aframework%20to%20derive%20a%20new%20online%20method%20named%20Aioli%2C%20which%20directly%20estimates%0Athe%20mixing%20law%20parameters%20throughout%20training%20and%20uses%20them%20to%20dynamically%0Aadjust%20proportions.%20Empirically%2C%20Aioli%20outperforms%20stratified%20sampling%20on%206%20out%0Aof%206%20datasets%20by%20an%20average%20of%200.28%20test%20perplexity%20points%2C%20whereas%20existing%0Amethods%20fail%20to%20consistently%20beat%20stratified%20sampling%2C%20doing%20up%20to%206.9%20points%0Aworse.%20Moreover%2C%20in%20a%20practical%20setting%20where%20proportions%20are%20learned%20on%0Ashorter%20runs%20due%20to%20computational%20constraints%2C%20Aioli%20can%20dynamically%20adjust%0Athese%20proportions%20over%20the%20full%20training%20run%2C%20consistently%20improving%0Aperformance%20over%20existing%20methods%20by%20up%20to%2012.01%20test%20perplexity%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05735v1&entry.124074799=Read"},
{"title": "Improvement of Spiking Neural Network with Bit Planes and Color Models", "author": "Nhan T. Luu and Duong T. Luu and Nam N. Pham and Thang C. Truong", "abstract": "  Spiking neural network (SNN) has emerged as a promising paradigm in\ncomputational neuroscience and artificial intelligence, offering advantages\nsuch as low energy consumption and small memory footprint. However, their\npractical adoption is constrained by several challenges, prominently among them\nbeing performance optimization. In this study, we present a novel approach to\nenhance the performance of SNN for images through a new coding method that\nexploits bit plane representation. Our proposed technique is designed to\nimprove the accuracy of SNN without increasing model size. Also, we investigate\nthe impacts of color models of the proposed coding process. Through extensive\nexperimental validation, we demonstrate the effectiveness of our coding\nstrategy in achieving performance gain across multiple datasets. To the best of\nour knowledge, this is the first research that considers bit planes and color\nmodels in the context of SNN. By leveraging the unique characteristics of bit\nplanes, we hope to unlock new potentials in SNNs performance, potentially\npaving the way for more efficient and effective SNNs models in future\nresearches and applications.\n", "link": "http://arxiv.org/abs/2410.08229v2", "date": "2024-11-08", "relevancy": 2.0223, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5343}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5024}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improvement%20of%20Spiking%20Neural%20Network%20with%20Bit%20Planes%20and%20Color%20Models&body=Title%3A%20Improvement%20of%20Spiking%20Neural%20Network%20with%20Bit%20Planes%20and%20Color%20Models%0AAuthor%3A%20Nhan%20T.%20Luu%20and%20Duong%20T.%20Luu%20and%20Nam%20N.%20Pham%20and%20Thang%20C.%20Truong%0AAbstract%3A%20%20%20Spiking%20neural%20network%20%28SNN%29%20has%20emerged%20as%20a%20promising%20paradigm%20in%0Acomputational%20neuroscience%20and%20artificial%20intelligence%2C%20offering%20advantages%0Asuch%20as%20low%20energy%20consumption%20and%20small%20memory%20footprint.%20However%2C%20their%0Apractical%20adoption%20is%20constrained%20by%20several%20challenges%2C%20prominently%20among%20them%0Abeing%20performance%20optimization.%20In%20this%20study%2C%20we%20present%20a%20novel%20approach%20to%0Aenhance%20the%20performance%20of%20SNN%20for%20images%20through%20a%20new%20coding%20method%20that%0Aexploits%20bit%20plane%20representation.%20Our%20proposed%20technique%20is%20designed%20to%0Aimprove%20the%20accuracy%20of%20SNN%20without%20increasing%20model%20size.%20Also%2C%20we%20investigate%0Athe%20impacts%20of%20color%20models%20of%20the%20proposed%20coding%20process.%20Through%20extensive%0Aexperimental%20validation%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20coding%0Astrategy%20in%20achieving%20performance%20gain%20across%20multiple%20datasets.%20To%20the%20best%20of%0Aour%20knowledge%2C%20this%20is%20the%20first%20research%20that%20considers%20bit%20planes%20and%20color%0Amodels%20in%20the%20context%20of%20SNN.%20By%20leveraging%20the%20unique%20characteristics%20of%20bit%0Aplanes%2C%20we%20hope%20to%20unlock%20new%20potentials%20in%20SNNs%20performance%2C%20potentially%0Apaving%20the%20way%20for%20more%20efficient%20and%20effective%20SNNs%20models%20in%20future%0Aresearches%20and%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08229v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImprovement%2520of%2520Spiking%2520Neural%2520Network%2520with%2520Bit%2520Planes%2520and%2520Color%2520Models%26entry.906535625%3DNhan%2520T.%2520Luu%2520and%2520Duong%2520T.%2520Luu%2520and%2520Nam%2520N.%2520Pham%2520and%2520Thang%2520C.%2520Truong%26entry.1292438233%3D%2520%2520Spiking%2520neural%2520network%2520%2528SNN%2529%2520has%2520emerged%2520as%2520a%2520promising%2520paradigm%2520in%250Acomputational%2520neuroscience%2520and%2520artificial%2520intelligence%252C%2520offering%2520advantages%250Asuch%2520as%2520low%2520energy%2520consumption%2520and%2520small%2520memory%2520footprint.%2520However%252C%2520their%250Apractical%2520adoption%2520is%2520constrained%2520by%2520several%2520challenges%252C%2520prominently%2520among%2520them%250Abeing%2520performance%2520optimization.%2520In%2520this%2520study%252C%2520we%2520present%2520a%2520novel%2520approach%2520to%250Aenhance%2520the%2520performance%2520of%2520SNN%2520for%2520images%2520through%2520a%2520new%2520coding%2520method%2520that%250Aexploits%2520bit%2520plane%2520representation.%2520Our%2520proposed%2520technique%2520is%2520designed%2520to%250Aimprove%2520the%2520accuracy%2520of%2520SNN%2520without%2520increasing%2520model%2520size.%2520Also%252C%2520we%2520investigate%250Athe%2520impacts%2520of%2520color%2520models%2520of%2520the%2520proposed%2520coding%2520process.%2520Through%2520extensive%250Aexperimental%2520validation%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520coding%250Astrategy%2520in%2520achieving%2520performance%2520gain%2520across%2520multiple%2520datasets.%2520To%2520the%2520best%2520of%250Aour%2520knowledge%252C%2520this%2520is%2520the%2520first%2520research%2520that%2520considers%2520bit%2520planes%2520and%2520color%250Amodels%2520in%2520the%2520context%2520of%2520SNN.%2520By%2520leveraging%2520the%2520unique%2520characteristics%2520of%2520bit%250Aplanes%252C%2520we%2520hope%2520to%2520unlock%2520new%2520potentials%2520in%2520SNNs%2520performance%252C%2520potentially%250Apaving%2520the%2520way%2520for%2520more%2520efficient%2520and%2520effective%2520SNNs%2520models%2520in%2520future%250Aresearches%2520and%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08229v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improvement%20of%20Spiking%20Neural%20Network%20with%20Bit%20Planes%20and%20Color%20Models&entry.906535625=Nhan%20T.%20Luu%20and%20Duong%20T.%20Luu%20and%20Nam%20N.%20Pham%20and%20Thang%20C.%20Truong&entry.1292438233=%20%20Spiking%20neural%20network%20%28SNN%29%20has%20emerged%20as%20a%20promising%20paradigm%20in%0Acomputational%20neuroscience%20and%20artificial%20intelligence%2C%20offering%20advantages%0Asuch%20as%20low%20energy%20consumption%20and%20small%20memory%20footprint.%20However%2C%20their%0Apractical%20adoption%20is%20constrained%20by%20several%20challenges%2C%20prominently%20among%20them%0Abeing%20performance%20optimization.%20In%20this%20study%2C%20we%20present%20a%20novel%20approach%20to%0Aenhance%20the%20performance%20of%20SNN%20for%20images%20through%20a%20new%20coding%20method%20that%0Aexploits%20bit%20plane%20representation.%20Our%20proposed%20technique%20is%20designed%20to%0Aimprove%20the%20accuracy%20of%20SNN%20without%20increasing%20model%20size.%20Also%2C%20we%20investigate%0Athe%20impacts%20of%20color%20models%20of%20the%20proposed%20coding%20process.%20Through%20extensive%0Aexperimental%20validation%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20coding%0Astrategy%20in%20achieving%20performance%20gain%20across%20multiple%20datasets.%20To%20the%20best%20of%0Aour%20knowledge%2C%20this%20is%20the%20first%20research%20that%20considers%20bit%20planes%20and%20color%0Amodels%20in%20the%20context%20of%20SNN.%20By%20leveraging%20the%20unique%20characteristics%20of%20bit%0Aplanes%2C%20we%20hope%20to%20unlock%20new%20potentials%20in%20SNNs%20performance%2C%20potentially%0Apaving%20the%20way%20for%20more%20efficient%20and%20effective%20SNNs%20models%20in%20future%0Aresearches%20and%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08229v2&entry.124074799=Read"},
{"title": "Efficient Audio-Visual Fusion for Video Classification", "author": "Mahrukh Awan and Asmar Nadeem and Armin Mustafa", "abstract": "  We present Attend-Fusion, a novel and efficient approach for audio-visual\nfusion in video classification tasks. Our method addresses the challenge of\nexploiting both audio and visual modalities while maintaining a compact model\narchitecture. Through extensive experiments on the YouTube-8M dataset, we\ndemonstrate that our Attend-Fusion achieves competitive performance with\nsignificantly reduced model complexity compared to larger baseline models.\n", "link": "http://arxiv.org/abs/2411.05603v1", "date": "2024-11-08", "relevancy": 2.0188, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5244}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5069}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Audio-Visual%20Fusion%20for%20Video%20Classification&body=Title%3A%20Efficient%20Audio-Visual%20Fusion%20for%20Video%20Classification%0AAuthor%3A%20Mahrukh%20Awan%20and%20Asmar%20Nadeem%20and%20Armin%20Mustafa%0AAbstract%3A%20%20%20We%20present%20Attend-Fusion%2C%20a%20novel%20and%20efficient%20approach%20for%20audio-visual%0Afusion%20in%20video%20classification%20tasks.%20Our%20method%20addresses%20the%20challenge%20of%0Aexploiting%20both%20audio%20and%20visual%20modalities%20while%20maintaining%20a%20compact%20model%0Aarchitecture.%20Through%20extensive%20experiments%20on%20the%20YouTube-8M%20dataset%2C%20we%0Ademonstrate%20that%20our%20Attend-Fusion%20achieves%20competitive%20performance%20with%0Asignificantly%20reduced%20model%20complexity%20compared%20to%20larger%20baseline%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Audio-Visual%2520Fusion%2520for%2520Video%2520Classification%26entry.906535625%3DMahrukh%2520Awan%2520and%2520Asmar%2520Nadeem%2520and%2520Armin%2520Mustafa%26entry.1292438233%3D%2520%2520We%2520present%2520Attend-Fusion%252C%2520a%2520novel%2520and%2520efficient%2520approach%2520for%2520audio-visual%250Afusion%2520in%2520video%2520classification%2520tasks.%2520Our%2520method%2520addresses%2520the%2520challenge%2520of%250Aexploiting%2520both%2520audio%2520and%2520visual%2520modalities%2520while%2520maintaining%2520a%2520compact%2520model%250Aarchitecture.%2520Through%2520extensive%2520experiments%2520on%2520the%2520YouTube-8M%2520dataset%252C%2520we%250Ademonstrate%2520that%2520our%2520Attend-Fusion%2520achieves%2520competitive%2520performance%2520with%250Asignificantly%2520reduced%2520model%2520complexity%2520compared%2520to%2520larger%2520baseline%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Audio-Visual%20Fusion%20for%20Video%20Classification&entry.906535625=Mahrukh%20Awan%20and%20Asmar%20Nadeem%20and%20Armin%20Mustafa&entry.1292438233=%20%20We%20present%20Attend-Fusion%2C%20a%20novel%20and%20efficient%20approach%20for%20audio-visual%0Afusion%20in%20video%20classification%20tasks.%20Our%20method%20addresses%20the%20challenge%20of%0Aexploiting%20both%20audio%20and%20visual%20modalities%20while%20maintaining%20a%20compact%20model%0Aarchitecture.%20Through%20extensive%20experiments%20on%20the%20YouTube-8M%20dataset%2C%20we%0Ademonstrate%20that%20our%20Attend-Fusion%20achieves%20competitive%20performance%20with%0Asignificantly%20reduced%20model%20complexity%20compared%20to%20larger%20baseline%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05603v1&entry.124074799=Read"},
{"title": "Secret Collusion among Generative AI Agents", "author": "Sumeet Ramesh Motwani and Mikhail Baranchuk and Martin Strohmeier and Vijay Bolina and Philip H. S. Torr and Lewis Hammond and Christian Schroeder de Witt", "abstract": "  Recent capability increases in large language models (LLMs) open up\napplications in which groups of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models.\n", "link": "http://arxiv.org/abs/2402.07510v3", "date": "2024-11-08", "relevancy": 2.0165, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5172}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5118}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Secret%20Collusion%20among%20Generative%20AI%20Agents&body=Title%3A%20Secret%20Collusion%20among%20Generative%20AI%20Agents%0AAuthor%3A%20Sumeet%20Ramesh%20Motwani%20and%20Mikhail%20Baranchuk%20and%20Martin%20Strohmeier%20and%20Vijay%20Bolina%20and%20Philip%20H.%20S.%20Torr%20and%20Lewis%20Hammond%20and%20Christian%20Schroeder%20de%20Witt%0AAbstract%3A%20%20%20Recent%20capability%20increases%20in%20large%20language%20models%20%28LLMs%29%20open%20up%0Aapplications%20in%20which%20groups%20of%20communicating%20generative%20AI%20agents%20solve%20joint%0Atasks.%20This%20poses%20privacy%20and%20security%20challenges%20concerning%20the%20unauthorised%0Asharing%20of%20information%2C%20or%20other%20unwanted%20forms%20of%20agent%20coordination.%20Modern%0Asteganographic%20techniques%20could%20render%20such%20dynamics%20hard%20to%20detect.%20In%20this%0Apaper%2C%20we%20comprehensively%20formalise%20the%20problem%20of%20secret%20collusion%20in%20systems%0Aof%20generative%20AI%20agents%20by%20drawing%20on%20relevant%20concepts%20from%20both%20AI%20and%0Asecurity%20literature.%20We%20study%20incentives%20for%20the%20use%20of%20steganography%2C%20and%0Apropose%20a%20variety%20of%20mitigation%20measures.%20Our%20investigations%20result%20in%20a%20model%0Aevaluation%20framework%20that%20systematically%20tests%20capabilities%20required%20for%0Avarious%20forms%20of%20secret%20collusion.%20We%20provide%20extensive%20empirical%20results%0Aacross%20a%20range%20of%20contemporary%20LLMs.%20While%20the%20steganographic%20capabilities%20of%0Acurrent%20models%20remain%20limited%2C%20GPT-4%20displays%20a%20capability%20jump%20suggesting%20the%0Aneed%20for%20continuous%20monitoring%20of%20steganographic%20frontier%20model%20capabilities.%0AWe%20conclude%20by%20laying%20out%20a%20comprehensive%20research%20program%20to%20mitigate%20future%0Arisks%20of%20collusion%20between%20generative%20AI%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07510v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecret%2520Collusion%2520among%2520Generative%2520AI%2520Agents%26entry.906535625%3DSumeet%2520Ramesh%2520Motwani%2520and%2520Mikhail%2520Baranchuk%2520and%2520Martin%2520Strohmeier%2520and%2520Vijay%2520Bolina%2520and%2520Philip%2520H.%2520S.%2520Torr%2520and%2520Lewis%2520Hammond%2520and%2520Christian%2520Schroeder%2520de%2520Witt%26entry.1292438233%3D%2520%2520Recent%2520capability%2520increases%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520open%2520up%250Aapplications%2520in%2520which%2520groups%2520of%2520communicating%2520generative%2520AI%2520agents%2520solve%2520joint%250Atasks.%2520This%2520poses%2520privacy%2520and%2520security%2520challenges%2520concerning%2520the%2520unauthorised%250Asharing%2520of%2520information%252C%2520or%2520other%2520unwanted%2520forms%2520of%2520agent%2520coordination.%2520Modern%250Asteganographic%2520techniques%2520could%2520render%2520such%2520dynamics%2520hard%2520to%2520detect.%2520In%2520this%250Apaper%252C%2520we%2520comprehensively%2520formalise%2520the%2520problem%2520of%2520secret%2520collusion%2520in%2520systems%250Aof%2520generative%2520AI%2520agents%2520by%2520drawing%2520on%2520relevant%2520concepts%2520from%2520both%2520AI%2520and%250Asecurity%2520literature.%2520We%2520study%2520incentives%2520for%2520the%2520use%2520of%2520steganography%252C%2520and%250Apropose%2520a%2520variety%2520of%2520mitigation%2520measures.%2520Our%2520investigations%2520result%2520in%2520a%2520model%250Aevaluation%2520framework%2520that%2520systematically%2520tests%2520capabilities%2520required%2520for%250Avarious%2520forms%2520of%2520secret%2520collusion.%2520We%2520provide%2520extensive%2520empirical%2520results%250Aacross%2520a%2520range%2520of%2520contemporary%2520LLMs.%2520While%2520the%2520steganographic%2520capabilities%2520of%250Acurrent%2520models%2520remain%2520limited%252C%2520GPT-4%2520displays%2520a%2520capability%2520jump%2520suggesting%2520the%250Aneed%2520for%2520continuous%2520monitoring%2520of%2520steganographic%2520frontier%2520model%2520capabilities.%250AWe%2520conclude%2520by%2520laying%2520out%2520a%2520comprehensive%2520research%2520program%2520to%2520mitigate%2520future%250Arisks%2520of%2520collusion%2520between%2520generative%2520AI%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07510v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Secret%20Collusion%20among%20Generative%20AI%20Agents&entry.906535625=Sumeet%20Ramesh%20Motwani%20and%20Mikhail%20Baranchuk%20and%20Martin%20Strohmeier%20and%20Vijay%20Bolina%20and%20Philip%20H.%20S.%20Torr%20and%20Lewis%20Hammond%20and%20Christian%20Schroeder%20de%20Witt&entry.1292438233=%20%20Recent%20capability%20increases%20in%20large%20language%20models%20%28LLMs%29%20open%20up%0Aapplications%20in%20which%20groups%20of%20communicating%20generative%20AI%20agents%20solve%20joint%0Atasks.%20This%20poses%20privacy%20and%20security%20challenges%20concerning%20the%20unauthorised%0Asharing%20of%20information%2C%20or%20other%20unwanted%20forms%20of%20agent%20coordination.%20Modern%0Asteganographic%20techniques%20could%20render%20such%20dynamics%20hard%20to%20detect.%20In%20this%0Apaper%2C%20we%20comprehensively%20formalise%20the%20problem%20of%20secret%20collusion%20in%20systems%0Aof%20generative%20AI%20agents%20by%20drawing%20on%20relevant%20concepts%20from%20both%20AI%20and%0Asecurity%20literature.%20We%20study%20incentives%20for%20the%20use%20of%20steganography%2C%20and%0Apropose%20a%20variety%20of%20mitigation%20measures.%20Our%20investigations%20result%20in%20a%20model%0Aevaluation%20framework%20that%20systematically%20tests%20capabilities%20required%20for%0Avarious%20forms%20of%20secret%20collusion.%20We%20provide%20extensive%20empirical%20results%0Aacross%20a%20range%20of%20contemporary%20LLMs.%20While%20the%20steganographic%20capabilities%20of%0Acurrent%20models%20remain%20limited%2C%20GPT-4%20displays%20a%20capability%20jump%20suggesting%20the%0Aneed%20for%20continuous%20monitoring%20of%20steganographic%20frontier%20model%20capabilities.%0AWe%20conclude%20by%20laying%20out%20a%20comprehensive%20research%20program%20to%20mitigate%20future%0Arisks%20of%20collusion%20between%20generative%20AI%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07510v3&entry.124074799=Read"},
{"title": "Boulder2Vec: Modeling Climber Performances in Professional Bouldering\n  Competitions", "author": "Ethan Baron and Victor Hau and Zeke Weng", "abstract": "  Using data from professional bouldering competitions from 2008 to 2022, we\ntrain a logistic regression to predict climber results and measure climber\nskill. However, this approach is limited, as a single numeric coefficient per\nclimber cannot adequately capture the intricacies of climbers' varying\nstrengths and weaknesses in different boulder problems. For example, some\nclimbers might prefer more static, technical routes while other climbers may\nspecialize in powerful, dynamic problems.\n  To this end, we apply Probabilistic Matrix Factorization (PMF), a framework\ncommonly used in recommender systems, to represent the unique characteristics\nof climbers and problems with latent, multi-dimensional vectors. In this\nframework, a climber's performance on a given problem is predicted by taking\nthe dot product of the corresponding climber vector and problem vectors. PMF\neffectively handles sparse datasets, such as our dataset where only a subset of\nclimbers attempt each particular problem, by extrapolating patterns from\nsimilar climbers.\n  We contrast the empirical performance of PMF to the logistic regression\napproach and investigate the multivariate representations produced by PMF to\ngain insights into climber characteristics. Our results show that the\nmultivariate PMF representations improve predictive performance of professional\nbouldering competitions by capturing both the overall strength of climbers and\ntheir specialized skill sets. We provide our code open-source at\nhttps://github.com/baronet2/boulder2vec.\n", "link": "http://arxiv.org/abs/2411.02343v2", "date": "2024-11-08", "relevancy": 2.0033, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5672}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4876}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boulder2Vec%3A%20Modeling%20Climber%20Performances%20in%20Professional%20Bouldering%0A%20%20Competitions&body=Title%3A%20Boulder2Vec%3A%20Modeling%20Climber%20Performances%20in%20Professional%20Bouldering%0A%20%20Competitions%0AAuthor%3A%20Ethan%20Baron%20and%20Victor%20Hau%20and%20Zeke%20Weng%0AAbstract%3A%20%20%20Using%20data%20from%20professional%20bouldering%20competitions%20from%202008%20to%202022%2C%20we%0Atrain%20a%20logistic%20regression%20to%20predict%20climber%20results%20and%20measure%20climber%0Askill.%20However%2C%20this%20approach%20is%20limited%2C%20as%20a%20single%20numeric%20coefficient%20per%0Aclimber%20cannot%20adequately%20capture%20the%20intricacies%20of%20climbers%27%20varying%0Astrengths%20and%20weaknesses%20in%20different%20boulder%20problems.%20For%20example%2C%20some%0Aclimbers%20might%20prefer%20more%20static%2C%20technical%20routes%20while%20other%20climbers%20may%0Aspecialize%20in%20powerful%2C%20dynamic%20problems.%0A%20%20To%20this%20end%2C%20we%20apply%20Probabilistic%20Matrix%20Factorization%20%28PMF%29%2C%20a%20framework%0Acommonly%20used%20in%20recommender%20systems%2C%20to%20represent%20the%20unique%20characteristics%0Aof%20climbers%20and%20problems%20with%20latent%2C%20multi-dimensional%20vectors.%20In%20this%0Aframework%2C%20a%20climber%27s%20performance%20on%20a%20given%20problem%20is%20predicted%20by%20taking%0Athe%20dot%20product%20of%20the%20corresponding%20climber%20vector%20and%20problem%20vectors.%20PMF%0Aeffectively%20handles%20sparse%20datasets%2C%20such%20as%20our%20dataset%20where%20only%20a%20subset%20of%0Aclimbers%20attempt%20each%20particular%20problem%2C%20by%20extrapolating%20patterns%20from%0Asimilar%20climbers.%0A%20%20We%20contrast%20the%20empirical%20performance%20of%20PMF%20to%20the%20logistic%20regression%0Aapproach%20and%20investigate%20the%20multivariate%20representations%20produced%20by%20PMF%20to%0Again%20insights%20into%20climber%20characteristics.%20Our%20results%20show%20that%20the%0Amultivariate%20PMF%20representations%20improve%20predictive%20performance%20of%20professional%0Abouldering%20competitions%20by%20capturing%20both%20the%20overall%20strength%20of%20climbers%20and%0Atheir%20specialized%20skill%20sets.%20We%20provide%20our%20code%20open-source%20at%0Ahttps%3A//github.com/baronet2/boulder2vec.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.02343v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoulder2Vec%253A%2520Modeling%2520Climber%2520Performances%2520in%2520Professional%2520Bouldering%250A%2520%2520Competitions%26entry.906535625%3DEthan%2520Baron%2520and%2520Victor%2520Hau%2520and%2520Zeke%2520Weng%26entry.1292438233%3D%2520%2520Using%2520data%2520from%2520professional%2520bouldering%2520competitions%2520from%25202008%2520to%25202022%252C%2520we%250Atrain%2520a%2520logistic%2520regression%2520to%2520predict%2520climber%2520results%2520and%2520measure%2520climber%250Askill.%2520However%252C%2520this%2520approach%2520is%2520limited%252C%2520as%2520a%2520single%2520numeric%2520coefficient%2520per%250Aclimber%2520cannot%2520adequately%2520capture%2520the%2520intricacies%2520of%2520climbers%2527%2520varying%250Astrengths%2520and%2520weaknesses%2520in%2520different%2520boulder%2520problems.%2520For%2520example%252C%2520some%250Aclimbers%2520might%2520prefer%2520more%2520static%252C%2520technical%2520routes%2520while%2520other%2520climbers%2520may%250Aspecialize%2520in%2520powerful%252C%2520dynamic%2520problems.%250A%2520%2520To%2520this%2520end%252C%2520we%2520apply%2520Probabilistic%2520Matrix%2520Factorization%2520%2528PMF%2529%252C%2520a%2520framework%250Acommonly%2520used%2520in%2520recommender%2520systems%252C%2520to%2520represent%2520the%2520unique%2520characteristics%250Aof%2520climbers%2520and%2520problems%2520with%2520latent%252C%2520multi-dimensional%2520vectors.%2520In%2520this%250Aframework%252C%2520a%2520climber%2527s%2520performance%2520on%2520a%2520given%2520problem%2520is%2520predicted%2520by%2520taking%250Athe%2520dot%2520product%2520of%2520the%2520corresponding%2520climber%2520vector%2520and%2520problem%2520vectors.%2520PMF%250Aeffectively%2520handles%2520sparse%2520datasets%252C%2520such%2520as%2520our%2520dataset%2520where%2520only%2520a%2520subset%2520of%250Aclimbers%2520attempt%2520each%2520particular%2520problem%252C%2520by%2520extrapolating%2520patterns%2520from%250Asimilar%2520climbers.%250A%2520%2520We%2520contrast%2520the%2520empirical%2520performance%2520of%2520PMF%2520to%2520the%2520logistic%2520regression%250Aapproach%2520and%2520investigate%2520the%2520multivariate%2520representations%2520produced%2520by%2520PMF%2520to%250Again%2520insights%2520into%2520climber%2520characteristics.%2520Our%2520results%2520show%2520that%2520the%250Amultivariate%2520PMF%2520representations%2520improve%2520predictive%2520performance%2520of%2520professional%250Abouldering%2520competitions%2520by%2520capturing%2520both%2520the%2520overall%2520strength%2520of%2520climbers%2520and%250Atheir%2520specialized%2520skill%2520sets.%2520We%2520provide%2520our%2520code%2520open-source%2520at%250Ahttps%253A//github.com/baronet2/boulder2vec.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.02343v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boulder2Vec%3A%20Modeling%20Climber%20Performances%20in%20Professional%20Bouldering%0A%20%20Competitions&entry.906535625=Ethan%20Baron%20and%20Victor%20Hau%20and%20Zeke%20Weng&entry.1292438233=%20%20Using%20data%20from%20professional%20bouldering%20competitions%20from%202008%20to%202022%2C%20we%0Atrain%20a%20logistic%20regression%20to%20predict%20climber%20results%20and%20measure%20climber%0Askill.%20However%2C%20this%20approach%20is%20limited%2C%20as%20a%20single%20numeric%20coefficient%20per%0Aclimber%20cannot%20adequately%20capture%20the%20intricacies%20of%20climbers%27%20varying%0Astrengths%20and%20weaknesses%20in%20different%20boulder%20problems.%20For%20example%2C%20some%0Aclimbers%20might%20prefer%20more%20static%2C%20technical%20routes%20while%20other%20climbers%20may%0Aspecialize%20in%20powerful%2C%20dynamic%20problems.%0A%20%20To%20this%20end%2C%20we%20apply%20Probabilistic%20Matrix%20Factorization%20%28PMF%29%2C%20a%20framework%0Acommonly%20used%20in%20recommender%20systems%2C%20to%20represent%20the%20unique%20characteristics%0Aof%20climbers%20and%20problems%20with%20latent%2C%20multi-dimensional%20vectors.%20In%20this%0Aframework%2C%20a%20climber%27s%20performance%20on%20a%20given%20problem%20is%20predicted%20by%20taking%0Athe%20dot%20product%20of%20the%20corresponding%20climber%20vector%20and%20problem%20vectors.%20PMF%0Aeffectively%20handles%20sparse%20datasets%2C%20such%20as%20our%20dataset%20where%20only%20a%20subset%20of%0Aclimbers%20attempt%20each%20particular%20problem%2C%20by%20extrapolating%20patterns%20from%0Asimilar%20climbers.%0A%20%20We%20contrast%20the%20empirical%20performance%20of%20PMF%20to%20the%20logistic%20regression%0Aapproach%20and%20investigate%20the%20multivariate%20representations%20produced%20by%20PMF%20to%0Again%20insights%20into%20climber%20characteristics.%20Our%20results%20show%20that%20the%0Amultivariate%20PMF%20representations%20improve%20predictive%20performance%20of%20professional%0Abouldering%20competitions%20by%20capturing%20both%20the%20overall%20strength%20of%20climbers%20and%0Atheir%20specialized%20skill%20sets.%20We%20provide%20our%20code%20open-source%20at%0Ahttps%3A//github.com/baronet2/boulder2vec.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.02343v2&entry.124074799=Read"},
{"title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression", "author": "Utkarsh Saxena and Gobinda Saha and Sakshi Choudhary and Kaushik Roy", "abstract": "  Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn.\n", "link": "http://arxiv.org/abs/2408.05646v2", "date": "2024-11-08", "relevancy": 1.9975, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5494}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4647}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eigen%20Attention%3A%20Attention%20in%20Low-Rank%20Space%20for%20KV%20Cache%20Compression&body=Title%3A%20Eigen%20Attention%3A%20Attention%20in%20Low-Rank%20Space%20for%20KV%20Cache%20Compression%0AAuthor%3A%20Utkarsh%20Saxena%20and%20Gobinda%20Saha%20and%20Sakshi%20Choudhary%20and%20Kaushik%20Roy%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20represent%20a%20groundbreaking%20advancement%20in%20the%0Adomain%20of%20natural%20language%20processing%20due%20to%20their%20impressive%20reasoning%0Aabilities.%20Recently%2C%20there%20has%20been%20considerable%20interest%20in%20increasing%20the%0Acontext%20lengths%20for%20these%20models%20to%20enhance%20their%20applicability%20to%20complex%0Atasks.%20However%2C%20at%20long%20context%20lengths%20and%20large%20batch%20sizes%2C%20the%20key-value%0A%28KV%29%20cache%2C%20which%20stores%20the%20attention%20keys%20and%20values%2C%20emerges%20as%20the%20new%0Abottleneck%20in%20memory%20usage%20during%20inference.%20To%20address%20this%2C%20we%20propose%20Eigen%0AAttention%2C%20which%20performs%20the%20attention%20operation%20in%20a%20low-rank%20space%2C%20thereby%0Areducing%20the%20KV%20cache%20memory%20overhead.%20Our%20proposed%20approach%20is%20orthogonal%20to%0Aexisting%20KV%20cache%20compression%20techniques%20and%20can%20be%20used%20synergistically%20with%0Athem.%20Through%20extensive%20experiments%20over%20OPT%2C%20MPT%2C%20and%20Llama%20model%20families%2C%20we%0Ademonstrate%20that%20Eigen%20Attention%20results%20in%20up%20to%2040%25%20reduction%20in%20KV%20cache%0Asizes%20and%20up%20to%2060%25%20reduction%20in%20attention%20operation%20latency%20with%20minimal%20drop%0Ain%20performance.%20Code%20is%20available%20at%0Ahttps%3A//github.com/UtkarshSaxena1/EigenAttn.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05646v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEigen%2520Attention%253A%2520Attention%2520in%2520Low-Rank%2520Space%2520for%2520KV%2520Cache%2520Compression%26entry.906535625%3DUtkarsh%2520Saxena%2520and%2520Gobinda%2520Saha%2520and%2520Sakshi%2520Choudhary%2520and%2520Kaushik%2520Roy%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520represent%2520a%2520groundbreaking%2520advancement%2520in%2520the%250Adomain%2520of%2520natural%2520language%2520processing%2520due%2520to%2520their%2520impressive%2520reasoning%250Aabilities.%2520Recently%252C%2520there%2520has%2520been%2520considerable%2520interest%2520in%2520increasing%2520the%250Acontext%2520lengths%2520for%2520these%2520models%2520to%2520enhance%2520their%2520applicability%2520to%2520complex%250Atasks.%2520However%252C%2520at%2520long%2520context%2520lengths%2520and%2520large%2520batch%2520sizes%252C%2520the%2520key-value%250A%2528KV%2529%2520cache%252C%2520which%2520stores%2520the%2520attention%2520keys%2520and%2520values%252C%2520emerges%2520as%2520the%2520new%250Abottleneck%2520in%2520memory%2520usage%2520during%2520inference.%2520To%2520address%2520this%252C%2520we%2520propose%2520Eigen%250AAttention%252C%2520which%2520performs%2520the%2520attention%2520operation%2520in%2520a%2520low-rank%2520space%252C%2520thereby%250Areducing%2520the%2520KV%2520cache%2520memory%2520overhead.%2520Our%2520proposed%2520approach%2520is%2520orthogonal%2520to%250Aexisting%2520KV%2520cache%2520compression%2520techniques%2520and%2520can%2520be%2520used%2520synergistically%2520with%250Athem.%2520Through%2520extensive%2520experiments%2520over%2520OPT%252C%2520MPT%252C%2520and%2520Llama%2520model%2520families%252C%2520we%250Ademonstrate%2520that%2520Eigen%2520Attention%2520results%2520in%2520up%2520to%252040%2525%2520reduction%2520in%2520KV%2520cache%250Asizes%2520and%2520up%2520to%252060%2525%2520reduction%2520in%2520attention%2520operation%2520latency%2520with%2520minimal%2520drop%250Ain%2520performance.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/UtkarshSaxena1/EigenAttn.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05646v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eigen%20Attention%3A%20Attention%20in%20Low-Rank%20Space%20for%20KV%20Cache%20Compression&entry.906535625=Utkarsh%20Saxena%20and%20Gobinda%20Saha%20and%20Sakshi%20Choudhary%20and%20Kaushik%20Roy&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20represent%20a%20groundbreaking%20advancement%20in%20the%0Adomain%20of%20natural%20language%20processing%20due%20to%20their%20impressive%20reasoning%0Aabilities.%20Recently%2C%20there%20has%20been%20considerable%20interest%20in%20increasing%20the%0Acontext%20lengths%20for%20these%20models%20to%20enhance%20their%20applicability%20to%20complex%0Atasks.%20However%2C%20at%20long%20context%20lengths%20and%20large%20batch%20sizes%2C%20the%20key-value%0A%28KV%29%20cache%2C%20which%20stores%20the%20attention%20keys%20and%20values%2C%20emerges%20as%20the%20new%0Abottleneck%20in%20memory%20usage%20during%20inference.%20To%20address%20this%2C%20we%20propose%20Eigen%0AAttention%2C%20which%20performs%20the%20attention%20operation%20in%20a%20low-rank%20space%2C%20thereby%0Areducing%20the%20KV%20cache%20memory%20overhead.%20Our%20proposed%20approach%20is%20orthogonal%20to%0Aexisting%20KV%20cache%20compression%20techniques%20and%20can%20be%20used%20synergistically%20with%0Athem.%20Through%20extensive%20experiments%20over%20OPT%2C%20MPT%2C%20and%20Llama%20model%20families%2C%20we%0Ademonstrate%20that%20Eigen%20Attention%20results%20in%20up%20to%2040%25%20reduction%20in%20KV%20cache%0Asizes%20and%20up%20to%2060%25%20reduction%20in%20attention%20operation%20latency%20with%20minimal%20drop%0Ain%20performance.%20Code%20is%20available%20at%0Ahttps%3A//github.com/UtkarshSaxena1/EigenAttn.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05646v2&entry.124074799=Read"},
{"title": "IPMN Risk Assessment under Federated Learning Paradigm", "author": "Hongyi Pan and Ziliang Hong and Gorkem Durak and Elif Keles and Halil Ertugrul Aktas and Yavuz Taktak and Alpay Medetalibeyoglu and Zheyuan Zhang and Yury Velichko and Concetto Spampinato and Ivo Schoots and Marco J. Bruno and Pallavi Tiwari and Candice Bolan and Tamas Gonda and Frank Miller and Rajesh N. Keswani and Michael B. Wallace and Ziyue Xu and Ulas Bagci", "abstract": "  Accurate classification of Intraductal Papillary Mucinous Neoplasms (IPMN) is\nessential for identifying high-risk cases that require timely intervention. In\nthis study, we develop a federated learning framework for multi-center IPMN\nclassification utilizing a comprehensive pancreas MRI dataset. This dataset\nincludes 653 T1-weighted and 656 T2-weighted MRI images, accompanied by\ncorresponding IPMN risk scores from 7 leading medical institutions, making it\nthe largest and most diverse dataset for IPMN classification to date. We assess\nthe performance of DenseNet-121 in both centralized and federated settings for\ntraining on distributed data. Our results demonstrate that the federated\nlearning approach achieves high classification accuracy comparable to\ncentralized learning while ensuring data privacy across institutions. This work\nmarks a significant advancement in collaborative IPMN classification,\nfacilitating secure and high-accuracy model training across multiple centers.\n", "link": "http://arxiv.org/abs/2411.05697v1", "date": "2024-11-08", "relevancy": 1.9963, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5227}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4898}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IPMN%20Risk%20Assessment%20under%20Federated%20Learning%20Paradigm&body=Title%3A%20IPMN%20Risk%20Assessment%20under%20Federated%20Learning%20Paradigm%0AAuthor%3A%20Hongyi%20Pan%20and%20Ziliang%20Hong%20and%20Gorkem%20Durak%20and%20Elif%20Keles%20and%20Halil%20Ertugrul%20Aktas%20and%20Yavuz%20Taktak%20and%20Alpay%20Medetalibeyoglu%20and%20Zheyuan%20Zhang%20and%20Yury%20Velichko%20and%20Concetto%20Spampinato%20and%20Ivo%20Schoots%20and%20Marco%20J.%20Bruno%20and%20Pallavi%20Tiwari%20and%20Candice%20Bolan%20and%20Tamas%20Gonda%20and%20Frank%20Miller%20and%20Rajesh%20N.%20Keswani%20and%20Michael%20B.%20Wallace%20and%20Ziyue%20Xu%20and%20Ulas%20Bagci%0AAbstract%3A%20%20%20Accurate%20classification%20of%20Intraductal%20Papillary%20Mucinous%20Neoplasms%20%28IPMN%29%20is%0Aessential%20for%20identifying%20high-risk%20cases%20that%20require%20timely%20intervention.%20In%0Athis%20study%2C%20we%20develop%20a%20federated%20learning%20framework%20for%20multi-center%20IPMN%0Aclassification%20utilizing%20a%20comprehensive%20pancreas%20MRI%20dataset.%20This%20dataset%0Aincludes%20653%20T1-weighted%20and%20656%20T2-weighted%20MRI%20images%2C%20accompanied%20by%0Acorresponding%20IPMN%20risk%20scores%20from%207%20leading%20medical%20institutions%2C%20making%20it%0Athe%20largest%20and%20most%20diverse%20dataset%20for%20IPMN%20classification%20to%20date.%20We%20assess%0Athe%20performance%20of%20DenseNet-121%20in%20both%20centralized%20and%20federated%20settings%20for%0Atraining%20on%20distributed%20data.%20Our%20results%20demonstrate%20that%20the%20federated%0Alearning%20approach%20achieves%20high%20classification%20accuracy%20comparable%20to%0Acentralized%20learning%20while%20ensuring%20data%20privacy%20across%20institutions.%20This%20work%0Amarks%20a%20significant%20advancement%20in%20collaborative%20IPMN%20classification%2C%0Afacilitating%20secure%20and%20high-accuracy%20model%20training%20across%20multiple%20centers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIPMN%2520Risk%2520Assessment%2520under%2520Federated%2520Learning%2520Paradigm%26entry.906535625%3DHongyi%2520Pan%2520and%2520Ziliang%2520Hong%2520and%2520Gorkem%2520Durak%2520and%2520Elif%2520Keles%2520and%2520Halil%2520Ertugrul%2520Aktas%2520and%2520Yavuz%2520Taktak%2520and%2520Alpay%2520Medetalibeyoglu%2520and%2520Zheyuan%2520Zhang%2520and%2520Yury%2520Velichko%2520and%2520Concetto%2520Spampinato%2520and%2520Ivo%2520Schoots%2520and%2520Marco%2520J.%2520Bruno%2520and%2520Pallavi%2520Tiwari%2520and%2520Candice%2520Bolan%2520and%2520Tamas%2520Gonda%2520and%2520Frank%2520Miller%2520and%2520Rajesh%2520N.%2520Keswani%2520and%2520Michael%2520B.%2520Wallace%2520and%2520Ziyue%2520Xu%2520and%2520Ulas%2520Bagci%26entry.1292438233%3D%2520%2520Accurate%2520classification%2520of%2520Intraductal%2520Papillary%2520Mucinous%2520Neoplasms%2520%2528IPMN%2529%2520is%250Aessential%2520for%2520identifying%2520high-risk%2520cases%2520that%2520require%2520timely%2520intervention.%2520In%250Athis%2520study%252C%2520we%2520develop%2520a%2520federated%2520learning%2520framework%2520for%2520multi-center%2520IPMN%250Aclassification%2520utilizing%2520a%2520comprehensive%2520pancreas%2520MRI%2520dataset.%2520This%2520dataset%250Aincludes%2520653%2520T1-weighted%2520and%2520656%2520T2-weighted%2520MRI%2520images%252C%2520accompanied%2520by%250Acorresponding%2520IPMN%2520risk%2520scores%2520from%25207%2520leading%2520medical%2520institutions%252C%2520making%2520it%250Athe%2520largest%2520and%2520most%2520diverse%2520dataset%2520for%2520IPMN%2520classification%2520to%2520date.%2520We%2520assess%250Athe%2520performance%2520of%2520DenseNet-121%2520in%2520both%2520centralized%2520and%2520federated%2520settings%2520for%250Atraining%2520on%2520distributed%2520data.%2520Our%2520results%2520demonstrate%2520that%2520the%2520federated%250Alearning%2520approach%2520achieves%2520high%2520classification%2520accuracy%2520comparable%2520to%250Acentralized%2520learning%2520while%2520ensuring%2520data%2520privacy%2520across%2520institutions.%2520This%2520work%250Amarks%2520a%2520significant%2520advancement%2520in%2520collaborative%2520IPMN%2520classification%252C%250Afacilitating%2520secure%2520and%2520high-accuracy%2520model%2520training%2520across%2520multiple%2520centers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IPMN%20Risk%20Assessment%20under%20Federated%20Learning%20Paradigm&entry.906535625=Hongyi%20Pan%20and%20Ziliang%20Hong%20and%20Gorkem%20Durak%20and%20Elif%20Keles%20and%20Halil%20Ertugrul%20Aktas%20and%20Yavuz%20Taktak%20and%20Alpay%20Medetalibeyoglu%20and%20Zheyuan%20Zhang%20and%20Yury%20Velichko%20and%20Concetto%20Spampinato%20and%20Ivo%20Schoots%20and%20Marco%20J.%20Bruno%20and%20Pallavi%20Tiwari%20and%20Candice%20Bolan%20and%20Tamas%20Gonda%20and%20Frank%20Miller%20and%20Rajesh%20N.%20Keswani%20and%20Michael%20B.%20Wallace%20and%20Ziyue%20Xu%20and%20Ulas%20Bagci&entry.1292438233=%20%20Accurate%20classification%20of%20Intraductal%20Papillary%20Mucinous%20Neoplasms%20%28IPMN%29%20is%0Aessential%20for%20identifying%20high-risk%20cases%20that%20require%20timely%20intervention.%20In%0Athis%20study%2C%20we%20develop%20a%20federated%20learning%20framework%20for%20multi-center%20IPMN%0Aclassification%20utilizing%20a%20comprehensive%20pancreas%20MRI%20dataset.%20This%20dataset%0Aincludes%20653%20T1-weighted%20and%20656%20T2-weighted%20MRI%20images%2C%20accompanied%20by%0Acorresponding%20IPMN%20risk%20scores%20from%207%20leading%20medical%20institutions%2C%20making%20it%0Athe%20largest%20and%20most%20diverse%20dataset%20for%20IPMN%20classification%20to%20date.%20We%20assess%0Athe%20performance%20of%20DenseNet-121%20in%20both%20centralized%20and%20federated%20settings%20for%0Atraining%20on%20distributed%20data.%20Our%20results%20demonstrate%20that%20the%20federated%0Alearning%20approach%20achieves%20high%20classification%20accuracy%20comparable%20to%0Acentralized%20learning%20while%20ensuring%20data%20privacy%20across%20institutions.%20This%20work%0Amarks%20a%20significant%20advancement%20in%20collaborative%20IPMN%20classification%2C%0Afacilitating%20secure%20and%20high-accuracy%20model%20training%20across%20multiple%20centers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05697v1&entry.124074799=Read"},
{"title": "Knowledge Distillation Neural Network for Predicting Car-following\n  Behaviour of Human-driven and Autonomous Vehicles", "author": "Ayobami Adewale and Chris Lee and Amnir Hadachi and Nicolly Lima da Silva", "abstract": "  As we move towards a mixed-traffic scenario of Autonomous vehicles (AVs) and\nHuman-driven vehicles (HDVs), understanding the car-following behaviour is\nimportant to improve traffic efficiency and road safety. Using a real-world\ntrajectory dataset, this study uses descriptive and statistical analysis to\ninvestigate the car-following behaviours of three vehicle pairs: HDV-AV, AV-HDV\nand HDV-HDV in mixed traffic. The ANOVA test showed that car-following\nbehaviours across different vehicle pairs are statistically significant\n(p-value < 0.05).\n  We also introduce a data-driven Knowledge Distillation Neural Network (KDNN)\nmodel for predicting car-following behaviour in terms of speed. The KDNN model\ndemonstrates comparable predictive accuracy to its teacher network, a Long\nShort-Term Memory (LSTM) network, and outperforms both the standalone student\nnetwork, a Multilayer Perceptron (MLP), and traditional physics-based models\nlike the Gipps model. Notably, the KDNN model better prevents collisions,\nmeasured by minimum Time-to-Collision (TTC), and operates with lower\ncomputational power, making it ideal for AVs or driving simulators requiring\nefficient computing.\n", "link": "http://arxiv.org/abs/2411.05618v1", "date": "2024-11-08", "relevancy": 1.9953, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5157}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5028}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Distillation%20Neural%20Network%20for%20Predicting%20Car-following%0A%20%20Behaviour%20of%20Human-driven%20and%20Autonomous%20Vehicles&body=Title%3A%20Knowledge%20Distillation%20Neural%20Network%20for%20Predicting%20Car-following%0A%20%20Behaviour%20of%20Human-driven%20and%20Autonomous%20Vehicles%0AAuthor%3A%20Ayobami%20Adewale%20and%20Chris%20Lee%20and%20Amnir%20Hadachi%20and%20Nicolly%20Lima%20da%20Silva%0AAbstract%3A%20%20%20As%20we%20move%20towards%20a%20mixed-traffic%20scenario%20of%20Autonomous%20vehicles%20%28AVs%29%20and%0AHuman-driven%20vehicles%20%28HDVs%29%2C%20understanding%20the%20car-following%20behaviour%20is%0Aimportant%20to%20improve%20traffic%20efficiency%20and%20road%20safety.%20Using%20a%20real-world%0Atrajectory%20dataset%2C%20this%20study%20uses%20descriptive%20and%20statistical%20analysis%20to%0Ainvestigate%20the%20car-following%20behaviours%20of%20three%20vehicle%20pairs%3A%20HDV-AV%2C%20AV-HDV%0Aand%20HDV-HDV%20in%20mixed%20traffic.%20The%20ANOVA%20test%20showed%20that%20car-following%0Abehaviours%20across%20different%20vehicle%20pairs%20are%20statistically%20significant%0A%28p-value%20%3C%200.05%29.%0A%20%20We%20also%20introduce%20a%20data-driven%20Knowledge%20Distillation%20Neural%20Network%20%28KDNN%29%0Amodel%20for%20predicting%20car-following%20behaviour%20in%20terms%20of%20speed.%20The%20KDNN%20model%0Ademonstrates%20comparable%20predictive%20accuracy%20to%20its%20teacher%20network%2C%20a%20Long%0AShort-Term%20Memory%20%28LSTM%29%20network%2C%20and%20outperforms%20both%20the%20standalone%20student%0Anetwork%2C%20a%20Multilayer%20Perceptron%20%28MLP%29%2C%20and%20traditional%20physics-based%20models%0Alike%20the%20Gipps%20model.%20Notably%2C%20the%20KDNN%20model%20better%20prevents%20collisions%2C%0Ameasured%20by%20minimum%20Time-to-Collision%20%28TTC%29%2C%20and%20operates%20with%20lower%0Acomputational%20power%2C%20making%20it%20ideal%20for%20AVs%20or%20driving%20simulators%20requiring%0Aefficient%20computing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05618v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Distillation%2520Neural%2520Network%2520for%2520Predicting%2520Car-following%250A%2520%2520Behaviour%2520of%2520Human-driven%2520and%2520Autonomous%2520Vehicles%26entry.906535625%3DAyobami%2520Adewale%2520and%2520Chris%2520Lee%2520and%2520Amnir%2520Hadachi%2520and%2520Nicolly%2520Lima%2520da%2520Silva%26entry.1292438233%3D%2520%2520As%2520we%2520move%2520towards%2520a%2520mixed-traffic%2520scenario%2520of%2520Autonomous%2520vehicles%2520%2528AVs%2529%2520and%250AHuman-driven%2520vehicles%2520%2528HDVs%2529%252C%2520understanding%2520the%2520car-following%2520behaviour%2520is%250Aimportant%2520to%2520improve%2520traffic%2520efficiency%2520and%2520road%2520safety.%2520Using%2520a%2520real-world%250Atrajectory%2520dataset%252C%2520this%2520study%2520uses%2520descriptive%2520and%2520statistical%2520analysis%2520to%250Ainvestigate%2520the%2520car-following%2520behaviours%2520of%2520three%2520vehicle%2520pairs%253A%2520HDV-AV%252C%2520AV-HDV%250Aand%2520HDV-HDV%2520in%2520mixed%2520traffic.%2520The%2520ANOVA%2520test%2520showed%2520that%2520car-following%250Abehaviours%2520across%2520different%2520vehicle%2520pairs%2520are%2520statistically%2520significant%250A%2528p-value%2520%253C%25200.05%2529.%250A%2520%2520We%2520also%2520introduce%2520a%2520data-driven%2520Knowledge%2520Distillation%2520Neural%2520Network%2520%2528KDNN%2529%250Amodel%2520for%2520predicting%2520car-following%2520behaviour%2520in%2520terms%2520of%2520speed.%2520The%2520KDNN%2520model%250Ademonstrates%2520comparable%2520predictive%2520accuracy%2520to%2520its%2520teacher%2520network%252C%2520a%2520Long%250AShort-Term%2520Memory%2520%2528LSTM%2529%2520network%252C%2520and%2520outperforms%2520both%2520the%2520standalone%2520student%250Anetwork%252C%2520a%2520Multilayer%2520Perceptron%2520%2528MLP%2529%252C%2520and%2520traditional%2520physics-based%2520models%250Alike%2520the%2520Gipps%2520model.%2520Notably%252C%2520the%2520KDNN%2520model%2520better%2520prevents%2520collisions%252C%250Ameasured%2520by%2520minimum%2520Time-to-Collision%2520%2528TTC%2529%252C%2520and%2520operates%2520with%2520lower%250Acomputational%2520power%252C%2520making%2520it%2520ideal%2520for%2520AVs%2520or%2520driving%2520simulators%2520requiring%250Aefficient%2520computing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05618v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Distillation%20Neural%20Network%20for%20Predicting%20Car-following%0A%20%20Behaviour%20of%20Human-driven%20and%20Autonomous%20Vehicles&entry.906535625=Ayobami%20Adewale%20and%20Chris%20Lee%20and%20Amnir%20Hadachi%20and%20Nicolly%20Lima%20da%20Silva&entry.1292438233=%20%20As%20we%20move%20towards%20a%20mixed-traffic%20scenario%20of%20Autonomous%20vehicles%20%28AVs%29%20and%0AHuman-driven%20vehicles%20%28HDVs%29%2C%20understanding%20the%20car-following%20behaviour%20is%0Aimportant%20to%20improve%20traffic%20efficiency%20and%20road%20safety.%20Using%20a%20real-world%0Atrajectory%20dataset%2C%20this%20study%20uses%20descriptive%20and%20statistical%20analysis%20to%0Ainvestigate%20the%20car-following%20behaviours%20of%20three%20vehicle%20pairs%3A%20HDV-AV%2C%20AV-HDV%0Aand%20HDV-HDV%20in%20mixed%20traffic.%20The%20ANOVA%20test%20showed%20that%20car-following%0Abehaviours%20across%20different%20vehicle%20pairs%20are%20statistically%20significant%0A%28p-value%20%3C%200.05%29.%0A%20%20We%20also%20introduce%20a%20data-driven%20Knowledge%20Distillation%20Neural%20Network%20%28KDNN%29%0Amodel%20for%20predicting%20car-following%20behaviour%20in%20terms%20of%20speed.%20The%20KDNN%20model%0Ademonstrates%20comparable%20predictive%20accuracy%20to%20its%20teacher%20network%2C%20a%20Long%0AShort-Term%20Memory%20%28LSTM%29%20network%2C%20and%20outperforms%20both%20the%20standalone%20student%0Anetwork%2C%20a%20Multilayer%20Perceptron%20%28MLP%29%2C%20and%20traditional%20physics-based%20models%0Alike%20the%20Gipps%20model.%20Notably%2C%20the%20KDNN%20model%20better%20prevents%20collisions%2C%0Ameasured%20by%20minimum%20Time-to-Collision%20%28TTC%29%2C%20and%20operates%20with%20lower%0Acomputational%20power%2C%20making%20it%20ideal%20for%20AVs%20or%20driving%20simulators%20requiring%0Aefficient%20computing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05618v1&entry.124074799=Read"},
{"title": "Fairness-Aware Estimation of Graphical Models", "author": "Zhuoping Zhou and Davoud Ataee Tarzanagh and Bojian Hou and Qi Long and Li Shen", "abstract": "  This paper examines the issue of fairness in the estimation of graphical\nmodels (GMs), particularly Gaussian, Covariance, and Ising models. These models\nplay a vital role in understanding complex relationships in high-dimensional\ndata. However, standard GMs can result in biased outcomes, especially when the\nunderlying data involves sensitive characteristics or protected groups. To\naddress this, we introduce a comprehensive framework designed to reduce bias in\nthe estimation of GMs related to protected attributes. Our approach involves\nthe integration of the pairwise graph disparity error and a tailored loss\nfunction into a nonsmooth multi-objective optimization problem, striving to\nachieve fairness across different sensitive groups while maintaining the\neffectiveness of the GMs. Experimental evaluations on synthetic and real-world\ndatasets demonstrate that our framework effectively mitigates bias without\nundermining GMs' performance.\n", "link": "http://arxiv.org/abs/2408.17396v2", "date": "2024-11-08", "relevancy": 1.9824, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4975}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4958}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4904}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fairness-Aware%20Estimation%20of%20Graphical%20Models&body=Title%3A%20Fairness-Aware%20Estimation%20of%20Graphical%20Models%0AAuthor%3A%20Zhuoping%20Zhou%20and%20Davoud%20Ataee%20Tarzanagh%20and%20Bojian%20Hou%20and%20Qi%20Long%20and%20Li%20Shen%0AAbstract%3A%20%20%20This%20paper%20examines%20the%20issue%20of%20fairness%20in%20the%20estimation%20of%20graphical%0Amodels%20%28GMs%29%2C%20particularly%20Gaussian%2C%20Covariance%2C%20and%20Ising%20models.%20These%20models%0Aplay%20a%20vital%20role%20in%20understanding%20complex%20relationships%20in%20high-dimensional%0Adata.%20However%2C%20standard%20GMs%20can%20result%20in%20biased%20outcomes%2C%20especially%20when%20the%0Aunderlying%20data%20involves%20sensitive%20characteristics%20or%20protected%20groups.%20To%0Aaddress%20this%2C%20we%20introduce%20a%20comprehensive%20framework%20designed%20to%20reduce%20bias%20in%0Athe%20estimation%20of%20GMs%20related%20to%20protected%20attributes.%20Our%20approach%20involves%0Athe%20integration%20of%20the%20pairwise%20graph%20disparity%20error%20and%20a%20tailored%20loss%0Afunction%20into%20a%20nonsmooth%20multi-objective%20optimization%20problem%2C%20striving%20to%0Aachieve%20fairness%20across%20different%20sensitive%20groups%20while%20maintaining%20the%0Aeffectiveness%20of%20the%20GMs.%20Experimental%20evaluations%20on%20synthetic%20and%20real-world%0Adatasets%20demonstrate%20that%20our%20framework%20effectively%20mitigates%20bias%20without%0Aundermining%20GMs%27%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.17396v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFairness-Aware%2520Estimation%2520of%2520Graphical%2520Models%26entry.906535625%3DZhuoping%2520Zhou%2520and%2520Davoud%2520Ataee%2520Tarzanagh%2520and%2520Bojian%2520Hou%2520and%2520Qi%2520Long%2520and%2520Li%2520Shen%26entry.1292438233%3D%2520%2520This%2520paper%2520examines%2520the%2520issue%2520of%2520fairness%2520in%2520the%2520estimation%2520of%2520graphical%250Amodels%2520%2528GMs%2529%252C%2520particularly%2520Gaussian%252C%2520Covariance%252C%2520and%2520Ising%2520models.%2520These%2520models%250Aplay%2520a%2520vital%2520role%2520in%2520understanding%2520complex%2520relationships%2520in%2520high-dimensional%250Adata.%2520However%252C%2520standard%2520GMs%2520can%2520result%2520in%2520biased%2520outcomes%252C%2520especially%2520when%2520the%250Aunderlying%2520data%2520involves%2520sensitive%2520characteristics%2520or%2520protected%2520groups.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520a%2520comprehensive%2520framework%2520designed%2520to%2520reduce%2520bias%2520in%250Athe%2520estimation%2520of%2520GMs%2520related%2520to%2520protected%2520attributes.%2520Our%2520approach%2520involves%250Athe%2520integration%2520of%2520the%2520pairwise%2520graph%2520disparity%2520error%2520and%2520a%2520tailored%2520loss%250Afunction%2520into%2520a%2520nonsmooth%2520multi-objective%2520optimization%2520problem%252C%2520striving%2520to%250Aachieve%2520fairness%2520across%2520different%2520sensitive%2520groups%2520while%2520maintaining%2520the%250Aeffectiveness%2520of%2520the%2520GMs.%2520Experimental%2520evaluations%2520on%2520synthetic%2520and%2520real-world%250Adatasets%2520demonstrate%2520that%2520our%2520framework%2520effectively%2520mitigates%2520bias%2520without%250Aundermining%2520GMs%2527%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.17396v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fairness-Aware%20Estimation%20of%20Graphical%20Models&entry.906535625=Zhuoping%20Zhou%20and%20Davoud%20Ataee%20Tarzanagh%20and%20Bojian%20Hou%20and%20Qi%20Long%20and%20Li%20Shen&entry.1292438233=%20%20This%20paper%20examines%20the%20issue%20of%20fairness%20in%20the%20estimation%20of%20graphical%0Amodels%20%28GMs%29%2C%20particularly%20Gaussian%2C%20Covariance%2C%20and%20Ising%20models.%20These%20models%0Aplay%20a%20vital%20role%20in%20understanding%20complex%20relationships%20in%20high-dimensional%0Adata.%20However%2C%20standard%20GMs%20can%20result%20in%20biased%20outcomes%2C%20especially%20when%20the%0Aunderlying%20data%20involves%20sensitive%20characteristics%20or%20protected%20groups.%20To%0Aaddress%20this%2C%20we%20introduce%20a%20comprehensive%20framework%20designed%20to%20reduce%20bias%20in%0Athe%20estimation%20of%20GMs%20related%20to%20protected%20attributes.%20Our%20approach%20involves%0Athe%20integration%20of%20the%20pairwise%20graph%20disparity%20error%20and%20a%20tailored%20loss%0Afunction%20into%20a%20nonsmooth%20multi-objective%20optimization%20problem%2C%20striving%20to%0Aachieve%20fairness%20across%20different%20sensitive%20groups%20while%20maintaining%20the%0Aeffectiveness%20of%20the%20GMs.%20Experimental%20evaluations%20on%20synthetic%20and%20real-world%0Adatasets%20demonstrate%20that%20our%20framework%20effectively%20mitigates%20bias%20without%0Aundermining%20GMs%27%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.17396v2&entry.124074799=Read"},
{"title": "Almost Surely Asymptotically Constant Graph Neural Networks", "author": "Sam Adam-Day and Michael Benedikt and \u0130smail \u0130lkan Ceylan and Ben Finkelshtein", "abstract": "  We present a new angle on the expressive power of graph neural networks\n(GNNs) by studying how the predictions of real-valued GNN classifiers, such as\nthose classifying graphs probabilistically, evolve as we apply them on larger\ngraphs drawn from some random graph model. We show that the output converges to\na constant function, which upper-bounds what these classifiers can uniformly\nexpress. This strong convergence phenomenon applies to a very wide class of\nGNNs, including state of the art models, with aggregates including mean and the\nattention-based mechanism of graph transformers. Our results apply to a broad\nclass of random graph models, including sparse and dense variants of the\nErd\\H{o}s-R\\'enyi model, the stochastic block model, and the Barab\\'asi-Albert\nmodel. We empirically validate these findings, observing that the convergence\nphenomenon appears not only on random graphs but also on some real-world\ngraphs.\n", "link": "http://arxiv.org/abs/2403.03880v3", "date": "2024-11-08", "relevancy": 1.9812, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5259}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4757}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4678}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Almost%20Surely%20Asymptotically%20Constant%20Graph%20Neural%20Networks&body=Title%3A%20Almost%20Surely%20Asymptotically%20Constant%20Graph%20Neural%20Networks%0AAuthor%3A%20Sam%20Adam-Day%20and%20Michael%20Benedikt%20and%20%C4%B0smail%20%C4%B0lkan%20Ceylan%20and%20Ben%20Finkelshtein%0AAbstract%3A%20%20%20We%20present%20a%20new%20angle%20on%20the%20expressive%20power%20of%20graph%20neural%20networks%0A%28GNNs%29%20by%20studying%20how%20the%20predictions%20of%20real-valued%20GNN%20classifiers%2C%20such%20as%0Athose%20classifying%20graphs%20probabilistically%2C%20evolve%20as%20we%20apply%20them%20on%20larger%0Agraphs%20drawn%20from%20some%20random%20graph%20model.%20We%20show%20that%20the%20output%20converges%20to%0Aa%20constant%20function%2C%20which%20upper-bounds%20what%20these%20classifiers%20can%20uniformly%0Aexpress.%20This%20strong%20convergence%20phenomenon%20applies%20to%20a%20very%20wide%20class%20of%0AGNNs%2C%20including%20state%20of%20the%20art%20models%2C%20with%20aggregates%20including%20mean%20and%20the%0Aattention-based%20mechanism%20of%20graph%20transformers.%20Our%20results%20apply%20to%20a%20broad%0Aclass%20of%20random%20graph%20models%2C%20including%20sparse%20and%20dense%20variants%20of%20the%0AErd%5CH%7Bo%7Ds-R%5C%27enyi%20model%2C%20the%20stochastic%20block%20model%2C%20and%20the%20Barab%5C%27asi-Albert%0Amodel.%20We%20empirically%20validate%20these%20findings%2C%20observing%20that%20the%20convergence%0Aphenomenon%20appears%20not%20only%20on%20random%20graphs%20but%20also%20on%20some%20real-world%0Agraphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03880v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlmost%2520Surely%2520Asymptotically%2520Constant%2520Graph%2520Neural%2520Networks%26entry.906535625%3DSam%2520Adam-Day%2520and%2520Michael%2520Benedikt%2520and%2520%25C4%25B0smail%2520%25C4%25B0lkan%2520Ceylan%2520and%2520Ben%2520Finkelshtein%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520angle%2520on%2520the%2520expressive%2520power%2520of%2520graph%2520neural%2520networks%250A%2528GNNs%2529%2520by%2520studying%2520how%2520the%2520predictions%2520of%2520real-valued%2520GNN%2520classifiers%252C%2520such%2520as%250Athose%2520classifying%2520graphs%2520probabilistically%252C%2520evolve%2520as%2520we%2520apply%2520them%2520on%2520larger%250Agraphs%2520drawn%2520from%2520some%2520random%2520graph%2520model.%2520We%2520show%2520that%2520the%2520output%2520converges%2520to%250Aa%2520constant%2520function%252C%2520which%2520upper-bounds%2520what%2520these%2520classifiers%2520can%2520uniformly%250Aexpress.%2520This%2520strong%2520convergence%2520phenomenon%2520applies%2520to%2520a%2520very%2520wide%2520class%2520of%250AGNNs%252C%2520including%2520state%2520of%2520the%2520art%2520models%252C%2520with%2520aggregates%2520including%2520mean%2520and%2520the%250Aattention-based%2520mechanism%2520of%2520graph%2520transformers.%2520Our%2520results%2520apply%2520to%2520a%2520broad%250Aclass%2520of%2520random%2520graph%2520models%252C%2520including%2520sparse%2520and%2520dense%2520variants%2520of%2520the%250AErd%255CH%257Bo%257Ds-R%255C%2527enyi%2520model%252C%2520the%2520stochastic%2520block%2520model%252C%2520and%2520the%2520Barab%255C%2527asi-Albert%250Amodel.%2520We%2520empirically%2520validate%2520these%2520findings%252C%2520observing%2520that%2520the%2520convergence%250Aphenomenon%2520appears%2520not%2520only%2520on%2520random%2520graphs%2520but%2520also%2520on%2520some%2520real-world%250Agraphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03880v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Almost%20Surely%20Asymptotically%20Constant%20Graph%20Neural%20Networks&entry.906535625=Sam%20Adam-Day%20and%20Michael%20Benedikt%20and%20%C4%B0smail%20%C4%B0lkan%20Ceylan%20and%20Ben%20Finkelshtein&entry.1292438233=%20%20We%20present%20a%20new%20angle%20on%20the%20expressive%20power%20of%20graph%20neural%20networks%0A%28GNNs%29%20by%20studying%20how%20the%20predictions%20of%20real-valued%20GNN%20classifiers%2C%20such%20as%0Athose%20classifying%20graphs%20probabilistically%2C%20evolve%20as%20we%20apply%20them%20on%20larger%0Agraphs%20drawn%20from%20some%20random%20graph%20model.%20We%20show%20that%20the%20output%20converges%20to%0Aa%20constant%20function%2C%20which%20upper-bounds%20what%20these%20classifiers%20can%20uniformly%0Aexpress.%20This%20strong%20convergence%20phenomenon%20applies%20to%20a%20very%20wide%20class%20of%0AGNNs%2C%20including%20state%20of%20the%20art%20models%2C%20with%20aggregates%20including%20mean%20and%20the%0Aattention-based%20mechanism%20of%20graph%20transformers.%20Our%20results%20apply%20to%20a%20broad%0Aclass%20of%20random%20graph%20models%2C%20including%20sparse%20and%20dense%20variants%20of%20the%0AErd%5CH%7Bo%7Ds-R%5C%27enyi%20model%2C%20the%20stochastic%20block%20model%2C%20and%20the%20Barab%5C%27asi-Albert%0Amodel.%20We%20empirically%20validate%20these%20findings%2C%20observing%20that%20the%20convergence%0Aphenomenon%20appears%20not%20only%20on%20random%20graphs%20but%20also%20on%20some%20real-world%0Agraphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03880v3&entry.124074799=Read"},
{"title": "Initial Guessing Bias: How Untrained Networks Favor Some Classes", "author": "Emanuele Francazi and Aurelien Lucchi and Marco Baity-Jesi", "abstract": "  Understanding and controlling biasing effects in neural networks is crucial\nfor ensuring accurate and fair model performance. In the context of\nclassification problems, we provide a theoretical analysis demonstrating that\nthe structure of a deep neural network (DNN) can condition the model to assign\nall predictions to the same class, even before the beginning of training, and\nin the absence of explicit biases. We prove that, besides dataset properties,\nthe presence of this phenomenon, which we call \\textit{Initial Guessing Bias}\n(IGB), is influenced by model choices including dataset preprocessing methods,\nand architectural decisions, such as activation functions, max-pooling layers,\nand network depth. Our analysis of IGB provides information for architecture\nselection and model initialization. We also highlight theoretical consequences,\nsuch as the breakdown of node-permutation symmetry, the violation of\nself-averaging and the non-trivial effects that depth has on the phenomenon.\n", "link": "http://arxiv.org/abs/2306.00809v6", "date": "2024-11-08", "relevancy": 1.9793, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5465}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4609}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Initial%20Guessing%20Bias%3A%20How%20Untrained%20Networks%20Favor%20Some%20Classes&body=Title%3A%20Initial%20Guessing%20Bias%3A%20How%20Untrained%20Networks%20Favor%20Some%20Classes%0AAuthor%3A%20Emanuele%20Francazi%20and%20Aurelien%20Lucchi%20and%20Marco%20Baity-Jesi%0AAbstract%3A%20%20%20Understanding%20and%20controlling%20biasing%20effects%20in%20neural%20networks%20is%20crucial%0Afor%20ensuring%20accurate%20and%20fair%20model%20performance.%20In%20the%20context%20of%0Aclassification%20problems%2C%20we%20provide%20a%20theoretical%20analysis%20demonstrating%20that%0Athe%20structure%20of%20a%20deep%20neural%20network%20%28DNN%29%20can%20condition%20the%20model%20to%20assign%0Aall%20predictions%20to%20the%20same%20class%2C%20even%20before%20the%20beginning%20of%20training%2C%20and%0Ain%20the%20absence%20of%20explicit%20biases.%20We%20prove%20that%2C%20besides%20dataset%20properties%2C%0Athe%20presence%20of%20this%20phenomenon%2C%20which%20we%20call%20%5Ctextit%7BInitial%20Guessing%20Bias%7D%0A%28IGB%29%2C%20is%20influenced%20by%20model%20choices%20including%20dataset%20preprocessing%20methods%2C%0Aand%20architectural%20decisions%2C%20such%20as%20activation%20functions%2C%20max-pooling%20layers%2C%0Aand%20network%20depth.%20Our%20analysis%20of%20IGB%20provides%20information%20for%20architecture%0Aselection%20and%20model%20initialization.%20We%20also%20highlight%20theoretical%20consequences%2C%0Asuch%20as%20the%20breakdown%20of%20node-permutation%20symmetry%2C%20the%20violation%20of%0Aself-averaging%20and%20the%20non-trivial%20effects%20that%20depth%20has%20on%20the%20phenomenon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.00809v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInitial%2520Guessing%2520Bias%253A%2520How%2520Untrained%2520Networks%2520Favor%2520Some%2520Classes%26entry.906535625%3DEmanuele%2520Francazi%2520and%2520Aurelien%2520Lucchi%2520and%2520Marco%2520Baity-Jesi%26entry.1292438233%3D%2520%2520Understanding%2520and%2520controlling%2520biasing%2520effects%2520in%2520neural%2520networks%2520is%2520crucial%250Afor%2520ensuring%2520accurate%2520and%2520fair%2520model%2520performance.%2520In%2520the%2520context%2520of%250Aclassification%2520problems%252C%2520we%2520provide%2520a%2520theoretical%2520analysis%2520demonstrating%2520that%250Athe%2520structure%2520of%2520a%2520deep%2520neural%2520network%2520%2528DNN%2529%2520can%2520condition%2520the%2520model%2520to%2520assign%250Aall%2520predictions%2520to%2520the%2520same%2520class%252C%2520even%2520before%2520the%2520beginning%2520of%2520training%252C%2520and%250Ain%2520the%2520absence%2520of%2520explicit%2520biases.%2520We%2520prove%2520that%252C%2520besides%2520dataset%2520properties%252C%250Athe%2520presence%2520of%2520this%2520phenomenon%252C%2520which%2520we%2520call%2520%255Ctextit%257BInitial%2520Guessing%2520Bias%257D%250A%2528IGB%2529%252C%2520is%2520influenced%2520by%2520model%2520choices%2520including%2520dataset%2520preprocessing%2520methods%252C%250Aand%2520architectural%2520decisions%252C%2520such%2520as%2520activation%2520functions%252C%2520max-pooling%2520layers%252C%250Aand%2520network%2520depth.%2520Our%2520analysis%2520of%2520IGB%2520provides%2520information%2520for%2520architecture%250Aselection%2520and%2520model%2520initialization.%2520We%2520also%2520highlight%2520theoretical%2520consequences%252C%250Asuch%2520as%2520the%2520breakdown%2520of%2520node-permutation%2520symmetry%252C%2520the%2520violation%2520of%250Aself-averaging%2520and%2520the%2520non-trivial%2520effects%2520that%2520depth%2520has%2520on%2520the%2520phenomenon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.00809v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Initial%20Guessing%20Bias%3A%20How%20Untrained%20Networks%20Favor%20Some%20Classes&entry.906535625=Emanuele%20Francazi%20and%20Aurelien%20Lucchi%20and%20Marco%20Baity-Jesi&entry.1292438233=%20%20Understanding%20and%20controlling%20biasing%20effects%20in%20neural%20networks%20is%20crucial%0Afor%20ensuring%20accurate%20and%20fair%20model%20performance.%20In%20the%20context%20of%0Aclassification%20problems%2C%20we%20provide%20a%20theoretical%20analysis%20demonstrating%20that%0Athe%20structure%20of%20a%20deep%20neural%20network%20%28DNN%29%20can%20condition%20the%20model%20to%20assign%0Aall%20predictions%20to%20the%20same%20class%2C%20even%20before%20the%20beginning%20of%20training%2C%20and%0Ain%20the%20absence%20of%20explicit%20biases.%20We%20prove%20that%2C%20besides%20dataset%20properties%2C%0Athe%20presence%20of%20this%20phenomenon%2C%20which%20we%20call%20%5Ctextit%7BInitial%20Guessing%20Bias%7D%0A%28IGB%29%2C%20is%20influenced%20by%20model%20choices%20including%20dataset%20preprocessing%20methods%2C%0Aand%20architectural%20decisions%2C%20such%20as%20activation%20functions%2C%20max-pooling%20layers%2C%0Aand%20network%20depth.%20Our%20analysis%20of%20IGB%20provides%20information%20for%20architecture%0Aselection%20and%20model%20initialization.%20We%20also%20highlight%20theoretical%20consequences%2C%0Asuch%20as%20the%20breakdown%20of%20node-permutation%20symmetry%2C%20the%20violation%20of%0Aself-averaging%20and%20the%20non-trivial%20effects%20that%20depth%20has%20on%20the%20phenomenon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.00809v6&entry.124074799=Read"},
{"title": "STARS: Sensor-agnostic Transformer Architecture for Remote Sensing", "author": "Ethan King and Jaime Rodriguez and Diego Llanes and Timothy Doster and Tegan Emerson and James Koch", "abstract": "  We present a sensor-agnostic spectral transformer as the basis for spectral\nfoundation models. To that end, we introduce a Universal Spectral\nRepresentation (USR) that leverages sensor meta-data, such as sensing kernel\nspecifications and sensing wavelengths, to encode spectra obtained from any\nspectral instrument into a common representation, such that a single model can\ningest data from any sensor. Furthermore, we develop a methodology for\npre-training such models in a self-supervised manner using a novel random\nsensor-augmentation and reconstruction pipeline to learn spectral features\nindependent of the sensing paradigm. We demonstrate that our architecture can\nlearn sensor independent spectral features that generalize effectively to\nsensors not seen during training. This work sets the stage for training\nfoundation models that can both leverage and be effective for the growing\ndiversity of spectral data.\n", "link": "http://arxiv.org/abs/2411.05714v1", "date": "2024-11-08", "relevancy": 1.9666, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5096}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5089}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STARS%3A%20Sensor-agnostic%20Transformer%20Architecture%20for%20Remote%20Sensing&body=Title%3A%20STARS%3A%20Sensor-agnostic%20Transformer%20Architecture%20for%20Remote%20Sensing%0AAuthor%3A%20Ethan%20King%20and%20Jaime%20Rodriguez%20and%20Diego%20Llanes%20and%20Timothy%20Doster%20and%20Tegan%20Emerson%20and%20James%20Koch%0AAbstract%3A%20%20%20We%20present%20a%20sensor-agnostic%20spectral%20transformer%20as%20the%20basis%20for%20spectral%0Afoundation%20models.%20To%20that%20end%2C%20we%20introduce%20a%20Universal%20Spectral%0ARepresentation%20%28USR%29%20that%20leverages%20sensor%20meta-data%2C%20such%20as%20sensing%20kernel%0Aspecifications%20and%20sensing%20wavelengths%2C%20to%20encode%20spectra%20obtained%20from%20any%0Aspectral%20instrument%20into%20a%20common%20representation%2C%20such%20that%20a%20single%20model%20can%0Aingest%20data%20from%20any%20sensor.%20Furthermore%2C%20we%20develop%20a%20methodology%20for%0Apre-training%20such%20models%20in%20a%20self-supervised%20manner%20using%20a%20novel%20random%0Asensor-augmentation%20and%20reconstruction%20pipeline%20to%20learn%20spectral%20features%0Aindependent%20of%20the%20sensing%20paradigm.%20We%20demonstrate%20that%20our%20architecture%20can%0Alearn%20sensor%20independent%20spectral%20features%20that%20generalize%20effectively%20to%0Asensors%20not%20seen%20during%20training.%20This%20work%20sets%20the%20stage%20for%20training%0Afoundation%20models%20that%20can%20both%20leverage%20and%20be%20effective%20for%20the%20growing%0Adiversity%20of%20spectral%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTARS%253A%2520Sensor-agnostic%2520Transformer%2520Architecture%2520for%2520Remote%2520Sensing%26entry.906535625%3DEthan%2520King%2520and%2520Jaime%2520Rodriguez%2520and%2520Diego%2520Llanes%2520and%2520Timothy%2520Doster%2520and%2520Tegan%2520Emerson%2520and%2520James%2520Koch%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520sensor-agnostic%2520spectral%2520transformer%2520as%2520the%2520basis%2520for%2520spectral%250Afoundation%2520models.%2520To%2520that%2520end%252C%2520we%2520introduce%2520a%2520Universal%2520Spectral%250ARepresentation%2520%2528USR%2529%2520that%2520leverages%2520sensor%2520meta-data%252C%2520such%2520as%2520sensing%2520kernel%250Aspecifications%2520and%2520sensing%2520wavelengths%252C%2520to%2520encode%2520spectra%2520obtained%2520from%2520any%250Aspectral%2520instrument%2520into%2520a%2520common%2520representation%252C%2520such%2520that%2520a%2520single%2520model%2520can%250Aingest%2520data%2520from%2520any%2520sensor.%2520Furthermore%252C%2520we%2520develop%2520a%2520methodology%2520for%250Apre-training%2520such%2520models%2520in%2520a%2520self-supervised%2520manner%2520using%2520a%2520novel%2520random%250Asensor-augmentation%2520and%2520reconstruction%2520pipeline%2520to%2520learn%2520spectral%2520features%250Aindependent%2520of%2520the%2520sensing%2520paradigm.%2520We%2520demonstrate%2520that%2520our%2520architecture%2520can%250Alearn%2520sensor%2520independent%2520spectral%2520features%2520that%2520generalize%2520effectively%2520to%250Asensors%2520not%2520seen%2520during%2520training.%2520This%2520work%2520sets%2520the%2520stage%2520for%2520training%250Afoundation%2520models%2520that%2520can%2520both%2520leverage%2520and%2520be%2520effective%2520for%2520the%2520growing%250Adiversity%2520of%2520spectral%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STARS%3A%20Sensor-agnostic%20Transformer%20Architecture%20for%20Remote%20Sensing&entry.906535625=Ethan%20King%20and%20Jaime%20Rodriguez%20and%20Diego%20Llanes%20and%20Timothy%20Doster%20and%20Tegan%20Emerson%20and%20James%20Koch&entry.1292438233=%20%20We%20present%20a%20sensor-agnostic%20spectral%20transformer%20as%20the%20basis%20for%20spectral%0Afoundation%20models.%20To%20that%20end%2C%20we%20introduce%20a%20Universal%20Spectral%0ARepresentation%20%28USR%29%20that%20leverages%20sensor%20meta-data%2C%20such%20as%20sensing%20kernel%0Aspecifications%20and%20sensing%20wavelengths%2C%20to%20encode%20spectra%20obtained%20from%20any%0Aspectral%20instrument%20into%20a%20common%20representation%2C%20such%20that%20a%20single%20model%20can%0Aingest%20data%20from%20any%20sensor.%20Furthermore%2C%20we%20develop%20a%20methodology%20for%0Apre-training%20such%20models%20in%20a%20self-supervised%20manner%20using%20a%20novel%20random%0Asensor-augmentation%20and%20reconstruction%20pipeline%20to%20learn%20spectral%20features%0Aindependent%20of%20the%20sensing%20paradigm.%20We%20demonstrate%20that%20our%20architecture%20can%0Alearn%20sensor%20independent%20spectral%20features%20that%20generalize%20effectively%20to%0Asensors%20not%20seen%20during%20training.%20This%20work%20sets%20the%20stage%20for%20training%0Afoundation%20models%20that%20can%20both%20leverage%20and%20be%20effective%20for%20the%20growing%0Adiversity%20of%20spectral%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05714v1&entry.124074799=Read"},
{"title": "Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on\n  Tasks where Thinking Makes Humans Worse", "author": "Ryan Liu and Jiayi Geng and Addison J. Wu and Ilia Sucholutsky and Tania Lombrozo and Thomas L. Griffiths", "abstract": "  Chain-of-thought (CoT) prompting has become a widely used strategy for\nworking with large language and multimodal models. While CoT has been shown to\nimprove performance across many tasks, determining the settings in which it is\neffective remains an ongoing effort. In particular, it is still an open\nquestion in what settings CoT systematically reduces model performance. In this\npaper, we seek to identify the characteristics of tasks where CoT reduces\nperformance by drawing inspiration from cognitive psychology, looking at cases\nwhere (i) verbal thinking or deliberation hurts performance in humans, and (ii)\nthe constraints governing human performance generalize to language models.\nThree such cases are implicit statistical learning, visual recognition, and\nclassifying with patterns containing exceptions. In extensive experiments\nacross all three settings, we find that a diverse collection of\nstate-of-the-art models exhibit significant drop-offs in performance (e.g., up\nto 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using\ninference-time reasoning compared to zero-shot counterparts. We also identify\nthree tasks that satisfy condition (i) but not (ii), and find that while verbal\nthinking reduces human performance in these tasks, CoT retains or increases\nmodel performance. Overall, our results show that while there is not an exact\nparallel between the cognitive processes of models and those of humans,\nconsidering cases where thinking has negative consequences for human\nperformance can help us identify settings where it negatively impacts models.\nBy connecting the literature on human deliberation with evaluations of CoT, we\noffer a new tool that can be used in understanding the impact of prompt choices\nand inference-time reasoning.\n", "link": "http://arxiv.org/abs/2410.21333v3", "date": "2024-11-08", "relevancy": 1.9651, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4938}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4938}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20Your%20Step%20%28by%20Step%29%3A%20Chain-of-Thought%20can%20Reduce%20Performance%20on%0A%20%20Tasks%20where%20Thinking%20Makes%20Humans%20Worse&body=Title%3A%20Mind%20Your%20Step%20%28by%20Step%29%3A%20Chain-of-Thought%20can%20Reduce%20Performance%20on%0A%20%20Tasks%20where%20Thinking%20Makes%20Humans%20Worse%0AAuthor%3A%20Ryan%20Liu%20and%20Jiayi%20Geng%20and%20Addison%20J.%20Wu%20and%20Ilia%20Sucholutsky%20and%20Tania%20Lombrozo%20and%20Thomas%20L.%20Griffiths%0AAbstract%3A%20%20%20Chain-of-thought%20%28CoT%29%20prompting%20has%20become%20a%20widely%20used%20strategy%20for%0Aworking%20with%20large%20language%20and%20multimodal%20models.%20While%20CoT%20has%20been%20shown%20to%0Aimprove%20performance%20across%20many%20tasks%2C%20determining%20the%20settings%20in%20which%20it%20is%0Aeffective%20remains%20an%20ongoing%20effort.%20In%20particular%2C%20it%20is%20still%20an%20open%0Aquestion%20in%20what%20settings%20CoT%20systematically%20reduces%20model%20performance.%20In%20this%0Apaper%2C%20we%20seek%20to%20identify%20the%20characteristics%20of%20tasks%20where%20CoT%20reduces%0Aperformance%20by%20drawing%20inspiration%20from%20cognitive%20psychology%2C%20looking%20at%20cases%0Awhere%20%28i%29%20verbal%20thinking%20or%20deliberation%20hurts%20performance%20in%20humans%2C%20and%20%28ii%29%0Athe%20constraints%20governing%20human%20performance%20generalize%20to%20language%20models.%0AThree%20such%20cases%20are%20implicit%20statistical%20learning%2C%20visual%20recognition%2C%20and%0Aclassifying%20with%20patterns%20containing%20exceptions.%20In%20extensive%20experiments%0Aacross%20all%20three%20settings%2C%20we%20find%20that%20a%20diverse%20collection%20of%0Astate-of-the-art%20models%20exhibit%20significant%20drop-offs%20in%20performance%20%28e.g.%2C%20up%0Ato%2036.3%25%20absolute%20accuracy%20for%20OpenAI%20o1-preview%20compared%20to%20GPT-4o%29%20when%20using%0Ainference-time%20reasoning%20compared%20to%20zero-shot%20counterparts.%20We%20also%20identify%0Athree%20tasks%20that%20satisfy%20condition%20%28i%29%20but%20not%20%28ii%29%2C%20and%20find%20that%20while%20verbal%0Athinking%20reduces%20human%20performance%20in%20these%20tasks%2C%20CoT%20retains%20or%20increases%0Amodel%20performance.%20Overall%2C%20our%20results%20show%20that%20while%20there%20is%20not%20an%20exact%0Aparallel%20between%20the%20cognitive%20processes%20of%20models%20and%20those%20of%20humans%2C%0Aconsidering%20cases%20where%20thinking%20has%20negative%20consequences%20for%20human%0Aperformance%20can%20help%20us%20identify%20settings%20where%20it%20negatively%20impacts%20models.%0ABy%20connecting%20the%20literature%20on%20human%20deliberation%20with%20evaluations%20of%20CoT%2C%20we%0Aoffer%20a%20new%20tool%20that%20can%20be%20used%20in%20understanding%20the%20impact%20of%20prompt%20choices%0Aand%20inference-time%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.21333v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520Your%2520Step%2520%2528by%2520Step%2529%253A%2520Chain-of-Thought%2520can%2520Reduce%2520Performance%2520on%250A%2520%2520Tasks%2520where%2520Thinking%2520Makes%2520Humans%2520Worse%26entry.906535625%3DRyan%2520Liu%2520and%2520Jiayi%2520Geng%2520and%2520Addison%2520J.%2520Wu%2520and%2520Ilia%2520Sucholutsky%2520and%2520Tania%2520Lombrozo%2520and%2520Thomas%2520L.%2520Griffiths%26entry.1292438233%3D%2520%2520Chain-of-thought%2520%2528CoT%2529%2520prompting%2520has%2520become%2520a%2520widely%2520used%2520strategy%2520for%250Aworking%2520with%2520large%2520language%2520and%2520multimodal%2520models.%2520While%2520CoT%2520has%2520been%2520shown%2520to%250Aimprove%2520performance%2520across%2520many%2520tasks%252C%2520determining%2520the%2520settings%2520in%2520which%2520it%2520is%250Aeffective%2520remains%2520an%2520ongoing%2520effort.%2520In%2520particular%252C%2520it%2520is%2520still%2520an%2520open%250Aquestion%2520in%2520what%2520settings%2520CoT%2520systematically%2520reduces%2520model%2520performance.%2520In%2520this%250Apaper%252C%2520we%2520seek%2520to%2520identify%2520the%2520characteristics%2520of%2520tasks%2520where%2520CoT%2520reduces%250Aperformance%2520by%2520drawing%2520inspiration%2520from%2520cognitive%2520psychology%252C%2520looking%2520at%2520cases%250Awhere%2520%2528i%2529%2520verbal%2520thinking%2520or%2520deliberation%2520hurts%2520performance%2520in%2520humans%252C%2520and%2520%2528ii%2529%250Athe%2520constraints%2520governing%2520human%2520performance%2520generalize%2520to%2520language%2520models.%250AThree%2520such%2520cases%2520are%2520implicit%2520statistical%2520learning%252C%2520visual%2520recognition%252C%2520and%250Aclassifying%2520with%2520patterns%2520containing%2520exceptions.%2520In%2520extensive%2520experiments%250Aacross%2520all%2520three%2520settings%252C%2520we%2520find%2520that%2520a%2520diverse%2520collection%2520of%250Astate-of-the-art%2520models%2520exhibit%2520significant%2520drop-offs%2520in%2520performance%2520%2528e.g.%252C%2520up%250Ato%252036.3%2525%2520absolute%2520accuracy%2520for%2520OpenAI%2520o1-preview%2520compared%2520to%2520GPT-4o%2529%2520when%2520using%250Ainference-time%2520reasoning%2520compared%2520to%2520zero-shot%2520counterparts.%2520We%2520also%2520identify%250Athree%2520tasks%2520that%2520satisfy%2520condition%2520%2528i%2529%2520but%2520not%2520%2528ii%2529%252C%2520and%2520find%2520that%2520while%2520verbal%250Athinking%2520reduces%2520human%2520performance%2520in%2520these%2520tasks%252C%2520CoT%2520retains%2520or%2520increases%250Amodel%2520performance.%2520Overall%252C%2520our%2520results%2520show%2520that%2520while%2520there%2520is%2520not%2520an%2520exact%250Aparallel%2520between%2520the%2520cognitive%2520processes%2520of%2520models%2520and%2520those%2520of%2520humans%252C%250Aconsidering%2520cases%2520where%2520thinking%2520has%2520negative%2520consequences%2520for%2520human%250Aperformance%2520can%2520help%2520us%2520identify%2520settings%2520where%2520it%2520negatively%2520impacts%2520models.%250ABy%2520connecting%2520the%2520literature%2520on%2520human%2520deliberation%2520with%2520evaluations%2520of%2520CoT%252C%2520we%250Aoffer%2520a%2520new%2520tool%2520that%2520can%2520be%2520used%2520in%2520understanding%2520the%2520impact%2520of%2520prompt%2520choices%250Aand%2520inference-time%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.21333v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20Your%20Step%20%28by%20Step%29%3A%20Chain-of-Thought%20can%20Reduce%20Performance%20on%0A%20%20Tasks%20where%20Thinking%20Makes%20Humans%20Worse&entry.906535625=Ryan%20Liu%20and%20Jiayi%20Geng%20and%20Addison%20J.%20Wu%20and%20Ilia%20Sucholutsky%20and%20Tania%20Lombrozo%20and%20Thomas%20L.%20Griffiths&entry.1292438233=%20%20Chain-of-thought%20%28CoT%29%20prompting%20has%20become%20a%20widely%20used%20strategy%20for%0Aworking%20with%20large%20language%20and%20multimodal%20models.%20While%20CoT%20has%20been%20shown%20to%0Aimprove%20performance%20across%20many%20tasks%2C%20determining%20the%20settings%20in%20which%20it%20is%0Aeffective%20remains%20an%20ongoing%20effort.%20In%20particular%2C%20it%20is%20still%20an%20open%0Aquestion%20in%20what%20settings%20CoT%20systematically%20reduces%20model%20performance.%20In%20this%0Apaper%2C%20we%20seek%20to%20identify%20the%20characteristics%20of%20tasks%20where%20CoT%20reduces%0Aperformance%20by%20drawing%20inspiration%20from%20cognitive%20psychology%2C%20looking%20at%20cases%0Awhere%20%28i%29%20verbal%20thinking%20or%20deliberation%20hurts%20performance%20in%20humans%2C%20and%20%28ii%29%0Athe%20constraints%20governing%20human%20performance%20generalize%20to%20language%20models.%0AThree%20such%20cases%20are%20implicit%20statistical%20learning%2C%20visual%20recognition%2C%20and%0Aclassifying%20with%20patterns%20containing%20exceptions.%20In%20extensive%20experiments%0Aacross%20all%20three%20settings%2C%20we%20find%20that%20a%20diverse%20collection%20of%0Astate-of-the-art%20models%20exhibit%20significant%20drop-offs%20in%20performance%20%28e.g.%2C%20up%0Ato%2036.3%25%20absolute%20accuracy%20for%20OpenAI%20o1-preview%20compared%20to%20GPT-4o%29%20when%20using%0Ainference-time%20reasoning%20compared%20to%20zero-shot%20counterparts.%20We%20also%20identify%0Athree%20tasks%20that%20satisfy%20condition%20%28i%29%20but%20not%20%28ii%29%2C%20and%20find%20that%20while%20verbal%0Athinking%20reduces%20human%20performance%20in%20these%20tasks%2C%20CoT%20retains%20or%20increases%0Amodel%20performance.%20Overall%2C%20our%20results%20show%20that%20while%20there%20is%20not%20an%20exact%0Aparallel%20between%20the%20cognitive%20processes%20of%20models%20and%20those%20of%20humans%2C%0Aconsidering%20cases%20where%20thinking%20has%20negative%20consequences%20for%20human%0Aperformance%20can%20help%20us%20identify%20settings%20where%20it%20negatively%20impacts%20models.%0ABy%20connecting%20the%20literature%20on%20human%20deliberation%20with%20evaluations%20of%20CoT%2C%20we%0Aoffer%20a%20new%20tool%20that%20can%20be%20used%20in%20understanding%20the%20impact%20of%20prompt%20choices%0Aand%20inference-time%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.21333v3&entry.124074799=Read"},
{"title": "Do Histopathological Foundation Models Eliminate Batch Effects? A\n  Comparative Study", "author": "Jonah K\u00f6men and Hannah Marienwald and Jonas Dippel and Julius Hense", "abstract": "  Deep learning has led to remarkable advancements in computational\nhistopathology, e.g., in diagnostics, biomarker prediction, and outcome\nprognosis. Yet, the lack of annotated data and the impact of batch effects,\ne.g., systematic technical data differences across hospitals, hamper model\nrobustness and generalization. Recent histopathological foundation models --\npretrained on millions to billions of images -- have been reported to improve\ngeneralization performances on various downstream tasks. However, it has not\nbeen systematically assessed whether they fully eliminate batch effects. In\nthis study, we empirically show that the feature embeddings of the foundation\nmodels still contain distinct hospital signatures that can lead to biased\npredictions and misclassifications. We further find that the signatures are not\nremoved by stain normalization methods, dominate distances in feature space,\nand are evident across various principal components. Our work provides a novel\nperspective on the evaluation of medical foundation models, paving the way for\nmore robust pretraining strategies and downstream predictors.\n", "link": "http://arxiv.org/abs/2411.05489v1", "date": "2024-11-08", "relevancy": 1.9624, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4926}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Histopathological%20Foundation%20Models%20Eliminate%20Batch%20Effects%3F%20A%0A%20%20Comparative%20Study&body=Title%3A%20Do%20Histopathological%20Foundation%20Models%20Eliminate%20Batch%20Effects%3F%20A%0A%20%20Comparative%20Study%0AAuthor%3A%20Jonah%20K%C3%B6men%20and%20Hannah%20Marienwald%20and%20Jonas%20Dippel%20and%20Julius%20Hense%0AAbstract%3A%20%20%20Deep%20learning%20has%20led%20to%20remarkable%20advancements%20in%20computational%0Ahistopathology%2C%20e.g.%2C%20in%20diagnostics%2C%20biomarker%20prediction%2C%20and%20outcome%0Aprognosis.%20Yet%2C%20the%20lack%20of%20annotated%20data%20and%20the%20impact%20of%20batch%20effects%2C%0Ae.g.%2C%20systematic%20technical%20data%20differences%20across%20hospitals%2C%20hamper%20model%0Arobustness%20and%20generalization.%20Recent%20histopathological%20foundation%20models%20--%0Apretrained%20on%20millions%20to%20billions%20of%20images%20--%20have%20been%20reported%20to%20improve%0Ageneralization%20performances%20on%20various%20downstream%20tasks.%20However%2C%20it%20has%20not%0Abeen%20systematically%20assessed%20whether%20they%20fully%20eliminate%20batch%20effects.%20In%0Athis%20study%2C%20we%20empirically%20show%20that%20the%20feature%20embeddings%20of%20the%20foundation%0Amodels%20still%20contain%20distinct%20hospital%20signatures%20that%20can%20lead%20to%20biased%0Apredictions%20and%20misclassifications.%20We%20further%20find%20that%20the%20signatures%20are%20not%0Aremoved%20by%20stain%20normalization%20methods%2C%20dominate%20distances%20in%20feature%20space%2C%0Aand%20are%20evident%20across%20various%20principal%20components.%20Our%20work%20provides%20a%20novel%0Aperspective%20on%20the%20evaluation%20of%20medical%20foundation%20models%2C%20paving%20the%20way%20for%0Amore%20robust%20pretraining%20strategies%20and%20downstream%20predictors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05489v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Histopathological%2520Foundation%2520Models%2520Eliminate%2520Batch%2520Effects%253F%2520A%250A%2520%2520Comparative%2520Study%26entry.906535625%3DJonah%2520K%25C3%25B6men%2520and%2520Hannah%2520Marienwald%2520and%2520Jonas%2520Dippel%2520and%2520Julius%2520Hense%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520led%2520to%2520remarkable%2520advancements%2520in%2520computational%250Ahistopathology%252C%2520e.g.%252C%2520in%2520diagnostics%252C%2520biomarker%2520prediction%252C%2520and%2520outcome%250Aprognosis.%2520Yet%252C%2520the%2520lack%2520of%2520annotated%2520data%2520and%2520the%2520impact%2520of%2520batch%2520effects%252C%250Ae.g.%252C%2520systematic%2520technical%2520data%2520differences%2520across%2520hospitals%252C%2520hamper%2520model%250Arobustness%2520and%2520generalization.%2520Recent%2520histopathological%2520foundation%2520models%2520--%250Apretrained%2520on%2520millions%2520to%2520billions%2520of%2520images%2520--%2520have%2520been%2520reported%2520to%2520improve%250Ageneralization%2520performances%2520on%2520various%2520downstream%2520tasks.%2520However%252C%2520it%2520has%2520not%250Abeen%2520systematically%2520assessed%2520whether%2520they%2520fully%2520eliminate%2520batch%2520effects.%2520In%250Athis%2520study%252C%2520we%2520empirically%2520show%2520that%2520the%2520feature%2520embeddings%2520of%2520the%2520foundation%250Amodels%2520still%2520contain%2520distinct%2520hospital%2520signatures%2520that%2520can%2520lead%2520to%2520biased%250Apredictions%2520and%2520misclassifications.%2520We%2520further%2520find%2520that%2520the%2520signatures%2520are%2520not%250Aremoved%2520by%2520stain%2520normalization%2520methods%252C%2520dominate%2520distances%2520in%2520feature%2520space%252C%250Aand%2520are%2520evident%2520across%2520various%2520principal%2520components.%2520Our%2520work%2520provides%2520a%2520novel%250Aperspective%2520on%2520the%2520evaluation%2520of%2520medical%2520foundation%2520models%252C%2520paving%2520the%2520way%2520for%250Amore%2520robust%2520pretraining%2520strategies%2520and%2520downstream%2520predictors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05489v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Histopathological%20Foundation%20Models%20Eliminate%20Batch%20Effects%3F%20A%0A%20%20Comparative%20Study&entry.906535625=Jonah%20K%C3%B6men%20and%20Hannah%20Marienwald%20and%20Jonas%20Dippel%20and%20Julius%20Hense&entry.1292438233=%20%20Deep%20learning%20has%20led%20to%20remarkable%20advancements%20in%20computational%0Ahistopathology%2C%20e.g.%2C%20in%20diagnostics%2C%20biomarker%20prediction%2C%20and%20outcome%0Aprognosis.%20Yet%2C%20the%20lack%20of%20annotated%20data%20and%20the%20impact%20of%20batch%20effects%2C%0Ae.g.%2C%20systematic%20technical%20data%20differences%20across%20hospitals%2C%20hamper%20model%0Arobustness%20and%20generalization.%20Recent%20histopathological%20foundation%20models%20--%0Apretrained%20on%20millions%20to%20billions%20of%20images%20--%20have%20been%20reported%20to%20improve%0Ageneralization%20performances%20on%20various%20downstream%20tasks.%20However%2C%20it%20has%20not%0Abeen%20systematically%20assessed%20whether%20they%20fully%20eliminate%20batch%20effects.%20In%0Athis%20study%2C%20we%20empirically%20show%20that%20the%20feature%20embeddings%20of%20the%20foundation%0Amodels%20still%20contain%20distinct%20hospital%20signatures%20that%20can%20lead%20to%20biased%0Apredictions%20and%20misclassifications.%20We%20further%20find%20that%20the%20signatures%20are%20not%0Aremoved%20by%20stain%20normalization%20methods%2C%20dominate%20distances%20in%20feature%20space%2C%0Aand%20are%20evident%20across%20various%20principal%20components.%20Our%20work%20provides%20a%20novel%0Aperspective%20on%20the%20evaluation%20of%20medical%20foundation%20models%2C%20paving%20the%20way%20for%0Amore%20robust%20pretraining%20strategies%20and%20downstream%20predictors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05489v1&entry.124074799=Read"},
{"title": "Learning Human-like Representations to Enable Learning Human Values", "author": "Andrea Wynn and Ilia Sucholutsky and Thomas L. Griffiths", "abstract": "  How can we build AI systems that can learn any set of individual human values\nboth quickly and safely, avoiding causing harm or violating societal standards\nfor acceptable behavior during the learning process? We explore the effects of\nrepresentational alignment between humans and AI agents on learning human\nvalues. Making AI systems learn human-like representations of the world has\nmany known benefits, including improving generalization, robustness to domain\nshifts, and few-shot learning performance. We demonstrate that this kind of\nrepresentational alignment can also support safely learning and exploring human\nvalues in the context of personalization. We begin with a theoretical\nprediction, show that it applies to learning human morality judgments, then\nshow that our results generalize to ten different aspects of human values --\nincluding ethics, honesty, and fairness -- training AI agents on each set of\nvalues in a multi-armed bandit setting, where rewards reflect human value\njudgments over the chosen action. Using a set of textual action descriptions,\nwe collect value judgments from humans, as well as similarity judgments from\nboth humans and multiple language models, and demonstrate that representational\nalignment enables both safe exploration and improved generalization when\nlearning human values.\n", "link": "http://arxiv.org/abs/2312.14106v3", "date": "2024-11-08", "relevancy": 1.9589, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5156}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.493}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Human-like%20Representations%20to%20Enable%20Learning%20Human%20Values&body=Title%3A%20Learning%20Human-like%20Representations%20to%20Enable%20Learning%20Human%20Values%0AAuthor%3A%20Andrea%20Wynn%20and%20Ilia%20Sucholutsky%20and%20Thomas%20L.%20Griffiths%0AAbstract%3A%20%20%20How%20can%20we%20build%20AI%20systems%20that%20can%20learn%20any%20set%20of%20individual%20human%20values%0Aboth%20quickly%20and%20safely%2C%20avoiding%20causing%20harm%20or%20violating%20societal%20standards%0Afor%20acceptable%20behavior%20during%20the%20learning%20process%3F%20We%20explore%20the%20effects%20of%0Arepresentational%20alignment%20between%20humans%20and%20AI%20agents%20on%20learning%20human%0Avalues.%20Making%20AI%20systems%20learn%20human-like%20representations%20of%20the%20world%20has%0Amany%20known%20benefits%2C%20including%20improving%20generalization%2C%20robustness%20to%20domain%0Ashifts%2C%20and%20few-shot%20learning%20performance.%20We%20demonstrate%20that%20this%20kind%20of%0Arepresentational%20alignment%20can%20also%20support%20safely%20learning%20and%20exploring%20human%0Avalues%20in%20the%20context%20of%20personalization.%20We%20begin%20with%20a%20theoretical%0Aprediction%2C%20show%20that%20it%20applies%20to%20learning%20human%20morality%20judgments%2C%20then%0Ashow%20that%20our%20results%20generalize%20to%20ten%20different%20aspects%20of%20human%20values%20--%0Aincluding%20ethics%2C%20honesty%2C%20and%20fairness%20--%20training%20AI%20agents%20on%20each%20set%20of%0Avalues%20in%20a%20multi-armed%20bandit%20setting%2C%20where%20rewards%20reflect%20human%20value%0Ajudgments%20over%20the%20chosen%20action.%20Using%20a%20set%20of%20textual%20action%20descriptions%2C%0Awe%20collect%20value%20judgments%20from%20humans%2C%20as%20well%20as%20similarity%20judgments%20from%0Aboth%20humans%20and%20multiple%20language%20models%2C%20and%20demonstrate%20that%20representational%0Aalignment%20enables%20both%20safe%20exploration%20and%20improved%20generalization%20when%0Alearning%20human%20values.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14106v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Human-like%2520Representations%2520to%2520Enable%2520Learning%2520Human%2520Values%26entry.906535625%3DAndrea%2520Wynn%2520and%2520Ilia%2520Sucholutsky%2520and%2520Thomas%2520L.%2520Griffiths%26entry.1292438233%3D%2520%2520How%2520can%2520we%2520build%2520AI%2520systems%2520that%2520can%2520learn%2520any%2520set%2520of%2520individual%2520human%2520values%250Aboth%2520quickly%2520and%2520safely%252C%2520avoiding%2520causing%2520harm%2520or%2520violating%2520societal%2520standards%250Afor%2520acceptable%2520behavior%2520during%2520the%2520learning%2520process%253F%2520We%2520explore%2520the%2520effects%2520of%250Arepresentational%2520alignment%2520between%2520humans%2520and%2520AI%2520agents%2520on%2520learning%2520human%250Avalues.%2520Making%2520AI%2520systems%2520learn%2520human-like%2520representations%2520of%2520the%2520world%2520has%250Amany%2520known%2520benefits%252C%2520including%2520improving%2520generalization%252C%2520robustness%2520to%2520domain%250Ashifts%252C%2520and%2520few-shot%2520learning%2520performance.%2520We%2520demonstrate%2520that%2520this%2520kind%2520of%250Arepresentational%2520alignment%2520can%2520also%2520support%2520safely%2520learning%2520and%2520exploring%2520human%250Avalues%2520in%2520the%2520context%2520of%2520personalization.%2520We%2520begin%2520with%2520a%2520theoretical%250Aprediction%252C%2520show%2520that%2520it%2520applies%2520to%2520learning%2520human%2520morality%2520judgments%252C%2520then%250Ashow%2520that%2520our%2520results%2520generalize%2520to%2520ten%2520different%2520aspects%2520of%2520human%2520values%2520--%250Aincluding%2520ethics%252C%2520honesty%252C%2520and%2520fairness%2520--%2520training%2520AI%2520agents%2520on%2520each%2520set%2520of%250Avalues%2520in%2520a%2520multi-armed%2520bandit%2520setting%252C%2520where%2520rewards%2520reflect%2520human%2520value%250Ajudgments%2520over%2520the%2520chosen%2520action.%2520Using%2520a%2520set%2520of%2520textual%2520action%2520descriptions%252C%250Awe%2520collect%2520value%2520judgments%2520from%2520humans%252C%2520as%2520well%2520as%2520similarity%2520judgments%2520from%250Aboth%2520humans%2520and%2520multiple%2520language%2520models%252C%2520and%2520demonstrate%2520that%2520representational%250Aalignment%2520enables%2520both%2520safe%2520exploration%2520and%2520improved%2520generalization%2520when%250Alearning%2520human%2520values.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14106v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Human-like%20Representations%20to%20Enable%20Learning%20Human%20Values&entry.906535625=Andrea%20Wynn%20and%20Ilia%20Sucholutsky%20and%20Thomas%20L.%20Griffiths&entry.1292438233=%20%20How%20can%20we%20build%20AI%20systems%20that%20can%20learn%20any%20set%20of%20individual%20human%20values%0Aboth%20quickly%20and%20safely%2C%20avoiding%20causing%20harm%20or%20violating%20societal%20standards%0Afor%20acceptable%20behavior%20during%20the%20learning%20process%3F%20We%20explore%20the%20effects%20of%0Arepresentational%20alignment%20between%20humans%20and%20AI%20agents%20on%20learning%20human%0Avalues.%20Making%20AI%20systems%20learn%20human-like%20representations%20of%20the%20world%20has%0Amany%20known%20benefits%2C%20including%20improving%20generalization%2C%20robustness%20to%20domain%0Ashifts%2C%20and%20few-shot%20learning%20performance.%20We%20demonstrate%20that%20this%20kind%20of%0Arepresentational%20alignment%20can%20also%20support%20safely%20learning%20and%20exploring%20human%0Avalues%20in%20the%20context%20of%20personalization.%20We%20begin%20with%20a%20theoretical%0Aprediction%2C%20show%20that%20it%20applies%20to%20learning%20human%20morality%20judgments%2C%20then%0Ashow%20that%20our%20results%20generalize%20to%20ten%20different%20aspects%20of%20human%20values%20--%0Aincluding%20ethics%2C%20honesty%2C%20and%20fairness%20--%20training%20AI%20agents%20on%20each%20set%20of%0Avalues%20in%20a%20multi-armed%20bandit%20setting%2C%20where%20rewards%20reflect%20human%20value%0Ajudgments%20over%20the%20chosen%20action.%20Using%20a%20set%20of%20textual%20action%20descriptions%2C%0Awe%20collect%20value%20judgments%20from%20humans%2C%20as%20well%20as%20similarity%20judgments%20from%0Aboth%20humans%20and%20multiple%20language%20models%2C%20and%20demonstrate%20that%20representational%0Aalignment%20enables%20both%20safe%20exploration%20and%20improved%20generalization%20when%0Alearning%20human%20values.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14106v3&entry.124074799=Read"},
{"title": "TropNNC: Structured Neural Network Compression Using Tropical Geometry", "author": "Konstantinos Fotopoulos and Petros Maragos and Panagiotis Misiakos", "abstract": "  We present TropNNC, a framework for compressing neural networks with linear\nand convolutional layers and ReLU activations. TropNNC is a structured\ncompression framework based on a geometrical approach to machine/deep learning,\nusing tropical geometry and extending the work of Misiakos et al. (2022). We\nuse the Hausdorff distance of zonotopes in its standard continuous form to\nachieve a tighter approximation bound for tropical polynomials compared to\nprevious work. This enhancement leads to the development of an effective\ncompression algorithm that achieves superior functional approximations of\nneural networks. Our method is significantly easier to implement compared to\nother frameworks, and does not depend on the availability of training data\nsamples. We validate our framework through extensive empirical evaluations on\nthe MNIST, CIFAR, and ImageNet datasets. Our results demonstrate that TropNNC\nachieves performance on par with state-of-the-art methods like ThiNet (even\nsurpassing it in compressing linear layers) and CUP. To the best of our\nknowledge, it is the first method that achieves this using tropical geometry.\n", "link": "http://arxiv.org/abs/2409.03945v2", "date": "2024-11-08", "relevancy": 1.9433, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4963}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4821}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TropNNC%3A%20Structured%20Neural%20Network%20Compression%20Using%20Tropical%20Geometry&body=Title%3A%20TropNNC%3A%20Structured%20Neural%20Network%20Compression%20Using%20Tropical%20Geometry%0AAuthor%3A%20Konstantinos%20Fotopoulos%20and%20Petros%20Maragos%20and%20Panagiotis%20Misiakos%0AAbstract%3A%20%20%20We%20present%20TropNNC%2C%20a%20framework%20for%20compressing%20neural%20networks%20with%20linear%0Aand%20convolutional%20layers%20and%20ReLU%20activations.%20TropNNC%20is%20a%20structured%0Acompression%20framework%20based%20on%20a%20geometrical%20approach%20to%20machine/deep%20learning%2C%0Ausing%20tropical%20geometry%20and%20extending%20the%20work%20of%20Misiakos%20et%20al.%20%282022%29.%20We%0Ause%20the%20Hausdorff%20distance%20of%20zonotopes%20in%20its%20standard%20continuous%20form%20to%0Aachieve%20a%20tighter%20approximation%20bound%20for%20tropical%20polynomials%20compared%20to%0Aprevious%20work.%20This%20enhancement%20leads%20to%20the%20development%20of%20an%20effective%0Acompression%20algorithm%20that%20achieves%20superior%20functional%20approximations%20of%0Aneural%20networks.%20Our%20method%20is%20significantly%20easier%20to%20implement%20compared%20to%0Aother%20frameworks%2C%20and%20does%20not%20depend%20on%20the%20availability%20of%20training%20data%0Asamples.%20We%20validate%20our%20framework%20through%20extensive%20empirical%20evaluations%20on%0Athe%20MNIST%2C%20CIFAR%2C%20and%20ImageNet%20datasets.%20Our%20results%20demonstrate%20that%20TropNNC%0Aachieves%20performance%20on%20par%20with%20state-of-the-art%20methods%20like%20ThiNet%20%28even%0Asurpassing%20it%20in%20compressing%20linear%20layers%29%20and%20CUP.%20To%20the%20best%20of%20our%0Aknowledge%2C%20it%20is%20the%20first%20method%20that%20achieves%20this%20using%20tropical%20geometry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.03945v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTropNNC%253A%2520Structured%2520Neural%2520Network%2520Compression%2520Using%2520Tropical%2520Geometry%26entry.906535625%3DKonstantinos%2520Fotopoulos%2520and%2520Petros%2520Maragos%2520and%2520Panagiotis%2520Misiakos%26entry.1292438233%3D%2520%2520We%2520present%2520TropNNC%252C%2520a%2520framework%2520for%2520compressing%2520neural%2520networks%2520with%2520linear%250Aand%2520convolutional%2520layers%2520and%2520ReLU%2520activations.%2520TropNNC%2520is%2520a%2520structured%250Acompression%2520framework%2520based%2520on%2520a%2520geometrical%2520approach%2520to%2520machine/deep%2520learning%252C%250Ausing%2520tropical%2520geometry%2520and%2520extending%2520the%2520work%2520of%2520Misiakos%2520et%2520al.%2520%25282022%2529.%2520We%250Ause%2520the%2520Hausdorff%2520distance%2520of%2520zonotopes%2520in%2520its%2520standard%2520continuous%2520form%2520to%250Aachieve%2520a%2520tighter%2520approximation%2520bound%2520for%2520tropical%2520polynomials%2520compared%2520to%250Aprevious%2520work.%2520This%2520enhancement%2520leads%2520to%2520the%2520development%2520of%2520an%2520effective%250Acompression%2520algorithm%2520that%2520achieves%2520superior%2520functional%2520approximations%2520of%250Aneural%2520networks.%2520Our%2520method%2520is%2520significantly%2520easier%2520to%2520implement%2520compared%2520to%250Aother%2520frameworks%252C%2520and%2520does%2520not%2520depend%2520on%2520the%2520availability%2520of%2520training%2520data%250Asamples.%2520We%2520validate%2520our%2520framework%2520through%2520extensive%2520empirical%2520evaluations%2520on%250Athe%2520MNIST%252C%2520CIFAR%252C%2520and%2520ImageNet%2520datasets.%2520Our%2520results%2520demonstrate%2520that%2520TropNNC%250Aachieves%2520performance%2520on%2520par%2520with%2520state-of-the-art%2520methods%2520like%2520ThiNet%2520%2528even%250Asurpassing%2520it%2520in%2520compressing%2520linear%2520layers%2529%2520and%2520CUP.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520it%2520is%2520the%2520first%2520method%2520that%2520achieves%2520this%2520using%2520tropical%2520geometry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.03945v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TropNNC%3A%20Structured%20Neural%20Network%20Compression%20Using%20Tropical%20Geometry&entry.906535625=Konstantinos%20Fotopoulos%20and%20Petros%20Maragos%20and%20Panagiotis%20Misiakos&entry.1292438233=%20%20We%20present%20TropNNC%2C%20a%20framework%20for%20compressing%20neural%20networks%20with%20linear%0Aand%20convolutional%20layers%20and%20ReLU%20activations.%20TropNNC%20is%20a%20structured%0Acompression%20framework%20based%20on%20a%20geometrical%20approach%20to%20machine/deep%20learning%2C%0Ausing%20tropical%20geometry%20and%20extending%20the%20work%20of%20Misiakos%20et%20al.%20%282022%29.%20We%0Ause%20the%20Hausdorff%20distance%20of%20zonotopes%20in%20its%20standard%20continuous%20form%20to%0Aachieve%20a%20tighter%20approximation%20bound%20for%20tropical%20polynomials%20compared%20to%0Aprevious%20work.%20This%20enhancement%20leads%20to%20the%20development%20of%20an%20effective%0Acompression%20algorithm%20that%20achieves%20superior%20functional%20approximations%20of%0Aneural%20networks.%20Our%20method%20is%20significantly%20easier%20to%20implement%20compared%20to%0Aother%20frameworks%2C%20and%20does%20not%20depend%20on%20the%20availability%20of%20training%20data%0Asamples.%20We%20validate%20our%20framework%20through%20extensive%20empirical%20evaluations%20on%0Athe%20MNIST%2C%20CIFAR%2C%20and%20ImageNet%20datasets.%20Our%20results%20demonstrate%20that%20TropNNC%0Aachieves%20performance%20on%20par%20with%20state-of-the-art%20methods%20like%20ThiNet%20%28even%0Asurpassing%20it%20in%20compressing%20linear%20layers%29%20and%20CUP.%20To%20the%20best%20of%20our%0Aknowledge%2C%20it%20is%20the%20first%20method%20that%20achieves%20this%20using%20tropical%20geometry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.03945v2&entry.124074799=Read"},
{"title": "Logic Query of Thoughts: Guiding Large Language Models to Answer Complex\n  Logic Queries with Knowledge Graphs", "author": "Lihui Liu and Zihao Wang and Ruizhong Qiu and Yikun Ban and Eunice Chan and Yangqiu Song and Jingrui He and Hanghang Tong", "abstract": "  Despite the superb performance in many tasks, large language models (LLMs)\nbear the risk of generating hallucination or even wrong answers when confronted\nwith tasks that demand the accuracy of knowledge. The issue becomes even more\nnoticeable when addressing logic queries that require multiple logic reasoning\nsteps. On the other hand, knowledge graph (KG) based question answering methods\nare capable of accurately identifying the correct answers with the help of\nknowledge graph, yet its accuracy could quickly deteriorate when the knowledge\ngraph itself is sparse and incomplete. It remains a critical challenge on how\nto integrate knowledge graph reasoning with LLMs in a mutually beneficial way\nso as to mitigate both the hallucination problem of LLMs as well as the\nincompleteness issue of knowledge graphs. In this paper, we propose\n'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs\nwith knowledge graph based logic query reasoning. LGOT seamlessly combines\nknowledge graph reasoning and LLMs, effectively breaking down complex logic\nqueries into easy to answer subquestions. Through the utilization of both\nknowledge graph reasoning and LLMs, it successfully derives answers for each\nsubquestion. By aggregating these results and selecting the highest quality\ncandidate answers for each step, LGOT achieves accurate results to complex\nquestions. Our experimental findings demonstrate substantial performance\nenhancements, with up to 20% improvement over ChatGPT.\n", "link": "http://arxiv.org/abs/2404.04264v4", "date": "2024-11-08", "relevancy": 1.9296, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4874}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4814}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Logic%20Query%20of%20Thoughts%3A%20Guiding%20Large%20Language%20Models%20to%20Answer%20Complex%0A%20%20Logic%20Queries%20with%20Knowledge%20Graphs&body=Title%3A%20Logic%20Query%20of%20Thoughts%3A%20Guiding%20Large%20Language%20Models%20to%20Answer%20Complex%0A%20%20Logic%20Queries%20with%20Knowledge%20Graphs%0AAuthor%3A%20Lihui%20Liu%20and%20Zihao%20Wang%20and%20Ruizhong%20Qiu%20and%20Yikun%20Ban%20and%20Eunice%20Chan%20and%20Yangqiu%20Song%20and%20Jingrui%20He%20and%20Hanghang%20Tong%0AAbstract%3A%20%20%20Despite%20the%20superb%20performance%20in%20many%20tasks%2C%20large%20language%20models%20%28LLMs%29%0Abear%20the%20risk%20of%20generating%20hallucination%20or%20even%20wrong%20answers%20when%20confronted%0Awith%20tasks%20that%20demand%20the%20accuracy%20of%20knowledge.%20The%20issue%20becomes%20even%20more%0Anoticeable%20when%20addressing%20logic%20queries%20that%20require%20multiple%20logic%20reasoning%0Asteps.%20On%20the%20other%20hand%2C%20knowledge%20graph%20%28KG%29%20based%20question%20answering%20methods%0Aare%20capable%20of%20accurately%20identifying%20the%20correct%20answers%20with%20the%20help%20of%0Aknowledge%20graph%2C%20yet%20its%20accuracy%20could%20quickly%20deteriorate%20when%20the%20knowledge%0Agraph%20itself%20is%20sparse%20and%20incomplete.%20It%20remains%20a%20critical%20challenge%20on%20how%0Ato%20integrate%20knowledge%20graph%20reasoning%20with%20LLMs%20in%20a%20mutually%20beneficial%20way%0Aso%20as%20to%20mitigate%20both%20the%20hallucination%20problem%20of%20LLMs%20as%20well%20as%20the%0Aincompleteness%20issue%20of%20knowledge%20graphs.%20In%20this%20paper%2C%20we%20propose%0A%27Logic-Query-of-Thoughts%27%20%28LGOT%29%20which%20is%20the%20first%20of%20its%20kind%20to%20combine%20LLMs%0Awith%20knowledge%20graph%20based%20logic%20query%20reasoning.%20LGOT%20seamlessly%20combines%0Aknowledge%20graph%20reasoning%20and%20LLMs%2C%20effectively%20breaking%20down%20complex%20logic%0Aqueries%20into%20easy%20to%20answer%20subquestions.%20Through%20the%20utilization%20of%20both%0Aknowledge%20graph%20reasoning%20and%20LLMs%2C%20it%20successfully%20derives%20answers%20for%20each%0Asubquestion.%20By%20aggregating%20these%20results%20and%20selecting%20the%20highest%20quality%0Acandidate%20answers%20for%20each%20step%2C%20LGOT%20achieves%20accurate%20results%20to%20complex%0Aquestions.%20Our%20experimental%20findings%20demonstrate%20substantial%20performance%0Aenhancements%2C%20with%20up%20to%2020%25%20improvement%20over%20ChatGPT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.04264v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogic%2520Query%2520of%2520Thoughts%253A%2520Guiding%2520Large%2520Language%2520Models%2520to%2520Answer%2520Complex%250A%2520%2520Logic%2520Queries%2520with%2520Knowledge%2520Graphs%26entry.906535625%3DLihui%2520Liu%2520and%2520Zihao%2520Wang%2520and%2520Ruizhong%2520Qiu%2520and%2520Yikun%2520Ban%2520and%2520Eunice%2520Chan%2520and%2520Yangqiu%2520Song%2520and%2520Jingrui%2520He%2520and%2520Hanghang%2520Tong%26entry.1292438233%3D%2520%2520Despite%2520the%2520superb%2520performance%2520in%2520many%2520tasks%252C%2520large%2520language%2520models%2520%2528LLMs%2529%250Abear%2520the%2520risk%2520of%2520generating%2520hallucination%2520or%2520even%2520wrong%2520answers%2520when%2520confronted%250Awith%2520tasks%2520that%2520demand%2520the%2520accuracy%2520of%2520knowledge.%2520The%2520issue%2520becomes%2520even%2520more%250Anoticeable%2520when%2520addressing%2520logic%2520queries%2520that%2520require%2520multiple%2520logic%2520reasoning%250Asteps.%2520On%2520the%2520other%2520hand%252C%2520knowledge%2520graph%2520%2528KG%2529%2520based%2520question%2520answering%2520methods%250Aare%2520capable%2520of%2520accurately%2520identifying%2520the%2520correct%2520answers%2520with%2520the%2520help%2520of%250Aknowledge%2520graph%252C%2520yet%2520its%2520accuracy%2520could%2520quickly%2520deteriorate%2520when%2520the%2520knowledge%250Agraph%2520itself%2520is%2520sparse%2520and%2520incomplete.%2520It%2520remains%2520a%2520critical%2520challenge%2520on%2520how%250Ato%2520integrate%2520knowledge%2520graph%2520reasoning%2520with%2520LLMs%2520in%2520a%2520mutually%2520beneficial%2520way%250Aso%2520as%2520to%2520mitigate%2520both%2520the%2520hallucination%2520problem%2520of%2520LLMs%2520as%2520well%2520as%2520the%250Aincompleteness%2520issue%2520of%2520knowledge%2520graphs.%2520In%2520this%2520paper%252C%2520we%2520propose%250A%2527Logic-Query-of-Thoughts%2527%2520%2528LGOT%2529%2520which%2520is%2520the%2520first%2520of%2520its%2520kind%2520to%2520combine%2520LLMs%250Awith%2520knowledge%2520graph%2520based%2520logic%2520query%2520reasoning.%2520LGOT%2520seamlessly%2520combines%250Aknowledge%2520graph%2520reasoning%2520and%2520LLMs%252C%2520effectively%2520breaking%2520down%2520complex%2520logic%250Aqueries%2520into%2520easy%2520to%2520answer%2520subquestions.%2520Through%2520the%2520utilization%2520of%2520both%250Aknowledge%2520graph%2520reasoning%2520and%2520LLMs%252C%2520it%2520successfully%2520derives%2520answers%2520for%2520each%250Asubquestion.%2520By%2520aggregating%2520these%2520results%2520and%2520selecting%2520the%2520highest%2520quality%250Acandidate%2520answers%2520for%2520each%2520step%252C%2520LGOT%2520achieves%2520accurate%2520results%2520to%2520complex%250Aquestions.%2520Our%2520experimental%2520findings%2520demonstrate%2520substantial%2520performance%250Aenhancements%252C%2520with%2520up%2520to%252020%2525%2520improvement%2520over%2520ChatGPT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.04264v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Logic%20Query%20of%20Thoughts%3A%20Guiding%20Large%20Language%20Models%20to%20Answer%20Complex%0A%20%20Logic%20Queries%20with%20Knowledge%20Graphs&entry.906535625=Lihui%20Liu%20and%20Zihao%20Wang%20and%20Ruizhong%20Qiu%20and%20Yikun%20Ban%20and%20Eunice%20Chan%20and%20Yangqiu%20Song%20and%20Jingrui%20He%20and%20Hanghang%20Tong&entry.1292438233=%20%20Despite%20the%20superb%20performance%20in%20many%20tasks%2C%20large%20language%20models%20%28LLMs%29%0Abear%20the%20risk%20of%20generating%20hallucination%20or%20even%20wrong%20answers%20when%20confronted%0Awith%20tasks%20that%20demand%20the%20accuracy%20of%20knowledge.%20The%20issue%20becomes%20even%20more%0Anoticeable%20when%20addressing%20logic%20queries%20that%20require%20multiple%20logic%20reasoning%0Asteps.%20On%20the%20other%20hand%2C%20knowledge%20graph%20%28KG%29%20based%20question%20answering%20methods%0Aare%20capable%20of%20accurately%20identifying%20the%20correct%20answers%20with%20the%20help%20of%0Aknowledge%20graph%2C%20yet%20its%20accuracy%20could%20quickly%20deteriorate%20when%20the%20knowledge%0Agraph%20itself%20is%20sparse%20and%20incomplete.%20It%20remains%20a%20critical%20challenge%20on%20how%0Ato%20integrate%20knowledge%20graph%20reasoning%20with%20LLMs%20in%20a%20mutually%20beneficial%20way%0Aso%20as%20to%20mitigate%20both%20the%20hallucination%20problem%20of%20LLMs%20as%20well%20as%20the%0Aincompleteness%20issue%20of%20knowledge%20graphs.%20In%20this%20paper%2C%20we%20propose%0A%27Logic-Query-of-Thoughts%27%20%28LGOT%29%20which%20is%20the%20first%20of%20its%20kind%20to%20combine%20LLMs%0Awith%20knowledge%20graph%20based%20logic%20query%20reasoning.%20LGOT%20seamlessly%20combines%0Aknowledge%20graph%20reasoning%20and%20LLMs%2C%20effectively%20breaking%20down%20complex%20logic%0Aqueries%20into%20easy%20to%20answer%20subquestions.%20Through%20the%20utilization%20of%20both%0Aknowledge%20graph%20reasoning%20and%20LLMs%2C%20it%20successfully%20derives%20answers%20for%20each%0Asubquestion.%20By%20aggregating%20these%20results%20and%20selecting%20the%20highest%20quality%0Acandidate%20answers%20for%20each%20step%2C%20LGOT%20achieves%20accurate%20results%20to%20complex%0Aquestions.%20Our%20experimental%20findings%20demonstrate%20substantial%20performance%0Aenhancements%2C%20with%20up%20to%2020%25%20improvement%20over%20ChatGPT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.04264v4&entry.124074799=Read"},
{"title": "Tract-RLFormer: A Tract-Specific RL policy based Decoder-only\n  Transformer Network", "author": "Ankita Joshi and Ashutosh Sharma and Anoushkrit Goel and Ranjeet Ranjan Jha and Chirag Ahuja and Arnav Bhavsar and Aditya Nigam", "abstract": "  Fiber tractography is a cornerstone of neuroimaging, enabling the detailed\nmapping of the brain's white matter pathways through diffusion MRI. This is\ncrucial for understanding brain connectivity and function, making it a valuable\ntool in neurological applications. Despite its importance, tractography faces\nchallenges due to its complexity and susceptibility to false positives,\nmisrepresenting vital pathways. To address these issues, recent strategies have\nshifted towards deep learning, utilizing supervised learning, which depends on\nprecise ground truth, or reinforcement learning, which operates without it. In\nthis work, we propose Tract-RLFormer, a network utilizing both supervised and\nreinforcement learning, in a two-stage policy refinement process that markedly\nimproves the accuracy and generalizability across various data-sets. By\nemploying a tract-specific approach, our network directly delineates the tracts\nof interest, bypassing the traditional segmentation process. Through rigorous\nvalidation on datasets such as TractoInferno, HCP, and ISMRM-2015, our\nmethodology demonstrates a leap forward in tractography, showcasing its ability\nto accurately map the brain's white matter tracts.\n", "link": "http://arxiv.org/abs/2411.05757v1", "date": "2024-11-08", "relevancy": 1.9198, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4937}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4812}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tract-RLFormer%3A%20A%20Tract-Specific%20RL%20policy%20based%20Decoder-only%0A%20%20Transformer%20Network&body=Title%3A%20Tract-RLFormer%3A%20A%20Tract-Specific%20RL%20policy%20based%20Decoder-only%0A%20%20Transformer%20Network%0AAuthor%3A%20Ankita%20Joshi%20and%20Ashutosh%20Sharma%20and%20Anoushkrit%20Goel%20and%20Ranjeet%20Ranjan%20Jha%20and%20Chirag%20Ahuja%20and%20Arnav%20Bhavsar%20and%20Aditya%20Nigam%0AAbstract%3A%20%20%20Fiber%20tractography%20is%20a%20cornerstone%20of%20neuroimaging%2C%20enabling%20the%20detailed%0Amapping%20of%20the%20brain%27s%20white%20matter%20pathways%20through%20diffusion%20MRI.%20This%20is%0Acrucial%20for%20understanding%20brain%20connectivity%20and%20function%2C%20making%20it%20a%20valuable%0Atool%20in%20neurological%20applications.%20Despite%20its%20importance%2C%20tractography%20faces%0Achallenges%20due%20to%20its%20complexity%20and%20susceptibility%20to%20false%20positives%2C%0Amisrepresenting%20vital%20pathways.%20To%20address%20these%20issues%2C%20recent%20strategies%20have%0Ashifted%20towards%20deep%20learning%2C%20utilizing%20supervised%20learning%2C%20which%20depends%20on%0Aprecise%20ground%20truth%2C%20or%20reinforcement%20learning%2C%20which%20operates%20without%20it.%20In%0Athis%20work%2C%20we%20propose%20Tract-RLFormer%2C%20a%20network%20utilizing%20both%20supervised%20and%0Areinforcement%20learning%2C%20in%20a%20two-stage%20policy%20refinement%20process%20that%20markedly%0Aimproves%20the%20accuracy%20and%20generalizability%20across%20various%20data-sets.%20By%0Aemploying%20a%20tract-specific%20approach%2C%20our%20network%20directly%20delineates%20the%20tracts%0Aof%20interest%2C%20bypassing%20the%20traditional%20segmentation%20process.%20Through%20rigorous%0Avalidation%20on%20datasets%20such%20as%20TractoInferno%2C%20HCP%2C%20and%20ISMRM-2015%2C%20our%0Amethodology%20demonstrates%20a%20leap%20forward%20in%20tractography%2C%20showcasing%20its%20ability%0Ato%20accurately%20map%20the%20brain%27s%20white%20matter%20tracts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05757v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTract-RLFormer%253A%2520A%2520Tract-Specific%2520RL%2520policy%2520based%2520Decoder-only%250A%2520%2520Transformer%2520Network%26entry.906535625%3DAnkita%2520Joshi%2520and%2520Ashutosh%2520Sharma%2520and%2520Anoushkrit%2520Goel%2520and%2520Ranjeet%2520Ranjan%2520Jha%2520and%2520Chirag%2520Ahuja%2520and%2520Arnav%2520Bhavsar%2520and%2520Aditya%2520Nigam%26entry.1292438233%3D%2520%2520Fiber%2520tractography%2520is%2520a%2520cornerstone%2520of%2520neuroimaging%252C%2520enabling%2520the%2520detailed%250Amapping%2520of%2520the%2520brain%2527s%2520white%2520matter%2520pathways%2520through%2520diffusion%2520MRI.%2520This%2520is%250Acrucial%2520for%2520understanding%2520brain%2520connectivity%2520and%2520function%252C%2520making%2520it%2520a%2520valuable%250Atool%2520in%2520neurological%2520applications.%2520Despite%2520its%2520importance%252C%2520tractography%2520faces%250Achallenges%2520due%2520to%2520its%2520complexity%2520and%2520susceptibility%2520to%2520false%2520positives%252C%250Amisrepresenting%2520vital%2520pathways.%2520To%2520address%2520these%2520issues%252C%2520recent%2520strategies%2520have%250Ashifted%2520towards%2520deep%2520learning%252C%2520utilizing%2520supervised%2520learning%252C%2520which%2520depends%2520on%250Aprecise%2520ground%2520truth%252C%2520or%2520reinforcement%2520learning%252C%2520which%2520operates%2520without%2520it.%2520In%250Athis%2520work%252C%2520we%2520propose%2520Tract-RLFormer%252C%2520a%2520network%2520utilizing%2520both%2520supervised%2520and%250Areinforcement%2520learning%252C%2520in%2520a%2520two-stage%2520policy%2520refinement%2520process%2520that%2520markedly%250Aimproves%2520the%2520accuracy%2520and%2520generalizability%2520across%2520various%2520data-sets.%2520By%250Aemploying%2520a%2520tract-specific%2520approach%252C%2520our%2520network%2520directly%2520delineates%2520the%2520tracts%250Aof%2520interest%252C%2520bypassing%2520the%2520traditional%2520segmentation%2520process.%2520Through%2520rigorous%250Avalidation%2520on%2520datasets%2520such%2520as%2520TractoInferno%252C%2520HCP%252C%2520and%2520ISMRM-2015%252C%2520our%250Amethodology%2520demonstrates%2520a%2520leap%2520forward%2520in%2520tractography%252C%2520showcasing%2520its%2520ability%250Ato%2520accurately%2520map%2520the%2520brain%2527s%2520white%2520matter%2520tracts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05757v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tract-RLFormer%3A%20A%20Tract-Specific%20RL%20policy%20based%20Decoder-only%0A%20%20Transformer%20Network&entry.906535625=Ankita%20Joshi%20and%20Ashutosh%20Sharma%20and%20Anoushkrit%20Goel%20and%20Ranjeet%20Ranjan%20Jha%20and%20Chirag%20Ahuja%20and%20Arnav%20Bhavsar%20and%20Aditya%20Nigam&entry.1292438233=%20%20Fiber%20tractography%20is%20a%20cornerstone%20of%20neuroimaging%2C%20enabling%20the%20detailed%0Amapping%20of%20the%20brain%27s%20white%20matter%20pathways%20through%20diffusion%20MRI.%20This%20is%0Acrucial%20for%20understanding%20brain%20connectivity%20and%20function%2C%20making%20it%20a%20valuable%0Atool%20in%20neurological%20applications.%20Despite%20its%20importance%2C%20tractography%20faces%0Achallenges%20due%20to%20its%20complexity%20and%20susceptibility%20to%20false%20positives%2C%0Amisrepresenting%20vital%20pathways.%20To%20address%20these%20issues%2C%20recent%20strategies%20have%0Ashifted%20towards%20deep%20learning%2C%20utilizing%20supervised%20learning%2C%20which%20depends%20on%0Aprecise%20ground%20truth%2C%20or%20reinforcement%20learning%2C%20which%20operates%20without%20it.%20In%0Athis%20work%2C%20we%20propose%20Tract-RLFormer%2C%20a%20network%20utilizing%20both%20supervised%20and%0Areinforcement%20learning%2C%20in%20a%20two-stage%20policy%20refinement%20process%20that%20markedly%0Aimproves%20the%20accuracy%20and%20generalizability%20across%20various%20data-sets.%20By%0Aemploying%20a%20tract-specific%20approach%2C%20our%20network%20directly%20delineates%20the%20tracts%0Aof%20interest%2C%20bypassing%20the%20traditional%20segmentation%20process.%20Through%20rigorous%0Avalidation%20on%20datasets%20such%20as%20TractoInferno%2C%20HCP%2C%20and%20ISMRM-2015%2C%20our%0Amethodology%20demonstrates%20a%20leap%20forward%20in%20tractography%2C%20showcasing%20its%20ability%0Ato%20accurately%20map%20the%20brain%27s%20white%20matter%20tracts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05757v1&entry.124074799=Read"},
{"title": "Equivariant IMU Preintegration with Biases: an Inhomogeneous Galilean\n  Group Approach", "author": "Giulio Delama and Alessandro Fornasier and Robert Mahony and Stephan Weiss", "abstract": "  This letter proposes a new approach for Inertial Measurement Unit (IMU)\npreintegration, a fundamental building block that can be leveraged in different\noptimization-based Inertial Navigation System (INS) localization solutions.\nInspired by recent advancements in equivariant theory applied to biased INSs,\nwe derive a discrete-time formulation of the IMU preintegration on\n$\\mathbf{G}(3) \\ltimes \\mathfrak{g}(3)$, the tangent group of the inhomogeneous\nGalilean group $\\mathbf{G}(3)$. We define a novel preintegration error that\ngeometrically couples the navigation states and the bias leading to lower\nlinearization error. Our method improves in consistency compared to existing\npreintegration approaches which treat IMU biases as a separate state-space.\nExtensive validation against state-of-the-art methods, both in simulation and\nwith real-world IMU data, implementation in the Lie++ library, and\nopen-sourcing of the code are provided.\n", "link": "http://arxiv.org/abs/2411.05548v1", "date": "2024-11-08", "relevancy": 1.9195, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4877}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4769}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equivariant%20IMU%20Preintegration%20with%20Biases%3A%20an%20Inhomogeneous%20Galilean%0A%20%20Group%20Approach&body=Title%3A%20Equivariant%20IMU%20Preintegration%20with%20Biases%3A%20an%20Inhomogeneous%20Galilean%0A%20%20Group%20Approach%0AAuthor%3A%20Giulio%20Delama%20and%20Alessandro%20Fornasier%20and%20Robert%20Mahony%20and%20Stephan%20Weiss%0AAbstract%3A%20%20%20This%20letter%20proposes%20a%20new%20approach%20for%20Inertial%20Measurement%20Unit%20%28IMU%29%0Apreintegration%2C%20a%20fundamental%20building%20block%20that%20can%20be%20leveraged%20in%20different%0Aoptimization-based%20Inertial%20Navigation%20System%20%28INS%29%20localization%20solutions.%0AInspired%20by%20recent%20advancements%20in%20equivariant%20theory%20applied%20to%20biased%20INSs%2C%0Awe%20derive%20a%20discrete-time%20formulation%20of%20the%20IMU%20preintegration%20on%0A%24%5Cmathbf%7BG%7D%283%29%20%5Cltimes%20%5Cmathfrak%7Bg%7D%283%29%24%2C%20the%20tangent%20group%20of%20the%20inhomogeneous%0AGalilean%20group%20%24%5Cmathbf%7BG%7D%283%29%24.%20We%20define%20a%20novel%20preintegration%20error%20that%0Ageometrically%20couples%20the%20navigation%20states%20and%20the%20bias%20leading%20to%20lower%0Alinearization%20error.%20Our%20method%20improves%20in%20consistency%20compared%20to%20existing%0Apreintegration%20approaches%20which%20treat%20IMU%20biases%20as%20a%20separate%20state-space.%0AExtensive%20validation%20against%20state-of-the-art%20methods%2C%20both%20in%20simulation%20and%0Awith%20real-world%20IMU%20data%2C%20implementation%20in%20the%20Lie%2B%2B%20library%2C%20and%0Aopen-sourcing%20of%20the%20code%20are%20provided.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05548v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivariant%2520IMU%2520Preintegration%2520with%2520Biases%253A%2520an%2520Inhomogeneous%2520Galilean%250A%2520%2520Group%2520Approach%26entry.906535625%3DGiulio%2520Delama%2520and%2520Alessandro%2520Fornasier%2520and%2520Robert%2520Mahony%2520and%2520Stephan%2520Weiss%26entry.1292438233%3D%2520%2520This%2520letter%2520proposes%2520a%2520new%2520approach%2520for%2520Inertial%2520Measurement%2520Unit%2520%2528IMU%2529%250Apreintegration%252C%2520a%2520fundamental%2520building%2520block%2520that%2520can%2520be%2520leveraged%2520in%2520different%250Aoptimization-based%2520Inertial%2520Navigation%2520System%2520%2528INS%2529%2520localization%2520solutions.%250AInspired%2520by%2520recent%2520advancements%2520in%2520equivariant%2520theory%2520applied%2520to%2520biased%2520INSs%252C%250Awe%2520derive%2520a%2520discrete-time%2520formulation%2520of%2520the%2520IMU%2520preintegration%2520on%250A%2524%255Cmathbf%257BG%257D%25283%2529%2520%255Cltimes%2520%255Cmathfrak%257Bg%257D%25283%2529%2524%252C%2520the%2520tangent%2520group%2520of%2520the%2520inhomogeneous%250AGalilean%2520group%2520%2524%255Cmathbf%257BG%257D%25283%2529%2524.%2520We%2520define%2520a%2520novel%2520preintegration%2520error%2520that%250Ageometrically%2520couples%2520the%2520navigation%2520states%2520and%2520the%2520bias%2520leading%2520to%2520lower%250Alinearization%2520error.%2520Our%2520method%2520improves%2520in%2520consistency%2520compared%2520to%2520existing%250Apreintegration%2520approaches%2520which%2520treat%2520IMU%2520biases%2520as%2520a%2520separate%2520state-space.%250AExtensive%2520validation%2520against%2520state-of-the-art%2520methods%252C%2520both%2520in%2520simulation%2520and%250Awith%2520real-world%2520IMU%2520data%252C%2520implementation%2520in%2520the%2520Lie%252B%252B%2520library%252C%2520and%250Aopen-sourcing%2520of%2520the%2520code%2520are%2520provided.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05548v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivariant%20IMU%20Preintegration%20with%20Biases%3A%20an%20Inhomogeneous%20Galilean%0A%20%20Group%20Approach&entry.906535625=Giulio%20Delama%20and%20Alessandro%20Fornasier%20and%20Robert%20Mahony%20and%20Stephan%20Weiss&entry.1292438233=%20%20This%20letter%20proposes%20a%20new%20approach%20for%20Inertial%20Measurement%20Unit%20%28IMU%29%0Apreintegration%2C%20a%20fundamental%20building%20block%20that%20can%20be%20leveraged%20in%20different%0Aoptimization-based%20Inertial%20Navigation%20System%20%28INS%29%20localization%20solutions.%0AInspired%20by%20recent%20advancements%20in%20equivariant%20theory%20applied%20to%20biased%20INSs%2C%0Awe%20derive%20a%20discrete-time%20formulation%20of%20the%20IMU%20preintegration%20on%0A%24%5Cmathbf%7BG%7D%283%29%20%5Cltimes%20%5Cmathfrak%7Bg%7D%283%29%24%2C%20the%20tangent%20group%20of%20the%20inhomogeneous%0AGalilean%20group%20%24%5Cmathbf%7BG%7D%283%29%24.%20We%20define%20a%20novel%20preintegration%20error%20that%0Ageometrically%20couples%20the%20navigation%20states%20and%20the%20bias%20leading%20to%20lower%0Alinearization%20error.%20Our%20method%20improves%20in%20consistency%20compared%20to%20existing%0Apreintegration%20approaches%20which%20treat%20IMU%20biases%20as%20a%20separate%20state-space.%0AExtensive%20validation%20against%20state-of-the-art%20methods%2C%20both%20in%20simulation%20and%0Awith%20real-world%20IMU%20data%2C%20implementation%20in%20the%20Lie%2B%2B%20library%2C%20and%0Aopen-sourcing%20of%20the%20code%20are%20provided.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05548v1&entry.124074799=Read"},
{"title": "Loop Neural Networks for Parameter Sharing", "author": "Kei-Sing Ng and Qingchen Wang", "abstract": "  The success of large-scale language models like GPT can be attributed to\ntheir ability to efficiently predict the next token in a sequence. However,\nthese models rely on constant computational effort regardless of the complexity\nof the token they are predicting, lacking the capacity for iterative\nrefinement. In this paper, we introduce a novel Loop Neural Network, which\nachieves better performance by utilizing longer computational time without\nincreasing the model size. Our approach revisits the input multiple times,\nrefining the prediction by iteratively looping over a subset of the model with\nresidual connections. We demonstrate the effectiveness of this method through\nexperiments comparing versions of GPT-2 with our loop models, showing improved\nperformance in language modeling tasks while maintaining similar parameter\ncounts. Importantly, these improvements are achieved without the need for extra\ntraining data.\n", "link": "http://arxiv.org/abs/2409.14199v3", "date": "2024-11-08", "relevancy": 1.919, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4956}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4773}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Loop%20Neural%20Networks%20for%20Parameter%20Sharing&body=Title%3A%20Loop%20Neural%20Networks%20for%20Parameter%20Sharing%0AAuthor%3A%20Kei-Sing%20Ng%20and%20Qingchen%20Wang%0AAbstract%3A%20%20%20The%20success%20of%20large-scale%20language%20models%20like%20GPT%20can%20be%20attributed%20to%0Atheir%20ability%20to%20efficiently%20predict%20the%20next%20token%20in%20a%20sequence.%20However%2C%0Athese%20models%20rely%20on%20constant%20computational%20effort%20regardless%20of%20the%20complexity%0Aof%20the%20token%20they%20are%20predicting%2C%20lacking%20the%20capacity%20for%20iterative%0Arefinement.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20Loop%20Neural%20Network%2C%20which%0Aachieves%20better%20performance%20by%20utilizing%20longer%20computational%20time%20without%0Aincreasing%20the%20model%20size.%20Our%20approach%20revisits%20the%20input%20multiple%20times%2C%0Arefining%20the%20prediction%20by%20iteratively%20looping%20over%20a%20subset%20of%20the%20model%20with%0Aresidual%20connections.%20We%20demonstrate%20the%20effectiveness%20of%20this%20method%20through%0Aexperiments%20comparing%20versions%20of%20GPT-2%20with%20our%20loop%20models%2C%20showing%20improved%0Aperformance%20in%20language%20modeling%20tasks%20while%20maintaining%20similar%20parameter%0Acounts.%20Importantly%2C%20these%20improvements%20are%20achieved%20without%20the%20need%20for%20extra%0Atraining%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14199v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoop%2520Neural%2520Networks%2520for%2520Parameter%2520Sharing%26entry.906535625%3DKei-Sing%2520Ng%2520and%2520Qingchen%2520Wang%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520large-scale%2520language%2520models%2520like%2520GPT%2520can%2520be%2520attributed%2520to%250Atheir%2520ability%2520to%2520efficiently%2520predict%2520the%2520next%2520token%2520in%2520a%2520sequence.%2520However%252C%250Athese%2520models%2520rely%2520on%2520constant%2520computational%2520effort%2520regardless%2520of%2520the%2520complexity%250Aof%2520the%2520token%2520they%2520are%2520predicting%252C%2520lacking%2520the%2520capacity%2520for%2520iterative%250Arefinement.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520Loop%2520Neural%2520Network%252C%2520which%250Aachieves%2520better%2520performance%2520by%2520utilizing%2520longer%2520computational%2520time%2520without%250Aincreasing%2520the%2520model%2520size.%2520Our%2520approach%2520revisits%2520the%2520input%2520multiple%2520times%252C%250Arefining%2520the%2520prediction%2520by%2520iteratively%2520looping%2520over%2520a%2520subset%2520of%2520the%2520model%2520with%250Aresidual%2520connections.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520this%2520method%2520through%250Aexperiments%2520comparing%2520versions%2520of%2520GPT-2%2520with%2520our%2520loop%2520models%252C%2520showing%2520improved%250Aperformance%2520in%2520language%2520modeling%2520tasks%2520while%2520maintaining%2520similar%2520parameter%250Acounts.%2520Importantly%252C%2520these%2520improvements%2520are%2520achieved%2520without%2520the%2520need%2520for%2520extra%250Atraining%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14199v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Loop%20Neural%20Networks%20for%20Parameter%20Sharing&entry.906535625=Kei-Sing%20Ng%20and%20Qingchen%20Wang&entry.1292438233=%20%20The%20success%20of%20large-scale%20language%20models%20like%20GPT%20can%20be%20attributed%20to%0Atheir%20ability%20to%20efficiently%20predict%20the%20next%20token%20in%20a%20sequence.%20However%2C%0Athese%20models%20rely%20on%20constant%20computational%20effort%20regardless%20of%20the%20complexity%0Aof%20the%20token%20they%20are%20predicting%2C%20lacking%20the%20capacity%20for%20iterative%0Arefinement.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20Loop%20Neural%20Network%2C%20which%0Aachieves%20better%20performance%20by%20utilizing%20longer%20computational%20time%20without%0Aincreasing%20the%20model%20size.%20Our%20approach%20revisits%20the%20input%20multiple%20times%2C%0Arefining%20the%20prediction%20by%20iteratively%20looping%20over%20a%20subset%20of%20the%20model%20with%0Aresidual%20connections.%20We%20demonstrate%20the%20effectiveness%20of%20this%20method%20through%0Aexperiments%20comparing%20versions%20of%20GPT-2%20with%20our%20loop%20models%2C%20showing%20improved%0Aperformance%20in%20language%20modeling%20tasks%20while%20maintaining%20similar%20parameter%0Acounts.%20Importantly%2C%20these%20improvements%20are%20achieved%20without%20the%20need%20for%20extra%0Atraining%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14199v3&entry.124074799=Read"},
{"title": "SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion\n  Models", "author": "Muyang Li and Yujun Lin and Zhekai Zhang and Tianle Cai and Xiuyu Li and Junxian Guo and Enze Xie and Chenlin Meng and Jun-Yan Zhu and Song Han", "abstract": "  Diffusion models have been proven highly effective at generating high-quality\nimages. However, as these models grow larger, they require significantly more\nmemory and suffer from higher latency, posing substantial challenges for\ndeployment. In this work, we aim to accelerate diffusion models by quantizing\ntheir weights and activations to 4 bits. At such an aggressive level, both\nweights and activations are highly sensitive, where conventional post-training\nquantization methods for large language models like smoothing become\ninsufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit\nquantization paradigm. Different from smoothing which redistributes outliers\nbetween weights and activations, our approach absorbs these outliers using a\nlow-rank branch. We first consolidate the outliers by shifting them from\nactivations to weights, then employ a high-precision low-rank branch to take in\nthe weight outliers with Singular Value Decomposition (SVD). This process eases\nthe quantization on both sides. However, na\\\"{\\i}vely running the low-rank\nbranch independently incurs significant overhead due to extra data movement of\nactivations, negating the quantization speedup. To address this, we co-design\nan inference engine Nunchaku that fuses the kernels of the low-rank branch into\nthose of the low-bit branch to cut off redundant memory access. It can also\nseamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for\nre-quantization. Extensive experiments on SDXL, PixArt-$\\Sigma$, and FLUX.1\nvalidate the effectiveness of SVDQuant in preserving image quality. We reduce\nthe memory usage for the 12B FLUX.1 models by 3.5$\\times$, achieving\n3.0$\\times$ speedup over the 4-bit weight-only quantized baseline on the 16GB\nlaptop 4090 GPU, paving the way for more interactive applications on PCs. Our\nquantization library and inference engine are open-sourced.\n", "link": "http://arxiv.org/abs/2411.05007v2", "date": "2024-11-08", "relevancy": 1.9115, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.7068}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6227}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SVDQuant%3A%20Absorbing%20Outliers%20by%20Low-Rank%20Components%20for%204-Bit%20Diffusion%0A%20%20Models&body=Title%3A%20SVDQuant%3A%20Absorbing%20Outliers%20by%20Low-Rank%20Components%20for%204-Bit%20Diffusion%0A%20%20Models%0AAuthor%3A%20Muyang%20Li%20and%20Yujun%20Lin%20and%20Zhekai%20Zhang%20and%20Tianle%20Cai%20and%20Xiuyu%20Li%20and%20Junxian%20Guo%20and%20Enze%20Xie%20and%20Chenlin%20Meng%20and%20Jun-Yan%20Zhu%20and%20Song%20Han%0AAbstract%3A%20%20%20Diffusion%20models%20have%20been%20proven%20highly%20effective%20at%20generating%20high-quality%0Aimages.%20However%2C%20as%20these%20models%20grow%20larger%2C%20they%20require%20significantly%20more%0Amemory%20and%20suffer%20from%20higher%20latency%2C%20posing%20substantial%20challenges%20for%0Adeployment.%20In%20this%20work%2C%20we%20aim%20to%20accelerate%20diffusion%20models%20by%20quantizing%0Atheir%20weights%20and%20activations%20to%204%20bits.%20At%20such%20an%20aggressive%20level%2C%20both%0Aweights%20and%20activations%20are%20highly%20sensitive%2C%20where%20conventional%20post-training%0Aquantization%20methods%20for%20large%20language%20models%20like%20smoothing%20become%0Ainsufficient.%20To%20overcome%20this%20limitation%2C%20we%20propose%20SVDQuant%2C%20a%20new%204-bit%0Aquantization%20paradigm.%20Different%20from%20smoothing%20which%20redistributes%20outliers%0Abetween%20weights%20and%20activations%2C%20our%20approach%20absorbs%20these%20outliers%20using%20a%0Alow-rank%20branch.%20We%20first%20consolidate%20the%20outliers%20by%20shifting%20them%20from%0Aactivations%20to%20weights%2C%20then%20employ%20a%20high-precision%20low-rank%20branch%20to%20take%20in%0Athe%20weight%20outliers%20with%20Singular%20Value%20Decomposition%20%28SVD%29.%20This%20process%20eases%0Athe%20quantization%20on%20both%20sides.%20However%2C%20na%5C%22%7B%5Ci%7Dvely%20running%20the%20low-rank%0Abranch%20independently%20incurs%20significant%20overhead%20due%20to%20extra%20data%20movement%20of%0Aactivations%2C%20negating%20the%20quantization%20speedup.%20To%20address%20this%2C%20we%20co-design%0Aan%20inference%20engine%20Nunchaku%20that%20fuses%20the%20kernels%20of%20the%20low-rank%20branch%20into%0Athose%20of%20the%20low-bit%20branch%20to%20cut%20off%20redundant%20memory%20access.%20It%20can%20also%0Aseamlessly%20support%20off-the-shelf%20low-rank%20adapters%20%28LoRAs%29%20without%20the%20need%20for%0Are-quantization.%20Extensive%20experiments%20on%20SDXL%2C%20PixArt-%24%5CSigma%24%2C%20and%20FLUX.1%0Avalidate%20the%20effectiveness%20of%20SVDQuant%20in%20preserving%20image%20quality.%20We%20reduce%0Athe%20memory%20usage%20for%20the%2012B%20FLUX.1%20models%20by%203.5%24%5Ctimes%24%2C%20achieving%0A3.0%24%5Ctimes%24%20speedup%20over%20the%204-bit%20weight-only%20quantized%20baseline%20on%20the%2016GB%0Alaptop%204090%20GPU%2C%20paving%20the%20way%20for%20more%20interactive%20applications%20on%20PCs.%20Our%0Aquantization%20library%20and%20inference%20engine%20are%20open-sourced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05007v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSVDQuant%253A%2520Absorbing%2520Outliers%2520by%2520Low-Rank%2520Components%2520for%25204-Bit%2520Diffusion%250A%2520%2520Models%26entry.906535625%3DMuyang%2520Li%2520and%2520Yujun%2520Lin%2520and%2520Zhekai%2520Zhang%2520and%2520Tianle%2520Cai%2520and%2520Xiuyu%2520Li%2520and%2520Junxian%2520Guo%2520and%2520Enze%2520Xie%2520and%2520Chenlin%2520Meng%2520and%2520Jun-Yan%2520Zhu%2520and%2520Song%2520Han%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520been%2520proven%2520highly%2520effective%2520at%2520generating%2520high-quality%250Aimages.%2520However%252C%2520as%2520these%2520models%2520grow%2520larger%252C%2520they%2520require%2520significantly%2520more%250Amemory%2520and%2520suffer%2520from%2520higher%2520latency%252C%2520posing%2520substantial%2520challenges%2520for%250Adeployment.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520accelerate%2520diffusion%2520models%2520by%2520quantizing%250Atheir%2520weights%2520and%2520activations%2520to%25204%2520bits.%2520At%2520such%2520an%2520aggressive%2520level%252C%2520both%250Aweights%2520and%2520activations%2520are%2520highly%2520sensitive%252C%2520where%2520conventional%2520post-training%250Aquantization%2520methods%2520for%2520large%2520language%2520models%2520like%2520smoothing%2520become%250Ainsufficient.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520SVDQuant%252C%2520a%2520new%25204-bit%250Aquantization%2520paradigm.%2520Different%2520from%2520smoothing%2520which%2520redistributes%2520outliers%250Abetween%2520weights%2520and%2520activations%252C%2520our%2520approach%2520absorbs%2520these%2520outliers%2520using%2520a%250Alow-rank%2520branch.%2520We%2520first%2520consolidate%2520the%2520outliers%2520by%2520shifting%2520them%2520from%250Aactivations%2520to%2520weights%252C%2520then%2520employ%2520a%2520high-precision%2520low-rank%2520branch%2520to%2520take%2520in%250Athe%2520weight%2520outliers%2520with%2520Singular%2520Value%2520Decomposition%2520%2528SVD%2529.%2520This%2520process%2520eases%250Athe%2520quantization%2520on%2520both%2520sides.%2520However%252C%2520na%255C%2522%257B%255Ci%257Dvely%2520running%2520the%2520low-rank%250Abranch%2520independently%2520incurs%2520significant%2520overhead%2520due%2520to%2520extra%2520data%2520movement%2520of%250Aactivations%252C%2520negating%2520the%2520quantization%2520speedup.%2520To%2520address%2520this%252C%2520we%2520co-design%250Aan%2520inference%2520engine%2520Nunchaku%2520that%2520fuses%2520the%2520kernels%2520of%2520the%2520low-rank%2520branch%2520into%250Athose%2520of%2520the%2520low-bit%2520branch%2520to%2520cut%2520off%2520redundant%2520memory%2520access.%2520It%2520can%2520also%250Aseamlessly%2520support%2520off-the-shelf%2520low-rank%2520adapters%2520%2528LoRAs%2529%2520without%2520the%2520need%2520for%250Are-quantization.%2520Extensive%2520experiments%2520on%2520SDXL%252C%2520PixArt-%2524%255CSigma%2524%252C%2520and%2520FLUX.1%250Avalidate%2520the%2520effectiveness%2520of%2520SVDQuant%2520in%2520preserving%2520image%2520quality.%2520We%2520reduce%250Athe%2520memory%2520usage%2520for%2520the%252012B%2520FLUX.1%2520models%2520by%25203.5%2524%255Ctimes%2524%252C%2520achieving%250A3.0%2524%255Ctimes%2524%2520speedup%2520over%2520the%25204-bit%2520weight-only%2520quantized%2520baseline%2520on%2520the%252016GB%250Alaptop%25204090%2520GPU%252C%2520paving%2520the%2520way%2520for%2520more%2520interactive%2520applications%2520on%2520PCs.%2520Our%250Aquantization%2520library%2520and%2520inference%2520engine%2520are%2520open-sourced.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05007v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SVDQuant%3A%20Absorbing%20Outliers%20by%20Low-Rank%20Components%20for%204-Bit%20Diffusion%0A%20%20Models&entry.906535625=Muyang%20Li%20and%20Yujun%20Lin%20and%20Zhekai%20Zhang%20and%20Tianle%20Cai%20and%20Xiuyu%20Li%20and%20Junxian%20Guo%20and%20Enze%20Xie%20and%20Chenlin%20Meng%20and%20Jun-Yan%20Zhu%20and%20Song%20Han&entry.1292438233=%20%20Diffusion%20models%20have%20been%20proven%20highly%20effective%20at%20generating%20high-quality%0Aimages.%20However%2C%20as%20these%20models%20grow%20larger%2C%20they%20require%20significantly%20more%0Amemory%20and%20suffer%20from%20higher%20latency%2C%20posing%20substantial%20challenges%20for%0Adeployment.%20In%20this%20work%2C%20we%20aim%20to%20accelerate%20diffusion%20models%20by%20quantizing%0Atheir%20weights%20and%20activations%20to%204%20bits.%20At%20such%20an%20aggressive%20level%2C%20both%0Aweights%20and%20activations%20are%20highly%20sensitive%2C%20where%20conventional%20post-training%0Aquantization%20methods%20for%20large%20language%20models%20like%20smoothing%20become%0Ainsufficient.%20To%20overcome%20this%20limitation%2C%20we%20propose%20SVDQuant%2C%20a%20new%204-bit%0Aquantization%20paradigm.%20Different%20from%20smoothing%20which%20redistributes%20outliers%0Abetween%20weights%20and%20activations%2C%20our%20approach%20absorbs%20these%20outliers%20using%20a%0Alow-rank%20branch.%20We%20first%20consolidate%20the%20outliers%20by%20shifting%20them%20from%0Aactivations%20to%20weights%2C%20then%20employ%20a%20high-precision%20low-rank%20branch%20to%20take%20in%0Athe%20weight%20outliers%20with%20Singular%20Value%20Decomposition%20%28SVD%29.%20This%20process%20eases%0Athe%20quantization%20on%20both%20sides.%20However%2C%20na%5C%22%7B%5Ci%7Dvely%20running%20the%20low-rank%0Abranch%20independently%20incurs%20significant%20overhead%20due%20to%20extra%20data%20movement%20of%0Aactivations%2C%20negating%20the%20quantization%20speedup.%20To%20address%20this%2C%20we%20co-design%0Aan%20inference%20engine%20Nunchaku%20that%20fuses%20the%20kernels%20of%20the%20low-rank%20branch%20into%0Athose%20of%20the%20low-bit%20branch%20to%20cut%20off%20redundant%20memory%20access.%20It%20can%20also%0Aseamlessly%20support%20off-the-shelf%20low-rank%20adapters%20%28LoRAs%29%20without%20the%20need%20for%0Are-quantization.%20Extensive%20experiments%20on%20SDXL%2C%20PixArt-%24%5CSigma%24%2C%20and%20FLUX.1%0Avalidate%20the%20effectiveness%20of%20SVDQuant%20in%20preserving%20image%20quality.%20We%20reduce%0Athe%20memory%20usage%20for%20the%2012B%20FLUX.1%20models%20by%203.5%24%5Ctimes%24%2C%20achieving%0A3.0%24%5Ctimes%24%20speedup%20over%20the%204-bit%20weight-only%20quantized%20baseline%20on%20the%2016GB%0Alaptop%204090%20GPU%2C%20paving%20the%20way%20for%20more%20interactive%20applications%20on%20PCs.%20Our%0Aquantization%20library%20and%20inference%20engine%20are%20open-sourced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05007v2&entry.124074799=Read"},
{"title": "Untrained neural networks can demonstrate memorization-independent\n  abstract reasoning", "author": "Tomer Barak and Yonatan Loewenstein", "abstract": "  The nature of abstract reasoning is a matter of debate. Modern artificial\nneural network (ANN) models, like large language models, demonstrate impressive\nsuccess when tested on abstract reasoning problems. However, it has been argued\nthat their success reflects some form of memorization of similar problems (data\ncontamination) rather than a general-purpose abstract reasoning capability.\nThis concern is supported by evidence of brittleness, and the requirement of\nextensive training. In our study, we explored whether abstract reasoning can be\nachieved using the toolbox of ANNs, without prior training. Specifically, we\nstudied an ANN model in which the weights of a naive network are optimized\nduring the solution of the problem, using the problem data itself, rather than\nany prior knowledge. We tested this modeling approach on visual reasoning\nproblems and found that it performs relatively well. Crucially, this success\ndoes not rely on memorization of similar problems. We further suggest an\nexplanation of how it works. Finally, as problem solving is performed by\nchanging the ANN weights, we explored the connection between problem solving\nand the accumulation of knowledge in the ANNs.\n", "link": "http://arxiv.org/abs/2407.17791v2", "date": "2024-11-08", "relevancy": 1.9073, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4794}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Untrained%20neural%20networks%20can%20demonstrate%20memorization-independent%0A%20%20abstract%20reasoning&body=Title%3A%20Untrained%20neural%20networks%20can%20demonstrate%20memorization-independent%0A%20%20abstract%20reasoning%0AAuthor%3A%20Tomer%20Barak%20and%20Yonatan%20Loewenstein%0AAbstract%3A%20%20%20The%20nature%20of%20abstract%20reasoning%20is%20a%20matter%20of%20debate.%20Modern%20artificial%0Aneural%20network%20%28ANN%29%20models%2C%20like%20large%20language%20models%2C%20demonstrate%20impressive%0Asuccess%20when%20tested%20on%20abstract%20reasoning%20problems.%20However%2C%20it%20has%20been%20argued%0Athat%20their%20success%20reflects%20some%20form%20of%20memorization%20of%20similar%20problems%20%28data%0Acontamination%29%20rather%20than%20a%20general-purpose%20abstract%20reasoning%20capability.%0AThis%20concern%20is%20supported%20by%20evidence%20of%20brittleness%2C%20and%20the%20requirement%20of%0Aextensive%20training.%20In%20our%20study%2C%20we%20explored%20whether%20abstract%20reasoning%20can%20be%0Aachieved%20using%20the%20toolbox%20of%20ANNs%2C%20without%20prior%20training.%20Specifically%2C%20we%0Astudied%20an%20ANN%20model%20in%20which%20the%20weights%20of%20a%20naive%20network%20are%20optimized%0Aduring%20the%20solution%20of%20the%20problem%2C%20using%20the%20problem%20data%20itself%2C%20rather%20than%0Aany%20prior%20knowledge.%20We%20tested%20this%20modeling%20approach%20on%20visual%20reasoning%0Aproblems%20and%20found%20that%20it%20performs%20relatively%20well.%20Crucially%2C%20this%20success%0Adoes%20not%20rely%20on%20memorization%20of%20similar%20problems.%20We%20further%20suggest%20an%0Aexplanation%20of%20how%20it%20works.%20Finally%2C%20as%20problem%20solving%20is%20performed%20by%0Achanging%20the%20ANN%20weights%2C%20we%20explored%20the%20connection%20between%20problem%20solving%0Aand%20the%20accumulation%20of%20knowledge%20in%20the%20ANNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17791v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUntrained%2520neural%2520networks%2520can%2520demonstrate%2520memorization-independent%250A%2520%2520abstract%2520reasoning%26entry.906535625%3DTomer%2520Barak%2520and%2520Yonatan%2520Loewenstein%26entry.1292438233%3D%2520%2520The%2520nature%2520of%2520abstract%2520reasoning%2520is%2520a%2520matter%2520of%2520debate.%2520Modern%2520artificial%250Aneural%2520network%2520%2528ANN%2529%2520models%252C%2520like%2520large%2520language%2520models%252C%2520demonstrate%2520impressive%250Asuccess%2520when%2520tested%2520on%2520abstract%2520reasoning%2520problems.%2520However%252C%2520it%2520has%2520been%2520argued%250Athat%2520their%2520success%2520reflects%2520some%2520form%2520of%2520memorization%2520of%2520similar%2520problems%2520%2528data%250Acontamination%2529%2520rather%2520than%2520a%2520general-purpose%2520abstract%2520reasoning%2520capability.%250AThis%2520concern%2520is%2520supported%2520by%2520evidence%2520of%2520brittleness%252C%2520and%2520the%2520requirement%2520of%250Aextensive%2520training.%2520In%2520our%2520study%252C%2520we%2520explored%2520whether%2520abstract%2520reasoning%2520can%2520be%250Aachieved%2520using%2520the%2520toolbox%2520of%2520ANNs%252C%2520without%2520prior%2520training.%2520Specifically%252C%2520we%250Astudied%2520an%2520ANN%2520model%2520in%2520which%2520the%2520weights%2520of%2520a%2520naive%2520network%2520are%2520optimized%250Aduring%2520the%2520solution%2520of%2520the%2520problem%252C%2520using%2520the%2520problem%2520data%2520itself%252C%2520rather%2520than%250Aany%2520prior%2520knowledge.%2520We%2520tested%2520this%2520modeling%2520approach%2520on%2520visual%2520reasoning%250Aproblems%2520and%2520found%2520that%2520it%2520performs%2520relatively%2520well.%2520Crucially%252C%2520this%2520success%250Adoes%2520not%2520rely%2520on%2520memorization%2520of%2520similar%2520problems.%2520We%2520further%2520suggest%2520an%250Aexplanation%2520of%2520how%2520it%2520works.%2520Finally%252C%2520as%2520problem%2520solving%2520is%2520performed%2520by%250Achanging%2520the%2520ANN%2520weights%252C%2520we%2520explored%2520the%2520connection%2520between%2520problem%2520solving%250Aand%2520the%2520accumulation%2520of%2520knowledge%2520in%2520the%2520ANNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17791v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Untrained%20neural%20networks%20can%20demonstrate%20memorization-independent%0A%20%20abstract%20reasoning&entry.906535625=Tomer%20Barak%20and%20Yonatan%20Loewenstein&entry.1292438233=%20%20The%20nature%20of%20abstract%20reasoning%20is%20a%20matter%20of%20debate.%20Modern%20artificial%0Aneural%20network%20%28ANN%29%20models%2C%20like%20large%20language%20models%2C%20demonstrate%20impressive%0Asuccess%20when%20tested%20on%20abstract%20reasoning%20problems.%20However%2C%20it%20has%20been%20argued%0Athat%20their%20success%20reflects%20some%20form%20of%20memorization%20of%20similar%20problems%20%28data%0Acontamination%29%20rather%20than%20a%20general-purpose%20abstract%20reasoning%20capability.%0AThis%20concern%20is%20supported%20by%20evidence%20of%20brittleness%2C%20and%20the%20requirement%20of%0Aextensive%20training.%20In%20our%20study%2C%20we%20explored%20whether%20abstract%20reasoning%20can%20be%0Aachieved%20using%20the%20toolbox%20of%20ANNs%2C%20without%20prior%20training.%20Specifically%2C%20we%0Astudied%20an%20ANN%20model%20in%20which%20the%20weights%20of%20a%20naive%20network%20are%20optimized%0Aduring%20the%20solution%20of%20the%20problem%2C%20using%20the%20problem%20data%20itself%2C%20rather%20than%0Aany%20prior%20knowledge.%20We%20tested%20this%20modeling%20approach%20on%20visual%20reasoning%0Aproblems%20and%20found%20that%20it%20performs%20relatively%20well.%20Crucially%2C%20this%20success%0Adoes%20not%20rely%20on%20memorization%20of%20similar%20problems.%20We%20further%20suggest%20an%0Aexplanation%20of%20how%20it%20works.%20Finally%2C%20as%20problem%20solving%20is%20performed%20by%0Achanging%20the%20ANN%20weights%2C%20we%20explored%20the%20connection%20between%20problem%20solving%0Aand%20the%20accumulation%20of%20knowledge%20in%20the%20ANNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17791v2&entry.124074799=Read"},
{"title": "Free Record-Level Privacy Risk Evaluation Through Artifact-Based Methods", "author": "Joseph Pollock and Igor Shilov and Euodia Dodd and Yves-Alexandre de Montjoye", "abstract": "  Membership inference attacks (MIAs) are widely used to empirically assess the\nprivacy risks of samples used to train a target machine learning model.\nState-of-the-art methods however require training hundreds of shadow models,\nwith the same size and architecture of the target model, solely to evaluate the\nprivacy risk. While one might be able to afford this for small models, the cost\noften becomes prohibitive for medium and large models.\n  We here instead propose a novel approach to identify the at-risk samples\nusing only artifacts available during training, with little to no additional\ncomputational overhead. Our method analyzes individual per-sample loss traces\nand uses them to identify the vulnerable data samples. We demonstrate the\neffectiveness of our artifact-based approach through experiments on the CIFAR10\ndataset, showing high precision in identifying vulnerable samples as determined\nby a SOTA shadow model-based MIA (LiRA). Impressively, our method reaches the\nsame precision as another SOTA MIA when measured against LiRA, despite it being\norders of magnitude cheaper. We then show LT-IQR to outperform alternative loss\naggregation methods, perform ablation studies on hyperparameters, and validate\nthe robustness of our method to the target metric. Finally, we study the\nevolution of the vulnerability score distribution throughout training as a\nmetric for model-level risk assessment.\n", "link": "http://arxiv.org/abs/2411.05743v1", "date": "2024-11-08", "relevancy": 1.9005, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5127}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4773}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Free%20Record-Level%20Privacy%20Risk%20Evaluation%20Through%20Artifact-Based%20Methods&body=Title%3A%20Free%20Record-Level%20Privacy%20Risk%20Evaluation%20Through%20Artifact-Based%20Methods%0AAuthor%3A%20Joseph%20Pollock%20and%20Igor%20Shilov%20and%20Euodia%20Dodd%20and%20Yves-Alexandre%20de%20Montjoye%0AAbstract%3A%20%20%20Membership%20inference%20attacks%20%28MIAs%29%20are%20widely%20used%20to%20empirically%20assess%20the%0Aprivacy%20risks%20of%20samples%20used%20to%20train%20a%20target%20machine%20learning%20model.%0AState-of-the-art%20methods%20however%20require%20training%20hundreds%20of%20shadow%20models%2C%0Awith%20the%20same%20size%20and%20architecture%20of%20the%20target%20model%2C%20solely%20to%20evaluate%20the%0Aprivacy%20risk.%20While%20one%20might%20be%20able%20to%20afford%20this%20for%20small%20models%2C%20the%20cost%0Aoften%20becomes%20prohibitive%20for%20medium%20and%20large%20models.%0A%20%20We%20here%20instead%20propose%20a%20novel%20approach%20to%20identify%20the%20at-risk%20samples%0Ausing%20only%20artifacts%20available%20during%20training%2C%20with%20little%20to%20no%20additional%0Acomputational%20overhead.%20Our%20method%20analyzes%20individual%20per-sample%20loss%20traces%0Aand%20uses%20them%20to%20identify%20the%20vulnerable%20data%20samples.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20artifact-based%20approach%20through%20experiments%20on%20the%20CIFAR10%0Adataset%2C%20showing%20high%20precision%20in%20identifying%20vulnerable%20samples%20as%20determined%0Aby%20a%20SOTA%20shadow%20model-based%20MIA%20%28LiRA%29.%20Impressively%2C%20our%20method%20reaches%20the%0Asame%20precision%20as%20another%20SOTA%20MIA%20when%20measured%20against%20LiRA%2C%20despite%20it%20being%0Aorders%20of%20magnitude%20cheaper.%20We%20then%20show%20LT-IQR%20to%20outperform%20alternative%20loss%0Aaggregation%20methods%2C%20perform%20ablation%20studies%20on%20hyperparameters%2C%20and%20validate%0Athe%20robustness%20of%20our%20method%20to%20the%20target%20metric.%20Finally%2C%20we%20study%20the%0Aevolution%20of%20the%20vulnerability%20score%20distribution%20throughout%20training%20as%20a%0Ametric%20for%20model-level%20risk%20assessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFree%2520Record-Level%2520Privacy%2520Risk%2520Evaluation%2520Through%2520Artifact-Based%2520Methods%26entry.906535625%3DJoseph%2520Pollock%2520and%2520Igor%2520Shilov%2520and%2520Euodia%2520Dodd%2520and%2520Yves-Alexandre%2520de%2520Montjoye%26entry.1292438233%3D%2520%2520Membership%2520inference%2520attacks%2520%2528MIAs%2529%2520are%2520widely%2520used%2520to%2520empirically%2520assess%2520the%250Aprivacy%2520risks%2520of%2520samples%2520used%2520to%2520train%2520a%2520target%2520machine%2520learning%2520model.%250AState-of-the-art%2520methods%2520however%2520require%2520training%2520hundreds%2520of%2520shadow%2520models%252C%250Awith%2520the%2520same%2520size%2520and%2520architecture%2520of%2520the%2520target%2520model%252C%2520solely%2520to%2520evaluate%2520the%250Aprivacy%2520risk.%2520While%2520one%2520might%2520be%2520able%2520to%2520afford%2520this%2520for%2520small%2520models%252C%2520the%2520cost%250Aoften%2520becomes%2520prohibitive%2520for%2520medium%2520and%2520large%2520models.%250A%2520%2520We%2520here%2520instead%2520propose%2520a%2520novel%2520approach%2520to%2520identify%2520the%2520at-risk%2520samples%250Ausing%2520only%2520artifacts%2520available%2520during%2520training%252C%2520with%2520little%2520to%2520no%2520additional%250Acomputational%2520overhead.%2520Our%2520method%2520analyzes%2520individual%2520per-sample%2520loss%2520traces%250Aand%2520uses%2520them%2520to%2520identify%2520the%2520vulnerable%2520data%2520samples.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520artifact-based%2520approach%2520through%2520experiments%2520on%2520the%2520CIFAR10%250Adataset%252C%2520showing%2520high%2520precision%2520in%2520identifying%2520vulnerable%2520samples%2520as%2520determined%250Aby%2520a%2520SOTA%2520shadow%2520model-based%2520MIA%2520%2528LiRA%2529.%2520Impressively%252C%2520our%2520method%2520reaches%2520the%250Asame%2520precision%2520as%2520another%2520SOTA%2520MIA%2520when%2520measured%2520against%2520LiRA%252C%2520despite%2520it%2520being%250Aorders%2520of%2520magnitude%2520cheaper.%2520We%2520then%2520show%2520LT-IQR%2520to%2520outperform%2520alternative%2520loss%250Aaggregation%2520methods%252C%2520perform%2520ablation%2520studies%2520on%2520hyperparameters%252C%2520and%2520validate%250Athe%2520robustness%2520of%2520our%2520method%2520to%2520the%2520target%2520metric.%2520Finally%252C%2520we%2520study%2520the%250Aevolution%2520of%2520the%2520vulnerability%2520score%2520distribution%2520throughout%2520training%2520as%2520a%250Ametric%2520for%2520model-level%2520risk%2520assessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Free%20Record-Level%20Privacy%20Risk%20Evaluation%20Through%20Artifact-Based%20Methods&entry.906535625=Joseph%20Pollock%20and%20Igor%20Shilov%20and%20Euodia%20Dodd%20and%20Yves-Alexandre%20de%20Montjoye&entry.1292438233=%20%20Membership%20inference%20attacks%20%28MIAs%29%20are%20widely%20used%20to%20empirically%20assess%20the%0Aprivacy%20risks%20of%20samples%20used%20to%20train%20a%20target%20machine%20learning%20model.%0AState-of-the-art%20methods%20however%20require%20training%20hundreds%20of%20shadow%20models%2C%0Awith%20the%20same%20size%20and%20architecture%20of%20the%20target%20model%2C%20solely%20to%20evaluate%20the%0Aprivacy%20risk.%20While%20one%20might%20be%20able%20to%20afford%20this%20for%20small%20models%2C%20the%20cost%0Aoften%20becomes%20prohibitive%20for%20medium%20and%20large%20models.%0A%20%20We%20here%20instead%20propose%20a%20novel%20approach%20to%20identify%20the%20at-risk%20samples%0Ausing%20only%20artifacts%20available%20during%20training%2C%20with%20little%20to%20no%20additional%0Acomputational%20overhead.%20Our%20method%20analyzes%20individual%20per-sample%20loss%20traces%0Aand%20uses%20them%20to%20identify%20the%20vulnerable%20data%20samples.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20artifact-based%20approach%20through%20experiments%20on%20the%20CIFAR10%0Adataset%2C%20showing%20high%20precision%20in%20identifying%20vulnerable%20samples%20as%20determined%0Aby%20a%20SOTA%20shadow%20model-based%20MIA%20%28LiRA%29.%20Impressively%2C%20our%20method%20reaches%20the%0Asame%20precision%20as%20another%20SOTA%20MIA%20when%20measured%20against%20LiRA%2C%20despite%20it%20being%0Aorders%20of%20magnitude%20cheaper.%20We%20then%20show%20LT-IQR%20to%20outperform%20alternative%20loss%0Aaggregation%20methods%2C%20perform%20ablation%20studies%20on%20hyperparameters%2C%20and%20validate%0Athe%20robustness%20of%20our%20method%20to%20the%20target%20metric.%20Finally%2C%20we%20study%20the%0Aevolution%20of%20the%20vulnerability%20score%20distribution%20throughout%20training%20as%20a%0Ametric%20for%20model-level%20risk%20assessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05743v1&entry.124074799=Read"},
{"title": "xAI-Drop: Don't Use What You Cannot Explain", "author": "Vincenzo Marco De Luca and Antonio Longa and Andrea Passerini and Pietro Li\u00f2", "abstract": "  Graph Neural Networks (GNNs) have emerged as the predominant paradigm for\nlearning from graph-structured data, offering a wide range of applications from\nsocial network analysis to bioinformatics. Despite their versatility, GNNs face\nchallenges such as lack of generalization and poor interpretability, which\nhinder their wider adoption and reliability in critical applications. Dropping\nhas emerged as an effective paradigm for improving the generalization\ncapabilities of GNNs. However, existing approaches often rely on random or\nheuristic-based selection criteria, lacking a principled method to identify and\nexclude nodes that contribute to noise and over-complexity in the model. In\nthis work, we argue that explainability should be a key indicator of a model's\nquality throughout its training phase. To this end, we introduce xAI-Drop, a\nnovel topological-level dropping regularizer that leverages explainability to\npinpoint noisy network elements to be excluded from the GNN propagation\nmechanism. An empirical evaluation on diverse real-world datasets demonstrates\nthat our method outperforms current state-of-the-art dropping approaches in\naccuracy, and improves explanation quality.\n", "link": "http://arxiv.org/abs/2407.20067v2", "date": "2024-11-08", "relevancy": 1.8951, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4958}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4651}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20xAI-Drop%3A%20Don%27t%20Use%20What%20You%20Cannot%20Explain&body=Title%3A%20xAI-Drop%3A%20Don%27t%20Use%20What%20You%20Cannot%20Explain%0AAuthor%3A%20Vincenzo%20Marco%20De%20Luca%20and%20Antonio%20Longa%20and%20Andrea%20Passerini%20and%20Pietro%20Li%C3%B2%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20the%20predominant%20paradigm%20for%0Alearning%20from%20graph-structured%20data%2C%20offering%20a%20wide%20range%20of%20applications%20from%0Asocial%20network%20analysis%20to%20bioinformatics.%20Despite%20their%20versatility%2C%20GNNs%20face%0Achallenges%20such%20as%20lack%20of%20generalization%20and%20poor%20interpretability%2C%20which%0Ahinder%20their%20wider%20adoption%20and%20reliability%20in%20critical%20applications.%20Dropping%0Ahas%20emerged%20as%20an%20effective%20paradigm%20for%20improving%20the%20generalization%0Acapabilities%20of%20GNNs.%20However%2C%20existing%20approaches%20often%20rely%20on%20random%20or%0Aheuristic-based%20selection%20criteria%2C%20lacking%20a%20principled%20method%20to%20identify%20and%0Aexclude%20nodes%20that%20contribute%20to%20noise%20and%20over-complexity%20in%20the%20model.%20In%0Athis%20work%2C%20we%20argue%20that%20explainability%20should%20be%20a%20key%20indicator%20of%20a%20model%27s%0Aquality%20throughout%20its%20training%20phase.%20To%20this%20end%2C%20we%20introduce%20xAI-Drop%2C%20a%0Anovel%20topological-level%20dropping%20regularizer%20that%20leverages%20explainability%20to%0Apinpoint%20noisy%20network%20elements%20to%20be%20excluded%20from%20the%20GNN%20propagation%0Amechanism.%20An%20empirical%20evaluation%20on%20diverse%20real-world%20datasets%20demonstrates%0Athat%20our%20method%20outperforms%20current%20state-of-the-art%20dropping%20approaches%20in%0Aaccuracy%2C%20and%20improves%20explanation%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20067v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DxAI-Drop%253A%2520Don%2527t%2520Use%2520What%2520You%2520Cannot%2520Explain%26entry.906535625%3DVincenzo%2520Marco%2520De%2520Luca%2520and%2520Antonio%2520Longa%2520and%2520Andrea%2520Passerini%2520and%2520Pietro%2520Li%25C3%25B2%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520emerged%2520as%2520the%2520predominant%2520paradigm%2520for%250Alearning%2520from%2520graph-structured%2520data%252C%2520offering%2520a%2520wide%2520range%2520of%2520applications%2520from%250Asocial%2520network%2520analysis%2520to%2520bioinformatics.%2520Despite%2520their%2520versatility%252C%2520GNNs%2520face%250Achallenges%2520such%2520as%2520lack%2520of%2520generalization%2520and%2520poor%2520interpretability%252C%2520which%250Ahinder%2520their%2520wider%2520adoption%2520and%2520reliability%2520in%2520critical%2520applications.%2520Dropping%250Ahas%2520emerged%2520as%2520an%2520effective%2520paradigm%2520for%2520improving%2520the%2520generalization%250Acapabilities%2520of%2520GNNs.%2520However%252C%2520existing%2520approaches%2520often%2520rely%2520on%2520random%2520or%250Aheuristic-based%2520selection%2520criteria%252C%2520lacking%2520a%2520principled%2520method%2520to%2520identify%2520and%250Aexclude%2520nodes%2520that%2520contribute%2520to%2520noise%2520and%2520over-complexity%2520in%2520the%2520model.%2520In%250Athis%2520work%252C%2520we%2520argue%2520that%2520explainability%2520should%2520be%2520a%2520key%2520indicator%2520of%2520a%2520model%2527s%250Aquality%2520throughout%2520its%2520training%2520phase.%2520To%2520this%2520end%252C%2520we%2520introduce%2520xAI-Drop%252C%2520a%250Anovel%2520topological-level%2520dropping%2520regularizer%2520that%2520leverages%2520explainability%2520to%250Apinpoint%2520noisy%2520network%2520elements%2520to%2520be%2520excluded%2520from%2520the%2520GNN%2520propagation%250Amechanism.%2520An%2520empirical%2520evaluation%2520on%2520diverse%2520real-world%2520datasets%2520demonstrates%250Athat%2520our%2520method%2520outperforms%2520current%2520state-of-the-art%2520dropping%2520approaches%2520in%250Aaccuracy%252C%2520and%2520improves%2520explanation%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20067v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=xAI-Drop%3A%20Don%27t%20Use%20What%20You%20Cannot%20Explain&entry.906535625=Vincenzo%20Marco%20De%20Luca%20and%20Antonio%20Longa%20and%20Andrea%20Passerini%20and%20Pietro%20Li%C3%B2&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20the%20predominant%20paradigm%20for%0Alearning%20from%20graph-structured%20data%2C%20offering%20a%20wide%20range%20of%20applications%20from%0Asocial%20network%20analysis%20to%20bioinformatics.%20Despite%20their%20versatility%2C%20GNNs%20face%0Achallenges%20such%20as%20lack%20of%20generalization%20and%20poor%20interpretability%2C%20which%0Ahinder%20their%20wider%20adoption%20and%20reliability%20in%20critical%20applications.%20Dropping%0Ahas%20emerged%20as%20an%20effective%20paradigm%20for%20improving%20the%20generalization%0Acapabilities%20of%20GNNs.%20However%2C%20existing%20approaches%20often%20rely%20on%20random%20or%0Aheuristic-based%20selection%20criteria%2C%20lacking%20a%20principled%20method%20to%20identify%20and%0Aexclude%20nodes%20that%20contribute%20to%20noise%20and%20over-complexity%20in%20the%20model.%20In%0Athis%20work%2C%20we%20argue%20that%20explainability%20should%20be%20a%20key%20indicator%20of%20a%20model%27s%0Aquality%20throughout%20its%20training%20phase.%20To%20this%20end%2C%20we%20introduce%20xAI-Drop%2C%20a%0Anovel%20topological-level%20dropping%20regularizer%20that%20leverages%20explainability%20to%0Apinpoint%20noisy%20network%20elements%20to%20be%20excluded%20from%20the%20GNN%20propagation%0Amechanism.%20An%20empirical%20evaluation%20on%20diverse%20real-world%20datasets%20demonstrates%0Athat%20our%20method%20outperforms%20current%20state-of-the-art%20dropping%20approaches%20in%0Aaccuracy%2C%20and%20improves%20explanation%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20067v2&entry.124074799=Read"},
{"title": "Quantitative Assessment of Intersectional Empathetic Bias and\n  Understanding", "author": "Vojtech Formanek and Ondrej Sotolar", "abstract": "  A growing amount of literature critiques the current operationalizations of\nempathy based on loose definitions of the construct. Such definitions\nnegatively affect dataset quality, model robustness, and evaluation\nreliability. We propose an empathy evaluation framework that operationalizes\nempathy close to its psychological origins. The framework measures the variance\nin responses of LLMs to prompts using existing metrics for empathy and\nemotional valence. The variance is introduced through the controlled generation\nof the prompts by varying social biases affecting context understanding, thus\nimpacting empathetic understanding. The control over generation ensures high\ntheoretical validity of the constructs in the prompt dataset. Also, it makes\nhigh-quality translation, especially into languages that currently have\nlittle-to-no way of evaluating empathy or bias, such as the Slavonic family,\nmore manageable. Using chosen LLMs and various prompt types, we demonstrate the\nempathy evaluation with the framework, including multiple-choice answers and\nfree generation. The variance in our initial evaluation sample is small and we\nwere unable to measure convincing differences between the empathetic\nunderstanding in contexts given by different social groups. However, the\nresults are promising because the models showed significant alterations their\nreasoning chains needed to capture the relatively subtle changes in the\nprompts. This provides the basis for future research into the construction of\nthe evaluation sample and statistical methods for measuring the results.\n", "link": "http://arxiv.org/abs/2411.05777v1", "date": "2024-11-08", "relevancy": 1.877, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantitative%20Assessment%20of%20Intersectional%20Empathetic%20Bias%20and%0A%20%20Understanding&body=Title%3A%20Quantitative%20Assessment%20of%20Intersectional%20Empathetic%20Bias%20and%0A%20%20Understanding%0AAuthor%3A%20Vojtech%20Formanek%20and%20Ondrej%20Sotolar%0AAbstract%3A%20%20%20A%20growing%20amount%20of%20literature%20critiques%20the%20current%20operationalizations%20of%0Aempathy%20based%20on%20loose%20definitions%20of%20the%20construct.%20Such%20definitions%0Anegatively%20affect%20dataset%20quality%2C%20model%20robustness%2C%20and%20evaluation%0Areliability.%20We%20propose%20an%20empathy%20evaluation%20framework%20that%20operationalizes%0Aempathy%20close%20to%20its%20psychological%20origins.%20The%20framework%20measures%20the%20variance%0Ain%20responses%20of%20LLMs%20to%20prompts%20using%20existing%20metrics%20for%20empathy%20and%0Aemotional%20valence.%20The%20variance%20is%20introduced%20through%20the%20controlled%20generation%0Aof%20the%20prompts%20by%20varying%20social%20biases%20affecting%20context%20understanding%2C%20thus%0Aimpacting%20empathetic%20understanding.%20The%20control%20over%20generation%20ensures%20high%0Atheoretical%20validity%20of%20the%20constructs%20in%20the%20prompt%20dataset.%20Also%2C%20it%20makes%0Ahigh-quality%20translation%2C%20especially%20into%20languages%20that%20currently%20have%0Alittle-to-no%20way%20of%20evaluating%20empathy%20or%20bias%2C%20such%20as%20the%20Slavonic%20family%2C%0Amore%20manageable.%20Using%20chosen%20LLMs%20and%20various%20prompt%20types%2C%20we%20demonstrate%20the%0Aempathy%20evaluation%20with%20the%20framework%2C%20including%20multiple-choice%20answers%20and%0Afree%20generation.%20The%20variance%20in%20our%20initial%20evaluation%20sample%20is%20small%20and%20we%0Awere%20unable%20to%20measure%20convincing%20differences%20between%20the%20empathetic%0Aunderstanding%20in%20contexts%20given%20by%20different%20social%20groups.%20However%2C%20the%0Aresults%20are%20promising%20because%20the%20models%20showed%20significant%20alterations%20their%0Areasoning%20chains%20needed%20to%20capture%20the%20relatively%20subtle%20changes%20in%20the%0Aprompts.%20This%20provides%20the%20basis%20for%20future%20research%20into%20the%20construction%20of%0Athe%20evaluation%20sample%20and%20statistical%20methods%20for%20measuring%20the%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantitative%2520Assessment%2520of%2520Intersectional%2520Empathetic%2520Bias%2520and%250A%2520%2520Understanding%26entry.906535625%3DVojtech%2520Formanek%2520and%2520Ondrej%2520Sotolar%26entry.1292438233%3D%2520%2520A%2520growing%2520amount%2520of%2520literature%2520critiques%2520the%2520current%2520operationalizations%2520of%250Aempathy%2520based%2520on%2520loose%2520definitions%2520of%2520the%2520construct.%2520Such%2520definitions%250Anegatively%2520affect%2520dataset%2520quality%252C%2520model%2520robustness%252C%2520and%2520evaluation%250Areliability.%2520We%2520propose%2520an%2520empathy%2520evaluation%2520framework%2520that%2520operationalizes%250Aempathy%2520close%2520to%2520its%2520psychological%2520origins.%2520The%2520framework%2520measures%2520the%2520variance%250Ain%2520responses%2520of%2520LLMs%2520to%2520prompts%2520using%2520existing%2520metrics%2520for%2520empathy%2520and%250Aemotional%2520valence.%2520The%2520variance%2520is%2520introduced%2520through%2520the%2520controlled%2520generation%250Aof%2520the%2520prompts%2520by%2520varying%2520social%2520biases%2520affecting%2520context%2520understanding%252C%2520thus%250Aimpacting%2520empathetic%2520understanding.%2520The%2520control%2520over%2520generation%2520ensures%2520high%250Atheoretical%2520validity%2520of%2520the%2520constructs%2520in%2520the%2520prompt%2520dataset.%2520Also%252C%2520it%2520makes%250Ahigh-quality%2520translation%252C%2520especially%2520into%2520languages%2520that%2520currently%2520have%250Alittle-to-no%2520way%2520of%2520evaluating%2520empathy%2520or%2520bias%252C%2520such%2520as%2520the%2520Slavonic%2520family%252C%250Amore%2520manageable.%2520Using%2520chosen%2520LLMs%2520and%2520various%2520prompt%2520types%252C%2520we%2520demonstrate%2520the%250Aempathy%2520evaluation%2520with%2520the%2520framework%252C%2520including%2520multiple-choice%2520answers%2520and%250Afree%2520generation.%2520The%2520variance%2520in%2520our%2520initial%2520evaluation%2520sample%2520is%2520small%2520and%2520we%250Awere%2520unable%2520to%2520measure%2520convincing%2520differences%2520between%2520the%2520empathetic%250Aunderstanding%2520in%2520contexts%2520given%2520by%2520different%2520social%2520groups.%2520However%252C%2520the%250Aresults%2520are%2520promising%2520because%2520the%2520models%2520showed%2520significant%2520alterations%2520their%250Areasoning%2520chains%2520needed%2520to%2520capture%2520the%2520relatively%2520subtle%2520changes%2520in%2520the%250Aprompts.%2520This%2520provides%2520the%2520basis%2520for%2520future%2520research%2520into%2520the%2520construction%2520of%250Athe%2520evaluation%2520sample%2520and%2520statistical%2520methods%2520for%2520measuring%2520the%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantitative%20Assessment%20of%20Intersectional%20Empathetic%20Bias%20and%0A%20%20Understanding&entry.906535625=Vojtech%20Formanek%20and%20Ondrej%20Sotolar&entry.1292438233=%20%20A%20growing%20amount%20of%20literature%20critiques%20the%20current%20operationalizations%20of%0Aempathy%20based%20on%20loose%20definitions%20of%20the%20construct.%20Such%20definitions%0Anegatively%20affect%20dataset%20quality%2C%20model%20robustness%2C%20and%20evaluation%0Areliability.%20We%20propose%20an%20empathy%20evaluation%20framework%20that%20operationalizes%0Aempathy%20close%20to%20its%20psychological%20origins.%20The%20framework%20measures%20the%20variance%0Ain%20responses%20of%20LLMs%20to%20prompts%20using%20existing%20metrics%20for%20empathy%20and%0Aemotional%20valence.%20The%20variance%20is%20introduced%20through%20the%20controlled%20generation%0Aof%20the%20prompts%20by%20varying%20social%20biases%20affecting%20context%20understanding%2C%20thus%0Aimpacting%20empathetic%20understanding.%20The%20control%20over%20generation%20ensures%20high%0Atheoretical%20validity%20of%20the%20constructs%20in%20the%20prompt%20dataset.%20Also%2C%20it%20makes%0Ahigh-quality%20translation%2C%20especially%20into%20languages%20that%20currently%20have%0Alittle-to-no%20way%20of%20evaluating%20empathy%20or%20bias%2C%20such%20as%20the%20Slavonic%20family%2C%0Amore%20manageable.%20Using%20chosen%20LLMs%20and%20various%20prompt%20types%2C%20we%20demonstrate%20the%0Aempathy%20evaluation%20with%20the%20framework%2C%20including%20multiple-choice%20answers%20and%0Afree%20generation.%20The%20variance%20in%20our%20initial%20evaluation%20sample%20is%20small%20and%20we%0Awere%20unable%20to%20measure%20convincing%20differences%20between%20the%20empathetic%0Aunderstanding%20in%20contexts%20given%20by%20different%20social%20groups.%20However%2C%20the%0Aresults%20are%20promising%20because%20the%20models%20showed%20significant%20alterations%20their%0Areasoning%20chains%20needed%20to%20capture%20the%20relatively%20subtle%20changes%20in%20the%0Aprompts.%20This%20provides%20the%20basis%20for%20future%20research%20into%20the%20construction%20of%0Athe%20evaluation%20sample%20and%20statistical%20methods%20for%20measuring%20the%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05777v1&entry.124074799=Read"},
{"title": "Soft Gripping System for Space Exploration Legged Robots", "author": "Arthur Candalot and Malik-Manel Hashim and Brigid Hickey and Mickael Laine and Mitch Hunter-Scullion and Kazuya Yoshida", "abstract": "  Although wheeled robots have been predominant for planetary exploration,\ntheir geometry limits their capabilities when traveling over steep slopes,\nthrough rocky terrains, and in microgravity. Legged robots equipped with\ngrippers are a viable alternative to overcome these obstacles. This paper\nproposes a gripping system that can provide legged space-explorer robots a\nreliable anchor on uneven rocky terrain. This gripper provides the benefits of\nsoft gripping technology by using segmented tendon-driven fingers to adapt to\nthe target shape, and creates a strong adhesion to rocky surfaces with the help\nof microspines. The gripping performances are showcased, and multiple\nexperiments demonstrate the impact of the pulling angle, target shape, spine\nconfiguration, and actuation power on the performances. The results show that\nthe proposed gripper can be a suitable solution for advanced space exploration,\nincluding climbing, lunar caves, or exploration of the surface of asteroids.\n", "link": "http://arxiv.org/abs/2411.05482v1", "date": "2024-11-08", "relevancy": 1.5678, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5515}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5162}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Soft%20Gripping%20System%20for%20Space%20Exploration%20Legged%20Robots&body=Title%3A%20Soft%20Gripping%20System%20for%20Space%20Exploration%20Legged%20Robots%0AAuthor%3A%20Arthur%20Candalot%20and%20Malik-Manel%20Hashim%20and%20Brigid%20Hickey%20and%20Mickael%20Laine%20and%20Mitch%20Hunter-Scullion%20and%20Kazuya%20Yoshida%0AAbstract%3A%20%20%20Although%20wheeled%20robots%20have%20been%20predominant%20for%20planetary%20exploration%2C%0Atheir%20geometry%20limits%20their%20capabilities%20when%20traveling%20over%20steep%20slopes%2C%0Athrough%20rocky%20terrains%2C%20and%20in%20microgravity.%20Legged%20robots%20equipped%20with%0Agrippers%20are%20a%20viable%20alternative%20to%20overcome%20these%20obstacles.%20This%20paper%0Aproposes%20a%20gripping%20system%20that%20can%20provide%20legged%20space-explorer%20robots%20a%0Areliable%20anchor%20on%20uneven%20rocky%20terrain.%20This%20gripper%20provides%20the%20benefits%20of%0Asoft%20gripping%20technology%20by%20using%20segmented%20tendon-driven%20fingers%20to%20adapt%20to%0Athe%20target%20shape%2C%20and%20creates%20a%20strong%20adhesion%20to%20rocky%20surfaces%20with%20the%20help%0Aof%20microspines.%20The%20gripping%20performances%20are%20showcased%2C%20and%20multiple%0Aexperiments%20demonstrate%20the%20impact%20of%20the%20pulling%20angle%2C%20target%20shape%2C%20spine%0Aconfiguration%2C%20and%20actuation%20power%20on%20the%20performances.%20The%20results%20show%20that%0Athe%20proposed%20gripper%20can%20be%20a%20suitable%20solution%20for%20advanced%20space%20exploration%2C%0Aincluding%20climbing%2C%20lunar%20caves%2C%20or%20exploration%20of%20the%20surface%20of%20asteroids.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoft%2520Gripping%2520System%2520for%2520Space%2520Exploration%2520Legged%2520Robots%26entry.906535625%3DArthur%2520Candalot%2520and%2520Malik-Manel%2520Hashim%2520and%2520Brigid%2520Hickey%2520and%2520Mickael%2520Laine%2520and%2520Mitch%2520Hunter-Scullion%2520and%2520Kazuya%2520Yoshida%26entry.1292438233%3D%2520%2520Although%2520wheeled%2520robots%2520have%2520been%2520predominant%2520for%2520planetary%2520exploration%252C%250Atheir%2520geometry%2520limits%2520their%2520capabilities%2520when%2520traveling%2520over%2520steep%2520slopes%252C%250Athrough%2520rocky%2520terrains%252C%2520and%2520in%2520microgravity.%2520Legged%2520robots%2520equipped%2520with%250Agrippers%2520are%2520a%2520viable%2520alternative%2520to%2520overcome%2520these%2520obstacles.%2520This%2520paper%250Aproposes%2520a%2520gripping%2520system%2520that%2520can%2520provide%2520legged%2520space-explorer%2520robots%2520a%250Areliable%2520anchor%2520on%2520uneven%2520rocky%2520terrain.%2520This%2520gripper%2520provides%2520the%2520benefits%2520of%250Asoft%2520gripping%2520technology%2520by%2520using%2520segmented%2520tendon-driven%2520fingers%2520to%2520adapt%2520to%250Athe%2520target%2520shape%252C%2520and%2520creates%2520a%2520strong%2520adhesion%2520to%2520rocky%2520surfaces%2520with%2520the%2520help%250Aof%2520microspines.%2520The%2520gripping%2520performances%2520are%2520showcased%252C%2520and%2520multiple%250Aexperiments%2520demonstrate%2520the%2520impact%2520of%2520the%2520pulling%2520angle%252C%2520target%2520shape%252C%2520spine%250Aconfiguration%252C%2520and%2520actuation%2520power%2520on%2520the%2520performances.%2520The%2520results%2520show%2520that%250Athe%2520proposed%2520gripper%2520can%2520be%2520a%2520suitable%2520solution%2520for%2520advanced%2520space%2520exploration%252C%250Aincluding%2520climbing%252C%2520lunar%2520caves%252C%2520or%2520exploration%2520of%2520the%2520surface%2520of%2520asteroids.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Soft%20Gripping%20System%20for%20Space%20Exploration%20Legged%20Robots&entry.906535625=Arthur%20Candalot%20and%20Malik-Manel%20Hashim%20and%20Brigid%20Hickey%20and%20Mickael%20Laine%20and%20Mitch%20Hunter-Scullion%20and%20Kazuya%20Yoshida&entry.1292438233=%20%20Although%20wheeled%20robots%20have%20been%20predominant%20for%20planetary%20exploration%2C%0Atheir%20geometry%20limits%20their%20capabilities%20when%20traveling%20over%20steep%20slopes%2C%0Athrough%20rocky%20terrains%2C%20and%20in%20microgravity.%20Legged%20robots%20equipped%20with%0Agrippers%20are%20a%20viable%20alternative%20to%20overcome%20these%20obstacles.%20This%20paper%0Aproposes%20a%20gripping%20system%20that%20can%20provide%20legged%20space-explorer%20robots%20a%0Areliable%20anchor%20on%20uneven%20rocky%20terrain.%20This%20gripper%20provides%20the%20benefits%20of%0Asoft%20gripping%20technology%20by%20using%20segmented%20tendon-driven%20fingers%20to%20adapt%20to%0Athe%20target%20shape%2C%20and%20creates%20a%20strong%20adhesion%20to%20rocky%20surfaces%20with%20the%20help%0Aof%20microspines.%20The%20gripping%20performances%20are%20showcased%2C%20and%20multiple%0Aexperiments%20demonstrate%20the%20impact%20of%20the%20pulling%20angle%2C%20target%20shape%2C%20spine%0Aconfiguration%2C%20and%20actuation%20power%20on%20the%20performances.%20The%20results%20show%20that%0Athe%20proposed%20gripper%20can%20be%20a%20suitable%20solution%20for%20advanced%20space%20exploration%2C%0Aincluding%20climbing%2C%20lunar%20caves%2C%20or%20exploration%20of%20the%20surface%20of%20asteroids.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05482v1&entry.124074799=Read"},
{"title": "The influence of persona and conversational task on social interactions\n  with a LLM-controlled embodied conversational agent", "author": "Leon O. H. Kroczek and Alexander May and Selina Hettenkofer and Andreas Ruider and Bernd Ludwig and Andreas M\u00fchlberger", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable capabilities in\nconversational tasks. Embodying an LLM as a virtual human allows users to\nengage in face-to-face social interactions in Virtual Reality. However, the\ninfluence of person- and task-related factors in social interactions with\nLLM-controlled agents remains unclear. In this study, forty-six participants\ninteracted with a virtual agent whose persona was manipulated as extravert or\nintrovert in three different conversational tasks (small talk, knowledge test,\nconvincing). Social-evaluation, emotional experience, and realism were assessed\nusing ratings. Interactive engagement was measured by quantifying participants'\nwords and conversational turns. Finally, we measured participants' willingness\nto ask the agent for help during the knowledge test. Our findings show that the\nextraverted agent was more positively evaluated, elicited a more pleasant\nexperience and greater engagement, and was assessed as more realistic compared\nto the introverted agent. Whereas persona did not affect the tendency to ask\nfor help, participants were generally more confident in the answer when they\nhad help of the LLM. Variation of personality traits of LLM-controlled embodied\nvirtual agents, therefore, affects social-emotional processing and behavior in\nvirtual interactions. Embodied virtual agents allow the presentation of\nnaturalistic social encounters in a virtual environment.\n", "link": "http://arxiv.org/abs/2411.05653v1", "date": "2024-11-08", "relevancy": 1.4297, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4815}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4757}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20influence%20of%20persona%20and%20conversational%20task%20on%20social%20interactions%0A%20%20with%20a%20LLM-controlled%20embodied%20conversational%20agent&body=Title%3A%20The%20influence%20of%20persona%20and%20conversational%20task%20on%20social%20interactions%0A%20%20with%20a%20LLM-controlled%20embodied%20conversational%20agent%0AAuthor%3A%20Leon%20O.%20H.%20Kroczek%20and%20Alexander%20May%20and%20Selina%20Hettenkofer%20and%20Andreas%20Ruider%20and%20Bernd%20Ludwig%20and%20Andreas%20M%C3%BChlberger%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%0Aconversational%20tasks.%20Embodying%20an%20LLM%20as%20a%20virtual%20human%20allows%20users%20to%0Aengage%20in%20face-to-face%20social%20interactions%20in%20Virtual%20Reality.%20However%2C%20the%0Ainfluence%20of%20person-%20and%20task-related%20factors%20in%20social%20interactions%20with%0ALLM-controlled%20agents%20remains%20unclear.%20In%20this%20study%2C%20forty-six%20participants%0Ainteracted%20with%20a%20virtual%20agent%20whose%20persona%20was%20manipulated%20as%20extravert%20or%0Aintrovert%20in%20three%20different%20conversational%20tasks%20%28small%20talk%2C%20knowledge%20test%2C%0Aconvincing%29.%20Social-evaluation%2C%20emotional%20experience%2C%20and%20realism%20were%20assessed%0Ausing%20ratings.%20Interactive%20engagement%20was%20measured%20by%20quantifying%20participants%27%0Awords%20and%20conversational%20turns.%20Finally%2C%20we%20measured%20participants%27%20willingness%0Ato%20ask%20the%20agent%20for%20help%20during%20the%20knowledge%20test.%20Our%20findings%20show%20that%20the%0Aextraverted%20agent%20was%20more%20positively%20evaluated%2C%20elicited%20a%20more%20pleasant%0Aexperience%20and%20greater%20engagement%2C%20and%20was%20assessed%20as%20more%20realistic%20compared%0Ato%20the%20introverted%20agent.%20Whereas%20persona%20did%20not%20affect%20the%20tendency%20to%20ask%0Afor%20help%2C%20participants%20were%20generally%20more%20confident%20in%20the%20answer%20when%20they%0Ahad%20help%20of%20the%20LLM.%20Variation%20of%20personality%20traits%20of%20LLM-controlled%20embodied%0Avirtual%20agents%2C%20therefore%2C%20affects%20social-emotional%20processing%20and%20behavior%20in%0Avirtual%20interactions.%20Embodied%20virtual%20agents%20allow%20the%20presentation%20of%0Anaturalistic%20social%20encounters%20in%20a%20virtual%20environment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520influence%2520of%2520persona%2520and%2520conversational%2520task%2520on%2520social%2520interactions%250A%2520%2520with%2520a%2520LLM-controlled%2520embodied%2520conversational%2520agent%26entry.906535625%3DLeon%2520O.%2520H.%2520Kroczek%2520and%2520Alexander%2520May%2520and%2520Selina%2520Hettenkofer%2520and%2520Andreas%2520Ruider%2520and%2520Bernd%2520Ludwig%2520and%2520Andreas%2520M%25C3%25BChlberger%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%250Aconversational%2520tasks.%2520Embodying%2520an%2520LLM%2520as%2520a%2520virtual%2520human%2520allows%2520users%2520to%250Aengage%2520in%2520face-to-face%2520social%2520interactions%2520in%2520Virtual%2520Reality.%2520However%252C%2520the%250Ainfluence%2520of%2520person-%2520and%2520task-related%2520factors%2520in%2520social%2520interactions%2520with%250ALLM-controlled%2520agents%2520remains%2520unclear.%2520In%2520this%2520study%252C%2520forty-six%2520participants%250Ainteracted%2520with%2520a%2520virtual%2520agent%2520whose%2520persona%2520was%2520manipulated%2520as%2520extravert%2520or%250Aintrovert%2520in%2520three%2520different%2520conversational%2520tasks%2520%2528small%2520talk%252C%2520knowledge%2520test%252C%250Aconvincing%2529.%2520Social-evaluation%252C%2520emotional%2520experience%252C%2520and%2520realism%2520were%2520assessed%250Ausing%2520ratings.%2520Interactive%2520engagement%2520was%2520measured%2520by%2520quantifying%2520participants%2527%250Awords%2520and%2520conversational%2520turns.%2520Finally%252C%2520we%2520measured%2520participants%2527%2520willingness%250Ato%2520ask%2520the%2520agent%2520for%2520help%2520during%2520the%2520knowledge%2520test.%2520Our%2520findings%2520show%2520that%2520the%250Aextraverted%2520agent%2520was%2520more%2520positively%2520evaluated%252C%2520elicited%2520a%2520more%2520pleasant%250Aexperience%2520and%2520greater%2520engagement%252C%2520and%2520was%2520assessed%2520as%2520more%2520realistic%2520compared%250Ato%2520the%2520introverted%2520agent.%2520Whereas%2520persona%2520did%2520not%2520affect%2520the%2520tendency%2520to%2520ask%250Afor%2520help%252C%2520participants%2520were%2520generally%2520more%2520confident%2520in%2520the%2520answer%2520when%2520they%250Ahad%2520help%2520of%2520the%2520LLM.%2520Variation%2520of%2520personality%2520traits%2520of%2520LLM-controlled%2520embodied%250Avirtual%2520agents%252C%2520therefore%252C%2520affects%2520social-emotional%2520processing%2520and%2520behavior%2520in%250Avirtual%2520interactions.%2520Embodied%2520virtual%2520agents%2520allow%2520the%2520presentation%2520of%250Anaturalistic%2520social%2520encounters%2520in%2520a%2520virtual%2520environment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20influence%20of%20persona%20and%20conversational%20task%20on%20social%20interactions%0A%20%20with%20a%20LLM-controlled%20embodied%20conversational%20agent&entry.906535625=Leon%20O.%20H.%20Kroczek%20and%20Alexander%20May%20and%20Selina%20Hettenkofer%20and%20Andreas%20Ruider%20and%20Bernd%20Ludwig%20and%20Andreas%20M%C3%BChlberger&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%0Aconversational%20tasks.%20Embodying%20an%20LLM%20as%20a%20virtual%20human%20allows%20users%20to%0Aengage%20in%20face-to-face%20social%20interactions%20in%20Virtual%20Reality.%20However%2C%20the%0Ainfluence%20of%20person-%20and%20task-related%20factors%20in%20social%20interactions%20with%0ALLM-controlled%20agents%20remains%20unclear.%20In%20this%20study%2C%20forty-six%20participants%0Ainteracted%20with%20a%20virtual%20agent%20whose%20persona%20was%20manipulated%20as%20extravert%20or%0Aintrovert%20in%20three%20different%20conversational%20tasks%20%28small%20talk%2C%20knowledge%20test%2C%0Aconvincing%29.%20Social-evaluation%2C%20emotional%20experience%2C%20and%20realism%20were%20assessed%0Ausing%20ratings.%20Interactive%20engagement%20was%20measured%20by%20quantifying%20participants%27%0Awords%20and%20conversational%20turns.%20Finally%2C%20we%20measured%20participants%27%20willingness%0Ato%20ask%20the%20agent%20for%20help%20during%20the%20knowledge%20test.%20Our%20findings%20show%20that%20the%0Aextraverted%20agent%20was%20more%20positively%20evaluated%2C%20elicited%20a%20more%20pleasant%0Aexperience%20and%20greater%20engagement%2C%20and%20was%20assessed%20as%20more%20realistic%20compared%0Ato%20the%20introverted%20agent.%20Whereas%20persona%20did%20not%20affect%20the%20tendency%20to%20ask%0Afor%20help%2C%20participants%20were%20generally%20more%20confident%20in%20the%20answer%20when%20they%0Ahad%20help%20of%20the%20LLM.%20Variation%20of%20personality%20traits%20of%20LLM-controlled%20embodied%0Avirtual%20agents%2C%20therefore%2C%20affects%20social-emotional%20processing%20and%20behavior%20in%0Avirtual%20interactions.%20Embodied%20virtual%20agents%20allow%20the%20presentation%20of%0Anaturalistic%20social%20encounters%20in%20a%20virtual%20environment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05653v1&entry.124074799=Read"},
{"title": "FisherMask: Enhancing Neural Network Labeling Efficiency in Image\n  Classification Using Fisher Information", "author": "Shreen Gul and Mohamed Elmahallawy and Sanjay Madria and Ardhendu Tripathy", "abstract": "  Deep learning (DL) models are popular across various domains due to their\nremarkable performance and efficiency. However, their effectiveness relies\nheavily on large amounts of labeled data, which are often time-consuming and\nlabor-intensive to generate manually. To overcome this challenge, it is\nessential to develop strategies that reduce reliance on extensive labeled data\nwhile preserving model performance. In this paper, we propose FisherMask, a\nFisher information-based active learning (AL) approach that identifies key\nnetwork parameters by masking them based on their Fisher information values.\nFisherMask enhances batch AL by using Fisher information to select the most\ncritical parameters, allowing the identification of the most impactful samples\nduring AL training. Moreover, Fisher information possesses favorable\nstatistical properties, offering valuable insights into model behavior and\nproviding a better understanding of the performance characteristics within the\nAL pipeline. Our extensive experiments demonstrate that FisherMask\nsignificantly outperforms state-of-the-art methods on diverse datasets,\nincluding CIFAR-10 and FashionMNIST, especially under imbalanced settings.\nThese improvements lead to substantial gains in labeling efficiency. Hence\nserving as an effective tool to measure the sensitivity of model parameters to\ndata samples. Our code is available on\n\\url{https://github.com/sgchr273/FisherMask}.\n", "link": "http://arxiv.org/abs/2411.05752v1", "date": "2024-11-08", "relevancy": 1.5139, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5269}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5184}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4902}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FisherMask%3A%20Enhancing%20Neural%20Network%20Labeling%20Efficiency%20in%20Image%0A%20%20Classification%20Using%20Fisher%20Information&body=Title%3A%20FisherMask%3A%20Enhancing%20Neural%20Network%20Labeling%20Efficiency%20in%20Image%0A%20%20Classification%20Using%20Fisher%20Information%0AAuthor%3A%20Shreen%20Gul%20and%20Mohamed%20Elmahallawy%20and%20Sanjay%20Madria%20and%20Ardhendu%20Tripathy%0AAbstract%3A%20%20%20Deep%20learning%20%28DL%29%20models%20are%20popular%20across%20various%20domains%20due%20to%20their%0Aremarkable%20performance%20and%20efficiency.%20However%2C%20their%20effectiveness%20relies%0Aheavily%20on%20large%20amounts%20of%20labeled%20data%2C%20which%20are%20often%20time-consuming%20and%0Alabor-intensive%20to%20generate%20manually.%20To%20overcome%20this%20challenge%2C%20it%20is%0Aessential%20to%20develop%20strategies%20that%20reduce%20reliance%20on%20extensive%20labeled%20data%0Awhile%20preserving%20model%20performance.%20In%20this%20paper%2C%20we%20propose%20FisherMask%2C%20a%0AFisher%20information-based%20active%20learning%20%28AL%29%20approach%20that%20identifies%20key%0Anetwork%20parameters%20by%20masking%20them%20based%20on%20their%20Fisher%20information%20values.%0AFisherMask%20enhances%20batch%20AL%20by%20using%20Fisher%20information%20to%20select%20the%20most%0Acritical%20parameters%2C%20allowing%20the%20identification%20of%20the%20most%20impactful%20samples%0Aduring%20AL%20training.%20Moreover%2C%20Fisher%20information%20possesses%20favorable%0Astatistical%20properties%2C%20offering%20valuable%20insights%20into%20model%20behavior%20and%0Aproviding%20a%20better%20understanding%20of%20the%20performance%20characteristics%20within%20the%0AAL%20pipeline.%20Our%20extensive%20experiments%20demonstrate%20that%20FisherMask%0Asignificantly%20outperforms%20state-of-the-art%20methods%20on%20diverse%20datasets%2C%0Aincluding%20CIFAR-10%20and%20FashionMNIST%2C%20especially%20under%20imbalanced%20settings.%0AThese%20improvements%20lead%20to%20substantial%20gains%20in%20labeling%20efficiency.%20Hence%0Aserving%20as%20an%20effective%20tool%20to%20measure%20the%20sensitivity%20of%20model%20parameters%20to%0Adata%20samples.%20Our%20code%20is%20available%20on%0A%5Curl%7Bhttps%3A//github.com/sgchr273/FisherMask%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05752v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFisherMask%253A%2520Enhancing%2520Neural%2520Network%2520Labeling%2520Efficiency%2520in%2520Image%250A%2520%2520Classification%2520Using%2520Fisher%2520Information%26entry.906535625%3DShreen%2520Gul%2520and%2520Mohamed%2520Elmahallawy%2520and%2520Sanjay%2520Madria%2520and%2520Ardhendu%2520Tripathy%26entry.1292438233%3D%2520%2520Deep%2520learning%2520%2528DL%2529%2520models%2520are%2520popular%2520across%2520various%2520domains%2520due%2520to%2520their%250Aremarkable%2520performance%2520and%2520efficiency.%2520However%252C%2520their%2520effectiveness%2520relies%250Aheavily%2520on%2520large%2520amounts%2520of%2520labeled%2520data%252C%2520which%2520are%2520often%2520time-consuming%2520and%250Alabor-intensive%2520to%2520generate%2520manually.%2520To%2520overcome%2520this%2520challenge%252C%2520it%2520is%250Aessential%2520to%2520develop%2520strategies%2520that%2520reduce%2520reliance%2520on%2520extensive%2520labeled%2520data%250Awhile%2520preserving%2520model%2520performance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520FisherMask%252C%2520a%250AFisher%2520information-based%2520active%2520learning%2520%2528AL%2529%2520approach%2520that%2520identifies%2520key%250Anetwork%2520parameters%2520by%2520masking%2520them%2520based%2520on%2520their%2520Fisher%2520information%2520values.%250AFisherMask%2520enhances%2520batch%2520AL%2520by%2520using%2520Fisher%2520information%2520to%2520select%2520the%2520most%250Acritical%2520parameters%252C%2520allowing%2520the%2520identification%2520of%2520the%2520most%2520impactful%2520samples%250Aduring%2520AL%2520training.%2520Moreover%252C%2520Fisher%2520information%2520possesses%2520favorable%250Astatistical%2520properties%252C%2520offering%2520valuable%2520insights%2520into%2520model%2520behavior%2520and%250Aproviding%2520a%2520better%2520understanding%2520of%2520the%2520performance%2520characteristics%2520within%2520the%250AAL%2520pipeline.%2520Our%2520extensive%2520experiments%2520demonstrate%2520that%2520FisherMask%250Asignificantly%2520outperforms%2520state-of-the-art%2520methods%2520on%2520diverse%2520datasets%252C%250Aincluding%2520CIFAR-10%2520and%2520FashionMNIST%252C%2520especially%2520under%2520imbalanced%2520settings.%250AThese%2520improvements%2520lead%2520to%2520substantial%2520gains%2520in%2520labeling%2520efficiency.%2520Hence%250Aserving%2520as%2520an%2520effective%2520tool%2520to%2520measure%2520the%2520sensitivity%2520of%2520model%2520parameters%2520to%250Adata%2520samples.%2520Our%2520code%2520is%2520available%2520on%250A%255Curl%257Bhttps%253A//github.com/sgchr273/FisherMask%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05752v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FisherMask%3A%20Enhancing%20Neural%20Network%20Labeling%20Efficiency%20in%20Image%0A%20%20Classification%20Using%20Fisher%20Information&entry.906535625=Shreen%20Gul%20and%20Mohamed%20Elmahallawy%20and%20Sanjay%20Madria%20and%20Ardhendu%20Tripathy&entry.1292438233=%20%20Deep%20learning%20%28DL%29%20models%20are%20popular%20across%20various%20domains%20due%20to%20their%0Aremarkable%20performance%20and%20efficiency.%20However%2C%20their%20effectiveness%20relies%0Aheavily%20on%20large%20amounts%20of%20labeled%20data%2C%20which%20are%20often%20time-consuming%20and%0Alabor-intensive%20to%20generate%20manually.%20To%20overcome%20this%20challenge%2C%20it%20is%0Aessential%20to%20develop%20strategies%20that%20reduce%20reliance%20on%20extensive%20labeled%20data%0Awhile%20preserving%20model%20performance.%20In%20this%20paper%2C%20we%20propose%20FisherMask%2C%20a%0AFisher%20information-based%20active%20learning%20%28AL%29%20approach%20that%20identifies%20key%0Anetwork%20parameters%20by%20masking%20them%20based%20on%20their%20Fisher%20information%20values.%0AFisherMask%20enhances%20batch%20AL%20by%20using%20Fisher%20information%20to%20select%20the%20most%0Acritical%20parameters%2C%20allowing%20the%20identification%20of%20the%20most%20impactful%20samples%0Aduring%20AL%20training.%20Moreover%2C%20Fisher%20information%20possesses%20favorable%0Astatistical%20properties%2C%20offering%20valuable%20insights%20into%20model%20behavior%20and%0Aproviding%20a%20better%20understanding%20of%20the%20performance%20characteristics%20within%20the%0AAL%20pipeline.%20Our%20extensive%20experiments%20demonstrate%20that%20FisherMask%0Asignificantly%20outperforms%20state-of-the-art%20methods%20on%20diverse%20datasets%2C%0Aincluding%20CIFAR-10%20and%20FashionMNIST%2C%20especially%20under%20imbalanced%20settings.%0AThese%20improvements%20lead%20to%20substantial%20gains%20in%20labeling%20efficiency.%20Hence%0Aserving%20as%20an%20effective%20tool%20to%20measure%20the%20sensitivity%20of%20model%20parameters%20to%0Adata%20samples.%20Our%20code%20is%20available%20on%0A%5Curl%7Bhttps%3A//github.com/sgchr273/FisherMask%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05752v1&entry.124074799=Read"},
{"title": "The Limits of Differential Privacy in Online Learning", "author": "Bo Li and Wei Wang and Peng Ye", "abstract": "  Differential privacy (DP) is a formal notion that restricts the privacy\nleakage of an algorithm when running on sensitive data, in which\nprivacy-utility trade-off is one of the central problems in private data\nanalysis. In this work, we investigate the fundamental limits of differential\nprivacy in online learning algorithms and present evidence that separates three\ntypes of constraints: no DP, pure DP, and approximate DP. We first describe a\nhypothesis class that is online learnable under approximate DP but not online\nlearnable under pure DP under the adaptive adversarial setting. This indicates\nthat approximate DP must be adopted when dealing with adaptive adversaries. We\nthen prove that any private online learner must make an infinite number of\nmistakes for almost all hypothesis classes. This essentially generalizes\nprevious results and shows a strong separation between private and non-private\nsettings since a finite mistake bound is always attainable (as long as the\nclass is online learnable) when there is no privacy requirement.\n", "link": "http://arxiv.org/abs/2411.05483v1", "date": "2024-11-08", "relevancy": 1.2395, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.417}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4146}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4022}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Limits%20of%20Differential%20Privacy%20in%20Online%20Learning&body=Title%3A%20The%20Limits%20of%20Differential%20Privacy%20in%20Online%20Learning%0AAuthor%3A%20Bo%20Li%20and%20Wei%20Wang%20and%20Peng%20Ye%0AAbstract%3A%20%20%20Differential%20privacy%20%28DP%29%20is%20a%20formal%20notion%20that%20restricts%20the%20privacy%0Aleakage%20of%20an%20algorithm%20when%20running%20on%20sensitive%20data%2C%20in%20which%0Aprivacy-utility%20trade-off%20is%20one%20of%20the%20central%20problems%20in%20private%20data%0Aanalysis.%20In%20this%20work%2C%20we%20investigate%20the%20fundamental%20limits%20of%20differential%0Aprivacy%20in%20online%20learning%20algorithms%20and%20present%20evidence%20that%20separates%20three%0Atypes%20of%20constraints%3A%20no%20DP%2C%20pure%20DP%2C%20and%20approximate%20DP.%20We%20first%20describe%20a%0Ahypothesis%20class%20that%20is%20online%20learnable%20under%20approximate%20DP%20but%20not%20online%0Alearnable%20under%20pure%20DP%20under%20the%20adaptive%20adversarial%20setting.%20This%20indicates%0Athat%20approximate%20DP%20must%20be%20adopted%20when%20dealing%20with%20adaptive%20adversaries.%20We%0Athen%20prove%20that%20any%20private%20online%20learner%20must%20make%20an%20infinite%20number%20of%0Amistakes%20for%20almost%20all%20hypothesis%20classes.%20This%20essentially%20generalizes%0Aprevious%20results%20and%20shows%20a%20strong%20separation%20between%20private%20and%20non-private%0Asettings%20since%20a%20finite%20mistake%20bound%20is%20always%20attainable%20%28as%20long%20as%20the%0Aclass%20is%20online%20learnable%29%20when%20there%20is%20no%20privacy%20requirement.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05483v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Limits%2520of%2520Differential%2520Privacy%2520in%2520Online%2520Learning%26entry.906535625%3DBo%2520Li%2520and%2520Wei%2520Wang%2520and%2520Peng%2520Ye%26entry.1292438233%3D%2520%2520Differential%2520privacy%2520%2528DP%2529%2520is%2520a%2520formal%2520notion%2520that%2520restricts%2520the%2520privacy%250Aleakage%2520of%2520an%2520algorithm%2520when%2520running%2520on%2520sensitive%2520data%252C%2520in%2520which%250Aprivacy-utility%2520trade-off%2520is%2520one%2520of%2520the%2520central%2520problems%2520in%2520private%2520data%250Aanalysis.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520fundamental%2520limits%2520of%2520differential%250Aprivacy%2520in%2520online%2520learning%2520algorithms%2520and%2520present%2520evidence%2520that%2520separates%2520three%250Atypes%2520of%2520constraints%253A%2520no%2520DP%252C%2520pure%2520DP%252C%2520and%2520approximate%2520DP.%2520We%2520first%2520describe%2520a%250Ahypothesis%2520class%2520that%2520is%2520online%2520learnable%2520under%2520approximate%2520DP%2520but%2520not%2520online%250Alearnable%2520under%2520pure%2520DP%2520under%2520the%2520adaptive%2520adversarial%2520setting.%2520This%2520indicates%250Athat%2520approximate%2520DP%2520must%2520be%2520adopted%2520when%2520dealing%2520with%2520adaptive%2520adversaries.%2520We%250Athen%2520prove%2520that%2520any%2520private%2520online%2520learner%2520must%2520make%2520an%2520infinite%2520number%2520of%250Amistakes%2520for%2520almost%2520all%2520hypothesis%2520classes.%2520This%2520essentially%2520generalizes%250Aprevious%2520results%2520and%2520shows%2520a%2520strong%2520separation%2520between%2520private%2520and%2520non-private%250Asettings%2520since%2520a%2520finite%2520mistake%2520bound%2520is%2520always%2520attainable%2520%2528as%2520long%2520as%2520the%250Aclass%2520is%2520online%2520learnable%2529%2520when%2520there%2520is%2520no%2520privacy%2520requirement.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05483v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Limits%20of%20Differential%20Privacy%20in%20Online%20Learning&entry.906535625=Bo%20Li%20and%20Wei%20Wang%20and%20Peng%20Ye&entry.1292438233=%20%20Differential%20privacy%20%28DP%29%20is%20a%20formal%20notion%20that%20restricts%20the%20privacy%0Aleakage%20of%20an%20algorithm%20when%20running%20on%20sensitive%20data%2C%20in%20which%0Aprivacy-utility%20trade-off%20is%20one%20of%20the%20central%20problems%20in%20private%20data%0Aanalysis.%20In%20this%20work%2C%20we%20investigate%20the%20fundamental%20limits%20of%20differential%0Aprivacy%20in%20online%20learning%20algorithms%20and%20present%20evidence%20that%20separates%20three%0Atypes%20of%20constraints%3A%20no%20DP%2C%20pure%20DP%2C%20and%20approximate%20DP.%20We%20first%20describe%20a%0Ahypothesis%20class%20that%20is%20online%20learnable%20under%20approximate%20DP%20but%20not%20online%0Alearnable%20under%20pure%20DP%20under%20the%20adaptive%20adversarial%20setting.%20This%20indicates%0Athat%20approximate%20DP%20must%20be%20adopted%20when%20dealing%20with%20adaptive%20adversaries.%20We%0Athen%20prove%20that%20any%20private%20online%20learner%20must%20make%20an%20infinite%20number%20of%0Amistakes%20for%20almost%20all%20hypothesis%20classes.%20This%20essentially%20generalizes%0Aprevious%20results%20and%20shows%20a%20strong%20separation%20between%20private%20and%20non-private%0Asettings%20since%20a%20finite%20mistake%20bound%20is%20always%20attainable%20%28as%20long%20as%20the%0Aclass%20is%20online%20learnable%29%20when%20there%20is%20no%20privacy%20requirement.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05483v1&entry.124074799=Read"},
{"title": "CRepair: CVAE-based Automatic Vulnerability Repair Technology", "author": "Penghui Liu and Yingzhou Bi and Jiangtao Huang and Xinxin Jiang and Lianmei Wang", "abstract": "  Software vulnerabilities are flaws in computer software systems that pose\nsignificant threats to the integrity, security, and reliability of modern\nsoftware and its application data. These vulnerabilities can lead to\nsubstantial economic losses across various industries. Manual vulnerability\nrepair is not only time-consuming but also prone to errors. To address the\nchallenges of vulnerability repair, researchers have proposed various\nsolutions, with learning-based automatic vulnerability repair techniques\ngaining widespread attention. However, existing methods often focus on learning\nmore vulnerability data to improve repair outcomes, while neglecting the\ndiverse characteristics of vulnerable code, and suffer from imprecise\nvulnerability localization.To address these shortcomings, this paper proposes\nCRepair, a CVAE-based automatic vulnerability repair technology aimed at fixing\nsecurity vulnerabilities in system code. We first preprocess the vulnerability\ndata using a prompt-based method to serve as input to the model. Then, we apply\ncausal inference techniques to map the vulnerability feature data to\nprobability distributions. By employing multi-sample feature fusion, we capture\ndiverse vulnerability feature information. Finally, conditional control is used\nto guide the model in repairing the vulnerabilities.Experimental results\ndemonstrate that the proposed method significantly outperforms other benchmark\nmodels, achieving a perfect repair rate of 52%. The effectiveness of the\napproach is validated from multiple perspectives, advancing AI-driven code\nvulnerability repair and showing promising applications.\n", "link": "http://arxiv.org/abs/2411.05540v1", "date": "2024-11-08", "relevancy": 1.2672, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4379}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.419}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CRepair%3A%20CVAE-based%20Automatic%20Vulnerability%20Repair%20Technology&body=Title%3A%20CRepair%3A%20CVAE-based%20Automatic%20Vulnerability%20Repair%20Technology%0AAuthor%3A%20Penghui%20Liu%20and%20Yingzhou%20Bi%20and%20Jiangtao%20Huang%20and%20Xinxin%20Jiang%20and%20Lianmei%20Wang%0AAbstract%3A%20%20%20Software%20vulnerabilities%20are%20flaws%20in%20computer%20software%20systems%20that%20pose%0Asignificant%20threats%20to%20the%20integrity%2C%20security%2C%20and%20reliability%20of%20modern%0Asoftware%20and%20its%20application%20data.%20These%20vulnerabilities%20can%20lead%20to%0Asubstantial%20economic%20losses%20across%20various%20industries.%20Manual%20vulnerability%0Arepair%20is%20not%20only%20time-consuming%20but%20also%20prone%20to%20errors.%20To%20address%20the%0Achallenges%20of%20vulnerability%20repair%2C%20researchers%20have%20proposed%20various%0Asolutions%2C%20with%20learning-based%20automatic%20vulnerability%20repair%20techniques%0Againing%20widespread%20attention.%20However%2C%20existing%20methods%20often%20focus%20on%20learning%0Amore%20vulnerability%20data%20to%20improve%20repair%20outcomes%2C%20while%20neglecting%20the%0Adiverse%20characteristics%20of%20vulnerable%20code%2C%20and%20suffer%20from%20imprecise%0Avulnerability%20localization.To%20address%20these%20shortcomings%2C%20this%20paper%20proposes%0ACRepair%2C%20a%20CVAE-based%20automatic%20vulnerability%20repair%20technology%20aimed%20at%20fixing%0Asecurity%20vulnerabilities%20in%20system%20code.%20We%20first%20preprocess%20the%20vulnerability%0Adata%20using%20a%20prompt-based%20method%20to%20serve%20as%20input%20to%20the%20model.%20Then%2C%20we%20apply%0Acausal%20inference%20techniques%20to%20map%20the%20vulnerability%20feature%20data%20to%0Aprobability%20distributions.%20By%20employing%20multi-sample%20feature%20fusion%2C%20we%20capture%0Adiverse%20vulnerability%20feature%20information.%20Finally%2C%20conditional%20control%20is%20used%0Ato%20guide%20the%20model%20in%20repairing%20the%20vulnerabilities.Experimental%20results%0Ademonstrate%20that%20the%20proposed%20method%20significantly%20outperforms%20other%20benchmark%0Amodels%2C%20achieving%20a%20perfect%20repair%20rate%20of%2052%25.%20The%20effectiveness%20of%20the%0Aapproach%20is%20validated%20from%20multiple%20perspectives%2C%20advancing%20AI-driven%20code%0Avulnerability%20repair%20and%20showing%20promising%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCRepair%253A%2520CVAE-based%2520Automatic%2520Vulnerability%2520Repair%2520Technology%26entry.906535625%3DPenghui%2520Liu%2520and%2520Yingzhou%2520Bi%2520and%2520Jiangtao%2520Huang%2520and%2520Xinxin%2520Jiang%2520and%2520Lianmei%2520Wang%26entry.1292438233%3D%2520%2520Software%2520vulnerabilities%2520are%2520flaws%2520in%2520computer%2520software%2520systems%2520that%2520pose%250Asignificant%2520threats%2520to%2520the%2520integrity%252C%2520security%252C%2520and%2520reliability%2520of%2520modern%250Asoftware%2520and%2520its%2520application%2520data.%2520These%2520vulnerabilities%2520can%2520lead%2520to%250Asubstantial%2520economic%2520losses%2520across%2520various%2520industries.%2520Manual%2520vulnerability%250Arepair%2520is%2520not%2520only%2520time-consuming%2520but%2520also%2520prone%2520to%2520errors.%2520To%2520address%2520the%250Achallenges%2520of%2520vulnerability%2520repair%252C%2520researchers%2520have%2520proposed%2520various%250Asolutions%252C%2520with%2520learning-based%2520automatic%2520vulnerability%2520repair%2520techniques%250Againing%2520widespread%2520attention.%2520However%252C%2520existing%2520methods%2520often%2520focus%2520on%2520learning%250Amore%2520vulnerability%2520data%2520to%2520improve%2520repair%2520outcomes%252C%2520while%2520neglecting%2520the%250Adiverse%2520characteristics%2520of%2520vulnerable%2520code%252C%2520and%2520suffer%2520from%2520imprecise%250Avulnerability%2520localization.To%2520address%2520these%2520shortcomings%252C%2520this%2520paper%2520proposes%250ACRepair%252C%2520a%2520CVAE-based%2520automatic%2520vulnerability%2520repair%2520technology%2520aimed%2520at%2520fixing%250Asecurity%2520vulnerabilities%2520in%2520system%2520code.%2520We%2520first%2520preprocess%2520the%2520vulnerability%250Adata%2520using%2520a%2520prompt-based%2520method%2520to%2520serve%2520as%2520input%2520to%2520the%2520model.%2520Then%252C%2520we%2520apply%250Acausal%2520inference%2520techniques%2520to%2520map%2520the%2520vulnerability%2520feature%2520data%2520to%250Aprobability%2520distributions.%2520By%2520employing%2520multi-sample%2520feature%2520fusion%252C%2520we%2520capture%250Adiverse%2520vulnerability%2520feature%2520information.%2520Finally%252C%2520conditional%2520control%2520is%2520used%250Ato%2520guide%2520the%2520model%2520in%2520repairing%2520the%2520vulnerabilities.Experimental%2520results%250Ademonstrate%2520that%2520the%2520proposed%2520method%2520significantly%2520outperforms%2520other%2520benchmark%250Amodels%252C%2520achieving%2520a%2520perfect%2520repair%2520rate%2520of%252052%2525.%2520The%2520effectiveness%2520of%2520the%250Aapproach%2520is%2520validated%2520from%2520multiple%2520perspectives%252C%2520advancing%2520AI-driven%2520code%250Avulnerability%2520repair%2520and%2520showing%2520promising%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CRepair%3A%20CVAE-based%20Automatic%20Vulnerability%20Repair%20Technology&entry.906535625=Penghui%20Liu%20and%20Yingzhou%20Bi%20and%20Jiangtao%20Huang%20and%20Xinxin%20Jiang%20and%20Lianmei%20Wang&entry.1292438233=%20%20Software%20vulnerabilities%20are%20flaws%20in%20computer%20software%20systems%20that%20pose%0Asignificant%20threats%20to%20the%20integrity%2C%20security%2C%20and%20reliability%20of%20modern%0Asoftware%20and%20its%20application%20data.%20These%20vulnerabilities%20can%20lead%20to%0Asubstantial%20economic%20losses%20across%20various%20industries.%20Manual%20vulnerability%0Arepair%20is%20not%20only%20time-consuming%20but%20also%20prone%20to%20errors.%20To%20address%20the%0Achallenges%20of%20vulnerability%20repair%2C%20researchers%20have%20proposed%20various%0Asolutions%2C%20with%20learning-based%20automatic%20vulnerability%20repair%20techniques%0Againing%20widespread%20attention.%20However%2C%20existing%20methods%20often%20focus%20on%20learning%0Amore%20vulnerability%20data%20to%20improve%20repair%20outcomes%2C%20while%20neglecting%20the%0Adiverse%20characteristics%20of%20vulnerable%20code%2C%20and%20suffer%20from%20imprecise%0Avulnerability%20localization.To%20address%20these%20shortcomings%2C%20this%20paper%20proposes%0ACRepair%2C%20a%20CVAE-based%20automatic%20vulnerability%20repair%20technology%20aimed%20at%20fixing%0Asecurity%20vulnerabilities%20in%20system%20code.%20We%20first%20preprocess%20the%20vulnerability%0Adata%20using%20a%20prompt-based%20method%20to%20serve%20as%20input%20to%20the%20model.%20Then%2C%20we%20apply%0Acausal%20inference%20techniques%20to%20map%20the%20vulnerability%20feature%20data%20to%0Aprobability%20distributions.%20By%20employing%20multi-sample%20feature%20fusion%2C%20we%20capture%0Adiverse%20vulnerability%20feature%20information.%20Finally%2C%20conditional%20control%20is%20used%0Ato%20guide%20the%20model%20in%20repairing%20the%20vulnerabilities.Experimental%20results%0Ademonstrate%20that%20the%20proposed%20method%20significantly%20outperforms%20other%20benchmark%0Amodels%2C%20achieving%20a%20perfect%20repair%20rate%20of%2052%25.%20The%20effectiveness%20of%20the%0Aapproach%20is%20validated%20from%20multiple%20perspectives%2C%20advancing%20AI-driven%20code%0Avulnerability%20repair%20and%20showing%20promising%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05540v1&entry.124074799=Read"},
{"title": "Counterfactual Fairness by Combining Factual and Counterfactual\n  Predictions", "author": "Zeyu Zhou and Tianci Liu and Ruqi Bai and Jing Gao and Murat Kocaoglu and David I. Inouye", "abstract": "  In high-stake domains such as healthcare and hiring, the role of machine\nlearning (ML) in decision-making raises significant fairness concerns. This\nwork focuses on Counterfactual Fairness (CF), which posits that an ML model's\noutcome on any individual should remain unchanged if they had belonged to a\ndifferent demographic group. Previous works have proposed methods that\nguarantee CF. Notwithstanding, their effects on the model's predictive\nperformance remains largely unclear. To fill in this gap, we provide a\ntheoretical study on the inherent trade-off between CF and predictive\nperformance in a model-agnostic manner. We first propose a simple but effective\nmethod to cast an optimal but potentially unfair predictor into a fair one\nwithout losing the optimality. By analyzing its excess risk in order to achieve\nCF, we quantify this inherent trade-off. Further analysis on our method's\nperformance with access to only incomplete causal knowledge is also conducted.\nBuilt upon it, we propose a performant algorithm that can be applied in such\nscenarios. Experiments on both synthetic and semi-synthetic datasets\ndemonstrate the validity of our analysis and methods.\n", "link": "http://arxiv.org/abs/2409.01977v2", "date": "2024-11-08", "relevancy": 1.4366, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4917}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4789}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Counterfactual%20Fairness%20by%20Combining%20Factual%20and%20Counterfactual%0A%20%20Predictions&body=Title%3A%20Counterfactual%20Fairness%20by%20Combining%20Factual%20and%20Counterfactual%0A%20%20Predictions%0AAuthor%3A%20Zeyu%20Zhou%20and%20Tianci%20Liu%20and%20Ruqi%20Bai%20and%20Jing%20Gao%20and%20Murat%20Kocaoglu%20and%20David%20I.%20Inouye%0AAbstract%3A%20%20%20In%20high-stake%20domains%20such%20as%20healthcare%20and%20hiring%2C%20the%20role%20of%20machine%0Alearning%20%28ML%29%20in%20decision-making%20raises%20significant%20fairness%20concerns.%20This%0Awork%20focuses%20on%20Counterfactual%20Fairness%20%28CF%29%2C%20which%20posits%20that%20an%20ML%20model%27s%0Aoutcome%20on%20any%20individual%20should%20remain%20unchanged%20if%20they%20had%20belonged%20to%20a%0Adifferent%20demographic%20group.%20Previous%20works%20have%20proposed%20methods%20that%0Aguarantee%20CF.%20Notwithstanding%2C%20their%20effects%20on%20the%20model%27s%20predictive%0Aperformance%20remains%20largely%20unclear.%20To%20fill%20in%20this%20gap%2C%20we%20provide%20a%0Atheoretical%20study%20on%20the%20inherent%20trade-off%20between%20CF%20and%20predictive%0Aperformance%20in%20a%20model-agnostic%20manner.%20We%20first%20propose%20a%20simple%20but%20effective%0Amethod%20to%20cast%20an%20optimal%20but%20potentially%20unfair%20predictor%20into%20a%20fair%20one%0Awithout%20losing%20the%20optimality.%20By%20analyzing%20its%20excess%20risk%20in%20order%20to%20achieve%0ACF%2C%20we%20quantify%20this%20inherent%20trade-off.%20Further%20analysis%20on%20our%20method%27s%0Aperformance%20with%20access%20to%20only%20incomplete%20causal%20knowledge%20is%20also%20conducted.%0ABuilt%20upon%20it%2C%20we%20propose%20a%20performant%20algorithm%20that%20can%20be%20applied%20in%20such%0Ascenarios.%20Experiments%20on%20both%20synthetic%20and%20semi-synthetic%20datasets%0Ademonstrate%20the%20validity%20of%20our%20analysis%20and%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01977v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCounterfactual%2520Fairness%2520by%2520Combining%2520Factual%2520and%2520Counterfactual%250A%2520%2520Predictions%26entry.906535625%3DZeyu%2520Zhou%2520and%2520Tianci%2520Liu%2520and%2520Ruqi%2520Bai%2520and%2520Jing%2520Gao%2520and%2520Murat%2520Kocaoglu%2520and%2520David%2520I.%2520Inouye%26entry.1292438233%3D%2520%2520In%2520high-stake%2520domains%2520such%2520as%2520healthcare%2520and%2520hiring%252C%2520the%2520role%2520of%2520machine%250Alearning%2520%2528ML%2529%2520in%2520decision-making%2520raises%2520significant%2520fairness%2520concerns.%2520This%250Awork%2520focuses%2520on%2520Counterfactual%2520Fairness%2520%2528CF%2529%252C%2520which%2520posits%2520that%2520an%2520ML%2520model%2527s%250Aoutcome%2520on%2520any%2520individual%2520should%2520remain%2520unchanged%2520if%2520they%2520had%2520belonged%2520to%2520a%250Adifferent%2520demographic%2520group.%2520Previous%2520works%2520have%2520proposed%2520methods%2520that%250Aguarantee%2520CF.%2520Notwithstanding%252C%2520their%2520effects%2520on%2520the%2520model%2527s%2520predictive%250Aperformance%2520remains%2520largely%2520unclear.%2520To%2520fill%2520in%2520this%2520gap%252C%2520we%2520provide%2520a%250Atheoretical%2520study%2520on%2520the%2520inherent%2520trade-off%2520between%2520CF%2520and%2520predictive%250Aperformance%2520in%2520a%2520model-agnostic%2520manner.%2520We%2520first%2520propose%2520a%2520simple%2520but%2520effective%250Amethod%2520to%2520cast%2520an%2520optimal%2520but%2520potentially%2520unfair%2520predictor%2520into%2520a%2520fair%2520one%250Awithout%2520losing%2520the%2520optimality.%2520By%2520analyzing%2520its%2520excess%2520risk%2520in%2520order%2520to%2520achieve%250ACF%252C%2520we%2520quantify%2520this%2520inherent%2520trade-off.%2520Further%2520analysis%2520on%2520our%2520method%2527s%250Aperformance%2520with%2520access%2520to%2520only%2520incomplete%2520causal%2520knowledge%2520is%2520also%2520conducted.%250ABuilt%2520upon%2520it%252C%2520we%2520propose%2520a%2520performant%2520algorithm%2520that%2520can%2520be%2520applied%2520in%2520such%250Ascenarios.%2520Experiments%2520on%2520both%2520synthetic%2520and%2520semi-synthetic%2520datasets%250Ademonstrate%2520the%2520validity%2520of%2520our%2520analysis%2520and%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01977v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Counterfactual%20Fairness%20by%20Combining%20Factual%20and%20Counterfactual%0A%20%20Predictions&entry.906535625=Zeyu%20Zhou%20and%20Tianci%20Liu%20and%20Ruqi%20Bai%20and%20Jing%20Gao%20and%20Murat%20Kocaoglu%20and%20David%20I.%20Inouye&entry.1292438233=%20%20In%20high-stake%20domains%20such%20as%20healthcare%20and%20hiring%2C%20the%20role%20of%20machine%0Alearning%20%28ML%29%20in%20decision-making%20raises%20significant%20fairness%20concerns.%20This%0Awork%20focuses%20on%20Counterfactual%20Fairness%20%28CF%29%2C%20which%20posits%20that%20an%20ML%20model%27s%0Aoutcome%20on%20any%20individual%20should%20remain%20unchanged%20if%20they%20had%20belonged%20to%20a%0Adifferent%20demographic%20group.%20Previous%20works%20have%20proposed%20methods%20that%0Aguarantee%20CF.%20Notwithstanding%2C%20their%20effects%20on%20the%20model%27s%20predictive%0Aperformance%20remains%20largely%20unclear.%20To%20fill%20in%20this%20gap%2C%20we%20provide%20a%0Atheoretical%20study%20on%20the%20inherent%20trade-off%20between%20CF%20and%20predictive%0Aperformance%20in%20a%20model-agnostic%20manner.%20We%20first%20propose%20a%20simple%20but%20effective%0Amethod%20to%20cast%20an%20optimal%20but%20potentially%20unfair%20predictor%20into%20a%20fair%20one%0Awithout%20losing%20the%20optimality.%20By%20analyzing%20its%20excess%20risk%20in%20order%20to%20achieve%0ACF%2C%20we%20quantify%20this%20inherent%20trade-off.%20Further%20analysis%20on%20our%20method%27s%0Aperformance%20with%20access%20to%20only%20incomplete%20causal%20knowledge%20is%20also%20conducted.%0ABuilt%20upon%20it%2C%20we%20propose%20a%20performant%20algorithm%20that%20can%20be%20applied%20in%20such%0Ascenarios.%20Experiments%20on%20both%20synthetic%20and%20semi-synthetic%20datasets%0Ademonstrate%20the%20validity%20of%20our%20analysis%20and%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01977v2&entry.124074799=Read"},
{"title": "Adaptive Refinement Protocols for Distributed Distribution Estimation\n  under $\\ell^p$-Losses", "author": "Deheng Yuan and Tao Guo and Zhongyi Huang", "abstract": "  Consider the communication-constrained estimation of discrete distributions\nunder $\\ell^p$ losses, where each distributed terminal holds multiple\nindependent samples and uses limited number of bits to describe the samples. We\nobtain the minimax optimal rates of the problem in most parameter regimes. An\nelbow effect of the optimal rates at $p=2$ is clearly identified. To show the\noptimal rates, we first design estimation protocols to achieve them. The key\ningredient of these protocols is to introduce adaptive refinement mechanisms,\nwhich first generate rough estimate by partial information and then establish\nrefined estimate in subsequent steps guided by the rough estimate. The\nprotocols leverage successive refinement, sample compression, thresholding and\nrandom hashing methods to achieve the optimal rates in different parameter\nregimes. The optimality of the protocols is shown by deriving compatible\nminimax lower bounds.\n", "link": "http://arxiv.org/abs/2410.06884v2", "date": "2024-11-08", "relevancy": 1.2241, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4189}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3961}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Refinement%20Protocols%20for%20Distributed%20Distribution%20Estimation%0A%20%20under%20%24%5Cell%5Ep%24-Losses&body=Title%3A%20Adaptive%20Refinement%20Protocols%20for%20Distributed%20Distribution%20Estimation%0A%20%20under%20%24%5Cell%5Ep%24-Losses%0AAuthor%3A%20Deheng%20Yuan%20and%20Tao%20Guo%20and%20Zhongyi%20Huang%0AAbstract%3A%20%20%20Consider%20the%20communication-constrained%20estimation%20of%20discrete%20distributions%0Aunder%20%24%5Cell%5Ep%24%20losses%2C%20where%20each%20distributed%20terminal%20holds%20multiple%0Aindependent%20samples%20and%20uses%20limited%20number%20of%20bits%20to%20describe%20the%20samples.%20We%0Aobtain%20the%20minimax%20optimal%20rates%20of%20the%20problem%20in%20most%20parameter%20regimes.%20An%0Aelbow%20effect%20of%20the%20optimal%20rates%20at%20%24p%3D2%24%20is%20clearly%20identified.%20To%20show%20the%0Aoptimal%20rates%2C%20we%20first%20design%20estimation%20protocols%20to%20achieve%20them.%20The%20key%0Aingredient%20of%20these%20protocols%20is%20to%20introduce%20adaptive%20refinement%20mechanisms%2C%0Awhich%20first%20generate%20rough%20estimate%20by%20partial%20information%20and%20then%20establish%0Arefined%20estimate%20in%20subsequent%20steps%20guided%20by%20the%20rough%20estimate.%20The%0Aprotocols%20leverage%20successive%20refinement%2C%20sample%20compression%2C%20thresholding%20and%0Arandom%20hashing%20methods%20to%20achieve%20the%20optimal%20rates%20in%20different%20parameter%0Aregimes.%20The%20optimality%20of%20the%20protocols%20is%20shown%20by%20deriving%20compatible%0Aminimax%20lower%20bounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06884v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Refinement%2520Protocols%2520for%2520Distributed%2520Distribution%2520Estimation%250A%2520%2520under%2520%2524%255Cell%255Ep%2524-Losses%26entry.906535625%3DDeheng%2520Yuan%2520and%2520Tao%2520Guo%2520and%2520Zhongyi%2520Huang%26entry.1292438233%3D%2520%2520Consider%2520the%2520communication-constrained%2520estimation%2520of%2520discrete%2520distributions%250Aunder%2520%2524%255Cell%255Ep%2524%2520losses%252C%2520where%2520each%2520distributed%2520terminal%2520holds%2520multiple%250Aindependent%2520samples%2520and%2520uses%2520limited%2520number%2520of%2520bits%2520to%2520describe%2520the%2520samples.%2520We%250Aobtain%2520the%2520minimax%2520optimal%2520rates%2520of%2520the%2520problem%2520in%2520most%2520parameter%2520regimes.%2520An%250Aelbow%2520effect%2520of%2520the%2520optimal%2520rates%2520at%2520%2524p%253D2%2524%2520is%2520clearly%2520identified.%2520To%2520show%2520the%250Aoptimal%2520rates%252C%2520we%2520first%2520design%2520estimation%2520protocols%2520to%2520achieve%2520them.%2520The%2520key%250Aingredient%2520of%2520these%2520protocols%2520is%2520to%2520introduce%2520adaptive%2520refinement%2520mechanisms%252C%250Awhich%2520first%2520generate%2520rough%2520estimate%2520by%2520partial%2520information%2520and%2520then%2520establish%250Arefined%2520estimate%2520in%2520subsequent%2520steps%2520guided%2520by%2520the%2520rough%2520estimate.%2520The%250Aprotocols%2520leverage%2520successive%2520refinement%252C%2520sample%2520compression%252C%2520thresholding%2520and%250Arandom%2520hashing%2520methods%2520to%2520achieve%2520the%2520optimal%2520rates%2520in%2520different%2520parameter%250Aregimes.%2520The%2520optimality%2520of%2520the%2520protocols%2520is%2520shown%2520by%2520deriving%2520compatible%250Aminimax%2520lower%2520bounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06884v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Refinement%20Protocols%20for%20Distributed%20Distribution%20Estimation%0A%20%20under%20%24%5Cell%5Ep%24-Losses&entry.906535625=Deheng%20Yuan%20and%20Tao%20Guo%20and%20Zhongyi%20Huang&entry.1292438233=%20%20Consider%20the%20communication-constrained%20estimation%20of%20discrete%20distributions%0Aunder%20%24%5Cell%5Ep%24%20losses%2C%20where%20each%20distributed%20terminal%20holds%20multiple%0Aindependent%20samples%20and%20uses%20limited%20number%20of%20bits%20to%20describe%20the%20samples.%20We%0Aobtain%20the%20minimax%20optimal%20rates%20of%20the%20problem%20in%20most%20parameter%20regimes.%20An%0Aelbow%20effect%20of%20the%20optimal%20rates%20at%20%24p%3D2%24%20is%20clearly%20identified.%20To%20show%20the%0Aoptimal%20rates%2C%20we%20first%20design%20estimation%20protocols%20to%20achieve%20them.%20The%20key%0Aingredient%20of%20these%20protocols%20is%20to%20introduce%20adaptive%20refinement%20mechanisms%2C%0Awhich%20first%20generate%20rough%20estimate%20by%20partial%20information%20and%20then%20establish%0Arefined%20estimate%20in%20subsequent%20steps%20guided%20by%20the%20rough%20estimate.%20The%0Aprotocols%20leverage%20successive%20refinement%2C%20sample%20compression%2C%20thresholding%20and%0Arandom%20hashing%20methods%20to%20achieve%20the%20optimal%20rates%20in%20different%20parameter%0Aregimes.%20The%20optimality%20of%20the%20protocols%20is%20shown%20by%20deriving%20compatible%0Aminimax%20lower%20bounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06884v2&entry.124074799=Read"},
{"title": "Enhancing Robustness in Language-Driven Robotics: A Modular Approach to\n  Failure Reduction", "author": "\u00c9miland Garrab\u00e9 and Pierre Teixeira and Mahdi Khoramshahi and St\u00e9phane Doncieux", "abstract": "  Recent advances in large language models (LLMs) have led to significant\nprogress in robotics, enabling embodied agents to better understand and execute\nopen-ended tasks. However, existing approaches using LLMs face limitations in\ngrounding their outputs within the physical environment and aligning with the\ncapabilities of the robot. This challenge becomes even more pronounced with\nsmaller language models, which are more computationally efficient but less\nrobust in task planning and execution. In this paper, we present a novel\nmodular architecture designed to enhance the robustness of LLM-driven robotics\nby addressing these grounding and alignment issues. We formalize the task\nplanning problem within a goal-conditioned POMDP framework, identify key\nfailure modes in LLM-driven planning, and propose targeted design principles to\nmitigate these issues. Our architecture introduces an ``expected outcomes''\nmodule to prevent mischaracterization of subgoals and a feedback mechanism to\nenable real-time error recovery. Experimental results, both in simulation and\non physical robots, demonstrate that our approach significantly improves task\nsuccess rates for pick-and-place and manipulation tasks compared to both larger\nLLMs and standard baselines. Through hardware experiments, we also demonstrate\nhow our architecture can be run efficiently and locally. This work highlights\nthe potential of smaller, locally-executable LLMs in robotics and provides a\nscalable, efficient solution for robust task execution.\n", "link": "http://arxiv.org/abs/2411.05474v1", "date": "2024-11-08", "relevancy": 1.7008, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5978}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5911}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Robustness%20in%20Language-Driven%20Robotics%3A%20A%20Modular%20Approach%20to%0A%20%20Failure%20Reduction&body=Title%3A%20Enhancing%20Robustness%20in%20Language-Driven%20Robotics%3A%20A%20Modular%20Approach%20to%0A%20%20Failure%20Reduction%0AAuthor%3A%20%C3%89miland%20Garrab%C3%A9%20and%20Pierre%20Teixeira%20and%20Mahdi%20Khoramshahi%20and%20St%C3%A9phane%20Doncieux%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20led%20to%20significant%0Aprogress%20in%20robotics%2C%20enabling%20embodied%20agents%20to%20better%20understand%20and%20execute%0Aopen-ended%20tasks.%20However%2C%20existing%20approaches%20using%20LLMs%20face%20limitations%20in%0Agrounding%20their%20outputs%20within%20the%20physical%20environment%20and%20aligning%20with%20the%0Acapabilities%20of%20the%20robot.%20This%20challenge%20becomes%20even%20more%20pronounced%20with%0Asmaller%20language%20models%2C%20which%20are%20more%20computationally%20efficient%20but%20less%0Arobust%20in%20task%20planning%20and%20execution.%20In%20this%20paper%2C%20we%20present%20a%20novel%0Amodular%20architecture%20designed%20to%20enhance%20the%20robustness%20of%20LLM-driven%20robotics%0Aby%20addressing%20these%20grounding%20and%20alignment%20issues.%20We%20formalize%20the%20task%0Aplanning%20problem%20within%20a%20goal-conditioned%20POMDP%20framework%2C%20identify%20key%0Afailure%20modes%20in%20LLM-driven%20planning%2C%20and%20propose%20targeted%20design%20principles%20to%0Amitigate%20these%20issues.%20Our%20architecture%20introduces%20an%20%60%60expected%20outcomes%27%27%0Amodule%20to%20prevent%20mischaracterization%20of%20subgoals%20and%20a%20feedback%20mechanism%20to%0Aenable%20real-time%20error%20recovery.%20Experimental%20results%2C%20both%20in%20simulation%20and%0Aon%20physical%20robots%2C%20demonstrate%20that%20our%20approach%20significantly%20improves%20task%0Asuccess%20rates%20for%20pick-and-place%20and%20manipulation%20tasks%20compared%20to%20both%20larger%0ALLMs%20and%20standard%20baselines.%20Through%20hardware%20experiments%2C%20we%20also%20demonstrate%0Ahow%20our%20architecture%20can%20be%20run%20efficiently%20and%20locally.%20This%20work%20highlights%0Athe%20potential%20of%20smaller%2C%20locally-executable%20LLMs%20in%20robotics%20and%20provides%20a%0Ascalable%2C%20efficient%20solution%20for%20robust%20task%20execution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05474v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Robustness%2520in%2520Language-Driven%2520Robotics%253A%2520A%2520Modular%2520Approach%2520to%250A%2520%2520Failure%2520Reduction%26entry.906535625%3D%25C3%2589miland%2520Garrab%25C3%25A9%2520and%2520Pierre%2520Teixeira%2520and%2520Mahdi%2520Khoramshahi%2520and%2520St%25C3%25A9phane%2520Doncieux%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520led%2520to%2520significant%250Aprogress%2520in%2520robotics%252C%2520enabling%2520embodied%2520agents%2520to%2520better%2520understand%2520and%2520execute%250Aopen-ended%2520tasks.%2520However%252C%2520existing%2520approaches%2520using%2520LLMs%2520face%2520limitations%2520in%250Agrounding%2520their%2520outputs%2520within%2520the%2520physical%2520environment%2520and%2520aligning%2520with%2520the%250Acapabilities%2520of%2520the%2520robot.%2520This%2520challenge%2520becomes%2520even%2520more%2520pronounced%2520with%250Asmaller%2520language%2520models%252C%2520which%2520are%2520more%2520computationally%2520efficient%2520but%2520less%250Arobust%2520in%2520task%2520planning%2520and%2520execution.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%250Amodular%2520architecture%2520designed%2520to%2520enhance%2520the%2520robustness%2520of%2520LLM-driven%2520robotics%250Aby%2520addressing%2520these%2520grounding%2520and%2520alignment%2520issues.%2520We%2520formalize%2520the%2520task%250Aplanning%2520problem%2520within%2520a%2520goal-conditioned%2520POMDP%2520framework%252C%2520identify%2520key%250Afailure%2520modes%2520in%2520LLM-driven%2520planning%252C%2520and%2520propose%2520targeted%2520design%2520principles%2520to%250Amitigate%2520these%2520issues.%2520Our%2520architecture%2520introduces%2520an%2520%2560%2560expected%2520outcomes%2527%2527%250Amodule%2520to%2520prevent%2520mischaracterization%2520of%2520subgoals%2520and%2520a%2520feedback%2520mechanism%2520to%250Aenable%2520real-time%2520error%2520recovery.%2520Experimental%2520results%252C%2520both%2520in%2520simulation%2520and%250Aon%2520physical%2520robots%252C%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520improves%2520task%250Asuccess%2520rates%2520for%2520pick-and-place%2520and%2520manipulation%2520tasks%2520compared%2520to%2520both%2520larger%250ALLMs%2520and%2520standard%2520baselines.%2520Through%2520hardware%2520experiments%252C%2520we%2520also%2520demonstrate%250Ahow%2520our%2520architecture%2520can%2520be%2520run%2520efficiently%2520and%2520locally.%2520This%2520work%2520highlights%250Athe%2520potential%2520of%2520smaller%252C%2520locally-executable%2520LLMs%2520in%2520robotics%2520and%2520provides%2520a%250Ascalable%252C%2520efficient%2520solution%2520for%2520robust%2520task%2520execution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05474v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Robustness%20in%20Language-Driven%20Robotics%3A%20A%20Modular%20Approach%20to%0A%20%20Failure%20Reduction&entry.906535625=%C3%89miland%20Garrab%C3%A9%20and%20Pierre%20Teixeira%20and%20Mahdi%20Khoramshahi%20and%20St%C3%A9phane%20Doncieux&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20led%20to%20significant%0Aprogress%20in%20robotics%2C%20enabling%20embodied%20agents%20to%20better%20understand%20and%20execute%0Aopen-ended%20tasks.%20However%2C%20existing%20approaches%20using%20LLMs%20face%20limitations%20in%0Agrounding%20their%20outputs%20within%20the%20physical%20environment%20and%20aligning%20with%20the%0Acapabilities%20of%20the%20robot.%20This%20challenge%20becomes%20even%20more%20pronounced%20with%0Asmaller%20language%20models%2C%20which%20are%20more%20computationally%20efficient%20but%20less%0Arobust%20in%20task%20planning%20and%20execution.%20In%20this%20paper%2C%20we%20present%20a%20novel%0Amodular%20architecture%20designed%20to%20enhance%20the%20robustness%20of%20LLM-driven%20robotics%0Aby%20addressing%20these%20grounding%20and%20alignment%20issues.%20We%20formalize%20the%20task%0Aplanning%20problem%20within%20a%20goal-conditioned%20POMDP%20framework%2C%20identify%20key%0Afailure%20modes%20in%20LLM-driven%20planning%2C%20and%20propose%20targeted%20design%20principles%20to%0Amitigate%20these%20issues.%20Our%20architecture%20introduces%20an%20%60%60expected%20outcomes%27%27%0Amodule%20to%20prevent%20mischaracterization%20of%20subgoals%20and%20a%20feedback%20mechanism%20to%0Aenable%20real-time%20error%20recovery.%20Experimental%20results%2C%20both%20in%20simulation%20and%0Aon%20physical%20robots%2C%20demonstrate%20that%20our%20approach%20significantly%20improves%20task%0Asuccess%20rates%20for%20pick-and-place%20and%20manipulation%20tasks%20compared%20to%20both%20larger%0ALLMs%20and%20standard%20baselines.%20Through%20hardware%20experiments%2C%20we%20also%20demonstrate%0Ahow%20our%20architecture%20can%20be%20run%20efficiently%20and%20locally.%20This%20work%20highlights%0Athe%20potential%20of%20smaller%2C%20locally-executable%20LLMs%20in%20robotics%20and%20provides%20a%0Ascalable%2C%20efficient%20solution%20for%20robust%20task%20execution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05474v1&entry.124074799=Read"},
{"title": "Pseudo-Labeling for Kernel Ridge Regression under Covariate Shift", "author": "Kaizheng Wang", "abstract": "  We develop and analyze a principled approach to kernel ridge regression under\ncovariate shift. The goal is to learn a regression function with small mean\nsquared error over a target distribution, based on unlabeled data from there\nand labeled data that may have a different feature distribution. We propose to\nsplit the labeled data into two subsets, and conduct kernel ridge regression on\nthem separately to obtain a collection of candidate models and an imputation\nmodel. We use the latter to fill the missing labels and then select the best\ncandidate accordingly. Our non-asymptotic excess risk bounds demonstrate that\nour estimator adapts effectively to both the structure of the target\ndistribution and the covariate shift. This adaptation is quantified through a\nnotion of effective sample size that reflects the value of labeled source data\nfor the target regression task. Our estimator achieves the minimax optimal\nerror rate up to a polylogarithmic factor, and we find that using pseudo-labels\nfor model selection does not significantly hinder performance.\n", "link": "http://arxiv.org/abs/2302.10160v3", "date": "2024-11-08", "relevancy": 1.6705, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4343}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4154}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pseudo-Labeling%20for%20Kernel%20Ridge%20Regression%20under%20Covariate%20Shift&body=Title%3A%20Pseudo-Labeling%20for%20Kernel%20Ridge%20Regression%20under%20Covariate%20Shift%0AAuthor%3A%20Kaizheng%20Wang%0AAbstract%3A%20%20%20We%20develop%20and%20analyze%20a%20principled%20approach%20to%20kernel%20ridge%20regression%20under%0Acovariate%20shift.%20The%20goal%20is%20to%20learn%20a%20regression%20function%20with%20small%20mean%0Asquared%20error%20over%20a%20target%20distribution%2C%20based%20on%20unlabeled%20data%20from%20there%0Aand%20labeled%20data%20that%20may%20have%20a%20different%20feature%20distribution.%20We%20propose%20to%0Asplit%20the%20labeled%20data%20into%20two%20subsets%2C%20and%20conduct%20kernel%20ridge%20regression%20on%0Athem%20separately%20to%20obtain%20a%20collection%20of%20candidate%20models%20and%20an%20imputation%0Amodel.%20We%20use%20the%20latter%20to%20fill%20the%20missing%20labels%20and%20then%20select%20the%20best%0Acandidate%20accordingly.%20Our%20non-asymptotic%20excess%20risk%20bounds%20demonstrate%20that%0Aour%20estimator%20adapts%20effectively%20to%20both%20the%20structure%20of%20the%20target%0Adistribution%20and%20the%20covariate%20shift.%20This%20adaptation%20is%20quantified%20through%20a%0Anotion%20of%20effective%20sample%20size%20that%20reflects%20the%20value%20of%20labeled%20source%20data%0Afor%20the%20target%20regression%20task.%20Our%20estimator%20achieves%20the%20minimax%20optimal%0Aerror%20rate%20up%20to%20a%20polylogarithmic%20factor%2C%20and%20we%20find%20that%20using%20pseudo-labels%0Afor%20model%20selection%20does%20not%20significantly%20hinder%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.10160v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPseudo-Labeling%2520for%2520Kernel%2520Ridge%2520Regression%2520under%2520Covariate%2520Shift%26entry.906535625%3DKaizheng%2520Wang%26entry.1292438233%3D%2520%2520We%2520develop%2520and%2520analyze%2520a%2520principled%2520approach%2520to%2520kernel%2520ridge%2520regression%2520under%250Acovariate%2520shift.%2520The%2520goal%2520is%2520to%2520learn%2520a%2520regression%2520function%2520with%2520small%2520mean%250Asquared%2520error%2520over%2520a%2520target%2520distribution%252C%2520based%2520on%2520unlabeled%2520data%2520from%2520there%250Aand%2520labeled%2520data%2520that%2520may%2520have%2520a%2520different%2520feature%2520distribution.%2520We%2520propose%2520to%250Asplit%2520the%2520labeled%2520data%2520into%2520two%2520subsets%252C%2520and%2520conduct%2520kernel%2520ridge%2520regression%2520on%250Athem%2520separately%2520to%2520obtain%2520a%2520collection%2520of%2520candidate%2520models%2520and%2520an%2520imputation%250Amodel.%2520We%2520use%2520the%2520latter%2520to%2520fill%2520the%2520missing%2520labels%2520and%2520then%2520select%2520the%2520best%250Acandidate%2520accordingly.%2520Our%2520non-asymptotic%2520excess%2520risk%2520bounds%2520demonstrate%2520that%250Aour%2520estimator%2520adapts%2520effectively%2520to%2520both%2520the%2520structure%2520of%2520the%2520target%250Adistribution%2520and%2520the%2520covariate%2520shift.%2520This%2520adaptation%2520is%2520quantified%2520through%2520a%250Anotion%2520of%2520effective%2520sample%2520size%2520that%2520reflects%2520the%2520value%2520of%2520labeled%2520source%2520data%250Afor%2520the%2520target%2520regression%2520task.%2520Our%2520estimator%2520achieves%2520the%2520minimax%2520optimal%250Aerror%2520rate%2520up%2520to%2520a%2520polylogarithmic%2520factor%252C%2520and%2520we%2520find%2520that%2520using%2520pseudo-labels%250Afor%2520model%2520selection%2520does%2520not%2520significantly%2520hinder%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.10160v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pseudo-Labeling%20for%20Kernel%20Ridge%20Regression%20under%20Covariate%20Shift&entry.906535625=Kaizheng%20Wang&entry.1292438233=%20%20We%20develop%20and%20analyze%20a%20principled%20approach%20to%20kernel%20ridge%20regression%20under%0Acovariate%20shift.%20The%20goal%20is%20to%20learn%20a%20regression%20function%20with%20small%20mean%0Asquared%20error%20over%20a%20target%20distribution%2C%20based%20on%20unlabeled%20data%20from%20there%0Aand%20labeled%20data%20that%20may%20have%20a%20different%20feature%20distribution.%20We%20propose%20to%0Asplit%20the%20labeled%20data%20into%20two%20subsets%2C%20and%20conduct%20kernel%20ridge%20regression%20on%0Athem%20separately%20to%20obtain%20a%20collection%20of%20candidate%20models%20and%20an%20imputation%0Amodel.%20We%20use%20the%20latter%20to%20fill%20the%20missing%20labels%20and%20then%20select%20the%20best%0Acandidate%20accordingly.%20Our%20non-asymptotic%20excess%20risk%20bounds%20demonstrate%20that%0Aour%20estimator%20adapts%20effectively%20to%20both%20the%20structure%20of%20the%20target%0Adistribution%20and%20the%20covariate%20shift.%20This%20adaptation%20is%20quantified%20through%20a%0Anotion%20of%20effective%20sample%20size%20that%20reflects%20the%20value%20of%20labeled%20source%20data%0Afor%20the%20target%20regression%20task.%20Our%20estimator%20achieves%20the%20minimax%20optimal%0Aerror%20rate%20up%20to%20a%20polylogarithmic%20factor%2C%20and%20we%20find%20that%20using%20pseudo-labels%0Afor%20model%20selection%20does%20not%20significantly%20hinder%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.10160v3&entry.124074799=Read"},
{"title": "Multi-Dimensional Reconfigurable, Physically Composable Hybrid\n  Diffractive Optical Neural Network", "author": "Ziang Yin and Yu Yao and Jeff Zhang and Jiaqi Gu", "abstract": "  Diffractive optical neural networks (DONNs), leveraging free-space light wave\npropagation for ultra-parallel, high-efficiency computing, have emerged as\npromising artificial intelligence (AI) accelerators. However, their inherent\nlack of reconfigurability due to fixed optical structures post-fabrication\nhinders practical deployment in the face of dynamic AI workloads and evolving\napplications. To overcome this challenge, we introduce, for the first time, a\nmulti-dimensional reconfigurable hybrid diffractive ONN system (MDR-HDONN), a\nphysically composable architecture that unlocks a new degree of freedom and\nunprecedented versatility in DONNs. By leveraging full-system learnability,\nMDR-HDONN repurposes fixed fabricated optical hardware, achieving exponentially\nexpanded functionality and superior task adaptability through the\ndifferentiable learning of system variables. Furthermore, MDR-HDONN adopts a\nhybrid optical/photonic design, combining the reconfigurability of integrated\nphotonics with the ultra-parallelism of free-space diffractive systems.\nExtensive evaluations demonstrate that MDR-HDONN has digital-comparable\naccuracy on various task adaptations with 74x faster speed and 194x lower\nenergy. Compared to prior DONNs, MDR-HDONN shows exponentially larger\nfunctional space with 5x faster training speed, paving the way for a new\nparadigm of versatile, composable, hybrid optical/photonic AI computing. We\nwill open-source our codes.\n", "link": "http://arxiv.org/abs/2411.05748v1", "date": "2024-11-08", "relevancy": 1.5886, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5451}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5105}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Dimensional%20Reconfigurable%2C%20Physically%20Composable%20Hybrid%0A%20%20Diffractive%20Optical%20Neural%20Network&body=Title%3A%20Multi-Dimensional%20Reconfigurable%2C%20Physically%20Composable%20Hybrid%0A%20%20Diffractive%20Optical%20Neural%20Network%0AAuthor%3A%20Ziang%20Yin%20and%20Yu%20Yao%20and%20Jeff%20Zhang%20and%20Jiaqi%20Gu%0AAbstract%3A%20%20%20Diffractive%20optical%20neural%20networks%20%28DONNs%29%2C%20leveraging%20free-space%20light%20wave%0Apropagation%20for%20ultra-parallel%2C%20high-efficiency%20computing%2C%20have%20emerged%20as%0Apromising%20artificial%20intelligence%20%28AI%29%20accelerators.%20However%2C%20their%20inherent%0Alack%20of%20reconfigurability%20due%20to%20fixed%20optical%20structures%20post-fabrication%0Ahinders%20practical%20deployment%20in%20the%20face%20of%20dynamic%20AI%20workloads%20and%20evolving%0Aapplications.%20To%20overcome%20this%20challenge%2C%20we%20introduce%2C%20for%20the%20first%20time%2C%20a%0Amulti-dimensional%20reconfigurable%20hybrid%20diffractive%20ONN%20system%20%28MDR-HDONN%29%2C%20a%0Aphysically%20composable%20architecture%20that%20unlocks%20a%20new%20degree%20of%20freedom%20and%0Aunprecedented%20versatility%20in%20DONNs.%20By%20leveraging%20full-system%20learnability%2C%0AMDR-HDONN%20repurposes%20fixed%20fabricated%20optical%20hardware%2C%20achieving%20exponentially%0Aexpanded%20functionality%20and%20superior%20task%20adaptability%20through%20the%0Adifferentiable%20learning%20of%20system%20variables.%20Furthermore%2C%20MDR-HDONN%20adopts%20a%0Ahybrid%20optical/photonic%20design%2C%20combining%20the%20reconfigurability%20of%20integrated%0Aphotonics%20with%20the%20ultra-parallelism%20of%20free-space%20diffractive%20systems.%0AExtensive%20evaluations%20demonstrate%20that%20MDR-HDONN%20has%20digital-comparable%0Aaccuracy%20on%20various%20task%20adaptations%20with%2074x%20faster%20speed%20and%20194x%20lower%0Aenergy.%20Compared%20to%20prior%20DONNs%2C%20MDR-HDONN%20shows%20exponentially%20larger%0Afunctional%20space%20with%205x%20faster%20training%20speed%2C%20paving%20the%20way%20for%20a%20new%0Aparadigm%20of%20versatile%2C%20composable%2C%20hybrid%20optical/photonic%20AI%20computing.%20We%0Awill%20open-source%20our%20codes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.05748v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Dimensional%2520Reconfigurable%252C%2520Physically%2520Composable%2520Hybrid%250A%2520%2520Diffractive%2520Optical%2520Neural%2520Network%26entry.906535625%3DZiang%2520Yin%2520and%2520Yu%2520Yao%2520and%2520Jeff%2520Zhang%2520and%2520Jiaqi%2520Gu%26entry.1292438233%3D%2520%2520Diffractive%2520optical%2520neural%2520networks%2520%2528DONNs%2529%252C%2520leveraging%2520free-space%2520light%2520wave%250Apropagation%2520for%2520ultra-parallel%252C%2520high-efficiency%2520computing%252C%2520have%2520emerged%2520as%250Apromising%2520artificial%2520intelligence%2520%2528AI%2529%2520accelerators.%2520However%252C%2520their%2520inherent%250Alack%2520of%2520reconfigurability%2520due%2520to%2520fixed%2520optical%2520structures%2520post-fabrication%250Ahinders%2520practical%2520deployment%2520in%2520the%2520face%2520of%2520dynamic%2520AI%2520workloads%2520and%2520evolving%250Aapplications.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520introduce%252C%2520for%2520the%2520first%2520time%252C%2520a%250Amulti-dimensional%2520reconfigurable%2520hybrid%2520diffractive%2520ONN%2520system%2520%2528MDR-HDONN%2529%252C%2520a%250Aphysically%2520composable%2520architecture%2520that%2520unlocks%2520a%2520new%2520degree%2520of%2520freedom%2520and%250Aunprecedented%2520versatility%2520in%2520DONNs.%2520By%2520leveraging%2520full-system%2520learnability%252C%250AMDR-HDONN%2520repurposes%2520fixed%2520fabricated%2520optical%2520hardware%252C%2520achieving%2520exponentially%250Aexpanded%2520functionality%2520and%2520superior%2520task%2520adaptability%2520through%2520the%250Adifferentiable%2520learning%2520of%2520system%2520variables.%2520Furthermore%252C%2520MDR-HDONN%2520adopts%2520a%250Ahybrid%2520optical/photonic%2520design%252C%2520combining%2520the%2520reconfigurability%2520of%2520integrated%250Aphotonics%2520with%2520the%2520ultra-parallelism%2520of%2520free-space%2520diffractive%2520systems.%250AExtensive%2520evaluations%2520demonstrate%2520that%2520MDR-HDONN%2520has%2520digital-comparable%250Aaccuracy%2520on%2520various%2520task%2520adaptations%2520with%252074x%2520faster%2520speed%2520and%2520194x%2520lower%250Aenergy.%2520Compared%2520to%2520prior%2520DONNs%252C%2520MDR-HDONN%2520shows%2520exponentially%2520larger%250Afunctional%2520space%2520with%25205x%2520faster%2520training%2520speed%252C%2520paving%2520the%2520way%2520for%2520a%2520new%250Aparadigm%2520of%2520versatile%252C%2520composable%252C%2520hybrid%2520optical/photonic%2520AI%2520computing.%2520We%250Awill%2520open-source%2520our%2520codes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.05748v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Dimensional%20Reconfigurable%2C%20Physically%20Composable%20Hybrid%0A%20%20Diffractive%20Optical%20Neural%20Network&entry.906535625=Ziang%20Yin%20and%20Yu%20Yao%20and%20Jeff%20Zhang%20and%20Jiaqi%20Gu&entry.1292438233=%20%20Diffractive%20optical%20neural%20networks%20%28DONNs%29%2C%20leveraging%20free-space%20light%20wave%0Apropagation%20for%20ultra-parallel%2C%20high-efficiency%20computing%2C%20have%20emerged%20as%0Apromising%20artificial%20intelligence%20%28AI%29%20accelerators.%20However%2C%20their%20inherent%0Alack%20of%20reconfigurability%20due%20to%20fixed%20optical%20structures%20post-fabrication%0Ahinders%20practical%20deployment%20in%20the%20face%20of%20dynamic%20AI%20workloads%20and%20evolving%0Aapplications.%20To%20overcome%20this%20challenge%2C%20we%20introduce%2C%20for%20the%20first%20time%2C%20a%0Amulti-dimensional%20reconfigurable%20hybrid%20diffractive%20ONN%20system%20%28MDR-HDONN%29%2C%20a%0Aphysically%20composable%20architecture%20that%20unlocks%20a%20new%20degree%20of%20freedom%20and%0Aunprecedented%20versatility%20in%20DONNs.%20By%20leveraging%20full-system%20learnability%2C%0AMDR-HDONN%20repurposes%20fixed%20fabricated%20optical%20hardware%2C%20achieving%20exponentially%0Aexpanded%20functionality%20and%20superior%20task%20adaptability%20through%20the%0Adifferentiable%20learning%20of%20system%20variables.%20Furthermore%2C%20MDR-HDONN%20adopts%20a%0Ahybrid%20optical/photonic%20design%2C%20combining%20the%20reconfigurability%20of%20integrated%0Aphotonics%20with%20the%20ultra-parallelism%20of%20free-space%20diffractive%20systems.%0AExtensive%20evaluations%20demonstrate%20that%20MDR-HDONN%20has%20digital-comparable%0Aaccuracy%20on%20various%20task%20adaptations%20with%2074x%20faster%20speed%20and%20194x%20lower%0Aenergy.%20Compared%20to%20prior%20DONNs%2C%20MDR-HDONN%20shows%20exponentially%20larger%0Afunctional%20space%20with%205x%20faster%20training%20speed%2C%20paving%20the%20way%20for%20a%20new%0Aparadigm%20of%20versatile%2C%20composable%2C%20hybrid%20optical/photonic%20AI%20computing.%20We%0Awill%20open-source%20our%20codes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.05748v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


