<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240612.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Human 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent\n  Diffusion Models", "author": "Yuxuan Xue and Xianghui Xie and Riccardo Marin and Gerard Pons-Moll", "abstract": "  Creating realistic avatars from a single RGB image is an attractive yet\nchallenging problem. Due to its ill-posed nature, recent works leverage\npowerful prior from 2D diffusion models pretrained on large datasets. Although\n2D diffusion models demonstrate strong generalization capability, they cannot\nprovide multi-view shape priors with guaranteed 3D consistency. We propose\nHuman 3Diffusion: Realistic Avatar Creation via Explicit 3D Consistent\nDiffusion. Our key insight is that 2D multi-view diffusion and 3D\nreconstruction models provide complementary information for each other, and by\ncoupling them in a tight manner, we can fully leverage the potential of both\nmodels. We introduce a novel image-conditioned generative 3D Gaussian Splats\nreconstruction model that leverages the priors from 2D multi-view diffusion\nmodels, and provides an explicit 3D representation, which further guides the 2D\nreverse sampling process to have better 3D consistency. Experiments show that\nour proposed framework outperforms state-of-the-art methods and enables the\ncreation of realistic avatars from a single RGB image, achieving high-fidelity\nin both geometry and appearance. Extensive ablations also validate the efficacy\nof our design, (1) multi-view 2D priors conditioning in generative 3D\nreconstruction and (2) consistency refinement of sampling trajectory via the\nexplicit 3D representation. Our code and models will be released on\nhttps://yuxuan-xue.com/human-3diffusion.\n", "link": "http://arxiv.org/abs/2406.08475v1", "date": "2024-06-12", "relevancy": 3.4494, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7043}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.7043}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%203Diffusion%3A%20Realistic%20Avatar%20Creation%20via%20Explicit%203D%20Consistent%0A%20%20Diffusion%20Models&body=Title%3A%20Human%203Diffusion%3A%20Realistic%20Avatar%20Creation%20via%20Explicit%203D%20Consistent%0A%20%20Diffusion%20Models%0AAuthor%3A%20Yuxuan%20Xue%20and%20Xianghui%20Xie%20and%20Riccardo%20Marin%20and%20Gerard%20Pons-Moll%0AAbstract%3A%20%20%20Creating%20realistic%20avatars%20from%20a%20single%20RGB%20image%20is%20an%20attractive%20yet%0Achallenging%20problem.%20Due%20to%20its%20ill-posed%20nature%2C%20recent%20works%20leverage%0Apowerful%20prior%20from%202D%20diffusion%20models%20pretrained%20on%20large%20datasets.%20Although%0A2D%20diffusion%20models%20demonstrate%20strong%20generalization%20capability%2C%20they%20cannot%0Aprovide%20multi-view%20shape%20priors%20with%20guaranteed%203D%20consistency.%20We%20propose%0AHuman%203Diffusion%3A%20Realistic%20Avatar%20Creation%20via%20Explicit%203D%20Consistent%0ADiffusion.%20Our%20key%20insight%20is%20that%202D%20multi-view%20diffusion%20and%203D%0Areconstruction%20models%20provide%20complementary%20information%20for%20each%20other%2C%20and%20by%0Acoupling%20them%20in%20a%20tight%20manner%2C%20we%20can%20fully%20leverage%20the%20potential%20of%20both%0Amodels.%20We%20introduce%20a%20novel%20image-conditioned%20generative%203D%20Gaussian%20Splats%0Areconstruction%20model%20that%20leverages%20the%20priors%20from%202D%20multi-view%20diffusion%0Amodels%2C%20and%20provides%20an%20explicit%203D%20representation%2C%20which%20further%20guides%20the%202D%0Areverse%20sampling%20process%20to%20have%20better%203D%20consistency.%20Experiments%20show%20that%0Aour%20proposed%20framework%20outperforms%20state-of-the-art%20methods%20and%20enables%20the%0Acreation%20of%20realistic%20avatars%20from%20a%20single%20RGB%20image%2C%20achieving%20high-fidelity%0Ain%20both%20geometry%20and%20appearance.%20Extensive%20ablations%20also%20validate%20the%20efficacy%0Aof%20our%20design%2C%20%281%29%20multi-view%202D%20priors%20conditioning%20in%20generative%203D%0Areconstruction%20and%20%282%29%20consistency%20refinement%20of%20sampling%20trajectory%20via%20the%0Aexplicit%203D%20representation.%20Our%20code%20and%20models%20will%20be%20released%20on%0Ahttps%3A//yuxuan-xue.com/human-3diffusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%25203Diffusion%253A%2520Realistic%2520Avatar%2520Creation%2520via%2520Explicit%25203D%2520Consistent%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DYuxuan%2520Xue%2520and%2520Xianghui%2520Xie%2520and%2520Riccardo%2520Marin%2520and%2520Gerard%2520Pons-Moll%26entry.1292438233%3D%2520%2520Creating%2520realistic%2520avatars%2520from%2520a%2520single%2520RGB%2520image%2520is%2520an%2520attractive%2520yet%250Achallenging%2520problem.%2520Due%2520to%2520its%2520ill-posed%2520nature%252C%2520recent%2520works%2520leverage%250Apowerful%2520prior%2520from%25202D%2520diffusion%2520models%2520pretrained%2520on%2520large%2520datasets.%2520Although%250A2D%2520diffusion%2520models%2520demonstrate%2520strong%2520generalization%2520capability%252C%2520they%2520cannot%250Aprovide%2520multi-view%2520shape%2520priors%2520with%2520guaranteed%25203D%2520consistency.%2520We%2520propose%250AHuman%25203Diffusion%253A%2520Realistic%2520Avatar%2520Creation%2520via%2520Explicit%25203D%2520Consistent%250ADiffusion.%2520Our%2520key%2520insight%2520is%2520that%25202D%2520multi-view%2520diffusion%2520and%25203D%250Areconstruction%2520models%2520provide%2520complementary%2520information%2520for%2520each%2520other%252C%2520and%2520by%250Acoupling%2520them%2520in%2520a%2520tight%2520manner%252C%2520we%2520can%2520fully%2520leverage%2520the%2520potential%2520of%2520both%250Amodels.%2520We%2520introduce%2520a%2520novel%2520image-conditioned%2520generative%25203D%2520Gaussian%2520Splats%250Areconstruction%2520model%2520that%2520leverages%2520the%2520priors%2520from%25202D%2520multi-view%2520diffusion%250Amodels%252C%2520and%2520provides%2520an%2520explicit%25203D%2520representation%252C%2520which%2520further%2520guides%2520the%25202D%250Areverse%2520sampling%2520process%2520to%2520have%2520better%25203D%2520consistency.%2520Experiments%2520show%2520that%250Aour%2520proposed%2520framework%2520outperforms%2520state-of-the-art%2520methods%2520and%2520enables%2520the%250Acreation%2520of%2520realistic%2520avatars%2520from%2520a%2520single%2520RGB%2520image%252C%2520achieving%2520high-fidelity%250Ain%2520both%2520geometry%2520and%2520appearance.%2520Extensive%2520ablations%2520also%2520validate%2520the%2520efficacy%250Aof%2520our%2520design%252C%2520%25281%2529%2520multi-view%25202D%2520priors%2520conditioning%2520in%2520generative%25203D%250Areconstruction%2520and%2520%25282%2529%2520consistency%2520refinement%2520of%2520sampling%2520trajectory%2520via%2520the%250Aexplicit%25203D%2520representation.%2520Our%2520code%2520and%2520models%2520will%2520be%2520released%2520on%250Ahttps%253A//yuxuan-xue.com/human-3diffusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%203Diffusion%3A%20Realistic%20Avatar%20Creation%20via%20Explicit%203D%20Consistent%0A%20%20Diffusion%20Models&entry.906535625=Yuxuan%20Xue%20and%20Xianghui%20Xie%20and%20Riccardo%20Marin%20and%20Gerard%20Pons-Moll&entry.1292438233=%20%20Creating%20realistic%20avatars%20from%20a%20single%20RGB%20image%20is%20an%20attractive%20yet%0Achallenging%20problem.%20Due%20to%20its%20ill-posed%20nature%2C%20recent%20works%20leverage%0Apowerful%20prior%20from%202D%20diffusion%20models%20pretrained%20on%20large%20datasets.%20Although%0A2D%20diffusion%20models%20demonstrate%20strong%20generalization%20capability%2C%20they%20cannot%0Aprovide%20multi-view%20shape%20priors%20with%20guaranteed%203D%20consistency.%20We%20propose%0AHuman%203Diffusion%3A%20Realistic%20Avatar%20Creation%20via%20Explicit%203D%20Consistent%0ADiffusion.%20Our%20key%20insight%20is%20that%202D%20multi-view%20diffusion%20and%203D%0Areconstruction%20models%20provide%20complementary%20information%20for%20each%20other%2C%20and%20by%0Acoupling%20them%20in%20a%20tight%20manner%2C%20we%20can%20fully%20leverage%20the%20potential%20of%20both%0Amodels.%20We%20introduce%20a%20novel%20image-conditioned%20generative%203D%20Gaussian%20Splats%0Areconstruction%20model%20that%20leverages%20the%20priors%20from%202D%20multi-view%20diffusion%0Amodels%2C%20and%20provides%20an%20explicit%203D%20representation%2C%20which%20further%20guides%20the%202D%0Areverse%20sampling%20process%20to%20have%20better%203D%20consistency.%20Experiments%20show%20that%0Aour%20proposed%20framework%20outperforms%20state-of-the-art%20methods%20and%20enables%20the%0Acreation%20of%20realistic%20avatars%20from%20a%20single%20RGB%20image%2C%20achieving%20high-fidelity%0Ain%20both%20geometry%20and%20appearance.%20Extensive%20ablations%20also%20validate%20the%20efficacy%0Aof%20our%20design%2C%20%281%29%20multi-view%202D%20priors%20conditioning%20in%20generative%203D%0Areconstruction%20and%20%282%29%20consistency%20refinement%20of%20sampling%20trajectory%20via%20the%0Aexplicit%203D%20representation.%20Our%20code%20and%20models%20will%20be%20released%20on%0Ahttps%3A//yuxuan-xue.com/human-3diffusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08475v1&entry.124074799=Read"},
{"title": "From Chaos to Clarity: 3DGS in the Dark", "author": "Zhihao Li and Yufei Wang and Alex Kot and Bihan Wen", "abstract": "  Novel view synthesis from raw images provides superior high dynamic range\n(HDR) information compared to reconstructions from low dynamic range RGB\nimages. However, the inherent noise in unprocessed raw images compromises the\naccuracy of 3D scene representation. Our study reveals that 3D Gaussian\nSplatting (3DGS) is particularly susceptible to this noise, leading to numerous\nelongated Gaussian shapes that overfit the noise, thereby significantly\ndegrading reconstruction quality and reducing inference speed, especially in\nscenarios with limited views. To address these issues, we introduce a novel\nself-supervised learning framework designed to reconstruct HDR 3DGS from a\nlimited number of noisy raw images. This framework enhances 3DGS by integrating\na noise extractor and employing a noise-robust reconstruction loss that\nleverages a noise distribution prior. Experimental results show that our method\noutperforms LDR/HDR 3DGS and previous state-of-the-art (SOTA) self-supervised\nand supervised pre-trained models in both reconstruction quality and inference\nspeed on the RawNeRF dataset across a broad range of training views. Code can\nbe found in \\url{https://lizhihao6.github.io/Raw3DGS}.\n", "link": "http://arxiv.org/abs/2406.08300v1", "date": "2024-06-12", "relevancy": 3.234, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.721}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6321}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Chaos%20to%20Clarity%3A%203DGS%20in%20the%20Dark&body=Title%3A%20From%20Chaos%20to%20Clarity%3A%203DGS%20in%20the%20Dark%0AAuthor%3A%20Zhihao%20Li%20and%20Yufei%20Wang%20and%20Alex%20Kot%20and%20Bihan%20Wen%0AAbstract%3A%20%20%20Novel%20view%20synthesis%20from%20raw%20images%20provides%20superior%20high%20dynamic%20range%0A%28HDR%29%20information%20compared%20to%20reconstructions%20from%20low%20dynamic%20range%20RGB%0Aimages.%20However%2C%20the%20inherent%20noise%20in%20unprocessed%20raw%20images%20compromises%20the%0Aaccuracy%20of%203D%20scene%20representation.%20Our%20study%20reveals%20that%203D%20Gaussian%0ASplatting%20%283DGS%29%20is%20particularly%20susceptible%20to%20this%20noise%2C%20leading%20to%20numerous%0Aelongated%20Gaussian%20shapes%20that%20overfit%20the%20noise%2C%20thereby%20significantly%0Adegrading%20reconstruction%20quality%20and%20reducing%20inference%20speed%2C%20especially%20in%0Ascenarios%20with%20limited%20views.%20To%20address%20these%20issues%2C%20we%20introduce%20a%20novel%0Aself-supervised%20learning%20framework%20designed%20to%20reconstruct%20HDR%203DGS%20from%20a%0Alimited%20number%20of%20noisy%20raw%20images.%20This%20framework%20enhances%203DGS%20by%20integrating%0Aa%20noise%20extractor%20and%20employing%20a%20noise-robust%20reconstruction%20loss%20that%0Aleverages%20a%20noise%20distribution%20prior.%20Experimental%20results%20show%20that%20our%20method%0Aoutperforms%20LDR/HDR%203DGS%20and%20previous%20state-of-the-art%20%28SOTA%29%20self-supervised%0Aand%20supervised%20pre-trained%20models%20in%20both%20reconstruction%20quality%20and%20inference%0Aspeed%20on%20the%20RawNeRF%20dataset%20across%20a%20broad%20range%20of%20training%20views.%20Code%20can%0Abe%20found%20in%20%5Curl%7Bhttps%3A//lizhihao6.github.io/Raw3DGS%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08300v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Chaos%2520to%2520Clarity%253A%25203DGS%2520in%2520the%2520Dark%26entry.906535625%3DZhihao%2520Li%2520and%2520Yufei%2520Wang%2520and%2520Alex%2520Kot%2520and%2520Bihan%2520Wen%26entry.1292438233%3D%2520%2520Novel%2520view%2520synthesis%2520from%2520raw%2520images%2520provides%2520superior%2520high%2520dynamic%2520range%250A%2528HDR%2529%2520information%2520compared%2520to%2520reconstructions%2520from%2520low%2520dynamic%2520range%2520RGB%250Aimages.%2520However%252C%2520the%2520inherent%2520noise%2520in%2520unprocessed%2520raw%2520images%2520compromises%2520the%250Aaccuracy%2520of%25203D%2520scene%2520representation.%2520Our%2520study%2520reveals%2520that%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%2520is%2520particularly%2520susceptible%2520to%2520this%2520noise%252C%2520leading%2520to%2520numerous%250Aelongated%2520Gaussian%2520shapes%2520that%2520overfit%2520the%2520noise%252C%2520thereby%2520significantly%250Adegrading%2520reconstruction%2520quality%2520and%2520reducing%2520inference%2520speed%252C%2520especially%2520in%250Ascenarios%2520with%2520limited%2520views.%2520To%2520address%2520these%2520issues%252C%2520we%2520introduce%2520a%2520novel%250Aself-supervised%2520learning%2520framework%2520designed%2520to%2520reconstruct%2520HDR%25203DGS%2520from%2520a%250Alimited%2520number%2520of%2520noisy%2520raw%2520images.%2520This%2520framework%2520enhances%25203DGS%2520by%2520integrating%250Aa%2520noise%2520extractor%2520and%2520employing%2520a%2520noise-robust%2520reconstruction%2520loss%2520that%250Aleverages%2520a%2520noise%2520distribution%2520prior.%2520Experimental%2520results%2520show%2520that%2520our%2520method%250Aoutperforms%2520LDR/HDR%25203DGS%2520and%2520previous%2520state-of-the-art%2520%2528SOTA%2529%2520self-supervised%250Aand%2520supervised%2520pre-trained%2520models%2520in%2520both%2520reconstruction%2520quality%2520and%2520inference%250Aspeed%2520on%2520the%2520RawNeRF%2520dataset%2520across%2520a%2520broad%2520range%2520of%2520training%2520views.%2520Code%2520can%250Abe%2520found%2520in%2520%255Curl%257Bhttps%253A//lizhihao6.github.io/Raw3DGS%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08300v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Chaos%20to%20Clarity%3A%203DGS%20in%20the%20Dark&entry.906535625=Zhihao%20Li%20and%20Yufei%20Wang%20and%20Alex%20Kot%20and%20Bihan%20Wen&entry.1292438233=%20%20Novel%20view%20synthesis%20from%20raw%20images%20provides%20superior%20high%20dynamic%20range%0A%28HDR%29%20information%20compared%20to%20reconstructions%20from%20low%20dynamic%20range%20RGB%0Aimages.%20However%2C%20the%20inherent%20noise%20in%20unprocessed%20raw%20images%20compromises%20the%0Aaccuracy%20of%203D%20scene%20representation.%20Our%20study%20reveals%20that%203D%20Gaussian%0ASplatting%20%283DGS%29%20is%20particularly%20susceptible%20to%20this%20noise%2C%20leading%20to%20numerous%0Aelongated%20Gaussian%20shapes%20that%20overfit%20the%20noise%2C%20thereby%20significantly%0Adegrading%20reconstruction%20quality%20and%20reducing%20inference%20speed%2C%20especially%20in%0Ascenarios%20with%20limited%20views.%20To%20address%20these%20issues%2C%20we%20introduce%20a%20novel%0Aself-supervised%20learning%20framework%20designed%20to%20reconstruct%20HDR%203DGS%20from%20a%0Alimited%20number%20of%20noisy%20raw%20images.%20This%20framework%20enhances%203DGS%20by%20integrating%0Aa%20noise%20extractor%20and%20employing%20a%20noise-robust%20reconstruction%20loss%20that%0Aleverages%20a%20noise%20distribution%20prior.%20Experimental%20results%20show%20that%20our%20method%0Aoutperforms%20LDR/HDR%203DGS%20and%20previous%20state-of-the-art%20%28SOTA%29%20self-supervised%0Aand%20supervised%20pre-trained%20models%20in%20both%20reconstruction%20quality%20and%20inference%0Aspeed%20on%20the%20RawNeRF%20dataset%20across%20a%20broad%20range%20of%20training%20views.%20Code%20can%0Abe%20found%20in%20%5Curl%7Bhttps%3A//lizhihao6.github.io/Raw3DGS%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08300v1&entry.124074799=Read"},
{"title": "Self-supervised Learning of Neural Implicit Feature Fields for Camera\n  Pose Refinement", "author": "Maxime Pietrantoni and Gabriela Csurka and Martin Humenberger and Torsten Sattler", "abstract": "  Visual localization techniques rely upon some underlying scene representation\nto localize against. These representations can be explicit such as 3D SFM map\nor implicit, such as a neural network that learns to encode the scene. The\nformer requires sparse feature extractors and matchers to build the scene\nrepresentation. The latter might lack geometric grounding not capturing the 3D\nstructure of the scene well enough. This paper proposes to jointly learn the\nscene representation along with a 3D dense feature field and a 2D feature\nextractor whose outputs are embedded in the same metric space. Through a\ncontrastive framework we align this volumetric field with the image-based\nextractor and regularize the latter with a ranking loss from learned surface\ninformation. We learn the underlying geometry of the scene with an implicit\nfield through volumetric rendering and design our feature field to leverage\nintermediate geometric information encoded in the implicit field. The resulting\nfeatures are discriminative and robust to viewpoint change while maintaining\nrich encoded information. Visual localization is then achieved by aligning the\nimage-based features and the rendered volumetric features. We show the\neffectiveness of our approach on real-world scenes, demonstrating that our\napproach outperforms prior and concurrent work on leveraging implicit scene\nrepresentations for localization.\n", "link": "http://arxiv.org/abs/2406.08463v1", "date": "2024-06-12", "relevancy": 3.2109, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6844}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6216}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20Learning%20of%20Neural%20Implicit%20Feature%20Fields%20for%20Camera%0A%20%20Pose%20Refinement&body=Title%3A%20Self-supervised%20Learning%20of%20Neural%20Implicit%20Feature%20Fields%20for%20Camera%0A%20%20Pose%20Refinement%0AAuthor%3A%20Maxime%20Pietrantoni%20and%20Gabriela%20Csurka%20and%20Martin%20Humenberger%20and%20Torsten%20Sattler%0AAbstract%3A%20%20%20Visual%20localization%20techniques%20rely%20upon%20some%20underlying%20scene%20representation%0Ato%20localize%20against.%20These%20representations%20can%20be%20explicit%20such%20as%203D%20SFM%20map%0Aor%20implicit%2C%20such%20as%20a%20neural%20network%20that%20learns%20to%20encode%20the%20scene.%20The%0Aformer%20requires%20sparse%20feature%20extractors%20and%20matchers%20to%20build%20the%20scene%0Arepresentation.%20The%20latter%20might%20lack%20geometric%20grounding%20not%20capturing%20the%203D%0Astructure%20of%20the%20scene%20well%20enough.%20This%20paper%20proposes%20to%20jointly%20learn%20the%0Ascene%20representation%20along%20with%20a%203D%20dense%20feature%20field%20and%20a%202D%20feature%0Aextractor%20whose%20outputs%20are%20embedded%20in%20the%20same%20metric%20space.%20Through%20a%0Acontrastive%20framework%20we%20align%20this%20volumetric%20field%20with%20the%20image-based%0Aextractor%20and%20regularize%20the%20latter%20with%20a%20ranking%20loss%20from%20learned%20surface%0Ainformation.%20We%20learn%20the%20underlying%20geometry%20of%20the%20scene%20with%20an%20implicit%0Afield%20through%20volumetric%20rendering%20and%20design%20our%20feature%20field%20to%20leverage%0Aintermediate%20geometric%20information%20encoded%20in%20the%20implicit%20field.%20The%20resulting%0Afeatures%20are%20discriminative%20and%20robust%20to%20viewpoint%20change%20while%20maintaining%0Arich%20encoded%20information.%20Visual%20localization%20is%20then%20achieved%20by%20aligning%20the%0Aimage-based%20features%20and%20the%20rendered%20volumetric%20features.%20We%20show%20the%0Aeffectiveness%20of%20our%20approach%20on%20real-world%20scenes%2C%20demonstrating%20that%20our%0Aapproach%20outperforms%20prior%20and%20concurrent%20work%20on%20leveraging%20implicit%20scene%0Arepresentations%20for%20localization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08463v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520Learning%2520of%2520Neural%2520Implicit%2520Feature%2520Fields%2520for%2520Camera%250A%2520%2520Pose%2520Refinement%26entry.906535625%3DMaxime%2520Pietrantoni%2520and%2520Gabriela%2520Csurka%2520and%2520Martin%2520Humenberger%2520and%2520Torsten%2520Sattler%26entry.1292438233%3D%2520%2520Visual%2520localization%2520techniques%2520rely%2520upon%2520some%2520underlying%2520scene%2520representation%250Ato%2520localize%2520against.%2520These%2520representations%2520can%2520be%2520explicit%2520such%2520as%25203D%2520SFM%2520map%250Aor%2520implicit%252C%2520such%2520as%2520a%2520neural%2520network%2520that%2520learns%2520to%2520encode%2520the%2520scene.%2520The%250Aformer%2520requires%2520sparse%2520feature%2520extractors%2520and%2520matchers%2520to%2520build%2520the%2520scene%250Arepresentation.%2520The%2520latter%2520might%2520lack%2520geometric%2520grounding%2520not%2520capturing%2520the%25203D%250Astructure%2520of%2520the%2520scene%2520well%2520enough.%2520This%2520paper%2520proposes%2520to%2520jointly%2520learn%2520the%250Ascene%2520representation%2520along%2520with%2520a%25203D%2520dense%2520feature%2520field%2520and%2520a%25202D%2520feature%250Aextractor%2520whose%2520outputs%2520are%2520embedded%2520in%2520the%2520same%2520metric%2520space.%2520Through%2520a%250Acontrastive%2520framework%2520we%2520align%2520this%2520volumetric%2520field%2520with%2520the%2520image-based%250Aextractor%2520and%2520regularize%2520the%2520latter%2520with%2520a%2520ranking%2520loss%2520from%2520learned%2520surface%250Ainformation.%2520We%2520learn%2520the%2520underlying%2520geometry%2520of%2520the%2520scene%2520with%2520an%2520implicit%250Afield%2520through%2520volumetric%2520rendering%2520and%2520design%2520our%2520feature%2520field%2520to%2520leverage%250Aintermediate%2520geometric%2520information%2520encoded%2520in%2520the%2520implicit%2520field.%2520The%2520resulting%250Afeatures%2520are%2520discriminative%2520and%2520robust%2520to%2520viewpoint%2520change%2520while%2520maintaining%250Arich%2520encoded%2520information.%2520Visual%2520localization%2520is%2520then%2520achieved%2520by%2520aligning%2520the%250Aimage-based%2520features%2520and%2520the%2520rendered%2520volumetric%2520features.%2520We%2520show%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520on%2520real-world%2520scenes%252C%2520demonstrating%2520that%2520our%250Aapproach%2520outperforms%2520prior%2520and%2520concurrent%2520work%2520on%2520leveraging%2520implicit%2520scene%250Arepresentations%2520for%2520localization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08463v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20Learning%20of%20Neural%20Implicit%20Feature%20Fields%20for%20Camera%0A%20%20Pose%20Refinement&entry.906535625=Maxime%20Pietrantoni%20and%20Gabriela%20Csurka%20and%20Martin%20Humenberger%20and%20Torsten%20Sattler&entry.1292438233=%20%20Visual%20localization%20techniques%20rely%20upon%20some%20underlying%20scene%20representation%0Ato%20localize%20against.%20These%20representations%20can%20be%20explicit%20such%20as%203D%20SFM%20map%0Aor%20implicit%2C%20such%20as%20a%20neural%20network%20that%20learns%20to%20encode%20the%20scene.%20The%0Aformer%20requires%20sparse%20feature%20extractors%20and%20matchers%20to%20build%20the%20scene%0Arepresentation.%20The%20latter%20might%20lack%20geometric%20grounding%20not%20capturing%20the%203D%0Astructure%20of%20the%20scene%20well%20enough.%20This%20paper%20proposes%20to%20jointly%20learn%20the%0Ascene%20representation%20along%20with%20a%203D%20dense%20feature%20field%20and%20a%202D%20feature%0Aextractor%20whose%20outputs%20are%20embedded%20in%20the%20same%20metric%20space.%20Through%20a%0Acontrastive%20framework%20we%20align%20this%20volumetric%20field%20with%20the%20image-based%0Aextractor%20and%20regularize%20the%20latter%20with%20a%20ranking%20loss%20from%20learned%20surface%0Ainformation.%20We%20learn%20the%20underlying%20geometry%20of%20the%20scene%20with%20an%20implicit%0Afield%20through%20volumetric%20rendering%20and%20design%20our%20feature%20field%20to%20leverage%0Aintermediate%20geometric%20information%20encoded%20in%20the%20implicit%20field.%20The%20resulting%0Afeatures%20are%20discriminative%20and%20robust%20to%20viewpoint%20change%20while%20maintaining%0Arich%20encoded%20information.%20Visual%20localization%20is%20then%20achieved%20by%20aligning%20the%0Aimage-based%20features%20and%20the%20rendered%20volumetric%20features.%20We%20show%20the%0Aeffectiveness%20of%20our%20approach%20on%20real-world%20scenes%2C%20demonstrating%20that%20our%0Aapproach%20outperforms%20prior%20and%20concurrent%20work%20on%20leveraging%20implicit%20scene%0Arepresentations%20for%20localization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08463v1&entry.124074799=Read"},
{"title": "Real3D: Scaling Up Large Reconstruction Models with Real-World Images", "author": "Hanwen Jiang and Qixing Huang and Georgios Pavlakos", "abstract": "  The default strategy for training single-view Large Reconstruction Models\n(LRMs) follows the fully supervised route using large-scale datasets of\nsynthetic 3D assets or multi-view captures. Although these resources simplify\nthe training procedure, they are hard to scale up beyond the existing datasets\nand they are not necessarily representative of the real distribution of object\nshapes. To address these limitations, in this paper, we introduce Real3D, the\nfirst LRM system that can be trained using single-view real-world images.\nReal3D introduces a novel self-training framework that can benefit from both\nthe existing synthetic data and diverse single-view real images. We propose two\nunsupervised losses that allow us to supervise LRMs at the pixel- and\nsemantic-level, even for training examples without ground-truth 3D or novel\nviews. To further improve performance and scale up the image data, we develop\nan automatic data curation approach to collect high-quality examples from\nin-the-wild images. Our experiments show that Real3D consistently outperforms\nprior work in four diverse evaluation settings that include real and synthetic\ndata, as well as both in-domain and out-of-domain shapes. Code and model can be\nfound here: https://hwjiang1510.github.io/Real3D/\n", "link": "http://arxiv.org/abs/2406.08479v1", "date": "2024-06-12", "relevancy": 3.1322, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.656}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6117}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real3D%3A%20Scaling%20Up%20Large%20Reconstruction%20Models%20with%20Real-World%20Images&body=Title%3A%20Real3D%3A%20Scaling%20Up%20Large%20Reconstruction%20Models%20with%20Real-World%20Images%0AAuthor%3A%20Hanwen%20Jiang%20and%20Qixing%20Huang%20and%20Georgios%20Pavlakos%0AAbstract%3A%20%20%20The%20default%20strategy%20for%20training%20single-view%20Large%20Reconstruction%20Models%0A%28LRMs%29%20follows%20the%20fully%20supervised%20route%20using%20large-scale%20datasets%20of%0Asynthetic%203D%20assets%20or%20multi-view%20captures.%20Although%20these%20resources%20simplify%0Athe%20training%20procedure%2C%20they%20are%20hard%20to%20scale%20up%20beyond%20the%20existing%20datasets%0Aand%20they%20are%20not%20necessarily%20representative%20of%20the%20real%20distribution%20of%20object%0Ashapes.%20To%20address%20these%20limitations%2C%20in%20this%20paper%2C%20we%20introduce%20Real3D%2C%20the%0Afirst%20LRM%20system%20that%20can%20be%20trained%20using%20single-view%20real-world%20images.%0AReal3D%20introduces%20a%20novel%20self-training%20framework%20that%20can%20benefit%20from%20both%0Athe%20existing%20synthetic%20data%20and%20diverse%20single-view%20real%20images.%20We%20propose%20two%0Aunsupervised%20losses%20that%20allow%20us%20to%20supervise%20LRMs%20at%20the%20pixel-%20and%0Asemantic-level%2C%20even%20for%20training%20examples%20without%20ground-truth%203D%20or%20novel%0Aviews.%20To%20further%20improve%20performance%20and%20scale%20up%20the%20image%20data%2C%20we%20develop%0Aan%20automatic%20data%20curation%20approach%20to%20collect%20high-quality%20examples%20from%0Ain-the-wild%20images.%20Our%20experiments%20show%20that%20Real3D%20consistently%20outperforms%0Aprior%20work%20in%20four%20diverse%20evaluation%20settings%20that%20include%20real%20and%20synthetic%0Adata%2C%20as%20well%20as%20both%20in-domain%20and%20out-of-domain%20shapes.%20Code%20and%20model%20can%20be%0Afound%20here%3A%20https%3A//hwjiang1510.github.io/Real3D/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08479v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal3D%253A%2520Scaling%2520Up%2520Large%2520Reconstruction%2520Models%2520with%2520Real-World%2520Images%26entry.906535625%3DHanwen%2520Jiang%2520and%2520Qixing%2520Huang%2520and%2520Georgios%2520Pavlakos%26entry.1292438233%3D%2520%2520The%2520default%2520strategy%2520for%2520training%2520single-view%2520Large%2520Reconstruction%2520Models%250A%2528LRMs%2529%2520follows%2520the%2520fully%2520supervised%2520route%2520using%2520large-scale%2520datasets%2520of%250Asynthetic%25203D%2520assets%2520or%2520multi-view%2520captures.%2520Although%2520these%2520resources%2520simplify%250Athe%2520training%2520procedure%252C%2520they%2520are%2520hard%2520to%2520scale%2520up%2520beyond%2520the%2520existing%2520datasets%250Aand%2520they%2520are%2520not%2520necessarily%2520representative%2520of%2520the%2520real%2520distribution%2520of%2520object%250Ashapes.%2520To%2520address%2520these%2520limitations%252C%2520in%2520this%2520paper%252C%2520we%2520introduce%2520Real3D%252C%2520the%250Afirst%2520LRM%2520system%2520that%2520can%2520be%2520trained%2520using%2520single-view%2520real-world%2520images.%250AReal3D%2520introduces%2520a%2520novel%2520self-training%2520framework%2520that%2520can%2520benefit%2520from%2520both%250Athe%2520existing%2520synthetic%2520data%2520and%2520diverse%2520single-view%2520real%2520images.%2520We%2520propose%2520two%250Aunsupervised%2520losses%2520that%2520allow%2520us%2520to%2520supervise%2520LRMs%2520at%2520the%2520pixel-%2520and%250Asemantic-level%252C%2520even%2520for%2520training%2520examples%2520without%2520ground-truth%25203D%2520or%2520novel%250Aviews.%2520To%2520further%2520improve%2520performance%2520and%2520scale%2520up%2520the%2520image%2520data%252C%2520we%2520develop%250Aan%2520automatic%2520data%2520curation%2520approach%2520to%2520collect%2520high-quality%2520examples%2520from%250Ain-the-wild%2520images.%2520Our%2520experiments%2520show%2520that%2520Real3D%2520consistently%2520outperforms%250Aprior%2520work%2520in%2520four%2520diverse%2520evaluation%2520settings%2520that%2520include%2520real%2520and%2520synthetic%250Adata%252C%2520as%2520well%2520as%2520both%2520in-domain%2520and%2520out-of-domain%2520shapes.%2520Code%2520and%2520model%2520can%2520be%250Afound%2520here%253A%2520https%253A//hwjiang1510.github.io/Real3D/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08479v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real3D%3A%20Scaling%20Up%20Large%20Reconstruction%20Models%20with%20Real-World%20Images&entry.906535625=Hanwen%20Jiang%20and%20Qixing%20Huang%20and%20Georgios%20Pavlakos&entry.1292438233=%20%20The%20default%20strategy%20for%20training%20single-view%20Large%20Reconstruction%20Models%0A%28LRMs%29%20follows%20the%20fully%20supervised%20route%20using%20large-scale%20datasets%20of%0Asynthetic%203D%20assets%20or%20multi-view%20captures.%20Although%20these%20resources%20simplify%0Athe%20training%20procedure%2C%20they%20are%20hard%20to%20scale%20up%20beyond%20the%20existing%20datasets%0Aand%20they%20are%20not%20necessarily%20representative%20of%20the%20real%20distribution%20of%20object%0Ashapes.%20To%20address%20these%20limitations%2C%20in%20this%20paper%2C%20we%20introduce%20Real3D%2C%20the%0Afirst%20LRM%20system%20that%20can%20be%20trained%20using%20single-view%20real-world%20images.%0AReal3D%20introduces%20a%20novel%20self-training%20framework%20that%20can%20benefit%20from%20both%0Athe%20existing%20synthetic%20data%20and%20diverse%20single-view%20real%20images.%20We%20propose%20two%0Aunsupervised%20losses%20that%20allow%20us%20to%20supervise%20LRMs%20at%20the%20pixel-%20and%0Asemantic-level%2C%20even%20for%20training%20examples%20without%20ground-truth%203D%20or%20novel%0Aviews.%20To%20further%20improve%20performance%20and%20scale%20up%20the%20image%20data%2C%20we%20develop%0Aan%20automatic%20data%20curation%20approach%20to%20collect%20high-quality%20examples%20from%0Ain-the-wild%20images.%20Our%20experiments%20show%20that%20Real3D%20consistently%20outperforms%0Aprior%20work%20in%20four%20diverse%20evaluation%20settings%20that%20include%20real%20and%20synthetic%0Adata%2C%20as%20well%20as%20both%20in-domain%20and%20out-of-domain%20shapes.%20Code%20and%20model%20can%20be%0Afound%20here%3A%20https%3A//hwjiang1510.github.io/Real3D/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08479v1&entry.124074799=Read"},
{"title": "ICE-G: Image Conditional Editing of 3D Gaussian Splats", "author": "Vishnu Jaganathan and Hannah Hanyun Huang and Muhammad Zubair Irshad and Varun Jampani and Amit Raj and Zsolt Kira", "abstract": "  Recently many techniques have emerged to create high quality 3D assets and\nscenes. When it comes to editing of these objects, however, existing approaches\nare either slow, compromise on quality, or do not provide enough customization.\nWe introduce a novel approach to quickly edit a 3D model from a single\nreference view. Our technique first segments the edit image, and then matches\nsemantically corresponding regions across chosen segmented dataset views using\nDINO features. A color or texture change from a particular region of the edit\nimage can then be applied to other views automatically in a semantically\nsensible manner. These edited views act as an updated dataset to further train\nand re-style the 3D scene. The end-result is therefore an edited 3D model. Our\nframework enables a wide variety of editing tasks such as manual local edits,\ncorrespondence based style transfer from any example image, and a combination\nof different styles from multiple example images. We use Gaussian Splats as our\nprimary 3D representation due to their speed and ease of local editing, but our\ntechnique works for other methods such as NeRFs as well. We show through\nmultiple examples that our method produces higher quality results while\noffering fine-grained control of editing. Project page: ice-gaussian.github.io\n", "link": "http://arxiv.org/abs/2406.08488v1", "date": "2024-06-12", "relevancy": 3.079, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6297}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6144}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ICE-G%3A%20Image%20Conditional%20Editing%20of%203D%20Gaussian%20Splats&body=Title%3A%20ICE-G%3A%20Image%20Conditional%20Editing%20of%203D%20Gaussian%20Splats%0AAuthor%3A%20Vishnu%20Jaganathan%20and%20Hannah%20Hanyun%20Huang%20and%20Muhammad%20Zubair%20Irshad%20and%20Varun%20Jampani%20and%20Amit%20Raj%20and%20Zsolt%20Kira%0AAbstract%3A%20%20%20Recently%20many%20techniques%20have%20emerged%20to%20create%20high%20quality%203D%20assets%20and%0Ascenes.%20When%20it%20comes%20to%20editing%20of%20these%20objects%2C%20however%2C%20existing%20approaches%0Aare%20either%20slow%2C%20compromise%20on%20quality%2C%20or%20do%20not%20provide%20enough%20customization.%0AWe%20introduce%20a%20novel%20approach%20to%20quickly%20edit%20a%203D%20model%20from%20a%20single%0Areference%20view.%20Our%20technique%20first%20segments%20the%20edit%20image%2C%20and%20then%20matches%0Asemantically%20corresponding%20regions%20across%20chosen%20segmented%20dataset%20views%20using%0ADINO%20features.%20A%20color%20or%20texture%20change%20from%20a%20particular%20region%20of%20the%20edit%0Aimage%20can%20then%20be%20applied%20to%20other%20views%20automatically%20in%20a%20semantically%0Asensible%20manner.%20These%20edited%20views%20act%20as%20an%20updated%20dataset%20to%20further%20train%0Aand%20re-style%20the%203D%20scene.%20The%20end-result%20is%20therefore%20an%20edited%203D%20model.%20Our%0Aframework%20enables%20a%20wide%20variety%20of%20editing%20tasks%20such%20as%20manual%20local%20edits%2C%0Acorrespondence%20based%20style%20transfer%20from%20any%20example%20image%2C%20and%20a%20combination%0Aof%20different%20styles%20from%20multiple%20example%20images.%20We%20use%20Gaussian%20Splats%20as%20our%0Aprimary%203D%20representation%20due%20to%20their%20speed%20and%20ease%20of%20local%20editing%2C%20but%20our%0Atechnique%20works%20for%20other%20methods%20such%20as%20NeRFs%20as%20well.%20We%20show%20through%0Amultiple%20examples%20that%20our%20method%20produces%20higher%20quality%20results%20while%0Aoffering%20fine-grained%20control%20of%20editing.%20Project%20page%3A%20ice-gaussian.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DICE-G%253A%2520Image%2520Conditional%2520Editing%2520of%25203D%2520Gaussian%2520Splats%26entry.906535625%3DVishnu%2520Jaganathan%2520and%2520Hannah%2520Hanyun%2520Huang%2520and%2520Muhammad%2520Zubair%2520Irshad%2520and%2520Varun%2520Jampani%2520and%2520Amit%2520Raj%2520and%2520Zsolt%2520Kira%26entry.1292438233%3D%2520%2520Recently%2520many%2520techniques%2520have%2520emerged%2520to%2520create%2520high%2520quality%25203D%2520assets%2520and%250Ascenes.%2520When%2520it%2520comes%2520to%2520editing%2520of%2520these%2520objects%252C%2520however%252C%2520existing%2520approaches%250Aare%2520either%2520slow%252C%2520compromise%2520on%2520quality%252C%2520or%2520do%2520not%2520provide%2520enough%2520customization.%250AWe%2520introduce%2520a%2520novel%2520approach%2520to%2520quickly%2520edit%2520a%25203D%2520model%2520from%2520a%2520single%250Areference%2520view.%2520Our%2520technique%2520first%2520segments%2520the%2520edit%2520image%252C%2520and%2520then%2520matches%250Asemantically%2520corresponding%2520regions%2520across%2520chosen%2520segmented%2520dataset%2520views%2520using%250ADINO%2520features.%2520A%2520color%2520or%2520texture%2520change%2520from%2520a%2520particular%2520region%2520of%2520the%2520edit%250Aimage%2520can%2520then%2520be%2520applied%2520to%2520other%2520views%2520automatically%2520in%2520a%2520semantically%250Asensible%2520manner.%2520These%2520edited%2520views%2520act%2520as%2520an%2520updated%2520dataset%2520to%2520further%2520train%250Aand%2520re-style%2520the%25203D%2520scene.%2520The%2520end-result%2520is%2520therefore%2520an%2520edited%25203D%2520model.%2520Our%250Aframework%2520enables%2520a%2520wide%2520variety%2520of%2520editing%2520tasks%2520such%2520as%2520manual%2520local%2520edits%252C%250Acorrespondence%2520based%2520style%2520transfer%2520from%2520any%2520example%2520image%252C%2520and%2520a%2520combination%250Aof%2520different%2520styles%2520from%2520multiple%2520example%2520images.%2520We%2520use%2520Gaussian%2520Splats%2520as%2520our%250Aprimary%25203D%2520representation%2520due%2520to%2520their%2520speed%2520and%2520ease%2520of%2520local%2520editing%252C%2520but%2520our%250Atechnique%2520works%2520for%2520other%2520methods%2520such%2520as%2520NeRFs%2520as%2520well.%2520We%2520show%2520through%250Amultiple%2520examples%2520that%2520our%2520method%2520produces%2520higher%2520quality%2520results%2520while%250Aoffering%2520fine-grained%2520control%2520of%2520editing.%2520Project%2520page%253A%2520ice-gaussian.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ICE-G%3A%20Image%20Conditional%20Editing%20of%203D%20Gaussian%20Splats&entry.906535625=Vishnu%20Jaganathan%20and%20Hannah%20Hanyun%20Huang%20and%20Muhammad%20Zubair%20Irshad%20and%20Varun%20Jampani%20and%20Amit%20Raj%20and%20Zsolt%20Kira&entry.1292438233=%20%20Recently%20many%20techniques%20have%20emerged%20to%20create%20high%20quality%203D%20assets%20and%0Ascenes.%20When%20it%20comes%20to%20editing%20of%20these%20objects%2C%20however%2C%20existing%20approaches%0Aare%20either%20slow%2C%20compromise%20on%20quality%2C%20or%20do%20not%20provide%20enough%20customization.%0AWe%20introduce%20a%20novel%20approach%20to%20quickly%20edit%20a%203D%20model%20from%20a%20single%0Areference%20view.%20Our%20technique%20first%20segments%20the%20edit%20image%2C%20and%20then%20matches%0Asemantically%20corresponding%20regions%20across%20chosen%20segmented%20dataset%20views%20using%0ADINO%20features.%20A%20color%20or%20texture%20change%20from%20a%20particular%20region%20of%20the%20edit%0Aimage%20can%20then%20be%20applied%20to%20other%20views%20automatically%20in%20a%20semantically%0Asensible%20manner.%20These%20edited%20views%20act%20as%20an%20updated%20dataset%20to%20further%20train%0Aand%20re-style%20the%203D%20scene.%20The%20end-result%20is%20therefore%20an%20edited%203D%20model.%20Our%0Aframework%20enables%20a%20wide%20variety%20of%20editing%20tasks%20such%20as%20manual%20local%20edits%2C%0Acorrespondence%20based%20style%20transfer%20from%20any%20example%20image%2C%20and%20a%20combination%0Aof%20different%20styles%20from%20multiple%20example%20images.%20We%20use%20Gaussian%20Splats%20as%20our%0Aprimary%203D%20representation%20due%20to%20their%20speed%20and%20ease%20of%20local%20editing%2C%20but%20our%0Atechnique%20works%20for%20other%20methods%20such%20as%20NeRFs%20as%20well.%20We%20show%20through%0Amultiple%20examples%20that%20our%20method%20produces%20higher%20quality%20results%20while%0Aoffering%20fine-grained%20control%20of%20editing.%20Project%20page%3A%20ice-gaussian.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08488v1&entry.124074799=Read"},
{"title": "2.5D Multi-view Averaging Diffusion Model for 3D Medical Image\n  Translation: Application to Low-count PET Reconstruction with CT-less\n  Attenuation Correction", "author": "Tianqi Chen and Jun Hou and Yinchi Zhou and Huidong Xie and Xiongchao Chen and Qiong Liu and Xueqi Guo and Menghua Xia and James S. Duncan and Chi Liu and Bo Zhou", "abstract": "  Positron Emission Tomography (PET) is an important clinical imaging tool but\ninevitably introduces radiation hazards to patients and healthcare providers.\nReducing the tracer injection dose and eliminating the CT acquisition for\nattenuation correction can reduce the overall radiation dose, but often results\nin PET with high noise and bias. Thus, it is desirable to develop 3D methods to\ntranslate the non-attenuation-corrected low-dose PET (NAC-LDPET) into\nattenuation-corrected standard-dose PET (AC-SDPET). Recently, diffusion models\nhave emerged as a new state-of-the-art deep learning method for image-to-image\ntranslation, better than traditional CNN-based methods. However, due to the\nhigh computation cost and memory burden, it is largely limited to 2D\napplications. To address these challenges, we developed a novel 2.5D Multi-view\nAveraging Diffusion Model (MADM) for 3D image-to-image translation with\napplication on NAC-LDPET to AC-SDPET translation. Specifically, MADM employs\nseparate diffusion models for axial, coronal, and sagittal views, whose outputs\nare averaged in each sampling step to ensure the 3D generation quality from\nmultiple views. To accelerate the 3D sampling process, we also proposed a\nstrategy to use the CNN-based 3D generation as a prior for the diffusion model.\nOur experimental results on human patient studies suggested that MADM can\ngenerate high-quality 3D translation images, outperforming previous CNN-based\nand Diffusion-based baseline methods.\n", "link": "http://arxiv.org/abs/2406.08374v1", "date": "2024-06-12", "relevancy": 2.7258, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6872}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6872}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%202.5D%20Multi-view%20Averaging%20Diffusion%20Model%20for%203D%20Medical%20Image%0A%20%20Translation%3A%20Application%20to%20Low-count%20PET%20Reconstruction%20with%20CT-less%0A%20%20Attenuation%20Correction&body=Title%3A%202.5D%20Multi-view%20Averaging%20Diffusion%20Model%20for%203D%20Medical%20Image%0A%20%20Translation%3A%20Application%20to%20Low-count%20PET%20Reconstruction%20with%20CT-less%0A%20%20Attenuation%20Correction%0AAuthor%3A%20Tianqi%20Chen%20and%20Jun%20Hou%20and%20Yinchi%20Zhou%20and%20Huidong%20Xie%20and%20Xiongchao%20Chen%20and%20Qiong%20Liu%20and%20Xueqi%20Guo%20and%20Menghua%20Xia%20and%20James%20S.%20Duncan%20and%20Chi%20Liu%20and%20Bo%20Zhou%0AAbstract%3A%20%20%20Positron%20Emission%20Tomography%20%28PET%29%20is%20an%20important%20clinical%20imaging%20tool%20but%0Ainevitably%20introduces%20radiation%20hazards%20to%20patients%20and%20healthcare%20providers.%0AReducing%20the%20tracer%20injection%20dose%20and%20eliminating%20the%20CT%20acquisition%20for%0Aattenuation%20correction%20can%20reduce%20the%20overall%20radiation%20dose%2C%20but%20often%20results%0Ain%20PET%20with%20high%20noise%20and%20bias.%20Thus%2C%20it%20is%20desirable%20to%20develop%203D%20methods%20to%0Atranslate%20the%20non-attenuation-corrected%20low-dose%20PET%20%28NAC-LDPET%29%20into%0Aattenuation-corrected%20standard-dose%20PET%20%28AC-SDPET%29.%20Recently%2C%20diffusion%20models%0Ahave%20emerged%20as%20a%20new%20state-of-the-art%20deep%20learning%20method%20for%20image-to-image%0Atranslation%2C%20better%20than%20traditional%20CNN-based%20methods.%20However%2C%20due%20to%20the%0Ahigh%20computation%20cost%20and%20memory%20burden%2C%20it%20is%20largely%20limited%20to%202D%0Aapplications.%20To%20address%20these%20challenges%2C%20we%20developed%20a%20novel%202.5D%20Multi-view%0AAveraging%20Diffusion%20Model%20%28MADM%29%20for%203D%20image-to-image%20translation%20with%0Aapplication%20on%20NAC-LDPET%20to%20AC-SDPET%20translation.%20Specifically%2C%20MADM%20employs%0Aseparate%20diffusion%20models%20for%20axial%2C%20coronal%2C%20and%20sagittal%20views%2C%20whose%20outputs%0Aare%20averaged%20in%20each%20sampling%20step%20to%20ensure%20the%203D%20generation%20quality%20from%0Amultiple%20views.%20To%20accelerate%20the%203D%20sampling%20process%2C%20we%20also%20proposed%20a%0Astrategy%20to%20use%20the%20CNN-based%203D%20generation%20as%20a%20prior%20for%20the%20diffusion%20model.%0AOur%20experimental%20results%20on%20human%20patient%20studies%20suggested%20that%20MADM%20can%0Agenerate%20high-quality%203D%20translation%20images%2C%20outperforming%20previous%20CNN-based%0Aand%20Diffusion-based%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08374v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D2.5D%2520Multi-view%2520Averaging%2520Diffusion%2520Model%2520for%25203D%2520Medical%2520Image%250A%2520%2520Translation%253A%2520Application%2520to%2520Low-count%2520PET%2520Reconstruction%2520with%2520CT-less%250A%2520%2520Attenuation%2520Correction%26entry.906535625%3DTianqi%2520Chen%2520and%2520Jun%2520Hou%2520and%2520Yinchi%2520Zhou%2520and%2520Huidong%2520Xie%2520and%2520Xiongchao%2520Chen%2520and%2520Qiong%2520Liu%2520and%2520Xueqi%2520Guo%2520and%2520Menghua%2520Xia%2520and%2520James%2520S.%2520Duncan%2520and%2520Chi%2520Liu%2520and%2520Bo%2520Zhou%26entry.1292438233%3D%2520%2520Positron%2520Emission%2520Tomography%2520%2528PET%2529%2520is%2520an%2520important%2520clinical%2520imaging%2520tool%2520but%250Ainevitably%2520introduces%2520radiation%2520hazards%2520to%2520patients%2520and%2520healthcare%2520providers.%250AReducing%2520the%2520tracer%2520injection%2520dose%2520and%2520eliminating%2520the%2520CT%2520acquisition%2520for%250Aattenuation%2520correction%2520can%2520reduce%2520the%2520overall%2520radiation%2520dose%252C%2520but%2520often%2520results%250Ain%2520PET%2520with%2520high%2520noise%2520and%2520bias.%2520Thus%252C%2520it%2520is%2520desirable%2520to%2520develop%25203D%2520methods%2520to%250Atranslate%2520the%2520non-attenuation-corrected%2520low-dose%2520PET%2520%2528NAC-LDPET%2529%2520into%250Aattenuation-corrected%2520standard-dose%2520PET%2520%2528AC-SDPET%2529.%2520Recently%252C%2520diffusion%2520models%250Ahave%2520emerged%2520as%2520a%2520new%2520state-of-the-art%2520deep%2520learning%2520method%2520for%2520image-to-image%250Atranslation%252C%2520better%2520than%2520traditional%2520CNN-based%2520methods.%2520However%252C%2520due%2520to%2520the%250Ahigh%2520computation%2520cost%2520and%2520memory%2520burden%252C%2520it%2520is%2520largely%2520limited%2520to%25202D%250Aapplications.%2520To%2520address%2520these%2520challenges%252C%2520we%2520developed%2520a%2520novel%25202.5D%2520Multi-view%250AAveraging%2520Diffusion%2520Model%2520%2528MADM%2529%2520for%25203D%2520image-to-image%2520translation%2520with%250Aapplication%2520on%2520NAC-LDPET%2520to%2520AC-SDPET%2520translation.%2520Specifically%252C%2520MADM%2520employs%250Aseparate%2520diffusion%2520models%2520for%2520axial%252C%2520coronal%252C%2520and%2520sagittal%2520views%252C%2520whose%2520outputs%250Aare%2520averaged%2520in%2520each%2520sampling%2520step%2520to%2520ensure%2520the%25203D%2520generation%2520quality%2520from%250Amultiple%2520views.%2520To%2520accelerate%2520the%25203D%2520sampling%2520process%252C%2520we%2520also%2520proposed%2520a%250Astrategy%2520to%2520use%2520the%2520CNN-based%25203D%2520generation%2520as%2520a%2520prior%2520for%2520the%2520diffusion%2520model.%250AOur%2520experimental%2520results%2520on%2520human%2520patient%2520studies%2520suggested%2520that%2520MADM%2520can%250Agenerate%2520high-quality%25203D%2520translation%2520images%252C%2520outperforming%2520previous%2520CNN-based%250Aand%2520Diffusion-based%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08374v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=2.5D%20Multi-view%20Averaging%20Diffusion%20Model%20for%203D%20Medical%20Image%0A%20%20Translation%3A%20Application%20to%20Low-count%20PET%20Reconstruction%20with%20CT-less%0A%20%20Attenuation%20Correction&entry.906535625=Tianqi%20Chen%20and%20Jun%20Hou%20and%20Yinchi%20Zhou%20and%20Huidong%20Xie%20and%20Xiongchao%20Chen%20and%20Qiong%20Liu%20and%20Xueqi%20Guo%20and%20Menghua%20Xia%20and%20James%20S.%20Duncan%20and%20Chi%20Liu%20and%20Bo%20Zhou&entry.1292438233=%20%20Positron%20Emission%20Tomography%20%28PET%29%20is%20an%20important%20clinical%20imaging%20tool%20but%0Ainevitably%20introduces%20radiation%20hazards%20to%20patients%20and%20healthcare%20providers.%0AReducing%20the%20tracer%20injection%20dose%20and%20eliminating%20the%20CT%20acquisition%20for%0Aattenuation%20correction%20can%20reduce%20the%20overall%20radiation%20dose%2C%20but%20often%20results%0Ain%20PET%20with%20high%20noise%20and%20bias.%20Thus%2C%20it%20is%20desirable%20to%20develop%203D%20methods%20to%0Atranslate%20the%20non-attenuation-corrected%20low-dose%20PET%20%28NAC-LDPET%29%20into%0Aattenuation-corrected%20standard-dose%20PET%20%28AC-SDPET%29.%20Recently%2C%20diffusion%20models%0Ahave%20emerged%20as%20a%20new%20state-of-the-art%20deep%20learning%20method%20for%20image-to-image%0Atranslation%2C%20better%20than%20traditional%20CNN-based%20methods.%20However%2C%20due%20to%20the%0Ahigh%20computation%20cost%20and%20memory%20burden%2C%20it%20is%20largely%20limited%20to%202D%0Aapplications.%20To%20address%20these%20challenges%2C%20we%20developed%20a%20novel%202.5D%20Multi-view%0AAveraging%20Diffusion%20Model%20%28MADM%29%20for%203D%20image-to-image%20translation%20with%0Aapplication%20on%20NAC-LDPET%20to%20AC-SDPET%20translation.%20Specifically%2C%20MADM%20employs%0Aseparate%20diffusion%20models%20for%20axial%2C%20coronal%2C%20and%20sagittal%20views%2C%20whose%20outputs%0Aare%20averaged%20in%20each%20sampling%20step%20to%20ensure%20the%203D%20generation%20quality%20from%0Amultiple%20views.%20To%20accelerate%20the%203D%20sampling%20process%2C%20we%20also%20proposed%20a%0Astrategy%20to%20use%20the%20CNN-based%203D%20generation%20as%20a%20prior%20for%20the%20diffusion%20model.%0AOur%20experimental%20results%20on%20human%20patient%20studies%20suggested%20that%20MADM%20can%0Agenerate%20high-quality%203D%20translation%20images%2C%20outperforming%20previous%20CNN-based%0Aand%20Diffusion-based%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08374v1&entry.124074799=Read"},
{"title": "A$^{2}$-MAE: A spatial-temporal-spectral unified remote sensing\n  pre-training method based on anchor-aware masked autoencoder", "author": "Lixian Zhang and Yi Zhao and Runmin Dong and Jinxiao Zhang and Shuai Yuan and Shilei Cao and Mengxuan Chen and Juepeng Zheng and Weijia Li and Wei Liu and Litong Feng and Haohuan Fu", "abstract": "  Vast amounts of remote sensing (RS) data provide Earth observations across\nmultiple dimensions, encompassing critical spatial, temporal, and spectral\ninformation which is essential for addressing global-scale challenges such as\nland use monitoring, disaster prevention, and environmental change mitigation.\nDespite various pre-training methods tailored to the characteristics of RS\ndata, a key limitation persists: the inability to effectively integrate\nspatial, temporal, and spectral information within a single unified model. To\nunlock the potential of RS data, we construct a Spatial-Temporal-Spectral\nStructured Dataset (STSSD) characterized by the incorporation of multiple RS\nsources, diverse coverage, unified locations within image sets, and\nheterogeneity within images. Building upon this structured dataset, we propose\nan Anchor-Aware Masked AutoEncoder method (A$^{2}$-MAE), leveraging intrinsic\ncomplementary information from the different kinds of images and\ngeo-information to reconstruct the masked patches during the pre-training\nphase. A$^{2}$-MAE integrates an anchor-aware masking strategy and a geographic\nencoding module to comprehensively exploit the properties of RS images.\nSpecifically, the proposed anchor-aware masking strategy dynamically adapts the\nmasking process based on the meta-information of a pre-selected anchor image,\nthereby facilitating the training on images captured by diverse types of RS\nsources within one model. Furthermore, we propose a geographic encoding method\nto leverage accurate spatial patterns, enhancing the model generalization\ncapabilities for downstream applications that are generally location-related.\nExtensive experiments demonstrate our method achieves comprehensive\nimprovements across various downstream tasks compared with existing RS\npre-training methods, including image classification, semantic segmentation,\nand change detection tasks.\n", "link": "http://arxiv.org/abs/2406.08079v1", "date": "2024-06-12", "relevancy": 2.7024, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5457}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5449}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%24%5E%7B2%7D%24-MAE%3A%20A%20spatial-temporal-spectral%20unified%20remote%20sensing%0A%20%20pre-training%20method%20based%20on%20anchor-aware%20masked%20autoencoder&body=Title%3A%20A%24%5E%7B2%7D%24-MAE%3A%20A%20spatial-temporal-spectral%20unified%20remote%20sensing%0A%20%20pre-training%20method%20based%20on%20anchor-aware%20masked%20autoencoder%0AAuthor%3A%20Lixian%20Zhang%20and%20Yi%20Zhao%20and%20Runmin%20Dong%20and%20Jinxiao%20Zhang%20and%20Shuai%20Yuan%20and%20Shilei%20Cao%20and%20Mengxuan%20Chen%20and%20Juepeng%20Zheng%20and%20Weijia%20Li%20and%20Wei%20Liu%20and%20Litong%20Feng%20and%20Haohuan%20Fu%0AAbstract%3A%20%20%20Vast%20amounts%20of%20remote%20sensing%20%28RS%29%20data%20provide%20Earth%20observations%20across%0Amultiple%20dimensions%2C%20encompassing%20critical%20spatial%2C%20temporal%2C%20and%20spectral%0Ainformation%20which%20is%20essential%20for%20addressing%20global-scale%20challenges%20such%20as%0Aland%20use%20monitoring%2C%20disaster%20prevention%2C%20and%20environmental%20change%20mitigation.%0ADespite%20various%20pre-training%20methods%20tailored%20to%20the%20characteristics%20of%20RS%0Adata%2C%20a%20key%20limitation%20persists%3A%20the%20inability%20to%20effectively%20integrate%0Aspatial%2C%20temporal%2C%20and%20spectral%20information%20within%20a%20single%20unified%20model.%20To%0Aunlock%20the%20potential%20of%20RS%20data%2C%20we%20construct%20a%20Spatial-Temporal-Spectral%0AStructured%20Dataset%20%28STSSD%29%20characterized%20by%20the%20incorporation%20of%20multiple%20RS%0Asources%2C%20diverse%20coverage%2C%20unified%20locations%20within%20image%20sets%2C%20and%0Aheterogeneity%20within%20images.%20Building%20upon%20this%20structured%20dataset%2C%20we%20propose%0Aan%20Anchor-Aware%20Masked%20AutoEncoder%20method%20%28A%24%5E%7B2%7D%24-MAE%29%2C%20leveraging%20intrinsic%0Acomplementary%20information%20from%20the%20different%20kinds%20of%20images%20and%0Ageo-information%20to%20reconstruct%20the%20masked%20patches%20during%20the%20pre-training%0Aphase.%20A%24%5E%7B2%7D%24-MAE%20integrates%20an%20anchor-aware%20masking%20strategy%20and%20a%20geographic%0Aencoding%20module%20to%20comprehensively%20exploit%20the%20properties%20of%20RS%20images.%0ASpecifically%2C%20the%20proposed%20anchor-aware%20masking%20strategy%20dynamically%20adapts%20the%0Amasking%20process%20based%20on%20the%20meta-information%20of%20a%20pre-selected%20anchor%20image%2C%0Athereby%20facilitating%20the%20training%20on%20images%20captured%20by%20diverse%20types%20of%20RS%0Asources%20within%20one%20model.%20Furthermore%2C%20we%20propose%20a%20geographic%20encoding%20method%0Ato%20leverage%20accurate%20spatial%20patterns%2C%20enhancing%20the%20model%20generalization%0Acapabilities%20for%20downstream%20applications%20that%20are%20generally%20location-related.%0AExtensive%20experiments%20demonstrate%20our%20method%20achieves%20comprehensive%0Aimprovements%20across%20various%20downstream%20tasks%20compared%20with%20existing%20RS%0Apre-training%20methods%2C%20including%20image%20classification%2C%20semantic%20segmentation%2C%0Aand%20change%20detection%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2524%255E%257B2%257D%2524-MAE%253A%2520A%2520spatial-temporal-spectral%2520unified%2520remote%2520sensing%250A%2520%2520pre-training%2520method%2520based%2520on%2520anchor-aware%2520masked%2520autoencoder%26entry.906535625%3DLixian%2520Zhang%2520and%2520Yi%2520Zhao%2520and%2520Runmin%2520Dong%2520and%2520Jinxiao%2520Zhang%2520and%2520Shuai%2520Yuan%2520and%2520Shilei%2520Cao%2520and%2520Mengxuan%2520Chen%2520and%2520Juepeng%2520Zheng%2520and%2520Weijia%2520Li%2520and%2520Wei%2520Liu%2520and%2520Litong%2520Feng%2520and%2520Haohuan%2520Fu%26entry.1292438233%3D%2520%2520Vast%2520amounts%2520of%2520remote%2520sensing%2520%2528RS%2529%2520data%2520provide%2520Earth%2520observations%2520across%250Amultiple%2520dimensions%252C%2520encompassing%2520critical%2520spatial%252C%2520temporal%252C%2520and%2520spectral%250Ainformation%2520which%2520is%2520essential%2520for%2520addressing%2520global-scale%2520challenges%2520such%2520as%250Aland%2520use%2520monitoring%252C%2520disaster%2520prevention%252C%2520and%2520environmental%2520change%2520mitigation.%250ADespite%2520various%2520pre-training%2520methods%2520tailored%2520to%2520the%2520characteristics%2520of%2520RS%250Adata%252C%2520a%2520key%2520limitation%2520persists%253A%2520the%2520inability%2520to%2520effectively%2520integrate%250Aspatial%252C%2520temporal%252C%2520and%2520spectral%2520information%2520within%2520a%2520single%2520unified%2520model.%2520To%250Aunlock%2520the%2520potential%2520of%2520RS%2520data%252C%2520we%2520construct%2520a%2520Spatial-Temporal-Spectral%250AStructured%2520Dataset%2520%2528STSSD%2529%2520characterized%2520by%2520the%2520incorporation%2520of%2520multiple%2520RS%250Asources%252C%2520diverse%2520coverage%252C%2520unified%2520locations%2520within%2520image%2520sets%252C%2520and%250Aheterogeneity%2520within%2520images.%2520Building%2520upon%2520this%2520structured%2520dataset%252C%2520we%2520propose%250Aan%2520Anchor-Aware%2520Masked%2520AutoEncoder%2520method%2520%2528A%2524%255E%257B2%257D%2524-MAE%2529%252C%2520leveraging%2520intrinsic%250Acomplementary%2520information%2520from%2520the%2520different%2520kinds%2520of%2520images%2520and%250Ageo-information%2520to%2520reconstruct%2520the%2520masked%2520patches%2520during%2520the%2520pre-training%250Aphase.%2520A%2524%255E%257B2%257D%2524-MAE%2520integrates%2520an%2520anchor-aware%2520masking%2520strategy%2520and%2520a%2520geographic%250Aencoding%2520module%2520to%2520comprehensively%2520exploit%2520the%2520properties%2520of%2520RS%2520images.%250ASpecifically%252C%2520the%2520proposed%2520anchor-aware%2520masking%2520strategy%2520dynamically%2520adapts%2520the%250Amasking%2520process%2520based%2520on%2520the%2520meta-information%2520of%2520a%2520pre-selected%2520anchor%2520image%252C%250Athereby%2520facilitating%2520the%2520training%2520on%2520images%2520captured%2520by%2520diverse%2520types%2520of%2520RS%250Asources%2520within%2520one%2520model.%2520Furthermore%252C%2520we%2520propose%2520a%2520geographic%2520encoding%2520method%250Ato%2520leverage%2520accurate%2520spatial%2520patterns%252C%2520enhancing%2520the%2520model%2520generalization%250Acapabilities%2520for%2520downstream%2520applications%2520that%2520are%2520generally%2520location-related.%250AExtensive%2520experiments%2520demonstrate%2520our%2520method%2520achieves%2520comprehensive%250Aimprovements%2520across%2520various%2520downstream%2520tasks%2520compared%2520with%2520existing%2520RS%250Apre-training%2520methods%252C%2520including%2520image%2520classification%252C%2520semantic%2520segmentation%252C%250Aand%2520change%2520detection%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%24%5E%7B2%7D%24-MAE%3A%20A%20spatial-temporal-spectral%20unified%20remote%20sensing%0A%20%20pre-training%20method%20based%20on%20anchor-aware%20masked%20autoencoder&entry.906535625=Lixian%20Zhang%20and%20Yi%20Zhao%20and%20Runmin%20Dong%20and%20Jinxiao%20Zhang%20and%20Shuai%20Yuan%20and%20Shilei%20Cao%20and%20Mengxuan%20Chen%20and%20Juepeng%20Zheng%20and%20Weijia%20Li%20and%20Wei%20Liu%20and%20Litong%20Feng%20and%20Haohuan%20Fu&entry.1292438233=%20%20Vast%20amounts%20of%20remote%20sensing%20%28RS%29%20data%20provide%20Earth%20observations%20across%0Amultiple%20dimensions%2C%20encompassing%20critical%20spatial%2C%20temporal%2C%20and%20spectral%0Ainformation%20which%20is%20essential%20for%20addressing%20global-scale%20challenges%20such%20as%0Aland%20use%20monitoring%2C%20disaster%20prevention%2C%20and%20environmental%20change%20mitigation.%0ADespite%20various%20pre-training%20methods%20tailored%20to%20the%20characteristics%20of%20RS%0Adata%2C%20a%20key%20limitation%20persists%3A%20the%20inability%20to%20effectively%20integrate%0Aspatial%2C%20temporal%2C%20and%20spectral%20information%20within%20a%20single%20unified%20model.%20To%0Aunlock%20the%20potential%20of%20RS%20data%2C%20we%20construct%20a%20Spatial-Temporal-Spectral%0AStructured%20Dataset%20%28STSSD%29%20characterized%20by%20the%20incorporation%20of%20multiple%20RS%0Asources%2C%20diverse%20coverage%2C%20unified%20locations%20within%20image%20sets%2C%20and%0Aheterogeneity%20within%20images.%20Building%20upon%20this%20structured%20dataset%2C%20we%20propose%0Aan%20Anchor-Aware%20Masked%20AutoEncoder%20method%20%28A%24%5E%7B2%7D%24-MAE%29%2C%20leveraging%20intrinsic%0Acomplementary%20information%20from%20the%20different%20kinds%20of%20images%20and%0Ageo-information%20to%20reconstruct%20the%20masked%20patches%20during%20the%20pre-training%0Aphase.%20A%24%5E%7B2%7D%24-MAE%20integrates%20an%20anchor-aware%20masking%20strategy%20and%20a%20geographic%0Aencoding%20module%20to%20comprehensively%20exploit%20the%20properties%20of%20RS%20images.%0ASpecifically%2C%20the%20proposed%20anchor-aware%20masking%20strategy%20dynamically%20adapts%20the%0Amasking%20process%20based%20on%20the%20meta-information%20of%20a%20pre-selected%20anchor%20image%2C%0Athereby%20facilitating%20the%20training%20on%20images%20captured%20by%20diverse%20types%20of%20RS%0Asources%20within%20one%20model.%20Furthermore%2C%20we%20propose%20a%20geographic%20encoding%20method%0Ato%20leverage%20accurate%20spatial%20patterns%2C%20enhancing%20the%20model%20generalization%0Acapabilities%20for%20downstream%20applications%20that%20are%20generally%20location-related.%0AExtensive%20experiments%20demonstrate%20our%20method%20achieves%20comprehensive%0Aimprovements%20across%20various%20downstream%20tasks%20compared%20with%20existing%20RS%0Apre-training%20methods%2C%20including%20image%20classification%2C%20semantic%20segmentation%2C%0Aand%20change%20detection%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08079v1&entry.124074799=Read"},
{"title": "Utilizing Navigation Path to Generate Target Point for Enhanced\n  End-to-End Autonomous Driving Planning", "author": "Yuanhua Shen and Jun Li", "abstract": "  In recent years, end-to-end autonomous driving frameworks have been shown to\nnot only enhance perception performance but also improve planning capabilities.\nHowever, most previous end-to-end autonomous driving frameworks have primarily\nfocused on enhancing environment perception while neglecting the learning of\nautonomous vehicle planning intent. Within the end-to-end framework, this paper\nproposes a method termed NTT, which obtains explicit planning intent through\nthe navigation path. NTT first generates the future target point for the\nautonomous vehicle based on the navigation path, thereby enhancing planning\nperformance within the end-to-end framework. On one hand, the generation of the\ntarget point allows the autonomous vehicle to learn explicit intention from the\nnavigation path, enhancing the practicality of planning. On the other hand,\nplanning trajectory generated based on the target point can adapt more flexibly\nto environmental changes, thus effectively improving planning safety. We\nachieved excellent planning performance on the widely used nuScenes dataset and\nvalidated the effectiveness of our method through ablation experiments.\n", "link": "http://arxiv.org/abs/2406.08349v1", "date": "2024-06-12", "relevancy": 2.6935, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5536}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5369}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5256}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Utilizing%20Navigation%20Path%20to%20Generate%20Target%20Point%20for%20Enhanced%0A%20%20End-to-End%20Autonomous%20Driving%20Planning&body=Title%3A%20Utilizing%20Navigation%20Path%20to%20Generate%20Target%20Point%20for%20Enhanced%0A%20%20End-to-End%20Autonomous%20Driving%20Planning%0AAuthor%3A%20Yuanhua%20Shen%20and%20Jun%20Li%0AAbstract%3A%20%20%20In%20recent%20years%2C%20end-to-end%20autonomous%20driving%20frameworks%20have%20been%20shown%20to%0Anot%20only%20enhance%20perception%20performance%20but%20also%20improve%20planning%20capabilities.%0AHowever%2C%20most%20previous%20end-to-end%20autonomous%20driving%20frameworks%20have%20primarily%0Afocused%20on%20enhancing%20environment%20perception%20while%20neglecting%20the%20learning%20of%0Aautonomous%20vehicle%20planning%20intent.%20Within%20the%20end-to-end%20framework%2C%20this%20paper%0Aproposes%20a%20method%20termed%20NTT%2C%20which%20obtains%20explicit%20planning%20intent%20through%0Athe%20navigation%20path.%20NTT%20first%20generates%20the%20future%20target%20point%20for%20the%0Aautonomous%20vehicle%20based%20on%20the%20navigation%20path%2C%20thereby%20enhancing%20planning%0Aperformance%20within%20the%20end-to-end%20framework.%20On%20one%20hand%2C%20the%20generation%20of%20the%0Atarget%20point%20allows%20the%20autonomous%20vehicle%20to%20learn%20explicit%20intention%20from%20the%0Anavigation%20path%2C%20enhancing%20the%20practicality%20of%20planning.%20On%20the%20other%20hand%2C%0Aplanning%20trajectory%20generated%20based%20on%20the%20target%20point%20can%20adapt%20more%20flexibly%0Ato%20environmental%20changes%2C%20thus%20effectively%20improving%20planning%20safety.%20We%0Aachieved%20excellent%20planning%20performance%20on%20the%20widely%20used%20nuScenes%20dataset%20and%0Avalidated%20the%20effectiveness%20of%20our%20method%20through%20ablation%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08349v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUtilizing%2520Navigation%2520Path%2520to%2520Generate%2520Target%2520Point%2520for%2520Enhanced%250A%2520%2520End-to-End%2520Autonomous%2520Driving%2520Planning%26entry.906535625%3DYuanhua%2520Shen%2520and%2520Jun%2520Li%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520end-to-end%2520autonomous%2520driving%2520frameworks%2520have%2520been%2520shown%2520to%250Anot%2520only%2520enhance%2520perception%2520performance%2520but%2520also%2520improve%2520planning%2520capabilities.%250AHowever%252C%2520most%2520previous%2520end-to-end%2520autonomous%2520driving%2520frameworks%2520have%2520primarily%250Afocused%2520on%2520enhancing%2520environment%2520perception%2520while%2520neglecting%2520the%2520learning%2520of%250Aautonomous%2520vehicle%2520planning%2520intent.%2520Within%2520the%2520end-to-end%2520framework%252C%2520this%2520paper%250Aproposes%2520a%2520method%2520termed%2520NTT%252C%2520which%2520obtains%2520explicit%2520planning%2520intent%2520through%250Athe%2520navigation%2520path.%2520NTT%2520first%2520generates%2520the%2520future%2520target%2520point%2520for%2520the%250Aautonomous%2520vehicle%2520based%2520on%2520the%2520navigation%2520path%252C%2520thereby%2520enhancing%2520planning%250Aperformance%2520within%2520the%2520end-to-end%2520framework.%2520On%2520one%2520hand%252C%2520the%2520generation%2520of%2520the%250Atarget%2520point%2520allows%2520the%2520autonomous%2520vehicle%2520to%2520learn%2520explicit%2520intention%2520from%2520the%250Anavigation%2520path%252C%2520enhancing%2520the%2520practicality%2520of%2520planning.%2520On%2520the%2520other%2520hand%252C%250Aplanning%2520trajectory%2520generated%2520based%2520on%2520the%2520target%2520point%2520can%2520adapt%2520more%2520flexibly%250Ato%2520environmental%2520changes%252C%2520thus%2520effectively%2520improving%2520planning%2520safety.%2520We%250Aachieved%2520excellent%2520planning%2520performance%2520on%2520the%2520widely%2520used%2520nuScenes%2520dataset%2520and%250Avalidated%2520the%2520effectiveness%2520of%2520our%2520method%2520through%2520ablation%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08349v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Utilizing%20Navigation%20Path%20to%20Generate%20Target%20Point%20for%20Enhanced%0A%20%20End-to-End%20Autonomous%20Driving%20Planning&entry.906535625=Yuanhua%20Shen%20and%20Jun%20Li&entry.1292438233=%20%20In%20recent%20years%2C%20end-to-end%20autonomous%20driving%20frameworks%20have%20been%20shown%20to%0Anot%20only%20enhance%20perception%20performance%20but%20also%20improve%20planning%20capabilities.%0AHowever%2C%20most%20previous%20end-to-end%20autonomous%20driving%20frameworks%20have%20primarily%0Afocused%20on%20enhancing%20environment%20perception%20while%20neglecting%20the%20learning%20of%0Aautonomous%20vehicle%20planning%20intent.%20Within%20the%20end-to-end%20framework%2C%20this%20paper%0Aproposes%20a%20method%20termed%20NTT%2C%20which%20obtains%20explicit%20planning%20intent%20through%0Athe%20navigation%20path.%20NTT%20first%20generates%20the%20future%20target%20point%20for%20the%0Aautonomous%20vehicle%20based%20on%20the%20navigation%20path%2C%20thereby%20enhancing%20planning%0Aperformance%20within%20the%20end-to-end%20framework.%20On%20one%20hand%2C%20the%20generation%20of%20the%0Atarget%20point%20allows%20the%20autonomous%20vehicle%20to%20learn%20explicit%20intention%20from%20the%0Anavigation%20path%2C%20enhancing%20the%20practicality%20of%20planning.%20On%20the%20other%20hand%2C%0Aplanning%20trajectory%20generated%20based%20on%20the%20target%20point%20can%20adapt%20more%20flexibly%0Ato%20environmental%20changes%2C%20thus%20effectively%20improving%20planning%20safety.%20We%0Aachieved%20excellent%20planning%20performance%20on%20the%20widely%20used%20nuScenes%20dataset%20and%0Avalidated%20the%20effectiveness%20of%20our%20method%20through%20ablation%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08349v1&entry.124074799=Read"},
{"title": "Category-level Neural Field for Reconstruction of Partially Observed\n  Objects in Indoor Environment", "author": "Taekbeom Lee and Youngseok Jang and H. Jin Kim", "abstract": "  Neural implicit representation has attracted attention in 3D reconstruction\nthrough various success cases. For further applications such as scene\nunderstanding or editing, several works have shown progress towards object\ncompositional reconstruction. Despite their superior performance in observed\nregions, their performance is still limited in reconstructing objects that are\npartially observed. To better treat this problem, we introduce category-level\nneural fields that learn meaningful common 3D information among objects\nbelonging to the same category present in the scene. Our key idea is to\nsubcategorize objects based on their observed shape for better training of the\ncategory-level model. Then we take advantage of the neural field to conduct the\nchallenging task of registering partially observed objects by selecting and\naligning against representative objects selected by ray-based uncertainty.\nExperiments on both simulation and real-world datasets demonstrate that our\nmethod improves the reconstruction of unobserved parts for several categories.\n", "link": "http://arxiv.org/abs/2406.08176v1", "date": "2024-06-12", "relevancy": 2.6868, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5463}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5338}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Category-level%20Neural%20Field%20for%20Reconstruction%20of%20Partially%20Observed%0A%20%20Objects%20in%20Indoor%20Environment&body=Title%3A%20Category-level%20Neural%20Field%20for%20Reconstruction%20of%20Partially%20Observed%0A%20%20Objects%20in%20Indoor%20Environment%0AAuthor%3A%20Taekbeom%20Lee%20and%20Youngseok%20Jang%20and%20H.%20Jin%20Kim%0AAbstract%3A%20%20%20Neural%20implicit%20representation%20has%20attracted%20attention%20in%203D%20reconstruction%0Athrough%20various%20success%20cases.%20For%20further%20applications%20such%20as%20scene%0Aunderstanding%20or%20editing%2C%20several%20works%20have%20shown%20progress%20towards%20object%0Acompositional%20reconstruction.%20Despite%20their%20superior%20performance%20in%20observed%0Aregions%2C%20their%20performance%20is%20still%20limited%20in%20reconstructing%20objects%20that%20are%0Apartially%20observed.%20To%20better%20treat%20this%20problem%2C%20we%20introduce%20category-level%0Aneural%20fields%20that%20learn%20meaningful%20common%203D%20information%20among%20objects%0Abelonging%20to%20the%20same%20category%20present%20in%20the%20scene.%20Our%20key%20idea%20is%20to%0Asubcategorize%20objects%20based%20on%20their%20observed%20shape%20for%20better%20training%20of%20the%0Acategory-level%20model.%20Then%20we%20take%20advantage%20of%20the%20neural%20field%20to%20conduct%20the%0Achallenging%20task%20of%20registering%20partially%20observed%20objects%20by%20selecting%20and%0Aaligning%20against%20representative%20objects%20selected%20by%20ray-based%20uncertainty.%0AExperiments%20on%20both%20simulation%20and%20real-world%20datasets%20demonstrate%20that%20our%0Amethod%20improves%20the%20reconstruction%20of%20unobserved%20parts%20for%20several%20categories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08176v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCategory-level%2520Neural%2520Field%2520for%2520Reconstruction%2520of%2520Partially%2520Observed%250A%2520%2520Objects%2520in%2520Indoor%2520Environment%26entry.906535625%3DTaekbeom%2520Lee%2520and%2520Youngseok%2520Jang%2520and%2520H.%2520Jin%2520Kim%26entry.1292438233%3D%2520%2520Neural%2520implicit%2520representation%2520has%2520attracted%2520attention%2520in%25203D%2520reconstruction%250Athrough%2520various%2520success%2520cases.%2520For%2520further%2520applications%2520such%2520as%2520scene%250Aunderstanding%2520or%2520editing%252C%2520several%2520works%2520have%2520shown%2520progress%2520towards%2520object%250Acompositional%2520reconstruction.%2520Despite%2520their%2520superior%2520performance%2520in%2520observed%250Aregions%252C%2520their%2520performance%2520is%2520still%2520limited%2520in%2520reconstructing%2520objects%2520that%2520are%250Apartially%2520observed.%2520To%2520better%2520treat%2520this%2520problem%252C%2520we%2520introduce%2520category-level%250Aneural%2520fields%2520that%2520learn%2520meaningful%2520common%25203D%2520information%2520among%2520objects%250Abelonging%2520to%2520the%2520same%2520category%2520present%2520in%2520the%2520scene.%2520Our%2520key%2520idea%2520is%2520to%250Asubcategorize%2520objects%2520based%2520on%2520their%2520observed%2520shape%2520for%2520better%2520training%2520of%2520the%250Acategory-level%2520model.%2520Then%2520we%2520take%2520advantage%2520of%2520the%2520neural%2520field%2520to%2520conduct%2520the%250Achallenging%2520task%2520of%2520registering%2520partially%2520observed%2520objects%2520by%2520selecting%2520and%250Aaligning%2520against%2520representative%2520objects%2520selected%2520by%2520ray-based%2520uncertainty.%250AExperiments%2520on%2520both%2520simulation%2520and%2520real-world%2520datasets%2520demonstrate%2520that%2520our%250Amethod%2520improves%2520the%2520reconstruction%2520of%2520unobserved%2520parts%2520for%2520several%2520categories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08176v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Category-level%20Neural%20Field%20for%20Reconstruction%20of%20Partially%20Observed%0A%20%20Objects%20in%20Indoor%20Environment&entry.906535625=Taekbeom%20Lee%20and%20Youngseok%20Jang%20and%20H.%20Jin%20Kim&entry.1292438233=%20%20Neural%20implicit%20representation%20has%20attracted%20attention%20in%203D%20reconstruction%0Athrough%20various%20success%20cases.%20For%20further%20applications%20such%20as%20scene%0Aunderstanding%20or%20editing%2C%20several%20works%20have%20shown%20progress%20towards%20object%0Acompositional%20reconstruction.%20Despite%20their%20superior%20performance%20in%20observed%0Aregions%2C%20their%20performance%20is%20still%20limited%20in%20reconstructing%20objects%20that%20are%0Apartially%20observed.%20To%20better%20treat%20this%20problem%2C%20we%20introduce%20category-level%0Aneural%20fields%20that%20learn%20meaningful%20common%203D%20information%20among%20objects%0Abelonging%20to%20the%20same%20category%20present%20in%20the%20scene.%20Our%20key%20idea%20is%20to%0Asubcategorize%20objects%20based%20on%20their%20observed%20shape%20for%20better%20training%20of%20the%0Acategory-level%20model.%20Then%20we%20take%20advantage%20of%20the%20neural%20field%20to%20conduct%20the%0Achallenging%20task%20of%20registering%20partially%20observed%20objects%20by%20selecting%20and%0Aaligning%20against%20representative%20objects%20selected%20by%20ray-based%20uncertainty.%0AExperiments%20on%20both%20simulation%20and%20real-world%20datasets%20demonstrate%20that%20our%0Amethod%20improves%20the%20reconstruction%20of%20unobserved%20parts%20for%20several%20categories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08176v1&entry.124074799=Read"},
{"title": "Inductive Global and Local Manifold Approximation and Projection", "author": "Jungeum Kim and Xiao Wang", "abstract": "  Nonlinear dimensional reduction with the manifold assumption, often called\nmanifold learning, has proven its usefulness in a wide range of\nhigh-dimensional data analysis. The significant impact of t-SNE and UMAP has\ncatalyzed intense research interest, seeking further innovations toward\nvisualizing not only the local but also the global structure information of the\ndata. Moreover, there have been consistent efforts toward generalizable\ndimensional reduction that handles unseen data. In this paper, we first propose\nGLoMAP, a novel manifold learning method for dimensional reduction and\nhigh-dimensional data visualization. GLoMAP preserves locally and globally\nmeaningful distance estimates and displays a progression from global to local\nformation during the course of optimization. Furthermore, we extend GLoMAP to\nits inductive version, iGLoMAP, which utilizes a deep neural network to map\ndata to its lower-dimensional representation. This allows iGLoMAP to provide\nlower-dimensional embeddings for unseen points without needing to re-train the\nalgorithm. iGLoMAP is also well-suited for mini-batch learning, enabling\nlarge-scale, accelerated gradient calculations. We have successfully applied\nboth GLoMAP and iGLoMAP to the simulated and real-data settings, with\ncompetitive experiments against the state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2406.08097v1", "date": "2024-06-12", "relevancy": 2.6845, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5504}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5362}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inductive%20Global%20and%20Local%20Manifold%20Approximation%20and%20Projection&body=Title%3A%20Inductive%20Global%20and%20Local%20Manifold%20Approximation%20and%20Projection%0AAuthor%3A%20Jungeum%20Kim%20and%20Xiao%20Wang%0AAbstract%3A%20%20%20Nonlinear%20dimensional%20reduction%20with%20the%20manifold%20assumption%2C%20often%20called%0Amanifold%20learning%2C%20has%20proven%20its%20usefulness%20in%20a%20wide%20range%20of%0Ahigh-dimensional%20data%20analysis.%20The%20significant%20impact%20of%20t-SNE%20and%20UMAP%20has%0Acatalyzed%20intense%20research%20interest%2C%20seeking%20further%20innovations%20toward%0Avisualizing%20not%20only%20the%20local%20but%20also%20the%20global%20structure%20information%20of%20the%0Adata.%20Moreover%2C%20there%20have%20been%20consistent%20efforts%20toward%20generalizable%0Adimensional%20reduction%20that%20handles%20unseen%20data.%20In%20this%20paper%2C%20we%20first%20propose%0AGLoMAP%2C%20a%20novel%20manifold%20learning%20method%20for%20dimensional%20reduction%20and%0Ahigh-dimensional%20data%20visualization.%20GLoMAP%20preserves%20locally%20and%20globally%0Ameaningful%20distance%20estimates%20and%20displays%20a%20progression%20from%20global%20to%20local%0Aformation%20during%20the%20course%20of%20optimization.%20Furthermore%2C%20we%20extend%20GLoMAP%20to%0Aits%20inductive%20version%2C%20iGLoMAP%2C%20which%20utilizes%20a%20deep%20neural%20network%20to%20map%0Adata%20to%20its%20lower-dimensional%20representation.%20This%20allows%20iGLoMAP%20to%20provide%0Alower-dimensional%20embeddings%20for%20unseen%20points%20without%20needing%20to%20re-train%20the%0Aalgorithm.%20iGLoMAP%20is%20also%20well-suited%20for%20mini-batch%20learning%2C%20enabling%0Alarge-scale%2C%20accelerated%20gradient%20calculations.%20We%20have%20successfully%20applied%0Aboth%20GLoMAP%20and%20iGLoMAP%20to%20the%20simulated%20and%20real-data%20settings%2C%20with%0Acompetitive%20experiments%20against%20the%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInductive%2520Global%2520and%2520Local%2520Manifold%2520Approximation%2520and%2520Projection%26entry.906535625%3DJungeum%2520Kim%2520and%2520Xiao%2520Wang%26entry.1292438233%3D%2520%2520Nonlinear%2520dimensional%2520reduction%2520with%2520the%2520manifold%2520assumption%252C%2520often%2520called%250Amanifold%2520learning%252C%2520has%2520proven%2520its%2520usefulness%2520in%2520a%2520wide%2520range%2520of%250Ahigh-dimensional%2520data%2520analysis.%2520The%2520significant%2520impact%2520of%2520t-SNE%2520and%2520UMAP%2520has%250Acatalyzed%2520intense%2520research%2520interest%252C%2520seeking%2520further%2520innovations%2520toward%250Avisualizing%2520not%2520only%2520the%2520local%2520but%2520also%2520the%2520global%2520structure%2520information%2520of%2520the%250Adata.%2520Moreover%252C%2520there%2520have%2520been%2520consistent%2520efforts%2520toward%2520generalizable%250Adimensional%2520reduction%2520that%2520handles%2520unseen%2520data.%2520In%2520this%2520paper%252C%2520we%2520first%2520propose%250AGLoMAP%252C%2520a%2520novel%2520manifold%2520learning%2520method%2520for%2520dimensional%2520reduction%2520and%250Ahigh-dimensional%2520data%2520visualization.%2520GLoMAP%2520preserves%2520locally%2520and%2520globally%250Ameaningful%2520distance%2520estimates%2520and%2520displays%2520a%2520progression%2520from%2520global%2520to%2520local%250Aformation%2520during%2520the%2520course%2520of%2520optimization.%2520Furthermore%252C%2520we%2520extend%2520GLoMAP%2520to%250Aits%2520inductive%2520version%252C%2520iGLoMAP%252C%2520which%2520utilizes%2520a%2520deep%2520neural%2520network%2520to%2520map%250Adata%2520to%2520its%2520lower-dimensional%2520representation.%2520This%2520allows%2520iGLoMAP%2520to%2520provide%250Alower-dimensional%2520embeddings%2520for%2520unseen%2520points%2520without%2520needing%2520to%2520re-train%2520the%250Aalgorithm.%2520iGLoMAP%2520is%2520also%2520well-suited%2520for%2520mini-batch%2520learning%252C%2520enabling%250Alarge-scale%252C%2520accelerated%2520gradient%2520calculations.%2520We%2520have%2520successfully%2520applied%250Aboth%2520GLoMAP%2520and%2520iGLoMAP%2520to%2520the%2520simulated%2520and%2520real-data%2520settings%252C%2520with%250Acompetitive%2520experiments%2520against%2520the%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inductive%20Global%20and%20Local%20Manifold%20Approximation%20and%20Projection&entry.906535625=Jungeum%20Kim%20and%20Xiao%20Wang&entry.1292438233=%20%20Nonlinear%20dimensional%20reduction%20with%20the%20manifold%20assumption%2C%20often%20called%0Amanifold%20learning%2C%20has%20proven%20its%20usefulness%20in%20a%20wide%20range%20of%0Ahigh-dimensional%20data%20analysis.%20The%20significant%20impact%20of%20t-SNE%20and%20UMAP%20has%0Acatalyzed%20intense%20research%20interest%2C%20seeking%20further%20innovations%20toward%0Avisualizing%20not%20only%20the%20local%20but%20also%20the%20global%20structure%20information%20of%20the%0Adata.%20Moreover%2C%20there%20have%20been%20consistent%20efforts%20toward%20generalizable%0Adimensional%20reduction%20that%20handles%20unseen%20data.%20In%20this%20paper%2C%20we%20first%20propose%0AGLoMAP%2C%20a%20novel%20manifold%20learning%20method%20for%20dimensional%20reduction%20and%0Ahigh-dimensional%20data%20visualization.%20GLoMAP%20preserves%20locally%20and%20globally%0Ameaningful%20distance%20estimates%20and%20displays%20a%20progression%20from%20global%20to%20local%0Aformation%20during%20the%20course%20of%20optimization.%20Furthermore%2C%20we%20extend%20GLoMAP%20to%0Aits%20inductive%20version%2C%20iGLoMAP%2C%20which%20utilizes%20a%20deep%20neural%20network%20to%20map%0Adata%20to%20its%20lower-dimensional%20representation.%20This%20allows%20iGLoMAP%20to%20provide%0Alower-dimensional%20embeddings%20for%20unseen%20points%20without%20needing%20to%20re-train%20the%0Aalgorithm.%20iGLoMAP%20is%20also%20well-suited%20for%20mini-batch%20learning%2C%20enabling%0Alarge-scale%2C%20accelerated%20gradient%20calculations.%20We%20have%20successfully%20applied%0Aboth%20GLoMAP%20and%20iGLoMAP%20to%20the%20simulated%20and%20real-data%20settings%2C%20with%0Acompetitive%20experiments%20against%20the%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08097v1&entry.124074799=Read"},
{"title": "CoopHash: Cooperative Learning of Multipurpose Descriptor and\n  Contrastive Pair Generator via Variational MCMC Teaching for Supervised Image\n  Hashing", "author": "Khoa D. Doan and Jianwen Xie and Yaxuan Zhu and Yang Zhao and Ping Li", "abstract": "  Leveraging supervised information can lead to superior retrieval performance\nin the image hashing domain but the performance degrades significantly without\nenough labeled data. One effective solution to boost performance is to employ\ngenerative models, such as Generative Adversarial Networks (GANs), to generate\nsynthetic data in an image hashing model. However, GAN-based methods are\ndifficult to train, which prevents the hashing approaches from jointly training\nthe generative models and the hash functions. This limitation results in\nsub-optimal retrieval performance. To overcome this limitation, we propose a\nnovel framework, the generative cooperative hashing network, which is based on\nenergy-based cooperative learning. This framework jointly learns a powerful\ngenerative representation of the data and a robust hash function via two\ncomponents: a top-down contrastive pair generator that synthesizes contrastive\nimages and a bottom-up multipurpose descriptor that simultaneously represents\nthe images from multiple perspectives, including probability density, hash\ncode, latent code, and category. The two components are jointly learned via a\nnovel likelihood-based cooperative learning scheme. We conduct experiments on\nseveral real-world datasets and show that the proposed method outperforms the\ncompeting hashing supervised methods, achieving up to 10\\% relative improvement\nover the current state-of-the-art supervised hashing methods, and exhibits a\nsignificantly better performance in out-of-distribution retrieval.\n", "link": "http://arxiv.org/abs/2210.04288v4", "date": "2024-06-12", "relevancy": 2.6741, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5464}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5403}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoopHash%3A%20Cooperative%20Learning%20of%20Multipurpose%20Descriptor%20and%0A%20%20Contrastive%20Pair%20Generator%20via%20Variational%20MCMC%20Teaching%20for%20Supervised%20Image%0A%20%20Hashing&body=Title%3A%20CoopHash%3A%20Cooperative%20Learning%20of%20Multipurpose%20Descriptor%20and%0A%20%20Contrastive%20Pair%20Generator%20via%20Variational%20MCMC%20Teaching%20for%20Supervised%20Image%0A%20%20Hashing%0AAuthor%3A%20Khoa%20D.%20Doan%20and%20Jianwen%20Xie%20and%20Yaxuan%20Zhu%20and%20Yang%20Zhao%20and%20Ping%20Li%0AAbstract%3A%20%20%20Leveraging%20supervised%20information%20can%20lead%20to%20superior%20retrieval%20performance%0Ain%20the%20image%20hashing%20domain%20but%20the%20performance%20degrades%20significantly%20without%0Aenough%20labeled%20data.%20One%20effective%20solution%20to%20boost%20performance%20is%20to%20employ%0Agenerative%20models%2C%20such%20as%20Generative%20Adversarial%20Networks%20%28GANs%29%2C%20to%20generate%0Asynthetic%20data%20in%20an%20image%20hashing%20model.%20However%2C%20GAN-based%20methods%20are%0Adifficult%20to%20train%2C%20which%20prevents%20the%20hashing%20approaches%20from%20jointly%20training%0Athe%20generative%20models%20and%20the%20hash%20functions.%20This%20limitation%20results%20in%0Asub-optimal%20retrieval%20performance.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%0Anovel%20framework%2C%20the%20generative%20cooperative%20hashing%20network%2C%20which%20is%20based%20on%0Aenergy-based%20cooperative%20learning.%20This%20framework%20jointly%20learns%20a%20powerful%0Agenerative%20representation%20of%20the%20data%20and%20a%20robust%20hash%20function%20via%20two%0Acomponents%3A%20a%20top-down%20contrastive%20pair%20generator%20that%20synthesizes%20contrastive%0Aimages%20and%20a%20bottom-up%20multipurpose%20descriptor%20that%20simultaneously%20represents%0Athe%20images%20from%20multiple%20perspectives%2C%20including%20probability%20density%2C%20hash%0Acode%2C%20latent%20code%2C%20and%20category.%20The%20two%20components%20are%20jointly%20learned%20via%20a%0Anovel%20likelihood-based%20cooperative%20learning%20scheme.%20We%20conduct%20experiments%20on%0Aseveral%20real-world%20datasets%20and%20show%20that%20the%20proposed%20method%20outperforms%20the%0Acompeting%20hashing%20supervised%20methods%2C%20achieving%20up%20to%2010%5C%25%20relative%20improvement%0Aover%20the%20current%20state-of-the-art%20supervised%20hashing%20methods%2C%20and%20exhibits%20a%0Asignificantly%20better%20performance%20in%20out-of-distribution%20retrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.04288v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoopHash%253A%2520Cooperative%2520Learning%2520of%2520Multipurpose%2520Descriptor%2520and%250A%2520%2520Contrastive%2520Pair%2520Generator%2520via%2520Variational%2520MCMC%2520Teaching%2520for%2520Supervised%2520Image%250A%2520%2520Hashing%26entry.906535625%3DKhoa%2520D.%2520Doan%2520and%2520Jianwen%2520Xie%2520and%2520Yaxuan%2520Zhu%2520and%2520Yang%2520Zhao%2520and%2520Ping%2520Li%26entry.1292438233%3D%2520%2520Leveraging%2520supervised%2520information%2520can%2520lead%2520to%2520superior%2520retrieval%2520performance%250Ain%2520the%2520image%2520hashing%2520domain%2520but%2520the%2520performance%2520degrades%2520significantly%2520without%250Aenough%2520labeled%2520data.%2520One%2520effective%2520solution%2520to%2520boost%2520performance%2520is%2520to%2520employ%250Agenerative%2520models%252C%2520such%2520as%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%252C%2520to%2520generate%250Asynthetic%2520data%2520in%2520an%2520image%2520hashing%2520model.%2520However%252C%2520GAN-based%2520methods%2520are%250Adifficult%2520to%2520train%252C%2520which%2520prevents%2520the%2520hashing%2520approaches%2520from%2520jointly%2520training%250Athe%2520generative%2520models%2520and%2520the%2520hash%2520functions.%2520This%2520limitation%2520results%2520in%250Asub-optimal%2520retrieval%2520performance.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520a%250Anovel%2520framework%252C%2520the%2520generative%2520cooperative%2520hashing%2520network%252C%2520which%2520is%2520based%2520on%250Aenergy-based%2520cooperative%2520learning.%2520This%2520framework%2520jointly%2520learns%2520a%2520powerful%250Agenerative%2520representation%2520of%2520the%2520data%2520and%2520a%2520robust%2520hash%2520function%2520via%2520two%250Acomponents%253A%2520a%2520top-down%2520contrastive%2520pair%2520generator%2520that%2520synthesizes%2520contrastive%250Aimages%2520and%2520a%2520bottom-up%2520multipurpose%2520descriptor%2520that%2520simultaneously%2520represents%250Athe%2520images%2520from%2520multiple%2520perspectives%252C%2520including%2520probability%2520density%252C%2520hash%250Acode%252C%2520latent%2520code%252C%2520and%2520category.%2520The%2520two%2520components%2520are%2520jointly%2520learned%2520via%2520a%250Anovel%2520likelihood-based%2520cooperative%2520learning%2520scheme.%2520We%2520conduct%2520experiments%2520on%250Aseveral%2520real-world%2520datasets%2520and%2520show%2520that%2520the%2520proposed%2520method%2520outperforms%2520the%250Acompeting%2520hashing%2520supervised%2520methods%252C%2520achieving%2520up%2520to%252010%255C%2525%2520relative%2520improvement%250Aover%2520the%2520current%2520state-of-the-art%2520supervised%2520hashing%2520methods%252C%2520and%2520exhibits%2520a%250Asignificantly%2520better%2520performance%2520in%2520out-of-distribution%2520retrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.04288v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoopHash%3A%20Cooperative%20Learning%20of%20Multipurpose%20Descriptor%20and%0A%20%20Contrastive%20Pair%20Generator%20via%20Variational%20MCMC%20Teaching%20for%20Supervised%20Image%0A%20%20Hashing&entry.906535625=Khoa%20D.%20Doan%20and%20Jianwen%20Xie%20and%20Yaxuan%20Zhu%20and%20Yang%20Zhao%20and%20Ping%20Li&entry.1292438233=%20%20Leveraging%20supervised%20information%20can%20lead%20to%20superior%20retrieval%20performance%0Ain%20the%20image%20hashing%20domain%20but%20the%20performance%20degrades%20significantly%20without%0Aenough%20labeled%20data.%20One%20effective%20solution%20to%20boost%20performance%20is%20to%20employ%0Agenerative%20models%2C%20such%20as%20Generative%20Adversarial%20Networks%20%28GANs%29%2C%20to%20generate%0Asynthetic%20data%20in%20an%20image%20hashing%20model.%20However%2C%20GAN-based%20methods%20are%0Adifficult%20to%20train%2C%20which%20prevents%20the%20hashing%20approaches%20from%20jointly%20training%0Athe%20generative%20models%20and%20the%20hash%20functions.%20This%20limitation%20results%20in%0Asub-optimal%20retrieval%20performance.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%0Anovel%20framework%2C%20the%20generative%20cooperative%20hashing%20network%2C%20which%20is%20based%20on%0Aenergy-based%20cooperative%20learning.%20This%20framework%20jointly%20learns%20a%20powerful%0Agenerative%20representation%20of%20the%20data%20and%20a%20robust%20hash%20function%20via%20two%0Acomponents%3A%20a%20top-down%20contrastive%20pair%20generator%20that%20synthesizes%20contrastive%0Aimages%20and%20a%20bottom-up%20multipurpose%20descriptor%20that%20simultaneously%20represents%0Athe%20images%20from%20multiple%20perspectives%2C%20including%20probability%20density%2C%20hash%0Acode%2C%20latent%20code%2C%20and%20category.%20The%20two%20components%20are%20jointly%20learned%20via%20a%0Anovel%20likelihood-based%20cooperative%20learning%20scheme.%20We%20conduct%20experiments%20on%0Aseveral%20real-world%20datasets%20and%20show%20that%20the%20proposed%20method%20outperforms%20the%0Acompeting%20hashing%20supervised%20methods%2C%20achieving%20up%20to%2010%5C%25%20relative%20improvement%0Aover%20the%20current%20state-of-the-art%20supervised%20hashing%20methods%2C%20and%20exhibits%20a%0Asignificantly%20better%20performance%20in%20out-of-distribution%20retrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.04288v4&entry.124074799=Read"},
{"title": "Pre-Training Identification of Graph Winning Tickets in Adaptive\n  Spatial-Temporal Graph Neural Networks", "author": "Wenying Duan and Tianxiang Fang and Hong Rao and Xiaoxi He", "abstract": "  In this paper, we present a novel method to significantly enhance the\ncomputational efficiency of Adaptive Spatial-Temporal Graph Neural Networks\n(ASTGNNs) by introducing the concept of the Graph Winning Ticket (GWT), derived\nfrom the Lottery Ticket Hypothesis (LTH). By adopting a pre-determined star\ntopology as a GWT prior to training, we balance edge reduction with efficient\ninformation propagation, reducing computational demands while maintaining high\nmodel performance. Both the time and memory computational complexity of\ngenerating adaptive spatial-temporal graphs is significantly reduced from\n$\\mathcal{O}(N^2)$ to $\\mathcal{O}(N)$. Our approach streamlines the ASTGNN\ndeployment by eliminating the need for exhaustive training, pruning, and\nretraining cycles, and demonstrates empirically across various datasets that it\nis possible to achieve comparable performance to full models with substantially\nlower computational costs. Specifically, our approach enables training ASTGNNs\non the largest scale spatial-temporal dataset using a single A6000 equipped\nwith 48 GB of memory, overcoming the out-of-memory issue encountered during\noriginal training and even achieving state-of-the-art performance.\n{Furthermore, we delve into the effectiveness of the GWT from the perspective\nof spectral graph theory, providing substantial theoretical support.} This\nadvancement not only proves the existence of efficient sub-networks within\nASTGNNs but also broadens the applicability of the LTH in resource-constrained\nsettings, marking a significant step forward in the field of graph neural\nnetworks. Code is available at https://anonymous.4open.science/r/paper-1430.\n", "link": "http://arxiv.org/abs/2406.08287v1", "date": "2024-06-12", "relevancy": 2.6281, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5412}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5211}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pre-Training%20Identification%20of%20Graph%20Winning%20Tickets%20in%20Adaptive%0A%20%20Spatial-Temporal%20Graph%20Neural%20Networks&body=Title%3A%20Pre-Training%20Identification%20of%20Graph%20Winning%20Tickets%20in%20Adaptive%0A%20%20Spatial-Temporal%20Graph%20Neural%20Networks%0AAuthor%3A%20Wenying%20Duan%20and%20Tianxiang%20Fang%20and%20Hong%20Rao%20and%20Xiaoxi%20He%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20method%20to%20significantly%20enhance%20the%0Acomputational%20efficiency%20of%20Adaptive%20Spatial-Temporal%20Graph%20Neural%20Networks%0A%28ASTGNNs%29%20by%20introducing%20the%20concept%20of%20the%20Graph%20Winning%20Ticket%20%28GWT%29%2C%20derived%0Afrom%20the%20Lottery%20Ticket%20Hypothesis%20%28LTH%29.%20By%20adopting%20a%20pre-determined%20star%0Atopology%20as%20a%20GWT%20prior%20to%20training%2C%20we%20balance%20edge%20reduction%20with%20efficient%0Ainformation%20propagation%2C%20reducing%20computational%20demands%20while%20maintaining%20high%0Amodel%20performance.%20Both%20the%20time%20and%20memory%20computational%20complexity%20of%0Agenerating%20adaptive%20spatial-temporal%20graphs%20is%20significantly%20reduced%20from%0A%24%5Cmathcal%7BO%7D%28N%5E2%29%24%20to%20%24%5Cmathcal%7BO%7D%28N%29%24.%20Our%20approach%20streamlines%20the%20ASTGNN%0Adeployment%20by%20eliminating%20the%20need%20for%20exhaustive%20training%2C%20pruning%2C%20and%0Aretraining%20cycles%2C%20and%20demonstrates%20empirically%20across%20various%20datasets%20that%20it%0Ais%20possible%20to%20achieve%20comparable%20performance%20to%20full%20models%20with%20substantially%0Alower%20computational%20costs.%20Specifically%2C%20our%20approach%20enables%20training%20ASTGNNs%0Aon%20the%20largest%20scale%20spatial-temporal%20dataset%20using%20a%20single%20A6000%20equipped%0Awith%2048%20GB%20of%20memory%2C%20overcoming%20the%20out-of-memory%20issue%20encountered%20during%0Aoriginal%20training%20and%20even%20achieving%20state-of-the-art%20performance.%0A%7BFurthermore%2C%20we%20delve%20into%20the%20effectiveness%20of%20the%20GWT%20from%20the%20perspective%0Aof%20spectral%20graph%20theory%2C%20providing%20substantial%20theoretical%20support.%7D%20This%0Aadvancement%20not%20only%20proves%20the%20existence%20of%20efficient%20sub-networks%20within%0AASTGNNs%20but%20also%20broadens%20the%20applicability%20of%20the%20LTH%20in%20resource-constrained%0Asettings%2C%20marking%20a%20significant%20step%20forward%20in%20the%20field%20of%20graph%20neural%0Anetworks.%20Code%20is%20available%20at%20https%3A//anonymous.4open.science/r/paper-1430.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08287v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPre-Training%2520Identification%2520of%2520Graph%2520Winning%2520Tickets%2520in%2520Adaptive%250A%2520%2520Spatial-Temporal%2520Graph%2520Neural%2520Networks%26entry.906535625%3DWenying%2520Duan%2520and%2520Tianxiang%2520Fang%2520and%2520Hong%2520Rao%2520and%2520Xiaoxi%2520He%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520method%2520to%2520significantly%2520enhance%2520the%250Acomputational%2520efficiency%2520of%2520Adaptive%2520Spatial-Temporal%2520Graph%2520Neural%2520Networks%250A%2528ASTGNNs%2529%2520by%2520introducing%2520the%2520concept%2520of%2520the%2520Graph%2520Winning%2520Ticket%2520%2528GWT%2529%252C%2520derived%250Afrom%2520the%2520Lottery%2520Ticket%2520Hypothesis%2520%2528LTH%2529.%2520By%2520adopting%2520a%2520pre-determined%2520star%250Atopology%2520as%2520a%2520GWT%2520prior%2520to%2520training%252C%2520we%2520balance%2520edge%2520reduction%2520with%2520efficient%250Ainformation%2520propagation%252C%2520reducing%2520computational%2520demands%2520while%2520maintaining%2520high%250Amodel%2520performance.%2520Both%2520the%2520time%2520and%2520memory%2520computational%2520complexity%2520of%250Agenerating%2520adaptive%2520spatial-temporal%2520graphs%2520is%2520significantly%2520reduced%2520from%250A%2524%255Cmathcal%257BO%257D%2528N%255E2%2529%2524%2520to%2520%2524%255Cmathcal%257BO%257D%2528N%2529%2524.%2520Our%2520approach%2520streamlines%2520the%2520ASTGNN%250Adeployment%2520by%2520eliminating%2520the%2520need%2520for%2520exhaustive%2520training%252C%2520pruning%252C%2520and%250Aretraining%2520cycles%252C%2520and%2520demonstrates%2520empirically%2520across%2520various%2520datasets%2520that%2520it%250Ais%2520possible%2520to%2520achieve%2520comparable%2520performance%2520to%2520full%2520models%2520with%2520substantially%250Alower%2520computational%2520costs.%2520Specifically%252C%2520our%2520approach%2520enables%2520training%2520ASTGNNs%250Aon%2520the%2520largest%2520scale%2520spatial-temporal%2520dataset%2520using%2520a%2520single%2520A6000%2520equipped%250Awith%252048%2520GB%2520of%2520memory%252C%2520overcoming%2520the%2520out-of-memory%2520issue%2520encountered%2520during%250Aoriginal%2520training%2520and%2520even%2520achieving%2520state-of-the-art%2520performance.%250A%257BFurthermore%252C%2520we%2520delve%2520into%2520the%2520effectiveness%2520of%2520the%2520GWT%2520from%2520the%2520perspective%250Aof%2520spectral%2520graph%2520theory%252C%2520providing%2520substantial%2520theoretical%2520support.%257D%2520This%250Aadvancement%2520not%2520only%2520proves%2520the%2520existence%2520of%2520efficient%2520sub-networks%2520within%250AASTGNNs%2520but%2520also%2520broadens%2520the%2520applicability%2520of%2520the%2520LTH%2520in%2520resource-constrained%250Asettings%252C%2520marking%2520a%2520significant%2520step%2520forward%2520in%2520the%2520field%2520of%2520graph%2520neural%250Anetworks.%2520Code%2520is%2520available%2520at%2520https%253A//anonymous.4open.science/r/paper-1430.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08287v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-Training%20Identification%20of%20Graph%20Winning%20Tickets%20in%20Adaptive%0A%20%20Spatial-Temporal%20Graph%20Neural%20Networks&entry.906535625=Wenying%20Duan%20and%20Tianxiang%20Fang%20and%20Hong%20Rao%20and%20Xiaoxi%20He&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20novel%20method%20to%20significantly%20enhance%20the%0Acomputational%20efficiency%20of%20Adaptive%20Spatial-Temporal%20Graph%20Neural%20Networks%0A%28ASTGNNs%29%20by%20introducing%20the%20concept%20of%20the%20Graph%20Winning%20Ticket%20%28GWT%29%2C%20derived%0Afrom%20the%20Lottery%20Ticket%20Hypothesis%20%28LTH%29.%20By%20adopting%20a%20pre-determined%20star%0Atopology%20as%20a%20GWT%20prior%20to%20training%2C%20we%20balance%20edge%20reduction%20with%20efficient%0Ainformation%20propagation%2C%20reducing%20computational%20demands%20while%20maintaining%20high%0Amodel%20performance.%20Both%20the%20time%20and%20memory%20computational%20complexity%20of%0Agenerating%20adaptive%20spatial-temporal%20graphs%20is%20significantly%20reduced%20from%0A%24%5Cmathcal%7BO%7D%28N%5E2%29%24%20to%20%24%5Cmathcal%7BO%7D%28N%29%24.%20Our%20approach%20streamlines%20the%20ASTGNN%0Adeployment%20by%20eliminating%20the%20need%20for%20exhaustive%20training%2C%20pruning%2C%20and%0Aretraining%20cycles%2C%20and%20demonstrates%20empirically%20across%20various%20datasets%20that%20it%0Ais%20possible%20to%20achieve%20comparable%20performance%20to%20full%20models%20with%20substantially%0Alower%20computational%20costs.%20Specifically%2C%20our%20approach%20enables%20training%20ASTGNNs%0Aon%20the%20largest%20scale%20spatial-temporal%20dataset%20using%20a%20single%20A6000%20equipped%0Awith%2048%20GB%20of%20memory%2C%20overcoming%20the%20out-of-memory%20issue%20encountered%20during%0Aoriginal%20training%20and%20even%20achieving%20state-of-the-art%20performance.%0A%7BFurthermore%2C%20we%20delve%20into%20the%20effectiveness%20of%20the%20GWT%20from%20the%20perspective%0Aof%20spectral%20graph%20theory%2C%20providing%20substantial%20theoretical%20support.%7D%20This%0Aadvancement%20not%20only%20proves%20the%20existence%20of%20efficient%20sub-networks%20within%0AASTGNNs%20but%20also%20broadens%20the%20applicability%20of%20the%20LTH%20in%20resource-constrained%0Asettings%2C%20marking%20a%20significant%20step%20forward%20in%20the%20field%20of%20graph%20neural%0Anetworks.%20Code%20is%20available%20at%20https%3A//anonymous.4open.science/r/paper-1430.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08287v1&entry.124074799=Read"},
{"title": "Adversarial Patch for 3D Local Feature Extractor", "author": "Yu Wen Pao and Li Chang Lai and Hong-Yi Lin", "abstract": "  Local feature extractors are the cornerstone of many computer vision tasks.\nHowever, their vulnerability to adversarial attacks can significantly\ncompromise their effectiveness. This paper discusses approaches to attack\nsophisticated local feature extraction algorithms and models to achieve two\ndistinct goals: (1) forcing a match between originally non-matching image\nregions, and (2) preventing a match between originally matching regions. At the\nend of the paper, we discuss the performance and drawbacks of different patch\ngeneration methods.\n", "link": "http://arxiv.org/abs/2406.08102v1", "date": "2024-06-12", "relevancy": 2.6127, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5891}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4985}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Patch%20for%203D%20Local%20Feature%20Extractor&body=Title%3A%20Adversarial%20Patch%20for%203D%20Local%20Feature%20Extractor%0AAuthor%3A%20Yu%20Wen%20Pao%20and%20Li%20Chang%20Lai%20and%20Hong-Yi%20Lin%0AAbstract%3A%20%20%20Local%20feature%20extractors%20are%20the%20cornerstone%20of%20many%20computer%20vision%20tasks.%0AHowever%2C%20their%20vulnerability%20to%20adversarial%20attacks%20can%20significantly%0Acompromise%20their%20effectiveness.%20This%20paper%20discusses%20approaches%20to%20attack%0Asophisticated%20local%20feature%20extraction%20algorithms%20and%20models%20to%20achieve%20two%0Adistinct%20goals%3A%20%281%29%20forcing%20a%20match%20between%20originally%20non-matching%20image%0Aregions%2C%20and%20%282%29%20preventing%20a%20match%20between%20originally%20matching%20regions.%20At%20the%0Aend%20of%20the%20paper%2C%20we%20discuss%20the%20performance%20and%20drawbacks%20of%20different%20patch%0Ageneration%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08102v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Patch%2520for%25203D%2520Local%2520Feature%2520Extractor%26entry.906535625%3DYu%2520Wen%2520Pao%2520and%2520Li%2520Chang%2520Lai%2520and%2520Hong-Yi%2520Lin%26entry.1292438233%3D%2520%2520Local%2520feature%2520extractors%2520are%2520the%2520cornerstone%2520of%2520many%2520computer%2520vision%2520tasks.%250AHowever%252C%2520their%2520vulnerability%2520to%2520adversarial%2520attacks%2520can%2520significantly%250Acompromise%2520their%2520effectiveness.%2520This%2520paper%2520discusses%2520approaches%2520to%2520attack%250Asophisticated%2520local%2520feature%2520extraction%2520algorithms%2520and%2520models%2520to%2520achieve%2520two%250Adistinct%2520goals%253A%2520%25281%2529%2520forcing%2520a%2520match%2520between%2520originally%2520non-matching%2520image%250Aregions%252C%2520and%2520%25282%2529%2520preventing%2520a%2520match%2520between%2520originally%2520matching%2520regions.%2520At%2520the%250Aend%2520of%2520the%2520paper%252C%2520we%2520discuss%2520the%2520performance%2520and%2520drawbacks%2520of%2520different%2520patch%250Ageneration%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08102v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Patch%20for%203D%20Local%20Feature%20Extractor&entry.906535625=Yu%20Wen%20Pao%20and%20Li%20Chang%20Lai%20and%20Hong-Yi%20Lin&entry.1292438233=%20%20Local%20feature%20extractors%20are%20the%20cornerstone%20of%20many%20computer%20vision%20tasks.%0AHowever%2C%20their%20vulnerability%20to%20adversarial%20attacks%20can%20significantly%0Acompromise%20their%20effectiveness.%20This%20paper%20discusses%20approaches%20to%20attack%0Asophisticated%20local%20feature%20extraction%20algorithms%20and%20models%20to%20achieve%20two%0Adistinct%20goals%3A%20%281%29%20forcing%20a%20match%20between%20originally%20non-matching%20image%0Aregions%2C%20and%20%282%29%20preventing%20a%20match%20between%20originally%20matching%20regions.%20At%20the%0Aend%20of%20the%20paper%2C%20we%20discuss%20the%20performance%20and%20drawbacks%20of%20different%20patch%0Ageneration%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08102v1&entry.124074799=Read"},
{"title": "OpenCOLE: Towards Reproducible Automatic Graphic Design Generation", "author": "Naoto Inoue and Kento Masui and Wataru Shimoda and Kota Yamaguchi", "abstract": "  Automatic generation of graphic designs has recently received considerable\nattention. However, the state-of-the-art approaches are complex and rely on\nproprietary datasets, which creates reproducibility barriers. In this paper, we\npropose an open framework for automatic graphic design called OpenCOLE, where\nwe build a modified version of the pioneering COLE and train our model\nexclusively on publicly available datasets. Based on GPT4V evaluations, our\nmodel shows promising performance comparable to the original COLE. We release\nthe pipeline and training results to encourage open development.\n", "link": "http://arxiv.org/abs/2406.08232v1", "date": "2024-06-12", "relevancy": 2.6106, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5619}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5054}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenCOLE%3A%20Towards%20Reproducible%20Automatic%20Graphic%20Design%20Generation&body=Title%3A%20OpenCOLE%3A%20Towards%20Reproducible%20Automatic%20Graphic%20Design%20Generation%0AAuthor%3A%20Naoto%20Inoue%20and%20Kento%20Masui%20and%20Wataru%20Shimoda%20and%20Kota%20Yamaguchi%0AAbstract%3A%20%20%20Automatic%20generation%20of%20graphic%20designs%20has%20recently%20received%20considerable%0Aattention.%20However%2C%20the%20state-of-the-art%20approaches%20are%20complex%20and%20rely%20on%0Aproprietary%20datasets%2C%20which%20creates%20reproducibility%20barriers.%20In%20this%20paper%2C%20we%0Apropose%20an%20open%20framework%20for%20automatic%20graphic%20design%20called%20OpenCOLE%2C%20where%0Awe%20build%20a%20modified%20version%20of%20the%20pioneering%20COLE%20and%20train%20our%20model%0Aexclusively%20on%20publicly%20available%20datasets.%20Based%20on%20GPT4V%20evaluations%2C%20our%0Amodel%20shows%20promising%20performance%20comparable%20to%20the%20original%20COLE.%20We%20release%0Athe%20pipeline%20and%20training%20results%20to%20encourage%20open%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenCOLE%253A%2520Towards%2520Reproducible%2520Automatic%2520Graphic%2520Design%2520Generation%26entry.906535625%3DNaoto%2520Inoue%2520and%2520Kento%2520Masui%2520and%2520Wataru%2520Shimoda%2520and%2520Kota%2520Yamaguchi%26entry.1292438233%3D%2520%2520Automatic%2520generation%2520of%2520graphic%2520designs%2520has%2520recently%2520received%2520considerable%250Aattention.%2520However%252C%2520the%2520state-of-the-art%2520approaches%2520are%2520complex%2520and%2520rely%2520on%250Aproprietary%2520datasets%252C%2520which%2520creates%2520reproducibility%2520barriers.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520an%2520open%2520framework%2520for%2520automatic%2520graphic%2520design%2520called%2520OpenCOLE%252C%2520where%250Awe%2520build%2520a%2520modified%2520version%2520of%2520the%2520pioneering%2520COLE%2520and%2520train%2520our%2520model%250Aexclusively%2520on%2520publicly%2520available%2520datasets.%2520Based%2520on%2520GPT4V%2520evaluations%252C%2520our%250Amodel%2520shows%2520promising%2520performance%2520comparable%2520to%2520the%2520original%2520COLE.%2520We%2520release%250Athe%2520pipeline%2520and%2520training%2520results%2520to%2520encourage%2520open%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenCOLE%3A%20Towards%20Reproducible%20Automatic%20Graphic%20Design%20Generation&entry.906535625=Naoto%20Inoue%20and%20Kento%20Masui%20and%20Wataru%20Shimoda%20and%20Kota%20Yamaguchi&entry.1292438233=%20%20Automatic%20generation%20of%20graphic%20designs%20has%20recently%20received%20considerable%0Aattention.%20However%2C%20the%20state-of-the-art%20approaches%20are%20complex%20and%20rely%20on%0Aproprietary%20datasets%2C%20which%20creates%20reproducibility%20barriers.%20In%20this%20paper%2C%20we%0Apropose%20an%20open%20framework%20for%20automatic%20graphic%20design%20called%20OpenCOLE%2C%20where%0Awe%20build%20a%20modified%20version%20of%20the%20pioneering%20COLE%20and%20train%20our%20model%0Aexclusively%20on%20publicly%20available%20datasets.%20Based%20on%20GPT4V%20evaluations%2C%20our%0Amodel%20shows%20promising%20performance%20comparable%20to%20the%20original%20COLE.%20We%20release%0Athe%20pipeline%20and%20training%20results%20to%20encourage%20open%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08232v1&entry.124074799=Read"},
{"title": "SPIN: Spacecraft Imagery for Navigation", "author": "Javier Montalvo and Juan Ignacio Bravo P\u00e9rez-Villar and \u00c1lvaro Garc\u00eda-Mart\u00edn and Pablo Carballeira and Jes\u00fas Besc'os", "abstract": "  Data acquired in space operational conditions is scarce due to the costs and\ncomplexity of space operations. This poses a challenge to learning-based\nvisual-based navigation algorithms employed in autonomous spacecraft\nnavigation. Existing datasets, which largely depend on computer-simulated data,\nhave partially filled this gap. However, the image generation tools they use\nare proprietary, which limits the evaluation of methods to unseen scenarios.\nFurthermore, these datasets provide limited ground-truth data, primarily\nfocusing on the spacecraft's translation and rotation relative to the camera.\nTo address these limitations, we present SPIN (SPacecraft Imagery for\nNavigation), an open-source realistic spacecraft image generation tool for\nrelative navigation between two spacecrafts. SPIN provides a wide variety of\nground-truth data and allows researchers to employ custom 3D models of\nsatellites, define specific camera-relative poses, and adjust various settings\nsuch as camera parameters and environmental illumination conditions. For the\ntask of spacecraft pose estimation, we compare the results of training with a\nSPIN-generated dataset against existing synthetic datasets. We show a %50\naverage error reduction in common testbed data (that simulates realistic space\nconditions). Both the SPIN tool (and source code) and our enhanced version of\nthe synthetic datasets will be publicly released upon paper acceptance on\nGitHub https://github.com/vpulab/SPIN.\n", "link": "http://arxiv.org/abs/2406.07500v2", "date": "2024-06-12", "relevancy": 2.5751, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5178}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5141}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPIN%3A%20Spacecraft%20Imagery%20for%20Navigation&body=Title%3A%20SPIN%3A%20Spacecraft%20Imagery%20for%20Navigation%0AAuthor%3A%20Javier%20Montalvo%20and%20Juan%20Ignacio%20Bravo%20P%C3%A9rez-Villar%20and%20%C3%81lvaro%20Garc%C3%ADa-Mart%C3%ADn%20and%20Pablo%20Carballeira%20and%20Jes%C3%BAs%20Besc%27os%0AAbstract%3A%20%20%20Data%20acquired%20in%20space%20operational%20conditions%20is%20scarce%20due%20to%20the%20costs%20and%0Acomplexity%20of%20space%20operations.%20This%20poses%20a%20challenge%20to%20learning-based%0Avisual-based%20navigation%20algorithms%20employed%20in%20autonomous%20spacecraft%0Anavigation.%20Existing%20datasets%2C%20which%20largely%20depend%20on%20computer-simulated%20data%2C%0Ahave%20partially%20filled%20this%20gap.%20However%2C%20the%20image%20generation%20tools%20they%20use%0Aare%20proprietary%2C%20which%20limits%20the%20evaluation%20of%20methods%20to%20unseen%20scenarios.%0AFurthermore%2C%20these%20datasets%20provide%20limited%20ground-truth%20data%2C%20primarily%0Afocusing%20on%20the%20spacecraft%27s%20translation%20and%20rotation%20relative%20to%20the%20camera.%0ATo%20address%20these%20limitations%2C%20we%20present%20SPIN%20%28SPacecraft%20Imagery%20for%0ANavigation%29%2C%20an%20open-source%20realistic%20spacecraft%20image%20generation%20tool%20for%0Arelative%20navigation%20between%20two%20spacecrafts.%20SPIN%20provides%20a%20wide%20variety%20of%0Aground-truth%20data%20and%20allows%20researchers%20to%20employ%20custom%203D%20models%20of%0Asatellites%2C%20define%20specific%20camera-relative%20poses%2C%20and%20adjust%20various%20settings%0Asuch%20as%20camera%20parameters%20and%20environmental%20illumination%20conditions.%20For%20the%0Atask%20of%20spacecraft%20pose%20estimation%2C%20we%20compare%20the%20results%20of%20training%20with%20a%0ASPIN-generated%20dataset%20against%20existing%20synthetic%20datasets.%20We%20show%20a%20%2550%0Aaverage%20error%20reduction%20in%20common%20testbed%20data%20%28that%20simulates%20realistic%20space%0Aconditions%29.%20Both%20the%20SPIN%20tool%20%28and%20source%20code%29%20and%20our%20enhanced%20version%20of%0Athe%20synthetic%20datasets%20will%20be%20publicly%20released%20upon%20paper%20acceptance%20on%0AGitHub%20https%3A//github.com/vpulab/SPIN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07500v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPIN%253A%2520Spacecraft%2520Imagery%2520for%2520Navigation%26entry.906535625%3DJavier%2520Montalvo%2520and%2520Juan%2520Ignacio%2520Bravo%2520P%25C3%25A9rez-Villar%2520and%2520%25C3%2581lvaro%2520Garc%25C3%25ADa-Mart%25C3%25ADn%2520and%2520Pablo%2520Carballeira%2520and%2520Jes%25C3%25BAs%2520Besc%2527os%26entry.1292438233%3D%2520%2520Data%2520acquired%2520in%2520space%2520operational%2520conditions%2520is%2520scarce%2520due%2520to%2520the%2520costs%2520and%250Acomplexity%2520of%2520space%2520operations.%2520This%2520poses%2520a%2520challenge%2520to%2520learning-based%250Avisual-based%2520navigation%2520algorithms%2520employed%2520in%2520autonomous%2520spacecraft%250Anavigation.%2520Existing%2520datasets%252C%2520which%2520largely%2520depend%2520on%2520computer-simulated%2520data%252C%250Ahave%2520partially%2520filled%2520this%2520gap.%2520However%252C%2520the%2520image%2520generation%2520tools%2520they%2520use%250Aare%2520proprietary%252C%2520which%2520limits%2520the%2520evaluation%2520of%2520methods%2520to%2520unseen%2520scenarios.%250AFurthermore%252C%2520these%2520datasets%2520provide%2520limited%2520ground-truth%2520data%252C%2520primarily%250Afocusing%2520on%2520the%2520spacecraft%2527s%2520translation%2520and%2520rotation%2520relative%2520to%2520the%2520camera.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520present%2520SPIN%2520%2528SPacecraft%2520Imagery%2520for%250ANavigation%2529%252C%2520an%2520open-source%2520realistic%2520spacecraft%2520image%2520generation%2520tool%2520for%250Arelative%2520navigation%2520between%2520two%2520spacecrafts.%2520SPIN%2520provides%2520a%2520wide%2520variety%2520of%250Aground-truth%2520data%2520and%2520allows%2520researchers%2520to%2520employ%2520custom%25203D%2520models%2520of%250Asatellites%252C%2520define%2520specific%2520camera-relative%2520poses%252C%2520and%2520adjust%2520various%2520settings%250Asuch%2520as%2520camera%2520parameters%2520and%2520environmental%2520illumination%2520conditions.%2520For%2520the%250Atask%2520of%2520spacecraft%2520pose%2520estimation%252C%2520we%2520compare%2520the%2520results%2520of%2520training%2520with%2520a%250ASPIN-generated%2520dataset%2520against%2520existing%2520synthetic%2520datasets.%2520We%2520show%2520a%2520%252550%250Aaverage%2520error%2520reduction%2520in%2520common%2520testbed%2520data%2520%2528that%2520simulates%2520realistic%2520space%250Aconditions%2529.%2520Both%2520the%2520SPIN%2520tool%2520%2528and%2520source%2520code%2529%2520and%2520our%2520enhanced%2520version%2520of%250Athe%2520synthetic%2520datasets%2520will%2520be%2520publicly%2520released%2520upon%2520paper%2520acceptance%2520on%250AGitHub%2520https%253A//github.com/vpulab/SPIN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07500v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPIN%3A%20Spacecraft%20Imagery%20for%20Navigation&entry.906535625=Javier%20Montalvo%20and%20Juan%20Ignacio%20Bravo%20P%C3%A9rez-Villar%20and%20%C3%81lvaro%20Garc%C3%ADa-Mart%C3%ADn%20and%20Pablo%20Carballeira%20and%20Jes%C3%BAs%20Besc%27os&entry.1292438233=%20%20Data%20acquired%20in%20space%20operational%20conditions%20is%20scarce%20due%20to%20the%20costs%20and%0Acomplexity%20of%20space%20operations.%20This%20poses%20a%20challenge%20to%20learning-based%0Avisual-based%20navigation%20algorithms%20employed%20in%20autonomous%20spacecraft%0Anavigation.%20Existing%20datasets%2C%20which%20largely%20depend%20on%20computer-simulated%20data%2C%0Ahave%20partially%20filled%20this%20gap.%20However%2C%20the%20image%20generation%20tools%20they%20use%0Aare%20proprietary%2C%20which%20limits%20the%20evaluation%20of%20methods%20to%20unseen%20scenarios.%0AFurthermore%2C%20these%20datasets%20provide%20limited%20ground-truth%20data%2C%20primarily%0Afocusing%20on%20the%20spacecraft%27s%20translation%20and%20rotation%20relative%20to%20the%20camera.%0ATo%20address%20these%20limitations%2C%20we%20present%20SPIN%20%28SPacecraft%20Imagery%20for%0ANavigation%29%2C%20an%20open-source%20realistic%20spacecraft%20image%20generation%20tool%20for%0Arelative%20navigation%20between%20two%20spacecrafts.%20SPIN%20provides%20a%20wide%20variety%20of%0Aground-truth%20data%20and%20allows%20researchers%20to%20employ%20custom%203D%20models%20of%0Asatellites%2C%20define%20specific%20camera-relative%20poses%2C%20and%20adjust%20various%20settings%0Asuch%20as%20camera%20parameters%20and%20environmental%20illumination%20conditions.%20For%20the%0Atask%20of%20spacecraft%20pose%20estimation%2C%20we%20compare%20the%20results%20of%20training%20with%20a%0ASPIN-generated%20dataset%20against%20existing%20synthetic%20datasets.%20We%20show%20a%20%2550%0Aaverage%20error%20reduction%20in%20common%20testbed%20data%20%28that%20simulates%20realistic%20space%0Aconditions%29.%20Both%20the%20SPIN%20tool%20%28and%20source%20code%29%20and%20our%20enhanced%20version%20of%0Athe%20synthetic%20datasets%20will%20be%20publicly%20released%20upon%20paper%20acceptance%20on%0AGitHub%20https%3A//github.com/vpulab/SPIN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07500v2&entry.124074799=Read"},
{"title": "Continuous fake media detection: adapting deepfake detectors to new\n  generative techniques", "author": "Francesco Tassone and Luca Maiano and Irene Amerini", "abstract": "  Generative techniques continue to evolve at an impressively high rate, driven\nby the hype about these technologies. This rapid advancement severely limits\nthe application of deepfake detectors, which, despite numerous efforts by the\nscientific community, struggle to achieve sufficiently robust performance\nagainst the ever-changing content. To address these limitations, in this paper,\nwe propose an analysis of two continuous learning techniques on a Short and a\nLong sequence of fake media. Both sequences include a complex and heterogeneous\nrange of deepfakes generated from GANs, computer graphics techniques, and\nunknown sources. Our study shows that continual learning could be important in\nmitigating the need for generalizability. In fact, we show that, although with\nsome limitations, continual learning methods help to maintain good performance\nacross the entire training sequence. For these techniques to work in a\nsufficiently robust way, however, it is necessary that the tasks in the\nsequence share similarities. In fact, according to our experiments, the order\nand similarity of the tasks can affect the performance of the models over time.\nTo address this problem, we show that it is possible to group tasks based on\ntheir similarity. This small measure allows for a significant improvement even\nin longer sequences. This result suggests that continual techniques can be\ncombined with the most promising detection methods, allowing them to catch up\nwith the latest generative techniques. In addition to this, we propose an\noverview of how this learning approach can be integrated into a deepfake\ndetection pipeline for continuous integration and continuous deployment\n(CI/CD). This allows you to keep track of different funds, such as social\nnetworks, new generative tools, or third-party datasets, and through the\nintegration of continuous learning, allows constant maintenance of the\ndetectors.\n", "link": "http://arxiv.org/abs/2406.08171v1", "date": "2024-06-12", "relevancy": 2.5747, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5248}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.522}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuous%20fake%20media%20detection%3A%20adapting%20deepfake%20detectors%20to%20new%0A%20%20generative%20techniques&body=Title%3A%20Continuous%20fake%20media%20detection%3A%20adapting%20deepfake%20detectors%20to%20new%0A%20%20generative%20techniques%0AAuthor%3A%20Francesco%20Tassone%20and%20Luca%20Maiano%20and%20Irene%20Amerini%0AAbstract%3A%20%20%20Generative%20techniques%20continue%20to%20evolve%20at%20an%20impressively%20high%20rate%2C%20driven%0Aby%20the%20hype%20about%20these%20technologies.%20This%20rapid%20advancement%20severely%20limits%0Athe%20application%20of%20deepfake%20detectors%2C%20which%2C%20despite%20numerous%20efforts%20by%20the%0Ascientific%20community%2C%20struggle%20to%20achieve%20sufficiently%20robust%20performance%0Aagainst%20the%20ever-changing%20content.%20To%20address%20these%20limitations%2C%20in%20this%20paper%2C%0Awe%20propose%20an%20analysis%20of%20two%20continuous%20learning%20techniques%20on%20a%20Short%20and%20a%0ALong%20sequence%20of%20fake%20media.%20Both%20sequences%20include%20a%20complex%20and%20heterogeneous%0Arange%20of%20deepfakes%20generated%20from%20GANs%2C%20computer%20graphics%20techniques%2C%20and%0Aunknown%20sources.%20Our%20study%20shows%20that%20continual%20learning%20could%20be%20important%20in%0Amitigating%20the%20need%20for%20generalizability.%20In%20fact%2C%20we%20show%20that%2C%20although%20with%0Asome%20limitations%2C%20continual%20learning%20methods%20help%20to%20maintain%20good%20performance%0Aacross%20the%20entire%20training%20sequence.%20For%20these%20techniques%20to%20work%20in%20a%0Asufficiently%20robust%20way%2C%20however%2C%20it%20is%20necessary%20that%20the%20tasks%20in%20the%0Asequence%20share%20similarities.%20In%20fact%2C%20according%20to%20our%20experiments%2C%20the%20order%0Aand%20similarity%20of%20the%20tasks%20can%20affect%20the%20performance%20of%20the%20models%20over%20time.%0ATo%20address%20this%20problem%2C%20we%20show%20that%20it%20is%20possible%20to%20group%20tasks%20based%20on%0Atheir%20similarity.%20This%20small%20measure%20allows%20for%20a%20significant%20improvement%20even%0Ain%20longer%20sequences.%20This%20result%20suggests%20that%20continual%20techniques%20can%20be%0Acombined%20with%20the%20most%20promising%20detection%20methods%2C%20allowing%20them%20to%20catch%20up%0Awith%20the%20latest%20generative%20techniques.%20In%20addition%20to%20this%2C%20we%20propose%20an%0Aoverview%20of%20how%20this%20learning%20approach%20can%20be%20integrated%20into%20a%20deepfake%0Adetection%20pipeline%20for%20continuous%20integration%20and%20continuous%20deployment%0A%28CI/CD%29.%20This%20allows%20you%20to%20keep%20track%20of%20different%20funds%2C%20such%20as%20social%0Anetworks%2C%20new%20generative%20tools%2C%20or%20third-party%20datasets%2C%20and%20through%20the%0Aintegration%20of%20continuous%20learning%2C%20allows%20constant%20maintenance%20of%20the%0Adetectors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08171v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuous%2520fake%2520media%2520detection%253A%2520adapting%2520deepfake%2520detectors%2520to%2520new%250A%2520%2520generative%2520techniques%26entry.906535625%3DFrancesco%2520Tassone%2520and%2520Luca%2520Maiano%2520and%2520Irene%2520Amerini%26entry.1292438233%3D%2520%2520Generative%2520techniques%2520continue%2520to%2520evolve%2520at%2520an%2520impressively%2520high%2520rate%252C%2520driven%250Aby%2520the%2520hype%2520about%2520these%2520technologies.%2520This%2520rapid%2520advancement%2520severely%2520limits%250Athe%2520application%2520of%2520deepfake%2520detectors%252C%2520which%252C%2520despite%2520numerous%2520efforts%2520by%2520the%250Ascientific%2520community%252C%2520struggle%2520to%2520achieve%2520sufficiently%2520robust%2520performance%250Aagainst%2520the%2520ever-changing%2520content.%2520To%2520address%2520these%2520limitations%252C%2520in%2520this%2520paper%252C%250Awe%2520propose%2520an%2520analysis%2520of%2520two%2520continuous%2520learning%2520techniques%2520on%2520a%2520Short%2520and%2520a%250ALong%2520sequence%2520of%2520fake%2520media.%2520Both%2520sequences%2520include%2520a%2520complex%2520and%2520heterogeneous%250Arange%2520of%2520deepfakes%2520generated%2520from%2520GANs%252C%2520computer%2520graphics%2520techniques%252C%2520and%250Aunknown%2520sources.%2520Our%2520study%2520shows%2520that%2520continual%2520learning%2520could%2520be%2520important%2520in%250Amitigating%2520the%2520need%2520for%2520generalizability.%2520In%2520fact%252C%2520we%2520show%2520that%252C%2520although%2520with%250Asome%2520limitations%252C%2520continual%2520learning%2520methods%2520help%2520to%2520maintain%2520good%2520performance%250Aacross%2520the%2520entire%2520training%2520sequence.%2520For%2520these%2520techniques%2520to%2520work%2520in%2520a%250Asufficiently%2520robust%2520way%252C%2520however%252C%2520it%2520is%2520necessary%2520that%2520the%2520tasks%2520in%2520the%250Asequence%2520share%2520similarities.%2520In%2520fact%252C%2520according%2520to%2520our%2520experiments%252C%2520the%2520order%250Aand%2520similarity%2520of%2520the%2520tasks%2520can%2520affect%2520the%2520performance%2520of%2520the%2520models%2520over%2520time.%250ATo%2520address%2520this%2520problem%252C%2520we%2520show%2520that%2520it%2520is%2520possible%2520to%2520group%2520tasks%2520based%2520on%250Atheir%2520similarity.%2520This%2520small%2520measure%2520allows%2520for%2520a%2520significant%2520improvement%2520even%250Ain%2520longer%2520sequences.%2520This%2520result%2520suggests%2520that%2520continual%2520techniques%2520can%2520be%250Acombined%2520with%2520the%2520most%2520promising%2520detection%2520methods%252C%2520allowing%2520them%2520to%2520catch%2520up%250Awith%2520the%2520latest%2520generative%2520techniques.%2520In%2520addition%2520to%2520this%252C%2520we%2520propose%2520an%250Aoverview%2520of%2520how%2520this%2520learning%2520approach%2520can%2520be%2520integrated%2520into%2520a%2520deepfake%250Adetection%2520pipeline%2520for%2520continuous%2520integration%2520and%2520continuous%2520deployment%250A%2528CI/CD%2529.%2520This%2520allows%2520you%2520to%2520keep%2520track%2520of%2520different%2520funds%252C%2520such%2520as%2520social%250Anetworks%252C%2520new%2520generative%2520tools%252C%2520or%2520third-party%2520datasets%252C%2520and%2520through%2520the%250Aintegration%2520of%2520continuous%2520learning%252C%2520allows%2520constant%2520maintenance%2520of%2520the%250Adetectors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08171v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuous%20fake%20media%20detection%3A%20adapting%20deepfake%20detectors%20to%20new%0A%20%20generative%20techniques&entry.906535625=Francesco%20Tassone%20and%20Luca%20Maiano%20and%20Irene%20Amerini&entry.1292438233=%20%20Generative%20techniques%20continue%20to%20evolve%20at%20an%20impressively%20high%20rate%2C%20driven%0Aby%20the%20hype%20about%20these%20technologies.%20This%20rapid%20advancement%20severely%20limits%0Athe%20application%20of%20deepfake%20detectors%2C%20which%2C%20despite%20numerous%20efforts%20by%20the%0Ascientific%20community%2C%20struggle%20to%20achieve%20sufficiently%20robust%20performance%0Aagainst%20the%20ever-changing%20content.%20To%20address%20these%20limitations%2C%20in%20this%20paper%2C%0Awe%20propose%20an%20analysis%20of%20two%20continuous%20learning%20techniques%20on%20a%20Short%20and%20a%0ALong%20sequence%20of%20fake%20media.%20Both%20sequences%20include%20a%20complex%20and%20heterogeneous%0Arange%20of%20deepfakes%20generated%20from%20GANs%2C%20computer%20graphics%20techniques%2C%20and%0Aunknown%20sources.%20Our%20study%20shows%20that%20continual%20learning%20could%20be%20important%20in%0Amitigating%20the%20need%20for%20generalizability.%20In%20fact%2C%20we%20show%20that%2C%20although%20with%0Asome%20limitations%2C%20continual%20learning%20methods%20help%20to%20maintain%20good%20performance%0Aacross%20the%20entire%20training%20sequence.%20For%20these%20techniques%20to%20work%20in%20a%0Asufficiently%20robust%20way%2C%20however%2C%20it%20is%20necessary%20that%20the%20tasks%20in%20the%0Asequence%20share%20similarities.%20In%20fact%2C%20according%20to%20our%20experiments%2C%20the%20order%0Aand%20similarity%20of%20the%20tasks%20can%20affect%20the%20performance%20of%20the%20models%20over%20time.%0ATo%20address%20this%20problem%2C%20we%20show%20that%20it%20is%20possible%20to%20group%20tasks%20based%20on%0Atheir%20similarity.%20This%20small%20measure%20allows%20for%20a%20significant%20improvement%20even%0Ain%20longer%20sequences.%20This%20result%20suggests%20that%20continual%20techniques%20can%20be%0Acombined%20with%20the%20most%20promising%20detection%20methods%2C%20allowing%20them%20to%20catch%20up%0Awith%20the%20latest%20generative%20techniques.%20In%20addition%20to%20this%2C%20we%20propose%20an%0Aoverview%20of%20how%20this%20learning%20approach%20can%20be%20integrated%20into%20a%20deepfake%0Adetection%20pipeline%20for%20continuous%20integration%20and%20continuous%20deployment%0A%28CI/CD%29.%20This%20allows%20you%20to%20keep%20track%20of%20different%20funds%2C%20such%20as%20social%0Anetworks%2C%20new%20generative%20tools%2C%20or%20third-party%20datasets%2C%20and%20through%20the%0Aintegration%20of%20continuous%20learning%2C%20allows%20constant%20maintenance%20of%20the%0Adetectors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08171v1&entry.124074799=Read"},
{"title": "Identification of Conversation Partners from Egocentric Video", "author": "Tobias Dorszewski and S\u00f8ren A. Fuglsang and Jens Hjortkj\u00e6r", "abstract": "  Communicating in noisy, multi-talker environments is challenging, especially\nfor people with hearing impairments. Egocentric video data can potentially be\nused to identify a user's conversation partners, which could be used to inform\nselective acoustic amplification of relevant speakers. Recent introduction of\ndatasets and tasks in computer vision enable progress towards analyzing social\ninteractions from an egocentric perspective. Building on this, we focus on the\ntask of identifying conversation partners from egocentric video and describe a\nsuitable dataset. Our dataset comprises 69 hours of egocentric video of diverse\nmulti-conversation scenarios where each individual was assigned one or more\nconversation partners, providing the labels for our computer vision task. This\ndataset enables the development and assessment of algorithms for identifying\nconversation partners and evaluating related approaches. Here, we describe the\ndataset alongside initial baseline results of this ongoing work, aiming to\ncontribute to the exciting advancements in egocentric video analysis for social\nsettings.\n", "link": "http://arxiv.org/abs/2406.08089v1", "date": "2024-06-12", "relevancy": 2.5575, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5317}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5154}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Identification%20of%20Conversation%20Partners%20from%20Egocentric%20Video&body=Title%3A%20Identification%20of%20Conversation%20Partners%20from%20Egocentric%20Video%0AAuthor%3A%20Tobias%20Dorszewski%20and%20S%C3%B8ren%20A.%20Fuglsang%20and%20Jens%20Hjortkj%C3%A6r%0AAbstract%3A%20%20%20Communicating%20in%20noisy%2C%20multi-talker%20environments%20is%20challenging%2C%20especially%0Afor%20people%20with%20hearing%20impairments.%20Egocentric%20video%20data%20can%20potentially%20be%0Aused%20to%20identify%20a%20user%27s%20conversation%20partners%2C%20which%20could%20be%20used%20to%20inform%0Aselective%20acoustic%20amplification%20of%20relevant%20speakers.%20Recent%20introduction%20of%0Adatasets%20and%20tasks%20in%20computer%20vision%20enable%20progress%20towards%20analyzing%20social%0Ainteractions%20from%20an%20egocentric%20perspective.%20Building%20on%20this%2C%20we%20focus%20on%20the%0Atask%20of%20identifying%20conversation%20partners%20from%20egocentric%20video%20and%20describe%20a%0Asuitable%20dataset.%20Our%20dataset%20comprises%2069%20hours%20of%20egocentric%20video%20of%20diverse%0Amulti-conversation%20scenarios%20where%20each%20individual%20was%20assigned%20one%20or%20more%0Aconversation%20partners%2C%20providing%20the%20labels%20for%20our%20computer%20vision%20task.%20This%0Adataset%20enables%20the%20development%20and%20assessment%20of%20algorithms%20for%20identifying%0Aconversation%20partners%20and%20evaluating%20related%20approaches.%20Here%2C%20we%20describe%20the%0Adataset%20alongside%20initial%20baseline%20results%20of%20this%20ongoing%20work%2C%20aiming%20to%0Acontribute%20to%20the%20exciting%20advancements%20in%20egocentric%20video%20analysis%20for%20social%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdentification%2520of%2520Conversation%2520Partners%2520from%2520Egocentric%2520Video%26entry.906535625%3DTobias%2520Dorszewski%2520and%2520S%25C3%25B8ren%2520A.%2520Fuglsang%2520and%2520Jens%2520Hjortkj%25C3%25A6r%26entry.1292438233%3D%2520%2520Communicating%2520in%2520noisy%252C%2520multi-talker%2520environments%2520is%2520challenging%252C%2520especially%250Afor%2520people%2520with%2520hearing%2520impairments.%2520Egocentric%2520video%2520data%2520can%2520potentially%2520be%250Aused%2520to%2520identify%2520a%2520user%2527s%2520conversation%2520partners%252C%2520which%2520could%2520be%2520used%2520to%2520inform%250Aselective%2520acoustic%2520amplification%2520of%2520relevant%2520speakers.%2520Recent%2520introduction%2520of%250Adatasets%2520and%2520tasks%2520in%2520computer%2520vision%2520enable%2520progress%2520towards%2520analyzing%2520social%250Ainteractions%2520from%2520an%2520egocentric%2520perspective.%2520Building%2520on%2520this%252C%2520we%2520focus%2520on%2520the%250Atask%2520of%2520identifying%2520conversation%2520partners%2520from%2520egocentric%2520video%2520and%2520describe%2520a%250Asuitable%2520dataset.%2520Our%2520dataset%2520comprises%252069%2520hours%2520of%2520egocentric%2520video%2520of%2520diverse%250Amulti-conversation%2520scenarios%2520where%2520each%2520individual%2520was%2520assigned%2520one%2520or%2520more%250Aconversation%2520partners%252C%2520providing%2520the%2520labels%2520for%2520our%2520computer%2520vision%2520task.%2520This%250Adataset%2520enables%2520the%2520development%2520and%2520assessment%2520of%2520algorithms%2520for%2520identifying%250Aconversation%2520partners%2520and%2520evaluating%2520related%2520approaches.%2520Here%252C%2520we%2520describe%2520the%250Adataset%2520alongside%2520initial%2520baseline%2520results%2520of%2520this%2520ongoing%2520work%252C%2520aiming%2520to%250Acontribute%2520to%2520the%2520exciting%2520advancements%2520in%2520egocentric%2520video%2520analysis%2520for%2520social%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identification%20of%20Conversation%20Partners%20from%20Egocentric%20Video&entry.906535625=Tobias%20Dorszewski%20and%20S%C3%B8ren%20A.%20Fuglsang%20and%20Jens%20Hjortkj%C3%A6r&entry.1292438233=%20%20Communicating%20in%20noisy%2C%20multi-talker%20environments%20is%20challenging%2C%20especially%0Afor%20people%20with%20hearing%20impairments.%20Egocentric%20video%20data%20can%20potentially%20be%0Aused%20to%20identify%20a%20user%27s%20conversation%20partners%2C%20which%20could%20be%20used%20to%20inform%0Aselective%20acoustic%20amplification%20of%20relevant%20speakers.%20Recent%20introduction%20of%0Adatasets%20and%20tasks%20in%20computer%20vision%20enable%20progress%20towards%20analyzing%20social%0Ainteractions%20from%20an%20egocentric%20perspective.%20Building%20on%20this%2C%20we%20focus%20on%20the%0Atask%20of%20identifying%20conversation%20partners%20from%20egocentric%20video%20and%20describe%20a%0Asuitable%20dataset.%20Our%20dataset%20comprises%2069%20hours%20of%20egocentric%20video%20of%20diverse%0Amulti-conversation%20scenarios%20where%20each%20individual%20was%20assigned%20one%20or%20more%0Aconversation%20partners%2C%20providing%20the%20labels%20for%20our%20computer%20vision%20task.%20This%0Adataset%20enables%20the%20development%20and%20assessment%20of%20algorithms%20for%20identifying%0Aconversation%20partners%20and%20evaluating%20related%20approaches.%20Here%2C%20we%20describe%20the%0Adataset%20alongside%20initial%20baseline%20results%20of%20this%20ongoing%20work%2C%20aiming%20to%0Acontribute%20to%20the%20exciting%20advancements%20in%20egocentric%20video%20analysis%20for%20social%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08089v1&entry.124074799=Read"},
{"title": "RMem: Restricted Memory Banks Improve Video Object Segmentation", "author": "Junbao Zhou and Ziqi Pang and Yu-Xiong Wang", "abstract": "  With recent video object segmentation (VOS) benchmarks evolving to\nchallenging scenarios, we revisit a simple but overlooked strategy: restricting\nthe size of memory banks. This diverges from the prevalent practice of\nexpanding memory banks to accommodate extensive historical information. Our\nspecially designed \"memory deciphering\" study offers a pivotal insight\nunderpinning such a strategy: expanding memory banks, while seemingly\nbeneficial, actually increases the difficulty for VOS modules to decode\nrelevant features due to the confusion from redundant information. By\nrestricting memory banks to a limited number of essential frames, we achieve a\nnotable improvement in VOS accuracy. This process balances the importance and\nfreshness of frames to maintain an informative memory bank within a bounded\ncapacity. Additionally, restricted memory banks reduce the training-inference\ndiscrepancy in memory lengths compared with continuous expansion. This fosters\nnew opportunities in temporal reasoning and enables us to introduce the\npreviously overlooked \"temporal positional embedding.\" Finally, our insights\nare embodied in \"RMem\" (\"R\" for restricted), a simple yet effective VOS\nmodification that excels at challenging VOS scenarios and establishes new state\nof the art for object state changes (on the VOST dataset) and long videos (on\nthe Long Videos dataset). Our code and demo are available at\nhttps://restricted-memory.github.io/.\n", "link": "http://arxiv.org/abs/2406.08476v1", "date": "2024-06-12", "relevancy": 2.5486, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5143}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5118}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RMem%3A%20Restricted%20Memory%20Banks%20Improve%20Video%20Object%20Segmentation&body=Title%3A%20RMem%3A%20Restricted%20Memory%20Banks%20Improve%20Video%20Object%20Segmentation%0AAuthor%3A%20Junbao%20Zhou%20and%20Ziqi%20Pang%20and%20Yu-Xiong%20Wang%0AAbstract%3A%20%20%20With%20recent%20video%20object%20segmentation%20%28VOS%29%20benchmarks%20evolving%20to%0Achallenging%20scenarios%2C%20we%20revisit%20a%20simple%20but%20overlooked%20strategy%3A%20restricting%0Athe%20size%20of%20memory%20banks.%20This%20diverges%20from%20the%20prevalent%20practice%20of%0Aexpanding%20memory%20banks%20to%20accommodate%20extensive%20historical%20information.%20Our%0Aspecially%20designed%20%22memory%20deciphering%22%20study%20offers%20a%20pivotal%20insight%0Aunderpinning%20such%20a%20strategy%3A%20expanding%20memory%20banks%2C%20while%20seemingly%0Abeneficial%2C%20actually%20increases%20the%20difficulty%20for%20VOS%20modules%20to%20decode%0Arelevant%20features%20due%20to%20the%20confusion%20from%20redundant%20information.%20By%0Arestricting%20memory%20banks%20to%20a%20limited%20number%20of%20essential%20frames%2C%20we%20achieve%20a%0Anotable%20improvement%20in%20VOS%20accuracy.%20This%20process%20balances%20the%20importance%20and%0Afreshness%20of%20frames%20to%20maintain%20an%20informative%20memory%20bank%20within%20a%20bounded%0Acapacity.%20Additionally%2C%20restricted%20memory%20banks%20reduce%20the%20training-inference%0Adiscrepancy%20in%20memory%20lengths%20compared%20with%20continuous%20expansion.%20This%20fosters%0Anew%20opportunities%20in%20temporal%20reasoning%20and%20enables%20us%20to%20introduce%20the%0Apreviously%20overlooked%20%22temporal%20positional%20embedding.%22%20Finally%2C%20our%20insights%0Aare%20embodied%20in%20%22RMem%22%20%28%22R%22%20for%20restricted%29%2C%20a%20simple%20yet%20effective%20VOS%0Amodification%20that%20excels%20at%20challenging%20VOS%20scenarios%20and%20establishes%20new%20state%0Aof%20the%20art%20for%20object%20state%20changes%20%28on%20the%20VOST%20dataset%29%20and%20long%20videos%20%28on%0Athe%20Long%20Videos%20dataset%29.%20Our%20code%20and%20demo%20are%20available%20at%0Ahttps%3A//restricted-memory.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRMem%253A%2520Restricted%2520Memory%2520Banks%2520Improve%2520Video%2520Object%2520Segmentation%26entry.906535625%3DJunbao%2520Zhou%2520and%2520Ziqi%2520Pang%2520and%2520Yu-Xiong%2520Wang%26entry.1292438233%3D%2520%2520With%2520recent%2520video%2520object%2520segmentation%2520%2528VOS%2529%2520benchmarks%2520evolving%2520to%250Achallenging%2520scenarios%252C%2520we%2520revisit%2520a%2520simple%2520but%2520overlooked%2520strategy%253A%2520restricting%250Athe%2520size%2520of%2520memory%2520banks.%2520This%2520diverges%2520from%2520the%2520prevalent%2520practice%2520of%250Aexpanding%2520memory%2520banks%2520to%2520accommodate%2520extensive%2520historical%2520information.%2520Our%250Aspecially%2520designed%2520%2522memory%2520deciphering%2522%2520study%2520offers%2520a%2520pivotal%2520insight%250Aunderpinning%2520such%2520a%2520strategy%253A%2520expanding%2520memory%2520banks%252C%2520while%2520seemingly%250Abeneficial%252C%2520actually%2520increases%2520the%2520difficulty%2520for%2520VOS%2520modules%2520to%2520decode%250Arelevant%2520features%2520due%2520to%2520the%2520confusion%2520from%2520redundant%2520information.%2520By%250Arestricting%2520memory%2520banks%2520to%2520a%2520limited%2520number%2520of%2520essential%2520frames%252C%2520we%2520achieve%2520a%250Anotable%2520improvement%2520in%2520VOS%2520accuracy.%2520This%2520process%2520balances%2520the%2520importance%2520and%250Afreshness%2520of%2520frames%2520to%2520maintain%2520an%2520informative%2520memory%2520bank%2520within%2520a%2520bounded%250Acapacity.%2520Additionally%252C%2520restricted%2520memory%2520banks%2520reduce%2520the%2520training-inference%250Adiscrepancy%2520in%2520memory%2520lengths%2520compared%2520with%2520continuous%2520expansion.%2520This%2520fosters%250Anew%2520opportunities%2520in%2520temporal%2520reasoning%2520and%2520enables%2520us%2520to%2520introduce%2520the%250Apreviously%2520overlooked%2520%2522temporal%2520positional%2520embedding.%2522%2520Finally%252C%2520our%2520insights%250Aare%2520embodied%2520in%2520%2522RMem%2522%2520%2528%2522R%2522%2520for%2520restricted%2529%252C%2520a%2520simple%2520yet%2520effective%2520VOS%250Amodification%2520that%2520excels%2520at%2520challenging%2520VOS%2520scenarios%2520and%2520establishes%2520new%2520state%250Aof%2520the%2520art%2520for%2520object%2520state%2520changes%2520%2528on%2520the%2520VOST%2520dataset%2529%2520and%2520long%2520videos%2520%2528on%250Athe%2520Long%2520Videos%2520dataset%2529.%2520Our%2520code%2520and%2520demo%2520are%2520available%2520at%250Ahttps%253A//restricted-memory.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RMem%3A%20Restricted%20Memory%20Banks%20Improve%20Video%20Object%20Segmentation&entry.906535625=Junbao%20Zhou%20and%20Ziqi%20Pang%20and%20Yu-Xiong%20Wang&entry.1292438233=%20%20With%20recent%20video%20object%20segmentation%20%28VOS%29%20benchmarks%20evolving%20to%0Achallenging%20scenarios%2C%20we%20revisit%20a%20simple%20but%20overlooked%20strategy%3A%20restricting%0Athe%20size%20of%20memory%20banks.%20This%20diverges%20from%20the%20prevalent%20practice%20of%0Aexpanding%20memory%20banks%20to%20accommodate%20extensive%20historical%20information.%20Our%0Aspecially%20designed%20%22memory%20deciphering%22%20study%20offers%20a%20pivotal%20insight%0Aunderpinning%20such%20a%20strategy%3A%20expanding%20memory%20banks%2C%20while%20seemingly%0Abeneficial%2C%20actually%20increases%20the%20difficulty%20for%20VOS%20modules%20to%20decode%0Arelevant%20features%20due%20to%20the%20confusion%20from%20redundant%20information.%20By%0Arestricting%20memory%20banks%20to%20a%20limited%20number%20of%20essential%20frames%2C%20we%20achieve%20a%0Anotable%20improvement%20in%20VOS%20accuracy.%20This%20process%20balances%20the%20importance%20and%0Afreshness%20of%20frames%20to%20maintain%20an%20informative%20memory%20bank%20within%20a%20bounded%0Acapacity.%20Additionally%2C%20restricted%20memory%20banks%20reduce%20the%20training-inference%0Adiscrepancy%20in%20memory%20lengths%20compared%20with%20continuous%20expansion.%20This%20fosters%0Anew%20opportunities%20in%20temporal%20reasoning%20and%20enables%20us%20to%20introduce%20the%0Apreviously%20overlooked%20%22temporal%20positional%20embedding.%22%20Finally%2C%20our%20insights%0Aare%20embodied%20in%20%22RMem%22%20%28%22R%22%20for%20restricted%29%2C%20a%20simple%20yet%20effective%20VOS%0Amodification%20that%20excels%20at%20challenging%20VOS%20scenarios%20and%20establishes%20new%20state%0Aof%20the%20art%20for%20object%20state%20changes%20%28on%20the%20VOST%20dataset%29%20and%20long%20videos%20%28on%0Athe%20Long%20Videos%20dataset%29.%20Our%20code%20and%20demo%20are%20available%20at%0Ahttps%3A//restricted-memory.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08476v1&entry.124074799=Read"},
{"title": "Consistency Regularization for Domain Generalization with Logit\n  Attribution Matching", "author": "Han Gao and Kaican Li and Weiyan Xie and Zhi Lin and Yongxiang Huang and Luning Wang and Caleb Chen Cao and Nevin L. Zhang", "abstract": "  Domain generalization (DG) is about training models that generalize well\nunder domain shift. Previous research on DG has been conducted mostly in\nsingle-source or multi-source settings. In this paper, we consider a third,\nlesser-known setting where a training domain is endowed with a collection of\npairs of examples that share the same semantic information. Such semantic\nsharing (SS) pairs can be created via data augmentation and then utilized for\nconsistency regularization (CR). We present a theory showing CR is conducive to\nDG and propose a novel CR method called Logit Attribution Matching (LAM). We\nconduct experiments on five DG benchmarks and four pretrained models with SS\npairs created by both generic and targeted data augmentation methods. LAM\noutperforms representative single/multi-source DG methods and various CR\nmethods that leverage SS pairs. The code and data of this project are available\nat https://github.com/Gaohan123/LAM\n", "link": "http://arxiv.org/abs/2305.07888v2", "date": "2024-06-12", "relevancy": 2.5402, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5187}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.517}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistency%20Regularization%20for%20Domain%20Generalization%20with%20Logit%0A%20%20Attribution%20Matching&body=Title%3A%20Consistency%20Regularization%20for%20Domain%20Generalization%20with%20Logit%0A%20%20Attribution%20Matching%0AAuthor%3A%20Han%20Gao%20and%20Kaican%20Li%20and%20Weiyan%20Xie%20and%20Zhi%20Lin%20and%20Yongxiang%20Huang%20and%20Luning%20Wang%20and%20Caleb%20Chen%20Cao%20and%20Nevin%20L.%20Zhang%0AAbstract%3A%20%20%20Domain%20generalization%20%28DG%29%20is%20about%20training%20models%20that%20generalize%20well%0Aunder%20domain%20shift.%20Previous%20research%20on%20DG%20has%20been%20conducted%20mostly%20in%0Asingle-source%20or%20multi-source%20settings.%20In%20this%20paper%2C%20we%20consider%20a%20third%2C%0Alesser-known%20setting%20where%20a%20training%20domain%20is%20endowed%20with%20a%20collection%20of%0Apairs%20of%20examples%20that%20share%20the%20same%20semantic%20information.%20Such%20semantic%0Asharing%20%28SS%29%20pairs%20can%20be%20created%20via%20data%20augmentation%20and%20then%20utilized%20for%0Aconsistency%20regularization%20%28CR%29.%20We%20present%20a%20theory%20showing%20CR%20is%20conducive%20to%0ADG%20and%20propose%20a%20novel%20CR%20method%20called%20Logit%20Attribution%20Matching%20%28LAM%29.%20We%0Aconduct%20experiments%20on%20five%20DG%20benchmarks%20and%20four%20pretrained%20models%20with%20SS%0Apairs%20created%20by%20both%20generic%20and%20targeted%20data%20augmentation%20methods.%20LAM%0Aoutperforms%20representative%20single/multi-source%20DG%20methods%20and%20various%20CR%0Amethods%20that%20leverage%20SS%20pairs.%20The%20code%20and%20data%20of%20this%20project%20are%20available%0Aat%20https%3A//github.com/Gaohan123/LAM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.07888v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistency%2520Regularization%2520for%2520Domain%2520Generalization%2520with%2520Logit%250A%2520%2520Attribution%2520Matching%26entry.906535625%3DHan%2520Gao%2520and%2520Kaican%2520Li%2520and%2520Weiyan%2520Xie%2520and%2520Zhi%2520Lin%2520and%2520Yongxiang%2520Huang%2520and%2520Luning%2520Wang%2520and%2520Caleb%2520Chen%2520Cao%2520and%2520Nevin%2520L.%2520Zhang%26entry.1292438233%3D%2520%2520Domain%2520generalization%2520%2528DG%2529%2520is%2520about%2520training%2520models%2520that%2520generalize%2520well%250Aunder%2520domain%2520shift.%2520Previous%2520research%2520on%2520DG%2520has%2520been%2520conducted%2520mostly%2520in%250Asingle-source%2520or%2520multi-source%2520settings.%2520In%2520this%2520paper%252C%2520we%2520consider%2520a%2520third%252C%250Alesser-known%2520setting%2520where%2520a%2520training%2520domain%2520is%2520endowed%2520with%2520a%2520collection%2520of%250Apairs%2520of%2520examples%2520that%2520share%2520the%2520same%2520semantic%2520information.%2520Such%2520semantic%250Asharing%2520%2528SS%2529%2520pairs%2520can%2520be%2520created%2520via%2520data%2520augmentation%2520and%2520then%2520utilized%2520for%250Aconsistency%2520regularization%2520%2528CR%2529.%2520We%2520present%2520a%2520theory%2520showing%2520CR%2520is%2520conducive%2520to%250ADG%2520and%2520propose%2520a%2520novel%2520CR%2520method%2520called%2520Logit%2520Attribution%2520Matching%2520%2528LAM%2529.%2520We%250Aconduct%2520experiments%2520on%2520five%2520DG%2520benchmarks%2520and%2520four%2520pretrained%2520models%2520with%2520SS%250Apairs%2520created%2520by%2520both%2520generic%2520and%2520targeted%2520data%2520augmentation%2520methods.%2520LAM%250Aoutperforms%2520representative%2520single/multi-source%2520DG%2520methods%2520and%2520various%2520CR%250Amethods%2520that%2520leverage%2520SS%2520pairs.%2520The%2520code%2520and%2520data%2520of%2520this%2520project%2520are%2520available%250Aat%2520https%253A//github.com/Gaohan123/LAM%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.07888v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistency%20Regularization%20for%20Domain%20Generalization%20with%20Logit%0A%20%20Attribution%20Matching&entry.906535625=Han%20Gao%20and%20Kaican%20Li%20and%20Weiyan%20Xie%20and%20Zhi%20Lin%20and%20Yongxiang%20Huang%20and%20Luning%20Wang%20and%20Caleb%20Chen%20Cao%20and%20Nevin%20L.%20Zhang&entry.1292438233=%20%20Domain%20generalization%20%28DG%29%20is%20about%20training%20models%20that%20generalize%20well%0Aunder%20domain%20shift.%20Previous%20research%20on%20DG%20has%20been%20conducted%20mostly%20in%0Asingle-source%20or%20multi-source%20settings.%20In%20this%20paper%2C%20we%20consider%20a%20third%2C%0Alesser-known%20setting%20where%20a%20training%20domain%20is%20endowed%20with%20a%20collection%20of%0Apairs%20of%20examples%20that%20share%20the%20same%20semantic%20information.%20Such%20semantic%0Asharing%20%28SS%29%20pairs%20can%20be%20created%20via%20data%20augmentation%20and%20then%20utilized%20for%0Aconsistency%20regularization%20%28CR%29.%20We%20present%20a%20theory%20showing%20CR%20is%20conducive%20to%0ADG%20and%20propose%20a%20novel%20CR%20method%20called%20Logit%20Attribution%20Matching%20%28LAM%29.%20We%0Aconduct%20experiments%20on%20five%20DG%20benchmarks%20and%20four%20pretrained%20models%20with%20SS%0Apairs%20created%20by%20both%20generic%20and%20targeted%20data%20augmentation%20methods.%20LAM%0Aoutperforms%20representative%20single/multi-source%20DG%20methods%20and%20various%20CR%0Amethods%20that%20leverage%20SS%20pairs.%20The%20code%20and%20data%20of%20this%20project%20are%20available%0Aat%20https%3A//github.com/Gaohan123/LAM%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.07888v2&entry.124074799=Read"},
{"title": "Manifold Learning by Mixture Models of VAEs for Inverse Problems", "author": "Giovanni S. Alberti and Johannes Hertrich and Matteo Santacesaria and Silvia Sciutto", "abstract": "  Representing a manifold of very high-dimensional data with generative models\nhas been shown to be computationally efficient in practice. However, this\nrequires that the data manifold admits a global parameterization. In order to\nrepresent manifolds of arbitrary topology, we propose to learn a mixture model\nof variational autoencoders. Here, every encoder-decoder pair represents one\nchart of a manifold. We propose a loss function for maximum likelihood\nestimation of the model weights and choose an architecture that provides us the\nanalytical expression of the charts and of their inverses. Once the manifold is\nlearned, we use it for solving inverse problems by minimizing a data fidelity\nterm restricted to the learned manifold. To solve the arising minimization\nproblem we propose a Riemannian gradient descent algorithm on the learned\nmanifold. We demonstrate the performance of our method for low-dimensional toy\nexamples as well as for deblurring and electrical impedance tomography on\ncertain image manifolds.\n", "link": "http://arxiv.org/abs/2303.15244v2", "date": "2024-06-12", "relevancy": 2.5294, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5306}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Manifold%20Learning%20by%20Mixture%20Models%20of%20VAEs%20for%20Inverse%20Problems&body=Title%3A%20Manifold%20Learning%20by%20Mixture%20Models%20of%20VAEs%20for%20Inverse%20Problems%0AAuthor%3A%20Giovanni%20S.%20Alberti%20and%20Johannes%20Hertrich%20and%20Matteo%20Santacesaria%20and%20Silvia%20Sciutto%0AAbstract%3A%20%20%20Representing%20a%20manifold%20of%20very%20high-dimensional%20data%20with%20generative%20models%0Ahas%20been%20shown%20to%20be%20computationally%20efficient%20in%20practice.%20However%2C%20this%0Arequires%20that%20the%20data%20manifold%20admits%20a%20global%20parameterization.%20In%20order%20to%0Arepresent%20manifolds%20of%20arbitrary%20topology%2C%20we%20propose%20to%20learn%20a%20mixture%20model%0Aof%20variational%20autoencoders.%20Here%2C%20every%20encoder-decoder%20pair%20represents%20one%0Achart%20of%20a%20manifold.%20We%20propose%20a%20loss%20function%20for%20maximum%20likelihood%0Aestimation%20of%20the%20model%20weights%20and%20choose%20an%20architecture%20that%20provides%20us%20the%0Aanalytical%20expression%20of%20the%20charts%20and%20of%20their%20inverses.%20Once%20the%20manifold%20is%0Alearned%2C%20we%20use%20it%20for%20solving%20inverse%20problems%20by%20minimizing%20a%20data%20fidelity%0Aterm%20restricted%20to%20the%20learned%20manifold.%20To%20solve%20the%20arising%20minimization%0Aproblem%20we%20propose%20a%20Riemannian%20gradient%20descent%20algorithm%20on%20the%20learned%0Amanifold.%20We%20demonstrate%20the%20performance%20of%20our%20method%20for%20low-dimensional%20toy%0Aexamples%20as%20well%20as%20for%20deblurring%20and%20electrical%20impedance%20tomography%20on%0Acertain%20image%20manifolds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.15244v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DManifold%2520Learning%2520by%2520Mixture%2520Models%2520of%2520VAEs%2520for%2520Inverse%2520Problems%26entry.906535625%3DGiovanni%2520S.%2520Alberti%2520and%2520Johannes%2520Hertrich%2520and%2520Matteo%2520Santacesaria%2520and%2520Silvia%2520Sciutto%26entry.1292438233%3D%2520%2520Representing%2520a%2520manifold%2520of%2520very%2520high-dimensional%2520data%2520with%2520generative%2520models%250Ahas%2520been%2520shown%2520to%2520be%2520computationally%2520efficient%2520in%2520practice.%2520However%252C%2520this%250Arequires%2520that%2520the%2520data%2520manifold%2520admits%2520a%2520global%2520parameterization.%2520In%2520order%2520to%250Arepresent%2520manifolds%2520of%2520arbitrary%2520topology%252C%2520we%2520propose%2520to%2520learn%2520a%2520mixture%2520model%250Aof%2520variational%2520autoencoders.%2520Here%252C%2520every%2520encoder-decoder%2520pair%2520represents%2520one%250Achart%2520of%2520a%2520manifold.%2520We%2520propose%2520a%2520loss%2520function%2520for%2520maximum%2520likelihood%250Aestimation%2520of%2520the%2520model%2520weights%2520and%2520choose%2520an%2520architecture%2520that%2520provides%2520us%2520the%250Aanalytical%2520expression%2520of%2520the%2520charts%2520and%2520of%2520their%2520inverses.%2520Once%2520the%2520manifold%2520is%250Alearned%252C%2520we%2520use%2520it%2520for%2520solving%2520inverse%2520problems%2520by%2520minimizing%2520a%2520data%2520fidelity%250Aterm%2520restricted%2520to%2520the%2520learned%2520manifold.%2520To%2520solve%2520the%2520arising%2520minimization%250Aproblem%2520we%2520propose%2520a%2520Riemannian%2520gradient%2520descent%2520algorithm%2520on%2520the%2520learned%250Amanifold.%2520We%2520demonstrate%2520the%2520performance%2520of%2520our%2520method%2520for%2520low-dimensional%2520toy%250Aexamples%2520as%2520well%2520as%2520for%2520deblurring%2520and%2520electrical%2520impedance%2520tomography%2520on%250Acertain%2520image%2520manifolds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.15244v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Manifold%20Learning%20by%20Mixture%20Models%20of%20VAEs%20for%20Inverse%20Problems&entry.906535625=Giovanni%20S.%20Alberti%20and%20Johannes%20Hertrich%20and%20Matteo%20Santacesaria%20and%20Silvia%20Sciutto&entry.1292438233=%20%20Representing%20a%20manifold%20of%20very%20high-dimensional%20data%20with%20generative%20models%0Ahas%20been%20shown%20to%20be%20computationally%20efficient%20in%20practice.%20However%2C%20this%0Arequires%20that%20the%20data%20manifold%20admits%20a%20global%20parameterization.%20In%20order%20to%0Arepresent%20manifolds%20of%20arbitrary%20topology%2C%20we%20propose%20to%20learn%20a%20mixture%20model%0Aof%20variational%20autoencoders.%20Here%2C%20every%20encoder-decoder%20pair%20represents%20one%0Achart%20of%20a%20manifold.%20We%20propose%20a%20loss%20function%20for%20maximum%20likelihood%0Aestimation%20of%20the%20model%20weights%20and%20choose%20an%20architecture%20that%20provides%20us%20the%0Aanalytical%20expression%20of%20the%20charts%20and%20of%20their%20inverses.%20Once%20the%20manifold%20is%0Alearned%2C%20we%20use%20it%20for%20solving%20inverse%20problems%20by%20minimizing%20a%20data%20fidelity%0Aterm%20restricted%20to%20the%20learned%20manifold.%20To%20solve%20the%20arising%20minimization%0Aproblem%20we%20propose%20a%20Riemannian%20gradient%20descent%20algorithm%20on%20the%20learned%0Amanifold.%20We%20demonstrate%20the%20performance%20of%20our%20method%20for%20low-dimensional%20toy%0Aexamples%20as%20well%20as%20for%20deblurring%20and%20electrical%20impedance%20tomography%20on%0Acertain%20image%20manifolds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.15244v2&entry.124074799=Read"},
{"title": "Graph Condensation for Open-World Graph Learning", "author": "Xinyi Gao and Tong Chen and Wentao Zhang and Yayong Li and Xiangguo Sun and Hongzhi Yin", "abstract": "  The burgeoning volume of graph data presents significant computational\nchallenges in training graph neural networks (GNNs), critically impeding their\nefficiency in various applications. To tackle this challenge, graph\ncondensation (GC) has emerged as a promising acceleration solution, focusing on\nthe synthesis of a compact yet representative graph for efficiently training\nGNNs while retaining performance. Despite the potential to promote scalable use\nof GNNs, existing GC methods are limited to aligning the condensed graph with\nmerely the observed static graph distribution. This limitation significantly\nrestricts the generalization capacity of condensed graphs, particularly in\nadapting to dynamic distribution changes. In real-world scenarios, however,\ngraphs are dynamic and constantly evolving, with new nodes and edges being\ncontinually integrated. Consequently, due to the limited generalization\ncapacity of condensed graphs, applications that employ GC for efficient GNN\ntraining end up with sub-optimal GNNs when confronted with evolving graph\nstructures and distributions in dynamic real-world situations. To overcome this\nissue, we propose open-world graph condensation (OpenGC), a robust GC framework\nthat integrates structure-aware distribution shift to simulate evolving graph\npatterns and exploit the temporal environments for invariance condensation.\nThis approach is designed to extract temporal invariant patterns from the\noriginal graph, thereby enhancing the generalization capabilities of the\ncondensed graph and, subsequently, the GNNs trained on it. Extensive\nexperiments on both real-world and synthetic evolving graphs demonstrate that\nOpenGC outperforms state-of-the-art (SOTA) GC methods in adapting to dynamic\nchanges in open-world graph environments.\n", "link": "http://arxiv.org/abs/2405.17003v2", "date": "2024-06-12", "relevancy": 2.5292, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5231}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5061}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Condensation%20for%20Open-World%20Graph%20Learning&body=Title%3A%20Graph%20Condensation%20for%20Open-World%20Graph%20Learning%0AAuthor%3A%20Xinyi%20Gao%20and%20Tong%20Chen%20and%20Wentao%20Zhang%20and%20Yayong%20Li%20and%20Xiangguo%20Sun%20and%20Hongzhi%20Yin%0AAbstract%3A%20%20%20The%20burgeoning%20volume%20of%20graph%20data%20presents%20significant%20computational%0Achallenges%20in%20training%20graph%20neural%20networks%20%28GNNs%29%2C%20critically%20impeding%20their%0Aefficiency%20in%20various%20applications.%20To%20tackle%20this%20challenge%2C%20graph%0Acondensation%20%28GC%29%20has%20emerged%20as%20a%20promising%20acceleration%20solution%2C%20focusing%20on%0Athe%20synthesis%20of%20a%20compact%20yet%20representative%20graph%20for%20efficiently%20training%0AGNNs%20while%20retaining%20performance.%20Despite%20the%20potential%20to%20promote%20scalable%20use%0Aof%20GNNs%2C%20existing%20GC%20methods%20are%20limited%20to%20aligning%20the%20condensed%20graph%20with%0Amerely%20the%20observed%20static%20graph%20distribution.%20This%20limitation%20significantly%0Arestricts%20the%20generalization%20capacity%20of%20condensed%20graphs%2C%20particularly%20in%0Aadapting%20to%20dynamic%20distribution%20changes.%20In%20real-world%20scenarios%2C%20however%2C%0Agraphs%20are%20dynamic%20and%20constantly%20evolving%2C%20with%20new%20nodes%20and%20edges%20being%0Acontinually%20integrated.%20Consequently%2C%20due%20to%20the%20limited%20generalization%0Acapacity%20of%20condensed%20graphs%2C%20applications%20that%20employ%20GC%20for%20efficient%20GNN%0Atraining%20end%20up%20with%20sub-optimal%20GNNs%20when%20confronted%20with%20evolving%20graph%0Astructures%20and%20distributions%20in%20dynamic%20real-world%20situations.%20To%20overcome%20this%0Aissue%2C%20we%20propose%20open-world%20graph%20condensation%20%28OpenGC%29%2C%20a%20robust%20GC%20framework%0Athat%20integrates%20structure-aware%20distribution%20shift%20to%20simulate%20evolving%20graph%0Apatterns%20and%20exploit%20the%20temporal%20environments%20for%20invariance%20condensation.%0AThis%20approach%20is%20designed%20to%20extract%20temporal%20invariant%20patterns%20from%20the%0Aoriginal%20graph%2C%20thereby%20enhancing%20the%20generalization%20capabilities%20of%20the%0Acondensed%20graph%20and%2C%20subsequently%2C%20the%20GNNs%20trained%20on%20it.%20Extensive%0Aexperiments%20on%20both%20real-world%20and%20synthetic%20evolving%20graphs%20demonstrate%20that%0AOpenGC%20outperforms%20state-of-the-art%20%28SOTA%29%20GC%20methods%20in%20adapting%20to%20dynamic%0Achanges%20in%20open-world%20graph%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17003v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Condensation%2520for%2520Open-World%2520Graph%2520Learning%26entry.906535625%3DXinyi%2520Gao%2520and%2520Tong%2520Chen%2520and%2520Wentao%2520Zhang%2520and%2520Yayong%2520Li%2520and%2520Xiangguo%2520Sun%2520and%2520Hongzhi%2520Yin%26entry.1292438233%3D%2520%2520The%2520burgeoning%2520volume%2520of%2520graph%2520data%2520presents%2520significant%2520computational%250Achallenges%2520in%2520training%2520graph%2520neural%2520networks%2520%2528GNNs%2529%252C%2520critically%2520impeding%2520their%250Aefficiency%2520in%2520various%2520applications.%2520To%2520tackle%2520this%2520challenge%252C%2520graph%250Acondensation%2520%2528GC%2529%2520has%2520emerged%2520as%2520a%2520promising%2520acceleration%2520solution%252C%2520focusing%2520on%250Athe%2520synthesis%2520of%2520a%2520compact%2520yet%2520representative%2520graph%2520for%2520efficiently%2520training%250AGNNs%2520while%2520retaining%2520performance.%2520Despite%2520the%2520potential%2520to%2520promote%2520scalable%2520use%250Aof%2520GNNs%252C%2520existing%2520GC%2520methods%2520are%2520limited%2520to%2520aligning%2520the%2520condensed%2520graph%2520with%250Amerely%2520the%2520observed%2520static%2520graph%2520distribution.%2520This%2520limitation%2520significantly%250Arestricts%2520the%2520generalization%2520capacity%2520of%2520condensed%2520graphs%252C%2520particularly%2520in%250Aadapting%2520to%2520dynamic%2520distribution%2520changes.%2520In%2520real-world%2520scenarios%252C%2520however%252C%250Agraphs%2520are%2520dynamic%2520and%2520constantly%2520evolving%252C%2520with%2520new%2520nodes%2520and%2520edges%2520being%250Acontinually%2520integrated.%2520Consequently%252C%2520due%2520to%2520the%2520limited%2520generalization%250Acapacity%2520of%2520condensed%2520graphs%252C%2520applications%2520that%2520employ%2520GC%2520for%2520efficient%2520GNN%250Atraining%2520end%2520up%2520with%2520sub-optimal%2520GNNs%2520when%2520confronted%2520with%2520evolving%2520graph%250Astructures%2520and%2520distributions%2520in%2520dynamic%2520real-world%2520situations.%2520To%2520overcome%2520this%250Aissue%252C%2520we%2520propose%2520open-world%2520graph%2520condensation%2520%2528OpenGC%2529%252C%2520a%2520robust%2520GC%2520framework%250Athat%2520integrates%2520structure-aware%2520distribution%2520shift%2520to%2520simulate%2520evolving%2520graph%250Apatterns%2520and%2520exploit%2520the%2520temporal%2520environments%2520for%2520invariance%2520condensation.%250AThis%2520approach%2520is%2520designed%2520to%2520extract%2520temporal%2520invariant%2520patterns%2520from%2520the%250Aoriginal%2520graph%252C%2520thereby%2520enhancing%2520the%2520generalization%2520capabilities%2520of%2520the%250Acondensed%2520graph%2520and%252C%2520subsequently%252C%2520the%2520GNNs%2520trained%2520on%2520it.%2520Extensive%250Aexperiments%2520on%2520both%2520real-world%2520and%2520synthetic%2520evolving%2520graphs%2520demonstrate%2520that%250AOpenGC%2520outperforms%2520state-of-the-art%2520%2528SOTA%2529%2520GC%2520methods%2520in%2520adapting%2520to%2520dynamic%250Achanges%2520in%2520open-world%2520graph%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17003v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Condensation%20for%20Open-World%20Graph%20Learning&entry.906535625=Xinyi%20Gao%20and%20Tong%20Chen%20and%20Wentao%20Zhang%20and%20Yayong%20Li%20and%20Xiangguo%20Sun%20and%20Hongzhi%20Yin&entry.1292438233=%20%20The%20burgeoning%20volume%20of%20graph%20data%20presents%20significant%20computational%0Achallenges%20in%20training%20graph%20neural%20networks%20%28GNNs%29%2C%20critically%20impeding%20their%0Aefficiency%20in%20various%20applications.%20To%20tackle%20this%20challenge%2C%20graph%0Acondensation%20%28GC%29%20has%20emerged%20as%20a%20promising%20acceleration%20solution%2C%20focusing%20on%0Athe%20synthesis%20of%20a%20compact%20yet%20representative%20graph%20for%20efficiently%20training%0AGNNs%20while%20retaining%20performance.%20Despite%20the%20potential%20to%20promote%20scalable%20use%0Aof%20GNNs%2C%20existing%20GC%20methods%20are%20limited%20to%20aligning%20the%20condensed%20graph%20with%0Amerely%20the%20observed%20static%20graph%20distribution.%20This%20limitation%20significantly%0Arestricts%20the%20generalization%20capacity%20of%20condensed%20graphs%2C%20particularly%20in%0Aadapting%20to%20dynamic%20distribution%20changes.%20In%20real-world%20scenarios%2C%20however%2C%0Agraphs%20are%20dynamic%20and%20constantly%20evolving%2C%20with%20new%20nodes%20and%20edges%20being%0Acontinually%20integrated.%20Consequently%2C%20due%20to%20the%20limited%20generalization%0Acapacity%20of%20condensed%20graphs%2C%20applications%20that%20employ%20GC%20for%20efficient%20GNN%0Atraining%20end%20up%20with%20sub-optimal%20GNNs%20when%20confronted%20with%20evolving%20graph%0Astructures%20and%20distributions%20in%20dynamic%20real-world%20situations.%20To%20overcome%20this%0Aissue%2C%20we%20propose%20open-world%20graph%20condensation%20%28OpenGC%29%2C%20a%20robust%20GC%20framework%0Athat%20integrates%20structure-aware%20distribution%20shift%20to%20simulate%20evolving%20graph%0Apatterns%20and%20exploit%20the%20temporal%20environments%20for%20invariance%20condensation.%0AThis%20approach%20is%20designed%20to%20extract%20temporal%20invariant%20patterns%20from%20the%0Aoriginal%20graph%2C%20thereby%20enhancing%20the%20generalization%20capabilities%20of%20the%0Acondensed%20graph%20and%2C%20subsequently%2C%20the%20GNNs%20trained%20on%20it.%20Extensive%0Aexperiments%20on%20both%20real-world%20and%20synthetic%20evolving%20graphs%20demonstrate%20that%0AOpenGC%20outperforms%20state-of-the-art%20%28SOTA%29%20GC%20methods%20in%20adapting%20to%20dynamic%0Achanges%20in%20open-world%20graph%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17003v2&entry.124074799=Read"},
{"title": "Interpretable Representation Learning of Cardiac MRI via Attribute\n  Regularization", "author": "Maxime Di Folco and Cosmin I. Bercea and Julia A. Schnabel", "abstract": "  Interpretability is essential in medical imaging to ensure that clinicians\ncan comprehend and trust artificial intelligence models. Several approaches\nhave been recently considered to encode attributes in the latent space to\nenhance its interpretability. Notably, attribute regularization aims to encode\na set of attributes along the dimensions of a latent representation. However,\nthis approach is based on Variational AutoEncoder and suffers from blurry\nreconstruction. In this paper, we propose an Attributed-regularized Soft\nIntrospective Variational Autoencoder that combines attribute regularization of\nthe latent space within the framework of an adversarially trained variational\nautoencoder. We demonstrate on short-axis cardiac Magnetic Resonance images of\nthe UK Biobank the ability of the proposed method to address blurry\nreconstruction issues of variational autoencoder methods while preserving the\nlatent space interpretability.\n", "link": "http://arxiv.org/abs/2406.08282v1", "date": "2024-06-12", "relevancy": 2.5132, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5097}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5015}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Representation%20Learning%20of%20Cardiac%20MRI%20via%20Attribute%0A%20%20Regularization&body=Title%3A%20Interpretable%20Representation%20Learning%20of%20Cardiac%20MRI%20via%20Attribute%0A%20%20Regularization%0AAuthor%3A%20Maxime%20Di%20Folco%20and%20Cosmin%20I.%20Bercea%20and%20Julia%20A.%20Schnabel%0AAbstract%3A%20%20%20Interpretability%20is%20essential%20in%20medical%20imaging%20to%20ensure%20that%20clinicians%0Acan%20comprehend%20and%20trust%20artificial%20intelligence%20models.%20Several%20approaches%0Ahave%20been%20recently%20considered%20to%20encode%20attributes%20in%20the%20latent%20space%20to%0Aenhance%20its%20interpretability.%20Notably%2C%20attribute%20regularization%20aims%20to%20encode%0Aa%20set%20of%20attributes%20along%20the%20dimensions%20of%20a%20latent%20representation.%20However%2C%0Athis%20approach%20is%20based%20on%20Variational%20AutoEncoder%20and%20suffers%20from%20blurry%0Areconstruction.%20In%20this%20paper%2C%20we%20propose%20an%20Attributed-regularized%20Soft%0AIntrospective%20Variational%20Autoencoder%20that%20combines%20attribute%20regularization%20of%0Athe%20latent%20space%20within%20the%20framework%20of%20an%20adversarially%20trained%20variational%0Aautoencoder.%20We%20demonstrate%20on%20short-axis%20cardiac%20Magnetic%20Resonance%20images%20of%0Athe%20UK%20Biobank%20the%20ability%20of%20the%20proposed%20method%20to%20address%20blurry%0Areconstruction%20issues%20of%20variational%20autoencoder%20methods%20while%20preserving%20the%0Alatent%20space%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08282v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Representation%2520Learning%2520of%2520Cardiac%2520MRI%2520via%2520Attribute%250A%2520%2520Regularization%26entry.906535625%3DMaxime%2520Di%2520Folco%2520and%2520Cosmin%2520I.%2520Bercea%2520and%2520Julia%2520A.%2520Schnabel%26entry.1292438233%3D%2520%2520Interpretability%2520is%2520essential%2520in%2520medical%2520imaging%2520to%2520ensure%2520that%2520clinicians%250Acan%2520comprehend%2520and%2520trust%2520artificial%2520intelligence%2520models.%2520Several%2520approaches%250Ahave%2520been%2520recently%2520considered%2520to%2520encode%2520attributes%2520in%2520the%2520latent%2520space%2520to%250Aenhance%2520its%2520interpretability.%2520Notably%252C%2520attribute%2520regularization%2520aims%2520to%2520encode%250Aa%2520set%2520of%2520attributes%2520along%2520the%2520dimensions%2520of%2520a%2520latent%2520representation.%2520However%252C%250Athis%2520approach%2520is%2520based%2520on%2520Variational%2520AutoEncoder%2520and%2520suffers%2520from%2520blurry%250Areconstruction.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520Attributed-regularized%2520Soft%250AIntrospective%2520Variational%2520Autoencoder%2520that%2520combines%2520attribute%2520regularization%2520of%250Athe%2520latent%2520space%2520within%2520the%2520framework%2520of%2520an%2520adversarially%2520trained%2520variational%250Aautoencoder.%2520We%2520demonstrate%2520on%2520short-axis%2520cardiac%2520Magnetic%2520Resonance%2520images%2520of%250Athe%2520UK%2520Biobank%2520the%2520ability%2520of%2520the%2520proposed%2520method%2520to%2520address%2520blurry%250Areconstruction%2520issues%2520of%2520variational%2520autoencoder%2520methods%2520while%2520preserving%2520the%250Alatent%2520space%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08282v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Representation%20Learning%20of%20Cardiac%20MRI%20via%20Attribute%0A%20%20Regularization&entry.906535625=Maxime%20Di%20Folco%20and%20Cosmin%20I.%20Bercea%20and%20Julia%20A.%20Schnabel&entry.1292438233=%20%20Interpretability%20is%20essential%20in%20medical%20imaging%20to%20ensure%20that%20clinicians%0Acan%20comprehend%20and%20trust%20artificial%20intelligence%20models.%20Several%20approaches%0Ahave%20been%20recently%20considered%20to%20encode%20attributes%20in%20the%20latent%20space%20to%0Aenhance%20its%20interpretability.%20Notably%2C%20attribute%20regularization%20aims%20to%20encode%0Aa%20set%20of%20attributes%20along%20the%20dimensions%20of%20a%20latent%20representation.%20However%2C%0Athis%20approach%20is%20based%20on%20Variational%20AutoEncoder%20and%20suffers%20from%20blurry%0Areconstruction.%20In%20this%20paper%2C%20we%20propose%20an%20Attributed-regularized%20Soft%0AIntrospective%20Variational%20Autoencoder%20that%20combines%20attribute%20regularization%20of%0Athe%20latent%20space%20within%20the%20framework%20of%20an%20adversarially%20trained%20variational%0Aautoencoder.%20We%20demonstrate%20on%20short-axis%20cardiac%20Magnetic%20Resonance%20images%20of%0Athe%20UK%20Biobank%20the%20ability%20of%20the%20proposed%20method%20to%20address%20blurry%0Areconstruction%20issues%20of%20variational%20autoencoder%20methods%20while%20preserving%20the%0Alatent%20space%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08282v1&entry.124074799=Read"},
{"title": "FIFO-Diffusion: Generating Infinite Videos from Text without Training", "author": "Jihwan Kim and Junoh Kang and Jinyoung Choi and Bohyung Han", "abstract": "  We propose a novel inference technique based on a pretrained diffusion model\nfor text-conditional video generation. Our approach, called FIFO-Diffusion, is\nconceptually capable of generating infinitely long videos without additional\ntraining. This is achieved by iteratively performing diagonal denoising, which\nconcurrently processes a series of consecutive frames with increasing noise\nlevels in a queue; our method dequeues a fully denoised frame at the head while\nenqueuing a new random noise frame at the tail. However, diagonal denoising is\na double-edged sword as the frames near the tail can take advantage of cleaner\nones by forward reference but such a strategy induces the discrepancy between\ntraining and inference. Hence, we introduce latent partitioning to reduce the\ntraining-inference gap and lookahead denoising to leverage the benefit of\nforward referencing. Practically, FIFO-Diffusion consumes a constant amount of\nmemory regardless of the target video length given a baseline model, while\nwell-suited for parallel inference on multiple GPUs. We have demonstrated the\npromising results and effectiveness of the proposed methods on existing\ntext-to-video generation baselines. Generated video samples and source codes\nare available at our project page.\n", "link": "http://arxiv.org/abs/2405.11473v3", "date": "2024-06-12", "relevancy": 2.5127, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6454}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6175}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FIFO-Diffusion%3A%20Generating%20Infinite%20Videos%20from%20Text%20without%20Training&body=Title%3A%20FIFO-Diffusion%3A%20Generating%20Infinite%20Videos%20from%20Text%20without%20Training%0AAuthor%3A%20Jihwan%20Kim%20and%20Junoh%20Kang%20and%20Jinyoung%20Choi%20and%20Bohyung%20Han%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20inference%20technique%20based%20on%20a%20pretrained%20diffusion%20model%0Afor%20text-conditional%20video%20generation.%20Our%20approach%2C%20called%20FIFO-Diffusion%2C%20is%0Aconceptually%20capable%20of%20generating%20infinitely%20long%20videos%20without%20additional%0Atraining.%20This%20is%20achieved%20by%20iteratively%20performing%20diagonal%20denoising%2C%20which%0Aconcurrently%20processes%20a%20series%20of%20consecutive%20frames%20with%20increasing%20noise%0Alevels%20in%20a%20queue%3B%20our%20method%20dequeues%20a%20fully%20denoised%20frame%20at%20the%20head%20while%0Aenqueuing%20a%20new%20random%20noise%20frame%20at%20the%20tail.%20However%2C%20diagonal%20denoising%20is%0Aa%20double-edged%20sword%20as%20the%20frames%20near%20the%20tail%20can%20take%20advantage%20of%20cleaner%0Aones%20by%20forward%20reference%20but%20such%20a%20strategy%20induces%20the%20discrepancy%20between%0Atraining%20and%20inference.%20Hence%2C%20we%20introduce%20latent%20partitioning%20to%20reduce%20the%0Atraining-inference%20gap%20and%20lookahead%20denoising%20to%20leverage%20the%20benefit%20of%0Aforward%20referencing.%20Practically%2C%20FIFO-Diffusion%20consumes%20a%20constant%20amount%20of%0Amemory%20regardless%20of%20the%20target%20video%20length%20given%20a%20baseline%20model%2C%20while%0Awell-suited%20for%20parallel%20inference%20on%20multiple%20GPUs.%20We%20have%20demonstrated%20the%0Apromising%20results%20and%20effectiveness%20of%20the%20proposed%20methods%20on%20existing%0Atext-to-video%20generation%20baselines.%20Generated%20video%20samples%20and%20source%20codes%0Aare%20available%20at%20our%20project%20page.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11473v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFIFO-Diffusion%253A%2520Generating%2520Infinite%2520Videos%2520from%2520Text%2520without%2520Training%26entry.906535625%3DJihwan%2520Kim%2520and%2520Junoh%2520Kang%2520and%2520Jinyoung%2520Choi%2520and%2520Bohyung%2520Han%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520inference%2520technique%2520based%2520on%2520a%2520pretrained%2520diffusion%2520model%250Afor%2520text-conditional%2520video%2520generation.%2520Our%2520approach%252C%2520called%2520FIFO-Diffusion%252C%2520is%250Aconceptually%2520capable%2520of%2520generating%2520infinitely%2520long%2520videos%2520without%2520additional%250Atraining.%2520This%2520is%2520achieved%2520by%2520iteratively%2520performing%2520diagonal%2520denoising%252C%2520which%250Aconcurrently%2520processes%2520a%2520series%2520of%2520consecutive%2520frames%2520with%2520increasing%2520noise%250Alevels%2520in%2520a%2520queue%253B%2520our%2520method%2520dequeues%2520a%2520fully%2520denoised%2520frame%2520at%2520the%2520head%2520while%250Aenqueuing%2520a%2520new%2520random%2520noise%2520frame%2520at%2520the%2520tail.%2520However%252C%2520diagonal%2520denoising%2520is%250Aa%2520double-edged%2520sword%2520as%2520the%2520frames%2520near%2520the%2520tail%2520can%2520take%2520advantage%2520of%2520cleaner%250Aones%2520by%2520forward%2520reference%2520but%2520such%2520a%2520strategy%2520induces%2520the%2520discrepancy%2520between%250Atraining%2520and%2520inference.%2520Hence%252C%2520we%2520introduce%2520latent%2520partitioning%2520to%2520reduce%2520the%250Atraining-inference%2520gap%2520and%2520lookahead%2520denoising%2520to%2520leverage%2520the%2520benefit%2520of%250Aforward%2520referencing.%2520Practically%252C%2520FIFO-Diffusion%2520consumes%2520a%2520constant%2520amount%2520of%250Amemory%2520regardless%2520of%2520the%2520target%2520video%2520length%2520given%2520a%2520baseline%2520model%252C%2520while%250Awell-suited%2520for%2520parallel%2520inference%2520on%2520multiple%2520GPUs.%2520We%2520have%2520demonstrated%2520the%250Apromising%2520results%2520and%2520effectiveness%2520of%2520the%2520proposed%2520methods%2520on%2520existing%250Atext-to-video%2520generation%2520baselines.%2520Generated%2520video%2520samples%2520and%2520source%2520codes%250Aare%2520available%2520at%2520our%2520project%2520page.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11473v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FIFO-Diffusion%3A%20Generating%20Infinite%20Videos%20from%20Text%20without%20Training&entry.906535625=Jihwan%20Kim%20and%20Junoh%20Kang%20and%20Jinyoung%20Choi%20and%20Bohyung%20Han&entry.1292438233=%20%20We%20propose%20a%20novel%20inference%20technique%20based%20on%20a%20pretrained%20diffusion%20model%0Afor%20text-conditional%20video%20generation.%20Our%20approach%2C%20called%20FIFO-Diffusion%2C%20is%0Aconceptually%20capable%20of%20generating%20infinitely%20long%20videos%20without%20additional%0Atraining.%20This%20is%20achieved%20by%20iteratively%20performing%20diagonal%20denoising%2C%20which%0Aconcurrently%20processes%20a%20series%20of%20consecutive%20frames%20with%20increasing%20noise%0Alevels%20in%20a%20queue%3B%20our%20method%20dequeues%20a%20fully%20denoised%20frame%20at%20the%20head%20while%0Aenqueuing%20a%20new%20random%20noise%20frame%20at%20the%20tail.%20However%2C%20diagonal%20denoising%20is%0Aa%20double-edged%20sword%20as%20the%20frames%20near%20the%20tail%20can%20take%20advantage%20of%20cleaner%0Aones%20by%20forward%20reference%20but%20such%20a%20strategy%20induces%20the%20discrepancy%20between%0Atraining%20and%20inference.%20Hence%2C%20we%20introduce%20latent%20partitioning%20to%20reduce%20the%0Atraining-inference%20gap%20and%20lookahead%20denoising%20to%20leverage%20the%20benefit%20of%0Aforward%20referencing.%20Practically%2C%20FIFO-Diffusion%20consumes%20a%20constant%20amount%20of%0Amemory%20regardless%20of%20the%20target%20video%20length%20given%20a%20baseline%20model%2C%20while%0Awell-suited%20for%20parallel%20inference%20on%20multiple%20GPUs.%20We%20have%20demonstrated%20the%0Apromising%20results%20and%20effectiveness%20of%20the%20proposed%20methods%20on%20existing%0Atext-to-video%20generation%20baselines.%20Generated%20video%20samples%20and%20source%20codes%0Aare%20available%20at%20our%20project%20page.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11473v3&entry.124074799=Read"},
{"title": "Neural Dynamic Data Valuation", "author": "Zhangyong Liang and Huanhuan Gao and Ji Zhang", "abstract": "  Data constitute the foundational component of the data economy and its\nmarketplaces. Efficient and fair data valuation has emerged as a topic of\nsignificant interest.\\ Many approaches based on marginal contribution have\nshown promising results in various downstream tasks. However, they are well\nknown to be computationally expensive as they require training a large number\nof utility functions, which are used to evaluate the usefulness or value of a\ngiven dataset for a specific purpose. As a result, it has been recognized as\ninfeasible to apply these methods to a data marketplace involving large-scale\ndatasets. Consequently, a critical issue arises: how can the re-training of the\nutility function be avoided? To address this issue, we propose a novel data\nvaluation method from the perspective of optimal control, named the neural\ndynamic data valuation (NDDV). Our method has solid theoretical interpretations\nto accurately identify the data valuation via the sensitivity of the data\noptimal control state. In addition, we implement a data re-weighting strategy\nto capture the unique features of data points, ensuring fairness through the\ninteraction between data points and the mean-field states. Notably, our method\nrequires only training once to estimate the value of all data points,\nsignificantly improving the computational efficiency. We conduct comprehensive\nexperiments using different datasets and tasks. The results demonstrate that\nthe proposed NDDV method outperforms the existing state-of-the-art data\nvaluation methods in accurately identifying data points with either high or low\nvalues and is more computationally efficient.\n", "link": "http://arxiv.org/abs/2404.19557v3", "date": "2024-06-12", "relevancy": 2.4931, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5407}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4783}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Dynamic%20Data%20Valuation&body=Title%3A%20Neural%20Dynamic%20Data%20Valuation%0AAuthor%3A%20Zhangyong%20Liang%20and%20Huanhuan%20Gao%20and%20Ji%20Zhang%0AAbstract%3A%20%20%20Data%20constitute%20the%20foundational%20component%20of%20the%20data%20economy%20and%20its%0Amarketplaces.%20Efficient%20and%20fair%20data%20valuation%20has%20emerged%20as%20a%20topic%20of%0Asignificant%20interest.%5C%20Many%20approaches%20based%20on%20marginal%20contribution%20have%0Ashown%20promising%20results%20in%20various%20downstream%20tasks.%20However%2C%20they%20are%20well%0Aknown%20to%20be%20computationally%20expensive%20as%20they%20require%20training%20a%20large%20number%0Aof%20utility%20functions%2C%20which%20are%20used%20to%20evaluate%20the%20usefulness%20or%20value%20of%20a%0Agiven%20dataset%20for%20a%20specific%20purpose.%20As%20a%20result%2C%20it%20has%20been%20recognized%20as%0Ainfeasible%20to%20apply%20these%20methods%20to%20a%20data%20marketplace%20involving%20large-scale%0Adatasets.%20Consequently%2C%20a%20critical%20issue%20arises%3A%20how%20can%20the%20re-training%20of%20the%0Autility%20function%20be%20avoided%3F%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20data%0Avaluation%20method%20from%20the%20perspective%20of%20optimal%20control%2C%20named%20the%20neural%0Adynamic%20data%20valuation%20%28NDDV%29.%20Our%20method%20has%20solid%20theoretical%20interpretations%0Ato%20accurately%20identify%20the%20data%20valuation%20via%20the%20sensitivity%20of%20the%20data%0Aoptimal%20control%20state.%20In%20addition%2C%20we%20implement%20a%20data%20re-weighting%20strategy%0Ato%20capture%20the%20unique%20features%20of%20data%20points%2C%20ensuring%20fairness%20through%20the%0Ainteraction%20between%20data%20points%20and%20the%20mean-field%20states.%20Notably%2C%20our%20method%0Arequires%20only%20training%20once%20to%20estimate%20the%20value%20of%20all%20data%20points%2C%0Asignificantly%20improving%20the%20computational%20efficiency.%20We%20conduct%20comprehensive%0Aexperiments%20using%20different%20datasets%20and%20tasks.%20The%20results%20demonstrate%20that%0Athe%20proposed%20NDDV%20method%20outperforms%20the%20existing%20state-of-the-art%20data%0Avaluation%20methods%20in%20accurately%20identifying%20data%20points%20with%20either%20high%20or%20low%0Avalues%20and%20is%20more%20computationally%20efficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19557v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Dynamic%2520Data%2520Valuation%26entry.906535625%3DZhangyong%2520Liang%2520and%2520Huanhuan%2520Gao%2520and%2520Ji%2520Zhang%26entry.1292438233%3D%2520%2520Data%2520constitute%2520the%2520foundational%2520component%2520of%2520the%2520data%2520economy%2520and%2520its%250Amarketplaces.%2520Efficient%2520and%2520fair%2520data%2520valuation%2520has%2520emerged%2520as%2520a%2520topic%2520of%250Asignificant%2520interest.%255C%2520Many%2520approaches%2520based%2520on%2520marginal%2520contribution%2520have%250Ashown%2520promising%2520results%2520in%2520various%2520downstream%2520tasks.%2520However%252C%2520they%2520are%2520well%250Aknown%2520to%2520be%2520computationally%2520expensive%2520as%2520they%2520require%2520training%2520a%2520large%2520number%250Aof%2520utility%2520functions%252C%2520which%2520are%2520used%2520to%2520evaluate%2520the%2520usefulness%2520or%2520value%2520of%2520a%250Agiven%2520dataset%2520for%2520a%2520specific%2520purpose.%2520As%2520a%2520result%252C%2520it%2520has%2520been%2520recognized%2520as%250Ainfeasible%2520to%2520apply%2520these%2520methods%2520to%2520a%2520data%2520marketplace%2520involving%2520large-scale%250Adatasets.%2520Consequently%252C%2520a%2520critical%2520issue%2520arises%253A%2520how%2520can%2520the%2520re-training%2520of%2520the%250Autility%2520function%2520be%2520avoided%253F%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520novel%2520data%250Avaluation%2520method%2520from%2520the%2520perspective%2520of%2520optimal%2520control%252C%2520named%2520the%2520neural%250Adynamic%2520data%2520valuation%2520%2528NDDV%2529.%2520Our%2520method%2520has%2520solid%2520theoretical%2520interpretations%250Ato%2520accurately%2520identify%2520the%2520data%2520valuation%2520via%2520the%2520sensitivity%2520of%2520the%2520data%250Aoptimal%2520control%2520state.%2520In%2520addition%252C%2520we%2520implement%2520a%2520data%2520re-weighting%2520strategy%250Ato%2520capture%2520the%2520unique%2520features%2520of%2520data%2520points%252C%2520ensuring%2520fairness%2520through%2520the%250Ainteraction%2520between%2520data%2520points%2520and%2520the%2520mean-field%2520states.%2520Notably%252C%2520our%2520method%250Arequires%2520only%2520training%2520once%2520to%2520estimate%2520the%2520value%2520of%2520all%2520data%2520points%252C%250Asignificantly%2520improving%2520the%2520computational%2520efficiency.%2520We%2520conduct%2520comprehensive%250Aexperiments%2520using%2520different%2520datasets%2520and%2520tasks.%2520The%2520results%2520demonstrate%2520that%250Athe%2520proposed%2520NDDV%2520method%2520outperforms%2520the%2520existing%2520state-of-the-art%2520data%250Avaluation%2520methods%2520in%2520accurately%2520identifying%2520data%2520points%2520with%2520either%2520high%2520or%2520low%250Avalues%2520and%2520is%2520more%2520computationally%2520efficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19557v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Dynamic%20Data%20Valuation&entry.906535625=Zhangyong%20Liang%20and%20Huanhuan%20Gao%20and%20Ji%20Zhang&entry.1292438233=%20%20Data%20constitute%20the%20foundational%20component%20of%20the%20data%20economy%20and%20its%0Amarketplaces.%20Efficient%20and%20fair%20data%20valuation%20has%20emerged%20as%20a%20topic%20of%0Asignificant%20interest.%5C%20Many%20approaches%20based%20on%20marginal%20contribution%20have%0Ashown%20promising%20results%20in%20various%20downstream%20tasks.%20However%2C%20they%20are%20well%0Aknown%20to%20be%20computationally%20expensive%20as%20they%20require%20training%20a%20large%20number%0Aof%20utility%20functions%2C%20which%20are%20used%20to%20evaluate%20the%20usefulness%20or%20value%20of%20a%0Agiven%20dataset%20for%20a%20specific%20purpose.%20As%20a%20result%2C%20it%20has%20been%20recognized%20as%0Ainfeasible%20to%20apply%20these%20methods%20to%20a%20data%20marketplace%20involving%20large-scale%0Adatasets.%20Consequently%2C%20a%20critical%20issue%20arises%3A%20how%20can%20the%20re-training%20of%20the%0Autility%20function%20be%20avoided%3F%20To%20address%20this%20issue%2C%20we%20propose%20a%20novel%20data%0Avaluation%20method%20from%20the%20perspective%20of%20optimal%20control%2C%20named%20the%20neural%0Adynamic%20data%20valuation%20%28NDDV%29.%20Our%20method%20has%20solid%20theoretical%20interpretations%0Ato%20accurately%20identify%20the%20data%20valuation%20via%20the%20sensitivity%20of%20the%20data%0Aoptimal%20control%20state.%20In%20addition%2C%20we%20implement%20a%20data%20re-weighting%20strategy%0Ato%20capture%20the%20unique%20features%20of%20data%20points%2C%20ensuring%20fairness%20through%20the%0Ainteraction%20between%20data%20points%20and%20the%20mean-field%20states.%20Notably%2C%20our%20method%0Arequires%20only%20training%20once%20to%20estimate%20the%20value%20of%20all%20data%20points%2C%0Asignificantly%20improving%20the%20computational%20efficiency.%20We%20conduct%20comprehensive%0Aexperiments%20using%20different%20datasets%20and%20tasks.%20The%20results%20demonstrate%20that%0Athe%20proposed%20NDDV%20method%20outperforms%20the%20existing%20state-of-the-art%20data%0Avaluation%20methods%20in%20accurately%20identifying%20data%20points%20with%20either%20high%20or%20low%0Avalues%20and%20is%20more%20computationally%20efficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19557v3&entry.124074799=Read"},
{"title": "Vessel Re-identification and Activity Detection in Thermal Domain for\n  Maritime Surveillance", "author": "Yasod Ginige and Ransika Gunasekara and Darsha Hewavitharana and Manjula Ariyarathne and Ranga Rodrigo and Peshala Jayasekara", "abstract": "  Maritime surveillance is vital to mitigate illegal activities such as drug\nsmuggling, illegal fishing, and human trafficking. Vision-based maritime\nsurveillance is challenging mainly due to visibility issues at night, which\nresults in failures in re-identifying vessels and detecting suspicious\nactivities. In this paper, we introduce a thermal, vision-based approach for\nmaritime surveillance with object tracking, vessel re-identification, and\nsuspicious activity detection capabilities. For vessel re-identification, we\npropose a novel viewpoint-independent algorithm which compares features of the\nsides of the vessel separately (separate side-spaces) leveraging shape\ninformation in the absence of color features. We propose techniques to adapt\ntracking and activity detection algorithms for the thermal domain and train\nthem using a thermal dataset we created. This dataset will be the first\npublicly available benchmark dataset for thermal maritime surveillance. Our\nsystem is capable of re-identifying vessels with an 81.8% Top1 score and\nidentifying suspicious activities with a 72.4\\% frame mAP score; a new\nbenchmark for each task in the thermal domain.\n", "link": "http://arxiv.org/abs/2406.08294v1", "date": "2024-06-12", "relevancy": 2.4926, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5087}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4979}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vessel%20Re-identification%20and%20Activity%20Detection%20in%20Thermal%20Domain%20for%0A%20%20Maritime%20Surveillance&body=Title%3A%20Vessel%20Re-identification%20and%20Activity%20Detection%20in%20Thermal%20Domain%20for%0A%20%20Maritime%20Surveillance%0AAuthor%3A%20Yasod%20Ginige%20and%20Ransika%20Gunasekara%20and%20Darsha%20Hewavitharana%20and%20Manjula%20Ariyarathne%20and%20Ranga%20Rodrigo%20and%20Peshala%20Jayasekara%0AAbstract%3A%20%20%20Maritime%20surveillance%20is%20vital%20to%20mitigate%20illegal%20activities%20such%20as%20drug%0Asmuggling%2C%20illegal%20fishing%2C%20and%20human%20trafficking.%20Vision-based%20maritime%0Asurveillance%20is%20challenging%20mainly%20due%20to%20visibility%20issues%20at%20night%2C%20which%0Aresults%20in%20failures%20in%20re-identifying%20vessels%20and%20detecting%20suspicious%0Aactivities.%20In%20this%20paper%2C%20we%20introduce%20a%20thermal%2C%20vision-based%20approach%20for%0Amaritime%20surveillance%20with%20object%20tracking%2C%20vessel%20re-identification%2C%20and%0Asuspicious%20activity%20detection%20capabilities.%20For%20vessel%20re-identification%2C%20we%0Apropose%20a%20novel%20viewpoint-independent%20algorithm%20which%20compares%20features%20of%20the%0Asides%20of%20the%20vessel%20separately%20%28separate%20side-spaces%29%20leveraging%20shape%0Ainformation%20in%20the%20absence%20of%20color%20features.%20We%20propose%20techniques%20to%20adapt%0Atracking%20and%20activity%20detection%20algorithms%20for%20the%20thermal%20domain%20and%20train%0Athem%20using%20a%20thermal%20dataset%20we%20created.%20This%20dataset%20will%20be%20the%20first%0Apublicly%20available%20benchmark%20dataset%20for%20thermal%20maritime%20surveillance.%20Our%0Asystem%20is%20capable%20of%20re-identifying%20vessels%20with%20an%2081.8%25%20Top1%20score%20and%0Aidentifying%20suspicious%20activities%20with%20a%2072.4%5C%25%20frame%20mAP%20score%3B%20a%20new%0Abenchmark%20for%20each%20task%20in%20the%20thermal%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08294v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVessel%2520Re-identification%2520and%2520Activity%2520Detection%2520in%2520Thermal%2520Domain%2520for%250A%2520%2520Maritime%2520Surveillance%26entry.906535625%3DYasod%2520Ginige%2520and%2520Ransika%2520Gunasekara%2520and%2520Darsha%2520Hewavitharana%2520and%2520Manjula%2520Ariyarathne%2520and%2520Ranga%2520Rodrigo%2520and%2520Peshala%2520Jayasekara%26entry.1292438233%3D%2520%2520Maritime%2520surveillance%2520is%2520vital%2520to%2520mitigate%2520illegal%2520activities%2520such%2520as%2520drug%250Asmuggling%252C%2520illegal%2520fishing%252C%2520and%2520human%2520trafficking.%2520Vision-based%2520maritime%250Asurveillance%2520is%2520challenging%2520mainly%2520due%2520to%2520visibility%2520issues%2520at%2520night%252C%2520which%250Aresults%2520in%2520failures%2520in%2520re-identifying%2520vessels%2520and%2520detecting%2520suspicious%250Aactivities.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520thermal%252C%2520vision-based%2520approach%2520for%250Amaritime%2520surveillance%2520with%2520object%2520tracking%252C%2520vessel%2520re-identification%252C%2520and%250Asuspicious%2520activity%2520detection%2520capabilities.%2520For%2520vessel%2520re-identification%252C%2520we%250Apropose%2520a%2520novel%2520viewpoint-independent%2520algorithm%2520which%2520compares%2520features%2520of%2520the%250Asides%2520of%2520the%2520vessel%2520separately%2520%2528separate%2520side-spaces%2529%2520leveraging%2520shape%250Ainformation%2520in%2520the%2520absence%2520of%2520color%2520features.%2520We%2520propose%2520techniques%2520to%2520adapt%250Atracking%2520and%2520activity%2520detection%2520algorithms%2520for%2520the%2520thermal%2520domain%2520and%2520train%250Athem%2520using%2520a%2520thermal%2520dataset%2520we%2520created.%2520This%2520dataset%2520will%2520be%2520the%2520first%250Apublicly%2520available%2520benchmark%2520dataset%2520for%2520thermal%2520maritime%2520surveillance.%2520Our%250Asystem%2520is%2520capable%2520of%2520re-identifying%2520vessels%2520with%2520an%252081.8%2525%2520Top1%2520score%2520and%250Aidentifying%2520suspicious%2520activities%2520with%2520a%252072.4%255C%2525%2520frame%2520mAP%2520score%253B%2520a%2520new%250Abenchmark%2520for%2520each%2520task%2520in%2520the%2520thermal%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08294v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vessel%20Re-identification%20and%20Activity%20Detection%20in%20Thermal%20Domain%20for%0A%20%20Maritime%20Surveillance&entry.906535625=Yasod%20Ginige%20and%20Ransika%20Gunasekara%20and%20Darsha%20Hewavitharana%20and%20Manjula%20Ariyarathne%20and%20Ranga%20Rodrigo%20and%20Peshala%20Jayasekara&entry.1292438233=%20%20Maritime%20surveillance%20is%20vital%20to%20mitigate%20illegal%20activities%20such%20as%20drug%0Asmuggling%2C%20illegal%20fishing%2C%20and%20human%20trafficking.%20Vision-based%20maritime%0Asurveillance%20is%20challenging%20mainly%20due%20to%20visibility%20issues%20at%20night%2C%20which%0Aresults%20in%20failures%20in%20re-identifying%20vessels%20and%20detecting%20suspicious%0Aactivities.%20In%20this%20paper%2C%20we%20introduce%20a%20thermal%2C%20vision-based%20approach%20for%0Amaritime%20surveillance%20with%20object%20tracking%2C%20vessel%20re-identification%2C%20and%0Asuspicious%20activity%20detection%20capabilities.%20For%20vessel%20re-identification%2C%20we%0Apropose%20a%20novel%20viewpoint-independent%20algorithm%20which%20compares%20features%20of%20the%0Asides%20of%20the%20vessel%20separately%20%28separate%20side-spaces%29%20leveraging%20shape%0Ainformation%20in%20the%20absence%20of%20color%20features.%20We%20propose%20techniques%20to%20adapt%0Atracking%20and%20activity%20detection%20algorithms%20for%20the%20thermal%20domain%20and%20train%0Athem%20using%20a%20thermal%20dataset%20we%20created.%20This%20dataset%20will%20be%20the%20first%0Apublicly%20available%20benchmark%20dataset%20for%20thermal%20maritime%20surveillance.%20Our%0Asystem%20is%20capable%20of%20re-identifying%20vessels%20with%20an%2081.8%25%20Top1%20score%20and%0Aidentifying%20suspicious%20activities%20with%20a%2072.4%5C%25%20frame%20mAP%20score%3B%20a%20new%0Abenchmark%20for%20each%20task%20in%20the%20thermal%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08294v1&entry.124074799=Read"},
{"title": "3D CBCT Challenge 2024: Improved Cone Beam CT Reconstruction using\n  SwinIR-Based Sinogram and Image Enhancement", "author": "Sasidhar Alavala and Subrahmanyam Gorthi", "abstract": "  In this paper, we present our approach to the 3D CBCT Challenge 2024, a part\nof ICASSP SP Grand Challenges 2024. Improvement in Cone Beam Computed\nTomography (CBCT) reconstruction has been achieved by integrating Swin Image\nRestoration (SwinIR) based sinogram and image enhancement modules. The proposed\nmethodology uses Nesterov Accelerated Gradient Descent (NAG) to solve the least\nsquares (NAG-LS) problem in CT image reconstruction. The integration of\nsinogram and image enhancement modules aims to enhance image clarity and\npreserve fine details, offering a promising solution for both low dose and\nclinical dose CBCT reconstruction. The averaged mean squared error (MSE) over\nthe validation dataset has decreased significantly, in the case of low dose by\none-fifth and clinical dose by one-tenth. Our solution is one of the top 5\napproaches in this challenge.\n", "link": "http://arxiv.org/abs/2406.08048v1", "date": "2024-06-12", "relevancy": 2.4914, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5091}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4929}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20CBCT%20Challenge%202024%3A%20Improved%20Cone%20Beam%20CT%20Reconstruction%20using%0A%20%20SwinIR-Based%20Sinogram%20and%20Image%20Enhancement&body=Title%3A%203D%20CBCT%20Challenge%202024%3A%20Improved%20Cone%20Beam%20CT%20Reconstruction%20using%0A%20%20SwinIR-Based%20Sinogram%20and%20Image%20Enhancement%0AAuthor%3A%20Sasidhar%20Alavala%20and%20Subrahmanyam%20Gorthi%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20our%20approach%20to%20the%203D%20CBCT%20Challenge%202024%2C%20a%20part%0Aof%20ICASSP%20SP%20Grand%20Challenges%202024.%20Improvement%20in%20Cone%20Beam%20Computed%0ATomography%20%28CBCT%29%20reconstruction%20has%20been%20achieved%20by%20integrating%20Swin%20Image%0ARestoration%20%28SwinIR%29%20based%20sinogram%20and%20image%20enhancement%20modules.%20The%20proposed%0Amethodology%20uses%20Nesterov%20Accelerated%20Gradient%20Descent%20%28NAG%29%20to%20solve%20the%20least%0Asquares%20%28NAG-LS%29%20problem%20in%20CT%20image%20reconstruction.%20The%20integration%20of%0Asinogram%20and%20image%20enhancement%20modules%20aims%20to%20enhance%20image%20clarity%20and%0Apreserve%20fine%20details%2C%20offering%20a%20promising%20solution%20for%20both%20low%20dose%20and%0Aclinical%20dose%20CBCT%20reconstruction.%20The%20averaged%20mean%20squared%20error%20%28MSE%29%20over%0Athe%20validation%20dataset%20has%20decreased%20significantly%2C%20in%20the%20case%20of%20low%20dose%20by%0Aone-fifth%20and%20clinical%20dose%20by%20one-tenth.%20Our%20solution%20is%20one%20of%20the%20top%205%0Aapproaches%20in%20this%20challenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08048v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520CBCT%2520Challenge%25202024%253A%2520Improved%2520Cone%2520Beam%2520CT%2520Reconstruction%2520using%250A%2520%2520SwinIR-Based%2520Sinogram%2520and%2520Image%2520Enhancement%26entry.906535625%3DSasidhar%2520Alavala%2520and%2520Subrahmanyam%2520Gorthi%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520our%2520approach%2520to%2520the%25203D%2520CBCT%2520Challenge%25202024%252C%2520a%2520part%250Aof%2520ICASSP%2520SP%2520Grand%2520Challenges%25202024.%2520Improvement%2520in%2520Cone%2520Beam%2520Computed%250ATomography%2520%2528CBCT%2529%2520reconstruction%2520has%2520been%2520achieved%2520by%2520integrating%2520Swin%2520Image%250ARestoration%2520%2528SwinIR%2529%2520based%2520sinogram%2520and%2520image%2520enhancement%2520modules.%2520The%2520proposed%250Amethodology%2520uses%2520Nesterov%2520Accelerated%2520Gradient%2520Descent%2520%2528NAG%2529%2520to%2520solve%2520the%2520least%250Asquares%2520%2528NAG-LS%2529%2520problem%2520in%2520CT%2520image%2520reconstruction.%2520The%2520integration%2520of%250Asinogram%2520and%2520image%2520enhancement%2520modules%2520aims%2520to%2520enhance%2520image%2520clarity%2520and%250Apreserve%2520fine%2520details%252C%2520offering%2520a%2520promising%2520solution%2520for%2520both%2520low%2520dose%2520and%250Aclinical%2520dose%2520CBCT%2520reconstruction.%2520The%2520averaged%2520mean%2520squared%2520error%2520%2528MSE%2529%2520over%250Athe%2520validation%2520dataset%2520has%2520decreased%2520significantly%252C%2520in%2520the%2520case%2520of%2520low%2520dose%2520by%250Aone-fifth%2520and%2520clinical%2520dose%2520by%2520one-tenth.%2520Our%2520solution%2520is%2520one%2520of%2520the%2520top%25205%250Aapproaches%2520in%2520this%2520challenge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08048v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20CBCT%20Challenge%202024%3A%20Improved%20Cone%20Beam%20CT%20Reconstruction%20using%0A%20%20SwinIR-Based%20Sinogram%20and%20Image%20Enhancement&entry.906535625=Sasidhar%20Alavala%20and%20Subrahmanyam%20Gorthi&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20our%20approach%20to%20the%203D%20CBCT%20Challenge%202024%2C%20a%20part%0Aof%20ICASSP%20SP%20Grand%20Challenges%202024.%20Improvement%20in%20Cone%20Beam%20Computed%0ATomography%20%28CBCT%29%20reconstruction%20has%20been%20achieved%20by%20integrating%20Swin%20Image%0ARestoration%20%28SwinIR%29%20based%20sinogram%20and%20image%20enhancement%20modules.%20The%20proposed%0Amethodology%20uses%20Nesterov%20Accelerated%20Gradient%20Descent%20%28NAG%29%20to%20solve%20the%20least%0Asquares%20%28NAG-LS%29%20problem%20in%20CT%20image%20reconstruction.%20The%20integration%20of%0Asinogram%20and%20image%20enhancement%20modules%20aims%20to%20enhance%20image%20clarity%20and%0Apreserve%20fine%20details%2C%20offering%20a%20promising%20solution%20for%20both%20low%20dose%20and%0Aclinical%20dose%20CBCT%20reconstruction.%20The%20averaged%20mean%20squared%20error%20%28MSE%29%20over%0Athe%20validation%20dataset%20has%20decreased%20significantly%2C%20in%20the%20case%20of%20low%20dose%20by%0Aone-fifth%20and%20clinical%20dose%20by%20one-tenth.%20Our%20solution%20is%20one%20of%20the%20top%205%0Aapproaches%20in%20this%20challenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08048v1&entry.124074799=Read"},
{"title": "CT3D++: Improving 3D Object Detection with Keypoint-induced Channel-wise\n  Transformer", "author": "Hualian Sheng and Sijia Cai and Na Zhao and Bing Deng and Qiao Liang and Min-Jian Zhao and Jieping Ye", "abstract": "  The field of 3D object detection from point clouds is rapidly advancing in\ncomputer vision, aiming to accurately and efficiently detect and localize\nobjects in three-dimensional space. Current 3D detectors commonly fall short in\nterms of flexibility and scalability, with ample room for advancements in\nperformance. In this paper, our objective is to address these limitations by\nintroducing two frameworks for 3D object detection with minimal hand-crafted\ndesign. Firstly, we propose CT3D, which sequentially performs raw-point-based\nembedding, a standard Transformer encoder, and a channel-wise decoder for point\nfeatures within each proposal. Secondly, we present an enhanced network called\nCT3D++, which incorporates geometric and semantic fusion-based embedding to\nextract more valuable and comprehensive proposal-aware information.\nAdditionally, CT3D ++ utilizes a point-to-key bidirectional encoder for more\nefficient feature encoding with reduced computational cost. By replacing the\ncorresponding components of CT3D with these novel modules, CT3D++ achieves\nstate-of-the-art performance on both the KITTI dataset and the large-scale\nWay\\-mo Open Dataset. The source code for our frameworks will be made\naccessible at https://github.com/hlsheng1/CT3D-plusplus.\n", "link": "http://arxiv.org/abs/2406.08152v1", "date": "2024-06-12", "relevancy": 2.4764, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6624}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5994}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.56}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CT3D%2B%2B%3A%20Improving%203D%20Object%20Detection%20with%20Keypoint-induced%20Channel-wise%0A%20%20Transformer&body=Title%3A%20CT3D%2B%2B%3A%20Improving%203D%20Object%20Detection%20with%20Keypoint-induced%20Channel-wise%0A%20%20Transformer%0AAuthor%3A%20Hualian%20Sheng%20and%20Sijia%20Cai%20and%20Na%20Zhao%20and%20Bing%20Deng%20and%20Qiao%20Liang%20and%20Min-Jian%20Zhao%20and%20Jieping%20Ye%0AAbstract%3A%20%20%20The%20field%20of%203D%20object%20detection%20from%20point%20clouds%20is%20rapidly%20advancing%20in%0Acomputer%20vision%2C%20aiming%20to%20accurately%20and%20efficiently%20detect%20and%20localize%0Aobjects%20in%20three-dimensional%20space.%20Current%203D%20detectors%20commonly%20fall%20short%20in%0Aterms%20of%20flexibility%20and%20scalability%2C%20with%20ample%20room%20for%20advancements%20in%0Aperformance.%20In%20this%20paper%2C%20our%20objective%20is%20to%20address%20these%20limitations%20by%0Aintroducing%20two%20frameworks%20for%203D%20object%20detection%20with%20minimal%20hand-crafted%0Adesign.%20Firstly%2C%20we%20propose%20CT3D%2C%20which%20sequentially%20performs%20raw-point-based%0Aembedding%2C%20a%20standard%20Transformer%20encoder%2C%20and%20a%20channel-wise%20decoder%20for%20point%0Afeatures%20within%20each%20proposal.%20Secondly%2C%20we%20present%20an%20enhanced%20network%20called%0ACT3D%2B%2B%2C%20which%20incorporates%20geometric%20and%20semantic%20fusion-based%20embedding%20to%0Aextract%20more%20valuable%20and%20comprehensive%20proposal-aware%20information.%0AAdditionally%2C%20CT3D%20%2B%2B%20utilizes%20a%20point-to-key%20bidirectional%20encoder%20for%20more%0Aefficient%20feature%20encoding%20with%20reduced%20computational%20cost.%20By%20replacing%20the%0Acorresponding%20components%20of%20CT3D%20with%20these%20novel%20modules%2C%20CT3D%2B%2B%20achieves%0Astate-of-the-art%20performance%20on%20both%20the%20KITTI%20dataset%20and%20the%20large-scale%0AWay%5C-mo%20Open%20Dataset.%20The%20source%20code%20for%20our%20frameworks%20will%20be%20made%0Aaccessible%20at%20https%3A//github.com/hlsheng1/CT3D-plusplus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCT3D%252B%252B%253A%2520Improving%25203D%2520Object%2520Detection%2520with%2520Keypoint-induced%2520Channel-wise%250A%2520%2520Transformer%26entry.906535625%3DHualian%2520Sheng%2520and%2520Sijia%2520Cai%2520and%2520Na%2520Zhao%2520and%2520Bing%2520Deng%2520and%2520Qiao%2520Liang%2520and%2520Min-Jian%2520Zhao%2520and%2520Jieping%2520Ye%26entry.1292438233%3D%2520%2520The%2520field%2520of%25203D%2520object%2520detection%2520from%2520point%2520clouds%2520is%2520rapidly%2520advancing%2520in%250Acomputer%2520vision%252C%2520aiming%2520to%2520accurately%2520and%2520efficiently%2520detect%2520and%2520localize%250Aobjects%2520in%2520three-dimensional%2520space.%2520Current%25203D%2520detectors%2520commonly%2520fall%2520short%2520in%250Aterms%2520of%2520flexibility%2520and%2520scalability%252C%2520with%2520ample%2520room%2520for%2520advancements%2520in%250Aperformance.%2520In%2520this%2520paper%252C%2520our%2520objective%2520is%2520to%2520address%2520these%2520limitations%2520by%250Aintroducing%2520two%2520frameworks%2520for%25203D%2520object%2520detection%2520with%2520minimal%2520hand-crafted%250Adesign.%2520Firstly%252C%2520we%2520propose%2520CT3D%252C%2520which%2520sequentially%2520performs%2520raw-point-based%250Aembedding%252C%2520a%2520standard%2520Transformer%2520encoder%252C%2520and%2520a%2520channel-wise%2520decoder%2520for%2520point%250Afeatures%2520within%2520each%2520proposal.%2520Secondly%252C%2520we%2520present%2520an%2520enhanced%2520network%2520called%250ACT3D%252B%252B%252C%2520which%2520incorporates%2520geometric%2520and%2520semantic%2520fusion-based%2520embedding%2520to%250Aextract%2520more%2520valuable%2520and%2520comprehensive%2520proposal-aware%2520information.%250AAdditionally%252C%2520CT3D%2520%252B%252B%2520utilizes%2520a%2520point-to-key%2520bidirectional%2520encoder%2520for%2520more%250Aefficient%2520feature%2520encoding%2520with%2520reduced%2520computational%2520cost.%2520By%2520replacing%2520the%250Acorresponding%2520components%2520of%2520CT3D%2520with%2520these%2520novel%2520modules%252C%2520CT3D%252B%252B%2520achieves%250Astate-of-the-art%2520performance%2520on%2520both%2520the%2520KITTI%2520dataset%2520and%2520the%2520large-scale%250AWay%255C-mo%2520Open%2520Dataset.%2520The%2520source%2520code%2520for%2520our%2520frameworks%2520will%2520be%2520made%250Aaccessible%2520at%2520https%253A//github.com/hlsheng1/CT3D-plusplus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CT3D%2B%2B%3A%20Improving%203D%20Object%20Detection%20with%20Keypoint-induced%20Channel-wise%0A%20%20Transformer&entry.906535625=Hualian%20Sheng%20and%20Sijia%20Cai%20and%20Na%20Zhao%20and%20Bing%20Deng%20and%20Qiao%20Liang%20and%20Min-Jian%20Zhao%20and%20Jieping%20Ye&entry.1292438233=%20%20The%20field%20of%203D%20object%20detection%20from%20point%20clouds%20is%20rapidly%20advancing%20in%0Acomputer%20vision%2C%20aiming%20to%20accurately%20and%20efficiently%20detect%20and%20localize%0Aobjects%20in%20three-dimensional%20space.%20Current%203D%20detectors%20commonly%20fall%20short%20in%0Aterms%20of%20flexibility%20and%20scalability%2C%20with%20ample%20room%20for%20advancements%20in%0Aperformance.%20In%20this%20paper%2C%20our%20objective%20is%20to%20address%20these%20limitations%20by%0Aintroducing%20two%20frameworks%20for%203D%20object%20detection%20with%20minimal%20hand-crafted%0Adesign.%20Firstly%2C%20we%20propose%20CT3D%2C%20which%20sequentially%20performs%20raw-point-based%0Aembedding%2C%20a%20standard%20Transformer%20encoder%2C%20and%20a%20channel-wise%20decoder%20for%20point%0Afeatures%20within%20each%20proposal.%20Secondly%2C%20we%20present%20an%20enhanced%20network%20called%0ACT3D%2B%2B%2C%20which%20incorporates%20geometric%20and%20semantic%20fusion-based%20embedding%20to%0Aextract%20more%20valuable%20and%20comprehensive%20proposal-aware%20information.%0AAdditionally%2C%20CT3D%20%2B%2B%20utilizes%20a%20point-to-key%20bidirectional%20encoder%20for%20more%0Aefficient%20feature%20encoding%20with%20reduced%20computational%20cost.%20By%20replacing%20the%0Acorresponding%20components%20of%20CT3D%20with%20these%20novel%20modules%2C%20CT3D%2B%2B%20achieves%0Astate-of-the-art%20performance%20on%20both%20the%20KITTI%20dataset%20and%20the%20large-scale%0AWay%5C-mo%20Open%20Dataset.%20The%20source%20code%20for%20our%20frameworks%20will%20be%20made%0Aaccessible%20at%20https%3A//github.com/hlsheng1/CT3D-plusplus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08152v1&entry.124074799=Read"},
{"title": "Nonconvex Federated Learning on Compact Smooth Submanifolds With\n  Heterogeneous Data", "author": "Jiaojiao Zhang and Jiang Hu and Anthony Man-Cho So and Mikael Johansson", "abstract": "  Many machine learning tasks, such as principal component analysis and\nlow-rank matrix completion, give rise to manifold optimization problems.\nAlthough there is a large body of work studying the design and analysis of\nalgorithms for manifold optimization in the centralized setting, there are\ncurrently very few works addressing the federated setting. In this paper, we\nconsider nonconvex federated learning over a compact smooth submanifold in the\nsetting of heterogeneous client data. We propose an algorithm that leverages\nstochastic Riemannian gradients and a manifold projection operator to improve\ncomputational efficiency, uses local updates to improve communication\nefficiency, and avoids client drift. Theoretically, we show that our proposed\nalgorithm converges sub-linearly to a neighborhood of a first-order optimal\nsolution by using a novel analysis that jointly exploits the manifold structure\nand properties of the loss functions. Numerical experiments demonstrate that\nour algorithm has significantly smaller computational and communication\noverhead than existing methods.\n", "link": "http://arxiv.org/abs/2406.08465v1", "date": "2024-06-12", "relevancy": 2.4313, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4943}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4934}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nonconvex%20Federated%20Learning%20on%20Compact%20Smooth%20Submanifolds%20With%0A%20%20Heterogeneous%20Data&body=Title%3A%20Nonconvex%20Federated%20Learning%20on%20Compact%20Smooth%20Submanifolds%20With%0A%20%20Heterogeneous%20Data%0AAuthor%3A%20Jiaojiao%20Zhang%20and%20Jiang%20Hu%20and%20Anthony%20Man-Cho%20So%20and%20Mikael%20Johansson%0AAbstract%3A%20%20%20Many%20machine%20learning%20tasks%2C%20such%20as%20principal%20component%20analysis%20and%0Alow-rank%20matrix%20completion%2C%20give%20rise%20to%20manifold%20optimization%20problems.%0AAlthough%20there%20is%20a%20large%20body%20of%20work%20studying%20the%20design%20and%20analysis%20of%0Aalgorithms%20for%20manifold%20optimization%20in%20the%20centralized%20setting%2C%20there%20are%0Acurrently%20very%20few%20works%20addressing%20the%20federated%20setting.%20In%20this%20paper%2C%20we%0Aconsider%20nonconvex%20federated%20learning%20over%20a%20compact%20smooth%20submanifold%20in%20the%0Asetting%20of%20heterogeneous%20client%20data.%20We%20propose%20an%20algorithm%20that%20leverages%0Astochastic%20Riemannian%20gradients%20and%20a%20manifold%20projection%20operator%20to%20improve%0Acomputational%20efficiency%2C%20uses%20local%20updates%20to%20improve%20communication%0Aefficiency%2C%20and%20avoids%20client%20drift.%20Theoretically%2C%20we%20show%20that%20our%20proposed%0Aalgorithm%20converges%20sub-linearly%20to%20a%20neighborhood%20of%20a%20first-order%20optimal%0Asolution%20by%20using%20a%20novel%20analysis%20that%20jointly%20exploits%20the%20manifold%20structure%0Aand%20properties%20of%20the%20loss%20functions.%20Numerical%20experiments%20demonstrate%20that%0Aour%20algorithm%20has%20significantly%20smaller%20computational%20and%20communication%0Aoverhead%20than%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNonconvex%2520Federated%2520Learning%2520on%2520Compact%2520Smooth%2520Submanifolds%2520With%250A%2520%2520Heterogeneous%2520Data%26entry.906535625%3DJiaojiao%2520Zhang%2520and%2520Jiang%2520Hu%2520and%2520Anthony%2520Man-Cho%2520So%2520and%2520Mikael%2520Johansson%26entry.1292438233%3D%2520%2520Many%2520machine%2520learning%2520tasks%252C%2520such%2520as%2520principal%2520component%2520analysis%2520and%250Alow-rank%2520matrix%2520completion%252C%2520give%2520rise%2520to%2520manifold%2520optimization%2520problems.%250AAlthough%2520there%2520is%2520a%2520large%2520body%2520of%2520work%2520studying%2520the%2520design%2520and%2520analysis%2520of%250Aalgorithms%2520for%2520manifold%2520optimization%2520in%2520the%2520centralized%2520setting%252C%2520there%2520are%250Acurrently%2520very%2520few%2520works%2520addressing%2520the%2520federated%2520setting.%2520In%2520this%2520paper%252C%2520we%250Aconsider%2520nonconvex%2520federated%2520learning%2520over%2520a%2520compact%2520smooth%2520submanifold%2520in%2520the%250Asetting%2520of%2520heterogeneous%2520client%2520data.%2520We%2520propose%2520an%2520algorithm%2520that%2520leverages%250Astochastic%2520Riemannian%2520gradients%2520and%2520a%2520manifold%2520projection%2520operator%2520to%2520improve%250Acomputational%2520efficiency%252C%2520uses%2520local%2520updates%2520to%2520improve%2520communication%250Aefficiency%252C%2520and%2520avoids%2520client%2520drift.%2520Theoretically%252C%2520we%2520show%2520that%2520our%2520proposed%250Aalgorithm%2520converges%2520sub-linearly%2520to%2520a%2520neighborhood%2520of%2520a%2520first-order%2520optimal%250Asolution%2520by%2520using%2520a%2520novel%2520analysis%2520that%2520jointly%2520exploits%2520the%2520manifold%2520structure%250Aand%2520properties%2520of%2520the%2520loss%2520functions.%2520Numerical%2520experiments%2520demonstrate%2520that%250Aour%2520algorithm%2520has%2520significantly%2520smaller%2520computational%2520and%2520communication%250Aoverhead%2520than%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonconvex%20Federated%20Learning%20on%20Compact%20Smooth%20Submanifolds%20With%0A%20%20Heterogeneous%20Data&entry.906535625=Jiaojiao%20Zhang%20and%20Jiang%20Hu%20and%20Anthony%20Man-Cho%20So%20and%20Mikael%20Johansson&entry.1292438233=%20%20Many%20machine%20learning%20tasks%2C%20such%20as%20principal%20component%20analysis%20and%0Alow-rank%20matrix%20completion%2C%20give%20rise%20to%20manifold%20optimization%20problems.%0AAlthough%20there%20is%20a%20large%20body%20of%20work%20studying%20the%20design%20and%20analysis%20of%0Aalgorithms%20for%20manifold%20optimization%20in%20the%20centralized%20setting%2C%20there%20are%0Acurrently%20very%20few%20works%20addressing%20the%20federated%20setting.%20In%20this%20paper%2C%20we%0Aconsider%20nonconvex%20federated%20learning%20over%20a%20compact%20smooth%20submanifold%20in%20the%0Asetting%20of%20heterogeneous%20client%20data.%20We%20propose%20an%20algorithm%20that%20leverages%0Astochastic%20Riemannian%20gradients%20and%20a%20manifold%20projection%20operator%20to%20improve%0Acomputational%20efficiency%2C%20uses%20local%20updates%20to%20improve%20communication%0Aefficiency%2C%20and%20avoids%20client%20drift.%20Theoretically%2C%20we%20show%20that%20our%20proposed%0Aalgorithm%20converges%20sub-linearly%20to%20a%20neighborhood%20of%20a%20first-order%20optimal%0Asolution%20by%20using%20a%20novel%20analysis%20that%20jointly%20exploits%20the%20manifold%20structure%0Aand%20properties%20of%20the%20loss%20functions.%20Numerical%20experiments%20demonstrate%20that%0Aour%20algorithm%20has%20significantly%20smaller%20computational%20and%20communication%0Aoverhead%20than%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08465v1&entry.124074799=Read"},
{"title": "MotionClone: Training-Free Motion Cloning for Controllable Video\n  Generation", "author": "Pengyang Ling and Jiazi Bu and Pan Zhang and Xiaoyi Dong and Yuhang Zang and Tong Wu and Huaian Chen and Jiaqi Wang and Yi Jin", "abstract": "  Motion-based controllable text-to-video generation involves motions to\ncontrol the video generation. Previous methods typically require the training\nof models to encode motion cues or the fine-tuning of video diffusion models.\nHowever, these approaches often result in suboptimal motion generation when\napplied outside the trained domain. In this work, we propose MotionClone, a\ntraining-free framework that enables motion cloning from a reference video to\ncontrol text-to-video generation. We employ temporal attention in video\ninversion to represent the motions in the reference video and introduce primary\ntemporal-attention guidance to mitigate the influence of noisy or very subtle\nmotions within the attention weights. Furthermore, to assist the generation\nmodel in synthesizing reasonable spatial relationships and enhance its\nprompt-following capability, we propose a location-aware semantic guidance\nmechanism that leverages the coarse location of the foreground from the\nreference video and original classifier-free guidance features to guide the\nvideo generation. Extensive experiments demonstrate that MotionClone exhibits\nproficiency in both global camera motion and local object motion, with notable\nsuperiority in terms of motion fidelity, textual alignment, and temporal\nconsistency.\n", "link": "http://arxiv.org/abs/2406.05338v2", "date": "2024-06-12", "relevancy": 2.4279, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6788}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.612}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MotionClone%3A%20Training-Free%20Motion%20Cloning%20for%20Controllable%20Video%0A%20%20Generation&body=Title%3A%20MotionClone%3A%20Training-Free%20Motion%20Cloning%20for%20Controllable%20Video%0A%20%20Generation%0AAuthor%3A%20Pengyang%20Ling%20and%20Jiazi%20Bu%20and%20Pan%20Zhang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Tong%20Wu%20and%20Huaian%20Chen%20and%20Jiaqi%20Wang%20and%20Yi%20Jin%0AAbstract%3A%20%20%20Motion-based%20controllable%20text-to-video%20generation%20involves%20motions%20to%0Acontrol%20the%20video%20generation.%20Previous%20methods%20typically%20require%20the%20training%0Aof%20models%20to%20encode%20motion%20cues%20or%20the%20fine-tuning%20of%20video%20diffusion%20models.%0AHowever%2C%20these%20approaches%20often%20result%20in%20suboptimal%20motion%20generation%20when%0Aapplied%20outside%20the%20trained%20domain.%20In%20this%20work%2C%20we%20propose%20MotionClone%2C%20a%0Atraining-free%20framework%20that%20enables%20motion%20cloning%20from%20a%20reference%20video%20to%0Acontrol%20text-to-video%20generation.%20We%20employ%20temporal%20attention%20in%20video%0Ainversion%20to%20represent%20the%20motions%20in%20the%20reference%20video%20and%20introduce%20primary%0Atemporal-attention%20guidance%20to%20mitigate%20the%20influence%20of%20noisy%20or%20very%20subtle%0Amotions%20within%20the%20attention%20weights.%20Furthermore%2C%20to%20assist%20the%20generation%0Amodel%20in%20synthesizing%20reasonable%20spatial%20relationships%20and%20enhance%20its%0Aprompt-following%20capability%2C%20we%20propose%20a%20location-aware%20semantic%20guidance%0Amechanism%20that%20leverages%20the%20coarse%20location%20of%20the%20foreground%20from%20the%0Areference%20video%20and%20original%20classifier-free%20guidance%20features%20to%20guide%20the%0Avideo%20generation.%20Extensive%20experiments%20demonstrate%20that%20MotionClone%20exhibits%0Aproficiency%20in%20both%20global%20camera%20motion%20and%20local%20object%20motion%2C%20with%20notable%0Asuperiority%20in%20terms%20of%20motion%20fidelity%2C%20textual%20alignment%2C%20and%20temporal%0Aconsistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05338v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotionClone%253A%2520Training-Free%2520Motion%2520Cloning%2520for%2520Controllable%2520Video%250A%2520%2520Generation%26entry.906535625%3DPengyang%2520Ling%2520and%2520Jiazi%2520Bu%2520and%2520Pan%2520Zhang%2520and%2520Xiaoyi%2520Dong%2520and%2520Yuhang%2520Zang%2520and%2520Tong%2520Wu%2520and%2520Huaian%2520Chen%2520and%2520Jiaqi%2520Wang%2520and%2520Yi%2520Jin%26entry.1292438233%3D%2520%2520Motion-based%2520controllable%2520text-to-video%2520generation%2520involves%2520motions%2520to%250Acontrol%2520the%2520video%2520generation.%2520Previous%2520methods%2520typically%2520require%2520the%2520training%250Aof%2520models%2520to%2520encode%2520motion%2520cues%2520or%2520the%2520fine-tuning%2520of%2520video%2520diffusion%2520models.%250AHowever%252C%2520these%2520approaches%2520often%2520result%2520in%2520suboptimal%2520motion%2520generation%2520when%250Aapplied%2520outside%2520the%2520trained%2520domain.%2520In%2520this%2520work%252C%2520we%2520propose%2520MotionClone%252C%2520a%250Atraining-free%2520framework%2520that%2520enables%2520motion%2520cloning%2520from%2520a%2520reference%2520video%2520to%250Acontrol%2520text-to-video%2520generation.%2520We%2520employ%2520temporal%2520attention%2520in%2520video%250Ainversion%2520to%2520represent%2520the%2520motions%2520in%2520the%2520reference%2520video%2520and%2520introduce%2520primary%250Atemporal-attention%2520guidance%2520to%2520mitigate%2520the%2520influence%2520of%2520noisy%2520or%2520very%2520subtle%250Amotions%2520within%2520the%2520attention%2520weights.%2520Furthermore%252C%2520to%2520assist%2520the%2520generation%250Amodel%2520in%2520synthesizing%2520reasonable%2520spatial%2520relationships%2520and%2520enhance%2520its%250Aprompt-following%2520capability%252C%2520we%2520propose%2520a%2520location-aware%2520semantic%2520guidance%250Amechanism%2520that%2520leverages%2520the%2520coarse%2520location%2520of%2520the%2520foreground%2520from%2520the%250Areference%2520video%2520and%2520original%2520classifier-free%2520guidance%2520features%2520to%2520guide%2520the%250Avideo%2520generation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520MotionClone%2520exhibits%250Aproficiency%2520in%2520both%2520global%2520camera%2520motion%2520and%2520local%2520object%2520motion%252C%2520with%2520notable%250Asuperiority%2520in%2520terms%2520of%2520motion%2520fidelity%252C%2520textual%2520alignment%252C%2520and%2520temporal%250Aconsistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05338v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MotionClone%3A%20Training-Free%20Motion%20Cloning%20for%20Controllable%20Video%0A%20%20Generation&entry.906535625=Pengyang%20Ling%20and%20Jiazi%20Bu%20and%20Pan%20Zhang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Tong%20Wu%20and%20Huaian%20Chen%20and%20Jiaqi%20Wang%20and%20Yi%20Jin&entry.1292438233=%20%20Motion-based%20controllable%20text-to-video%20generation%20involves%20motions%20to%0Acontrol%20the%20video%20generation.%20Previous%20methods%20typically%20require%20the%20training%0Aof%20models%20to%20encode%20motion%20cues%20or%20the%20fine-tuning%20of%20video%20diffusion%20models.%0AHowever%2C%20these%20approaches%20often%20result%20in%20suboptimal%20motion%20generation%20when%0Aapplied%20outside%20the%20trained%20domain.%20In%20this%20work%2C%20we%20propose%20MotionClone%2C%20a%0Atraining-free%20framework%20that%20enables%20motion%20cloning%20from%20a%20reference%20video%20to%0Acontrol%20text-to-video%20generation.%20We%20employ%20temporal%20attention%20in%20video%0Ainversion%20to%20represent%20the%20motions%20in%20the%20reference%20video%20and%20introduce%20primary%0Atemporal-attention%20guidance%20to%20mitigate%20the%20influence%20of%20noisy%20or%20very%20subtle%0Amotions%20within%20the%20attention%20weights.%20Furthermore%2C%20to%20assist%20the%20generation%0Amodel%20in%20synthesizing%20reasonable%20spatial%20relationships%20and%20enhance%20its%0Aprompt-following%20capability%2C%20we%20propose%20a%20location-aware%20semantic%20guidance%0Amechanism%20that%20leverages%20the%20coarse%20location%20of%20the%20foreground%20from%20the%0Areference%20video%20and%20original%20classifier-free%20guidance%20features%20to%20guide%20the%0Avideo%20generation.%20Extensive%20experiments%20demonstrate%20that%20MotionClone%20exhibits%0Aproficiency%20in%20both%20global%20camera%20motion%20and%20local%20object%20motion%2C%20with%20notable%0Asuperiority%20in%20terms%20of%20motion%20fidelity%2C%20textual%20alignment%2C%20and%20temporal%0Aconsistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05338v2&entry.124074799=Read"},
{"title": "Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs\n  with Nothing", "author": "Zhangchen Xu and Fengqing Jiang and Luyao Niu and Yuntian Deng and Radha Poovendran and Yejin Choi and Bill Yuchen Lin", "abstract": "  High-quality instruction data is critical for aligning large language models\n(LLMs). Although some models, such as Llama-3-Instruct, have open weights,\ntheir alignment data remain private, which hinders the democratization of AI.\nHigh human labor costs and a limited, predefined scope for prompting prevent\nexisting open-source data creation methods from scaling effectively,\npotentially limiting the diversity and quality of public alignment datasets. Is\nit possible to synthesize high-quality instruction data at scale by extracting\nit directly from an aligned LLM? We present a self-synthesis method for\ngenerating large-scale alignment data named Magpie. Our key observation is that\naligned LLMs like Llama-3-Instruct can generate a user query when we input only\nthe left-side templates up to the position reserved for user messages, thanks\nto their auto-regressive nature. We use this method to prompt Llama-3-Instruct\nand generate 4 million instructions along with their corresponding responses.\nWe perform a comprehensive analysis of the extracted data and select 300K\nhigh-quality instances. To compare Magpie data with other public instruction\ndatasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the\nperformance of the fine-tuned models. Our results indicate that in some tasks,\nmodels fine-tuned with Magpie perform comparably to the official\nLlama-3-8B-Instruct, despite the latter being enhanced with 10 million data\npoints through supervised fine-tuning (SFT) and subsequent feedback learning.\nWe also show that using Magpie solely for SFT can surpass the performance of\nprevious public datasets utilized for both SFT and preference optimization,\nsuch as direct preference optimization with UltraFeedback. This advantage is\nevident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.\n", "link": "http://arxiv.org/abs/2406.08464v1", "date": "2024-06-12", "relevancy": 2.4186, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.486}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4833}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Magpie%3A%20Alignment%20Data%20Synthesis%20from%20Scratch%20by%20Prompting%20Aligned%20LLMs%0A%20%20with%20Nothing&body=Title%3A%20Magpie%3A%20Alignment%20Data%20Synthesis%20from%20Scratch%20by%20Prompting%20Aligned%20LLMs%0A%20%20with%20Nothing%0AAuthor%3A%20Zhangchen%20Xu%20and%20Fengqing%20Jiang%20and%20Luyao%20Niu%20and%20Yuntian%20Deng%20and%20Radha%20Poovendran%20and%20Yejin%20Choi%20and%20Bill%20Yuchen%20Lin%0AAbstract%3A%20%20%20High-quality%20instruction%20data%20is%20critical%20for%20aligning%20large%20language%20models%0A%28LLMs%29.%20Although%20some%20models%2C%20such%20as%20Llama-3-Instruct%2C%20have%20open%20weights%2C%0Atheir%20alignment%20data%20remain%20private%2C%20which%20hinders%20the%20democratization%20of%20AI.%0AHigh%20human%20labor%20costs%20and%20a%20limited%2C%20predefined%20scope%20for%20prompting%20prevent%0Aexisting%20open-source%20data%20creation%20methods%20from%20scaling%20effectively%2C%0Apotentially%20limiting%20the%20diversity%20and%20quality%20of%20public%20alignment%20datasets.%20Is%0Ait%20possible%20to%20synthesize%20high-quality%20instruction%20data%20at%20scale%20by%20extracting%0Ait%20directly%20from%20an%20aligned%20LLM%3F%20We%20present%20a%20self-synthesis%20method%20for%0Agenerating%20large-scale%20alignment%20data%20named%20Magpie.%20Our%20key%20observation%20is%20that%0Aaligned%20LLMs%20like%20Llama-3-Instruct%20can%20generate%20a%20user%20query%20when%20we%20input%20only%0Athe%20left-side%20templates%20up%20to%20the%20position%20reserved%20for%20user%20messages%2C%20thanks%0Ato%20their%20auto-regressive%20nature.%20We%20use%20this%20method%20to%20prompt%20Llama-3-Instruct%0Aand%20generate%204%20million%20instructions%20along%20with%20their%20corresponding%20responses.%0AWe%20perform%20a%20comprehensive%20analysis%20of%20the%20extracted%20data%20and%20select%20300K%0Ahigh-quality%20instances.%20To%20compare%20Magpie%20data%20with%20other%20public%20instruction%0Adatasets%2C%20we%20fine-tune%20Llama-3-8B-Base%20with%20each%20dataset%20and%20evaluate%20the%0Aperformance%20of%20the%20fine-tuned%20models.%20Our%20results%20indicate%20that%20in%20some%20tasks%2C%0Amodels%20fine-tuned%20with%20Magpie%20perform%20comparably%20to%20the%20official%0ALlama-3-8B-Instruct%2C%20despite%20the%20latter%20being%20enhanced%20with%2010%20million%20data%0Apoints%20through%20supervised%20fine-tuning%20%28SFT%29%20and%20subsequent%20feedback%20learning.%0AWe%20also%20show%20that%20using%20Magpie%20solely%20for%20SFT%20can%20surpass%20the%20performance%20of%0Aprevious%20public%20datasets%20utilized%20for%20both%20SFT%20and%20preference%20optimization%2C%0Asuch%20as%20direct%20preference%20optimization%20with%20UltraFeedback.%20This%20advantage%20is%0Aevident%20on%20alignment%20benchmarks%20such%20as%20AlpacaEval%2C%20ArenaHard%2C%20and%20WildBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagpie%253A%2520Alignment%2520Data%2520Synthesis%2520from%2520Scratch%2520by%2520Prompting%2520Aligned%2520LLMs%250A%2520%2520with%2520Nothing%26entry.906535625%3DZhangchen%2520Xu%2520and%2520Fengqing%2520Jiang%2520and%2520Luyao%2520Niu%2520and%2520Yuntian%2520Deng%2520and%2520Radha%2520Poovendran%2520and%2520Yejin%2520Choi%2520and%2520Bill%2520Yuchen%2520Lin%26entry.1292438233%3D%2520%2520High-quality%2520instruction%2520data%2520is%2520critical%2520for%2520aligning%2520large%2520language%2520models%250A%2528LLMs%2529.%2520Although%2520some%2520models%252C%2520such%2520as%2520Llama-3-Instruct%252C%2520have%2520open%2520weights%252C%250Atheir%2520alignment%2520data%2520remain%2520private%252C%2520which%2520hinders%2520the%2520democratization%2520of%2520AI.%250AHigh%2520human%2520labor%2520costs%2520and%2520a%2520limited%252C%2520predefined%2520scope%2520for%2520prompting%2520prevent%250Aexisting%2520open-source%2520data%2520creation%2520methods%2520from%2520scaling%2520effectively%252C%250Apotentially%2520limiting%2520the%2520diversity%2520and%2520quality%2520of%2520public%2520alignment%2520datasets.%2520Is%250Ait%2520possible%2520to%2520synthesize%2520high-quality%2520instruction%2520data%2520at%2520scale%2520by%2520extracting%250Ait%2520directly%2520from%2520an%2520aligned%2520LLM%253F%2520We%2520present%2520a%2520self-synthesis%2520method%2520for%250Agenerating%2520large-scale%2520alignment%2520data%2520named%2520Magpie.%2520Our%2520key%2520observation%2520is%2520that%250Aaligned%2520LLMs%2520like%2520Llama-3-Instruct%2520can%2520generate%2520a%2520user%2520query%2520when%2520we%2520input%2520only%250Athe%2520left-side%2520templates%2520up%2520to%2520the%2520position%2520reserved%2520for%2520user%2520messages%252C%2520thanks%250Ato%2520their%2520auto-regressive%2520nature.%2520We%2520use%2520this%2520method%2520to%2520prompt%2520Llama-3-Instruct%250Aand%2520generate%25204%2520million%2520instructions%2520along%2520with%2520their%2520corresponding%2520responses.%250AWe%2520perform%2520a%2520comprehensive%2520analysis%2520of%2520the%2520extracted%2520data%2520and%2520select%2520300K%250Ahigh-quality%2520instances.%2520To%2520compare%2520Magpie%2520data%2520with%2520other%2520public%2520instruction%250Adatasets%252C%2520we%2520fine-tune%2520Llama-3-8B-Base%2520with%2520each%2520dataset%2520and%2520evaluate%2520the%250Aperformance%2520of%2520the%2520fine-tuned%2520models.%2520Our%2520results%2520indicate%2520that%2520in%2520some%2520tasks%252C%250Amodels%2520fine-tuned%2520with%2520Magpie%2520perform%2520comparably%2520to%2520the%2520official%250ALlama-3-8B-Instruct%252C%2520despite%2520the%2520latter%2520being%2520enhanced%2520with%252010%2520million%2520data%250Apoints%2520through%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520subsequent%2520feedback%2520learning.%250AWe%2520also%2520show%2520that%2520using%2520Magpie%2520solely%2520for%2520SFT%2520can%2520surpass%2520the%2520performance%2520of%250Aprevious%2520public%2520datasets%2520utilized%2520for%2520both%2520SFT%2520and%2520preference%2520optimization%252C%250Asuch%2520as%2520direct%2520preference%2520optimization%2520with%2520UltraFeedback.%2520This%2520advantage%2520is%250Aevident%2520on%2520alignment%2520benchmarks%2520such%2520as%2520AlpacaEval%252C%2520ArenaHard%252C%2520and%2520WildBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Magpie%3A%20Alignment%20Data%20Synthesis%20from%20Scratch%20by%20Prompting%20Aligned%20LLMs%0A%20%20with%20Nothing&entry.906535625=Zhangchen%20Xu%20and%20Fengqing%20Jiang%20and%20Luyao%20Niu%20and%20Yuntian%20Deng%20and%20Radha%20Poovendran%20and%20Yejin%20Choi%20and%20Bill%20Yuchen%20Lin&entry.1292438233=%20%20High-quality%20instruction%20data%20is%20critical%20for%20aligning%20large%20language%20models%0A%28LLMs%29.%20Although%20some%20models%2C%20such%20as%20Llama-3-Instruct%2C%20have%20open%20weights%2C%0Atheir%20alignment%20data%20remain%20private%2C%20which%20hinders%20the%20democratization%20of%20AI.%0AHigh%20human%20labor%20costs%20and%20a%20limited%2C%20predefined%20scope%20for%20prompting%20prevent%0Aexisting%20open-source%20data%20creation%20methods%20from%20scaling%20effectively%2C%0Apotentially%20limiting%20the%20diversity%20and%20quality%20of%20public%20alignment%20datasets.%20Is%0Ait%20possible%20to%20synthesize%20high-quality%20instruction%20data%20at%20scale%20by%20extracting%0Ait%20directly%20from%20an%20aligned%20LLM%3F%20We%20present%20a%20self-synthesis%20method%20for%0Agenerating%20large-scale%20alignment%20data%20named%20Magpie.%20Our%20key%20observation%20is%20that%0Aaligned%20LLMs%20like%20Llama-3-Instruct%20can%20generate%20a%20user%20query%20when%20we%20input%20only%0Athe%20left-side%20templates%20up%20to%20the%20position%20reserved%20for%20user%20messages%2C%20thanks%0Ato%20their%20auto-regressive%20nature.%20We%20use%20this%20method%20to%20prompt%20Llama-3-Instruct%0Aand%20generate%204%20million%20instructions%20along%20with%20their%20corresponding%20responses.%0AWe%20perform%20a%20comprehensive%20analysis%20of%20the%20extracted%20data%20and%20select%20300K%0Ahigh-quality%20instances.%20To%20compare%20Magpie%20data%20with%20other%20public%20instruction%0Adatasets%2C%20we%20fine-tune%20Llama-3-8B-Base%20with%20each%20dataset%20and%20evaluate%20the%0Aperformance%20of%20the%20fine-tuned%20models.%20Our%20results%20indicate%20that%20in%20some%20tasks%2C%0Amodels%20fine-tuned%20with%20Magpie%20perform%20comparably%20to%20the%20official%0ALlama-3-8B-Instruct%2C%20despite%20the%20latter%20being%20enhanced%20with%2010%20million%20data%0Apoints%20through%20supervised%20fine-tuning%20%28SFT%29%20and%20subsequent%20feedback%20learning.%0AWe%20also%20show%20that%20using%20Magpie%20solely%20for%20SFT%20can%20surpass%20the%20performance%20of%0Aprevious%20public%20datasets%20utilized%20for%20both%20SFT%20and%20preference%20optimization%2C%0Asuch%20as%20direct%20preference%20optimization%20with%20UltraFeedback.%20This%20advantage%20is%0Aevident%20on%20alignment%20benchmarks%20such%20as%20AlpacaEval%2C%20ArenaHard%2C%20and%20WildBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08464v1&entry.124074799=Read"},
{"title": "Make Your Actor Talk: Generalizable and High-Fidelity Lip Sync with\n  Motion and Appearance Disentanglement", "author": "Runyi Yu and Tianyu He and Ailing Zeng and Yuchi Wang and Junliang Guo and Xu Tan and Chang Liu and Jie Chen and Jiang Bian", "abstract": "  We aim to edit the lip movements in talking video according to the given\nspeech while preserving the personal identity and visual details. The task can\nbe decomposed into two sub-problems: (1) speech-driven lip motion generation\nand (2) visual appearance synthesis. Current solutions handle the two\nsub-problems within a single generative model, resulting in a challenging\ntrade-off between lip-sync quality and visual details preservation. Instead, we\npropose to disentangle the motion and appearance, and then generate them one by\none with a speech-to-motion diffusion model and a motion-conditioned appearance\ngeneration model. However, there still remain challenges in each stage, such as\nmotion-aware identity preservation in (1) and visual details preservation in\n(2). Therefore, to preserve personal identity, we adopt landmarks to represent\nthe motion, and further employ a landmark-based identity loss. To capture\nmotion-agnostic visual details, we use separate encoders to encode the lip,\nnon-lip appearance and motion, and then integrate them with a learned fusion\nmodule. We train MyTalk on a large-scale and diverse dataset. Experiments show\nthat our method generalizes well to the unknown, even out-of-domain person, in\nterms of both lip sync and visual detail preservation. We encourage the readers\nto watch the videos on our project page (https://Ingrid789.github.io/MyTalk/).\n", "link": "http://arxiv.org/abs/2406.08096v1", "date": "2024-06-12", "relevancy": 2.4001, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6715}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6174}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Make%20Your%20Actor%20Talk%3A%20Generalizable%20and%20High-Fidelity%20Lip%20Sync%20with%0A%20%20Motion%20and%20Appearance%20Disentanglement&body=Title%3A%20Make%20Your%20Actor%20Talk%3A%20Generalizable%20and%20High-Fidelity%20Lip%20Sync%20with%0A%20%20Motion%20and%20Appearance%20Disentanglement%0AAuthor%3A%20Runyi%20Yu%20and%20Tianyu%20He%20and%20Ailing%20Zeng%20and%20Yuchi%20Wang%20and%20Junliang%20Guo%20and%20Xu%20Tan%20and%20Chang%20Liu%20and%20Jie%20Chen%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20We%20aim%20to%20edit%20the%20lip%20movements%20in%20talking%20video%20according%20to%20the%20given%0Aspeech%20while%20preserving%20the%20personal%20identity%20and%20visual%20details.%20The%20task%20can%0Abe%20decomposed%20into%20two%20sub-problems%3A%20%281%29%20speech-driven%20lip%20motion%20generation%0Aand%20%282%29%20visual%20appearance%20synthesis.%20Current%20solutions%20handle%20the%20two%0Asub-problems%20within%20a%20single%20generative%20model%2C%20resulting%20in%20a%20challenging%0Atrade-off%20between%20lip-sync%20quality%20and%20visual%20details%20preservation.%20Instead%2C%20we%0Apropose%20to%20disentangle%20the%20motion%20and%20appearance%2C%20and%20then%20generate%20them%20one%20by%0Aone%20with%20a%20speech-to-motion%20diffusion%20model%20and%20a%20motion-conditioned%20appearance%0Ageneration%20model.%20However%2C%20there%20still%20remain%20challenges%20in%20each%20stage%2C%20such%20as%0Amotion-aware%20identity%20preservation%20in%20%281%29%20and%20visual%20details%20preservation%20in%0A%282%29.%20Therefore%2C%20to%20preserve%20personal%20identity%2C%20we%20adopt%20landmarks%20to%20represent%0Athe%20motion%2C%20and%20further%20employ%20a%20landmark-based%20identity%20loss.%20To%20capture%0Amotion-agnostic%20visual%20details%2C%20we%20use%20separate%20encoders%20to%20encode%20the%20lip%2C%0Anon-lip%20appearance%20and%20motion%2C%20and%20then%20integrate%20them%20with%20a%20learned%20fusion%0Amodule.%20We%20train%20MyTalk%20on%20a%20large-scale%20and%20diverse%20dataset.%20Experiments%20show%0Athat%20our%20method%20generalizes%20well%20to%20the%20unknown%2C%20even%20out-of-domain%20person%2C%20in%0Aterms%20of%20both%20lip%20sync%20and%20visual%20detail%20preservation.%20We%20encourage%20the%20readers%0Ato%20watch%20the%20videos%20on%20our%20project%20page%20%28https%3A//Ingrid789.github.io/MyTalk/%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08096v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMake%2520Your%2520Actor%2520Talk%253A%2520Generalizable%2520and%2520High-Fidelity%2520Lip%2520Sync%2520with%250A%2520%2520Motion%2520and%2520Appearance%2520Disentanglement%26entry.906535625%3DRunyi%2520Yu%2520and%2520Tianyu%2520He%2520and%2520Ailing%2520Zeng%2520and%2520Yuchi%2520Wang%2520and%2520Junliang%2520Guo%2520and%2520Xu%2520Tan%2520and%2520Chang%2520Liu%2520and%2520Jie%2520Chen%2520and%2520Jiang%2520Bian%26entry.1292438233%3D%2520%2520We%2520aim%2520to%2520edit%2520the%2520lip%2520movements%2520in%2520talking%2520video%2520according%2520to%2520the%2520given%250Aspeech%2520while%2520preserving%2520the%2520personal%2520identity%2520and%2520visual%2520details.%2520The%2520task%2520can%250Abe%2520decomposed%2520into%2520two%2520sub-problems%253A%2520%25281%2529%2520speech-driven%2520lip%2520motion%2520generation%250Aand%2520%25282%2529%2520visual%2520appearance%2520synthesis.%2520Current%2520solutions%2520handle%2520the%2520two%250Asub-problems%2520within%2520a%2520single%2520generative%2520model%252C%2520resulting%2520in%2520a%2520challenging%250Atrade-off%2520between%2520lip-sync%2520quality%2520and%2520visual%2520details%2520preservation.%2520Instead%252C%2520we%250Apropose%2520to%2520disentangle%2520the%2520motion%2520and%2520appearance%252C%2520and%2520then%2520generate%2520them%2520one%2520by%250Aone%2520with%2520a%2520speech-to-motion%2520diffusion%2520model%2520and%2520a%2520motion-conditioned%2520appearance%250Ageneration%2520model.%2520However%252C%2520there%2520still%2520remain%2520challenges%2520in%2520each%2520stage%252C%2520such%2520as%250Amotion-aware%2520identity%2520preservation%2520in%2520%25281%2529%2520and%2520visual%2520details%2520preservation%2520in%250A%25282%2529.%2520Therefore%252C%2520to%2520preserve%2520personal%2520identity%252C%2520we%2520adopt%2520landmarks%2520to%2520represent%250Athe%2520motion%252C%2520and%2520further%2520employ%2520a%2520landmark-based%2520identity%2520loss.%2520To%2520capture%250Amotion-agnostic%2520visual%2520details%252C%2520we%2520use%2520separate%2520encoders%2520to%2520encode%2520the%2520lip%252C%250Anon-lip%2520appearance%2520and%2520motion%252C%2520and%2520then%2520integrate%2520them%2520with%2520a%2520learned%2520fusion%250Amodule.%2520We%2520train%2520MyTalk%2520on%2520a%2520large-scale%2520and%2520diverse%2520dataset.%2520Experiments%2520show%250Athat%2520our%2520method%2520generalizes%2520well%2520to%2520the%2520unknown%252C%2520even%2520out-of-domain%2520person%252C%2520in%250Aterms%2520of%2520both%2520lip%2520sync%2520and%2520visual%2520detail%2520preservation.%2520We%2520encourage%2520the%2520readers%250Ato%2520watch%2520the%2520videos%2520on%2520our%2520project%2520page%2520%2528https%253A//Ingrid789.github.io/MyTalk/%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08096v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Make%20Your%20Actor%20Talk%3A%20Generalizable%20and%20High-Fidelity%20Lip%20Sync%20with%0A%20%20Motion%20and%20Appearance%20Disentanglement&entry.906535625=Runyi%20Yu%20and%20Tianyu%20He%20and%20Ailing%20Zeng%20and%20Yuchi%20Wang%20and%20Junliang%20Guo%20and%20Xu%20Tan%20and%20Chang%20Liu%20and%20Jie%20Chen%20and%20Jiang%20Bian&entry.1292438233=%20%20We%20aim%20to%20edit%20the%20lip%20movements%20in%20talking%20video%20according%20to%20the%20given%0Aspeech%20while%20preserving%20the%20personal%20identity%20and%20visual%20details.%20The%20task%20can%0Abe%20decomposed%20into%20two%20sub-problems%3A%20%281%29%20speech-driven%20lip%20motion%20generation%0Aand%20%282%29%20visual%20appearance%20synthesis.%20Current%20solutions%20handle%20the%20two%0Asub-problems%20within%20a%20single%20generative%20model%2C%20resulting%20in%20a%20challenging%0Atrade-off%20between%20lip-sync%20quality%20and%20visual%20details%20preservation.%20Instead%2C%20we%0Apropose%20to%20disentangle%20the%20motion%20and%20appearance%2C%20and%20then%20generate%20them%20one%20by%0Aone%20with%20a%20speech-to-motion%20diffusion%20model%20and%20a%20motion-conditioned%20appearance%0Ageneration%20model.%20However%2C%20there%20still%20remain%20challenges%20in%20each%20stage%2C%20such%20as%0Amotion-aware%20identity%20preservation%20in%20%281%29%20and%20visual%20details%20preservation%20in%0A%282%29.%20Therefore%2C%20to%20preserve%20personal%20identity%2C%20we%20adopt%20landmarks%20to%20represent%0Athe%20motion%2C%20and%20further%20employ%20a%20landmark-based%20identity%20loss.%20To%20capture%0Amotion-agnostic%20visual%20details%2C%20we%20use%20separate%20encoders%20to%20encode%20the%20lip%2C%0Anon-lip%20appearance%20and%20motion%2C%20and%20then%20integrate%20them%20with%20a%20learned%20fusion%0Amodule.%20We%20train%20MyTalk%20on%20a%20large-scale%20and%20diverse%20dataset.%20Experiments%20show%0Athat%20our%20method%20generalizes%20well%20to%20the%20unknown%2C%20even%20out-of-domain%20person%2C%20in%0Aterms%20of%20both%20lip%20sync%20and%20visual%20detail%20preservation.%20We%20encourage%20the%20readers%0Ato%20watch%20the%20videos%20on%20our%20project%20page%20%28https%3A//Ingrid789.github.io/MyTalk/%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08096v1&entry.124074799=Read"},
{"title": "RILe: Reinforced Imitation Learning", "author": "Mert Albaba and Sammy Christen and Christoph Gebhardt and Thomas Langarek and Michael J. Black and Otmar Hilliges", "abstract": "  Reinforcement Learning has achieved significant success in generating complex\nbehavior but often requires extensive reward function engineering. Adversarial\nvariants of Imitation Learning and Inverse Reinforcement Learning offer an\nalternative by learning policies from expert demonstrations via a\ndiscriminator. Employing discriminators increases their data- and computational\nefficiency over the standard approaches; however, results in sensitivity to\nimperfections in expert data. We propose RILe, a teacher-student system that\nachieves both robustness to imperfect data and efficiency. In RILe, the student\nlearns an action policy while the teacher dynamically adjusts a reward function\nbased on the student's performance and its alignment with expert\ndemonstrations. By tailoring the reward function to both performance of the\nstudent and expert similarity, our system reduces dependence on the\ndiscriminator and, hence, increases robustness against data imperfections.\nExperiments show that RILe outperforms existing methods by 2x in settings with\nlimited or noisy expert data.\n", "link": "http://arxiv.org/abs/2406.08472v1", "date": "2024-06-12", "relevancy": 2.3997, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4845}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4784}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RILe%3A%20Reinforced%20Imitation%20Learning&body=Title%3A%20RILe%3A%20Reinforced%20Imitation%20Learning%0AAuthor%3A%20Mert%20Albaba%20and%20Sammy%20Christen%20and%20Christoph%20Gebhardt%20and%20Thomas%20Langarek%20and%20Michael%20J.%20Black%20and%20Otmar%20Hilliges%0AAbstract%3A%20%20%20Reinforcement%20Learning%20has%20achieved%20significant%20success%20in%20generating%20complex%0Abehavior%20but%20often%20requires%20extensive%20reward%20function%20engineering.%20Adversarial%0Avariants%20of%20Imitation%20Learning%20and%20Inverse%20Reinforcement%20Learning%20offer%20an%0Aalternative%20by%20learning%20policies%20from%20expert%20demonstrations%20via%20a%0Adiscriminator.%20Employing%20discriminators%20increases%20their%20data-%20and%20computational%0Aefficiency%20over%20the%20standard%20approaches%3B%20however%2C%20results%20in%20sensitivity%20to%0Aimperfections%20in%20expert%20data.%20We%20propose%20RILe%2C%20a%20teacher-student%20system%20that%0Aachieves%20both%20robustness%20to%20imperfect%20data%20and%20efficiency.%20In%20RILe%2C%20the%20student%0Alearns%20an%20action%20policy%20while%20the%20teacher%20dynamically%20adjusts%20a%20reward%20function%0Abased%20on%20the%20student%27s%20performance%20and%20its%20alignment%20with%20expert%0Ademonstrations.%20By%20tailoring%20the%20reward%20function%20to%20both%20performance%20of%20the%0Astudent%20and%20expert%20similarity%2C%20our%20system%20reduces%20dependence%20on%20the%0Adiscriminator%20and%2C%20hence%2C%20increases%20robustness%20against%20data%20imperfections.%0AExperiments%20show%20that%20RILe%20outperforms%20existing%20methods%20by%202x%20in%20settings%20with%0Alimited%20or%20noisy%20expert%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRILe%253A%2520Reinforced%2520Imitation%2520Learning%26entry.906535625%3DMert%2520Albaba%2520and%2520Sammy%2520Christen%2520and%2520Christoph%2520Gebhardt%2520and%2520Thomas%2520Langarek%2520and%2520Michael%2520J.%2520Black%2520and%2520Otmar%2520Hilliges%26entry.1292438233%3D%2520%2520Reinforcement%2520Learning%2520has%2520achieved%2520significant%2520success%2520in%2520generating%2520complex%250Abehavior%2520but%2520often%2520requires%2520extensive%2520reward%2520function%2520engineering.%2520Adversarial%250Avariants%2520of%2520Imitation%2520Learning%2520and%2520Inverse%2520Reinforcement%2520Learning%2520offer%2520an%250Aalternative%2520by%2520learning%2520policies%2520from%2520expert%2520demonstrations%2520via%2520a%250Adiscriminator.%2520Employing%2520discriminators%2520increases%2520their%2520data-%2520and%2520computational%250Aefficiency%2520over%2520the%2520standard%2520approaches%253B%2520however%252C%2520results%2520in%2520sensitivity%2520to%250Aimperfections%2520in%2520expert%2520data.%2520We%2520propose%2520RILe%252C%2520a%2520teacher-student%2520system%2520that%250Aachieves%2520both%2520robustness%2520to%2520imperfect%2520data%2520and%2520efficiency.%2520In%2520RILe%252C%2520the%2520student%250Alearns%2520an%2520action%2520policy%2520while%2520the%2520teacher%2520dynamically%2520adjusts%2520a%2520reward%2520function%250Abased%2520on%2520the%2520student%2527s%2520performance%2520and%2520its%2520alignment%2520with%2520expert%250Ademonstrations.%2520By%2520tailoring%2520the%2520reward%2520function%2520to%2520both%2520performance%2520of%2520the%250Astudent%2520and%2520expert%2520similarity%252C%2520our%2520system%2520reduces%2520dependence%2520on%2520the%250Adiscriminator%2520and%252C%2520hence%252C%2520increases%2520robustness%2520against%2520data%2520imperfections.%250AExperiments%2520show%2520that%2520RILe%2520outperforms%2520existing%2520methods%2520by%25202x%2520in%2520settings%2520with%250Alimited%2520or%2520noisy%2520expert%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RILe%3A%20Reinforced%20Imitation%20Learning&entry.906535625=Mert%20Albaba%20and%20Sammy%20Christen%20and%20Christoph%20Gebhardt%20and%20Thomas%20Langarek%20and%20Michael%20J.%20Black%20and%20Otmar%20Hilliges&entry.1292438233=%20%20Reinforcement%20Learning%20has%20achieved%20significant%20success%20in%20generating%20complex%0Abehavior%20but%20often%20requires%20extensive%20reward%20function%20engineering.%20Adversarial%0Avariants%20of%20Imitation%20Learning%20and%20Inverse%20Reinforcement%20Learning%20offer%20an%0Aalternative%20by%20learning%20policies%20from%20expert%20demonstrations%20via%20a%0Adiscriminator.%20Employing%20discriminators%20increases%20their%20data-%20and%20computational%0Aefficiency%20over%20the%20standard%20approaches%3B%20however%2C%20results%20in%20sensitivity%20to%0Aimperfections%20in%20expert%20data.%20We%20propose%20RILe%2C%20a%20teacher-student%20system%20that%0Aachieves%20both%20robustness%20to%20imperfect%20data%20and%20efficiency.%20In%20RILe%2C%20the%20student%0Alearns%20an%20action%20policy%20while%20the%20teacher%20dynamically%20adjusts%20a%20reward%20function%0Abased%20on%20the%20student%27s%20performance%20and%20its%20alignment%20with%20expert%0Ademonstrations.%20By%20tailoring%20the%20reward%20function%20to%20both%20performance%20of%20the%0Astudent%20and%20expert%20similarity%2C%20our%20system%20reduces%20dependence%20on%20the%0Adiscriminator%20and%2C%20hence%2C%20increases%20robustness%20against%20data%20imperfections.%0AExperiments%20show%20that%20RILe%20outperforms%20existing%20methods%20by%202x%20in%20settings%20with%0Alimited%20or%20noisy%20expert%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08472v1&entry.124074799=Read"},
{"title": "Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models", "author": "Yi-Fan Zhang and Qingsong Wen and Chaoyou Fu and Xue Wang and Zhang Zhang and Liang Wang and Rong Jin", "abstract": "  Seeing clearly with high resolution is a foundation of Large Multimodal\nModels (LMMs), which has been proven to be vital for visual perception and\nreasoning. Existing works usually employ a straightforward resolution upscaling\nmethod, where the image consists of global and local branches, with the latter\nbeing the sliced image patches but resized to the same resolution as the\nformer. This means that higher resolution requires more local patches,\nresulting in exorbitant computational expenses, and meanwhile, the dominance of\nlocal image tokens may diminish the global context. In this paper, we dive into\nthe problems and propose a new framework as well as an elaborate optimization\nstrategy. Specifically, we extract contextual information from the global view\nusing a mixture of adapters, based on the observation that different adapters\nexcel at different tasks. With regard to local patches, learnable query\nembeddings are introduced to reduce image tokens, the most important tokens\naccounting for the user question will be further selected by a similarity-based\nselector. Our empirical results demonstrate a `less is more' pattern, where\n\\textit{utilizing fewer but more informative local image tokens leads to\nimproved performance}. Besides, a significant challenge lies in the training\nstrategy, as simultaneous end-to-end training of the global mining block and\nlocal compression block does not yield optimal results. We thus advocate for an\nalternating training way, ensuring balanced learning between global and local\naspects. Finally, we also introduce a challenging dataset with high\nrequirements for image detail, enhancing the training of the local compression\nlayer. The proposed method, termed LMM with Sophisticated Tasks, Local image\ncompression, and Mixture of global Experts (SliME), achieves leading\nperformance across various benchmarks with only 2 million training data.\n", "link": "http://arxiv.org/abs/2406.08487v1", "date": "2024-06-12", "relevancy": 2.3922, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6089}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6064}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20LLaVA-HD%3A%20Diving%20into%20High-Resolution%20Large%20Multimodal%20Models&body=Title%3A%20Beyond%20LLaVA-HD%3A%20Diving%20into%20High-Resolution%20Large%20Multimodal%20Models%0AAuthor%3A%20Yi-Fan%20Zhang%20and%20Qingsong%20Wen%20and%20Chaoyou%20Fu%20and%20Xue%20Wang%20and%20Zhang%20Zhang%20and%20Liang%20Wang%20and%20Rong%20Jin%0AAbstract%3A%20%20%20Seeing%20clearly%20with%20high%20resolution%20is%20a%20foundation%20of%20Large%20Multimodal%0AModels%20%28LMMs%29%2C%20which%20has%20been%20proven%20to%20be%20vital%20for%20visual%20perception%20and%0Areasoning.%20Existing%20works%20usually%20employ%20a%20straightforward%20resolution%20upscaling%0Amethod%2C%20where%20the%20image%20consists%20of%20global%20and%20local%20branches%2C%20with%20the%20latter%0Abeing%20the%20sliced%20image%20patches%20but%20resized%20to%20the%20same%20resolution%20as%20the%0Aformer.%20This%20means%20that%20higher%20resolution%20requires%20more%20local%20patches%2C%0Aresulting%20in%20exorbitant%20computational%20expenses%2C%20and%20meanwhile%2C%20the%20dominance%20of%0Alocal%20image%20tokens%20may%20diminish%20the%20global%20context.%20In%20this%20paper%2C%20we%20dive%20into%0Athe%20problems%20and%20propose%20a%20new%20framework%20as%20well%20as%20an%20elaborate%20optimization%0Astrategy.%20Specifically%2C%20we%20extract%20contextual%20information%20from%20the%20global%20view%0Ausing%20a%20mixture%20of%20adapters%2C%20based%20on%20the%20observation%20that%20different%20adapters%0Aexcel%20at%20different%20tasks.%20With%20regard%20to%20local%20patches%2C%20learnable%20query%0Aembeddings%20are%20introduced%20to%20reduce%20image%20tokens%2C%20the%20most%20important%20tokens%0Aaccounting%20for%20the%20user%20question%20will%20be%20further%20selected%20by%20a%20similarity-based%0Aselector.%20Our%20empirical%20results%20demonstrate%20a%20%60less%20is%20more%27%20pattern%2C%20where%0A%5Ctextit%7Butilizing%20fewer%20but%20more%20informative%20local%20image%20tokens%20leads%20to%0Aimproved%20performance%7D.%20Besides%2C%20a%20significant%20challenge%20lies%20in%20the%20training%0Astrategy%2C%20as%20simultaneous%20end-to-end%20training%20of%20the%20global%20mining%20block%20and%0Alocal%20compression%20block%20does%20not%20yield%20optimal%20results.%20We%20thus%20advocate%20for%20an%0Aalternating%20training%20way%2C%20ensuring%20balanced%20learning%20between%20global%20and%20local%0Aaspects.%20Finally%2C%20we%20also%20introduce%20a%20challenging%20dataset%20with%20high%0Arequirements%20for%20image%20detail%2C%20enhancing%20the%20training%20of%20the%20local%20compression%0Alayer.%20The%20proposed%20method%2C%20termed%20LMM%20with%20Sophisticated%20Tasks%2C%20Local%20image%0Acompression%2C%20and%20Mixture%20of%20global%20Experts%20%28SliME%29%2C%20achieves%20leading%0Aperformance%20across%20various%20benchmarks%20with%20only%202%20million%20training%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520LLaVA-HD%253A%2520Diving%2520into%2520High-Resolution%2520Large%2520Multimodal%2520Models%26entry.906535625%3DYi-Fan%2520Zhang%2520and%2520Qingsong%2520Wen%2520and%2520Chaoyou%2520Fu%2520and%2520Xue%2520Wang%2520and%2520Zhang%2520Zhang%2520and%2520Liang%2520Wang%2520and%2520Rong%2520Jin%26entry.1292438233%3D%2520%2520Seeing%2520clearly%2520with%2520high%2520resolution%2520is%2520a%2520foundation%2520of%2520Large%2520Multimodal%250AModels%2520%2528LMMs%2529%252C%2520which%2520has%2520been%2520proven%2520to%2520be%2520vital%2520for%2520visual%2520perception%2520and%250Areasoning.%2520Existing%2520works%2520usually%2520employ%2520a%2520straightforward%2520resolution%2520upscaling%250Amethod%252C%2520where%2520the%2520image%2520consists%2520of%2520global%2520and%2520local%2520branches%252C%2520with%2520the%2520latter%250Abeing%2520the%2520sliced%2520image%2520patches%2520but%2520resized%2520to%2520the%2520same%2520resolution%2520as%2520the%250Aformer.%2520This%2520means%2520that%2520higher%2520resolution%2520requires%2520more%2520local%2520patches%252C%250Aresulting%2520in%2520exorbitant%2520computational%2520expenses%252C%2520and%2520meanwhile%252C%2520the%2520dominance%2520of%250Alocal%2520image%2520tokens%2520may%2520diminish%2520the%2520global%2520context.%2520In%2520this%2520paper%252C%2520we%2520dive%2520into%250Athe%2520problems%2520and%2520propose%2520a%2520new%2520framework%2520as%2520well%2520as%2520an%2520elaborate%2520optimization%250Astrategy.%2520Specifically%252C%2520we%2520extract%2520contextual%2520information%2520from%2520the%2520global%2520view%250Ausing%2520a%2520mixture%2520of%2520adapters%252C%2520based%2520on%2520the%2520observation%2520that%2520different%2520adapters%250Aexcel%2520at%2520different%2520tasks.%2520With%2520regard%2520to%2520local%2520patches%252C%2520learnable%2520query%250Aembeddings%2520are%2520introduced%2520to%2520reduce%2520image%2520tokens%252C%2520the%2520most%2520important%2520tokens%250Aaccounting%2520for%2520the%2520user%2520question%2520will%2520be%2520further%2520selected%2520by%2520a%2520similarity-based%250Aselector.%2520Our%2520empirical%2520results%2520demonstrate%2520a%2520%2560less%2520is%2520more%2527%2520pattern%252C%2520where%250A%255Ctextit%257Butilizing%2520fewer%2520but%2520more%2520informative%2520local%2520image%2520tokens%2520leads%2520to%250Aimproved%2520performance%257D.%2520Besides%252C%2520a%2520significant%2520challenge%2520lies%2520in%2520the%2520training%250Astrategy%252C%2520as%2520simultaneous%2520end-to-end%2520training%2520of%2520the%2520global%2520mining%2520block%2520and%250Alocal%2520compression%2520block%2520does%2520not%2520yield%2520optimal%2520results.%2520We%2520thus%2520advocate%2520for%2520an%250Aalternating%2520training%2520way%252C%2520ensuring%2520balanced%2520learning%2520between%2520global%2520and%2520local%250Aaspects.%2520Finally%252C%2520we%2520also%2520introduce%2520a%2520challenging%2520dataset%2520with%2520high%250Arequirements%2520for%2520image%2520detail%252C%2520enhancing%2520the%2520training%2520of%2520the%2520local%2520compression%250Alayer.%2520The%2520proposed%2520method%252C%2520termed%2520LMM%2520with%2520Sophisticated%2520Tasks%252C%2520Local%2520image%250Acompression%252C%2520and%2520Mixture%2520of%2520global%2520Experts%2520%2528SliME%2529%252C%2520achieves%2520leading%250Aperformance%2520across%2520various%2520benchmarks%2520with%2520only%25202%2520million%2520training%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20LLaVA-HD%3A%20Diving%20into%20High-Resolution%20Large%20Multimodal%20Models&entry.906535625=Yi-Fan%20Zhang%20and%20Qingsong%20Wen%20and%20Chaoyou%20Fu%20and%20Xue%20Wang%20and%20Zhang%20Zhang%20and%20Liang%20Wang%20and%20Rong%20Jin&entry.1292438233=%20%20Seeing%20clearly%20with%20high%20resolution%20is%20a%20foundation%20of%20Large%20Multimodal%0AModels%20%28LMMs%29%2C%20which%20has%20been%20proven%20to%20be%20vital%20for%20visual%20perception%20and%0Areasoning.%20Existing%20works%20usually%20employ%20a%20straightforward%20resolution%20upscaling%0Amethod%2C%20where%20the%20image%20consists%20of%20global%20and%20local%20branches%2C%20with%20the%20latter%0Abeing%20the%20sliced%20image%20patches%20but%20resized%20to%20the%20same%20resolution%20as%20the%0Aformer.%20This%20means%20that%20higher%20resolution%20requires%20more%20local%20patches%2C%0Aresulting%20in%20exorbitant%20computational%20expenses%2C%20and%20meanwhile%2C%20the%20dominance%20of%0Alocal%20image%20tokens%20may%20diminish%20the%20global%20context.%20In%20this%20paper%2C%20we%20dive%20into%0Athe%20problems%20and%20propose%20a%20new%20framework%20as%20well%20as%20an%20elaborate%20optimization%0Astrategy.%20Specifically%2C%20we%20extract%20contextual%20information%20from%20the%20global%20view%0Ausing%20a%20mixture%20of%20adapters%2C%20based%20on%20the%20observation%20that%20different%20adapters%0Aexcel%20at%20different%20tasks.%20With%20regard%20to%20local%20patches%2C%20learnable%20query%0Aembeddings%20are%20introduced%20to%20reduce%20image%20tokens%2C%20the%20most%20important%20tokens%0Aaccounting%20for%20the%20user%20question%20will%20be%20further%20selected%20by%20a%20similarity-based%0Aselector.%20Our%20empirical%20results%20demonstrate%20a%20%60less%20is%20more%27%20pattern%2C%20where%0A%5Ctextit%7Butilizing%20fewer%20but%20more%20informative%20local%20image%20tokens%20leads%20to%0Aimproved%20performance%7D.%20Besides%2C%20a%20significant%20challenge%20lies%20in%20the%20training%0Astrategy%2C%20as%20simultaneous%20end-to-end%20training%20of%20the%20global%20mining%20block%20and%0Alocal%20compression%20block%20does%20not%20yield%20optimal%20results.%20We%20thus%20advocate%20for%20an%0Aalternating%20training%20way%2C%20ensuring%20balanced%20learning%20between%20global%20and%20local%0Aaspects.%20Finally%2C%20we%20also%20introduce%20a%20challenging%20dataset%20with%20high%0Arequirements%20for%20image%20detail%2C%20enhancing%20the%20training%20of%20the%20local%20compression%0Alayer.%20The%20proposed%20method%2C%20termed%20LMM%20with%20Sophisticated%20Tasks%2C%20Local%20image%0Acompression%2C%20and%20Mixture%20of%20global%20Experts%20%28SliME%29%2C%20achieves%20leading%0Aperformance%20across%20various%20benchmarks%20with%20only%202%20million%20training%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08487v1&entry.124074799=Read"},
{"title": "Tailoring Generative AI Chatbots for Multiethnic Communities in Disaster\n  Preparedness Communication: Extending the CASA Paradigm", "author": "Xinyan Zhao and Yuan Sun and Wenlin Liu and Chau-Wai Wong", "abstract": "  This study is among the first to develop different prototypes of generative\nAI (GenAI) chatbots powered by GPT 4 to communicate hurricane preparedness\ninformation to diverse residents. Drawing from the Computers Are Social Actors\n(CASA) paradigm and the literature on disaster vulnerability and cultural\ntailoring, this study conducted a between-subjects experiment with 441 Black,\nHispanic, and Caucasian residents of Florida. A computational analysis of chat\nlogs (N = 7,848) shows that anthropomorphism and personalization are key\ncommunication topics in GenAI chatbot-user interactions. SEM results (N = 441)\nsuggest that GenAI chatbots varying in tone formality and cultural tailoring\nsignificantly predict bot perceptions and, subsequently, hurricane preparedness\noutcomes. These results highlight the potential of using GenAI chatbots to\nimprove diverse communities' disaster preparedness.\n", "link": "http://arxiv.org/abs/2406.08411v1", "date": "2024-06-12", "relevancy": 2.3825, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5049}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4788}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tailoring%20Generative%20AI%20Chatbots%20for%20Multiethnic%20Communities%20in%20Disaster%0A%20%20Preparedness%20Communication%3A%20Extending%20the%20CASA%20Paradigm&body=Title%3A%20Tailoring%20Generative%20AI%20Chatbots%20for%20Multiethnic%20Communities%20in%20Disaster%0A%20%20Preparedness%20Communication%3A%20Extending%20the%20CASA%20Paradigm%0AAuthor%3A%20Xinyan%20Zhao%20and%20Yuan%20Sun%20and%20Wenlin%20Liu%20and%20Chau-Wai%20Wong%0AAbstract%3A%20%20%20This%20study%20is%20among%20the%20first%20to%20develop%20different%20prototypes%20of%20generative%0AAI%20%28GenAI%29%20chatbots%20powered%20by%20GPT%204%20to%20communicate%20hurricane%20preparedness%0Ainformation%20to%20diverse%20residents.%20Drawing%20from%20the%20Computers%20Are%20Social%20Actors%0A%28CASA%29%20paradigm%20and%20the%20literature%20on%20disaster%20vulnerability%20and%20cultural%0Atailoring%2C%20this%20study%20conducted%20a%20between-subjects%20experiment%20with%20441%20Black%2C%0AHispanic%2C%20and%20Caucasian%20residents%20of%20Florida.%20A%20computational%20analysis%20of%20chat%0Alogs%20%28N%20%3D%207%2C848%29%20shows%20that%20anthropomorphism%20and%20personalization%20are%20key%0Acommunication%20topics%20in%20GenAI%20chatbot-user%20interactions.%20SEM%20results%20%28N%20%3D%20441%29%0Asuggest%20that%20GenAI%20chatbots%20varying%20in%20tone%20formality%20and%20cultural%20tailoring%0Asignificantly%20predict%20bot%20perceptions%20and%2C%20subsequently%2C%20hurricane%20preparedness%0Aoutcomes.%20These%20results%20highlight%20the%20potential%20of%20using%20GenAI%20chatbots%20to%0Aimprove%20diverse%20communities%27%20disaster%20preparedness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08411v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTailoring%2520Generative%2520AI%2520Chatbots%2520for%2520Multiethnic%2520Communities%2520in%2520Disaster%250A%2520%2520Preparedness%2520Communication%253A%2520Extending%2520the%2520CASA%2520Paradigm%26entry.906535625%3DXinyan%2520Zhao%2520and%2520Yuan%2520Sun%2520and%2520Wenlin%2520Liu%2520and%2520Chau-Wai%2520Wong%26entry.1292438233%3D%2520%2520This%2520study%2520is%2520among%2520the%2520first%2520to%2520develop%2520different%2520prototypes%2520of%2520generative%250AAI%2520%2528GenAI%2529%2520chatbots%2520powered%2520by%2520GPT%25204%2520to%2520communicate%2520hurricane%2520preparedness%250Ainformation%2520to%2520diverse%2520residents.%2520Drawing%2520from%2520the%2520Computers%2520Are%2520Social%2520Actors%250A%2528CASA%2529%2520paradigm%2520and%2520the%2520literature%2520on%2520disaster%2520vulnerability%2520and%2520cultural%250Atailoring%252C%2520this%2520study%2520conducted%2520a%2520between-subjects%2520experiment%2520with%2520441%2520Black%252C%250AHispanic%252C%2520and%2520Caucasian%2520residents%2520of%2520Florida.%2520A%2520computational%2520analysis%2520of%2520chat%250Alogs%2520%2528N%2520%253D%25207%252C848%2529%2520shows%2520that%2520anthropomorphism%2520and%2520personalization%2520are%2520key%250Acommunication%2520topics%2520in%2520GenAI%2520chatbot-user%2520interactions.%2520SEM%2520results%2520%2528N%2520%253D%2520441%2529%250Asuggest%2520that%2520GenAI%2520chatbots%2520varying%2520in%2520tone%2520formality%2520and%2520cultural%2520tailoring%250Asignificantly%2520predict%2520bot%2520perceptions%2520and%252C%2520subsequently%252C%2520hurricane%2520preparedness%250Aoutcomes.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520using%2520GenAI%2520chatbots%2520to%250Aimprove%2520diverse%2520communities%2527%2520disaster%2520preparedness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08411v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tailoring%20Generative%20AI%20Chatbots%20for%20Multiethnic%20Communities%20in%20Disaster%0A%20%20Preparedness%20Communication%3A%20Extending%20the%20CASA%20Paradigm&entry.906535625=Xinyan%20Zhao%20and%20Yuan%20Sun%20and%20Wenlin%20Liu%20and%20Chau-Wai%20Wong&entry.1292438233=%20%20This%20study%20is%20among%20the%20first%20to%20develop%20different%20prototypes%20of%20generative%0AAI%20%28GenAI%29%20chatbots%20powered%20by%20GPT%204%20to%20communicate%20hurricane%20preparedness%0Ainformation%20to%20diverse%20residents.%20Drawing%20from%20the%20Computers%20Are%20Social%20Actors%0A%28CASA%29%20paradigm%20and%20the%20literature%20on%20disaster%20vulnerability%20and%20cultural%0Atailoring%2C%20this%20study%20conducted%20a%20between-subjects%20experiment%20with%20441%20Black%2C%0AHispanic%2C%20and%20Caucasian%20residents%20of%20Florida.%20A%20computational%20analysis%20of%20chat%0Alogs%20%28N%20%3D%207%2C848%29%20shows%20that%20anthropomorphism%20and%20personalization%20are%20key%0Acommunication%20topics%20in%20GenAI%20chatbot-user%20interactions.%20SEM%20results%20%28N%20%3D%20441%29%0Asuggest%20that%20GenAI%20chatbots%20varying%20in%20tone%20formality%20and%20cultural%20tailoring%0Asignificantly%20predict%20bot%20perceptions%20and%2C%20subsequently%2C%20hurricane%20preparedness%0Aoutcomes.%20These%20results%20highlight%20the%20potential%20of%20using%20GenAI%20chatbots%20to%0Aimprove%20diverse%20communities%27%20disaster%20preparedness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08411v1&entry.124074799=Read"},
{"title": "AWGUNET: Attention-Aided Wavelet Guided U-Net for Nuclei Segmentation in\n  Histopathology Images", "author": "Ayush Roy and Payel Pramanik and Dmitrii Kaplun and Sergei Antonov and Ram Sarkar", "abstract": "  Accurate nuclei segmentation in histopathological images is crucial for\ncancer diagnosis. Automating this process offers valuable support to clinical\nexperts, as manual annotation is time-consuming and prone to human errors.\nHowever, automating nuclei segmentation presents challenges due to uncertain\ncell boundaries, intricate staining, and diverse structures. In this paper, we\npresent a segmentation approach that combines the U-Net architecture with a\nDenseNet-121 backbone, harnessing the strengths of both to capture\ncomprehensive contextual and spatial information. Our model introduces the\nWavelet-guided channel attention module to enhance cell boundary delineation,\nalong with a learnable weighted global attention module for channel-specific\nattention. The decoder module, composed of an upsample block and convolution\nblock, further refines segmentation in handling staining patterns. The\nexperimental results conducted on two publicly accessible histopathology\ndatasets, namely Monuseg and TNBC, underscore the superiority of our proposed\nmodel, demonstrating its potential to advance histopathological image analysis\nand cancer diagnosis. The code is made available at:\nhttps://github.com/AyushRoy2001/AWGUNET.\n", "link": "http://arxiv.org/abs/2406.08425v1", "date": "2024-06-12", "relevancy": 2.3767, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4958}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4673}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AWGUNET%3A%20Attention-Aided%20Wavelet%20Guided%20U-Net%20for%20Nuclei%20Segmentation%20in%0A%20%20Histopathology%20Images&body=Title%3A%20AWGUNET%3A%20Attention-Aided%20Wavelet%20Guided%20U-Net%20for%20Nuclei%20Segmentation%20in%0A%20%20Histopathology%20Images%0AAuthor%3A%20Ayush%20Roy%20and%20Payel%20Pramanik%20and%20Dmitrii%20Kaplun%20and%20Sergei%20Antonov%20and%20Ram%20Sarkar%0AAbstract%3A%20%20%20Accurate%20nuclei%20segmentation%20in%20histopathological%20images%20is%20crucial%20for%0Acancer%20diagnosis.%20Automating%20this%20process%20offers%20valuable%20support%20to%20clinical%0Aexperts%2C%20as%20manual%20annotation%20is%20time-consuming%20and%20prone%20to%20human%20errors.%0AHowever%2C%20automating%20nuclei%20segmentation%20presents%20challenges%20due%20to%20uncertain%0Acell%20boundaries%2C%20intricate%20staining%2C%20and%20diverse%20structures.%20In%20this%20paper%2C%20we%0Apresent%20a%20segmentation%20approach%20that%20combines%20the%20U-Net%20architecture%20with%20a%0ADenseNet-121%20backbone%2C%20harnessing%20the%20strengths%20of%20both%20to%20capture%0Acomprehensive%20contextual%20and%20spatial%20information.%20Our%20model%20introduces%20the%0AWavelet-guided%20channel%20attention%20module%20to%20enhance%20cell%20boundary%20delineation%2C%0Aalong%20with%20a%20learnable%20weighted%20global%20attention%20module%20for%20channel-specific%0Aattention.%20The%20decoder%20module%2C%20composed%20of%20an%20upsample%20block%20and%20convolution%0Ablock%2C%20further%20refines%20segmentation%20in%20handling%20staining%20patterns.%20The%0Aexperimental%20results%20conducted%20on%20two%20publicly%20accessible%20histopathology%0Adatasets%2C%20namely%20Monuseg%20and%20TNBC%2C%20underscore%20the%20superiority%20of%20our%20proposed%0Amodel%2C%20demonstrating%20its%20potential%20to%20advance%20histopathological%20image%20analysis%0Aand%20cancer%20diagnosis.%20The%20code%20is%20made%20available%20at%3A%0Ahttps%3A//github.com/AyushRoy2001/AWGUNET.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAWGUNET%253A%2520Attention-Aided%2520Wavelet%2520Guided%2520U-Net%2520for%2520Nuclei%2520Segmentation%2520in%250A%2520%2520Histopathology%2520Images%26entry.906535625%3DAyush%2520Roy%2520and%2520Payel%2520Pramanik%2520and%2520Dmitrii%2520Kaplun%2520and%2520Sergei%2520Antonov%2520and%2520Ram%2520Sarkar%26entry.1292438233%3D%2520%2520Accurate%2520nuclei%2520segmentation%2520in%2520histopathological%2520images%2520is%2520crucial%2520for%250Acancer%2520diagnosis.%2520Automating%2520this%2520process%2520offers%2520valuable%2520support%2520to%2520clinical%250Aexperts%252C%2520as%2520manual%2520annotation%2520is%2520time-consuming%2520and%2520prone%2520to%2520human%2520errors.%250AHowever%252C%2520automating%2520nuclei%2520segmentation%2520presents%2520challenges%2520due%2520to%2520uncertain%250Acell%2520boundaries%252C%2520intricate%2520staining%252C%2520and%2520diverse%2520structures.%2520In%2520this%2520paper%252C%2520we%250Apresent%2520a%2520segmentation%2520approach%2520that%2520combines%2520the%2520U-Net%2520architecture%2520with%2520a%250ADenseNet-121%2520backbone%252C%2520harnessing%2520the%2520strengths%2520of%2520both%2520to%2520capture%250Acomprehensive%2520contextual%2520and%2520spatial%2520information.%2520Our%2520model%2520introduces%2520the%250AWavelet-guided%2520channel%2520attention%2520module%2520to%2520enhance%2520cell%2520boundary%2520delineation%252C%250Aalong%2520with%2520a%2520learnable%2520weighted%2520global%2520attention%2520module%2520for%2520channel-specific%250Aattention.%2520The%2520decoder%2520module%252C%2520composed%2520of%2520an%2520upsample%2520block%2520and%2520convolution%250Ablock%252C%2520further%2520refines%2520segmentation%2520in%2520handling%2520staining%2520patterns.%2520The%250Aexperimental%2520results%2520conducted%2520on%2520two%2520publicly%2520accessible%2520histopathology%250Adatasets%252C%2520namely%2520Monuseg%2520and%2520TNBC%252C%2520underscore%2520the%2520superiority%2520of%2520our%2520proposed%250Amodel%252C%2520demonstrating%2520its%2520potential%2520to%2520advance%2520histopathological%2520image%2520analysis%250Aand%2520cancer%2520diagnosis.%2520The%2520code%2520is%2520made%2520available%2520at%253A%250Ahttps%253A//github.com/AyushRoy2001/AWGUNET.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AWGUNET%3A%20Attention-Aided%20Wavelet%20Guided%20U-Net%20for%20Nuclei%20Segmentation%20in%0A%20%20Histopathology%20Images&entry.906535625=Ayush%20Roy%20and%20Payel%20Pramanik%20and%20Dmitrii%20Kaplun%20and%20Sergei%20Antonov%20and%20Ram%20Sarkar&entry.1292438233=%20%20Accurate%20nuclei%20segmentation%20in%20histopathological%20images%20is%20crucial%20for%0Acancer%20diagnosis.%20Automating%20this%20process%20offers%20valuable%20support%20to%20clinical%0Aexperts%2C%20as%20manual%20annotation%20is%20time-consuming%20and%20prone%20to%20human%20errors.%0AHowever%2C%20automating%20nuclei%20segmentation%20presents%20challenges%20due%20to%20uncertain%0Acell%20boundaries%2C%20intricate%20staining%2C%20and%20diverse%20structures.%20In%20this%20paper%2C%20we%0Apresent%20a%20segmentation%20approach%20that%20combines%20the%20U-Net%20architecture%20with%20a%0ADenseNet-121%20backbone%2C%20harnessing%20the%20strengths%20of%20both%20to%20capture%0Acomprehensive%20contextual%20and%20spatial%20information.%20Our%20model%20introduces%20the%0AWavelet-guided%20channel%20attention%20module%20to%20enhance%20cell%20boundary%20delineation%2C%0Aalong%20with%20a%20learnable%20weighted%20global%20attention%20module%20for%20channel-specific%0Aattention.%20The%20decoder%20module%2C%20composed%20of%20an%20upsample%20block%20and%20convolution%0Ablock%2C%20further%20refines%20segmentation%20in%20handling%20staining%20patterns.%20The%0Aexperimental%20results%20conducted%20on%20two%20publicly%20accessible%20histopathology%0Adatasets%2C%20namely%20Monuseg%20and%20TNBC%2C%20underscore%20the%20superiority%20of%20our%20proposed%0Amodel%2C%20demonstrating%20its%20potential%20to%20advance%20histopathological%20image%20analysis%0Aand%20cancer%20diagnosis.%20The%20code%20is%20made%20available%20at%3A%0Ahttps%3A//github.com/AyushRoy2001/AWGUNET.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08425v1&entry.124074799=Read"},
{"title": "A deep cut into Split Federated Self-supervised Learning", "author": "Marcin Przewi\u0119\u017alikowski and Marcin Osial and Bartosz Zieli\u0144ski and Marek \u015amieja", "abstract": "  Collaborative self-supervised learning has recently become feasible in highly\ndistributed environments by dividing the network layers between client devices\nand a central server. However, state-of-the-art methods, such as MocoSFL, are\noptimized for network division at the initial layers, which decreases the\nprotection of the client data and increases communication overhead. In this\npaper, we demonstrate that splitting depth is crucial for maintaining privacy\nand communication efficiency in distributed training. We also show that MocoSFL\nsuffers from a catastrophic quality deterioration for the minimal communication\noverhead. As a remedy, we introduce Momentum-Aligned contrastive Split\nFederated Learning (MonAcoSFL), which aligns online and momentum client models\nduring training procedure. Consequently, we achieve state-of-the-art accuracy\nwhile significantly reducing the communication overhead, making MonAcoSFL more\npractical in real-world scenarios.\n", "link": "http://arxiv.org/abs/2406.08267v1", "date": "2024-06-12", "relevancy": 2.3751, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4917}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4713}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20deep%20cut%20into%20Split%20Federated%20Self-supervised%20Learning&body=Title%3A%20A%20deep%20cut%20into%20Split%20Federated%20Self-supervised%20Learning%0AAuthor%3A%20Marcin%20Przewi%C4%99%C5%BAlikowski%20and%20Marcin%20Osial%20and%20Bartosz%20Zieli%C5%84ski%20and%20Marek%20%C5%9Amieja%0AAbstract%3A%20%20%20Collaborative%20self-supervised%20learning%20has%20recently%20become%20feasible%20in%20highly%0Adistributed%20environments%20by%20dividing%20the%20network%20layers%20between%20client%20devices%0Aand%20a%20central%20server.%20However%2C%20state-of-the-art%20methods%2C%20such%20as%20MocoSFL%2C%20are%0Aoptimized%20for%20network%20division%20at%20the%20initial%20layers%2C%20which%20decreases%20the%0Aprotection%20of%20the%20client%20data%20and%20increases%20communication%20overhead.%20In%20this%0Apaper%2C%20we%20demonstrate%20that%20splitting%20depth%20is%20crucial%20for%20maintaining%20privacy%0Aand%20communication%20efficiency%20in%20distributed%20training.%20We%20also%20show%20that%20MocoSFL%0Asuffers%20from%20a%20catastrophic%20quality%20deterioration%20for%20the%20minimal%20communication%0Aoverhead.%20As%20a%20remedy%2C%20we%20introduce%20Momentum-Aligned%20contrastive%20Split%0AFederated%20Learning%20%28MonAcoSFL%29%2C%20which%20aligns%20online%20and%20momentum%20client%20models%0Aduring%20training%20procedure.%20Consequently%2C%20we%20achieve%20state-of-the-art%20accuracy%0Awhile%20significantly%20reducing%20the%20communication%20overhead%2C%20making%20MonAcoSFL%20more%0Apractical%20in%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520deep%2520cut%2520into%2520Split%2520Federated%2520Self-supervised%2520Learning%26entry.906535625%3DMarcin%2520Przewi%25C4%2599%25C5%25BAlikowski%2520and%2520Marcin%2520Osial%2520and%2520Bartosz%2520Zieli%25C5%2584ski%2520and%2520Marek%2520%25C5%259Amieja%26entry.1292438233%3D%2520%2520Collaborative%2520self-supervised%2520learning%2520has%2520recently%2520become%2520feasible%2520in%2520highly%250Adistributed%2520environments%2520by%2520dividing%2520the%2520network%2520layers%2520between%2520client%2520devices%250Aand%2520a%2520central%2520server.%2520However%252C%2520state-of-the-art%2520methods%252C%2520such%2520as%2520MocoSFL%252C%2520are%250Aoptimized%2520for%2520network%2520division%2520at%2520the%2520initial%2520layers%252C%2520which%2520decreases%2520the%250Aprotection%2520of%2520the%2520client%2520data%2520and%2520increases%2520communication%2520overhead.%2520In%2520this%250Apaper%252C%2520we%2520demonstrate%2520that%2520splitting%2520depth%2520is%2520crucial%2520for%2520maintaining%2520privacy%250Aand%2520communication%2520efficiency%2520in%2520distributed%2520training.%2520We%2520also%2520show%2520that%2520MocoSFL%250Asuffers%2520from%2520a%2520catastrophic%2520quality%2520deterioration%2520for%2520the%2520minimal%2520communication%250Aoverhead.%2520As%2520a%2520remedy%252C%2520we%2520introduce%2520Momentum-Aligned%2520contrastive%2520Split%250AFederated%2520Learning%2520%2528MonAcoSFL%2529%252C%2520which%2520aligns%2520online%2520and%2520momentum%2520client%2520models%250Aduring%2520training%2520procedure.%2520Consequently%252C%2520we%2520achieve%2520state-of-the-art%2520accuracy%250Awhile%2520significantly%2520reducing%2520the%2520communication%2520overhead%252C%2520making%2520MonAcoSFL%2520more%250Apractical%2520in%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20deep%20cut%20into%20Split%20Federated%20Self-supervised%20Learning&entry.906535625=Marcin%20Przewi%C4%99%C5%BAlikowski%20and%20Marcin%20Osial%20and%20Bartosz%20Zieli%C5%84ski%20and%20Marek%20%C5%9Amieja&entry.1292438233=%20%20Collaborative%20self-supervised%20learning%20has%20recently%20become%20feasible%20in%20highly%0Adistributed%20environments%20by%20dividing%20the%20network%20layers%20between%20client%20devices%0Aand%20a%20central%20server.%20However%2C%20state-of-the-art%20methods%2C%20such%20as%20MocoSFL%2C%20are%0Aoptimized%20for%20network%20division%20at%20the%20initial%20layers%2C%20which%20decreases%20the%0Aprotection%20of%20the%20client%20data%20and%20increases%20communication%20overhead.%20In%20this%0Apaper%2C%20we%20demonstrate%20that%20splitting%20depth%20is%20crucial%20for%20maintaining%20privacy%0Aand%20communication%20efficiency%20in%20distributed%20training.%20We%20also%20show%20that%20MocoSFL%0Asuffers%20from%20a%20catastrophic%20quality%20deterioration%20for%20the%20minimal%20communication%0Aoverhead.%20As%20a%20remedy%2C%20we%20introduce%20Momentum-Aligned%20contrastive%20Split%0AFederated%20Learning%20%28MonAcoSFL%29%2C%20which%20aligns%20online%20and%20momentum%20client%20models%0Aduring%20training%20procedure.%20Consequently%2C%20we%20achieve%20state-of-the-art%20accuracy%0Awhile%20significantly%20reducing%20the%20communication%20overhead%2C%20making%20MonAcoSFL%20more%0Apractical%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08267v1&entry.124074799=Read"},
{"title": "Diffusion-Promoted HDR Video Reconstruction", "author": "Yuanshen Guan and Ruikang Xu and Mingde Yao and Ruisheng Gao and Lizhi Wang and Zhiwei Xiong", "abstract": "  High dynamic range (HDR) video reconstruction aims to generate HDR videos\nfrom low dynamic range (LDR) frames captured with alternating exposures. Most\nexisting works solely rely on the regression-based paradigm, leading to adverse\neffects such as ghosting artifacts and missing details in saturated regions. In\nthis paper, we propose a diffusion-promoted method for HDR video\nreconstruction, termed HDR-V-Diff, which incorporates a diffusion model to\ncapture the HDR distribution. As such, HDR-V-Diff can reconstruct HDR videos\nwith realistic details while alleviating ghosting artifacts. However, the\ndirect introduction of video diffusion models would impose massive\ncomputational burden. Instead, to alleviate this burden, we first propose an\nHDR Latent Diffusion Model (HDR-LDM) to learn the distribution prior of single\nHDR frames. Specifically, HDR-LDM incorporates a tonemapping strategy to\ncompress HDR frames into the latent space and a novel exposure embedding to\naggregate the exposure information into the diffusion process. We then propose\na Temporal-Consistent Alignment Module (TCAM) to learn the temporal information\nas a complement for HDR-LDM, which conducts coarse-to-fine feature alignment at\ndifferent scales among video frames. Finally, we design a Zero-Init\nCross-Attention (ZiCA) mechanism to effectively integrate the learned\ndistribution prior and temporal information for generating HDR frames.\nExtensive experiments validate that HDR-V-Diff achieves state-of-the-art\nresults on several representative datasets.\n", "link": "http://arxiv.org/abs/2406.08204v1", "date": "2024-06-12", "relevancy": 2.3694, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6269}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5873}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion-Promoted%20HDR%20Video%20Reconstruction&body=Title%3A%20Diffusion-Promoted%20HDR%20Video%20Reconstruction%0AAuthor%3A%20Yuanshen%20Guan%20and%20Ruikang%20Xu%20and%20Mingde%20Yao%20and%20Ruisheng%20Gao%20and%20Lizhi%20Wang%20and%20Zhiwei%20Xiong%0AAbstract%3A%20%20%20High%20dynamic%20range%20%28HDR%29%20video%20reconstruction%20aims%20to%20generate%20HDR%20videos%0Afrom%20low%20dynamic%20range%20%28LDR%29%20frames%20captured%20with%20alternating%20exposures.%20Most%0Aexisting%20works%20solely%20rely%20on%20the%20regression-based%20paradigm%2C%20leading%20to%20adverse%0Aeffects%20such%20as%20ghosting%20artifacts%20and%20missing%20details%20in%20saturated%20regions.%20In%0Athis%20paper%2C%20we%20propose%20a%20diffusion-promoted%20method%20for%20HDR%20video%0Areconstruction%2C%20termed%20HDR-V-Diff%2C%20which%20incorporates%20a%20diffusion%20model%20to%0Acapture%20the%20HDR%20distribution.%20As%20such%2C%20HDR-V-Diff%20can%20reconstruct%20HDR%20videos%0Awith%20realistic%20details%20while%20alleviating%20ghosting%20artifacts.%20However%2C%20the%0Adirect%20introduction%20of%20video%20diffusion%20models%20would%20impose%20massive%0Acomputational%20burden.%20Instead%2C%20to%20alleviate%20this%20burden%2C%20we%20first%20propose%20an%0AHDR%20Latent%20Diffusion%20Model%20%28HDR-LDM%29%20to%20learn%20the%20distribution%20prior%20of%20single%0AHDR%20frames.%20Specifically%2C%20HDR-LDM%20incorporates%20a%20tonemapping%20strategy%20to%0Acompress%20HDR%20frames%20into%20the%20latent%20space%20and%20a%20novel%20exposure%20embedding%20to%0Aaggregate%20the%20exposure%20information%20into%20the%20diffusion%20process.%20We%20then%20propose%0Aa%20Temporal-Consistent%20Alignment%20Module%20%28TCAM%29%20to%20learn%20the%20temporal%20information%0Aas%20a%20complement%20for%20HDR-LDM%2C%20which%20conducts%20coarse-to-fine%20feature%20alignment%20at%0Adifferent%20scales%20among%20video%20frames.%20Finally%2C%20we%20design%20a%20Zero-Init%0ACross-Attention%20%28ZiCA%29%20mechanism%20to%20effectively%20integrate%20the%20learned%0Adistribution%20prior%20and%20temporal%20information%20for%20generating%20HDR%20frames.%0AExtensive%20experiments%20validate%20that%20HDR-V-Diff%20achieves%20state-of-the-art%0Aresults%20on%20several%20representative%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion-Promoted%2520HDR%2520Video%2520Reconstruction%26entry.906535625%3DYuanshen%2520Guan%2520and%2520Ruikang%2520Xu%2520and%2520Mingde%2520Yao%2520and%2520Ruisheng%2520Gao%2520and%2520Lizhi%2520Wang%2520and%2520Zhiwei%2520Xiong%26entry.1292438233%3D%2520%2520High%2520dynamic%2520range%2520%2528HDR%2529%2520video%2520reconstruction%2520aims%2520to%2520generate%2520HDR%2520videos%250Afrom%2520low%2520dynamic%2520range%2520%2528LDR%2529%2520frames%2520captured%2520with%2520alternating%2520exposures.%2520Most%250Aexisting%2520works%2520solely%2520rely%2520on%2520the%2520regression-based%2520paradigm%252C%2520leading%2520to%2520adverse%250Aeffects%2520such%2520as%2520ghosting%2520artifacts%2520and%2520missing%2520details%2520in%2520saturated%2520regions.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520diffusion-promoted%2520method%2520for%2520HDR%2520video%250Areconstruction%252C%2520termed%2520HDR-V-Diff%252C%2520which%2520incorporates%2520a%2520diffusion%2520model%2520to%250Acapture%2520the%2520HDR%2520distribution.%2520As%2520such%252C%2520HDR-V-Diff%2520can%2520reconstruct%2520HDR%2520videos%250Awith%2520realistic%2520details%2520while%2520alleviating%2520ghosting%2520artifacts.%2520However%252C%2520the%250Adirect%2520introduction%2520of%2520video%2520diffusion%2520models%2520would%2520impose%2520massive%250Acomputational%2520burden.%2520Instead%252C%2520to%2520alleviate%2520this%2520burden%252C%2520we%2520first%2520propose%2520an%250AHDR%2520Latent%2520Diffusion%2520Model%2520%2528HDR-LDM%2529%2520to%2520learn%2520the%2520distribution%2520prior%2520of%2520single%250AHDR%2520frames.%2520Specifically%252C%2520HDR-LDM%2520incorporates%2520a%2520tonemapping%2520strategy%2520to%250Acompress%2520HDR%2520frames%2520into%2520the%2520latent%2520space%2520and%2520a%2520novel%2520exposure%2520embedding%2520to%250Aaggregate%2520the%2520exposure%2520information%2520into%2520the%2520diffusion%2520process.%2520We%2520then%2520propose%250Aa%2520Temporal-Consistent%2520Alignment%2520Module%2520%2528TCAM%2529%2520to%2520learn%2520the%2520temporal%2520information%250Aas%2520a%2520complement%2520for%2520HDR-LDM%252C%2520which%2520conducts%2520coarse-to-fine%2520feature%2520alignment%2520at%250Adifferent%2520scales%2520among%2520video%2520frames.%2520Finally%252C%2520we%2520design%2520a%2520Zero-Init%250ACross-Attention%2520%2528ZiCA%2529%2520mechanism%2520to%2520effectively%2520integrate%2520the%2520learned%250Adistribution%2520prior%2520and%2520temporal%2520information%2520for%2520generating%2520HDR%2520frames.%250AExtensive%2520experiments%2520validate%2520that%2520HDR-V-Diff%2520achieves%2520state-of-the-art%250Aresults%2520on%2520several%2520representative%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-Promoted%20HDR%20Video%20Reconstruction&entry.906535625=Yuanshen%20Guan%20and%20Ruikang%20Xu%20and%20Mingde%20Yao%20and%20Ruisheng%20Gao%20and%20Lizhi%20Wang%20and%20Zhiwei%20Xiong&entry.1292438233=%20%20High%20dynamic%20range%20%28HDR%29%20video%20reconstruction%20aims%20to%20generate%20HDR%20videos%0Afrom%20low%20dynamic%20range%20%28LDR%29%20frames%20captured%20with%20alternating%20exposures.%20Most%0Aexisting%20works%20solely%20rely%20on%20the%20regression-based%20paradigm%2C%20leading%20to%20adverse%0Aeffects%20such%20as%20ghosting%20artifacts%20and%20missing%20details%20in%20saturated%20regions.%20In%0Athis%20paper%2C%20we%20propose%20a%20diffusion-promoted%20method%20for%20HDR%20video%0Areconstruction%2C%20termed%20HDR-V-Diff%2C%20which%20incorporates%20a%20diffusion%20model%20to%0Acapture%20the%20HDR%20distribution.%20As%20such%2C%20HDR-V-Diff%20can%20reconstruct%20HDR%20videos%0Awith%20realistic%20details%20while%20alleviating%20ghosting%20artifacts.%20However%2C%20the%0Adirect%20introduction%20of%20video%20diffusion%20models%20would%20impose%20massive%0Acomputational%20burden.%20Instead%2C%20to%20alleviate%20this%20burden%2C%20we%20first%20propose%20an%0AHDR%20Latent%20Diffusion%20Model%20%28HDR-LDM%29%20to%20learn%20the%20distribution%20prior%20of%20single%0AHDR%20frames.%20Specifically%2C%20HDR-LDM%20incorporates%20a%20tonemapping%20strategy%20to%0Acompress%20HDR%20frames%20into%20the%20latent%20space%20and%20a%20novel%20exposure%20embedding%20to%0Aaggregate%20the%20exposure%20information%20into%20the%20diffusion%20process.%20We%20then%20propose%0Aa%20Temporal-Consistent%20Alignment%20Module%20%28TCAM%29%20to%20learn%20the%20temporal%20information%0Aas%20a%20complement%20for%20HDR-LDM%2C%20which%20conducts%20coarse-to-fine%20feature%20alignment%20at%0Adifferent%20scales%20among%20video%20frames.%20Finally%2C%20we%20design%20a%20Zero-Init%0ACross-Attention%20%28ZiCA%29%20mechanism%20to%20effectively%20integrate%20the%20learned%0Adistribution%20prior%20and%20temporal%20information%20for%20generating%20HDR%20frames.%0AExtensive%20experiments%20validate%20that%20HDR-V-Diff%20achieves%20state-of-the-art%0Aresults%20on%20several%20representative%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08204v1&entry.124074799=Read"},
{"title": "ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs", "author": "Irene Huang and Wei Lin and M. Jehanzeb Mirza and Jacob A. Hansen and Sivan Doveh and Victor Ion Butoi and Roei Herzig and Assaf Arbelle and Hilde Kuhene and Trevor Darrel and Chuang Gan and Aude Oliva and Rogerio Feris and Leonid Karlinsky", "abstract": "  Compositional Reasoning (CR) entails grasping the significance of attributes,\nrelations, and word order. Recent Vision-Language Models (VLMs), comprising a\nvisual encoder and a Large Language Model (LLM) decoder, have demonstrated\nremarkable proficiency in such reasoning tasks. This prompts a crucial\nquestion: have VLMs effectively tackled the CR challenge? We conjecture that\nexisting CR benchmarks may not adequately push the boundaries of modern VLMs\ndue to the reliance on an LLM-only negative text generation pipeline.\nConsequently, the negatives produced either appear as outliers from the natural\nlanguage distribution learned by VLMs' LLM decoders or as improbable within the\ncorresponding image context. To address these limitations, we introduce ConMe\n-- a compositional reasoning benchmark and a novel data generation pipeline\nleveraging VLMs to produce `hard CR Q&A'. Through a new concept of VLMs\nconversing with each other to collaboratively expose their weaknesses, our\npipeline autonomously generates, evaluates, and selects challenging\ncompositional reasoning questions, establishing a robust CR benchmark, also\nsubsequently validated manually. Our benchmark provokes a noteworthy, up to\n33%, decrease in CR performance compared to preceding benchmarks, reinstating\nthe CR challenge even for state-of-the-art VLMs.\n", "link": "http://arxiv.org/abs/2406.08164v1", "date": "2024-06-12", "relevancy": 2.3636, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4846}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4751}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConMe%3A%20Rethinking%20Evaluation%20of%20Compositional%20Reasoning%20for%20Modern%20VLMs&body=Title%3A%20ConMe%3A%20Rethinking%20Evaluation%20of%20Compositional%20Reasoning%20for%20Modern%20VLMs%0AAuthor%3A%20Irene%20Huang%20and%20Wei%20Lin%20and%20M.%20Jehanzeb%20Mirza%20and%20Jacob%20A.%20Hansen%20and%20Sivan%20Doveh%20and%20Victor%20Ion%20Butoi%20and%20Roei%20Herzig%20and%20Assaf%20Arbelle%20and%20Hilde%20Kuhene%20and%20Trevor%20Darrel%20and%20Chuang%20Gan%20and%20Aude%20Oliva%20and%20Rogerio%20Feris%20and%20Leonid%20Karlinsky%0AAbstract%3A%20%20%20Compositional%20Reasoning%20%28CR%29%20entails%20grasping%20the%20significance%20of%20attributes%2C%0Arelations%2C%20and%20word%20order.%20Recent%20Vision-Language%20Models%20%28VLMs%29%2C%20comprising%20a%0Avisual%20encoder%20and%20a%20Large%20Language%20Model%20%28LLM%29%20decoder%2C%20have%20demonstrated%0Aremarkable%20proficiency%20in%20such%20reasoning%20tasks.%20This%20prompts%20a%20crucial%0Aquestion%3A%20have%20VLMs%20effectively%20tackled%20the%20CR%20challenge%3F%20We%20conjecture%20that%0Aexisting%20CR%20benchmarks%20may%20not%20adequately%20push%20the%20boundaries%20of%20modern%20VLMs%0Adue%20to%20the%20reliance%20on%20an%20LLM-only%20negative%20text%20generation%20pipeline.%0AConsequently%2C%20the%20negatives%20produced%20either%20appear%20as%20outliers%20from%20the%20natural%0Alanguage%20distribution%20learned%20by%20VLMs%27%20LLM%20decoders%20or%20as%20improbable%20within%20the%0Acorresponding%20image%20context.%20To%20address%20these%20limitations%2C%20we%20introduce%20ConMe%0A--%20a%20compositional%20reasoning%20benchmark%20and%20a%20novel%20data%20generation%20pipeline%0Aleveraging%20VLMs%20to%20produce%20%60hard%20CR%20Q%26A%27.%20Through%20a%20new%20concept%20of%20VLMs%0Aconversing%20with%20each%20other%20to%20collaboratively%20expose%20their%20weaknesses%2C%20our%0Apipeline%20autonomously%20generates%2C%20evaluates%2C%20and%20selects%20challenging%0Acompositional%20reasoning%20questions%2C%20establishing%20a%20robust%20CR%20benchmark%2C%20also%0Asubsequently%20validated%20manually.%20Our%20benchmark%20provokes%20a%20noteworthy%2C%20up%20to%0A33%25%2C%20decrease%20in%20CR%20performance%20compared%20to%20preceding%20benchmarks%2C%20reinstating%0Athe%20CR%20challenge%20even%20for%20state-of-the-art%20VLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConMe%253A%2520Rethinking%2520Evaluation%2520of%2520Compositional%2520Reasoning%2520for%2520Modern%2520VLMs%26entry.906535625%3DIrene%2520Huang%2520and%2520Wei%2520Lin%2520and%2520M.%2520Jehanzeb%2520Mirza%2520and%2520Jacob%2520A.%2520Hansen%2520and%2520Sivan%2520Doveh%2520and%2520Victor%2520Ion%2520Butoi%2520and%2520Roei%2520Herzig%2520and%2520Assaf%2520Arbelle%2520and%2520Hilde%2520Kuhene%2520and%2520Trevor%2520Darrel%2520and%2520Chuang%2520Gan%2520and%2520Aude%2520Oliva%2520and%2520Rogerio%2520Feris%2520and%2520Leonid%2520Karlinsky%26entry.1292438233%3D%2520%2520Compositional%2520Reasoning%2520%2528CR%2529%2520entails%2520grasping%2520the%2520significance%2520of%2520attributes%252C%250Arelations%252C%2520and%2520word%2520order.%2520Recent%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520comprising%2520a%250Avisual%2520encoder%2520and%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520decoder%252C%2520have%2520demonstrated%250Aremarkable%2520proficiency%2520in%2520such%2520reasoning%2520tasks.%2520This%2520prompts%2520a%2520crucial%250Aquestion%253A%2520have%2520VLMs%2520effectively%2520tackled%2520the%2520CR%2520challenge%253F%2520We%2520conjecture%2520that%250Aexisting%2520CR%2520benchmarks%2520may%2520not%2520adequately%2520push%2520the%2520boundaries%2520of%2520modern%2520VLMs%250Adue%2520to%2520the%2520reliance%2520on%2520an%2520LLM-only%2520negative%2520text%2520generation%2520pipeline.%250AConsequently%252C%2520the%2520negatives%2520produced%2520either%2520appear%2520as%2520outliers%2520from%2520the%2520natural%250Alanguage%2520distribution%2520learned%2520by%2520VLMs%2527%2520LLM%2520decoders%2520or%2520as%2520improbable%2520within%2520the%250Acorresponding%2520image%2520context.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520ConMe%250A--%2520a%2520compositional%2520reasoning%2520benchmark%2520and%2520a%2520novel%2520data%2520generation%2520pipeline%250Aleveraging%2520VLMs%2520to%2520produce%2520%2560hard%2520CR%2520Q%2526A%2527.%2520Through%2520a%2520new%2520concept%2520of%2520VLMs%250Aconversing%2520with%2520each%2520other%2520to%2520collaboratively%2520expose%2520their%2520weaknesses%252C%2520our%250Apipeline%2520autonomously%2520generates%252C%2520evaluates%252C%2520and%2520selects%2520challenging%250Acompositional%2520reasoning%2520questions%252C%2520establishing%2520a%2520robust%2520CR%2520benchmark%252C%2520also%250Asubsequently%2520validated%2520manually.%2520Our%2520benchmark%2520provokes%2520a%2520noteworthy%252C%2520up%2520to%250A33%2525%252C%2520decrease%2520in%2520CR%2520performance%2520compared%2520to%2520preceding%2520benchmarks%252C%2520reinstating%250Athe%2520CR%2520challenge%2520even%2520for%2520state-of-the-art%2520VLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConMe%3A%20Rethinking%20Evaluation%20of%20Compositional%20Reasoning%20for%20Modern%20VLMs&entry.906535625=Irene%20Huang%20and%20Wei%20Lin%20and%20M.%20Jehanzeb%20Mirza%20and%20Jacob%20A.%20Hansen%20and%20Sivan%20Doveh%20and%20Victor%20Ion%20Butoi%20and%20Roei%20Herzig%20and%20Assaf%20Arbelle%20and%20Hilde%20Kuhene%20and%20Trevor%20Darrel%20and%20Chuang%20Gan%20and%20Aude%20Oliva%20and%20Rogerio%20Feris%20and%20Leonid%20Karlinsky&entry.1292438233=%20%20Compositional%20Reasoning%20%28CR%29%20entails%20grasping%20the%20significance%20of%20attributes%2C%0Arelations%2C%20and%20word%20order.%20Recent%20Vision-Language%20Models%20%28VLMs%29%2C%20comprising%20a%0Avisual%20encoder%20and%20a%20Large%20Language%20Model%20%28LLM%29%20decoder%2C%20have%20demonstrated%0Aremarkable%20proficiency%20in%20such%20reasoning%20tasks.%20This%20prompts%20a%20crucial%0Aquestion%3A%20have%20VLMs%20effectively%20tackled%20the%20CR%20challenge%3F%20We%20conjecture%20that%0Aexisting%20CR%20benchmarks%20may%20not%20adequately%20push%20the%20boundaries%20of%20modern%20VLMs%0Adue%20to%20the%20reliance%20on%20an%20LLM-only%20negative%20text%20generation%20pipeline.%0AConsequently%2C%20the%20negatives%20produced%20either%20appear%20as%20outliers%20from%20the%20natural%0Alanguage%20distribution%20learned%20by%20VLMs%27%20LLM%20decoders%20or%20as%20improbable%20within%20the%0Acorresponding%20image%20context.%20To%20address%20these%20limitations%2C%20we%20introduce%20ConMe%0A--%20a%20compositional%20reasoning%20benchmark%20and%20a%20novel%20data%20generation%20pipeline%0Aleveraging%20VLMs%20to%20produce%20%60hard%20CR%20Q%26A%27.%20Through%20a%20new%20concept%20of%20VLMs%0Aconversing%20with%20each%20other%20to%20collaboratively%20expose%20their%20weaknesses%2C%20our%0Apipeline%20autonomously%20generates%2C%20evaluates%2C%20and%20selects%20challenging%0Acompositional%20reasoning%20questions%2C%20establishing%20a%20robust%20CR%20benchmark%2C%20also%0Asubsequently%20validated%20manually.%20Our%20benchmark%20provokes%20a%20noteworthy%2C%20up%20to%0A33%25%2C%20decrease%20in%20CR%20performance%20compared%20to%20preceding%20benchmarks%2C%20reinstating%0Athe%20CR%20challenge%20even%20for%20state-of-the-art%20VLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08164v1&entry.124074799=Read"},
{"title": "Suppressing unknown disturbances to dynamical systems using machine\n  learning", "author": "Juan G. Restrepo and Clayton P. Byers and Per Sebastian Skardal", "abstract": "  Identifying and suppressing unknown disturbances to dynamical systems is a\nproblem with applications in many different fields. Here we present a\nmodel-free method to identify and suppress an unknown disturbance to an unknown\nsystem based only on previous observations of the system under the influence of\na known forcing function. We find that, under very mild restrictions on the\ntraining function, our method is able to robustly identify and suppress a large\nclass of unknown disturbances. We illustrate our scheme with the identification\nof both deterministic and stochastic unknown disturbances to an analog electric\nchaotic circuit and with a numerical example where a chaotic disturbance to the\nLorenz system is identified and suppressed.\n", "link": "http://arxiv.org/abs/2307.03690v4", "date": "2024-06-12", "relevancy": 2.3572, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4856}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4649}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Suppressing%20unknown%20disturbances%20to%20dynamical%20systems%20using%20machine%0A%20%20learning&body=Title%3A%20Suppressing%20unknown%20disturbances%20to%20dynamical%20systems%20using%20machine%0A%20%20learning%0AAuthor%3A%20Juan%20G.%20Restrepo%20and%20Clayton%20P.%20Byers%20and%20Per%20Sebastian%20Skardal%0AAbstract%3A%20%20%20Identifying%20and%20suppressing%20unknown%20disturbances%20to%20dynamical%20systems%20is%20a%0Aproblem%20with%20applications%20in%20many%20different%20fields.%20Here%20we%20present%20a%0Amodel-free%20method%20to%20identify%20and%20suppress%20an%20unknown%20disturbance%20to%20an%20unknown%0Asystem%20based%20only%20on%20previous%20observations%20of%20the%20system%20under%20the%20influence%20of%0Aa%20known%20forcing%20function.%20We%20find%20that%2C%20under%20very%20mild%20restrictions%20on%20the%0Atraining%20function%2C%20our%20method%20is%20able%20to%20robustly%20identify%20and%20suppress%20a%20large%0Aclass%20of%20unknown%20disturbances.%20We%20illustrate%20our%20scheme%20with%20the%20identification%0Aof%20both%20deterministic%20and%20stochastic%20unknown%20disturbances%20to%20an%20analog%20electric%0Achaotic%20circuit%20and%20with%20a%20numerical%20example%20where%20a%20chaotic%20disturbance%20to%20the%0ALorenz%20system%20is%20identified%20and%20suppressed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.03690v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuppressing%2520unknown%2520disturbances%2520to%2520dynamical%2520systems%2520using%2520machine%250A%2520%2520learning%26entry.906535625%3DJuan%2520G.%2520Restrepo%2520and%2520Clayton%2520P.%2520Byers%2520and%2520Per%2520Sebastian%2520Skardal%26entry.1292438233%3D%2520%2520Identifying%2520and%2520suppressing%2520unknown%2520disturbances%2520to%2520dynamical%2520systems%2520is%2520a%250Aproblem%2520with%2520applications%2520in%2520many%2520different%2520fields.%2520Here%2520we%2520present%2520a%250Amodel-free%2520method%2520to%2520identify%2520and%2520suppress%2520an%2520unknown%2520disturbance%2520to%2520an%2520unknown%250Asystem%2520based%2520only%2520on%2520previous%2520observations%2520of%2520the%2520system%2520under%2520the%2520influence%2520of%250Aa%2520known%2520forcing%2520function.%2520We%2520find%2520that%252C%2520under%2520very%2520mild%2520restrictions%2520on%2520the%250Atraining%2520function%252C%2520our%2520method%2520is%2520able%2520to%2520robustly%2520identify%2520and%2520suppress%2520a%2520large%250Aclass%2520of%2520unknown%2520disturbances.%2520We%2520illustrate%2520our%2520scheme%2520with%2520the%2520identification%250Aof%2520both%2520deterministic%2520and%2520stochastic%2520unknown%2520disturbances%2520to%2520an%2520analog%2520electric%250Achaotic%2520circuit%2520and%2520with%2520a%2520numerical%2520example%2520where%2520a%2520chaotic%2520disturbance%2520to%2520the%250ALorenz%2520system%2520is%2520identified%2520and%2520suppressed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.03690v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Suppressing%20unknown%20disturbances%20to%20dynamical%20systems%20using%20machine%0A%20%20learning&entry.906535625=Juan%20G.%20Restrepo%20and%20Clayton%20P.%20Byers%20and%20Per%20Sebastian%20Skardal&entry.1292438233=%20%20Identifying%20and%20suppressing%20unknown%20disturbances%20to%20dynamical%20systems%20is%20a%0Aproblem%20with%20applications%20in%20many%20different%20fields.%20Here%20we%20present%20a%0Amodel-free%20method%20to%20identify%20and%20suppress%20an%20unknown%20disturbance%20to%20an%20unknown%0Asystem%20based%20only%20on%20previous%20observations%20of%20the%20system%20under%20the%20influence%20of%0Aa%20known%20forcing%20function.%20We%20find%20that%2C%20under%20very%20mild%20restrictions%20on%20the%0Atraining%20function%2C%20our%20method%20is%20able%20to%20robustly%20identify%20and%20suppress%20a%20large%0Aclass%20of%20unknown%20disturbances.%20We%20illustrate%20our%20scheme%20with%20the%20identification%0Aof%20both%20deterministic%20and%20stochastic%20unknown%20disturbances%20to%20an%20analog%20electric%0Achaotic%20circuit%20and%20with%20a%20numerical%20example%20where%20a%20chaotic%20disturbance%20to%20the%0ALorenz%20system%20is%20identified%20and%20suppressed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.03690v4&entry.124074799=Read"},
{"title": "From Sim-to-Real: Toward General Event-based Low-light Frame\n  Interpolation with Per-scene Optimization", "author": "Ziran Zhang and Yongrui Ma and Yueting Chen and Feng Zhang and Jinwei Gu and Tianfan Xue and Shi Guo", "abstract": "  Video Frame Interpolation (VFI) is important for video enhancement, frame\nrate up-conversion, and slow-motion generation. The introduction of event\ncameras, which capture per-pixel brightness changes asynchronously, has\nsignificantly enhanced VFI capabilities, particularly for high-speed, nonlinear\nmotions. However, these event-based methods encounter challenges in low-light\nconditions, notably trailing artifacts and signal latency, which hinder their\ndirect applicability and generalization. Addressing these issues, we propose a\nnovel per-scene optimization strategy tailored for low-light conditions. This\napproach utilizes the internal statistics of a sequence to handle degraded\nevent data under low-light conditions, improving the generalizability to\ndifferent lighting and camera settings. To evaluate its robustness in low-light\ncondition, we further introduce EVFI-LL, a unique RGB+Event dataset captured\nunder low-light conditions. Our results demonstrate state-of-the-art\nperformance in low-light environments. Both the dataset and the source code\nwill be made publicly available upon publication. Project page:\nhttps://naturezhanghn.github.io/sim2real.\n", "link": "http://arxiv.org/abs/2406.08090v1", "date": "2024-06-12", "relevancy": 2.3441, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6171}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5659}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Sim-to-Real%3A%20Toward%20General%20Event-based%20Low-light%20Frame%0A%20%20Interpolation%20with%20Per-scene%20Optimization&body=Title%3A%20From%20Sim-to-Real%3A%20Toward%20General%20Event-based%20Low-light%20Frame%0A%20%20Interpolation%20with%20Per-scene%20Optimization%0AAuthor%3A%20Ziran%20Zhang%20and%20Yongrui%20Ma%20and%20Yueting%20Chen%20and%20Feng%20Zhang%20and%20Jinwei%20Gu%20and%20Tianfan%20Xue%20and%20Shi%20Guo%0AAbstract%3A%20%20%20Video%20Frame%20Interpolation%20%28VFI%29%20is%20important%20for%20video%20enhancement%2C%20frame%0Arate%20up-conversion%2C%20and%20slow-motion%20generation.%20The%20introduction%20of%20event%0Acameras%2C%20which%20capture%20per-pixel%20brightness%20changes%20asynchronously%2C%20has%0Asignificantly%20enhanced%20VFI%20capabilities%2C%20particularly%20for%20high-speed%2C%20nonlinear%0Amotions.%20However%2C%20these%20event-based%20methods%20encounter%20challenges%20in%20low-light%0Aconditions%2C%20notably%20trailing%20artifacts%20and%20signal%20latency%2C%20which%20hinder%20their%0Adirect%20applicability%20and%20generalization.%20Addressing%20these%20issues%2C%20we%20propose%20a%0Anovel%20per-scene%20optimization%20strategy%20tailored%20for%20low-light%20conditions.%20This%0Aapproach%20utilizes%20the%20internal%20statistics%20of%20a%20sequence%20to%20handle%20degraded%0Aevent%20data%20under%20low-light%20conditions%2C%20improving%20the%20generalizability%20to%0Adifferent%20lighting%20and%20camera%20settings.%20To%20evaluate%20its%20robustness%20in%20low-light%0Acondition%2C%20we%20further%20introduce%20EVFI-LL%2C%20a%20unique%20RGB%2BEvent%20dataset%20captured%0Aunder%20low-light%20conditions.%20Our%20results%20demonstrate%20state-of-the-art%0Aperformance%20in%20low-light%20environments.%20Both%20the%20dataset%20and%20the%20source%20code%0Awill%20be%20made%20publicly%20available%20upon%20publication.%20Project%20page%3A%0Ahttps%3A//naturezhanghn.github.io/sim2real.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08090v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Sim-to-Real%253A%2520Toward%2520General%2520Event-based%2520Low-light%2520Frame%250A%2520%2520Interpolation%2520with%2520Per-scene%2520Optimization%26entry.906535625%3DZiran%2520Zhang%2520and%2520Yongrui%2520Ma%2520and%2520Yueting%2520Chen%2520and%2520Feng%2520Zhang%2520and%2520Jinwei%2520Gu%2520and%2520Tianfan%2520Xue%2520and%2520Shi%2520Guo%26entry.1292438233%3D%2520%2520Video%2520Frame%2520Interpolation%2520%2528VFI%2529%2520is%2520important%2520for%2520video%2520enhancement%252C%2520frame%250Arate%2520up-conversion%252C%2520and%2520slow-motion%2520generation.%2520The%2520introduction%2520of%2520event%250Acameras%252C%2520which%2520capture%2520per-pixel%2520brightness%2520changes%2520asynchronously%252C%2520has%250Asignificantly%2520enhanced%2520VFI%2520capabilities%252C%2520particularly%2520for%2520high-speed%252C%2520nonlinear%250Amotions.%2520However%252C%2520these%2520event-based%2520methods%2520encounter%2520challenges%2520in%2520low-light%250Aconditions%252C%2520notably%2520trailing%2520artifacts%2520and%2520signal%2520latency%252C%2520which%2520hinder%2520their%250Adirect%2520applicability%2520and%2520generalization.%2520Addressing%2520these%2520issues%252C%2520we%2520propose%2520a%250Anovel%2520per-scene%2520optimization%2520strategy%2520tailored%2520for%2520low-light%2520conditions.%2520This%250Aapproach%2520utilizes%2520the%2520internal%2520statistics%2520of%2520a%2520sequence%2520to%2520handle%2520degraded%250Aevent%2520data%2520under%2520low-light%2520conditions%252C%2520improving%2520the%2520generalizability%2520to%250Adifferent%2520lighting%2520and%2520camera%2520settings.%2520To%2520evaluate%2520its%2520robustness%2520in%2520low-light%250Acondition%252C%2520we%2520further%2520introduce%2520EVFI-LL%252C%2520a%2520unique%2520RGB%252BEvent%2520dataset%2520captured%250Aunder%2520low-light%2520conditions.%2520Our%2520results%2520demonstrate%2520state-of-the-art%250Aperformance%2520in%2520low-light%2520environments.%2520Both%2520the%2520dataset%2520and%2520the%2520source%2520code%250Awill%2520be%2520made%2520publicly%2520available%2520upon%2520publication.%2520Project%2520page%253A%250Ahttps%253A//naturezhanghn.github.io/sim2real.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08090v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Sim-to-Real%3A%20Toward%20General%20Event-based%20Low-light%20Frame%0A%20%20Interpolation%20with%20Per-scene%20Optimization&entry.906535625=Ziran%20Zhang%20and%20Yongrui%20Ma%20and%20Yueting%20Chen%20and%20Feng%20Zhang%20and%20Jinwei%20Gu%20and%20Tianfan%20Xue%20and%20Shi%20Guo&entry.1292438233=%20%20Video%20Frame%20Interpolation%20%28VFI%29%20is%20important%20for%20video%20enhancement%2C%20frame%0Arate%20up-conversion%2C%20and%20slow-motion%20generation.%20The%20introduction%20of%20event%0Acameras%2C%20which%20capture%20per-pixel%20brightness%20changes%20asynchronously%2C%20has%0Asignificantly%20enhanced%20VFI%20capabilities%2C%20particularly%20for%20high-speed%2C%20nonlinear%0Amotions.%20However%2C%20these%20event-based%20methods%20encounter%20challenges%20in%20low-light%0Aconditions%2C%20notably%20trailing%20artifacts%20and%20signal%20latency%2C%20which%20hinder%20their%0Adirect%20applicability%20and%20generalization.%20Addressing%20these%20issues%2C%20we%20propose%20a%0Anovel%20per-scene%20optimization%20strategy%20tailored%20for%20low-light%20conditions.%20This%0Aapproach%20utilizes%20the%20internal%20statistics%20of%20a%20sequence%20to%20handle%20degraded%0Aevent%20data%20under%20low-light%20conditions%2C%20improving%20the%20generalizability%20to%0Adifferent%20lighting%20and%20camera%20settings.%20To%20evaluate%20its%20robustness%20in%20low-light%0Acondition%2C%20we%20further%20introduce%20EVFI-LL%2C%20a%20unique%20RGB%2BEvent%20dataset%20captured%0Aunder%20low-light%20conditions.%20Our%20results%20demonstrate%20state-of-the-art%0Aperformance%20in%20low-light%20environments.%20Both%20the%20dataset%20and%20the%20source%20code%0Awill%20be%20made%20publicly%20available%20upon%20publication.%20Project%20page%3A%0Ahttps%3A//naturezhanghn.github.io/sim2real.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08090v1&entry.124074799=Read"},
{"title": "ConceptHash: Interpretable Fine-Grained Hashing via Concept Discovery", "author": "Kam Woh Ng and Xiatian Zhu and Yi-Zhe Song and Tao Xiang", "abstract": "  Existing fine-grained hashing methods typically lack code interpretability as\nthey compute hash code bits holistically using both global and local features.\nTo address this limitation, we propose ConceptHash, a novel method that\nachieves sub-code level interpretability. In ConceptHash, each sub-code\ncorresponds to a human-understandable concept, such as an object part, and\nthese concepts are automatically discovered without human annotations.\nSpecifically, we leverage a Vision Transformer architecture and introduce\nconcept tokens as visual prompts, along with image patch tokens as model\ninputs. Each concept is then mapped to a specific sub-code at the model output,\nproviding natural sub-code interpretability. To capture subtle visual\ndifferences among highly similar sub-categories (e.g., bird species), we\nincorporate language guidance to ensure that the learned hash codes are\ndistinguishable within fine-grained object classes while maintaining semantic\nalignment. This approach allows us to develop hash codes that exhibit\nsimilarity within families of species while remaining distinct from species in\nother families. Extensive experiments on four fine-grained image retrieval\nbenchmarks demonstrate that ConceptHash outperforms previous methods by a\nsignificant margin, offering unique sub-code interpretability as an additional\nbenefit. Code at: https://github.com/kamwoh/concepthash.\n", "link": "http://arxiv.org/abs/2406.08457v1", "date": "2024-06-12", "relevancy": 2.329, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4724}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4713}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConceptHash%3A%20Interpretable%20Fine-Grained%20Hashing%20via%20Concept%20Discovery&body=Title%3A%20ConceptHash%3A%20Interpretable%20Fine-Grained%20Hashing%20via%20Concept%20Discovery%0AAuthor%3A%20Kam%20Woh%20Ng%20and%20Xiatian%20Zhu%20and%20Yi-Zhe%20Song%20and%20Tao%20Xiang%0AAbstract%3A%20%20%20Existing%20fine-grained%20hashing%20methods%20typically%20lack%20code%20interpretability%20as%0Athey%20compute%20hash%20code%20bits%20holistically%20using%20both%20global%20and%20local%20features.%0ATo%20address%20this%20limitation%2C%20we%20propose%20ConceptHash%2C%20a%20novel%20method%20that%0Aachieves%20sub-code%20level%20interpretability.%20In%20ConceptHash%2C%20each%20sub-code%0Acorresponds%20to%20a%20human-understandable%20concept%2C%20such%20as%20an%20object%20part%2C%20and%0Athese%20concepts%20are%20automatically%20discovered%20without%20human%20annotations.%0ASpecifically%2C%20we%20leverage%20a%20Vision%20Transformer%20architecture%20and%20introduce%0Aconcept%20tokens%20as%20visual%20prompts%2C%20along%20with%20image%20patch%20tokens%20as%20model%0Ainputs.%20Each%20concept%20is%20then%20mapped%20to%20a%20specific%20sub-code%20at%20the%20model%20output%2C%0Aproviding%20natural%20sub-code%20interpretability.%20To%20capture%20subtle%20visual%0Adifferences%20among%20highly%20similar%20sub-categories%20%28e.g.%2C%20bird%20species%29%2C%20we%0Aincorporate%20language%20guidance%20to%20ensure%20that%20the%20learned%20hash%20codes%20are%0Adistinguishable%20within%20fine-grained%20object%20classes%20while%20maintaining%20semantic%0Aalignment.%20This%20approach%20allows%20us%20to%20develop%20hash%20codes%20that%20exhibit%0Asimilarity%20within%20families%20of%20species%20while%20remaining%20distinct%20from%20species%20in%0Aother%20families.%20Extensive%20experiments%20on%20four%20fine-grained%20image%20retrieval%0Abenchmarks%20demonstrate%20that%20ConceptHash%20outperforms%20previous%20methods%20by%20a%0Asignificant%20margin%2C%20offering%20unique%20sub-code%20interpretability%20as%20an%20additional%0Abenefit.%20Code%20at%3A%20https%3A//github.com/kamwoh/concepthash.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConceptHash%253A%2520Interpretable%2520Fine-Grained%2520Hashing%2520via%2520Concept%2520Discovery%26entry.906535625%3DKam%2520Woh%2520Ng%2520and%2520Xiatian%2520Zhu%2520and%2520Yi-Zhe%2520Song%2520and%2520Tao%2520Xiang%26entry.1292438233%3D%2520%2520Existing%2520fine-grained%2520hashing%2520methods%2520typically%2520lack%2520code%2520interpretability%2520as%250Athey%2520compute%2520hash%2520code%2520bits%2520holistically%2520using%2520both%2520global%2520and%2520local%2520features.%250ATo%2520address%2520this%2520limitation%252C%2520we%2520propose%2520ConceptHash%252C%2520a%2520novel%2520method%2520that%250Aachieves%2520sub-code%2520level%2520interpretability.%2520In%2520ConceptHash%252C%2520each%2520sub-code%250Acorresponds%2520to%2520a%2520human-understandable%2520concept%252C%2520such%2520as%2520an%2520object%2520part%252C%2520and%250Athese%2520concepts%2520are%2520automatically%2520discovered%2520without%2520human%2520annotations.%250ASpecifically%252C%2520we%2520leverage%2520a%2520Vision%2520Transformer%2520architecture%2520and%2520introduce%250Aconcept%2520tokens%2520as%2520visual%2520prompts%252C%2520along%2520with%2520image%2520patch%2520tokens%2520as%2520model%250Ainputs.%2520Each%2520concept%2520is%2520then%2520mapped%2520to%2520a%2520specific%2520sub-code%2520at%2520the%2520model%2520output%252C%250Aproviding%2520natural%2520sub-code%2520interpretability.%2520To%2520capture%2520subtle%2520visual%250Adifferences%2520among%2520highly%2520similar%2520sub-categories%2520%2528e.g.%252C%2520bird%2520species%2529%252C%2520we%250Aincorporate%2520language%2520guidance%2520to%2520ensure%2520that%2520the%2520learned%2520hash%2520codes%2520are%250Adistinguishable%2520within%2520fine-grained%2520object%2520classes%2520while%2520maintaining%2520semantic%250Aalignment.%2520This%2520approach%2520allows%2520us%2520to%2520develop%2520hash%2520codes%2520that%2520exhibit%250Asimilarity%2520within%2520families%2520of%2520species%2520while%2520remaining%2520distinct%2520from%2520species%2520in%250Aother%2520families.%2520Extensive%2520experiments%2520on%2520four%2520fine-grained%2520image%2520retrieval%250Abenchmarks%2520demonstrate%2520that%2520ConceptHash%2520outperforms%2520previous%2520methods%2520by%2520a%250Asignificant%2520margin%252C%2520offering%2520unique%2520sub-code%2520interpretability%2520as%2520an%2520additional%250Abenefit.%2520Code%2520at%253A%2520https%253A//github.com/kamwoh/concepthash.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConceptHash%3A%20Interpretable%20Fine-Grained%20Hashing%20via%20Concept%20Discovery&entry.906535625=Kam%20Woh%20Ng%20and%20Xiatian%20Zhu%20and%20Yi-Zhe%20Song%20and%20Tao%20Xiang&entry.1292438233=%20%20Existing%20fine-grained%20hashing%20methods%20typically%20lack%20code%20interpretability%20as%0Athey%20compute%20hash%20code%20bits%20holistically%20using%20both%20global%20and%20local%20features.%0ATo%20address%20this%20limitation%2C%20we%20propose%20ConceptHash%2C%20a%20novel%20method%20that%0Aachieves%20sub-code%20level%20interpretability.%20In%20ConceptHash%2C%20each%20sub-code%0Acorresponds%20to%20a%20human-understandable%20concept%2C%20such%20as%20an%20object%20part%2C%20and%0Athese%20concepts%20are%20automatically%20discovered%20without%20human%20annotations.%0ASpecifically%2C%20we%20leverage%20a%20Vision%20Transformer%20architecture%20and%20introduce%0Aconcept%20tokens%20as%20visual%20prompts%2C%20along%20with%20image%20patch%20tokens%20as%20model%0Ainputs.%20Each%20concept%20is%20then%20mapped%20to%20a%20specific%20sub-code%20at%20the%20model%20output%2C%0Aproviding%20natural%20sub-code%20interpretability.%20To%20capture%20subtle%20visual%0Adifferences%20among%20highly%20similar%20sub-categories%20%28e.g.%2C%20bird%20species%29%2C%20we%0Aincorporate%20language%20guidance%20to%20ensure%20that%20the%20learned%20hash%20codes%20are%0Adistinguishable%20within%20fine-grained%20object%20classes%20while%20maintaining%20semantic%0Aalignment.%20This%20approach%20allows%20us%20to%20develop%20hash%20codes%20that%20exhibit%0Asimilarity%20within%20families%20of%20species%20while%20remaining%20distinct%20from%20species%20in%0Aother%20families.%20Extensive%20experiments%20on%20four%20fine-grained%20image%20retrieval%0Abenchmarks%20demonstrate%20that%20ConceptHash%20outperforms%20previous%20methods%20by%20a%0Asignificant%20margin%2C%20offering%20unique%20sub-code%20interpretability%20as%20an%20additional%0Abenefit.%20Code%20at%3A%20https%3A//github.com/kamwoh/concepthash.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08457v1&entry.124074799=Read"},
{"title": "Rankability-enhanced Revenue Uplift Modeling Framework for Online\n  Marketing", "author": "Bowei He and Yunpeng Weng and Xing Tang and Ziqiang Cui and Zexu Sun and Liang Chen and Xiuqiang He and Chen Ma", "abstract": "  Uplift modeling has been widely employed in online marketing by predicting\nthe response difference between the treatment and control groups, so as to\nidentify the sensitive individuals toward interventions like coupons or\ndiscounts. Compared with traditional \\textit{conversion uplift modeling},\n\\textit{revenue uplift modeling} exhibits higher potential due to its direct\nconnection with the corporate income. However, previous works can hardly handle\nthe continuous long-tail response distribution in revenue uplift modeling.\nMoreover, they have neglected to optimize the uplift ranking among different\nindividuals, which is actually the core of uplift modeling. To address such\nissues, in this paper, we first utilize the zero-inflated lognormal (ZILN) loss\nto regress the responses and customize the corresponding modeling network,\nwhich can be adapted to different existing uplift models. Then, we study the\nranking-related uplift modeling error from the theoretical perspective and\npropose two tighter error bounds as the additional loss terms to the\nconventional response regression loss. Finally, we directly model the uplift\nranking error for the entire population with a listwise uplift ranking loss.\nThe experiment results on offline public and industrial datasets validate the\neffectiveness of our method for revenue uplift modeling. Furthermore, we\nconduct large-scale experiments on a prominent online fintech marketing\nplatform, Tencent FiT, which further demonstrates the superiority of our method\nin real-world applications.\n", "link": "http://arxiv.org/abs/2405.15301v2", "date": "2024-06-12", "relevancy": 2.3238, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4737}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4624}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rankability-enhanced%20Revenue%20Uplift%20Modeling%20Framework%20for%20Online%0A%20%20Marketing&body=Title%3A%20Rankability-enhanced%20Revenue%20Uplift%20Modeling%20Framework%20for%20Online%0A%20%20Marketing%0AAuthor%3A%20Bowei%20He%20and%20Yunpeng%20Weng%20and%20Xing%20Tang%20and%20Ziqiang%20Cui%20and%20Zexu%20Sun%20and%20Liang%20Chen%20and%20Xiuqiang%20He%20and%20Chen%20Ma%0AAbstract%3A%20%20%20Uplift%20modeling%20has%20been%20widely%20employed%20in%20online%20marketing%20by%20predicting%0Athe%20response%20difference%20between%20the%20treatment%20and%20control%20groups%2C%20so%20as%20to%0Aidentify%20the%20sensitive%20individuals%20toward%20interventions%20like%20coupons%20or%0Adiscounts.%20Compared%20with%20traditional%20%5Ctextit%7Bconversion%20uplift%20modeling%7D%2C%0A%5Ctextit%7Brevenue%20uplift%20modeling%7D%20exhibits%20higher%20potential%20due%20to%20its%20direct%0Aconnection%20with%20the%20corporate%20income.%20However%2C%20previous%20works%20can%20hardly%20handle%0Athe%20continuous%20long-tail%20response%20distribution%20in%20revenue%20uplift%20modeling.%0AMoreover%2C%20they%20have%20neglected%20to%20optimize%20the%20uplift%20ranking%20among%20different%0Aindividuals%2C%20which%20is%20actually%20the%20core%20of%20uplift%20modeling.%20To%20address%20such%0Aissues%2C%20in%20this%20paper%2C%20we%20first%20utilize%20the%20zero-inflated%20lognormal%20%28ZILN%29%20loss%0Ato%20regress%20the%20responses%20and%20customize%20the%20corresponding%20modeling%20network%2C%0Awhich%20can%20be%20adapted%20to%20different%20existing%20uplift%20models.%20Then%2C%20we%20study%20the%0Aranking-related%20uplift%20modeling%20error%20from%20the%20theoretical%20perspective%20and%0Apropose%20two%20tighter%20error%20bounds%20as%20the%20additional%20loss%20terms%20to%20the%0Aconventional%20response%20regression%20loss.%20Finally%2C%20we%20directly%20model%20the%20uplift%0Aranking%20error%20for%20the%20entire%20population%20with%20a%20listwise%20uplift%20ranking%20loss.%0AThe%20experiment%20results%20on%20offline%20public%20and%20industrial%20datasets%20validate%20the%0Aeffectiveness%20of%20our%20method%20for%20revenue%20uplift%20modeling.%20Furthermore%2C%20we%0Aconduct%20large-scale%20experiments%20on%20a%20prominent%20online%20fintech%20marketing%0Aplatform%2C%20Tencent%20FiT%2C%20which%20further%20demonstrates%20the%20superiority%20of%20our%20method%0Ain%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15301v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRankability-enhanced%2520Revenue%2520Uplift%2520Modeling%2520Framework%2520for%2520Online%250A%2520%2520Marketing%26entry.906535625%3DBowei%2520He%2520and%2520Yunpeng%2520Weng%2520and%2520Xing%2520Tang%2520and%2520Ziqiang%2520Cui%2520and%2520Zexu%2520Sun%2520and%2520Liang%2520Chen%2520and%2520Xiuqiang%2520He%2520and%2520Chen%2520Ma%26entry.1292438233%3D%2520%2520Uplift%2520modeling%2520has%2520been%2520widely%2520employed%2520in%2520online%2520marketing%2520by%2520predicting%250Athe%2520response%2520difference%2520between%2520the%2520treatment%2520and%2520control%2520groups%252C%2520so%2520as%2520to%250Aidentify%2520the%2520sensitive%2520individuals%2520toward%2520interventions%2520like%2520coupons%2520or%250Adiscounts.%2520Compared%2520with%2520traditional%2520%255Ctextit%257Bconversion%2520uplift%2520modeling%257D%252C%250A%255Ctextit%257Brevenue%2520uplift%2520modeling%257D%2520exhibits%2520higher%2520potential%2520due%2520to%2520its%2520direct%250Aconnection%2520with%2520the%2520corporate%2520income.%2520However%252C%2520previous%2520works%2520can%2520hardly%2520handle%250Athe%2520continuous%2520long-tail%2520response%2520distribution%2520in%2520revenue%2520uplift%2520modeling.%250AMoreover%252C%2520they%2520have%2520neglected%2520to%2520optimize%2520the%2520uplift%2520ranking%2520among%2520different%250Aindividuals%252C%2520which%2520is%2520actually%2520the%2520core%2520of%2520uplift%2520modeling.%2520To%2520address%2520such%250Aissues%252C%2520in%2520this%2520paper%252C%2520we%2520first%2520utilize%2520the%2520zero-inflated%2520lognormal%2520%2528ZILN%2529%2520loss%250Ato%2520regress%2520the%2520responses%2520and%2520customize%2520the%2520corresponding%2520modeling%2520network%252C%250Awhich%2520can%2520be%2520adapted%2520to%2520different%2520existing%2520uplift%2520models.%2520Then%252C%2520we%2520study%2520the%250Aranking-related%2520uplift%2520modeling%2520error%2520from%2520the%2520theoretical%2520perspective%2520and%250Apropose%2520two%2520tighter%2520error%2520bounds%2520as%2520the%2520additional%2520loss%2520terms%2520to%2520the%250Aconventional%2520response%2520regression%2520loss.%2520Finally%252C%2520we%2520directly%2520model%2520the%2520uplift%250Aranking%2520error%2520for%2520the%2520entire%2520population%2520with%2520a%2520listwise%2520uplift%2520ranking%2520loss.%250AThe%2520experiment%2520results%2520on%2520offline%2520public%2520and%2520industrial%2520datasets%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520method%2520for%2520revenue%2520uplift%2520modeling.%2520Furthermore%252C%2520we%250Aconduct%2520large-scale%2520experiments%2520on%2520a%2520prominent%2520online%2520fintech%2520marketing%250Aplatform%252C%2520Tencent%2520FiT%252C%2520which%2520further%2520demonstrates%2520the%2520superiority%2520of%2520our%2520method%250Ain%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15301v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rankability-enhanced%20Revenue%20Uplift%20Modeling%20Framework%20for%20Online%0A%20%20Marketing&entry.906535625=Bowei%20He%20and%20Yunpeng%20Weng%20and%20Xing%20Tang%20and%20Ziqiang%20Cui%20and%20Zexu%20Sun%20and%20Liang%20Chen%20and%20Xiuqiang%20He%20and%20Chen%20Ma&entry.1292438233=%20%20Uplift%20modeling%20has%20been%20widely%20employed%20in%20online%20marketing%20by%20predicting%0Athe%20response%20difference%20between%20the%20treatment%20and%20control%20groups%2C%20so%20as%20to%0Aidentify%20the%20sensitive%20individuals%20toward%20interventions%20like%20coupons%20or%0Adiscounts.%20Compared%20with%20traditional%20%5Ctextit%7Bconversion%20uplift%20modeling%7D%2C%0A%5Ctextit%7Brevenue%20uplift%20modeling%7D%20exhibits%20higher%20potential%20due%20to%20its%20direct%0Aconnection%20with%20the%20corporate%20income.%20However%2C%20previous%20works%20can%20hardly%20handle%0Athe%20continuous%20long-tail%20response%20distribution%20in%20revenue%20uplift%20modeling.%0AMoreover%2C%20they%20have%20neglected%20to%20optimize%20the%20uplift%20ranking%20among%20different%0Aindividuals%2C%20which%20is%20actually%20the%20core%20of%20uplift%20modeling.%20To%20address%20such%0Aissues%2C%20in%20this%20paper%2C%20we%20first%20utilize%20the%20zero-inflated%20lognormal%20%28ZILN%29%20loss%0Ato%20regress%20the%20responses%20and%20customize%20the%20corresponding%20modeling%20network%2C%0Awhich%20can%20be%20adapted%20to%20different%20existing%20uplift%20models.%20Then%2C%20we%20study%20the%0Aranking-related%20uplift%20modeling%20error%20from%20the%20theoretical%20perspective%20and%0Apropose%20two%20tighter%20error%20bounds%20as%20the%20additional%20loss%20terms%20to%20the%0Aconventional%20response%20regression%20loss.%20Finally%2C%20we%20directly%20model%20the%20uplift%0Aranking%20error%20for%20the%20entire%20population%20with%20a%20listwise%20uplift%20ranking%20loss.%0AThe%20experiment%20results%20on%20offline%20public%20and%20industrial%20datasets%20validate%20the%0Aeffectiveness%20of%20our%20method%20for%20revenue%20uplift%20modeling.%20Furthermore%2C%20we%0Aconduct%20large-scale%20experiments%20on%20a%20prominent%20online%20fintech%20marketing%0Aplatform%2C%20Tencent%20FiT%2C%20which%20further%20demonstrates%20the%20superiority%20of%20our%20method%0Ain%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15301v2&entry.124074799=Read"},
{"title": "Outdoor Scene Extrapolation with Hierarchical Generative Cellular\n  Automata", "author": "Dongsu Zhang and Francis Williams and Zan Gojcic and Karsten Kreis and Sanja Fidler and Young Min Kim and Amlan Kar", "abstract": "  We aim to generate fine-grained 3D geometry from large-scale sparse LiDAR\nscans, abundantly captured by autonomous vehicles (AV). Contrary to prior work\non AV scene completion, we aim to extrapolate fine geometry from unlabeled and\nbeyond spatial limits of LiDAR scans, taking a step towards generating\nrealistic, high-resolution simulation-ready 3D street environments. We propose\nhierarchical Generative Cellular Automata (hGCA), a spatially scalable\nconditional 3D generative model, which grows geometry recursively with local\nkernels following, in a coarse-to-fine manner, equipped with a light-weight\nplanner to induce global consistency. Experiments on synthetic scenes show that\nhGCA generates plausible scene geometry with higher fidelity and completeness\ncompared to state-of-the-art baselines. Our model generalizes strongly from\nsim-to-real, qualitatively outperforming baselines on the Waymo-open dataset.\nWe also show anecdotal evidence of the ability to create novel objects from\nreal-world geometric cues even when trained on limited synthetic content. More\nresults and details can be found on\nhttps://research.nvidia.com/labs/toronto-ai/hGCA/.\n", "link": "http://arxiv.org/abs/2406.08292v1", "date": "2024-06-12", "relevancy": 2.3094, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5897}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5784}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Outdoor%20Scene%20Extrapolation%20with%20Hierarchical%20Generative%20Cellular%0A%20%20Automata&body=Title%3A%20Outdoor%20Scene%20Extrapolation%20with%20Hierarchical%20Generative%20Cellular%0A%20%20Automata%0AAuthor%3A%20Dongsu%20Zhang%20and%20Francis%20Williams%20and%20Zan%20Gojcic%20and%20Karsten%20Kreis%20and%20Sanja%20Fidler%20and%20Young%20Min%20Kim%20and%20Amlan%20Kar%0AAbstract%3A%20%20%20We%20aim%20to%20generate%20fine-grained%203D%20geometry%20from%20large-scale%20sparse%20LiDAR%0Ascans%2C%20abundantly%20captured%20by%20autonomous%20vehicles%20%28AV%29.%20Contrary%20to%20prior%20work%0Aon%20AV%20scene%20completion%2C%20we%20aim%20to%20extrapolate%20fine%20geometry%20from%20unlabeled%20and%0Abeyond%20spatial%20limits%20of%20LiDAR%20scans%2C%20taking%20a%20step%20towards%20generating%0Arealistic%2C%20high-resolution%20simulation-ready%203D%20street%20environments.%20We%20propose%0Ahierarchical%20Generative%20Cellular%20Automata%20%28hGCA%29%2C%20a%20spatially%20scalable%0Aconditional%203D%20generative%20model%2C%20which%20grows%20geometry%20recursively%20with%20local%0Akernels%20following%2C%20in%20a%20coarse-to-fine%20manner%2C%20equipped%20with%20a%20light-weight%0Aplanner%20to%20induce%20global%20consistency.%20Experiments%20on%20synthetic%20scenes%20show%20that%0AhGCA%20generates%20plausible%20scene%20geometry%20with%20higher%20fidelity%20and%20completeness%0Acompared%20to%20state-of-the-art%20baselines.%20Our%20model%20generalizes%20strongly%20from%0Asim-to-real%2C%20qualitatively%20outperforming%20baselines%20on%20the%20Waymo-open%20dataset.%0AWe%20also%20show%20anecdotal%20evidence%20of%20the%20ability%20to%20create%20novel%20objects%20from%0Areal-world%20geometric%20cues%20even%20when%20trained%20on%20limited%20synthetic%20content.%20More%0Aresults%20and%20details%20can%20be%20found%20on%0Ahttps%3A//research.nvidia.com/labs/toronto-ai/hGCA/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOutdoor%2520Scene%2520Extrapolation%2520with%2520Hierarchical%2520Generative%2520Cellular%250A%2520%2520Automata%26entry.906535625%3DDongsu%2520Zhang%2520and%2520Francis%2520Williams%2520and%2520Zan%2520Gojcic%2520and%2520Karsten%2520Kreis%2520and%2520Sanja%2520Fidler%2520and%2520Young%2520Min%2520Kim%2520and%2520Amlan%2520Kar%26entry.1292438233%3D%2520%2520We%2520aim%2520to%2520generate%2520fine-grained%25203D%2520geometry%2520from%2520large-scale%2520sparse%2520LiDAR%250Ascans%252C%2520abundantly%2520captured%2520by%2520autonomous%2520vehicles%2520%2528AV%2529.%2520Contrary%2520to%2520prior%2520work%250Aon%2520AV%2520scene%2520completion%252C%2520we%2520aim%2520to%2520extrapolate%2520fine%2520geometry%2520from%2520unlabeled%2520and%250Abeyond%2520spatial%2520limits%2520of%2520LiDAR%2520scans%252C%2520taking%2520a%2520step%2520towards%2520generating%250Arealistic%252C%2520high-resolution%2520simulation-ready%25203D%2520street%2520environments.%2520We%2520propose%250Ahierarchical%2520Generative%2520Cellular%2520Automata%2520%2528hGCA%2529%252C%2520a%2520spatially%2520scalable%250Aconditional%25203D%2520generative%2520model%252C%2520which%2520grows%2520geometry%2520recursively%2520with%2520local%250Akernels%2520following%252C%2520in%2520a%2520coarse-to-fine%2520manner%252C%2520equipped%2520with%2520a%2520light-weight%250Aplanner%2520to%2520induce%2520global%2520consistency.%2520Experiments%2520on%2520synthetic%2520scenes%2520show%2520that%250AhGCA%2520generates%2520plausible%2520scene%2520geometry%2520with%2520higher%2520fidelity%2520and%2520completeness%250Acompared%2520to%2520state-of-the-art%2520baselines.%2520Our%2520model%2520generalizes%2520strongly%2520from%250Asim-to-real%252C%2520qualitatively%2520outperforming%2520baselines%2520on%2520the%2520Waymo-open%2520dataset.%250AWe%2520also%2520show%2520anecdotal%2520evidence%2520of%2520the%2520ability%2520to%2520create%2520novel%2520objects%2520from%250Areal-world%2520geometric%2520cues%2520even%2520when%2520trained%2520on%2520limited%2520synthetic%2520content.%2520More%250Aresults%2520and%2520details%2520can%2520be%2520found%2520on%250Ahttps%253A//research.nvidia.com/labs/toronto-ai/hGCA/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Outdoor%20Scene%20Extrapolation%20with%20Hierarchical%20Generative%20Cellular%0A%20%20Automata&entry.906535625=Dongsu%20Zhang%20and%20Francis%20Williams%20and%20Zan%20Gojcic%20and%20Karsten%20Kreis%20and%20Sanja%20Fidler%20and%20Young%20Min%20Kim%20and%20Amlan%20Kar&entry.1292438233=%20%20We%20aim%20to%20generate%20fine-grained%203D%20geometry%20from%20large-scale%20sparse%20LiDAR%0Ascans%2C%20abundantly%20captured%20by%20autonomous%20vehicles%20%28AV%29.%20Contrary%20to%20prior%20work%0Aon%20AV%20scene%20completion%2C%20we%20aim%20to%20extrapolate%20fine%20geometry%20from%20unlabeled%20and%0Abeyond%20spatial%20limits%20of%20LiDAR%20scans%2C%20taking%20a%20step%20towards%20generating%0Arealistic%2C%20high-resolution%20simulation-ready%203D%20street%20environments.%20We%20propose%0Ahierarchical%20Generative%20Cellular%20Automata%20%28hGCA%29%2C%20a%20spatially%20scalable%0Aconditional%203D%20generative%20model%2C%20which%20grows%20geometry%20recursively%20with%20local%0Akernels%20following%2C%20in%20a%20coarse-to-fine%20manner%2C%20equipped%20with%20a%20light-weight%0Aplanner%20to%20induce%20global%20consistency.%20Experiments%20on%20synthetic%20scenes%20show%20that%0AhGCA%20generates%20plausible%20scene%20geometry%20with%20higher%20fidelity%20and%20completeness%0Acompared%20to%20state-of-the-art%20baselines.%20Our%20model%20generalizes%20strongly%20from%0Asim-to-real%2C%20qualitatively%20outperforming%20baselines%20on%20the%20Waymo-open%20dataset.%0AWe%20also%20show%20anecdotal%20evidence%20of%20the%20ability%20to%20create%20novel%20objects%20from%0Areal-world%20geometric%20cues%20even%20when%20trained%20on%20limited%20synthetic%20content.%20More%0Aresults%20and%20details%20can%20be%20found%20on%0Ahttps%3A//research.nvidia.com/labs/toronto-ai/hGCA/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08292v1&entry.124074799=Read"},
{"title": "UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object\n  Detection with Sparse LiDAR and Large Domain Gaps", "author": "Maciej K Wozniak and Mattias Hansson and Marko Thiel and Patric Jensfelt", "abstract": "  In this study, we address a gap in existing unsupervised domain adaptation\napproaches on LiDAR-based 3D object detection, which have predominantly\nconcentrated on adapting between established, high-density autonomous driving\ndatasets. We focus on sparser point clouds, capturing scenarios from different\nperspectives: not just from vehicles on the road but also from mobile robots on\nsidewalks, which encounter significantly different environmental conditions and\nsensor configurations. We introduce Unsupervised Adversarial Domain Adaptation\nfor 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source\nmodels or teacher-student architectures. Instead, it uses an adversarial\napproach to directly learn domain-invariant features. We demonstrate its\nefficacy in various adaptation scenarios, showing significant improvements in\nboth self-driving car and mobile robot domains. Our code is open-source and\nwill be available soon.\n", "link": "http://arxiv.org/abs/2403.17633v3", "date": "2024-06-12", "relevancy": 2.3052, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6063}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5821}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5585}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UADA3D%3A%20Unsupervised%20Adversarial%20Domain%20Adaptation%20for%203D%20Object%0A%20%20Detection%20with%20Sparse%20LiDAR%20and%20Large%20Domain%20Gaps&body=Title%3A%20UADA3D%3A%20Unsupervised%20Adversarial%20Domain%20Adaptation%20for%203D%20Object%0A%20%20Detection%20with%20Sparse%20LiDAR%20and%20Large%20Domain%20Gaps%0AAuthor%3A%20Maciej%20K%20Wozniak%20and%20Mattias%20Hansson%20and%20Marko%20Thiel%20and%20Patric%20Jensfelt%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20address%20a%20gap%20in%20existing%20unsupervised%20domain%20adaptation%0Aapproaches%20on%20LiDAR-based%203D%20object%20detection%2C%20which%20have%20predominantly%0Aconcentrated%20on%20adapting%20between%20established%2C%20high-density%20autonomous%20driving%0Adatasets.%20We%20focus%20on%20sparser%20point%20clouds%2C%20capturing%20scenarios%20from%20different%0Aperspectives%3A%20not%20just%20from%20vehicles%20on%20the%20road%20but%20also%20from%20mobile%20robots%20on%0Asidewalks%2C%20which%20encounter%20significantly%20different%20environmental%20conditions%20and%0Asensor%20configurations.%20We%20introduce%20Unsupervised%20Adversarial%20Domain%20Adaptation%0Afor%203D%20Object%20Detection%20%28UADA3D%29.%20UADA3D%20does%20not%20depend%20on%20pre-trained%20source%0Amodels%20or%20teacher-student%20architectures.%20Instead%2C%20it%20uses%20an%20adversarial%0Aapproach%20to%20directly%20learn%20domain-invariant%20features.%20We%20demonstrate%20its%0Aefficacy%20in%20various%20adaptation%20scenarios%2C%20showing%20significant%20improvements%20in%0Aboth%20self-driving%20car%20and%20mobile%20robot%20domains.%20Our%20code%20is%20open-source%20and%0Awill%20be%20available%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17633v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUADA3D%253A%2520Unsupervised%2520Adversarial%2520Domain%2520Adaptation%2520for%25203D%2520Object%250A%2520%2520Detection%2520with%2520Sparse%2520LiDAR%2520and%2520Large%2520Domain%2520Gaps%26entry.906535625%3DMaciej%2520K%2520Wozniak%2520and%2520Mattias%2520Hansson%2520and%2520Marko%2520Thiel%2520and%2520Patric%2520Jensfelt%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520we%2520address%2520a%2520gap%2520in%2520existing%2520unsupervised%2520domain%2520adaptation%250Aapproaches%2520on%2520LiDAR-based%25203D%2520object%2520detection%252C%2520which%2520have%2520predominantly%250Aconcentrated%2520on%2520adapting%2520between%2520established%252C%2520high-density%2520autonomous%2520driving%250Adatasets.%2520We%2520focus%2520on%2520sparser%2520point%2520clouds%252C%2520capturing%2520scenarios%2520from%2520different%250Aperspectives%253A%2520not%2520just%2520from%2520vehicles%2520on%2520the%2520road%2520but%2520also%2520from%2520mobile%2520robots%2520on%250Asidewalks%252C%2520which%2520encounter%2520significantly%2520different%2520environmental%2520conditions%2520and%250Asensor%2520configurations.%2520We%2520introduce%2520Unsupervised%2520Adversarial%2520Domain%2520Adaptation%250Afor%25203D%2520Object%2520Detection%2520%2528UADA3D%2529.%2520UADA3D%2520does%2520not%2520depend%2520on%2520pre-trained%2520source%250Amodels%2520or%2520teacher-student%2520architectures.%2520Instead%252C%2520it%2520uses%2520an%2520adversarial%250Aapproach%2520to%2520directly%2520learn%2520domain-invariant%2520features.%2520We%2520demonstrate%2520its%250Aefficacy%2520in%2520various%2520adaptation%2520scenarios%252C%2520showing%2520significant%2520improvements%2520in%250Aboth%2520self-driving%2520car%2520and%2520mobile%2520robot%2520domains.%2520Our%2520code%2520is%2520open-source%2520and%250Awill%2520be%2520available%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17633v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UADA3D%3A%20Unsupervised%20Adversarial%20Domain%20Adaptation%20for%203D%20Object%0A%20%20Detection%20with%20Sparse%20LiDAR%20and%20Large%20Domain%20Gaps&entry.906535625=Maciej%20K%20Wozniak%20and%20Mattias%20Hansson%20and%20Marko%20Thiel%20and%20Patric%20Jensfelt&entry.1292438233=%20%20In%20this%20study%2C%20we%20address%20a%20gap%20in%20existing%20unsupervised%20domain%20adaptation%0Aapproaches%20on%20LiDAR-based%203D%20object%20detection%2C%20which%20have%20predominantly%0Aconcentrated%20on%20adapting%20between%20established%2C%20high-density%20autonomous%20driving%0Adatasets.%20We%20focus%20on%20sparser%20point%20clouds%2C%20capturing%20scenarios%20from%20different%0Aperspectives%3A%20not%20just%20from%20vehicles%20on%20the%20road%20but%20also%20from%20mobile%20robots%20on%0Asidewalks%2C%20which%20encounter%20significantly%20different%20environmental%20conditions%20and%0Asensor%20configurations.%20We%20introduce%20Unsupervised%20Adversarial%20Domain%20Adaptation%0Afor%203D%20Object%20Detection%20%28UADA3D%29.%20UADA3D%20does%20not%20depend%20on%20pre-trained%20source%0Amodels%20or%20teacher-student%20architectures.%20Instead%2C%20it%20uses%20an%20adversarial%0Aapproach%20to%20directly%20learn%20domain-invariant%20features.%20We%20demonstrate%20its%0Aefficacy%20in%20various%20adaptation%20scenarios%2C%20showing%20significant%20improvements%20in%0Aboth%20self-driving%20car%20and%20mobile%20robot%20domains.%20Our%20code%20is%20open-source%20and%0Awill%20be%20available%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17633v3&entry.124074799=Read"},
{"title": "Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for\n  Chinese Mental Health Text Analysis", "author": "Wei Zhai and Hongzhi Qi and Qing Zhao and Jianqiang Li and Ziqi Wang and Han Wang and Bing Xiang Yang and Guanghui Fu", "abstract": "  In the current environment, psychological issues are prevalent and\nwidespread, with social media serving as a key outlet for individuals to share\ntheir feelings. This results in the generation of vast quantities of data\ndaily, where negative emotions have the potential to precipitate crisis\nsituations. There is a recognized need for models capable of efficient\nanalysis. While pre-trained language models have demonstrated their\neffectiveness broadly, there's a noticeable gap in pre-trained models tailored\nfor specialized domains like psychology. To address this, we have collected a\nhuge dataset from Chinese social media platforms and enriched it with publicly\navailable datasets to create a comprehensive database encompassing 3.36 million\ntext entries. To enhance the model's applicability to psychological text\nanalysis, we integrated psychological lexicons into the pre-training masking\nmechanism. Building on an existing Chinese language model, we performed\nadaptive training to develop a model specialized for the psychological domain.\nWe evaluated our model's performance across six public datasets, where it\ndemonstrated improvements compared to eight other models. Additionally, in the\nqualitative comparison experiment, our model provided psychologically relevant\npredictions given the masked sentences. Due to concerns regarding data privacy,\nthe dataset will not be made publicly available. However, we have made the\npre-trained models and codes publicly accessible to the community via:\nhttps://github.com/zwzzzQAQ/Chinese-MentalBERT.\n", "link": "http://arxiv.org/abs/2402.09151v2", "date": "2024-06-12", "relevancy": 2.2986, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.471}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4586}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chinese%20MentalBERT%3A%20Domain-Adaptive%20Pre-training%20on%20Social%20Media%20for%0A%20%20Chinese%20Mental%20Health%20Text%20Analysis&body=Title%3A%20Chinese%20MentalBERT%3A%20Domain-Adaptive%20Pre-training%20on%20Social%20Media%20for%0A%20%20Chinese%20Mental%20Health%20Text%20Analysis%0AAuthor%3A%20Wei%20Zhai%20and%20Hongzhi%20Qi%20and%20Qing%20Zhao%20and%20Jianqiang%20Li%20and%20Ziqi%20Wang%20and%20Han%20Wang%20and%20Bing%20Xiang%20Yang%20and%20Guanghui%20Fu%0AAbstract%3A%20%20%20In%20the%20current%20environment%2C%20psychological%20issues%20are%20prevalent%20and%0Awidespread%2C%20with%20social%20media%20serving%20as%20a%20key%20outlet%20for%20individuals%20to%20share%0Atheir%20feelings.%20This%20results%20in%20the%20generation%20of%20vast%20quantities%20of%20data%0Adaily%2C%20where%20negative%20emotions%20have%20the%20potential%20to%20precipitate%20crisis%0Asituations.%20There%20is%20a%20recognized%20need%20for%20models%20capable%20of%20efficient%0Aanalysis.%20While%20pre-trained%20language%20models%20have%20demonstrated%20their%0Aeffectiveness%20broadly%2C%20there%27s%20a%20noticeable%20gap%20in%20pre-trained%20models%20tailored%0Afor%20specialized%20domains%20like%20psychology.%20To%20address%20this%2C%20we%20have%20collected%20a%0Ahuge%20dataset%20from%20Chinese%20social%20media%20platforms%20and%20enriched%20it%20with%20publicly%0Aavailable%20datasets%20to%20create%20a%20comprehensive%20database%20encompassing%203.36%20million%0Atext%20entries.%20To%20enhance%20the%20model%27s%20applicability%20to%20psychological%20text%0Aanalysis%2C%20we%20integrated%20psychological%20lexicons%20into%20the%20pre-training%20masking%0Amechanism.%20Building%20on%20an%20existing%20Chinese%20language%20model%2C%20we%20performed%0Aadaptive%20training%20to%20develop%20a%20model%20specialized%20for%20the%20psychological%20domain.%0AWe%20evaluated%20our%20model%27s%20performance%20across%20six%20public%20datasets%2C%20where%20it%0Ademonstrated%20improvements%20compared%20to%20eight%20other%20models.%20Additionally%2C%20in%20the%0Aqualitative%20comparison%20experiment%2C%20our%20model%20provided%20psychologically%20relevant%0Apredictions%20given%20the%20masked%20sentences.%20Due%20to%20concerns%20regarding%20data%20privacy%2C%0Athe%20dataset%20will%20not%20be%20made%20publicly%20available.%20However%2C%20we%20have%20made%20the%0Apre-trained%20models%20and%20codes%20publicly%20accessible%20to%20the%20community%20via%3A%0Ahttps%3A//github.com/zwzzzQAQ/Chinese-MentalBERT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09151v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChinese%2520MentalBERT%253A%2520Domain-Adaptive%2520Pre-training%2520on%2520Social%2520Media%2520for%250A%2520%2520Chinese%2520Mental%2520Health%2520Text%2520Analysis%26entry.906535625%3DWei%2520Zhai%2520and%2520Hongzhi%2520Qi%2520and%2520Qing%2520Zhao%2520and%2520Jianqiang%2520Li%2520and%2520Ziqi%2520Wang%2520and%2520Han%2520Wang%2520and%2520Bing%2520Xiang%2520Yang%2520and%2520Guanghui%2520Fu%26entry.1292438233%3D%2520%2520In%2520the%2520current%2520environment%252C%2520psychological%2520issues%2520are%2520prevalent%2520and%250Awidespread%252C%2520with%2520social%2520media%2520serving%2520as%2520a%2520key%2520outlet%2520for%2520individuals%2520to%2520share%250Atheir%2520feelings.%2520This%2520results%2520in%2520the%2520generation%2520of%2520vast%2520quantities%2520of%2520data%250Adaily%252C%2520where%2520negative%2520emotions%2520have%2520the%2520potential%2520to%2520precipitate%2520crisis%250Asituations.%2520There%2520is%2520a%2520recognized%2520need%2520for%2520models%2520capable%2520of%2520efficient%250Aanalysis.%2520While%2520pre-trained%2520language%2520models%2520have%2520demonstrated%2520their%250Aeffectiveness%2520broadly%252C%2520there%2527s%2520a%2520noticeable%2520gap%2520in%2520pre-trained%2520models%2520tailored%250Afor%2520specialized%2520domains%2520like%2520psychology.%2520To%2520address%2520this%252C%2520we%2520have%2520collected%2520a%250Ahuge%2520dataset%2520from%2520Chinese%2520social%2520media%2520platforms%2520and%2520enriched%2520it%2520with%2520publicly%250Aavailable%2520datasets%2520to%2520create%2520a%2520comprehensive%2520database%2520encompassing%25203.36%2520million%250Atext%2520entries.%2520To%2520enhance%2520the%2520model%2527s%2520applicability%2520to%2520psychological%2520text%250Aanalysis%252C%2520we%2520integrated%2520psychological%2520lexicons%2520into%2520the%2520pre-training%2520masking%250Amechanism.%2520Building%2520on%2520an%2520existing%2520Chinese%2520language%2520model%252C%2520we%2520performed%250Aadaptive%2520training%2520to%2520develop%2520a%2520model%2520specialized%2520for%2520the%2520psychological%2520domain.%250AWe%2520evaluated%2520our%2520model%2527s%2520performance%2520across%2520six%2520public%2520datasets%252C%2520where%2520it%250Ademonstrated%2520improvements%2520compared%2520to%2520eight%2520other%2520models.%2520Additionally%252C%2520in%2520the%250Aqualitative%2520comparison%2520experiment%252C%2520our%2520model%2520provided%2520psychologically%2520relevant%250Apredictions%2520given%2520the%2520masked%2520sentences.%2520Due%2520to%2520concerns%2520regarding%2520data%2520privacy%252C%250Athe%2520dataset%2520will%2520not%2520be%2520made%2520publicly%2520available.%2520However%252C%2520we%2520have%2520made%2520the%250Apre-trained%2520models%2520and%2520codes%2520publicly%2520accessible%2520to%2520the%2520community%2520via%253A%250Ahttps%253A//github.com/zwzzzQAQ/Chinese-MentalBERT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09151v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chinese%20MentalBERT%3A%20Domain-Adaptive%20Pre-training%20on%20Social%20Media%20for%0A%20%20Chinese%20Mental%20Health%20Text%20Analysis&entry.906535625=Wei%20Zhai%20and%20Hongzhi%20Qi%20and%20Qing%20Zhao%20and%20Jianqiang%20Li%20and%20Ziqi%20Wang%20and%20Han%20Wang%20and%20Bing%20Xiang%20Yang%20and%20Guanghui%20Fu&entry.1292438233=%20%20In%20the%20current%20environment%2C%20psychological%20issues%20are%20prevalent%20and%0Awidespread%2C%20with%20social%20media%20serving%20as%20a%20key%20outlet%20for%20individuals%20to%20share%0Atheir%20feelings.%20This%20results%20in%20the%20generation%20of%20vast%20quantities%20of%20data%0Adaily%2C%20where%20negative%20emotions%20have%20the%20potential%20to%20precipitate%20crisis%0Asituations.%20There%20is%20a%20recognized%20need%20for%20models%20capable%20of%20efficient%0Aanalysis.%20While%20pre-trained%20language%20models%20have%20demonstrated%20their%0Aeffectiveness%20broadly%2C%20there%27s%20a%20noticeable%20gap%20in%20pre-trained%20models%20tailored%0Afor%20specialized%20domains%20like%20psychology.%20To%20address%20this%2C%20we%20have%20collected%20a%0Ahuge%20dataset%20from%20Chinese%20social%20media%20platforms%20and%20enriched%20it%20with%20publicly%0Aavailable%20datasets%20to%20create%20a%20comprehensive%20database%20encompassing%203.36%20million%0Atext%20entries.%20To%20enhance%20the%20model%27s%20applicability%20to%20psychological%20text%0Aanalysis%2C%20we%20integrated%20psychological%20lexicons%20into%20the%20pre-training%20masking%0Amechanism.%20Building%20on%20an%20existing%20Chinese%20language%20model%2C%20we%20performed%0Aadaptive%20training%20to%20develop%20a%20model%20specialized%20for%20the%20psychological%20domain.%0AWe%20evaluated%20our%20model%27s%20performance%20across%20six%20public%20datasets%2C%20where%20it%0Ademonstrated%20improvements%20compared%20to%20eight%20other%20models.%20Additionally%2C%20in%20the%0Aqualitative%20comparison%20experiment%2C%20our%20model%20provided%20psychologically%20relevant%0Apredictions%20given%20the%20masked%20sentences.%20Due%20to%20concerns%20regarding%20data%20privacy%2C%0Athe%20dataset%20will%20not%20be%20made%20publicly%20available.%20However%2C%20we%20have%20made%20the%0Apre-trained%20models%20and%20codes%20publicly%20accessible%20to%20the%20community%20via%3A%0Ahttps%3A//github.com/zwzzzQAQ/Chinese-MentalBERT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09151v2&entry.124074799=Read"},
{"title": "PE: A Poincare Explanation Method for Fast Text Hierarchy Generation", "author": "Qian Chen and Dongyang Li and Xiaofeng He and Hongzhao Li and Hongyu Yi", "abstract": "  The black-box nature of deep learning models in NLP hinders their widespread\napplication. The research focus has shifted to Hierarchical Attribution (HA)\nfor its ability to model feature interactions. Recent works model\nnon-contiguous combinations with a time-costly greedy search in Eculidean\nspaces, neglecting underlying linguistic information in feature\nrepresentations. In this work, we introduce a novel method, namely Poincare\nExplanation (PE), for modeling feature interactions with hyperbolic spaces in a\ntime efficient manner. Specifically, we take building text hierarchies as\nfinding spanning trees in hyperbolic spaces. First we project the embeddings\ninto hyperbolic spaces to elicit inherit semantic and syntax hierarchical\nstructures. Then we propose a simple yet effective strategy to calculate\nShapley score. Finally we build the the hierarchy with proving the constructing\nprocess in the projected space could be viewed as building a minimum spanning\ntree and introduce a time efficient building algorithm. Experimental results\ndemonstrate the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2403.16554v2", "date": "2024-06-12", "relevancy": 2.2982, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4644}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4636}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PE%3A%20A%20Poincare%20Explanation%20Method%20for%20Fast%20Text%20Hierarchy%20Generation&body=Title%3A%20PE%3A%20A%20Poincare%20Explanation%20Method%20for%20Fast%20Text%20Hierarchy%20Generation%0AAuthor%3A%20Qian%20Chen%20and%20Dongyang%20Li%20and%20Xiaofeng%20He%20and%20Hongzhao%20Li%20and%20Hongyu%20Yi%0AAbstract%3A%20%20%20The%20black-box%20nature%20of%20deep%20learning%20models%20in%20NLP%20hinders%20their%20widespread%0Aapplication.%20The%20research%20focus%20has%20shifted%20to%20Hierarchical%20Attribution%20%28HA%29%0Afor%20its%20ability%20to%20model%20feature%20interactions.%20Recent%20works%20model%0Anon-contiguous%20combinations%20with%20a%20time-costly%20greedy%20search%20in%20Eculidean%0Aspaces%2C%20neglecting%20underlying%20linguistic%20information%20in%20feature%0Arepresentations.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20method%2C%20namely%20Poincare%0AExplanation%20%28PE%29%2C%20for%20modeling%20feature%20interactions%20with%20hyperbolic%20spaces%20in%20a%0Atime%20efficient%20manner.%20Specifically%2C%20we%20take%20building%20text%20hierarchies%20as%0Afinding%20spanning%20trees%20in%20hyperbolic%20spaces.%20First%20we%20project%20the%20embeddings%0Ainto%20hyperbolic%20spaces%20to%20elicit%20inherit%20semantic%20and%20syntax%20hierarchical%0Astructures.%20Then%20we%20propose%20a%20simple%20yet%20effective%20strategy%20to%20calculate%0AShapley%20score.%20Finally%20we%20build%20the%20the%20hierarchy%20with%20proving%20the%20constructing%0Aprocess%20in%20the%20projected%20space%20could%20be%20viewed%20as%20building%20a%20minimum%20spanning%0Atree%20and%20introduce%20a%20time%20efficient%20building%20algorithm.%20Experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16554v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPE%253A%2520A%2520Poincare%2520Explanation%2520Method%2520for%2520Fast%2520Text%2520Hierarchy%2520Generation%26entry.906535625%3DQian%2520Chen%2520and%2520Dongyang%2520Li%2520and%2520Xiaofeng%2520He%2520and%2520Hongzhao%2520Li%2520and%2520Hongyu%2520Yi%26entry.1292438233%3D%2520%2520The%2520black-box%2520nature%2520of%2520deep%2520learning%2520models%2520in%2520NLP%2520hinders%2520their%2520widespread%250Aapplication.%2520The%2520research%2520focus%2520has%2520shifted%2520to%2520Hierarchical%2520Attribution%2520%2528HA%2529%250Afor%2520its%2520ability%2520to%2520model%2520feature%2520interactions.%2520Recent%2520works%2520model%250Anon-contiguous%2520combinations%2520with%2520a%2520time-costly%2520greedy%2520search%2520in%2520Eculidean%250Aspaces%252C%2520neglecting%2520underlying%2520linguistic%2520information%2520in%2520feature%250Arepresentations.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520method%252C%2520namely%2520Poincare%250AExplanation%2520%2528PE%2529%252C%2520for%2520modeling%2520feature%2520interactions%2520with%2520hyperbolic%2520spaces%2520in%2520a%250Atime%2520efficient%2520manner.%2520Specifically%252C%2520we%2520take%2520building%2520text%2520hierarchies%2520as%250Afinding%2520spanning%2520trees%2520in%2520hyperbolic%2520spaces.%2520First%2520we%2520project%2520the%2520embeddings%250Ainto%2520hyperbolic%2520spaces%2520to%2520elicit%2520inherit%2520semantic%2520and%2520syntax%2520hierarchical%250Astructures.%2520Then%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520strategy%2520to%2520calculate%250AShapley%2520score.%2520Finally%2520we%2520build%2520the%2520the%2520hierarchy%2520with%2520proving%2520the%2520constructing%250Aprocess%2520in%2520the%2520projected%2520space%2520could%2520be%2520viewed%2520as%2520building%2520a%2520minimum%2520spanning%250Atree%2520and%2520introduce%2520a%2520time%2520efficient%2520building%2520algorithm.%2520Experimental%2520results%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16554v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PE%3A%20A%20Poincare%20Explanation%20Method%20for%20Fast%20Text%20Hierarchy%20Generation&entry.906535625=Qian%20Chen%20and%20Dongyang%20Li%20and%20Xiaofeng%20He%20and%20Hongzhao%20Li%20and%20Hongyu%20Yi&entry.1292438233=%20%20The%20black-box%20nature%20of%20deep%20learning%20models%20in%20NLP%20hinders%20their%20widespread%0Aapplication.%20The%20research%20focus%20has%20shifted%20to%20Hierarchical%20Attribution%20%28HA%29%0Afor%20its%20ability%20to%20model%20feature%20interactions.%20Recent%20works%20model%0Anon-contiguous%20combinations%20with%20a%20time-costly%20greedy%20search%20in%20Eculidean%0Aspaces%2C%20neglecting%20underlying%20linguistic%20information%20in%20feature%0Arepresentations.%20In%20this%20work%2C%20we%20introduce%20a%20novel%20method%2C%20namely%20Poincare%0AExplanation%20%28PE%29%2C%20for%20modeling%20feature%20interactions%20with%20hyperbolic%20spaces%20in%20a%0Atime%20efficient%20manner.%20Specifically%2C%20we%20take%20building%20text%20hierarchies%20as%0Afinding%20spanning%20trees%20in%20hyperbolic%20spaces.%20First%20we%20project%20the%20embeddings%0Ainto%20hyperbolic%20spaces%20to%20elicit%20inherit%20semantic%20and%20syntax%20hierarchical%0Astructures.%20Then%20we%20propose%20a%20simple%20yet%20effective%20strategy%20to%20calculate%0AShapley%20score.%20Finally%20we%20build%20the%20the%20hierarchy%20with%20proving%20the%20constructing%0Aprocess%20in%20the%20projected%20space%20could%20be%20viewed%20as%20building%20a%20minimum%20spanning%0Atree%20and%20introduce%20a%20time%20efficient%20building%20algorithm.%20Experimental%20results%0Ademonstrate%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16554v2&entry.124074799=Read"},
{"title": "What If We Recaption Billions of Web Images with LLaMA-3?", "author": "Xianhang Li and Haoqin Tu and Mude Hui and Zeyu Wang and Bingchen Zhao and Junfei Xiao and Sucheng Ren and Jieru Mei and Qing Liu and Huangjie Zheng and Yuyin Zhou and Cihang Xie", "abstract": "  Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate\nthat semantically aligning and enriching textual descriptions of these pairs\ncan significantly enhance model training across various vision-language tasks,\nparticularly text-to-image generation. However, large-scale investigations in\nthis area remain predominantly closed-source. Our paper aims to bridge this\ncommunity effort, leveraging the powerful and \\textit{open-sourced} LLaMA-3, a\nGPT-4 level LLM. Our recaptioning pipeline is simple: first, we fine-tune a\nLLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images\nfrom the DataComp-1B dataset. Our empirical results confirm that this enhanced\ndataset, Recap-DataComp-1B, offers substantial benefits in training advanced\nvision-language models. For discriminative models like CLIP, we observe\nenhanced zero-shot performance in cross-modal retrieval tasks. For generative\nmodels like text-to-image Diffusion Transformers, the generated images exhibit\na significant improvement in alignment with users' text instructions,\nespecially in following complex queries. Our project page is\nhttps://www.haqtu.me/Recap-Datacomp-1B/\n", "link": "http://arxiv.org/abs/2406.08478v1", "date": "2024-06-12", "relevancy": 2.2869, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6091}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5461}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20If%20We%20Recaption%20Billions%20of%20Web%20Images%20with%20LLaMA-3%3F&body=Title%3A%20What%20If%20We%20Recaption%20Billions%20of%20Web%20Images%20with%20LLaMA-3%3F%0AAuthor%3A%20Xianhang%20Li%20and%20Haoqin%20Tu%20and%20Mude%20Hui%20and%20Zeyu%20Wang%20and%20Bingchen%20Zhao%20and%20Junfei%20Xiao%20and%20Sucheng%20Ren%20and%20Jieru%20Mei%20and%20Qing%20Liu%20and%20Huangjie%20Zheng%20and%20Yuyin%20Zhou%20and%20Cihang%20Xie%0AAbstract%3A%20%20%20Web-crawled%20image-text%20pairs%20are%20inherently%20noisy.%20Prior%20studies%20demonstrate%0Athat%20semantically%20aligning%20and%20enriching%20textual%20descriptions%20of%20these%20pairs%0Acan%20significantly%20enhance%20model%20training%20across%20various%20vision-language%20tasks%2C%0Aparticularly%20text-to-image%20generation.%20However%2C%20large-scale%20investigations%20in%0Athis%20area%20remain%20predominantly%20closed-source.%20Our%20paper%20aims%20to%20bridge%20this%0Acommunity%20effort%2C%20leveraging%20the%20powerful%20and%20%5Ctextit%7Bopen-sourced%7D%20LLaMA-3%2C%20a%0AGPT-4%20level%20LLM.%20Our%20recaptioning%20pipeline%20is%20simple%3A%20first%2C%20we%20fine-tune%20a%0ALLaMA-3-8B%20powered%20LLaVA-1.5%20and%20then%20employ%20it%20to%20recaption%201.3%20billion%20images%0Afrom%20the%20DataComp-1B%20dataset.%20Our%20empirical%20results%20confirm%20that%20this%20enhanced%0Adataset%2C%20Recap-DataComp-1B%2C%20offers%20substantial%20benefits%20in%20training%20advanced%0Avision-language%20models.%20For%20discriminative%20models%20like%20CLIP%2C%20we%20observe%0Aenhanced%20zero-shot%20performance%20in%20cross-modal%20retrieval%20tasks.%20For%20generative%0Amodels%20like%20text-to-image%20Diffusion%20Transformers%2C%20the%20generated%20images%20exhibit%0Aa%20significant%20improvement%20in%20alignment%20with%20users%27%20text%20instructions%2C%0Aespecially%20in%20following%20complex%20queries.%20Our%20project%20page%20is%0Ahttps%3A//www.haqtu.me/Recap-Datacomp-1B/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520If%2520We%2520Recaption%2520Billions%2520of%2520Web%2520Images%2520with%2520LLaMA-3%253F%26entry.906535625%3DXianhang%2520Li%2520and%2520Haoqin%2520Tu%2520and%2520Mude%2520Hui%2520and%2520Zeyu%2520Wang%2520and%2520Bingchen%2520Zhao%2520and%2520Junfei%2520Xiao%2520and%2520Sucheng%2520Ren%2520and%2520Jieru%2520Mei%2520and%2520Qing%2520Liu%2520and%2520Huangjie%2520Zheng%2520and%2520Yuyin%2520Zhou%2520and%2520Cihang%2520Xie%26entry.1292438233%3D%2520%2520Web-crawled%2520image-text%2520pairs%2520are%2520inherently%2520noisy.%2520Prior%2520studies%2520demonstrate%250Athat%2520semantically%2520aligning%2520and%2520enriching%2520textual%2520descriptions%2520of%2520these%2520pairs%250Acan%2520significantly%2520enhance%2520model%2520training%2520across%2520various%2520vision-language%2520tasks%252C%250Aparticularly%2520text-to-image%2520generation.%2520However%252C%2520large-scale%2520investigations%2520in%250Athis%2520area%2520remain%2520predominantly%2520closed-source.%2520Our%2520paper%2520aims%2520to%2520bridge%2520this%250Acommunity%2520effort%252C%2520leveraging%2520the%2520powerful%2520and%2520%255Ctextit%257Bopen-sourced%257D%2520LLaMA-3%252C%2520a%250AGPT-4%2520level%2520LLM.%2520Our%2520recaptioning%2520pipeline%2520is%2520simple%253A%2520first%252C%2520we%2520fine-tune%2520a%250ALLaMA-3-8B%2520powered%2520LLaVA-1.5%2520and%2520then%2520employ%2520it%2520to%2520recaption%25201.3%2520billion%2520images%250Afrom%2520the%2520DataComp-1B%2520dataset.%2520Our%2520empirical%2520results%2520confirm%2520that%2520this%2520enhanced%250Adataset%252C%2520Recap-DataComp-1B%252C%2520offers%2520substantial%2520benefits%2520in%2520training%2520advanced%250Avision-language%2520models.%2520For%2520discriminative%2520models%2520like%2520CLIP%252C%2520we%2520observe%250Aenhanced%2520zero-shot%2520performance%2520in%2520cross-modal%2520retrieval%2520tasks.%2520For%2520generative%250Amodels%2520like%2520text-to-image%2520Diffusion%2520Transformers%252C%2520the%2520generated%2520images%2520exhibit%250Aa%2520significant%2520improvement%2520in%2520alignment%2520with%2520users%2527%2520text%2520instructions%252C%250Aespecially%2520in%2520following%2520complex%2520queries.%2520Our%2520project%2520page%2520is%250Ahttps%253A//www.haqtu.me/Recap-Datacomp-1B/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20If%20We%20Recaption%20Billions%20of%20Web%20Images%20with%20LLaMA-3%3F&entry.906535625=Xianhang%20Li%20and%20Haoqin%20Tu%20and%20Mude%20Hui%20and%20Zeyu%20Wang%20and%20Bingchen%20Zhao%20and%20Junfei%20Xiao%20and%20Sucheng%20Ren%20and%20Jieru%20Mei%20and%20Qing%20Liu%20and%20Huangjie%20Zheng%20and%20Yuyin%20Zhou%20and%20Cihang%20Xie&entry.1292438233=%20%20Web-crawled%20image-text%20pairs%20are%20inherently%20noisy.%20Prior%20studies%20demonstrate%0Athat%20semantically%20aligning%20and%20enriching%20textual%20descriptions%20of%20these%20pairs%0Acan%20significantly%20enhance%20model%20training%20across%20various%20vision-language%20tasks%2C%0Aparticularly%20text-to-image%20generation.%20However%2C%20large-scale%20investigations%20in%0Athis%20area%20remain%20predominantly%20closed-source.%20Our%20paper%20aims%20to%20bridge%20this%0Acommunity%20effort%2C%20leveraging%20the%20powerful%20and%20%5Ctextit%7Bopen-sourced%7D%20LLaMA-3%2C%20a%0AGPT-4%20level%20LLM.%20Our%20recaptioning%20pipeline%20is%20simple%3A%20first%2C%20we%20fine-tune%20a%0ALLaMA-3-8B%20powered%20LLaVA-1.5%20and%20then%20employ%20it%20to%20recaption%201.3%20billion%20images%0Afrom%20the%20DataComp-1B%20dataset.%20Our%20empirical%20results%20confirm%20that%20this%20enhanced%0Adataset%2C%20Recap-DataComp-1B%2C%20offers%20substantial%20benefits%20in%20training%20advanced%0Avision-language%20models.%20For%20discriminative%20models%20like%20CLIP%2C%20we%20observe%0Aenhanced%20zero-shot%20performance%20in%20cross-modal%20retrieval%20tasks.%20For%20generative%0Amodels%20like%20text-to-image%20Diffusion%20Transformers%2C%20the%20generated%20images%20exhibit%0Aa%20significant%20improvement%20in%20alignment%20with%20users%27%20text%20instructions%2C%0Aespecially%20in%20following%20complex%20queries.%20Our%20project%20page%20is%0Ahttps%3A//www.haqtu.me/Recap-Datacomp-1B/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08478v1&entry.124074799=Read"},
{"title": "LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry", "author": "Weirong Chen and Le Chen and Rui Wang and Marc Pollefeys", "abstract": "  Visual odometry estimates the motion of a moving camera based on visual\ninput. Existing methods, mostly focusing on two-view point tracking, often\nignore the rich temporal context in the image sequence, thereby overlooking the\nglobal motion patterns and providing no assessment of the full trajectory\nreliability. These shortcomings hinder performance in scenarios with occlusion,\ndynamic objects, and low-texture areas. To address these challenges, we present\nthe Long-term Effective Any Point Tracking (LEAP) module. LEAP innovatively\ncombines visual, inter-track, and temporal cues with mindfully selected anchors\nfor dynamic track estimation. Moreover, LEAP's temporal probabilistic\nformulation integrates distribution updates into a learnable iterative\nrefinement module to reason about point-wise uncertainty. Based on these\ntraits, we develop LEAP-VO, a robust visual odometry system adept at handling\nocclusions and dynamic scenes. Our mindful integration showcases a novel\npractice by employing long-term point tracking as the front-end. Extensive\nexperiments demonstrate that the proposed pipeline significantly outperforms\nexisting baselines across various visual odometry benchmarks.\n", "link": "http://arxiv.org/abs/2401.01887v2", "date": "2024-06-12", "relevancy": 2.2817, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6031}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5604}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEAP-VO%3A%20Long-term%20Effective%20Any%20Point%20Tracking%20for%20Visual%20Odometry&body=Title%3A%20LEAP-VO%3A%20Long-term%20Effective%20Any%20Point%20Tracking%20for%20Visual%20Odometry%0AAuthor%3A%20Weirong%20Chen%20and%20Le%20Chen%20and%20Rui%20Wang%20and%20Marc%20Pollefeys%0AAbstract%3A%20%20%20Visual%20odometry%20estimates%20the%20motion%20of%20a%20moving%20camera%20based%20on%20visual%0Ainput.%20Existing%20methods%2C%20mostly%20focusing%20on%20two-view%20point%20tracking%2C%20often%0Aignore%20the%20rich%20temporal%20context%20in%20the%20image%20sequence%2C%20thereby%20overlooking%20the%0Aglobal%20motion%20patterns%20and%20providing%20no%20assessment%20of%20the%20full%20trajectory%0Areliability.%20These%20shortcomings%20hinder%20performance%20in%20scenarios%20with%20occlusion%2C%0Adynamic%20objects%2C%20and%20low-texture%20areas.%20To%20address%20these%20challenges%2C%20we%20present%0Athe%20Long-term%20Effective%20Any%20Point%20Tracking%20%28LEAP%29%20module.%20LEAP%20innovatively%0Acombines%20visual%2C%20inter-track%2C%20and%20temporal%20cues%20with%20mindfully%20selected%20anchors%0Afor%20dynamic%20track%20estimation.%20Moreover%2C%20LEAP%27s%20temporal%20probabilistic%0Aformulation%20integrates%20distribution%20updates%20into%20a%20learnable%20iterative%0Arefinement%20module%20to%20reason%20about%20point-wise%20uncertainty.%20Based%20on%20these%0Atraits%2C%20we%20develop%20LEAP-VO%2C%20a%20robust%20visual%20odometry%20system%20adept%20at%20handling%0Aocclusions%20and%20dynamic%20scenes.%20Our%20mindful%20integration%20showcases%20a%20novel%0Apractice%20by%20employing%20long-term%20point%20tracking%20as%20the%20front-end.%20Extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20pipeline%20significantly%20outperforms%0Aexisting%20baselines%20across%20various%20visual%20odometry%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.01887v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEAP-VO%253A%2520Long-term%2520Effective%2520Any%2520Point%2520Tracking%2520for%2520Visual%2520Odometry%26entry.906535625%3DWeirong%2520Chen%2520and%2520Le%2520Chen%2520and%2520Rui%2520Wang%2520and%2520Marc%2520Pollefeys%26entry.1292438233%3D%2520%2520Visual%2520odometry%2520estimates%2520the%2520motion%2520of%2520a%2520moving%2520camera%2520based%2520on%2520visual%250Ainput.%2520Existing%2520methods%252C%2520mostly%2520focusing%2520on%2520two-view%2520point%2520tracking%252C%2520often%250Aignore%2520the%2520rich%2520temporal%2520context%2520in%2520the%2520image%2520sequence%252C%2520thereby%2520overlooking%2520the%250Aglobal%2520motion%2520patterns%2520and%2520providing%2520no%2520assessment%2520of%2520the%2520full%2520trajectory%250Areliability.%2520These%2520shortcomings%2520hinder%2520performance%2520in%2520scenarios%2520with%2520occlusion%252C%250Adynamic%2520objects%252C%2520and%2520low-texture%2520areas.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%250Athe%2520Long-term%2520Effective%2520Any%2520Point%2520Tracking%2520%2528LEAP%2529%2520module.%2520LEAP%2520innovatively%250Acombines%2520visual%252C%2520inter-track%252C%2520and%2520temporal%2520cues%2520with%2520mindfully%2520selected%2520anchors%250Afor%2520dynamic%2520track%2520estimation.%2520Moreover%252C%2520LEAP%2527s%2520temporal%2520probabilistic%250Aformulation%2520integrates%2520distribution%2520updates%2520into%2520a%2520learnable%2520iterative%250Arefinement%2520module%2520to%2520reason%2520about%2520point-wise%2520uncertainty.%2520Based%2520on%2520these%250Atraits%252C%2520we%2520develop%2520LEAP-VO%252C%2520a%2520robust%2520visual%2520odometry%2520system%2520adept%2520at%2520handling%250Aocclusions%2520and%2520dynamic%2520scenes.%2520Our%2520mindful%2520integration%2520showcases%2520a%2520novel%250Apractice%2520by%2520employing%2520long-term%2520point%2520tracking%2520as%2520the%2520front-end.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520the%2520proposed%2520pipeline%2520significantly%2520outperforms%250Aexisting%2520baselines%2520across%2520various%2520visual%2520odometry%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.01887v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEAP-VO%3A%20Long-term%20Effective%20Any%20Point%20Tracking%20for%20Visual%20Odometry&entry.906535625=Weirong%20Chen%20and%20Le%20Chen%20and%20Rui%20Wang%20and%20Marc%20Pollefeys&entry.1292438233=%20%20Visual%20odometry%20estimates%20the%20motion%20of%20a%20moving%20camera%20based%20on%20visual%0Ainput.%20Existing%20methods%2C%20mostly%20focusing%20on%20two-view%20point%20tracking%2C%20often%0Aignore%20the%20rich%20temporal%20context%20in%20the%20image%20sequence%2C%20thereby%20overlooking%20the%0Aglobal%20motion%20patterns%20and%20providing%20no%20assessment%20of%20the%20full%20trajectory%0Areliability.%20These%20shortcomings%20hinder%20performance%20in%20scenarios%20with%20occlusion%2C%0Adynamic%20objects%2C%20and%20low-texture%20areas.%20To%20address%20these%20challenges%2C%20we%20present%0Athe%20Long-term%20Effective%20Any%20Point%20Tracking%20%28LEAP%29%20module.%20LEAP%20innovatively%0Acombines%20visual%2C%20inter-track%2C%20and%20temporal%20cues%20with%20mindfully%20selected%20anchors%0Afor%20dynamic%20track%20estimation.%20Moreover%2C%20LEAP%27s%20temporal%20probabilistic%0Aformulation%20integrates%20distribution%20updates%20into%20a%20learnable%20iterative%0Arefinement%20module%20to%20reason%20about%20point-wise%20uncertainty.%20Based%20on%20these%0Atraits%2C%20we%20develop%20LEAP-VO%2C%20a%20robust%20visual%20odometry%20system%20adept%20at%20handling%0Aocclusions%20and%20dynamic%20scenes.%20Our%20mindful%20integration%20showcases%20a%20novel%0Apractice%20by%20employing%20long-term%20point%20tracking%20as%20the%20front-end.%20Extensive%0Aexperiments%20demonstrate%20that%20the%20proposed%20pipeline%20significantly%20outperforms%0Aexisting%20baselines%20across%20various%20visual%20odometry%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01887v2&entry.124074799=Read"},
{"title": "A novel approach to graph distinction through GENEOs and permutants", "author": "Giovanni Bocchi and Massimo Ferri and Patrizio Frosini", "abstract": "  The theory of Group Equivariant Non-Expansive Operators (GENEOs) was\ninitially developed in Topological Data Analysis for the geometric\napproximation of data observers, including their invariances and symmetries.\nThis paper departs from that line of research and explores the use of GENEOs\nfor distinguishing $r$-regular graphs up to isomorphisms. In doing so, we aim\nto test the capabilities and flexibility of these operators. Our experiments\nshow that GENEOs offer a good compromise between efficiency and computational\ncost in comparing $r$-regular graphs, while their actions on data are easily\ninterpretable. This supports the idea that GENEOs could be a general-purpose\napproach to discriminative problems in Machine Learning when some structural\ninformation about data and observers is explicitly given.\n", "link": "http://arxiv.org/abs/2406.08045v1", "date": "2024-06-12", "relevancy": 2.2805, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4774}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4605}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20novel%20approach%20to%20graph%20distinction%20through%20GENEOs%20and%20permutants&body=Title%3A%20A%20novel%20approach%20to%20graph%20distinction%20through%20GENEOs%20and%20permutants%0AAuthor%3A%20Giovanni%20Bocchi%20and%20Massimo%20Ferri%20and%20Patrizio%20Frosini%0AAbstract%3A%20%20%20The%20theory%20of%20Group%20Equivariant%20Non-Expansive%20Operators%20%28GENEOs%29%20was%0Ainitially%20developed%20in%20Topological%20Data%20Analysis%20for%20the%20geometric%0Aapproximation%20of%20data%20observers%2C%20including%20their%20invariances%20and%20symmetries.%0AThis%20paper%20departs%20from%20that%20line%20of%20research%20and%20explores%20the%20use%20of%20GENEOs%0Afor%20distinguishing%20%24r%24-regular%20graphs%20up%20to%20isomorphisms.%20In%20doing%20so%2C%20we%20aim%0Ato%20test%20the%20capabilities%20and%20flexibility%20of%20these%20operators.%20Our%20experiments%0Ashow%20that%20GENEOs%20offer%20a%20good%20compromise%20between%20efficiency%20and%20computational%0Acost%20in%20comparing%20%24r%24-regular%20graphs%2C%20while%20their%20actions%20on%20data%20are%20easily%0Ainterpretable.%20This%20supports%20the%20idea%20that%20GENEOs%20could%20be%20a%20general-purpose%0Aapproach%20to%20discriminative%20problems%20in%20Machine%20Learning%20when%20some%20structural%0Ainformation%20about%20data%20and%20observers%20is%20explicitly%20given.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08045v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520novel%2520approach%2520to%2520graph%2520distinction%2520through%2520GENEOs%2520and%2520permutants%26entry.906535625%3DGiovanni%2520Bocchi%2520and%2520Massimo%2520Ferri%2520and%2520Patrizio%2520Frosini%26entry.1292438233%3D%2520%2520The%2520theory%2520of%2520Group%2520Equivariant%2520Non-Expansive%2520Operators%2520%2528GENEOs%2529%2520was%250Ainitially%2520developed%2520in%2520Topological%2520Data%2520Analysis%2520for%2520the%2520geometric%250Aapproximation%2520of%2520data%2520observers%252C%2520including%2520their%2520invariances%2520and%2520symmetries.%250AThis%2520paper%2520departs%2520from%2520that%2520line%2520of%2520research%2520and%2520explores%2520the%2520use%2520of%2520GENEOs%250Afor%2520distinguishing%2520%2524r%2524-regular%2520graphs%2520up%2520to%2520isomorphisms.%2520In%2520doing%2520so%252C%2520we%2520aim%250Ato%2520test%2520the%2520capabilities%2520and%2520flexibility%2520of%2520these%2520operators.%2520Our%2520experiments%250Ashow%2520that%2520GENEOs%2520offer%2520a%2520good%2520compromise%2520between%2520efficiency%2520and%2520computational%250Acost%2520in%2520comparing%2520%2524r%2524-regular%2520graphs%252C%2520while%2520their%2520actions%2520on%2520data%2520are%2520easily%250Ainterpretable.%2520This%2520supports%2520the%2520idea%2520that%2520GENEOs%2520could%2520be%2520a%2520general-purpose%250Aapproach%2520to%2520discriminative%2520problems%2520in%2520Machine%2520Learning%2520when%2520some%2520structural%250Ainformation%2520about%2520data%2520and%2520observers%2520is%2520explicitly%2520given.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08045v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20novel%20approach%20to%20graph%20distinction%20through%20GENEOs%20and%20permutants&entry.906535625=Giovanni%20Bocchi%20and%20Massimo%20Ferri%20and%20Patrizio%20Frosini&entry.1292438233=%20%20The%20theory%20of%20Group%20Equivariant%20Non-Expansive%20Operators%20%28GENEOs%29%20was%0Ainitially%20developed%20in%20Topological%20Data%20Analysis%20for%20the%20geometric%0Aapproximation%20of%20data%20observers%2C%20including%20their%20invariances%20and%20symmetries.%0AThis%20paper%20departs%20from%20that%20line%20of%20research%20and%20explores%20the%20use%20of%20GENEOs%0Afor%20distinguishing%20%24r%24-regular%20graphs%20up%20to%20isomorphisms.%20In%20doing%20so%2C%20we%20aim%0Ato%20test%20the%20capabilities%20and%20flexibility%20of%20these%20operators.%20Our%20experiments%0Ashow%20that%20GENEOs%20offer%20a%20good%20compromise%20between%20efficiency%20and%20computational%0Acost%20in%20comparing%20%24r%24-regular%20graphs%2C%20while%20their%20actions%20on%20data%20are%20easily%0Ainterpretable.%20This%20supports%20the%20idea%20that%20GENEOs%20could%20be%20a%20general-purpose%0Aapproach%20to%20discriminative%20problems%20in%20Machine%20Learning%20when%20some%20structural%0Ainformation%20about%20data%20and%20observers%20is%20explicitly%20given.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08045v1&entry.124074799=Read"},
{"title": "Learning-based Traversability Costmap for Autonomous Off-road Navigation", "author": "Qiumin Zhu and Zhen Sun and Songpengcheng Xia and Guoqing Liu and Kehui Ma and Ling Pei and Zheng Gong", "abstract": "  Traversability estimation in off-road terrains is an essential procedure for\nautonomous navigation. However, creating reliable labels for complex\ninteractions between the robot and the surface is still a challenging problem\nin learning-based costmap generation. To address this, we propose a method that\npredicts traversability costmaps by leveraging both visual and geometric\ninformation of the environment. To quantify the surface properties like\nroughness and bumpiness, we introduce a novel way of risk-aware labelling with\nproprioceptive information for network training. We validate our method in\ncostmap prediction and navigation tasks for complex off-road scenarios. Our\nresults demonstrate that our costmap prediction method excels in terms of\naverage accuracy and MSE. The navigation results indicate that using our\nlearned costmaps leads to safer and smoother driving, outperforming previous\nmethods in terms of the highest success rate, lowest normalized trajectory\nlength, lowest time cost, and highest mean stability across two scenarios.\n", "link": "http://arxiv.org/abs/2406.08187v1", "date": "2024-06-12", "relevancy": 2.2711, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5979}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5805}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning-based%20Traversability%20Costmap%20for%20Autonomous%20Off-road%20Navigation&body=Title%3A%20Learning-based%20Traversability%20Costmap%20for%20Autonomous%20Off-road%20Navigation%0AAuthor%3A%20Qiumin%20Zhu%20and%20Zhen%20Sun%20and%20Songpengcheng%20Xia%20and%20Guoqing%20Liu%20and%20Kehui%20Ma%20and%20Ling%20Pei%20and%20Zheng%20Gong%0AAbstract%3A%20%20%20Traversability%20estimation%20in%20off-road%20terrains%20is%20an%20essential%20procedure%20for%0Aautonomous%20navigation.%20However%2C%20creating%20reliable%20labels%20for%20complex%0Ainteractions%20between%20the%20robot%20and%20the%20surface%20is%20still%20a%20challenging%20problem%0Ain%20learning-based%20costmap%20generation.%20To%20address%20this%2C%20we%20propose%20a%20method%20that%0Apredicts%20traversability%20costmaps%20by%20leveraging%20both%20visual%20and%20geometric%0Ainformation%20of%20the%20environment.%20To%20quantify%20the%20surface%20properties%20like%0Aroughness%20and%20bumpiness%2C%20we%20introduce%20a%20novel%20way%20of%20risk-aware%20labelling%20with%0Aproprioceptive%20information%20for%20network%20training.%20We%20validate%20our%20method%20in%0Acostmap%20prediction%20and%20navigation%20tasks%20for%20complex%20off-road%20scenarios.%20Our%0Aresults%20demonstrate%20that%20our%20costmap%20prediction%20method%20excels%20in%20terms%20of%0Aaverage%20accuracy%20and%20MSE.%20The%20navigation%20results%20indicate%20that%20using%20our%0Alearned%20costmaps%20leads%20to%20safer%20and%20smoother%20driving%2C%20outperforming%20previous%0Amethods%20in%20terms%20of%20the%20highest%20success%20rate%2C%20lowest%20normalized%20trajectory%0Alength%2C%20lowest%20time%20cost%2C%20and%20highest%20mean%20stability%20across%20two%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08187v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning-based%2520Traversability%2520Costmap%2520for%2520Autonomous%2520Off-road%2520Navigation%26entry.906535625%3DQiumin%2520Zhu%2520and%2520Zhen%2520Sun%2520and%2520Songpengcheng%2520Xia%2520and%2520Guoqing%2520Liu%2520and%2520Kehui%2520Ma%2520and%2520Ling%2520Pei%2520and%2520Zheng%2520Gong%26entry.1292438233%3D%2520%2520Traversability%2520estimation%2520in%2520off-road%2520terrains%2520is%2520an%2520essential%2520procedure%2520for%250Aautonomous%2520navigation.%2520However%252C%2520creating%2520reliable%2520labels%2520for%2520complex%250Ainteractions%2520between%2520the%2520robot%2520and%2520the%2520surface%2520is%2520still%2520a%2520challenging%2520problem%250Ain%2520learning-based%2520costmap%2520generation.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520method%2520that%250Apredicts%2520traversability%2520costmaps%2520by%2520leveraging%2520both%2520visual%2520and%2520geometric%250Ainformation%2520of%2520the%2520environment.%2520To%2520quantify%2520the%2520surface%2520properties%2520like%250Aroughness%2520and%2520bumpiness%252C%2520we%2520introduce%2520a%2520novel%2520way%2520of%2520risk-aware%2520labelling%2520with%250Aproprioceptive%2520information%2520for%2520network%2520training.%2520We%2520validate%2520our%2520method%2520in%250Acostmap%2520prediction%2520and%2520navigation%2520tasks%2520for%2520complex%2520off-road%2520scenarios.%2520Our%250Aresults%2520demonstrate%2520that%2520our%2520costmap%2520prediction%2520method%2520excels%2520in%2520terms%2520of%250Aaverage%2520accuracy%2520and%2520MSE.%2520The%2520navigation%2520results%2520indicate%2520that%2520using%2520our%250Alearned%2520costmaps%2520leads%2520to%2520safer%2520and%2520smoother%2520driving%252C%2520outperforming%2520previous%250Amethods%2520in%2520terms%2520of%2520the%2520highest%2520success%2520rate%252C%2520lowest%2520normalized%2520trajectory%250Alength%252C%2520lowest%2520time%2520cost%252C%2520and%2520highest%2520mean%2520stability%2520across%2520two%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08187v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning-based%20Traversability%20Costmap%20for%20Autonomous%20Off-road%20Navigation&entry.906535625=Qiumin%20Zhu%20and%20Zhen%20Sun%20and%20Songpengcheng%20Xia%20and%20Guoqing%20Liu%20and%20Kehui%20Ma%20and%20Ling%20Pei%20and%20Zheng%20Gong&entry.1292438233=%20%20Traversability%20estimation%20in%20off-road%20terrains%20is%20an%20essential%20procedure%20for%0Aautonomous%20navigation.%20However%2C%20creating%20reliable%20labels%20for%20complex%0Ainteractions%20between%20the%20robot%20and%20the%20surface%20is%20still%20a%20challenging%20problem%0Ain%20learning-based%20costmap%20generation.%20To%20address%20this%2C%20we%20propose%20a%20method%20that%0Apredicts%20traversability%20costmaps%20by%20leveraging%20both%20visual%20and%20geometric%0Ainformation%20of%20the%20environment.%20To%20quantify%20the%20surface%20properties%20like%0Aroughness%20and%20bumpiness%2C%20we%20introduce%20a%20novel%20way%20of%20risk-aware%20labelling%20with%0Aproprioceptive%20information%20for%20network%20training.%20We%20validate%20our%20method%20in%0Acostmap%20prediction%20and%20navigation%20tasks%20for%20complex%20off-road%20scenarios.%20Our%0Aresults%20demonstrate%20that%20our%20costmap%20prediction%20method%20excels%20in%20terms%20of%0Aaverage%20accuracy%20and%20MSE.%20The%20navigation%20results%20indicate%20that%20using%20our%0Alearned%20costmaps%20leads%20to%20safer%20and%20smoother%20driving%2C%20outperforming%20previous%0Amethods%20in%20terms%20of%20the%20highest%20success%20rate%2C%20lowest%20normalized%20trajectory%0Alength%2C%20lowest%20time%20cost%2C%20and%20highest%20mean%20stability%20across%20two%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08187v1&entry.124074799=Read"},
{"title": "LaneCPP: Continuous 3D Lane Detection using Physical Priors", "author": "Maximilian Pittner and Joel Janai and Alexandru P. Condurache", "abstract": "  Monocular 3D lane detection has become a fundamental problem in the context\nof autonomous driving, which comprises the tasks of finding the road surface\nand locating lane markings. One major challenge lies in a flexible but robust\nline representation capable of modeling complex lane structures, while still\navoiding unpredictable behavior. While previous methods rely on fully\ndata-driven approaches, we instead introduce a novel approach LaneCPP that uses\na continuous 3D lane detection model leveraging physical prior knowledge about\nthe lane structure and road geometry. While our sophisticated lane model is\ncapable of modeling complex road structures, it also shows robust behavior\nsince physical constraints are incorporated by means of a regularization scheme\nthat can be analytically applied to our parametric representation. Moreover, we\nincorporate prior knowledge about the road geometry into the 3D feature space\nby modeling geometry-aware spatial features, guiding the network to learn an\ninternal road surface representation. In our experiments, we show the benefits\nof our contributions and prove the meaningfulness of using priors to make 3D\nlane detection more robust. The results show that LaneCPP achieves\nstate-of-the-art performance in terms of F-Score and geometric errors.\n", "link": "http://arxiv.org/abs/2406.08381v1", "date": "2024-06-12", "relevancy": 2.2564, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5669}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5638}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaneCPP%3A%20Continuous%203D%20Lane%20Detection%20using%20Physical%20Priors&body=Title%3A%20LaneCPP%3A%20Continuous%203D%20Lane%20Detection%20using%20Physical%20Priors%0AAuthor%3A%20Maximilian%20Pittner%20and%20Joel%20Janai%20and%20Alexandru%20P.%20Condurache%0AAbstract%3A%20%20%20Monocular%203D%20lane%20detection%20has%20become%20a%20fundamental%20problem%20in%20the%20context%0Aof%20autonomous%20driving%2C%20which%20comprises%20the%20tasks%20of%20finding%20the%20road%20surface%0Aand%20locating%20lane%20markings.%20One%20major%20challenge%20lies%20in%20a%20flexible%20but%20robust%0Aline%20representation%20capable%20of%20modeling%20complex%20lane%20structures%2C%20while%20still%0Aavoiding%20unpredictable%20behavior.%20While%20previous%20methods%20rely%20on%20fully%0Adata-driven%20approaches%2C%20we%20instead%20introduce%20a%20novel%20approach%20LaneCPP%20that%20uses%0Aa%20continuous%203D%20lane%20detection%20model%20leveraging%20physical%20prior%20knowledge%20about%0Athe%20lane%20structure%20and%20road%20geometry.%20While%20our%20sophisticated%20lane%20model%20is%0Acapable%20of%20modeling%20complex%20road%20structures%2C%20it%20also%20shows%20robust%20behavior%0Asince%20physical%20constraints%20are%20incorporated%20by%20means%20of%20a%20regularization%20scheme%0Athat%20can%20be%20analytically%20applied%20to%20our%20parametric%20representation.%20Moreover%2C%20we%0Aincorporate%20prior%20knowledge%20about%20the%20road%20geometry%20into%20the%203D%20feature%20space%0Aby%20modeling%20geometry-aware%20spatial%20features%2C%20guiding%20the%20network%20to%20learn%20an%0Ainternal%20road%20surface%20representation.%20In%20our%20experiments%2C%20we%20show%20the%20benefits%0Aof%20our%20contributions%20and%20prove%20the%20meaningfulness%20of%20using%20priors%20to%20make%203D%0Alane%20detection%20more%20robust.%20The%20results%20show%20that%20LaneCPP%20achieves%0Astate-of-the-art%20performance%20in%20terms%20of%20F-Score%20and%20geometric%20errors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaneCPP%253A%2520Continuous%25203D%2520Lane%2520Detection%2520using%2520Physical%2520Priors%26entry.906535625%3DMaximilian%2520Pittner%2520and%2520Joel%2520Janai%2520and%2520Alexandru%2520P.%2520Condurache%26entry.1292438233%3D%2520%2520Monocular%25203D%2520lane%2520detection%2520has%2520become%2520a%2520fundamental%2520problem%2520in%2520the%2520context%250Aof%2520autonomous%2520driving%252C%2520which%2520comprises%2520the%2520tasks%2520of%2520finding%2520the%2520road%2520surface%250Aand%2520locating%2520lane%2520markings.%2520One%2520major%2520challenge%2520lies%2520in%2520a%2520flexible%2520but%2520robust%250Aline%2520representation%2520capable%2520of%2520modeling%2520complex%2520lane%2520structures%252C%2520while%2520still%250Aavoiding%2520unpredictable%2520behavior.%2520While%2520previous%2520methods%2520rely%2520on%2520fully%250Adata-driven%2520approaches%252C%2520we%2520instead%2520introduce%2520a%2520novel%2520approach%2520LaneCPP%2520that%2520uses%250Aa%2520continuous%25203D%2520lane%2520detection%2520model%2520leveraging%2520physical%2520prior%2520knowledge%2520about%250Athe%2520lane%2520structure%2520and%2520road%2520geometry.%2520While%2520our%2520sophisticated%2520lane%2520model%2520is%250Acapable%2520of%2520modeling%2520complex%2520road%2520structures%252C%2520it%2520also%2520shows%2520robust%2520behavior%250Asince%2520physical%2520constraints%2520are%2520incorporated%2520by%2520means%2520of%2520a%2520regularization%2520scheme%250Athat%2520can%2520be%2520analytically%2520applied%2520to%2520our%2520parametric%2520representation.%2520Moreover%252C%2520we%250Aincorporate%2520prior%2520knowledge%2520about%2520the%2520road%2520geometry%2520into%2520the%25203D%2520feature%2520space%250Aby%2520modeling%2520geometry-aware%2520spatial%2520features%252C%2520guiding%2520the%2520network%2520to%2520learn%2520an%250Ainternal%2520road%2520surface%2520representation.%2520In%2520our%2520experiments%252C%2520we%2520show%2520the%2520benefits%250Aof%2520our%2520contributions%2520and%2520prove%2520the%2520meaningfulness%2520of%2520using%2520priors%2520to%2520make%25203D%250Alane%2520detection%2520more%2520robust.%2520The%2520results%2520show%2520that%2520LaneCPP%2520achieves%250Astate-of-the-art%2520performance%2520in%2520terms%2520of%2520F-Score%2520and%2520geometric%2520errors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaneCPP%3A%20Continuous%203D%20Lane%20Detection%20using%20Physical%20Priors&entry.906535625=Maximilian%20Pittner%20and%20Joel%20Janai%20and%20Alexandru%20P.%20Condurache&entry.1292438233=%20%20Monocular%203D%20lane%20detection%20has%20become%20a%20fundamental%20problem%20in%20the%20context%0Aof%20autonomous%20driving%2C%20which%20comprises%20the%20tasks%20of%20finding%20the%20road%20surface%0Aand%20locating%20lane%20markings.%20One%20major%20challenge%20lies%20in%20a%20flexible%20but%20robust%0Aline%20representation%20capable%20of%20modeling%20complex%20lane%20structures%2C%20while%20still%0Aavoiding%20unpredictable%20behavior.%20While%20previous%20methods%20rely%20on%20fully%0Adata-driven%20approaches%2C%20we%20instead%20introduce%20a%20novel%20approach%20LaneCPP%20that%20uses%0Aa%20continuous%203D%20lane%20detection%20model%20leveraging%20physical%20prior%20knowledge%20about%0Athe%20lane%20structure%20and%20road%20geometry.%20While%20our%20sophisticated%20lane%20model%20is%0Acapable%20of%20modeling%20complex%20road%20structures%2C%20it%20also%20shows%20robust%20behavior%0Asince%20physical%20constraints%20are%20incorporated%20by%20means%20of%20a%20regularization%20scheme%0Athat%20can%20be%20analytically%20applied%20to%20our%20parametric%20representation.%20Moreover%2C%20we%0Aincorporate%20prior%20knowledge%20about%20the%20road%20geometry%20into%20the%203D%20feature%20space%0Aby%20modeling%20geometry-aware%20spatial%20features%2C%20guiding%20the%20network%20to%20learn%20an%0Ainternal%20road%20surface%20representation.%20In%20our%20experiments%2C%20we%20show%20the%20benefits%0Aof%20our%20contributions%20and%20prove%20the%20meaningfulness%20of%20using%20priors%20to%20make%203D%0Alane%20detection%20more%20robust.%20The%20results%20show%20that%20LaneCPP%20achieves%0Astate-of-the-art%20performance%20in%20terms%20of%20F-Score%20and%20geometric%20errors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08381v1&entry.124074799=Read"},
{"title": "3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and\n  Less Hallucination", "author": "Jianing Yang and Xuweiyi Chen and Nikhil Madaan and Madhavan Iyengar and Shengyi Qian and David F. Fouhey and Joyce Chai", "abstract": "  The integration of language and 3D perception is crucial for developing\nembodied agents and robots that comprehend and interact with the physical\nworld. While large language models (LLMs) have demonstrated impressive language\nunderstanding and generation capabilities, their adaptation to 3D environments\n(3D-LLMs) remains in its early stages. A primary challenge is the absence of\nlarge-scale datasets that provide dense grounding between language and 3D\nscenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset\ncomprising 40,087 household scenes paired with 6.2 million densely-grounded\nscene-language instructions. Our results show that instruction tuning with\n3D-GRAND significantly enhances grounding capabilities and reduces\nhallucinations in 3D-LLMs. As part of our contributions, we propose a\ncomprehensive benchmark 3D-POPE to systematically evaluate hallucination in\n3D-LLMs, enabling fair comparisons among future models. Our experiments\nhighlight a scaling effect between dataset size and 3D-LLM performance,\nemphasizing the critical role of large-scale 3D-text datasets in advancing\nembodied AI research. Notably, our results demonstrate early signals for\neffective sim-to-real transfer, indicating that models trained on large\nsynthetic data can perform well on real-world 3D scans. Through 3D-GRAND and\n3D-POPE, we aim to equip the embodied AI community with essential resources and\ninsights, setting the stage for more reliable and better-grounded 3D-LLMs.\nProject website: https://3d-grand.github.io\n", "link": "http://arxiv.org/abs/2406.05132v2", "date": "2024-06-12", "relevancy": 2.2363, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.572}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5572}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-GRAND%3A%20A%20Million-Scale%20Dataset%20for%203D-LLMs%20with%20Better%20Grounding%20and%0A%20%20Less%20Hallucination&body=Title%3A%203D-GRAND%3A%20A%20Million-Scale%20Dataset%20for%203D-LLMs%20with%20Better%20Grounding%20and%0A%20%20Less%20Hallucination%0AAuthor%3A%20Jianing%20Yang%20and%20Xuweiyi%20Chen%20and%20Nikhil%20Madaan%20and%20Madhavan%20Iyengar%20and%20Shengyi%20Qian%20and%20David%20F.%20Fouhey%20and%20Joyce%20Chai%0AAbstract%3A%20%20%20The%20integration%20of%20language%20and%203D%20perception%20is%20crucial%20for%20developing%0Aembodied%20agents%20and%20robots%20that%20comprehend%20and%20interact%20with%20the%20physical%0Aworld.%20While%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20language%0Aunderstanding%20and%20generation%20capabilities%2C%20their%20adaptation%20to%203D%20environments%0A%283D-LLMs%29%20remains%20in%20its%20early%20stages.%20A%20primary%20challenge%20is%20the%20absence%20of%0Alarge-scale%20datasets%20that%20provide%20dense%20grounding%20between%20language%20and%203D%0Ascenes.%20In%20this%20paper%2C%20we%20introduce%203D-GRAND%2C%20a%20pioneering%20large-scale%20dataset%0Acomprising%2040%2C087%20household%20scenes%20paired%20with%206.2%20million%20densely-grounded%0Ascene-language%20instructions.%20Our%20results%20show%20that%20instruction%20tuning%20with%0A3D-GRAND%20significantly%20enhances%20grounding%20capabilities%20and%20reduces%0Ahallucinations%20in%203D-LLMs.%20As%20part%20of%20our%20contributions%2C%20we%20propose%20a%0Acomprehensive%20benchmark%203D-POPE%20to%20systematically%20evaluate%20hallucination%20in%0A3D-LLMs%2C%20enabling%20fair%20comparisons%20among%20future%20models.%20Our%20experiments%0Ahighlight%20a%20scaling%20effect%20between%20dataset%20size%20and%203D-LLM%20performance%2C%0Aemphasizing%20the%20critical%20role%20of%20large-scale%203D-text%20datasets%20in%20advancing%0Aembodied%20AI%20research.%20Notably%2C%20our%20results%20demonstrate%20early%20signals%20for%0Aeffective%20sim-to-real%20transfer%2C%20indicating%20that%20models%20trained%20on%20large%0Asynthetic%20data%20can%20perform%20well%20on%20real-world%203D%20scans.%20Through%203D-GRAND%20and%0A3D-POPE%2C%20we%20aim%20to%20equip%20the%20embodied%20AI%20community%20with%20essential%20resources%20and%0Ainsights%2C%20setting%20the%20stage%20for%20more%20reliable%20and%20better-grounded%203D-LLMs.%0AProject%20website%3A%20https%3A//3d-grand.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05132v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-GRAND%253A%2520A%2520Million-Scale%2520Dataset%2520for%25203D-LLMs%2520with%2520Better%2520Grounding%2520and%250A%2520%2520Less%2520Hallucination%26entry.906535625%3DJianing%2520Yang%2520and%2520Xuweiyi%2520Chen%2520and%2520Nikhil%2520Madaan%2520and%2520Madhavan%2520Iyengar%2520and%2520Shengyi%2520Qian%2520and%2520David%2520F.%2520Fouhey%2520and%2520Joyce%2520Chai%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520language%2520and%25203D%2520perception%2520is%2520crucial%2520for%2520developing%250Aembodied%2520agents%2520and%2520robots%2520that%2520comprehend%2520and%2520interact%2520with%2520the%2520physical%250Aworld.%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520language%250Aunderstanding%2520and%2520generation%2520capabilities%252C%2520their%2520adaptation%2520to%25203D%2520environments%250A%25283D-LLMs%2529%2520remains%2520in%2520its%2520early%2520stages.%2520A%2520primary%2520challenge%2520is%2520the%2520absence%2520of%250Alarge-scale%2520datasets%2520that%2520provide%2520dense%2520grounding%2520between%2520language%2520and%25203D%250Ascenes.%2520In%2520this%2520paper%252C%2520we%2520introduce%25203D-GRAND%252C%2520a%2520pioneering%2520large-scale%2520dataset%250Acomprising%252040%252C087%2520household%2520scenes%2520paired%2520with%25206.2%2520million%2520densely-grounded%250Ascene-language%2520instructions.%2520Our%2520results%2520show%2520that%2520instruction%2520tuning%2520with%250A3D-GRAND%2520significantly%2520enhances%2520grounding%2520capabilities%2520and%2520reduces%250Ahallucinations%2520in%25203D-LLMs.%2520As%2520part%2520of%2520our%2520contributions%252C%2520we%2520propose%2520a%250Acomprehensive%2520benchmark%25203D-POPE%2520to%2520systematically%2520evaluate%2520hallucination%2520in%250A3D-LLMs%252C%2520enabling%2520fair%2520comparisons%2520among%2520future%2520models.%2520Our%2520experiments%250Ahighlight%2520a%2520scaling%2520effect%2520between%2520dataset%2520size%2520and%25203D-LLM%2520performance%252C%250Aemphasizing%2520the%2520critical%2520role%2520of%2520large-scale%25203D-text%2520datasets%2520in%2520advancing%250Aembodied%2520AI%2520research.%2520Notably%252C%2520our%2520results%2520demonstrate%2520early%2520signals%2520for%250Aeffective%2520sim-to-real%2520transfer%252C%2520indicating%2520that%2520models%2520trained%2520on%2520large%250Asynthetic%2520data%2520can%2520perform%2520well%2520on%2520real-world%25203D%2520scans.%2520Through%25203D-GRAND%2520and%250A3D-POPE%252C%2520we%2520aim%2520to%2520equip%2520the%2520embodied%2520AI%2520community%2520with%2520essential%2520resources%2520and%250Ainsights%252C%2520setting%2520the%2520stage%2520for%2520more%2520reliable%2520and%2520better-grounded%25203D-LLMs.%250AProject%2520website%253A%2520https%253A//3d-grand.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05132v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-GRAND%3A%20A%20Million-Scale%20Dataset%20for%203D-LLMs%20with%20Better%20Grounding%20and%0A%20%20Less%20Hallucination&entry.906535625=Jianing%20Yang%20and%20Xuweiyi%20Chen%20and%20Nikhil%20Madaan%20and%20Madhavan%20Iyengar%20and%20Shengyi%20Qian%20and%20David%20F.%20Fouhey%20and%20Joyce%20Chai&entry.1292438233=%20%20The%20integration%20of%20language%20and%203D%20perception%20is%20crucial%20for%20developing%0Aembodied%20agents%20and%20robots%20that%20comprehend%20and%20interact%20with%20the%20physical%0Aworld.%20While%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20language%0Aunderstanding%20and%20generation%20capabilities%2C%20their%20adaptation%20to%203D%20environments%0A%283D-LLMs%29%20remains%20in%20its%20early%20stages.%20A%20primary%20challenge%20is%20the%20absence%20of%0Alarge-scale%20datasets%20that%20provide%20dense%20grounding%20between%20language%20and%203D%0Ascenes.%20In%20this%20paper%2C%20we%20introduce%203D-GRAND%2C%20a%20pioneering%20large-scale%20dataset%0Acomprising%2040%2C087%20household%20scenes%20paired%20with%206.2%20million%20densely-grounded%0Ascene-language%20instructions.%20Our%20results%20show%20that%20instruction%20tuning%20with%0A3D-GRAND%20significantly%20enhances%20grounding%20capabilities%20and%20reduces%0Ahallucinations%20in%203D-LLMs.%20As%20part%20of%20our%20contributions%2C%20we%20propose%20a%0Acomprehensive%20benchmark%203D-POPE%20to%20systematically%20evaluate%20hallucination%20in%0A3D-LLMs%2C%20enabling%20fair%20comparisons%20among%20future%20models.%20Our%20experiments%0Ahighlight%20a%20scaling%20effect%20between%20dataset%20size%20and%203D-LLM%20performance%2C%0Aemphasizing%20the%20critical%20role%20of%20large-scale%203D-text%20datasets%20in%20advancing%0Aembodied%20AI%20research.%20Notably%2C%20our%20results%20demonstrate%20early%20signals%20for%0Aeffective%20sim-to-real%20transfer%2C%20indicating%20that%20models%20trained%20on%20large%0Asynthetic%20data%20can%20perform%20well%20on%20real-world%203D%20scans.%20Through%203D-GRAND%20and%0A3D-POPE%2C%20we%20aim%20to%20equip%20the%20embodied%20AI%20community%20with%20essential%20resources%20and%0Ainsights%2C%20setting%20the%20stage%20for%20more%20reliable%20and%20better-grounded%203D-LLMs.%0AProject%20website%3A%20https%3A//3d-grand.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05132v2&entry.124074799=Read"},
{"title": "DocSynthv2: A Practical Autoregressive Modeling for Document Generation", "author": "Sanket Biswas and Rajiv Jain and Vlad I. Morariu and Jiuxiang Gu and Puneet Mathur and Curtis Wigington and Tong Sun and Josep Llad\u00f3s", "abstract": "  While the generation of document layouts has been extensively explored,\ncomprehensive document generation encompassing both layout and content presents\na more complex challenge. This paper delves into this advanced domain,\nproposing a novel approach called DocSynthv2 through the development of a\nsimple yet effective autoregressive structured model. Our model, distinct in\nits integration of both layout and textual cues, marks a step beyond existing\nlayout-generation approaches. By focusing on the relationship between the\nstructural elements and the textual content within documents, we aim to\ngenerate cohesive and contextually relevant documents without any reliance on\nvisual components. Through experimental studies on our curated benchmark for\nthe new task, we demonstrate the ability of our model combining layout and\ntextual information in enhancing the generation quality and relevance of\ndocuments, opening new pathways for research in document creation and automated\ndesign. Our findings emphasize the effectiveness of autoregressive models in\nhandling complex document generation tasks.\n", "link": "http://arxiv.org/abs/2406.08354v1", "date": "2024-06-12", "relevancy": 2.2357, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5725}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5514}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DocSynthv2%3A%20A%20Practical%20Autoregressive%20Modeling%20for%20Document%20Generation&body=Title%3A%20DocSynthv2%3A%20A%20Practical%20Autoregressive%20Modeling%20for%20Document%20Generation%0AAuthor%3A%20Sanket%20Biswas%20and%20Rajiv%20Jain%20and%20Vlad%20I.%20Morariu%20and%20Jiuxiang%20Gu%20and%20Puneet%20Mathur%20and%20Curtis%20Wigington%20and%20Tong%20Sun%20and%20Josep%20Llad%C3%B3s%0AAbstract%3A%20%20%20While%20the%20generation%20of%20document%20layouts%20has%20been%20extensively%20explored%2C%0Acomprehensive%20document%20generation%20encompassing%20both%20layout%20and%20content%20presents%0Aa%20more%20complex%20challenge.%20This%20paper%20delves%20into%20this%20advanced%20domain%2C%0Aproposing%20a%20novel%20approach%20called%20DocSynthv2%20through%20the%20development%20of%20a%0Asimple%20yet%20effective%20autoregressive%20structured%20model.%20Our%20model%2C%20distinct%20in%0Aits%20integration%20of%20both%20layout%20and%20textual%20cues%2C%20marks%20a%20step%20beyond%20existing%0Alayout-generation%20approaches.%20By%20focusing%20on%20the%20relationship%20between%20the%0Astructural%20elements%20and%20the%20textual%20content%20within%20documents%2C%20we%20aim%20to%0Agenerate%20cohesive%20and%20contextually%20relevant%20documents%20without%20any%20reliance%20on%0Avisual%20components.%20Through%20experimental%20studies%20on%20our%20curated%20benchmark%20for%0Athe%20new%20task%2C%20we%20demonstrate%20the%20ability%20of%20our%20model%20combining%20layout%20and%0Atextual%20information%20in%20enhancing%20the%20generation%20quality%20and%20relevance%20of%0Adocuments%2C%20opening%20new%20pathways%20for%20research%20in%20document%20creation%20and%20automated%0Adesign.%20Our%20findings%20emphasize%20the%20effectiveness%20of%20autoregressive%20models%20in%0Ahandling%20complex%20document%20generation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08354v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocSynthv2%253A%2520A%2520Practical%2520Autoregressive%2520Modeling%2520for%2520Document%2520Generation%26entry.906535625%3DSanket%2520Biswas%2520and%2520Rajiv%2520Jain%2520and%2520Vlad%2520I.%2520Morariu%2520and%2520Jiuxiang%2520Gu%2520and%2520Puneet%2520Mathur%2520and%2520Curtis%2520Wigington%2520and%2520Tong%2520Sun%2520and%2520Josep%2520Llad%25C3%25B3s%26entry.1292438233%3D%2520%2520While%2520the%2520generation%2520of%2520document%2520layouts%2520has%2520been%2520extensively%2520explored%252C%250Acomprehensive%2520document%2520generation%2520encompassing%2520both%2520layout%2520and%2520content%2520presents%250Aa%2520more%2520complex%2520challenge.%2520This%2520paper%2520delves%2520into%2520this%2520advanced%2520domain%252C%250Aproposing%2520a%2520novel%2520approach%2520called%2520DocSynthv2%2520through%2520the%2520development%2520of%2520a%250Asimple%2520yet%2520effective%2520autoregressive%2520structured%2520model.%2520Our%2520model%252C%2520distinct%2520in%250Aits%2520integration%2520of%2520both%2520layout%2520and%2520textual%2520cues%252C%2520marks%2520a%2520step%2520beyond%2520existing%250Alayout-generation%2520approaches.%2520By%2520focusing%2520on%2520the%2520relationship%2520between%2520the%250Astructural%2520elements%2520and%2520the%2520textual%2520content%2520within%2520documents%252C%2520we%2520aim%2520to%250Agenerate%2520cohesive%2520and%2520contextually%2520relevant%2520documents%2520without%2520any%2520reliance%2520on%250Avisual%2520components.%2520Through%2520experimental%2520studies%2520on%2520our%2520curated%2520benchmark%2520for%250Athe%2520new%2520task%252C%2520we%2520demonstrate%2520the%2520ability%2520of%2520our%2520model%2520combining%2520layout%2520and%250Atextual%2520information%2520in%2520enhancing%2520the%2520generation%2520quality%2520and%2520relevance%2520of%250Adocuments%252C%2520opening%2520new%2520pathways%2520for%2520research%2520in%2520document%2520creation%2520and%2520automated%250Adesign.%2520Our%2520findings%2520emphasize%2520the%2520effectiveness%2520of%2520autoregressive%2520models%2520in%250Ahandling%2520complex%2520document%2520generation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08354v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DocSynthv2%3A%20A%20Practical%20Autoregressive%20Modeling%20for%20Document%20Generation&entry.906535625=Sanket%20Biswas%20and%20Rajiv%20Jain%20and%20Vlad%20I.%20Morariu%20and%20Jiuxiang%20Gu%20and%20Puneet%20Mathur%20and%20Curtis%20Wigington%20and%20Tong%20Sun%20and%20Josep%20Llad%C3%B3s&entry.1292438233=%20%20While%20the%20generation%20of%20document%20layouts%20has%20been%20extensively%20explored%2C%0Acomprehensive%20document%20generation%20encompassing%20both%20layout%20and%20content%20presents%0Aa%20more%20complex%20challenge.%20This%20paper%20delves%20into%20this%20advanced%20domain%2C%0Aproposing%20a%20novel%20approach%20called%20DocSynthv2%20through%20the%20development%20of%20a%0Asimple%20yet%20effective%20autoregressive%20structured%20model.%20Our%20model%2C%20distinct%20in%0Aits%20integration%20of%20both%20layout%20and%20textual%20cues%2C%20marks%20a%20step%20beyond%20existing%0Alayout-generation%20approaches.%20By%20focusing%20on%20the%20relationship%20between%20the%0Astructural%20elements%20and%20the%20textual%20content%20within%20documents%2C%20we%20aim%20to%0Agenerate%20cohesive%20and%20contextually%20relevant%20documents%20without%20any%20reliance%20on%0Avisual%20components.%20Through%20experimental%20studies%20on%20our%20curated%20benchmark%20for%0Athe%20new%20task%2C%20we%20demonstrate%20the%20ability%20of%20our%20model%20combining%20layout%20and%0Atextual%20information%20in%20enhancing%20the%20generation%20quality%20and%20relevance%20of%0Adocuments%2C%20opening%20new%20pathways%20for%20research%20in%20document%20creation%20and%20automated%0Adesign.%20Our%20findings%20emphasize%20the%20effectiveness%20of%20autoregressive%20models%20in%0Ahandling%20complex%20document%20generation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08354v1&entry.124074799=Read"},
{"title": "Exploring Frequency-Inspired Optimization in Transformer for Efficient\n  Single Image Super-Resolution", "author": "Ao Li and Le Zhang and Yun Liu and Ce Zhu", "abstract": "  Transformer-based methods have exhibited remarkable potential in single image\nsuper-resolution (SISR) by effectively extracting long-range dependencies.\nHowever, most of the current research in this area has prioritized the design\nof transformer blocks to capture global information, while overlooking the\nimportance of incorporating high-frequency priors, which we believe could be\nbeneficial. In our study, we conducted a series of experiments and found that\ntransformer structures are more adept at capturing low-frequency information,\nbut have limited capacity in constructing high-frequency representations when\ncompared to their convolutional counterparts. Our proposed solution, the\ncross-refinement adaptive feature modulation transformer (CRAFT), integrates\nthe strengths of both convolutional and transformer structures. It comprises\nthree key components: the high-frequency enhancement residual block (HFERB) for\nextracting high-frequency information, the shift rectangle window attention\nblock (SRWAB) for capturing global information, and the hybrid fusion block\n(HFB) for refining the global representation. To tackle the inherent\nintricacies of transformer structures, we introduce a frequency-guided\npost-training quantization (PTQ) method aimed at enhancing CRAFT's efficiency.\nThese strategies incorporate adaptive dual clipping and boundary refinement. To\nfurther amplify the versatility of our proposed approach, we extend our PTQ\nstrategy to function as a general quantization method for transformer-based\nSISR techniques. Our experimental findings showcase CRAFT's superiority over\ncurrent state-of-the-art methods, both in full-precision and quantization\nscenarios. These results underscore the efficacy and universality of our PTQ\nstrategy.\n", "link": "http://arxiv.org/abs/2308.05022v3", "date": "2024-06-12", "relevancy": 2.2346, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6277}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5543}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Frequency-Inspired%20Optimization%20in%20Transformer%20for%20Efficient%0A%20%20Single%20Image%20Super-Resolution&body=Title%3A%20Exploring%20Frequency-Inspired%20Optimization%20in%20Transformer%20for%20Efficient%0A%20%20Single%20Image%20Super-Resolution%0AAuthor%3A%20Ao%20Li%20and%20Le%20Zhang%20and%20Yun%20Liu%20and%20Ce%20Zhu%0AAbstract%3A%20%20%20Transformer-based%20methods%20have%20exhibited%20remarkable%20potential%20in%20single%20image%0Asuper-resolution%20%28SISR%29%20by%20effectively%20extracting%20long-range%20dependencies.%0AHowever%2C%20most%20of%20the%20current%20research%20in%20this%20area%20has%20prioritized%20the%20design%0Aof%20transformer%20blocks%20to%20capture%20global%20information%2C%20while%20overlooking%20the%0Aimportance%20of%20incorporating%20high-frequency%20priors%2C%20which%20we%20believe%20could%20be%0Abeneficial.%20In%20our%20study%2C%20we%20conducted%20a%20series%20of%20experiments%20and%20found%20that%0Atransformer%20structures%20are%20more%20adept%20at%20capturing%20low-frequency%20information%2C%0Abut%20have%20limited%20capacity%20in%20constructing%20high-frequency%20representations%20when%0Acompared%20to%20their%20convolutional%20counterparts.%20Our%20proposed%20solution%2C%20the%0Across-refinement%20adaptive%20feature%20modulation%20transformer%20%28CRAFT%29%2C%20integrates%0Athe%20strengths%20of%20both%20convolutional%20and%20transformer%20structures.%20It%20comprises%0Athree%20key%20components%3A%20the%20high-frequency%20enhancement%20residual%20block%20%28HFERB%29%20for%0Aextracting%20high-frequency%20information%2C%20the%20shift%20rectangle%20window%20attention%0Ablock%20%28SRWAB%29%20for%20capturing%20global%20information%2C%20and%20the%20hybrid%20fusion%20block%0A%28HFB%29%20for%20refining%20the%20global%20representation.%20To%20tackle%20the%20inherent%0Aintricacies%20of%20transformer%20structures%2C%20we%20introduce%20a%20frequency-guided%0Apost-training%20quantization%20%28PTQ%29%20method%20aimed%20at%20enhancing%20CRAFT%27s%20efficiency.%0AThese%20strategies%20incorporate%20adaptive%20dual%20clipping%20and%20boundary%20refinement.%20To%0Afurther%20amplify%20the%20versatility%20of%20our%20proposed%20approach%2C%20we%20extend%20our%20PTQ%0Astrategy%20to%20function%20as%20a%20general%20quantization%20method%20for%20transformer-based%0ASISR%20techniques.%20Our%20experimental%20findings%20showcase%20CRAFT%27s%20superiority%20over%0Acurrent%20state-of-the-art%20methods%2C%20both%20in%20full-precision%20and%20quantization%0Ascenarios.%20These%20results%20underscore%20the%20efficacy%20and%20universality%20of%20our%20PTQ%0Astrategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.05022v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Frequency-Inspired%2520Optimization%2520in%2520Transformer%2520for%2520Efficient%250A%2520%2520Single%2520Image%2520Super-Resolution%26entry.906535625%3DAo%2520Li%2520and%2520Le%2520Zhang%2520and%2520Yun%2520Liu%2520and%2520Ce%2520Zhu%26entry.1292438233%3D%2520%2520Transformer-based%2520methods%2520have%2520exhibited%2520remarkable%2520potential%2520in%2520single%2520image%250Asuper-resolution%2520%2528SISR%2529%2520by%2520effectively%2520extracting%2520long-range%2520dependencies.%250AHowever%252C%2520most%2520of%2520the%2520current%2520research%2520in%2520this%2520area%2520has%2520prioritized%2520the%2520design%250Aof%2520transformer%2520blocks%2520to%2520capture%2520global%2520information%252C%2520while%2520overlooking%2520the%250Aimportance%2520of%2520incorporating%2520high-frequency%2520priors%252C%2520which%2520we%2520believe%2520could%2520be%250Abeneficial.%2520In%2520our%2520study%252C%2520we%2520conducted%2520a%2520series%2520of%2520experiments%2520and%2520found%2520that%250Atransformer%2520structures%2520are%2520more%2520adept%2520at%2520capturing%2520low-frequency%2520information%252C%250Abut%2520have%2520limited%2520capacity%2520in%2520constructing%2520high-frequency%2520representations%2520when%250Acompared%2520to%2520their%2520convolutional%2520counterparts.%2520Our%2520proposed%2520solution%252C%2520the%250Across-refinement%2520adaptive%2520feature%2520modulation%2520transformer%2520%2528CRAFT%2529%252C%2520integrates%250Athe%2520strengths%2520of%2520both%2520convolutional%2520and%2520transformer%2520structures.%2520It%2520comprises%250Athree%2520key%2520components%253A%2520the%2520high-frequency%2520enhancement%2520residual%2520block%2520%2528HFERB%2529%2520for%250Aextracting%2520high-frequency%2520information%252C%2520the%2520shift%2520rectangle%2520window%2520attention%250Ablock%2520%2528SRWAB%2529%2520for%2520capturing%2520global%2520information%252C%2520and%2520the%2520hybrid%2520fusion%2520block%250A%2528HFB%2529%2520for%2520refining%2520the%2520global%2520representation.%2520To%2520tackle%2520the%2520inherent%250Aintricacies%2520of%2520transformer%2520structures%252C%2520we%2520introduce%2520a%2520frequency-guided%250Apost-training%2520quantization%2520%2528PTQ%2529%2520method%2520aimed%2520at%2520enhancing%2520CRAFT%2527s%2520efficiency.%250AThese%2520strategies%2520incorporate%2520adaptive%2520dual%2520clipping%2520and%2520boundary%2520refinement.%2520To%250Afurther%2520amplify%2520the%2520versatility%2520of%2520our%2520proposed%2520approach%252C%2520we%2520extend%2520our%2520PTQ%250Astrategy%2520to%2520function%2520as%2520a%2520general%2520quantization%2520method%2520for%2520transformer-based%250ASISR%2520techniques.%2520Our%2520experimental%2520findings%2520showcase%2520CRAFT%2527s%2520superiority%2520over%250Acurrent%2520state-of-the-art%2520methods%252C%2520both%2520in%2520full-precision%2520and%2520quantization%250Ascenarios.%2520These%2520results%2520underscore%2520the%2520efficacy%2520and%2520universality%2520of%2520our%2520PTQ%250Astrategy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.05022v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Frequency-Inspired%20Optimization%20in%20Transformer%20for%20Efficient%0A%20%20Single%20Image%20Super-Resolution&entry.906535625=Ao%20Li%20and%20Le%20Zhang%20and%20Yun%20Liu%20and%20Ce%20Zhu&entry.1292438233=%20%20Transformer-based%20methods%20have%20exhibited%20remarkable%20potential%20in%20single%20image%0Asuper-resolution%20%28SISR%29%20by%20effectively%20extracting%20long-range%20dependencies.%0AHowever%2C%20most%20of%20the%20current%20research%20in%20this%20area%20has%20prioritized%20the%20design%0Aof%20transformer%20blocks%20to%20capture%20global%20information%2C%20while%20overlooking%20the%0Aimportance%20of%20incorporating%20high-frequency%20priors%2C%20which%20we%20believe%20could%20be%0Abeneficial.%20In%20our%20study%2C%20we%20conducted%20a%20series%20of%20experiments%20and%20found%20that%0Atransformer%20structures%20are%20more%20adept%20at%20capturing%20low-frequency%20information%2C%0Abut%20have%20limited%20capacity%20in%20constructing%20high-frequency%20representations%20when%0Acompared%20to%20their%20convolutional%20counterparts.%20Our%20proposed%20solution%2C%20the%0Across-refinement%20adaptive%20feature%20modulation%20transformer%20%28CRAFT%29%2C%20integrates%0Athe%20strengths%20of%20both%20convolutional%20and%20transformer%20structures.%20It%20comprises%0Athree%20key%20components%3A%20the%20high-frequency%20enhancement%20residual%20block%20%28HFERB%29%20for%0Aextracting%20high-frequency%20information%2C%20the%20shift%20rectangle%20window%20attention%0Ablock%20%28SRWAB%29%20for%20capturing%20global%20information%2C%20and%20the%20hybrid%20fusion%20block%0A%28HFB%29%20for%20refining%20the%20global%20representation.%20To%20tackle%20the%20inherent%0Aintricacies%20of%20transformer%20structures%2C%20we%20introduce%20a%20frequency-guided%0Apost-training%20quantization%20%28PTQ%29%20method%20aimed%20at%20enhancing%20CRAFT%27s%20efficiency.%0AThese%20strategies%20incorporate%20adaptive%20dual%20clipping%20and%20boundary%20refinement.%20To%0Afurther%20amplify%20the%20versatility%20of%20our%20proposed%20approach%2C%20we%20extend%20our%20PTQ%0Astrategy%20to%20function%20as%20a%20general%20quantization%20method%20for%20transformer-based%0ASISR%20techniques.%20Our%20experimental%20findings%20showcase%20CRAFT%27s%20superiority%20over%0Acurrent%20state-of-the-art%20methods%2C%20both%20in%20full-precision%20and%20quantization%0Ascenarios.%20These%20results%20underscore%20the%20efficacy%20and%20universality%20of%20our%20PTQ%0Astrategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.05022v3&entry.124074799=Read"},
{"title": "Enhancing End-to-End Autonomous Driving with Latent World Model", "author": "Yingyan Li and Lue Fan and Jiawei He and Yuqi Wang and Yuntao Chen and Zhaoxiang Zhang and Tieniu Tan", "abstract": "  End-to-end autonomous driving has garnered widespread attention. Current\nend-to-end approaches largely rely on the supervision from perception tasks\nsuch as detection, tracking, and map segmentation to aid in learning scene\nrepresentations. However, these methods require extensive annotations,\nhindering the data scalability. To address this challenge, we propose a novel\nself-supervised method to enhance end-to-end driving without the need for\ncostly labels. Specifically, our framework \\textbf{LAW} uses a LAtent World\nmodel to predict future latent features based on the predicted ego actions and\nthe latent feature of the current frame. The predicted latent features are\nsupervised by the actually observed features in the future. This supervision\njointly optimizes the latent feature learning and action prediction, which\ngreatly enhances the driving performance. As a result, our approach achieves\nstate-of-the-art performance in both open-loop and closed-loop benchmarks\nwithout costly annotations.\n", "link": "http://arxiv.org/abs/2406.08481v1", "date": "2024-06-12", "relevancy": 2.2261, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.566}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5532}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20End-to-End%20Autonomous%20Driving%20with%20Latent%20World%20Model&body=Title%3A%20Enhancing%20End-to-End%20Autonomous%20Driving%20with%20Latent%20World%20Model%0AAuthor%3A%20Yingyan%20Li%20and%20Lue%20Fan%20and%20Jiawei%20He%20and%20Yuqi%20Wang%20and%20Yuntao%20Chen%20and%20Zhaoxiang%20Zhang%20and%20Tieniu%20Tan%0AAbstract%3A%20%20%20End-to-end%20autonomous%20driving%20has%20garnered%20widespread%20attention.%20Current%0Aend-to-end%20approaches%20largely%20rely%20on%20the%20supervision%20from%20perception%20tasks%0Asuch%20as%20detection%2C%20tracking%2C%20and%20map%20segmentation%20to%20aid%20in%20learning%20scene%0Arepresentations.%20However%2C%20these%20methods%20require%20extensive%20annotations%2C%0Ahindering%20the%20data%20scalability.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%0Aself-supervised%20method%20to%20enhance%20end-to-end%20driving%20without%20the%20need%20for%0Acostly%20labels.%20Specifically%2C%20our%20framework%20%5Ctextbf%7BLAW%7D%20uses%20a%20LAtent%20World%0Amodel%20to%20predict%20future%20latent%20features%20based%20on%20the%20predicted%20ego%20actions%20and%0Athe%20latent%20feature%20of%20the%20current%20frame.%20The%20predicted%20latent%20features%20are%0Asupervised%20by%20the%20actually%20observed%20features%20in%20the%20future.%20This%20supervision%0Ajointly%20optimizes%20the%20latent%20feature%20learning%20and%20action%20prediction%2C%20which%0Agreatly%20enhances%20the%20driving%20performance.%20As%20a%20result%2C%20our%20approach%20achieves%0Astate-of-the-art%20performance%20in%20both%20open-loop%20and%20closed-loop%20benchmarks%0Awithout%20costly%20annotations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520End-to-End%2520Autonomous%2520Driving%2520with%2520Latent%2520World%2520Model%26entry.906535625%3DYingyan%2520Li%2520and%2520Lue%2520Fan%2520and%2520Jiawei%2520He%2520and%2520Yuqi%2520Wang%2520and%2520Yuntao%2520Chen%2520and%2520Zhaoxiang%2520Zhang%2520and%2520Tieniu%2520Tan%26entry.1292438233%3D%2520%2520End-to-end%2520autonomous%2520driving%2520has%2520garnered%2520widespread%2520attention.%2520Current%250Aend-to-end%2520approaches%2520largely%2520rely%2520on%2520the%2520supervision%2520from%2520perception%2520tasks%250Asuch%2520as%2520detection%252C%2520tracking%252C%2520and%2520map%2520segmentation%2520to%2520aid%2520in%2520learning%2520scene%250Arepresentations.%2520However%252C%2520these%2520methods%2520require%2520extensive%2520annotations%252C%250Ahindering%2520the%2520data%2520scalability.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%250Aself-supervised%2520method%2520to%2520enhance%2520end-to-end%2520driving%2520without%2520the%2520need%2520for%250Acostly%2520labels.%2520Specifically%252C%2520our%2520framework%2520%255Ctextbf%257BLAW%257D%2520uses%2520a%2520LAtent%2520World%250Amodel%2520to%2520predict%2520future%2520latent%2520features%2520based%2520on%2520the%2520predicted%2520ego%2520actions%2520and%250Athe%2520latent%2520feature%2520of%2520the%2520current%2520frame.%2520The%2520predicted%2520latent%2520features%2520are%250Asupervised%2520by%2520the%2520actually%2520observed%2520features%2520in%2520the%2520future.%2520This%2520supervision%250Ajointly%2520optimizes%2520the%2520latent%2520feature%2520learning%2520and%2520action%2520prediction%252C%2520which%250Agreatly%2520enhances%2520the%2520driving%2520performance.%2520As%2520a%2520result%252C%2520our%2520approach%2520achieves%250Astate-of-the-art%2520performance%2520in%2520both%2520open-loop%2520and%2520closed-loop%2520benchmarks%250Awithout%2520costly%2520annotations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20End-to-End%20Autonomous%20Driving%20with%20Latent%20World%20Model&entry.906535625=Yingyan%20Li%20and%20Lue%20Fan%20and%20Jiawei%20He%20and%20Yuqi%20Wang%20and%20Yuntao%20Chen%20and%20Zhaoxiang%20Zhang%20and%20Tieniu%20Tan&entry.1292438233=%20%20End-to-end%20autonomous%20driving%20has%20garnered%20widespread%20attention.%20Current%0Aend-to-end%20approaches%20largely%20rely%20on%20the%20supervision%20from%20perception%20tasks%0Asuch%20as%20detection%2C%20tracking%2C%20and%20map%20segmentation%20to%20aid%20in%20learning%20scene%0Arepresentations.%20However%2C%20these%20methods%20require%20extensive%20annotations%2C%0Ahindering%20the%20data%20scalability.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%0Aself-supervised%20method%20to%20enhance%20end-to-end%20driving%20without%20the%20need%20for%0Acostly%20labels.%20Specifically%2C%20our%20framework%20%5Ctextbf%7BLAW%7D%20uses%20a%20LAtent%20World%0Amodel%20to%20predict%20future%20latent%20features%20based%20on%20the%20predicted%20ego%20actions%20and%0Athe%20latent%20feature%20of%20the%20current%20frame.%20The%20predicted%20latent%20features%20are%0Asupervised%20by%20the%20actually%20observed%20features%20in%20the%20future.%20This%20supervision%0Ajointly%20optimizes%20the%20latent%20feature%20learning%20and%20action%20prediction%2C%20which%0Agreatly%20enhances%20the%20driving%20performance.%20As%20a%20result%2C%20our%20approach%20achieves%0Astate-of-the-art%20performance%20in%20both%20open-loop%20and%20closed-loop%20benchmarks%0Awithout%20costly%20annotations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08481v1&entry.124074799=Read"},
{"title": "ECG Classification System for Arrhythmia Detection Using Convolutional\n  Neural Networks", "author": "Aryan Odugoudar and Jaskaran Singh Walia", "abstract": "  Arrhythmia is just one of the many cardiovascular illnesses that have been\nextensively studied throughout the years. Using multi-lead ECG data, this\nresearch describes a deep learning (DL) pipeline technique based on\nconvolutional neural network (CNN) algorithms to detect cardiovascular lar\narrhythmia in patients. The suggested model architecture has hidden layers with\na residual block in addition to the input and output layers. In this study, the\nclassification of the ECG signals into five main groups, namely: Left Bundle\nBranch Block (LBBB), Right Bundle Branch Block (RBBB), Atrial Premature\nContraction (APC), Premature Ventricular Contraction (PVC), and Normal Beat\n(N), are performed. Using the MIT-BIH arrhythmia dataset, we assessed the\nsuggested technique. The findings show that our suggested strategy classified\n15,000 cases with a high accuracy of 98.2%\n", "link": "http://arxiv.org/abs/2303.03660v2", "date": "2024-06-12", "relevancy": 2.2179, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4775}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4322}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ECG%20Classification%20System%20for%20Arrhythmia%20Detection%20Using%20Convolutional%0A%20%20Neural%20Networks&body=Title%3A%20ECG%20Classification%20System%20for%20Arrhythmia%20Detection%20Using%20Convolutional%0A%20%20Neural%20Networks%0AAuthor%3A%20Aryan%20Odugoudar%20and%20Jaskaran%20Singh%20Walia%0AAbstract%3A%20%20%20Arrhythmia%20is%20just%20one%20of%20the%20many%20cardiovascular%20illnesses%20that%20have%20been%0Aextensively%20studied%20throughout%20the%20years.%20Using%20multi-lead%20ECG%20data%2C%20this%0Aresearch%20describes%20a%20deep%20learning%20%28DL%29%20pipeline%20technique%20based%20on%0Aconvolutional%20neural%20network%20%28CNN%29%20algorithms%20to%20detect%20cardiovascular%20lar%0Aarrhythmia%20in%20patients.%20The%20suggested%20model%20architecture%20has%20hidden%20layers%20with%0Aa%20residual%20block%20in%20addition%20to%20the%20input%20and%20output%20layers.%20In%20this%20study%2C%20the%0Aclassification%20of%20the%20ECG%20signals%20into%20five%20main%20groups%2C%20namely%3A%20Left%20Bundle%0ABranch%20Block%20%28LBBB%29%2C%20Right%20Bundle%20Branch%20Block%20%28RBBB%29%2C%20Atrial%20Premature%0AContraction%20%28APC%29%2C%20Premature%20Ventricular%20Contraction%20%28PVC%29%2C%20and%20Normal%20Beat%0A%28N%29%2C%20are%20performed.%20Using%20the%20MIT-BIH%20arrhythmia%20dataset%2C%20we%20assessed%20the%0Asuggested%20technique.%20The%20findings%20show%20that%20our%20suggested%20strategy%20classified%0A15%2C000%20cases%20with%20a%20high%20accuracy%20of%2098.2%25%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.03660v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DECG%2520Classification%2520System%2520for%2520Arrhythmia%2520Detection%2520Using%2520Convolutional%250A%2520%2520Neural%2520Networks%26entry.906535625%3DAryan%2520Odugoudar%2520and%2520Jaskaran%2520Singh%2520Walia%26entry.1292438233%3D%2520%2520Arrhythmia%2520is%2520just%2520one%2520of%2520the%2520many%2520cardiovascular%2520illnesses%2520that%2520have%2520been%250Aextensively%2520studied%2520throughout%2520the%2520years.%2520Using%2520multi-lead%2520ECG%2520data%252C%2520this%250Aresearch%2520describes%2520a%2520deep%2520learning%2520%2528DL%2529%2520pipeline%2520technique%2520based%2520on%250Aconvolutional%2520neural%2520network%2520%2528CNN%2529%2520algorithms%2520to%2520detect%2520cardiovascular%2520lar%250Aarrhythmia%2520in%2520patients.%2520The%2520suggested%2520model%2520architecture%2520has%2520hidden%2520layers%2520with%250Aa%2520residual%2520block%2520in%2520addition%2520to%2520the%2520input%2520and%2520output%2520layers.%2520In%2520this%2520study%252C%2520the%250Aclassification%2520of%2520the%2520ECG%2520signals%2520into%2520five%2520main%2520groups%252C%2520namely%253A%2520Left%2520Bundle%250ABranch%2520Block%2520%2528LBBB%2529%252C%2520Right%2520Bundle%2520Branch%2520Block%2520%2528RBBB%2529%252C%2520Atrial%2520Premature%250AContraction%2520%2528APC%2529%252C%2520Premature%2520Ventricular%2520Contraction%2520%2528PVC%2529%252C%2520and%2520Normal%2520Beat%250A%2528N%2529%252C%2520are%2520performed.%2520Using%2520the%2520MIT-BIH%2520arrhythmia%2520dataset%252C%2520we%2520assessed%2520the%250Asuggested%2520technique.%2520The%2520findings%2520show%2520that%2520our%2520suggested%2520strategy%2520classified%250A15%252C000%2520cases%2520with%2520a%2520high%2520accuracy%2520of%252098.2%2525%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.03660v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECG%20Classification%20System%20for%20Arrhythmia%20Detection%20Using%20Convolutional%0A%20%20Neural%20Networks&entry.906535625=Aryan%20Odugoudar%20and%20Jaskaran%20Singh%20Walia&entry.1292438233=%20%20Arrhythmia%20is%20just%20one%20of%20the%20many%20cardiovascular%20illnesses%20that%20have%20been%0Aextensively%20studied%20throughout%20the%20years.%20Using%20multi-lead%20ECG%20data%2C%20this%0Aresearch%20describes%20a%20deep%20learning%20%28DL%29%20pipeline%20technique%20based%20on%0Aconvolutional%20neural%20network%20%28CNN%29%20algorithms%20to%20detect%20cardiovascular%20lar%0Aarrhythmia%20in%20patients.%20The%20suggested%20model%20architecture%20has%20hidden%20layers%20with%0Aa%20residual%20block%20in%20addition%20to%20the%20input%20and%20output%20layers.%20In%20this%20study%2C%20the%0Aclassification%20of%20the%20ECG%20signals%20into%20five%20main%20groups%2C%20namely%3A%20Left%20Bundle%0ABranch%20Block%20%28LBBB%29%2C%20Right%20Bundle%20Branch%20Block%20%28RBBB%29%2C%20Atrial%20Premature%0AContraction%20%28APC%29%2C%20Premature%20Ventricular%20Contraction%20%28PVC%29%2C%20and%20Normal%20Beat%0A%28N%29%2C%20are%20performed.%20Using%20the%20MIT-BIH%20arrhythmia%20dataset%2C%20we%20assessed%20the%0Asuggested%20technique.%20The%20findings%20show%20that%20our%20suggested%20strategy%20classified%0A15%2C000%20cases%20with%20a%20high%20accuracy%20of%2098.2%25%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.03660v2&entry.124074799=Read"},
{"title": "A New Class Biorthogonal Spline Wavelet for Image Edge Detection", "author": "Dujuan Zhou and Zizhao Yuan", "abstract": "  Spline wavelets have shown favorable characteristics for localizing in both\ntime and frequency. In this paper, we propose a new biorthogonal cubic special\nspline wavelet (BCSSW), based on the Cohen-Daubechies-Feauveau wavelet\nconstruction method and the cubic special spline algorithm. BCSSW has better\nproperties in compact support, symmetry, and frequency domain characteristics.\nHowever, current mainstream detection operators usually ignore the uncertain\nrepresentation of regional pixels and global structures. To solve these\nproblems, we propose a structural uncertainty-aware and multi-structure\noperator fusion detection algorithm (EDBSW) based on a new BCSSW spline\nwavelet. By constructing a spline wavelet that efficiently handles edge\neffects, we utilize structural uncertainty-aware modulus maxima to detect\nhighly uncertain edge samples. The proposed wavelet detection operator utilizes\nthe multi-structure morphological operator and fusion reconstruction strategy\nto effectively address anti-noise processing and edge information of different\nfrequencies. Numerous experiments have demonstrated its excellent performance\nin reducing noise and capturing edge structure details.\n", "link": "http://arxiv.org/abs/2406.08285v1", "date": "2024-06-12", "relevancy": 2.2137, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.449}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4405}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20New%20Class%20Biorthogonal%20Spline%20Wavelet%20for%20Image%20Edge%20Detection&body=Title%3A%20A%20New%20Class%20Biorthogonal%20Spline%20Wavelet%20for%20Image%20Edge%20Detection%0AAuthor%3A%20Dujuan%20Zhou%20and%20Zizhao%20Yuan%0AAbstract%3A%20%20%20Spline%20wavelets%20have%20shown%20favorable%20characteristics%20for%20localizing%20in%20both%0Atime%20and%20frequency.%20In%20this%20paper%2C%20we%20propose%20a%20new%20biorthogonal%20cubic%20special%0Aspline%20wavelet%20%28BCSSW%29%2C%20based%20on%20the%20Cohen-Daubechies-Feauveau%20wavelet%0Aconstruction%20method%20and%20the%20cubic%20special%20spline%20algorithm.%20BCSSW%20has%20better%0Aproperties%20in%20compact%20support%2C%20symmetry%2C%20and%20frequency%20domain%20characteristics.%0AHowever%2C%20current%20mainstream%20detection%20operators%20usually%20ignore%20the%20uncertain%0Arepresentation%20of%20regional%20pixels%20and%20global%20structures.%20To%20solve%20these%0Aproblems%2C%20we%20propose%20a%20structural%20uncertainty-aware%20and%20multi-structure%0Aoperator%20fusion%20detection%20algorithm%20%28EDBSW%29%20based%20on%20a%20new%20BCSSW%20spline%0Awavelet.%20By%20constructing%20a%20spline%20wavelet%20that%20efficiently%20handles%20edge%0Aeffects%2C%20we%20utilize%20structural%20uncertainty-aware%20modulus%20maxima%20to%20detect%0Ahighly%20uncertain%20edge%20samples.%20The%20proposed%20wavelet%20detection%20operator%20utilizes%0Athe%20multi-structure%20morphological%20operator%20and%20fusion%20reconstruction%20strategy%0Ato%20effectively%20address%20anti-noise%20processing%20and%20edge%20information%20of%20different%0Afrequencies.%20Numerous%20experiments%20have%20demonstrated%20its%20excellent%20performance%0Ain%20reducing%20noise%20and%20capturing%20edge%20structure%20details.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08285v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520New%2520Class%2520Biorthogonal%2520Spline%2520Wavelet%2520for%2520Image%2520Edge%2520Detection%26entry.906535625%3DDujuan%2520Zhou%2520and%2520Zizhao%2520Yuan%26entry.1292438233%3D%2520%2520Spline%2520wavelets%2520have%2520shown%2520favorable%2520characteristics%2520for%2520localizing%2520in%2520both%250Atime%2520and%2520frequency.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520biorthogonal%2520cubic%2520special%250Aspline%2520wavelet%2520%2528BCSSW%2529%252C%2520based%2520on%2520the%2520Cohen-Daubechies-Feauveau%2520wavelet%250Aconstruction%2520method%2520and%2520the%2520cubic%2520special%2520spline%2520algorithm.%2520BCSSW%2520has%2520better%250Aproperties%2520in%2520compact%2520support%252C%2520symmetry%252C%2520and%2520frequency%2520domain%2520characteristics.%250AHowever%252C%2520current%2520mainstream%2520detection%2520operators%2520usually%2520ignore%2520the%2520uncertain%250Arepresentation%2520of%2520regional%2520pixels%2520and%2520global%2520structures.%2520To%2520solve%2520these%250Aproblems%252C%2520we%2520propose%2520a%2520structural%2520uncertainty-aware%2520and%2520multi-structure%250Aoperator%2520fusion%2520detection%2520algorithm%2520%2528EDBSW%2529%2520based%2520on%2520a%2520new%2520BCSSW%2520spline%250Awavelet.%2520By%2520constructing%2520a%2520spline%2520wavelet%2520that%2520efficiently%2520handles%2520edge%250Aeffects%252C%2520we%2520utilize%2520structural%2520uncertainty-aware%2520modulus%2520maxima%2520to%2520detect%250Ahighly%2520uncertain%2520edge%2520samples.%2520The%2520proposed%2520wavelet%2520detection%2520operator%2520utilizes%250Athe%2520multi-structure%2520morphological%2520operator%2520and%2520fusion%2520reconstruction%2520strategy%250Ato%2520effectively%2520address%2520anti-noise%2520processing%2520and%2520edge%2520information%2520of%2520different%250Afrequencies.%2520Numerous%2520experiments%2520have%2520demonstrated%2520its%2520excellent%2520performance%250Ain%2520reducing%2520noise%2520and%2520capturing%2520edge%2520structure%2520details.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08285v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20New%20Class%20Biorthogonal%20Spline%20Wavelet%20for%20Image%20Edge%20Detection&entry.906535625=Dujuan%20Zhou%20and%20Zizhao%20Yuan&entry.1292438233=%20%20Spline%20wavelets%20have%20shown%20favorable%20characteristics%20for%20localizing%20in%20both%0Atime%20and%20frequency.%20In%20this%20paper%2C%20we%20propose%20a%20new%20biorthogonal%20cubic%20special%0Aspline%20wavelet%20%28BCSSW%29%2C%20based%20on%20the%20Cohen-Daubechies-Feauveau%20wavelet%0Aconstruction%20method%20and%20the%20cubic%20special%20spline%20algorithm.%20BCSSW%20has%20better%0Aproperties%20in%20compact%20support%2C%20symmetry%2C%20and%20frequency%20domain%20characteristics.%0AHowever%2C%20current%20mainstream%20detection%20operators%20usually%20ignore%20the%20uncertain%0Arepresentation%20of%20regional%20pixels%20and%20global%20structures.%20To%20solve%20these%0Aproblems%2C%20we%20propose%20a%20structural%20uncertainty-aware%20and%20multi-structure%0Aoperator%20fusion%20detection%20algorithm%20%28EDBSW%29%20based%20on%20a%20new%20BCSSW%20spline%0Awavelet.%20By%20constructing%20a%20spline%20wavelet%20that%20efficiently%20handles%20edge%0Aeffects%2C%20we%20utilize%20structural%20uncertainty-aware%20modulus%20maxima%20to%20detect%0Ahighly%20uncertain%20edge%20samples.%20The%20proposed%20wavelet%20detection%20operator%20utilizes%0Athe%20multi-structure%20morphological%20operator%20and%20fusion%20reconstruction%20strategy%0Ato%20effectively%20address%20anti-noise%20processing%20and%20edge%20information%20of%20different%0Afrequencies.%20Numerous%20experiments%20have%20demonstrated%20its%20excellent%20performance%0Ain%20reducing%20noise%20and%20capturing%20edge%20structure%20details.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08285v1&entry.124074799=Read"},
{"title": "Real2Code: Reconstruct Articulated Objects via Code Generation", "author": "Zhao Mandi and Yijia Weng and Dominik Bauer and Shuran Song", "abstract": "  We present Real2Code, a novel approach to reconstructing articulated objects\nvia code generation. Given visual observations of an object, we first\nreconstruct its part geometry using an image segmentation model and a shape\ncompletion model. We then represent the object parts with oriented bounding\nboxes, which are input to a fine-tuned large language model (LLM) to predict\njoint articulation as code. By leveraging pre-trained vision and language\nmodels, our approach scales elegantly with the number of articulated parts, and\ngeneralizes from synthetic training data to real world objects in unstructured\nenvironments. Experimental results demonstrate that Real2Code significantly\noutperforms previous state-of-the-art in reconstruction accuracy, and is the\nfirst approach to extrapolate beyond objects' structural complexity in the\ntraining set, and reconstructs objects with up to 10 articulated parts. When\nincorporated with a stereo reconstruction model, Real2Code also generalizes to\nreal world objects from a handful of multi-view RGB images, without the need\nfor depth or camera information.\n", "link": "http://arxiv.org/abs/2406.08474v1", "date": "2024-06-12", "relevancy": 2.2114, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5568}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5529}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real2Code%3A%20Reconstruct%20Articulated%20Objects%20via%20Code%20Generation&body=Title%3A%20Real2Code%3A%20Reconstruct%20Articulated%20Objects%20via%20Code%20Generation%0AAuthor%3A%20Zhao%20Mandi%20and%20Yijia%20Weng%20and%20Dominik%20Bauer%20and%20Shuran%20Song%0AAbstract%3A%20%20%20We%20present%20Real2Code%2C%20a%20novel%20approach%20to%20reconstructing%20articulated%20objects%0Avia%20code%20generation.%20Given%20visual%20observations%20of%20an%20object%2C%20we%20first%0Areconstruct%20its%20part%20geometry%20using%20an%20image%20segmentation%20model%20and%20a%20shape%0Acompletion%20model.%20We%20then%20represent%20the%20object%20parts%20with%20oriented%20bounding%0Aboxes%2C%20which%20are%20input%20to%20a%20fine-tuned%20large%20language%20model%20%28LLM%29%20to%20predict%0Ajoint%20articulation%20as%20code.%20By%20leveraging%20pre-trained%20vision%20and%20language%0Amodels%2C%20our%20approach%20scales%20elegantly%20with%20the%20number%20of%20articulated%20parts%2C%20and%0Ageneralizes%20from%20synthetic%20training%20data%20to%20real%20world%20objects%20in%20unstructured%0Aenvironments.%20Experimental%20results%20demonstrate%20that%20Real2Code%20significantly%0Aoutperforms%20previous%20state-of-the-art%20in%20reconstruction%20accuracy%2C%20and%20is%20the%0Afirst%20approach%20to%20extrapolate%20beyond%20objects%27%20structural%20complexity%20in%20the%0Atraining%20set%2C%20and%20reconstructs%20objects%20with%20up%20to%2010%20articulated%20parts.%20When%0Aincorporated%20with%20a%20stereo%20reconstruction%20model%2C%20Real2Code%20also%20generalizes%20to%0Areal%20world%20objects%20from%20a%20handful%20of%20multi-view%20RGB%20images%2C%20without%20the%20need%0Afor%20depth%20or%20camera%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08474v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal2Code%253A%2520Reconstruct%2520Articulated%2520Objects%2520via%2520Code%2520Generation%26entry.906535625%3DZhao%2520Mandi%2520and%2520Yijia%2520Weng%2520and%2520Dominik%2520Bauer%2520and%2520Shuran%2520Song%26entry.1292438233%3D%2520%2520We%2520present%2520Real2Code%252C%2520a%2520novel%2520approach%2520to%2520reconstructing%2520articulated%2520objects%250Avia%2520code%2520generation.%2520Given%2520visual%2520observations%2520of%2520an%2520object%252C%2520we%2520first%250Areconstruct%2520its%2520part%2520geometry%2520using%2520an%2520image%2520segmentation%2520model%2520and%2520a%2520shape%250Acompletion%2520model.%2520We%2520then%2520represent%2520the%2520object%2520parts%2520with%2520oriented%2520bounding%250Aboxes%252C%2520which%2520are%2520input%2520to%2520a%2520fine-tuned%2520large%2520language%2520model%2520%2528LLM%2529%2520to%2520predict%250Ajoint%2520articulation%2520as%2520code.%2520By%2520leveraging%2520pre-trained%2520vision%2520and%2520language%250Amodels%252C%2520our%2520approach%2520scales%2520elegantly%2520with%2520the%2520number%2520of%2520articulated%2520parts%252C%2520and%250Ageneralizes%2520from%2520synthetic%2520training%2520data%2520to%2520real%2520world%2520objects%2520in%2520unstructured%250Aenvironments.%2520Experimental%2520results%2520demonstrate%2520that%2520Real2Code%2520significantly%250Aoutperforms%2520previous%2520state-of-the-art%2520in%2520reconstruction%2520accuracy%252C%2520and%2520is%2520the%250Afirst%2520approach%2520to%2520extrapolate%2520beyond%2520objects%2527%2520structural%2520complexity%2520in%2520the%250Atraining%2520set%252C%2520and%2520reconstructs%2520objects%2520with%2520up%2520to%252010%2520articulated%2520parts.%2520When%250Aincorporated%2520with%2520a%2520stereo%2520reconstruction%2520model%252C%2520Real2Code%2520also%2520generalizes%2520to%250Areal%2520world%2520objects%2520from%2520a%2520handful%2520of%2520multi-view%2520RGB%2520images%252C%2520without%2520the%2520need%250Afor%2520depth%2520or%2520camera%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08474v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real2Code%3A%20Reconstruct%20Articulated%20Objects%20via%20Code%20Generation&entry.906535625=Zhao%20Mandi%20and%20Yijia%20Weng%20and%20Dominik%20Bauer%20and%20Shuran%20Song&entry.1292438233=%20%20We%20present%20Real2Code%2C%20a%20novel%20approach%20to%20reconstructing%20articulated%20objects%0Avia%20code%20generation.%20Given%20visual%20observations%20of%20an%20object%2C%20we%20first%0Areconstruct%20its%20part%20geometry%20using%20an%20image%20segmentation%20model%20and%20a%20shape%0Acompletion%20model.%20We%20then%20represent%20the%20object%20parts%20with%20oriented%20bounding%0Aboxes%2C%20which%20are%20input%20to%20a%20fine-tuned%20large%20language%20model%20%28LLM%29%20to%20predict%0Ajoint%20articulation%20as%20code.%20By%20leveraging%20pre-trained%20vision%20and%20language%0Amodels%2C%20our%20approach%20scales%20elegantly%20with%20the%20number%20of%20articulated%20parts%2C%20and%0Ageneralizes%20from%20synthetic%20training%20data%20to%20real%20world%20objects%20in%20unstructured%0Aenvironments.%20Experimental%20results%20demonstrate%20that%20Real2Code%20significantly%0Aoutperforms%20previous%20state-of-the-art%20in%20reconstruction%20accuracy%2C%20and%20is%20the%0Afirst%20approach%20to%20extrapolate%20beyond%20objects%27%20structural%20complexity%20in%20the%0Atraining%20set%2C%20and%20reconstructs%20objects%20with%20up%20to%2010%20articulated%20parts.%20When%0Aincorporated%20with%20a%20stereo%20reconstruction%20model%2C%20Real2Code%20also%20generalizes%20to%0Areal%20world%20objects%20from%20a%20handful%20of%20multi-view%20RGB%20images%2C%20without%20the%20need%0Afor%20depth%20or%20camera%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08474v1&entry.124074799=Read"},
{"title": "Chemistry3D: Robotic Interaction Benchmark for Chemistry Experiments", "author": "Shoujie Li and Yan Huang and Changqing Guo and Tong Wu and Jiawei Zhang and Linrui Zhang and Wenbo Ding", "abstract": "  The advent of simulation engines has revolutionized learning and operational\nefficiency for robots, offering cost-effective and swift pipelines. However,\nthe lack of a universal simulation platform tailored for chemical scenarios\nimpedes progress in robotic manipulation and visualization of reaction\nprocesses. Addressing this void, we present Chemistry3D, an innovative toolkit\nthat integrates extensive chemical and robotic knowledge. Chemistry3D not only\nenables robots to perform chemical experiments but also provides real-time\nvisualization of temperature, color, and pH changes during reactions. Built on\nthe NVIDIA Omniverse platform, Chemistry3D offers interfaces for robot\noperation, visual inspection, and liquid flow control, facilitating the\nsimulation of special objects such as liquids and transparent entities.\nLeveraging this toolkit, we have devised RL tasks, object detection, and robot\noperation scenarios. Additionally, to discern disparities between the rendering\nengine and the real world, we conducted transparent object detection\nexperiments using Sim2Real, validating the toolkit's exceptional simulation\nperformance. The source code is available at\nhttps://github.com/huangyan28/Chemistry3D, and a related tutorial can be found\nat https://www.omni-chemistry.com.\n", "link": "http://arxiv.org/abs/2406.08160v1", "date": "2024-06-12", "relevancy": 2.2094, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.602}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5424}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chemistry3D%3A%20Robotic%20Interaction%20Benchmark%20for%20Chemistry%20Experiments&body=Title%3A%20Chemistry3D%3A%20Robotic%20Interaction%20Benchmark%20for%20Chemistry%20Experiments%0AAuthor%3A%20Shoujie%20Li%20and%20Yan%20Huang%20and%20Changqing%20Guo%20and%20Tong%20Wu%20and%20Jiawei%20Zhang%20and%20Linrui%20Zhang%20and%20Wenbo%20Ding%0AAbstract%3A%20%20%20The%20advent%20of%20simulation%20engines%20has%20revolutionized%20learning%20and%20operational%0Aefficiency%20for%20robots%2C%20offering%20cost-effective%20and%20swift%20pipelines.%20However%2C%0Athe%20lack%20of%20a%20universal%20simulation%20platform%20tailored%20for%20chemical%20scenarios%0Aimpedes%20progress%20in%20robotic%20manipulation%20and%20visualization%20of%20reaction%0Aprocesses.%20Addressing%20this%20void%2C%20we%20present%20Chemistry3D%2C%20an%20innovative%20toolkit%0Athat%20integrates%20extensive%20chemical%20and%20robotic%20knowledge.%20Chemistry3D%20not%20only%0Aenables%20robots%20to%20perform%20chemical%20experiments%20but%20also%20provides%20real-time%0Avisualization%20of%20temperature%2C%20color%2C%20and%20pH%20changes%20during%20reactions.%20Built%20on%0Athe%20NVIDIA%20Omniverse%20platform%2C%20Chemistry3D%20offers%20interfaces%20for%20robot%0Aoperation%2C%20visual%20inspection%2C%20and%20liquid%20flow%20control%2C%20facilitating%20the%0Asimulation%20of%20special%20objects%20such%20as%20liquids%20and%20transparent%20entities.%0ALeveraging%20this%20toolkit%2C%20we%20have%20devised%20RL%20tasks%2C%20object%20detection%2C%20and%20robot%0Aoperation%20scenarios.%20Additionally%2C%20to%20discern%20disparities%20between%20the%20rendering%0Aengine%20and%20the%20real%20world%2C%20we%20conducted%20transparent%20object%20detection%0Aexperiments%20using%20Sim2Real%2C%20validating%20the%20toolkit%27s%20exceptional%20simulation%0Aperformance.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/huangyan28/Chemistry3D%2C%20and%20a%20related%20tutorial%20can%20be%20found%0Aat%20https%3A//www.omni-chemistry.com.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08160v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChemistry3D%253A%2520Robotic%2520Interaction%2520Benchmark%2520for%2520Chemistry%2520Experiments%26entry.906535625%3DShoujie%2520Li%2520and%2520Yan%2520Huang%2520and%2520Changqing%2520Guo%2520and%2520Tong%2520Wu%2520and%2520Jiawei%2520Zhang%2520and%2520Linrui%2520Zhang%2520and%2520Wenbo%2520Ding%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520simulation%2520engines%2520has%2520revolutionized%2520learning%2520and%2520operational%250Aefficiency%2520for%2520robots%252C%2520offering%2520cost-effective%2520and%2520swift%2520pipelines.%2520However%252C%250Athe%2520lack%2520of%2520a%2520universal%2520simulation%2520platform%2520tailored%2520for%2520chemical%2520scenarios%250Aimpedes%2520progress%2520in%2520robotic%2520manipulation%2520and%2520visualization%2520of%2520reaction%250Aprocesses.%2520Addressing%2520this%2520void%252C%2520we%2520present%2520Chemistry3D%252C%2520an%2520innovative%2520toolkit%250Athat%2520integrates%2520extensive%2520chemical%2520and%2520robotic%2520knowledge.%2520Chemistry3D%2520not%2520only%250Aenables%2520robots%2520to%2520perform%2520chemical%2520experiments%2520but%2520also%2520provides%2520real-time%250Avisualization%2520of%2520temperature%252C%2520color%252C%2520and%2520pH%2520changes%2520during%2520reactions.%2520Built%2520on%250Athe%2520NVIDIA%2520Omniverse%2520platform%252C%2520Chemistry3D%2520offers%2520interfaces%2520for%2520robot%250Aoperation%252C%2520visual%2520inspection%252C%2520and%2520liquid%2520flow%2520control%252C%2520facilitating%2520the%250Asimulation%2520of%2520special%2520objects%2520such%2520as%2520liquids%2520and%2520transparent%2520entities.%250ALeveraging%2520this%2520toolkit%252C%2520we%2520have%2520devised%2520RL%2520tasks%252C%2520object%2520detection%252C%2520and%2520robot%250Aoperation%2520scenarios.%2520Additionally%252C%2520to%2520discern%2520disparities%2520between%2520the%2520rendering%250Aengine%2520and%2520the%2520real%2520world%252C%2520we%2520conducted%2520transparent%2520object%2520detection%250Aexperiments%2520using%2520Sim2Real%252C%2520validating%2520the%2520toolkit%2527s%2520exceptional%2520simulation%250Aperformance.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/huangyan28/Chemistry3D%252C%2520and%2520a%2520related%2520tutorial%2520can%2520be%2520found%250Aat%2520https%253A//www.omni-chemistry.com.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08160v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chemistry3D%3A%20Robotic%20Interaction%20Benchmark%20for%20Chemistry%20Experiments&entry.906535625=Shoujie%20Li%20and%20Yan%20Huang%20and%20Changqing%20Guo%20and%20Tong%20Wu%20and%20Jiawei%20Zhang%20and%20Linrui%20Zhang%20and%20Wenbo%20Ding&entry.1292438233=%20%20The%20advent%20of%20simulation%20engines%20has%20revolutionized%20learning%20and%20operational%0Aefficiency%20for%20robots%2C%20offering%20cost-effective%20and%20swift%20pipelines.%20However%2C%0Athe%20lack%20of%20a%20universal%20simulation%20platform%20tailored%20for%20chemical%20scenarios%0Aimpedes%20progress%20in%20robotic%20manipulation%20and%20visualization%20of%20reaction%0Aprocesses.%20Addressing%20this%20void%2C%20we%20present%20Chemistry3D%2C%20an%20innovative%20toolkit%0Athat%20integrates%20extensive%20chemical%20and%20robotic%20knowledge.%20Chemistry3D%20not%20only%0Aenables%20robots%20to%20perform%20chemical%20experiments%20but%20also%20provides%20real-time%0Avisualization%20of%20temperature%2C%20color%2C%20and%20pH%20changes%20during%20reactions.%20Built%20on%0Athe%20NVIDIA%20Omniverse%20platform%2C%20Chemistry3D%20offers%20interfaces%20for%20robot%0Aoperation%2C%20visual%20inspection%2C%20and%20liquid%20flow%20control%2C%20facilitating%20the%0Asimulation%20of%20special%20objects%20such%20as%20liquids%20and%20transparent%20entities.%0ALeveraging%20this%20toolkit%2C%20we%20have%20devised%20RL%20tasks%2C%20object%20detection%2C%20and%20robot%0Aoperation%20scenarios.%20Additionally%2C%20to%20discern%20disparities%20between%20the%20rendering%0Aengine%20and%20the%20real%20world%2C%20we%20conducted%20transparent%20object%20detection%0Aexperiments%20using%20Sim2Real%2C%20validating%20the%20toolkit%27s%20exceptional%20simulation%0Aperformance.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/huangyan28/Chemistry3D%2C%20and%20a%20related%20tutorial%20can%20be%20found%0Aat%20https%3A//www.omni-chemistry.com.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08160v1&entry.124074799=Read"},
{"title": "LaMOT: Language-Guided Multi-Object Tracking", "author": "Yunhao Li and Xiaoqiong Liu and Luke Liu and Heng Fan and Libo Zhang", "abstract": "  Vision-Language MOT is a crucial tracking problem and has drawn increasing\nattention recently. It aims to track objects based on human language commands,\nreplacing the traditional use of templates or pre-set information from training\nsets in conventional tracking tasks. Despite various efforts, a key challenge\nlies in the lack of a clear understanding of why language is used for tracking,\nwhich hinders further development in this field. In this paper, we address this\nchallenge by introducing Language-Guided MOT, a unified task framework, along\nwith a corresponding large-scale benchmark, termed LaMOT, which encompasses\ndiverse scenarios and language descriptions. Specially, LaMOT comprises 1,660\nsequences from 4 different datasets and aims to unify various Vision-Language\nMOT tasks while providing a standardized evaluation platform. To ensure\nhigh-quality annotations, we manually assign appropriate descriptive texts to\neach target in every video and conduct careful inspection and correction. To\nthe best of our knowledge, LaMOT is the first benchmark dedicated to\nLanguage-Guided MOT. Additionally, we propose a simple yet effective tracker,\ntermed LaMOTer. By establishing a unified task framework, providing challenging\nbenchmarks, and offering insights for future algorithm design and evaluation,\nwe expect to contribute to the advancement of research in Vision-Language MOT.\nWe will release the data at https://github.com/Nathan-Li123/LaMOT.\n", "link": "http://arxiv.org/abs/2406.08324v1", "date": "2024-06-12", "relevancy": 2.1892, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5657}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.537}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaMOT%3A%20Language-Guided%20Multi-Object%20Tracking&body=Title%3A%20LaMOT%3A%20Language-Guided%20Multi-Object%20Tracking%0AAuthor%3A%20Yunhao%20Li%20and%20Xiaoqiong%20Liu%20and%20Luke%20Liu%20and%20Heng%20Fan%20and%20Libo%20Zhang%0AAbstract%3A%20%20%20Vision-Language%20MOT%20is%20a%20crucial%20tracking%20problem%20and%20has%20drawn%20increasing%0Aattention%20recently.%20It%20aims%20to%20track%20objects%20based%20on%20human%20language%20commands%2C%0Areplacing%20the%20traditional%20use%20of%20templates%20or%20pre-set%20information%20from%20training%0Asets%20in%20conventional%20tracking%20tasks.%20Despite%20various%20efforts%2C%20a%20key%20challenge%0Alies%20in%20the%20lack%20of%20a%20clear%20understanding%20of%20why%20language%20is%20used%20for%20tracking%2C%0Awhich%20hinders%20further%20development%20in%20this%20field.%20In%20this%20paper%2C%20we%20address%20this%0Achallenge%20by%20introducing%20Language-Guided%20MOT%2C%20a%20unified%20task%20framework%2C%20along%0Awith%20a%20corresponding%20large-scale%20benchmark%2C%20termed%20LaMOT%2C%20which%20encompasses%0Adiverse%20scenarios%20and%20language%20descriptions.%20Specially%2C%20LaMOT%20comprises%201%2C660%0Asequences%20from%204%20different%20datasets%20and%20aims%20to%20unify%20various%20Vision-Language%0AMOT%20tasks%20while%20providing%20a%20standardized%20evaluation%20platform.%20To%20ensure%0Ahigh-quality%20annotations%2C%20we%20manually%20assign%20appropriate%20descriptive%20texts%20to%0Aeach%20target%20in%20every%20video%20and%20conduct%20careful%20inspection%20and%20correction.%20To%0Athe%20best%20of%20our%20knowledge%2C%20LaMOT%20is%20the%20first%20benchmark%20dedicated%20to%0ALanguage-Guided%20MOT.%20Additionally%2C%20we%20propose%20a%20simple%20yet%20effective%20tracker%2C%0Atermed%20LaMOTer.%20By%20establishing%20a%20unified%20task%20framework%2C%20providing%20challenging%0Abenchmarks%2C%20and%20offering%20insights%20for%20future%20algorithm%20design%20and%20evaluation%2C%0Awe%20expect%20to%20contribute%20to%20the%20advancement%20of%20research%20in%20Vision-Language%20MOT.%0AWe%20will%20release%20the%20data%20at%20https%3A//github.com/Nathan-Li123/LaMOT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08324v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaMOT%253A%2520Language-Guided%2520Multi-Object%2520Tracking%26entry.906535625%3DYunhao%2520Li%2520and%2520Xiaoqiong%2520Liu%2520and%2520Luke%2520Liu%2520and%2520Heng%2520Fan%2520and%2520Libo%2520Zhang%26entry.1292438233%3D%2520%2520Vision-Language%2520MOT%2520is%2520a%2520crucial%2520tracking%2520problem%2520and%2520has%2520drawn%2520increasing%250Aattention%2520recently.%2520It%2520aims%2520to%2520track%2520objects%2520based%2520on%2520human%2520language%2520commands%252C%250Areplacing%2520the%2520traditional%2520use%2520of%2520templates%2520or%2520pre-set%2520information%2520from%2520training%250Asets%2520in%2520conventional%2520tracking%2520tasks.%2520Despite%2520various%2520efforts%252C%2520a%2520key%2520challenge%250Alies%2520in%2520the%2520lack%2520of%2520a%2520clear%2520understanding%2520of%2520why%2520language%2520is%2520used%2520for%2520tracking%252C%250Awhich%2520hinders%2520further%2520development%2520in%2520this%2520field.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%250Achallenge%2520by%2520introducing%2520Language-Guided%2520MOT%252C%2520a%2520unified%2520task%2520framework%252C%2520along%250Awith%2520a%2520corresponding%2520large-scale%2520benchmark%252C%2520termed%2520LaMOT%252C%2520which%2520encompasses%250Adiverse%2520scenarios%2520and%2520language%2520descriptions.%2520Specially%252C%2520LaMOT%2520comprises%25201%252C660%250Asequences%2520from%25204%2520different%2520datasets%2520and%2520aims%2520to%2520unify%2520various%2520Vision-Language%250AMOT%2520tasks%2520while%2520providing%2520a%2520standardized%2520evaluation%2520platform.%2520To%2520ensure%250Ahigh-quality%2520annotations%252C%2520we%2520manually%2520assign%2520appropriate%2520descriptive%2520texts%2520to%250Aeach%2520target%2520in%2520every%2520video%2520and%2520conduct%2520careful%2520inspection%2520and%2520correction.%2520To%250Athe%2520best%2520of%2520our%2520knowledge%252C%2520LaMOT%2520is%2520the%2520first%2520benchmark%2520dedicated%2520to%250ALanguage-Guided%2520MOT.%2520Additionally%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520tracker%252C%250Atermed%2520LaMOTer.%2520By%2520establishing%2520a%2520unified%2520task%2520framework%252C%2520providing%2520challenging%250Abenchmarks%252C%2520and%2520offering%2520insights%2520for%2520future%2520algorithm%2520design%2520and%2520evaluation%252C%250Awe%2520expect%2520to%2520contribute%2520to%2520the%2520advancement%2520of%2520research%2520in%2520Vision-Language%2520MOT.%250AWe%2520will%2520release%2520the%2520data%2520at%2520https%253A//github.com/Nathan-Li123/LaMOT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08324v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaMOT%3A%20Language-Guided%20Multi-Object%20Tracking&entry.906535625=Yunhao%20Li%20and%20Xiaoqiong%20Liu%20and%20Luke%20Liu%20and%20Heng%20Fan%20and%20Libo%20Zhang&entry.1292438233=%20%20Vision-Language%20MOT%20is%20a%20crucial%20tracking%20problem%20and%20has%20drawn%20increasing%0Aattention%20recently.%20It%20aims%20to%20track%20objects%20based%20on%20human%20language%20commands%2C%0Areplacing%20the%20traditional%20use%20of%20templates%20or%20pre-set%20information%20from%20training%0Asets%20in%20conventional%20tracking%20tasks.%20Despite%20various%20efforts%2C%20a%20key%20challenge%0Alies%20in%20the%20lack%20of%20a%20clear%20understanding%20of%20why%20language%20is%20used%20for%20tracking%2C%0Awhich%20hinders%20further%20development%20in%20this%20field.%20In%20this%20paper%2C%20we%20address%20this%0Achallenge%20by%20introducing%20Language-Guided%20MOT%2C%20a%20unified%20task%20framework%2C%20along%0Awith%20a%20corresponding%20large-scale%20benchmark%2C%20termed%20LaMOT%2C%20which%20encompasses%0Adiverse%20scenarios%20and%20language%20descriptions.%20Specially%2C%20LaMOT%20comprises%201%2C660%0Asequences%20from%204%20different%20datasets%20and%20aims%20to%20unify%20various%20Vision-Language%0AMOT%20tasks%20while%20providing%20a%20standardized%20evaluation%20platform.%20To%20ensure%0Ahigh-quality%20annotations%2C%20we%20manually%20assign%20appropriate%20descriptive%20texts%20to%0Aeach%20target%20in%20every%20video%20and%20conduct%20careful%20inspection%20and%20correction.%20To%0Athe%20best%20of%20our%20knowledge%2C%20LaMOT%20is%20the%20first%20benchmark%20dedicated%20to%0ALanguage-Guided%20MOT.%20Additionally%2C%20we%20propose%20a%20simple%20yet%20effective%20tracker%2C%0Atermed%20LaMOTer.%20By%20establishing%20a%20unified%20task%20framework%2C%20providing%20challenging%0Abenchmarks%2C%20and%20offering%20insights%20for%20future%20algorithm%20design%20and%20evaluation%2C%0Awe%20expect%20to%20contribute%20to%20the%20advancement%20of%20research%20in%20Vision-Language%20MOT.%0AWe%20will%20release%20the%20data%20at%20https%3A//github.com/Nathan-Li123/LaMOT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08324v1&entry.124074799=Read"},
{"title": "PAL: Pluralistic Alignment Framework for Learning from Heterogeneous\n  Preferences", "author": "Daiwei Chen and Yi Chen and Aniket Rege and Ramya Korlakai Vinayak", "abstract": "  Large foundation models pretrained on raw web-scale data are not readily\ndeployable without additional step of extensive alignment to human preferences.\nSuch alignment is typically done by collecting large amounts of pairwise\ncomparisons from humans (\"Do you prefer output A or B?\") and learning a reward\nmodel or a policy with the Bradley-Terry-Luce (BTL) model as a proxy for a\nhuman's underlying implicit preferences. These methods generally suffer from\nassuming a universal preference shared by all humans, which lacks the\nflexibility of adapting to plurality of opinions and preferences. In this work,\nwe propose PAL, a framework to model human preference complementary to existing\npretraining strategies, which incorporates plurality from the ground up. We\npropose using the ideal point model as a lens to view alignment using\npreference comparisons. Together with our novel reformulation and using mixture\nmodeling, our framework captures the plurality of population preferences while\nsimultaneously learning a common preference latent space across different\npreferences, which can few-shot generalize to new, unseen users. Our approach\nenables us to use the penultimate-layer representation of large foundation\nmodels and simple MLP layers to learn reward functions that are on-par with the\nexisting large state-of-the-art reward models, thereby enhancing efficiency of\nreward modeling significantly. We show that PAL achieves competitive reward\nmodel accuracy compared to strong baselines on 1) Language models with Summary\ndataset ; 2) Image Generative models with Pick-a-Pic dataset ; 3) A new\nsemisynthetic heterogeneous dataset generated using Anthropic Personas.\nFinally, our experiments also highlight the shortcoming of current preference\ndatasets that are created using rigid rubrics which wash away heterogeneity,\nand call for more nuanced data collection approaches.\n", "link": "http://arxiv.org/abs/2406.08469v1", "date": "2024-06-12", "relevancy": 2.1883, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5691}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5472}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PAL%3A%20Pluralistic%20Alignment%20Framework%20for%20Learning%20from%20Heterogeneous%0A%20%20Preferences&body=Title%3A%20PAL%3A%20Pluralistic%20Alignment%20Framework%20for%20Learning%20from%20Heterogeneous%0A%20%20Preferences%0AAuthor%3A%20Daiwei%20Chen%20and%20Yi%20Chen%20and%20Aniket%20Rege%20and%20Ramya%20Korlakai%20Vinayak%0AAbstract%3A%20%20%20Large%20foundation%20models%20pretrained%20on%20raw%20web-scale%20data%20are%20not%20readily%0Adeployable%20without%20additional%20step%20of%20extensive%20alignment%20to%20human%20preferences.%0ASuch%20alignment%20is%20typically%20done%20by%20collecting%20large%20amounts%20of%20pairwise%0Acomparisons%20from%20humans%20%28%22Do%20you%20prefer%20output%20A%20or%20B%3F%22%29%20and%20learning%20a%20reward%0Amodel%20or%20a%20policy%20with%20the%20Bradley-Terry-Luce%20%28BTL%29%20model%20as%20a%20proxy%20for%20a%0Ahuman%27s%20underlying%20implicit%20preferences.%20These%20methods%20generally%20suffer%20from%0Aassuming%20a%20universal%20preference%20shared%20by%20all%20humans%2C%20which%20lacks%20the%0Aflexibility%20of%20adapting%20to%20plurality%20of%20opinions%20and%20preferences.%20In%20this%20work%2C%0Awe%20propose%20PAL%2C%20a%20framework%20to%20model%20human%20preference%20complementary%20to%20existing%0Apretraining%20strategies%2C%20which%20incorporates%20plurality%20from%20the%20ground%20up.%20We%0Apropose%20using%20the%20ideal%20point%20model%20as%20a%20lens%20to%20view%20alignment%20using%0Apreference%20comparisons.%20Together%20with%20our%20novel%20reformulation%20and%20using%20mixture%0Amodeling%2C%20our%20framework%20captures%20the%20plurality%20of%20population%20preferences%20while%0Asimultaneously%20learning%20a%20common%20preference%20latent%20space%20across%20different%0Apreferences%2C%20which%20can%20few-shot%20generalize%20to%20new%2C%20unseen%20users.%20Our%20approach%0Aenables%20us%20to%20use%20the%20penultimate-layer%20representation%20of%20large%20foundation%0Amodels%20and%20simple%20MLP%20layers%20to%20learn%20reward%20functions%20that%20are%20on-par%20with%20the%0Aexisting%20large%20state-of-the-art%20reward%20models%2C%20thereby%20enhancing%20efficiency%20of%0Areward%20modeling%20significantly.%20We%20show%20that%20PAL%20achieves%20competitive%20reward%0Amodel%20accuracy%20compared%20to%20strong%20baselines%20on%201%29%20Language%20models%20with%20Summary%0Adataset%20%3B%202%29%20Image%20Generative%20models%20with%20Pick-a-Pic%20dataset%20%3B%203%29%20A%20new%0Asemisynthetic%20heterogeneous%20dataset%20generated%20using%20Anthropic%20Personas.%0AFinally%2C%20our%20experiments%20also%20highlight%20the%20shortcoming%20of%20current%20preference%0Adatasets%20that%20are%20created%20using%20rigid%20rubrics%20which%20wash%20away%20heterogeneity%2C%0Aand%20call%20for%20more%20nuanced%20data%20collection%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08469v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPAL%253A%2520Pluralistic%2520Alignment%2520Framework%2520for%2520Learning%2520from%2520Heterogeneous%250A%2520%2520Preferences%26entry.906535625%3DDaiwei%2520Chen%2520and%2520Yi%2520Chen%2520and%2520Aniket%2520Rege%2520and%2520Ramya%2520Korlakai%2520Vinayak%26entry.1292438233%3D%2520%2520Large%2520foundation%2520models%2520pretrained%2520on%2520raw%2520web-scale%2520data%2520are%2520not%2520readily%250Adeployable%2520without%2520additional%2520step%2520of%2520extensive%2520alignment%2520to%2520human%2520preferences.%250ASuch%2520alignment%2520is%2520typically%2520done%2520by%2520collecting%2520large%2520amounts%2520of%2520pairwise%250Acomparisons%2520from%2520humans%2520%2528%2522Do%2520you%2520prefer%2520output%2520A%2520or%2520B%253F%2522%2529%2520and%2520learning%2520a%2520reward%250Amodel%2520or%2520a%2520policy%2520with%2520the%2520Bradley-Terry-Luce%2520%2528BTL%2529%2520model%2520as%2520a%2520proxy%2520for%2520a%250Ahuman%2527s%2520underlying%2520implicit%2520preferences.%2520These%2520methods%2520generally%2520suffer%2520from%250Aassuming%2520a%2520universal%2520preference%2520shared%2520by%2520all%2520humans%252C%2520which%2520lacks%2520the%250Aflexibility%2520of%2520adapting%2520to%2520plurality%2520of%2520opinions%2520and%2520preferences.%2520In%2520this%2520work%252C%250Awe%2520propose%2520PAL%252C%2520a%2520framework%2520to%2520model%2520human%2520preference%2520complementary%2520to%2520existing%250Apretraining%2520strategies%252C%2520which%2520incorporates%2520plurality%2520from%2520the%2520ground%2520up.%2520We%250Apropose%2520using%2520the%2520ideal%2520point%2520model%2520as%2520a%2520lens%2520to%2520view%2520alignment%2520using%250Apreference%2520comparisons.%2520Together%2520with%2520our%2520novel%2520reformulation%2520and%2520using%2520mixture%250Amodeling%252C%2520our%2520framework%2520captures%2520the%2520plurality%2520of%2520population%2520preferences%2520while%250Asimultaneously%2520learning%2520a%2520common%2520preference%2520latent%2520space%2520across%2520different%250Apreferences%252C%2520which%2520can%2520few-shot%2520generalize%2520to%2520new%252C%2520unseen%2520users.%2520Our%2520approach%250Aenables%2520us%2520to%2520use%2520the%2520penultimate-layer%2520representation%2520of%2520large%2520foundation%250Amodels%2520and%2520simple%2520MLP%2520layers%2520to%2520learn%2520reward%2520functions%2520that%2520are%2520on-par%2520with%2520the%250Aexisting%2520large%2520state-of-the-art%2520reward%2520models%252C%2520thereby%2520enhancing%2520efficiency%2520of%250Areward%2520modeling%2520significantly.%2520We%2520show%2520that%2520PAL%2520achieves%2520competitive%2520reward%250Amodel%2520accuracy%2520compared%2520to%2520strong%2520baselines%2520on%25201%2529%2520Language%2520models%2520with%2520Summary%250Adataset%2520%253B%25202%2529%2520Image%2520Generative%2520models%2520with%2520Pick-a-Pic%2520dataset%2520%253B%25203%2529%2520A%2520new%250Asemisynthetic%2520heterogeneous%2520dataset%2520generated%2520using%2520Anthropic%2520Personas.%250AFinally%252C%2520our%2520experiments%2520also%2520highlight%2520the%2520shortcoming%2520of%2520current%2520preference%250Adatasets%2520that%2520are%2520created%2520using%2520rigid%2520rubrics%2520which%2520wash%2520away%2520heterogeneity%252C%250Aand%2520call%2520for%2520more%2520nuanced%2520data%2520collection%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08469v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAL%3A%20Pluralistic%20Alignment%20Framework%20for%20Learning%20from%20Heterogeneous%0A%20%20Preferences&entry.906535625=Daiwei%20Chen%20and%20Yi%20Chen%20and%20Aniket%20Rege%20and%20Ramya%20Korlakai%20Vinayak&entry.1292438233=%20%20Large%20foundation%20models%20pretrained%20on%20raw%20web-scale%20data%20are%20not%20readily%0Adeployable%20without%20additional%20step%20of%20extensive%20alignment%20to%20human%20preferences.%0ASuch%20alignment%20is%20typically%20done%20by%20collecting%20large%20amounts%20of%20pairwise%0Acomparisons%20from%20humans%20%28%22Do%20you%20prefer%20output%20A%20or%20B%3F%22%29%20and%20learning%20a%20reward%0Amodel%20or%20a%20policy%20with%20the%20Bradley-Terry-Luce%20%28BTL%29%20model%20as%20a%20proxy%20for%20a%0Ahuman%27s%20underlying%20implicit%20preferences.%20These%20methods%20generally%20suffer%20from%0Aassuming%20a%20universal%20preference%20shared%20by%20all%20humans%2C%20which%20lacks%20the%0Aflexibility%20of%20adapting%20to%20plurality%20of%20opinions%20and%20preferences.%20In%20this%20work%2C%0Awe%20propose%20PAL%2C%20a%20framework%20to%20model%20human%20preference%20complementary%20to%20existing%0Apretraining%20strategies%2C%20which%20incorporates%20plurality%20from%20the%20ground%20up.%20We%0Apropose%20using%20the%20ideal%20point%20model%20as%20a%20lens%20to%20view%20alignment%20using%0Apreference%20comparisons.%20Together%20with%20our%20novel%20reformulation%20and%20using%20mixture%0Amodeling%2C%20our%20framework%20captures%20the%20plurality%20of%20population%20preferences%20while%0Asimultaneously%20learning%20a%20common%20preference%20latent%20space%20across%20different%0Apreferences%2C%20which%20can%20few-shot%20generalize%20to%20new%2C%20unseen%20users.%20Our%20approach%0Aenables%20us%20to%20use%20the%20penultimate-layer%20representation%20of%20large%20foundation%0Amodels%20and%20simple%20MLP%20layers%20to%20learn%20reward%20functions%20that%20are%20on-par%20with%20the%0Aexisting%20large%20state-of-the-art%20reward%20models%2C%20thereby%20enhancing%20efficiency%20of%0Areward%20modeling%20significantly.%20We%20show%20that%20PAL%20achieves%20competitive%20reward%0Amodel%20accuracy%20compared%20to%20strong%20baselines%20on%201%29%20Language%20models%20with%20Summary%0Adataset%20%3B%202%29%20Image%20Generative%20models%20with%20Pick-a-Pic%20dataset%20%3B%203%29%20A%20new%0Asemisynthetic%20heterogeneous%20dataset%20generated%20using%20Anthropic%20Personas.%0AFinally%2C%20our%20experiments%20also%20highlight%20the%20shortcoming%20of%20current%20preference%0Adatasets%20that%20are%20created%20using%20rigid%20rubrics%20which%20wash%20away%20heterogeneity%2C%0Aand%20call%20for%20more%20nuanced%20data%20collection%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08469v1&entry.124074799=Read"},
{"title": "CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\n  Compositional Reasoning via Counterfactual Examples", "author": "Jianrui Zhang and Mu Cai and Tengyang Xie and Yong Jae Lee", "abstract": "  We propose CounterCurate, a framework to comprehensively improve the\nvisio-linguistic compositional reasoning capability for both contrastive and\ngenerative multimodal models. In particular, we identify two critical\nunder-explored problems: the neglect of the physically grounded reasoning\n(counting and position understanding) and the potential of using highly capable\ntext and image generation models for semantic counterfactual fine-tuning. Our\nwork pioneers an approach that addresses these gaps. We first spotlight the\nnear-chance performance of multimodal models like CLIP and LLaVA in physically\ngrounded compositional reasoning. We then apply simple data augmentation using\ngrounded image generation model GLIGEN to generate fine-tuning data, resulting\nin significant performance improvements: +33% and +37% for CLIP and LLaVA,\nrespectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we\nexploit the capabilities of high-performing text generation and image\ngeneration models, specifically GPT-4V and DALLE-3, to curate challenging\nsemantic counterfactuals, thereby further enhancing compositional reasoning\ncapabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms\nGPT-4V. To facilitate future research, we release our code, dataset, benchmark,\nand checkpoints at https://countercurate.github.io.\n", "link": "http://arxiv.org/abs/2402.13254v4", "date": "2024-06-12", "relevancy": 2.1864, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5497}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5458}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CounterCurate%3A%20Enhancing%20Physical%20and%20Semantic%20Visio-Linguistic%0A%20%20Compositional%20Reasoning%20via%20Counterfactual%20Examples&body=Title%3A%20CounterCurate%3A%20Enhancing%20Physical%20and%20Semantic%20Visio-Linguistic%0A%20%20Compositional%20Reasoning%20via%20Counterfactual%20Examples%0AAuthor%3A%20Jianrui%20Zhang%20and%20Mu%20Cai%20and%20Tengyang%20Xie%20and%20Yong%20Jae%20Lee%0AAbstract%3A%20%20%20We%20propose%20CounterCurate%2C%20a%20framework%20to%20comprehensively%20improve%20the%0Avisio-linguistic%20compositional%20reasoning%20capability%20for%20both%20contrastive%20and%0Agenerative%20multimodal%20models.%20In%20particular%2C%20we%20identify%20two%20critical%0Aunder-explored%20problems%3A%20the%20neglect%20of%20the%20physically%20grounded%20reasoning%0A%28counting%20and%20position%20understanding%29%20and%20the%20potential%20of%20using%20highly%20capable%0Atext%20and%20image%20generation%20models%20for%20semantic%20counterfactual%20fine-tuning.%20Our%0Awork%20pioneers%20an%20approach%20that%20addresses%20these%20gaps.%20We%20first%20spotlight%20the%0Anear-chance%20performance%20of%20multimodal%20models%20like%20CLIP%20and%20LLaVA%20in%20physically%0Agrounded%20compositional%20reasoning.%20We%20then%20apply%20simple%20data%20augmentation%20using%0Agrounded%20image%20generation%20model%20GLIGEN%20to%20generate%20fine-tuning%20data%2C%20resulting%0Ain%20significant%20performance%20improvements%3A%20%2B33%25%20and%20%2B37%25%20for%20CLIP%20and%20LLaVA%2C%0Arespectively%2C%20on%20our%20newly%20curated%20Flickr30k-Positions%20benchmark.%20Moreover%2C%20we%0Aexploit%20the%20capabilities%20of%20high-performing%20text%20generation%20and%20image%0Ageneration%20models%2C%20specifically%20GPT-4V%20and%20DALLE-3%2C%20to%20curate%20challenging%0Asemantic%20counterfactuals%2C%20thereby%20further%20enhancing%20compositional%20reasoning%0Acapabilities%20on%20benchmarks%20such%20as%20SugarCrepe%2C%20where%20CounterCurate%20outperforms%0AGPT-4V.%20To%20facilitate%20future%20research%2C%20we%20release%20our%20code%2C%20dataset%2C%20benchmark%2C%0Aand%20checkpoints%20at%20https%3A//countercurate.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13254v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCounterCurate%253A%2520Enhancing%2520Physical%2520and%2520Semantic%2520Visio-Linguistic%250A%2520%2520Compositional%2520Reasoning%2520via%2520Counterfactual%2520Examples%26entry.906535625%3DJianrui%2520Zhang%2520and%2520Mu%2520Cai%2520and%2520Tengyang%2520Xie%2520and%2520Yong%2520Jae%2520Lee%26entry.1292438233%3D%2520%2520We%2520propose%2520CounterCurate%252C%2520a%2520framework%2520to%2520comprehensively%2520improve%2520the%250Avisio-linguistic%2520compositional%2520reasoning%2520capability%2520for%2520both%2520contrastive%2520and%250Agenerative%2520multimodal%2520models.%2520In%2520particular%252C%2520we%2520identify%2520two%2520critical%250Aunder-explored%2520problems%253A%2520the%2520neglect%2520of%2520the%2520physically%2520grounded%2520reasoning%250A%2528counting%2520and%2520position%2520understanding%2529%2520and%2520the%2520potential%2520of%2520using%2520highly%2520capable%250Atext%2520and%2520image%2520generation%2520models%2520for%2520semantic%2520counterfactual%2520fine-tuning.%2520Our%250Awork%2520pioneers%2520an%2520approach%2520that%2520addresses%2520these%2520gaps.%2520We%2520first%2520spotlight%2520the%250Anear-chance%2520performance%2520of%2520multimodal%2520models%2520like%2520CLIP%2520and%2520LLaVA%2520in%2520physically%250Agrounded%2520compositional%2520reasoning.%2520We%2520then%2520apply%2520simple%2520data%2520augmentation%2520using%250Agrounded%2520image%2520generation%2520model%2520GLIGEN%2520to%2520generate%2520fine-tuning%2520data%252C%2520resulting%250Ain%2520significant%2520performance%2520improvements%253A%2520%252B33%2525%2520and%2520%252B37%2525%2520for%2520CLIP%2520and%2520LLaVA%252C%250Arespectively%252C%2520on%2520our%2520newly%2520curated%2520Flickr30k-Positions%2520benchmark.%2520Moreover%252C%2520we%250Aexploit%2520the%2520capabilities%2520of%2520high-performing%2520text%2520generation%2520and%2520image%250Ageneration%2520models%252C%2520specifically%2520GPT-4V%2520and%2520DALLE-3%252C%2520to%2520curate%2520challenging%250Asemantic%2520counterfactuals%252C%2520thereby%2520further%2520enhancing%2520compositional%2520reasoning%250Acapabilities%2520on%2520benchmarks%2520such%2520as%2520SugarCrepe%252C%2520where%2520CounterCurate%2520outperforms%250AGPT-4V.%2520To%2520facilitate%2520future%2520research%252C%2520we%2520release%2520our%2520code%252C%2520dataset%252C%2520benchmark%252C%250Aand%2520checkpoints%2520at%2520https%253A//countercurate.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13254v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CounterCurate%3A%20Enhancing%20Physical%20and%20Semantic%20Visio-Linguistic%0A%20%20Compositional%20Reasoning%20via%20Counterfactual%20Examples&entry.906535625=Jianrui%20Zhang%20and%20Mu%20Cai%20and%20Tengyang%20Xie%20and%20Yong%20Jae%20Lee&entry.1292438233=%20%20We%20propose%20CounterCurate%2C%20a%20framework%20to%20comprehensively%20improve%20the%0Avisio-linguistic%20compositional%20reasoning%20capability%20for%20both%20contrastive%20and%0Agenerative%20multimodal%20models.%20In%20particular%2C%20we%20identify%20two%20critical%0Aunder-explored%20problems%3A%20the%20neglect%20of%20the%20physically%20grounded%20reasoning%0A%28counting%20and%20position%20understanding%29%20and%20the%20potential%20of%20using%20highly%20capable%0Atext%20and%20image%20generation%20models%20for%20semantic%20counterfactual%20fine-tuning.%20Our%0Awork%20pioneers%20an%20approach%20that%20addresses%20these%20gaps.%20We%20first%20spotlight%20the%0Anear-chance%20performance%20of%20multimodal%20models%20like%20CLIP%20and%20LLaVA%20in%20physically%0Agrounded%20compositional%20reasoning.%20We%20then%20apply%20simple%20data%20augmentation%20using%0Agrounded%20image%20generation%20model%20GLIGEN%20to%20generate%20fine-tuning%20data%2C%20resulting%0Ain%20significant%20performance%20improvements%3A%20%2B33%25%20and%20%2B37%25%20for%20CLIP%20and%20LLaVA%2C%0Arespectively%2C%20on%20our%20newly%20curated%20Flickr30k-Positions%20benchmark.%20Moreover%2C%20we%0Aexploit%20the%20capabilities%20of%20high-performing%20text%20generation%20and%20image%0Ageneration%20models%2C%20specifically%20GPT-4V%20and%20DALLE-3%2C%20to%20curate%20challenging%0Asemantic%20counterfactuals%2C%20thereby%20further%20enhancing%20compositional%20reasoning%0Acapabilities%20on%20benchmarks%20such%20as%20SugarCrepe%2C%20where%20CounterCurate%20outperforms%0AGPT-4V.%20To%20facilitate%20future%20research%2C%20we%20release%20our%20code%2C%20dataset%2C%20benchmark%2C%0Aand%20checkpoints%20at%20https%3A//countercurate.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13254v4&entry.124074799=Read"},
{"title": "Adaptive Swarm Mesh Refinement using Deep Reinforcement Learning with\n  Local Rewards", "author": "Niklas Freymuth and Philipp Dahlinger and Tobias W\u00fcrth and Simon Reisch and Luise K\u00e4rger and Gerhard Neumann", "abstract": "  Simulating physical systems is essential in engineering, but analytical\nsolutions are limited to straightforward problems. Consequently, numerical\nmethods like the Finite Element Method (FEM) are widely used. However, the FEM\nbecomes computationally expensive as problem complexity and accuracy demands\nincrease. Adaptive Mesh Refinement (AMR) improves the FEM by dynamically\nallocating mesh elements on the domain, balancing computational speed and\naccuracy. Classical AMR depends on heuristics or expensive error estimators,\nlimiting its use in complex simulations. While learning-based AMR methods are\npromising, they currently only scale to simple problems. In this work, we\nformulate AMR as a system of collaborating, homogeneous agents that iteratively\nsplit into multiple new agents. This agent-wise perspective enables a spatial\nreward formulation focused on reducing the maximum mesh element error. Our\napproach, Adaptive Swarm Mesh Refinement (ASMR), offers efficient, stable\noptimization and generates highly adaptive meshes at user-defined resolution\nduring inference. Extensive experiments, including volumetric meshes and\nNeumann boundary conditions, demonstrate that ASMR exceeds heuristic approaches\nand learned baselines, matching the performance of expensive error-based oracle\nAMR strategies. ASMR additionally generalizes to different domains during\ninference, and produces meshes that simulate up to 2 orders of magnitude faster\nthan uniform refinements in more demanding settings.\n", "link": "http://arxiv.org/abs/2406.08440v1", "date": "2024-06-12", "relevancy": 2.1842, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5822}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5229}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Swarm%20Mesh%20Refinement%20using%20Deep%20Reinforcement%20Learning%20with%0A%20%20Local%20Rewards&body=Title%3A%20Adaptive%20Swarm%20Mesh%20Refinement%20using%20Deep%20Reinforcement%20Learning%20with%0A%20%20Local%20Rewards%0AAuthor%3A%20Niklas%20Freymuth%20and%20Philipp%20Dahlinger%20and%20Tobias%20W%C3%BCrth%20and%20Simon%20Reisch%20and%20Luise%20K%C3%A4rger%20and%20Gerhard%20Neumann%0AAbstract%3A%20%20%20Simulating%20physical%20systems%20is%20essential%20in%20engineering%2C%20but%20analytical%0Asolutions%20are%20limited%20to%20straightforward%20problems.%20Consequently%2C%20numerical%0Amethods%20like%20the%20Finite%20Element%20Method%20%28FEM%29%20are%20widely%20used.%20However%2C%20the%20FEM%0Abecomes%20computationally%20expensive%20as%20problem%20complexity%20and%20accuracy%20demands%0Aincrease.%20Adaptive%20Mesh%20Refinement%20%28AMR%29%20improves%20the%20FEM%20by%20dynamically%0Aallocating%20mesh%20elements%20on%20the%20domain%2C%20balancing%20computational%20speed%20and%0Aaccuracy.%20Classical%20AMR%20depends%20on%20heuristics%20or%20expensive%20error%20estimators%2C%0Alimiting%20its%20use%20in%20complex%20simulations.%20While%20learning-based%20AMR%20methods%20are%0Apromising%2C%20they%20currently%20only%20scale%20to%20simple%20problems.%20In%20this%20work%2C%20we%0Aformulate%20AMR%20as%20a%20system%20of%20collaborating%2C%20homogeneous%20agents%20that%20iteratively%0Asplit%20into%20multiple%20new%20agents.%20This%20agent-wise%20perspective%20enables%20a%20spatial%0Areward%20formulation%20focused%20on%20reducing%20the%20maximum%20mesh%20element%20error.%20Our%0Aapproach%2C%20Adaptive%20Swarm%20Mesh%20Refinement%20%28ASMR%29%2C%20offers%20efficient%2C%20stable%0Aoptimization%20and%20generates%20highly%20adaptive%20meshes%20at%20user-defined%20resolution%0Aduring%20inference.%20Extensive%20experiments%2C%20including%20volumetric%20meshes%20and%0ANeumann%20boundary%20conditions%2C%20demonstrate%20that%20ASMR%20exceeds%20heuristic%20approaches%0Aand%20learned%20baselines%2C%20matching%20the%20performance%20of%20expensive%20error-based%20oracle%0AAMR%20strategies.%20ASMR%20additionally%20generalizes%20to%20different%20domains%20during%0Ainference%2C%20and%20produces%20meshes%20that%20simulate%20up%20to%202%20orders%20of%20magnitude%20faster%0Athan%20uniform%20refinements%20in%20more%20demanding%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Swarm%2520Mesh%2520Refinement%2520using%2520Deep%2520Reinforcement%2520Learning%2520with%250A%2520%2520Local%2520Rewards%26entry.906535625%3DNiklas%2520Freymuth%2520and%2520Philipp%2520Dahlinger%2520and%2520Tobias%2520W%25C3%25BCrth%2520and%2520Simon%2520Reisch%2520and%2520Luise%2520K%25C3%25A4rger%2520and%2520Gerhard%2520Neumann%26entry.1292438233%3D%2520%2520Simulating%2520physical%2520systems%2520is%2520essential%2520in%2520engineering%252C%2520but%2520analytical%250Asolutions%2520are%2520limited%2520to%2520straightforward%2520problems.%2520Consequently%252C%2520numerical%250Amethods%2520like%2520the%2520Finite%2520Element%2520Method%2520%2528FEM%2529%2520are%2520widely%2520used.%2520However%252C%2520the%2520FEM%250Abecomes%2520computationally%2520expensive%2520as%2520problem%2520complexity%2520and%2520accuracy%2520demands%250Aincrease.%2520Adaptive%2520Mesh%2520Refinement%2520%2528AMR%2529%2520improves%2520the%2520FEM%2520by%2520dynamically%250Aallocating%2520mesh%2520elements%2520on%2520the%2520domain%252C%2520balancing%2520computational%2520speed%2520and%250Aaccuracy.%2520Classical%2520AMR%2520depends%2520on%2520heuristics%2520or%2520expensive%2520error%2520estimators%252C%250Alimiting%2520its%2520use%2520in%2520complex%2520simulations.%2520While%2520learning-based%2520AMR%2520methods%2520are%250Apromising%252C%2520they%2520currently%2520only%2520scale%2520to%2520simple%2520problems.%2520In%2520this%2520work%252C%2520we%250Aformulate%2520AMR%2520as%2520a%2520system%2520of%2520collaborating%252C%2520homogeneous%2520agents%2520that%2520iteratively%250Asplit%2520into%2520multiple%2520new%2520agents.%2520This%2520agent-wise%2520perspective%2520enables%2520a%2520spatial%250Areward%2520formulation%2520focused%2520on%2520reducing%2520the%2520maximum%2520mesh%2520element%2520error.%2520Our%250Aapproach%252C%2520Adaptive%2520Swarm%2520Mesh%2520Refinement%2520%2528ASMR%2529%252C%2520offers%2520efficient%252C%2520stable%250Aoptimization%2520and%2520generates%2520highly%2520adaptive%2520meshes%2520at%2520user-defined%2520resolution%250Aduring%2520inference.%2520Extensive%2520experiments%252C%2520including%2520volumetric%2520meshes%2520and%250ANeumann%2520boundary%2520conditions%252C%2520demonstrate%2520that%2520ASMR%2520exceeds%2520heuristic%2520approaches%250Aand%2520learned%2520baselines%252C%2520matching%2520the%2520performance%2520of%2520expensive%2520error-based%2520oracle%250AAMR%2520strategies.%2520ASMR%2520additionally%2520generalizes%2520to%2520different%2520domains%2520during%250Ainference%252C%2520and%2520produces%2520meshes%2520that%2520simulate%2520up%2520to%25202%2520orders%2520of%2520magnitude%2520faster%250Athan%2520uniform%2520refinements%2520in%2520more%2520demanding%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Swarm%20Mesh%20Refinement%20using%20Deep%20Reinforcement%20Learning%20with%0A%20%20Local%20Rewards&entry.906535625=Niklas%20Freymuth%20and%20Philipp%20Dahlinger%20and%20Tobias%20W%C3%BCrth%20and%20Simon%20Reisch%20and%20Luise%20K%C3%A4rger%20and%20Gerhard%20Neumann&entry.1292438233=%20%20Simulating%20physical%20systems%20is%20essential%20in%20engineering%2C%20but%20analytical%0Asolutions%20are%20limited%20to%20straightforward%20problems.%20Consequently%2C%20numerical%0Amethods%20like%20the%20Finite%20Element%20Method%20%28FEM%29%20are%20widely%20used.%20However%2C%20the%20FEM%0Abecomes%20computationally%20expensive%20as%20problem%20complexity%20and%20accuracy%20demands%0Aincrease.%20Adaptive%20Mesh%20Refinement%20%28AMR%29%20improves%20the%20FEM%20by%20dynamically%0Aallocating%20mesh%20elements%20on%20the%20domain%2C%20balancing%20computational%20speed%20and%0Aaccuracy.%20Classical%20AMR%20depends%20on%20heuristics%20or%20expensive%20error%20estimators%2C%0Alimiting%20its%20use%20in%20complex%20simulations.%20While%20learning-based%20AMR%20methods%20are%0Apromising%2C%20they%20currently%20only%20scale%20to%20simple%20problems.%20In%20this%20work%2C%20we%0Aformulate%20AMR%20as%20a%20system%20of%20collaborating%2C%20homogeneous%20agents%20that%20iteratively%0Asplit%20into%20multiple%20new%20agents.%20This%20agent-wise%20perspective%20enables%20a%20spatial%0Areward%20formulation%20focused%20on%20reducing%20the%20maximum%20mesh%20element%20error.%20Our%0Aapproach%2C%20Adaptive%20Swarm%20Mesh%20Refinement%20%28ASMR%29%2C%20offers%20efficient%2C%20stable%0Aoptimization%20and%20generates%20highly%20adaptive%20meshes%20at%20user-defined%20resolution%0Aduring%20inference.%20Extensive%20experiments%2C%20including%20volumetric%20meshes%20and%0ANeumann%20boundary%20conditions%2C%20demonstrate%20that%20ASMR%20exceeds%20heuristic%20approaches%0Aand%20learned%20baselines%2C%20matching%20the%20performance%20of%20expensive%20error-based%20oracle%0AAMR%20strategies.%20ASMR%20additionally%20generalizes%20to%20different%20domains%20during%0Ainference%2C%20and%20produces%20meshes%20that%20simulate%20up%20to%202%20orders%20of%20magnitude%20faster%0Athan%20uniform%20refinements%20in%20more%20demanding%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08440v1&entry.124074799=Read"},
{"title": "SE(3)-Equivariant and Noise-Invariant 3D Rigid Motion Tracking in Brain\n  MRI", "author": "Benjamin Billot and Neel Dey and Daniel Moyer and Malte Hoffmann and Esra Abaci Turk and Borjan Gagoski and Ellen Grant and Polina Golland", "abstract": "  Rigid motion tracking is paramount in many medical imaging applications where\nmovements need to be detected, corrected, or accounted for. Modern strategies\nrely on convolutional neural networks (CNN) and pose this problem as rigid\nregistration. Yet, CNNs do not exploit natural symmetries in this task, as they\nare equivariant to translations (their outputs shift with their inputs) but not\nto rotations. Here we propose EquiTrack, the first method that uses recent\nsteerable SE(3)-equivariant CNNs (E-CNN) for motion tracking. While steerable\nE-CNNs can extract corresponding features across different poses, testing them\non noisy medical images reveals that they do not have enough learning capacity\nto learn noise invariance. Thus, we introduce a hybrid architecture that pairs\na denoiser with an E-CNN to decouple the processing of anatomically irrelevant\nintensity features from the extraction of equivariant spatial features. Rigid\ntransforms are then estimated in closed-form. EquiTrack outperforms\nstate-of-the-art learning and optimisation methods for motion tracking in adult\nbrain MRI and fetal MRI time series. Our code is available at\nhttps://github.com/BBillot/EquiTrack.\n", "link": "http://arxiv.org/abs/2312.13534v3", "date": "2024-06-12", "relevancy": 2.1717, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5642}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5368}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SE%283%29-Equivariant%20and%20Noise-Invariant%203D%20Rigid%20Motion%20Tracking%20in%20Brain%0A%20%20MRI&body=Title%3A%20SE%283%29-Equivariant%20and%20Noise-Invariant%203D%20Rigid%20Motion%20Tracking%20in%20Brain%0A%20%20MRI%0AAuthor%3A%20Benjamin%20Billot%20and%20Neel%20Dey%20and%20Daniel%20Moyer%20and%20Malte%20Hoffmann%20and%20Esra%20Abaci%20Turk%20and%20Borjan%20Gagoski%20and%20Ellen%20Grant%20and%20Polina%20Golland%0AAbstract%3A%20%20%20Rigid%20motion%20tracking%20is%20paramount%20in%20many%20medical%20imaging%20applications%20where%0Amovements%20need%20to%20be%20detected%2C%20corrected%2C%20or%20accounted%20for.%20Modern%20strategies%0Arely%20on%20convolutional%20neural%20networks%20%28CNN%29%20and%20pose%20this%20problem%20as%20rigid%0Aregistration.%20Yet%2C%20CNNs%20do%20not%20exploit%20natural%20symmetries%20in%20this%20task%2C%20as%20they%0Aare%20equivariant%20to%20translations%20%28their%20outputs%20shift%20with%20their%20inputs%29%20but%20not%0Ato%20rotations.%20Here%20we%20propose%20EquiTrack%2C%20the%20first%20method%20that%20uses%20recent%0Asteerable%20SE%283%29-equivariant%20CNNs%20%28E-CNN%29%20for%20motion%20tracking.%20While%20steerable%0AE-CNNs%20can%20extract%20corresponding%20features%20across%20different%20poses%2C%20testing%20them%0Aon%20noisy%20medical%20images%20reveals%20that%20they%20do%20not%20have%20enough%20learning%20capacity%0Ato%20learn%20noise%20invariance.%20Thus%2C%20we%20introduce%20a%20hybrid%20architecture%20that%20pairs%0Aa%20denoiser%20with%20an%20E-CNN%20to%20decouple%20the%20processing%20of%20anatomically%20irrelevant%0Aintensity%20features%20from%20the%20extraction%20of%20equivariant%20spatial%20features.%20Rigid%0Atransforms%20are%20then%20estimated%20in%20closed-form.%20EquiTrack%20outperforms%0Astate-of-the-art%20learning%20and%20optimisation%20methods%20for%20motion%20tracking%20in%20adult%0Abrain%20MRI%20and%20fetal%20MRI%20time%20series.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/BBillot/EquiTrack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13534v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSE%25283%2529-Equivariant%2520and%2520Noise-Invariant%25203D%2520Rigid%2520Motion%2520Tracking%2520in%2520Brain%250A%2520%2520MRI%26entry.906535625%3DBenjamin%2520Billot%2520and%2520Neel%2520Dey%2520and%2520Daniel%2520Moyer%2520and%2520Malte%2520Hoffmann%2520and%2520Esra%2520Abaci%2520Turk%2520and%2520Borjan%2520Gagoski%2520and%2520Ellen%2520Grant%2520and%2520Polina%2520Golland%26entry.1292438233%3D%2520%2520Rigid%2520motion%2520tracking%2520is%2520paramount%2520in%2520many%2520medical%2520imaging%2520applications%2520where%250Amovements%2520need%2520to%2520be%2520detected%252C%2520corrected%252C%2520or%2520accounted%2520for.%2520Modern%2520strategies%250Arely%2520on%2520convolutional%2520neural%2520networks%2520%2528CNN%2529%2520and%2520pose%2520this%2520problem%2520as%2520rigid%250Aregistration.%2520Yet%252C%2520CNNs%2520do%2520not%2520exploit%2520natural%2520symmetries%2520in%2520this%2520task%252C%2520as%2520they%250Aare%2520equivariant%2520to%2520translations%2520%2528their%2520outputs%2520shift%2520with%2520their%2520inputs%2529%2520but%2520not%250Ato%2520rotations.%2520Here%2520we%2520propose%2520EquiTrack%252C%2520the%2520first%2520method%2520that%2520uses%2520recent%250Asteerable%2520SE%25283%2529-equivariant%2520CNNs%2520%2528E-CNN%2529%2520for%2520motion%2520tracking.%2520While%2520steerable%250AE-CNNs%2520can%2520extract%2520corresponding%2520features%2520across%2520different%2520poses%252C%2520testing%2520them%250Aon%2520noisy%2520medical%2520images%2520reveals%2520that%2520they%2520do%2520not%2520have%2520enough%2520learning%2520capacity%250Ato%2520learn%2520noise%2520invariance.%2520Thus%252C%2520we%2520introduce%2520a%2520hybrid%2520architecture%2520that%2520pairs%250Aa%2520denoiser%2520with%2520an%2520E-CNN%2520to%2520decouple%2520the%2520processing%2520of%2520anatomically%2520irrelevant%250Aintensity%2520features%2520from%2520the%2520extraction%2520of%2520equivariant%2520spatial%2520features.%2520Rigid%250Atransforms%2520are%2520then%2520estimated%2520in%2520closed-form.%2520EquiTrack%2520outperforms%250Astate-of-the-art%2520learning%2520and%2520optimisation%2520methods%2520for%2520motion%2520tracking%2520in%2520adult%250Abrain%2520MRI%2520and%2520fetal%2520MRI%2520time%2520series.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/BBillot/EquiTrack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13534v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SE%283%29-Equivariant%20and%20Noise-Invariant%203D%20Rigid%20Motion%20Tracking%20in%20Brain%0A%20%20MRI&entry.906535625=Benjamin%20Billot%20and%20Neel%20Dey%20and%20Daniel%20Moyer%20and%20Malte%20Hoffmann%20and%20Esra%20Abaci%20Turk%20and%20Borjan%20Gagoski%20and%20Ellen%20Grant%20and%20Polina%20Golland&entry.1292438233=%20%20Rigid%20motion%20tracking%20is%20paramount%20in%20many%20medical%20imaging%20applications%20where%0Amovements%20need%20to%20be%20detected%2C%20corrected%2C%20or%20accounted%20for.%20Modern%20strategies%0Arely%20on%20convolutional%20neural%20networks%20%28CNN%29%20and%20pose%20this%20problem%20as%20rigid%0Aregistration.%20Yet%2C%20CNNs%20do%20not%20exploit%20natural%20symmetries%20in%20this%20task%2C%20as%20they%0Aare%20equivariant%20to%20translations%20%28their%20outputs%20shift%20with%20their%20inputs%29%20but%20not%0Ato%20rotations.%20Here%20we%20propose%20EquiTrack%2C%20the%20first%20method%20that%20uses%20recent%0Asteerable%20SE%283%29-equivariant%20CNNs%20%28E-CNN%29%20for%20motion%20tracking.%20While%20steerable%0AE-CNNs%20can%20extract%20corresponding%20features%20across%20different%20poses%2C%20testing%20them%0Aon%20noisy%20medical%20images%20reveals%20that%20they%20do%20not%20have%20enough%20learning%20capacity%0Ato%20learn%20noise%20invariance.%20Thus%2C%20we%20introduce%20a%20hybrid%20architecture%20that%20pairs%0Aa%20denoiser%20with%20an%20E-CNN%20to%20decouple%20the%20processing%20of%20anatomically%20irrelevant%0Aintensity%20features%20from%20the%20extraction%20of%20equivariant%20spatial%20features.%20Rigid%0Atransforms%20are%20then%20estimated%20in%20closed-form.%20EquiTrack%20outperforms%0Astate-of-the-art%20learning%20and%20optimisation%20methods%20for%20motion%20tracking%20in%20adult%0Abrain%20MRI%20and%20fetal%20MRI%20time%20series.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/BBillot/EquiTrack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13534v3&entry.124074799=Read"},
{"title": "OccFeat: Self-supervised Occupancy Feature Prediction for Pretraining\n  BEV Segmentation Networks", "author": "Sophia Sirko-Galouchenko and Alexandre Boulch and Spyros Gidaris and Andrei Bursuc and Antonin Vobecky and Patrick P\u00e9rez and Renaud Marlet", "abstract": "  We introduce a self-supervised pretraining method, called OccFeat, for\ncamera-only Bird's-Eye-View (BEV) segmentation networks. With OccFeat, we\npretrain a BEV network via occupancy prediction and feature distillation tasks.\nOccupancy prediction provides a 3D geometric understanding of the scene to the\nmodel. However, the geometry learned is class-agnostic. Hence, we add semantic\ninformation to the model in the 3D space through distillation from a\nself-supervised pretrained image foundation model. Models pretrained with our\nmethod exhibit improved BEV semantic segmentation performance, particularly in\nlow-data scenarios. Moreover, empirical results affirm the efficacy of\nintegrating feature distillation with 3D occupancy prediction in our\npretraining approach. Repository: https://github.com/valeoai/Occfeat\n", "link": "http://arxiv.org/abs/2404.14027v3", "date": "2024-06-12", "relevancy": 2.1661, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5725}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5315}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OccFeat%3A%20Self-supervised%20Occupancy%20Feature%20Prediction%20for%20Pretraining%0A%20%20BEV%20Segmentation%20Networks&body=Title%3A%20OccFeat%3A%20Self-supervised%20Occupancy%20Feature%20Prediction%20for%20Pretraining%0A%20%20BEV%20Segmentation%20Networks%0AAuthor%3A%20Sophia%20Sirko-Galouchenko%20and%20Alexandre%20Boulch%20and%20Spyros%20Gidaris%20and%20Andrei%20Bursuc%20and%20Antonin%20Vobecky%20and%20Patrick%20P%C3%A9rez%20and%20Renaud%20Marlet%0AAbstract%3A%20%20%20We%20introduce%20a%20self-supervised%20pretraining%20method%2C%20called%20OccFeat%2C%20for%0Acamera-only%20Bird%27s-Eye-View%20%28BEV%29%20segmentation%20networks.%20With%20OccFeat%2C%20we%0Apretrain%20a%20BEV%20network%20via%20occupancy%20prediction%20and%20feature%20distillation%20tasks.%0AOccupancy%20prediction%20provides%20a%203D%20geometric%20understanding%20of%20the%20scene%20to%20the%0Amodel.%20However%2C%20the%20geometry%20learned%20is%20class-agnostic.%20Hence%2C%20we%20add%20semantic%0Ainformation%20to%20the%20model%20in%20the%203D%20space%20through%20distillation%20from%20a%0Aself-supervised%20pretrained%20image%20foundation%20model.%20Models%20pretrained%20with%20our%0Amethod%20exhibit%20improved%20BEV%20semantic%20segmentation%20performance%2C%20particularly%20in%0Alow-data%20scenarios.%20Moreover%2C%20empirical%20results%20affirm%20the%20efficacy%20of%0Aintegrating%20feature%20distillation%20with%203D%20occupancy%20prediction%20in%20our%0Apretraining%20approach.%20Repository%3A%20https%3A//github.com/valeoai/Occfeat%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14027v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOccFeat%253A%2520Self-supervised%2520Occupancy%2520Feature%2520Prediction%2520for%2520Pretraining%250A%2520%2520BEV%2520Segmentation%2520Networks%26entry.906535625%3DSophia%2520Sirko-Galouchenko%2520and%2520Alexandre%2520Boulch%2520and%2520Spyros%2520Gidaris%2520and%2520Andrei%2520Bursuc%2520and%2520Antonin%2520Vobecky%2520and%2520Patrick%2520P%25C3%25A9rez%2520and%2520Renaud%2520Marlet%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520self-supervised%2520pretraining%2520method%252C%2520called%2520OccFeat%252C%2520for%250Acamera-only%2520Bird%2527s-Eye-View%2520%2528BEV%2529%2520segmentation%2520networks.%2520With%2520OccFeat%252C%2520we%250Apretrain%2520a%2520BEV%2520network%2520via%2520occupancy%2520prediction%2520and%2520feature%2520distillation%2520tasks.%250AOccupancy%2520prediction%2520provides%2520a%25203D%2520geometric%2520understanding%2520of%2520the%2520scene%2520to%2520the%250Amodel.%2520However%252C%2520the%2520geometry%2520learned%2520is%2520class-agnostic.%2520Hence%252C%2520we%2520add%2520semantic%250Ainformation%2520to%2520the%2520model%2520in%2520the%25203D%2520space%2520through%2520distillation%2520from%2520a%250Aself-supervised%2520pretrained%2520image%2520foundation%2520model.%2520Models%2520pretrained%2520with%2520our%250Amethod%2520exhibit%2520improved%2520BEV%2520semantic%2520segmentation%2520performance%252C%2520particularly%2520in%250Alow-data%2520scenarios.%2520Moreover%252C%2520empirical%2520results%2520affirm%2520the%2520efficacy%2520of%250Aintegrating%2520feature%2520distillation%2520with%25203D%2520occupancy%2520prediction%2520in%2520our%250Apretraining%2520approach.%2520Repository%253A%2520https%253A//github.com/valeoai/Occfeat%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14027v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OccFeat%3A%20Self-supervised%20Occupancy%20Feature%20Prediction%20for%20Pretraining%0A%20%20BEV%20Segmentation%20Networks&entry.906535625=Sophia%20Sirko-Galouchenko%20and%20Alexandre%20Boulch%20and%20Spyros%20Gidaris%20and%20Andrei%20Bursuc%20and%20Antonin%20Vobecky%20and%20Patrick%20P%C3%A9rez%20and%20Renaud%20Marlet&entry.1292438233=%20%20We%20introduce%20a%20self-supervised%20pretraining%20method%2C%20called%20OccFeat%2C%20for%0Acamera-only%20Bird%27s-Eye-View%20%28BEV%29%20segmentation%20networks.%20With%20OccFeat%2C%20we%0Apretrain%20a%20BEV%20network%20via%20occupancy%20prediction%20and%20feature%20distillation%20tasks.%0AOccupancy%20prediction%20provides%20a%203D%20geometric%20understanding%20of%20the%20scene%20to%20the%0Amodel.%20However%2C%20the%20geometry%20learned%20is%20class-agnostic.%20Hence%2C%20we%20add%20semantic%0Ainformation%20to%20the%20model%20in%20the%203D%20space%20through%20distillation%20from%20a%0Aself-supervised%20pretrained%20image%20foundation%20model.%20Models%20pretrained%20with%20our%0Amethod%20exhibit%20improved%20BEV%20semantic%20segmentation%20performance%2C%20particularly%20in%0Alow-data%20scenarios.%20Moreover%2C%20empirical%20results%20affirm%20the%20efficacy%20of%0Aintegrating%20feature%20distillation%20with%203D%20occupancy%20prediction%20in%20our%0Apretraining%20approach.%20Repository%3A%20https%3A//github.com/valeoai/Occfeat%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14027v3&entry.124074799=Read"},
{"title": "A PDE-based Explanation of Extreme Numerical Sensitivities and Edge of\n  Stability in Training Neural Networks", "author": "Yuxin Sun and Dong Lao and Ganesh Sundaramoorthi and Anthony Yezzi", "abstract": "  We discover restrained numerical instabilities in current training practices\nof deep networks with stochastic gradient descent (SGD), and its variants. We\nshow numerical error (on the order of the smallest floating point bit and thus\nthe most extreme or limiting numerical perturbations induced from floating\npoint arithmetic in training deep nets can be amplified significantly and\nresult in significant test accuracy variance (sensitivities), comparable to the\ntest accuracy variance due to stochasticity in SGD. We show how this is likely\ntraced to instabilities of the optimization dynamics that are restrained, i.e.,\nlocalized over iterations and regions of the weight tensor space. We do this by\npresenting a theoretical framework using numerical analysis of partial\ndifferential equations (PDE), and analyzing the gradient descent PDE of\nconvolutional neural networks (CNNs). We show that it is stable only under\ncertain conditions on the learning rate and weight decay. We show that rather\nthan blowing up when the conditions are violated, the instability can be\nrestrained. We show this is a consequence of the non-linear PDE associated with\nthe gradient descent of the CNN, whose local linearization changes when\nover-driving the step size of the discretization, resulting in a stabilizing\neffect. We link restrained instabilities to the recently discovered Edge of\nStability (EoS) phenomena, in which the stable step size predicted by classical\ntheory is exceeded while continuing to optimize the loss and still converging.\nBecause restrained instabilities occur at the EoS, our theory provides new\ninsights and predictions about the EoS, in particular, the role of\nregularization and the dependence on the network complexity.\n", "link": "http://arxiv.org/abs/2206.02001v4", "date": "2024-06-12", "relevancy": 2.166, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5947}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5107}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20PDE-based%20Explanation%20of%20Extreme%20Numerical%20Sensitivities%20and%20Edge%20of%0A%20%20Stability%20in%20Training%20Neural%20Networks&body=Title%3A%20A%20PDE-based%20Explanation%20of%20Extreme%20Numerical%20Sensitivities%20and%20Edge%20of%0A%20%20Stability%20in%20Training%20Neural%20Networks%0AAuthor%3A%20Yuxin%20Sun%20and%20Dong%20Lao%20and%20Ganesh%20Sundaramoorthi%20and%20Anthony%20Yezzi%0AAbstract%3A%20%20%20We%20discover%20restrained%20numerical%20instabilities%20in%20current%20training%20practices%0Aof%20deep%20networks%20with%20stochastic%20gradient%20descent%20%28SGD%29%2C%20and%20its%20variants.%20We%0Ashow%20numerical%20error%20%28on%20the%20order%20of%20the%20smallest%20floating%20point%20bit%20and%20thus%0Athe%20most%20extreme%20or%20limiting%20numerical%20perturbations%20induced%20from%20floating%0Apoint%20arithmetic%20in%20training%20deep%20nets%20can%20be%20amplified%20significantly%20and%0Aresult%20in%20significant%20test%20accuracy%20variance%20%28sensitivities%29%2C%20comparable%20to%20the%0Atest%20accuracy%20variance%20due%20to%20stochasticity%20in%20SGD.%20We%20show%20how%20this%20is%20likely%0Atraced%20to%20instabilities%20of%20the%20optimization%20dynamics%20that%20are%20restrained%2C%20i.e.%2C%0Alocalized%20over%20iterations%20and%20regions%20of%20the%20weight%20tensor%20space.%20We%20do%20this%20by%0Apresenting%20a%20theoretical%20framework%20using%20numerical%20analysis%20of%20partial%0Adifferential%20equations%20%28PDE%29%2C%20and%20analyzing%20the%20gradient%20descent%20PDE%20of%0Aconvolutional%20neural%20networks%20%28CNNs%29.%20We%20show%20that%20it%20is%20stable%20only%20under%0Acertain%20conditions%20on%20the%20learning%20rate%20and%20weight%20decay.%20We%20show%20that%20rather%0Athan%20blowing%20up%20when%20the%20conditions%20are%20violated%2C%20the%20instability%20can%20be%0Arestrained.%20We%20show%20this%20is%20a%20consequence%20of%20the%20non-linear%20PDE%20associated%20with%0Athe%20gradient%20descent%20of%20the%20CNN%2C%20whose%20local%20linearization%20changes%20when%0Aover-driving%20the%20step%20size%20of%20the%20discretization%2C%20resulting%20in%20a%20stabilizing%0Aeffect.%20We%20link%20restrained%20instabilities%20to%20the%20recently%20discovered%20Edge%20of%0AStability%20%28EoS%29%20phenomena%2C%20in%20which%20the%20stable%20step%20size%20predicted%20by%20classical%0Atheory%20is%20exceeded%20while%20continuing%20to%20optimize%20the%20loss%20and%20still%20converging.%0ABecause%20restrained%20instabilities%20occur%20at%20the%20EoS%2C%20our%20theory%20provides%20new%0Ainsights%20and%20predictions%20about%20the%20EoS%2C%20in%20particular%2C%20the%20role%20of%0Aregularization%20and%20the%20dependence%20on%20the%20network%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2206.02001v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520PDE-based%2520Explanation%2520of%2520Extreme%2520Numerical%2520Sensitivities%2520and%2520Edge%2520of%250A%2520%2520Stability%2520in%2520Training%2520Neural%2520Networks%26entry.906535625%3DYuxin%2520Sun%2520and%2520Dong%2520Lao%2520and%2520Ganesh%2520Sundaramoorthi%2520and%2520Anthony%2520Yezzi%26entry.1292438233%3D%2520%2520We%2520discover%2520restrained%2520numerical%2520instabilities%2520in%2520current%2520training%2520practices%250Aof%2520deep%2520networks%2520with%2520stochastic%2520gradient%2520descent%2520%2528SGD%2529%252C%2520and%2520its%2520variants.%2520We%250Ashow%2520numerical%2520error%2520%2528on%2520the%2520order%2520of%2520the%2520smallest%2520floating%2520point%2520bit%2520and%2520thus%250Athe%2520most%2520extreme%2520or%2520limiting%2520numerical%2520perturbations%2520induced%2520from%2520floating%250Apoint%2520arithmetic%2520in%2520training%2520deep%2520nets%2520can%2520be%2520amplified%2520significantly%2520and%250Aresult%2520in%2520significant%2520test%2520accuracy%2520variance%2520%2528sensitivities%2529%252C%2520comparable%2520to%2520the%250Atest%2520accuracy%2520variance%2520due%2520to%2520stochasticity%2520in%2520SGD.%2520We%2520show%2520how%2520this%2520is%2520likely%250Atraced%2520to%2520instabilities%2520of%2520the%2520optimization%2520dynamics%2520that%2520are%2520restrained%252C%2520i.e.%252C%250Alocalized%2520over%2520iterations%2520and%2520regions%2520of%2520the%2520weight%2520tensor%2520space.%2520We%2520do%2520this%2520by%250Apresenting%2520a%2520theoretical%2520framework%2520using%2520numerical%2520analysis%2520of%2520partial%250Adifferential%2520equations%2520%2528PDE%2529%252C%2520and%2520analyzing%2520the%2520gradient%2520descent%2520PDE%2520of%250Aconvolutional%2520neural%2520networks%2520%2528CNNs%2529.%2520We%2520show%2520that%2520it%2520is%2520stable%2520only%2520under%250Acertain%2520conditions%2520on%2520the%2520learning%2520rate%2520and%2520weight%2520decay.%2520We%2520show%2520that%2520rather%250Athan%2520blowing%2520up%2520when%2520the%2520conditions%2520are%2520violated%252C%2520the%2520instability%2520can%2520be%250Arestrained.%2520We%2520show%2520this%2520is%2520a%2520consequence%2520of%2520the%2520non-linear%2520PDE%2520associated%2520with%250Athe%2520gradient%2520descent%2520of%2520the%2520CNN%252C%2520whose%2520local%2520linearization%2520changes%2520when%250Aover-driving%2520the%2520step%2520size%2520of%2520the%2520discretization%252C%2520resulting%2520in%2520a%2520stabilizing%250Aeffect.%2520We%2520link%2520restrained%2520instabilities%2520to%2520the%2520recently%2520discovered%2520Edge%2520of%250AStability%2520%2528EoS%2529%2520phenomena%252C%2520in%2520which%2520the%2520stable%2520step%2520size%2520predicted%2520by%2520classical%250Atheory%2520is%2520exceeded%2520while%2520continuing%2520to%2520optimize%2520the%2520loss%2520and%2520still%2520converging.%250ABecause%2520restrained%2520instabilities%2520occur%2520at%2520the%2520EoS%252C%2520our%2520theory%2520provides%2520new%250Ainsights%2520and%2520predictions%2520about%2520the%2520EoS%252C%2520in%2520particular%252C%2520the%2520role%2520of%250Aregularization%2520and%2520the%2520dependence%2520on%2520the%2520network%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2206.02001v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20PDE-based%20Explanation%20of%20Extreme%20Numerical%20Sensitivities%20and%20Edge%20of%0A%20%20Stability%20in%20Training%20Neural%20Networks&entry.906535625=Yuxin%20Sun%20and%20Dong%20Lao%20and%20Ganesh%20Sundaramoorthi%20and%20Anthony%20Yezzi&entry.1292438233=%20%20We%20discover%20restrained%20numerical%20instabilities%20in%20current%20training%20practices%0Aof%20deep%20networks%20with%20stochastic%20gradient%20descent%20%28SGD%29%2C%20and%20its%20variants.%20We%0Ashow%20numerical%20error%20%28on%20the%20order%20of%20the%20smallest%20floating%20point%20bit%20and%20thus%0Athe%20most%20extreme%20or%20limiting%20numerical%20perturbations%20induced%20from%20floating%0Apoint%20arithmetic%20in%20training%20deep%20nets%20can%20be%20amplified%20significantly%20and%0Aresult%20in%20significant%20test%20accuracy%20variance%20%28sensitivities%29%2C%20comparable%20to%20the%0Atest%20accuracy%20variance%20due%20to%20stochasticity%20in%20SGD.%20We%20show%20how%20this%20is%20likely%0Atraced%20to%20instabilities%20of%20the%20optimization%20dynamics%20that%20are%20restrained%2C%20i.e.%2C%0Alocalized%20over%20iterations%20and%20regions%20of%20the%20weight%20tensor%20space.%20We%20do%20this%20by%0Apresenting%20a%20theoretical%20framework%20using%20numerical%20analysis%20of%20partial%0Adifferential%20equations%20%28PDE%29%2C%20and%20analyzing%20the%20gradient%20descent%20PDE%20of%0Aconvolutional%20neural%20networks%20%28CNNs%29.%20We%20show%20that%20it%20is%20stable%20only%20under%0Acertain%20conditions%20on%20the%20learning%20rate%20and%20weight%20decay.%20We%20show%20that%20rather%0Athan%20blowing%20up%20when%20the%20conditions%20are%20violated%2C%20the%20instability%20can%20be%0Arestrained.%20We%20show%20this%20is%20a%20consequence%20of%20the%20non-linear%20PDE%20associated%20with%0Athe%20gradient%20descent%20of%20the%20CNN%2C%20whose%20local%20linearization%20changes%20when%0Aover-driving%20the%20step%20size%20of%20the%20discretization%2C%20resulting%20in%20a%20stabilizing%0Aeffect.%20We%20link%20restrained%20instabilities%20to%20the%20recently%20discovered%20Edge%20of%0AStability%20%28EoS%29%20phenomena%2C%20in%20which%20the%20stable%20step%20size%20predicted%20by%20classical%0Atheory%20is%20exceeded%20while%20continuing%20to%20optimize%20the%20loss%20and%20still%20converging.%0ABecause%20restrained%20instabilities%20occur%20at%20the%20EoS%2C%20our%20theory%20provides%20new%0Ainsights%20and%20predictions%20about%20the%20EoS%2C%20in%20particular%2C%20the%20role%20of%0Aregularization%20and%20the%20dependence%20on%20the%20network%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2206.02001v4&entry.124074799=Read"},
{"title": "MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation\n  in Videos", "author": "Xuehai He and Weixi Feng and Kaizhi Zheng and Yujie Lu and Wanrong Zhu and Jiachen Li and Yue Fan and Jianfeng Wang and Linjie Li and Zhengyuan Yang and Kevin Lin and William Yang Wang and Lijuan Wang and Xin Eric Wang", "abstract": "  Multimodal Language Language Models (MLLMs) demonstrate the emerging\nabilities of \"world models\" -- interpreting and reasoning about complex\nreal-world dynamics. To assess these abilities, we posit videos are the ideal\nmedium, as they encapsulate rich representations of real-world dynamics and\ncausalities. To this end, we introduce MMWorld, a new benchmark for\nmulti-discipline, multi-faceted multimodal video understanding. MMWorld\ndistinguishes itself from previous video understanding benchmarks with two\nunique advantages: (1) multi-discipline, covering various disciplines that\noften require domain expertise for comprehensive understanding; (2)\nmulti-faceted reasoning, including explanation, counterfactual thinking, future\nprediction, etc. MMWorld consists of a human-annotated dataset to evaluate\nMLLMs with questions about the whole videos and a synthetic dataset to analyze\nMLLMs within a single modality of perception. Together, MMWorld encompasses\n1,910 videos across seven broad disciplines and 69 subdisciplines, complete\nwith 6,627 question-answer pairs and associated captions. The evaluation\nincludes 2 proprietary and 10 open-source MLLMs, which struggle on MMWorld\n(e.g., GPT-4V performs the best with only 52.3\\% accuracy), showing large room\nfor improvement. Further ablation studies reveal other interesting findings\nsuch as models' different skill sets from humans. We hope MMWorld can serve as\nan essential step towards world model evaluation in videos.\n", "link": "http://arxiv.org/abs/2406.08407v1", "date": "2024-06-12", "relevancy": 2.1505, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5432}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.536}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMWorld%3A%20Towards%20Multi-discipline%20Multi-faceted%20World%20Model%20Evaluation%0A%20%20in%20Videos&body=Title%3A%20MMWorld%3A%20Towards%20Multi-discipline%20Multi-faceted%20World%20Model%20Evaluation%0A%20%20in%20Videos%0AAuthor%3A%20Xuehai%20He%20and%20Weixi%20Feng%20and%20Kaizhi%20Zheng%20and%20Yujie%20Lu%20and%20Wanrong%20Zhu%20and%20Jiachen%20Li%20and%20Yue%20Fan%20and%20Jianfeng%20Wang%20and%20Linjie%20Li%20and%20Zhengyuan%20Yang%20and%20Kevin%20Lin%20and%20William%20Yang%20Wang%20and%20Lijuan%20Wang%20and%20Xin%20Eric%20Wang%0AAbstract%3A%20%20%20Multimodal%20Language%20Language%20Models%20%28MLLMs%29%20demonstrate%20the%20emerging%0Aabilities%20of%20%22world%20models%22%20--%20interpreting%20and%20reasoning%20about%20complex%0Areal-world%20dynamics.%20To%20assess%20these%20abilities%2C%20we%20posit%20videos%20are%20the%20ideal%0Amedium%2C%20as%20they%20encapsulate%20rich%20representations%20of%20real-world%20dynamics%20and%0Acausalities.%20To%20this%20end%2C%20we%20introduce%20MMWorld%2C%20a%20new%20benchmark%20for%0Amulti-discipline%2C%20multi-faceted%20multimodal%20video%20understanding.%20MMWorld%0Adistinguishes%20itself%20from%20previous%20video%20understanding%20benchmarks%20with%20two%0Aunique%20advantages%3A%20%281%29%20multi-discipline%2C%20covering%20various%20disciplines%20that%0Aoften%20require%20domain%20expertise%20for%20comprehensive%20understanding%3B%20%282%29%0Amulti-faceted%20reasoning%2C%20including%20explanation%2C%20counterfactual%20thinking%2C%20future%0Aprediction%2C%20etc.%20MMWorld%20consists%20of%20a%20human-annotated%20dataset%20to%20evaluate%0AMLLMs%20with%20questions%20about%20the%20whole%20videos%20and%20a%20synthetic%20dataset%20to%20analyze%0AMLLMs%20within%20a%20single%20modality%20of%20perception.%20Together%2C%20MMWorld%20encompasses%0A1%2C910%20videos%20across%20seven%20broad%20disciplines%20and%2069%20subdisciplines%2C%20complete%0Awith%206%2C627%20question-answer%20pairs%20and%20associated%20captions.%20The%20evaluation%0Aincludes%202%20proprietary%20and%2010%20open-source%20MLLMs%2C%20which%20struggle%20on%20MMWorld%0A%28e.g.%2C%20GPT-4V%20performs%20the%20best%20with%20only%2052.3%5C%25%20accuracy%29%2C%20showing%20large%20room%0Afor%20improvement.%20Further%20ablation%20studies%20reveal%20other%20interesting%20findings%0Asuch%20as%20models%27%20different%20skill%20sets%20from%20humans.%20We%20hope%20MMWorld%20can%20serve%20as%0Aan%20essential%20step%20towards%20world%20model%20evaluation%20in%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMWorld%253A%2520Towards%2520Multi-discipline%2520Multi-faceted%2520World%2520Model%2520Evaluation%250A%2520%2520in%2520Videos%26entry.906535625%3DXuehai%2520He%2520and%2520Weixi%2520Feng%2520and%2520Kaizhi%2520Zheng%2520and%2520Yujie%2520Lu%2520and%2520Wanrong%2520Zhu%2520and%2520Jiachen%2520Li%2520and%2520Yue%2520Fan%2520and%2520Jianfeng%2520Wang%2520and%2520Linjie%2520Li%2520and%2520Zhengyuan%2520Yang%2520and%2520Kevin%2520Lin%2520and%2520William%2520Yang%2520Wang%2520and%2520Lijuan%2520Wang%2520and%2520Xin%2520Eric%2520Wang%26entry.1292438233%3D%2520%2520Multimodal%2520Language%2520Language%2520Models%2520%2528MLLMs%2529%2520demonstrate%2520the%2520emerging%250Aabilities%2520of%2520%2522world%2520models%2522%2520--%2520interpreting%2520and%2520reasoning%2520about%2520complex%250Areal-world%2520dynamics.%2520To%2520assess%2520these%2520abilities%252C%2520we%2520posit%2520videos%2520are%2520the%2520ideal%250Amedium%252C%2520as%2520they%2520encapsulate%2520rich%2520representations%2520of%2520real-world%2520dynamics%2520and%250Acausalities.%2520To%2520this%2520end%252C%2520we%2520introduce%2520MMWorld%252C%2520a%2520new%2520benchmark%2520for%250Amulti-discipline%252C%2520multi-faceted%2520multimodal%2520video%2520understanding.%2520MMWorld%250Adistinguishes%2520itself%2520from%2520previous%2520video%2520understanding%2520benchmarks%2520with%2520two%250Aunique%2520advantages%253A%2520%25281%2529%2520multi-discipline%252C%2520covering%2520various%2520disciplines%2520that%250Aoften%2520require%2520domain%2520expertise%2520for%2520comprehensive%2520understanding%253B%2520%25282%2529%250Amulti-faceted%2520reasoning%252C%2520including%2520explanation%252C%2520counterfactual%2520thinking%252C%2520future%250Aprediction%252C%2520etc.%2520MMWorld%2520consists%2520of%2520a%2520human-annotated%2520dataset%2520to%2520evaluate%250AMLLMs%2520with%2520questions%2520about%2520the%2520whole%2520videos%2520and%2520a%2520synthetic%2520dataset%2520to%2520analyze%250AMLLMs%2520within%2520a%2520single%2520modality%2520of%2520perception.%2520Together%252C%2520MMWorld%2520encompasses%250A1%252C910%2520videos%2520across%2520seven%2520broad%2520disciplines%2520and%252069%2520subdisciplines%252C%2520complete%250Awith%25206%252C627%2520question-answer%2520pairs%2520and%2520associated%2520captions.%2520The%2520evaluation%250Aincludes%25202%2520proprietary%2520and%252010%2520open-source%2520MLLMs%252C%2520which%2520struggle%2520on%2520MMWorld%250A%2528e.g.%252C%2520GPT-4V%2520performs%2520the%2520best%2520with%2520only%252052.3%255C%2525%2520accuracy%2529%252C%2520showing%2520large%2520room%250Afor%2520improvement.%2520Further%2520ablation%2520studies%2520reveal%2520other%2520interesting%2520findings%250Asuch%2520as%2520models%2527%2520different%2520skill%2520sets%2520from%2520humans.%2520We%2520hope%2520MMWorld%2520can%2520serve%2520as%250Aan%2520essential%2520step%2520towards%2520world%2520model%2520evaluation%2520in%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMWorld%3A%20Towards%20Multi-discipline%20Multi-faceted%20World%20Model%20Evaluation%0A%20%20in%20Videos&entry.906535625=Xuehai%20He%20and%20Weixi%20Feng%20and%20Kaizhi%20Zheng%20and%20Yujie%20Lu%20and%20Wanrong%20Zhu%20and%20Jiachen%20Li%20and%20Yue%20Fan%20and%20Jianfeng%20Wang%20and%20Linjie%20Li%20and%20Zhengyuan%20Yang%20and%20Kevin%20Lin%20and%20William%20Yang%20Wang%20and%20Lijuan%20Wang%20and%20Xin%20Eric%20Wang&entry.1292438233=%20%20Multimodal%20Language%20Language%20Models%20%28MLLMs%29%20demonstrate%20the%20emerging%0Aabilities%20of%20%22world%20models%22%20--%20interpreting%20and%20reasoning%20about%20complex%0Areal-world%20dynamics.%20To%20assess%20these%20abilities%2C%20we%20posit%20videos%20are%20the%20ideal%0Amedium%2C%20as%20they%20encapsulate%20rich%20representations%20of%20real-world%20dynamics%20and%0Acausalities.%20To%20this%20end%2C%20we%20introduce%20MMWorld%2C%20a%20new%20benchmark%20for%0Amulti-discipline%2C%20multi-faceted%20multimodal%20video%20understanding.%20MMWorld%0Adistinguishes%20itself%20from%20previous%20video%20understanding%20benchmarks%20with%20two%0Aunique%20advantages%3A%20%281%29%20multi-discipline%2C%20covering%20various%20disciplines%20that%0Aoften%20require%20domain%20expertise%20for%20comprehensive%20understanding%3B%20%282%29%0Amulti-faceted%20reasoning%2C%20including%20explanation%2C%20counterfactual%20thinking%2C%20future%0Aprediction%2C%20etc.%20MMWorld%20consists%20of%20a%20human-annotated%20dataset%20to%20evaluate%0AMLLMs%20with%20questions%20about%20the%20whole%20videos%20and%20a%20synthetic%20dataset%20to%20analyze%0AMLLMs%20within%20a%20single%20modality%20of%20perception.%20Together%2C%20MMWorld%20encompasses%0A1%2C910%20videos%20across%20seven%20broad%20disciplines%20and%2069%20subdisciplines%2C%20complete%0Awith%206%2C627%20question-answer%20pairs%20and%20associated%20captions.%20The%20evaluation%0Aincludes%202%20proprietary%20and%2010%20open-source%20MLLMs%2C%20which%20struggle%20on%20MMWorld%0A%28e.g.%2C%20GPT-4V%20performs%20the%20best%20with%20only%2052.3%5C%25%20accuracy%29%2C%20showing%20large%20room%0Afor%20improvement.%20Further%20ablation%20studies%20reveal%20other%20interesting%20findings%0Asuch%20as%20models%27%20different%20skill%20sets%20from%20humans.%20We%20hope%20MMWorld%20can%20serve%20as%0Aan%20essential%20step%20towards%20world%20model%20evaluation%20in%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08407v1&entry.124074799=Read"},
{"title": "Learning H-Infinity Locomotion Control", "author": "Junfeng Long and Wenye Yu and Quanyi Li and Zirui Wang and Dahua Lin and Jiangmiao Pang", "abstract": "  Stable locomotion in precipitous environments is an essential task for\nquadruped robots, requiring the ability to resist various external\ndisturbances. Recent neural policies enhance robustness against disturbances by\nlearning to resist external forces sampled from a fixed distribution in the\nsimulated environment. However, the force generation process doesn't consider\nthe robot's current state, making it difficult to identify the most effective\ndirection and magnitude that can push the robot to the most unstable but\nrecoverable state. Thus, challenging cases in the buffer are insufficient to\noptimize robustness. In this paper, we propose to model the robust locomotion\nlearning process as an adversarial interaction between the locomotion policy\nand a learnable disturbance that is conditioned on the robot state to generate\nappropriate external forces. To make the joint optimization stable, our novel\n$H_{\\infty}$ constraint mandates the bound of the ratio between the cost and\nthe intensity of the external forces. We verify the robustness of our approach\nin both simulated environments and real-world deployment, on quadrupedal\nlocomotion tasks and a more challenging task where the quadruped performs\nlocomotion merely on hind legs. Training and deployment code will be made\npublic.\n", "link": "http://arxiv.org/abs/2404.14405v2", "date": "2024-06-12", "relevancy": 2.1326, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5698}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5356}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20H-Infinity%20Locomotion%20Control&body=Title%3A%20Learning%20H-Infinity%20Locomotion%20Control%0AAuthor%3A%20Junfeng%20Long%20and%20Wenye%20Yu%20and%20Quanyi%20Li%20and%20Zirui%20Wang%20and%20Dahua%20Lin%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20Stable%20locomotion%20in%20precipitous%20environments%20is%20an%20essential%20task%20for%0Aquadruped%20robots%2C%20requiring%20the%20ability%20to%20resist%20various%20external%0Adisturbances.%20Recent%20neural%20policies%20enhance%20robustness%20against%20disturbances%20by%0Alearning%20to%20resist%20external%20forces%20sampled%20from%20a%20fixed%20distribution%20in%20the%0Asimulated%20environment.%20However%2C%20the%20force%20generation%20process%20doesn%27t%20consider%0Athe%20robot%27s%20current%20state%2C%20making%20it%20difficult%20to%20identify%20the%20most%20effective%0Adirection%20and%20magnitude%20that%20can%20push%20the%20robot%20to%20the%20most%20unstable%20but%0Arecoverable%20state.%20Thus%2C%20challenging%20cases%20in%20the%20buffer%20are%20insufficient%20to%0Aoptimize%20robustness.%20In%20this%20paper%2C%20we%20propose%20to%20model%20the%20robust%20locomotion%0Alearning%20process%20as%20an%20adversarial%20interaction%20between%20the%20locomotion%20policy%0Aand%20a%20learnable%20disturbance%20that%20is%20conditioned%20on%20the%20robot%20state%20to%20generate%0Aappropriate%20external%20forces.%20To%20make%20the%20joint%20optimization%20stable%2C%20our%20novel%0A%24H_%7B%5Cinfty%7D%24%20constraint%20mandates%20the%20bound%20of%20the%20ratio%20between%20the%20cost%20and%0Athe%20intensity%20of%20the%20external%20forces.%20We%20verify%20the%20robustness%20of%20our%20approach%0Ain%20both%20simulated%20environments%20and%20real-world%20deployment%2C%20on%20quadrupedal%0Alocomotion%20tasks%20and%20a%20more%20challenging%20task%20where%20the%20quadruped%20performs%0Alocomotion%20merely%20on%20hind%20legs.%20Training%20and%20deployment%20code%20will%20be%20made%0Apublic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14405v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520H-Infinity%2520Locomotion%2520Control%26entry.906535625%3DJunfeng%2520Long%2520and%2520Wenye%2520Yu%2520and%2520Quanyi%2520Li%2520and%2520Zirui%2520Wang%2520and%2520Dahua%2520Lin%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520Stable%2520locomotion%2520in%2520precipitous%2520environments%2520is%2520an%2520essential%2520task%2520for%250Aquadruped%2520robots%252C%2520requiring%2520the%2520ability%2520to%2520resist%2520various%2520external%250Adisturbances.%2520Recent%2520neural%2520policies%2520enhance%2520robustness%2520against%2520disturbances%2520by%250Alearning%2520to%2520resist%2520external%2520forces%2520sampled%2520from%2520a%2520fixed%2520distribution%2520in%2520the%250Asimulated%2520environment.%2520However%252C%2520the%2520force%2520generation%2520process%2520doesn%2527t%2520consider%250Athe%2520robot%2527s%2520current%2520state%252C%2520making%2520it%2520difficult%2520to%2520identify%2520the%2520most%2520effective%250Adirection%2520and%2520magnitude%2520that%2520can%2520push%2520the%2520robot%2520to%2520the%2520most%2520unstable%2520but%250Arecoverable%2520state.%2520Thus%252C%2520challenging%2520cases%2520in%2520the%2520buffer%2520are%2520insufficient%2520to%250Aoptimize%2520robustness.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520model%2520the%2520robust%2520locomotion%250Alearning%2520process%2520as%2520an%2520adversarial%2520interaction%2520between%2520the%2520locomotion%2520policy%250Aand%2520a%2520learnable%2520disturbance%2520that%2520is%2520conditioned%2520on%2520the%2520robot%2520state%2520to%2520generate%250Aappropriate%2520external%2520forces.%2520To%2520make%2520the%2520joint%2520optimization%2520stable%252C%2520our%2520novel%250A%2524H_%257B%255Cinfty%257D%2524%2520constraint%2520mandates%2520the%2520bound%2520of%2520the%2520ratio%2520between%2520the%2520cost%2520and%250Athe%2520intensity%2520of%2520the%2520external%2520forces.%2520We%2520verify%2520the%2520robustness%2520of%2520our%2520approach%250Ain%2520both%2520simulated%2520environments%2520and%2520real-world%2520deployment%252C%2520on%2520quadrupedal%250Alocomotion%2520tasks%2520and%2520a%2520more%2520challenging%2520task%2520where%2520the%2520quadruped%2520performs%250Alocomotion%2520merely%2520on%2520hind%2520legs.%2520Training%2520and%2520deployment%2520code%2520will%2520be%2520made%250Apublic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14405v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20H-Infinity%20Locomotion%20Control&entry.906535625=Junfeng%20Long%20and%20Wenye%20Yu%20and%20Quanyi%20Li%20and%20Zirui%20Wang%20and%20Dahua%20Lin%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20Stable%20locomotion%20in%20precipitous%20environments%20is%20an%20essential%20task%20for%0Aquadruped%20robots%2C%20requiring%20the%20ability%20to%20resist%20various%20external%0Adisturbances.%20Recent%20neural%20policies%20enhance%20robustness%20against%20disturbances%20by%0Alearning%20to%20resist%20external%20forces%20sampled%20from%20a%20fixed%20distribution%20in%20the%0Asimulated%20environment.%20However%2C%20the%20force%20generation%20process%20doesn%27t%20consider%0Athe%20robot%27s%20current%20state%2C%20making%20it%20difficult%20to%20identify%20the%20most%20effective%0Adirection%20and%20magnitude%20that%20can%20push%20the%20robot%20to%20the%20most%20unstable%20but%0Arecoverable%20state.%20Thus%2C%20challenging%20cases%20in%20the%20buffer%20are%20insufficient%20to%0Aoptimize%20robustness.%20In%20this%20paper%2C%20we%20propose%20to%20model%20the%20robust%20locomotion%0Alearning%20process%20as%20an%20adversarial%20interaction%20between%20the%20locomotion%20policy%0Aand%20a%20learnable%20disturbance%20that%20is%20conditioned%20on%20the%20robot%20state%20to%20generate%0Aappropriate%20external%20forces.%20To%20make%20the%20joint%20optimization%20stable%2C%20our%20novel%0A%24H_%7B%5Cinfty%7D%24%20constraint%20mandates%20the%20bound%20of%20the%20ratio%20between%20the%20cost%20and%0Athe%20intensity%20of%20the%20external%20forces.%20We%20verify%20the%20robustness%20of%20our%20approach%0Ain%20both%20simulated%20environments%20and%20real-world%20deployment%2C%20on%20quadrupedal%0Alocomotion%20tasks%20and%20a%20more%20challenging%20task%20where%20the%20quadruped%20performs%0Alocomotion%20merely%20on%20hind%20legs.%20Training%20and%20deployment%20code%20will%20be%20made%0Apublic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14405v2&entry.124074799=Read"},
{"title": "A Sociotechnical Lens for Evaluating Computer Vision Models: A Case\n  Study on Detecting and Reasoning about Gender and Emotion", "author": "Sha Luo and Sang Jung Kim and Zening Duan and Kaiping Chen", "abstract": "  In the evolving landscape of computer vision (CV) technologies, the automatic\ndetection and interpretation of gender and emotion in images is a critical area\nof study. This paper investigates social biases in CV models, emphasizing the\nlimitations of traditional evaluation metrics such as precision, recall, and\naccuracy. These metrics often fall short in capturing the complexities of\ngender and emotion, which are fluid and culturally nuanced constructs. Our\nstudy proposes a sociotechnical framework for evaluating CV models,\nincorporating both technical performance measures and considerations of social\nfairness. Using a dataset of 5,570 images related to vaccination and climate\nchange, we empirically compared the performance of various CV models, including\ntraditional models like DeepFace and FER, and generative models like GPT-4\nVision. Our analysis involved manually validating the gender and emotional\nexpressions in a subset of images to serve as benchmarks. Our findings reveal\nthat while GPT-4 Vision outperforms other models in technical accuracy for\ngender classification, it exhibits discriminatory biases, particularly in\nresponse to transgender and non-binary personas. Furthermore, the model's\nemotion detection skew heavily towards positive emotions, with a notable bias\ntowards associating female images with happiness, especially when prompted by\nmale personas. These findings underscore the necessity of developing more\ncomprehensive evaluation criteria that address both validity and discriminatory\nbiases in CV models. Our proposed framework provides guidelines for researchers\nto critically assess CV tools, ensuring their application in communication\nresearch is both ethical and effective. The significant contribution of this\nstudy lies in its emphasis on a sociotechnical approach, advocating for CV\ntechnologies that support social good and mitigate biases rather than\nperpetuate them.\n", "link": "http://arxiv.org/abs/2406.08222v1", "date": "2024-06-12", "relevancy": 2.1281, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5597}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5277}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Sociotechnical%20Lens%20for%20Evaluating%20Computer%20Vision%20Models%3A%20A%20Case%0A%20%20Study%20on%20Detecting%20and%20Reasoning%20about%20Gender%20and%20Emotion&body=Title%3A%20A%20Sociotechnical%20Lens%20for%20Evaluating%20Computer%20Vision%20Models%3A%20A%20Case%0A%20%20Study%20on%20Detecting%20and%20Reasoning%20about%20Gender%20and%20Emotion%0AAuthor%3A%20Sha%20Luo%20and%20Sang%20Jung%20Kim%20and%20Zening%20Duan%20and%20Kaiping%20Chen%0AAbstract%3A%20%20%20In%20the%20evolving%20landscape%20of%20computer%20vision%20%28CV%29%20technologies%2C%20the%20automatic%0Adetection%20and%20interpretation%20of%20gender%20and%20emotion%20in%20images%20is%20a%20critical%20area%0Aof%20study.%20This%20paper%20investigates%20social%20biases%20in%20CV%20models%2C%20emphasizing%20the%0Alimitations%20of%20traditional%20evaluation%20metrics%20such%20as%20precision%2C%20recall%2C%20and%0Aaccuracy.%20These%20metrics%20often%20fall%20short%20in%20capturing%20the%20complexities%20of%0Agender%20and%20emotion%2C%20which%20are%20fluid%20and%20culturally%20nuanced%20constructs.%20Our%0Astudy%20proposes%20a%20sociotechnical%20framework%20for%20evaluating%20CV%20models%2C%0Aincorporating%20both%20technical%20performance%20measures%20and%20considerations%20of%20social%0Afairness.%20Using%20a%20dataset%20of%205%2C570%20images%20related%20to%20vaccination%20and%20climate%0Achange%2C%20we%20empirically%20compared%20the%20performance%20of%20various%20CV%20models%2C%20including%0Atraditional%20models%20like%20DeepFace%20and%20FER%2C%20and%20generative%20models%20like%20GPT-4%0AVision.%20Our%20analysis%20involved%20manually%20validating%20the%20gender%20and%20emotional%0Aexpressions%20in%20a%20subset%20of%20images%20to%20serve%20as%20benchmarks.%20Our%20findings%20reveal%0Athat%20while%20GPT-4%20Vision%20outperforms%20other%20models%20in%20technical%20accuracy%20for%0Agender%20classification%2C%20it%20exhibits%20discriminatory%20biases%2C%20particularly%20in%0Aresponse%20to%20transgender%20and%20non-binary%20personas.%20Furthermore%2C%20the%20model%27s%0Aemotion%20detection%20skew%20heavily%20towards%20positive%20emotions%2C%20with%20a%20notable%20bias%0Atowards%20associating%20female%20images%20with%20happiness%2C%20especially%20when%20prompted%20by%0Amale%20personas.%20These%20findings%20underscore%20the%20necessity%20of%20developing%20more%0Acomprehensive%20evaluation%20criteria%20that%20address%20both%20validity%20and%20discriminatory%0Abiases%20in%20CV%20models.%20Our%20proposed%20framework%20provides%20guidelines%20for%20researchers%0Ato%20critically%20assess%20CV%20tools%2C%20ensuring%20their%20application%20in%20communication%0Aresearch%20is%20both%20ethical%20and%20effective.%20The%20significant%20contribution%20of%20this%0Astudy%20lies%20in%20its%20emphasis%20on%20a%20sociotechnical%20approach%2C%20advocating%20for%20CV%0Atechnologies%20that%20support%20social%20good%20and%20mitigate%20biases%20rather%20than%0Aperpetuate%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08222v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Sociotechnical%2520Lens%2520for%2520Evaluating%2520Computer%2520Vision%2520Models%253A%2520A%2520Case%250A%2520%2520Study%2520on%2520Detecting%2520and%2520Reasoning%2520about%2520Gender%2520and%2520Emotion%26entry.906535625%3DSha%2520Luo%2520and%2520Sang%2520Jung%2520Kim%2520and%2520Zening%2520Duan%2520and%2520Kaiping%2520Chen%26entry.1292438233%3D%2520%2520In%2520the%2520evolving%2520landscape%2520of%2520computer%2520vision%2520%2528CV%2529%2520technologies%252C%2520the%2520automatic%250Adetection%2520and%2520interpretation%2520of%2520gender%2520and%2520emotion%2520in%2520images%2520is%2520a%2520critical%2520area%250Aof%2520study.%2520This%2520paper%2520investigates%2520social%2520biases%2520in%2520CV%2520models%252C%2520emphasizing%2520the%250Alimitations%2520of%2520traditional%2520evaluation%2520metrics%2520such%2520as%2520precision%252C%2520recall%252C%2520and%250Aaccuracy.%2520These%2520metrics%2520often%2520fall%2520short%2520in%2520capturing%2520the%2520complexities%2520of%250Agender%2520and%2520emotion%252C%2520which%2520are%2520fluid%2520and%2520culturally%2520nuanced%2520constructs.%2520Our%250Astudy%2520proposes%2520a%2520sociotechnical%2520framework%2520for%2520evaluating%2520CV%2520models%252C%250Aincorporating%2520both%2520technical%2520performance%2520measures%2520and%2520considerations%2520of%2520social%250Afairness.%2520Using%2520a%2520dataset%2520of%25205%252C570%2520images%2520related%2520to%2520vaccination%2520and%2520climate%250Achange%252C%2520we%2520empirically%2520compared%2520the%2520performance%2520of%2520various%2520CV%2520models%252C%2520including%250Atraditional%2520models%2520like%2520DeepFace%2520and%2520FER%252C%2520and%2520generative%2520models%2520like%2520GPT-4%250AVision.%2520Our%2520analysis%2520involved%2520manually%2520validating%2520the%2520gender%2520and%2520emotional%250Aexpressions%2520in%2520a%2520subset%2520of%2520images%2520to%2520serve%2520as%2520benchmarks.%2520Our%2520findings%2520reveal%250Athat%2520while%2520GPT-4%2520Vision%2520outperforms%2520other%2520models%2520in%2520technical%2520accuracy%2520for%250Agender%2520classification%252C%2520it%2520exhibits%2520discriminatory%2520biases%252C%2520particularly%2520in%250Aresponse%2520to%2520transgender%2520and%2520non-binary%2520personas.%2520Furthermore%252C%2520the%2520model%2527s%250Aemotion%2520detection%2520skew%2520heavily%2520towards%2520positive%2520emotions%252C%2520with%2520a%2520notable%2520bias%250Atowards%2520associating%2520female%2520images%2520with%2520happiness%252C%2520especially%2520when%2520prompted%2520by%250Amale%2520personas.%2520These%2520findings%2520underscore%2520the%2520necessity%2520of%2520developing%2520more%250Acomprehensive%2520evaluation%2520criteria%2520that%2520address%2520both%2520validity%2520and%2520discriminatory%250Abiases%2520in%2520CV%2520models.%2520Our%2520proposed%2520framework%2520provides%2520guidelines%2520for%2520researchers%250Ato%2520critically%2520assess%2520CV%2520tools%252C%2520ensuring%2520their%2520application%2520in%2520communication%250Aresearch%2520is%2520both%2520ethical%2520and%2520effective.%2520The%2520significant%2520contribution%2520of%2520this%250Astudy%2520lies%2520in%2520its%2520emphasis%2520on%2520a%2520sociotechnical%2520approach%252C%2520advocating%2520for%2520CV%250Atechnologies%2520that%2520support%2520social%2520good%2520and%2520mitigate%2520biases%2520rather%2520than%250Aperpetuate%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08222v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Sociotechnical%20Lens%20for%20Evaluating%20Computer%20Vision%20Models%3A%20A%20Case%0A%20%20Study%20on%20Detecting%20and%20Reasoning%20about%20Gender%20and%20Emotion&entry.906535625=Sha%20Luo%20and%20Sang%20Jung%20Kim%20and%20Zening%20Duan%20and%20Kaiping%20Chen&entry.1292438233=%20%20In%20the%20evolving%20landscape%20of%20computer%20vision%20%28CV%29%20technologies%2C%20the%20automatic%0Adetection%20and%20interpretation%20of%20gender%20and%20emotion%20in%20images%20is%20a%20critical%20area%0Aof%20study.%20This%20paper%20investigates%20social%20biases%20in%20CV%20models%2C%20emphasizing%20the%0Alimitations%20of%20traditional%20evaluation%20metrics%20such%20as%20precision%2C%20recall%2C%20and%0Aaccuracy.%20These%20metrics%20often%20fall%20short%20in%20capturing%20the%20complexities%20of%0Agender%20and%20emotion%2C%20which%20are%20fluid%20and%20culturally%20nuanced%20constructs.%20Our%0Astudy%20proposes%20a%20sociotechnical%20framework%20for%20evaluating%20CV%20models%2C%0Aincorporating%20both%20technical%20performance%20measures%20and%20considerations%20of%20social%0Afairness.%20Using%20a%20dataset%20of%205%2C570%20images%20related%20to%20vaccination%20and%20climate%0Achange%2C%20we%20empirically%20compared%20the%20performance%20of%20various%20CV%20models%2C%20including%0Atraditional%20models%20like%20DeepFace%20and%20FER%2C%20and%20generative%20models%20like%20GPT-4%0AVision.%20Our%20analysis%20involved%20manually%20validating%20the%20gender%20and%20emotional%0Aexpressions%20in%20a%20subset%20of%20images%20to%20serve%20as%20benchmarks.%20Our%20findings%20reveal%0Athat%20while%20GPT-4%20Vision%20outperforms%20other%20models%20in%20technical%20accuracy%20for%0Agender%20classification%2C%20it%20exhibits%20discriminatory%20biases%2C%20particularly%20in%0Aresponse%20to%20transgender%20and%20non-binary%20personas.%20Furthermore%2C%20the%20model%27s%0Aemotion%20detection%20skew%20heavily%20towards%20positive%20emotions%2C%20with%20a%20notable%20bias%0Atowards%20associating%20female%20images%20with%20happiness%2C%20especially%20when%20prompted%20by%0Amale%20personas.%20These%20findings%20underscore%20the%20necessity%20of%20developing%20more%0Acomprehensive%20evaluation%20criteria%20that%20address%20both%20validity%20and%20discriminatory%0Abiases%20in%20CV%20models.%20Our%20proposed%20framework%20provides%20guidelines%20for%20researchers%0Ato%20critically%20assess%20CV%20tools%2C%20ensuring%20their%20application%20in%20communication%0Aresearch%20is%20both%20ethical%20and%20effective.%20The%20significant%20contribution%20of%20this%0Astudy%20lies%20in%20its%20emphasis%20on%20a%20sociotechnical%20approach%2C%20advocating%20for%20CV%0Atechnologies%20that%20support%20social%20good%20and%20mitigate%20biases%20rather%20than%0Aperpetuate%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08222v1&entry.124074799=Read"},
{"title": "Trajectory optimization of tail-sitter considering speed constraints", "author": "Mingyue Fan and Fangfang Xie and Tingwei Ji and Yao Zheng", "abstract": "  Tail-sitters combine the advantages of fixed-wing unmanned aerial vehicles\n(UAVs) and vertical take-off and landing UAVs, and have been widely designed\nand researched in recent years. With the change in modern UAV application\nscenarios, it is required that UAVs have fast maneuverable three-dimensional\nflight capabilities. Due to the highly nonlinear aerodynamics produced by the\nfuselage and wings of the tail-sitter, how to quickly generate a smooth and\nexecutable trajectory is a problem that needs to be solved urgently. We\nconstrain the speed of the tail-sitter, eliminate the differential dynamics\nconstraints in the trajectory generation process of the tail-sitter through\ndifferential flatness, and allocate the time variable of the trajectory through\nthe state-of-the-art trajectory generation method named MINCO. Because we\ndiscretize the trajectory in time, we convert the speed constraint on the\nvehicle into a soft constraint, thereby achieving the time-optimal trajectory\nfor the tail-sitter to fly through any given waypoints.\n", "link": "http://arxiv.org/abs/2406.08347v1", "date": "2024-06-12", "relevancy": 2.1274, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4324}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4244}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trajectory%20optimization%20of%20tail-sitter%20considering%20speed%20constraints&body=Title%3A%20Trajectory%20optimization%20of%20tail-sitter%20considering%20speed%20constraints%0AAuthor%3A%20Mingyue%20Fan%20and%20Fangfang%20Xie%20and%20Tingwei%20Ji%20and%20Yao%20Zheng%0AAbstract%3A%20%20%20Tail-sitters%20combine%20the%20advantages%20of%20fixed-wing%20unmanned%20aerial%20vehicles%0A%28UAVs%29%20and%20vertical%20take-off%20and%20landing%20UAVs%2C%20and%20have%20been%20widely%20designed%0Aand%20researched%20in%20recent%20years.%20With%20the%20change%20in%20modern%20UAV%20application%0Ascenarios%2C%20it%20is%20required%20that%20UAVs%20have%20fast%20maneuverable%20three-dimensional%0Aflight%20capabilities.%20Due%20to%20the%20highly%20nonlinear%20aerodynamics%20produced%20by%20the%0Afuselage%20and%20wings%20of%20the%20tail-sitter%2C%20how%20to%20quickly%20generate%20a%20smooth%20and%0Aexecutable%20trajectory%20is%20a%20problem%20that%20needs%20to%20be%20solved%20urgently.%20We%0Aconstrain%20the%20speed%20of%20the%20tail-sitter%2C%20eliminate%20the%20differential%20dynamics%0Aconstraints%20in%20the%20trajectory%20generation%20process%20of%20the%20tail-sitter%20through%0Adifferential%20flatness%2C%20and%20allocate%20the%20time%20variable%20of%20the%20trajectory%20through%0Athe%20state-of-the-art%20trajectory%20generation%20method%20named%20MINCO.%20Because%20we%0Adiscretize%20the%20trajectory%20in%20time%2C%20we%20convert%20the%20speed%20constraint%20on%20the%0Avehicle%20into%20a%20soft%20constraint%2C%20thereby%20achieving%20the%20time-optimal%20trajectory%0Afor%20the%20tail-sitter%20to%20fly%20through%20any%20given%20waypoints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08347v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrajectory%2520optimization%2520of%2520tail-sitter%2520considering%2520speed%2520constraints%26entry.906535625%3DMingyue%2520Fan%2520and%2520Fangfang%2520Xie%2520and%2520Tingwei%2520Ji%2520and%2520Yao%2520Zheng%26entry.1292438233%3D%2520%2520Tail-sitters%2520combine%2520the%2520advantages%2520of%2520fixed-wing%2520unmanned%2520aerial%2520vehicles%250A%2528UAVs%2529%2520and%2520vertical%2520take-off%2520and%2520landing%2520UAVs%252C%2520and%2520have%2520been%2520widely%2520designed%250Aand%2520researched%2520in%2520recent%2520years.%2520With%2520the%2520change%2520in%2520modern%2520UAV%2520application%250Ascenarios%252C%2520it%2520is%2520required%2520that%2520UAVs%2520have%2520fast%2520maneuverable%2520three-dimensional%250Aflight%2520capabilities.%2520Due%2520to%2520the%2520highly%2520nonlinear%2520aerodynamics%2520produced%2520by%2520the%250Afuselage%2520and%2520wings%2520of%2520the%2520tail-sitter%252C%2520how%2520to%2520quickly%2520generate%2520a%2520smooth%2520and%250Aexecutable%2520trajectory%2520is%2520a%2520problem%2520that%2520needs%2520to%2520be%2520solved%2520urgently.%2520We%250Aconstrain%2520the%2520speed%2520of%2520the%2520tail-sitter%252C%2520eliminate%2520the%2520differential%2520dynamics%250Aconstraints%2520in%2520the%2520trajectory%2520generation%2520process%2520of%2520the%2520tail-sitter%2520through%250Adifferential%2520flatness%252C%2520and%2520allocate%2520the%2520time%2520variable%2520of%2520the%2520trajectory%2520through%250Athe%2520state-of-the-art%2520trajectory%2520generation%2520method%2520named%2520MINCO.%2520Because%2520we%250Adiscretize%2520the%2520trajectory%2520in%2520time%252C%2520we%2520convert%2520the%2520speed%2520constraint%2520on%2520the%250Avehicle%2520into%2520a%2520soft%2520constraint%252C%2520thereby%2520achieving%2520the%2520time-optimal%2520trajectory%250Afor%2520the%2520tail-sitter%2520to%2520fly%2520through%2520any%2520given%2520waypoints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08347v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trajectory%20optimization%20of%20tail-sitter%20considering%20speed%20constraints&entry.906535625=Mingyue%20Fan%20and%20Fangfang%20Xie%20and%20Tingwei%20Ji%20and%20Yao%20Zheng&entry.1292438233=%20%20Tail-sitters%20combine%20the%20advantages%20of%20fixed-wing%20unmanned%20aerial%20vehicles%0A%28UAVs%29%20and%20vertical%20take-off%20and%20landing%20UAVs%2C%20and%20have%20been%20widely%20designed%0Aand%20researched%20in%20recent%20years.%20With%20the%20change%20in%20modern%20UAV%20application%0Ascenarios%2C%20it%20is%20required%20that%20UAVs%20have%20fast%20maneuverable%20three-dimensional%0Aflight%20capabilities.%20Due%20to%20the%20highly%20nonlinear%20aerodynamics%20produced%20by%20the%0Afuselage%20and%20wings%20of%20the%20tail-sitter%2C%20how%20to%20quickly%20generate%20a%20smooth%20and%0Aexecutable%20trajectory%20is%20a%20problem%20that%20needs%20to%20be%20solved%20urgently.%20We%0Aconstrain%20the%20speed%20of%20the%20tail-sitter%2C%20eliminate%20the%20differential%20dynamics%0Aconstraints%20in%20the%20trajectory%20generation%20process%20of%20the%20tail-sitter%20through%0Adifferential%20flatness%2C%20and%20allocate%20the%20time%20variable%20of%20the%20trajectory%20through%0Athe%20state-of-the-art%20trajectory%20generation%20method%20named%20MINCO.%20Because%20we%0Adiscretize%20the%20trajectory%20in%20time%2C%20we%20convert%20the%20speed%20constraint%20on%20the%0Avehicle%20into%20a%20soft%20constraint%2C%20thereby%20achieving%20the%20time-optimal%20trajectory%0Afor%20the%20tail-sitter%20to%20fly%20through%20any%20given%20waypoints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08347v1&entry.124074799=Read"},
{"title": "A Unified Characterization of Private Learnability via Graph Theory", "author": "Noga Alon and Shay Moran and Hilla Schefler and Amir Yehudayoff", "abstract": "  We provide a unified framework for characterizing pure and approximate\ndifferentially private (DP) learnability. The framework uses the language of\ngraph theory: for a concept class $\\mathcal{H}$, we define the contradiction\ngraph $G$ of $\\mathcal{H}$. Its vertices are realizable datasets, and two\ndatasets $S,S'$ are connected by an edge if they contradict each other (i.e.,\nthere is a point $x$ that is labeled differently in $S$ and $S'$). Our main\nfinding is that the combinatorial structure of $G$ is deeply related to\nlearning $\\mathcal{H}$ under DP. Learning $\\mathcal{H}$ under pure DP is\ncaptured by the fractional clique number of $G$. Learning $\\mathcal{H}$ under\napproximate DP is captured by the clique number of $G$. Consequently, we\nidentify graph-theoretic dimensions that characterize DP learnability: the\nclique dimension and fractional clique dimension. Along the way, we reveal\nproperties of the contradiction graph which may be of independent interest. We\nalso suggest several open questions and directions for future research.\n", "link": "http://arxiv.org/abs/2304.03996v4", "date": "2024-06-12", "relevancy": 2.1235, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4369}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4238}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Characterization%20of%20Private%20Learnability%20via%20Graph%20Theory&body=Title%3A%20A%20Unified%20Characterization%20of%20Private%20Learnability%20via%20Graph%20Theory%0AAuthor%3A%20Noga%20Alon%20and%20Shay%20Moran%20and%20Hilla%20Schefler%20and%20Amir%20Yehudayoff%0AAbstract%3A%20%20%20We%20provide%20a%20unified%20framework%20for%20characterizing%20pure%20and%20approximate%0Adifferentially%20private%20%28DP%29%20learnability.%20The%20framework%20uses%20the%20language%20of%0Agraph%20theory%3A%20for%20a%20concept%20class%20%24%5Cmathcal%7BH%7D%24%2C%20we%20define%20the%20contradiction%0Agraph%20%24G%24%20of%20%24%5Cmathcal%7BH%7D%24.%20Its%20vertices%20are%20realizable%20datasets%2C%20and%20two%0Adatasets%20%24S%2CS%27%24%20are%20connected%20by%20an%20edge%20if%20they%20contradict%20each%20other%20%28i.e.%2C%0Athere%20is%20a%20point%20%24x%24%20that%20is%20labeled%20differently%20in%20%24S%24%20and%20%24S%27%24%29.%20Our%20main%0Afinding%20is%20that%20the%20combinatorial%20structure%20of%20%24G%24%20is%20deeply%20related%20to%0Alearning%20%24%5Cmathcal%7BH%7D%24%20under%20DP.%20Learning%20%24%5Cmathcal%7BH%7D%24%20under%20pure%20DP%20is%0Acaptured%20by%20the%20fractional%20clique%20number%20of%20%24G%24.%20Learning%20%24%5Cmathcal%7BH%7D%24%20under%0Aapproximate%20DP%20is%20captured%20by%20the%20clique%20number%20of%20%24G%24.%20Consequently%2C%20we%0Aidentify%20graph-theoretic%20dimensions%20that%20characterize%20DP%20learnability%3A%20the%0Aclique%20dimension%20and%20fractional%20clique%20dimension.%20Along%20the%20way%2C%20we%20reveal%0Aproperties%20of%20the%20contradiction%20graph%20which%20may%20be%20of%20independent%20interest.%20We%0Aalso%20suggest%20several%20open%20questions%20and%20directions%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.03996v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Characterization%2520of%2520Private%2520Learnability%2520via%2520Graph%2520Theory%26entry.906535625%3DNoga%2520Alon%2520and%2520Shay%2520Moran%2520and%2520Hilla%2520Schefler%2520and%2520Amir%2520Yehudayoff%26entry.1292438233%3D%2520%2520We%2520provide%2520a%2520unified%2520framework%2520for%2520characterizing%2520pure%2520and%2520approximate%250Adifferentially%2520private%2520%2528DP%2529%2520learnability.%2520The%2520framework%2520uses%2520the%2520language%2520of%250Agraph%2520theory%253A%2520for%2520a%2520concept%2520class%2520%2524%255Cmathcal%257BH%257D%2524%252C%2520we%2520define%2520the%2520contradiction%250Agraph%2520%2524G%2524%2520of%2520%2524%255Cmathcal%257BH%257D%2524.%2520Its%2520vertices%2520are%2520realizable%2520datasets%252C%2520and%2520two%250Adatasets%2520%2524S%252CS%2527%2524%2520are%2520connected%2520by%2520an%2520edge%2520if%2520they%2520contradict%2520each%2520other%2520%2528i.e.%252C%250Athere%2520is%2520a%2520point%2520%2524x%2524%2520that%2520is%2520labeled%2520differently%2520in%2520%2524S%2524%2520and%2520%2524S%2527%2524%2529.%2520Our%2520main%250Afinding%2520is%2520that%2520the%2520combinatorial%2520structure%2520of%2520%2524G%2524%2520is%2520deeply%2520related%2520to%250Alearning%2520%2524%255Cmathcal%257BH%257D%2524%2520under%2520DP.%2520Learning%2520%2524%255Cmathcal%257BH%257D%2524%2520under%2520pure%2520DP%2520is%250Acaptured%2520by%2520the%2520fractional%2520clique%2520number%2520of%2520%2524G%2524.%2520Learning%2520%2524%255Cmathcal%257BH%257D%2524%2520under%250Aapproximate%2520DP%2520is%2520captured%2520by%2520the%2520clique%2520number%2520of%2520%2524G%2524.%2520Consequently%252C%2520we%250Aidentify%2520graph-theoretic%2520dimensions%2520that%2520characterize%2520DP%2520learnability%253A%2520the%250Aclique%2520dimension%2520and%2520fractional%2520clique%2520dimension.%2520Along%2520the%2520way%252C%2520we%2520reveal%250Aproperties%2520of%2520the%2520contradiction%2520graph%2520which%2520may%2520be%2520of%2520independent%2520interest.%2520We%250Aalso%2520suggest%2520several%2520open%2520questions%2520and%2520directions%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.03996v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Characterization%20of%20Private%20Learnability%20via%20Graph%20Theory&entry.906535625=Noga%20Alon%20and%20Shay%20Moran%20and%20Hilla%20Schefler%20and%20Amir%20Yehudayoff&entry.1292438233=%20%20We%20provide%20a%20unified%20framework%20for%20characterizing%20pure%20and%20approximate%0Adifferentially%20private%20%28DP%29%20learnability.%20The%20framework%20uses%20the%20language%20of%0Agraph%20theory%3A%20for%20a%20concept%20class%20%24%5Cmathcal%7BH%7D%24%2C%20we%20define%20the%20contradiction%0Agraph%20%24G%24%20of%20%24%5Cmathcal%7BH%7D%24.%20Its%20vertices%20are%20realizable%20datasets%2C%20and%20two%0Adatasets%20%24S%2CS%27%24%20are%20connected%20by%20an%20edge%20if%20they%20contradict%20each%20other%20%28i.e.%2C%0Athere%20is%20a%20point%20%24x%24%20that%20is%20labeled%20differently%20in%20%24S%24%20and%20%24S%27%24%29.%20Our%20main%0Afinding%20is%20that%20the%20combinatorial%20structure%20of%20%24G%24%20is%20deeply%20related%20to%0Alearning%20%24%5Cmathcal%7BH%7D%24%20under%20DP.%20Learning%20%24%5Cmathcal%7BH%7D%24%20under%20pure%20DP%20is%0Acaptured%20by%20the%20fractional%20clique%20number%20of%20%24G%24.%20Learning%20%24%5Cmathcal%7BH%7D%24%20under%0Aapproximate%20DP%20is%20captured%20by%20the%20clique%20number%20of%20%24G%24.%20Consequently%2C%20we%0Aidentify%20graph-theoretic%20dimensions%20that%20characterize%20DP%20learnability%3A%20the%0Aclique%20dimension%20and%20fractional%20clique%20dimension.%20Along%20the%20way%2C%20we%20reveal%0Aproperties%20of%20the%20contradiction%20graph%20which%20may%20be%20of%20independent%20interest.%20We%0Aalso%20suggest%20several%20open%20questions%20and%20directions%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.03996v4&entry.124074799=Read"},
{"title": "PanoTree: Autonomous Photo-Spot Explorer in Virtual Reality Scenes", "author": "Tomohiro Hayase and Sacha Braun and Hikari Yanagawa and Itsuki Orito and Yuichi Hiroi", "abstract": "  Social VR platforms enable social, economic, and creative activities by\nallowing users to create and share their own virtual spaces. In social VR,\nphotography within a VR scene is an important indicator of visitors'\nactivities. Although automatic identification of photo spots within a VR scene\ncan facilitate the process of creating a VR scene and enhance the visitor\nexperience, there are challenges in quantitatively evaluating photos taken in\nthe VR scene and efficiently exploring the large VR scene. We propose PanoTree,\nan automated photo-spot explorer in VR scenes. To assess the aesthetics of\nimages captured in VR scenes, a deep scoring network is trained on a large\ndataset of photos collected by a social VR platform to determine whether humans\nare likely to take similar photos. Furthermore, we propose a Hierarchical\nOptimistic Optimization (HOO)-based search algorithm to efficiently explore 3D\nVR spaces with the reward from the scoring network. Our user study shows that\nthe scoring network achieves human-level performance in distinguishing randomly\ntaken images from those taken by humans. In addition, we show applications\nusing the explored photo spots, such as automatic thumbnail generation, support\nfor VR world creation, and visitor flow planning within a VR scene.\n", "link": "http://arxiv.org/abs/2405.17136v2", "date": "2024-06-12", "relevancy": 2.1218, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5313}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5313}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PanoTree%3A%20Autonomous%20Photo-Spot%20Explorer%20in%20Virtual%20Reality%20Scenes&body=Title%3A%20PanoTree%3A%20Autonomous%20Photo-Spot%20Explorer%20in%20Virtual%20Reality%20Scenes%0AAuthor%3A%20Tomohiro%20Hayase%20and%20Sacha%20Braun%20and%20Hikari%20Yanagawa%20and%20Itsuki%20Orito%20and%20Yuichi%20Hiroi%0AAbstract%3A%20%20%20Social%20VR%20platforms%20enable%20social%2C%20economic%2C%20and%20creative%20activities%20by%0Aallowing%20users%20to%20create%20and%20share%20their%20own%20virtual%20spaces.%20In%20social%20VR%2C%0Aphotography%20within%20a%20VR%20scene%20is%20an%20important%20indicator%20of%20visitors%27%0Aactivities.%20Although%20automatic%20identification%20of%20photo%20spots%20within%20a%20VR%20scene%0Acan%20facilitate%20the%20process%20of%20creating%20a%20VR%20scene%20and%20enhance%20the%20visitor%0Aexperience%2C%20there%20are%20challenges%20in%20quantitatively%20evaluating%20photos%20taken%20in%0Athe%20VR%20scene%20and%20efficiently%20exploring%20the%20large%20VR%20scene.%20We%20propose%20PanoTree%2C%0Aan%20automated%20photo-spot%20explorer%20in%20VR%20scenes.%20To%20assess%20the%20aesthetics%20of%0Aimages%20captured%20in%20VR%20scenes%2C%20a%20deep%20scoring%20network%20is%20trained%20on%20a%20large%0Adataset%20of%20photos%20collected%20by%20a%20social%20VR%20platform%20to%20determine%20whether%20humans%0Aare%20likely%20to%20take%20similar%20photos.%20Furthermore%2C%20we%20propose%20a%20Hierarchical%0AOptimistic%20Optimization%20%28HOO%29-based%20search%20algorithm%20to%20efficiently%20explore%203D%0AVR%20spaces%20with%20the%20reward%20from%20the%20scoring%20network.%20Our%20user%20study%20shows%20that%0Athe%20scoring%20network%20achieves%20human-level%20performance%20in%20distinguishing%20randomly%0Ataken%20images%20from%20those%20taken%20by%20humans.%20In%20addition%2C%20we%20show%20applications%0Ausing%20the%20explored%20photo%20spots%2C%20such%20as%20automatic%20thumbnail%20generation%2C%20support%0Afor%20VR%20world%20creation%2C%20and%20visitor%20flow%20planning%20within%20a%20VR%20scene.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17136v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanoTree%253A%2520Autonomous%2520Photo-Spot%2520Explorer%2520in%2520Virtual%2520Reality%2520Scenes%26entry.906535625%3DTomohiro%2520Hayase%2520and%2520Sacha%2520Braun%2520and%2520Hikari%2520Yanagawa%2520and%2520Itsuki%2520Orito%2520and%2520Yuichi%2520Hiroi%26entry.1292438233%3D%2520%2520Social%2520VR%2520platforms%2520enable%2520social%252C%2520economic%252C%2520and%2520creative%2520activities%2520by%250Aallowing%2520users%2520to%2520create%2520and%2520share%2520their%2520own%2520virtual%2520spaces.%2520In%2520social%2520VR%252C%250Aphotography%2520within%2520a%2520VR%2520scene%2520is%2520an%2520important%2520indicator%2520of%2520visitors%2527%250Aactivities.%2520Although%2520automatic%2520identification%2520of%2520photo%2520spots%2520within%2520a%2520VR%2520scene%250Acan%2520facilitate%2520the%2520process%2520of%2520creating%2520a%2520VR%2520scene%2520and%2520enhance%2520the%2520visitor%250Aexperience%252C%2520there%2520are%2520challenges%2520in%2520quantitatively%2520evaluating%2520photos%2520taken%2520in%250Athe%2520VR%2520scene%2520and%2520efficiently%2520exploring%2520the%2520large%2520VR%2520scene.%2520We%2520propose%2520PanoTree%252C%250Aan%2520automated%2520photo-spot%2520explorer%2520in%2520VR%2520scenes.%2520To%2520assess%2520the%2520aesthetics%2520of%250Aimages%2520captured%2520in%2520VR%2520scenes%252C%2520a%2520deep%2520scoring%2520network%2520is%2520trained%2520on%2520a%2520large%250Adataset%2520of%2520photos%2520collected%2520by%2520a%2520social%2520VR%2520platform%2520to%2520determine%2520whether%2520humans%250Aare%2520likely%2520to%2520take%2520similar%2520photos.%2520Furthermore%252C%2520we%2520propose%2520a%2520Hierarchical%250AOptimistic%2520Optimization%2520%2528HOO%2529-based%2520search%2520algorithm%2520to%2520efficiently%2520explore%25203D%250AVR%2520spaces%2520with%2520the%2520reward%2520from%2520the%2520scoring%2520network.%2520Our%2520user%2520study%2520shows%2520that%250Athe%2520scoring%2520network%2520achieves%2520human-level%2520performance%2520in%2520distinguishing%2520randomly%250Ataken%2520images%2520from%2520those%2520taken%2520by%2520humans.%2520In%2520addition%252C%2520we%2520show%2520applications%250Ausing%2520the%2520explored%2520photo%2520spots%252C%2520such%2520as%2520automatic%2520thumbnail%2520generation%252C%2520support%250Afor%2520VR%2520world%2520creation%252C%2520and%2520visitor%2520flow%2520planning%2520within%2520a%2520VR%2520scene.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17136v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PanoTree%3A%20Autonomous%20Photo-Spot%20Explorer%20in%20Virtual%20Reality%20Scenes&entry.906535625=Tomohiro%20Hayase%20and%20Sacha%20Braun%20and%20Hikari%20Yanagawa%20and%20Itsuki%20Orito%20and%20Yuichi%20Hiroi&entry.1292438233=%20%20Social%20VR%20platforms%20enable%20social%2C%20economic%2C%20and%20creative%20activities%20by%0Aallowing%20users%20to%20create%20and%20share%20their%20own%20virtual%20spaces.%20In%20social%20VR%2C%0Aphotography%20within%20a%20VR%20scene%20is%20an%20important%20indicator%20of%20visitors%27%0Aactivities.%20Although%20automatic%20identification%20of%20photo%20spots%20within%20a%20VR%20scene%0Acan%20facilitate%20the%20process%20of%20creating%20a%20VR%20scene%20and%20enhance%20the%20visitor%0Aexperience%2C%20there%20are%20challenges%20in%20quantitatively%20evaluating%20photos%20taken%20in%0Athe%20VR%20scene%20and%20efficiently%20exploring%20the%20large%20VR%20scene.%20We%20propose%20PanoTree%2C%0Aan%20automated%20photo-spot%20explorer%20in%20VR%20scenes.%20To%20assess%20the%20aesthetics%20of%0Aimages%20captured%20in%20VR%20scenes%2C%20a%20deep%20scoring%20network%20is%20trained%20on%20a%20large%0Adataset%20of%20photos%20collected%20by%20a%20social%20VR%20platform%20to%20determine%20whether%20humans%0Aare%20likely%20to%20take%20similar%20photos.%20Furthermore%2C%20we%20propose%20a%20Hierarchical%0AOptimistic%20Optimization%20%28HOO%29-based%20search%20algorithm%20to%20efficiently%20explore%203D%0AVR%20spaces%20with%20the%20reward%20from%20the%20scoring%20network.%20Our%20user%20study%20shows%20that%0Athe%20scoring%20network%20achieves%20human-level%20performance%20in%20distinguishing%20randomly%0Ataken%20images%20from%20those%20taken%20by%20humans.%20In%20addition%2C%20we%20show%20applications%0Ausing%20the%20explored%20photo%20spots%2C%20such%20as%20automatic%20thumbnail%20generation%2C%20support%0Afor%20VR%20world%20creation%2C%20and%20visitor%20flow%20planning%20within%20a%20VR%20scene.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17136v2&entry.124074799=Read"},
{"title": "From a Social Cognitive Perspective: Context-aware Visual Social\n  Relationship Recognition", "author": "Shiwei Wu and Chao Zhang and Joya Chen and Tong Xu and Likang Wu and Yao Hu and Enhong Chen", "abstract": "  People's social relationships are often manifested through their\nsurroundings, with certain objects or interactions acting as symbols for\nspecific relationships, e.g., wedding rings, roses, hugs, or holding hands.\nThis brings unique challenges to recognizing social relationships, requiring\nunderstanding and capturing the essence of these contexts from visual\nappearances. However, current methods of social relationship understanding rely\non the basic classification paradigm of detected persons and objects, which\nfails to understand the comprehensive context and often overlooks decisive\nsocial factors, especially subtle visual cues. To highlight the social-aware\ncontext and intricate details, we propose a novel approach that recognizes\n\\textbf{Con}textual \\textbf{So}cial \\textbf{R}elationships (\\textbf{ConSoR})\nfrom a social cognitive perspective. Specifically, to incorporate social-aware\nsemantics, we build a lightweight adapter upon the frozen CLIP to learn social\nconcepts via our novel multi-modal side adapter tuning mechanism. Further, we\nconstruct social-aware descriptive language prompts (e.g., scene, activity,\nobjects, emotions) with social relationships for each image, and then compel\nConSoR to concentrate more intensively on the decisive visual social factors\nvia visual-linguistic contrasting. Impressively, ConSoR outperforms previous\nmethods with a 12.2\\% gain on the People-in-Social-Context (PISC) dataset and a\n9.8\\% increase on the People-in-Photo-Album (PIPA) benchmark. Furthermore, we\nobserve that ConSoR excels at finding critical visual evidence to reveal social\nrelationships.\n", "link": "http://arxiv.org/abs/2406.08358v1", "date": "2024-06-12", "relevancy": 2.1206, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5412}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5237}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20a%20Social%20Cognitive%20Perspective%3A%20Context-aware%20Visual%20Social%0A%20%20Relationship%20Recognition&body=Title%3A%20From%20a%20Social%20Cognitive%20Perspective%3A%20Context-aware%20Visual%20Social%0A%20%20Relationship%20Recognition%0AAuthor%3A%20Shiwei%20Wu%20and%20Chao%20Zhang%20and%20Joya%20Chen%20and%20Tong%20Xu%20and%20Likang%20Wu%20and%20Yao%20Hu%20and%20Enhong%20Chen%0AAbstract%3A%20%20%20People%27s%20social%20relationships%20are%20often%20manifested%20through%20their%0Asurroundings%2C%20with%20certain%20objects%20or%20interactions%20acting%20as%20symbols%20for%0Aspecific%20relationships%2C%20e.g.%2C%20wedding%20rings%2C%20roses%2C%20hugs%2C%20or%20holding%20hands.%0AThis%20brings%20unique%20challenges%20to%20recognizing%20social%20relationships%2C%20requiring%0Aunderstanding%20and%20capturing%20the%20essence%20of%20these%20contexts%20from%20visual%0Aappearances.%20However%2C%20current%20methods%20of%20social%20relationship%20understanding%20rely%0Aon%20the%20basic%20classification%20paradigm%20of%20detected%20persons%20and%20objects%2C%20which%0Afails%20to%20understand%20the%20comprehensive%20context%20and%20often%20overlooks%20decisive%0Asocial%20factors%2C%20especially%20subtle%20visual%20cues.%20To%20highlight%20the%20social-aware%0Acontext%20and%20intricate%20details%2C%20we%20propose%20a%20novel%20approach%20that%20recognizes%0A%5Ctextbf%7BCon%7Dtextual%20%5Ctextbf%7BSo%7Dcial%20%5Ctextbf%7BR%7Delationships%20%28%5Ctextbf%7BConSoR%7D%29%0Afrom%20a%20social%20cognitive%20perspective.%20Specifically%2C%20to%20incorporate%20social-aware%0Asemantics%2C%20we%20build%20a%20lightweight%20adapter%20upon%20the%20frozen%20CLIP%20to%20learn%20social%0Aconcepts%20via%20our%20novel%20multi-modal%20side%20adapter%20tuning%20mechanism.%20Further%2C%20we%0Aconstruct%20social-aware%20descriptive%20language%20prompts%20%28e.g.%2C%20scene%2C%20activity%2C%0Aobjects%2C%20emotions%29%20with%20social%20relationships%20for%20each%20image%2C%20and%20then%20compel%0AConSoR%20to%20concentrate%20more%20intensively%20on%20the%20decisive%20visual%20social%20factors%0Avia%20visual-linguistic%20contrasting.%20Impressively%2C%20ConSoR%20outperforms%20previous%0Amethods%20with%20a%2012.2%5C%25%20gain%20on%20the%20People-in-Social-Context%20%28PISC%29%20dataset%20and%20a%0A9.8%5C%25%20increase%20on%20the%20People-in-Photo-Album%20%28PIPA%29%20benchmark.%20Furthermore%2C%20we%0Aobserve%20that%20ConSoR%20excels%20at%20finding%20critical%20visual%20evidence%20to%20reveal%20social%0Arelationships.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08358v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520a%2520Social%2520Cognitive%2520Perspective%253A%2520Context-aware%2520Visual%2520Social%250A%2520%2520Relationship%2520Recognition%26entry.906535625%3DShiwei%2520Wu%2520and%2520Chao%2520Zhang%2520and%2520Joya%2520Chen%2520and%2520Tong%2520Xu%2520and%2520Likang%2520Wu%2520and%2520Yao%2520Hu%2520and%2520Enhong%2520Chen%26entry.1292438233%3D%2520%2520People%2527s%2520social%2520relationships%2520are%2520often%2520manifested%2520through%2520their%250Asurroundings%252C%2520with%2520certain%2520objects%2520or%2520interactions%2520acting%2520as%2520symbols%2520for%250Aspecific%2520relationships%252C%2520e.g.%252C%2520wedding%2520rings%252C%2520roses%252C%2520hugs%252C%2520or%2520holding%2520hands.%250AThis%2520brings%2520unique%2520challenges%2520to%2520recognizing%2520social%2520relationships%252C%2520requiring%250Aunderstanding%2520and%2520capturing%2520the%2520essence%2520of%2520these%2520contexts%2520from%2520visual%250Aappearances.%2520However%252C%2520current%2520methods%2520of%2520social%2520relationship%2520understanding%2520rely%250Aon%2520the%2520basic%2520classification%2520paradigm%2520of%2520detected%2520persons%2520and%2520objects%252C%2520which%250Afails%2520to%2520understand%2520the%2520comprehensive%2520context%2520and%2520often%2520overlooks%2520decisive%250Asocial%2520factors%252C%2520especially%2520subtle%2520visual%2520cues.%2520To%2520highlight%2520the%2520social-aware%250Acontext%2520and%2520intricate%2520details%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%2520recognizes%250A%255Ctextbf%257BCon%257Dtextual%2520%255Ctextbf%257BSo%257Dcial%2520%255Ctextbf%257BR%257Delationships%2520%2528%255Ctextbf%257BConSoR%257D%2529%250Afrom%2520a%2520social%2520cognitive%2520perspective.%2520Specifically%252C%2520to%2520incorporate%2520social-aware%250Asemantics%252C%2520we%2520build%2520a%2520lightweight%2520adapter%2520upon%2520the%2520frozen%2520CLIP%2520to%2520learn%2520social%250Aconcepts%2520via%2520our%2520novel%2520multi-modal%2520side%2520adapter%2520tuning%2520mechanism.%2520Further%252C%2520we%250Aconstruct%2520social-aware%2520descriptive%2520language%2520prompts%2520%2528e.g.%252C%2520scene%252C%2520activity%252C%250Aobjects%252C%2520emotions%2529%2520with%2520social%2520relationships%2520for%2520each%2520image%252C%2520and%2520then%2520compel%250AConSoR%2520to%2520concentrate%2520more%2520intensively%2520on%2520the%2520decisive%2520visual%2520social%2520factors%250Avia%2520visual-linguistic%2520contrasting.%2520Impressively%252C%2520ConSoR%2520outperforms%2520previous%250Amethods%2520with%2520a%252012.2%255C%2525%2520gain%2520on%2520the%2520People-in-Social-Context%2520%2528PISC%2529%2520dataset%2520and%2520a%250A9.8%255C%2525%2520increase%2520on%2520the%2520People-in-Photo-Album%2520%2528PIPA%2529%2520benchmark.%2520Furthermore%252C%2520we%250Aobserve%2520that%2520ConSoR%2520excels%2520at%2520finding%2520critical%2520visual%2520evidence%2520to%2520reveal%2520social%250Arelationships.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08358v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20a%20Social%20Cognitive%20Perspective%3A%20Context-aware%20Visual%20Social%0A%20%20Relationship%20Recognition&entry.906535625=Shiwei%20Wu%20and%20Chao%20Zhang%20and%20Joya%20Chen%20and%20Tong%20Xu%20and%20Likang%20Wu%20and%20Yao%20Hu%20and%20Enhong%20Chen&entry.1292438233=%20%20People%27s%20social%20relationships%20are%20often%20manifested%20through%20their%0Asurroundings%2C%20with%20certain%20objects%20or%20interactions%20acting%20as%20symbols%20for%0Aspecific%20relationships%2C%20e.g.%2C%20wedding%20rings%2C%20roses%2C%20hugs%2C%20or%20holding%20hands.%0AThis%20brings%20unique%20challenges%20to%20recognizing%20social%20relationships%2C%20requiring%0Aunderstanding%20and%20capturing%20the%20essence%20of%20these%20contexts%20from%20visual%0Aappearances.%20However%2C%20current%20methods%20of%20social%20relationship%20understanding%20rely%0Aon%20the%20basic%20classification%20paradigm%20of%20detected%20persons%20and%20objects%2C%20which%0Afails%20to%20understand%20the%20comprehensive%20context%20and%20often%20overlooks%20decisive%0Asocial%20factors%2C%20especially%20subtle%20visual%20cues.%20To%20highlight%20the%20social-aware%0Acontext%20and%20intricate%20details%2C%20we%20propose%20a%20novel%20approach%20that%20recognizes%0A%5Ctextbf%7BCon%7Dtextual%20%5Ctextbf%7BSo%7Dcial%20%5Ctextbf%7BR%7Delationships%20%28%5Ctextbf%7BConSoR%7D%29%0Afrom%20a%20social%20cognitive%20perspective.%20Specifically%2C%20to%20incorporate%20social-aware%0Asemantics%2C%20we%20build%20a%20lightweight%20adapter%20upon%20the%20frozen%20CLIP%20to%20learn%20social%0Aconcepts%20via%20our%20novel%20multi-modal%20side%20adapter%20tuning%20mechanism.%20Further%2C%20we%0Aconstruct%20social-aware%20descriptive%20language%20prompts%20%28e.g.%2C%20scene%2C%20activity%2C%0Aobjects%2C%20emotions%29%20with%20social%20relationships%20for%20each%20image%2C%20and%20then%20compel%0AConSoR%20to%20concentrate%20more%20intensively%20on%20the%20decisive%20visual%20social%20factors%0Avia%20visual-linguistic%20contrasting.%20Impressively%2C%20ConSoR%20outperforms%20previous%0Amethods%20with%20a%2012.2%5C%25%20gain%20on%20the%20People-in-Social-Context%20%28PISC%29%20dataset%20and%20a%0A9.8%5C%25%20increase%20on%20the%20People-in-Photo-Album%20%28PIPA%29%20benchmark.%20Furthermore%2C%20we%0Aobserve%20that%20ConSoR%20excels%20at%20finding%20critical%20visual%20evidence%20to%20reveal%20social%0Arelationships.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08358v1&entry.124074799=Read"},
{"title": "Eyes Wide Unshut: Unsupervised Mistake Detection in Egocentric Video by\n  Detecting Unpredictable Gaze", "author": "Michele Mazzamuto and Antonino Furnari and Giovanni Maria Farinella", "abstract": "  In this paper, we address the challenge of unsupervised mistake detection in\negocentric video through the analysis of gaze signals, a critical component for\nadvancing user assistance in smart glasses. Traditional supervised methods,\nreliant on manually labeled mistakes, suffer from domain-dependence and\nscalability issues. This research introduces an unsupervised method for\ndetecting mistakes in videos of human activities, overcoming the challenges of\ndomain-specific requirements and the necessity for annotated data. By analyzing\nunusual gaze patterns that signal user disorientation during tasks, we propose\na gaze completion model that forecasts eye gaze trajectories from incomplete\ninputs. The difference between the anticipated and observed gaze paths acts as\nan indicator for identifying errors. Our method is validated on the EPIC-Tent\ndataset, showing its superiority compared to current one-class supervised and\nunsupervised techniques.\n", "link": "http://arxiv.org/abs/2406.08379v1", "date": "2024-06-12", "relevancy": 2.1195, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5367}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5341}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eyes%20Wide%20Unshut%3A%20Unsupervised%20Mistake%20Detection%20in%20Egocentric%20Video%20by%0A%20%20Detecting%20Unpredictable%20Gaze&body=Title%3A%20Eyes%20Wide%20Unshut%3A%20Unsupervised%20Mistake%20Detection%20in%20Egocentric%20Video%20by%0A%20%20Detecting%20Unpredictable%20Gaze%0AAuthor%3A%20Michele%20Mazzamuto%20and%20Antonino%20Furnari%20and%20Giovanni%20Maria%20Farinella%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20the%20challenge%20of%20unsupervised%20mistake%20detection%20in%0Aegocentric%20video%20through%20the%20analysis%20of%20gaze%20signals%2C%20a%20critical%20component%20for%0Aadvancing%20user%20assistance%20in%20smart%20glasses.%20Traditional%20supervised%20methods%2C%0Areliant%20on%20manually%20labeled%20mistakes%2C%20suffer%20from%20domain-dependence%20and%0Ascalability%20issues.%20This%20research%20introduces%20an%20unsupervised%20method%20for%0Adetecting%20mistakes%20in%20videos%20of%20human%20activities%2C%20overcoming%20the%20challenges%20of%0Adomain-specific%20requirements%20and%20the%20necessity%20for%20annotated%20data.%20By%20analyzing%0Aunusual%20gaze%20patterns%20that%20signal%20user%20disorientation%20during%20tasks%2C%20we%20propose%0Aa%20gaze%20completion%20model%20that%20forecasts%20eye%20gaze%20trajectories%20from%20incomplete%0Ainputs.%20The%20difference%20between%20the%20anticipated%20and%20observed%20gaze%20paths%20acts%20as%0Aan%20indicator%20for%20identifying%20errors.%20Our%20method%20is%20validated%20on%20the%20EPIC-Tent%0Adataset%2C%20showing%20its%20superiority%20compared%20to%20current%20one-class%20supervised%20and%0Aunsupervised%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08379v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEyes%2520Wide%2520Unshut%253A%2520Unsupervised%2520Mistake%2520Detection%2520in%2520Egocentric%2520Video%2520by%250A%2520%2520Detecting%2520Unpredictable%2520Gaze%26entry.906535625%3DMichele%2520Mazzamuto%2520and%2520Antonino%2520Furnari%2520and%2520Giovanni%2520Maria%2520Farinella%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520challenge%2520of%2520unsupervised%2520mistake%2520detection%2520in%250Aegocentric%2520video%2520through%2520the%2520analysis%2520of%2520gaze%2520signals%252C%2520a%2520critical%2520component%2520for%250Aadvancing%2520user%2520assistance%2520in%2520smart%2520glasses.%2520Traditional%2520supervised%2520methods%252C%250Areliant%2520on%2520manually%2520labeled%2520mistakes%252C%2520suffer%2520from%2520domain-dependence%2520and%250Ascalability%2520issues.%2520This%2520research%2520introduces%2520an%2520unsupervised%2520method%2520for%250Adetecting%2520mistakes%2520in%2520videos%2520of%2520human%2520activities%252C%2520overcoming%2520the%2520challenges%2520of%250Adomain-specific%2520requirements%2520and%2520the%2520necessity%2520for%2520annotated%2520data.%2520By%2520analyzing%250Aunusual%2520gaze%2520patterns%2520that%2520signal%2520user%2520disorientation%2520during%2520tasks%252C%2520we%2520propose%250Aa%2520gaze%2520completion%2520model%2520that%2520forecasts%2520eye%2520gaze%2520trajectories%2520from%2520incomplete%250Ainputs.%2520The%2520difference%2520between%2520the%2520anticipated%2520and%2520observed%2520gaze%2520paths%2520acts%2520as%250Aan%2520indicator%2520for%2520identifying%2520errors.%2520Our%2520method%2520is%2520validated%2520on%2520the%2520EPIC-Tent%250Adataset%252C%2520showing%2520its%2520superiority%2520compared%2520to%2520current%2520one-class%2520supervised%2520and%250Aunsupervised%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08379v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eyes%20Wide%20Unshut%3A%20Unsupervised%20Mistake%20Detection%20in%20Egocentric%20Video%20by%0A%20%20Detecting%20Unpredictable%20Gaze&entry.906535625=Michele%20Mazzamuto%20and%20Antonino%20Furnari%20and%20Giovanni%20Maria%20Farinella&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20challenge%20of%20unsupervised%20mistake%20detection%20in%0Aegocentric%20video%20through%20the%20analysis%20of%20gaze%20signals%2C%20a%20critical%20component%20for%0Aadvancing%20user%20assistance%20in%20smart%20glasses.%20Traditional%20supervised%20methods%2C%0Areliant%20on%20manually%20labeled%20mistakes%2C%20suffer%20from%20domain-dependence%20and%0Ascalability%20issues.%20This%20research%20introduces%20an%20unsupervised%20method%20for%0Adetecting%20mistakes%20in%20videos%20of%20human%20activities%2C%20overcoming%20the%20challenges%20of%0Adomain-specific%20requirements%20and%20the%20necessity%20for%20annotated%20data.%20By%20analyzing%0Aunusual%20gaze%20patterns%20that%20signal%20user%20disorientation%20during%20tasks%2C%20we%20propose%0Aa%20gaze%20completion%20model%20that%20forecasts%20eye%20gaze%20trajectories%20from%20incomplete%0Ainputs.%20The%20difference%20between%20the%20anticipated%20and%20observed%20gaze%20paths%20acts%20as%0Aan%20indicator%20for%20identifying%20errors.%20Our%20method%20is%20validated%20on%20the%20EPIC-Tent%0Adataset%2C%20showing%20its%20superiority%20compared%20to%20current%20one-class%20supervised%20and%0Aunsupervised%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08379v1&entry.124074799=Read"},
{"title": "OmniCorpus: An Unified Multimodal Corpus of 10 Billion-Level Images\n  Interleaved with Text", "author": "Qingyun Li and Zhe Chen and Weiyun Wang and Wenhai Wang and Shenglong Ye and Zhenjiang Jin and Guanzhou Chen and Yinan He and Zhangwei Gao and Erfei Cui and Jiashuo Yu and Hao Tian and Jiasheng Zhou and Chao Xu and Bin Wang and Xingjian Wei and Wei Li and Wenjian Zhang and Bo Zhang and Pinlong Cai and Licheng Wen and Xiangchao Yan and Pei Chu and Yi Wang and Min Dou and Changyao Tian and Xizhou Zhu and Lewei Lu and Yushi Chen and Junjun He and Tong Lu and Yali Wang and Limin Wang and Dahua Lin and Yu Qiao and Botian Shi and Conghui He and Jifeng Dai", "abstract": "  Image-text interleaved data, consisting of multiple images and texts arranged\nin a natural document format, aligns with the presentation paradigm of internet\ndata and closely resembles human reading habits. Recent studies have shown that\nsuch data aids multimodal in-context learning and maintains the capabilities of\nlarge language models during multimodal fine-tuning. However, the limited scale\nand diversity of current image-text interleaved data restrict the development\nof multimodal large language models. In this paper, we introduce OmniCorpus, a\n10 billion-scale image-text interleaved dataset. Using an efficient data\nengine, we filter and extract large-scale high-quality documents, which contain\n8.6 billion images and 1,696 billion text tokens. Compared to counterparts\n(e.g., MMC4, OBELICS), our dataset 1) has 15 times larger scales while\nmaintaining good data quality; 2) features more diverse sources, including both\nEnglish and non-English websites as well as video-centric websites; 3) is more\nflexible, easily degradable from an image-text interleaved format to pure text\ncorpus and image-text pairs. Through comprehensive analysis and experiments, we\nvalidate the quality, usability, and effectiveness of the proposed dataset. We\nhope this could provide a solid data foundation for future multimodal model\nresearch. Code and data are released at\nhttps://github.com/OpenGVLab/OmniCorpus.\n", "link": "http://arxiv.org/abs/2406.08418v1", "date": "2024-06-12", "relevancy": 2.1108, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5362}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5263}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5099}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniCorpus%3A%20An%20Unified%20Multimodal%20Corpus%20of%2010%20Billion-Level%20Images%0A%20%20Interleaved%20with%20Text&body=Title%3A%20OmniCorpus%3A%20An%20Unified%20Multimodal%20Corpus%20of%2010%20Billion-Level%20Images%0A%20%20Interleaved%20with%20Text%0AAuthor%3A%20Qingyun%20Li%20and%20Zhe%20Chen%20and%20Weiyun%20Wang%20and%20Wenhai%20Wang%20and%20Shenglong%20Ye%20and%20Zhenjiang%20Jin%20and%20Guanzhou%20Chen%20and%20Yinan%20He%20and%20Zhangwei%20Gao%20and%20Erfei%20Cui%20and%20Jiashuo%20Yu%20and%20Hao%20Tian%20and%20Jiasheng%20Zhou%20and%20Chao%20Xu%20and%20Bin%20Wang%20and%20Xingjian%20Wei%20and%20Wei%20Li%20and%20Wenjian%20Zhang%20and%20Bo%20Zhang%20and%20Pinlong%20Cai%20and%20Licheng%20Wen%20and%20Xiangchao%20Yan%20and%20Pei%20Chu%20and%20Yi%20Wang%20and%20Min%20Dou%20and%20Changyao%20Tian%20and%20Xizhou%20Zhu%20and%20Lewei%20Lu%20and%20Yushi%20Chen%20and%20Junjun%20He%20and%20Tong%20Lu%20and%20Yali%20Wang%20and%20Limin%20Wang%20and%20Dahua%20Lin%20and%20Yu%20Qiao%20and%20Botian%20Shi%20and%20Conghui%20He%20and%20Jifeng%20Dai%0AAbstract%3A%20%20%20Image-text%20interleaved%20data%2C%20consisting%20of%20multiple%20images%20and%20texts%20arranged%0Ain%20a%20natural%20document%20format%2C%20aligns%20with%20the%20presentation%20paradigm%20of%20internet%0Adata%20and%20closely%20resembles%20human%20reading%20habits.%20Recent%20studies%20have%20shown%20that%0Asuch%20data%20aids%20multimodal%20in-context%20learning%20and%20maintains%20the%20capabilities%20of%0Alarge%20language%20models%20during%20multimodal%20fine-tuning.%20However%2C%20the%20limited%20scale%0Aand%20diversity%20of%20current%20image-text%20interleaved%20data%20restrict%20the%20development%0Aof%20multimodal%20large%20language%20models.%20In%20this%20paper%2C%20we%20introduce%20OmniCorpus%2C%20a%0A10%20billion-scale%20image-text%20interleaved%20dataset.%20Using%20an%20efficient%20data%0Aengine%2C%20we%20filter%20and%20extract%20large-scale%20high-quality%20documents%2C%20which%20contain%0A8.6%20billion%20images%20and%201%2C696%20billion%20text%20tokens.%20Compared%20to%20counterparts%0A%28e.g.%2C%20MMC4%2C%20OBELICS%29%2C%20our%20dataset%201%29%20has%2015%20times%20larger%20scales%20while%0Amaintaining%20good%20data%20quality%3B%202%29%20features%20more%20diverse%20sources%2C%20including%20both%0AEnglish%20and%20non-English%20websites%20as%20well%20as%20video-centric%20websites%3B%203%29%20is%20more%0Aflexible%2C%20easily%20degradable%20from%20an%20image-text%20interleaved%20format%20to%20pure%20text%0Acorpus%20and%20image-text%20pairs.%20Through%20comprehensive%20analysis%20and%20experiments%2C%20we%0Avalidate%20the%20quality%2C%20usability%2C%20and%20effectiveness%20of%20the%20proposed%20dataset.%20We%0Ahope%20this%20could%20provide%20a%20solid%20data%20foundation%20for%20future%20multimodal%20model%0Aresearch.%20Code%20and%20data%20are%20released%20at%0Ahttps%3A//github.com/OpenGVLab/OmniCorpus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniCorpus%253A%2520An%2520Unified%2520Multimodal%2520Corpus%2520of%252010%2520Billion-Level%2520Images%250A%2520%2520Interleaved%2520with%2520Text%26entry.906535625%3DQingyun%2520Li%2520and%2520Zhe%2520Chen%2520and%2520Weiyun%2520Wang%2520and%2520Wenhai%2520Wang%2520and%2520Shenglong%2520Ye%2520and%2520Zhenjiang%2520Jin%2520and%2520Guanzhou%2520Chen%2520and%2520Yinan%2520He%2520and%2520Zhangwei%2520Gao%2520and%2520Erfei%2520Cui%2520and%2520Jiashuo%2520Yu%2520and%2520Hao%2520Tian%2520and%2520Jiasheng%2520Zhou%2520and%2520Chao%2520Xu%2520and%2520Bin%2520Wang%2520and%2520Xingjian%2520Wei%2520and%2520Wei%2520Li%2520and%2520Wenjian%2520Zhang%2520and%2520Bo%2520Zhang%2520and%2520Pinlong%2520Cai%2520and%2520Licheng%2520Wen%2520and%2520Xiangchao%2520Yan%2520and%2520Pei%2520Chu%2520and%2520Yi%2520Wang%2520and%2520Min%2520Dou%2520and%2520Changyao%2520Tian%2520and%2520Xizhou%2520Zhu%2520and%2520Lewei%2520Lu%2520and%2520Yushi%2520Chen%2520and%2520Junjun%2520He%2520and%2520Tong%2520Lu%2520and%2520Yali%2520Wang%2520and%2520Limin%2520Wang%2520and%2520Dahua%2520Lin%2520and%2520Yu%2520Qiao%2520and%2520Botian%2520Shi%2520and%2520Conghui%2520He%2520and%2520Jifeng%2520Dai%26entry.1292438233%3D%2520%2520Image-text%2520interleaved%2520data%252C%2520consisting%2520of%2520multiple%2520images%2520and%2520texts%2520arranged%250Ain%2520a%2520natural%2520document%2520format%252C%2520aligns%2520with%2520the%2520presentation%2520paradigm%2520of%2520internet%250Adata%2520and%2520closely%2520resembles%2520human%2520reading%2520habits.%2520Recent%2520studies%2520have%2520shown%2520that%250Asuch%2520data%2520aids%2520multimodal%2520in-context%2520learning%2520and%2520maintains%2520the%2520capabilities%2520of%250Alarge%2520language%2520models%2520during%2520multimodal%2520fine-tuning.%2520However%252C%2520the%2520limited%2520scale%250Aand%2520diversity%2520of%2520current%2520image-text%2520interleaved%2520data%2520restrict%2520the%2520development%250Aof%2520multimodal%2520large%2520language%2520models.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520OmniCorpus%252C%2520a%250A10%2520billion-scale%2520image-text%2520interleaved%2520dataset.%2520Using%2520an%2520efficient%2520data%250Aengine%252C%2520we%2520filter%2520and%2520extract%2520large-scale%2520high-quality%2520documents%252C%2520which%2520contain%250A8.6%2520billion%2520images%2520and%25201%252C696%2520billion%2520text%2520tokens.%2520Compared%2520to%2520counterparts%250A%2528e.g.%252C%2520MMC4%252C%2520OBELICS%2529%252C%2520our%2520dataset%25201%2529%2520has%252015%2520times%2520larger%2520scales%2520while%250Amaintaining%2520good%2520data%2520quality%253B%25202%2529%2520features%2520more%2520diverse%2520sources%252C%2520including%2520both%250AEnglish%2520and%2520non-English%2520websites%2520as%2520well%2520as%2520video-centric%2520websites%253B%25203%2529%2520is%2520more%250Aflexible%252C%2520easily%2520degradable%2520from%2520an%2520image-text%2520interleaved%2520format%2520to%2520pure%2520text%250Acorpus%2520and%2520image-text%2520pairs.%2520Through%2520comprehensive%2520analysis%2520and%2520experiments%252C%2520we%250Avalidate%2520the%2520quality%252C%2520usability%252C%2520and%2520effectiveness%2520of%2520the%2520proposed%2520dataset.%2520We%250Ahope%2520this%2520could%2520provide%2520a%2520solid%2520data%2520foundation%2520for%2520future%2520multimodal%2520model%250Aresearch.%2520Code%2520and%2520data%2520are%2520released%2520at%250Ahttps%253A//github.com/OpenGVLab/OmniCorpus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniCorpus%3A%20An%20Unified%20Multimodal%20Corpus%20of%2010%20Billion-Level%20Images%0A%20%20Interleaved%20with%20Text&entry.906535625=Qingyun%20Li%20and%20Zhe%20Chen%20and%20Weiyun%20Wang%20and%20Wenhai%20Wang%20and%20Shenglong%20Ye%20and%20Zhenjiang%20Jin%20and%20Guanzhou%20Chen%20and%20Yinan%20He%20and%20Zhangwei%20Gao%20and%20Erfei%20Cui%20and%20Jiashuo%20Yu%20and%20Hao%20Tian%20and%20Jiasheng%20Zhou%20and%20Chao%20Xu%20and%20Bin%20Wang%20and%20Xingjian%20Wei%20and%20Wei%20Li%20and%20Wenjian%20Zhang%20and%20Bo%20Zhang%20and%20Pinlong%20Cai%20and%20Licheng%20Wen%20and%20Xiangchao%20Yan%20and%20Pei%20Chu%20and%20Yi%20Wang%20and%20Min%20Dou%20and%20Changyao%20Tian%20and%20Xizhou%20Zhu%20and%20Lewei%20Lu%20and%20Yushi%20Chen%20and%20Junjun%20He%20and%20Tong%20Lu%20and%20Yali%20Wang%20and%20Limin%20Wang%20and%20Dahua%20Lin%20and%20Yu%20Qiao%20and%20Botian%20Shi%20and%20Conghui%20He%20and%20Jifeng%20Dai&entry.1292438233=%20%20Image-text%20interleaved%20data%2C%20consisting%20of%20multiple%20images%20and%20texts%20arranged%0Ain%20a%20natural%20document%20format%2C%20aligns%20with%20the%20presentation%20paradigm%20of%20internet%0Adata%20and%20closely%20resembles%20human%20reading%20habits.%20Recent%20studies%20have%20shown%20that%0Asuch%20data%20aids%20multimodal%20in-context%20learning%20and%20maintains%20the%20capabilities%20of%0Alarge%20language%20models%20during%20multimodal%20fine-tuning.%20However%2C%20the%20limited%20scale%0Aand%20diversity%20of%20current%20image-text%20interleaved%20data%20restrict%20the%20development%0Aof%20multimodal%20large%20language%20models.%20In%20this%20paper%2C%20we%20introduce%20OmniCorpus%2C%20a%0A10%20billion-scale%20image-text%20interleaved%20dataset.%20Using%20an%20efficient%20data%0Aengine%2C%20we%20filter%20and%20extract%20large-scale%20high-quality%20documents%2C%20which%20contain%0A8.6%20billion%20images%20and%201%2C696%20billion%20text%20tokens.%20Compared%20to%20counterparts%0A%28e.g.%2C%20MMC4%2C%20OBELICS%29%2C%20our%20dataset%201%29%20has%2015%20times%20larger%20scales%20while%0Amaintaining%20good%20data%20quality%3B%202%29%20features%20more%20diverse%20sources%2C%20including%20both%0AEnglish%20and%20non-English%20websites%20as%20well%20as%20video-centric%20websites%3B%203%29%20is%20more%0Aflexible%2C%20easily%20degradable%20from%20an%20image-text%20interleaved%20format%20to%20pure%20text%0Acorpus%20and%20image-text%20pairs.%20Through%20comprehensive%20analysis%20and%20experiments%2C%20we%0Avalidate%20the%20quality%2C%20usability%2C%20and%20effectiveness%20of%20the%20proposed%20dataset.%20We%0Ahope%20this%20could%20provide%20a%20solid%20data%20foundation%20for%20future%20multimodal%20model%0Aresearch.%20Code%20and%20data%20are%20released%20at%0Ahttps%3A//github.com/OpenGVLab/OmniCorpus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08418v1&entry.124074799=Read"},
{"title": "Explore-Go: Leveraging Exploration for Generalisation in Deep\n  Reinforcement Learning", "author": "Max Weltevrede and Felix Kaubek and Matthijs T. J. Spaan and Wendelin B\u00f6hmer", "abstract": "  One of the remaining challenges in reinforcement learning is to develop\nagents that can generalise to novel scenarios they might encounter once\ndeployed. This challenge is often framed in a multi-task setting where agents\ntrain on a fixed set of tasks and have to generalise to new tasks. Recent work\nhas shown that in this setting increased exploration during training can be\nleveraged to increase the generalisation performance of the agent. This makes\nsense when the states encountered during testing can actually be explored\nduring training. In this paper, we provide intuition why exploration can also\nbenefit generalisation to states that cannot be explicitly encountered during\ntraining. Additionally, we propose a novel method Explore-Go that exploits this\nintuition by increasing the number of states on which the agent trains.\nExplore-Go effectively increases the starting state distribution of the agent\nand as a result can be used in conjunction with most existing on-policy or\noff-policy reinforcement learning algorithms. We show empirically that our\nmethod can increase generalisation performance in an illustrative environment\nand on the Procgen benchmark.\n", "link": "http://arxiv.org/abs/2406.08069v1", "date": "2024-06-12", "relevancy": 2.0997, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5623}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5529}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explore-Go%3A%20Leveraging%20Exploration%20for%20Generalisation%20in%20Deep%0A%20%20Reinforcement%20Learning&body=Title%3A%20Explore-Go%3A%20Leveraging%20Exploration%20for%20Generalisation%20in%20Deep%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Max%20Weltevrede%20and%20Felix%20Kaubek%20and%20Matthijs%20T.%20J.%20Spaan%20and%20Wendelin%20B%C3%B6hmer%0AAbstract%3A%20%20%20One%20of%20the%20remaining%20challenges%20in%20reinforcement%20learning%20is%20to%20develop%0Aagents%20that%20can%20generalise%20to%20novel%20scenarios%20they%20might%20encounter%20once%0Adeployed.%20This%20challenge%20is%20often%20framed%20in%20a%20multi-task%20setting%20where%20agents%0Atrain%20on%20a%20fixed%20set%20of%20tasks%20and%20have%20to%20generalise%20to%20new%20tasks.%20Recent%20work%0Ahas%20shown%20that%20in%20this%20setting%20increased%20exploration%20during%20training%20can%20be%0Aleveraged%20to%20increase%20the%20generalisation%20performance%20of%20the%20agent.%20This%20makes%0Asense%20when%20the%20states%20encountered%20during%20testing%20can%20actually%20be%20explored%0Aduring%20training.%20In%20this%20paper%2C%20we%20provide%20intuition%20why%20exploration%20can%20also%0Abenefit%20generalisation%20to%20states%20that%20cannot%20be%20explicitly%20encountered%20during%0Atraining.%20Additionally%2C%20we%20propose%20a%20novel%20method%20Explore-Go%20that%20exploits%20this%0Aintuition%20by%20increasing%20the%20number%20of%20states%20on%20which%20the%20agent%20trains.%0AExplore-Go%20effectively%20increases%20the%20starting%20state%20distribution%20of%20the%20agent%0Aand%20as%20a%20result%20can%20be%20used%20in%20conjunction%20with%20most%20existing%20on-policy%20or%0Aoff-policy%20reinforcement%20learning%20algorithms.%20We%20show%20empirically%20that%20our%0Amethod%20can%20increase%20generalisation%20performance%20in%20an%20illustrative%20environment%0Aand%20on%20the%20Procgen%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08069v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplore-Go%253A%2520Leveraging%2520Exploration%2520for%2520Generalisation%2520in%2520Deep%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DMax%2520Weltevrede%2520and%2520Felix%2520Kaubek%2520and%2520Matthijs%2520T.%2520J.%2520Spaan%2520and%2520Wendelin%2520B%25C3%25B6hmer%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520remaining%2520challenges%2520in%2520reinforcement%2520learning%2520is%2520to%2520develop%250Aagents%2520that%2520can%2520generalise%2520to%2520novel%2520scenarios%2520they%2520might%2520encounter%2520once%250Adeployed.%2520This%2520challenge%2520is%2520often%2520framed%2520in%2520a%2520multi-task%2520setting%2520where%2520agents%250Atrain%2520on%2520a%2520fixed%2520set%2520of%2520tasks%2520and%2520have%2520to%2520generalise%2520to%2520new%2520tasks.%2520Recent%2520work%250Ahas%2520shown%2520that%2520in%2520this%2520setting%2520increased%2520exploration%2520during%2520training%2520can%2520be%250Aleveraged%2520to%2520increase%2520the%2520generalisation%2520performance%2520of%2520the%2520agent.%2520This%2520makes%250Asense%2520when%2520the%2520states%2520encountered%2520during%2520testing%2520can%2520actually%2520be%2520explored%250Aduring%2520training.%2520In%2520this%2520paper%252C%2520we%2520provide%2520intuition%2520why%2520exploration%2520can%2520also%250Abenefit%2520generalisation%2520to%2520states%2520that%2520cannot%2520be%2520explicitly%2520encountered%2520during%250Atraining.%2520Additionally%252C%2520we%2520propose%2520a%2520novel%2520method%2520Explore-Go%2520that%2520exploits%2520this%250Aintuition%2520by%2520increasing%2520the%2520number%2520of%2520states%2520on%2520which%2520the%2520agent%2520trains.%250AExplore-Go%2520effectively%2520increases%2520the%2520starting%2520state%2520distribution%2520of%2520the%2520agent%250Aand%2520as%2520a%2520result%2520can%2520be%2520used%2520in%2520conjunction%2520with%2520most%2520existing%2520on-policy%2520or%250Aoff-policy%2520reinforcement%2520learning%2520algorithms.%2520We%2520show%2520empirically%2520that%2520our%250Amethod%2520can%2520increase%2520generalisation%2520performance%2520in%2520an%2520illustrative%2520environment%250Aand%2520on%2520the%2520Procgen%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08069v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explore-Go%3A%20Leveraging%20Exploration%20for%20Generalisation%20in%20Deep%0A%20%20Reinforcement%20Learning&entry.906535625=Max%20Weltevrede%20and%20Felix%20Kaubek%20and%20Matthijs%20T.%20J.%20Spaan%20and%20Wendelin%20B%C3%B6hmer&entry.1292438233=%20%20One%20of%20the%20remaining%20challenges%20in%20reinforcement%20learning%20is%20to%20develop%0Aagents%20that%20can%20generalise%20to%20novel%20scenarios%20they%20might%20encounter%20once%0Adeployed.%20This%20challenge%20is%20often%20framed%20in%20a%20multi-task%20setting%20where%20agents%0Atrain%20on%20a%20fixed%20set%20of%20tasks%20and%20have%20to%20generalise%20to%20new%20tasks.%20Recent%20work%0Ahas%20shown%20that%20in%20this%20setting%20increased%20exploration%20during%20training%20can%20be%0Aleveraged%20to%20increase%20the%20generalisation%20performance%20of%20the%20agent.%20This%20makes%0Asense%20when%20the%20states%20encountered%20during%20testing%20can%20actually%20be%20explored%0Aduring%20training.%20In%20this%20paper%2C%20we%20provide%20intuition%20why%20exploration%20can%20also%0Abenefit%20generalisation%20to%20states%20that%20cannot%20be%20explicitly%20encountered%20during%0Atraining.%20Additionally%2C%20we%20propose%20a%20novel%20method%20Explore-Go%20that%20exploits%20this%0Aintuition%20by%20increasing%20the%20number%20of%20states%20on%20which%20the%20agent%20trains.%0AExplore-Go%20effectively%20increases%20the%20starting%20state%20distribution%20of%20the%20agent%0Aand%20as%20a%20result%20can%20be%20used%20in%20conjunction%20with%20most%20existing%20on-policy%20or%0Aoff-policy%20reinforcement%20learning%20algorithms.%20We%20show%20empirically%20that%20our%0Amethod%20can%20increase%20generalisation%20performance%20in%20an%20illustrative%20environment%0Aand%20on%20the%20Procgen%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08069v1&entry.124074799=Read"},
{"title": "AdaNCA: Neural Cellular Automata As Adaptors For More Robust Vision\n  Transformer", "author": "Yitao Xu and Tong Zhang and Sabine S\u00fcsstrunk", "abstract": "  Vision Transformers (ViTs) have demonstrated remarkable performance in image\nclassification tasks, particularly when equipped with local information via\nregion attention or convolutions. While such architectures improve the feature\naggregation from different granularities, they often fail to contribute to the\nrobustness of the networks. Neural Cellular Automata (NCA) enables the modeling\nof global cell representations through local interactions, with its training\nstrategies and architecture design conferring strong generalization ability and\nrobustness against noisy inputs. In this paper, we propose Adaptor Neural\nCellular Automata (AdaNCA) for Vision Transformer that uses NCA as plug-in-play\nadaptors between ViT layers, enhancing ViT's performance and robustness against\nadversarial samples as well as out-of-distribution inputs. To overcome the\nlarge computational overhead of standard NCAs, we propose Dynamic Interaction\nfor more efficient interaction learning. Furthermore, we develop an algorithm\nfor identifying the most effective insertion points for AdaNCA based on our\nanalysis of AdaNCA placement and robustness improvement. With less than a 3%\nincrease in parameters, AdaNCA contributes to more than 10% absolute\nimprovement in accuracy under adversarial attacks on the ImageNet1K benchmark.\nMoreover, we demonstrate with extensive evaluations across 8 robustness\nbenchmarks and 4 ViT architectures that AdaNCA, as a plug-in-play module,\nconsistently improves the robustness of ViTs.\n", "link": "http://arxiv.org/abs/2406.08298v1", "date": "2024-06-12", "relevancy": 2.0877, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5359}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5124}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaNCA%3A%20Neural%20Cellular%20Automata%20As%20Adaptors%20For%20More%20Robust%20Vision%0A%20%20Transformer&body=Title%3A%20AdaNCA%3A%20Neural%20Cellular%20Automata%20As%20Adaptors%20For%20More%20Robust%20Vision%0A%20%20Transformer%0AAuthor%3A%20Yitao%20Xu%20and%20Tong%20Zhang%20and%20Sabine%20S%C3%BCsstrunk%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20have%20demonstrated%20remarkable%20performance%20in%20image%0Aclassification%20tasks%2C%20particularly%20when%20equipped%20with%20local%20information%20via%0Aregion%20attention%20or%20convolutions.%20While%20such%20architectures%20improve%20the%20feature%0Aaggregation%20from%20different%20granularities%2C%20they%20often%20fail%20to%20contribute%20to%20the%0Arobustness%20of%20the%20networks.%20Neural%20Cellular%20Automata%20%28NCA%29%20enables%20the%20modeling%0Aof%20global%20cell%20representations%20through%20local%20interactions%2C%20with%20its%20training%0Astrategies%20and%20architecture%20design%20conferring%20strong%20generalization%20ability%20and%0Arobustness%20against%20noisy%20inputs.%20In%20this%20paper%2C%20we%20propose%20Adaptor%20Neural%0ACellular%20Automata%20%28AdaNCA%29%20for%20Vision%20Transformer%20that%20uses%20NCA%20as%20plug-in-play%0Aadaptors%20between%20ViT%20layers%2C%20enhancing%20ViT%27s%20performance%20and%20robustness%20against%0Aadversarial%20samples%20as%20well%20as%20out-of-distribution%20inputs.%20To%20overcome%20the%0Alarge%20computational%20overhead%20of%20standard%20NCAs%2C%20we%20propose%20Dynamic%20Interaction%0Afor%20more%20efficient%20interaction%20learning.%20Furthermore%2C%20we%20develop%20an%20algorithm%0Afor%20identifying%20the%20most%20effective%20insertion%20points%20for%20AdaNCA%20based%20on%20our%0Aanalysis%20of%20AdaNCA%20placement%20and%20robustness%20improvement.%20With%20less%20than%20a%203%25%0Aincrease%20in%20parameters%2C%20AdaNCA%20contributes%20to%20more%20than%2010%25%20absolute%0Aimprovement%20in%20accuracy%20under%20adversarial%20attacks%20on%20the%20ImageNet1K%20benchmark.%0AMoreover%2C%20we%20demonstrate%20with%20extensive%20evaluations%20across%208%20robustness%0Abenchmarks%20and%204%20ViT%20architectures%20that%20AdaNCA%2C%20as%20a%20plug-in-play%20module%2C%0Aconsistently%20improves%20the%20robustness%20of%20ViTs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08298v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaNCA%253A%2520Neural%2520Cellular%2520Automata%2520As%2520Adaptors%2520For%2520More%2520Robust%2520Vision%250A%2520%2520Transformer%26entry.906535625%3DYitao%2520Xu%2520and%2520Tong%2520Zhang%2520and%2520Sabine%2520S%25C3%25BCsstrunk%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520demonstrated%2520remarkable%2520performance%2520in%2520image%250Aclassification%2520tasks%252C%2520particularly%2520when%2520equipped%2520with%2520local%2520information%2520via%250Aregion%2520attention%2520or%2520convolutions.%2520While%2520such%2520architectures%2520improve%2520the%2520feature%250Aaggregation%2520from%2520different%2520granularities%252C%2520they%2520often%2520fail%2520to%2520contribute%2520to%2520the%250Arobustness%2520of%2520the%2520networks.%2520Neural%2520Cellular%2520Automata%2520%2528NCA%2529%2520enables%2520the%2520modeling%250Aof%2520global%2520cell%2520representations%2520through%2520local%2520interactions%252C%2520with%2520its%2520training%250Astrategies%2520and%2520architecture%2520design%2520conferring%2520strong%2520generalization%2520ability%2520and%250Arobustness%2520against%2520noisy%2520inputs.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Adaptor%2520Neural%250ACellular%2520Automata%2520%2528AdaNCA%2529%2520for%2520Vision%2520Transformer%2520that%2520uses%2520NCA%2520as%2520plug-in-play%250Aadaptors%2520between%2520ViT%2520layers%252C%2520enhancing%2520ViT%2527s%2520performance%2520and%2520robustness%2520against%250Aadversarial%2520samples%2520as%2520well%2520as%2520out-of-distribution%2520inputs.%2520To%2520overcome%2520the%250Alarge%2520computational%2520overhead%2520of%2520standard%2520NCAs%252C%2520we%2520propose%2520Dynamic%2520Interaction%250Afor%2520more%2520efficient%2520interaction%2520learning.%2520Furthermore%252C%2520we%2520develop%2520an%2520algorithm%250Afor%2520identifying%2520the%2520most%2520effective%2520insertion%2520points%2520for%2520AdaNCA%2520based%2520on%2520our%250Aanalysis%2520of%2520AdaNCA%2520placement%2520and%2520robustness%2520improvement.%2520With%2520less%2520than%2520a%25203%2525%250Aincrease%2520in%2520parameters%252C%2520AdaNCA%2520contributes%2520to%2520more%2520than%252010%2525%2520absolute%250Aimprovement%2520in%2520accuracy%2520under%2520adversarial%2520attacks%2520on%2520the%2520ImageNet1K%2520benchmark.%250AMoreover%252C%2520we%2520demonstrate%2520with%2520extensive%2520evaluations%2520across%25208%2520robustness%250Abenchmarks%2520and%25204%2520ViT%2520architectures%2520that%2520AdaNCA%252C%2520as%2520a%2520plug-in-play%2520module%252C%250Aconsistently%2520improves%2520the%2520robustness%2520of%2520ViTs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08298v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaNCA%3A%20Neural%20Cellular%20Automata%20As%20Adaptors%20For%20More%20Robust%20Vision%0A%20%20Transformer&entry.906535625=Yitao%20Xu%20and%20Tong%20Zhang%20and%20Sabine%20S%C3%BCsstrunk&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20have%20demonstrated%20remarkable%20performance%20in%20image%0Aclassification%20tasks%2C%20particularly%20when%20equipped%20with%20local%20information%20via%0Aregion%20attention%20or%20convolutions.%20While%20such%20architectures%20improve%20the%20feature%0Aaggregation%20from%20different%20granularities%2C%20they%20often%20fail%20to%20contribute%20to%20the%0Arobustness%20of%20the%20networks.%20Neural%20Cellular%20Automata%20%28NCA%29%20enables%20the%20modeling%0Aof%20global%20cell%20representations%20through%20local%20interactions%2C%20with%20its%20training%0Astrategies%20and%20architecture%20design%20conferring%20strong%20generalization%20ability%20and%0Arobustness%20against%20noisy%20inputs.%20In%20this%20paper%2C%20we%20propose%20Adaptor%20Neural%0ACellular%20Automata%20%28AdaNCA%29%20for%20Vision%20Transformer%20that%20uses%20NCA%20as%20plug-in-play%0Aadaptors%20between%20ViT%20layers%2C%20enhancing%20ViT%27s%20performance%20and%20robustness%20against%0Aadversarial%20samples%20as%20well%20as%20out-of-distribution%20inputs.%20To%20overcome%20the%0Alarge%20computational%20overhead%20of%20standard%20NCAs%2C%20we%20propose%20Dynamic%20Interaction%0Afor%20more%20efficient%20interaction%20learning.%20Furthermore%2C%20we%20develop%20an%20algorithm%0Afor%20identifying%20the%20most%20effective%20insertion%20points%20for%20AdaNCA%20based%20on%20our%0Aanalysis%20of%20AdaNCA%20placement%20and%20robustness%20improvement.%20With%20less%20than%20a%203%25%0Aincrease%20in%20parameters%2C%20AdaNCA%20contributes%20to%20more%20than%2010%25%20absolute%0Aimprovement%20in%20accuracy%20under%20adversarial%20attacks%20on%20the%20ImageNet1K%20benchmark.%0AMoreover%2C%20we%20demonstrate%20with%20extensive%20evaluations%20across%208%20robustness%0Abenchmarks%20and%204%20ViT%20architectures%20that%20AdaNCA%2C%20as%20a%20plug-in-play%20module%2C%0Aconsistently%20improves%20the%20robustness%20of%20ViTs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08298v1&entry.124074799=Read"},
{"title": "Descriptive Image Quality Assessment in the Wild", "author": "Zhiyuan You and Jinjin Gu and Zheyuan Li and Xin Cai and Kaiwen Zhu and Chao Dong and Tianfan Xue", "abstract": "  With the rapid advancement of Vision Language Models (VLMs), VLM-based Image\nQuality Assessment (IQA) seeks to describe image quality linguistically to\nalign with human expression and capture the multifaceted nature of IQA tasks.\nHowever, current methods are still far from practical usage. First, prior works\nfocus narrowly on specific sub-tasks or settings, which do not align with\ndiverse real-world applications. Second, their performance is sub-optimal due\nto limitations in dataset coverage, scale, and quality. To overcome these\nchallenges, we introduce Depicted image Quality Assessment in the Wild\n(DepictQA-Wild). Our method includes a multi-functional IQA task paradigm that\nencompasses both assessment and comparison tasks, brief and detailed responses,\nfull-reference and non-reference scenarios. We introduce a\nground-truth-informed dataset construction approach to enhance data quality,\nand scale up the dataset to 495K under the brief-detail joint framework.\nConsequently, we construct a comprehensive, large-scale, and high-quality\ndataset, named DQ-495K. We also retain image resolution during training to\nbetter handle resolution-related quality issues, and estimate a confidence\nscore that is helpful to filter out low-quality responses. Experimental results\ndemonstrate that DepictQA-Wild significantly outperforms traditional\nscore-based methods, prior VLM-based IQA models, and proprietary GPT-4V in\ndistortion identification, instant rating, and reasoning tasks. Our advantages\nare further confirmed by real-world applications including assessing the\nweb-downloaded images and ranking model-processed images. Datasets and codes\nwill be released in https://depictqa.github.io/depictqa-wild/.\n", "link": "http://arxiv.org/abs/2405.18842v2", "date": "2024-06-12", "relevancy": 2.0869, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5291}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5261}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Descriptive%20Image%20Quality%20Assessment%20in%20the%20Wild&body=Title%3A%20Descriptive%20Image%20Quality%20Assessment%20in%20the%20Wild%0AAuthor%3A%20Zhiyuan%20You%20and%20Jinjin%20Gu%20and%20Zheyuan%20Li%20and%20Xin%20Cai%20and%20Kaiwen%20Zhu%20and%20Chao%20Dong%20and%20Tianfan%20Xue%0AAbstract%3A%20%20%20With%20the%20rapid%20advancement%20of%20Vision%20Language%20Models%20%28VLMs%29%2C%20VLM-based%20Image%0AQuality%20Assessment%20%28IQA%29%20seeks%20to%20describe%20image%20quality%20linguistically%20to%0Aalign%20with%20human%20expression%20and%20capture%20the%20multifaceted%20nature%20of%20IQA%20tasks.%0AHowever%2C%20current%20methods%20are%20still%20far%20from%20practical%20usage.%20First%2C%20prior%20works%0Afocus%20narrowly%20on%20specific%20sub-tasks%20or%20settings%2C%20which%20do%20not%20align%20with%0Adiverse%20real-world%20applications.%20Second%2C%20their%20performance%20is%20sub-optimal%20due%0Ato%20limitations%20in%20dataset%20coverage%2C%20scale%2C%20and%20quality.%20To%20overcome%20these%0Achallenges%2C%20we%20introduce%20Depicted%20image%20Quality%20Assessment%20in%20the%20Wild%0A%28DepictQA-Wild%29.%20Our%20method%20includes%20a%20multi-functional%20IQA%20task%20paradigm%20that%0Aencompasses%20both%20assessment%20and%20comparison%20tasks%2C%20brief%20and%20detailed%20responses%2C%0Afull-reference%20and%20non-reference%20scenarios.%20We%20introduce%20a%0Aground-truth-informed%20dataset%20construction%20approach%20to%20enhance%20data%20quality%2C%0Aand%20scale%20up%20the%20dataset%20to%20495K%20under%20the%20brief-detail%20joint%20framework.%0AConsequently%2C%20we%20construct%20a%20comprehensive%2C%20large-scale%2C%20and%20high-quality%0Adataset%2C%20named%20DQ-495K.%20We%20also%20retain%20image%20resolution%20during%20training%20to%0Abetter%20handle%20resolution-related%20quality%20issues%2C%20and%20estimate%20a%20confidence%0Ascore%20that%20is%20helpful%20to%20filter%20out%20low-quality%20responses.%20Experimental%20results%0Ademonstrate%20that%20DepictQA-Wild%20significantly%20outperforms%20traditional%0Ascore-based%20methods%2C%20prior%20VLM-based%20IQA%20models%2C%20and%20proprietary%20GPT-4V%20in%0Adistortion%20identification%2C%20instant%20rating%2C%20and%20reasoning%20tasks.%20Our%20advantages%0Aare%20further%20confirmed%20by%20real-world%20applications%20including%20assessing%20the%0Aweb-downloaded%20images%20and%20ranking%20model-processed%20images.%20Datasets%20and%20codes%0Awill%20be%20released%20in%20https%3A//depictqa.github.io/depictqa-wild/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18842v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDescriptive%2520Image%2520Quality%2520Assessment%2520in%2520the%2520Wild%26entry.906535625%3DZhiyuan%2520You%2520and%2520Jinjin%2520Gu%2520and%2520Zheyuan%2520Li%2520and%2520Xin%2520Cai%2520and%2520Kaiwen%2520Zhu%2520and%2520Chao%2520Dong%2520and%2520Tianfan%2520Xue%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancement%2520of%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%252C%2520VLM-based%2520Image%250AQuality%2520Assessment%2520%2528IQA%2529%2520seeks%2520to%2520describe%2520image%2520quality%2520linguistically%2520to%250Aalign%2520with%2520human%2520expression%2520and%2520capture%2520the%2520multifaceted%2520nature%2520of%2520IQA%2520tasks.%250AHowever%252C%2520current%2520methods%2520are%2520still%2520far%2520from%2520practical%2520usage.%2520First%252C%2520prior%2520works%250Afocus%2520narrowly%2520on%2520specific%2520sub-tasks%2520or%2520settings%252C%2520which%2520do%2520not%2520align%2520with%250Adiverse%2520real-world%2520applications.%2520Second%252C%2520their%2520performance%2520is%2520sub-optimal%2520due%250Ato%2520limitations%2520in%2520dataset%2520coverage%252C%2520scale%252C%2520and%2520quality.%2520To%2520overcome%2520these%250Achallenges%252C%2520we%2520introduce%2520Depicted%2520image%2520Quality%2520Assessment%2520in%2520the%2520Wild%250A%2528DepictQA-Wild%2529.%2520Our%2520method%2520includes%2520a%2520multi-functional%2520IQA%2520task%2520paradigm%2520that%250Aencompasses%2520both%2520assessment%2520and%2520comparison%2520tasks%252C%2520brief%2520and%2520detailed%2520responses%252C%250Afull-reference%2520and%2520non-reference%2520scenarios.%2520We%2520introduce%2520a%250Aground-truth-informed%2520dataset%2520construction%2520approach%2520to%2520enhance%2520data%2520quality%252C%250Aand%2520scale%2520up%2520the%2520dataset%2520to%2520495K%2520under%2520the%2520brief-detail%2520joint%2520framework.%250AConsequently%252C%2520we%2520construct%2520a%2520comprehensive%252C%2520large-scale%252C%2520and%2520high-quality%250Adataset%252C%2520named%2520DQ-495K.%2520We%2520also%2520retain%2520image%2520resolution%2520during%2520training%2520to%250Abetter%2520handle%2520resolution-related%2520quality%2520issues%252C%2520and%2520estimate%2520a%2520confidence%250Ascore%2520that%2520is%2520helpful%2520to%2520filter%2520out%2520low-quality%2520responses.%2520Experimental%2520results%250Ademonstrate%2520that%2520DepictQA-Wild%2520significantly%2520outperforms%2520traditional%250Ascore-based%2520methods%252C%2520prior%2520VLM-based%2520IQA%2520models%252C%2520and%2520proprietary%2520GPT-4V%2520in%250Adistortion%2520identification%252C%2520instant%2520rating%252C%2520and%2520reasoning%2520tasks.%2520Our%2520advantages%250Aare%2520further%2520confirmed%2520by%2520real-world%2520applications%2520including%2520assessing%2520the%250Aweb-downloaded%2520images%2520and%2520ranking%2520model-processed%2520images.%2520Datasets%2520and%2520codes%250Awill%2520be%2520released%2520in%2520https%253A//depictqa.github.io/depictqa-wild/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18842v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Descriptive%20Image%20Quality%20Assessment%20in%20the%20Wild&entry.906535625=Zhiyuan%20You%20and%20Jinjin%20Gu%20and%20Zheyuan%20Li%20and%20Xin%20Cai%20and%20Kaiwen%20Zhu%20and%20Chao%20Dong%20and%20Tianfan%20Xue&entry.1292438233=%20%20With%20the%20rapid%20advancement%20of%20Vision%20Language%20Models%20%28VLMs%29%2C%20VLM-based%20Image%0AQuality%20Assessment%20%28IQA%29%20seeks%20to%20describe%20image%20quality%20linguistically%20to%0Aalign%20with%20human%20expression%20and%20capture%20the%20multifaceted%20nature%20of%20IQA%20tasks.%0AHowever%2C%20current%20methods%20are%20still%20far%20from%20practical%20usage.%20First%2C%20prior%20works%0Afocus%20narrowly%20on%20specific%20sub-tasks%20or%20settings%2C%20which%20do%20not%20align%20with%0Adiverse%20real-world%20applications.%20Second%2C%20their%20performance%20is%20sub-optimal%20due%0Ato%20limitations%20in%20dataset%20coverage%2C%20scale%2C%20and%20quality.%20To%20overcome%20these%0Achallenges%2C%20we%20introduce%20Depicted%20image%20Quality%20Assessment%20in%20the%20Wild%0A%28DepictQA-Wild%29.%20Our%20method%20includes%20a%20multi-functional%20IQA%20task%20paradigm%20that%0Aencompasses%20both%20assessment%20and%20comparison%20tasks%2C%20brief%20and%20detailed%20responses%2C%0Afull-reference%20and%20non-reference%20scenarios.%20We%20introduce%20a%0Aground-truth-informed%20dataset%20construction%20approach%20to%20enhance%20data%20quality%2C%0Aand%20scale%20up%20the%20dataset%20to%20495K%20under%20the%20brief-detail%20joint%20framework.%0AConsequently%2C%20we%20construct%20a%20comprehensive%2C%20large-scale%2C%20and%20high-quality%0Adataset%2C%20named%20DQ-495K.%20We%20also%20retain%20image%20resolution%20during%20training%20to%0Abetter%20handle%20resolution-related%20quality%20issues%2C%20and%20estimate%20a%20confidence%0Ascore%20that%20is%20helpful%20to%20filter%20out%20low-quality%20responses.%20Experimental%20results%0Ademonstrate%20that%20DepictQA-Wild%20significantly%20outperforms%20traditional%0Ascore-based%20methods%2C%20prior%20VLM-based%20IQA%20models%2C%20and%20proprietary%20GPT-4V%20in%0Adistortion%20identification%2C%20instant%20rating%2C%20and%20reasoning%20tasks.%20Our%20advantages%0Aare%20further%20confirmed%20by%20real-world%20applications%20including%20assessing%20the%0Aweb-downloaded%20images%20and%20ranking%20model-processed%20images.%20Datasets%20and%20codes%0Awill%20be%20released%20in%20https%3A//depictqa.github.io/depictqa-wild/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18842v2&entry.124074799=Read"},
{"title": "GUI Odyssey: A Comprehensive Dataset for Cross-App GUI Navigation on\n  Mobile Devices", "author": "Quanfeng Lu and Wenqi Shao and Zitao Liu and Fanqing Meng and Boxuan Li and Botong Chen and Siyuan Huang and Kaipeng Zhang and Yu Qiao and Ping Luo", "abstract": "  Smartphone users often navigate across multiple applications (apps) to\ncomplete tasks such as sharing content between social media platforms.\nAutonomous Graphical User Interface (GUI) navigation agents can enhance user\nexperience in communication, entertainment, and productivity by streamlining\nworkflows and reducing manual intervention. However, prior GUI agents often\ntrained with datasets comprising simple tasks that can be completed within a\nsingle app, leading to poor performance in cross-app navigation. To address\nthis problem, we introduce GUI Odyssey, a comprehensive dataset for training\nand evaluating cross-app navigation agents. GUI Odyssey consists of 7,735\nepisodes from 6 mobile devices, spanning 6 types of cross-app tasks, 201 apps,\nand 1.4K app combos. Leveraging GUI Odyssey, we developed OdysseyAgent, a\nmultimodal cross-app navigation agent by fine-tuning the Qwen-VL model with a\nhistory resampling module. Extensive experiments demonstrate OdysseyAgent's\nsuperior accuracy compared to existing models. For instance, OdysseyAgent\nsurpasses fine-tuned Qwen-VL and zero-shot GPT-4V by 1.44\\% and 55.49\\%\nin-domain accuracy, and 2.29\\% and 48.14\\% out-of-domain accuracy on average.\nThe dataset and code will be released in\n\\url{https://github.com/OpenGVLab/GUI-Odyssey}.\n", "link": "http://arxiv.org/abs/2406.08451v1", "date": "2024-06-12", "relevancy": 2.085, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.533}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.522}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GUI%20Odyssey%3A%20A%20Comprehensive%20Dataset%20for%20Cross-App%20GUI%20Navigation%20on%0A%20%20Mobile%20Devices&body=Title%3A%20GUI%20Odyssey%3A%20A%20Comprehensive%20Dataset%20for%20Cross-App%20GUI%20Navigation%20on%0A%20%20Mobile%20Devices%0AAuthor%3A%20Quanfeng%20Lu%20and%20Wenqi%20Shao%20and%20Zitao%20Liu%20and%20Fanqing%20Meng%20and%20Boxuan%20Li%20and%20Botong%20Chen%20and%20Siyuan%20Huang%20and%20Kaipeng%20Zhang%20and%20Yu%20Qiao%20and%20Ping%20Luo%0AAbstract%3A%20%20%20Smartphone%20users%20often%20navigate%20across%20multiple%20applications%20%28apps%29%20to%0Acomplete%20tasks%20such%20as%20sharing%20content%20between%20social%20media%20platforms.%0AAutonomous%20Graphical%20User%20Interface%20%28GUI%29%20navigation%20agents%20can%20enhance%20user%0Aexperience%20in%20communication%2C%20entertainment%2C%20and%20productivity%20by%20streamlining%0Aworkflows%20and%20reducing%20manual%20intervention.%20However%2C%20prior%20GUI%20agents%20often%0Atrained%20with%20datasets%20comprising%20simple%20tasks%20that%20can%20be%20completed%20within%20a%0Asingle%20app%2C%20leading%20to%20poor%20performance%20in%20cross-app%20navigation.%20To%20address%0Athis%20problem%2C%20we%20introduce%20GUI%20Odyssey%2C%20a%20comprehensive%20dataset%20for%20training%0Aand%20evaluating%20cross-app%20navigation%20agents.%20GUI%20Odyssey%20consists%20of%207%2C735%0Aepisodes%20from%206%20mobile%20devices%2C%20spanning%206%20types%20of%20cross-app%20tasks%2C%20201%20apps%2C%0Aand%201.4K%20app%20combos.%20Leveraging%20GUI%20Odyssey%2C%20we%20developed%20OdysseyAgent%2C%20a%0Amultimodal%20cross-app%20navigation%20agent%20by%20fine-tuning%20the%20Qwen-VL%20model%20with%20a%0Ahistory%20resampling%20module.%20Extensive%20experiments%20demonstrate%20OdysseyAgent%27s%0Asuperior%20accuracy%20compared%20to%20existing%20models.%20For%20instance%2C%20OdysseyAgent%0Asurpasses%20fine-tuned%20Qwen-VL%20and%20zero-shot%20GPT-4V%20by%201.44%5C%25%20and%2055.49%5C%25%0Ain-domain%20accuracy%2C%20and%202.29%5C%25%20and%2048.14%5C%25%20out-of-domain%20accuracy%20on%20average.%0AThe%20dataset%20and%20code%20will%20be%20released%20in%0A%5Curl%7Bhttps%3A//github.com/OpenGVLab/GUI-Odyssey%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGUI%2520Odyssey%253A%2520A%2520Comprehensive%2520Dataset%2520for%2520Cross-App%2520GUI%2520Navigation%2520on%250A%2520%2520Mobile%2520Devices%26entry.906535625%3DQuanfeng%2520Lu%2520and%2520Wenqi%2520Shao%2520and%2520Zitao%2520Liu%2520and%2520Fanqing%2520Meng%2520and%2520Boxuan%2520Li%2520and%2520Botong%2520Chen%2520and%2520Siyuan%2520Huang%2520and%2520Kaipeng%2520Zhang%2520and%2520Yu%2520Qiao%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520Smartphone%2520users%2520often%2520navigate%2520across%2520multiple%2520applications%2520%2528apps%2529%2520to%250Acomplete%2520tasks%2520such%2520as%2520sharing%2520content%2520between%2520social%2520media%2520platforms.%250AAutonomous%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%2520navigation%2520agents%2520can%2520enhance%2520user%250Aexperience%2520in%2520communication%252C%2520entertainment%252C%2520and%2520productivity%2520by%2520streamlining%250Aworkflows%2520and%2520reducing%2520manual%2520intervention.%2520However%252C%2520prior%2520GUI%2520agents%2520often%250Atrained%2520with%2520datasets%2520comprising%2520simple%2520tasks%2520that%2520can%2520be%2520completed%2520within%2520a%250Asingle%2520app%252C%2520leading%2520to%2520poor%2520performance%2520in%2520cross-app%2520navigation.%2520To%2520address%250Athis%2520problem%252C%2520we%2520introduce%2520GUI%2520Odyssey%252C%2520a%2520comprehensive%2520dataset%2520for%2520training%250Aand%2520evaluating%2520cross-app%2520navigation%2520agents.%2520GUI%2520Odyssey%2520consists%2520of%25207%252C735%250Aepisodes%2520from%25206%2520mobile%2520devices%252C%2520spanning%25206%2520types%2520of%2520cross-app%2520tasks%252C%2520201%2520apps%252C%250Aand%25201.4K%2520app%2520combos.%2520Leveraging%2520GUI%2520Odyssey%252C%2520we%2520developed%2520OdysseyAgent%252C%2520a%250Amultimodal%2520cross-app%2520navigation%2520agent%2520by%2520fine-tuning%2520the%2520Qwen-VL%2520model%2520with%2520a%250Ahistory%2520resampling%2520module.%2520Extensive%2520experiments%2520demonstrate%2520OdysseyAgent%2527s%250Asuperior%2520accuracy%2520compared%2520to%2520existing%2520models.%2520For%2520instance%252C%2520OdysseyAgent%250Asurpasses%2520fine-tuned%2520Qwen-VL%2520and%2520zero-shot%2520GPT-4V%2520by%25201.44%255C%2525%2520and%252055.49%255C%2525%250Ain-domain%2520accuracy%252C%2520and%25202.29%255C%2525%2520and%252048.14%255C%2525%2520out-of-domain%2520accuracy%2520on%2520average.%250AThe%2520dataset%2520and%2520code%2520will%2520be%2520released%2520in%250A%255Curl%257Bhttps%253A//github.com/OpenGVLab/GUI-Odyssey%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GUI%20Odyssey%3A%20A%20Comprehensive%20Dataset%20for%20Cross-App%20GUI%20Navigation%20on%0A%20%20Mobile%20Devices&entry.906535625=Quanfeng%20Lu%20and%20Wenqi%20Shao%20and%20Zitao%20Liu%20and%20Fanqing%20Meng%20and%20Boxuan%20Li%20and%20Botong%20Chen%20and%20Siyuan%20Huang%20and%20Kaipeng%20Zhang%20and%20Yu%20Qiao%20and%20Ping%20Luo&entry.1292438233=%20%20Smartphone%20users%20often%20navigate%20across%20multiple%20applications%20%28apps%29%20to%0Acomplete%20tasks%20such%20as%20sharing%20content%20between%20social%20media%20platforms.%0AAutonomous%20Graphical%20User%20Interface%20%28GUI%29%20navigation%20agents%20can%20enhance%20user%0Aexperience%20in%20communication%2C%20entertainment%2C%20and%20productivity%20by%20streamlining%0Aworkflows%20and%20reducing%20manual%20intervention.%20However%2C%20prior%20GUI%20agents%20often%0Atrained%20with%20datasets%20comprising%20simple%20tasks%20that%20can%20be%20completed%20within%20a%0Asingle%20app%2C%20leading%20to%20poor%20performance%20in%20cross-app%20navigation.%20To%20address%0Athis%20problem%2C%20we%20introduce%20GUI%20Odyssey%2C%20a%20comprehensive%20dataset%20for%20training%0Aand%20evaluating%20cross-app%20navigation%20agents.%20GUI%20Odyssey%20consists%20of%207%2C735%0Aepisodes%20from%206%20mobile%20devices%2C%20spanning%206%20types%20of%20cross-app%20tasks%2C%20201%20apps%2C%0Aand%201.4K%20app%20combos.%20Leveraging%20GUI%20Odyssey%2C%20we%20developed%20OdysseyAgent%2C%20a%0Amultimodal%20cross-app%20navigation%20agent%20by%20fine-tuning%20the%20Qwen-VL%20model%20with%20a%0Ahistory%20resampling%20module.%20Extensive%20experiments%20demonstrate%20OdysseyAgent%27s%0Asuperior%20accuracy%20compared%20to%20existing%20models.%20For%20instance%2C%20OdysseyAgent%0Asurpasses%20fine-tuned%20Qwen-VL%20and%20zero-shot%20GPT-4V%20by%201.44%5C%25%20and%2055.49%5C%25%0Ain-domain%20accuracy%2C%20and%202.29%5C%25%20and%2048.14%5C%25%20out-of-domain%20accuracy%20on%20average.%0AThe%20dataset%20and%20code%20will%20be%20released%20in%0A%5Curl%7Bhttps%3A//github.com/OpenGVLab/GUI-Odyssey%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08451v1&entry.124074799=Read"},
{"title": "APSeg: Auto-Prompt Network for Cross-Domain Few-Shot Semantic\n  Segmentatio", "author": "Weizhao He and Yang Zhang and Wei Zhuo and Linlin Shen and Jiaqi Yang and Songhe Deng and Liang Sun", "abstract": "  Few-shot semantic segmentation (FSS) endeavors to segment unseen classes with\nonly a few labeled samples. Current FSS methods are commonly built on the\nassumption that their training and application scenarios share similar domains,\nand their performances degrade significantly while applied to a distinct\ndomain. To this end, we propose to leverage the cutting-edge foundation model,\nthe Segment Anything Model (SAM), for generalization enhancement. The SAM\nhowever performs unsatisfactorily on domains that are distinct from its\ntraining data, which primarily comprise natural scene images, and it does not\nsupport automatic segmentation of specific semantics due to its interactive\nprompting mechanism. In our work, we introduce APSeg, a novel auto-prompt\nnetwork for cross-domain few-shot semantic segmentation (CD-FSS), which is\ndesigned to be auto-prompted for guiding cross-domain segmentation.\nSpecifically, we propose a Dual Prototype Anchor Transformation (DPAT) module\nthat fuses pseudo query prototypes extracted based on cycle-consistency with\nsupport prototypes, allowing features to be transformed into a more stable\ndomain-agnostic space. Additionally, a Meta Prompt Generator (MPG) module is\nintroduced to automatically generate prompt embeddings, eliminating the need\nfor manual visual prompts. We build an efficient model which can be applied\ndirectly to target domains without fine-tuning. Extensive experiments on four\ncross-domain datasets show that our model outperforms the state-of-the-art\nCD-FSS method by 5.24% and 3.10% in average accuracy on 1-shot and 5-shot\nsettings, respectively.\n", "link": "http://arxiv.org/abs/2406.08372v1", "date": "2024-06-12", "relevancy": 2.0842, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.542}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5123}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20APSeg%3A%20Auto-Prompt%20Network%20for%20Cross-Domain%20Few-Shot%20Semantic%0A%20%20Segmentatio&body=Title%3A%20APSeg%3A%20Auto-Prompt%20Network%20for%20Cross-Domain%20Few-Shot%20Semantic%0A%20%20Segmentatio%0AAuthor%3A%20Weizhao%20He%20and%20Yang%20Zhang%20and%20Wei%20Zhuo%20and%20Linlin%20Shen%20and%20Jiaqi%20Yang%20and%20Songhe%20Deng%20and%20Liang%20Sun%0AAbstract%3A%20%20%20Few-shot%20semantic%20segmentation%20%28FSS%29%20endeavors%20to%20segment%20unseen%20classes%20with%0Aonly%20a%20few%20labeled%20samples.%20Current%20FSS%20methods%20are%20commonly%20built%20on%20the%0Aassumption%20that%20their%20training%20and%20application%20scenarios%20share%20similar%20domains%2C%0Aand%20their%20performances%20degrade%20significantly%20while%20applied%20to%20a%20distinct%0Adomain.%20To%20this%20end%2C%20we%20propose%20to%20leverage%20the%20cutting-edge%20foundation%20model%2C%0Athe%20Segment%20Anything%20Model%20%28SAM%29%2C%20for%20generalization%20enhancement.%20The%20SAM%0Ahowever%20performs%20unsatisfactorily%20on%20domains%20that%20are%20distinct%20from%20its%0Atraining%20data%2C%20which%20primarily%20comprise%20natural%20scene%20images%2C%20and%20it%20does%20not%0Asupport%20automatic%20segmentation%20of%20specific%20semantics%20due%20to%20its%20interactive%0Aprompting%20mechanism.%20In%20our%20work%2C%20we%20introduce%20APSeg%2C%20a%20novel%20auto-prompt%0Anetwork%20for%20cross-domain%20few-shot%20semantic%20segmentation%20%28CD-FSS%29%2C%20which%20is%0Adesigned%20to%20be%20auto-prompted%20for%20guiding%20cross-domain%20segmentation.%0ASpecifically%2C%20we%20propose%20a%20Dual%20Prototype%20Anchor%20Transformation%20%28DPAT%29%20module%0Athat%20fuses%20pseudo%20query%20prototypes%20extracted%20based%20on%20cycle-consistency%20with%0Asupport%20prototypes%2C%20allowing%20features%20to%20be%20transformed%20into%20a%20more%20stable%0Adomain-agnostic%20space.%20Additionally%2C%20a%20Meta%20Prompt%20Generator%20%28MPG%29%20module%20is%0Aintroduced%20to%20automatically%20generate%20prompt%20embeddings%2C%20eliminating%20the%20need%0Afor%20manual%20visual%20prompts.%20We%20build%20an%20efficient%20model%20which%20can%20be%20applied%0Adirectly%20to%20target%20domains%20without%20fine-tuning.%20Extensive%20experiments%20on%20four%0Across-domain%20datasets%20show%20that%20our%20model%20outperforms%20the%20state-of-the-art%0ACD-FSS%20method%20by%205.24%25%20and%203.10%25%20in%20average%20accuracy%20on%201-shot%20and%205-shot%0Asettings%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAPSeg%253A%2520Auto-Prompt%2520Network%2520for%2520Cross-Domain%2520Few-Shot%2520Semantic%250A%2520%2520Segmentatio%26entry.906535625%3DWeizhao%2520He%2520and%2520Yang%2520Zhang%2520and%2520Wei%2520Zhuo%2520and%2520Linlin%2520Shen%2520and%2520Jiaqi%2520Yang%2520and%2520Songhe%2520Deng%2520and%2520Liang%2520Sun%26entry.1292438233%3D%2520%2520Few-shot%2520semantic%2520segmentation%2520%2528FSS%2529%2520endeavors%2520to%2520segment%2520unseen%2520classes%2520with%250Aonly%2520a%2520few%2520labeled%2520samples.%2520Current%2520FSS%2520methods%2520are%2520commonly%2520built%2520on%2520the%250Aassumption%2520that%2520their%2520training%2520and%2520application%2520scenarios%2520share%2520similar%2520domains%252C%250Aand%2520their%2520performances%2520degrade%2520significantly%2520while%2520applied%2520to%2520a%2520distinct%250Adomain.%2520To%2520this%2520end%252C%2520we%2520propose%2520to%2520leverage%2520the%2520cutting-edge%2520foundation%2520model%252C%250Athe%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520for%2520generalization%2520enhancement.%2520The%2520SAM%250Ahowever%2520performs%2520unsatisfactorily%2520on%2520domains%2520that%2520are%2520distinct%2520from%2520its%250Atraining%2520data%252C%2520which%2520primarily%2520comprise%2520natural%2520scene%2520images%252C%2520and%2520it%2520does%2520not%250Asupport%2520automatic%2520segmentation%2520of%2520specific%2520semantics%2520due%2520to%2520its%2520interactive%250Aprompting%2520mechanism.%2520In%2520our%2520work%252C%2520we%2520introduce%2520APSeg%252C%2520a%2520novel%2520auto-prompt%250Anetwork%2520for%2520cross-domain%2520few-shot%2520semantic%2520segmentation%2520%2528CD-FSS%2529%252C%2520which%2520is%250Adesigned%2520to%2520be%2520auto-prompted%2520for%2520guiding%2520cross-domain%2520segmentation.%250ASpecifically%252C%2520we%2520propose%2520a%2520Dual%2520Prototype%2520Anchor%2520Transformation%2520%2528DPAT%2529%2520module%250Athat%2520fuses%2520pseudo%2520query%2520prototypes%2520extracted%2520based%2520on%2520cycle-consistency%2520with%250Asupport%2520prototypes%252C%2520allowing%2520features%2520to%2520be%2520transformed%2520into%2520a%2520more%2520stable%250Adomain-agnostic%2520space.%2520Additionally%252C%2520a%2520Meta%2520Prompt%2520Generator%2520%2528MPG%2529%2520module%2520is%250Aintroduced%2520to%2520automatically%2520generate%2520prompt%2520embeddings%252C%2520eliminating%2520the%2520need%250Afor%2520manual%2520visual%2520prompts.%2520We%2520build%2520an%2520efficient%2520model%2520which%2520can%2520be%2520applied%250Adirectly%2520to%2520target%2520domains%2520without%2520fine-tuning.%2520Extensive%2520experiments%2520on%2520four%250Across-domain%2520datasets%2520show%2520that%2520our%2520model%2520outperforms%2520the%2520state-of-the-art%250ACD-FSS%2520method%2520by%25205.24%2525%2520and%25203.10%2525%2520in%2520average%2520accuracy%2520on%25201-shot%2520and%25205-shot%250Asettings%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=APSeg%3A%20Auto-Prompt%20Network%20for%20Cross-Domain%20Few-Shot%20Semantic%0A%20%20Segmentatio&entry.906535625=Weizhao%20He%20and%20Yang%20Zhang%20and%20Wei%20Zhuo%20and%20Linlin%20Shen%20and%20Jiaqi%20Yang%20and%20Songhe%20Deng%20and%20Liang%20Sun&entry.1292438233=%20%20Few-shot%20semantic%20segmentation%20%28FSS%29%20endeavors%20to%20segment%20unseen%20classes%20with%0Aonly%20a%20few%20labeled%20samples.%20Current%20FSS%20methods%20are%20commonly%20built%20on%20the%0Aassumption%20that%20their%20training%20and%20application%20scenarios%20share%20similar%20domains%2C%0Aand%20their%20performances%20degrade%20significantly%20while%20applied%20to%20a%20distinct%0Adomain.%20To%20this%20end%2C%20we%20propose%20to%20leverage%20the%20cutting-edge%20foundation%20model%2C%0Athe%20Segment%20Anything%20Model%20%28SAM%29%2C%20for%20generalization%20enhancement.%20The%20SAM%0Ahowever%20performs%20unsatisfactorily%20on%20domains%20that%20are%20distinct%20from%20its%0Atraining%20data%2C%20which%20primarily%20comprise%20natural%20scene%20images%2C%20and%20it%20does%20not%0Asupport%20automatic%20segmentation%20of%20specific%20semantics%20due%20to%20its%20interactive%0Aprompting%20mechanism.%20In%20our%20work%2C%20we%20introduce%20APSeg%2C%20a%20novel%20auto-prompt%0Anetwork%20for%20cross-domain%20few-shot%20semantic%20segmentation%20%28CD-FSS%29%2C%20which%20is%0Adesigned%20to%20be%20auto-prompted%20for%20guiding%20cross-domain%20segmentation.%0ASpecifically%2C%20we%20propose%20a%20Dual%20Prototype%20Anchor%20Transformation%20%28DPAT%29%20module%0Athat%20fuses%20pseudo%20query%20prototypes%20extracted%20based%20on%20cycle-consistency%20with%0Asupport%20prototypes%2C%20allowing%20features%20to%20be%20transformed%20into%20a%20more%20stable%0Adomain-agnostic%20space.%20Additionally%2C%20a%20Meta%20Prompt%20Generator%20%28MPG%29%20module%20is%0Aintroduced%20to%20automatically%20generate%20prompt%20embeddings%2C%20eliminating%20the%20need%0Afor%20manual%20visual%20prompts.%20We%20build%20an%20efficient%20model%20which%20can%20be%20applied%0Adirectly%20to%20target%20domains%20without%20fine-tuning.%20Extensive%20experiments%20on%20four%0Across-domain%20datasets%20show%20that%20our%20model%20outperforms%20the%20state-of-the-art%0ACD-FSS%20method%20by%205.24%25%20and%203.10%25%20in%20average%20accuracy%20on%201-shot%20and%205-shot%0Asettings%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08372v1&entry.124074799=Read"},
{"title": "ProTrain: Efficient LLM Training via Memory-Aware Techniques", "author": "Hanmei Yang and Jin Zhou and Yao Fu and Xiaoqun Wang and Ramine Roane and Hui Guan and Tongping Liu", "abstract": "  It is extremely memory-hungry to train Large Language Models (LLM). To solve\nthis problem, existing work exploits the combination of CPU and GPU for the\ntraining process, such as ZeRO-Offload. Such a technique largely democratizes\nbillion-scale model training, making it possible to train with few consumer\ngraphics cards. However, based on our observation, existing frameworks often\nprovide coarse-grained memory management and require experienced experts in\nconfiguration tuning, leading to suboptimal hardware utilization and\nperformance. This paper proposes ProTrain, a novel training system that\nintelligently balances memory usage and performance by coordinating memory,\ncomputation, and IO. ProTrain achieves adaptive memory management through\nChunk-Based Model State Management and Block-Wise Activation Management, guided\nby a Memory-Aware Runtime Profiler without user intervention. ProTrain does not\nchange the training algorithm and thus does not compromise accuracy.\nExperiments show that ProTrain improves training throughput by 1.43$\\times$ to\n2.71$\\times$ compared to the SOTA training systems.\n", "link": "http://arxiv.org/abs/2406.08334v1", "date": "2024-06-12", "relevancy": 2.0807, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5206}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.52}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProTrain%3A%20Efficient%20LLM%20Training%20via%20Memory-Aware%20Techniques&body=Title%3A%20ProTrain%3A%20Efficient%20LLM%20Training%20via%20Memory-Aware%20Techniques%0AAuthor%3A%20Hanmei%20Yang%20and%20Jin%20Zhou%20and%20Yao%20Fu%20and%20Xiaoqun%20Wang%20and%20Ramine%20Roane%20and%20Hui%20Guan%20and%20Tongping%20Liu%0AAbstract%3A%20%20%20It%20is%20extremely%20memory-hungry%20to%20train%20Large%20Language%20Models%20%28LLM%29.%20To%20solve%0Athis%20problem%2C%20existing%20work%20exploits%20the%20combination%20of%20CPU%20and%20GPU%20for%20the%0Atraining%20process%2C%20such%20as%20ZeRO-Offload.%20Such%20a%20technique%20largely%20democratizes%0Abillion-scale%20model%20training%2C%20making%20it%20possible%20to%20train%20with%20few%20consumer%0Agraphics%20cards.%20However%2C%20based%20on%20our%20observation%2C%20existing%20frameworks%20often%0Aprovide%20coarse-grained%20memory%20management%20and%20require%20experienced%20experts%20in%0Aconfiguration%20tuning%2C%20leading%20to%20suboptimal%20hardware%20utilization%20and%0Aperformance.%20This%20paper%20proposes%20ProTrain%2C%20a%20novel%20training%20system%20that%0Aintelligently%20balances%20memory%20usage%20and%20performance%20by%20coordinating%20memory%2C%0Acomputation%2C%20and%20IO.%20ProTrain%20achieves%20adaptive%20memory%20management%20through%0AChunk-Based%20Model%20State%20Management%20and%20Block-Wise%20Activation%20Management%2C%20guided%0Aby%20a%20Memory-Aware%20Runtime%20Profiler%20without%20user%20intervention.%20ProTrain%20does%20not%0Achange%20the%20training%20algorithm%20and%20thus%20does%20not%20compromise%20accuracy.%0AExperiments%20show%20that%20ProTrain%20improves%20training%20throughput%20by%201.43%24%5Ctimes%24%20to%0A2.71%24%5Ctimes%24%20compared%20to%20the%20SOTA%20training%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProTrain%253A%2520Efficient%2520LLM%2520Training%2520via%2520Memory-Aware%2520Techniques%26entry.906535625%3DHanmei%2520Yang%2520and%2520Jin%2520Zhou%2520and%2520Yao%2520Fu%2520and%2520Xiaoqun%2520Wang%2520and%2520Ramine%2520Roane%2520and%2520Hui%2520Guan%2520and%2520Tongping%2520Liu%26entry.1292438233%3D%2520%2520It%2520is%2520extremely%2520memory-hungry%2520to%2520train%2520Large%2520Language%2520Models%2520%2528LLM%2529.%2520To%2520solve%250Athis%2520problem%252C%2520existing%2520work%2520exploits%2520the%2520combination%2520of%2520CPU%2520and%2520GPU%2520for%2520the%250Atraining%2520process%252C%2520such%2520as%2520ZeRO-Offload.%2520Such%2520a%2520technique%2520largely%2520democratizes%250Abillion-scale%2520model%2520training%252C%2520making%2520it%2520possible%2520to%2520train%2520with%2520few%2520consumer%250Agraphics%2520cards.%2520However%252C%2520based%2520on%2520our%2520observation%252C%2520existing%2520frameworks%2520often%250Aprovide%2520coarse-grained%2520memory%2520management%2520and%2520require%2520experienced%2520experts%2520in%250Aconfiguration%2520tuning%252C%2520leading%2520to%2520suboptimal%2520hardware%2520utilization%2520and%250Aperformance.%2520This%2520paper%2520proposes%2520ProTrain%252C%2520a%2520novel%2520training%2520system%2520that%250Aintelligently%2520balances%2520memory%2520usage%2520and%2520performance%2520by%2520coordinating%2520memory%252C%250Acomputation%252C%2520and%2520IO.%2520ProTrain%2520achieves%2520adaptive%2520memory%2520management%2520through%250AChunk-Based%2520Model%2520State%2520Management%2520and%2520Block-Wise%2520Activation%2520Management%252C%2520guided%250Aby%2520a%2520Memory-Aware%2520Runtime%2520Profiler%2520without%2520user%2520intervention.%2520ProTrain%2520does%2520not%250Achange%2520the%2520training%2520algorithm%2520and%2520thus%2520does%2520not%2520compromise%2520accuracy.%250AExperiments%2520show%2520that%2520ProTrain%2520improves%2520training%2520throughput%2520by%25201.43%2524%255Ctimes%2524%2520to%250A2.71%2524%255Ctimes%2524%2520compared%2520to%2520the%2520SOTA%2520training%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProTrain%3A%20Efficient%20LLM%20Training%20via%20Memory-Aware%20Techniques&entry.906535625=Hanmei%20Yang%20and%20Jin%20Zhou%20and%20Yao%20Fu%20and%20Xiaoqun%20Wang%20and%20Ramine%20Roane%20and%20Hui%20Guan%20and%20Tongping%20Liu&entry.1292438233=%20%20It%20is%20extremely%20memory-hungry%20to%20train%20Large%20Language%20Models%20%28LLM%29.%20To%20solve%0Athis%20problem%2C%20existing%20work%20exploits%20the%20combination%20of%20CPU%20and%20GPU%20for%20the%0Atraining%20process%2C%20such%20as%20ZeRO-Offload.%20Such%20a%20technique%20largely%20democratizes%0Abillion-scale%20model%20training%2C%20making%20it%20possible%20to%20train%20with%20few%20consumer%0Agraphics%20cards.%20However%2C%20based%20on%20our%20observation%2C%20existing%20frameworks%20often%0Aprovide%20coarse-grained%20memory%20management%20and%20require%20experienced%20experts%20in%0Aconfiguration%20tuning%2C%20leading%20to%20suboptimal%20hardware%20utilization%20and%0Aperformance.%20This%20paper%20proposes%20ProTrain%2C%20a%20novel%20training%20system%20that%0Aintelligently%20balances%20memory%20usage%20and%20performance%20by%20coordinating%20memory%2C%0Acomputation%2C%20and%20IO.%20ProTrain%20achieves%20adaptive%20memory%20management%20through%0AChunk-Based%20Model%20State%20Management%20and%20Block-Wise%20Activation%20Management%2C%20guided%0Aby%20a%20Memory-Aware%20Runtime%20Profiler%20without%20user%20intervention.%20ProTrain%20does%20not%0Achange%20the%20training%20algorithm%20and%20thus%20does%20not%20compromise%20accuracy.%0AExperiments%20show%20that%20ProTrain%20improves%20training%20throughput%20by%201.43%24%5Ctimes%24%20to%0A2.71%24%5Ctimes%24%20compared%20to%20the%20SOTA%20training%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08334v1&entry.124074799=Read"},
{"title": "Short-Long Convolutions Help Hardware-Efficient Linear Attention to\n  Focus on Long Sequences", "author": "Zicheng Liu and Siyuan Li and Li Wang and Zedong Wang and Yunfan Liu and Stan Z. Li", "abstract": "  To mitigate the computational complexity in the self-attention mechanism on\nlong sequences, linear attention utilizes computation tricks to achieve linear\ncomplexity, while state space models (SSMs) popularize a favorable practice of\nusing non-data-dependent memory pattern, i.e., emphasize the near and neglect\nthe distant, to processing sequences. Recent studies have shown the priorities\nby combining them as one. However, the efficiency of linear attention remains\nonly at the theoretical level in a causal setting, and SSMs require various\ndesigned constraints to operate effectively on specific data. Therefore, in\norder to unveil the true power of the hybrid design, the following two issues\nneed to be addressed: (1) hardware-efficient implementation for linear\nattention and (2) stabilization of SSMs. To achieve this, we leverage the\nthought of tiling and hierarchy to propose CHELA (short-long Convolutions with\nHardware-Efficient Linear Attention), which replaces SSMs with short-long\nconvolutions and implements linear attention in a divide-and-conquer manner.\nThis approach enjoys global abstraction and data-dependent selection from\nstable SSM and linear attention while maintaining real linear complexity. Our\ncomprehensive experiments on the Long Range Arena benchmark and language\nmodeling tasks demonstrate the effectiveness of the proposed method.\n", "link": "http://arxiv.org/abs/2406.08128v1", "date": "2024-06-12", "relevancy": 2.077, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5391}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5059}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5031}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Short-Long%20Convolutions%20Help%20Hardware-Efficient%20Linear%20Attention%20to%0A%20%20Focus%20on%20Long%20Sequences&body=Title%3A%20Short-Long%20Convolutions%20Help%20Hardware-Efficient%20Linear%20Attention%20to%0A%20%20Focus%20on%20Long%20Sequences%0AAuthor%3A%20Zicheng%20Liu%20and%20Siyuan%20Li%20and%20Li%20Wang%20and%20Zedong%20Wang%20and%20Yunfan%20Liu%20and%20Stan%20Z.%20Li%0AAbstract%3A%20%20%20To%20mitigate%20the%20computational%20complexity%20in%20the%20self-attention%20mechanism%20on%0Along%20sequences%2C%20linear%20attention%20utilizes%20computation%20tricks%20to%20achieve%20linear%0Acomplexity%2C%20while%20state%20space%20models%20%28SSMs%29%20popularize%20a%20favorable%20practice%20of%0Ausing%20non-data-dependent%20memory%20pattern%2C%20i.e.%2C%20emphasize%20the%20near%20and%20neglect%0Athe%20distant%2C%20to%20processing%20sequences.%20Recent%20studies%20have%20shown%20the%20priorities%0Aby%20combining%20them%20as%20one.%20However%2C%20the%20efficiency%20of%20linear%20attention%20remains%0Aonly%20at%20the%20theoretical%20level%20in%20a%20causal%20setting%2C%20and%20SSMs%20require%20various%0Adesigned%20constraints%20to%20operate%20effectively%20on%20specific%20data.%20Therefore%2C%20in%0Aorder%20to%20unveil%20the%20true%20power%20of%20the%20hybrid%20design%2C%20the%20following%20two%20issues%0Aneed%20to%20be%20addressed%3A%20%281%29%20hardware-efficient%20implementation%20for%20linear%0Aattention%20and%20%282%29%20stabilization%20of%20SSMs.%20To%20achieve%20this%2C%20we%20leverage%20the%0Athought%20of%20tiling%20and%20hierarchy%20to%20propose%20CHELA%20%28short-long%20Convolutions%20with%0AHardware-Efficient%20Linear%20Attention%29%2C%20which%20replaces%20SSMs%20with%20short-long%0Aconvolutions%20and%20implements%20linear%20attention%20in%20a%20divide-and-conquer%20manner.%0AThis%20approach%20enjoys%20global%20abstraction%20and%20data-dependent%20selection%20from%0Astable%20SSM%20and%20linear%20attention%20while%20maintaining%20real%20linear%20complexity.%20Our%0Acomprehensive%20experiments%20on%20the%20Long%20Range%20Arena%20benchmark%20and%20language%0Amodeling%20tasks%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08128v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShort-Long%2520Convolutions%2520Help%2520Hardware-Efficient%2520Linear%2520Attention%2520to%250A%2520%2520Focus%2520on%2520Long%2520Sequences%26entry.906535625%3DZicheng%2520Liu%2520and%2520Siyuan%2520Li%2520and%2520Li%2520Wang%2520and%2520Zedong%2520Wang%2520and%2520Yunfan%2520Liu%2520and%2520Stan%2520Z.%2520Li%26entry.1292438233%3D%2520%2520To%2520mitigate%2520the%2520computational%2520complexity%2520in%2520the%2520self-attention%2520mechanism%2520on%250Along%2520sequences%252C%2520linear%2520attention%2520utilizes%2520computation%2520tricks%2520to%2520achieve%2520linear%250Acomplexity%252C%2520while%2520state%2520space%2520models%2520%2528SSMs%2529%2520popularize%2520a%2520favorable%2520practice%2520of%250Ausing%2520non-data-dependent%2520memory%2520pattern%252C%2520i.e.%252C%2520emphasize%2520the%2520near%2520and%2520neglect%250Athe%2520distant%252C%2520to%2520processing%2520sequences.%2520Recent%2520studies%2520have%2520shown%2520the%2520priorities%250Aby%2520combining%2520them%2520as%2520one.%2520However%252C%2520the%2520efficiency%2520of%2520linear%2520attention%2520remains%250Aonly%2520at%2520the%2520theoretical%2520level%2520in%2520a%2520causal%2520setting%252C%2520and%2520SSMs%2520require%2520various%250Adesigned%2520constraints%2520to%2520operate%2520effectively%2520on%2520specific%2520data.%2520Therefore%252C%2520in%250Aorder%2520to%2520unveil%2520the%2520true%2520power%2520of%2520the%2520hybrid%2520design%252C%2520the%2520following%2520two%2520issues%250Aneed%2520to%2520be%2520addressed%253A%2520%25281%2529%2520hardware-efficient%2520implementation%2520for%2520linear%250Aattention%2520and%2520%25282%2529%2520stabilization%2520of%2520SSMs.%2520To%2520achieve%2520this%252C%2520we%2520leverage%2520the%250Athought%2520of%2520tiling%2520and%2520hierarchy%2520to%2520propose%2520CHELA%2520%2528short-long%2520Convolutions%2520with%250AHardware-Efficient%2520Linear%2520Attention%2529%252C%2520which%2520replaces%2520SSMs%2520with%2520short-long%250Aconvolutions%2520and%2520implements%2520linear%2520attention%2520in%2520a%2520divide-and-conquer%2520manner.%250AThis%2520approach%2520enjoys%2520global%2520abstraction%2520and%2520data-dependent%2520selection%2520from%250Astable%2520SSM%2520and%2520linear%2520attention%2520while%2520maintaining%2520real%2520linear%2520complexity.%2520Our%250Acomprehensive%2520experiments%2520on%2520the%2520Long%2520Range%2520Arena%2520benchmark%2520and%2520language%250Amodeling%2520tasks%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08128v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Short-Long%20Convolutions%20Help%20Hardware-Efficient%20Linear%20Attention%20to%0A%20%20Focus%20on%20Long%20Sequences&entry.906535625=Zicheng%20Liu%20and%20Siyuan%20Li%20and%20Li%20Wang%20and%20Zedong%20Wang%20and%20Yunfan%20Liu%20and%20Stan%20Z.%20Li&entry.1292438233=%20%20To%20mitigate%20the%20computational%20complexity%20in%20the%20self-attention%20mechanism%20on%0Along%20sequences%2C%20linear%20attention%20utilizes%20computation%20tricks%20to%20achieve%20linear%0Acomplexity%2C%20while%20state%20space%20models%20%28SSMs%29%20popularize%20a%20favorable%20practice%20of%0Ausing%20non-data-dependent%20memory%20pattern%2C%20i.e.%2C%20emphasize%20the%20near%20and%20neglect%0Athe%20distant%2C%20to%20processing%20sequences.%20Recent%20studies%20have%20shown%20the%20priorities%0Aby%20combining%20them%20as%20one.%20However%2C%20the%20efficiency%20of%20linear%20attention%20remains%0Aonly%20at%20the%20theoretical%20level%20in%20a%20causal%20setting%2C%20and%20SSMs%20require%20various%0Adesigned%20constraints%20to%20operate%20effectively%20on%20specific%20data.%20Therefore%2C%20in%0Aorder%20to%20unveil%20the%20true%20power%20of%20the%20hybrid%20design%2C%20the%20following%20two%20issues%0Aneed%20to%20be%20addressed%3A%20%281%29%20hardware-efficient%20implementation%20for%20linear%0Aattention%20and%20%282%29%20stabilization%20of%20SSMs.%20To%20achieve%20this%2C%20we%20leverage%20the%0Athought%20of%20tiling%20and%20hierarchy%20to%20propose%20CHELA%20%28short-long%20Convolutions%20with%0AHardware-Efficient%20Linear%20Attention%29%2C%20which%20replaces%20SSMs%20with%20short-long%0Aconvolutions%20and%20implements%20linear%20attention%20in%20a%20divide-and-conquer%20manner.%0AThis%20approach%20enjoys%20global%20abstraction%20and%20data-dependent%20selection%20from%0Astable%20SSM%20and%20linear%20attention%20while%20maintaining%20real%20linear%20complexity.%20Our%0Acomprehensive%20experiments%20on%20the%20Long%20Range%20Arena%20benchmark%20and%20language%0Amodeling%20tasks%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08128v1&entry.124074799=Read"},
{"title": "Smartphone region-wise image indoor localization using deep learning for\n  indoor tourist attraction", "author": "Gabriel Toshio Hirokawa Higa and Rodrigo Stuqui Monzani and Jorge Fernando da Silva Cecatto and Maria Fernanda Balestieri Mariano de Souza and Vanessa Aparecida de Moraes Weber and Hemerson Pistori and Edson Takashi Matsubara", "abstract": "  Smart indoor tourist attractions, such as smart museums and aquariums,\nusually require a significant investment in indoor localization devices. The\nsmartphone Global Positional Systems use is unsuitable for scenarios where\ndense materials such as concrete and metal block weaken the GPS signals, which\nis the most common scenario in an indoor tourist attraction. Deep learning\nmakes it possible to perform region-wise indoor localization using smartphone\nimages. This approach does not require any investment in infrastructure,\nreducing the cost and time to turn museums and aquariums into smart museums or\nsmart aquariums. This paper proposes using deep learning algorithms to classify\nlocations using smartphone camera images for indoor tourism attractions. We\nevaluate our proposal in a real-world scenario in Brazil. We extensively\ncollect images from ten different smartphones to classify biome-themed fish\ntanks inside the Pantanal Biopark, creating a new dataset of 3654 images. We\ntested seven state-of-the-art neural networks, three being transformer-based,\nachieving precision around 90% on average and recall and f-score around 89% on\naverage. The results indicate good feasibility of the proposal in a most indoor\ntourist attractions.\n", "link": "http://arxiv.org/abs/2403.07621v2", "date": "2024-06-12", "relevancy": 2.0761, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5323}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.513}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smartphone%20region-wise%20image%20indoor%20localization%20using%20deep%20learning%20for%0A%20%20indoor%20tourist%20attraction&body=Title%3A%20Smartphone%20region-wise%20image%20indoor%20localization%20using%20deep%20learning%20for%0A%20%20indoor%20tourist%20attraction%0AAuthor%3A%20Gabriel%20Toshio%20Hirokawa%20Higa%20and%20Rodrigo%20Stuqui%20Monzani%20and%20Jorge%20Fernando%20da%20Silva%20Cecatto%20and%20Maria%20Fernanda%20Balestieri%20Mariano%20de%20Souza%20and%20Vanessa%20Aparecida%20de%20Moraes%20Weber%20and%20Hemerson%20Pistori%20and%20Edson%20Takashi%20Matsubara%0AAbstract%3A%20%20%20Smart%20indoor%20tourist%20attractions%2C%20such%20as%20smart%20museums%20and%20aquariums%2C%0Ausually%20require%20a%20significant%20investment%20in%20indoor%20localization%20devices.%20The%0Asmartphone%20Global%20Positional%20Systems%20use%20is%20unsuitable%20for%20scenarios%20where%0Adense%20materials%20such%20as%20concrete%20and%20metal%20block%20weaken%20the%20GPS%20signals%2C%20which%0Ais%20the%20most%20common%20scenario%20in%20an%20indoor%20tourist%20attraction.%20Deep%20learning%0Amakes%20it%20possible%20to%20perform%20region-wise%20indoor%20localization%20using%20smartphone%0Aimages.%20This%20approach%20does%20not%20require%20any%20investment%20in%20infrastructure%2C%0Areducing%20the%20cost%20and%20time%20to%20turn%20museums%20and%20aquariums%20into%20smart%20museums%20or%0Asmart%20aquariums.%20This%20paper%20proposes%20using%20deep%20learning%20algorithms%20to%20classify%0Alocations%20using%20smartphone%20camera%20images%20for%20indoor%20tourism%20attractions.%20We%0Aevaluate%20our%20proposal%20in%20a%20real-world%20scenario%20in%20Brazil.%20We%20extensively%0Acollect%20images%20from%20ten%20different%20smartphones%20to%20classify%20biome-themed%20fish%0Atanks%20inside%20the%20Pantanal%20Biopark%2C%20creating%20a%20new%20dataset%20of%203654%20images.%20We%0Atested%20seven%20state-of-the-art%20neural%20networks%2C%20three%20being%20transformer-based%2C%0Aachieving%20precision%20around%2090%25%20on%20average%20and%20recall%20and%20f-score%20around%2089%25%20on%0Aaverage.%20The%20results%20indicate%20good%20feasibility%20of%20the%20proposal%20in%20a%20most%20indoor%0Atourist%20attractions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07621v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmartphone%2520region-wise%2520image%2520indoor%2520localization%2520using%2520deep%2520learning%2520for%250A%2520%2520indoor%2520tourist%2520attraction%26entry.906535625%3DGabriel%2520Toshio%2520Hirokawa%2520Higa%2520and%2520Rodrigo%2520Stuqui%2520Monzani%2520and%2520Jorge%2520Fernando%2520da%2520Silva%2520Cecatto%2520and%2520Maria%2520Fernanda%2520Balestieri%2520Mariano%2520de%2520Souza%2520and%2520Vanessa%2520Aparecida%2520de%2520Moraes%2520Weber%2520and%2520Hemerson%2520Pistori%2520and%2520Edson%2520Takashi%2520Matsubara%26entry.1292438233%3D%2520%2520Smart%2520indoor%2520tourist%2520attractions%252C%2520such%2520as%2520smart%2520museums%2520and%2520aquariums%252C%250Ausually%2520require%2520a%2520significant%2520investment%2520in%2520indoor%2520localization%2520devices.%2520The%250Asmartphone%2520Global%2520Positional%2520Systems%2520use%2520is%2520unsuitable%2520for%2520scenarios%2520where%250Adense%2520materials%2520such%2520as%2520concrete%2520and%2520metal%2520block%2520weaken%2520the%2520GPS%2520signals%252C%2520which%250Ais%2520the%2520most%2520common%2520scenario%2520in%2520an%2520indoor%2520tourist%2520attraction.%2520Deep%2520learning%250Amakes%2520it%2520possible%2520to%2520perform%2520region-wise%2520indoor%2520localization%2520using%2520smartphone%250Aimages.%2520This%2520approach%2520does%2520not%2520require%2520any%2520investment%2520in%2520infrastructure%252C%250Areducing%2520the%2520cost%2520and%2520time%2520to%2520turn%2520museums%2520and%2520aquariums%2520into%2520smart%2520museums%2520or%250Asmart%2520aquariums.%2520This%2520paper%2520proposes%2520using%2520deep%2520learning%2520algorithms%2520to%2520classify%250Alocations%2520using%2520smartphone%2520camera%2520images%2520for%2520indoor%2520tourism%2520attractions.%2520We%250Aevaluate%2520our%2520proposal%2520in%2520a%2520real-world%2520scenario%2520in%2520Brazil.%2520We%2520extensively%250Acollect%2520images%2520from%2520ten%2520different%2520smartphones%2520to%2520classify%2520biome-themed%2520fish%250Atanks%2520inside%2520the%2520Pantanal%2520Biopark%252C%2520creating%2520a%2520new%2520dataset%2520of%25203654%2520images.%2520We%250Atested%2520seven%2520state-of-the-art%2520neural%2520networks%252C%2520three%2520being%2520transformer-based%252C%250Aachieving%2520precision%2520around%252090%2525%2520on%2520average%2520and%2520recall%2520and%2520f-score%2520around%252089%2525%2520on%250Aaverage.%2520The%2520results%2520indicate%2520good%2520feasibility%2520of%2520the%2520proposal%2520in%2520a%2520most%2520indoor%250Atourist%2520attractions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07621v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smartphone%20region-wise%20image%20indoor%20localization%20using%20deep%20learning%20for%0A%20%20indoor%20tourist%20attraction&entry.906535625=Gabriel%20Toshio%20Hirokawa%20Higa%20and%20Rodrigo%20Stuqui%20Monzani%20and%20Jorge%20Fernando%20da%20Silva%20Cecatto%20and%20Maria%20Fernanda%20Balestieri%20Mariano%20de%20Souza%20and%20Vanessa%20Aparecida%20de%20Moraes%20Weber%20and%20Hemerson%20Pistori%20and%20Edson%20Takashi%20Matsubara&entry.1292438233=%20%20Smart%20indoor%20tourist%20attractions%2C%20such%20as%20smart%20museums%20and%20aquariums%2C%0Ausually%20require%20a%20significant%20investment%20in%20indoor%20localization%20devices.%20The%0Asmartphone%20Global%20Positional%20Systems%20use%20is%20unsuitable%20for%20scenarios%20where%0Adense%20materials%20such%20as%20concrete%20and%20metal%20block%20weaken%20the%20GPS%20signals%2C%20which%0Ais%20the%20most%20common%20scenario%20in%20an%20indoor%20tourist%20attraction.%20Deep%20learning%0Amakes%20it%20possible%20to%20perform%20region-wise%20indoor%20localization%20using%20smartphone%0Aimages.%20This%20approach%20does%20not%20require%20any%20investment%20in%20infrastructure%2C%0Areducing%20the%20cost%20and%20time%20to%20turn%20museums%20and%20aquariums%20into%20smart%20museums%20or%0Asmart%20aquariums.%20This%20paper%20proposes%20using%20deep%20learning%20algorithms%20to%20classify%0Alocations%20using%20smartphone%20camera%20images%20for%20indoor%20tourism%20attractions.%20We%0Aevaluate%20our%20proposal%20in%20a%20real-world%20scenario%20in%20Brazil.%20We%20extensively%0Acollect%20images%20from%20ten%20different%20smartphones%20to%20classify%20biome-themed%20fish%0Atanks%20inside%20the%20Pantanal%20Biopark%2C%20creating%20a%20new%20dataset%20of%203654%20images.%20We%0Atested%20seven%20state-of-the-art%20neural%20networks%2C%20three%20being%20transformer-based%2C%0Aachieving%20precision%20around%2090%25%20on%20average%20and%20recall%20and%20f-score%20around%2089%25%20on%0Aaverage.%20The%20results%20indicate%20good%20feasibility%20of%20the%20proposal%20in%20a%20most%20indoor%0Atourist%20attractions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07621v2&entry.124074799=Read"},
{"title": "Large Language Models Must Be Taught to Know What They Don't Know", "author": "Sanyam Kapoor and Nate Gruver and Manley Roberts and Katherine Collins and Arka Pal and Umang Bhatt and Adrian Weller and Samuel Dooley and Micah Goldblum and Andrew Gordon Wilson", "abstract": "  When using large language models (LLMs) in high-stakes applications, we need\nto know when we can trust their predictions. Some works argue that prompting\nhigh-performance LLMs is sufficient to produce calibrated uncertainties, while\nothers introduce sampling methods that can be prohibitively expensive. In this\nwork, we first argue that prompting on its own is insufficient to achieve good\ncalibration and then show that fine-tuning on a small dataset of correct and\nincorrect answers can create an uncertainty estimate with good generalization\nand small computational overhead. We show that a thousand graded examples are\nsufficient to outperform baseline methods and that training through the\nfeatures of a model is necessary for good performance and tractable for large\nopen-source models when using LoRA. We also investigate the mechanisms that\nenable reliable LLM uncertainty estimation, finding that many models can be\nused as general-purpose uncertainty estimators, applicable not just to their\nown uncertainties but also the uncertainty of other models. Lastly, we show\nthat uncertainty estimates inform human use of LLMs in human-AI collaborative\nsettings through a user study.\n", "link": "http://arxiv.org/abs/2406.08391v1", "date": "2024-06-12", "relevancy": 2.0756, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5809}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5192}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Must%20Be%20Taught%20to%20Know%20What%20They%20Don%27t%20Know&body=Title%3A%20Large%20Language%20Models%20Must%20Be%20Taught%20to%20Know%20What%20They%20Don%27t%20Know%0AAuthor%3A%20Sanyam%20Kapoor%20and%20Nate%20Gruver%20and%20Manley%20Roberts%20and%20Katherine%20Collins%20and%20Arka%20Pal%20and%20Umang%20Bhatt%20and%20Adrian%20Weller%20and%20Samuel%20Dooley%20and%20Micah%20Goldblum%20and%20Andrew%20Gordon%20Wilson%0AAbstract%3A%20%20%20When%20using%20large%20language%20models%20%28LLMs%29%20in%20high-stakes%20applications%2C%20we%20need%0Ato%20know%20when%20we%20can%20trust%20their%20predictions.%20Some%20works%20argue%20that%20prompting%0Ahigh-performance%20LLMs%20is%20sufficient%20to%20produce%20calibrated%20uncertainties%2C%20while%0Aothers%20introduce%20sampling%20methods%20that%20can%20be%20prohibitively%20expensive.%20In%20this%0Awork%2C%20we%20first%20argue%20that%20prompting%20on%20its%20own%20is%20insufficient%20to%20achieve%20good%0Acalibration%20and%20then%20show%20that%20fine-tuning%20on%20a%20small%20dataset%20of%20correct%20and%0Aincorrect%20answers%20can%20create%20an%20uncertainty%20estimate%20with%20good%20generalization%0Aand%20small%20computational%20overhead.%20We%20show%20that%20a%20thousand%20graded%20examples%20are%0Asufficient%20to%20outperform%20baseline%20methods%20and%20that%20training%20through%20the%0Afeatures%20of%20a%20model%20is%20necessary%20for%20good%20performance%20and%20tractable%20for%20large%0Aopen-source%20models%20when%20using%20LoRA.%20We%20also%20investigate%20the%20mechanisms%20that%0Aenable%20reliable%20LLM%20uncertainty%20estimation%2C%20finding%20that%20many%20models%20can%20be%0Aused%20as%20general-purpose%20uncertainty%20estimators%2C%20applicable%20not%20just%20to%20their%0Aown%20uncertainties%20but%20also%20the%20uncertainty%20of%20other%20models.%20Lastly%2C%20we%20show%0Athat%20uncertainty%20estimates%20inform%20human%20use%20of%20LLMs%20in%20human-AI%20collaborative%0Asettings%20through%20a%20user%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08391v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Must%2520Be%2520Taught%2520to%2520Know%2520What%2520They%2520Don%2527t%2520Know%26entry.906535625%3DSanyam%2520Kapoor%2520and%2520Nate%2520Gruver%2520and%2520Manley%2520Roberts%2520and%2520Katherine%2520Collins%2520and%2520Arka%2520Pal%2520and%2520Umang%2520Bhatt%2520and%2520Adrian%2520Weller%2520and%2520Samuel%2520Dooley%2520and%2520Micah%2520Goldblum%2520and%2520Andrew%2520Gordon%2520Wilson%26entry.1292438233%3D%2520%2520When%2520using%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520high-stakes%2520applications%252C%2520we%2520need%250Ato%2520know%2520when%2520we%2520can%2520trust%2520their%2520predictions.%2520Some%2520works%2520argue%2520that%2520prompting%250Ahigh-performance%2520LLMs%2520is%2520sufficient%2520to%2520produce%2520calibrated%2520uncertainties%252C%2520while%250Aothers%2520introduce%2520sampling%2520methods%2520that%2520can%2520be%2520prohibitively%2520expensive.%2520In%2520this%250Awork%252C%2520we%2520first%2520argue%2520that%2520prompting%2520on%2520its%2520own%2520is%2520insufficient%2520to%2520achieve%2520good%250Acalibration%2520and%2520then%2520show%2520that%2520fine-tuning%2520on%2520a%2520small%2520dataset%2520of%2520correct%2520and%250Aincorrect%2520answers%2520can%2520create%2520an%2520uncertainty%2520estimate%2520with%2520good%2520generalization%250Aand%2520small%2520computational%2520overhead.%2520We%2520show%2520that%2520a%2520thousand%2520graded%2520examples%2520are%250Asufficient%2520to%2520outperform%2520baseline%2520methods%2520and%2520that%2520training%2520through%2520the%250Afeatures%2520of%2520a%2520model%2520is%2520necessary%2520for%2520good%2520performance%2520and%2520tractable%2520for%2520large%250Aopen-source%2520models%2520when%2520using%2520LoRA.%2520We%2520also%2520investigate%2520the%2520mechanisms%2520that%250Aenable%2520reliable%2520LLM%2520uncertainty%2520estimation%252C%2520finding%2520that%2520many%2520models%2520can%2520be%250Aused%2520as%2520general-purpose%2520uncertainty%2520estimators%252C%2520applicable%2520not%2520just%2520to%2520their%250Aown%2520uncertainties%2520but%2520also%2520the%2520uncertainty%2520of%2520other%2520models.%2520Lastly%252C%2520we%2520show%250Athat%2520uncertainty%2520estimates%2520inform%2520human%2520use%2520of%2520LLMs%2520in%2520human-AI%2520collaborative%250Asettings%2520through%2520a%2520user%2520study.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08391v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Must%20Be%20Taught%20to%20Know%20What%20They%20Don%27t%20Know&entry.906535625=Sanyam%20Kapoor%20and%20Nate%20Gruver%20and%20Manley%20Roberts%20and%20Katherine%20Collins%20and%20Arka%20Pal%20and%20Umang%20Bhatt%20and%20Adrian%20Weller%20and%20Samuel%20Dooley%20and%20Micah%20Goldblum%20and%20Andrew%20Gordon%20Wilson&entry.1292438233=%20%20When%20using%20large%20language%20models%20%28LLMs%29%20in%20high-stakes%20applications%2C%20we%20need%0Ato%20know%20when%20we%20can%20trust%20their%20predictions.%20Some%20works%20argue%20that%20prompting%0Ahigh-performance%20LLMs%20is%20sufficient%20to%20produce%20calibrated%20uncertainties%2C%20while%0Aothers%20introduce%20sampling%20methods%20that%20can%20be%20prohibitively%20expensive.%20In%20this%0Awork%2C%20we%20first%20argue%20that%20prompting%20on%20its%20own%20is%20insufficient%20to%20achieve%20good%0Acalibration%20and%20then%20show%20that%20fine-tuning%20on%20a%20small%20dataset%20of%20correct%20and%0Aincorrect%20answers%20can%20create%20an%20uncertainty%20estimate%20with%20good%20generalization%0Aand%20small%20computational%20overhead.%20We%20show%20that%20a%20thousand%20graded%20examples%20are%0Asufficient%20to%20outperform%20baseline%20methods%20and%20that%20training%20through%20the%0Afeatures%20of%20a%20model%20is%20necessary%20for%20good%20performance%20and%20tractable%20for%20large%0Aopen-source%20models%20when%20using%20LoRA.%20We%20also%20investigate%20the%20mechanisms%20that%0Aenable%20reliable%20LLM%20uncertainty%20estimation%2C%20finding%20that%20many%20models%20can%20be%0Aused%20as%20general-purpose%20uncertainty%20estimators%2C%20applicable%20not%20just%20to%20their%0Aown%20uncertainties%20but%20also%20the%20uncertainty%20of%20other%20models.%20Lastly%2C%20we%20show%0Athat%20uncertainty%20estimates%20inform%20human%20use%20of%20LLMs%20in%20human-AI%20collaborative%0Asettings%20through%20a%20user%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08391v1&entry.124074799=Read"},
{"title": "Hierarchical Features Matter: A Deep Exploration of GAN Priors for\n  Improved Dataset Distillation", "author": "Xinhao Zhong and Hao Fang and Bin Chen and Xulin Gu and Tao Dai and Meikang Qiu and Shu-Tao Xia", "abstract": "  Dataset distillation is an emerging dataset reduction method, which condenses\nlarge-scale datasets while maintaining task accuracy. Current methods have\nintegrated parameterization techniques to boost synthetic dataset performance\nby shifting the optimization space from pixel to another informative feature\ndomain. However, they limit themselves to a fixed optimization space for\ndistillation, neglecting the diverse guidance across different informative\nlatent spaces. To overcome this limitation, we propose a novel parameterization\nmethod dubbed Hierarchical Generative Latent Distillation (H-GLaD), to\nsystematically explore hierarchical layers within the generative adversarial\nnetworks (GANs). This allows us to progressively span from the initial latent\nspace to the final pixel space. In addition, we introduce a novel\nclass-relevant feature distance metric to alleviate the computational burden\nassociated with synthetic dataset evaluation, bridging the gap between\nsynthetic and original datasets. Experimental results demonstrate that the\nproposed H-GLaD achieves a significant improvement in both same-architecture\nand cross-architecture performance with equivalent time consumption.\n", "link": "http://arxiv.org/abs/2406.05704v2", "date": "2024-06-12", "relevancy": 2.0633, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5347}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5176}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Features%20Matter%3A%20A%20Deep%20Exploration%20of%20GAN%20Priors%20for%0A%20%20Improved%20Dataset%20Distillation&body=Title%3A%20Hierarchical%20Features%20Matter%3A%20A%20Deep%20Exploration%20of%20GAN%20Priors%20for%0A%20%20Improved%20Dataset%20Distillation%0AAuthor%3A%20Xinhao%20Zhong%20and%20Hao%20Fang%20and%20Bin%20Chen%20and%20Xulin%20Gu%20and%20Tao%20Dai%20and%20Meikang%20Qiu%20and%20Shu-Tao%20Xia%0AAbstract%3A%20%20%20Dataset%20distillation%20is%20an%20emerging%20dataset%20reduction%20method%2C%20which%20condenses%0Alarge-scale%20datasets%20while%20maintaining%20task%20accuracy.%20Current%20methods%20have%0Aintegrated%20parameterization%20techniques%20to%20boost%20synthetic%20dataset%20performance%0Aby%20shifting%20the%20optimization%20space%20from%20pixel%20to%20another%20informative%20feature%0Adomain.%20However%2C%20they%20limit%20themselves%20to%20a%20fixed%20optimization%20space%20for%0Adistillation%2C%20neglecting%20the%20diverse%20guidance%20across%20different%20informative%0Alatent%20spaces.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20novel%20parameterization%0Amethod%20dubbed%20Hierarchical%20Generative%20Latent%20Distillation%20%28H-GLaD%29%2C%20to%0Asystematically%20explore%20hierarchical%20layers%20within%20the%20generative%20adversarial%0Anetworks%20%28GANs%29.%20This%20allows%20us%20to%20progressively%20span%20from%20the%20initial%20latent%0Aspace%20to%20the%20final%20pixel%20space.%20In%20addition%2C%20we%20introduce%20a%20novel%0Aclass-relevant%20feature%20distance%20metric%20to%20alleviate%20the%20computational%20burden%0Aassociated%20with%20synthetic%20dataset%20evaluation%2C%20bridging%20the%20gap%20between%0Asynthetic%20and%20original%20datasets.%20Experimental%20results%20demonstrate%20that%20the%0Aproposed%20H-GLaD%20achieves%20a%20significant%20improvement%20in%20both%20same-architecture%0Aand%20cross-architecture%20performance%20with%20equivalent%20time%20consumption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05704v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Features%2520Matter%253A%2520A%2520Deep%2520Exploration%2520of%2520GAN%2520Priors%2520for%250A%2520%2520Improved%2520Dataset%2520Distillation%26entry.906535625%3DXinhao%2520Zhong%2520and%2520Hao%2520Fang%2520and%2520Bin%2520Chen%2520and%2520Xulin%2520Gu%2520and%2520Tao%2520Dai%2520and%2520Meikang%2520Qiu%2520and%2520Shu-Tao%2520Xia%26entry.1292438233%3D%2520%2520Dataset%2520distillation%2520is%2520an%2520emerging%2520dataset%2520reduction%2520method%252C%2520which%2520condenses%250Alarge-scale%2520datasets%2520while%2520maintaining%2520task%2520accuracy.%2520Current%2520methods%2520have%250Aintegrated%2520parameterization%2520techniques%2520to%2520boost%2520synthetic%2520dataset%2520performance%250Aby%2520shifting%2520the%2520optimization%2520space%2520from%2520pixel%2520to%2520another%2520informative%2520feature%250Adomain.%2520However%252C%2520they%2520limit%2520themselves%2520to%2520a%2520fixed%2520optimization%2520space%2520for%250Adistillation%252C%2520neglecting%2520the%2520diverse%2520guidance%2520across%2520different%2520informative%250Alatent%2520spaces.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520a%2520novel%2520parameterization%250Amethod%2520dubbed%2520Hierarchical%2520Generative%2520Latent%2520Distillation%2520%2528H-GLaD%2529%252C%2520to%250Asystematically%2520explore%2520hierarchical%2520layers%2520within%2520the%2520generative%2520adversarial%250Anetworks%2520%2528GANs%2529.%2520This%2520allows%2520us%2520to%2520progressively%2520span%2520from%2520the%2520initial%2520latent%250Aspace%2520to%2520the%2520final%2520pixel%2520space.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520novel%250Aclass-relevant%2520feature%2520distance%2520metric%2520to%2520alleviate%2520the%2520computational%2520burden%250Aassociated%2520with%2520synthetic%2520dataset%2520evaluation%252C%2520bridging%2520the%2520gap%2520between%250Asynthetic%2520and%2520original%2520datasets.%2520Experimental%2520results%2520demonstrate%2520that%2520the%250Aproposed%2520H-GLaD%2520achieves%2520a%2520significant%2520improvement%2520in%2520both%2520same-architecture%250Aand%2520cross-architecture%2520performance%2520with%2520equivalent%2520time%2520consumption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05704v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Features%20Matter%3A%20A%20Deep%20Exploration%20of%20GAN%20Priors%20for%0A%20%20Improved%20Dataset%20Distillation&entry.906535625=Xinhao%20Zhong%20and%20Hao%20Fang%20and%20Bin%20Chen%20and%20Xulin%20Gu%20and%20Tao%20Dai%20and%20Meikang%20Qiu%20and%20Shu-Tao%20Xia&entry.1292438233=%20%20Dataset%20distillation%20is%20an%20emerging%20dataset%20reduction%20method%2C%20which%20condenses%0Alarge-scale%20datasets%20while%20maintaining%20task%20accuracy.%20Current%20methods%20have%0Aintegrated%20parameterization%20techniques%20to%20boost%20synthetic%20dataset%20performance%0Aby%20shifting%20the%20optimization%20space%20from%20pixel%20to%20another%20informative%20feature%0Adomain.%20However%2C%20they%20limit%20themselves%20to%20a%20fixed%20optimization%20space%20for%0Adistillation%2C%20neglecting%20the%20diverse%20guidance%20across%20different%20informative%0Alatent%20spaces.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20novel%20parameterization%0Amethod%20dubbed%20Hierarchical%20Generative%20Latent%20Distillation%20%28H-GLaD%29%2C%20to%0Asystematically%20explore%20hierarchical%20layers%20within%20the%20generative%20adversarial%0Anetworks%20%28GANs%29.%20This%20allows%20us%20to%20progressively%20span%20from%20the%20initial%20latent%0Aspace%20to%20the%20final%20pixel%20space.%20In%20addition%2C%20we%20introduce%20a%20novel%0Aclass-relevant%20feature%20distance%20metric%20to%20alleviate%20the%20computational%20burden%0Aassociated%20with%20synthetic%20dataset%20evaluation%2C%20bridging%20the%20gap%20between%0Asynthetic%20and%20original%20datasets.%20Experimental%20results%20demonstrate%20that%20the%0Aproposed%20H-GLaD%20achieves%20a%20significant%20improvement%20in%20both%20same-architecture%0Aand%20cross-architecture%20performance%20with%20equivalent%20time%20consumption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05704v2&entry.124074799=Read"},
{"title": "tinyCLAP: Distilling Constrastive Language-Audio Pretrained Models", "author": "Francesco Paissan and Elisabetta Farella", "abstract": "  Contrastive Language-Audio Pretraining (CLAP) became of crucial importance in\nthe field of audio and speech processing. Its employment ranges from sound\nevent detection to text-to-audio generation. However, one of the main\nlimitations is the considerable amount of data required in the training process\nand the overall computational complexity during inference. This paper\ninvestigates how we can reduce the complexity of contrastive language-audio\npre-trained models, yielding an efficient model that we call tinyCLAP. We\nderive an unimodal distillation loss from first principles and explore how the\ndimensionality of the shared, multimodal latent space can be reduced via\npruning. TinyCLAP uses only 6% of the original Microsoft CLAP parameters with a\nminimal reduction (less than 5%) in zero-shot classification performance across\nthe three sound event detection datasets on which it was tested\n", "link": "http://arxiv.org/abs/2311.14517v2", "date": "2024-06-12", "relevancy": 2.0607, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5621}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4876}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20tinyCLAP%3A%20Distilling%20Constrastive%20Language-Audio%20Pretrained%20Models&body=Title%3A%20tinyCLAP%3A%20Distilling%20Constrastive%20Language-Audio%20Pretrained%20Models%0AAuthor%3A%20Francesco%20Paissan%20and%20Elisabetta%20Farella%0AAbstract%3A%20%20%20Contrastive%20Language-Audio%20Pretraining%20%28CLAP%29%20became%20of%20crucial%20importance%20in%0Athe%20field%20of%20audio%20and%20speech%20processing.%20Its%20employment%20ranges%20from%20sound%0Aevent%20detection%20to%20text-to-audio%20generation.%20However%2C%20one%20of%20the%20main%0Alimitations%20is%20the%20considerable%20amount%20of%20data%20required%20in%20the%20training%20process%0Aand%20the%20overall%20computational%20complexity%20during%20inference.%20This%20paper%0Ainvestigates%20how%20we%20can%20reduce%20the%20complexity%20of%20contrastive%20language-audio%0Apre-trained%20models%2C%20yielding%20an%20efficient%20model%20that%20we%20call%20tinyCLAP.%20We%0Aderive%20an%20unimodal%20distillation%20loss%20from%20first%20principles%20and%20explore%20how%20the%0Adimensionality%20of%20the%20shared%2C%20multimodal%20latent%20space%20can%20be%20reduced%20via%0Apruning.%20TinyCLAP%20uses%20only%206%25%20of%20the%20original%20Microsoft%20CLAP%20parameters%20with%20a%0Aminimal%20reduction%20%28less%20than%205%25%29%20in%20zero-shot%20classification%20performance%20across%0Athe%20three%20sound%20event%20detection%20datasets%20on%20which%20it%20was%20tested%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.14517v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DtinyCLAP%253A%2520Distilling%2520Constrastive%2520Language-Audio%2520Pretrained%2520Models%26entry.906535625%3DFrancesco%2520Paissan%2520and%2520Elisabetta%2520Farella%26entry.1292438233%3D%2520%2520Contrastive%2520Language-Audio%2520Pretraining%2520%2528CLAP%2529%2520became%2520of%2520crucial%2520importance%2520in%250Athe%2520field%2520of%2520audio%2520and%2520speech%2520processing.%2520Its%2520employment%2520ranges%2520from%2520sound%250Aevent%2520detection%2520to%2520text-to-audio%2520generation.%2520However%252C%2520one%2520of%2520the%2520main%250Alimitations%2520is%2520the%2520considerable%2520amount%2520of%2520data%2520required%2520in%2520the%2520training%2520process%250Aand%2520the%2520overall%2520computational%2520complexity%2520during%2520inference.%2520This%2520paper%250Ainvestigates%2520how%2520we%2520can%2520reduce%2520the%2520complexity%2520of%2520contrastive%2520language-audio%250Apre-trained%2520models%252C%2520yielding%2520an%2520efficient%2520model%2520that%2520we%2520call%2520tinyCLAP.%2520We%250Aderive%2520an%2520unimodal%2520distillation%2520loss%2520from%2520first%2520principles%2520and%2520explore%2520how%2520the%250Adimensionality%2520of%2520the%2520shared%252C%2520multimodal%2520latent%2520space%2520can%2520be%2520reduced%2520via%250Apruning.%2520TinyCLAP%2520uses%2520only%25206%2525%2520of%2520the%2520original%2520Microsoft%2520CLAP%2520parameters%2520with%2520a%250Aminimal%2520reduction%2520%2528less%2520than%25205%2525%2529%2520in%2520zero-shot%2520classification%2520performance%2520across%250Athe%2520three%2520sound%2520event%2520detection%2520datasets%2520on%2520which%2520it%2520was%2520tested%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.14517v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=tinyCLAP%3A%20Distilling%20Constrastive%20Language-Audio%20Pretrained%20Models&entry.906535625=Francesco%20Paissan%20and%20Elisabetta%20Farella&entry.1292438233=%20%20Contrastive%20Language-Audio%20Pretraining%20%28CLAP%29%20became%20of%20crucial%20importance%20in%0Athe%20field%20of%20audio%20and%20speech%20processing.%20Its%20employment%20ranges%20from%20sound%0Aevent%20detection%20to%20text-to-audio%20generation.%20However%2C%20one%20of%20the%20main%0Alimitations%20is%20the%20considerable%20amount%20of%20data%20required%20in%20the%20training%20process%0Aand%20the%20overall%20computational%20complexity%20during%20inference.%20This%20paper%0Ainvestigates%20how%20we%20can%20reduce%20the%20complexity%20of%20contrastive%20language-audio%0Apre-trained%20models%2C%20yielding%20an%20efficient%20model%20that%20we%20call%20tinyCLAP.%20We%0Aderive%20an%20unimodal%20distillation%20loss%20from%20first%20principles%20and%20explore%20how%20the%0Adimensionality%20of%20the%20shared%2C%20multimodal%20latent%20space%20can%20be%20reduced%20via%0Apruning.%20TinyCLAP%20uses%20only%206%25%20of%20the%20original%20Microsoft%20CLAP%20parameters%20with%20a%0Aminimal%20reduction%20%28less%20than%205%25%29%20in%20zero-shot%20classification%20performance%20across%0Athe%20three%20sound%20event%20detection%20datasets%20on%20which%20it%20was%20tested%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.14517v2&entry.124074799=Read"},
{"title": "Using Deep Convolutional Neural Networks to Detect Rendered Glitches in\n  Video Games", "author": "Carlos Garcia Ling and Konrad Tollmar and Linus Gisslen", "abstract": "  In this paper, we present a method using Deep Convolutional Neural Networks\n(DCNNs) to detect common glitches in video games. The problem setting consists\nof an image (800x800 RGB) as input to be classified into one of five defined\nclasses, normal image, or one of four different kinds of glitches (stretched,\nlow resolution, missing and placeholder textures). Using a supervised approach,\nwe train a ShuffleNetV2 using generated data. This work focuses on detecting\ntexture graphical anomalies achieving arguably good performance with an\naccuracy of 86.8\\%, detecting 88\\% of the glitches with a false positive rate\nof 8.7\\%, and with the models being able to generalize and detect glitches even\nin unseen objects. We apply a confidence measure as well to tackle the issue\nwith false positives as well as an effective way of aggregating images to\nachieve better detection in production. The main use of this work is the\npartial automatization of graphical testing in the final stages of video game\ndevelopment.\n", "link": "http://arxiv.org/abs/2406.08231v1", "date": "2024-06-12", "relevancy": 2.0409, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5179}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5056}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Deep%20Convolutional%20Neural%20Networks%20to%20Detect%20Rendered%20Glitches%20in%0A%20%20Video%20Games&body=Title%3A%20Using%20Deep%20Convolutional%20Neural%20Networks%20to%20Detect%20Rendered%20Glitches%20in%0A%20%20Video%20Games%0AAuthor%3A%20Carlos%20Garcia%20Ling%20and%20Konrad%20Tollmar%20and%20Linus%20Gisslen%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20method%20using%20Deep%20Convolutional%20Neural%20Networks%0A%28DCNNs%29%20to%20detect%20common%20glitches%20in%20video%20games.%20The%20problem%20setting%20consists%0Aof%20an%20image%20%28800x800%20RGB%29%20as%20input%20to%20be%20classified%20into%20one%20of%20five%20defined%0Aclasses%2C%20normal%20image%2C%20or%20one%20of%20four%20different%20kinds%20of%20glitches%20%28stretched%2C%0Alow%20resolution%2C%20missing%20and%20placeholder%20textures%29.%20Using%20a%20supervised%20approach%2C%0Awe%20train%20a%20ShuffleNetV2%20using%20generated%20data.%20This%20work%20focuses%20on%20detecting%0Atexture%20graphical%20anomalies%20achieving%20arguably%20good%20performance%20with%20an%0Aaccuracy%20of%2086.8%5C%25%2C%20detecting%2088%5C%25%20of%20the%20glitches%20with%20a%20false%20positive%20rate%0Aof%208.7%5C%25%2C%20and%20with%20the%20models%20being%20able%20to%20generalize%20and%20detect%20glitches%20even%0Ain%20unseen%20objects.%20We%20apply%20a%20confidence%20measure%20as%20well%20to%20tackle%20the%20issue%0Awith%20false%20positives%20as%20well%20as%20an%20effective%20way%20of%20aggregating%20images%20to%0Aachieve%20better%20detection%20in%20production.%20The%20main%20use%20of%20this%20work%20is%20the%0Apartial%20automatization%20of%20graphical%20testing%20in%20the%20final%20stages%20of%20video%20game%0Adevelopment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08231v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Deep%2520Convolutional%2520Neural%2520Networks%2520to%2520Detect%2520Rendered%2520Glitches%2520in%250A%2520%2520Video%2520Games%26entry.906535625%3DCarlos%2520Garcia%2520Ling%2520and%2520Konrad%2520Tollmar%2520and%2520Linus%2520Gisslen%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520method%2520using%2520Deep%2520Convolutional%2520Neural%2520Networks%250A%2528DCNNs%2529%2520to%2520detect%2520common%2520glitches%2520in%2520video%2520games.%2520The%2520problem%2520setting%2520consists%250Aof%2520an%2520image%2520%2528800x800%2520RGB%2529%2520as%2520input%2520to%2520be%2520classified%2520into%2520one%2520of%2520five%2520defined%250Aclasses%252C%2520normal%2520image%252C%2520or%2520one%2520of%2520four%2520different%2520kinds%2520of%2520glitches%2520%2528stretched%252C%250Alow%2520resolution%252C%2520missing%2520and%2520placeholder%2520textures%2529.%2520Using%2520a%2520supervised%2520approach%252C%250Awe%2520train%2520a%2520ShuffleNetV2%2520using%2520generated%2520data.%2520This%2520work%2520focuses%2520on%2520detecting%250Atexture%2520graphical%2520anomalies%2520achieving%2520arguably%2520good%2520performance%2520with%2520an%250Aaccuracy%2520of%252086.8%255C%2525%252C%2520detecting%252088%255C%2525%2520of%2520the%2520glitches%2520with%2520a%2520false%2520positive%2520rate%250Aof%25208.7%255C%2525%252C%2520and%2520with%2520the%2520models%2520being%2520able%2520to%2520generalize%2520and%2520detect%2520glitches%2520even%250Ain%2520unseen%2520objects.%2520We%2520apply%2520a%2520confidence%2520measure%2520as%2520well%2520to%2520tackle%2520the%2520issue%250Awith%2520false%2520positives%2520as%2520well%2520as%2520an%2520effective%2520way%2520of%2520aggregating%2520images%2520to%250Aachieve%2520better%2520detection%2520in%2520production.%2520The%2520main%2520use%2520of%2520this%2520work%2520is%2520the%250Apartial%2520automatization%2520of%2520graphical%2520testing%2520in%2520the%2520final%2520stages%2520of%2520video%2520game%250Adevelopment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08231v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Deep%20Convolutional%20Neural%20Networks%20to%20Detect%20Rendered%20Glitches%20in%0A%20%20Video%20Games&entry.906535625=Carlos%20Garcia%20Ling%20and%20Konrad%20Tollmar%20and%20Linus%20Gisslen&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20method%20using%20Deep%20Convolutional%20Neural%20Networks%0A%28DCNNs%29%20to%20detect%20common%20glitches%20in%20video%20games.%20The%20problem%20setting%20consists%0Aof%20an%20image%20%28800x800%20RGB%29%20as%20input%20to%20be%20classified%20into%20one%20of%20five%20defined%0Aclasses%2C%20normal%20image%2C%20or%20one%20of%20four%20different%20kinds%20of%20glitches%20%28stretched%2C%0Alow%20resolution%2C%20missing%20and%20placeholder%20textures%29.%20Using%20a%20supervised%20approach%2C%0Awe%20train%20a%20ShuffleNetV2%20using%20generated%20data.%20This%20work%20focuses%20on%20detecting%0Atexture%20graphical%20anomalies%20achieving%20arguably%20good%20performance%20with%20an%0Aaccuracy%20of%2086.8%5C%25%2C%20detecting%2088%5C%25%20of%20the%20glitches%20with%20a%20false%20positive%20rate%0Aof%208.7%5C%25%2C%20and%20with%20the%20models%20being%20able%20to%20generalize%20and%20detect%20glitches%20even%0Ain%20unseen%20objects.%20We%20apply%20a%20confidence%20measure%20as%20well%20to%20tackle%20the%20issue%0Awith%20false%20positives%20as%20well%20as%20an%20effective%20way%20of%20aggregating%20images%20to%0Aachieve%20better%20detection%20in%20production.%20The%20main%20use%20of%20this%20work%20is%20the%0Apartial%20automatization%20of%20graphical%20testing%20in%20the%20final%20stages%20of%20video%20game%0Adevelopment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08231v1&entry.124074799=Read"},
{"title": "DistilDoc: Knowledge Distillation for Visually-Rich Document\n  Applications", "author": "Jordy Van Landeghem and Subhajit Maity and Ayan Banerjee and Matthew Blaschko and Marie-Francine Moens and Josep Llad\u00f3s and Sanket Biswas", "abstract": "  This work explores knowledge distillation (KD) for visually-rich document\n(VRD) applications such as document layout analysis (DLA) and document image\nclassification (DIC). While VRD research is dependent on increasingly\nsophisticated and cumbersome models, the field has neglected to study\nefficiency via model compression. Here, we design a KD experimentation\nmethodology for more lean, performant models on document understanding (DU)\ntasks that are integral within larger task pipelines. We carefully selected KD\nstrategies (response-based, feature-based) for distilling knowledge to and from\nbackbones with different architectures (ResNet, ViT, DiT) and capacities (base,\nsmall, tiny). We study what affects the teacher-student knowledge gap and find\nthat some methods (tuned vanilla KD, MSE, SimKD with an apt projector) can\nconsistently outperform supervised student training. Furthermore, we design\ndownstream task setups to evaluate covariate shift and the robustness of\ndistilled DLA models on zero-shot layout-aware document visual question\nanswering (DocVQA). DLA-KD experiments result in a large mAP knowledge gap,\nwhich unpredictably translates to downstream robustness, accentuating the need\nto further explore how to efficiently obtain more semantic document layout\nawareness.\n", "link": "http://arxiv.org/abs/2406.08226v1", "date": "2024-06-12", "relevancy": 2.0342, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.524}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5085}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5024}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DistilDoc%3A%20Knowledge%20Distillation%20for%20Visually-Rich%20Document%0A%20%20Applications&body=Title%3A%20DistilDoc%3A%20Knowledge%20Distillation%20for%20Visually-Rich%20Document%0A%20%20Applications%0AAuthor%3A%20Jordy%20Van%20Landeghem%20and%20Subhajit%20Maity%20and%20Ayan%20Banerjee%20and%20Matthew%20Blaschko%20and%20Marie-Francine%20Moens%20and%20Josep%20Llad%C3%B3s%20and%20Sanket%20Biswas%0AAbstract%3A%20%20%20This%20work%20explores%20knowledge%20distillation%20%28KD%29%20for%20visually-rich%20document%0A%28VRD%29%20applications%20such%20as%20document%20layout%20analysis%20%28DLA%29%20and%20document%20image%0Aclassification%20%28DIC%29.%20While%20VRD%20research%20is%20dependent%20on%20increasingly%0Asophisticated%20and%20cumbersome%20models%2C%20the%20field%20has%20neglected%20to%20study%0Aefficiency%20via%20model%20compression.%20Here%2C%20we%20design%20a%20KD%20experimentation%0Amethodology%20for%20more%20lean%2C%20performant%20models%20on%20document%20understanding%20%28DU%29%0Atasks%20that%20are%20integral%20within%20larger%20task%20pipelines.%20We%20carefully%20selected%20KD%0Astrategies%20%28response-based%2C%20feature-based%29%20for%20distilling%20knowledge%20to%20and%20from%0Abackbones%20with%20different%20architectures%20%28ResNet%2C%20ViT%2C%20DiT%29%20and%20capacities%20%28base%2C%0Asmall%2C%20tiny%29.%20We%20study%20what%20affects%20the%20teacher-student%20knowledge%20gap%20and%20find%0Athat%20some%20methods%20%28tuned%20vanilla%20KD%2C%20MSE%2C%20SimKD%20with%20an%20apt%20projector%29%20can%0Aconsistently%20outperform%20supervised%20student%20training.%20Furthermore%2C%20we%20design%0Adownstream%20task%20setups%20to%20evaluate%20covariate%20shift%20and%20the%20robustness%20of%0Adistilled%20DLA%20models%20on%20zero-shot%20layout-aware%20document%20visual%20question%0Aanswering%20%28DocVQA%29.%20DLA-KD%20experiments%20result%20in%20a%20large%20mAP%20knowledge%20gap%2C%0Awhich%20unpredictably%20translates%20to%20downstream%20robustness%2C%20accentuating%20the%20need%0Ato%20further%20explore%20how%20to%20efficiently%20obtain%20more%20semantic%20document%20layout%0Aawareness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08226v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistilDoc%253A%2520Knowledge%2520Distillation%2520for%2520Visually-Rich%2520Document%250A%2520%2520Applications%26entry.906535625%3DJordy%2520Van%2520Landeghem%2520and%2520Subhajit%2520Maity%2520and%2520Ayan%2520Banerjee%2520and%2520Matthew%2520Blaschko%2520and%2520Marie-Francine%2520Moens%2520and%2520Josep%2520Llad%25C3%25B3s%2520and%2520Sanket%2520Biswas%26entry.1292438233%3D%2520%2520This%2520work%2520explores%2520knowledge%2520distillation%2520%2528KD%2529%2520for%2520visually-rich%2520document%250A%2528VRD%2529%2520applications%2520such%2520as%2520document%2520layout%2520analysis%2520%2528DLA%2529%2520and%2520document%2520image%250Aclassification%2520%2528DIC%2529.%2520While%2520VRD%2520research%2520is%2520dependent%2520on%2520increasingly%250Asophisticated%2520and%2520cumbersome%2520models%252C%2520the%2520field%2520has%2520neglected%2520to%2520study%250Aefficiency%2520via%2520model%2520compression.%2520Here%252C%2520we%2520design%2520a%2520KD%2520experimentation%250Amethodology%2520for%2520more%2520lean%252C%2520performant%2520models%2520on%2520document%2520understanding%2520%2528DU%2529%250Atasks%2520that%2520are%2520integral%2520within%2520larger%2520task%2520pipelines.%2520We%2520carefully%2520selected%2520KD%250Astrategies%2520%2528response-based%252C%2520feature-based%2529%2520for%2520distilling%2520knowledge%2520to%2520and%2520from%250Abackbones%2520with%2520different%2520architectures%2520%2528ResNet%252C%2520ViT%252C%2520DiT%2529%2520and%2520capacities%2520%2528base%252C%250Asmall%252C%2520tiny%2529.%2520We%2520study%2520what%2520affects%2520the%2520teacher-student%2520knowledge%2520gap%2520and%2520find%250Athat%2520some%2520methods%2520%2528tuned%2520vanilla%2520KD%252C%2520MSE%252C%2520SimKD%2520with%2520an%2520apt%2520projector%2529%2520can%250Aconsistently%2520outperform%2520supervised%2520student%2520training.%2520Furthermore%252C%2520we%2520design%250Adownstream%2520task%2520setups%2520to%2520evaluate%2520covariate%2520shift%2520and%2520the%2520robustness%2520of%250Adistilled%2520DLA%2520models%2520on%2520zero-shot%2520layout-aware%2520document%2520visual%2520question%250Aanswering%2520%2528DocVQA%2529.%2520DLA-KD%2520experiments%2520result%2520in%2520a%2520large%2520mAP%2520knowledge%2520gap%252C%250Awhich%2520unpredictably%2520translates%2520to%2520downstream%2520robustness%252C%2520accentuating%2520the%2520need%250Ato%2520further%2520explore%2520how%2520to%2520efficiently%2520obtain%2520more%2520semantic%2520document%2520layout%250Aawareness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08226v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DistilDoc%3A%20Knowledge%20Distillation%20for%20Visually-Rich%20Document%0A%20%20Applications&entry.906535625=Jordy%20Van%20Landeghem%20and%20Subhajit%20Maity%20and%20Ayan%20Banerjee%20and%20Matthew%20Blaschko%20and%20Marie-Francine%20Moens%20and%20Josep%20Llad%C3%B3s%20and%20Sanket%20Biswas&entry.1292438233=%20%20This%20work%20explores%20knowledge%20distillation%20%28KD%29%20for%20visually-rich%20document%0A%28VRD%29%20applications%20such%20as%20document%20layout%20analysis%20%28DLA%29%20and%20document%20image%0Aclassification%20%28DIC%29.%20While%20VRD%20research%20is%20dependent%20on%20increasingly%0Asophisticated%20and%20cumbersome%20models%2C%20the%20field%20has%20neglected%20to%20study%0Aefficiency%20via%20model%20compression.%20Here%2C%20we%20design%20a%20KD%20experimentation%0Amethodology%20for%20more%20lean%2C%20performant%20models%20on%20document%20understanding%20%28DU%29%0Atasks%20that%20are%20integral%20within%20larger%20task%20pipelines.%20We%20carefully%20selected%20KD%0Astrategies%20%28response-based%2C%20feature-based%29%20for%20distilling%20knowledge%20to%20and%20from%0Abackbones%20with%20different%20architectures%20%28ResNet%2C%20ViT%2C%20DiT%29%20and%20capacities%20%28base%2C%0Asmall%2C%20tiny%29.%20We%20study%20what%20affects%20the%20teacher-student%20knowledge%20gap%20and%20find%0Athat%20some%20methods%20%28tuned%20vanilla%20KD%2C%20MSE%2C%20SimKD%20with%20an%20apt%20projector%29%20can%0Aconsistently%20outperform%20supervised%20student%20training.%20Furthermore%2C%20we%20design%0Adownstream%20task%20setups%20to%20evaluate%20covariate%20shift%20and%20the%20robustness%20of%0Adistilled%20DLA%20models%20on%20zero-shot%20layout-aware%20document%20visual%20question%0Aanswering%20%28DocVQA%29.%20DLA-KD%20experiments%20result%20in%20a%20large%20mAP%20knowledge%20gap%2C%0Awhich%20unpredictably%20translates%20to%20downstream%20robustness%2C%20accentuating%20the%20need%0Ato%20further%20explore%20how%20to%20efficiently%20obtain%20more%20semantic%20document%20layout%0Aawareness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08226v1&entry.124074799=Read"},
{"title": "Valeo4Cast: A Modular Approach to End-to-End Forecasting", "author": "Yihong Xu and \u00c9loi Zablocki and Alexandre Boulch and Gilles Puy and Mickael Chen and Florent Bartoccioni and Nermin Samet and Oriane Sim\u00e9oni and Spyros Gidaris and Tuan-Hung Vu and Andrei Bursuc and Eduardo Valle and Renaud Marlet and Matthieu Cord", "abstract": "  Motion forecasting is crucial in autonomous driving systems to anticipate the\nfuture trajectories of surrounding agents such as pedestrians, vehicles, and\ntraffic signals. In end-to-end forecasting, the model must jointly detect from\nsensor data (cameras or LiDARs) the position and past trajectories of the\ndifferent elements of the scene and predict their future location. We depart\nfrom the current trend of tackling this task via end-to-end training from\nperception to forecasting and we use a modular approach instead. Following a\nrecent study, we individually build and train detection, tracking, and\nforecasting modules. We then only use consecutive finetuning steps to integrate\nthe modules better and alleviate compounding errors. Our study reveals that\nthis simple yet effective approach significantly improves performance on the\nend-to-end forecasting benchmark. Consequently, our solution ranks first in the\nArgoverse 2 end-to-end Forecasting Challenge held at CVPR 2024 Workshop on\nAutonomous Driving (WAD), with 63.82 mAPf. We surpass forecasting results by\n+17.1 points over last year's winner and by +13.3 points over this year's\nrunner-up. This remarkable performance in forecasting can be explained by our\nmodular paradigm, which integrates finetuning strategies and significantly\noutperforms the end-to-end-trained counterparts.\n", "link": "http://arxiv.org/abs/2406.08113v1", "date": "2024-06-12", "relevancy": 2.0283, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5377}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5138}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Valeo4Cast%3A%20A%20Modular%20Approach%20to%20End-to-End%20Forecasting&body=Title%3A%20Valeo4Cast%3A%20A%20Modular%20Approach%20to%20End-to-End%20Forecasting%0AAuthor%3A%20Yihong%20Xu%20and%20%C3%89loi%20Zablocki%20and%20Alexandre%20Boulch%20and%20Gilles%20Puy%20and%20Mickael%20Chen%20and%20Florent%20Bartoccioni%20and%20Nermin%20Samet%20and%20Oriane%20Sim%C3%A9oni%20and%20Spyros%20Gidaris%20and%20Tuan-Hung%20Vu%20and%20Andrei%20Bursuc%20and%20Eduardo%20Valle%20and%20Renaud%20Marlet%20and%20Matthieu%20Cord%0AAbstract%3A%20%20%20Motion%20forecasting%20is%20crucial%20in%20autonomous%20driving%20systems%20to%20anticipate%20the%0Afuture%20trajectories%20of%20surrounding%20agents%20such%20as%20pedestrians%2C%20vehicles%2C%20and%0Atraffic%20signals.%20In%20end-to-end%20forecasting%2C%20the%20model%20must%20jointly%20detect%20from%0Asensor%20data%20%28cameras%20or%20LiDARs%29%20the%20position%20and%20past%20trajectories%20of%20the%0Adifferent%20elements%20of%20the%20scene%20and%20predict%20their%20future%20location.%20We%20depart%0Afrom%20the%20current%20trend%20of%20tackling%20this%20task%20via%20end-to-end%20training%20from%0Aperception%20to%20forecasting%20and%20we%20use%20a%20modular%20approach%20instead.%20Following%20a%0Arecent%20study%2C%20we%20individually%20build%20and%20train%20detection%2C%20tracking%2C%20and%0Aforecasting%20modules.%20We%20then%20only%20use%20consecutive%20finetuning%20steps%20to%20integrate%0Athe%20modules%20better%20and%20alleviate%20compounding%20errors.%20Our%20study%20reveals%20that%0Athis%20simple%20yet%20effective%20approach%20significantly%20improves%20performance%20on%20the%0Aend-to-end%20forecasting%20benchmark.%20Consequently%2C%20our%20solution%20ranks%20first%20in%20the%0AArgoverse%202%20end-to-end%20Forecasting%20Challenge%20held%20at%20CVPR%202024%20Workshop%20on%0AAutonomous%20Driving%20%28WAD%29%2C%20with%2063.82%20mAPf.%20We%20surpass%20forecasting%20results%20by%0A%2B17.1%20points%20over%20last%20year%27s%20winner%20and%20by%20%2B13.3%20points%20over%20this%20year%27s%0Arunner-up.%20This%20remarkable%20performance%20in%20forecasting%20can%20be%20explained%20by%20our%0Amodular%20paradigm%2C%20which%20integrates%20finetuning%20strategies%20and%20significantly%0Aoutperforms%20the%20end-to-end-trained%20counterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08113v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DValeo4Cast%253A%2520A%2520Modular%2520Approach%2520to%2520End-to-End%2520Forecasting%26entry.906535625%3DYihong%2520Xu%2520and%2520%25C3%2589loi%2520Zablocki%2520and%2520Alexandre%2520Boulch%2520and%2520Gilles%2520Puy%2520and%2520Mickael%2520Chen%2520and%2520Florent%2520Bartoccioni%2520and%2520Nermin%2520Samet%2520and%2520Oriane%2520Sim%25C3%25A9oni%2520and%2520Spyros%2520Gidaris%2520and%2520Tuan-Hung%2520Vu%2520and%2520Andrei%2520Bursuc%2520and%2520Eduardo%2520Valle%2520and%2520Renaud%2520Marlet%2520and%2520Matthieu%2520Cord%26entry.1292438233%3D%2520%2520Motion%2520forecasting%2520is%2520crucial%2520in%2520autonomous%2520driving%2520systems%2520to%2520anticipate%2520the%250Afuture%2520trajectories%2520of%2520surrounding%2520agents%2520such%2520as%2520pedestrians%252C%2520vehicles%252C%2520and%250Atraffic%2520signals.%2520In%2520end-to-end%2520forecasting%252C%2520the%2520model%2520must%2520jointly%2520detect%2520from%250Asensor%2520data%2520%2528cameras%2520or%2520LiDARs%2529%2520the%2520position%2520and%2520past%2520trajectories%2520of%2520the%250Adifferent%2520elements%2520of%2520the%2520scene%2520and%2520predict%2520their%2520future%2520location.%2520We%2520depart%250Afrom%2520the%2520current%2520trend%2520of%2520tackling%2520this%2520task%2520via%2520end-to-end%2520training%2520from%250Aperception%2520to%2520forecasting%2520and%2520we%2520use%2520a%2520modular%2520approach%2520instead.%2520Following%2520a%250Arecent%2520study%252C%2520we%2520individually%2520build%2520and%2520train%2520detection%252C%2520tracking%252C%2520and%250Aforecasting%2520modules.%2520We%2520then%2520only%2520use%2520consecutive%2520finetuning%2520steps%2520to%2520integrate%250Athe%2520modules%2520better%2520and%2520alleviate%2520compounding%2520errors.%2520Our%2520study%2520reveals%2520that%250Athis%2520simple%2520yet%2520effective%2520approach%2520significantly%2520improves%2520performance%2520on%2520the%250Aend-to-end%2520forecasting%2520benchmark.%2520Consequently%252C%2520our%2520solution%2520ranks%2520first%2520in%2520the%250AArgoverse%25202%2520end-to-end%2520Forecasting%2520Challenge%2520held%2520at%2520CVPR%25202024%2520Workshop%2520on%250AAutonomous%2520Driving%2520%2528WAD%2529%252C%2520with%252063.82%2520mAPf.%2520We%2520surpass%2520forecasting%2520results%2520by%250A%252B17.1%2520points%2520over%2520last%2520year%2527s%2520winner%2520and%2520by%2520%252B13.3%2520points%2520over%2520this%2520year%2527s%250Arunner-up.%2520This%2520remarkable%2520performance%2520in%2520forecasting%2520can%2520be%2520explained%2520by%2520our%250Amodular%2520paradigm%252C%2520which%2520integrates%2520finetuning%2520strategies%2520and%2520significantly%250Aoutperforms%2520the%2520end-to-end-trained%2520counterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08113v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Valeo4Cast%3A%20A%20Modular%20Approach%20to%20End-to-End%20Forecasting&entry.906535625=Yihong%20Xu%20and%20%C3%89loi%20Zablocki%20and%20Alexandre%20Boulch%20and%20Gilles%20Puy%20and%20Mickael%20Chen%20and%20Florent%20Bartoccioni%20and%20Nermin%20Samet%20and%20Oriane%20Sim%C3%A9oni%20and%20Spyros%20Gidaris%20and%20Tuan-Hung%20Vu%20and%20Andrei%20Bursuc%20and%20Eduardo%20Valle%20and%20Renaud%20Marlet%20and%20Matthieu%20Cord&entry.1292438233=%20%20Motion%20forecasting%20is%20crucial%20in%20autonomous%20driving%20systems%20to%20anticipate%20the%0Afuture%20trajectories%20of%20surrounding%20agents%20such%20as%20pedestrians%2C%20vehicles%2C%20and%0Atraffic%20signals.%20In%20end-to-end%20forecasting%2C%20the%20model%20must%20jointly%20detect%20from%0Asensor%20data%20%28cameras%20or%20LiDARs%29%20the%20position%20and%20past%20trajectories%20of%20the%0Adifferent%20elements%20of%20the%20scene%20and%20predict%20their%20future%20location.%20We%20depart%0Afrom%20the%20current%20trend%20of%20tackling%20this%20task%20via%20end-to-end%20training%20from%0Aperception%20to%20forecasting%20and%20we%20use%20a%20modular%20approach%20instead.%20Following%20a%0Arecent%20study%2C%20we%20individually%20build%20and%20train%20detection%2C%20tracking%2C%20and%0Aforecasting%20modules.%20We%20then%20only%20use%20consecutive%20finetuning%20steps%20to%20integrate%0Athe%20modules%20better%20and%20alleviate%20compounding%20errors.%20Our%20study%20reveals%20that%0Athis%20simple%20yet%20effective%20approach%20significantly%20improves%20performance%20on%20the%0Aend-to-end%20forecasting%20benchmark.%20Consequently%2C%20our%20solution%20ranks%20first%20in%20the%0AArgoverse%202%20end-to-end%20Forecasting%20Challenge%20held%20at%20CVPR%202024%20Workshop%20on%0AAutonomous%20Driving%20%28WAD%29%2C%20with%2063.82%20mAPf.%20We%20surpass%20forecasting%20results%20by%0A%2B17.1%20points%20over%20last%20year%27s%20winner%20and%20by%20%2B13.3%20points%20over%20this%20year%27s%0Arunner-up.%20This%20remarkable%20performance%20in%20forecasting%20can%20be%20explained%20by%20our%0Amodular%20paradigm%2C%20which%20integrates%20finetuning%20strategies%20and%20significantly%0Aoutperforms%20the%20end-to-end-trained%20counterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08113v1&entry.124074799=Read"},
{"title": "Runtime Freezing: Dynamic Class Loss for Multi-Organ 3D Segmentation", "author": "James Willoughby and Irina Voiculescu", "abstract": "  Segmentation has become a crucial pre-processing step to many refined\ndownstream tasks, and particularly so in the medical domain. Even with recent\nimprovements in segmentation models, many segmentation tasks remain difficult.\nWhen multiple organs are segmented simultaneously, difficulties are due not\nonly to the limited availability of labelled data, but also to class imbalance.\nIn this work we propose dynamic class-based loss strategies to mitigate the\neffects of highly imbalanced training data. We show how our approach improves\nsegmentation performance on a challenging Multi-Class 3D Abdominal Organ\ndataset.\n", "link": "http://arxiv.org/abs/2406.08217v1", "date": "2024-06-12", "relevancy": 2.0086, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5158}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5058}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Runtime%20Freezing%3A%20Dynamic%20Class%20Loss%20for%20Multi-Organ%203D%20Segmentation&body=Title%3A%20Runtime%20Freezing%3A%20Dynamic%20Class%20Loss%20for%20Multi-Organ%203D%20Segmentation%0AAuthor%3A%20James%20Willoughby%20and%20Irina%20Voiculescu%0AAbstract%3A%20%20%20Segmentation%20has%20become%20a%20crucial%20pre-processing%20step%20to%20many%20refined%0Adownstream%20tasks%2C%20and%20particularly%20so%20in%20the%20medical%20domain.%20Even%20with%20recent%0Aimprovements%20in%20segmentation%20models%2C%20many%20segmentation%20tasks%20remain%20difficult.%0AWhen%20multiple%20organs%20are%20segmented%20simultaneously%2C%20difficulties%20are%20due%20not%0Aonly%20to%20the%20limited%20availability%20of%20labelled%20data%2C%20but%20also%20to%20class%20imbalance.%0AIn%20this%20work%20we%20propose%20dynamic%20class-based%20loss%20strategies%20to%20mitigate%20the%0Aeffects%20of%20highly%20imbalanced%20training%20data.%20We%20show%20how%20our%20approach%20improves%0Asegmentation%20performance%20on%20a%20challenging%20Multi-Class%203D%20Abdominal%20Organ%0Adataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08217v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRuntime%2520Freezing%253A%2520Dynamic%2520Class%2520Loss%2520for%2520Multi-Organ%25203D%2520Segmentation%26entry.906535625%3DJames%2520Willoughby%2520and%2520Irina%2520Voiculescu%26entry.1292438233%3D%2520%2520Segmentation%2520has%2520become%2520a%2520crucial%2520pre-processing%2520step%2520to%2520many%2520refined%250Adownstream%2520tasks%252C%2520and%2520particularly%2520so%2520in%2520the%2520medical%2520domain.%2520Even%2520with%2520recent%250Aimprovements%2520in%2520segmentation%2520models%252C%2520many%2520segmentation%2520tasks%2520remain%2520difficult.%250AWhen%2520multiple%2520organs%2520are%2520segmented%2520simultaneously%252C%2520difficulties%2520are%2520due%2520not%250Aonly%2520to%2520the%2520limited%2520availability%2520of%2520labelled%2520data%252C%2520but%2520also%2520to%2520class%2520imbalance.%250AIn%2520this%2520work%2520we%2520propose%2520dynamic%2520class-based%2520loss%2520strategies%2520to%2520mitigate%2520the%250Aeffects%2520of%2520highly%2520imbalanced%2520training%2520data.%2520We%2520show%2520how%2520our%2520approach%2520improves%250Asegmentation%2520performance%2520on%2520a%2520challenging%2520Multi-Class%25203D%2520Abdominal%2520Organ%250Adataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08217v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Runtime%20Freezing%3A%20Dynamic%20Class%20Loss%20for%20Multi-Organ%203D%20Segmentation&entry.906535625=James%20Willoughby%20and%20Irina%20Voiculescu&entry.1292438233=%20%20Segmentation%20has%20become%20a%20crucial%20pre-processing%20step%20to%20many%20refined%0Adownstream%20tasks%2C%20and%20particularly%20so%20in%20the%20medical%20domain.%20Even%20with%20recent%0Aimprovements%20in%20segmentation%20models%2C%20many%20segmentation%20tasks%20remain%20difficult.%0AWhen%20multiple%20organs%20are%20segmented%20simultaneously%2C%20difficulties%20are%20due%20not%0Aonly%20to%20the%20limited%20availability%20of%20labelled%20data%2C%20but%20also%20to%20class%20imbalance.%0AIn%20this%20work%20we%20propose%20dynamic%20class-based%20loss%20strategies%20to%20mitigate%20the%0Aeffects%20of%20highly%20imbalanced%20training%20data.%20We%20show%20how%20our%20approach%20improves%0Asegmentation%20performance%20on%20a%20challenging%20Multi-Class%203D%20Abdominal%20Organ%0Adataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08217v1&entry.124074799=Read"},
{"title": "Design, modeling, and characteristics of ringshaped robot actuated by\n  functional fluid", "author": "Zebing Mao and Xuehang Bai and Yanhong Peng and Yayi Shen", "abstract": "  The controlled actuation of hydraulic and pneumatic actuators has unveiled\nfresh and thrilling opportunities for designing mobile robots with adaptable\nstructures. Previously reported rolling robots, which were powered by fluidic\nsystems, often relied on complex principles, cumbersome pump and valve systems,\nand intricate control strategies, limiting their applicability in other fields.\nIn this investigation, we employed a distinct category of functional fluid\nidentified as Electrohydrodynamic (EHD) fluid, serving as the pivotal element\nwithin the ring-shaped actuator. A short stream of functional fluid is placed\nwithin a fluidic channel and is then actuated by applying a direct current\nvoltage aiming at shifting the center of mass of the robot and finally pushed\nthe actuator to roll. We designed a ring-shaped fluidic robot, manufactured it\nusing digital machining methods, and evaluated the robot's characteristics.\nFurthermore, we developed static and dynamic models to analyze the oscillation\nand rolling motion of the ring-shaped robots using the Lagrange method. This\nstudy is anticipated to contribute to the expansion of current research on EHD\nflexible actuators, enabling the realization of complex robotic systems.\n", "link": "http://arxiv.org/abs/2406.08135v1", "date": "2024-06-12", "relevancy": 0.8589, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.446}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4234}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Design%2C%20modeling%2C%20and%20characteristics%20of%20ringshaped%20robot%20actuated%20by%0A%20%20functional%20fluid&body=Title%3A%20Design%2C%20modeling%2C%20and%20characteristics%20of%20ringshaped%20robot%20actuated%20by%0A%20%20functional%20fluid%0AAuthor%3A%20Zebing%20Mao%20and%20Xuehang%20Bai%20and%20Yanhong%20Peng%20and%20Yayi%20Shen%0AAbstract%3A%20%20%20The%20controlled%20actuation%20of%20hydraulic%20and%20pneumatic%20actuators%20has%20unveiled%0Afresh%20and%20thrilling%20opportunities%20for%20designing%20mobile%20robots%20with%20adaptable%0Astructures.%20Previously%20reported%20rolling%20robots%2C%20which%20were%20powered%20by%20fluidic%0Asystems%2C%20often%20relied%20on%20complex%20principles%2C%20cumbersome%20pump%20and%20valve%20systems%2C%0Aand%20intricate%20control%20strategies%2C%20limiting%20their%20applicability%20in%20other%20fields.%0AIn%20this%20investigation%2C%20we%20employed%20a%20distinct%20category%20of%20functional%20fluid%0Aidentified%20as%20Electrohydrodynamic%20%28EHD%29%20fluid%2C%20serving%20as%20the%20pivotal%20element%0Awithin%20the%20ring-shaped%20actuator.%20A%20short%20stream%20of%20functional%20fluid%20is%20placed%0Awithin%20a%20fluidic%20channel%20and%20is%20then%20actuated%20by%20applying%20a%20direct%20current%0Avoltage%20aiming%20at%20shifting%20the%20center%20of%20mass%20of%20the%20robot%20and%20finally%20pushed%0Athe%20actuator%20to%20roll.%20We%20designed%20a%20ring-shaped%20fluidic%20robot%2C%20manufactured%20it%0Ausing%20digital%20machining%20methods%2C%20and%20evaluated%20the%20robot%27s%20characteristics.%0AFurthermore%2C%20we%20developed%20static%20and%20dynamic%20models%20to%20analyze%20the%20oscillation%0Aand%20rolling%20motion%20of%20the%20ring-shaped%20robots%20using%20the%20Lagrange%20method.%20This%0Astudy%20is%20anticipated%20to%20contribute%20to%20the%20expansion%20of%20current%20research%20on%20EHD%0Aflexible%20actuators%2C%20enabling%20the%20realization%20of%20complex%20robotic%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08135v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDesign%252C%2520modeling%252C%2520and%2520characteristics%2520of%2520ringshaped%2520robot%2520actuated%2520by%250A%2520%2520functional%2520fluid%26entry.906535625%3DZebing%2520Mao%2520and%2520Xuehang%2520Bai%2520and%2520Yanhong%2520Peng%2520and%2520Yayi%2520Shen%26entry.1292438233%3D%2520%2520The%2520controlled%2520actuation%2520of%2520hydraulic%2520and%2520pneumatic%2520actuators%2520has%2520unveiled%250Afresh%2520and%2520thrilling%2520opportunities%2520for%2520designing%2520mobile%2520robots%2520with%2520adaptable%250Astructures.%2520Previously%2520reported%2520rolling%2520robots%252C%2520which%2520were%2520powered%2520by%2520fluidic%250Asystems%252C%2520often%2520relied%2520on%2520complex%2520principles%252C%2520cumbersome%2520pump%2520and%2520valve%2520systems%252C%250Aand%2520intricate%2520control%2520strategies%252C%2520limiting%2520their%2520applicability%2520in%2520other%2520fields.%250AIn%2520this%2520investigation%252C%2520we%2520employed%2520a%2520distinct%2520category%2520of%2520functional%2520fluid%250Aidentified%2520as%2520Electrohydrodynamic%2520%2528EHD%2529%2520fluid%252C%2520serving%2520as%2520the%2520pivotal%2520element%250Awithin%2520the%2520ring-shaped%2520actuator.%2520A%2520short%2520stream%2520of%2520functional%2520fluid%2520is%2520placed%250Awithin%2520a%2520fluidic%2520channel%2520and%2520is%2520then%2520actuated%2520by%2520applying%2520a%2520direct%2520current%250Avoltage%2520aiming%2520at%2520shifting%2520the%2520center%2520of%2520mass%2520of%2520the%2520robot%2520and%2520finally%2520pushed%250Athe%2520actuator%2520to%2520roll.%2520We%2520designed%2520a%2520ring-shaped%2520fluidic%2520robot%252C%2520manufactured%2520it%250Ausing%2520digital%2520machining%2520methods%252C%2520and%2520evaluated%2520the%2520robot%2527s%2520characteristics.%250AFurthermore%252C%2520we%2520developed%2520static%2520and%2520dynamic%2520models%2520to%2520analyze%2520the%2520oscillation%250Aand%2520rolling%2520motion%2520of%2520the%2520ring-shaped%2520robots%2520using%2520the%2520Lagrange%2520method.%2520This%250Astudy%2520is%2520anticipated%2520to%2520contribute%2520to%2520the%2520expansion%2520of%2520current%2520research%2520on%2520EHD%250Aflexible%2520actuators%252C%2520enabling%2520the%2520realization%2520of%2520complex%2520robotic%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08135v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Design%2C%20modeling%2C%20and%20characteristics%20of%20ringshaped%20robot%20actuated%20by%0A%20%20functional%20fluid&entry.906535625=Zebing%20Mao%20and%20Xuehang%20Bai%20and%20Yanhong%20Peng%20and%20Yayi%20Shen&entry.1292438233=%20%20The%20controlled%20actuation%20of%20hydraulic%20and%20pneumatic%20actuators%20has%20unveiled%0Afresh%20and%20thrilling%20opportunities%20for%20designing%20mobile%20robots%20with%20adaptable%0Astructures.%20Previously%20reported%20rolling%20robots%2C%20which%20were%20powered%20by%20fluidic%0Asystems%2C%20often%20relied%20on%20complex%20principles%2C%20cumbersome%20pump%20and%20valve%20systems%2C%0Aand%20intricate%20control%20strategies%2C%20limiting%20their%20applicability%20in%20other%20fields.%0AIn%20this%20investigation%2C%20we%20employed%20a%20distinct%20category%20of%20functional%20fluid%0Aidentified%20as%20Electrohydrodynamic%20%28EHD%29%20fluid%2C%20serving%20as%20the%20pivotal%20element%0Awithin%20the%20ring-shaped%20actuator.%20A%20short%20stream%20of%20functional%20fluid%20is%20placed%0Awithin%20a%20fluidic%20channel%20and%20is%20then%20actuated%20by%20applying%20a%20direct%20current%0Avoltage%20aiming%20at%20shifting%20the%20center%20of%20mass%20of%20the%20robot%20and%20finally%20pushed%0Athe%20actuator%20to%20roll.%20We%20designed%20a%20ring-shaped%20fluidic%20robot%2C%20manufactured%20it%0Ausing%20digital%20machining%20methods%2C%20and%20evaluated%20the%20robot%27s%20characteristics.%0AFurthermore%2C%20we%20developed%20static%20and%20dynamic%20models%20to%20analyze%20the%20oscillation%0Aand%20rolling%20motion%20of%20the%20ring-shaped%20robots%20using%20the%20Lagrange%20method.%20This%0Astudy%20is%20anticipated%20to%20contribute%20to%20the%20expansion%20of%20current%20research%20on%20EHD%0Aflexible%20actuators%2C%20enabling%20the%20realization%20of%20complex%20robotic%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08135v1&entry.124074799=Read"},
{"title": "Sources of Gain: Decomposing Performance in Conditional Average Dose\n  Response Estimation", "author": "Christopher Bockel-Rickermann and Toon Vanderschueren and Tim Verdonck and Wouter Verbeke", "abstract": "  Estimating conditional average dose responses (CADR) is an important but\nchallenging problem. Estimators must correctly model the potentially complex\nrelationships between covariates, interventions, doses, and outcomes. In recent\nyears, the machine learning community has shown great interest in developing\ntailored CADR estimators that target specific challenges. Their performance is\ntypically evaluated against other methods on (semi-) synthetic benchmark\ndatasets. Our paper analyses this practice and shows that using popular\nbenchmark datasets without further analysis is insufficient to judge model\nperformance. Established benchmarks entail multiple challenges, whose impacts\nmust be disentangled. Therefore, we propose a novel decomposition scheme that\nallows the evaluation of the impact of five distinct components contributing to\nCADR estimator performance. We apply this scheme to eight popular CADR\nestimators on four widely-used benchmark datasets, running nearly 1,500\nindividual experiments. Our results reveal that most established benchmarks are\nchallenging for reasons different from their creators' claims. Notably,\nconfounding, the key challenge tackled by most estimators, is not an issue in\nany of the considered datasets. We discuss the major implications of our\nfindings and present directions for future research.\n", "link": "http://arxiv.org/abs/2406.08206v1", "date": "2024-06-12", "relevancy": 1.3738, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4628}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4545}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sources%20of%20Gain%3A%20Decomposing%20Performance%20in%20Conditional%20Average%20Dose%0A%20%20Response%20Estimation&body=Title%3A%20Sources%20of%20Gain%3A%20Decomposing%20Performance%20in%20Conditional%20Average%20Dose%0A%20%20Response%20Estimation%0AAuthor%3A%20Christopher%20Bockel-Rickermann%20and%20Toon%20Vanderschueren%20and%20Tim%20Verdonck%20and%20Wouter%20Verbeke%0AAbstract%3A%20%20%20Estimating%20conditional%20average%20dose%20responses%20%28CADR%29%20is%20an%20important%20but%0Achallenging%20problem.%20Estimators%20must%20correctly%20model%20the%20potentially%20complex%0Arelationships%20between%20covariates%2C%20interventions%2C%20doses%2C%20and%20outcomes.%20In%20recent%0Ayears%2C%20the%20machine%20learning%20community%20has%20shown%20great%20interest%20in%20developing%0Atailored%20CADR%20estimators%20that%20target%20specific%20challenges.%20Their%20performance%20is%0Atypically%20evaluated%20against%20other%20methods%20on%20%28semi-%29%20synthetic%20benchmark%0Adatasets.%20Our%20paper%20analyses%20this%20practice%20and%20shows%20that%20using%20popular%0Abenchmark%20datasets%20without%20further%20analysis%20is%20insufficient%20to%20judge%20model%0Aperformance.%20Established%20benchmarks%20entail%20multiple%20challenges%2C%20whose%20impacts%0Amust%20be%20disentangled.%20Therefore%2C%20we%20propose%20a%20novel%20decomposition%20scheme%20that%0Aallows%20the%20evaluation%20of%20the%20impact%20of%20five%20distinct%20components%20contributing%20to%0ACADR%20estimator%20performance.%20We%20apply%20this%20scheme%20to%20eight%20popular%20CADR%0Aestimators%20on%20four%20widely-used%20benchmark%20datasets%2C%20running%20nearly%201%2C500%0Aindividual%20experiments.%20Our%20results%20reveal%20that%20most%20established%20benchmarks%20are%0Achallenging%20for%20reasons%20different%20from%20their%20creators%27%20claims.%20Notably%2C%0Aconfounding%2C%20the%20key%20challenge%20tackled%20by%20most%20estimators%2C%20is%20not%20an%20issue%20in%0Aany%20of%20the%20considered%20datasets.%20We%20discuss%20the%20major%20implications%20of%20our%0Afindings%20and%20present%20directions%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSources%2520of%2520Gain%253A%2520Decomposing%2520Performance%2520in%2520Conditional%2520Average%2520Dose%250A%2520%2520Response%2520Estimation%26entry.906535625%3DChristopher%2520Bockel-Rickermann%2520and%2520Toon%2520Vanderschueren%2520and%2520Tim%2520Verdonck%2520and%2520Wouter%2520Verbeke%26entry.1292438233%3D%2520%2520Estimating%2520conditional%2520average%2520dose%2520responses%2520%2528CADR%2529%2520is%2520an%2520important%2520but%250Achallenging%2520problem.%2520Estimators%2520must%2520correctly%2520model%2520the%2520potentially%2520complex%250Arelationships%2520between%2520covariates%252C%2520interventions%252C%2520doses%252C%2520and%2520outcomes.%2520In%2520recent%250Ayears%252C%2520the%2520machine%2520learning%2520community%2520has%2520shown%2520great%2520interest%2520in%2520developing%250Atailored%2520CADR%2520estimators%2520that%2520target%2520specific%2520challenges.%2520Their%2520performance%2520is%250Atypically%2520evaluated%2520against%2520other%2520methods%2520on%2520%2528semi-%2529%2520synthetic%2520benchmark%250Adatasets.%2520Our%2520paper%2520analyses%2520this%2520practice%2520and%2520shows%2520that%2520using%2520popular%250Abenchmark%2520datasets%2520without%2520further%2520analysis%2520is%2520insufficient%2520to%2520judge%2520model%250Aperformance.%2520Established%2520benchmarks%2520entail%2520multiple%2520challenges%252C%2520whose%2520impacts%250Amust%2520be%2520disentangled.%2520Therefore%252C%2520we%2520propose%2520a%2520novel%2520decomposition%2520scheme%2520that%250Aallows%2520the%2520evaluation%2520of%2520the%2520impact%2520of%2520five%2520distinct%2520components%2520contributing%2520to%250ACADR%2520estimator%2520performance.%2520We%2520apply%2520this%2520scheme%2520to%2520eight%2520popular%2520CADR%250Aestimators%2520on%2520four%2520widely-used%2520benchmark%2520datasets%252C%2520running%2520nearly%25201%252C500%250Aindividual%2520experiments.%2520Our%2520results%2520reveal%2520that%2520most%2520established%2520benchmarks%2520are%250Achallenging%2520for%2520reasons%2520different%2520from%2520their%2520creators%2527%2520claims.%2520Notably%252C%250Aconfounding%252C%2520the%2520key%2520challenge%2520tackled%2520by%2520most%2520estimators%252C%2520is%2520not%2520an%2520issue%2520in%250Aany%2520of%2520the%2520considered%2520datasets.%2520We%2520discuss%2520the%2520major%2520implications%2520of%2520our%250Afindings%2520and%2520present%2520directions%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sources%20of%20Gain%3A%20Decomposing%20Performance%20in%20Conditional%20Average%20Dose%0A%20%20Response%20Estimation&entry.906535625=Christopher%20Bockel-Rickermann%20and%20Toon%20Vanderschueren%20and%20Tim%20Verdonck%20and%20Wouter%20Verbeke&entry.1292438233=%20%20Estimating%20conditional%20average%20dose%20responses%20%28CADR%29%20is%20an%20important%20but%0Achallenging%20problem.%20Estimators%20must%20correctly%20model%20the%20potentially%20complex%0Arelationships%20between%20covariates%2C%20interventions%2C%20doses%2C%20and%20outcomes.%20In%20recent%0Ayears%2C%20the%20machine%20learning%20community%20has%20shown%20great%20interest%20in%20developing%0Atailored%20CADR%20estimators%20that%20target%20specific%20challenges.%20Their%20performance%20is%0Atypically%20evaluated%20against%20other%20methods%20on%20%28semi-%29%20synthetic%20benchmark%0Adatasets.%20Our%20paper%20analyses%20this%20practice%20and%20shows%20that%20using%20popular%0Abenchmark%20datasets%20without%20further%20analysis%20is%20insufficient%20to%20judge%20model%0Aperformance.%20Established%20benchmarks%20entail%20multiple%20challenges%2C%20whose%20impacts%0Amust%20be%20disentangled.%20Therefore%2C%20we%20propose%20a%20novel%20decomposition%20scheme%20that%0Aallows%20the%20evaluation%20of%20the%20impact%20of%20five%20distinct%20components%20contributing%20to%0ACADR%20estimator%20performance.%20We%20apply%20this%20scheme%20to%20eight%20popular%20CADR%0Aestimators%20on%20four%20widely-used%20benchmark%20datasets%2C%20running%20nearly%201%2C500%0Aindividual%20experiments.%20Our%20results%20reveal%20that%20most%20established%20benchmarks%20are%0Achallenging%20for%20reasons%20different%20from%20their%20creators%27%20claims.%20Notably%2C%0Aconfounding%2C%20the%20key%20challenge%20tackled%20by%20most%20estimators%2C%20is%20not%20an%20issue%20in%0Aany%20of%20the%20considered%20datasets.%20We%20discuss%20the%20major%20implications%20of%20our%0Afindings%20and%20present%20directions%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08206v1&entry.124074799=Read"},
{"title": "Audio Editing with Non-Rigid Text Prompts", "author": "Francesco Paissan and Luca Della Libera and Zhepei Wang and Mirco Ravanelli and Paris Smaragdis and Cem Subakan", "abstract": "  In this paper, we explore audio-editing with non-rigid text edits. We show\nthat the proposed editing pipeline is able to create audio edits that remain\nfaithful to the input audio. We explore text prompts that perform addition,\nstyle transfer, and in-painting. We quantitatively and qualitatively show that\nthe edits are able to obtain results which outperform Audio-LDM, a recently\nreleased text-prompted audio generation model. Qualitative inspection of the\nresults points out that the edits given by our approach remain more faithful to\nthe input audio in terms of keeping the original onsets and offsets of the\naudio events.\n", "link": "http://arxiv.org/abs/2310.12858v2", "date": "2024-06-12", "relevancy": 1.4414, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5092}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4954}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Audio%20Editing%20with%20Non-Rigid%20Text%20Prompts&body=Title%3A%20Audio%20Editing%20with%20Non-Rigid%20Text%20Prompts%0AAuthor%3A%20Francesco%20Paissan%20and%20Luca%20Della%20Libera%20and%20Zhepei%20Wang%20and%20Mirco%20Ravanelli%20and%20Paris%20Smaragdis%20and%20Cem%20Subakan%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20explore%20audio-editing%20with%20non-rigid%20text%20edits.%20We%20show%0Athat%20the%20proposed%20editing%20pipeline%20is%20able%20to%20create%20audio%20edits%20that%20remain%0Afaithful%20to%20the%20input%20audio.%20We%20explore%20text%20prompts%20that%20perform%20addition%2C%0Astyle%20transfer%2C%20and%20in-painting.%20We%20quantitatively%20and%20qualitatively%20show%20that%0Athe%20edits%20are%20able%20to%20obtain%20results%20which%20outperform%20Audio-LDM%2C%20a%20recently%0Areleased%20text-prompted%20audio%20generation%20model.%20Qualitative%20inspection%20of%20the%0Aresults%20points%20out%20that%20the%20edits%20given%20by%20our%20approach%20remain%20more%20faithful%20to%0Athe%20input%20audio%20in%20terms%20of%20keeping%20the%20original%20onsets%20and%20offsets%20of%20the%0Aaudio%20events.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.12858v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudio%2520Editing%2520with%2520Non-Rigid%2520Text%2520Prompts%26entry.906535625%3DFrancesco%2520Paissan%2520and%2520Luca%2520Della%2520Libera%2520and%2520Zhepei%2520Wang%2520and%2520Mirco%2520Ravanelli%2520and%2520Paris%2520Smaragdis%2520and%2520Cem%2520Subakan%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520explore%2520audio-editing%2520with%2520non-rigid%2520text%2520edits.%2520We%2520show%250Athat%2520the%2520proposed%2520editing%2520pipeline%2520is%2520able%2520to%2520create%2520audio%2520edits%2520that%2520remain%250Afaithful%2520to%2520the%2520input%2520audio.%2520We%2520explore%2520text%2520prompts%2520that%2520perform%2520addition%252C%250Astyle%2520transfer%252C%2520and%2520in-painting.%2520We%2520quantitatively%2520and%2520qualitatively%2520show%2520that%250Athe%2520edits%2520are%2520able%2520to%2520obtain%2520results%2520which%2520outperform%2520Audio-LDM%252C%2520a%2520recently%250Areleased%2520text-prompted%2520audio%2520generation%2520model.%2520Qualitative%2520inspection%2520of%2520the%250Aresults%2520points%2520out%2520that%2520the%2520edits%2520given%2520by%2520our%2520approach%2520remain%2520more%2520faithful%2520to%250Athe%2520input%2520audio%2520in%2520terms%2520of%2520keeping%2520the%2520original%2520onsets%2520and%2520offsets%2520of%2520the%250Aaudio%2520events.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.12858v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Audio%20Editing%20with%20Non-Rigid%20Text%20Prompts&entry.906535625=Francesco%20Paissan%20and%20Luca%20Della%20Libera%20and%20Zhepei%20Wang%20and%20Mirco%20Ravanelli%20and%20Paris%20Smaragdis%20and%20Cem%20Subakan&entry.1292438233=%20%20In%20this%20paper%2C%20we%20explore%20audio-editing%20with%20non-rigid%20text%20edits.%20We%20show%0Athat%20the%20proposed%20editing%20pipeline%20is%20able%20to%20create%20audio%20edits%20that%20remain%0Afaithful%20to%20the%20input%20audio.%20We%20explore%20text%20prompts%20that%20perform%20addition%2C%0Astyle%20transfer%2C%20and%20in-painting.%20We%20quantitatively%20and%20qualitatively%20show%20that%0Athe%20edits%20are%20able%20to%20obtain%20results%20which%20outperform%20Audio-LDM%2C%20a%20recently%0Areleased%20text-prompted%20audio%20generation%20model.%20Qualitative%20inspection%20of%20the%0Aresults%20points%20out%20that%20the%20edits%20given%20by%20our%20approach%20remain%20more%20faithful%20to%0Athe%20input%20audio%20in%20terms%20of%20keeping%20the%20original%20onsets%20and%20offsets%20of%20the%0Aaudio%20events.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.12858v2&entry.124074799=Read"},
{"title": "2nd Place Solution for MOSE Track in CVPR 2024 PVUW workshop: Complex\n  Video Object Segmentation", "author": "Zhensong Xu and Jiangtao Yao and Chengjing Wu and Ting Liu and Luoqi Liu", "abstract": "  Complex video object segmentation serves as a fundamental task for a wide\nrange of downstream applications such as video editing and automatic data\nannotation. Here we present the 2nd place solution in the MOSE track of PVUW\n2024. To mitigate problems caused by tiny objects, similar objects and fast\nmovements in MOSE. We use instance segmentation to generate extra pretraining\ndata from the valid and test set of MOSE. The segmented instances are combined\nwith objects extracted from COCO to augment the training data and enhance\nsemantic representation of the baseline model. Besides, motion blur is added\nduring training to increase robustness against image blur induced by motion.\nFinally, we apply test time augmentation (TTA) and memory strategy to the\ninference stage. Our method ranked 2nd in the MOSE track of PVUW 2024, with a\n$\\mathcal{J}$ of 0.8007, a $\\mathcal{F}$ of 0.8683 and a\n$\\mathcal{J}$\\&$\\mathcal{F}$ of 0.8345.\n", "link": "http://arxiv.org/abs/2406.08192v1", "date": "2024-06-12", "relevancy": 1.4504, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4856}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4842}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%202nd%20Place%20Solution%20for%20MOSE%20Track%20in%20CVPR%202024%20PVUW%20workshop%3A%20Complex%0A%20%20Video%20Object%20Segmentation&body=Title%3A%202nd%20Place%20Solution%20for%20MOSE%20Track%20in%20CVPR%202024%20PVUW%20workshop%3A%20Complex%0A%20%20Video%20Object%20Segmentation%0AAuthor%3A%20Zhensong%20Xu%20and%20Jiangtao%20Yao%20and%20Chengjing%20Wu%20and%20Ting%20Liu%20and%20Luoqi%20Liu%0AAbstract%3A%20%20%20Complex%20video%20object%20segmentation%20serves%20as%20a%20fundamental%20task%20for%20a%20wide%0Arange%20of%20downstream%20applications%20such%20as%20video%20editing%20and%20automatic%20data%0Aannotation.%20Here%20we%20present%20the%202nd%20place%20solution%20in%20the%20MOSE%20track%20of%20PVUW%0A2024.%20To%20mitigate%20problems%20caused%20by%20tiny%20objects%2C%20similar%20objects%20and%20fast%0Amovements%20in%20MOSE.%20We%20use%20instance%20segmentation%20to%20generate%20extra%20pretraining%0Adata%20from%20the%20valid%20and%20test%20set%20of%20MOSE.%20The%20segmented%20instances%20are%20combined%0Awith%20objects%20extracted%20from%20COCO%20to%20augment%20the%20training%20data%20and%20enhance%0Asemantic%20representation%20of%20the%20baseline%20model.%20Besides%2C%20motion%20blur%20is%20added%0Aduring%20training%20to%20increase%20robustness%20against%20image%20blur%20induced%20by%20motion.%0AFinally%2C%20we%20apply%20test%20time%20augmentation%20%28TTA%29%20and%20memory%20strategy%20to%20the%0Ainference%20stage.%20Our%20method%20ranked%202nd%20in%20the%20MOSE%20track%20of%20PVUW%202024%2C%20with%20a%0A%24%5Cmathcal%7BJ%7D%24%20of%200.8007%2C%20a%20%24%5Cmathcal%7BF%7D%24%20of%200.8683%20and%20a%0A%24%5Cmathcal%7BJ%7D%24%5C%26%24%5Cmathcal%7BF%7D%24%20of%200.8345.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08192v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D2nd%2520Place%2520Solution%2520for%2520MOSE%2520Track%2520in%2520CVPR%25202024%2520PVUW%2520workshop%253A%2520Complex%250A%2520%2520Video%2520Object%2520Segmentation%26entry.906535625%3DZhensong%2520Xu%2520and%2520Jiangtao%2520Yao%2520and%2520Chengjing%2520Wu%2520and%2520Ting%2520Liu%2520and%2520Luoqi%2520Liu%26entry.1292438233%3D%2520%2520Complex%2520video%2520object%2520segmentation%2520serves%2520as%2520a%2520fundamental%2520task%2520for%2520a%2520wide%250Arange%2520of%2520downstream%2520applications%2520such%2520as%2520video%2520editing%2520and%2520automatic%2520data%250Aannotation.%2520Here%2520we%2520present%2520the%25202nd%2520place%2520solution%2520in%2520the%2520MOSE%2520track%2520of%2520PVUW%250A2024.%2520To%2520mitigate%2520problems%2520caused%2520by%2520tiny%2520objects%252C%2520similar%2520objects%2520and%2520fast%250Amovements%2520in%2520MOSE.%2520We%2520use%2520instance%2520segmentation%2520to%2520generate%2520extra%2520pretraining%250Adata%2520from%2520the%2520valid%2520and%2520test%2520set%2520of%2520MOSE.%2520The%2520segmented%2520instances%2520are%2520combined%250Awith%2520objects%2520extracted%2520from%2520COCO%2520to%2520augment%2520the%2520training%2520data%2520and%2520enhance%250Asemantic%2520representation%2520of%2520the%2520baseline%2520model.%2520Besides%252C%2520motion%2520blur%2520is%2520added%250Aduring%2520training%2520to%2520increase%2520robustness%2520against%2520image%2520blur%2520induced%2520by%2520motion.%250AFinally%252C%2520we%2520apply%2520test%2520time%2520augmentation%2520%2528TTA%2529%2520and%2520memory%2520strategy%2520to%2520the%250Ainference%2520stage.%2520Our%2520method%2520ranked%25202nd%2520in%2520the%2520MOSE%2520track%2520of%2520PVUW%25202024%252C%2520with%2520a%250A%2524%255Cmathcal%257BJ%257D%2524%2520of%25200.8007%252C%2520a%2520%2524%255Cmathcal%257BF%257D%2524%2520of%25200.8683%2520and%2520a%250A%2524%255Cmathcal%257BJ%257D%2524%255C%2526%2524%255Cmathcal%257BF%257D%2524%2520of%25200.8345.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08192v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=2nd%20Place%20Solution%20for%20MOSE%20Track%20in%20CVPR%202024%20PVUW%20workshop%3A%20Complex%0A%20%20Video%20Object%20Segmentation&entry.906535625=Zhensong%20Xu%20and%20Jiangtao%20Yao%20and%20Chengjing%20Wu%20and%20Ting%20Liu%20and%20Luoqi%20Liu&entry.1292438233=%20%20Complex%20video%20object%20segmentation%20serves%20as%20a%20fundamental%20task%20for%20a%20wide%0Arange%20of%20downstream%20applications%20such%20as%20video%20editing%20and%20automatic%20data%0Aannotation.%20Here%20we%20present%20the%202nd%20place%20solution%20in%20the%20MOSE%20track%20of%20PVUW%0A2024.%20To%20mitigate%20problems%20caused%20by%20tiny%20objects%2C%20similar%20objects%20and%20fast%0Amovements%20in%20MOSE.%20We%20use%20instance%20segmentation%20to%20generate%20extra%20pretraining%0Adata%20from%20the%20valid%20and%20test%20set%20of%20MOSE.%20The%20segmented%20instances%20are%20combined%0Awith%20objects%20extracted%20from%20COCO%20to%20augment%20the%20training%20data%20and%20enhance%0Asemantic%20representation%20of%20the%20baseline%20model.%20Besides%2C%20motion%20blur%20is%20added%0Aduring%20training%20to%20increase%20robustness%20against%20image%20blur%20induced%20by%20motion.%0AFinally%2C%20we%20apply%20test%20time%20augmentation%20%28TTA%29%20and%20memory%20strategy%20to%20the%0Ainference%20stage.%20Our%20method%20ranked%202nd%20in%20the%20MOSE%20track%20of%20PVUW%202024%2C%20with%20a%0A%24%5Cmathcal%7BJ%7D%24%20of%200.8007%2C%20a%20%24%5Cmathcal%7BF%7D%24%20of%200.8683%20and%20a%0A%24%5Cmathcal%7BJ%7D%24%5C%26%24%5Cmathcal%7BF%7D%24%20of%200.8345.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08192v1&entry.124074799=Read"},
{"title": "Forward-Euler time-discretization for Wasserstein gradient flows can be\n  wrong", "author": "Yewei Xu and Qin Li", "abstract": "  In this note, we examine the forward-Euler discretization for simulating\nWasserstein gradient flows. We provide two counter-examples showcasing the\nfailure of this discretization even for a simple case where the energy\nfunctional is defined as the KL divergence against some nicely structured\nprobability densities. A simple explanation of this failure is also discussed.\n", "link": "http://arxiv.org/abs/2406.08209v1", "date": "2024-06-12", "relevancy": 1.2141, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4424}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4011}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forward-Euler%20time-discretization%20for%20Wasserstein%20gradient%20flows%20can%20be%0A%20%20wrong&body=Title%3A%20Forward-Euler%20time-discretization%20for%20Wasserstein%20gradient%20flows%20can%20be%0A%20%20wrong%0AAuthor%3A%20Yewei%20Xu%20and%20Qin%20Li%0AAbstract%3A%20%20%20In%20this%20note%2C%20we%20examine%20the%20forward-Euler%20discretization%20for%20simulating%0AWasserstein%20gradient%20flows.%20We%20provide%20two%20counter-examples%20showcasing%20the%0Afailure%20of%20this%20discretization%20even%20for%20a%20simple%20case%20where%20the%20energy%0Afunctional%20is%20defined%20as%20the%20KL%20divergence%20against%20some%20nicely%20structured%0Aprobability%20densities.%20A%20simple%20explanation%20of%20this%20failure%20is%20also%20discussed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08209v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForward-Euler%2520time-discretization%2520for%2520Wasserstein%2520gradient%2520flows%2520can%2520be%250A%2520%2520wrong%26entry.906535625%3DYewei%2520Xu%2520and%2520Qin%2520Li%26entry.1292438233%3D%2520%2520In%2520this%2520note%252C%2520we%2520examine%2520the%2520forward-Euler%2520discretization%2520for%2520simulating%250AWasserstein%2520gradient%2520flows.%2520We%2520provide%2520two%2520counter-examples%2520showcasing%2520the%250Afailure%2520of%2520this%2520discretization%2520even%2520for%2520a%2520simple%2520case%2520where%2520the%2520energy%250Afunctional%2520is%2520defined%2520as%2520the%2520KL%2520divergence%2520against%2520some%2520nicely%2520structured%250Aprobability%2520densities.%2520A%2520simple%2520explanation%2520of%2520this%2520failure%2520is%2520also%2520discussed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08209v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forward-Euler%20time-discretization%20for%20Wasserstein%20gradient%20flows%20can%20be%0A%20%20wrong&entry.906535625=Yewei%20Xu%20and%20Qin%20Li&entry.1292438233=%20%20In%20this%20note%2C%20we%20examine%20the%20forward-Euler%20discretization%20for%20simulating%0AWasserstein%20gradient%20flows.%20We%20provide%20two%20counter-examples%20showcasing%20the%0Afailure%20of%20this%20discretization%20even%20for%20a%20simple%20case%20where%20the%20energy%0Afunctional%20is%20defined%20as%20the%20KL%20divergence%20against%20some%20nicely%20structured%0Aprobability%20densities.%20A%20simple%20explanation%20of%20this%20failure%20is%20also%20discussed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08209v1&entry.124074799=Read"},
{"title": "cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations\n  in Scientific Papers", "author": "Anirudh Sundar and Jin Xu and William Gay and Christopher Richardson and Larry Heck", "abstract": "  An emerging area of research in situated and multimodal interactive\nconversations (SIMMC) includes interactions in scientific papers. Since\nscientific papers are primarily composed of text, equations, figures, and\ntables, SIMMC methods must be developed specifically for each component to\nsupport the depth of inquiry and interactions required by research scientists.\nThis work introduces Conversational Papers (cPAPERS), a dataset of\nconversational question-answer pairs from reviews of academic papers grounded\nin these paper components and their associated references from scientific\ndocuments available on arXiv. We present a data collection strategy to collect\nthese question-answer pairs from OpenReview and associate them with contextual\ninformation from LaTeX source files. Additionally, we present a series of\nbaseline approaches utilizing Large Language Models (LLMs) in both zero-shot\nand fine-tuned configurations to address the cPAPERS dataset.\n", "link": "http://arxiv.org/abs/2406.08398v1", "date": "2024-06-12", "relevancy": 1.3476, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4561}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4486}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20cPAPERS%3A%20A%20Dataset%20of%20Situated%20and%20Multimodal%20Interactive%20Conversations%0A%20%20in%20Scientific%20Papers&body=Title%3A%20cPAPERS%3A%20A%20Dataset%20of%20Situated%20and%20Multimodal%20Interactive%20Conversations%0A%20%20in%20Scientific%20Papers%0AAuthor%3A%20Anirudh%20Sundar%20and%20Jin%20Xu%20and%20William%20Gay%20and%20Christopher%20Richardson%20and%20Larry%20Heck%0AAbstract%3A%20%20%20An%20emerging%20area%20of%20research%20in%20situated%20and%20multimodal%20interactive%0Aconversations%20%28SIMMC%29%20includes%20interactions%20in%20scientific%20papers.%20Since%0Ascientific%20papers%20are%20primarily%20composed%20of%20text%2C%20equations%2C%20figures%2C%20and%0Atables%2C%20SIMMC%20methods%20must%20be%20developed%20specifically%20for%20each%20component%20to%0Asupport%20the%20depth%20of%20inquiry%20and%20interactions%20required%20by%20research%20scientists.%0AThis%20work%20introduces%20Conversational%20Papers%20%28cPAPERS%29%2C%20a%20dataset%20of%0Aconversational%20question-answer%20pairs%20from%20reviews%20of%20academic%20papers%20grounded%0Ain%20these%20paper%20components%20and%20their%20associated%20references%20from%20scientific%0Adocuments%20available%20on%20arXiv.%20We%20present%20a%20data%20collection%20strategy%20to%20collect%0Athese%20question-answer%20pairs%20from%20OpenReview%20and%20associate%20them%20with%20contextual%0Ainformation%20from%20LaTeX%20source%20files.%20Additionally%2C%20we%20present%20a%20series%20of%0Abaseline%20approaches%20utilizing%20Large%20Language%20Models%20%28LLMs%29%20in%20both%20zero-shot%0Aand%20fine-tuned%20configurations%20to%20address%20the%20cPAPERS%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08398v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DcPAPERS%253A%2520A%2520Dataset%2520of%2520Situated%2520and%2520Multimodal%2520Interactive%2520Conversations%250A%2520%2520in%2520Scientific%2520Papers%26entry.906535625%3DAnirudh%2520Sundar%2520and%2520Jin%2520Xu%2520and%2520William%2520Gay%2520and%2520Christopher%2520Richardson%2520and%2520Larry%2520Heck%26entry.1292438233%3D%2520%2520An%2520emerging%2520area%2520of%2520research%2520in%2520situated%2520and%2520multimodal%2520interactive%250Aconversations%2520%2528SIMMC%2529%2520includes%2520interactions%2520in%2520scientific%2520papers.%2520Since%250Ascientific%2520papers%2520are%2520primarily%2520composed%2520of%2520text%252C%2520equations%252C%2520figures%252C%2520and%250Atables%252C%2520SIMMC%2520methods%2520must%2520be%2520developed%2520specifically%2520for%2520each%2520component%2520to%250Asupport%2520the%2520depth%2520of%2520inquiry%2520and%2520interactions%2520required%2520by%2520research%2520scientists.%250AThis%2520work%2520introduces%2520Conversational%2520Papers%2520%2528cPAPERS%2529%252C%2520a%2520dataset%2520of%250Aconversational%2520question-answer%2520pairs%2520from%2520reviews%2520of%2520academic%2520papers%2520grounded%250Ain%2520these%2520paper%2520components%2520and%2520their%2520associated%2520references%2520from%2520scientific%250Adocuments%2520available%2520on%2520arXiv.%2520We%2520present%2520a%2520data%2520collection%2520strategy%2520to%2520collect%250Athese%2520question-answer%2520pairs%2520from%2520OpenReview%2520and%2520associate%2520them%2520with%2520contextual%250Ainformation%2520from%2520LaTeX%2520source%2520files.%2520Additionally%252C%2520we%2520present%2520a%2520series%2520of%250Abaseline%2520approaches%2520utilizing%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520both%2520zero-shot%250Aand%2520fine-tuned%2520configurations%2520to%2520address%2520the%2520cPAPERS%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08398v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=cPAPERS%3A%20A%20Dataset%20of%20Situated%20and%20Multimodal%20Interactive%20Conversations%0A%20%20in%20Scientific%20Papers&entry.906535625=Anirudh%20Sundar%20and%20Jin%20Xu%20and%20William%20Gay%20and%20Christopher%20Richardson%20and%20Larry%20Heck&entry.1292438233=%20%20An%20emerging%20area%20of%20research%20in%20situated%20and%20multimodal%20interactive%0Aconversations%20%28SIMMC%29%20includes%20interactions%20in%20scientific%20papers.%20Since%0Ascientific%20papers%20are%20primarily%20composed%20of%20text%2C%20equations%2C%20figures%2C%20and%0Atables%2C%20SIMMC%20methods%20must%20be%20developed%20specifically%20for%20each%20component%20to%0Asupport%20the%20depth%20of%20inquiry%20and%20interactions%20required%20by%20research%20scientists.%0AThis%20work%20introduces%20Conversational%20Papers%20%28cPAPERS%29%2C%20a%20dataset%20of%0Aconversational%20question-answer%20pairs%20from%20reviews%20of%20academic%20papers%20grounded%0Ain%20these%20paper%20components%20and%20their%20associated%20references%20from%20scientific%0Adocuments%20available%20on%20arXiv.%20We%20present%20a%20data%20collection%20strategy%20to%20collect%0Athese%20question-answer%20pairs%20from%20OpenReview%20and%20associate%20them%20with%20contextual%0Ainformation%20from%20LaTeX%20source%20files.%20Additionally%2C%20we%20present%20a%20series%20of%0Abaseline%20approaches%20utilizing%20Large%20Language%20Models%20%28LLMs%29%20in%20both%20zero-shot%0Aand%20fine-tuned%20configurations%20to%20address%20the%20cPAPERS%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08398v1&entry.124074799=Read"},
{"title": "VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model\n  for Hundreds of Vision-Language Tasks", "author": "Jiannan Wu and Muyan Zhong and Sen Xing and Zeqiang Lai and Zhaoyang Liu and Wenhai Wang and Zhe Chen and Xizhou Zhu and Lewei Lu and Tong Lu and Ping Luo and Yu Qiao and Jifeng Dai", "abstract": "  We present VisionLLM v2, an end-to-end generalist multimodal large model\n(MLLM) that unifies visual perception, understanding, and generation within a\nsingle framework. Unlike traditional MLLMs limited to text output, VisionLLM v2\nsignificantly broadens its application scope. It excels not only in\nconventional visual question answering (VQA) but also in open-ended,\ncross-domain vision tasks such as object localization, pose estimation, and\nimage generation and editing. To this end, we propose a new information\ntransmission mechanism termed \"super link\", as a medium to connect MLLM with\ntask-specific decoders. It not only allows flexible transmission of task\ninformation and gradient feedback between the MLLM and multiple downstream\ndecoders but also effectively resolves training conflicts in multi-tasking\nscenarios. In addition, to support the diverse range of tasks, we carefully\ncollected and combed training data from hundreds of public vision and\nvision-language tasks. In this way, our model can be joint-trained end-to-end\non hundreds of vision language tasks and generalize to these tasks using a set\nof shared parameters through different user prompts, achieving performance\ncomparable to task-specific models. We believe VisionLLM v2 will offer a new\nperspective on the generalization of MLLMs.\n", "link": "http://arxiv.org/abs/2406.08394v1", "date": "2024-06-12", "relevancy": 1.5943, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5526}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5145}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisionLLM%20v2%3A%20An%20End-to-End%20Generalist%20Multimodal%20Large%20Language%20Model%0A%20%20for%20Hundreds%20of%20Vision-Language%20Tasks&body=Title%3A%20VisionLLM%20v2%3A%20An%20End-to-End%20Generalist%20Multimodal%20Large%20Language%20Model%0A%20%20for%20Hundreds%20of%20Vision-Language%20Tasks%0AAuthor%3A%20Jiannan%20Wu%20and%20Muyan%20Zhong%20and%20Sen%20Xing%20and%20Zeqiang%20Lai%20and%20Zhaoyang%20Liu%20and%20Wenhai%20Wang%20and%20Zhe%20Chen%20and%20Xizhou%20Zhu%20and%20Lewei%20Lu%20and%20Tong%20Lu%20and%20Ping%20Luo%20and%20Yu%20Qiao%20and%20Jifeng%20Dai%0AAbstract%3A%20%20%20We%20present%20VisionLLM%20v2%2C%20an%20end-to-end%20generalist%20multimodal%20large%20model%0A%28MLLM%29%20that%20unifies%20visual%20perception%2C%20understanding%2C%20and%20generation%20within%20a%0Asingle%20framework.%20Unlike%20traditional%20MLLMs%20limited%20to%20text%20output%2C%20VisionLLM%20v2%0Asignificantly%20broadens%20its%20application%20scope.%20It%20excels%20not%20only%20in%0Aconventional%20visual%20question%20answering%20%28VQA%29%20but%20also%20in%20open-ended%2C%0Across-domain%20vision%20tasks%20such%20as%20object%20localization%2C%20pose%20estimation%2C%20and%0Aimage%20generation%20and%20editing.%20To%20this%20end%2C%20we%20propose%20a%20new%20information%0Atransmission%20mechanism%20termed%20%22super%20link%22%2C%20as%20a%20medium%20to%20connect%20MLLM%20with%0Atask-specific%20decoders.%20It%20not%20only%20allows%20flexible%20transmission%20of%20task%0Ainformation%20and%20gradient%20feedback%20between%20the%20MLLM%20and%20multiple%20downstream%0Adecoders%20but%20also%20effectively%20resolves%20training%20conflicts%20in%20multi-tasking%0Ascenarios.%20In%20addition%2C%20to%20support%20the%20diverse%20range%20of%20tasks%2C%20we%20carefully%0Acollected%20and%20combed%20training%20data%20from%20hundreds%20of%20public%20vision%20and%0Avision-language%20tasks.%20In%20this%20way%2C%20our%20model%20can%20be%20joint-trained%20end-to-end%0Aon%20hundreds%20of%20vision%20language%20tasks%20and%20generalize%20to%20these%20tasks%20using%20a%20set%0Aof%20shared%20parameters%20through%20different%20user%20prompts%2C%20achieving%20performance%0Acomparable%20to%20task-specific%20models.%20We%20believe%20VisionLLM%20v2%20will%20offer%20a%20new%0Aperspective%20on%20the%20generalization%20of%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08394v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisionLLM%2520v2%253A%2520An%2520End-to-End%2520Generalist%2520Multimodal%2520Large%2520Language%2520Model%250A%2520%2520for%2520Hundreds%2520of%2520Vision-Language%2520Tasks%26entry.906535625%3DJiannan%2520Wu%2520and%2520Muyan%2520Zhong%2520and%2520Sen%2520Xing%2520and%2520Zeqiang%2520Lai%2520and%2520Zhaoyang%2520Liu%2520and%2520Wenhai%2520Wang%2520and%2520Zhe%2520Chen%2520and%2520Xizhou%2520Zhu%2520and%2520Lewei%2520Lu%2520and%2520Tong%2520Lu%2520and%2520Ping%2520Luo%2520and%2520Yu%2520Qiao%2520and%2520Jifeng%2520Dai%26entry.1292438233%3D%2520%2520We%2520present%2520VisionLLM%2520v2%252C%2520an%2520end-to-end%2520generalist%2520multimodal%2520large%2520model%250A%2528MLLM%2529%2520that%2520unifies%2520visual%2520perception%252C%2520understanding%252C%2520and%2520generation%2520within%2520a%250Asingle%2520framework.%2520Unlike%2520traditional%2520MLLMs%2520limited%2520to%2520text%2520output%252C%2520VisionLLM%2520v2%250Asignificantly%2520broadens%2520its%2520application%2520scope.%2520It%2520excels%2520not%2520only%2520in%250Aconventional%2520visual%2520question%2520answering%2520%2528VQA%2529%2520but%2520also%2520in%2520open-ended%252C%250Across-domain%2520vision%2520tasks%2520such%2520as%2520object%2520localization%252C%2520pose%2520estimation%252C%2520and%250Aimage%2520generation%2520and%2520editing.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520new%2520information%250Atransmission%2520mechanism%2520termed%2520%2522super%2520link%2522%252C%2520as%2520a%2520medium%2520to%2520connect%2520MLLM%2520with%250Atask-specific%2520decoders.%2520It%2520not%2520only%2520allows%2520flexible%2520transmission%2520of%2520task%250Ainformation%2520and%2520gradient%2520feedback%2520between%2520the%2520MLLM%2520and%2520multiple%2520downstream%250Adecoders%2520but%2520also%2520effectively%2520resolves%2520training%2520conflicts%2520in%2520multi-tasking%250Ascenarios.%2520In%2520addition%252C%2520to%2520support%2520the%2520diverse%2520range%2520of%2520tasks%252C%2520we%2520carefully%250Acollected%2520and%2520combed%2520training%2520data%2520from%2520hundreds%2520of%2520public%2520vision%2520and%250Avision-language%2520tasks.%2520In%2520this%2520way%252C%2520our%2520model%2520can%2520be%2520joint-trained%2520end-to-end%250Aon%2520hundreds%2520of%2520vision%2520language%2520tasks%2520and%2520generalize%2520to%2520these%2520tasks%2520using%2520a%2520set%250Aof%2520shared%2520parameters%2520through%2520different%2520user%2520prompts%252C%2520achieving%2520performance%250Acomparable%2520to%2520task-specific%2520models.%2520We%2520believe%2520VisionLLM%2520v2%2520will%2520offer%2520a%2520new%250Aperspective%2520on%2520the%2520generalization%2520of%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08394v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisionLLM%20v2%3A%20An%20End-to-End%20Generalist%20Multimodal%20Large%20Language%20Model%0A%20%20for%20Hundreds%20of%20Vision-Language%20Tasks&entry.906535625=Jiannan%20Wu%20and%20Muyan%20Zhong%20and%20Sen%20Xing%20and%20Zeqiang%20Lai%20and%20Zhaoyang%20Liu%20and%20Wenhai%20Wang%20and%20Zhe%20Chen%20and%20Xizhou%20Zhu%20and%20Lewei%20Lu%20and%20Tong%20Lu%20and%20Ping%20Luo%20and%20Yu%20Qiao%20and%20Jifeng%20Dai&entry.1292438233=%20%20We%20present%20VisionLLM%20v2%2C%20an%20end-to-end%20generalist%20multimodal%20large%20model%0A%28MLLM%29%20that%20unifies%20visual%20perception%2C%20understanding%2C%20and%20generation%20within%20a%0Asingle%20framework.%20Unlike%20traditional%20MLLMs%20limited%20to%20text%20output%2C%20VisionLLM%20v2%0Asignificantly%20broadens%20its%20application%20scope.%20It%20excels%20not%20only%20in%0Aconventional%20visual%20question%20answering%20%28VQA%29%20but%20also%20in%20open-ended%2C%0Across-domain%20vision%20tasks%20such%20as%20object%20localization%2C%20pose%20estimation%2C%20and%0Aimage%20generation%20and%20editing.%20To%20this%20end%2C%20we%20propose%20a%20new%20information%0Atransmission%20mechanism%20termed%20%22super%20link%22%2C%20as%20a%20medium%20to%20connect%20MLLM%20with%0Atask-specific%20decoders.%20It%20not%20only%20allows%20flexible%20transmission%20of%20task%0Ainformation%20and%20gradient%20feedback%20between%20the%20MLLM%20and%20multiple%20downstream%0Adecoders%20but%20also%20effectively%20resolves%20training%20conflicts%20in%20multi-tasking%0Ascenarios.%20In%20addition%2C%20to%20support%20the%20diverse%20range%20of%20tasks%2C%20we%20carefully%0Acollected%20and%20combed%20training%20data%20from%20hundreds%20of%20public%20vision%20and%0Avision-language%20tasks.%20In%20this%20way%2C%20our%20model%20can%20be%20joint-trained%20end-to-end%0Aon%20hundreds%20of%20vision%20language%20tasks%20and%20generalize%20to%20these%20tasks%20using%20a%20set%0Aof%20shared%20parameters%20through%20different%20user%20prompts%2C%20achieving%20performance%0Acomparable%20to%20task-specific%20models.%20We%20believe%20VisionLLM%20v2%20will%20offer%20a%20new%0Aperspective%20on%20the%20generalization%20of%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08394v1&entry.124074799=Read"},
{"title": "A Survey of Pipeline Tools for Data Engineering", "author": "Anthony Mbata and Yaji Sripada and Mingjun Zhong", "abstract": "  Currently, a variety of pipeline tools are available for use in data\nengineering. Data scientists can use these tools to resolve data wrangling\nissues associated with data and accomplish some data engineering tasks from\ndata ingestion through data preparation to utilization as input for machine\nlearning (ML). Some of these tools have essential built-in components or can be\ncombined with other tools to perform desired data engineering operations. While\nsome tools are wholly or partly commercial, several open-source tools are\navailable to perform expert-level data engineering tasks. This survey examines\nthe broad categories and examples of pipeline tools based on their design and\ndata engineering intentions. These categories are Extract Transform\nLoad/Extract Load Transform (ETL/ELT), pipelines for Data Integration,\nIngestion, and Transformation, Data Pipeline Orchestration and Workflow\nManagement, and Machine Learning Pipelines. The survey also provides a broad\noutline of the utilization with examples within these broad groups and finally,\na discussion is presented with case studies indicating the usage of pipeline\ntools for data engineering. The studies present some first-user application\nexperiences with sample data, some complexities of the applied pipeline, and a\nsummary note of approaches to using these tools to prepare data for machine\nlearning.\n", "link": "http://arxiv.org/abs/2406.08335v1", "date": "2024-06-12", "relevancy": 0.7679, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3999}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3811}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.3707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Pipeline%20Tools%20for%20Data%20Engineering&body=Title%3A%20A%20Survey%20of%20Pipeline%20Tools%20for%20Data%20Engineering%0AAuthor%3A%20Anthony%20Mbata%20and%20Yaji%20Sripada%20and%20Mingjun%20Zhong%0AAbstract%3A%20%20%20Currently%2C%20a%20variety%20of%20pipeline%20tools%20are%20available%20for%20use%20in%20data%0Aengineering.%20Data%20scientists%20can%20use%20these%20tools%20to%20resolve%20data%20wrangling%0Aissues%20associated%20with%20data%20and%20accomplish%20some%20data%20engineering%20tasks%20from%0Adata%20ingestion%20through%20data%20preparation%20to%20utilization%20as%20input%20for%20machine%0Alearning%20%28ML%29.%20Some%20of%20these%20tools%20have%20essential%20built-in%20components%20or%20can%20be%0Acombined%20with%20other%20tools%20to%20perform%20desired%20data%20engineering%20operations.%20While%0Asome%20tools%20are%20wholly%20or%20partly%20commercial%2C%20several%20open-source%20tools%20are%0Aavailable%20to%20perform%20expert-level%20data%20engineering%20tasks.%20This%20survey%20examines%0Athe%20broad%20categories%20and%20examples%20of%20pipeline%20tools%20based%20on%20their%20design%20and%0Adata%20engineering%20intentions.%20These%20categories%20are%20Extract%20Transform%0ALoad/Extract%20Load%20Transform%20%28ETL/ELT%29%2C%20pipelines%20for%20Data%20Integration%2C%0AIngestion%2C%20and%20Transformation%2C%20Data%20Pipeline%20Orchestration%20and%20Workflow%0AManagement%2C%20and%20Machine%20Learning%20Pipelines.%20The%20survey%20also%20provides%20a%20broad%0Aoutline%20of%20the%20utilization%20with%20examples%20within%20these%20broad%20groups%20and%20finally%2C%0Aa%20discussion%20is%20presented%20with%20case%20studies%20indicating%20the%20usage%20of%20pipeline%0Atools%20for%20data%20engineering.%20The%20studies%20present%20some%20first-user%20application%0Aexperiences%20with%20sample%20data%2C%20some%20complexities%20of%20the%20applied%20pipeline%2C%20and%20a%0Asummary%20note%20of%20approaches%20to%20using%20these%20tools%20to%20prepare%20data%20for%20machine%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Pipeline%2520Tools%2520for%2520Data%2520Engineering%26entry.906535625%3DAnthony%2520Mbata%2520and%2520Yaji%2520Sripada%2520and%2520Mingjun%2520Zhong%26entry.1292438233%3D%2520%2520Currently%252C%2520a%2520variety%2520of%2520pipeline%2520tools%2520are%2520available%2520for%2520use%2520in%2520data%250Aengineering.%2520Data%2520scientists%2520can%2520use%2520these%2520tools%2520to%2520resolve%2520data%2520wrangling%250Aissues%2520associated%2520with%2520data%2520and%2520accomplish%2520some%2520data%2520engineering%2520tasks%2520from%250Adata%2520ingestion%2520through%2520data%2520preparation%2520to%2520utilization%2520as%2520input%2520for%2520machine%250Alearning%2520%2528ML%2529.%2520Some%2520of%2520these%2520tools%2520have%2520essential%2520built-in%2520components%2520or%2520can%2520be%250Acombined%2520with%2520other%2520tools%2520to%2520perform%2520desired%2520data%2520engineering%2520operations.%2520While%250Asome%2520tools%2520are%2520wholly%2520or%2520partly%2520commercial%252C%2520several%2520open-source%2520tools%2520are%250Aavailable%2520to%2520perform%2520expert-level%2520data%2520engineering%2520tasks.%2520This%2520survey%2520examines%250Athe%2520broad%2520categories%2520and%2520examples%2520of%2520pipeline%2520tools%2520based%2520on%2520their%2520design%2520and%250Adata%2520engineering%2520intentions.%2520These%2520categories%2520are%2520Extract%2520Transform%250ALoad/Extract%2520Load%2520Transform%2520%2528ETL/ELT%2529%252C%2520pipelines%2520for%2520Data%2520Integration%252C%250AIngestion%252C%2520and%2520Transformation%252C%2520Data%2520Pipeline%2520Orchestration%2520and%2520Workflow%250AManagement%252C%2520and%2520Machine%2520Learning%2520Pipelines.%2520The%2520survey%2520also%2520provides%2520a%2520broad%250Aoutline%2520of%2520the%2520utilization%2520with%2520examples%2520within%2520these%2520broad%2520groups%2520and%2520finally%252C%250Aa%2520discussion%2520is%2520presented%2520with%2520case%2520studies%2520indicating%2520the%2520usage%2520of%2520pipeline%250Atools%2520for%2520data%2520engineering.%2520The%2520studies%2520present%2520some%2520first-user%2520application%250Aexperiences%2520with%2520sample%2520data%252C%2520some%2520complexities%2520of%2520the%2520applied%2520pipeline%252C%2520and%2520a%250Asummary%2520note%2520of%2520approaches%2520to%2520using%2520these%2520tools%2520to%2520prepare%2520data%2520for%2520machine%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Pipeline%20Tools%20for%20Data%20Engineering&entry.906535625=Anthony%20Mbata%20and%20Yaji%20Sripada%20and%20Mingjun%20Zhong&entry.1292438233=%20%20Currently%2C%20a%20variety%20of%20pipeline%20tools%20are%20available%20for%20use%20in%20data%0Aengineering.%20Data%20scientists%20can%20use%20these%20tools%20to%20resolve%20data%20wrangling%0Aissues%20associated%20with%20data%20and%20accomplish%20some%20data%20engineering%20tasks%20from%0Adata%20ingestion%20through%20data%20preparation%20to%20utilization%20as%20input%20for%20machine%0Alearning%20%28ML%29.%20Some%20of%20these%20tools%20have%20essential%20built-in%20components%20or%20can%20be%0Acombined%20with%20other%20tools%20to%20perform%20desired%20data%20engineering%20operations.%20While%0Asome%20tools%20are%20wholly%20or%20partly%20commercial%2C%20several%20open-source%20tools%20are%0Aavailable%20to%20perform%20expert-level%20data%20engineering%20tasks.%20This%20survey%20examines%0Athe%20broad%20categories%20and%20examples%20of%20pipeline%20tools%20based%20on%20their%20design%20and%0Adata%20engineering%20intentions.%20These%20categories%20are%20Extract%20Transform%0ALoad/Extract%20Load%20Transform%20%28ETL/ELT%29%2C%20pipelines%20for%20Data%20Integration%2C%0AIngestion%2C%20and%20Transformation%2C%20Data%20Pipeline%20Orchestration%20and%20Workflow%0AManagement%2C%20and%20Machine%20Learning%20Pipelines.%20The%20survey%20also%20provides%20a%20broad%0Aoutline%20of%20the%20utilization%20with%20examples%20within%20these%20broad%20groups%20and%20finally%2C%0Aa%20discussion%20is%20presented%20with%20case%20studies%20indicating%20the%20usage%20of%20pipeline%0Atools%20for%20data%20engineering.%20The%20studies%20present%20some%20first-user%20application%0Aexperiences%20with%20sample%20data%2C%20some%20complexities%20of%20the%20applied%20pipeline%2C%20and%20a%0Asummary%20note%20of%20approaches%20to%20using%20these%20tools%20to%20prepare%20data%20for%20machine%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08335v1&entry.124074799=Read"},
{"title": "Neural Thermodynamic Integration: Free Energies from Energy-based\n  Diffusion Models", "author": "B\u00e1lint M\u00e1t\u00e9 and Fran\u00e7ois Fleuret and Tristan Bereau", "abstract": "  Thermodynamic integration (TI) offers a rigorous method for estimating\nfree-energy differences by integrating over a sequence of interpolating\nconformational ensembles. However, TI calculations are computationally\nexpensive and typically limited to coupling a small number of degrees of\nfreedom due to the need to sample numerous intermediate ensembles with\nsufficient conformational-space overlap. In this work, we propose to perform TI\nalong an alchemical pathway represented by a trainable neural network, which we\nterm Neural TI. Critically, we parametrize a time-dependent Hamiltonian\ninterpolating between the interacting and non-interacting systems, and optimize\nits gradient using a denoising-diffusion objective. The ability of the\nresulting energy-based diffusion model to sample all intermediate ensembles\nallows us to perform TI from a single reference calculation. We apply our\nmethod to Lennard-Jones fluids, where we report accurate calculations of the\nexcess chemical potential, demonstrating that Neural TI is capable of coupling\nhundreds of degrees of freedom at once.\n", "link": "http://arxiv.org/abs/2406.02313v2", "date": "2024-06-12", "relevancy": 1.4262, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4857}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.479}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Thermodynamic%20Integration%3A%20Free%20Energies%20from%20Energy-based%0A%20%20Diffusion%20Models&body=Title%3A%20Neural%20Thermodynamic%20Integration%3A%20Free%20Energies%20from%20Energy-based%0A%20%20Diffusion%20Models%0AAuthor%3A%20B%C3%A1lint%20M%C3%A1t%C3%A9%20and%20Fran%C3%A7ois%20Fleuret%20and%20Tristan%20Bereau%0AAbstract%3A%20%20%20Thermodynamic%20integration%20%28TI%29%20offers%20a%20rigorous%20method%20for%20estimating%0Afree-energy%20differences%20by%20integrating%20over%20a%20sequence%20of%20interpolating%0Aconformational%20ensembles.%20However%2C%20TI%20calculations%20are%20computationally%0Aexpensive%20and%20typically%20limited%20to%20coupling%20a%20small%20number%20of%20degrees%20of%0Afreedom%20due%20to%20the%20need%20to%20sample%20numerous%20intermediate%20ensembles%20with%0Asufficient%20conformational-space%20overlap.%20In%20this%20work%2C%20we%20propose%20to%20perform%20TI%0Aalong%20an%20alchemical%20pathway%20represented%20by%20a%20trainable%20neural%20network%2C%20which%20we%0Aterm%20Neural%20TI.%20Critically%2C%20we%20parametrize%20a%20time-dependent%20Hamiltonian%0Ainterpolating%20between%20the%20interacting%20and%20non-interacting%20systems%2C%20and%20optimize%0Aits%20gradient%20using%20a%20denoising-diffusion%20objective.%20The%20ability%20of%20the%0Aresulting%20energy-based%20diffusion%20model%20to%20sample%20all%20intermediate%20ensembles%0Aallows%20us%20to%20perform%20TI%20from%20a%20single%20reference%20calculation.%20We%20apply%20our%0Amethod%20to%20Lennard-Jones%20fluids%2C%20where%20we%20report%20accurate%20calculations%20of%20the%0Aexcess%20chemical%20potential%2C%20demonstrating%20that%20Neural%20TI%20is%20capable%20of%20coupling%0Ahundreds%20of%20degrees%20of%20freedom%20at%20once.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02313v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Thermodynamic%2520Integration%253A%2520Free%2520Energies%2520from%2520Energy-based%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DB%25C3%25A1lint%2520M%25C3%25A1t%25C3%25A9%2520and%2520Fran%25C3%25A7ois%2520Fleuret%2520and%2520Tristan%2520Bereau%26entry.1292438233%3D%2520%2520Thermodynamic%2520integration%2520%2528TI%2529%2520offers%2520a%2520rigorous%2520method%2520for%2520estimating%250Afree-energy%2520differences%2520by%2520integrating%2520over%2520a%2520sequence%2520of%2520interpolating%250Aconformational%2520ensembles.%2520However%252C%2520TI%2520calculations%2520are%2520computationally%250Aexpensive%2520and%2520typically%2520limited%2520to%2520coupling%2520a%2520small%2520number%2520of%2520degrees%2520of%250Afreedom%2520due%2520to%2520the%2520need%2520to%2520sample%2520numerous%2520intermediate%2520ensembles%2520with%250Asufficient%2520conformational-space%2520overlap.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%2520perform%2520TI%250Aalong%2520an%2520alchemical%2520pathway%2520represented%2520by%2520a%2520trainable%2520neural%2520network%252C%2520which%2520we%250Aterm%2520Neural%2520TI.%2520Critically%252C%2520we%2520parametrize%2520a%2520time-dependent%2520Hamiltonian%250Ainterpolating%2520between%2520the%2520interacting%2520and%2520non-interacting%2520systems%252C%2520and%2520optimize%250Aits%2520gradient%2520using%2520a%2520denoising-diffusion%2520objective.%2520The%2520ability%2520of%2520the%250Aresulting%2520energy-based%2520diffusion%2520model%2520to%2520sample%2520all%2520intermediate%2520ensembles%250Aallows%2520us%2520to%2520perform%2520TI%2520from%2520a%2520single%2520reference%2520calculation.%2520We%2520apply%2520our%250Amethod%2520to%2520Lennard-Jones%2520fluids%252C%2520where%2520we%2520report%2520accurate%2520calculations%2520of%2520the%250Aexcess%2520chemical%2520potential%252C%2520demonstrating%2520that%2520Neural%2520TI%2520is%2520capable%2520of%2520coupling%250Ahundreds%2520of%2520degrees%2520of%2520freedom%2520at%2520once.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02313v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Thermodynamic%20Integration%3A%20Free%20Energies%20from%20Energy-based%0A%20%20Diffusion%20Models&entry.906535625=B%C3%A1lint%20M%C3%A1t%C3%A9%20and%20Fran%C3%A7ois%20Fleuret%20and%20Tristan%20Bereau&entry.1292438233=%20%20Thermodynamic%20integration%20%28TI%29%20offers%20a%20rigorous%20method%20for%20estimating%0Afree-energy%20differences%20by%20integrating%20over%20a%20sequence%20of%20interpolating%0Aconformational%20ensembles.%20However%2C%20TI%20calculations%20are%20computationally%0Aexpensive%20and%20typically%20limited%20to%20coupling%20a%20small%20number%20of%20degrees%20of%0Afreedom%20due%20to%20the%20need%20to%20sample%20numerous%20intermediate%20ensembles%20with%0Asufficient%20conformational-space%20overlap.%20In%20this%20work%2C%20we%20propose%20to%20perform%20TI%0Aalong%20an%20alchemical%20pathway%20represented%20by%20a%20trainable%20neural%20network%2C%20which%20we%0Aterm%20Neural%20TI.%20Critically%2C%20we%20parametrize%20a%20time-dependent%20Hamiltonian%0Ainterpolating%20between%20the%20interacting%20and%20non-interacting%20systems%2C%20and%20optimize%0Aits%20gradient%20using%20a%20denoising-diffusion%20objective.%20The%20ability%20of%20the%0Aresulting%20energy-based%20diffusion%20model%20to%20sample%20all%20intermediate%20ensembles%0Aallows%20us%20to%20perform%20TI%20from%20a%20single%20reference%20calculation.%20We%20apply%20our%0Amethod%20to%20Lennard-Jones%20fluids%2C%20where%20we%20report%20accurate%20calculations%20of%20the%0Aexcess%20chemical%20potential%2C%20demonstrating%20that%20Neural%20TI%20is%20capable%20of%20coupling%0Ahundreds%20of%20degrees%20of%20freedom%20at%20once.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02313v2&entry.124074799=Read"},
{"title": "Differentiable Cost-Parameterized Monge Map Estimators", "author": "Samuel Howard and George Deligiannidis and Patrick Rebeschini and James Thornton", "abstract": "  Within the field of optimal transport (OT), the choice of ground cost is\ncrucial to ensuring that the optimality of a transport map corresponds to\nusefulness in real-world applications. It is therefore desirable to use known\ninformation to tailor cost functions and hence learn OT maps which are adapted\nto the problem at hand. By considering a class of neural ground costs whose\nMonge maps have a known form, we construct a differentiable Monge map estimator\nwhich can be optimized to be consistent with known information about an OT map.\nIn doing so, we simultaneously learn both an OT map estimator and a\ncorresponding adapted cost function. Through suitable choices of loss function,\nour method provides a general approach for incorporating prior information\nabout the Monge map itself when learning adapted OT maps and cost functions.\n", "link": "http://arxiv.org/abs/2406.08399v1", "date": "2024-06-12", "relevancy": 1.7351, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4763}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4291}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiable%20Cost-Parameterized%20Monge%20Map%20Estimators&body=Title%3A%20Differentiable%20Cost-Parameterized%20Monge%20Map%20Estimators%0AAuthor%3A%20Samuel%20Howard%20and%20George%20Deligiannidis%20and%20Patrick%20Rebeschini%20and%20James%20Thornton%0AAbstract%3A%20%20%20Within%20the%20field%20of%20optimal%20transport%20%28OT%29%2C%20the%20choice%20of%20ground%20cost%20is%0Acrucial%20to%20ensuring%20that%20the%20optimality%20of%20a%20transport%20map%20corresponds%20to%0Ausefulness%20in%20real-world%20applications.%20It%20is%20therefore%20desirable%20to%20use%20known%0Ainformation%20to%20tailor%20cost%20functions%20and%20hence%20learn%20OT%20maps%20which%20are%20adapted%0Ato%20the%20problem%20at%20hand.%20By%20considering%20a%20class%20of%20neural%20ground%20costs%20whose%0AMonge%20maps%20have%20a%20known%20form%2C%20we%20construct%20a%20differentiable%20Monge%20map%20estimator%0Awhich%20can%20be%20optimized%20to%20be%20consistent%20with%20known%20information%20about%20an%20OT%20map.%0AIn%20doing%20so%2C%20we%20simultaneously%20learn%20both%20an%20OT%20map%20estimator%20and%20a%0Acorresponding%20adapted%20cost%20function.%20Through%20suitable%20choices%20of%20loss%20function%2C%0Aour%20method%20provides%20a%20general%20approach%20for%20incorporating%20prior%20information%0Aabout%20the%20Monge%20map%20itself%20when%20learning%20adapted%20OT%20maps%20and%20cost%20functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiable%2520Cost-Parameterized%2520Monge%2520Map%2520Estimators%26entry.906535625%3DSamuel%2520Howard%2520and%2520George%2520Deligiannidis%2520and%2520Patrick%2520Rebeschini%2520and%2520James%2520Thornton%26entry.1292438233%3D%2520%2520Within%2520the%2520field%2520of%2520optimal%2520transport%2520%2528OT%2529%252C%2520the%2520choice%2520of%2520ground%2520cost%2520is%250Acrucial%2520to%2520ensuring%2520that%2520the%2520optimality%2520of%2520a%2520transport%2520map%2520corresponds%2520to%250Ausefulness%2520in%2520real-world%2520applications.%2520It%2520is%2520therefore%2520desirable%2520to%2520use%2520known%250Ainformation%2520to%2520tailor%2520cost%2520functions%2520and%2520hence%2520learn%2520OT%2520maps%2520which%2520are%2520adapted%250Ato%2520the%2520problem%2520at%2520hand.%2520By%2520considering%2520a%2520class%2520of%2520neural%2520ground%2520costs%2520whose%250AMonge%2520maps%2520have%2520a%2520known%2520form%252C%2520we%2520construct%2520a%2520differentiable%2520Monge%2520map%2520estimator%250Awhich%2520can%2520be%2520optimized%2520to%2520be%2520consistent%2520with%2520known%2520information%2520about%2520an%2520OT%2520map.%250AIn%2520doing%2520so%252C%2520we%2520simultaneously%2520learn%2520both%2520an%2520OT%2520map%2520estimator%2520and%2520a%250Acorresponding%2520adapted%2520cost%2520function.%2520Through%2520suitable%2520choices%2520of%2520loss%2520function%252C%250Aour%2520method%2520provides%2520a%2520general%2520approach%2520for%2520incorporating%2520prior%2520information%250Aabout%2520the%2520Monge%2520map%2520itself%2520when%2520learning%2520adapted%2520OT%2520maps%2520and%2520cost%2520functions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiable%20Cost-Parameterized%20Monge%20Map%20Estimators&entry.906535625=Samuel%20Howard%20and%20George%20Deligiannidis%20and%20Patrick%20Rebeschini%20and%20James%20Thornton&entry.1292438233=%20%20Within%20the%20field%20of%20optimal%20transport%20%28OT%29%2C%20the%20choice%20of%20ground%20cost%20is%0Acrucial%20to%20ensuring%20that%20the%20optimality%20of%20a%20transport%20map%20corresponds%20to%0Ausefulness%20in%20real-world%20applications.%20It%20is%20therefore%20desirable%20to%20use%20known%0Ainformation%20to%20tailor%20cost%20functions%20and%20hence%20learn%20OT%20maps%20which%20are%20adapted%0Ato%20the%20problem%20at%20hand.%20By%20considering%20a%20class%20of%20neural%20ground%20costs%20whose%0AMonge%20maps%20have%20a%20known%20form%2C%20we%20construct%20a%20differentiable%20Monge%20map%20estimator%0Awhich%20can%20be%20optimized%20to%20be%20consistent%20with%20known%20information%20about%20an%20OT%20map.%0AIn%20doing%20so%2C%20we%20simultaneously%20learn%20both%20an%20OT%20map%20estimator%20and%20a%0Acorresponding%20adapted%20cost%20function.%20Through%20suitable%20choices%20of%20loss%20function%2C%0Aour%20method%20provides%20a%20general%20approach%20for%20incorporating%20prior%20information%0Aabout%20the%20Monge%20map%20itself%20when%20learning%20adapted%20OT%20maps%20and%20cost%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08399v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


