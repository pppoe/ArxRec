<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240725.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Streetscapes: Large-scale Consistent Street View Generation Using\n  Autoregressive Video Diffusion", "author": "Boyang Deng and Richard Tucker and Zhengqi Li and Leonidas Guibas and Noah Snavely and Gordon Wetzstein", "abstract": "  We present a method for generating Streetscapes-long sequences of views\nthrough an on-the-fly synthesized city-scale scene. Our generation is\nconditioned by language input (e.g., city name, weather), as well as an\nunderlying map/layout hosting the desired trajectory. Compared to recent models\nfor video generation or 3D view synthesis, our method can scale to much\nlonger-range camera trajectories, spanning several city blocks, while\nmaintaining visual quality and consistency. To achieve this goal, we build on\nrecent work on video diffusion, used within an autoregressive framework that\ncan easily scale to long sequences. In particular, we introduce a new temporal\nimputation method that prevents our autoregressive approach from drifting from\nthe distribution of realistic city imagery. We train our Streetscapes system on\na compelling source of data-posed imagery from Google Street View, along with\ncontextual map data-which allows users to generate city views conditioned on\nany desired city layout, with controllable camera poses. Please see more\nresults at our project page at https://boyangdeng.com/streetscapes.\n", "link": "http://arxiv.org/abs/2407.13759v2", "date": "2024-07-25", "relevancy": 3.1401, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6418}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6418}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Streetscapes%3A%20Large-scale%20Consistent%20Street%20View%20Generation%20Using%0A%20%20Autoregressive%20Video%20Diffusion&body=Title%3A%20Streetscapes%3A%20Large-scale%20Consistent%20Street%20View%20Generation%20Using%0A%20%20Autoregressive%20Video%20Diffusion%0AAuthor%3A%20Boyang%20Deng%20and%20Richard%20Tucker%20and%20Zhengqi%20Li%20and%20Leonidas%20Guibas%20and%20Noah%20Snavely%20and%20Gordon%20Wetzstein%0AAbstract%3A%20%20%20We%20present%20a%20method%20for%20generating%20Streetscapes-long%20sequences%20of%20views%0Athrough%20an%20on-the-fly%20synthesized%20city-scale%20scene.%20Our%20generation%20is%0Aconditioned%20by%20language%20input%20%28e.g.%2C%20city%20name%2C%20weather%29%2C%20as%20well%20as%20an%0Aunderlying%20map/layout%20hosting%20the%20desired%20trajectory.%20Compared%20to%20recent%20models%0Afor%20video%20generation%20or%203D%20view%20synthesis%2C%20our%20method%20can%20scale%20to%20much%0Alonger-range%20camera%20trajectories%2C%20spanning%20several%20city%20blocks%2C%20while%0Amaintaining%20visual%20quality%20and%20consistency.%20To%20achieve%20this%20goal%2C%20we%20build%20on%0Arecent%20work%20on%20video%20diffusion%2C%20used%20within%20an%20autoregressive%20framework%20that%0Acan%20easily%20scale%20to%20long%20sequences.%20In%20particular%2C%20we%20introduce%20a%20new%20temporal%0Aimputation%20method%20that%20prevents%20our%20autoregressive%20approach%20from%20drifting%20from%0Athe%20distribution%20of%20realistic%20city%20imagery.%20We%20train%20our%20Streetscapes%20system%20on%0Aa%20compelling%20source%20of%20data-posed%20imagery%20from%20Google%20Street%20View%2C%20along%20with%0Acontextual%20map%20data-which%20allows%20users%20to%20generate%20city%20views%20conditioned%20on%0Aany%20desired%20city%20layout%2C%20with%20controllable%20camera%20poses.%20Please%20see%20more%0Aresults%20at%20our%20project%20page%20at%20https%3A//boyangdeng.com/streetscapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13759v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreetscapes%253A%2520Large-scale%2520Consistent%2520Street%2520View%2520Generation%2520Using%250A%2520%2520Autoregressive%2520Video%2520Diffusion%26entry.906535625%3DBoyang%2520Deng%2520and%2520Richard%2520Tucker%2520and%2520Zhengqi%2520Li%2520and%2520Leonidas%2520Guibas%2520and%2520Noah%2520Snavely%2520and%2520Gordon%2520Wetzstein%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520method%2520for%2520generating%2520Streetscapes-long%2520sequences%2520of%2520views%250Athrough%2520an%2520on-the-fly%2520synthesized%2520city-scale%2520scene.%2520Our%2520generation%2520is%250Aconditioned%2520by%2520language%2520input%2520%2528e.g.%252C%2520city%2520name%252C%2520weather%2529%252C%2520as%2520well%2520as%2520an%250Aunderlying%2520map/layout%2520hosting%2520the%2520desired%2520trajectory.%2520Compared%2520to%2520recent%2520models%250Afor%2520video%2520generation%2520or%25203D%2520view%2520synthesis%252C%2520our%2520method%2520can%2520scale%2520to%2520much%250Alonger-range%2520camera%2520trajectories%252C%2520spanning%2520several%2520city%2520blocks%252C%2520while%250Amaintaining%2520visual%2520quality%2520and%2520consistency.%2520To%2520achieve%2520this%2520goal%252C%2520we%2520build%2520on%250Arecent%2520work%2520on%2520video%2520diffusion%252C%2520used%2520within%2520an%2520autoregressive%2520framework%2520that%250Acan%2520easily%2520scale%2520to%2520long%2520sequences.%2520In%2520particular%252C%2520we%2520introduce%2520a%2520new%2520temporal%250Aimputation%2520method%2520that%2520prevents%2520our%2520autoregressive%2520approach%2520from%2520drifting%2520from%250Athe%2520distribution%2520of%2520realistic%2520city%2520imagery.%2520We%2520train%2520our%2520Streetscapes%2520system%2520on%250Aa%2520compelling%2520source%2520of%2520data-posed%2520imagery%2520from%2520Google%2520Street%2520View%252C%2520along%2520with%250Acontextual%2520map%2520data-which%2520allows%2520users%2520to%2520generate%2520city%2520views%2520conditioned%2520on%250Aany%2520desired%2520city%2520layout%252C%2520with%2520controllable%2520camera%2520poses.%2520Please%2520see%2520more%250Aresults%2520at%2520our%2520project%2520page%2520at%2520https%253A//boyangdeng.com/streetscapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13759v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Streetscapes%3A%20Large-scale%20Consistent%20Street%20View%20Generation%20Using%0A%20%20Autoregressive%20Video%20Diffusion&entry.906535625=Boyang%20Deng%20and%20Richard%20Tucker%20and%20Zhengqi%20Li%20and%20Leonidas%20Guibas%20and%20Noah%20Snavely%20and%20Gordon%20Wetzstein&entry.1292438233=%20%20We%20present%20a%20method%20for%20generating%20Streetscapes-long%20sequences%20of%20views%0Athrough%20an%20on-the-fly%20synthesized%20city-scale%20scene.%20Our%20generation%20is%0Aconditioned%20by%20language%20input%20%28e.g.%2C%20city%20name%2C%20weather%29%2C%20as%20well%20as%20an%0Aunderlying%20map/layout%20hosting%20the%20desired%20trajectory.%20Compared%20to%20recent%20models%0Afor%20video%20generation%20or%203D%20view%20synthesis%2C%20our%20method%20can%20scale%20to%20much%0Alonger-range%20camera%20trajectories%2C%20spanning%20several%20city%20blocks%2C%20while%0Amaintaining%20visual%20quality%20and%20consistency.%20To%20achieve%20this%20goal%2C%20we%20build%20on%0Arecent%20work%20on%20video%20diffusion%2C%20used%20within%20an%20autoregressive%20framework%20that%0Acan%20easily%20scale%20to%20long%20sequences.%20In%20particular%2C%20we%20introduce%20a%20new%20temporal%0Aimputation%20method%20that%20prevents%20our%20autoregressive%20approach%20from%20drifting%20from%0Athe%20distribution%20of%20realistic%20city%20imagery.%20We%20train%20our%20Streetscapes%20system%20on%0Aa%20compelling%20source%20of%20data-posed%20imagery%20from%20Google%20Street%20View%2C%20along%20with%0Acontextual%20map%20data-which%20allows%20users%20to%20generate%20city%20views%20conditioned%20on%0Aany%20desired%20city%20layout%2C%20with%20controllable%20camera%20poses.%20Please%20see%20more%0Aresults%20at%20our%20project%20page%20at%20https%3A//boyangdeng.com/streetscapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13759v2&entry.124074799=Read"},
{"title": "Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian\n  Splatting", "author": "Jeongmin Bae and Seoha Kim and Youngsik Yun and Hahyun Lee and Gun Bang and Youngjung Uh", "abstract": "  As 3D Gaussian Splatting (3DGS) provides fast and high-quality novel view\nsynthesis, it is a natural extension to deform a canonical 3DGS to multiple\nframes for representing a dynamic scene. However, previous works fail to\naccurately reconstruct complex dynamic scenes. We attribute the failure to the\ndesign of the deformation field, which is built as a coordinate-based function.\nThis approach is problematic because 3DGS is a mixture of multiple fields\ncentered at the Gaussians, not just a single coordinate-based framework. To\nresolve this problem, we define the deformation as a function of per-Gaussian\nembeddings and temporal embeddings. Moreover, we decompose deformations as\ncoarse and fine deformations to model slow and fast movements, respectively.\nAlso, we introduce a local smoothness regularization for per-Gaussian embedding\nto improve the details in dynamic regions. Project page:\nhttps://jeongminb.github.io/e-d3dgs/\n", "link": "http://arxiv.org/abs/2404.03613v4", "date": "2024-07-25", "relevancy": 3.135, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6837}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6441}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Per-Gaussian%20Embedding-Based%20Deformation%20for%20Deformable%203D%20Gaussian%0A%20%20Splatting&body=Title%3A%20Per-Gaussian%20Embedding-Based%20Deformation%20for%20Deformable%203D%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Jeongmin%20Bae%20and%20Seoha%20Kim%20and%20Youngsik%20Yun%20and%20Hahyun%20Lee%20and%20Gun%20Bang%20and%20Youngjung%20Uh%0AAbstract%3A%20%20%20As%203D%20Gaussian%20Splatting%20%283DGS%29%20provides%20fast%20and%20high-quality%20novel%20view%0Asynthesis%2C%20it%20is%20a%20natural%20extension%20to%20deform%20a%20canonical%203DGS%20to%20multiple%0Aframes%20for%20representing%20a%20dynamic%20scene.%20However%2C%20previous%20works%20fail%20to%0Aaccurately%20reconstruct%20complex%20dynamic%20scenes.%20We%20attribute%20the%20failure%20to%20the%0Adesign%20of%20the%20deformation%20field%2C%20which%20is%20built%20as%20a%20coordinate-based%20function.%0AThis%20approach%20is%20problematic%20because%203DGS%20is%20a%20mixture%20of%20multiple%20fields%0Acentered%20at%20the%20Gaussians%2C%20not%20just%20a%20single%20coordinate-based%20framework.%20To%0Aresolve%20this%20problem%2C%20we%20define%20the%20deformation%20as%20a%20function%20of%20per-Gaussian%0Aembeddings%20and%20temporal%20embeddings.%20Moreover%2C%20we%20decompose%20deformations%20as%0Acoarse%20and%20fine%20deformations%20to%20model%20slow%20and%20fast%20movements%2C%20respectively.%0AAlso%2C%20we%20introduce%20a%20local%20smoothness%20regularization%20for%20per-Gaussian%20embedding%0Ato%20improve%20the%20details%20in%20dynamic%20regions.%20Project%20page%3A%0Ahttps%3A//jeongminb.github.io/e-d3dgs/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03613v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPer-Gaussian%2520Embedding-Based%2520Deformation%2520for%2520Deformable%25203D%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DJeongmin%2520Bae%2520and%2520Seoha%2520Kim%2520and%2520Youngsik%2520Yun%2520and%2520Hahyun%2520Lee%2520and%2520Gun%2520Bang%2520and%2520Youngjung%2520Uh%26entry.1292438233%3D%2520%2520As%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520provides%2520fast%2520and%2520high-quality%2520novel%2520view%250Asynthesis%252C%2520it%2520is%2520a%2520natural%2520extension%2520to%2520deform%2520a%2520canonical%25203DGS%2520to%2520multiple%250Aframes%2520for%2520representing%2520a%2520dynamic%2520scene.%2520However%252C%2520previous%2520works%2520fail%2520to%250Aaccurately%2520reconstruct%2520complex%2520dynamic%2520scenes.%2520We%2520attribute%2520the%2520failure%2520to%2520the%250Adesign%2520of%2520the%2520deformation%2520field%252C%2520which%2520is%2520built%2520as%2520a%2520coordinate-based%2520function.%250AThis%2520approach%2520is%2520problematic%2520because%25203DGS%2520is%2520a%2520mixture%2520of%2520multiple%2520fields%250Acentered%2520at%2520the%2520Gaussians%252C%2520not%2520just%2520a%2520single%2520coordinate-based%2520framework.%2520To%250Aresolve%2520this%2520problem%252C%2520we%2520define%2520the%2520deformation%2520as%2520a%2520function%2520of%2520per-Gaussian%250Aembeddings%2520and%2520temporal%2520embeddings.%2520Moreover%252C%2520we%2520decompose%2520deformations%2520as%250Acoarse%2520and%2520fine%2520deformations%2520to%2520model%2520slow%2520and%2520fast%2520movements%252C%2520respectively.%250AAlso%252C%2520we%2520introduce%2520a%2520local%2520smoothness%2520regularization%2520for%2520per-Gaussian%2520embedding%250Ato%2520improve%2520the%2520details%2520in%2520dynamic%2520regions.%2520Project%2520page%253A%250Ahttps%253A//jeongminb.github.io/e-d3dgs/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03613v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Per-Gaussian%20Embedding-Based%20Deformation%20for%20Deformable%203D%20Gaussian%0A%20%20Splatting&entry.906535625=Jeongmin%20Bae%20and%20Seoha%20Kim%20and%20Youngsik%20Yun%20and%20Hahyun%20Lee%20and%20Gun%20Bang%20and%20Youngjung%20Uh&entry.1292438233=%20%20As%203D%20Gaussian%20Splatting%20%283DGS%29%20provides%20fast%20and%20high-quality%20novel%20view%0Asynthesis%2C%20it%20is%20a%20natural%20extension%20to%20deform%20a%20canonical%203DGS%20to%20multiple%0Aframes%20for%20representing%20a%20dynamic%20scene.%20However%2C%20previous%20works%20fail%20to%0Aaccurately%20reconstruct%20complex%20dynamic%20scenes.%20We%20attribute%20the%20failure%20to%20the%0Adesign%20of%20the%20deformation%20field%2C%20which%20is%20built%20as%20a%20coordinate-based%20function.%0AThis%20approach%20is%20problematic%20because%203DGS%20is%20a%20mixture%20of%20multiple%20fields%0Acentered%20at%20the%20Gaussians%2C%20not%20just%20a%20single%20coordinate-based%20framework.%20To%0Aresolve%20this%20problem%2C%20we%20define%20the%20deformation%20as%20a%20function%20of%20per-Gaussian%0Aembeddings%20and%20temporal%20embeddings.%20Moreover%2C%20we%20decompose%20deformations%20as%0Acoarse%20and%20fine%20deformations%20to%20model%20slow%20and%20fast%20movements%2C%20respectively.%0AAlso%2C%20we%20introduce%20a%20local%20smoothness%20regularization%20for%20per-Gaussian%20embedding%0Ato%20improve%20the%20details%20in%20dynamic%20regions.%20Project%20page%3A%0Ahttps%3A//jeongminb.github.io/e-d3dgs/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03613v4&entry.124074799=Read"},
{"title": "VGGHeads: A Large-Scale Synthetic Dataset for 3D Human Heads", "author": "Orest Kupyn and Eugene Khvedchenia and Christian Rupprecht", "abstract": "  Human head detection, keypoint estimation, and 3D head model fitting are\nimportant tasks with many applications. However, traditional real-world\ndatasets often suffer from bias, privacy, and ethical concerns, and they have\nbeen recorded in laboratory environments, which makes it difficult for trained\nmodels to generalize. Here, we introduce VGGHeads -- a large scale synthetic\ndataset generated with diffusion models for human head detection and 3D mesh\nestimation. Our dataset comprises over 1 million high-resolution images, each\nannotated with detailed 3D head meshes, facial landmarks, and bounding boxes.\nUsing this dataset we introduce a new model architecture capable of\nsimultaneous heads detection and head meshes reconstruction from a single image\nin a single step. Through extensive experimental evaluations, we demonstrate\nthat models trained on our synthetic data achieve strong performance on real\nimages. Furthermore, the versatility of our dataset makes it applicable across\na broad spectrum of tasks, offering a general and comprehensive representation\nof human heads. Additionally, we provide detailed information about the\nsynthetic data generation pipeline, enabling it to be re-used for other tasks\nand domains.\n", "link": "http://arxiv.org/abs/2407.18245v1", "date": "2024-07-25", "relevancy": 3.0607, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6248}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6248}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VGGHeads%3A%20A%20Large-Scale%20Synthetic%20Dataset%20for%203D%20Human%20Heads&body=Title%3A%20VGGHeads%3A%20A%20Large-Scale%20Synthetic%20Dataset%20for%203D%20Human%20Heads%0AAuthor%3A%20Orest%20Kupyn%20and%20Eugene%20Khvedchenia%20and%20Christian%20Rupprecht%0AAbstract%3A%20%20%20Human%20head%20detection%2C%20keypoint%20estimation%2C%20and%203D%20head%20model%20fitting%20are%0Aimportant%20tasks%20with%20many%20applications.%20However%2C%20traditional%20real-world%0Adatasets%20often%20suffer%20from%20bias%2C%20privacy%2C%20and%20ethical%20concerns%2C%20and%20they%20have%0Abeen%20recorded%20in%20laboratory%20environments%2C%20which%20makes%20it%20difficult%20for%20trained%0Amodels%20to%20generalize.%20Here%2C%20we%20introduce%20VGGHeads%20--%20a%20large%20scale%20synthetic%0Adataset%20generated%20with%20diffusion%20models%20for%20human%20head%20detection%20and%203D%20mesh%0Aestimation.%20Our%20dataset%20comprises%20over%201%20million%20high-resolution%20images%2C%20each%0Aannotated%20with%20detailed%203D%20head%20meshes%2C%20facial%20landmarks%2C%20and%20bounding%20boxes.%0AUsing%20this%20dataset%20we%20introduce%20a%20new%20model%20architecture%20capable%20of%0Asimultaneous%20heads%20detection%20and%20head%20meshes%20reconstruction%20from%20a%20single%20image%0Ain%20a%20single%20step.%20Through%20extensive%20experimental%20evaluations%2C%20we%20demonstrate%0Athat%20models%20trained%20on%20our%20synthetic%20data%20achieve%20strong%20performance%20on%20real%0Aimages.%20Furthermore%2C%20the%20versatility%20of%20our%20dataset%20makes%20it%20applicable%20across%0Aa%20broad%20spectrum%20of%20tasks%2C%20offering%20a%20general%20and%20comprehensive%20representation%0Aof%20human%20heads.%20Additionally%2C%20we%20provide%20detailed%20information%20about%20the%0Asynthetic%20data%20generation%20pipeline%2C%20enabling%20it%20to%20be%20re-used%20for%20other%20tasks%0Aand%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18245v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVGGHeads%253A%2520A%2520Large-Scale%2520Synthetic%2520Dataset%2520for%25203D%2520Human%2520Heads%26entry.906535625%3DOrest%2520Kupyn%2520and%2520Eugene%2520Khvedchenia%2520and%2520Christian%2520Rupprecht%26entry.1292438233%3D%2520%2520Human%2520head%2520detection%252C%2520keypoint%2520estimation%252C%2520and%25203D%2520head%2520model%2520fitting%2520are%250Aimportant%2520tasks%2520with%2520many%2520applications.%2520However%252C%2520traditional%2520real-world%250Adatasets%2520often%2520suffer%2520from%2520bias%252C%2520privacy%252C%2520and%2520ethical%2520concerns%252C%2520and%2520they%2520have%250Abeen%2520recorded%2520in%2520laboratory%2520environments%252C%2520which%2520makes%2520it%2520difficult%2520for%2520trained%250Amodels%2520to%2520generalize.%2520Here%252C%2520we%2520introduce%2520VGGHeads%2520--%2520a%2520large%2520scale%2520synthetic%250Adataset%2520generated%2520with%2520diffusion%2520models%2520for%2520human%2520head%2520detection%2520and%25203D%2520mesh%250Aestimation.%2520Our%2520dataset%2520comprises%2520over%25201%2520million%2520high-resolution%2520images%252C%2520each%250Aannotated%2520with%2520detailed%25203D%2520head%2520meshes%252C%2520facial%2520landmarks%252C%2520and%2520bounding%2520boxes.%250AUsing%2520this%2520dataset%2520we%2520introduce%2520a%2520new%2520model%2520architecture%2520capable%2520of%250Asimultaneous%2520heads%2520detection%2520and%2520head%2520meshes%2520reconstruction%2520from%2520a%2520single%2520image%250Ain%2520a%2520single%2520step.%2520Through%2520extensive%2520experimental%2520evaluations%252C%2520we%2520demonstrate%250Athat%2520models%2520trained%2520on%2520our%2520synthetic%2520data%2520achieve%2520strong%2520performance%2520on%2520real%250Aimages.%2520Furthermore%252C%2520the%2520versatility%2520of%2520our%2520dataset%2520makes%2520it%2520applicable%2520across%250Aa%2520broad%2520spectrum%2520of%2520tasks%252C%2520offering%2520a%2520general%2520and%2520comprehensive%2520representation%250Aof%2520human%2520heads.%2520Additionally%252C%2520we%2520provide%2520detailed%2520information%2520about%2520the%250Asynthetic%2520data%2520generation%2520pipeline%252C%2520enabling%2520it%2520to%2520be%2520re-used%2520for%2520other%2520tasks%250Aand%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18245v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VGGHeads%3A%20A%20Large-Scale%20Synthetic%20Dataset%20for%203D%20Human%20Heads&entry.906535625=Orest%20Kupyn%20and%20Eugene%20Khvedchenia%20and%20Christian%20Rupprecht&entry.1292438233=%20%20Human%20head%20detection%2C%20keypoint%20estimation%2C%20and%203D%20head%20model%20fitting%20are%0Aimportant%20tasks%20with%20many%20applications.%20However%2C%20traditional%20real-world%0Adatasets%20often%20suffer%20from%20bias%2C%20privacy%2C%20and%20ethical%20concerns%2C%20and%20they%20have%0Abeen%20recorded%20in%20laboratory%20environments%2C%20which%20makes%20it%20difficult%20for%20trained%0Amodels%20to%20generalize.%20Here%2C%20we%20introduce%20VGGHeads%20--%20a%20large%20scale%20synthetic%0Adataset%20generated%20with%20diffusion%20models%20for%20human%20head%20detection%20and%203D%20mesh%0Aestimation.%20Our%20dataset%20comprises%20over%201%20million%20high-resolution%20images%2C%20each%0Aannotated%20with%20detailed%203D%20head%20meshes%2C%20facial%20landmarks%2C%20and%20bounding%20boxes.%0AUsing%20this%20dataset%20we%20introduce%20a%20new%20model%20architecture%20capable%20of%0Asimultaneous%20heads%20detection%20and%20head%20meshes%20reconstruction%20from%20a%20single%20image%0Ain%20a%20single%20step.%20Through%20extensive%20experimental%20evaluations%2C%20we%20demonstrate%0Athat%20models%20trained%20on%20our%20synthetic%20data%20achieve%20strong%20performance%20on%20real%0Aimages.%20Furthermore%2C%20the%20versatility%20of%20our%20dataset%20makes%20it%20applicable%20across%0Aa%20broad%20spectrum%20of%20tasks%2C%20offering%20a%20general%20and%20comprehensive%20representation%0Aof%20human%20heads.%20Additionally%2C%20we%20provide%20detailed%20information%20about%20the%0Asynthetic%20data%20generation%20pipeline%2C%20enabling%20it%20to%20be%20re-used%20for%20other%20tasks%0Aand%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18245v1&entry.124074799=Read"},
{"title": "Holoported Characters: Real-time Free-viewpoint Rendering of Humans from\n  Sparse RGB Cameras", "author": "Ashwath Shetty and Marc Habermann and Guoxing Sun and Diogo Luvizon and Vladislav Golyanik and Christian Theobalt", "abstract": "  We present the first approach to render highly realistic free-viewpoint\nvideos of a human actor in general apparel, from sparse multi-view recording to\ndisplay, in real-time at an unprecedented 4K resolution. At inference, our\nmethod only requires four camera views of the moving actor and the respective\n3D skeletal pose. It handles actors in wide clothing, and reproduces even\nfine-scale dynamic detail, e.g. clothing wrinkles, face expressions, and hand\ngestures. At training time, our learning-based approach expects dense\nmulti-view video and a rigged static surface scan of the actor. Our method\ncomprises three main stages. Stage 1 is a skeleton-driven neural approach for\nhigh-quality capture of the detailed dynamic mesh geometry. Stage 2 is a novel\nsolution to create a view-dependent texture using four test-time camera views\nas input. Finally, stage 3 comprises a new image-based refinement network\nrendering the final 4K image given the output from the previous stages. Our\napproach establishes a new benchmark for real-time rendering resolution and\nquality using sparse input camera views, unlocking possibilities for immersive\ntelepresence.\n", "link": "http://arxiv.org/abs/2312.07423v2", "date": "2024-07-25", "relevancy": 3.0011, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6047}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5991}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Holoported%20Characters%3A%20Real-time%20Free-viewpoint%20Rendering%20of%20Humans%20from%0A%20%20Sparse%20RGB%20Cameras&body=Title%3A%20Holoported%20Characters%3A%20Real-time%20Free-viewpoint%20Rendering%20of%20Humans%20from%0A%20%20Sparse%20RGB%20Cameras%0AAuthor%3A%20Ashwath%20Shetty%20and%20Marc%20Habermann%20and%20Guoxing%20Sun%20and%20Diogo%20Luvizon%20and%20Vladislav%20Golyanik%20and%20Christian%20Theobalt%0AAbstract%3A%20%20%20We%20present%20the%20first%20approach%20to%20render%20highly%20realistic%20free-viewpoint%0Avideos%20of%20a%20human%20actor%20in%20general%20apparel%2C%20from%20sparse%20multi-view%20recording%20to%0Adisplay%2C%20in%20real-time%20at%20an%20unprecedented%204K%20resolution.%20At%20inference%2C%20our%0Amethod%20only%20requires%20four%20camera%20views%20of%20the%20moving%20actor%20and%20the%20respective%0A3D%20skeletal%20pose.%20It%20handles%20actors%20in%20wide%20clothing%2C%20and%20reproduces%20even%0Afine-scale%20dynamic%20detail%2C%20e.g.%20clothing%20wrinkles%2C%20face%20expressions%2C%20and%20hand%0Agestures.%20At%20training%20time%2C%20our%20learning-based%20approach%20expects%20dense%0Amulti-view%20video%20and%20a%20rigged%20static%20surface%20scan%20of%20the%20actor.%20Our%20method%0Acomprises%20three%20main%20stages.%20Stage%201%20is%20a%20skeleton-driven%20neural%20approach%20for%0Ahigh-quality%20capture%20of%20the%20detailed%20dynamic%20mesh%20geometry.%20Stage%202%20is%20a%20novel%0Asolution%20to%20create%20a%20view-dependent%20texture%20using%20four%20test-time%20camera%20views%0Aas%20input.%20Finally%2C%20stage%203%20comprises%20a%20new%20image-based%20refinement%20network%0Arendering%20the%20final%204K%20image%20given%20the%20output%20from%20the%20previous%20stages.%20Our%0Aapproach%20establishes%20a%20new%20benchmark%20for%20real-time%20rendering%20resolution%20and%0Aquality%20using%20sparse%20input%20camera%20views%2C%20unlocking%20possibilities%20for%20immersive%0Atelepresence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07423v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoloported%2520Characters%253A%2520Real-time%2520Free-viewpoint%2520Rendering%2520of%2520Humans%2520from%250A%2520%2520Sparse%2520RGB%2520Cameras%26entry.906535625%3DAshwath%2520Shetty%2520and%2520Marc%2520Habermann%2520and%2520Guoxing%2520Sun%2520and%2520Diogo%2520Luvizon%2520and%2520Vladislav%2520Golyanik%2520and%2520Christian%2520Theobalt%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520first%2520approach%2520to%2520render%2520highly%2520realistic%2520free-viewpoint%250Avideos%2520of%2520a%2520human%2520actor%2520in%2520general%2520apparel%252C%2520from%2520sparse%2520multi-view%2520recording%2520to%250Adisplay%252C%2520in%2520real-time%2520at%2520an%2520unprecedented%25204K%2520resolution.%2520At%2520inference%252C%2520our%250Amethod%2520only%2520requires%2520four%2520camera%2520views%2520of%2520the%2520moving%2520actor%2520and%2520the%2520respective%250A3D%2520skeletal%2520pose.%2520It%2520handles%2520actors%2520in%2520wide%2520clothing%252C%2520and%2520reproduces%2520even%250Afine-scale%2520dynamic%2520detail%252C%2520e.g.%2520clothing%2520wrinkles%252C%2520face%2520expressions%252C%2520and%2520hand%250Agestures.%2520At%2520training%2520time%252C%2520our%2520learning-based%2520approach%2520expects%2520dense%250Amulti-view%2520video%2520and%2520a%2520rigged%2520static%2520surface%2520scan%2520of%2520the%2520actor.%2520Our%2520method%250Acomprises%2520three%2520main%2520stages.%2520Stage%25201%2520is%2520a%2520skeleton-driven%2520neural%2520approach%2520for%250Ahigh-quality%2520capture%2520of%2520the%2520detailed%2520dynamic%2520mesh%2520geometry.%2520Stage%25202%2520is%2520a%2520novel%250Asolution%2520to%2520create%2520a%2520view-dependent%2520texture%2520using%2520four%2520test-time%2520camera%2520views%250Aas%2520input.%2520Finally%252C%2520stage%25203%2520comprises%2520a%2520new%2520image-based%2520refinement%2520network%250Arendering%2520the%2520final%25204K%2520image%2520given%2520the%2520output%2520from%2520the%2520previous%2520stages.%2520Our%250Aapproach%2520establishes%2520a%2520new%2520benchmark%2520for%2520real-time%2520rendering%2520resolution%2520and%250Aquality%2520using%2520sparse%2520input%2520camera%2520views%252C%2520unlocking%2520possibilities%2520for%2520immersive%250Atelepresence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07423v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Holoported%20Characters%3A%20Real-time%20Free-viewpoint%20Rendering%20of%20Humans%20from%0A%20%20Sparse%20RGB%20Cameras&entry.906535625=Ashwath%20Shetty%20and%20Marc%20Habermann%20and%20Guoxing%20Sun%20and%20Diogo%20Luvizon%20and%20Vladislav%20Golyanik%20and%20Christian%20Theobalt&entry.1292438233=%20%20We%20present%20the%20first%20approach%20to%20render%20highly%20realistic%20free-viewpoint%0Avideos%20of%20a%20human%20actor%20in%20general%20apparel%2C%20from%20sparse%20multi-view%20recording%20to%0Adisplay%2C%20in%20real-time%20at%20an%20unprecedented%204K%20resolution.%20At%20inference%2C%20our%0Amethod%20only%20requires%20four%20camera%20views%20of%20the%20moving%20actor%20and%20the%20respective%0A3D%20skeletal%20pose.%20It%20handles%20actors%20in%20wide%20clothing%2C%20and%20reproduces%20even%0Afine-scale%20dynamic%20detail%2C%20e.g.%20clothing%20wrinkles%2C%20face%20expressions%2C%20and%20hand%0Agestures.%20At%20training%20time%2C%20our%20learning-based%20approach%20expects%20dense%0Amulti-view%20video%20and%20a%20rigged%20static%20surface%20scan%20of%20the%20actor.%20Our%20method%0Acomprises%20three%20main%20stages.%20Stage%201%20is%20a%20skeleton-driven%20neural%20approach%20for%0Ahigh-quality%20capture%20of%20the%20detailed%20dynamic%20mesh%20geometry.%20Stage%202%20is%20a%20novel%0Asolution%20to%20create%20a%20view-dependent%20texture%20using%20four%20test-time%20camera%20views%0Aas%20input.%20Finally%2C%20stage%203%20comprises%20a%20new%20image-based%20refinement%20network%0Arendering%20the%20final%204K%20image%20given%20the%20output%20from%20the%20previous%20stages.%20Our%0Aapproach%20establishes%20a%20new%20benchmark%20for%20real-time%20rendering%20resolution%20and%0Aquality%20using%20sparse%20input%20camera%20views%2C%20unlocking%20possibilities%20for%20immersive%0Atelepresence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07423v2&entry.124074799=Read"},
{"title": "AttentionHand: Text-driven Controllable Hand Image Generation for 3D\n  Hand Reconstruction in the Wild", "author": "Junho Park and Kyeongbo Kong and Suk-Ju Kang", "abstract": "  Recently, there has been a significant amount of research conducted on 3D\nhand reconstruction to use various forms of human-computer interaction.\nHowever, 3D hand reconstruction in the wild is challenging due to extreme lack\nof in-the-wild 3D hand datasets. Especially, when hands are in complex pose\nsuch as interacting hands, the problems like appearance similarity, self-handed\noccclusion and depth ambiguity make it more difficult. To overcome these\nissues, we propose AttentionHand, a novel method for text-driven controllable\nhand image generation. Since AttentionHand can generate various and numerous\nin-the-wild hand images well-aligned with 3D hand label, we can acquire a new\n3D hand dataset, and can relieve the domain gap between indoor and outdoor\nscenes. Our method needs easy-to-use four modalities (i.e, an RGB image, a hand\nmesh image from 3D label, a bounding box, and a text prompt). These modalities\nare embedded into the latent space by the encoding phase. Then, through the\ntext attention stage, hand-related tokens from the given text prompt are\nattended to highlight hand-related regions of the latent embedding. After the\nhighlighted embedding is fed to the visual attention stage, hand-related\nregions in the embedding are attended by conditioning global and local hand\nmesh images with the diffusion-based pipeline. In the decoding phase, the final\nfeature is decoded to new hand images, which are well-aligned with the given\nhand mesh image and text prompt. As a result, AttentionHand achieved\nstate-of-the-art among text-to-hand image generation models, and the\nperformance of 3D hand mesh reconstruction was improved by additionally\ntraining with hand images generated by AttentionHand.\n", "link": "http://arxiv.org/abs/2407.18034v1", "date": "2024-07-25", "relevancy": 2.9608, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6138}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5901}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AttentionHand%3A%20Text-driven%20Controllable%20Hand%20Image%20Generation%20for%203D%0A%20%20Hand%20Reconstruction%20in%20the%20Wild&body=Title%3A%20AttentionHand%3A%20Text-driven%20Controllable%20Hand%20Image%20Generation%20for%203D%0A%20%20Hand%20Reconstruction%20in%20the%20Wild%0AAuthor%3A%20Junho%20Park%20and%20Kyeongbo%20Kong%20and%20Suk-Ju%20Kang%0AAbstract%3A%20%20%20Recently%2C%20there%20has%20been%20a%20significant%20amount%20of%20research%20conducted%20on%203D%0Ahand%20reconstruction%20to%20use%20various%20forms%20of%20human-computer%20interaction.%0AHowever%2C%203D%20hand%20reconstruction%20in%20the%20wild%20is%20challenging%20due%20to%20extreme%20lack%0Aof%20in-the-wild%203D%20hand%20datasets.%20Especially%2C%20when%20hands%20are%20in%20complex%20pose%0Asuch%20as%20interacting%20hands%2C%20the%20problems%20like%20appearance%20similarity%2C%20self-handed%0Aoccclusion%20and%20depth%20ambiguity%20make%20it%20more%20difficult.%20To%20overcome%20these%0Aissues%2C%20we%20propose%20AttentionHand%2C%20a%20novel%20method%20for%20text-driven%20controllable%0Ahand%20image%20generation.%20Since%20AttentionHand%20can%20generate%20various%20and%20numerous%0Ain-the-wild%20hand%20images%20well-aligned%20with%203D%20hand%20label%2C%20we%20can%20acquire%20a%20new%0A3D%20hand%20dataset%2C%20and%20can%20relieve%20the%20domain%20gap%20between%20indoor%20and%20outdoor%0Ascenes.%20Our%20method%20needs%20easy-to-use%20four%20modalities%20%28i.e%2C%20an%20RGB%20image%2C%20a%20hand%0Amesh%20image%20from%203D%20label%2C%20a%20bounding%20box%2C%20and%20a%20text%20prompt%29.%20These%20modalities%0Aare%20embedded%20into%20the%20latent%20space%20by%20the%20encoding%20phase.%20Then%2C%20through%20the%0Atext%20attention%20stage%2C%20hand-related%20tokens%20from%20the%20given%20text%20prompt%20are%0Aattended%20to%20highlight%20hand-related%20regions%20of%20the%20latent%20embedding.%20After%20the%0Ahighlighted%20embedding%20is%20fed%20to%20the%20visual%20attention%20stage%2C%20hand-related%0Aregions%20in%20the%20embedding%20are%20attended%20by%20conditioning%20global%20and%20local%20hand%0Amesh%20images%20with%20the%20diffusion-based%20pipeline.%20In%20the%20decoding%20phase%2C%20the%20final%0Afeature%20is%20decoded%20to%20new%20hand%20images%2C%20which%20are%20well-aligned%20with%20the%20given%0Ahand%20mesh%20image%20and%20text%20prompt.%20As%20a%20result%2C%20AttentionHand%20achieved%0Astate-of-the-art%20among%20text-to-hand%20image%20generation%20models%2C%20and%20the%0Aperformance%20of%203D%20hand%20mesh%20reconstruction%20was%20improved%20by%20additionally%0Atraining%20with%20hand%20images%20generated%20by%20AttentionHand.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttentionHand%253A%2520Text-driven%2520Controllable%2520Hand%2520Image%2520Generation%2520for%25203D%250A%2520%2520Hand%2520Reconstruction%2520in%2520the%2520Wild%26entry.906535625%3DJunho%2520Park%2520and%2520Kyeongbo%2520Kong%2520and%2520Suk-Ju%2520Kang%26entry.1292438233%3D%2520%2520Recently%252C%2520there%2520has%2520been%2520a%2520significant%2520amount%2520of%2520research%2520conducted%2520on%25203D%250Ahand%2520reconstruction%2520to%2520use%2520various%2520forms%2520of%2520human-computer%2520interaction.%250AHowever%252C%25203D%2520hand%2520reconstruction%2520in%2520the%2520wild%2520is%2520challenging%2520due%2520to%2520extreme%2520lack%250Aof%2520in-the-wild%25203D%2520hand%2520datasets.%2520Especially%252C%2520when%2520hands%2520are%2520in%2520complex%2520pose%250Asuch%2520as%2520interacting%2520hands%252C%2520the%2520problems%2520like%2520appearance%2520similarity%252C%2520self-handed%250Aoccclusion%2520and%2520depth%2520ambiguity%2520make%2520it%2520more%2520difficult.%2520To%2520overcome%2520these%250Aissues%252C%2520we%2520propose%2520AttentionHand%252C%2520a%2520novel%2520method%2520for%2520text-driven%2520controllable%250Ahand%2520image%2520generation.%2520Since%2520AttentionHand%2520can%2520generate%2520various%2520and%2520numerous%250Ain-the-wild%2520hand%2520images%2520well-aligned%2520with%25203D%2520hand%2520label%252C%2520we%2520can%2520acquire%2520a%2520new%250A3D%2520hand%2520dataset%252C%2520and%2520can%2520relieve%2520the%2520domain%2520gap%2520between%2520indoor%2520and%2520outdoor%250Ascenes.%2520Our%2520method%2520needs%2520easy-to-use%2520four%2520modalities%2520%2528i.e%252C%2520an%2520RGB%2520image%252C%2520a%2520hand%250Amesh%2520image%2520from%25203D%2520label%252C%2520a%2520bounding%2520box%252C%2520and%2520a%2520text%2520prompt%2529.%2520These%2520modalities%250Aare%2520embedded%2520into%2520the%2520latent%2520space%2520by%2520the%2520encoding%2520phase.%2520Then%252C%2520through%2520the%250Atext%2520attention%2520stage%252C%2520hand-related%2520tokens%2520from%2520the%2520given%2520text%2520prompt%2520are%250Aattended%2520to%2520highlight%2520hand-related%2520regions%2520of%2520the%2520latent%2520embedding.%2520After%2520the%250Ahighlighted%2520embedding%2520is%2520fed%2520to%2520the%2520visual%2520attention%2520stage%252C%2520hand-related%250Aregions%2520in%2520the%2520embedding%2520are%2520attended%2520by%2520conditioning%2520global%2520and%2520local%2520hand%250Amesh%2520images%2520with%2520the%2520diffusion-based%2520pipeline.%2520In%2520the%2520decoding%2520phase%252C%2520the%2520final%250Afeature%2520is%2520decoded%2520to%2520new%2520hand%2520images%252C%2520which%2520are%2520well-aligned%2520with%2520the%2520given%250Ahand%2520mesh%2520image%2520and%2520text%2520prompt.%2520As%2520a%2520result%252C%2520AttentionHand%2520achieved%250Astate-of-the-art%2520among%2520text-to-hand%2520image%2520generation%2520models%252C%2520and%2520the%250Aperformance%2520of%25203D%2520hand%2520mesh%2520reconstruction%2520was%2520improved%2520by%2520additionally%250Atraining%2520with%2520hand%2520images%2520generated%2520by%2520AttentionHand.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AttentionHand%3A%20Text-driven%20Controllable%20Hand%20Image%20Generation%20for%203D%0A%20%20Hand%20Reconstruction%20in%20the%20Wild&entry.906535625=Junho%20Park%20and%20Kyeongbo%20Kong%20and%20Suk-Ju%20Kang&entry.1292438233=%20%20Recently%2C%20there%20has%20been%20a%20significant%20amount%20of%20research%20conducted%20on%203D%0Ahand%20reconstruction%20to%20use%20various%20forms%20of%20human-computer%20interaction.%0AHowever%2C%203D%20hand%20reconstruction%20in%20the%20wild%20is%20challenging%20due%20to%20extreme%20lack%0Aof%20in-the-wild%203D%20hand%20datasets.%20Especially%2C%20when%20hands%20are%20in%20complex%20pose%0Asuch%20as%20interacting%20hands%2C%20the%20problems%20like%20appearance%20similarity%2C%20self-handed%0Aoccclusion%20and%20depth%20ambiguity%20make%20it%20more%20difficult.%20To%20overcome%20these%0Aissues%2C%20we%20propose%20AttentionHand%2C%20a%20novel%20method%20for%20text-driven%20controllable%0Ahand%20image%20generation.%20Since%20AttentionHand%20can%20generate%20various%20and%20numerous%0Ain-the-wild%20hand%20images%20well-aligned%20with%203D%20hand%20label%2C%20we%20can%20acquire%20a%20new%0A3D%20hand%20dataset%2C%20and%20can%20relieve%20the%20domain%20gap%20between%20indoor%20and%20outdoor%0Ascenes.%20Our%20method%20needs%20easy-to-use%20four%20modalities%20%28i.e%2C%20an%20RGB%20image%2C%20a%20hand%0Amesh%20image%20from%203D%20label%2C%20a%20bounding%20box%2C%20and%20a%20text%20prompt%29.%20These%20modalities%0Aare%20embedded%20into%20the%20latent%20space%20by%20the%20encoding%20phase.%20Then%2C%20through%20the%0Atext%20attention%20stage%2C%20hand-related%20tokens%20from%20the%20given%20text%20prompt%20are%0Aattended%20to%20highlight%20hand-related%20regions%20of%20the%20latent%20embedding.%20After%20the%0Ahighlighted%20embedding%20is%20fed%20to%20the%20visual%20attention%20stage%2C%20hand-related%0Aregions%20in%20the%20embedding%20are%20attended%20by%20conditioning%20global%20and%20local%20hand%0Amesh%20images%20with%20the%20diffusion-based%20pipeline.%20In%20the%20decoding%20phase%2C%20the%20final%0Afeature%20is%20decoded%20to%20new%20hand%20images%2C%20which%20are%20well-aligned%20with%20the%20given%0Ahand%20mesh%20image%20and%20text%20prompt.%20As%20a%20result%2C%20AttentionHand%20achieved%0Astate-of-the-art%20among%20text-to-hand%20image%20generation%20models%2C%20and%20the%0Aperformance%20of%203D%20hand%20mesh%20reconstruction%20was%20improved%20by%20additionally%0Atraining%20with%20hand%20images%20generated%20by%20AttentionHand.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18034v1&entry.124074799=Read"},
{"title": "SPOT: Scalable 3D Pre-training via Occupancy Prediction for Learning\n  Transferable 3D Representations", "author": "Xiangchao Yan and Runjian Chen and Bo Zhang and Hancheng Ye and Renqiu Xia and Jiakang Yuan and Hongbin Zhou and Xinyu Cai and Botian Shi and Wenqi Shao and Ping Luo and Yu Qiao and Tao Chen and Junchi Yan", "abstract": "  Annotating 3D LiDAR point clouds for perception tasks is fundamental for many\napplications e.g., autonomous driving, yet it still remains notoriously\nlabor-intensive. Pretraining-finetuning approach can alleviate the labeling\nburden by fine-tuning a pre-trained backbone across various downstream datasets\nas well as tasks. In this paper, we propose SPOT, namely Scalable Pre-training\nvia Occupancy prediction for learning Transferable 3D representations under\nsuch a label-efficient fine-tuning paradigm. SPOT achieves effectiveness on\nvarious public datasets with different downstream tasks, showcasing its general\nrepresentation power, cross-domain robustness and data scalability which are\nthree key factors for real-world application. Specifically, we both\ntheoretically and empirically show, for the first time, that general\nrepresentations learning can be achieved through the task of occupancy\nprediction. Then, to address the domain gap caused by different LiDAR sensors\nand annotation methods, we develop a beam re-sampling technique for point cloud\naugmentation combined with class-balancing strategy. Furthermore, scalable\npre-training is observed, that is, the downstream performance across all the\nexperiments gets better with more pre-training data. Additionally, such\npre-training strategy also remains compatible with unlabeled data. The hope is\nthat our findings will facilitate the understanding of LiDAR points and pave\nthe way for future advancements in LiDAR pre-training.\n", "link": "http://arxiv.org/abs/2309.10527v3", "date": "2024-07-25", "relevancy": 2.9351, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5962}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5836}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPOT%3A%20Scalable%203D%20Pre-training%20via%20Occupancy%20Prediction%20for%20Learning%0A%20%20Transferable%203D%20Representations&body=Title%3A%20SPOT%3A%20Scalable%203D%20Pre-training%20via%20Occupancy%20Prediction%20for%20Learning%0A%20%20Transferable%203D%20Representations%0AAuthor%3A%20Xiangchao%20Yan%20and%20Runjian%20Chen%20and%20Bo%20Zhang%20and%20Hancheng%20Ye%20and%20Renqiu%20Xia%20and%20Jiakang%20Yuan%20and%20Hongbin%20Zhou%20and%20Xinyu%20Cai%20and%20Botian%20Shi%20and%20Wenqi%20Shao%20and%20Ping%20Luo%20and%20Yu%20Qiao%20and%20Tao%20Chen%20and%20Junchi%20Yan%0AAbstract%3A%20%20%20Annotating%203D%20LiDAR%20point%20clouds%20for%20perception%20tasks%20is%20fundamental%20for%20many%0Aapplications%20e.g.%2C%20autonomous%20driving%2C%20yet%20it%20still%20remains%20notoriously%0Alabor-intensive.%20Pretraining-finetuning%20approach%20can%20alleviate%20the%20labeling%0Aburden%20by%20fine-tuning%20a%20pre-trained%20backbone%20across%20various%20downstream%20datasets%0Aas%20well%20as%20tasks.%20In%20this%20paper%2C%20we%20propose%20SPOT%2C%20namely%20Scalable%20Pre-training%0Avia%20Occupancy%20prediction%20for%20learning%20Transferable%203D%20representations%20under%0Asuch%20a%20label-efficient%20fine-tuning%20paradigm.%20SPOT%20achieves%20effectiveness%20on%0Avarious%20public%20datasets%20with%20different%20downstream%20tasks%2C%20showcasing%20its%20general%0Arepresentation%20power%2C%20cross-domain%20robustness%20and%20data%20scalability%20which%20are%0Athree%20key%20factors%20for%20real-world%20application.%20Specifically%2C%20we%20both%0Atheoretically%20and%20empirically%20show%2C%20for%20the%20first%20time%2C%20that%20general%0Arepresentations%20learning%20can%20be%20achieved%20through%20the%20task%20of%20occupancy%0Aprediction.%20Then%2C%20to%20address%20the%20domain%20gap%20caused%20by%20different%20LiDAR%20sensors%0Aand%20annotation%20methods%2C%20we%20develop%20a%20beam%20re-sampling%20technique%20for%20point%20cloud%0Aaugmentation%20combined%20with%20class-balancing%20strategy.%20Furthermore%2C%20scalable%0Apre-training%20is%20observed%2C%20that%20is%2C%20the%20downstream%20performance%20across%20all%20the%0Aexperiments%20gets%20better%20with%20more%20pre-training%20data.%20Additionally%2C%20such%0Apre-training%20strategy%20also%20remains%20compatible%20with%20unlabeled%20data.%20The%20hope%20is%0Athat%20our%20findings%20will%20facilitate%20the%20understanding%20of%20LiDAR%20points%20and%20pave%0Athe%20way%20for%20future%20advancements%20in%20LiDAR%20pre-training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.10527v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPOT%253A%2520Scalable%25203D%2520Pre-training%2520via%2520Occupancy%2520Prediction%2520for%2520Learning%250A%2520%2520Transferable%25203D%2520Representations%26entry.906535625%3DXiangchao%2520Yan%2520and%2520Runjian%2520Chen%2520and%2520Bo%2520Zhang%2520and%2520Hancheng%2520Ye%2520and%2520Renqiu%2520Xia%2520and%2520Jiakang%2520Yuan%2520and%2520Hongbin%2520Zhou%2520and%2520Xinyu%2520Cai%2520and%2520Botian%2520Shi%2520and%2520Wenqi%2520Shao%2520and%2520Ping%2520Luo%2520and%2520Yu%2520Qiao%2520and%2520Tao%2520Chen%2520and%2520Junchi%2520Yan%26entry.1292438233%3D%2520%2520Annotating%25203D%2520LiDAR%2520point%2520clouds%2520for%2520perception%2520tasks%2520is%2520fundamental%2520for%2520many%250Aapplications%2520e.g.%252C%2520autonomous%2520driving%252C%2520yet%2520it%2520still%2520remains%2520notoriously%250Alabor-intensive.%2520Pretraining-finetuning%2520approach%2520can%2520alleviate%2520the%2520labeling%250Aburden%2520by%2520fine-tuning%2520a%2520pre-trained%2520backbone%2520across%2520various%2520downstream%2520datasets%250Aas%2520well%2520as%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520SPOT%252C%2520namely%2520Scalable%2520Pre-training%250Avia%2520Occupancy%2520prediction%2520for%2520learning%2520Transferable%25203D%2520representations%2520under%250Asuch%2520a%2520label-efficient%2520fine-tuning%2520paradigm.%2520SPOT%2520achieves%2520effectiveness%2520on%250Avarious%2520public%2520datasets%2520with%2520different%2520downstream%2520tasks%252C%2520showcasing%2520its%2520general%250Arepresentation%2520power%252C%2520cross-domain%2520robustness%2520and%2520data%2520scalability%2520which%2520are%250Athree%2520key%2520factors%2520for%2520real-world%2520application.%2520Specifically%252C%2520we%2520both%250Atheoretically%2520and%2520empirically%2520show%252C%2520for%2520the%2520first%2520time%252C%2520that%2520general%250Arepresentations%2520learning%2520can%2520be%2520achieved%2520through%2520the%2520task%2520of%2520occupancy%250Aprediction.%2520Then%252C%2520to%2520address%2520the%2520domain%2520gap%2520caused%2520by%2520different%2520LiDAR%2520sensors%250Aand%2520annotation%2520methods%252C%2520we%2520develop%2520a%2520beam%2520re-sampling%2520technique%2520for%2520point%2520cloud%250Aaugmentation%2520combined%2520with%2520class-balancing%2520strategy.%2520Furthermore%252C%2520scalable%250Apre-training%2520is%2520observed%252C%2520that%2520is%252C%2520the%2520downstream%2520performance%2520across%2520all%2520the%250Aexperiments%2520gets%2520better%2520with%2520more%2520pre-training%2520data.%2520Additionally%252C%2520such%250Apre-training%2520strategy%2520also%2520remains%2520compatible%2520with%2520unlabeled%2520data.%2520The%2520hope%2520is%250Athat%2520our%2520findings%2520will%2520facilitate%2520the%2520understanding%2520of%2520LiDAR%2520points%2520and%2520pave%250Athe%2520way%2520for%2520future%2520advancements%2520in%2520LiDAR%2520pre-training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.10527v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPOT%3A%20Scalable%203D%20Pre-training%20via%20Occupancy%20Prediction%20for%20Learning%0A%20%20Transferable%203D%20Representations&entry.906535625=Xiangchao%20Yan%20and%20Runjian%20Chen%20and%20Bo%20Zhang%20and%20Hancheng%20Ye%20and%20Renqiu%20Xia%20and%20Jiakang%20Yuan%20and%20Hongbin%20Zhou%20and%20Xinyu%20Cai%20and%20Botian%20Shi%20and%20Wenqi%20Shao%20and%20Ping%20Luo%20and%20Yu%20Qiao%20and%20Tao%20Chen%20and%20Junchi%20Yan&entry.1292438233=%20%20Annotating%203D%20LiDAR%20point%20clouds%20for%20perception%20tasks%20is%20fundamental%20for%20many%0Aapplications%20e.g.%2C%20autonomous%20driving%2C%20yet%20it%20still%20remains%20notoriously%0Alabor-intensive.%20Pretraining-finetuning%20approach%20can%20alleviate%20the%20labeling%0Aburden%20by%20fine-tuning%20a%20pre-trained%20backbone%20across%20various%20downstream%20datasets%0Aas%20well%20as%20tasks.%20In%20this%20paper%2C%20we%20propose%20SPOT%2C%20namely%20Scalable%20Pre-training%0Avia%20Occupancy%20prediction%20for%20learning%20Transferable%203D%20representations%20under%0Asuch%20a%20label-efficient%20fine-tuning%20paradigm.%20SPOT%20achieves%20effectiveness%20on%0Avarious%20public%20datasets%20with%20different%20downstream%20tasks%2C%20showcasing%20its%20general%0Arepresentation%20power%2C%20cross-domain%20robustness%20and%20data%20scalability%20which%20are%0Athree%20key%20factors%20for%20real-world%20application.%20Specifically%2C%20we%20both%0Atheoretically%20and%20empirically%20show%2C%20for%20the%20first%20time%2C%20that%20general%0Arepresentations%20learning%20can%20be%20achieved%20through%20the%20task%20of%20occupancy%0Aprediction.%20Then%2C%20to%20address%20the%20domain%20gap%20caused%20by%20different%20LiDAR%20sensors%0Aand%20annotation%20methods%2C%20we%20develop%20a%20beam%20re-sampling%20technique%20for%20point%20cloud%0Aaugmentation%20combined%20with%20class-balancing%20strategy.%20Furthermore%2C%20scalable%0Apre-training%20is%20observed%2C%20that%20is%2C%20the%20downstream%20performance%20across%20all%20the%0Aexperiments%20gets%20better%20with%20more%20pre-training%20data.%20Additionally%2C%20such%0Apre-training%20strategy%20also%20remains%20compatible%20with%20unlabeled%20data.%20The%20hope%20is%0Athat%20our%20findings%20will%20facilitate%20the%20understanding%20of%20LiDAR%20points%20and%20pave%0Athe%20way%20for%20future%20advancements%20in%20LiDAR%20pre-training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.10527v3&entry.124074799=Read"},
{"title": "MeshSegmenter: Zero-Shot Mesh Semantic Segmentation via Texture\n  Synthesis", "author": "Ziming Zhong and Yanxu Xu and Jing Li and Jiale Xu and Zhengxin Li and Chaohui Yu and Shenghua Gao", "abstract": "  We present MeshSegmenter, a simple yet effective framework designed for\nzero-shot 3D semantic segmentation. This model successfully extends the\npowerful capabilities of 2D segmentation models to 3D meshes, delivering\naccurate 3D segmentation across diverse meshes and segment descriptions.\nSpecifically, our model leverages the Segment Anything Model (SAM) model to\nsegment the target regions from images rendered from the 3D shape. In light of\nthe importance of the texture for segmentation, we also leverage the pretrained\nstable diffusion model to generate images with textures from 3D shape, and\nleverage SAM to segment the target regions from images with textures. Textures\nsupplement the shape for segmentation and facilitate accurate 3D segmentation\neven in geometrically non-prominent areas, such as segmenting a car door within\na car mesh. To achieve the 3D segments, we render 2D images from different\nviews and conduct segmentation for both textured and untextured images. Lastly,\nwe develop a multi-view revoting scheme that integrates 2D segmentation results\nand confidence scores from various views onto the 3D mesh, ensuring the 3D\nconsistency of segmentation results and eliminating inaccuracies from specific\nperspectives. Through these innovations, MeshSegmenter offers stable and\nreliable 3D segmentation results both quantitatively and qualitatively,\nhighlighting its potential as a transformative tool in the field of 3D\nzero-shot segmentation. The code is available at\n\\url{https://github.com/zimingzhong/MeshSegmenter}.\n", "link": "http://arxiv.org/abs/2407.13675v3", "date": "2024-07-25", "relevancy": 2.8725, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5904}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5836}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MeshSegmenter%3A%20Zero-Shot%20Mesh%20Semantic%20Segmentation%20via%20Texture%0A%20%20Synthesis&body=Title%3A%20MeshSegmenter%3A%20Zero-Shot%20Mesh%20Semantic%20Segmentation%20via%20Texture%0A%20%20Synthesis%0AAuthor%3A%20Ziming%20Zhong%20and%20Yanxu%20Xu%20and%20Jing%20Li%20and%20Jiale%20Xu%20and%20Zhengxin%20Li%20and%20Chaohui%20Yu%20and%20Shenghua%20Gao%0AAbstract%3A%20%20%20We%20present%20MeshSegmenter%2C%20a%20simple%20yet%20effective%20framework%20designed%20for%0Azero-shot%203D%20semantic%20segmentation.%20This%20model%20successfully%20extends%20the%0Apowerful%20capabilities%20of%202D%20segmentation%20models%20to%203D%20meshes%2C%20delivering%0Aaccurate%203D%20segmentation%20across%20diverse%20meshes%20and%20segment%20descriptions.%0ASpecifically%2C%20our%20model%20leverages%20the%20Segment%20Anything%20Model%20%28SAM%29%20model%20to%0Asegment%20the%20target%20regions%20from%20images%20rendered%20from%20the%203D%20shape.%20In%20light%20of%0Athe%20importance%20of%20the%20texture%20for%20segmentation%2C%20we%20also%20leverage%20the%20pretrained%0Astable%20diffusion%20model%20to%20generate%20images%20with%20textures%20from%203D%20shape%2C%20and%0Aleverage%20SAM%20to%20segment%20the%20target%20regions%20from%20images%20with%20textures.%20Textures%0Asupplement%20the%20shape%20for%20segmentation%20and%20facilitate%20accurate%203D%20segmentation%0Aeven%20in%20geometrically%20non-prominent%20areas%2C%20such%20as%20segmenting%20a%20car%20door%20within%0Aa%20car%20mesh.%20To%20achieve%20the%203D%20segments%2C%20we%20render%202D%20images%20from%20different%0Aviews%20and%20conduct%20segmentation%20for%20both%20textured%20and%20untextured%20images.%20Lastly%2C%0Awe%20develop%20a%20multi-view%20revoting%20scheme%20that%20integrates%202D%20segmentation%20results%0Aand%20confidence%20scores%20from%20various%20views%20onto%20the%203D%20mesh%2C%20ensuring%20the%203D%0Aconsistency%20of%20segmentation%20results%20and%20eliminating%20inaccuracies%20from%20specific%0Aperspectives.%20Through%20these%20innovations%2C%20MeshSegmenter%20offers%20stable%20and%0Areliable%203D%20segmentation%20results%20both%20quantitatively%20and%20qualitatively%2C%0Ahighlighting%20its%20potential%20as%20a%20transformative%20tool%20in%20the%20field%20of%203D%0Azero-shot%20segmentation.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/zimingzhong/MeshSegmenter%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13675v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeshSegmenter%253A%2520Zero-Shot%2520Mesh%2520Semantic%2520Segmentation%2520via%2520Texture%250A%2520%2520Synthesis%26entry.906535625%3DZiming%2520Zhong%2520and%2520Yanxu%2520Xu%2520and%2520Jing%2520Li%2520and%2520Jiale%2520Xu%2520and%2520Zhengxin%2520Li%2520and%2520Chaohui%2520Yu%2520and%2520Shenghua%2520Gao%26entry.1292438233%3D%2520%2520We%2520present%2520MeshSegmenter%252C%2520a%2520simple%2520yet%2520effective%2520framework%2520designed%2520for%250Azero-shot%25203D%2520semantic%2520segmentation.%2520This%2520model%2520successfully%2520extends%2520the%250Apowerful%2520capabilities%2520of%25202D%2520segmentation%2520models%2520to%25203D%2520meshes%252C%2520delivering%250Aaccurate%25203D%2520segmentation%2520across%2520diverse%2520meshes%2520and%2520segment%2520descriptions.%250ASpecifically%252C%2520our%2520model%2520leverages%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520model%2520to%250Asegment%2520the%2520target%2520regions%2520from%2520images%2520rendered%2520from%2520the%25203D%2520shape.%2520In%2520light%2520of%250Athe%2520importance%2520of%2520the%2520texture%2520for%2520segmentation%252C%2520we%2520also%2520leverage%2520the%2520pretrained%250Astable%2520diffusion%2520model%2520to%2520generate%2520images%2520with%2520textures%2520from%25203D%2520shape%252C%2520and%250Aleverage%2520SAM%2520to%2520segment%2520the%2520target%2520regions%2520from%2520images%2520with%2520textures.%2520Textures%250Asupplement%2520the%2520shape%2520for%2520segmentation%2520and%2520facilitate%2520accurate%25203D%2520segmentation%250Aeven%2520in%2520geometrically%2520non-prominent%2520areas%252C%2520such%2520as%2520segmenting%2520a%2520car%2520door%2520within%250Aa%2520car%2520mesh.%2520To%2520achieve%2520the%25203D%2520segments%252C%2520we%2520render%25202D%2520images%2520from%2520different%250Aviews%2520and%2520conduct%2520segmentation%2520for%2520both%2520textured%2520and%2520untextured%2520images.%2520Lastly%252C%250Awe%2520develop%2520a%2520multi-view%2520revoting%2520scheme%2520that%2520integrates%25202D%2520segmentation%2520results%250Aand%2520confidence%2520scores%2520from%2520various%2520views%2520onto%2520the%25203D%2520mesh%252C%2520ensuring%2520the%25203D%250Aconsistency%2520of%2520segmentation%2520results%2520and%2520eliminating%2520inaccuracies%2520from%2520specific%250Aperspectives.%2520Through%2520these%2520innovations%252C%2520MeshSegmenter%2520offers%2520stable%2520and%250Areliable%25203D%2520segmentation%2520results%2520both%2520quantitatively%2520and%2520qualitatively%252C%250Ahighlighting%2520its%2520potential%2520as%2520a%2520transformative%2520tool%2520in%2520the%2520field%2520of%25203D%250Azero-shot%2520segmentation.%2520The%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/zimingzhong/MeshSegmenter%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13675v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshSegmenter%3A%20Zero-Shot%20Mesh%20Semantic%20Segmentation%20via%20Texture%0A%20%20Synthesis&entry.906535625=Ziming%20Zhong%20and%20Yanxu%20Xu%20and%20Jing%20Li%20and%20Jiale%20Xu%20and%20Zhengxin%20Li%20and%20Chaohui%20Yu%20and%20Shenghua%20Gao&entry.1292438233=%20%20We%20present%20MeshSegmenter%2C%20a%20simple%20yet%20effective%20framework%20designed%20for%0Azero-shot%203D%20semantic%20segmentation.%20This%20model%20successfully%20extends%20the%0Apowerful%20capabilities%20of%202D%20segmentation%20models%20to%203D%20meshes%2C%20delivering%0Aaccurate%203D%20segmentation%20across%20diverse%20meshes%20and%20segment%20descriptions.%0ASpecifically%2C%20our%20model%20leverages%20the%20Segment%20Anything%20Model%20%28SAM%29%20model%20to%0Asegment%20the%20target%20regions%20from%20images%20rendered%20from%20the%203D%20shape.%20In%20light%20of%0Athe%20importance%20of%20the%20texture%20for%20segmentation%2C%20we%20also%20leverage%20the%20pretrained%0Astable%20diffusion%20model%20to%20generate%20images%20with%20textures%20from%203D%20shape%2C%20and%0Aleverage%20SAM%20to%20segment%20the%20target%20regions%20from%20images%20with%20textures.%20Textures%0Asupplement%20the%20shape%20for%20segmentation%20and%20facilitate%20accurate%203D%20segmentation%0Aeven%20in%20geometrically%20non-prominent%20areas%2C%20such%20as%20segmenting%20a%20car%20door%20within%0Aa%20car%20mesh.%20To%20achieve%20the%203D%20segments%2C%20we%20render%202D%20images%20from%20different%0Aviews%20and%20conduct%20segmentation%20for%20both%20textured%20and%20untextured%20images.%20Lastly%2C%0Awe%20develop%20a%20multi-view%20revoting%20scheme%20that%20integrates%202D%20segmentation%20results%0Aand%20confidence%20scores%20from%20various%20views%20onto%20the%203D%20mesh%2C%20ensuring%20the%203D%0Aconsistency%20of%20segmentation%20results%20and%20eliminating%20inaccuracies%20from%20specific%0Aperspectives.%20Through%20these%20innovations%2C%20MeshSegmenter%20offers%20stable%20and%0Areliable%203D%20segmentation%20results%20both%20quantitatively%20and%20qualitatively%2C%0Ahighlighting%20its%20potential%20as%20a%20transformative%20tool%20in%20the%20field%20of%203D%0Azero-shot%20segmentation.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/zimingzhong/MeshSegmenter%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13675v3&entry.124074799=Read"},
{"title": "Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric\n  Videos", "author": "Changan Chen and Puyuan Peng and Ami Baid and Zihui Xue and Wei-Ning Hsu and David Harwath and Kristen Grauman", "abstract": "  Generating realistic audio for human actions is important for many\napplications, such as creating sound effects for films or virtual reality\ngames. Existing approaches implicitly assume total correspondence between the\nvideo and audio during training, yet many sounds happen off-screen and have\nweak to no correspondence with the visuals -- resulting in uncontrolled ambient\nsounds or hallucinations at test time. We propose a novel ambient-aware audio\ngeneration model, AV-LDM. We devise a novel audio-conditioning mechanism to\nlearn to disentangle foreground action sounds from the ambient background\nsounds in in-the-wild training videos. Given a novel silent video, our model\nuses retrieval-augmented generation to create audio that matches the visual\ncontent both semantically and temporally. We train and evaluate our model on\ntwo in-the-wild egocentric video datasets, Ego4D and EPIC-KITCHENS, and we\nintroduce Ego4D-Sounds -- 1.2M curated clips with action-audio correspondence.\nOur model outperforms an array of existing methods, allows controllable\ngeneration of the ambient sound, and even shows promise for generalizing to\ncomputer graphics game clips. Overall, our approach is the first to focus\nvideo-to-audio generation faithfully on the observed visual content despite\ntraining from uncurated clips with natural background sounds.\n", "link": "http://arxiv.org/abs/2406.09272v3", "date": "2024-07-25", "relevancy": 2.851, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6075}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5564}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Action2Sound%3A%20Ambient-Aware%20Generation%20of%20Action%20Sounds%20from%20Egocentric%0A%20%20Videos&body=Title%3A%20Action2Sound%3A%20Ambient-Aware%20Generation%20of%20Action%20Sounds%20from%20Egocentric%0A%20%20Videos%0AAuthor%3A%20Changan%20Chen%20and%20Puyuan%20Peng%20and%20Ami%20Baid%20and%20Zihui%20Xue%20and%20Wei-Ning%20Hsu%20and%20David%20Harwath%20and%20Kristen%20Grauman%0AAbstract%3A%20%20%20Generating%20realistic%20audio%20for%20human%20actions%20is%20important%20for%20many%0Aapplications%2C%20such%20as%20creating%20sound%20effects%20for%20films%20or%20virtual%20reality%0Agames.%20Existing%20approaches%20implicitly%20assume%20total%20correspondence%20between%20the%0Avideo%20and%20audio%20during%20training%2C%20yet%20many%20sounds%20happen%20off-screen%20and%20have%0Aweak%20to%20no%20correspondence%20with%20the%20visuals%20--%20resulting%20in%20uncontrolled%20ambient%0Asounds%20or%20hallucinations%20at%20test%20time.%20We%20propose%20a%20novel%20ambient-aware%20audio%0Ageneration%20model%2C%20AV-LDM.%20We%20devise%20a%20novel%20audio-conditioning%20mechanism%20to%0Alearn%20to%20disentangle%20foreground%20action%20sounds%20from%20the%20ambient%20background%0Asounds%20in%20in-the-wild%20training%20videos.%20Given%20a%20novel%20silent%20video%2C%20our%20model%0Auses%20retrieval-augmented%20generation%20to%20create%20audio%20that%20matches%20the%20visual%0Acontent%20both%20semantically%20and%20temporally.%20We%20train%20and%20evaluate%20our%20model%20on%0Atwo%20in-the-wild%20egocentric%20video%20datasets%2C%20Ego4D%20and%20EPIC-KITCHENS%2C%20and%20we%0Aintroduce%20Ego4D-Sounds%20--%201.2M%20curated%20clips%20with%20action-audio%20correspondence.%0AOur%20model%20outperforms%20an%20array%20of%20existing%20methods%2C%20allows%20controllable%0Ageneration%20of%20the%20ambient%20sound%2C%20and%20even%20shows%20promise%20for%20generalizing%20to%0Acomputer%20graphics%20game%20clips.%20Overall%2C%20our%20approach%20is%20the%20first%20to%20focus%0Avideo-to-audio%20generation%20faithfully%20on%20the%20observed%20visual%20content%20despite%0Atraining%20from%20uncurated%20clips%20with%20natural%20background%20sounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09272v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAction2Sound%253A%2520Ambient-Aware%2520Generation%2520of%2520Action%2520Sounds%2520from%2520Egocentric%250A%2520%2520Videos%26entry.906535625%3DChangan%2520Chen%2520and%2520Puyuan%2520Peng%2520and%2520Ami%2520Baid%2520and%2520Zihui%2520Xue%2520and%2520Wei-Ning%2520Hsu%2520and%2520David%2520Harwath%2520and%2520Kristen%2520Grauman%26entry.1292438233%3D%2520%2520Generating%2520realistic%2520audio%2520for%2520human%2520actions%2520is%2520important%2520for%2520many%250Aapplications%252C%2520such%2520as%2520creating%2520sound%2520effects%2520for%2520films%2520or%2520virtual%2520reality%250Agames.%2520Existing%2520approaches%2520implicitly%2520assume%2520total%2520correspondence%2520between%2520the%250Avideo%2520and%2520audio%2520during%2520training%252C%2520yet%2520many%2520sounds%2520happen%2520off-screen%2520and%2520have%250Aweak%2520to%2520no%2520correspondence%2520with%2520the%2520visuals%2520--%2520resulting%2520in%2520uncontrolled%2520ambient%250Asounds%2520or%2520hallucinations%2520at%2520test%2520time.%2520We%2520propose%2520a%2520novel%2520ambient-aware%2520audio%250Ageneration%2520model%252C%2520AV-LDM.%2520We%2520devise%2520a%2520novel%2520audio-conditioning%2520mechanism%2520to%250Alearn%2520to%2520disentangle%2520foreground%2520action%2520sounds%2520from%2520the%2520ambient%2520background%250Asounds%2520in%2520in-the-wild%2520training%2520videos.%2520Given%2520a%2520novel%2520silent%2520video%252C%2520our%2520model%250Auses%2520retrieval-augmented%2520generation%2520to%2520create%2520audio%2520that%2520matches%2520the%2520visual%250Acontent%2520both%2520semantically%2520and%2520temporally.%2520We%2520train%2520and%2520evaluate%2520our%2520model%2520on%250Atwo%2520in-the-wild%2520egocentric%2520video%2520datasets%252C%2520Ego4D%2520and%2520EPIC-KITCHENS%252C%2520and%2520we%250Aintroduce%2520Ego4D-Sounds%2520--%25201.2M%2520curated%2520clips%2520with%2520action-audio%2520correspondence.%250AOur%2520model%2520outperforms%2520an%2520array%2520of%2520existing%2520methods%252C%2520allows%2520controllable%250Ageneration%2520of%2520the%2520ambient%2520sound%252C%2520and%2520even%2520shows%2520promise%2520for%2520generalizing%2520to%250Acomputer%2520graphics%2520game%2520clips.%2520Overall%252C%2520our%2520approach%2520is%2520the%2520first%2520to%2520focus%250Avideo-to-audio%2520generation%2520faithfully%2520on%2520the%2520observed%2520visual%2520content%2520despite%250Atraining%2520from%2520uncurated%2520clips%2520with%2520natural%2520background%2520sounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09272v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Action2Sound%3A%20Ambient-Aware%20Generation%20of%20Action%20Sounds%20from%20Egocentric%0A%20%20Videos&entry.906535625=Changan%20Chen%20and%20Puyuan%20Peng%20and%20Ami%20Baid%20and%20Zihui%20Xue%20and%20Wei-Ning%20Hsu%20and%20David%20Harwath%20and%20Kristen%20Grauman&entry.1292438233=%20%20Generating%20realistic%20audio%20for%20human%20actions%20is%20important%20for%20many%0Aapplications%2C%20such%20as%20creating%20sound%20effects%20for%20films%20or%20virtual%20reality%0Agames.%20Existing%20approaches%20implicitly%20assume%20total%20correspondence%20between%20the%0Avideo%20and%20audio%20during%20training%2C%20yet%20many%20sounds%20happen%20off-screen%20and%20have%0Aweak%20to%20no%20correspondence%20with%20the%20visuals%20--%20resulting%20in%20uncontrolled%20ambient%0Asounds%20or%20hallucinations%20at%20test%20time.%20We%20propose%20a%20novel%20ambient-aware%20audio%0Ageneration%20model%2C%20AV-LDM.%20We%20devise%20a%20novel%20audio-conditioning%20mechanism%20to%0Alearn%20to%20disentangle%20foreground%20action%20sounds%20from%20the%20ambient%20background%0Asounds%20in%20in-the-wild%20training%20videos.%20Given%20a%20novel%20silent%20video%2C%20our%20model%0Auses%20retrieval-augmented%20generation%20to%20create%20audio%20that%20matches%20the%20visual%0Acontent%20both%20semantically%20and%20temporally.%20We%20train%20and%20evaluate%20our%20model%20on%0Atwo%20in-the-wild%20egocentric%20video%20datasets%2C%20Ego4D%20and%20EPIC-KITCHENS%2C%20and%20we%0Aintroduce%20Ego4D-Sounds%20--%201.2M%20curated%20clips%20with%20action-audio%20correspondence.%0AOur%20model%20outperforms%20an%20array%20of%20existing%20methods%2C%20allows%20controllable%0Ageneration%20of%20the%20ambient%20sound%2C%20and%20even%20shows%20promise%20for%20generalizing%20to%0Acomputer%20graphics%20game%20clips.%20Overall%2C%20our%20approach%20is%20the%20first%20to%20focus%0Avideo-to-audio%20generation%20faithfully%20on%20the%20observed%20visual%20content%20despite%0Atraining%20from%20uncurated%20clips%20with%20natural%20background%20sounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09272v3&entry.124074799=Read"},
{"title": "Better Call SAL: Towards Learning to Segment Anything in Lidar", "author": "Aljo\u0161a O\u0161ep and Tim Meinhardt and Francesco Ferroni and Neehar Peri and Deva Ramanan and Laura Leal-Taix\u00e9", "abstract": "  We propose the SAL (Segment Anything in Lidar) method consisting of a\ntext-promptable zero-shot model for segmenting and classifying any object in\nLidar, and a pseudo-labeling engine that facilitates model training without\nmanual supervision. While the established paradigm for Lidar Panoptic\nSegmentation (LPS) relies on manual supervision for a handful of object classes\ndefined a priori, we utilize 2D vision foundation models to generate 3D\nsupervision ``for free''. Our pseudo-labels consist of instance masks and\ncorresponding CLIP tokens, which we lift to Lidar using calibrated multi-modal\ndata. By training our model on these labels, we distill the 2D foundation\nmodels into our Lidar SAL model. Even without manual labels, our model achieves\n$91\\%$ in terms of class-agnostic segmentation and $54\\%$ in terms of zero-shot\nLidar Panoptic Segmentation of the fully supervised state-of-the-art.\nFurthermore, we outperform several baselines that do not distill but only lift\nimage features to 3D. More importantly, we demonstrate that SAL supports\narbitrary class prompts, can be easily extended to new datasets, and shows\nsignificant potential to improve with increasing amounts of self-labeled data.\nCode and models are available at this\n$\\href{https://github.com/nv-dvl/segment-anything-lidar}{URL}$.\n", "link": "http://arxiv.org/abs/2403.13129v2", "date": "2024-07-25", "relevancy": 2.8222, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.571}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5628}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Better%20Call%20SAL%3A%20Towards%20Learning%20to%20Segment%20Anything%20in%20Lidar&body=Title%3A%20Better%20Call%20SAL%3A%20Towards%20Learning%20to%20Segment%20Anything%20in%20Lidar%0AAuthor%3A%20Aljo%C5%A1a%20O%C5%A1ep%20and%20Tim%20Meinhardt%20and%20Francesco%20Ferroni%20and%20Neehar%20Peri%20and%20Deva%20Ramanan%20and%20Laura%20Leal-Taix%C3%A9%0AAbstract%3A%20%20%20We%20propose%20the%20SAL%20%28Segment%20Anything%20in%20Lidar%29%20method%20consisting%20of%20a%0Atext-promptable%20zero-shot%20model%20for%20segmenting%20and%20classifying%20any%20object%20in%0ALidar%2C%20and%20a%20pseudo-labeling%20engine%20that%20facilitates%20model%20training%20without%0Amanual%20supervision.%20While%20the%20established%20paradigm%20for%20Lidar%20Panoptic%0ASegmentation%20%28LPS%29%20relies%20on%20manual%20supervision%20for%20a%20handful%20of%20object%20classes%0Adefined%20a%20priori%2C%20we%20utilize%202D%20vision%20foundation%20models%20to%20generate%203D%0Asupervision%20%60%60for%20free%27%27.%20Our%20pseudo-labels%20consist%20of%20instance%20masks%20and%0Acorresponding%20CLIP%20tokens%2C%20which%20we%20lift%20to%20Lidar%20using%20calibrated%20multi-modal%0Adata.%20By%20training%20our%20model%20on%20these%20labels%2C%20we%20distill%20the%202D%20foundation%0Amodels%20into%20our%20Lidar%20SAL%20model.%20Even%20without%20manual%20labels%2C%20our%20model%20achieves%0A%2491%5C%25%24%20in%20terms%20of%20class-agnostic%20segmentation%20and%20%2454%5C%25%24%20in%20terms%20of%20zero-shot%0ALidar%20Panoptic%20Segmentation%20of%20the%20fully%20supervised%20state-of-the-art.%0AFurthermore%2C%20we%20outperform%20several%20baselines%20that%20do%20not%20distill%20but%20only%20lift%0Aimage%20features%20to%203D.%20More%20importantly%2C%20we%20demonstrate%20that%20SAL%20supports%0Aarbitrary%20class%20prompts%2C%20can%20be%20easily%20extended%20to%20new%20datasets%2C%20and%20shows%0Asignificant%20potential%20to%20improve%20with%20increasing%20amounts%20of%20self-labeled%20data.%0ACode%20and%20models%20are%20available%20at%20this%0A%24%5Chref%7Bhttps%3A//github.com/nv-dvl/segment-anything-lidar%7D%7BURL%7D%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13129v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBetter%2520Call%2520SAL%253A%2520Towards%2520Learning%2520to%2520Segment%2520Anything%2520in%2520Lidar%26entry.906535625%3DAljo%25C5%25A1a%2520O%25C5%25A1ep%2520and%2520Tim%2520Meinhardt%2520and%2520Francesco%2520Ferroni%2520and%2520Neehar%2520Peri%2520and%2520Deva%2520Ramanan%2520and%2520Laura%2520Leal-Taix%25C3%25A9%26entry.1292438233%3D%2520%2520We%2520propose%2520the%2520SAL%2520%2528Segment%2520Anything%2520in%2520Lidar%2529%2520method%2520consisting%2520of%2520a%250Atext-promptable%2520zero-shot%2520model%2520for%2520segmenting%2520and%2520classifying%2520any%2520object%2520in%250ALidar%252C%2520and%2520a%2520pseudo-labeling%2520engine%2520that%2520facilitates%2520model%2520training%2520without%250Amanual%2520supervision.%2520While%2520the%2520established%2520paradigm%2520for%2520Lidar%2520Panoptic%250ASegmentation%2520%2528LPS%2529%2520relies%2520on%2520manual%2520supervision%2520for%2520a%2520handful%2520of%2520object%2520classes%250Adefined%2520a%2520priori%252C%2520we%2520utilize%25202D%2520vision%2520foundation%2520models%2520to%2520generate%25203D%250Asupervision%2520%2560%2560for%2520free%2527%2527.%2520Our%2520pseudo-labels%2520consist%2520of%2520instance%2520masks%2520and%250Acorresponding%2520CLIP%2520tokens%252C%2520which%2520we%2520lift%2520to%2520Lidar%2520using%2520calibrated%2520multi-modal%250Adata.%2520By%2520training%2520our%2520model%2520on%2520these%2520labels%252C%2520we%2520distill%2520the%25202D%2520foundation%250Amodels%2520into%2520our%2520Lidar%2520SAL%2520model.%2520Even%2520without%2520manual%2520labels%252C%2520our%2520model%2520achieves%250A%252491%255C%2525%2524%2520in%2520terms%2520of%2520class-agnostic%2520segmentation%2520and%2520%252454%255C%2525%2524%2520in%2520terms%2520of%2520zero-shot%250ALidar%2520Panoptic%2520Segmentation%2520of%2520the%2520fully%2520supervised%2520state-of-the-art.%250AFurthermore%252C%2520we%2520outperform%2520several%2520baselines%2520that%2520do%2520not%2520distill%2520but%2520only%2520lift%250Aimage%2520features%2520to%25203D.%2520More%2520importantly%252C%2520we%2520demonstrate%2520that%2520SAL%2520supports%250Aarbitrary%2520class%2520prompts%252C%2520can%2520be%2520easily%2520extended%2520to%2520new%2520datasets%252C%2520and%2520shows%250Asignificant%2520potential%2520to%2520improve%2520with%2520increasing%2520amounts%2520of%2520self-labeled%2520data.%250ACode%2520and%2520models%2520are%2520available%2520at%2520this%250A%2524%255Chref%257Bhttps%253A//github.com/nv-dvl/segment-anything-lidar%257D%257BURL%257D%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13129v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Better%20Call%20SAL%3A%20Towards%20Learning%20to%20Segment%20Anything%20in%20Lidar&entry.906535625=Aljo%C5%A1a%20O%C5%A1ep%20and%20Tim%20Meinhardt%20and%20Francesco%20Ferroni%20and%20Neehar%20Peri%20and%20Deva%20Ramanan%20and%20Laura%20Leal-Taix%C3%A9&entry.1292438233=%20%20We%20propose%20the%20SAL%20%28Segment%20Anything%20in%20Lidar%29%20method%20consisting%20of%20a%0Atext-promptable%20zero-shot%20model%20for%20segmenting%20and%20classifying%20any%20object%20in%0ALidar%2C%20and%20a%20pseudo-labeling%20engine%20that%20facilitates%20model%20training%20without%0Amanual%20supervision.%20While%20the%20established%20paradigm%20for%20Lidar%20Panoptic%0ASegmentation%20%28LPS%29%20relies%20on%20manual%20supervision%20for%20a%20handful%20of%20object%20classes%0Adefined%20a%20priori%2C%20we%20utilize%202D%20vision%20foundation%20models%20to%20generate%203D%0Asupervision%20%60%60for%20free%27%27.%20Our%20pseudo-labels%20consist%20of%20instance%20masks%20and%0Acorresponding%20CLIP%20tokens%2C%20which%20we%20lift%20to%20Lidar%20using%20calibrated%20multi-modal%0Adata.%20By%20training%20our%20model%20on%20these%20labels%2C%20we%20distill%20the%202D%20foundation%0Amodels%20into%20our%20Lidar%20SAL%20model.%20Even%20without%20manual%20labels%2C%20our%20model%20achieves%0A%2491%5C%25%24%20in%20terms%20of%20class-agnostic%20segmentation%20and%20%2454%5C%25%24%20in%20terms%20of%20zero-shot%0ALidar%20Panoptic%20Segmentation%20of%20the%20fully%20supervised%20state-of-the-art.%0AFurthermore%2C%20we%20outperform%20several%20baselines%20that%20do%20not%20distill%20but%20only%20lift%0Aimage%20features%20to%203D.%20More%20importantly%2C%20we%20demonstrate%20that%20SAL%20supports%0Aarbitrary%20class%20prompts%2C%20can%20be%20easily%20extended%20to%20new%20datasets%2C%20and%20shows%0Asignificant%20potential%20to%20improve%20with%20increasing%20amounts%20of%20self-labeled%20data.%0ACode%20and%20models%20are%20available%20at%20this%0A%24%5Chref%7Bhttps%3A//github.com/nv-dvl/segment-anything-lidar%7D%7BURL%7D%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13129v2&entry.124074799=Read"},
{"title": "LION: Linear Group RNN for 3D Object Detection in Point Clouds", "author": "Zhe Liu and Jinghua Hou and Xinyu Wang and Xiaoqing Ye and Jingdong Wang and Hengshuang Zhao and Xiang Bai", "abstract": "  The benefit of transformers in large-scale 3D point cloud perception tasks,\nsuch as 3D object detection, is limited by their quadratic computation cost\nwhen modeling long-range relationships. In contrast, linear RNNs have low\ncomputational complexity and are suitable for long-range modeling. Toward this\ngoal, we propose a simple and effective window-based framework built on LInear\ngrOup RNN (i.e., perform linear RNN for grouped features) for accurate 3D\nobject detection, called LION. The key property is to allow sufficient feature\ninteraction in a much larger group than transformer-based methods. However,\neffectively applying linear group RNN to 3D object detection in highly sparse\npoint clouds is not trivial due to its limitation in handling spatial modeling.\nTo tackle this problem, we simply introduce a 3D spatial feature descriptor and\nintegrate it into the linear group RNN operators to enhance their spatial\nfeatures rather than blindly increasing the number of scanning orders for voxel\nfeatures. To further address the challenge in highly sparse point clouds, we\npropose a 3D voxel generation strategy to densify foreground features thanks to\nlinear group RNN as a natural property of auto-regressive models. Extensive\nexperiments verify the effectiveness of the proposed components and the\ngeneralization of our LION on different linear group RNN operators including\nMamba, RWKV, and RetNet. Furthermore, it is worth mentioning that our\nLION-Mamba achieves state-of-the-art on Waymo, nuScenes, Argoverse V2, and ONCE\ndataset. Last but not least, our method supports kinds of advanced linear RNN\noperators (e.g., RetNet, RWKV, Mamba, xLSTM and TTT) on small but popular KITTI\ndataset for a quick experience with our linear RNN-based framework.\n", "link": "http://arxiv.org/abs/2407.18232v1", "date": "2024-07-25", "relevancy": 2.8176, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6072}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5513}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LION%3A%20Linear%20Group%20RNN%20for%203D%20Object%20Detection%20in%20Point%20Clouds&body=Title%3A%20LION%3A%20Linear%20Group%20RNN%20for%203D%20Object%20Detection%20in%20Point%20Clouds%0AAuthor%3A%20Zhe%20Liu%20and%20Jinghua%20Hou%20and%20Xinyu%20Wang%20and%20Xiaoqing%20Ye%20and%20Jingdong%20Wang%20and%20Hengshuang%20Zhao%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20The%20benefit%20of%20transformers%20in%20large-scale%203D%20point%20cloud%20perception%20tasks%2C%0Asuch%20as%203D%20object%20detection%2C%20is%20limited%20by%20their%20quadratic%20computation%20cost%0Awhen%20modeling%20long-range%20relationships.%20In%20contrast%2C%20linear%20RNNs%20have%20low%0Acomputational%20complexity%20and%20are%20suitable%20for%20long-range%20modeling.%20Toward%20this%0Agoal%2C%20we%20propose%20a%20simple%20and%20effective%20window-based%20framework%20built%20on%20LInear%0AgrOup%20RNN%20%28i.e.%2C%20perform%20linear%20RNN%20for%20grouped%20features%29%20for%20accurate%203D%0Aobject%20detection%2C%20called%20LION.%20The%20key%20property%20is%20to%20allow%20sufficient%20feature%0Ainteraction%20in%20a%20much%20larger%20group%20than%20transformer-based%20methods.%20However%2C%0Aeffectively%20applying%20linear%20group%20RNN%20to%203D%20object%20detection%20in%20highly%20sparse%0Apoint%20clouds%20is%20not%20trivial%20due%20to%20its%20limitation%20in%20handling%20spatial%20modeling.%0ATo%20tackle%20this%20problem%2C%20we%20simply%20introduce%20a%203D%20spatial%20feature%20descriptor%20and%0Aintegrate%20it%20into%20the%20linear%20group%20RNN%20operators%20to%20enhance%20their%20spatial%0Afeatures%20rather%20than%20blindly%20increasing%20the%20number%20of%20scanning%20orders%20for%20voxel%0Afeatures.%20To%20further%20address%20the%20challenge%20in%20highly%20sparse%20point%20clouds%2C%20we%0Apropose%20a%203D%20voxel%20generation%20strategy%20to%20densify%20foreground%20features%20thanks%20to%0Alinear%20group%20RNN%20as%20a%20natural%20property%20of%20auto-regressive%20models.%20Extensive%0Aexperiments%20verify%20the%20effectiveness%20of%20the%20proposed%20components%20and%20the%0Ageneralization%20of%20our%20LION%20on%20different%20linear%20group%20RNN%20operators%20including%0AMamba%2C%20RWKV%2C%20and%20RetNet.%20Furthermore%2C%20it%20is%20worth%20mentioning%20that%20our%0ALION-Mamba%20achieves%20state-of-the-art%20on%20Waymo%2C%20nuScenes%2C%20Argoverse%20V2%2C%20and%20ONCE%0Adataset.%20Last%20but%20not%20least%2C%20our%20method%20supports%20kinds%20of%20advanced%20linear%20RNN%0Aoperators%20%28e.g.%2C%20RetNet%2C%20RWKV%2C%20Mamba%2C%20xLSTM%20and%20TTT%29%20on%20small%20but%20popular%20KITTI%0Adataset%20for%20a%20quick%20experience%20with%20our%20linear%20RNN-based%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLION%253A%2520Linear%2520Group%2520RNN%2520for%25203D%2520Object%2520Detection%2520in%2520Point%2520Clouds%26entry.906535625%3DZhe%2520Liu%2520and%2520Jinghua%2520Hou%2520and%2520Xinyu%2520Wang%2520and%2520Xiaoqing%2520Ye%2520and%2520Jingdong%2520Wang%2520and%2520Hengshuang%2520Zhao%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520The%2520benefit%2520of%2520transformers%2520in%2520large-scale%25203D%2520point%2520cloud%2520perception%2520tasks%252C%250Asuch%2520as%25203D%2520object%2520detection%252C%2520is%2520limited%2520by%2520their%2520quadratic%2520computation%2520cost%250Awhen%2520modeling%2520long-range%2520relationships.%2520In%2520contrast%252C%2520linear%2520RNNs%2520have%2520low%250Acomputational%2520complexity%2520and%2520are%2520suitable%2520for%2520long-range%2520modeling.%2520Toward%2520this%250Agoal%252C%2520we%2520propose%2520a%2520simple%2520and%2520effective%2520window-based%2520framework%2520built%2520on%2520LInear%250AgrOup%2520RNN%2520%2528i.e.%252C%2520perform%2520linear%2520RNN%2520for%2520grouped%2520features%2529%2520for%2520accurate%25203D%250Aobject%2520detection%252C%2520called%2520LION.%2520The%2520key%2520property%2520is%2520to%2520allow%2520sufficient%2520feature%250Ainteraction%2520in%2520a%2520much%2520larger%2520group%2520than%2520transformer-based%2520methods.%2520However%252C%250Aeffectively%2520applying%2520linear%2520group%2520RNN%2520to%25203D%2520object%2520detection%2520in%2520highly%2520sparse%250Apoint%2520clouds%2520is%2520not%2520trivial%2520due%2520to%2520its%2520limitation%2520in%2520handling%2520spatial%2520modeling.%250ATo%2520tackle%2520this%2520problem%252C%2520we%2520simply%2520introduce%2520a%25203D%2520spatial%2520feature%2520descriptor%2520and%250Aintegrate%2520it%2520into%2520the%2520linear%2520group%2520RNN%2520operators%2520to%2520enhance%2520their%2520spatial%250Afeatures%2520rather%2520than%2520blindly%2520increasing%2520the%2520number%2520of%2520scanning%2520orders%2520for%2520voxel%250Afeatures.%2520To%2520further%2520address%2520the%2520challenge%2520in%2520highly%2520sparse%2520point%2520clouds%252C%2520we%250Apropose%2520a%25203D%2520voxel%2520generation%2520strategy%2520to%2520densify%2520foreground%2520features%2520thanks%2520to%250Alinear%2520group%2520RNN%2520as%2520a%2520natural%2520property%2520of%2520auto-regressive%2520models.%2520Extensive%250Aexperiments%2520verify%2520the%2520effectiveness%2520of%2520the%2520proposed%2520components%2520and%2520the%250Ageneralization%2520of%2520our%2520LION%2520on%2520different%2520linear%2520group%2520RNN%2520operators%2520including%250AMamba%252C%2520RWKV%252C%2520and%2520RetNet.%2520Furthermore%252C%2520it%2520is%2520worth%2520mentioning%2520that%2520our%250ALION-Mamba%2520achieves%2520state-of-the-art%2520on%2520Waymo%252C%2520nuScenes%252C%2520Argoverse%2520V2%252C%2520and%2520ONCE%250Adataset.%2520Last%2520but%2520not%2520least%252C%2520our%2520method%2520supports%2520kinds%2520of%2520advanced%2520linear%2520RNN%250Aoperators%2520%2528e.g.%252C%2520RetNet%252C%2520RWKV%252C%2520Mamba%252C%2520xLSTM%2520and%2520TTT%2529%2520on%2520small%2520but%2520popular%2520KITTI%250Adataset%2520for%2520a%2520quick%2520experience%2520with%2520our%2520linear%2520RNN-based%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LION%3A%20Linear%20Group%20RNN%20for%203D%20Object%20Detection%20in%20Point%20Clouds&entry.906535625=Zhe%20Liu%20and%20Jinghua%20Hou%20and%20Xinyu%20Wang%20and%20Xiaoqing%20Ye%20and%20Jingdong%20Wang%20and%20Hengshuang%20Zhao%20and%20Xiang%20Bai&entry.1292438233=%20%20The%20benefit%20of%20transformers%20in%20large-scale%203D%20point%20cloud%20perception%20tasks%2C%0Asuch%20as%203D%20object%20detection%2C%20is%20limited%20by%20their%20quadratic%20computation%20cost%0Awhen%20modeling%20long-range%20relationships.%20In%20contrast%2C%20linear%20RNNs%20have%20low%0Acomputational%20complexity%20and%20are%20suitable%20for%20long-range%20modeling.%20Toward%20this%0Agoal%2C%20we%20propose%20a%20simple%20and%20effective%20window-based%20framework%20built%20on%20LInear%0AgrOup%20RNN%20%28i.e.%2C%20perform%20linear%20RNN%20for%20grouped%20features%29%20for%20accurate%203D%0Aobject%20detection%2C%20called%20LION.%20The%20key%20property%20is%20to%20allow%20sufficient%20feature%0Ainteraction%20in%20a%20much%20larger%20group%20than%20transformer-based%20methods.%20However%2C%0Aeffectively%20applying%20linear%20group%20RNN%20to%203D%20object%20detection%20in%20highly%20sparse%0Apoint%20clouds%20is%20not%20trivial%20due%20to%20its%20limitation%20in%20handling%20spatial%20modeling.%0ATo%20tackle%20this%20problem%2C%20we%20simply%20introduce%20a%203D%20spatial%20feature%20descriptor%20and%0Aintegrate%20it%20into%20the%20linear%20group%20RNN%20operators%20to%20enhance%20their%20spatial%0Afeatures%20rather%20than%20blindly%20increasing%20the%20number%20of%20scanning%20orders%20for%20voxel%0Afeatures.%20To%20further%20address%20the%20challenge%20in%20highly%20sparse%20point%20clouds%2C%20we%0Apropose%20a%203D%20voxel%20generation%20strategy%20to%20densify%20foreground%20features%20thanks%20to%0Alinear%20group%20RNN%20as%20a%20natural%20property%20of%20auto-regressive%20models.%20Extensive%0Aexperiments%20verify%20the%20effectiveness%20of%20the%20proposed%20components%20and%20the%0Ageneralization%20of%20our%20LION%20on%20different%20linear%20group%20RNN%20operators%20including%0AMamba%2C%20RWKV%2C%20and%20RetNet.%20Furthermore%2C%20it%20is%20worth%20mentioning%20that%20our%0ALION-Mamba%20achieves%20state-of-the-art%20on%20Waymo%2C%20nuScenes%2C%20Argoverse%20V2%2C%20and%20ONCE%0Adataset.%20Last%20but%20not%20least%2C%20our%20method%20supports%20kinds%20of%20advanced%20linear%20RNN%0Aoperators%20%28e.g.%2C%20RetNet%2C%20RWKV%2C%20Mamba%2C%20xLSTM%20and%20TTT%29%20on%20small%20but%20popular%20KITTI%0Adataset%20for%20a%20quick%20experience%20with%20our%20linear%20RNN-based%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18232v1&entry.124074799=Read"},
{"title": "Self-supervised learning of video representations from a child's\n  perspective", "author": "A. Emin Orhan and Wentao Wang and Alex N. Wang and Mengye Ren and Brenden M. Lake", "abstract": "  Children learn powerful internal models of the world around them from a few\nyears of egocentric visual experience. Can such internal models be learned from\na child's visual experience with highly generic learning algorithms or do they\nrequire strong inductive biases? Recent advances in collecting large-scale,\nlongitudinal, developmentally realistic video datasets and generic\nself-supervised learning (SSL) algorithms are allowing us to begin to tackle\nthis nature vs. nurture question. However, existing work typically focuses on\nimage-based SSL algorithms and visual capabilities that can be learned from\nstatic images (e.g. object recognition), thus ignoring temporal aspects of the\nworld. To close this gap, here we train self-supervised video models on\nlongitudinal, egocentric headcam recordings collected from a child over a two\nyear period in their early development (6-31 months). The resulting models are\nhighly effective at facilitating the learning of action concepts from a small\nnumber of labeled examples; they have favorable data size scaling properties;\nand they display emergent video interpolation capabilities. Video models also\nlearn more robust object representations than image-based models trained with\nthe exact same data. These results suggest that important temporal aspects of a\nchild's internal model of the world may be learnable from their visual\nexperience using highly generic learning algorithms and without strong\ninductive biases.\n", "link": "http://arxiv.org/abs/2402.00300v2", "date": "2024-07-25", "relevancy": 2.7809, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5657}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5538}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.549}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20learning%20of%20video%20representations%20from%20a%20child%27s%0A%20%20perspective&body=Title%3A%20Self-supervised%20learning%20of%20video%20representations%20from%20a%20child%27s%0A%20%20perspective%0AAuthor%3A%20A.%20Emin%20Orhan%20and%20Wentao%20Wang%20and%20Alex%20N.%20Wang%20and%20Mengye%20Ren%20and%20Brenden%20M.%20Lake%0AAbstract%3A%20%20%20Children%20learn%20powerful%20internal%20models%20of%20the%20world%20around%20them%20from%20a%20few%0Ayears%20of%20egocentric%20visual%20experience.%20Can%20such%20internal%20models%20be%20learned%20from%0Aa%20child%27s%20visual%20experience%20with%20highly%20generic%20learning%20algorithms%20or%20do%20they%0Arequire%20strong%20inductive%20biases%3F%20Recent%20advances%20in%20collecting%20large-scale%2C%0Alongitudinal%2C%20developmentally%20realistic%20video%20datasets%20and%20generic%0Aself-supervised%20learning%20%28SSL%29%20algorithms%20are%20allowing%20us%20to%20begin%20to%20tackle%0Athis%20nature%20vs.%20nurture%20question.%20However%2C%20existing%20work%20typically%20focuses%20on%0Aimage-based%20SSL%20algorithms%20and%20visual%20capabilities%20that%20can%20be%20learned%20from%0Astatic%20images%20%28e.g.%20object%20recognition%29%2C%20thus%20ignoring%20temporal%20aspects%20of%20the%0Aworld.%20To%20close%20this%20gap%2C%20here%20we%20train%20self-supervised%20video%20models%20on%0Alongitudinal%2C%20egocentric%20headcam%20recordings%20collected%20from%20a%20child%20over%20a%20two%0Ayear%20period%20in%20their%20early%20development%20%286-31%20months%29.%20The%20resulting%20models%20are%0Ahighly%20effective%20at%20facilitating%20the%20learning%20of%20action%20concepts%20from%20a%20small%0Anumber%20of%20labeled%20examples%3B%20they%20have%20favorable%20data%20size%20scaling%20properties%3B%0Aand%20they%20display%20emergent%20video%20interpolation%20capabilities.%20Video%20models%20also%0Alearn%20more%20robust%20object%20representations%20than%20image-based%20models%20trained%20with%0Athe%20exact%20same%20data.%20These%20results%20suggest%20that%20important%20temporal%20aspects%20of%20a%0Achild%27s%20internal%20model%20of%20the%20world%20may%20be%20learnable%20from%20their%20visual%0Aexperience%20using%20highly%20generic%20learning%20algorithms%20and%20without%20strong%0Ainductive%20biases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00300v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520learning%2520of%2520video%2520representations%2520from%2520a%2520child%2527s%250A%2520%2520perspective%26entry.906535625%3DA.%2520Emin%2520Orhan%2520and%2520Wentao%2520Wang%2520and%2520Alex%2520N.%2520Wang%2520and%2520Mengye%2520Ren%2520and%2520Brenden%2520M.%2520Lake%26entry.1292438233%3D%2520%2520Children%2520learn%2520powerful%2520internal%2520models%2520of%2520the%2520world%2520around%2520them%2520from%2520a%2520few%250Ayears%2520of%2520egocentric%2520visual%2520experience.%2520Can%2520such%2520internal%2520models%2520be%2520learned%2520from%250Aa%2520child%2527s%2520visual%2520experience%2520with%2520highly%2520generic%2520learning%2520algorithms%2520or%2520do%2520they%250Arequire%2520strong%2520inductive%2520biases%253F%2520Recent%2520advances%2520in%2520collecting%2520large-scale%252C%250Alongitudinal%252C%2520developmentally%2520realistic%2520video%2520datasets%2520and%2520generic%250Aself-supervised%2520learning%2520%2528SSL%2529%2520algorithms%2520are%2520allowing%2520us%2520to%2520begin%2520to%2520tackle%250Athis%2520nature%2520vs.%2520nurture%2520question.%2520However%252C%2520existing%2520work%2520typically%2520focuses%2520on%250Aimage-based%2520SSL%2520algorithms%2520and%2520visual%2520capabilities%2520that%2520can%2520be%2520learned%2520from%250Astatic%2520images%2520%2528e.g.%2520object%2520recognition%2529%252C%2520thus%2520ignoring%2520temporal%2520aspects%2520of%2520the%250Aworld.%2520To%2520close%2520this%2520gap%252C%2520here%2520we%2520train%2520self-supervised%2520video%2520models%2520on%250Alongitudinal%252C%2520egocentric%2520headcam%2520recordings%2520collected%2520from%2520a%2520child%2520over%2520a%2520two%250Ayear%2520period%2520in%2520their%2520early%2520development%2520%25286-31%2520months%2529.%2520The%2520resulting%2520models%2520are%250Ahighly%2520effective%2520at%2520facilitating%2520the%2520learning%2520of%2520action%2520concepts%2520from%2520a%2520small%250Anumber%2520of%2520labeled%2520examples%253B%2520they%2520have%2520favorable%2520data%2520size%2520scaling%2520properties%253B%250Aand%2520they%2520display%2520emergent%2520video%2520interpolation%2520capabilities.%2520Video%2520models%2520also%250Alearn%2520more%2520robust%2520object%2520representations%2520than%2520image-based%2520models%2520trained%2520with%250Athe%2520exact%2520same%2520data.%2520These%2520results%2520suggest%2520that%2520important%2520temporal%2520aspects%2520of%2520a%250Achild%2527s%2520internal%2520model%2520of%2520the%2520world%2520may%2520be%2520learnable%2520from%2520their%2520visual%250Aexperience%2520using%2520highly%2520generic%2520learning%2520algorithms%2520and%2520without%2520strong%250Ainductive%2520biases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00300v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20learning%20of%20video%20representations%20from%20a%20child%27s%0A%20%20perspective&entry.906535625=A.%20Emin%20Orhan%20and%20Wentao%20Wang%20and%20Alex%20N.%20Wang%20and%20Mengye%20Ren%20and%20Brenden%20M.%20Lake&entry.1292438233=%20%20Children%20learn%20powerful%20internal%20models%20of%20the%20world%20around%20them%20from%20a%20few%0Ayears%20of%20egocentric%20visual%20experience.%20Can%20such%20internal%20models%20be%20learned%20from%0Aa%20child%27s%20visual%20experience%20with%20highly%20generic%20learning%20algorithms%20or%20do%20they%0Arequire%20strong%20inductive%20biases%3F%20Recent%20advances%20in%20collecting%20large-scale%2C%0Alongitudinal%2C%20developmentally%20realistic%20video%20datasets%20and%20generic%0Aself-supervised%20learning%20%28SSL%29%20algorithms%20are%20allowing%20us%20to%20begin%20to%20tackle%0Athis%20nature%20vs.%20nurture%20question.%20However%2C%20existing%20work%20typically%20focuses%20on%0Aimage-based%20SSL%20algorithms%20and%20visual%20capabilities%20that%20can%20be%20learned%20from%0Astatic%20images%20%28e.g.%20object%20recognition%29%2C%20thus%20ignoring%20temporal%20aspects%20of%20the%0Aworld.%20To%20close%20this%20gap%2C%20here%20we%20train%20self-supervised%20video%20models%20on%0Alongitudinal%2C%20egocentric%20headcam%20recordings%20collected%20from%20a%20child%20over%20a%20two%0Ayear%20period%20in%20their%20early%20development%20%286-31%20months%29.%20The%20resulting%20models%20are%0Ahighly%20effective%20at%20facilitating%20the%20learning%20of%20action%20concepts%20from%20a%20small%0Anumber%20of%20labeled%20examples%3B%20they%20have%20favorable%20data%20size%20scaling%20properties%3B%0Aand%20they%20display%20emergent%20video%20interpolation%20capabilities.%20Video%20models%20also%0Alearn%20more%20robust%20object%20representations%20than%20image-based%20models%20trained%20with%0Athe%20exact%20same%20data.%20These%20results%20suggest%20that%20important%20temporal%20aspects%20of%20a%0Achild%27s%20internal%20model%20of%20the%20world%20may%20be%20learnable%20from%20their%20visual%0Aexperience%20using%20highly%20generic%20learning%20algorithms%20and%20without%20strong%0Ainductive%20biases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00300v2&entry.124074799=Read"},
{"title": "Reference-Based 3D-Aware Image Editing with Triplanes", "author": "Bahri Batuhan Bilecen and Yigit Yalin and Ning Yu and Aysegul Dundar", "abstract": "  Generative Adversarial Networks (GANs) have emerged as powerful tools for\nhigh-quality image generation and real image editing by manipulating their\nlatent spaces. Recent advancements in GANs include 3D-aware models such as\nEG3D, which feature efficient triplane-based architectures capable of\nreconstructing 3D geometry from single images. However, limited attention has\nbeen given to providing an integrated framework for 3D-aware, high-quality,\nreference-based image editing. This study addresses this gap by exploring and\ndemonstrating the effectiveness of the triplane space for advanced\nreference-based edits. Our novel approach integrates encoding, automatic\nlocalization, spatial disentanglement of triplane features, and fusion learning\nto achieve the desired edits. Additionally, our framework demonstrates\nversatility and robustness across various domains, extending its effectiveness\nto animal face edits, partially stylized edits like cartoon faces, full-body\nclothing edits, and 360-degree head edits. Our method shows state-of-the-art\nperformance over relevant latent direction, text, and image-guided 2D and\n3D-aware diffusion and GAN methods, both qualitatively and quantitatively.\n", "link": "http://arxiv.org/abs/2404.03632v2", "date": "2024-07-25", "relevancy": 2.7732, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5654}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5493}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reference-Based%203D-Aware%20Image%20Editing%20with%20Triplanes&body=Title%3A%20Reference-Based%203D-Aware%20Image%20Editing%20with%20Triplanes%0AAuthor%3A%20Bahri%20Batuhan%20Bilecen%20and%20Yigit%20Yalin%20and%20Ning%20Yu%20and%20Aysegul%20Dundar%0AAbstract%3A%20%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20emerged%20as%20powerful%20tools%20for%0Ahigh-quality%20image%20generation%20and%20real%20image%20editing%20by%20manipulating%20their%0Alatent%20spaces.%20Recent%20advancements%20in%20GANs%20include%203D-aware%20models%20such%20as%0AEG3D%2C%20which%20feature%20efficient%20triplane-based%20architectures%20capable%20of%0Areconstructing%203D%20geometry%20from%20single%20images.%20However%2C%20limited%20attention%20has%0Abeen%20given%20to%20providing%20an%20integrated%20framework%20for%203D-aware%2C%20high-quality%2C%0Areference-based%20image%20editing.%20This%20study%20addresses%20this%20gap%20by%20exploring%20and%0Ademonstrating%20the%20effectiveness%20of%20the%20triplane%20space%20for%20advanced%0Areference-based%20edits.%20Our%20novel%20approach%20integrates%20encoding%2C%20automatic%0Alocalization%2C%20spatial%20disentanglement%20of%20triplane%20features%2C%20and%20fusion%20learning%0Ato%20achieve%20the%20desired%20edits.%20Additionally%2C%20our%20framework%20demonstrates%0Aversatility%20and%20robustness%20across%20various%20domains%2C%20extending%20its%20effectiveness%0Ato%20animal%20face%20edits%2C%20partially%20stylized%20edits%20like%20cartoon%20faces%2C%20full-body%0Aclothing%20edits%2C%20and%20360-degree%20head%20edits.%20Our%20method%20shows%20state-of-the-art%0Aperformance%20over%20relevant%20latent%20direction%2C%20text%2C%20and%20image-guided%202D%20and%0A3D-aware%20diffusion%20and%20GAN%20methods%2C%20both%20qualitatively%20and%20quantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03632v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReference-Based%25203D-Aware%2520Image%2520Editing%2520with%2520Triplanes%26entry.906535625%3DBahri%2520Batuhan%2520Bilecen%2520and%2520Yigit%2520Yalin%2520and%2520Ning%2520Yu%2520and%2520Aysegul%2520Dundar%26entry.1292438233%3D%2520%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520have%2520emerged%2520as%2520powerful%2520tools%2520for%250Ahigh-quality%2520image%2520generation%2520and%2520real%2520image%2520editing%2520by%2520manipulating%2520their%250Alatent%2520spaces.%2520Recent%2520advancements%2520in%2520GANs%2520include%25203D-aware%2520models%2520such%2520as%250AEG3D%252C%2520which%2520feature%2520efficient%2520triplane-based%2520architectures%2520capable%2520of%250Areconstructing%25203D%2520geometry%2520from%2520single%2520images.%2520However%252C%2520limited%2520attention%2520has%250Abeen%2520given%2520to%2520providing%2520an%2520integrated%2520framework%2520for%25203D-aware%252C%2520high-quality%252C%250Areference-based%2520image%2520editing.%2520This%2520study%2520addresses%2520this%2520gap%2520by%2520exploring%2520and%250Ademonstrating%2520the%2520effectiveness%2520of%2520the%2520triplane%2520space%2520for%2520advanced%250Areference-based%2520edits.%2520Our%2520novel%2520approach%2520integrates%2520encoding%252C%2520automatic%250Alocalization%252C%2520spatial%2520disentanglement%2520of%2520triplane%2520features%252C%2520and%2520fusion%2520learning%250Ato%2520achieve%2520the%2520desired%2520edits.%2520Additionally%252C%2520our%2520framework%2520demonstrates%250Aversatility%2520and%2520robustness%2520across%2520various%2520domains%252C%2520extending%2520its%2520effectiveness%250Ato%2520animal%2520face%2520edits%252C%2520partially%2520stylized%2520edits%2520like%2520cartoon%2520faces%252C%2520full-body%250Aclothing%2520edits%252C%2520and%2520360-degree%2520head%2520edits.%2520Our%2520method%2520shows%2520state-of-the-art%250Aperformance%2520over%2520relevant%2520latent%2520direction%252C%2520text%252C%2520and%2520image-guided%25202D%2520and%250A3D-aware%2520diffusion%2520and%2520GAN%2520methods%252C%2520both%2520qualitatively%2520and%2520quantitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03632v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reference-Based%203D-Aware%20Image%20Editing%20with%20Triplanes&entry.906535625=Bahri%20Batuhan%20Bilecen%20and%20Yigit%20Yalin%20and%20Ning%20Yu%20and%20Aysegul%20Dundar&entry.1292438233=%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20emerged%20as%20powerful%20tools%20for%0Ahigh-quality%20image%20generation%20and%20real%20image%20editing%20by%20manipulating%20their%0Alatent%20spaces.%20Recent%20advancements%20in%20GANs%20include%203D-aware%20models%20such%20as%0AEG3D%2C%20which%20feature%20efficient%20triplane-based%20architectures%20capable%20of%0Areconstructing%203D%20geometry%20from%20single%20images.%20However%2C%20limited%20attention%20has%0Abeen%20given%20to%20providing%20an%20integrated%20framework%20for%203D-aware%2C%20high-quality%2C%0Areference-based%20image%20editing.%20This%20study%20addresses%20this%20gap%20by%20exploring%20and%0Ademonstrating%20the%20effectiveness%20of%20the%20triplane%20space%20for%20advanced%0Areference-based%20edits.%20Our%20novel%20approach%20integrates%20encoding%2C%20automatic%0Alocalization%2C%20spatial%20disentanglement%20of%20triplane%20features%2C%20and%20fusion%20learning%0Ato%20achieve%20the%20desired%20edits.%20Additionally%2C%20our%20framework%20demonstrates%0Aversatility%20and%20robustness%20across%20various%20domains%2C%20extending%20its%20effectiveness%0Ato%20animal%20face%20edits%2C%20partially%20stylized%20edits%20like%20cartoon%20faces%2C%20full-body%0Aclothing%20edits%2C%20and%20360-degree%20head%20edits.%20Our%20method%20shows%20state-of-the-art%0Aperformance%20over%20relevant%20latent%20direction%2C%20text%2C%20and%20image-guided%202D%20and%0A3D-aware%20diffusion%20and%20GAN%20methods%2C%20both%20qualitatively%20and%20quantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03632v2&entry.124074799=Read"},
{"title": "Castling-ViT: Compressing Self-Attention via Switching Towards\n  Linear-Angular Attention at Vision Transformer Inference", "author": "Haoran You and Yunyang Xiong and Xiaoliang Dai and Bichen Wu and Peizhao Zhang and Haoqi Fan and Peter Vajda and Yingyan Celine Lin", "abstract": "  Vision Transformers (ViTs) have shown impressive performance but still\nrequire a high computation cost as compared to convolutional neural networks\n(CNNs), one reason is that ViTs' attention measures global similarities and\nthus has a quadratic complexity with the number of input tokens. Existing\nefficient ViTs adopt local attention (e.g., Swin) or linear attention (e.g.,\nPerformer), which sacrifice ViTs' capabilities of capturing either global or\nlocal context. In this work, we ask an important research question: Can ViTs\nlearn both global and local context while being more efficient during\ninference? To this end, we propose a framework called Castling-ViT, which\ntrains ViTs using both linear-angular attention and masked softmax-based\nquadratic attention, but then switches to having only linear angular attention\nduring ViT inference. Our Castling-ViT leverages angular kernels to measure the\nsimilarities between queries and keys via spectral angles. And we further\nsimplify it with two techniques: (1) a novel linear-angular attention\nmechanism: we decompose the angular kernels into linear terms and high-order\nresiduals, and only keep the linear terms; and (2) we adopt two parameterized\nmodules to approximate high-order residuals: a depthwise convolution and an\nauxiliary masked softmax attention to help learn both global and local\ninformation, where the masks for softmax attention are regularized to gradually\nbecome zeros and thus incur no overhead during ViT inference. Extensive\nexperiments and ablation studies on three tasks consistently validate the\neffectiveness of the proposed Castling-ViT, e.g., achieving up to a 1.8% higher\naccuracy or 40% MACs reduction on ImageNet classification and 1.2 higher mAP on\nCOCO detection under comparable FLOPs, as compared to ViTs with vanilla\nsoftmax-based attentions.\n", "link": "http://arxiv.org/abs/2211.10526v5", "date": "2024-07-25", "relevancy": 2.71, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5565}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5441}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Castling-ViT%3A%20Compressing%20Self-Attention%20via%20Switching%20Towards%0A%20%20Linear-Angular%20Attention%20at%20Vision%20Transformer%20Inference&body=Title%3A%20Castling-ViT%3A%20Compressing%20Self-Attention%20via%20Switching%20Towards%0A%20%20Linear-Angular%20Attention%20at%20Vision%20Transformer%20Inference%0AAuthor%3A%20Haoran%20You%20and%20Yunyang%20Xiong%20and%20Xiaoliang%20Dai%20and%20Bichen%20Wu%20and%20Peizhao%20Zhang%20and%20Haoqi%20Fan%20and%20Peter%20Vajda%20and%20Yingyan%20Celine%20Lin%0AAbstract%3A%20%20%20Vision%20Transformers%20%28ViTs%29%20have%20shown%20impressive%20performance%20but%20still%0Arequire%20a%20high%20computation%20cost%20as%20compared%20to%20convolutional%20neural%20networks%0A%28CNNs%29%2C%20one%20reason%20is%20that%20ViTs%27%20attention%20measures%20global%20similarities%20and%0Athus%20has%20a%20quadratic%20complexity%20with%20the%20number%20of%20input%20tokens.%20Existing%0Aefficient%20ViTs%20adopt%20local%20attention%20%28e.g.%2C%20Swin%29%20or%20linear%20attention%20%28e.g.%2C%0APerformer%29%2C%20which%20sacrifice%20ViTs%27%20capabilities%20of%20capturing%20either%20global%20or%0Alocal%20context.%20In%20this%20work%2C%20we%20ask%20an%20important%20research%20question%3A%20Can%20ViTs%0Alearn%20both%20global%20and%20local%20context%20while%20being%20more%20efficient%20during%0Ainference%3F%20To%20this%20end%2C%20we%20propose%20a%20framework%20called%20Castling-ViT%2C%20which%0Atrains%20ViTs%20using%20both%20linear-angular%20attention%20and%20masked%20softmax-based%0Aquadratic%20attention%2C%20but%20then%20switches%20to%20having%20only%20linear%20angular%20attention%0Aduring%20ViT%20inference.%20Our%20Castling-ViT%20leverages%20angular%20kernels%20to%20measure%20the%0Asimilarities%20between%20queries%20and%20keys%20via%20spectral%20angles.%20And%20we%20further%0Asimplify%20it%20with%20two%20techniques%3A%20%281%29%20a%20novel%20linear-angular%20attention%0Amechanism%3A%20we%20decompose%20the%20angular%20kernels%20into%20linear%20terms%20and%20high-order%0Aresiduals%2C%20and%20only%20keep%20the%20linear%20terms%3B%20and%20%282%29%20we%20adopt%20two%20parameterized%0Amodules%20to%20approximate%20high-order%20residuals%3A%20a%20depthwise%20convolution%20and%20an%0Aauxiliary%20masked%20softmax%20attention%20to%20help%20learn%20both%20global%20and%20local%0Ainformation%2C%20where%20the%20masks%20for%20softmax%20attention%20are%20regularized%20to%20gradually%0Abecome%20zeros%20and%20thus%20incur%20no%20overhead%20during%20ViT%20inference.%20Extensive%0Aexperiments%20and%20ablation%20studies%20on%20three%20tasks%20consistently%20validate%20the%0Aeffectiveness%20of%20the%20proposed%20Castling-ViT%2C%20e.g.%2C%20achieving%20up%20to%20a%201.8%25%20higher%0Aaccuracy%20or%2040%25%20MACs%20reduction%20on%20ImageNet%20classification%20and%201.2%20higher%20mAP%20on%0ACOCO%20detection%20under%20comparable%20FLOPs%2C%20as%20compared%20to%20ViTs%20with%20vanilla%0Asoftmax-based%20attentions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.10526v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCastling-ViT%253A%2520Compressing%2520Self-Attention%2520via%2520Switching%2520Towards%250A%2520%2520Linear-Angular%2520Attention%2520at%2520Vision%2520Transformer%2520Inference%26entry.906535625%3DHaoran%2520You%2520and%2520Yunyang%2520Xiong%2520and%2520Xiaoliang%2520Dai%2520and%2520Bichen%2520Wu%2520and%2520Peizhao%2520Zhang%2520and%2520Haoqi%2520Fan%2520and%2520Peter%2520Vajda%2520and%2520Yingyan%2520Celine%2520Lin%26entry.1292438233%3D%2520%2520Vision%2520Transformers%2520%2528ViTs%2529%2520have%2520shown%2520impressive%2520performance%2520but%2520still%250Arequire%2520a%2520high%2520computation%2520cost%2520as%2520compared%2520to%2520convolutional%2520neural%2520networks%250A%2528CNNs%2529%252C%2520one%2520reason%2520is%2520that%2520ViTs%2527%2520attention%2520measures%2520global%2520similarities%2520and%250Athus%2520has%2520a%2520quadratic%2520complexity%2520with%2520the%2520number%2520of%2520input%2520tokens.%2520Existing%250Aefficient%2520ViTs%2520adopt%2520local%2520attention%2520%2528e.g.%252C%2520Swin%2529%2520or%2520linear%2520attention%2520%2528e.g.%252C%250APerformer%2529%252C%2520which%2520sacrifice%2520ViTs%2527%2520capabilities%2520of%2520capturing%2520either%2520global%2520or%250Alocal%2520context.%2520In%2520this%2520work%252C%2520we%2520ask%2520an%2520important%2520research%2520question%253A%2520Can%2520ViTs%250Alearn%2520both%2520global%2520and%2520local%2520context%2520while%2520being%2520more%2520efficient%2520during%250Ainference%253F%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520framework%2520called%2520Castling-ViT%252C%2520which%250Atrains%2520ViTs%2520using%2520both%2520linear-angular%2520attention%2520and%2520masked%2520softmax-based%250Aquadratic%2520attention%252C%2520but%2520then%2520switches%2520to%2520having%2520only%2520linear%2520angular%2520attention%250Aduring%2520ViT%2520inference.%2520Our%2520Castling-ViT%2520leverages%2520angular%2520kernels%2520to%2520measure%2520the%250Asimilarities%2520between%2520queries%2520and%2520keys%2520via%2520spectral%2520angles.%2520And%2520we%2520further%250Asimplify%2520it%2520with%2520two%2520techniques%253A%2520%25281%2529%2520a%2520novel%2520linear-angular%2520attention%250Amechanism%253A%2520we%2520decompose%2520the%2520angular%2520kernels%2520into%2520linear%2520terms%2520and%2520high-order%250Aresiduals%252C%2520and%2520only%2520keep%2520the%2520linear%2520terms%253B%2520and%2520%25282%2529%2520we%2520adopt%2520two%2520parameterized%250Amodules%2520to%2520approximate%2520high-order%2520residuals%253A%2520a%2520depthwise%2520convolution%2520and%2520an%250Aauxiliary%2520masked%2520softmax%2520attention%2520to%2520help%2520learn%2520both%2520global%2520and%2520local%250Ainformation%252C%2520where%2520the%2520masks%2520for%2520softmax%2520attention%2520are%2520regularized%2520to%2520gradually%250Abecome%2520zeros%2520and%2520thus%2520incur%2520no%2520overhead%2520during%2520ViT%2520inference.%2520Extensive%250Aexperiments%2520and%2520ablation%2520studies%2520on%2520three%2520tasks%2520consistently%2520validate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520Castling-ViT%252C%2520e.g.%252C%2520achieving%2520up%2520to%2520a%25201.8%2525%2520higher%250Aaccuracy%2520or%252040%2525%2520MACs%2520reduction%2520on%2520ImageNet%2520classification%2520and%25201.2%2520higher%2520mAP%2520on%250ACOCO%2520detection%2520under%2520comparable%2520FLOPs%252C%2520as%2520compared%2520to%2520ViTs%2520with%2520vanilla%250Asoftmax-based%2520attentions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.10526v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Castling-ViT%3A%20Compressing%20Self-Attention%20via%20Switching%20Towards%0A%20%20Linear-Angular%20Attention%20at%20Vision%20Transformer%20Inference&entry.906535625=Haoran%20You%20and%20Yunyang%20Xiong%20and%20Xiaoliang%20Dai%20and%20Bichen%20Wu%20and%20Peizhao%20Zhang%20and%20Haoqi%20Fan%20and%20Peter%20Vajda%20and%20Yingyan%20Celine%20Lin&entry.1292438233=%20%20Vision%20Transformers%20%28ViTs%29%20have%20shown%20impressive%20performance%20but%20still%0Arequire%20a%20high%20computation%20cost%20as%20compared%20to%20convolutional%20neural%20networks%0A%28CNNs%29%2C%20one%20reason%20is%20that%20ViTs%27%20attention%20measures%20global%20similarities%20and%0Athus%20has%20a%20quadratic%20complexity%20with%20the%20number%20of%20input%20tokens.%20Existing%0Aefficient%20ViTs%20adopt%20local%20attention%20%28e.g.%2C%20Swin%29%20or%20linear%20attention%20%28e.g.%2C%0APerformer%29%2C%20which%20sacrifice%20ViTs%27%20capabilities%20of%20capturing%20either%20global%20or%0Alocal%20context.%20In%20this%20work%2C%20we%20ask%20an%20important%20research%20question%3A%20Can%20ViTs%0Alearn%20both%20global%20and%20local%20context%20while%20being%20more%20efficient%20during%0Ainference%3F%20To%20this%20end%2C%20we%20propose%20a%20framework%20called%20Castling-ViT%2C%20which%0Atrains%20ViTs%20using%20both%20linear-angular%20attention%20and%20masked%20softmax-based%0Aquadratic%20attention%2C%20but%20then%20switches%20to%20having%20only%20linear%20angular%20attention%0Aduring%20ViT%20inference.%20Our%20Castling-ViT%20leverages%20angular%20kernels%20to%20measure%20the%0Asimilarities%20between%20queries%20and%20keys%20via%20spectral%20angles.%20And%20we%20further%0Asimplify%20it%20with%20two%20techniques%3A%20%281%29%20a%20novel%20linear-angular%20attention%0Amechanism%3A%20we%20decompose%20the%20angular%20kernels%20into%20linear%20terms%20and%20high-order%0Aresiduals%2C%20and%20only%20keep%20the%20linear%20terms%3B%20and%20%282%29%20we%20adopt%20two%20parameterized%0Amodules%20to%20approximate%20high-order%20residuals%3A%20a%20depthwise%20convolution%20and%20an%0Aauxiliary%20masked%20softmax%20attention%20to%20help%20learn%20both%20global%20and%20local%0Ainformation%2C%20where%20the%20masks%20for%20softmax%20attention%20are%20regularized%20to%20gradually%0Abecome%20zeros%20and%20thus%20incur%20no%20overhead%20during%20ViT%20inference.%20Extensive%0Aexperiments%20and%20ablation%20studies%20on%20three%20tasks%20consistently%20validate%20the%0Aeffectiveness%20of%20the%20proposed%20Castling-ViT%2C%20e.g.%2C%20achieving%20up%20to%20a%201.8%25%20higher%0Aaccuracy%20or%2040%25%20MACs%20reduction%20on%20ImageNet%20classification%20and%201.2%20higher%20mAP%20on%0ACOCO%20detection%20under%20comparable%20FLOPs%2C%20as%20compared%20to%20ViTs%20with%20vanilla%0Asoftmax-based%20attentions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.10526v5&entry.124074799=Read"},
{"title": "SSTD: Stripe-Like Space Target Detection using Single-Point Supervision", "author": "Zijian Zhu and Ali Zia and Xuesong Li and Bingbing Dan and Yuebo Ma and Enhai Liu and Rujin Zhao", "abstract": "  Stripe-like space target detection (SSTD) plays a key role in enhancing space\nsituational awareness and assessing spacecraft behaviour. This domain faces\nthree challenges: the lack of publicly available datasets, interference from\nstray light and stars, and the variability of stripe-like targets, which\ncomplicates pixel-level annotation. In response, we introduces\n`AstroStripeSet', a pioneering dataset designed for SSTD, aiming to bridge the\ngap in academic resources and advance research in SSTD. Furthermore, we propose\na novel pseudo-label evolution teacher-student framework with single-point\nsupervision. This framework starts with generating initial pseudo-labels using\nthe zero-shot capabilities of the Segment Anything Model (SAM) in a\nsingle-point setting, and refines these labels iteratively. In our framework,\nthe fine-tuned StripeSAM serves as the teacher and the newly developed\nStripeNet as the student, consistently improving segmentation performance by\nimproving the quality of pseudo-labels. We also introduce `GeoDice', a new loss\nfunction customized for the linear characteristics of stripe-like targets.\nExtensive experiments show that the performance of our approach matches fully\nsupervised methods on all evaluation metrics, establishing a new\nstate-of-the-art (SOTA) benchmark. Our dataset and code will be made publicly\navailable.\n", "link": "http://arxiv.org/abs/2407.18097v1", "date": "2024-07-25", "relevancy": 2.6787, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5488}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.534}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSTD%3A%20Stripe-Like%20Space%20Target%20Detection%20using%20Single-Point%20Supervision&body=Title%3A%20SSTD%3A%20Stripe-Like%20Space%20Target%20Detection%20using%20Single-Point%20Supervision%0AAuthor%3A%20Zijian%20Zhu%20and%20Ali%20Zia%20and%20Xuesong%20Li%20and%20Bingbing%20Dan%20and%20Yuebo%20Ma%20and%20Enhai%20Liu%20and%20Rujin%20Zhao%0AAbstract%3A%20%20%20Stripe-like%20space%20target%20detection%20%28SSTD%29%20plays%20a%20key%20role%20in%20enhancing%20space%0Asituational%20awareness%20and%20assessing%20spacecraft%20behaviour.%20This%20domain%20faces%0Athree%20challenges%3A%20the%20lack%20of%20publicly%20available%20datasets%2C%20interference%20from%0Astray%20light%20and%20stars%2C%20and%20the%20variability%20of%20stripe-like%20targets%2C%20which%0Acomplicates%20pixel-level%20annotation.%20In%20response%2C%20we%20introduces%0A%60AstroStripeSet%27%2C%20a%20pioneering%20dataset%20designed%20for%20SSTD%2C%20aiming%20to%20bridge%20the%0Agap%20in%20academic%20resources%20and%20advance%20research%20in%20SSTD.%20Furthermore%2C%20we%20propose%0Aa%20novel%20pseudo-label%20evolution%20teacher-student%20framework%20with%20single-point%0Asupervision.%20This%20framework%20starts%20with%20generating%20initial%20pseudo-labels%20using%0Athe%20zero-shot%20capabilities%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%20in%20a%0Asingle-point%20setting%2C%20and%20refines%20these%20labels%20iteratively.%20In%20our%20framework%2C%0Athe%20fine-tuned%20StripeSAM%20serves%20as%20the%20teacher%20and%20the%20newly%20developed%0AStripeNet%20as%20the%20student%2C%20consistently%20improving%20segmentation%20performance%20by%0Aimproving%20the%20quality%20of%20pseudo-labels.%20We%20also%20introduce%20%60GeoDice%27%2C%20a%20new%20loss%0Afunction%20customized%20for%20the%20linear%20characteristics%20of%20stripe-like%20targets.%0AExtensive%20experiments%20show%20that%20the%20performance%20of%20our%20approach%20matches%20fully%0Asupervised%20methods%20on%20all%20evaluation%20metrics%2C%20establishing%20a%20new%0Astate-of-the-art%20%28SOTA%29%20benchmark.%20Our%20dataset%20and%20code%20will%20be%20made%20publicly%0Aavailable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSTD%253A%2520Stripe-Like%2520Space%2520Target%2520Detection%2520using%2520Single-Point%2520Supervision%26entry.906535625%3DZijian%2520Zhu%2520and%2520Ali%2520Zia%2520and%2520Xuesong%2520Li%2520and%2520Bingbing%2520Dan%2520and%2520Yuebo%2520Ma%2520and%2520Enhai%2520Liu%2520and%2520Rujin%2520Zhao%26entry.1292438233%3D%2520%2520Stripe-like%2520space%2520target%2520detection%2520%2528SSTD%2529%2520plays%2520a%2520key%2520role%2520in%2520enhancing%2520space%250Asituational%2520awareness%2520and%2520assessing%2520spacecraft%2520behaviour.%2520This%2520domain%2520faces%250Athree%2520challenges%253A%2520the%2520lack%2520of%2520publicly%2520available%2520datasets%252C%2520interference%2520from%250Astray%2520light%2520and%2520stars%252C%2520and%2520the%2520variability%2520of%2520stripe-like%2520targets%252C%2520which%250Acomplicates%2520pixel-level%2520annotation.%2520In%2520response%252C%2520we%2520introduces%250A%2560AstroStripeSet%2527%252C%2520a%2520pioneering%2520dataset%2520designed%2520for%2520SSTD%252C%2520aiming%2520to%2520bridge%2520the%250Agap%2520in%2520academic%2520resources%2520and%2520advance%2520research%2520in%2520SSTD.%2520Furthermore%252C%2520we%2520propose%250Aa%2520novel%2520pseudo-label%2520evolution%2520teacher-student%2520framework%2520with%2520single-point%250Asupervision.%2520This%2520framework%2520starts%2520with%2520generating%2520initial%2520pseudo-labels%2520using%250Athe%2520zero-shot%2520capabilities%2520of%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520in%2520a%250Asingle-point%2520setting%252C%2520and%2520refines%2520these%2520labels%2520iteratively.%2520In%2520our%2520framework%252C%250Athe%2520fine-tuned%2520StripeSAM%2520serves%2520as%2520the%2520teacher%2520and%2520the%2520newly%2520developed%250AStripeNet%2520as%2520the%2520student%252C%2520consistently%2520improving%2520segmentation%2520performance%2520by%250Aimproving%2520the%2520quality%2520of%2520pseudo-labels.%2520We%2520also%2520introduce%2520%2560GeoDice%2527%252C%2520a%2520new%2520loss%250Afunction%2520customized%2520for%2520the%2520linear%2520characteristics%2520of%2520stripe-like%2520targets.%250AExtensive%2520experiments%2520show%2520that%2520the%2520performance%2520of%2520our%2520approach%2520matches%2520fully%250Asupervised%2520methods%2520on%2520all%2520evaluation%2520metrics%252C%2520establishing%2520a%2520new%250Astate-of-the-art%2520%2528SOTA%2529%2520benchmark.%2520Our%2520dataset%2520and%2520code%2520will%2520be%2520made%2520publicly%250Aavailable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSTD%3A%20Stripe-Like%20Space%20Target%20Detection%20using%20Single-Point%20Supervision&entry.906535625=Zijian%20Zhu%20and%20Ali%20Zia%20and%20Xuesong%20Li%20and%20Bingbing%20Dan%20and%20Yuebo%20Ma%20and%20Enhai%20Liu%20and%20Rujin%20Zhao&entry.1292438233=%20%20Stripe-like%20space%20target%20detection%20%28SSTD%29%20plays%20a%20key%20role%20in%20enhancing%20space%0Asituational%20awareness%20and%20assessing%20spacecraft%20behaviour.%20This%20domain%20faces%0Athree%20challenges%3A%20the%20lack%20of%20publicly%20available%20datasets%2C%20interference%20from%0Astray%20light%20and%20stars%2C%20and%20the%20variability%20of%20stripe-like%20targets%2C%20which%0Acomplicates%20pixel-level%20annotation.%20In%20response%2C%20we%20introduces%0A%60AstroStripeSet%27%2C%20a%20pioneering%20dataset%20designed%20for%20SSTD%2C%20aiming%20to%20bridge%20the%0Agap%20in%20academic%20resources%20and%20advance%20research%20in%20SSTD.%20Furthermore%2C%20we%20propose%0Aa%20novel%20pseudo-label%20evolution%20teacher-student%20framework%20with%20single-point%0Asupervision.%20This%20framework%20starts%20with%20generating%20initial%20pseudo-labels%20using%0Athe%20zero-shot%20capabilities%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%20in%20a%0Asingle-point%20setting%2C%20and%20refines%20these%20labels%20iteratively.%20In%20our%20framework%2C%0Athe%20fine-tuned%20StripeSAM%20serves%20as%20the%20teacher%20and%20the%20newly%20developed%0AStripeNet%20as%20the%20student%2C%20consistently%20improving%20segmentation%20performance%20by%0Aimproving%20the%20quality%20of%20pseudo-labels.%20We%20also%20introduce%20%60GeoDice%27%2C%20a%20new%20loss%0Afunction%20customized%20for%20the%20linear%20characteristics%20of%20stripe-like%20targets.%0AExtensive%20experiments%20show%20that%20the%20performance%20of%20our%20approach%20matches%20fully%0Asupervised%20methods%20on%20all%20evaluation%20metrics%2C%20establishing%20a%20new%0Astate-of-the-art%20%28SOTA%29%20benchmark.%20Our%20dataset%20and%20code%20will%20be%20made%20publicly%0Aavailable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18097v1&entry.124074799=Read"},
{"title": "$\\mathbb{X}$-Sample Contrastive Loss: Improving Contrastive Learning\n  with Sample Similarity Graphs", "author": "Vlad Sobal and Mark Ibrahim and Randall Balestriero and Vivien Cabannes and Diane Bouchacourt and Pietro Astolfi and Kyunghyun Cho and Yann LeCun", "abstract": "  Learning good representations involves capturing the diverse ways in which\ndata samples relate. Contrastive loss - an objective matching related samples -\nunderlies methods from self-supervised to multimodal learning. Contrastive\nlosses, however, can be viewed more broadly as modifying a similarity graph to\nindicate how samples should relate in the embedding space. This view reveals a\nshortcoming in contrastive learning: the similarity graph is binary, as only\none sample is the related positive sample. Crucially, similarities\n\\textit{across} samples are ignored. Based on this observation, we revise the\nstandard contrastive loss to explicitly encode how a sample relates to others.\nWe experiment with this new objective, called $\\mathbb{X}$-Sample Contrastive,\nto train vision models based on similarities in class or text caption\ndescriptions. Our study spans three scales: ImageNet-1k with 1 million, CC3M\nwith 3 million, and CC12M with 12 million samples. The representations learned\nvia our objective outperform both contrastive self-supervised and\nvision-language models trained on the same data across a range of tasks. When\ntraining on CC12M, we outperform CLIP by $0.6\\%$ on both ImageNet and ImageNet\nReal. Our objective appears to work particularly well in lower-data regimes,\nwith gains over CLIP of $16.8\\%$ on ImageNet and $18.1\\%$ on ImageNet Real when\ntraining with CC3M. Finally, our objective seems to encourage the model to\nlearn representations that separate objects from their attributes and\nbackgrounds, with gains of $3.3$-$5.6$\\% over CLIP on ImageNet9. We hope the\nproposed solution takes a small step towards developing richer learning\nobjectives for understanding sample relations in foundation models.\n", "link": "http://arxiv.org/abs/2407.18134v1", "date": "2024-07-25", "relevancy": 2.5611, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5311}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5093}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%24%5Cmathbb%7BX%7D%24-Sample%20Contrastive%20Loss%3A%20Improving%20Contrastive%20Learning%0A%20%20with%20Sample%20Similarity%20Graphs&body=Title%3A%20%24%5Cmathbb%7BX%7D%24-Sample%20Contrastive%20Loss%3A%20Improving%20Contrastive%20Learning%0A%20%20with%20Sample%20Similarity%20Graphs%0AAuthor%3A%20Vlad%20Sobal%20and%20Mark%20Ibrahim%20and%20Randall%20Balestriero%20and%20Vivien%20Cabannes%20and%20Diane%20Bouchacourt%20and%20Pietro%20Astolfi%20and%20Kyunghyun%20Cho%20and%20Yann%20LeCun%0AAbstract%3A%20%20%20Learning%20good%20representations%20involves%20capturing%20the%20diverse%20ways%20in%20which%0Adata%20samples%20relate.%20Contrastive%20loss%20-%20an%20objective%20matching%20related%20samples%20-%0Aunderlies%20methods%20from%20self-supervised%20to%20multimodal%20learning.%20Contrastive%0Alosses%2C%20however%2C%20can%20be%20viewed%20more%20broadly%20as%20modifying%20a%20similarity%20graph%20to%0Aindicate%20how%20samples%20should%20relate%20in%20the%20embedding%20space.%20This%20view%20reveals%20a%0Ashortcoming%20in%20contrastive%20learning%3A%20the%20similarity%20graph%20is%20binary%2C%20as%20only%0Aone%20sample%20is%20the%20related%20positive%20sample.%20Crucially%2C%20similarities%0A%5Ctextit%7Bacross%7D%20samples%20are%20ignored.%20Based%20on%20this%20observation%2C%20we%20revise%20the%0Astandard%20contrastive%20loss%20to%20explicitly%20encode%20how%20a%20sample%20relates%20to%20others.%0AWe%20experiment%20with%20this%20new%20objective%2C%20called%20%24%5Cmathbb%7BX%7D%24-Sample%20Contrastive%2C%0Ato%20train%20vision%20models%20based%20on%20similarities%20in%20class%20or%20text%20caption%0Adescriptions.%20Our%20study%20spans%20three%20scales%3A%20ImageNet-1k%20with%201%20million%2C%20CC3M%0Awith%203%20million%2C%20and%20CC12M%20with%2012%20million%20samples.%20The%20representations%20learned%0Avia%20our%20objective%20outperform%20both%20contrastive%20self-supervised%20and%0Avision-language%20models%20trained%20on%20the%20same%20data%20across%20a%20range%20of%20tasks.%20When%0Atraining%20on%20CC12M%2C%20we%20outperform%20CLIP%20by%20%240.6%5C%25%24%20on%20both%20ImageNet%20and%20ImageNet%0AReal.%20Our%20objective%20appears%20to%20work%20particularly%20well%20in%20lower-data%20regimes%2C%0Awith%20gains%20over%20CLIP%20of%20%2416.8%5C%25%24%20on%20ImageNet%20and%20%2418.1%5C%25%24%20on%20ImageNet%20Real%20when%0Atraining%20with%20CC3M.%20Finally%2C%20our%20objective%20seems%20to%20encourage%20the%20model%20to%0Alearn%20representations%20that%20separate%20objects%20from%20their%20attributes%20and%0Abackgrounds%2C%20with%20gains%20of%20%243.3%24-%245.6%24%5C%25%20over%20CLIP%20on%20ImageNet9.%20We%20hope%20the%0Aproposed%20solution%20takes%20a%20small%20step%20towards%20developing%20richer%20learning%0Aobjectives%20for%20understanding%20sample%20relations%20in%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18134v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2524%255Cmathbb%257BX%257D%2524-Sample%2520Contrastive%2520Loss%253A%2520Improving%2520Contrastive%2520Learning%250A%2520%2520with%2520Sample%2520Similarity%2520Graphs%26entry.906535625%3DVlad%2520Sobal%2520and%2520Mark%2520Ibrahim%2520and%2520Randall%2520Balestriero%2520and%2520Vivien%2520Cabannes%2520and%2520Diane%2520Bouchacourt%2520and%2520Pietro%2520Astolfi%2520and%2520Kyunghyun%2520Cho%2520and%2520Yann%2520LeCun%26entry.1292438233%3D%2520%2520Learning%2520good%2520representations%2520involves%2520capturing%2520the%2520diverse%2520ways%2520in%2520which%250Adata%2520samples%2520relate.%2520Contrastive%2520loss%2520-%2520an%2520objective%2520matching%2520related%2520samples%2520-%250Aunderlies%2520methods%2520from%2520self-supervised%2520to%2520multimodal%2520learning.%2520Contrastive%250Alosses%252C%2520however%252C%2520can%2520be%2520viewed%2520more%2520broadly%2520as%2520modifying%2520a%2520similarity%2520graph%2520to%250Aindicate%2520how%2520samples%2520should%2520relate%2520in%2520the%2520embedding%2520space.%2520This%2520view%2520reveals%2520a%250Ashortcoming%2520in%2520contrastive%2520learning%253A%2520the%2520similarity%2520graph%2520is%2520binary%252C%2520as%2520only%250Aone%2520sample%2520is%2520the%2520related%2520positive%2520sample.%2520Crucially%252C%2520similarities%250A%255Ctextit%257Bacross%257D%2520samples%2520are%2520ignored.%2520Based%2520on%2520this%2520observation%252C%2520we%2520revise%2520the%250Astandard%2520contrastive%2520loss%2520to%2520explicitly%2520encode%2520how%2520a%2520sample%2520relates%2520to%2520others.%250AWe%2520experiment%2520with%2520this%2520new%2520objective%252C%2520called%2520%2524%255Cmathbb%257BX%257D%2524-Sample%2520Contrastive%252C%250Ato%2520train%2520vision%2520models%2520based%2520on%2520similarities%2520in%2520class%2520or%2520text%2520caption%250Adescriptions.%2520Our%2520study%2520spans%2520three%2520scales%253A%2520ImageNet-1k%2520with%25201%2520million%252C%2520CC3M%250Awith%25203%2520million%252C%2520and%2520CC12M%2520with%252012%2520million%2520samples.%2520The%2520representations%2520learned%250Avia%2520our%2520objective%2520outperform%2520both%2520contrastive%2520self-supervised%2520and%250Avision-language%2520models%2520trained%2520on%2520the%2520same%2520data%2520across%2520a%2520range%2520of%2520tasks.%2520When%250Atraining%2520on%2520CC12M%252C%2520we%2520outperform%2520CLIP%2520by%2520%25240.6%255C%2525%2524%2520on%2520both%2520ImageNet%2520and%2520ImageNet%250AReal.%2520Our%2520objective%2520appears%2520to%2520work%2520particularly%2520well%2520in%2520lower-data%2520regimes%252C%250Awith%2520gains%2520over%2520CLIP%2520of%2520%252416.8%255C%2525%2524%2520on%2520ImageNet%2520and%2520%252418.1%255C%2525%2524%2520on%2520ImageNet%2520Real%2520when%250Atraining%2520with%2520CC3M.%2520Finally%252C%2520our%2520objective%2520seems%2520to%2520encourage%2520the%2520model%2520to%250Alearn%2520representations%2520that%2520separate%2520objects%2520from%2520their%2520attributes%2520and%250Abackgrounds%252C%2520with%2520gains%2520of%2520%25243.3%2524-%25245.6%2524%255C%2525%2520over%2520CLIP%2520on%2520ImageNet9.%2520We%2520hope%2520the%250Aproposed%2520solution%2520takes%2520a%2520small%2520step%2520towards%2520developing%2520richer%2520learning%250Aobjectives%2520for%2520understanding%2520sample%2520relations%2520in%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18134v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%24%5Cmathbb%7BX%7D%24-Sample%20Contrastive%20Loss%3A%20Improving%20Contrastive%20Learning%0A%20%20with%20Sample%20Similarity%20Graphs&entry.906535625=Vlad%20Sobal%20and%20Mark%20Ibrahim%20and%20Randall%20Balestriero%20and%20Vivien%20Cabannes%20and%20Diane%20Bouchacourt%20and%20Pietro%20Astolfi%20and%20Kyunghyun%20Cho%20and%20Yann%20LeCun&entry.1292438233=%20%20Learning%20good%20representations%20involves%20capturing%20the%20diverse%20ways%20in%20which%0Adata%20samples%20relate.%20Contrastive%20loss%20-%20an%20objective%20matching%20related%20samples%20-%0Aunderlies%20methods%20from%20self-supervised%20to%20multimodal%20learning.%20Contrastive%0Alosses%2C%20however%2C%20can%20be%20viewed%20more%20broadly%20as%20modifying%20a%20similarity%20graph%20to%0Aindicate%20how%20samples%20should%20relate%20in%20the%20embedding%20space.%20This%20view%20reveals%20a%0Ashortcoming%20in%20contrastive%20learning%3A%20the%20similarity%20graph%20is%20binary%2C%20as%20only%0Aone%20sample%20is%20the%20related%20positive%20sample.%20Crucially%2C%20similarities%0A%5Ctextit%7Bacross%7D%20samples%20are%20ignored.%20Based%20on%20this%20observation%2C%20we%20revise%20the%0Astandard%20contrastive%20loss%20to%20explicitly%20encode%20how%20a%20sample%20relates%20to%20others.%0AWe%20experiment%20with%20this%20new%20objective%2C%20called%20%24%5Cmathbb%7BX%7D%24-Sample%20Contrastive%2C%0Ato%20train%20vision%20models%20based%20on%20similarities%20in%20class%20or%20text%20caption%0Adescriptions.%20Our%20study%20spans%20three%20scales%3A%20ImageNet-1k%20with%201%20million%2C%20CC3M%0Awith%203%20million%2C%20and%20CC12M%20with%2012%20million%20samples.%20The%20representations%20learned%0Avia%20our%20objective%20outperform%20both%20contrastive%20self-supervised%20and%0Avision-language%20models%20trained%20on%20the%20same%20data%20across%20a%20range%20of%20tasks.%20When%0Atraining%20on%20CC12M%2C%20we%20outperform%20CLIP%20by%20%240.6%5C%25%24%20on%20both%20ImageNet%20and%20ImageNet%0AReal.%20Our%20objective%20appears%20to%20work%20particularly%20well%20in%20lower-data%20regimes%2C%0Awith%20gains%20over%20CLIP%20of%20%2416.8%5C%25%24%20on%20ImageNet%20and%20%2418.1%5C%25%24%20on%20ImageNet%20Real%20when%0Atraining%20with%20CC3M.%20Finally%2C%20our%20objective%20seems%20to%20encourage%20the%20model%20to%0Alearn%20representations%20that%20separate%20objects%20from%20their%20attributes%20and%0Abackgrounds%2C%20with%20gains%20of%20%243.3%24-%245.6%24%5C%25%20over%20CLIP%20on%20ImageNet9.%20We%20hope%20the%0Aproposed%20solution%20takes%20a%20small%20step%20towards%20developing%20richer%20learning%0Aobjectives%20for%20understanding%20sample%20relations%20in%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18134v1&entry.124074799=Read"},
{"title": "SlideSLAM: Sparse, Lightweight, Decentralized Metric-Semantic SLAM for\n  Multi-Robot Navigation", "author": "Xu Liu and Jiuzhou Lei and Ankit Prabhu and Yuezhan Tao and Igor Spasojevic and Pratik Chaudhari and Nikolay Atanasov and Vijay Kumar", "abstract": "  This paper develops a real-time decentralized metric-semantic Simultaneous\nLocalization and Mapping (SLAM) approach that leverages a sparse and\nlightweight object-based representation to enable a heterogeneous robot team to\nautonomously explore 3D environments featuring indoor, urban, and forested\nareas without relying on GPS. We use a hierarchical metric-semantic\nrepresentation of the environment, including high-level sparse semantic maps of\nobject models and low-level voxel maps. We leverage the informativeness and\nviewpoint invariance of the high-level semantic map to obtain an effective\nsemantics-driven place-recognition algorithm for inter-robot loop closure\ndetection across aerial and ground robots with different sensing modalities. A\ncommunication module is designed to track each robot's own observations and\nthose of other robots whenever communication links are available. Such\nobservations are then used to construct a merged map. Our framework enables\nreal-time decentralized operations onboard robots, allowing them to\nopportunistically leverage communication. We integrate and deploy our proposed\nframework on three types of aerial and ground robots. Extensive experimental\nresults show an average inter-robot localization error of approximately 20 cm\nin position and 0.2 degrees in orientation, an object mapping F1 score\nconsistently over 0.9, and a communication packet size of merely 2-3 megabytes\nper kilometer trajectory with as many as 1,000 landmarks. The project website\ncan be found at https://xurobotics.github.io/slideslam/.\n", "link": "http://arxiv.org/abs/2406.17249v3", "date": "2024-07-25", "relevancy": 2.5587, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.646}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6387}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SlideSLAM%3A%20Sparse%2C%20Lightweight%2C%20Decentralized%20Metric-Semantic%20SLAM%20for%0A%20%20Multi-Robot%20Navigation&body=Title%3A%20SlideSLAM%3A%20Sparse%2C%20Lightweight%2C%20Decentralized%20Metric-Semantic%20SLAM%20for%0A%20%20Multi-Robot%20Navigation%0AAuthor%3A%20Xu%20Liu%20and%20Jiuzhou%20Lei%20and%20Ankit%20Prabhu%20and%20Yuezhan%20Tao%20and%20Igor%20Spasojevic%20and%20Pratik%20Chaudhari%20and%20Nikolay%20Atanasov%20and%20Vijay%20Kumar%0AAbstract%3A%20%20%20This%20paper%20develops%20a%20real-time%20decentralized%20metric-semantic%20Simultaneous%0ALocalization%20and%20Mapping%20%28SLAM%29%20approach%20that%20leverages%20a%20sparse%20and%0Alightweight%20object-based%20representation%20to%20enable%20a%20heterogeneous%20robot%20team%20to%0Aautonomously%20explore%203D%20environments%20featuring%20indoor%2C%20urban%2C%20and%20forested%0Aareas%20without%20relying%20on%20GPS.%20We%20use%20a%20hierarchical%20metric-semantic%0Arepresentation%20of%20the%20environment%2C%20including%20high-level%20sparse%20semantic%20maps%20of%0Aobject%20models%20and%20low-level%20voxel%20maps.%20We%20leverage%20the%20informativeness%20and%0Aviewpoint%20invariance%20of%20the%20high-level%20semantic%20map%20to%20obtain%20an%20effective%0Asemantics-driven%20place-recognition%20algorithm%20for%20inter-robot%20loop%20closure%0Adetection%20across%20aerial%20and%20ground%20robots%20with%20different%20sensing%20modalities.%20A%0Acommunication%20module%20is%20designed%20to%20track%20each%20robot%27s%20own%20observations%20and%0Athose%20of%20other%20robots%20whenever%20communication%20links%20are%20available.%20Such%0Aobservations%20are%20then%20used%20to%20construct%20a%20merged%20map.%20Our%20framework%20enables%0Areal-time%20decentralized%20operations%20onboard%20robots%2C%20allowing%20them%20to%0Aopportunistically%20leverage%20communication.%20We%20integrate%20and%20deploy%20our%20proposed%0Aframework%20on%20three%20types%20of%20aerial%20and%20ground%20robots.%20Extensive%20experimental%0Aresults%20show%20an%20average%20inter-robot%20localization%20error%20of%20approximately%2020%20cm%0Ain%20position%20and%200.2%20degrees%20in%20orientation%2C%20an%20object%20mapping%20F1%20score%0Aconsistently%20over%200.9%2C%20and%20a%20communication%20packet%20size%20of%20merely%202-3%20megabytes%0Aper%20kilometer%20trajectory%20with%20as%20many%20as%201%2C000%20landmarks.%20The%20project%20website%0Acan%20be%20found%20at%20https%3A//xurobotics.github.io/slideslam/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17249v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlideSLAM%253A%2520Sparse%252C%2520Lightweight%252C%2520Decentralized%2520Metric-Semantic%2520SLAM%2520for%250A%2520%2520Multi-Robot%2520Navigation%26entry.906535625%3DXu%2520Liu%2520and%2520Jiuzhou%2520Lei%2520and%2520Ankit%2520Prabhu%2520and%2520Yuezhan%2520Tao%2520and%2520Igor%2520Spasojevic%2520and%2520Pratik%2520Chaudhari%2520and%2520Nikolay%2520Atanasov%2520and%2520Vijay%2520Kumar%26entry.1292438233%3D%2520%2520This%2520paper%2520develops%2520a%2520real-time%2520decentralized%2520metric-semantic%2520Simultaneous%250ALocalization%2520and%2520Mapping%2520%2528SLAM%2529%2520approach%2520that%2520leverages%2520a%2520sparse%2520and%250Alightweight%2520object-based%2520representation%2520to%2520enable%2520a%2520heterogeneous%2520robot%2520team%2520to%250Aautonomously%2520explore%25203D%2520environments%2520featuring%2520indoor%252C%2520urban%252C%2520and%2520forested%250Aareas%2520without%2520relying%2520on%2520GPS.%2520We%2520use%2520a%2520hierarchical%2520metric-semantic%250Arepresentation%2520of%2520the%2520environment%252C%2520including%2520high-level%2520sparse%2520semantic%2520maps%2520of%250Aobject%2520models%2520and%2520low-level%2520voxel%2520maps.%2520We%2520leverage%2520the%2520informativeness%2520and%250Aviewpoint%2520invariance%2520of%2520the%2520high-level%2520semantic%2520map%2520to%2520obtain%2520an%2520effective%250Asemantics-driven%2520place-recognition%2520algorithm%2520for%2520inter-robot%2520loop%2520closure%250Adetection%2520across%2520aerial%2520and%2520ground%2520robots%2520with%2520different%2520sensing%2520modalities.%2520A%250Acommunication%2520module%2520is%2520designed%2520to%2520track%2520each%2520robot%2527s%2520own%2520observations%2520and%250Athose%2520of%2520other%2520robots%2520whenever%2520communication%2520links%2520are%2520available.%2520Such%250Aobservations%2520are%2520then%2520used%2520to%2520construct%2520a%2520merged%2520map.%2520Our%2520framework%2520enables%250Areal-time%2520decentralized%2520operations%2520onboard%2520robots%252C%2520allowing%2520them%2520to%250Aopportunistically%2520leverage%2520communication.%2520We%2520integrate%2520and%2520deploy%2520our%2520proposed%250Aframework%2520on%2520three%2520types%2520of%2520aerial%2520and%2520ground%2520robots.%2520Extensive%2520experimental%250Aresults%2520show%2520an%2520average%2520inter-robot%2520localization%2520error%2520of%2520approximately%252020%2520cm%250Ain%2520position%2520and%25200.2%2520degrees%2520in%2520orientation%252C%2520an%2520object%2520mapping%2520F1%2520score%250Aconsistently%2520over%25200.9%252C%2520and%2520a%2520communication%2520packet%2520size%2520of%2520merely%25202-3%2520megabytes%250Aper%2520kilometer%2520trajectory%2520with%2520as%2520many%2520as%25201%252C000%2520landmarks.%2520The%2520project%2520website%250Acan%2520be%2520found%2520at%2520https%253A//xurobotics.github.io/slideslam/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17249v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SlideSLAM%3A%20Sparse%2C%20Lightweight%2C%20Decentralized%20Metric-Semantic%20SLAM%20for%0A%20%20Multi-Robot%20Navigation&entry.906535625=Xu%20Liu%20and%20Jiuzhou%20Lei%20and%20Ankit%20Prabhu%20and%20Yuezhan%20Tao%20and%20Igor%20Spasojevic%20and%20Pratik%20Chaudhari%20and%20Nikolay%20Atanasov%20and%20Vijay%20Kumar&entry.1292438233=%20%20This%20paper%20develops%20a%20real-time%20decentralized%20metric-semantic%20Simultaneous%0ALocalization%20and%20Mapping%20%28SLAM%29%20approach%20that%20leverages%20a%20sparse%20and%0Alightweight%20object-based%20representation%20to%20enable%20a%20heterogeneous%20robot%20team%20to%0Aautonomously%20explore%203D%20environments%20featuring%20indoor%2C%20urban%2C%20and%20forested%0Aareas%20without%20relying%20on%20GPS.%20We%20use%20a%20hierarchical%20metric-semantic%0Arepresentation%20of%20the%20environment%2C%20including%20high-level%20sparse%20semantic%20maps%20of%0Aobject%20models%20and%20low-level%20voxel%20maps.%20We%20leverage%20the%20informativeness%20and%0Aviewpoint%20invariance%20of%20the%20high-level%20semantic%20map%20to%20obtain%20an%20effective%0Asemantics-driven%20place-recognition%20algorithm%20for%20inter-robot%20loop%20closure%0Adetection%20across%20aerial%20and%20ground%20robots%20with%20different%20sensing%20modalities.%20A%0Acommunication%20module%20is%20designed%20to%20track%20each%20robot%27s%20own%20observations%20and%0Athose%20of%20other%20robots%20whenever%20communication%20links%20are%20available.%20Such%0Aobservations%20are%20then%20used%20to%20construct%20a%20merged%20map.%20Our%20framework%20enables%0Areal-time%20decentralized%20operations%20onboard%20robots%2C%20allowing%20them%20to%0Aopportunistically%20leverage%20communication.%20We%20integrate%20and%20deploy%20our%20proposed%0Aframework%20on%20three%20types%20of%20aerial%20and%20ground%20robots.%20Extensive%20experimental%0Aresults%20show%20an%20average%20inter-robot%20localization%20error%20of%20approximately%2020%20cm%0Ain%20position%20and%200.2%20degrees%20in%20orientation%2C%20an%20object%20mapping%20F1%20score%0Aconsistently%20over%200.9%2C%20and%20a%20communication%20packet%20size%20of%20merely%202-3%20megabytes%0Aper%20kilometer%20trajectory%20with%20as%20many%20as%201%2C000%20landmarks.%20The%20project%20website%0Acan%20be%20found%20at%20https%3A//xurobotics.github.io/slideslam/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17249v3&entry.124074799=Read"},
{"title": "Geometry Fidelity for Spherical Images", "author": "Anders Christensen and Nooshin Mojab and Khushman Patel and Karan Ahuja and Zeynep Akata and Ole Winther and Mar Gonzalez-Franco and Andrea Colaco", "abstract": "  Spherical or omni-directional images offer an immersive visual format\nappealing to a wide range of computer vision applications. However, geometric\nproperties of spherical images pose a major challenge for models and metrics\ndesigned for ordinary 2D images. Here, we show that direct application of\nFr\\'echet Inception Distance (FID) is insufficient for quantifying geometric\nfidelity in spherical images. We introduce two quantitative metrics accounting\nfor geometric constraints, namely Omnidirectional FID (OmniFID) and\nDiscontinuity Score (DS). OmniFID is an extension of FID tailored to\nadditionally capture field-of-view requirements of the spherical format by\nleveraging cubemap projections. DS is a kernel-based seam alignment score of\ncontinuity across borders of 2D representations of spherical images. In\nexperiments, OmniFID and DS quantify geometry fidelity issues that are\nundetected by FID.\n", "link": "http://arxiv.org/abs/2407.18207v1", "date": "2024-07-25", "relevancy": 2.5567, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5211}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5064}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometry%20Fidelity%20for%20Spherical%20Images&body=Title%3A%20Geometry%20Fidelity%20for%20Spherical%20Images%0AAuthor%3A%20Anders%20Christensen%20and%20Nooshin%20Mojab%20and%20Khushman%20Patel%20and%20Karan%20Ahuja%20and%20Zeynep%20Akata%20and%20Ole%20Winther%20and%20Mar%20Gonzalez-Franco%20and%20Andrea%20Colaco%0AAbstract%3A%20%20%20Spherical%20or%20omni-directional%20images%20offer%20an%20immersive%20visual%20format%0Aappealing%20to%20a%20wide%20range%20of%20computer%20vision%20applications.%20However%2C%20geometric%0Aproperties%20of%20spherical%20images%20pose%20a%20major%20challenge%20for%20models%20and%20metrics%0Adesigned%20for%20ordinary%202D%20images.%20Here%2C%20we%20show%20that%20direct%20application%20of%0AFr%5C%27echet%20Inception%20Distance%20%28FID%29%20is%20insufficient%20for%20quantifying%20geometric%0Afidelity%20in%20spherical%20images.%20We%20introduce%20two%20quantitative%20metrics%20accounting%0Afor%20geometric%20constraints%2C%20namely%20Omnidirectional%20FID%20%28OmniFID%29%20and%0ADiscontinuity%20Score%20%28DS%29.%20OmniFID%20is%20an%20extension%20of%20FID%20tailored%20to%0Aadditionally%20capture%20field-of-view%20requirements%20of%20the%20spherical%20format%20by%0Aleveraging%20cubemap%20projections.%20DS%20is%20a%20kernel-based%20seam%20alignment%20score%20of%0Acontinuity%20across%20borders%20of%202D%20representations%20of%20spherical%20images.%20In%0Aexperiments%2C%20OmniFID%20and%20DS%20quantify%20geometry%20fidelity%20issues%20that%20are%0Aundetected%20by%20FID.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometry%2520Fidelity%2520for%2520Spherical%2520Images%26entry.906535625%3DAnders%2520Christensen%2520and%2520Nooshin%2520Mojab%2520and%2520Khushman%2520Patel%2520and%2520Karan%2520Ahuja%2520and%2520Zeynep%2520Akata%2520and%2520Ole%2520Winther%2520and%2520Mar%2520Gonzalez-Franco%2520and%2520Andrea%2520Colaco%26entry.1292438233%3D%2520%2520Spherical%2520or%2520omni-directional%2520images%2520offer%2520an%2520immersive%2520visual%2520format%250Aappealing%2520to%2520a%2520wide%2520range%2520of%2520computer%2520vision%2520applications.%2520However%252C%2520geometric%250Aproperties%2520of%2520spherical%2520images%2520pose%2520a%2520major%2520challenge%2520for%2520models%2520and%2520metrics%250Adesigned%2520for%2520ordinary%25202D%2520images.%2520Here%252C%2520we%2520show%2520that%2520direct%2520application%2520of%250AFr%255C%2527echet%2520Inception%2520Distance%2520%2528FID%2529%2520is%2520insufficient%2520for%2520quantifying%2520geometric%250Afidelity%2520in%2520spherical%2520images.%2520We%2520introduce%2520two%2520quantitative%2520metrics%2520accounting%250Afor%2520geometric%2520constraints%252C%2520namely%2520Omnidirectional%2520FID%2520%2528OmniFID%2529%2520and%250ADiscontinuity%2520Score%2520%2528DS%2529.%2520OmniFID%2520is%2520an%2520extension%2520of%2520FID%2520tailored%2520to%250Aadditionally%2520capture%2520field-of-view%2520requirements%2520of%2520the%2520spherical%2520format%2520by%250Aleveraging%2520cubemap%2520projections.%2520DS%2520is%2520a%2520kernel-based%2520seam%2520alignment%2520score%2520of%250Acontinuity%2520across%2520borders%2520of%25202D%2520representations%2520of%2520spherical%2520images.%2520In%250Aexperiments%252C%2520OmniFID%2520and%2520DS%2520quantify%2520geometry%2520fidelity%2520issues%2520that%2520are%250Aundetected%2520by%2520FID.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometry%20Fidelity%20for%20Spherical%20Images&entry.906535625=Anders%20Christensen%20and%20Nooshin%20Mojab%20and%20Khushman%20Patel%20and%20Karan%20Ahuja%20and%20Zeynep%20Akata%20and%20Ole%20Winther%20and%20Mar%20Gonzalez-Franco%20and%20Andrea%20Colaco&entry.1292438233=%20%20Spherical%20or%20omni-directional%20images%20offer%20an%20immersive%20visual%20format%0Aappealing%20to%20a%20wide%20range%20of%20computer%20vision%20applications.%20However%2C%20geometric%0Aproperties%20of%20spherical%20images%20pose%20a%20major%20challenge%20for%20models%20and%20metrics%0Adesigned%20for%20ordinary%202D%20images.%20Here%2C%20we%20show%20that%20direct%20application%20of%0AFr%5C%27echet%20Inception%20Distance%20%28FID%29%20is%20insufficient%20for%20quantifying%20geometric%0Afidelity%20in%20spherical%20images.%20We%20introduce%20two%20quantitative%20metrics%20accounting%0Afor%20geometric%20constraints%2C%20namely%20Omnidirectional%20FID%20%28OmniFID%29%20and%0ADiscontinuity%20Score%20%28DS%29.%20OmniFID%20is%20an%20extension%20of%20FID%20tailored%20to%0Aadditionally%20capture%20field-of-view%20requirements%20of%20the%20spherical%20format%20by%0Aleveraging%20cubemap%20projections.%20DS%20is%20a%20kernel-based%20seam%20alignment%20score%20of%0Acontinuity%20across%20borders%20of%202D%20representations%20of%20spherical%20images.%20In%0Aexperiments%2C%20OmniFID%20and%20DS%20quantify%20geometry%20fidelity%20issues%20that%20are%0Aundetected%20by%20FID.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18207v1&entry.124074799=Read"},
{"title": "Personalized and Context-aware Route Planning for Edge-assisted Vehicles", "author": "Dinesh Cyril Selvaraj and Falko Dressler and Carla Fabiana Chiasserini", "abstract": "  Conventional route planning services typically offer the same routes to all\ndrivers, focusing primarily on a few standardized factors such as travel\ndistance or time, overlooking individual driver preferences. With the inception\nof autonomous vehicles expected in the coming years, where vehicles will rely\non routes decided by such planners, there arises a need to incorporate the\nspecific preferences of each driver, ensuring personalized navigation\nexperiences. In this work, we propose a novel approach based on graph neural\nnetworks (GNNs) and deep reinforcement learning (DRL), aimed at customizing\nroutes to suit individual preferences. By analyzing the historical trajectories\nof individual drivers, we classify their driving behavior and associate it with\nrelevant road attributes as indicators of driver preferences. The GNN is\ncapable of representing the road network as graph-structured data effectively,\nwhile DRL is capable of making decisions utilizing reward mechanisms to\noptimize route selection with factors such as travel costs, congestion level,\nand driver satisfaction. We evaluate our proposed GNN-based DRL framework using\na real-world road network and demonstrate its ability to accommodate driver\npreferences, offering a range of route options tailored to individual drivers.\nThe results indicate that our framework can select routes that accommodate\ndriver's preferences with up to a 17% improvement compared to a generic route\nplanner, and reduce the travel time by 33% (afternoon) and 46% (evening)\nrelatively to the shortest distance-based approach.\n", "link": "http://arxiv.org/abs/2407.17980v1", "date": "2024-07-25", "relevancy": 2.5413, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5362}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5005}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20and%20Context-aware%20Route%20Planning%20for%20Edge-assisted%20Vehicles&body=Title%3A%20Personalized%20and%20Context-aware%20Route%20Planning%20for%20Edge-assisted%20Vehicles%0AAuthor%3A%20Dinesh%20Cyril%20Selvaraj%20and%20Falko%20Dressler%20and%20Carla%20Fabiana%20Chiasserini%0AAbstract%3A%20%20%20Conventional%20route%20planning%20services%20typically%20offer%20the%20same%20routes%20to%20all%0Adrivers%2C%20focusing%20primarily%20on%20a%20few%20standardized%20factors%20such%20as%20travel%0Adistance%20or%20time%2C%20overlooking%20individual%20driver%20preferences.%20With%20the%20inception%0Aof%20autonomous%20vehicles%20expected%20in%20the%20coming%20years%2C%20where%20vehicles%20will%20rely%0Aon%20routes%20decided%20by%20such%20planners%2C%20there%20arises%20a%20need%20to%20incorporate%20the%0Aspecific%20preferences%20of%20each%20driver%2C%20ensuring%20personalized%20navigation%0Aexperiences.%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20based%20on%20graph%20neural%0Anetworks%20%28GNNs%29%20and%20deep%20reinforcement%20learning%20%28DRL%29%2C%20aimed%20at%20customizing%0Aroutes%20to%20suit%20individual%20preferences.%20By%20analyzing%20the%20historical%20trajectories%0Aof%20individual%20drivers%2C%20we%20classify%20their%20driving%20behavior%20and%20associate%20it%20with%0Arelevant%20road%20attributes%20as%20indicators%20of%20driver%20preferences.%20The%20GNN%20is%0Acapable%20of%20representing%20the%20road%20network%20as%20graph-structured%20data%20effectively%2C%0Awhile%20DRL%20is%20capable%20of%20making%20decisions%20utilizing%20reward%20mechanisms%20to%0Aoptimize%20route%20selection%20with%20factors%20such%20as%20travel%20costs%2C%20congestion%20level%2C%0Aand%20driver%20satisfaction.%20We%20evaluate%20our%20proposed%20GNN-based%20DRL%20framework%20using%0Aa%20real-world%20road%20network%20and%20demonstrate%20its%20ability%20to%20accommodate%20driver%0Apreferences%2C%20offering%20a%20range%20of%20route%20options%20tailored%20to%20individual%20drivers.%0AThe%20results%20indicate%20that%20our%20framework%20can%20select%20routes%20that%20accommodate%0Adriver%27s%20preferences%20with%20up%20to%20a%2017%25%20improvement%20compared%20to%20a%20generic%20route%0Aplanner%2C%20and%20reduce%20the%20travel%20time%20by%2033%25%20%28afternoon%29%20and%2046%25%20%28evening%29%0Arelatively%20to%20the%20shortest%20distance-based%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520and%2520Context-aware%2520Route%2520Planning%2520for%2520Edge-assisted%2520Vehicles%26entry.906535625%3DDinesh%2520Cyril%2520Selvaraj%2520and%2520Falko%2520Dressler%2520and%2520Carla%2520Fabiana%2520Chiasserini%26entry.1292438233%3D%2520%2520Conventional%2520route%2520planning%2520services%2520typically%2520offer%2520the%2520same%2520routes%2520to%2520all%250Adrivers%252C%2520focusing%2520primarily%2520on%2520a%2520few%2520standardized%2520factors%2520such%2520as%2520travel%250Adistance%2520or%2520time%252C%2520overlooking%2520individual%2520driver%2520preferences.%2520With%2520the%2520inception%250Aof%2520autonomous%2520vehicles%2520expected%2520in%2520the%2520coming%2520years%252C%2520where%2520vehicles%2520will%2520rely%250Aon%2520routes%2520decided%2520by%2520such%2520planners%252C%2520there%2520arises%2520a%2520need%2520to%2520incorporate%2520the%250Aspecific%2520preferences%2520of%2520each%2520driver%252C%2520ensuring%2520personalized%2520navigation%250Aexperiences.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520approach%2520based%2520on%2520graph%2520neural%250Anetworks%2520%2528GNNs%2529%2520and%2520deep%2520reinforcement%2520learning%2520%2528DRL%2529%252C%2520aimed%2520at%2520customizing%250Aroutes%2520to%2520suit%2520individual%2520preferences.%2520By%2520analyzing%2520the%2520historical%2520trajectories%250Aof%2520individual%2520drivers%252C%2520we%2520classify%2520their%2520driving%2520behavior%2520and%2520associate%2520it%2520with%250Arelevant%2520road%2520attributes%2520as%2520indicators%2520of%2520driver%2520preferences.%2520The%2520GNN%2520is%250Acapable%2520of%2520representing%2520the%2520road%2520network%2520as%2520graph-structured%2520data%2520effectively%252C%250Awhile%2520DRL%2520is%2520capable%2520of%2520making%2520decisions%2520utilizing%2520reward%2520mechanisms%2520to%250Aoptimize%2520route%2520selection%2520with%2520factors%2520such%2520as%2520travel%2520costs%252C%2520congestion%2520level%252C%250Aand%2520driver%2520satisfaction.%2520We%2520evaluate%2520our%2520proposed%2520GNN-based%2520DRL%2520framework%2520using%250Aa%2520real-world%2520road%2520network%2520and%2520demonstrate%2520its%2520ability%2520to%2520accommodate%2520driver%250Apreferences%252C%2520offering%2520a%2520range%2520of%2520route%2520options%2520tailored%2520to%2520individual%2520drivers.%250AThe%2520results%2520indicate%2520that%2520our%2520framework%2520can%2520select%2520routes%2520that%2520accommodate%250Adriver%2527s%2520preferences%2520with%2520up%2520to%2520a%252017%2525%2520improvement%2520compared%2520to%2520a%2520generic%2520route%250Aplanner%252C%2520and%2520reduce%2520the%2520travel%2520time%2520by%252033%2525%2520%2528afternoon%2529%2520and%252046%2525%2520%2528evening%2529%250Arelatively%2520to%2520the%2520shortest%2520distance-based%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20and%20Context-aware%20Route%20Planning%20for%20Edge-assisted%20Vehicles&entry.906535625=Dinesh%20Cyril%20Selvaraj%20and%20Falko%20Dressler%20and%20Carla%20Fabiana%20Chiasserini&entry.1292438233=%20%20Conventional%20route%20planning%20services%20typically%20offer%20the%20same%20routes%20to%20all%0Adrivers%2C%20focusing%20primarily%20on%20a%20few%20standardized%20factors%20such%20as%20travel%0Adistance%20or%20time%2C%20overlooking%20individual%20driver%20preferences.%20With%20the%20inception%0Aof%20autonomous%20vehicles%20expected%20in%20the%20coming%20years%2C%20where%20vehicles%20will%20rely%0Aon%20routes%20decided%20by%20such%20planners%2C%20there%20arises%20a%20need%20to%20incorporate%20the%0Aspecific%20preferences%20of%20each%20driver%2C%20ensuring%20personalized%20navigation%0Aexperiences.%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20based%20on%20graph%20neural%0Anetworks%20%28GNNs%29%20and%20deep%20reinforcement%20learning%20%28DRL%29%2C%20aimed%20at%20customizing%0Aroutes%20to%20suit%20individual%20preferences.%20By%20analyzing%20the%20historical%20trajectories%0Aof%20individual%20drivers%2C%20we%20classify%20their%20driving%20behavior%20and%20associate%20it%20with%0Arelevant%20road%20attributes%20as%20indicators%20of%20driver%20preferences.%20The%20GNN%20is%0Acapable%20of%20representing%20the%20road%20network%20as%20graph-structured%20data%20effectively%2C%0Awhile%20DRL%20is%20capable%20of%20making%20decisions%20utilizing%20reward%20mechanisms%20to%0Aoptimize%20route%20selection%20with%20factors%20such%20as%20travel%20costs%2C%20congestion%20level%2C%0Aand%20driver%20satisfaction.%20We%20evaluate%20our%20proposed%20GNN-based%20DRL%20framework%20using%0Aa%20real-world%20road%20network%20and%20demonstrate%20its%20ability%20to%20accommodate%20driver%0Apreferences%2C%20offering%20a%20range%20of%20route%20options%20tailored%20to%20individual%20drivers.%0AThe%20results%20indicate%20that%20our%20framework%20can%20select%20routes%20that%20accommodate%0Adriver%27s%20preferences%20with%20up%20to%20a%2017%25%20improvement%20compared%20to%20a%20generic%20route%0Aplanner%2C%20and%20reduce%20the%20travel%20time%20by%2033%25%20%28afternoon%29%20and%2046%25%20%28evening%29%0Arelatively%20to%20the%20shortest%20distance-based%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17980v1&entry.124074799=Read"},
{"title": "Towards More Practical Group Activity Detection: A New Benchmark and\n  Model", "author": "Dongkeun Kim and Youngkil Song and Minsu Cho and Suha Kwak", "abstract": "  Group activity detection (GAD) is the task of identifying members of each\ngroup and classifying the activity of the group at the same time in a video.\nWhile GAD has been studied recently, there is still much room for improvement\nin both dataset and methodology due to their limited capability to address\npractical GAD scenarios. To resolve these issues, we first present a new\ndataset, dubbed Caf\\'e. Unlike existing datasets, Caf\\'e is constructed\nprimarily for GAD and presents more practical scenarios and metrics, as well as\nbeing large-scale and providing rich annotations. Along with the dataset, we\npropose a new GAD model that deals with an unknown number of groups and latent\ngroup members efficiently and effectively. We evaluated our model on three\ndatasets including Caf\\'e, where it outperformed previous work in terms of both\naccuracy and inference speed.\n", "link": "http://arxiv.org/abs/2312.02878v2", "date": "2024-07-25", "relevancy": 2.525, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5397}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4891}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20More%20Practical%20Group%20Activity%20Detection%3A%20A%20New%20Benchmark%20and%0A%20%20Model&body=Title%3A%20Towards%20More%20Practical%20Group%20Activity%20Detection%3A%20A%20New%20Benchmark%20and%0A%20%20Model%0AAuthor%3A%20Dongkeun%20Kim%20and%20Youngkil%20Song%20and%20Minsu%20Cho%20and%20Suha%20Kwak%0AAbstract%3A%20%20%20Group%20activity%20detection%20%28GAD%29%20is%20the%20task%20of%20identifying%20members%20of%20each%0Agroup%20and%20classifying%20the%20activity%20of%20the%20group%20at%20the%20same%20time%20in%20a%20video.%0AWhile%20GAD%20has%20been%20studied%20recently%2C%20there%20is%20still%20much%20room%20for%20improvement%0Ain%20both%20dataset%20and%20methodology%20due%20to%20their%20limited%20capability%20to%20address%0Apractical%20GAD%20scenarios.%20To%20resolve%20these%20issues%2C%20we%20first%20present%20a%20new%0Adataset%2C%20dubbed%20Caf%5C%27e.%20Unlike%20existing%20datasets%2C%20Caf%5C%27e%20is%20constructed%0Aprimarily%20for%20GAD%20and%20presents%20more%20practical%20scenarios%20and%20metrics%2C%20as%20well%20as%0Abeing%20large-scale%20and%20providing%20rich%20annotations.%20Along%20with%20the%20dataset%2C%20we%0Apropose%20a%20new%20GAD%20model%20that%20deals%20with%20an%20unknown%20number%20of%20groups%20and%20latent%0Agroup%20members%20efficiently%20and%20effectively.%20We%20evaluated%20our%20model%20on%20three%0Adatasets%20including%20Caf%5C%27e%2C%20where%20it%20outperformed%20previous%20work%20in%20terms%20of%20both%0Aaccuracy%20and%20inference%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02878v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520More%2520Practical%2520Group%2520Activity%2520Detection%253A%2520A%2520New%2520Benchmark%2520and%250A%2520%2520Model%26entry.906535625%3DDongkeun%2520Kim%2520and%2520Youngkil%2520Song%2520and%2520Minsu%2520Cho%2520and%2520Suha%2520Kwak%26entry.1292438233%3D%2520%2520Group%2520activity%2520detection%2520%2528GAD%2529%2520is%2520the%2520task%2520of%2520identifying%2520members%2520of%2520each%250Agroup%2520and%2520classifying%2520the%2520activity%2520of%2520the%2520group%2520at%2520the%2520same%2520time%2520in%2520a%2520video.%250AWhile%2520GAD%2520has%2520been%2520studied%2520recently%252C%2520there%2520is%2520still%2520much%2520room%2520for%2520improvement%250Ain%2520both%2520dataset%2520and%2520methodology%2520due%2520to%2520their%2520limited%2520capability%2520to%2520address%250Apractical%2520GAD%2520scenarios.%2520To%2520resolve%2520these%2520issues%252C%2520we%2520first%2520present%2520a%2520new%250Adataset%252C%2520dubbed%2520Caf%255C%2527e.%2520Unlike%2520existing%2520datasets%252C%2520Caf%255C%2527e%2520is%2520constructed%250Aprimarily%2520for%2520GAD%2520and%2520presents%2520more%2520practical%2520scenarios%2520and%2520metrics%252C%2520as%2520well%2520as%250Abeing%2520large-scale%2520and%2520providing%2520rich%2520annotations.%2520Along%2520with%2520the%2520dataset%252C%2520we%250Apropose%2520a%2520new%2520GAD%2520model%2520that%2520deals%2520with%2520an%2520unknown%2520number%2520of%2520groups%2520and%2520latent%250Agroup%2520members%2520efficiently%2520and%2520effectively.%2520We%2520evaluated%2520our%2520model%2520on%2520three%250Adatasets%2520including%2520Caf%255C%2527e%252C%2520where%2520it%2520outperformed%2520previous%2520work%2520in%2520terms%2520of%2520both%250Aaccuracy%2520and%2520inference%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02878v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20More%20Practical%20Group%20Activity%20Detection%3A%20A%20New%20Benchmark%20and%0A%20%20Model&entry.906535625=Dongkeun%20Kim%20and%20Youngkil%20Song%20and%20Minsu%20Cho%20and%20Suha%20Kwak&entry.1292438233=%20%20Group%20activity%20detection%20%28GAD%29%20is%20the%20task%20of%20identifying%20members%20of%20each%0Agroup%20and%20classifying%20the%20activity%20of%20the%20group%20at%20the%20same%20time%20in%20a%20video.%0AWhile%20GAD%20has%20been%20studied%20recently%2C%20there%20is%20still%20much%20room%20for%20improvement%0Ain%20both%20dataset%20and%20methodology%20due%20to%20their%20limited%20capability%20to%20address%0Apractical%20GAD%20scenarios.%20To%20resolve%20these%20issues%2C%20we%20first%20present%20a%20new%0Adataset%2C%20dubbed%20Caf%5C%27e.%20Unlike%20existing%20datasets%2C%20Caf%5C%27e%20is%20constructed%0Aprimarily%20for%20GAD%20and%20presents%20more%20practical%20scenarios%20and%20metrics%2C%20as%20well%20as%0Abeing%20large-scale%20and%20providing%20rich%20annotations.%20Along%20with%20the%20dataset%2C%20we%0Apropose%20a%20new%20GAD%20model%20that%20deals%20with%20an%20unknown%20number%20of%20groups%20and%20latent%0Agroup%20members%20efficiently%20and%20effectively.%20We%20evaluated%20our%20model%20on%20three%0Adatasets%20including%20Caf%5C%27e%2C%20where%20it%20outperformed%20previous%20work%20in%20terms%20of%20both%0Aaccuracy%20and%20inference%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02878v2&entry.124074799=Read"},
{"title": "Category Adaptation Meets Projected Distillation in Generalized\n  Continual Category Discovery", "author": "Grzegorz Rype\u015b\u0107 and Daniel Marczak and Sebastian Cygert and Tomasz Trzci\u0144ski and Bart\u0142omiej Twardowski", "abstract": "  Generalized Continual Category Discovery (GCCD) tackles learning from\nsequentially arriving, partially labeled datasets while uncovering new\ncategories. Traditional methods depend on feature distillation to prevent\nforgetting the old knowledge. However, this strategy restricts the model's\nability to adapt and effectively distinguish new categories. To address this,\nwe introduce a novel technique integrating a learnable projector with feature\ndistillation, thus enhancing model adaptability without sacrificing past\nknowledge. The resulting distribution shift of the previously learned\ncategories is mitigated with the auxiliary category adaptation network. We\ndemonstrate that while each component offers modest benefits individually,\ntheir combination - dubbed CAMP (Category Adaptation Meets Projected\ndistillation) - significantly improves the balance between learning new\ninformation and retaining old. CAMP exhibits superior performance across\nseveral GCCD and Class Incremental Learning scenarios. The code is available at\nhttps://github.com/grypesc/CAMP.\n", "link": "http://arxiv.org/abs/2308.12112v4", "date": "2024-07-25", "relevancy": 2.5014, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5272}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4887}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Category%20Adaptation%20Meets%20Projected%20Distillation%20in%20Generalized%0A%20%20Continual%20Category%20Discovery&body=Title%3A%20Category%20Adaptation%20Meets%20Projected%20Distillation%20in%20Generalized%0A%20%20Continual%20Category%20Discovery%0AAuthor%3A%20Grzegorz%20Rype%C5%9B%C4%87%20and%20Daniel%20Marczak%20and%20Sebastian%20Cygert%20and%20Tomasz%20Trzci%C5%84ski%20and%20Bart%C5%82omiej%20Twardowski%0AAbstract%3A%20%20%20Generalized%20Continual%20Category%20Discovery%20%28GCCD%29%20tackles%20learning%20from%0Asequentially%20arriving%2C%20partially%20labeled%20datasets%20while%20uncovering%20new%0Acategories.%20Traditional%20methods%20depend%20on%20feature%20distillation%20to%20prevent%0Aforgetting%20the%20old%20knowledge.%20However%2C%20this%20strategy%20restricts%20the%20model%27s%0Aability%20to%20adapt%20and%20effectively%20distinguish%20new%20categories.%20To%20address%20this%2C%0Awe%20introduce%20a%20novel%20technique%20integrating%20a%20learnable%20projector%20with%20feature%0Adistillation%2C%20thus%20enhancing%20model%20adaptability%20without%20sacrificing%20past%0Aknowledge.%20The%20resulting%20distribution%20shift%20of%20the%20previously%20learned%0Acategories%20is%20mitigated%20with%20the%20auxiliary%20category%20adaptation%20network.%20We%0Ademonstrate%20that%20while%20each%20component%20offers%20modest%20benefits%20individually%2C%0Atheir%20combination%20-%20dubbed%20CAMP%20%28Category%20Adaptation%20Meets%20Projected%0Adistillation%29%20-%20significantly%20improves%20the%20balance%20between%20learning%20new%0Ainformation%20and%20retaining%20old.%20CAMP%20exhibits%20superior%20performance%20across%0Aseveral%20GCCD%20and%20Class%20Incremental%20Learning%20scenarios.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/grypesc/CAMP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.12112v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCategory%2520Adaptation%2520Meets%2520Projected%2520Distillation%2520in%2520Generalized%250A%2520%2520Continual%2520Category%2520Discovery%26entry.906535625%3DGrzegorz%2520Rype%25C5%259B%25C4%2587%2520and%2520Daniel%2520Marczak%2520and%2520Sebastian%2520Cygert%2520and%2520Tomasz%2520Trzci%25C5%2584ski%2520and%2520Bart%25C5%2582omiej%2520Twardowski%26entry.1292438233%3D%2520%2520Generalized%2520Continual%2520Category%2520Discovery%2520%2528GCCD%2529%2520tackles%2520learning%2520from%250Asequentially%2520arriving%252C%2520partially%2520labeled%2520datasets%2520while%2520uncovering%2520new%250Acategories.%2520Traditional%2520methods%2520depend%2520on%2520feature%2520distillation%2520to%2520prevent%250Aforgetting%2520the%2520old%2520knowledge.%2520However%252C%2520this%2520strategy%2520restricts%2520the%2520model%2527s%250Aability%2520to%2520adapt%2520and%2520effectively%2520distinguish%2520new%2520categories.%2520To%2520address%2520this%252C%250Awe%2520introduce%2520a%2520novel%2520technique%2520integrating%2520a%2520learnable%2520projector%2520with%2520feature%250Adistillation%252C%2520thus%2520enhancing%2520model%2520adaptability%2520without%2520sacrificing%2520past%250Aknowledge.%2520The%2520resulting%2520distribution%2520shift%2520of%2520the%2520previously%2520learned%250Acategories%2520is%2520mitigated%2520with%2520the%2520auxiliary%2520category%2520adaptation%2520network.%2520We%250Ademonstrate%2520that%2520while%2520each%2520component%2520offers%2520modest%2520benefits%2520individually%252C%250Atheir%2520combination%2520-%2520dubbed%2520CAMP%2520%2528Category%2520Adaptation%2520Meets%2520Projected%250Adistillation%2529%2520-%2520significantly%2520improves%2520the%2520balance%2520between%2520learning%2520new%250Ainformation%2520and%2520retaining%2520old.%2520CAMP%2520exhibits%2520superior%2520performance%2520across%250Aseveral%2520GCCD%2520and%2520Class%2520Incremental%2520Learning%2520scenarios.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/grypesc/CAMP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.12112v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Category%20Adaptation%20Meets%20Projected%20Distillation%20in%20Generalized%0A%20%20Continual%20Category%20Discovery&entry.906535625=Grzegorz%20Rype%C5%9B%C4%87%20and%20Daniel%20Marczak%20and%20Sebastian%20Cygert%20and%20Tomasz%20Trzci%C5%84ski%20and%20Bart%C5%82omiej%20Twardowski&entry.1292438233=%20%20Generalized%20Continual%20Category%20Discovery%20%28GCCD%29%20tackles%20learning%20from%0Asequentially%20arriving%2C%20partially%20labeled%20datasets%20while%20uncovering%20new%0Acategories.%20Traditional%20methods%20depend%20on%20feature%20distillation%20to%20prevent%0Aforgetting%20the%20old%20knowledge.%20However%2C%20this%20strategy%20restricts%20the%20model%27s%0Aability%20to%20adapt%20and%20effectively%20distinguish%20new%20categories.%20To%20address%20this%2C%0Awe%20introduce%20a%20novel%20technique%20integrating%20a%20learnable%20projector%20with%20feature%0Adistillation%2C%20thus%20enhancing%20model%20adaptability%20without%20sacrificing%20past%0Aknowledge.%20The%20resulting%20distribution%20shift%20of%20the%20previously%20learned%0Acategories%20is%20mitigated%20with%20the%20auxiliary%20category%20adaptation%20network.%20We%0Ademonstrate%20that%20while%20each%20component%20offers%20modest%20benefits%20individually%2C%0Atheir%20combination%20-%20dubbed%20CAMP%20%28Category%20Adaptation%20Meets%20Projected%0Adistillation%29%20-%20significantly%20improves%20the%20balance%20between%20learning%20new%0Ainformation%20and%20retaining%20old.%20CAMP%20exhibits%20superior%20performance%20across%0Aseveral%20GCCD%20and%20Class%20Incremental%20Learning%20scenarios.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/grypesc/CAMP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.12112v4&entry.124074799=Read"},
{"title": "PCR-99: A Practical Method for Point Cloud Registration with 99%\n  Outliers", "author": "Seong Hun Lee and Javier Civera and Patrick Vandewalle", "abstract": "  We propose a robust method for point cloud registration that can handle both\nunknown scales and extreme outlier ratios. Our method, dubbed PCR-99, uses a\ndeterministic 3-point sampling approach with two novel mechanisms that\nsignificantly boost the speed: (1) an improved ordering of the samples based on\npairwise scale consistency, prioritizing the point correspondences that are\nmore likely to be inliers, and (2) an efficient outlier rejection scheme based\non triplet scale consistency, prescreening bad samples and reducing the number\nof hypotheses to be tested. Our evaluation shows that, up to 98% outlier ratio,\nthe proposed method achieves comparable performance to the state of the art. At\n99% outlier ratio, however, it outperforms the state of the art for both\nknown-scale and unknown-scale problems. Especially for the latter, we observe a\nclear superiority in terms of robustness and speed.\n", "link": "http://arxiv.org/abs/2402.16598v4", "date": "2024-07-25", "relevancy": 2.4883, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5126}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4924}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PCR-99%3A%20A%20Practical%20Method%20for%20Point%20Cloud%20Registration%20with%2099%25%0A%20%20Outliers&body=Title%3A%20PCR-99%3A%20A%20Practical%20Method%20for%20Point%20Cloud%20Registration%20with%2099%25%0A%20%20Outliers%0AAuthor%3A%20Seong%20Hun%20Lee%20and%20Javier%20Civera%20and%20Patrick%20Vandewalle%0AAbstract%3A%20%20%20We%20propose%20a%20robust%20method%20for%20point%20cloud%20registration%20that%20can%20handle%20both%0Aunknown%20scales%20and%20extreme%20outlier%20ratios.%20Our%20method%2C%20dubbed%20PCR-99%2C%20uses%20a%0Adeterministic%203-point%20sampling%20approach%20with%20two%20novel%20mechanisms%20that%0Asignificantly%20boost%20the%20speed%3A%20%281%29%20an%20improved%20ordering%20of%20the%20samples%20based%20on%0Apairwise%20scale%20consistency%2C%20prioritizing%20the%20point%20correspondences%20that%20are%0Amore%20likely%20to%20be%20inliers%2C%20and%20%282%29%20an%20efficient%20outlier%20rejection%20scheme%20based%0Aon%20triplet%20scale%20consistency%2C%20prescreening%20bad%20samples%20and%20reducing%20the%20number%0Aof%20hypotheses%20to%20be%20tested.%20Our%20evaluation%20shows%20that%2C%20up%20to%2098%25%20outlier%20ratio%2C%0Athe%20proposed%20method%20achieves%20comparable%20performance%20to%20the%20state%20of%20the%20art.%20At%0A99%25%20outlier%20ratio%2C%20however%2C%20it%20outperforms%20the%20state%20of%20the%20art%20for%20both%0Aknown-scale%20and%20unknown-scale%20problems.%20Especially%20for%20the%20latter%2C%20we%20observe%20a%0Aclear%20superiority%20in%20terms%20of%20robustness%20and%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16598v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPCR-99%253A%2520A%2520Practical%2520Method%2520for%2520Point%2520Cloud%2520Registration%2520with%252099%2525%250A%2520%2520Outliers%26entry.906535625%3DSeong%2520Hun%2520Lee%2520and%2520Javier%2520Civera%2520and%2520Patrick%2520Vandewalle%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520robust%2520method%2520for%2520point%2520cloud%2520registration%2520that%2520can%2520handle%2520both%250Aunknown%2520scales%2520and%2520extreme%2520outlier%2520ratios.%2520Our%2520method%252C%2520dubbed%2520PCR-99%252C%2520uses%2520a%250Adeterministic%25203-point%2520sampling%2520approach%2520with%2520two%2520novel%2520mechanisms%2520that%250Asignificantly%2520boost%2520the%2520speed%253A%2520%25281%2529%2520an%2520improved%2520ordering%2520of%2520the%2520samples%2520based%2520on%250Apairwise%2520scale%2520consistency%252C%2520prioritizing%2520the%2520point%2520correspondences%2520that%2520are%250Amore%2520likely%2520to%2520be%2520inliers%252C%2520and%2520%25282%2529%2520an%2520efficient%2520outlier%2520rejection%2520scheme%2520based%250Aon%2520triplet%2520scale%2520consistency%252C%2520prescreening%2520bad%2520samples%2520and%2520reducing%2520the%2520number%250Aof%2520hypotheses%2520to%2520be%2520tested.%2520Our%2520evaluation%2520shows%2520that%252C%2520up%2520to%252098%2525%2520outlier%2520ratio%252C%250Athe%2520proposed%2520method%2520achieves%2520comparable%2520performance%2520to%2520the%2520state%2520of%2520the%2520art.%2520At%250A99%2525%2520outlier%2520ratio%252C%2520however%252C%2520it%2520outperforms%2520the%2520state%2520of%2520the%2520art%2520for%2520both%250Aknown-scale%2520and%2520unknown-scale%2520problems.%2520Especially%2520for%2520the%2520latter%252C%2520we%2520observe%2520a%250Aclear%2520superiority%2520in%2520terms%2520of%2520robustness%2520and%2520speed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.16598v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PCR-99%3A%20A%20Practical%20Method%20for%20Point%20Cloud%20Registration%20with%2099%25%0A%20%20Outliers&entry.906535625=Seong%20Hun%20Lee%20and%20Javier%20Civera%20and%20Patrick%20Vandewalle&entry.1292438233=%20%20We%20propose%20a%20robust%20method%20for%20point%20cloud%20registration%20that%20can%20handle%20both%0Aunknown%20scales%20and%20extreme%20outlier%20ratios.%20Our%20method%2C%20dubbed%20PCR-99%2C%20uses%20a%0Adeterministic%203-point%20sampling%20approach%20with%20two%20novel%20mechanisms%20that%0Asignificantly%20boost%20the%20speed%3A%20%281%29%20an%20improved%20ordering%20of%20the%20samples%20based%20on%0Apairwise%20scale%20consistency%2C%20prioritizing%20the%20point%20correspondences%20that%20are%0Amore%20likely%20to%20be%20inliers%2C%20and%20%282%29%20an%20efficient%20outlier%20rejection%20scheme%20based%0Aon%20triplet%20scale%20consistency%2C%20prescreening%20bad%20samples%20and%20reducing%20the%20number%0Aof%20hypotheses%20to%20be%20tested.%20Our%20evaluation%20shows%20that%2C%20up%20to%2098%25%20outlier%20ratio%2C%0Athe%20proposed%20method%20achieves%20comparable%20performance%20to%20the%20state%20of%20the%20art.%20At%0A99%25%20outlier%20ratio%2C%20however%2C%20it%20outperforms%20the%20state%20of%20the%20art%20for%20both%0Aknown-scale%20and%20unknown-scale%20problems.%20Especially%20for%20the%20latter%2C%20we%20observe%20a%0Aclear%20superiority%20in%20terms%20of%20robustness%20and%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16598v4&entry.124074799=Read"},
{"title": "ReCorD: Reasoning and Correcting Diffusion for HOI Generation", "author": "Jian-Yu Jiang-Lin and Kang-Yang Huang and Ling Lo and Yi-Ning Huang and Terence Lin and Jhih-Ciang Wu and Hong-Han Shuai and Wen-Huang Cheng", "abstract": "  Diffusion models revolutionize image generation by leveraging natural\nlanguage to guide the creation of multimedia content. Despite significant\nadvancements in such generative models, challenges persist in depicting\ndetailed human-object interactions, especially regarding pose and object\nplacement accuracy. We introduce a training-free method named Reasoning and\nCorrecting Diffusion (ReCorD) to address these challenges. Our model couples\nLatent Diffusion Models with Visual Language Models to refine the generation\nprocess, ensuring precise depictions of HOIs. We propose an interaction-aware\nreasoning module to improve the interpretation of the interaction, along with\nan interaction correcting module to refine the output image for more precise\nHOI generation delicately. Through a meticulous process of pose selection and\nobject positioning, ReCorD achieves superior fidelity in generated images while\nefficiently reducing computational requirements. We conduct comprehensive\nexperiments on three benchmarks to demonstrate the significant progress in\nsolving text-to-image generation tasks, showcasing ReCorD's ability to render\ncomplex interactions accurately by outperforming existing methods in HOI\nclassification score, as well as FID and Verb CLIP-Score. Project website is\navailable at https://alberthkyhky.github.io/ReCorD/ .\n", "link": "http://arxiv.org/abs/2407.17911v1", "date": "2024-07-25", "relevancy": 2.4806, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6273}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6169}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReCorD%3A%20Reasoning%20and%20Correcting%20Diffusion%20for%20HOI%20Generation&body=Title%3A%20ReCorD%3A%20Reasoning%20and%20Correcting%20Diffusion%20for%20HOI%20Generation%0AAuthor%3A%20Jian-Yu%20Jiang-Lin%20and%20Kang-Yang%20Huang%20and%20Ling%20Lo%20and%20Yi-Ning%20Huang%20and%20Terence%20Lin%20and%20Jhih-Ciang%20Wu%20and%20Hong-Han%20Shuai%20and%20Wen-Huang%20Cheng%0AAbstract%3A%20%20%20Diffusion%20models%20revolutionize%20image%20generation%20by%20leveraging%20natural%0Alanguage%20to%20guide%20the%20creation%20of%20multimedia%20content.%20Despite%20significant%0Aadvancements%20in%20such%20generative%20models%2C%20challenges%20persist%20in%20depicting%0Adetailed%20human-object%20interactions%2C%20especially%20regarding%20pose%20and%20object%0Aplacement%20accuracy.%20We%20introduce%20a%20training-free%20method%20named%20Reasoning%20and%0ACorrecting%20Diffusion%20%28ReCorD%29%20to%20address%20these%20challenges.%20Our%20model%20couples%0ALatent%20Diffusion%20Models%20with%20Visual%20Language%20Models%20to%20refine%20the%20generation%0Aprocess%2C%20ensuring%20precise%20depictions%20of%20HOIs.%20We%20propose%20an%20interaction-aware%0Areasoning%20module%20to%20improve%20the%20interpretation%20of%20the%20interaction%2C%20along%20with%0Aan%20interaction%20correcting%20module%20to%20refine%20the%20output%20image%20for%20more%20precise%0AHOI%20generation%20delicately.%20Through%20a%20meticulous%20process%20of%20pose%20selection%20and%0Aobject%20positioning%2C%20ReCorD%20achieves%20superior%20fidelity%20in%20generated%20images%20while%0Aefficiently%20reducing%20computational%20requirements.%20We%20conduct%20comprehensive%0Aexperiments%20on%20three%20benchmarks%20to%20demonstrate%20the%20significant%20progress%20in%0Asolving%20text-to-image%20generation%20tasks%2C%20showcasing%20ReCorD%27s%20ability%20to%20render%0Acomplex%20interactions%20accurately%20by%20outperforming%20existing%20methods%20in%20HOI%0Aclassification%20score%2C%20as%20well%20as%20FID%20and%20Verb%20CLIP-Score.%20Project%20website%20is%0Aavailable%20at%20https%3A//alberthkyhky.github.io/ReCorD/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17911v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReCorD%253A%2520Reasoning%2520and%2520Correcting%2520Diffusion%2520for%2520HOI%2520Generation%26entry.906535625%3DJian-Yu%2520Jiang-Lin%2520and%2520Kang-Yang%2520Huang%2520and%2520Ling%2520Lo%2520and%2520Yi-Ning%2520Huang%2520and%2520Terence%2520Lin%2520and%2520Jhih-Ciang%2520Wu%2520and%2520Hong-Han%2520Shuai%2520and%2520Wen-Huang%2520Cheng%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520revolutionize%2520image%2520generation%2520by%2520leveraging%2520natural%250Alanguage%2520to%2520guide%2520the%2520creation%2520of%2520multimedia%2520content.%2520Despite%2520significant%250Aadvancements%2520in%2520such%2520generative%2520models%252C%2520challenges%2520persist%2520in%2520depicting%250Adetailed%2520human-object%2520interactions%252C%2520especially%2520regarding%2520pose%2520and%2520object%250Aplacement%2520accuracy.%2520We%2520introduce%2520a%2520training-free%2520method%2520named%2520Reasoning%2520and%250ACorrecting%2520Diffusion%2520%2528ReCorD%2529%2520to%2520address%2520these%2520challenges.%2520Our%2520model%2520couples%250ALatent%2520Diffusion%2520Models%2520with%2520Visual%2520Language%2520Models%2520to%2520refine%2520the%2520generation%250Aprocess%252C%2520ensuring%2520precise%2520depictions%2520of%2520HOIs.%2520We%2520propose%2520an%2520interaction-aware%250Areasoning%2520module%2520to%2520improve%2520the%2520interpretation%2520of%2520the%2520interaction%252C%2520along%2520with%250Aan%2520interaction%2520correcting%2520module%2520to%2520refine%2520the%2520output%2520image%2520for%2520more%2520precise%250AHOI%2520generation%2520delicately.%2520Through%2520a%2520meticulous%2520process%2520of%2520pose%2520selection%2520and%250Aobject%2520positioning%252C%2520ReCorD%2520achieves%2520superior%2520fidelity%2520in%2520generated%2520images%2520while%250Aefficiently%2520reducing%2520computational%2520requirements.%2520We%2520conduct%2520comprehensive%250Aexperiments%2520on%2520three%2520benchmarks%2520to%2520demonstrate%2520the%2520significant%2520progress%2520in%250Asolving%2520text-to-image%2520generation%2520tasks%252C%2520showcasing%2520ReCorD%2527s%2520ability%2520to%2520render%250Acomplex%2520interactions%2520accurately%2520by%2520outperforming%2520existing%2520methods%2520in%2520HOI%250Aclassification%2520score%252C%2520as%2520well%2520as%2520FID%2520and%2520Verb%2520CLIP-Score.%2520Project%2520website%2520is%250Aavailable%2520at%2520https%253A//alberthkyhky.github.io/ReCorD/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17911v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReCorD%3A%20Reasoning%20and%20Correcting%20Diffusion%20for%20HOI%20Generation&entry.906535625=Jian-Yu%20Jiang-Lin%20and%20Kang-Yang%20Huang%20and%20Ling%20Lo%20and%20Yi-Ning%20Huang%20and%20Terence%20Lin%20and%20Jhih-Ciang%20Wu%20and%20Hong-Han%20Shuai%20and%20Wen-Huang%20Cheng&entry.1292438233=%20%20Diffusion%20models%20revolutionize%20image%20generation%20by%20leveraging%20natural%0Alanguage%20to%20guide%20the%20creation%20of%20multimedia%20content.%20Despite%20significant%0Aadvancements%20in%20such%20generative%20models%2C%20challenges%20persist%20in%20depicting%0Adetailed%20human-object%20interactions%2C%20especially%20regarding%20pose%20and%20object%0Aplacement%20accuracy.%20We%20introduce%20a%20training-free%20method%20named%20Reasoning%20and%0ACorrecting%20Diffusion%20%28ReCorD%29%20to%20address%20these%20challenges.%20Our%20model%20couples%0ALatent%20Diffusion%20Models%20with%20Visual%20Language%20Models%20to%20refine%20the%20generation%0Aprocess%2C%20ensuring%20precise%20depictions%20of%20HOIs.%20We%20propose%20an%20interaction-aware%0Areasoning%20module%20to%20improve%20the%20interpretation%20of%20the%20interaction%2C%20along%20with%0Aan%20interaction%20correcting%20module%20to%20refine%20the%20output%20image%20for%20more%20precise%0AHOI%20generation%20delicately.%20Through%20a%20meticulous%20process%20of%20pose%20selection%20and%0Aobject%20positioning%2C%20ReCorD%20achieves%20superior%20fidelity%20in%20generated%20images%20while%0Aefficiently%20reducing%20computational%20requirements.%20We%20conduct%20comprehensive%0Aexperiments%20on%20three%20benchmarks%20to%20demonstrate%20the%20significant%20progress%20in%0Asolving%20text-to-image%20generation%20tasks%2C%20showcasing%20ReCorD%27s%20ability%20to%20render%0Acomplex%20interactions%20accurately%20by%20outperforming%20existing%20methods%20in%20HOI%0Aclassification%20score%2C%20as%20well%20as%20FID%20and%20Verb%20CLIP-Score.%20Project%20website%20is%0Aavailable%20at%20https%3A//alberthkyhky.github.io/ReCorD/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17911v1&entry.124074799=Read"},
{"title": "BIV-Priv-Seg: Locating Private Content in Images Taken by People With\n  Visual Impairments", "author": "Yu-Yun Tseng and Tanusree Sharma and Lotus Zhang and Abigale Stangl and Leah Findlater and Yang Wang and Danna Gurari Yu-Yun Tseng and Tanusree Sharma and Lotus Zhang and Abigale Stangl and Leah Findlater and Yang Wang and Danna Gurari", "abstract": "  Individuals who are blind or have low vision (BLV) are at a heightened risk\nof sharing private information if they share photographs they have taken. To\nfacilitate developing technologies that can help preserve privacy, we introduce\nBIV-Priv-Seg, the first localization dataset originating from people with\nvisual impairments that shows private content. It contains 1,028 images with\nsegmentation annotations for 16 private object categories. We first\ncharacterize BIV-Priv-Seg and then evaluate modern models' performance for\nlocating private content in the dataset. We find modern models struggle most\nwith locating private objects that are not salient, small, and lack text as\nwell as recognizing when private content is absent from an image. We facilitate\nfuture extensions by sharing our new dataset with the evaluation server at\nhttps://vizwiz.org/tasks-and-datasets/object-localization.\n", "link": "http://arxiv.org/abs/2407.18243v1", "date": "2024-07-25", "relevancy": 2.4689, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4987}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4971}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4856}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BIV-Priv-Seg%3A%20Locating%20Private%20Content%20in%20Images%20Taken%20by%20People%20With%0A%20%20Visual%20Impairments&body=Title%3A%20BIV-Priv-Seg%3A%20Locating%20Private%20Content%20in%20Images%20Taken%20by%20People%20With%0A%20%20Visual%20Impairments%0AAuthor%3A%20Yu-Yun%20Tseng%20and%20Tanusree%20Sharma%20and%20Lotus%20Zhang%20and%20Abigale%20Stangl%20and%20Leah%20Findlater%20and%20Yang%20Wang%20and%20Danna%20Gurari%20Yu-Yun%20Tseng%20and%20Tanusree%20Sharma%20and%20Lotus%20Zhang%20and%20Abigale%20Stangl%20and%20Leah%20Findlater%20and%20Yang%20Wang%20and%20Danna%20Gurari%0AAbstract%3A%20%20%20Individuals%20who%20are%20blind%20or%20have%20low%20vision%20%28BLV%29%20are%20at%20a%20heightened%20risk%0Aof%20sharing%20private%20information%20if%20they%20share%20photographs%20they%20have%20taken.%20To%0Afacilitate%20developing%20technologies%20that%20can%20help%20preserve%20privacy%2C%20we%20introduce%0ABIV-Priv-Seg%2C%20the%20first%20localization%20dataset%20originating%20from%20people%20with%0Avisual%20impairments%20that%20shows%20private%20content.%20It%20contains%201%2C028%20images%20with%0Asegmentation%20annotations%20for%2016%20private%20object%20categories.%20We%20first%0Acharacterize%20BIV-Priv-Seg%20and%20then%20evaluate%20modern%20models%27%20performance%20for%0Alocating%20private%20content%20in%20the%20dataset.%20We%20find%20modern%20models%20struggle%20most%0Awith%20locating%20private%20objects%20that%20are%20not%20salient%2C%20small%2C%20and%20lack%20text%20as%0Awell%20as%20recognizing%20when%20private%20content%20is%20absent%20from%20an%20image.%20We%20facilitate%0Afuture%20extensions%20by%20sharing%20our%20new%20dataset%20with%20the%20evaluation%20server%20at%0Ahttps%3A//vizwiz.org/tasks-and-datasets/object-localization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18243v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBIV-Priv-Seg%253A%2520Locating%2520Private%2520Content%2520in%2520Images%2520Taken%2520by%2520People%2520With%250A%2520%2520Visual%2520Impairments%26entry.906535625%3DYu-Yun%2520Tseng%2520and%2520Tanusree%2520Sharma%2520and%2520Lotus%2520Zhang%2520and%2520Abigale%2520Stangl%2520and%2520Leah%2520Findlater%2520and%2520Yang%2520Wang%2520and%2520Danna%2520Gurari%2520Yu-Yun%2520Tseng%2520and%2520Tanusree%2520Sharma%2520and%2520Lotus%2520Zhang%2520and%2520Abigale%2520Stangl%2520and%2520Leah%2520Findlater%2520and%2520Yang%2520Wang%2520and%2520Danna%2520Gurari%26entry.1292438233%3D%2520%2520Individuals%2520who%2520are%2520blind%2520or%2520have%2520low%2520vision%2520%2528BLV%2529%2520are%2520at%2520a%2520heightened%2520risk%250Aof%2520sharing%2520private%2520information%2520if%2520they%2520share%2520photographs%2520they%2520have%2520taken.%2520To%250Afacilitate%2520developing%2520technologies%2520that%2520can%2520help%2520preserve%2520privacy%252C%2520we%2520introduce%250ABIV-Priv-Seg%252C%2520the%2520first%2520localization%2520dataset%2520originating%2520from%2520people%2520with%250Avisual%2520impairments%2520that%2520shows%2520private%2520content.%2520It%2520contains%25201%252C028%2520images%2520with%250Asegmentation%2520annotations%2520for%252016%2520private%2520object%2520categories.%2520We%2520first%250Acharacterize%2520BIV-Priv-Seg%2520and%2520then%2520evaluate%2520modern%2520models%2527%2520performance%2520for%250Alocating%2520private%2520content%2520in%2520the%2520dataset.%2520We%2520find%2520modern%2520models%2520struggle%2520most%250Awith%2520locating%2520private%2520objects%2520that%2520are%2520not%2520salient%252C%2520small%252C%2520and%2520lack%2520text%2520as%250Awell%2520as%2520recognizing%2520when%2520private%2520content%2520is%2520absent%2520from%2520an%2520image.%2520We%2520facilitate%250Afuture%2520extensions%2520by%2520sharing%2520our%2520new%2520dataset%2520with%2520the%2520evaluation%2520server%2520at%250Ahttps%253A//vizwiz.org/tasks-and-datasets/object-localization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18243v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BIV-Priv-Seg%3A%20Locating%20Private%20Content%20in%20Images%20Taken%20by%20People%20With%0A%20%20Visual%20Impairments&entry.906535625=Yu-Yun%20Tseng%20and%20Tanusree%20Sharma%20and%20Lotus%20Zhang%20and%20Abigale%20Stangl%20and%20Leah%20Findlater%20and%20Yang%20Wang%20and%20Danna%20Gurari%20Yu-Yun%20Tseng%20and%20Tanusree%20Sharma%20and%20Lotus%20Zhang%20and%20Abigale%20Stangl%20and%20Leah%20Findlater%20and%20Yang%20Wang%20and%20Danna%20Gurari&entry.1292438233=%20%20Individuals%20who%20are%20blind%20or%20have%20low%20vision%20%28BLV%29%20are%20at%20a%20heightened%20risk%0Aof%20sharing%20private%20information%20if%20they%20share%20photographs%20they%20have%20taken.%20To%0Afacilitate%20developing%20technologies%20that%20can%20help%20preserve%20privacy%2C%20we%20introduce%0ABIV-Priv-Seg%2C%20the%20first%20localization%20dataset%20originating%20from%20people%20with%0Avisual%20impairments%20that%20shows%20private%20content.%20It%20contains%201%2C028%20images%20with%0Asegmentation%20annotations%20for%2016%20private%20object%20categories.%20We%20first%0Acharacterize%20BIV-Priv-Seg%20and%20then%20evaluate%20modern%20models%27%20performance%20for%0Alocating%20private%20content%20in%20the%20dataset.%20We%20find%20modern%20models%20struggle%20most%0Awith%20locating%20private%20objects%20that%20are%20not%20salient%2C%20small%2C%20and%20lack%20text%20as%0Awell%20as%20recognizing%20when%20private%20content%20is%20absent%20from%20an%20image.%20We%20facilitate%0Afuture%20extensions%20by%20sharing%20our%20new%20dataset%20with%20the%20evaluation%20server%20at%0Ahttps%3A//vizwiz.org/tasks-and-datasets/object-localization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18243v1&entry.124074799=Read"},
{"title": "DINOv2 Rocks Geological Image Analysis: Classification, Segmentation,\n  and Interpretability", "author": "Florent Brondolo and Samuel Beaussant", "abstract": "  This study investigates the interpretability, classification, and\nsegmentation of CT-scan images of rock samples, with a particular focus on the\napplication of DINOv2 within Geosciences. We compared various segmentation\ntechniques to evaluate their efficacy, efficiency, and adaptability in\ngeological image analysis. The methods assessed include the Otsu thresholding\nmethod, clustering techniques (K-means and fuzzy C-means), a supervised machine\nlearning approach (Random Forest), and deep learning methods (UNet and DINOv2).\nWe tested these methods using ten binary sandstone datasets and three\nmulti-class calcite datasets. To begin, we provide a thorough interpretability\nanalysis of DINOv2's features in the geoscientific context, discussing its\nsuitability and inherent ability to process CT-scanned rock data. In terms of\nclassification, the out-of-the-box DINOv2 demonstrates an impressive capability\nto perfectly classify rock images, even when the CT scans are out of its\noriginal training set. Regarding segmentation, thresholding and unsupervised\nmethods, while fast, perform poorly despite image preprocessing, whereas\nsupervised methods show better results. We underscore the computational demands\nof deep learning but highlight its minimal intervention, superior\ngeneralization, and performance without additional image preprocessing.\nAdditionally, we observe a lack of correlation between a network's depth or the\nnumber of parameters and its performance. Our results show that a LoRA\nfine-tuned DINOv2 excels in out-of-distribution segmentation and significantly\noutperforms other methods in multi-class segmentation. By systematically\ncomparing these methods, we identify the most efficient strategy for meticulous\nand laborious segmentation tasks. DINOv2 proves advantageous, achieving\nsegmentations that could be described as \"better than ground-truth\" against\nrelatively small training sets.\n", "link": "http://arxiv.org/abs/2407.18100v1", "date": "2024-07-25", "relevancy": 2.4648, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4963}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4913}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DINOv2%20Rocks%20Geological%20Image%20Analysis%3A%20Classification%2C%20Segmentation%2C%0A%20%20and%20Interpretability&body=Title%3A%20DINOv2%20Rocks%20Geological%20Image%20Analysis%3A%20Classification%2C%20Segmentation%2C%0A%20%20and%20Interpretability%0AAuthor%3A%20Florent%20Brondolo%20and%20Samuel%20Beaussant%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20interpretability%2C%20classification%2C%20and%0Asegmentation%20of%20CT-scan%20images%20of%20rock%20samples%2C%20with%20a%20particular%20focus%20on%20the%0Aapplication%20of%20DINOv2%20within%20Geosciences.%20We%20compared%20various%20segmentation%0Atechniques%20to%20evaluate%20their%20efficacy%2C%20efficiency%2C%20and%20adaptability%20in%0Ageological%20image%20analysis.%20The%20methods%20assessed%20include%20the%20Otsu%20thresholding%0Amethod%2C%20clustering%20techniques%20%28K-means%20and%20fuzzy%20C-means%29%2C%20a%20supervised%20machine%0Alearning%20approach%20%28Random%20Forest%29%2C%20and%20deep%20learning%20methods%20%28UNet%20and%20DINOv2%29.%0AWe%20tested%20these%20methods%20using%20ten%20binary%20sandstone%20datasets%20and%20three%0Amulti-class%20calcite%20datasets.%20To%20begin%2C%20we%20provide%20a%20thorough%20interpretability%0Aanalysis%20of%20DINOv2%27s%20features%20in%20the%20geoscientific%20context%2C%20discussing%20its%0Asuitability%20and%20inherent%20ability%20to%20process%20CT-scanned%20rock%20data.%20In%20terms%20of%0Aclassification%2C%20the%20out-of-the-box%20DINOv2%20demonstrates%20an%20impressive%20capability%0Ato%20perfectly%20classify%20rock%20images%2C%20even%20when%20the%20CT%20scans%20are%20out%20of%20its%0Aoriginal%20training%20set.%20Regarding%20segmentation%2C%20thresholding%20and%20unsupervised%0Amethods%2C%20while%20fast%2C%20perform%20poorly%20despite%20image%20preprocessing%2C%20whereas%0Asupervised%20methods%20show%20better%20results.%20We%20underscore%20the%20computational%20demands%0Aof%20deep%20learning%20but%20highlight%20its%20minimal%20intervention%2C%20superior%0Ageneralization%2C%20and%20performance%20without%20additional%20image%20preprocessing.%0AAdditionally%2C%20we%20observe%20a%20lack%20of%20correlation%20between%20a%20network%27s%20depth%20or%20the%0Anumber%20of%20parameters%20and%20its%20performance.%20Our%20results%20show%20that%20a%20LoRA%0Afine-tuned%20DINOv2%20excels%20in%20out-of-distribution%20segmentation%20and%20significantly%0Aoutperforms%20other%20methods%20in%20multi-class%20segmentation.%20By%20systematically%0Acomparing%20these%20methods%2C%20we%20identify%20the%20most%20efficient%20strategy%20for%20meticulous%0Aand%20laborious%20segmentation%20tasks.%20DINOv2%20proves%20advantageous%2C%20achieving%0Asegmentations%20that%20could%20be%20described%20as%20%22better%20than%20ground-truth%22%20against%0Arelatively%20small%20training%20sets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18100v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDINOv2%2520Rocks%2520Geological%2520Image%2520Analysis%253A%2520Classification%252C%2520Segmentation%252C%250A%2520%2520and%2520Interpretability%26entry.906535625%3DFlorent%2520Brondolo%2520and%2520Samuel%2520Beaussant%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520the%2520interpretability%252C%2520classification%252C%2520and%250Asegmentation%2520of%2520CT-scan%2520images%2520of%2520rock%2520samples%252C%2520with%2520a%2520particular%2520focus%2520on%2520the%250Aapplication%2520of%2520DINOv2%2520within%2520Geosciences.%2520We%2520compared%2520various%2520segmentation%250Atechniques%2520to%2520evaluate%2520their%2520efficacy%252C%2520efficiency%252C%2520and%2520adaptability%2520in%250Ageological%2520image%2520analysis.%2520The%2520methods%2520assessed%2520include%2520the%2520Otsu%2520thresholding%250Amethod%252C%2520clustering%2520techniques%2520%2528K-means%2520and%2520fuzzy%2520C-means%2529%252C%2520a%2520supervised%2520machine%250Alearning%2520approach%2520%2528Random%2520Forest%2529%252C%2520and%2520deep%2520learning%2520methods%2520%2528UNet%2520and%2520DINOv2%2529.%250AWe%2520tested%2520these%2520methods%2520using%2520ten%2520binary%2520sandstone%2520datasets%2520and%2520three%250Amulti-class%2520calcite%2520datasets.%2520To%2520begin%252C%2520we%2520provide%2520a%2520thorough%2520interpretability%250Aanalysis%2520of%2520DINOv2%2527s%2520features%2520in%2520the%2520geoscientific%2520context%252C%2520discussing%2520its%250Asuitability%2520and%2520inherent%2520ability%2520to%2520process%2520CT-scanned%2520rock%2520data.%2520In%2520terms%2520of%250Aclassification%252C%2520the%2520out-of-the-box%2520DINOv2%2520demonstrates%2520an%2520impressive%2520capability%250Ato%2520perfectly%2520classify%2520rock%2520images%252C%2520even%2520when%2520the%2520CT%2520scans%2520are%2520out%2520of%2520its%250Aoriginal%2520training%2520set.%2520Regarding%2520segmentation%252C%2520thresholding%2520and%2520unsupervised%250Amethods%252C%2520while%2520fast%252C%2520perform%2520poorly%2520despite%2520image%2520preprocessing%252C%2520whereas%250Asupervised%2520methods%2520show%2520better%2520results.%2520We%2520underscore%2520the%2520computational%2520demands%250Aof%2520deep%2520learning%2520but%2520highlight%2520its%2520minimal%2520intervention%252C%2520superior%250Ageneralization%252C%2520and%2520performance%2520without%2520additional%2520image%2520preprocessing.%250AAdditionally%252C%2520we%2520observe%2520a%2520lack%2520of%2520correlation%2520between%2520a%2520network%2527s%2520depth%2520or%2520the%250Anumber%2520of%2520parameters%2520and%2520its%2520performance.%2520Our%2520results%2520show%2520that%2520a%2520LoRA%250Afine-tuned%2520DINOv2%2520excels%2520in%2520out-of-distribution%2520segmentation%2520and%2520significantly%250Aoutperforms%2520other%2520methods%2520in%2520multi-class%2520segmentation.%2520By%2520systematically%250Acomparing%2520these%2520methods%252C%2520we%2520identify%2520the%2520most%2520efficient%2520strategy%2520for%2520meticulous%250Aand%2520laborious%2520segmentation%2520tasks.%2520DINOv2%2520proves%2520advantageous%252C%2520achieving%250Asegmentations%2520that%2520could%2520be%2520described%2520as%2520%2522better%2520than%2520ground-truth%2522%2520against%250Arelatively%2520small%2520training%2520sets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18100v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DINOv2%20Rocks%20Geological%20Image%20Analysis%3A%20Classification%2C%20Segmentation%2C%0A%20%20and%20Interpretability&entry.906535625=Florent%20Brondolo%20and%20Samuel%20Beaussant&entry.1292438233=%20%20This%20study%20investigates%20the%20interpretability%2C%20classification%2C%20and%0Asegmentation%20of%20CT-scan%20images%20of%20rock%20samples%2C%20with%20a%20particular%20focus%20on%20the%0Aapplication%20of%20DINOv2%20within%20Geosciences.%20We%20compared%20various%20segmentation%0Atechniques%20to%20evaluate%20their%20efficacy%2C%20efficiency%2C%20and%20adaptability%20in%0Ageological%20image%20analysis.%20The%20methods%20assessed%20include%20the%20Otsu%20thresholding%0Amethod%2C%20clustering%20techniques%20%28K-means%20and%20fuzzy%20C-means%29%2C%20a%20supervised%20machine%0Alearning%20approach%20%28Random%20Forest%29%2C%20and%20deep%20learning%20methods%20%28UNet%20and%20DINOv2%29.%0AWe%20tested%20these%20methods%20using%20ten%20binary%20sandstone%20datasets%20and%20three%0Amulti-class%20calcite%20datasets.%20To%20begin%2C%20we%20provide%20a%20thorough%20interpretability%0Aanalysis%20of%20DINOv2%27s%20features%20in%20the%20geoscientific%20context%2C%20discussing%20its%0Asuitability%20and%20inherent%20ability%20to%20process%20CT-scanned%20rock%20data.%20In%20terms%20of%0Aclassification%2C%20the%20out-of-the-box%20DINOv2%20demonstrates%20an%20impressive%20capability%0Ato%20perfectly%20classify%20rock%20images%2C%20even%20when%20the%20CT%20scans%20are%20out%20of%20its%0Aoriginal%20training%20set.%20Regarding%20segmentation%2C%20thresholding%20and%20unsupervised%0Amethods%2C%20while%20fast%2C%20perform%20poorly%20despite%20image%20preprocessing%2C%20whereas%0Asupervised%20methods%20show%20better%20results.%20We%20underscore%20the%20computational%20demands%0Aof%20deep%20learning%20but%20highlight%20its%20minimal%20intervention%2C%20superior%0Ageneralization%2C%20and%20performance%20without%20additional%20image%20preprocessing.%0AAdditionally%2C%20we%20observe%20a%20lack%20of%20correlation%20between%20a%20network%27s%20depth%20or%20the%0Anumber%20of%20parameters%20and%20its%20performance.%20Our%20results%20show%20that%20a%20LoRA%0Afine-tuned%20DINOv2%20excels%20in%20out-of-distribution%20segmentation%20and%20significantly%0Aoutperforms%20other%20methods%20in%20multi-class%20segmentation.%20By%20systematically%0Acomparing%20these%20methods%2C%20we%20identify%20the%20most%20efficient%20strategy%20for%20meticulous%0Aand%20laborious%20segmentation%20tasks.%20DINOv2%20proves%20advantageous%2C%20achieving%0Asegmentations%20that%20could%20be%20described%20as%20%22better%20than%20ground-truth%22%20against%0Arelatively%20small%20training%20sets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18100v1&entry.124074799=Read"},
{"title": "Continual Panoptic Perception: Towards Multi-modal Incremental\n  Interpretation of Remote Sensing Images", "author": "Bo Yuan and Danpei Zhao and Zhuoran Liu and Wentao Li and Tian Li", "abstract": "  Continual learning (CL) breaks off the one-way training manner and enables a\nmodel to adapt to new data, semantics and tasks continuously. However, current\nCL methods mainly focus on single tasks. Besides, CL models are plagued by\ncatastrophic forgetting and semantic drift since the lack of old data, which\noften occurs in remote-sensing interpretation due to the intricate fine-grained\nsemantics. In this paper, we propose Continual Panoptic Perception (CPP), a\nunified continual learning model that leverages multi-task joint learning\ncovering pixel-level classification, instance-level segmentation and\nimage-level perception for universal interpretation in remote sensing images.\nConcretely, we propose a collaborative cross-modal encoder (CCE) to extract the\ninput image features, which supports pixel classification and caption\ngeneration synchronously. To inherit the knowledge from the old model without\nexemplar memory, we propose a task-interactive knowledge distillation (TKD)\nmethod, which leverages cross-modal optimization and task-asymmetric\npseudo-labeling (TPL) to alleviate catastrophic forgetting. Furthermore, we\nalso propose a joint optimization mechanism to achieve end-to-end multi-modal\npanoptic perception. Experimental results on the fine-grained panoptic\nperception dataset validate the effectiveness of the proposed model, and also\nprove that joint optimization can boost sub-task CL efficiency with over 13\\%\nrelative improvement on panoptic quality.\n", "link": "http://arxiv.org/abs/2407.14242v2", "date": "2024-07-25", "relevancy": 2.4647, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6305}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6151}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20Panoptic%20Perception%3A%20Towards%20Multi-modal%20Incremental%0A%20%20Interpretation%20of%20Remote%20Sensing%20Images&body=Title%3A%20Continual%20Panoptic%20Perception%3A%20Towards%20Multi-modal%20Incremental%0A%20%20Interpretation%20of%20Remote%20Sensing%20Images%0AAuthor%3A%20Bo%20Yuan%20and%20Danpei%20Zhao%20and%20Zhuoran%20Liu%20and%20Wentao%20Li%20and%20Tian%20Li%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20breaks%20off%20the%20one-way%20training%20manner%20and%20enables%20a%0Amodel%20to%20adapt%20to%20new%20data%2C%20semantics%20and%20tasks%20continuously.%20However%2C%20current%0ACL%20methods%20mainly%20focus%20on%20single%20tasks.%20Besides%2C%20CL%20models%20are%20plagued%20by%0Acatastrophic%20forgetting%20and%20semantic%20drift%20since%20the%20lack%20of%20old%20data%2C%20which%0Aoften%20occurs%20in%20remote-sensing%20interpretation%20due%20to%20the%20intricate%20fine-grained%0Asemantics.%20In%20this%20paper%2C%20we%20propose%20Continual%20Panoptic%20Perception%20%28CPP%29%2C%20a%0Aunified%20continual%20learning%20model%20that%20leverages%20multi-task%20joint%20learning%0Acovering%20pixel-level%20classification%2C%20instance-level%20segmentation%20and%0Aimage-level%20perception%20for%20universal%20interpretation%20in%20remote%20sensing%20images.%0AConcretely%2C%20we%20propose%20a%20collaborative%20cross-modal%20encoder%20%28CCE%29%20to%20extract%20the%0Ainput%20image%20features%2C%20which%20supports%20pixel%20classification%20and%20caption%0Ageneration%20synchronously.%20To%20inherit%20the%20knowledge%20from%20the%20old%20model%20without%0Aexemplar%20memory%2C%20we%20propose%20a%20task-interactive%20knowledge%20distillation%20%28TKD%29%0Amethod%2C%20which%20leverages%20cross-modal%20optimization%20and%20task-asymmetric%0Apseudo-labeling%20%28TPL%29%20to%20alleviate%20catastrophic%20forgetting.%20Furthermore%2C%20we%0Aalso%20propose%20a%20joint%20optimization%20mechanism%20to%20achieve%20end-to-end%20multi-modal%0Apanoptic%20perception.%20Experimental%20results%20on%20the%20fine-grained%20panoptic%0Aperception%20dataset%20validate%20the%20effectiveness%20of%20the%20proposed%20model%2C%20and%20also%0Aprove%20that%20joint%20optimization%20can%20boost%20sub-task%20CL%20efficiency%20with%20over%2013%5C%25%0Arelative%20improvement%20on%20panoptic%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.14242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520Panoptic%2520Perception%253A%2520Towards%2520Multi-modal%2520Incremental%250A%2520%2520Interpretation%2520of%2520Remote%2520Sensing%2520Images%26entry.906535625%3DBo%2520Yuan%2520and%2520Danpei%2520Zhao%2520and%2520Zhuoran%2520Liu%2520and%2520Wentao%2520Li%2520and%2520Tian%2520Li%26entry.1292438233%3D%2520%2520Continual%2520learning%2520%2528CL%2529%2520breaks%2520off%2520the%2520one-way%2520training%2520manner%2520and%2520enables%2520a%250Amodel%2520to%2520adapt%2520to%2520new%2520data%252C%2520semantics%2520and%2520tasks%2520continuously.%2520However%252C%2520current%250ACL%2520methods%2520mainly%2520focus%2520on%2520single%2520tasks.%2520Besides%252C%2520CL%2520models%2520are%2520plagued%2520by%250Acatastrophic%2520forgetting%2520and%2520semantic%2520drift%2520since%2520the%2520lack%2520of%2520old%2520data%252C%2520which%250Aoften%2520occurs%2520in%2520remote-sensing%2520interpretation%2520due%2520to%2520the%2520intricate%2520fine-grained%250Asemantics.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Continual%2520Panoptic%2520Perception%2520%2528CPP%2529%252C%2520a%250Aunified%2520continual%2520learning%2520model%2520that%2520leverages%2520multi-task%2520joint%2520learning%250Acovering%2520pixel-level%2520classification%252C%2520instance-level%2520segmentation%2520and%250Aimage-level%2520perception%2520for%2520universal%2520interpretation%2520in%2520remote%2520sensing%2520images.%250AConcretely%252C%2520we%2520propose%2520a%2520collaborative%2520cross-modal%2520encoder%2520%2528CCE%2529%2520to%2520extract%2520the%250Ainput%2520image%2520features%252C%2520which%2520supports%2520pixel%2520classification%2520and%2520caption%250Ageneration%2520synchronously.%2520To%2520inherit%2520the%2520knowledge%2520from%2520the%2520old%2520model%2520without%250Aexemplar%2520memory%252C%2520we%2520propose%2520a%2520task-interactive%2520knowledge%2520distillation%2520%2528TKD%2529%250Amethod%252C%2520which%2520leverages%2520cross-modal%2520optimization%2520and%2520task-asymmetric%250Apseudo-labeling%2520%2528TPL%2529%2520to%2520alleviate%2520catastrophic%2520forgetting.%2520Furthermore%252C%2520we%250Aalso%2520propose%2520a%2520joint%2520optimization%2520mechanism%2520to%2520achieve%2520end-to-end%2520multi-modal%250Apanoptic%2520perception.%2520Experimental%2520results%2520on%2520the%2520fine-grained%2520panoptic%250Aperception%2520dataset%2520validate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520model%252C%2520and%2520also%250Aprove%2520that%2520joint%2520optimization%2520can%2520boost%2520sub-task%2520CL%2520efficiency%2520with%2520over%252013%255C%2525%250Arelative%2520improvement%2520on%2520panoptic%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.14242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Panoptic%20Perception%3A%20Towards%20Multi-modal%20Incremental%0A%20%20Interpretation%20of%20Remote%20Sensing%20Images&entry.906535625=Bo%20Yuan%20and%20Danpei%20Zhao%20and%20Zhuoran%20Liu%20and%20Wentao%20Li%20and%20Tian%20Li&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20breaks%20off%20the%20one-way%20training%20manner%20and%20enables%20a%0Amodel%20to%20adapt%20to%20new%20data%2C%20semantics%20and%20tasks%20continuously.%20However%2C%20current%0ACL%20methods%20mainly%20focus%20on%20single%20tasks.%20Besides%2C%20CL%20models%20are%20plagued%20by%0Acatastrophic%20forgetting%20and%20semantic%20drift%20since%20the%20lack%20of%20old%20data%2C%20which%0Aoften%20occurs%20in%20remote-sensing%20interpretation%20due%20to%20the%20intricate%20fine-grained%0Asemantics.%20In%20this%20paper%2C%20we%20propose%20Continual%20Panoptic%20Perception%20%28CPP%29%2C%20a%0Aunified%20continual%20learning%20model%20that%20leverages%20multi-task%20joint%20learning%0Acovering%20pixel-level%20classification%2C%20instance-level%20segmentation%20and%0Aimage-level%20perception%20for%20universal%20interpretation%20in%20remote%20sensing%20images.%0AConcretely%2C%20we%20propose%20a%20collaborative%20cross-modal%20encoder%20%28CCE%29%20to%20extract%20the%0Ainput%20image%20features%2C%20which%20supports%20pixel%20classification%20and%20caption%0Ageneration%20synchronously.%20To%20inherit%20the%20knowledge%20from%20the%20old%20model%20without%0Aexemplar%20memory%2C%20we%20propose%20a%20task-interactive%20knowledge%20distillation%20%28TKD%29%0Amethod%2C%20which%20leverages%20cross-modal%20optimization%20and%20task-asymmetric%0Apseudo-labeling%20%28TPL%29%20to%20alleviate%20catastrophic%20forgetting.%20Furthermore%2C%20we%0Aalso%20propose%20a%20joint%20optimization%20mechanism%20to%20achieve%20end-to-end%20multi-modal%0Apanoptic%20perception.%20Experimental%20results%20on%20the%20fine-grained%20panoptic%0Aperception%20dataset%20validate%20the%20effectiveness%20of%20the%20proposed%20model%2C%20and%20also%0Aprove%20that%20joint%20optimization%20can%20boost%20sub-task%20CL%20efficiency%20with%20over%2013%5C%25%0Arelative%20improvement%20on%20panoptic%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.14242v2&entry.124074799=Read"},
{"title": "RefMask3D: Language-Guided Transformer for 3D Referring Segmentation", "author": "Shuting He and Henghui Ding", "abstract": "  3D referring segmentation is an emerging and challenging vision-language task\nthat aims to segment the object described by a natural language expression in a\npoint cloud scene. The key challenge behind this task is vision-language\nfeature fusion and alignment. In this work, we propose RefMask3D to explore the\ncomprehensive multi-modal feature interaction and understanding. First, we\npropose a Geometry-Enhanced Group-Word Attention to integrate language with\ngeometrically coherent sub-clouds through cross-modal group-word attention,\nwhich effectively addresses the challenges posed by the sparse and irregular\nnature of point clouds. Then, we introduce a Linguistic Primitives Construction\nto produce semantic primitives representing distinct semantic attributes, which\ngreatly enhance the vision-language understanding at the decoding stage.\nFurthermore, we introduce an Object Cluster Module that analyzes the\ninterrelationships among linguistic primitives to consolidate their insights\nand pinpoint common characteristics, helping to capture holistic information\nand enhance the precision of target identification. The proposed RefMask3D\nachieves new state-of-the-art performance on 3D referring segmentation, 3D\nvisual grounding, and also 2D referring image segmentation. Especially,\nRefMask3D outperforms previous state-of-the-art method by a large margin of\n3.16% mIoU} on the challenging ScanRefer dataset. Code is available at\nhttps://github.com/heshuting555/RefMask3D.\n", "link": "http://arxiv.org/abs/2407.18244v1", "date": "2024-07-25", "relevancy": 2.45, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6432}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5982}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RefMask3D%3A%20Language-Guided%20Transformer%20for%203D%20Referring%20Segmentation&body=Title%3A%20RefMask3D%3A%20Language-Guided%20Transformer%20for%203D%20Referring%20Segmentation%0AAuthor%3A%20Shuting%20He%20and%20Henghui%20Ding%0AAbstract%3A%20%20%203D%20referring%20segmentation%20is%20an%20emerging%20and%20challenging%20vision-language%20task%0Athat%20aims%20to%20segment%20the%20object%20described%20by%20a%20natural%20language%20expression%20in%20a%0Apoint%20cloud%20scene.%20The%20key%20challenge%20behind%20this%20task%20is%20vision-language%0Afeature%20fusion%20and%20alignment.%20In%20this%20work%2C%20we%20propose%20RefMask3D%20to%20explore%20the%0Acomprehensive%20multi-modal%20feature%20interaction%20and%20understanding.%20First%2C%20we%0Apropose%20a%20Geometry-Enhanced%20Group-Word%20Attention%20to%20integrate%20language%20with%0Ageometrically%20coherent%20sub-clouds%20through%20cross-modal%20group-word%20attention%2C%0Awhich%20effectively%20addresses%20the%20challenges%20posed%20by%20the%20sparse%20and%20irregular%0Anature%20of%20point%20clouds.%20Then%2C%20we%20introduce%20a%20Linguistic%20Primitives%20Construction%0Ato%20produce%20semantic%20primitives%20representing%20distinct%20semantic%20attributes%2C%20which%0Agreatly%20enhance%20the%20vision-language%20understanding%20at%20the%20decoding%20stage.%0AFurthermore%2C%20we%20introduce%20an%20Object%20Cluster%20Module%20that%20analyzes%20the%0Ainterrelationships%20among%20linguistic%20primitives%20to%20consolidate%20their%20insights%0Aand%20pinpoint%20common%20characteristics%2C%20helping%20to%20capture%20holistic%20information%0Aand%20enhance%20the%20precision%20of%20target%20identification.%20The%20proposed%20RefMask3D%0Aachieves%20new%20state-of-the-art%20performance%20on%203D%20referring%20segmentation%2C%203D%0Avisual%20grounding%2C%20and%20also%202D%20referring%20image%20segmentation.%20Especially%2C%0ARefMask3D%20outperforms%20previous%20state-of-the-art%20method%20by%20a%20large%20margin%20of%0A3.16%25%20mIoU%7D%20on%20the%20challenging%20ScanRefer%20dataset.%20Code%20is%20available%20at%0Ahttps%3A//github.com/heshuting555/RefMask3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18244v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRefMask3D%253A%2520Language-Guided%2520Transformer%2520for%25203D%2520Referring%2520Segmentation%26entry.906535625%3DShuting%2520He%2520and%2520Henghui%2520Ding%26entry.1292438233%3D%2520%25203D%2520referring%2520segmentation%2520is%2520an%2520emerging%2520and%2520challenging%2520vision-language%2520task%250Athat%2520aims%2520to%2520segment%2520the%2520object%2520described%2520by%2520a%2520natural%2520language%2520expression%2520in%2520a%250Apoint%2520cloud%2520scene.%2520The%2520key%2520challenge%2520behind%2520this%2520task%2520is%2520vision-language%250Afeature%2520fusion%2520and%2520alignment.%2520In%2520this%2520work%252C%2520we%2520propose%2520RefMask3D%2520to%2520explore%2520the%250Acomprehensive%2520multi-modal%2520feature%2520interaction%2520and%2520understanding.%2520First%252C%2520we%250Apropose%2520a%2520Geometry-Enhanced%2520Group-Word%2520Attention%2520to%2520integrate%2520language%2520with%250Ageometrically%2520coherent%2520sub-clouds%2520through%2520cross-modal%2520group-word%2520attention%252C%250Awhich%2520effectively%2520addresses%2520the%2520challenges%2520posed%2520by%2520the%2520sparse%2520and%2520irregular%250Anature%2520of%2520point%2520clouds.%2520Then%252C%2520we%2520introduce%2520a%2520Linguistic%2520Primitives%2520Construction%250Ato%2520produce%2520semantic%2520primitives%2520representing%2520distinct%2520semantic%2520attributes%252C%2520which%250Agreatly%2520enhance%2520the%2520vision-language%2520understanding%2520at%2520the%2520decoding%2520stage.%250AFurthermore%252C%2520we%2520introduce%2520an%2520Object%2520Cluster%2520Module%2520that%2520analyzes%2520the%250Ainterrelationships%2520among%2520linguistic%2520primitives%2520to%2520consolidate%2520their%2520insights%250Aand%2520pinpoint%2520common%2520characteristics%252C%2520helping%2520to%2520capture%2520holistic%2520information%250Aand%2520enhance%2520the%2520precision%2520of%2520target%2520identification.%2520The%2520proposed%2520RefMask3D%250Aachieves%2520new%2520state-of-the-art%2520performance%2520on%25203D%2520referring%2520segmentation%252C%25203D%250Avisual%2520grounding%252C%2520and%2520also%25202D%2520referring%2520image%2520segmentation.%2520Especially%252C%250ARefMask3D%2520outperforms%2520previous%2520state-of-the-art%2520method%2520by%2520a%2520large%2520margin%2520of%250A3.16%2525%2520mIoU%257D%2520on%2520the%2520challenging%2520ScanRefer%2520dataset.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/heshuting555/RefMask3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18244v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RefMask3D%3A%20Language-Guided%20Transformer%20for%203D%20Referring%20Segmentation&entry.906535625=Shuting%20He%20and%20Henghui%20Ding&entry.1292438233=%20%203D%20referring%20segmentation%20is%20an%20emerging%20and%20challenging%20vision-language%20task%0Athat%20aims%20to%20segment%20the%20object%20described%20by%20a%20natural%20language%20expression%20in%20a%0Apoint%20cloud%20scene.%20The%20key%20challenge%20behind%20this%20task%20is%20vision-language%0Afeature%20fusion%20and%20alignment.%20In%20this%20work%2C%20we%20propose%20RefMask3D%20to%20explore%20the%0Acomprehensive%20multi-modal%20feature%20interaction%20and%20understanding.%20First%2C%20we%0Apropose%20a%20Geometry-Enhanced%20Group-Word%20Attention%20to%20integrate%20language%20with%0Ageometrically%20coherent%20sub-clouds%20through%20cross-modal%20group-word%20attention%2C%0Awhich%20effectively%20addresses%20the%20challenges%20posed%20by%20the%20sparse%20and%20irregular%0Anature%20of%20point%20clouds.%20Then%2C%20we%20introduce%20a%20Linguistic%20Primitives%20Construction%0Ato%20produce%20semantic%20primitives%20representing%20distinct%20semantic%20attributes%2C%20which%0Agreatly%20enhance%20the%20vision-language%20understanding%20at%20the%20decoding%20stage.%0AFurthermore%2C%20we%20introduce%20an%20Object%20Cluster%20Module%20that%20analyzes%20the%0Ainterrelationships%20among%20linguistic%20primitives%20to%20consolidate%20their%20insights%0Aand%20pinpoint%20common%20characteristics%2C%20helping%20to%20capture%20holistic%20information%0Aand%20enhance%20the%20precision%20of%20target%20identification.%20The%20proposed%20RefMask3D%0Aachieves%20new%20state-of-the-art%20performance%20on%203D%20referring%20segmentation%2C%203D%0Avisual%20grounding%2C%20and%20also%202D%20referring%20image%20segmentation.%20Especially%2C%0ARefMask3D%20outperforms%20previous%20state-of-the-art%20method%20by%20a%20large%20margin%20of%0A3.16%25%20mIoU%7D%20on%20the%20challenging%20ScanRefer%20dataset.%20Code%20is%20available%20at%0Ahttps%3A//github.com/heshuting555/RefMask3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18244v1&entry.124074799=Read"},
{"title": "GaussianSR: High Fidelity 2D Gaussian Splatting for Arbitrary-Scale\n  Image Super-Resolution", "author": "Jintong Hu and Bin Xia and Bin Chen and Wenming Yang and Lei Zhang", "abstract": "  Implicit neural representations (INRs) have significantly advanced the field\nof arbitrary-scale super-resolution (ASSR) of images. Most existing INR-based\nASSR networks first extract features from the given low-resolution image using\nan encoder, and then render the super-resolved result via a multi-layer\nperceptron decoder. Although these approaches have shown promising results,\ntheir performance is constrained by the limited representation ability of\ndiscrete latent codes in the encoded features. In this paper, we propose a\nnovel ASSR method named GaussianSR that overcomes this limitation through 2D\nGaussian Splatting (2DGS). Unlike traditional methods that treat pixels as\ndiscrete points, GaussianSR represents each pixel as a continuous Gaussian\nfield. The encoded features are simultaneously refined and upsampled by\nrendering the mutually stacked Gaussian fields. As a result, long-range\ndependencies are established to enhance representation ability. In addition, a\nclassifier is developed to dynamically assign Gaussian kernels to all pixels to\nfurther improve flexibility. All components of GaussianSR (i.e., encoder,\nclassifier, Gaussian kernels, and decoder) are jointly learned end-to-end.\nExperiments demonstrate that GaussianSR achieves superior ASSR performance with\nfewer parameters than existing methods while enjoying interpretable and\ncontent-aware feature aggregations.\n", "link": "http://arxiv.org/abs/2407.18046v1", "date": "2024-07-25", "relevancy": 2.416, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6769}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5646}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianSR%3A%20High%20Fidelity%202D%20Gaussian%20Splatting%20for%20Arbitrary-Scale%0A%20%20Image%20Super-Resolution&body=Title%3A%20GaussianSR%3A%20High%20Fidelity%202D%20Gaussian%20Splatting%20for%20Arbitrary-Scale%0A%20%20Image%20Super-Resolution%0AAuthor%3A%20Jintong%20Hu%20and%20Bin%20Xia%20and%20Bin%20Chen%20and%20Wenming%20Yang%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20Implicit%20neural%20representations%20%28INRs%29%20have%20significantly%20advanced%20the%20field%0Aof%20arbitrary-scale%20super-resolution%20%28ASSR%29%20of%20images.%20Most%20existing%20INR-based%0AASSR%20networks%20first%20extract%20features%20from%20the%20given%20low-resolution%20image%20using%0Aan%20encoder%2C%20and%20then%20render%20the%20super-resolved%20result%20via%20a%20multi-layer%0Aperceptron%20decoder.%20Although%20these%20approaches%20have%20shown%20promising%20results%2C%0Atheir%20performance%20is%20constrained%20by%20the%20limited%20representation%20ability%20of%0Adiscrete%20latent%20codes%20in%20the%20encoded%20features.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20ASSR%20method%20named%20GaussianSR%20that%20overcomes%20this%20limitation%20through%202D%0AGaussian%20Splatting%20%282DGS%29.%20Unlike%20traditional%20methods%20that%20treat%20pixels%20as%0Adiscrete%20points%2C%20GaussianSR%20represents%20each%20pixel%20as%20a%20continuous%20Gaussian%0Afield.%20The%20encoded%20features%20are%20simultaneously%20refined%20and%20upsampled%20by%0Arendering%20the%20mutually%20stacked%20Gaussian%20fields.%20As%20a%20result%2C%20long-range%0Adependencies%20are%20established%20to%20enhance%20representation%20ability.%20In%20addition%2C%20a%0Aclassifier%20is%20developed%20to%20dynamically%20assign%20Gaussian%20kernels%20to%20all%20pixels%20to%0Afurther%20improve%20flexibility.%20All%20components%20of%20GaussianSR%20%28i.e.%2C%20encoder%2C%0Aclassifier%2C%20Gaussian%20kernels%2C%20and%20decoder%29%20are%20jointly%20learned%20end-to-end.%0AExperiments%20demonstrate%20that%20GaussianSR%20achieves%20superior%20ASSR%20performance%20with%0Afewer%20parameters%20than%20existing%20methods%20while%20enjoying%20interpretable%20and%0Acontent-aware%20feature%20aggregations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianSR%253A%2520High%2520Fidelity%25202D%2520Gaussian%2520Splatting%2520for%2520Arbitrary-Scale%250A%2520%2520Image%2520Super-Resolution%26entry.906535625%3DJintong%2520Hu%2520and%2520Bin%2520Xia%2520and%2520Bin%2520Chen%2520and%2520Wenming%2520Yang%2520and%2520Lei%2520Zhang%26entry.1292438233%3D%2520%2520Implicit%2520neural%2520representations%2520%2528INRs%2529%2520have%2520significantly%2520advanced%2520the%2520field%250Aof%2520arbitrary-scale%2520super-resolution%2520%2528ASSR%2529%2520of%2520images.%2520Most%2520existing%2520INR-based%250AASSR%2520networks%2520first%2520extract%2520features%2520from%2520the%2520given%2520low-resolution%2520image%2520using%250Aan%2520encoder%252C%2520and%2520then%2520render%2520the%2520super-resolved%2520result%2520via%2520a%2520multi-layer%250Aperceptron%2520decoder.%2520Although%2520these%2520approaches%2520have%2520shown%2520promising%2520results%252C%250Atheir%2520performance%2520is%2520constrained%2520by%2520the%2520limited%2520representation%2520ability%2520of%250Adiscrete%2520latent%2520codes%2520in%2520the%2520encoded%2520features.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520ASSR%2520method%2520named%2520GaussianSR%2520that%2520overcomes%2520this%2520limitation%2520through%25202D%250AGaussian%2520Splatting%2520%25282DGS%2529.%2520Unlike%2520traditional%2520methods%2520that%2520treat%2520pixels%2520as%250Adiscrete%2520points%252C%2520GaussianSR%2520represents%2520each%2520pixel%2520as%2520a%2520continuous%2520Gaussian%250Afield.%2520The%2520encoded%2520features%2520are%2520simultaneously%2520refined%2520and%2520upsampled%2520by%250Arendering%2520the%2520mutually%2520stacked%2520Gaussian%2520fields.%2520As%2520a%2520result%252C%2520long-range%250Adependencies%2520are%2520established%2520to%2520enhance%2520representation%2520ability.%2520In%2520addition%252C%2520a%250Aclassifier%2520is%2520developed%2520to%2520dynamically%2520assign%2520Gaussian%2520kernels%2520to%2520all%2520pixels%2520to%250Afurther%2520improve%2520flexibility.%2520All%2520components%2520of%2520GaussianSR%2520%2528i.e.%252C%2520encoder%252C%250Aclassifier%252C%2520Gaussian%2520kernels%252C%2520and%2520decoder%2529%2520are%2520jointly%2520learned%2520end-to-end.%250AExperiments%2520demonstrate%2520that%2520GaussianSR%2520achieves%2520superior%2520ASSR%2520performance%2520with%250Afewer%2520parameters%2520than%2520existing%2520methods%2520while%2520enjoying%2520interpretable%2520and%250Acontent-aware%2520feature%2520aggregations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianSR%3A%20High%20Fidelity%202D%20Gaussian%20Splatting%20for%20Arbitrary-Scale%0A%20%20Image%20Super-Resolution&entry.906535625=Jintong%20Hu%20and%20Bin%20Xia%20and%20Bin%20Chen%20and%20Wenming%20Yang%20and%20Lei%20Zhang&entry.1292438233=%20%20Implicit%20neural%20representations%20%28INRs%29%20have%20significantly%20advanced%20the%20field%0Aof%20arbitrary-scale%20super-resolution%20%28ASSR%29%20of%20images.%20Most%20existing%20INR-based%0AASSR%20networks%20first%20extract%20features%20from%20the%20given%20low-resolution%20image%20using%0Aan%20encoder%2C%20and%20then%20render%20the%20super-resolved%20result%20via%20a%20multi-layer%0Aperceptron%20decoder.%20Although%20these%20approaches%20have%20shown%20promising%20results%2C%0Atheir%20performance%20is%20constrained%20by%20the%20limited%20representation%20ability%20of%0Adiscrete%20latent%20codes%20in%20the%20encoded%20features.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20ASSR%20method%20named%20GaussianSR%20that%20overcomes%20this%20limitation%20through%202D%0AGaussian%20Splatting%20%282DGS%29.%20Unlike%20traditional%20methods%20that%20treat%20pixels%20as%0Adiscrete%20points%2C%20GaussianSR%20represents%20each%20pixel%20as%20a%20continuous%20Gaussian%0Afield.%20The%20encoded%20features%20are%20simultaneously%20refined%20and%20upsampled%20by%0Arendering%20the%20mutually%20stacked%20Gaussian%20fields.%20As%20a%20result%2C%20long-range%0Adependencies%20are%20established%20to%20enhance%20representation%20ability.%20In%20addition%2C%20a%0Aclassifier%20is%20developed%20to%20dynamically%20assign%20Gaussian%20kernels%20to%20all%20pixels%20to%0Afurther%20improve%20flexibility.%20All%20components%20of%20GaussianSR%20%28i.e.%2C%20encoder%2C%0Aclassifier%2C%20Gaussian%20kernels%2C%20and%20decoder%29%20are%20jointly%20learned%20end-to-end.%0AExperiments%20demonstrate%20that%20GaussianSR%20achieves%20superior%20ASSR%20performance%20with%0Afewer%20parameters%20than%20existing%20methods%20while%20enjoying%20interpretable%20and%0Acontent-aware%20feature%20aggregations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18046v1&entry.124074799=Read"},
{"title": "InternVideo2: Scaling Foundation Models for Multimodal Video\n  Understanding", "author": "Yi Wang and Kunchang Li and Xinhao Li and Jiashuo Yu and Yinan He and Chenting Wang and Guo Chen and Baoqi Pei and Ziang Yan and Rongkun Zheng and Jilan Xu and Zun Wang and Yansong Shi and Tianxiang Jiang and Songze Li and Hongjie Zhang and Yifei Huang and Yu Qiao and Yali Wang and Limin Wang", "abstract": "  We introduce InternVideo2, a new family of video foundation models (ViFM)\nthat achieve the state-of-the-art results in video recognition, video-text\ntasks, and video-centric dialogue. Our core design is a progressive training\napproach that unifies the masked video modeling, crossmodal contrastive\nlearning, and next token prediction, scaling up the video encoder size to 6B\nparameters. At the data level, we prioritize spatiotemporal consistency by\nsemantically segmenting videos and generating video-audio-speech captions. This\nimproves the alignment between video and text. Through extensive experiments,\nwe validate our designs and demonstrate superior performance on over 60 video\nand audio tasks. Notably, our model outperforms others on various video-related\ndialogue and long video understanding benchmarks, highlighting its ability to\nreason and comprehend longer contexts. Code and models are available at\nhttps://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2/.\n", "link": "http://arxiv.org/abs/2403.15377v2", "date": "2024-07-25", "relevancy": 2.3548, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.602}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5928}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InternVideo2%3A%20Scaling%20Foundation%20Models%20for%20Multimodal%20Video%0A%20%20Understanding&body=Title%3A%20InternVideo2%3A%20Scaling%20Foundation%20Models%20for%20Multimodal%20Video%0A%20%20Understanding%0AAuthor%3A%20Yi%20Wang%20and%20Kunchang%20Li%20and%20Xinhao%20Li%20and%20Jiashuo%20Yu%20and%20Yinan%20He%20and%20Chenting%20Wang%20and%20Guo%20Chen%20and%20Baoqi%20Pei%20and%20Ziang%20Yan%20and%20Rongkun%20Zheng%20and%20Jilan%20Xu%20and%20Zun%20Wang%20and%20Yansong%20Shi%20and%20Tianxiang%20Jiang%20and%20Songze%20Li%20and%20Hongjie%20Zhang%20and%20Yifei%20Huang%20and%20Yu%20Qiao%20and%20Yali%20Wang%20and%20Limin%20Wang%0AAbstract%3A%20%20%20We%20introduce%20InternVideo2%2C%20a%20new%20family%20of%20video%20foundation%20models%20%28ViFM%29%0Athat%20achieve%20the%20state-of-the-art%20results%20in%20video%20recognition%2C%20video-text%0Atasks%2C%20and%20video-centric%20dialogue.%20Our%20core%20design%20is%20a%20progressive%20training%0Aapproach%20that%20unifies%20the%20masked%20video%20modeling%2C%20crossmodal%20contrastive%0Alearning%2C%20and%20next%20token%20prediction%2C%20scaling%20up%20the%20video%20encoder%20size%20to%206B%0Aparameters.%20At%20the%20data%20level%2C%20we%20prioritize%20spatiotemporal%20consistency%20by%0Asemantically%20segmenting%20videos%20and%20generating%20video-audio-speech%20captions.%20This%0Aimproves%20the%20alignment%20between%20video%20and%20text.%20Through%20extensive%20experiments%2C%0Awe%20validate%20our%20designs%20and%20demonstrate%20superior%20performance%20on%20over%2060%20video%0Aand%20audio%20tasks.%20Notably%2C%20our%20model%20outperforms%20others%20on%20various%20video-related%0Adialogue%20and%20long%20video%20understanding%20benchmarks%2C%20highlighting%20its%20ability%20to%0Areason%20and%20comprehend%20longer%20contexts.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/OpenGVLab/InternVideo/tree/main/InternVideo2/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15377v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInternVideo2%253A%2520Scaling%2520Foundation%2520Models%2520for%2520Multimodal%2520Video%250A%2520%2520Understanding%26entry.906535625%3DYi%2520Wang%2520and%2520Kunchang%2520Li%2520and%2520Xinhao%2520Li%2520and%2520Jiashuo%2520Yu%2520and%2520Yinan%2520He%2520and%2520Chenting%2520Wang%2520and%2520Guo%2520Chen%2520and%2520Baoqi%2520Pei%2520and%2520Ziang%2520Yan%2520and%2520Rongkun%2520Zheng%2520and%2520Jilan%2520Xu%2520and%2520Zun%2520Wang%2520and%2520Yansong%2520Shi%2520and%2520Tianxiang%2520Jiang%2520and%2520Songze%2520Li%2520and%2520Hongjie%2520Zhang%2520and%2520Yifei%2520Huang%2520and%2520Yu%2520Qiao%2520and%2520Yali%2520Wang%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520InternVideo2%252C%2520a%2520new%2520family%2520of%2520video%2520foundation%2520models%2520%2528ViFM%2529%250Athat%2520achieve%2520the%2520state-of-the-art%2520results%2520in%2520video%2520recognition%252C%2520video-text%250Atasks%252C%2520and%2520video-centric%2520dialogue.%2520Our%2520core%2520design%2520is%2520a%2520progressive%2520training%250Aapproach%2520that%2520unifies%2520the%2520masked%2520video%2520modeling%252C%2520crossmodal%2520contrastive%250Alearning%252C%2520and%2520next%2520token%2520prediction%252C%2520scaling%2520up%2520the%2520video%2520encoder%2520size%2520to%25206B%250Aparameters.%2520At%2520the%2520data%2520level%252C%2520we%2520prioritize%2520spatiotemporal%2520consistency%2520by%250Asemantically%2520segmenting%2520videos%2520and%2520generating%2520video-audio-speech%2520captions.%2520This%250Aimproves%2520the%2520alignment%2520between%2520video%2520and%2520text.%2520Through%2520extensive%2520experiments%252C%250Awe%2520validate%2520our%2520designs%2520and%2520demonstrate%2520superior%2520performance%2520on%2520over%252060%2520video%250Aand%2520audio%2520tasks.%2520Notably%252C%2520our%2520model%2520outperforms%2520others%2520on%2520various%2520video-related%250Adialogue%2520and%2520long%2520video%2520understanding%2520benchmarks%252C%2520highlighting%2520its%2520ability%2520to%250Areason%2520and%2520comprehend%2520longer%2520contexts.%2520Code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/OpenGVLab/InternVideo/tree/main/InternVideo2/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15377v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InternVideo2%3A%20Scaling%20Foundation%20Models%20for%20Multimodal%20Video%0A%20%20Understanding&entry.906535625=Yi%20Wang%20and%20Kunchang%20Li%20and%20Xinhao%20Li%20and%20Jiashuo%20Yu%20and%20Yinan%20He%20and%20Chenting%20Wang%20and%20Guo%20Chen%20and%20Baoqi%20Pei%20and%20Ziang%20Yan%20and%20Rongkun%20Zheng%20and%20Jilan%20Xu%20and%20Zun%20Wang%20and%20Yansong%20Shi%20and%20Tianxiang%20Jiang%20and%20Songze%20Li%20and%20Hongjie%20Zhang%20and%20Yifei%20Huang%20and%20Yu%20Qiao%20and%20Yali%20Wang%20and%20Limin%20Wang&entry.1292438233=%20%20We%20introduce%20InternVideo2%2C%20a%20new%20family%20of%20video%20foundation%20models%20%28ViFM%29%0Athat%20achieve%20the%20state-of-the-art%20results%20in%20video%20recognition%2C%20video-text%0Atasks%2C%20and%20video-centric%20dialogue.%20Our%20core%20design%20is%20a%20progressive%20training%0Aapproach%20that%20unifies%20the%20masked%20video%20modeling%2C%20crossmodal%20contrastive%0Alearning%2C%20and%20next%20token%20prediction%2C%20scaling%20up%20the%20video%20encoder%20size%20to%206B%0Aparameters.%20At%20the%20data%20level%2C%20we%20prioritize%20spatiotemporal%20consistency%20by%0Asemantically%20segmenting%20videos%20and%20generating%20video-audio-speech%20captions.%20This%0Aimproves%20the%20alignment%20between%20video%20and%20text.%20Through%20extensive%20experiments%2C%0Awe%20validate%20our%20designs%20and%20demonstrate%20superior%20performance%20on%20over%2060%20video%0Aand%20audio%20tasks.%20Notably%2C%20our%20model%20outperforms%20others%20on%20various%20video-related%0Adialogue%20and%20long%20video%20understanding%20benchmarks%2C%20highlighting%20its%20ability%20to%0Areason%20and%20comprehend%20longer%20contexts.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/OpenGVLab/InternVideo/tree/main/InternVideo2/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15377v2&entry.124074799=Read"},
{"title": "HAIFIT: Human-Centered AI for Fashion Image Translation", "author": "Jianan Jiang and Xinglin Li and Weiren Yu and Di Wu", "abstract": "  In the realm of fashion design, sketches serve as the canvas for expressing\nan artist's distinctive drawing style and creative vision, capturing intricate\ndetails like stroke variations and texture nuances. The advent of\nsketch-to-image cross-modal translation technology has notably aided designers.\nHowever, existing methods often compromise these sketch details during image\ngeneration, resulting in images that deviate from the designer's intended\nconcept. This limitation hampers the ability to offer designers a precise\npreview of the final output. To overcome this challenge, we introduce HAIFIT, a\nnovel approach that transforms sketches into high-fidelity, lifelike clothing\nimages by integrating multi-scale features and capturing extensive feature map\ndependencies from diverse perspectives. Through extensive qualitative and\nquantitative evaluations conducted on our self-collected dataset, our method\ndemonstrates superior performance compared to existing methods in generating\nphotorealistic clothing images. Our method excels in preserving the distinctive\nstyle and intricate details essential for fashion design applications. In\naddition, our method also has obvious advantages in model training and\ninference speed, contributing to reducing designers' time costs and improving\ndesign efficiency.\n", "link": "http://arxiv.org/abs/2403.08651v3", "date": "2024-07-25", "relevancy": 2.3332, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6319}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5909}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAIFIT%3A%20Human-Centered%20AI%20for%20Fashion%20Image%20Translation&body=Title%3A%20HAIFIT%3A%20Human-Centered%20AI%20for%20Fashion%20Image%20Translation%0AAuthor%3A%20Jianan%20Jiang%20and%20Xinglin%20Li%20and%20Weiren%20Yu%20and%20Di%20Wu%0AAbstract%3A%20%20%20In%20the%20realm%20of%20fashion%20design%2C%20sketches%20serve%20as%20the%20canvas%20for%20expressing%0Aan%20artist%27s%20distinctive%20drawing%20style%20and%20creative%20vision%2C%20capturing%20intricate%0Adetails%20like%20stroke%20variations%20and%20texture%20nuances.%20The%20advent%20of%0Asketch-to-image%20cross-modal%20translation%20technology%20has%20notably%20aided%20designers.%0AHowever%2C%20existing%20methods%20often%20compromise%20these%20sketch%20details%20during%20image%0Ageneration%2C%20resulting%20in%20images%20that%20deviate%20from%20the%20designer%27s%20intended%0Aconcept.%20This%20limitation%20hampers%20the%20ability%20to%20offer%20designers%20a%20precise%0Apreview%20of%20the%20final%20output.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20HAIFIT%2C%20a%0Anovel%20approach%20that%20transforms%20sketches%20into%20high-fidelity%2C%20lifelike%20clothing%0Aimages%20by%20integrating%20multi-scale%20features%20and%20capturing%20extensive%20feature%20map%0Adependencies%20from%20diverse%20perspectives.%20Through%20extensive%20qualitative%20and%0Aquantitative%20evaluations%20conducted%20on%20our%20self-collected%20dataset%2C%20our%20method%0Ademonstrates%20superior%20performance%20compared%20to%20existing%20methods%20in%20generating%0Aphotorealistic%20clothing%20images.%20Our%20method%20excels%20in%20preserving%20the%20distinctive%0Astyle%20and%20intricate%20details%20essential%20for%20fashion%20design%20applications.%20In%0Aaddition%2C%20our%20method%20also%20has%20obvious%20advantages%20in%20model%20training%20and%0Ainference%20speed%2C%20contributing%20to%20reducing%20designers%27%20time%20costs%20and%20improving%0Adesign%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08651v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAIFIT%253A%2520Human-Centered%2520AI%2520for%2520Fashion%2520Image%2520Translation%26entry.906535625%3DJianan%2520Jiang%2520and%2520Xinglin%2520Li%2520and%2520Weiren%2520Yu%2520and%2520Di%2520Wu%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520fashion%2520design%252C%2520sketches%2520serve%2520as%2520the%2520canvas%2520for%2520expressing%250Aan%2520artist%2527s%2520distinctive%2520drawing%2520style%2520and%2520creative%2520vision%252C%2520capturing%2520intricate%250Adetails%2520like%2520stroke%2520variations%2520and%2520texture%2520nuances.%2520The%2520advent%2520of%250Asketch-to-image%2520cross-modal%2520translation%2520technology%2520has%2520notably%2520aided%2520designers.%250AHowever%252C%2520existing%2520methods%2520often%2520compromise%2520these%2520sketch%2520details%2520during%2520image%250Ageneration%252C%2520resulting%2520in%2520images%2520that%2520deviate%2520from%2520the%2520designer%2527s%2520intended%250Aconcept.%2520This%2520limitation%2520hampers%2520the%2520ability%2520to%2520offer%2520designers%2520a%2520precise%250Apreview%2520of%2520the%2520final%2520output.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520introduce%2520HAIFIT%252C%2520a%250Anovel%2520approach%2520that%2520transforms%2520sketches%2520into%2520high-fidelity%252C%2520lifelike%2520clothing%250Aimages%2520by%2520integrating%2520multi-scale%2520features%2520and%2520capturing%2520extensive%2520feature%2520map%250Adependencies%2520from%2520diverse%2520perspectives.%2520Through%2520extensive%2520qualitative%2520and%250Aquantitative%2520evaluations%2520conducted%2520on%2520our%2520self-collected%2520dataset%252C%2520our%2520method%250Ademonstrates%2520superior%2520performance%2520compared%2520to%2520existing%2520methods%2520in%2520generating%250Aphotorealistic%2520clothing%2520images.%2520Our%2520method%2520excels%2520in%2520preserving%2520the%2520distinctive%250Astyle%2520and%2520intricate%2520details%2520essential%2520for%2520fashion%2520design%2520applications.%2520In%250Aaddition%252C%2520our%2520method%2520also%2520has%2520obvious%2520advantages%2520in%2520model%2520training%2520and%250Ainference%2520speed%252C%2520contributing%2520to%2520reducing%2520designers%2527%2520time%2520costs%2520and%2520improving%250Adesign%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08651v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAIFIT%3A%20Human-Centered%20AI%20for%20Fashion%20Image%20Translation&entry.906535625=Jianan%20Jiang%20and%20Xinglin%20Li%20and%20Weiren%20Yu%20and%20Di%20Wu&entry.1292438233=%20%20In%20the%20realm%20of%20fashion%20design%2C%20sketches%20serve%20as%20the%20canvas%20for%20expressing%0Aan%20artist%27s%20distinctive%20drawing%20style%20and%20creative%20vision%2C%20capturing%20intricate%0Adetails%20like%20stroke%20variations%20and%20texture%20nuances.%20The%20advent%20of%0Asketch-to-image%20cross-modal%20translation%20technology%20has%20notably%20aided%20designers.%0AHowever%2C%20existing%20methods%20often%20compromise%20these%20sketch%20details%20during%20image%0Ageneration%2C%20resulting%20in%20images%20that%20deviate%20from%20the%20designer%27s%20intended%0Aconcept.%20This%20limitation%20hampers%20the%20ability%20to%20offer%20designers%20a%20precise%0Apreview%20of%20the%20final%20output.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20HAIFIT%2C%20a%0Anovel%20approach%20that%20transforms%20sketches%20into%20high-fidelity%2C%20lifelike%20clothing%0Aimages%20by%20integrating%20multi-scale%20features%20and%20capturing%20extensive%20feature%20map%0Adependencies%20from%20diverse%20perspectives.%20Through%20extensive%20qualitative%20and%0Aquantitative%20evaluations%20conducted%20on%20our%20self-collected%20dataset%2C%20our%20method%0Ademonstrates%20superior%20performance%20compared%20to%20existing%20methods%20in%20generating%0Aphotorealistic%20clothing%20images.%20Our%20method%20excels%20in%20preserving%20the%20distinctive%0Astyle%20and%20intricate%20details%20essential%20for%20fashion%20design%20applications.%20In%0Aaddition%2C%20our%20method%20also%20has%20obvious%20advantages%20in%20model%20training%20and%0Ainference%20speed%2C%20contributing%20to%20reducing%20designers%27%20time%20costs%20and%20improving%0Adesign%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08651v3&entry.124074799=Read"},
{"title": "Neural Networks for Generating Better Local Optima in Topology\n  Optimization", "author": "Leon Herrmann and Ole Sigmund and Viola Muning Li and Christian Vogl and Stefan Kollmannsberger", "abstract": "  Neural networks have recently been employed as material discretizations\nwithin adjoint optimization frameworks for inverse problems and topology\noptimization. While advantageous regularization effects and better optima have\nbeen found for some inverse problems, the benefit for topology optimization has\nbeen limited -- where the focus of investigations has been the compliance\nproblem. We demonstrate how neural network material discretizations can, under\ncertain conditions, find better local optima in more challenging optimization\nproblems, where we here specifically consider acoustic topology optimization.\nThe chances of identifying a better optimum can significantly be improved by\nrunning multiple partial optimizations with different neural network\ninitializations. Furthermore, we show that the neural network material\ndiscretization's advantage comes from the interplay with the Adam optimizer and\nemphasize its current limitations when competing with constrained and\nhigher-order optimization techniques. At the moment, this discretization has\nonly been shown to be beneficial for unconstrained first-order optimization.\n", "link": "http://arxiv.org/abs/2407.17957v1", "date": "2024-07-25", "relevancy": 2.3291, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4741}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4625}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4609}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Networks%20for%20Generating%20Better%20Local%20Optima%20in%20Topology%0A%20%20Optimization&body=Title%3A%20Neural%20Networks%20for%20Generating%20Better%20Local%20Optima%20in%20Topology%0A%20%20Optimization%0AAuthor%3A%20Leon%20Herrmann%20and%20Ole%20Sigmund%20and%20Viola%20Muning%20Li%20and%20Christian%20Vogl%20and%20Stefan%20Kollmannsberger%0AAbstract%3A%20%20%20Neural%20networks%20have%20recently%20been%20employed%20as%20material%20discretizations%0Awithin%20adjoint%20optimization%20frameworks%20for%20inverse%20problems%20and%20topology%0Aoptimization.%20While%20advantageous%20regularization%20effects%20and%20better%20optima%20have%0Abeen%20found%20for%20some%20inverse%20problems%2C%20the%20benefit%20for%20topology%20optimization%20has%0Abeen%20limited%20--%20where%20the%20focus%20of%20investigations%20has%20been%20the%20compliance%0Aproblem.%20We%20demonstrate%20how%20neural%20network%20material%20discretizations%20can%2C%20under%0Acertain%20conditions%2C%20find%20better%20local%20optima%20in%20more%20challenging%20optimization%0Aproblems%2C%20where%20we%20here%20specifically%20consider%20acoustic%20topology%20optimization.%0AThe%20chances%20of%20identifying%20a%20better%20optimum%20can%20significantly%20be%20improved%20by%0Arunning%20multiple%20partial%20optimizations%20with%20different%20neural%20network%0Ainitializations.%20Furthermore%2C%20we%20show%20that%20the%20neural%20network%20material%0Adiscretization%27s%20advantage%20comes%20from%20the%20interplay%20with%20the%20Adam%20optimizer%20and%0Aemphasize%20its%20current%20limitations%20when%20competing%20with%20constrained%20and%0Ahigher-order%20optimization%20techniques.%20At%20the%20moment%2C%20this%20discretization%20has%0Aonly%20been%20shown%20to%20be%20beneficial%20for%20unconstrained%20first-order%20optimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17957v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Networks%2520for%2520Generating%2520Better%2520Local%2520Optima%2520in%2520Topology%250A%2520%2520Optimization%26entry.906535625%3DLeon%2520Herrmann%2520and%2520Ole%2520Sigmund%2520and%2520Viola%2520Muning%2520Li%2520and%2520Christian%2520Vogl%2520and%2520Stefan%2520Kollmannsberger%26entry.1292438233%3D%2520%2520Neural%2520networks%2520have%2520recently%2520been%2520employed%2520as%2520material%2520discretizations%250Awithin%2520adjoint%2520optimization%2520frameworks%2520for%2520inverse%2520problems%2520and%2520topology%250Aoptimization.%2520While%2520advantageous%2520regularization%2520effects%2520and%2520better%2520optima%2520have%250Abeen%2520found%2520for%2520some%2520inverse%2520problems%252C%2520the%2520benefit%2520for%2520topology%2520optimization%2520has%250Abeen%2520limited%2520--%2520where%2520the%2520focus%2520of%2520investigations%2520has%2520been%2520the%2520compliance%250Aproblem.%2520We%2520demonstrate%2520how%2520neural%2520network%2520material%2520discretizations%2520can%252C%2520under%250Acertain%2520conditions%252C%2520find%2520better%2520local%2520optima%2520in%2520more%2520challenging%2520optimization%250Aproblems%252C%2520where%2520we%2520here%2520specifically%2520consider%2520acoustic%2520topology%2520optimization.%250AThe%2520chances%2520of%2520identifying%2520a%2520better%2520optimum%2520can%2520significantly%2520be%2520improved%2520by%250Arunning%2520multiple%2520partial%2520optimizations%2520with%2520different%2520neural%2520network%250Ainitializations.%2520Furthermore%252C%2520we%2520show%2520that%2520the%2520neural%2520network%2520material%250Adiscretization%2527s%2520advantage%2520comes%2520from%2520the%2520interplay%2520with%2520the%2520Adam%2520optimizer%2520and%250Aemphasize%2520its%2520current%2520limitations%2520when%2520competing%2520with%2520constrained%2520and%250Ahigher-order%2520optimization%2520techniques.%2520At%2520the%2520moment%252C%2520this%2520discretization%2520has%250Aonly%2520been%2520shown%2520to%2520be%2520beneficial%2520for%2520unconstrained%2520first-order%2520optimization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17957v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Networks%20for%20Generating%20Better%20Local%20Optima%20in%20Topology%0A%20%20Optimization&entry.906535625=Leon%20Herrmann%20and%20Ole%20Sigmund%20and%20Viola%20Muning%20Li%20and%20Christian%20Vogl%20and%20Stefan%20Kollmannsberger&entry.1292438233=%20%20Neural%20networks%20have%20recently%20been%20employed%20as%20material%20discretizations%0Awithin%20adjoint%20optimization%20frameworks%20for%20inverse%20problems%20and%20topology%0Aoptimization.%20While%20advantageous%20regularization%20effects%20and%20better%20optima%20have%0Abeen%20found%20for%20some%20inverse%20problems%2C%20the%20benefit%20for%20topology%20optimization%20has%0Abeen%20limited%20--%20where%20the%20focus%20of%20investigations%20has%20been%20the%20compliance%0Aproblem.%20We%20demonstrate%20how%20neural%20network%20material%20discretizations%20can%2C%20under%0Acertain%20conditions%2C%20find%20better%20local%20optima%20in%20more%20challenging%20optimization%0Aproblems%2C%20where%20we%20here%20specifically%20consider%20acoustic%20topology%20optimization.%0AThe%20chances%20of%20identifying%20a%20better%20optimum%20can%20significantly%20be%20improved%20by%0Arunning%20multiple%20partial%20optimizations%20with%20different%20neural%20network%0Ainitializations.%20Furthermore%2C%20we%20show%20that%20the%20neural%20network%20material%0Adiscretization%27s%20advantage%20comes%20from%20the%20interplay%20with%20the%20Adam%20optimizer%20and%0Aemphasize%20its%20current%20limitations%20when%20competing%20with%20constrained%20and%0Ahigher-order%20optimization%20techniques.%20At%20the%20moment%2C%20this%20discretization%20has%0Aonly%20been%20shown%20to%20be%20beneficial%20for%20unconstrained%20first-order%20optimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17957v1&entry.124074799=Read"},
{"title": "TiCoSS: Tightening the Coupling between Semantic Segmentation and Stereo\n  Matching within A Joint Learning Framework", "author": "Guanfeng Tang and Zhiyuan Wu and Rui Fan", "abstract": "  Semantic segmentation and stereo matching, respectively analogous to the\nventral and dorsal streams in our human brain, are two key components of\nautonomous driving perception systems. Addressing these two tasks with separate\nnetworks is no longer the mainstream direction in developing computer vision\nalgorithms, particularly with the recent advances in large vision models and\nembodied artificial intelligence. The trend is shifting towards combining them\nwithin a joint learning framework, especially emphasizing feature sharing\nbetween the two tasks. The major contributions of this study lie in\ncomprehensively tightening the coupling between semantic segmentation and\nstereo matching. Specifically, this study introduces three novelties: (1) a\ntightly coupled, gated feature fusion strategy, (2) a hierarchical deep\nsupervision strategy, and (3) a coupling tightening loss function. The combined\nuse of these technical contributions results in TiCoSS, a state-of-the-art\njoint learning framework that simultaneously tackles semantic segmentation and\nstereo matching. Through extensive experiments on the KITTI and vKITTI2\ndatasets, along with qualitative and quantitative analyses, we validate the\neffectiveness of our developed strategies and loss function, and demonstrate\nits superior performance compared to prior arts, with a notable increase in\nmIoU by over 9%. Our source code will be publicly available at\nmias.group/TiCoSS upon publication.\n", "link": "http://arxiv.org/abs/2407.18038v1", "date": "2024-07-25", "relevancy": 2.3132, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5817}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5778}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TiCoSS%3A%20Tightening%20the%20Coupling%20between%20Semantic%20Segmentation%20and%20Stereo%0A%20%20Matching%20within%20A%20Joint%20Learning%20Framework&body=Title%3A%20TiCoSS%3A%20Tightening%20the%20Coupling%20between%20Semantic%20Segmentation%20and%20Stereo%0A%20%20Matching%20within%20A%20Joint%20Learning%20Framework%0AAuthor%3A%20Guanfeng%20Tang%20and%20Zhiyuan%20Wu%20and%20Rui%20Fan%0AAbstract%3A%20%20%20Semantic%20segmentation%20and%20stereo%20matching%2C%20respectively%20analogous%20to%20the%0Aventral%20and%20dorsal%20streams%20in%20our%20human%20brain%2C%20are%20two%20key%20components%20of%0Aautonomous%20driving%20perception%20systems.%20Addressing%20these%20two%20tasks%20with%20separate%0Anetworks%20is%20no%20longer%20the%20mainstream%20direction%20in%20developing%20computer%20vision%0Aalgorithms%2C%20particularly%20with%20the%20recent%20advances%20in%20large%20vision%20models%20and%0Aembodied%20artificial%20intelligence.%20The%20trend%20is%20shifting%20towards%20combining%20them%0Awithin%20a%20joint%20learning%20framework%2C%20especially%20emphasizing%20feature%20sharing%0Abetween%20the%20two%20tasks.%20The%20major%20contributions%20of%20this%20study%20lie%20in%0Acomprehensively%20tightening%20the%20coupling%20between%20semantic%20segmentation%20and%0Astereo%20matching.%20Specifically%2C%20this%20study%20introduces%20three%20novelties%3A%20%281%29%20a%0Atightly%20coupled%2C%20gated%20feature%20fusion%20strategy%2C%20%282%29%20a%20hierarchical%20deep%0Asupervision%20strategy%2C%20and%20%283%29%20a%20coupling%20tightening%20loss%20function.%20The%20combined%0Ause%20of%20these%20technical%20contributions%20results%20in%20TiCoSS%2C%20a%20state-of-the-art%0Ajoint%20learning%20framework%20that%20simultaneously%20tackles%20semantic%20segmentation%20and%0Astereo%20matching.%20Through%20extensive%20experiments%20on%20the%20KITTI%20and%20vKITTI2%0Adatasets%2C%20along%20with%20qualitative%20and%20quantitative%20analyses%2C%20we%20validate%20the%0Aeffectiveness%20of%20our%20developed%20strategies%20and%20loss%20function%2C%20and%20demonstrate%0Aits%20superior%20performance%20compared%20to%20prior%20arts%2C%20with%20a%20notable%20increase%20in%0AmIoU%20by%20over%209%25.%20Our%20source%20code%20will%20be%20publicly%20available%20at%0Amias.group/TiCoSS%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiCoSS%253A%2520Tightening%2520the%2520Coupling%2520between%2520Semantic%2520Segmentation%2520and%2520Stereo%250A%2520%2520Matching%2520within%2520A%2520Joint%2520Learning%2520Framework%26entry.906535625%3DGuanfeng%2520Tang%2520and%2520Zhiyuan%2520Wu%2520and%2520Rui%2520Fan%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520and%2520stereo%2520matching%252C%2520respectively%2520analogous%2520to%2520the%250Aventral%2520and%2520dorsal%2520streams%2520in%2520our%2520human%2520brain%252C%2520are%2520two%2520key%2520components%2520of%250Aautonomous%2520driving%2520perception%2520systems.%2520Addressing%2520these%2520two%2520tasks%2520with%2520separate%250Anetworks%2520is%2520no%2520longer%2520the%2520mainstream%2520direction%2520in%2520developing%2520computer%2520vision%250Aalgorithms%252C%2520particularly%2520with%2520the%2520recent%2520advances%2520in%2520large%2520vision%2520models%2520and%250Aembodied%2520artificial%2520intelligence.%2520The%2520trend%2520is%2520shifting%2520towards%2520combining%2520them%250Awithin%2520a%2520joint%2520learning%2520framework%252C%2520especially%2520emphasizing%2520feature%2520sharing%250Abetween%2520the%2520two%2520tasks.%2520The%2520major%2520contributions%2520of%2520this%2520study%2520lie%2520in%250Acomprehensively%2520tightening%2520the%2520coupling%2520between%2520semantic%2520segmentation%2520and%250Astereo%2520matching.%2520Specifically%252C%2520this%2520study%2520introduces%2520three%2520novelties%253A%2520%25281%2529%2520a%250Atightly%2520coupled%252C%2520gated%2520feature%2520fusion%2520strategy%252C%2520%25282%2529%2520a%2520hierarchical%2520deep%250Asupervision%2520strategy%252C%2520and%2520%25283%2529%2520a%2520coupling%2520tightening%2520loss%2520function.%2520The%2520combined%250Ause%2520of%2520these%2520technical%2520contributions%2520results%2520in%2520TiCoSS%252C%2520a%2520state-of-the-art%250Ajoint%2520learning%2520framework%2520that%2520simultaneously%2520tackles%2520semantic%2520segmentation%2520and%250Astereo%2520matching.%2520Through%2520extensive%2520experiments%2520on%2520the%2520KITTI%2520and%2520vKITTI2%250Adatasets%252C%2520along%2520with%2520qualitative%2520and%2520quantitative%2520analyses%252C%2520we%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520developed%2520strategies%2520and%2520loss%2520function%252C%2520and%2520demonstrate%250Aits%2520superior%2520performance%2520compared%2520to%2520prior%2520arts%252C%2520with%2520a%2520notable%2520increase%2520in%250AmIoU%2520by%2520over%25209%2525.%2520Our%2520source%2520code%2520will%2520be%2520publicly%2520available%2520at%250Amias.group/TiCoSS%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TiCoSS%3A%20Tightening%20the%20Coupling%20between%20Semantic%20Segmentation%20and%20Stereo%0A%20%20Matching%20within%20A%20Joint%20Learning%20Framework&entry.906535625=Guanfeng%20Tang%20and%20Zhiyuan%20Wu%20and%20Rui%20Fan&entry.1292438233=%20%20Semantic%20segmentation%20and%20stereo%20matching%2C%20respectively%20analogous%20to%20the%0Aventral%20and%20dorsal%20streams%20in%20our%20human%20brain%2C%20are%20two%20key%20components%20of%0Aautonomous%20driving%20perception%20systems.%20Addressing%20these%20two%20tasks%20with%20separate%0Anetworks%20is%20no%20longer%20the%20mainstream%20direction%20in%20developing%20computer%20vision%0Aalgorithms%2C%20particularly%20with%20the%20recent%20advances%20in%20large%20vision%20models%20and%0Aembodied%20artificial%20intelligence.%20The%20trend%20is%20shifting%20towards%20combining%20them%0Awithin%20a%20joint%20learning%20framework%2C%20especially%20emphasizing%20feature%20sharing%0Abetween%20the%20two%20tasks.%20The%20major%20contributions%20of%20this%20study%20lie%20in%0Acomprehensively%20tightening%20the%20coupling%20between%20semantic%20segmentation%20and%0Astereo%20matching.%20Specifically%2C%20this%20study%20introduces%20three%20novelties%3A%20%281%29%20a%0Atightly%20coupled%2C%20gated%20feature%20fusion%20strategy%2C%20%282%29%20a%20hierarchical%20deep%0Asupervision%20strategy%2C%20and%20%283%29%20a%20coupling%20tightening%20loss%20function.%20The%20combined%0Ause%20of%20these%20technical%20contributions%20results%20in%20TiCoSS%2C%20a%20state-of-the-art%0Ajoint%20learning%20framework%20that%20simultaneously%20tackles%20semantic%20segmentation%20and%0Astereo%20matching.%20Through%20extensive%20experiments%20on%20the%20KITTI%20and%20vKITTI2%0Adatasets%2C%20along%20with%20qualitative%20and%20quantitative%20analyses%2C%20we%20validate%20the%0Aeffectiveness%20of%20our%20developed%20strategies%20and%20loss%20function%2C%20and%20demonstrate%0Aits%20superior%20performance%20compared%20to%20prior%20arts%2C%20with%20a%20notable%20increase%20in%0AmIoU%20by%20over%209%25.%20Our%20source%20code%20will%20be%20publicly%20available%20at%0Amias.group/TiCoSS%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18038v1&entry.124074799=Read"},
{"title": "CodedVO: Coded Visual Odometry", "author": "Sachin Shah and Naitri Rajyaguru and Chahat Deep Singh and Christopher Metzler and Yiannis Aloimonos", "abstract": "  Autonomous robots often rely on monocular cameras for odometry estimation and\nnavigation. However, the scale ambiguity problem presents a critical barrier to\neffective monocular visual odometry. In this paper, we present CodedVO, a novel\nmonocular visual odometry method that overcomes the scale ambiguity problem by\nemploying custom optics to physically encode metric depth information into\nimagery. By incorporating this information into our odometry pipeline, we\nachieve state-of-the-art performance in monocular visual odometry with a known\nscale. We evaluate our method in diverse indoor environments and demonstrate\nits robustness and adaptability. We achieve a 0.08m average trajectory error in\nodometry evaluation on the ICL-NUIM indoor odometry dataset.\n", "link": "http://arxiv.org/abs/2407.18240v1", "date": "2024-07-25", "relevancy": 2.2916, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5878}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5874}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CodedVO%3A%20Coded%20Visual%20Odometry&body=Title%3A%20CodedVO%3A%20Coded%20Visual%20Odometry%0AAuthor%3A%20Sachin%20Shah%20and%20Naitri%20Rajyaguru%20and%20Chahat%20Deep%20Singh%20and%20Christopher%20Metzler%20and%20Yiannis%20Aloimonos%0AAbstract%3A%20%20%20Autonomous%20robots%20often%20rely%20on%20monocular%20cameras%20for%20odometry%20estimation%20and%0Anavigation.%20However%2C%20the%20scale%20ambiguity%20problem%20presents%20a%20critical%20barrier%20to%0Aeffective%20monocular%20visual%20odometry.%20In%20this%20paper%2C%20we%20present%20CodedVO%2C%20a%20novel%0Amonocular%20visual%20odometry%20method%20that%20overcomes%20the%20scale%20ambiguity%20problem%20by%0Aemploying%20custom%20optics%20to%20physically%20encode%20metric%20depth%20information%20into%0Aimagery.%20By%20incorporating%20this%20information%20into%20our%20odometry%20pipeline%2C%20we%0Aachieve%20state-of-the-art%20performance%20in%20monocular%20visual%20odometry%20with%20a%20known%0Ascale.%20We%20evaluate%20our%20method%20in%20diverse%20indoor%20environments%20and%20demonstrate%0Aits%20robustness%20and%20adaptability.%20We%20achieve%20a%200.08m%20average%20trajectory%20error%20in%0Aodometry%20evaluation%20on%20the%20ICL-NUIM%20indoor%20odometry%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCodedVO%253A%2520Coded%2520Visual%2520Odometry%26entry.906535625%3DSachin%2520Shah%2520and%2520Naitri%2520Rajyaguru%2520and%2520Chahat%2520Deep%2520Singh%2520and%2520Christopher%2520Metzler%2520and%2520Yiannis%2520Aloimonos%26entry.1292438233%3D%2520%2520Autonomous%2520robots%2520often%2520rely%2520on%2520monocular%2520cameras%2520for%2520odometry%2520estimation%2520and%250Anavigation.%2520However%252C%2520the%2520scale%2520ambiguity%2520problem%2520presents%2520a%2520critical%2520barrier%2520to%250Aeffective%2520monocular%2520visual%2520odometry.%2520In%2520this%2520paper%252C%2520we%2520present%2520CodedVO%252C%2520a%2520novel%250Amonocular%2520visual%2520odometry%2520method%2520that%2520overcomes%2520the%2520scale%2520ambiguity%2520problem%2520by%250Aemploying%2520custom%2520optics%2520to%2520physically%2520encode%2520metric%2520depth%2520information%2520into%250Aimagery.%2520By%2520incorporating%2520this%2520information%2520into%2520our%2520odometry%2520pipeline%252C%2520we%250Aachieve%2520state-of-the-art%2520performance%2520in%2520monocular%2520visual%2520odometry%2520with%2520a%2520known%250Ascale.%2520We%2520evaluate%2520our%2520method%2520in%2520diverse%2520indoor%2520environments%2520and%2520demonstrate%250Aits%2520robustness%2520and%2520adaptability.%2520We%2520achieve%2520a%25200.08m%2520average%2520trajectory%2520error%2520in%250Aodometry%2520evaluation%2520on%2520the%2520ICL-NUIM%2520indoor%2520odometry%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CodedVO%3A%20Coded%20Visual%20Odometry&entry.906535625=Sachin%20Shah%20and%20Naitri%20Rajyaguru%20and%20Chahat%20Deep%20Singh%20and%20Christopher%20Metzler%20and%20Yiannis%20Aloimonos&entry.1292438233=%20%20Autonomous%20robots%20often%20rely%20on%20monocular%20cameras%20for%20odometry%20estimation%20and%0Anavigation.%20However%2C%20the%20scale%20ambiguity%20problem%20presents%20a%20critical%20barrier%20to%0Aeffective%20monocular%20visual%20odometry.%20In%20this%20paper%2C%20we%20present%20CodedVO%2C%20a%20novel%0Amonocular%20visual%20odometry%20method%20that%20overcomes%20the%20scale%20ambiguity%20problem%20by%0Aemploying%20custom%20optics%20to%20physically%20encode%20metric%20depth%20information%20into%0Aimagery.%20By%20incorporating%20this%20information%20into%20our%20odometry%20pipeline%2C%20we%0Aachieve%20state-of-the-art%20performance%20in%20monocular%20visual%20odometry%20with%20a%20known%0Ascale.%20We%20evaluate%20our%20method%20in%20diverse%20indoor%20environments%20and%20demonstrate%0Aits%20robustness%20and%20adaptability.%20We%20achieve%20a%200.08m%20average%20trajectory%20error%20in%0Aodometry%20evaluation%20on%20the%20ICL-NUIM%20indoor%20odometry%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18240v1&entry.124074799=Read"},
{"title": "3D Diffuser Actor: Policy Diffusion with 3D Scene Representations", "author": "Tsung-Wei Ke and Nikolaos Gkanatsios and Katerina Fragkiadaki", "abstract": "  Diffusion policies are conditional diffusion models that learn robot action\ndistributions conditioned on the robot and environment state. They have\nrecently shown to outperform both deterministic and alternative action\ndistribution learning formulations. 3D robot policies use 3D scene feature\nrepresentations aggregated from a single or multiple camera views using sensed\ndepth. They have shown to generalize better than their 2D counterparts across\ncamera viewpoints. We unify these two lines of work and present 3D Diffuser\nActor, a neural policy equipped with a novel 3D denoising transformer that\nfuses information from the 3D visual scene, a language instruction and\nproprioception to predict the noise in noised 3D robot pose trajectories. 3D\nDiffuser Actor sets a new state-of-the-art on RLBench with an absolute\nperformance gain of 18.1% over the current SOTA on a multi-view setup and an\nabsolute gain of 13.1% on a single-view setup. On the CALVIN benchmark, it\nimproves over the current SOTA by a 9% relative increase. It also learns to\ncontrol a robot manipulator in the real world from a handful of demonstrations.\nThrough thorough comparisons with the current SOTA policies and ablations of\nour model, we show 3D Diffuser Actor's design choices dramatically outperform\n2D representations, regression and classification objectives, absolute\nattentions, and holistic non-tokenized 3D scene embeddings.\n", "link": "http://arxiv.org/abs/2402.10885v3", "date": "2024-07-25", "relevancy": 2.2759, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5698}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5688}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Diffuser%20Actor%3A%20Policy%20Diffusion%20with%203D%20Scene%20Representations&body=Title%3A%203D%20Diffuser%20Actor%3A%20Policy%20Diffusion%20with%203D%20Scene%20Representations%0AAuthor%3A%20Tsung-Wei%20Ke%20and%20Nikolaos%20Gkanatsios%20and%20Katerina%20Fragkiadaki%0AAbstract%3A%20%20%20Diffusion%20policies%20are%20conditional%20diffusion%20models%20that%20learn%20robot%20action%0Adistributions%20conditioned%20on%20the%20robot%20and%20environment%20state.%20They%20have%0Arecently%20shown%20to%20outperform%20both%20deterministic%20and%20alternative%20action%0Adistribution%20learning%20formulations.%203D%20robot%20policies%20use%203D%20scene%20feature%0Arepresentations%20aggregated%20from%20a%20single%20or%20multiple%20camera%20views%20using%20sensed%0Adepth.%20They%20have%20shown%20to%20generalize%20better%20than%20their%202D%20counterparts%20across%0Acamera%20viewpoints.%20We%20unify%20these%20two%20lines%20of%20work%20and%20present%203D%20Diffuser%0AActor%2C%20a%20neural%20policy%20equipped%20with%20a%20novel%203D%20denoising%20transformer%20that%0Afuses%20information%20from%20the%203D%20visual%20scene%2C%20a%20language%20instruction%20and%0Aproprioception%20to%20predict%20the%20noise%20in%20noised%203D%20robot%20pose%20trajectories.%203D%0ADiffuser%20Actor%20sets%20a%20new%20state-of-the-art%20on%20RLBench%20with%20an%20absolute%0Aperformance%20gain%20of%2018.1%25%20over%20the%20current%20SOTA%20on%20a%20multi-view%20setup%20and%20an%0Aabsolute%20gain%20of%2013.1%25%20on%20a%20single-view%20setup.%20On%20the%20CALVIN%20benchmark%2C%20it%0Aimproves%20over%20the%20current%20SOTA%20by%20a%209%25%20relative%20increase.%20It%20also%20learns%20to%0Acontrol%20a%20robot%20manipulator%20in%20the%20real%20world%20from%20a%20handful%20of%20demonstrations.%0AThrough%20thorough%20comparisons%20with%20the%20current%20SOTA%20policies%20and%20ablations%20of%0Aour%20model%2C%20we%20show%203D%20Diffuser%20Actor%27s%20design%20choices%20dramatically%20outperform%0A2D%20representations%2C%20regression%20and%20classification%20objectives%2C%20absolute%0Aattentions%2C%20and%20holistic%20non-tokenized%203D%20scene%20embeddings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10885v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Diffuser%2520Actor%253A%2520Policy%2520Diffusion%2520with%25203D%2520Scene%2520Representations%26entry.906535625%3DTsung-Wei%2520Ke%2520and%2520Nikolaos%2520Gkanatsios%2520and%2520Katerina%2520Fragkiadaki%26entry.1292438233%3D%2520%2520Diffusion%2520policies%2520are%2520conditional%2520diffusion%2520models%2520that%2520learn%2520robot%2520action%250Adistributions%2520conditioned%2520on%2520the%2520robot%2520and%2520environment%2520state.%2520They%2520have%250Arecently%2520shown%2520to%2520outperform%2520both%2520deterministic%2520and%2520alternative%2520action%250Adistribution%2520learning%2520formulations.%25203D%2520robot%2520policies%2520use%25203D%2520scene%2520feature%250Arepresentations%2520aggregated%2520from%2520a%2520single%2520or%2520multiple%2520camera%2520views%2520using%2520sensed%250Adepth.%2520They%2520have%2520shown%2520to%2520generalize%2520better%2520than%2520their%25202D%2520counterparts%2520across%250Acamera%2520viewpoints.%2520We%2520unify%2520these%2520two%2520lines%2520of%2520work%2520and%2520present%25203D%2520Diffuser%250AActor%252C%2520a%2520neural%2520policy%2520equipped%2520with%2520a%2520novel%25203D%2520denoising%2520transformer%2520that%250Afuses%2520information%2520from%2520the%25203D%2520visual%2520scene%252C%2520a%2520language%2520instruction%2520and%250Aproprioception%2520to%2520predict%2520the%2520noise%2520in%2520noised%25203D%2520robot%2520pose%2520trajectories.%25203D%250ADiffuser%2520Actor%2520sets%2520a%2520new%2520state-of-the-art%2520on%2520RLBench%2520with%2520an%2520absolute%250Aperformance%2520gain%2520of%252018.1%2525%2520over%2520the%2520current%2520SOTA%2520on%2520a%2520multi-view%2520setup%2520and%2520an%250Aabsolute%2520gain%2520of%252013.1%2525%2520on%2520a%2520single-view%2520setup.%2520On%2520the%2520CALVIN%2520benchmark%252C%2520it%250Aimproves%2520over%2520the%2520current%2520SOTA%2520by%2520a%25209%2525%2520relative%2520increase.%2520It%2520also%2520learns%2520to%250Acontrol%2520a%2520robot%2520manipulator%2520in%2520the%2520real%2520world%2520from%2520a%2520handful%2520of%2520demonstrations.%250AThrough%2520thorough%2520comparisons%2520with%2520the%2520current%2520SOTA%2520policies%2520and%2520ablations%2520of%250Aour%2520model%252C%2520we%2520show%25203D%2520Diffuser%2520Actor%2527s%2520design%2520choices%2520dramatically%2520outperform%250A2D%2520representations%252C%2520regression%2520and%2520classification%2520objectives%252C%2520absolute%250Aattentions%252C%2520and%2520holistic%2520non-tokenized%25203D%2520scene%2520embeddings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10885v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Diffuser%20Actor%3A%20Policy%20Diffusion%20with%203D%20Scene%20Representations&entry.906535625=Tsung-Wei%20Ke%20and%20Nikolaos%20Gkanatsios%20and%20Katerina%20Fragkiadaki&entry.1292438233=%20%20Diffusion%20policies%20are%20conditional%20diffusion%20models%20that%20learn%20robot%20action%0Adistributions%20conditioned%20on%20the%20robot%20and%20environment%20state.%20They%20have%0Arecently%20shown%20to%20outperform%20both%20deterministic%20and%20alternative%20action%0Adistribution%20learning%20formulations.%203D%20robot%20policies%20use%203D%20scene%20feature%0Arepresentations%20aggregated%20from%20a%20single%20or%20multiple%20camera%20views%20using%20sensed%0Adepth.%20They%20have%20shown%20to%20generalize%20better%20than%20their%202D%20counterparts%20across%0Acamera%20viewpoints.%20We%20unify%20these%20two%20lines%20of%20work%20and%20present%203D%20Diffuser%0AActor%2C%20a%20neural%20policy%20equipped%20with%20a%20novel%203D%20denoising%20transformer%20that%0Afuses%20information%20from%20the%203D%20visual%20scene%2C%20a%20language%20instruction%20and%0Aproprioception%20to%20predict%20the%20noise%20in%20noised%203D%20robot%20pose%20trajectories.%203D%0ADiffuser%20Actor%20sets%20a%20new%20state-of-the-art%20on%20RLBench%20with%20an%20absolute%0Aperformance%20gain%20of%2018.1%25%20over%20the%20current%20SOTA%20on%20a%20multi-view%20setup%20and%20an%0Aabsolute%20gain%20of%2013.1%25%20on%20a%20single-view%20setup.%20On%20the%20CALVIN%20benchmark%2C%20it%0Aimproves%20over%20the%20current%20SOTA%20by%20a%209%25%20relative%20increase.%20It%20also%20learns%20to%0Acontrol%20a%20robot%20manipulator%20in%20the%20real%20world%20from%20a%20handful%20of%20demonstrations.%0AThrough%20thorough%20comparisons%20with%20the%20current%20SOTA%20policies%20and%20ablations%20of%0Aour%20model%2C%20we%20show%203D%20Diffuser%20Actor%27s%20design%20choices%20dramatically%20outperform%0A2D%20representations%2C%20regression%20and%20classification%20objectives%2C%20absolute%0Aattentions%2C%20and%20holistic%20non-tokenized%203D%20scene%20embeddings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10885v3&entry.124074799=Read"},
{"title": "StreamMOS: Streaming Moving Object Segmentation with Multi-View\n  Perception and Dual-Span Memory", "author": "Zhiheng Li and Yubo Cui and Jiexi Zhong and Zheng Fang", "abstract": "  Moving object segmentation based on LiDAR is a crucial and challenging task\nfor autonomous driving and mobile robotics. Most approaches explore\nspatio-temporal information from LiDAR sequences to predict moving objects in\nthe current frame. However, they often focus on transferring temporal cues in a\nsingle inference and regard every prediction as independent of others. This may\ncause inconsistent segmentation results for the same object in different\nframes. To overcome this issue, we propose a streaming network with a memory\nmechanism, called StreamMOS, to build the association of features and\npredictions among multiple inferences. Specifically, we utilize a short-term\nmemory to convey historical features, which can be regarded as spatial prior of\nmoving objects and adopted to enhance current inference by temporal fusion.\nMeanwhile, we build a long-term memory to store previous predictions and\nexploit them to refine the present forecast at voxel and instance levels\nthrough voting. Besides, we present multi-view encoder with cascade projection\nand asymmetric convolution to extract motion feature of objects in different\nrepresentations. Extensive experiments validate that our algorithm gets\ncompetitive performance on SemanticKITTI and Sipailou Campus datasets. Code\nwill be released at https://github.com/NEU-REAL/StreamMOS.git.\n", "link": "http://arxiv.org/abs/2407.17905v1", "date": "2024-07-25", "relevancy": 2.2756, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5995}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5753}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StreamMOS%3A%20Streaming%20Moving%20Object%20Segmentation%20with%20Multi-View%0A%20%20Perception%20and%20Dual-Span%20Memory&body=Title%3A%20StreamMOS%3A%20Streaming%20Moving%20Object%20Segmentation%20with%20Multi-View%0A%20%20Perception%20and%20Dual-Span%20Memory%0AAuthor%3A%20Zhiheng%20Li%20and%20Yubo%20Cui%20and%20Jiexi%20Zhong%20and%20Zheng%20Fang%0AAbstract%3A%20%20%20Moving%20object%20segmentation%20based%20on%20LiDAR%20is%20a%20crucial%20and%20challenging%20task%0Afor%20autonomous%20driving%20and%20mobile%20robotics.%20Most%20approaches%20explore%0Aspatio-temporal%20information%20from%20LiDAR%20sequences%20to%20predict%20moving%20objects%20in%0Athe%20current%20frame.%20However%2C%20they%20often%20focus%20on%20transferring%20temporal%20cues%20in%20a%0Asingle%20inference%20and%20regard%20every%20prediction%20as%20independent%20of%20others.%20This%20may%0Acause%20inconsistent%20segmentation%20results%20for%20the%20same%20object%20in%20different%0Aframes.%20To%20overcome%20this%20issue%2C%20we%20propose%20a%20streaming%20network%20with%20a%20memory%0Amechanism%2C%20called%20StreamMOS%2C%20to%20build%20the%20association%20of%20features%20and%0Apredictions%20among%20multiple%20inferences.%20Specifically%2C%20we%20utilize%20a%20short-term%0Amemory%20to%20convey%20historical%20features%2C%20which%20can%20be%20regarded%20as%20spatial%20prior%20of%0Amoving%20objects%20and%20adopted%20to%20enhance%20current%20inference%20by%20temporal%20fusion.%0AMeanwhile%2C%20we%20build%20a%20long-term%20memory%20to%20store%20previous%20predictions%20and%0Aexploit%20them%20to%20refine%20the%20present%20forecast%20at%20voxel%20and%20instance%20levels%0Athrough%20voting.%20Besides%2C%20we%20present%20multi-view%20encoder%20with%20cascade%20projection%0Aand%20asymmetric%20convolution%20to%20extract%20motion%20feature%20of%20objects%20in%20different%0Arepresentations.%20Extensive%20experiments%20validate%20that%20our%20algorithm%20gets%0Acompetitive%20performance%20on%20SemanticKITTI%20and%20Sipailou%20Campus%20datasets.%20Code%0Awill%20be%20released%20at%20https%3A//github.com/NEU-REAL/StreamMOS.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreamMOS%253A%2520Streaming%2520Moving%2520Object%2520Segmentation%2520with%2520Multi-View%250A%2520%2520Perception%2520and%2520Dual-Span%2520Memory%26entry.906535625%3DZhiheng%2520Li%2520and%2520Yubo%2520Cui%2520and%2520Jiexi%2520Zhong%2520and%2520Zheng%2520Fang%26entry.1292438233%3D%2520%2520Moving%2520object%2520segmentation%2520based%2520on%2520LiDAR%2520is%2520a%2520crucial%2520and%2520challenging%2520task%250Afor%2520autonomous%2520driving%2520and%2520mobile%2520robotics.%2520Most%2520approaches%2520explore%250Aspatio-temporal%2520information%2520from%2520LiDAR%2520sequences%2520to%2520predict%2520moving%2520objects%2520in%250Athe%2520current%2520frame.%2520However%252C%2520they%2520often%2520focus%2520on%2520transferring%2520temporal%2520cues%2520in%2520a%250Asingle%2520inference%2520and%2520regard%2520every%2520prediction%2520as%2520independent%2520of%2520others.%2520This%2520may%250Acause%2520inconsistent%2520segmentation%2520results%2520for%2520the%2520same%2520object%2520in%2520different%250Aframes.%2520To%2520overcome%2520this%2520issue%252C%2520we%2520propose%2520a%2520streaming%2520network%2520with%2520a%2520memory%250Amechanism%252C%2520called%2520StreamMOS%252C%2520to%2520build%2520the%2520association%2520of%2520features%2520and%250Apredictions%2520among%2520multiple%2520inferences.%2520Specifically%252C%2520we%2520utilize%2520a%2520short-term%250Amemory%2520to%2520convey%2520historical%2520features%252C%2520which%2520can%2520be%2520regarded%2520as%2520spatial%2520prior%2520of%250Amoving%2520objects%2520and%2520adopted%2520to%2520enhance%2520current%2520inference%2520by%2520temporal%2520fusion.%250AMeanwhile%252C%2520we%2520build%2520a%2520long-term%2520memory%2520to%2520store%2520previous%2520predictions%2520and%250Aexploit%2520them%2520to%2520refine%2520the%2520present%2520forecast%2520at%2520voxel%2520and%2520instance%2520levels%250Athrough%2520voting.%2520Besides%252C%2520we%2520present%2520multi-view%2520encoder%2520with%2520cascade%2520projection%250Aand%2520asymmetric%2520convolution%2520to%2520extract%2520motion%2520feature%2520of%2520objects%2520in%2520different%250Arepresentations.%2520Extensive%2520experiments%2520validate%2520that%2520our%2520algorithm%2520gets%250Acompetitive%2520performance%2520on%2520SemanticKITTI%2520and%2520Sipailou%2520Campus%2520datasets.%2520Code%250Awill%2520be%2520released%2520at%2520https%253A//github.com/NEU-REAL/StreamMOS.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StreamMOS%3A%20Streaming%20Moving%20Object%20Segmentation%20with%20Multi-View%0A%20%20Perception%20and%20Dual-Span%20Memory&entry.906535625=Zhiheng%20Li%20and%20Yubo%20Cui%20and%20Jiexi%20Zhong%20and%20Zheng%20Fang&entry.1292438233=%20%20Moving%20object%20segmentation%20based%20on%20LiDAR%20is%20a%20crucial%20and%20challenging%20task%0Afor%20autonomous%20driving%20and%20mobile%20robotics.%20Most%20approaches%20explore%0Aspatio-temporal%20information%20from%20LiDAR%20sequences%20to%20predict%20moving%20objects%20in%0Athe%20current%20frame.%20However%2C%20they%20often%20focus%20on%20transferring%20temporal%20cues%20in%20a%0Asingle%20inference%20and%20regard%20every%20prediction%20as%20independent%20of%20others.%20This%20may%0Acause%20inconsistent%20segmentation%20results%20for%20the%20same%20object%20in%20different%0Aframes.%20To%20overcome%20this%20issue%2C%20we%20propose%20a%20streaming%20network%20with%20a%20memory%0Amechanism%2C%20called%20StreamMOS%2C%20to%20build%20the%20association%20of%20features%20and%0Apredictions%20among%20multiple%20inferences.%20Specifically%2C%20we%20utilize%20a%20short-term%0Amemory%20to%20convey%20historical%20features%2C%20which%20can%20be%20regarded%20as%20spatial%20prior%20of%0Amoving%20objects%20and%20adopted%20to%20enhance%20current%20inference%20by%20temporal%20fusion.%0AMeanwhile%2C%20we%20build%20a%20long-term%20memory%20to%20store%20previous%20predictions%20and%0Aexploit%20them%20to%20refine%20the%20present%20forecast%20at%20voxel%20and%20instance%20levels%0Athrough%20voting.%20Besides%2C%20we%20present%20multi-view%20encoder%20with%20cascade%20projection%0Aand%20asymmetric%20convolution%20to%20extract%20motion%20feature%20of%20objects%20in%20different%0Arepresentations.%20Extensive%20experiments%20validate%20that%20our%20algorithm%20gets%0Acompetitive%20performance%20on%20SemanticKITTI%20and%20Sipailou%20Campus%20datasets.%20Code%0Awill%20be%20released%20at%20https%3A//github.com/NEU-REAL/StreamMOS.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17905v1&entry.124074799=Read"},
{"title": "Semantic Diversity-aware Prototype-based Learning for Unbiased Scene\n  Graph Generation", "author": "Jaehyeong Jeon and Kibum Kim and Kanghoon Yoon and Chanyoung Park", "abstract": "  The scene graph generation (SGG) task involves detecting objects within an\nimage and predicting predicates that represent the relationships between the\nobjects. However, in SGG benchmark datasets, each subject-object pair is\nannotated with a single predicate even though a single predicate may exhibit\ndiverse semantics (i.e., semantic diversity), existing SGG models are trained\nto predict the one and only predicate for each pair. This in turn results in\nthe SGG models to overlook the semantic diversity that may exist in a\npredicate, thus leading to biased predictions. In this paper, we propose a\nnovel model-agnostic Semantic Diversity-aware Prototype-based Learning (DPL)\nframework that enables unbiased predictions based on the understanding of the\nsemantic diversity of predicates. Specifically, DPL learns the regions in the\nsemantic space covered by each predicate to distinguish among the various\ndifferent semantics that a single predicate can represent. Extensive\nexperiments demonstrate that our proposed model-agnostic DPL framework brings\nsignificant performance improvement on existing SGG models, and also\neffectively understands the semantic diversity of predicates.\n", "link": "http://arxiv.org/abs/2407.15396v2", "date": "2024-07-25", "relevancy": 2.2753, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5819}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5605}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Diversity-aware%20Prototype-based%20Learning%20for%20Unbiased%20Scene%0A%20%20Graph%20Generation&body=Title%3A%20Semantic%20Diversity-aware%20Prototype-based%20Learning%20for%20Unbiased%20Scene%0A%20%20Graph%20Generation%0AAuthor%3A%20Jaehyeong%20Jeon%20and%20Kibum%20Kim%20and%20Kanghoon%20Yoon%20and%20Chanyoung%20Park%0AAbstract%3A%20%20%20The%20scene%20graph%20generation%20%28SGG%29%20task%20involves%20detecting%20objects%20within%20an%0Aimage%20and%20predicting%20predicates%20that%20represent%20the%20relationships%20between%20the%0Aobjects.%20However%2C%20in%20SGG%20benchmark%20datasets%2C%20each%20subject-object%20pair%20is%0Aannotated%20with%20a%20single%20predicate%20even%20though%20a%20single%20predicate%20may%20exhibit%0Adiverse%20semantics%20%28i.e.%2C%20semantic%20diversity%29%2C%20existing%20SGG%20models%20are%20trained%0Ato%20predict%20the%20one%20and%20only%20predicate%20for%20each%20pair.%20This%20in%20turn%20results%20in%0Athe%20SGG%20models%20to%20overlook%20the%20semantic%20diversity%20that%20may%20exist%20in%20a%0Apredicate%2C%20thus%20leading%20to%20biased%20predictions.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20model-agnostic%20Semantic%20Diversity-aware%20Prototype-based%20Learning%20%28DPL%29%0Aframework%20that%20enables%20unbiased%20predictions%20based%20on%20the%20understanding%20of%20the%0Asemantic%20diversity%20of%20predicates.%20Specifically%2C%20DPL%20learns%20the%20regions%20in%20the%0Asemantic%20space%20covered%20by%20each%20predicate%20to%20distinguish%20among%20the%20various%0Adifferent%20semantics%20that%20a%20single%20predicate%20can%20represent.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20proposed%20model-agnostic%20DPL%20framework%20brings%0Asignificant%20performance%20improvement%20on%20existing%20SGG%20models%2C%20and%20also%0Aeffectively%20understands%20the%20semantic%20diversity%20of%20predicates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15396v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Diversity-aware%2520Prototype-based%2520Learning%2520for%2520Unbiased%2520Scene%250A%2520%2520Graph%2520Generation%26entry.906535625%3DJaehyeong%2520Jeon%2520and%2520Kibum%2520Kim%2520and%2520Kanghoon%2520Yoon%2520and%2520Chanyoung%2520Park%26entry.1292438233%3D%2520%2520The%2520scene%2520graph%2520generation%2520%2528SGG%2529%2520task%2520involves%2520detecting%2520objects%2520within%2520an%250Aimage%2520and%2520predicting%2520predicates%2520that%2520represent%2520the%2520relationships%2520between%2520the%250Aobjects.%2520However%252C%2520in%2520SGG%2520benchmark%2520datasets%252C%2520each%2520subject-object%2520pair%2520is%250Aannotated%2520with%2520a%2520single%2520predicate%2520even%2520though%2520a%2520single%2520predicate%2520may%2520exhibit%250Adiverse%2520semantics%2520%2528i.e.%252C%2520semantic%2520diversity%2529%252C%2520existing%2520SGG%2520models%2520are%2520trained%250Ato%2520predict%2520the%2520one%2520and%2520only%2520predicate%2520for%2520each%2520pair.%2520This%2520in%2520turn%2520results%2520in%250Athe%2520SGG%2520models%2520to%2520overlook%2520the%2520semantic%2520diversity%2520that%2520may%2520exist%2520in%2520a%250Apredicate%252C%2520thus%2520leading%2520to%2520biased%2520predictions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520model-agnostic%2520Semantic%2520Diversity-aware%2520Prototype-based%2520Learning%2520%2528DPL%2529%250Aframework%2520that%2520enables%2520unbiased%2520predictions%2520based%2520on%2520the%2520understanding%2520of%2520the%250Asemantic%2520diversity%2520of%2520predicates.%2520Specifically%252C%2520DPL%2520learns%2520the%2520regions%2520in%2520the%250Asemantic%2520space%2520covered%2520by%2520each%2520predicate%2520to%2520distinguish%2520among%2520the%2520various%250Adifferent%2520semantics%2520that%2520a%2520single%2520predicate%2520can%2520represent.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520proposed%2520model-agnostic%2520DPL%2520framework%2520brings%250Asignificant%2520performance%2520improvement%2520on%2520existing%2520SGG%2520models%252C%2520and%2520also%250Aeffectively%2520understands%2520the%2520semantic%2520diversity%2520of%2520predicates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15396v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Diversity-aware%20Prototype-based%20Learning%20for%20Unbiased%20Scene%0A%20%20Graph%20Generation&entry.906535625=Jaehyeong%20Jeon%20and%20Kibum%20Kim%20and%20Kanghoon%20Yoon%20and%20Chanyoung%20Park&entry.1292438233=%20%20The%20scene%20graph%20generation%20%28SGG%29%20task%20involves%20detecting%20objects%20within%20an%0Aimage%20and%20predicting%20predicates%20that%20represent%20the%20relationships%20between%20the%0Aobjects.%20However%2C%20in%20SGG%20benchmark%20datasets%2C%20each%20subject-object%20pair%20is%0Aannotated%20with%20a%20single%20predicate%20even%20though%20a%20single%20predicate%20may%20exhibit%0Adiverse%20semantics%20%28i.e.%2C%20semantic%20diversity%29%2C%20existing%20SGG%20models%20are%20trained%0Ato%20predict%20the%20one%20and%20only%20predicate%20for%20each%20pair.%20This%20in%20turn%20results%20in%0Athe%20SGG%20models%20to%20overlook%20the%20semantic%20diversity%20that%20may%20exist%20in%20a%0Apredicate%2C%20thus%20leading%20to%20biased%20predictions.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20model-agnostic%20Semantic%20Diversity-aware%20Prototype-based%20Learning%20%28DPL%29%0Aframework%20that%20enables%20unbiased%20predictions%20based%20on%20the%20understanding%20of%20the%0Asemantic%20diversity%20of%20predicates.%20Specifically%2C%20DPL%20learns%20the%20regions%20in%20the%0Asemantic%20space%20covered%20by%20each%20predicate%20to%20distinguish%20among%20the%20various%0Adifferent%20semantics%20that%20a%20single%20predicate%20can%20represent.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20proposed%20model-agnostic%20DPL%20framework%20brings%0Asignificant%20performance%20improvement%20on%20existing%20SGG%20models%2C%20and%20also%0Aeffectively%20understands%20the%20semantic%20diversity%20of%20predicates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15396v2&entry.124074799=Read"},
{"title": "BEVCar: Camera-Radar Fusion for BEV Map and Object Segmentation", "author": "Jonas Schramm and Niclas V\u00f6disch and K\u00fcrsat Petek and B Ravi Kiran and Senthil Yogamani and Wolfram Burgard and Abhinav Valada", "abstract": "  Semantic scene segmentation from a bird's-eye-view (BEV) perspective plays a\ncrucial role in facilitating planning and decision-making for mobile robots.\nAlthough recent vision-only methods have demonstrated notable advancements in\nperformance, they often struggle under adverse illumination conditions such as\nrain or nighttime. While active sensors offer a solution to this challenge, the\nprohibitively high cost of LiDARs remains a limiting factor. Fusing camera data\nwith automotive radars poses a more inexpensive alternative but has received\nless attention in prior research. In this work, we aim to advance this\npromising avenue by introducing BEVCar, a novel approach for joint BEV object\nand map segmentation. The core novelty of our approach lies in first learning a\npoint-based encoding of raw radar data, which is then leveraged to efficiently\ninitialize the lifting of image features into the BEV space. We perform\nextensive experiments on the nuScenes dataset and demonstrate that BEVCar\noutperforms the current state of the art. Moreover, we show that incorporating\nradar information significantly enhances robustness in challenging\nenvironmental conditions and improves segmentation performance for distant\nobjects. To foster future research, we provide the weather split of the\nnuScenes dataset used in our experiments, along with our code and trained\nmodels at http://bevcar.cs.uni-freiburg.de.\n", "link": "http://arxiv.org/abs/2403.11761v2", "date": "2024-07-25", "relevancy": 2.2673, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5999}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5846}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BEVCar%3A%20Camera-Radar%20Fusion%20for%20BEV%20Map%20and%20Object%20Segmentation&body=Title%3A%20BEVCar%3A%20Camera-Radar%20Fusion%20for%20BEV%20Map%20and%20Object%20Segmentation%0AAuthor%3A%20Jonas%20Schramm%20and%20Niclas%20V%C3%B6disch%20and%20K%C3%BCrsat%20Petek%20and%20B%20Ravi%20Kiran%20and%20Senthil%20Yogamani%20and%20Wolfram%20Burgard%20and%20Abhinav%20Valada%0AAbstract%3A%20%20%20Semantic%20scene%20segmentation%20from%20a%20bird%27s-eye-view%20%28BEV%29%20perspective%20plays%20a%0Acrucial%20role%20in%20facilitating%20planning%20and%20decision-making%20for%20mobile%20robots.%0AAlthough%20recent%20vision-only%20methods%20have%20demonstrated%20notable%20advancements%20in%0Aperformance%2C%20they%20often%20struggle%20under%20adverse%20illumination%20conditions%20such%20as%0Arain%20or%20nighttime.%20While%20active%20sensors%20offer%20a%20solution%20to%20this%20challenge%2C%20the%0Aprohibitively%20high%20cost%20of%20LiDARs%20remains%20a%20limiting%20factor.%20Fusing%20camera%20data%0Awith%20automotive%20radars%20poses%20a%20more%20inexpensive%20alternative%20but%20has%20received%0Aless%20attention%20in%20prior%20research.%20In%20this%20work%2C%20we%20aim%20to%20advance%20this%0Apromising%20avenue%20by%20introducing%20BEVCar%2C%20a%20novel%20approach%20for%20joint%20BEV%20object%0Aand%20map%20segmentation.%20The%20core%20novelty%20of%20our%20approach%20lies%20in%20first%20learning%20a%0Apoint-based%20encoding%20of%20raw%20radar%20data%2C%20which%20is%20then%20leveraged%20to%20efficiently%0Ainitialize%20the%20lifting%20of%20image%20features%20into%20the%20BEV%20space.%20We%20perform%0Aextensive%20experiments%20on%20the%20nuScenes%20dataset%20and%20demonstrate%20that%20BEVCar%0Aoutperforms%20the%20current%20state%20of%20the%20art.%20Moreover%2C%20we%20show%20that%20incorporating%0Aradar%20information%20significantly%20enhances%20robustness%20in%20challenging%0Aenvironmental%20conditions%20and%20improves%20segmentation%20performance%20for%20distant%0Aobjects.%20To%20foster%20future%20research%2C%20we%20provide%20the%20weather%20split%20of%20the%0AnuScenes%20dataset%20used%20in%20our%20experiments%2C%20along%20with%20our%20code%20and%20trained%0Amodels%20at%20http%3A//bevcar.cs.uni-freiburg.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11761v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBEVCar%253A%2520Camera-Radar%2520Fusion%2520for%2520BEV%2520Map%2520and%2520Object%2520Segmentation%26entry.906535625%3DJonas%2520Schramm%2520and%2520Niclas%2520V%25C3%25B6disch%2520and%2520K%25C3%25BCrsat%2520Petek%2520and%2520B%2520Ravi%2520Kiran%2520and%2520Senthil%2520Yogamani%2520and%2520Wolfram%2520Burgard%2520and%2520Abhinav%2520Valada%26entry.1292438233%3D%2520%2520Semantic%2520scene%2520segmentation%2520from%2520a%2520bird%2527s-eye-view%2520%2528BEV%2529%2520perspective%2520plays%2520a%250Acrucial%2520role%2520in%2520facilitating%2520planning%2520and%2520decision-making%2520for%2520mobile%2520robots.%250AAlthough%2520recent%2520vision-only%2520methods%2520have%2520demonstrated%2520notable%2520advancements%2520in%250Aperformance%252C%2520they%2520often%2520struggle%2520under%2520adverse%2520illumination%2520conditions%2520such%2520as%250Arain%2520or%2520nighttime.%2520While%2520active%2520sensors%2520offer%2520a%2520solution%2520to%2520this%2520challenge%252C%2520the%250Aprohibitively%2520high%2520cost%2520of%2520LiDARs%2520remains%2520a%2520limiting%2520factor.%2520Fusing%2520camera%2520data%250Awith%2520automotive%2520radars%2520poses%2520a%2520more%2520inexpensive%2520alternative%2520but%2520has%2520received%250Aless%2520attention%2520in%2520prior%2520research.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520advance%2520this%250Apromising%2520avenue%2520by%2520introducing%2520BEVCar%252C%2520a%2520novel%2520approach%2520for%2520joint%2520BEV%2520object%250Aand%2520map%2520segmentation.%2520The%2520core%2520novelty%2520of%2520our%2520approach%2520lies%2520in%2520first%2520learning%2520a%250Apoint-based%2520encoding%2520of%2520raw%2520radar%2520data%252C%2520which%2520is%2520then%2520leveraged%2520to%2520efficiently%250Ainitialize%2520the%2520lifting%2520of%2520image%2520features%2520into%2520the%2520BEV%2520space.%2520We%2520perform%250Aextensive%2520experiments%2520on%2520the%2520nuScenes%2520dataset%2520and%2520demonstrate%2520that%2520BEVCar%250Aoutperforms%2520the%2520current%2520state%2520of%2520the%2520art.%2520Moreover%252C%2520we%2520show%2520that%2520incorporating%250Aradar%2520information%2520significantly%2520enhances%2520robustness%2520in%2520challenging%250Aenvironmental%2520conditions%2520and%2520improves%2520segmentation%2520performance%2520for%2520distant%250Aobjects.%2520To%2520foster%2520future%2520research%252C%2520we%2520provide%2520the%2520weather%2520split%2520of%2520the%250AnuScenes%2520dataset%2520used%2520in%2520our%2520experiments%252C%2520along%2520with%2520our%2520code%2520and%2520trained%250Amodels%2520at%2520http%253A//bevcar.cs.uni-freiburg.de.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11761v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEVCar%3A%20Camera-Radar%20Fusion%20for%20BEV%20Map%20and%20Object%20Segmentation&entry.906535625=Jonas%20Schramm%20and%20Niclas%20V%C3%B6disch%20and%20K%C3%BCrsat%20Petek%20and%20B%20Ravi%20Kiran%20and%20Senthil%20Yogamani%20and%20Wolfram%20Burgard%20and%20Abhinav%20Valada&entry.1292438233=%20%20Semantic%20scene%20segmentation%20from%20a%20bird%27s-eye-view%20%28BEV%29%20perspective%20plays%20a%0Acrucial%20role%20in%20facilitating%20planning%20and%20decision-making%20for%20mobile%20robots.%0AAlthough%20recent%20vision-only%20methods%20have%20demonstrated%20notable%20advancements%20in%0Aperformance%2C%20they%20often%20struggle%20under%20adverse%20illumination%20conditions%20such%20as%0Arain%20or%20nighttime.%20While%20active%20sensors%20offer%20a%20solution%20to%20this%20challenge%2C%20the%0Aprohibitively%20high%20cost%20of%20LiDARs%20remains%20a%20limiting%20factor.%20Fusing%20camera%20data%0Awith%20automotive%20radars%20poses%20a%20more%20inexpensive%20alternative%20but%20has%20received%0Aless%20attention%20in%20prior%20research.%20In%20this%20work%2C%20we%20aim%20to%20advance%20this%0Apromising%20avenue%20by%20introducing%20BEVCar%2C%20a%20novel%20approach%20for%20joint%20BEV%20object%0Aand%20map%20segmentation.%20The%20core%20novelty%20of%20our%20approach%20lies%20in%20first%20learning%20a%0Apoint-based%20encoding%20of%20raw%20radar%20data%2C%20which%20is%20then%20leveraged%20to%20efficiently%0Ainitialize%20the%20lifting%20of%20image%20features%20into%20the%20BEV%20space.%20We%20perform%0Aextensive%20experiments%20on%20the%20nuScenes%20dataset%20and%20demonstrate%20that%20BEVCar%0Aoutperforms%20the%20current%20state%20of%20the%20art.%20Moreover%2C%20we%20show%20that%20incorporating%0Aradar%20information%20significantly%20enhances%20robustness%20in%20challenging%0Aenvironmental%20conditions%20and%20improves%20segmentation%20performance%20for%20distant%0Aobjects.%20To%20foster%20future%20research%2C%20we%20provide%20the%20weather%20split%20of%20the%0AnuScenes%20dataset%20used%20in%20our%20experiments%2C%20along%20with%20our%20code%20and%20trained%0Amodels%20at%20http%3A//bevcar.cs.uni-freiburg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11761v2&entry.124074799=Read"},
{"title": "SaccadeDet: A Novel Dual-Stage Architecture for Rapid and Accurate\n  Detection in Gigapixel Images", "author": "Wenxi Li and Ruxin Zhang and Haozhe Lin and Yuchen Guo and Chao Ma and Xiaokang Yang", "abstract": "  The advancement of deep learning in object detection has predominantly\nfocused on megapixel images, leaving a critical gap in the efficient processing\nof gigapixel images. These super high-resolution images present unique\nchallenges due to their immense size and computational demands. To address\nthis, we introduce 'SaccadeDet', an innovative architecture for gigapixel-level\nobject detection, inspired by the human eye saccadic movement. The cornerstone\nof SaccadeDet is its ability to strategically select and process image regions,\ndramatically reducing computational load. This is achieved through a two-stage\nprocess: the 'saccade' stage, which identifies regions of probable interest,\nand the 'gaze' stage, which refines detection in these targeted areas. Our\napproach, evaluated on the PANDA dataset, not only achieves an 8x speed\nincrease over the state-of-the-art methods but also demonstrates significant\npotential in gigapixel-level pathology analysis through its application to\nWhole Slide Imaging.\n", "link": "http://arxiv.org/abs/2407.17956v1", "date": "2024-07-25", "relevancy": 2.2667, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.587}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.578}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SaccadeDet%3A%20A%20Novel%20Dual-Stage%20Architecture%20for%20Rapid%20and%20Accurate%0A%20%20Detection%20in%20Gigapixel%20Images&body=Title%3A%20SaccadeDet%3A%20A%20Novel%20Dual-Stage%20Architecture%20for%20Rapid%20and%20Accurate%0A%20%20Detection%20in%20Gigapixel%20Images%0AAuthor%3A%20Wenxi%20Li%20and%20Ruxin%20Zhang%20and%20Haozhe%20Lin%20and%20Yuchen%20Guo%20and%20Chao%20Ma%20and%20Xiaokang%20Yang%0AAbstract%3A%20%20%20The%20advancement%20of%20deep%20learning%20in%20object%20detection%20has%20predominantly%0Afocused%20on%20megapixel%20images%2C%20leaving%20a%20critical%20gap%20in%20the%20efficient%20processing%0Aof%20gigapixel%20images.%20These%20super%20high-resolution%20images%20present%20unique%0Achallenges%20due%20to%20their%20immense%20size%20and%20computational%20demands.%20To%20address%0Athis%2C%20we%20introduce%20%27SaccadeDet%27%2C%20an%20innovative%20architecture%20for%20gigapixel-level%0Aobject%20detection%2C%20inspired%20by%20the%20human%20eye%20saccadic%20movement.%20The%20cornerstone%0Aof%20SaccadeDet%20is%20its%20ability%20to%20strategically%20select%20and%20process%20image%20regions%2C%0Adramatically%20reducing%20computational%20load.%20This%20is%20achieved%20through%20a%20two-stage%0Aprocess%3A%20the%20%27saccade%27%20stage%2C%20which%20identifies%20regions%20of%20probable%20interest%2C%0Aand%20the%20%27gaze%27%20stage%2C%20which%20refines%20detection%20in%20these%20targeted%20areas.%20Our%0Aapproach%2C%20evaluated%20on%20the%20PANDA%20dataset%2C%20not%20only%20achieves%20an%208x%20speed%0Aincrease%20over%20the%20state-of-the-art%20methods%20but%20also%20demonstrates%20significant%0Apotential%20in%20gigapixel-level%20pathology%20analysis%20through%20its%20application%20to%0AWhole%20Slide%20Imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17956v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSaccadeDet%253A%2520A%2520Novel%2520Dual-Stage%2520Architecture%2520for%2520Rapid%2520and%2520Accurate%250A%2520%2520Detection%2520in%2520Gigapixel%2520Images%26entry.906535625%3DWenxi%2520Li%2520and%2520Ruxin%2520Zhang%2520and%2520Haozhe%2520Lin%2520and%2520Yuchen%2520Guo%2520and%2520Chao%2520Ma%2520and%2520Xiaokang%2520Yang%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520deep%2520learning%2520in%2520object%2520detection%2520has%2520predominantly%250Afocused%2520on%2520megapixel%2520images%252C%2520leaving%2520a%2520critical%2520gap%2520in%2520the%2520efficient%2520processing%250Aof%2520gigapixel%2520images.%2520These%2520super%2520high-resolution%2520images%2520present%2520unique%250Achallenges%2520due%2520to%2520their%2520immense%2520size%2520and%2520computational%2520demands.%2520To%2520address%250Athis%252C%2520we%2520introduce%2520%2527SaccadeDet%2527%252C%2520an%2520innovative%2520architecture%2520for%2520gigapixel-level%250Aobject%2520detection%252C%2520inspired%2520by%2520the%2520human%2520eye%2520saccadic%2520movement.%2520The%2520cornerstone%250Aof%2520SaccadeDet%2520is%2520its%2520ability%2520to%2520strategically%2520select%2520and%2520process%2520image%2520regions%252C%250Adramatically%2520reducing%2520computational%2520load.%2520This%2520is%2520achieved%2520through%2520a%2520two-stage%250Aprocess%253A%2520the%2520%2527saccade%2527%2520stage%252C%2520which%2520identifies%2520regions%2520of%2520probable%2520interest%252C%250Aand%2520the%2520%2527gaze%2527%2520stage%252C%2520which%2520refines%2520detection%2520in%2520these%2520targeted%2520areas.%2520Our%250Aapproach%252C%2520evaluated%2520on%2520the%2520PANDA%2520dataset%252C%2520not%2520only%2520achieves%2520an%25208x%2520speed%250Aincrease%2520over%2520the%2520state-of-the-art%2520methods%2520but%2520also%2520demonstrates%2520significant%250Apotential%2520in%2520gigapixel-level%2520pathology%2520analysis%2520through%2520its%2520application%2520to%250AWhole%2520Slide%2520Imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17956v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SaccadeDet%3A%20A%20Novel%20Dual-Stage%20Architecture%20for%20Rapid%20and%20Accurate%0A%20%20Detection%20in%20Gigapixel%20Images&entry.906535625=Wenxi%20Li%20and%20Ruxin%20Zhang%20and%20Haozhe%20Lin%20and%20Yuchen%20Guo%20and%20Chao%20Ma%20and%20Xiaokang%20Yang&entry.1292438233=%20%20The%20advancement%20of%20deep%20learning%20in%20object%20detection%20has%20predominantly%0Afocused%20on%20megapixel%20images%2C%20leaving%20a%20critical%20gap%20in%20the%20efficient%20processing%0Aof%20gigapixel%20images.%20These%20super%20high-resolution%20images%20present%20unique%0Achallenges%20due%20to%20their%20immense%20size%20and%20computational%20demands.%20To%20address%0Athis%2C%20we%20introduce%20%27SaccadeDet%27%2C%20an%20innovative%20architecture%20for%20gigapixel-level%0Aobject%20detection%2C%20inspired%20by%20the%20human%20eye%20saccadic%20movement.%20The%20cornerstone%0Aof%20SaccadeDet%20is%20its%20ability%20to%20strategically%20select%20and%20process%20image%20regions%2C%0Adramatically%20reducing%20computational%20load.%20This%20is%20achieved%20through%20a%20two-stage%0Aprocess%3A%20the%20%27saccade%27%20stage%2C%20which%20identifies%20regions%20of%20probable%20interest%2C%0Aand%20the%20%27gaze%27%20stage%2C%20which%20refines%20detection%20in%20these%20targeted%20areas.%20Our%0Aapproach%2C%20evaluated%20on%20the%20PANDA%20dataset%2C%20not%20only%20achieves%20an%208x%20speed%0Aincrease%20over%20the%20state-of-the-art%20methods%20but%20also%20demonstrates%20significant%0Apotential%20in%20gigapixel-level%20pathology%20analysis%20through%20its%20application%20to%0AWhole%20Slide%20Imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17956v1&entry.124074799=Read"},
{"title": "Taxonomy-Aware Continual Semantic Segmentation in Hyperbolic Spaces for\n  Open-World Perception", "author": "Julia Hindel and Daniele Cattaneo and Abhinav Valada", "abstract": "  Semantic segmentation models are typically trained on a fixed set of classes,\nlimiting their applicability in open-world scenarios. Class-incremental\nsemantic segmentation aims to update models with emerging new classes while\npreventing catastrophic forgetting of previously learned ones. However,\nexisting methods impose strict rigidity on old classes, reducing their\neffectiveness in learning new incremental classes. In this work, we propose\nTaxonomy-Oriented Poincar\\'e-regularized Incremental-Class Segmentation\n(TOPICS) that learns feature embeddings in hyperbolic space following explicit\ntaxonomy-tree structures. This supervision provides plasticity for old classes,\nupdating ancestors based on new classes while integrating new classes at\nfitting positions. Additionally, we maintain implicit class relational\nconstraints on the geometric basis of the Poincar\\'e ball. This ensures that\nthe latent space can continuously adapt to new constraints while maintaining a\nrobust structure to combat catastrophic forgetting. We also establish eight\nrealistic incremental learning protocols for autonomous driving scenarios,\nwhere novel classes can originate from known classes or the background.\nExtensive evaluations of TOPICS on the Cityscapes and Mapillary Vistas 2.0\nbenchmarks demonstrate that it achieves state-of-the-art performance. We make\nthe code and trained models publicly available at\nhttp://topics.cs.uni-freiburg.de.\n", "link": "http://arxiv.org/abs/2407.18145v1", "date": "2024-07-25", "relevancy": 2.2577, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5947}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5765}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taxonomy-Aware%20Continual%20Semantic%20Segmentation%20in%20Hyperbolic%20Spaces%20for%0A%20%20Open-World%20Perception&body=Title%3A%20Taxonomy-Aware%20Continual%20Semantic%20Segmentation%20in%20Hyperbolic%20Spaces%20for%0A%20%20Open-World%20Perception%0AAuthor%3A%20Julia%20Hindel%20and%20Daniele%20Cattaneo%20and%20Abhinav%20Valada%0AAbstract%3A%20%20%20Semantic%20segmentation%20models%20are%20typically%20trained%20on%20a%20fixed%20set%20of%20classes%2C%0Alimiting%20their%20applicability%20in%20open-world%20scenarios.%20Class-incremental%0Asemantic%20segmentation%20aims%20to%20update%20models%20with%20emerging%20new%20classes%20while%0Apreventing%20catastrophic%20forgetting%20of%20previously%20learned%20ones.%20However%2C%0Aexisting%20methods%20impose%20strict%20rigidity%20on%20old%20classes%2C%20reducing%20their%0Aeffectiveness%20in%20learning%20new%20incremental%20classes.%20In%20this%20work%2C%20we%20propose%0ATaxonomy-Oriented%20Poincar%5C%27e-regularized%20Incremental-Class%20Segmentation%0A%28TOPICS%29%20that%20learns%20feature%20embeddings%20in%20hyperbolic%20space%20following%20explicit%0Ataxonomy-tree%20structures.%20This%20supervision%20provides%20plasticity%20for%20old%20classes%2C%0Aupdating%20ancestors%20based%20on%20new%20classes%20while%20integrating%20new%20classes%20at%0Afitting%20positions.%20Additionally%2C%20we%20maintain%20implicit%20class%20relational%0Aconstraints%20on%20the%20geometric%20basis%20of%20the%20Poincar%5C%27e%20ball.%20This%20ensures%20that%0Athe%20latent%20space%20can%20continuously%20adapt%20to%20new%20constraints%20while%20maintaining%20a%0Arobust%20structure%20to%20combat%20catastrophic%20forgetting.%20We%20also%20establish%20eight%0Arealistic%20incremental%20learning%20protocols%20for%20autonomous%20driving%20scenarios%2C%0Awhere%20novel%20classes%20can%20originate%20from%20known%20classes%20or%20the%20background.%0AExtensive%20evaluations%20of%20TOPICS%20on%20the%20Cityscapes%20and%20Mapillary%20Vistas%202.0%0Abenchmarks%20demonstrate%20that%20it%20achieves%20state-of-the-art%20performance.%20We%20make%0Athe%20code%20and%20trained%20models%20publicly%20available%20at%0Ahttp%3A//topics.cs.uni-freiburg.de.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaxonomy-Aware%2520Continual%2520Semantic%2520Segmentation%2520in%2520Hyperbolic%2520Spaces%2520for%250A%2520%2520Open-World%2520Perception%26entry.906535625%3DJulia%2520Hindel%2520and%2520Daniele%2520Cattaneo%2520and%2520Abhinav%2520Valada%26entry.1292438233%3D%2520%2520Semantic%2520segmentation%2520models%2520are%2520typically%2520trained%2520on%2520a%2520fixed%2520set%2520of%2520classes%252C%250Alimiting%2520their%2520applicability%2520in%2520open-world%2520scenarios.%2520Class-incremental%250Asemantic%2520segmentation%2520aims%2520to%2520update%2520models%2520with%2520emerging%2520new%2520classes%2520while%250Apreventing%2520catastrophic%2520forgetting%2520of%2520previously%2520learned%2520ones.%2520However%252C%250Aexisting%2520methods%2520impose%2520strict%2520rigidity%2520on%2520old%2520classes%252C%2520reducing%2520their%250Aeffectiveness%2520in%2520learning%2520new%2520incremental%2520classes.%2520In%2520this%2520work%252C%2520we%2520propose%250ATaxonomy-Oriented%2520Poincar%255C%2527e-regularized%2520Incremental-Class%2520Segmentation%250A%2528TOPICS%2529%2520that%2520learns%2520feature%2520embeddings%2520in%2520hyperbolic%2520space%2520following%2520explicit%250Ataxonomy-tree%2520structures.%2520This%2520supervision%2520provides%2520plasticity%2520for%2520old%2520classes%252C%250Aupdating%2520ancestors%2520based%2520on%2520new%2520classes%2520while%2520integrating%2520new%2520classes%2520at%250Afitting%2520positions.%2520Additionally%252C%2520we%2520maintain%2520implicit%2520class%2520relational%250Aconstraints%2520on%2520the%2520geometric%2520basis%2520of%2520the%2520Poincar%255C%2527e%2520ball.%2520This%2520ensures%2520that%250Athe%2520latent%2520space%2520can%2520continuously%2520adapt%2520to%2520new%2520constraints%2520while%2520maintaining%2520a%250Arobust%2520structure%2520to%2520combat%2520catastrophic%2520forgetting.%2520We%2520also%2520establish%2520eight%250Arealistic%2520incremental%2520learning%2520protocols%2520for%2520autonomous%2520driving%2520scenarios%252C%250Awhere%2520novel%2520classes%2520can%2520originate%2520from%2520known%2520classes%2520or%2520the%2520background.%250AExtensive%2520evaluations%2520of%2520TOPICS%2520on%2520the%2520Cityscapes%2520and%2520Mapillary%2520Vistas%25202.0%250Abenchmarks%2520demonstrate%2520that%2520it%2520achieves%2520state-of-the-art%2520performance.%2520We%2520make%250Athe%2520code%2520and%2520trained%2520models%2520publicly%2520available%2520at%250Ahttp%253A//topics.cs.uni-freiburg.de.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taxonomy-Aware%20Continual%20Semantic%20Segmentation%20in%20Hyperbolic%20Spaces%20for%0A%20%20Open-World%20Perception&entry.906535625=Julia%20Hindel%20and%20Daniele%20Cattaneo%20and%20Abhinav%20Valada&entry.1292438233=%20%20Semantic%20segmentation%20models%20are%20typically%20trained%20on%20a%20fixed%20set%20of%20classes%2C%0Alimiting%20their%20applicability%20in%20open-world%20scenarios.%20Class-incremental%0Asemantic%20segmentation%20aims%20to%20update%20models%20with%20emerging%20new%20classes%20while%0Apreventing%20catastrophic%20forgetting%20of%20previously%20learned%20ones.%20However%2C%0Aexisting%20methods%20impose%20strict%20rigidity%20on%20old%20classes%2C%20reducing%20their%0Aeffectiveness%20in%20learning%20new%20incremental%20classes.%20In%20this%20work%2C%20we%20propose%0ATaxonomy-Oriented%20Poincar%5C%27e-regularized%20Incremental-Class%20Segmentation%0A%28TOPICS%29%20that%20learns%20feature%20embeddings%20in%20hyperbolic%20space%20following%20explicit%0Ataxonomy-tree%20structures.%20This%20supervision%20provides%20plasticity%20for%20old%20classes%2C%0Aupdating%20ancestors%20based%20on%20new%20classes%20while%20integrating%20new%20classes%20at%0Afitting%20positions.%20Additionally%2C%20we%20maintain%20implicit%20class%20relational%0Aconstraints%20on%20the%20geometric%20basis%20of%20the%20Poincar%5C%27e%20ball.%20This%20ensures%20that%0Athe%20latent%20space%20can%20continuously%20adapt%20to%20new%20constraints%20while%20maintaining%20a%0Arobust%20structure%20to%20combat%20catastrophic%20forgetting.%20We%20also%20establish%20eight%0Arealistic%20incremental%20learning%20protocols%20for%20autonomous%20driving%20scenarios%2C%0Awhere%20novel%20classes%20can%20originate%20from%20known%20classes%20or%20the%20background.%0AExtensive%20evaluations%20of%20TOPICS%20on%20the%20Cityscapes%20and%20Mapillary%20Vistas%202.0%0Abenchmarks%20demonstrate%20that%20it%20achieves%20state-of-the-art%20performance.%20We%20make%0Athe%20code%20and%20trained%20models%20publicly%20available%20at%0Ahttp%3A//topics.cs.uni-freiburg.de.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18145v1&entry.124074799=Read"},
{"title": "Lifelong Graph Summarization with Neural Networks: 2012, 2022, and a\n  Time Warp", "author": "Jonatan Frank and Marcel Hoffmann and Nicolas Lell and David Richerby and Ansgar Scherp", "abstract": "  Summarizing web graphs is challenging due to the heterogeneity of the modeled\ninformation and its changes over time. We investigate the use of neural\nnetworks for lifelong graph summarization. Assuming we observe the web graph at\na certain time, we train the networks to summarize graph vertices. We apply\nthis trained network to summarize the vertices of the changed graph at the next\npoint in time. Subsequently, we continue training and evaluating the network to\nperform lifelong graph summarization. We use the GNNs Graph-MLP and GraphSAINT,\nas well as an MLP baseline, to summarize the temporal graphs. We compare\n$1$-hop and $2$-hop summaries. We investigate the impact of reusing parameters\nfrom a previous snapshot by measuring the backward and forward transfer and the\nforgetting rate of the neural networks. Our extensive experiments on ten weekly\nsnapshots of a web graph with over $100$M edges, sampled in 2012 and 2022, show\nthat all networks predominantly use $1$-hop information to determine the\nsummary, even when performing $2$-hop summarization. Due to the heterogeneity\nof web graphs, in some snapshots, the $2$-hop summary produces over ten times\nmore vertex summaries than the $1$-hop summary. When using the network trained\non the last snapshot from 2012 and applying it to the first snapshot of 2022,\nwe observe a strong drop in accuracy. We attribute this drop over the ten-year\ntime warp to the strongly increased heterogeneity of the web graph in 2022.\n", "link": "http://arxiv.org/abs/2407.18042v1", "date": "2024-07-25", "relevancy": 2.2416, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4746}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4381}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lifelong%20Graph%20Summarization%20with%20Neural%20Networks%3A%202012%2C%202022%2C%20and%20a%0A%20%20Time%20Warp&body=Title%3A%20Lifelong%20Graph%20Summarization%20with%20Neural%20Networks%3A%202012%2C%202022%2C%20and%20a%0A%20%20Time%20Warp%0AAuthor%3A%20Jonatan%20Frank%20and%20Marcel%20Hoffmann%20and%20Nicolas%20Lell%20and%20David%20Richerby%20and%20Ansgar%20Scherp%0AAbstract%3A%20%20%20Summarizing%20web%20graphs%20is%20challenging%20due%20to%20the%20heterogeneity%20of%20the%20modeled%0Ainformation%20and%20its%20changes%20over%20time.%20We%20investigate%20the%20use%20of%20neural%0Anetworks%20for%20lifelong%20graph%20summarization.%20Assuming%20we%20observe%20the%20web%20graph%20at%0Aa%20certain%20time%2C%20we%20train%20the%20networks%20to%20summarize%20graph%20vertices.%20We%20apply%0Athis%20trained%20network%20to%20summarize%20the%20vertices%20of%20the%20changed%20graph%20at%20the%20next%0Apoint%20in%20time.%20Subsequently%2C%20we%20continue%20training%20and%20evaluating%20the%20network%20to%0Aperform%20lifelong%20graph%20summarization.%20We%20use%20the%20GNNs%20Graph-MLP%20and%20GraphSAINT%2C%0Aas%20well%20as%20an%20MLP%20baseline%2C%20to%20summarize%20the%20temporal%20graphs.%20We%20compare%0A%241%24-hop%20and%20%242%24-hop%20summaries.%20We%20investigate%20the%20impact%20of%20reusing%20parameters%0Afrom%20a%20previous%20snapshot%20by%20measuring%20the%20backward%20and%20forward%20transfer%20and%20the%0Aforgetting%20rate%20of%20the%20neural%20networks.%20Our%20extensive%20experiments%20on%20ten%20weekly%0Asnapshots%20of%20a%20web%20graph%20with%20over%20%24100%24M%20edges%2C%20sampled%20in%202012%20and%202022%2C%20show%0Athat%20all%20networks%20predominantly%20use%20%241%24-hop%20information%20to%20determine%20the%0Asummary%2C%20even%20when%20performing%20%242%24-hop%20summarization.%20Due%20to%20the%20heterogeneity%0Aof%20web%20graphs%2C%20in%20some%20snapshots%2C%20the%20%242%24-hop%20summary%20produces%20over%20ten%20times%0Amore%20vertex%20summaries%20than%20the%20%241%24-hop%20summary.%20When%20using%20the%20network%20trained%0Aon%20the%20last%20snapshot%20from%202012%20and%20applying%20it%20to%20the%20first%20snapshot%20of%202022%2C%0Awe%20observe%20a%20strong%20drop%20in%20accuracy.%20We%20attribute%20this%20drop%20over%20the%20ten-year%0Atime%20warp%20to%20the%20strongly%20increased%20heterogeneity%20of%20the%20web%20graph%20in%202022.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLifelong%2520Graph%2520Summarization%2520with%2520Neural%2520Networks%253A%25202012%252C%25202022%252C%2520and%2520a%250A%2520%2520Time%2520Warp%26entry.906535625%3DJonatan%2520Frank%2520and%2520Marcel%2520Hoffmann%2520and%2520Nicolas%2520Lell%2520and%2520David%2520Richerby%2520and%2520Ansgar%2520Scherp%26entry.1292438233%3D%2520%2520Summarizing%2520web%2520graphs%2520is%2520challenging%2520due%2520to%2520the%2520heterogeneity%2520of%2520the%2520modeled%250Ainformation%2520and%2520its%2520changes%2520over%2520time.%2520We%2520investigate%2520the%2520use%2520of%2520neural%250Anetworks%2520for%2520lifelong%2520graph%2520summarization.%2520Assuming%2520we%2520observe%2520the%2520web%2520graph%2520at%250Aa%2520certain%2520time%252C%2520we%2520train%2520the%2520networks%2520to%2520summarize%2520graph%2520vertices.%2520We%2520apply%250Athis%2520trained%2520network%2520to%2520summarize%2520the%2520vertices%2520of%2520the%2520changed%2520graph%2520at%2520the%2520next%250Apoint%2520in%2520time.%2520Subsequently%252C%2520we%2520continue%2520training%2520and%2520evaluating%2520the%2520network%2520to%250Aperform%2520lifelong%2520graph%2520summarization.%2520We%2520use%2520the%2520GNNs%2520Graph-MLP%2520and%2520GraphSAINT%252C%250Aas%2520well%2520as%2520an%2520MLP%2520baseline%252C%2520to%2520summarize%2520the%2520temporal%2520graphs.%2520We%2520compare%250A%25241%2524-hop%2520and%2520%25242%2524-hop%2520summaries.%2520We%2520investigate%2520the%2520impact%2520of%2520reusing%2520parameters%250Afrom%2520a%2520previous%2520snapshot%2520by%2520measuring%2520the%2520backward%2520and%2520forward%2520transfer%2520and%2520the%250Aforgetting%2520rate%2520of%2520the%2520neural%2520networks.%2520Our%2520extensive%2520experiments%2520on%2520ten%2520weekly%250Asnapshots%2520of%2520a%2520web%2520graph%2520with%2520over%2520%2524100%2524M%2520edges%252C%2520sampled%2520in%25202012%2520and%25202022%252C%2520show%250Athat%2520all%2520networks%2520predominantly%2520use%2520%25241%2524-hop%2520information%2520to%2520determine%2520the%250Asummary%252C%2520even%2520when%2520performing%2520%25242%2524-hop%2520summarization.%2520Due%2520to%2520the%2520heterogeneity%250Aof%2520web%2520graphs%252C%2520in%2520some%2520snapshots%252C%2520the%2520%25242%2524-hop%2520summary%2520produces%2520over%2520ten%2520times%250Amore%2520vertex%2520summaries%2520than%2520the%2520%25241%2524-hop%2520summary.%2520When%2520using%2520the%2520network%2520trained%250Aon%2520the%2520last%2520snapshot%2520from%25202012%2520and%2520applying%2520it%2520to%2520the%2520first%2520snapshot%2520of%25202022%252C%250Awe%2520observe%2520a%2520strong%2520drop%2520in%2520accuracy.%2520We%2520attribute%2520this%2520drop%2520over%2520the%2520ten-year%250Atime%2520warp%2520to%2520the%2520strongly%2520increased%2520heterogeneity%2520of%2520the%2520web%2520graph%2520in%25202022.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lifelong%20Graph%20Summarization%20with%20Neural%20Networks%3A%202012%2C%202022%2C%20and%20a%0A%20%20Time%20Warp&entry.906535625=Jonatan%20Frank%20and%20Marcel%20Hoffmann%20and%20Nicolas%20Lell%20and%20David%20Richerby%20and%20Ansgar%20Scherp&entry.1292438233=%20%20Summarizing%20web%20graphs%20is%20challenging%20due%20to%20the%20heterogeneity%20of%20the%20modeled%0Ainformation%20and%20its%20changes%20over%20time.%20We%20investigate%20the%20use%20of%20neural%0Anetworks%20for%20lifelong%20graph%20summarization.%20Assuming%20we%20observe%20the%20web%20graph%20at%0Aa%20certain%20time%2C%20we%20train%20the%20networks%20to%20summarize%20graph%20vertices.%20We%20apply%0Athis%20trained%20network%20to%20summarize%20the%20vertices%20of%20the%20changed%20graph%20at%20the%20next%0Apoint%20in%20time.%20Subsequently%2C%20we%20continue%20training%20and%20evaluating%20the%20network%20to%0Aperform%20lifelong%20graph%20summarization.%20We%20use%20the%20GNNs%20Graph-MLP%20and%20GraphSAINT%2C%0Aas%20well%20as%20an%20MLP%20baseline%2C%20to%20summarize%20the%20temporal%20graphs.%20We%20compare%0A%241%24-hop%20and%20%242%24-hop%20summaries.%20We%20investigate%20the%20impact%20of%20reusing%20parameters%0Afrom%20a%20previous%20snapshot%20by%20measuring%20the%20backward%20and%20forward%20transfer%20and%20the%0Aforgetting%20rate%20of%20the%20neural%20networks.%20Our%20extensive%20experiments%20on%20ten%20weekly%0Asnapshots%20of%20a%20web%20graph%20with%20over%20%24100%24M%20edges%2C%20sampled%20in%202012%20and%202022%2C%20show%0Athat%20all%20networks%20predominantly%20use%20%241%24-hop%20information%20to%20determine%20the%0Asummary%2C%20even%20when%20performing%20%242%24-hop%20summarization.%20Due%20to%20the%20heterogeneity%0Aof%20web%20graphs%2C%20in%20some%20snapshots%2C%20the%20%242%24-hop%20summary%20produces%20over%20ten%20times%0Amore%20vertex%20summaries%20than%20the%20%241%24-hop%20summary.%20When%20using%20the%20network%20trained%0Aon%20the%20last%20snapshot%20from%202012%20and%20applying%20it%20to%20the%20first%20snapshot%20of%202022%2C%0Awe%20observe%20a%20strong%20drop%20in%20accuracy.%20We%20attribute%20this%20drop%20over%20the%20ten-year%0Atime%20warp%20to%20the%20strongly%20increased%20heterogeneity%20of%20the%20web%20graph%20in%202022.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18042v1&entry.124074799=Read"},
{"title": "Guided Latent Slot Diffusion for Object-Centric Learning", "author": "Krishnakant Singh and Simone Schaub-Meyer and Stefan Roth", "abstract": "  Slot attention aims to decompose an input image into a set of meaningful\nobject files (slots). These latent object representations enable various\ndownstream tasks. Yet, these slots often bind to object parts, not objects\nthemselves, especially for real-world datasets. To address this, we introduce\nGuided Latent Slot Diffusion - GLASS, an object-centric model that uses\ngenerated captions as a guiding signal to better align slots with objects. Our\nkey insight is to learn the slot-attention module in the space of generated\nimages. This allows us to repurpose the pre-trained diffusion decoder model,\nwhich reconstructs the images from the slots, as a semantic mask generator\nbased on the generated captions. GLASS learns an object-level representation\nsuitable for multiple tasks simultaneously, e.g., segmentation, image\ngeneration, and property prediction, outperforming previous methods. For object\ndiscovery, GLASS achieves approx. a +35% and +10% relative improvement for mIoU\nover the previous state-of-the-art (SOTA) method on the VOC and COCO datasets,\nrespectively, and establishes a new SOTA FID score for conditional image\ngeneration amongst slot-attention-based methods. For the segmentation task,\nGLASS surpasses SOTA weakly-supervised and language-based segmentation models,\nwhich were specifically designed for the task.\n", "link": "http://arxiv.org/abs/2407.17929v1", "date": "2024-07-25", "relevancy": 2.2264, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5828}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5519}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guided%20Latent%20Slot%20Diffusion%20for%20Object-Centric%20Learning&body=Title%3A%20Guided%20Latent%20Slot%20Diffusion%20for%20Object-Centric%20Learning%0AAuthor%3A%20Krishnakant%20Singh%20and%20Simone%20Schaub-Meyer%20and%20Stefan%20Roth%0AAbstract%3A%20%20%20Slot%20attention%20aims%20to%20decompose%20an%20input%20image%20into%20a%20set%20of%20meaningful%0Aobject%20files%20%28slots%29.%20These%20latent%20object%20representations%20enable%20various%0Adownstream%20tasks.%20Yet%2C%20these%20slots%20often%20bind%20to%20object%20parts%2C%20not%20objects%0Athemselves%2C%20especially%20for%20real-world%20datasets.%20To%20address%20this%2C%20we%20introduce%0AGuided%20Latent%20Slot%20Diffusion%20-%20GLASS%2C%20an%20object-centric%20model%20that%20uses%0Agenerated%20captions%20as%20a%20guiding%20signal%20to%20better%20align%20slots%20with%20objects.%20Our%0Akey%20insight%20is%20to%20learn%20the%20slot-attention%20module%20in%20the%20space%20of%20generated%0Aimages.%20This%20allows%20us%20to%20repurpose%20the%20pre-trained%20diffusion%20decoder%20model%2C%0Awhich%20reconstructs%20the%20images%20from%20the%20slots%2C%20as%20a%20semantic%20mask%20generator%0Abased%20on%20the%20generated%20captions.%20GLASS%20learns%20an%20object-level%20representation%0Asuitable%20for%20multiple%20tasks%20simultaneously%2C%20e.g.%2C%20segmentation%2C%20image%0Ageneration%2C%20and%20property%20prediction%2C%20outperforming%20previous%20methods.%20For%20object%0Adiscovery%2C%20GLASS%20achieves%20approx.%20a%20%2B35%25%20and%20%2B10%25%20relative%20improvement%20for%20mIoU%0Aover%20the%20previous%20state-of-the-art%20%28SOTA%29%20method%20on%20the%20VOC%20and%20COCO%20datasets%2C%0Arespectively%2C%20and%20establishes%20a%20new%20SOTA%20FID%20score%20for%20conditional%20image%0Ageneration%20amongst%20slot-attention-based%20methods.%20For%20the%20segmentation%20task%2C%0AGLASS%20surpasses%20SOTA%20weakly-supervised%20and%20language-based%20segmentation%20models%2C%0Awhich%20were%20specifically%20designed%20for%20the%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17929v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuided%2520Latent%2520Slot%2520Diffusion%2520for%2520Object-Centric%2520Learning%26entry.906535625%3DKrishnakant%2520Singh%2520and%2520Simone%2520Schaub-Meyer%2520and%2520Stefan%2520Roth%26entry.1292438233%3D%2520%2520Slot%2520attention%2520aims%2520to%2520decompose%2520an%2520input%2520image%2520into%2520a%2520set%2520of%2520meaningful%250Aobject%2520files%2520%2528slots%2529.%2520These%2520latent%2520object%2520representations%2520enable%2520various%250Adownstream%2520tasks.%2520Yet%252C%2520these%2520slots%2520often%2520bind%2520to%2520object%2520parts%252C%2520not%2520objects%250Athemselves%252C%2520especially%2520for%2520real-world%2520datasets.%2520To%2520address%2520this%252C%2520we%2520introduce%250AGuided%2520Latent%2520Slot%2520Diffusion%2520-%2520GLASS%252C%2520an%2520object-centric%2520model%2520that%2520uses%250Agenerated%2520captions%2520as%2520a%2520guiding%2520signal%2520to%2520better%2520align%2520slots%2520with%2520objects.%2520Our%250Akey%2520insight%2520is%2520to%2520learn%2520the%2520slot-attention%2520module%2520in%2520the%2520space%2520of%2520generated%250Aimages.%2520This%2520allows%2520us%2520to%2520repurpose%2520the%2520pre-trained%2520diffusion%2520decoder%2520model%252C%250Awhich%2520reconstructs%2520the%2520images%2520from%2520the%2520slots%252C%2520as%2520a%2520semantic%2520mask%2520generator%250Abased%2520on%2520the%2520generated%2520captions.%2520GLASS%2520learns%2520an%2520object-level%2520representation%250Asuitable%2520for%2520multiple%2520tasks%2520simultaneously%252C%2520e.g.%252C%2520segmentation%252C%2520image%250Ageneration%252C%2520and%2520property%2520prediction%252C%2520outperforming%2520previous%2520methods.%2520For%2520object%250Adiscovery%252C%2520GLASS%2520achieves%2520approx.%2520a%2520%252B35%2525%2520and%2520%252B10%2525%2520relative%2520improvement%2520for%2520mIoU%250Aover%2520the%2520previous%2520state-of-the-art%2520%2528SOTA%2529%2520method%2520on%2520the%2520VOC%2520and%2520COCO%2520datasets%252C%250Arespectively%252C%2520and%2520establishes%2520a%2520new%2520SOTA%2520FID%2520score%2520for%2520conditional%2520image%250Ageneration%2520amongst%2520slot-attention-based%2520methods.%2520For%2520the%2520segmentation%2520task%252C%250AGLASS%2520surpasses%2520SOTA%2520weakly-supervised%2520and%2520language-based%2520segmentation%2520models%252C%250Awhich%2520were%2520specifically%2520designed%2520for%2520the%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17929v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guided%20Latent%20Slot%20Diffusion%20for%20Object-Centric%20Learning&entry.906535625=Krishnakant%20Singh%20and%20Simone%20Schaub-Meyer%20and%20Stefan%20Roth&entry.1292438233=%20%20Slot%20attention%20aims%20to%20decompose%20an%20input%20image%20into%20a%20set%20of%20meaningful%0Aobject%20files%20%28slots%29.%20These%20latent%20object%20representations%20enable%20various%0Adownstream%20tasks.%20Yet%2C%20these%20slots%20often%20bind%20to%20object%20parts%2C%20not%20objects%0Athemselves%2C%20especially%20for%20real-world%20datasets.%20To%20address%20this%2C%20we%20introduce%0AGuided%20Latent%20Slot%20Diffusion%20-%20GLASS%2C%20an%20object-centric%20model%20that%20uses%0Agenerated%20captions%20as%20a%20guiding%20signal%20to%20better%20align%20slots%20with%20objects.%20Our%0Akey%20insight%20is%20to%20learn%20the%20slot-attention%20module%20in%20the%20space%20of%20generated%0Aimages.%20This%20allows%20us%20to%20repurpose%20the%20pre-trained%20diffusion%20decoder%20model%2C%0Awhich%20reconstructs%20the%20images%20from%20the%20slots%2C%20as%20a%20semantic%20mask%20generator%0Abased%20on%20the%20generated%20captions.%20GLASS%20learns%20an%20object-level%20representation%0Asuitable%20for%20multiple%20tasks%20simultaneously%2C%20e.g.%2C%20segmentation%2C%20image%0Ageneration%2C%20and%20property%20prediction%2C%20outperforming%20previous%20methods.%20For%20object%0Adiscovery%2C%20GLASS%20achieves%20approx.%20a%20%2B35%25%20and%20%2B10%25%20relative%20improvement%20for%20mIoU%0Aover%20the%20previous%20state-of-the-art%20%28SOTA%29%20method%20on%20the%20VOC%20and%20COCO%20datasets%2C%0Arespectively%2C%20and%20establishes%20a%20new%20SOTA%20FID%20score%20for%20conditional%20image%0Ageneration%20amongst%20slot-attention-based%20methods.%20For%20the%20segmentation%20task%2C%0AGLASS%20surpasses%20SOTA%20weakly-supervised%20and%20language-based%20segmentation%20models%2C%0Awhich%20were%20specifically%20designed%20for%20the%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17929v1&entry.124074799=Read"},
{"title": "Multi-Agent Deep Reinforcement Learning for Resilience Optimization in\n  5G RAN", "author": "Soumeya Kaada and Dinh-Hieu Tran and Nguyen Van Huynh and Marie-Line Alberi Morel and Sofiene Jelassi and Gerardo Rubino", "abstract": "  Resilience is defined as the ability of a network to resist, adapt, and\nquickly recover from disruptions, and to continue to maintain an acceptable\nlevel of services from users' perspective. With the advent of future radio\nnetworks, including advanced 5G and upcoming 6G, critical services become\nintegral to future networks, requiring uninterrupted service delivery for end\nusers. Unfortunately, with the growing network complexity, user mobility and\ndiversity, it becomes challenging to scale current resilience management\ntechniques that rely on local optimizations to large dense network deployments.\nThis paper aims to address this problem by globally optimizing the resilience\nof a dense multi-cell network based on multi-agent deep reinforcement learning.\nSpecifically, our proposed solution can dynamically tilt cell antennas and\nreconfigure transmit power to mitigate outages and increase both coverage and\nservice availability. A multi-objective optimization problem is formulated to\nsimultaneously satisfy resiliency constraints while maximizing the service\nquality in the network area in order to minimize the impact of outages on\nneighbouring cells. Extensive simulations then demonstrate that with our\nproposed solution, the average service availability in terms of user throughput\ncan be increased by up to 50-60% on average, while reaching a coverage\navailability of 99% in best cases.\n", "link": "http://arxiv.org/abs/2407.18066v1", "date": "2024-07-25", "relevancy": 2.2223, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4602}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4372}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Agent%20Deep%20Reinforcement%20Learning%20for%20Resilience%20Optimization%20in%0A%20%205G%20RAN&body=Title%3A%20Multi-Agent%20Deep%20Reinforcement%20Learning%20for%20Resilience%20Optimization%20in%0A%20%205G%20RAN%0AAuthor%3A%20Soumeya%20Kaada%20and%20Dinh-Hieu%20Tran%20and%20Nguyen%20Van%20Huynh%20and%20Marie-Line%20Alberi%20Morel%20and%20Sofiene%20Jelassi%20and%20Gerardo%20Rubino%0AAbstract%3A%20%20%20Resilience%20is%20defined%20as%20the%20ability%20of%20a%20network%20to%20resist%2C%20adapt%2C%20and%0Aquickly%20recover%20from%20disruptions%2C%20and%20to%20continue%20to%20maintain%20an%20acceptable%0Alevel%20of%20services%20from%20users%27%20perspective.%20With%20the%20advent%20of%20future%20radio%0Anetworks%2C%20including%20advanced%205G%20and%20upcoming%206G%2C%20critical%20services%20become%0Aintegral%20to%20future%20networks%2C%20requiring%20uninterrupted%20service%20delivery%20for%20end%0Ausers.%20Unfortunately%2C%20with%20the%20growing%20network%20complexity%2C%20user%20mobility%20and%0Adiversity%2C%20it%20becomes%20challenging%20to%20scale%20current%20resilience%20management%0Atechniques%20that%20rely%20on%20local%20optimizations%20to%20large%20dense%20network%20deployments.%0AThis%20paper%20aims%20to%20address%20this%20problem%20by%20globally%20optimizing%20the%20resilience%0Aof%20a%20dense%20multi-cell%20network%20based%20on%20multi-agent%20deep%20reinforcement%20learning.%0ASpecifically%2C%20our%20proposed%20solution%20can%20dynamically%20tilt%20cell%20antennas%20and%0Areconfigure%20transmit%20power%20to%20mitigate%20outages%20and%20increase%20both%20coverage%20and%0Aservice%20availability.%20A%20multi-objective%20optimization%20problem%20is%20formulated%20to%0Asimultaneously%20satisfy%20resiliency%20constraints%20while%20maximizing%20the%20service%0Aquality%20in%20the%20network%20area%20in%20order%20to%20minimize%20the%20impact%20of%20outages%20on%0Aneighbouring%20cells.%20Extensive%20simulations%20then%20demonstrate%20that%20with%20our%0Aproposed%20solution%2C%20the%20average%20service%20availability%20in%20terms%20of%20user%20throughput%0Acan%20be%20increased%20by%20up%20to%2050-60%25%20on%20average%2C%20while%20reaching%20a%20coverage%0Aavailability%20of%2099%25%20in%20best%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Agent%2520Deep%2520Reinforcement%2520Learning%2520for%2520Resilience%2520Optimization%2520in%250A%2520%25205G%2520RAN%26entry.906535625%3DSoumeya%2520Kaada%2520and%2520Dinh-Hieu%2520Tran%2520and%2520Nguyen%2520Van%2520Huynh%2520and%2520Marie-Line%2520Alberi%2520Morel%2520and%2520Sofiene%2520Jelassi%2520and%2520Gerardo%2520Rubino%26entry.1292438233%3D%2520%2520Resilience%2520is%2520defined%2520as%2520the%2520ability%2520of%2520a%2520network%2520to%2520resist%252C%2520adapt%252C%2520and%250Aquickly%2520recover%2520from%2520disruptions%252C%2520and%2520to%2520continue%2520to%2520maintain%2520an%2520acceptable%250Alevel%2520of%2520services%2520from%2520users%2527%2520perspective.%2520With%2520the%2520advent%2520of%2520future%2520radio%250Anetworks%252C%2520including%2520advanced%25205G%2520and%2520upcoming%25206G%252C%2520critical%2520services%2520become%250Aintegral%2520to%2520future%2520networks%252C%2520requiring%2520uninterrupted%2520service%2520delivery%2520for%2520end%250Ausers.%2520Unfortunately%252C%2520with%2520the%2520growing%2520network%2520complexity%252C%2520user%2520mobility%2520and%250Adiversity%252C%2520it%2520becomes%2520challenging%2520to%2520scale%2520current%2520resilience%2520management%250Atechniques%2520that%2520rely%2520on%2520local%2520optimizations%2520to%2520large%2520dense%2520network%2520deployments.%250AThis%2520paper%2520aims%2520to%2520address%2520this%2520problem%2520by%2520globally%2520optimizing%2520the%2520resilience%250Aof%2520a%2520dense%2520multi-cell%2520network%2520based%2520on%2520multi-agent%2520deep%2520reinforcement%2520learning.%250ASpecifically%252C%2520our%2520proposed%2520solution%2520can%2520dynamically%2520tilt%2520cell%2520antennas%2520and%250Areconfigure%2520transmit%2520power%2520to%2520mitigate%2520outages%2520and%2520increase%2520both%2520coverage%2520and%250Aservice%2520availability.%2520A%2520multi-objective%2520optimization%2520problem%2520is%2520formulated%2520to%250Asimultaneously%2520satisfy%2520resiliency%2520constraints%2520while%2520maximizing%2520the%2520service%250Aquality%2520in%2520the%2520network%2520area%2520in%2520order%2520to%2520minimize%2520the%2520impact%2520of%2520outages%2520on%250Aneighbouring%2520cells.%2520Extensive%2520simulations%2520then%2520demonstrate%2520that%2520with%2520our%250Aproposed%2520solution%252C%2520the%2520average%2520service%2520availability%2520in%2520terms%2520of%2520user%2520throughput%250Acan%2520be%2520increased%2520by%2520up%2520to%252050-60%2525%2520on%2520average%252C%2520while%2520reaching%2520a%2520coverage%250Aavailability%2520of%252099%2525%2520in%2520best%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Agent%20Deep%20Reinforcement%20Learning%20for%20Resilience%20Optimization%20in%0A%20%205G%20RAN&entry.906535625=Soumeya%20Kaada%20and%20Dinh-Hieu%20Tran%20and%20Nguyen%20Van%20Huynh%20and%20Marie-Line%20Alberi%20Morel%20and%20Sofiene%20Jelassi%20and%20Gerardo%20Rubino&entry.1292438233=%20%20Resilience%20is%20defined%20as%20the%20ability%20of%20a%20network%20to%20resist%2C%20adapt%2C%20and%0Aquickly%20recover%20from%20disruptions%2C%20and%20to%20continue%20to%20maintain%20an%20acceptable%0Alevel%20of%20services%20from%20users%27%20perspective.%20With%20the%20advent%20of%20future%20radio%0Anetworks%2C%20including%20advanced%205G%20and%20upcoming%206G%2C%20critical%20services%20become%0Aintegral%20to%20future%20networks%2C%20requiring%20uninterrupted%20service%20delivery%20for%20end%0Ausers.%20Unfortunately%2C%20with%20the%20growing%20network%20complexity%2C%20user%20mobility%20and%0Adiversity%2C%20it%20becomes%20challenging%20to%20scale%20current%20resilience%20management%0Atechniques%20that%20rely%20on%20local%20optimizations%20to%20large%20dense%20network%20deployments.%0AThis%20paper%20aims%20to%20address%20this%20problem%20by%20globally%20optimizing%20the%20resilience%0Aof%20a%20dense%20multi-cell%20network%20based%20on%20multi-agent%20deep%20reinforcement%20learning.%0ASpecifically%2C%20our%20proposed%20solution%20can%20dynamically%20tilt%20cell%20antennas%20and%0Areconfigure%20transmit%20power%20to%20mitigate%20outages%20and%20increase%20both%20coverage%20and%0Aservice%20availability.%20A%20multi-objective%20optimization%20problem%20is%20formulated%20to%0Asimultaneously%20satisfy%20resiliency%20constraints%20while%20maximizing%20the%20service%0Aquality%20in%20the%20network%20area%20in%20order%20to%20minimize%20the%20impact%20of%20outages%20on%0Aneighbouring%20cells.%20Extensive%20simulations%20then%20demonstrate%20that%20with%20our%0Aproposed%20solution%2C%20the%20average%20service%20availability%20in%20terms%20of%20user%20throughput%0Acan%20be%20increased%20by%20up%20to%2050-60%25%20on%20average%2C%20while%20reaching%20a%20coverage%0Aavailability%20of%2099%25%20in%20best%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18066v1&entry.124074799=Read"},
{"title": "YOCO: You Only Calibrate Once for Accurate Extrinsic Parameter in\n  LiDAR-Camera Systems", "author": "Tianle Zeng and Dengke He and Feifan Yan and Meixi He", "abstract": "  In a multi-sensor fusion system composed of cameras and LiDAR, precise\nextrinsic calibration contributes to the system's long-term stability and\naccurate perception of the environment. However, methods based on extracting\nand registering corresponding points still face challenges in terms of\nautomation and precision. This paper proposes a novel fully automatic extrinsic\ncalibration method for LiDAR-camera systems that circumvents the need for\ncorresponding point registration. In our approach, a novel algorithm to extract\nrequired LiDAR correspondence point is proposed. This method can effectively\nfilter out irrelevant points by computing the orientation of plane point clouds\nand extracting points by applying distance- and density-based thresholds. We\navoid the need for corresponding point registration by introducing extrinsic\nparameters between the LiDAR and camera into the projection of extracted points\nand constructing co-planar constraints. These parameters are then optimized to\nsolve for the extrinsic. We validated our method across multiple sets of\nLiDAR-camera systems. In synthetic experiments, our method demonstrates\nsuperior performance compared to current calibration techniques. Real-world\ndata experiments further confirm the precision and robustness of the proposed\nalgorithm, with average rotation and translation calibration errors between\nLiDAR and camera of less than 0.05 degree and 0.015m, respectively. This method\nenables automatic and accurate extrinsic calibration in a single one step,\nemphasizing the potential of calibration algorithms beyond using corresponding\npoint registration to enhance the automation and precision of LiDAR-camera\nsystem calibration.\n", "link": "http://arxiv.org/abs/2407.18043v1", "date": "2024-07-25", "relevancy": 2.2182, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.567}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5605}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YOCO%3A%20You%20Only%20Calibrate%20Once%20for%20Accurate%20Extrinsic%20Parameter%20in%0A%20%20LiDAR-Camera%20Systems&body=Title%3A%20YOCO%3A%20You%20Only%20Calibrate%20Once%20for%20Accurate%20Extrinsic%20Parameter%20in%0A%20%20LiDAR-Camera%20Systems%0AAuthor%3A%20Tianle%20Zeng%20and%20Dengke%20He%20and%20Feifan%20Yan%20and%20Meixi%20He%0AAbstract%3A%20%20%20In%20a%20multi-sensor%20fusion%20system%20composed%20of%20cameras%20and%20LiDAR%2C%20precise%0Aextrinsic%20calibration%20contributes%20to%20the%20system%27s%20long-term%20stability%20and%0Aaccurate%20perception%20of%20the%20environment.%20However%2C%20methods%20based%20on%20extracting%0Aand%20registering%20corresponding%20points%20still%20face%20challenges%20in%20terms%20of%0Aautomation%20and%20precision.%20This%20paper%20proposes%20a%20novel%20fully%20automatic%20extrinsic%0Acalibration%20method%20for%20LiDAR-camera%20systems%20that%20circumvents%20the%20need%20for%0Acorresponding%20point%20registration.%20In%20our%20approach%2C%20a%20novel%20algorithm%20to%20extract%0Arequired%20LiDAR%20correspondence%20point%20is%20proposed.%20This%20method%20can%20effectively%0Afilter%20out%20irrelevant%20points%20by%20computing%20the%20orientation%20of%20plane%20point%20clouds%0Aand%20extracting%20points%20by%20applying%20distance-%20and%20density-based%20thresholds.%20We%0Aavoid%20the%20need%20for%20corresponding%20point%20registration%20by%20introducing%20extrinsic%0Aparameters%20between%20the%20LiDAR%20and%20camera%20into%20the%20projection%20of%20extracted%20points%0Aand%20constructing%20co-planar%20constraints.%20These%20parameters%20are%20then%20optimized%20to%0Asolve%20for%20the%20extrinsic.%20We%20validated%20our%20method%20across%20multiple%20sets%20of%0ALiDAR-camera%20systems.%20In%20synthetic%20experiments%2C%20our%20method%20demonstrates%0Asuperior%20performance%20compared%20to%20current%20calibration%20techniques.%20Real-world%0Adata%20experiments%20further%20confirm%20the%20precision%20and%20robustness%20of%20the%20proposed%0Aalgorithm%2C%20with%20average%20rotation%20and%20translation%20calibration%20errors%20between%0ALiDAR%20and%20camera%20of%20less%20than%200.05%20degree%20and%200.015m%2C%20respectively.%20This%20method%0Aenables%20automatic%20and%20accurate%20extrinsic%20calibration%20in%20a%20single%20one%20step%2C%0Aemphasizing%20the%20potential%20of%20calibration%20algorithms%20beyond%20using%20corresponding%0Apoint%20registration%20to%20enhance%20the%20automation%20and%20precision%20of%20LiDAR-camera%0Asystem%20calibration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18043v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYOCO%253A%2520You%2520Only%2520Calibrate%2520Once%2520for%2520Accurate%2520Extrinsic%2520Parameter%2520in%250A%2520%2520LiDAR-Camera%2520Systems%26entry.906535625%3DTianle%2520Zeng%2520and%2520Dengke%2520He%2520and%2520Feifan%2520Yan%2520and%2520Meixi%2520He%26entry.1292438233%3D%2520%2520In%2520a%2520multi-sensor%2520fusion%2520system%2520composed%2520of%2520cameras%2520and%2520LiDAR%252C%2520precise%250Aextrinsic%2520calibration%2520contributes%2520to%2520the%2520system%2527s%2520long-term%2520stability%2520and%250Aaccurate%2520perception%2520of%2520the%2520environment.%2520However%252C%2520methods%2520based%2520on%2520extracting%250Aand%2520registering%2520corresponding%2520points%2520still%2520face%2520challenges%2520in%2520terms%2520of%250Aautomation%2520and%2520precision.%2520This%2520paper%2520proposes%2520a%2520novel%2520fully%2520automatic%2520extrinsic%250Acalibration%2520method%2520for%2520LiDAR-camera%2520systems%2520that%2520circumvents%2520the%2520need%2520for%250Acorresponding%2520point%2520registration.%2520In%2520our%2520approach%252C%2520a%2520novel%2520algorithm%2520to%2520extract%250Arequired%2520LiDAR%2520correspondence%2520point%2520is%2520proposed.%2520This%2520method%2520can%2520effectively%250Afilter%2520out%2520irrelevant%2520points%2520by%2520computing%2520the%2520orientation%2520of%2520plane%2520point%2520clouds%250Aand%2520extracting%2520points%2520by%2520applying%2520distance-%2520and%2520density-based%2520thresholds.%2520We%250Aavoid%2520the%2520need%2520for%2520corresponding%2520point%2520registration%2520by%2520introducing%2520extrinsic%250Aparameters%2520between%2520the%2520LiDAR%2520and%2520camera%2520into%2520the%2520projection%2520of%2520extracted%2520points%250Aand%2520constructing%2520co-planar%2520constraints.%2520These%2520parameters%2520are%2520then%2520optimized%2520to%250Asolve%2520for%2520the%2520extrinsic.%2520We%2520validated%2520our%2520method%2520across%2520multiple%2520sets%2520of%250ALiDAR-camera%2520systems.%2520In%2520synthetic%2520experiments%252C%2520our%2520method%2520demonstrates%250Asuperior%2520performance%2520compared%2520to%2520current%2520calibration%2520techniques.%2520Real-world%250Adata%2520experiments%2520further%2520confirm%2520the%2520precision%2520and%2520robustness%2520of%2520the%2520proposed%250Aalgorithm%252C%2520with%2520average%2520rotation%2520and%2520translation%2520calibration%2520errors%2520between%250ALiDAR%2520and%2520camera%2520of%2520less%2520than%25200.05%2520degree%2520and%25200.015m%252C%2520respectively.%2520This%2520method%250Aenables%2520automatic%2520and%2520accurate%2520extrinsic%2520calibration%2520in%2520a%2520single%2520one%2520step%252C%250Aemphasizing%2520the%2520potential%2520of%2520calibration%2520algorithms%2520beyond%2520using%2520corresponding%250Apoint%2520registration%2520to%2520enhance%2520the%2520automation%2520and%2520precision%2520of%2520LiDAR-camera%250Asystem%2520calibration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18043v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YOCO%3A%20You%20Only%20Calibrate%20Once%20for%20Accurate%20Extrinsic%20Parameter%20in%0A%20%20LiDAR-Camera%20Systems&entry.906535625=Tianle%20Zeng%20and%20Dengke%20He%20and%20Feifan%20Yan%20and%20Meixi%20He&entry.1292438233=%20%20In%20a%20multi-sensor%20fusion%20system%20composed%20of%20cameras%20and%20LiDAR%2C%20precise%0Aextrinsic%20calibration%20contributes%20to%20the%20system%27s%20long-term%20stability%20and%0Aaccurate%20perception%20of%20the%20environment.%20However%2C%20methods%20based%20on%20extracting%0Aand%20registering%20corresponding%20points%20still%20face%20challenges%20in%20terms%20of%0Aautomation%20and%20precision.%20This%20paper%20proposes%20a%20novel%20fully%20automatic%20extrinsic%0Acalibration%20method%20for%20LiDAR-camera%20systems%20that%20circumvents%20the%20need%20for%0Acorresponding%20point%20registration.%20In%20our%20approach%2C%20a%20novel%20algorithm%20to%20extract%0Arequired%20LiDAR%20correspondence%20point%20is%20proposed.%20This%20method%20can%20effectively%0Afilter%20out%20irrelevant%20points%20by%20computing%20the%20orientation%20of%20plane%20point%20clouds%0Aand%20extracting%20points%20by%20applying%20distance-%20and%20density-based%20thresholds.%20We%0Aavoid%20the%20need%20for%20corresponding%20point%20registration%20by%20introducing%20extrinsic%0Aparameters%20between%20the%20LiDAR%20and%20camera%20into%20the%20projection%20of%20extracted%20points%0Aand%20constructing%20co-planar%20constraints.%20These%20parameters%20are%20then%20optimized%20to%0Asolve%20for%20the%20extrinsic.%20We%20validated%20our%20method%20across%20multiple%20sets%20of%0ALiDAR-camera%20systems.%20In%20synthetic%20experiments%2C%20our%20method%20demonstrates%0Asuperior%20performance%20compared%20to%20current%20calibration%20techniques.%20Real-world%0Adata%20experiments%20further%20confirm%20the%20precision%20and%20robustness%20of%20the%20proposed%0Aalgorithm%2C%20with%20average%20rotation%20and%20translation%20calibration%20errors%20between%0ALiDAR%20and%20camera%20of%20less%20than%200.05%20degree%20and%200.015m%2C%20respectively.%20This%20method%0Aenables%20automatic%20and%20accurate%20extrinsic%20calibration%20in%20a%20single%20one%20step%2C%0Aemphasizing%20the%20potential%20of%20calibration%20algorithms%20beyond%20using%20corresponding%0Apoint%20registration%20to%20enhance%20the%20automation%20and%20precision%20of%20LiDAR-camera%0Asystem%20calibration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18043v1&entry.124074799=Read"},
{"title": "Self-supervised pre-training with diffusion model for few-shot landmark\n  detection in x-ray images", "author": "Roberto Di Via and Francesca Odone and Vito Paolo Pastore", "abstract": "  In the last few years, deep neural networks have been extensively applied in\nthe medical domain for different tasks, ranging from image classification and\nsegmentation to landmark detection. However, the application of these\ntechnologies in the medical domain is often hindered by data scarcity, both in\nterms of available annotations and images. This study introduces a new\nself-supervised pre-training protocol based on diffusion models for landmark\ndetection in x-ray images. Our results show that the proposed self-supervised\nframework can provide accurate landmark detection with a minimal number of\navailable annotated training images (up to 50), outperforming ImageNet\nsupervised pre-training and state-of-the-art self-supervised pre-trainings for\nthree popular x-ray benchmark datasets. To our knowledge, this is the first\nexploration of diffusion models for self-supervised learning in landmark\ndetection, which may offer a valuable pre-training approach in few-shot\nregimes, for mitigating data scarcity.\n", "link": "http://arxiv.org/abs/2407.18125v1", "date": "2024-07-25", "relevancy": 2.2134, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.601}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5531}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20pre-training%20with%20diffusion%20model%20for%20few-shot%20landmark%0A%20%20detection%20in%20x-ray%20images&body=Title%3A%20Self-supervised%20pre-training%20with%20diffusion%20model%20for%20few-shot%20landmark%0A%20%20detection%20in%20x-ray%20images%0AAuthor%3A%20Roberto%20Di%20Via%20and%20Francesca%20Odone%20and%20Vito%20Paolo%20Pastore%0AAbstract%3A%20%20%20In%20the%20last%20few%20years%2C%20deep%20neural%20networks%20have%20been%20extensively%20applied%20in%0Athe%20medical%20domain%20for%20different%20tasks%2C%20ranging%20from%20image%20classification%20and%0Asegmentation%20to%20landmark%20detection.%20However%2C%20the%20application%20of%20these%0Atechnologies%20in%20the%20medical%20domain%20is%20often%20hindered%20by%20data%20scarcity%2C%20both%20in%0Aterms%20of%20available%20annotations%20and%20images.%20This%20study%20introduces%20a%20new%0Aself-supervised%20pre-training%20protocol%20based%20on%20diffusion%20models%20for%20landmark%0Adetection%20in%20x-ray%20images.%20Our%20results%20show%20that%20the%20proposed%20self-supervised%0Aframework%20can%20provide%20accurate%20landmark%20detection%20with%20a%20minimal%20number%20of%0Aavailable%20annotated%20training%20images%20%28up%20to%2050%29%2C%20outperforming%20ImageNet%0Asupervised%20pre-training%20and%20state-of-the-art%20self-supervised%20pre-trainings%20for%0Athree%20popular%20x-ray%20benchmark%20datasets.%20To%20our%20knowledge%2C%20this%20is%20the%20first%0Aexploration%20of%20diffusion%20models%20for%20self-supervised%20learning%20in%20landmark%0Adetection%2C%20which%20may%20offer%20a%20valuable%20pre-training%20approach%20in%20few-shot%0Aregimes%2C%20for%20mitigating%20data%20scarcity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-supervised%2520pre-training%2520with%2520diffusion%2520model%2520for%2520few-shot%2520landmark%250A%2520%2520detection%2520in%2520x-ray%2520images%26entry.906535625%3DRoberto%2520Di%2520Via%2520and%2520Francesca%2520Odone%2520and%2520Vito%2520Paolo%2520Pastore%26entry.1292438233%3D%2520%2520In%2520the%2520last%2520few%2520years%252C%2520deep%2520neural%2520networks%2520have%2520been%2520extensively%2520applied%2520in%250Athe%2520medical%2520domain%2520for%2520different%2520tasks%252C%2520ranging%2520from%2520image%2520classification%2520and%250Asegmentation%2520to%2520landmark%2520detection.%2520However%252C%2520the%2520application%2520of%2520these%250Atechnologies%2520in%2520the%2520medical%2520domain%2520is%2520often%2520hindered%2520by%2520data%2520scarcity%252C%2520both%2520in%250Aterms%2520of%2520available%2520annotations%2520and%2520images.%2520This%2520study%2520introduces%2520a%2520new%250Aself-supervised%2520pre-training%2520protocol%2520based%2520on%2520diffusion%2520models%2520for%2520landmark%250Adetection%2520in%2520x-ray%2520images.%2520Our%2520results%2520show%2520that%2520the%2520proposed%2520self-supervised%250Aframework%2520can%2520provide%2520accurate%2520landmark%2520detection%2520with%2520a%2520minimal%2520number%2520of%250Aavailable%2520annotated%2520training%2520images%2520%2528up%2520to%252050%2529%252C%2520outperforming%2520ImageNet%250Asupervised%2520pre-training%2520and%2520state-of-the-art%2520self-supervised%2520pre-trainings%2520for%250Athree%2520popular%2520x-ray%2520benchmark%2520datasets.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%250Aexploration%2520of%2520diffusion%2520models%2520for%2520self-supervised%2520learning%2520in%2520landmark%250Adetection%252C%2520which%2520may%2520offer%2520a%2520valuable%2520pre-training%2520approach%2520in%2520few-shot%250Aregimes%252C%2520for%2520mitigating%2520data%2520scarcity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20pre-training%20with%20diffusion%20model%20for%20few-shot%20landmark%0A%20%20detection%20in%20x-ray%20images&entry.906535625=Roberto%20Di%20Via%20and%20Francesca%20Odone%20and%20Vito%20Paolo%20Pastore&entry.1292438233=%20%20In%20the%20last%20few%20years%2C%20deep%20neural%20networks%20have%20been%20extensively%20applied%20in%0Athe%20medical%20domain%20for%20different%20tasks%2C%20ranging%20from%20image%20classification%20and%0Asegmentation%20to%20landmark%20detection.%20However%2C%20the%20application%20of%20these%0Atechnologies%20in%20the%20medical%20domain%20is%20often%20hindered%20by%20data%20scarcity%2C%20both%20in%0Aterms%20of%20available%20annotations%20and%20images.%20This%20study%20introduces%20a%20new%0Aself-supervised%20pre-training%20protocol%20based%20on%20diffusion%20models%20for%20landmark%0Adetection%20in%20x-ray%20images.%20Our%20results%20show%20that%20the%20proposed%20self-supervised%0Aframework%20can%20provide%20accurate%20landmark%20detection%20with%20a%20minimal%20number%20of%0Aavailable%20annotated%20training%20images%20%28up%20to%2050%29%2C%20outperforming%20ImageNet%0Asupervised%20pre-training%20and%20state-of-the-art%20self-supervised%20pre-trainings%20for%0Athree%20popular%20x-ray%20benchmark%20datasets.%20To%20our%20knowledge%2C%20this%20is%20the%20first%0Aexploration%20of%20diffusion%20models%20for%20self-supervised%20learning%20in%20landmark%0Adetection%2C%20which%20may%20offer%20a%20valuable%20pre-training%20approach%20in%20few-shot%0Aregimes%2C%20for%20mitigating%20data%20scarcity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18125v1&entry.124074799=Read"},
{"title": "Auto-Vocabulary Segmentation for LiDAR Points", "author": "Weijie Wei and Osman \u00dclger and Fatemeh Karimi Nejadasl and Theo Gevers and Martin R. Oswald", "abstract": "  Existing perception methods for autonomous driving fall short of recognizing\nunknown entities not covered in the training data. Open-vocabulary methods\noffer promising capabilities in detecting any object but are limited by\nuser-specified queries representing target classes. We propose AutoVoc3D, a\nframework for automatic object class recognition and open-ended segmentation.\nEvaluation on nuScenes showcases AutoVoc3D's ability to generate precise\nsemantic classes and accurate point-wise segmentation. Moreover, we introduce\nText-Point Semantic Similarity, a new metric to assess the semantic similarity\nbetween text and point cloud without eliminating novel classes.\n", "link": "http://arxiv.org/abs/2406.09126v2", "date": "2024-07-25", "relevancy": 2.2006, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5646}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5433}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auto-Vocabulary%20Segmentation%20for%20LiDAR%20Points&body=Title%3A%20Auto-Vocabulary%20Segmentation%20for%20LiDAR%20Points%0AAuthor%3A%20Weijie%20Wei%20and%20Osman%20%C3%9Clger%20and%20Fatemeh%20Karimi%20Nejadasl%20and%20Theo%20Gevers%20and%20Martin%20R.%20Oswald%0AAbstract%3A%20%20%20Existing%20perception%20methods%20for%20autonomous%20driving%20fall%20short%20of%20recognizing%0Aunknown%20entities%20not%20covered%20in%20the%20training%20data.%20Open-vocabulary%20methods%0Aoffer%20promising%20capabilities%20in%20detecting%20any%20object%20but%20are%20limited%20by%0Auser-specified%20queries%20representing%20target%20classes.%20We%20propose%20AutoVoc3D%2C%20a%0Aframework%20for%20automatic%20object%20class%20recognition%20and%20open-ended%20segmentation.%0AEvaluation%20on%20nuScenes%20showcases%20AutoVoc3D%27s%20ability%20to%20generate%20precise%0Asemantic%20classes%20and%20accurate%20point-wise%20segmentation.%20Moreover%2C%20we%20introduce%0AText-Point%20Semantic%20Similarity%2C%20a%20new%20metric%20to%20assess%20the%20semantic%20similarity%0Abetween%20text%20and%20point%20cloud%20without%20eliminating%20novel%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09126v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuto-Vocabulary%2520Segmentation%2520for%2520LiDAR%2520Points%26entry.906535625%3DWeijie%2520Wei%2520and%2520Osman%2520%25C3%259Clger%2520and%2520Fatemeh%2520Karimi%2520Nejadasl%2520and%2520Theo%2520Gevers%2520and%2520Martin%2520R.%2520Oswald%26entry.1292438233%3D%2520%2520Existing%2520perception%2520methods%2520for%2520autonomous%2520driving%2520fall%2520short%2520of%2520recognizing%250Aunknown%2520entities%2520not%2520covered%2520in%2520the%2520training%2520data.%2520Open-vocabulary%2520methods%250Aoffer%2520promising%2520capabilities%2520in%2520detecting%2520any%2520object%2520but%2520are%2520limited%2520by%250Auser-specified%2520queries%2520representing%2520target%2520classes.%2520We%2520propose%2520AutoVoc3D%252C%2520a%250Aframework%2520for%2520automatic%2520object%2520class%2520recognition%2520and%2520open-ended%2520segmentation.%250AEvaluation%2520on%2520nuScenes%2520showcases%2520AutoVoc3D%2527s%2520ability%2520to%2520generate%2520precise%250Asemantic%2520classes%2520and%2520accurate%2520point-wise%2520segmentation.%2520Moreover%252C%2520we%2520introduce%250AText-Point%2520Semantic%2520Similarity%252C%2520a%2520new%2520metric%2520to%2520assess%2520the%2520semantic%2520similarity%250Abetween%2520text%2520and%2520point%2520cloud%2520without%2520eliminating%2520novel%2520classes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09126v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auto-Vocabulary%20Segmentation%20for%20LiDAR%20Points&entry.906535625=Weijie%20Wei%20and%20Osman%20%C3%9Clger%20and%20Fatemeh%20Karimi%20Nejadasl%20and%20Theo%20Gevers%20and%20Martin%20R.%20Oswald&entry.1292438233=%20%20Existing%20perception%20methods%20for%20autonomous%20driving%20fall%20short%20of%20recognizing%0Aunknown%20entities%20not%20covered%20in%20the%20training%20data.%20Open-vocabulary%20methods%0Aoffer%20promising%20capabilities%20in%20detecting%20any%20object%20but%20are%20limited%20by%0Auser-specified%20queries%20representing%20target%20classes.%20We%20propose%20AutoVoc3D%2C%20a%0Aframework%20for%20automatic%20object%20class%20recognition%20and%20open-ended%20segmentation.%0AEvaluation%20on%20nuScenes%20showcases%20AutoVoc3D%27s%20ability%20to%20generate%20precise%0Asemantic%20classes%20and%20accurate%20point-wise%20segmentation.%20Moreover%2C%20we%20introduce%0AText-Point%20Semantic%20Similarity%2C%20a%20new%20metric%20to%20assess%20the%20semantic%20similarity%0Abetween%20text%20and%20point%20cloud%20without%20eliminating%20novel%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09126v2&entry.124074799=Read"},
{"title": "Pose, Velocity and Landmark Position Estimation Using IMU and Bearing\n  Measurements", "author": "Miaomiao Wang and Abdelhamid Tayebi", "abstract": "  This paper investigates the estimation problem of the pose (orientation and\nposition) and linear velocity of a rigid body, as well as the landmark\npositions, using an inertial measurement unit (IMU) and a monocular camera.\nFirst, we propose a globally exponentially stable (GES) linear time-varying\n(LTV) observer for the estimation of body-frame landmark positions and\nvelocity, using IMU and monocular bearing measurements. Thereafter, using the\ngyro measurements, some landmarks known in the inertial frame and the estimates\nfrom the LTV observer, we propose a nonlinear pose observer on $\\SO(3)\\times\n\\mathbb{R}^3$. The overall estimation system is shown to be almost globally\nasymptotically stable (AGAS) using the notion of almost global input-to-state\nstability (ISS). Interestingly, we show that with the knowledge (in the\ninertial frame) of a small number of landmarks, we can recover (under some\nconditions) the unknown positions (in the inertial frame) of a large number of\nlandmarks. Numerical simulation results are presented to illustrate the\nperformance of the proposed estimation scheme.\n", "link": "http://arxiv.org/abs/2407.18099v1", "date": "2024-07-25", "relevancy": 2.1853, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5667}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5662}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pose%2C%20Velocity%20and%20Landmark%20Position%20Estimation%20Using%20IMU%20and%20Bearing%0A%20%20Measurements&body=Title%3A%20Pose%2C%20Velocity%20and%20Landmark%20Position%20Estimation%20Using%20IMU%20and%20Bearing%0A%20%20Measurements%0AAuthor%3A%20Miaomiao%20Wang%20and%20Abdelhamid%20Tayebi%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20estimation%20problem%20of%20the%20pose%20%28orientation%20and%0Aposition%29%20and%20linear%20velocity%20of%20a%20rigid%20body%2C%20as%20well%20as%20the%20landmark%0Apositions%2C%20using%20an%20inertial%20measurement%20unit%20%28IMU%29%20and%20a%20monocular%20camera.%0AFirst%2C%20we%20propose%20a%20globally%20exponentially%20stable%20%28GES%29%20linear%20time-varying%0A%28LTV%29%20observer%20for%20the%20estimation%20of%20body-frame%20landmark%20positions%20and%0Avelocity%2C%20using%20IMU%20and%20monocular%20bearing%20measurements.%20Thereafter%2C%20using%20the%0Agyro%20measurements%2C%20some%20landmarks%20known%20in%20the%20inertial%20frame%20and%20the%20estimates%0Afrom%20the%20LTV%20observer%2C%20we%20propose%20a%20nonlinear%20pose%20observer%20on%20%24%5CSO%283%29%5Ctimes%0A%5Cmathbb%7BR%7D%5E3%24.%20The%20overall%20estimation%20system%20is%20shown%20to%20be%20almost%20globally%0Aasymptotically%20stable%20%28AGAS%29%20using%20the%20notion%20of%20almost%20global%20input-to-state%0Astability%20%28ISS%29.%20Interestingly%2C%20we%20show%20that%20with%20the%20knowledge%20%28in%20the%0Ainertial%20frame%29%20of%20a%20small%20number%20of%20landmarks%2C%20we%20can%20recover%20%28under%20some%0Aconditions%29%20the%20unknown%20positions%20%28in%20the%20inertial%20frame%29%20of%20a%20large%20number%20of%0Alandmarks.%20Numerical%20simulation%20results%20are%20presented%20to%20illustrate%20the%0Aperformance%20of%20the%20proposed%20estimation%20scheme.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18099v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPose%252C%2520Velocity%2520and%2520Landmark%2520Position%2520Estimation%2520Using%2520IMU%2520and%2520Bearing%250A%2520%2520Measurements%26entry.906535625%3DMiaomiao%2520Wang%2520and%2520Abdelhamid%2520Tayebi%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520estimation%2520problem%2520of%2520the%2520pose%2520%2528orientation%2520and%250Aposition%2529%2520and%2520linear%2520velocity%2520of%2520a%2520rigid%2520body%252C%2520as%2520well%2520as%2520the%2520landmark%250Apositions%252C%2520using%2520an%2520inertial%2520measurement%2520unit%2520%2528IMU%2529%2520and%2520a%2520monocular%2520camera.%250AFirst%252C%2520we%2520propose%2520a%2520globally%2520exponentially%2520stable%2520%2528GES%2529%2520linear%2520time-varying%250A%2528LTV%2529%2520observer%2520for%2520the%2520estimation%2520of%2520body-frame%2520landmark%2520positions%2520and%250Avelocity%252C%2520using%2520IMU%2520and%2520monocular%2520bearing%2520measurements.%2520Thereafter%252C%2520using%2520the%250Agyro%2520measurements%252C%2520some%2520landmarks%2520known%2520in%2520the%2520inertial%2520frame%2520and%2520the%2520estimates%250Afrom%2520the%2520LTV%2520observer%252C%2520we%2520propose%2520a%2520nonlinear%2520pose%2520observer%2520on%2520%2524%255CSO%25283%2529%255Ctimes%250A%255Cmathbb%257BR%257D%255E3%2524.%2520The%2520overall%2520estimation%2520system%2520is%2520shown%2520to%2520be%2520almost%2520globally%250Aasymptotically%2520stable%2520%2528AGAS%2529%2520using%2520the%2520notion%2520of%2520almost%2520global%2520input-to-state%250Astability%2520%2528ISS%2529.%2520Interestingly%252C%2520we%2520show%2520that%2520with%2520the%2520knowledge%2520%2528in%2520the%250Ainertial%2520frame%2529%2520of%2520a%2520small%2520number%2520of%2520landmarks%252C%2520we%2520can%2520recover%2520%2528under%2520some%250Aconditions%2529%2520the%2520unknown%2520positions%2520%2528in%2520the%2520inertial%2520frame%2529%2520of%2520a%2520large%2520number%2520of%250Alandmarks.%2520Numerical%2520simulation%2520results%2520are%2520presented%2520to%2520illustrate%2520the%250Aperformance%2520of%2520the%2520proposed%2520estimation%2520scheme.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18099v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pose%2C%20Velocity%20and%20Landmark%20Position%20Estimation%20Using%20IMU%20and%20Bearing%0A%20%20Measurements&entry.906535625=Miaomiao%20Wang%20and%20Abdelhamid%20Tayebi&entry.1292438233=%20%20This%20paper%20investigates%20the%20estimation%20problem%20of%20the%20pose%20%28orientation%20and%0Aposition%29%20and%20linear%20velocity%20of%20a%20rigid%20body%2C%20as%20well%20as%20the%20landmark%0Apositions%2C%20using%20an%20inertial%20measurement%20unit%20%28IMU%29%20and%20a%20monocular%20camera.%0AFirst%2C%20we%20propose%20a%20globally%20exponentially%20stable%20%28GES%29%20linear%20time-varying%0A%28LTV%29%20observer%20for%20the%20estimation%20of%20body-frame%20landmark%20positions%20and%0Avelocity%2C%20using%20IMU%20and%20monocular%20bearing%20measurements.%20Thereafter%2C%20using%20the%0Agyro%20measurements%2C%20some%20landmarks%20known%20in%20the%20inertial%20frame%20and%20the%20estimates%0Afrom%20the%20LTV%20observer%2C%20we%20propose%20a%20nonlinear%20pose%20observer%20on%20%24%5CSO%283%29%5Ctimes%0A%5Cmathbb%7BR%7D%5E3%24.%20The%20overall%20estimation%20system%20is%20shown%20to%20be%20almost%20globally%0Aasymptotically%20stable%20%28AGAS%29%20using%20the%20notion%20of%20almost%20global%20input-to-state%0Astability%20%28ISS%29.%20Interestingly%2C%20we%20show%20that%20with%20the%20knowledge%20%28in%20the%0Ainertial%20frame%29%20of%20a%20small%20number%20of%20landmarks%2C%20we%20can%20recover%20%28under%20some%0Aconditions%29%20the%20unknown%20positions%20%28in%20the%20inertial%20frame%29%20of%20a%20large%20number%20of%0Alandmarks.%20Numerical%20simulation%20results%20are%20presented%20to%20illustrate%20the%0Aperformance%20of%20the%20proposed%20estimation%20scheme.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18099v1&entry.124074799=Read"},
{"title": "Trajectory-aligned Space-time Tokens for Few-shot Action Recognition", "author": "Pulkit Kumar and Namitha Padmanabhan and Luke Luo and Sai Saketh Rambhatla and Abhinav Shrivastava", "abstract": "  We propose a simple yet effective approach for few-shot action recognition,\nemphasizing the disentanglement of motion and appearance representations. By\nharnessing recent progress in tracking, specifically point trajectories and\nself-supervised representation learning, we build trajectory-aligned tokens\n(TATs) that capture motion and appearance information. This approach\nsignificantly reduces the data requirements while retaining essential\ninformation. To process these representations, we use a Masked Space-time\nTransformer that effectively learns to aggregate information to facilitate\nfew-shot action recognition. We demonstrate state-of-the-art results on\nfew-shot action recognition across multiple datasets. Our project page is\navailable at https://www.cs.umd.edu/~pulkit/tats\n", "link": "http://arxiv.org/abs/2407.18249v1", "date": "2024-07-25", "relevancy": 2.1846, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5645}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.563}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Trajectory-aligned%20Space-time%20Tokens%20for%20Few-shot%20Action%20Recognition&body=Title%3A%20Trajectory-aligned%20Space-time%20Tokens%20for%20Few-shot%20Action%20Recognition%0AAuthor%3A%20Pulkit%20Kumar%20and%20Namitha%20Padmanabhan%20and%20Luke%20Luo%20and%20Sai%20Saketh%20Rambhatla%20and%20Abhinav%20Shrivastava%0AAbstract%3A%20%20%20We%20propose%20a%20simple%20yet%20effective%20approach%20for%20few-shot%20action%20recognition%2C%0Aemphasizing%20the%20disentanglement%20of%20motion%20and%20appearance%20representations.%20By%0Aharnessing%20recent%20progress%20in%20tracking%2C%20specifically%20point%20trajectories%20and%0Aself-supervised%20representation%20learning%2C%20we%20build%20trajectory-aligned%20tokens%0A%28TATs%29%20that%20capture%20motion%20and%20appearance%20information.%20This%20approach%0Asignificantly%20reduces%20the%20data%20requirements%20while%20retaining%20essential%0Ainformation.%20To%20process%20these%20representations%2C%20we%20use%20a%20Masked%20Space-time%0ATransformer%20that%20effectively%20learns%20to%20aggregate%20information%20to%20facilitate%0Afew-shot%20action%20recognition.%20We%20demonstrate%20state-of-the-art%20results%20on%0Afew-shot%20action%20recognition%20across%20multiple%20datasets.%20Our%20project%20page%20is%0Aavailable%20at%20https%3A//www.cs.umd.edu/~pulkit/tats%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrajectory-aligned%2520Space-time%2520Tokens%2520for%2520Few-shot%2520Action%2520Recognition%26entry.906535625%3DPulkit%2520Kumar%2520and%2520Namitha%2520Padmanabhan%2520and%2520Luke%2520Luo%2520and%2520Sai%2520Saketh%2520Rambhatla%2520and%2520Abhinav%2520Shrivastava%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520simple%2520yet%2520effective%2520approach%2520for%2520few-shot%2520action%2520recognition%252C%250Aemphasizing%2520the%2520disentanglement%2520of%2520motion%2520and%2520appearance%2520representations.%2520By%250Aharnessing%2520recent%2520progress%2520in%2520tracking%252C%2520specifically%2520point%2520trajectories%2520and%250Aself-supervised%2520representation%2520learning%252C%2520we%2520build%2520trajectory-aligned%2520tokens%250A%2528TATs%2529%2520that%2520capture%2520motion%2520and%2520appearance%2520information.%2520This%2520approach%250Asignificantly%2520reduces%2520the%2520data%2520requirements%2520while%2520retaining%2520essential%250Ainformation.%2520To%2520process%2520these%2520representations%252C%2520we%2520use%2520a%2520Masked%2520Space-time%250ATransformer%2520that%2520effectively%2520learns%2520to%2520aggregate%2520information%2520to%2520facilitate%250Afew-shot%2520action%2520recognition.%2520We%2520demonstrate%2520state-of-the-art%2520results%2520on%250Afew-shot%2520action%2520recognition%2520across%2520multiple%2520datasets.%2520Our%2520project%2520page%2520is%250Aavailable%2520at%2520https%253A//www.cs.umd.edu/~pulkit/tats%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Trajectory-aligned%20Space-time%20Tokens%20for%20Few-shot%20Action%20Recognition&entry.906535625=Pulkit%20Kumar%20and%20Namitha%20Padmanabhan%20and%20Luke%20Luo%20and%20Sai%20Saketh%20Rambhatla%20and%20Abhinav%20Shrivastava&entry.1292438233=%20%20We%20propose%20a%20simple%20yet%20effective%20approach%20for%20few-shot%20action%20recognition%2C%0Aemphasizing%20the%20disentanglement%20of%20motion%20and%20appearance%20representations.%20By%0Aharnessing%20recent%20progress%20in%20tracking%2C%20specifically%20point%20trajectories%20and%0Aself-supervised%20representation%20learning%2C%20we%20build%20trajectory-aligned%20tokens%0A%28TATs%29%20that%20capture%20motion%20and%20appearance%20information.%20This%20approach%0Asignificantly%20reduces%20the%20data%20requirements%20while%20retaining%20essential%0Ainformation.%20To%20process%20these%20representations%2C%20we%20use%20a%20Masked%20Space-time%0ATransformer%20that%20effectively%20learns%20to%20aggregate%20information%20to%20facilitate%0Afew-shot%20action%20recognition.%20We%20demonstrate%20state-of-the-art%20results%20on%0Afew-shot%20action%20recognition%20across%20multiple%20datasets.%20Our%20project%20page%20is%0Aavailable%20at%20https%3A//www.cs.umd.edu/~pulkit/tats%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18249v1&entry.124074799=Read"},
{"title": "A Novel Perception Entropy Metric for Optimizing Vehicle Perception with\n  LiDAR Deployment", "author": "Yongjiang He and Peng Cao and Zhongling Su and Xiaobo Liu", "abstract": "  Developing an effective evaluation metric is crucial for accurately and\nswiftly measuring LiDAR perception performance. One major issue is the lack of\nmetrics that can simultaneously generate fast and accurate evaluations based on\neither object detection or point cloud data. In this study, we propose a novel\nLiDAR perception entropy metric based on the probability of vehicle grid\noccupancy. This metric reflects the influence of point cloud distribution on\nvehicle detection performance. Based on this, we also introduce a LiDAR\ndeployment optimization model, which is solved using a differential\nevolution-based particle swarm optimization algorithm. A comparative experiment\ndemonstrated that the proposed PE-VGOP offers a correlation of more than 0.98\nwith vehicle detection ground truth in evaluating LiDAR perception performance.\nFurthermore, compared to the base deployment, field experiments indicate that\nthe proposed optimization model can significantly enhance the perception\ncapabilities of various types of LiDARs, including RS-16, RS-32, and RS-80.\nNotably, it achieves a 25% increase in detection Recall for the RS-32 LiDAR.\n", "link": "http://arxiv.org/abs/2407.17942v1", "date": "2024-07-25", "relevancy": 2.1769, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5905}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5498}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Perception%20Entropy%20Metric%20for%20Optimizing%20Vehicle%20Perception%20with%0A%20%20LiDAR%20Deployment&body=Title%3A%20A%20Novel%20Perception%20Entropy%20Metric%20for%20Optimizing%20Vehicle%20Perception%20with%0A%20%20LiDAR%20Deployment%0AAuthor%3A%20Yongjiang%20He%20and%20Peng%20Cao%20and%20Zhongling%20Su%20and%20Xiaobo%20Liu%0AAbstract%3A%20%20%20Developing%20an%20effective%20evaluation%20metric%20is%20crucial%20for%20accurately%20and%0Aswiftly%20measuring%20LiDAR%20perception%20performance.%20One%20major%20issue%20is%20the%20lack%20of%0Ametrics%20that%20can%20simultaneously%20generate%20fast%20and%20accurate%20evaluations%20based%20on%0Aeither%20object%20detection%20or%20point%20cloud%20data.%20In%20this%20study%2C%20we%20propose%20a%20novel%0ALiDAR%20perception%20entropy%20metric%20based%20on%20the%20probability%20of%20vehicle%20grid%0Aoccupancy.%20This%20metric%20reflects%20the%20influence%20of%20point%20cloud%20distribution%20on%0Avehicle%20detection%20performance.%20Based%20on%20this%2C%20we%20also%20introduce%20a%20LiDAR%0Adeployment%20optimization%20model%2C%20which%20is%20solved%20using%20a%20differential%0Aevolution-based%20particle%20swarm%20optimization%20algorithm.%20A%20comparative%20experiment%0Ademonstrated%20that%20the%20proposed%20PE-VGOP%20offers%20a%20correlation%20of%20more%20than%200.98%0Awith%20vehicle%20detection%20ground%20truth%20in%20evaluating%20LiDAR%20perception%20performance.%0AFurthermore%2C%20compared%20to%20the%20base%20deployment%2C%20field%20experiments%20indicate%20that%0Athe%20proposed%20optimization%20model%20can%20significantly%20enhance%20the%20perception%0Acapabilities%20of%20various%20types%20of%20LiDARs%2C%20including%20RS-16%2C%20RS-32%2C%20and%20RS-80.%0ANotably%2C%20it%20achieves%20a%2025%25%20increase%20in%20detection%20Recall%20for%20the%20RS-32%20LiDAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Perception%2520Entropy%2520Metric%2520for%2520Optimizing%2520Vehicle%2520Perception%2520with%250A%2520%2520LiDAR%2520Deployment%26entry.906535625%3DYongjiang%2520He%2520and%2520Peng%2520Cao%2520and%2520Zhongling%2520Su%2520and%2520Xiaobo%2520Liu%26entry.1292438233%3D%2520%2520Developing%2520an%2520effective%2520evaluation%2520metric%2520is%2520crucial%2520for%2520accurately%2520and%250Aswiftly%2520measuring%2520LiDAR%2520perception%2520performance.%2520One%2520major%2520issue%2520is%2520the%2520lack%2520of%250Ametrics%2520that%2520can%2520simultaneously%2520generate%2520fast%2520and%2520accurate%2520evaluations%2520based%2520on%250Aeither%2520object%2520detection%2520or%2520point%2520cloud%2520data.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%250ALiDAR%2520perception%2520entropy%2520metric%2520based%2520on%2520the%2520probability%2520of%2520vehicle%2520grid%250Aoccupancy.%2520This%2520metric%2520reflects%2520the%2520influence%2520of%2520point%2520cloud%2520distribution%2520on%250Avehicle%2520detection%2520performance.%2520Based%2520on%2520this%252C%2520we%2520also%2520introduce%2520a%2520LiDAR%250Adeployment%2520optimization%2520model%252C%2520which%2520is%2520solved%2520using%2520a%2520differential%250Aevolution-based%2520particle%2520swarm%2520optimization%2520algorithm.%2520A%2520comparative%2520experiment%250Ademonstrated%2520that%2520the%2520proposed%2520PE-VGOP%2520offers%2520a%2520correlation%2520of%2520more%2520than%25200.98%250Awith%2520vehicle%2520detection%2520ground%2520truth%2520in%2520evaluating%2520LiDAR%2520perception%2520performance.%250AFurthermore%252C%2520compared%2520to%2520the%2520base%2520deployment%252C%2520field%2520experiments%2520indicate%2520that%250Athe%2520proposed%2520optimization%2520model%2520can%2520significantly%2520enhance%2520the%2520perception%250Acapabilities%2520of%2520various%2520types%2520of%2520LiDARs%252C%2520including%2520RS-16%252C%2520RS-32%252C%2520and%2520RS-80.%250ANotably%252C%2520it%2520achieves%2520a%252025%2525%2520increase%2520in%2520detection%2520Recall%2520for%2520the%2520RS-32%2520LiDAR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Perception%20Entropy%20Metric%20for%20Optimizing%20Vehicle%20Perception%20with%0A%20%20LiDAR%20Deployment&entry.906535625=Yongjiang%20He%20and%20Peng%20Cao%20and%20Zhongling%20Su%20and%20Xiaobo%20Liu&entry.1292438233=%20%20Developing%20an%20effective%20evaluation%20metric%20is%20crucial%20for%20accurately%20and%0Aswiftly%20measuring%20LiDAR%20perception%20performance.%20One%20major%20issue%20is%20the%20lack%20of%0Ametrics%20that%20can%20simultaneously%20generate%20fast%20and%20accurate%20evaluations%20based%20on%0Aeither%20object%20detection%20or%20point%20cloud%20data.%20In%20this%20study%2C%20we%20propose%20a%20novel%0ALiDAR%20perception%20entropy%20metric%20based%20on%20the%20probability%20of%20vehicle%20grid%0Aoccupancy.%20This%20metric%20reflects%20the%20influence%20of%20point%20cloud%20distribution%20on%0Avehicle%20detection%20performance.%20Based%20on%20this%2C%20we%20also%20introduce%20a%20LiDAR%0Adeployment%20optimization%20model%2C%20which%20is%20solved%20using%20a%20differential%0Aevolution-based%20particle%20swarm%20optimization%20algorithm.%20A%20comparative%20experiment%0Ademonstrated%20that%20the%20proposed%20PE-VGOP%20offers%20a%20correlation%20of%20more%20than%200.98%0Awith%20vehicle%20detection%20ground%20truth%20in%20evaluating%20LiDAR%20perception%20performance.%0AFurthermore%2C%20compared%20to%20the%20base%20deployment%2C%20field%20experiments%20indicate%20that%0Athe%20proposed%20optimization%20model%20can%20significantly%20enhance%20the%20perception%0Acapabilities%20of%20various%20types%20of%20LiDARs%2C%20including%20RS-16%2C%20RS-32%2C%20and%20RS-80.%0ANotably%2C%20it%20achieves%20a%2025%25%20increase%20in%20detection%20Recall%20for%20the%20RS-32%20LiDAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17942v1&entry.124074799=Read"},
{"title": "RIDA: A Robust Attack Framework on Incomplete Graphs", "author": "Jianke Yu and Hanchen Wang and Chen Chen and Xiaoyang Wang and Wenjie Zhang and Ying Zhang", "abstract": "  Graph Neural Networks (GNNs) are vital in data science but are increasingly\nsusceptible to adversarial attacks. To help researchers develop more robust GNN\nmodels, it's essential to focus on designing strong attack models as\nfoundational benchmarks and guiding references. Among adversarial attacks,\ngray-box poisoning attacks are noteworthy due to their effectiveness and fewer\nconstraints. These attacks exploit GNNs' need for retraining on updated data,\nthereby impacting their performance by perturbing these datasets. However,\ncurrent research overlooks the real-world scenario of incomplete graphs.To\naddress this gap, we introduce the Robust Incomplete Deep Attack Framework\n(RIDA). It is the first algorithm for robust gray-box poisoning attacks on\nincomplete graphs. The approach innovatively aggregates distant vertex\ninformation and ensures powerful data utilization.Extensive tests against 9\nSOTA baselines on 3 real-world datasets demonstrate RIDA's superiority in\nhandling incompleteness and high attack performance on the incomplete graph.\n", "link": "http://arxiv.org/abs/2407.18170v1", "date": "2024-07-25", "relevancy": 2.1768, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4388}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4358}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RIDA%3A%20A%20Robust%20Attack%20Framework%20on%20Incomplete%20Graphs&body=Title%3A%20RIDA%3A%20A%20Robust%20Attack%20Framework%20on%20Incomplete%20Graphs%0AAuthor%3A%20Jianke%20Yu%20and%20Hanchen%20Wang%20and%20Chen%20Chen%20and%20Xiaoyang%20Wang%20and%20Wenjie%20Zhang%20and%20Ying%20Zhang%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20vital%20in%20data%20science%20but%20are%20increasingly%0Asusceptible%20to%20adversarial%20attacks.%20To%20help%20researchers%20develop%20more%20robust%20GNN%0Amodels%2C%20it%27s%20essential%20to%20focus%20on%20designing%20strong%20attack%20models%20as%0Afoundational%20benchmarks%20and%20guiding%20references.%20Among%20adversarial%20attacks%2C%0Agray-box%20poisoning%20attacks%20are%20noteworthy%20due%20to%20their%20effectiveness%20and%20fewer%0Aconstraints.%20These%20attacks%20exploit%20GNNs%27%20need%20for%20retraining%20on%20updated%20data%2C%0Athereby%20impacting%20their%20performance%20by%20perturbing%20these%20datasets.%20However%2C%0Acurrent%20research%20overlooks%20the%20real-world%20scenario%20of%20incomplete%20graphs.To%0Aaddress%20this%20gap%2C%20we%20introduce%20the%20Robust%20Incomplete%20Deep%20Attack%20Framework%0A%28RIDA%29.%20It%20is%20the%20first%20algorithm%20for%20robust%20gray-box%20poisoning%20attacks%20on%0Aincomplete%20graphs.%20The%20approach%20innovatively%20aggregates%20distant%20vertex%0Ainformation%20and%20ensures%20powerful%20data%20utilization.Extensive%20tests%20against%209%0ASOTA%20baselines%20on%203%20real-world%20datasets%20demonstrate%20RIDA%27s%20superiority%20in%0Ahandling%20incompleteness%20and%20high%20attack%20performance%20on%20the%20incomplete%20graph.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18170v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRIDA%253A%2520A%2520Robust%2520Attack%2520Framework%2520on%2520Incomplete%2520Graphs%26entry.906535625%3DJianke%2520Yu%2520and%2520Hanchen%2520Wang%2520and%2520Chen%2520Chen%2520and%2520Xiaoyang%2520Wang%2520and%2520Wenjie%2520Zhang%2520and%2520Ying%2520Zhang%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520vital%2520in%2520data%2520science%2520but%2520are%2520increasingly%250Asusceptible%2520to%2520adversarial%2520attacks.%2520To%2520help%2520researchers%2520develop%2520more%2520robust%2520GNN%250Amodels%252C%2520it%2527s%2520essential%2520to%2520focus%2520on%2520designing%2520strong%2520attack%2520models%2520as%250Afoundational%2520benchmarks%2520and%2520guiding%2520references.%2520Among%2520adversarial%2520attacks%252C%250Agray-box%2520poisoning%2520attacks%2520are%2520noteworthy%2520due%2520to%2520their%2520effectiveness%2520and%2520fewer%250Aconstraints.%2520These%2520attacks%2520exploit%2520GNNs%2527%2520need%2520for%2520retraining%2520on%2520updated%2520data%252C%250Athereby%2520impacting%2520their%2520performance%2520by%2520perturbing%2520these%2520datasets.%2520However%252C%250Acurrent%2520research%2520overlooks%2520the%2520real-world%2520scenario%2520of%2520incomplete%2520graphs.To%250Aaddress%2520this%2520gap%252C%2520we%2520introduce%2520the%2520Robust%2520Incomplete%2520Deep%2520Attack%2520Framework%250A%2528RIDA%2529.%2520It%2520is%2520the%2520first%2520algorithm%2520for%2520robust%2520gray-box%2520poisoning%2520attacks%2520on%250Aincomplete%2520graphs.%2520The%2520approach%2520innovatively%2520aggregates%2520distant%2520vertex%250Ainformation%2520and%2520ensures%2520powerful%2520data%2520utilization.Extensive%2520tests%2520against%25209%250ASOTA%2520baselines%2520on%25203%2520real-world%2520datasets%2520demonstrate%2520RIDA%2527s%2520superiority%2520in%250Ahandling%2520incompleteness%2520and%2520high%2520attack%2520performance%2520on%2520the%2520incomplete%2520graph.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18170v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RIDA%3A%20A%20Robust%20Attack%20Framework%20on%20Incomplete%20Graphs&entry.906535625=Jianke%20Yu%20and%20Hanchen%20Wang%20and%20Chen%20Chen%20and%20Xiaoyang%20Wang%20and%20Wenjie%20Zhang%20and%20Ying%20Zhang&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20vital%20in%20data%20science%20but%20are%20increasingly%0Asusceptible%20to%20adversarial%20attacks.%20To%20help%20researchers%20develop%20more%20robust%20GNN%0Amodels%2C%20it%27s%20essential%20to%20focus%20on%20designing%20strong%20attack%20models%20as%0Afoundational%20benchmarks%20and%20guiding%20references.%20Among%20adversarial%20attacks%2C%0Agray-box%20poisoning%20attacks%20are%20noteworthy%20due%20to%20their%20effectiveness%20and%20fewer%0Aconstraints.%20These%20attacks%20exploit%20GNNs%27%20need%20for%20retraining%20on%20updated%20data%2C%0Athereby%20impacting%20their%20performance%20by%20perturbing%20these%20datasets.%20However%2C%0Acurrent%20research%20overlooks%20the%20real-world%20scenario%20of%20incomplete%20graphs.To%0Aaddress%20this%20gap%2C%20we%20introduce%20the%20Robust%20Incomplete%20Deep%20Attack%20Framework%0A%28RIDA%29.%20It%20is%20the%20first%20algorithm%20for%20robust%20gray-box%20poisoning%20attacks%20on%0Aincomplete%20graphs.%20The%20approach%20innovatively%20aggregates%20distant%20vertex%0Ainformation%20and%20ensures%20powerful%20data%20utilization.Extensive%20tests%20against%209%0ASOTA%20baselines%20on%203%20real-world%20datasets%20demonstrate%20RIDA%27s%20superiority%20in%0Ahandling%20incompleteness%20and%20high%20attack%20performance%20on%20the%20incomplete%20graph.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18170v1&entry.124074799=Read"},
{"title": "Segmentation-guided MRI reconstruction for meaningfully diverse\n  reconstructions", "author": "Jan Nikolas Morshuis and Matthias Hein and Christian F. Baumgartner", "abstract": "  Inverse problems, such as accelerated MRI reconstruction, are ill-posed and\nan infinite amount of possible and plausible solutions exist. This may not only\nlead to uncertainty in the reconstructed image but also in downstream tasks\nsuch as semantic segmentation. This uncertainty, however, is mostly not\nanalyzed in the literature, even though probabilistic reconstruction models are\ncommonly used. These models can be prone to ignore plausible but unlikely\nsolutions like rare pathologies. Building on MRI reconstruction approaches\nbased on diffusion models, we add guidance to the diffusion process during\ninference, generating two meaningfully diverse reconstructions corresponding to\nan upper and lower bound segmentation. The reconstruction uncertainty can then\nbe quantified by the difference between these bounds, which we coin the\n'uncertainty boundary'. We analyzed the behavior of the upper and lower bound\nsegmentations for a wide range of acceleration factors and found the\nuncertainty boundary to be both more reliable and more accurate compared to\nrepeated sampling. Code is available at https://github.com/NikolasMorshuis/SGR\n", "link": "http://arxiv.org/abs/2407.18026v1", "date": "2024-07-25", "relevancy": 2.138, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5508}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5278}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segmentation-guided%20MRI%20reconstruction%20for%20meaningfully%20diverse%0A%20%20reconstructions&body=Title%3A%20Segmentation-guided%20MRI%20reconstruction%20for%20meaningfully%20diverse%0A%20%20reconstructions%0AAuthor%3A%20Jan%20Nikolas%20Morshuis%20and%20Matthias%20Hein%20and%20Christian%20F.%20Baumgartner%0AAbstract%3A%20%20%20Inverse%20problems%2C%20such%20as%20accelerated%20MRI%20reconstruction%2C%20are%20ill-posed%20and%0Aan%20infinite%20amount%20of%20possible%20and%20plausible%20solutions%20exist.%20This%20may%20not%20only%0Alead%20to%20uncertainty%20in%20the%20reconstructed%20image%20but%20also%20in%20downstream%20tasks%0Asuch%20as%20semantic%20segmentation.%20This%20uncertainty%2C%20however%2C%20is%20mostly%20not%0Aanalyzed%20in%20the%20literature%2C%20even%20though%20probabilistic%20reconstruction%20models%20are%0Acommonly%20used.%20These%20models%20can%20be%20prone%20to%20ignore%20plausible%20but%20unlikely%0Asolutions%20like%20rare%20pathologies.%20Building%20on%20MRI%20reconstruction%20approaches%0Abased%20on%20diffusion%20models%2C%20we%20add%20guidance%20to%20the%20diffusion%20process%20during%0Ainference%2C%20generating%20two%20meaningfully%20diverse%20reconstructions%20corresponding%20to%0Aan%20upper%20and%20lower%20bound%20segmentation.%20The%20reconstruction%20uncertainty%20can%20then%0Abe%20quantified%20by%20the%20difference%20between%20these%20bounds%2C%20which%20we%20coin%20the%0A%27uncertainty%20boundary%27.%20We%20analyzed%20the%20behavior%20of%20the%20upper%20and%20lower%20bound%0Asegmentations%20for%20a%20wide%20range%20of%20acceleration%20factors%20and%20found%20the%0Auncertainty%20boundary%20to%20be%20both%20more%20reliable%20and%20more%20accurate%20compared%20to%0Arepeated%20sampling.%20Code%20is%20available%20at%20https%3A//github.com/NikolasMorshuis/SGR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18026v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegmentation-guided%2520MRI%2520reconstruction%2520for%2520meaningfully%2520diverse%250A%2520%2520reconstructions%26entry.906535625%3DJan%2520Nikolas%2520Morshuis%2520and%2520Matthias%2520Hein%2520and%2520Christian%2520F.%2520Baumgartner%26entry.1292438233%3D%2520%2520Inverse%2520problems%252C%2520such%2520as%2520accelerated%2520MRI%2520reconstruction%252C%2520are%2520ill-posed%2520and%250Aan%2520infinite%2520amount%2520of%2520possible%2520and%2520plausible%2520solutions%2520exist.%2520This%2520may%2520not%2520only%250Alead%2520to%2520uncertainty%2520in%2520the%2520reconstructed%2520image%2520but%2520also%2520in%2520downstream%2520tasks%250Asuch%2520as%2520semantic%2520segmentation.%2520This%2520uncertainty%252C%2520however%252C%2520is%2520mostly%2520not%250Aanalyzed%2520in%2520the%2520literature%252C%2520even%2520though%2520probabilistic%2520reconstruction%2520models%2520are%250Acommonly%2520used.%2520These%2520models%2520can%2520be%2520prone%2520to%2520ignore%2520plausible%2520but%2520unlikely%250Asolutions%2520like%2520rare%2520pathologies.%2520Building%2520on%2520MRI%2520reconstruction%2520approaches%250Abased%2520on%2520diffusion%2520models%252C%2520we%2520add%2520guidance%2520to%2520the%2520diffusion%2520process%2520during%250Ainference%252C%2520generating%2520two%2520meaningfully%2520diverse%2520reconstructions%2520corresponding%2520to%250Aan%2520upper%2520and%2520lower%2520bound%2520segmentation.%2520The%2520reconstruction%2520uncertainty%2520can%2520then%250Abe%2520quantified%2520by%2520the%2520difference%2520between%2520these%2520bounds%252C%2520which%2520we%2520coin%2520the%250A%2527uncertainty%2520boundary%2527.%2520We%2520analyzed%2520the%2520behavior%2520of%2520the%2520upper%2520and%2520lower%2520bound%250Asegmentations%2520for%2520a%2520wide%2520range%2520of%2520acceleration%2520factors%2520and%2520found%2520the%250Auncertainty%2520boundary%2520to%2520be%2520both%2520more%2520reliable%2520and%2520more%2520accurate%2520compared%2520to%250Arepeated%2520sampling.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/NikolasMorshuis/SGR%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18026v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segmentation-guided%20MRI%20reconstruction%20for%20meaningfully%20diverse%0A%20%20reconstructions&entry.906535625=Jan%20Nikolas%20Morshuis%20and%20Matthias%20Hein%20and%20Christian%20F.%20Baumgartner&entry.1292438233=%20%20Inverse%20problems%2C%20such%20as%20accelerated%20MRI%20reconstruction%2C%20are%20ill-posed%20and%0Aan%20infinite%20amount%20of%20possible%20and%20plausible%20solutions%20exist.%20This%20may%20not%20only%0Alead%20to%20uncertainty%20in%20the%20reconstructed%20image%20but%20also%20in%20downstream%20tasks%0Asuch%20as%20semantic%20segmentation.%20This%20uncertainty%2C%20however%2C%20is%20mostly%20not%0Aanalyzed%20in%20the%20literature%2C%20even%20though%20probabilistic%20reconstruction%20models%20are%0Acommonly%20used.%20These%20models%20can%20be%20prone%20to%20ignore%20plausible%20but%20unlikely%0Asolutions%20like%20rare%20pathologies.%20Building%20on%20MRI%20reconstruction%20approaches%0Abased%20on%20diffusion%20models%2C%20we%20add%20guidance%20to%20the%20diffusion%20process%20during%0Ainference%2C%20generating%20two%20meaningfully%20diverse%20reconstructions%20corresponding%20to%0Aan%20upper%20and%20lower%20bound%20segmentation.%20The%20reconstruction%20uncertainty%20can%20then%0Abe%20quantified%20by%20the%20difference%20between%20these%20bounds%2C%20which%20we%20coin%20the%0A%27uncertainty%20boundary%27.%20We%20analyzed%20the%20behavior%20of%20the%20upper%20and%20lower%20bound%0Asegmentations%20for%20a%20wide%20range%20of%20acceleration%20factors%20and%20found%20the%0Auncertainty%20boundary%20to%20be%20both%20more%20reliable%20and%20more%20accurate%20compared%20to%0Arepeated%20sampling.%20Code%20is%20available%20at%20https%3A//github.com/NikolasMorshuis/SGR%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18026v1&entry.124074799=Read"},
{"title": "HVM-1: Large-scale video models pretrained with nearly 5000 hours of\n  human-like video data", "author": "A. Emin Orhan", "abstract": "  We introduce Human-like Video Models (HVM-1), large-scale video models\npretrained with nearly 5000 hours of curated human-like video data (mostly\negocentric, temporally extended, continuous video recordings), using the\nspatiotemporal masked autoencoder (ST-MAE) algorithm. We release two 633M\nparameter models trained at spatial resolutions of 224x224 and 448x448 pixels.\nWe evaluate the performance of these models in downstream few-shot video and\nimage recognition tasks and compare them against a model pretrained with 1330\nhours of short action-oriented video clips from YouTube (Kinetics-700). HVM-1\nmodels perform competitively against the Kinetics-700 pretrained model in\ndownstream evaluations despite substantial qualitative differences between the\nspatiotemporal characteristics of the corresponding pretraining datasets. HVM-1\nmodels also learn more accurate and more robust object representations compared\nto models pretrained with the image-based MAE algorithm on the same data,\ndemonstrating the potential benefits of learning to predict temporal\nregularities in natural videos for learning better object representations.\n", "link": "http://arxiv.org/abs/2407.18067v1", "date": "2024-07-25", "relevancy": 2.1229, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5365}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.532}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HVM-1%3A%20Large-scale%20video%20models%20pretrained%20with%20nearly%205000%20hours%20of%0A%20%20human-like%20video%20data&body=Title%3A%20HVM-1%3A%20Large-scale%20video%20models%20pretrained%20with%20nearly%205000%20hours%20of%0A%20%20human-like%20video%20data%0AAuthor%3A%20A.%20Emin%20Orhan%0AAbstract%3A%20%20%20We%20introduce%20Human-like%20Video%20Models%20%28HVM-1%29%2C%20large-scale%20video%20models%0Apretrained%20with%20nearly%205000%20hours%20of%20curated%20human-like%20video%20data%20%28mostly%0Aegocentric%2C%20temporally%20extended%2C%20continuous%20video%20recordings%29%2C%20using%20the%0Aspatiotemporal%20masked%20autoencoder%20%28ST-MAE%29%20algorithm.%20We%20release%20two%20633M%0Aparameter%20models%20trained%20at%20spatial%20resolutions%20of%20224x224%20and%20448x448%20pixels.%0AWe%20evaluate%20the%20performance%20of%20these%20models%20in%20downstream%20few-shot%20video%20and%0Aimage%20recognition%20tasks%20and%20compare%20them%20against%20a%20model%20pretrained%20with%201330%0Ahours%20of%20short%20action-oriented%20video%20clips%20from%20YouTube%20%28Kinetics-700%29.%20HVM-1%0Amodels%20perform%20competitively%20against%20the%20Kinetics-700%20pretrained%20model%20in%0Adownstream%20evaluations%20despite%20substantial%20qualitative%20differences%20between%20the%0Aspatiotemporal%20characteristics%20of%20the%20corresponding%20pretraining%20datasets.%20HVM-1%0Amodels%20also%20learn%20more%20accurate%20and%20more%20robust%20object%20representations%20compared%0Ato%20models%20pretrained%20with%20the%20image-based%20MAE%20algorithm%20on%20the%20same%20data%2C%0Ademonstrating%20the%20potential%20benefits%20of%20learning%20to%20predict%20temporal%0Aregularities%20in%20natural%20videos%20for%20learning%20better%20object%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18067v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHVM-1%253A%2520Large-scale%2520video%2520models%2520pretrained%2520with%2520nearly%25205000%2520hours%2520of%250A%2520%2520human-like%2520video%2520data%26entry.906535625%3DA.%2520Emin%2520Orhan%26entry.1292438233%3D%2520%2520We%2520introduce%2520Human-like%2520Video%2520Models%2520%2528HVM-1%2529%252C%2520large-scale%2520video%2520models%250Apretrained%2520with%2520nearly%25205000%2520hours%2520of%2520curated%2520human-like%2520video%2520data%2520%2528mostly%250Aegocentric%252C%2520temporally%2520extended%252C%2520continuous%2520video%2520recordings%2529%252C%2520using%2520the%250Aspatiotemporal%2520masked%2520autoencoder%2520%2528ST-MAE%2529%2520algorithm.%2520We%2520release%2520two%2520633M%250Aparameter%2520models%2520trained%2520at%2520spatial%2520resolutions%2520of%2520224x224%2520and%2520448x448%2520pixels.%250AWe%2520evaluate%2520the%2520performance%2520of%2520these%2520models%2520in%2520downstream%2520few-shot%2520video%2520and%250Aimage%2520recognition%2520tasks%2520and%2520compare%2520them%2520against%2520a%2520model%2520pretrained%2520with%25201330%250Ahours%2520of%2520short%2520action-oriented%2520video%2520clips%2520from%2520YouTube%2520%2528Kinetics-700%2529.%2520HVM-1%250Amodels%2520perform%2520competitively%2520against%2520the%2520Kinetics-700%2520pretrained%2520model%2520in%250Adownstream%2520evaluations%2520despite%2520substantial%2520qualitative%2520differences%2520between%2520the%250Aspatiotemporal%2520characteristics%2520of%2520the%2520corresponding%2520pretraining%2520datasets.%2520HVM-1%250Amodels%2520also%2520learn%2520more%2520accurate%2520and%2520more%2520robust%2520object%2520representations%2520compared%250Ato%2520models%2520pretrained%2520with%2520the%2520image-based%2520MAE%2520algorithm%2520on%2520the%2520same%2520data%252C%250Ademonstrating%2520the%2520potential%2520benefits%2520of%2520learning%2520to%2520predict%2520temporal%250Aregularities%2520in%2520natural%2520videos%2520for%2520learning%2520better%2520object%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18067v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HVM-1%3A%20Large-scale%20video%20models%20pretrained%20with%20nearly%205000%20hours%20of%0A%20%20human-like%20video%20data&entry.906535625=A.%20Emin%20Orhan&entry.1292438233=%20%20We%20introduce%20Human-like%20Video%20Models%20%28HVM-1%29%2C%20large-scale%20video%20models%0Apretrained%20with%20nearly%205000%20hours%20of%20curated%20human-like%20video%20data%20%28mostly%0Aegocentric%2C%20temporally%20extended%2C%20continuous%20video%20recordings%29%2C%20using%20the%0Aspatiotemporal%20masked%20autoencoder%20%28ST-MAE%29%20algorithm.%20We%20release%20two%20633M%0Aparameter%20models%20trained%20at%20spatial%20resolutions%20of%20224x224%20and%20448x448%20pixels.%0AWe%20evaluate%20the%20performance%20of%20these%20models%20in%20downstream%20few-shot%20video%20and%0Aimage%20recognition%20tasks%20and%20compare%20them%20against%20a%20model%20pretrained%20with%201330%0Ahours%20of%20short%20action-oriented%20video%20clips%20from%20YouTube%20%28Kinetics-700%29.%20HVM-1%0Amodels%20perform%20competitively%20against%20the%20Kinetics-700%20pretrained%20model%20in%0Adownstream%20evaluations%20despite%20substantial%20qualitative%20differences%20between%20the%0Aspatiotemporal%20characteristics%20of%20the%20corresponding%20pretraining%20datasets.%20HVM-1%0Amodels%20also%20learn%20more%20accurate%20and%20more%20robust%20object%20representations%20compared%0Ato%20models%20pretrained%20with%20the%20image-based%20MAE%20algorithm%20on%20the%20same%20data%2C%0Ademonstrating%20the%20potential%20benefits%20of%20learning%20to%20predict%20temporal%0Aregularities%20in%20natural%20videos%20for%20learning%20better%20object%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18067v1&entry.124074799=Read"},
{"title": "Multicenter Privacy-Preserving Model Training for Deep Learning Brain\n  Metastases Autosegmentation", "author": "Yixing Huang and Zahra Khodabakhshi and Ahmed Gomaa and Manuel Schmidt and Rainer Fietkau and Matthias Guckenberger and Nicolaus Andratschke and Christoph Bert and Stephanie Tanadini-Lang and Florian Putz", "abstract": "  Objectives: This work aims to explore the impact of multicenter data\nheterogeneity on deep learning brain metastases (BM) autosegmentation\nperformance, and assess the efficacy of an incremental transfer learning\ntechnique, namely learning without forgetting (LWF), to improve model\ngeneralizability without sharing raw data.\n  Materials and methods: A total of six BM datasets from University Hospital\nErlangen (UKER), University Hospital Zurich (USZ), Stanford, UCSF, NYU and\nBraTS Challenge 2023 on BM segmentation were used for this evaluation. First,\nthe multicenter performance of a convolutional neural network (DeepMedic) for\nBM autosegmentation was established for exclusive single-center training and\nfor training on pooled data, respectively. Subsequently bilateral collaboration\nwas evaluated, where a UKER pretrained model is shared to another center for\nfurther training using transfer learning (TL) either with or without LWF.\n  Results: For single-center training, average F1 scores of BM detection range\nfrom 0.625 (NYU) to 0.876 (UKER) on respective single-center test data. Mixed\nmulticenter training notably improves F1 scores at Stanford and NYU, with\nnegligible improvement at other centers. When the UKER pretrained model is\napplied to USZ, LWF achieves a higher average F1 score (0.839) than naive TL\n(0.570) and single-center training (0.688) on combined UKER and USZ test data.\nNaive TL improves sensitivity and contouring accuracy, but compromises\nprecision. Conversely, LWF demonstrates commendable sensitivity, precision and\ncontouring accuracy. When applied to Stanford, similar performance was\nobserved.\n  Conclusion: Data heterogeneity results in varying performance in BM\nautosegmentation, posing challenges to model generalizability. LWF is a\npromising approach to peer-to-peer privacy-preserving model training.\n", "link": "http://arxiv.org/abs/2405.10870v2", "date": "2024-07-25", "relevancy": 2.1081, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5517}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5255}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5187}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multicenter%20Privacy-Preserving%20Model%20Training%20for%20Deep%20Learning%20Brain%0A%20%20Metastases%20Autosegmentation&body=Title%3A%20Multicenter%20Privacy-Preserving%20Model%20Training%20for%20Deep%20Learning%20Brain%0A%20%20Metastases%20Autosegmentation%0AAuthor%3A%20Yixing%20Huang%20and%20Zahra%20Khodabakhshi%20and%20Ahmed%20Gomaa%20and%20Manuel%20Schmidt%20and%20Rainer%20Fietkau%20and%20Matthias%20Guckenberger%20and%20Nicolaus%20Andratschke%20and%20Christoph%20Bert%20and%20Stephanie%20Tanadini-Lang%20and%20Florian%20Putz%0AAbstract%3A%20%20%20Objectives%3A%20This%20work%20aims%20to%20explore%20the%20impact%20of%20multicenter%20data%0Aheterogeneity%20on%20deep%20learning%20brain%20metastases%20%28BM%29%20autosegmentation%0Aperformance%2C%20and%20assess%20the%20efficacy%20of%20an%20incremental%20transfer%20learning%0Atechnique%2C%20namely%20learning%20without%20forgetting%20%28LWF%29%2C%20to%20improve%20model%0Ageneralizability%20without%20sharing%20raw%20data.%0A%20%20Materials%20and%20methods%3A%20A%20total%20of%20six%20BM%20datasets%20from%20University%20Hospital%0AErlangen%20%28UKER%29%2C%20University%20Hospital%20Zurich%20%28USZ%29%2C%20Stanford%2C%20UCSF%2C%20NYU%20and%0ABraTS%20Challenge%202023%20on%20BM%20segmentation%20were%20used%20for%20this%20evaluation.%20First%2C%0Athe%20multicenter%20performance%20of%20a%20convolutional%20neural%20network%20%28DeepMedic%29%20for%0ABM%20autosegmentation%20was%20established%20for%20exclusive%20single-center%20training%20and%0Afor%20training%20on%20pooled%20data%2C%20respectively.%20Subsequently%20bilateral%20collaboration%0Awas%20evaluated%2C%20where%20a%20UKER%20pretrained%20model%20is%20shared%20to%20another%20center%20for%0Afurther%20training%20using%20transfer%20learning%20%28TL%29%20either%20with%20or%20without%20LWF.%0A%20%20Results%3A%20For%20single-center%20training%2C%20average%20F1%20scores%20of%20BM%20detection%20range%0Afrom%200.625%20%28NYU%29%20to%200.876%20%28UKER%29%20on%20respective%20single-center%20test%20data.%20Mixed%0Amulticenter%20training%20notably%20improves%20F1%20scores%20at%20Stanford%20and%20NYU%2C%20with%0Anegligible%20improvement%20at%20other%20centers.%20When%20the%20UKER%20pretrained%20model%20is%0Aapplied%20to%20USZ%2C%20LWF%20achieves%20a%20higher%20average%20F1%20score%20%280.839%29%20than%20naive%20TL%0A%280.570%29%20and%20single-center%20training%20%280.688%29%20on%20combined%20UKER%20and%20USZ%20test%20data.%0ANaive%20TL%20improves%20sensitivity%20and%20contouring%20accuracy%2C%20but%20compromises%0Aprecision.%20Conversely%2C%20LWF%20demonstrates%20commendable%20sensitivity%2C%20precision%20and%0Acontouring%20accuracy.%20When%20applied%20to%20Stanford%2C%20similar%20performance%20was%0Aobserved.%0A%20%20Conclusion%3A%20Data%20heterogeneity%20results%20in%20varying%20performance%20in%20BM%0Aautosegmentation%2C%20posing%20challenges%20to%20model%20generalizability.%20LWF%20is%20a%0Apromising%20approach%20to%20peer-to-peer%20privacy-preserving%20model%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10870v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulticenter%2520Privacy-Preserving%2520Model%2520Training%2520for%2520Deep%2520Learning%2520Brain%250A%2520%2520Metastases%2520Autosegmentation%26entry.906535625%3DYixing%2520Huang%2520and%2520Zahra%2520Khodabakhshi%2520and%2520Ahmed%2520Gomaa%2520and%2520Manuel%2520Schmidt%2520and%2520Rainer%2520Fietkau%2520and%2520Matthias%2520Guckenberger%2520and%2520Nicolaus%2520Andratschke%2520and%2520Christoph%2520Bert%2520and%2520Stephanie%2520Tanadini-Lang%2520and%2520Florian%2520Putz%26entry.1292438233%3D%2520%2520Objectives%253A%2520This%2520work%2520aims%2520to%2520explore%2520the%2520impact%2520of%2520multicenter%2520data%250Aheterogeneity%2520on%2520deep%2520learning%2520brain%2520metastases%2520%2528BM%2529%2520autosegmentation%250Aperformance%252C%2520and%2520assess%2520the%2520efficacy%2520of%2520an%2520incremental%2520transfer%2520learning%250Atechnique%252C%2520namely%2520learning%2520without%2520forgetting%2520%2528LWF%2529%252C%2520to%2520improve%2520model%250Ageneralizability%2520without%2520sharing%2520raw%2520data.%250A%2520%2520Materials%2520and%2520methods%253A%2520A%2520total%2520of%2520six%2520BM%2520datasets%2520from%2520University%2520Hospital%250AErlangen%2520%2528UKER%2529%252C%2520University%2520Hospital%2520Zurich%2520%2528USZ%2529%252C%2520Stanford%252C%2520UCSF%252C%2520NYU%2520and%250ABraTS%2520Challenge%25202023%2520on%2520BM%2520segmentation%2520were%2520used%2520for%2520this%2520evaluation.%2520First%252C%250Athe%2520multicenter%2520performance%2520of%2520a%2520convolutional%2520neural%2520network%2520%2528DeepMedic%2529%2520for%250ABM%2520autosegmentation%2520was%2520established%2520for%2520exclusive%2520single-center%2520training%2520and%250Afor%2520training%2520on%2520pooled%2520data%252C%2520respectively.%2520Subsequently%2520bilateral%2520collaboration%250Awas%2520evaluated%252C%2520where%2520a%2520UKER%2520pretrained%2520model%2520is%2520shared%2520to%2520another%2520center%2520for%250Afurther%2520training%2520using%2520transfer%2520learning%2520%2528TL%2529%2520either%2520with%2520or%2520without%2520LWF.%250A%2520%2520Results%253A%2520For%2520single-center%2520training%252C%2520average%2520F1%2520scores%2520of%2520BM%2520detection%2520range%250Afrom%25200.625%2520%2528NYU%2529%2520to%25200.876%2520%2528UKER%2529%2520on%2520respective%2520single-center%2520test%2520data.%2520Mixed%250Amulticenter%2520training%2520notably%2520improves%2520F1%2520scores%2520at%2520Stanford%2520and%2520NYU%252C%2520with%250Anegligible%2520improvement%2520at%2520other%2520centers.%2520When%2520the%2520UKER%2520pretrained%2520model%2520is%250Aapplied%2520to%2520USZ%252C%2520LWF%2520achieves%2520a%2520higher%2520average%2520F1%2520score%2520%25280.839%2529%2520than%2520naive%2520TL%250A%25280.570%2529%2520and%2520single-center%2520training%2520%25280.688%2529%2520on%2520combined%2520UKER%2520and%2520USZ%2520test%2520data.%250ANaive%2520TL%2520improves%2520sensitivity%2520and%2520contouring%2520accuracy%252C%2520but%2520compromises%250Aprecision.%2520Conversely%252C%2520LWF%2520demonstrates%2520commendable%2520sensitivity%252C%2520precision%2520and%250Acontouring%2520accuracy.%2520When%2520applied%2520to%2520Stanford%252C%2520similar%2520performance%2520was%250Aobserved.%250A%2520%2520Conclusion%253A%2520Data%2520heterogeneity%2520results%2520in%2520varying%2520performance%2520in%2520BM%250Aautosegmentation%252C%2520posing%2520challenges%2520to%2520model%2520generalizability.%2520LWF%2520is%2520a%250Apromising%2520approach%2520to%2520peer-to-peer%2520privacy-preserving%2520model%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10870v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multicenter%20Privacy-Preserving%20Model%20Training%20for%20Deep%20Learning%20Brain%0A%20%20Metastases%20Autosegmentation&entry.906535625=Yixing%20Huang%20and%20Zahra%20Khodabakhshi%20and%20Ahmed%20Gomaa%20and%20Manuel%20Schmidt%20and%20Rainer%20Fietkau%20and%20Matthias%20Guckenberger%20and%20Nicolaus%20Andratschke%20and%20Christoph%20Bert%20and%20Stephanie%20Tanadini-Lang%20and%20Florian%20Putz&entry.1292438233=%20%20Objectives%3A%20This%20work%20aims%20to%20explore%20the%20impact%20of%20multicenter%20data%0Aheterogeneity%20on%20deep%20learning%20brain%20metastases%20%28BM%29%20autosegmentation%0Aperformance%2C%20and%20assess%20the%20efficacy%20of%20an%20incremental%20transfer%20learning%0Atechnique%2C%20namely%20learning%20without%20forgetting%20%28LWF%29%2C%20to%20improve%20model%0Ageneralizability%20without%20sharing%20raw%20data.%0A%20%20Materials%20and%20methods%3A%20A%20total%20of%20six%20BM%20datasets%20from%20University%20Hospital%0AErlangen%20%28UKER%29%2C%20University%20Hospital%20Zurich%20%28USZ%29%2C%20Stanford%2C%20UCSF%2C%20NYU%20and%0ABraTS%20Challenge%202023%20on%20BM%20segmentation%20were%20used%20for%20this%20evaluation.%20First%2C%0Athe%20multicenter%20performance%20of%20a%20convolutional%20neural%20network%20%28DeepMedic%29%20for%0ABM%20autosegmentation%20was%20established%20for%20exclusive%20single-center%20training%20and%0Afor%20training%20on%20pooled%20data%2C%20respectively.%20Subsequently%20bilateral%20collaboration%0Awas%20evaluated%2C%20where%20a%20UKER%20pretrained%20model%20is%20shared%20to%20another%20center%20for%0Afurther%20training%20using%20transfer%20learning%20%28TL%29%20either%20with%20or%20without%20LWF.%0A%20%20Results%3A%20For%20single-center%20training%2C%20average%20F1%20scores%20of%20BM%20detection%20range%0Afrom%200.625%20%28NYU%29%20to%200.876%20%28UKER%29%20on%20respective%20single-center%20test%20data.%20Mixed%0Amulticenter%20training%20notably%20improves%20F1%20scores%20at%20Stanford%20and%20NYU%2C%20with%0Anegligible%20improvement%20at%20other%20centers.%20When%20the%20UKER%20pretrained%20model%20is%0Aapplied%20to%20USZ%2C%20LWF%20achieves%20a%20higher%20average%20F1%20score%20%280.839%29%20than%20naive%20TL%0A%280.570%29%20and%20single-center%20training%20%280.688%29%20on%20combined%20UKER%20and%20USZ%20test%20data.%0ANaive%20TL%20improves%20sensitivity%20and%20contouring%20accuracy%2C%20but%20compromises%0Aprecision.%20Conversely%2C%20LWF%20demonstrates%20commendable%20sensitivity%2C%20precision%20and%0Acontouring%20accuracy.%20When%20applied%20to%20Stanford%2C%20similar%20performance%20was%0Aobserved.%0A%20%20Conclusion%3A%20Data%20heterogeneity%20results%20in%20varying%20performance%20in%20BM%0Aautosegmentation%2C%20posing%20challenges%20to%20model%20generalizability.%20LWF%20is%20a%0Apromising%20approach%20to%20peer-to-peer%20privacy-preserving%20model%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10870v2&entry.124074799=Read"},
{"title": "nnU-Net Revisited: A Call for Rigorous Validation in 3D Medical Image\n  Segmentation", "author": "Fabian Isensee and Tassilo Wald and Constantin Ulrich and Michael Baumgartner and Saikat Roy and Klaus Maier-Hein and Paul F. Jaeger", "abstract": "  The release of nnU-Net marked a paradigm shift in 3D medical image\nsegmentation, demonstrating that a properly configured U-Net architecture could\nstill achieve state-of-the-art results. Despite this, the pursuit of novel\narchitectures, and the respective claims of superior performance over the U-Net\nbaseline, continued. In this study, we demonstrate that many of these recent\nclaims fail to hold up when scrutinized for common validation shortcomings,\nsuch as the use of inadequate baselines, insufficient datasets, and neglected\ncomputational resources. By meticulously avoiding these pitfalls, we conduct a\nthorough and comprehensive benchmarking of current segmentation methods\nincluding CNN-based, Transformer-based, and Mamba-based approaches. In contrast\nto current beliefs, we find that the recipe for state-of-the-art performance is\n1) employing CNN-based U-Net models, including ResNet and ConvNeXt variants, 2)\nusing the nnU-Net framework, and 3) scaling models to modern hardware\nresources. These results indicate an ongoing innovation bias towards novel\narchitectures in the field and underscore the need for more stringent\nvalidation standards in the quest for scientific progress.\n", "link": "http://arxiv.org/abs/2404.09556v2", "date": "2024-07-25", "relevancy": 2.1061, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5582}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5247}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20nnU-Net%20Revisited%3A%20A%20Call%20for%20Rigorous%20Validation%20in%203D%20Medical%20Image%0A%20%20Segmentation&body=Title%3A%20nnU-Net%20Revisited%3A%20A%20Call%20for%20Rigorous%20Validation%20in%203D%20Medical%20Image%0A%20%20Segmentation%0AAuthor%3A%20Fabian%20Isensee%20and%20Tassilo%20Wald%20and%20Constantin%20Ulrich%20and%20Michael%20Baumgartner%20and%20Saikat%20Roy%20and%20Klaus%20Maier-Hein%20and%20Paul%20F.%20Jaeger%0AAbstract%3A%20%20%20The%20release%20of%20nnU-Net%20marked%20a%20paradigm%20shift%20in%203D%20medical%20image%0Asegmentation%2C%20demonstrating%20that%20a%20properly%20configured%20U-Net%20architecture%20could%0Astill%20achieve%20state-of-the-art%20results.%20Despite%20this%2C%20the%20pursuit%20of%20novel%0Aarchitectures%2C%20and%20the%20respective%20claims%20of%20superior%20performance%20over%20the%20U-Net%0Abaseline%2C%20continued.%20In%20this%20study%2C%20we%20demonstrate%20that%20many%20of%20these%20recent%0Aclaims%20fail%20to%20hold%20up%20when%20scrutinized%20for%20common%20validation%20shortcomings%2C%0Asuch%20as%20the%20use%20of%20inadequate%20baselines%2C%20insufficient%20datasets%2C%20and%20neglected%0Acomputational%20resources.%20By%20meticulously%20avoiding%20these%20pitfalls%2C%20we%20conduct%20a%0Athorough%20and%20comprehensive%20benchmarking%20of%20current%20segmentation%20methods%0Aincluding%20CNN-based%2C%20Transformer-based%2C%20and%20Mamba-based%20approaches.%20In%20contrast%0Ato%20current%20beliefs%2C%20we%20find%20that%20the%20recipe%20for%20state-of-the-art%20performance%20is%0A1%29%20employing%20CNN-based%20U-Net%20models%2C%20including%20ResNet%20and%20ConvNeXt%20variants%2C%202%29%0Ausing%20the%20nnU-Net%20framework%2C%20and%203%29%20scaling%20models%20to%20modern%20hardware%0Aresources.%20These%20results%20indicate%20an%20ongoing%20innovation%20bias%20towards%20novel%0Aarchitectures%20in%20the%20field%20and%20underscore%20the%20need%20for%20more%20stringent%0Avalidation%20standards%20in%20the%20quest%20for%20scientific%20progress.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09556v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DnnU-Net%2520Revisited%253A%2520A%2520Call%2520for%2520Rigorous%2520Validation%2520in%25203D%2520Medical%2520Image%250A%2520%2520Segmentation%26entry.906535625%3DFabian%2520Isensee%2520and%2520Tassilo%2520Wald%2520and%2520Constantin%2520Ulrich%2520and%2520Michael%2520Baumgartner%2520and%2520Saikat%2520Roy%2520and%2520Klaus%2520Maier-Hein%2520and%2520Paul%2520F.%2520Jaeger%26entry.1292438233%3D%2520%2520The%2520release%2520of%2520nnU-Net%2520marked%2520a%2520paradigm%2520shift%2520in%25203D%2520medical%2520image%250Asegmentation%252C%2520demonstrating%2520that%2520a%2520properly%2520configured%2520U-Net%2520architecture%2520could%250Astill%2520achieve%2520state-of-the-art%2520results.%2520Despite%2520this%252C%2520the%2520pursuit%2520of%2520novel%250Aarchitectures%252C%2520and%2520the%2520respective%2520claims%2520of%2520superior%2520performance%2520over%2520the%2520U-Net%250Abaseline%252C%2520continued.%2520In%2520this%2520study%252C%2520we%2520demonstrate%2520that%2520many%2520of%2520these%2520recent%250Aclaims%2520fail%2520to%2520hold%2520up%2520when%2520scrutinized%2520for%2520common%2520validation%2520shortcomings%252C%250Asuch%2520as%2520the%2520use%2520of%2520inadequate%2520baselines%252C%2520insufficient%2520datasets%252C%2520and%2520neglected%250Acomputational%2520resources.%2520By%2520meticulously%2520avoiding%2520these%2520pitfalls%252C%2520we%2520conduct%2520a%250Athorough%2520and%2520comprehensive%2520benchmarking%2520of%2520current%2520segmentation%2520methods%250Aincluding%2520CNN-based%252C%2520Transformer-based%252C%2520and%2520Mamba-based%2520approaches.%2520In%2520contrast%250Ato%2520current%2520beliefs%252C%2520we%2520find%2520that%2520the%2520recipe%2520for%2520state-of-the-art%2520performance%2520is%250A1%2529%2520employing%2520CNN-based%2520U-Net%2520models%252C%2520including%2520ResNet%2520and%2520ConvNeXt%2520variants%252C%25202%2529%250Ausing%2520the%2520nnU-Net%2520framework%252C%2520and%25203%2529%2520scaling%2520models%2520to%2520modern%2520hardware%250Aresources.%2520These%2520results%2520indicate%2520an%2520ongoing%2520innovation%2520bias%2520towards%2520novel%250Aarchitectures%2520in%2520the%2520field%2520and%2520underscore%2520the%2520need%2520for%2520more%2520stringent%250Avalidation%2520standards%2520in%2520the%2520quest%2520for%2520scientific%2520progress.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09556v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=nnU-Net%20Revisited%3A%20A%20Call%20for%20Rigorous%20Validation%20in%203D%20Medical%20Image%0A%20%20Segmentation&entry.906535625=Fabian%20Isensee%20and%20Tassilo%20Wald%20and%20Constantin%20Ulrich%20and%20Michael%20Baumgartner%20and%20Saikat%20Roy%20and%20Klaus%20Maier-Hein%20and%20Paul%20F.%20Jaeger&entry.1292438233=%20%20The%20release%20of%20nnU-Net%20marked%20a%20paradigm%20shift%20in%203D%20medical%20image%0Asegmentation%2C%20demonstrating%20that%20a%20properly%20configured%20U-Net%20architecture%20could%0Astill%20achieve%20state-of-the-art%20results.%20Despite%20this%2C%20the%20pursuit%20of%20novel%0Aarchitectures%2C%20and%20the%20respective%20claims%20of%20superior%20performance%20over%20the%20U-Net%0Abaseline%2C%20continued.%20In%20this%20study%2C%20we%20demonstrate%20that%20many%20of%20these%20recent%0Aclaims%20fail%20to%20hold%20up%20when%20scrutinized%20for%20common%20validation%20shortcomings%2C%0Asuch%20as%20the%20use%20of%20inadequate%20baselines%2C%20insufficient%20datasets%2C%20and%20neglected%0Acomputational%20resources.%20By%20meticulously%20avoiding%20these%20pitfalls%2C%20we%20conduct%20a%0Athorough%20and%20comprehensive%20benchmarking%20of%20current%20segmentation%20methods%0Aincluding%20CNN-based%2C%20Transformer-based%2C%20and%20Mamba-based%20approaches.%20In%20contrast%0Ato%20current%20beliefs%2C%20we%20find%20that%20the%20recipe%20for%20state-of-the-art%20performance%20is%0A1%29%20employing%20CNN-based%20U-Net%20models%2C%20including%20ResNet%20and%20ConvNeXt%20variants%2C%202%29%0Ausing%20the%20nnU-Net%20framework%2C%20and%203%29%20scaling%20models%20to%20modern%20hardware%0Aresources.%20These%20results%20indicate%20an%20ongoing%20innovation%20bias%20towards%20novel%0Aarchitectures%20in%20the%20field%20and%20underscore%20the%20need%20for%20more%20stringent%0Avalidation%20standards%20in%20the%20quest%20for%20scientific%20progress.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09556v2&entry.124074799=Read"},
{"title": "Generative Learning of Continuous Data by Tensor Networks", "author": "Alex Meiburg and Jing Chen and Jacob Miller and Rapha\u00eblle Tihon and Guillaume Rabusseau and Alejandro Perdomo-Ortiz", "abstract": "  Beyond their origin in modeling many-body quantum systems, tensor networks\nhave emerged as a promising class of models for solving machine learning\nproblems, notably in unsupervised generative learning. While possessing many\ndesirable features arising from their quantum-inspired nature, tensor network\ngenerative models have previously been largely restricted to binary or\ncategorical data, limiting their utility in real-world modeling problems. We\novercome this by introducing a new family of tensor network generative models\nfor continuous data, which are capable of learning from distributions\ncontaining continuous random variables. We develop our method in the setting of\nmatrix product states, first deriving a universal expressivity theorem proving\nthe ability of this model family to approximate any reasonably smooth\nprobability density function with arbitrary precision. We then benchmark the\nperformance of this model on several synthetic and real-world datasets, finding\nthat the model learns and generalizes well on distributions of continuous and\ndiscrete variables. We develop methods for modeling different data domains, and\nintroduce a trainable compression layer which is found to increase model\nperformance given limited memory or computational resources. Overall, our\nmethods give important theoretical and empirical evidence of the efficacy of\nquantum-inspired methods for the rapidly growing field of generative learning.\n", "link": "http://arxiv.org/abs/2310.20498v2", "date": "2024-07-25", "relevancy": 2.0959, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5461}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5457}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Learning%20of%20Continuous%20Data%20by%20Tensor%20Networks&body=Title%3A%20Generative%20Learning%20of%20Continuous%20Data%20by%20Tensor%20Networks%0AAuthor%3A%20Alex%20Meiburg%20and%20Jing%20Chen%20and%20Jacob%20Miller%20and%20Rapha%C3%ABlle%20Tihon%20and%20Guillaume%20Rabusseau%20and%20Alejandro%20Perdomo-Ortiz%0AAbstract%3A%20%20%20Beyond%20their%20origin%20in%20modeling%20many-body%20quantum%20systems%2C%20tensor%20networks%0Ahave%20emerged%20as%20a%20promising%20class%20of%20models%20for%20solving%20machine%20learning%0Aproblems%2C%20notably%20in%20unsupervised%20generative%20learning.%20While%20possessing%20many%0Adesirable%20features%20arising%20from%20their%20quantum-inspired%20nature%2C%20tensor%20network%0Agenerative%20models%20have%20previously%20been%20largely%20restricted%20to%20binary%20or%0Acategorical%20data%2C%20limiting%20their%20utility%20in%20real-world%20modeling%20problems.%20We%0Aovercome%20this%20by%20introducing%20a%20new%20family%20of%20tensor%20network%20generative%20models%0Afor%20continuous%20data%2C%20which%20are%20capable%20of%20learning%20from%20distributions%0Acontaining%20continuous%20random%20variables.%20We%20develop%20our%20method%20in%20the%20setting%20of%0Amatrix%20product%20states%2C%20first%20deriving%20a%20universal%20expressivity%20theorem%20proving%0Athe%20ability%20of%20this%20model%20family%20to%20approximate%20any%20reasonably%20smooth%0Aprobability%20density%20function%20with%20arbitrary%20precision.%20We%20then%20benchmark%20the%0Aperformance%20of%20this%20model%20on%20several%20synthetic%20and%20real-world%20datasets%2C%20finding%0Athat%20the%20model%20learns%20and%20generalizes%20well%20on%20distributions%20of%20continuous%20and%0Adiscrete%20variables.%20We%20develop%20methods%20for%20modeling%20different%20data%20domains%2C%20and%0Aintroduce%20a%20trainable%20compression%20layer%20which%20is%20found%20to%20increase%20model%0Aperformance%20given%20limited%20memory%20or%20computational%20resources.%20Overall%2C%20our%0Amethods%20give%20important%20theoretical%20and%20empirical%20evidence%20of%20the%20efficacy%20of%0Aquantum-inspired%20methods%20for%20the%20rapidly%20growing%20field%20of%20generative%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.20498v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Learning%2520of%2520Continuous%2520Data%2520by%2520Tensor%2520Networks%26entry.906535625%3DAlex%2520Meiburg%2520and%2520Jing%2520Chen%2520and%2520Jacob%2520Miller%2520and%2520Rapha%25C3%25ABlle%2520Tihon%2520and%2520Guillaume%2520Rabusseau%2520and%2520Alejandro%2520Perdomo-Ortiz%26entry.1292438233%3D%2520%2520Beyond%2520their%2520origin%2520in%2520modeling%2520many-body%2520quantum%2520systems%252C%2520tensor%2520networks%250Ahave%2520emerged%2520as%2520a%2520promising%2520class%2520of%2520models%2520for%2520solving%2520machine%2520learning%250Aproblems%252C%2520notably%2520in%2520unsupervised%2520generative%2520learning.%2520While%2520possessing%2520many%250Adesirable%2520features%2520arising%2520from%2520their%2520quantum-inspired%2520nature%252C%2520tensor%2520network%250Agenerative%2520models%2520have%2520previously%2520been%2520largely%2520restricted%2520to%2520binary%2520or%250Acategorical%2520data%252C%2520limiting%2520their%2520utility%2520in%2520real-world%2520modeling%2520problems.%2520We%250Aovercome%2520this%2520by%2520introducing%2520a%2520new%2520family%2520of%2520tensor%2520network%2520generative%2520models%250Afor%2520continuous%2520data%252C%2520which%2520are%2520capable%2520of%2520learning%2520from%2520distributions%250Acontaining%2520continuous%2520random%2520variables.%2520We%2520develop%2520our%2520method%2520in%2520the%2520setting%2520of%250Amatrix%2520product%2520states%252C%2520first%2520deriving%2520a%2520universal%2520expressivity%2520theorem%2520proving%250Athe%2520ability%2520of%2520this%2520model%2520family%2520to%2520approximate%2520any%2520reasonably%2520smooth%250Aprobability%2520density%2520function%2520with%2520arbitrary%2520precision.%2520We%2520then%2520benchmark%2520the%250Aperformance%2520of%2520this%2520model%2520on%2520several%2520synthetic%2520and%2520real-world%2520datasets%252C%2520finding%250Athat%2520the%2520model%2520learns%2520and%2520generalizes%2520well%2520on%2520distributions%2520of%2520continuous%2520and%250Adiscrete%2520variables.%2520We%2520develop%2520methods%2520for%2520modeling%2520different%2520data%2520domains%252C%2520and%250Aintroduce%2520a%2520trainable%2520compression%2520layer%2520which%2520is%2520found%2520to%2520increase%2520model%250Aperformance%2520given%2520limited%2520memory%2520or%2520computational%2520resources.%2520Overall%252C%2520our%250Amethods%2520give%2520important%2520theoretical%2520and%2520empirical%2520evidence%2520of%2520the%2520efficacy%2520of%250Aquantum-inspired%2520methods%2520for%2520the%2520rapidly%2520growing%2520field%2520of%2520generative%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.20498v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Learning%20of%20Continuous%20Data%20by%20Tensor%20Networks&entry.906535625=Alex%20Meiburg%20and%20Jing%20Chen%20and%20Jacob%20Miller%20and%20Rapha%C3%ABlle%20Tihon%20and%20Guillaume%20Rabusseau%20and%20Alejandro%20Perdomo-Ortiz&entry.1292438233=%20%20Beyond%20their%20origin%20in%20modeling%20many-body%20quantum%20systems%2C%20tensor%20networks%0Ahave%20emerged%20as%20a%20promising%20class%20of%20models%20for%20solving%20machine%20learning%0Aproblems%2C%20notably%20in%20unsupervised%20generative%20learning.%20While%20possessing%20many%0Adesirable%20features%20arising%20from%20their%20quantum-inspired%20nature%2C%20tensor%20network%0Agenerative%20models%20have%20previously%20been%20largely%20restricted%20to%20binary%20or%0Acategorical%20data%2C%20limiting%20their%20utility%20in%20real-world%20modeling%20problems.%20We%0Aovercome%20this%20by%20introducing%20a%20new%20family%20of%20tensor%20network%20generative%20models%0Afor%20continuous%20data%2C%20which%20are%20capable%20of%20learning%20from%20distributions%0Acontaining%20continuous%20random%20variables.%20We%20develop%20our%20method%20in%20the%20setting%20of%0Amatrix%20product%20states%2C%20first%20deriving%20a%20universal%20expressivity%20theorem%20proving%0Athe%20ability%20of%20this%20model%20family%20to%20approximate%20any%20reasonably%20smooth%0Aprobability%20density%20function%20with%20arbitrary%20precision.%20We%20then%20benchmark%20the%0Aperformance%20of%20this%20model%20on%20several%20synthetic%20and%20real-world%20datasets%2C%20finding%0Athat%20the%20model%20learns%20and%20generalizes%20well%20on%20distributions%20of%20continuous%20and%0Adiscrete%20variables.%20We%20develop%20methods%20for%20modeling%20different%20data%20domains%2C%20and%0Aintroduce%20a%20trainable%20compression%20layer%20which%20is%20found%20to%20increase%20model%0Aperformance%20given%20limited%20memory%20or%20computational%20resources.%20Overall%2C%20our%0Amethods%20give%20important%20theoretical%20and%20empirical%20evidence%20of%20the%20efficacy%20of%0Aquantum-inspired%20methods%20for%20the%20rapidly%20growing%20field%20of%20generative%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.20498v2&entry.124074799=Read"},
{"title": "PianoMime: Learning a Generalist, Dexterous Piano Player from Internet\n  Demonstrations", "author": "Cheng Qian and Julen Urain and Kevin Zakka and Jan Peters", "abstract": "  In this work, we introduce PianoMime, a framework for training a\npiano-playing agent using internet demonstrations. The internet is a promising\nsource of large-scale demonstrations for training our robot agents. In\nparticular, for the case of piano-playing, Youtube is full of videos of\nprofessional pianists playing a wide myriad of songs. In our work, we leverage\nthese demonstrations to learn a generalist piano-playing agent capable of\nplaying any arbitrary song. Our framework is divided into three parts: a data\npreparation phase to extract the informative features from the Youtube videos,\na policy learning phase to train song-specific expert policies from the\ndemonstrations and a policy distillation phase to distil the policies into a\nsingle generalist agent. We explore different policy designs to represent the\nagent and evaluate the influence of the amount of training data on the\ngeneralization capability of the agent to novel songs not available in the\ndataset. We show that we are able to learn a policy with up to 56\\% F1 score on\nunseen songs.\n", "link": "http://arxiv.org/abs/2407.18178v1", "date": "2024-07-25", "relevancy": 2.0788, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5488}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.499}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PianoMime%3A%20Learning%20a%20Generalist%2C%20Dexterous%20Piano%20Player%20from%20Internet%0A%20%20Demonstrations&body=Title%3A%20PianoMime%3A%20Learning%20a%20Generalist%2C%20Dexterous%20Piano%20Player%20from%20Internet%0A%20%20Demonstrations%0AAuthor%3A%20Cheng%20Qian%20and%20Julen%20Urain%20and%20Kevin%20Zakka%20and%20Jan%20Peters%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20PianoMime%2C%20a%20framework%20for%20training%20a%0Apiano-playing%20agent%20using%20internet%20demonstrations.%20The%20internet%20is%20a%20promising%0Asource%20of%20large-scale%20demonstrations%20for%20training%20our%20robot%20agents.%20In%0Aparticular%2C%20for%20the%20case%20of%20piano-playing%2C%20Youtube%20is%20full%20of%20videos%20of%0Aprofessional%20pianists%20playing%20a%20wide%20myriad%20of%20songs.%20In%20our%20work%2C%20we%20leverage%0Athese%20demonstrations%20to%20learn%20a%20generalist%20piano-playing%20agent%20capable%20of%0Aplaying%20any%20arbitrary%20song.%20Our%20framework%20is%20divided%20into%20three%20parts%3A%20a%20data%0Apreparation%20phase%20to%20extract%20the%20informative%20features%20from%20the%20Youtube%20videos%2C%0Aa%20policy%20learning%20phase%20to%20train%20song-specific%20expert%20policies%20from%20the%0Ademonstrations%20and%20a%20policy%20distillation%20phase%20to%20distil%20the%20policies%20into%20a%0Asingle%20generalist%20agent.%20We%20explore%20different%20policy%20designs%20to%20represent%20the%0Aagent%20and%20evaluate%20the%20influence%20of%20the%20amount%20of%20training%20data%20on%20the%0Ageneralization%20capability%20of%20the%20agent%20to%20novel%20songs%20not%20available%20in%20the%0Adataset.%20We%20show%20that%20we%20are%20able%20to%20learn%20a%20policy%20with%20up%20to%2056%5C%25%20F1%20score%20on%0Aunseen%20songs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPianoMime%253A%2520Learning%2520a%2520Generalist%252C%2520Dexterous%2520Piano%2520Player%2520from%2520Internet%250A%2520%2520Demonstrations%26entry.906535625%3DCheng%2520Qian%2520and%2520Julen%2520Urain%2520and%2520Kevin%2520Zakka%2520and%2520Jan%2520Peters%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520PianoMime%252C%2520a%2520framework%2520for%2520training%2520a%250Apiano-playing%2520agent%2520using%2520internet%2520demonstrations.%2520The%2520internet%2520is%2520a%2520promising%250Asource%2520of%2520large-scale%2520demonstrations%2520for%2520training%2520our%2520robot%2520agents.%2520In%250Aparticular%252C%2520for%2520the%2520case%2520of%2520piano-playing%252C%2520Youtube%2520is%2520full%2520of%2520videos%2520of%250Aprofessional%2520pianists%2520playing%2520a%2520wide%2520myriad%2520of%2520songs.%2520In%2520our%2520work%252C%2520we%2520leverage%250Athese%2520demonstrations%2520to%2520learn%2520a%2520generalist%2520piano-playing%2520agent%2520capable%2520of%250Aplaying%2520any%2520arbitrary%2520song.%2520Our%2520framework%2520is%2520divided%2520into%2520three%2520parts%253A%2520a%2520data%250Apreparation%2520phase%2520to%2520extract%2520the%2520informative%2520features%2520from%2520the%2520Youtube%2520videos%252C%250Aa%2520policy%2520learning%2520phase%2520to%2520train%2520song-specific%2520expert%2520policies%2520from%2520the%250Ademonstrations%2520and%2520a%2520policy%2520distillation%2520phase%2520to%2520distil%2520the%2520policies%2520into%2520a%250Asingle%2520generalist%2520agent.%2520We%2520explore%2520different%2520policy%2520designs%2520to%2520represent%2520the%250Aagent%2520and%2520evaluate%2520the%2520influence%2520of%2520the%2520amount%2520of%2520training%2520data%2520on%2520the%250Ageneralization%2520capability%2520of%2520the%2520agent%2520to%2520novel%2520songs%2520not%2520available%2520in%2520the%250Adataset.%2520We%2520show%2520that%2520we%2520are%2520able%2520to%2520learn%2520a%2520policy%2520with%2520up%2520to%252056%255C%2525%2520F1%2520score%2520on%250Aunseen%2520songs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PianoMime%3A%20Learning%20a%20Generalist%2C%20Dexterous%20Piano%20Player%20from%20Internet%0A%20%20Demonstrations&entry.906535625=Cheng%20Qian%20and%20Julen%20Urain%20and%20Kevin%20Zakka%20and%20Jan%20Peters&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20PianoMime%2C%20a%20framework%20for%20training%20a%0Apiano-playing%20agent%20using%20internet%20demonstrations.%20The%20internet%20is%20a%20promising%0Asource%20of%20large-scale%20demonstrations%20for%20training%20our%20robot%20agents.%20In%0Aparticular%2C%20for%20the%20case%20of%20piano-playing%2C%20Youtube%20is%20full%20of%20videos%20of%0Aprofessional%20pianists%20playing%20a%20wide%20myriad%20of%20songs.%20In%20our%20work%2C%20we%20leverage%0Athese%20demonstrations%20to%20learn%20a%20generalist%20piano-playing%20agent%20capable%20of%0Aplaying%20any%20arbitrary%20song.%20Our%20framework%20is%20divided%20into%20three%20parts%3A%20a%20data%0Apreparation%20phase%20to%20extract%20the%20informative%20features%20from%20the%20Youtube%20videos%2C%0Aa%20policy%20learning%20phase%20to%20train%20song-specific%20expert%20policies%20from%20the%0Ademonstrations%20and%20a%20policy%20distillation%20phase%20to%20distil%20the%20policies%20into%20a%0Asingle%20generalist%20agent.%20We%20explore%20different%20policy%20designs%20to%20represent%20the%0Aagent%20and%20evaluate%20the%20influence%20of%20the%20amount%20of%20training%20data%20on%20the%0Ageneralization%20capability%20of%20the%20agent%20to%20novel%20songs%20not%20available%20in%20the%0Adataset.%20We%20show%20that%20we%20are%20able%20to%20learn%20a%20policy%20with%20up%20to%2056%5C%25%20F1%20score%20on%0Aunseen%20songs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18178v1&entry.124074799=Read"},
{"title": "Practical X-ray Gastric Cancer Screening Using Refined Stochastic Data\n  Augmentation and Hard Boundary Box Training", "author": "Hideaki Okamoto and Takakiyo Nomura and Kazuhito Nabeshima and Jun Hashimoto and Hitoshi Iyatomi", "abstract": "  Endoscopy is widely used to diagnose gastric cancer and has a high diagnostic\nperformance, but because it must be performed by a physician, the number of\npeople who can be diagnosed is limited. Gastric X-ray, on the other hand, can\nbe performed by technicians and can screen a much larger number of patients\nthan endoscopy, but its correct diagnosis requires experience. We propose an\nunprecedented and practical gastric cancer diagnosis support system for gastric\nX-ray images, which will enable more people to be screened. The system is based\non a general deep learning-based object detection model and includes two novel\ntechnical proposals: refined probabilistic stomach image augmentation (R-sGAIA)\nand hard boundary box learning (HBBT). R-sGAIA is a probabilistic gastric fold\nregion enhancement method that provides more learning patterns for cancer\ndetection models. HBBT is an efficient training method for object detection\nmodels that allows the use of unannotated negative (i.e., healthy control)\nsamples that cannot be used for training in conventional detection models,\nthereby improving model performance. The sensitivity (SE) of the proposed\nsystem for gastric cancer (90.2%) is higher than that of the expert (85.5%),\nand two out of five candidates detected box are cancerous, achieving a high\nprecision while maintaining a high processing speed of 0.51 seconds/image. The\nproposed system showed 5.9 points higher on the F1 score compared to methods\nusing the same object detection model and state-of-the-art data augmentation.\nIn short, the system quickly and efficiently shows the radiologist where to\nlook, greatly reducing the radiologist's workload.\n", "link": "http://arxiv.org/abs/2108.08158v3", "date": "2024-07-25", "relevancy": 2.0702, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5427}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.504}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Practical%20X-ray%20Gastric%20Cancer%20Screening%20Using%20Refined%20Stochastic%20Data%0A%20%20Augmentation%20and%20Hard%20Boundary%20Box%20Training&body=Title%3A%20Practical%20X-ray%20Gastric%20Cancer%20Screening%20Using%20Refined%20Stochastic%20Data%0A%20%20Augmentation%20and%20Hard%20Boundary%20Box%20Training%0AAuthor%3A%20Hideaki%20Okamoto%20and%20Takakiyo%20Nomura%20and%20Kazuhito%20Nabeshima%20and%20Jun%20Hashimoto%20and%20Hitoshi%20Iyatomi%0AAbstract%3A%20%20%20Endoscopy%20is%20widely%20used%20to%20diagnose%20gastric%20cancer%20and%20has%20a%20high%20diagnostic%0Aperformance%2C%20but%20because%20it%20must%20be%20performed%20by%20a%20physician%2C%20the%20number%20of%0Apeople%20who%20can%20be%20diagnosed%20is%20limited.%20Gastric%20X-ray%2C%20on%20the%20other%20hand%2C%20can%0Abe%20performed%20by%20technicians%20and%20can%20screen%20a%20much%20larger%20number%20of%20patients%0Athan%20endoscopy%2C%20but%20its%20correct%20diagnosis%20requires%20experience.%20We%20propose%20an%0Aunprecedented%20and%20practical%20gastric%20cancer%20diagnosis%20support%20system%20for%20gastric%0AX-ray%20images%2C%20which%20will%20enable%20more%20people%20to%20be%20screened.%20The%20system%20is%20based%0Aon%20a%20general%20deep%20learning-based%20object%20detection%20model%20and%20includes%20two%20novel%0Atechnical%20proposals%3A%20refined%20probabilistic%20stomach%20image%20augmentation%20%28R-sGAIA%29%0Aand%20hard%20boundary%20box%20learning%20%28HBBT%29.%20R-sGAIA%20is%20a%20probabilistic%20gastric%20fold%0Aregion%20enhancement%20method%20that%20provides%20more%20learning%20patterns%20for%20cancer%0Adetection%20models.%20HBBT%20is%20an%20efficient%20training%20method%20for%20object%20detection%0Amodels%20that%20allows%20the%20use%20of%20unannotated%20negative%20%28i.e.%2C%20healthy%20control%29%0Asamples%20that%20cannot%20be%20used%20for%20training%20in%20conventional%20detection%20models%2C%0Athereby%20improving%20model%20performance.%20The%20sensitivity%20%28SE%29%20of%20the%20proposed%0Asystem%20for%20gastric%20cancer%20%2890.2%25%29%20is%20higher%20than%20that%20of%20the%20expert%20%2885.5%25%29%2C%0Aand%20two%20out%20of%20five%20candidates%20detected%20box%20are%20cancerous%2C%20achieving%20a%20high%0Aprecision%20while%20maintaining%20a%20high%20processing%20speed%20of%200.51%20seconds/image.%20The%0Aproposed%20system%20showed%205.9%20points%20higher%20on%20the%20F1%20score%20compared%20to%20methods%0Ausing%20the%20same%20object%20detection%20model%20and%20state-of-the-art%20data%20augmentation.%0AIn%20short%2C%20the%20system%20quickly%20and%20efficiently%20shows%20the%20radiologist%20where%20to%0Alook%2C%20greatly%20reducing%20the%20radiologist%27s%20workload.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2108.08158v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPractical%2520X-ray%2520Gastric%2520Cancer%2520Screening%2520Using%2520Refined%2520Stochastic%2520Data%250A%2520%2520Augmentation%2520and%2520Hard%2520Boundary%2520Box%2520Training%26entry.906535625%3DHideaki%2520Okamoto%2520and%2520Takakiyo%2520Nomura%2520and%2520Kazuhito%2520Nabeshima%2520and%2520Jun%2520Hashimoto%2520and%2520Hitoshi%2520Iyatomi%26entry.1292438233%3D%2520%2520Endoscopy%2520is%2520widely%2520used%2520to%2520diagnose%2520gastric%2520cancer%2520and%2520has%2520a%2520high%2520diagnostic%250Aperformance%252C%2520but%2520because%2520it%2520must%2520be%2520performed%2520by%2520a%2520physician%252C%2520the%2520number%2520of%250Apeople%2520who%2520can%2520be%2520diagnosed%2520is%2520limited.%2520Gastric%2520X-ray%252C%2520on%2520the%2520other%2520hand%252C%2520can%250Abe%2520performed%2520by%2520technicians%2520and%2520can%2520screen%2520a%2520much%2520larger%2520number%2520of%2520patients%250Athan%2520endoscopy%252C%2520but%2520its%2520correct%2520diagnosis%2520requires%2520experience.%2520We%2520propose%2520an%250Aunprecedented%2520and%2520practical%2520gastric%2520cancer%2520diagnosis%2520support%2520system%2520for%2520gastric%250AX-ray%2520images%252C%2520which%2520will%2520enable%2520more%2520people%2520to%2520be%2520screened.%2520The%2520system%2520is%2520based%250Aon%2520a%2520general%2520deep%2520learning-based%2520object%2520detection%2520model%2520and%2520includes%2520two%2520novel%250Atechnical%2520proposals%253A%2520refined%2520probabilistic%2520stomach%2520image%2520augmentation%2520%2528R-sGAIA%2529%250Aand%2520hard%2520boundary%2520box%2520learning%2520%2528HBBT%2529.%2520R-sGAIA%2520is%2520a%2520probabilistic%2520gastric%2520fold%250Aregion%2520enhancement%2520method%2520that%2520provides%2520more%2520learning%2520patterns%2520for%2520cancer%250Adetection%2520models.%2520HBBT%2520is%2520an%2520efficient%2520training%2520method%2520for%2520object%2520detection%250Amodels%2520that%2520allows%2520the%2520use%2520of%2520unannotated%2520negative%2520%2528i.e.%252C%2520healthy%2520control%2529%250Asamples%2520that%2520cannot%2520be%2520used%2520for%2520training%2520in%2520conventional%2520detection%2520models%252C%250Athereby%2520improving%2520model%2520performance.%2520The%2520sensitivity%2520%2528SE%2529%2520of%2520the%2520proposed%250Asystem%2520for%2520gastric%2520cancer%2520%252890.2%2525%2529%2520is%2520higher%2520than%2520that%2520of%2520the%2520expert%2520%252885.5%2525%2529%252C%250Aand%2520two%2520out%2520of%2520five%2520candidates%2520detected%2520box%2520are%2520cancerous%252C%2520achieving%2520a%2520high%250Aprecision%2520while%2520maintaining%2520a%2520high%2520processing%2520speed%2520of%25200.51%2520seconds/image.%2520The%250Aproposed%2520system%2520showed%25205.9%2520points%2520higher%2520on%2520the%2520F1%2520score%2520compared%2520to%2520methods%250Ausing%2520the%2520same%2520object%2520detection%2520model%2520and%2520state-of-the-art%2520data%2520augmentation.%250AIn%2520short%252C%2520the%2520system%2520quickly%2520and%2520efficiently%2520shows%2520the%2520radiologist%2520where%2520to%250Alook%252C%2520greatly%2520reducing%2520the%2520radiologist%2527s%2520workload.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2108.08158v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Practical%20X-ray%20Gastric%20Cancer%20Screening%20Using%20Refined%20Stochastic%20Data%0A%20%20Augmentation%20and%20Hard%20Boundary%20Box%20Training&entry.906535625=Hideaki%20Okamoto%20and%20Takakiyo%20Nomura%20and%20Kazuhito%20Nabeshima%20and%20Jun%20Hashimoto%20and%20Hitoshi%20Iyatomi&entry.1292438233=%20%20Endoscopy%20is%20widely%20used%20to%20diagnose%20gastric%20cancer%20and%20has%20a%20high%20diagnostic%0Aperformance%2C%20but%20because%20it%20must%20be%20performed%20by%20a%20physician%2C%20the%20number%20of%0Apeople%20who%20can%20be%20diagnosed%20is%20limited.%20Gastric%20X-ray%2C%20on%20the%20other%20hand%2C%20can%0Abe%20performed%20by%20technicians%20and%20can%20screen%20a%20much%20larger%20number%20of%20patients%0Athan%20endoscopy%2C%20but%20its%20correct%20diagnosis%20requires%20experience.%20We%20propose%20an%0Aunprecedented%20and%20practical%20gastric%20cancer%20diagnosis%20support%20system%20for%20gastric%0AX-ray%20images%2C%20which%20will%20enable%20more%20people%20to%20be%20screened.%20The%20system%20is%20based%0Aon%20a%20general%20deep%20learning-based%20object%20detection%20model%20and%20includes%20two%20novel%0Atechnical%20proposals%3A%20refined%20probabilistic%20stomach%20image%20augmentation%20%28R-sGAIA%29%0Aand%20hard%20boundary%20box%20learning%20%28HBBT%29.%20R-sGAIA%20is%20a%20probabilistic%20gastric%20fold%0Aregion%20enhancement%20method%20that%20provides%20more%20learning%20patterns%20for%20cancer%0Adetection%20models.%20HBBT%20is%20an%20efficient%20training%20method%20for%20object%20detection%0Amodels%20that%20allows%20the%20use%20of%20unannotated%20negative%20%28i.e.%2C%20healthy%20control%29%0Asamples%20that%20cannot%20be%20used%20for%20training%20in%20conventional%20detection%20models%2C%0Athereby%20improving%20model%20performance.%20The%20sensitivity%20%28SE%29%20of%20the%20proposed%0Asystem%20for%20gastric%20cancer%20%2890.2%25%29%20is%20higher%20than%20that%20of%20the%20expert%20%2885.5%25%29%2C%0Aand%20two%20out%20of%20five%20candidates%20detected%20box%20are%20cancerous%2C%20achieving%20a%20high%0Aprecision%20while%20maintaining%20a%20high%20processing%20speed%20of%200.51%20seconds/image.%20The%0Aproposed%20system%20showed%205.9%20points%20higher%20on%20the%20F1%20score%20compared%20to%20methods%0Ausing%20the%20same%20object%20detection%20model%20and%20state-of-the-art%20data%20augmentation.%0AIn%20short%2C%20the%20system%20quickly%20and%20efficiently%20shows%20the%20radiologist%20where%20to%0Alook%2C%20greatly%20reducing%20the%20radiologist%27s%20workload.%0A&entry.1838667208=http%3A//arxiv.org/abs/2108.08158v3&entry.124074799=Read"},
{"title": "Learning mental states estimation through self-observation: a\n  developmental synergy between intentions and beliefs representations in a\n  deep-learning model of Theory of Mind", "author": "Francesca Bianco and Silvia Rigato and Maria Laura Filippetti and Dimitri Ognibene", "abstract": "  Theory of Mind (ToM), the ability to attribute beliefs, intentions, or mental\nstates to others, is a crucial feature of human social interaction. In complex\nenvironments, where the human sensory system reaches its limits, behaviour is\nstrongly driven by our beliefs about the state of the world around us.\nAccessing others' mental states, e.g., beliefs and intentions, allows for more\neffective social interactions in natural contexts. Yet, these variables are not\ndirectly observable, making understanding ToM a challenging quest of interest\nfor different fields, including psychology, machine learning and robotics. In\nthis paper, we contribute to this topic by showing a developmental synergy\nbetween learning to predict low-level mental states (e.g., intentions, goals)\nand attributing high-level ones (i.e., beliefs). Specifically, we assume that\nlearning beliefs attribution can occur by observing one's own decision\nprocesses involving beliefs, e.g., in a partially observable environment. Using\na simple feed-forward deep learning model, we show that, when learning to\npredict others' intentions and actions, more accurate predictions can be\nacquired earlier if beliefs attribution is learnt simultaneously. Furthermore,\nwe show that the learning performance improves even when observed actors have a\ndifferent embodiment than the observer and the gain is higher when observing\nbeliefs-driven chunks of behaviour. We propose that our computational approach\ncan inform the understanding of human social cognitive development and be\nrelevant for the design of future adaptive social robots able to autonomously\nunderstand, assist, and learn from human interaction partners in novel natural\nenvironments and tasks.\n", "link": "http://arxiv.org/abs/2407.18022v1", "date": "2024-07-25", "relevancy": 2.0497, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5915}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5056}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20mental%20states%20estimation%20through%20self-observation%3A%20a%0A%20%20developmental%20synergy%20between%20intentions%20and%20beliefs%20representations%20in%20a%0A%20%20deep-learning%20model%20of%20Theory%20of%20Mind&body=Title%3A%20Learning%20mental%20states%20estimation%20through%20self-observation%3A%20a%0A%20%20developmental%20synergy%20between%20intentions%20and%20beliefs%20representations%20in%20a%0A%20%20deep-learning%20model%20of%20Theory%20of%20Mind%0AAuthor%3A%20Francesca%20Bianco%20and%20Silvia%20Rigato%20and%20Maria%20Laura%20Filippetti%20and%20Dimitri%20Ognibene%0AAbstract%3A%20%20%20Theory%20of%20Mind%20%28ToM%29%2C%20the%20ability%20to%20attribute%20beliefs%2C%20intentions%2C%20or%20mental%0Astates%20to%20others%2C%20is%20a%20crucial%20feature%20of%20human%20social%20interaction.%20In%20complex%0Aenvironments%2C%20where%20the%20human%20sensory%20system%20reaches%20its%20limits%2C%20behaviour%20is%0Astrongly%20driven%20by%20our%20beliefs%20about%20the%20state%20of%20the%20world%20around%20us.%0AAccessing%20others%27%20mental%20states%2C%20e.g.%2C%20beliefs%20and%20intentions%2C%20allows%20for%20more%0Aeffective%20social%20interactions%20in%20natural%20contexts.%20Yet%2C%20these%20variables%20are%20not%0Adirectly%20observable%2C%20making%20understanding%20ToM%20a%20challenging%20quest%20of%20interest%0Afor%20different%20fields%2C%20including%20psychology%2C%20machine%20learning%20and%20robotics.%20In%0Athis%20paper%2C%20we%20contribute%20to%20this%20topic%20by%20showing%20a%20developmental%20synergy%0Abetween%20learning%20to%20predict%20low-level%20mental%20states%20%28e.g.%2C%20intentions%2C%20goals%29%0Aand%20attributing%20high-level%20ones%20%28i.e.%2C%20beliefs%29.%20Specifically%2C%20we%20assume%20that%0Alearning%20beliefs%20attribution%20can%20occur%20by%20observing%20one%27s%20own%20decision%0Aprocesses%20involving%20beliefs%2C%20e.g.%2C%20in%20a%20partially%20observable%20environment.%20Using%0Aa%20simple%20feed-forward%20deep%20learning%20model%2C%20we%20show%20that%2C%20when%20learning%20to%0Apredict%20others%27%20intentions%20and%20actions%2C%20more%20accurate%20predictions%20can%20be%0Aacquired%20earlier%20if%20beliefs%20attribution%20is%20learnt%20simultaneously.%20Furthermore%2C%0Awe%20show%20that%20the%20learning%20performance%20improves%20even%20when%20observed%20actors%20have%20a%0Adifferent%20embodiment%20than%20the%20observer%20and%20the%20gain%20is%20higher%20when%20observing%0Abeliefs-driven%20chunks%20of%20behaviour.%20We%20propose%20that%20our%20computational%20approach%0Acan%20inform%20the%20understanding%20of%20human%20social%20cognitive%20development%20and%20be%0Arelevant%20for%20the%20design%20of%20future%20adaptive%20social%20robots%20able%20to%20autonomously%0Aunderstand%2C%20assist%2C%20and%20learn%20from%20human%20interaction%20partners%20in%20novel%20natural%0Aenvironments%20and%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18022v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520mental%2520states%2520estimation%2520through%2520self-observation%253A%2520a%250A%2520%2520developmental%2520synergy%2520between%2520intentions%2520and%2520beliefs%2520representations%2520in%2520a%250A%2520%2520deep-learning%2520model%2520of%2520Theory%2520of%2520Mind%26entry.906535625%3DFrancesca%2520Bianco%2520and%2520Silvia%2520Rigato%2520and%2520Maria%2520Laura%2520Filippetti%2520and%2520Dimitri%2520Ognibene%26entry.1292438233%3D%2520%2520Theory%2520of%2520Mind%2520%2528ToM%2529%252C%2520the%2520ability%2520to%2520attribute%2520beliefs%252C%2520intentions%252C%2520or%2520mental%250Astates%2520to%2520others%252C%2520is%2520a%2520crucial%2520feature%2520of%2520human%2520social%2520interaction.%2520In%2520complex%250Aenvironments%252C%2520where%2520the%2520human%2520sensory%2520system%2520reaches%2520its%2520limits%252C%2520behaviour%2520is%250Astrongly%2520driven%2520by%2520our%2520beliefs%2520about%2520the%2520state%2520of%2520the%2520world%2520around%2520us.%250AAccessing%2520others%2527%2520mental%2520states%252C%2520e.g.%252C%2520beliefs%2520and%2520intentions%252C%2520allows%2520for%2520more%250Aeffective%2520social%2520interactions%2520in%2520natural%2520contexts.%2520Yet%252C%2520these%2520variables%2520are%2520not%250Adirectly%2520observable%252C%2520making%2520understanding%2520ToM%2520a%2520challenging%2520quest%2520of%2520interest%250Afor%2520different%2520fields%252C%2520including%2520psychology%252C%2520machine%2520learning%2520and%2520robotics.%2520In%250Athis%2520paper%252C%2520we%2520contribute%2520to%2520this%2520topic%2520by%2520showing%2520a%2520developmental%2520synergy%250Abetween%2520learning%2520to%2520predict%2520low-level%2520mental%2520states%2520%2528e.g.%252C%2520intentions%252C%2520goals%2529%250Aand%2520attributing%2520high-level%2520ones%2520%2528i.e.%252C%2520beliefs%2529.%2520Specifically%252C%2520we%2520assume%2520that%250Alearning%2520beliefs%2520attribution%2520can%2520occur%2520by%2520observing%2520one%2527s%2520own%2520decision%250Aprocesses%2520involving%2520beliefs%252C%2520e.g.%252C%2520in%2520a%2520partially%2520observable%2520environment.%2520Using%250Aa%2520simple%2520feed-forward%2520deep%2520learning%2520model%252C%2520we%2520show%2520that%252C%2520when%2520learning%2520to%250Apredict%2520others%2527%2520intentions%2520and%2520actions%252C%2520more%2520accurate%2520predictions%2520can%2520be%250Aacquired%2520earlier%2520if%2520beliefs%2520attribution%2520is%2520learnt%2520simultaneously.%2520Furthermore%252C%250Awe%2520show%2520that%2520the%2520learning%2520performance%2520improves%2520even%2520when%2520observed%2520actors%2520have%2520a%250Adifferent%2520embodiment%2520than%2520the%2520observer%2520and%2520the%2520gain%2520is%2520higher%2520when%2520observing%250Abeliefs-driven%2520chunks%2520of%2520behaviour.%2520We%2520propose%2520that%2520our%2520computational%2520approach%250Acan%2520inform%2520the%2520understanding%2520of%2520human%2520social%2520cognitive%2520development%2520and%2520be%250Arelevant%2520for%2520the%2520design%2520of%2520future%2520adaptive%2520social%2520robots%2520able%2520to%2520autonomously%250Aunderstand%252C%2520assist%252C%2520and%2520learn%2520from%2520human%2520interaction%2520partners%2520in%2520novel%2520natural%250Aenvironments%2520and%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18022v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20mental%20states%20estimation%20through%20self-observation%3A%20a%0A%20%20developmental%20synergy%20between%20intentions%20and%20beliefs%20representations%20in%20a%0A%20%20deep-learning%20model%20of%20Theory%20of%20Mind&entry.906535625=Francesca%20Bianco%20and%20Silvia%20Rigato%20and%20Maria%20Laura%20Filippetti%20and%20Dimitri%20Ognibene&entry.1292438233=%20%20Theory%20of%20Mind%20%28ToM%29%2C%20the%20ability%20to%20attribute%20beliefs%2C%20intentions%2C%20or%20mental%0Astates%20to%20others%2C%20is%20a%20crucial%20feature%20of%20human%20social%20interaction.%20In%20complex%0Aenvironments%2C%20where%20the%20human%20sensory%20system%20reaches%20its%20limits%2C%20behaviour%20is%0Astrongly%20driven%20by%20our%20beliefs%20about%20the%20state%20of%20the%20world%20around%20us.%0AAccessing%20others%27%20mental%20states%2C%20e.g.%2C%20beliefs%20and%20intentions%2C%20allows%20for%20more%0Aeffective%20social%20interactions%20in%20natural%20contexts.%20Yet%2C%20these%20variables%20are%20not%0Adirectly%20observable%2C%20making%20understanding%20ToM%20a%20challenging%20quest%20of%20interest%0Afor%20different%20fields%2C%20including%20psychology%2C%20machine%20learning%20and%20robotics.%20In%0Athis%20paper%2C%20we%20contribute%20to%20this%20topic%20by%20showing%20a%20developmental%20synergy%0Abetween%20learning%20to%20predict%20low-level%20mental%20states%20%28e.g.%2C%20intentions%2C%20goals%29%0Aand%20attributing%20high-level%20ones%20%28i.e.%2C%20beliefs%29.%20Specifically%2C%20we%20assume%20that%0Alearning%20beliefs%20attribution%20can%20occur%20by%20observing%20one%27s%20own%20decision%0Aprocesses%20involving%20beliefs%2C%20e.g.%2C%20in%20a%20partially%20observable%20environment.%20Using%0Aa%20simple%20feed-forward%20deep%20learning%20model%2C%20we%20show%20that%2C%20when%20learning%20to%0Apredict%20others%27%20intentions%20and%20actions%2C%20more%20accurate%20predictions%20can%20be%0Aacquired%20earlier%20if%20beliefs%20attribution%20is%20learnt%20simultaneously.%20Furthermore%2C%0Awe%20show%20that%20the%20learning%20performance%20improves%20even%20when%20observed%20actors%20have%20a%0Adifferent%20embodiment%20than%20the%20observer%20and%20the%20gain%20is%20higher%20when%20observing%0Abeliefs-driven%20chunks%20of%20behaviour.%20We%20propose%20that%20our%20computational%20approach%0Acan%20inform%20the%20understanding%20of%20human%20social%20cognitive%20development%20and%20be%0Arelevant%20for%20the%20design%20of%20future%20adaptive%20social%20robots%20able%20to%20autonomously%0Aunderstand%2C%20assist%2C%20and%20learn%20from%20human%20interaction%20partners%20in%20novel%20natural%0Aenvironments%20and%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18022v1&entry.124074799=Read"},
{"title": "Looking at Model Debiasing through the Lens of Anomaly Detection", "author": "Vito Paolo Pastore and Massimiliano Ciranni and Davide Marinelli and Francesca Odone and Vittorio Murino", "abstract": "  It is widely recognized that deep neural networks are sensitive to bias in\nthe data. This means that during training these models are likely to learn\nspurious correlations between data and labels, resulting in limited\ngeneralization abilities and low performance. In this context, model debiasing\napproaches can be devised aiming at reducing the model's dependency on such\nunwanted correlations, either leveraging the knowledge of bias information or\nnot. In this work, we focus on the latter and more realistic scenario, showing\nthe importance of accurately predicting the bias-conflicting and bias-aligned\nsamples to obtain compelling performance in bias mitigation. On this ground, we\npropose to conceive the problem of model bias from an out-of-distribution\nperspective, introducing a new bias identification method based on anomaly\ndetection. We claim that when data is mostly biased, bias-conflicting samples\ncan be regarded as outliers with respect to the bias-aligned distribution in\nthe feature space of a biased model, thus allowing for precisely detecting them\nwith an anomaly detection method. Coupling the proposed bias identification\napproach with bias-conflicting data upsampling and augmentation in a two-step\nstrategy, we reach state-of-the-art performance on synthetic and real benchmark\ndatasets. Ultimately, our proposed approach shows that the data bias issue does\nnot necessarily require complex debiasing methods, given that an accurate bias\nidentification procedure is defined.\n", "link": "http://arxiv.org/abs/2407.17449v2", "date": "2024-07-25", "relevancy": 2.0462, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5162}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5111}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Looking%20at%20Model%20Debiasing%20through%20the%20Lens%20of%20Anomaly%20Detection&body=Title%3A%20Looking%20at%20Model%20Debiasing%20through%20the%20Lens%20of%20Anomaly%20Detection%0AAuthor%3A%20Vito%20Paolo%20Pastore%20and%20Massimiliano%20Ciranni%20and%20Davide%20Marinelli%20and%20Francesca%20Odone%20and%20Vittorio%20Murino%0AAbstract%3A%20%20%20It%20is%20widely%20recognized%20that%20deep%20neural%20networks%20are%20sensitive%20to%20bias%20in%0Athe%20data.%20This%20means%20that%20during%20training%20these%20models%20are%20likely%20to%20learn%0Aspurious%20correlations%20between%20data%20and%20labels%2C%20resulting%20in%20limited%0Ageneralization%20abilities%20and%20low%20performance.%20In%20this%20context%2C%20model%20debiasing%0Aapproaches%20can%20be%20devised%20aiming%20at%20reducing%20the%20model%27s%20dependency%20on%20such%0Aunwanted%20correlations%2C%20either%20leveraging%20the%20knowledge%20of%20bias%20information%20or%0Anot.%20In%20this%20work%2C%20we%20focus%20on%20the%20latter%20and%20more%20realistic%20scenario%2C%20showing%0Athe%20importance%20of%20accurately%20predicting%20the%20bias-conflicting%20and%20bias-aligned%0Asamples%20to%20obtain%20compelling%20performance%20in%20bias%20mitigation.%20On%20this%20ground%2C%20we%0Apropose%20to%20conceive%20the%20problem%20of%20model%20bias%20from%20an%20out-of-distribution%0Aperspective%2C%20introducing%20a%20new%20bias%20identification%20method%20based%20on%20anomaly%0Adetection.%20We%20claim%20that%20when%20data%20is%20mostly%20biased%2C%20bias-conflicting%20samples%0Acan%20be%20regarded%20as%20outliers%20with%20respect%20to%20the%20bias-aligned%20distribution%20in%0Athe%20feature%20space%20of%20a%20biased%20model%2C%20thus%20allowing%20for%20precisely%20detecting%20them%0Awith%20an%20anomaly%20detection%20method.%20Coupling%20the%20proposed%20bias%20identification%0Aapproach%20with%20bias-conflicting%20data%20upsampling%20and%20augmentation%20in%20a%20two-step%0Astrategy%2C%20we%20reach%20state-of-the-art%20performance%20on%20synthetic%20and%20real%20benchmark%0Adatasets.%20Ultimately%2C%20our%20proposed%20approach%20shows%20that%20the%20data%20bias%20issue%20does%0Anot%20necessarily%20require%20complex%20debiasing%20methods%2C%20given%20that%20an%20accurate%20bias%0Aidentification%20procedure%20is%20defined.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17449v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLooking%2520at%2520Model%2520Debiasing%2520through%2520the%2520Lens%2520of%2520Anomaly%2520Detection%26entry.906535625%3DVito%2520Paolo%2520Pastore%2520and%2520Massimiliano%2520Ciranni%2520and%2520Davide%2520Marinelli%2520and%2520Francesca%2520Odone%2520and%2520Vittorio%2520Murino%26entry.1292438233%3D%2520%2520It%2520is%2520widely%2520recognized%2520that%2520deep%2520neural%2520networks%2520are%2520sensitive%2520to%2520bias%2520in%250Athe%2520data.%2520This%2520means%2520that%2520during%2520training%2520these%2520models%2520are%2520likely%2520to%2520learn%250Aspurious%2520correlations%2520between%2520data%2520and%2520labels%252C%2520resulting%2520in%2520limited%250Ageneralization%2520abilities%2520and%2520low%2520performance.%2520In%2520this%2520context%252C%2520model%2520debiasing%250Aapproaches%2520can%2520be%2520devised%2520aiming%2520at%2520reducing%2520the%2520model%2527s%2520dependency%2520on%2520such%250Aunwanted%2520correlations%252C%2520either%2520leveraging%2520the%2520knowledge%2520of%2520bias%2520information%2520or%250Anot.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520the%2520latter%2520and%2520more%2520realistic%2520scenario%252C%2520showing%250Athe%2520importance%2520of%2520accurately%2520predicting%2520the%2520bias-conflicting%2520and%2520bias-aligned%250Asamples%2520to%2520obtain%2520compelling%2520performance%2520in%2520bias%2520mitigation.%2520On%2520this%2520ground%252C%2520we%250Apropose%2520to%2520conceive%2520the%2520problem%2520of%2520model%2520bias%2520from%2520an%2520out-of-distribution%250Aperspective%252C%2520introducing%2520a%2520new%2520bias%2520identification%2520method%2520based%2520on%2520anomaly%250Adetection.%2520We%2520claim%2520that%2520when%2520data%2520is%2520mostly%2520biased%252C%2520bias-conflicting%2520samples%250Acan%2520be%2520regarded%2520as%2520outliers%2520with%2520respect%2520to%2520the%2520bias-aligned%2520distribution%2520in%250Athe%2520feature%2520space%2520of%2520a%2520biased%2520model%252C%2520thus%2520allowing%2520for%2520precisely%2520detecting%2520them%250Awith%2520an%2520anomaly%2520detection%2520method.%2520Coupling%2520the%2520proposed%2520bias%2520identification%250Aapproach%2520with%2520bias-conflicting%2520data%2520upsampling%2520and%2520augmentation%2520in%2520a%2520two-step%250Astrategy%252C%2520we%2520reach%2520state-of-the-art%2520performance%2520on%2520synthetic%2520and%2520real%2520benchmark%250Adatasets.%2520Ultimately%252C%2520our%2520proposed%2520approach%2520shows%2520that%2520the%2520data%2520bias%2520issue%2520does%250Anot%2520necessarily%2520require%2520complex%2520debiasing%2520methods%252C%2520given%2520that%2520an%2520accurate%2520bias%250Aidentification%2520procedure%2520is%2520defined.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17449v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Looking%20at%20Model%20Debiasing%20through%20the%20Lens%20of%20Anomaly%20Detection&entry.906535625=Vito%20Paolo%20Pastore%20and%20Massimiliano%20Ciranni%20and%20Davide%20Marinelli%20and%20Francesca%20Odone%20and%20Vittorio%20Murino&entry.1292438233=%20%20It%20is%20widely%20recognized%20that%20deep%20neural%20networks%20are%20sensitive%20to%20bias%20in%0Athe%20data.%20This%20means%20that%20during%20training%20these%20models%20are%20likely%20to%20learn%0Aspurious%20correlations%20between%20data%20and%20labels%2C%20resulting%20in%20limited%0Ageneralization%20abilities%20and%20low%20performance.%20In%20this%20context%2C%20model%20debiasing%0Aapproaches%20can%20be%20devised%20aiming%20at%20reducing%20the%20model%27s%20dependency%20on%20such%0Aunwanted%20correlations%2C%20either%20leveraging%20the%20knowledge%20of%20bias%20information%20or%0Anot.%20In%20this%20work%2C%20we%20focus%20on%20the%20latter%20and%20more%20realistic%20scenario%2C%20showing%0Athe%20importance%20of%20accurately%20predicting%20the%20bias-conflicting%20and%20bias-aligned%0Asamples%20to%20obtain%20compelling%20performance%20in%20bias%20mitigation.%20On%20this%20ground%2C%20we%0Apropose%20to%20conceive%20the%20problem%20of%20model%20bias%20from%20an%20out-of-distribution%0Aperspective%2C%20introducing%20a%20new%20bias%20identification%20method%20based%20on%20anomaly%0Adetection.%20We%20claim%20that%20when%20data%20is%20mostly%20biased%2C%20bias-conflicting%20samples%0Acan%20be%20regarded%20as%20outliers%20with%20respect%20to%20the%20bias-aligned%20distribution%20in%0Athe%20feature%20space%20of%20a%20biased%20model%2C%20thus%20allowing%20for%20precisely%20detecting%20them%0Awith%20an%20anomaly%20detection%20method.%20Coupling%20the%20proposed%20bias%20identification%0Aapproach%20with%20bias-conflicting%20data%20upsampling%20and%20augmentation%20in%20a%20two-step%0Astrategy%2C%20we%20reach%20state-of-the-art%20performance%20on%20synthetic%20and%20real%20benchmark%0Adatasets.%20Ultimately%2C%20our%20proposed%20approach%20shows%20that%20the%20data%20bias%20issue%20does%0Anot%20necessarily%20require%20complex%20debiasing%20methods%2C%20given%20that%20an%20accurate%20bias%0Aidentification%20procedure%20is%20defined.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17449v2&entry.124074799=Read"},
{"title": "Egocentric Robots in a Human-Centric World? Exploring\n  Group-Robot-Interaction in Public Spaces", "author": "Ana M\u00fcller and Anja Richert", "abstract": "  The deployment of social robots in real-world scenarios is increasing,\nsupporting humans in various contexts. However, they still struggle to grasp\nsocial dynamics, especially in public spaces, sometimes resulting in violations\nof social norms, such as interrupting human conversations. This behavior,\noriginating from a limited processing of social norms, might be perceived as\nrobot-centered. Understanding social dynamics, particularly in\ngroup-robot-interactions (GRI), underscores the need for further research and\ndevelopment in human-robot-interaction (HRI). Enhancing the interaction\nabilities of social robots, especially in GRIs, can improve their effectiveness\nin real-world applications on a micro-level, as group interactions lead to\nincreased motivation and comfort. In this study, we assessed the influence of\nthe interaction condition (dyadic vs. triadic) on the perceived extraversion\n(ext.) of social robots in public spaces. The research involved 40 HRIs,\nincluding 24 dyadic (i.e., one human and one robot) interactions and 16 triadic\ninteractions, which involve at least three entities, including the robot.\n", "link": "http://arxiv.org/abs/2407.18009v1", "date": "2024-07-25", "relevancy": 2.0412, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5766}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5079}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Egocentric%20Robots%20in%20a%20Human-Centric%20World%3F%20Exploring%0A%20%20Group-Robot-Interaction%20in%20Public%20Spaces&body=Title%3A%20Egocentric%20Robots%20in%20a%20Human-Centric%20World%3F%20Exploring%0A%20%20Group-Robot-Interaction%20in%20Public%20Spaces%0AAuthor%3A%20Ana%20M%C3%BCller%20and%20Anja%20Richert%0AAbstract%3A%20%20%20The%20deployment%20of%20social%20robots%20in%20real-world%20scenarios%20is%20increasing%2C%0Asupporting%20humans%20in%20various%20contexts.%20However%2C%20they%20still%20struggle%20to%20grasp%0Asocial%20dynamics%2C%20especially%20in%20public%20spaces%2C%20sometimes%20resulting%20in%20violations%0Aof%20social%20norms%2C%20such%20as%20interrupting%20human%20conversations.%20This%20behavior%2C%0Aoriginating%20from%20a%20limited%20processing%20of%20social%20norms%2C%20might%20be%20perceived%20as%0Arobot-centered.%20Understanding%20social%20dynamics%2C%20particularly%20in%0Agroup-robot-interactions%20%28GRI%29%2C%20underscores%20the%20need%20for%20further%20research%20and%0Adevelopment%20in%20human-robot-interaction%20%28HRI%29.%20Enhancing%20the%20interaction%0Aabilities%20of%20social%20robots%2C%20especially%20in%20GRIs%2C%20can%20improve%20their%20effectiveness%0Ain%20real-world%20applications%20on%20a%20micro-level%2C%20as%20group%20interactions%20lead%20to%0Aincreased%20motivation%20and%20comfort.%20In%20this%20study%2C%20we%20assessed%20the%20influence%20of%0Athe%20interaction%20condition%20%28dyadic%20vs.%20triadic%29%20on%20the%20perceived%20extraversion%0A%28ext.%29%20of%20social%20robots%20in%20public%20spaces.%20The%20research%20involved%2040%20HRIs%2C%0Aincluding%2024%20dyadic%20%28i.e.%2C%20one%20human%20and%20one%20robot%29%20interactions%20and%2016%20triadic%0Ainteractions%2C%20which%20involve%20at%20least%20three%20entities%2C%20including%20the%20robot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18009v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgocentric%2520Robots%2520in%2520a%2520Human-Centric%2520World%253F%2520Exploring%250A%2520%2520Group-Robot-Interaction%2520in%2520Public%2520Spaces%26entry.906535625%3DAna%2520M%25C3%25BCller%2520and%2520Anja%2520Richert%26entry.1292438233%3D%2520%2520The%2520deployment%2520of%2520social%2520robots%2520in%2520real-world%2520scenarios%2520is%2520increasing%252C%250Asupporting%2520humans%2520in%2520various%2520contexts.%2520However%252C%2520they%2520still%2520struggle%2520to%2520grasp%250Asocial%2520dynamics%252C%2520especially%2520in%2520public%2520spaces%252C%2520sometimes%2520resulting%2520in%2520violations%250Aof%2520social%2520norms%252C%2520such%2520as%2520interrupting%2520human%2520conversations.%2520This%2520behavior%252C%250Aoriginating%2520from%2520a%2520limited%2520processing%2520of%2520social%2520norms%252C%2520might%2520be%2520perceived%2520as%250Arobot-centered.%2520Understanding%2520social%2520dynamics%252C%2520particularly%2520in%250Agroup-robot-interactions%2520%2528GRI%2529%252C%2520underscores%2520the%2520need%2520for%2520further%2520research%2520and%250Adevelopment%2520in%2520human-robot-interaction%2520%2528HRI%2529.%2520Enhancing%2520the%2520interaction%250Aabilities%2520of%2520social%2520robots%252C%2520especially%2520in%2520GRIs%252C%2520can%2520improve%2520their%2520effectiveness%250Ain%2520real-world%2520applications%2520on%2520a%2520micro-level%252C%2520as%2520group%2520interactions%2520lead%2520to%250Aincreased%2520motivation%2520and%2520comfort.%2520In%2520this%2520study%252C%2520we%2520assessed%2520the%2520influence%2520of%250Athe%2520interaction%2520condition%2520%2528dyadic%2520vs.%2520triadic%2529%2520on%2520the%2520perceived%2520extraversion%250A%2528ext.%2529%2520of%2520social%2520robots%2520in%2520public%2520spaces.%2520The%2520research%2520involved%252040%2520HRIs%252C%250Aincluding%252024%2520dyadic%2520%2528i.e.%252C%2520one%2520human%2520and%2520one%2520robot%2529%2520interactions%2520and%252016%2520triadic%250Ainteractions%252C%2520which%2520involve%2520at%2520least%2520three%2520entities%252C%2520including%2520the%2520robot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18009v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Egocentric%20Robots%20in%20a%20Human-Centric%20World%3F%20Exploring%0A%20%20Group-Robot-Interaction%20in%20Public%20Spaces&entry.906535625=Ana%20M%C3%BCller%20and%20Anja%20Richert&entry.1292438233=%20%20The%20deployment%20of%20social%20robots%20in%20real-world%20scenarios%20is%20increasing%2C%0Asupporting%20humans%20in%20various%20contexts.%20However%2C%20they%20still%20struggle%20to%20grasp%0Asocial%20dynamics%2C%20especially%20in%20public%20spaces%2C%20sometimes%20resulting%20in%20violations%0Aof%20social%20norms%2C%20such%20as%20interrupting%20human%20conversations.%20This%20behavior%2C%0Aoriginating%20from%20a%20limited%20processing%20of%20social%20norms%2C%20might%20be%20perceived%20as%0Arobot-centered.%20Understanding%20social%20dynamics%2C%20particularly%20in%0Agroup-robot-interactions%20%28GRI%29%2C%20underscores%20the%20need%20for%20further%20research%20and%0Adevelopment%20in%20human-robot-interaction%20%28HRI%29.%20Enhancing%20the%20interaction%0Aabilities%20of%20social%20robots%2C%20especially%20in%20GRIs%2C%20can%20improve%20their%20effectiveness%0Ain%20real-world%20applications%20on%20a%20micro-level%2C%20as%20group%20interactions%20lead%20to%0Aincreased%20motivation%20and%20comfort.%20In%20this%20study%2C%20we%20assessed%20the%20influence%20of%0Athe%20interaction%20condition%20%28dyadic%20vs.%20triadic%29%20on%20the%20perceived%20extraversion%0A%28ext.%29%20of%20social%20robots%20in%20public%20spaces.%20The%20research%20involved%2040%20HRIs%2C%0Aincluding%2024%20dyadic%20%28i.e.%2C%20one%20human%20and%20one%20robot%29%20interactions%20and%2016%20triadic%0Ainteractions%2C%20which%20involve%20at%20least%20three%20entities%2C%20including%20the%20robot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18009v1&entry.124074799=Read"},
{"title": "PARSE-Ego4D: Personal Action Recommendation Suggestions for Egocentric\n  Videos", "author": "Steven Abreu and Tiffany D. Do and Karan Ahuja and Eric J. Gonzalez and Lee Payne and Daniel McDuff and Mar Gonzalez-Franco", "abstract": "  Intelligent assistance involves not only understanding but also action.\nExisting ego-centric video datasets contain rich annotations of the videos, but\nnot of actions that an intelligent assistant could perform in the moment. To\naddress this gap, we release PARSE-Ego4D, a new set of personal action\nrecommendation annotations for the Ego4D dataset. We take a multi-stage\napproach to generating and evaluating these annotations. First, we used a\nprompt-engineered large language model (LLM) to generate context-aware action\nsuggestions and identified over 18,000 action suggestions. While these\nsynthetic action suggestions are valuable, the inherent limitations of LLMs\nnecessitate human evaluation. To ensure high-quality and user-centered\nrecommendations, we conducted a large-scale human annotation study that\nprovides grounding in human preferences for all of PARSE-Ego4D. We analyze the\ninter-rater agreement and evaluate subjective preferences of participants.\nBased on our synthetic dataset and complete human annotations, we propose\nseveral new tasks for action suggestions based on ego-centric videos. We\nencourage novel solutions that improve latency and energy requirements. The\nannotations in PARSE-Ego4D will support researchers and developers who are\nworking on building action recommendation systems for augmented and virtual\nreality systems.\n", "link": "http://arxiv.org/abs/2407.09503v2", "date": "2024-07-25", "relevancy": 2.0394, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.538}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5193}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PARSE-Ego4D%3A%20Personal%20Action%20Recommendation%20Suggestions%20for%20Egocentric%0A%20%20Videos&body=Title%3A%20PARSE-Ego4D%3A%20Personal%20Action%20Recommendation%20Suggestions%20for%20Egocentric%0A%20%20Videos%0AAuthor%3A%20Steven%20Abreu%20and%20Tiffany%20D.%20Do%20and%20Karan%20Ahuja%20and%20Eric%20J.%20Gonzalez%20and%20Lee%20Payne%20and%20Daniel%20McDuff%20and%20Mar%20Gonzalez-Franco%0AAbstract%3A%20%20%20Intelligent%20assistance%20involves%20not%20only%20understanding%20but%20also%20action.%0AExisting%20ego-centric%20video%20datasets%20contain%20rich%20annotations%20of%20the%20videos%2C%20but%0Anot%20of%20actions%20that%20an%20intelligent%20assistant%20could%20perform%20in%20the%20moment.%20To%0Aaddress%20this%20gap%2C%20we%20release%20PARSE-Ego4D%2C%20a%20new%20set%20of%20personal%20action%0Arecommendation%20annotations%20for%20the%20Ego4D%20dataset.%20We%20take%20a%20multi-stage%0Aapproach%20to%20generating%20and%20evaluating%20these%20annotations.%20First%2C%20we%20used%20a%0Aprompt-engineered%20large%20language%20model%20%28LLM%29%20to%20generate%20context-aware%20action%0Asuggestions%20and%20identified%20over%2018%2C000%20action%20suggestions.%20While%20these%0Asynthetic%20action%20suggestions%20are%20valuable%2C%20the%20inherent%20limitations%20of%20LLMs%0Anecessitate%20human%20evaluation.%20To%20ensure%20high-quality%20and%20user-centered%0Arecommendations%2C%20we%20conducted%20a%20large-scale%20human%20annotation%20study%20that%0Aprovides%20grounding%20in%20human%20preferences%20for%20all%20of%20PARSE-Ego4D.%20We%20analyze%20the%0Ainter-rater%20agreement%20and%20evaluate%20subjective%20preferences%20of%20participants.%0ABased%20on%20our%20synthetic%20dataset%20and%20complete%20human%20annotations%2C%20we%20propose%0Aseveral%20new%20tasks%20for%20action%20suggestions%20based%20on%20ego-centric%20videos.%20We%0Aencourage%20novel%20solutions%20that%20improve%20latency%20and%20energy%20requirements.%20The%0Aannotations%20in%20PARSE-Ego4D%20will%20support%20researchers%20and%20developers%20who%20are%0Aworking%20on%20building%20action%20recommendation%20systems%20for%20augmented%20and%20virtual%0Areality%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09503v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPARSE-Ego4D%253A%2520Personal%2520Action%2520Recommendation%2520Suggestions%2520for%2520Egocentric%250A%2520%2520Videos%26entry.906535625%3DSteven%2520Abreu%2520and%2520Tiffany%2520D.%2520Do%2520and%2520Karan%2520Ahuja%2520and%2520Eric%2520J.%2520Gonzalez%2520and%2520Lee%2520Payne%2520and%2520Daniel%2520McDuff%2520and%2520Mar%2520Gonzalez-Franco%26entry.1292438233%3D%2520%2520Intelligent%2520assistance%2520involves%2520not%2520only%2520understanding%2520but%2520also%2520action.%250AExisting%2520ego-centric%2520video%2520datasets%2520contain%2520rich%2520annotations%2520of%2520the%2520videos%252C%2520but%250Anot%2520of%2520actions%2520that%2520an%2520intelligent%2520assistant%2520could%2520perform%2520in%2520the%2520moment.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520release%2520PARSE-Ego4D%252C%2520a%2520new%2520set%2520of%2520personal%2520action%250Arecommendation%2520annotations%2520for%2520the%2520Ego4D%2520dataset.%2520We%2520take%2520a%2520multi-stage%250Aapproach%2520to%2520generating%2520and%2520evaluating%2520these%2520annotations.%2520First%252C%2520we%2520used%2520a%250Aprompt-engineered%2520large%2520language%2520model%2520%2528LLM%2529%2520to%2520generate%2520context-aware%2520action%250Asuggestions%2520and%2520identified%2520over%252018%252C000%2520action%2520suggestions.%2520While%2520these%250Asynthetic%2520action%2520suggestions%2520are%2520valuable%252C%2520the%2520inherent%2520limitations%2520of%2520LLMs%250Anecessitate%2520human%2520evaluation.%2520To%2520ensure%2520high-quality%2520and%2520user-centered%250Arecommendations%252C%2520we%2520conducted%2520a%2520large-scale%2520human%2520annotation%2520study%2520that%250Aprovides%2520grounding%2520in%2520human%2520preferences%2520for%2520all%2520of%2520PARSE-Ego4D.%2520We%2520analyze%2520the%250Ainter-rater%2520agreement%2520and%2520evaluate%2520subjective%2520preferences%2520of%2520participants.%250ABased%2520on%2520our%2520synthetic%2520dataset%2520and%2520complete%2520human%2520annotations%252C%2520we%2520propose%250Aseveral%2520new%2520tasks%2520for%2520action%2520suggestions%2520based%2520on%2520ego-centric%2520videos.%2520We%250Aencourage%2520novel%2520solutions%2520that%2520improve%2520latency%2520and%2520energy%2520requirements.%2520The%250Aannotations%2520in%2520PARSE-Ego4D%2520will%2520support%2520researchers%2520and%2520developers%2520who%2520are%250Aworking%2520on%2520building%2520action%2520recommendation%2520systems%2520for%2520augmented%2520and%2520virtual%250Areality%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09503v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PARSE-Ego4D%3A%20Personal%20Action%20Recommendation%20Suggestions%20for%20Egocentric%0A%20%20Videos&entry.906535625=Steven%20Abreu%20and%20Tiffany%20D.%20Do%20and%20Karan%20Ahuja%20and%20Eric%20J.%20Gonzalez%20and%20Lee%20Payne%20and%20Daniel%20McDuff%20and%20Mar%20Gonzalez-Franco&entry.1292438233=%20%20Intelligent%20assistance%20involves%20not%20only%20understanding%20but%20also%20action.%0AExisting%20ego-centric%20video%20datasets%20contain%20rich%20annotations%20of%20the%20videos%2C%20but%0Anot%20of%20actions%20that%20an%20intelligent%20assistant%20could%20perform%20in%20the%20moment.%20To%0Aaddress%20this%20gap%2C%20we%20release%20PARSE-Ego4D%2C%20a%20new%20set%20of%20personal%20action%0Arecommendation%20annotations%20for%20the%20Ego4D%20dataset.%20We%20take%20a%20multi-stage%0Aapproach%20to%20generating%20and%20evaluating%20these%20annotations.%20First%2C%20we%20used%20a%0Aprompt-engineered%20large%20language%20model%20%28LLM%29%20to%20generate%20context-aware%20action%0Asuggestions%20and%20identified%20over%2018%2C000%20action%20suggestions.%20While%20these%0Asynthetic%20action%20suggestions%20are%20valuable%2C%20the%20inherent%20limitations%20of%20LLMs%0Anecessitate%20human%20evaluation.%20To%20ensure%20high-quality%20and%20user-centered%0Arecommendations%2C%20we%20conducted%20a%20large-scale%20human%20annotation%20study%20that%0Aprovides%20grounding%20in%20human%20preferences%20for%20all%20of%20PARSE-Ego4D.%20We%20analyze%20the%0Ainter-rater%20agreement%20and%20evaluate%20subjective%20preferences%20of%20participants.%0ABased%20on%20our%20synthetic%20dataset%20and%20complete%20human%20annotations%2C%20we%20propose%0Aseveral%20new%20tasks%20for%20action%20suggestions%20based%20on%20ego-centric%20videos.%20We%0Aencourage%20novel%20solutions%20that%20improve%20latency%20and%20energy%20requirements.%20The%0Aannotations%20in%20PARSE-Ego4D%20will%20support%20researchers%20and%20developers%20who%20are%0Aworking%20on%20building%20action%20recommendation%20systems%20for%20augmented%20and%20virtual%0Areality%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09503v2&entry.124074799=Read"},
{"title": "Targeted stochastic gradient Markov chain Monte Carlo for hidden Markov\n  models with rare latent states", "author": "Rihui Ou and Deborshee Sen and Alexander L Young and David B Dunson", "abstract": "  Markov chain Monte Carlo (MCMC) algorithms for hidden Markov models often\nrely on the forward-backward sampler. This makes them computationally slow as\nthe length of the time series increases, motivating the development of\nsub-sampling-based approaches. These approximate the full posterior by using\nsmall random subsequences of the data at each MCMC iteration within stochastic\ngradient MCMC. In the presence of imbalanced data resulting from rare latent\nstates, subsequences often exclude rare latent state data, leading to\ninaccurate inference and prediction/detection of rare events. We propose a\ntargeted sub-sampling (TASS) approach that over-samples observations\ncorresponding to rare latent states when calculating the stochastic gradient of\nparameters associated with them. TASS uses an initial clustering of the data to\nconstruct subsequence weights that reduce the variance in gradient estimation.\nThis leads to improved sampling efficiency, in particular in settings where the\nrare latent states correspond to extreme observations. We demonstrate\nsubstantial gains in predictive and inferential accuracy on real and synthetic\nexamples.\n", "link": "http://arxiv.org/abs/1810.13431v3", "date": "2024-07-25", "relevancy": 2.0297, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5291}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4957}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Targeted%20stochastic%20gradient%20Markov%20chain%20Monte%20Carlo%20for%20hidden%20Markov%0A%20%20models%20with%20rare%20latent%20states&body=Title%3A%20Targeted%20stochastic%20gradient%20Markov%20chain%20Monte%20Carlo%20for%20hidden%20Markov%0A%20%20models%20with%20rare%20latent%20states%0AAuthor%3A%20Rihui%20Ou%20and%20Deborshee%20Sen%20and%20Alexander%20L%20Young%20and%20David%20B%20Dunson%0AAbstract%3A%20%20%20Markov%20chain%20Monte%20Carlo%20%28MCMC%29%20algorithms%20for%20hidden%20Markov%20models%20often%0Arely%20on%20the%20forward-backward%20sampler.%20This%20makes%20them%20computationally%20slow%20as%0Athe%20length%20of%20the%20time%20series%20increases%2C%20motivating%20the%20development%20of%0Asub-sampling-based%20approaches.%20These%20approximate%20the%20full%20posterior%20by%20using%0Asmall%20random%20subsequences%20of%20the%20data%20at%20each%20MCMC%20iteration%20within%20stochastic%0Agradient%20MCMC.%20In%20the%20presence%20of%20imbalanced%20data%20resulting%20from%20rare%20latent%0Astates%2C%20subsequences%20often%20exclude%20rare%20latent%20state%20data%2C%20leading%20to%0Ainaccurate%20inference%20and%20prediction/detection%20of%20rare%20events.%20We%20propose%20a%0Atargeted%20sub-sampling%20%28TASS%29%20approach%20that%20over-samples%20observations%0Acorresponding%20to%20rare%20latent%20states%20when%20calculating%20the%20stochastic%20gradient%20of%0Aparameters%20associated%20with%20them.%20TASS%20uses%20an%20initial%20clustering%20of%20the%20data%20to%0Aconstruct%20subsequence%20weights%20that%20reduce%20the%20variance%20in%20gradient%20estimation.%0AThis%20leads%20to%20improved%20sampling%20efficiency%2C%20in%20particular%20in%20settings%20where%20the%0Arare%20latent%20states%20correspond%20to%20extreme%20observations.%20We%20demonstrate%0Asubstantial%20gains%20in%20predictive%20and%20inferential%20accuracy%20on%20real%20and%20synthetic%0Aexamples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/1810.13431v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTargeted%2520stochastic%2520gradient%2520Markov%2520chain%2520Monte%2520Carlo%2520for%2520hidden%2520Markov%250A%2520%2520models%2520with%2520rare%2520latent%2520states%26entry.906535625%3DRihui%2520Ou%2520and%2520Deborshee%2520Sen%2520and%2520Alexander%2520L%2520Young%2520and%2520David%2520B%2520Dunson%26entry.1292438233%3D%2520%2520Markov%2520chain%2520Monte%2520Carlo%2520%2528MCMC%2529%2520algorithms%2520for%2520hidden%2520Markov%2520models%2520often%250Arely%2520on%2520the%2520forward-backward%2520sampler.%2520This%2520makes%2520them%2520computationally%2520slow%2520as%250Athe%2520length%2520of%2520the%2520time%2520series%2520increases%252C%2520motivating%2520the%2520development%2520of%250Asub-sampling-based%2520approaches.%2520These%2520approximate%2520the%2520full%2520posterior%2520by%2520using%250Asmall%2520random%2520subsequences%2520of%2520the%2520data%2520at%2520each%2520MCMC%2520iteration%2520within%2520stochastic%250Agradient%2520MCMC.%2520In%2520the%2520presence%2520of%2520imbalanced%2520data%2520resulting%2520from%2520rare%2520latent%250Astates%252C%2520subsequences%2520often%2520exclude%2520rare%2520latent%2520state%2520data%252C%2520leading%2520to%250Ainaccurate%2520inference%2520and%2520prediction/detection%2520of%2520rare%2520events.%2520We%2520propose%2520a%250Atargeted%2520sub-sampling%2520%2528TASS%2529%2520approach%2520that%2520over-samples%2520observations%250Acorresponding%2520to%2520rare%2520latent%2520states%2520when%2520calculating%2520the%2520stochastic%2520gradient%2520of%250Aparameters%2520associated%2520with%2520them.%2520TASS%2520uses%2520an%2520initial%2520clustering%2520of%2520the%2520data%2520to%250Aconstruct%2520subsequence%2520weights%2520that%2520reduce%2520the%2520variance%2520in%2520gradient%2520estimation.%250AThis%2520leads%2520to%2520improved%2520sampling%2520efficiency%252C%2520in%2520particular%2520in%2520settings%2520where%2520the%250Arare%2520latent%2520states%2520correspond%2520to%2520extreme%2520observations.%2520We%2520demonstrate%250Asubstantial%2520gains%2520in%2520predictive%2520and%2520inferential%2520accuracy%2520on%2520real%2520and%2520synthetic%250Aexamples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/1810.13431v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Targeted%20stochastic%20gradient%20Markov%20chain%20Monte%20Carlo%20for%20hidden%20Markov%0A%20%20models%20with%20rare%20latent%20states&entry.906535625=Rihui%20Ou%20and%20Deborshee%20Sen%20and%20Alexander%20L%20Young%20and%20David%20B%20Dunson&entry.1292438233=%20%20Markov%20chain%20Monte%20Carlo%20%28MCMC%29%20algorithms%20for%20hidden%20Markov%20models%20often%0Arely%20on%20the%20forward-backward%20sampler.%20This%20makes%20them%20computationally%20slow%20as%0Athe%20length%20of%20the%20time%20series%20increases%2C%20motivating%20the%20development%20of%0Asub-sampling-based%20approaches.%20These%20approximate%20the%20full%20posterior%20by%20using%0Asmall%20random%20subsequences%20of%20the%20data%20at%20each%20MCMC%20iteration%20within%20stochastic%0Agradient%20MCMC.%20In%20the%20presence%20of%20imbalanced%20data%20resulting%20from%20rare%20latent%0Astates%2C%20subsequences%20often%20exclude%20rare%20latent%20state%20data%2C%20leading%20to%0Ainaccurate%20inference%20and%20prediction/detection%20of%20rare%20events.%20We%20propose%20a%0Atargeted%20sub-sampling%20%28TASS%29%20approach%20that%20over-samples%20observations%0Acorresponding%20to%20rare%20latent%20states%20when%20calculating%20the%20stochastic%20gradient%20of%0Aparameters%20associated%20with%20them.%20TASS%20uses%20an%20initial%20clustering%20of%20the%20data%20to%0Aconstruct%20subsequence%20weights%20that%20reduce%20the%20variance%20in%20gradient%20estimation.%0AThis%20leads%20to%20improved%20sampling%20efficiency%2C%20in%20particular%20in%20settings%20where%20the%0Arare%20latent%20states%20correspond%20to%20extreme%20observations.%20We%20demonstrate%0Asubstantial%20gains%20in%20predictive%20and%20inferential%20accuracy%20on%20real%20and%20synthetic%0Aexamples.%0A&entry.1838667208=http%3A//arxiv.org/abs/1810.13431v3&entry.124074799=Read"},
{"title": "Network Inversion of Convolutional Neural Nets", "author": "Pirzada Suhail and Amit Sethi", "abstract": "  Neural networks have emerged as powerful tools across various applications,\nyet their decision-making process often remains opaque, leading to them being\nperceived as \"black boxes.\" This opacity raises concerns about their\ninterpretability and reliability, especially in safety-critical scenarios.\nNetwork inversion techniques offer a solution by allowing us to peek inside\nthese black boxes, revealing the features and patterns learned by the networks\nbehind their decision-making processes and thereby provide valuable insights\ninto how neural networks arrive at their conclusions, making them more\ninterpretable and trustworthy. This paper presents a simple yet effective\napproach to network inversion using a carefully conditioned generator that\nlearns the data distribution in the input space of the trained neural network,\nenabling the reconstruction of inputs that would most likely lead to the\ndesired outputs. To capture the diversity in the input space for a given\noutput, instead of simply revealing the conditioning labels to the generator,\nwe hideously encode the conditioning label information into vectors, further\nexemplified by heavy dropout in the generation process and minimisation of\ncosine similarity between the features corresponding to the generated images.\nThe paper concludes with immediate applications of Network Inversion including\nin interpretability, explainability and generation of adversarial samples.\n", "link": "http://arxiv.org/abs/2407.18002v1", "date": "2024-07-25", "relevancy": 2.0186, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5211}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5059}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Network%20Inversion%20of%20Convolutional%20Neural%20Nets&body=Title%3A%20Network%20Inversion%20of%20Convolutional%20Neural%20Nets%0AAuthor%3A%20Pirzada%20Suhail%20and%20Amit%20Sethi%0AAbstract%3A%20%20%20Neural%20networks%20have%20emerged%20as%20powerful%20tools%20across%20various%20applications%2C%0Ayet%20their%20decision-making%20process%20often%20remains%20opaque%2C%20leading%20to%20them%20being%0Aperceived%20as%20%22black%20boxes.%22%20This%20opacity%20raises%20concerns%20about%20their%0Ainterpretability%20and%20reliability%2C%20especially%20in%20safety-critical%20scenarios.%0ANetwork%20inversion%20techniques%20offer%20a%20solution%20by%20allowing%20us%20to%20peek%20inside%0Athese%20black%20boxes%2C%20revealing%20the%20features%20and%20patterns%20learned%20by%20the%20networks%0Abehind%20their%20decision-making%20processes%20and%20thereby%20provide%20valuable%20insights%0Ainto%20how%20neural%20networks%20arrive%20at%20their%20conclusions%2C%20making%20them%20more%0Ainterpretable%20and%20trustworthy.%20This%20paper%20presents%20a%20simple%20yet%20effective%0Aapproach%20to%20network%20inversion%20using%20a%20carefully%20conditioned%20generator%20that%0Alearns%20the%20data%20distribution%20in%20the%20input%20space%20of%20the%20trained%20neural%20network%2C%0Aenabling%20the%20reconstruction%20of%20inputs%20that%20would%20most%20likely%20lead%20to%20the%0Adesired%20outputs.%20To%20capture%20the%20diversity%20in%20the%20input%20space%20for%20a%20given%0Aoutput%2C%20instead%20of%20simply%20revealing%20the%20conditioning%20labels%20to%20the%20generator%2C%0Awe%20hideously%20encode%20the%20conditioning%20label%20information%20into%20vectors%2C%20further%0Aexemplified%20by%20heavy%20dropout%20in%20the%20generation%20process%20and%20minimisation%20of%0Acosine%20similarity%20between%20the%20features%20corresponding%20to%20the%20generated%20images.%0AThe%20paper%20concludes%20with%20immediate%20applications%20of%20Network%20Inversion%20including%0Ain%20interpretability%2C%20explainability%20and%20generation%20of%20adversarial%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNetwork%2520Inversion%2520of%2520Convolutional%2520Neural%2520Nets%26entry.906535625%3DPirzada%2520Suhail%2520and%2520Amit%2520Sethi%26entry.1292438233%3D%2520%2520Neural%2520networks%2520have%2520emerged%2520as%2520powerful%2520tools%2520across%2520various%2520applications%252C%250Ayet%2520their%2520decision-making%2520process%2520often%2520remains%2520opaque%252C%2520leading%2520to%2520them%2520being%250Aperceived%2520as%2520%2522black%2520boxes.%2522%2520This%2520opacity%2520raises%2520concerns%2520about%2520their%250Ainterpretability%2520and%2520reliability%252C%2520especially%2520in%2520safety-critical%2520scenarios.%250ANetwork%2520inversion%2520techniques%2520offer%2520a%2520solution%2520by%2520allowing%2520us%2520to%2520peek%2520inside%250Athese%2520black%2520boxes%252C%2520revealing%2520the%2520features%2520and%2520patterns%2520learned%2520by%2520the%2520networks%250Abehind%2520their%2520decision-making%2520processes%2520and%2520thereby%2520provide%2520valuable%2520insights%250Ainto%2520how%2520neural%2520networks%2520arrive%2520at%2520their%2520conclusions%252C%2520making%2520them%2520more%250Ainterpretable%2520and%2520trustworthy.%2520This%2520paper%2520presents%2520a%2520simple%2520yet%2520effective%250Aapproach%2520to%2520network%2520inversion%2520using%2520a%2520carefully%2520conditioned%2520generator%2520that%250Alearns%2520the%2520data%2520distribution%2520in%2520the%2520input%2520space%2520of%2520the%2520trained%2520neural%2520network%252C%250Aenabling%2520the%2520reconstruction%2520of%2520inputs%2520that%2520would%2520most%2520likely%2520lead%2520to%2520the%250Adesired%2520outputs.%2520To%2520capture%2520the%2520diversity%2520in%2520the%2520input%2520space%2520for%2520a%2520given%250Aoutput%252C%2520instead%2520of%2520simply%2520revealing%2520the%2520conditioning%2520labels%2520to%2520the%2520generator%252C%250Awe%2520hideously%2520encode%2520the%2520conditioning%2520label%2520information%2520into%2520vectors%252C%2520further%250Aexemplified%2520by%2520heavy%2520dropout%2520in%2520the%2520generation%2520process%2520and%2520minimisation%2520of%250Acosine%2520similarity%2520between%2520the%2520features%2520corresponding%2520to%2520the%2520generated%2520images.%250AThe%2520paper%2520concludes%2520with%2520immediate%2520applications%2520of%2520Network%2520Inversion%2520including%250Ain%2520interpretability%252C%2520explainability%2520and%2520generation%2520of%2520adversarial%2520samples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Network%20Inversion%20of%20Convolutional%20Neural%20Nets&entry.906535625=Pirzada%20Suhail%20and%20Amit%20Sethi&entry.1292438233=%20%20Neural%20networks%20have%20emerged%20as%20powerful%20tools%20across%20various%20applications%2C%0Ayet%20their%20decision-making%20process%20often%20remains%20opaque%2C%20leading%20to%20them%20being%0Aperceived%20as%20%22black%20boxes.%22%20This%20opacity%20raises%20concerns%20about%20their%0Ainterpretability%20and%20reliability%2C%20especially%20in%20safety-critical%20scenarios.%0ANetwork%20inversion%20techniques%20offer%20a%20solution%20by%20allowing%20us%20to%20peek%20inside%0Athese%20black%20boxes%2C%20revealing%20the%20features%20and%20patterns%20learned%20by%20the%20networks%0Abehind%20their%20decision-making%20processes%20and%20thereby%20provide%20valuable%20insights%0Ainto%20how%20neural%20networks%20arrive%20at%20their%20conclusions%2C%20making%20them%20more%0Ainterpretable%20and%20trustworthy.%20This%20paper%20presents%20a%20simple%20yet%20effective%0Aapproach%20to%20network%20inversion%20using%20a%20carefully%20conditioned%20generator%20that%0Alearns%20the%20data%20distribution%20in%20the%20input%20space%20of%20the%20trained%20neural%20network%2C%0Aenabling%20the%20reconstruction%20of%20inputs%20that%20would%20most%20likely%20lead%20to%20the%0Adesired%20outputs.%20To%20capture%20the%20diversity%20in%20the%20input%20space%20for%20a%20given%0Aoutput%2C%20instead%20of%20simply%20revealing%20the%20conditioning%20labels%20to%20the%20generator%2C%0Awe%20hideously%20encode%20the%20conditioning%20label%20information%20into%20vectors%2C%20further%0Aexemplified%20by%20heavy%20dropout%20in%20the%20generation%20process%20and%20minimisation%20of%0Acosine%20similarity%20between%20the%20features%20corresponding%20to%20the%20generated%20images.%0AThe%20paper%20concludes%20with%20immediate%20applications%20of%20Network%20Inversion%20including%0Ain%20interpretability%2C%20explainability%20and%20generation%20of%20adversarial%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18002v1&entry.124074799=Read"},
{"title": "Keypoint Promptable Re-Identification", "author": "Vladimir Somers and Christophe De Vleeschouwer and Alexandre Alahi", "abstract": "  Occluded Person Re-Identification (ReID) is a metric learning task that\ninvolves matching occluded individuals based on their appearance. While many\nstudies have tackled occlusions caused by objects, multi-person occlusions\nremain less explored. In this work, we identify and address a critical\nchallenge overlooked by previous occluded ReID methods: the Multi-Person\nAmbiguity (MPA) arising when multiple individuals are visible in the same\nbounding box, making it impossible to determine the intended ReID target among\nthe candidates. Inspired by recent work on prompting in vision, we introduce\nKeypoint Promptable ReID (KPR), a novel formulation of the ReID problem that\nexplicitly complements the input bounding box with a set of semantic keypoints\nindicating the intended target. Since promptable re-identification is an\nunexplored paradigm, existing ReID datasets lack the pixel-level annotations\nnecessary for prompting. To bridge this gap and foster further research on this\ntopic, we introduce Occluded-PoseTrack ReID, a novel ReID dataset with\nkeypoints labels, that features strong inter-person occlusions. Furthermore, we\nrelease custom keypoint labels for four popular ReID benchmarks. Experiments on\nperson retrieval, but also on pose tracking, demonstrate that our method\nsystematically surpasses previous state-of-the-art approaches on various\noccluded scenarios. Our code, dataset and annotations are available at\nhttps://github.com/VlSomers/keypoint_promptable_reidentification.\n", "link": "http://arxiv.org/abs/2407.18112v1", "date": "2024-07-25", "relevancy": 2.016, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5126}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.499}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4949}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Keypoint%20Promptable%20Re-Identification&body=Title%3A%20Keypoint%20Promptable%20Re-Identification%0AAuthor%3A%20Vladimir%20Somers%20and%20Christophe%20De%20Vleeschouwer%20and%20Alexandre%20Alahi%0AAbstract%3A%20%20%20Occluded%20Person%20Re-Identification%20%28ReID%29%20is%20a%20metric%20learning%20task%20that%0Ainvolves%20matching%20occluded%20individuals%20based%20on%20their%20appearance.%20While%20many%0Astudies%20have%20tackled%20occlusions%20caused%20by%20objects%2C%20multi-person%20occlusions%0Aremain%20less%20explored.%20In%20this%20work%2C%20we%20identify%20and%20address%20a%20critical%0Achallenge%20overlooked%20by%20previous%20occluded%20ReID%20methods%3A%20the%20Multi-Person%0AAmbiguity%20%28MPA%29%20arising%20when%20multiple%20individuals%20are%20visible%20in%20the%20same%0Abounding%20box%2C%20making%20it%20impossible%20to%20determine%20the%20intended%20ReID%20target%20among%0Athe%20candidates.%20Inspired%20by%20recent%20work%20on%20prompting%20in%20vision%2C%20we%20introduce%0AKeypoint%20Promptable%20ReID%20%28KPR%29%2C%20a%20novel%20formulation%20of%20the%20ReID%20problem%20that%0Aexplicitly%20complements%20the%20input%20bounding%20box%20with%20a%20set%20of%20semantic%20keypoints%0Aindicating%20the%20intended%20target.%20Since%20promptable%20re-identification%20is%20an%0Aunexplored%20paradigm%2C%20existing%20ReID%20datasets%20lack%20the%20pixel-level%20annotations%0Anecessary%20for%20prompting.%20To%20bridge%20this%20gap%20and%20foster%20further%20research%20on%20this%0Atopic%2C%20we%20introduce%20Occluded-PoseTrack%20ReID%2C%20a%20novel%20ReID%20dataset%20with%0Akeypoints%20labels%2C%20that%20features%20strong%20inter-person%20occlusions.%20Furthermore%2C%20we%0Arelease%20custom%20keypoint%20labels%20for%20four%20popular%20ReID%20benchmarks.%20Experiments%20on%0Aperson%20retrieval%2C%20but%20also%20on%20pose%20tracking%2C%20demonstrate%20that%20our%20method%0Asystematically%20surpasses%20previous%20state-of-the-art%20approaches%20on%20various%0Aoccluded%20scenarios.%20Our%20code%2C%20dataset%20and%20annotations%20are%20available%20at%0Ahttps%3A//github.com/VlSomers/keypoint_promptable_reidentification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18112v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKeypoint%2520Promptable%2520Re-Identification%26entry.906535625%3DVladimir%2520Somers%2520and%2520Christophe%2520De%2520Vleeschouwer%2520and%2520Alexandre%2520Alahi%26entry.1292438233%3D%2520%2520Occluded%2520Person%2520Re-Identification%2520%2528ReID%2529%2520is%2520a%2520metric%2520learning%2520task%2520that%250Ainvolves%2520matching%2520occluded%2520individuals%2520based%2520on%2520their%2520appearance.%2520While%2520many%250Astudies%2520have%2520tackled%2520occlusions%2520caused%2520by%2520objects%252C%2520multi-person%2520occlusions%250Aremain%2520less%2520explored.%2520In%2520this%2520work%252C%2520we%2520identify%2520and%2520address%2520a%2520critical%250Achallenge%2520overlooked%2520by%2520previous%2520occluded%2520ReID%2520methods%253A%2520the%2520Multi-Person%250AAmbiguity%2520%2528MPA%2529%2520arising%2520when%2520multiple%2520individuals%2520are%2520visible%2520in%2520the%2520same%250Abounding%2520box%252C%2520making%2520it%2520impossible%2520to%2520determine%2520the%2520intended%2520ReID%2520target%2520among%250Athe%2520candidates.%2520Inspired%2520by%2520recent%2520work%2520on%2520prompting%2520in%2520vision%252C%2520we%2520introduce%250AKeypoint%2520Promptable%2520ReID%2520%2528KPR%2529%252C%2520a%2520novel%2520formulation%2520of%2520the%2520ReID%2520problem%2520that%250Aexplicitly%2520complements%2520the%2520input%2520bounding%2520box%2520with%2520a%2520set%2520of%2520semantic%2520keypoints%250Aindicating%2520the%2520intended%2520target.%2520Since%2520promptable%2520re-identification%2520is%2520an%250Aunexplored%2520paradigm%252C%2520existing%2520ReID%2520datasets%2520lack%2520the%2520pixel-level%2520annotations%250Anecessary%2520for%2520prompting.%2520To%2520bridge%2520this%2520gap%2520and%2520foster%2520further%2520research%2520on%2520this%250Atopic%252C%2520we%2520introduce%2520Occluded-PoseTrack%2520ReID%252C%2520a%2520novel%2520ReID%2520dataset%2520with%250Akeypoints%2520labels%252C%2520that%2520features%2520strong%2520inter-person%2520occlusions.%2520Furthermore%252C%2520we%250Arelease%2520custom%2520keypoint%2520labels%2520for%2520four%2520popular%2520ReID%2520benchmarks.%2520Experiments%2520on%250Aperson%2520retrieval%252C%2520but%2520also%2520on%2520pose%2520tracking%252C%2520demonstrate%2520that%2520our%2520method%250Asystematically%2520surpasses%2520previous%2520state-of-the-art%2520approaches%2520on%2520various%250Aoccluded%2520scenarios.%2520Our%2520code%252C%2520dataset%2520and%2520annotations%2520are%2520available%2520at%250Ahttps%253A//github.com/VlSomers/keypoint_promptable_reidentification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18112v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Keypoint%20Promptable%20Re-Identification&entry.906535625=Vladimir%20Somers%20and%20Christophe%20De%20Vleeschouwer%20and%20Alexandre%20Alahi&entry.1292438233=%20%20Occluded%20Person%20Re-Identification%20%28ReID%29%20is%20a%20metric%20learning%20task%20that%0Ainvolves%20matching%20occluded%20individuals%20based%20on%20their%20appearance.%20While%20many%0Astudies%20have%20tackled%20occlusions%20caused%20by%20objects%2C%20multi-person%20occlusions%0Aremain%20less%20explored.%20In%20this%20work%2C%20we%20identify%20and%20address%20a%20critical%0Achallenge%20overlooked%20by%20previous%20occluded%20ReID%20methods%3A%20the%20Multi-Person%0AAmbiguity%20%28MPA%29%20arising%20when%20multiple%20individuals%20are%20visible%20in%20the%20same%0Abounding%20box%2C%20making%20it%20impossible%20to%20determine%20the%20intended%20ReID%20target%20among%0Athe%20candidates.%20Inspired%20by%20recent%20work%20on%20prompting%20in%20vision%2C%20we%20introduce%0AKeypoint%20Promptable%20ReID%20%28KPR%29%2C%20a%20novel%20formulation%20of%20the%20ReID%20problem%20that%0Aexplicitly%20complements%20the%20input%20bounding%20box%20with%20a%20set%20of%20semantic%20keypoints%0Aindicating%20the%20intended%20target.%20Since%20promptable%20re-identification%20is%20an%0Aunexplored%20paradigm%2C%20existing%20ReID%20datasets%20lack%20the%20pixel-level%20annotations%0Anecessary%20for%20prompting.%20To%20bridge%20this%20gap%20and%20foster%20further%20research%20on%20this%0Atopic%2C%20we%20introduce%20Occluded-PoseTrack%20ReID%2C%20a%20novel%20ReID%20dataset%20with%0Akeypoints%20labels%2C%20that%20features%20strong%20inter-person%20occlusions.%20Furthermore%2C%20we%0Arelease%20custom%20keypoint%20labels%20for%20four%20popular%20ReID%20benchmarks.%20Experiments%20on%0Aperson%20retrieval%2C%20but%20also%20on%20pose%20tracking%2C%20demonstrate%20that%20our%20method%0Asystematically%20surpasses%20previous%20state-of-the-art%20approaches%20on%20various%0Aoccluded%20scenarios.%20Our%20code%2C%20dataset%20and%20annotations%20are%20available%20at%0Ahttps%3A//github.com/VlSomers/keypoint_promptable_reidentification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18112v1&entry.124074799=Read"},
{"title": "When Linear Attention Meets Autoregressive Decoding: Towards More\n  Effective and Efficient Linearized Large Language Models", "author": "Haoran You and Yichao Fu and Zheng Wang and Amir Yazdanbakhsh and Yingyan Celine Lin", "abstract": "  Autoregressive Large Language Models (LLMs) have achieved impressive\nperformance in language tasks but face two significant bottlenecks: (1)\nquadratic complexity in the attention module as the number of tokens increases,\nand (2) limited efficiency due to the sequential processing nature of\nautoregressive LLMs during generation. While linear attention and speculative\ndecoding offer potential solutions, their applicability and synergistic\npotential for enhancing autoregressive LLMs remain uncertain. We conduct the\nfirst comprehensive study on the efficacy of existing linear attention methods\nfor autoregressive LLMs, integrating them with speculative decoding. We\nintroduce an augmentation technique for linear attention that ensures\ncompatibility with speculative decoding, enabling more efficient training and\nserving of LLMs. Extensive experiments and ablation studies involving seven\nexisting linear attention models and five encoder/decoder-based LLMs\nconsistently validate the effectiveness of our augmented linearized LLMs.\nNotably, our approach achieves up to a 6.67 reduction in perplexity on the\nLLaMA model and up to a 2$\\times$ speedup during generation compared to prior\nlinear attention methods. Codes and models are available at\nhttps://github.com/GATECH-EIC/Linearized-LLM.\n", "link": "http://arxiv.org/abs/2406.07368v2", "date": "2024-07-25", "relevancy": 1.9971, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5063}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5052}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Linear%20Attention%20Meets%20Autoregressive%20Decoding%3A%20Towards%20More%0A%20%20Effective%20and%20Efficient%20Linearized%20Large%20Language%20Models&body=Title%3A%20When%20Linear%20Attention%20Meets%20Autoregressive%20Decoding%3A%20Towards%20More%0A%20%20Effective%20and%20Efficient%20Linearized%20Large%20Language%20Models%0AAuthor%3A%20Haoran%20You%20and%20Yichao%20Fu%20and%20Zheng%20Wang%20and%20Amir%20Yazdanbakhsh%20and%20Yingyan%20Celine%20Lin%0AAbstract%3A%20%20%20Autoregressive%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20impressive%0Aperformance%20in%20language%20tasks%20but%20face%20two%20significant%20bottlenecks%3A%20%281%29%0Aquadratic%20complexity%20in%20the%20attention%20module%20as%20the%20number%20of%20tokens%20increases%2C%0Aand%20%282%29%20limited%20efficiency%20due%20to%20the%20sequential%20processing%20nature%20of%0Aautoregressive%20LLMs%20during%20generation.%20While%20linear%20attention%20and%20speculative%0Adecoding%20offer%20potential%20solutions%2C%20their%20applicability%20and%20synergistic%0Apotential%20for%20enhancing%20autoregressive%20LLMs%20remain%20uncertain.%20We%20conduct%20the%0Afirst%20comprehensive%20study%20on%20the%20efficacy%20of%20existing%20linear%20attention%20methods%0Afor%20autoregressive%20LLMs%2C%20integrating%20them%20with%20speculative%20decoding.%20We%0Aintroduce%20an%20augmentation%20technique%20for%20linear%20attention%20that%20ensures%0Acompatibility%20with%20speculative%20decoding%2C%20enabling%20more%20efficient%20training%20and%0Aserving%20of%20LLMs.%20Extensive%20experiments%20and%20ablation%20studies%20involving%20seven%0Aexisting%20linear%20attention%20models%20and%20five%20encoder/decoder-based%20LLMs%0Aconsistently%20validate%20the%20effectiveness%20of%20our%20augmented%20linearized%20LLMs.%0ANotably%2C%20our%20approach%20achieves%20up%20to%20a%206.67%20reduction%20in%20perplexity%20on%20the%0ALLaMA%20model%20and%20up%20to%20a%202%24%5Ctimes%24%20speedup%20during%20generation%20compared%20to%20prior%0Alinear%20attention%20methods.%20Codes%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/GATECH-EIC/Linearized-LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07368v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Linear%2520Attention%2520Meets%2520Autoregressive%2520Decoding%253A%2520Towards%2520More%250A%2520%2520Effective%2520and%2520Efficient%2520Linearized%2520Large%2520Language%2520Models%26entry.906535625%3DHaoran%2520You%2520and%2520Yichao%2520Fu%2520and%2520Zheng%2520Wang%2520and%2520Amir%2520Yazdanbakhsh%2520and%2520Yingyan%2520Celine%2520Lin%26entry.1292438233%3D%2520%2520Autoregressive%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520achieved%2520impressive%250Aperformance%2520in%2520language%2520tasks%2520but%2520face%2520two%2520significant%2520bottlenecks%253A%2520%25281%2529%250Aquadratic%2520complexity%2520in%2520the%2520attention%2520module%2520as%2520the%2520number%2520of%2520tokens%2520increases%252C%250Aand%2520%25282%2529%2520limited%2520efficiency%2520due%2520to%2520the%2520sequential%2520processing%2520nature%2520of%250Aautoregressive%2520LLMs%2520during%2520generation.%2520While%2520linear%2520attention%2520and%2520speculative%250Adecoding%2520offer%2520potential%2520solutions%252C%2520their%2520applicability%2520and%2520synergistic%250Apotential%2520for%2520enhancing%2520autoregressive%2520LLMs%2520remain%2520uncertain.%2520We%2520conduct%2520the%250Afirst%2520comprehensive%2520study%2520on%2520the%2520efficacy%2520of%2520existing%2520linear%2520attention%2520methods%250Afor%2520autoregressive%2520LLMs%252C%2520integrating%2520them%2520with%2520speculative%2520decoding.%2520We%250Aintroduce%2520an%2520augmentation%2520technique%2520for%2520linear%2520attention%2520that%2520ensures%250Acompatibility%2520with%2520speculative%2520decoding%252C%2520enabling%2520more%2520efficient%2520training%2520and%250Aserving%2520of%2520LLMs.%2520Extensive%2520experiments%2520and%2520ablation%2520studies%2520involving%2520seven%250Aexisting%2520linear%2520attention%2520models%2520and%2520five%2520encoder/decoder-based%2520LLMs%250Aconsistently%2520validate%2520the%2520effectiveness%2520of%2520our%2520augmented%2520linearized%2520LLMs.%250ANotably%252C%2520our%2520approach%2520achieves%2520up%2520to%2520a%25206.67%2520reduction%2520in%2520perplexity%2520on%2520the%250ALLaMA%2520model%2520and%2520up%2520to%2520a%25202%2524%255Ctimes%2524%2520speedup%2520during%2520generation%2520compared%2520to%2520prior%250Alinear%2520attention%2520methods.%2520Codes%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/GATECH-EIC/Linearized-LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07368v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Linear%20Attention%20Meets%20Autoregressive%20Decoding%3A%20Towards%20More%0A%20%20Effective%20and%20Efficient%20Linearized%20Large%20Language%20Models&entry.906535625=Haoran%20You%20and%20Yichao%20Fu%20and%20Zheng%20Wang%20and%20Amir%20Yazdanbakhsh%20and%20Yingyan%20Celine%20Lin&entry.1292438233=%20%20Autoregressive%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20impressive%0Aperformance%20in%20language%20tasks%20but%20face%20two%20significant%20bottlenecks%3A%20%281%29%0Aquadratic%20complexity%20in%20the%20attention%20module%20as%20the%20number%20of%20tokens%20increases%2C%0Aand%20%282%29%20limited%20efficiency%20due%20to%20the%20sequential%20processing%20nature%20of%0Aautoregressive%20LLMs%20during%20generation.%20While%20linear%20attention%20and%20speculative%0Adecoding%20offer%20potential%20solutions%2C%20their%20applicability%20and%20synergistic%0Apotential%20for%20enhancing%20autoregressive%20LLMs%20remain%20uncertain.%20We%20conduct%20the%0Afirst%20comprehensive%20study%20on%20the%20efficacy%20of%20existing%20linear%20attention%20methods%0Afor%20autoregressive%20LLMs%2C%20integrating%20them%20with%20speculative%20decoding.%20We%0Aintroduce%20an%20augmentation%20technique%20for%20linear%20attention%20that%20ensures%0Acompatibility%20with%20speculative%20decoding%2C%20enabling%20more%20efficient%20training%20and%0Aserving%20of%20LLMs.%20Extensive%20experiments%20and%20ablation%20studies%20involving%20seven%0Aexisting%20linear%20attention%20models%20and%20five%20encoder/decoder-based%20LLMs%0Aconsistently%20validate%20the%20effectiveness%20of%20our%20augmented%20linearized%20LLMs.%0ANotably%2C%20our%20approach%20achieves%20up%20to%20a%206.67%20reduction%20in%20perplexity%20on%20the%0ALLaMA%20model%20and%20up%20to%20a%202%24%5Ctimes%24%20speedup%20during%20generation%20compared%20to%20prior%0Alinear%20attention%20methods.%20Codes%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/GATECH-EIC/Linearized-LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07368v2&entry.124074799=Read"},
{"title": "Pruning Boolean d-DNNF Circuits Through Tseitin-Awareness", "author": "Vincent Derkinderen", "abstract": "  Boolean circuits in d-DNNF form enable tractable probabilistic inference.\nHowever, as a key insight of this work, we show that commonly used d-DNNF\ncompilation approaches introduce irrelevant subcircuits. We call these\nsubcircuits Tseitin artifacts, as they are introduced due to the Tseitin\ntransformation step -- a well-established procedure to transform any circuit\ninto the CNF format required by several d-DNNF knowledge compilers. We discuss\nhow to detect and remove both Tseitin variables and Tseitin artifacts, leading\nto more succinct circuits. We empirically observe an average size reduction of\n77.5% when removing both Tseitin variables and artifacts. The additional\npruning of Tseitin artifacts reduces the size by 22.2% on average. This\nsignificantly improves downstream tasks that benefit from a more succinct\ncircuit, e.g., probabilistic inference tasks.\n", "link": "http://arxiv.org/abs/2407.17951v1", "date": "2024-07-25", "relevancy": 1.9762, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4142}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.3977}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pruning%20Boolean%20d-DNNF%20Circuits%20Through%20Tseitin-Awareness&body=Title%3A%20Pruning%20Boolean%20d-DNNF%20Circuits%20Through%20Tseitin-Awareness%0AAuthor%3A%20Vincent%20Derkinderen%0AAbstract%3A%20%20%20Boolean%20circuits%20in%20d-DNNF%20form%20enable%20tractable%20probabilistic%20inference.%0AHowever%2C%20as%20a%20key%20insight%20of%20this%20work%2C%20we%20show%20that%20commonly%20used%20d-DNNF%0Acompilation%20approaches%20introduce%20irrelevant%20subcircuits.%20We%20call%20these%0Asubcircuits%20Tseitin%20artifacts%2C%20as%20they%20are%20introduced%20due%20to%20the%20Tseitin%0Atransformation%20step%20--%20a%20well-established%20procedure%20to%20transform%20any%20circuit%0Ainto%20the%20CNF%20format%20required%20by%20several%20d-DNNF%20knowledge%20compilers.%20We%20discuss%0Ahow%20to%20detect%20and%20remove%20both%20Tseitin%20variables%20and%20Tseitin%20artifacts%2C%20leading%0Ato%20more%20succinct%20circuits.%20We%20empirically%20observe%20an%20average%20size%20reduction%20of%0A77.5%25%20when%20removing%20both%20Tseitin%20variables%20and%20artifacts.%20The%20additional%0Apruning%20of%20Tseitin%20artifacts%20reduces%20the%20size%20by%2022.2%25%20on%20average.%20This%0Asignificantly%20improves%20downstream%20tasks%20that%20benefit%20from%20a%20more%20succinct%0Acircuit%2C%20e.g.%2C%20probabilistic%20inference%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPruning%2520Boolean%2520d-DNNF%2520Circuits%2520Through%2520Tseitin-Awareness%26entry.906535625%3DVincent%2520Derkinderen%26entry.1292438233%3D%2520%2520Boolean%2520circuits%2520in%2520d-DNNF%2520form%2520enable%2520tractable%2520probabilistic%2520inference.%250AHowever%252C%2520as%2520a%2520key%2520insight%2520of%2520this%2520work%252C%2520we%2520show%2520that%2520commonly%2520used%2520d-DNNF%250Acompilation%2520approaches%2520introduce%2520irrelevant%2520subcircuits.%2520We%2520call%2520these%250Asubcircuits%2520Tseitin%2520artifacts%252C%2520as%2520they%2520are%2520introduced%2520due%2520to%2520the%2520Tseitin%250Atransformation%2520step%2520--%2520a%2520well-established%2520procedure%2520to%2520transform%2520any%2520circuit%250Ainto%2520the%2520CNF%2520format%2520required%2520by%2520several%2520d-DNNF%2520knowledge%2520compilers.%2520We%2520discuss%250Ahow%2520to%2520detect%2520and%2520remove%2520both%2520Tseitin%2520variables%2520and%2520Tseitin%2520artifacts%252C%2520leading%250Ato%2520more%2520succinct%2520circuits.%2520We%2520empirically%2520observe%2520an%2520average%2520size%2520reduction%2520of%250A77.5%2525%2520when%2520removing%2520both%2520Tseitin%2520variables%2520and%2520artifacts.%2520The%2520additional%250Apruning%2520of%2520Tseitin%2520artifacts%2520reduces%2520the%2520size%2520by%252022.2%2525%2520on%2520average.%2520This%250Asignificantly%2520improves%2520downstream%2520tasks%2520that%2520benefit%2520from%2520a%2520more%2520succinct%250Acircuit%252C%2520e.g.%252C%2520probabilistic%2520inference%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pruning%20Boolean%20d-DNNF%20Circuits%20Through%20Tseitin-Awareness&entry.906535625=Vincent%20Derkinderen&entry.1292438233=%20%20Boolean%20circuits%20in%20d-DNNF%20form%20enable%20tractable%20probabilistic%20inference.%0AHowever%2C%20as%20a%20key%20insight%20of%20this%20work%2C%20we%20show%20that%20commonly%20used%20d-DNNF%0Acompilation%20approaches%20introduce%20irrelevant%20subcircuits.%20We%20call%20these%0Asubcircuits%20Tseitin%20artifacts%2C%20as%20they%20are%20introduced%20due%20to%20the%20Tseitin%0Atransformation%20step%20--%20a%20well-established%20procedure%20to%20transform%20any%20circuit%0Ainto%20the%20CNF%20format%20required%20by%20several%20d-DNNF%20knowledge%20compilers.%20We%20discuss%0Ahow%20to%20detect%20and%20remove%20both%20Tseitin%20variables%20and%20Tseitin%20artifacts%2C%20leading%0Ato%20more%20succinct%20circuits.%20We%20empirically%20observe%20an%20average%20size%20reduction%20of%0A77.5%25%20when%20removing%20both%20Tseitin%20variables%20and%20artifacts.%20The%20additional%0Apruning%20of%20Tseitin%20artifacts%20reduces%20the%20size%20by%2022.2%25%20on%20average.%20This%0Asignificantly%20improves%20downstream%20tasks%20that%20benefit%20from%20a%20more%20succinct%0Acircuit%2C%20e.g.%2C%20probabilistic%20inference%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17951v1&entry.124074799=Read"},
{"title": "Relating the Seemingly Unrelated: Principled Understanding of\n  Generalization for Generative Models in Arithmetic Reasoning Tasks", "author": "Xingcheng Xu and Zibo Zhao and Haipeng Zhang and Yanqing Yang", "abstract": "  Large language models (LLMs) have demonstrated impressive versatility across\nnumerous tasks, yet their generalization capabilities remain poorly understood.\nTo investigate these behaviors, arithmetic tasks serve as important venues. In\nprevious studies, seemingly unrelated mysteries still exist -- (1) models with\nappropriate positional embeddings can correctly perform longer unseen\narithmetic operations such as addition, but their effectiveness varies in more\ncomplex tasks like multiplication; (2) models perform well for longer unseen\ncases in modular addition under specific moduli (e.g., modulo 100) but struggle\nunder very close moduli (e.g., modulo 101), regardless of the positional\nencoding used. We believe previous studies have been treating the symptoms\nrather than addressing the root cause -- they have paid excessive attention to\nimproving model components, while overlooking the differences in task\nproperties that may be the real drivers. This is confirmed by our unified\ntheoretical framework for different arithmetic scenarios. For example, unlike\nmultiplication, the digital addition task has the property of translation\ninvariance which naturally aligns with the relative positional encoding, and\nthis combination leads to successful generalization of addition to unseen\nlonger domains. The discrepancy in operations modulo 100 and 101 arises from\nthe base. Modulo 100, unlike 101, is compatible with the decimal system (base\n10), such that unseen information in digits beyond the units digit and the tens\ndigit is actually not needed for the task. Extensive experiments with GPT-like\nmodels validate our theoretical predictions. These findings deepen our\nunderstanding of the generalization mechanisms, and facilitate more\ndata-efficient model training and objective-oriented AI alignment.\n", "link": "http://arxiv.org/abs/2407.17963v1", "date": "2024-07-25", "relevancy": 1.9736, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5175}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4934}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relating%20the%20Seemingly%20Unrelated%3A%20Principled%20Understanding%20of%0A%20%20Generalization%20for%20Generative%20Models%20in%20Arithmetic%20Reasoning%20Tasks&body=Title%3A%20Relating%20the%20Seemingly%20Unrelated%3A%20Principled%20Understanding%20of%0A%20%20Generalization%20for%20Generative%20Models%20in%20Arithmetic%20Reasoning%20Tasks%0AAuthor%3A%20Xingcheng%20Xu%20and%20Zibo%20Zhao%20and%20Haipeng%20Zhang%20and%20Yanqing%20Yang%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20versatility%20across%0Anumerous%20tasks%2C%20yet%20their%20generalization%20capabilities%20remain%20poorly%20understood.%0ATo%20investigate%20these%20behaviors%2C%20arithmetic%20tasks%20serve%20as%20important%20venues.%20In%0Aprevious%20studies%2C%20seemingly%20unrelated%20mysteries%20still%20exist%20--%20%281%29%20models%20with%0Aappropriate%20positional%20embeddings%20can%20correctly%20perform%20longer%20unseen%0Aarithmetic%20operations%20such%20as%20addition%2C%20but%20their%20effectiveness%20varies%20in%20more%0Acomplex%20tasks%20like%20multiplication%3B%20%282%29%20models%20perform%20well%20for%20longer%20unseen%0Acases%20in%20modular%20addition%20under%20specific%20moduli%20%28e.g.%2C%20modulo%20100%29%20but%20struggle%0Aunder%20very%20close%20moduli%20%28e.g.%2C%20modulo%20101%29%2C%20regardless%20of%20the%20positional%0Aencoding%20used.%20We%20believe%20previous%20studies%20have%20been%20treating%20the%20symptoms%0Arather%20than%20addressing%20the%20root%20cause%20--%20they%20have%20paid%20excessive%20attention%20to%0Aimproving%20model%20components%2C%20while%20overlooking%20the%20differences%20in%20task%0Aproperties%20that%20may%20be%20the%20real%20drivers.%20This%20is%20confirmed%20by%20our%20unified%0Atheoretical%20framework%20for%20different%20arithmetic%20scenarios.%20For%20example%2C%20unlike%0Amultiplication%2C%20the%20digital%20addition%20task%20has%20the%20property%20of%20translation%0Ainvariance%20which%20naturally%20aligns%20with%20the%20relative%20positional%20encoding%2C%20and%0Athis%20combination%20leads%20to%20successful%20generalization%20of%20addition%20to%20unseen%0Alonger%20domains.%20The%20discrepancy%20in%20operations%20modulo%20100%20and%20101%20arises%20from%0Athe%20base.%20Modulo%20100%2C%20unlike%20101%2C%20is%20compatible%20with%20the%20decimal%20system%20%28base%0A10%29%2C%20such%20that%20unseen%20information%20in%20digits%20beyond%20the%20units%20digit%20and%20the%20tens%0Adigit%20is%20actually%20not%20needed%20for%20the%20task.%20Extensive%20experiments%20with%20GPT-like%0Amodels%20validate%20our%20theoretical%20predictions.%20These%20findings%20deepen%20our%0Aunderstanding%20of%20the%20generalization%20mechanisms%2C%20and%20facilitate%20more%0Adata-efficient%20model%20training%20and%20objective-oriented%20AI%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17963v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelating%2520the%2520Seemingly%2520Unrelated%253A%2520Principled%2520Understanding%2520of%250A%2520%2520Generalization%2520for%2520Generative%2520Models%2520in%2520Arithmetic%2520Reasoning%2520Tasks%26entry.906535625%3DXingcheng%2520Xu%2520and%2520Zibo%2520Zhao%2520and%2520Haipeng%2520Zhang%2520and%2520Yanqing%2520Yang%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520impressive%2520versatility%2520across%250Anumerous%2520tasks%252C%2520yet%2520their%2520generalization%2520capabilities%2520remain%2520poorly%2520understood.%250ATo%2520investigate%2520these%2520behaviors%252C%2520arithmetic%2520tasks%2520serve%2520as%2520important%2520venues.%2520In%250Aprevious%2520studies%252C%2520seemingly%2520unrelated%2520mysteries%2520still%2520exist%2520--%2520%25281%2529%2520models%2520with%250Aappropriate%2520positional%2520embeddings%2520can%2520correctly%2520perform%2520longer%2520unseen%250Aarithmetic%2520operations%2520such%2520as%2520addition%252C%2520but%2520their%2520effectiveness%2520varies%2520in%2520more%250Acomplex%2520tasks%2520like%2520multiplication%253B%2520%25282%2529%2520models%2520perform%2520well%2520for%2520longer%2520unseen%250Acases%2520in%2520modular%2520addition%2520under%2520specific%2520moduli%2520%2528e.g.%252C%2520modulo%2520100%2529%2520but%2520struggle%250Aunder%2520very%2520close%2520moduli%2520%2528e.g.%252C%2520modulo%2520101%2529%252C%2520regardless%2520of%2520the%2520positional%250Aencoding%2520used.%2520We%2520believe%2520previous%2520studies%2520have%2520been%2520treating%2520the%2520symptoms%250Arather%2520than%2520addressing%2520the%2520root%2520cause%2520--%2520they%2520have%2520paid%2520excessive%2520attention%2520to%250Aimproving%2520model%2520components%252C%2520while%2520overlooking%2520the%2520differences%2520in%2520task%250Aproperties%2520that%2520may%2520be%2520the%2520real%2520drivers.%2520This%2520is%2520confirmed%2520by%2520our%2520unified%250Atheoretical%2520framework%2520for%2520different%2520arithmetic%2520scenarios.%2520For%2520example%252C%2520unlike%250Amultiplication%252C%2520the%2520digital%2520addition%2520task%2520has%2520the%2520property%2520of%2520translation%250Ainvariance%2520which%2520naturally%2520aligns%2520with%2520the%2520relative%2520positional%2520encoding%252C%2520and%250Athis%2520combination%2520leads%2520to%2520successful%2520generalization%2520of%2520addition%2520to%2520unseen%250Alonger%2520domains.%2520The%2520discrepancy%2520in%2520operations%2520modulo%2520100%2520and%2520101%2520arises%2520from%250Athe%2520base.%2520Modulo%2520100%252C%2520unlike%2520101%252C%2520is%2520compatible%2520with%2520the%2520decimal%2520system%2520%2528base%250A10%2529%252C%2520such%2520that%2520unseen%2520information%2520in%2520digits%2520beyond%2520the%2520units%2520digit%2520and%2520the%2520tens%250Adigit%2520is%2520actually%2520not%2520needed%2520for%2520the%2520task.%2520Extensive%2520experiments%2520with%2520GPT-like%250Amodels%2520validate%2520our%2520theoretical%2520predictions.%2520These%2520findings%2520deepen%2520our%250Aunderstanding%2520of%2520the%2520generalization%2520mechanisms%252C%2520and%2520facilitate%2520more%250Adata-efficient%2520model%2520training%2520and%2520objective-oriented%2520AI%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17963v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relating%20the%20Seemingly%20Unrelated%3A%20Principled%20Understanding%20of%0A%20%20Generalization%20for%20Generative%20Models%20in%20Arithmetic%20Reasoning%20Tasks&entry.906535625=Xingcheng%20Xu%20and%20Zibo%20Zhao%20and%20Haipeng%20Zhang%20and%20Yanqing%20Yang&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20impressive%20versatility%20across%0Anumerous%20tasks%2C%20yet%20their%20generalization%20capabilities%20remain%20poorly%20understood.%0ATo%20investigate%20these%20behaviors%2C%20arithmetic%20tasks%20serve%20as%20important%20venues.%20In%0Aprevious%20studies%2C%20seemingly%20unrelated%20mysteries%20still%20exist%20--%20%281%29%20models%20with%0Aappropriate%20positional%20embeddings%20can%20correctly%20perform%20longer%20unseen%0Aarithmetic%20operations%20such%20as%20addition%2C%20but%20their%20effectiveness%20varies%20in%20more%0Acomplex%20tasks%20like%20multiplication%3B%20%282%29%20models%20perform%20well%20for%20longer%20unseen%0Acases%20in%20modular%20addition%20under%20specific%20moduli%20%28e.g.%2C%20modulo%20100%29%20but%20struggle%0Aunder%20very%20close%20moduli%20%28e.g.%2C%20modulo%20101%29%2C%20regardless%20of%20the%20positional%0Aencoding%20used.%20We%20believe%20previous%20studies%20have%20been%20treating%20the%20symptoms%0Arather%20than%20addressing%20the%20root%20cause%20--%20they%20have%20paid%20excessive%20attention%20to%0Aimproving%20model%20components%2C%20while%20overlooking%20the%20differences%20in%20task%0Aproperties%20that%20may%20be%20the%20real%20drivers.%20This%20is%20confirmed%20by%20our%20unified%0Atheoretical%20framework%20for%20different%20arithmetic%20scenarios.%20For%20example%2C%20unlike%0Amultiplication%2C%20the%20digital%20addition%20task%20has%20the%20property%20of%20translation%0Ainvariance%20which%20naturally%20aligns%20with%20the%20relative%20positional%20encoding%2C%20and%0Athis%20combination%20leads%20to%20successful%20generalization%20of%20addition%20to%20unseen%0Alonger%20domains.%20The%20discrepancy%20in%20operations%20modulo%20100%20and%20101%20arises%20from%0Athe%20base.%20Modulo%20100%2C%20unlike%20101%2C%20is%20compatible%20with%20the%20decimal%20system%20%28base%0A10%29%2C%20such%20that%20unseen%20information%20in%20digits%20beyond%20the%20units%20digit%20and%20the%20tens%0Adigit%20is%20actually%20not%20needed%20for%20the%20task.%20Extensive%20experiments%20with%20GPT-like%0Amodels%20validate%20our%20theoretical%20predictions.%20These%20findings%20deepen%20our%0Aunderstanding%20of%20the%20generalization%20mechanisms%2C%20and%20facilitate%20more%0Adata-efficient%20model%20training%20and%20objective-oriented%20AI%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17963v1&entry.124074799=Read"},
{"title": "A unified theory and statistical learning approach for traffic conflict\n  detection", "author": "Yiru Jiao and Simeon C. Calvert and Sander van Cranenburgh and Hans van Lint", "abstract": "  This study proposes a unified theory and statistical learning approach for\ntraffic conflict detection, addressing the long-existing call for a consistent\nand comprehensive methodology to evaluate the collision risk emerging in road\nuser interactions. The proposed theory assumes context-dependent probabilistic\ncollision risk and frames conflict detection as assessing this risk by\nstatistical learning of extreme events in daily interactions. Experiments using\nreal-world trajectory data are conducted in this study, where a unified metric\nof conflict is trained with lane-changing interactions on German highways and\napplied to near-crash events from the 100-Car Naturalistic Driving Study in the\nU.S. Results of the experiments demonstrate that the trained metric provides\neffective collision warnings, generalises across distinct datasets and traffic\nenvironments, covers a broad range of conflicts, and delivers a long-tailed\ndistribution of conflict intensity. Reflecting on these results, the unified\ntheory ensures consistent evaluation by a generic formulation that encompasses\nvarying assumptions of traffic conflicts; the statistical learning approach\nthen enables a comprehensive consideration of influencing factors such as\nmotion states of road users, environment conditions, and participant\ncharacteristics. Therefore, the theory and learning approach jointly provide an\nexplainable and adaptable methodology for conflict detection among different\nroad users and across various interaction scenarios. This promises to reduce\naccidents and improve overall traffic safety, by enhanced safety assessment of\ntraffic infrastructures, more effective collision warning systems for\nautonomous driving, and a deeper understanding of road user behaviour in\ndifferent traffic conditions.\n", "link": "http://arxiv.org/abs/2407.10959v2", "date": "2024-07-25", "relevancy": 1.9708, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5163}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4759}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20unified%20theory%20and%20statistical%20learning%20approach%20for%20traffic%20conflict%0A%20%20detection&body=Title%3A%20A%20unified%20theory%20and%20statistical%20learning%20approach%20for%20traffic%20conflict%0A%20%20detection%0AAuthor%3A%20Yiru%20Jiao%20and%20Simeon%20C.%20Calvert%20and%20Sander%20van%20Cranenburgh%20and%20Hans%20van%20Lint%0AAbstract%3A%20%20%20This%20study%20proposes%20a%20unified%20theory%20and%20statistical%20learning%20approach%20for%0Atraffic%20conflict%20detection%2C%20addressing%20the%20long-existing%20call%20for%20a%20consistent%0Aand%20comprehensive%20methodology%20to%20evaluate%20the%20collision%20risk%20emerging%20in%20road%0Auser%20interactions.%20The%20proposed%20theory%20assumes%20context-dependent%20probabilistic%0Acollision%20risk%20and%20frames%20conflict%20detection%20as%20assessing%20this%20risk%20by%0Astatistical%20learning%20of%20extreme%20events%20in%20daily%20interactions.%20Experiments%20using%0Areal-world%20trajectory%20data%20are%20conducted%20in%20this%20study%2C%20where%20a%20unified%20metric%0Aof%20conflict%20is%20trained%20with%20lane-changing%20interactions%20on%20German%20highways%20and%0Aapplied%20to%20near-crash%20events%20from%20the%20100-Car%20Naturalistic%20Driving%20Study%20in%20the%0AU.S.%20Results%20of%20the%20experiments%20demonstrate%20that%20the%20trained%20metric%20provides%0Aeffective%20collision%20warnings%2C%20generalises%20across%20distinct%20datasets%20and%20traffic%0Aenvironments%2C%20covers%20a%20broad%20range%20of%20conflicts%2C%20and%20delivers%20a%20long-tailed%0Adistribution%20of%20conflict%20intensity.%20Reflecting%20on%20these%20results%2C%20the%20unified%0Atheory%20ensures%20consistent%20evaluation%20by%20a%20generic%20formulation%20that%20encompasses%0Avarying%20assumptions%20of%20traffic%20conflicts%3B%20the%20statistical%20learning%20approach%0Athen%20enables%20a%20comprehensive%20consideration%20of%20influencing%20factors%20such%20as%0Amotion%20states%20of%20road%20users%2C%20environment%20conditions%2C%20and%20participant%0Acharacteristics.%20Therefore%2C%20the%20theory%20and%20learning%20approach%20jointly%20provide%20an%0Aexplainable%20and%20adaptable%20methodology%20for%20conflict%20detection%20among%20different%0Aroad%20users%20and%20across%20various%20interaction%20scenarios.%20This%20promises%20to%20reduce%0Aaccidents%20and%20improve%20overall%20traffic%20safety%2C%20by%20enhanced%20safety%20assessment%20of%0Atraffic%20infrastructures%2C%20more%20effective%20collision%20warning%20systems%20for%0Aautonomous%20driving%2C%20and%20a%20deeper%20understanding%20of%20road%20user%20behaviour%20in%0Adifferent%20traffic%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10959v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520unified%2520theory%2520and%2520statistical%2520learning%2520approach%2520for%2520traffic%2520conflict%250A%2520%2520detection%26entry.906535625%3DYiru%2520Jiao%2520and%2520Simeon%2520C.%2520Calvert%2520and%2520Sander%2520van%2520Cranenburgh%2520and%2520Hans%2520van%2520Lint%26entry.1292438233%3D%2520%2520This%2520study%2520proposes%2520a%2520unified%2520theory%2520and%2520statistical%2520learning%2520approach%2520for%250Atraffic%2520conflict%2520detection%252C%2520addressing%2520the%2520long-existing%2520call%2520for%2520a%2520consistent%250Aand%2520comprehensive%2520methodology%2520to%2520evaluate%2520the%2520collision%2520risk%2520emerging%2520in%2520road%250Auser%2520interactions.%2520The%2520proposed%2520theory%2520assumes%2520context-dependent%2520probabilistic%250Acollision%2520risk%2520and%2520frames%2520conflict%2520detection%2520as%2520assessing%2520this%2520risk%2520by%250Astatistical%2520learning%2520of%2520extreme%2520events%2520in%2520daily%2520interactions.%2520Experiments%2520using%250Areal-world%2520trajectory%2520data%2520are%2520conducted%2520in%2520this%2520study%252C%2520where%2520a%2520unified%2520metric%250Aof%2520conflict%2520is%2520trained%2520with%2520lane-changing%2520interactions%2520on%2520German%2520highways%2520and%250Aapplied%2520to%2520near-crash%2520events%2520from%2520the%2520100-Car%2520Naturalistic%2520Driving%2520Study%2520in%2520the%250AU.S.%2520Results%2520of%2520the%2520experiments%2520demonstrate%2520that%2520the%2520trained%2520metric%2520provides%250Aeffective%2520collision%2520warnings%252C%2520generalises%2520across%2520distinct%2520datasets%2520and%2520traffic%250Aenvironments%252C%2520covers%2520a%2520broad%2520range%2520of%2520conflicts%252C%2520and%2520delivers%2520a%2520long-tailed%250Adistribution%2520of%2520conflict%2520intensity.%2520Reflecting%2520on%2520these%2520results%252C%2520the%2520unified%250Atheory%2520ensures%2520consistent%2520evaluation%2520by%2520a%2520generic%2520formulation%2520that%2520encompasses%250Avarying%2520assumptions%2520of%2520traffic%2520conflicts%253B%2520the%2520statistical%2520learning%2520approach%250Athen%2520enables%2520a%2520comprehensive%2520consideration%2520of%2520influencing%2520factors%2520such%2520as%250Amotion%2520states%2520of%2520road%2520users%252C%2520environment%2520conditions%252C%2520and%2520participant%250Acharacteristics.%2520Therefore%252C%2520the%2520theory%2520and%2520learning%2520approach%2520jointly%2520provide%2520an%250Aexplainable%2520and%2520adaptable%2520methodology%2520for%2520conflict%2520detection%2520among%2520different%250Aroad%2520users%2520and%2520across%2520various%2520interaction%2520scenarios.%2520This%2520promises%2520to%2520reduce%250Aaccidents%2520and%2520improve%2520overall%2520traffic%2520safety%252C%2520by%2520enhanced%2520safety%2520assessment%2520of%250Atraffic%2520infrastructures%252C%2520more%2520effective%2520collision%2520warning%2520systems%2520for%250Aautonomous%2520driving%252C%2520and%2520a%2520deeper%2520understanding%2520of%2520road%2520user%2520behaviour%2520in%250Adifferent%2520traffic%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10959v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20unified%20theory%20and%20statistical%20learning%20approach%20for%20traffic%20conflict%0A%20%20detection&entry.906535625=Yiru%20Jiao%20and%20Simeon%20C.%20Calvert%20and%20Sander%20van%20Cranenburgh%20and%20Hans%20van%20Lint&entry.1292438233=%20%20This%20study%20proposes%20a%20unified%20theory%20and%20statistical%20learning%20approach%20for%0Atraffic%20conflict%20detection%2C%20addressing%20the%20long-existing%20call%20for%20a%20consistent%0Aand%20comprehensive%20methodology%20to%20evaluate%20the%20collision%20risk%20emerging%20in%20road%0Auser%20interactions.%20The%20proposed%20theory%20assumes%20context-dependent%20probabilistic%0Acollision%20risk%20and%20frames%20conflict%20detection%20as%20assessing%20this%20risk%20by%0Astatistical%20learning%20of%20extreme%20events%20in%20daily%20interactions.%20Experiments%20using%0Areal-world%20trajectory%20data%20are%20conducted%20in%20this%20study%2C%20where%20a%20unified%20metric%0Aof%20conflict%20is%20trained%20with%20lane-changing%20interactions%20on%20German%20highways%20and%0Aapplied%20to%20near-crash%20events%20from%20the%20100-Car%20Naturalistic%20Driving%20Study%20in%20the%0AU.S.%20Results%20of%20the%20experiments%20demonstrate%20that%20the%20trained%20metric%20provides%0Aeffective%20collision%20warnings%2C%20generalises%20across%20distinct%20datasets%20and%20traffic%0Aenvironments%2C%20covers%20a%20broad%20range%20of%20conflicts%2C%20and%20delivers%20a%20long-tailed%0Adistribution%20of%20conflict%20intensity.%20Reflecting%20on%20these%20results%2C%20the%20unified%0Atheory%20ensures%20consistent%20evaluation%20by%20a%20generic%20formulation%20that%20encompasses%0Avarying%20assumptions%20of%20traffic%20conflicts%3B%20the%20statistical%20learning%20approach%0Athen%20enables%20a%20comprehensive%20consideration%20of%20influencing%20factors%20such%20as%0Amotion%20states%20of%20road%20users%2C%20environment%20conditions%2C%20and%20participant%0Acharacteristics.%20Therefore%2C%20the%20theory%20and%20learning%20approach%20jointly%20provide%20an%0Aexplainable%20and%20adaptable%20methodology%20for%20conflict%20detection%20among%20different%0Aroad%20users%20and%20across%20various%20interaction%20scenarios.%20This%20promises%20to%20reduce%0Aaccidents%20and%20improve%20overall%20traffic%20safety%2C%20by%20enhanced%20safety%20assessment%20of%0Atraffic%20infrastructures%2C%20more%20effective%20collision%20warning%20systems%20for%0Aautonomous%20driving%2C%20and%20a%20deeper%20understanding%20of%20road%20user%20behaviour%20in%0Adifferent%20traffic%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10959v2&entry.124074799=Read"},
{"title": "Hierarchical Object Detection and Recognition Framework for Practical\n  Plant Disease Diagnosis", "author": "Kohei Iwano and Shogo Shibuya and Satoshi Kagiwada and Hitoshi Iyatomi", "abstract": "  Recently, object detection methods (OD; e.g., YOLO-based models) have been\nwidely utilized in plant disease diagnosis. These methods demonstrate\nrobustness to distance variations and excel at detecting small lesions compared\nto classification methods (CL; e.g., CNN models). However, there are issues\nsuch as low diagnostic performance for hard-to-detect diseases and high\nlabeling costs. Additionally, since healthy cases cannot be explicitly trained,\nthere is a risk of false positives. We propose the Hierarchical object\ndetection and recognition framework (HODRF), a sophisticated and highly\nintegrated two-stage system that combines the strengths of both OD and CL for\nplant disease diagnosis. In the first stage, HODRF uses OD to identify regions\nof interest (ROIs) without specifying the disease. In the second stage, CL\ndiagnoses diseases surrounding the ROIs. HODRF offers several advantages: (1)\nSince OD detects only one type of ROI, HODRF can detect diseases with limited\ntraining images by leveraging its ability to identify other lesions. (2) While\nOD over-detects healthy cases, HODRF significantly reduces these errors by\nusing CL in the second stage. (3) CL's accuracy improves in HODRF as it\nidentifies diagnostic targets given as ROIs, making it less vulnerable to size\nchanges. (4) HODRF benefits from CL's lower annotation costs, allowing it to\nlearn from a larger number of images. We implemented HODRF using YOLOv7 for OD\nand EfficientNetV2 for CL and evaluated its performance on a large-scale\ndataset (4 crops, 20 diseased and healthy classes, 281K images). HODRF\noutperformed YOLOv7 alone by 5.8 to 21.5 points on healthy data and 0.6 to 7.5\npoints on macro F1 scores, and it improved macro F1 by 1.1 to 7.2 points over\nEfficientNetV2.\n", "link": "http://arxiv.org/abs/2407.17906v1", "date": "2024-07-25", "relevancy": 1.9663, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5038}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4901}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Object%20Detection%20and%20Recognition%20Framework%20for%20Practical%0A%20%20Plant%20Disease%20Diagnosis&body=Title%3A%20Hierarchical%20Object%20Detection%20and%20Recognition%20Framework%20for%20Practical%0A%20%20Plant%20Disease%20Diagnosis%0AAuthor%3A%20Kohei%20Iwano%20and%20Shogo%20Shibuya%20and%20Satoshi%20Kagiwada%20and%20Hitoshi%20Iyatomi%0AAbstract%3A%20%20%20Recently%2C%20object%20detection%20methods%20%28OD%3B%20e.g.%2C%20YOLO-based%20models%29%20have%20been%0Awidely%20utilized%20in%20plant%20disease%20diagnosis.%20These%20methods%20demonstrate%0Arobustness%20to%20distance%20variations%20and%20excel%20at%20detecting%20small%20lesions%20compared%0Ato%20classification%20methods%20%28CL%3B%20e.g.%2C%20CNN%20models%29.%20However%2C%20there%20are%20issues%0Asuch%20as%20low%20diagnostic%20performance%20for%20hard-to-detect%20diseases%20and%20high%0Alabeling%20costs.%20Additionally%2C%20since%20healthy%20cases%20cannot%20be%20explicitly%20trained%2C%0Athere%20is%20a%20risk%20of%20false%20positives.%20We%20propose%20the%20Hierarchical%20object%0Adetection%20and%20recognition%20framework%20%28HODRF%29%2C%20a%20sophisticated%20and%20highly%0Aintegrated%20two-stage%20system%20that%20combines%20the%20strengths%20of%20both%20OD%20and%20CL%20for%0Aplant%20disease%20diagnosis.%20In%20the%20first%20stage%2C%20HODRF%20uses%20OD%20to%20identify%20regions%0Aof%20interest%20%28ROIs%29%20without%20specifying%20the%20disease.%20In%20the%20second%20stage%2C%20CL%0Adiagnoses%20diseases%20surrounding%20the%20ROIs.%20HODRF%20offers%20several%20advantages%3A%20%281%29%0ASince%20OD%20detects%20only%20one%20type%20of%20ROI%2C%20HODRF%20can%20detect%20diseases%20with%20limited%0Atraining%20images%20by%20leveraging%20its%20ability%20to%20identify%20other%20lesions.%20%282%29%20While%0AOD%20over-detects%20healthy%20cases%2C%20HODRF%20significantly%20reduces%20these%20errors%20by%0Ausing%20CL%20in%20the%20second%20stage.%20%283%29%20CL%27s%20accuracy%20improves%20in%20HODRF%20as%20it%0Aidentifies%20diagnostic%20targets%20given%20as%20ROIs%2C%20making%20it%20less%20vulnerable%20to%20size%0Achanges.%20%284%29%20HODRF%20benefits%20from%20CL%27s%20lower%20annotation%20costs%2C%20allowing%20it%20to%0Alearn%20from%20a%20larger%20number%20of%20images.%20We%20implemented%20HODRF%20using%20YOLOv7%20for%20OD%0Aand%20EfficientNetV2%20for%20CL%20and%20evaluated%20its%20performance%20on%20a%20large-scale%0Adataset%20%284%20crops%2C%2020%20diseased%20and%20healthy%20classes%2C%20281K%20images%29.%20HODRF%0Aoutperformed%20YOLOv7%20alone%20by%205.8%20to%2021.5%20points%20on%20healthy%20data%20and%200.6%20to%207.5%0Apoints%20on%20macro%20F1%20scores%2C%20and%20it%20improved%20macro%20F1%20by%201.1%20to%207.2%20points%20over%0AEfficientNetV2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Object%2520Detection%2520and%2520Recognition%2520Framework%2520for%2520Practical%250A%2520%2520Plant%2520Disease%2520Diagnosis%26entry.906535625%3DKohei%2520Iwano%2520and%2520Shogo%2520Shibuya%2520and%2520Satoshi%2520Kagiwada%2520and%2520Hitoshi%2520Iyatomi%26entry.1292438233%3D%2520%2520Recently%252C%2520object%2520detection%2520methods%2520%2528OD%253B%2520e.g.%252C%2520YOLO-based%2520models%2529%2520have%2520been%250Awidely%2520utilized%2520in%2520plant%2520disease%2520diagnosis.%2520These%2520methods%2520demonstrate%250Arobustness%2520to%2520distance%2520variations%2520and%2520excel%2520at%2520detecting%2520small%2520lesions%2520compared%250Ato%2520classification%2520methods%2520%2528CL%253B%2520e.g.%252C%2520CNN%2520models%2529.%2520However%252C%2520there%2520are%2520issues%250Asuch%2520as%2520low%2520diagnostic%2520performance%2520for%2520hard-to-detect%2520diseases%2520and%2520high%250Alabeling%2520costs.%2520Additionally%252C%2520since%2520healthy%2520cases%2520cannot%2520be%2520explicitly%2520trained%252C%250Athere%2520is%2520a%2520risk%2520of%2520false%2520positives.%2520We%2520propose%2520the%2520Hierarchical%2520object%250Adetection%2520and%2520recognition%2520framework%2520%2528HODRF%2529%252C%2520a%2520sophisticated%2520and%2520highly%250Aintegrated%2520two-stage%2520system%2520that%2520combines%2520the%2520strengths%2520of%2520both%2520OD%2520and%2520CL%2520for%250Aplant%2520disease%2520diagnosis.%2520In%2520the%2520first%2520stage%252C%2520HODRF%2520uses%2520OD%2520to%2520identify%2520regions%250Aof%2520interest%2520%2528ROIs%2529%2520without%2520specifying%2520the%2520disease.%2520In%2520the%2520second%2520stage%252C%2520CL%250Adiagnoses%2520diseases%2520surrounding%2520the%2520ROIs.%2520HODRF%2520offers%2520several%2520advantages%253A%2520%25281%2529%250ASince%2520OD%2520detects%2520only%2520one%2520type%2520of%2520ROI%252C%2520HODRF%2520can%2520detect%2520diseases%2520with%2520limited%250Atraining%2520images%2520by%2520leveraging%2520its%2520ability%2520to%2520identify%2520other%2520lesions.%2520%25282%2529%2520While%250AOD%2520over-detects%2520healthy%2520cases%252C%2520HODRF%2520significantly%2520reduces%2520these%2520errors%2520by%250Ausing%2520CL%2520in%2520the%2520second%2520stage.%2520%25283%2529%2520CL%2527s%2520accuracy%2520improves%2520in%2520HODRF%2520as%2520it%250Aidentifies%2520diagnostic%2520targets%2520given%2520as%2520ROIs%252C%2520making%2520it%2520less%2520vulnerable%2520to%2520size%250Achanges.%2520%25284%2529%2520HODRF%2520benefits%2520from%2520CL%2527s%2520lower%2520annotation%2520costs%252C%2520allowing%2520it%2520to%250Alearn%2520from%2520a%2520larger%2520number%2520of%2520images.%2520We%2520implemented%2520HODRF%2520using%2520YOLOv7%2520for%2520OD%250Aand%2520EfficientNetV2%2520for%2520CL%2520and%2520evaluated%2520its%2520performance%2520on%2520a%2520large-scale%250Adataset%2520%25284%2520crops%252C%252020%2520diseased%2520and%2520healthy%2520classes%252C%2520281K%2520images%2529.%2520HODRF%250Aoutperformed%2520YOLOv7%2520alone%2520by%25205.8%2520to%252021.5%2520points%2520on%2520healthy%2520data%2520and%25200.6%2520to%25207.5%250Apoints%2520on%2520macro%2520F1%2520scores%252C%2520and%2520it%2520improved%2520macro%2520F1%2520by%25201.1%2520to%25207.2%2520points%2520over%250AEfficientNetV2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Object%20Detection%20and%20Recognition%20Framework%20for%20Practical%0A%20%20Plant%20Disease%20Diagnosis&entry.906535625=Kohei%20Iwano%20and%20Shogo%20Shibuya%20and%20Satoshi%20Kagiwada%20and%20Hitoshi%20Iyatomi&entry.1292438233=%20%20Recently%2C%20object%20detection%20methods%20%28OD%3B%20e.g.%2C%20YOLO-based%20models%29%20have%20been%0Awidely%20utilized%20in%20plant%20disease%20diagnosis.%20These%20methods%20demonstrate%0Arobustness%20to%20distance%20variations%20and%20excel%20at%20detecting%20small%20lesions%20compared%0Ato%20classification%20methods%20%28CL%3B%20e.g.%2C%20CNN%20models%29.%20However%2C%20there%20are%20issues%0Asuch%20as%20low%20diagnostic%20performance%20for%20hard-to-detect%20diseases%20and%20high%0Alabeling%20costs.%20Additionally%2C%20since%20healthy%20cases%20cannot%20be%20explicitly%20trained%2C%0Athere%20is%20a%20risk%20of%20false%20positives.%20We%20propose%20the%20Hierarchical%20object%0Adetection%20and%20recognition%20framework%20%28HODRF%29%2C%20a%20sophisticated%20and%20highly%0Aintegrated%20two-stage%20system%20that%20combines%20the%20strengths%20of%20both%20OD%20and%20CL%20for%0Aplant%20disease%20diagnosis.%20In%20the%20first%20stage%2C%20HODRF%20uses%20OD%20to%20identify%20regions%0Aof%20interest%20%28ROIs%29%20without%20specifying%20the%20disease.%20In%20the%20second%20stage%2C%20CL%0Adiagnoses%20diseases%20surrounding%20the%20ROIs.%20HODRF%20offers%20several%20advantages%3A%20%281%29%0ASince%20OD%20detects%20only%20one%20type%20of%20ROI%2C%20HODRF%20can%20detect%20diseases%20with%20limited%0Atraining%20images%20by%20leveraging%20its%20ability%20to%20identify%20other%20lesions.%20%282%29%20While%0AOD%20over-detects%20healthy%20cases%2C%20HODRF%20significantly%20reduces%20these%20errors%20by%0Ausing%20CL%20in%20the%20second%20stage.%20%283%29%20CL%27s%20accuracy%20improves%20in%20HODRF%20as%20it%0Aidentifies%20diagnostic%20targets%20given%20as%20ROIs%2C%20making%20it%20less%20vulnerable%20to%20size%0Achanges.%20%284%29%20HODRF%20benefits%20from%20CL%27s%20lower%20annotation%20costs%2C%20allowing%20it%20to%0Alearn%20from%20a%20larger%20number%20of%20images.%20We%20implemented%20HODRF%20using%20YOLOv7%20for%20OD%0Aand%20EfficientNetV2%20for%20CL%20and%20evaluated%20its%20performance%20on%20a%20large-scale%0Adataset%20%284%20crops%2C%2020%20diseased%20and%20healthy%20classes%2C%20281K%20images%29.%20HODRF%0Aoutperformed%20YOLOv7%20alone%20by%205.8%20to%2021.5%20points%20on%20healthy%20data%20and%200.6%20to%207.5%0Apoints%20on%20macro%20F1%20scores%2C%20and%20it%20improved%20macro%20F1%20by%201.1%20to%207.2%20points%20over%0AEfficientNetV2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17906v1&entry.124074799=Read"},
{"title": "ECG Arrhythmia Detection Using Disease-specific Attention-based Deep\n  Learning Model", "author": "Linpeng Jin", "abstract": "  The electrocardiogram (ECG) is one of the most commonly-used tools to\ndiagnose cardiovascular disease in clinical practice. Although deep learning\nmodels have achieved very impressive success in the field of automatic ECG\nanalysis, they often lack model interpretability that is significantly\nimportant in the healthcare applications. To this end, many schemes such as\ngeneral-purpose attention mechanism, Grad-CAM technique and ECG knowledge graph\nwere proposed to be integrated with deep learning models. However, they either\nresult in decreased classification performance or do not consist with the one\nin cardiologists' mind when interpreting ECG. In this study, we propose a novel\ndisease-specific attention-based deep learning model (DANet) for arrhythmia\ndetection from short ECG recordings. The novel idea is to introduce a\nsoft-coding or hard-coding waveform enhanced module into existing deep neural\nnetworks, which amends original ECG signals with the guidance of the rule for\ndiagnosis of a given disease type before being fed into the classification\nmodule. For the soft-coding DANet, we also develop a learning framework\ncombining self-supervised pre-training with two-stage supervised training. To\nverify the effectiveness of our proposed DANet, we applied it to the problem of\natrial premature contraction detection and the experimental results shows that\nit demonstrates superior performance compared to the benchmark model. Moreover,\nit also provides the waveform regions that deserve special attention in the\nmodel's decision-making process, allowing it to be a medical diagnostic\nassistant for physicians.\n", "link": "http://arxiv.org/abs/2407.18033v1", "date": "2024-07-25", "relevancy": 1.9545, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5112}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4769}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4616}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ECG%20Arrhythmia%20Detection%20Using%20Disease-specific%20Attention-based%20Deep%0A%20%20Learning%20Model&body=Title%3A%20ECG%20Arrhythmia%20Detection%20Using%20Disease-specific%20Attention-based%20Deep%0A%20%20Learning%20Model%0AAuthor%3A%20Linpeng%20Jin%0AAbstract%3A%20%20%20The%20electrocardiogram%20%28ECG%29%20is%20one%20of%20the%20most%20commonly-used%20tools%20to%0Adiagnose%20cardiovascular%20disease%20in%20clinical%20practice.%20Although%20deep%20learning%0Amodels%20have%20achieved%20very%20impressive%20success%20in%20the%20field%20of%20automatic%20ECG%0Aanalysis%2C%20they%20often%20lack%20model%20interpretability%20that%20is%20significantly%0Aimportant%20in%20the%20healthcare%20applications.%20To%20this%20end%2C%20many%20schemes%20such%20as%0Ageneral-purpose%20attention%20mechanism%2C%20Grad-CAM%20technique%20and%20ECG%20knowledge%20graph%0Awere%20proposed%20to%20be%20integrated%20with%20deep%20learning%20models.%20However%2C%20they%20either%0Aresult%20in%20decreased%20classification%20performance%20or%20do%20not%20consist%20with%20the%20one%0Ain%20cardiologists%27%20mind%20when%20interpreting%20ECG.%20In%20this%20study%2C%20we%20propose%20a%20novel%0Adisease-specific%20attention-based%20deep%20learning%20model%20%28DANet%29%20for%20arrhythmia%0Adetection%20from%20short%20ECG%20recordings.%20The%20novel%20idea%20is%20to%20introduce%20a%0Asoft-coding%20or%20hard-coding%20waveform%20enhanced%20module%20into%20existing%20deep%20neural%0Anetworks%2C%20which%20amends%20original%20ECG%20signals%20with%20the%20guidance%20of%20the%20rule%20for%0Adiagnosis%20of%20a%20given%20disease%20type%20before%20being%20fed%20into%20the%20classification%0Amodule.%20For%20the%20soft-coding%20DANet%2C%20we%20also%20develop%20a%20learning%20framework%0Acombining%20self-supervised%20pre-training%20with%20two-stage%20supervised%20training.%20To%0Averify%20the%20effectiveness%20of%20our%20proposed%20DANet%2C%20we%20applied%20it%20to%20the%20problem%20of%0Aatrial%20premature%20contraction%20detection%20and%20the%20experimental%20results%20shows%20that%0Ait%20demonstrates%20superior%20performance%20compared%20to%20the%20benchmark%20model.%20Moreover%2C%0Ait%20also%20provides%20the%20waveform%20regions%20that%20deserve%20special%20attention%20in%20the%0Amodel%27s%20decision-making%20process%2C%20allowing%20it%20to%20be%20a%20medical%20diagnostic%0Aassistant%20for%20physicians.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18033v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DECG%2520Arrhythmia%2520Detection%2520Using%2520Disease-specific%2520Attention-based%2520Deep%250A%2520%2520Learning%2520Model%26entry.906535625%3DLinpeng%2520Jin%26entry.1292438233%3D%2520%2520The%2520electrocardiogram%2520%2528ECG%2529%2520is%2520one%2520of%2520the%2520most%2520commonly-used%2520tools%2520to%250Adiagnose%2520cardiovascular%2520disease%2520in%2520clinical%2520practice.%2520Although%2520deep%2520learning%250Amodels%2520have%2520achieved%2520very%2520impressive%2520success%2520in%2520the%2520field%2520of%2520automatic%2520ECG%250Aanalysis%252C%2520they%2520often%2520lack%2520model%2520interpretability%2520that%2520is%2520significantly%250Aimportant%2520in%2520the%2520healthcare%2520applications.%2520To%2520this%2520end%252C%2520many%2520schemes%2520such%2520as%250Ageneral-purpose%2520attention%2520mechanism%252C%2520Grad-CAM%2520technique%2520and%2520ECG%2520knowledge%2520graph%250Awere%2520proposed%2520to%2520be%2520integrated%2520with%2520deep%2520learning%2520models.%2520However%252C%2520they%2520either%250Aresult%2520in%2520decreased%2520classification%2520performance%2520or%2520do%2520not%2520consist%2520with%2520the%2520one%250Ain%2520cardiologists%2527%2520mind%2520when%2520interpreting%2520ECG.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%250Adisease-specific%2520attention-based%2520deep%2520learning%2520model%2520%2528DANet%2529%2520for%2520arrhythmia%250Adetection%2520from%2520short%2520ECG%2520recordings.%2520The%2520novel%2520idea%2520is%2520to%2520introduce%2520a%250Asoft-coding%2520or%2520hard-coding%2520waveform%2520enhanced%2520module%2520into%2520existing%2520deep%2520neural%250Anetworks%252C%2520which%2520amends%2520original%2520ECG%2520signals%2520with%2520the%2520guidance%2520of%2520the%2520rule%2520for%250Adiagnosis%2520of%2520a%2520given%2520disease%2520type%2520before%2520being%2520fed%2520into%2520the%2520classification%250Amodule.%2520For%2520the%2520soft-coding%2520DANet%252C%2520we%2520also%2520develop%2520a%2520learning%2520framework%250Acombining%2520self-supervised%2520pre-training%2520with%2520two-stage%2520supervised%2520training.%2520To%250Averify%2520the%2520effectiveness%2520of%2520our%2520proposed%2520DANet%252C%2520we%2520applied%2520it%2520to%2520the%2520problem%2520of%250Aatrial%2520premature%2520contraction%2520detection%2520and%2520the%2520experimental%2520results%2520shows%2520that%250Ait%2520demonstrates%2520superior%2520performance%2520compared%2520to%2520the%2520benchmark%2520model.%2520Moreover%252C%250Ait%2520also%2520provides%2520the%2520waveform%2520regions%2520that%2520deserve%2520special%2520attention%2520in%2520the%250Amodel%2527s%2520decision-making%2520process%252C%2520allowing%2520it%2520to%2520be%2520a%2520medical%2520diagnostic%250Aassistant%2520for%2520physicians.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18033v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECG%20Arrhythmia%20Detection%20Using%20Disease-specific%20Attention-based%20Deep%0A%20%20Learning%20Model&entry.906535625=Linpeng%20Jin&entry.1292438233=%20%20The%20electrocardiogram%20%28ECG%29%20is%20one%20of%20the%20most%20commonly-used%20tools%20to%0Adiagnose%20cardiovascular%20disease%20in%20clinical%20practice.%20Although%20deep%20learning%0Amodels%20have%20achieved%20very%20impressive%20success%20in%20the%20field%20of%20automatic%20ECG%0Aanalysis%2C%20they%20often%20lack%20model%20interpretability%20that%20is%20significantly%0Aimportant%20in%20the%20healthcare%20applications.%20To%20this%20end%2C%20many%20schemes%20such%20as%0Ageneral-purpose%20attention%20mechanism%2C%20Grad-CAM%20technique%20and%20ECG%20knowledge%20graph%0Awere%20proposed%20to%20be%20integrated%20with%20deep%20learning%20models.%20However%2C%20they%20either%0Aresult%20in%20decreased%20classification%20performance%20or%20do%20not%20consist%20with%20the%20one%0Ain%20cardiologists%27%20mind%20when%20interpreting%20ECG.%20In%20this%20study%2C%20we%20propose%20a%20novel%0Adisease-specific%20attention-based%20deep%20learning%20model%20%28DANet%29%20for%20arrhythmia%0Adetection%20from%20short%20ECG%20recordings.%20The%20novel%20idea%20is%20to%20introduce%20a%0Asoft-coding%20or%20hard-coding%20waveform%20enhanced%20module%20into%20existing%20deep%20neural%0Anetworks%2C%20which%20amends%20original%20ECG%20signals%20with%20the%20guidance%20of%20the%20rule%20for%0Adiagnosis%20of%20a%20given%20disease%20type%20before%20being%20fed%20into%20the%20classification%0Amodule.%20For%20the%20soft-coding%20DANet%2C%20we%20also%20develop%20a%20learning%20framework%0Acombining%20self-supervised%20pre-training%20with%20two-stage%20supervised%20training.%20To%0Averify%20the%20effectiveness%20of%20our%20proposed%20DANet%2C%20we%20applied%20it%20to%20the%20problem%20of%0Aatrial%20premature%20contraction%20detection%20and%20the%20experimental%20results%20shows%20that%0Ait%20demonstrates%20superior%20performance%20compared%20to%20the%20benchmark%20model.%20Moreover%2C%0Ait%20also%20provides%20the%20waveform%20regions%20that%20deserve%20special%20attention%20in%20the%0Amodel%27s%20decision-making%20process%2C%20allowing%20it%20to%20be%20a%20medical%20diagnostic%0Aassistant%20for%20physicians.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18033v1&entry.124074799=Read"},
{"title": "Gene Regulatory Network Inference from Pre-trained Single-Cell\n  Transcriptomics Transformer with Joint Graph Learning", "author": "Sindhura Kommu and Yizhi Wang and Yue Wang and Xuan Wang", "abstract": "  Inferring gene regulatory networks (GRNs) from single-cell RNA sequencing\n(scRNA-seq) data is a complex challenge that requires capturing the intricate\nrelationships between genes and their regulatory interactions. In this study,\nwe tackle this challenge by leveraging the single-cell BERT-based pre-trained\ntransformer model (scBERT), trained on extensive unlabeled scRNA-seq data, to\naugment structured biological knowledge from existing GRNs. We introduce a\nnovel joint graph learning approach that combines the rich contextual\nrepresentations learned by pre-trained single-cell language models with the\nstructured knowledge encoded in GRNs using graph neural networks (GNNs). By\nintegrating these two modalities, our approach effectively reasons over boththe\ngene expression level constraints provided by the scRNA-seq data and the\nstructured biological knowledge inherent in GRNs. We evaluate our method on\nhuman cell benchmark datasets from the BEELINE study with cell type-specific\nground truth networks. The results demonstrate superior performance over\ncurrent state-of-the-art baselines, offering a deeper understanding of cellular\nregulatory mechanisms.\n", "link": "http://arxiv.org/abs/2407.18181v1", "date": "2024-07-25", "relevancy": 1.9415, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4982}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4848}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gene%20Regulatory%20Network%20Inference%20from%20Pre-trained%20Single-Cell%0A%20%20Transcriptomics%20Transformer%20with%20Joint%20Graph%20Learning&body=Title%3A%20Gene%20Regulatory%20Network%20Inference%20from%20Pre-trained%20Single-Cell%0A%20%20Transcriptomics%20Transformer%20with%20Joint%20Graph%20Learning%0AAuthor%3A%20Sindhura%20Kommu%20and%20Yizhi%20Wang%20and%20Yue%20Wang%20and%20Xuan%20Wang%0AAbstract%3A%20%20%20Inferring%20gene%20regulatory%20networks%20%28GRNs%29%20from%20single-cell%20RNA%20sequencing%0A%28scRNA-seq%29%20data%20is%20a%20complex%20challenge%20that%20requires%20capturing%20the%20intricate%0Arelationships%20between%20genes%20and%20their%20regulatory%20interactions.%20In%20this%20study%2C%0Awe%20tackle%20this%20challenge%20by%20leveraging%20the%20single-cell%20BERT-based%20pre-trained%0Atransformer%20model%20%28scBERT%29%2C%20trained%20on%20extensive%20unlabeled%20scRNA-seq%20data%2C%20to%0Aaugment%20structured%20biological%20knowledge%20from%20existing%20GRNs.%20We%20introduce%20a%0Anovel%20joint%20graph%20learning%20approach%20that%20combines%20the%20rich%20contextual%0Arepresentations%20learned%20by%20pre-trained%20single-cell%20language%20models%20with%20the%0Astructured%20knowledge%20encoded%20in%20GRNs%20using%20graph%20neural%20networks%20%28GNNs%29.%20By%0Aintegrating%20these%20two%20modalities%2C%20our%20approach%20effectively%20reasons%20over%20boththe%0Agene%20expression%20level%20constraints%20provided%20by%20the%20scRNA-seq%20data%20and%20the%0Astructured%20biological%20knowledge%20inherent%20in%20GRNs.%20We%20evaluate%20our%20method%20on%0Ahuman%20cell%20benchmark%20datasets%20from%20the%20BEELINE%20study%20with%20cell%20type-specific%0Aground%20truth%20networks.%20The%20results%20demonstrate%20superior%20performance%20over%0Acurrent%20state-of-the-art%20baselines%2C%20offering%20a%20deeper%20understanding%20of%20cellular%0Aregulatory%20mechanisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18181v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGene%2520Regulatory%2520Network%2520Inference%2520from%2520Pre-trained%2520Single-Cell%250A%2520%2520Transcriptomics%2520Transformer%2520with%2520Joint%2520Graph%2520Learning%26entry.906535625%3DSindhura%2520Kommu%2520and%2520Yizhi%2520Wang%2520and%2520Yue%2520Wang%2520and%2520Xuan%2520Wang%26entry.1292438233%3D%2520%2520Inferring%2520gene%2520regulatory%2520networks%2520%2528GRNs%2529%2520from%2520single-cell%2520RNA%2520sequencing%250A%2528scRNA-seq%2529%2520data%2520is%2520a%2520complex%2520challenge%2520that%2520requires%2520capturing%2520the%2520intricate%250Arelationships%2520between%2520genes%2520and%2520their%2520regulatory%2520interactions.%2520In%2520this%2520study%252C%250Awe%2520tackle%2520this%2520challenge%2520by%2520leveraging%2520the%2520single-cell%2520BERT-based%2520pre-trained%250Atransformer%2520model%2520%2528scBERT%2529%252C%2520trained%2520on%2520extensive%2520unlabeled%2520scRNA-seq%2520data%252C%2520to%250Aaugment%2520structured%2520biological%2520knowledge%2520from%2520existing%2520GRNs.%2520We%2520introduce%2520a%250Anovel%2520joint%2520graph%2520learning%2520approach%2520that%2520combines%2520the%2520rich%2520contextual%250Arepresentations%2520learned%2520by%2520pre-trained%2520single-cell%2520language%2520models%2520with%2520the%250Astructured%2520knowledge%2520encoded%2520in%2520GRNs%2520using%2520graph%2520neural%2520networks%2520%2528GNNs%2529.%2520By%250Aintegrating%2520these%2520two%2520modalities%252C%2520our%2520approach%2520effectively%2520reasons%2520over%2520boththe%250Agene%2520expression%2520level%2520constraints%2520provided%2520by%2520the%2520scRNA-seq%2520data%2520and%2520the%250Astructured%2520biological%2520knowledge%2520inherent%2520in%2520GRNs.%2520We%2520evaluate%2520our%2520method%2520on%250Ahuman%2520cell%2520benchmark%2520datasets%2520from%2520the%2520BEELINE%2520study%2520with%2520cell%2520type-specific%250Aground%2520truth%2520networks.%2520The%2520results%2520demonstrate%2520superior%2520performance%2520over%250Acurrent%2520state-of-the-art%2520baselines%252C%2520offering%2520a%2520deeper%2520understanding%2520of%2520cellular%250Aregulatory%2520mechanisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18181v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gene%20Regulatory%20Network%20Inference%20from%20Pre-trained%20Single-Cell%0A%20%20Transcriptomics%20Transformer%20with%20Joint%20Graph%20Learning&entry.906535625=Sindhura%20Kommu%20and%20Yizhi%20Wang%20and%20Yue%20Wang%20and%20Xuan%20Wang&entry.1292438233=%20%20Inferring%20gene%20regulatory%20networks%20%28GRNs%29%20from%20single-cell%20RNA%20sequencing%0A%28scRNA-seq%29%20data%20is%20a%20complex%20challenge%20that%20requires%20capturing%20the%20intricate%0Arelationships%20between%20genes%20and%20their%20regulatory%20interactions.%20In%20this%20study%2C%0Awe%20tackle%20this%20challenge%20by%20leveraging%20the%20single-cell%20BERT-based%20pre-trained%0Atransformer%20model%20%28scBERT%29%2C%20trained%20on%20extensive%20unlabeled%20scRNA-seq%20data%2C%20to%0Aaugment%20structured%20biological%20knowledge%20from%20existing%20GRNs.%20We%20introduce%20a%0Anovel%20joint%20graph%20learning%20approach%20that%20combines%20the%20rich%20contextual%0Arepresentations%20learned%20by%20pre-trained%20single-cell%20language%20models%20with%20the%0Astructured%20knowledge%20encoded%20in%20GRNs%20using%20graph%20neural%20networks%20%28GNNs%29.%20By%0Aintegrating%20these%20two%20modalities%2C%20our%20approach%20effectively%20reasons%20over%20boththe%0Agene%20expression%20level%20constraints%20provided%20by%20the%20scRNA-seq%20data%20and%20the%0Astructured%20biological%20knowledge%20inherent%20in%20GRNs.%20We%20evaluate%20our%20method%20on%0Ahuman%20cell%20benchmark%20datasets%20from%20the%20BEELINE%20study%20with%20cell%20type-specific%0Aground%20truth%20networks.%20The%20results%20demonstrate%20superior%20performance%20over%0Acurrent%20state-of-the-art%20baselines%2C%20offering%20a%20deeper%20understanding%20of%20cellular%0Aregulatory%20mechanisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18181v1&entry.124074799=Read"},
{"title": "PEFT-U: Parameter-Efficient Fine-Tuning for User Personalization", "author": "Christopher Clarke and Yuzhao Heng and Lingjia Tang and Jason Mars", "abstract": "  The recent emergence of Large Language Models (LLMs) has heralded a new era\nof human-AI interaction. These sophisticated models, exemplified by Chat-GPT\nand its successors, have exhibited remarkable capabilities in language\nunderstanding. However, as these LLMs have undergone exponential growth, a\ncrucial dimension that remains understudied is the personalization of these\nmodels. Large foundation models such as GPT-3 etc. focus on creating a\nuniversal model that serves a broad range of tasks and users. This approach\nemphasizes the model's generalization capabilities, treating users as a\ncollective rather than as distinct individuals. While practical for many common\napplications, this one-size-fits-all approach often fails to address the rich\ntapestry of human diversity and individual needs. To explore this issue we\nintroduce the PEFT-U Benchmark: a new dataset for building and evaluating NLP\nmodels for user personalization. \\datasetname{} consists of a series of\nuser-centered tasks containing diverse and individualized expressions where the\npreferences of users can potentially differ for the same input. Using PEFT-U,\nwe explore the challenge of efficiently personalizing LLMs to accommodate\nuser-specific preferences in the context of diverse user-centered tasks.\n", "link": "http://arxiv.org/abs/2407.18078v1", "date": "2024-07-25", "relevancy": 1.9282, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5262}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4756}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PEFT-U%3A%20Parameter-Efficient%20Fine-Tuning%20for%20User%20Personalization&body=Title%3A%20PEFT-U%3A%20Parameter-Efficient%20Fine-Tuning%20for%20User%20Personalization%0AAuthor%3A%20Christopher%20Clarke%20and%20Yuzhao%20Heng%20and%20Lingjia%20Tang%20and%20Jason%20Mars%0AAbstract%3A%20%20%20The%20recent%20emergence%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20heralded%20a%20new%20era%0Aof%20human-AI%20interaction.%20These%20sophisticated%20models%2C%20exemplified%20by%20Chat-GPT%0Aand%20its%20successors%2C%20have%20exhibited%20remarkable%20capabilities%20in%20language%0Aunderstanding.%20However%2C%20as%20these%20LLMs%20have%20undergone%20exponential%20growth%2C%20a%0Acrucial%20dimension%20that%20remains%20understudied%20is%20the%20personalization%20of%20these%0Amodels.%20Large%20foundation%20models%20such%20as%20GPT-3%20etc.%20focus%20on%20creating%20a%0Auniversal%20model%20that%20serves%20a%20broad%20range%20of%20tasks%20and%20users.%20This%20approach%0Aemphasizes%20the%20model%27s%20generalization%20capabilities%2C%20treating%20users%20as%20a%0Acollective%20rather%20than%20as%20distinct%20individuals.%20While%20practical%20for%20many%20common%0Aapplications%2C%20this%20one-size-fits-all%20approach%20often%20fails%20to%20address%20the%20rich%0Atapestry%20of%20human%20diversity%20and%20individual%20needs.%20To%20explore%20this%20issue%20we%0Aintroduce%20the%20PEFT-U%20Benchmark%3A%20a%20new%20dataset%20for%20building%20and%20evaluating%20NLP%0Amodels%20for%20user%20personalization.%20%5Cdatasetname%7B%7D%20consists%20of%20a%20series%20of%0Auser-centered%20tasks%20containing%20diverse%20and%20individualized%20expressions%20where%20the%0Apreferences%20of%20users%20can%20potentially%20differ%20for%20the%20same%20input.%20Using%20PEFT-U%2C%0Awe%20explore%20the%20challenge%20of%20efficiently%20personalizing%20LLMs%20to%20accommodate%0Auser-specific%20preferences%20in%20the%20context%20of%20diverse%20user-centered%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPEFT-U%253A%2520Parameter-Efficient%2520Fine-Tuning%2520for%2520User%2520Personalization%26entry.906535625%3DChristopher%2520Clarke%2520and%2520Yuzhao%2520Heng%2520and%2520Lingjia%2520Tang%2520and%2520Jason%2520Mars%26entry.1292438233%3D%2520%2520The%2520recent%2520emergence%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520heralded%2520a%2520new%2520era%250Aof%2520human-AI%2520interaction.%2520These%2520sophisticated%2520models%252C%2520exemplified%2520by%2520Chat-GPT%250Aand%2520its%2520successors%252C%2520have%2520exhibited%2520remarkable%2520capabilities%2520in%2520language%250Aunderstanding.%2520However%252C%2520as%2520these%2520LLMs%2520have%2520undergone%2520exponential%2520growth%252C%2520a%250Acrucial%2520dimension%2520that%2520remains%2520understudied%2520is%2520the%2520personalization%2520of%2520these%250Amodels.%2520Large%2520foundation%2520models%2520such%2520as%2520GPT-3%2520etc.%2520focus%2520on%2520creating%2520a%250Auniversal%2520model%2520that%2520serves%2520a%2520broad%2520range%2520of%2520tasks%2520and%2520users.%2520This%2520approach%250Aemphasizes%2520the%2520model%2527s%2520generalization%2520capabilities%252C%2520treating%2520users%2520as%2520a%250Acollective%2520rather%2520than%2520as%2520distinct%2520individuals.%2520While%2520practical%2520for%2520many%2520common%250Aapplications%252C%2520this%2520one-size-fits-all%2520approach%2520often%2520fails%2520to%2520address%2520the%2520rich%250Atapestry%2520of%2520human%2520diversity%2520and%2520individual%2520needs.%2520To%2520explore%2520this%2520issue%2520we%250Aintroduce%2520the%2520PEFT-U%2520Benchmark%253A%2520a%2520new%2520dataset%2520for%2520building%2520and%2520evaluating%2520NLP%250Amodels%2520for%2520user%2520personalization.%2520%255Cdatasetname%257B%257D%2520consists%2520of%2520a%2520series%2520of%250Auser-centered%2520tasks%2520containing%2520diverse%2520and%2520individualized%2520expressions%2520where%2520the%250Apreferences%2520of%2520users%2520can%2520potentially%2520differ%2520for%2520the%2520same%2520input.%2520Using%2520PEFT-U%252C%250Awe%2520explore%2520the%2520challenge%2520of%2520efficiently%2520personalizing%2520LLMs%2520to%2520accommodate%250Auser-specific%2520preferences%2520in%2520the%2520context%2520of%2520diverse%2520user-centered%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PEFT-U%3A%20Parameter-Efficient%20Fine-Tuning%20for%20User%20Personalization&entry.906535625=Christopher%20Clarke%20and%20Yuzhao%20Heng%20and%20Lingjia%20Tang%20and%20Jason%20Mars&entry.1292438233=%20%20The%20recent%20emergence%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20heralded%20a%20new%20era%0Aof%20human-AI%20interaction.%20These%20sophisticated%20models%2C%20exemplified%20by%20Chat-GPT%0Aand%20its%20successors%2C%20have%20exhibited%20remarkable%20capabilities%20in%20language%0Aunderstanding.%20However%2C%20as%20these%20LLMs%20have%20undergone%20exponential%20growth%2C%20a%0Acrucial%20dimension%20that%20remains%20understudied%20is%20the%20personalization%20of%20these%0Amodels.%20Large%20foundation%20models%20such%20as%20GPT-3%20etc.%20focus%20on%20creating%20a%0Auniversal%20model%20that%20serves%20a%20broad%20range%20of%20tasks%20and%20users.%20This%20approach%0Aemphasizes%20the%20model%27s%20generalization%20capabilities%2C%20treating%20users%20as%20a%0Acollective%20rather%20than%20as%20distinct%20individuals.%20While%20practical%20for%20many%20common%0Aapplications%2C%20this%20one-size-fits-all%20approach%20often%20fails%20to%20address%20the%20rich%0Atapestry%20of%20human%20diversity%20and%20individual%20needs.%20To%20explore%20this%20issue%20we%0Aintroduce%20the%20PEFT-U%20Benchmark%3A%20a%20new%20dataset%20for%20building%20and%20evaluating%20NLP%0Amodels%20for%20user%20personalization.%20%5Cdatasetname%7B%7D%20consists%20of%20a%20series%20of%0Auser-centered%20tasks%20containing%20diverse%20and%20individualized%20expressions%20where%20the%0Apreferences%20of%20users%20can%20potentially%20differ%20for%20the%20same%20input.%20Using%20PEFT-U%2C%0Awe%20explore%20the%20challenge%20of%20efficiently%20personalizing%20LLMs%20to%20accommodate%0Auser-specific%20preferences%20in%20the%20context%20of%20diverse%20user-centered%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18078v1&entry.124074799=Read"},
{"title": "Clustering with minimum spanning trees: How good can it be?", "author": "Marek Gagolewski and Anna Cena and Maciej Bartoszuk and \u0141ukasz Brzozowski", "abstract": "  Minimum spanning trees (MSTs) provide a convenient representation of datasets\nin numerous pattern recognition activities. Moreover, they are relatively fast\nto compute. In this paper, we quantify the extent to which they are meaningful\nin low-dimensional partitional data clustering tasks. By identifying the upper\nbounds for the agreement between the best (oracle) algorithm and the expert\nlabels from a large battery of benchmark data, we discover that MST methods can\nbe very competitive. Next, we review, study, extend, and generalise a few\nexisting, state-of-the-art MST-based partitioning schemes. This leads to some\nnew noteworthy approaches. Overall, the Genie and the information-theoretic\nmethods often outperform the non-MST algorithms such as K-means, Gaussian\nmixtures, spectral clustering, Birch, density-based, and classical hierarchical\nagglomerative procedures. Nevertheless, we identify that there is still some\nroom for improvement, and thus the development of novel algorithms is\nencouraged.\n", "link": "http://arxiv.org/abs/2303.05679v3", "date": "2024-07-25", "relevancy": 1.9253, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4081}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3782}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Clustering%20with%20minimum%20spanning%20trees%3A%20How%20good%20can%20it%20be%3F&body=Title%3A%20Clustering%20with%20minimum%20spanning%20trees%3A%20How%20good%20can%20it%20be%3F%0AAuthor%3A%20Marek%20Gagolewski%20and%20Anna%20Cena%20and%20Maciej%20Bartoszuk%20and%20%C5%81ukasz%20Brzozowski%0AAbstract%3A%20%20%20Minimum%20spanning%20trees%20%28MSTs%29%20provide%20a%20convenient%20representation%20of%20datasets%0Ain%20numerous%20pattern%20recognition%20activities.%20Moreover%2C%20they%20are%20relatively%20fast%0Ato%20compute.%20In%20this%20paper%2C%20we%20quantify%20the%20extent%20to%20which%20they%20are%20meaningful%0Ain%20low-dimensional%20partitional%20data%20clustering%20tasks.%20By%20identifying%20the%20upper%0Abounds%20for%20the%20agreement%20between%20the%20best%20%28oracle%29%20algorithm%20and%20the%20expert%0Alabels%20from%20a%20large%20battery%20of%20benchmark%20data%2C%20we%20discover%20that%20MST%20methods%20can%0Abe%20very%20competitive.%20Next%2C%20we%20review%2C%20study%2C%20extend%2C%20and%20generalise%20a%20few%0Aexisting%2C%20state-of-the-art%20MST-based%20partitioning%20schemes.%20This%20leads%20to%20some%0Anew%20noteworthy%20approaches.%20Overall%2C%20the%20Genie%20and%20the%20information-theoretic%0Amethods%20often%20outperform%20the%20non-MST%20algorithms%20such%20as%20K-means%2C%20Gaussian%0Amixtures%2C%20spectral%20clustering%2C%20Birch%2C%20density-based%2C%20and%20classical%20hierarchical%0Aagglomerative%20procedures.%20Nevertheless%2C%20we%20identify%20that%20there%20is%20still%20some%0Aroom%20for%20improvement%2C%20and%20thus%20the%20development%20of%20novel%20algorithms%20is%0Aencouraged.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.05679v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClustering%2520with%2520minimum%2520spanning%2520trees%253A%2520How%2520good%2520can%2520it%2520be%253F%26entry.906535625%3DMarek%2520Gagolewski%2520and%2520Anna%2520Cena%2520and%2520Maciej%2520Bartoszuk%2520and%2520%25C5%2581ukasz%2520Brzozowski%26entry.1292438233%3D%2520%2520Minimum%2520spanning%2520trees%2520%2528MSTs%2529%2520provide%2520a%2520convenient%2520representation%2520of%2520datasets%250Ain%2520numerous%2520pattern%2520recognition%2520activities.%2520Moreover%252C%2520they%2520are%2520relatively%2520fast%250Ato%2520compute.%2520In%2520this%2520paper%252C%2520we%2520quantify%2520the%2520extent%2520to%2520which%2520they%2520are%2520meaningful%250Ain%2520low-dimensional%2520partitional%2520data%2520clustering%2520tasks.%2520By%2520identifying%2520the%2520upper%250Abounds%2520for%2520the%2520agreement%2520between%2520the%2520best%2520%2528oracle%2529%2520algorithm%2520and%2520the%2520expert%250Alabels%2520from%2520a%2520large%2520battery%2520of%2520benchmark%2520data%252C%2520we%2520discover%2520that%2520MST%2520methods%2520can%250Abe%2520very%2520competitive.%2520Next%252C%2520we%2520review%252C%2520study%252C%2520extend%252C%2520and%2520generalise%2520a%2520few%250Aexisting%252C%2520state-of-the-art%2520MST-based%2520partitioning%2520schemes.%2520This%2520leads%2520to%2520some%250Anew%2520noteworthy%2520approaches.%2520Overall%252C%2520the%2520Genie%2520and%2520the%2520information-theoretic%250Amethods%2520often%2520outperform%2520the%2520non-MST%2520algorithms%2520such%2520as%2520K-means%252C%2520Gaussian%250Amixtures%252C%2520spectral%2520clustering%252C%2520Birch%252C%2520density-based%252C%2520and%2520classical%2520hierarchical%250Aagglomerative%2520procedures.%2520Nevertheless%252C%2520we%2520identify%2520that%2520there%2520is%2520still%2520some%250Aroom%2520for%2520improvement%252C%2520and%2520thus%2520the%2520development%2520of%2520novel%2520algorithms%2520is%250Aencouraged.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.05679v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clustering%20with%20minimum%20spanning%20trees%3A%20How%20good%20can%20it%20be%3F&entry.906535625=Marek%20Gagolewski%20and%20Anna%20Cena%20and%20Maciej%20Bartoszuk%20and%20%C5%81ukasz%20Brzozowski&entry.1292438233=%20%20Minimum%20spanning%20trees%20%28MSTs%29%20provide%20a%20convenient%20representation%20of%20datasets%0Ain%20numerous%20pattern%20recognition%20activities.%20Moreover%2C%20they%20are%20relatively%20fast%0Ato%20compute.%20In%20this%20paper%2C%20we%20quantify%20the%20extent%20to%20which%20they%20are%20meaningful%0Ain%20low-dimensional%20partitional%20data%20clustering%20tasks.%20By%20identifying%20the%20upper%0Abounds%20for%20the%20agreement%20between%20the%20best%20%28oracle%29%20algorithm%20and%20the%20expert%0Alabels%20from%20a%20large%20battery%20of%20benchmark%20data%2C%20we%20discover%20that%20MST%20methods%20can%0Abe%20very%20competitive.%20Next%2C%20we%20review%2C%20study%2C%20extend%2C%20and%20generalise%20a%20few%0Aexisting%2C%20state-of-the-art%20MST-based%20partitioning%20schemes.%20This%20leads%20to%20some%0Anew%20noteworthy%20approaches.%20Overall%2C%20the%20Genie%20and%20the%20information-theoretic%0Amethods%20often%20outperform%20the%20non-MST%20algorithms%20such%20as%20K-means%2C%20Gaussian%0Amixtures%2C%20spectral%20clustering%2C%20Birch%2C%20density-based%2C%20and%20classical%20hierarchical%0Aagglomerative%20procedures.%20Nevertheless%2C%20we%20identify%20that%20there%20is%20still%20some%0Aroom%20for%20improvement%2C%20and%20thus%20the%20development%20of%20novel%20algorithms%20is%0Aencouraged.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.05679v3&entry.124074799=Read"},
{"title": "Expressivity and Generalization: Fragment-Biases for Molecular GNNs", "author": "Tom Wollschl\u00e4ger and Niklas Kemper and Leon Hetzel and Johanna Sommer and Stephan G\u00fcnnemann", "abstract": "  Although recent advances in higher-order Graph Neural Networks (GNNs) improve\nthe theoretical expressiveness and molecular property predictive performance,\nthey often fall short of the empirical performance of models that explicitly\nuse fragment information as inductive bias. However, for these approaches,\nthere exists no theoretic expressivity study. In this work, we propose the\nFragment-WL test, an extension to the well-known Weisfeiler & Leman (WL) test,\nwhich enables the theoretic analysis of these fragment-biased GNNs. Building on\nthe insights gained from the Fragment-WL test, we develop a new GNN\narchitecture and a fragmentation with infinite vocabulary that significantly\nboosts expressiveness. We show the effectiveness of our model on synthetic and\nreal-world data where we outperform all GNNs on Peptides and have 12% lower\nerror than all GNNs on ZINC and 34% lower error than other fragment-biased\nmodels. Furthermore, we show that our model exhibits superior generalization\ncapabilities compared to the latest transformer-based architectures,\npositioning it as a robust solution for a range of molecular modeling tasks.\n", "link": "http://arxiv.org/abs/2406.08210v2", "date": "2024-07-25", "relevancy": 1.9173, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4892}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4792}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expressivity%20and%20Generalization%3A%20Fragment-Biases%20for%20Molecular%20GNNs&body=Title%3A%20Expressivity%20and%20Generalization%3A%20Fragment-Biases%20for%20Molecular%20GNNs%0AAuthor%3A%20Tom%20Wollschl%C3%A4ger%20and%20Niklas%20Kemper%20and%20Leon%20Hetzel%20and%20Johanna%20Sommer%20and%20Stephan%20G%C3%BCnnemann%0AAbstract%3A%20%20%20Although%20recent%20advances%20in%20higher-order%20Graph%20Neural%20Networks%20%28GNNs%29%20improve%0Athe%20theoretical%20expressiveness%20and%20molecular%20property%20predictive%20performance%2C%0Athey%20often%20fall%20short%20of%20the%20empirical%20performance%20of%20models%20that%20explicitly%0Ause%20fragment%20information%20as%20inductive%20bias.%20However%2C%20for%20these%20approaches%2C%0Athere%20exists%20no%20theoretic%20expressivity%20study.%20In%20this%20work%2C%20we%20propose%20the%0AFragment-WL%20test%2C%20an%20extension%20to%20the%20well-known%20Weisfeiler%20%26%20Leman%20%28WL%29%20test%2C%0Awhich%20enables%20the%20theoretic%20analysis%20of%20these%20fragment-biased%20GNNs.%20Building%20on%0Athe%20insights%20gained%20from%20the%20Fragment-WL%20test%2C%20we%20develop%20a%20new%20GNN%0Aarchitecture%20and%20a%20fragmentation%20with%20infinite%20vocabulary%20that%20significantly%0Aboosts%20expressiveness.%20We%20show%20the%20effectiveness%20of%20our%20model%20on%20synthetic%20and%0Areal-world%20data%20where%20we%20outperform%20all%20GNNs%20on%20Peptides%20and%20have%2012%25%20lower%0Aerror%20than%20all%20GNNs%20on%20ZINC%20and%2034%25%20lower%20error%20than%20other%20fragment-biased%0Amodels.%20Furthermore%2C%20we%20show%20that%20our%20model%20exhibits%20superior%20generalization%0Acapabilities%20compared%20to%20the%20latest%20transformer-based%20architectures%2C%0Apositioning%20it%20as%20a%20robust%20solution%20for%20a%20range%20of%20molecular%20modeling%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08210v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpressivity%2520and%2520Generalization%253A%2520Fragment-Biases%2520for%2520Molecular%2520GNNs%26entry.906535625%3DTom%2520Wollschl%25C3%25A4ger%2520and%2520Niklas%2520Kemper%2520and%2520Leon%2520Hetzel%2520and%2520Johanna%2520Sommer%2520and%2520Stephan%2520G%25C3%25BCnnemann%26entry.1292438233%3D%2520%2520Although%2520recent%2520advances%2520in%2520higher-order%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520improve%250Athe%2520theoretical%2520expressiveness%2520and%2520molecular%2520property%2520predictive%2520performance%252C%250Athey%2520often%2520fall%2520short%2520of%2520the%2520empirical%2520performance%2520of%2520models%2520that%2520explicitly%250Ause%2520fragment%2520information%2520as%2520inductive%2520bias.%2520However%252C%2520for%2520these%2520approaches%252C%250Athere%2520exists%2520no%2520theoretic%2520expressivity%2520study.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%250AFragment-WL%2520test%252C%2520an%2520extension%2520to%2520the%2520well-known%2520Weisfeiler%2520%2526%2520Leman%2520%2528WL%2529%2520test%252C%250Awhich%2520enables%2520the%2520theoretic%2520analysis%2520of%2520these%2520fragment-biased%2520GNNs.%2520Building%2520on%250Athe%2520insights%2520gained%2520from%2520the%2520Fragment-WL%2520test%252C%2520we%2520develop%2520a%2520new%2520GNN%250Aarchitecture%2520and%2520a%2520fragmentation%2520with%2520infinite%2520vocabulary%2520that%2520significantly%250Aboosts%2520expressiveness.%2520We%2520show%2520the%2520effectiveness%2520of%2520our%2520model%2520on%2520synthetic%2520and%250Areal-world%2520data%2520where%2520we%2520outperform%2520all%2520GNNs%2520on%2520Peptides%2520and%2520have%252012%2525%2520lower%250Aerror%2520than%2520all%2520GNNs%2520on%2520ZINC%2520and%252034%2525%2520lower%2520error%2520than%2520other%2520fragment-biased%250Amodels.%2520Furthermore%252C%2520we%2520show%2520that%2520our%2520model%2520exhibits%2520superior%2520generalization%250Acapabilities%2520compared%2520to%2520the%2520latest%2520transformer-based%2520architectures%252C%250Apositioning%2520it%2520as%2520a%2520robust%2520solution%2520for%2520a%2520range%2520of%2520molecular%2520modeling%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08210v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expressivity%20and%20Generalization%3A%20Fragment-Biases%20for%20Molecular%20GNNs&entry.906535625=Tom%20Wollschl%C3%A4ger%20and%20Niklas%20Kemper%20and%20Leon%20Hetzel%20and%20Johanna%20Sommer%20and%20Stephan%20G%C3%BCnnemann&entry.1292438233=%20%20Although%20recent%20advances%20in%20higher-order%20Graph%20Neural%20Networks%20%28GNNs%29%20improve%0Athe%20theoretical%20expressiveness%20and%20molecular%20property%20predictive%20performance%2C%0Athey%20often%20fall%20short%20of%20the%20empirical%20performance%20of%20models%20that%20explicitly%0Ause%20fragment%20information%20as%20inductive%20bias.%20However%2C%20for%20these%20approaches%2C%0Athere%20exists%20no%20theoretic%20expressivity%20study.%20In%20this%20work%2C%20we%20propose%20the%0AFragment-WL%20test%2C%20an%20extension%20to%20the%20well-known%20Weisfeiler%20%26%20Leman%20%28WL%29%20test%2C%0Awhich%20enables%20the%20theoretic%20analysis%20of%20these%20fragment-biased%20GNNs.%20Building%20on%0Athe%20insights%20gained%20from%20the%20Fragment-WL%20test%2C%20we%20develop%20a%20new%20GNN%0Aarchitecture%20and%20a%20fragmentation%20with%20infinite%20vocabulary%20that%20significantly%0Aboosts%20expressiveness.%20We%20show%20the%20effectiveness%20of%20our%20model%20on%20synthetic%20and%0Areal-world%20data%20where%20we%20outperform%20all%20GNNs%20on%20Peptides%20and%20have%2012%25%20lower%0Aerror%20than%20all%20GNNs%20on%20ZINC%20and%2034%25%20lower%20error%20than%20other%20fragment-biased%0Amodels.%20Furthermore%2C%20we%20show%20that%20our%20model%20exhibits%20superior%20generalization%0Acapabilities%20compared%20to%20the%20latest%20transformer-based%20architectures%2C%0Apositioning%20it%20as%20a%20robust%20solution%20for%20a%20range%20of%20molecular%20modeling%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08210v2&entry.124074799=Read"},
{"title": "Amortized Active Learning for Nonparametric Functions", "author": "Cen-You Li and Marc Toussaint and Barbara Rakitsch and Christoph Zimmer", "abstract": "  Active learning (AL) is a sequential learning scheme aiming to select the\nmost informative data. AL reduces data consumption and avoids the cost of\nlabeling large amounts of data. However, AL trains the model and solves an\nacquisition optimization for each selection. It becomes expensive when the\nmodel training or acquisition optimization is challenging. In this paper, we\nfocus on active nonparametric function learning, where the gold standard\nGaussian process (GP) approaches suffer from cubic time complexity. We propose\nan amortized AL method, where new data are suggested by a neural network which\nis trained up-front without any real data (Figure 1). Our method avoids\nrepeated model training and requires no acquisition optimization during the AL\ndeployment. We (i) utilize GPs as function priors to construct an AL simulator,\n(ii) train an AL policy that can zero-shot generalize from simulation to real\nlearning problems of nonparametric functions and (iii) achieve real-time data\nselection and comparable learning performances to time-consuming baseline\nmethods.\n", "link": "http://arxiv.org/abs/2407.17992v1", "date": "2024-07-25", "relevancy": 1.909, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4806}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4772}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Amortized%20Active%20Learning%20for%20Nonparametric%20Functions&body=Title%3A%20Amortized%20Active%20Learning%20for%20Nonparametric%20Functions%0AAuthor%3A%20Cen-You%20Li%20and%20Marc%20Toussaint%20and%20Barbara%20Rakitsch%20and%20Christoph%20Zimmer%0AAbstract%3A%20%20%20Active%20learning%20%28AL%29%20is%20a%20sequential%20learning%20scheme%20aiming%20to%20select%20the%0Amost%20informative%20data.%20AL%20reduces%20data%20consumption%20and%20avoids%20the%20cost%20of%0Alabeling%20large%20amounts%20of%20data.%20However%2C%20AL%20trains%20the%20model%20and%20solves%20an%0Aacquisition%20optimization%20for%20each%20selection.%20It%20becomes%20expensive%20when%20the%0Amodel%20training%20or%20acquisition%20optimization%20is%20challenging.%20In%20this%20paper%2C%20we%0Afocus%20on%20active%20nonparametric%20function%20learning%2C%20where%20the%20gold%20standard%0AGaussian%20process%20%28GP%29%20approaches%20suffer%20from%20cubic%20time%20complexity.%20We%20propose%0Aan%20amortized%20AL%20method%2C%20where%20new%20data%20are%20suggested%20by%20a%20neural%20network%20which%0Ais%20trained%20up-front%20without%20any%20real%20data%20%28Figure%201%29.%20Our%20method%20avoids%0Arepeated%20model%20training%20and%20requires%20no%20acquisition%20optimization%20during%20the%20AL%0Adeployment.%20We%20%28i%29%20utilize%20GPs%20as%20function%20priors%20to%20construct%20an%20AL%20simulator%2C%0A%28ii%29%20train%20an%20AL%20policy%20that%20can%20zero-shot%20generalize%20from%20simulation%20to%20real%0Alearning%20problems%20of%20nonparametric%20functions%20and%20%28iii%29%20achieve%20real-time%20data%0Aselection%20and%20comparable%20learning%20performances%20to%20time-consuming%20baseline%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17992v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAmortized%2520Active%2520Learning%2520for%2520Nonparametric%2520Functions%26entry.906535625%3DCen-You%2520Li%2520and%2520Marc%2520Toussaint%2520and%2520Barbara%2520Rakitsch%2520and%2520Christoph%2520Zimmer%26entry.1292438233%3D%2520%2520Active%2520learning%2520%2528AL%2529%2520is%2520a%2520sequential%2520learning%2520scheme%2520aiming%2520to%2520select%2520the%250Amost%2520informative%2520data.%2520AL%2520reduces%2520data%2520consumption%2520and%2520avoids%2520the%2520cost%2520of%250Alabeling%2520large%2520amounts%2520of%2520data.%2520However%252C%2520AL%2520trains%2520the%2520model%2520and%2520solves%2520an%250Aacquisition%2520optimization%2520for%2520each%2520selection.%2520It%2520becomes%2520expensive%2520when%2520the%250Amodel%2520training%2520or%2520acquisition%2520optimization%2520is%2520challenging.%2520In%2520this%2520paper%252C%2520we%250Afocus%2520on%2520active%2520nonparametric%2520function%2520learning%252C%2520where%2520the%2520gold%2520standard%250AGaussian%2520process%2520%2528GP%2529%2520approaches%2520suffer%2520from%2520cubic%2520time%2520complexity.%2520We%2520propose%250Aan%2520amortized%2520AL%2520method%252C%2520where%2520new%2520data%2520are%2520suggested%2520by%2520a%2520neural%2520network%2520which%250Ais%2520trained%2520up-front%2520without%2520any%2520real%2520data%2520%2528Figure%25201%2529.%2520Our%2520method%2520avoids%250Arepeated%2520model%2520training%2520and%2520requires%2520no%2520acquisition%2520optimization%2520during%2520the%2520AL%250Adeployment.%2520We%2520%2528i%2529%2520utilize%2520GPs%2520as%2520function%2520priors%2520to%2520construct%2520an%2520AL%2520simulator%252C%250A%2528ii%2529%2520train%2520an%2520AL%2520policy%2520that%2520can%2520zero-shot%2520generalize%2520from%2520simulation%2520to%2520real%250Alearning%2520problems%2520of%2520nonparametric%2520functions%2520and%2520%2528iii%2529%2520achieve%2520real-time%2520data%250Aselection%2520and%2520comparable%2520learning%2520performances%2520to%2520time-consuming%2520baseline%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17992v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Amortized%20Active%20Learning%20for%20Nonparametric%20Functions&entry.906535625=Cen-You%20Li%20and%20Marc%20Toussaint%20and%20Barbara%20Rakitsch%20and%20Christoph%20Zimmer&entry.1292438233=%20%20Active%20learning%20%28AL%29%20is%20a%20sequential%20learning%20scheme%20aiming%20to%20select%20the%0Amost%20informative%20data.%20AL%20reduces%20data%20consumption%20and%20avoids%20the%20cost%20of%0Alabeling%20large%20amounts%20of%20data.%20However%2C%20AL%20trains%20the%20model%20and%20solves%20an%0Aacquisition%20optimization%20for%20each%20selection.%20It%20becomes%20expensive%20when%20the%0Amodel%20training%20or%20acquisition%20optimization%20is%20challenging.%20In%20this%20paper%2C%20we%0Afocus%20on%20active%20nonparametric%20function%20learning%2C%20where%20the%20gold%20standard%0AGaussian%20process%20%28GP%29%20approaches%20suffer%20from%20cubic%20time%20complexity.%20We%20propose%0Aan%20amortized%20AL%20method%2C%20where%20new%20data%20are%20suggested%20by%20a%20neural%20network%20which%0Ais%20trained%20up-front%20without%20any%20real%20data%20%28Figure%201%29.%20Our%20method%20avoids%0Arepeated%20model%20training%20and%20requires%20no%20acquisition%20optimization%20during%20the%20AL%0Adeployment.%20We%20%28i%29%20utilize%20GPs%20as%20function%20priors%20to%20construct%20an%20AL%20simulator%2C%0A%28ii%29%20train%20an%20AL%20policy%20that%20can%20zero-shot%20generalize%20from%20simulation%20to%20real%0Alearning%20problems%20of%20nonparametric%20functions%20and%20%28iii%29%20achieve%20real-time%20data%0Aselection%20and%20comparable%20learning%20performances%20to%20time-consuming%20baseline%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17992v1&entry.124074799=Read"},
{"title": "Uncovering Latent Memories: Assessing Data Leakage and Memorization\n  Patterns in Frontier AI Models", "author": "Sunny Duan and Mikail Khona and Abhiram Iyer and Rylan Schaeffer and Ila R Fiete", "abstract": "  Frontier AI systems are making transformative impacts across society, but\nsuch benefits are not without costs: models trained on web-scale datasets\ncontaining personal and private data raise profound concerns about data privacy\nand security. Language models are trained on extensive corpora including\npotentially sensitive or proprietary information, and the risk of data leakage\n- where the model response reveals pieces of such information - remains\ninadequately understood. Prior work has investigated what factors drive\nmemorization and have identified that sequence complexity and the number of\nrepetitions drive memorization. Here, we focus on the evolution of memorization\nover training. We begin by reproducing findings that the probability of\nmemorizing a sequence scales logarithmically with the number of times it is\npresent in the data. We next show that sequences which are apparently not\nmemorized after the first encounter can be \"uncovered\" throughout the course of\ntraining even without subsequent encounters, a phenomenon we term \"latent\nmemorization\". The presence of latent memorization presents a challenge for\ndata privacy as memorized sequences may be hidden at the final checkpoint of\nthe model but remain easily recoverable. To this end, we develop a diagnostic\ntest relying on the cross entropy loss to uncover latent memorized sequences\nwith high accuracy.\n", "link": "http://arxiv.org/abs/2406.14549v2", "date": "2024-07-25", "relevancy": 1.8837, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4718}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4704}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20Latent%20Memories%3A%20Assessing%20Data%20Leakage%20and%20Memorization%0A%20%20Patterns%20in%20Frontier%20AI%20Models&body=Title%3A%20Uncovering%20Latent%20Memories%3A%20Assessing%20Data%20Leakage%20and%20Memorization%0A%20%20Patterns%20in%20Frontier%20AI%20Models%0AAuthor%3A%20Sunny%20Duan%20and%20Mikail%20Khona%20and%20Abhiram%20Iyer%20and%20Rylan%20Schaeffer%20and%20Ila%20R%20Fiete%0AAbstract%3A%20%20%20Frontier%20AI%20systems%20are%20making%20transformative%20impacts%20across%20society%2C%20but%0Asuch%20benefits%20are%20not%20without%20costs%3A%20models%20trained%20on%20web-scale%20datasets%0Acontaining%20personal%20and%20private%20data%20raise%20profound%20concerns%20about%20data%20privacy%0Aand%20security.%20Language%20models%20are%20trained%20on%20extensive%20corpora%20including%0Apotentially%20sensitive%20or%20proprietary%20information%2C%20and%20the%20risk%20of%20data%20leakage%0A-%20where%20the%20model%20response%20reveals%20pieces%20of%20such%20information%20-%20remains%0Ainadequately%20understood.%20Prior%20work%20has%20investigated%20what%20factors%20drive%0Amemorization%20and%20have%20identified%20that%20sequence%20complexity%20and%20the%20number%20of%0Arepetitions%20drive%20memorization.%20Here%2C%20we%20focus%20on%20the%20evolution%20of%20memorization%0Aover%20training.%20We%20begin%20by%20reproducing%20findings%20that%20the%20probability%20of%0Amemorizing%20a%20sequence%20scales%20logarithmically%20with%20the%20number%20of%20times%20it%20is%0Apresent%20in%20the%20data.%20We%20next%20show%20that%20sequences%20which%20are%20apparently%20not%0Amemorized%20after%20the%20first%20encounter%20can%20be%20%22uncovered%22%20throughout%20the%20course%20of%0Atraining%20even%20without%20subsequent%20encounters%2C%20a%20phenomenon%20we%20term%20%22latent%0Amemorization%22.%20The%20presence%20of%20latent%20memorization%20presents%20a%20challenge%20for%0Adata%20privacy%20as%20memorized%20sequences%20may%20be%20hidden%20at%20the%20final%20checkpoint%20of%0Athe%20model%20but%20remain%20easily%20recoverable.%20To%20this%20end%2C%20we%20develop%20a%20diagnostic%0Atest%20relying%20on%20the%20cross%20entropy%20loss%20to%20uncover%20latent%20memorized%20sequences%0Awith%20high%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14549v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520Latent%2520Memories%253A%2520Assessing%2520Data%2520Leakage%2520and%2520Memorization%250A%2520%2520Patterns%2520in%2520Frontier%2520AI%2520Models%26entry.906535625%3DSunny%2520Duan%2520and%2520Mikail%2520Khona%2520and%2520Abhiram%2520Iyer%2520and%2520Rylan%2520Schaeffer%2520and%2520Ila%2520R%2520Fiete%26entry.1292438233%3D%2520%2520Frontier%2520AI%2520systems%2520are%2520making%2520transformative%2520impacts%2520across%2520society%252C%2520but%250Asuch%2520benefits%2520are%2520not%2520without%2520costs%253A%2520models%2520trained%2520on%2520web-scale%2520datasets%250Acontaining%2520personal%2520and%2520private%2520data%2520raise%2520profound%2520concerns%2520about%2520data%2520privacy%250Aand%2520security.%2520Language%2520models%2520are%2520trained%2520on%2520extensive%2520corpora%2520including%250Apotentially%2520sensitive%2520or%2520proprietary%2520information%252C%2520and%2520the%2520risk%2520of%2520data%2520leakage%250A-%2520where%2520the%2520model%2520response%2520reveals%2520pieces%2520of%2520such%2520information%2520-%2520remains%250Ainadequately%2520understood.%2520Prior%2520work%2520has%2520investigated%2520what%2520factors%2520drive%250Amemorization%2520and%2520have%2520identified%2520that%2520sequence%2520complexity%2520and%2520the%2520number%2520of%250Arepetitions%2520drive%2520memorization.%2520Here%252C%2520we%2520focus%2520on%2520the%2520evolution%2520of%2520memorization%250Aover%2520training.%2520We%2520begin%2520by%2520reproducing%2520findings%2520that%2520the%2520probability%2520of%250Amemorizing%2520a%2520sequence%2520scales%2520logarithmically%2520with%2520the%2520number%2520of%2520times%2520it%2520is%250Apresent%2520in%2520the%2520data.%2520We%2520next%2520show%2520that%2520sequences%2520which%2520are%2520apparently%2520not%250Amemorized%2520after%2520the%2520first%2520encounter%2520can%2520be%2520%2522uncovered%2522%2520throughout%2520the%2520course%2520of%250Atraining%2520even%2520without%2520subsequent%2520encounters%252C%2520a%2520phenomenon%2520we%2520term%2520%2522latent%250Amemorization%2522.%2520The%2520presence%2520of%2520latent%2520memorization%2520presents%2520a%2520challenge%2520for%250Adata%2520privacy%2520as%2520memorized%2520sequences%2520may%2520be%2520hidden%2520at%2520the%2520final%2520checkpoint%2520of%250Athe%2520model%2520but%2520remain%2520easily%2520recoverable.%2520To%2520this%2520end%252C%2520we%2520develop%2520a%2520diagnostic%250Atest%2520relying%2520on%2520the%2520cross%2520entropy%2520loss%2520to%2520uncover%2520latent%2520memorized%2520sequences%250Awith%2520high%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14549v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20Latent%20Memories%3A%20Assessing%20Data%20Leakage%20and%20Memorization%0A%20%20Patterns%20in%20Frontier%20AI%20Models&entry.906535625=Sunny%20Duan%20and%20Mikail%20Khona%20and%20Abhiram%20Iyer%20and%20Rylan%20Schaeffer%20and%20Ila%20R%20Fiete&entry.1292438233=%20%20Frontier%20AI%20systems%20are%20making%20transformative%20impacts%20across%20society%2C%20but%0Asuch%20benefits%20are%20not%20without%20costs%3A%20models%20trained%20on%20web-scale%20datasets%0Acontaining%20personal%20and%20private%20data%20raise%20profound%20concerns%20about%20data%20privacy%0Aand%20security.%20Language%20models%20are%20trained%20on%20extensive%20corpora%20including%0Apotentially%20sensitive%20or%20proprietary%20information%2C%20and%20the%20risk%20of%20data%20leakage%0A-%20where%20the%20model%20response%20reveals%20pieces%20of%20such%20information%20-%20remains%0Ainadequately%20understood.%20Prior%20work%20has%20investigated%20what%20factors%20drive%0Amemorization%20and%20have%20identified%20that%20sequence%20complexity%20and%20the%20number%20of%0Arepetitions%20drive%20memorization.%20Here%2C%20we%20focus%20on%20the%20evolution%20of%20memorization%0Aover%20training.%20We%20begin%20by%20reproducing%20findings%20that%20the%20probability%20of%0Amemorizing%20a%20sequence%20scales%20logarithmically%20with%20the%20number%20of%20times%20it%20is%0Apresent%20in%20the%20data.%20We%20next%20show%20that%20sequences%20which%20are%20apparently%20not%0Amemorized%20after%20the%20first%20encounter%20can%20be%20%22uncovered%22%20throughout%20the%20course%20of%0Atraining%20even%20without%20subsequent%20encounters%2C%20a%20phenomenon%20we%20term%20%22latent%0Amemorization%22.%20The%20presence%20of%20latent%20memorization%20presents%20a%20challenge%20for%0Adata%20privacy%20as%20memorized%20sequences%20may%20be%20hidden%20at%20the%20final%20checkpoint%20of%0Athe%20model%20but%20remain%20easily%20recoverable.%20To%20this%20end%2C%20we%20develop%20a%20diagnostic%0Atest%20relying%20on%20the%20cross%20entropy%20loss%20to%20uncover%20latent%20memorized%20sequences%0Awith%20high%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14549v2&entry.124074799=Read"},
{"title": "A Unified Framework for Model Editing", "author": "Akshat Gupta and Dev Sajnani and Gopala Anumanchipalli", "abstract": "  ROME and MEMIT are largely believed to be two different model editing\nalgorithms, with the major difference between them being the ability to perform\nbatched edits. In this paper, we unify these two algorithms under a single\nconceptual umbrella, optimizing for the same goal, which we call the\npreservation-memorization objective. ROME uses an equality constraint to\noptimize this objective to perform one edit at a time, whereas MEMIT employs a\nmore flexible least-square constraint that allows for batched edits. We\ngeneralize ROME and enable batched editing with equality constraint in the form\nof EMMET - an Equality-constrained Mass Model Editing algorithm for\nTransformers, a new batched memory-editing algorithm. EMMET can perform\nbatched-edits up to a batch-size of 10,000, with very similar performance to\nMEMIT across multiple dimensions. With the introduction of EMMET, we truly\nunify ROME and MEMIT and show that both algorithms are equivalent in terms of\ntheir optimization objective, their abilities (singular and batched editing),\ntheir model editing performance and their limitations.\n", "link": "http://arxiv.org/abs/2403.14236v4", "date": "2024-07-25", "relevancy": 1.8814, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4727}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4699}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Framework%20for%20Model%20Editing&body=Title%3A%20A%20Unified%20Framework%20for%20Model%20Editing%0AAuthor%3A%20Akshat%20Gupta%20and%20Dev%20Sajnani%20and%20Gopala%20Anumanchipalli%0AAbstract%3A%20%20%20ROME%20and%20MEMIT%20are%20largely%20believed%20to%20be%20two%20different%20model%20editing%0Aalgorithms%2C%20with%20the%20major%20difference%20between%20them%20being%20the%20ability%20to%20perform%0Abatched%20edits.%20In%20this%20paper%2C%20we%20unify%20these%20two%20algorithms%20under%20a%20single%0Aconceptual%20umbrella%2C%20optimizing%20for%20the%20same%20goal%2C%20which%20we%20call%20the%0Apreservation-memorization%20objective.%20ROME%20uses%20an%20equality%20constraint%20to%0Aoptimize%20this%20objective%20to%20perform%20one%20edit%20at%20a%20time%2C%20whereas%20MEMIT%20employs%20a%0Amore%20flexible%20least-square%20constraint%20that%20allows%20for%20batched%20edits.%20We%0Ageneralize%20ROME%20and%20enable%20batched%20editing%20with%20equality%20constraint%20in%20the%20form%0Aof%20EMMET%20-%20an%20Equality-constrained%20Mass%20Model%20Editing%20algorithm%20for%0ATransformers%2C%20a%20new%20batched%20memory-editing%20algorithm.%20EMMET%20can%20perform%0Abatched-edits%20up%20to%20a%20batch-size%20of%2010%2C000%2C%20with%20very%20similar%20performance%20to%0AMEMIT%20across%20multiple%20dimensions.%20With%20the%20introduction%20of%20EMMET%2C%20we%20truly%0Aunify%20ROME%20and%20MEMIT%20and%20show%20that%20both%20algorithms%20are%20equivalent%20in%20terms%20of%0Atheir%20optimization%20objective%2C%20their%20abilities%20%28singular%20and%20batched%20editing%29%2C%0Atheir%20model%20editing%20performance%20and%20their%20limitations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14236v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Framework%2520for%2520Model%2520Editing%26entry.906535625%3DAkshat%2520Gupta%2520and%2520Dev%2520Sajnani%2520and%2520Gopala%2520Anumanchipalli%26entry.1292438233%3D%2520%2520ROME%2520and%2520MEMIT%2520are%2520largely%2520believed%2520to%2520be%2520two%2520different%2520model%2520editing%250Aalgorithms%252C%2520with%2520the%2520major%2520difference%2520between%2520them%2520being%2520the%2520ability%2520to%2520perform%250Abatched%2520edits.%2520In%2520this%2520paper%252C%2520we%2520unify%2520these%2520two%2520algorithms%2520under%2520a%2520single%250Aconceptual%2520umbrella%252C%2520optimizing%2520for%2520the%2520same%2520goal%252C%2520which%2520we%2520call%2520the%250Apreservation-memorization%2520objective.%2520ROME%2520uses%2520an%2520equality%2520constraint%2520to%250Aoptimize%2520this%2520objective%2520to%2520perform%2520one%2520edit%2520at%2520a%2520time%252C%2520whereas%2520MEMIT%2520employs%2520a%250Amore%2520flexible%2520least-square%2520constraint%2520that%2520allows%2520for%2520batched%2520edits.%2520We%250Ageneralize%2520ROME%2520and%2520enable%2520batched%2520editing%2520with%2520equality%2520constraint%2520in%2520the%2520form%250Aof%2520EMMET%2520-%2520an%2520Equality-constrained%2520Mass%2520Model%2520Editing%2520algorithm%2520for%250ATransformers%252C%2520a%2520new%2520batched%2520memory-editing%2520algorithm.%2520EMMET%2520can%2520perform%250Abatched-edits%2520up%2520to%2520a%2520batch-size%2520of%252010%252C000%252C%2520with%2520very%2520similar%2520performance%2520to%250AMEMIT%2520across%2520multiple%2520dimensions.%2520With%2520the%2520introduction%2520of%2520EMMET%252C%2520we%2520truly%250Aunify%2520ROME%2520and%2520MEMIT%2520and%2520show%2520that%2520both%2520algorithms%2520are%2520equivalent%2520in%2520terms%2520of%250Atheir%2520optimization%2520objective%252C%2520their%2520abilities%2520%2528singular%2520and%2520batched%2520editing%2529%252C%250Atheir%2520model%2520editing%2520performance%2520and%2520their%2520limitations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14236v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Framework%20for%20Model%20Editing&entry.906535625=Akshat%20Gupta%20and%20Dev%20Sajnani%20and%20Gopala%20Anumanchipalli&entry.1292438233=%20%20ROME%20and%20MEMIT%20are%20largely%20believed%20to%20be%20two%20different%20model%20editing%0Aalgorithms%2C%20with%20the%20major%20difference%20between%20them%20being%20the%20ability%20to%20perform%0Abatched%20edits.%20In%20this%20paper%2C%20we%20unify%20these%20two%20algorithms%20under%20a%20single%0Aconceptual%20umbrella%2C%20optimizing%20for%20the%20same%20goal%2C%20which%20we%20call%20the%0Apreservation-memorization%20objective.%20ROME%20uses%20an%20equality%20constraint%20to%0Aoptimize%20this%20objective%20to%20perform%20one%20edit%20at%20a%20time%2C%20whereas%20MEMIT%20employs%20a%0Amore%20flexible%20least-square%20constraint%20that%20allows%20for%20batched%20edits.%20We%0Ageneralize%20ROME%20and%20enable%20batched%20editing%20with%20equality%20constraint%20in%20the%20form%0Aof%20EMMET%20-%20an%20Equality-constrained%20Mass%20Model%20Editing%20algorithm%20for%0ATransformers%2C%20a%20new%20batched%20memory-editing%20algorithm.%20EMMET%20can%20perform%0Abatched-edits%20up%20to%20a%20batch-size%20of%2010%2C000%2C%20with%20very%20similar%20performance%20to%0AMEMIT%20across%20multiple%20dimensions.%20With%20the%20introduction%20of%20EMMET%2C%20we%20truly%0Aunify%20ROME%20and%20MEMIT%20and%20show%20that%20both%20algorithms%20are%20equivalent%20in%20terms%20of%0Atheir%20optimization%20objective%2C%20their%20abilities%20%28singular%20and%20batched%20editing%29%2C%0Atheir%20model%20editing%20performance%20and%20their%20limitations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14236v4&entry.124074799=Read"},
{"title": "Machine Translation Hallucination Detection for Low and High Resource\n  Languages using Large Language Models", "author": "Kenza Benkirane and Laura Gongas and Shahar Pelles and Naomi Fuchs and Joshua Darmon and Pontus Stenetorp and David Ifeoluwa Adelani and Eduardo S\u00e1nchez", "abstract": "  Recent advancements in massively multilingual machine translation systems\nhave significantly enhanced translation accuracy; however, even the best\nperforming systems still generate hallucinations, severely impacting user\ntrust. Detecting hallucinations in Machine Translation (MT) remains a critical\nchallenge, particularly since existing methods excel with High-Resource\nLanguages (HRLs) but exhibit substantial limitations when applied to\nLow-Resource Languages (LRLs). This paper evaluates hallucination detection\napproaches using Large Language Models (LLMs) and semantic similarity within\nmassively multilingual embeddings. Our study spans 16 language directions,\ncovering HRLs, LRLs, with diverse scripts. We find that the choice of model is\nessential for performance. On average, for HRLs, Llama3-70B outperforms the\nprevious state of the art by as much as 0.16 MCC (Matthews Correlation\nCoefficient). However, for LRLs we observe that Claude Sonnet outperforms other\nLLMs on average by 0.03 MCC. The key takeaway from our study is that LLMs can\nachieve performance comparable or even better than previously proposed models,\ndespite not being explicitly trained for any machine translation task. However,\ntheir advantage is less significant for LRLs.\n", "link": "http://arxiv.org/abs/2407.16470v2", "date": "2024-07-25", "relevancy": 1.8535, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5024}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4605}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Translation%20Hallucination%20Detection%20for%20Low%20and%20High%20Resource%0A%20%20Languages%20using%20Large%20Language%20Models&body=Title%3A%20Machine%20Translation%20Hallucination%20Detection%20for%20Low%20and%20High%20Resource%0A%20%20Languages%20using%20Large%20Language%20Models%0AAuthor%3A%20Kenza%20Benkirane%20and%20Laura%20Gongas%20and%20Shahar%20Pelles%20and%20Naomi%20Fuchs%20and%20Joshua%20Darmon%20and%20Pontus%20Stenetorp%20and%20David%20Ifeoluwa%20Adelani%20and%20Eduardo%20S%C3%A1nchez%0AAbstract%3A%20%20%20Recent%20advancements%20in%20massively%20multilingual%20machine%20translation%20systems%0Ahave%20significantly%20enhanced%20translation%20accuracy%3B%20however%2C%20even%20the%20best%0Aperforming%20systems%20still%20generate%20hallucinations%2C%20severely%20impacting%20user%0Atrust.%20Detecting%20hallucinations%20in%20Machine%20Translation%20%28MT%29%20remains%20a%20critical%0Achallenge%2C%20particularly%20since%20existing%20methods%20excel%20with%20High-Resource%0ALanguages%20%28HRLs%29%20but%20exhibit%20substantial%20limitations%20when%20applied%20to%0ALow-Resource%20Languages%20%28LRLs%29.%20This%20paper%20evaluates%20hallucination%20detection%0Aapproaches%20using%20Large%20Language%20Models%20%28LLMs%29%20and%20semantic%20similarity%20within%0Amassively%20multilingual%20embeddings.%20Our%20study%20spans%2016%20language%20directions%2C%0Acovering%20HRLs%2C%20LRLs%2C%20with%20diverse%20scripts.%20We%20find%20that%20the%20choice%20of%20model%20is%0Aessential%20for%20performance.%20On%20average%2C%20for%20HRLs%2C%20Llama3-70B%20outperforms%20the%0Aprevious%20state%20of%20the%20art%20by%20as%20much%20as%200.16%20MCC%20%28Matthews%20Correlation%0ACoefficient%29.%20However%2C%20for%20LRLs%20we%20observe%20that%20Claude%20Sonnet%20outperforms%20other%0ALLMs%20on%20average%20by%200.03%20MCC.%20The%20key%20takeaway%20from%20our%20study%20is%20that%20LLMs%20can%0Aachieve%20performance%20comparable%20or%20even%20better%20than%20previously%20proposed%20models%2C%0Adespite%20not%20being%20explicitly%20trained%20for%20any%20machine%20translation%20task.%20However%2C%0Atheir%20advantage%20is%20less%20significant%20for%20LRLs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16470v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Translation%2520Hallucination%2520Detection%2520for%2520Low%2520and%2520High%2520Resource%250A%2520%2520Languages%2520using%2520Large%2520Language%2520Models%26entry.906535625%3DKenza%2520Benkirane%2520and%2520Laura%2520Gongas%2520and%2520Shahar%2520Pelles%2520and%2520Naomi%2520Fuchs%2520and%2520Joshua%2520Darmon%2520and%2520Pontus%2520Stenetorp%2520and%2520David%2520Ifeoluwa%2520Adelani%2520and%2520Eduardo%2520S%25C3%25A1nchez%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520massively%2520multilingual%2520machine%2520translation%2520systems%250Ahave%2520significantly%2520enhanced%2520translation%2520accuracy%253B%2520however%252C%2520even%2520the%2520best%250Aperforming%2520systems%2520still%2520generate%2520hallucinations%252C%2520severely%2520impacting%2520user%250Atrust.%2520Detecting%2520hallucinations%2520in%2520Machine%2520Translation%2520%2528MT%2529%2520remains%2520a%2520critical%250Achallenge%252C%2520particularly%2520since%2520existing%2520methods%2520excel%2520with%2520High-Resource%250ALanguages%2520%2528HRLs%2529%2520but%2520exhibit%2520substantial%2520limitations%2520when%2520applied%2520to%250ALow-Resource%2520Languages%2520%2528LRLs%2529.%2520This%2520paper%2520evaluates%2520hallucination%2520detection%250Aapproaches%2520using%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520semantic%2520similarity%2520within%250Amassively%2520multilingual%2520embeddings.%2520Our%2520study%2520spans%252016%2520language%2520directions%252C%250Acovering%2520HRLs%252C%2520LRLs%252C%2520with%2520diverse%2520scripts.%2520We%2520find%2520that%2520the%2520choice%2520of%2520model%2520is%250Aessential%2520for%2520performance.%2520On%2520average%252C%2520for%2520HRLs%252C%2520Llama3-70B%2520outperforms%2520the%250Aprevious%2520state%2520of%2520the%2520art%2520by%2520as%2520much%2520as%25200.16%2520MCC%2520%2528Matthews%2520Correlation%250ACoefficient%2529.%2520However%252C%2520for%2520LRLs%2520we%2520observe%2520that%2520Claude%2520Sonnet%2520outperforms%2520other%250ALLMs%2520on%2520average%2520by%25200.03%2520MCC.%2520The%2520key%2520takeaway%2520from%2520our%2520study%2520is%2520that%2520LLMs%2520can%250Aachieve%2520performance%2520comparable%2520or%2520even%2520better%2520than%2520previously%2520proposed%2520models%252C%250Adespite%2520not%2520being%2520explicitly%2520trained%2520for%2520any%2520machine%2520translation%2520task.%2520However%252C%250Atheir%2520advantage%2520is%2520less%2520significant%2520for%2520LRLs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16470v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Translation%20Hallucination%20Detection%20for%20Low%20and%20High%20Resource%0A%20%20Languages%20using%20Large%20Language%20Models&entry.906535625=Kenza%20Benkirane%20and%20Laura%20Gongas%20and%20Shahar%20Pelles%20and%20Naomi%20Fuchs%20and%20Joshua%20Darmon%20and%20Pontus%20Stenetorp%20and%20David%20Ifeoluwa%20Adelani%20and%20Eduardo%20S%C3%A1nchez&entry.1292438233=%20%20Recent%20advancements%20in%20massively%20multilingual%20machine%20translation%20systems%0Ahave%20significantly%20enhanced%20translation%20accuracy%3B%20however%2C%20even%20the%20best%0Aperforming%20systems%20still%20generate%20hallucinations%2C%20severely%20impacting%20user%0Atrust.%20Detecting%20hallucinations%20in%20Machine%20Translation%20%28MT%29%20remains%20a%20critical%0Achallenge%2C%20particularly%20since%20existing%20methods%20excel%20with%20High-Resource%0ALanguages%20%28HRLs%29%20but%20exhibit%20substantial%20limitations%20when%20applied%20to%0ALow-Resource%20Languages%20%28LRLs%29.%20This%20paper%20evaluates%20hallucination%20detection%0Aapproaches%20using%20Large%20Language%20Models%20%28LLMs%29%20and%20semantic%20similarity%20within%0Amassively%20multilingual%20embeddings.%20Our%20study%20spans%2016%20language%20directions%2C%0Acovering%20HRLs%2C%20LRLs%2C%20with%20diverse%20scripts.%20We%20find%20that%20the%20choice%20of%20model%20is%0Aessential%20for%20performance.%20On%20average%2C%20for%20HRLs%2C%20Llama3-70B%20outperforms%20the%0Aprevious%20state%20of%20the%20art%20by%20as%20much%20as%200.16%20MCC%20%28Matthews%20Correlation%0ACoefficient%29.%20However%2C%20for%20LRLs%20we%20observe%20that%20Claude%20Sonnet%20outperforms%20other%0ALLMs%20on%20average%20by%200.03%20MCC.%20The%20key%20takeaway%20from%20our%20study%20is%20that%20LLMs%20can%0Aachieve%20performance%20comparable%20or%20even%20better%20than%20previously%20proposed%20models%2C%0Adespite%20not%20being%20explicitly%20trained%20for%20any%20machine%20translation%20task.%20However%2C%0Atheir%20advantage%20is%20less%20significant%20for%20LRLs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16470v2&entry.124074799=Read"},
{"title": "Multi-Resolution Histopathology Patch Graphs for Ovarian Cancer\n  Subtyping", "author": "Jack Breen and Katie Allen and Kieran Zucker and Nicolas M. Orsi and Nishant Ravikumar", "abstract": "  Computer vision models are increasingly capable of classifying ovarian\nepithelial cancer subtypes, but they differ from pathologists by processing\nsmall tissue patches at a single resolution. Multi-resolution graph models\nleverage the spatial relationships of patches at multiple magnifications,\nlearning the context for each patch. In this study, we conduct the most\nthorough validation of a graph model for ovarian cancer subtyping to date.\nSeven models were tuned and trained using five-fold cross-validation on a set\nof 1864 whole slide images (WSIs) from 434 patients treated at Leeds Teaching\nHospitals NHS Trust. The cross-validation models were ensembled and evaluated\nusing a balanced hold-out test set of 100 WSIs from 30 patients, and an\nexternal validation set of 80 WSIs from 80 patients in the Transcanadian Study.\nThe best-performing model, a graph model using 10x+20x magnification data, gave\nbalanced accuracies of 73%, 88%, and 99% in cross-validation, hold-out testing,\nand external validation, respectively. However, this only exceeded the\nperformance of attention-based multiple instance learning in external\nvalidation, with a 93% balanced accuracy. Graph models benefitted greatly from\nusing the UNI foundation model rather than an ImageNet-pretrained ResNet50 for\nfeature extraction, with this having a much greater effect on performance than\nchanging the subsequent classification approach. The accuracy of the combined\nfoundation model and multi-resolution graph network offers a step towards the\nclinical applicability of these models, with a new highest-reported performance\nfor this task, though further validations are still required to ensure the\nrobustness and usability of the models.\n", "link": "http://arxiv.org/abs/2407.18105v1", "date": "2024-07-25", "relevancy": 1.8516, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4715}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4663}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Resolution%20Histopathology%20Patch%20Graphs%20for%20Ovarian%20Cancer%0A%20%20Subtyping&body=Title%3A%20Multi-Resolution%20Histopathology%20Patch%20Graphs%20for%20Ovarian%20Cancer%0A%20%20Subtyping%0AAuthor%3A%20Jack%20Breen%20and%20Katie%20Allen%20and%20Kieran%20Zucker%20and%20Nicolas%20M.%20Orsi%20and%20Nishant%20Ravikumar%0AAbstract%3A%20%20%20Computer%20vision%20models%20are%20increasingly%20capable%20of%20classifying%20ovarian%0Aepithelial%20cancer%20subtypes%2C%20but%20they%20differ%20from%20pathologists%20by%20processing%0Asmall%20tissue%20patches%20at%20a%20single%20resolution.%20Multi-resolution%20graph%20models%0Aleverage%20the%20spatial%20relationships%20of%20patches%20at%20multiple%20magnifications%2C%0Alearning%20the%20context%20for%20each%20patch.%20In%20this%20study%2C%20we%20conduct%20the%20most%0Athorough%20validation%20of%20a%20graph%20model%20for%20ovarian%20cancer%20subtyping%20to%20date.%0ASeven%20models%20were%20tuned%20and%20trained%20using%20five-fold%20cross-validation%20on%20a%20set%0Aof%201864%20whole%20slide%20images%20%28WSIs%29%20from%20434%20patients%20treated%20at%20Leeds%20Teaching%0AHospitals%20NHS%20Trust.%20The%20cross-validation%20models%20were%20ensembled%20and%20evaluated%0Ausing%20a%20balanced%20hold-out%20test%20set%20of%20100%20WSIs%20from%2030%20patients%2C%20and%20an%0Aexternal%20validation%20set%20of%2080%20WSIs%20from%2080%20patients%20in%20the%20Transcanadian%20Study.%0AThe%20best-performing%20model%2C%20a%20graph%20model%20using%2010x%2B20x%20magnification%20data%2C%20gave%0Abalanced%20accuracies%20of%2073%25%2C%2088%25%2C%20and%2099%25%20in%20cross-validation%2C%20hold-out%20testing%2C%0Aand%20external%20validation%2C%20respectively.%20However%2C%20this%20only%20exceeded%20the%0Aperformance%20of%20attention-based%20multiple%20instance%20learning%20in%20external%0Avalidation%2C%20with%20a%2093%25%20balanced%20accuracy.%20Graph%20models%20benefitted%20greatly%20from%0Ausing%20the%20UNI%20foundation%20model%20rather%20than%20an%20ImageNet-pretrained%20ResNet50%20for%0Afeature%20extraction%2C%20with%20this%20having%20a%20much%20greater%20effect%20on%20performance%20than%0Achanging%20the%20subsequent%20classification%20approach.%20The%20accuracy%20of%20the%20combined%0Afoundation%20model%20and%20multi-resolution%20graph%20network%20offers%20a%20step%20towards%20the%0Aclinical%20applicability%20of%20these%20models%2C%20with%20a%20new%20highest-reported%20performance%0Afor%20this%20task%2C%20though%20further%20validations%20are%20still%20required%20to%20ensure%20the%0Arobustness%20and%20usability%20of%20the%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Resolution%2520Histopathology%2520Patch%2520Graphs%2520for%2520Ovarian%2520Cancer%250A%2520%2520Subtyping%26entry.906535625%3DJack%2520Breen%2520and%2520Katie%2520Allen%2520and%2520Kieran%2520Zucker%2520and%2520Nicolas%2520M.%2520Orsi%2520and%2520Nishant%2520Ravikumar%26entry.1292438233%3D%2520%2520Computer%2520vision%2520models%2520are%2520increasingly%2520capable%2520of%2520classifying%2520ovarian%250Aepithelial%2520cancer%2520subtypes%252C%2520but%2520they%2520differ%2520from%2520pathologists%2520by%2520processing%250Asmall%2520tissue%2520patches%2520at%2520a%2520single%2520resolution.%2520Multi-resolution%2520graph%2520models%250Aleverage%2520the%2520spatial%2520relationships%2520of%2520patches%2520at%2520multiple%2520magnifications%252C%250Alearning%2520the%2520context%2520for%2520each%2520patch.%2520In%2520this%2520study%252C%2520we%2520conduct%2520the%2520most%250Athorough%2520validation%2520of%2520a%2520graph%2520model%2520for%2520ovarian%2520cancer%2520subtyping%2520to%2520date.%250ASeven%2520models%2520were%2520tuned%2520and%2520trained%2520using%2520five-fold%2520cross-validation%2520on%2520a%2520set%250Aof%25201864%2520whole%2520slide%2520images%2520%2528WSIs%2529%2520from%2520434%2520patients%2520treated%2520at%2520Leeds%2520Teaching%250AHospitals%2520NHS%2520Trust.%2520The%2520cross-validation%2520models%2520were%2520ensembled%2520and%2520evaluated%250Ausing%2520a%2520balanced%2520hold-out%2520test%2520set%2520of%2520100%2520WSIs%2520from%252030%2520patients%252C%2520and%2520an%250Aexternal%2520validation%2520set%2520of%252080%2520WSIs%2520from%252080%2520patients%2520in%2520the%2520Transcanadian%2520Study.%250AThe%2520best-performing%2520model%252C%2520a%2520graph%2520model%2520using%252010x%252B20x%2520magnification%2520data%252C%2520gave%250Abalanced%2520accuracies%2520of%252073%2525%252C%252088%2525%252C%2520and%252099%2525%2520in%2520cross-validation%252C%2520hold-out%2520testing%252C%250Aand%2520external%2520validation%252C%2520respectively.%2520However%252C%2520this%2520only%2520exceeded%2520the%250Aperformance%2520of%2520attention-based%2520multiple%2520instance%2520learning%2520in%2520external%250Avalidation%252C%2520with%2520a%252093%2525%2520balanced%2520accuracy.%2520Graph%2520models%2520benefitted%2520greatly%2520from%250Ausing%2520the%2520UNI%2520foundation%2520model%2520rather%2520than%2520an%2520ImageNet-pretrained%2520ResNet50%2520for%250Afeature%2520extraction%252C%2520with%2520this%2520having%2520a%2520much%2520greater%2520effect%2520on%2520performance%2520than%250Achanging%2520the%2520subsequent%2520classification%2520approach.%2520The%2520accuracy%2520of%2520the%2520combined%250Afoundation%2520model%2520and%2520multi-resolution%2520graph%2520network%2520offers%2520a%2520step%2520towards%2520the%250Aclinical%2520applicability%2520of%2520these%2520models%252C%2520with%2520a%2520new%2520highest-reported%2520performance%250Afor%2520this%2520task%252C%2520though%2520further%2520validations%2520are%2520still%2520required%2520to%2520ensure%2520the%250Arobustness%2520and%2520usability%2520of%2520the%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Resolution%20Histopathology%20Patch%20Graphs%20for%20Ovarian%20Cancer%0A%20%20Subtyping&entry.906535625=Jack%20Breen%20and%20Katie%20Allen%20and%20Kieran%20Zucker%20and%20Nicolas%20M.%20Orsi%20and%20Nishant%20Ravikumar&entry.1292438233=%20%20Computer%20vision%20models%20are%20increasingly%20capable%20of%20classifying%20ovarian%0Aepithelial%20cancer%20subtypes%2C%20but%20they%20differ%20from%20pathologists%20by%20processing%0Asmall%20tissue%20patches%20at%20a%20single%20resolution.%20Multi-resolution%20graph%20models%0Aleverage%20the%20spatial%20relationships%20of%20patches%20at%20multiple%20magnifications%2C%0Alearning%20the%20context%20for%20each%20patch.%20In%20this%20study%2C%20we%20conduct%20the%20most%0Athorough%20validation%20of%20a%20graph%20model%20for%20ovarian%20cancer%20subtyping%20to%20date.%0ASeven%20models%20were%20tuned%20and%20trained%20using%20five-fold%20cross-validation%20on%20a%20set%0Aof%201864%20whole%20slide%20images%20%28WSIs%29%20from%20434%20patients%20treated%20at%20Leeds%20Teaching%0AHospitals%20NHS%20Trust.%20The%20cross-validation%20models%20were%20ensembled%20and%20evaluated%0Ausing%20a%20balanced%20hold-out%20test%20set%20of%20100%20WSIs%20from%2030%20patients%2C%20and%20an%0Aexternal%20validation%20set%20of%2080%20WSIs%20from%2080%20patients%20in%20the%20Transcanadian%20Study.%0AThe%20best-performing%20model%2C%20a%20graph%20model%20using%2010x%2B20x%20magnification%20data%2C%20gave%0Abalanced%20accuracies%20of%2073%25%2C%2088%25%2C%20and%2099%25%20in%20cross-validation%2C%20hold-out%20testing%2C%0Aand%20external%20validation%2C%20respectively.%20However%2C%20this%20only%20exceeded%20the%0Aperformance%20of%20attention-based%20multiple%20instance%20learning%20in%20external%0Avalidation%2C%20with%20a%2093%25%20balanced%20accuracy.%20Graph%20models%20benefitted%20greatly%20from%0Ausing%20the%20UNI%20foundation%20model%20rather%20than%20an%20ImageNet-pretrained%20ResNet50%20for%0Afeature%20extraction%2C%20with%20this%20having%20a%20much%20greater%20effect%20on%20performance%20than%0Achanging%20the%20subsequent%20classification%20approach.%20The%20accuracy%20of%20the%20combined%0Afoundation%20model%20and%20multi-resolution%20graph%20network%20offers%20a%20step%20towards%20the%0Aclinical%20applicability%20of%20these%20models%2C%20with%20a%20new%20highest-reported%20performance%0Afor%20this%20task%2C%20though%20further%20validations%20are%20still%20required%20to%20ensure%20the%0Arobustness%20and%20usability%20of%20the%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18105v1&entry.124074799=Read"},
{"title": "AsEP: Benchmarking Deep Learning Methods for Antibody-specific Epitope\n  Prediction", "author": "Chunan Liu and Lilian Denzler and Yihong Chen and Andrew Martin and Brooks Paige", "abstract": "  Epitope identification is vital for antibody design yet challenging due to\nthe inherent variability in antibodies. While many deep learning methods have\nbeen developed for general protein binding site prediction tasks, whether they\nwork for epitope prediction remains an understudied research question. The\nchallenge is also heightened by the lack of a consistent evaluation pipeline\nwith sufficient dataset size and epitope diversity. We introduce a filtered\nantibody-antigen complex structure dataset, AsEP (Antibody-specific Epitope\nPrediction). AsEP is the largest of its kind and provides clustered epitope\ngroups, allowing the community to develop and test novel epitope prediction\nmethods. AsEP comes with an easy-to-use interface in Python and pre-built graph\nrepresentations of each antibody-antigen complex while also supporting\ncustomizable embedding methods. Based on this new dataset, we benchmarked\nvarious representative general protein-binding site prediction methods and find\nthat their performances are not satisfactory as expected for epitope\nprediction. We thus propose a new method, WALLE, that leverages both protein\nlanguage models and graph neural networks. WALLE demonstrate about 5X\nperformance gain over existing methods. Our empirical findings evidence that\nepitope prediction benefits from combining sequential embeddings provided by\nlanguage models and geometrical information from graph representations,\nproviding a guideline for future method design. In addition, we reformulate the\ntask as bipartite link prediction, allowing easy model performance attribution\nand interpretability. We open-source our data and code at\nhttps://github.com/biochunan/AsEP-dataset.\n", "link": "http://arxiv.org/abs/2407.18184v1", "date": "2024-07-25", "relevancy": 1.8451, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4995}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4577}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AsEP%3A%20Benchmarking%20Deep%20Learning%20Methods%20for%20Antibody-specific%20Epitope%0A%20%20Prediction&body=Title%3A%20AsEP%3A%20Benchmarking%20Deep%20Learning%20Methods%20for%20Antibody-specific%20Epitope%0A%20%20Prediction%0AAuthor%3A%20Chunan%20Liu%20and%20Lilian%20Denzler%20and%20Yihong%20Chen%20and%20Andrew%20Martin%20and%20Brooks%20Paige%0AAbstract%3A%20%20%20Epitope%20identification%20is%20vital%20for%20antibody%20design%20yet%20challenging%20due%20to%0Athe%20inherent%20variability%20in%20antibodies.%20While%20many%20deep%20learning%20methods%20have%0Abeen%20developed%20for%20general%20protein%20binding%20site%20prediction%20tasks%2C%20whether%20they%0Awork%20for%20epitope%20prediction%20remains%20an%20understudied%20research%20question.%20The%0Achallenge%20is%20also%20heightened%20by%20the%20lack%20of%20a%20consistent%20evaluation%20pipeline%0Awith%20sufficient%20dataset%20size%20and%20epitope%20diversity.%20We%20introduce%20a%20filtered%0Aantibody-antigen%20complex%20structure%20dataset%2C%20AsEP%20%28Antibody-specific%20Epitope%0APrediction%29.%20AsEP%20is%20the%20largest%20of%20its%20kind%20and%20provides%20clustered%20epitope%0Agroups%2C%20allowing%20the%20community%20to%20develop%20and%20test%20novel%20epitope%20prediction%0Amethods.%20AsEP%20comes%20with%20an%20easy-to-use%20interface%20in%20Python%20and%20pre-built%20graph%0Arepresentations%20of%20each%20antibody-antigen%20complex%20while%20also%20supporting%0Acustomizable%20embedding%20methods.%20Based%20on%20this%20new%20dataset%2C%20we%20benchmarked%0Avarious%20representative%20general%20protein-binding%20site%20prediction%20methods%20and%20find%0Athat%20their%20performances%20are%20not%20satisfactory%20as%20expected%20for%20epitope%0Aprediction.%20We%20thus%20propose%20a%20new%20method%2C%20WALLE%2C%20that%20leverages%20both%20protein%0Alanguage%20models%20and%20graph%20neural%20networks.%20WALLE%20demonstrate%20about%205X%0Aperformance%20gain%20over%20existing%20methods.%20Our%20empirical%20findings%20evidence%20that%0Aepitope%20prediction%20benefits%20from%20combining%20sequential%20embeddings%20provided%20by%0Alanguage%20models%20and%20geometrical%20information%20from%20graph%20representations%2C%0Aproviding%20a%20guideline%20for%20future%20method%20design.%20In%20addition%2C%20we%20reformulate%20the%0Atask%20as%20bipartite%20link%20prediction%2C%20allowing%20easy%20model%20performance%20attribution%0Aand%20interpretability.%20We%20open-source%20our%20data%20and%20code%20at%0Ahttps%3A//github.com/biochunan/AsEP-dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18184v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsEP%253A%2520Benchmarking%2520Deep%2520Learning%2520Methods%2520for%2520Antibody-specific%2520Epitope%250A%2520%2520Prediction%26entry.906535625%3DChunan%2520Liu%2520and%2520Lilian%2520Denzler%2520and%2520Yihong%2520Chen%2520and%2520Andrew%2520Martin%2520and%2520Brooks%2520Paige%26entry.1292438233%3D%2520%2520Epitope%2520identification%2520is%2520vital%2520for%2520antibody%2520design%2520yet%2520challenging%2520due%2520to%250Athe%2520inherent%2520variability%2520in%2520antibodies.%2520While%2520many%2520deep%2520learning%2520methods%2520have%250Abeen%2520developed%2520for%2520general%2520protein%2520binding%2520site%2520prediction%2520tasks%252C%2520whether%2520they%250Awork%2520for%2520epitope%2520prediction%2520remains%2520an%2520understudied%2520research%2520question.%2520The%250Achallenge%2520is%2520also%2520heightened%2520by%2520the%2520lack%2520of%2520a%2520consistent%2520evaluation%2520pipeline%250Awith%2520sufficient%2520dataset%2520size%2520and%2520epitope%2520diversity.%2520We%2520introduce%2520a%2520filtered%250Aantibody-antigen%2520complex%2520structure%2520dataset%252C%2520AsEP%2520%2528Antibody-specific%2520Epitope%250APrediction%2529.%2520AsEP%2520is%2520the%2520largest%2520of%2520its%2520kind%2520and%2520provides%2520clustered%2520epitope%250Agroups%252C%2520allowing%2520the%2520community%2520to%2520develop%2520and%2520test%2520novel%2520epitope%2520prediction%250Amethods.%2520AsEP%2520comes%2520with%2520an%2520easy-to-use%2520interface%2520in%2520Python%2520and%2520pre-built%2520graph%250Arepresentations%2520of%2520each%2520antibody-antigen%2520complex%2520while%2520also%2520supporting%250Acustomizable%2520embedding%2520methods.%2520Based%2520on%2520this%2520new%2520dataset%252C%2520we%2520benchmarked%250Avarious%2520representative%2520general%2520protein-binding%2520site%2520prediction%2520methods%2520and%2520find%250Athat%2520their%2520performances%2520are%2520not%2520satisfactory%2520as%2520expected%2520for%2520epitope%250Aprediction.%2520We%2520thus%2520propose%2520a%2520new%2520method%252C%2520WALLE%252C%2520that%2520leverages%2520both%2520protein%250Alanguage%2520models%2520and%2520graph%2520neural%2520networks.%2520WALLE%2520demonstrate%2520about%25205X%250Aperformance%2520gain%2520over%2520existing%2520methods.%2520Our%2520empirical%2520findings%2520evidence%2520that%250Aepitope%2520prediction%2520benefits%2520from%2520combining%2520sequential%2520embeddings%2520provided%2520by%250Alanguage%2520models%2520and%2520geometrical%2520information%2520from%2520graph%2520representations%252C%250Aproviding%2520a%2520guideline%2520for%2520future%2520method%2520design.%2520In%2520addition%252C%2520we%2520reformulate%2520the%250Atask%2520as%2520bipartite%2520link%2520prediction%252C%2520allowing%2520easy%2520model%2520performance%2520attribution%250Aand%2520interpretability.%2520We%2520open-source%2520our%2520data%2520and%2520code%2520at%250Ahttps%253A//github.com/biochunan/AsEP-dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18184v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AsEP%3A%20Benchmarking%20Deep%20Learning%20Methods%20for%20Antibody-specific%20Epitope%0A%20%20Prediction&entry.906535625=Chunan%20Liu%20and%20Lilian%20Denzler%20and%20Yihong%20Chen%20and%20Andrew%20Martin%20and%20Brooks%20Paige&entry.1292438233=%20%20Epitope%20identification%20is%20vital%20for%20antibody%20design%20yet%20challenging%20due%20to%0Athe%20inherent%20variability%20in%20antibodies.%20While%20many%20deep%20learning%20methods%20have%0Abeen%20developed%20for%20general%20protein%20binding%20site%20prediction%20tasks%2C%20whether%20they%0Awork%20for%20epitope%20prediction%20remains%20an%20understudied%20research%20question.%20The%0Achallenge%20is%20also%20heightened%20by%20the%20lack%20of%20a%20consistent%20evaluation%20pipeline%0Awith%20sufficient%20dataset%20size%20and%20epitope%20diversity.%20We%20introduce%20a%20filtered%0Aantibody-antigen%20complex%20structure%20dataset%2C%20AsEP%20%28Antibody-specific%20Epitope%0APrediction%29.%20AsEP%20is%20the%20largest%20of%20its%20kind%20and%20provides%20clustered%20epitope%0Agroups%2C%20allowing%20the%20community%20to%20develop%20and%20test%20novel%20epitope%20prediction%0Amethods.%20AsEP%20comes%20with%20an%20easy-to-use%20interface%20in%20Python%20and%20pre-built%20graph%0Arepresentations%20of%20each%20antibody-antigen%20complex%20while%20also%20supporting%0Acustomizable%20embedding%20methods.%20Based%20on%20this%20new%20dataset%2C%20we%20benchmarked%0Avarious%20representative%20general%20protein-binding%20site%20prediction%20methods%20and%20find%0Athat%20their%20performances%20are%20not%20satisfactory%20as%20expected%20for%20epitope%0Aprediction.%20We%20thus%20propose%20a%20new%20method%2C%20WALLE%2C%20that%20leverages%20both%20protein%0Alanguage%20models%20and%20graph%20neural%20networks.%20WALLE%20demonstrate%20about%205X%0Aperformance%20gain%20over%20existing%20methods.%20Our%20empirical%20findings%20evidence%20that%0Aepitope%20prediction%20benefits%20from%20combining%20sequential%20embeddings%20provided%20by%0Alanguage%20models%20and%20geometrical%20information%20from%20graph%20representations%2C%0Aproviding%20a%20guideline%20for%20future%20method%20design.%20In%20addition%2C%20we%20reformulate%20the%0Atask%20as%20bipartite%20link%20prediction%2C%20allowing%20easy%20model%20performance%20attribution%0Aand%20interpretability.%20We%20open-source%20our%20data%20and%20code%20at%0Ahttps%3A//github.com/biochunan/AsEP-dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18184v1&entry.124074799=Read"},
{"title": "Sparse Incremental Aggregation in Multi-Hop Federated Learning", "author": "Sourav Mukherjee and Nasrin Razmi and Armin Dekorsy and Petar Popovski and Bho Matthiesen", "abstract": "  This paper investigates federated learning (FL) in a multi-hop communication\nsetup, such as in constellations with inter-satellite links. In this setup,\npart of the FL clients are responsible for forwarding other client's results to\nthe parameter server. Instead of using conventional routing, the communication\nefficiency can be improved significantly by using in-network model aggregation\nat each intermediate hop, known as incremental aggregation (IA). Prior works\n[1] have indicated diminishing gains for IA under gradient sparsification. Here\nwe study this issue and propose several novel correlated sparsification methods\nfor IA. Numerical results show that, for some of these algorithms, the full\npotential of IA is still available under sparsification without impairing\nconvergence. We demonstrate a 15x improvement in communication efficiency over\nconventional routing and a 11x improvement over state-of-the-art (SoA) sparse\nIA.\n", "link": "http://arxiv.org/abs/2407.18200v1", "date": "2024-07-25", "relevancy": 1.8375, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4725}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4563}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Incremental%20Aggregation%20in%20Multi-Hop%20Federated%20Learning&body=Title%3A%20Sparse%20Incremental%20Aggregation%20in%20Multi-Hop%20Federated%20Learning%0AAuthor%3A%20Sourav%20Mukherjee%20and%20Nasrin%20Razmi%20and%20Armin%20Dekorsy%20and%20Petar%20Popovski%20and%20Bho%20Matthiesen%0AAbstract%3A%20%20%20This%20paper%20investigates%20federated%20learning%20%28FL%29%20in%20a%20multi-hop%20communication%0Asetup%2C%20such%20as%20in%20constellations%20with%20inter-satellite%20links.%20In%20this%20setup%2C%0Apart%20of%20the%20FL%20clients%20are%20responsible%20for%20forwarding%20other%20client%27s%20results%20to%0Athe%20parameter%20server.%20Instead%20of%20using%20conventional%20routing%2C%20the%20communication%0Aefficiency%20can%20be%20improved%20significantly%20by%20using%20in-network%20model%20aggregation%0Aat%20each%20intermediate%20hop%2C%20known%20as%20incremental%20aggregation%20%28IA%29.%20Prior%20works%0A%5B1%5D%20have%20indicated%20diminishing%20gains%20for%20IA%20under%20gradient%20sparsification.%20Here%0Awe%20study%20this%20issue%20and%20propose%20several%20novel%20correlated%20sparsification%20methods%0Afor%20IA.%20Numerical%20results%20show%20that%2C%20for%20some%20of%20these%20algorithms%2C%20the%20full%0Apotential%20of%20IA%20is%20still%20available%20under%20sparsification%20without%20impairing%0Aconvergence.%20We%20demonstrate%20a%2015x%20improvement%20in%20communication%20efficiency%20over%0Aconventional%20routing%20and%20a%2011x%20improvement%20over%20state-of-the-art%20%28SoA%29%20sparse%0AIA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Incremental%2520Aggregation%2520in%2520Multi-Hop%2520Federated%2520Learning%26entry.906535625%3DSourav%2520Mukherjee%2520and%2520Nasrin%2520Razmi%2520and%2520Armin%2520Dekorsy%2520and%2520Petar%2520Popovski%2520and%2520Bho%2520Matthiesen%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520federated%2520learning%2520%2528FL%2529%2520in%2520a%2520multi-hop%2520communication%250Asetup%252C%2520such%2520as%2520in%2520constellations%2520with%2520inter-satellite%2520links.%2520In%2520this%2520setup%252C%250Apart%2520of%2520the%2520FL%2520clients%2520are%2520responsible%2520for%2520forwarding%2520other%2520client%2527s%2520results%2520to%250Athe%2520parameter%2520server.%2520Instead%2520of%2520using%2520conventional%2520routing%252C%2520the%2520communication%250Aefficiency%2520can%2520be%2520improved%2520significantly%2520by%2520using%2520in-network%2520model%2520aggregation%250Aat%2520each%2520intermediate%2520hop%252C%2520known%2520as%2520incremental%2520aggregation%2520%2528IA%2529.%2520Prior%2520works%250A%255B1%255D%2520have%2520indicated%2520diminishing%2520gains%2520for%2520IA%2520under%2520gradient%2520sparsification.%2520Here%250Awe%2520study%2520this%2520issue%2520and%2520propose%2520several%2520novel%2520correlated%2520sparsification%2520methods%250Afor%2520IA.%2520Numerical%2520results%2520show%2520that%252C%2520for%2520some%2520of%2520these%2520algorithms%252C%2520the%2520full%250Apotential%2520of%2520IA%2520is%2520still%2520available%2520under%2520sparsification%2520without%2520impairing%250Aconvergence.%2520We%2520demonstrate%2520a%252015x%2520improvement%2520in%2520communication%2520efficiency%2520over%250Aconventional%2520routing%2520and%2520a%252011x%2520improvement%2520over%2520state-of-the-art%2520%2528SoA%2529%2520sparse%250AIA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Incremental%20Aggregation%20in%20Multi-Hop%20Federated%20Learning&entry.906535625=Sourav%20Mukherjee%20and%20Nasrin%20Razmi%20and%20Armin%20Dekorsy%20and%20Petar%20Popovski%20and%20Bho%20Matthiesen&entry.1292438233=%20%20This%20paper%20investigates%20federated%20learning%20%28FL%29%20in%20a%20multi-hop%20communication%0Asetup%2C%20such%20as%20in%20constellations%20with%20inter-satellite%20links.%20In%20this%20setup%2C%0Apart%20of%20the%20FL%20clients%20are%20responsible%20for%20forwarding%20other%20client%27s%20results%20to%0Athe%20parameter%20server.%20Instead%20of%20using%20conventional%20routing%2C%20the%20communication%0Aefficiency%20can%20be%20improved%20significantly%20by%20using%20in-network%20model%20aggregation%0Aat%20each%20intermediate%20hop%2C%20known%20as%20incremental%20aggregation%20%28IA%29.%20Prior%20works%0A%5B1%5D%20have%20indicated%20diminishing%20gains%20for%20IA%20under%20gradient%20sparsification.%20Here%0Awe%20study%20this%20issue%20and%20propose%20several%20novel%20correlated%20sparsification%20methods%0Afor%20IA.%20Numerical%20results%20show%20that%2C%20for%20some%20of%20these%20algorithms%2C%20the%20full%0Apotential%20of%20IA%20is%20still%20available%20under%20sparsification%20without%20impairing%0Aconvergence.%20We%20demonstrate%20a%2015x%20improvement%20in%20communication%20efficiency%20over%0Aconventional%20routing%20and%20a%2011x%20improvement%20over%20state-of-the-art%20%28SoA%29%20sparse%0AIA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18200v1&entry.124074799=Read"},
{"title": "Quadratic Advantage with Quantum Randomized Smoothing Applied to\n  Time-Series Analysis", "author": "Nicola Franco and Marie Kempkes and Jakob Spiegelberg and Jeanette Miriam Lorenz", "abstract": "  As quantum machine learning continues to develop at a rapid pace, the\nimportance of ensuring the robustness and efficiency of quantum algorithms\ncannot be overstated. Our research presents an analysis of quantum randomized\nsmoothing, how data encoding and perturbation modeling approaches can be\nmatched to achieve meaningful robustness certificates. By utilizing an\ninnovative approach integrating Grover's algorithm, a quadratic sampling\nadvantage over classical randomized smoothing is achieved. This strategy\nnecessitates a basis state encoding, thus restricting the space of meaningful\nperturbations. We show how constrained $k$-distant Hamming weight perturbations\nare a suitable noise distribution here, and elucidate how they can be\nconstructed on a quantum computer. The efficacy of the proposed framework is\ndemonstrated on a time series classification task employing a Bag-of-Words\npre-processing solution. The advantage of quadratic sample reduction is\nrecovered especially in the regime with large number of samples. This may allow\nquantum computers to efficiently scale randomized smoothing to more complex\ntasks beyond the reach of classical methods.\n", "link": "http://arxiv.org/abs/2407.18021v1", "date": "2024-07-25", "relevancy": 1.8343, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4742}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4624}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quadratic%20Advantage%20with%20Quantum%20Randomized%20Smoothing%20Applied%20to%0A%20%20Time-Series%20Analysis&body=Title%3A%20Quadratic%20Advantage%20with%20Quantum%20Randomized%20Smoothing%20Applied%20to%0A%20%20Time-Series%20Analysis%0AAuthor%3A%20Nicola%20Franco%20and%20Marie%20Kempkes%20and%20Jakob%20Spiegelberg%20and%20Jeanette%20Miriam%20Lorenz%0AAbstract%3A%20%20%20As%20quantum%20machine%20learning%20continues%20to%20develop%20at%20a%20rapid%20pace%2C%20the%0Aimportance%20of%20ensuring%20the%20robustness%20and%20efficiency%20of%20quantum%20algorithms%0Acannot%20be%20overstated.%20Our%20research%20presents%20an%20analysis%20of%20quantum%20randomized%0Asmoothing%2C%20how%20data%20encoding%20and%20perturbation%20modeling%20approaches%20can%20be%0Amatched%20to%20achieve%20meaningful%20robustness%20certificates.%20By%20utilizing%20an%0Ainnovative%20approach%20integrating%20Grover%27s%20algorithm%2C%20a%20quadratic%20sampling%0Aadvantage%20over%20classical%20randomized%20smoothing%20is%20achieved.%20This%20strategy%0Anecessitates%20a%20basis%20state%20encoding%2C%20thus%20restricting%20the%20space%20of%20meaningful%0Aperturbations.%20We%20show%20how%20constrained%20%24k%24-distant%20Hamming%20weight%20perturbations%0Aare%20a%20suitable%20noise%20distribution%20here%2C%20and%20elucidate%20how%20they%20can%20be%0Aconstructed%20on%20a%20quantum%20computer.%20The%20efficacy%20of%20the%20proposed%20framework%20is%0Ademonstrated%20on%20a%20time%20series%20classification%20task%20employing%20a%20Bag-of-Words%0Apre-processing%20solution.%20The%20advantage%20of%20quadratic%20sample%20reduction%20is%0Arecovered%20especially%20in%20the%20regime%20with%20large%20number%20of%20samples.%20This%20may%20allow%0Aquantum%20computers%20to%20efficiently%20scale%20randomized%20smoothing%20to%20more%20complex%0Atasks%20beyond%20the%20reach%20of%20classical%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuadratic%2520Advantage%2520with%2520Quantum%2520Randomized%2520Smoothing%2520Applied%2520to%250A%2520%2520Time-Series%2520Analysis%26entry.906535625%3DNicola%2520Franco%2520and%2520Marie%2520Kempkes%2520and%2520Jakob%2520Spiegelberg%2520and%2520Jeanette%2520Miriam%2520Lorenz%26entry.1292438233%3D%2520%2520As%2520quantum%2520machine%2520learning%2520continues%2520to%2520develop%2520at%2520a%2520rapid%2520pace%252C%2520the%250Aimportance%2520of%2520ensuring%2520the%2520robustness%2520and%2520efficiency%2520of%2520quantum%2520algorithms%250Acannot%2520be%2520overstated.%2520Our%2520research%2520presents%2520an%2520analysis%2520of%2520quantum%2520randomized%250Asmoothing%252C%2520how%2520data%2520encoding%2520and%2520perturbation%2520modeling%2520approaches%2520can%2520be%250Amatched%2520to%2520achieve%2520meaningful%2520robustness%2520certificates.%2520By%2520utilizing%2520an%250Ainnovative%2520approach%2520integrating%2520Grover%2527s%2520algorithm%252C%2520a%2520quadratic%2520sampling%250Aadvantage%2520over%2520classical%2520randomized%2520smoothing%2520is%2520achieved.%2520This%2520strategy%250Anecessitates%2520a%2520basis%2520state%2520encoding%252C%2520thus%2520restricting%2520the%2520space%2520of%2520meaningful%250Aperturbations.%2520We%2520show%2520how%2520constrained%2520%2524k%2524-distant%2520Hamming%2520weight%2520perturbations%250Aare%2520a%2520suitable%2520noise%2520distribution%2520here%252C%2520and%2520elucidate%2520how%2520they%2520can%2520be%250Aconstructed%2520on%2520a%2520quantum%2520computer.%2520The%2520efficacy%2520of%2520the%2520proposed%2520framework%2520is%250Ademonstrated%2520on%2520a%2520time%2520series%2520classification%2520task%2520employing%2520a%2520Bag-of-Words%250Apre-processing%2520solution.%2520The%2520advantage%2520of%2520quadratic%2520sample%2520reduction%2520is%250Arecovered%2520especially%2520in%2520the%2520regime%2520with%2520large%2520number%2520of%2520samples.%2520This%2520may%2520allow%250Aquantum%2520computers%2520to%2520efficiently%2520scale%2520randomized%2520smoothing%2520to%2520more%2520complex%250Atasks%2520beyond%2520the%2520reach%2520of%2520classical%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quadratic%20Advantage%20with%20Quantum%20Randomized%20Smoothing%20Applied%20to%0A%20%20Time-Series%20Analysis&entry.906535625=Nicola%20Franco%20and%20Marie%20Kempkes%20and%20Jakob%20Spiegelberg%20and%20Jeanette%20Miriam%20Lorenz&entry.1292438233=%20%20As%20quantum%20machine%20learning%20continues%20to%20develop%20at%20a%20rapid%20pace%2C%20the%0Aimportance%20of%20ensuring%20the%20robustness%20and%20efficiency%20of%20quantum%20algorithms%0Acannot%20be%20overstated.%20Our%20research%20presents%20an%20analysis%20of%20quantum%20randomized%0Asmoothing%2C%20how%20data%20encoding%20and%20perturbation%20modeling%20approaches%20can%20be%0Amatched%20to%20achieve%20meaningful%20robustness%20certificates.%20By%20utilizing%20an%0Ainnovative%20approach%20integrating%20Grover%27s%20algorithm%2C%20a%20quadratic%20sampling%0Aadvantage%20over%20classical%20randomized%20smoothing%20is%20achieved.%20This%20strategy%0Anecessitates%20a%20basis%20state%20encoding%2C%20thus%20restricting%20the%20space%20of%20meaningful%0Aperturbations.%20We%20show%20how%20constrained%20%24k%24-distant%20Hamming%20weight%20perturbations%0Aare%20a%20suitable%20noise%20distribution%20here%2C%20and%20elucidate%20how%20they%20can%20be%0Aconstructed%20on%20a%20quantum%20computer.%20The%20efficacy%20of%20the%20proposed%20framework%20is%0Ademonstrated%20on%20a%20time%20series%20classification%20task%20employing%20a%20Bag-of-Words%0Apre-processing%20solution.%20The%20advantage%20of%20quadratic%20sample%20reduction%20is%0Arecovered%20especially%20in%20the%20regime%20with%20large%20number%20of%20samples.%20This%20may%20allow%0Aquantum%20computers%20to%20efficiently%20scale%20randomized%20smoothing%20to%20more%20complex%0Atasks%20beyond%20the%20reach%20of%20classical%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18021v1&entry.124074799=Read"},
{"title": "Investigation to answer three key questions concerning plant pest\n  identification and development of a practical identification framework", "author": "Ryosuke Wayama and Yuki Sasaki and Satoshi Kagiwada and Nobusuke Iwasaki and Hitoshi Iyatomi", "abstract": "  The development of practical and robust automated diagnostic systems for\nidentifying plant pests is crucial for efficient agricultural production. In\nthis paper, we first investigate three key research questions (RQs) that have\nnot been addressed thus far in the field of image-based plant pest\nidentification. Based on the knowledge gained, we then develop an accurate,\nrobust, and fast plant pest identification framework using 334K images\ncomprising 78 combinations of four plant portions (the leaf front, leaf back,\nfruit, and flower of cucumber, tomato, strawberry, and eggplant) and 20 pest\nspecies captured at 27 farms. The results reveal the following. (1) For an\nappropriate evaluation of the model, the test data should not include images of\nthe field from which the training images were collected, or other\nconsiderations to increase the diversity of the test set should be taken into\naccount. (2) Pre-extraction of ROIs, such as leaves and fruits, helps to\nimprove identification accuracy. (3) Integration of closely related species\nusing the same control methods and cross-crop training methods for the same\npests, are effective. Our two-stage plant pest identification framework,\nenabling ROI detection and convolutional neural network (CNN)-based\nidentification, achieved a highly practical performance of 91.0% and 88.5% in\nmean accuracy and macro F1 score, respectively, for 12,223 instances of test\ndata of 21 classes collected from unseen fields, where 25 classes of images\nfrom 318,971 samples were used for training; the average identification time\nwas 476 ms/image.\n", "link": "http://arxiv.org/abs/2407.18000v1", "date": "2024-07-25", "relevancy": 1.8279, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4728}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4543}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigation%20to%20answer%20three%20key%20questions%20concerning%20plant%20pest%0A%20%20identification%20and%20development%20of%20a%20practical%20identification%20framework&body=Title%3A%20Investigation%20to%20answer%20three%20key%20questions%20concerning%20plant%20pest%0A%20%20identification%20and%20development%20of%20a%20practical%20identification%20framework%0AAuthor%3A%20Ryosuke%20Wayama%20and%20Yuki%20Sasaki%20and%20Satoshi%20Kagiwada%20and%20Nobusuke%20Iwasaki%20and%20Hitoshi%20Iyatomi%0AAbstract%3A%20%20%20The%20development%20of%20practical%20and%20robust%20automated%20diagnostic%20systems%20for%0Aidentifying%20plant%20pests%20is%20crucial%20for%20efficient%20agricultural%20production.%20In%0Athis%20paper%2C%20we%20first%20investigate%20three%20key%20research%20questions%20%28RQs%29%20that%20have%0Anot%20been%20addressed%20thus%20far%20in%20the%20field%20of%20image-based%20plant%20pest%0Aidentification.%20Based%20on%20the%20knowledge%20gained%2C%20we%20then%20develop%20an%20accurate%2C%0Arobust%2C%20and%20fast%20plant%20pest%20identification%20framework%20using%20334K%20images%0Acomprising%2078%20combinations%20of%20four%20plant%20portions%20%28the%20leaf%20front%2C%20leaf%20back%2C%0Afruit%2C%20and%20flower%20of%20cucumber%2C%20tomato%2C%20strawberry%2C%20and%20eggplant%29%20and%2020%20pest%0Aspecies%20captured%20at%2027%20farms.%20The%20results%20reveal%20the%20following.%20%281%29%20For%20an%0Aappropriate%20evaluation%20of%20the%20model%2C%20the%20test%20data%20should%20not%20include%20images%20of%0Athe%20field%20from%20which%20the%20training%20images%20were%20collected%2C%20or%20other%0Aconsiderations%20to%20increase%20the%20diversity%20of%20the%20test%20set%20should%20be%20taken%20into%0Aaccount.%20%282%29%20Pre-extraction%20of%20ROIs%2C%20such%20as%20leaves%20and%20fruits%2C%20helps%20to%0Aimprove%20identification%20accuracy.%20%283%29%20Integration%20of%20closely%20related%20species%0Ausing%20the%20same%20control%20methods%20and%20cross-crop%20training%20methods%20for%20the%20same%0Apests%2C%20are%20effective.%20Our%20two-stage%20plant%20pest%20identification%20framework%2C%0Aenabling%20ROI%20detection%20and%20convolutional%20neural%20network%20%28CNN%29-based%0Aidentification%2C%20achieved%20a%20highly%20practical%20performance%20of%2091.0%25%20and%2088.5%25%20in%0Amean%20accuracy%20and%20macro%20F1%20score%2C%20respectively%2C%20for%2012%2C223%20instances%20of%20test%0Adata%20of%2021%20classes%20collected%20from%20unseen%20fields%2C%20where%2025%20classes%20of%20images%0Afrom%20318%2C971%20samples%20were%20used%20for%20training%3B%20the%20average%20identification%20time%0Awas%20476%20ms/image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigation%2520to%2520answer%2520three%2520key%2520questions%2520concerning%2520plant%2520pest%250A%2520%2520identification%2520and%2520development%2520of%2520a%2520practical%2520identification%2520framework%26entry.906535625%3DRyosuke%2520Wayama%2520and%2520Yuki%2520Sasaki%2520and%2520Satoshi%2520Kagiwada%2520and%2520Nobusuke%2520Iwasaki%2520and%2520Hitoshi%2520Iyatomi%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520practical%2520and%2520robust%2520automated%2520diagnostic%2520systems%2520for%250Aidentifying%2520plant%2520pests%2520is%2520crucial%2520for%2520efficient%2520agricultural%2520production.%2520In%250Athis%2520paper%252C%2520we%2520first%2520investigate%2520three%2520key%2520research%2520questions%2520%2528RQs%2529%2520that%2520have%250Anot%2520been%2520addressed%2520thus%2520far%2520in%2520the%2520field%2520of%2520image-based%2520plant%2520pest%250Aidentification.%2520Based%2520on%2520the%2520knowledge%2520gained%252C%2520we%2520then%2520develop%2520an%2520accurate%252C%250Arobust%252C%2520and%2520fast%2520plant%2520pest%2520identification%2520framework%2520using%2520334K%2520images%250Acomprising%252078%2520combinations%2520of%2520four%2520plant%2520portions%2520%2528the%2520leaf%2520front%252C%2520leaf%2520back%252C%250Afruit%252C%2520and%2520flower%2520of%2520cucumber%252C%2520tomato%252C%2520strawberry%252C%2520and%2520eggplant%2529%2520and%252020%2520pest%250Aspecies%2520captured%2520at%252027%2520farms.%2520The%2520results%2520reveal%2520the%2520following.%2520%25281%2529%2520For%2520an%250Aappropriate%2520evaluation%2520of%2520the%2520model%252C%2520the%2520test%2520data%2520should%2520not%2520include%2520images%2520of%250Athe%2520field%2520from%2520which%2520the%2520training%2520images%2520were%2520collected%252C%2520or%2520other%250Aconsiderations%2520to%2520increase%2520the%2520diversity%2520of%2520the%2520test%2520set%2520should%2520be%2520taken%2520into%250Aaccount.%2520%25282%2529%2520Pre-extraction%2520of%2520ROIs%252C%2520such%2520as%2520leaves%2520and%2520fruits%252C%2520helps%2520to%250Aimprove%2520identification%2520accuracy.%2520%25283%2529%2520Integration%2520of%2520closely%2520related%2520species%250Ausing%2520the%2520same%2520control%2520methods%2520and%2520cross-crop%2520training%2520methods%2520for%2520the%2520same%250Apests%252C%2520are%2520effective.%2520Our%2520two-stage%2520plant%2520pest%2520identification%2520framework%252C%250Aenabling%2520ROI%2520detection%2520and%2520convolutional%2520neural%2520network%2520%2528CNN%2529-based%250Aidentification%252C%2520achieved%2520a%2520highly%2520practical%2520performance%2520of%252091.0%2525%2520and%252088.5%2525%2520in%250Amean%2520accuracy%2520and%2520macro%2520F1%2520score%252C%2520respectively%252C%2520for%252012%252C223%2520instances%2520of%2520test%250Adata%2520of%252021%2520classes%2520collected%2520from%2520unseen%2520fields%252C%2520where%252025%2520classes%2520of%2520images%250Afrom%2520318%252C971%2520samples%2520were%2520used%2520for%2520training%253B%2520the%2520average%2520identification%2520time%250Awas%2520476%2520ms/image.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigation%20to%20answer%20three%20key%20questions%20concerning%20plant%20pest%0A%20%20identification%20and%20development%20of%20a%20practical%20identification%20framework&entry.906535625=Ryosuke%20Wayama%20and%20Yuki%20Sasaki%20and%20Satoshi%20Kagiwada%20and%20Nobusuke%20Iwasaki%20and%20Hitoshi%20Iyatomi&entry.1292438233=%20%20The%20development%20of%20practical%20and%20robust%20automated%20diagnostic%20systems%20for%0Aidentifying%20plant%20pests%20is%20crucial%20for%20efficient%20agricultural%20production.%20In%0Athis%20paper%2C%20we%20first%20investigate%20three%20key%20research%20questions%20%28RQs%29%20that%20have%0Anot%20been%20addressed%20thus%20far%20in%20the%20field%20of%20image-based%20plant%20pest%0Aidentification.%20Based%20on%20the%20knowledge%20gained%2C%20we%20then%20develop%20an%20accurate%2C%0Arobust%2C%20and%20fast%20plant%20pest%20identification%20framework%20using%20334K%20images%0Acomprising%2078%20combinations%20of%20four%20plant%20portions%20%28the%20leaf%20front%2C%20leaf%20back%2C%0Afruit%2C%20and%20flower%20of%20cucumber%2C%20tomato%2C%20strawberry%2C%20and%20eggplant%29%20and%2020%20pest%0Aspecies%20captured%20at%2027%20farms.%20The%20results%20reveal%20the%20following.%20%281%29%20For%20an%0Aappropriate%20evaluation%20of%20the%20model%2C%20the%20test%20data%20should%20not%20include%20images%20of%0Athe%20field%20from%20which%20the%20training%20images%20were%20collected%2C%20or%20other%0Aconsiderations%20to%20increase%20the%20diversity%20of%20the%20test%20set%20should%20be%20taken%20into%0Aaccount.%20%282%29%20Pre-extraction%20of%20ROIs%2C%20such%20as%20leaves%20and%20fruits%2C%20helps%20to%0Aimprove%20identification%20accuracy.%20%283%29%20Integration%20of%20closely%20related%20species%0Ausing%20the%20same%20control%20methods%20and%20cross-crop%20training%20methods%20for%20the%20same%0Apests%2C%20are%20effective.%20Our%20two-stage%20plant%20pest%20identification%20framework%2C%0Aenabling%20ROI%20detection%20and%20convolutional%20neural%20network%20%28CNN%29-based%0Aidentification%2C%20achieved%20a%20highly%20practical%20performance%20of%2091.0%25%20and%2088.5%25%20in%0Amean%20accuracy%20and%20macro%20F1%20score%2C%20respectively%2C%20for%2012%2C223%20instances%20of%20test%0Adata%20of%2021%20classes%20collected%20from%20unseen%20fields%2C%20where%2025%20classes%20of%20images%0Afrom%20318%2C971%20samples%20were%20used%20for%20training%3B%20the%20average%20identification%20time%0Awas%20476%20ms/image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18000v1&entry.124074799=Read"},
{"title": "LoRA-Pro: Are Low-Rank Adapters Properly Optimized?", "author": "Zhengbo Wang and Jian Liang", "abstract": "  Low-Rank Adaptation, also known as LoRA, has emerged as a prominent method\nfor parameter-efficient fine-tuning foundation models by re-parameterizing the\noriginal matrix into the product of two low-rank matrices. Despite its\nefficiency, LoRA often yields inferior performance compared to full\nfine-tuning. In this paper, we propose LoRA-Pro to bridge this performance gap.\nFirstly, we delve into the optimization processes in LoRA and full fine-tuning.\nWe reveal that while LoRA employs low-rank approximation, it neglects to\napproximate the optimization process of full fine-tuning. To address this, we\nintroduce a novel concept called the \"equivalent gradient.\" This virtual\ngradient makes the optimization process on the re-parameterized matrix\nequivalent to LoRA, which can be used to quantify the differences between LoRA\nand full fine-tuning. The equivalent gradient is derived from the gradients of\nmatrices $A$ and $B$. To narrow the performance gap, our approach minimizes the\ndifferences between the equivalent gradient and the gradient obtained from full\nfine-tuning during the optimization process. By solving this objective, we\nderive optimal closed-form solutions for updating matrices $A$ and $B$. Our\nmethod constrains the optimization process, shrinking the performance gap\nbetween LoRA and full fine-tuning. Extensive experiments on natural language\nprocessing tasks validate the effectiveness of our method.\n", "link": "http://arxiv.org/abs/2407.18242v1", "date": "2024-07-25", "relevancy": 1.8233, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.463}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4534}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoRA-Pro%3A%20Are%20Low-Rank%20Adapters%20Properly%20Optimized%3F&body=Title%3A%20LoRA-Pro%3A%20Are%20Low-Rank%20Adapters%20Properly%20Optimized%3F%0AAuthor%3A%20Zhengbo%20Wang%20and%20Jian%20Liang%0AAbstract%3A%20%20%20Low-Rank%20Adaptation%2C%20also%20known%20as%20LoRA%2C%20has%20emerged%20as%20a%20prominent%20method%0Afor%20parameter-efficient%20fine-tuning%20foundation%20models%20by%20re-parameterizing%20the%0Aoriginal%20matrix%20into%20the%20product%20of%20two%20low-rank%20matrices.%20Despite%20its%0Aefficiency%2C%20LoRA%20often%20yields%20inferior%20performance%20compared%20to%20full%0Afine-tuning.%20In%20this%20paper%2C%20we%20propose%20LoRA-Pro%20to%20bridge%20this%20performance%20gap.%0AFirstly%2C%20we%20delve%20into%20the%20optimization%20processes%20in%20LoRA%20and%20full%20fine-tuning.%0AWe%20reveal%20that%20while%20LoRA%20employs%20low-rank%20approximation%2C%20it%20neglects%20to%0Aapproximate%20the%20optimization%20process%20of%20full%20fine-tuning.%20To%20address%20this%2C%20we%0Aintroduce%20a%20novel%20concept%20called%20the%20%22equivalent%20gradient.%22%20This%20virtual%0Agradient%20makes%20the%20optimization%20process%20on%20the%20re-parameterized%20matrix%0Aequivalent%20to%20LoRA%2C%20which%20can%20be%20used%20to%20quantify%20the%20differences%20between%20LoRA%0Aand%20full%20fine-tuning.%20The%20equivalent%20gradient%20is%20derived%20from%20the%20gradients%20of%0Amatrices%20%24A%24%20and%20%24B%24.%20To%20narrow%20the%20performance%20gap%2C%20our%20approach%20minimizes%20the%0Adifferences%20between%20the%20equivalent%20gradient%20and%20the%20gradient%20obtained%20from%20full%0Afine-tuning%20during%20the%20optimization%20process.%20By%20solving%20this%20objective%2C%20we%0Aderive%20optimal%20closed-form%20solutions%20for%20updating%20matrices%20%24A%24%20and%20%24B%24.%20Our%0Amethod%20constrains%20the%20optimization%20process%2C%20shrinking%20the%20performance%20gap%0Abetween%20LoRA%20and%20full%20fine-tuning.%20Extensive%20experiments%20on%20natural%20language%0Aprocessing%20tasks%20validate%20the%20effectiveness%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoRA-Pro%253A%2520Are%2520Low-Rank%2520Adapters%2520Properly%2520Optimized%253F%26entry.906535625%3DZhengbo%2520Wang%2520and%2520Jian%2520Liang%26entry.1292438233%3D%2520%2520Low-Rank%2520Adaptation%252C%2520also%2520known%2520as%2520LoRA%252C%2520has%2520emerged%2520as%2520a%2520prominent%2520method%250Afor%2520parameter-efficient%2520fine-tuning%2520foundation%2520models%2520by%2520re-parameterizing%2520the%250Aoriginal%2520matrix%2520into%2520the%2520product%2520of%2520two%2520low-rank%2520matrices.%2520Despite%2520its%250Aefficiency%252C%2520LoRA%2520often%2520yields%2520inferior%2520performance%2520compared%2520to%2520full%250Afine-tuning.%2520In%2520this%2520paper%252C%2520we%2520propose%2520LoRA-Pro%2520to%2520bridge%2520this%2520performance%2520gap.%250AFirstly%252C%2520we%2520delve%2520into%2520the%2520optimization%2520processes%2520in%2520LoRA%2520and%2520full%2520fine-tuning.%250AWe%2520reveal%2520that%2520while%2520LoRA%2520employs%2520low-rank%2520approximation%252C%2520it%2520neglects%2520to%250Aapproximate%2520the%2520optimization%2520process%2520of%2520full%2520fine-tuning.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520a%2520novel%2520concept%2520called%2520the%2520%2522equivalent%2520gradient.%2522%2520This%2520virtual%250Agradient%2520makes%2520the%2520optimization%2520process%2520on%2520the%2520re-parameterized%2520matrix%250Aequivalent%2520to%2520LoRA%252C%2520which%2520can%2520be%2520used%2520to%2520quantify%2520the%2520differences%2520between%2520LoRA%250Aand%2520full%2520fine-tuning.%2520The%2520equivalent%2520gradient%2520is%2520derived%2520from%2520the%2520gradients%2520of%250Amatrices%2520%2524A%2524%2520and%2520%2524B%2524.%2520To%2520narrow%2520the%2520performance%2520gap%252C%2520our%2520approach%2520minimizes%2520the%250Adifferences%2520between%2520the%2520equivalent%2520gradient%2520and%2520the%2520gradient%2520obtained%2520from%2520full%250Afine-tuning%2520during%2520the%2520optimization%2520process.%2520By%2520solving%2520this%2520objective%252C%2520we%250Aderive%2520optimal%2520closed-form%2520solutions%2520for%2520updating%2520matrices%2520%2524A%2524%2520and%2520%2524B%2524.%2520Our%250Amethod%2520constrains%2520the%2520optimization%2520process%252C%2520shrinking%2520the%2520performance%2520gap%250Abetween%2520LoRA%2520and%2520full%2520fine-tuning.%2520Extensive%2520experiments%2520on%2520natural%2520language%250Aprocessing%2520tasks%2520validate%2520the%2520effectiveness%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRA-Pro%3A%20Are%20Low-Rank%20Adapters%20Properly%20Optimized%3F&entry.906535625=Zhengbo%20Wang%20and%20Jian%20Liang&entry.1292438233=%20%20Low-Rank%20Adaptation%2C%20also%20known%20as%20LoRA%2C%20has%20emerged%20as%20a%20prominent%20method%0Afor%20parameter-efficient%20fine-tuning%20foundation%20models%20by%20re-parameterizing%20the%0Aoriginal%20matrix%20into%20the%20product%20of%20two%20low-rank%20matrices.%20Despite%20its%0Aefficiency%2C%20LoRA%20often%20yields%20inferior%20performance%20compared%20to%20full%0Afine-tuning.%20In%20this%20paper%2C%20we%20propose%20LoRA-Pro%20to%20bridge%20this%20performance%20gap.%0AFirstly%2C%20we%20delve%20into%20the%20optimization%20processes%20in%20LoRA%20and%20full%20fine-tuning.%0AWe%20reveal%20that%20while%20LoRA%20employs%20low-rank%20approximation%2C%20it%20neglects%20to%0Aapproximate%20the%20optimization%20process%20of%20full%20fine-tuning.%20To%20address%20this%2C%20we%0Aintroduce%20a%20novel%20concept%20called%20the%20%22equivalent%20gradient.%22%20This%20virtual%0Agradient%20makes%20the%20optimization%20process%20on%20the%20re-parameterized%20matrix%0Aequivalent%20to%20LoRA%2C%20which%20can%20be%20used%20to%20quantify%20the%20differences%20between%20LoRA%0Aand%20full%20fine-tuning.%20The%20equivalent%20gradient%20is%20derived%20from%20the%20gradients%20of%0Amatrices%20%24A%24%20and%20%24B%24.%20To%20narrow%20the%20performance%20gap%2C%20our%20approach%20minimizes%20the%0Adifferences%20between%20the%20equivalent%20gradient%20and%20the%20gradient%20obtained%20from%20full%0Afine-tuning%20during%20the%20optimization%20process.%20By%20solving%20this%20objective%2C%20we%0Aderive%20optimal%20closed-form%20solutions%20for%20updating%20matrices%20%24A%24%20and%20%24B%24.%20Our%0Amethod%20constrains%20the%20optimization%20process%2C%20shrinking%20the%20performance%20gap%0Abetween%20LoRA%20and%20full%20fine-tuning.%20Extensive%20experiments%20on%20natural%20language%0Aprocessing%20tasks%20validate%20the%20effectiveness%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18242v1&entry.124074799=Read"},
{"title": "Towards Interpretable Visuo-Tactile Predictive Models for Soft Robot\n  Interactions", "author": "Enrico Donato and Thomas George Thuruthel and Egidio Falotico", "abstract": "  Autonomous systems face the intricate challenge of navigating unpredictable\nenvironments and interacting with external objects. The successful integration\nof robotic agents into real-world situations hinges on their perception\ncapabilities, which involve amalgamating world models and predictive skills.\nEffective perception models build upon the fusion of various sensory modalities\nto probe the surroundings. Deep learning applied to raw sensory modalities\noffers a viable option. However, learning-based perceptive representations\nbecome difficult to interpret. This challenge is particularly pronounced in\nsoft robots, where the compliance of structures and materials makes prediction\neven harder. Our work addresses this complexity by harnessing a generative\nmodel to construct a multi-modal perception model for soft robots and to\nleverage proprioceptive and visual information to anticipate and interpret\ncontact interactions with external objects. A suite of tools to interpret the\nperception model is furnished, shedding light on the fusion and prediction\nprocesses across multiple sensory inputs after the learning phase. We will\ndelve into the outlooks of the perception model and its implications for\ncontrol purposes.\n", "link": "http://arxiv.org/abs/2407.12197v2", "date": "2024-07-25", "relevancy": 1.8147, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6876}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5916}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Interpretable%20Visuo-Tactile%20Predictive%20Models%20for%20Soft%20Robot%0A%20%20Interactions&body=Title%3A%20Towards%20Interpretable%20Visuo-Tactile%20Predictive%20Models%20for%20Soft%20Robot%0A%20%20Interactions%0AAuthor%3A%20Enrico%20Donato%20and%20Thomas%20George%20Thuruthel%20and%20Egidio%20Falotico%0AAbstract%3A%20%20%20Autonomous%20systems%20face%20the%20intricate%20challenge%20of%20navigating%20unpredictable%0Aenvironments%20and%20interacting%20with%20external%20objects.%20The%20successful%20integration%0Aof%20robotic%20agents%20into%20real-world%20situations%20hinges%20on%20their%20perception%0Acapabilities%2C%20which%20involve%20amalgamating%20world%20models%20and%20predictive%20skills.%0AEffective%20perception%20models%20build%20upon%20the%20fusion%20of%20various%20sensory%20modalities%0Ato%20probe%20the%20surroundings.%20Deep%20learning%20applied%20to%20raw%20sensory%20modalities%0Aoffers%20a%20viable%20option.%20However%2C%20learning-based%20perceptive%20representations%0Abecome%20difficult%20to%20interpret.%20This%20challenge%20is%20particularly%20pronounced%20in%0Asoft%20robots%2C%20where%20the%20compliance%20of%20structures%20and%20materials%20makes%20prediction%0Aeven%20harder.%20Our%20work%20addresses%20this%20complexity%20by%20harnessing%20a%20generative%0Amodel%20to%20construct%20a%20multi-modal%20perception%20model%20for%20soft%20robots%20and%20to%0Aleverage%20proprioceptive%20and%20visual%20information%20to%20anticipate%20and%20interpret%0Acontact%20interactions%20with%20external%20objects.%20A%20suite%20of%20tools%20to%20interpret%20the%0Aperception%20model%20is%20furnished%2C%20shedding%20light%20on%20the%20fusion%20and%20prediction%0Aprocesses%20across%20multiple%20sensory%20inputs%20after%20the%20learning%20phase.%20We%20will%0Adelve%20into%20the%20outlooks%20of%20the%20perception%20model%20and%20its%20implications%20for%0Acontrol%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12197v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Interpretable%2520Visuo-Tactile%2520Predictive%2520Models%2520for%2520Soft%2520Robot%250A%2520%2520Interactions%26entry.906535625%3DEnrico%2520Donato%2520and%2520Thomas%2520George%2520Thuruthel%2520and%2520Egidio%2520Falotico%26entry.1292438233%3D%2520%2520Autonomous%2520systems%2520face%2520the%2520intricate%2520challenge%2520of%2520navigating%2520unpredictable%250Aenvironments%2520and%2520interacting%2520with%2520external%2520objects.%2520The%2520successful%2520integration%250Aof%2520robotic%2520agents%2520into%2520real-world%2520situations%2520hinges%2520on%2520their%2520perception%250Acapabilities%252C%2520which%2520involve%2520amalgamating%2520world%2520models%2520and%2520predictive%2520skills.%250AEffective%2520perception%2520models%2520build%2520upon%2520the%2520fusion%2520of%2520various%2520sensory%2520modalities%250Ato%2520probe%2520the%2520surroundings.%2520Deep%2520learning%2520applied%2520to%2520raw%2520sensory%2520modalities%250Aoffers%2520a%2520viable%2520option.%2520However%252C%2520learning-based%2520perceptive%2520representations%250Abecome%2520difficult%2520to%2520interpret.%2520This%2520challenge%2520is%2520particularly%2520pronounced%2520in%250Asoft%2520robots%252C%2520where%2520the%2520compliance%2520of%2520structures%2520and%2520materials%2520makes%2520prediction%250Aeven%2520harder.%2520Our%2520work%2520addresses%2520this%2520complexity%2520by%2520harnessing%2520a%2520generative%250Amodel%2520to%2520construct%2520a%2520multi-modal%2520perception%2520model%2520for%2520soft%2520robots%2520and%2520to%250Aleverage%2520proprioceptive%2520and%2520visual%2520information%2520to%2520anticipate%2520and%2520interpret%250Acontact%2520interactions%2520with%2520external%2520objects.%2520A%2520suite%2520of%2520tools%2520to%2520interpret%2520the%250Aperception%2520model%2520is%2520furnished%252C%2520shedding%2520light%2520on%2520the%2520fusion%2520and%2520prediction%250Aprocesses%2520across%2520multiple%2520sensory%2520inputs%2520after%2520the%2520learning%2520phase.%2520We%2520will%250Adelve%2520into%2520the%2520outlooks%2520of%2520the%2520perception%2520model%2520and%2520its%2520implications%2520for%250Acontrol%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12197v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Interpretable%20Visuo-Tactile%20Predictive%20Models%20for%20Soft%20Robot%0A%20%20Interactions&entry.906535625=Enrico%20Donato%20and%20Thomas%20George%20Thuruthel%20and%20Egidio%20Falotico&entry.1292438233=%20%20Autonomous%20systems%20face%20the%20intricate%20challenge%20of%20navigating%20unpredictable%0Aenvironments%20and%20interacting%20with%20external%20objects.%20The%20successful%20integration%0Aof%20robotic%20agents%20into%20real-world%20situations%20hinges%20on%20their%20perception%0Acapabilities%2C%20which%20involve%20amalgamating%20world%20models%20and%20predictive%20skills.%0AEffective%20perception%20models%20build%20upon%20the%20fusion%20of%20various%20sensory%20modalities%0Ato%20probe%20the%20surroundings.%20Deep%20learning%20applied%20to%20raw%20sensory%20modalities%0Aoffers%20a%20viable%20option.%20However%2C%20learning-based%20perceptive%20representations%0Abecome%20difficult%20to%20interpret.%20This%20challenge%20is%20particularly%20pronounced%20in%0Asoft%20robots%2C%20where%20the%20compliance%20of%20structures%20and%20materials%20makes%20prediction%0Aeven%20harder.%20Our%20work%20addresses%20this%20complexity%20by%20harnessing%20a%20generative%0Amodel%20to%20construct%20a%20multi-modal%20perception%20model%20for%20soft%20robots%20and%20to%0Aleverage%20proprioceptive%20and%20visual%20information%20to%20anticipate%20and%20interpret%0Acontact%20interactions%20with%20external%20objects.%20A%20suite%20of%20tools%20to%20interpret%20the%0Aperception%20model%20is%20furnished%2C%20shedding%20light%20on%20the%20fusion%20and%20prediction%0Aprocesses%20across%20multiple%20sensory%20inputs%20after%20the%20learning%20phase.%20We%20will%0Adelve%20into%20the%20outlooks%20of%20the%20perception%20model%20and%20its%20implications%20for%0Acontrol%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12197v2&entry.124074799=Read"},
{"title": "On the Effect of Purely Synthetic Training Data for Different Automatic\n  Speech Recognition Architectures", "author": "Nick Rossenbach and Benedikt Hilmes and Ralf Schl\u00fcter", "abstract": "  In this work we evaluate the utility of synthetic data for training automatic\nspeech recognition (ASR). We use the ASR training data to train a\ntext-to-speech (TTS) system similar to FastSpeech-2. With this TTS we reproduce\nthe original training data, training ASR systems solely on synthetic data. For\nASR, we use three different architectures, attention-based encoder-decoder,\nhybrid deep neural network hidden Markov model and a Gaussian mixture hidden\nMarkov model, showing the different sensitivity of the models to synthetic data\ngeneration. In order to extend previous work, we present a number of ablation\nstudies on the effectiveness of synthetic vs. real training data for ASR. In\nparticular we focus on how the gap between training on synthetic and real data\nchanges by varying the speaker embedding or by scaling the model size. For the\nlatter we show that the TTS models generalize well, even when training scores\nindicate overfitting.\n", "link": "http://arxiv.org/abs/2407.17997v1", "date": "2024-07-25", "relevancy": 1.8135, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5063}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4463}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Effect%20of%20Purely%20Synthetic%20Training%20Data%20for%20Different%20Automatic%0A%20%20Speech%20Recognition%20Architectures&body=Title%3A%20On%20the%20Effect%20of%20Purely%20Synthetic%20Training%20Data%20for%20Different%20Automatic%0A%20%20Speech%20Recognition%20Architectures%0AAuthor%3A%20Nick%20Rossenbach%20and%20Benedikt%20Hilmes%20and%20Ralf%20Schl%C3%BCter%0AAbstract%3A%20%20%20In%20this%20work%20we%20evaluate%20the%20utility%20of%20synthetic%20data%20for%20training%20automatic%0Aspeech%20recognition%20%28ASR%29.%20We%20use%20the%20ASR%20training%20data%20to%20train%20a%0Atext-to-speech%20%28TTS%29%20system%20similar%20to%20FastSpeech-2.%20With%20this%20TTS%20we%20reproduce%0Athe%20original%20training%20data%2C%20training%20ASR%20systems%20solely%20on%20synthetic%20data.%20For%0AASR%2C%20we%20use%20three%20different%20architectures%2C%20attention-based%20encoder-decoder%2C%0Ahybrid%20deep%20neural%20network%20hidden%20Markov%20model%20and%20a%20Gaussian%20mixture%20hidden%0AMarkov%20model%2C%20showing%20the%20different%20sensitivity%20of%20the%20models%20to%20synthetic%20data%0Ageneration.%20In%20order%20to%20extend%20previous%20work%2C%20we%20present%20a%20number%20of%20ablation%0Astudies%20on%20the%20effectiveness%20of%20synthetic%20vs.%20real%20training%20data%20for%20ASR.%20In%0Aparticular%20we%20focus%20on%20how%20the%20gap%20between%20training%20on%20synthetic%20and%20real%20data%0Achanges%20by%20varying%20the%20speaker%20embedding%20or%20by%20scaling%20the%20model%20size.%20For%20the%0Alatter%20we%20show%20that%20the%20TTS%20models%20generalize%20well%2C%20even%20when%20training%20scores%0Aindicate%20overfitting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Effect%2520of%2520Purely%2520Synthetic%2520Training%2520Data%2520for%2520Different%2520Automatic%250A%2520%2520Speech%2520Recognition%2520Architectures%26entry.906535625%3DNick%2520Rossenbach%2520and%2520Benedikt%2520Hilmes%2520and%2520Ralf%2520Schl%25C3%25BCter%26entry.1292438233%3D%2520%2520In%2520this%2520work%2520we%2520evaluate%2520the%2520utility%2520of%2520synthetic%2520data%2520for%2520training%2520automatic%250Aspeech%2520recognition%2520%2528ASR%2529.%2520We%2520use%2520the%2520ASR%2520training%2520data%2520to%2520train%2520a%250Atext-to-speech%2520%2528TTS%2529%2520system%2520similar%2520to%2520FastSpeech-2.%2520With%2520this%2520TTS%2520we%2520reproduce%250Athe%2520original%2520training%2520data%252C%2520training%2520ASR%2520systems%2520solely%2520on%2520synthetic%2520data.%2520For%250AASR%252C%2520we%2520use%2520three%2520different%2520architectures%252C%2520attention-based%2520encoder-decoder%252C%250Ahybrid%2520deep%2520neural%2520network%2520hidden%2520Markov%2520model%2520and%2520a%2520Gaussian%2520mixture%2520hidden%250AMarkov%2520model%252C%2520showing%2520the%2520different%2520sensitivity%2520of%2520the%2520models%2520to%2520synthetic%2520data%250Ageneration.%2520In%2520order%2520to%2520extend%2520previous%2520work%252C%2520we%2520present%2520a%2520number%2520of%2520ablation%250Astudies%2520on%2520the%2520effectiveness%2520of%2520synthetic%2520vs.%2520real%2520training%2520data%2520for%2520ASR.%2520In%250Aparticular%2520we%2520focus%2520on%2520how%2520the%2520gap%2520between%2520training%2520on%2520synthetic%2520and%2520real%2520data%250Achanges%2520by%2520varying%2520the%2520speaker%2520embedding%2520or%2520by%2520scaling%2520the%2520model%2520size.%2520For%2520the%250Alatter%2520we%2520show%2520that%2520the%2520TTS%2520models%2520generalize%2520well%252C%2520even%2520when%2520training%2520scores%250Aindicate%2520overfitting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Effect%20of%20Purely%20Synthetic%20Training%20Data%20for%20Different%20Automatic%0A%20%20Speech%20Recognition%20Architectures&entry.906535625=Nick%20Rossenbach%20and%20Benedikt%20Hilmes%20and%20Ralf%20Schl%C3%BCter&entry.1292438233=%20%20In%20this%20work%20we%20evaluate%20the%20utility%20of%20synthetic%20data%20for%20training%20automatic%0Aspeech%20recognition%20%28ASR%29.%20We%20use%20the%20ASR%20training%20data%20to%20train%20a%0Atext-to-speech%20%28TTS%29%20system%20similar%20to%20FastSpeech-2.%20With%20this%20TTS%20we%20reproduce%0Athe%20original%20training%20data%2C%20training%20ASR%20systems%20solely%20on%20synthetic%20data.%20For%0AASR%2C%20we%20use%20three%20different%20architectures%2C%20attention-based%20encoder-decoder%2C%0Ahybrid%20deep%20neural%20network%20hidden%20Markov%20model%20and%20a%20Gaussian%20mixture%20hidden%0AMarkov%20model%2C%20showing%20the%20different%20sensitivity%20of%20the%20models%20to%20synthetic%20data%0Ageneration.%20In%20order%20to%20extend%20previous%20work%2C%20we%20present%20a%20number%20of%20ablation%0Astudies%20on%20the%20effectiveness%20of%20synthetic%20vs.%20real%20training%20data%20for%20ASR.%20In%0Aparticular%20we%20focus%20on%20how%20the%20gap%20between%20training%20on%20synthetic%20and%20real%20data%0Achanges%20by%20varying%20the%20speaker%20embedding%20or%20by%20scaling%20the%20model%20size.%20For%20the%0Alatter%20we%20show%20that%20the%20TTS%20models%20generalize%20well%2C%20even%20when%20training%20scores%0Aindicate%20overfitting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17997v1&entry.124074799=Read"},
{"title": "Harmonic LLMs are Trustworthy", "author": "Nicholas S. Kersting and Mohammad Rahman and Suchismitha Vedala and Yang Wang", "abstract": "  We introduce an intuitive method to test the robustness (stability and\nexplainability) of any black-box LLM in real-time via its local deviation from\nharmoniticity, denoted as $\\gamma$. To the best of our knowledge this is the\nfirst completely model-agnostic and unsupervised method of measuring the\nrobustness of any given response from an LLM, based upon the model itself\nconforming to a purely mathematical standard. To show general application and\nimmediacy of results, we measure $\\gamma$ in 10 popular LLMs (ChatGPT,\nClaude-2.1, Claude3.0, GPT-4, GPT-4o, Smaug-72B, Mixtral-8x7B, Llama2-7B,\nMistral-7B and MPT-7B) across thousands of queries in three objective domains:\nWebQA, ProgrammingQA, and TruthfulQA. Across all models and domains tested,\nhuman annotation confirms that $\\gamma \\to 0$ indicates trustworthiness, and\nconversely searching higher values of $\\gamma$ easily exposes examples of\nhallucination, a fact that enables efficient adversarial prompt generation\nthrough stochastic gradient ascent in $\\gamma$. The low-$\\gamma$ leaders among\nthe models in the respective domains are GPT-4o, GPT-4, and Smaug-72B,\nproviding evidence that mid-size open-source models can win out against large\ncommercial models.\n", "link": "http://arxiv.org/abs/2404.19708v2", "date": "2024-07-25", "relevancy": 1.81, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4891}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.448}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harmonic%20LLMs%20are%20Trustworthy&body=Title%3A%20Harmonic%20LLMs%20are%20Trustworthy%0AAuthor%3A%20Nicholas%20S.%20Kersting%20and%20Mohammad%20Rahman%20and%20Suchismitha%20Vedala%20and%20Yang%20Wang%0AAbstract%3A%20%20%20We%20introduce%20an%20intuitive%20method%20to%20test%20the%20robustness%20%28stability%20and%0Aexplainability%29%20of%20any%20black-box%20LLM%20in%20real-time%20via%20its%20local%20deviation%20from%0Aharmoniticity%2C%20denoted%20as%20%24%5Cgamma%24.%20To%20the%20best%20of%20our%20knowledge%20this%20is%20the%0Afirst%20completely%20model-agnostic%20and%20unsupervised%20method%20of%20measuring%20the%0Arobustness%20of%20any%20given%20response%20from%20an%20LLM%2C%20based%20upon%20the%20model%20itself%0Aconforming%20to%20a%20purely%20mathematical%20standard.%20To%20show%20general%20application%20and%0Aimmediacy%20of%20results%2C%20we%20measure%20%24%5Cgamma%24%20in%2010%20popular%20LLMs%20%28ChatGPT%2C%0AClaude-2.1%2C%20Claude3.0%2C%20GPT-4%2C%20GPT-4o%2C%20Smaug-72B%2C%20Mixtral-8x7B%2C%20Llama2-7B%2C%0AMistral-7B%20and%20MPT-7B%29%20across%20thousands%20of%20queries%20in%20three%20objective%20domains%3A%0AWebQA%2C%20ProgrammingQA%2C%20and%20TruthfulQA.%20Across%20all%20models%20and%20domains%20tested%2C%0Ahuman%20annotation%20confirms%20that%20%24%5Cgamma%20%5Cto%200%24%20indicates%20trustworthiness%2C%20and%0Aconversely%20searching%20higher%20values%20of%20%24%5Cgamma%24%20easily%20exposes%20examples%20of%0Ahallucination%2C%20a%20fact%20that%20enables%20efficient%20adversarial%20prompt%20generation%0Athrough%20stochastic%20gradient%20ascent%20in%20%24%5Cgamma%24.%20The%20low-%24%5Cgamma%24%20leaders%20among%0Athe%20models%20in%20the%20respective%20domains%20are%20GPT-4o%2C%20GPT-4%2C%20and%20Smaug-72B%2C%0Aproviding%20evidence%20that%20mid-size%20open-source%20models%20can%20win%20out%20against%20large%0Acommercial%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19708v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarmonic%2520LLMs%2520are%2520Trustworthy%26entry.906535625%3DNicholas%2520S.%2520Kersting%2520and%2520Mohammad%2520Rahman%2520and%2520Suchismitha%2520Vedala%2520and%2520Yang%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520an%2520intuitive%2520method%2520to%2520test%2520the%2520robustness%2520%2528stability%2520and%250Aexplainability%2529%2520of%2520any%2520black-box%2520LLM%2520in%2520real-time%2520via%2520its%2520local%2520deviation%2520from%250Aharmoniticity%252C%2520denoted%2520as%2520%2524%255Cgamma%2524.%2520To%2520the%2520best%2520of%2520our%2520knowledge%2520this%2520is%2520the%250Afirst%2520completely%2520model-agnostic%2520and%2520unsupervised%2520method%2520of%2520measuring%2520the%250Arobustness%2520of%2520any%2520given%2520response%2520from%2520an%2520LLM%252C%2520based%2520upon%2520the%2520model%2520itself%250Aconforming%2520to%2520a%2520purely%2520mathematical%2520standard.%2520To%2520show%2520general%2520application%2520and%250Aimmediacy%2520of%2520results%252C%2520we%2520measure%2520%2524%255Cgamma%2524%2520in%252010%2520popular%2520LLMs%2520%2528ChatGPT%252C%250AClaude-2.1%252C%2520Claude3.0%252C%2520GPT-4%252C%2520GPT-4o%252C%2520Smaug-72B%252C%2520Mixtral-8x7B%252C%2520Llama2-7B%252C%250AMistral-7B%2520and%2520MPT-7B%2529%2520across%2520thousands%2520of%2520queries%2520in%2520three%2520objective%2520domains%253A%250AWebQA%252C%2520ProgrammingQA%252C%2520and%2520TruthfulQA.%2520Across%2520all%2520models%2520and%2520domains%2520tested%252C%250Ahuman%2520annotation%2520confirms%2520that%2520%2524%255Cgamma%2520%255Cto%25200%2524%2520indicates%2520trustworthiness%252C%2520and%250Aconversely%2520searching%2520higher%2520values%2520of%2520%2524%255Cgamma%2524%2520easily%2520exposes%2520examples%2520of%250Ahallucination%252C%2520a%2520fact%2520that%2520enables%2520efficient%2520adversarial%2520prompt%2520generation%250Athrough%2520stochastic%2520gradient%2520ascent%2520in%2520%2524%255Cgamma%2524.%2520The%2520low-%2524%255Cgamma%2524%2520leaders%2520among%250Athe%2520models%2520in%2520the%2520respective%2520domains%2520are%2520GPT-4o%252C%2520GPT-4%252C%2520and%2520Smaug-72B%252C%250Aproviding%2520evidence%2520that%2520mid-size%2520open-source%2520models%2520can%2520win%2520out%2520against%2520large%250Acommercial%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19708v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harmonic%20LLMs%20are%20Trustworthy&entry.906535625=Nicholas%20S.%20Kersting%20and%20Mohammad%20Rahman%20and%20Suchismitha%20Vedala%20and%20Yang%20Wang&entry.1292438233=%20%20We%20introduce%20an%20intuitive%20method%20to%20test%20the%20robustness%20%28stability%20and%0Aexplainability%29%20of%20any%20black-box%20LLM%20in%20real-time%20via%20its%20local%20deviation%20from%0Aharmoniticity%2C%20denoted%20as%20%24%5Cgamma%24.%20To%20the%20best%20of%20our%20knowledge%20this%20is%20the%0Afirst%20completely%20model-agnostic%20and%20unsupervised%20method%20of%20measuring%20the%0Arobustness%20of%20any%20given%20response%20from%20an%20LLM%2C%20based%20upon%20the%20model%20itself%0Aconforming%20to%20a%20purely%20mathematical%20standard.%20To%20show%20general%20application%20and%0Aimmediacy%20of%20results%2C%20we%20measure%20%24%5Cgamma%24%20in%2010%20popular%20LLMs%20%28ChatGPT%2C%0AClaude-2.1%2C%20Claude3.0%2C%20GPT-4%2C%20GPT-4o%2C%20Smaug-72B%2C%20Mixtral-8x7B%2C%20Llama2-7B%2C%0AMistral-7B%20and%20MPT-7B%29%20across%20thousands%20of%20queries%20in%20three%20objective%20domains%3A%0AWebQA%2C%20ProgrammingQA%2C%20and%20TruthfulQA.%20Across%20all%20models%20and%20domains%20tested%2C%0Ahuman%20annotation%20confirms%20that%20%24%5Cgamma%20%5Cto%200%24%20indicates%20trustworthiness%2C%20and%0Aconversely%20searching%20higher%20values%20of%20%24%5Cgamma%24%20easily%20exposes%20examples%20of%0Ahallucination%2C%20a%20fact%20that%20enables%20efficient%20adversarial%20prompt%20generation%0Athrough%20stochastic%20gradient%20ascent%20in%20%24%5Cgamma%24.%20The%20low-%24%5Cgamma%24%20leaders%20among%0Athe%20models%20in%20the%20respective%20domains%20are%20GPT-4o%2C%20GPT-4%2C%20and%20Smaug-72B%2C%0Aproviding%20evidence%20that%20mid-size%20open-source%20models%20can%20win%20out%20against%20large%0Acommercial%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19708v2&entry.124074799=Read"},
{"title": "MapTune: Advancing ASIC Technology Mapping via Reinforcement Learning\n  Guided Library Tuning", "author": "Mingju Liu and Daniel Robinson and Yingjie Li and Cunxi Yu", "abstract": "  Technology mapping involves mapping logical circuits to a library of cells.\nTraditionally, the full technology library is used, leading to a large search\nspace and potential overhead. Motivated by randomly sampled technology mapping\ncase studies, we propose MapTune framework that addresses this challenge by\nutilizing reinforcement learning to make design-specific choices during cell\nselection. By learning from the environment, MapTune refines the cell selection\nprocess, resulting in a reduced search space and potentially improved mapping\nquality.\n  The effectiveness of MapTune is evaluated on a wide range of benchmarks,\ndifferent technology libraries and technology mappers. The experimental results\ndemonstrate that MapTune achieves higher mapping accuracy and reducing\ndelay/area across diverse circuit designs, technology libraries and mappers.\nThe paper also discusses the Pareto-Optimal exploration and confirms the\nperpetual delay-area trade-off. Conducted on benchmark suites ISCAS 85/89,\nITC/ISCAS 99, VTR8.0 and EPFL benchmarks, the post-technology mapping and\npost-sizing quality-of-results (QoR) have been significantly improved, with\naverage Area-Delay Product (ADP) improvement of 22.54\\% among all different\nexploration settings in MapTune. The improvements are consistently remained for\nfour different technologies (7nm, 45nm, 130nm, and 180 nm) and two different\nmappers.\n", "link": "http://arxiv.org/abs/2407.18110v1", "date": "2024-07-25", "relevancy": 1.8016, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4691}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4376}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MapTune%3A%20Advancing%20ASIC%20Technology%20Mapping%20via%20Reinforcement%20Learning%0A%20%20Guided%20Library%20Tuning&body=Title%3A%20MapTune%3A%20Advancing%20ASIC%20Technology%20Mapping%20via%20Reinforcement%20Learning%0A%20%20Guided%20Library%20Tuning%0AAuthor%3A%20Mingju%20Liu%20and%20Daniel%20Robinson%20and%20Yingjie%20Li%20and%20Cunxi%20Yu%0AAbstract%3A%20%20%20Technology%20mapping%20involves%20mapping%20logical%20circuits%20to%20a%20library%20of%20cells.%0ATraditionally%2C%20the%20full%20technology%20library%20is%20used%2C%20leading%20to%20a%20large%20search%0Aspace%20and%20potential%20overhead.%20Motivated%20by%20randomly%20sampled%20technology%20mapping%0Acase%20studies%2C%20we%20propose%20MapTune%20framework%20that%20addresses%20this%20challenge%20by%0Autilizing%20reinforcement%20learning%20to%20make%20design-specific%20choices%20during%20cell%0Aselection.%20By%20learning%20from%20the%20environment%2C%20MapTune%20refines%20the%20cell%20selection%0Aprocess%2C%20resulting%20in%20a%20reduced%20search%20space%20and%20potentially%20improved%20mapping%0Aquality.%0A%20%20The%20effectiveness%20of%20MapTune%20is%20evaluated%20on%20a%20wide%20range%20of%20benchmarks%2C%0Adifferent%20technology%20libraries%20and%20technology%20mappers.%20The%20experimental%20results%0Ademonstrate%20that%20MapTune%20achieves%20higher%20mapping%20accuracy%20and%20reducing%0Adelay/area%20across%20diverse%20circuit%20designs%2C%20technology%20libraries%20and%20mappers.%0AThe%20paper%20also%20discusses%20the%20Pareto-Optimal%20exploration%20and%20confirms%20the%0Aperpetual%20delay-area%20trade-off.%20Conducted%20on%20benchmark%20suites%20ISCAS%2085/89%2C%0AITC/ISCAS%2099%2C%20VTR8.0%20and%20EPFL%20benchmarks%2C%20the%20post-technology%20mapping%20and%0Apost-sizing%20quality-of-results%20%28QoR%29%20have%20been%20significantly%20improved%2C%20with%0Aaverage%20Area-Delay%20Product%20%28ADP%29%20improvement%20of%2022.54%5C%25%20among%20all%20different%0Aexploration%20settings%20in%20MapTune.%20The%20improvements%20are%20consistently%20remained%20for%0Afour%20different%20technologies%20%287nm%2C%2045nm%2C%20130nm%2C%20and%20180%20nm%29%20and%20two%20different%0Amappers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18110v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapTune%253A%2520Advancing%2520ASIC%2520Technology%2520Mapping%2520via%2520Reinforcement%2520Learning%250A%2520%2520Guided%2520Library%2520Tuning%26entry.906535625%3DMingju%2520Liu%2520and%2520Daniel%2520Robinson%2520and%2520Yingjie%2520Li%2520and%2520Cunxi%2520Yu%26entry.1292438233%3D%2520%2520Technology%2520mapping%2520involves%2520mapping%2520logical%2520circuits%2520to%2520a%2520library%2520of%2520cells.%250ATraditionally%252C%2520the%2520full%2520technology%2520library%2520is%2520used%252C%2520leading%2520to%2520a%2520large%2520search%250Aspace%2520and%2520potential%2520overhead.%2520Motivated%2520by%2520randomly%2520sampled%2520technology%2520mapping%250Acase%2520studies%252C%2520we%2520propose%2520MapTune%2520framework%2520that%2520addresses%2520this%2520challenge%2520by%250Autilizing%2520reinforcement%2520learning%2520to%2520make%2520design-specific%2520choices%2520during%2520cell%250Aselection.%2520By%2520learning%2520from%2520the%2520environment%252C%2520MapTune%2520refines%2520the%2520cell%2520selection%250Aprocess%252C%2520resulting%2520in%2520a%2520reduced%2520search%2520space%2520and%2520potentially%2520improved%2520mapping%250Aquality.%250A%2520%2520The%2520effectiveness%2520of%2520MapTune%2520is%2520evaluated%2520on%2520a%2520wide%2520range%2520of%2520benchmarks%252C%250Adifferent%2520technology%2520libraries%2520and%2520technology%2520mappers.%2520The%2520experimental%2520results%250Ademonstrate%2520that%2520MapTune%2520achieves%2520higher%2520mapping%2520accuracy%2520and%2520reducing%250Adelay/area%2520across%2520diverse%2520circuit%2520designs%252C%2520technology%2520libraries%2520and%2520mappers.%250AThe%2520paper%2520also%2520discusses%2520the%2520Pareto-Optimal%2520exploration%2520and%2520confirms%2520the%250Aperpetual%2520delay-area%2520trade-off.%2520Conducted%2520on%2520benchmark%2520suites%2520ISCAS%252085/89%252C%250AITC/ISCAS%252099%252C%2520VTR8.0%2520and%2520EPFL%2520benchmarks%252C%2520the%2520post-technology%2520mapping%2520and%250Apost-sizing%2520quality-of-results%2520%2528QoR%2529%2520have%2520been%2520significantly%2520improved%252C%2520with%250Aaverage%2520Area-Delay%2520Product%2520%2528ADP%2529%2520improvement%2520of%252022.54%255C%2525%2520among%2520all%2520different%250Aexploration%2520settings%2520in%2520MapTune.%2520The%2520improvements%2520are%2520consistently%2520remained%2520for%250Afour%2520different%2520technologies%2520%25287nm%252C%252045nm%252C%2520130nm%252C%2520and%2520180%2520nm%2529%2520and%2520two%2520different%250Amappers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18110v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MapTune%3A%20Advancing%20ASIC%20Technology%20Mapping%20via%20Reinforcement%20Learning%0A%20%20Guided%20Library%20Tuning&entry.906535625=Mingju%20Liu%20and%20Daniel%20Robinson%20and%20Yingjie%20Li%20and%20Cunxi%20Yu&entry.1292438233=%20%20Technology%20mapping%20involves%20mapping%20logical%20circuits%20to%20a%20library%20of%20cells.%0ATraditionally%2C%20the%20full%20technology%20library%20is%20used%2C%20leading%20to%20a%20large%20search%0Aspace%20and%20potential%20overhead.%20Motivated%20by%20randomly%20sampled%20technology%20mapping%0Acase%20studies%2C%20we%20propose%20MapTune%20framework%20that%20addresses%20this%20challenge%20by%0Autilizing%20reinforcement%20learning%20to%20make%20design-specific%20choices%20during%20cell%0Aselection.%20By%20learning%20from%20the%20environment%2C%20MapTune%20refines%20the%20cell%20selection%0Aprocess%2C%20resulting%20in%20a%20reduced%20search%20space%20and%20potentially%20improved%20mapping%0Aquality.%0A%20%20The%20effectiveness%20of%20MapTune%20is%20evaluated%20on%20a%20wide%20range%20of%20benchmarks%2C%0Adifferent%20technology%20libraries%20and%20technology%20mappers.%20The%20experimental%20results%0Ademonstrate%20that%20MapTune%20achieves%20higher%20mapping%20accuracy%20and%20reducing%0Adelay/area%20across%20diverse%20circuit%20designs%2C%20technology%20libraries%20and%20mappers.%0AThe%20paper%20also%20discusses%20the%20Pareto-Optimal%20exploration%20and%20confirms%20the%0Aperpetual%20delay-area%20trade-off.%20Conducted%20on%20benchmark%20suites%20ISCAS%2085/89%2C%0AITC/ISCAS%2099%2C%20VTR8.0%20and%20EPFL%20benchmarks%2C%20the%20post-technology%20mapping%20and%0Apost-sizing%20quality-of-results%20%28QoR%29%20have%20been%20significantly%20improved%2C%20with%0Aaverage%20Area-Delay%20Product%20%28ADP%29%20improvement%20of%2022.54%5C%25%20among%20all%20different%0Aexploration%20settings%20in%20MapTune.%20The%20improvements%20are%20consistently%20remained%20for%0Afour%20different%20technologies%20%287nm%2C%2045nm%2C%20130nm%2C%20and%20180%20nm%29%20and%20two%20different%0Amappers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18110v1&entry.124074799=Read"},
{"title": "No Representation, No Trust: Connecting Representation, Collapse, and\n  Trust Issues in PPO", "author": "Skander Moalla and Andrea Miele and Razvan Pascanu and Caglar Gulcehre", "abstract": "  Reinforcement learning (RL) is inherently rife with non-stationarity since\nthe states and rewards the agent observes during training depend on its\nchanging policy. Therefore, networks in deep RL must be capable of adapting to\nnew observations and fitting new targets. However, previous works have observed\nthat networks in off-policy deep value-based methods exhibit a decrease in\nrepresentation rank, often correlated with an inability to continue learning or\na collapse in performance. Although this phenomenon has generally been\nattributed to neural network learning under non-stationarity, it has been\noverlooked in on-policy policy optimization methods which are often thought\ncapable of training indefinitely. In this work, we empirically study\nrepresentation dynamics in Proximal Policy Optimization (PPO) on the Atari and\nMuJoCo environments, revealing that PPO agents are also affected by feature\nrank deterioration and loss of plasticity. We show that this is aggravated with\nstronger non-stationarity, ultimately driving the actor's performance to\ncollapse, regardless of the performance of the critic. We ask why the trust\nregion, specific to methods like PPO, cannot alleviate or prevent the collapse.\nWe find that there is a connection between representation collapse and the\ndegradation of the trust region, one exacerbating the other, and present\nProximal Feature Optimization (PFO), a novel auxiliary loss that, along with\nother interventions, shows that regularizing the representation dynamics\nimproves the performance of PPO agents.\n", "link": "http://arxiv.org/abs/2405.00662v2", "date": "2024-07-25", "relevancy": 1.7984, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4587}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4533}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20Representation%2C%20No%20Trust%3A%20Connecting%20Representation%2C%20Collapse%2C%20and%0A%20%20Trust%20Issues%20in%20PPO&body=Title%3A%20No%20Representation%2C%20No%20Trust%3A%20Connecting%20Representation%2C%20Collapse%2C%20and%0A%20%20Trust%20Issues%20in%20PPO%0AAuthor%3A%20Skander%20Moalla%20and%20Andrea%20Miele%20and%20Razvan%20Pascanu%20and%20Caglar%20Gulcehre%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20is%20inherently%20rife%20with%20non-stationarity%20since%0Athe%20states%20and%20rewards%20the%20agent%20observes%20during%20training%20depend%20on%20its%0Achanging%20policy.%20Therefore%2C%20networks%20in%20deep%20RL%20must%20be%20capable%20of%20adapting%20to%0Anew%20observations%20and%20fitting%20new%20targets.%20However%2C%20previous%20works%20have%20observed%0Athat%20networks%20in%20off-policy%20deep%20value-based%20methods%20exhibit%20a%20decrease%20in%0Arepresentation%20rank%2C%20often%20correlated%20with%20an%20inability%20to%20continue%20learning%20or%0Aa%20collapse%20in%20performance.%20Although%20this%20phenomenon%20has%20generally%20been%0Aattributed%20to%20neural%20network%20learning%20under%20non-stationarity%2C%20it%20has%20been%0Aoverlooked%20in%20on-policy%20policy%20optimization%20methods%20which%20are%20often%20thought%0Acapable%20of%20training%20indefinitely.%20In%20this%20work%2C%20we%20empirically%20study%0Arepresentation%20dynamics%20in%20Proximal%20Policy%20Optimization%20%28PPO%29%20on%20the%20Atari%20and%0AMuJoCo%20environments%2C%20revealing%20that%20PPO%20agents%20are%20also%20affected%20by%20feature%0Arank%20deterioration%20and%20loss%20of%20plasticity.%20We%20show%20that%20this%20is%20aggravated%20with%0Astronger%20non-stationarity%2C%20ultimately%20driving%20the%20actor%27s%20performance%20to%0Acollapse%2C%20regardless%20of%20the%20performance%20of%20the%20critic.%20We%20ask%20why%20the%20trust%0Aregion%2C%20specific%20to%20methods%20like%20PPO%2C%20cannot%20alleviate%20or%20prevent%20the%20collapse.%0AWe%20find%20that%20there%20is%20a%20connection%20between%20representation%20collapse%20and%20the%0Adegradation%20of%20the%20trust%20region%2C%20one%20exacerbating%20the%20other%2C%20and%20present%0AProximal%20Feature%20Optimization%20%28PFO%29%2C%20a%20novel%20auxiliary%20loss%20that%2C%20along%20with%0Aother%20interventions%2C%20shows%20that%20regularizing%20the%20representation%20dynamics%0Aimproves%20the%20performance%20of%20PPO%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00662v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520Representation%252C%2520No%2520Trust%253A%2520Connecting%2520Representation%252C%2520Collapse%252C%2520and%250A%2520%2520Trust%2520Issues%2520in%2520PPO%26entry.906535625%3DSkander%2520Moalla%2520and%2520Andrea%2520Miele%2520and%2520Razvan%2520Pascanu%2520and%2520Caglar%2520Gulcehre%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520is%2520inherently%2520rife%2520with%2520non-stationarity%2520since%250Athe%2520states%2520and%2520rewards%2520the%2520agent%2520observes%2520during%2520training%2520depend%2520on%2520its%250Achanging%2520policy.%2520Therefore%252C%2520networks%2520in%2520deep%2520RL%2520must%2520be%2520capable%2520of%2520adapting%2520to%250Anew%2520observations%2520and%2520fitting%2520new%2520targets.%2520However%252C%2520previous%2520works%2520have%2520observed%250Athat%2520networks%2520in%2520off-policy%2520deep%2520value-based%2520methods%2520exhibit%2520a%2520decrease%2520in%250Arepresentation%2520rank%252C%2520often%2520correlated%2520with%2520an%2520inability%2520to%2520continue%2520learning%2520or%250Aa%2520collapse%2520in%2520performance.%2520Although%2520this%2520phenomenon%2520has%2520generally%2520been%250Aattributed%2520to%2520neural%2520network%2520learning%2520under%2520non-stationarity%252C%2520it%2520has%2520been%250Aoverlooked%2520in%2520on-policy%2520policy%2520optimization%2520methods%2520which%2520are%2520often%2520thought%250Acapable%2520of%2520training%2520indefinitely.%2520In%2520this%2520work%252C%2520we%2520empirically%2520study%250Arepresentation%2520dynamics%2520in%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529%2520on%2520the%2520Atari%2520and%250AMuJoCo%2520environments%252C%2520revealing%2520that%2520PPO%2520agents%2520are%2520also%2520affected%2520by%2520feature%250Arank%2520deterioration%2520and%2520loss%2520of%2520plasticity.%2520We%2520show%2520that%2520this%2520is%2520aggravated%2520with%250Astronger%2520non-stationarity%252C%2520ultimately%2520driving%2520the%2520actor%2527s%2520performance%2520to%250Acollapse%252C%2520regardless%2520of%2520the%2520performance%2520of%2520the%2520critic.%2520We%2520ask%2520why%2520the%2520trust%250Aregion%252C%2520specific%2520to%2520methods%2520like%2520PPO%252C%2520cannot%2520alleviate%2520or%2520prevent%2520the%2520collapse.%250AWe%2520find%2520that%2520there%2520is%2520a%2520connection%2520between%2520representation%2520collapse%2520and%2520the%250Adegradation%2520of%2520the%2520trust%2520region%252C%2520one%2520exacerbating%2520the%2520other%252C%2520and%2520present%250AProximal%2520Feature%2520Optimization%2520%2528PFO%2529%252C%2520a%2520novel%2520auxiliary%2520loss%2520that%252C%2520along%2520with%250Aother%2520interventions%252C%2520shows%2520that%2520regularizing%2520the%2520representation%2520dynamics%250Aimproves%2520the%2520performance%2520of%2520PPO%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00662v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20Representation%2C%20No%20Trust%3A%20Connecting%20Representation%2C%20Collapse%2C%20and%0A%20%20Trust%20Issues%20in%20PPO&entry.906535625=Skander%20Moalla%20and%20Andrea%20Miele%20and%20Razvan%20Pascanu%20and%20Caglar%20Gulcehre&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20is%20inherently%20rife%20with%20non-stationarity%20since%0Athe%20states%20and%20rewards%20the%20agent%20observes%20during%20training%20depend%20on%20its%0Achanging%20policy.%20Therefore%2C%20networks%20in%20deep%20RL%20must%20be%20capable%20of%20adapting%20to%0Anew%20observations%20and%20fitting%20new%20targets.%20However%2C%20previous%20works%20have%20observed%0Athat%20networks%20in%20off-policy%20deep%20value-based%20methods%20exhibit%20a%20decrease%20in%0Arepresentation%20rank%2C%20often%20correlated%20with%20an%20inability%20to%20continue%20learning%20or%0Aa%20collapse%20in%20performance.%20Although%20this%20phenomenon%20has%20generally%20been%0Aattributed%20to%20neural%20network%20learning%20under%20non-stationarity%2C%20it%20has%20been%0Aoverlooked%20in%20on-policy%20policy%20optimization%20methods%20which%20are%20often%20thought%0Acapable%20of%20training%20indefinitely.%20In%20this%20work%2C%20we%20empirically%20study%0Arepresentation%20dynamics%20in%20Proximal%20Policy%20Optimization%20%28PPO%29%20on%20the%20Atari%20and%0AMuJoCo%20environments%2C%20revealing%20that%20PPO%20agents%20are%20also%20affected%20by%20feature%0Arank%20deterioration%20and%20loss%20of%20plasticity.%20We%20show%20that%20this%20is%20aggravated%20with%0Astronger%20non-stationarity%2C%20ultimately%20driving%20the%20actor%27s%20performance%20to%0Acollapse%2C%20regardless%20of%20the%20performance%20of%20the%20critic.%20We%20ask%20why%20the%20trust%0Aregion%2C%20specific%20to%20methods%20like%20PPO%2C%20cannot%20alleviate%20or%20prevent%20the%20collapse.%0AWe%20find%20that%20there%20is%20a%20connection%20between%20representation%20collapse%20and%20the%0Adegradation%20of%20the%20trust%20region%2C%20one%20exacerbating%20the%20other%2C%20and%20present%0AProximal%20Feature%20Optimization%20%28PFO%29%2C%20a%20novel%20auxiliary%20loss%20that%2C%20along%20with%0Aother%20interventions%2C%20shows%20that%20regularizing%20the%20representation%20dynamics%0Aimproves%20the%20performance%20of%20PPO%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00662v2&entry.124074799=Read"},
{"title": "Lightweight Language-driven Grasp Detection using Conditional\n  Consistency Model", "author": "Nghia Nguyen and Minh Nhat Vu and Baoru Huang and An Vuong and Ngan Le and Thieu Vo and Anh Nguyen", "abstract": "  Language-driven grasp detection is a fundamental yet challenging task in\nrobotics with various industrial applications. In this work, we present a new\napproach for language-driven grasp detection that leverages the concept of\nlightweight diffusion models to achieve fast inference time. By integrating\ndiffusion processes with grasping prompts in natural language, our method can\neffectively encode visual and textual information, enabling more accurate and\nversatile grasp positioning that aligns well with the text query. To overcome\nthe long inference time problem in diffusion models, we leverage the image and\ntext features as the condition in the consistency model to reduce the number of\ndenoising timesteps during inference. The intensive experimental results show\nthat our method outperforms other recent grasp detection methods and\nlightweight diffusion models by a clear margin. We further validate our method\nin real-world robotic experiments to demonstrate its fast inference time\ncapability.\n", "link": "http://arxiv.org/abs/2407.17967v1", "date": "2024-07-25", "relevancy": 1.7912, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6128}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5953}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20Language-driven%20Grasp%20Detection%20using%20Conditional%0A%20%20Consistency%20Model&body=Title%3A%20Lightweight%20Language-driven%20Grasp%20Detection%20using%20Conditional%0A%20%20Consistency%20Model%0AAuthor%3A%20Nghia%20Nguyen%20and%20Minh%20Nhat%20Vu%20and%20Baoru%20Huang%20and%20An%20Vuong%20and%20Ngan%20Le%20and%20Thieu%20Vo%20and%20Anh%20Nguyen%0AAbstract%3A%20%20%20Language-driven%20grasp%20detection%20is%20a%20fundamental%20yet%20challenging%20task%20in%0Arobotics%20with%20various%20industrial%20applications.%20In%20this%20work%2C%20we%20present%20a%20new%0Aapproach%20for%20language-driven%20grasp%20detection%20that%20leverages%20the%20concept%20of%0Alightweight%20diffusion%20models%20to%20achieve%20fast%20inference%20time.%20By%20integrating%0Adiffusion%20processes%20with%20grasping%20prompts%20in%20natural%20language%2C%20our%20method%20can%0Aeffectively%20encode%20visual%20and%20textual%20information%2C%20enabling%20more%20accurate%20and%0Aversatile%20grasp%20positioning%20that%20aligns%20well%20with%20the%20text%20query.%20To%20overcome%0Athe%20long%20inference%20time%20problem%20in%20diffusion%20models%2C%20we%20leverage%20the%20image%20and%0Atext%20features%20as%20the%20condition%20in%20the%20consistency%20model%20to%20reduce%20the%20number%20of%0Adenoising%20timesteps%20during%20inference.%20The%20intensive%20experimental%20results%20show%0Athat%20our%20method%20outperforms%20other%20recent%20grasp%20detection%20methods%20and%0Alightweight%20diffusion%20models%20by%20a%20clear%20margin.%20We%20further%20validate%20our%20method%0Ain%20real-world%20robotic%20experiments%20to%20demonstrate%20its%20fast%20inference%20time%0Acapability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520Language-driven%2520Grasp%2520Detection%2520using%2520Conditional%250A%2520%2520Consistency%2520Model%26entry.906535625%3DNghia%2520Nguyen%2520and%2520Minh%2520Nhat%2520Vu%2520and%2520Baoru%2520Huang%2520and%2520An%2520Vuong%2520and%2520Ngan%2520Le%2520and%2520Thieu%2520Vo%2520and%2520Anh%2520Nguyen%26entry.1292438233%3D%2520%2520Language-driven%2520grasp%2520detection%2520is%2520a%2520fundamental%2520yet%2520challenging%2520task%2520in%250Arobotics%2520with%2520various%2520industrial%2520applications.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520new%250Aapproach%2520for%2520language-driven%2520grasp%2520detection%2520that%2520leverages%2520the%2520concept%2520of%250Alightweight%2520diffusion%2520models%2520to%2520achieve%2520fast%2520inference%2520time.%2520By%2520integrating%250Adiffusion%2520processes%2520with%2520grasping%2520prompts%2520in%2520natural%2520language%252C%2520our%2520method%2520can%250Aeffectively%2520encode%2520visual%2520and%2520textual%2520information%252C%2520enabling%2520more%2520accurate%2520and%250Aversatile%2520grasp%2520positioning%2520that%2520aligns%2520well%2520with%2520the%2520text%2520query.%2520To%2520overcome%250Athe%2520long%2520inference%2520time%2520problem%2520in%2520diffusion%2520models%252C%2520we%2520leverage%2520the%2520image%2520and%250Atext%2520features%2520as%2520the%2520condition%2520in%2520the%2520consistency%2520model%2520to%2520reduce%2520the%2520number%2520of%250Adenoising%2520timesteps%2520during%2520inference.%2520The%2520intensive%2520experimental%2520results%2520show%250Athat%2520our%2520method%2520outperforms%2520other%2520recent%2520grasp%2520detection%2520methods%2520and%250Alightweight%2520diffusion%2520models%2520by%2520a%2520clear%2520margin.%2520We%2520further%2520validate%2520our%2520method%250Ain%2520real-world%2520robotic%2520experiments%2520to%2520demonstrate%2520its%2520fast%2520inference%2520time%250Acapability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20Language-driven%20Grasp%20Detection%20using%20Conditional%0A%20%20Consistency%20Model&entry.906535625=Nghia%20Nguyen%20and%20Minh%20Nhat%20Vu%20and%20Baoru%20Huang%20and%20An%20Vuong%20and%20Ngan%20Le%20and%20Thieu%20Vo%20and%20Anh%20Nguyen&entry.1292438233=%20%20Language-driven%20grasp%20detection%20is%20a%20fundamental%20yet%20challenging%20task%20in%0Arobotics%20with%20various%20industrial%20applications.%20In%20this%20work%2C%20we%20present%20a%20new%0Aapproach%20for%20language-driven%20grasp%20detection%20that%20leverages%20the%20concept%20of%0Alightweight%20diffusion%20models%20to%20achieve%20fast%20inference%20time.%20By%20integrating%0Adiffusion%20processes%20with%20grasping%20prompts%20in%20natural%20language%2C%20our%20method%20can%0Aeffectively%20encode%20visual%20and%20textual%20information%2C%20enabling%20more%20accurate%20and%0Aversatile%20grasp%20positioning%20that%20aligns%20well%20with%20the%20text%20query.%20To%20overcome%0Athe%20long%20inference%20time%20problem%20in%20diffusion%20models%2C%20we%20leverage%20the%20image%20and%0Atext%20features%20as%20the%20condition%20in%20the%20consistency%20model%20to%20reduce%20the%20number%20of%0Adenoising%20timesteps%20during%20inference.%20The%20intensive%20experimental%20results%20show%0Athat%20our%20method%20outperforms%20other%20recent%20grasp%20detection%20methods%20and%0Alightweight%20diffusion%20models%20by%20a%20clear%20margin.%20We%20further%20validate%20our%20method%0Ain%20real-world%20robotic%20experiments%20to%20demonstrate%20its%20fast%20inference%20time%0Acapability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17967v1&entry.124074799=Read"},
{"title": "Language-Driven 6-DoF Grasp Detection Using Negative Prompt Guidance", "author": "Toan Nguyen and Minh Nhat Vu and Baoru Huang and An Vuong and Quan Vuong and Ngan Le and Thieu Vo and Anh Nguyen", "abstract": "  6-DoF grasp detection has been a fundamental and challenging problem in\nrobotic vision. While previous works have focused on ensuring grasp stability,\nthey often do not consider human intention conveyed through natural language,\nhindering effective collaboration between robots and users in complex 3D\nenvironments. In this paper, we present a new approach for language-driven\n6-DoF grasp detection in cluttered point clouds. We first introduce\nGrasp-Anything-6D, a large-scale dataset for the language-driven 6-DoF grasp\ndetection task with 1M point cloud scenes and more than 200M\nlanguage-associated 3D grasp poses. We further introduce a novel diffusion\nmodel that incorporates a new negative prompt guidance learning strategy. The\nproposed negative prompt strategy directs the detection process toward the\ndesired object while steering away from unwanted ones given the language input.\nOur method enables an end-to-end framework where humans can command the robot\nto grasp desired objects in a cluttered scene using natural language. Intensive\nexperimental results show the effectiveness of our method in both benchmarking\nexperiments and real-world scenarios, surpassing other baselines. In addition,\nwe demonstrate the practicality of our approach in real-world robotic\napplications. Our project is available at\nhttps://airvlab.github.io/grasp-anything.\n", "link": "http://arxiv.org/abs/2407.13842v2", "date": "2024-07-25", "relevancy": 1.7897, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6251}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5638}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-Driven%206-DoF%20Grasp%20Detection%20Using%20Negative%20Prompt%20Guidance&body=Title%3A%20Language-Driven%206-DoF%20Grasp%20Detection%20Using%20Negative%20Prompt%20Guidance%0AAuthor%3A%20Toan%20Nguyen%20and%20Minh%20Nhat%20Vu%20and%20Baoru%20Huang%20and%20An%20Vuong%20and%20Quan%20Vuong%20and%20Ngan%20Le%20and%20Thieu%20Vo%20and%20Anh%20Nguyen%0AAbstract%3A%20%20%206-DoF%20grasp%20detection%20has%20been%20a%20fundamental%20and%20challenging%20problem%20in%0Arobotic%20vision.%20While%20previous%20works%20have%20focused%20on%20ensuring%20grasp%20stability%2C%0Athey%20often%20do%20not%20consider%20human%20intention%20conveyed%20through%20natural%20language%2C%0Ahindering%20effective%20collaboration%20between%20robots%20and%20users%20in%20complex%203D%0Aenvironments.%20In%20this%20paper%2C%20we%20present%20a%20new%20approach%20for%20language-driven%0A6-DoF%20grasp%20detection%20in%20cluttered%20point%20clouds.%20We%20first%20introduce%0AGrasp-Anything-6D%2C%20a%20large-scale%20dataset%20for%20the%20language-driven%206-DoF%20grasp%0Adetection%20task%20with%201M%20point%20cloud%20scenes%20and%20more%20than%20200M%0Alanguage-associated%203D%20grasp%20poses.%20We%20further%20introduce%20a%20novel%20diffusion%0Amodel%20that%20incorporates%20a%20new%20negative%20prompt%20guidance%20learning%20strategy.%20The%0Aproposed%20negative%20prompt%20strategy%20directs%20the%20detection%20process%20toward%20the%0Adesired%20object%20while%20steering%20away%20from%20unwanted%20ones%20given%20the%20language%20input.%0AOur%20method%20enables%20an%20end-to-end%20framework%20where%20humans%20can%20command%20the%20robot%0Ato%20grasp%20desired%20objects%20in%20a%20cluttered%20scene%20using%20natural%20language.%20Intensive%0Aexperimental%20results%20show%20the%20effectiveness%20of%20our%20method%20in%20both%20benchmarking%0Aexperiments%20and%20real-world%20scenarios%2C%20surpassing%20other%20baselines.%20In%20addition%2C%0Awe%20demonstrate%20the%20practicality%20of%20our%20approach%20in%20real-world%20robotic%0Aapplications.%20Our%20project%20is%20available%20at%0Ahttps%3A//airvlab.github.io/grasp-anything.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.13842v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-Driven%25206-DoF%2520Grasp%2520Detection%2520Using%2520Negative%2520Prompt%2520Guidance%26entry.906535625%3DToan%2520Nguyen%2520and%2520Minh%2520Nhat%2520Vu%2520and%2520Baoru%2520Huang%2520and%2520An%2520Vuong%2520and%2520Quan%2520Vuong%2520and%2520Ngan%2520Le%2520and%2520Thieu%2520Vo%2520and%2520Anh%2520Nguyen%26entry.1292438233%3D%2520%25206-DoF%2520grasp%2520detection%2520has%2520been%2520a%2520fundamental%2520and%2520challenging%2520problem%2520in%250Arobotic%2520vision.%2520While%2520previous%2520works%2520have%2520focused%2520on%2520ensuring%2520grasp%2520stability%252C%250Athey%2520often%2520do%2520not%2520consider%2520human%2520intention%2520conveyed%2520through%2520natural%2520language%252C%250Ahindering%2520effective%2520collaboration%2520between%2520robots%2520and%2520users%2520in%2520complex%25203D%250Aenvironments.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520new%2520approach%2520for%2520language-driven%250A6-DoF%2520grasp%2520detection%2520in%2520cluttered%2520point%2520clouds.%2520We%2520first%2520introduce%250AGrasp-Anything-6D%252C%2520a%2520large-scale%2520dataset%2520for%2520the%2520language-driven%25206-DoF%2520grasp%250Adetection%2520task%2520with%25201M%2520point%2520cloud%2520scenes%2520and%2520more%2520than%2520200M%250Alanguage-associated%25203D%2520grasp%2520poses.%2520We%2520further%2520introduce%2520a%2520novel%2520diffusion%250Amodel%2520that%2520incorporates%2520a%2520new%2520negative%2520prompt%2520guidance%2520learning%2520strategy.%2520The%250Aproposed%2520negative%2520prompt%2520strategy%2520directs%2520the%2520detection%2520process%2520toward%2520the%250Adesired%2520object%2520while%2520steering%2520away%2520from%2520unwanted%2520ones%2520given%2520the%2520language%2520input.%250AOur%2520method%2520enables%2520an%2520end-to-end%2520framework%2520where%2520humans%2520can%2520command%2520the%2520robot%250Ato%2520grasp%2520desired%2520objects%2520in%2520a%2520cluttered%2520scene%2520using%2520natural%2520language.%2520Intensive%250Aexperimental%2520results%2520show%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520both%2520benchmarking%250Aexperiments%2520and%2520real-world%2520scenarios%252C%2520surpassing%2520other%2520baselines.%2520In%2520addition%252C%250Awe%2520demonstrate%2520the%2520practicality%2520of%2520our%2520approach%2520in%2520real-world%2520robotic%250Aapplications.%2520Our%2520project%2520is%2520available%2520at%250Ahttps%253A//airvlab.github.io/grasp-anything.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.13842v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Driven%206-DoF%20Grasp%20Detection%20Using%20Negative%20Prompt%20Guidance&entry.906535625=Toan%20Nguyen%20and%20Minh%20Nhat%20Vu%20and%20Baoru%20Huang%20and%20An%20Vuong%20and%20Quan%20Vuong%20and%20Ngan%20Le%20and%20Thieu%20Vo%20and%20Anh%20Nguyen&entry.1292438233=%20%206-DoF%20grasp%20detection%20has%20been%20a%20fundamental%20and%20challenging%20problem%20in%0Arobotic%20vision.%20While%20previous%20works%20have%20focused%20on%20ensuring%20grasp%20stability%2C%0Athey%20often%20do%20not%20consider%20human%20intention%20conveyed%20through%20natural%20language%2C%0Ahindering%20effective%20collaboration%20between%20robots%20and%20users%20in%20complex%203D%0Aenvironments.%20In%20this%20paper%2C%20we%20present%20a%20new%20approach%20for%20language-driven%0A6-DoF%20grasp%20detection%20in%20cluttered%20point%20clouds.%20We%20first%20introduce%0AGrasp-Anything-6D%2C%20a%20large-scale%20dataset%20for%20the%20language-driven%206-DoF%20grasp%0Adetection%20task%20with%201M%20point%20cloud%20scenes%20and%20more%20than%20200M%0Alanguage-associated%203D%20grasp%20poses.%20We%20further%20introduce%20a%20novel%20diffusion%0Amodel%20that%20incorporates%20a%20new%20negative%20prompt%20guidance%20learning%20strategy.%20The%0Aproposed%20negative%20prompt%20strategy%20directs%20the%20detection%20process%20toward%20the%0Adesired%20object%20while%20steering%20away%20from%20unwanted%20ones%20given%20the%20language%20input.%0AOur%20method%20enables%20an%20end-to-end%20framework%20where%20humans%20can%20command%20the%20robot%0Ato%20grasp%20desired%20objects%20in%20a%20cluttered%20scene%20using%20natural%20language.%20Intensive%0Aexperimental%20results%20show%20the%20effectiveness%20of%20our%20method%20in%20both%20benchmarking%0Aexperiments%20and%20real-world%20scenarios%2C%20surpassing%20other%20baselines.%20In%20addition%2C%0Awe%20demonstrate%20the%20practicality%20of%20our%20approach%20in%20real-world%20robotic%0Aapplications.%20Our%20project%20is%20available%20at%0Ahttps%3A//airvlab.github.io/grasp-anything.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.13842v2&entry.124074799=Read"},
{"title": "Normalised clustering accuracy: An asymmetric external cluster validity\n  measure", "author": "Marek Gagolewski", "abstract": "  There is no, nor will there ever be, single best clustering algorithm.\nNevertheless, we would still like to be able to distinguish between methods\nthat work well on certain task types and those that systematically\nunderperform. Clustering algorithms are traditionally evaluated using either\ninternal or external validity measures. Internal measures quantify different\naspects of the obtained partitions, e.g., the average degree of cluster\ncompactness or point separability. However, their validity is questionable\nbecause the clusterings they endorse can sometimes be meaningless. External\nmeasures, on the other hand, compare the algorithms' outputs to fixed ground\ntruth groupings provided by experts. In this paper, we argue that the commonly\nused classical partition similarity scores, such as the normalised mutual\ninformation, Fowlkes-Mallows, or adjusted Rand index, miss some desirable\nproperties. In particular, they do not identify worst-case scenarios correctly,\nnor are they easily interpretable. As a consequence, the evaluation of\nclustering algorithms on diverse benchmark datasets can be difficult. To remedy\nthese issues, we propose and analyse a new measure: a version of the optimal\nset-matching accuracy, which is normalised, monotonic with respect to some\nsimilarity relation, scale-invariant, and corrected for the imbalancedness of\ncluster sizes (but neither symmetric nor adjusted for chance).\n", "link": "http://arxiv.org/abs/2209.02935v4", "date": "2024-07-25", "relevancy": 1.7824, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3614}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3552}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Normalised%20clustering%20accuracy%3A%20An%20asymmetric%20external%20cluster%20validity%0A%20%20measure&body=Title%3A%20Normalised%20clustering%20accuracy%3A%20An%20asymmetric%20external%20cluster%20validity%0A%20%20measure%0AAuthor%3A%20Marek%20Gagolewski%0AAbstract%3A%20%20%20There%20is%20no%2C%20nor%20will%20there%20ever%20be%2C%20single%20best%20clustering%20algorithm.%0ANevertheless%2C%20we%20would%20still%20like%20to%20be%20able%20to%20distinguish%20between%20methods%0Athat%20work%20well%20on%20certain%20task%20types%20and%20those%20that%20systematically%0Aunderperform.%20Clustering%20algorithms%20are%20traditionally%20evaluated%20using%20either%0Ainternal%20or%20external%20validity%20measures.%20Internal%20measures%20quantify%20different%0Aaspects%20of%20the%20obtained%20partitions%2C%20e.g.%2C%20the%20average%20degree%20of%20cluster%0Acompactness%20or%20point%20separability.%20However%2C%20their%20validity%20is%20questionable%0Abecause%20the%20clusterings%20they%20endorse%20can%20sometimes%20be%20meaningless.%20External%0Ameasures%2C%20on%20the%20other%20hand%2C%20compare%20the%20algorithms%27%20outputs%20to%20fixed%20ground%0Atruth%20groupings%20provided%20by%20experts.%20In%20this%20paper%2C%20we%20argue%20that%20the%20commonly%0Aused%20classical%20partition%20similarity%20scores%2C%20such%20as%20the%20normalised%20mutual%0Ainformation%2C%20Fowlkes-Mallows%2C%20or%20adjusted%20Rand%20index%2C%20miss%20some%20desirable%0Aproperties.%20In%20particular%2C%20they%20do%20not%20identify%20worst-case%20scenarios%20correctly%2C%0Anor%20are%20they%20easily%20interpretable.%20As%20a%20consequence%2C%20the%20evaluation%20of%0Aclustering%20algorithms%20on%20diverse%20benchmark%20datasets%20can%20be%20difficult.%20To%20remedy%0Athese%20issues%2C%20we%20propose%20and%20analyse%20a%20new%20measure%3A%20a%20version%20of%20the%20optimal%0Aset-matching%20accuracy%2C%20which%20is%20normalised%2C%20monotonic%20with%20respect%20to%20some%0Asimilarity%20relation%2C%20scale-invariant%2C%20and%20corrected%20for%20the%20imbalancedness%20of%0Acluster%20sizes%20%28but%20neither%20symmetric%20nor%20adjusted%20for%20chance%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.02935v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNormalised%2520clustering%2520accuracy%253A%2520An%2520asymmetric%2520external%2520cluster%2520validity%250A%2520%2520measure%26entry.906535625%3DMarek%2520Gagolewski%26entry.1292438233%3D%2520%2520There%2520is%2520no%252C%2520nor%2520will%2520there%2520ever%2520be%252C%2520single%2520best%2520clustering%2520algorithm.%250ANevertheless%252C%2520we%2520would%2520still%2520like%2520to%2520be%2520able%2520to%2520distinguish%2520between%2520methods%250Athat%2520work%2520well%2520on%2520certain%2520task%2520types%2520and%2520those%2520that%2520systematically%250Aunderperform.%2520Clustering%2520algorithms%2520are%2520traditionally%2520evaluated%2520using%2520either%250Ainternal%2520or%2520external%2520validity%2520measures.%2520Internal%2520measures%2520quantify%2520different%250Aaspects%2520of%2520the%2520obtained%2520partitions%252C%2520e.g.%252C%2520the%2520average%2520degree%2520of%2520cluster%250Acompactness%2520or%2520point%2520separability.%2520However%252C%2520their%2520validity%2520is%2520questionable%250Abecause%2520the%2520clusterings%2520they%2520endorse%2520can%2520sometimes%2520be%2520meaningless.%2520External%250Ameasures%252C%2520on%2520the%2520other%2520hand%252C%2520compare%2520the%2520algorithms%2527%2520outputs%2520to%2520fixed%2520ground%250Atruth%2520groupings%2520provided%2520by%2520experts.%2520In%2520this%2520paper%252C%2520we%2520argue%2520that%2520the%2520commonly%250Aused%2520classical%2520partition%2520similarity%2520scores%252C%2520such%2520as%2520the%2520normalised%2520mutual%250Ainformation%252C%2520Fowlkes-Mallows%252C%2520or%2520adjusted%2520Rand%2520index%252C%2520miss%2520some%2520desirable%250Aproperties.%2520In%2520particular%252C%2520they%2520do%2520not%2520identify%2520worst-case%2520scenarios%2520correctly%252C%250Anor%2520are%2520they%2520easily%2520interpretable.%2520As%2520a%2520consequence%252C%2520the%2520evaluation%2520of%250Aclustering%2520algorithms%2520on%2520diverse%2520benchmark%2520datasets%2520can%2520be%2520difficult.%2520To%2520remedy%250Athese%2520issues%252C%2520we%2520propose%2520and%2520analyse%2520a%2520new%2520measure%253A%2520a%2520version%2520of%2520the%2520optimal%250Aset-matching%2520accuracy%252C%2520which%2520is%2520normalised%252C%2520monotonic%2520with%2520respect%2520to%2520some%250Asimilarity%2520relation%252C%2520scale-invariant%252C%2520and%2520corrected%2520for%2520the%2520imbalancedness%2520of%250Acluster%2520sizes%2520%2528but%2520neither%2520symmetric%2520nor%2520adjusted%2520for%2520chance%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.02935v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Normalised%20clustering%20accuracy%3A%20An%20asymmetric%20external%20cluster%20validity%0A%20%20measure&entry.906535625=Marek%20Gagolewski&entry.1292438233=%20%20There%20is%20no%2C%20nor%20will%20there%20ever%20be%2C%20single%20best%20clustering%20algorithm.%0ANevertheless%2C%20we%20would%20still%20like%20to%20be%20able%20to%20distinguish%20between%20methods%0Athat%20work%20well%20on%20certain%20task%20types%20and%20those%20that%20systematically%0Aunderperform.%20Clustering%20algorithms%20are%20traditionally%20evaluated%20using%20either%0Ainternal%20or%20external%20validity%20measures.%20Internal%20measures%20quantify%20different%0Aaspects%20of%20the%20obtained%20partitions%2C%20e.g.%2C%20the%20average%20degree%20of%20cluster%0Acompactness%20or%20point%20separability.%20However%2C%20their%20validity%20is%20questionable%0Abecause%20the%20clusterings%20they%20endorse%20can%20sometimes%20be%20meaningless.%20External%0Ameasures%2C%20on%20the%20other%20hand%2C%20compare%20the%20algorithms%27%20outputs%20to%20fixed%20ground%0Atruth%20groupings%20provided%20by%20experts.%20In%20this%20paper%2C%20we%20argue%20that%20the%20commonly%0Aused%20classical%20partition%20similarity%20scores%2C%20such%20as%20the%20normalised%20mutual%0Ainformation%2C%20Fowlkes-Mallows%2C%20or%20adjusted%20Rand%20index%2C%20miss%20some%20desirable%0Aproperties.%20In%20particular%2C%20they%20do%20not%20identify%20worst-case%20scenarios%20correctly%2C%0Anor%20are%20they%20easily%20interpretable.%20As%20a%20consequence%2C%20the%20evaluation%20of%0Aclustering%20algorithms%20on%20diverse%20benchmark%20datasets%20can%20be%20difficult.%20To%20remedy%0Athese%20issues%2C%20we%20propose%20and%20analyse%20a%20new%20measure%3A%20a%20version%20of%20the%20optimal%0Aset-matching%20accuracy%2C%20which%20is%20normalised%2C%20monotonic%20with%20respect%20to%20some%0Asimilarity%20relation%2C%20scale-invariant%2C%20and%20corrected%20for%20the%20imbalancedness%20of%0Acluster%20sizes%20%28but%20neither%20symmetric%20nor%20adjusted%20for%20chance%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.02935v4&entry.124074799=Read"},
{"title": "Can time series forecasting be automated? A benchmark and analysis", "author": "Anvitha Thirthapura Sreedhara and Joaquin Vanschoren", "abstract": "  In the field of machine learning and artificial intelligence, time series\nforecasting plays a pivotal role across various domains such as finance,\nhealthcare, and weather. However, the task of selecting the most suitable\nforecasting method for a given dataset is a complex task due to the diversity\nof data patterns and characteristics. This research aims to address this\nchallenge by proposing a comprehensive benchmark for evaluating and ranking\ntime series forecasting methods across a wide range of datasets. This study\ninvestigates the comparative performance of many methods from two prominent\ntime series forecasting frameworks, AutoGluon-Timeseries, and sktime to shed\nlight on their applicability in different real-world scenarios. This research\ncontributes to the field of time series forecasting by providing a robust\nbenchmarking methodology and facilitating informed decision-making when\nchoosing forecasting methods for achieving optimal prediction.\n", "link": "http://arxiv.org/abs/2407.16445v2", "date": "2024-07-25", "relevancy": 1.5724, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4085}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3984}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.3816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20time%20series%20forecasting%20be%20automated%3F%20A%20benchmark%20and%20analysis&body=Title%3A%20Can%20time%20series%20forecasting%20be%20automated%3F%20A%20benchmark%20and%20analysis%0AAuthor%3A%20Anvitha%20Thirthapura%20Sreedhara%20and%20Joaquin%20Vanschoren%0AAbstract%3A%20%20%20In%20the%20field%20of%20machine%20learning%20and%20artificial%20intelligence%2C%20time%20series%0Aforecasting%20plays%20a%20pivotal%20role%20across%20various%20domains%20such%20as%20finance%2C%0Ahealthcare%2C%20and%20weather.%20However%2C%20the%20task%20of%20selecting%20the%20most%20suitable%0Aforecasting%20method%20for%20a%20given%20dataset%20is%20a%20complex%20task%20due%20to%20the%20diversity%0Aof%20data%20patterns%20and%20characteristics.%20This%20research%20aims%20to%20address%20this%0Achallenge%20by%20proposing%20a%20comprehensive%20benchmark%20for%20evaluating%20and%20ranking%0Atime%20series%20forecasting%20methods%20across%20a%20wide%20range%20of%20datasets.%20This%20study%0Ainvestigates%20the%20comparative%20performance%20of%20many%20methods%20from%20two%20prominent%0Atime%20series%20forecasting%20frameworks%2C%20AutoGluon-Timeseries%2C%20and%20sktime%20to%20shed%0Alight%20on%20their%20applicability%20in%20different%20real-world%20scenarios.%20This%20research%0Acontributes%20to%20the%20field%20of%20time%20series%20forecasting%20by%20providing%20a%20robust%0Abenchmarking%20methodology%20and%20facilitating%20informed%20decision-making%20when%0Achoosing%20forecasting%20methods%20for%20achieving%20optimal%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16445v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520time%2520series%2520forecasting%2520be%2520automated%253F%2520A%2520benchmark%2520and%2520analysis%26entry.906535625%3DAnvitha%2520Thirthapura%2520Sreedhara%2520and%2520Joaquin%2520Vanschoren%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520machine%2520learning%2520and%2520artificial%2520intelligence%252C%2520time%2520series%250Aforecasting%2520plays%2520a%2520pivotal%2520role%2520across%2520various%2520domains%2520such%2520as%2520finance%252C%250Ahealthcare%252C%2520and%2520weather.%2520However%252C%2520the%2520task%2520of%2520selecting%2520the%2520most%2520suitable%250Aforecasting%2520method%2520for%2520a%2520given%2520dataset%2520is%2520a%2520complex%2520task%2520due%2520to%2520the%2520diversity%250Aof%2520data%2520patterns%2520and%2520characteristics.%2520This%2520research%2520aims%2520to%2520address%2520this%250Achallenge%2520by%2520proposing%2520a%2520comprehensive%2520benchmark%2520for%2520evaluating%2520and%2520ranking%250Atime%2520series%2520forecasting%2520methods%2520across%2520a%2520wide%2520range%2520of%2520datasets.%2520This%2520study%250Ainvestigates%2520the%2520comparative%2520performance%2520of%2520many%2520methods%2520from%2520two%2520prominent%250Atime%2520series%2520forecasting%2520frameworks%252C%2520AutoGluon-Timeseries%252C%2520and%2520sktime%2520to%2520shed%250Alight%2520on%2520their%2520applicability%2520in%2520different%2520real-world%2520scenarios.%2520This%2520research%250Acontributes%2520to%2520the%2520field%2520of%2520time%2520series%2520forecasting%2520by%2520providing%2520a%2520robust%250Abenchmarking%2520methodology%2520and%2520facilitating%2520informed%2520decision-making%2520when%250Achoosing%2520forecasting%2520methods%2520for%2520achieving%2520optimal%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16445v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20time%20series%20forecasting%20be%20automated%3F%20A%20benchmark%20and%20analysis&entry.906535625=Anvitha%20Thirthapura%20Sreedhara%20and%20Joaquin%20Vanschoren&entry.1292438233=%20%20In%20the%20field%20of%20machine%20learning%20and%20artificial%20intelligence%2C%20time%20series%0Aforecasting%20plays%20a%20pivotal%20role%20across%20various%20domains%20such%20as%20finance%2C%0Ahealthcare%2C%20and%20weather.%20However%2C%20the%20task%20of%20selecting%20the%20most%20suitable%0Aforecasting%20method%20for%20a%20given%20dataset%20is%20a%20complex%20task%20due%20to%20the%20diversity%0Aof%20data%20patterns%20and%20characteristics.%20This%20research%20aims%20to%20address%20this%0Achallenge%20by%20proposing%20a%20comprehensive%20benchmark%20for%20evaluating%20and%20ranking%0Atime%20series%20forecasting%20methods%20across%20a%20wide%20range%20of%20datasets.%20This%20study%0Ainvestigates%20the%20comparative%20performance%20of%20many%20methods%20from%20two%20prominent%0Atime%20series%20forecasting%20frameworks%2C%20AutoGluon-Timeseries%2C%20and%20sktime%20to%20shed%0Alight%20on%20their%20applicability%20in%20different%20real-world%20scenarios.%20This%20research%0Acontributes%20to%20the%20field%20of%20time%20series%20forecasting%20by%20providing%20a%20robust%0Abenchmarking%20methodology%20and%20facilitating%20informed%20decision-making%20when%0Achoosing%20forecasting%20methods%20for%20achieving%20optimal%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16445v2&entry.124074799=Read"},
{"title": "Particle identification with machine learning from incomplete data in\n  the ALICE experiment", "author": "Maja Karwowska and \u0141ukasz Graczykowski and Kamil Deja and Mi\u0142osz Kasak and Ma\u0142gorzata Janik", "abstract": "  The ALICE experiment at the LHC measures properties of the strongly\ninteracting matter formed in ultrarelativistic heavy-ion collisions. Such\nstudies require accurate particle identification (PID). ALICE provides PID\ninformation via several detectors for particles with momentum from about 100\nMeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular\ncuts. A much better performance can be achieved with machine learning (ML)\nmethods. Our solution uses multiple neural networks (NN) serving as binary\nclassifiers. Moreover, we extended our particle classifier with Feature Set\nEmbedding and attention in order to train on data with incomplete samples. We\nalso present the integration of the ML project with the ALICE analysis\nsoftware, and we discuss domain adaptation, the ML technique needed to transfer\nthe knowledge between simulated and real experimental data.\n", "link": "http://arxiv.org/abs/2403.17436v3", "date": "2024-07-25", "relevancy": 1.3694, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4736}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4571}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Particle%20identification%20with%20machine%20learning%20from%20incomplete%20data%20in%0A%20%20the%20ALICE%20experiment&body=Title%3A%20Particle%20identification%20with%20machine%20learning%20from%20incomplete%20data%20in%0A%20%20the%20ALICE%20experiment%0AAuthor%3A%20Maja%20Karwowska%20and%20%C5%81ukasz%20Graczykowski%20and%20Kamil%20Deja%20and%20Mi%C5%82osz%20Kasak%20and%20Ma%C5%82gorzata%20Janik%0AAbstract%3A%20%20%20The%20ALICE%20experiment%20at%20the%20LHC%20measures%20properties%20of%20the%20strongly%0Ainteracting%20matter%20formed%20in%20ultrarelativistic%20heavy-ion%20collisions.%20Such%0Astudies%20require%20accurate%20particle%20identification%20%28PID%29.%20ALICE%20provides%20PID%0Ainformation%20via%20several%20detectors%20for%20particles%20with%20momentum%20from%20about%20100%0AMeV/c%20up%20to%2020%20GeV/c.%20Traditionally%2C%20particles%20are%20selected%20with%20rectangular%0Acuts.%20A%20much%20better%20performance%20can%20be%20achieved%20with%20machine%20learning%20%28ML%29%0Amethods.%20Our%20solution%20uses%20multiple%20neural%20networks%20%28NN%29%20serving%20as%20binary%0Aclassifiers.%20Moreover%2C%20we%20extended%20our%20particle%20classifier%20with%20Feature%20Set%0AEmbedding%20and%20attention%20in%20order%20to%20train%20on%20data%20with%20incomplete%20samples.%20We%0Aalso%20present%20the%20integration%20of%20the%20ML%20project%20with%20the%20ALICE%20analysis%0Asoftware%2C%20and%20we%20discuss%20domain%20adaptation%2C%20the%20ML%20technique%20needed%20to%20transfer%0Athe%20knowledge%20between%20simulated%20and%20real%20experimental%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17436v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParticle%2520identification%2520with%2520machine%2520learning%2520from%2520incomplete%2520data%2520in%250A%2520%2520the%2520ALICE%2520experiment%26entry.906535625%3DMaja%2520Karwowska%2520and%2520%25C5%2581ukasz%2520Graczykowski%2520and%2520Kamil%2520Deja%2520and%2520Mi%25C5%2582osz%2520Kasak%2520and%2520Ma%25C5%2582gorzata%2520Janik%26entry.1292438233%3D%2520%2520The%2520ALICE%2520experiment%2520at%2520the%2520LHC%2520measures%2520properties%2520of%2520the%2520strongly%250Ainteracting%2520matter%2520formed%2520in%2520ultrarelativistic%2520heavy-ion%2520collisions.%2520Such%250Astudies%2520require%2520accurate%2520particle%2520identification%2520%2528PID%2529.%2520ALICE%2520provides%2520PID%250Ainformation%2520via%2520several%2520detectors%2520for%2520particles%2520with%2520momentum%2520from%2520about%2520100%250AMeV/c%2520up%2520to%252020%2520GeV/c.%2520Traditionally%252C%2520particles%2520are%2520selected%2520with%2520rectangular%250Acuts.%2520A%2520much%2520better%2520performance%2520can%2520be%2520achieved%2520with%2520machine%2520learning%2520%2528ML%2529%250Amethods.%2520Our%2520solution%2520uses%2520multiple%2520neural%2520networks%2520%2528NN%2529%2520serving%2520as%2520binary%250Aclassifiers.%2520Moreover%252C%2520we%2520extended%2520our%2520particle%2520classifier%2520with%2520Feature%2520Set%250AEmbedding%2520and%2520attention%2520in%2520order%2520to%2520train%2520on%2520data%2520with%2520incomplete%2520samples.%2520We%250Aalso%2520present%2520the%2520integration%2520of%2520the%2520ML%2520project%2520with%2520the%2520ALICE%2520analysis%250Asoftware%252C%2520and%2520we%2520discuss%2520domain%2520adaptation%252C%2520the%2520ML%2520technique%2520needed%2520to%2520transfer%250Athe%2520knowledge%2520between%2520simulated%2520and%2520real%2520experimental%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17436v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Particle%20identification%20with%20machine%20learning%20from%20incomplete%20data%20in%0A%20%20the%20ALICE%20experiment&entry.906535625=Maja%20Karwowska%20and%20%C5%81ukasz%20Graczykowski%20and%20Kamil%20Deja%20and%20Mi%C5%82osz%20Kasak%20and%20Ma%C5%82gorzata%20Janik&entry.1292438233=%20%20The%20ALICE%20experiment%20at%20the%20LHC%20measures%20properties%20of%20the%20strongly%0Ainteracting%20matter%20formed%20in%20ultrarelativistic%20heavy-ion%20collisions.%20Such%0Astudies%20require%20accurate%20particle%20identification%20%28PID%29.%20ALICE%20provides%20PID%0Ainformation%20via%20several%20detectors%20for%20particles%20with%20momentum%20from%20about%20100%0AMeV/c%20up%20to%2020%20GeV/c.%20Traditionally%2C%20particles%20are%20selected%20with%20rectangular%0Acuts.%20A%20much%20better%20performance%20can%20be%20achieved%20with%20machine%20learning%20%28ML%29%0Amethods.%20Our%20solution%20uses%20multiple%20neural%20networks%20%28NN%29%20serving%20as%20binary%0Aclassifiers.%20Moreover%2C%20we%20extended%20our%20particle%20classifier%20with%20Feature%20Set%0AEmbedding%20and%20attention%20in%20order%20to%20train%20on%20data%20with%20incomplete%20samples.%20We%0Aalso%20present%20the%20integration%20of%20the%20ML%20project%20with%20the%20ALICE%20analysis%0Asoftware%2C%20and%20we%20discuss%20domain%20adaptation%2C%20the%20ML%20technique%20needed%20to%20transfer%0Athe%20knowledge%20between%20simulated%20and%20real%20experimental%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17436v3&entry.124074799=Read"},
{"title": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache", "author": "Zuyan Liu and Benlin Liu and Jiahui Wang and Yuhao Dong and Guangyi Chen and Yongming Rao and Ranjay Krishna and Jiwen Lu", "abstract": "  In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache\n", "link": "http://arxiv.org/abs/2407.18121v1", "date": "2024-07-25", "relevancy": 1.5134, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5107}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.504}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Inference%20of%20Vision%20Instruction-Following%20Models%20with%20Elastic%0A%20%20Cache&body=Title%3A%20Efficient%20Inference%20of%20Vision%20Instruction-Following%20Models%20with%20Elastic%0A%20%20Cache%0AAuthor%3A%20Zuyan%20Liu%20and%20Benlin%20Liu%20and%20Jiahui%20Wang%20and%20Yuhao%20Dong%20and%20Guangyi%20Chen%20and%20Yongming%20Rao%20and%20Ranjay%20Krishna%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20In%20the%20field%20of%20instruction-following%20large%20vision-language%20models%20%28LVLMs%29%2C%0Athe%20efficient%20deployment%20of%20these%20models%20faces%20challenges%2C%20notably%20due%20to%20the%0Ahigh%20memory%20demands%20of%20their%20key-value%20%28KV%29%20caches.%20Conventional%20cache%0Amanagement%20strategies%20for%20LLMs%20focus%20on%20cache%20eviction%2C%20which%20often%20fails%20to%0Aaddress%20the%20specific%20needs%20of%20multimodal%20instruction-following%20models.%0ARecognizing%20this%20gap%2C%20in%20this%20paper%2C%20we%20introduce%20Elastic%20Cache%2C%20a%20novel%0Aapproach%20that%20benefits%20from%20applying%20distinct%20acceleration%20methods%20for%0Ainstruction%20encoding%20and%20output%20generation%20stages.%20We%20investigate%20the%20metrics%0Aof%20importance%20in%20different%20stages%20and%20propose%20an%20importance-driven%20cache%0Amerging%20strategy%20to%20prune%20redundancy%20caches.%20Instead%20of%20discarding%20less%0Aimportant%20caches%2C%20our%20strategy%20identifies%20important%20key/value%20vectors%20as%20anchor%0Apoints.%20Surrounding%20less%20important%20caches%20are%20then%20merged%20with%20these%20anchors%2C%0Aenhancing%20the%20preservation%20of%20contextual%20information%20in%20the%20KV%20caches%20while%0Ayielding%20an%20arbitrary%20acceleration%20ratio.%20For%20instruction%20encoding%2C%20we%20utilize%0Athe%20frequency%20to%20evaluate%20the%20importance%20of%20caches.%20Regarding%20output%0Ageneration%2C%20we%20prioritize%20tokens%20based%20on%20their%20distance%20with%20an%20offset%2C%20by%0Awhich%20both%20the%20initial%20and%20most%20recent%20tokens%20are%20retained.%20Results%20on%20a%20range%0Aof%20LVLMs%20demonstrate%20that%20Elastic%20Cache%20not%20only%20boosts%20efficiency%20but%20also%0Anotably%20outperforms%20existing%20pruning%20methods%20in%20language%20generation%20across%0Avarious%20tasks.%20Code%20is%20available%20at%20https%3A//github.com/liuzuyan/ElasticCache%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18121v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Inference%2520of%2520Vision%2520Instruction-Following%2520Models%2520with%2520Elastic%250A%2520%2520Cache%26entry.906535625%3DZuyan%2520Liu%2520and%2520Benlin%2520Liu%2520and%2520Jiahui%2520Wang%2520and%2520Yuhao%2520Dong%2520and%2520Guangyi%2520Chen%2520and%2520Yongming%2520Rao%2520and%2520Ranjay%2520Krishna%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520instruction-following%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%252C%250Athe%2520efficient%2520deployment%2520of%2520these%2520models%2520faces%2520challenges%252C%2520notably%2520due%2520to%2520the%250Ahigh%2520memory%2520demands%2520of%2520their%2520key-value%2520%2528KV%2529%2520caches.%2520Conventional%2520cache%250Amanagement%2520strategies%2520for%2520LLMs%2520focus%2520on%2520cache%2520eviction%252C%2520which%2520often%2520fails%2520to%250Aaddress%2520the%2520specific%2520needs%2520of%2520multimodal%2520instruction-following%2520models.%250ARecognizing%2520this%2520gap%252C%2520in%2520this%2520paper%252C%2520we%2520introduce%2520Elastic%2520Cache%252C%2520a%2520novel%250Aapproach%2520that%2520benefits%2520from%2520applying%2520distinct%2520acceleration%2520methods%2520for%250Ainstruction%2520encoding%2520and%2520output%2520generation%2520stages.%2520We%2520investigate%2520the%2520metrics%250Aof%2520importance%2520in%2520different%2520stages%2520and%2520propose%2520an%2520importance-driven%2520cache%250Amerging%2520strategy%2520to%2520prune%2520redundancy%2520caches.%2520Instead%2520of%2520discarding%2520less%250Aimportant%2520caches%252C%2520our%2520strategy%2520identifies%2520important%2520key/value%2520vectors%2520as%2520anchor%250Apoints.%2520Surrounding%2520less%2520important%2520caches%2520are%2520then%2520merged%2520with%2520these%2520anchors%252C%250Aenhancing%2520the%2520preservation%2520of%2520contextual%2520information%2520in%2520the%2520KV%2520caches%2520while%250Ayielding%2520an%2520arbitrary%2520acceleration%2520ratio.%2520For%2520instruction%2520encoding%252C%2520we%2520utilize%250Athe%2520frequency%2520to%2520evaluate%2520the%2520importance%2520of%2520caches.%2520Regarding%2520output%250Ageneration%252C%2520we%2520prioritize%2520tokens%2520based%2520on%2520their%2520distance%2520with%2520an%2520offset%252C%2520by%250Awhich%2520both%2520the%2520initial%2520and%2520most%2520recent%2520tokens%2520are%2520retained.%2520Results%2520on%2520a%2520range%250Aof%2520LVLMs%2520demonstrate%2520that%2520Elastic%2520Cache%2520not%2520only%2520boosts%2520efficiency%2520but%2520also%250Anotably%2520outperforms%2520existing%2520pruning%2520methods%2520in%2520language%2520generation%2520across%250Avarious%2520tasks.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/liuzuyan/ElasticCache%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18121v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Inference%20of%20Vision%20Instruction-Following%20Models%20with%20Elastic%0A%20%20Cache&entry.906535625=Zuyan%20Liu%20and%20Benlin%20Liu%20and%20Jiahui%20Wang%20and%20Yuhao%20Dong%20and%20Guangyi%20Chen%20and%20Yongming%20Rao%20and%20Ranjay%20Krishna%20and%20Jiwen%20Lu&entry.1292438233=%20%20In%20the%20field%20of%20instruction-following%20large%20vision-language%20models%20%28LVLMs%29%2C%0Athe%20efficient%20deployment%20of%20these%20models%20faces%20challenges%2C%20notably%20due%20to%20the%0Ahigh%20memory%20demands%20of%20their%20key-value%20%28KV%29%20caches.%20Conventional%20cache%0Amanagement%20strategies%20for%20LLMs%20focus%20on%20cache%20eviction%2C%20which%20often%20fails%20to%0Aaddress%20the%20specific%20needs%20of%20multimodal%20instruction-following%20models.%0ARecognizing%20this%20gap%2C%20in%20this%20paper%2C%20we%20introduce%20Elastic%20Cache%2C%20a%20novel%0Aapproach%20that%20benefits%20from%20applying%20distinct%20acceleration%20methods%20for%0Ainstruction%20encoding%20and%20output%20generation%20stages.%20We%20investigate%20the%20metrics%0Aof%20importance%20in%20different%20stages%20and%20propose%20an%20importance-driven%20cache%0Amerging%20strategy%20to%20prune%20redundancy%20caches.%20Instead%20of%20discarding%20less%0Aimportant%20caches%2C%20our%20strategy%20identifies%20important%20key/value%20vectors%20as%20anchor%0Apoints.%20Surrounding%20less%20important%20caches%20are%20then%20merged%20with%20these%20anchors%2C%0Aenhancing%20the%20preservation%20of%20contextual%20information%20in%20the%20KV%20caches%20while%0Ayielding%20an%20arbitrary%20acceleration%20ratio.%20For%20instruction%20encoding%2C%20we%20utilize%0Athe%20frequency%20to%20evaluate%20the%20importance%20of%20caches.%20Regarding%20output%0Ageneration%2C%20we%20prioritize%20tokens%20based%20on%20their%20distance%20with%20an%20offset%2C%20by%0Awhich%20both%20the%20initial%20and%20most%20recent%20tokens%20are%20retained.%20Results%20on%20a%20range%0Aof%20LVLMs%20demonstrate%20that%20Elastic%20Cache%20not%20only%20boosts%20efficiency%20but%20also%0Anotably%20outperforms%20existing%20pruning%20methods%20in%20language%20generation%20across%0Avarious%20tasks.%20Code%20is%20available%20at%20https%3A//github.com/liuzuyan/ElasticCache%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18121v1&entry.124074799=Read"},
{"title": "Causal Deepsets for Off-policy Evaluation under Spatial or\n  Spatio-temporal Interferences", "author": "Runpeng Dai and Jianing Wang and Fan Zhou and Shikai Luo and Zhiwei Qin and Chengchun Shi and Hongtu Zhu", "abstract": "  Off-policy evaluation (OPE) is widely applied in sectors such as\npharmaceuticals and e-commerce to evaluate the efficacy of novel products or\npolicies from offline datasets. This paper introduces a causal deepset\nframework that relaxes several key structural assumptions, primarily the\nmean-field assumption, prevalent in existing OPE methodologies that handle\nspatio-temporal interference. These traditional assumptions frequently prove\ninadequate in real-world settings, thereby restricting the capability of\ncurrent OPE methods to effectively address complex interference effects. In\nresponse, we advocate for the implementation of the permutation invariance (PI)\nassumption. This innovative approach enables the data-driven, adaptive learning\nof the mean-field function, offering a more flexible estimation method beyond\nconventional averaging. Furthermore, we present novel algorithms that\nincorporate the PI assumption into OPE and thoroughly examine their theoretical\nfoundations. Our numerical analyses demonstrate that this novel approach yields\nsignificantly more precise estimations than existing baseline algorithms,\nthereby substantially improving the practical applicability and effectiveness\nof OPE methodologies. A Python implementation of our proposed method is\navailable at https://github.com/BIG-S2/Causal-Deepsets.\n", "link": "http://arxiv.org/abs/2407.17910v1", "date": "2024-07-25", "relevancy": 1.4057, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.504}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4594}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Deepsets%20for%20Off-policy%20Evaluation%20under%20Spatial%20or%0A%20%20Spatio-temporal%20Interferences&body=Title%3A%20Causal%20Deepsets%20for%20Off-policy%20Evaluation%20under%20Spatial%20or%0A%20%20Spatio-temporal%20Interferences%0AAuthor%3A%20Runpeng%20Dai%20and%20Jianing%20Wang%20and%20Fan%20Zhou%20and%20Shikai%20Luo%20and%20Zhiwei%20Qin%20and%20Chengchun%20Shi%20and%20Hongtu%20Zhu%0AAbstract%3A%20%20%20Off-policy%20evaluation%20%28OPE%29%20is%20widely%20applied%20in%20sectors%20such%20as%0Apharmaceuticals%20and%20e-commerce%20to%20evaluate%20the%20efficacy%20of%20novel%20products%20or%0Apolicies%20from%20offline%20datasets.%20This%20paper%20introduces%20a%20causal%20deepset%0Aframework%20that%20relaxes%20several%20key%20structural%20assumptions%2C%20primarily%20the%0Amean-field%20assumption%2C%20prevalent%20in%20existing%20OPE%20methodologies%20that%20handle%0Aspatio-temporal%20interference.%20These%20traditional%20assumptions%20frequently%20prove%0Ainadequate%20in%20real-world%20settings%2C%20thereby%20restricting%20the%20capability%20of%0Acurrent%20OPE%20methods%20to%20effectively%20address%20complex%20interference%20effects.%20In%0Aresponse%2C%20we%20advocate%20for%20the%20implementation%20of%20the%20permutation%20invariance%20%28PI%29%0Aassumption.%20This%20innovative%20approach%20enables%20the%20data-driven%2C%20adaptive%20learning%0Aof%20the%20mean-field%20function%2C%20offering%20a%20more%20flexible%20estimation%20method%20beyond%0Aconventional%20averaging.%20Furthermore%2C%20we%20present%20novel%20algorithms%20that%0Aincorporate%20the%20PI%20assumption%20into%20OPE%20and%20thoroughly%20examine%20their%20theoretical%0Afoundations.%20Our%20numerical%20analyses%20demonstrate%20that%20this%20novel%20approach%20yields%0Asignificantly%20more%20precise%20estimations%20than%20existing%20baseline%20algorithms%2C%0Athereby%20substantially%20improving%20the%20practical%20applicability%20and%20effectiveness%0Aof%20OPE%20methodologies.%20A%20Python%20implementation%20of%20our%20proposed%20method%20is%0Aavailable%20at%20https%3A//github.com/BIG-S2/Causal-Deepsets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Deepsets%2520for%2520Off-policy%2520Evaluation%2520under%2520Spatial%2520or%250A%2520%2520Spatio-temporal%2520Interferences%26entry.906535625%3DRunpeng%2520Dai%2520and%2520Jianing%2520Wang%2520and%2520Fan%2520Zhou%2520and%2520Shikai%2520Luo%2520and%2520Zhiwei%2520Qin%2520and%2520Chengchun%2520Shi%2520and%2520Hongtu%2520Zhu%26entry.1292438233%3D%2520%2520Off-policy%2520evaluation%2520%2528OPE%2529%2520is%2520widely%2520applied%2520in%2520sectors%2520such%2520as%250Apharmaceuticals%2520and%2520e-commerce%2520to%2520evaluate%2520the%2520efficacy%2520of%2520novel%2520products%2520or%250Apolicies%2520from%2520offline%2520datasets.%2520This%2520paper%2520introduces%2520a%2520causal%2520deepset%250Aframework%2520that%2520relaxes%2520several%2520key%2520structural%2520assumptions%252C%2520primarily%2520the%250Amean-field%2520assumption%252C%2520prevalent%2520in%2520existing%2520OPE%2520methodologies%2520that%2520handle%250Aspatio-temporal%2520interference.%2520These%2520traditional%2520assumptions%2520frequently%2520prove%250Ainadequate%2520in%2520real-world%2520settings%252C%2520thereby%2520restricting%2520the%2520capability%2520of%250Acurrent%2520OPE%2520methods%2520to%2520effectively%2520address%2520complex%2520interference%2520effects.%2520In%250Aresponse%252C%2520we%2520advocate%2520for%2520the%2520implementation%2520of%2520the%2520permutation%2520invariance%2520%2528PI%2529%250Aassumption.%2520This%2520innovative%2520approach%2520enables%2520the%2520data-driven%252C%2520adaptive%2520learning%250Aof%2520the%2520mean-field%2520function%252C%2520offering%2520a%2520more%2520flexible%2520estimation%2520method%2520beyond%250Aconventional%2520averaging.%2520Furthermore%252C%2520we%2520present%2520novel%2520algorithms%2520that%250Aincorporate%2520the%2520PI%2520assumption%2520into%2520OPE%2520and%2520thoroughly%2520examine%2520their%2520theoretical%250Afoundations.%2520Our%2520numerical%2520analyses%2520demonstrate%2520that%2520this%2520novel%2520approach%2520yields%250Asignificantly%2520more%2520precise%2520estimations%2520than%2520existing%2520baseline%2520algorithms%252C%250Athereby%2520substantially%2520improving%2520the%2520practical%2520applicability%2520and%2520effectiveness%250Aof%2520OPE%2520methodologies.%2520A%2520Python%2520implementation%2520of%2520our%2520proposed%2520method%2520is%250Aavailable%2520at%2520https%253A//github.com/BIG-S2/Causal-Deepsets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Deepsets%20for%20Off-policy%20Evaluation%20under%20Spatial%20or%0A%20%20Spatio-temporal%20Interferences&entry.906535625=Runpeng%20Dai%20and%20Jianing%20Wang%20and%20Fan%20Zhou%20and%20Shikai%20Luo%20and%20Zhiwei%20Qin%20and%20Chengchun%20Shi%20and%20Hongtu%20Zhu&entry.1292438233=%20%20Off-policy%20evaluation%20%28OPE%29%20is%20widely%20applied%20in%20sectors%20such%20as%0Apharmaceuticals%20and%20e-commerce%20to%20evaluate%20the%20efficacy%20of%20novel%20products%20or%0Apolicies%20from%20offline%20datasets.%20This%20paper%20introduces%20a%20causal%20deepset%0Aframework%20that%20relaxes%20several%20key%20structural%20assumptions%2C%20primarily%20the%0Amean-field%20assumption%2C%20prevalent%20in%20existing%20OPE%20methodologies%20that%20handle%0Aspatio-temporal%20interference.%20These%20traditional%20assumptions%20frequently%20prove%0Ainadequate%20in%20real-world%20settings%2C%20thereby%20restricting%20the%20capability%20of%0Acurrent%20OPE%20methods%20to%20effectively%20address%20complex%20interference%20effects.%20In%0Aresponse%2C%20we%20advocate%20for%20the%20implementation%20of%20the%20permutation%20invariance%20%28PI%29%0Aassumption.%20This%20innovative%20approach%20enables%20the%20data-driven%2C%20adaptive%20learning%0Aof%20the%20mean-field%20function%2C%20offering%20a%20more%20flexible%20estimation%20method%20beyond%0Aconventional%20averaging.%20Furthermore%2C%20we%20present%20novel%20algorithms%20that%0Aincorporate%20the%20PI%20assumption%20into%20OPE%20and%20thoroughly%20examine%20their%20theoretical%0Afoundations.%20Our%20numerical%20analyses%20demonstrate%20that%20this%20novel%20approach%20yields%0Asignificantly%20more%20precise%20estimations%20than%20existing%20baseline%20algorithms%2C%0Athereby%20substantially%20improving%20the%20practical%20applicability%20and%20effectiveness%0Aof%20OPE%20methodologies.%20A%20Python%20implementation%20of%20our%20proposed%20method%20is%0Aavailable%20at%20https%3A//github.com/BIG-S2/Causal-Deepsets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17910v1&entry.124074799=Read"},
{"title": "Light Curve Classification with DistClassiPy: a new distance-based\n  classifier", "author": "Siddharth Chaini and Ashish Mahabal and Ajit Kembhavi and Federica B. Bianco", "abstract": "  The rise of synoptic sky surveys has ushered in an era of big data in\ntime-domain astronomy, making data science and machine learning essential tools\nfor studying celestial objects. While tree-based models (e.g. Random Forests)\nand deep learning models dominate the field, we explore the use of different\ndistance metrics to aid in the classification of astrophysical objects. We\ndeveloped DistClassiPy, a new distance metric based classifier. The direct use\nof distance metrics is unexplored in time-domain astronomy, but distance-based\nmethods can help make classification more interpretable and decrease\ncomputational costs. In particular, we applied DistClassiPy to classify light\ncurves of variable stars, comparing the distances between objects of different\nclasses. Using 18 distance metrics on a catalog of 6,000 variable stars across\n10 classes, we demonstrate classification and dimensionality reduction. Our\nclassifier meets state-of-the-art performance but has lower computational\nrequirements and improved interpretability. Additionally, DistClassiPy can be\ntailored to specific objects by identifying the most effective distance metric\nfor that classification. To facilitate broader applications within and beyond\nastronomy, we have made DistClassiPy open-source and available at\nhttps://pypi.org/project/distclassipy/.\n", "link": "http://arxiv.org/abs/2403.12120v2", "date": "2024-07-25", "relevancy": 1.5447, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3995}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.3835}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.3835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Light%20Curve%20Classification%20with%20DistClassiPy%3A%20a%20new%20distance-based%0A%20%20classifier&body=Title%3A%20Light%20Curve%20Classification%20with%20DistClassiPy%3A%20a%20new%20distance-based%0A%20%20classifier%0AAuthor%3A%20Siddharth%20Chaini%20and%20Ashish%20Mahabal%20and%20Ajit%20Kembhavi%20and%20Federica%20B.%20Bianco%0AAbstract%3A%20%20%20The%20rise%20of%20synoptic%20sky%20surveys%20has%20ushered%20in%20an%20era%20of%20big%20data%20in%0Atime-domain%20astronomy%2C%20making%20data%20science%20and%20machine%20learning%20essential%20tools%0Afor%20studying%20celestial%20objects.%20While%20tree-based%20models%20%28e.g.%20Random%20Forests%29%0Aand%20deep%20learning%20models%20dominate%20the%20field%2C%20we%20explore%20the%20use%20of%20different%0Adistance%20metrics%20to%20aid%20in%20the%20classification%20of%20astrophysical%20objects.%20We%0Adeveloped%20DistClassiPy%2C%20a%20new%20distance%20metric%20based%20classifier.%20The%20direct%20use%0Aof%20distance%20metrics%20is%20unexplored%20in%20time-domain%20astronomy%2C%20but%20distance-based%0Amethods%20can%20help%20make%20classification%20more%20interpretable%20and%20decrease%0Acomputational%20costs.%20In%20particular%2C%20we%20applied%20DistClassiPy%20to%20classify%20light%0Acurves%20of%20variable%20stars%2C%20comparing%20the%20distances%20between%20objects%20of%20different%0Aclasses.%20Using%2018%20distance%20metrics%20on%20a%20catalog%20of%206%2C000%20variable%20stars%20across%0A10%20classes%2C%20we%20demonstrate%20classification%20and%20dimensionality%20reduction.%20Our%0Aclassifier%20meets%20state-of-the-art%20performance%20but%20has%20lower%20computational%0Arequirements%20and%20improved%20interpretability.%20Additionally%2C%20DistClassiPy%20can%20be%0Atailored%20to%20specific%20objects%20by%20identifying%20the%20most%20effective%20distance%20metric%0Afor%20that%20classification.%20To%20facilitate%20broader%20applications%20within%20and%20beyond%0Aastronomy%2C%20we%20have%20made%20DistClassiPy%20open-source%20and%20available%20at%0Ahttps%3A//pypi.org/project/distclassipy/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12120v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLight%2520Curve%2520Classification%2520with%2520DistClassiPy%253A%2520a%2520new%2520distance-based%250A%2520%2520classifier%26entry.906535625%3DSiddharth%2520Chaini%2520and%2520Ashish%2520Mahabal%2520and%2520Ajit%2520Kembhavi%2520and%2520Federica%2520B.%2520Bianco%26entry.1292438233%3D%2520%2520The%2520rise%2520of%2520synoptic%2520sky%2520surveys%2520has%2520ushered%2520in%2520an%2520era%2520of%2520big%2520data%2520in%250Atime-domain%2520astronomy%252C%2520making%2520data%2520science%2520and%2520machine%2520learning%2520essential%2520tools%250Afor%2520studying%2520celestial%2520objects.%2520While%2520tree-based%2520models%2520%2528e.g.%2520Random%2520Forests%2529%250Aand%2520deep%2520learning%2520models%2520dominate%2520the%2520field%252C%2520we%2520explore%2520the%2520use%2520of%2520different%250Adistance%2520metrics%2520to%2520aid%2520in%2520the%2520classification%2520of%2520astrophysical%2520objects.%2520We%250Adeveloped%2520DistClassiPy%252C%2520a%2520new%2520distance%2520metric%2520based%2520classifier.%2520The%2520direct%2520use%250Aof%2520distance%2520metrics%2520is%2520unexplored%2520in%2520time-domain%2520astronomy%252C%2520but%2520distance-based%250Amethods%2520can%2520help%2520make%2520classification%2520more%2520interpretable%2520and%2520decrease%250Acomputational%2520costs.%2520In%2520particular%252C%2520we%2520applied%2520DistClassiPy%2520to%2520classify%2520light%250Acurves%2520of%2520variable%2520stars%252C%2520comparing%2520the%2520distances%2520between%2520objects%2520of%2520different%250Aclasses.%2520Using%252018%2520distance%2520metrics%2520on%2520a%2520catalog%2520of%25206%252C000%2520variable%2520stars%2520across%250A10%2520classes%252C%2520we%2520demonstrate%2520classification%2520and%2520dimensionality%2520reduction.%2520Our%250Aclassifier%2520meets%2520state-of-the-art%2520performance%2520but%2520has%2520lower%2520computational%250Arequirements%2520and%2520improved%2520interpretability.%2520Additionally%252C%2520DistClassiPy%2520can%2520be%250Atailored%2520to%2520specific%2520objects%2520by%2520identifying%2520the%2520most%2520effective%2520distance%2520metric%250Afor%2520that%2520classification.%2520To%2520facilitate%2520broader%2520applications%2520within%2520and%2520beyond%250Aastronomy%252C%2520we%2520have%2520made%2520DistClassiPy%2520open-source%2520and%2520available%2520at%250Ahttps%253A//pypi.org/project/distclassipy/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12120v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Light%20Curve%20Classification%20with%20DistClassiPy%3A%20a%20new%20distance-based%0A%20%20classifier&entry.906535625=Siddharth%20Chaini%20and%20Ashish%20Mahabal%20and%20Ajit%20Kembhavi%20and%20Federica%20B.%20Bianco&entry.1292438233=%20%20The%20rise%20of%20synoptic%20sky%20surveys%20has%20ushered%20in%20an%20era%20of%20big%20data%20in%0Atime-domain%20astronomy%2C%20making%20data%20science%20and%20machine%20learning%20essential%20tools%0Afor%20studying%20celestial%20objects.%20While%20tree-based%20models%20%28e.g.%20Random%20Forests%29%0Aand%20deep%20learning%20models%20dominate%20the%20field%2C%20we%20explore%20the%20use%20of%20different%0Adistance%20metrics%20to%20aid%20in%20the%20classification%20of%20astrophysical%20objects.%20We%0Adeveloped%20DistClassiPy%2C%20a%20new%20distance%20metric%20based%20classifier.%20The%20direct%20use%0Aof%20distance%20metrics%20is%20unexplored%20in%20time-domain%20astronomy%2C%20but%20distance-based%0Amethods%20can%20help%20make%20classification%20more%20interpretable%20and%20decrease%0Acomputational%20costs.%20In%20particular%2C%20we%20applied%20DistClassiPy%20to%20classify%20light%0Acurves%20of%20variable%20stars%2C%20comparing%20the%20distances%20between%20objects%20of%20different%0Aclasses.%20Using%2018%20distance%20metrics%20on%20a%20catalog%20of%206%2C000%20variable%20stars%20across%0A10%20classes%2C%20we%20demonstrate%20classification%20and%20dimensionality%20reduction.%20Our%0Aclassifier%20meets%20state-of-the-art%20performance%20but%20has%20lower%20computational%0Arequirements%20and%20improved%20interpretability.%20Additionally%2C%20DistClassiPy%20can%20be%0Atailored%20to%20specific%20objects%20by%20identifying%20the%20most%20effective%20distance%20metric%0Afor%20that%20classification.%20To%20facilitate%20broader%20applications%20within%20and%20beyond%0Aastronomy%2C%20we%20have%20made%20DistClassiPy%20open-source%20and%20available%20at%0Ahttps%3A//pypi.org/project/distclassipy/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12120v2&entry.124074799=Read"},
{"title": "Time-Optimal Planning for Long-Range Quadrotor Flights: An Automatic\n  Optimal Synthesis Approach", "author": "Chao Qin and Jingxiang Chen and Yifan Lin and Abhishek Goudar and Angela P. Schoellig and Hugh H. -T. Liu", "abstract": "  Time-critical tasks such as drone racing typically cover large operation\nareas. However, it is difficult and computationally intensive for current\ntime-optimal motion planners to accommodate long flight distances since a large\nyet unknown number of knot points is required to represent the trajectory. We\npresent a polynomial-based automatic optimal synthesis (AOS) approach that can\naddress this challenge. Our method not only achieves superior time optimality\nbut also maintains a consistently low computational cost across different\nranges while considering the full quadrotor dynamics. First, we analyze the\nproperties of time-optimal quadrotor maneuvers to determine the minimal number\nof polynomial pieces required to capture the dominant structure of time-optimal\ntrajectories. This enables us to represent substantially long minimum-time\ntrajectories with a minimal set of variables. Then, a robust optimization\nscheme is developed to handle arbitrary start and end conditions as well as\nintermediate waypoints. Extensive comparisons show that our approach is faster\nthan the state-of-the-art approach by orders of magnitude with comparable time\noptimality. Real-world experiments further validate the quality of the\nresulting trajectories, demonstrating aggressive time-optimal maneuvers with a\npeak velocity of 8.86 m/s.\n", "link": "http://arxiv.org/abs/2407.17944v1", "date": "2024-07-25", "relevancy": 1.3994, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4691}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4689}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4644}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time-Optimal%20Planning%20for%20Long-Range%20Quadrotor%20Flights%3A%20An%20Automatic%0A%20%20Optimal%20Synthesis%20Approach&body=Title%3A%20Time-Optimal%20Planning%20for%20Long-Range%20Quadrotor%20Flights%3A%20An%20Automatic%0A%20%20Optimal%20Synthesis%20Approach%0AAuthor%3A%20Chao%20Qin%20and%20Jingxiang%20Chen%20and%20Yifan%20Lin%20and%20Abhishek%20Goudar%20and%20Angela%20P.%20Schoellig%20and%20Hugh%20H.%20-T.%20Liu%0AAbstract%3A%20%20%20Time-critical%20tasks%20such%20as%20drone%20racing%20typically%20cover%20large%20operation%0Aareas.%20However%2C%20it%20is%20difficult%20and%20computationally%20intensive%20for%20current%0Atime-optimal%20motion%20planners%20to%20accommodate%20long%20flight%20distances%20since%20a%20large%0Ayet%20unknown%20number%20of%20knot%20points%20is%20required%20to%20represent%20the%20trajectory.%20We%0Apresent%20a%20polynomial-based%20automatic%20optimal%20synthesis%20%28AOS%29%20approach%20that%20can%0Aaddress%20this%20challenge.%20Our%20method%20not%20only%20achieves%20superior%20time%20optimality%0Abut%20also%20maintains%20a%20consistently%20low%20computational%20cost%20across%20different%0Aranges%20while%20considering%20the%20full%20quadrotor%20dynamics.%20First%2C%20we%20analyze%20the%0Aproperties%20of%20time-optimal%20quadrotor%20maneuvers%20to%20determine%20the%20minimal%20number%0Aof%20polynomial%20pieces%20required%20to%20capture%20the%20dominant%20structure%20of%20time-optimal%0Atrajectories.%20This%20enables%20us%20to%20represent%20substantially%20long%20minimum-time%0Atrajectories%20with%20a%20minimal%20set%20of%20variables.%20Then%2C%20a%20robust%20optimization%0Ascheme%20is%20developed%20to%20handle%20arbitrary%20start%20and%20end%20conditions%20as%20well%20as%0Aintermediate%20waypoints.%20Extensive%20comparisons%20show%20that%20our%20approach%20is%20faster%0Athan%20the%20state-of-the-art%20approach%20by%20orders%20of%20magnitude%20with%20comparable%20time%0Aoptimality.%20Real-world%20experiments%20further%20validate%20the%20quality%20of%20the%0Aresulting%20trajectories%2C%20demonstrating%20aggressive%20time-optimal%20maneuvers%20with%20a%0Apeak%20velocity%20of%208.86%20m/s.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17944v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime-Optimal%2520Planning%2520for%2520Long-Range%2520Quadrotor%2520Flights%253A%2520An%2520Automatic%250A%2520%2520Optimal%2520Synthesis%2520Approach%26entry.906535625%3DChao%2520Qin%2520and%2520Jingxiang%2520Chen%2520and%2520Yifan%2520Lin%2520and%2520Abhishek%2520Goudar%2520and%2520Angela%2520P.%2520Schoellig%2520and%2520Hugh%2520H.%2520-T.%2520Liu%26entry.1292438233%3D%2520%2520Time-critical%2520tasks%2520such%2520as%2520drone%2520racing%2520typically%2520cover%2520large%2520operation%250Aareas.%2520However%252C%2520it%2520is%2520difficult%2520and%2520computationally%2520intensive%2520for%2520current%250Atime-optimal%2520motion%2520planners%2520to%2520accommodate%2520long%2520flight%2520distances%2520since%2520a%2520large%250Ayet%2520unknown%2520number%2520of%2520knot%2520points%2520is%2520required%2520to%2520represent%2520the%2520trajectory.%2520We%250Apresent%2520a%2520polynomial-based%2520automatic%2520optimal%2520synthesis%2520%2528AOS%2529%2520approach%2520that%2520can%250Aaddress%2520this%2520challenge.%2520Our%2520method%2520not%2520only%2520achieves%2520superior%2520time%2520optimality%250Abut%2520also%2520maintains%2520a%2520consistently%2520low%2520computational%2520cost%2520across%2520different%250Aranges%2520while%2520considering%2520the%2520full%2520quadrotor%2520dynamics.%2520First%252C%2520we%2520analyze%2520the%250Aproperties%2520of%2520time-optimal%2520quadrotor%2520maneuvers%2520to%2520determine%2520the%2520minimal%2520number%250Aof%2520polynomial%2520pieces%2520required%2520to%2520capture%2520the%2520dominant%2520structure%2520of%2520time-optimal%250Atrajectories.%2520This%2520enables%2520us%2520to%2520represent%2520substantially%2520long%2520minimum-time%250Atrajectories%2520with%2520a%2520minimal%2520set%2520of%2520variables.%2520Then%252C%2520a%2520robust%2520optimization%250Ascheme%2520is%2520developed%2520to%2520handle%2520arbitrary%2520start%2520and%2520end%2520conditions%2520as%2520well%2520as%250Aintermediate%2520waypoints.%2520Extensive%2520comparisons%2520show%2520that%2520our%2520approach%2520is%2520faster%250Athan%2520the%2520state-of-the-art%2520approach%2520by%2520orders%2520of%2520magnitude%2520with%2520comparable%2520time%250Aoptimality.%2520Real-world%2520experiments%2520further%2520validate%2520the%2520quality%2520of%2520the%250Aresulting%2520trajectories%252C%2520demonstrating%2520aggressive%2520time-optimal%2520maneuvers%2520with%2520a%250Apeak%2520velocity%2520of%25208.86%2520m/s.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17944v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time-Optimal%20Planning%20for%20Long-Range%20Quadrotor%20Flights%3A%20An%20Automatic%0A%20%20Optimal%20Synthesis%20Approach&entry.906535625=Chao%20Qin%20and%20Jingxiang%20Chen%20and%20Yifan%20Lin%20and%20Abhishek%20Goudar%20and%20Angela%20P.%20Schoellig%20and%20Hugh%20H.%20-T.%20Liu&entry.1292438233=%20%20Time-critical%20tasks%20such%20as%20drone%20racing%20typically%20cover%20large%20operation%0Aareas.%20However%2C%20it%20is%20difficult%20and%20computationally%20intensive%20for%20current%0Atime-optimal%20motion%20planners%20to%20accommodate%20long%20flight%20distances%20since%20a%20large%0Ayet%20unknown%20number%20of%20knot%20points%20is%20required%20to%20represent%20the%20trajectory.%20We%0Apresent%20a%20polynomial-based%20automatic%20optimal%20synthesis%20%28AOS%29%20approach%20that%20can%0Aaddress%20this%20challenge.%20Our%20method%20not%20only%20achieves%20superior%20time%20optimality%0Abut%20also%20maintains%20a%20consistently%20low%20computational%20cost%20across%20different%0Aranges%20while%20considering%20the%20full%20quadrotor%20dynamics.%20First%2C%20we%20analyze%20the%0Aproperties%20of%20time-optimal%20quadrotor%20maneuvers%20to%20determine%20the%20minimal%20number%0Aof%20polynomial%20pieces%20required%20to%20capture%20the%20dominant%20structure%20of%20time-optimal%0Atrajectories.%20This%20enables%20us%20to%20represent%20substantially%20long%20minimum-time%0Atrajectories%20with%20a%20minimal%20set%20of%20variables.%20Then%2C%20a%20robust%20optimization%0Ascheme%20is%20developed%20to%20handle%20arbitrary%20start%20and%20end%20conditions%20as%20well%20as%0Aintermediate%20waypoints.%20Extensive%20comparisons%20show%20that%20our%20approach%20is%20faster%0Athan%20the%20state-of-the-art%20approach%20by%20orders%20of%20magnitude%20with%20comparable%20time%0Aoptimality.%20Real-world%20experiments%20further%20validate%20the%20quality%20of%20the%0Aresulting%20trajectories%2C%20demonstrating%20aggressive%20time-optimal%20maneuvers%20with%20a%0Apeak%20velocity%20of%208.86%20m/s.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17944v1&entry.124074799=Read"},
{"title": "Self-Supervision Improves Diffusion Models for Tabular Data Imputation", "author": "Yixin Liu and Thalaiyasingam Ajanthan and Hisham Husain and Vu Nguyen", "abstract": "  The ubiquity of missing data has sparked considerable attention and focus on\ntabular data imputation methods. Diffusion models, recognized as the\ncutting-edge technique for data generation, demonstrate significant potential\nin tabular data imputation tasks. However, in pursuit of diversity, vanilla\ndiffusion models often exhibit sensitivity to initialized noises, which hinders\nthe models from generating stable and accurate imputation results.\nAdditionally, the sparsity inherent in tabular data poses challenges for\ndiffusion models in accurately modeling the data manifold, impacting the\nrobustness of these models for data imputation. To tackle these challenges,\nthis paper introduces an advanced diffusion model named Self-supervised\nimputation Diffusion Model (SimpDM for brevity), specifically tailored for\ntabular data imputation tasks. To mitigate sensitivity to noise, we introduce a\nself-supervised alignment mechanism that aims to regularize the model, ensuring\nconsistent and stable imputation predictions. Furthermore, we introduce a\ncarefully devised state-dependent data augmentation strategy within SimpDM,\nenhancing the robustness of the diffusion model when dealing with limited data.\nExtensive experiments demonstrate that SimpDM matches or outperforms\nstate-of-the-art imputation methods across various scenarios.\n", "link": "http://arxiv.org/abs/2407.18013v1", "date": "2024-07-25", "relevancy": 1.57, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5498}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5198}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervision%20Improves%20Diffusion%20Models%20for%20Tabular%20Data%20Imputation&body=Title%3A%20Self-Supervision%20Improves%20Diffusion%20Models%20for%20Tabular%20Data%20Imputation%0AAuthor%3A%20Yixin%20Liu%20and%20Thalaiyasingam%20Ajanthan%20and%20Hisham%20Husain%20and%20Vu%20Nguyen%0AAbstract%3A%20%20%20The%20ubiquity%20of%20missing%20data%20has%20sparked%20considerable%20attention%20and%20focus%20on%0Atabular%20data%20imputation%20methods.%20Diffusion%20models%2C%20recognized%20as%20the%0Acutting-edge%20technique%20for%20data%20generation%2C%20demonstrate%20significant%20potential%0Ain%20tabular%20data%20imputation%20tasks.%20However%2C%20in%20pursuit%20of%20diversity%2C%20vanilla%0Adiffusion%20models%20often%20exhibit%20sensitivity%20to%20initialized%20noises%2C%20which%20hinders%0Athe%20models%20from%20generating%20stable%20and%20accurate%20imputation%20results.%0AAdditionally%2C%20the%20sparsity%20inherent%20in%20tabular%20data%20poses%20challenges%20for%0Adiffusion%20models%20in%20accurately%20modeling%20the%20data%20manifold%2C%20impacting%20the%0Arobustness%20of%20these%20models%20for%20data%20imputation.%20To%20tackle%20these%20challenges%2C%0Athis%20paper%20introduces%20an%20advanced%20diffusion%20model%20named%20Self-supervised%0Aimputation%20Diffusion%20Model%20%28SimpDM%20for%20brevity%29%2C%20specifically%20tailored%20for%0Atabular%20data%20imputation%20tasks.%20To%20mitigate%20sensitivity%20to%20noise%2C%20we%20introduce%20a%0Aself-supervised%20alignment%20mechanism%20that%20aims%20to%20regularize%20the%20model%2C%20ensuring%0Aconsistent%20and%20stable%20imputation%20predictions.%20Furthermore%2C%20we%20introduce%20a%0Acarefully%20devised%20state-dependent%20data%20augmentation%20strategy%20within%20SimpDM%2C%0Aenhancing%20the%20robustness%20of%20the%20diffusion%20model%20when%20dealing%20with%20limited%20data.%0AExtensive%20experiments%20demonstrate%20that%20SimpDM%20matches%20or%20outperforms%0Astate-of-the-art%20imputation%20methods%20across%20various%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervision%2520Improves%2520Diffusion%2520Models%2520for%2520Tabular%2520Data%2520Imputation%26entry.906535625%3DYixin%2520Liu%2520and%2520Thalaiyasingam%2520Ajanthan%2520and%2520Hisham%2520Husain%2520and%2520Vu%2520Nguyen%26entry.1292438233%3D%2520%2520The%2520ubiquity%2520of%2520missing%2520data%2520has%2520sparked%2520considerable%2520attention%2520and%2520focus%2520on%250Atabular%2520data%2520imputation%2520methods.%2520Diffusion%2520models%252C%2520recognized%2520as%2520the%250Acutting-edge%2520technique%2520for%2520data%2520generation%252C%2520demonstrate%2520significant%2520potential%250Ain%2520tabular%2520data%2520imputation%2520tasks.%2520However%252C%2520in%2520pursuit%2520of%2520diversity%252C%2520vanilla%250Adiffusion%2520models%2520often%2520exhibit%2520sensitivity%2520to%2520initialized%2520noises%252C%2520which%2520hinders%250Athe%2520models%2520from%2520generating%2520stable%2520and%2520accurate%2520imputation%2520results.%250AAdditionally%252C%2520the%2520sparsity%2520inherent%2520in%2520tabular%2520data%2520poses%2520challenges%2520for%250Adiffusion%2520models%2520in%2520accurately%2520modeling%2520the%2520data%2520manifold%252C%2520impacting%2520the%250Arobustness%2520of%2520these%2520models%2520for%2520data%2520imputation.%2520To%2520tackle%2520these%2520challenges%252C%250Athis%2520paper%2520introduces%2520an%2520advanced%2520diffusion%2520model%2520named%2520Self-supervised%250Aimputation%2520Diffusion%2520Model%2520%2528SimpDM%2520for%2520brevity%2529%252C%2520specifically%2520tailored%2520for%250Atabular%2520data%2520imputation%2520tasks.%2520To%2520mitigate%2520sensitivity%2520to%2520noise%252C%2520we%2520introduce%2520a%250Aself-supervised%2520alignment%2520mechanism%2520that%2520aims%2520to%2520regularize%2520the%2520model%252C%2520ensuring%250Aconsistent%2520and%2520stable%2520imputation%2520predictions.%2520Furthermore%252C%2520we%2520introduce%2520a%250Acarefully%2520devised%2520state-dependent%2520data%2520augmentation%2520strategy%2520within%2520SimpDM%252C%250Aenhancing%2520the%2520robustness%2520of%2520the%2520diffusion%2520model%2520when%2520dealing%2520with%2520limited%2520data.%250AExtensive%2520experiments%2520demonstrate%2520that%2520SimpDM%2520matches%2520or%2520outperforms%250Astate-of-the-art%2520imputation%2520methods%2520across%2520various%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervision%20Improves%20Diffusion%20Models%20for%20Tabular%20Data%20Imputation&entry.906535625=Yixin%20Liu%20and%20Thalaiyasingam%20Ajanthan%20and%20Hisham%20Husain%20and%20Vu%20Nguyen&entry.1292438233=%20%20The%20ubiquity%20of%20missing%20data%20has%20sparked%20considerable%20attention%20and%20focus%20on%0Atabular%20data%20imputation%20methods.%20Diffusion%20models%2C%20recognized%20as%20the%0Acutting-edge%20technique%20for%20data%20generation%2C%20demonstrate%20significant%20potential%0Ain%20tabular%20data%20imputation%20tasks.%20However%2C%20in%20pursuit%20of%20diversity%2C%20vanilla%0Adiffusion%20models%20often%20exhibit%20sensitivity%20to%20initialized%20noises%2C%20which%20hinders%0Athe%20models%20from%20generating%20stable%20and%20accurate%20imputation%20results.%0AAdditionally%2C%20the%20sparsity%20inherent%20in%20tabular%20data%20poses%20challenges%20for%0Adiffusion%20models%20in%20accurately%20modeling%20the%20data%20manifold%2C%20impacting%20the%0Arobustness%20of%20these%20models%20for%20data%20imputation.%20To%20tackle%20these%20challenges%2C%0Athis%20paper%20introduces%20an%20advanced%20diffusion%20model%20named%20Self-supervised%0Aimputation%20Diffusion%20Model%20%28SimpDM%20for%20brevity%29%2C%20specifically%20tailored%20for%0Atabular%20data%20imputation%20tasks.%20To%20mitigate%20sensitivity%20to%20noise%2C%20we%20introduce%20a%0Aself-supervised%20alignment%20mechanism%20that%20aims%20to%20regularize%20the%20model%2C%20ensuring%0Aconsistent%20and%20stable%20imputation%20predictions.%20Furthermore%2C%20we%20introduce%20a%0Acarefully%20devised%20state-dependent%20data%20augmentation%20strategy%20within%20SimpDM%2C%0Aenhancing%20the%20robustness%20of%20the%20diffusion%20model%20when%20dealing%20with%20limited%20data.%0AExtensive%20experiments%20demonstrate%20that%20SimpDM%20matches%20or%20outperforms%0Astate-of-the-art%20imputation%20methods%20across%20various%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18013v1&entry.124074799=Read"},
{"title": "Evaluating the design space of diffusion-based generative models", "author": "Yuqing Wang and Ye He and Molei Tao", "abstract": "  Most existing theoretical investigations of the accuracy of diffusion models,\nalbeit significant, assume the score function has been approximated to a\ncertain accuracy, and then use this a priori bound to control the error of\ngeneration. This article instead provides a first quantitative understanding of\nthe whole generation process, i.e., both training and sampling. More precisely,\nit conducts a non-asymptotic convergence analysis of denoising score matching\nunder gradient descent. In addition, a refined sampling error analysis for\nvariance exploding models is also provided. The combination of these two\nresults yields a full error analysis, which elucidates (again, but this time\ntheoretically) how to design the training and sampling processes for effective\ngeneration. For instance, our theory implies a preference toward noise\ndistribution and loss weighting in training that qualitatively agree with the\nones used in [Karras et al. 2022]. It also provides perspectives on the choices\nof time and variance schedules in sampling: when the score is well trained, the\ndesign in [Song et al. 2020] is more preferable, but when it is less trained,\nthe design in [Karras et al. 2022] becomes more preferable.\n", "link": "http://arxiv.org/abs/2406.12839v2", "date": "2024-07-25", "relevancy": 1.7106, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5909}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5659}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20design%20space%20of%20diffusion-based%20generative%20models&body=Title%3A%20Evaluating%20the%20design%20space%20of%20diffusion-based%20generative%20models%0AAuthor%3A%20Yuqing%20Wang%20and%20Ye%20He%20and%20Molei%20Tao%0AAbstract%3A%20%20%20Most%20existing%20theoretical%20investigations%20of%20the%20accuracy%20of%20diffusion%20models%2C%0Aalbeit%20significant%2C%20assume%20the%20score%20function%20has%20been%20approximated%20to%20a%0Acertain%20accuracy%2C%20and%20then%20use%20this%20a%20priori%20bound%20to%20control%20the%20error%20of%0Ageneration.%20This%20article%20instead%20provides%20a%20first%20quantitative%20understanding%20of%0Athe%20whole%20generation%20process%2C%20i.e.%2C%20both%20training%20and%20sampling.%20More%20precisely%2C%0Ait%20conducts%20a%20non-asymptotic%20convergence%20analysis%20of%20denoising%20score%20matching%0Aunder%20gradient%20descent.%20In%20addition%2C%20a%20refined%20sampling%20error%20analysis%20for%0Avariance%20exploding%20models%20is%20also%20provided.%20The%20combination%20of%20these%20two%0Aresults%20yields%20a%20full%20error%20analysis%2C%20which%20elucidates%20%28again%2C%20but%20this%20time%0Atheoretically%29%20how%20to%20design%20the%20training%20and%20sampling%20processes%20for%20effective%0Ageneration.%20For%20instance%2C%20our%20theory%20implies%20a%20preference%20toward%20noise%0Adistribution%20and%20loss%20weighting%20in%20training%20that%20qualitatively%20agree%20with%20the%0Aones%20used%20in%20%5BKarras%20et%20al.%202022%5D.%20It%20also%20provides%20perspectives%20on%20the%20choices%0Aof%20time%20and%20variance%20schedules%20in%20sampling%3A%20when%20the%20score%20is%20well%20trained%2C%20the%0Adesign%20in%20%5BSong%20et%20al.%202020%5D%20is%20more%20preferable%2C%20but%20when%20it%20is%20less%20trained%2C%0Athe%20design%20in%20%5BKarras%20et%20al.%202022%5D%20becomes%20more%20preferable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12839v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520design%2520space%2520of%2520diffusion-based%2520generative%2520models%26entry.906535625%3DYuqing%2520Wang%2520and%2520Ye%2520He%2520and%2520Molei%2520Tao%26entry.1292438233%3D%2520%2520Most%2520existing%2520theoretical%2520investigations%2520of%2520the%2520accuracy%2520of%2520diffusion%2520models%252C%250Aalbeit%2520significant%252C%2520assume%2520the%2520score%2520function%2520has%2520been%2520approximated%2520to%2520a%250Acertain%2520accuracy%252C%2520and%2520then%2520use%2520this%2520a%2520priori%2520bound%2520to%2520control%2520the%2520error%2520of%250Ageneration.%2520This%2520article%2520instead%2520provides%2520a%2520first%2520quantitative%2520understanding%2520of%250Athe%2520whole%2520generation%2520process%252C%2520i.e.%252C%2520both%2520training%2520and%2520sampling.%2520More%2520precisely%252C%250Ait%2520conducts%2520a%2520non-asymptotic%2520convergence%2520analysis%2520of%2520denoising%2520score%2520matching%250Aunder%2520gradient%2520descent.%2520In%2520addition%252C%2520a%2520refined%2520sampling%2520error%2520analysis%2520for%250Avariance%2520exploding%2520models%2520is%2520also%2520provided.%2520The%2520combination%2520of%2520these%2520two%250Aresults%2520yields%2520a%2520full%2520error%2520analysis%252C%2520which%2520elucidates%2520%2528again%252C%2520but%2520this%2520time%250Atheoretically%2529%2520how%2520to%2520design%2520the%2520training%2520and%2520sampling%2520processes%2520for%2520effective%250Ageneration.%2520For%2520instance%252C%2520our%2520theory%2520implies%2520a%2520preference%2520toward%2520noise%250Adistribution%2520and%2520loss%2520weighting%2520in%2520training%2520that%2520qualitatively%2520agree%2520with%2520the%250Aones%2520used%2520in%2520%255BKarras%2520et%2520al.%25202022%255D.%2520It%2520also%2520provides%2520perspectives%2520on%2520the%2520choices%250Aof%2520time%2520and%2520variance%2520schedules%2520in%2520sampling%253A%2520when%2520the%2520score%2520is%2520well%2520trained%252C%2520the%250Adesign%2520in%2520%255BSong%2520et%2520al.%25202020%255D%2520is%2520more%2520preferable%252C%2520but%2520when%2520it%2520is%2520less%2520trained%252C%250Athe%2520design%2520in%2520%255BKarras%2520et%2520al.%25202022%255D%2520becomes%2520more%2520preferable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12839v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20design%20space%20of%20diffusion-based%20generative%20models&entry.906535625=Yuqing%20Wang%20and%20Ye%20He%20and%20Molei%20Tao&entry.1292438233=%20%20Most%20existing%20theoretical%20investigations%20of%20the%20accuracy%20of%20diffusion%20models%2C%0Aalbeit%20significant%2C%20assume%20the%20score%20function%20has%20been%20approximated%20to%20a%0Acertain%20accuracy%2C%20and%20then%20use%20this%20a%20priori%20bound%20to%20control%20the%20error%20of%0Ageneration.%20This%20article%20instead%20provides%20a%20first%20quantitative%20understanding%20of%0Athe%20whole%20generation%20process%2C%20i.e.%2C%20both%20training%20and%20sampling.%20More%20precisely%2C%0Ait%20conducts%20a%20non-asymptotic%20convergence%20analysis%20of%20denoising%20score%20matching%0Aunder%20gradient%20descent.%20In%20addition%2C%20a%20refined%20sampling%20error%20analysis%20for%0Avariance%20exploding%20models%20is%20also%20provided.%20The%20combination%20of%20these%20two%0Aresults%20yields%20a%20full%20error%20analysis%2C%20which%20elucidates%20%28again%2C%20but%20this%20time%0Atheoretically%29%20how%20to%20design%20the%20training%20and%20sampling%20processes%20for%20effective%0Ageneration.%20For%20instance%2C%20our%20theory%20implies%20a%20preference%20toward%20noise%0Adistribution%20and%20loss%20weighting%20in%20training%20that%20qualitatively%20agree%20with%20the%0Aones%20used%20in%20%5BKarras%20et%20al.%202022%5D.%20It%20also%20provides%20perspectives%20on%20the%20choices%0Aof%20time%20and%20variance%20schedules%20in%20sampling%3A%20when%20the%20score%20is%20well%20trained%2C%20the%0Adesign%20in%20%5BSong%20et%20al.%202020%5D%20is%20more%20preferable%2C%20but%20when%20it%20is%20less%20trained%2C%0Athe%20design%20in%20%5BKarras%20et%20al.%202022%5D%20becomes%20more%20preferable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12839v2&entry.124074799=Read"},
{"title": "Lightweight Industrial Cohorted Federated Learning for Heterogeneous\n  Assets", "author": "Madapu Amarlingam and Abhishek Wani and Adarsh NL", "abstract": "  Federated Learning (FL) is the most widely adopted collaborative learning\napproach for training decentralized Machine Learning (ML) models by exchanging\nlearning between clients without sharing the data and compromising privacy.\nHowever, since great data similarity or homogeneity is taken for granted in all\nFL tasks, FL is still not specifically designed for the industrial setting.\nRarely this is the case in industrial data because there are differences in\nmachine type, firmware version, operational conditions, environmental factors,\nand hence, data distribution. Albeit its popularity, it has been observed that\nFL performance degrades if the clients have heterogeneous data distributions.\nTherefore, we propose a Lightweight Industrial Cohorted FL (LICFL) algorithm\nthat uses model parameters for cohorting without any additional on-edge\n(clientlevel) computations and communications than standard FL and mitigates\nthe shortcomings from data heterogeneity in industrial applications. Our\napproach enhances client-level model performance by allowing them to\ncollaborate with similar clients and train more specialized or personalized\nmodels. Also, we propose an adaptive aggregation algorithm that extends the\nLICFL to Adaptive LICFL (ALICFL) for further improving the global model\nperformance and speeding up the convergence. Through numerical experiments on\nreal-time data, we demonstrate the efficacy of the proposed algorithms and\ncompare the performance with existing approaches.\n", "link": "http://arxiv.org/abs/2407.17999v1", "date": "2024-07-25", "relevancy": 1.4107, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4895}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4668}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20Industrial%20Cohorted%20Federated%20Learning%20for%20Heterogeneous%0A%20%20Assets&body=Title%3A%20Lightweight%20Industrial%20Cohorted%20Federated%20Learning%20for%20Heterogeneous%0A%20%20Assets%0AAuthor%3A%20Madapu%20Amarlingam%20and%20Abhishek%20Wani%20and%20Adarsh%20NL%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20the%20most%20widely%20adopted%20collaborative%20learning%0Aapproach%20for%20training%20decentralized%20Machine%20Learning%20%28ML%29%20models%20by%20exchanging%0Alearning%20between%20clients%20without%20sharing%20the%20data%20and%20compromising%20privacy.%0AHowever%2C%20since%20great%20data%20similarity%20or%20homogeneity%20is%20taken%20for%20granted%20in%20all%0AFL%20tasks%2C%20FL%20is%20still%20not%20specifically%20designed%20for%20the%20industrial%20setting.%0ARarely%20this%20is%20the%20case%20in%20industrial%20data%20because%20there%20are%20differences%20in%0Amachine%20type%2C%20firmware%20version%2C%20operational%20conditions%2C%20environmental%20factors%2C%0Aand%20hence%2C%20data%20distribution.%20Albeit%20its%20popularity%2C%20it%20has%20been%20observed%20that%0AFL%20performance%20degrades%20if%20the%20clients%20have%20heterogeneous%20data%20distributions.%0ATherefore%2C%20we%20propose%20a%20Lightweight%20Industrial%20Cohorted%20FL%20%28LICFL%29%20algorithm%0Athat%20uses%20model%20parameters%20for%20cohorting%20without%20any%20additional%20on-edge%0A%28clientlevel%29%20computations%20and%20communications%20than%20standard%20FL%20and%20mitigates%0Athe%20shortcomings%20from%20data%20heterogeneity%20in%20industrial%20applications.%20Our%0Aapproach%20enhances%20client-level%20model%20performance%20by%20allowing%20them%20to%0Acollaborate%20with%20similar%20clients%20and%20train%20more%20specialized%20or%20personalized%0Amodels.%20Also%2C%20we%20propose%20an%20adaptive%20aggregation%20algorithm%20that%20extends%20the%0ALICFL%20to%20Adaptive%20LICFL%20%28ALICFL%29%20for%20further%20improving%20the%20global%20model%0Aperformance%20and%20speeding%20up%20the%20convergence.%20Through%20numerical%20experiments%20on%0Areal-time%20data%2C%20we%20demonstrate%20the%20efficacy%20of%20the%20proposed%20algorithms%20and%0Acompare%20the%20performance%20with%20existing%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520Industrial%2520Cohorted%2520Federated%2520Learning%2520for%2520Heterogeneous%250A%2520%2520Assets%26entry.906535625%3DMadapu%2520Amarlingam%2520and%2520Abhishek%2520Wani%2520and%2520Adarsh%2520NL%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520the%2520most%2520widely%2520adopted%2520collaborative%2520learning%250Aapproach%2520for%2520training%2520decentralized%2520Machine%2520Learning%2520%2528ML%2529%2520models%2520by%2520exchanging%250Alearning%2520between%2520clients%2520without%2520sharing%2520the%2520data%2520and%2520compromising%2520privacy.%250AHowever%252C%2520since%2520great%2520data%2520similarity%2520or%2520homogeneity%2520is%2520taken%2520for%2520granted%2520in%2520all%250AFL%2520tasks%252C%2520FL%2520is%2520still%2520not%2520specifically%2520designed%2520for%2520the%2520industrial%2520setting.%250ARarely%2520this%2520is%2520the%2520case%2520in%2520industrial%2520data%2520because%2520there%2520are%2520differences%2520in%250Amachine%2520type%252C%2520firmware%2520version%252C%2520operational%2520conditions%252C%2520environmental%2520factors%252C%250Aand%2520hence%252C%2520data%2520distribution.%2520Albeit%2520its%2520popularity%252C%2520it%2520has%2520been%2520observed%2520that%250AFL%2520performance%2520degrades%2520if%2520the%2520clients%2520have%2520heterogeneous%2520data%2520distributions.%250ATherefore%252C%2520we%2520propose%2520a%2520Lightweight%2520Industrial%2520Cohorted%2520FL%2520%2528LICFL%2529%2520algorithm%250Athat%2520uses%2520model%2520parameters%2520for%2520cohorting%2520without%2520any%2520additional%2520on-edge%250A%2528clientlevel%2529%2520computations%2520and%2520communications%2520than%2520standard%2520FL%2520and%2520mitigates%250Athe%2520shortcomings%2520from%2520data%2520heterogeneity%2520in%2520industrial%2520applications.%2520Our%250Aapproach%2520enhances%2520client-level%2520model%2520performance%2520by%2520allowing%2520them%2520to%250Acollaborate%2520with%2520similar%2520clients%2520and%2520train%2520more%2520specialized%2520or%2520personalized%250Amodels.%2520Also%252C%2520we%2520propose%2520an%2520adaptive%2520aggregation%2520algorithm%2520that%2520extends%2520the%250ALICFL%2520to%2520Adaptive%2520LICFL%2520%2528ALICFL%2529%2520for%2520further%2520improving%2520the%2520global%2520model%250Aperformance%2520and%2520speeding%2520up%2520the%2520convergence.%2520Through%2520numerical%2520experiments%2520on%250Areal-time%2520data%252C%2520we%2520demonstrate%2520the%2520efficacy%2520of%2520the%2520proposed%2520algorithms%2520and%250Acompare%2520the%2520performance%2520with%2520existing%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20Industrial%20Cohorted%20Federated%20Learning%20for%20Heterogeneous%0A%20%20Assets&entry.906535625=Madapu%20Amarlingam%20and%20Abhishek%20Wani%20and%20Adarsh%20NL&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20the%20most%20widely%20adopted%20collaborative%20learning%0Aapproach%20for%20training%20decentralized%20Machine%20Learning%20%28ML%29%20models%20by%20exchanging%0Alearning%20between%20clients%20without%20sharing%20the%20data%20and%20compromising%20privacy.%0AHowever%2C%20since%20great%20data%20similarity%20or%20homogeneity%20is%20taken%20for%20granted%20in%20all%0AFL%20tasks%2C%20FL%20is%20still%20not%20specifically%20designed%20for%20the%20industrial%20setting.%0ARarely%20this%20is%20the%20case%20in%20industrial%20data%20because%20there%20are%20differences%20in%0Amachine%20type%2C%20firmware%20version%2C%20operational%20conditions%2C%20environmental%20factors%2C%0Aand%20hence%2C%20data%20distribution.%20Albeit%20its%20popularity%2C%20it%20has%20been%20observed%20that%0AFL%20performance%20degrades%20if%20the%20clients%20have%20heterogeneous%20data%20distributions.%0ATherefore%2C%20we%20propose%20a%20Lightweight%20Industrial%20Cohorted%20FL%20%28LICFL%29%20algorithm%0Athat%20uses%20model%20parameters%20for%20cohorting%20without%20any%20additional%20on-edge%0A%28clientlevel%29%20computations%20and%20communications%20than%20standard%20FL%20and%20mitigates%0Athe%20shortcomings%20from%20data%20heterogeneity%20in%20industrial%20applications.%20Our%0Aapproach%20enhances%20client-level%20model%20performance%20by%20allowing%20them%20to%0Acollaborate%20with%20similar%20clients%20and%20train%20more%20specialized%20or%20personalized%0Amodels.%20Also%2C%20we%20propose%20an%20adaptive%20aggregation%20algorithm%20that%20extends%20the%0ALICFL%20to%20Adaptive%20LICFL%20%28ALICFL%29%20for%20further%20improving%20the%20global%20model%0Aperformance%20and%20speeding%20up%20the%20convergence.%20Through%20numerical%20experiments%20on%0Areal-time%20data%2C%20we%20demonstrate%20the%20efficacy%20of%20the%20proposed%20algorithms%20and%0Acompare%20the%20performance%20with%20existing%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17999v1&entry.124074799=Read"},
{"title": "Sparse vs Contiguous Adversarial Pixel Perturbations in Multimodal\n  Models: An Empirical Analysis", "author": "Cristian-Alexandru Botocan and Raphael Meier and Ljiljana Dolamic", "abstract": "  Assessing the robustness of multimodal models against adversarial examples is\nan important aspect for the safety of its users. We craft L0-norm perturbation\nattacks on the preprocessed input images. We launch them in a black-box setup\nagainst four multimodal models and two unimodal DNNs, considering both targeted\nand untargeted misclassification. Our attacks target less than 0.04% of\nperturbed image area and integrate different spatial positioning of perturbed\npixels: sparse positioning and pixels arranged in different contiguous shapes\n(row, column, diagonal, and patch). To the best of our knowledge, we are the\nfirst to assess the robustness of three state-of-the-art multimodal models\n(ALIGN, AltCLIP, GroupViT) against different sparse and contiguous pixel\ndistribution perturbations. The obtained results indicate that unimodal DNNs\nare more robust than multimodal models. Furthermore, models using CNN-based\nImage Encoder are more vulnerable than models with ViT - for untargeted\nattacks, we obtain a 99% success rate by perturbing less than 0.02% of the\nimage area.\n", "link": "http://arxiv.org/abs/2407.18251v1", "date": "2024-07-25", "relevancy": 1.6444, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5644}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.551}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20vs%20Contiguous%20Adversarial%20Pixel%20Perturbations%20in%20Multimodal%0A%20%20Models%3A%20An%20Empirical%20Analysis&body=Title%3A%20Sparse%20vs%20Contiguous%20Adversarial%20Pixel%20Perturbations%20in%20Multimodal%0A%20%20Models%3A%20An%20Empirical%20Analysis%0AAuthor%3A%20Cristian-Alexandru%20Botocan%20and%20Raphael%20Meier%20and%20Ljiljana%20Dolamic%0AAbstract%3A%20%20%20Assessing%20the%20robustness%20of%20multimodal%20models%20against%20adversarial%20examples%20is%0Aan%20important%20aspect%20for%20the%20safety%20of%20its%20users.%20We%20craft%20L0-norm%20perturbation%0Aattacks%20on%20the%20preprocessed%20input%20images.%20We%20launch%20them%20in%20a%20black-box%20setup%0Aagainst%20four%20multimodal%20models%20and%20two%20unimodal%20DNNs%2C%20considering%20both%20targeted%0Aand%20untargeted%20misclassification.%20Our%20attacks%20target%20less%20than%200.04%25%20of%0Aperturbed%20image%20area%20and%20integrate%20different%20spatial%20positioning%20of%20perturbed%0Apixels%3A%20sparse%20positioning%20and%20pixels%20arranged%20in%20different%20contiguous%20shapes%0A%28row%2C%20column%2C%20diagonal%2C%20and%20patch%29.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20are%20the%0Afirst%20to%20assess%20the%20robustness%20of%20three%20state-of-the-art%20multimodal%20models%0A%28ALIGN%2C%20AltCLIP%2C%20GroupViT%29%20against%20different%20sparse%20and%20contiguous%20pixel%0Adistribution%20perturbations.%20The%20obtained%20results%20indicate%20that%20unimodal%20DNNs%0Aare%20more%20robust%20than%20multimodal%20models.%20Furthermore%2C%20models%20using%20CNN-based%0AImage%20Encoder%20are%20more%20vulnerable%20than%20models%20with%20ViT%20-%20for%20untargeted%0Aattacks%2C%20we%20obtain%20a%2099%25%20success%20rate%20by%20perturbing%20less%20than%200.02%25%20of%20the%0Aimage%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.18251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520vs%2520Contiguous%2520Adversarial%2520Pixel%2520Perturbations%2520in%2520Multimodal%250A%2520%2520Models%253A%2520An%2520Empirical%2520Analysis%26entry.906535625%3DCristian-Alexandru%2520Botocan%2520and%2520Raphael%2520Meier%2520and%2520Ljiljana%2520Dolamic%26entry.1292438233%3D%2520%2520Assessing%2520the%2520robustness%2520of%2520multimodal%2520models%2520against%2520adversarial%2520examples%2520is%250Aan%2520important%2520aspect%2520for%2520the%2520safety%2520of%2520its%2520users.%2520We%2520craft%2520L0-norm%2520perturbation%250Aattacks%2520on%2520the%2520preprocessed%2520input%2520images.%2520We%2520launch%2520them%2520in%2520a%2520black-box%2520setup%250Aagainst%2520four%2520multimodal%2520models%2520and%2520two%2520unimodal%2520DNNs%252C%2520considering%2520both%2520targeted%250Aand%2520untargeted%2520misclassification.%2520Our%2520attacks%2520target%2520less%2520than%25200.04%2525%2520of%250Aperturbed%2520image%2520area%2520and%2520integrate%2520different%2520spatial%2520positioning%2520of%2520perturbed%250Apixels%253A%2520sparse%2520positioning%2520and%2520pixels%2520arranged%2520in%2520different%2520contiguous%2520shapes%250A%2528row%252C%2520column%252C%2520diagonal%252C%2520and%2520patch%2529.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520we%2520are%2520the%250Afirst%2520to%2520assess%2520the%2520robustness%2520of%2520three%2520state-of-the-art%2520multimodal%2520models%250A%2528ALIGN%252C%2520AltCLIP%252C%2520GroupViT%2529%2520against%2520different%2520sparse%2520and%2520contiguous%2520pixel%250Adistribution%2520perturbations.%2520The%2520obtained%2520results%2520indicate%2520that%2520unimodal%2520DNNs%250Aare%2520more%2520robust%2520than%2520multimodal%2520models.%2520Furthermore%252C%2520models%2520using%2520CNN-based%250AImage%2520Encoder%2520are%2520more%2520vulnerable%2520than%2520models%2520with%2520ViT%2520-%2520for%2520untargeted%250Aattacks%252C%2520we%2520obtain%2520a%252099%2525%2520success%2520rate%2520by%2520perturbing%2520less%2520than%25200.02%2525%2520of%2520the%250Aimage%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20vs%20Contiguous%20Adversarial%20Pixel%20Perturbations%20in%20Multimodal%0A%20%20Models%3A%20An%20Empirical%20Analysis&entry.906535625=Cristian-Alexandru%20Botocan%20and%20Raphael%20Meier%20and%20Ljiljana%20Dolamic&entry.1292438233=%20%20Assessing%20the%20robustness%20of%20multimodal%20models%20against%20adversarial%20examples%20is%0Aan%20important%20aspect%20for%20the%20safety%20of%20its%20users.%20We%20craft%20L0-norm%20perturbation%0Aattacks%20on%20the%20preprocessed%20input%20images.%20We%20launch%20them%20in%20a%20black-box%20setup%0Aagainst%20four%20multimodal%20models%20and%20two%20unimodal%20DNNs%2C%20considering%20both%20targeted%0Aand%20untargeted%20misclassification.%20Our%20attacks%20target%20less%20than%200.04%25%20of%0Aperturbed%20image%20area%20and%20integrate%20different%20spatial%20positioning%20of%20perturbed%0Apixels%3A%20sparse%20positioning%20and%20pixels%20arranged%20in%20different%20contiguous%20shapes%0A%28row%2C%20column%2C%20diagonal%2C%20and%20patch%29.%20To%20the%20best%20of%20our%20knowledge%2C%20we%20are%20the%0Afirst%20to%20assess%20the%20robustness%20of%20three%20state-of-the-art%20multimodal%20models%0A%28ALIGN%2C%20AltCLIP%2C%20GroupViT%29%20against%20different%20sparse%20and%20contiguous%20pixel%0Adistribution%20perturbations.%20The%20obtained%20results%20indicate%20that%20unimodal%20DNNs%0Aare%20more%20robust%20than%20multimodal%20models.%20Furthermore%2C%20models%20using%20CNN-based%0AImage%20Encoder%20are%20more%20vulnerable%20than%20models%20with%20ViT%20-%20for%20untargeted%0Aattacks%2C%20we%20obtain%20a%2099%25%20success%20rate%20by%20perturbing%20less%20than%200.02%25%20of%20the%0Aimage%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.18251v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


