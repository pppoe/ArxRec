<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240828.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Towards Realistic Example-based Modeling via 3D Gaussian Stitching", "author": "Xinyu Gao and Ziyi Yang and Bingchen Gong and Xiaoguang Han and Sipeng Yang and Xiaogang Jin", "abstract": "  Using parts of existing models to rebuild new models, commonly termed as\nexample-based modeling, is a classical methodology in the realm of computer\ngraphics. Previous works mostly focus on shape composition, making them very\nhard to use for realistic composition of 3D objects captured from real-world\nscenes. This leads to combining multiple NeRFs into a single 3D scene to\nachieve seamless appearance blending. However, the current SeamlessNeRF method\nstruggles to achieve interactive editing and harmonious stitching for\nreal-world scenes due to its gradient-based strategy and grid-based\nrepresentation. To this end, we present an example-based modeling method that\ncombines multiple Gaussian fields in a point-based representation using\nsample-guided synthesis. Specifically, as for composition, we create a GUI to\nsegment and transform multiple fields in real time, easily obtaining a\nsemantically meaningful composition of models represented by 3D Gaussian\nSplatting (3DGS). For texture blending, due to the discrete and irregular\nnature of 3DGS, straightforwardly applying gradient propagation as SeamlssNeRF\nis not supported. Thus, a novel sampling-based cloning method is proposed to\nharmonize the blending while preserving the original rich texture and content.\nOur workflow consists of three steps: 1) real-time segmentation and\ntransformation of a Gaussian model using a well-tailored GUI, 2) KNN analysis\nto identify boundary points in the intersecting area between the source and\ntarget models, and 3) two-phase optimization of the target model using\nsampling-based cloning and gradient constraints. Extensive experimental results\nvalidate that our approach significantly outperforms previous works in terms of\nrealistic synthesis, demonstrating its practicality. More demos are available\nat https://ingra14m.github.io/gs_stitching_website.\n", "link": "http://arxiv.org/abs/2408.15708v1", "date": "2024-08-28", "relevancy": 3.1649, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6592}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6312}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6086}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Realistic%20Example-based%20Modeling%20via%203D%20Gaussian%20Stitching&body=Title%3A%20Towards%20Realistic%20Example-based%20Modeling%20via%203D%20Gaussian%20Stitching%0AAuthor%3A%20Xinyu%20Gao%20and%20Ziyi%20Yang%20and%20Bingchen%20Gong%20and%20Xiaoguang%20Han%20and%20Sipeng%20Yang%20and%20Xiaogang%20Jin%0AAbstract%3A%20%20%20Using%20parts%20of%20existing%20models%20to%20rebuild%20new%20models%2C%20commonly%20termed%20as%0Aexample-based%20modeling%2C%20is%20a%20classical%20methodology%20in%20the%20realm%20of%20computer%0Agraphics.%20Previous%20works%20mostly%20focus%20on%20shape%20composition%2C%20making%20them%20very%0Ahard%20to%20use%20for%20realistic%20composition%20of%203D%20objects%20captured%20from%20real-world%0Ascenes.%20This%20leads%20to%20combining%20multiple%20NeRFs%20into%20a%20single%203D%20scene%20to%0Aachieve%20seamless%20appearance%20blending.%20However%2C%20the%20current%20SeamlessNeRF%20method%0Astruggles%20to%20achieve%20interactive%20editing%20and%20harmonious%20stitching%20for%0Areal-world%20scenes%20due%20to%20its%20gradient-based%20strategy%20and%20grid-based%0Arepresentation.%20To%20this%20end%2C%20we%20present%20an%20example-based%20modeling%20method%20that%0Acombines%20multiple%20Gaussian%20fields%20in%20a%20point-based%20representation%20using%0Asample-guided%20synthesis.%20Specifically%2C%20as%20for%20composition%2C%20we%20create%20a%20GUI%20to%0Asegment%20and%20transform%20multiple%20fields%20in%20real%20time%2C%20easily%20obtaining%20a%0Asemantically%20meaningful%20composition%20of%20models%20represented%20by%203D%20Gaussian%0ASplatting%20%283DGS%29.%20For%20texture%20blending%2C%20due%20to%20the%20discrete%20and%20irregular%0Anature%20of%203DGS%2C%20straightforwardly%20applying%20gradient%20propagation%20as%20SeamlssNeRF%0Ais%20not%20supported.%20Thus%2C%20a%20novel%20sampling-based%20cloning%20method%20is%20proposed%20to%0Aharmonize%20the%20blending%20while%20preserving%20the%20original%20rich%20texture%20and%20content.%0AOur%20workflow%20consists%20of%20three%20steps%3A%201%29%20real-time%20segmentation%20and%0Atransformation%20of%20a%20Gaussian%20model%20using%20a%20well-tailored%20GUI%2C%202%29%20KNN%20analysis%0Ato%20identify%20boundary%20points%20in%20the%20intersecting%20area%20between%20the%20source%20and%0Atarget%20models%2C%20and%203%29%20two-phase%20optimization%20of%20the%20target%20model%20using%0Asampling-based%20cloning%20and%20gradient%20constraints.%20Extensive%20experimental%20results%0Avalidate%20that%20our%20approach%20significantly%20outperforms%20previous%20works%20in%20terms%20of%0Arealistic%20synthesis%2C%20demonstrating%20its%20practicality.%20More%20demos%20are%20available%0Aat%20https%3A//ingra14m.github.io/gs_stitching_website.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15708v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Realistic%2520Example-based%2520Modeling%2520via%25203D%2520Gaussian%2520Stitching%26entry.906535625%3DXinyu%2520Gao%2520and%2520Ziyi%2520Yang%2520and%2520Bingchen%2520Gong%2520and%2520Xiaoguang%2520Han%2520and%2520Sipeng%2520Yang%2520and%2520Xiaogang%2520Jin%26entry.1292438233%3D%2520%2520Using%2520parts%2520of%2520existing%2520models%2520to%2520rebuild%2520new%2520models%252C%2520commonly%2520termed%2520as%250Aexample-based%2520modeling%252C%2520is%2520a%2520classical%2520methodology%2520in%2520the%2520realm%2520of%2520computer%250Agraphics.%2520Previous%2520works%2520mostly%2520focus%2520on%2520shape%2520composition%252C%2520making%2520them%2520very%250Ahard%2520to%2520use%2520for%2520realistic%2520composition%2520of%25203D%2520objects%2520captured%2520from%2520real-world%250Ascenes.%2520This%2520leads%2520to%2520combining%2520multiple%2520NeRFs%2520into%2520a%2520single%25203D%2520scene%2520to%250Aachieve%2520seamless%2520appearance%2520blending.%2520However%252C%2520the%2520current%2520SeamlessNeRF%2520method%250Astruggles%2520to%2520achieve%2520interactive%2520editing%2520and%2520harmonious%2520stitching%2520for%250Areal-world%2520scenes%2520due%2520to%2520its%2520gradient-based%2520strategy%2520and%2520grid-based%250Arepresentation.%2520To%2520this%2520end%252C%2520we%2520present%2520an%2520example-based%2520modeling%2520method%2520that%250Acombines%2520multiple%2520Gaussian%2520fields%2520in%2520a%2520point-based%2520representation%2520using%250Asample-guided%2520synthesis.%2520Specifically%252C%2520as%2520for%2520composition%252C%2520we%2520create%2520a%2520GUI%2520to%250Asegment%2520and%2520transform%2520multiple%2520fields%2520in%2520real%2520time%252C%2520easily%2520obtaining%2520a%250Asemantically%2520meaningful%2520composition%2520of%2520models%2520represented%2520by%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529.%2520For%2520texture%2520blending%252C%2520due%2520to%2520the%2520discrete%2520and%2520irregular%250Anature%2520of%25203DGS%252C%2520straightforwardly%2520applying%2520gradient%2520propagation%2520as%2520SeamlssNeRF%250Ais%2520not%2520supported.%2520Thus%252C%2520a%2520novel%2520sampling-based%2520cloning%2520method%2520is%2520proposed%2520to%250Aharmonize%2520the%2520blending%2520while%2520preserving%2520the%2520original%2520rich%2520texture%2520and%2520content.%250AOur%2520workflow%2520consists%2520of%2520three%2520steps%253A%25201%2529%2520real-time%2520segmentation%2520and%250Atransformation%2520of%2520a%2520Gaussian%2520model%2520using%2520a%2520well-tailored%2520GUI%252C%25202%2529%2520KNN%2520analysis%250Ato%2520identify%2520boundary%2520points%2520in%2520the%2520intersecting%2520area%2520between%2520the%2520source%2520and%250Atarget%2520models%252C%2520and%25203%2529%2520two-phase%2520optimization%2520of%2520the%2520target%2520model%2520using%250Asampling-based%2520cloning%2520and%2520gradient%2520constraints.%2520Extensive%2520experimental%2520results%250Avalidate%2520that%2520our%2520approach%2520significantly%2520outperforms%2520previous%2520works%2520in%2520terms%2520of%250Arealistic%2520synthesis%252C%2520demonstrating%2520its%2520practicality.%2520More%2520demos%2520are%2520available%250Aat%2520https%253A//ingra14m.github.io/gs_stitching_website.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15708v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Realistic%20Example-based%20Modeling%20via%203D%20Gaussian%20Stitching&entry.906535625=Xinyu%20Gao%20and%20Ziyi%20Yang%20and%20Bingchen%20Gong%20and%20Xiaoguang%20Han%20and%20Sipeng%20Yang%20and%20Xiaogang%20Jin&entry.1292438233=%20%20Using%20parts%20of%20existing%20models%20to%20rebuild%20new%20models%2C%20commonly%20termed%20as%0Aexample-based%20modeling%2C%20is%20a%20classical%20methodology%20in%20the%20realm%20of%20computer%0Agraphics.%20Previous%20works%20mostly%20focus%20on%20shape%20composition%2C%20making%20them%20very%0Ahard%20to%20use%20for%20realistic%20composition%20of%203D%20objects%20captured%20from%20real-world%0Ascenes.%20This%20leads%20to%20combining%20multiple%20NeRFs%20into%20a%20single%203D%20scene%20to%0Aachieve%20seamless%20appearance%20blending.%20However%2C%20the%20current%20SeamlessNeRF%20method%0Astruggles%20to%20achieve%20interactive%20editing%20and%20harmonious%20stitching%20for%0Areal-world%20scenes%20due%20to%20its%20gradient-based%20strategy%20and%20grid-based%0Arepresentation.%20To%20this%20end%2C%20we%20present%20an%20example-based%20modeling%20method%20that%0Acombines%20multiple%20Gaussian%20fields%20in%20a%20point-based%20representation%20using%0Asample-guided%20synthesis.%20Specifically%2C%20as%20for%20composition%2C%20we%20create%20a%20GUI%20to%0Asegment%20and%20transform%20multiple%20fields%20in%20real%20time%2C%20easily%20obtaining%20a%0Asemantically%20meaningful%20composition%20of%20models%20represented%20by%203D%20Gaussian%0ASplatting%20%283DGS%29.%20For%20texture%20blending%2C%20due%20to%20the%20discrete%20and%20irregular%0Anature%20of%203DGS%2C%20straightforwardly%20applying%20gradient%20propagation%20as%20SeamlssNeRF%0Ais%20not%20supported.%20Thus%2C%20a%20novel%20sampling-based%20cloning%20method%20is%20proposed%20to%0Aharmonize%20the%20blending%20while%20preserving%20the%20original%20rich%20texture%20and%20content.%0AOur%20workflow%20consists%20of%20three%20steps%3A%201%29%20real-time%20segmentation%20and%0Atransformation%20of%20a%20Gaussian%20model%20using%20a%20well-tailored%20GUI%2C%202%29%20KNN%20analysis%0Ato%20identify%20boundary%20points%20in%20the%20intersecting%20area%20between%20the%20source%20and%0Atarget%20models%2C%20and%203%29%20two-phase%20optimization%20of%20the%20target%20model%20using%0Asampling-based%20cloning%20and%20gradient%20constraints.%20Extensive%20experimental%20results%0Avalidate%20that%20our%20approach%20significantly%20outperforms%20previous%20works%20in%20terms%20of%0Arealistic%20synthesis%2C%20demonstrating%20its%20practicality.%20More%20demos%20are%20available%0Aat%20https%3A//ingra14m.github.io/gs_stitching_website.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15708v1&entry.124074799=Read"},
{"title": "SLAM2REF: Advancing Long-Term Mapping with 3D LiDAR and Reference Map\n  Integration for Precise 6-DoF Trajectory Estimation and Map Extension", "author": "Miguel Arturo Vega Torres and Alexander Braun and Andr\u00e9 Borrmann", "abstract": "  This paper presents a pioneering solution to the task of integrating mobile\n3D LiDAR and inertial measurement unit (IMU) data with existing building\ninformation models or point clouds, which is crucial for achieving precise\nlong-term localization and mapping in indoor, GPS-denied environments. Our\nproposed framework, SLAM2REF, introduces a novel approach for automatic\nalignment and map extension utilizing reference 3D maps. The methodology is\nsupported by a sophisticated multi-session anchoring technique, which\nintegrates novel descriptors and registration methodologies. Real-world\nexperiments reveal the framework's remarkable robustness and accuracy,\nsurpassing current state-of-the-art methods. Our open-source framework's\nsignificance lies in its contribution to resilient map data management,\nenhancing processes across diverse sectors such as construction site\nmonitoring, emergency response, disaster management, and others, where\nfast-updated digital 3D maps contribute to better decision-making and\nproductivity. Moreover, it offers advancements in localization and mapping\nresearch. Link to the repository: https://github.com/MigVega/SLAM2REF, Data:\nhttps://doi.org/10.14459/2024mp1743877.\n", "link": "http://arxiv.org/abs/2408.15948v1", "date": "2024-08-28", "relevancy": 3.1386, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6618}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6324}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLAM2REF%3A%20Advancing%20Long-Term%20Mapping%20with%203D%20LiDAR%20and%20Reference%20Map%0A%20%20Integration%20for%20Precise%206-DoF%20Trajectory%20Estimation%20and%20Map%20Extension&body=Title%3A%20SLAM2REF%3A%20Advancing%20Long-Term%20Mapping%20with%203D%20LiDAR%20and%20Reference%20Map%0A%20%20Integration%20for%20Precise%206-DoF%20Trajectory%20Estimation%20and%20Map%20Extension%0AAuthor%3A%20Miguel%20Arturo%20Vega%20Torres%20and%20Alexander%20Braun%20and%20Andr%C3%A9%20Borrmann%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20pioneering%20solution%20to%20the%20task%20of%20integrating%20mobile%0A3D%20LiDAR%20and%20inertial%20measurement%20unit%20%28IMU%29%20data%20with%20existing%20building%0Ainformation%20models%20or%20point%20clouds%2C%20which%20is%20crucial%20for%20achieving%20precise%0Along-term%20localization%20and%20mapping%20in%20indoor%2C%20GPS-denied%20environments.%20Our%0Aproposed%20framework%2C%20SLAM2REF%2C%20introduces%20a%20novel%20approach%20for%20automatic%0Aalignment%20and%20map%20extension%20utilizing%20reference%203D%20maps.%20The%20methodology%20is%0Asupported%20by%20a%20sophisticated%20multi-session%20anchoring%20technique%2C%20which%0Aintegrates%20novel%20descriptors%20and%20registration%20methodologies.%20Real-world%0Aexperiments%20reveal%20the%20framework%27s%20remarkable%20robustness%20and%20accuracy%2C%0Asurpassing%20current%20state-of-the-art%20methods.%20Our%20open-source%20framework%27s%0Asignificance%20lies%20in%20its%20contribution%20to%20resilient%20map%20data%20management%2C%0Aenhancing%20processes%20across%20diverse%20sectors%20such%20as%20construction%20site%0Amonitoring%2C%20emergency%20response%2C%20disaster%20management%2C%20and%20others%2C%20where%0Afast-updated%20digital%203D%20maps%20contribute%20to%20better%20decision-making%20and%0Aproductivity.%20Moreover%2C%20it%20offers%20advancements%20in%20localization%20and%20mapping%0Aresearch.%20Link%20to%20the%20repository%3A%20https%3A//github.com/MigVega/SLAM2REF%2C%20Data%3A%0Ahttps%3A//doi.org/10.14459/2024mp1743877.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15948v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLAM2REF%253A%2520Advancing%2520Long-Term%2520Mapping%2520with%25203D%2520LiDAR%2520and%2520Reference%2520Map%250A%2520%2520Integration%2520for%2520Precise%25206-DoF%2520Trajectory%2520Estimation%2520and%2520Map%2520Extension%26entry.906535625%3DMiguel%2520Arturo%2520Vega%2520Torres%2520and%2520Alexander%2520Braun%2520and%2520Andr%25C3%25A9%2520Borrmann%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520pioneering%2520solution%2520to%2520the%2520task%2520of%2520integrating%2520mobile%250A3D%2520LiDAR%2520and%2520inertial%2520measurement%2520unit%2520%2528IMU%2529%2520data%2520with%2520existing%2520building%250Ainformation%2520models%2520or%2520point%2520clouds%252C%2520which%2520is%2520crucial%2520for%2520achieving%2520precise%250Along-term%2520localization%2520and%2520mapping%2520in%2520indoor%252C%2520GPS-denied%2520environments.%2520Our%250Aproposed%2520framework%252C%2520SLAM2REF%252C%2520introduces%2520a%2520novel%2520approach%2520for%2520automatic%250Aalignment%2520and%2520map%2520extension%2520utilizing%2520reference%25203D%2520maps.%2520The%2520methodology%2520is%250Asupported%2520by%2520a%2520sophisticated%2520multi-session%2520anchoring%2520technique%252C%2520which%250Aintegrates%2520novel%2520descriptors%2520and%2520registration%2520methodologies.%2520Real-world%250Aexperiments%2520reveal%2520the%2520framework%2527s%2520remarkable%2520robustness%2520and%2520accuracy%252C%250Asurpassing%2520current%2520state-of-the-art%2520methods.%2520Our%2520open-source%2520framework%2527s%250Asignificance%2520lies%2520in%2520its%2520contribution%2520to%2520resilient%2520map%2520data%2520management%252C%250Aenhancing%2520processes%2520across%2520diverse%2520sectors%2520such%2520as%2520construction%2520site%250Amonitoring%252C%2520emergency%2520response%252C%2520disaster%2520management%252C%2520and%2520others%252C%2520where%250Afast-updated%2520digital%25203D%2520maps%2520contribute%2520to%2520better%2520decision-making%2520and%250Aproductivity.%2520Moreover%252C%2520it%2520offers%2520advancements%2520in%2520localization%2520and%2520mapping%250Aresearch.%2520Link%2520to%2520the%2520repository%253A%2520https%253A//github.com/MigVega/SLAM2REF%252C%2520Data%253A%250Ahttps%253A//doi.org/10.14459/2024mp1743877.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15948v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLAM2REF%3A%20Advancing%20Long-Term%20Mapping%20with%203D%20LiDAR%20and%20Reference%20Map%0A%20%20Integration%20for%20Precise%206-DoF%20Trajectory%20Estimation%20and%20Map%20Extension&entry.906535625=Miguel%20Arturo%20Vega%20Torres%20and%20Alexander%20Braun%20and%20Andr%C3%A9%20Borrmann&entry.1292438233=%20%20This%20paper%20presents%20a%20pioneering%20solution%20to%20the%20task%20of%20integrating%20mobile%0A3D%20LiDAR%20and%20inertial%20measurement%20unit%20%28IMU%29%20data%20with%20existing%20building%0Ainformation%20models%20or%20point%20clouds%2C%20which%20is%20crucial%20for%20achieving%20precise%0Along-term%20localization%20and%20mapping%20in%20indoor%2C%20GPS-denied%20environments.%20Our%0Aproposed%20framework%2C%20SLAM2REF%2C%20introduces%20a%20novel%20approach%20for%20automatic%0Aalignment%20and%20map%20extension%20utilizing%20reference%203D%20maps.%20The%20methodology%20is%0Asupported%20by%20a%20sophisticated%20multi-session%20anchoring%20technique%2C%20which%0Aintegrates%20novel%20descriptors%20and%20registration%20methodologies.%20Real-world%0Aexperiments%20reveal%20the%20framework%27s%20remarkable%20robustness%20and%20accuracy%2C%0Asurpassing%20current%20state-of-the-art%20methods.%20Our%20open-source%20framework%27s%0Asignificance%20lies%20in%20its%20contribution%20to%20resilient%20map%20data%20management%2C%0Aenhancing%20processes%20across%20diverse%20sectors%20such%20as%20construction%20site%0Amonitoring%2C%20emergency%20response%2C%20disaster%20management%2C%20and%20others%2C%20where%0Afast-updated%20digital%203D%20maps%20contribute%20to%20better%20decision-making%20and%0Aproductivity.%20Moreover%2C%20it%20offers%20advancements%20in%20localization%20and%20mapping%0Aresearch.%20Link%20to%20the%20repository%3A%20https%3A//github.com/MigVega/SLAM2REF%2C%20Data%3A%0Ahttps%3A//doi.org/10.14459/2024mp1743877.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15948v1&entry.124074799=Read"},
{"title": "G-Style: Stylized Gaussian Splatting", "author": "\u00c1ron Samuel Kov\u00e1cs and Pedro Hermosilla and Renata G. Raidou", "abstract": "  We introduce G-Style, a novel algorithm designed to transfer the style of an\nimage onto a 3D scene represented using Gaussian Splatting. Gaussian Splatting\nis a powerful 3D representation for novel view synthesis, as -- compared to\nother approaches based on Neural Radiance Fields -- it provides fast scene\nrenderings and user control over the scene. Recent pre-prints have demonstrated\nthat the style of Gaussian Splatting scenes can be modified using an image\nexemplar. However, since the scene geometry remains fixed during the\nstylization process, current solutions fall short of producing satisfactory\nresults. Our algorithm aims to address these limitations by following a\nthree-step process: In a pre-processing step, we remove undesirable Gaussians\nwith large projection areas or highly elongated shapes. Subsequently, we\ncombine several losses carefully designed to preserve different scales of the\nstyle in the image, while maintaining as much as possible the integrity of the\noriginal scene content. During the stylization process and following the\noriginal design of Gaussian Splatting, we split Gaussians where additional\ndetail is necessary within our scene by tracking the gradient of the stylized\ncolor. Our experiments demonstrate that G-Style generates high-quality\nstylizations within just a few minutes, outperforming existing methods both\nqualitatively and quantitatively.\n", "link": "http://arxiv.org/abs/2408.15695v1", "date": "2024-08-28", "relevancy": 3.0255, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6545}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5804}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G-Style%3A%20Stylized%20Gaussian%20Splatting&body=Title%3A%20G-Style%3A%20Stylized%20Gaussian%20Splatting%0AAuthor%3A%20%C3%81ron%20Samuel%20Kov%C3%A1cs%20and%20Pedro%20Hermosilla%20and%20Renata%20G.%20Raidou%0AAbstract%3A%20%20%20We%20introduce%20G-Style%2C%20a%20novel%20algorithm%20designed%20to%20transfer%20the%20style%20of%20an%0Aimage%20onto%20a%203D%20scene%20represented%20using%20Gaussian%20Splatting.%20Gaussian%20Splatting%0Ais%20a%20powerful%203D%20representation%20for%20novel%20view%20synthesis%2C%20as%20--%20compared%20to%0Aother%20approaches%20based%20on%20Neural%20Radiance%20Fields%20--%20it%20provides%20fast%20scene%0Arenderings%20and%20user%20control%20over%20the%20scene.%20Recent%20pre-prints%20have%20demonstrated%0Athat%20the%20style%20of%20Gaussian%20Splatting%20scenes%20can%20be%20modified%20using%20an%20image%0Aexemplar.%20However%2C%20since%20the%20scene%20geometry%20remains%20fixed%20during%20the%0Astylization%20process%2C%20current%20solutions%20fall%20short%20of%20producing%20satisfactory%0Aresults.%20Our%20algorithm%20aims%20to%20address%20these%20limitations%20by%20following%20a%0Athree-step%20process%3A%20In%20a%20pre-processing%20step%2C%20we%20remove%20undesirable%20Gaussians%0Awith%20large%20projection%20areas%20or%20highly%20elongated%20shapes.%20Subsequently%2C%20we%0Acombine%20several%20losses%20carefully%20designed%20to%20preserve%20different%20scales%20of%20the%0Astyle%20in%20the%20image%2C%20while%20maintaining%20as%20much%20as%20possible%20the%20integrity%20of%20the%0Aoriginal%20scene%20content.%20During%20the%20stylization%20process%20and%20following%20the%0Aoriginal%20design%20of%20Gaussian%20Splatting%2C%20we%20split%20Gaussians%20where%20additional%0Adetail%20is%20necessary%20within%20our%20scene%20by%20tracking%20the%20gradient%20of%20the%20stylized%0Acolor.%20Our%20experiments%20demonstrate%20that%20G-Style%20generates%20high-quality%0Astylizations%20within%20just%20a%20few%20minutes%2C%20outperforming%20existing%20methods%20both%0Aqualitatively%20and%20quantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15695v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG-Style%253A%2520Stylized%2520Gaussian%2520Splatting%26entry.906535625%3D%25C3%2581ron%2520Samuel%2520Kov%25C3%25A1cs%2520and%2520Pedro%2520Hermosilla%2520and%2520Renata%2520G.%2520Raidou%26entry.1292438233%3D%2520%2520We%2520introduce%2520G-Style%252C%2520a%2520novel%2520algorithm%2520designed%2520to%2520transfer%2520the%2520style%2520of%2520an%250Aimage%2520onto%2520a%25203D%2520scene%2520represented%2520using%2520Gaussian%2520Splatting.%2520Gaussian%2520Splatting%250Ais%2520a%2520powerful%25203D%2520representation%2520for%2520novel%2520view%2520synthesis%252C%2520as%2520--%2520compared%2520to%250Aother%2520approaches%2520based%2520on%2520Neural%2520Radiance%2520Fields%2520--%2520it%2520provides%2520fast%2520scene%250Arenderings%2520and%2520user%2520control%2520over%2520the%2520scene.%2520Recent%2520pre-prints%2520have%2520demonstrated%250Athat%2520the%2520style%2520of%2520Gaussian%2520Splatting%2520scenes%2520can%2520be%2520modified%2520using%2520an%2520image%250Aexemplar.%2520However%252C%2520since%2520the%2520scene%2520geometry%2520remains%2520fixed%2520during%2520the%250Astylization%2520process%252C%2520current%2520solutions%2520fall%2520short%2520of%2520producing%2520satisfactory%250Aresults.%2520Our%2520algorithm%2520aims%2520to%2520address%2520these%2520limitations%2520by%2520following%2520a%250Athree-step%2520process%253A%2520In%2520a%2520pre-processing%2520step%252C%2520we%2520remove%2520undesirable%2520Gaussians%250Awith%2520large%2520projection%2520areas%2520or%2520highly%2520elongated%2520shapes.%2520Subsequently%252C%2520we%250Acombine%2520several%2520losses%2520carefully%2520designed%2520to%2520preserve%2520different%2520scales%2520of%2520the%250Astyle%2520in%2520the%2520image%252C%2520while%2520maintaining%2520as%2520much%2520as%2520possible%2520the%2520integrity%2520of%2520the%250Aoriginal%2520scene%2520content.%2520During%2520the%2520stylization%2520process%2520and%2520following%2520the%250Aoriginal%2520design%2520of%2520Gaussian%2520Splatting%252C%2520we%2520split%2520Gaussians%2520where%2520additional%250Adetail%2520is%2520necessary%2520within%2520our%2520scene%2520by%2520tracking%2520the%2520gradient%2520of%2520the%2520stylized%250Acolor.%2520Our%2520experiments%2520demonstrate%2520that%2520G-Style%2520generates%2520high-quality%250Astylizations%2520within%2520just%2520a%2520few%2520minutes%252C%2520outperforming%2520existing%2520methods%2520both%250Aqualitatively%2520and%2520quantitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15695v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G-Style%3A%20Stylized%20Gaussian%20Splatting&entry.906535625=%C3%81ron%20Samuel%20Kov%C3%A1cs%20and%20Pedro%20Hermosilla%20and%20Renata%20G.%20Raidou&entry.1292438233=%20%20We%20introduce%20G-Style%2C%20a%20novel%20algorithm%20designed%20to%20transfer%20the%20style%20of%20an%0Aimage%20onto%20a%203D%20scene%20represented%20using%20Gaussian%20Splatting.%20Gaussian%20Splatting%0Ais%20a%20powerful%203D%20representation%20for%20novel%20view%20synthesis%2C%20as%20--%20compared%20to%0Aother%20approaches%20based%20on%20Neural%20Radiance%20Fields%20--%20it%20provides%20fast%20scene%0Arenderings%20and%20user%20control%20over%20the%20scene.%20Recent%20pre-prints%20have%20demonstrated%0Athat%20the%20style%20of%20Gaussian%20Splatting%20scenes%20can%20be%20modified%20using%20an%20image%0Aexemplar.%20However%2C%20since%20the%20scene%20geometry%20remains%20fixed%20during%20the%0Astylization%20process%2C%20current%20solutions%20fall%20short%20of%20producing%20satisfactory%0Aresults.%20Our%20algorithm%20aims%20to%20address%20these%20limitations%20by%20following%20a%0Athree-step%20process%3A%20In%20a%20pre-processing%20step%2C%20we%20remove%20undesirable%20Gaussians%0Awith%20large%20projection%20areas%20or%20highly%20elongated%20shapes.%20Subsequently%2C%20we%0Acombine%20several%20losses%20carefully%20designed%20to%20preserve%20different%20scales%20of%20the%0Astyle%20in%20the%20image%2C%20while%20maintaining%20as%20much%20as%20possible%20the%20integrity%20of%20the%0Aoriginal%20scene%20content.%20During%20the%20stylization%20process%20and%20following%20the%0Aoriginal%20design%20of%20Gaussian%20Splatting%2C%20we%20split%20Gaussians%20where%20additional%0Adetail%20is%20necessary%20within%20our%20scene%20by%20tracking%20the%20gradient%20of%20the%20stylized%0Acolor.%20Our%20experiments%20demonstrate%20that%20G-Style%20generates%20high-quality%0Astylizations%20within%20just%20a%20few%20minutes%2C%20outperforming%20existing%20methods%20both%0Aqualitatively%20and%20quantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15695v1&entry.124074799=Read"},
{"title": "DiffAge3D: Diffusion-based 3D-aware Face Aging", "author": "Junaid Wahid and Fangneng Zhan and Pramod Rao and Christian Theobalt", "abstract": "  Face aging is the process of converting an individual's appearance to a\nyounger or older version of themselves. Existing face aging techniques have\nbeen limited to 2D settings, which often weaken their applications as there is\na growing demand for 3D face modeling. Moreover, existing aging methods\nstruggle to perform faithful aging, maintain identity, and retain the fine\ndetails of the input images. Given these limitations and the need for a\n3D-aware aging method, we propose DiffAge3D, the first 3D-aware aging framework\nthat not only performs faithful aging and identity preservation but also\noperates in a 3D setting. Our aging framework allows to model the aging and\ncamera pose separately by only taking a single image with a target age. Our\nframework includes a robust 3D-aware aging dataset generation pipeline by\nutilizing a pre-trained 3D GAN and the rich text embedding capabilities within\nCLIP model. Notably, we do not employ any inversion bottleneck in dataset\ngeneration. Instead, we randomly generate training samples from the latent\nspace of 3D GAN, allowing us to manipulate the rich latent space of GAN to\ngenerate ages even with large gaps. With the generated dataset, we train a\nviewpoint-aware diffusion-based aging model to control the camera pose and\nfacial age. Through quantitative and qualitative evaluations, we demonstrate\nthat DiffAge3D outperforms existing methods, particularly in\nmultiview-consistent aging and fine details preservation.\n", "link": "http://arxiv.org/abs/2408.15922v1", "date": "2024-08-28", "relevancy": 2.973, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5995}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5995}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffAge3D%3A%20Diffusion-based%203D-aware%20Face%20Aging&body=Title%3A%20DiffAge3D%3A%20Diffusion-based%203D-aware%20Face%20Aging%0AAuthor%3A%20Junaid%20Wahid%20and%20Fangneng%20Zhan%20and%20Pramod%20Rao%20and%20Christian%20Theobalt%0AAbstract%3A%20%20%20Face%20aging%20is%20the%20process%20of%20converting%20an%20individual%27s%20appearance%20to%20a%0Ayounger%20or%20older%20version%20of%20themselves.%20Existing%20face%20aging%20techniques%20have%0Abeen%20limited%20to%202D%20settings%2C%20which%20often%20weaken%20their%20applications%20as%20there%20is%0Aa%20growing%20demand%20for%203D%20face%20modeling.%20Moreover%2C%20existing%20aging%20methods%0Astruggle%20to%20perform%20faithful%20aging%2C%20maintain%20identity%2C%20and%20retain%20the%20fine%0Adetails%20of%20the%20input%20images.%20Given%20these%20limitations%20and%20the%20need%20for%20a%0A3D-aware%20aging%20method%2C%20we%20propose%20DiffAge3D%2C%20the%20first%203D-aware%20aging%20framework%0Athat%20not%20only%20performs%20faithful%20aging%20and%20identity%20preservation%20but%20also%0Aoperates%20in%20a%203D%20setting.%20Our%20aging%20framework%20allows%20to%20model%20the%20aging%20and%0Acamera%20pose%20separately%20by%20only%20taking%20a%20single%20image%20with%20a%20target%20age.%20Our%0Aframework%20includes%20a%20robust%203D-aware%20aging%20dataset%20generation%20pipeline%20by%0Autilizing%20a%20pre-trained%203D%20GAN%20and%20the%20rich%20text%20embedding%20capabilities%20within%0ACLIP%20model.%20Notably%2C%20we%20do%20not%20employ%20any%20inversion%20bottleneck%20in%20dataset%0Ageneration.%20Instead%2C%20we%20randomly%20generate%20training%20samples%20from%20the%20latent%0Aspace%20of%203D%20GAN%2C%20allowing%20us%20to%20manipulate%20the%20rich%20latent%20space%20of%20GAN%20to%0Agenerate%20ages%20even%20with%20large%20gaps.%20With%20the%20generated%20dataset%2C%20we%20train%20a%0Aviewpoint-aware%20diffusion-based%20aging%20model%20to%20control%20the%20camera%20pose%20and%0Afacial%20age.%20Through%20quantitative%20and%20qualitative%20evaluations%2C%20we%20demonstrate%0Athat%20DiffAge3D%20outperforms%20existing%20methods%2C%20particularly%20in%0Amultiview-consistent%20aging%20and%20fine%20details%20preservation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15922v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffAge3D%253A%2520Diffusion-based%25203D-aware%2520Face%2520Aging%26entry.906535625%3DJunaid%2520Wahid%2520and%2520Fangneng%2520Zhan%2520and%2520Pramod%2520Rao%2520and%2520Christian%2520Theobalt%26entry.1292438233%3D%2520%2520Face%2520aging%2520is%2520the%2520process%2520of%2520converting%2520an%2520individual%2527s%2520appearance%2520to%2520a%250Ayounger%2520or%2520older%2520version%2520of%2520themselves.%2520Existing%2520face%2520aging%2520techniques%2520have%250Abeen%2520limited%2520to%25202D%2520settings%252C%2520which%2520often%2520weaken%2520their%2520applications%2520as%2520there%2520is%250Aa%2520growing%2520demand%2520for%25203D%2520face%2520modeling.%2520Moreover%252C%2520existing%2520aging%2520methods%250Astruggle%2520to%2520perform%2520faithful%2520aging%252C%2520maintain%2520identity%252C%2520and%2520retain%2520the%2520fine%250Adetails%2520of%2520the%2520input%2520images.%2520Given%2520these%2520limitations%2520and%2520the%2520need%2520for%2520a%250A3D-aware%2520aging%2520method%252C%2520we%2520propose%2520DiffAge3D%252C%2520the%2520first%25203D-aware%2520aging%2520framework%250Athat%2520not%2520only%2520performs%2520faithful%2520aging%2520and%2520identity%2520preservation%2520but%2520also%250Aoperates%2520in%2520a%25203D%2520setting.%2520Our%2520aging%2520framework%2520allows%2520to%2520model%2520the%2520aging%2520and%250Acamera%2520pose%2520separately%2520by%2520only%2520taking%2520a%2520single%2520image%2520with%2520a%2520target%2520age.%2520Our%250Aframework%2520includes%2520a%2520robust%25203D-aware%2520aging%2520dataset%2520generation%2520pipeline%2520by%250Autilizing%2520a%2520pre-trained%25203D%2520GAN%2520and%2520the%2520rich%2520text%2520embedding%2520capabilities%2520within%250ACLIP%2520model.%2520Notably%252C%2520we%2520do%2520not%2520employ%2520any%2520inversion%2520bottleneck%2520in%2520dataset%250Ageneration.%2520Instead%252C%2520we%2520randomly%2520generate%2520training%2520samples%2520from%2520the%2520latent%250Aspace%2520of%25203D%2520GAN%252C%2520allowing%2520us%2520to%2520manipulate%2520the%2520rich%2520latent%2520space%2520of%2520GAN%2520to%250Agenerate%2520ages%2520even%2520with%2520large%2520gaps.%2520With%2520the%2520generated%2520dataset%252C%2520we%2520train%2520a%250Aviewpoint-aware%2520diffusion-based%2520aging%2520model%2520to%2520control%2520the%2520camera%2520pose%2520and%250Afacial%2520age.%2520Through%2520quantitative%2520and%2520qualitative%2520evaluations%252C%2520we%2520demonstrate%250Athat%2520DiffAge3D%2520outperforms%2520existing%2520methods%252C%2520particularly%2520in%250Amultiview-consistent%2520aging%2520and%2520fine%2520details%2520preservation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15922v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffAge3D%3A%20Diffusion-based%203D-aware%20Face%20Aging&entry.906535625=Junaid%20Wahid%20and%20Fangneng%20Zhan%20and%20Pramod%20Rao%20and%20Christian%20Theobalt&entry.1292438233=%20%20Face%20aging%20is%20the%20process%20of%20converting%20an%20individual%27s%20appearance%20to%20a%0Ayounger%20or%20older%20version%20of%20themselves.%20Existing%20face%20aging%20techniques%20have%0Abeen%20limited%20to%202D%20settings%2C%20which%20often%20weaken%20their%20applications%20as%20there%20is%0Aa%20growing%20demand%20for%203D%20face%20modeling.%20Moreover%2C%20existing%20aging%20methods%0Astruggle%20to%20perform%20faithful%20aging%2C%20maintain%20identity%2C%20and%20retain%20the%20fine%0Adetails%20of%20the%20input%20images.%20Given%20these%20limitations%20and%20the%20need%20for%20a%0A3D-aware%20aging%20method%2C%20we%20propose%20DiffAge3D%2C%20the%20first%203D-aware%20aging%20framework%0Athat%20not%20only%20performs%20faithful%20aging%20and%20identity%20preservation%20but%20also%0Aoperates%20in%20a%203D%20setting.%20Our%20aging%20framework%20allows%20to%20model%20the%20aging%20and%0Acamera%20pose%20separately%20by%20only%20taking%20a%20single%20image%20with%20a%20target%20age.%20Our%0Aframework%20includes%20a%20robust%203D-aware%20aging%20dataset%20generation%20pipeline%20by%0Autilizing%20a%20pre-trained%203D%20GAN%20and%20the%20rich%20text%20embedding%20capabilities%20within%0ACLIP%20model.%20Notably%2C%20we%20do%20not%20employ%20any%20inversion%20bottleneck%20in%20dataset%0Ageneration.%20Instead%2C%20we%20randomly%20generate%20training%20samples%20from%20the%20latent%0Aspace%20of%203D%20GAN%2C%20allowing%20us%20to%20manipulate%20the%20rich%20latent%20space%20of%20GAN%20to%0Agenerate%20ages%20even%20with%20large%20gaps.%20With%20the%20generated%20dataset%2C%20we%20train%20a%0Aviewpoint-aware%20diffusion-based%20aging%20model%20to%20control%20the%20camera%20pose%20and%0Afacial%20age.%20Through%20quantitative%20and%20qualitative%20evaluations%2C%20we%20demonstrate%0Athat%20DiffAge3D%20outperforms%20existing%20methods%2C%20particularly%20in%0Amultiview-consistent%20aging%20and%20fine%20details%20preservation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15922v1&entry.124074799=Read"},
{"title": "Re-Nerfing: Improving Novel View Synthesis through Novel View Synthesis", "author": "Felix Tristram and Stefano Gasperini and Nassir Navab and Federico Tombari", "abstract": "  Recent neural rendering and reconstruction techniques, such as NeRFs or\nGaussian Splatting, have shown remarkable novel view synthesis capabilities but\nrequire hundreds of images of the scene from diverse viewpoints to render\nhigh-quality novel views. With fewer images available, these methods start to\nfail since they can no longer correctly triangulate the underlying 3D geometry\nand converge to a non-optimal solution. These failures can manifest as floaters\nor blurry renderings in sparsely observed areas of the scene. In this paper, we\npropose Re-Nerfing, a simple and general add-on approach that leverages novel\nview synthesis itself to tackle this problem. Using an already trained NVS\nmethod, we render novel views between existing ones and augment the training\ndata to optimize a second model. This introduces additional multi-view\nconstraints and allows the second model to converge to a better solution. With\nRe-Nerfing we achieve significant improvements upon multiple pipelines based on\nNeRF and Gaussian-Splatting in sparse view settings of the mip-NeRF 360 and\nLLFF datasets. Notably, Re-Nerfing does not require prior knowledge or extra\nsupervision signals, making it a flexible and practical add-on.\n", "link": "http://arxiv.org/abs/2312.02255v3", "date": "2024-08-28", "relevancy": 2.9131, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6078}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.587}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Re-Nerfing%3A%20Improving%20Novel%20View%20Synthesis%20through%20Novel%20View%20Synthesis&body=Title%3A%20Re-Nerfing%3A%20Improving%20Novel%20View%20Synthesis%20through%20Novel%20View%20Synthesis%0AAuthor%3A%20Felix%20Tristram%20and%20Stefano%20Gasperini%20and%20Nassir%20Navab%20and%20Federico%20Tombari%0AAbstract%3A%20%20%20Recent%20neural%20rendering%20and%20reconstruction%20techniques%2C%20such%20as%20NeRFs%20or%0AGaussian%20Splatting%2C%20have%20shown%20remarkable%20novel%20view%20synthesis%20capabilities%20but%0Arequire%20hundreds%20of%20images%20of%20the%20scene%20from%20diverse%20viewpoints%20to%20render%0Ahigh-quality%20novel%20views.%20With%20fewer%20images%20available%2C%20these%20methods%20start%20to%0Afail%20since%20they%20can%20no%20longer%20correctly%20triangulate%20the%20underlying%203D%20geometry%0Aand%20converge%20to%20a%20non-optimal%20solution.%20These%20failures%20can%20manifest%20as%20floaters%0Aor%20blurry%20renderings%20in%20sparsely%20observed%20areas%20of%20the%20scene.%20In%20this%20paper%2C%20we%0Apropose%20Re-Nerfing%2C%20a%20simple%20and%20general%20add-on%20approach%20that%20leverages%20novel%0Aview%20synthesis%20itself%20to%20tackle%20this%20problem.%20Using%20an%20already%20trained%20NVS%0Amethod%2C%20we%20render%20novel%20views%20between%20existing%20ones%20and%20augment%20the%20training%0Adata%20to%20optimize%20a%20second%20model.%20This%20introduces%20additional%20multi-view%0Aconstraints%20and%20allows%20the%20second%20model%20to%20converge%20to%20a%20better%20solution.%20With%0ARe-Nerfing%20we%20achieve%20significant%20improvements%20upon%20multiple%20pipelines%20based%20on%0ANeRF%20and%20Gaussian-Splatting%20in%20sparse%20view%20settings%20of%20the%20mip-NeRF%20360%20and%0ALLFF%20datasets.%20Notably%2C%20Re-Nerfing%20does%20not%20require%20prior%20knowledge%20or%20extra%0Asupervision%20signals%2C%20making%20it%20a%20flexible%20and%20practical%20add-on.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02255v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRe-Nerfing%253A%2520Improving%2520Novel%2520View%2520Synthesis%2520through%2520Novel%2520View%2520Synthesis%26entry.906535625%3DFelix%2520Tristram%2520and%2520Stefano%2520Gasperini%2520and%2520Nassir%2520Navab%2520and%2520Federico%2520Tombari%26entry.1292438233%3D%2520%2520Recent%2520neural%2520rendering%2520and%2520reconstruction%2520techniques%252C%2520such%2520as%2520NeRFs%2520or%250AGaussian%2520Splatting%252C%2520have%2520shown%2520remarkable%2520novel%2520view%2520synthesis%2520capabilities%2520but%250Arequire%2520hundreds%2520of%2520images%2520of%2520the%2520scene%2520from%2520diverse%2520viewpoints%2520to%2520render%250Ahigh-quality%2520novel%2520views.%2520With%2520fewer%2520images%2520available%252C%2520these%2520methods%2520start%2520to%250Afail%2520since%2520they%2520can%2520no%2520longer%2520correctly%2520triangulate%2520the%2520underlying%25203D%2520geometry%250Aand%2520converge%2520to%2520a%2520non-optimal%2520solution.%2520These%2520failures%2520can%2520manifest%2520as%2520floaters%250Aor%2520blurry%2520renderings%2520in%2520sparsely%2520observed%2520areas%2520of%2520the%2520scene.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520Re-Nerfing%252C%2520a%2520simple%2520and%2520general%2520add-on%2520approach%2520that%2520leverages%2520novel%250Aview%2520synthesis%2520itself%2520to%2520tackle%2520this%2520problem.%2520Using%2520an%2520already%2520trained%2520NVS%250Amethod%252C%2520we%2520render%2520novel%2520views%2520between%2520existing%2520ones%2520and%2520augment%2520the%2520training%250Adata%2520to%2520optimize%2520a%2520second%2520model.%2520This%2520introduces%2520additional%2520multi-view%250Aconstraints%2520and%2520allows%2520the%2520second%2520model%2520to%2520converge%2520to%2520a%2520better%2520solution.%2520With%250ARe-Nerfing%2520we%2520achieve%2520significant%2520improvements%2520upon%2520multiple%2520pipelines%2520based%2520on%250ANeRF%2520and%2520Gaussian-Splatting%2520in%2520sparse%2520view%2520settings%2520of%2520the%2520mip-NeRF%2520360%2520and%250ALLFF%2520datasets.%2520Notably%252C%2520Re-Nerfing%2520does%2520not%2520require%2520prior%2520knowledge%2520or%2520extra%250Asupervision%2520signals%252C%2520making%2520it%2520a%2520flexible%2520and%2520practical%2520add-on.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02255v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Re-Nerfing%3A%20Improving%20Novel%20View%20Synthesis%20through%20Novel%20View%20Synthesis&entry.906535625=Felix%20Tristram%20and%20Stefano%20Gasperini%20and%20Nassir%20Navab%20and%20Federico%20Tombari&entry.1292438233=%20%20Recent%20neural%20rendering%20and%20reconstruction%20techniques%2C%20such%20as%20NeRFs%20or%0AGaussian%20Splatting%2C%20have%20shown%20remarkable%20novel%20view%20synthesis%20capabilities%20but%0Arequire%20hundreds%20of%20images%20of%20the%20scene%20from%20diverse%20viewpoints%20to%20render%0Ahigh-quality%20novel%20views.%20With%20fewer%20images%20available%2C%20these%20methods%20start%20to%0Afail%20since%20they%20can%20no%20longer%20correctly%20triangulate%20the%20underlying%203D%20geometry%0Aand%20converge%20to%20a%20non-optimal%20solution.%20These%20failures%20can%20manifest%20as%20floaters%0Aor%20blurry%20renderings%20in%20sparsely%20observed%20areas%20of%20the%20scene.%20In%20this%20paper%2C%20we%0Apropose%20Re-Nerfing%2C%20a%20simple%20and%20general%20add-on%20approach%20that%20leverages%20novel%0Aview%20synthesis%20itself%20to%20tackle%20this%20problem.%20Using%20an%20already%20trained%20NVS%0Amethod%2C%20we%20render%20novel%20views%20between%20existing%20ones%20and%20augment%20the%20training%0Adata%20to%20optimize%20a%20second%20model.%20This%20introduces%20additional%20multi-view%0Aconstraints%20and%20allows%20the%20second%20model%20to%20converge%20to%20a%20better%20solution.%20With%0ARe-Nerfing%20we%20achieve%20significant%20improvements%20upon%20multiple%20pipelines%20based%20on%0ANeRF%20and%20Gaussian-Splatting%20in%20sparse%20view%20settings%20of%20the%20mip-NeRF%20360%20and%0ALLFF%20datasets.%20Notably%2C%20Re-Nerfing%20does%20not%20require%20prior%20knowledge%20or%20extra%0Asupervision%20signals%2C%20making%20it%20a%20flexible%20and%20practical%20add-on.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02255v3&entry.124074799=Read"},
{"title": "Str-L Pose: Integrating Point and Structured Line for Relative Pose\n  Estimation in Dual-Graph", "author": "Zherong Zhang and Chunyu Lin and Shujuan Huang and Shangrong Yang and Yao Zhao", "abstract": "  Relative pose estimation is crucial for various computer vision applications,\nincluding Robotic and Autonomous Driving. Current methods primarily depend on\nselecting and matching feature points prone to incorrect matches, leading to\npoor performance. Consequently, relying solely on point-matching relationships\nfor pose estimation is a huge challenge. To overcome these limitations, we\npropose a Geometric Correspondence Graph neural network that integrates point\nfeatures with extra structured line segments. This integration of matched\npoints and line segments further exploits the geometry constraints and enhances\nmodel performance across different environments. We employ the Dual-Graph\nmodule and Feature Weighted Fusion Module to aggregate geometric and visual\nfeatures effectively, facilitating complex scene understanding. We demonstrate\nour approach through extensive experiments on the DeMoN and KITTI Odometry\ndatasets. The results show that our method is competitive with state-of-the-art\ntechniques.\n", "link": "http://arxiv.org/abs/2408.15750v1", "date": "2024-08-28", "relevancy": 2.748, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5875}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5354}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Str-L%20Pose%3A%20Integrating%20Point%20and%20Structured%20Line%20for%20Relative%20Pose%0A%20%20Estimation%20in%20Dual-Graph&body=Title%3A%20Str-L%20Pose%3A%20Integrating%20Point%20and%20Structured%20Line%20for%20Relative%20Pose%0A%20%20Estimation%20in%20Dual-Graph%0AAuthor%3A%20Zherong%20Zhang%20and%20Chunyu%20Lin%20and%20Shujuan%20Huang%20and%20Shangrong%20Yang%20and%20Yao%20Zhao%0AAbstract%3A%20%20%20Relative%20pose%20estimation%20is%20crucial%20for%20various%20computer%20vision%20applications%2C%0Aincluding%20Robotic%20and%20Autonomous%20Driving.%20Current%20methods%20primarily%20depend%20on%0Aselecting%20and%20matching%20feature%20points%20prone%20to%20incorrect%20matches%2C%20leading%20to%0Apoor%20performance.%20Consequently%2C%20relying%20solely%20on%20point-matching%20relationships%0Afor%20pose%20estimation%20is%20a%20huge%20challenge.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20a%20Geometric%20Correspondence%20Graph%20neural%20network%20that%20integrates%20point%0Afeatures%20with%20extra%20structured%20line%20segments.%20This%20integration%20of%20matched%0Apoints%20and%20line%20segments%20further%20exploits%20the%20geometry%20constraints%20and%20enhances%0Amodel%20performance%20across%20different%20environments.%20We%20employ%20the%20Dual-Graph%0Amodule%20and%20Feature%20Weighted%20Fusion%20Module%20to%20aggregate%20geometric%20and%20visual%0Afeatures%20effectively%2C%20facilitating%20complex%20scene%20understanding.%20We%20demonstrate%0Aour%20approach%20through%20extensive%20experiments%20on%20the%20DeMoN%20and%20KITTI%20Odometry%0Adatasets.%20The%20results%20show%20that%20our%20method%20is%20competitive%20with%20state-of-the-art%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15750v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStr-L%2520Pose%253A%2520Integrating%2520Point%2520and%2520Structured%2520Line%2520for%2520Relative%2520Pose%250A%2520%2520Estimation%2520in%2520Dual-Graph%26entry.906535625%3DZherong%2520Zhang%2520and%2520Chunyu%2520Lin%2520and%2520Shujuan%2520Huang%2520and%2520Shangrong%2520Yang%2520and%2520Yao%2520Zhao%26entry.1292438233%3D%2520%2520Relative%2520pose%2520estimation%2520is%2520crucial%2520for%2520various%2520computer%2520vision%2520applications%252C%250Aincluding%2520Robotic%2520and%2520Autonomous%2520Driving.%2520Current%2520methods%2520primarily%2520depend%2520on%250Aselecting%2520and%2520matching%2520feature%2520points%2520prone%2520to%2520incorrect%2520matches%252C%2520leading%2520to%250Apoor%2520performance.%2520Consequently%252C%2520relying%2520solely%2520on%2520point-matching%2520relationships%250Afor%2520pose%2520estimation%2520is%2520a%2520huge%2520challenge.%2520To%2520overcome%2520these%2520limitations%252C%2520we%250Apropose%2520a%2520Geometric%2520Correspondence%2520Graph%2520neural%2520network%2520that%2520integrates%2520point%250Afeatures%2520with%2520extra%2520structured%2520line%2520segments.%2520This%2520integration%2520of%2520matched%250Apoints%2520and%2520line%2520segments%2520further%2520exploits%2520the%2520geometry%2520constraints%2520and%2520enhances%250Amodel%2520performance%2520across%2520different%2520environments.%2520We%2520employ%2520the%2520Dual-Graph%250Amodule%2520and%2520Feature%2520Weighted%2520Fusion%2520Module%2520to%2520aggregate%2520geometric%2520and%2520visual%250Afeatures%2520effectively%252C%2520facilitating%2520complex%2520scene%2520understanding.%2520We%2520demonstrate%250Aour%2520approach%2520through%2520extensive%2520experiments%2520on%2520the%2520DeMoN%2520and%2520KITTI%2520Odometry%250Adatasets.%2520The%2520results%2520show%2520that%2520our%2520method%2520is%2520competitive%2520with%2520state-of-the-art%250Atechniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15750v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Str-L%20Pose%3A%20Integrating%20Point%20and%20Structured%20Line%20for%20Relative%20Pose%0A%20%20Estimation%20in%20Dual-Graph&entry.906535625=Zherong%20Zhang%20and%20Chunyu%20Lin%20and%20Shujuan%20Huang%20and%20Shangrong%20Yang%20and%20Yao%20Zhao&entry.1292438233=%20%20Relative%20pose%20estimation%20is%20crucial%20for%20various%20computer%20vision%20applications%2C%0Aincluding%20Robotic%20and%20Autonomous%20Driving.%20Current%20methods%20primarily%20depend%20on%0Aselecting%20and%20matching%20feature%20points%20prone%20to%20incorrect%20matches%2C%20leading%20to%0Apoor%20performance.%20Consequently%2C%20relying%20solely%20on%20point-matching%20relationships%0Afor%20pose%20estimation%20is%20a%20huge%20challenge.%20To%20overcome%20these%20limitations%2C%20we%0Apropose%20a%20Geometric%20Correspondence%20Graph%20neural%20network%20that%20integrates%20point%0Afeatures%20with%20extra%20structured%20line%20segments.%20This%20integration%20of%20matched%0Apoints%20and%20line%20segments%20further%20exploits%20the%20geometry%20constraints%20and%20enhances%0Amodel%20performance%20across%20different%20environments.%20We%20employ%20the%20Dual-Graph%0Amodule%20and%20Feature%20Weighted%20Fusion%20Module%20to%20aggregate%20geometric%20and%20visual%0Afeatures%20effectively%2C%20facilitating%20complex%20scene%20understanding.%20We%20demonstrate%0Aour%20approach%20through%20extensive%20experiments%20on%20the%20DeMoN%20and%20KITTI%20Odometry%0Adatasets.%20The%20results%20show%20that%20our%20method%20is%20competitive%20with%20state-of-the-art%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15750v1&entry.124074799=Read"},
{"title": "Addressing the challenges of loop detection in agricultural environments", "author": "Nicol\u00e1s Soncini and Javier Civera and Taih\u00fa Pire", "abstract": "  While visual SLAM systems are well studied and achieve impressive results in\nindoor and urban settings, natural, outdoor and open-field environments are\nmuch less explored and still present relevant research challenges. Visual\nnavigation and local mapping have shown a relatively good performance in\nopen-field environments. However, globally consistent mapping and long-term\nlocalization still depend on the robustness of loop detection and closure, for\nwhich the literature is scarce. In this work we propose a novel method to pave\nthe way towards robust loop detection in open fields, particularly in\nagricultural settings, based on local feature search and stereo geometric\nrefinement, with a final stage of relative pose estimation. Our method\nconsistently achieves good loop detections, with a median error of 15cm. We aim\nto characterize open fields as a novel environment for loop detection,\nunderstanding the limitations and problems that arise when dealing with them.\n", "link": "http://arxiv.org/abs/2408.15761v1", "date": "2024-08-28", "relevancy": 2.7046, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5777}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.525}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5201}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Addressing%20the%20challenges%20of%20loop%20detection%20in%20agricultural%20environments&body=Title%3A%20Addressing%20the%20challenges%20of%20loop%20detection%20in%20agricultural%20environments%0AAuthor%3A%20Nicol%C3%A1s%20Soncini%20and%20Javier%20Civera%20and%20Taih%C3%BA%20Pire%0AAbstract%3A%20%20%20While%20visual%20SLAM%20systems%20are%20well%20studied%20and%20achieve%20impressive%20results%20in%0Aindoor%20and%20urban%20settings%2C%20natural%2C%20outdoor%20and%20open-field%20environments%20are%0Amuch%20less%20explored%20and%20still%20present%20relevant%20research%20challenges.%20Visual%0Anavigation%20and%20local%20mapping%20have%20shown%20a%20relatively%20good%20performance%20in%0Aopen-field%20environments.%20However%2C%20globally%20consistent%20mapping%20and%20long-term%0Alocalization%20still%20depend%20on%20the%20robustness%20of%20loop%20detection%20and%20closure%2C%20for%0Awhich%20the%20literature%20is%20scarce.%20In%20this%20work%20we%20propose%20a%20novel%20method%20to%20pave%0Athe%20way%20towards%20robust%20loop%20detection%20in%20open%20fields%2C%20particularly%20in%0Aagricultural%20settings%2C%20based%20on%20local%20feature%20search%20and%20stereo%20geometric%0Arefinement%2C%20with%20a%20final%20stage%20of%20relative%20pose%20estimation.%20Our%20method%0Aconsistently%20achieves%20good%20loop%20detections%2C%20with%20a%20median%20error%20of%2015cm.%20We%20aim%0Ato%20characterize%20open%20fields%20as%20a%20novel%20environment%20for%20loop%20detection%2C%0Aunderstanding%20the%20limitations%20and%20problems%20that%20arise%20when%20dealing%20with%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAddressing%2520the%2520challenges%2520of%2520loop%2520detection%2520in%2520agricultural%2520environments%26entry.906535625%3DNicol%25C3%25A1s%2520Soncini%2520and%2520Javier%2520Civera%2520and%2520Taih%25C3%25BA%2520Pire%26entry.1292438233%3D%2520%2520While%2520visual%2520SLAM%2520systems%2520are%2520well%2520studied%2520and%2520achieve%2520impressive%2520results%2520in%250Aindoor%2520and%2520urban%2520settings%252C%2520natural%252C%2520outdoor%2520and%2520open-field%2520environments%2520are%250Amuch%2520less%2520explored%2520and%2520still%2520present%2520relevant%2520research%2520challenges.%2520Visual%250Anavigation%2520and%2520local%2520mapping%2520have%2520shown%2520a%2520relatively%2520good%2520performance%2520in%250Aopen-field%2520environments.%2520However%252C%2520globally%2520consistent%2520mapping%2520and%2520long-term%250Alocalization%2520still%2520depend%2520on%2520the%2520robustness%2520of%2520loop%2520detection%2520and%2520closure%252C%2520for%250Awhich%2520the%2520literature%2520is%2520scarce.%2520In%2520this%2520work%2520we%2520propose%2520a%2520novel%2520method%2520to%2520pave%250Athe%2520way%2520towards%2520robust%2520loop%2520detection%2520in%2520open%2520fields%252C%2520particularly%2520in%250Aagricultural%2520settings%252C%2520based%2520on%2520local%2520feature%2520search%2520and%2520stereo%2520geometric%250Arefinement%252C%2520with%2520a%2520final%2520stage%2520of%2520relative%2520pose%2520estimation.%2520Our%2520method%250Aconsistently%2520achieves%2520good%2520loop%2520detections%252C%2520with%2520a%2520median%2520error%2520of%252015cm.%2520We%2520aim%250Ato%2520characterize%2520open%2520fields%2520as%2520a%2520novel%2520environment%2520for%2520loop%2520detection%252C%250Aunderstanding%2520the%2520limitations%2520and%2520problems%2520that%2520arise%2520when%2520dealing%2520with%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20the%20challenges%20of%20loop%20detection%20in%20agricultural%20environments&entry.906535625=Nicol%C3%A1s%20Soncini%20and%20Javier%20Civera%20and%20Taih%C3%BA%20Pire&entry.1292438233=%20%20While%20visual%20SLAM%20systems%20are%20well%20studied%20and%20achieve%20impressive%20results%20in%0Aindoor%20and%20urban%20settings%2C%20natural%2C%20outdoor%20and%20open-field%20environments%20are%0Amuch%20less%20explored%20and%20still%20present%20relevant%20research%20challenges.%20Visual%0Anavigation%20and%20local%20mapping%20have%20shown%20a%20relatively%20good%20performance%20in%0Aopen-field%20environments.%20However%2C%20globally%20consistent%20mapping%20and%20long-term%0Alocalization%20still%20depend%20on%20the%20robustness%20of%20loop%20detection%20and%20closure%2C%20for%0Awhich%20the%20literature%20is%20scarce.%20In%20this%20work%20we%20propose%20a%20novel%20method%20to%20pave%0Athe%20way%20towards%20robust%20loop%20detection%20in%20open%20fields%2C%20particularly%20in%0Aagricultural%20settings%2C%20based%20on%20local%20feature%20search%20and%20stereo%20geometric%0Arefinement%2C%20with%20a%20final%20stage%20of%20relative%20pose%20estimation.%20Our%20method%0Aconsistently%20achieves%20good%20loop%20detections%2C%20with%20a%20median%20error%20of%2015cm.%20We%20aim%0Ato%20characterize%20open%20fields%20as%20a%20novel%20environment%20for%20loop%20detection%2C%0Aunderstanding%20the%20limitations%20and%20problems%20that%20arise%20when%20dealing%20with%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15761v1&entry.124074799=Read"},
{"title": "wav2pos: Sound Source Localization using Masked Autoencoders", "author": "Axel Berg and Jens Gulin and Mark O'Connor and Chuteng Zhou and Karl \u00c5str\u00f6m and Magnus Oskarsson", "abstract": "  We present a novel approach to the 3D sound source localization task for\ndistributed ad-hoc microphone arrays by formulating it as a set-to-set\nregression problem. By training a multi-modal masked autoencoder model that\noperates on audio recordings and microphone coordinates, we show that such a\nformulation allows for accurate localization of the sound source, by\nreconstructing coordinates masked in the input. Our approach is flexible in the\nsense that a single model can be used with an arbitrary number of microphones,\neven when a subset of audio recordings and microphone coordinates are missing.\nWe test our method on simulated and real-world recordings of music and speech\nin indoor environments, and demonstrate competitive performance compared to\nboth classical and other learning based localization methods.\n", "link": "http://arxiv.org/abs/2408.15771v1", "date": "2024-08-28", "relevancy": 2.6225, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5683}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.51}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20wav2pos%3A%20Sound%20Source%20Localization%20using%20Masked%20Autoencoders&body=Title%3A%20wav2pos%3A%20Sound%20Source%20Localization%20using%20Masked%20Autoencoders%0AAuthor%3A%20Axel%20Berg%20and%20Jens%20Gulin%20and%20Mark%20O%27Connor%20and%20Chuteng%20Zhou%20and%20Karl%20%C3%85str%C3%B6m%20and%20Magnus%20Oskarsson%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20to%20the%203D%20sound%20source%20localization%20task%20for%0Adistributed%20ad-hoc%20microphone%20arrays%20by%20formulating%20it%20as%20a%20set-to-set%0Aregression%20problem.%20By%20training%20a%20multi-modal%20masked%20autoencoder%20model%20that%0Aoperates%20on%20audio%20recordings%20and%20microphone%20coordinates%2C%20we%20show%20that%20such%20a%0Aformulation%20allows%20for%20accurate%20localization%20of%20the%20sound%20source%2C%20by%0Areconstructing%20coordinates%20masked%20in%20the%20input.%20Our%20approach%20is%20flexible%20in%20the%0Asense%20that%20a%20single%20model%20can%20be%20used%20with%20an%20arbitrary%20number%20of%20microphones%2C%0Aeven%20when%20a%20subset%20of%20audio%20recordings%20and%20microphone%20coordinates%20are%20missing.%0AWe%20test%20our%20method%20on%20simulated%20and%20real-world%20recordings%20of%20music%20and%20speech%0Ain%20indoor%20environments%2C%20and%20demonstrate%20competitive%20performance%20compared%20to%0Aboth%20classical%20and%20other%20learning%20based%20localization%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15771v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dwav2pos%253A%2520Sound%2520Source%2520Localization%2520using%2520Masked%2520Autoencoders%26entry.906535625%3DAxel%2520Berg%2520and%2520Jens%2520Gulin%2520and%2520Mark%2520O%2527Connor%2520and%2520Chuteng%2520Zhou%2520and%2520Karl%2520%25C3%2585str%25C3%25B6m%2520and%2520Magnus%2520Oskarsson%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520to%2520the%25203D%2520sound%2520source%2520localization%2520task%2520for%250Adistributed%2520ad-hoc%2520microphone%2520arrays%2520by%2520formulating%2520it%2520as%2520a%2520set-to-set%250Aregression%2520problem.%2520By%2520training%2520a%2520multi-modal%2520masked%2520autoencoder%2520model%2520that%250Aoperates%2520on%2520audio%2520recordings%2520and%2520microphone%2520coordinates%252C%2520we%2520show%2520that%2520such%2520a%250Aformulation%2520allows%2520for%2520accurate%2520localization%2520of%2520the%2520sound%2520source%252C%2520by%250Areconstructing%2520coordinates%2520masked%2520in%2520the%2520input.%2520Our%2520approach%2520is%2520flexible%2520in%2520the%250Asense%2520that%2520a%2520single%2520model%2520can%2520be%2520used%2520with%2520an%2520arbitrary%2520number%2520of%2520microphones%252C%250Aeven%2520when%2520a%2520subset%2520of%2520audio%2520recordings%2520and%2520microphone%2520coordinates%2520are%2520missing.%250AWe%2520test%2520our%2520method%2520on%2520simulated%2520and%2520real-world%2520recordings%2520of%2520music%2520and%2520speech%250Ain%2520indoor%2520environments%252C%2520and%2520demonstrate%2520competitive%2520performance%2520compared%2520to%250Aboth%2520classical%2520and%2520other%2520learning%2520based%2520localization%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15771v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=wav2pos%3A%20Sound%20Source%20Localization%20using%20Masked%20Autoencoders&entry.906535625=Axel%20Berg%20and%20Jens%20Gulin%20and%20Mark%20O%27Connor%20and%20Chuteng%20Zhou%20and%20Karl%20%C3%85str%C3%B6m%20and%20Magnus%20Oskarsson&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20to%20the%203D%20sound%20source%20localization%20task%20for%0Adistributed%20ad-hoc%20microphone%20arrays%20by%20formulating%20it%20as%20a%20set-to-set%0Aregression%20problem.%20By%20training%20a%20multi-modal%20masked%20autoencoder%20model%20that%0Aoperates%20on%20audio%20recordings%20and%20microphone%20coordinates%2C%20we%20show%20that%20such%20a%0Aformulation%20allows%20for%20accurate%20localization%20of%20the%20sound%20source%2C%20by%0Areconstructing%20coordinates%20masked%20in%20the%20input.%20Our%20approach%20is%20flexible%20in%20the%0Asense%20that%20a%20single%20model%20can%20be%20used%20with%20an%20arbitrary%20number%20of%20microphones%2C%0Aeven%20when%20a%20subset%20of%20audio%20recordings%20and%20microphone%20coordinates%20are%20missing.%0AWe%20test%20our%20method%20on%20simulated%20and%20real-world%20recordings%20of%20music%20and%20speech%0Ain%20indoor%20environments%2C%20and%20demonstrate%20competitive%20performance%20compared%20to%0Aboth%20classical%20and%20other%20learning%20based%20localization%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15771v1&entry.124074799=Read"},
{"title": "FERGI: Automatic Annotation of User Preferences for Text-to-Image\n  Generation from Spontaneous Facial Expression Reaction", "author": "Shuangquan Feng and Junhua Ma and Virginia R. de Sa", "abstract": "  Researchers have proposed to use data of human preference feedback to\nfine-tune text-to-image generative models. However, the scalability of human\nfeedback collection has been limited by its reliance on manual annotation.\nTherefore, we develop and test a method to automatically score user preferences\nfrom their spontaneous facial expression reaction to the generated images. We\ncollect a dataset of Facial Expression Reaction to Generated Images (FERGI) and\nshow that the activations of multiple facial action units (AUs) are highly\ncorrelated with user evaluations of the generated images. We develop an FAU-Net\n(Facial Action Units Neural Network), which receives inputs from an AU\nestimation model, to automatically score user preferences for text-to-image\ngeneration based on their facial expression reactions, which is complementary\nto the pre-trained scoring models based on the input text prompts and generated\nimages. Integrating our FAU-Net valence score with the pre-trained scoring\nmodels improves their consistency with human preferences. This method of\nautomatic annotation with facial expression analysis can be potentially\ngeneralized to other generation tasks. The code is available at\nhttps://github.com/ShuangquanFeng/FERGI, and the dataset is also available at\nthe same link for research purposes.\n", "link": "http://arxiv.org/abs/2312.03187v3", "date": "2024-08-28", "relevancy": 2.6025, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5409}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5159}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FERGI%3A%20Automatic%20Annotation%20of%20User%20Preferences%20for%20Text-to-Image%0A%20%20Generation%20from%20Spontaneous%20Facial%20Expression%20Reaction&body=Title%3A%20FERGI%3A%20Automatic%20Annotation%20of%20User%20Preferences%20for%20Text-to-Image%0A%20%20Generation%20from%20Spontaneous%20Facial%20Expression%20Reaction%0AAuthor%3A%20Shuangquan%20Feng%20and%20Junhua%20Ma%20and%20Virginia%20R.%20de%20Sa%0AAbstract%3A%20%20%20Researchers%20have%20proposed%20to%20use%20data%20of%20human%20preference%20feedback%20to%0Afine-tune%20text-to-image%20generative%20models.%20However%2C%20the%20scalability%20of%20human%0Afeedback%20collection%20has%20been%20limited%20by%20its%20reliance%20on%20manual%20annotation.%0ATherefore%2C%20we%20develop%20and%20test%20a%20method%20to%20automatically%20score%20user%20preferences%0Afrom%20their%20spontaneous%20facial%20expression%20reaction%20to%20the%20generated%20images.%20We%0Acollect%20a%20dataset%20of%20Facial%20Expression%20Reaction%20to%20Generated%20Images%20%28FERGI%29%20and%0Ashow%20that%20the%20activations%20of%20multiple%20facial%20action%20units%20%28AUs%29%20are%20highly%0Acorrelated%20with%20user%20evaluations%20of%20the%20generated%20images.%20We%20develop%20an%20FAU-Net%0A%28Facial%20Action%20Units%20Neural%20Network%29%2C%20which%20receives%20inputs%20from%20an%20AU%0Aestimation%20model%2C%20to%20automatically%20score%20user%20preferences%20for%20text-to-image%0Ageneration%20based%20on%20their%20facial%20expression%20reactions%2C%20which%20is%20complementary%0Ato%20the%20pre-trained%20scoring%20models%20based%20on%20the%20input%20text%20prompts%20and%20generated%0Aimages.%20Integrating%20our%20FAU-Net%20valence%20score%20with%20the%20pre-trained%20scoring%0Amodels%20improves%20their%20consistency%20with%20human%20preferences.%20This%20method%20of%0Aautomatic%20annotation%20with%20facial%20expression%20analysis%20can%20be%20potentially%0Ageneralized%20to%20other%20generation%20tasks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ShuangquanFeng/FERGI%2C%20and%20the%20dataset%20is%20also%20available%20at%0Athe%20same%20link%20for%20research%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03187v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFERGI%253A%2520Automatic%2520Annotation%2520of%2520User%2520Preferences%2520for%2520Text-to-Image%250A%2520%2520Generation%2520from%2520Spontaneous%2520Facial%2520Expression%2520Reaction%26entry.906535625%3DShuangquan%2520Feng%2520and%2520Junhua%2520Ma%2520and%2520Virginia%2520R.%2520de%2520Sa%26entry.1292438233%3D%2520%2520Researchers%2520have%2520proposed%2520to%2520use%2520data%2520of%2520human%2520preference%2520feedback%2520to%250Afine-tune%2520text-to-image%2520generative%2520models.%2520However%252C%2520the%2520scalability%2520of%2520human%250Afeedback%2520collection%2520has%2520been%2520limited%2520by%2520its%2520reliance%2520on%2520manual%2520annotation.%250ATherefore%252C%2520we%2520develop%2520and%2520test%2520a%2520method%2520to%2520automatically%2520score%2520user%2520preferences%250Afrom%2520their%2520spontaneous%2520facial%2520expression%2520reaction%2520to%2520the%2520generated%2520images.%2520We%250Acollect%2520a%2520dataset%2520of%2520Facial%2520Expression%2520Reaction%2520to%2520Generated%2520Images%2520%2528FERGI%2529%2520and%250Ashow%2520that%2520the%2520activations%2520of%2520multiple%2520facial%2520action%2520units%2520%2528AUs%2529%2520are%2520highly%250Acorrelated%2520with%2520user%2520evaluations%2520of%2520the%2520generated%2520images.%2520We%2520develop%2520an%2520FAU-Net%250A%2528Facial%2520Action%2520Units%2520Neural%2520Network%2529%252C%2520which%2520receives%2520inputs%2520from%2520an%2520AU%250Aestimation%2520model%252C%2520to%2520automatically%2520score%2520user%2520preferences%2520for%2520text-to-image%250Ageneration%2520based%2520on%2520their%2520facial%2520expression%2520reactions%252C%2520which%2520is%2520complementary%250Ato%2520the%2520pre-trained%2520scoring%2520models%2520based%2520on%2520the%2520input%2520text%2520prompts%2520and%2520generated%250Aimages.%2520Integrating%2520our%2520FAU-Net%2520valence%2520score%2520with%2520the%2520pre-trained%2520scoring%250Amodels%2520improves%2520their%2520consistency%2520with%2520human%2520preferences.%2520This%2520method%2520of%250Aautomatic%2520annotation%2520with%2520facial%2520expression%2520analysis%2520can%2520be%2520potentially%250Ageneralized%2520to%2520other%2520generation%2520tasks.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/ShuangquanFeng/FERGI%252C%2520and%2520the%2520dataset%2520is%2520also%2520available%2520at%250Athe%2520same%2520link%2520for%2520research%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.03187v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FERGI%3A%20Automatic%20Annotation%20of%20User%20Preferences%20for%20Text-to-Image%0A%20%20Generation%20from%20Spontaneous%20Facial%20Expression%20Reaction&entry.906535625=Shuangquan%20Feng%20and%20Junhua%20Ma%20and%20Virginia%20R.%20de%20Sa&entry.1292438233=%20%20Researchers%20have%20proposed%20to%20use%20data%20of%20human%20preference%20feedback%20to%0Afine-tune%20text-to-image%20generative%20models.%20However%2C%20the%20scalability%20of%20human%0Afeedback%20collection%20has%20been%20limited%20by%20its%20reliance%20on%20manual%20annotation.%0ATherefore%2C%20we%20develop%20and%20test%20a%20method%20to%20automatically%20score%20user%20preferences%0Afrom%20their%20spontaneous%20facial%20expression%20reaction%20to%20the%20generated%20images.%20We%0Acollect%20a%20dataset%20of%20Facial%20Expression%20Reaction%20to%20Generated%20Images%20%28FERGI%29%20and%0Ashow%20that%20the%20activations%20of%20multiple%20facial%20action%20units%20%28AUs%29%20are%20highly%0Acorrelated%20with%20user%20evaluations%20of%20the%20generated%20images.%20We%20develop%20an%20FAU-Net%0A%28Facial%20Action%20Units%20Neural%20Network%29%2C%20which%20receives%20inputs%20from%20an%20AU%0Aestimation%20model%2C%20to%20automatically%20score%20user%20preferences%20for%20text-to-image%0Ageneration%20based%20on%20their%20facial%20expression%20reactions%2C%20which%20is%20complementary%0Ato%20the%20pre-trained%20scoring%20models%20based%20on%20the%20input%20text%20prompts%20and%20generated%0Aimages.%20Integrating%20our%20FAU-Net%20valence%20score%20with%20the%20pre-trained%20scoring%0Amodels%20improves%20their%20consistency%20with%20human%20preferences.%20This%20method%20of%0Aautomatic%20annotation%20with%20facial%20expression%20analysis%20can%20be%20potentially%0Ageneralized%20to%20other%20generation%20tasks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/ShuangquanFeng/FERGI%2C%20and%20the%20dataset%20is%20also%20available%20at%0Athe%20same%20link%20for%20research%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03187v3&entry.124074799=Read"},
{"title": "CoGen: Learning from Feedback with Coupled Comprehension and Generation", "author": "Mustafa Omer Gul and Yoav Artzi", "abstract": "  Systems with both language comprehension and generation capabilities can\nbenefit from the tight connection between the two. This work studies coupling\ncomprehension and generation with focus on continually learning from\ninteraction with users. We propose techniques to tightly integrate the two\ncapabilities for both learning and inference. We situate our studies in\ntwo-player reference games, and deploy various models for thousands of\ninteractions with human users, while learning from interaction feedback\nsignals. We show dramatic improvements in performance over time, with\ncomprehension-generation coupling leading to performance improvements up to 26%\nin absolute terms and up to 17% higher accuracies compared to a non-coupled\nsystem. Our analysis also shows coupling has substantial qualitative impact on\nthe system's language, making it significantly more human-like.\n", "link": "http://arxiv.org/abs/2408.15992v1", "date": "2024-08-28", "relevancy": 2.5628, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5423}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5099}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoGen%3A%20Learning%20from%20Feedback%20with%20Coupled%20Comprehension%20and%20Generation&body=Title%3A%20CoGen%3A%20Learning%20from%20Feedback%20with%20Coupled%20Comprehension%20and%20Generation%0AAuthor%3A%20Mustafa%20Omer%20Gul%20and%20Yoav%20Artzi%0AAbstract%3A%20%20%20Systems%20with%20both%20language%20comprehension%20and%20generation%20capabilities%20can%0Abenefit%20from%20the%20tight%20connection%20between%20the%20two.%20This%20work%20studies%20coupling%0Acomprehension%20and%20generation%20with%20focus%20on%20continually%20learning%20from%0Ainteraction%20with%20users.%20We%20propose%20techniques%20to%20tightly%20integrate%20the%20two%0Acapabilities%20for%20both%20learning%20and%20inference.%20We%20situate%20our%20studies%20in%0Atwo-player%20reference%20games%2C%20and%20deploy%20various%20models%20for%20thousands%20of%0Ainteractions%20with%20human%20users%2C%20while%20learning%20from%20interaction%20feedback%0Asignals.%20We%20show%20dramatic%20improvements%20in%20performance%20over%20time%2C%20with%0Acomprehension-generation%20coupling%20leading%20to%20performance%20improvements%20up%20to%2026%25%0Ain%20absolute%20terms%20and%20up%20to%2017%25%20higher%20accuracies%20compared%20to%20a%20non-coupled%0Asystem.%20Our%20analysis%20also%20shows%20coupling%20has%20substantial%20qualitative%20impact%20on%0Athe%20system%27s%20language%2C%20making%20it%20significantly%20more%20human-like.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15992v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoGen%253A%2520Learning%2520from%2520Feedback%2520with%2520Coupled%2520Comprehension%2520and%2520Generation%26entry.906535625%3DMustafa%2520Omer%2520Gul%2520and%2520Yoav%2520Artzi%26entry.1292438233%3D%2520%2520Systems%2520with%2520both%2520language%2520comprehension%2520and%2520generation%2520capabilities%2520can%250Abenefit%2520from%2520the%2520tight%2520connection%2520between%2520the%2520two.%2520This%2520work%2520studies%2520coupling%250Acomprehension%2520and%2520generation%2520with%2520focus%2520on%2520continually%2520learning%2520from%250Ainteraction%2520with%2520users.%2520We%2520propose%2520techniques%2520to%2520tightly%2520integrate%2520the%2520two%250Acapabilities%2520for%2520both%2520learning%2520and%2520inference.%2520We%2520situate%2520our%2520studies%2520in%250Atwo-player%2520reference%2520games%252C%2520and%2520deploy%2520various%2520models%2520for%2520thousands%2520of%250Ainteractions%2520with%2520human%2520users%252C%2520while%2520learning%2520from%2520interaction%2520feedback%250Asignals.%2520We%2520show%2520dramatic%2520improvements%2520in%2520performance%2520over%2520time%252C%2520with%250Acomprehension-generation%2520coupling%2520leading%2520to%2520performance%2520improvements%2520up%2520to%252026%2525%250Ain%2520absolute%2520terms%2520and%2520up%2520to%252017%2525%2520higher%2520accuracies%2520compared%2520to%2520a%2520non-coupled%250Asystem.%2520Our%2520analysis%2520also%2520shows%2520coupling%2520has%2520substantial%2520qualitative%2520impact%2520on%250Athe%2520system%2527s%2520language%252C%2520making%2520it%2520significantly%2520more%2520human-like.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15992v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoGen%3A%20Learning%20from%20Feedback%20with%20Coupled%20Comprehension%20and%20Generation&entry.906535625=Mustafa%20Omer%20Gul%20and%20Yoav%20Artzi&entry.1292438233=%20%20Systems%20with%20both%20language%20comprehension%20and%20generation%20capabilities%20can%0Abenefit%20from%20the%20tight%20connection%20between%20the%20two.%20This%20work%20studies%20coupling%0Acomprehension%20and%20generation%20with%20focus%20on%20continually%20learning%20from%0Ainteraction%20with%20users.%20We%20propose%20techniques%20to%20tightly%20integrate%20the%20two%0Acapabilities%20for%20both%20learning%20and%20inference.%20We%20situate%20our%20studies%20in%0Atwo-player%20reference%20games%2C%20and%20deploy%20various%20models%20for%20thousands%20of%0Ainteractions%20with%20human%20users%2C%20while%20learning%20from%20interaction%20feedback%0Asignals.%20We%20show%20dramatic%20improvements%20in%20performance%20over%20time%2C%20with%0Acomprehension-generation%20coupling%20leading%20to%20performance%20improvements%20up%20to%2026%25%0Ain%20absolute%20terms%20and%20up%20to%2017%25%20higher%20accuracies%20compared%20to%20a%20non-coupled%0Asystem.%20Our%20analysis%20also%20shows%20coupling%20has%20substantial%20qualitative%20impact%20on%0Athe%20system%27s%20language%2C%20making%20it%20significantly%20more%20human-like.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15992v1&entry.124074799=Read"},
{"title": "Auxiliary Input in Training: Incorporating Catheter Features into Deep\n  Learning Models for ECG-Free Dynamic Coronary Roadmapping", "author": "Yikang Liu and Lin Zhao and Eric Z. Chen and Xiao Chen and Terrence Chen and Shanhui Sun", "abstract": "  Dynamic coronary roadmapping is a technology that overlays the vessel maps\n(the \"roadmap\") extracted from an offline image sequence of X-ray angiography\nonto a live stream of X-ray fluoroscopy in real-time. It aims to offer\nnavigational guidance for interventional surgeries without the need for\nrepeated contrast agent injections, thereby reducing the risks associated with\nradiation exposure and kidney failure. The precision of the roadmaps is\ncontingent upon the accurate alignment of angiographic and fluoroscopic images\nbased on their cardiac phases, as well as precise catheter tip tracking. The\nformer ensures the selection of a roadmap that closely matches the vessel shape\nin the current frame, while the latter uses catheter tips as reference points\nto adjust for translational motion between the roadmap and the present vessel\ntree. Training deep learning models for both tasks is challenging and\nunderexplored. However, incorporating catheter features into the models could\noffer substantial benefits, given humans heavily rely on catheters to complete\nthe tasks. To this end, we introduce a simple but effective method, auxiliary\ninput in training (AIT), and demonstrate that it enhances model performance\nacross both tasks, outperforming baseline methods in knowledge incorporation\nand transfer learning.\n", "link": "http://arxiv.org/abs/2408.15947v1", "date": "2024-08-28", "relevancy": 2.5592, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5193}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5168}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Auxiliary%20Input%20in%20Training%3A%20Incorporating%20Catheter%20Features%20into%20Deep%0A%20%20Learning%20Models%20for%20ECG-Free%20Dynamic%20Coronary%20Roadmapping&body=Title%3A%20Auxiliary%20Input%20in%20Training%3A%20Incorporating%20Catheter%20Features%20into%20Deep%0A%20%20Learning%20Models%20for%20ECG-Free%20Dynamic%20Coronary%20Roadmapping%0AAuthor%3A%20Yikang%20Liu%20and%20Lin%20Zhao%20and%20Eric%20Z.%20Chen%20and%20Xiao%20Chen%20and%20Terrence%20Chen%20and%20Shanhui%20Sun%0AAbstract%3A%20%20%20Dynamic%20coronary%20roadmapping%20is%20a%20technology%20that%20overlays%20the%20vessel%20maps%0A%28the%20%22roadmap%22%29%20extracted%20from%20an%20offline%20image%20sequence%20of%20X-ray%20angiography%0Aonto%20a%20live%20stream%20of%20X-ray%20fluoroscopy%20in%20real-time.%20It%20aims%20to%20offer%0Anavigational%20guidance%20for%20interventional%20surgeries%20without%20the%20need%20for%0Arepeated%20contrast%20agent%20injections%2C%20thereby%20reducing%20the%20risks%20associated%20with%0Aradiation%20exposure%20and%20kidney%20failure.%20The%20precision%20of%20the%20roadmaps%20is%0Acontingent%20upon%20the%20accurate%20alignment%20of%20angiographic%20and%20fluoroscopic%20images%0Abased%20on%20their%20cardiac%20phases%2C%20as%20well%20as%20precise%20catheter%20tip%20tracking.%20The%0Aformer%20ensures%20the%20selection%20of%20a%20roadmap%20that%20closely%20matches%20the%20vessel%20shape%0Ain%20the%20current%20frame%2C%20while%20the%20latter%20uses%20catheter%20tips%20as%20reference%20points%0Ato%20adjust%20for%20translational%20motion%20between%20the%20roadmap%20and%20the%20present%20vessel%0Atree.%20Training%20deep%20learning%20models%20for%20both%20tasks%20is%20challenging%20and%0Aunderexplored.%20However%2C%20incorporating%20catheter%20features%20into%20the%20models%20could%0Aoffer%20substantial%20benefits%2C%20given%20humans%20heavily%20rely%20on%20catheters%20to%20complete%0Athe%20tasks.%20To%20this%20end%2C%20we%20introduce%20a%20simple%20but%20effective%20method%2C%20auxiliary%0Ainput%20in%20training%20%28AIT%29%2C%20and%20demonstrate%20that%20it%20enhances%20model%20performance%0Aacross%20both%20tasks%2C%20outperforming%20baseline%20methods%20in%20knowledge%20incorporation%0Aand%20transfer%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15947v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuxiliary%2520Input%2520in%2520Training%253A%2520Incorporating%2520Catheter%2520Features%2520into%2520Deep%250A%2520%2520Learning%2520Models%2520for%2520ECG-Free%2520Dynamic%2520Coronary%2520Roadmapping%26entry.906535625%3DYikang%2520Liu%2520and%2520Lin%2520Zhao%2520and%2520Eric%2520Z.%2520Chen%2520and%2520Xiao%2520Chen%2520and%2520Terrence%2520Chen%2520and%2520Shanhui%2520Sun%26entry.1292438233%3D%2520%2520Dynamic%2520coronary%2520roadmapping%2520is%2520a%2520technology%2520that%2520overlays%2520the%2520vessel%2520maps%250A%2528the%2520%2522roadmap%2522%2529%2520extracted%2520from%2520an%2520offline%2520image%2520sequence%2520of%2520X-ray%2520angiography%250Aonto%2520a%2520live%2520stream%2520of%2520X-ray%2520fluoroscopy%2520in%2520real-time.%2520It%2520aims%2520to%2520offer%250Anavigational%2520guidance%2520for%2520interventional%2520surgeries%2520without%2520the%2520need%2520for%250Arepeated%2520contrast%2520agent%2520injections%252C%2520thereby%2520reducing%2520the%2520risks%2520associated%2520with%250Aradiation%2520exposure%2520and%2520kidney%2520failure.%2520The%2520precision%2520of%2520the%2520roadmaps%2520is%250Acontingent%2520upon%2520the%2520accurate%2520alignment%2520of%2520angiographic%2520and%2520fluoroscopic%2520images%250Abased%2520on%2520their%2520cardiac%2520phases%252C%2520as%2520well%2520as%2520precise%2520catheter%2520tip%2520tracking.%2520The%250Aformer%2520ensures%2520the%2520selection%2520of%2520a%2520roadmap%2520that%2520closely%2520matches%2520the%2520vessel%2520shape%250Ain%2520the%2520current%2520frame%252C%2520while%2520the%2520latter%2520uses%2520catheter%2520tips%2520as%2520reference%2520points%250Ato%2520adjust%2520for%2520translational%2520motion%2520between%2520the%2520roadmap%2520and%2520the%2520present%2520vessel%250Atree.%2520Training%2520deep%2520learning%2520models%2520for%2520both%2520tasks%2520is%2520challenging%2520and%250Aunderexplored.%2520However%252C%2520incorporating%2520catheter%2520features%2520into%2520the%2520models%2520could%250Aoffer%2520substantial%2520benefits%252C%2520given%2520humans%2520heavily%2520rely%2520on%2520catheters%2520to%2520complete%250Athe%2520tasks.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520simple%2520but%2520effective%2520method%252C%2520auxiliary%250Ainput%2520in%2520training%2520%2528AIT%2529%252C%2520and%2520demonstrate%2520that%2520it%2520enhances%2520model%2520performance%250Aacross%2520both%2520tasks%252C%2520outperforming%2520baseline%2520methods%2520in%2520knowledge%2520incorporation%250Aand%2520transfer%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15947v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Auxiliary%20Input%20in%20Training%3A%20Incorporating%20Catheter%20Features%20into%20Deep%0A%20%20Learning%20Models%20for%20ECG-Free%20Dynamic%20Coronary%20Roadmapping&entry.906535625=Yikang%20Liu%20and%20Lin%20Zhao%20and%20Eric%20Z.%20Chen%20and%20Xiao%20Chen%20and%20Terrence%20Chen%20and%20Shanhui%20Sun&entry.1292438233=%20%20Dynamic%20coronary%20roadmapping%20is%20a%20technology%20that%20overlays%20the%20vessel%20maps%0A%28the%20%22roadmap%22%29%20extracted%20from%20an%20offline%20image%20sequence%20of%20X-ray%20angiography%0Aonto%20a%20live%20stream%20of%20X-ray%20fluoroscopy%20in%20real-time.%20It%20aims%20to%20offer%0Anavigational%20guidance%20for%20interventional%20surgeries%20without%20the%20need%20for%0Arepeated%20contrast%20agent%20injections%2C%20thereby%20reducing%20the%20risks%20associated%20with%0Aradiation%20exposure%20and%20kidney%20failure.%20The%20precision%20of%20the%20roadmaps%20is%0Acontingent%20upon%20the%20accurate%20alignment%20of%20angiographic%20and%20fluoroscopic%20images%0Abased%20on%20their%20cardiac%20phases%2C%20as%20well%20as%20precise%20catheter%20tip%20tracking.%20The%0Aformer%20ensures%20the%20selection%20of%20a%20roadmap%20that%20closely%20matches%20the%20vessel%20shape%0Ain%20the%20current%20frame%2C%20while%20the%20latter%20uses%20catheter%20tips%20as%20reference%20points%0Ato%20adjust%20for%20translational%20motion%20between%20the%20roadmap%20and%20the%20present%20vessel%0Atree.%20Training%20deep%20learning%20models%20for%20both%20tasks%20is%20challenging%20and%0Aunderexplored.%20However%2C%20incorporating%20catheter%20features%20into%20the%20models%20could%0Aoffer%20substantial%20benefits%2C%20given%20humans%20heavily%20rely%20on%20catheters%20to%20complete%0Athe%20tasks.%20To%20this%20end%2C%20we%20introduce%20a%20simple%20but%20effective%20method%2C%20auxiliary%0Ainput%20in%20training%20%28AIT%29%2C%20and%20demonstrate%20that%20it%20enhances%20model%20performance%0Aacross%20both%20tasks%2C%20outperforming%20baseline%20methods%20in%20knowledge%20incorporation%0Aand%20transfer%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15947v1&entry.124074799=Read"},
{"title": "Language Adaptation on a Tight Academic Compute Budget: Tokenizer\n  Swapping Works and Pure bfloat16 Is Enough", "author": "Konstantin Dobler and Gerard de Melo", "abstract": "  We investigate continued pretraining of LLMs for language adaptation on a\ntight academic budget: a setting in which only a few GPUs can be used in\nparallel, for a heavily constrained duration. We focus on adapting Mistral-7B\nto German or Arabic and evaluate several techniques to improve efficiency and\neffectiveness in this setting. Our German models adapted on this tight compute\nbudget underperform compared to the base Mistral-7B, while our Arabic models\noutperform several baselines, showing that for sufficiently well-represented\nlanguages, continued pretraining for specialization is not always helpful. Our\nmain findings focus on training precision and tokenizer swapping. Our results\nshow that pure bfloat16 training is a viable alternative to mixed-precision\ntraining, while being much faster when only using a few GPUs. Swapping the\ntokenizer for a specialized one yields more efficient tokenization and is\ncompetitive with the original tokenizer, which already contains some German\ntokens, but did not significantly increase performance for German. Code and\nmodel weights are available at on GitHub.\n", "link": "http://arxiv.org/abs/2408.15793v1", "date": "2024-08-28", "relevancy": 2.5216, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5289}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4937}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language%20Adaptation%20on%20a%20Tight%20Academic%20Compute%20Budget%3A%20Tokenizer%0A%20%20Swapping%20Works%20and%20Pure%20bfloat16%20Is%20Enough&body=Title%3A%20Language%20Adaptation%20on%20a%20Tight%20Academic%20Compute%20Budget%3A%20Tokenizer%0A%20%20Swapping%20Works%20and%20Pure%20bfloat16%20Is%20Enough%0AAuthor%3A%20Konstantin%20Dobler%20and%20Gerard%20de%20Melo%0AAbstract%3A%20%20%20We%20investigate%20continued%20pretraining%20of%20LLMs%20for%20language%20adaptation%20on%20a%0Atight%20academic%20budget%3A%20a%20setting%20in%20which%20only%20a%20few%20GPUs%20can%20be%20used%20in%0Aparallel%2C%20for%20a%20heavily%20constrained%20duration.%20We%20focus%20on%20adapting%20Mistral-7B%0Ato%20German%20or%20Arabic%20and%20evaluate%20several%20techniques%20to%20improve%20efficiency%20and%0Aeffectiveness%20in%20this%20setting.%20Our%20German%20models%20adapted%20on%20this%20tight%20compute%0Abudget%20underperform%20compared%20to%20the%20base%20Mistral-7B%2C%20while%20our%20Arabic%20models%0Aoutperform%20several%20baselines%2C%20showing%20that%20for%20sufficiently%20well-represented%0Alanguages%2C%20continued%20pretraining%20for%20specialization%20is%20not%20always%20helpful.%20Our%0Amain%20findings%20focus%20on%20training%20precision%20and%20tokenizer%20swapping.%20Our%20results%0Ashow%20that%20pure%20bfloat16%20training%20is%20a%20viable%20alternative%20to%20mixed-precision%0Atraining%2C%20while%20being%20much%20faster%20when%20only%20using%20a%20few%20GPUs.%20Swapping%20the%0Atokenizer%20for%20a%20specialized%20one%20yields%20more%20efficient%20tokenization%20and%20is%0Acompetitive%20with%20the%20original%20tokenizer%2C%20which%20already%20contains%20some%20German%0Atokens%2C%20but%20did%20not%20significantly%20increase%20performance%20for%20German.%20Code%20and%0Amodel%20weights%20are%20available%20at%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15793v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage%2520Adaptation%2520on%2520a%2520Tight%2520Academic%2520Compute%2520Budget%253A%2520Tokenizer%250A%2520%2520Swapping%2520Works%2520and%2520Pure%2520bfloat16%2520Is%2520Enough%26entry.906535625%3DKonstantin%2520Dobler%2520and%2520Gerard%2520de%2520Melo%26entry.1292438233%3D%2520%2520We%2520investigate%2520continued%2520pretraining%2520of%2520LLMs%2520for%2520language%2520adaptation%2520on%2520a%250Atight%2520academic%2520budget%253A%2520a%2520setting%2520in%2520which%2520only%2520a%2520few%2520GPUs%2520can%2520be%2520used%2520in%250Aparallel%252C%2520for%2520a%2520heavily%2520constrained%2520duration.%2520We%2520focus%2520on%2520adapting%2520Mistral-7B%250Ato%2520German%2520or%2520Arabic%2520and%2520evaluate%2520several%2520techniques%2520to%2520improve%2520efficiency%2520and%250Aeffectiveness%2520in%2520this%2520setting.%2520Our%2520German%2520models%2520adapted%2520on%2520this%2520tight%2520compute%250Abudget%2520underperform%2520compared%2520to%2520the%2520base%2520Mistral-7B%252C%2520while%2520our%2520Arabic%2520models%250Aoutperform%2520several%2520baselines%252C%2520showing%2520that%2520for%2520sufficiently%2520well-represented%250Alanguages%252C%2520continued%2520pretraining%2520for%2520specialization%2520is%2520not%2520always%2520helpful.%2520Our%250Amain%2520findings%2520focus%2520on%2520training%2520precision%2520and%2520tokenizer%2520swapping.%2520Our%2520results%250Ashow%2520that%2520pure%2520bfloat16%2520training%2520is%2520a%2520viable%2520alternative%2520to%2520mixed-precision%250Atraining%252C%2520while%2520being%2520much%2520faster%2520when%2520only%2520using%2520a%2520few%2520GPUs.%2520Swapping%2520the%250Atokenizer%2520for%2520a%2520specialized%2520one%2520yields%2520more%2520efficient%2520tokenization%2520and%2520is%250Acompetitive%2520with%2520the%2520original%2520tokenizer%252C%2520which%2520already%2520contains%2520some%2520German%250Atokens%252C%2520but%2520did%2520not%2520significantly%2520increase%2520performance%2520for%2520German.%2520Code%2520and%250Amodel%2520weights%2520are%2520available%2520at%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15793v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language%20Adaptation%20on%20a%20Tight%20Academic%20Compute%20Budget%3A%20Tokenizer%0A%20%20Swapping%20Works%20and%20Pure%20bfloat16%20Is%20Enough&entry.906535625=Konstantin%20Dobler%20and%20Gerard%20de%20Melo&entry.1292438233=%20%20We%20investigate%20continued%20pretraining%20of%20LLMs%20for%20language%20adaptation%20on%20a%0Atight%20academic%20budget%3A%20a%20setting%20in%20which%20only%20a%20few%20GPUs%20can%20be%20used%20in%0Aparallel%2C%20for%20a%20heavily%20constrained%20duration.%20We%20focus%20on%20adapting%20Mistral-7B%0Ato%20German%20or%20Arabic%20and%20evaluate%20several%20techniques%20to%20improve%20efficiency%20and%0Aeffectiveness%20in%20this%20setting.%20Our%20German%20models%20adapted%20on%20this%20tight%20compute%0Abudget%20underperform%20compared%20to%20the%20base%20Mistral-7B%2C%20while%20our%20Arabic%20models%0Aoutperform%20several%20baselines%2C%20showing%20that%20for%20sufficiently%20well-represented%0Alanguages%2C%20continued%20pretraining%20for%20specialization%20is%20not%20always%20helpful.%20Our%0Amain%20findings%20focus%20on%20training%20precision%20and%20tokenizer%20swapping.%20Our%20results%0Ashow%20that%20pure%20bfloat16%20training%20is%20a%20viable%20alternative%20to%20mixed-precision%0Atraining%2C%20while%20being%20much%20faster%20when%20only%20using%20a%20few%20GPUs.%20Swapping%20the%0Atokenizer%20for%20a%20specialized%20one%20yields%20more%20efficient%20tokenization%20and%20is%0Acompetitive%20with%20the%20original%20tokenizer%2C%20which%20already%20contains%20some%20German%0Atokens%2C%20but%20did%20not%20significantly%20increase%20performance%20for%20German.%20Code%20and%0Amodel%20weights%20are%20available%20at%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15793v1&entry.124074799=Read"},
{"title": "DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping", "author": "Kutay Y\u0131lmaz and Matthias Nie\u00dfner and Anastasiia Kornilova and Alexey Artemov", "abstract": "  Recently, significant progress has been achieved in sensing real large-scale\noutdoor 3D environments, particularly by using modern acquisition equipment\nsuch as LiDAR sensors. Unfortunately, they are fundamentally limited in their\nability to produce dense, complete 3D scenes. To address this issue, recent\nlearning-based methods integrate neural implicit representations and\noptimizable feature grids to approximate surfaces of 3D scenes. However,\nnaively fitting samples along raw LiDAR rays leads to noisy 3D mapping results\ndue to the nature of sparse, conflicting LiDAR measurements. Instead, in this\nwork we depart from fitting LiDAR data exactly, instead letting the network\noptimize a non-metric monotonic implicit field defined in 3D space. To fit our\nfield, we design a learning system integrating a monotonicity loss that enables\noptimizing neural monotonic fields and leverages recent progress in large-scale\n3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as\ncaptured by multiple quantitative and perceptual measures and visual results\nobtained for Mai City, Newer College, and KITTI benchmarks. The code of our\napproach will be made publicly available.\n", "link": "http://arxiv.org/abs/2403.17550v2", "date": "2024-08-28", "relevancy": 2.4582, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6281}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6073}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepMIF%3A%20Deep%20Monotonic%20Implicit%20Fields%20for%20Large-Scale%20LiDAR%203D%20Mapping&body=Title%3A%20DeepMIF%3A%20Deep%20Monotonic%20Implicit%20Fields%20for%20Large-Scale%20LiDAR%203D%20Mapping%0AAuthor%3A%20Kutay%20Y%C4%B1lmaz%20and%20Matthias%20Nie%C3%9Fner%20and%20Anastasiia%20Kornilova%20and%20Alexey%20Artemov%0AAbstract%3A%20%20%20Recently%2C%20significant%20progress%20has%20been%20achieved%20in%20sensing%20real%20large-scale%0Aoutdoor%203D%20environments%2C%20particularly%20by%20using%20modern%20acquisition%20equipment%0Asuch%20as%20LiDAR%20sensors.%20Unfortunately%2C%20they%20are%20fundamentally%20limited%20in%20their%0Aability%20to%20produce%20dense%2C%20complete%203D%20scenes.%20To%20address%20this%20issue%2C%20recent%0Alearning-based%20methods%20integrate%20neural%20implicit%20representations%20and%0Aoptimizable%20feature%20grids%20to%20approximate%20surfaces%20of%203D%20scenes.%20However%2C%0Anaively%20fitting%20samples%20along%20raw%20LiDAR%20rays%20leads%20to%20noisy%203D%20mapping%20results%0Adue%20to%20the%20nature%20of%20sparse%2C%20conflicting%20LiDAR%20measurements.%20Instead%2C%20in%20this%0Awork%20we%20depart%20from%20fitting%20LiDAR%20data%20exactly%2C%20instead%20letting%20the%20network%0Aoptimize%20a%20non-metric%20monotonic%20implicit%20field%20defined%20in%203D%20space.%20To%20fit%20our%0Afield%2C%20we%20design%20a%20learning%20system%20integrating%20a%20monotonicity%20loss%20that%20enables%0Aoptimizing%20neural%20monotonic%20fields%20and%20leverages%20recent%20progress%20in%20large-scale%0A3D%20mapping.%20Our%20algorithm%20achieves%20high-quality%20dense%203D%20mapping%20performance%20as%0Acaptured%20by%20multiple%20quantitative%20and%20perceptual%20measures%20and%20visual%20results%0Aobtained%20for%20Mai%20City%2C%20Newer%20College%2C%20and%20KITTI%20benchmarks.%20The%20code%20of%20our%0Aapproach%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.17550v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepMIF%253A%2520Deep%2520Monotonic%2520Implicit%2520Fields%2520for%2520Large-Scale%2520LiDAR%25203D%2520Mapping%26entry.906535625%3DKutay%2520Y%25C4%25B1lmaz%2520and%2520Matthias%2520Nie%25C3%259Fner%2520and%2520Anastasiia%2520Kornilova%2520and%2520Alexey%2520Artemov%26entry.1292438233%3D%2520%2520Recently%252C%2520significant%2520progress%2520has%2520been%2520achieved%2520in%2520sensing%2520real%2520large-scale%250Aoutdoor%25203D%2520environments%252C%2520particularly%2520by%2520using%2520modern%2520acquisition%2520equipment%250Asuch%2520as%2520LiDAR%2520sensors.%2520Unfortunately%252C%2520they%2520are%2520fundamentally%2520limited%2520in%2520their%250Aability%2520to%2520produce%2520dense%252C%2520complete%25203D%2520scenes.%2520To%2520address%2520this%2520issue%252C%2520recent%250Alearning-based%2520methods%2520integrate%2520neural%2520implicit%2520representations%2520and%250Aoptimizable%2520feature%2520grids%2520to%2520approximate%2520surfaces%2520of%25203D%2520scenes.%2520However%252C%250Anaively%2520fitting%2520samples%2520along%2520raw%2520LiDAR%2520rays%2520leads%2520to%2520noisy%25203D%2520mapping%2520results%250Adue%2520to%2520the%2520nature%2520of%2520sparse%252C%2520conflicting%2520LiDAR%2520measurements.%2520Instead%252C%2520in%2520this%250Awork%2520we%2520depart%2520from%2520fitting%2520LiDAR%2520data%2520exactly%252C%2520instead%2520letting%2520the%2520network%250Aoptimize%2520a%2520non-metric%2520monotonic%2520implicit%2520field%2520defined%2520in%25203D%2520space.%2520To%2520fit%2520our%250Afield%252C%2520we%2520design%2520a%2520learning%2520system%2520integrating%2520a%2520monotonicity%2520loss%2520that%2520enables%250Aoptimizing%2520neural%2520monotonic%2520fields%2520and%2520leverages%2520recent%2520progress%2520in%2520large-scale%250A3D%2520mapping.%2520Our%2520algorithm%2520achieves%2520high-quality%2520dense%25203D%2520mapping%2520performance%2520as%250Acaptured%2520by%2520multiple%2520quantitative%2520and%2520perceptual%2520measures%2520and%2520visual%2520results%250Aobtained%2520for%2520Mai%2520City%252C%2520Newer%2520College%252C%2520and%2520KITTI%2520benchmarks.%2520The%2520code%2520of%2520our%250Aapproach%2520will%2520be%2520made%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.17550v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepMIF%3A%20Deep%20Monotonic%20Implicit%20Fields%20for%20Large-Scale%20LiDAR%203D%20Mapping&entry.906535625=Kutay%20Y%C4%B1lmaz%20and%20Matthias%20Nie%C3%9Fner%20and%20Anastasiia%20Kornilova%20and%20Alexey%20Artemov&entry.1292438233=%20%20Recently%2C%20significant%20progress%20has%20been%20achieved%20in%20sensing%20real%20large-scale%0Aoutdoor%203D%20environments%2C%20particularly%20by%20using%20modern%20acquisition%20equipment%0Asuch%20as%20LiDAR%20sensors.%20Unfortunately%2C%20they%20are%20fundamentally%20limited%20in%20their%0Aability%20to%20produce%20dense%2C%20complete%203D%20scenes.%20To%20address%20this%20issue%2C%20recent%0Alearning-based%20methods%20integrate%20neural%20implicit%20representations%20and%0Aoptimizable%20feature%20grids%20to%20approximate%20surfaces%20of%203D%20scenes.%20However%2C%0Anaively%20fitting%20samples%20along%20raw%20LiDAR%20rays%20leads%20to%20noisy%203D%20mapping%20results%0Adue%20to%20the%20nature%20of%20sparse%2C%20conflicting%20LiDAR%20measurements.%20Instead%2C%20in%20this%0Awork%20we%20depart%20from%20fitting%20LiDAR%20data%20exactly%2C%20instead%20letting%20the%20network%0Aoptimize%20a%20non-metric%20monotonic%20implicit%20field%20defined%20in%203D%20space.%20To%20fit%20our%0Afield%2C%20we%20design%20a%20learning%20system%20integrating%20a%20monotonicity%20loss%20that%20enables%0Aoptimizing%20neural%20monotonic%20fields%20and%20leverages%20recent%20progress%20in%20large-scale%0A3D%20mapping.%20Our%20algorithm%20achieves%20high-quality%20dense%203D%20mapping%20performance%20as%0Acaptured%20by%20multiple%20quantitative%20and%20perceptual%20measures%20and%20visual%20results%0Aobtained%20for%20Mai%20City%2C%20Newer%20College%2C%20and%20KITTI%20benchmarks.%20The%20code%20of%20our%0Aapproach%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.17550v2&entry.124074799=Read"},
{"title": "GenDDS: Generating Diverse Driving Video Scenarios with Prompt-to-Video\n  Generative Model", "author": "Yongjie Fu and Yunlong Li and Xuan Di", "abstract": "  Autonomous driving training requires a diverse range of datasets encompassing\nvarious traffic conditions, weather scenarios, and road types. Traditional data\naugmentation methods often struggle to generate datasets that represent rare\noccurrences. To address this challenge, we propose GenDDS, a novel approach for\ngenerating driving scenarios generation by leveraging the capabilities of\nStable Diffusion XL (SDXL), an advanced latent diffusion model. Our methodology\ninvolves the use of descriptive prompts to guide the synthesis process, aimed\nat producing realistic and diverse driving scenarios. With the power of the\nlatest computer vision techniques, such as ControlNet and Hotshot-XL, we have\nbuilt a complete pipeline for video generation together with SDXL. We employ\nthe KITTI dataset, which includes real-world driving videos, to train the\nmodel. Through a series of experiments, we demonstrate that our model can\ngenerate high-quality driving videos that closely replicate the complexity and\nvariability of real-world driving scenarios. This research contributes to the\ndevelopment of sophisticated training data for autonomous driving systems and\nopens new avenues for creating virtual environments for simulation and\nvalidation purposes.\n", "link": "http://arxiv.org/abs/2408.15868v1", "date": "2024-08-28", "relevancy": 2.4501, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6321}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6203}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenDDS%3A%20Generating%20Diverse%20Driving%20Video%20Scenarios%20with%20Prompt-to-Video%0A%20%20Generative%20Model&body=Title%3A%20GenDDS%3A%20Generating%20Diverse%20Driving%20Video%20Scenarios%20with%20Prompt-to-Video%0A%20%20Generative%20Model%0AAuthor%3A%20Yongjie%20Fu%20and%20Yunlong%20Li%20and%20Xuan%20Di%0AAbstract%3A%20%20%20Autonomous%20driving%20training%20requires%20a%20diverse%20range%20of%20datasets%20encompassing%0Avarious%20traffic%20conditions%2C%20weather%20scenarios%2C%20and%20road%20types.%20Traditional%20data%0Aaugmentation%20methods%20often%20struggle%20to%20generate%20datasets%20that%20represent%20rare%0Aoccurrences.%20To%20address%20this%20challenge%2C%20we%20propose%20GenDDS%2C%20a%20novel%20approach%20for%0Agenerating%20driving%20scenarios%20generation%20by%20leveraging%20the%20capabilities%20of%0AStable%20Diffusion%20XL%20%28SDXL%29%2C%20an%20advanced%20latent%20diffusion%20model.%20Our%20methodology%0Ainvolves%20the%20use%20of%20descriptive%20prompts%20to%20guide%20the%20synthesis%20process%2C%20aimed%0Aat%20producing%20realistic%20and%20diverse%20driving%20scenarios.%20With%20the%20power%20of%20the%0Alatest%20computer%20vision%20techniques%2C%20such%20as%20ControlNet%20and%20Hotshot-XL%2C%20we%20have%0Abuilt%20a%20complete%20pipeline%20for%20video%20generation%20together%20with%20SDXL.%20We%20employ%0Athe%20KITTI%20dataset%2C%20which%20includes%20real-world%20driving%20videos%2C%20to%20train%20the%0Amodel.%20Through%20a%20series%20of%20experiments%2C%20we%20demonstrate%20that%20our%20model%20can%0Agenerate%20high-quality%20driving%20videos%20that%20closely%20replicate%20the%20complexity%20and%0Avariability%20of%20real-world%20driving%20scenarios.%20This%20research%20contributes%20to%20the%0Adevelopment%20of%20sophisticated%20training%20data%20for%20autonomous%20driving%20systems%20and%0Aopens%20new%20avenues%20for%20creating%20virtual%20environments%20for%20simulation%20and%0Avalidation%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenDDS%253A%2520Generating%2520Diverse%2520Driving%2520Video%2520Scenarios%2520with%2520Prompt-to-Video%250A%2520%2520Generative%2520Model%26entry.906535625%3DYongjie%2520Fu%2520and%2520Yunlong%2520Li%2520and%2520Xuan%2520Di%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520training%2520requires%2520a%2520diverse%2520range%2520of%2520datasets%2520encompassing%250Avarious%2520traffic%2520conditions%252C%2520weather%2520scenarios%252C%2520and%2520road%2520types.%2520Traditional%2520data%250Aaugmentation%2520methods%2520often%2520struggle%2520to%2520generate%2520datasets%2520that%2520represent%2520rare%250Aoccurrences.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520GenDDS%252C%2520a%2520novel%2520approach%2520for%250Agenerating%2520driving%2520scenarios%2520generation%2520by%2520leveraging%2520the%2520capabilities%2520of%250AStable%2520Diffusion%2520XL%2520%2528SDXL%2529%252C%2520an%2520advanced%2520latent%2520diffusion%2520model.%2520Our%2520methodology%250Ainvolves%2520the%2520use%2520of%2520descriptive%2520prompts%2520to%2520guide%2520the%2520synthesis%2520process%252C%2520aimed%250Aat%2520producing%2520realistic%2520and%2520diverse%2520driving%2520scenarios.%2520With%2520the%2520power%2520of%2520the%250Alatest%2520computer%2520vision%2520techniques%252C%2520such%2520as%2520ControlNet%2520and%2520Hotshot-XL%252C%2520we%2520have%250Abuilt%2520a%2520complete%2520pipeline%2520for%2520video%2520generation%2520together%2520with%2520SDXL.%2520We%2520employ%250Athe%2520KITTI%2520dataset%252C%2520which%2520includes%2520real-world%2520driving%2520videos%252C%2520to%2520train%2520the%250Amodel.%2520Through%2520a%2520series%2520of%2520experiments%252C%2520we%2520demonstrate%2520that%2520our%2520model%2520can%250Agenerate%2520high-quality%2520driving%2520videos%2520that%2520closely%2520replicate%2520the%2520complexity%2520and%250Avariability%2520of%2520real-world%2520driving%2520scenarios.%2520This%2520research%2520contributes%2520to%2520the%250Adevelopment%2520of%2520sophisticated%2520training%2520data%2520for%2520autonomous%2520driving%2520systems%2520and%250Aopens%2520new%2520avenues%2520for%2520creating%2520virtual%2520environments%2520for%2520simulation%2520and%250Avalidation%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenDDS%3A%20Generating%20Diverse%20Driving%20Video%20Scenarios%20with%20Prompt-to-Video%0A%20%20Generative%20Model&entry.906535625=Yongjie%20Fu%20and%20Yunlong%20Li%20and%20Xuan%20Di&entry.1292438233=%20%20Autonomous%20driving%20training%20requires%20a%20diverse%20range%20of%20datasets%20encompassing%0Avarious%20traffic%20conditions%2C%20weather%20scenarios%2C%20and%20road%20types.%20Traditional%20data%0Aaugmentation%20methods%20often%20struggle%20to%20generate%20datasets%20that%20represent%20rare%0Aoccurrences.%20To%20address%20this%20challenge%2C%20we%20propose%20GenDDS%2C%20a%20novel%20approach%20for%0Agenerating%20driving%20scenarios%20generation%20by%20leveraging%20the%20capabilities%20of%0AStable%20Diffusion%20XL%20%28SDXL%29%2C%20an%20advanced%20latent%20diffusion%20model.%20Our%20methodology%0Ainvolves%20the%20use%20of%20descriptive%20prompts%20to%20guide%20the%20synthesis%20process%2C%20aimed%0Aat%20producing%20realistic%20and%20diverse%20driving%20scenarios.%20With%20the%20power%20of%20the%0Alatest%20computer%20vision%20techniques%2C%20such%20as%20ControlNet%20and%20Hotshot-XL%2C%20we%20have%0Abuilt%20a%20complete%20pipeline%20for%20video%20generation%20together%20with%20SDXL.%20We%20employ%0Athe%20KITTI%20dataset%2C%20which%20includes%20real-world%20driving%20videos%2C%20to%20train%20the%0Amodel.%20Through%20a%20series%20of%20experiments%2C%20we%20demonstrate%20that%20our%20model%20can%0Agenerate%20high-quality%20driving%20videos%20that%20closely%20replicate%20the%20complexity%20and%0Avariability%20of%20real-world%20driving%20scenarios.%20This%20research%20contributes%20to%20the%0Adevelopment%20of%20sophisticated%20training%20data%20for%20autonomous%20driving%20systems%20and%0Aopens%20new%20avenues%20for%20creating%20virtual%20environments%20for%20simulation%20and%0Avalidation%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15868v1&entry.124074799=Read"},
{"title": "TEDRA: Text-based Editing of Dynamic and Photoreal Actors", "author": "Basavaraj Sunagad and Heming Zhu and Mohit Mendiratta and Adam Kortylewski and Christian Theobalt and Marc Habermann", "abstract": "  Over the past years, significant progress has been made in creating\nphotorealistic and drivable 3D avatars solely from videos of real humans.\nHowever, a core remaining challenge is the fine-grained and user-friendly\nediting of clothing styles by means of textual descriptions. To this end, we\npresent TEDRA, the first method allowing text-based edits of an avatar, which\nmaintains the avatar's high fidelity, space-time coherency, as well as\ndynamics, and enables skeletal pose and view control. We begin by training a\nmodel to create a controllable and high-fidelity digital replica of the real\nactor. Next, we personalize a pretrained generative diffusion model by\nfine-tuning it on various frames of the real character captured from different\ncamera angles, ensuring the digital representation faithfully captures the\ndynamics and movements of the real person. This two-stage process lays the\nfoundation for our approach to dynamic human avatar editing. Utilizing this\npersonalized diffusion model, we modify the dynamic avatar based on a provided\ntext prompt using our Personalized Normal Aligned Score Distillation Sampling\n(PNA-SDS) within a model-based guidance framework. Additionally, we propose a\ntime step annealing strategy to ensure high-quality edits. Our results\ndemonstrate a clear improvement over prior work in functionality and visual\nquality.\n", "link": "http://arxiv.org/abs/2408.15995v1", "date": "2024-08-28", "relevancy": 2.4076, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6326}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6124}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TEDRA%3A%20Text-based%20Editing%20of%20Dynamic%20and%20Photoreal%20Actors&body=Title%3A%20TEDRA%3A%20Text-based%20Editing%20of%20Dynamic%20and%20Photoreal%20Actors%0AAuthor%3A%20Basavaraj%20Sunagad%20and%20Heming%20Zhu%20and%20Mohit%20Mendiratta%20and%20Adam%20Kortylewski%20and%20Christian%20Theobalt%20and%20Marc%20Habermann%0AAbstract%3A%20%20%20Over%20the%20past%20years%2C%20significant%20progress%20has%20been%20made%20in%20creating%0Aphotorealistic%20and%20drivable%203D%20avatars%20solely%20from%20videos%20of%20real%20humans.%0AHowever%2C%20a%20core%20remaining%20challenge%20is%20the%20fine-grained%20and%20user-friendly%0Aediting%20of%20clothing%20styles%20by%20means%20of%20textual%20descriptions.%20To%20this%20end%2C%20we%0Apresent%20TEDRA%2C%20the%20first%20method%20allowing%20text-based%20edits%20of%20an%20avatar%2C%20which%0Amaintains%20the%20avatar%27s%20high%20fidelity%2C%20space-time%20coherency%2C%20as%20well%20as%0Adynamics%2C%20and%20enables%20skeletal%20pose%20and%20view%20control.%20We%20begin%20by%20training%20a%0Amodel%20to%20create%20a%20controllable%20and%20high-fidelity%20digital%20replica%20of%20the%20real%0Aactor.%20Next%2C%20we%20personalize%20a%20pretrained%20generative%20diffusion%20model%20by%0Afine-tuning%20it%20on%20various%20frames%20of%20the%20real%20character%20captured%20from%20different%0Acamera%20angles%2C%20ensuring%20the%20digital%20representation%20faithfully%20captures%20the%0Adynamics%20and%20movements%20of%20the%20real%20person.%20This%20two-stage%20process%20lays%20the%0Afoundation%20for%20our%20approach%20to%20dynamic%20human%20avatar%20editing.%20Utilizing%20this%0Apersonalized%20diffusion%20model%2C%20we%20modify%20the%20dynamic%20avatar%20based%20on%20a%20provided%0Atext%20prompt%20using%20our%20Personalized%20Normal%20Aligned%20Score%20Distillation%20Sampling%0A%28PNA-SDS%29%20within%20a%20model-based%20guidance%20framework.%20Additionally%2C%20we%20propose%20a%0Atime%20step%20annealing%20strategy%20to%20ensure%20high-quality%20edits.%20Our%20results%0Ademonstrate%20a%20clear%20improvement%20over%20prior%20work%20in%20functionality%20and%20visual%0Aquality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15995v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTEDRA%253A%2520Text-based%2520Editing%2520of%2520Dynamic%2520and%2520Photoreal%2520Actors%26entry.906535625%3DBasavaraj%2520Sunagad%2520and%2520Heming%2520Zhu%2520and%2520Mohit%2520Mendiratta%2520and%2520Adam%2520Kortylewski%2520and%2520Christian%2520Theobalt%2520and%2520Marc%2520Habermann%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520years%252C%2520significant%2520progress%2520has%2520been%2520made%2520in%2520creating%250Aphotorealistic%2520and%2520drivable%25203D%2520avatars%2520solely%2520from%2520videos%2520of%2520real%2520humans.%250AHowever%252C%2520a%2520core%2520remaining%2520challenge%2520is%2520the%2520fine-grained%2520and%2520user-friendly%250Aediting%2520of%2520clothing%2520styles%2520by%2520means%2520of%2520textual%2520descriptions.%2520To%2520this%2520end%252C%2520we%250Apresent%2520TEDRA%252C%2520the%2520first%2520method%2520allowing%2520text-based%2520edits%2520of%2520an%2520avatar%252C%2520which%250Amaintains%2520the%2520avatar%2527s%2520high%2520fidelity%252C%2520space-time%2520coherency%252C%2520as%2520well%2520as%250Adynamics%252C%2520and%2520enables%2520skeletal%2520pose%2520and%2520view%2520control.%2520We%2520begin%2520by%2520training%2520a%250Amodel%2520to%2520create%2520a%2520controllable%2520and%2520high-fidelity%2520digital%2520replica%2520of%2520the%2520real%250Aactor.%2520Next%252C%2520we%2520personalize%2520a%2520pretrained%2520generative%2520diffusion%2520model%2520by%250Afine-tuning%2520it%2520on%2520various%2520frames%2520of%2520the%2520real%2520character%2520captured%2520from%2520different%250Acamera%2520angles%252C%2520ensuring%2520the%2520digital%2520representation%2520faithfully%2520captures%2520the%250Adynamics%2520and%2520movements%2520of%2520the%2520real%2520person.%2520This%2520two-stage%2520process%2520lays%2520the%250Afoundation%2520for%2520our%2520approach%2520to%2520dynamic%2520human%2520avatar%2520editing.%2520Utilizing%2520this%250Apersonalized%2520diffusion%2520model%252C%2520we%2520modify%2520the%2520dynamic%2520avatar%2520based%2520on%2520a%2520provided%250Atext%2520prompt%2520using%2520our%2520Personalized%2520Normal%2520Aligned%2520Score%2520Distillation%2520Sampling%250A%2528PNA-SDS%2529%2520within%2520a%2520model-based%2520guidance%2520framework.%2520Additionally%252C%2520we%2520propose%2520a%250Atime%2520step%2520annealing%2520strategy%2520to%2520ensure%2520high-quality%2520edits.%2520Our%2520results%250Ademonstrate%2520a%2520clear%2520improvement%2520over%2520prior%2520work%2520in%2520functionality%2520and%2520visual%250Aquality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15995v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TEDRA%3A%20Text-based%20Editing%20of%20Dynamic%20and%20Photoreal%20Actors&entry.906535625=Basavaraj%20Sunagad%20and%20Heming%20Zhu%20and%20Mohit%20Mendiratta%20and%20Adam%20Kortylewski%20and%20Christian%20Theobalt%20and%20Marc%20Habermann&entry.1292438233=%20%20Over%20the%20past%20years%2C%20significant%20progress%20has%20been%20made%20in%20creating%0Aphotorealistic%20and%20drivable%203D%20avatars%20solely%20from%20videos%20of%20real%20humans.%0AHowever%2C%20a%20core%20remaining%20challenge%20is%20the%20fine-grained%20and%20user-friendly%0Aediting%20of%20clothing%20styles%20by%20means%20of%20textual%20descriptions.%20To%20this%20end%2C%20we%0Apresent%20TEDRA%2C%20the%20first%20method%20allowing%20text-based%20edits%20of%20an%20avatar%2C%20which%0Amaintains%20the%20avatar%27s%20high%20fidelity%2C%20space-time%20coherency%2C%20as%20well%20as%0Adynamics%2C%20and%20enables%20skeletal%20pose%20and%20view%20control.%20We%20begin%20by%20training%20a%0Amodel%20to%20create%20a%20controllable%20and%20high-fidelity%20digital%20replica%20of%20the%20real%0Aactor.%20Next%2C%20we%20personalize%20a%20pretrained%20generative%20diffusion%20model%20by%0Afine-tuning%20it%20on%20various%20frames%20of%20the%20real%20character%20captured%20from%20different%0Acamera%20angles%2C%20ensuring%20the%20digital%20representation%20faithfully%20captures%20the%0Adynamics%20and%20movements%20of%20the%20real%20person.%20This%20two-stage%20process%20lays%20the%0Afoundation%20for%20our%20approach%20to%20dynamic%20human%20avatar%20editing.%20Utilizing%20this%0Apersonalized%20diffusion%20model%2C%20we%20modify%20the%20dynamic%20avatar%20based%20on%20a%20provided%0Atext%20prompt%20using%20our%20Personalized%20Normal%20Aligned%20Score%20Distillation%20Sampling%0A%28PNA-SDS%29%20within%20a%20model-based%20guidance%20framework.%20Additionally%2C%20we%20propose%20a%0Atime%20step%20annealing%20strategy%20to%20ensure%20high-quality%20edits.%20Our%20results%0Ademonstrate%20a%20clear%20improvement%20over%20prior%20work%20in%20functionality%20and%20visual%0Aquality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15995v1&entry.124074799=Read"},
{"title": "FAST-LIVO2: Fast, Direct LiDAR-Inertial-Visual Odometry", "author": "Chunran Zheng and Wei Xu and Zuhao Zou and Tong Hua and Chongjian Yuan and Dongjiao He and Bingyang Zhou and Zheng Liu and Jiarong Lin and Fangcheng Zhu and Yunfan Ren and Rong Wang and Fanle Meng and Fu Zhang", "abstract": "  This paper proposes FAST-LIVO2: a fast, direct LiDAR-inertial-visual odometry\nframework to achieve accurate and robust state estimation in SLAM tasks and\nprovide great potential in real-time, onboard robotic applications. FAST-LIVO2\nfuses the IMU, LiDAR and image measurements efficiently through an ESIKF. To\naddress the dimension mismatch between the heterogeneous LiDAR and image\nmeasurements, we use a sequential update strategy in the Kalman filter. To\nenhance the efficiency, we use direct methods for both the visual and LiDAR\nfusion, where the LiDAR module registers raw points without extracting edge or\nplane features and the visual module minimizes direct photometric errors\nwithout extracting ORB or FAST corner features. The fusion of both visual and\nLiDAR measurements is based on a single unified voxel map where the LiDAR\nmodule constructs the geometric structure for registering new LiDAR scans and\nthe visual module attaches image patches to the LiDAR points. To enhance the\naccuracy of image alignment, we use plane priors from the LiDAR points in the\nvoxel map (and even refine the plane prior) and update the reference patch\ndynamically after new images are aligned. Furthermore, to enhance the\nrobustness of image alignment, FAST-LIVO2 employs an on-demanding raycast\noperation and estimates the image exposure time in real time. Lastly, we detail\nthree applications of FAST-LIVO2: UAV onboard navigation demonstrating the\nsystem's computation efficiency for real-time onboard navigation, airborne\nmapping showcasing the system's mapping accuracy, and 3D model rendering\n(mesh-based and NeRF-based) underscoring the suitability of our reconstructed\ndense map for subsequent rendering tasks. We open source our code, dataset and\napplication on GitHub to benefit the robotics community.\n", "link": "http://arxiv.org/abs/2408.14035v2", "date": "2024-08-28", "relevancy": 2.3882, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6022}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6016}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FAST-LIVO2%3A%20Fast%2C%20Direct%20LiDAR-Inertial-Visual%20Odometry&body=Title%3A%20FAST-LIVO2%3A%20Fast%2C%20Direct%20LiDAR-Inertial-Visual%20Odometry%0AAuthor%3A%20Chunran%20Zheng%20and%20Wei%20Xu%20and%20Zuhao%20Zou%20and%20Tong%20Hua%20and%20Chongjian%20Yuan%20and%20Dongjiao%20He%20and%20Bingyang%20Zhou%20and%20Zheng%20Liu%20and%20Jiarong%20Lin%20and%20Fangcheng%20Zhu%20and%20Yunfan%20Ren%20and%20Rong%20Wang%20and%20Fanle%20Meng%20and%20Fu%20Zhang%0AAbstract%3A%20%20%20This%20paper%20proposes%20FAST-LIVO2%3A%20a%20fast%2C%20direct%20LiDAR-inertial-visual%20odometry%0Aframework%20to%20achieve%20accurate%20and%20robust%20state%20estimation%20in%20SLAM%20tasks%20and%0Aprovide%20great%20potential%20in%20real-time%2C%20onboard%20robotic%20applications.%20FAST-LIVO2%0Afuses%20the%20IMU%2C%20LiDAR%20and%20image%20measurements%20efficiently%20through%20an%20ESIKF.%20To%0Aaddress%20the%20dimension%20mismatch%20between%20the%20heterogeneous%20LiDAR%20and%20image%0Ameasurements%2C%20we%20use%20a%20sequential%20update%20strategy%20in%20the%20Kalman%20filter.%20To%0Aenhance%20the%20efficiency%2C%20we%20use%20direct%20methods%20for%20both%20the%20visual%20and%20LiDAR%0Afusion%2C%20where%20the%20LiDAR%20module%20registers%20raw%20points%20without%20extracting%20edge%20or%0Aplane%20features%20and%20the%20visual%20module%20minimizes%20direct%20photometric%20errors%0Awithout%20extracting%20ORB%20or%20FAST%20corner%20features.%20The%20fusion%20of%20both%20visual%20and%0ALiDAR%20measurements%20is%20based%20on%20a%20single%20unified%20voxel%20map%20where%20the%20LiDAR%0Amodule%20constructs%20the%20geometric%20structure%20for%20registering%20new%20LiDAR%20scans%20and%0Athe%20visual%20module%20attaches%20image%20patches%20to%20the%20LiDAR%20points.%20To%20enhance%20the%0Aaccuracy%20of%20image%20alignment%2C%20we%20use%20plane%20priors%20from%20the%20LiDAR%20points%20in%20the%0Avoxel%20map%20%28and%20even%20refine%20the%20plane%20prior%29%20and%20update%20the%20reference%20patch%0Adynamically%20after%20new%20images%20are%20aligned.%20Furthermore%2C%20to%20enhance%20the%0Arobustness%20of%20image%20alignment%2C%20FAST-LIVO2%20employs%20an%20on-demanding%20raycast%0Aoperation%20and%20estimates%20the%20image%20exposure%20time%20in%20real%20time.%20Lastly%2C%20we%20detail%0Athree%20applications%20of%20FAST-LIVO2%3A%20UAV%20onboard%20navigation%20demonstrating%20the%0Asystem%27s%20computation%20efficiency%20for%20real-time%20onboard%20navigation%2C%20airborne%0Amapping%20showcasing%20the%20system%27s%20mapping%20accuracy%2C%20and%203D%20model%20rendering%0A%28mesh-based%20and%20NeRF-based%29%20underscoring%20the%20suitability%20of%20our%20reconstructed%0Adense%20map%20for%20subsequent%20rendering%20tasks.%20We%20open%20source%20our%20code%2C%20dataset%20and%0Aapplication%20on%20GitHub%20to%20benefit%20the%20robotics%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14035v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFAST-LIVO2%253A%2520Fast%252C%2520Direct%2520LiDAR-Inertial-Visual%2520Odometry%26entry.906535625%3DChunran%2520Zheng%2520and%2520Wei%2520Xu%2520and%2520Zuhao%2520Zou%2520and%2520Tong%2520Hua%2520and%2520Chongjian%2520Yuan%2520and%2520Dongjiao%2520He%2520and%2520Bingyang%2520Zhou%2520and%2520Zheng%2520Liu%2520and%2520Jiarong%2520Lin%2520and%2520Fangcheng%2520Zhu%2520and%2520Yunfan%2520Ren%2520and%2520Rong%2520Wang%2520and%2520Fanle%2520Meng%2520and%2520Fu%2520Zhang%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520FAST-LIVO2%253A%2520a%2520fast%252C%2520direct%2520LiDAR-inertial-visual%2520odometry%250Aframework%2520to%2520achieve%2520accurate%2520and%2520robust%2520state%2520estimation%2520in%2520SLAM%2520tasks%2520and%250Aprovide%2520great%2520potential%2520in%2520real-time%252C%2520onboard%2520robotic%2520applications.%2520FAST-LIVO2%250Afuses%2520the%2520IMU%252C%2520LiDAR%2520and%2520image%2520measurements%2520efficiently%2520through%2520an%2520ESIKF.%2520To%250Aaddress%2520the%2520dimension%2520mismatch%2520between%2520the%2520heterogeneous%2520LiDAR%2520and%2520image%250Ameasurements%252C%2520we%2520use%2520a%2520sequential%2520update%2520strategy%2520in%2520the%2520Kalman%2520filter.%2520To%250Aenhance%2520the%2520efficiency%252C%2520we%2520use%2520direct%2520methods%2520for%2520both%2520the%2520visual%2520and%2520LiDAR%250Afusion%252C%2520where%2520the%2520LiDAR%2520module%2520registers%2520raw%2520points%2520without%2520extracting%2520edge%2520or%250Aplane%2520features%2520and%2520the%2520visual%2520module%2520minimizes%2520direct%2520photometric%2520errors%250Awithout%2520extracting%2520ORB%2520or%2520FAST%2520corner%2520features.%2520The%2520fusion%2520of%2520both%2520visual%2520and%250ALiDAR%2520measurements%2520is%2520based%2520on%2520a%2520single%2520unified%2520voxel%2520map%2520where%2520the%2520LiDAR%250Amodule%2520constructs%2520the%2520geometric%2520structure%2520for%2520registering%2520new%2520LiDAR%2520scans%2520and%250Athe%2520visual%2520module%2520attaches%2520image%2520patches%2520to%2520the%2520LiDAR%2520points.%2520To%2520enhance%2520the%250Aaccuracy%2520of%2520image%2520alignment%252C%2520we%2520use%2520plane%2520priors%2520from%2520the%2520LiDAR%2520points%2520in%2520the%250Avoxel%2520map%2520%2528and%2520even%2520refine%2520the%2520plane%2520prior%2529%2520and%2520update%2520the%2520reference%2520patch%250Adynamically%2520after%2520new%2520images%2520are%2520aligned.%2520Furthermore%252C%2520to%2520enhance%2520the%250Arobustness%2520of%2520image%2520alignment%252C%2520FAST-LIVO2%2520employs%2520an%2520on-demanding%2520raycast%250Aoperation%2520and%2520estimates%2520the%2520image%2520exposure%2520time%2520in%2520real%2520time.%2520Lastly%252C%2520we%2520detail%250Athree%2520applications%2520of%2520FAST-LIVO2%253A%2520UAV%2520onboard%2520navigation%2520demonstrating%2520the%250Asystem%2527s%2520computation%2520efficiency%2520for%2520real-time%2520onboard%2520navigation%252C%2520airborne%250Amapping%2520showcasing%2520the%2520system%2527s%2520mapping%2520accuracy%252C%2520and%25203D%2520model%2520rendering%250A%2528mesh-based%2520and%2520NeRF-based%2529%2520underscoring%2520the%2520suitability%2520of%2520our%2520reconstructed%250Adense%2520map%2520for%2520subsequent%2520rendering%2520tasks.%2520We%2520open%2520source%2520our%2520code%252C%2520dataset%2520and%250Aapplication%2520on%2520GitHub%2520to%2520benefit%2520the%2520robotics%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14035v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FAST-LIVO2%3A%20Fast%2C%20Direct%20LiDAR-Inertial-Visual%20Odometry&entry.906535625=Chunran%20Zheng%20and%20Wei%20Xu%20and%20Zuhao%20Zou%20and%20Tong%20Hua%20and%20Chongjian%20Yuan%20and%20Dongjiao%20He%20and%20Bingyang%20Zhou%20and%20Zheng%20Liu%20and%20Jiarong%20Lin%20and%20Fangcheng%20Zhu%20and%20Yunfan%20Ren%20and%20Rong%20Wang%20and%20Fanle%20Meng%20and%20Fu%20Zhang&entry.1292438233=%20%20This%20paper%20proposes%20FAST-LIVO2%3A%20a%20fast%2C%20direct%20LiDAR-inertial-visual%20odometry%0Aframework%20to%20achieve%20accurate%20and%20robust%20state%20estimation%20in%20SLAM%20tasks%20and%0Aprovide%20great%20potential%20in%20real-time%2C%20onboard%20robotic%20applications.%20FAST-LIVO2%0Afuses%20the%20IMU%2C%20LiDAR%20and%20image%20measurements%20efficiently%20through%20an%20ESIKF.%20To%0Aaddress%20the%20dimension%20mismatch%20between%20the%20heterogeneous%20LiDAR%20and%20image%0Ameasurements%2C%20we%20use%20a%20sequential%20update%20strategy%20in%20the%20Kalman%20filter.%20To%0Aenhance%20the%20efficiency%2C%20we%20use%20direct%20methods%20for%20both%20the%20visual%20and%20LiDAR%0Afusion%2C%20where%20the%20LiDAR%20module%20registers%20raw%20points%20without%20extracting%20edge%20or%0Aplane%20features%20and%20the%20visual%20module%20minimizes%20direct%20photometric%20errors%0Awithout%20extracting%20ORB%20or%20FAST%20corner%20features.%20The%20fusion%20of%20both%20visual%20and%0ALiDAR%20measurements%20is%20based%20on%20a%20single%20unified%20voxel%20map%20where%20the%20LiDAR%0Amodule%20constructs%20the%20geometric%20structure%20for%20registering%20new%20LiDAR%20scans%20and%0Athe%20visual%20module%20attaches%20image%20patches%20to%20the%20LiDAR%20points.%20To%20enhance%20the%0Aaccuracy%20of%20image%20alignment%2C%20we%20use%20plane%20priors%20from%20the%20LiDAR%20points%20in%20the%0Avoxel%20map%20%28and%20even%20refine%20the%20plane%20prior%29%20and%20update%20the%20reference%20patch%0Adynamically%20after%20new%20images%20are%20aligned.%20Furthermore%2C%20to%20enhance%20the%0Arobustness%20of%20image%20alignment%2C%20FAST-LIVO2%20employs%20an%20on-demanding%20raycast%0Aoperation%20and%20estimates%20the%20image%20exposure%20time%20in%20real%20time.%20Lastly%2C%20we%20detail%0Athree%20applications%20of%20FAST-LIVO2%3A%20UAV%20onboard%20navigation%20demonstrating%20the%0Asystem%27s%20computation%20efficiency%20for%20real-time%20onboard%20navigation%2C%20airborne%0Amapping%20showcasing%20the%20system%27s%20mapping%20accuracy%2C%20and%203D%20model%20rendering%0A%28mesh-based%20and%20NeRF-based%29%20underscoring%20the%20suitability%20of%20our%20reconstructed%0Adense%20map%20for%20subsequent%20rendering%20tasks.%20We%20open%20source%20our%20code%2C%20dataset%20and%0Aapplication%20on%20GitHub%20to%20benefit%20the%20robotics%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14035v2&entry.124074799=Read"},
{"title": "MambaPlace:Text-to-Point-Cloud Cross-Modal Place Recognition with\n  Attention Mamba Mechanisms", "author": "Tianyi Shang and Zhenyu Li and Wenhao Pei and Pengjie Xu and ZhaoJun Deng and Fanchen Kong", "abstract": "  Vision Language Place Recognition (VLVPR) enhances robot localization\nperformance by incorporating natural language descriptions from images. By\nutilizing language information, VLVPR directs robot place matching, overcoming\nthe constraint of solely depending on vision. The essence of multimodal fusion\nlies in mining the complementary information between different modalities.\nHowever, general fusion methods rely on traditional neural architectures and\nare not well equipped to capture the dynamics of cross modal interactions,\nespecially in the presence of complex intra modal and inter modal correlations.\nTo this end, this paper proposes a novel coarse to fine and end to end\nconnected cross modal place recognition framework, called MambaPlace. In the\ncoarse localization stage, the text description and 3D point cloud are encoded\nby the pretrained T5 and instance encoder, respectively. They are then\nprocessed using Text Attention Mamba (TAM) and Point Clouds Mamba (PCM) for\ndata enhancement and alignment. In the subsequent fine localization stage, the\nfeatures of the text description and 3D point cloud are cross modally fused and\nfurther enhanced through cascaded Cross Attention Mamba (CCAM). Finally, we\npredict the positional offset from the fused text point cloud features,\nachieving the most accurate localization. Extensive experiments show that\nMambaPlace achieves improved localization accuracy on the KITTI360Pose dataset\ncompared to the state of the art methods.\n", "link": "http://arxiv.org/abs/2408.15740v1", "date": "2024-08-28", "relevancy": 2.3632, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.613}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5781}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambaPlace%3AText-to-Point-Cloud%20Cross-Modal%20Place%20Recognition%20with%0A%20%20Attention%20Mamba%20Mechanisms&body=Title%3A%20MambaPlace%3AText-to-Point-Cloud%20Cross-Modal%20Place%20Recognition%20with%0A%20%20Attention%20Mamba%20Mechanisms%0AAuthor%3A%20Tianyi%20Shang%20and%20Zhenyu%20Li%20and%20Wenhao%20Pei%20and%20Pengjie%20Xu%20and%20ZhaoJun%20Deng%20and%20Fanchen%20Kong%0AAbstract%3A%20%20%20Vision%20Language%20Place%20Recognition%20%28VLVPR%29%20enhances%20robot%20localization%0Aperformance%20by%20incorporating%20natural%20language%20descriptions%20from%20images.%20By%0Autilizing%20language%20information%2C%20VLVPR%20directs%20robot%20place%20matching%2C%20overcoming%0Athe%20constraint%20of%20solely%20depending%20on%20vision.%20The%20essence%20of%20multimodal%20fusion%0Alies%20in%20mining%20the%20complementary%20information%20between%20different%20modalities.%0AHowever%2C%20general%20fusion%20methods%20rely%20on%20traditional%20neural%20architectures%20and%0Aare%20not%20well%20equipped%20to%20capture%20the%20dynamics%20of%20cross%20modal%20interactions%2C%0Aespecially%20in%20the%20presence%20of%20complex%20intra%20modal%20and%20inter%20modal%20correlations.%0ATo%20this%20end%2C%20this%20paper%20proposes%20a%20novel%20coarse%20to%20fine%20and%20end%20to%20end%0Aconnected%20cross%20modal%20place%20recognition%20framework%2C%20called%20MambaPlace.%20In%20the%0Acoarse%20localization%20stage%2C%20the%20text%20description%20and%203D%20point%20cloud%20are%20encoded%0Aby%20the%20pretrained%20T5%20and%20instance%20encoder%2C%20respectively.%20They%20are%20then%0Aprocessed%20using%20Text%20Attention%20Mamba%20%28TAM%29%20and%20Point%20Clouds%20Mamba%20%28PCM%29%20for%0Adata%20enhancement%20and%20alignment.%20In%20the%20subsequent%20fine%20localization%20stage%2C%20the%0Afeatures%20of%20the%20text%20description%20and%203D%20point%20cloud%20are%20cross%20modally%20fused%20and%0Afurther%20enhanced%20through%20cascaded%20Cross%20Attention%20Mamba%20%28CCAM%29.%20Finally%2C%20we%0Apredict%20the%20positional%20offset%20from%20the%20fused%20text%20point%20cloud%20features%2C%0Aachieving%20the%20most%20accurate%20localization.%20Extensive%20experiments%20show%20that%0AMambaPlace%20achieves%20improved%20localization%20accuracy%20on%20the%20KITTI360Pose%20dataset%0Acompared%20to%20the%20state%20of%20the%20art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambaPlace%253AText-to-Point-Cloud%2520Cross-Modal%2520Place%2520Recognition%2520with%250A%2520%2520Attention%2520Mamba%2520Mechanisms%26entry.906535625%3DTianyi%2520Shang%2520and%2520Zhenyu%2520Li%2520and%2520Wenhao%2520Pei%2520and%2520Pengjie%2520Xu%2520and%2520ZhaoJun%2520Deng%2520and%2520Fanchen%2520Kong%26entry.1292438233%3D%2520%2520Vision%2520Language%2520Place%2520Recognition%2520%2528VLVPR%2529%2520enhances%2520robot%2520localization%250Aperformance%2520by%2520incorporating%2520natural%2520language%2520descriptions%2520from%2520images.%2520By%250Autilizing%2520language%2520information%252C%2520VLVPR%2520directs%2520robot%2520place%2520matching%252C%2520overcoming%250Athe%2520constraint%2520of%2520solely%2520depending%2520on%2520vision.%2520The%2520essence%2520of%2520multimodal%2520fusion%250Alies%2520in%2520mining%2520the%2520complementary%2520information%2520between%2520different%2520modalities.%250AHowever%252C%2520general%2520fusion%2520methods%2520rely%2520on%2520traditional%2520neural%2520architectures%2520and%250Aare%2520not%2520well%2520equipped%2520to%2520capture%2520the%2520dynamics%2520of%2520cross%2520modal%2520interactions%252C%250Aespecially%2520in%2520the%2520presence%2520of%2520complex%2520intra%2520modal%2520and%2520inter%2520modal%2520correlations.%250ATo%2520this%2520end%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520coarse%2520to%2520fine%2520and%2520end%2520to%2520end%250Aconnected%2520cross%2520modal%2520place%2520recognition%2520framework%252C%2520called%2520MambaPlace.%2520In%2520the%250Acoarse%2520localization%2520stage%252C%2520the%2520text%2520description%2520and%25203D%2520point%2520cloud%2520are%2520encoded%250Aby%2520the%2520pretrained%2520T5%2520and%2520instance%2520encoder%252C%2520respectively.%2520They%2520are%2520then%250Aprocessed%2520using%2520Text%2520Attention%2520Mamba%2520%2528TAM%2529%2520and%2520Point%2520Clouds%2520Mamba%2520%2528PCM%2529%2520for%250Adata%2520enhancement%2520and%2520alignment.%2520In%2520the%2520subsequent%2520fine%2520localization%2520stage%252C%2520the%250Afeatures%2520of%2520the%2520text%2520description%2520and%25203D%2520point%2520cloud%2520are%2520cross%2520modally%2520fused%2520and%250Afurther%2520enhanced%2520through%2520cascaded%2520Cross%2520Attention%2520Mamba%2520%2528CCAM%2529.%2520Finally%252C%2520we%250Apredict%2520the%2520positional%2520offset%2520from%2520the%2520fused%2520text%2520point%2520cloud%2520features%252C%250Aachieving%2520the%2520most%2520accurate%2520localization.%2520Extensive%2520experiments%2520show%2520that%250AMambaPlace%2520achieves%2520improved%2520localization%2520accuracy%2520on%2520the%2520KITTI360Pose%2520dataset%250Acompared%2520to%2520the%2520state%2520of%2520the%2520art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaPlace%3AText-to-Point-Cloud%20Cross-Modal%20Place%20Recognition%20with%0A%20%20Attention%20Mamba%20Mechanisms&entry.906535625=Tianyi%20Shang%20and%20Zhenyu%20Li%20and%20Wenhao%20Pei%20and%20Pengjie%20Xu%20and%20ZhaoJun%20Deng%20and%20Fanchen%20Kong&entry.1292438233=%20%20Vision%20Language%20Place%20Recognition%20%28VLVPR%29%20enhances%20robot%20localization%0Aperformance%20by%20incorporating%20natural%20language%20descriptions%20from%20images.%20By%0Autilizing%20language%20information%2C%20VLVPR%20directs%20robot%20place%20matching%2C%20overcoming%0Athe%20constraint%20of%20solely%20depending%20on%20vision.%20The%20essence%20of%20multimodal%20fusion%0Alies%20in%20mining%20the%20complementary%20information%20between%20different%20modalities.%0AHowever%2C%20general%20fusion%20methods%20rely%20on%20traditional%20neural%20architectures%20and%0Aare%20not%20well%20equipped%20to%20capture%20the%20dynamics%20of%20cross%20modal%20interactions%2C%0Aespecially%20in%20the%20presence%20of%20complex%20intra%20modal%20and%20inter%20modal%20correlations.%0ATo%20this%20end%2C%20this%20paper%20proposes%20a%20novel%20coarse%20to%20fine%20and%20end%20to%20end%0Aconnected%20cross%20modal%20place%20recognition%20framework%2C%20called%20MambaPlace.%20In%20the%0Acoarse%20localization%20stage%2C%20the%20text%20description%20and%203D%20point%20cloud%20are%20encoded%0Aby%20the%20pretrained%20T5%20and%20instance%20encoder%2C%20respectively.%20They%20are%20then%0Aprocessed%20using%20Text%20Attention%20Mamba%20%28TAM%29%20and%20Point%20Clouds%20Mamba%20%28PCM%29%20for%0Adata%20enhancement%20and%20alignment.%20In%20the%20subsequent%20fine%20localization%20stage%2C%20the%0Afeatures%20of%20the%20text%20description%20and%203D%20point%20cloud%20are%20cross%20modally%20fused%20and%0Afurther%20enhanced%20through%20cascaded%20Cross%20Attention%20Mamba%20%28CCAM%29.%20Finally%2C%20we%0Apredict%20the%20positional%20offset%20from%20the%20fused%20text%20point%20cloud%20features%2C%0Aachieving%20the%20most%20accurate%20localization.%20Extensive%20experiments%20show%20that%0AMambaPlace%20achieves%20improved%20localization%20accuracy%20on%20the%20KITTI360Pose%20dataset%0Acompared%20to%20the%20state%20of%20the%20art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15740v1&entry.124074799=Read"},
{"title": "Spatio-Temporal Context Prompting for Zero-Shot Action Detection", "author": "Wei-Jhe Huang and Min-Hung Chen and Shang-Hong Lai", "abstract": "  Spatio-temporal action detection encompasses the tasks of localizing and\nclassifying individual actions within a video. Recent works aim to enhance this\nprocess by incorporating interaction modeling, which captures the relationship\nbetween people and their surrounding context. However, these approaches have\nprimarily focused on fully-supervised learning, and the current limitation lies\nin the lack of generalization capability to recognize unseen action categories.\nIn this paper, we aim to adapt the pretrained image-language models to detect\nunseen actions. To this end, we propose a method which can effectively leverage\nthe rich knowledge of visual-language models to perform Person-Context\nInteraction. Meanwhile, our Context Prompting module will utilize contextual\ninformation to prompt labels, thereby enhancing the generation of more\nrepresentative text features. Moreover, to address the challenge of recognizing\ndistinct actions by multiple people at the same timestamp, we design the\nInterest Token Spotting mechanism which employs pretrained visual knowledge to\nfind each person's interest context tokens, and then these tokens will be used\nfor prompting to generate text features tailored to each individual. To\nevaluate the ability to detect unseen actions, we propose a comprehensive\nbenchmark on J-HMDB, UCF101-24, and AVA datasets. The experiments show that our\nmethod achieves superior results compared to previous approaches and can be\nfurther extended to multi-action videos, bringing it closer to real-world\napplications. The code and data can be found in\nhttps://webber2933.github.io/ST-CLIP-project-page.\n", "link": "http://arxiv.org/abs/2408.15996v1", "date": "2024-08-28", "relevancy": 2.3591, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6432}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5717}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatio-Temporal%20Context%20Prompting%20for%20Zero-Shot%20Action%20Detection&body=Title%3A%20Spatio-Temporal%20Context%20Prompting%20for%20Zero-Shot%20Action%20Detection%0AAuthor%3A%20Wei-Jhe%20Huang%20and%20Min-Hung%20Chen%20and%20Shang-Hong%20Lai%0AAbstract%3A%20%20%20Spatio-temporal%20action%20detection%20encompasses%20the%20tasks%20of%20localizing%20and%0Aclassifying%20individual%20actions%20within%20a%20video.%20Recent%20works%20aim%20to%20enhance%20this%0Aprocess%20by%20incorporating%20interaction%20modeling%2C%20which%20captures%20the%20relationship%0Abetween%20people%20and%20their%20surrounding%20context.%20However%2C%20these%20approaches%20have%0Aprimarily%20focused%20on%20fully-supervised%20learning%2C%20and%20the%20current%20limitation%20lies%0Ain%20the%20lack%20of%20generalization%20capability%20to%20recognize%20unseen%20action%20categories.%0AIn%20this%20paper%2C%20we%20aim%20to%20adapt%20the%20pretrained%20image-language%20models%20to%20detect%0Aunseen%20actions.%20To%20this%20end%2C%20we%20propose%20a%20method%20which%20can%20effectively%20leverage%0Athe%20rich%20knowledge%20of%20visual-language%20models%20to%20perform%20Person-Context%0AInteraction.%20Meanwhile%2C%20our%20Context%20Prompting%20module%20will%20utilize%20contextual%0Ainformation%20to%20prompt%20labels%2C%20thereby%20enhancing%20the%20generation%20of%20more%0Arepresentative%20text%20features.%20Moreover%2C%20to%20address%20the%20challenge%20of%20recognizing%0Adistinct%20actions%20by%20multiple%20people%20at%20the%20same%20timestamp%2C%20we%20design%20the%0AInterest%20Token%20Spotting%20mechanism%20which%20employs%20pretrained%20visual%20knowledge%20to%0Afind%20each%20person%27s%20interest%20context%20tokens%2C%20and%20then%20these%20tokens%20will%20be%20used%0Afor%20prompting%20to%20generate%20text%20features%20tailored%20to%20each%20individual.%20To%0Aevaluate%20the%20ability%20to%20detect%20unseen%20actions%2C%20we%20propose%20a%20comprehensive%0Abenchmark%20on%20J-HMDB%2C%20UCF101-24%2C%20and%20AVA%20datasets.%20The%20experiments%20show%20that%20our%0Amethod%20achieves%20superior%20results%20compared%20to%20previous%20approaches%20and%20can%20be%0Afurther%20extended%20to%20multi-action%20videos%2C%20bringing%20it%20closer%20to%20real-world%0Aapplications.%20The%20code%20and%20data%20can%20be%20found%20in%0Ahttps%3A//webber2933.github.io/ST-CLIP-project-page.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15996v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatio-Temporal%2520Context%2520Prompting%2520for%2520Zero-Shot%2520Action%2520Detection%26entry.906535625%3DWei-Jhe%2520Huang%2520and%2520Min-Hung%2520Chen%2520and%2520Shang-Hong%2520Lai%26entry.1292438233%3D%2520%2520Spatio-temporal%2520action%2520detection%2520encompasses%2520the%2520tasks%2520of%2520localizing%2520and%250Aclassifying%2520individual%2520actions%2520within%2520a%2520video.%2520Recent%2520works%2520aim%2520to%2520enhance%2520this%250Aprocess%2520by%2520incorporating%2520interaction%2520modeling%252C%2520which%2520captures%2520the%2520relationship%250Abetween%2520people%2520and%2520their%2520surrounding%2520context.%2520However%252C%2520these%2520approaches%2520have%250Aprimarily%2520focused%2520on%2520fully-supervised%2520learning%252C%2520and%2520the%2520current%2520limitation%2520lies%250Ain%2520the%2520lack%2520of%2520generalization%2520capability%2520to%2520recognize%2520unseen%2520action%2520categories.%250AIn%2520this%2520paper%252C%2520we%2520aim%2520to%2520adapt%2520the%2520pretrained%2520image-language%2520models%2520to%2520detect%250Aunseen%2520actions.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520method%2520which%2520can%2520effectively%2520leverage%250Athe%2520rich%2520knowledge%2520of%2520visual-language%2520models%2520to%2520perform%2520Person-Context%250AInteraction.%2520Meanwhile%252C%2520our%2520Context%2520Prompting%2520module%2520will%2520utilize%2520contextual%250Ainformation%2520to%2520prompt%2520labels%252C%2520thereby%2520enhancing%2520the%2520generation%2520of%2520more%250Arepresentative%2520text%2520features.%2520Moreover%252C%2520to%2520address%2520the%2520challenge%2520of%2520recognizing%250Adistinct%2520actions%2520by%2520multiple%2520people%2520at%2520the%2520same%2520timestamp%252C%2520we%2520design%2520the%250AInterest%2520Token%2520Spotting%2520mechanism%2520which%2520employs%2520pretrained%2520visual%2520knowledge%2520to%250Afind%2520each%2520person%2527s%2520interest%2520context%2520tokens%252C%2520and%2520then%2520these%2520tokens%2520will%2520be%2520used%250Afor%2520prompting%2520to%2520generate%2520text%2520features%2520tailored%2520to%2520each%2520individual.%2520To%250Aevaluate%2520the%2520ability%2520to%2520detect%2520unseen%2520actions%252C%2520we%2520propose%2520a%2520comprehensive%250Abenchmark%2520on%2520J-HMDB%252C%2520UCF101-24%252C%2520and%2520AVA%2520datasets.%2520The%2520experiments%2520show%2520that%2520our%250Amethod%2520achieves%2520superior%2520results%2520compared%2520to%2520previous%2520approaches%2520and%2520can%2520be%250Afurther%2520extended%2520to%2520multi-action%2520videos%252C%2520bringing%2520it%2520closer%2520to%2520real-world%250Aapplications.%2520The%2520code%2520and%2520data%2520can%2520be%2520found%2520in%250Ahttps%253A//webber2933.github.io/ST-CLIP-project-page.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15996v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatio-Temporal%20Context%20Prompting%20for%20Zero-Shot%20Action%20Detection&entry.906535625=Wei-Jhe%20Huang%20and%20Min-Hung%20Chen%20and%20Shang-Hong%20Lai&entry.1292438233=%20%20Spatio-temporal%20action%20detection%20encompasses%20the%20tasks%20of%20localizing%20and%0Aclassifying%20individual%20actions%20within%20a%20video.%20Recent%20works%20aim%20to%20enhance%20this%0Aprocess%20by%20incorporating%20interaction%20modeling%2C%20which%20captures%20the%20relationship%0Abetween%20people%20and%20their%20surrounding%20context.%20However%2C%20these%20approaches%20have%0Aprimarily%20focused%20on%20fully-supervised%20learning%2C%20and%20the%20current%20limitation%20lies%0Ain%20the%20lack%20of%20generalization%20capability%20to%20recognize%20unseen%20action%20categories.%0AIn%20this%20paper%2C%20we%20aim%20to%20adapt%20the%20pretrained%20image-language%20models%20to%20detect%0Aunseen%20actions.%20To%20this%20end%2C%20we%20propose%20a%20method%20which%20can%20effectively%20leverage%0Athe%20rich%20knowledge%20of%20visual-language%20models%20to%20perform%20Person-Context%0AInteraction.%20Meanwhile%2C%20our%20Context%20Prompting%20module%20will%20utilize%20contextual%0Ainformation%20to%20prompt%20labels%2C%20thereby%20enhancing%20the%20generation%20of%20more%0Arepresentative%20text%20features.%20Moreover%2C%20to%20address%20the%20challenge%20of%20recognizing%0Adistinct%20actions%20by%20multiple%20people%20at%20the%20same%20timestamp%2C%20we%20design%20the%0AInterest%20Token%20Spotting%20mechanism%20which%20employs%20pretrained%20visual%20knowledge%20to%0Afind%20each%20person%27s%20interest%20context%20tokens%2C%20and%20then%20these%20tokens%20will%20be%20used%0Afor%20prompting%20to%20generate%20text%20features%20tailored%20to%20each%20individual.%20To%0Aevaluate%20the%20ability%20to%20detect%20unseen%20actions%2C%20we%20propose%20a%20comprehensive%0Abenchmark%20on%20J-HMDB%2C%20UCF101-24%2C%20and%20AVA%20datasets.%20The%20experiments%20show%20that%20our%0Amethod%20achieves%20superior%20results%20compared%20to%20previous%20approaches%20and%20can%20be%0Afurther%20extended%20to%20multi-action%20videos%2C%20bringing%20it%20closer%20to%20real-world%0Aapplications.%20The%20code%20and%20data%20can%20be%20found%20in%0Ahttps%3A//webber2933.github.io/ST-CLIP-project-page.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15996v1&entry.124074799=Read"},
{"title": "u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model", "author": "Jinjin Xu and Liwu Xu and Yuzhe Yang and Xiang Li and Fanyi Wang and Yanchun Xie and Yi-Jie Huang and Yaqian Li", "abstract": "  Recent advancements in multi-modal large language models (MLLMs) have led to\nsubstantial improvements in visual understanding, primarily driven by\nsophisticated modality alignment strategies. However, predominant approaches\nprioritize global or regional comprehension, with less focus on fine-grained,\npixel-level tasks. To address this gap, we introduce u-LLaVA, an innovative\nunifying multi-task framework that integrates pixel, regional, and global\nfeatures to refine the perceptual faculties of MLLMs. We commence by leveraging\nan efficient modality alignment approach, harnessing both image and video\ndatasets to bolster the model's foundational understanding across diverse\nvisual contexts. Subsequently, a joint instruction tuning method with\ntask-specific projectors and decoders for end-to-end downstream training is\npresented. Furthermore, this work contributes a novel mask-based multi-task\ndataset comprising 277K samples, crafted to challenge and assess the\nfine-grained perception capabilities of MLLMs. The overall framework is simple,\neffective, and achieves state-of-the-art performance across multiple\nbenchmarks. We also make our model, data, and code publicly accessible at\nhttps://github.com/OPPOMKLab/u-LLaVA.\n", "link": "http://arxiv.org/abs/2311.05348v4", "date": "2024-08-28", "relevancy": 2.3543, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5977}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.585}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20u-LLaVA%3A%20Unifying%20Multi-Modal%20Tasks%20via%20Large%20Language%20Model&body=Title%3A%20u-LLaVA%3A%20Unifying%20Multi-Modal%20Tasks%20via%20Large%20Language%20Model%0AAuthor%3A%20Jinjin%20Xu%20and%20Liwu%20Xu%20and%20Yuzhe%20Yang%20and%20Xiang%20Li%20and%20Fanyi%20Wang%20and%20Yanchun%20Xie%20and%20Yi-Jie%20Huang%20and%20Yaqian%20Li%0AAbstract%3A%20%20%20Recent%20advancements%20in%20multi-modal%20large%20language%20models%20%28MLLMs%29%20have%20led%20to%0Asubstantial%20improvements%20in%20visual%20understanding%2C%20primarily%20driven%20by%0Asophisticated%20modality%20alignment%20strategies.%20However%2C%20predominant%20approaches%0Aprioritize%20global%20or%20regional%20comprehension%2C%20with%20less%20focus%20on%20fine-grained%2C%0Apixel-level%20tasks.%20To%20address%20this%20gap%2C%20we%20introduce%20u-LLaVA%2C%20an%20innovative%0Aunifying%20multi-task%20framework%20that%20integrates%20pixel%2C%20regional%2C%20and%20global%0Afeatures%20to%20refine%20the%20perceptual%20faculties%20of%20MLLMs.%20We%20commence%20by%20leveraging%0Aan%20efficient%20modality%20alignment%20approach%2C%20harnessing%20both%20image%20and%20video%0Adatasets%20to%20bolster%20the%20model%27s%20foundational%20understanding%20across%20diverse%0Avisual%20contexts.%20Subsequently%2C%20a%20joint%20instruction%20tuning%20method%20with%0Atask-specific%20projectors%20and%20decoders%20for%20end-to-end%20downstream%20training%20is%0Apresented.%20Furthermore%2C%20this%20work%20contributes%20a%20novel%20mask-based%20multi-task%0Adataset%20comprising%20277K%20samples%2C%20crafted%20to%20challenge%20and%20assess%20the%0Afine-grained%20perception%20capabilities%20of%20MLLMs.%20The%20overall%20framework%20is%20simple%2C%0Aeffective%2C%20and%20achieves%20state-of-the-art%20performance%20across%20multiple%0Abenchmarks.%20We%20also%20make%20our%20model%2C%20data%2C%20and%20code%20publicly%20accessible%20at%0Ahttps%3A//github.com/OPPOMKLab/u-LLaVA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.05348v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Du-LLaVA%253A%2520Unifying%2520Multi-Modal%2520Tasks%2520via%2520Large%2520Language%2520Model%26entry.906535625%3DJinjin%2520Xu%2520and%2520Liwu%2520Xu%2520and%2520Yuzhe%2520Yang%2520and%2520Xiang%2520Li%2520and%2520Fanyi%2520Wang%2520and%2520Yanchun%2520Xie%2520and%2520Yi-Jie%2520Huang%2520and%2520Yaqian%2520Li%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520led%2520to%250Asubstantial%2520improvements%2520in%2520visual%2520understanding%252C%2520primarily%2520driven%2520by%250Asophisticated%2520modality%2520alignment%2520strategies.%2520However%252C%2520predominant%2520approaches%250Aprioritize%2520global%2520or%2520regional%2520comprehension%252C%2520with%2520less%2520focus%2520on%2520fine-grained%252C%250Apixel-level%2520tasks.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520u-LLaVA%252C%2520an%2520innovative%250Aunifying%2520multi-task%2520framework%2520that%2520integrates%2520pixel%252C%2520regional%252C%2520and%2520global%250Afeatures%2520to%2520refine%2520the%2520perceptual%2520faculties%2520of%2520MLLMs.%2520We%2520commence%2520by%2520leveraging%250Aan%2520efficient%2520modality%2520alignment%2520approach%252C%2520harnessing%2520both%2520image%2520and%2520video%250Adatasets%2520to%2520bolster%2520the%2520model%2527s%2520foundational%2520understanding%2520across%2520diverse%250Avisual%2520contexts.%2520Subsequently%252C%2520a%2520joint%2520instruction%2520tuning%2520method%2520with%250Atask-specific%2520projectors%2520and%2520decoders%2520for%2520end-to-end%2520downstream%2520training%2520is%250Apresented.%2520Furthermore%252C%2520this%2520work%2520contributes%2520a%2520novel%2520mask-based%2520multi-task%250Adataset%2520comprising%2520277K%2520samples%252C%2520crafted%2520to%2520challenge%2520and%2520assess%2520the%250Afine-grained%2520perception%2520capabilities%2520of%2520MLLMs.%2520The%2520overall%2520framework%2520is%2520simple%252C%250Aeffective%252C%2520and%2520achieves%2520state-of-the-art%2520performance%2520across%2520multiple%250Abenchmarks.%2520We%2520also%2520make%2520our%2520model%252C%2520data%252C%2520and%2520code%2520publicly%2520accessible%2520at%250Ahttps%253A//github.com/OPPOMKLab/u-LLaVA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.05348v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=u-LLaVA%3A%20Unifying%20Multi-Modal%20Tasks%20via%20Large%20Language%20Model&entry.906535625=Jinjin%20Xu%20and%20Liwu%20Xu%20and%20Yuzhe%20Yang%20and%20Xiang%20Li%20and%20Fanyi%20Wang%20and%20Yanchun%20Xie%20and%20Yi-Jie%20Huang%20and%20Yaqian%20Li&entry.1292438233=%20%20Recent%20advancements%20in%20multi-modal%20large%20language%20models%20%28MLLMs%29%20have%20led%20to%0Asubstantial%20improvements%20in%20visual%20understanding%2C%20primarily%20driven%20by%0Asophisticated%20modality%20alignment%20strategies.%20However%2C%20predominant%20approaches%0Aprioritize%20global%20or%20regional%20comprehension%2C%20with%20less%20focus%20on%20fine-grained%2C%0Apixel-level%20tasks.%20To%20address%20this%20gap%2C%20we%20introduce%20u-LLaVA%2C%20an%20innovative%0Aunifying%20multi-task%20framework%20that%20integrates%20pixel%2C%20regional%2C%20and%20global%0Afeatures%20to%20refine%20the%20perceptual%20faculties%20of%20MLLMs.%20We%20commence%20by%20leveraging%0Aan%20efficient%20modality%20alignment%20approach%2C%20harnessing%20both%20image%20and%20video%0Adatasets%20to%20bolster%20the%20model%27s%20foundational%20understanding%20across%20diverse%0Avisual%20contexts.%20Subsequently%2C%20a%20joint%20instruction%20tuning%20method%20with%0Atask-specific%20projectors%20and%20decoders%20for%20end-to-end%20downstream%20training%20is%0Apresented.%20Furthermore%2C%20this%20work%20contributes%20a%20novel%20mask-based%20multi-task%0Adataset%20comprising%20277K%20samples%2C%20crafted%20to%20challenge%20and%20assess%20the%0Afine-grained%20perception%20capabilities%20of%20MLLMs.%20The%20overall%20framework%20is%20simple%2C%0Aeffective%2C%20and%20achieves%20state-of-the-art%20performance%20across%20multiple%0Abenchmarks.%20We%20also%20make%20our%20model%2C%20data%2C%20and%20code%20publicly%20accessible%20at%0Ahttps%3A//github.com/OPPOMKLab/u-LLaVA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.05348v4&entry.124074799=Read"},
{"title": "More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding", "author": "Yuan Tang and Xu Han and Xianzhi Li and Qiao Yu and Jinfeng Xu and Yixue Hao and Long Hu and Min Chen", "abstract": "  Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM.\n", "link": "http://arxiv.org/abs/2408.15966v1", "date": "2024-08-28", "relevancy": 2.3444, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6118}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.578}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20More%20Text%2C%20Less%20Point%3A%20Towards%203D%20Data-Efficient%20Point-Language%0A%20%20Understanding&body=Title%3A%20More%20Text%2C%20Less%20Point%3A%20Towards%203D%20Data-Efficient%20Point-Language%0A%20%20Understanding%0AAuthor%3A%20Yuan%20Tang%20and%20Xu%20Han%20and%20Xianzhi%20Li%20and%20Qiao%20Yu%20and%20Jinfeng%20Xu%20and%20Yixue%20Hao%20and%20Long%20Hu%20and%20Min%20Chen%0AAbstract%3A%20%20%20Enabling%20Large%20Language%20Models%20%28LLMs%29%20to%20comprehend%20the%203D%20physical%20world%0Aremains%20a%20significant%20challenge.%20Due%20to%20the%20lack%20of%20large-scale%203D-text%20pair%0Adatasets%2C%20the%20success%20of%20LLMs%20has%20yet%20to%20be%20replicated%20in%203D%20understanding.%20In%0Athis%20paper%2C%20we%20rethink%20this%20issue%20and%20propose%20a%20new%20task%3A%203D%20Data-Efficient%0APoint-Language%20Understanding.%20The%20goal%20is%20to%20enable%20LLMs%20to%20achieve%20robust%203D%0Aobject%20understanding%20with%20minimal%203D%20point%20cloud%20and%20text%20data%20pairs.%20To%0Aaddress%20this%20task%2C%20we%20introduce%20GreenPLM%2C%20which%20leverages%20more%20text%20data%20to%0Acompensate%20for%20the%20lack%20of%203D%20data.%20First%2C%20inspired%20by%20using%20CLIP%20to%20align%0Aimages%20and%20text%2C%20we%20utilize%20a%20pre-trained%20point%20cloud-text%20encoder%20to%20map%20the%0A3D%20point%20cloud%20space%20to%20the%20text%20space.%20This%20mapping%20leaves%20us%20to%20seamlessly%0Aconnect%20the%20text%20space%20with%20LLMs.%20Once%20the%20point-text-LLM%20connection%20is%0Aestablished%2C%20we%20further%20enhance%20text-LLM%20alignment%20by%20expanding%20the%0Aintermediate%20text%20space%2C%20thereby%20reducing%20the%20reliance%20on%203D%20point%20cloud%20data.%0ASpecifically%2C%20we%20generate%206M%20free-text%20descriptions%20of%203D%20objects%2C%20and%20design%20a%0Athree-stage%20training%20strategy%20to%20help%20LLMs%20better%20explore%20the%20intrinsic%0Aconnections%20between%20different%20modalities.%20To%20achieve%20efficient%20modality%0Aalignment%2C%20we%20design%20a%20zero-parameter%20cross-attention%20module%20for%20token%20pooling.%0AExtensive%20experimental%20results%20show%20that%20GreenPLM%20requires%20only%2012%25%20of%20the%203D%0Atraining%20data%20used%20by%20existing%20state-of-the-art%20models%20to%20achieve%20superior%203D%0Aunderstanding.%20Remarkably%2C%20GreenPLM%20also%20achieves%20competitive%20performance%20using%0Atext-only%20data.%20The%20code%20and%20weights%20are%20available%20at%3A%0Ahttps%3A//github.com/TangYuan96/GreenPLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMore%2520Text%252C%2520Less%2520Point%253A%2520Towards%25203D%2520Data-Efficient%2520Point-Language%250A%2520%2520Understanding%26entry.906535625%3DYuan%2520Tang%2520and%2520Xu%2520Han%2520and%2520Xianzhi%2520Li%2520and%2520Qiao%2520Yu%2520and%2520Jinfeng%2520Xu%2520and%2520Yixue%2520Hao%2520and%2520Long%2520Hu%2520and%2520Min%2520Chen%26entry.1292438233%3D%2520%2520Enabling%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520comprehend%2520the%25203D%2520physical%2520world%250Aremains%2520a%2520significant%2520challenge.%2520Due%2520to%2520the%2520lack%2520of%2520large-scale%25203D-text%2520pair%250Adatasets%252C%2520the%2520success%2520of%2520LLMs%2520has%2520yet%2520to%2520be%2520replicated%2520in%25203D%2520understanding.%2520In%250Athis%2520paper%252C%2520we%2520rethink%2520this%2520issue%2520and%2520propose%2520a%2520new%2520task%253A%25203D%2520Data-Efficient%250APoint-Language%2520Understanding.%2520The%2520goal%2520is%2520to%2520enable%2520LLMs%2520to%2520achieve%2520robust%25203D%250Aobject%2520understanding%2520with%2520minimal%25203D%2520point%2520cloud%2520and%2520text%2520data%2520pairs.%2520To%250Aaddress%2520this%2520task%252C%2520we%2520introduce%2520GreenPLM%252C%2520which%2520leverages%2520more%2520text%2520data%2520to%250Acompensate%2520for%2520the%2520lack%2520of%25203D%2520data.%2520First%252C%2520inspired%2520by%2520using%2520CLIP%2520to%2520align%250Aimages%2520and%2520text%252C%2520we%2520utilize%2520a%2520pre-trained%2520point%2520cloud-text%2520encoder%2520to%2520map%2520the%250A3D%2520point%2520cloud%2520space%2520to%2520the%2520text%2520space.%2520This%2520mapping%2520leaves%2520us%2520to%2520seamlessly%250Aconnect%2520the%2520text%2520space%2520with%2520LLMs.%2520Once%2520the%2520point-text-LLM%2520connection%2520is%250Aestablished%252C%2520we%2520further%2520enhance%2520text-LLM%2520alignment%2520by%2520expanding%2520the%250Aintermediate%2520text%2520space%252C%2520thereby%2520reducing%2520the%2520reliance%2520on%25203D%2520point%2520cloud%2520data.%250ASpecifically%252C%2520we%2520generate%25206M%2520free-text%2520descriptions%2520of%25203D%2520objects%252C%2520and%2520design%2520a%250Athree-stage%2520training%2520strategy%2520to%2520help%2520LLMs%2520better%2520explore%2520the%2520intrinsic%250Aconnections%2520between%2520different%2520modalities.%2520To%2520achieve%2520efficient%2520modality%250Aalignment%252C%2520we%2520design%2520a%2520zero-parameter%2520cross-attention%2520module%2520for%2520token%2520pooling.%250AExtensive%2520experimental%2520results%2520show%2520that%2520GreenPLM%2520requires%2520only%252012%2525%2520of%2520the%25203D%250Atraining%2520data%2520used%2520by%2520existing%2520state-of-the-art%2520models%2520to%2520achieve%2520superior%25203D%250Aunderstanding.%2520Remarkably%252C%2520GreenPLM%2520also%2520achieves%2520competitive%2520performance%2520using%250Atext-only%2520data.%2520The%2520code%2520and%2520weights%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/TangYuan96/GreenPLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=More%20Text%2C%20Less%20Point%3A%20Towards%203D%20Data-Efficient%20Point-Language%0A%20%20Understanding&entry.906535625=Yuan%20Tang%20and%20Xu%20Han%20and%20Xianzhi%20Li%20and%20Qiao%20Yu%20and%20Jinfeng%20Xu%20and%20Yixue%20Hao%20and%20Long%20Hu%20and%20Min%20Chen&entry.1292438233=%20%20Enabling%20Large%20Language%20Models%20%28LLMs%29%20to%20comprehend%20the%203D%20physical%20world%0Aremains%20a%20significant%20challenge.%20Due%20to%20the%20lack%20of%20large-scale%203D-text%20pair%0Adatasets%2C%20the%20success%20of%20LLMs%20has%20yet%20to%20be%20replicated%20in%203D%20understanding.%20In%0Athis%20paper%2C%20we%20rethink%20this%20issue%20and%20propose%20a%20new%20task%3A%203D%20Data-Efficient%0APoint-Language%20Understanding.%20The%20goal%20is%20to%20enable%20LLMs%20to%20achieve%20robust%203D%0Aobject%20understanding%20with%20minimal%203D%20point%20cloud%20and%20text%20data%20pairs.%20To%0Aaddress%20this%20task%2C%20we%20introduce%20GreenPLM%2C%20which%20leverages%20more%20text%20data%20to%0Acompensate%20for%20the%20lack%20of%203D%20data.%20First%2C%20inspired%20by%20using%20CLIP%20to%20align%0Aimages%20and%20text%2C%20we%20utilize%20a%20pre-trained%20point%20cloud-text%20encoder%20to%20map%20the%0A3D%20point%20cloud%20space%20to%20the%20text%20space.%20This%20mapping%20leaves%20us%20to%20seamlessly%0Aconnect%20the%20text%20space%20with%20LLMs.%20Once%20the%20point-text-LLM%20connection%20is%0Aestablished%2C%20we%20further%20enhance%20text-LLM%20alignment%20by%20expanding%20the%0Aintermediate%20text%20space%2C%20thereby%20reducing%20the%20reliance%20on%203D%20point%20cloud%20data.%0ASpecifically%2C%20we%20generate%206M%20free-text%20descriptions%20of%203D%20objects%2C%20and%20design%20a%0Athree-stage%20training%20strategy%20to%20help%20LLMs%20better%20explore%20the%20intrinsic%0Aconnections%20between%20different%20modalities.%20To%20achieve%20efficient%20modality%0Aalignment%2C%20we%20design%20a%20zero-parameter%20cross-attention%20module%20for%20token%20pooling.%0AExtensive%20experimental%20results%20show%20that%20GreenPLM%20requires%20only%2012%25%20of%20the%203D%0Atraining%20data%20used%20by%20existing%20state-of-the-art%20models%20to%20achieve%20superior%203D%0Aunderstanding.%20Remarkably%2C%20GreenPLM%20also%20achieves%20competitive%20performance%20using%0Atext-only%20data.%20The%20code%20and%20weights%20are%20available%20at%3A%0Ahttps%3A//github.com/TangYuan96/GreenPLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15966v1&entry.124074799=Read"},
{"title": "Mining Field Data for Tree Species Recognition at Scale", "author": "Dimitri Gominski and Daniel Ortiz-Gonzalo and Martin Brandt and Maurice Mugabowindekwe and Rasmus Fensholt", "abstract": "  Individual tree species labels are particularly hard to acquire due to the\nexpert knowledge needed and the limitations of photointerpretation. Here, we\npresent a methodology to automatically mine species labels from public forest\ninventory data, using available pretrained tree detection models. We identify\ntree instances in aerial imagery and match them with field data with close to\nzero human involvement. We conduct a series of experiments on the resulting\ndataset, and show a beneficial effect when adding noisy or even unlabeled data\npoints, highlighting a strong potential for large-scale individual species\nmapping.\n", "link": "http://arxiv.org/abs/2408.15816v1", "date": "2024-08-28", "relevancy": 2.3182, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4785}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4609}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mining%20Field%20Data%20for%20Tree%20Species%20Recognition%20at%20Scale&body=Title%3A%20Mining%20Field%20Data%20for%20Tree%20Species%20Recognition%20at%20Scale%0AAuthor%3A%20Dimitri%20Gominski%20and%20Daniel%20Ortiz-Gonzalo%20and%20Martin%20Brandt%20and%20Maurice%20Mugabowindekwe%20and%20Rasmus%20Fensholt%0AAbstract%3A%20%20%20Individual%20tree%20species%20labels%20are%20particularly%20hard%20to%20acquire%20due%20to%20the%0Aexpert%20knowledge%20needed%20and%20the%20limitations%20of%20photointerpretation.%20Here%2C%20we%0Apresent%20a%20methodology%20to%20automatically%20mine%20species%20labels%20from%20public%20forest%0Ainventory%20data%2C%20using%20available%20pretrained%20tree%20detection%20models.%20We%20identify%0Atree%20instances%20in%20aerial%20imagery%20and%20match%20them%20with%20field%20data%20with%20close%20to%0Azero%20human%20involvement.%20We%20conduct%20a%20series%20of%20experiments%20on%20the%20resulting%0Adataset%2C%20and%20show%20a%20beneficial%20effect%20when%20adding%20noisy%20or%20even%20unlabeled%20data%0Apoints%2C%20highlighting%20a%20strong%20potential%20for%20large-scale%20individual%20species%0Amapping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15816v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMining%2520Field%2520Data%2520for%2520Tree%2520Species%2520Recognition%2520at%2520Scale%26entry.906535625%3DDimitri%2520Gominski%2520and%2520Daniel%2520Ortiz-Gonzalo%2520and%2520Martin%2520Brandt%2520and%2520Maurice%2520Mugabowindekwe%2520and%2520Rasmus%2520Fensholt%26entry.1292438233%3D%2520%2520Individual%2520tree%2520species%2520labels%2520are%2520particularly%2520hard%2520to%2520acquire%2520due%2520to%2520the%250Aexpert%2520knowledge%2520needed%2520and%2520the%2520limitations%2520of%2520photointerpretation.%2520Here%252C%2520we%250Apresent%2520a%2520methodology%2520to%2520automatically%2520mine%2520species%2520labels%2520from%2520public%2520forest%250Ainventory%2520data%252C%2520using%2520available%2520pretrained%2520tree%2520detection%2520models.%2520We%2520identify%250Atree%2520instances%2520in%2520aerial%2520imagery%2520and%2520match%2520them%2520with%2520field%2520data%2520with%2520close%2520to%250Azero%2520human%2520involvement.%2520We%2520conduct%2520a%2520series%2520of%2520experiments%2520on%2520the%2520resulting%250Adataset%252C%2520and%2520show%2520a%2520beneficial%2520effect%2520when%2520adding%2520noisy%2520or%2520even%2520unlabeled%2520data%250Apoints%252C%2520highlighting%2520a%2520strong%2520potential%2520for%2520large-scale%2520individual%2520species%250Amapping.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15816v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mining%20Field%20Data%20for%20Tree%20Species%20Recognition%20at%20Scale&entry.906535625=Dimitri%20Gominski%20and%20Daniel%20Ortiz-Gonzalo%20and%20Martin%20Brandt%20and%20Maurice%20Mugabowindekwe%20and%20Rasmus%20Fensholt&entry.1292438233=%20%20Individual%20tree%20species%20labels%20are%20particularly%20hard%20to%20acquire%20due%20to%20the%0Aexpert%20knowledge%20needed%20and%20the%20limitations%20of%20photointerpretation.%20Here%2C%20we%0Apresent%20a%20methodology%20to%20automatically%20mine%20species%20labels%20from%20public%20forest%0Ainventory%20data%2C%20using%20available%20pretrained%20tree%20detection%20models.%20We%20identify%0Atree%20instances%20in%20aerial%20imagery%20and%20match%20them%20with%20field%20data%20with%20close%20to%0Azero%20human%20involvement.%20We%20conduct%20a%20series%20of%20experiments%20on%20the%20resulting%0Adataset%2C%20and%20show%20a%20beneficial%20effect%20when%20adding%20noisy%20or%20even%20unlabeled%20data%0Apoints%2C%20highlighting%20a%20strong%20potential%20for%20large-scale%20individual%20species%0Amapping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15816v1&entry.124074799=Read"},
{"title": "FRAME: A Modular Framework for Autonomous Map Merging: Advancements in\n  the Field", "author": "Nikolaos Stathoulopoulos and Bj\u00f6rn Lindqvist and Anton Koval and Ali-akbar Agha-mohammadi and George Nikolakopoulos", "abstract": "  In this article, a novel approach for merging 3D point cloud maps in the\ncontext of egocentric multi-robot exploration is presented. Unlike traditional\nmethods, the proposed approach leverages state-of-the-art place recognition and\nlearned descriptors to efficiently detect overlap between maps, eliminating the\nneed for the time-consuming global feature extraction and feature matching\nprocess. The estimated overlapping regions are used to calculate a homogeneous\nrigid transform, which serves as an initial condition for the GICP point cloud\nregistration algorithm to refine the alignment between the maps. The advantages\nof this approach include faster processing time, improved accuracy, and\nincreased robustness in challenging environments. Furthermore, the\neffectiveness of the proposed framework is successfully demonstrated through\nmultiple field missions of robot exploration in a variety of different\nunderground environments.\n", "link": "http://arxiv.org/abs/2404.18006v2", "date": "2024-08-28", "relevancy": 2.3148, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5865}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5818}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FRAME%3A%20A%20Modular%20Framework%20for%20Autonomous%20Map%20Merging%3A%20Advancements%20in%0A%20%20the%20Field&body=Title%3A%20FRAME%3A%20A%20Modular%20Framework%20for%20Autonomous%20Map%20Merging%3A%20Advancements%20in%0A%20%20the%20Field%0AAuthor%3A%20Nikolaos%20Stathoulopoulos%20and%20Bj%C3%B6rn%20Lindqvist%20and%20Anton%20Koval%20and%20Ali-akbar%20Agha-mohammadi%20and%20George%20Nikolakopoulos%0AAbstract%3A%20%20%20In%20this%20article%2C%20a%20novel%20approach%20for%20merging%203D%20point%20cloud%20maps%20in%20the%0Acontext%20of%20egocentric%20multi-robot%20exploration%20is%20presented.%20Unlike%20traditional%0Amethods%2C%20the%20proposed%20approach%20leverages%20state-of-the-art%20place%20recognition%20and%0Alearned%20descriptors%20to%20efficiently%20detect%20overlap%20between%20maps%2C%20eliminating%20the%0Aneed%20for%20the%20time-consuming%20global%20feature%20extraction%20and%20feature%20matching%0Aprocess.%20The%20estimated%20overlapping%20regions%20are%20used%20to%20calculate%20a%20homogeneous%0Arigid%20transform%2C%20which%20serves%20as%20an%20initial%20condition%20for%20the%20GICP%20point%20cloud%0Aregistration%20algorithm%20to%20refine%20the%20alignment%20between%20the%20maps.%20The%20advantages%0Aof%20this%20approach%20include%20faster%20processing%20time%2C%20improved%20accuracy%2C%20and%0Aincreased%20robustness%20in%20challenging%20environments.%20Furthermore%2C%20the%0Aeffectiveness%20of%20the%20proposed%20framework%20is%20successfully%20demonstrated%20through%0Amultiple%20field%20missions%20of%20robot%20exploration%20in%20a%20variety%20of%20different%0Aunderground%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18006v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFRAME%253A%2520A%2520Modular%2520Framework%2520for%2520Autonomous%2520Map%2520Merging%253A%2520Advancements%2520in%250A%2520%2520the%2520Field%26entry.906535625%3DNikolaos%2520Stathoulopoulos%2520and%2520Bj%25C3%25B6rn%2520Lindqvist%2520and%2520Anton%2520Koval%2520and%2520Ali-akbar%2520Agha-mohammadi%2520and%2520George%2520Nikolakopoulos%26entry.1292438233%3D%2520%2520In%2520this%2520article%252C%2520a%2520novel%2520approach%2520for%2520merging%25203D%2520point%2520cloud%2520maps%2520in%2520the%250Acontext%2520of%2520egocentric%2520multi-robot%2520exploration%2520is%2520presented.%2520Unlike%2520traditional%250Amethods%252C%2520the%2520proposed%2520approach%2520leverages%2520state-of-the-art%2520place%2520recognition%2520and%250Alearned%2520descriptors%2520to%2520efficiently%2520detect%2520overlap%2520between%2520maps%252C%2520eliminating%2520the%250Aneed%2520for%2520the%2520time-consuming%2520global%2520feature%2520extraction%2520and%2520feature%2520matching%250Aprocess.%2520The%2520estimated%2520overlapping%2520regions%2520are%2520used%2520to%2520calculate%2520a%2520homogeneous%250Arigid%2520transform%252C%2520which%2520serves%2520as%2520an%2520initial%2520condition%2520for%2520the%2520GICP%2520point%2520cloud%250Aregistration%2520algorithm%2520to%2520refine%2520the%2520alignment%2520between%2520the%2520maps.%2520The%2520advantages%250Aof%2520this%2520approach%2520include%2520faster%2520processing%2520time%252C%2520improved%2520accuracy%252C%2520and%250Aincreased%2520robustness%2520in%2520challenging%2520environments.%2520Furthermore%252C%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520framework%2520is%2520successfully%2520demonstrated%2520through%250Amultiple%2520field%2520missions%2520of%2520robot%2520exploration%2520in%2520a%2520variety%2520of%2520different%250Aunderground%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18006v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRAME%3A%20A%20Modular%20Framework%20for%20Autonomous%20Map%20Merging%3A%20Advancements%20in%0A%20%20the%20Field&entry.906535625=Nikolaos%20Stathoulopoulos%20and%20Bj%C3%B6rn%20Lindqvist%20and%20Anton%20Koval%20and%20Ali-akbar%20Agha-mohammadi%20and%20George%20Nikolakopoulos&entry.1292438233=%20%20In%20this%20article%2C%20a%20novel%20approach%20for%20merging%203D%20point%20cloud%20maps%20in%20the%0Acontext%20of%20egocentric%20multi-robot%20exploration%20is%20presented.%20Unlike%20traditional%0Amethods%2C%20the%20proposed%20approach%20leverages%20state-of-the-art%20place%20recognition%20and%0Alearned%20descriptors%20to%20efficiently%20detect%20overlap%20between%20maps%2C%20eliminating%20the%0Aneed%20for%20the%20time-consuming%20global%20feature%20extraction%20and%20feature%20matching%0Aprocess.%20The%20estimated%20overlapping%20regions%20are%20used%20to%20calculate%20a%20homogeneous%0Arigid%20transform%2C%20which%20serves%20as%20an%20initial%20condition%20for%20the%20GICP%20point%20cloud%0Aregistration%20algorithm%20to%20refine%20the%20alignment%20between%20the%20maps.%20The%20advantages%0Aof%20this%20approach%20include%20faster%20processing%20time%2C%20improved%20accuracy%2C%20and%0Aincreased%20robustness%20in%20challenging%20environments.%20Furthermore%2C%20the%0Aeffectiveness%20of%20the%20proposed%20framework%20is%20successfully%20demonstrated%20through%0Amultiple%20field%20missions%20of%20robot%20exploration%20in%20a%20variety%20of%20different%0Aunderground%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18006v2&entry.124074799=Read"},
{"title": "BIM-SLAM: Integrating BIM Models in Multi-session SLAM for Lifelong\n  Mapping using 3D LiDAR", "author": "Miguel Arturo Vega Torres and Alexander Braun and Andr\u00e9 Borrmann", "abstract": "  While 3D LiDAR sensor technology is becoming more advanced and cheaper every\nday, the growth of digitalization in the AEC industry contributes to the fact\nthat 3D building information models (BIM models) are now available for a large\npart of the built environment. These two facts open the question of how 3D\nmodels can support 3D LiDAR long-term SLAM in indoor, GPS-denied environments.\nThis paper proposes a methodology that leverages BIM models to create an\nupdated map of indoor environments with sequential LiDAR measurements. Session\ndata (pose graph-based map and descriptors) are initially generated from BIM\nmodels. Then, real-world data is aligned with the session data from the model\nusing multi-session anchoring while minimizing the drift on the real-world\ndata. Finally, the new elements not present in the BIM model are identified,\ngrouped, and reconstructed in a surface representation, allowing a better\nvisualization next to the BIM model. The framework enables the creation of a\ncoherent map aligned with the BIM model that does not require prior knowledge\nof the initial pose of the robot, and it does not need to be inside the map.\n", "link": "http://arxiv.org/abs/2408.15870v1", "date": "2024-08-28", "relevancy": 2.2693, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5743}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5682}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BIM-SLAM%3A%20Integrating%20BIM%20Models%20in%20Multi-session%20SLAM%20for%20Lifelong%0A%20%20Mapping%20using%203D%20LiDAR&body=Title%3A%20BIM-SLAM%3A%20Integrating%20BIM%20Models%20in%20Multi-session%20SLAM%20for%20Lifelong%0A%20%20Mapping%20using%203D%20LiDAR%0AAuthor%3A%20Miguel%20Arturo%20Vega%20Torres%20and%20Alexander%20Braun%20and%20Andr%C3%A9%20Borrmann%0AAbstract%3A%20%20%20While%203D%20LiDAR%20sensor%20technology%20is%20becoming%20more%20advanced%20and%20cheaper%20every%0Aday%2C%20the%20growth%20of%20digitalization%20in%20the%20AEC%20industry%20contributes%20to%20the%20fact%0Athat%203D%20building%20information%20models%20%28BIM%20models%29%20are%20now%20available%20for%20a%20large%0Apart%20of%20the%20built%20environment.%20These%20two%20facts%20open%20the%20question%20of%20how%203D%0Amodels%20can%20support%203D%20LiDAR%20long-term%20SLAM%20in%20indoor%2C%20GPS-denied%20environments.%0AThis%20paper%20proposes%20a%20methodology%20that%20leverages%20BIM%20models%20to%20create%20an%0Aupdated%20map%20of%20indoor%20environments%20with%20sequential%20LiDAR%20measurements.%20Session%0Adata%20%28pose%20graph-based%20map%20and%20descriptors%29%20are%20initially%20generated%20from%20BIM%0Amodels.%20Then%2C%20real-world%20data%20is%20aligned%20with%20the%20session%20data%20from%20the%20model%0Ausing%20multi-session%20anchoring%20while%20minimizing%20the%20drift%20on%20the%20real-world%0Adata.%20Finally%2C%20the%20new%20elements%20not%20present%20in%20the%20BIM%20model%20are%20identified%2C%0Agrouped%2C%20and%20reconstructed%20in%20a%20surface%20representation%2C%20allowing%20a%20better%0Avisualization%20next%20to%20the%20BIM%20model.%20The%20framework%20enables%20the%20creation%20of%20a%0Acoherent%20map%20aligned%20with%20the%20BIM%20model%20that%20does%20not%20require%20prior%20knowledge%0Aof%20the%20initial%20pose%20of%20the%20robot%2C%20and%20it%20does%20not%20need%20to%20be%20inside%20the%20map.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBIM-SLAM%253A%2520Integrating%2520BIM%2520Models%2520in%2520Multi-session%2520SLAM%2520for%2520Lifelong%250A%2520%2520Mapping%2520using%25203D%2520LiDAR%26entry.906535625%3DMiguel%2520Arturo%2520Vega%2520Torres%2520and%2520Alexander%2520Braun%2520and%2520Andr%25C3%25A9%2520Borrmann%26entry.1292438233%3D%2520%2520While%25203D%2520LiDAR%2520sensor%2520technology%2520is%2520becoming%2520more%2520advanced%2520and%2520cheaper%2520every%250Aday%252C%2520the%2520growth%2520of%2520digitalization%2520in%2520the%2520AEC%2520industry%2520contributes%2520to%2520the%2520fact%250Athat%25203D%2520building%2520information%2520models%2520%2528BIM%2520models%2529%2520are%2520now%2520available%2520for%2520a%2520large%250Apart%2520of%2520the%2520built%2520environment.%2520These%2520two%2520facts%2520open%2520the%2520question%2520of%2520how%25203D%250Amodels%2520can%2520support%25203D%2520LiDAR%2520long-term%2520SLAM%2520in%2520indoor%252C%2520GPS-denied%2520environments.%250AThis%2520paper%2520proposes%2520a%2520methodology%2520that%2520leverages%2520BIM%2520models%2520to%2520create%2520an%250Aupdated%2520map%2520of%2520indoor%2520environments%2520with%2520sequential%2520LiDAR%2520measurements.%2520Session%250Adata%2520%2528pose%2520graph-based%2520map%2520and%2520descriptors%2529%2520are%2520initially%2520generated%2520from%2520BIM%250Amodels.%2520Then%252C%2520real-world%2520data%2520is%2520aligned%2520with%2520the%2520session%2520data%2520from%2520the%2520model%250Ausing%2520multi-session%2520anchoring%2520while%2520minimizing%2520the%2520drift%2520on%2520the%2520real-world%250Adata.%2520Finally%252C%2520the%2520new%2520elements%2520not%2520present%2520in%2520the%2520BIM%2520model%2520are%2520identified%252C%250Agrouped%252C%2520and%2520reconstructed%2520in%2520a%2520surface%2520representation%252C%2520allowing%2520a%2520better%250Avisualization%2520next%2520to%2520the%2520BIM%2520model.%2520The%2520framework%2520enables%2520the%2520creation%2520of%2520a%250Acoherent%2520map%2520aligned%2520with%2520the%2520BIM%2520model%2520that%2520does%2520not%2520require%2520prior%2520knowledge%250Aof%2520the%2520initial%2520pose%2520of%2520the%2520robot%252C%2520and%2520it%2520does%2520not%2520need%2520to%2520be%2520inside%2520the%2520map.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BIM-SLAM%3A%20Integrating%20BIM%20Models%20in%20Multi-session%20SLAM%20for%20Lifelong%0A%20%20Mapping%20using%203D%20LiDAR&entry.906535625=Miguel%20Arturo%20Vega%20Torres%20and%20Alexander%20Braun%20and%20Andr%C3%A9%20Borrmann&entry.1292438233=%20%20While%203D%20LiDAR%20sensor%20technology%20is%20becoming%20more%20advanced%20and%20cheaper%20every%0Aday%2C%20the%20growth%20of%20digitalization%20in%20the%20AEC%20industry%20contributes%20to%20the%20fact%0Athat%203D%20building%20information%20models%20%28BIM%20models%29%20are%20now%20available%20for%20a%20large%0Apart%20of%20the%20built%20environment.%20These%20two%20facts%20open%20the%20question%20of%20how%203D%0Amodels%20can%20support%203D%20LiDAR%20long-term%20SLAM%20in%20indoor%2C%20GPS-denied%20environments.%0AThis%20paper%20proposes%20a%20methodology%20that%20leverages%20BIM%20models%20to%20create%20an%0Aupdated%20map%20of%20indoor%20environments%20with%20sequential%20LiDAR%20measurements.%20Session%0Adata%20%28pose%20graph-based%20map%20and%20descriptors%29%20are%20initially%20generated%20from%20BIM%0Amodels.%20Then%2C%20real-world%20data%20is%20aligned%20with%20the%20session%20data%20from%20the%20model%0Ausing%20multi-session%20anchoring%20while%20minimizing%20the%20drift%20on%20the%20real-world%0Adata.%20Finally%2C%20the%20new%20elements%20not%20present%20in%20the%20BIM%20model%20are%20identified%2C%0Agrouped%2C%20and%20reconstructed%20in%20a%20surface%20representation%2C%20allowing%20a%20better%0Avisualization%20next%20to%20the%20BIM%20model.%20The%20framework%20enables%20the%20creation%20of%20a%0Acoherent%20map%20aligned%20with%20the%20BIM%20model%20that%20does%20not%20require%20prior%20knowledge%0Aof%20the%20initial%20pose%20of%20the%20robot%2C%20and%20it%20does%20not%20need%20to%20be%20inside%20the%20map.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15870v1&entry.124074799=Read"},
{"title": "DQFormer: Towards Unified LiDAR Panoptic Segmentation with Decoupled\n  Queries", "author": "Yu Yang and Jianbiao Mei and Liang Liu and Siliang Du and Yilin Xiao and Jongwon Ra and Yong Liu and Xiao Xu and Huifeng Wu", "abstract": "  LiDAR panoptic segmentation, which jointly performs instance and semantic\nsegmentation for things and stuff classes, plays a fundamental role in LiDAR\nperception tasks. While most existing methods explicitly separate these two\nsegmentation tasks and utilize different branches (i.e., semantic and instance\nbranches), some recent methods have embraced the query-based paradigm to unify\nLiDAR panoptic segmentation. However, the distinct spatial distribution and\ninherent characteristics of objects(things) and their surroundings(stuff) in 3D\nscenes lead to challenges, including the mutual competition of things/stuff and\nthe ambiguity of classification/segmentation. In this paper, we propose\ndecoupling things/stuff queries according to their intrinsic properties for\nindividual decoding and disentangling classification/segmentation to mitigate\nambiguity. To this end, we propose a novel framework dubbed DQFormer to\nimplement semantic and instance segmentation in a unified workflow.\nSpecifically, we design a decoupled query generator to propose informative\nqueries with semantics by localizing things/stuff positions and fusing\nmulti-level BEV embeddings. Moreover, a query-oriented mask decoder is\nintroduced to decode corresponding segmentation masks by performing masked\ncross-attention between queries and mask embeddings. Finally, the decoded masks\nare combined with the semantics of the queries to produce panoptic results.\nExtensive experiments on nuScenes and SemanticKITTI datasets demonstrate the\nsuperiority of our DQFormer framework.\n", "link": "http://arxiv.org/abs/2408.15813v1", "date": "2024-08-28", "relevancy": 2.2672, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5776}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5594}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DQFormer%3A%20Towards%20Unified%20LiDAR%20Panoptic%20Segmentation%20with%20Decoupled%0A%20%20Queries&body=Title%3A%20DQFormer%3A%20Towards%20Unified%20LiDAR%20Panoptic%20Segmentation%20with%20Decoupled%0A%20%20Queries%0AAuthor%3A%20Yu%20Yang%20and%20Jianbiao%20Mei%20and%20Liang%20Liu%20and%20Siliang%20Du%20and%20Yilin%20Xiao%20and%20Jongwon%20Ra%20and%20Yong%20Liu%20and%20Xiao%20Xu%20and%20Huifeng%20Wu%0AAbstract%3A%20%20%20LiDAR%20panoptic%20segmentation%2C%20which%20jointly%20performs%20instance%20and%20semantic%0Asegmentation%20for%20things%20and%20stuff%20classes%2C%20plays%20a%20fundamental%20role%20in%20LiDAR%0Aperception%20tasks.%20While%20most%20existing%20methods%20explicitly%20separate%20these%20two%0Asegmentation%20tasks%20and%20utilize%20different%20branches%20%28i.e.%2C%20semantic%20and%20instance%0Abranches%29%2C%20some%20recent%20methods%20have%20embraced%20the%20query-based%20paradigm%20to%20unify%0ALiDAR%20panoptic%20segmentation.%20However%2C%20the%20distinct%20spatial%20distribution%20and%0Ainherent%20characteristics%20of%20objects%28things%29%20and%20their%20surroundings%28stuff%29%20in%203D%0Ascenes%20lead%20to%20challenges%2C%20including%20the%20mutual%20competition%20of%20things/stuff%20and%0Athe%20ambiguity%20of%20classification/segmentation.%20In%20this%20paper%2C%20we%20propose%0Adecoupling%20things/stuff%20queries%20according%20to%20their%20intrinsic%20properties%20for%0Aindividual%20decoding%20and%20disentangling%20classification/segmentation%20to%20mitigate%0Aambiguity.%20To%20this%20end%2C%20we%20propose%20a%20novel%20framework%20dubbed%20DQFormer%20to%0Aimplement%20semantic%20and%20instance%20segmentation%20in%20a%20unified%20workflow.%0ASpecifically%2C%20we%20design%20a%20decoupled%20query%20generator%20to%20propose%20informative%0Aqueries%20with%20semantics%20by%20localizing%20things/stuff%20positions%20and%20fusing%0Amulti-level%20BEV%20embeddings.%20Moreover%2C%20a%20query-oriented%20mask%20decoder%20is%0Aintroduced%20to%20decode%20corresponding%20segmentation%20masks%20by%20performing%20masked%0Across-attention%20between%20queries%20and%20mask%20embeddings.%20Finally%2C%20the%20decoded%20masks%0Aare%20combined%20with%20the%20semantics%20of%20the%20queries%20to%20produce%20panoptic%20results.%0AExtensive%20experiments%20on%20nuScenes%20and%20SemanticKITTI%20datasets%20demonstrate%20the%0Asuperiority%20of%20our%20DQFormer%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDQFormer%253A%2520Towards%2520Unified%2520LiDAR%2520Panoptic%2520Segmentation%2520with%2520Decoupled%250A%2520%2520Queries%26entry.906535625%3DYu%2520Yang%2520and%2520Jianbiao%2520Mei%2520and%2520Liang%2520Liu%2520and%2520Siliang%2520Du%2520and%2520Yilin%2520Xiao%2520and%2520Jongwon%2520Ra%2520and%2520Yong%2520Liu%2520and%2520Xiao%2520Xu%2520and%2520Huifeng%2520Wu%26entry.1292438233%3D%2520%2520LiDAR%2520panoptic%2520segmentation%252C%2520which%2520jointly%2520performs%2520instance%2520and%2520semantic%250Asegmentation%2520for%2520things%2520and%2520stuff%2520classes%252C%2520plays%2520a%2520fundamental%2520role%2520in%2520LiDAR%250Aperception%2520tasks.%2520While%2520most%2520existing%2520methods%2520explicitly%2520separate%2520these%2520two%250Asegmentation%2520tasks%2520and%2520utilize%2520different%2520branches%2520%2528i.e.%252C%2520semantic%2520and%2520instance%250Abranches%2529%252C%2520some%2520recent%2520methods%2520have%2520embraced%2520the%2520query-based%2520paradigm%2520to%2520unify%250ALiDAR%2520panoptic%2520segmentation.%2520However%252C%2520the%2520distinct%2520spatial%2520distribution%2520and%250Ainherent%2520characteristics%2520of%2520objects%2528things%2529%2520and%2520their%2520surroundings%2528stuff%2529%2520in%25203D%250Ascenes%2520lead%2520to%2520challenges%252C%2520including%2520the%2520mutual%2520competition%2520of%2520things/stuff%2520and%250Athe%2520ambiguity%2520of%2520classification/segmentation.%2520In%2520this%2520paper%252C%2520we%2520propose%250Adecoupling%2520things/stuff%2520queries%2520according%2520to%2520their%2520intrinsic%2520properties%2520for%250Aindividual%2520decoding%2520and%2520disentangling%2520classification/segmentation%2520to%2520mitigate%250Aambiguity.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520framework%2520dubbed%2520DQFormer%2520to%250Aimplement%2520semantic%2520and%2520instance%2520segmentation%2520in%2520a%2520unified%2520workflow.%250ASpecifically%252C%2520we%2520design%2520a%2520decoupled%2520query%2520generator%2520to%2520propose%2520informative%250Aqueries%2520with%2520semantics%2520by%2520localizing%2520things/stuff%2520positions%2520and%2520fusing%250Amulti-level%2520BEV%2520embeddings.%2520Moreover%252C%2520a%2520query-oriented%2520mask%2520decoder%2520is%250Aintroduced%2520to%2520decode%2520corresponding%2520segmentation%2520masks%2520by%2520performing%2520masked%250Across-attention%2520between%2520queries%2520and%2520mask%2520embeddings.%2520Finally%252C%2520the%2520decoded%2520masks%250Aare%2520combined%2520with%2520the%2520semantics%2520of%2520the%2520queries%2520to%2520produce%2520panoptic%2520results.%250AExtensive%2520experiments%2520on%2520nuScenes%2520and%2520SemanticKITTI%2520datasets%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520DQFormer%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DQFormer%3A%20Towards%20Unified%20LiDAR%20Panoptic%20Segmentation%20with%20Decoupled%0A%20%20Queries&entry.906535625=Yu%20Yang%20and%20Jianbiao%20Mei%20and%20Liang%20Liu%20and%20Siliang%20Du%20and%20Yilin%20Xiao%20and%20Jongwon%20Ra%20and%20Yong%20Liu%20and%20Xiao%20Xu%20and%20Huifeng%20Wu&entry.1292438233=%20%20LiDAR%20panoptic%20segmentation%2C%20which%20jointly%20performs%20instance%20and%20semantic%0Asegmentation%20for%20things%20and%20stuff%20classes%2C%20plays%20a%20fundamental%20role%20in%20LiDAR%0Aperception%20tasks.%20While%20most%20existing%20methods%20explicitly%20separate%20these%20two%0Asegmentation%20tasks%20and%20utilize%20different%20branches%20%28i.e.%2C%20semantic%20and%20instance%0Abranches%29%2C%20some%20recent%20methods%20have%20embraced%20the%20query-based%20paradigm%20to%20unify%0ALiDAR%20panoptic%20segmentation.%20However%2C%20the%20distinct%20spatial%20distribution%20and%0Ainherent%20characteristics%20of%20objects%28things%29%20and%20their%20surroundings%28stuff%29%20in%203D%0Ascenes%20lead%20to%20challenges%2C%20including%20the%20mutual%20competition%20of%20things/stuff%20and%0Athe%20ambiguity%20of%20classification/segmentation.%20In%20this%20paper%2C%20we%20propose%0Adecoupling%20things/stuff%20queries%20according%20to%20their%20intrinsic%20properties%20for%0Aindividual%20decoding%20and%20disentangling%20classification/segmentation%20to%20mitigate%0Aambiguity.%20To%20this%20end%2C%20we%20propose%20a%20novel%20framework%20dubbed%20DQFormer%20to%0Aimplement%20semantic%20and%20instance%20segmentation%20in%20a%20unified%20workflow.%0ASpecifically%2C%20we%20design%20a%20decoupled%20query%20generator%20to%20propose%20informative%0Aqueries%20with%20semantics%20by%20localizing%20things/stuff%20positions%20and%20fusing%0Amulti-level%20BEV%20embeddings.%20Moreover%2C%20a%20query-oriented%20mask%20decoder%20is%0Aintroduced%20to%20decode%20corresponding%20segmentation%20masks%20by%20performing%20masked%0Across-attention%20between%20queries%20and%20mask%20embeddings.%20Finally%2C%20the%20decoded%20masks%0Aare%20combined%20with%20the%20semantics%20of%20the%20queries%20to%20produce%20panoptic%20results.%0AExtensive%20experiments%20on%20nuScenes%20and%20SemanticKITTI%20datasets%20demonstrate%20the%0Asuperiority%20of%20our%20DQFormer%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15813v1&entry.124074799=Read"},
{"title": "Gen-Swarms: Adapting Deep Generative Models to Swarms of Drones", "author": "Carlos Plou and Pablo Pueyo and Ruben Martinez-Cantin and Mac Schwager and Ana C. Murillo and Eduardo Montijano", "abstract": "  Gen-Swarms is an innovative method that leverages and combines the\ncapabilities of deep generative models with reactive navigation algorithms to\nautomate the creation of drone shows. Advancements in deep generative models,\nparticularly diffusion models, have demonstrated remarkable effectiveness in\ngenerating high-quality 2D images. Building on this success, various works have\nextended diffusion models to 3D point cloud generation. In contrast,\nalternative generative models such as flow matching have been proposed,\noffering a simple and intuitive transition from noise to meaningful outputs.\nHowever, the application of flow matching models to 3D point cloud generation\nremains largely unexplored. Gen-Swarms adapts these models to automatically\ngenerate drone shows. Existing 3D point cloud generative models create point\ntrajectories which are impractical for drone swarms. In contrast, our method\nnot only generates accurate 3D shapes but also guides the swarm motion,\nproducing smooth trajectories and accounting for potential collisions through a\nreactive navigation algorithm incorporated into the sampling process. For\nexample, when given a text category like Airplane, Gen-Swarms can rapidly and\ncontinuously generate numerous variations of 3D airplane shapes. Our\nexperiments demonstrate that this approach is particularly well-suited for\ndrone shows, providing feasible trajectories, creating representative final\nshapes, and significantly enhancing the overall performance of drone show\ngeneration.\n", "link": "http://arxiv.org/abs/2408.15899v1", "date": "2024-08-28", "relevancy": 2.24, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5771}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5513}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gen-Swarms%3A%20Adapting%20Deep%20Generative%20Models%20to%20Swarms%20of%20Drones&body=Title%3A%20Gen-Swarms%3A%20Adapting%20Deep%20Generative%20Models%20to%20Swarms%20of%20Drones%0AAuthor%3A%20Carlos%20Plou%20and%20Pablo%20Pueyo%20and%20Ruben%20Martinez-Cantin%20and%20Mac%20Schwager%20and%20Ana%20C.%20Murillo%20and%20Eduardo%20Montijano%0AAbstract%3A%20%20%20Gen-Swarms%20is%20an%20innovative%20method%20that%20leverages%20and%20combines%20the%0Acapabilities%20of%20deep%20generative%20models%20with%20reactive%20navigation%20algorithms%20to%0Aautomate%20the%20creation%20of%20drone%20shows.%20Advancements%20in%20deep%20generative%20models%2C%0Aparticularly%20diffusion%20models%2C%20have%20demonstrated%20remarkable%20effectiveness%20in%0Agenerating%20high-quality%202D%20images.%20Building%20on%20this%20success%2C%20various%20works%20have%0Aextended%20diffusion%20models%20to%203D%20point%20cloud%20generation.%20In%20contrast%2C%0Aalternative%20generative%20models%20such%20as%20flow%20matching%20have%20been%20proposed%2C%0Aoffering%20a%20simple%20and%20intuitive%20transition%20from%20noise%20to%20meaningful%20outputs.%0AHowever%2C%20the%20application%20of%20flow%20matching%20models%20to%203D%20point%20cloud%20generation%0Aremains%20largely%20unexplored.%20Gen-Swarms%20adapts%20these%20models%20to%20automatically%0Agenerate%20drone%20shows.%20Existing%203D%20point%20cloud%20generative%20models%20create%20point%0Atrajectories%20which%20are%20impractical%20for%20drone%20swarms.%20In%20contrast%2C%20our%20method%0Anot%20only%20generates%20accurate%203D%20shapes%20but%20also%20guides%20the%20swarm%20motion%2C%0Aproducing%20smooth%20trajectories%20and%20accounting%20for%20potential%20collisions%20through%20a%0Areactive%20navigation%20algorithm%20incorporated%20into%20the%20sampling%20process.%20For%0Aexample%2C%20when%20given%20a%20text%20category%20like%20Airplane%2C%20Gen-Swarms%20can%20rapidly%20and%0Acontinuously%20generate%20numerous%20variations%20of%203D%20airplane%20shapes.%20Our%0Aexperiments%20demonstrate%20that%20this%20approach%20is%20particularly%20well-suited%20for%0Adrone%20shows%2C%20providing%20feasible%20trajectories%2C%20creating%20representative%20final%0Ashapes%2C%20and%20significantly%20enhancing%20the%20overall%20performance%20of%20drone%20show%0Ageneration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15899v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGen-Swarms%253A%2520Adapting%2520Deep%2520Generative%2520Models%2520to%2520Swarms%2520of%2520Drones%26entry.906535625%3DCarlos%2520Plou%2520and%2520Pablo%2520Pueyo%2520and%2520Ruben%2520Martinez-Cantin%2520and%2520Mac%2520Schwager%2520and%2520Ana%2520C.%2520Murillo%2520and%2520Eduardo%2520Montijano%26entry.1292438233%3D%2520%2520Gen-Swarms%2520is%2520an%2520innovative%2520method%2520that%2520leverages%2520and%2520combines%2520the%250Acapabilities%2520of%2520deep%2520generative%2520models%2520with%2520reactive%2520navigation%2520algorithms%2520to%250Aautomate%2520the%2520creation%2520of%2520drone%2520shows.%2520Advancements%2520in%2520deep%2520generative%2520models%252C%250Aparticularly%2520diffusion%2520models%252C%2520have%2520demonstrated%2520remarkable%2520effectiveness%2520in%250Agenerating%2520high-quality%25202D%2520images.%2520Building%2520on%2520this%2520success%252C%2520various%2520works%2520have%250Aextended%2520diffusion%2520models%2520to%25203D%2520point%2520cloud%2520generation.%2520In%2520contrast%252C%250Aalternative%2520generative%2520models%2520such%2520as%2520flow%2520matching%2520have%2520been%2520proposed%252C%250Aoffering%2520a%2520simple%2520and%2520intuitive%2520transition%2520from%2520noise%2520to%2520meaningful%2520outputs.%250AHowever%252C%2520the%2520application%2520of%2520flow%2520matching%2520models%2520to%25203D%2520point%2520cloud%2520generation%250Aremains%2520largely%2520unexplored.%2520Gen-Swarms%2520adapts%2520these%2520models%2520to%2520automatically%250Agenerate%2520drone%2520shows.%2520Existing%25203D%2520point%2520cloud%2520generative%2520models%2520create%2520point%250Atrajectories%2520which%2520are%2520impractical%2520for%2520drone%2520swarms.%2520In%2520contrast%252C%2520our%2520method%250Anot%2520only%2520generates%2520accurate%25203D%2520shapes%2520but%2520also%2520guides%2520the%2520swarm%2520motion%252C%250Aproducing%2520smooth%2520trajectories%2520and%2520accounting%2520for%2520potential%2520collisions%2520through%2520a%250Areactive%2520navigation%2520algorithm%2520incorporated%2520into%2520the%2520sampling%2520process.%2520For%250Aexample%252C%2520when%2520given%2520a%2520text%2520category%2520like%2520Airplane%252C%2520Gen-Swarms%2520can%2520rapidly%2520and%250Acontinuously%2520generate%2520numerous%2520variations%2520of%25203D%2520airplane%2520shapes.%2520Our%250Aexperiments%2520demonstrate%2520that%2520this%2520approach%2520is%2520particularly%2520well-suited%2520for%250Adrone%2520shows%252C%2520providing%2520feasible%2520trajectories%252C%2520creating%2520representative%2520final%250Ashapes%252C%2520and%2520significantly%2520enhancing%2520the%2520overall%2520performance%2520of%2520drone%2520show%250Ageneration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15899v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gen-Swarms%3A%20Adapting%20Deep%20Generative%20Models%20to%20Swarms%20of%20Drones&entry.906535625=Carlos%20Plou%20and%20Pablo%20Pueyo%20and%20Ruben%20Martinez-Cantin%20and%20Mac%20Schwager%20and%20Ana%20C.%20Murillo%20and%20Eduardo%20Montijano&entry.1292438233=%20%20Gen-Swarms%20is%20an%20innovative%20method%20that%20leverages%20and%20combines%20the%0Acapabilities%20of%20deep%20generative%20models%20with%20reactive%20navigation%20algorithms%20to%0Aautomate%20the%20creation%20of%20drone%20shows.%20Advancements%20in%20deep%20generative%20models%2C%0Aparticularly%20diffusion%20models%2C%20have%20demonstrated%20remarkable%20effectiveness%20in%0Agenerating%20high-quality%202D%20images.%20Building%20on%20this%20success%2C%20various%20works%20have%0Aextended%20diffusion%20models%20to%203D%20point%20cloud%20generation.%20In%20contrast%2C%0Aalternative%20generative%20models%20such%20as%20flow%20matching%20have%20been%20proposed%2C%0Aoffering%20a%20simple%20and%20intuitive%20transition%20from%20noise%20to%20meaningful%20outputs.%0AHowever%2C%20the%20application%20of%20flow%20matching%20models%20to%203D%20point%20cloud%20generation%0Aremains%20largely%20unexplored.%20Gen-Swarms%20adapts%20these%20models%20to%20automatically%0Agenerate%20drone%20shows.%20Existing%203D%20point%20cloud%20generative%20models%20create%20point%0Atrajectories%20which%20are%20impractical%20for%20drone%20swarms.%20In%20contrast%2C%20our%20method%0Anot%20only%20generates%20accurate%203D%20shapes%20but%20also%20guides%20the%20swarm%20motion%2C%0Aproducing%20smooth%20trajectories%20and%20accounting%20for%20potential%20collisions%20through%20a%0Areactive%20navigation%20algorithm%20incorporated%20into%20the%20sampling%20process.%20For%0Aexample%2C%20when%20given%20a%20text%20category%20like%20Airplane%2C%20Gen-Swarms%20can%20rapidly%20and%0Acontinuously%20generate%20numerous%20variations%20of%203D%20airplane%20shapes.%20Our%0Aexperiments%20demonstrate%20that%20this%20approach%20is%20particularly%20well-suited%20for%0Adrone%20shows%2C%20providing%20feasible%20trajectories%2C%20creating%20representative%20final%0Ashapes%2C%20and%20significantly%20enhancing%20the%20overall%20performance%20of%20drone%20show%0Ageneration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15899v1&entry.124074799=Read"},
{"title": "Synthetic Forehead-creases Biometric Generation for Reliable User\n  Verification", "author": "Abhishek Tandon and Geetanjali Sharma and Gaurav Jaswal and Aditya Nigam and Raghavendra Ramachandra", "abstract": "  Recent studies have emphasized the potential of forehead-crease patterns as\nan alternative for face, iris, and periocular recognition, presenting\ncontactless and convenient solutions, particularly in situations where faces\nare covered by surgical masks. However, collecting forehead data presents\nchallenges, including cost and time constraints, as developing and optimizing\nforehead verification methods requires a substantial number of high-quality\nimages. To tackle these challenges, the generation of synthetic biometric data\nhas gained traction due to its ability to protect privacy while enabling\neffective training of deep learning-based biometric verification methods. In\nthis paper, we present a new framework to synthesize forehead-crease image data\nwhile maintaining important features, such as uniqueness and realism. The\nproposed framework consists of two main modules: a Subject-Specific Generation\nModule (SSGM), based on an image-to-image Brownian Bridge Diffusion Model\n(BBDM), which learns a one-to-many mapping between image pairs to generate\nidentity-aware synthetic forehead creases corresponding to real subjects, and a\nSubject-Agnostic Generation Module (SAGM), which samples new synthetic\nidentities with assistance from the SSGM. We evaluate the diversity and realism\nof the generated forehead-crease images primarily using the Fr\\'echet Inception\nDistance (FID) and the Structural Similarity Index Measure (SSIM). In addition,\nwe assess the utility of synthetically generated forehead-crease images using a\nforehead-crease verification system (FHCVS). The results indicate an\nimprovement in the verification accuracy of the FHCVS by utilizing synthetic\ndata.\n", "link": "http://arxiv.org/abs/2408.15693v1", "date": "2024-08-28", "relevancy": 2.2297, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5662}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5557}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Forehead-creases%20Biometric%20Generation%20for%20Reliable%20User%0A%20%20Verification&body=Title%3A%20Synthetic%20Forehead-creases%20Biometric%20Generation%20for%20Reliable%20User%0A%20%20Verification%0AAuthor%3A%20Abhishek%20Tandon%20and%20Geetanjali%20Sharma%20and%20Gaurav%20Jaswal%20and%20Aditya%20Nigam%20and%20Raghavendra%20Ramachandra%0AAbstract%3A%20%20%20Recent%20studies%20have%20emphasized%20the%20potential%20of%20forehead-crease%20patterns%20as%0Aan%20alternative%20for%20face%2C%20iris%2C%20and%20periocular%20recognition%2C%20presenting%0Acontactless%20and%20convenient%20solutions%2C%20particularly%20in%20situations%20where%20faces%0Aare%20covered%20by%20surgical%20masks.%20However%2C%20collecting%20forehead%20data%20presents%0Achallenges%2C%20including%20cost%20and%20time%20constraints%2C%20as%20developing%20and%20optimizing%0Aforehead%20verification%20methods%20requires%20a%20substantial%20number%20of%20high-quality%0Aimages.%20To%20tackle%20these%20challenges%2C%20the%20generation%20of%20synthetic%20biometric%20data%0Ahas%20gained%20traction%20due%20to%20its%20ability%20to%20protect%20privacy%20while%20enabling%0Aeffective%20training%20of%20deep%20learning-based%20biometric%20verification%20methods.%20In%0Athis%20paper%2C%20we%20present%20a%20new%20framework%20to%20synthesize%20forehead-crease%20image%20data%0Awhile%20maintaining%20important%20features%2C%20such%20as%20uniqueness%20and%20realism.%20The%0Aproposed%20framework%20consists%20of%20two%20main%20modules%3A%20a%20Subject-Specific%20Generation%0AModule%20%28SSGM%29%2C%20based%20on%20an%20image-to-image%20Brownian%20Bridge%20Diffusion%20Model%0A%28BBDM%29%2C%20which%20learns%20a%20one-to-many%20mapping%20between%20image%20pairs%20to%20generate%0Aidentity-aware%20synthetic%20forehead%20creases%20corresponding%20to%20real%20subjects%2C%20and%20a%0ASubject-Agnostic%20Generation%20Module%20%28SAGM%29%2C%20which%20samples%20new%20synthetic%0Aidentities%20with%20assistance%20from%20the%20SSGM.%20We%20evaluate%20the%20diversity%20and%20realism%0Aof%20the%20generated%20forehead-crease%20images%20primarily%20using%20the%20Fr%5C%27echet%20Inception%0ADistance%20%28FID%29%20and%20the%20Structural%20Similarity%20Index%20Measure%20%28SSIM%29.%20In%20addition%2C%0Awe%20assess%20the%20utility%20of%20synthetically%20generated%20forehead-crease%20images%20using%20a%0Aforehead-crease%20verification%20system%20%28FHCVS%29.%20The%20results%20indicate%20an%0Aimprovement%20in%20the%20verification%20accuracy%20of%20the%20FHCVS%20by%20utilizing%20synthetic%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Forehead-creases%2520Biometric%2520Generation%2520for%2520Reliable%2520User%250A%2520%2520Verification%26entry.906535625%3DAbhishek%2520Tandon%2520and%2520Geetanjali%2520Sharma%2520and%2520Gaurav%2520Jaswal%2520and%2520Aditya%2520Nigam%2520and%2520Raghavendra%2520Ramachandra%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520emphasized%2520the%2520potential%2520of%2520forehead-crease%2520patterns%2520as%250Aan%2520alternative%2520for%2520face%252C%2520iris%252C%2520and%2520periocular%2520recognition%252C%2520presenting%250Acontactless%2520and%2520convenient%2520solutions%252C%2520particularly%2520in%2520situations%2520where%2520faces%250Aare%2520covered%2520by%2520surgical%2520masks.%2520However%252C%2520collecting%2520forehead%2520data%2520presents%250Achallenges%252C%2520including%2520cost%2520and%2520time%2520constraints%252C%2520as%2520developing%2520and%2520optimizing%250Aforehead%2520verification%2520methods%2520requires%2520a%2520substantial%2520number%2520of%2520high-quality%250Aimages.%2520To%2520tackle%2520these%2520challenges%252C%2520the%2520generation%2520of%2520synthetic%2520biometric%2520data%250Ahas%2520gained%2520traction%2520due%2520to%2520its%2520ability%2520to%2520protect%2520privacy%2520while%2520enabling%250Aeffective%2520training%2520of%2520deep%2520learning-based%2520biometric%2520verification%2520methods.%2520In%250Athis%2520paper%252C%2520we%2520present%2520a%2520new%2520framework%2520to%2520synthesize%2520forehead-crease%2520image%2520data%250Awhile%2520maintaining%2520important%2520features%252C%2520such%2520as%2520uniqueness%2520and%2520realism.%2520The%250Aproposed%2520framework%2520consists%2520of%2520two%2520main%2520modules%253A%2520a%2520Subject-Specific%2520Generation%250AModule%2520%2528SSGM%2529%252C%2520based%2520on%2520an%2520image-to-image%2520Brownian%2520Bridge%2520Diffusion%2520Model%250A%2528BBDM%2529%252C%2520which%2520learns%2520a%2520one-to-many%2520mapping%2520between%2520image%2520pairs%2520to%2520generate%250Aidentity-aware%2520synthetic%2520forehead%2520creases%2520corresponding%2520to%2520real%2520subjects%252C%2520and%2520a%250ASubject-Agnostic%2520Generation%2520Module%2520%2528SAGM%2529%252C%2520which%2520samples%2520new%2520synthetic%250Aidentities%2520with%2520assistance%2520from%2520the%2520SSGM.%2520We%2520evaluate%2520the%2520diversity%2520and%2520realism%250Aof%2520the%2520generated%2520forehead-crease%2520images%2520primarily%2520using%2520the%2520Fr%255C%2527echet%2520Inception%250ADistance%2520%2528FID%2529%2520and%2520the%2520Structural%2520Similarity%2520Index%2520Measure%2520%2528SSIM%2529.%2520In%2520addition%252C%250Awe%2520assess%2520the%2520utility%2520of%2520synthetically%2520generated%2520forehead-crease%2520images%2520using%2520a%250Aforehead-crease%2520verification%2520system%2520%2528FHCVS%2529.%2520The%2520results%2520indicate%2520an%250Aimprovement%2520in%2520the%2520verification%2520accuracy%2520of%2520the%2520FHCVS%2520by%2520utilizing%2520synthetic%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Forehead-creases%20Biometric%20Generation%20for%20Reliable%20User%0A%20%20Verification&entry.906535625=Abhishek%20Tandon%20and%20Geetanjali%20Sharma%20and%20Gaurav%20Jaswal%20and%20Aditya%20Nigam%20and%20Raghavendra%20Ramachandra&entry.1292438233=%20%20Recent%20studies%20have%20emphasized%20the%20potential%20of%20forehead-crease%20patterns%20as%0Aan%20alternative%20for%20face%2C%20iris%2C%20and%20periocular%20recognition%2C%20presenting%0Acontactless%20and%20convenient%20solutions%2C%20particularly%20in%20situations%20where%20faces%0Aare%20covered%20by%20surgical%20masks.%20However%2C%20collecting%20forehead%20data%20presents%0Achallenges%2C%20including%20cost%20and%20time%20constraints%2C%20as%20developing%20and%20optimizing%0Aforehead%20verification%20methods%20requires%20a%20substantial%20number%20of%20high-quality%0Aimages.%20To%20tackle%20these%20challenges%2C%20the%20generation%20of%20synthetic%20biometric%20data%0Ahas%20gained%20traction%20due%20to%20its%20ability%20to%20protect%20privacy%20while%20enabling%0Aeffective%20training%20of%20deep%20learning-based%20biometric%20verification%20methods.%20In%0Athis%20paper%2C%20we%20present%20a%20new%20framework%20to%20synthesize%20forehead-crease%20image%20data%0Awhile%20maintaining%20important%20features%2C%20such%20as%20uniqueness%20and%20realism.%20The%0Aproposed%20framework%20consists%20of%20two%20main%20modules%3A%20a%20Subject-Specific%20Generation%0AModule%20%28SSGM%29%2C%20based%20on%20an%20image-to-image%20Brownian%20Bridge%20Diffusion%20Model%0A%28BBDM%29%2C%20which%20learns%20a%20one-to-many%20mapping%20between%20image%20pairs%20to%20generate%0Aidentity-aware%20synthetic%20forehead%20creases%20corresponding%20to%20real%20subjects%2C%20and%20a%0ASubject-Agnostic%20Generation%20Module%20%28SAGM%29%2C%20which%20samples%20new%20synthetic%0Aidentities%20with%20assistance%20from%20the%20SSGM.%20We%20evaluate%20the%20diversity%20and%20realism%0Aof%20the%20generated%20forehead-crease%20images%20primarily%20using%20the%20Fr%5C%27echet%20Inception%0ADistance%20%28FID%29%20and%20the%20Structural%20Similarity%20Index%20Measure%20%28SSIM%29.%20In%20addition%2C%0Awe%20assess%20the%20utility%20of%20synthetically%20generated%20forehead-crease%20images%20using%20a%0Aforehead-crease%20verification%20system%20%28FHCVS%29.%20The%20results%20indicate%20an%0Aimprovement%20in%20the%20verification%20accuracy%20of%20the%20FHCVS%20by%20utilizing%20synthetic%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15693v1&entry.124074799=Read"},
{"title": "Benchmarking ML Approaches to UWB-Based Range-Only Posture Recognition\n  for Human Robot-Interaction", "author": "Salma Salimi and Sahar Salimpour and Jorge Pe\u00f1a Queralta and Wallace Moreira Bessa and Tomi Westerlund", "abstract": "  Human pose estimation involves detecting and tracking the positions of\nvarious body parts using input data from sources such as images, videos, or\nmotion and inertial sensors. This paper presents a novel approach to human pose\nestimation using machine learning algorithms to predict human posture and\ntranslate them into robot motion commands using ultra-wideband (UWB) nodes, as\nan alternative to motion sensors. The study utilizes five UWB sensors\nimplemented on the human body to enable the classification of still poses and\nmore robust posture recognition. This approach ensures effective posture\nrecognition across a variety of subjects. These range measurements serve as\ninput features for posture prediction models, which are implemented and\ncompared for accuracy. For this purpose, machine learning algorithms including\nK-Nearest Neighbors (KNN), Support Vector Machine (SVM), and deep Multi-Layer\nPerceptron (MLP) neural network are employed and compared in predicting\ncorresponding postures. We demonstrate the proposed approach for real-time\ncontrol of different mobile/aerial robots with inference implemented in a ROS 2\nnode. Experimental results demonstrate the efficacy of the approach, showcasing\nsuccessful prediction of human posture and corresponding robot movements with\nhigh accuracy.\n", "link": "http://arxiv.org/abs/2408.15717v1", "date": "2024-08-28", "relevancy": 2.1972, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6097}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5393}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20ML%20Approaches%20to%20UWB-Based%20Range-Only%20Posture%20Recognition%0A%20%20for%20Human%20Robot-Interaction&body=Title%3A%20Benchmarking%20ML%20Approaches%20to%20UWB-Based%20Range-Only%20Posture%20Recognition%0A%20%20for%20Human%20Robot-Interaction%0AAuthor%3A%20Salma%20Salimi%20and%20Sahar%20Salimpour%20and%20Jorge%20Pe%C3%B1a%20Queralta%20and%20Wallace%20Moreira%20Bessa%20and%20Tomi%20Westerlund%0AAbstract%3A%20%20%20Human%20pose%20estimation%20involves%20detecting%20and%20tracking%20the%20positions%20of%0Avarious%20body%20parts%20using%20input%20data%20from%20sources%20such%20as%20images%2C%20videos%2C%20or%0Amotion%20and%20inertial%20sensors.%20This%20paper%20presents%20a%20novel%20approach%20to%20human%20pose%0Aestimation%20using%20machine%20learning%20algorithms%20to%20predict%20human%20posture%20and%0Atranslate%20them%20into%20robot%20motion%20commands%20using%20ultra-wideband%20%28UWB%29%20nodes%2C%20as%0Aan%20alternative%20to%20motion%20sensors.%20The%20study%20utilizes%20five%20UWB%20sensors%0Aimplemented%20on%20the%20human%20body%20to%20enable%20the%20classification%20of%20still%20poses%20and%0Amore%20robust%20posture%20recognition.%20This%20approach%20ensures%20effective%20posture%0Arecognition%20across%20a%20variety%20of%20subjects.%20These%20range%20measurements%20serve%20as%0Ainput%20features%20for%20posture%20prediction%20models%2C%20which%20are%20implemented%20and%0Acompared%20for%20accuracy.%20For%20this%20purpose%2C%20machine%20learning%20algorithms%20including%0AK-Nearest%20Neighbors%20%28KNN%29%2C%20Support%20Vector%20Machine%20%28SVM%29%2C%20and%20deep%20Multi-Layer%0APerceptron%20%28MLP%29%20neural%20network%20are%20employed%20and%20compared%20in%20predicting%0Acorresponding%20postures.%20We%20demonstrate%20the%20proposed%20approach%20for%20real-time%0Acontrol%20of%20different%20mobile/aerial%20robots%20with%20inference%20implemented%20in%20a%20ROS%202%0Anode.%20Experimental%20results%20demonstrate%20the%20efficacy%20of%20the%20approach%2C%20showcasing%0Asuccessful%20prediction%20of%20human%20posture%20and%20corresponding%20robot%20movements%20with%0Ahigh%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15717v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520ML%2520Approaches%2520to%2520UWB-Based%2520Range-Only%2520Posture%2520Recognition%250A%2520%2520for%2520Human%2520Robot-Interaction%26entry.906535625%3DSalma%2520Salimi%2520and%2520Sahar%2520Salimpour%2520and%2520Jorge%2520Pe%25C3%25B1a%2520Queralta%2520and%2520Wallace%2520Moreira%2520Bessa%2520and%2520Tomi%2520Westerlund%26entry.1292438233%3D%2520%2520Human%2520pose%2520estimation%2520involves%2520detecting%2520and%2520tracking%2520the%2520positions%2520of%250Avarious%2520body%2520parts%2520using%2520input%2520data%2520from%2520sources%2520such%2520as%2520images%252C%2520videos%252C%2520or%250Amotion%2520and%2520inertial%2520sensors.%2520This%2520paper%2520presents%2520a%2520novel%2520approach%2520to%2520human%2520pose%250Aestimation%2520using%2520machine%2520learning%2520algorithms%2520to%2520predict%2520human%2520posture%2520and%250Atranslate%2520them%2520into%2520robot%2520motion%2520commands%2520using%2520ultra-wideband%2520%2528UWB%2529%2520nodes%252C%2520as%250Aan%2520alternative%2520to%2520motion%2520sensors.%2520The%2520study%2520utilizes%2520five%2520UWB%2520sensors%250Aimplemented%2520on%2520the%2520human%2520body%2520to%2520enable%2520the%2520classification%2520of%2520still%2520poses%2520and%250Amore%2520robust%2520posture%2520recognition.%2520This%2520approach%2520ensures%2520effective%2520posture%250Arecognition%2520across%2520a%2520variety%2520of%2520subjects.%2520These%2520range%2520measurements%2520serve%2520as%250Ainput%2520features%2520for%2520posture%2520prediction%2520models%252C%2520which%2520are%2520implemented%2520and%250Acompared%2520for%2520accuracy.%2520For%2520this%2520purpose%252C%2520machine%2520learning%2520algorithms%2520including%250AK-Nearest%2520Neighbors%2520%2528KNN%2529%252C%2520Support%2520Vector%2520Machine%2520%2528SVM%2529%252C%2520and%2520deep%2520Multi-Layer%250APerceptron%2520%2528MLP%2529%2520neural%2520network%2520are%2520employed%2520and%2520compared%2520in%2520predicting%250Acorresponding%2520postures.%2520We%2520demonstrate%2520the%2520proposed%2520approach%2520for%2520real-time%250Acontrol%2520of%2520different%2520mobile/aerial%2520robots%2520with%2520inference%2520implemented%2520in%2520a%2520ROS%25202%250Anode.%2520Experimental%2520results%2520demonstrate%2520the%2520efficacy%2520of%2520the%2520approach%252C%2520showcasing%250Asuccessful%2520prediction%2520of%2520human%2520posture%2520and%2520corresponding%2520robot%2520movements%2520with%250Ahigh%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15717v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20ML%20Approaches%20to%20UWB-Based%20Range-Only%20Posture%20Recognition%0A%20%20for%20Human%20Robot-Interaction&entry.906535625=Salma%20Salimi%20and%20Sahar%20Salimpour%20and%20Jorge%20Pe%C3%B1a%20Queralta%20and%20Wallace%20Moreira%20Bessa%20and%20Tomi%20Westerlund&entry.1292438233=%20%20Human%20pose%20estimation%20involves%20detecting%20and%20tracking%20the%20positions%20of%0Avarious%20body%20parts%20using%20input%20data%20from%20sources%20such%20as%20images%2C%20videos%2C%20or%0Amotion%20and%20inertial%20sensors.%20This%20paper%20presents%20a%20novel%20approach%20to%20human%20pose%0Aestimation%20using%20machine%20learning%20algorithms%20to%20predict%20human%20posture%20and%0Atranslate%20them%20into%20robot%20motion%20commands%20using%20ultra-wideband%20%28UWB%29%20nodes%2C%20as%0Aan%20alternative%20to%20motion%20sensors.%20The%20study%20utilizes%20five%20UWB%20sensors%0Aimplemented%20on%20the%20human%20body%20to%20enable%20the%20classification%20of%20still%20poses%20and%0Amore%20robust%20posture%20recognition.%20This%20approach%20ensures%20effective%20posture%0Arecognition%20across%20a%20variety%20of%20subjects.%20These%20range%20measurements%20serve%20as%0Ainput%20features%20for%20posture%20prediction%20models%2C%20which%20are%20implemented%20and%0Acompared%20for%20accuracy.%20For%20this%20purpose%2C%20machine%20learning%20algorithms%20including%0AK-Nearest%20Neighbors%20%28KNN%29%2C%20Support%20Vector%20Machine%20%28SVM%29%2C%20and%20deep%20Multi-Layer%0APerceptron%20%28MLP%29%20neural%20network%20are%20employed%20and%20compared%20in%20predicting%0Acorresponding%20postures.%20We%20demonstrate%20the%20proposed%20approach%20for%20real-time%0Acontrol%20of%20different%20mobile/aerial%20robots%20with%20inference%20implemented%20in%20a%20ROS%202%0Anode.%20Experimental%20results%20demonstrate%20the%20efficacy%20of%20the%20approach%2C%20showcasing%0Asuccessful%20prediction%20of%20human%20posture%20and%20corresponding%20robot%20movements%20with%0Ahigh%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15717v1&entry.124074799=Read"},
{"title": "Sigma Flows for Image and Data Labeling and Learning Structured\n  Prediction", "author": "Jonas Cassel and Bastian Boll and Stefania Petra and Peter Albers and Christoph Schn\u00f6rr", "abstract": "  This paper introduces the sigma flow model for the prediction of structured\nlabelings of data observed on Riemannian manifolds, including Euclidean image\ndomains as special case. The approach combines the Laplace-Beltrami framework\nfor image denoising and enhancement, introduced by Sochen, Kimmel and Malladi\nabout 25 years ago, and the assignment flow approach introduced and studied by\nthe authors.\n  The sigma flow arises as Riemannian gradient flow of generalized harmonic\nenergies and thus is governed by a nonlinear geometric PDE which determines a\nharmonic map from a closed Riemannian domain manifold to a statistical\nmanifold, equipped with the Fisher-Rao metric from information geometry. A\nspecific ingredient of the sigma flow is the mutual dependency of the\nRiemannian metric of the domain manifold on the evolving state. This makes the\napproach amenable to machine learning in a specific way, by realizing this\ndependency through a mapping with compact time-variant parametrization that can\nbe learned from data. Proof of concept experiments demonstrate the expressivity\nof the sigma flow model and prediction performance.\n  Structural similarities to transformer network architectures and networks\ngenerated by the geometric integration of sigma flows are pointed out, which\nhighlights the connection to deep learning and, conversely, may stimulate the\nuse of geometric design principles for structured prediction in other areas of\nscientific machine learning.\n", "link": "http://arxiv.org/abs/2408.15946v1", "date": "2024-08-28", "relevancy": 2.1918, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6253}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.56}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sigma%20Flows%20for%20Image%20and%20Data%20Labeling%20and%20Learning%20Structured%0A%20%20Prediction&body=Title%3A%20Sigma%20Flows%20for%20Image%20and%20Data%20Labeling%20and%20Learning%20Structured%0A%20%20Prediction%0AAuthor%3A%20Jonas%20Cassel%20and%20Bastian%20Boll%20and%20Stefania%20Petra%20and%20Peter%20Albers%20and%20Christoph%20Schn%C3%B6rr%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20sigma%20flow%20model%20for%20the%20prediction%20of%20structured%0Alabelings%20of%20data%20observed%20on%20Riemannian%20manifolds%2C%20including%20Euclidean%20image%0Adomains%20as%20special%20case.%20The%20approach%20combines%20the%20Laplace-Beltrami%20framework%0Afor%20image%20denoising%20and%20enhancement%2C%20introduced%20by%20Sochen%2C%20Kimmel%20and%20Malladi%0Aabout%2025%20years%20ago%2C%20and%20the%20assignment%20flow%20approach%20introduced%20and%20studied%20by%0Athe%20authors.%0A%20%20The%20sigma%20flow%20arises%20as%20Riemannian%20gradient%20flow%20of%20generalized%20harmonic%0Aenergies%20and%20thus%20is%20governed%20by%20a%20nonlinear%20geometric%20PDE%20which%20determines%20a%0Aharmonic%20map%20from%20a%20closed%20Riemannian%20domain%20manifold%20to%20a%20statistical%0Amanifold%2C%20equipped%20with%20the%20Fisher-Rao%20metric%20from%20information%20geometry.%20A%0Aspecific%20ingredient%20of%20the%20sigma%20flow%20is%20the%20mutual%20dependency%20of%20the%0ARiemannian%20metric%20of%20the%20domain%20manifold%20on%20the%20evolving%20state.%20This%20makes%20the%0Aapproach%20amenable%20to%20machine%20learning%20in%20a%20specific%20way%2C%20by%20realizing%20this%0Adependency%20through%20a%20mapping%20with%20compact%20time-variant%20parametrization%20that%20can%0Abe%20learned%20from%20data.%20Proof%20of%20concept%20experiments%20demonstrate%20the%20expressivity%0Aof%20the%20sigma%20flow%20model%20and%20prediction%20performance.%0A%20%20Structural%20similarities%20to%20transformer%20network%20architectures%20and%20networks%0Agenerated%20by%20the%20geometric%20integration%20of%20sigma%20flows%20are%20pointed%20out%2C%20which%0Ahighlights%20the%20connection%20to%20deep%20learning%20and%2C%20conversely%2C%20may%20stimulate%20the%0Ause%20of%20geometric%20design%20principles%20for%20structured%20prediction%20in%20other%20areas%20of%0Ascientific%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSigma%2520Flows%2520for%2520Image%2520and%2520Data%2520Labeling%2520and%2520Learning%2520Structured%250A%2520%2520Prediction%26entry.906535625%3DJonas%2520Cassel%2520and%2520Bastian%2520Boll%2520and%2520Stefania%2520Petra%2520and%2520Peter%2520Albers%2520and%2520Christoph%2520Schn%25C3%25B6rr%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520sigma%2520flow%2520model%2520for%2520the%2520prediction%2520of%2520structured%250Alabelings%2520of%2520data%2520observed%2520on%2520Riemannian%2520manifolds%252C%2520including%2520Euclidean%2520image%250Adomains%2520as%2520special%2520case.%2520The%2520approach%2520combines%2520the%2520Laplace-Beltrami%2520framework%250Afor%2520image%2520denoising%2520and%2520enhancement%252C%2520introduced%2520by%2520Sochen%252C%2520Kimmel%2520and%2520Malladi%250Aabout%252025%2520years%2520ago%252C%2520and%2520the%2520assignment%2520flow%2520approach%2520introduced%2520and%2520studied%2520by%250Athe%2520authors.%250A%2520%2520The%2520sigma%2520flow%2520arises%2520as%2520Riemannian%2520gradient%2520flow%2520of%2520generalized%2520harmonic%250Aenergies%2520and%2520thus%2520is%2520governed%2520by%2520a%2520nonlinear%2520geometric%2520PDE%2520which%2520determines%2520a%250Aharmonic%2520map%2520from%2520a%2520closed%2520Riemannian%2520domain%2520manifold%2520to%2520a%2520statistical%250Amanifold%252C%2520equipped%2520with%2520the%2520Fisher-Rao%2520metric%2520from%2520information%2520geometry.%2520A%250Aspecific%2520ingredient%2520of%2520the%2520sigma%2520flow%2520is%2520the%2520mutual%2520dependency%2520of%2520the%250ARiemannian%2520metric%2520of%2520the%2520domain%2520manifold%2520on%2520the%2520evolving%2520state.%2520This%2520makes%2520the%250Aapproach%2520amenable%2520to%2520machine%2520learning%2520in%2520a%2520specific%2520way%252C%2520by%2520realizing%2520this%250Adependency%2520through%2520a%2520mapping%2520with%2520compact%2520time-variant%2520parametrization%2520that%2520can%250Abe%2520learned%2520from%2520data.%2520Proof%2520of%2520concept%2520experiments%2520demonstrate%2520the%2520expressivity%250Aof%2520the%2520sigma%2520flow%2520model%2520and%2520prediction%2520performance.%250A%2520%2520Structural%2520similarities%2520to%2520transformer%2520network%2520architectures%2520and%2520networks%250Agenerated%2520by%2520the%2520geometric%2520integration%2520of%2520sigma%2520flows%2520are%2520pointed%2520out%252C%2520which%250Ahighlights%2520the%2520connection%2520to%2520deep%2520learning%2520and%252C%2520conversely%252C%2520may%2520stimulate%2520the%250Ause%2520of%2520geometric%2520design%2520principles%2520for%2520structured%2520prediction%2520in%2520other%2520areas%2520of%250Ascientific%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sigma%20Flows%20for%20Image%20and%20Data%20Labeling%20and%20Learning%20Structured%0A%20%20Prediction&entry.906535625=Jonas%20Cassel%20and%20Bastian%20Boll%20and%20Stefania%20Petra%20and%20Peter%20Albers%20and%20Christoph%20Schn%C3%B6rr&entry.1292438233=%20%20This%20paper%20introduces%20the%20sigma%20flow%20model%20for%20the%20prediction%20of%20structured%0Alabelings%20of%20data%20observed%20on%20Riemannian%20manifolds%2C%20including%20Euclidean%20image%0Adomains%20as%20special%20case.%20The%20approach%20combines%20the%20Laplace-Beltrami%20framework%0Afor%20image%20denoising%20and%20enhancement%2C%20introduced%20by%20Sochen%2C%20Kimmel%20and%20Malladi%0Aabout%2025%20years%20ago%2C%20and%20the%20assignment%20flow%20approach%20introduced%20and%20studied%20by%0Athe%20authors.%0A%20%20The%20sigma%20flow%20arises%20as%20Riemannian%20gradient%20flow%20of%20generalized%20harmonic%0Aenergies%20and%20thus%20is%20governed%20by%20a%20nonlinear%20geometric%20PDE%20which%20determines%20a%0Aharmonic%20map%20from%20a%20closed%20Riemannian%20domain%20manifold%20to%20a%20statistical%0Amanifold%2C%20equipped%20with%20the%20Fisher-Rao%20metric%20from%20information%20geometry.%20A%0Aspecific%20ingredient%20of%20the%20sigma%20flow%20is%20the%20mutual%20dependency%20of%20the%0ARiemannian%20metric%20of%20the%20domain%20manifold%20on%20the%20evolving%20state.%20This%20makes%20the%0Aapproach%20amenable%20to%20machine%20learning%20in%20a%20specific%20way%2C%20by%20realizing%20this%0Adependency%20through%20a%20mapping%20with%20compact%20time-variant%20parametrization%20that%20can%0Abe%20learned%20from%20data.%20Proof%20of%20concept%20experiments%20demonstrate%20the%20expressivity%0Aof%20the%20sigma%20flow%20model%20and%20prediction%20performance.%0A%20%20Structural%20similarities%20to%20transformer%20network%20architectures%20and%20networks%0Agenerated%20by%20the%20geometric%20integration%20of%20sigma%20flows%20are%20pointed%20out%2C%20which%0Ahighlights%20the%20connection%20to%20deep%20learning%20and%2C%20conversely%2C%20may%20stimulate%20the%0Ause%20of%20geometric%20design%20principles%20for%20structured%20prediction%20in%20other%20areas%20of%0Ascientific%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15946v1&entry.124074799=Read"},
{"title": "CoRe: Context-Regularized Text Embedding Learning for Text-to-Image\n  Personalization", "author": "Feize Wu and Yun Pang and Junyi Zhang and Lianyu Pang and Jian Yin and Baoquan Zhao and Qing Li and Xudong Mao", "abstract": "  Recent advances in text-to-image personalization have enabled high-quality\nand controllable image synthesis for user-provided concepts. However, existing\nmethods still struggle to balance identity preservation with text alignment.\nOur approach is based on the fact that generating prompt-aligned images\nrequires a precise semantic understanding of the prompt, which involves\naccurately processing the interactions between the new concept and its\nsurrounding context tokens within the CLIP text encoder. To address this, we\naim to embed the new concept properly into the input embedding space of the\ntext encoder, allowing for seamless integration with existing tokens. We\nintroduce Context Regularization (CoRe), which enhances the learning of the new\nconcept's text embedding by regularizing its context tokens in the prompt. This\nis based on the insight that appropriate output vectors of the text encoder for\nthe context tokens can only be achieved if the new concept's text embedding is\ncorrectly learned. CoRe can be applied to arbitrary prompts without requiring\nthe generation of corresponding images, thus improving the generalization of\nthe learned text embedding. Additionally, CoRe can serve as a test-time\noptimization technique to further enhance the generations for specific prompts.\nComprehensive experiments demonstrate that our method outperforms several\nbaseline methods in both identity preservation and text alignment. Code will be\nmade publicly available.\n", "link": "http://arxiv.org/abs/2408.15914v1", "date": "2024-08-28", "relevancy": 2.1834, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5593}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5368}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoRe%3A%20Context-Regularized%20Text%20Embedding%20Learning%20for%20Text-to-Image%0A%20%20Personalization&body=Title%3A%20CoRe%3A%20Context-Regularized%20Text%20Embedding%20Learning%20for%20Text-to-Image%0A%20%20Personalization%0AAuthor%3A%20Feize%20Wu%20and%20Yun%20Pang%20and%20Junyi%20Zhang%20and%20Lianyu%20Pang%20and%20Jian%20Yin%20and%20Baoquan%20Zhao%20and%20Qing%20Li%20and%20Xudong%20Mao%0AAbstract%3A%20%20%20Recent%20advances%20in%20text-to-image%20personalization%20have%20enabled%20high-quality%0Aand%20controllable%20image%20synthesis%20for%20user-provided%20concepts.%20However%2C%20existing%0Amethods%20still%20struggle%20to%20balance%20identity%20preservation%20with%20text%20alignment.%0AOur%20approach%20is%20based%20on%20the%20fact%20that%20generating%20prompt-aligned%20images%0Arequires%20a%20precise%20semantic%20understanding%20of%20the%20prompt%2C%20which%20involves%0Aaccurately%20processing%20the%20interactions%20between%20the%20new%20concept%20and%20its%0Asurrounding%20context%20tokens%20within%20the%20CLIP%20text%20encoder.%20To%20address%20this%2C%20we%0Aaim%20to%20embed%20the%20new%20concept%20properly%20into%20the%20input%20embedding%20space%20of%20the%0Atext%20encoder%2C%20allowing%20for%20seamless%20integration%20with%20existing%20tokens.%20We%0Aintroduce%20Context%20Regularization%20%28CoRe%29%2C%20which%20enhances%20the%20learning%20of%20the%20new%0Aconcept%27s%20text%20embedding%20by%20regularizing%20its%20context%20tokens%20in%20the%20prompt.%20This%0Ais%20based%20on%20the%20insight%20that%20appropriate%20output%20vectors%20of%20the%20text%20encoder%20for%0Athe%20context%20tokens%20can%20only%20be%20achieved%20if%20the%20new%20concept%27s%20text%20embedding%20is%0Acorrectly%20learned.%20CoRe%20can%20be%20applied%20to%20arbitrary%20prompts%20without%20requiring%0Athe%20generation%20of%20corresponding%20images%2C%20thus%20improving%20the%20generalization%20of%0Athe%20learned%20text%20embedding.%20Additionally%2C%20CoRe%20can%20serve%20as%20a%20test-time%0Aoptimization%20technique%20to%20further%20enhance%20the%20generations%20for%20specific%20prompts.%0AComprehensive%20experiments%20demonstrate%20that%20our%20method%20outperforms%20several%0Abaseline%20methods%20in%20both%20identity%20preservation%20and%20text%20alignment.%20Code%20will%20be%0Amade%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15914v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoRe%253A%2520Context-Regularized%2520Text%2520Embedding%2520Learning%2520for%2520Text-to-Image%250A%2520%2520Personalization%26entry.906535625%3DFeize%2520Wu%2520and%2520Yun%2520Pang%2520and%2520Junyi%2520Zhang%2520and%2520Lianyu%2520Pang%2520and%2520Jian%2520Yin%2520and%2520Baoquan%2520Zhao%2520and%2520Qing%2520Li%2520and%2520Xudong%2520Mao%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520text-to-image%2520personalization%2520have%2520enabled%2520high-quality%250Aand%2520controllable%2520image%2520synthesis%2520for%2520user-provided%2520concepts.%2520However%252C%2520existing%250Amethods%2520still%2520struggle%2520to%2520balance%2520identity%2520preservation%2520with%2520text%2520alignment.%250AOur%2520approach%2520is%2520based%2520on%2520the%2520fact%2520that%2520generating%2520prompt-aligned%2520images%250Arequires%2520a%2520precise%2520semantic%2520understanding%2520of%2520the%2520prompt%252C%2520which%2520involves%250Aaccurately%2520processing%2520the%2520interactions%2520between%2520the%2520new%2520concept%2520and%2520its%250Asurrounding%2520context%2520tokens%2520within%2520the%2520CLIP%2520text%2520encoder.%2520To%2520address%2520this%252C%2520we%250Aaim%2520to%2520embed%2520the%2520new%2520concept%2520properly%2520into%2520the%2520input%2520embedding%2520space%2520of%2520the%250Atext%2520encoder%252C%2520allowing%2520for%2520seamless%2520integration%2520with%2520existing%2520tokens.%2520We%250Aintroduce%2520Context%2520Regularization%2520%2528CoRe%2529%252C%2520which%2520enhances%2520the%2520learning%2520of%2520the%2520new%250Aconcept%2527s%2520text%2520embedding%2520by%2520regularizing%2520its%2520context%2520tokens%2520in%2520the%2520prompt.%2520This%250Ais%2520based%2520on%2520the%2520insight%2520that%2520appropriate%2520output%2520vectors%2520of%2520the%2520text%2520encoder%2520for%250Athe%2520context%2520tokens%2520can%2520only%2520be%2520achieved%2520if%2520the%2520new%2520concept%2527s%2520text%2520embedding%2520is%250Acorrectly%2520learned.%2520CoRe%2520can%2520be%2520applied%2520to%2520arbitrary%2520prompts%2520without%2520requiring%250Athe%2520generation%2520of%2520corresponding%2520images%252C%2520thus%2520improving%2520the%2520generalization%2520of%250Athe%2520learned%2520text%2520embedding.%2520Additionally%252C%2520CoRe%2520can%2520serve%2520as%2520a%2520test-time%250Aoptimization%2520technique%2520to%2520further%2520enhance%2520the%2520generations%2520for%2520specific%2520prompts.%250AComprehensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520several%250Abaseline%2520methods%2520in%2520both%2520identity%2520preservation%2520and%2520text%2520alignment.%2520Code%2520will%2520be%250Amade%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15914v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoRe%3A%20Context-Regularized%20Text%20Embedding%20Learning%20for%20Text-to-Image%0A%20%20Personalization&entry.906535625=Feize%20Wu%20and%20Yun%20Pang%20and%20Junyi%20Zhang%20and%20Lianyu%20Pang%20and%20Jian%20Yin%20and%20Baoquan%20Zhao%20and%20Qing%20Li%20and%20Xudong%20Mao&entry.1292438233=%20%20Recent%20advances%20in%20text-to-image%20personalization%20have%20enabled%20high-quality%0Aand%20controllable%20image%20synthesis%20for%20user-provided%20concepts.%20However%2C%20existing%0Amethods%20still%20struggle%20to%20balance%20identity%20preservation%20with%20text%20alignment.%0AOur%20approach%20is%20based%20on%20the%20fact%20that%20generating%20prompt-aligned%20images%0Arequires%20a%20precise%20semantic%20understanding%20of%20the%20prompt%2C%20which%20involves%0Aaccurately%20processing%20the%20interactions%20between%20the%20new%20concept%20and%20its%0Asurrounding%20context%20tokens%20within%20the%20CLIP%20text%20encoder.%20To%20address%20this%2C%20we%0Aaim%20to%20embed%20the%20new%20concept%20properly%20into%20the%20input%20embedding%20space%20of%20the%0Atext%20encoder%2C%20allowing%20for%20seamless%20integration%20with%20existing%20tokens.%20We%0Aintroduce%20Context%20Regularization%20%28CoRe%29%2C%20which%20enhances%20the%20learning%20of%20the%20new%0Aconcept%27s%20text%20embedding%20by%20regularizing%20its%20context%20tokens%20in%20the%20prompt.%20This%0Ais%20based%20on%20the%20insight%20that%20appropriate%20output%20vectors%20of%20the%20text%20encoder%20for%0Athe%20context%20tokens%20can%20only%20be%20achieved%20if%20the%20new%20concept%27s%20text%20embedding%20is%0Acorrectly%20learned.%20CoRe%20can%20be%20applied%20to%20arbitrary%20prompts%20without%20requiring%0Athe%20generation%20of%20corresponding%20images%2C%20thus%20improving%20the%20generalization%20of%0Athe%20learned%20text%20embedding.%20Additionally%2C%20CoRe%20can%20serve%20as%20a%20test-time%0Aoptimization%20technique%20to%20further%20enhance%20the%20generations%20for%20specific%20prompts.%0AComprehensive%20experiments%20demonstrate%20that%20our%20method%20outperforms%20several%0Abaseline%20methods%20in%20both%20identity%20preservation%20and%20text%20alignment.%20Code%20will%20be%0Amade%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15914v1&entry.124074799=Read"},
{"title": "Unleashing the Temporal-Spatial Reasoning Capacity of GPT for\n  Training-Free Audio and Language Referenced Video Object Segmentation", "author": "Shaofei Huang and Rui Ling and Hongyu Li and Tianrui Hui and Zongheng Tang and Xiaoming Wei and Jizhong Han and Si Liu", "abstract": "  In this paper, we propose an Audio-Language-Referenced SAM 2 (AL-Ref-SAM 2)\npipeline to explore the training-free paradigm for audio and\nlanguage-referenced video object segmentation, namely AVS and RVOS tasks. The\nintuitive solution leverages GroundingDINO to identify the target object from a\nsingle frame and SAM 2 to segment the identified object throughout the video,\nwhich is less robust to spatiotemporal variations due to a lack of video\ncontext exploration. Thus, in our AL-Ref-SAM 2 pipeline, we propose a novel\nGPT-assisted Pivot Selection (GPT-PS) module to instruct GPT-4 to perform\ntwo-step temporal-spatial reasoning for sequentially selecting pivot frames and\npivot boxes, thereby providing SAM 2 with a high-quality initial object prompt.\nWithin GPT-PS, two task-specific Chain-of-Thought prompts are designed to\nunleash GPT's temporal-spatial reasoning capacity by guiding GPT to make\nselections based on a comprehensive understanding of video and reference\ninformation. Furthermore, we propose a Language-Binded Reference Unification\n(LBRU) module to convert audio signals into language-formatted references,\nthereby unifying the formats of AVS and RVOS tasks in the same pipeline.\nExtensive experiments on both tasks show that our training-free AL-Ref-SAM 2\npipeline achieves performances comparable to or even better than\nfully-supervised fine-tuning methods. The code is available at:\nhttps://github.com/appletea233/AL-Ref-SAM2.\n", "link": "http://arxiv.org/abs/2408.15876v1", "date": "2024-08-28", "relevancy": 2.1832, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5638}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5341}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.53}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20the%20Temporal-Spatial%20Reasoning%20Capacity%20of%20GPT%20for%0A%20%20Training-Free%20Audio%20and%20Language%20Referenced%20Video%20Object%20Segmentation&body=Title%3A%20Unleashing%20the%20Temporal-Spatial%20Reasoning%20Capacity%20of%20GPT%20for%0A%20%20Training-Free%20Audio%20and%20Language%20Referenced%20Video%20Object%20Segmentation%0AAuthor%3A%20Shaofei%20Huang%20and%20Rui%20Ling%20and%20Hongyu%20Li%20and%20Tianrui%20Hui%20and%20Zongheng%20Tang%20and%20Xiaoming%20Wei%20and%20Jizhong%20Han%20and%20Si%20Liu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20an%20Audio-Language-Referenced%20SAM%202%20%28AL-Ref-SAM%202%29%0Apipeline%20to%20explore%20the%20training-free%20paradigm%20for%20audio%20and%0Alanguage-referenced%20video%20object%20segmentation%2C%20namely%20AVS%20and%20RVOS%20tasks.%20The%0Aintuitive%20solution%20leverages%20GroundingDINO%20to%20identify%20the%20target%20object%20from%20a%0Asingle%20frame%20and%20SAM%202%20to%20segment%20the%20identified%20object%20throughout%20the%20video%2C%0Awhich%20is%20less%20robust%20to%20spatiotemporal%20variations%20due%20to%20a%20lack%20of%20video%0Acontext%20exploration.%20Thus%2C%20in%20our%20AL-Ref-SAM%202%20pipeline%2C%20we%20propose%20a%20novel%0AGPT-assisted%20Pivot%20Selection%20%28GPT-PS%29%20module%20to%20instruct%20GPT-4%20to%20perform%0Atwo-step%20temporal-spatial%20reasoning%20for%20sequentially%20selecting%20pivot%20frames%20and%0Apivot%20boxes%2C%20thereby%20providing%20SAM%202%20with%20a%20high-quality%20initial%20object%20prompt.%0AWithin%20GPT-PS%2C%20two%20task-specific%20Chain-of-Thought%20prompts%20are%20designed%20to%0Aunleash%20GPT%27s%20temporal-spatial%20reasoning%20capacity%20by%20guiding%20GPT%20to%20make%0Aselections%20based%20on%20a%20comprehensive%20understanding%20of%20video%20and%20reference%0Ainformation.%20Furthermore%2C%20we%20propose%20a%20Language-Binded%20Reference%20Unification%0A%28LBRU%29%20module%20to%20convert%20audio%20signals%20into%20language-formatted%20references%2C%0Athereby%20unifying%20the%20formats%20of%20AVS%20and%20RVOS%20tasks%20in%20the%20same%20pipeline.%0AExtensive%20experiments%20on%20both%20tasks%20show%20that%20our%20training-free%20AL-Ref-SAM%202%0Apipeline%20achieves%20performances%20comparable%20to%20or%20even%20better%20than%0Afully-supervised%20fine-tuning%20methods.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/appletea233/AL-Ref-SAM2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15876v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520the%2520Temporal-Spatial%2520Reasoning%2520Capacity%2520of%2520GPT%2520for%250A%2520%2520Training-Free%2520Audio%2520and%2520Language%2520Referenced%2520Video%2520Object%2520Segmentation%26entry.906535625%3DShaofei%2520Huang%2520and%2520Rui%2520Ling%2520and%2520Hongyu%2520Li%2520and%2520Tianrui%2520Hui%2520and%2520Zongheng%2520Tang%2520and%2520Xiaoming%2520Wei%2520and%2520Jizhong%2520Han%2520and%2520Si%2520Liu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520Audio-Language-Referenced%2520SAM%25202%2520%2528AL-Ref-SAM%25202%2529%250Apipeline%2520to%2520explore%2520the%2520training-free%2520paradigm%2520for%2520audio%2520and%250Alanguage-referenced%2520video%2520object%2520segmentation%252C%2520namely%2520AVS%2520and%2520RVOS%2520tasks.%2520The%250Aintuitive%2520solution%2520leverages%2520GroundingDINO%2520to%2520identify%2520the%2520target%2520object%2520from%2520a%250Asingle%2520frame%2520and%2520SAM%25202%2520to%2520segment%2520the%2520identified%2520object%2520throughout%2520the%2520video%252C%250Awhich%2520is%2520less%2520robust%2520to%2520spatiotemporal%2520variations%2520due%2520to%2520a%2520lack%2520of%2520video%250Acontext%2520exploration.%2520Thus%252C%2520in%2520our%2520AL-Ref-SAM%25202%2520pipeline%252C%2520we%2520propose%2520a%2520novel%250AGPT-assisted%2520Pivot%2520Selection%2520%2528GPT-PS%2529%2520module%2520to%2520instruct%2520GPT-4%2520to%2520perform%250Atwo-step%2520temporal-spatial%2520reasoning%2520for%2520sequentially%2520selecting%2520pivot%2520frames%2520and%250Apivot%2520boxes%252C%2520thereby%2520providing%2520SAM%25202%2520with%2520a%2520high-quality%2520initial%2520object%2520prompt.%250AWithin%2520GPT-PS%252C%2520two%2520task-specific%2520Chain-of-Thought%2520prompts%2520are%2520designed%2520to%250Aunleash%2520GPT%2527s%2520temporal-spatial%2520reasoning%2520capacity%2520by%2520guiding%2520GPT%2520to%2520make%250Aselections%2520based%2520on%2520a%2520comprehensive%2520understanding%2520of%2520video%2520and%2520reference%250Ainformation.%2520Furthermore%252C%2520we%2520propose%2520a%2520Language-Binded%2520Reference%2520Unification%250A%2528LBRU%2529%2520module%2520to%2520convert%2520audio%2520signals%2520into%2520language-formatted%2520references%252C%250Athereby%2520unifying%2520the%2520formats%2520of%2520AVS%2520and%2520RVOS%2520tasks%2520in%2520the%2520same%2520pipeline.%250AExtensive%2520experiments%2520on%2520both%2520tasks%2520show%2520that%2520our%2520training-free%2520AL-Ref-SAM%25202%250Apipeline%2520achieves%2520performances%2520comparable%2520to%2520or%2520even%2520better%2520than%250Afully-supervised%2520fine-tuning%2520methods.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/appletea233/AL-Ref-SAM2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15876v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20the%20Temporal-Spatial%20Reasoning%20Capacity%20of%20GPT%20for%0A%20%20Training-Free%20Audio%20and%20Language%20Referenced%20Video%20Object%20Segmentation&entry.906535625=Shaofei%20Huang%20and%20Rui%20Ling%20and%20Hongyu%20Li%20and%20Tianrui%20Hui%20and%20Zongheng%20Tang%20and%20Xiaoming%20Wei%20and%20Jizhong%20Han%20and%20Si%20Liu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20an%20Audio-Language-Referenced%20SAM%202%20%28AL-Ref-SAM%202%29%0Apipeline%20to%20explore%20the%20training-free%20paradigm%20for%20audio%20and%0Alanguage-referenced%20video%20object%20segmentation%2C%20namely%20AVS%20and%20RVOS%20tasks.%20The%0Aintuitive%20solution%20leverages%20GroundingDINO%20to%20identify%20the%20target%20object%20from%20a%0Asingle%20frame%20and%20SAM%202%20to%20segment%20the%20identified%20object%20throughout%20the%20video%2C%0Awhich%20is%20less%20robust%20to%20spatiotemporal%20variations%20due%20to%20a%20lack%20of%20video%0Acontext%20exploration.%20Thus%2C%20in%20our%20AL-Ref-SAM%202%20pipeline%2C%20we%20propose%20a%20novel%0AGPT-assisted%20Pivot%20Selection%20%28GPT-PS%29%20module%20to%20instruct%20GPT-4%20to%20perform%0Atwo-step%20temporal-spatial%20reasoning%20for%20sequentially%20selecting%20pivot%20frames%20and%0Apivot%20boxes%2C%20thereby%20providing%20SAM%202%20with%20a%20high-quality%20initial%20object%20prompt.%0AWithin%20GPT-PS%2C%20two%20task-specific%20Chain-of-Thought%20prompts%20are%20designed%20to%0Aunleash%20GPT%27s%20temporal-spatial%20reasoning%20capacity%20by%20guiding%20GPT%20to%20make%0Aselections%20based%20on%20a%20comprehensive%20understanding%20of%20video%20and%20reference%0Ainformation.%20Furthermore%2C%20we%20propose%20a%20Language-Binded%20Reference%20Unification%0A%28LBRU%29%20module%20to%20convert%20audio%20signals%20into%20language-formatted%20references%2C%0Athereby%20unifying%20the%20formats%20of%20AVS%20and%20RVOS%20tasks%20in%20the%20same%20pipeline.%0AExtensive%20experiments%20on%20both%20tasks%20show%20that%20our%20training-free%20AL-Ref-SAM%202%0Apipeline%20achieves%20performances%20comparable%20to%20or%20even%20better%20than%0Afully-supervised%20fine-tuning%20methods.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/appletea233/AL-Ref-SAM2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15876v1&entry.124074799=Read"},
{"title": "Provable Probabilistic Imaging using Score-Based Generative Priors", "author": "Yu Sun and Zihui Wu and Yifan Chen and Berthy T. Feng and Katherine L. Bouman", "abstract": "  Estimating high-quality images while also quantifying their uncertainty are\ntwo desired features in an image reconstruction algorithm for solving ill-posed\ninverse problems. In this paper, we propose plug-and-play Monte Carlo (PMC) as\na principled framework for characterizing the space of possible solutions to a\ngeneral inverse problem. PMC is able to incorporate expressive score-based\ngenerative priors for high-quality image reconstruction while also performing\nuncertainty quantification via posterior sampling. In particular, we develop\ntwo PMC algorithms that can be viewed as the sampling analogues of the\ntraditional plug-and-play priors (PnP) and regularization by denoising (RED)\nalgorithms. To improve the sampling efficiency, we introduce weighted annealing\ninto these PMC algorithms, further developing two additional annealed PMC\nalgorithms (APMC). We establish a theoretical analysis for characterizing the\nconvergence behavior of PMC algorithms. Our analysis provides non-asymptotic\nstationarity guarantees in terms of the Fisher information, fully compatible\nwith the joint presence of weighted annealing, potentially non-log-concave\nlikelihoods, and imperfect score networks. We demonstrate the performance of\nthe PMC algorithms on multiple representative inverse problems with both linear\nand nonlinear forward models. Experimental results show that PMC significantly\nimproves reconstruction quality and enables high-fidelity uncertainty\nquantification.\n", "link": "http://arxiv.org/abs/2310.10835v3", "date": "2024-08-28", "relevancy": 2.1483, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5596}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.525}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provable%20Probabilistic%20Imaging%20using%20Score-Based%20Generative%20Priors&body=Title%3A%20Provable%20Probabilistic%20Imaging%20using%20Score-Based%20Generative%20Priors%0AAuthor%3A%20Yu%20Sun%20and%20Zihui%20Wu%20and%20Yifan%20Chen%20and%20Berthy%20T.%20Feng%20and%20Katherine%20L.%20Bouman%0AAbstract%3A%20%20%20Estimating%20high-quality%20images%20while%20also%20quantifying%20their%20uncertainty%20are%0Atwo%20desired%20features%20in%20an%20image%20reconstruction%20algorithm%20for%20solving%20ill-posed%0Ainverse%20problems.%20In%20this%20paper%2C%20we%20propose%20plug-and-play%20Monte%20Carlo%20%28PMC%29%20as%0Aa%20principled%20framework%20for%20characterizing%20the%20space%20of%20possible%20solutions%20to%20a%0Ageneral%20inverse%20problem.%20PMC%20is%20able%20to%20incorporate%20expressive%20score-based%0Agenerative%20priors%20for%20high-quality%20image%20reconstruction%20while%20also%20performing%0Auncertainty%20quantification%20via%20posterior%20sampling.%20In%20particular%2C%20we%20develop%0Atwo%20PMC%20algorithms%20that%20can%20be%20viewed%20as%20the%20sampling%20analogues%20of%20the%0Atraditional%20plug-and-play%20priors%20%28PnP%29%20and%20regularization%20by%20denoising%20%28RED%29%0Aalgorithms.%20To%20improve%20the%20sampling%20efficiency%2C%20we%20introduce%20weighted%20annealing%0Ainto%20these%20PMC%20algorithms%2C%20further%20developing%20two%20additional%20annealed%20PMC%0Aalgorithms%20%28APMC%29.%20We%20establish%20a%20theoretical%20analysis%20for%20characterizing%20the%0Aconvergence%20behavior%20of%20PMC%20algorithms.%20Our%20analysis%20provides%20non-asymptotic%0Astationarity%20guarantees%20in%20terms%20of%20the%20Fisher%20information%2C%20fully%20compatible%0Awith%20the%20joint%20presence%20of%20weighted%20annealing%2C%20potentially%20non-log-concave%0Alikelihoods%2C%20and%20imperfect%20score%20networks.%20We%20demonstrate%20the%20performance%20of%0Athe%20PMC%20algorithms%20on%20multiple%20representative%20inverse%20problems%20with%20both%20linear%0Aand%20nonlinear%20forward%20models.%20Experimental%20results%20show%20that%20PMC%20significantly%0Aimproves%20reconstruction%20quality%20and%20enables%20high-fidelity%20uncertainty%0Aquantification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.10835v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvable%2520Probabilistic%2520Imaging%2520using%2520Score-Based%2520Generative%2520Priors%26entry.906535625%3DYu%2520Sun%2520and%2520Zihui%2520Wu%2520and%2520Yifan%2520Chen%2520and%2520Berthy%2520T.%2520Feng%2520and%2520Katherine%2520L.%2520Bouman%26entry.1292438233%3D%2520%2520Estimating%2520high-quality%2520images%2520while%2520also%2520quantifying%2520their%2520uncertainty%2520are%250Atwo%2520desired%2520features%2520in%2520an%2520image%2520reconstruction%2520algorithm%2520for%2520solving%2520ill-posed%250Ainverse%2520problems.%2520In%2520this%2520paper%252C%2520we%2520propose%2520plug-and-play%2520Monte%2520Carlo%2520%2528PMC%2529%2520as%250Aa%2520principled%2520framework%2520for%2520characterizing%2520the%2520space%2520of%2520possible%2520solutions%2520to%2520a%250Ageneral%2520inverse%2520problem.%2520PMC%2520is%2520able%2520to%2520incorporate%2520expressive%2520score-based%250Agenerative%2520priors%2520for%2520high-quality%2520image%2520reconstruction%2520while%2520also%2520performing%250Auncertainty%2520quantification%2520via%2520posterior%2520sampling.%2520In%2520particular%252C%2520we%2520develop%250Atwo%2520PMC%2520algorithms%2520that%2520can%2520be%2520viewed%2520as%2520the%2520sampling%2520analogues%2520of%2520the%250Atraditional%2520plug-and-play%2520priors%2520%2528PnP%2529%2520and%2520regularization%2520by%2520denoising%2520%2528RED%2529%250Aalgorithms.%2520To%2520improve%2520the%2520sampling%2520efficiency%252C%2520we%2520introduce%2520weighted%2520annealing%250Ainto%2520these%2520PMC%2520algorithms%252C%2520further%2520developing%2520two%2520additional%2520annealed%2520PMC%250Aalgorithms%2520%2528APMC%2529.%2520We%2520establish%2520a%2520theoretical%2520analysis%2520for%2520characterizing%2520the%250Aconvergence%2520behavior%2520of%2520PMC%2520algorithms.%2520Our%2520analysis%2520provides%2520non-asymptotic%250Astationarity%2520guarantees%2520in%2520terms%2520of%2520the%2520Fisher%2520information%252C%2520fully%2520compatible%250Awith%2520the%2520joint%2520presence%2520of%2520weighted%2520annealing%252C%2520potentially%2520non-log-concave%250Alikelihoods%252C%2520and%2520imperfect%2520score%2520networks.%2520We%2520demonstrate%2520the%2520performance%2520of%250Athe%2520PMC%2520algorithms%2520on%2520multiple%2520representative%2520inverse%2520problems%2520with%2520both%2520linear%250Aand%2520nonlinear%2520forward%2520models.%2520Experimental%2520results%2520show%2520that%2520PMC%2520significantly%250Aimproves%2520reconstruction%2520quality%2520and%2520enables%2520high-fidelity%2520uncertainty%250Aquantification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.10835v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provable%20Probabilistic%20Imaging%20using%20Score-Based%20Generative%20Priors&entry.906535625=Yu%20Sun%20and%20Zihui%20Wu%20and%20Yifan%20Chen%20and%20Berthy%20T.%20Feng%20and%20Katherine%20L.%20Bouman&entry.1292438233=%20%20Estimating%20high-quality%20images%20while%20also%20quantifying%20their%20uncertainty%20are%0Atwo%20desired%20features%20in%20an%20image%20reconstruction%20algorithm%20for%20solving%20ill-posed%0Ainverse%20problems.%20In%20this%20paper%2C%20we%20propose%20plug-and-play%20Monte%20Carlo%20%28PMC%29%20as%0Aa%20principled%20framework%20for%20characterizing%20the%20space%20of%20possible%20solutions%20to%20a%0Ageneral%20inverse%20problem.%20PMC%20is%20able%20to%20incorporate%20expressive%20score-based%0Agenerative%20priors%20for%20high-quality%20image%20reconstruction%20while%20also%20performing%0Auncertainty%20quantification%20via%20posterior%20sampling.%20In%20particular%2C%20we%20develop%0Atwo%20PMC%20algorithms%20that%20can%20be%20viewed%20as%20the%20sampling%20analogues%20of%20the%0Atraditional%20plug-and-play%20priors%20%28PnP%29%20and%20regularization%20by%20denoising%20%28RED%29%0Aalgorithms.%20To%20improve%20the%20sampling%20efficiency%2C%20we%20introduce%20weighted%20annealing%0Ainto%20these%20PMC%20algorithms%2C%20further%20developing%20two%20additional%20annealed%20PMC%0Aalgorithms%20%28APMC%29.%20We%20establish%20a%20theoretical%20analysis%20for%20characterizing%20the%0Aconvergence%20behavior%20of%20PMC%20algorithms.%20Our%20analysis%20provides%20non-asymptotic%0Astationarity%20guarantees%20in%20terms%20of%20the%20Fisher%20information%2C%20fully%20compatible%0Awith%20the%20joint%20presence%20of%20weighted%20annealing%2C%20potentially%20non-log-concave%0Alikelihoods%2C%20and%20imperfect%20score%20networks.%20We%20demonstrate%20the%20performance%20of%0Athe%20PMC%20algorithms%20on%20multiple%20representative%20inverse%20problems%20with%20both%20linear%0Aand%20nonlinear%20forward%20models.%20Experimental%20results%20show%20that%20PMC%20significantly%0Aimproves%20reconstruction%20quality%20and%20enables%20high-fidelity%20uncertainty%0Aquantification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10835v3&entry.124074799=Read"},
{"title": "A Platform-Agnostic Deep Reinforcement Learning Framework for Effective\n  Sim2Real Transfer towards Autonomous Driving", "author": "Dianzhao Li and Ostap Okhrin", "abstract": "  Deep Reinforcement Learning (DRL) has shown remarkable success in solving\ncomplex tasks across various research fields. However, transferring DRL agents\nto the real world is still challenging due to the significant discrepancies\nbetween simulation and reality. To address this issue, we propose a robust DRL\nframework that leverages platform-dependent perception modules to extract\ntask-relevant information and train a lane-following and overtaking agent in\nsimulation. This framework facilitates the seamless transfer of the DRL agent\nto new simulated environments and the real world with minimal effort. We\nevaluate the performance of the agent in various driving scenarios in both\nsimulation and the real world, and compare it to human players and the PID\nbaseline in simulation. Our proposed framework significantly reduces the gaps\nbetween different platforms and the Sim2Real gap, enabling the trained agent to\nachieve similar performance in both simulation and the real world, driving the\nvehicle effectively.\n", "link": "http://arxiv.org/abs/2304.08235v3", "date": "2024-08-28", "relevancy": 2.1384, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5574}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5524}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Platform-Agnostic%20Deep%20Reinforcement%20Learning%20Framework%20for%20Effective%0A%20%20Sim2Real%20Transfer%20towards%20Autonomous%20Driving&body=Title%3A%20A%20Platform-Agnostic%20Deep%20Reinforcement%20Learning%20Framework%20for%20Effective%0A%20%20Sim2Real%20Transfer%20towards%20Autonomous%20Driving%0AAuthor%3A%20Dianzhao%20Li%20and%20Ostap%20Okhrin%0AAbstract%3A%20%20%20Deep%20Reinforcement%20Learning%20%28DRL%29%20has%20shown%20remarkable%20success%20in%20solving%0Acomplex%20tasks%20across%20various%20research%20fields.%20However%2C%20transferring%20DRL%20agents%0Ato%20the%20real%20world%20is%20still%20challenging%20due%20to%20the%20significant%20discrepancies%0Abetween%20simulation%20and%20reality.%20To%20address%20this%20issue%2C%20we%20propose%20a%20robust%20DRL%0Aframework%20that%20leverages%20platform-dependent%20perception%20modules%20to%20extract%0Atask-relevant%20information%20and%20train%20a%20lane-following%20and%20overtaking%20agent%20in%0Asimulation.%20This%20framework%20facilitates%20the%20seamless%20transfer%20of%20the%20DRL%20agent%0Ato%20new%20simulated%20environments%20and%20the%20real%20world%20with%20minimal%20effort.%20We%0Aevaluate%20the%20performance%20of%20the%20agent%20in%20various%20driving%20scenarios%20in%20both%0Asimulation%20and%20the%20real%20world%2C%20and%20compare%20it%20to%20human%20players%20and%20the%20PID%0Abaseline%20in%20simulation.%20Our%20proposed%20framework%20significantly%20reduces%20the%20gaps%0Abetween%20different%20platforms%20and%20the%20Sim2Real%20gap%2C%20enabling%20the%20trained%20agent%20to%0Aachieve%20similar%20performance%20in%20both%20simulation%20and%20the%20real%20world%2C%20driving%20the%0Avehicle%20effectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.08235v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Platform-Agnostic%2520Deep%2520Reinforcement%2520Learning%2520Framework%2520for%2520Effective%250A%2520%2520Sim2Real%2520Transfer%2520towards%2520Autonomous%2520Driving%26entry.906535625%3DDianzhao%2520Li%2520and%2520Ostap%2520Okhrin%26entry.1292438233%3D%2520%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520has%2520shown%2520remarkable%2520success%2520in%2520solving%250Acomplex%2520tasks%2520across%2520various%2520research%2520fields.%2520However%252C%2520transferring%2520DRL%2520agents%250Ato%2520the%2520real%2520world%2520is%2520still%2520challenging%2520due%2520to%2520the%2520significant%2520discrepancies%250Abetween%2520simulation%2520and%2520reality.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520robust%2520DRL%250Aframework%2520that%2520leverages%2520platform-dependent%2520perception%2520modules%2520to%2520extract%250Atask-relevant%2520information%2520and%2520train%2520a%2520lane-following%2520and%2520overtaking%2520agent%2520in%250Asimulation.%2520This%2520framework%2520facilitates%2520the%2520seamless%2520transfer%2520of%2520the%2520DRL%2520agent%250Ato%2520new%2520simulated%2520environments%2520and%2520the%2520real%2520world%2520with%2520minimal%2520effort.%2520We%250Aevaluate%2520the%2520performance%2520of%2520the%2520agent%2520in%2520various%2520driving%2520scenarios%2520in%2520both%250Asimulation%2520and%2520the%2520real%2520world%252C%2520and%2520compare%2520it%2520to%2520human%2520players%2520and%2520the%2520PID%250Abaseline%2520in%2520simulation.%2520Our%2520proposed%2520framework%2520significantly%2520reduces%2520the%2520gaps%250Abetween%2520different%2520platforms%2520and%2520the%2520Sim2Real%2520gap%252C%2520enabling%2520the%2520trained%2520agent%2520to%250Aachieve%2520similar%2520performance%2520in%2520both%2520simulation%2520and%2520the%2520real%2520world%252C%2520driving%2520the%250Avehicle%2520effectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.08235v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Platform-Agnostic%20Deep%20Reinforcement%20Learning%20Framework%20for%20Effective%0A%20%20Sim2Real%20Transfer%20towards%20Autonomous%20Driving&entry.906535625=Dianzhao%20Li%20and%20Ostap%20Okhrin&entry.1292438233=%20%20Deep%20Reinforcement%20Learning%20%28DRL%29%20has%20shown%20remarkable%20success%20in%20solving%0Acomplex%20tasks%20across%20various%20research%20fields.%20However%2C%20transferring%20DRL%20agents%0Ato%20the%20real%20world%20is%20still%20challenging%20due%20to%20the%20significant%20discrepancies%0Abetween%20simulation%20and%20reality.%20To%20address%20this%20issue%2C%20we%20propose%20a%20robust%20DRL%0Aframework%20that%20leverages%20platform-dependent%20perception%20modules%20to%20extract%0Atask-relevant%20information%20and%20train%20a%20lane-following%20and%20overtaking%20agent%20in%0Asimulation.%20This%20framework%20facilitates%20the%20seamless%20transfer%20of%20the%20DRL%20agent%0Ato%20new%20simulated%20environments%20and%20the%20real%20world%20with%20minimal%20effort.%20We%0Aevaluate%20the%20performance%20of%20the%20agent%20in%20various%20driving%20scenarios%20in%20both%0Asimulation%20and%20the%20real%20world%2C%20and%20compare%20it%20to%20human%20players%20and%20the%20PID%0Abaseline%20in%20simulation.%20Our%20proposed%20framework%20significantly%20reduces%20the%20gaps%0Abetween%20different%20platforms%20and%20the%20Sim2Real%20gap%2C%20enabling%20the%20trained%20agent%20to%0Aachieve%20similar%20performance%20in%20both%20simulation%20and%20the%20real%20world%2C%20driving%20the%0Avehicle%20effectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.08235v3&entry.124074799=Read"},
{"title": "SITransformer: Shared Information-Guided Transformer for Extreme\n  Multimodal Summarization", "author": "Sicheng Liu and Lintao Wang and Xiaogan Zhu and Xuequan Lu and Zhiyong Wang and Kun Hu", "abstract": "  Extreme Multimodal Summarization with Multimodal Output (XMSMO) becomes an\nattractive summarization approach by integrating various types of information\nto create extremely concise yet informative summaries for individual\nmodalities. Existing methods overlook the issue that multimodal data often\ncontains more topic irrelevant information, which can mislead the model into\nproducing inaccurate summaries especially for extremely short ones. In this\npaper, we propose SITransformer, a \\textbf{S}hared \\textbf{I}nformation-guided\n\\textbf{T}ransformer for extreme multimodal summarization. It has a shared\ninformation guided pipeline which involves a cross-modal shared information\nextractor and a cross-modal interaction module. The extractor formulates\nsemantically shared salient information from different modalities by devising a\nnovel filtering process consisting of a differentiable top-k selector and a\nshared-information guided gating unit. As a result, the common, salient, and\nrelevant contents across modalities are identified. Next, a transformer with\ncross-modal attentions is developed for intra- and inter-modality learning with\nthe shared information guidance to produce the extreme summary. Comprehensive\nexperiments demonstrate that SITransformer significantly enhances the\nsummarization quality for both video and text summaries for XMSMO. Our code\nwill be publicly available at https://github.com/SichengLeoLiu/MMAsia24-XMSMO.\n", "link": "http://arxiv.org/abs/2408.15829v1", "date": "2024-08-28", "relevancy": 2.1353, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5807}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5037}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SITransformer%3A%20Shared%20Information-Guided%20Transformer%20for%20Extreme%0A%20%20Multimodal%20Summarization&body=Title%3A%20SITransformer%3A%20Shared%20Information-Guided%20Transformer%20for%20Extreme%0A%20%20Multimodal%20Summarization%0AAuthor%3A%20Sicheng%20Liu%20and%20Lintao%20Wang%20and%20Xiaogan%20Zhu%20and%20Xuequan%20Lu%20and%20Zhiyong%20Wang%20and%20Kun%20Hu%0AAbstract%3A%20%20%20Extreme%20Multimodal%20Summarization%20with%20Multimodal%20Output%20%28XMSMO%29%20becomes%20an%0Aattractive%20summarization%20approach%20by%20integrating%20various%20types%20of%20information%0Ato%20create%20extremely%20concise%20yet%20informative%20summaries%20for%20individual%0Amodalities.%20Existing%20methods%20overlook%20the%20issue%20that%20multimodal%20data%20often%0Acontains%20more%20topic%20irrelevant%20information%2C%20which%20can%20mislead%20the%20model%20into%0Aproducing%20inaccurate%20summaries%20especially%20for%20extremely%20short%20ones.%20In%20this%0Apaper%2C%20we%20propose%20SITransformer%2C%20a%20%5Ctextbf%7BS%7Dhared%20%5Ctextbf%7BI%7Dnformation-guided%0A%5Ctextbf%7BT%7Dransformer%20for%20extreme%20multimodal%20summarization.%20It%20has%20a%20shared%0Ainformation%20guided%20pipeline%20which%20involves%20a%20cross-modal%20shared%20information%0Aextractor%20and%20a%20cross-modal%20interaction%20module.%20The%20extractor%20formulates%0Asemantically%20shared%20salient%20information%20from%20different%20modalities%20by%20devising%20a%0Anovel%20filtering%20process%20consisting%20of%20a%20differentiable%20top-k%20selector%20and%20a%0Ashared-information%20guided%20gating%20unit.%20As%20a%20result%2C%20the%20common%2C%20salient%2C%20and%0Arelevant%20contents%20across%20modalities%20are%20identified.%20Next%2C%20a%20transformer%20with%0Across-modal%20attentions%20is%20developed%20for%20intra-%20and%20inter-modality%20learning%20with%0Athe%20shared%20information%20guidance%20to%20produce%20the%20extreme%20summary.%20Comprehensive%0Aexperiments%20demonstrate%20that%20SITransformer%20significantly%20enhances%20the%0Asummarization%20quality%20for%20both%20video%20and%20text%20summaries%20for%20XMSMO.%20Our%20code%0Awill%20be%20publicly%20available%20at%20https%3A//github.com/SichengLeoLiu/MMAsia24-XMSMO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15829v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSITransformer%253A%2520Shared%2520Information-Guided%2520Transformer%2520for%2520Extreme%250A%2520%2520Multimodal%2520Summarization%26entry.906535625%3DSicheng%2520Liu%2520and%2520Lintao%2520Wang%2520and%2520Xiaogan%2520Zhu%2520and%2520Xuequan%2520Lu%2520and%2520Zhiyong%2520Wang%2520and%2520Kun%2520Hu%26entry.1292438233%3D%2520%2520Extreme%2520Multimodal%2520Summarization%2520with%2520Multimodal%2520Output%2520%2528XMSMO%2529%2520becomes%2520an%250Aattractive%2520summarization%2520approach%2520by%2520integrating%2520various%2520types%2520of%2520information%250Ato%2520create%2520extremely%2520concise%2520yet%2520informative%2520summaries%2520for%2520individual%250Amodalities.%2520Existing%2520methods%2520overlook%2520the%2520issue%2520that%2520multimodal%2520data%2520often%250Acontains%2520more%2520topic%2520irrelevant%2520information%252C%2520which%2520can%2520mislead%2520the%2520model%2520into%250Aproducing%2520inaccurate%2520summaries%2520especially%2520for%2520extremely%2520short%2520ones.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520SITransformer%252C%2520a%2520%255Ctextbf%257BS%257Dhared%2520%255Ctextbf%257BI%257Dnformation-guided%250A%255Ctextbf%257BT%257Dransformer%2520for%2520extreme%2520multimodal%2520summarization.%2520It%2520has%2520a%2520shared%250Ainformation%2520guided%2520pipeline%2520which%2520involves%2520a%2520cross-modal%2520shared%2520information%250Aextractor%2520and%2520a%2520cross-modal%2520interaction%2520module.%2520The%2520extractor%2520formulates%250Asemantically%2520shared%2520salient%2520information%2520from%2520different%2520modalities%2520by%2520devising%2520a%250Anovel%2520filtering%2520process%2520consisting%2520of%2520a%2520differentiable%2520top-k%2520selector%2520and%2520a%250Ashared-information%2520guided%2520gating%2520unit.%2520As%2520a%2520result%252C%2520the%2520common%252C%2520salient%252C%2520and%250Arelevant%2520contents%2520across%2520modalities%2520are%2520identified.%2520Next%252C%2520a%2520transformer%2520with%250Across-modal%2520attentions%2520is%2520developed%2520for%2520intra-%2520and%2520inter-modality%2520learning%2520with%250Athe%2520shared%2520information%2520guidance%2520to%2520produce%2520the%2520extreme%2520summary.%2520Comprehensive%250Aexperiments%2520demonstrate%2520that%2520SITransformer%2520significantly%2520enhances%2520the%250Asummarization%2520quality%2520for%2520both%2520video%2520and%2520text%2520summaries%2520for%2520XMSMO.%2520Our%2520code%250Awill%2520be%2520publicly%2520available%2520at%2520https%253A//github.com/SichengLeoLiu/MMAsia24-XMSMO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15829v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SITransformer%3A%20Shared%20Information-Guided%20Transformer%20for%20Extreme%0A%20%20Multimodal%20Summarization&entry.906535625=Sicheng%20Liu%20and%20Lintao%20Wang%20and%20Xiaogan%20Zhu%20and%20Xuequan%20Lu%20and%20Zhiyong%20Wang%20and%20Kun%20Hu&entry.1292438233=%20%20Extreme%20Multimodal%20Summarization%20with%20Multimodal%20Output%20%28XMSMO%29%20becomes%20an%0Aattractive%20summarization%20approach%20by%20integrating%20various%20types%20of%20information%0Ato%20create%20extremely%20concise%20yet%20informative%20summaries%20for%20individual%0Amodalities.%20Existing%20methods%20overlook%20the%20issue%20that%20multimodal%20data%20often%0Acontains%20more%20topic%20irrelevant%20information%2C%20which%20can%20mislead%20the%20model%20into%0Aproducing%20inaccurate%20summaries%20especially%20for%20extremely%20short%20ones.%20In%20this%0Apaper%2C%20we%20propose%20SITransformer%2C%20a%20%5Ctextbf%7BS%7Dhared%20%5Ctextbf%7BI%7Dnformation-guided%0A%5Ctextbf%7BT%7Dransformer%20for%20extreme%20multimodal%20summarization.%20It%20has%20a%20shared%0Ainformation%20guided%20pipeline%20which%20involves%20a%20cross-modal%20shared%20information%0Aextractor%20and%20a%20cross-modal%20interaction%20module.%20The%20extractor%20formulates%0Asemantically%20shared%20salient%20information%20from%20different%20modalities%20by%20devising%20a%0Anovel%20filtering%20process%20consisting%20of%20a%20differentiable%20top-k%20selector%20and%20a%0Ashared-information%20guided%20gating%20unit.%20As%20a%20result%2C%20the%20common%2C%20salient%2C%20and%0Arelevant%20contents%20across%20modalities%20are%20identified.%20Next%2C%20a%20transformer%20with%0Across-modal%20attentions%20is%20developed%20for%20intra-%20and%20inter-modality%20learning%20with%0Athe%20shared%20information%20guidance%20to%20produce%20the%20extreme%20summary.%20Comprehensive%0Aexperiments%20demonstrate%20that%20SITransformer%20significantly%20enhances%20the%0Asummarization%20quality%20for%20both%20video%20and%20text%20summaries%20for%20XMSMO.%20Our%20code%0Awill%20be%20publicly%20available%20at%20https%3A//github.com/SichengLeoLiu/MMAsia24-XMSMO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15829v1&entry.124074799=Read"},
{"title": "SpineMamba: Enhancing 3D Spinal Segmentation in Clinical Imaging through\n  Residual Visual Mamba Layers and Shape Priors", "author": "Zhiqing Zhang and Tianyong Liu and Guojia Fan and Bin Li and Qianjin Feng and Shoujun Zhou", "abstract": "  Accurate segmentation of 3D clinical medical images is critical in the\ndiagnosis and treatment of spinal diseases. However, the inherent complexity of\nspinal anatomy and uncertainty inherent in current imaging technologies, poses\nsignificant challenges for semantic segmentation of spinal images. Although\nconvolutional neural networks (CNNs) and Transformer-based models have made\nsome progress in spinal segmentation, their limitations in handling long-range\ndependencies hinder further improvements in segmentation accuracy.To address\nthese challenges, we introduce a residual visual Mamba layer to effectively\ncapture and model the deep semantic features and long-range spatial\ndependencies of 3D spinal data. To further enhance the structural semantic\nunderstanding of the vertebrae, we also propose a novel spinal shape prior\nmodule that captures specific anatomical information of the spine from medical\nimages, significantly enhancing the model's ability to extract structural\nsemantic information of the vertebrae. Comparative and ablation experiments on\ntwo datasets demonstrate that SpineMamba outperforms existing state-of-the-art\nmodels. On the CT dataset, the average Dice similarity coefficient for\nsegmentation reaches as high as 94.40, while on the MR dataset, it reaches\n86.95. Notably, compared to the renowned nnU-Net, SpineMamba achieves superior\nsegmentation performance, exceeding it by up to 2 percentage points. This\nunderscores its accuracy, robustness, and excellent generalization\ncapabilities.\n", "link": "http://arxiv.org/abs/2408.15887v1", "date": "2024-08-28", "relevancy": 2.1228, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5552}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5153}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpineMamba%3A%20Enhancing%203D%20Spinal%20Segmentation%20in%20Clinical%20Imaging%20through%0A%20%20Residual%20Visual%20Mamba%20Layers%20and%20Shape%20Priors&body=Title%3A%20SpineMamba%3A%20Enhancing%203D%20Spinal%20Segmentation%20in%20Clinical%20Imaging%20through%0A%20%20Residual%20Visual%20Mamba%20Layers%20and%20Shape%20Priors%0AAuthor%3A%20Zhiqing%20Zhang%20and%20Tianyong%20Liu%20and%20Guojia%20Fan%20and%20Bin%20Li%20and%20Qianjin%20Feng%20and%20Shoujun%20Zhou%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%203D%20clinical%20medical%20images%20is%20critical%20in%20the%0Adiagnosis%20and%20treatment%20of%20spinal%20diseases.%20However%2C%20the%20inherent%20complexity%20of%0Aspinal%20anatomy%20and%20uncertainty%20inherent%20in%20current%20imaging%20technologies%2C%20poses%0Asignificant%20challenges%20for%20semantic%20segmentation%20of%20spinal%20images.%20Although%0Aconvolutional%20neural%20networks%20%28CNNs%29%20and%20Transformer-based%20models%20have%20made%0Asome%20progress%20in%20spinal%20segmentation%2C%20their%20limitations%20in%20handling%20long-range%0Adependencies%20hinder%20further%20improvements%20in%20segmentation%20accuracy.To%20address%0Athese%20challenges%2C%20we%20introduce%20a%20residual%20visual%20Mamba%20layer%20to%20effectively%0Acapture%20and%20model%20the%20deep%20semantic%20features%20and%20long-range%20spatial%0Adependencies%20of%203D%20spinal%20data.%20To%20further%20enhance%20the%20structural%20semantic%0Aunderstanding%20of%20the%20vertebrae%2C%20we%20also%20propose%20a%20novel%20spinal%20shape%20prior%0Amodule%20that%20captures%20specific%20anatomical%20information%20of%20the%20spine%20from%20medical%0Aimages%2C%20significantly%20enhancing%20the%20model%27s%20ability%20to%20extract%20structural%0Asemantic%20information%20of%20the%20vertebrae.%20Comparative%20and%20ablation%20experiments%20on%0Atwo%20datasets%20demonstrate%20that%20SpineMamba%20outperforms%20existing%20state-of-the-art%0Amodels.%20On%20the%20CT%20dataset%2C%20the%20average%20Dice%20similarity%20coefficient%20for%0Asegmentation%20reaches%20as%20high%20as%2094.40%2C%20while%20on%20the%20MR%20dataset%2C%20it%20reaches%0A86.95.%20Notably%2C%20compared%20to%20the%20renowned%20nnU-Net%2C%20SpineMamba%20achieves%20superior%0Asegmentation%20performance%2C%20exceeding%20it%20by%20up%20to%202%20percentage%20points.%20This%0Aunderscores%20its%20accuracy%2C%20robustness%2C%20and%20excellent%20generalization%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15887v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpineMamba%253A%2520Enhancing%25203D%2520Spinal%2520Segmentation%2520in%2520Clinical%2520Imaging%2520through%250A%2520%2520Residual%2520Visual%2520Mamba%2520Layers%2520and%2520Shape%2520Priors%26entry.906535625%3DZhiqing%2520Zhang%2520and%2520Tianyong%2520Liu%2520and%2520Guojia%2520Fan%2520and%2520Bin%2520Li%2520and%2520Qianjin%2520Feng%2520and%2520Shoujun%2520Zhou%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%25203D%2520clinical%2520medical%2520images%2520is%2520critical%2520in%2520the%250Adiagnosis%2520and%2520treatment%2520of%2520spinal%2520diseases.%2520However%252C%2520the%2520inherent%2520complexity%2520of%250Aspinal%2520anatomy%2520and%2520uncertainty%2520inherent%2520in%2520current%2520imaging%2520technologies%252C%2520poses%250Asignificant%2520challenges%2520for%2520semantic%2520segmentation%2520of%2520spinal%2520images.%2520Although%250Aconvolutional%2520neural%2520networks%2520%2528CNNs%2529%2520and%2520Transformer-based%2520models%2520have%2520made%250Asome%2520progress%2520in%2520spinal%2520segmentation%252C%2520their%2520limitations%2520in%2520handling%2520long-range%250Adependencies%2520hinder%2520further%2520improvements%2520in%2520segmentation%2520accuracy.To%2520address%250Athese%2520challenges%252C%2520we%2520introduce%2520a%2520residual%2520visual%2520Mamba%2520layer%2520to%2520effectively%250Acapture%2520and%2520model%2520the%2520deep%2520semantic%2520features%2520and%2520long-range%2520spatial%250Adependencies%2520of%25203D%2520spinal%2520data.%2520To%2520further%2520enhance%2520the%2520structural%2520semantic%250Aunderstanding%2520of%2520the%2520vertebrae%252C%2520we%2520also%2520propose%2520a%2520novel%2520spinal%2520shape%2520prior%250Amodule%2520that%2520captures%2520specific%2520anatomical%2520information%2520of%2520the%2520spine%2520from%2520medical%250Aimages%252C%2520significantly%2520enhancing%2520the%2520model%2527s%2520ability%2520to%2520extract%2520structural%250Asemantic%2520information%2520of%2520the%2520vertebrae.%2520Comparative%2520and%2520ablation%2520experiments%2520on%250Atwo%2520datasets%2520demonstrate%2520that%2520SpineMamba%2520outperforms%2520existing%2520state-of-the-art%250Amodels.%2520On%2520the%2520CT%2520dataset%252C%2520the%2520average%2520Dice%2520similarity%2520coefficient%2520for%250Asegmentation%2520reaches%2520as%2520high%2520as%252094.40%252C%2520while%2520on%2520the%2520MR%2520dataset%252C%2520it%2520reaches%250A86.95.%2520Notably%252C%2520compared%2520to%2520the%2520renowned%2520nnU-Net%252C%2520SpineMamba%2520achieves%2520superior%250Asegmentation%2520performance%252C%2520exceeding%2520it%2520by%2520up%2520to%25202%2520percentage%2520points.%2520This%250Aunderscores%2520its%2520accuracy%252C%2520robustness%252C%2520and%2520excellent%2520generalization%250Acapabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15887v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpineMamba%3A%20Enhancing%203D%20Spinal%20Segmentation%20in%20Clinical%20Imaging%20through%0A%20%20Residual%20Visual%20Mamba%20Layers%20and%20Shape%20Priors&entry.906535625=Zhiqing%20Zhang%20and%20Tianyong%20Liu%20and%20Guojia%20Fan%20and%20Bin%20Li%20and%20Qianjin%20Feng%20and%20Shoujun%20Zhou&entry.1292438233=%20%20Accurate%20segmentation%20of%203D%20clinical%20medical%20images%20is%20critical%20in%20the%0Adiagnosis%20and%20treatment%20of%20spinal%20diseases.%20However%2C%20the%20inherent%20complexity%20of%0Aspinal%20anatomy%20and%20uncertainty%20inherent%20in%20current%20imaging%20technologies%2C%20poses%0Asignificant%20challenges%20for%20semantic%20segmentation%20of%20spinal%20images.%20Although%0Aconvolutional%20neural%20networks%20%28CNNs%29%20and%20Transformer-based%20models%20have%20made%0Asome%20progress%20in%20spinal%20segmentation%2C%20their%20limitations%20in%20handling%20long-range%0Adependencies%20hinder%20further%20improvements%20in%20segmentation%20accuracy.To%20address%0Athese%20challenges%2C%20we%20introduce%20a%20residual%20visual%20Mamba%20layer%20to%20effectively%0Acapture%20and%20model%20the%20deep%20semantic%20features%20and%20long-range%20spatial%0Adependencies%20of%203D%20spinal%20data.%20To%20further%20enhance%20the%20structural%20semantic%0Aunderstanding%20of%20the%20vertebrae%2C%20we%20also%20propose%20a%20novel%20spinal%20shape%20prior%0Amodule%20that%20captures%20specific%20anatomical%20information%20of%20the%20spine%20from%20medical%0Aimages%2C%20significantly%20enhancing%20the%20model%27s%20ability%20to%20extract%20structural%0Asemantic%20information%20of%20the%20vertebrae.%20Comparative%20and%20ablation%20experiments%20on%0Atwo%20datasets%20demonstrate%20that%20SpineMamba%20outperforms%20existing%20state-of-the-art%0Amodels.%20On%20the%20CT%20dataset%2C%20the%20average%20Dice%20similarity%20coefficient%20for%0Asegmentation%20reaches%20as%20high%20as%2094.40%2C%20while%20on%20the%20MR%20dataset%2C%20it%20reaches%0A86.95.%20Notably%2C%20compared%20to%20the%20renowned%20nnU-Net%2C%20SpineMamba%20achieves%20superior%0Asegmentation%20performance%2C%20exceeding%20it%20by%20up%20to%202%20percentage%20points.%20This%0Aunderscores%20its%20accuracy%2C%20robustness%2C%20and%20excellent%20generalization%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15887v1&entry.124074799=Read"},
{"title": "DEAR: Depth-Enhanced Action Recognition", "author": "Sadegh Rahmaniboldaji and Filip Rybansky and Quoc Vuong and Frank Guerin and Andrew Gilbert", "abstract": "  Detecting actions in videos, particularly within cluttered scenes, poses\nsignificant challenges due to the limitations of 2D frame analysis from a\ncamera perspective. Unlike human vision, which benefits from 3D understanding,\nrecognizing actions in such environments can be difficult. This research\nintroduces a novel approach integrating 3D features and depth maps alongside\nRGB features to enhance action recognition accuracy. Our method involves\nprocessing estimated depth maps through a separate branch from the RGB feature\nencoder and fusing the features to understand the scene and actions\ncomprehensively. Using the Side4Video framework and VideoMamba, which employ\nCLIP and VisionMamba for spatial feature extraction, our approach outperformed\nour implementation of the Side4Video network on the Something-Something V2\ndataset. Our code is available at: https://github.com/SadeghRahmaniB/DEAR\n", "link": "http://arxiv.org/abs/2408.15679v1", "date": "2024-08-28", "relevancy": 2.1141, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5307}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5295}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DEAR%3A%20Depth-Enhanced%20Action%20Recognition&body=Title%3A%20DEAR%3A%20Depth-Enhanced%20Action%20Recognition%0AAuthor%3A%20Sadegh%20Rahmaniboldaji%20and%20Filip%20Rybansky%20and%20Quoc%20Vuong%20and%20Frank%20Guerin%20and%20Andrew%20Gilbert%0AAbstract%3A%20%20%20Detecting%20actions%20in%20videos%2C%20particularly%20within%20cluttered%20scenes%2C%20poses%0Asignificant%20challenges%20due%20to%20the%20limitations%20of%202D%20frame%20analysis%20from%20a%0Acamera%20perspective.%20Unlike%20human%20vision%2C%20which%20benefits%20from%203D%20understanding%2C%0Arecognizing%20actions%20in%20such%20environments%20can%20be%20difficult.%20This%20research%0Aintroduces%20a%20novel%20approach%20integrating%203D%20features%20and%20depth%20maps%20alongside%0ARGB%20features%20to%20enhance%20action%20recognition%20accuracy.%20Our%20method%20involves%0Aprocessing%20estimated%20depth%20maps%20through%20a%20separate%20branch%20from%20the%20RGB%20feature%0Aencoder%20and%20fusing%20the%20features%20to%20understand%20the%20scene%20and%20actions%0Acomprehensively.%20Using%20the%20Side4Video%20framework%20and%20VideoMamba%2C%20which%20employ%0ACLIP%20and%20VisionMamba%20for%20spatial%20feature%20extraction%2C%20our%20approach%20outperformed%0Aour%20implementation%20of%20the%20Side4Video%20network%20on%20the%20Something-Something%20V2%0Adataset.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/SadeghRahmaniB/DEAR%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15679v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDEAR%253A%2520Depth-Enhanced%2520Action%2520Recognition%26entry.906535625%3DSadegh%2520Rahmaniboldaji%2520and%2520Filip%2520Rybansky%2520and%2520Quoc%2520Vuong%2520and%2520Frank%2520Guerin%2520and%2520Andrew%2520Gilbert%26entry.1292438233%3D%2520%2520Detecting%2520actions%2520in%2520videos%252C%2520particularly%2520within%2520cluttered%2520scenes%252C%2520poses%250Asignificant%2520challenges%2520due%2520to%2520the%2520limitations%2520of%25202D%2520frame%2520analysis%2520from%2520a%250Acamera%2520perspective.%2520Unlike%2520human%2520vision%252C%2520which%2520benefits%2520from%25203D%2520understanding%252C%250Arecognizing%2520actions%2520in%2520such%2520environments%2520can%2520be%2520difficult.%2520This%2520research%250Aintroduces%2520a%2520novel%2520approach%2520integrating%25203D%2520features%2520and%2520depth%2520maps%2520alongside%250ARGB%2520features%2520to%2520enhance%2520action%2520recognition%2520accuracy.%2520Our%2520method%2520involves%250Aprocessing%2520estimated%2520depth%2520maps%2520through%2520a%2520separate%2520branch%2520from%2520the%2520RGB%2520feature%250Aencoder%2520and%2520fusing%2520the%2520features%2520to%2520understand%2520the%2520scene%2520and%2520actions%250Acomprehensively.%2520Using%2520the%2520Side4Video%2520framework%2520and%2520VideoMamba%252C%2520which%2520employ%250ACLIP%2520and%2520VisionMamba%2520for%2520spatial%2520feature%2520extraction%252C%2520our%2520approach%2520outperformed%250Aour%2520implementation%2520of%2520the%2520Side4Video%2520network%2520on%2520the%2520Something-Something%2520V2%250Adataset.%2520Our%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/SadeghRahmaniB/DEAR%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15679v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DEAR%3A%20Depth-Enhanced%20Action%20Recognition&entry.906535625=Sadegh%20Rahmaniboldaji%20and%20Filip%20Rybansky%20and%20Quoc%20Vuong%20and%20Frank%20Guerin%20and%20Andrew%20Gilbert&entry.1292438233=%20%20Detecting%20actions%20in%20videos%2C%20particularly%20within%20cluttered%20scenes%2C%20poses%0Asignificant%20challenges%20due%20to%20the%20limitations%20of%202D%20frame%20analysis%20from%20a%0Acamera%20perspective.%20Unlike%20human%20vision%2C%20which%20benefits%20from%203D%20understanding%2C%0Arecognizing%20actions%20in%20such%20environments%20can%20be%20difficult.%20This%20research%0Aintroduces%20a%20novel%20approach%20integrating%203D%20features%20and%20depth%20maps%20alongside%0ARGB%20features%20to%20enhance%20action%20recognition%20accuracy.%20Our%20method%20involves%0Aprocessing%20estimated%20depth%20maps%20through%20a%20separate%20branch%20from%20the%20RGB%20feature%0Aencoder%20and%20fusing%20the%20features%20to%20understand%20the%20scene%20and%20actions%0Acomprehensively.%20Using%20the%20Side4Video%20framework%20and%20VideoMamba%2C%20which%20employ%0ACLIP%20and%20VisionMamba%20for%20spatial%20feature%20extraction%2C%20our%20approach%20outperformed%0Aour%20implementation%20of%20the%20Side4Video%20network%20on%20the%20Something-Something%20V2%0Adataset.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/SadeghRahmaniB/DEAR%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15679v1&entry.124074799=Read"},
{"title": "When Multi-Task Learning Meets Partial Supervision: A Computer Vision\n  Review", "author": "Maxime Fontana and Michael Spratling and Miaojing Shi", "abstract": "  Multi-Task Learning (MTL) aims to learn multiple tasks simultaneously while\nexploiting their mutual relationships. By using shared resources to\nsimultaneously calculate multiple outputs, this learning paradigm has the\npotential to have lower memory requirements and inference times compared to the\ntraditional approach of using separate methods for each task. Previous work in\nMTL has mainly focused on fully-supervised methods, as task relationships can\nnot only be leveraged to lower the level of data-dependency of those methods\nbut they can also improve performance. However, MTL introduces a set of\nchallenges due to a complex optimisation scheme and a higher labeling\nrequirement. This review focuses on how MTL could be utilised under different\npartial supervision settings to address these challenges. First, this review\nanalyses how MTL traditionally uses different parameter sharing techniques to\ntransfer knowledge in between tasks. Second, it presents the different\nchallenges arising from such a multi-objective optimisation scheme. Third, it\nintroduces how task groupings can be achieved by analysing task relationships.\nFourth, it focuses on how partially supervised methods applied to MTL can\ntackle the aforementioned challenges. Lastly, this review presents the\navailable datasets, tools and benchmarking results of such methods.\n", "link": "http://arxiv.org/abs/2307.14382v2", "date": "2024-08-28", "relevancy": 2.1072, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5486}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5173}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Multi-Task%20Learning%20Meets%20Partial%20Supervision%3A%20A%20Computer%20Vision%0A%20%20Review&body=Title%3A%20When%20Multi-Task%20Learning%20Meets%20Partial%20Supervision%3A%20A%20Computer%20Vision%0A%20%20Review%0AAuthor%3A%20Maxime%20Fontana%20and%20Michael%20Spratling%20and%20Miaojing%20Shi%0AAbstract%3A%20%20%20Multi-Task%20Learning%20%28MTL%29%20aims%20to%20learn%20multiple%20tasks%20simultaneously%20while%0Aexploiting%20their%20mutual%20relationships.%20By%20using%20shared%20resources%20to%0Asimultaneously%20calculate%20multiple%20outputs%2C%20this%20learning%20paradigm%20has%20the%0Apotential%20to%20have%20lower%20memory%20requirements%20and%20inference%20times%20compared%20to%20the%0Atraditional%20approach%20of%20using%20separate%20methods%20for%20each%20task.%20Previous%20work%20in%0AMTL%20has%20mainly%20focused%20on%20fully-supervised%20methods%2C%20as%20task%20relationships%20can%0Anot%20only%20be%20leveraged%20to%20lower%20the%20level%20of%20data-dependency%20of%20those%20methods%0Abut%20they%20can%20also%20improve%20performance.%20However%2C%20MTL%20introduces%20a%20set%20of%0Achallenges%20due%20to%20a%20complex%20optimisation%20scheme%20and%20a%20higher%20labeling%0Arequirement.%20This%20review%20focuses%20on%20how%20MTL%20could%20be%20utilised%20under%20different%0Apartial%20supervision%20settings%20to%20address%20these%20challenges.%20First%2C%20this%20review%0Aanalyses%20how%20MTL%20traditionally%20uses%20different%20parameter%20sharing%20techniques%20to%0Atransfer%20knowledge%20in%20between%20tasks.%20Second%2C%20it%20presents%20the%20different%0Achallenges%20arising%20from%20such%20a%20multi-objective%20optimisation%20scheme.%20Third%2C%20it%0Aintroduces%20how%20task%20groupings%20can%20be%20achieved%20by%20analysing%20task%20relationships.%0AFourth%2C%20it%20focuses%20on%20how%20partially%20supervised%20methods%20applied%20to%20MTL%20can%0Atackle%20the%20aforementioned%20challenges.%20Lastly%2C%20this%20review%20presents%20the%0Aavailable%20datasets%2C%20tools%20and%20benchmarking%20results%20of%20such%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.14382v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Multi-Task%2520Learning%2520Meets%2520Partial%2520Supervision%253A%2520A%2520Computer%2520Vision%250A%2520%2520Review%26entry.906535625%3DMaxime%2520Fontana%2520and%2520Michael%2520Spratling%2520and%2520Miaojing%2520Shi%26entry.1292438233%3D%2520%2520Multi-Task%2520Learning%2520%2528MTL%2529%2520aims%2520to%2520learn%2520multiple%2520tasks%2520simultaneously%2520while%250Aexploiting%2520their%2520mutual%2520relationships.%2520By%2520using%2520shared%2520resources%2520to%250Asimultaneously%2520calculate%2520multiple%2520outputs%252C%2520this%2520learning%2520paradigm%2520has%2520the%250Apotential%2520to%2520have%2520lower%2520memory%2520requirements%2520and%2520inference%2520times%2520compared%2520to%2520the%250Atraditional%2520approach%2520of%2520using%2520separate%2520methods%2520for%2520each%2520task.%2520Previous%2520work%2520in%250AMTL%2520has%2520mainly%2520focused%2520on%2520fully-supervised%2520methods%252C%2520as%2520task%2520relationships%2520can%250Anot%2520only%2520be%2520leveraged%2520to%2520lower%2520the%2520level%2520of%2520data-dependency%2520of%2520those%2520methods%250Abut%2520they%2520can%2520also%2520improve%2520performance.%2520However%252C%2520MTL%2520introduces%2520a%2520set%2520of%250Achallenges%2520due%2520to%2520a%2520complex%2520optimisation%2520scheme%2520and%2520a%2520higher%2520labeling%250Arequirement.%2520This%2520review%2520focuses%2520on%2520how%2520MTL%2520could%2520be%2520utilised%2520under%2520different%250Apartial%2520supervision%2520settings%2520to%2520address%2520these%2520challenges.%2520First%252C%2520this%2520review%250Aanalyses%2520how%2520MTL%2520traditionally%2520uses%2520different%2520parameter%2520sharing%2520techniques%2520to%250Atransfer%2520knowledge%2520in%2520between%2520tasks.%2520Second%252C%2520it%2520presents%2520the%2520different%250Achallenges%2520arising%2520from%2520such%2520a%2520multi-objective%2520optimisation%2520scheme.%2520Third%252C%2520it%250Aintroduces%2520how%2520task%2520groupings%2520can%2520be%2520achieved%2520by%2520analysing%2520task%2520relationships.%250AFourth%252C%2520it%2520focuses%2520on%2520how%2520partially%2520supervised%2520methods%2520applied%2520to%2520MTL%2520can%250Atackle%2520the%2520aforementioned%2520challenges.%2520Lastly%252C%2520this%2520review%2520presents%2520the%250Aavailable%2520datasets%252C%2520tools%2520and%2520benchmarking%2520results%2520of%2520such%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.14382v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Multi-Task%20Learning%20Meets%20Partial%20Supervision%3A%20A%20Computer%20Vision%0A%20%20Review&entry.906535625=Maxime%20Fontana%20and%20Michael%20Spratling%20and%20Miaojing%20Shi&entry.1292438233=%20%20Multi-Task%20Learning%20%28MTL%29%20aims%20to%20learn%20multiple%20tasks%20simultaneously%20while%0Aexploiting%20their%20mutual%20relationships.%20By%20using%20shared%20resources%20to%0Asimultaneously%20calculate%20multiple%20outputs%2C%20this%20learning%20paradigm%20has%20the%0Apotential%20to%20have%20lower%20memory%20requirements%20and%20inference%20times%20compared%20to%20the%0Atraditional%20approach%20of%20using%20separate%20methods%20for%20each%20task.%20Previous%20work%20in%0AMTL%20has%20mainly%20focused%20on%20fully-supervised%20methods%2C%20as%20task%20relationships%20can%0Anot%20only%20be%20leveraged%20to%20lower%20the%20level%20of%20data-dependency%20of%20those%20methods%0Abut%20they%20can%20also%20improve%20performance.%20However%2C%20MTL%20introduces%20a%20set%20of%0Achallenges%20due%20to%20a%20complex%20optimisation%20scheme%20and%20a%20higher%20labeling%0Arequirement.%20This%20review%20focuses%20on%20how%20MTL%20could%20be%20utilised%20under%20different%0Apartial%20supervision%20settings%20to%20address%20these%20challenges.%20First%2C%20this%20review%0Aanalyses%20how%20MTL%20traditionally%20uses%20different%20parameter%20sharing%20techniques%20to%0Atransfer%20knowledge%20in%20between%20tasks.%20Second%2C%20it%20presents%20the%20different%0Achallenges%20arising%20from%20such%20a%20multi-objective%20optimisation%20scheme.%20Third%2C%20it%0Aintroduces%20how%20task%20groupings%20can%20be%20achieved%20by%20analysing%20task%20relationships.%0AFourth%2C%20it%20focuses%20on%20how%20partially%20supervised%20methods%20applied%20to%20MTL%20can%0Atackle%20the%20aforementioned%20challenges.%20Lastly%2C%20this%20review%20presents%20the%0Aavailable%20datasets%2C%20tools%20and%20benchmarking%20results%20of%20such%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.14382v2&entry.124074799=Read"},
{"title": "Local Descriptors Weighted Adaptive Threshold Filtering For Few-Shot\n  Learning", "author": "Bingchen Yan", "abstract": "  Few-shot image classification is a challenging task in the field of machine\nlearning, involving the identification of new categories using a limited number\nof labeled samples. In recent years, methods based on local descriptors have\nmade significant progress in this area. However, the key to improving\nclassification accuracy lies in effectively filtering background noise and\naccurately selecting critical local descriptors highly relevant to image\ncategory information.\n  To address this challenge, we propose an innovative weighted adaptive\nthreshold filtering (WATF) strategy for local descriptors. This strategy can\ndynamically adjust based on the current task and image context, thereby\nselecting local descriptors most relevant to the image category. This enables\nthe model to better focus on category-related information while effectively\nmitigating interference from irrelevant background regions.\n  To evaluate the effectiveness of our method, we adopted the N-way K-shot\nexperimental framework. Experimental results show that our method not only\nimproves the clustering effect of selected local descriptors but also\nsignificantly enhances the discriminative ability between image categories.\nNotably, our method maintains a simple and lightweight design philosophy\nwithout introducing additional learnable parameters. This feature ensures\nconsistency in filtering capability during both training and testing phases,\nfurther enhancing the reliability and practicality of the method.\n", "link": "http://arxiv.org/abs/2408.15924v1", "date": "2024-08-28", "relevancy": 2.1062, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5363}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5311}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local%20Descriptors%20Weighted%20Adaptive%20Threshold%20Filtering%20For%20Few-Shot%0A%20%20Learning&body=Title%3A%20Local%20Descriptors%20Weighted%20Adaptive%20Threshold%20Filtering%20For%20Few-Shot%0A%20%20Learning%0AAuthor%3A%20Bingchen%20Yan%0AAbstract%3A%20%20%20Few-shot%20image%20classification%20is%20a%20challenging%20task%20in%20the%20field%20of%20machine%0Alearning%2C%20involving%20the%20identification%20of%20new%20categories%20using%20a%20limited%20number%0Aof%20labeled%20samples.%20In%20recent%20years%2C%20methods%20based%20on%20local%20descriptors%20have%0Amade%20significant%20progress%20in%20this%20area.%20However%2C%20the%20key%20to%20improving%0Aclassification%20accuracy%20lies%20in%20effectively%20filtering%20background%20noise%20and%0Aaccurately%20selecting%20critical%20local%20descriptors%20highly%20relevant%20to%20image%0Acategory%20information.%0A%20%20To%20address%20this%20challenge%2C%20we%20propose%20an%20innovative%20weighted%20adaptive%0Athreshold%20filtering%20%28WATF%29%20strategy%20for%20local%20descriptors.%20This%20strategy%20can%0Adynamically%20adjust%20based%20on%20the%20current%20task%20and%20image%20context%2C%20thereby%0Aselecting%20local%20descriptors%20most%20relevant%20to%20the%20image%20category.%20This%20enables%0Athe%20model%20to%20better%20focus%20on%20category-related%20information%20while%20effectively%0Amitigating%20interference%20from%20irrelevant%20background%20regions.%0A%20%20To%20evaluate%20the%20effectiveness%20of%20our%20method%2C%20we%20adopted%20the%20N-way%20K-shot%0Aexperimental%20framework.%20Experimental%20results%20show%20that%20our%20method%20not%20only%0Aimproves%20the%20clustering%20effect%20of%20selected%20local%20descriptors%20but%20also%0Asignificantly%20enhances%20the%20discriminative%20ability%20between%20image%20categories.%0ANotably%2C%20our%20method%20maintains%20a%20simple%20and%20lightweight%20design%20philosophy%0Awithout%20introducing%20additional%20learnable%20parameters.%20This%20feature%20ensures%0Aconsistency%20in%20filtering%20capability%20during%20both%20training%20and%20testing%20phases%2C%0Afurther%20enhancing%20the%20reliability%20and%20practicality%20of%20the%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal%2520Descriptors%2520Weighted%2520Adaptive%2520Threshold%2520Filtering%2520For%2520Few-Shot%250A%2520%2520Learning%26entry.906535625%3DBingchen%2520Yan%26entry.1292438233%3D%2520%2520Few-shot%2520image%2520classification%2520is%2520a%2520challenging%2520task%2520in%2520the%2520field%2520of%2520machine%250Alearning%252C%2520involving%2520the%2520identification%2520of%2520new%2520categories%2520using%2520a%2520limited%2520number%250Aof%2520labeled%2520samples.%2520In%2520recent%2520years%252C%2520methods%2520based%2520on%2520local%2520descriptors%2520have%250Amade%2520significant%2520progress%2520in%2520this%2520area.%2520However%252C%2520the%2520key%2520to%2520improving%250Aclassification%2520accuracy%2520lies%2520in%2520effectively%2520filtering%2520background%2520noise%2520and%250Aaccurately%2520selecting%2520critical%2520local%2520descriptors%2520highly%2520relevant%2520to%2520image%250Acategory%2520information.%250A%2520%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520an%2520innovative%2520weighted%2520adaptive%250Athreshold%2520filtering%2520%2528WATF%2529%2520strategy%2520for%2520local%2520descriptors.%2520This%2520strategy%2520can%250Adynamically%2520adjust%2520based%2520on%2520the%2520current%2520task%2520and%2520image%2520context%252C%2520thereby%250Aselecting%2520local%2520descriptors%2520most%2520relevant%2520to%2520the%2520image%2520category.%2520This%2520enables%250Athe%2520model%2520to%2520better%2520focus%2520on%2520category-related%2520information%2520while%2520effectively%250Amitigating%2520interference%2520from%2520irrelevant%2520background%2520regions.%250A%2520%2520To%2520evaluate%2520the%2520effectiveness%2520of%2520our%2520method%252C%2520we%2520adopted%2520the%2520N-way%2520K-shot%250Aexperimental%2520framework.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520not%2520only%250Aimproves%2520the%2520clustering%2520effect%2520of%2520selected%2520local%2520descriptors%2520but%2520also%250Asignificantly%2520enhances%2520the%2520discriminative%2520ability%2520between%2520image%2520categories.%250ANotably%252C%2520our%2520method%2520maintains%2520a%2520simple%2520and%2520lightweight%2520design%2520philosophy%250Awithout%2520introducing%2520additional%2520learnable%2520parameters.%2520This%2520feature%2520ensures%250Aconsistency%2520in%2520filtering%2520capability%2520during%2520both%2520training%2520and%2520testing%2520phases%252C%250Afurther%2520enhancing%2520the%2520reliability%2520and%2520practicality%2520of%2520the%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local%20Descriptors%20Weighted%20Adaptive%20Threshold%20Filtering%20For%20Few-Shot%0A%20%20Learning&entry.906535625=Bingchen%20Yan&entry.1292438233=%20%20Few-shot%20image%20classification%20is%20a%20challenging%20task%20in%20the%20field%20of%20machine%0Alearning%2C%20involving%20the%20identification%20of%20new%20categories%20using%20a%20limited%20number%0Aof%20labeled%20samples.%20In%20recent%20years%2C%20methods%20based%20on%20local%20descriptors%20have%0Amade%20significant%20progress%20in%20this%20area.%20However%2C%20the%20key%20to%20improving%0Aclassification%20accuracy%20lies%20in%20effectively%20filtering%20background%20noise%20and%0Aaccurately%20selecting%20critical%20local%20descriptors%20highly%20relevant%20to%20image%0Acategory%20information.%0A%20%20To%20address%20this%20challenge%2C%20we%20propose%20an%20innovative%20weighted%20adaptive%0Athreshold%20filtering%20%28WATF%29%20strategy%20for%20local%20descriptors.%20This%20strategy%20can%0Adynamically%20adjust%20based%20on%20the%20current%20task%20and%20image%20context%2C%20thereby%0Aselecting%20local%20descriptors%20most%20relevant%20to%20the%20image%20category.%20This%20enables%0Athe%20model%20to%20better%20focus%20on%20category-related%20information%20while%20effectively%0Amitigating%20interference%20from%20irrelevant%20background%20regions.%0A%20%20To%20evaluate%20the%20effectiveness%20of%20our%20method%2C%20we%20adopted%20the%20N-way%20K-shot%0Aexperimental%20framework.%20Experimental%20results%20show%20that%20our%20method%20not%20only%0Aimproves%20the%20clustering%20effect%20of%20selected%20local%20descriptors%20but%20also%0Asignificantly%20enhances%20the%20discriminative%20ability%20between%20image%20categories.%0ANotably%2C%20our%20method%20maintains%20a%20simple%20and%20lightweight%20design%20philosophy%0Awithout%20introducing%20additional%20learnable%20parameters.%20This%20feature%20ensures%0Aconsistency%20in%20filtering%20capability%20during%20both%20training%20and%20testing%20phases%2C%0Afurther%20enhancing%20the%20reliability%20and%20practicality%20of%20the%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15924v1&entry.124074799=Read"},
{"title": "Automated Real-World Sustainability Data Generation from Images of\n  Buildings", "author": "Peter J Bentley and Soo Ling Lim and Rajat Mathur and Sid Narang", "abstract": "  When data on building features is unavailable, the task of determining how to\nimprove that building in terms of carbon emissions becomes infeasible. We show\nthat from only a set of images, a Large Language Model with appropriate prompt\nengineering and domain knowledge can successfully estimate a range of building\nfeatures relevant for sustainability calculations. We compare our novel\nimage-to-data method with a ground truth comprising real building data for 47\napartments and achieve accuracy better than a human performing the same task.\nWe also demonstrate that the method can generate tailored recommendations to\nthe owner on how best to improve their properties and discuss methods to scale\nthe approach.\n", "link": "http://arxiv.org/abs/2405.18064v2", "date": "2024-08-28", "relevancy": 2.1001, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5391}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5354}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Real-World%20Sustainability%20Data%20Generation%20from%20Images%20of%0A%20%20Buildings&body=Title%3A%20Automated%20Real-World%20Sustainability%20Data%20Generation%20from%20Images%20of%0A%20%20Buildings%0AAuthor%3A%20Peter%20J%20Bentley%20and%20Soo%20Ling%20Lim%20and%20Rajat%20Mathur%20and%20Sid%20Narang%0AAbstract%3A%20%20%20When%20data%20on%20building%20features%20is%20unavailable%2C%20the%20task%20of%20determining%20how%20to%0Aimprove%20that%20building%20in%20terms%20of%20carbon%20emissions%20becomes%20infeasible.%20We%20show%0Athat%20from%20only%20a%20set%20of%20images%2C%20a%20Large%20Language%20Model%20with%20appropriate%20prompt%0Aengineering%20and%20domain%20knowledge%20can%20successfully%20estimate%20a%20range%20of%20building%0Afeatures%20relevant%20for%20sustainability%20calculations.%20We%20compare%20our%20novel%0Aimage-to-data%20method%20with%20a%20ground%20truth%20comprising%20real%20building%20data%20for%2047%0Aapartments%20and%20achieve%20accuracy%20better%20than%20a%20human%20performing%20the%20same%20task.%0AWe%20also%20demonstrate%20that%20the%20method%20can%20generate%20tailored%20recommendations%20to%0Athe%20owner%20on%20how%20best%20to%20improve%20their%20properties%20and%20discuss%20methods%20to%20scale%0Athe%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18064v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Real-World%2520Sustainability%2520Data%2520Generation%2520from%2520Images%2520of%250A%2520%2520Buildings%26entry.906535625%3DPeter%2520J%2520Bentley%2520and%2520Soo%2520Ling%2520Lim%2520and%2520Rajat%2520Mathur%2520and%2520Sid%2520Narang%26entry.1292438233%3D%2520%2520When%2520data%2520on%2520building%2520features%2520is%2520unavailable%252C%2520the%2520task%2520of%2520determining%2520how%2520to%250Aimprove%2520that%2520building%2520in%2520terms%2520of%2520carbon%2520emissions%2520becomes%2520infeasible.%2520We%2520show%250Athat%2520from%2520only%2520a%2520set%2520of%2520images%252C%2520a%2520Large%2520Language%2520Model%2520with%2520appropriate%2520prompt%250Aengineering%2520and%2520domain%2520knowledge%2520can%2520successfully%2520estimate%2520a%2520range%2520of%2520building%250Afeatures%2520relevant%2520for%2520sustainability%2520calculations.%2520We%2520compare%2520our%2520novel%250Aimage-to-data%2520method%2520with%2520a%2520ground%2520truth%2520comprising%2520real%2520building%2520data%2520for%252047%250Aapartments%2520and%2520achieve%2520accuracy%2520better%2520than%2520a%2520human%2520performing%2520the%2520same%2520task.%250AWe%2520also%2520demonstrate%2520that%2520the%2520method%2520can%2520generate%2520tailored%2520recommendations%2520to%250Athe%2520owner%2520on%2520how%2520best%2520to%2520improve%2520their%2520properties%2520and%2520discuss%2520methods%2520to%2520scale%250Athe%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18064v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Real-World%20Sustainability%20Data%20Generation%20from%20Images%20of%0A%20%20Buildings&entry.906535625=Peter%20J%20Bentley%20and%20Soo%20Ling%20Lim%20and%20Rajat%20Mathur%20and%20Sid%20Narang&entry.1292438233=%20%20When%20data%20on%20building%20features%20is%20unavailable%2C%20the%20task%20of%20determining%20how%20to%0Aimprove%20that%20building%20in%20terms%20of%20carbon%20emissions%20becomes%20infeasible.%20We%20show%0Athat%20from%20only%20a%20set%20of%20images%2C%20a%20Large%20Language%20Model%20with%20appropriate%20prompt%0Aengineering%20and%20domain%20knowledge%20can%20successfully%20estimate%20a%20range%20of%20building%0Afeatures%20relevant%20for%20sustainability%20calculations.%20We%20compare%20our%20novel%0Aimage-to-data%20method%20with%20a%20ground%20truth%20comprising%20real%20building%20data%20for%2047%0Aapartments%20and%20achieve%20accuracy%20better%20than%20a%20human%20performing%20the%20same%20task.%0AWe%20also%20demonstrate%20that%20the%20method%20can%20generate%20tailored%20recommendations%20to%0Athe%20owner%20on%20how%20best%20to%20improve%20their%20properties%20and%20discuss%20methods%20to%20scale%0Athe%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18064v2&entry.124074799=Read"},
{"title": "MetaGFN: Exploring Distant Modes with Adapted Metadynamics for\n  Continuous GFlowNets", "author": "Dominic Phillips and Flaviu Cipcigan", "abstract": "  Generative Flow Networks (GFlowNets) are a class of generative models that\nsample objects in proportion to a specified reward function through a learned\npolicy. They can be trained either on-policy or off-policy, needing a balance\nbetween exploration and exploitation for fast convergence to a target\ndistribution. While exploration strategies for discrete GFlowNets have been\nstudied, exploration in the continuous case remains to be investigated, despite\nthe potential for novel exploration algorithms due to the local connectedness\nof continuous domains. Here, we introduce Adapted Metadynamics, a variant of\nmetadynamics that can be applied to arbitrary black-box reward functions on\ncontinuous domains. We use Adapted Metadynamics as an exploration strategy for\ncontinuous GFlowNets. We show three continuous domains where the resulting\nalgorithm, MetaGFN, accelerates convergence to the target distribution and\ndiscovers more distant reward modes than previous off-policy exploration\nstrategies used for GFlowNets.\n", "link": "http://arxiv.org/abs/2408.15905v1", "date": "2024-08-28", "relevancy": 2.0854, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5333}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5323}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaGFN%3A%20Exploring%20Distant%20Modes%20with%20Adapted%20Metadynamics%20for%0A%20%20Continuous%20GFlowNets&body=Title%3A%20MetaGFN%3A%20Exploring%20Distant%20Modes%20with%20Adapted%20Metadynamics%20for%0A%20%20Continuous%20GFlowNets%0AAuthor%3A%20Dominic%20Phillips%20and%20Flaviu%20Cipcigan%0AAbstract%3A%20%20%20Generative%20Flow%20Networks%20%28GFlowNets%29%20are%20a%20class%20of%20generative%20models%20that%0Asample%20objects%20in%20proportion%20to%20a%20specified%20reward%20function%20through%20a%20learned%0Apolicy.%20They%20can%20be%20trained%20either%20on-policy%20or%20off-policy%2C%20needing%20a%20balance%0Abetween%20exploration%20and%20exploitation%20for%20fast%20convergence%20to%20a%20target%0Adistribution.%20While%20exploration%20strategies%20for%20discrete%20GFlowNets%20have%20been%0Astudied%2C%20exploration%20in%20the%20continuous%20case%20remains%20to%20be%20investigated%2C%20despite%0Athe%20potential%20for%20novel%20exploration%20algorithms%20due%20to%20the%20local%20connectedness%0Aof%20continuous%20domains.%20Here%2C%20we%20introduce%20Adapted%20Metadynamics%2C%20a%20variant%20of%0Ametadynamics%20that%20can%20be%20applied%20to%20arbitrary%20black-box%20reward%20functions%20on%0Acontinuous%20domains.%20We%20use%20Adapted%20Metadynamics%20as%20an%20exploration%20strategy%20for%0Acontinuous%20GFlowNets.%20We%20show%20three%20continuous%20domains%20where%20the%20resulting%0Aalgorithm%2C%20MetaGFN%2C%20accelerates%20convergence%20to%20the%20target%20distribution%20and%0Adiscovers%20more%20distant%20reward%20modes%20than%20previous%20off-policy%20exploration%0Astrategies%20used%20for%20GFlowNets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaGFN%253A%2520Exploring%2520Distant%2520Modes%2520with%2520Adapted%2520Metadynamics%2520for%250A%2520%2520Continuous%2520GFlowNets%26entry.906535625%3DDominic%2520Phillips%2520and%2520Flaviu%2520Cipcigan%26entry.1292438233%3D%2520%2520Generative%2520Flow%2520Networks%2520%2528GFlowNets%2529%2520are%2520a%2520class%2520of%2520generative%2520models%2520that%250Asample%2520objects%2520in%2520proportion%2520to%2520a%2520specified%2520reward%2520function%2520through%2520a%2520learned%250Apolicy.%2520They%2520can%2520be%2520trained%2520either%2520on-policy%2520or%2520off-policy%252C%2520needing%2520a%2520balance%250Abetween%2520exploration%2520and%2520exploitation%2520for%2520fast%2520convergence%2520to%2520a%2520target%250Adistribution.%2520While%2520exploration%2520strategies%2520for%2520discrete%2520GFlowNets%2520have%2520been%250Astudied%252C%2520exploration%2520in%2520the%2520continuous%2520case%2520remains%2520to%2520be%2520investigated%252C%2520despite%250Athe%2520potential%2520for%2520novel%2520exploration%2520algorithms%2520due%2520to%2520the%2520local%2520connectedness%250Aof%2520continuous%2520domains.%2520Here%252C%2520we%2520introduce%2520Adapted%2520Metadynamics%252C%2520a%2520variant%2520of%250Ametadynamics%2520that%2520can%2520be%2520applied%2520to%2520arbitrary%2520black-box%2520reward%2520functions%2520on%250Acontinuous%2520domains.%2520We%2520use%2520Adapted%2520Metadynamics%2520as%2520an%2520exploration%2520strategy%2520for%250Acontinuous%2520GFlowNets.%2520We%2520show%2520three%2520continuous%2520domains%2520where%2520the%2520resulting%250Aalgorithm%252C%2520MetaGFN%252C%2520accelerates%2520convergence%2520to%2520the%2520target%2520distribution%2520and%250Adiscovers%2520more%2520distant%2520reward%2520modes%2520than%2520previous%2520off-policy%2520exploration%250Astrategies%2520used%2520for%2520GFlowNets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaGFN%3A%20Exploring%20Distant%20Modes%20with%20Adapted%20Metadynamics%20for%0A%20%20Continuous%20GFlowNets&entry.906535625=Dominic%20Phillips%20and%20Flaviu%20Cipcigan&entry.1292438233=%20%20Generative%20Flow%20Networks%20%28GFlowNets%29%20are%20a%20class%20of%20generative%20models%20that%0Asample%20objects%20in%20proportion%20to%20a%20specified%20reward%20function%20through%20a%20learned%0Apolicy.%20They%20can%20be%20trained%20either%20on-policy%20or%20off-policy%2C%20needing%20a%20balance%0Abetween%20exploration%20and%20exploitation%20for%20fast%20convergence%20to%20a%20target%0Adistribution.%20While%20exploration%20strategies%20for%20discrete%20GFlowNets%20have%20been%0Astudied%2C%20exploration%20in%20the%20continuous%20case%20remains%20to%20be%20investigated%2C%20despite%0Athe%20potential%20for%20novel%20exploration%20algorithms%20due%20to%20the%20local%20connectedness%0Aof%20continuous%20domains.%20Here%2C%20we%20introduce%20Adapted%20Metadynamics%2C%20a%20variant%20of%0Ametadynamics%20that%20can%20be%20applied%20to%20arbitrary%20black-box%20reward%20functions%20on%0Acontinuous%20domains.%20We%20use%20Adapted%20Metadynamics%20as%20an%20exploration%20strategy%20for%0Acontinuous%20GFlowNets.%20We%20show%20three%20continuous%20domains%20where%20the%20resulting%0Aalgorithm%2C%20MetaGFN%2C%20accelerates%20convergence%20to%20the%20target%20distribution%20and%0Adiscovers%20more%20distant%20reward%20modes%20than%20previous%20off-policy%20exploration%0Astrategies%20used%20for%20GFlowNets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15905v1&entry.124074799=Read"},
{"title": "Automated Label Unification for Multi-Dataset Semantic Segmentation with\n  GNNs", "author": "Rong Ma and Jie Chen and Xiangyang Xue and Jian Pu", "abstract": "  Deep supervised models possess significant capability to assimilate extensive\ntraining data, thereby presenting an opportunity to enhance model performance\nthrough training on multiple datasets. However, conflicts arising from\ndifferent label spaces among datasets may adversely affect model performance.\nIn this paper, we propose a novel approach to automatically construct a unified\nlabel space across multiple datasets using graph neural networks. This enables\nsemantic segmentation models to be trained simultaneously on multiple datasets,\nresulting in performance improvements. Unlike existing methods, our approach\nfacilitates seamless training without the need for additional manual\nreannotation or taxonomy reconciliation. This significantly enhances the\nefficiency and effectiveness of multi-dataset segmentation model training. The\nresults demonstrate that our method significantly outperforms other\nmulti-dataset training methods when trained on seven datasets simultaneously,\nand achieves state-of-the-art performance on the WildDash 2 benchmark.\n", "link": "http://arxiv.org/abs/2407.10534v2", "date": "2024-08-28", "relevancy": 2.0813, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5247}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.523}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Label%20Unification%20for%20Multi-Dataset%20Semantic%20Segmentation%20with%0A%20%20GNNs&body=Title%3A%20Automated%20Label%20Unification%20for%20Multi-Dataset%20Semantic%20Segmentation%20with%0A%20%20GNNs%0AAuthor%3A%20Rong%20Ma%20and%20Jie%20Chen%20and%20Xiangyang%20Xue%20and%20Jian%20Pu%0AAbstract%3A%20%20%20Deep%20supervised%20models%20possess%20significant%20capability%20to%20assimilate%20extensive%0Atraining%20data%2C%20thereby%20presenting%20an%20opportunity%20to%20enhance%20model%20performance%0Athrough%20training%20on%20multiple%20datasets.%20However%2C%20conflicts%20arising%20from%0Adifferent%20label%20spaces%20among%20datasets%20may%20adversely%20affect%20model%20performance.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%20automatically%20construct%20a%20unified%0Alabel%20space%20across%20multiple%20datasets%20using%20graph%20neural%20networks.%20This%20enables%0Asemantic%20segmentation%20models%20to%20be%20trained%20simultaneously%20on%20multiple%20datasets%2C%0Aresulting%20in%20performance%20improvements.%20Unlike%20existing%20methods%2C%20our%20approach%0Afacilitates%20seamless%20training%20without%20the%20need%20for%20additional%20manual%0Areannotation%20or%20taxonomy%20reconciliation.%20This%20significantly%20enhances%20the%0Aefficiency%20and%20effectiveness%20of%20multi-dataset%20segmentation%20model%20training.%20The%0Aresults%20demonstrate%20that%20our%20method%20significantly%20outperforms%20other%0Amulti-dataset%20training%20methods%20when%20trained%20on%20seven%20datasets%20simultaneously%2C%0Aand%20achieves%20state-of-the-art%20performance%20on%20the%20WildDash%202%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10534v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Label%2520Unification%2520for%2520Multi-Dataset%2520Semantic%2520Segmentation%2520with%250A%2520%2520GNNs%26entry.906535625%3DRong%2520Ma%2520and%2520Jie%2520Chen%2520and%2520Xiangyang%2520Xue%2520and%2520Jian%2520Pu%26entry.1292438233%3D%2520%2520Deep%2520supervised%2520models%2520possess%2520significant%2520capability%2520to%2520assimilate%2520extensive%250Atraining%2520data%252C%2520thereby%2520presenting%2520an%2520opportunity%2520to%2520enhance%2520model%2520performance%250Athrough%2520training%2520on%2520multiple%2520datasets.%2520However%252C%2520conflicts%2520arising%2520from%250Adifferent%2520label%2520spaces%2520among%2520datasets%2520may%2520adversely%2520affect%2520model%2520performance.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520to%2520automatically%2520construct%2520a%2520unified%250Alabel%2520space%2520across%2520multiple%2520datasets%2520using%2520graph%2520neural%2520networks.%2520This%2520enables%250Asemantic%2520segmentation%2520models%2520to%2520be%2520trained%2520simultaneously%2520on%2520multiple%2520datasets%252C%250Aresulting%2520in%2520performance%2520improvements.%2520Unlike%2520existing%2520methods%252C%2520our%2520approach%250Afacilitates%2520seamless%2520training%2520without%2520the%2520need%2520for%2520additional%2520manual%250Areannotation%2520or%2520taxonomy%2520reconciliation.%2520This%2520significantly%2520enhances%2520the%250Aefficiency%2520and%2520effectiveness%2520of%2520multi-dataset%2520segmentation%2520model%2520training.%2520The%250Aresults%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520other%250Amulti-dataset%2520training%2520methods%2520when%2520trained%2520on%2520seven%2520datasets%2520simultaneously%252C%250Aand%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%2520WildDash%25202%2520benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10534v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Label%20Unification%20for%20Multi-Dataset%20Semantic%20Segmentation%20with%0A%20%20GNNs&entry.906535625=Rong%20Ma%20and%20Jie%20Chen%20and%20Xiangyang%20Xue%20and%20Jian%20Pu&entry.1292438233=%20%20Deep%20supervised%20models%20possess%20significant%20capability%20to%20assimilate%20extensive%0Atraining%20data%2C%20thereby%20presenting%20an%20opportunity%20to%20enhance%20model%20performance%0Athrough%20training%20on%20multiple%20datasets.%20However%2C%20conflicts%20arising%20from%0Adifferent%20label%20spaces%20among%20datasets%20may%20adversely%20affect%20model%20performance.%0AIn%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20to%20automatically%20construct%20a%20unified%0Alabel%20space%20across%20multiple%20datasets%20using%20graph%20neural%20networks.%20This%20enables%0Asemantic%20segmentation%20models%20to%20be%20trained%20simultaneously%20on%20multiple%20datasets%2C%0Aresulting%20in%20performance%20improvements.%20Unlike%20existing%20methods%2C%20our%20approach%0Afacilitates%20seamless%20training%20without%20the%20need%20for%20additional%20manual%0Areannotation%20or%20taxonomy%20reconciliation.%20This%20significantly%20enhances%20the%0Aefficiency%20and%20effectiveness%20of%20multi-dataset%20segmentation%20model%20training.%20The%0Aresults%20demonstrate%20that%20our%20method%20significantly%20outperforms%20other%0Amulti-dataset%20training%20methods%20when%20trained%20on%20seven%20datasets%20simultaneously%2C%0Aand%20achieves%20state-of-the-art%20performance%20on%20the%20WildDash%202%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10534v2&entry.124074799=Read"},
{"title": "Efficient Slice Anomaly Detection Network for 3D Brain MRI Volume", "author": "Zeduo Zhang and Yalda Mohsenzadeh", "abstract": "  Current anomaly detection methods excel with benchmark industrial data but\nstruggle with natural images and medical data due to varying definitions of\n'normal' and 'abnormal.' This makes accurate identification of deviations in\nthese fields particularly challenging. Especially for 3D brain MRI data, all\nthe state-of-the-art models are reconstruction-based with 3D convolutional\nneural networks which are memory-intensive, time-consuming and producing noisy\noutputs that require further post-processing. We propose a framework called\nSimple Slice-based Network (SimpleSliceNet), which utilizes a model pre-trained\non ImageNet and fine-tuned on a separate MRI dataset as a 2D slice feature\nextractor to reduce computational cost. We aggregate the extracted features to\nperform anomaly detection tasks on 3D brain MRI volumes. Our model integrates a\nconditional normalizing flow to calculate log likelihood of features and\nemploys the Semi-Push-Pull Mechanism to enhance anomaly detection accuracy. The\nresults indicate improved performance, showcasing our model's remarkable\nadaptability and effectiveness when addressing the challenges exists in brain\nMRI data. In addition, for the large-scale 3D brain volumes, our model\nSimpleSliceNet outperforms the state-of-the-art 2D and 3D models in terms of\naccuracy, memory usage and time consumption. Code is available at:\nhttps://anonymous.4open.science/r/SimpleSliceNet-8EA3.\n", "link": "http://arxiv.org/abs/2408.15958v1", "date": "2024-08-28", "relevancy": 2.0571, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5199}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5132}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Slice%20Anomaly%20Detection%20Network%20for%203D%20Brain%20MRI%20Volume&body=Title%3A%20Efficient%20Slice%20Anomaly%20Detection%20Network%20for%203D%20Brain%20MRI%20Volume%0AAuthor%3A%20Zeduo%20Zhang%20and%20Yalda%20Mohsenzadeh%0AAbstract%3A%20%20%20Current%20anomaly%20detection%20methods%20excel%20with%20benchmark%20industrial%20data%20but%0Astruggle%20with%20natural%20images%20and%20medical%20data%20due%20to%20varying%20definitions%20of%0A%27normal%27%20and%20%27abnormal.%27%20This%20makes%20accurate%20identification%20of%20deviations%20in%0Athese%20fields%20particularly%20challenging.%20Especially%20for%203D%20brain%20MRI%20data%2C%20all%0Athe%20state-of-the-art%20models%20are%20reconstruction-based%20with%203D%20convolutional%0Aneural%20networks%20which%20are%20memory-intensive%2C%20time-consuming%20and%20producing%20noisy%0Aoutputs%20that%20require%20further%20post-processing.%20We%20propose%20a%20framework%20called%0ASimple%20Slice-based%20Network%20%28SimpleSliceNet%29%2C%20which%20utilizes%20a%20model%20pre-trained%0Aon%20ImageNet%20and%20fine-tuned%20on%20a%20separate%20MRI%20dataset%20as%20a%202D%20slice%20feature%0Aextractor%20to%20reduce%20computational%20cost.%20We%20aggregate%20the%20extracted%20features%20to%0Aperform%20anomaly%20detection%20tasks%20on%203D%20brain%20MRI%20volumes.%20Our%20model%20integrates%20a%0Aconditional%20normalizing%20flow%20to%20calculate%20log%20likelihood%20of%20features%20and%0Aemploys%20the%20Semi-Push-Pull%20Mechanism%20to%20enhance%20anomaly%20detection%20accuracy.%20The%0Aresults%20indicate%20improved%20performance%2C%20showcasing%20our%20model%27s%20remarkable%0Aadaptability%20and%20effectiveness%20when%20addressing%20the%20challenges%20exists%20in%20brain%0AMRI%20data.%20In%20addition%2C%20for%20the%20large-scale%203D%20brain%20volumes%2C%20our%20model%0ASimpleSliceNet%20outperforms%20the%20state-of-the-art%202D%20and%203D%20models%20in%20terms%20of%0Aaccuracy%2C%20memory%20usage%20and%20time%20consumption.%20Code%20is%20available%20at%3A%0Ahttps%3A//anonymous.4open.science/r/SimpleSliceNet-8EA3.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15958v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Slice%2520Anomaly%2520Detection%2520Network%2520for%25203D%2520Brain%2520MRI%2520Volume%26entry.906535625%3DZeduo%2520Zhang%2520and%2520Yalda%2520Mohsenzadeh%26entry.1292438233%3D%2520%2520Current%2520anomaly%2520detection%2520methods%2520excel%2520with%2520benchmark%2520industrial%2520data%2520but%250Astruggle%2520with%2520natural%2520images%2520and%2520medical%2520data%2520due%2520to%2520varying%2520definitions%2520of%250A%2527normal%2527%2520and%2520%2527abnormal.%2527%2520This%2520makes%2520accurate%2520identification%2520of%2520deviations%2520in%250Athese%2520fields%2520particularly%2520challenging.%2520Especially%2520for%25203D%2520brain%2520MRI%2520data%252C%2520all%250Athe%2520state-of-the-art%2520models%2520are%2520reconstruction-based%2520with%25203D%2520convolutional%250Aneural%2520networks%2520which%2520are%2520memory-intensive%252C%2520time-consuming%2520and%2520producing%2520noisy%250Aoutputs%2520that%2520require%2520further%2520post-processing.%2520We%2520propose%2520a%2520framework%2520called%250ASimple%2520Slice-based%2520Network%2520%2528SimpleSliceNet%2529%252C%2520which%2520utilizes%2520a%2520model%2520pre-trained%250Aon%2520ImageNet%2520and%2520fine-tuned%2520on%2520a%2520separate%2520MRI%2520dataset%2520as%2520a%25202D%2520slice%2520feature%250Aextractor%2520to%2520reduce%2520computational%2520cost.%2520We%2520aggregate%2520the%2520extracted%2520features%2520to%250Aperform%2520anomaly%2520detection%2520tasks%2520on%25203D%2520brain%2520MRI%2520volumes.%2520Our%2520model%2520integrates%2520a%250Aconditional%2520normalizing%2520flow%2520to%2520calculate%2520log%2520likelihood%2520of%2520features%2520and%250Aemploys%2520the%2520Semi-Push-Pull%2520Mechanism%2520to%2520enhance%2520anomaly%2520detection%2520accuracy.%2520The%250Aresults%2520indicate%2520improved%2520performance%252C%2520showcasing%2520our%2520model%2527s%2520remarkable%250Aadaptability%2520and%2520effectiveness%2520when%2520addressing%2520the%2520challenges%2520exists%2520in%2520brain%250AMRI%2520data.%2520In%2520addition%252C%2520for%2520the%2520large-scale%25203D%2520brain%2520volumes%252C%2520our%2520model%250ASimpleSliceNet%2520outperforms%2520the%2520state-of-the-art%25202D%2520and%25203D%2520models%2520in%2520terms%2520of%250Aaccuracy%252C%2520memory%2520usage%2520and%2520time%2520consumption.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//anonymous.4open.science/r/SimpleSliceNet-8EA3.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15958v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Slice%20Anomaly%20Detection%20Network%20for%203D%20Brain%20MRI%20Volume&entry.906535625=Zeduo%20Zhang%20and%20Yalda%20Mohsenzadeh&entry.1292438233=%20%20Current%20anomaly%20detection%20methods%20excel%20with%20benchmark%20industrial%20data%20but%0Astruggle%20with%20natural%20images%20and%20medical%20data%20due%20to%20varying%20definitions%20of%0A%27normal%27%20and%20%27abnormal.%27%20This%20makes%20accurate%20identification%20of%20deviations%20in%0Athese%20fields%20particularly%20challenging.%20Especially%20for%203D%20brain%20MRI%20data%2C%20all%0Athe%20state-of-the-art%20models%20are%20reconstruction-based%20with%203D%20convolutional%0Aneural%20networks%20which%20are%20memory-intensive%2C%20time-consuming%20and%20producing%20noisy%0Aoutputs%20that%20require%20further%20post-processing.%20We%20propose%20a%20framework%20called%0ASimple%20Slice-based%20Network%20%28SimpleSliceNet%29%2C%20which%20utilizes%20a%20model%20pre-trained%0Aon%20ImageNet%20and%20fine-tuned%20on%20a%20separate%20MRI%20dataset%20as%20a%202D%20slice%20feature%0Aextractor%20to%20reduce%20computational%20cost.%20We%20aggregate%20the%20extracted%20features%20to%0Aperform%20anomaly%20detection%20tasks%20on%203D%20brain%20MRI%20volumes.%20Our%20model%20integrates%20a%0Aconditional%20normalizing%20flow%20to%20calculate%20log%20likelihood%20of%20features%20and%0Aemploys%20the%20Semi-Push-Pull%20Mechanism%20to%20enhance%20anomaly%20detection%20accuracy.%20The%0Aresults%20indicate%20improved%20performance%2C%20showcasing%20our%20model%27s%20remarkable%0Aadaptability%20and%20effectiveness%20when%20addressing%20the%20challenges%20exists%20in%20brain%0AMRI%20data.%20In%20addition%2C%20for%20the%20large-scale%203D%20brain%20volumes%2C%20our%20model%0ASimpleSliceNet%20outperforms%20the%20state-of-the-art%202D%20and%203D%20models%20in%20terms%20of%0Aaccuracy%2C%20memory%20usage%20and%20time%20consumption.%20Code%20is%20available%20at%3A%0Ahttps%3A//anonymous.4open.science/r/SimpleSliceNet-8EA3.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15958v1&entry.124074799=Read"},
{"title": "Perceive-IR: Learning to Perceive Degradation Better for All-in-One\n  Image Restoration", "author": "Xu Zhang and Jiaqi Ma and Guoli Wang and Qian Zhang and Huan Zhang and Lefei Zhang", "abstract": "  The limitations of task-specific and general image restoration methods for\nspecific degradation have prompted the development of all-in-one image\nrestoration techniques. However, the diversity of patterns among multiple\ndegradation, along with the significant uncertainties in mapping between\ndegraded images of different severities and their corresponding undistorted\nversions, pose significant challenges to the all-in-one restoration tasks. To\naddress these challenges, we propose Perceive-IR, an all-in-one image restorer\ndesigned to achieve fine-grained quality control that enables restored images\nto more closely resemble their undistorted counterparts, regardless of the type\nor severity of degradation. Specifically, Perceive-IR contains two stages: (1)\nprompt learning stage and (2) restoration stage. In the prompt learning stage,\nwe leverage prompt learning to acquire a fine-grained quality perceiver capable\nof distinguishing three-tier quality levels by constraining the prompt-image\nsimilarity in the CLIP perception space. Subsequently, this quality perceiver\nand difficulty-adaptive perceptual loss are integrated as a quality-aware\nlearning strategy to realize fine-grained quality control in restoration stage.\nFor the restoration stage, a semantic guidance module (SGM) and compact feature\nextraction (CFE) are proposed to further promote the restoration process by\nutilizing the robust semantic information from the pre-trained large scale\nvision models and distinguishing degradation-specific features. Extensive\nexperiments demonstrate that our Perceive-IR outperforms state-of-the-art\nmethods in all-in-one image restoration tasks and exhibit superior\ngeneralization ability when dealing with unseen tasks.\n", "link": "http://arxiv.org/abs/2408.15994v1", "date": "2024-08-28", "relevancy": 2.0323, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5193}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5041}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perceive-IR%3A%20Learning%20to%20Perceive%20Degradation%20Better%20for%20All-in-One%0A%20%20Image%20Restoration&body=Title%3A%20Perceive-IR%3A%20Learning%20to%20Perceive%20Degradation%20Better%20for%20All-in-One%0A%20%20Image%20Restoration%0AAuthor%3A%20Xu%20Zhang%20and%20Jiaqi%20Ma%20and%20Guoli%20Wang%20and%20Qian%20Zhang%20and%20Huan%20Zhang%20and%20Lefei%20Zhang%0AAbstract%3A%20%20%20The%20limitations%20of%20task-specific%20and%20general%20image%20restoration%20methods%20for%0Aspecific%20degradation%20have%20prompted%20the%20development%20of%20all-in-one%20image%0Arestoration%20techniques.%20However%2C%20the%20diversity%20of%20patterns%20among%20multiple%0Adegradation%2C%20along%20with%20the%20significant%20uncertainties%20in%20mapping%20between%0Adegraded%20images%20of%20different%20severities%20and%20their%20corresponding%20undistorted%0Aversions%2C%20pose%20significant%20challenges%20to%20the%20all-in-one%20restoration%20tasks.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20Perceive-IR%2C%20an%20all-in-one%20image%20restorer%0Adesigned%20to%20achieve%20fine-grained%20quality%20control%20that%20enables%20restored%20images%0Ato%20more%20closely%20resemble%20their%20undistorted%20counterparts%2C%20regardless%20of%20the%20type%0Aor%20severity%20of%20degradation.%20Specifically%2C%20Perceive-IR%20contains%20two%20stages%3A%20%281%29%0Aprompt%20learning%20stage%20and%20%282%29%20restoration%20stage.%20In%20the%20prompt%20learning%20stage%2C%0Awe%20leverage%20prompt%20learning%20to%20acquire%20a%20fine-grained%20quality%20perceiver%20capable%0Aof%20distinguishing%20three-tier%20quality%20levels%20by%20constraining%20the%20prompt-image%0Asimilarity%20in%20the%20CLIP%20perception%20space.%20Subsequently%2C%20this%20quality%20perceiver%0Aand%20difficulty-adaptive%20perceptual%20loss%20are%20integrated%20as%20a%20quality-aware%0Alearning%20strategy%20to%20realize%20fine-grained%20quality%20control%20in%20restoration%20stage.%0AFor%20the%20restoration%20stage%2C%20a%20semantic%20guidance%20module%20%28SGM%29%20and%20compact%20feature%0Aextraction%20%28CFE%29%20are%20proposed%20to%20further%20promote%20the%20restoration%20process%20by%0Autilizing%20the%20robust%20semantic%20information%20from%20the%20pre-trained%20large%20scale%0Avision%20models%20and%20distinguishing%20degradation-specific%20features.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20Perceive-IR%20outperforms%20state-of-the-art%0Amethods%20in%20all-in-one%20image%20restoration%20tasks%20and%20exhibit%20superior%0Ageneralization%20ability%20when%20dealing%20with%20unseen%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15994v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerceive-IR%253A%2520Learning%2520to%2520Perceive%2520Degradation%2520Better%2520for%2520All-in-One%250A%2520%2520Image%2520Restoration%26entry.906535625%3DXu%2520Zhang%2520and%2520Jiaqi%2520Ma%2520and%2520Guoli%2520Wang%2520and%2520Qian%2520Zhang%2520and%2520Huan%2520Zhang%2520and%2520Lefei%2520Zhang%26entry.1292438233%3D%2520%2520The%2520limitations%2520of%2520task-specific%2520and%2520general%2520image%2520restoration%2520methods%2520for%250Aspecific%2520degradation%2520have%2520prompted%2520the%2520development%2520of%2520all-in-one%2520image%250Arestoration%2520techniques.%2520However%252C%2520the%2520diversity%2520of%2520patterns%2520among%2520multiple%250Adegradation%252C%2520along%2520with%2520the%2520significant%2520uncertainties%2520in%2520mapping%2520between%250Adegraded%2520images%2520of%2520different%2520severities%2520and%2520their%2520corresponding%2520undistorted%250Aversions%252C%2520pose%2520significant%2520challenges%2520to%2520the%2520all-in-one%2520restoration%2520tasks.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520Perceive-IR%252C%2520an%2520all-in-one%2520image%2520restorer%250Adesigned%2520to%2520achieve%2520fine-grained%2520quality%2520control%2520that%2520enables%2520restored%2520images%250Ato%2520more%2520closely%2520resemble%2520their%2520undistorted%2520counterparts%252C%2520regardless%2520of%2520the%2520type%250Aor%2520severity%2520of%2520degradation.%2520Specifically%252C%2520Perceive-IR%2520contains%2520two%2520stages%253A%2520%25281%2529%250Aprompt%2520learning%2520stage%2520and%2520%25282%2529%2520restoration%2520stage.%2520In%2520the%2520prompt%2520learning%2520stage%252C%250Awe%2520leverage%2520prompt%2520learning%2520to%2520acquire%2520a%2520fine-grained%2520quality%2520perceiver%2520capable%250Aof%2520distinguishing%2520three-tier%2520quality%2520levels%2520by%2520constraining%2520the%2520prompt-image%250Asimilarity%2520in%2520the%2520CLIP%2520perception%2520space.%2520Subsequently%252C%2520this%2520quality%2520perceiver%250Aand%2520difficulty-adaptive%2520perceptual%2520loss%2520are%2520integrated%2520as%2520a%2520quality-aware%250Alearning%2520strategy%2520to%2520realize%2520fine-grained%2520quality%2520control%2520in%2520restoration%2520stage.%250AFor%2520the%2520restoration%2520stage%252C%2520a%2520semantic%2520guidance%2520module%2520%2528SGM%2529%2520and%2520compact%2520feature%250Aextraction%2520%2528CFE%2529%2520are%2520proposed%2520to%2520further%2520promote%2520the%2520restoration%2520process%2520by%250Autilizing%2520the%2520robust%2520semantic%2520information%2520from%2520the%2520pre-trained%2520large%2520scale%250Avision%2520models%2520and%2520distinguishing%2520degradation-specific%2520features.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520Perceive-IR%2520outperforms%2520state-of-the-art%250Amethods%2520in%2520all-in-one%2520image%2520restoration%2520tasks%2520and%2520exhibit%2520superior%250Ageneralization%2520ability%2520when%2520dealing%2520with%2520unseen%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15994v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perceive-IR%3A%20Learning%20to%20Perceive%20Degradation%20Better%20for%20All-in-One%0A%20%20Image%20Restoration&entry.906535625=Xu%20Zhang%20and%20Jiaqi%20Ma%20and%20Guoli%20Wang%20and%20Qian%20Zhang%20and%20Huan%20Zhang%20and%20Lefei%20Zhang&entry.1292438233=%20%20The%20limitations%20of%20task-specific%20and%20general%20image%20restoration%20methods%20for%0Aspecific%20degradation%20have%20prompted%20the%20development%20of%20all-in-one%20image%0Arestoration%20techniques.%20However%2C%20the%20diversity%20of%20patterns%20among%20multiple%0Adegradation%2C%20along%20with%20the%20significant%20uncertainties%20in%20mapping%20between%0Adegraded%20images%20of%20different%20severities%20and%20their%20corresponding%20undistorted%0Aversions%2C%20pose%20significant%20challenges%20to%20the%20all-in-one%20restoration%20tasks.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20Perceive-IR%2C%20an%20all-in-one%20image%20restorer%0Adesigned%20to%20achieve%20fine-grained%20quality%20control%20that%20enables%20restored%20images%0Ato%20more%20closely%20resemble%20their%20undistorted%20counterparts%2C%20regardless%20of%20the%20type%0Aor%20severity%20of%20degradation.%20Specifically%2C%20Perceive-IR%20contains%20two%20stages%3A%20%281%29%0Aprompt%20learning%20stage%20and%20%282%29%20restoration%20stage.%20In%20the%20prompt%20learning%20stage%2C%0Awe%20leverage%20prompt%20learning%20to%20acquire%20a%20fine-grained%20quality%20perceiver%20capable%0Aof%20distinguishing%20three-tier%20quality%20levels%20by%20constraining%20the%20prompt-image%0Asimilarity%20in%20the%20CLIP%20perception%20space.%20Subsequently%2C%20this%20quality%20perceiver%0Aand%20difficulty-adaptive%20perceptual%20loss%20are%20integrated%20as%20a%20quality-aware%0Alearning%20strategy%20to%20realize%20fine-grained%20quality%20control%20in%20restoration%20stage.%0AFor%20the%20restoration%20stage%2C%20a%20semantic%20guidance%20module%20%28SGM%29%20and%20compact%20feature%0Aextraction%20%28CFE%29%20are%20proposed%20to%20further%20promote%20the%20restoration%20process%20by%0Autilizing%20the%20robust%20semantic%20information%20from%20the%20pre-trained%20large%20scale%0Avision%20models%20and%20distinguishing%20degradation-specific%20features.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20Perceive-IR%20outperforms%20state-of-the-art%0Amethods%20in%20all-in-one%20image%20restoration%20tasks%20and%20exhibit%20superior%0Ageneralization%20ability%20when%20dealing%20with%20unseen%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15994v1&entry.124074799=Read"},
{"title": "Harmonized Speculative Sampling", "author": "Lefan Zhang and Xiaodan Wang and Yanhua Huang and Ruiwen Xu", "abstract": "  Speculative sampling has proven to be an effective solution to accelerate\ndecoding from large language models, where the acceptance rate significantly\ndetermines the performance. Most previous works on improving the acceptance\nrate focus on aligned training and efficient decoding, implicitly paying less\nattention to the linkage of training and decoding. In this work, we first\ninvestigate the linkage of training and decoding for speculative sampling and\nthen propose a solution named HArmonized Speculative Sampling (HASS). HASS\nimproves the acceptance rate without extra inference overhead by harmonizing\ntraining and decoding on their objectives and contexts. Experiments on three\nLLaMA models demonstrate that HASS achieves 2.81x-3.65x wall-clock time speedup\nratio averaging across three datasets, which is 8%-15% faster than EAGLE-2.\n", "link": "http://arxiv.org/abs/2408.15766v1", "date": "2024-08-28", "relevancy": 2.0297, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5482}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4792}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harmonized%20Speculative%20Sampling&body=Title%3A%20Harmonized%20Speculative%20Sampling%0AAuthor%3A%20Lefan%20Zhang%20and%20Xiaodan%20Wang%20and%20Yanhua%20Huang%20and%20Ruiwen%20Xu%0AAbstract%3A%20%20%20Speculative%20sampling%20has%20proven%20to%20be%20an%20effective%20solution%20to%20accelerate%0Adecoding%20from%20large%20language%20models%2C%20where%20the%20acceptance%20rate%20significantly%0Adetermines%20the%20performance.%20Most%20previous%20works%20on%20improving%20the%20acceptance%0Arate%20focus%20on%20aligned%20training%20and%20efficient%20decoding%2C%20implicitly%20paying%20less%0Aattention%20to%20the%20linkage%20of%20training%20and%20decoding.%20In%20this%20work%2C%20we%20first%0Ainvestigate%20the%20linkage%20of%20training%20and%20decoding%20for%20speculative%20sampling%20and%0Athen%20propose%20a%20solution%20named%20HArmonized%20Speculative%20Sampling%20%28HASS%29.%20HASS%0Aimproves%20the%20acceptance%20rate%20without%20extra%20inference%20overhead%20by%20harmonizing%0Atraining%20and%20decoding%20on%20their%20objectives%20and%20contexts.%20Experiments%20on%20three%0ALLaMA%20models%20demonstrate%20that%20HASS%20achieves%202.81x-3.65x%20wall-clock%20time%20speedup%0Aratio%20averaging%20across%20three%20datasets%2C%20which%20is%208%25-15%25%20faster%20than%20EAGLE-2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarmonized%2520Speculative%2520Sampling%26entry.906535625%3DLefan%2520Zhang%2520and%2520Xiaodan%2520Wang%2520and%2520Yanhua%2520Huang%2520and%2520Ruiwen%2520Xu%26entry.1292438233%3D%2520%2520Speculative%2520sampling%2520has%2520proven%2520to%2520be%2520an%2520effective%2520solution%2520to%2520accelerate%250Adecoding%2520from%2520large%2520language%2520models%252C%2520where%2520the%2520acceptance%2520rate%2520significantly%250Adetermines%2520the%2520performance.%2520Most%2520previous%2520works%2520on%2520improving%2520the%2520acceptance%250Arate%2520focus%2520on%2520aligned%2520training%2520and%2520efficient%2520decoding%252C%2520implicitly%2520paying%2520less%250Aattention%2520to%2520the%2520linkage%2520of%2520training%2520and%2520decoding.%2520In%2520this%2520work%252C%2520we%2520first%250Ainvestigate%2520the%2520linkage%2520of%2520training%2520and%2520decoding%2520for%2520speculative%2520sampling%2520and%250Athen%2520propose%2520a%2520solution%2520named%2520HArmonized%2520Speculative%2520Sampling%2520%2528HASS%2529.%2520HASS%250Aimproves%2520the%2520acceptance%2520rate%2520without%2520extra%2520inference%2520overhead%2520by%2520harmonizing%250Atraining%2520and%2520decoding%2520on%2520their%2520objectives%2520and%2520contexts.%2520Experiments%2520on%2520three%250ALLaMA%2520models%2520demonstrate%2520that%2520HASS%2520achieves%25202.81x-3.65x%2520wall-clock%2520time%2520speedup%250Aratio%2520averaging%2520across%2520three%2520datasets%252C%2520which%2520is%25208%2525-15%2525%2520faster%2520than%2520EAGLE-2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harmonized%20Speculative%20Sampling&entry.906535625=Lefan%20Zhang%20and%20Xiaodan%20Wang%20and%20Yanhua%20Huang%20and%20Ruiwen%20Xu&entry.1292438233=%20%20Speculative%20sampling%20has%20proven%20to%20be%20an%20effective%20solution%20to%20accelerate%0Adecoding%20from%20large%20language%20models%2C%20where%20the%20acceptance%20rate%20significantly%0Adetermines%20the%20performance.%20Most%20previous%20works%20on%20improving%20the%20acceptance%0Arate%20focus%20on%20aligned%20training%20and%20efficient%20decoding%2C%20implicitly%20paying%20less%0Aattention%20to%20the%20linkage%20of%20training%20and%20decoding.%20In%20this%20work%2C%20we%20first%0Ainvestigate%20the%20linkage%20of%20training%20and%20decoding%20for%20speculative%20sampling%20and%0Athen%20propose%20a%20solution%20named%20HArmonized%20Speculative%20Sampling%20%28HASS%29.%20HASS%0Aimproves%20the%20acceptance%20rate%20without%20extra%20inference%20overhead%20by%20harmonizing%0Atraining%20and%20decoding%20on%20their%20objectives%20and%20contexts.%20Experiments%20on%20three%0ALLaMA%20models%20demonstrate%20that%20HASS%20achieves%202.81x-3.65x%20wall-clock%20time%20speedup%0Aratio%20averaging%20across%20three%20datasets%2C%20which%20is%208%25-15%25%20faster%20than%20EAGLE-2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15766v1&entry.124074799=Read"},
{"title": "Segmentation-guided Layer-wise Image Vectorization with Gradient Fills", "author": "Hengyu Zhou and Hui Zhang and Bin Wang", "abstract": "  The widespread use of vector graphics creates a significant demand for\nvectorization methods. While recent learning-based techniques have shown their\ncapability to create vector images of clear topology, filling these primitives\nwith gradients remains a challenge. In this paper, we propose a\nsegmentation-guided vectorization framework to convert raster images into\nconcise vector graphics with radial gradient fills. With the guidance of an\nembedded gradient-aware segmentation subroutine, our approach progressively\nappends gradient-filled B\\'ezier paths to the output, where primitive\nparameters are initiated with our newly designed initialization technique and\nare optimized to minimize our novel loss function. We build our method on a\ndifferentiable renderer with traditional segmentation algorithms to develop it\nas a model-free tool for raster-to-vector conversion. It is tested on various\ninputs to demonstrate its feasibility, independent of datasets, to synthesize\nvector graphics with improved visual quality and layer-wise topology compared\nto prior work.\n", "link": "http://arxiv.org/abs/2408.15741v1", "date": "2024-08-28", "relevancy": 2.0245, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5091}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5056}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segmentation-guided%20Layer-wise%20Image%20Vectorization%20with%20Gradient%20Fills&body=Title%3A%20Segmentation-guided%20Layer-wise%20Image%20Vectorization%20with%20Gradient%20Fills%0AAuthor%3A%20Hengyu%20Zhou%20and%20Hui%20Zhang%20and%20Bin%20Wang%0AAbstract%3A%20%20%20The%20widespread%20use%20of%20vector%20graphics%20creates%20a%20significant%20demand%20for%0Avectorization%20methods.%20While%20recent%20learning-based%20techniques%20have%20shown%20their%0Acapability%20to%20create%20vector%20images%20of%20clear%20topology%2C%20filling%20these%20primitives%0Awith%20gradients%20remains%20a%20challenge.%20In%20this%20paper%2C%20we%20propose%20a%0Asegmentation-guided%20vectorization%20framework%20to%20convert%20raster%20images%20into%0Aconcise%20vector%20graphics%20with%20radial%20gradient%20fills.%20With%20the%20guidance%20of%20an%0Aembedded%20gradient-aware%20segmentation%20subroutine%2C%20our%20approach%20progressively%0Aappends%20gradient-filled%20B%5C%27ezier%20paths%20to%20the%20output%2C%20where%20primitive%0Aparameters%20are%20initiated%20with%20our%20newly%20designed%20initialization%20technique%20and%0Aare%20optimized%20to%20minimize%20our%20novel%20loss%20function.%20We%20build%20our%20method%20on%20a%0Adifferentiable%20renderer%20with%20traditional%20segmentation%20algorithms%20to%20develop%20it%0Aas%20a%20model-free%20tool%20for%20raster-to-vector%20conversion.%20It%20is%20tested%20on%20various%0Ainputs%20to%20demonstrate%20its%20feasibility%2C%20independent%20of%20datasets%2C%20to%20synthesize%0Avector%20graphics%20with%20improved%20visual%20quality%20and%20layer-wise%20topology%20compared%0Ato%20prior%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15741v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegmentation-guided%2520Layer-wise%2520Image%2520Vectorization%2520with%2520Gradient%2520Fills%26entry.906535625%3DHengyu%2520Zhou%2520and%2520Hui%2520Zhang%2520and%2520Bin%2520Wang%26entry.1292438233%3D%2520%2520The%2520widespread%2520use%2520of%2520vector%2520graphics%2520creates%2520a%2520significant%2520demand%2520for%250Avectorization%2520methods.%2520While%2520recent%2520learning-based%2520techniques%2520have%2520shown%2520their%250Acapability%2520to%2520create%2520vector%2520images%2520of%2520clear%2520topology%252C%2520filling%2520these%2520primitives%250Awith%2520gradients%2520remains%2520a%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Asegmentation-guided%2520vectorization%2520framework%2520to%2520convert%2520raster%2520images%2520into%250Aconcise%2520vector%2520graphics%2520with%2520radial%2520gradient%2520fills.%2520With%2520the%2520guidance%2520of%2520an%250Aembedded%2520gradient-aware%2520segmentation%2520subroutine%252C%2520our%2520approach%2520progressively%250Aappends%2520gradient-filled%2520B%255C%2527ezier%2520paths%2520to%2520the%2520output%252C%2520where%2520primitive%250Aparameters%2520are%2520initiated%2520with%2520our%2520newly%2520designed%2520initialization%2520technique%2520and%250Aare%2520optimized%2520to%2520minimize%2520our%2520novel%2520loss%2520function.%2520We%2520build%2520our%2520method%2520on%2520a%250Adifferentiable%2520renderer%2520with%2520traditional%2520segmentation%2520algorithms%2520to%2520develop%2520it%250Aas%2520a%2520model-free%2520tool%2520for%2520raster-to-vector%2520conversion.%2520It%2520is%2520tested%2520on%2520various%250Ainputs%2520to%2520demonstrate%2520its%2520feasibility%252C%2520independent%2520of%2520datasets%252C%2520to%2520synthesize%250Avector%2520graphics%2520with%2520improved%2520visual%2520quality%2520and%2520layer-wise%2520topology%2520compared%250Ato%2520prior%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15741v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segmentation-guided%20Layer-wise%20Image%20Vectorization%20with%20Gradient%20Fills&entry.906535625=Hengyu%20Zhou%20and%20Hui%20Zhang%20and%20Bin%20Wang&entry.1292438233=%20%20The%20widespread%20use%20of%20vector%20graphics%20creates%20a%20significant%20demand%20for%0Avectorization%20methods.%20While%20recent%20learning-based%20techniques%20have%20shown%20their%0Acapability%20to%20create%20vector%20images%20of%20clear%20topology%2C%20filling%20these%20primitives%0Awith%20gradients%20remains%20a%20challenge.%20In%20this%20paper%2C%20we%20propose%20a%0Asegmentation-guided%20vectorization%20framework%20to%20convert%20raster%20images%20into%0Aconcise%20vector%20graphics%20with%20radial%20gradient%20fills.%20With%20the%20guidance%20of%20an%0Aembedded%20gradient-aware%20segmentation%20subroutine%2C%20our%20approach%20progressively%0Aappends%20gradient-filled%20B%5C%27ezier%20paths%20to%20the%20output%2C%20where%20primitive%0Aparameters%20are%20initiated%20with%20our%20newly%20designed%20initialization%20technique%20and%0Aare%20optimized%20to%20minimize%20our%20novel%20loss%20function.%20We%20build%20our%20method%20on%20a%0Adifferentiable%20renderer%20with%20traditional%20segmentation%20algorithms%20to%20develop%20it%0Aas%20a%20model-free%20tool%20for%20raster-to-vector%20conversion.%20It%20is%20tested%20on%20various%0Ainputs%20to%20demonstrate%20its%20feasibility%2C%20independent%20of%20datasets%2C%20to%20synthesize%0Avector%20graphics%20with%20improved%20visual%20quality%20and%20layer-wise%20topology%20compared%0Ato%20prior%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15741v1&entry.124074799=Read"},
{"title": "RecurrentGemma: Moving Past Transformers for Efficient Open Language\n  Models", "author": "Aleksandar Botev and Soham De and Samuel L Smith and Anushan Fernando and George-Cristian Muraru and Ruba Haroun and Leonard Berrada and Razvan Pascanu and Pier Giuseppe Sessa and Robert Dadashi and L\u00e9onard Hussenot and Johan Ferret and Sertan Girgin and Olivier Bachem and Alek Andreev and Kathleen Kenealy and Thomas Mesnard and Cassidy Hardin and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivi\u00e8re and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and Armand Joulin and Noah Fiedel and Evan Senter and Yutian Chen and Srivatsan Srinivasan and Guillaume Desjardins and David Budden and Arnaud Doucet and Sharad Vikram and Adam Paszke and Trevor Gale and Sebastian Borgeaud and Charlie Chen and Andy Brock and Antonia Paterson and Jenny Brennan and Meg Risdal and Raj Gundluru and Nesh Devanathan and Paul Mooney and Nilay Chauhan and Phil Culliton and Luiz Gustavo Martins and Elisa Bandy and David Huntsperger and Glenn Cameron and Arthur Zucker and Tris Warkentin and Ludovic Peran and Minh Giang and Zoubin Ghahramani and Cl\u00e9ment Farabet and Koray Kavukcuoglu and Demis Hassabis and Raia Hadsell and Yee Whye Teh and Nando de Frietas", "abstract": "  We introduce RecurrentGemma, a family of open language models which uses\nGoogle's novel Griffin architecture. Griffin combines linear recurrences with\nlocal attention to achieve excellent performance on language. It has a\nfixed-sized state, which reduces memory use and enables efficient inference on\nlong sequences. We provide two sizes of models, containing 2B and 9B\nparameters, and provide pre-trained and instruction tuned variants for both.\nOur models achieve comparable performance to similarly-sized Gemma baselines\ndespite being trained on fewer tokens.\n", "link": "http://arxiv.org/abs/2404.07839v2", "date": "2024-08-28", "relevancy": 2.0183, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5084}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5061}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RecurrentGemma%3A%20Moving%20Past%20Transformers%20for%20Efficient%20Open%20Language%0A%20%20Models&body=Title%3A%20RecurrentGemma%3A%20Moving%20Past%20Transformers%20for%20Efficient%20Open%20Language%0A%20%20Models%0AAuthor%3A%20Aleksandar%20Botev%20and%20Soham%20De%20and%20Samuel%20L%20Smith%20and%20Anushan%20Fernando%20and%20George-Cristian%20Muraru%20and%20Ruba%20Haroun%20and%20Leonard%20Berrada%20and%20Razvan%20Pascanu%20and%20Pier%20Giuseppe%20Sessa%20and%20Robert%20Dadashi%20and%20L%C3%A9onard%20Hussenot%20and%20Johan%20Ferret%20and%20Sertan%20Girgin%20and%20Olivier%20Bachem%20and%20Alek%20Andreev%20and%20Kathleen%20Kenealy%20and%20Thomas%20Mesnard%20and%20Cassidy%20Hardin%20and%20Surya%20Bhupatiraju%20and%20Shreya%20Pathak%20and%20Laurent%20Sifre%20and%20Morgane%20Rivi%C3%A8re%20and%20Mihir%20Sanjay%20Kale%20and%20Juliette%20Love%20and%20Pouya%20Tafti%20and%20Armand%20Joulin%20and%20Noah%20Fiedel%20and%20Evan%20Senter%20and%20Yutian%20Chen%20and%20Srivatsan%20Srinivasan%20and%20Guillaume%20Desjardins%20and%20David%20Budden%20and%20Arnaud%20Doucet%20and%20Sharad%20Vikram%20and%20Adam%20Paszke%20and%20Trevor%20Gale%20and%20Sebastian%20Borgeaud%20and%20Charlie%20Chen%20and%20Andy%20Brock%20and%20Antonia%20Paterson%20and%20Jenny%20Brennan%20and%20Meg%20Risdal%20and%20Raj%20Gundluru%20and%20Nesh%20Devanathan%20and%20Paul%20Mooney%20and%20Nilay%20Chauhan%20and%20Phil%20Culliton%20and%20Luiz%20Gustavo%20Martins%20and%20Elisa%20Bandy%20and%20David%20Huntsperger%20and%20Glenn%20Cameron%20and%20Arthur%20Zucker%20and%20Tris%20Warkentin%20and%20Ludovic%20Peran%20and%20Minh%20Giang%20and%20Zoubin%20Ghahramani%20and%20Cl%C3%A9ment%20Farabet%20and%20Koray%20Kavukcuoglu%20and%20Demis%20Hassabis%20and%20Raia%20Hadsell%20and%20Yee%20Whye%20Teh%20and%20Nando%20de%20Frietas%0AAbstract%3A%20%20%20We%20introduce%20RecurrentGemma%2C%20a%20family%20of%20open%20language%20models%20which%20uses%0AGoogle%27s%20novel%20Griffin%20architecture.%20Griffin%20combines%20linear%20recurrences%20with%0Alocal%20attention%20to%20achieve%20excellent%20performance%20on%20language.%20It%20has%20a%0Afixed-sized%20state%2C%20which%20reduces%20memory%20use%20and%20enables%20efficient%20inference%20on%0Along%20sequences.%20We%20provide%20two%20sizes%20of%20models%2C%20containing%202B%20and%209B%0Aparameters%2C%20and%20provide%20pre-trained%20and%20instruction%20tuned%20variants%20for%20both.%0AOur%20models%20achieve%20comparable%20performance%20to%20similarly-sized%20Gemma%20baselines%0Adespite%20being%20trained%20on%20fewer%20tokens.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.07839v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecurrentGemma%253A%2520Moving%2520Past%2520Transformers%2520for%2520Efficient%2520Open%2520Language%250A%2520%2520Models%26entry.906535625%3DAleksandar%2520Botev%2520and%2520Soham%2520De%2520and%2520Samuel%2520L%2520Smith%2520and%2520Anushan%2520Fernando%2520and%2520George-Cristian%2520Muraru%2520and%2520Ruba%2520Haroun%2520and%2520Leonard%2520Berrada%2520and%2520Razvan%2520Pascanu%2520and%2520Pier%2520Giuseppe%2520Sessa%2520and%2520Robert%2520Dadashi%2520and%2520L%25C3%25A9onard%2520Hussenot%2520and%2520Johan%2520Ferret%2520and%2520Sertan%2520Girgin%2520and%2520Olivier%2520Bachem%2520and%2520Alek%2520Andreev%2520and%2520Kathleen%2520Kenealy%2520and%2520Thomas%2520Mesnard%2520and%2520Cassidy%2520Hardin%2520and%2520Surya%2520Bhupatiraju%2520and%2520Shreya%2520Pathak%2520and%2520Laurent%2520Sifre%2520and%2520Morgane%2520Rivi%25C3%25A8re%2520and%2520Mihir%2520Sanjay%2520Kale%2520and%2520Juliette%2520Love%2520and%2520Pouya%2520Tafti%2520and%2520Armand%2520Joulin%2520and%2520Noah%2520Fiedel%2520and%2520Evan%2520Senter%2520and%2520Yutian%2520Chen%2520and%2520Srivatsan%2520Srinivasan%2520and%2520Guillaume%2520Desjardins%2520and%2520David%2520Budden%2520and%2520Arnaud%2520Doucet%2520and%2520Sharad%2520Vikram%2520and%2520Adam%2520Paszke%2520and%2520Trevor%2520Gale%2520and%2520Sebastian%2520Borgeaud%2520and%2520Charlie%2520Chen%2520and%2520Andy%2520Brock%2520and%2520Antonia%2520Paterson%2520and%2520Jenny%2520Brennan%2520and%2520Meg%2520Risdal%2520and%2520Raj%2520Gundluru%2520and%2520Nesh%2520Devanathan%2520and%2520Paul%2520Mooney%2520and%2520Nilay%2520Chauhan%2520and%2520Phil%2520Culliton%2520and%2520Luiz%2520Gustavo%2520Martins%2520and%2520Elisa%2520Bandy%2520and%2520David%2520Huntsperger%2520and%2520Glenn%2520Cameron%2520and%2520Arthur%2520Zucker%2520and%2520Tris%2520Warkentin%2520and%2520Ludovic%2520Peran%2520and%2520Minh%2520Giang%2520and%2520Zoubin%2520Ghahramani%2520and%2520Cl%25C3%25A9ment%2520Farabet%2520and%2520Koray%2520Kavukcuoglu%2520and%2520Demis%2520Hassabis%2520and%2520Raia%2520Hadsell%2520and%2520Yee%2520Whye%2520Teh%2520and%2520Nando%2520de%2520Frietas%26entry.1292438233%3D%2520%2520We%2520introduce%2520RecurrentGemma%252C%2520a%2520family%2520of%2520open%2520language%2520models%2520which%2520uses%250AGoogle%2527s%2520novel%2520Griffin%2520architecture.%2520Griffin%2520combines%2520linear%2520recurrences%2520with%250Alocal%2520attention%2520to%2520achieve%2520excellent%2520performance%2520on%2520language.%2520It%2520has%2520a%250Afixed-sized%2520state%252C%2520which%2520reduces%2520memory%2520use%2520and%2520enables%2520efficient%2520inference%2520on%250Along%2520sequences.%2520We%2520provide%2520two%2520sizes%2520of%2520models%252C%2520containing%25202B%2520and%25209B%250Aparameters%252C%2520and%2520provide%2520pre-trained%2520and%2520instruction%2520tuned%2520variants%2520for%2520both.%250AOur%2520models%2520achieve%2520comparable%2520performance%2520to%2520similarly-sized%2520Gemma%2520baselines%250Adespite%2520being%2520trained%2520on%2520fewer%2520tokens.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.07839v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RecurrentGemma%3A%20Moving%20Past%20Transformers%20for%20Efficient%20Open%20Language%0A%20%20Models&entry.906535625=Aleksandar%20Botev%20and%20Soham%20De%20and%20Samuel%20L%20Smith%20and%20Anushan%20Fernando%20and%20George-Cristian%20Muraru%20and%20Ruba%20Haroun%20and%20Leonard%20Berrada%20and%20Razvan%20Pascanu%20and%20Pier%20Giuseppe%20Sessa%20and%20Robert%20Dadashi%20and%20L%C3%A9onard%20Hussenot%20and%20Johan%20Ferret%20and%20Sertan%20Girgin%20and%20Olivier%20Bachem%20and%20Alek%20Andreev%20and%20Kathleen%20Kenealy%20and%20Thomas%20Mesnard%20and%20Cassidy%20Hardin%20and%20Surya%20Bhupatiraju%20and%20Shreya%20Pathak%20and%20Laurent%20Sifre%20and%20Morgane%20Rivi%C3%A8re%20and%20Mihir%20Sanjay%20Kale%20and%20Juliette%20Love%20and%20Pouya%20Tafti%20and%20Armand%20Joulin%20and%20Noah%20Fiedel%20and%20Evan%20Senter%20and%20Yutian%20Chen%20and%20Srivatsan%20Srinivasan%20and%20Guillaume%20Desjardins%20and%20David%20Budden%20and%20Arnaud%20Doucet%20and%20Sharad%20Vikram%20and%20Adam%20Paszke%20and%20Trevor%20Gale%20and%20Sebastian%20Borgeaud%20and%20Charlie%20Chen%20and%20Andy%20Brock%20and%20Antonia%20Paterson%20and%20Jenny%20Brennan%20and%20Meg%20Risdal%20and%20Raj%20Gundluru%20and%20Nesh%20Devanathan%20and%20Paul%20Mooney%20and%20Nilay%20Chauhan%20and%20Phil%20Culliton%20and%20Luiz%20Gustavo%20Martins%20and%20Elisa%20Bandy%20and%20David%20Huntsperger%20and%20Glenn%20Cameron%20and%20Arthur%20Zucker%20and%20Tris%20Warkentin%20and%20Ludovic%20Peran%20and%20Minh%20Giang%20and%20Zoubin%20Ghahramani%20and%20Cl%C3%A9ment%20Farabet%20and%20Koray%20Kavukcuoglu%20and%20Demis%20Hassabis%20and%20Raia%20Hadsell%20and%20Yee%20Whye%20Teh%20and%20Nando%20de%20Frietas&entry.1292438233=%20%20We%20introduce%20RecurrentGemma%2C%20a%20family%20of%20open%20language%20models%20which%20uses%0AGoogle%27s%20novel%20Griffin%20architecture.%20Griffin%20combines%20linear%20recurrences%20with%0Alocal%20attention%20to%20achieve%20excellent%20performance%20on%20language.%20It%20has%20a%0Afixed-sized%20state%2C%20which%20reduces%20memory%20use%20and%20enables%20efficient%20inference%20on%0Along%20sequences.%20We%20provide%20two%20sizes%20of%20models%2C%20containing%202B%20and%209B%0Aparameters%2C%20and%20provide%20pre-trained%20and%20instruction%20tuned%20variants%20for%20both.%0AOur%20models%20achieve%20comparable%20performance%20to%20similarly-sized%20Gemma%20baselines%0Adespite%20being%20trained%20on%20fewer%20tokens.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.07839v2&entry.124074799=Read"},
{"title": "Network transferability of adversarial patches in real-time object\n  detection", "author": "Jens Bayer and Stefan Becker and David M\u00fcnch and Michael Arens", "abstract": "  Adversarial patches in computer vision can be used, to fool deep neural\nnetworks and manipulate their decision-making process. One of the most\nprominent examples of adversarial patches are evasion attacks for object\ndetectors. By covering parts of objects of interest, these patches suppress the\ndetections and thus make the target object 'invisible' to the object detector.\nSince these patches are usually optimized on a specific network with a specific\ntrain dataset, the transferability across multiple networks and datasets is not\ngiven. This paper addresses these issues and investigates the transferability\nacross numerous object detector architectures. Our extensive evaluation across\nvarious models on two distinct datasets indicates that patches optimized with\nlarger models provide better network transferability than patches that are\noptimized with smaller models.\n", "link": "http://arxiv.org/abs/2408.15833v1", "date": "2024-08-28", "relevancy": 2.0162, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5201}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Network%20transferability%20of%20adversarial%20patches%20in%20real-time%20object%0A%20%20detection&body=Title%3A%20Network%20transferability%20of%20adversarial%20patches%20in%20real-time%20object%0A%20%20detection%0AAuthor%3A%20Jens%20Bayer%20and%20Stefan%20Becker%20and%20David%20M%C3%BCnch%20and%20Michael%20Arens%0AAbstract%3A%20%20%20Adversarial%20patches%20in%20computer%20vision%20can%20be%20used%2C%20to%20fool%20deep%20neural%0Anetworks%20and%20manipulate%20their%20decision-making%20process.%20One%20of%20the%20most%0Aprominent%20examples%20of%20adversarial%20patches%20are%20evasion%20attacks%20for%20object%0Adetectors.%20By%20covering%20parts%20of%20objects%20of%20interest%2C%20these%20patches%20suppress%20the%0Adetections%20and%20thus%20make%20the%20target%20object%20%27invisible%27%20to%20the%20object%20detector.%0ASince%20these%20patches%20are%20usually%20optimized%20on%20a%20specific%20network%20with%20a%20specific%0Atrain%20dataset%2C%20the%20transferability%20across%20multiple%20networks%20and%20datasets%20is%20not%0Agiven.%20This%20paper%20addresses%20these%20issues%20and%20investigates%20the%20transferability%0Aacross%20numerous%20object%20detector%20architectures.%20Our%20extensive%20evaluation%20across%0Avarious%20models%20on%20two%20distinct%20datasets%20indicates%20that%20patches%20optimized%20with%0Alarger%20models%20provide%20better%20network%20transferability%20than%20patches%20that%20are%0Aoptimized%20with%20smaller%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15833v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNetwork%2520transferability%2520of%2520adversarial%2520patches%2520in%2520real-time%2520object%250A%2520%2520detection%26entry.906535625%3DJens%2520Bayer%2520and%2520Stefan%2520Becker%2520and%2520David%2520M%25C3%25BCnch%2520and%2520Michael%2520Arens%26entry.1292438233%3D%2520%2520Adversarial%2520patches%2520in%2520computer%2520vision%2520can%2520be%2520used%252C%2520to%2520fool%2520deep%2520neural%250Anetworks%2520and%2520manipulate%2520their%2520decision-making%2520process.%2520One%2520of%2520the%2520most%250Aprominent%2520examples%2520of%2520adversarial%2520patches%2520are%2520evasion%2520attacks%2520for%2520object%250Adetectors.%2520By%2520covering%2520parts%2520of%2520objects%2520of%2520interest%252C%2520these%2520patches%2520suppress%2520the%250Adetections%2520and%2520thus%2520make%2520the%2520target%2520object%2520%2527invisible%2527%2520to%2520the%2520object%2520detector.%250ASince%2520these%2520patches%2520are%2520usually%2520optimized%2520on%2520a%2520specific%2520network%2520with%2520a%2520specific%250Atrain%2520dataset%252C%2520the%2520transferability%2520across%2520multiple%2520networks%2520and%2520datasets%2520is%2520not%250Agiven.%2520This%2520paper%2520addresses%2520these%2520issues%2520and%2520investigates%2520the%2520transferability%250Aacross%2520numerous%2520object%2520detector%2520architectures.%2520Our%2520extensive%2520evaluation%2520across%250Avarious%2520models%2520on%2520two%2520distinct%2520datasets%2520indicates%2520that%2520patches%2520optimized%2520with%250Alarger%2520models%2520provide%2520better%2520network%2520transferability%2520than%2520patches%2520that%2520are%250Aoptimized%2520with%2520smaller%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15833v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Network%20transferability%20of%20adversarial%20patches%20in%20real-time%20object%0A%20%20detection&entry.906535625=Jens%20Bayer%20and%20Stefan%20Becker%20and%20David%20M%C3%BCnch%20and%20Michael%20Arens&entry.1292438233=%20%20Adversarial%20patches%20in%20computer%20vision%20can%20be%20used%2C%20to%20fool%20deep%20neural%0Anetworks%20and%20manipulate%20their%20decision-making%20process.%20One%20of%20the%20most%0Aprominent%20examples%20of%20adversarial%20patches%20are%20evasion%20attacks%20for%20object%0Adetectors.%20By%20covering%20parts%20of%20objects%20of%20interest%2C%20these%20patches%20suppress%20the%0Adetections%20and%20thus%20make%20the%20target%20object%20%27invisible%27%20to%20the%20object%20detector.%0ASince%20these%20patches%20are%20usually%20optimized%20on%20a%20specific%20network%20with%20a%20specific%0Atrain%20dataset%2C%20the%20transferability%20across%20multiple%20networks%20and%20datasets%20is%20not%0Agiven.%20This%20paper%20addresses%20these%20issues%20and%20investigates%20the%20transferability%0Aacross%20numerous%20object%20detector%20architectures.%20Our%20extensive%20evaluation%20across%0Avarious%20models%20on%20two%20distinct%20datasets%20indicates%20that%20patches%20optimized%20with%0Alarger%20models%20provide%20better%20network%20transferability%20than%20patches%20that%20are%0Aoptimized%20with%20smaller%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15833v1&entry.124074799=Read"},
{"title": "Geometric Neural Network based on Phase Space for BCI-EEG decoding", "author": "Igor Carrara and Bruno Aristimunha and Marie-Constance Corsi and Raphael Y. de Camargo and Sylvain Chevallier and Th\u00e9odore Papadopoulo", "abstract": "  Objective: The integration of Deep Learning (DL) algorithms on brain signal\nanalysis is still in its nascent stages compared to their success in fields\nlike Computer Vision. This is particularly true for BCI, where the brain\nactivity is decoded to control external devices without requiring muscle\ncontrol. Electroencephalography (EEG) is a widely adopted choice for designing\nBCI systems due to its non-invasive and cost-effective nature and excellent\ntemporal resolution. Still, it comes at the expense of limited training data,\npoor signal-to-noise, and a large variability across and within-subject\nrecordings. Finally, setting up a BCI system with many electrodes takes a long\ntime, hindering the widespread adoption of reliable DL architectures in BCIs\noutside research laboratories. To improve adoption, we need to improve user\ncomfort using, for instance, reliable algorithms that operate with few\nelectrodes. Approach: Our research aims to develop a DL algorithm that delivers\neffective results with a limited number of electrodes. Taking advantage of the\nAugmented Covariance Method and the framework of SPDNet, we propose the\nPhase-SPDNet architecture and analyze its performance and the interpretability\nof the results. The evaluation is conducted on 5-fold cross-validation, using\nonly three electrodes positioned above the Motor Cortex. The methodology was\ntested on nearly 100 subjects from several open-source datasets using the\nMother Of All BCI Benchmark (MOABB) framework. Main results: The results of our\nPhase-SPDNet demonstrate that the augmented approach combined with the SPDNet\nsignificantly outperforms all the current state-of-the-art DL architecture in\nMI decoding. Significance: This new architecture is explainable and with a low\nnumber of trainable parameters.\n", "link": "http://arxiv.org/abs/2403.05645v3", "date": "2024-08-28", "relevancy": 2.0161, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5349}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4936}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Neural%20Network%20based%20on%20Phase%20Space%20for%20BCI-EEG%20decoding&body=Title%3A%20Geometric%20Neural%20Network%20based%20on%20Phase%20Space%20for%20BCI-EEG%20decoding%0AAuthor%3A%20Igor%20Carrara%20and%20Bruno%20Aristimunha%20and%20Marie-Constance%20Corsi%20and%20Raphael%20Y.%20de%20Camargo%20and%20Sylvain%20Chevallier%20and%20Th%C3%A9odore%20Papadopoulo%0AAbstract%3A%20%20%20Objective%3A%20The%20integration%20of%20Deep%20Learning%20%28DL%29%20algorithms%20on%20brain%20signal%0Aanalysis%20is%20still%20in%20its%20nascent%20stages%20compared%20to%20their%20success%20in%20fields%0Alike%20Computer%20Vision.%20This%20is%20particularly%20true%20for%20BCI%2C%20where%20the%20brain%0Aactivity%20is%20decoded%20to%20control%20external%20devices%20without%20requiring%20muscle%0Acontrol.%20Electroencephalography%20%28EEG%29%20is%20a%20widely%20adopted%20choice%20for%20designing%0ABCI%20systems%20due%20to%20its%20non-invasive%20and%20cost-effective%20nature%20and%20excellent%0Atemporal%20resolution.%20Still%2C%20it%20comes%20at%20the%20expense%20of%20limited%20training%20data%2C%0Apoor%20signal-to-noise%2C%20and%20a%20large%20variability%20across%20and%20within-subject%0Arecordings.%20Finally%2C%20setting%20up%20a%20BCI%20system%20with%20many%20electrodes%20takes%20a%20long%0Atime%2C%20hindering%20the%20widespread%20adoption%20of%20reliable%20DL%20architectures%20in%20BCIs%0Aoutside%20research%20laboratories.%20To%20improve%20adoption%2C%20we%20need%20to%20improve%20user%0Acomfort%20using%2C%20for%20instance%2C%20reliable%20algorithms%20that%20operate%20with%20few%0Aelectrodes.%20Approach%3A%20Our%20research%20aims%20to%20develop%20a%20DL%20algorithm%20that%20delivers%0Aeffective%20results%20with%20a%20limited%20number%20of%20electrodes.%20Taking%20advantage%20of%20the%0AAugmented%20Covariance%20Method%20and%20the%20framework%20of%20SPDNet%2C%20we%20propose%20the%0APhase-SPDNet%20architecture%20and%20analyze%20its%20performance%20and%20the%20interpretability%0Aof%20the%20results.%20The%20evaluation%20is%20conducted%20on%205-fold%20cross-validation%2C%20using%0Aonly%20three%20electrodes%20positioned%20above%20the%20Motor%20Cortex.%20The%20methodology%20was%0Atested%20on%20nearly%20100%20subjects%20from%20several%20open-source%20datasets%20using%20the%0AMother%20Of%20All%20BCI%20Benchmark%20%28MOABB%29%20framework.%20Main%20results%3A%20The%20results%20of%20our%0APhase-SPDNet%20demonstrate%20that%20the%20augmented%20approach%20combined%20with%20the%20SPDNet%0Asignificantly%20outperforms%20all%20the%20current%20state-of-the-art%20DL%20architecture%20in%0AMI%20decoding.%20Significance%3A%20This%20new%20architecture%20is%20explainable%20and%20with%20a%20low%0Anumber%20of%20trainable%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05645v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Neural%2520Network%2520based%2520on%2520Phase%2520Space%2520for%2520BCI-EEG%2520decoding%26entry.906535625%3DIgor%2520Carrara%2520and%2520Bruno%2520Aristimunha%2520and%2520Marie-Constance%2520Corsi%2520and%2520Raphael%2520Y.%2520de%2520Camargo%2520and%2520Sylvain%2520Chevallier%2520and%2520Th%25C3%25A9odore%2520Papadopoulo%26entry.1292438233%3D%2520%2520Objective%253A%2520The%2520integration%2520of%2520Deep%2520Learning%2520%2528DL%2529%2520algorithms%2520on%2520brain%2520signal%250Aanalysis%2520is%2520still%2520in%2520its%2520nascent%2520stages%2520compared%2520to%2520their%2520success%2520in%2520fields%250Alike%2520Computer%2520Vision.%2520This%2520is%2520particularly%2520true%2520for%2520BCI%252C%2520where%2520the%2520brain%250Aactivity%2520is%2520decoded%2520to%2520control%2520external%2520devices%2520without%2520requiring%2520muscle%250Acontrol.%2520Electroencephalography%2520%2528EEG%2529%2520is%2520a%2520widely%2520adopted%2520choice%2520for%2520designing%250ABCI%2520systems%2520due%2520to%2520its%2520non-invasive%2520and%2520cost-effective%2520nature%2520and%2520excellent%250Atemporal%2520resolution.%2520Still%252C%2520it%2520comes%2520at%2520the%2520expense%2520of%2520limited%2520training%2520data%252C%250Apoor%2520signal-to-noise%252C%2520and%2520a%2520large%2520variability%2520across%2520and%2520within-subject%250Arecordings.%2520Finally%252C%2520setting%2520up%2520a%2520BCI%2520system%2520with%2520many%2520electrodes%2520takes%2520a%2520long%250Atime%252C%2520hindering%2520the%2520widespread%2520adoption%2520of%2520reliable%2520DL%2520architectures%2520in%2520BCIs%250Aoutside%2520research%2520laboratories.%2520To%2520improve%2520adoption%252C%2520we%2520need%2520to%2520improve%2520user%250Acomfort%2520using%252C%2520for%2520instance%252C%2520reliable%2520algorithms%2520that%2520operate%2520with%2520few%250Aelectrodes.%2520Approach%253A%2520Our%2520research%2520aims%2520to%2520develop%2520a%2520DL%2520algorithm%2520that%2520delivers%250Aeffective%2520results%2520with%2520a%2520limited%2520number%2520of%2520electrodes.%2520Taking%2520advantage%2520of%2520the%250AAugmented%2520Covariance%2520Method%2520and%2520the%2520framework%2520of%2520SPDNet%252C%2520we%2520propose%2520the%250APhase-SPDNet%2520architecture%2520and%2520analyze%2520its%2520performance%2520and%2520the%2520interpretability%250Aof%2520the%2520results.%2520The%2520evaluation%2520is%2520conducted%2520on%25205-fold%2520cross-validation%252C%2520using%250Aonly%2520three%2520electrodes%2520positioned%2520above%2520the%2520Motor%2520Cortex.%2520The%2520methodology%2520was%250Atested%2520on%2520nearly%2520100%2520subjects%2520from%2520several%2520open-source%2520datasets%2520using%2520the%250AMother%2520Of%2520All%2520BCI%2520Benchmark%2520%2528MOABB%2529%2520framework.%2520Main%2520results%253A%2520The%2520results%2520of%2520our%250APhase-SPDNet%2520demonstrate%2520that%2520the%2520augmented%2520approach%2520combined%2520with%2520the%2520SPDNet%250Asignificantly%2520outperforms%2520all%2520the%2520current%2520state-of-the-art%2520DL%2520architecture%2520in%250AMI%2520decoding.%2520Significance%253A%2520This%2520new%2520architecture%2520is%2520explainable%2520and%2520with%2520a%2520low%250Anumber%2520of%2520trainable%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05645v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Neural%20Network%20based%20on%20Phase%20Space%20for%20BCI-EEG%20decoding&entry.906535625=Igor%20Carrara%20and%20Bruno%20Aristimunha%20and%20Marie-Constance%20Corsi%20and%20Raphael%20Y.%20de%20Camargo%20and%20Sylvain%20Chevallier%20and%20Th%C3%A9odore%20Papadopoulo&entry.1292438233=%20%20Objective%3A%20The%20integration%20of%20Deep%20Learning%20%28DL%29%20algorithms%20on%20brain%20signal%0Aanalysis%20is%20still%20in%20its%20nascent%20stages%20compared%20to%20their%20success%20in%20fields%0Alike%20Computer%20Vision.%20This%20is%20particularly%20true%20for%20BCI%2C%20where%20the%20brain%0Aactivity%20is%20decoded%20to%20control%20external%20devices%20without%20requiring%20muscle%0Acontrol.%20Electroencephalography%20%28EEG%29%20is%20a%20widely%20adopted%20choice%20for%20designing%0ABCI%20systems%20due%20to%20its%20non-invasive%20and%20cost-effective%20nature%20and%20excellent%0Atemporal%20resolution.%20Still%2C%20it%20comes%20at%20the%20expense%20of%20limited%20training%20data%2C%0Apoor%20signal-to-noise%2C%20and%20a%20large%20variability%20across%20and%20within-subject%0Arecordings.%20Finally%2C%20setting%20up%20a%20BCI%20system%20with%20many%20electrodes%20takes%20a%20long%0Atime%2C%20hindering%20the%20widespread%20adoption%20of%20reliable%20DL%20architectures%20in%20BCIs%0Aoutside%20research%20laboratories.%20To%20improve%20adoption%2C%20we%20need%20to%20improve%20user%0Acomfort%20using%2C%20for%20instance%2C%20reliable%20algorithms%20that%20operate%20with%20few%0Aelectrodes.%20Approach%3A%20Our%20research%20aims%20to%20develop%20a%20DL%20algorithm%20that%20delivers%0Aeffective%20results%20with%20a%20limited%20number%20of%20electrodes.%20Taking%20advantage%20of%20the%0AAugmented%20Covariance%20Method%20and%20the%20framework%20of%20SPDNet%2C%20we%20propose%20the%0APhase-SPDNet%20architecture%20and%20analyze%20its%20performance%20and%20the%20interpretability%0Aof%20the%20results.%20The%20evaluation%20is%20conducted%20on%205-fold%20cross-validation%2C%20using%0Aonly%20three%20electrodes%20positioned%20above%20the%20Motor%20Cortex.%20The%20methodology%20was%0Atested%20on%20nearly%20100%20subjects%20from%20several%20open-source%20datasets%20using%20the%0AMother%20Of%20All%20BCI%20Benchmark%20%28MOABB%29%20framework.%20Main%20results%3A%20The%20results%20of%20our%0APhase-SPDNet%20demonstrate%20that%20the%20augmented%20approach%20combined%20with%20the%20SPDNet%0Asignificantly%20outperforms%20all%20the%20current%20state-of-the-art%20DL%20architecture%20in%0AMI%20decoding.%20Significance%3A%20This%20new%20architecture%20is%20explainable%20and%20with%20a%20low%0Anumber%20of%20trainable%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05645v3&entry.124074799=Read"},
{"title": "On-Device Training of Fully Quantized Deep Neural Networks on Cortex-M\n  Microcontrollers", "author": "Mark Deutel and Frank Hannig and Christopher Mutschler and J\u00fcrgen Teich", "abstract": "  On-device training of DNNs allows models to adapt and fine-tune to newly\ncollected data or changing domains while deployed on microcontroller units\n(MCUs). However, DNN training is a resource-intensive task, making the\nimplementation and execution of DNN training algorithms on MCUs challenging due\nto low processor speeds, constrained throughput, limited floating-point\nsupport, and memory constraints. In this work, we explore on-device training of\nDNNs for Cortex-M MCUs. We present a method that enables efficient training of\nDNNs completely in place on the MCU using fully quantized training (FQT) and\ndynamic partial gradient updates. We demonstrate the feasibility of our\napproach on multiple vision and time-series datasets and provide insights into\nthe tradeoff between training accuracy, memory overhead, energy, and latency on\nreal hardware.\n", "link": "http://arxiv.org/abs/2407.10734v2", "date": "2024-08-28", "relevancy": 1.9975, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5376}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.512}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On-Device%20Training%20of%20Fully%20Quantized%20Deep%20Neural%20Networks%20on%20Cortex-M%0A%20%20Microcontrollers&body=Title%3A%20On-Device%20Training%20of%20Fully%20Quantized%20Deep%20Neural%20Networks%20on%20Cortex-M%0A%20%20Microcontrollers%0AAuthor%3A%20Mark%20Deutel%20and%20Frank%20Hannig%20and%20Christopher%20Mutschler%20and%20J%C3%BCrgen%20Teich%0AAbstract%3A%20%20%20On-device%20training%20of%20DNNs%20allows%20models%20to%20adapt%20and%20fine-tune%20to%20newly%0Acollected%20data%20or%20changing%20domains%20while%20deployed%20on%20microcontroller%20units%0A%28MCUs%29.%20However%2C%20DNN%20training%20is%20a%20resource-intensive%20task%2C%20making%20the%0Aimplementation%20and%20execution%20of%20DNN%20training%20algorithms%20on%20MCUs%20challenging%20due%0Ato%20low%20processor%20speeds%2C%20constrained%20throughput%2C%20limited%20floating-point%0Asupport%2C%20and%20memory%20constraints.%20In%20this%20work%2C%20we%20explore%20on-device%20training%20of%0ADNNs%20for%20Cortex-M%20MCUs.%20We%20present%20a%20method%20that%20enables%20efficient%20training%20of%0ADNNs%20completely%20in%20place%20on%20the%20MCU%20using%20fully%20quantized%20training%20%28FQT%29%20and%0Adynamic%20partial%20gradient%20updates.%20We%20demonstrate%20the%20feasibility%20of%20our%0Aapproach%20on%20multiple%20vision%20and%20time-series%20datasets%20and%20provide%20insights%20into%0Athe%20tradeoff%20between%20training%20accuracy%2C%20memory%20overhead%2C%20energy%2C%20and%20latency%20on%0Areal%20hardware.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.10734v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn-Device%2520Training%2520of%2520Fully%2520Quantized%2520Deep%2520Neural%2520Networks%2520on%2520Cortex-M%250A%2520%2520Microcontrollers%26entry.906535625%3DMark%2520Deutel%2520and%2520Frank%2520Hannig%2520and%2520Christopher%2520Mutschler%2520and%2520J%25C3%25BCrgen%2520Teich%26entry.1292438233%3D%2520%2520On-device%2520training%2520of%2520DNNs%2520allows%2520models%2520to%2520adapt%2520and%2520fine-tune%2520to%2520newly%250Acollected%2520data%2520or%2520changing%2520domains%2520while%2520deployed%2520on%2520microcontroller%2520units%250A%2528MCUs%2529.%2520However%252C%2520DNN%2520training%2520is%2520a%2520resource-intensive%2520task%252C%2520making%2520the%250Aimplementation%2520and%2520execution%2520of%2520DNN%2520training%2520algorithms%2520on%2520MCUs%2520challenging%2520due%250Ato%2520low%2520processor%2520speeds%252C%2520constrained%2520throughput%252C%2520limited%2520floating-point%250Asupport%252C%2520and%2520memory%2520constraints.%2520In%2520this%2520work%252C%2520we%2520explore%2520on-device%2520training%2520of%250ADNNs%2520for%2520Cortex-M%2520MCUs.%2520We%2520present%2520a%2520method%2520that%2520enables%2520efficient%2520training%2520of%250ADNNs%2520completely%2520in%2520place%2520on%2520the%2520MCU%2520using%2520fully%2520quantized%2520training%2520%2528FQT%2529%2520and%250Adynamic%2520partial%2520gradient%2520updates.%2520We%2520demonstrate%2520the%2520feasibility%2520of%2520our%250Aapproach%2520on%2520multiple%2520vision%2520and%2520time-series%2520datasets%2520and%2520provide%2520insights%2520into%250Athe%2520tradeoff%2520between%2520training%2520accuracy%252C%2520memory%2520overhead%252C%2520energy%252C%2520and%2520latency%2520on%250Areal%2520hardware.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.10734v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On-Device%20Training%20of%20Fully%20Quantized%20Deep%20Neural%20Networks%20on%20Cortex-M%0A%20%20Microcontrollers&entry.906535625=Mark%20Deutel%20and%20Frank%20Hannig%20and%20Christopher%20Mutschler%20and%20J%C3%BCrgen%20Teich&entry.1292438233=%20%20On-device%20training%20of%20DNNs%20allows%20models%20to%20adapt%20and%20fine-tune%20to%20newly%0Acollected%20data%20or%20changing%20domains%20while%20deployed%20on%20microcontroller%20units%0A%28MCUs%29.%20However%2C%20DNN%20training%20is%20a%20resource-intensive%20task%2C%20making%20the%0Aimplementation%20and%20execution%20of%20DNN%20training%20algorithms%20on%20MCUs%20challenging%20due%0Ato%20low%20processor%20speeds%2C%20constrained%20throughput%2C%20limited%20floating-point%0Asupport%2C%20and%20memory%20constraints.%20In%20this%20work%2C%20we%20explore%20on-device%20training%20of%0ADNNs%20for%20Cortex-M%20MCUs.%20We%20present%20a%20method%20that%20enables%20efficient%20training%20of%0ADNNs%20completely%20in%20place%20on%20the%20MCU%20using%20fully%20quantized%20training%20%28FQT%29%20and%0Adynamic%20partial%20gradient%20updates.%20We%20demonstrate%20the%20feasibility%20of%20our%0Aapproach%20on%20multiple%20vision%20and%20time-series%20datasets%20and%20provide%20insights%20into%0Athe%20tradeoff%20between%20training%20accuracy%2C%20memory%20overhead%2C%20energy%2C%20and%20latency%20on%0Areal%20hardware.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.10734v2&entry.124074799=Read"},
{"title": "AIM 2024 Challenge on Compressed Video Quality Assessment: Methods and\n  Results", "author": "Maksim Smirnov and Aleksandr Gushchin and Anastasia Antsiferova and Dmitry Vatolin and Radu Timofte and Ziheng Jia and Zicheng Zhang and Wei Sun and Jiaying Qian and Yuqin Cao and Yinan Sun and Yuxin Zhu and Xiongkuo Min and Guangtao Zhai and Kanjar De and Qing Luo and Ao-Xiang Zhang and Peng Zhang and Haibo Lei and Linyan Jiang and Yaqing Li and Wenhui Meng and Xiaoheng Tan and Haiqiang Wang and Xiaozhong Xu and Shan Liu and Zhenzhong Chen and Zhengxue Cheng and Jiahao Xiao and Jun Xu and Chenlong He and Qi Zheng and Ruoxi Zhu and Min Li and Yibo Fan and Zhengzhong Tu", "abstract": "  Video quality assessment (VQA) is a crucial task in the development of video\ncompression standards, as it directly impacts the viewer experience. This paper\npresents the results of the Compressed Video Quality Assessment challenge, held\nin conjunction with the Advances in Image Manipulation (AIM) workshop at ECCV\n2024. The challenge aimed to evaluate the performance of VQA methods on a\ndiverse dataset of 459 videos, encoded with 14 codecs of various compression\nstandards (AVC/H.264, HEVC/H.265, AV1, and VVC/H.266) and containing a\ncomprehensive collection of compression artifacts. To measure the methods\nperformance, we employed traditional correlation coefficients between their\npredictions and subjective scores, which were collected via large-scale\ncrowdsourced pairwise human comparisons. For training purposes, participants\nwere provided with the Compressed Video Quality Assessment Dataset (CVQAD), a\npreviously developed dataset of 1022 videos. Up to 30 participating teams\nregistered for the challenge, while we report the results of 6 teams, which\nsubmitted valid final solutions and code for reproducing the results. Moreover,\nwe calculated and present the performance of state-of-the-art VQA methods on\nthe developed dataset, providing a comprehensive benchmark for future research.\nThe dataset, results, and online leaderboard are publicly available at\nhttps://challenges.videoprocessing.ai/challenges/compressedvideo-quality-assessment.html.\n", "link": "http://arxiv.org/abs/2408.11982v2", "date": "2024-08-28", "relevancy": 1.9954, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5037}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5024}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIM%202024%20Challenge%20on%20Compressed%20Video%20Quality%20Assessment%3A%20Methods%20and%0A%20%20Results&body=Title%3A%20AIM%202024%20Challenge%20on%20Compressed%20Video%20Quality%20Assessment%3A%20Methods%20and%0A%20%20Results%0AAuthor%3A%20Maksim%20Smirnov%20and%20Aleksandr%20Gushchin%20and%20Anastasia%20Antsiferova%20and%20Dmitry%20Vatolin%20and%20Radu%20Timofte%20and%20Ziheng%20Jia%20and%20Zicheng%20Zhang%20and%20Wei%20Sun%20and%20Jiaying%20Qian%20and%20Yuqin%20Cao%20and%20Yinan%20Sun%20and%20Yuxin%20Zhu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%20and%20Kanjar%20De%20and%20Qing%20Luo%20and%20Ao-Xiang%20Zhang%20and%20Peng%20Zhang%20and%20Haibo%20Lei%20and%20Linyan%20Jiang%20and%20Yaqing%20Li%20and%20Wenhui%20Meng%20and%20Xiaoheng%20Tan%20and%20Haiqiang%20Wang%20and%20Xiaozhong%20Xu%20and%20Shan%20Liu%20and%20Zhenzhong%20Chen%20and%20Zhengxue%20Cheng%20and%20Jiahao%20Xiao%20and%20Jun%20Xu%20and%20Chenlong%20He%20and%20Qi%20Zheng%20and%20Ruoxi%20Zhu%20and%20Min%20Li%20and%20Yibo%20Fan%20and%20Zhengzhong%20Tu%0AAbstract%3A%20%20%20Video%20quality%20assessment%20%28VQA%29%20is%20a%20crucial%20task%20in%20the%20development%20of%20video%0Acompression%20standards%2C%20as%20it%20directly%20impacts%20the%20viewer%20experience.%20This%20paper%0Apresents%20the%20results%20of%20the%20Compressed%20Video%20Quality%20Assessment%20challenge%2C%20held%0Ain%20conjunction%20with%20the%20Advances%20in%20Image%20Manipulation%20%28AIM%29%20workshop%20at%20ECCV%0A2024.%20The%20challenge%20aimed%20to%20evaluate%20the%20performance%20of%20VQA%20methods%20on%20a%0Adiverse%20dataset%20of%20459%20videos%2C%20encoded%20with%2014%20codecs%20of%20various%20compression%0Astandards%20%28AVC/H.264%2C%20HEVC/H.265%2C%20AV1%2C%20and%20VVC/H.266%29%20and%20containing%20a%0Acomprehensive%20collection%20of%20compression%20artifacts.%20To%20measure%20the%20methods%0Aperformance%2C%20we%20employed%20traditional%20correlation%20coefficients%20between%20their%0Apredictions%20and%20subjective%20scores%2C%20which%20were%20collected%20via%20large-scale%0Acrowdsourced%20pairwise%20human%20comparisons.%20For%20training%20purposes%2C%20participants%0Awere%20provided%20with%20the%20Compressed%20Video%20Quality%20Assessment%20Dataset%20%28CVQAD%29%2C%20a%0Apreviously%20developed%20dataset%20of%201022%20videos.%20Up%20to%2030%20participating%20teams%0Aregistered%20for%20the%20challenge%2C%20while%20we%20report%20the%20results%20of%206%20teams%2C%20which%0Asubmitted%20valid%20final%20solutions%20and%20code%20for%20reproducing%20the%20results.%20Moreover%2C%0Awe%20calculated%20and%20present%20the%20performance%20of%20state-of-the-art%20VQA%20methods%20on%0Athe%20developed%20dataset%2C%20providing%20a%20comprehensive%20benchmark%20for%20future%20research.%0AThe%20dataset%2C%20results%2C%20and%20online%20leaderboard%20are%20publicly%20available%20at%0Ahttps%3A//challenges.videoprocessing.ai/challenges/compressedvideo-quality-assessment.html.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11982v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIM%25202024%2520Challenge%2520on%2520Compressed%2520Video%2520Quality%2520Assessment%253A%2520Methods%2520and%250A%2520%2520Results%26entry.906535625%3DMaksim%2520Smirnov%2520and%2520Aleksandr%2520Gushchin%2520and%2520Anastasia%2520Antsiferova%2520and%2520Dmitry%2520Vatolin%2520and%2520Radu%2520Timofte%2520and%2520Ziheng%2520Jia%2520and%2520Zicheng%2520Zhang%2520and%2520Wei%2520Sun%2520and%2520Jiaying%2520Qian%2520and%2520Yuqin%2520Cao%2520and%2520Yinan%2520Sun%2520and%2520Yuxin%2520Zhu%2520and%2520Xiongkuo%2520Min%2520and%2520Guangtao%2520Zhai%2520and%2520Kanjar%2520De%2520and%2520Qing%2520Luo%2520and%2520Ao-Xiang%2520Zhang%2520and%2520Peng%2520Zhang%2520and%2520Haibo%2520Lei%2520and%2520Linyan%2520Jiang%2520and%2520Yaqing%2520Li%2520and%2520Wenhui%2520Meng%2520and%2520Xiaoheng%2520Tan%2520and%2520Haiqiang%2520Wang%2520and%2520Xiaozhong%2520Xu%2520and%2520Shan%2520Liu%2520and%2520Zhenzhong%2520Chen%2520and%2520Zhengxue%2520Cheng%2520and%2520Jiahao%2520Xiao%2520and%2520Jun%2520Xu%2520and%2520Chenlong%2520He%2520and%2520Qi%2520Zheng%2520and%2520Ruoxi%2520Zhu%2520and%2520Min%2520Li%2520and%2520Yibo%2520Fan%2520and%2520Zhengzhong%2520Tu%26entry.1292438233%3D%2520%2520Video%2520quality%2520assessment%2520%2528VQA%2529%2520is%2520a%2520crucial%2520task%2520in%2520the%2520development%2520of%2520video%250Acompression%2520standards%252C%2520as%2520it%2520directly%2520impacts%2520the%2520viewer%2520experience.%2520This%2520paper%250Apresents%2520the%2520results%2520of%2520the%2520Compressed%2520Video%2520Quality%2520Assessment%2520challenge%252C%2520held%250Ain%2520conjunction%2520with%2520the%2520Advances%2520in%2520Image%2520Manipulation%2520%2528AIM%2529%2520workshop%2520at%2520ECCV%250A2024.%2520The%2520challenge%2520aimed%2520to%2520evaluate%2520the%2520performance%2520of%2520VQA%2520methods%2520on%2520a%250Adiverse%2520dataset%2520of%2520459%2520videos%252C%2520encoded%2520with%252014%2520codecs%2520of%2520various%2520compression%250Astandards%2520%2528AVC/H.264%252C%2520HEVC/H.265%252C%2520AV1%252C%2520and%2520VVC/H.266%2529%2520and%2520containing%2520a%250Acomprehensive%2520collection%2520of%2520compression%2520artifacts.%2520To%2520measure%2520the%2520methods%250Aperformance%252C%2520we%2520employed%2520traditional%2520correlation%2520coefficients%2520between%2520their%250Apredictions%2520and%2520subjective%2520scores%252C%2520which%2520were%2520collected%2520via%2520large-scale%250Acrowdsourced%2520pairwise%2520human%2520comparisons.%2520For%2520training%2520purposes%252C%2520participants%250Awere%2520provided%2520with%2520the%2520Compressed%2520Video%2520Quality%2520Assessment%2520Dataset%2520%2528CVQAD%2529%252C%2520a%250Apreviously%2520developed%2520dataset%2520of%25201022%2520videos.%2520Up%2520to%252030%2520participating%2520teams%250Aregistered%2520for%2520the%2520challenge%252C%2520while%2520we%2520report%2520the%2520results%2520of%25206%2520teams%252C%2520which%250Asubmitted%2520valid%2520final%2520solutions%2520and%2520code%2520for%2520reproducing%2520the%2520results.%2520Moreover%252C%250Awe%2520calculated%2520and%2520present%2520the%2520performance%2520of%2520state-of-the-art%2520VQA%2520methods%2520on%250Athe%2520developed%2520dataset%252C%2520providing%2520a%2520comprehensive%2520benchmark%2520for%2520future%2520research.%250AThe%2520dataset%252C%2520results%252C%2520and%2520online%2520leaderboard%2520are%2520publicly%2520available%2520at%250Ahttps%253A//challenges.videoprocessing.ai/challenges/compressedvideo-quality-assessment.html.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11982v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIM%202024%20Challenge%20on%20Compressed%20Video%20Quality%20Assessment%3A%20Methods%20and%0A%20%20Results&entry.906535625=Maksim%20Smirnov%20and%20Aleksandr%20Gushchin%20and%20Anastasia%20Antsiferova%20and%20Dmitry%20Vatolin%20and%20Radu%20Timofte%20and%20Ziheng%20Jia%20and%20Zicheng%20Zhang%20and%20Wei%20Sun%20and%20Jiaying%20Qian%20and%20Yuqin%20Cao%20and%20Yinan%20Sun%20and%20Yuxin%20Zhu%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%20and%20Kanjar%20De%20and%20Qing%20Luo%20and%20Ao-Xiang%20Zhang%20and%20Peng%20Zhang%20and%20Haibo%20Lei%20and%20Linyan%20Jiang%20and%20Yaqing%20Li%20and%20Wenhui%20Meng%20and%20Xiaoheng%20Tan%20and%20Haiqiang%20Wang%20and%20Xiaozhong%20Xu%20and%20Shan%20Liu%20and%20Zhenzhong%20Chen%20and%20Zhengxue%20Cheng%20and%20Jiahao%20Xiao%20and%20Jun%20Xu%20and%20Chenlong%20He%20and%20Qi%20Zheng%20and%20Ruoxi%20Zhu%20and%20Min%20Li%20and%20Yibo%20Fan%20and%20Zhengzhong%20Tu&entry.1292438233=%20%20Video%20quality%20assessment%20%28VQA%29%20is%20a%20crucial%20task%20in%20the%20development%20of%20video%0Acompression%20standards%2C%20as%20it%20directly%20impacts%20the%20viewer%20experience.%20This%20paper%0Apresents%20the%20results%20of%20the%20Compressed%20Video%20Quality%20Assessment%20challenge%2C%20held%0Ain%20conjunction%20with%20the%20Advances%20in%20Image%20Manipulation%20%28AIM%29%20workshop%20at%20ECCV%0A2024.%20The%20challenge%20aimed%20to%20evaluate%20the%20performance%20of%20VQA%20methods%20on%20a%0Adiverse%20dataset%20of%20459%20videos%2C%20encoded%20with%2014%20codecs%20of%20various%20compression%0Astandards%20%28AVC/H.264%2C%20HEVC/H.265%2C%20AV1%2C%20and%20VVC/H.266%29%20and%20containing%20a%0Acomprehensive%20collection%20of%20compression%20artifacts.%20To%20measure%20the%20methods%0Aperformance%2C%20we%20employed%20traditional%20correlation%20coefficients%20between%20their%0Apredictions%20and%20subjective%20scores%2C%20which%20were%20collected%20via%20large-scale%0Acrowdsourced%20pairwise%20human%20comparisons.%20For%20training%20purposes%2C%20participants%0Awere%20provided%20with%20the%20Compressed%20Video%20Quality%20Assessment%20Dataset%20%28CVQAD%29%2C%20a%0Apreviously%20developed%20dataset%20of%201022%20videos.%20Up%20to%2030%20participating%20teams%0Aregistered%20for%20the%20challenge%2C%20while%20we%20report%20the%20results%20of%206%20teams%2C%20which%0Asubmitted%20valid%20final%20solutions%20and%20code%20for%20reproducing%20the%20results.%20Moreover%2C%0Awe%20calculated%20and%20present%20the%20performance%20of%20state-of-the-art%20VQA%20methods%20on%0Athe%20developed%20dataset%2C%20providing%20a%20comprehensive%20benchmark%20for%20future%20research.%0AThe%20dataset%2C%20results%2C%20and%20online%20leaderboard%20are%20publicly%20available%20at%0Ahttps%3A//challenges.videoprocessing.ai/challenges/compressedvideo-quality-assessment.html.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11982v2&entry.124074799=Read"},
{"title": "Pixels to Prose: Understanding the art of Image Captioning", "author": "Hrishikesh Singh and Aarti Sharma and Millie Pant", "abstract": "  In the era of evolving artificial intelligence, machines are increasingly\nemulating human-like capabilities, including visual perception and linguistic\nexpression. Image captioning stands at the intersection of these domains,\nenabling machines to interpret visual content and generate descriptive text.\nThis paper provides a thorough review of image captioning techniques, catering\nto individuals entering the field of machine learning who seek a comprehensive\nunderstanding of available options, from foundational methods to\nstate-of-the-art approaches. Beginning with an exploration of primitive\narchitectures, the review traces the evolution of image captioning models to\nthe latest cutting-edge solutions. By dissecting the components of these\narchitectures, readers gain insights into the underlying mechanisms and can\nselect suitable approaches tailored to specific problem requirements without\nduplicating efforts. The paper also delves into the application of image\ncaptioning in the medical domain, illuminating its significance in various\nreal-world scenarios.\n  Furthermore, the review offers guidance on evaluating the performance of\nimage captioning systems, highlighting key metrics for assessment. By\nsynthesizing theoretical concepts with practical application, this paper equips\nreaders with the knowledge needed to navigate the complex landscape of image\ncaptioning and harness its potential for diverse applications in machine\nlearning and beyond.\n", "link": "http://arxiv.org/abs/2408.15714v1", "date": "2024-08-28", "relevancy": 1.9838, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.506}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4889}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pixels%20to%20Prose%3A%20Understanding%20the%20art%20of%20Image%20Captioning&body=Title%3A%20Pixels%20to%20Prose%3A%20Understanding%20the%20art%20of%20Image%20Captioning%0AAuthor%3A%20Hrishikesh%20Singh%20and%20Aarti%20Sharma%20and%20Millie%20Pant%0AAbstract%3A%20%20%20In%20the%20era%20of%20evolving%20artificial%20intelligence%2C%20machines%20are%20increasingly%0Aemulating%20human-like%20capabilities%2C%20including%20visual%20perception%20and%20linguistic%0Aexpression.%20Image%20captioning%20stands%20at%20the%20intersection%20of%20these%20domains%2C%0Aenabling%20machines%20to%20interpret%20visual%20content%20and%20generate%20descriptive%20text.%0AThis%20paper%20provides%20a%20thorough%20review%20of%20image%20captioning%20techniques%2C%20catering%0Ato%20individuals%20entering%20the%20field%20of%20machine%20learning%20who%20seek%20a%20comprehensive%0Aunderstanding%20of%20available%20options%2C%20from%20foundational%20methods%20to%0Astate-of-the-art%20approaches.%20Beginning%20with%20an%20exploration%20of%20primitive%0Aarchitectures%2C%20the%20review%20traces%20the%20evolution%20of%20image%20captioning%20models%20to%0Athe%20latest%20cutting-edge%20solutions.%20By%20dissecting%20the%20components%20of%20these%0Aarchitectures%2C%20readers%20gain%20insights%20into%20the%20underlying%20mechanisms%20and%20can%0Aselect%20suitable%20approaches%20tailored%20to%20specific%20problem%20requirements%20without%0Aduplicating%20efforts.%20The%20paper%20also%20delves%20into%20the%20application%20of%20image%0Acaptioning%20in%20the%20medical%20domain%2C%20illuminating%20its%20significance%20in%20various%0Areal-world%20scenarios.%0A%20%20Furthermore%2C%20the%20review%20offers%20guidance%20on%20evaluating%20the%20performance%20of%0Aimage%20captioning%20systems%2C%20highlighting%20key%20metrics%20for%20assessment.%20By%0Asynthesizing%20theoretical%20concepts%20with%20practical%20application%2C%20this%20paper%20equips%0Areaders%20with%20the%20knowledge%20needed%20to%20navigate%20the%20complex%20landscape%20of%20image%0Acaptioning%20and%20harness%20its%20potential%20for%20diverse%20applications%20in%20machine%0Alearning%20and%20beyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixels%2520to%2520Prose%253A%2520Understanding%2520the%2520art%2520of%2520Image%2520Captioning%26entry.906535625%3DHrishikesh%2520Singh%2520and%2520Aarti%2520Sharma%2520and%2520Millie%2520Pant%26entry.1292438233%3D%2520%2520In%2520the%2520era%2520of%2520evolving%2520artificial%2520intelligence%252C%2520machines%2520are%2520increasingly%250Aemulating%2520human-like%2520capabilities%252C%2520including%2520visual%2520perception%2520and%2520linguistic%250Aexpression.%2520Image%2520captioning%2520stands%2520at%2520the%2520intersection%2520of%2520these%2520domains%252C%250Aenabling%2520machines%2520to%2520interpret%2520visual%2520content%2520and%2520generate%2520descriptive%2520text.%250AThis%2520paper%2520provides%2520a%2520thorough%2520review%2520of%2520image%2520captioning%2520techniques%252C%2520catering%250Ato%2520individuals%2520entering%2520the%2520field%2520of%2520machine%2520learning%2520who%2520seek%2520a%2520comprehensive%250Aunderstanding%2520of%2520available%2520options%252C%2520from%2520foundational%2520methods%2520to%250Astate-of-the-art%2520approaches.%2520Beginning%2520with%2520an%2520exploration%2520of%2520primitive%250Aarchitectures%252C%2520the%2520review%2520traces%2520the%2520evolution%2520of%2520image%2520captioning%2520models%2520to%250Athe%2520latest%2520cutting-edge%2520solutions.%2520By%2520dissecting%2520the%2520components%2520of%2520these%250Aarchitectures%252C%2520readers%2520gain%2520insights%2520into%2520the%2520underlying%2520mechanisms%2520and%2520can%250Aselect%2520suitable%2520approaches%2520tailored%2520to%2520specific%2520problem%2520requirements%2520without%250Aduplicating%2520efforts.%2520The%2520paper%2520also%2520delves%2520into%2520the%2520application%2520of%2520image%250Acaptioning%2520in%2520the%2520medical%2520domain%252C%2520illuminating%2520its%2520significance%2520in%2520various%250Areal-world%2520scenarios.%250A%2520%2520Furthermore%252C%2520the%2520review%2520offers%2520guidance%2520on%2520evaluating%2520the%2520performance%2520of%250Aimage%2520captioning%2520systems%252C%2520highlighting%2520key%2520metrics%2520for%2520assessment.%2520By%250Asynthesizing%2520theoretical%2520concepts%2520with%2520practical%2520application%252C%2520this%2520paper%2520equips%250Areaders%2520with%2520the%2520knowledge%2520needed%2520to%2520navigate%2520the%2520complex%2520landscape%2520of%2520image%250Acaptioning%2520and%2520harness%2520its%2520potential%2520for%2520diverse%2520applications%2520in%2520machine%250Alearning%2520and%2520beyond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixels%20to%20Prose%3A%20Understanding%20the%20art%20of%20Image%20Captioning&entry.906535625=Hrishikesh%20Singh%20and%20Aarti%20Sharma%20and%20Millie%20Pant&entry.1292438233=%20%20In%20the%20era%20of%20evolving%20artificial%20intelligence%2C%20machines%20are%20increasingly%0Aemulating%20human-like%20capabilities%2C%20including%20visual%20perception%20and%20linguistic%0Aexpression.%20Image%20captioning%20stands%20at%20the%20intersection%20of%20these%20domains%2C%0Aenabling%20machines%20to%20interpret%20visual%20content%20and%20generate%20descriptive%20text.%0AThis%20paper%20provides%20a%20thorough%20review%20of%20image%20captioning%20techniques%2C%20catering%0Ato%20individuals%20entering%20the%20field%20of%20machine%20learning%20who%20seek%20a%20comprehensive%0Aunderstanding%20of%20available%20options%2C%20from%20foundational%20methods%20to%0Astate-of-the-art%20approaches.%20Beginning%20with%20an%20exploration%20of%20primitive%0Aarchitectures%2C%20the%20review%20traces%20the%20evolution%20of%20image%20captioning%20models%20to%0Athe%20latest%20cutting-edge%20solutions.%20By%20dissecting%20the%20components%20of%20these%0Aarchitectures%2C%20readers%20gain%20insights%20into%20the%20underlying%20mechanisms%20and%20can%0Aselect%20suitable%20approaches%20tailored%20to%20specific%20problem%20requirements%20without%0Aduplicating%20efforts.%20The%20paper%20also%20delves%20into%20the%20application%20of%20image%0Acaptioning%20in%20the%20medical%20domain%2C%20illuminating%20its%20significance%20in%20various%0Areal-world%20scenarios.%0A%20%20Furthermore%2C%20the%20review%20offers%20guidance%20on%20evaluating%20the%20performance%20of%0Aimage%20captioning%20systems%2C%20highlighting%20key%20metrics%20for%20assessment.%20By%0Asynthesizing%20theoretical%20concepts%20with%20practical%20application%2C%20this%20paper%20equips%0Areaders%20with%20the%20knowledge%20needed%20to%20navigate%20the%20complex%20landscape%20of%20image%0Acaptioning%20and%20harness%20its%20potential%20for%20diverse%20applications%20in%20machine%0Alearning%20and%20beyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15714v1&entry.124074799=Read"},
{"title": "Deep Learning Based Speckle Filtering for Polarimetric SAR Images.\n  Application to Sentinel-1", "author": "Alejandro Mestre-Quereda and Juan M. Lopez-Sanchez", "abstract": "  Speckle suppression in synthetic aperture radar (SAR) images is a key\nprocessing step which continues to be a research topic. A wide variety of\nmethods, using either spatially-based approaches or transform-based strategies,\nhave been developed and have shown to provide outstanding results. However,\nrecent advances in deep learning techniques and their application to SAR image\ndespeckling have been demonstrated to offer state-of-the-art results.\nUnfortunately, they have been mostly applied to single-polarimetric images. The\nextension of a deep learning-based approach for speckle removal to polarimetric\nSAR (PolSAR) images is complicated because of the complex nature of the\nmeasured covariance matrices for every image pixel, the properties of which\nmust be preserved during filtering. In this work, we propose a complete\nframework to remove speckle in polarimetric SAR images using a convolutional\nneural network. The methodology includes a reversible transformation of the\noriginal complex covariance matrix to obtain a set of real-valued intensity\nbands which are fed to the neural network. In addition, the proposed method\nincludes a change detection strategy to avoid the neural network to learn\nerroneous features in areas strongly affected by temporal changes, so that the\nnetwork only learns the underlying speckle component present in the data. The\nmethod is implemented and tested with dual-polarimetric images acquired by\nSentinel-1. Experiments show that the proposed approach offers exceptional\nresults in both speckle reduction and resolution preservation. More\nimportantly, it is also shown that the neural network is not generating\nartifacts or introducing bias in the filtered images, making them suitable for\nfurther polarimetric processing and exploitation.\n", "link": "http://arxiv.org/abs/2408.15678v1", "date": "2024-08-28", "relevancy": 1.9834, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5281}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4907}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20Based%20Speckle%20Filtering%20for%20Polarimetric%20SAR%20Images.%0A%20%20Application%20to%20Sentinel-1&body=Title%3A%20Deep%20Learning%20Based%20Speckle%20Filtering%20for%20Polarimetric%20SAR%20Images.%0A%20%20Application%20to%20Sentinel-1%0AAuthor%3A%20Alejandro%20Mestre-Quereda%20and%20Juan%20M.%20Lopez-Sanchez%0AAbstract%3A%20%20%20Speckle%20suppression%20in%20synthetic%20aperture%20radar%20%28SAR%29%20images%20is%20a%20key%0Aprocessing%20step%20which%20continues%20to%20be%20a%20research%20topic.%20A%20wide%20variety%20of%0Amethods%2C%20using%20either%20spatially-based%20approaches%20or%20transform-based%20strategies%2C%0Ahave%20been%20developed%20and%20have%20shown%20to%20provide%20outstanding%20results.%20However%2C%0Arecent%20advances%20in%20deep%20learning%20techniques%20and%20their%20application%20to%20SAR%20image%0Adespeckling%20have%20been%20demonstrated%20to%20offer%20state-of-the-art%20results.%0AUnfortunately%2C%20they%20have%20been%20mostly%20applied%20to%20single-polarimetric%20images.%20The%0Aextension%20of%20a%20deep%20learning-based%20approach%20for%20speckle%20removal%20to%20polarimetric%0ASAR%20%28PolSAR%29%20images%20is%20complicated%20because%20of%20the%20complex%20nature%20of%20the%0Ameasured%20covariance%20matrices%20for%20every%20image%20pixel%2C%20the%20properties%20of%20which%0Amust%20be%20preserved%20during%20filtering.%20In%20this%20work%2C%20we%20propose%20a%20complete%0Aframework%20to%20remove%20speckle%20in%20polarimetric%20SAR%20images%20using%20a%20convolutional%0Aneural%20network.%20The%20methodology%20includes%20a%20reversible%20transformation%20of%20the%0Aoriginal%20complex%20covariance%20matrix%20to%20obtain%20a%20set%20of%20real-valued%20intensity%0Abands%20which%20are%20fed%20to%20the%20neural%20network.%20In%20addition%2C%20the%20proposed%20method%0Aincludes%20a%20change%20detection%20strategy%20to%20avoid%20the%20neural%20network%20to%20learn%0Aerroneous%20features%20in%20areas%20strongly%20affected%20by%20temporal%20changes%2C%20so%20that%20the%0Anetwork%20only%20learns%20the%20underlying%20speckle%20component%20present%20in%20the%20data.%20The%0Amethod%20is%20implemented%20and%20tested%20with%20dual-polarimetric%20images%20acquired%20by%0ASentinel-1.%20Experiments%20show%20that%20the%20proposed%20approach%20offers%20exceptional%0Aresults%20in%20both%20speckle%20reduction%20and%20resolution%20preservation.%20More%0Aimportantly%2C%20it%20is%20also%20shown%20that%20the%20neural%20network%20is%20not%20generating%0Aartifacts%20or%20introducing%20bias%20in%20the%20filtered%20images%2C%20making%20them%20suitable%20for%0Afurther%20polarimetric%20processing%20and%20exploitation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520Based%2520Speckle%2520Filtering%2520for%2520Polarimetric%2520SAR%2520Images.%250A%2520%2520Application%2520to%2520Sentinel-1%26entry.906535625%3DAlejandro%2520Mestre-Quereda%2520and%2520Juan%2520M.%2520Lopez-Sanchez%26entry.1292438233%3D%2520%2520Speckle%2520suppression%2520in%2520synthetic%2520aperture%2520radar%2520%2528SAR%2529%2520images%2520is%2520a%2520key%250Aprocessing%2520step%2520which%2520continues%2520to%2520be%2520a%2520research%2520topic.%2520A%2520wide%2520variety%2520of%250Amethods%252C%2520using%2520either%2520spatially-based%2520approaches%2520or%2520transform-based%2520strategies%252C%250Ahave%2520been%2520developed%2520and%2520have%2520shown%2520to%2520provide%2520outstanding%2520results.%2520However%252C%250Arecent%2520advances%2520in%2520deep%2520learning%2520techniques%2520and%2520their%2520application%2520to%2520SAR%2520image%250Adespeckling%2520have%2520been%2520demonstrated%2520to%2520offer%2520state-of-the-art%2520results.%250AUnfortunately%252C%2520they%2520have%2520been%2520mostly%2520applied%2520to%2520single-polarimetric%2520images.%2520The%250Aextension%2520of%2520a%2520deep%2520learning-based%2520approach%2520for%2520speckle%2520removal%2520to%2520polarimetric%250ASAR%2520%2528PolSAR%2529%2520images%2520is%2520complicated%2520because%2520of%2520the%2520complex%2520nature%2520of%2520the%250Ameasured%2520covariance%2520matrices%2520for%2520every%2520image%2520pixel%252C%2520the%2520properties%2520of%2520which%250Amust%2520be%2520preserved%2520during%2520filtering.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520complete%250Aframework%2520to%2520remove%2520speckle%2520in%2520polarimetric%2520SAR%2520images%2520using%2520a%2520convolutional%250Aneural%2520network.%2520The%2520methodology%2520includes%2520a%2520reversible%2520transformation%2520of%2520the%250Aoriginal%2520complex%2520covariance%2520matrix%2520to%2520obtain%2520a%2520set%2520of%2520real-valued%2520intensity%250Abands%2520which%2520are%2520fed%2520to%2520the%2520neural%2520network.%2520In%2520addition%252C%2520the%2520proposed%2520method%250Aincludes%2520a%2520change%2520detection%2520strategy%2520to%2520avoid%2520the%2520neural%2520network%2520to%2520learn%250Aerroneous%2520features%2520in%2520areas%2520strongly%2520affected%2520by%2520temporal%2520changes%252C%2520so%2520that%2520the%250Anetwork%2520only%2520learns%2520the%2520underlying%2520speckle%2520component%2520present%2520in%2520the%2520data.%2520The%250Amethod%2520is%2520implemented%2520and%2520tested%2520with%2520dual-polarimetric%2520images%2520acquired%2520by%250ASentinel-1.%2520Experiments%2520show%2520that%2520the%2520proposed%2520approach%2520offers%2520exceptional%250Aresults%2520in%2520both%2520speckle%2520reduction%2520and%2520resolution%2520preservation.%2520More%250Aimportantly%252C%2520it%2520is%2520also%2520shown%2520that%2520the%2520neural%2520network%2520is%2520not%2520generating%250Aartifacts%2520or%2520introducing%2520bias%2520in%2520the%2520filtered%2520images%252C%2520making%2520them%2520suitable%2520for%250Afurther%2520polarimetric%2520processing%2520and%2520exploitation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Based%20Speckle%20Filtering%20for%20Polarimetric%20SAR%20Images.%0A%20%20Application%20to%20Sentinel-1&entry.906535625=Alejandro%20Mestre-Quereda%20and%20Juan%20M.%20Lopez-Sanchez&entry.1292438233=%20%20Speckle%20suppression%20in%20synthetic%20aperture%20radar%20%28SAR%29%20images%20is%20a%20key%0Aprocessing%20step%20which%20continues%20to%20be%20a%20research%20topic.%20A%20wide%20variety%20of%0Amethods%2C%20using%20either%20spatially-based%20approaches%20or%20transform-based%20strategies%2C%0Ahave%20been%20developed%20and%20have%20shown%20to%20provide%20outstanding%20results.%20However%2C%0Arecent%20advances%20in%20deep%20learning%20techniques%20and%20their%20application%20to%20SAR%20image%0Adespeckling%20have%20been%20demonstrated%20to%20offer%20state-of-the-art%20results.%0AUnfortunately%2C%20they%20have%20been%20mostly%20applied%20to%20single-polarimetric%20images.%20The%0Aextension%20of%20a%20deep%20learning-based%20approach%20for%20speckle%20removal%20to%20polarimetric%0ASAR%20%28PolSAR%29%20images%20is%20complicated%20because%20of%20the%20complex%20nature%20of%20the%0Ameasured%20covariance%20matrices%20for%20every%20image%20pixel%2C%20the%20properties%20of%20which%0Amust%20be%20preserved%20during%20filtering.%20In%20this%20work%2C%20we%20propose%20a%20complete%0Aframework%20to%20remove%20speckle%20in%20polarimetric%20SAR%20images%20using%20a%20convolutional%0Aneural%20network.%20The%20methodology%20includes%20a%20reversible%20transformation%20of%20the%0Aoriginal%20complex%20covariance%20matrix%20to%20obtain%20a%20set%20of%20real-valued%20intensity%0Abands%20which%20are%20fed%20to%20the%20neural%20network.%20In%20addition%2C%20the%20proposed%20method%0Aincludes%20a%20change%20detection%20strategy%20to%20avoid%20the%20neural%20network%20to%20learn%0Aerroneous%20features%20in%20areas%20strongly%20affected%20by%20temporal%20changes%2C%20so%20that%20the%0Anetwork%20only%20learns%20the%20underlying%20speckle%20component%20present%20in%20the%20data.%20The%0Amethod%20is%20implemented%20and%20tested%20with%20dual-polarimetric%20images%20acquired%20by%0ASentinel-1.%20Experiments%20show%20that%20the%20proposed%20approach%20offers%20exceptional%0Aresults%20in%20both%20speckle%20reduction%20and%20resolution%20preservation.%20More%0Aimportantly%2C%20it%20is%20also%20shown%20that%20the%20neural%20network%20is%20not%20generating%0Aartifacts%20or%20introducing%20bias%20in%20the%20filtered%20images%2C%20making%20them%20suitable%20for%0Afurther%20polarimetric%20processing%20and%20exploitation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15678v1&entry.124074799=Read"},
{"title": "Generating Binary Species Range Maps", "author": "Filip Dorm and Christian Lange and Scott Loarie and Oisin Mac Aodha", "abstract": "  Accurately predicting the geographic ranges of species is crucial for\nassisting conservation efforts. Traditionally, range maps were manually created\nby experts. However, species distribution models (SDMs) and, more recently,\ndeep learning-based variants offer a potential automated alternative. Deep\nlearning-based SDMs generate a continuous probability representing the\npredicted presence of a species at a given location, which must be binarized by\nsetting per-species thresholds to obtain binary range maps. However, selecting\nappropriate per-species thresholds to binarize these predictions is non-trivial\nas different species can require distinct thresholds. In this work, we evaluate\ndifferent approaches for automatically identifying the best thresholds for\nbinarizing range maps using presence-only data. This includes approaches that\nrequire the generation of additional pseudo-absence data, along with ones that\nonly require presence data. We also propose an extension of an existing\npresence-only technique that is more robust to outliers. We perform a detailed\nevaluation of different thresholding techniques on the tasks of binary range\nestimation and large-scale fine-grained visual classification, and we\ndemonstrate improved performance over existing pseudo-absence free approaches\nusing our method.\n", "link": "http://arxiv.org/abs/2408.15956v1", "date": "2024-08-28", "relevancy": 1.9735, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5071}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4914}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generating%20Binary%20Species%20Range%20Maps&body=Title%3A%20Generating%20Binary%20Species%20Range%20Maps%0AAuthor%3A%20Filip%20Dorm%20and%20Christian%20Lange%20and%20Scott%20Loarie%20and%20Oisin%20Mac%20Aodha%0AAbstract%3A%20%20%20Accurately%20predicting%20the%20geographic%20ranges%20of%20species%20is%20crucial%20for%0Aassisting%20conservation%20efforts.%20Traditionally%2C%20range%20maps%20were%20manually%20created%0Aby%20experts.%20However%2C%20species%20distribution%20models%20%28SDMs%29%20and%2C%20more%20recently%2C%0Adeep%20learning-based%20variants%20offer%20a%20potential%20automated%20alternative.%20Deep%0Alearning-based%20SDMs%20generate%20a%20continuous%20probability%20representing%20the%0Apredicted%20presence%20of%20a%20species%20at%20a%20given%20location%2C%20which%20must%20be%20binarized%20by%0Asetting%20per-species%20thresholds%20to%20obtain%20binary%20range%20maps.%20However%2C%20selecting%0Aappropriate%20per-species%20thresholds%20to%20binarize%20these%20predictions%20is%20non-trivial%0Aas%20different%20species%20can%20require%20distinct%20thresholds.%20In%20this%20work%2C%20we%20evaluate%0Adifferent%20approaches%20for%20automatically%20identifying%20the%20best%20thresholds%20for%0Abinarizing%20range%20maps%20using%20presence-only%20data.%20This%20includes%20approaches%20that%0Arequire%20the%20generation%20of%20additional%20pseudo-absence%20data%2C%20along%20with%20ones%20that%0Aonly%20require%20presence%20data.%20We%20also%20propose%20an%20extension%20of%20an%20existing%0Apresence-only%20technique%20that%20is%20more%20robust%20to%20outliers.%20We%20perform%20a%20detailed%0Aevaluation%20of%20different%20thresholding%20techniques%20on%20the%20tasks%20of%20binary%20range%0Aestimation%20and%20large-scale%20fine-grained%20visual%20classification%2C%20and%20we%0Ademonstrate%20improved%20performance%20over%20existing%20pseudo-absence%20free%20approaches%0Ausing%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15956v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerating%2520Binary%2520Species%2520Range%2520Maps%26entry.906535625%3DFilip%2520Dorm%2520and%2520Christian%2520Lange%2520and%2520Scott%2520Loarie%2520and%2520Oisin%2520Mac%2520Aodha%26entry.1292438233%3D%2520%2520Accurately%2520predicting%2520the%2520geographic%2520ranges%2520of%2520species%2520is%2520crucial%2520for%250Aassisting%2520conservation%2520efforts.%2520Traditionally%252C%2520range%2520maps%2520were%2520manually%2520created%250Aby%2520experts.%2520However%252C%2520species%2520distribution%2520models%2520%2528SDMs%2529%2520and%252C%2520more%2520recently%252C%250Adeep%2520learning-based%2520variants%2520offer%2520a%2520potential%2520automated%2520alternative.%2520Deep%250Alearning-based%2520SDMs%2520generate%2520a%2520continuous%2520probability%2520representing%2520the%250Apredicted%2520presence%2520of%2520a%2520species%2520at%2520a%2520given%2520location%252C%2520which%2520must%2520be%2520binarized%2520by%250Asetting%2520per-species%2520thresholds%2520to%2520obtain%2520binary%2520range%2520maps.%2520However%252C%2520selecting%250Aappropriate%2520per-species%2520thresholds%2520to%2520binarize%2520these%2520predictions%2520is%2520non-trivial%250Aas%2520different%2520species%2520can%2520require%2520distinct%2520thresholds.%2520In%2520this%2520work%252C%2520we%2520evaluate%250Adifferent%2520approaches%2520for%2520automatically%2520identifying%2520the%2520best%2520thresholds%2520for%250Abinarizing%2520range%2520maps%2520using%2520presence-only%2520data.%2520This%2520includes%2520approaches%2520that%250Arequire%2520the%2520generation%2520of%2520additional%2520pseudo-absence%2520data%252C%2520along%2520with%2520ones%2520that%250Aonly%2520require%2520presence%2520data.%2520We%2520also%2520propose%2520an%2520extension%2520of%2520an%2520existing%250Apresence-only%2520technique%2520that%2520is%2520more%2520robust%2520to%2520outliers.%2520We%2520perform%2520a%2520detailed%250Aevaluation%2520of%2520different%2520thresholding%2520techniques%2520on%2520the%2520tasks%2520of%2520binary%2520range%250Aestimation%2520and%2520large-scale%2520fine-grained%2520visual%2520classification%252C%2520and%2520we%250Ademonstrate%2520improved%2520performance%2520over%2520existing%2520pseudo-absence%2520free%2520approaches%250Ausing%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15956v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generating%20Binary%20Species%20Range%20Maps&entry.906535625=Filip%20Dorm%20and%20Christian%20Lange%20and%20Scott%20Loarie%20and%20Oisin%20Mac%20Aodha&entry.1292438233=%20%20Accurately%20predicting%20the%20geographic%20ranges%20of%20species%20is%20crucial%20for%0Aassisting%20conservation%20efforts.%20Traditionally%2C%20range%20maps%20were%20manually%20created%0Aby%20experts.%20However%2C%20species%20distribution%20models%20%28SDMs%29%20and%2C%20more%20recently%2C%0Adeep%20learning-based%20variants%20offer%20a%20potential%20automated%20alternative.%20Deep%0Alearning-based%20SDMs%20generate%20a%20continuous%20probability%20representing%20the%0Apredicted%20presence%20of%20a%20species%20at%20a%20given%20location%2C%20which%20must%20be%20binarized%20by%0Asetting%20per-species%20thresholds%20to%20obtain%20binary%20range%20maps.%20However%2C%20selecting%0Aappropriate%20per-species%20thresholds%20to%20binarize%20these%20predictions%20is%20non-trivial%0Aas%20different%20species%20can%20require%20distinct%20thresholds.%20In%20this%20work%2C%20we%20evaluate%0Adifferent%20approaches%20for%20automatically%20identifying%20the%20best%20thresholds%20for%0Abinarizing%20range%20maps%20using%20presence-only%20data.%20This%20includes%20approaches%20that%0Arequire%20the%20generation%20of%20additional%20pseudo-absence%20data%2C%20along%20with%20ones%20that%0Aonly%20require%20presence%20data.%20We%20also%20propose%20an%20extension%20of%20an%20existing%0Apresence-only%20technique%20that%20is%20more%20robust%20to%20outliers.%20We%20perform%20a%20detailed%0Aevaluation%20of%20different%20thresholding%20techniques%20on%20the%20tasks%20of%20binary%20range%0Aestimation%20and%20large-scale%20fine-grained%20visual%20classification%2C%20and%20we%0Ademonstrate%20improved%20performance%20over%20existing%20pseudo-absence%20free%20approaches%0Ausing%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15956v1&entry.124074799=Read"},
{"title": "Secret Collusion among Generative AI Agents", "author": "Sumeet Ramesh Motwani and Mikhail Baranchuk and Martin Strohmeier and Vijay Bolina and Philip H. S. Torr and Lewis Hammond and Christian Schroeder de Witt", "abstract": "  Recent capability increases in large language models (LLMs) open up\napplications in which groups of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models.\n", "link": "http://arxiv.org/abs/2402.07510v2", "date": "2024-08-28", "relevancy": 1.9715, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5118}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4912}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Secret%20Collusion%20among%20Generative%20AI%20Agents&body=Title%3A%20Secret%20Collusion%20among%20Generative%20AI%20Agents%0AAuthor%3A%20Sumeet%20Ramesh%20Motwani%20and%20Mikhail%20Baranchuk%20and%20Martin%20Strohmeier%20and%20Vijay%20Bolina%20and%20Philip%20H.%20S.%20Torr%20and%20Lewis%20Hammond%20and%20Christian%20Schroeder%20de%20Witt%0AAbstract%3A%20%20%20Recent%20capability%20increases%20in%20large%20language%20models%20%28LLMs%29%20open%20up%0Aapplications%20in%20which%20groups%20of%20communicating%20generative%20AI%20agents%20solve%20joint%0Atasks.%20This%20poses%20privacy%20and%20security%20challenges%20concerning%20the%20unauthorised%0Asharing%20of%20information%2C%20or%20other%20unwanted%20forms%20of%20agent%20coordination.%20Modern%0Asteganographic%20techniques%20could%20render%20such%20dynamics%20hard%20to%20detect.%20In%20this%0Apaper%2C%20we%20comprehensively%20formalise%20the%20problem%20of%20secret%20collusion%20in%20systems%0Aof%20generative%20AI%20agents%20by%20drawing%20on%20relevant%20concepts%20from%20both%20AI%20and%0Asecurity%20literature.%20We%20study%20incentives%20for%20the%20use%20of%20steganography%2C%20and%0Apropose%20a%20variety%20of%20mitigation%20measures.%20Our%20investigations%20result%20in%20a%20model%0Aevaluation%20framework%20that%20systematically%20tests%20capabilities%20required%20for%0Avarious%20forms%20of%20secret%20collusion.%20We%20provide%20extensive%20empirical%20results%0Aacross%20a%20range%20of%20contemporary%20LLMs.%20While%20the%20steganographic%20capabilities%20of%0Acurrent%20models%20remain%20limited%2C%20GPT-4%20displays%20a%20capability%20jump%20suggesting%20the%0Aneed%20for%20continuous%20monitoring%20of%20steganographic%20frontier%20model%20capabilities.%0AWe%20conclude%20by%20laying%20out%20a%20comprehensive%20research%20program%20to%20mitigate%20future%0Arisks%20of%20collusion%20between%20generative%20AI%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07510v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecret%2520Collusion%2520among%2520Generative%2520AI%2520Agents%26entry.906535625%3DSumeet%2520Ramesh%2520Motwani%2520and%2520Mikhail%2520Baranchuk%2520and%2520Martin%2520Strohmeier%2520and%2520Vijay%2520Bolina%2520and%2520Philip%2520H.%2520S.%2520Torr%2520and%2520Lewis%2520Hammond%2520and%2520Christian%2520Schroeder%2520de%2520Witt%26entry.1292438233%3D%2520%2520Recent%2520capability%2520increases%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520open%2520up%250Aapplications%2520in%2520which%2520groups%2520of%2520communicating%2520generative%2520AI%2520agents%2520solve%2520joint%250Atasks.%2520This%2520poses%2520privacy%2520and%2520security%2520challenges%2520concerning%2520the%2520unauthorised%250Asharing%2520of%2520information%252C%2520or%2520other%2520unwanted%2520forms%2520of%2520agent%2520coordination.%2520Modern%250Asteganographic%2520techniques%2520could%2520render%2520such%2520dynamics%2520hard%2520to%2520detect.%2520In%2520this%250Apaper%252C%2520we%2520comprehensively%2520formalise%2520the%2520problem%2520of%2520secret%2520collusion%2520in%2520systems%250Aof%2520generative%2520AI%2520agents%2520by%2520drawing%2520on%2520relevant%2520concepts%2520from%2520both%2520AI%2520and%250Asecurity%2520literature.%2520We%2520study%2520incentives%2520for%2520the%2520use%2520of%2520steganography%252C%2520and%250Apropose%2520a%2520variety%2520of%2520mitigation%2520measures.%2520Our%2520investigations%2520result%2520in%2520a%2520model%250Aevaluation%2520framework%2520that%2520systematically%2520tests%2520capabilities%2520required%2520for%250Avarious%2520forms%2520of%2520secret%2520collusion.%2520We%2520provide%2520extensive%2520empirical%2520results%250Aacross%2520a%2520range%2520of%2520contemporary%2520LLMs.%2520While%2520the%2520steganographic%2520capabilities%2520of%250Acurrent%2520models%2520remain%2520limited%252C%2520GPT-4%2520displays%2520a%2520capability%2520jump%2520suggesting%2520the%250Aneed%2520for%2520continuous%2520monitoring%2520of%2520steganographic%2520frontier%2520model%2520capabilities.%250AWe%2520conclude%2520by%2520laying%2520out%2520a%2520comprehensive%2520research%2520program%2520to%2520mitigate%2520future%250Arisks%2520of%2520collusion%2520between%2520generative%2520AI%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.07510v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Secret%20Collusion%20among%20Generative%20AI%20Agents&entry.906535625=Sumeet%20Ramesh%20Motwani%20and%20Mikhail%20Baranchuk%20and%20Martin%20Strohmeier%20and%20Vijay%20Bolina%20and%20Philip%20H.%20S.%20Torr%20and%20Lewis%20Hammond%20and%20Christian%20Schroeder%20de%20Witt&entry.1292438233=%20%20Recent%20capability%20increases%20in%20large%20language%20models%20%28LLMs%29%20open%20up%0Aapplications%20in%20which%20groups%20of%20communicating%20generative%20AI%20agents%20solve%20joint%0Atasks.%20This%20poses%20privacy%20and%20security%20challenges%20concerning%20the%20unauthorised%0Asharing%20of%20information%2C%20or%20other%20unwanted%20forms%20of%20agent%20coordination.%20Modern%0Asteganographic%20techniques%20could%20render%20such%20dynamics%20hard%20to%20detect.%20In%20this%0Apaper%2C%20we%20comprehensively%20formalise%20the%20problem%20of%20secret%20collusion%20in%20systems%0Aof%20generative%20AI%20agents%20by%20drawing%20on%20relevant%20concepts%20from%20both%20AI%20and%0Asecurity%20literature.%20We%20study%20incentives%20for%20the%20use%20of%20steganography%2C%20and%0Apropose%20a%20variety%20of%20mitigation%20measures.%20Our%20investigations%20result%20in%20a%20model%0Aevaluation%20framework%20that%20systematically%20tests%20capabilities%20required%20for%0Avarious%20forms%20of%20secret%20collusion.%20We%20provide%20extensive%20empirical%20results%0Aacross%20a%20range%20of%20contemporary%20LLMs.%20While%20the%20steganographic%20capabilities%20of%0Acurrent%20models%20remain%20limited%2C%20GPT-4%20displays%20a%20capability%20jump%20suggesting%20the%0Aneed%20for%20continuous%20monitoring%20of%20steganographic%20frontier%20model%20capabilities.%0AWe%20conclude%20by%20laying%20out%20a%20comprehensive%20research%20program%20to%20mitigate%20future%0Arisks%20of%20collusion%20between%20generative%20AI%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07510v2&entry.124074799=Read"},
{"title": "A Survey on Facial Expression Recognition of Static and Dynamic Emotions", "author": "Yan Wang and Shaoqi Yan and Yang Liu and Wei Song and Jing Liu and Yang Chang and Xinji Mai and Xiping Hu and Wenqiang Zhang and Zhongxue Gan", "abstract": "  Facial expression recognition (FER) aims to analyze emotional states from\nstatic images and dynamic sequences, which is pivotal in enhancing\nanthropomorphic communication among humans, robots, and digital avatars by\nleveraging AI technologies. As the FER field evolves from controlled laboratory\nenvironments to more complex in-the-wild scenarios, advanced methods have been\nrapidly developed and new challenges and apporaches are encounted, which are\nnot well addressed in existing reviews of FER. This paper offers a\ncomprehensive survey of both image-based static FER (SFER) and video-based\ndynamic FER (DFER) methods, analyzing from model-oriented development to\nchallenge-focused categorization. We begin with a critical comparison of recent\nreviews, an introduction to common datasets and evaluation criteria, and an\nin-depth workflow on FER to establish a robust research foundation. We then\nsystematically review representative approaches addressing eight main\nchallenges in SFER (such as expression disturbance, uncertainties, compound\nemotions, and cross-domain inconsistency) as well as seven main challenges in\nDFER (such as key frame sampling, expression intensity variations, and\ncross-modal alignment). Additionally, we analyze recent advancements, benchmark\nperformances, major applications, and ethical considerations. Finally, we\npropose five promising future directions and development trends to guide\nongoing research. The project page for this paper can be found at\nhttps://github.com/wangyanckxx/SurveyFER.\n", "link": "http://arxiv.org/abs/2408.15777v1", "date": "2024-08-28", "relevancy": 1.9687, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5062}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.4924}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Facial%20Expression%20Recognition%20of%20Static%20and%20Dynamic%20Emotions&body=Title%3A%20A%20Survey%20on%20Facial%20Expression%20Recognition%20of%20Static%20and%20Dynamic%20Emotions%0AAuthor%3A%20Yan%20Wang%20and%20Shaoqi%20Yan%20and%20Yang%20Liu%20and%20Wei%20Song%20and%20Jing%20Liu%20and%20Yang%20Chang%20and%20Xinji%20Mai%20and%20Xiping%20Hu%20and%20Wenqiang%20Zhang%20and%20Zhongxue%20Gan%0AAbstract%3A%20%20%20Facial%20expression%20recognition%20%28FER%29%20aims%20to%20analyze%20emotional%20states%20from%0Astatic%20images%20and%20dynamic%20sequences%2C%20which%20is%20pivotal%20in%20enhancing%0Aanthropomorphic%20communication%20among%20humans%2C%20robots%2C%20and%20digital%20avatars%20by%0Aleveraging%20AI%20technologies.%20As%20the%20FER%20field%20evolves%20from%20controlled%20laboratory%0Aenvironments%20to%20more%20complex%20in-the-wild%20scenarios%2C%20advanced%20methods%20have%20been%0Arapidly%20developed%20and%20new%20challenges%20and%20apporaches%20are%20encounted%2C%20which%20are%0Anot%20well%20addressed%20in%20existing%20reviews%20of%20FER.%20This%20paper%20offers%20a%0Acomprehensive%20survey%20of%20both%20image-based%20static%20FER%20%28SFER%29%20and%20video-based%0Adynamic%20FER%20%28DFER%29%20methods%2C%20analyzing%20from%20model-oriented%20development%20to%0Achallenge-focused%20categorization.%20We%20begin%20with%20a%20critical%20comparison%20of%20recent%0Areviews%2C%20an%20introduction%20to%20common%20datasets%20and%20evaluation%20criteria%2C%20and%20an%0Ain-depth%20workflow%20on%20FER%20to%20establish%20a%20robust%20research%20foundation.%20We%20then%0Asystematically%20review%20representative%20approaches%20addressing%20eight%20main%0Achallenges%20in%20SFER%20%28such%20as%20expression%20disturbance%2C%20uncertainties%2C%20compound%0Aemotions%2C%20and%20cross-domain%20inconsistency%29%20as%20well%20as%20seven%20main%20challenges%20in%0ADFER%20%28such%20as%20key%20frame%20sampling%2C%20expression%20intensity%20variations%2C%20and%0Across-modal%20alignment%29.%20Additionally%2C%20we%20analyze%20recent%20advancements%2C%20benchmark%0Aperformances%2C%20major%20applications%2C%20and%20ethical%20considerations.%20Finally%2C%20we%0Apropose%20five%20promising%20future%20directions%20and%20development%20trends%20to%20guide%0Aongoing%20research.%20The%20project%20page%20for%20this%20paper%20can%20be%20found%20at%0Ahttps%3A//github.com/wangyanckxx/SurveyFER.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Facial%2520Expression%2520Recognition%2520of%2520Static%2520and%2520Dynamic%2520Emotions%26entry.906535625%3DYan%2520Wang%2520and%2520Shaoqi%2520Yan%2520and%2520Yang%2520Liu%2520and%2520Wei%2520Song%2520and%2520Jing%2520Liu%2520and%2520Yang%2520Chang%2520and%2520Xinji%2520Mai%2520and%2520Xiping%2520Hu%2520and%2520Wenqiang%2520Zhang%2520and%2520Zhongxue%2520Gan%26entry.1292438233%3D%2520%2520Facial%2520expression%2520recognition%2520%2528FER%2529%2520aims%2520to%2520analyze%2520emotional%2520states%2520from%250Astatic%2520images%2520and%2520dynamic%2520sequences%252C%2520which%2520is%2520pivotal%2520in%2520enhancing%250Aanthropomorphic%2520communication%2520among%2520humans%252C%2520robots%252C%2520and%2520digital%2520avatars%2520by%250Aleveraging%2520AI%2520technologies.%2520As%2520the%2520FER%2520field%2520evolves%2520from%2520controlled%2520laboratory%250Aenvironments%2520to%2520more%2520complex%2520in-the-wild%2520scenarios%252C%2520advanced%2520methods%2520have%2520been%250Arapidly%2520developed%2520and%2520new%2520challenges%2520and%2520apporaches%2520are%2520encounted%252C%2520which%2520are%250Anot%2520well%2520addressed%2520in%2520existing%2520reviews%2520of%2520FER.%2520This%2520paper%2520offers%2520a%250Acomprehensive%2520survey%2520of%2520both%2520image-based%2520static%2520FER%2520%2528SFER%2529%2520and%2520video-based%250Adynamic%2520FER%2520%2528DFER%2529%2520methods%252C%2520analyzing%2520from%2520model-oriented%2520development%2520to%250Achallenge-focused%2520categorization.%2520We%2520begin%2520with%2520a%2520critical%2520comparison%2520of%2520recent%250Areviews%252C%2520an%2520introduction%2520to%2520common%2520datasets%2520and%2520evaluation%2520criteria%252C%2520and%2520an%250Ain-depth%2520workflow%2520on%2520FER%2520to%2520establish%2520a%2520robust%2520research%2520foundation.%2520We%2520then%250Asystematically%2520review%2520representative%2520approaches%2520addressing%2520eight%2520main%250Achallenges%2520in%2520SFER%2520%2528such%2520as%2520expression%2520disturbance%252C%2520uncertainties%252C%2520compound%250Aemotions%252C%2520and%2520cross-domain%2520inconsistency%2529%2520as%2520well%2520as%2520seven%2520main%2520challenges%2520in%250ADFER%2520%2528such%2520as%2520key%2520frame%2520sampling%252C%2520expression%2520intensity%2520variations%252C%2520and%250Across-modal%2520alignment%2529.%2520Additionally%252C%2520we%2520analyze%2520recent%2520advancements%252C%2520benchmark%250Aperformances%252C%2520major%2520applications%252C%2520and%2520ethical%2520considerations.%2520Finally%252C%2520we%250Apropose%2520five%2520promising%2520future%2520directions%2520and%2520development%2520trends%2520to%2520guide%250Aongoing%2520research.%2520The%2520project%2520page%2520for%2520this%2520paper%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/wangyanckxx/SurveyFER.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Facial%20Expression%20Recognition%20of%20Static%20and%20Dynamic%20Emotions&entry.906535625=Yan%20Wang%20and%20Shaoqi%20Yan%20and%20Yang%20Liu%20and%20Wei%20Song%20and%20Jing%20Liu%20and%20Yang%20Chang%20and%20Xinji%20Mai%20and%20Xiping%20Hu%20and%20Wenqiang%20Zhang%20and%20Zhongxue%20Gan&entry.1292438233=%20%20Facial%20expression%20recognition%20%28FER%29%20aims%20to%20analyze%20emotional%20states%20from%0Astatic%20images%20and%20dynamic%20sequences%2C%20which%20is%20pivotal%20in%20enhancing%0Aanthropomorphic%20communication%20among%20humans%2C%20robots%2C%20and%20digital%20avatars%20by%0Aleveraging%20AI%20technologies.%20As%20the%20FER%20field%20evolves%20from%20controlled%20laboratory%0Aenvironments%20to%20more%20complex%20in-the-wild%20scenarios%2C%20advanced%20methods%20have%20been%0Arapidly%20developed%20and%20new%20challenges%20and%20apporaches%20are%20encounted%2C%20which%20are%0Anot%20well%20addressed%20in%20existing%20reviews%20of%20FER.%20This%20paper%20offers%20a%0Acomprehensive%20survey%20of%20both%20image-based%20static%20FER%20%28SFER%29%20and%20video-based%0Adynamic%20FER%20%28DFER%29%20methods%2C%20analyzing%20from%20model-oriented%20development%20to%0Achallenge-focused%20categorization.%20We%20begin%20with%20a%20critical%20comparison%20of%20recent%0Areviews%2C%20an%20introduction%20to%20common%20datasets%20and%20evaluation%20criteria%2C%20and%20an%0Ain-depth%20workflow%20on%20FER%20to%20establish%20a%20robust%20research%20foundation.%20We%20then%0Asystematically%20review%20representative%20approaches%20addressing%20eight%20main%0Achallenges%20in%20SFER%20%28such%20as%20expression%20disturbance%2C%20uncertainties%2C%20compound%0Aemotions%2C%20and%20cross-domain%20inconsistency%29%20as%20well%20as%20seven%20main%20challenges%20in%0ADFER%20%28such%20as%20key%20frame%20sampling%2C%20expression%20intensity%20variations%2C%20and%0Across-modal%20alignment%29.%20Additionally%2C%20we%20analyze%20recent%20advancements%2C%20benchmark%0Aperformances%2C%20major%20applications%2C%20and%20ethical%20considerations.%20Finally%2C%20we%0Apropose%20five%20promising%20future%20directions%20and%20development%20trends%20to%20guide%0Aongoing%20research.%20The%20project%20page%20for%20this%20paper%20can%20be%20found%20at%0Ahttps%3A//github.com/wangyanckxx/SurveyFER.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15777v1&entry.124074799=Read"},
{"title": "Nexus: Specialization meets Adaptability for Efficiently Training\n  Mixture of Experts", "author": "Nikolas Gritsch and Qizhen Zhang and Acyr Locatelli and Sara Hooker and Ahmet \u00dcst\u00fcn", "abstract": "  Efficiency, specialization, and adaptability to new data distributions are\nqualities that are hard to combine in current Large Language Models. The\nMixture of Experts (MoE) architecture has been the focus of significant\nresearch because its inherent conditional computation enables such desirable\nproperties. In this work, we focus on \"upcycling\" dense expert models into an\nMoE, aiming to improve specialization while also adding the ability to adapt to\nnew tasks easily. We introduce Nexus, an enhanced MoE architecture with\nadaptive routing where the model learns to project expert embeddings from\ndomain representations. This approach allows Nexus to flexibly add new experts\nafter the initial upcycling through separately trained dense models, without\nrequiring large-scale MoE training for unseen data domains. Our experiments\nshow that Nexus achieves a relative gain of up to 2.1% over the baseline for\ninitial upcycling, and a 18.8% relative gain for extending the MoE with a new\nexpert by using limited finetuning data. This flexibility of Nexus is crucial\nto enable an open-source ecosystem where every user continuously assembles\ntheir own MoE-mix according to their needs.\n", "link": "http://arxiv.org/abs/2408.15901v1", "date": "2024-08-28", "relevancy": 1.9676, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5151}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4778}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nexus%3A%20Specialization%20meets%20Adaptability%20for%20Efficiently%20Training%0A%20%20Mixture%20of%20Experts&body=Title%3A%20Nexus%3A%20Specialization%20meets%20Adaptability%20for%20Efficiently%20Training%0A%20%20Mixture%20of%20Experts%0AAuthor%3A%20Nikolas%20Gritsch%20and%20Qizhen%20Zhang%20and%20Acyr%20Locatelli%20and%20Sara%20Hooker%20and%20Ahmet%20%C3%9Cst%C3%BCn%0AAbstract%3A%20%20%20Efficiency%2C%20specialization%2C%20and%20adaptability%20to%20new%20data%20distributions%20are%0Aqualities%20that%20are%20hard%20to%20combine%20in%20current%20Large%20Language%20Models.%20The%0AMixture%20of%20Experts%20%28MoE%29%20architecture%20has%20been%20the%20focus%20of%20significant%0Aresearch%20because%20its%20inherent%20conditional%20computation%20enables%20such%20desirable%0Aproperties.%20In%20this%20work%2C%20we%20focus%20on%20%22upcycling%22%20dense%20expert%20models%20into%20an%0AMoE%2C%20aiming%20to%20improve%20specialization%20while%20also%20adding%20the%20ability%20to%20adapt%20to%0Anew%20tasks%20easily.%20We%20introduce%20Nexus%2C%20an%20enhanced%20MoE%20architecture%20with%0Aadaptive%20routing%20where%20the%20model%20learns%20to%20project%20expert%20embeddings%20from%0Adomain%20representations.%20This%20approach%20allows%20Nexus%20to%20flexibly%20add%20new%20experts%0Aafter%20the%20initial%20upcycling%20through%20separately%20trained%20dense%20models%2C%20without%0Arequiring%20large-scale%20MoE%20training%20for%20unseen%20data%20domains.%20Our%20experiments%0Ashow%20that%20Nexus%20achieves%20a%20relative%20gain%20of%20up%20to%202.1%25%20over%20the%20baseline%20for%0Ainitial%20upcycling%2C%20and%20a%2018.8%25%20relative%20gain%20for%20extending%20the%20MoE%20with%20a%20new%0Aexpert%20by%20using%20limited%20finetuning%20data.%20This%20flexibility%20of%20Nexus%20is%20crucial%0Ato%20enable%20an%20open-source%20ecosystem%20where%20every%20user%20continuously%20assembles%0Atheir%20own%20MoE-mix%20according%20to%20their%20needs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNexus%253A%2520Specialization%2520meets%2520Adaptability%2520for%2520Efficiently%2520Training%250A%2520%2520Mixture%2520of%2520Experts%26entry.906535625%3DNikolas%2520Gritsch%2520and%2520Qizhen%2520Zhang%2520and%2520Acyr%2520Locatelli%2520and%2520Sara%2520Hooker%2520and%2520Ahmet%2520%25C3%259Cst%25C3%25BCn%26entry.1292438233%3D%2520%2520Efficiency%252C%2520specialization%252C%2520and%2520adaptability%2520to%2520new%2520data%2520distributions%2520are%250Aqualities%2520that%2520are%2520hard%2520to%2520combine%2520in%2520current%2520Large%2520Language%2520Models.%2520The%250AMixture%2520of%2520Experts%2520%2528MoE%2529%2520architecture%2520has%2520been%2520the%2520focus%2520of%2520significant%250Aresearch%2520because%2520its%2520inherent%2520conditional%2520computation%2520enables%2520such%2520desirable%250Aproperties.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520%2522upcycling%2522%2520dense%2520expert%2520models%2520into%2520an%250AMoE%252C%2520aiming%2520to%2520improve%2520specialization%2520while%2520also%2520adding%2520the%2520ability%2520to%2520adapt%2520to%250Anew%2520tasks%2520easily.%2520We%2520introduce%2520Nexus%252C%2520an%2520enhanced%2520MoE%2520architecture%2520with%250Aadaptive%2520routing%2520where%2520the%2520model%2520learns%2520to%2520project%2520expert%2520embeddings%2520from%250Adomain%2520representations.%2520This%2520approach%2520allows%2520Nexus%2520to%2520flexibly%2520add%2520new%2520experts%250Aafter%2520the%2520initial%2520upcycling%2520through%2520separately%2520trained%2520dense%2520models%252C%2520without%250Arequiring%2520large-scale%2520MoE%2520training%2520for%2520unseen%2520data%2520domains.%2520Our%2520experiments%250Ashow%2520that%2520Nexus%2520achieves%2520a%2520relative%2520gain%2520of%2520up%2520to%25202.1%2525%2520over%2520the%2520baseline%2520for%250Ainitial%2520upcycling%252C%2520and%2520a%252018.8%2525%2520relative%2520gain%2520for%2520extending%2520the%2520MoE%2520with%2520a%2520new%250Aexpert%2520by%2520using%2520limited%2520finetuning%2520data.%2520This%2520flexibility%2520of%2520Nexus%2520is%2520crucial%250Ato%2520enable%2520an%2520open-source%2520ecosystem%2520where%2520every%2520user%2520continuously%2520assembles%250Atheir%2520own%2520MoE-mix%2520according%2520to%2520their%2520needs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nexus%3A%20Specialization%20meets%20Adaptability%20for%20Efficiently%20Training%0A%20%20Mixture%20of%20Experts&entry.906535625=Nikolas%20Gritsch%20and%20Qizhen%20Zhang%20and%20Acyr%20Locatelli%20and%20Sara%20Hooker%20and%20Ahmet%20%C3%9Cst%C3%BCn&entry.1292438233=%20%20Efficiency%2C%20specialization%2C%20and%20adaptability%20to%20new%20data%20distributions%20are%0Aqualities%20that%20are%20hard%20to%20combine%20in%20current%20Large%20Language%20Models.%20The%0AMixture%20of%20Experts%20%28MoE%29%20architecture%20has%20been%20the%20focus%20of%20significant%0Aresearch%20because%20its%20inherent%20conditional%20computation%20enables%20such%20desirable%0Aproperties.%20In%20this%20work%2C%20we%20focus%20on%20%22upcycling%22%20dense%20expert%20models%20into%20an%0AMoE%2C%20aiming%20to%20improve%20specialization%20while%20also%20adding%20the%20ability%20to%20adapt%20to%0Anew%20tasks%20easily.%20We%20introduce%20Nexus%2C%20an%20enhanced%20MoE%20architecture%20with%0Aadaptive%20routing%20where%20the%20model%20learns%20to%20project%20expert%20embeddings%20from%0Adomain%20representations.%20This%20approach%20allows%20Nexus%20to%20flexibly%20add%20new%20experts%0Aafter%20the%20initial%20upcycling%20through%20separately%20trained%20dense%20models%2C%20without%0Arequiring%20large-scale%20MoE%20training%20for%20unseen%20data%20domains.%20Our%20experiments%0Ashow%20that%20Nexus%20achieves%20a%20relative%20gain%20of%20up%20to%202.1%25%20over%20the%20baseline%20for%0Ainitial%20upcycling%2C%20and%20a%2018.8%25%20relative%20gain%20for%20extending%20the%20MoE%20with%20a%20new%0Aexpert%20by%20using%20limited%20finetuning%20data.%20This%20flexibility%20of%20Nexus%20is%20crucial%0Ato%20enable%20an%20open-source%20ecosystem%20where%20every%20user%20continuously%20assembles%0Atheir%20own%20MoE-mix%20according%20to%20their%20needs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15901v1&entry.124074799=Read"},
{"title": "Guaranteed Coverage Prediction Intervals with Gaussian Process\n  Regression", "author": "Harris Papadopoulos", "abstract": "  Gaussian Process Regression (GPR) is a popular regression method, which\nunlike most Machine Learning techniques, provides estimates of uncertainty for\nits predictions. These uncertainty estimates however, are based on the\nassumption that the model is well-specified, an assumption that is violated in\nmost practical applications, since the required knowledge is rarely available.\nAs a result, the produced uncertainty estimates can become very misleading; for\nexample the prediction intervals (PIs) produced for the 95% confidence level\nmay cover much less than 95% of the true labels. To address this issue, this\npaper introduces an extension of GPR based on a Machine Learning framework\ncalled, Conformal Prediction (CP). This extension guarantees the production of\nPIs with the required coverage even when the model is completely misspecified.\nThe proposed approach combines the advantages of GPR with the valid coverage\nguarantee of CP, while the performed experimental results demonstrate its\nsuperiority over existing methods.\n", "link": "http://arxiv.org/abs/2310.15641v2", "date": "2024-08-28", "relevancy": 1.9585, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4909}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4894}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guaranteed%20Coverage%20Prediction%20Intervals%20with%20Gaussian%20Process%0A%20%20Regression&body=Title%3A%20Guaranteed%20Coverage%20Prediction%20Intervals%20with%20Gaussian%20Process%0A%20%20Regression%0AAuthor%3A%20Harris%20Papadopoulos%0AAbstract%3A%20%20%20Gaussian%20Process%20Regression%20%28GPR%29%20is%20a%20popular%20regression%20method%2C%20which%0Aunlike%20most%20Machine%20Learning%20techniques%2C%20provides%20estimates%20of%20uncertainty%20for%0Aits%20predictions.%20These%20uncertainty%20estimates%20however%2C%20are%20based%20on%20the%0Aassumption%20that%20the%20model%20is%20well-specified%2C%20an%20assumption%20that%20is%20violated%20in%0Amost%20practical%20applications%2C%20since%20the%20required%20knowledge%20is%20rarely%20available.%0AAs%20a%20result%2C%20the%20produced%20uncertainty%20estimates%20can%20become%20very%20misleading%3B%20for%0Aexample%20the%20prediction%20intervals%20%28PIs%29%20produced%20for%20the%2095%25%20confidence%20level%0Amay%20cover%20much%20less%20than%2095%25%20of%20the%20true%20labels.%20To%20address%20this%20issue%2C%20this%0Apaper%20introduces%20an%20extension%20of%20GPR%20based%20on%20a%20Machine%20Learning%20framework%0Acalled%2C%20Conformal%20Prediction%20%28CP%29.%20This%20extension%20guarantees%20the%20production%20of%0APIs%20with%20the%20required%20coverage%20even%20when%20the%20model%20is%20completely%20misspecified.%0AThe%20proposed%20approach%20combines%20the%20advantages%20of%20GPR%20with%20the%20valid%20coverage%0Aguarantee%20of%20CP%2C%20while%20the%20performed%20experimental%20results%20demonstrate%20its%0Asuperiority%20over%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.15641v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuaranteed%2520Coverage%2520Prediction%2520Intervals%2520with%2520Gaussian%2520Process%250A%2520%2520Regression%26entry.906535625%3DHarris%2520Papadopoulos%26entry.1292438233%3D%2520%2520Gaussian%2520Process%2520Regression%2520%2528GPR%2529%2520is%2520a%2520popular%2520regression%2520method%252C%2520which%250Aunlike%2520most%2520Machine%2520Learning%2520techniques%252C%2520provides%2520estimates%2520of%2520uncertainty%2520for%250Aits%2520predictions.%2520These%2520uncertainty%2520estimates%2520however%252C%2520are%2520based%2520on%2520the%250Aassumption%2520that%2520the%2520model%2520is%2520well-specified%252C%2520an%2520assumption%2520that%2520is%2520violated%2520in%250Amost%2520practical%2520applications%252C%2520since%2520the%2520required%2520knowledge%2520is%2520rarely%2520available.%250AAs%2520a%2520result%252C%2520the%2520produced%2520uncertainty%2520estimates%2520can%2520become%2520very%2520misleading%253B%2520for%250Aexample%2520the%2520prediction%2520intervals%2520%2528PIs%2529%2520produced%2520for%2520the%252095%2525%2520confidence%2520level%250Amay%2520cover%2520much%2520less%2520than%252095%2525%2520of%2520the%2520true%2520labels.%2520To%2520address%2520this%2520issue%252C%2520this%250Apaper%2520introduces%2520an%2520extension%2520of%2520GPR%2520based%2520on%2520a%2520Machine%2520Learning%2520framework%250Acalled%252C%2520Conformal%2520Prediction%2520%2528CP%2529.%2520This%2520extension%2520guarantees%2520the%2520production%2520of%250APIs%2520with%2520the%2520required%2520coverage%2520even%2520when%2520the%2520model%2520is%2520completely%2520misspecified.%250AThe%2520proposed%2520approach%2520combines%2520the%2520advantages%2520of%2520GPR%2520with%2520the%2520valid%2520coverage%250Aguarantee%2520of%2520CP%252C%2520while%2520the%2520performed%2520experimental%2520results%2520demonstrate%2520its%250Asuperiority%2520over%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.15641v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guaranteed%20Coverage%20Prediction%20Intervals%20with%20Gaussian%20Process%0A%20%20Regression&entry.906535625=Harris%20Papadopoulos&entry.1292438233=%20%20Gaussian%20Process%20Regression%20%28GPR%29%20is%20a%20popular%20regression%20method%2C%20which%0Aunlike%20most%20Machine%20Learning%20techniques%2C%20provides%20estimates%20of%20uncertainty%20for%0Aits%20predictions.%20These%20uncertainty%20estimates%20however%2C%20are%20based%20on%20the%0Aassumption%20that%20the%20model%20is%20well-specified%2C%20an%20assumption%20that%20is%20violated%20in%0Amost%20practical%20applications%2C%20since%20the%20required%20knowledge%20is%20rarely%20available.%0AAs%20a%20result%2C%20the%20produced%20uncertainty%20estimates%20can%20become%20very%20misleading%3B%20for%0Aexample%20the%20prediction%20intervals%20%28PIs%29%20produced%20for%20the%2095%25%20confidence%20level%0Amay%20cover%20much%20less%20than%2095%25%20of%20the%20true%20labels.%20To%20address%20this%20issue%2C%20this%0Apaper%20introduces%20an%20extension%20of%20GPR%20based%20on%20a%20Machine%20Learning%20framework%0Acalled%2C%20Conformal%20Prediction%20%28CP%29.%20This%20extension%20guarantees%20the%20production%20of%0APIs%20with%20the%20required%20coverage%20even%20when%20the%20model%20is%20completely%20misspecified.%0AThe%20proposed%20approach%20combines%20the%20advantages%20of%20GPR%20with%20the%20valid%20coverage%0Aguarantee%20of%20CP%2C%20while%20the%20performed%20experimental%20results%20demonstrate%20its%0Asuperiority%20over%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.15641v2&entry.124074799=Read"},
{"title": "Evading AI-Generated Content Detectors using Homoglyphs", "author": "Aldan Creo and Shushanta Pudasaini", "abstract": "  The advent of large language models (LLMs) has enabled the generation of text\nthat increasingly exhibits human-like characteristics. As the detection of such\ncontent is of significant importance, numerous studies have been conducted with\nthe aim of developing reliable AI-generated text detectors. These detectors\nhave demonstrated promising results on test data, but recent research has\nrevealed that they can be circumvented by employing different techniques. In\nthis paper, we present homoglyph-based attacks ($a \\rightarrow {\\alpha}$) as a\nmeans of circumventing existing detectors. A comprehensive evaluation was\nconducted to assess the effectiveness of these attacks on seven detectors,\nincluding ArguGPT, Binoculars, DetectGPT, Fast-DetectGPT, Ghostbuster, OpenAI's\ndetector, and watermarking techniques, on five different datasets. Our findings\ndemonstrate that homoglyph-based attacks can effectively circumvent\nstate-of-the-art detectors, leading them to classify all texts as either\nAI-generated or human-written (decreasing the average Matthews Correlation\nCoefficient from 0.64 to -0.01). We then examine the effectiveness of these\nattacks by analyzing how homoglyphs impact different families of detectors.\nFinally, we discuss the implications of these findings and potential defenses\nagainst such attacks.\n", "link": "http://arxiv.org/abs/2406.11239v2", "date": "2024-08-28", "relevancy": 1.9563, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4972}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4911}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4838}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evading%20AI-Generated%20Content%20Detectors%20using%20Homoglyphs&body=Title%3A%20Evading%20AI-Generated%20Content%20Detectors%20using%20Homoglyphs%0AAuthor%3A%20Aldan%20Creo%20and%20Shushanta%20Pudasaini%0AAbstract%3A%20%20%20The%20advent%20of%20large%20language%20models%20%28LLMs%29%20has%20enabled%20the%20generation%20of%20text%0Athat%20increasingly%20exhibits%20human-like%20characteristics.%20As%20the%20detection%20of%20such%0Acontent%20is%20of%20significant%20importance%2C%20numerous%20studies%20have%20been%20conducted%20with%0Athe%20aim%20of%20developing%20reliable%20AI-generated%20text%20detectors.%20These%20detectors%0Ahave%20demonstrated%20promising%20results%20on%20test%20data%2C%20but%20recent%20research%20has%0Arevealed%20that%20they%20can%20be%20circumvented%20by%20employing%20different%20techniques.%20In%0Athis%20paper%2C%20we%20present%20homoglyph-based%20attacks%20%28%24a%20%5Crightarrow%20%7B%5Calpha%7D%24%29%20as%20a%0Ameans%20of%20circumventing%20existing%20detectors.%20A%20comprehensive%20evaluation%20was%0Aconducted%20to%20assess%20the%20effectiveness%20of%20these%20attacks%20on%20seven%20detectors%2C%0Aincluding%20ArguGPT%2C%20Binoculars%2C%20DetectGPT%2C%20Fast-DetectGPT%2C%20Ghostbuster%2C%20OpenAI%27s%0Adetector%2C%20and%20watermarking%20techniques%2C%20on%20five%20different%20datasets.%20Our%20findings%0Ademonstrate%20that%20homoglyph-based%20attacks%20can%20effectively%20circumvent%0Astate-of-the-art%20detectors%2C%20leading%20them%20to%20classify%20all%20texts%20as%20either%0AAI-generated%20or%20human-written%20%28decreasing%20the%20average%20Matthews%20Correlation%0ACoefficient%20from%200.64%20to%20-0.01%29.%20We%20then%20examine%20the%20effectiveness%20of%20these%0Aattacks%20by%20analyzing%20how%20homoglyphs%20impact%20different%20families%20of%20detectors.%0AFinally%2C%20we%20discuss%20the%20implications%20of%20these%20findings%20and%20potential%20defenses%0Aagainst%20such%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11239v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvading%2520AI-Generated%2520Content%2520Detectors%2520using%2520Homoglyphs%26entry.906535625%3DAldan%2520Creo%2520and%2520Shushanta%2520Pudasaini%26entry.1292438233%3D%2520%2520The%2520advent%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520enabled%2520the%2520generation%2520of%2520text%250Athat%2520increasingly%2520exhibits%2520human-like%2520characteristics.%2520As%2520the%2520detection%2520of%2520such%250Acontent%2520is%2520of%2520significant%2520importance%252C%2520numerous%2520studies%2520have%2520been%2520conducted%2520with%250Athe%2520aim%2520of%2520developing%2520reliable%2520AI-generated%2520text%2520detectors.%2520These%2520detectors%250Ahave%2520demonstrated%2520promising%2520results%2520on%2520test%2520data%252C%2520but%2520recent%2520research%2520has%250Arevealed%2520that%2520they%2520can%2520be%2520circumvented%2520by%2520employing%2520different%2520techniques.%2520In%250Athis%2520paper%252C%2520we%2520present%2520homoglyph-based%2520attacks%2520%2528%2524a%2520%255Crightarrow%2520%257B%255Calpha%257D%2524%2529%2520as%2520a%250Ameans%2520of%2520circumventing%2520existing%2520detectors.%2520A%2520comprehensive%2520evaluation%2520was%250Aconducted%2520to%2520assess%2520the%2520effectiveness%2520of%2520these%2520attacks%2520on%2520seven%2520detectors%252C%250Aincluding%2520ArguGPT%252C%2520Binoculars%252C%2520DetectGPT%252C%2520Fast-DetectGPT%252C%2520Ghostbuster%252C%2520OpenAI%2527s%250Adetector%252C%2520and%2520watermarking%2520techniques%252C%2520on%2520five%2520different%2520datasets.%2520Our%2520findings%250Ademonstrate%2520that%2520homoglyph-based%2520attacks%2520can%2520effectively%2520circumvent%250Astate-of-the-art%2520detectors%252C%2520leading%2520them%2520to%2520classify%2520all%2520texts%2520as%2520either%250AAI-generated%2520or%2520human-written%2520%2528decreasing%2520the%2520average%2520Matthews%2520Correlation%250ACoefficient%2520from%25200.64%2520to%2520-0.01%2529.%2520We%2520then%2520examine%2520the%2520effectiveness%2520of%2520these%250Aattacks%2520by%2520analyzing%2520how%2520homoglyphs%2520impact%2520different%2520families%2520of%2520detectors.%250AFinally%252C%2520we%2520discuss%2520the%2520implications%2520of%2520these%2520findings%2520and%2520potential%2520defenses%250Aagainst%2520such%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11239v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evading%20AI-Generated%20Content%20Detectors%20using%20Homoglyphs&entry.906535625=Aldan%20Creo%20and%20Shushanta%20Pudasaini&entry.1292438233=%20%20The%20advent%20of%20large%20language%20models%20%28LLMs%29%20has%20enabled%20the%20generation%20of%20text%0Athat%20increasingly%20exhibits%20human-like%20characteristics.%20As%20the%20detection%20of%20such%0Acontent%20is%20of%20significant%20importance%2C%20numerous%20studies%20have%20been%20conducted%20with%0Athe%20aim%20of%20developing%20reliable%20AI-generated%20text%20detectors.%20These%20detectors%0Ahave%20demonstrated%20promising%20results%20on%20test%20data%2C%20but%20recent%20research%20has%0Arevealed%20that%20they%20can%20be%20circumvented%20by%20employing%20different%20techniques.%20In%0Athis%20paper%2C%20we%20present%20homoglyph-based%20attacks%20%28%24a%20%5Crightarrow%20%7B%5Calpha%7D%24%29%20as%20a%0Ameans%20of%20circumventing%20existing%20detectors.%20A%20comprehensive%20evaluation%20was%0Aconducted%20to%20assess%20the%20effectiveness%20of%20these%20attacks%20on%20seven%20detectors%2C%0Aincluding%20ArguGPT%2C%20Binoculars%2C%20DetectGPT%2C%20Fast-DetectGPT%2C%20Ghostbuster%2C%20OpenAI%27s%0Adetector%2C%20and%20watermarking%20techniques%2C%20on%20five%20different%20datasets.%20Our%20findings%0Ademonstrate%20that%20homoglyph-based%20attacks%20can%20effectively%20circumvent%0Astate-of-the-art%20detectors%2C%20leading%20them%20to%20classify%20all%20texts%20as%20either%0AAI-generated%20or%20human-written%20%28decreasing%20the%20average%20Matthews%20Correlation%0ACoefficient%20from%200.64%20to%20-0.01%29.%20We%20then%20examine%20the%20effectiveness%20of%20these%0Aattacks%20by%20analyzing%20how%20homoglyphs%20impact%20different%20families%20of%20detectors.%0AFinally%2C%20we%20discuss%20the%20implications%20of%20these%20findings%20and%20potential%20defenses%0Aagainst%20such%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11239v2&entry.124074799=Read"},
{"title": "Fusing Pruned and Backdoored Models: Optimal Transport-based Data-free\n  Backdoor Mitigation", "author": "Weilin Lin and Li Liu and Jianze Li and Hui Xiong", "abstract": "  Backdoor attacks present a serious security threat to deep neuron networks\n(DNNs). Although numerous effective defense techniques have been proposed in\nrecent years, they inevitably rely on the availability of either clean or\npoisoned data. In contrast, data-free defense techniques have evolved slowly\nand still lag significantly in performance. To address this issue, different\nfrom the traditional approach of pruning followed by fine-tuning, we propose a\nnovel data-free defense method named Optimal Transport-based Backdoor Repairing\n(OTBR) in this work. This method, based on our findings on neuron weight\nchanges (NWCs) of random unlearning, uses optimal transport (OT)-based model\nfusion to combine the advantages of both pruned and backdoored models.\nSpecifically, we first demonstrate our findings that the NWCs of random\nunlearning are positively correlated with those of poison unlearning. Based on\nthis observation, we propose a random-unlearning NWC pruning technique to\neliminate the backdoor effect and obtain a backdoor-free pruned model. Then,\nmotivated by the OT-based model fusion, we propose the pruned-to-backdoored\nOT-based fusion technique, which fuses pruned and backdoored models to combine\nthe advantages of both, resulting in a model that demonstrates high clean\naccuracy and a low attack success rate. To our knowledge, this is the first\nwork to apply OT and model fusion techniques to backdoor defense. Extensive\nexperiments show that our method successfully defends against all seven\nbackdoor attacks across three benchmark datasets, outperforming both\nstate-of-the-art (SOTA) data-free and data-dependent methods. The code\nimplementation and Appendix are provided in the Supplementary Material.\n", "link": "http://arxiv.org/abs/2408.15861v1", "date": "2024-08-28", "relevancy": 1.9482, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4906}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4894}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4724}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fusing%20Pruned%20and%20Backdoored%20Models%3A%20Optimal%20Transport-based%20Data-free%0A%20%20Backdoor%20Mitigation&body=Title%3A%20Fusing%20Pruned%20and%20Backdoored%20Models%3A%20Optimal%20Transport-based%20Data-free%0A%20%20Backdoor%20Mitigation%0AAuthor%3A%20Weilin%20Lin%20and%20Li%20Liu%20and%20Jianze%20Li%20and%20Hui%20Xiong%0AAbstract%3A%20%20%20Backdoor%20attacks%20present%20a%20serious%20security%20threat%20to%20deep%20neuron%20networks%0A%28DNNs%29.%20Although%20numerous%20effective%20defense%20techniques%20have%20been%20proposed%20in%0Arecent%20years%2C%20they%20inevitably%20rely%20on%20the%20availability%20of%20either%20clean%20or%0Apoisoned%20data.%20In%20contrast%2C%20data-free%20defense%20techniques%20have%20evolved%20slowly%0Aand%20still%20lag%20significantly%20in%20performance.%20To%20address%20this%20issue%2C%20different%0Afrom%20the%20traditional%20approach%20of%20pruning%20followed%20by%20fine-tuning%2C%20we%20propose%20a%0Anovel%20data-free%20defense%20method%20named%20Optimal%20Transport-based%20Backdoor%20Repairing%0A%28OTBR%29%20in%20this%20work.%20This%20method%2C%20based%20on%20our%20findings%20on%20neuron%20weight%0Achanges%20%28NWCs%29%20of%20random%20unlearning%2C%20uses%20optimal%20transport%20%28OT%29-based%20model%0Afusion%20to%20combine%20the%20advantages%20of%20both%20pruned%20and%20backdoored%20models.%0ASpecifically%2C%20we%20first%20demonstrate%20our%20findings%20that%20the%20NWCs%20of%20random%0Aunlearning%20are%20positively%20correlated%20with%20those%20of%20poison%20unlearning.%20Based%20on%0Athis%20observation%2C%20we%20propose%20a%20random-unlearning%20NWC%20pruning%20technique%20to%0Aeliminate%20the%20backdoor%20effect%20and%20obtain%20a%20backdoor-free%20pruned%20model.%20Then%2C%0Amotivated%20by%20the%20OT-based%20model%20fusion%2C%20we%20propose%20the%20pruned-to-backdoored%0AOT-based%20fusion%20technique%2C%20which%20fuses%20pruned%20and%20backdoored%20models%20to%20combine%0Athe%20advantages%20of%20both%2C%20resulting%20in%20a%20model%20that%20demonstrates%20high%20clean%0Aaccuracy%20and%20a%20low%20attack%20success%20rate.%20To%20our%20knowledge%2C%20this%20is%20the%20first%0Awork%20to%20apply%20OT%20and%20model%20fusion%20techniques%20to%20backdoor%20defense.%20Extensive%0Aexperiments%20show%20that%20our%20method%20successfully%20defends%20against%20all%20seven%0Abackdoor%20attacks%20across%20three%20benchmark%20datasets%2C%20outperforming%20both%0Astate-of-the-art%20%28SOTA%29%20data-free%20and%20data-dependent%20methods.%20The%20code%0Aimplementation%20and%20Appendix%20are%20provided%20in%20the%20Supplementary%20Material.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15861v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusing%2520Pruned%2520and%2520Backdoored%2520Models%253A%2520Optimal%2520Transport-based%2520Data-free%250A%2520%2520Backdoor%2520Mitigation%26entry.906535625%3DWeilin%2520Lin%2520and%2520Li%2520Liu%2520and%2520Jianze%2520Li%2520and%2520Hui%2520Xiong%26entry.1292438233%3D%2520%2520Backdoor%2520attacks%2520present%2520a%2520serious%2520security%2520threat%2520to%2520deep%2520neuron%2520networks%250A%2528DNNs%2529.%2520Although%2520numerous%2520effective%2520defense%2520techniques%2520have%2520been%2520proposed%2520in%250Arecent%2520years%252C%2520they%2520inevitably%2520rely%2520on%2520the%2520availability%2520of%2520either%2520clean%2520or%250Apoisoned%2520data.%2520In%2520contrast%252C%2520data-free%2520defense%2520techniques%2520have%2520evolved%2520slowly%250Aand%2520still%2520lag%2520significantly%2520in%2520performance.%2520To%2520address%2520this%2520issue%252C%2520different%250Afrom%2520the%2520traditional%2520approach%2520of%2520pruning%2520followed%2520by%2520fine-tuning%252C%2520we%2520propose%2520a%250Anovel%2520data-free%2520defense%2520method%2520named%2520Optimal%2520Transport-based%2520Backdoor%2520Repairing%250A%2528OTBR%2529%2520in%2520this%2520work.%2520This%2520method%252C%2520based%2520on%2520our%2520findings%2520on%2520neuron%2520weight%250Achanges%2520%2528NWCs%2529%2520of%2520random%2520unlearning%252C%2520uses%2520optimal%2520transport%2520%2528OT%2529-based%2520model%250Afusion%2520to%2520combine%2520the%2520advantages%2520of%2520both%2520pruned%2520and%2520backdoored%2520models.%250ASpecifically%252C%2520we%2520first%2520demonstrate%2520our%2520findings%2520that%2520the%2520NWCs%2520of%2520random%250Aunlearning%2520are%2520positively%2520correlated%2520with%2520those%2520of%2520poison%2520unlearning.%2520Based%2520on%250Athis%2520observation%252C%2520we%2520propose%2520a%2520random-unlearning%2520NWC%2520pruning%2520technique%2520to%250Aeliminate%2520the%2520backdoor%2520effect%2520and%2520obtain%2520a%2520backdoor-free%2520pruned%2520model.%2520Then%252C%250Amotivated%2520by%2520the%2520OT-based%2520model%2520fusion%252C%2520we%2520propose%2520the%2520pruned-to-backdoored%250AOT-based%2520fusion%2520technique%252C%2520which%2520fuses%2520pruned%2520and%2520backdoored%2520models%2520to%2520combine%250Athe%2520advantages%2520of%2520both%252C%2520resulting%2520in%2520a%2520model%2520that%2520demonstrates%2520high%2520clean%250Aaccuracy%2520and%2520a%2520low%2520attack%2520success%2520rate.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%250Awork%2520to%2520apply%2520OT%2520and%2520model%2520fusion%2520techniques%2520to%2520backdoor%2520defense.%2520Extensive%250Aexperiments%2520show%2520that%2520our%2520method%2520successfully%2520defends%2520against%2520all%2520seven%250Abackdoor%2520attacks%2520across%2520three%2520benchmark%2520datasets%252C%2520outperforming%2520both%250Astate-of-the-art%2520%2528SOTA%2529%2520data-free%2520and%2520data-dependent%2520methods.%2520The%2520code%250Aimplementation%2520and%2520Appendix%2520are%2520provided%2520in%2520the%2520Supplementary%2520Material.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15861v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusing%20Pruned%20and%20Backdoored%20Models%3A%20Optimal%20Transport-based%20Data-free%0A%20%20Backdoor%20Mitigation&entry.906535625=Weilin%20Lin%20and%20Li%20Liu%20and%20Jianze%20Li%20and%20Hui%20Xiong&entry.1292438233=%20%20Backdoor%20attacks%20present%20a%20serious%20security%20threat%20to%20deep%20neuron%20networks%0A%28DNNs%29.%20Although%20numerous%20effective%20defense%20techniques%20have%20been%20proposed%20in%0Arecent%20years%2C%20they%20inevitably%20rely%20on%20the%20availability%20of%20either%20clean%20or%0Apoisoned%20data.%20In%20contrast%2C%20data-free%20defense%20techniques%20have%20evolved%20slowly%0Aand%20still%20lag%20significantly%20in%20performance.%20To%20address%20this%20issue%2C%20different%0Afrom%20the%20traditional%20approach%20of%20pruning%20followed%20by%20fine-tuning%2C%20we%20propose%20a%0Anovel%20data-free%20defense%20method%20named%20Optimal%20Transport-based%20Backdoor%20Repairing%0A%28OTBR%29%20in%20this%20work.%20This%20method%2C%20based%20on%20our%20findings%20on%20neuron%20weight%0Achanges%20%28NWCs%29%20of%20random%20unlearning%2C%20uses%20optimal%20transport%20%28OT%29-based%20model%0Afusion%20to%20combine%20the%20advantages%20of%20both%20pruned%20and%20backdoored%20models.%0ASpecifically%2C%20we%20first%20demonstrate%20our%20findings%20that%20the%20NWCs%20of%20random%0Aunlearning%20are%20positively%20correlated%20with%20those%20of%20poison%20unlearning.%20Based%20on%0Athis%20observation%2C%20we%20propose%20a%20random-unlearning%20NWC%20pruning%20technique%20to%0Aeliminate%20the%20backdoor%20effect%20and%20obtain%20a%20backdoor-free%20pruned%20model.%20Then%2C%0Amotivated%20by%20the%20OT-based%20model%20fusion%2C%20we%20propose%20the%20pruned-to-backdoored%0AOT-based%20fusion%20technique%2C%20which%20fuses%20pruned%20and%20backdoored%20models%20to%20combine%0Athe%20advantages%20of%20both%2C%20resulting%20in%20a%20model%20that%20demonstrates%20high%20clean%0Aaccuracy%20and%20a%20low%20attack%20success%20rate.%20To%20our%20knowledge%2C%20this%20is%20the%20first%0Awork%20to%20apply%20OT%20and%20model%20fusion%20techniques%20to%20backdoor%20defense.%20Extensive%0Aexperiments%20show%20that%20our%20method%20successfully%20defends%20against%20all%20seven%0Abackdoor%20attacks%20across%20three%20benchmark%20datasets%2C%20outperforming%20both%0Astate-of-the-art%20%28SOTA%29%20data-free%20and%20data-dependent%20methods.%20The%20code%0Aimplementation%20and%20Appendix%20are%20provided%20in%20the%20Supplementary%20Material.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15861v1&entry.124074799=Read"},
{"title": "Object Detection for Vehicle Dashcams using Transformers", "author": "Osama Mustafa and Khizer Ali and Anam Bibi and Imran Siddiqi and Momina Moetesum", "abstract": "  The use of intelligent automation is growing significantly in the automotive\nindustry, as it assists drivers and fleet management companies, thus increasing\ntheir productivity. Dash cams are now been used for this purpose which enables\nthe instant identification and understanding of multiple objects and\noccurrences in the surroundings. In this paper, we propose a novel approach for\nobject detection in dashcams using transformers. Our system is based on the\nstate-of-the-art DEtection TRansformer (DETR), which has demonstrated strong\nperformance in a variety of conditions, including different weather and\nillumination scenarios. The use of transformers allows for the consideration of\ncontextual information in decisionmaking, improving the accuracy of object\ndetection. To validate our approach, we have trained our DETR model on a\ndataset that represents real-world conditions. Our results show that the use of\nintelligent automation through transformers can significantly enhance the\ncapabilities of dashcam systems. The model achieves an mAP of 0.95 on\ndetection.\n", "link": "http://arxiv.org/abs/2408.15809v1", "date": "2024-08-28", "relevancy": 1.941, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4874}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4843}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object%20Detection%20for%20Vehicle%20Dashcams%20using%20Transformers&body=Title%3A%20Object%20Detection%20for%20Vehicle%20Dashcams%20using%20Transformers%0AAuthor%3A%20Osama%20Mustafa%20and%20Khizer%20Ali%20and%20Anam%20Bibi%20and%20Imran%20Siddiqi%20and%20Momina%20Moetesum%0AAbstract%3A%20%20%20The%20use%20of%20intelligent%20automation%20is%20growing%20significantly%20in%20the%20automotive%0Aindustry%2C%20as%20it%20assists%20drivers%20and%20fleet%20management%20companies%2C%20thus%20increasing%0Atheir%20productivity.%20Dash%20cams%20are%20now%20been%20used%20for%20this%20purpose%20which%20enables%0Athe%20instant%20identification%20and%20understanding%20of%20multiple%20objects%20and%0Aoccurrences%20in%20the%20surroundings.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20for%0Aobject%20detection%20in%20dashcams%20using%20transformers.%20Our%20system%20is%20based%20on%20the%0Astate-of-the-art%20DEtection%20TRansformer%20%28DETR%29%2C%20which%20has%20demonstrated%20strong%0Aperformance%20in%20a%20variety%20of%20conditions%2C%20including%20different%20weather%20and%0Aillumination%20scenarios.%20The%20use%20of%20transformers%20allows%20for%20the%20consideration%20of%0Acontextual%20information%20in%20decisionmaking%2C%20improving%20the%20accuracy%20of%20object%0Adetection.%20To%20validate%20our%20approach%2C%20we%20have%20trained%20our%20DETR%20model%20on%20a%0Adataset%20that%20represents%20real-world%20conditions.%20Our%20results%20show%20that%20the%20use%20of%0Aintelligent%20automation%20through%20transformers%20can%20significantly%20enhance%20the%0Acapabilities%20of%20dashcam%20systems.%20The%20model%20achieves%20an%20mAP%20of%200.95%20on%0Adetection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject%2520Detection%2520for%2520Vehicle%2520Dashcams%2520using%2520Transformers%26entry.906535625%3DOsama%2520Mustafa%2520and%2520Khizer%2520Ali%2520and%2520Anam%2520Bibi%2520and%2520Imran%2520Siddiqi%2520and%2520Momina%2520Moetesum%26entry.1292438233%3D%2520%2520The%2520use%2520of%2520intelligent%2520automation%2520is%2520growing%2520significantly%2520in%2520the%2520automotive%250Aindustry%252C%2520as%2520it%2520assists%2520drivers%2520and%2520fleet%2520management%2520companies%252C%2520thus%2520increasing%250Atheir%2520productivity.%2520Dash%2520cams%2520are%2520now%2520been%2520used%2520for%2520this%2520purpose%2520which%2520enables%250Athe%2520instant%2520identification%2520and%2520understanding%2520of%2520multiple%2520objects%2520and%250Aoccurrences%2520in%2520the%2520surroundings.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520approach%2520for%250Aobject%2520detection%2520in%2520dashcams%2520using%2520transformers.%2520Our%2520system%2520is%2520based%2520on%2520the%250Astate-of-the-art%2520DEtection%2520TRansformer%2520%2528DETR%2529%252C%2520which%2520has%2520demonstrated%2520strong%250Aperformance%2520in%2520a%2520variety%2520of%2520conditions%252C%2520including%2520different%2520weather%2520and%250Aillumination%2520scenarios.%2520The%2520use%2520of%2520transformers%2520allows%2520for%2520the%2520consideration%2520of%250Acontextual%2520information%2520in%2520decisionmaking%252C%2520improving%2520the%2520accuracy%2520of%2520object%250Adetection.%2520To%2520validate%2520our%2520approach%252C%2520we%2520have%2520trained%2520our%2520DETR%2520model%2520on%2520a%250Adataset%2520that%2520represents%2520real-world%2520conditions.%2520Our%2520results%2520show%2520that%2520the%2520use%2520of%250Aintelligent%2520automation%2520through%2520transformers%2520can%2520significantly%2520enhance%2520the%250Acapabilities%2520of%2520dashcam%2520systems.%2520The%2520model%2520achieves%2520an%2520mAP%2520of%25200.95%2520on%250Adetection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object%20Detection%20for%20Vehicle%20Dashcams%20using%20Transformers&entry.906535625=Osama%20Mustafa%20and%20Khizer%20Ali%20and%20Anam%20Bibi%20and%20Imran%20Siddiqi%20and%20Momina%20Moetesum&entry.1292438233=%20%20The%20use%20of%20intelligent%20automation%20is%20growing%20significantly%20in%20the%20automotive%0Aindustry%2C%20as%20it%20assists%20drivers%20and%20fleet%20management%20companies%2C%20thus%20increasing%0Atheir%20productivity.%20Dash%20cams%20are%20now%20been%20used%20for%20this%20purpose%20which%20enables%0Athe%20instant%20identification%20and%20understanding%20of%20multiple%20objects%20and%0Aoccurrences%20in%20the%20surroundings.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20approach%20for%0Aobject%20detection%20in%20dashcams%20using%20transformers.%20Our%20system%20is%20based%20on%20the%0Astate-of-the-art%20DEtection%20TRansformer%20%28DETR%29%2C%20which%20has%20demonstrated%20strong%0Aperformance%20in%20a%20variety%20of%20conditions%2C%20including%20different%20weather%20and%0Aillumination%20scenarios.%20The%20use%20of%20transformers%20allows%20for%20the%20consideration%20of%0Acontextual%20information%20in%20decisionmaking%2C%20improving%20the%20accuracy%20of%20object%0Adetection.%20To%20validate%20our%20approach%2C%20we%20have%20trained%20our%20DETR%20model%20on%20a%0Adataset%20that%20represents%20real-world%20conditions.%20Our%20results%20show%20that%20the%20use%20of%0Aintelligent%20automation%20through%20transformers%20can%20significantly%20enhance%20the%0Acapabilities%20of%20dashcam%20systems.%20The%20model%20achieves%20an%20mAP%20of%200.95%20on%0Adetection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15809v1&entry.124074799=Read"},
{"title": "The Role of Fibration Symmetries in Geometric Deep Learning", "author": "Osvaldo Velarde and Lucas Parra and Paolo Boldi and Hernan Makse", "abstract": "  Geometric Deep Learning (GDL) unifies a broad class of machine learning\ntechniques from the perspectives of symmetries, offering a framework for\nintroducing problem-specific inductive biases like Graph Neural Networks\n(GNNs). However, the current formulation of GDL is limited to global symmetries\nthat are not often found in real-world problems. We propose to relax GDL to\nallow for local symmetries, specifically fibration symmetries in graphs, to\nleverage regularities of realistic instances. We show that GNNs apply the\ninductive bias of fibration symmetries and derive a tighter upper bound for\ntheir expressive power. Additionally, by identifying symmetries in networks, we\ncollapse network nodes, thereby increasing their computational efficiency\nduring both inference and training of deep neural networks. The mathematical\nextension introduced here applies beyond graphs to manifolds, bundles, and\ngrids for the development of models with inductive biases induced by local\nsymmetries that can lead to better generalization.\n", "link": "http://arxiv.org/abs/2408.15894v1", "date": "2024-08-28", "relevancy": 1.9328, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5179}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.469}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Role%20of%20Fibration%20Symmetries%20in%20Geometric%20Deep%20Learning&body=Title%3A%20The%20Role%20of%20Fibration%20Symmetries%20in%20Geometric%20Deep%20Learning%0AAuthor%3A%20Osvaldo%20Velarde%20and%20Lucas%20Parra%20and%20Paolo%20Boldi%20and%20Hernan%20Makse%0AAbstract%3A%20%20%20Geometric%20Deep%20Learning%20%28GDL%29%20unifies%20a%20broad%20class%20of%20machine%20learning%0Atechniques%20from%20the%20perspectives%20of%20symmetries%2C%20offering%20a%20framework%20for%0Aintroducing%20problem-specific%20inductive%20biases%20like%20Graph%20Neural%20Networks%0A%28GNNs%29.%20However%2C%20the%20current%20formulation%20of%20GDL%20is%20limited%20to%20global%20symmetries%0Athat%20are%20not%20often%20found%20in%20real-world%20problems.%20We%20propose%20to%20relax%20GDL%20to%0Aallow%20for%20local%20symmetries%2C%20specifically%20fibration%20symmetries%20in%20graphs%2C%20to%0Aleverage%20regularities%20of%20realistic%20instances.%20We%20show%20that%20GNNs%20apply%20the%0Ainductive%20bias%20of%20fibration%20symmetries%20and%20derive%20a%20tighter%20upper%20bound%20for%0Atheir%20expressive%20power.%20Additionally%2C%20by%20identifying%20symmetries%20in%20networks%2C%20we%0Acollapse%20network%20nodes%2C%20thereby%20increasing%20their%20computational%20efficiency%0Aduring%20both%20inference%20and%20training%20of%20deep%20neural%20networks.%20The%20mathematical%0Aextension%20introduced%20here%20applies%20beyond%20graphs%20to%20manifolds%2C%20bundles%2C%20and%0Agrids%20for%20the%20development%20of%20models%20with%20inductive%20biases%20induced%20by%20local%0Asymmetries%20that%20can%20lead%20to%20better%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15894v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Role%2520of%2520Fibration%2520Symmetries%2520in%2520Geometric%2520Deep%2520Learning%26entry.906535625%3DOsvaldo%2520Velarde%2520and%2520Lucas%2520Parra%2520and%2520Paolo%2520Boldi%2520and%2520Hernan%2520Makse%26entry.1292438233%3D%2520%2520Geometric%2520Deep%2520Learning%2520%2528GDL%2529%2520unifies%2520a%2520broad%2520class%2520of%2520machine%2520learning%250Atechniques%2520from%2520the%2520perspectives%2520of%2520symmetries%252C%2520offering%2520a%2520framework%2520for%250Aintroducing%2520problem-specific%2520inductive%2520biases%2520like%2520Graph%2520Neural%2520Networks%250A%2528GNNs%2529.%2520However%252C%2520the%2520current%2520formulation%2520of%2520GDL%2520is%2520limited%2520to%2520global%2520symmetries%250Athat%2520are%2520not%2520often%2520found%2520in%2520real-world%2520problems.%2520We%2520propose%2520to%2520relax%2520GDL%2520to%250Aallow%2520for%2520local%2520symmetries%252C%2520specifically%2520fibration%2520symmetries%2520in%2520graphs%252C%2520to%250Aleverage%2520regularities%2520of%2520realistic%2520instances.%2520We%2520show%2520that%2520GNNs%2520apply%2520the%250Ainductive%2520bias%2520of%2520fibration%2520symmetries%2520and%2520derive%2520a%2520tighter%2520upper%2520bound%2520for%250Atheir%2520expressive%2520power.%2520Additionally%252C%2520by%2520identifying%2520symmetries%2520in%2520networks%252C%2520we%250Acollapse%2520network%2520nodes%252C%2520thereby%2520increasing%2520their%2520computational%2520efficiency%250Aduring%2520both%2520inference%2520and%2520training%2520of%2520deep%2520neural%2520networks.%2520The%2520mathematical%250Aextension%2520introduced%2520here%2520applies%2520beyond%2520graphs%2520to%2520manifolds%252C%2520bundles%252C%2520and%250Agrids%2520for%2520the%2520development%2520of%2520models%2520with%2520inductive%2520biases%2520induced%2520by%2520local%250Asymmetries%2520that%2520can%2520lead%2520to%2520better%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15894v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Role%20of%20Fibration%20Symmetries%20in%20Geometric%20Deep%20Learning&entry.906535625=Osvaldo%20Velarde%20and%20Lucas%20Parra%20and%20Paolo%20Boldi%20and%20Hernan%20Makse&entry.1292438233=%20%20Geometric%20Deep%20Learning%20%28GDL%29%20unifies%20a%20broad%20class%20of%20machine%20learning%0Atechniques%20from%20the%20perspectives%20of%20symmetries%2C%20offering%20a%20framework%20for%0Aintroducing%20problem-specific%20inductive%20biases%20like%20Graph%20Neural%20Networks%0A%28GNNs%29.%20However%2C%20the%20current%20formulation%20of%20GDL%20is%20limited%20to%20global%20symmetries%0Athat%20are%20not%20often%20found%20in%20real-world%20problems.%20We%20propose%20to%20relax%20GDL%20to%0Aallow%20for%20local%20symmetries%2C%20specifically%20fibration%20symmetries%20in%20graphs%2C%20to%0Aleverage%20regularities%20of%20realistic%20instances.%20We%20show%20that%20GNNs%20apply%20the%0Ainductive%20bias%20of%20fibration%20symmetries%20and%20derive%20a%20tighter%20upper%20bound%20for%0Atheir%20expressive%20power.%20Additionally%2C%20by%20identifying%20symmetries%20in%20networks%2C%20we%0Acollapse%20network%20nodes%2C%20thereby%20increasing%20their%20computational%20efficiency%0Aduring%20both%20inference%20and%20training%20of%20deep%20neural%20networks.%20The%20mathematical%0Aextension%20introduced%20here%20applies%20beyond%20graphs%20to%20manifolds%2C%20bundles%2C%20and%0Agrids%20for%20the%20development%20of%20models%20with%20inductive%20biases%20induced%20by%20local%0Asymmetries%20that%20can%20lead%20to%20better%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15894v1&entry.124074799=Read"},
{"title": "A Metric-based Principal Curve Approach for Learning One-dimensional\n  Manifold", "author": "Elvis Han Cui", "abstract": "  Principal curve is a well-known statistical method oriented in manifold\nlearning using concepts from differential geometry. In this paper, we propose a\nnovel metric-based principal curve (MPC) method that learns one-dimensional\nmanifold of spatial data. Synthetic datasets Real applications using MNIST\ndataset show that our method can learn the one-dimensional manifold well in\nterms of the shape.\n", "link": "http://arxiv.org/abs/2405.12390v2", "date": "2024-08-28", "relevancy": 1.927, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4899}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4834}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Metric-based%20Principal%20Curve%20Approach%20for%20Learning%20One-dimensional%0A%20%20Manifold&body=Title%3A%20A%20Metric-based%20Principal%20Curve%20Approach%20for%20Learning%20One-dimensional%0A%20%20Manifold%0AAuthor%3A%20Elvis%20Han%20Cui%0AAbstract%3A%20%20%20Principal%20curve%20is%20a%20well-known%20statistical%20method%20oriented%20in%20manifold%0Alearning%20using%20concepts%20from%20differential%20geometry.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20metric-based%20principal%20curve%20%28MPC%29%20method%20that%20learns%20one-dimensional%0Amanifold%20of%20spatial%20data.%20Synthetic%20datasets%20Real%20applications%20using%20MNIST%0Adataset%20show%20that%20our%20method%20can%20learn%20the%20one-dimensional%20manifold%20well%20in%0Aterms%20of%20the%20shape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12390v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Metric-based%2520Principal%2520Curve%2520Approach%2520for%2520Learning%2520One-dimensional%250A%2520%2520Manifold%26entry.906535625%3DElvis%2520Han%2520Cui%26entry.1292438233%3D%2520%2520Principal%2520curve%2520is%2520a%2520well-known%2520statistical%2520method%2520oriented%2520in%2520manifold%250Alearning%2520using%2520concepts%2520from%2520differential%2520geometry.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520metric-based%2520principal%2520curve%2520%2528MPC%2529%2520method%2520that%2520learns%2520one-dimensional%250Amanifold%2520of%2520spatial%2520data.%2520Synthetic%2520datasets%2520Real%2520applications%2520using%2520MNIST%250Adataset%2520show%2520that%2520our%2520method%2520can%2520learn%2520the%2520one-dimensional%2520manifold%2520well%2520in%250Aterms%2520of%2520the%2520shape.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12390v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Metric-based%20Principal%20Curve%20Approach%20for%20Learning%20One-dimensional%0A%20%20Manifold&entry.906535625=Elvis%20Han%20Cui&entry.1292438233=%20%20Principal%20curve%20is%20a%20well-known%20statistical%20method%20oriented%20in%20manifold%0Alearning%20using%20concepts%20from%20differential%20geometry.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20metric-based%20principal%20curve%20%28MPC%29%20method%20that%20learns%20one-dimensional%0Amanifold%20of%20spatial%20data.%20Synthetic%20datasets%20Real%20applications%20using%20MNIST%0Adataset%20show%20that%20our%20method%20can%20learn%20the%20one-dimensional%20manifold%20well%20in%0Aterms%20of%20the%20shape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12390v2&entry.124074799=Read"},
{"title": "Marked Neural Spatio-Temporal Point Process Involving a Dynamic Graph\n  Neural Network", "author": "Alice Moallemy-Oureh and Silvia Beddar-Wiesing and Yannick Nagel and R\u00fcdiger Nather and Josephine M. Thomas", "abstract": "  Temporal Point Processes (TPPs) have recently become increasingly interesting\nfor learning dynamics in graph data. A reason for this is that learning on\ndynamic graph data is becoming more relevant, since data from many scientific\nfields, ranging from mathematics, biology, social sciences, and physics to\ncomputer science, is naturally related and inherently dynamic. In addition,\nTPPs provide a meaningful characterization of event streams and a prediction\nmechanism for future events. Therefore, (semi-)parameterized Neural TPPs have\nbeen introduced whose characterization can be (partially) learned and, thus,\nenable the representation of more complex phenomena. However, the research on\nmodeling dynamic graphs with TPPs is relatively young, and only a few models\nfor node attribute changes or evolving edges have been proposed yet. To allow\nfor learning on fully dynamic graph streams, i.e., graphs that can change in\ntheir structure (addition/deletion of nodes/edge) and in their node/edge\nattributes, we propose a Marked Neural Spatio-Temporal Point Process (MNSTPP).\nIt leverages a Dynamic Graph Neural Network to learn a Marked TPP that handles\nattributes and spatial data to model and predict any event in a graph stream.\n", "link": "http://arxiv.org/abs/2206.03469v2", "date": "2024-08-28", "relevancy": 1.9224, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5117}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4617}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Marked%20Neural%20Spatio-Temporal%20Point%20Process%20Involving%20a%20Dynamic%20Graph%0A%20%20Neural%20Network&body=Title%3A%20Marked%20Neural%20Spatio-Temporal%20Point%20Process%20Involving%20a%20Dynamic%20Graph%0A%20%20Neural%20Network%0AAuthor%3A%20Alice%20Moallemy-Oureh%20and%20Silvia%20Beddar-Wiesing%20and%20Yannick%20Nagel%20and%20R%C3%BCdiger%20Nather%20and%20Josephine%20M.%20Thomas%0AAbstract%3A%20%20%20Temporal%20Point%20Processes%20%28TPPs%29%20have%20recently%20become%20increasingly%20interesting%0Afor%20learning%20dynamics%20in%20graph%20data.%20A%20reason%20for%20this%20is%20that%20learning%20on%0Adynamic%20graph%20data%20is%20becoming%20more%20relevant%2C%20since%20data%20from%20many%20scientific%0Afields%2C%20ranging%20from%20mathematics%2C%20biology%2C%20social%20sciences%2C%20and%20physics%20to%0Acomputer%20science%2C%20is%20naturally%20related%20and%20inherently%20dynamic.%20In%20addition%2C%0ATPPs%20provide%20a%20meaningful%20characterization%20of%20event%20streams%20and%20a%20prediction%0Amechanism%20for%20future%20events.%20Therefore%2C%20%28semi-%29parameterized%20Neural%20TPPs%20have%0Abeen%20introduced%20whose%20characterization%20can%20be%20%28partially%29%20learned%20and%2C%20thus%2C%0Aenable%20the%20representation%20of%20more%20complex%20phenomena.%20However%2C%20the%20research%20on%0Amodeling%20dynamic%20graphs%20with%20TPPs%20is%20relatively%20young%2C%20and%20only%20a%20few%20models%0Afor%20node%20attribute%20changes%20or%20evolving%20edges%20have%20been%20proposed%20yet.%20To%20allow%0Afor%20learning%20on%20fully%20dynamic%20graph%20streams%2C%20i.e.%2C%20graphs%20that%20can%20change%20in%0Atheir%20structure%20%28addition/deletion%20of%20nodes/edge%29%20and%20in%20their%20node/edge%0Aattributes%2C%20we%20propose%20a%20Marked%20Neural%20Spatio-Temporal%20Point%20Process%20%28MNSTPP%29.%0AIt%20leverages%20a%20Dynamic%20Graph%20Neural%20Network%20to%20learn%20a%20Marked%20TPP%20that%20handles%0Aattributes%20and%20spatial%20data%20to%20model%20and%20predict%20any%20event%20in%20a%20graph%20stream.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2206.03469v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMarked%2520Neural%2520Spatio-Temporal%2520Point%2520Process%2520Involving%2520a%2520Dynamic%2520Graph%250A%2520%2520Neural%2520Network%26entry.906535625%3DAlice%2520Moallemy-Oureh%2520and%2520Silvia%2520Beddar-Wiesing%2520and%2520Yannick%2520Nagel%2520and%2520R%25C3%25BCdiger%2520Nather%2520and%2520Josephine%2520M.%2520Thomas%26entry.1292438233%3D%2520%2520Temporal%2520Point%2520Processes%2520%2528TPPs%2529%2520have%2520recently%2520become%2520increasingly%2520interesting%250Afor%2520learning%2520dynamics%2520in%2520graph%2520data.%2520A%2520reason%2520for%2520this%2520is%2520that%2520learning%2520on%250Adynamic%2520graph%2520data%2520is%2520becoming%2520more%2520relevant%252C%2520since%2520data%2520from%2520many%2520scientific%250Afields%252C%2520ranging%2520from%2520mathematics%252C%2520biology%252C%2520social%2520sciences%252C%2520and%2520physics%2520to%250Acomputer%2520science%252C%2520is%2520naturally%2520related%2520and%2520inherently%2520dynamic.%2520In%2520addition%252C%250ATPPs%2520provide%2520a%2520meaningful%2520characterization%2520of%2520event%2520streams%2520and%2520a%2520prediction%250Amechanism%2520for%2520future%2520events.%2520Therefore%252C%2520%2528semi-%2529parameterized%2520Neural%2520TPPs%2520have%250Abeen%2520introduced%2520whose%2520characterization%2520can%2520be%2520%2528partially%2529%2520learned%2520and%252C%2520thus%252C%250Aenable%2520the%2520representation%2520of%2520more%2520complex%2520phenomena.%2520However%252C%2520the%2520research%2520on%250Amodeling%2520dynamic%2520graphs%2520with%2520TPPs%2520is%2520relatively%2520young%252C%2520and%2520only%2520a%2520few%2520models%250Afor%2520node%2520attribute%2520changes%2520or%2520evolving%2520edges%2520have%2520been%2520proposed%2520yet.%2520To%2520allow%250Afor%2520learning%2520on%2520fully%2520dynamic%2520graph%2520streams%252C%2520i.e.%252C%2520graphs%2520that%2520can%2520change%2520in%250Atheir%2520structure%2520%2528addition/deletion%2520of%2520nodes/edge%2529%2520and%2520in%2520their%2520node/edge%250Aattributes%252C%2520we%2520propose%2520a%2520Marked%2520Neural%2520Spatio-Temporal%2520Point%2520Process%2520%2528MNSTPP%2529.%250AIt%2520leverages%2520a%2520Dynamic%2520Graph%2520Neural%2520Network%2520to%2520learn%2520a%2520Marked%2520TPP%2520that%2520handles%250Aattributes%2520and%2520spatial%2520data%2520to%2520model%2520and%2520predict%2520any%2520event%2520in%2520a%2520graph%2520stream.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2206.03469v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Marked%20Neural%20Spatio-Temporal%20Point%20Process%20Involving%20a%20Dynamic%20Graph%0A%20%20Neural%20Network&entry.906535625=Alice%20Moallemy-Oureh%20and%20Silvia%20Beddar-Wiesing%20and%20Yannick%20Nagel%20and%20R%C3%BCdiger%20Nather%20and%20Josephine%20M.%20Thomas&entry.1292438233=%20%20Temporal%20Point%20Processes%20%28TPPs%29%20have%20recently%20become%20increasingly%20interesting%0Afor%20learning%20dynamics%20in%20graph%20data.%20A%20reason%20for%20this%20is%20that%20learning%20on%0Adynamic%20graph%20data%20is%20becoming%20more%20relevant%2C%20since%20data%20from%20many%20scientific%0Afields%2C%20ranging%20from%20mathematics%2C%20biology%2C%20social%20sciences%2C%20and%20physics%20to%0Acomputer%20science%2C%20is%20naturally%20related%20and%20inherently%20dynamic.%20In%20addition%2C%0ATPPs%20provide%20a%20meaningful%20characterization%20of%20event%20streams%20and%20a%20prediction%0Amechanism%20for%20future%20events.%20Therefore%2C%20%28semi-%29parameterized%20Neural%20TPPs%20have%0Abeen%20introduced%20whose%20characterization%20can%20be%20%28partially%29%20learned%20and%2C%20thus%2C%0Aenable%20the%20representation%20of%20more%20complex%20phenomena.%20However%2C%20the%20research%20on%0Amodeling%20dynamic%20graphs%20with%20TPPs%20is%20relatively%20young%2C%20and%20only%20a%20few%20models%0Afor%20node%20attribute%20changes%20or%20evolving%20edges%20have%20been%20proposed%20yet.%20To%20allow%0Afor%20learning%20on%20fully%20dynamic%20graph%20streams%2C%20i.e.%2C%20graphs%20that%20can%20change%20in%0Atheir%20structure%20%28addition/deletion%20of%20nodes/edge%29%20and%20in%20their%20node/edge%0Aattributes%2C%20we%20propose%20a%20Marked%20Neural%20Spatio-Temporal%20Point%20Process%20%28MNSTPP%29.%0AIt%20leverages%20a%20Dynamic%20Graph%20Neural%20Network%20to%20learn%20a%20Marked%20TPP%20that%20handles%0Aattributes%20and%20spatial%20data%20to%20model%20and%20predict%20any%20event%20in%20a%20graph%20stream.%0A&entry.1838667208=http%3A//arxiv.org/abs/2206.03469v2&entry.124074799=Read"},
{"title": "Infusion: internal diffusion for inpainting of dynamic textures and\n  complex motion", "author": "Nicolas Cherel and Andr\u00e9s Almansa and Yann Gousseau and Alasdair Newson", "abstract": "  Video inpainting is the task of filling a region in a video in a visually\nconvincing manner. It is very challenging due to the high dimensionality of the\ndata and the temporal consistency required for obtaining convincing results.\nRecently, diffusion models have shown impressive results in modeling complex\ndata distributions, including images and videos. Such models remain nonetheless\nvery expensive to train and to perform inference with, which strongly reduce\ntheir applicability to videos, and yields unreasonable computational loads. We\nshow that in the case of video inpainting, thanks to the highly auto-similar\nnature of videos, the training data of a diffusion model can be restricted to\nthe input video and still produce very satisfying results. This leads us to\nadopt an internal learning approach, which also allows us to greatly reduce the\nneural network size by about three orders of magnitude less than current\ndiffusion models used for image inpainting. We also introduce a new method for\nefficient training and inference of diffusion models in the context of internal\nlearning, by splitting the diffusion process into different learning intervals\ncorresponding to different noise levels of the diffusion process. To the best\nof our knowledge, this is the first video inpainting method based purely on\ndiffusion. Other methods require additional components such as optical flow\nestimation, which limits their performance in the case of dynamic textures and\ncomplex motions. We show qualitative and quantitative results, demonstrating\nthat our method reaches state of the art performance in the case of dynamic\ntextures and complex dynamic backgrounds.\n", "link": "http://arxiv.org/abs/2311.01090v3", "date": "2024-08-28", "relevancy": 1.901, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6626}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6271}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Infusion%3A%20internal%20diffusion%20for%20inpainting%20of%20dynamic%20textures%20and%0A%20%20complex%20motion&body=Title%3A%20Infusion%3A%20internal%20diffusion%20for%20inpainting%20of%20dynamic%20textures%20and%0A%20%20complex%20motion%0AAuthor%3A%20Nicolas%20Cherel%20and%20Andr%C3%A9s%20Almansa%20and%20Yann%20Gousseau%20and%20Alasdair%20Newson%0AAbstract%3A%20%20%20Video%20inpainting%20is%20the%20task%20of%20filling%20a%20region%20in%20a%20video%20in%20a%20visually%0Aconvincing%20manner.%20It%20is%20very%20challenging%20due%20to%20the%20high%20dimensionality%20of%20the%0Adata%20and%20the%20temporal%20consistency%20required%20for%20obtaining%20convincing%20results.%0ARecently%2C%20diffusion%20models%20have%20shown%20impressive%20results%20in%20modeling%20complex%0Adata%20distributions%2C%20including%20images%20and%20videos.%20Such%20models%20remain%20nonetheless%0Avery%20expensive%20to%20train%20and%20to%20perform%20inference%20with%2C%20which%20strongly%20reduce%0Atheir%20applicability%20to%20videos%2C%20and%20yields%20unreasonable%20computational%20loads.%20We%0Ashow%20that%20in%20the%20case%20of%20video%20inpainting%2C%20thanks%20to%20the%20highly%20auto-similar%0Anature%20of%20videos%2C%20the%20training%20data%20of%20a%20diffusion%20model%20can%20be%20restricted%20to%0Athe%20input%20video%20and%20still%20produce%20very%20satisfying%20results.%20This%20leads%20us%20to%0Aadopt%20an%20internal%20learning%20approach%2C%20which%20also%20allows%20us%20to%20greatly%20reduce%20the%0Aneural%20network%20size%20by%20about%20three%20orders%20of%20magnitude%20less%20than%20current%0Adiffusion%20models%20used%20for%20image%20inpainting.%20We%20also%20introduce%20a%20new%20method%20for%0Aefficient%20training%20and%20inference%20of%20diffusion%20models%20in%20the%20context%20of%20internal%0Alearning%2C%20by%20splitting%20the%20diffusion%20process%20into%20different%20learning%20intervals%0Acorresponding%20to%20different%20noise%20levels%20of%20the%20diffusion%20process.%20To%20the%20best%0Aof%20our%20knowledge%2C%20this%20is%20the%20first%20video%20inpainting%20method%20based%20purely%20on%0Adiffusion.%20Other%20methods%20require%20additional%20components%20such%20as%20optical%20flow%0Aestimation%2C%20which%20limits%20their%20performance%20in%20the%20case%20of%20dynamic%20textures%20and%0Acomplex%20motions.%20We%20show%20qualitative%20and%20quantitative%20results%2C%20demonstrating%0Athat%20our%20method%20reaches%20state%20of%20the%20art%20performance%20in%20the%20case%20of%20dynamic%0Atextures%20and%20complex%20dynamic%20backgrounds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.01090v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfusion%253A%2520internal%2520diffusion%2520for%2520inpainting%2520of%2520dynamic%2520textures%2520and%250A%2520%2520complex%2520motion%26entry.906535625%3DNicolas%2520Cherel%2520and%2520Andr%25C3%25A9s%2520Almansa%2520and%2520Yann%2520Gousseau%2520and%2520Alasdair%2520Newson%26entry.1292438233%3D%2520%2520Video%2520inpainting%2520is%2520the%2520task%2520of%2520filling%2520a%2520region%2520in%2520a%2520video%2520in%2520a%2520visually%250Aconvincing%2520manner.%2520It%2520is%2520very%2520challenging%2520due%2520to%2520the%2520high%2520dimensionality%2520of%2520the%250Adata%2520and%2520the%2520temporal%2520consistency%2520required%2520for%2520obtaining%2520convincing%2520results.%250ARecently%252C%2520diffusion%2520models%2520have%2520shown%2520impressive%2520results%2520in%2520modeling%2520complex%250Adata%2520distributions%252C%2520including%2520images%2520and%2520videos.%2520Such%2520models%2520remain%2520nonetheless%250Avery%2520expensive%2520to%2520train%2520and%2520to%2520perform%2520inference%2520with%252C%2520which%2520strongly%2520reduce%250Atheir%2520applicability%2520to%2520videos%252C%2520and%2520yields%2520unreasonable%2520computational%2520loads.%2520We%250Ashow%2520that%2520in%2520the%2520case%2520of%2520video%2520inpainting%252C%2520thanks%2520to%2520the%2520highly%2520auto-similar%250Anature%2520of%2520videos%252C%2520the%2520training%2520data%2520of%2520a%2520diffusion%2520model%2520can%2520be%2520restricted%2520to%250Athe%2520input%2520video%2520and%2520still%2520produce%2520very%2520satisfying%2520results.%2520This%2520leads%2520us%2520to%250Aadopt%2520an%2520internal%2520learning%2520approach%252C%2520which%2520also%2520allows%2520us%2520to%2520greatly%2520reduce%2520the%250Aneural%2520network%2520size%2520by%2520about%2520three%2520orders%2520of%2520magnitude%2520less%2520than%2520current%250Adiffusion%2520models%2520used%2520for%2520image%2520inpainting.%2520We%2520also%2520introduce%2520a%2520new%2520method%2520for%250Aefficient%2520training%2520and%2520inference%2520of%2520diffusion%2520models%2520in%2520the%2520context%2520of%2520internal%250Alearning%252C%2520by%2520splitting%2520the%2520diffusion%2520process%2520into%2520different%2520learning%2520intervals%250Acorresponding%2520to%2520different%2520noise%2520levels%2520of%2520the%2520diffusion%2520process.%2520To%2520the%2520best%250Aof%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520video%2520inpainting%2520method%2520based%2520purely%2520on%250Adiffusion.%2520Other%2520methods%2520require%2520additional%2520components%2520such%2520as%2520optical%2520flow%250Aestimation%252C%2520which%2520limits%2520their%2520performance%2520in%2520the%2520case%2520of%2520dynamic%2520textures%2520and%250Acomplex%2520motions.%2520We%2520show%2520qualitative%2520and%2520quantitative%2520results%252C%2520demonstrating%250Athat%2520our%2520method%2520reaches%2520state%2520of%2520the%2520art%2520performance%2520in%2520the%2520case%2520of%2520dynamic%250Atextures%2520and%2520complex%2520dynamic%2520backgrounds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.01090v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infusion%3A%20internal%20diffusion%20for%20inpainting%20of%20dynamic%20textures%20and%0A%20%20complex%20motion&entry.906535625=Nicolas%20Cherel%20and%20Andr%C3%A9s%20Almansa%20and%20Yann%20Gousseau%20and%20Alasdair%20Newson&entry.1292438233=%20%20Video%20inpainting%20is%20the%20task%20of%20filling%20a%20region%20in%20a%20video%20in%20a%20visually%0Aconvincing%20manner.%20It%20is%20very%20challenging%20due%20to%20the%20high%20dimensionality%20of%20the%0Adata%20and%20the%20temporal%20consistency%20required%20for%20obtaining%20convincing%20results.%0ARecently%2C%20diffusion%20models%20have%20shown%20impressive%20results%20in%20modeling%20complex%0Adata%20distributions%2C%20including%20images%20and%20videos.%20Such%20models%20remain%20nonetheless%0Avery%20expensive%20to%20train%20and%20to%20perform%20inference%20with%2C%20which%20strongly%20reduce%0Atheir%20applicability%20to%20videos%2C%20and%20yields%20unreasonable%20computational%20loads.%20We%0Ashow%20that%20in%20the%20case%20of%20video%20inpainting%2C%20thanks%20to%20the%20highly%20auto-similar%0Anature%20of%20videos%2C%20the%20training%20data%20of%20a%20diffusion%20model%20can%20be%20restricted%20to%0Athe%20input%20video%20and%20still%20produce%20very%20satisfying%20results.%20This%20leads%20us%20to%0Aadopt%20an%20internal%20learning%20approach%2C%20which%20also%20allows%20us%20to%20greatly%20reduce%20the%0Aneural%20network%20size%20by%20about%20three%20orders%20of%20magnitude%20less%20than%20current%0Adiffusion%20models%20used%20for%20image%20inpainting.%20We%20also%20introduce%20a%20new%20method%20for%0Aefficient%20training%20and%20inference%20of%20diffusion%20models%20in%20the%20context%20of%20internal%0Alearning%2C%20by%20splitting%20the%20diffusion%20process%20into%20different%20learning%20intervals%0Acorresponding%20to%20different%20noise%20levels%20of%20the%20diffusion%20process.%20To%20the%20best%0Aof%20our%20knowledge%2C%20this%20is%20the%20first%20video%20inpainting%20method%20based%20purely%20on%0Adiffusion.%20Other%20methods%20require%20additional%20components%20such%20as%20optical%20flow%0Aestimation%2C%20which%20limits%20their%20performance%20in%20the%20case%20of%20dynamic%20textures%20and%0Acomplex%20motions.%20We%20show%20qualitative%20and%20quantitative%20results%2C%20demonstrating%0Athat%20our%20method%20reaches%20state%20of%20the%20art%20performance%20in%20the%20case%20of%20dynamic%0Atextures%20and%20complex%20dynamic%20backgrounds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.01090v3&entry.124074799=Read"},
{"title": "HC3 Plus: A Semantic-Invariant Human ChatGPT Comparison Corpus", "author": "Zhenpeng Su and Xing Wu and Wei Zhou and Guangyuan Ma and Songlin Hu", "abstract": "  ChatGPT has garnered significant interest due to its impressive performance;\nhowever, there is growing concern about its potential risks, particularly in\nthe detection of AI-generated content (AIGC), which is often challenging for\nuntrained individuals to identify. Current datasets used for detecting\nChatGPT-generated text primarily focus on question-answering tasks, often\noverlooking tasks with semantic-invariant properties, such as summarization,\ntranslation, and paraphrasing. In this paper, we demonstrate that detecting\nmodel-generated text in semantic-invariant tasks is more challenging. To\naddress this gap, we introduce a more extensive and comprehensive dataset that\nincorporates a wider range of tasks than previous work, including those with\nsemantic-invariant properties.\n", "link": "http://arxiv.org/abs/2309.02731v3", "date": "2024-08-28", "relevancy": 1.8954, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4909}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4682}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HC3%20Plus%3A%20A%20Semantic-Invariant%20Human%20ChatGPT%20Comparison%20Corpus&body=Title%3A%20HC3%20Plus%3A%20A%20Semantic-Invariant%20Human%20ChatGPT%20Comparison%20Corpus%0AAuthor%3A%20Zhenpeng%20Su%20and%20Xing%20Wu%20and%20Wei%20Zhou%20and%20Guangyuan%20Ma%20and%20Songlin%20Hu%0AAbstract%3A%20%20%20ChatGPT%20has%20garnered%20significant%20interest%20due%20to%20its%20impressive%20performance%3B%0Ahowever%2C%20there%20is%20growing%20concern%20about%20its%20potential%20risks%2C%20particularly%20in%0Athe%20detection%20of%20AI-generated%20content%20%28AIGC%29%2C%20which%20is%20often%20challenging%20for%0Auntrained%20individuals%20to%20identify.%20Current%20datasets%20used%20for%20detecting%0AChatGPT-generated%20text%20primarily%20focus%20on%20question-answering%20tasks%2C%20often%0Aoverlooking%20tasks%20with%20semantic-invariant%20properties%2C%20such%20as%20summarization%2C%0Atranslation%2C%20and%20paraphrasing.%20In%20this%20paper%2C%20we%20demonstrate%20that%20detecting%0Amodel-generated%20text%20in%20semantic-invariant%20tasks%20is%20more%20challenging.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20a%20more%20extensive%20and%20comprehensive%20dataset%20that%0Aincorporates%20a%20wider%20range%20of%20tasks%20than%20previous%20work%2C%20including%20those%20with%0Asemantic-invariant%20properties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.02731v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHC3%2520Plus%253A%2520A%2520Semantic-Invariant%2520Human%2520ChatGPT%2520Comparison%2520Corpus%26entry.906535625%3DZhenpeng%2520Su%2520and%2520Xing%2520Wu%2520and%2520Wei%2520Zhou%2520and%2520Guangyuan%2520Ma%2520and%2520Songlin%2520Hu%26entry.1292438233%3D%2520%2520ChatGPT%2520has%2520garnered%2520significant%2520interest%2520due%2520to%2520its%2520impressive%2520performance%253B%250Ahowever%252C%2520there%2520is%2520growing%2520concern%2520about%2520its%2520potential%2520risks%252C%2520particularly%2520in%250Athe%2520detection%2520of%2520AI-generated%2520content%2520%2528AIGC%2529%252C%2520which%2520is%2520often%2520challenging%2520for%250Auntrained%2520individuals%2520to%2520identify.%2520Current%2520datasets%2520used%2520for%2520detecting%250AChatGPT-generated%2520text%2520primarily%2520focus%2520on%2520question-answering%2520tasks%252C%2520often%250Aoverlooking%2520tasks%2520with%2520semantic-invariant%2520properties%252C%2520such%2520as%2520summarization%252C%250Atranslation%252C%2520and%2520paraphrasing.%2520In%2520this%2520paper%252C%2520we%2520demonstrate%2520that%2520detecting%250Amodel-generated%2520text%2520in%2520semantic-invariant%2520tasks%2520is%2520more%2520challenging.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520introduce%2520a%2520more%2520extensive%2520and%2520comprehensive%2520dataset%2520that%250Aincorporates%2520a%2520wider%2520range%2520of%2520tasks%2520than%2520previous%2520work%252C%2520including%2520those%2520with%250Asemantic-invariant%2520properties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.02731v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HC3%20Plus%3A%20A%20Semantic-Invariant%20Human%20ChatGPT%20Comparison%20Corpus&entry.906535625=Zhenpeng%20Su%20and%20Xing%20Wu%20and%20Wei%20Zhou%20and%20Guangyuan%20Ma%20and%20Songlin%20Hu&entry.1292438233=%20%20ChatGPT%20has%20garnered%20significant%20interest%20due%20to%20its%20impressive%20performance%3B%0Ahowever%2C%20there%20is%20growing%20concern%20about%20its%20potential%20risks%2C%20particularly%20in%0Athe%20detection%20of%20AI-generated%20content%20%28AIGC%29%2C%20which%20is%20often%20challenging%20for%0Auntrained%20individuals%20to%20identify.%20Current%20datasets%20used%20for%20detecting%0AChatGPT-generated%20text%20primarily%20focus%20on%20question-answering%20tasks%2C%20often%0Aoverlooking%20tasks%20with%20semantic-invariant%20properties%2C%20such%20as%20summarization%2C%0Atranslation%2C%20and%20paraphrasing.%20In%20this%20paper%2C%20we%20demonstrate%20that%20detecting%0Amodel-generated%20text%20in%20semantic-invariant%20tasks%20is%20more%20challenging.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20a%20more%20extensive%20and%20comprehensive%20dataset%20that%0Aincorporates%20a%20wider%20range%20of%20tasks%20than%20previous%20work%2C%20including%20those%20with%0Asemantic-invariant%20properties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.02731v3&entry.124074799=Read"},
{"title": "Examining Pathological Bias in a Generative Adversarial Network\n  Discriminator: A Case Study on a StyleGAN3 Model", "author": "Alvin Grissom II and Ryan F. Lei and Matt Gusdorff and Jeova Farias Sales Rocha Neto and Bailey Lin and Ryan Trotter", "abstract": "  Generative adversarial networks (GANs) generate photorealistic faces that are\noften indistinguishable by humans from real faces. While biases in machine\nlearning models are often assumed to be due to biases in training data, we find\npathological internal color and luminance biases in the discriminator of a\npre-trained StyleGAN3-r model that are not explicable by the training data. We\nalso find that the discriminator systematically stratifies scores by both\nimage- and face-level qualities and that this disproportionately affects images\nacross gender, race, and other categories. We examine axes common in research\non stereotyping in social psychology.\n", "link": "http://arxiv.org/abs/2402.09786v4", "date": "2024-08-28", "relevancy": 1.8939, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4847}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4794}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Examining%20Pathological%20Bias%20in%20a%20Generative%20Adversarial%20Network%0A%20%20Discriminator%3A%20A%20Case%20Study%20on%20a%20StyleGAN3%20Model&body=Title%3A%20Examining%20Pathological%20Bias%20in%20a%20Generative%20Adversarial%20Network%0A%20%20Discriminator%3A%20A%20Case%20Study%20on%20a%20StyleGAN3%20Model%0AAuthor%3A%20Alvin%20Grissom%20II%20and%20Ryan%20F.%20Lei%20and%20Matt%20Gusdorff%20and%20Jeova%20Farias%20Sales%20Rocha%20Neto%20and%20Bailey%20Lin%20and%20Ryan%20Trotter%0AAbstract%3A%20%20%20Generative%20adversarial%20networks%20%28GANs%29%20generate%20photorealistic%20faces%20that%20are%0Aoften%20indistinguishable%20by%20humans%20from%20real%20faces.%20While%20biases%20in%20machine%0Alearning%20models%20are%20often%20assumed%20to%20be%20due%20to%20biases%20in%20training%20data%2C%20we%20find%0Apathological%20internal%20color%20and%20luminance%20biases%20in%20the%20discriminator%20of%20a%0Apre-trained%20StyleGAN3-r%20model%20that%20are%20not%20explicable%20by%20the%20training%20data.%20We%0Aalso%20find%20that%20the%20discriminator%20systematically%20stratifies%20scores%20by%20both%0Aimage-%20and%20face-level%20qualities%20and%20that%20this%20disproportionately%20affects%20images%0Aacross%20gender%2C%20race%2C%20and%20other%20categories.%20We%20examine%20axes%20common%20in%20research%0Aon%20stereotyping%20in%20social%20psychology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09786v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExamining%2520Pathological%2520Bias%2520in%2520a%2520Generative%2520Adversarial%2520Network%250A%2520%2520Discriminator%253A%2520A%2520Case%2520Study%2520on%2520a%2520StyleGAN3%2520Model%26entry.906535625%3DAlvin%2520Grissom%2520II%2520and%2520Ryan%2520F.%2520Lei%2520and%2520Matt%2520Gusdorff%2520and%2520Jeova%2520Farias%2520Sales%2520Rocha%2520Neto%2520and%2520Bailey%2520Lin%2520and%2520Ryan%2520Trotter%26entry.1292438233%3D%2520%2520Generative%2520adversarial%2520networks%2520%2528GANs%2529%2520generate%2520photorealistic%2520faces%2520that%2520are%250Aoften%2520indistinguishable%2520by%2520humans%2520from%2520real%2520faces.%2520While%2520biases%2520in%2520machine%250Alearning%2520models%2520are%2520often%2520assumed%2520to%2520be%2520due%2520to%2520biases%2520in%2520training%2520data%252C%2520we%2520find%250Apathological%2520internal%2520color%2520and%2520luminance%2520biases%2520in%2520the%2520discriminator%2520of%2520a%250Apre-trained%2520StyleGAN3-r%2520model%2520that%2520are%2520not%2520explicable%2520by%2520the%2520training%2520data.%2520We%250Aalso%2520find%2520that%2520the%2520discriminator%2520systematically%2520stratifies%2520scores%2520by%2520both%250Aimage-%2520and%2520face-level%2520qualities%2520and%2520that%2520this%2520disproportionately%2520affects%2520images%250Aacross%2520gender%252C%2520race%252C%2520and%2520other%2520categories.%2520We%2520examine%2520axes%2520common%2520in%2520research%250Aon%2520stereotyping%2520in%2520social%2520psychology.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09786v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Examining%20Pathological%20Bias%20in%20a%20Generative%20Adversarial%20Network%0A%20%20Discriminator%3A%20A%20Case%20Study%20on%20a%20StyleGAN3%20Model&entry.906535625=Alvin%20Grissom%20II%20and%20Ryan%20F.%20Lei%20and%20Matt%20Gusdorff%20and%20Jeova%20Farias%20Sales%20Rocha%20Neto%20and%20Bailey%20Lin%20and%20Ryan%20Trotter&entry.1292438233=%20%20Generative%20adversarial%20networks%20%28GANs%29%20generate%20photorealistic%20faces%20that%20are%0Aoften%20indistinguishable%20by%20humans%20from%20real%20faces.%20While%20biases%20in%20machine%0Alearning%20models%20are%20often%20assumed%20to%20be%20due%20to%20biases%20in%20training%20data%2C%20we%20find%0Apathological%20internal%20color%20and%20luminance%20biases%20in%20the%20discriminator%20of%20a%0Apre-trained%20StyleGAN3-r%20model%20that%20are%20not%20explicable%20by%20the%20training%20data.%20We%0Aalso%20find%20that%20the%20discriminator%20systematically%20stratifies%20scores%20by%20both%0Aimage-%20and%20face-level%20qualities%20and%20that%20this%20disproportionately%20affects%20images%0Aacross%20gender%2C%20race%2C%20and%20other%20categories.%20We%20examine%20axes%20common%20in%20research%0Aon%20stereotyping%20in%20social%20psychology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09786v4&entry.124074799=Read"},
{"title": "ClimDetect: A Benchmark Dataset for Climate Change Detection and\n  Attribution", "author": "Sungduk Yu and Brian L. White and Anahita Bhiwandiwalla and Musashi Hinck and Matthew Lyle Olson and Tung Nguyen and Vasudev Lal", "abstract": "  Detecting and attributing temperature increases due to climate change is\ncrucial for understanding global warming and guiding adaptation strategies. The\ncomplexity of distinguishing human-induced climate signals from natural\nvariability has challenged traditional detection and attribution (D&A)\napproaches, which seek to identify specific \"fingerprints\" in climate response\nvariables. Deep learning offers potential for discerning these complex patterns\nin expansive spatial datasets. However, lack of standard protocols has hindered\nconsistent comparisons across studies. We introduce ClimDetect, a standardized\ndataset of over 816k daily climate snapshots, designed to enhance model\naccuracy in identifying climate change signals. ClimDetect integrates various\ninput and target variables used in past research, ensuring comparability and\nconsistency. We also explore the application of vision transformers (ViT) to\nclimate data, a novel and modernizing approach in this context. Our open-access\ndata and code serve as a benchmark for advancing climate science through\nimproved model evaluations. ClimDetect is publicly accessible via Huggingface\ndataet respository at: https://huggingface.co/datasets/ClimDetect/ClimDetect.\n", "link": "http://arxiv.org/abs/2408.15993v1", "date": "2024-08-28", "relevancy": 1.8813, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4779}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4712}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4664}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ClimDetect%3A%20A%20Benchmark%20Dataset%20for%20Climate%20Change%20Detection%20and%0A%20%20Attribution&body=Title%3A%20ClimDetect%3A%20A%20Benchmark%20Dataset%20for%20Climate%20Change%20Detection%20and%0A%20%20Attribution%0AAuthor%3A%20Sungduk%20Yu%20and%20Brian%20L.%20White%20and%20Anahita%20Bhiwandiwalla%20and%20Musashi%20Hinck%20and%20Matthew%20Lyle%20Olson%20and%20Tung%20Nguyen%20and%20Vasudev%20Lal%0AAbstract%3A%20%20%20Detecting%20and%20attributing%20temperature%20increases%20due%20to%20climate%20change%20is%0Acrucial%20for%20understanding%20global%20warming%20and%20guiding%20adaptation%20strategies.%20The%0Acomplexity%20of%20distinguishing%20human-induced%20climate%20signals%20from%20natural%0Avariability%20has%20challenged%20traditional%20detection%20and%20attribution%20%28D%26A%29%0Aapproaches%2C%20which%20seek%20to%20identify%20specific%20%22fingerprints%22%20in%20climate%20response%0Avariables.%20Deep%20learning%20offers%20potential%20for%20discerning%20these%20complex%20patterns%0Ain%20expansive%20spatial%20datasets.%20However%2C%20lack%20of%20standard%20protocols%20has%20hindered%0Aconsistent%20comparisons%20across%20studies.%20We%20introduce%20ClimDetect%2C%20a%20standardized%0Adataset%20of%20over%20816k%20daily%20climate%20snapshots%2C%20designed%20to%20enhance%20model%0Aaccuracy%20in%20identifying%20climate%20change%20signals.%20ClimDetect%20integrates%20various%0Ainput%20and%20target%20variables%20used%20in%20past%20research%2C%20ensuring%20comparability%20and%0Aconsistency.%20We%20also%20explore%20the%20application%20of%20vision%20transformers%20%28ViT%29%20to%0Aclimate%20data%2C%20a%20novel%20and%20modernizing%20approach%20in%20this%20context.%20Our%20open-access%0Adata%20and%20code%20serve%20as%20a%20benchmark%20for%20advancing%20climate%20science%20through%0Aimproved%20model%20evaluations.%20ClimDetect%20is%20publicly%20accessible%20via%20Huggingface%0Adataet%20respository%20at%3A%20https%3A//huggingface.co/datasets/ClimDetect/ClimDetect.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15993v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClimDetect%253A%2520A%2520Benchmark%2520Dataset%2520for%2520Climate%2520Change%2520Detection%2520and%250A%2520%2520Attribution%26entry.906535625%3DSungduk%2520Yu%2520and%2520Brian%2520L.%2520White%2520and%2520Anahita%2520Bhiwandiwalla%2520and%2520Musashi%2520Hinck%2520and%2520Matthew%2520Lyle%2520Olson%2520and%2520Tung%2520Nguyen%2520and%2520Vasudev%2520Lal%26entry.1292438233%3D%2520%2520Detecting%2520and%2520attributing%2520temperature%2520increases%2520due%2520to%2520climate%2520change%2520is%250Acrucial%2520for%2520understanding%2520global%2520warming%2520and%2520guiding%2520adaptation%2520strategies.%2520The%250Acomplexity%2520of%2520distinguishing%2520human-induced%2520climate%2520signals%2520from%2520natural%250Avariability%2520has%2520challenged%2520traditional%2520detection%2520and%2520attribution%2520%2528D%2526A%2529%250Aapproaches%252C%2520which%2520seek%2520to%2520identify%2520specific%2520%2522fingerprints%2522%2520in%2520climate%2520response%250Avariables.%2520Deep%2520learning%2520offers%2520potential%2520for%2520discerning%2520these%2520complex%2520patterns%250Ain%2520expansive%2520spatial%2520datasets.%2520However%252C%2520lack%2520of%2520standard%2520protocols%2520has%2520hindered%250Aconsistent%2520comparisons%2520across%2520studies.%2520We%2520introduce%2520ClimDetect%252C%2520a%2520standardized%250Adataset%2520of%2520over%2520816k%2520daily%2520climate%2520snapshots%252C%2520designed%2520to%2520enhance%2520model%250Aaccuracy%2520in%2520identifying%2520climate%2520change%2520signals.%2520ClimDetect%2520integrates%2520various%250Ainput%2520and%2520target%2520variables%2520used%2520in%2520past%2520research%252C%2520ensuring%2520comparability%2520and%250Aconsistency.%2520We%2520also%2520explore%2520the%2520application%2520of%2520vision%2520transformers%2520%2528ViT%2529%2520to%250Aclimate%2520data%252C%2520a%2520novel%2520and%2520modernizing%2520approach%2520in%2520this%2520context.%2520Our%2520open-access%250Adata%2520and%2520code%2520serve%2520as%2520a%2520benchmark%2520for%2520advancing%2520climate%2520science%2520through%250Aimproved%2520model%2520evaluations.%2520ClimDetect%2520is%2520publicly%2520accessible%2520via%2520Huggingface%250Adataet%2520respository%2520at%253A%2520https%253A//huggingface.co/datasets/ClimDetect/ClimDetect.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15993v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ClimDetect%3A%20A%20Benchmark%20Dataset%20for%20Climate%20Change%20Detection%20and%0A%20%20Attribution&entry.906535625=Sungduk%20Yu%20and%20Brian%20L.%20White%20and%20Anahita%20Bhiwandiwalla%20and%20Musashi%20Hinck%20and%20Matthew%20Lyle%20Olson%20and%20Tung%20Nguyen%20and%20Vasudev%20Lal&entry.1292438233=%20%20Detecting%20and%20attributing%20temperature%20increases%20due%20to%20climate%20change%20is%0Acrucial%20for%20understanding%20global%20warming%20and%20guiding%20adaptation%20strategies.%20The%0Acomplexity%20of%20distinguishing%20human-induced%20climate%20signals%20from%20natural%0Avariability%20has%20challenged%20traditional%20detection%20and%20attribution%20%28D%26A%29%0Aapproaches%2C%20which%20seek%20to%20identify%20specific%20%22fingerprints%22%20in%20climate%20response%0Avariables.%20Deep%20learning%20offers%20potential%20for%20discerning%20these%20complex%20patterns%0Ain%20expansive%20spatial%20datasets.%20However%2C%20lack%20of%20standard%20protocols%20has%20hindered%0Aconsistent%20comparisons%20across%20studies.%20We%20introduce%20ClimDetect%2C%20a%20standardized%0Adataset%20of%20over%20816k%20daily%20climate%20snapshots%2C%20designed%20to%20enhance%20model%0Aaccuracy%20in%20identifying%20climate%20change%20signals.%20ClimDetect%20integrates%20various%0Ainput%20and%20target%20variables%20used%20in%20past%20research%2C%20ensuring%20comparability%20and%0Aconsistency.%20We%20also%20explore%20the%20application%20of%20vision%20transformers%20%28ViT%29%20to%0Aclimate%20data%2C%20a%20novel%20and%20modernizing%20approach%20in%20this%20context.%20Our%20open-access%0Adata%20and%20code%20serve%20as%20a%20benchmark%20for%20advancing%20climate%20science%20through%0Aimproved%20model%20evaluations.%20ClimDetect%20is%20publicly%20accessible%20via%20Huggingface%0Adataet%20respository%20at%3A%20https%3A//huggingface.co/datasets/ClimDetect/ClimDetect.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15993v1&entry.124074799=Read"},
{"title": "Stability of Primal-Dual Gradient Flow Dynamics for Multi-Block Convex\n  Optimization Problems", "author": "Ibrahim K. Ozaslan and Panagiotis Patrinos and Mihailo R. Jovanovi\u0107", "abstract": "  We examine stability properties of primal-dual gradient flow dynamics for\ncomposite convex optimization problems with multiple, possibly nonsmooth, terms\nin the objective function under the generalized consensus constraint. The\nproposed dynamics are based on the proximal augmented Lagrangian and they\nprovide a viable alternative to ADMM which faces significant challenges from\nboth analysis and implementation viewpoints in large-scale multi-block\nscenarios. In contrast to customized algorithms with individualized convergence\nguarantees, we provide a systematic approach for solving a broad class of\nchallenging composite optimization problems. We leverage various structural\nproperties to establish global (exponential) convergence guarantees for the\nproposed dynamics. Our assumptions are much weaker than those required to prove\n(exponential) stability of various primal-dual dynamics as well as (linear)\nconvergence of discrete-time methods, e.g., standard two-block and multi-block\nADMM and EXTRA algorithms. Finally, we show necessity of some of our structural\nassumptions for exponential stability and provide computational experiments to\ndemonstrate the convenience of the proposed dynamics for parallel and\ndistributed computing applications.\n", "link": "http://arxiv.org/abs/2408.15969v1", "date": "2024-08-28", "relevancy": 1.8616, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4927}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4474}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stability%20of%20Primal-Dual%20Gradient%20Flow%20Dynamics%20for%20Multi-Block%20Convex%0A%20%20Optimization%20Problems&body=Title%3A%20Stability%20of%20Primal-Dual%20Gradient%20Flow%20Dynamics%20for%20Multi-Block%20Convex%0A%20%20Optimization%20Problems%0AAuthor%3A%20Ibrahim%20K.%20Ozaslan%20and%20Panagiotis%20Patrinos%20and%20Mihailo%20R.%20Jovanovi%C4%87%0AAbstract%3A%20%20%20We%20examine%20stability%20properties%20of%20primal-dual%20gradient%20flow%20dynamics%20for%0Acomposite%20convex%20optimization%20problems%20with%20multiple%2C%20possibly%20nonsmooth%2C%20terms%0Ain%20the%20objective%20function%20under%20the%20generalized%20consensus%20constraint.%20The%0Aproposed%20dynamics%20are%20based%20on%20the%20proximal%20augmented%20Lagrangian%20and%20they%0Aprovide%20a%20viable%20alternative%20to%20ADMM%20which%20faces%20significant%20challenges%20from%0Aboth%20analysis%20and%20implementation%20viewpoints%20in%20large-scale%20multi-block%0Ascenarios.%20In%20contrast%20to%20customized%20algorithms%20with%20individualized%20convergence%0Aguarantees%2C%20we%20provide%20a%20systematic%20approach%20for%20solving%20a%20broad%20class%20of%0Achallenging%20composite%20optimization%20problems.%20We%20leverage%20various%20structural%0Aproperties%20to%20establish%20global%20%28exponential%29%20convergence%20guarantees%20for%20the%0Aproposed%20dynamics.%20Our%20assumptions%20are%20much%20weaker%20than%20those%20required%20to%20prove%0A%28exponential%29%20stability%20of%20various%20primal-dual%20dynamics%20as%20well%20as%20%28linear%29%0Aconvergence%20of%20discrete-time%20methods%2C%20e.g.%2C%20standard%20two-block%20and%20multi-block%0AADMM%20and%20EXTRA%20algorithms.%20Finally%2C%20we%20show%20necessity%20of%20some%20of%20our%20structural%0Aassumptions%20for%20exponential%20stability%20and%20provide%20computational%20experiments%20to%0Ademonstrate%20the%20convenience%20of%20the%20proposed%20dynamics%20for%20parallel%20and%0Adistributed%20computing%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15969v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStability%2520of%2520Primal-Dual%2520Gradient%2520Flow%2520Dynamics%2520for%2520Multi-Block%2520Convex%250A%2520%2520Optimization%2520Problems%26entry.906535625%3DIbrahim%2520K.%2520Ozaslan%2520and%2520Panagiotis%2520Patrinos%2520and%2520Mihailo%2520R.%2520Jovanovi%25C4%2587%26entry.1292438233%3D%2520%2520We%2520examine%2520stability%2520properties%2520of%2520primal-dual%2520gradient%2520flow%2520dynamics%2520for%250Acomposite%2520convex%2520optimization%2520problems%2520with%2520multiple%252C%2520possibly%2520nonsmooth%252C%2520terms%250Ain%2520the%2520objective%2520function%2520under%2520the%2520generalized%2520consensus%2520constraint.%2520The%250Aproposed%2520dynamics%2520are%2520based%2520on%2520the%2520proximal%2520augmented%2520Lagrangian%2520and%2520they%250Aprovide%2520a%2520viable%2520alternative%2520to%2520ADMM%2520which%2520faces%2520significant%2520challenges%2520from%250Aboth%2520analysis%2520and%2520implementation%2520viewpoints%2520in%2520large-scale%2520multi-block%250Ascenarios.%2520In%2520contrast%2520to%2520customized%2520algorithms%2520with%2520individualized%2520convergence%250Aguarantees%252C%2520we%2520provide%2520a%2520systematic%2520approach%2520for%2520solving%2520a%2520broad%2520class%2520of%250Achallenging%2520composite%2520optimization%2520problems.%2520We%2520leverage%2520various%2520structural%250Aproperties%2520to%2520establish%2520global%2520%2528exponential%2529%2520convergence%2520guarantees%2520for%2520the%250Aproposed%2520dynamics.%2520Our%2520assumptions%2520are%2520much%2520weaker%2520than%2520those%2520required%2520to%2520prove%250A%2528exponential%2529%2520stability%2520of%2520various%2520primal-dual%2520dynamics%2520as%2520well%2520as%2520%2528linear%2529%250Aconvergence%2520of%2520discrete-time%2520methods%252C%2520e.g.%252C%2520standard%2520two-block%2520and%2520multi-block%250AADMM%2520and%2520EXTRA%2520algorithms.%2520Finally%252C%2520we%2520show%2520necessity%2520of%2520some%2520of%2520our%2520structural%250Aassumptions%2520for%2520exponential%2520stability%2520and%2520provide%2520computational%2520experiments%2520to%250Ademonstrate%2520the%2520convenience%2520of%2520the%2520proposed%2520dynamics%2520for%2520parallel%2520and%250Adistributed%2520computing%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15969v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stability%20of%20Primal-Dual%20Gradient%20Flow%20Dynamics%20for%20Multi-Block%20Convex%0A%20%20Optimization%20Problems&entry.906535625=Ibrahim%20K.%20Ozaslan%20and%20Panagiotis%20Patrinos%20and%20Mihailo%20R.%20Jovanovi%C4%87&entry.1292438233=%20%20We%20examine%20stability%20properties%20of%20primal-dual%20gradient%20flow%20dynamics%20for%0Acomposite%20convex%20optimization%20problems%20with%20multiple%2C%20possibly%20nonsmooth%2C%20terms%0Ain%20the%20objective%20function%20under%20the%20generalized%20consensus%20constraint.%20The%0Aproposed%20dynamics%20are%20based%20on%20the%20proximal%20augmented%20Lagrangian%20and%20they%0Aprovide%20a%20viable%20alternative%20to%20ADMM%20which%20faces%20significant%20challenges%20from%0Aboth%20analysis%20and%20implementation%20viewpoints%20in%20large-scale%20multi-block%0Ascenarios.%20In%20contrast%20to%20customized%20algorithms%20with%20individualized%20convergence%0Aguarantees%2C%20we%20provide%20a%20systematic%20approach%20for%20solving%20a%20broad%20class%20of%0Achallenging%20composite%20optimization%20problems.%20We%20leverage%20various%20structural%0Aproperties%20to%20establish%20global%20%28exponential%29%20convergence%20guarantees%20for%20the%0Aproposed%20dynamics.%20Our%20assumptions%20are%20much%20weaker%20than%20those%20required%20to%20prove%0A%28exponential%29%20stability%20of%20various%20primal-dual%20dynamics%20as%20well%20as%20%28linear%29%0Aconvergence%20of%20discrete-time%20methods%2C%20e.g.%2C%20standard%20two-block%20and%20multi-block%0AADMM%20and%20EXTRA%20algorithms.%20Finally%2C%20we%20show%20necessity%20of%20some%20of%20our%20structural%0Aassumptions%20for%20exponential%20stability%20and%20provide%20computational%20experiments%20to%0Ademonstrate%20the%20convenience%20of%20the%20proposed%20dynamics%20for%20parallel%20and%0Adistributed%20computing%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15969v1&entry.124074799=Read"},
{"title": "Conceptual Design on the Field of View of Celestial Navigation Systems\n  for Maritime Autonomous Surface Ships", "author": "Kouki Wakita and Fuyuki Hane and Takeshi Sekiguchi and Shigehito Shimizu and Shinji Mitani and Youhei Akimoto and Atsuo Maki", "abstract": "  In order to understand the appropriate field of view (FOV) size of celestial\nautomatic navigation systems for surface ships, we investigate the variations\nof measurement accuracy of star position and probability of successful star\nidentification with respect to FOV, focusing on the decreasing number of\nobservable star magnitudes and the presence of physically covered stars in\nmarine environments. The results revealed that, although a larger FOV reduces\nthe measurement accuracy of star positions, it increases the number of\nobservable objects and thus improves the probability of star identification\nusing subgraph isomorphism-based methods. It was also found that, although at\nleast four objects need to be observed for accurate identification, four\nobjects may not be sufficient for wider FOVs. On the other hand, from the point\nof view of celestial navigation systems, a decrease in the measurement accuracy\nleads to a decrease in positioning accuracy. Therefore, it was found that\nmaximizing the FOV is required for celestial automatic navigation systems as\nlong as the desired positioning accuracy can be ensured. Furthermore, it was\nfound that algorithms incorporating more than four observed celestial objects\nare required to achieve highly accurate star identification over a wider FOV.\n", "link": "http://arxiv.org/abs/2408.15765v1", "date": "2024-08-28", "relevancy": 1.8555, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4731}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4669}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conceptual%20Design%20on%20the%20Field%20of%20View%20of%20Celestial%20Navigation%20Systems%0A%20%20for%20Maritime%20Autonomous%20Surface%20Ships&body=Title%3A%20Conceptual%20Design%20on%20the%20Field%20of%20View%20of%20Celestial%20Navigation%20Systems%0A%20%20for%20Maritime%20Autonomous%20Surface%20Ships%0AAuthor%3A%20Kouki%20Wakita%20and%20Fuyuki%20Hane%20and%20Takeshi%20Sekiguchi%20and%20Shigehito%20Shimizu%20and%20Shinji%20Mitani%20and%20Youhei%20Akimoto%20and%20Atsuo%20Maki%0AAbstract%3A%20%20%20In%20order%20to%20understand%20the%20appropriate%20field%20of%20view%20%28FOV%29%20size%20of%20celestial%0Aautomatic%20navigation%20systems%20for%20surface%20ships%2C%20we%20investigate%20the%20variations%0Aof%20measurement%20accuracy%20of%20star%20position%20and%20probability%20of%20successful%20star%0Aidentification%20with%20respect%20to%20FOV%2C%20focusing%20on%20the%20decreasing%20number%20of%0Aobservable%20star%20magnitudes%20and%20the%20presence%20of%20physically%20covered%20stars%20in%0Amarine%20environments.%20The%20results%20revealed%20that%2C%20although%20a%20larger%20FOV%20reduces%0Athe%20measurement%20accuracy%20of%20star%20positions%2C%20it%20increases%20the%20number%20of%0Aobservable%20objects%20and%20thus%20improves%20the%20probability%20of%20star%20identification%0Ausing%20subgraph%20isomorphism-based%20methods.%20It%20was%20also%20found%20that%2C%20although%20at%0Aleast%20four%20objects%20need%20to%20be%20observed%20for%20accurate%20identification%2C%20four%0Aobjects%20may%20not%20be%20sufficient%20for%20wider%20FOVs.%20On%20the%20other%20hand%2C%20from%20the%20point%0Aof%20view%20of%20celestial%20navigation%20systems%2C%20a%20decrease%20in%20the%20measurement%20accuracy%0Aleads%20to%20a%20decrease%20in%20positioning%20accuracy.%20Therefore%2C%20it%20was%20found%20that%0Amaximizing%20the%20FOV%20is%20required%20for%20celestial%20automatic%20navigation%20systems%20as%0Along%20as%20the%20desired%20positioning%20accuracy%20can%20be%20ensured.%20Furthermore%2C%20it%20was%0Afound%20that%20algorithms%20incorporating%20more%20than%20four%20observed%20celestial%20objects%0Aare%20required%20to%20achieve%20highly%20accurate%20star%20identification%20over%20a%20wider%20FOV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConceptual%2520Design%2520on%2520the%2520Field%2520of%2520View%2520of%2520Celestial%2520Navigation%2520Systems%250A%2520%2520for%2520Maritime%2520Autonomous%2520Surface%2520Ships%26entry.906535625%3DKouki%2520Wakita%2520and%2520Fuyuki%2520Hane%2520and%2520Takeshi%2520Sekiguchi%2520and%2520Shigehito%2520Shimizu%2520and%2520Shinji%2520Mitani%2520and%2520Youhei%2520Akimoto%2520and%2520Atsuo%2520Maki%26entry.1292438233%3D%2520%2520In%2520order%2520to%2520understand%2520the%2520appropriate%2520field%2520of%2520view%2520%2528FOV%2529%2520size%2520of%2520celestial%250Aautomatic%2520navigation%2520systems%2520for%2520surface%2520ships%252C%2520we%2520investigate%2520the%2520variations%250Aof%2520measurement%2520accuracy%2520of%2520star%2520position%2520and%2520probability%2520of%2520successful%2520star%250Aidentification%2520with%2520respect%2520to%2520FOV%252C%2520focusing%2520on%2520the%2520decreasing%2520number%2520of%250Aobservable%2520star%2520magnitudes%2520and%2520the%2520presence%2520of%2520physically%2520covered%2520stars%2520in%250Amarine%2520environments.%2520The%2520results%2520revealed%2520that%252C%2520although%2520a%2520larger%2520FOV%2520reduces%250Athe%2520measurement%2520accuracy%2520of%2520star%2520positions%252C%2520it%2520increases%2520the%2520number%2520of%250Aobservable%2520objects%2520and%2520thus%2520improves%2520the%2520probability%2520of%2520star%2520identification%250Ausing%2520subgraph%2520isomorphism-based%2520methods.%2520It%2520was%2520also%2520found%2520that%252C%2520although%2520at%250Aleast%2520four%2520objects%2520need%2520to%2520be%2520observed%2520for%2520accurate%2520identification%252C%2520four%250Aobjects%2520may%2520not%2520be%2520sufficient%2520for%2520wider%2520FOVs.%2520On%2520the%2520other%2520hand%252C%2520from%2520the%2520point%250Aof%2520view%2520of%2520celestial%2520navigation%2520systems%252C%2520a%2520decrease%2520in%2520the%2520measurement%2520accuracy%250Aleads%2520to%2520a%2520decrease%2520in%2520positioning%2520accuracy.%2520Therefore%252C%2520it%2520was%2520found%2520that%250Amaximizing%2520the%2520FOV%2520is%2520required%2520for%2520celestial%2520automatic%2520navigation%2520systems%2520as%250Along%2520as%2520the%2520desired%2520positioning%2520accuracy%2520can%2520be%2520ensured.%2520Furthermore%252C%2520it%2520was%250Afound%2520that%2520algorithms%2520incorporating%2520more%2520than%2520four%2520observed%2520celestial%2520objects%250Aare%2520required%2520to%2520achieve%2520highly%2520accurate%2520star%2520identification%2520over%2520a%2520wider%2520FOV.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conceptual%20Design%20on%20the%20Field%20of%20View%20of%20Celestial%20Navigation%20Systems%0A%20%20for%20Maritime%20Autonomous%20Surface%20Ships&entry.906535625=Kouki%20Wakita%20and%20Fuyuki%20Hane%20and%20Takeshi%20Sekiguchi%20and%20Shigehito%20Shimizu%20and%20Shinji%20Mitani%20and%20Youhei%20Akimoto%20and%20Atsuo%20Maki&entry.1292438233=%20%20In%20order%20to%20understand%20the%20appropriate%20field%20of%20view%20%28FOV%29%20size%20of%20celestial%0Aautomatic%20navigation%20systems%20for%20surface%20ships%2C%20we%20investigate%20the%20variations%0Aof%20measurement%20accuracy%20of%20star%20position%20and%20probability%20of%20successful%20star%0Aidentification%20with%20respect%20to%20FOV%2C%20focusing%20on%20the%20decreasing%20number%20of%0Aobservable%20star%20magnitudes%20and%20the%20presence%20of%20physically%20covered%20stars%20in%0Amarine%20environments.%20The%20results%20revealed%20that%2C%20although%20a%20larger%20FOV%20reduces%0Athe%20measurement%20accuracy%20of%20star%20positions%2C%20it%20increases%20the%20number%20of%0Aobservable%20objects%20and%20thus%20improves%20the%20probability%20of%20star%20identification%0Ausing%20subgraph%20isomorphism-based%20methods.%20It%20was%20also%20found%20that%2C%20although%20at%0Aleast%20four%20objects%20need%20to%20be%20observed%20for%20accurate%20identification%2C%20four%0Aobjects%20may%20not%20be%20sufficient%20for%20wider%20FOVs.%20On%20the%20other%20hand%2C%20from%20the%20point%0Aof%20view%20of%20celestial%20navigation%20systems%2C%20a%20decrease%20in%20the%20measurement%20accuracy%0Aleads%20to%20a%20decrease%20in%20positioning%20accuracy.%20Therefore%2C%20it%20was%20found%20that%0Amaximizing%20the%20FOV%20is%20required%20for%20celestial%20automatic%20navigation%20systems%20as%0Along%20as%20the%20desired%20positioning%20accuracy%20can%20be%20ensured.%20Furthermore%2C%20it%20was%0Afound%20that%20algorithms%20incorporating%20more%20than%20four%20observed%20celestial%20objects%0Aare%20required%20to%20achieve%20highly%20accurate%20star%20identification%20over%20a%20wider%20FOV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15765v1&entry.124074799=Read"},
{"title": "Shot Segmentation Based on Von Neumann Entropy for Key Frame Extraction", "author": "Xueqing Zhang. Di Fu and Naihao Liu", "abstract": "  Video key frame extraction is important in various fields, such as video\nsummary, retrieval, and compression. Therefore, we suggest a video key frame\nextraction algorithm based on shot segmentation using Von Neumann entropy. The\nsegmentation of shots is achieved through the computation of Von Neumann\nentropy of the similarity matrix among frames within the video sequence. The\ninitial frame of each shot is selected as key frames, which combines the\ntemporal sequence information of frames. The experimental results show the\nextracted key frames can fully and accurately represent the original video\ncontent while minimizing the number of repeated frames.\n", "link": "http://arxiv.org/abs/2408.15844v1", "date": "2024-08-28", "relevancy": 1.8541, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4767}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.467}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shot%20Segmentation%20Based%20on%20Von%20Neumann%20Entropy%20for%20Key%20Frame%20Extraction&body=Title%3A%20Shot%20Segmentation%20Based%20on%20Von%20Neumann%20Entropy%20for%20Key%20Frame%20Extraction%0AAuthor%3A%20Xueqing%20Zhang.%20Di%20Fu%20and%20Naihao%20Liu%0AAbstract%3A%20%20%20Video%20key%20frame%20extraction%20is%20important%20in%20various%20fields%2C%20such%20as%20video%0Asummary%2C%20retrieval%2C%20and%20compression.%20Therefore%2C%20we%20suggest%20a%20video%20key%20frame%0Aextraction%20algorithm%20based%20on%20shot%20segmentation%20using%20Von%20Neumann%20entropy.%20The%0Asegmentation%20of%20shots%20is%20achieved%20through%20the%20computation%20of%20Von%20Neumann%0Aentropy%20of%20the%20similarity%20matrix%20among%20frames%20within%20the%20video%20sequence.%20The%0Ainitial%20frame%20of%20each%20shot%20is%20selected%20as%20key%20frames%2C%20which%20combines%20the%0Atemporal%20sequence%20information%20of%20frames.%20The%20experimental%20results%20show%20the%0Aextracted%20key%20frames%20can%20fully%20and%20accurately%20represent%20the%20original%20video%0Acontent%20while%20minimizing%20the%20number%20of%20repeated%20frames.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShot%2520Segmentation%2520Based%2520on%2520Von%2520Neumann%2520Entropy%2520for%2520Key%2520Frame%2520Extraction%26entry.906535625%3DXueqing%2520Zhang.%2520Di%2520Fu%2520and%2520Naihao%2520Liu%26entry.1292438233%3D%2520%2520Video%2520key%2520frame%2520extraction%2520is%2520important%2520in%2520various%2520fields%252C%2520such%2520as%2520video%250Asummary%252C%2520retrieval%252C%2520and%2520compression.%2520Therefore%252C%2520we%2520suggest%2520a%2520video%2520key%2520frame%250Aextraction%2520algorithm%2520based%2520on%2520shot%2520segmentation%2520using%2520Von%2520Neumann%2520entropy.%2520The%250Asegmentation%2520of%2520shots%2520is%2520achieved%2520through%2520the%2520computation%2520of%2520Von%2520Neumann%250Aentropy%2520of%2520the%2520similarity%2520matrix%2520among%2520frames%2520within%2520the%2520video%2520sequence.%2520The%250Ainitial%2520frame%2520of%2520each%2520shot%2520is%2520selected%2520as%2520key%2520frames%252C%2520which%2520combines%2520the%250Atemporal%2520sequence%2520information%2520of%2520frames.%2520The%2520experimental%2520results%2520show%2520the%250Aextracted%2520key%2520frames%2520can%2520fully%2520and%2520accurately%2520represent%2520the%2520original%2520video%250Acontent%2520while%2520minimizing%2520the%2520number%2520of%2520repeated%2520frames.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shot%20Segmentation%20Based%20on%20Von%20Neumann%20Entropy%20for%20Key%20Frame%20Extraction&entry.906535625=Xueqing%20Zhang.%20Di%20Fu%20and%20Naihao%20Liu&entry.1292438233=%20%20Video%20key%20frame%20extraction%20is%20important%20in%20various%20fields%2C%20such%20as%20video%0Asummary%2C%20retrieval%2C%20and%20compression.%20Therefore%2C%20we%20suggest%20a%20video%20key%20frame%0Aextraction%20algorithm%20based%20on%20shot%20segmentation%20using%20Von%20Neumann%20entropy.%20The%0Asegmentation%20of%20shots%20is%20achieved%20through%20the%20computation%20of%20Von%20Neumann%0Aentropy%20of%20the%20similarity%20matrix%20among%20frames%20within%20the%20video%20sequence.%20The%0Ainitial%20frame%20of%20each%20shot%20is%20selected%20as%20key%20frames%2C%20which%20combines%20the%0Atemporal%20sequence%20information%20of%20frames.%20The%20experimental%20results%20show%20the%0Aextracted%20key%20frames%20can%20fully%20and%20accurately%20represent%20the%20original%20video%0Acontent%20while%20minimizing%20the%20number%20of%20repeated%20frames.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15844v1&entry.124074799=Read"},
{"title": "Evaluating Model Robustness Using Adaptive Sparse L0 Regularization", "author": "Weiyou Liu and Zhenyang Li and Weitong Chen", "abstract": "  Deep Neural Networks have demonstrated remarkable success in various domains\nbut remain susceptible to adversarial examples, which are slightly altered\ninputs designed to induce misclassification. While adversarial attacks\ntypically optimize under Lp norm constraints, attacks based on the L0 norm,\nprioritising input sparsity, are less studied due to their complex and non\nconvex nature. These sparse adversarial examples challenge existing defenses by\naltering a minimal subset of features, potentially uncovering more subtle DNN\nweaknesses. However, the current L0 norm attack methodologies face a trade off\nbetween accuracy and efficiency either precise but computationally intense or\nexpedient but imprecise. This paper proposes a novel, scalable, and effective\napproach to generate adversarial examples based on the L0 norm, aimed at\nrefining the robustness evaluation of DNNs against such perturbations.\n", "link": "http://arxiv.org/abs/2408.15702v1", "date": "2024-08-28", "relevancy": 1.8392, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4605}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4596}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Model%20Robustness%20Using%20Adaptive%20Sparse%20L0%20Regularization&body=Title%3A%20Evaluating%20Model%20Robustness%20Using%20Adaptive%20Sparse%20L0%20Regularization%0AAuthor%3A%20Weiyou%20Liu%20and%20Zhenyang%20Li%20and%20Weitong%20Chen%0AAbstract%3A%20%20%20Deep%20Neural%20Networks%20have%20demonstrated%20remarkable%20success%20in%20various%20domains%0Abut%20remain%20susceptible%20to%20adversarial%20examples%2C%20which%20are%20slightly%20altered%0Ainputs%20designed%20to%20induce%20misclassification.%20While%20adversarial%20attacks%0Atypically%20optimize%20under%20Lp%20norm%20constraints%2C%20attacks%20based%20on%20the%20L0%20norm%2C%0Aprioritising%20input%20sparsity%2C%20are%20less%20studied%20due%20to%20their%20complex%20and%20non%0Aconvex%20nature.%20These%20sparse%20adversarial%20examples%20challenge%20existing%20defenses%20by%0Aaltering%20a%20minimal%20subset%20of%20features%2C%20potentially%20uncovering%20more%20subtle%20DNN%0Aweaknesses.%20However%2C%20the%20current%20L0%20norm%20attack%20methodologies%20face%20a%20trade%20off%0Abetween%20accuracy%20and%20efficiency%20either%20precise%20but%20computationally%20intense%20or%0Aexpedient%20but%20imprecise.%20This%20paper%20proposes%20a%20novel%2C%20scalable%2C%20and%20effective%0Aapproach%20to%20generate%20adversarial%20examples%20based%20on%20the%20L0%20norm%2C%20aimed%20at%0Arefining%20the%20robustness%20evaluation%20of%20DNNs%20against%20such%20perturbations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Model%2520Robustness%2520Using%2520Adaptive%2520Sparse%2520L0%2520Regularization%26entry.906535625%3DWeiyou%2520Liu%2520and%2520Zhenyang%2520Li%2520and%2520Weitong%2520Chen%26entry.1292438233%3D%2520%2520Deep%2520Neural%2520Networks%2520have%2520demonstrated%2520remarkable%2520success%2520in%2520various%2520domains%250Abut%2520remain%2520susceptible%2520to%2520adversarial%2520examples%252C%2520which%2520are%2520slightly%2520altered%250Ainputs%2520designed%2520to%2520induce%2520misclassification.%2520While%2520adversarial%2520attacks%250Atypically%2520optimize%2520under%2520Lp%2520norm%2520constraints%252C%2520attacks%2520based%2520on%2520the%2520L0%2520norm%252C%250Aprioritising%2520input%2520sparsity%252C%2520are%2520less%2520studied%2520due%2520to%2520their%2520complex%2520and%2520non%250Aconvex%2520nature.%2520These%2520sparse%2520adversarial%2520examples%2520challenge%2520existing%2520defenses%2520by%250Aaltering%2520a%2520minimal%2520subset%2520of%2520features%252C%2520potentially%2520uncovering%2520more%2520subtle%2520DNN%250Aweaknesses.%2520However%252C%2520the%2520current%2520L0%2520norm%2520attack%2520methodologies%2520face%2520a%2520trade%2520off%250Abetween%2520accuracy%2520and%2520efficiency%2520either%2520precise%2520but%2520computationally%2520intense%2520or%250Aexpedient%2520but%2520imprecise.%2520This%2520paper%2520proposes%2520a%2520novel%252C%2520scalable%252C%2520and%2520effective%250Aapproach%2520to%2520generate%2520adversarial%2520examples%2520based%2520on%2520the%2520L0%2520norm%252C%2520aimed%2520at%250Arefining%2520the%2520robustness%2520evaluation%2520of%2520DNNs%2520against%2520such%2520perturbations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Model%20Robustness%20Using%20Adaptive%20Sparse%20L0%20Regularization&entry.906535625=Weiyou%20Liu%20and%20Zhenyang%20Li%20and%20Weitong%20Chen&entry.1292438233=%20%20Deep%20Neural%20Networks%20have%20demonstrated%20remarkable%20success%20in%20various%20domains%0Abut%20remain%20susceptible%20to%20adversarial%20examples%2C%20which%20are%20slightly%20altered%0Ainputs%20designed%20to%20induce%20misclassification.%20While%20adversarial%20attacks%0Atypically%20optimize%20under%20Lp%20norm%20constraints%2C%20attacks%20based%20on%20the%20L0%20norm%2C%0Aprioritising%20input%20sparsity%2C%20are%20less%20studied%20due%20to%20their%20complex%20and%20non%0Aconvex%20nature.%20These%20sparse%20adversarial%20examples%20challenge%20existing%20defenses%20by%0Aaltering%20a%20minimal%20subset%20of%20features%2C%20potentially%20uncovering%20more%20subtle%20DNN%0Aweaknesses.%20However%2C%20the%20current%20L0%20norm%20attack%20methodologies%20face%20a%20trade%20off%0Abetween%20accuracy%20and%20efficiency%20either%20precise%20but%20computationally%20intense%20or%0Aexpedient%20but%20imprecise.%20This%20paper%20proposes%20a%20novel%2C%20scalable%2C%20and%20effective%0Aapproach%20to%20generate%20adversarial%20examples%20based%20on%20the%20L0%20norm%2C%20aimed%20at%0Arefining%20the%20robustness%20evaluation%20of%20DNNs%20against%20such%20perturbations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15702v1&entry.124074799=Read"},
{"title": "Efficient LLM Scheduling by Learning to Rank", "author": "Yichao Fu and Siqi Zhu and Runlong Su and Aurick Qiao and Ion Stoica and Hao Zhang", "abstract": "  In Large Language Model (LLM) inference, the output length of an LLM request\nis typically regarded as not known a priori. Consequently, most LLM serving\nsystems employ a simple First-come-first-serve (FCFS) scheduling strategy,\nleading to Head-Of-Line (HOL) blocking and reduced throughput and service\nquality. In this paper, we reexamine this assumption -- we show that, although\npredicting the exact generation length of each request is infeasible, it is\npossible to predict the relative ranks of output lengths in a batch of\nrequests, using learning to rank. The ranking information offers valuable\nguidance for scheduling requests. Building on this insight, we develop a novel\nscheduler for LLM inference and serving that can approximate the\nshortest-job-first (SJF) schedule better than existing approaches. We integrate\nthis scheduler with the state-of-the-art LLM serving system and show\nsignificant performance improvement in several important applications: 2.8x\nlower latency in chatbot serving and 6.5x higher throughput in synthetic data\ngeneration. Our code is available at https://github.com/hao-ai-lab/vllm-ltr.git\n", "link": "http://arxiv.org/abs/2408.15792v1", "date": "2024-08-28", "relevancy": 1.8334, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.462}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4561}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20LLM%20Scheduling%20by%20Learning%20to%20Rank&body=Title%3A%20Efficient%20LLM%20Scheduling%20by%20Learning%20to%20Rank%0AAuthor%3A%20Yichao%20Fu%20and%20Siqi%20Zhu%20and%20Runlong%20Su%20and%20Aurick%20Qiao%20and%20Ion%20Stoica%20and%20Hao%20Zhang%0AAbstract%3A%20%20%20In%20Large%20Language%20Model%20%28LLM%29%20inference%2C%20the%20output%20length%20of%20an%20LLM%20request%0Ais%20typically%20regarded%20as%20not%20known%20a%20priori.%20Consequently%2C%20most%20LLM%20serving%0Asystems%20employ%20a%20simple%20First-come-first-serve%20%28FCFS%29%20scheduling%20strategy%2C%0Aleading%20to%20Head-Of-Line%20%28HOL%29%20blocking%20and%20reduced%20throughput%20and%20service%0Aquality.%20In%20this%20paper%2C%20we%20reexamine%20this%20assumption%20--%20we%20show%20that%2C%20although%0Apredicting%20the%20exact%20generation%20length%20of%20each%20request%20is%20infeasible%2C%20it%20is%0Apossible%20to%20predict%20the%20relative%20ranks%20of%20output%20lengths%20in%20a%20batch%20of%0Arequests%2C%20using%20learning%20to%20rank.%20The%20ranking%20information%20offers%20valuable%0Aguidance%20for%20scheduling%20requests.%20Building%20on%20this%20insight%2C%20we%20develop%20a%20novel%0Ascheduler%20for%20LLM%20inference%20and%20serving%20that%20can%20approximate%20the%0Ashortest-job-first%20%28SJF%29%20schedule%20better%20than%20existing%20approaches.%20We%20integrate%0Athis%20scheduler%20with%20the%20state-of-the-art%20LLM%20serving%20system%20and%20show%0Asignificant%20performance%20improvement%20in%20several%20important%20applications%3A%202.8x%0Alower%20latency%20in%20chatbot%20serving%20and%206.5x%20higher%20throughput%20in%20synthetic%20data%0Ageneration.%20Our%20code%20is%20available%20at%20https%3A//github.com/hao-ai-lab/vllm-ltr.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520LLM%2520Scheduling%2520by%2520Learning%2520to%2520Rank%26entry.906535625%3DYichao%2520Fu%2520and%2520Siqi%2520Zhu%2520and%2520Runlong%2520Su%2520and%2520Aurick%2520Qiao%2520and%2520Ion%2520Stoica%2520and%2520Hao%2520Zhang%26entry.1292438233%3D%2520%2520In%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520inference%252C%2520the%2520output%2520length%2520of%2520an%2520LLM%2520request%250Ais%2520typically%2520regarded%2520as%2520not%2520known%2520a%2520priori.%2520Consequently%252C%2520most%2520LLM%2520serving%250Asystems%2520employ%2520a%2520simple%2520First-come-first-serve%2520%2528FCFS%2529%2520scheduling%2520strategy%252C%250Aleading%2520to%2520Head-Of-Line%2520%2528HOL%2529%2520blocking%2520and%2520reduced%2520throughput%2520and%2520service%250Aquality.%2520In%2520this%2520paper%252C%2520we%2520reexamine%2520this%2520assumption%2520--%2520we%2520show%2520that%252C%2520although%250Apredicting%2520the%2520exact%2520generation%2520length%2520of%2520each%2520request%2520is%2520infeasible%252C%2520it%2520is%250Apossible%2520to%2520predict%2520the%2520relative%2520ranks%2520of%2520output%2520lengths%2520in%2520a%2520batch%2520of%250Arequests%252C%2520using%2520learning%2520to%2520rank.%2520The%2520ranking%2520information%2520offers%2520valuable%250Aguidance%2520for%2520scheduling%2520requests.%2520Building%2520on%2520this%2520insight%252C%2520we%2520develop%2520a%2520novel%250Ascheduler%2520for%2520LLM%2520inference%2520and%2520serving%2520that%2520can%2520approximate%2520the%250Ashortest-job-first%2520%2528SJF%2529%2520schedule%2520better%2520than%2520existing%2520approaches.%2520We%2520integrate%250Athis%2520scheduler%2520with%2520the%2520state-of-the-art%2520LLM%2520serving%2520system%2520and%2520show%250Asignificant%2520performance%2520improvement%2520in%2520several%2520important%2520applications%253A%25202.8x%250Alower%2520latency%2520in%2520chatbot%2520serving%2520and%25206.5x%2520higher%2520throughput%2520in%2520synthetic%2520data%250Ageneration.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/hao-ai-lab/vllm-ltr.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20LLM%20Scheduling%20by%20Learning%20to%20Rank&entry.906535625=Yichao%20Fu%20and%20Siqi%20Zhu%20and%20Runlong%20Su%20and%20Aurick%20Qiao%20and%20Ion%20Stoica%20and%20Hao%20Zhang&entry.1292438233=%20%20In%20Large%20Language%20Model%20%28LLM%29%20inference%2C%20the%20output%20length%20of%20an%20LLM%20request%0Ais%20typically%20regarded%20as%20not%20known%20a%20priori.%20Consequently%2C%20most%20LLM%20serving%0Asystems%20employ%20a%20simple%20First-come-first-serve%20%28FCFS%29%20scheduling%20strategy%2C%0Aleading%20to%20Head-Of-Line%20%28HOL%29%20blocking%20and%20reduced%20throughput%20and%20service%0Aquality.%20In%20this%20paper%2C%20we%20reexamine%20this%20assumption%20--%20we%20show%20that%2C%20although%0Apredicting%20the%20exact%20generation%20length%20of%20each%20request%20is%20infeasible%2C%20it%20is%0Apossible%20to%20predict%20the%20relative%20ranks%20of%20output%20lengths%20in%20a%20batch%20of%0Arequests%2C%20using%20learning%20to%20rank.%20The%20ranking%20information%20offers%20valuable%0Aguidance%20for%20scheduling%20requests.%20Building%20on%20this%20insight%2C%20we%20develop%20a%20novel%0Ascheduler%20for%20LLM%20inference%20and%20serving%20that%20can%20approximate%20the%0Ashortest-job-first%20%28SJF%29%20schedule%20better%20than%20existing%20approaches.%20We%20integrate%0Athis%20scheduler%20with%20the%20state-of-the-art%20LLM%20serving%20system%20and%20show%0Asignificant%20performance%20improvement%20in%20several%20important%20applications%3A%202.8x%0Alower%20latency%20in%20chatbot%20serving%20and%206.5x%20higher%20throughput%20in%20synthetic%20data%0Ageneration.%20Our%20code%20is%20available%20at%20https%3A//github.com/hao-ai-lab/vllm-ltr.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15792v1&entry.124074799=Read"},
{"title": "Implicit Regularization Paths of Weighted Neural Representations", "author": "Jin-Hong Du and Pratik Patil", "abstract": "  We study the implicit regularization effects induced by (observation)\nweighting of pretrained features. For weight and feature matrices of bounded\noperator norms that are infinitesimally free with respect to (normalized) trace\nfunctionals, we derive equivalence paths connecting different weighting\nmatrices and ridge regularization levels. Specifically, we show that ridge\nestimators trained on weighted features along the same path are asymptotically\nequivalent when evaluated against test vectors of bounded norms. These paths\ncan be interpreted as matching the effective degrees of freedom of ridge\nestimators fitted with weighted features. For the special case of subsampling\nwithout replacement, our results apply to independently sampled random features\nand kernel features and confirm recent conjectures (Conjectures 7 and 8) of the\nauthors on the existence of such paths in Patil et al. We also present an\nadditive risk decomposition for ensembles of weighted estimators and show that\nthe risks are equivalent along the paths when the ensemble size goes to\ninfinity. As a practical consequence of the path equivalences, we develop an\nefficient cross-validation method for tuning and apply it to subsampled\npretrained representations across several models (e.g., ResNet-50) and datasets\n(e.g., CIFAR-100).\n", "link": "http://arxiv.org/abs/2408.15784v1", "date": "2024-08-28", "relevancy": 1.8326, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4621}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.458}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20Regularization%20Paths%20of%20Weighted%20Neural%20Representations&body=Title%3A%20Implicit%20Regularization%20Paths%20of%20Weighted%20Neural%20Representations%0AAuthor%3A%20Jin-Hong%20Du%20and%20Pratik%20Patil%0AAbstract%3A%20%20%20We%20study%20the%20implicit%20regularization%20effects%20induced%20by%20%28observation%29%0Aweighting%20of%20pretrained%20features.%20For%20weight%20and%20feature%20matrices%20of%20bounded%0Aoperator%20norms%20that%20are%20infinitesimally%20free%20with%20respect%20to%20%28normalized%29%20trace%0Afunctionals%2C%20we%20derive%20equivalence%20paths%20connecting%20different%20weighting%0Amatrices%20and%20ridge%20regularization%20levels.%20Specifically%2C%20we%20show%20that%20ridge%0Aestimators%20trained%20on%20weighted%20features%20along%20the%20same%20path%20are%20asymptotically%0Aequivalent%20when%20evaluated%20against%20test%20vectors%20of%20bounded%20norms.%20These%20paths%0Acan%20be%20interpreted%20as%20matching%20the%20effective%20degrees%20of%20freedom%20of%20ridge%0Aestimators%20fitted%20with%20weighted%20features.%20For%20the%20special%20case%20of%20subsampling%0Awithout%20replacement%2C%20our%20results%20apply%20to%20independently%20sampled%20random%20features%0Aand%20kernel%20features%20and%20confirm%20recent%20conjectures%20%28Conjectures%207%20and%208%29%20of%20the%0Aauthors%20on%20the%20existence%20of%20such%20paths%20in%20Patil%20et%20al.%20We%20also%20present%20an%0Aadditive%20risk%20decomposition%20for%20ensembles%20of%20weighted%20estimators%20and%20show%20that%0Athe%20risks%20are%20equivalent%20along%20the%20paths%20when%20the%20ensemble%20size%20goes%20to%0Ainfinity.%20As%20a%20practical%20consequence%20of%20the%20path%20equivalences%2C%20we%20develop%20an%0Aefficient%20cross-validation%20method%20for%20tuning%20and%20apply%20it%20to%20subsampled%0Apretrained%20representations%20across%20several%20models%20%28e.g.%2C%20ResNet-50%29%20and%20datasets%0A%28e.g.%2C%20CIFAR-100%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15784v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520Regularization%2520Paths%2520of%2520Weighted%2520Neural%2520Representations%26entry.906535625%3DJin-Hong%2520Du%2520and%2520Pratik%2520Patil%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520implicit%2520regularization%2520effects%2520induced%2520by%2520%2528observation%2529%250Aweighting%2520of%2520pretrained%2520features.%2520For%2520weight%2520and%2520feature%2520matrices%2520of%2520bounded%250Aoperator%2520norms%2520that%2520are%2520infinitesimally%2520free%2520with%2520respect%2520to%2520%2528normalized%2529%2520trace%250Afunctionals%252C%2520we%2520derive%2520equivalence%2520paths%2520connecting%2520different%2520weighting%250Amatrices%2520and%2520ridge%2520regularization%2520levels.%2520Specifically%252C%2520we%2520show%2520that%2520ridge%250Aestimators%2520trained%2520on%2520weighted%2520features%2520along%2520the%2520same%2520path%2520are%2520asymptotically%250Aequivalent%2520when%2520evaluated%2520against%2520test%2520vectors%2520of%2520bounded%2520norms.%2520These%2520paths%250Acan%2520be%2520interpreted%2520as%2520matching%2520the%2520effective%2520degrees%2520of%2520freedom%2520of%2520ridge%250Aestimators%2520fitted%2520with%2520weighted%2520features.%2520For%2520the%2520special%2520case%2520of%2520subsampling%250Awithout%2520replacement%252C%2520our%2520results%2520apply%2520to%2520independently%2520sampled%2520random%2520features%250Aand%2520kernel%2520features%2520and%2520confirm%2520recent%2520conjectures%2520%2528Conjectures%25207%2520and%25208%2529%2520of%2520the%250Aauthors%2520on%2520the%2520existence%2520of%2520such%2520paths%2520in%2520Patil%2520et%2520al.%2520We%2520also%2520present%2520an%250Aadditive%2520risk%2520decomposition%2520for%2520ensembles%2520of%2520weighted%2520estimators%2520and%2520show%2520that%250Athe%2520risks%2520are%2520equivalent%2520along%2520the%2520paths%2520when%2520the%2520ensemble%2520size%2520goes%2520to%250Ainfinity.%2520As%2520a%2520practical%2520consequence%2520of%2520the%2520path%2520equivalences%252C%2520we%2520develop%2520an%250Aefficient%2520cross-validation%2520method%2520for%2520tuning%2520and%2520apply%2520it%2520to%2520subsampled%250Apretrained%2520representations%2520across%2520several%2520models%2520%2528e.g.%252C%2520ResNet-50%2529%2520and%2520datasets%250A%2528e.g.%252C%2520CIFAR-100%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15784v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Regularization%20Paths%20of%20Weighted%20Neural%20Representations&entry.906535625=Jin-Hong%20Du%20and%20Pratik%20Patil&entry.1292438233=%20%20We%20study%20the%20implicit%20regularization%20effects%20induced%20by%20%28observation%29%0Aweighting%20of%20pretrained%20features.%20For%20weight%20and%20feature%20matrices%20of%20bounded%0Aoperator%20norms%20that%20are%20infinitesimally%20free%20with%20respect%20to%20%28normalized%29%20trace%0Afunctionals%2C%20we%20derive%20equivalence%20paths%20connecting%20different%20weighting%0Amatrices%20and%20ridge%20regularization%20levels.%20Specifically%2C%20we%20show%20that%20ridge%0Aestimators%20trained%20on%20weighted%20features%20along%20the%20same%20path%20are%20asymptotically%0Aequivalent%20when%20evaluated%20against%20test%20vectors%20of%20bounded%20norms.%20These%20paths%0Acan%20be%20interpreted%20as%20matching%20the%20effective%20degrees%20of%20freedom%20of%20ridge%0Aestimators%20fitted%20with%20weighted%20features.%20For%20the%20special%20case%20of%20subsampling%0Awithout%20replacement%2C%20our%20results%20apply%20to%20independently%20sampled%20random%20features%0Aand%20kernel%20features%20and%20confirm%20recent%20conjectures%20%28Conjectures%207%20and%208%29%20of%20the%0Aauthors%20on%20the%20existence%20of%20such%20paths%20in%20Patil%20et%20al.%20We%20also%20present%20an%0Aadditive%20risk%20decomposition%20for%20ensembles%20of%20weighted%20estimators%20and%20show%20that%0Athe%20risks%20are%20equivalent%20along%20the%20paths%20when%20the%20ensemble%20size%20goes%20to%0Ainfinity.%20As%20a%20practical%20consequence%20of%20the%20path%20equivalences%2C%20we%20develop%20an%0Aefficient%20cross-validation%20method%20for%20tuning%20and%20apply%20it%20to%20subsampled%0Apretrained%20representations%20across%20several%20models%20%28e.g.%2C%20ResNet-50%29%20and%20datasets%0A%28e.g.%2C%20CIFAR-100%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15784v1&entry.124074799=Read"},
{"title": "InstanSeg: an embedding-based instance segmentation algorithm optimized\n  for accurate, efficient and portable cell segmentation", "author": "Thibaut Goldsborough and Ben Philps and Alan O'Callaghan and Fiona Inglis and Leo Leplat and Andrew Filby and Hakan Bilen and Peter Bankhead", "abstract": "  Cell and nucleus segmentation are fundamental tasks for quantitative bioimage\nanalysis. Despite progress in recent years, biologists and other domain experts\nstill require novel algorithms to handle increasingly large and complex\nreal-world datasets. These algorithms must not only achieve state-of-the-art\naccuracy, but also be optimized for efficiency, portability and\nuser-friendliness. Here, we introduce InstanSeg: a novel embedding-based\ninstance segmentation pipeline designed to identify cells and nuclei in\nmicroscopy images. Using six public cell segmentation datasets, we demonstrate\nthat InstanSeg can significantly improve accuracy when compared to the most\nwidely used alternative methods, while reducing the processing time by at least\n60%. Furthermore, InstanSeg is designed to be fully serializable as TorchScript\nand supports GPU acceleration on a range of hardware. We provide an open-source\nimplementation of InstanSeg in Python, in addition to a user-friendly,\ninteractive QuPath extension for inference written in Java. Our code and\npre-trained models are available at https://github.com/instanseg/instanseg .\n", "link": "http://arxiv.org/abs/2408.15954v1", "date": "2024-08-28", "relevancy": 1.8171, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4838}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.457}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstanSeg%3A%20an%20embedding-based%20instance%20segmentation%20algorithm%20optimized%0A%20%20for%20accurate%2C%20efficient%20and%20portable%20cell%20segmentation&body=Title%3A%20InstanSeg%3A%20an%20embedding-based%20instance%20segmentation%20algorithm%20optimized%0A%20%20for%20accurate%2C%20efficient%20and%20portable%20cell%20segmentation%0AAuthor%3A%20Thibaut%20Goldsborough%20and%20Ben%20Philps%20and%20Alan%20O%27Callaghan%20and%20Fiona%20Inglis%20and%20Leo%20Leplat%20and%20Andrew%20Filby%20and%20Hakan%20Bilen%20and%20Peter%20Bankhead%0AAbstract%3A%20%20%20Cell%20and%20nucleus%20segmentation%20are%20fundamental%20tasks%20for%20quantitative%20bioimage%0Aanalysis.%20Despite%20progress%20in%20recent%20years%2C%20biologists%20and%20other%20domain%20experts%0Astill%20require%20novel%20algorithms%20to%20handle%20increasingly%20large%20and%20complex%0Areal-world%20datasets.%20These%20algorithms%20must%20not%20only%20achieve%20state-of-the-art%0Aaccuracy%2C%20but%20also%20be%20optimized%20for%20efficiency%2C%20portability%20and%0Auser-friendliness.%20Here%2C%20we%20introduce%20InstanSeg%3A%20a%20novel%20embedding-based%0Ainstance%20segmentation%20pipeline%20designed%20to%20identify%20cells%20and%20nuclei%20in%0Amicroscopy%20images.%20Using%20six%20public%20cell%20segmentation%20datasets%2C%20we%20demonstrate%0Athat%20InstanSeg%20can%20significantly%20improve%20accuracy%20when%20compared%20to%20the%20most%0Awidely%20used%20alternative%20methods%2C%20while%20reducing%20the%20processing%20time%20by%20at%20least%0A60%25.%20Furthermore%2C%20InstanSeg%20is%20designed%20to%20be%20fully%20serializable%20as%20TorchScript%0Aand%20supports%20GPU%20acceleration%20on%20a%20range%20of%20hardware.%20We%20provide%20an%20open-source%0Aimplementation%20of%20InstanSeg%20in%20Python%2C%20in%20addition%20to%20a%20user-friendly%2C%0Ainteractive%20QuPath%20extension%20for%20inference%20written%20in%20Java.%20Our%20code%20and%0Apre-trained%20models%20are%20available%20at%20https%3A//github.com/instanseg/instanseg%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15954v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstanSeg%253A%2520an%2520embedding-based%2520instance%2520segmentation%2520algorithm%2520optimized%250A%2520%2520for%2520accurate%252C%2520efficient%2520and%2520portable%2520cell%2520segmentation%26entry.906535625%3DThibaut%2520Goldsborough%2520and%2520Ben%2520Philps%2520and%2520Alan%2520O%2527Callaghan%2520and%2520Fiona%2520Inglis%2520and%2520Leo%2520Leplat%2520and%2520Andrew%2520Filby%2520and%2520Hakan%2520Bilen%2520and%2520Peter%2520Bankhead%26entry.1292438233%3D%2520%2520Cell%2520and%2520nucleus%2520segmentation%2520are%2520fundamental%2520tasks%2520for%2520quantitative%2520bioimage%250Aanalysis.%2520Despite%2520progress%2520in%2520recent%2520years%252C%2520biologists%2520and%2520other%2520domain%2520experts%250Astill%2520require%2520novel%2520algorithms%2520to%2520handle%2520increasingly%2520large%2520and%2520complex%250Areal-world%2520datasets.%2520These%2520algorithms%2520must%2520not%2520only%2520achieve%2520state-of-the-art%250Aaccuracy%252C%2520but%2520also%2520be%2520optimized%2520for%2520efficiency%252C%2520portability%2520and%250Auser-friendliness.%2520Here%252C%2520we%2520introduce%2520InstanSeg%253A%2520a%2520novel%2520embedding-based%250Ainstance%2520segmentation%2520pipeline%2520designed%2520to%2520identify%2520cells%2520and%2520nuclei%2520in%250Amicroscopy%2520images.%2520Using%2520six%2520public%2520cell%2520segmentation%2520datasets%252C%2520we%2520demonstrate%250Athat%2520InstanSeg%2520can%2520significantly%2520improve%2520accuracy%2520when%2520compared%2520to%2520the%2520most%250Awidely%2520used%2520alternative%2520methods%252C%2520while%2520reducing%2520the%2520processing%2520time%2520by%2520at%2520least%250A60%2525.%2520Furthermore%252C%2520InstanSeg%2520is%2520designed%2520to%2520be%2520fully%2520serializable%2520as%2520TorchScript%250Aand%2520supports%2520GPU%2520acceleration%2520on%2520a%2520range%2520of%2520hardware.%2520We%2520provide%2520an%2520open-source%250Aimplementation%2520of%2520InstanSeg%2520in%2520Python%252C%2520in%2520addition%2520to%2520a%2520user-friendly%252C%250Ainteractive%2520QuPath%2520extension%2520for%2520inference%2520written%2520in%2520Java.%2520Our%2520code%2520and%250Apre-trained%2520models%2520are%2520available%2520at%2520https%253A//github.com/instanseg/instanseg%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15954v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstanSeg%3A%20an%20embedding-based%20instance%20segmentation%20algorithm%20optimized%0A%20%20for%20accurate%2C%20efficient%20and%20portable%20cell%20segmentation&entry.906535625=Thibaut%20Goldsborough%20and%20Ben%20Philps%20and%20Alan%20O%27Callaghan%20and%20Fiona%20Inglis%20and%20Leo%20Leplat%20and%20Andrew%20Filby%20and%20Hakan%20Bilen%20and%20Peter%20Bankhead&entry.1292438233=%20%20Cell%20and%20nucleus%20segmentation%20are%20fundamental%20tasks%20for%20quantitative%20bioimage%0Aanalysis.%20Despite%20progress%20in%20recent%20years%2C%20biologists%20and%20other%20domain%20experts%0Astill%20require%20novel%20algorithms%20to%20handle%20increasingly%20large%20and%20complex%0Areal-world%20datasets.%20These%20algorithms%20must%20not%20only%20achieve%20state-of-the-art%0Aaccuracy%2C%20but%20also%20be%20optimized%20for%20efficiency%2C%20portability%20and%0Auser-friendliness.%20Here%2C%20we%20introduce%20InstanSeg%3A%20a%20novel%20embedding-based%0Ainstance%20segmentation%20pipeline%20designed%20to%20identify%20cells%20and%20nuclei%20in%0Amicroscopy%20images.%20Using%20six%20public%20cell%20segmentation%20datasets%2C%20we%20demonstrate%0Athat%20InstanSeg%20can%20significantly%20improve%20accuracy%20when%20compared%20to%20the%20most%0Awidely%20used%20alternative%20methods%2C%20while%20reducing%20the%20processing%20time%20by%20at%20least%0A60%25.%20Furthermore%2C%20InstanSeg%20is%20designed%20to%20be%20fully%20serializable%20as%20TorchScript%0Aand%20supports%20GPU%20acceleration%20on%20a%20range%20of%20hardware.%20We%20provide%20an%20open-source%0Aimplementation%20of%20InstanSeg%20in%20Python%2C%20in%20addition%20to%20a%20user-friendly%2C%0Ainteractive%20QuPath%20extension%20for%20inference%20written%20in%20Java.%20Our%20code%20and%0Apre-trained%20models%20are%20available%20at%20https%3A//github.com/instanseg/instanseg%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15954v1&entry.124074799=Read"},
{"title": "Easy, Interpretable, Effective: openSMILE for voice deepfake detection", "author": "Octavian Pascu and Dan Oneata and Horia Cucu and Nicolas M. M\u00fcller", "abstract": "  In this paper, we demonstrate that attacks in the latest ASVspoof5 dataset --\na de facto standard in the field of voice authenticity and deepfake detection\n-- can be identified with surprising accuracy using a small subset of very\nsimplistic features. These are derived from the openSMILE library, and are\nscalar-valued, easy to compute, and human interpretable. For example, attack\nA10`s unvoiced segments have a mean length of 0.09 \\pm 0.02, while bona fide\ninstances have a mean length of 0.18 \\pm 0.07. Using this feature alone, a\nthreshold classifier achieves an Equal Error Rate (EER) of 10.3% for attack\nA10. Similarly, across all attacks, we achieve up to 0.8% EER, with an overall\nEER of 15.7 \\pm 6.0%. We explore the generalization capabilities of these\nfeatures and find that some of them transfer effectively between attacks,\nprimarily when the attacks originate from similar Text-to-Speech (TTS)\narchitectures. This finding may indicate that voice anti-spoofing is, in part,\na problem of identifying and remembering signatures or fingerprints of\nindividual TTS systems. This allows to better understand anti-spoofing models\nand their challenges in real-world application.\n", "link": "http://arxiv.org/abs/2408.15775v1", "date": "2024-08-28", "relevancy": 1.8096, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4579}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4509}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Easy%2C%20Interpretable%2C%20Effective%3A%20openSMILE%20for%20voice%20deepfake%20detection&body=Title%3A%20Easy%2C%20Interpretable%2C%20Effective%3A%20openSMILE%20for%20voice%20deepfake%20detection%0AAuthor%3A%20Octavian%20Pascu%20and%20Dan%20Oneata%20and%20Horia%20Cucu%20and%20Nicolas%20M.%20M%C3%BCller%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20demonstrate%20that%20attacks%20in%20the%20latest%20ASVspoof5%20dataset%20--%0Aa%20de%20facto%20standard%20in%20the%20field%20of%20voice%20authenticity%20and%20deepfake%20detection%0A--%20can%20be%20identified%20with%20surprising%20accuracy%20using%20a%20small%20subset%20of%20very%0Asimplistic%20features.%20These%20are%20derived%20from%20the%20openSMILE%20library%2C%20and%20are%0Ascalar-valued%2C%20easy%20to%20compute%2C%20and%20human%20interpretable.%20For%20example%2C%20attack%0AA10%60s%20unvoiced%20segments%20have%20a%20mean%20length%20of%200.09%20%5Cpm%200.02%2C%20while%20bona%20fide%0Ainstances%20have%20a%20mean%20length%20of%200.18%20%5Cpm%200.07.%20Using%20this%20feature%20alone%2C%20a%0Athreshold%20classifier%20achieves%20an%20Equal%20Error%20Rate%20%28EER%29%20of%2010.3%25%20for%20attack%0AA10.%20Similarly%2C%20across%20all%20attacks%2C%20we%20achieve%20up%20to%200.8%25%20EER%2C%20with%20an%20overall%0AEER%20of%2015.7%20%5Cpm%206.0%25.%20We%20explore%20the%20generalization%20capabilities%20of%20these%0Afeatures%20and%20find%20that%20some%20of%20them%20transfer%20effectively%20between%20attacks%2C%0Aprimarily%20when%20the%20attacks%20originate%20from%20similar%20Text-to-Speech%20%28TTS%29%0Aarchitectures.%20This%20finding%20may%20indicate%20that%20voice%20anti-spoofing%20is%2C%20in%20part%2C%0Aa%20problem%20of%20identifying%20and%20remembering%20signatures%20or%20fingerprints%20of%0Aindividual%20TTS%20systems.%20This%20allows%20to%20better%20understand%20anti-spoofing%20models%0Aand%20their%20challenges%20in%20real-world%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEasy%252C%2520Interpretable%252C%2520Effective%253A%2520openSMILE%2520for%2520voice%2520deepfake%2520detection%26entry.906535625%3DOctavian%2520Pascu%2520and%2520Dan%2520Oneata%2520and%2520Horia%2520Cucu%2520and%2520Nicolas%2520M.%2520M%25C3%25BCller%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520demonstrate%2520that%2520attacks%2520in%2520the%2520latest%2520ASVspoof5%2520dataset%2520--%250Aa%2520de%2520facto%2520standard%2520in%2520the%2520field%2520of%2520voice%2520authenticity%2520and%2520deepfake%2520detection%250A--%2520can%2520be%2520identified%2520with%2520surprising%2520accuracy%2520using%2520a%2520small%2520subset%2520of%2520very%250Asimplistic%2520features.%2520These%2520are%2520derived%2520from%2520the%2520openSMILE%2520library%252C%2520and%2520are%250Ascalar-valued%252C%2520easy%2520to%2520compute%252C%2520and%2520human%2520interpretable.%2520For%2520example%252C%2520attack%250AA10%2560s%2520unvoiced%2520segments%2520have%2520a%2520mean%2520length%2520of%25200.09%2520%255Cpm%25200.02%252C%2520while%2520bona%2520fide%250Ainstances%2520have%2520a%2520mean%2520length%2520of%25200.18%2520%255Cpm%25200.07.%2520Using%2520this%2520feature%2520alone%252C%2520a%250Athreshold%2520classifier%2520achieves%2520an%2520Equal%2520Error%2520Rate%2520%2528EER%2529%2520of%252010.3%2525%2520for%2520attack%250AA10.%2520Similarly%252C%2520across%2520all%2520attacks%252C%2520we%2520achieve%2520up%2520to%25200.8%2525%2520EER%252C%2520with%2520an%2520overall%250AEER%2520of%252015.7%2520%255Cpm%25206.0%2525.%2520We%2520explore%2520the%2520generalization%2520capabilities%2520of%2520these%250Afeatures%2520and%2520find%2520that%2520some%2520of%2520them%2520transfer%2520effectively%2520between%2520attacks%252C%250Aprimarily%2520when%2520the%2520attacks%2520originate%2520from%2520similar%2520Text-to-Speech%2520%2528TTS%2529%250Aarchitectures.%2520This%2520finding%2520may%2520indicate%2520that%2520voice%2520anti-spoofing%2520is%252C%2520in%2520part%252C%250Aa%2520problem%2520of%2520identifying%2520and%2520remembering%2520signatures%2520or%2520fingerprints%2520of%250Aindividual%2520TTS%2520systems.%2520This%2520allows%2520to%2520better%2520understand%2520anti-spoofing%2520models%250Aand%2520their%2520challenges%2520in%2520real-world%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Easy%2C%20Interpretable%2C%20Effective%3A%20openSMILE%20for%20voice%20deepfake%20detection&entry.906535625=Octavian%20Pascu%20and%20Dan%20Oneata%20and%20Horia%20Cucu%20and%20Nicolas%20M.%20M%C3%BCller&entry.1292438233=%20%20In%20this%20paper%2C%20we%20demonstrate%20that%20attacks%20in%20the%20latest%20ASVspoof5%20dataset%20--%0Aa%20de%20facto%20standard%20in%20the%20field%20of%20voice%20authenticity%20and%20deepfake%20detection%0A--%20can%20be%20identified%20with%20surprising%20accuracy%20using%20a%20small%20subset%20of%20very%0Asimplistic%20features.%20These%20are%20derived%20from%20the%20openSMILE%20library%2C%20and%20are%0Ascalar-valued%2C%20easy%20to%20compute%2C%20and%20human%20interpretable.%20For%20example%2C%20attack%0AA10%60s%20unvoiced%20segments%20have%20a%20mean%20length%20of%200.09%20%5Cpm%200.02%2C%20while%20bona%20fide%0Ainstances%20have%20a%20mean%20length%20of%200.18%20%5Cpm%200.07.%20Using%20this%20feature%20alone%2C%20a%0Athreshold%20classifier%20achieves%20an%20Equal%20Error%20Rate%20%28EER%29%20of%2010.3%25%20for%20attack%0AA10.%20Similarly%2C%20across%20all%20attacks%2C%20we%20achieve%20up%20to%200.8%25%20EER%2C%20with%20an%20overall%0AEER%20of%2015.7%20%5Cpm%206.0%25.%20We%20explore%20the%20generalization%20capabilities%20of%20these%0Afeatures%20and%20find%20that%20some%20of%20them%20transfer%20effectively%20between%20attacks%2C%0Aprimarily%20when%20the%20attacks%20originate%20from%20similar%20Text-to-Speech%20%28TTS%29%0Aarchitectures.%20This%20finding%20may%20indicate%20that%20voice%20anti-spoofing%20is%2C%20in%20part%2C%0Aa%20problem%20of%20identifying%20and%20remembering%20signatures%20or%20fingerprints%20of%0Aindividual%20TTS%20systems.%20This%20allows%20to%20better%20understand%20anti-spoofing%20models%0Aand%20their%20challenges%20in%20real-world%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15775v1&entry.124074799=Read"},
{"title": "Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal\n  Time Series Imputation", "author": "Baoyu Jing and Dawei Zhou and Kan Ren and Carl Yang", "abstract": "  Spatiotemporal time series are usually collected via monitoring sensors\nplaced at different locations, which usually contain missing values due to\nvarious failures, such as mechanical damages and Internet outages. Imputing the\nmissing values is crucial for analyzing time series. When recovering a specific\ndata point, most existing methods consider all the information relevant to that\npoint regardless of the cause-and-effect relationship. During data collection,\nit is inevitable that some unknown confounders are included, e.g., background\nnoise in time series and non-causal shortcut edges in the constructed sensor\nnetwork. These confounders could open backdoor paths and establish non-causal\ncorrelations between the input and output. Over-exploiting these non-causal\ncorrelations could cause overfitting. In this paper, we first revisit\nspatiotemporal time series imputation from a causal perspective and show how to\nblock the confounders via the frontdoor adjustment. Based on the results of\nfrontdoor adjustment, we introduce a novel Causality-Aware Spatiotemporal Graph\nNeural Network (Casper), which contains a novel Prompt Based Decoder (PBD) and\na Spatiotemporal Causal Attention (SCA). PBD could reduce the impact of\nconfounders and SCA could discover the sparse causal relationships among\nembeddings. Theoretical analysis reveals that SCA discovers causal\nrelationships based on the values of gradients. We evaluate Casper on three\nreal-world datasets, and the experimental results show that Casper could\noutperform the baselines and could effectively discover causal relationships.\n", "link": "http://arxiv.org/abs/2403.11960v3", "date": "2024-08-28", "relevancy": 1.8025, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.484}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4507}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causality-Aware%20Spatiotemporal%20Graph%20Neural%20Networks%20for%20Spatiotemporal%0A%20%20Time%20Series%20Imputation&body=Title%3A%20Causality-Aware%20Spatiotemporal%20Graph%20Neural%20Networks%20for%20Spatiotemporal%0A%20%20Time%20Series%20Imputation%0AAuthor%3A%20Baoyu%20Jing%20and%20Dawei%20Zhou%20and%20Kan%20Ren%20and%20Carl%20Yang%0AAbstract%3A%20%20%20Spatiotemporal%20time%20series%20are%20usually%20collected%20via%20monitoring%20sensors%0Aplaced%20at%20different%20locations%2C%20which%20usually%20contain%20missing%20values%20due%20to%0Avarious%20failures%2C%20such%20as%20mechanical%20damages%20and%20Internet%20outages.%20Imputing%20the%0Amissing%20values%20is%20crucial%20for%20analyzing%20time%20series.%20When%20recovering%20a%20specific%0Adata%20point%2C%20most%20existing%20methods%20consider%20all%20the%20information%20relevant%20to%20that%0Apoint%20regardless%20of%20the%20cause-and-effect%20relationship.%20During%20data%20collection%2C%0Ait%20is%20inevitable%20that%20some%20unknown%20confounders%20are%20included%2C%20e.g.%2C%20background%0Anoise%20in%20time%20series%20and%20non-causal%20shortcut%20edges%20in%20the%20constructed%20sensor%0Anetwork.%20These%20confounders%20could%20open%20backdoor%20paths%20and%20establish%20non-causal%0Acorrelations%20between%20the%20input%20and%20output.%20Over-exploiting%20these%20non-causal%0Acorrelations%20could%20cause%20overfitting.%20In%20this%20paper%2C%20we%20first%20revisit%0Aspatiotemporal%20time%20series%20imputation%20from%20a%20causal%20perspective%20and%20show%20how%20to%0Ablock%20the%20confounders%20via%20the%20frontdoor%20adjustment.%20Based%20on%20the%20results%20of%0Afrontdoor%20adjustment%2C%20we%20introduce%20a%20novel%20Causality-Aware%20Spatiotemporal%20Graph%0ANeural%20Network%20%28Casper%29%2C%20which%20contains%20a%20novel%20Prompt%20Based%20Decoder%20%28PBD%29%20and%0Aa%20Spatiotemporal%20Causal%20Attention%20%28SCA%29.%20PBD%20could%20reduce%20the%20impact%20of%0Aconfounders%20and%20SCA%20could%20discover%20the%20sparse%20causal%20relationships%20among%0Aembeddings.%20Theoretical%20analysis%20reveals%20that%20SCA%20discovers%20causal%0Arelationships%20based%20on%20the%20values%20of%20gradients.%20We%20evaluate%20Casper%20on%20three%0Areal-world%20datasets%2C%20and%20the%20experimental%20results%20show%20that%20Casper%20could%0Aoutperform%20the%20baselines%20and%20could%20effectively%20discover%20causal%20relationships.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11960v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausality-Aware%2520Spatiotemporal%2520Graph%2520Neural%2520Networks%2520for%2520Spatiotemporal%250A%2520%2520Time%2520Series%2520Imputation%26entry.906535625%3DBaoyu%2520Jing%2520and%2520Dawei%2520Zhou%2520and%2520Kan%2520Ren%2520and%2520Carl%2520Yang%26entry.1292438233%3D%2520%2520Spatiotemporal%2520time%2520series%2520are%2520usually%2520collected%2520via%2520monitoring%2520sensors%250Aplaced%2520at%2520different%2520locations%252C%2520which%2520usually%2520contain%2520missing%2520values%2520due%2520to%250Avarious%2520failures%252C%2520such%2520as%2520mechanical%2520damages%2520and%2520Internet%2520outages.%2520Imputing%2520the%250Amissing%2520values%2520is%2520crucial%2520for%2520analyzing%2520time%2520series.%2520When%2520recovering%2520a%2520specific%250Adata%2520point%252C%2520most%2520existing%2520methods%2520consider%2520all%2520the%2520information%2520relevant%2520to%2520that%250Apoint%2520regardless%2520of%2520the%2520cause-and-effect%2520relationship.%2520During%2520data%2520collection%252C%250Ait%2520is%2520inevitable%2520that%2520some%2520unknown%2520confounders%2520are%2520included%252C%2520e.g.%252C%2520background%250Anoise%2520in%2520time%2520series%2520and%2520non-causal%2520shortcut%2520edges%2520in%2520the%2520constructed%2520sensor%250Anetwork.%2520These%2520confounders%2520could%2520open%2520backdoor%2520paths%2520and%2520establish%2520non-causal%250Acorrelations%2520between%2520the%2520input%2520and%2520output.%2520Over-exploiting%2520these%2520non-causal%250Acorrelations%2520could%2520cause%2520overfitting.%2520In%2520this%2520paper%252C%2520we%2520first%2520revisit%250Aspatiotemporal%2520time%2520series%2520imputation%2520from%2520a%2520causal%2520perspective%2520and%2520show%2520how%2520to%250Ablock%2520the%2520confounders%2520via%2520the%2520frontdoor%2520adjustment.%2520Based%2520on%2520the%2520results%2520of%250Afrontdoor%2520adjustment%252C%2520we%2520introduce%2520a%2520novel%2520Causality-Aware%2520Spatiotemporal%2520Graph%250ANeural%2520Network%2520%2528Casper%2529%252C%2520which%2520contains%2520a%2520novel%2520Prompt%2520Based%2520Decoder%2520%2528PBD%2529%2520and%250Aa%2520Spatiotemporal%2520Causal%2520Attention%2520%2528SCA%2529.%2520PBD%2520could%2520reduce%2520the%2520impact%2520of%250Aconfounders%2520and%2520SCA%2520could%2520discover%2520the%2520sparse%2520causal%2520relationships%2520among%250Aembeddings.%2520Theoretical%2520analysis%2520reveals%2520that%2520SCA%2520discovers%2520causal%250Arelationships%2520based%2520on%2520the%2520values%2520of%2520gradients.%2520We%2520evaluate%2520Casper%2520on%2520three%250Areal-world%2520datasets%252C%2520and%2520the%2520experimental%2520results%2520show%2520that%2520Casper%2520could%250Aoutperform%2520the%2520baselines%2520and%2520could%2520effectively%2520discover%2520causal%2520relationships.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.11960v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causality-Aware%20Spatiotemporal%20Graph%20Neural%20Networks%20for%20Spatiotemporal%0A%20%20Time%20Series%20Imputation&entry.906535625=Baoyu%20Jing%20and%20Dawei%20Zhou%20and%20Kan%20Ren%20and%20Carl%20Yang&entry.1292438233=%20%20Spatiotemporal%20time%20series%20are%20usually%20collected%20via%20monitoring%20sensors%0Aplaced%20at%20different%20locations%2C%20which%20usually%20contain%20missing%20values%20due%20to%0Avarious%20failures%2C%20such%20as%20mechanical%20damages%20and%20Internet%20outages.%20Imputing%20the%0Amissing%20values%20is%20crucial%20for%20analyzing%20time%20series.%20When%20recovering%20a%20specific%0Adata%20point%2C%20most%20existing%20methods%20consider%20all%20the%20information%20relevant%20to%20that%0Apoint%20regardless%20of%20the%20cause-and-effect%20relationship.%20During%20data%20collection%2C%0Ait%20is%20inevitable%20that%20some%20unknown%20confounders%20are%20included%2C%20e.g.%2C%20background%0Anoise%20in%20time%20series%20and%20non-causal%20shortcut%20edges%20in%20the%20constructed%20sensor%0Anetwork.%20These%20confounders%20could%20open%20backdoor%20paths%20and%20establish%20non-causal%0Acorrelations%20between%20the%20input%20and%20output.%20Over-exploiting%20these%20non-causal%0Acorrelations%20could%20cause%20overfitting.%20In%20this%20paper%2C%20we%20first%20revisit%0Aspatiotemporal%20time%20series%20imputation%20from%20a%20causal%20perspective%20and%20show%20how%20to%0Ablock%20the%20confounders%20via%20the%20frontdoor%20adjustment.%20Based%20on%20the%20results%20of%0Afrontdoor%20adjustment%2C%20we%20introduce%20a%20novel%20Causality-Aware%20Spatiotemporal%20Graph%0ANeural%20Network%20%28Casper%29%2C%20which%20contains%20a%20novel%20Prompt%20Based%20Decoder%20%28PBD%29%20and%0Aa%20Spatiotemporal%20Causal%20Attention%20%28SCA%29.%20PBD%20could%20reduce%20the%20impact%20of%0Aconfounders%20and%20SCA%20could%20discover%20the%20sparse%20causal%20relationships%20among%0Aembeddings.%20Theoretical%20analysis%20reveals%20that%20SCA%20discovers%20causal%0Arelationships%20based%20on%20the%20values%20of%20gradients.%20We%20evaluate%20Casper%20on%20three%0Areal-world%20datasets%2C%20and%20the%20experimental%20results%20show%20that%20Casper%20could%0Aoutperform%20the%20baselines%20and%20could%20effectively%20discover%20causal%20relationships.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11960v3&entry.124074799=Read"},
{"title": "Generalized Naive Bayes", "author": "Edith Alice Kov\u00e1cs and Anna Orsz\u00e1g and D\u00e1niel Pfeifer and Andr\u00e1s Bencz\u00far", "abstract": "  In this paper we introduce the so-called Generalized Naive Bayes structure as\nan extension of the Naive Bayes structure. We give a new greedy algorithm that\nfinds a good fitting Generalized Naive Bayes (GNB) probability distribution. We\nprove that this fits the data at least as well as the probability distribution\ndetermined by the classical Naive Bayes (NB). Then, under a not very\nrestrictive condition, we give a second algorithm for which we can prove that\nit finds the optimal GNB probability distribution, i.e. best fitting structure\nin the sense of KL divergence. Both algorithms are constructed to maximize the\ninformation content and aim to minimize redundancy. Based on these algorithms,\nnew methods for feature selection are introduced. We discuss the similarities\nand differences to other related algorithms in terms of structure, methodology,\nand complexity. Experimental results show, that the algorithms introduced\noutperform the related algorithms in many cases.\n", "link": "http://arxiv.org/abs/2408.15923v1", "date": "2024-08-28", "relevancy": 1.801, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.456}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4468}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Naive%20Bayes&body=Title%3A%20Generalized%20Naive%20Bayes%0AAuthor%3A%20Edith%20Alice%20Kov%C3%A1cs%20and%20Anna%20Orsz%C3%A1g%20and%20D%C3%A1niel%20Pfeifer%20and%20Andr%C3%A1s%20Bencz%C3%BAr%0AAbstract%3A%20%20%20In%20this%20paper%20we%20introduce%20the%20so-called%20Generalized%20Naive%20Bayes%20structure%20as%0Aan%20extension%20of%20the%20Naive%20Bayes%20structure.%20We%20give%20a%20new%20greedy%20algorithm%20that%0Afinds%20a%20good%20fitting%20Generalized%20Naive%20Bayes%20%28GNB%29%20probability%20distribution.%20We%0Aprove%20that%20this%20fits%20the%20data%20at%20least%20as%20well%20as%20the%20probability%20distribution%0Adetermined%20by%20the%20classical%20Naive%20Bayes%20%28NB%29.%20Then%2C%20under%20a%20not%20very%0Arestrictive%20condition%2C%20we%20give%20a%20second%20algorithm%20for%20which%20we%20can%20prove%20that%0Ait%20finds%20the%20optimal%20GNB%20probability%20distribution%2C%20i.e.%20best%20fitting%20structure%0Ain%20the%20sense%20of%20KL%20divergence.%20Both%20algorithms%20are%20constructed%20to%20maximize%20the%0Ainformation%20content%20and%20aim%20to%20minimize%20redundancy.%20Based%20on%20these%20algorithms%2C%0Anew%20methods%20for%20feature%20selection%20are%20introduced.%20We%20discuss%20the%20similarities%0Aand%20differences%20to%20other%20related%20algorithms%20in%20terms%20of%20structure%2C%20methodology%2C%0Aand%20complexity.%20Experimental%20results%20show%2C%20that%20the%20algorithms%20introduced%0Aoutperform%20the%20related%20algorithms%20in%20many%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Naive%2520Bayes%26entry.906535625%3DEdith%2520Alice%2520Kov%25C3%25A1cs%2520and%2520Anna%2520Orsz%25C3%25A1g%2520and%2520D%25C3%25A1niel%2520Pfeifer%2520and%2520Andr%25C3%25A1s%2520Bencz%25C3%25BAr%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520introduce%2520the%2520so-called%2520Generalized%2520Naive%2520Bayes%2520structure%2520as%250Aan%2520extension%2520of%2520the%2520Naive%2520Bayes%2520structure.%2520We%2520give%2520a%2520new%2520greedy%2520algorithm%2520that%250Afinds%2520a%2520good%2520fitting%2520Generalized%2520Naive%2520Bayes%2520%2528GNB%2529%2520probability%2520distribution.%2520We%250Aprove%2520that%2520this%2520fits%2520the%2520data%2520at%2520least%2520as%2520well%2520as%2520the%2520probability%2520distribution%250Adetermined%2520by%2520the%2520classical%2520Naive%2520Bayes%2520%2528NB%2529.%2520Then%252C%2520under%2520a%2520not%2520very%250Arestrictive%2520condition%252C%2520we%2520give%2520a%2520second%2520algorithm%2520for%2520which%2520we%2520can%2520prove%2520that%250Ait%2520finds%2520the%2520optimal%2520GNB%2520probability%2520distribution%252C%2520i.e.%2520best%2520fitting%2520structure%250Ain%2520the%2520sense%2520of%2520KL%2520divergence.%2520Both%2520algorithms%2520are%2520constructed%2520to%2520maximize%2520the%250Ainformation%2520content%2520and%2520aim%2520to%2520minimize%2520redundancy.%2520Based%2520on%2520these%2520algorithms%252C%250Anew%2520methods%2520for%2520feature%2520selection%2520are%2520introduced.%2520We%2520discuss%2520the%2520similarities%250Aand%2520differences%2520to%2520other%2520related%2520algorithms%2520in%2520terms%2520of%2520structure%252C%2520methodology%252C%250Aand%2520complexity.%2520Experimental%2520results%2520show%252C%2520that%2520the%2520algorithms%2520introduced%250Aoutperform%2520the%2520related%2520algorithms%2520in%2520many%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Naive%20Bayes&entry.906535625=Edith%20Alice%20Kov%C3%A1cs%20and%20Anna%20Orsz%C3%A1g%20and%20D%C3%A1niel%20Pfeifer%20and%20Andr%C3%A1s%20Bencz%C3%BAr&entry.1292438233=%20%20In%20this%20paper%20we%20introduce%20the%20so-called%20Generalized%20Naive%20Bayes%20structure%20as%0Aan%20extension%20of%20the%20Naive%20Bayes%20structure.%20We%20give%20a%20new%20greedy%20algorithm%20that%0Afinds%20a%20good%20fitting%20Generalized%20Naive%20Bayes%20%28GNB%29%20probability%20distribution.%20We%0Aprove%20that%20this%20fits%20the%20data%20at%20least%20as%20well%20as%20the%20probability%20distribution%0Adetermined%20by%20the%20classical%20Naive%20Bayes%20%28NB%29.%20Then%2C%20under%20a%20not%20very%0Arestrictive%20condition%2C%20we%20give%20a%20second%20algorithm%20for%20which%20we%20can%20prove%20that%0Ait%20finds%20the%20optimal%20GNB%20probability%20distribution%2C%20i.e.%20best%20fitting%20structure%0Ain%20the%20sense%20of%20KL%20divergence.%20Both%20algorithms%20are%20constructed%20to%20maximize%20the%0Ainformation%20content%20and%20aim%20to%20minimize%20redundancy.%20Based%20on%20these%20algorithms%2C%0Anew%20methods%20for%20feature%20selection%20are%20introduced.%20We%20discuss%20the%20similarities%0Aand%20differences%20to%20other%20related%20algorithms%20in%20terms%20of%20structure%2C%20methodology%2C%0Aand%20complexity.%20Experimental%20results%20show%2C%20that%20the%20algorithms%20introduced%0Aoutperform%20the%20related%20algorithms%20in%20many%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15923v1&entry.124074799=Read"},
{"title": "Research on the Spatial Data Intelligent Foundation Model", "author": "Shaohua Wang and Xing Xie and Yong Li and Danhuai Guo and Zhi Cai and Yu Liu and Yang Yue and Xiao Pan and Feng Lu and Huayi Wu and Zhipeng Gui and Zhiming Ding and Bolong Zheng and Fuzheng Zhang and Jingyuan Wang and Zhengchao Chen and Hao Lu and Jiayi Li and Peng Yue and Wenhao Yu and Yao Yao and Leilei Sun and Yong Zhang and Longbiao Chen and Xiaoping Du and Xiang Li and Xueying Zhang and Kun Qin and Zhaoya Gong and Weihua Dong and Xiaofeng Meng", "abstract": "  This report focuses on spatial data intelligent large models, delving into\nthe principles, methods, and cutting-edge applications of these models. It\nprovides an in-depth discussion on the definition, development history, current\nstatus, and trends of spatial data intelligent large models, as well as the\nchallenges they face. The report systematically elucidates the key technologies\nof spatial data intelligent large models and their applications in urban\nenvironments, aerospace remote sensing, geography, transportation, and other\nscenarios. Additionally, it summarizes the latest application cases of spatial\ndata intelligent large models in themes such as urban development, multimodal\nsystems, remote sensing, smart transportation, and resource environments.\nFinally, the report concludes with an overview and outlook on the development\nprospects of spatial data intelligent large models.\n", "link": "http://arxiv.org/abs/2405.19730v5", "date": "2024-08-28", "relevancy": 1.7876, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4676}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4437}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4418}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Research%20on%20the%20Spatial%20Data%20Intelligent%20Foundation%20Model&body=Title%3A%20Research%20on%20the%20Spatial%20Data%20Intelligent%20Foundation%20Model%0AAuthor%3A%20Shaohua%20Wang%20and%20Xing%20Xie%20and%20Yong%20Li%20and%20Danhuai%20Guo%20and%20Zhi%20Cai%20and%20Yu%20Liu%20and%20Yang%20Yue%20and%20Xiao%20Pan%20and%20Feng%20Lu%20and%20Huayi%20Wu%20and%20Zhipeng%20Gui%20and%20Zhiming%20Ding%20and%20Bolong%20Zheng%20and%20Fuzheng%20Zhang%20and%20Jingyuan%20Wang%20and%20Zhengchao%20Chen%20and%20Hao%20Lu%20and%20Jiayi%20Li%20and%20Peng%20Yue%20and%20Wenhao%20Yu%20and%20Yao%20Yao%20and%20Leilei%20Sun%20and%20Yong%20Zhang%20and%20Longbiao%20Chen%20and%20Xiaoping%20Du%20and%20Xiang%20Li%20and%20Xueying%20Zhang%20and%20Kun%20Qin%20and%20Zhaoya%20Gong%20and%20Weihua%20Dong%20and%20Xiaofeng%20Meng%0AAbstract%3A%20%20%20This%20report%20focuses%20on%20spatial%20data%20intelligent%20large%20models%2C%20delving%20into%0Athe%20principles%2C%20methods%2C%20and%20cutting-edge%20applications%20of%20these%20models.%20It%0Aprovides%20an%20in-depth%20discussion%20on%20the%20definition%2C%20development%20history%2C%20current%0Astatus%2C%20and%20trends%20of%20spatial%20data%20intelligent%20large%20models%2C%20as%20well%20as%20the%0Achallenges%20they%20face.%20The%20report%20systematically%20elucidates%20the%20key%20technologies%0Aof%20spatial%20data%20intelligent%20large%20models%20and%20their%20applications%20in%20urban%0Aenvironments%2C%20aerospace%20remote%20sensing%2C%20geography%2C%20transportation%2C%20and%20other%0Ascenarios.%20Additionally%2C%20it%20summarizes%20the%20latest%20application%20cases%20of%20spatial%0Adata%20intelligent%20large%20models%20in%20themes%20such%20as%20urban%20development%2C%20multimodal%0Asystems%2C%20remote%20sensing%2C%20smart%20transportation%2C%20and%20resource%20environments.%0AFinally%2C%20the%20report%20concludes%20with%20an%20overview%20and%20outlook%20on%20the%20development%0Aprospects%20of%20spatial%20data%20intelligent%20large%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19730v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResearch%2520on%2520the%2520Spatial%2520Data%2520Intelligent%2520Foundation%2520Model%26entry.906535625%3DShaohua%2520Wang%2520and%2520Xing%2520Xie%2520and%2520Yong%2520Li%2520and%2520Danhuai%2520Guo%2520and%2520Zhi%2520Cai%2520and%2520Yu%2520Liu%2520and%2520Yang%2520Yue%2520and%2520Xiao%2520Pan%2520and%2520Feng%2520Lu%2520and%2520Huayi%2520Wu%2520and%2520Zhipeng%2520Gui%2520and%2520Zhiming%2520Ding%2520and%2520Bolong%2520Zheng%2520and%2520Fuzheng%2520Zhang%2520and%2520Jingyuan%2520Wang%2520and%2520Zhengchao%2520Chen%2520and%2520Hao%2520Lu%2520and%2520Jiayi%2520Li%2520and%2520Peng%2520Yue%2520and%2520Wenhao%2520Yu%2520and%2520Yao%2520Yao%2520and%2520Leilei%2520Sun%2520and%2520Yong%2520Zhang%2520and%2520Longbiao%2520Chen%2520and%2520Xiaoping%2520Du%2520and%2520Xiang%2520Li%2520and%2520Xueying%2520Zhang%2520and%2520Kun%2520Qin%2520and%2520Zhaoya%2520Gong%2520and%2520Weihua%2520Dong%2520and%2520Xiaofeng%2520Meng%26entry.1292438233%3D%2520%2520This%2520report%2520focuses%2520on%2520spatial%2520data%2520intelligent%2520large%2520models%252C%2520delving%2520into%250Athe%2520principles%252C%2520methods%252C%2520and%2520cutting-edge%2520applications%2520of%2520these%2520models.%2520It%250Aprovides%2520an%2520in-depth%2520discussion%2520on%2520the%2520definition%252C%2520development%2520history%252C%2520current%250Astatus%252C%2520and%2520trends%2520of%2520spatial%2520data%2520intelligent%2520large%2520models%252C%2520as%2520well%2520as%2520the%250Achallenges%2520they%2520face.%2520The%2520report%2520systematically%2520elucidates%2520the%2520key%2520technologies%250Aof%2520spatial%2520data%2520intelligent%2520large%2520models%2520and%2520their%2520applications%2520in%2520urban%250Aenvironments%252C%2520aerospace%2520remote%2520sensing%252C%2520geography%252C%2520transportation%252C%2520and%2520other%250Ascenarios.%2520Additionally%252C%2520it%2520summarizes%2520the%2520latest%2520application%2520cases%2520of%2520spatial%250Adata%2520intelligent%2520large%2520models%2520in%2520themes%2520such%2520as%2520urban%2520development%252C%2520multimodal%250Asystems%252C%2520remote%2520sensing%252C%2520smart%2520transportation%252C%2520and%2520resource%2520environments.%250AFinally%252C%2520the%2520report%2520concludes%2520with%2520an%2520overview%2520and%2520outlook%2520on%2520the%2520development%250Aprospects%2520of%2520spatial%2520data%2520intelligent%2520large%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19730v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Research%20on%20the%20Spatial%20Data%20Intelligent%20Foundation%20Model&entry.906535625=Shaohua%20Wang%20and%20Xing%20Xie%20and%20Yong%20Li%20and%20Danhuai%20Guo%20and%20Zhi%20Cai%20and%20Yu%20Liu%20and%20Yang%20Yue%20and%20Xiao%20Pan%20and%20Feng%20Lu%20and%20Huayi%20Wu%20and%20Zhipeng%20Gui%20and%20Zhiming%20Ding%20and%20Bolong%20Zheng%20and%20Fuzheng%20Zhang%20and%20Jingyuan%20Wang%20and%20Zhengchao%20Chen%20and%20Hao%20Lu%20and%20Jiayi%20Li%20and%20Peng%20Yue%20and%20Wenhao%20Yu%20and%20Yao%20Yao%20and%20Leilei%20Sun%20and%20Yong%20Zhang%20and%20Longbiao%20Chen%20and%20Xiaoping%20Du%20and%20Xiang%20Li%20and%20Xueying%20Zhang%20and%20Kun%20Qin%20and%20Zhaoya%20Gong%20and%20Weihua%20Dong%20and%20Xiaofeng%20Meng&entry.1292438233=%20%20This%20report%20focuses%20on%20spatial%20data%20intelligent%20large%20models%2C%20delving%20into%0Athe%20principles%2C%20methods%2C%20and%20cutting-edge%20applications%20of%20these%20models.%20It%0Aprovides%20an%20in-depth%20discussion%20on%20the%20definition%2C%20development%20history%2C%20current%0Astatus%2C%20and%20trends%20of%20spatial%20data%20intelligent%20large%20models%2C%20as%20well%20as%20the%0Achallenges%20they%20face.%20The%20report%20systematically%20elucidates%20the%20key%20technologies%0Aof%20spatial%20data%20intelligent%20large%20models%20and%20their%20applications%20in%20urban%0Aenvironments%2C%20aerospace%20remote%20sensing%2C%20geography%2C%20transportation%2C%20and%20other%0Ascenarios.%20Additionally%2C%20it%20summarizes%20the%20latest%20application%20cases%20of%20spatial%0Adata%20intelligent%20large%20models%20in%20themes%20such%20as%20urban%20development%2C%20multimodal%0Asystems%2C%20remote%20sensing%2C%20smart%20transportation%2C%20and%20resource%20environments.%0AFinally%2C%20the%20report%20concludes%20with%20an%20overview%20and%20outlook%20on%20the%20development%0Aprospects%20of%20spatial%20data%20intelligent%20large%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19730v5&entry.124074799=Read"},
{"title": "AlphaForge: A Framework to Mine and Dynamically Combine Formulaic Alpha\n  Factors", "author": "Hao Shi and Weili Song and Xinting Zhang and Jiahe Shi and Cuicui Luo and Xiang Ao and Hamid Arian and Luis Seco", "abstract": "  The complexity of financial data, characterized by its variability and low\nsignal-to-noise ratio, necessitates advanced methods in quantitative investment\nthat prioritize both performance and interpretability.Transitioning from early\nmanual extraction to genetic programming, the most advanced approach in the\nalpha factor mining domain currently employs reinforcement learning to mine a\nset of combination factors with fixed weights. However, the performance of\nresultant alpha factors exhibits inconsistency, and the inflexibility of fixed\nfactor weights proves insufficient in adapting to the dynamic nature of\nfinancial markets. To address this issue, this paper proposes a two-stage\nformulaic alpha generating framework AlphaForge, for alpha factor mining and\nfactor combination. This framework employs a generative-predictive neural\nnetwork to generate factors, leveraging the robust spatial exploration\ncapabilities inherent in deep learning while concurrently preserving diversity.\nThe combination model within the framework incorporates the temporal\nperformance of factors for selection and dynamically adjusts the weights\nassigned to each component alpha factor. Experiments conducted on real-world\ndatasets demonstrate that our proposed model outperforms contemporary\nbenchmarks in formulaic alpha factor mining. Furthermore, our model exhibits a\nnotable enhancement in portfolio returns within the realm of quantitative\ninvestment and real money investment.\n", "link": "http://arxiv.org/abs/2406.18394v4", "date": "2024-08-28", "relevancy": 1.7851, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4529}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4469}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlphaForge%3A%20A%20Framework%20to%20Mine%20and%20Dynamically%20Combine%20Formulaic%20Alpha%0A%20%20Factors&body=Title%3A%20AlphaForge%3A%20A%20Framework%20to%20Mine%20and%20Dynamically%20Combine%20Formulaic%20Alpha%0A%20%20Factors%0AAuthor%3A%20Hao%20Shi%20and%20Weili%20Song%20and%20Xinting%20Zhang%20and%20Jiahe%20Shi%20and%20Cuicui%20Luo%20and%20Xiang%20Ao%20and%20Hamid%20Arian%20and%20Luis%20Seco%0AAbstract%3A%20%20%20The%20complexity%20of%20financial%20data%2C%20characterized%20by%20its%20variability%20and%20low%0Asignal-to-noise%20ratio%2C%20necessitates%20advanced%20methods%20in%20quantitative%20investment%0Athat%20prioritize%20both%20performance%20and%20interpretability.Transitioning%20from%20early%0Amanual%20extraction%20to%20genetic%20programming%2C%20the%20most%20advanced%20approach%20in%20the%0Aalpha%20factor%20mining%20domain%20currently%20employs%20reinforcement%20learning%20to%20mine%20a%0Aset%20of%20combination%20factors%20with%20fixed%20weights.%20However%2C%20the%20performance%20of%0Aresultant%20alpha%20factors%20exhibits%20inconsistency%2C%20and%20the%20inflexibility%20of%20fixed%0Afactor%20weights%20proves%20insufficient%20in%20adapting%20to%20the%20dynamic%20nature%20of%0Afinancial%20markets.%20To%20address%20this%20issue%2C%20this%20paper%20proposes%20a%20two-stage%0Aformulaic%20alpha%20generating%20framework%20AlphaForge%2C%20for%20alpha%20factor%20mining%20and%0Afactor%20combination.%20This%20framework%20employs%20a%20generative-predictive%20neural%0Anetwork%20to%20generate%20factors%2C%20leveraging%20the%20robust%20spatial%20exploration%0Acapabilities%20inherent%20in%20deep%20learning%20while%20concurrently%20preserving%20diversity.%0AThe%20combination%20model%20within%20the%20framework%20incorporates%20the%20temporal%0Aperformance%20of%20factors%20for%20selection%20and%20dynamically%20adjusts%20the%20weights%0Aassigned%20to%20each%20component%20alpha%20factor.%20Experiments%20conducted%20on%20real-world%0Adatasets%20demonstrate%20that%20our%20proposed%20model%20outperforms%20contemporary%0Abenchmarks%20in%20formulaic%20alpha%20factor%20mining.%20Furthermore%2C%20our%20model%20exhibits%20a%0Anotable%20enhancement%20in%20portfolio%20returns%20within%20the%20realm%20of%20quantitative%0Ainvestment%20and%20real%20money%20investment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.18394v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlphaForge%253A%2520A%2520Framework%2520to%2520Mine%2520and%2520Dynamically%2520Combine%2520Formulaic%2520Alpha%250A%2520%2520Factors%26entry.906535625%3DHao%2520Shi%2520and%2520Weili%2520Song%2520and%2520Xinting%2520Zhang%2520and%2520Jiahe%2520Shi%2520and%2520Cuicui%2520Luo%2520and%2520Xiang%2520Ao%2520and%2520Hamid%2520Arian%2520and%2520Luis%2520Seco%26entry.1292438233%3D%2520%2520The%2520complexity%2520of%2520financial%2520data%252C%2520characterized%2520by%2520its%2520variability%2520and%2520low%250Asignal-to-noise%2520ratio%252C%2520necessitates%2520advanced%2520methods%2520in%2520quantitative%2520investment%250Athat%2520prioritize%2520both%2520performance%2520and%2520interpretability.Transitioning%2520from%2520early%250Amanual%2520extraction%2520to%2520genetic%2520programming%252C%2520the%2520most%2520advanced%2520approach%2520in%2520the%250Aalpha%2520factor%2520mining%2520domain%2520currently%2520employs%2520reinforcement%2520learning%2520to%2520mine%2520a%250Aset%2520of%2520combination%2520factors%2520with%2520fixed%2520weights.%2520However%252C%2520the%2520performance%2520of%250Aresultant%2520alpha%2520factors%2520exhibits%2520inconsistency%252C%2520and%2520the%2520inflexibility%2520of%2520fixed%250Afactor%2520weights%2520proves%2520insufficient%2520in%2520adapting%2520to%2520the%2520dynamic%2520nature%2520of%250Afinancial%2520markets.%2520To%2520address%2520this%2520issue%252C%2520this%2520paper%2520proposes%2520a%2520two-stage%250Aformulaic%2520alpha%2520generating%2520framework%2520AlphaForge%252C%2520for%2520alpha%2520factor%2520mining%2520and%250Afactor%2520combination.%2520This%2520framework%2520employs%2520a%2520generative-predictive%2520neural%250Anetwork%2520to%2520generate%2520factors%252C%2520leveraging%2520the%2520robust%2520spatial%2520exploration%250Acapabilities%2520inherent%2520in%2520deep%2520learning%2520while%2520concurrently%2520preserving%2520diversity.%250AThe%2520combination%2520model%2520within%2520the%2520framework%2520incorporates%2520the%2520temporal%250Aperformance%2520of%2520factors%2520for%2520selection%2520and%2520dynamically%2520adjusts%2520the%2520weights%250Aassigned%2520to%2520each%2520component%2520alpha%2520factor.%2520Experiments%2520conducted%2520on%2520real-world%250Adatasets%2520demonstrate%2520that%2520our%2520proposed%2520model%2520outperforms%2520contemporary%250Abenchmarks%2520in%2520formulaic%2520alpha%2520factor%2520mining.%2520Furthermore%252C%2520our%2520model%2520exhibits%2520a%250Anotable%2520enhancement%2520in%2520portfolio%2520returns%2520within%2520the%2520realm%2520of%2520quantitative%250Ainvestment%2520and%2520real%2520money%2520investment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.18394v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlphaForge%3A%20A%20Framework%20to%20Mine%20and%20Dynamically%20Combine%20Formulaic%20Alpha%0A%20%20Factors&entry.906535625=Hao%20Shi%20and%20Weili%20Song%20and%20Xinting%20Zhang%20and%20Jiahe%20Shi%20and%20Cuicui%20Luo%20and%20Xiang%20Ao%20and%20Hamid%20Arian%20and%20Luis%20Seco&entry.1292438233=%20%20The%20complexity%20of%20financial%20data%2C%20characterized%20by%20its%20variability%20and%20low%0Asignal-to-noise%20ratio%2C%20necessitates%20advanced%20methods%20in%20quantitative%20investment%0Athat%20prioritize%20both%20performance%20and%20interpretability.Transitioning%20from%20early%0Amanual%20extraction%20to%20genetic%20programming%2C%20the%20most%20advanced%20approach%20in%20the%0Aalpha%20factor%20mining%20domain%20currently%20employs%20reinforcement%20learning%20to%20mine%20a%0Aset%20of%20combination%20factors%20with%20fixed%20weights.%20However%2C%20the%20performance%20of%0Aresultant%20alpha%20factors%20exhibits%20inconsistency%2C%20and%20the%20inflexibility%20of%20fixed%0Afactor%20weights%20proves%20insufficient%20in%20adapting%20to%20the%20dynamic%20nature%20of%0Afinancial%20markets.%20To%20address%20this%20issue%2C%20this%20paper%20proposes%20a%20two-stage%0Aformulaic%20alpha%20generating%20framework%20AlphaForge%2C%20for%20alpha%20factor%20mining%20and%0Afactor%20combination.%20This%20framework%20employs%20a%20generative-predictive%20neural%0Anetwork%20to%20generate%20factors%2C%20leveraging%20the%20robust%20spatial%20exploration%0Acapabilities%20inherent%20in%20deep%20learning%20while%20concurrently%20preserving%20diversity.%0AThe%20combination%20model%20within%20the%20framework%20incorporates%20the%20temporal%0Aperformance%20of%20factors%20for%20selection%20and%20dynamically%20adjusts%20the%20weights%0Aassigned%20to%20each%20component%20alpha%20factor.%20Experiments%20conducted%20on%20real-world%0Adatasets%20demonstrate%20that%20our%20proposed%20model%20outperforms%20contemporary%0Abenchmarks%20in%20formulaic%20alpha%20factor%20mining.%20Furthermore%2C%20our%20model%20exhibits%20a%0Anotable%20enhancement%20in%20portfolio%20returns%20within%20the%20realm%20of%20quantitative%0Ainvestment%20and%20real%20money%20investment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.18394v4&entry.124074799=Read"},
{"title": "Contextual Bandit with Herding Effects: Algorithms and Recommendation\n  Applications", "author": "Luyue Xu and Liming Wang and Hong Xie and Mingqiang Zhou", "abstract": "  Contextual bandits serve as a fundamental algorithmic framework for\noptimizing recommendation decisions online. Though extensive attention has been\npaid to tailoring contextual bandits for recommendation applications, the\n\"herding effects\" in user feedback have been ignored. These herding effects\nbias user feedback toward historical ratings, breaking down the assumption of\nunbiased feedback inherent in contextual bandits. This paper develops a novel\nvariant of the contextual bandit that is tailored to address the feedback bias\ncaused by the herding effects. A user feedback model is formulated to capture\nthis feedback bias. We design the TS-Conf (Thompson Sampling under Conformity)\nalgorithm, which employs posterior sampling to balance the exploration and\nexploitation tradeoff. We prove an upper bound for the regret of the algorithm,\nrevealing the impact of herding effects on learning speed. Extensive\nexperiments on datasets demonstrate that TS-Conf outperforms four benchmark\nalgorithms. Analysis reveals that TS-Conf effectively mitigates the negative\nimpact of herding effects, resulting in faster learning and improved\nrecommendation accuracy.\n", "link": "http://arxiv.org/abs/2408.14432v2", "date": "2024-08-28", "relevancy": 1.7727, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4646}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4454}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20Bandit%20with%20Herding%20Effects%3A%20Algorithms%20and%20Recommendation%0A%20%20Applications&body=Title%3A%20Contextual%20Bandit%20with%20Herding%20Effects%3A%20Algorithms%20and%20Recommendation%0A%20%20Applications%0AAuthor%3A%20Luyue%20Xu%20and%20Liming%20Wang%20and%20Hong%20Xie%20and%20Mingqiang%20Zhou%0AAbstract%3A%20%20%20Contextual%20bandits%20serve%20as%20a%20fundamental%20algorithmic%20framework%20for%0Aoptimizing%20recommendation%20decisions%20online.%20Though%20extensive%20attention%20has%20been%0Apaid%20to%20tailoring%20contextual%20bandits%20for%20recommendation%20applications%2C%20the%0A%22herding%20effects%22%20in%20user%20feedback%20have%20been%20ignored.%20These%20herding%20effects%0Abias%20user%20feedback%20toward%20historical%20ratings%2C%20breaking%20down%20the%20assumption%20of%0Aunbiased%20feedback%20inherent%20in%20contextual%20bandits.%20This%20paper%20develops%20a%20novel%0Avariant%20of%20the%20contextual%20bandit%20that%20is%20tailored%20to%20address%20the%20feedback%20bias%0Acaused%20by%20the%20herding%20effects.%20A%20user%20feedback%20model%20is%20formulated%20to%20capture%0Athis%20feedback%20bias.%20We%20design%20the%20TS-Conf%20%28Thompson%20Sampling%20under%20Conformity%29%0Aalgorithm%2C%20which%20employs%20posterior%20sampling%20to%20balance%20the%20exploration%20and%0Aexploitation%20tradeoff.%20We%20prove%20an%20upper%20bound%20for%20the%20regret%20of%20the%20algorithm%2C%0Arevealing%20the%20impact%20of%20herding%20effects%20on%20learning%20speed.%20Extensive%0Aexperiments%20on%20datasets%20demonstrate%20that%20TS-Conf%20outperforms%20four%20benchmark%0Aalgorithms.%20Analysis%20reveals%20that%20TS-Conf%20effectively%20mitigates%20the%20negative%0Aimpact%20of%20herding%20effects%2C%20resulting%20in%20faster%20learning%20and%20improved%0Arecommendation%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14432v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520Bandit%2520with%2520Herding%2520Effects%253A%2520Algorithms%2520and%2520Recommendation%250A%2520%2520Applications%26entry.906535625%3DLuyue%2520Xu%2520and%2520Liming%2520Wang%2520and%2520Hong%2520Xie%2520and%2520Mingqiang%2520Zhou%26entry.1292438233%3D%2520%2520Contextual%2520bandits%2520serve%2520as%2520a%2520fundamental%2520algorithmic%2520framework%2520for%250Aoptimizing%2520recommendation%2520decisions%2520online.%2520Though%2520extensive%2520attention%2520has%2520been%250Apaid%2520to%2520tailoring%2520contextual%2520bandits%2520for%2520recommendation%2520applications%252C%2520the%250A%2522herding%2520effects%2522%2520in%2520user%2520feedback%2520have%2520been%2520ignored.%2520These%2520herding%2520effects%250Abias%2520user%2520feedback%2520toward%2520historical%2520ratings%252C%2520breaking%2520down%2520the%2520assumption%2520of%250Aunbiased%2520feedback%2520inherent%2520in%2520contextual%2520bandits.%2520This%2520paper%2520develops%2520a%2520novel%250Avariant%2520of%2520the%2520contextual%2520bandit%2520that%2520is%2520tailored%2520to%2520address%2520the%2520feedback%2520bias%250Acaused%2520by%2520the%2520herding%2520effects.%2520A%2520user%2520feedback%2520model%2520is%2520formulated%2520to%2520capture%250Athis%2520feedback%2520bias.%2520We%2520design%2520the%2520TS-Conf%2520%2528Thompson%2520Sampling%2520under%2520Conformity%2529%250Aalgorithm%252C%2520which%2520employs%2520posterior%2520sampling%2520to%2520balance%2520the%2520exploration%2520and%250Aexploitation%2520tradeoff.%2520We%2520prove%2520an%2520upper%2520bound%2520for%2520the%2520regret%2520of%2520the%2520algorithm%252C%250Arevealing%2520the%2520impact%2520of%2520herding%2520effects%2520on%2520learning%2520speed.%2520Extensive%250Aexperiments%2520on%2520datasets%2520demonstrate%2520that%2520TS-Conf%2520outperforms%2520four%2520benchmark%250Aalgorithms.%2520Analysis%2520reveals%2520that%2520TS-Conf%2520effectively%2520mitigates%2520the%2520negative%250Aimpact%2520of%2520herding%2520effects%252C%2520resulting%2520in%2520faster%2520learning%2520and%2520improved%250Arecommendation%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14432v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Bandit%20with%20Herding%20Effects%3A%20Algorithms%20and%20Recommendation%0A%20%20Applications&entry.906535625=Luyue%20Xu%20and%20Liming%20Wang%20and%20Hong%20Xie%20and%20Mingqiang%20Zhou&entry.1292438233=%20%20Contextual%20bandits%20serve%20as%20a%20fundamental%20algorithmic%20framework%20for%0Aoptimizing%20recommendation%20decisions%20online.%20Though%20extensive%20attention%20has%20been%0Apaid%20to%20tailoring%20contextual%20bandits%20for%20recommendation%20applications%2C%20the%0A%22herding%20effects%22%20in%20user%20feedback%20have%20been%20ignored.%20These%20herding%20effects%0Abias%20user%20feedback%20toward%20historical%20ratings%2C%20breaking%20down%20the%20assumption%20of%0Aunbiased%20feedback%20inherent%20in%20contextual%20bandits.%20This%20paper%20develops%20a%20novel%0Avariant%20of%20the%20contextual%20bandit%20that%20is%20tailored%20to%20address%20the%20feedback%20bias%0Acaused%20by%20the%20herding%20effects.%20A%20user%20feedback%20model%20is%20formulated%20to%20capture%0Athis%20feedback%20bias.%20We%20design%20the%20TS-Conf%20%28Thompson%20Sampling%20under%20Conformity%29%0Aalgorithm%2C%20which%20employs%20posterior%20sampling%20to%20balance%20the%20exploration%20and%0Aexploitation%20tradeoff.%20We%20prove%20an%20upper%20bound%20for%20the%20regret%20of%20the%20algorithm%2C%0Arevealing%20the%20impact%20of%20herding%20effects%20on%20learning%20speed.%20Extensive%0Aexperiments%20on%20datasets%20demonstrate%20that%20TS-Conf%20outperforms%20four%20benchmark%0Aalgorithms.%20Analysis%20reveals%20that%20TS-Conf%20effectively%20mitigates%20the%20negative%0Aimpact%20of%20herding%20effects%2C%20resulting%20in%20faster%20learning%20and%20improved%0Arecommendation%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14432v2&entry.124074799=Read"},
{"title": "FRANC: A Lightweight Framework for High-Quality Code Generation", "author": "Mohammed Latif Siddiq and Beatrice Casey and Joanna C. S. Santos", "abstract": "  In recent years, the use of automated source code generation utilizing\ntransformer-based generative models has expanded, and these models can generate\nfunctional code according to the requirements of the developers. However,\nrecent research revealed that these automatically generated source codes can\ncontain vulnerabilities and other quality issues. Despite researchers' and\npractitioners' attempts to enhance code generation models, retraining and\nfine-tuning large language models is time-consuming and resource-intensive.\nThus, we describe FRANC, a lightweight framework for recommending more secure\nand high-quality source code derived from transformer-based code generation\nmodels. FRANC includes a static filter to make the generated code compilable\nwith heuristics and a quality-aware ranker to sort the code snippets based on a\nquality score. Moreover, the framework uses prompt engineering to fix\npersistent quality issues. We evaluated the framework with five Python and Java\ncode generation models and six prompt datasets, including a newly created one\nin this work (SOEval). The static filter improves 9% to 46% Java suggestions\nand 10% to 43% Python suggestions regarding compilability. The average\nimprovement over the NDCG@10 score for the ranking system is 0.0763, and the\nrepairing techniques repair the highest 80% of prompts. FRANC takes, on\naverage, 1.98 seconds for Java; for Python, it takes 0.08 seconds.\n", "link": "http://arxiv.org/abs/2307.08220v2", "date": "2024-08-28", "relevancy": 1.7612, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4614}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4373}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FRANC%3A%20A%20Lightweight%20Framework%20for%20High-Quality%20Code%20Generation&body=Title%3A%20FRANC%3A%20A%20Lightweight%20Framework%20for%20High-Quality%20Code%20Generation%0AAuthor%3A%20Mohammed%20Latif%20Siddiq%20and%20Beatrice%20Casey%20and%20Joanna%20C.%20S.%20Santos%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20use%20of%20automated%20source%20code%20generation%20utilizing%0Atransformer-based%20generative%20models%20has%20expanded%2C%20and%20these%20models%20can%20generate%0Afunctional%20code%20according%20to%20the%20requirements%20of%20the%20developers.%20However%2C%0Arecent%20research%20revealed%20that%20these%20automatically%20generated%20source%20codes%20can%0Acontain%20vulnerabilities%20and%20other%20quality%20issues.%20Despite%20researchers%27%20and%0Apractitioners%27%20attempts%20to%20enhance%20code%20generation%20models%2C%20retraining%20and%0Afine-tuning%20large%20language%20models%20is%20time-consuming%20and%20resource-intensive.%0AThus%2C%20we%20describe%20FRANC%2C%20a%20lightweight%20framework%20for%20recommending%20more%20secure%0Aand%20high-quality%20source%20code%20derived%20from%20transformer-based%20code%20generation%0Amodels.%20FRANC%20includes%20a%20static%20filter%20to%20make%20the%20generated%20code%20compilable%0Awith%20heuristics%20and%20a%20quality-aware%20ranker%20to%20sort%20the%20code%20snippets%20based%20on%20a%0Aquality%20score.%20Moreover%2C%20the%20framework%20uses%20prompt%20engineering%20to%20fix%0Apersistent%20quality%20issues.%20We%20evaluated%20the%20framework%20with%20five%20Python%20and%20Java%0Acode%20generation%20models%20and%20six%20prompt%20datasets%2C%20including%20a%20newly%20created%20one%0Ain%20this%20work%20%28SOEval%29.%20The%20static%20filter%20improves%209%25%20to%2046%25%20Java%20suggestions%0Aand%2010%25%20to%2043%25%20Python%20suggestions%20regarding%20compilability.%20The%20average%0Aimprovement%20over%20the%20NDCG%4010%20score%20for%20the%20ranking%20system%20is%200.0763%2C%20and%20the%0Arepairing%20techniques%20repair%20the%20highest%2080%25%20of%20prompts.%20FRANC%20takes%2C%20on%0Aaverage%2C%201.98%20seconds%20for%20Java%3B%20for%20Python%2C%20it%20takes%200.08%20seconds.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.08220v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFRANC%253A%2520A%2520Lightweight%2520Framework%2520for%2520High-Quality%2520Code%2520Generation%26entry.906535625%3DMohammed%2520Latif%2520Siddiq%2520and%2520Beatrice%2520Casey%2520and%2520Joanna%2520C.%2520S.%2520Santos%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520use%2520of%2520automated%2520source%2520code%2520generation%2520utilizing%250Atransformer-based%2520generative%2520models%2520has%2520expanded%252C%2520and%2520these%2520models%2520can%2520generate%250Afunctional%2520code%2520according%2520to%2520the%2520requirements%2520of%2520the%2520developers.%2520However%252C%250Arecent%2520research%2520revealed%2520that%2520these%2520automatically%2520generated%2520source%2520codes%2520can%250Acontain%2520vulnerabilities%2520and%2520other%2520quality%2520issues.%2520Despite%2520researchers%2527%2520and%250Apractitioners%2527%2520attempts%2520to%2520enhance%2520code%2520generation%2520models%252C%2520retraining%2520and%250Afine-tuning%2520large%2520language%2520models%2520is%2520time-consuming%2520and%2520resource-intensive.%250AThus%252C%2520we%2520describe%2520FRANC%252C%2520a%2520lightweight%2520framework%2520for%2520recommending%2520more%2520secure%250Aand%2520high-quality%2520source%2520code%2520derived%2520from%2520transformer-based%2520code%2520generation%250Amodels.%2520FRANC%2520includes%2520a%2520static%2520filter%2520to%2520make%2520the%2520generated%2520code%2520compilable%250Awith%2520heuristics%2520and%2520a%2520quality-aware%2520ranker%2520to%2520sort%2520the%2520code%2520snippets%2520based%2520on%2520a%250Aquality%2520score.%2520Moreover%252C%2520the%2520framework%2520uses%2520prompt%2520engineering%2520to%2520fix%250Apersistent%2520quality%2520issues.%2520We%2520evaluated%2520the%2520framework%2520with%2520five%2520Python%2520and%2520Java%250Acode%2520generation%2520models%2520and%2520six%2520prompt%2520datasets%252C%2520including%2520a%2520newly%2520created%2520one%250Ain%2520this%2520work%2520%2528SOEval%2529.%2520The%2520static%2520filter%2520improves%25209%2525%2520to%252046%2525%2520Java%2520suggestions%250Aand%252010%2525%2520to%252043%2525%2520Python%2520suggestions%2520regarding%2520compilability.%2520The%2520average%250Aimprovement%2520over%2520the%2520NDCG%254010%2520score%2520for%2520the%2520ranking%2520system%2520is%25200.0763%252C%2520and%2520the%250Arepairing%2520techniques%2520repair%2520the%2520highest%252080%2525%2520of%2520prompts.%2520FRANC%2520takes%252C%2520on%250Aaverage%252C%25201.98%2520seconds%2520for%2520Java%253B%2520for%2520Python%252C%2520it%2520takes%25200.08%2520seconds.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.08220v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRANC%3A%20A%20Lightweight%20Framework%20for%20High-Quality%20Code%20Generation&entry.906535625=Mohammed%20Latif%20Siddiq%20and%20Beatrice%20Casey%20and%20Joanna%20C.%20S.%20Santos&entry.1292438233=%20%20In%20recent%20years%2C%20the%20use%20of%20automated%20source%20code%20generation%20utilizing%0Atransformer-based%20generative%20models%20has%20expanded%2C%20and%20these%20models%20can%20generate%0Afunctional%20code%20according%20to%20the%20requirements%20of%20the%20developers.%20However%2C%0Arecent%20research%20revealed%20that%20these%20automatically%20generated%20source%20codes%20can%0Acontain%20vulnerabilities%20and%20other%20quality%20issues.%20Despite%20researchers%27%20and%0Apractitioners%27%20attempts%20to%20enhance%20code%20generation%20models%2C%20retraining%20and%0Afine-tuning%20large%20language%20models%20is%20time-consuming%20and%20resource-intensive.%0AThus%2C%20we%20describe%20FRANC%2C%20a%20lightweight%20framework%20for%20recommending%20more%20secure%0Aand%20high-quality%20source%20code%20derived%20from%20transformer-based%20code%20generation%0Amodels.%20FRANC%20includes%20a%20static%20filter%20to%20make%20the%20generated%20code%20compilable%0Awith%20heuristics%20and%20a%20quality-aware%20ranker%20to%20sort%20the%20code%20snippets%20based%20on%20a%0Aquality%20score.%20Moreover%2C%20the%20framework%20uses%20prompt%20engineering%20to%20fix%0Apersistent%20quality%20issues.%20We%20evaluated%20the%20framework%20with%20five%20Python%20and%20Java%0Acode%20generation%20models%20and%20six%20prompt%20datasets%2C%20including%20a%20newly%20created%20one%0Ain%20this%20work%20%28SOEval%29.%20The%20static%20filter%20improves%209%25%20to%2046%25%20Java%20suggestions%0Aand%2010%25%20to%2043%25%20Python%20suggestions%20regarding%20compilability.%20The%20average%0Aimprovement%20over%20the%20NDCG%4010%20score%20for%20the%20ranking%20system%20is%200.0763%2C%20and%20the%0Arepairing%20techniques%20repair%20the%20highest%2080%25%20of%20prompts.%20FRANC%20takes%2C%20on%0Aaverage%2C%201.98%20seconds%20for%20Java%3B%20for%20Python%2C%20it%20takes%200.08%20seconds.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.08220v2&entry.124074799=Read"},
{"title": "DeMoBot: Deformable Mobile Manipulation with Vision-based Sub-goal\n  Retrieval", "author": "Yuying Zhang and Wenyan Yang and Joni Pajarinen", "abstract": "  Imitation learning (IL) algorithms typically distill experience into\nparametric behavior policies to mimic expert demonstrations. Despite their\neffectiveness, previous methods often struggle with data efficiency and\naccurately aligning the current state with expert demonstrations, especially in\ndeformable mobile manipulation tasks characterized by partial observations and\ndynamic object deformations. In this paper, we introduce \\textbf{DeMoBot}, a\nnovel IL approach that directly retrieves observations from demonstrations to\nguide robots in \\textbf{De}formable \\textbf{Mo}bile manipulation tasks. DeMoBot\nutilizes vision foundation models to identify relevant expert data based on\nvisual similarity and matches the current trajectory with demonstrated\ntrajectories using trajectory similarity and forward reachability constraints\nto select suitable sub-goals. Once a goal is determined, a motion generation\npolicy will guide the robot to the next state until the task is completed. We\nevaluated DeMoBot using a Spot robot in several simulated and real-world\nsettings, demonstrating its effectiveness and generalizability. With only 20\ndemonstrations, DeMoBot significantly outperforms the baselines, reaching a\n50\\% success rate in curtain opening and 85\\% in gap covering in simulation.\n", "link": "http://arxiv.org/abs/2408.15919v1", "date": "2024-08-28", "relevancy": 1.7317, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6126}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6062}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeMoBot%3A%20Deformable%20Mobile%20Manipulation%20with%20Vision-based%20Sub-goal%0A%20%20Retrieval&body=Title%3A%20DeMoBot%3A%20Deformable%20Mobile%20Manipulation%20with%20Vision-based%20Sub-goal%0A%20%20Retrieval%0AAuthor%3A%20Yuying%20Zhang%20and%20Wenyan%20Yang%20and%20Joni%20Pajarinen%0AAbstract%3A%20%20%20Imitation%20learning%20%28IL%29%20algorithms%20typically%20distill%20experience%20into%0Aparametric%20behavior%20policies%20to%20mimic%20expert%20demonstrations.%20Despite%20their%0Aeffectiveness%2C%20previous%20methods%20often%20struggle%20with%20data%20efficiency%20and%0Aaccurately%20aligning%20the%20current%20state%20with%20expert%20demonstrations%2C%20especially%20in%0Adeformable%20mobile%20manipulation%20tasks%20characterized%20by%20partial%20observations%20and%0Adynamic%20object%20deformations.%20In%20this%20paper%2C%20we%20introduce%20%5Ctextbf%7BDeMoBot%7D%2C%20a%0Anovel%20IL%20approach%20that%20directly%20retrieves%20observations%20from%20demonstrations%20to%0Aguide%20robots%20in%20%5Ctextbf%7BDe%7Dformable%20%5Ctextbf%7BMo%7Dbile%20manipulation%20tasks.%20DeMoBot%0Autilizes%20vision%20foundation%20models%20to%20identify%20relevant%20expert%20data%20based%20on%0Avisual%20similarity%20and%20matches%20the%20current%20trajectory%20with%20demonstrated%0Atrajectories%20using%20trajectory%20similarity%20and%20forward%20reachability%20constraints%0Ato%20select%20suitable%20sub-goals.%20Once%20a%20goal%20is%20determined%2C%20a%20motion%20generation%0Apolicy%20will%20guide%20the%20robot%20to%20the%20next%20state%20until%20the%20task%20is%20completed.%20We%0Aevaluated%20DeMoBot%20using%20a%20Spot%20robot%20in%20several%20simulated%20and%20real-world%0Asettings%2C%20demonstrating%20its%20effectiveness%20and%20generalizability.%20With%20only%2020%0Ademonstrations%2C%20DeMoBot%20significantly%20outperforms%20the%20baselines%2C%20reaching%20a%0A50%5C%25%20success%20rate%20in%20curtain%20opening%20and%2085%5C%25%20in%20gap%20covering%20in%20simulation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeMoBot%253A%2520Deformable%2520Mobile%2520Manipulation%2520with%2520Vision-based%2520Sub-goal%250A%2520%2520Retrieval%26entry.906535625%3DYuying%2520Zhang%2520and%2520Wenyan%2520Yang%2520and%2520Joni%2520Pajarinen%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520%2528IL%2529%2520algorithms%2520typically%2520distill%2520experience%2520into%250Aparametric%2520behavior%2520policies%2520to%2520mimic%2520expert%2520demonstrations.%2520Despite%2520their%250Aeffectiveness%252C%2520previous%2520methods%2520often%2520struggle%2520with%2520data%2520efficiency%2520and%250Aaccurately%2520aligning%2520the%2520current%2520state%2520with%2520expert%2520demonstrations%252C%2520especially%2520in%250Adeformable%2520mobile%2520manipulation%2520tasks%2520characterized%2520by%2520partial%2520observations%2520and%250Adynamic%2520object%2520deformations.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520%255Ctextbf%257BDeMoBot%257D%252C%2520a%250Anovel%2520IL%2520approach%2520that%2520directly%2520retrieves%2520observations%2520from%2520demonstrations%2520to%250Aguide%2520robots%2520in%2520%255Ctextbf%257BDe%257Dformable%2520%255Ctextbf%257BMo%257Dbile%2520manipulation%2520tasks.%2520DeMoBot%250Autilizes%2520vision%2520foundation%2520models%2520to%2520identify%2520relevant%2520expert%2520data%2520based%2520on%250Avisual%2520similarity%2520and%2520matches%2520the%2520current%2520trajectory%2520with%2520demonstrated%250Atrajectories%2520using%2520trajectory%2520similarity%2520and%2520forward%2520reachability%2520constraints%250Ato%2520select%2520suitable%2520sub-goals.%2520Once%2520a%2520goal%2520is%2520determined%252C%2520a%2520motion%2520generation%250Apolicy%2520will%2520guide%2520the%2520robot%2520to%2520the%2520next%2520state%2520until%2520the%2520task%2520is%2520completed.%2520We%250Aevaluated%2520DeMoBot%2520using%2520a%2520Spot%2520robot%2520in%2520several%2520simulated%2520and%2520real-world%250Asettings%252C%2520demonstrating%2520its%2520effectiveness%2520and%2520generalizability.%2520With%2520only%252020%250Ademonstrations%252C%2520DeMoBot%2520significantly%2520outperforms%2520the%2520baselines%252C%2520reaching%2520a%250A50%255C%2525%2520success%2520rate%2520in%2520curtain%2520opening%2520and%252085%255C%2525%2520in%2520gap%2520covering%2520in%2520simulation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeMoBot%3A%20Deformable%20Mobile%20Manipulation%20with%20Vision-based%20Sub-goal%0A%20%20Retrieval&entry.906535625=Yuying%20Zhang%20and%20Wenyan%20Yang%20and%20Joni%20Pajarinen&entry.1292438233=%20%20Imitation%20learning%20%28IL%29%20algorithms%20typically%20distill%20experience%20into%0Aparametric%20behavior%20policies%20to%20mimic%20expert%20demonstrations.%20Despite%20their%0Aeffectiveness%2C%20previous%20methods%20often%20struggle%20with%20data%20efficiency%20and%0Aaccurately%20aligning%20the%20current%20state%20with%20expert%20demonstrations%2C%20especially%20in%0Adeformable%20mobile%20manipulation%20tasks%20characterized%20by%20partial%20observations%20and%0Adynamic%20object%20deformations.%20In%20this%20paper%2C%20we%20introduce%20%5Ctextbf%7BDeMoBot%7D%2C%20a%0Anovel%20IL%20approach%20that%20directly%20retrieves%20observations%20from%20demonstrations%20to%0Aguide%20robots%20in%20%5Ctextbf%7BDe%7Dformable%20%5Ctextbf%7BMo%7Dbile%20manipulation%20tasks.%20DeMoBot%0Autilizes%20vision%20foundation%20models%20to%20identify%20relevant%20expert%20data%20based%20on%0Avisual%20similarity%20and%20matches%20the%20current%20trajectory%20with%20demonstrated%0Atrajectories%20using%20trajectory%20similarity%20and%20forward%20reachability%20constraints%0Ato%20select%20suitable%20sub-goals.%20Once%20a%20goal%20is%20determined%2C%20a%20motion%20generation%0Apolicy%20will%20guide%20the%20robot%20to%20the%20next%20state%20until%20the%20task%20is%20completed.%20We%0Aevaluated%20DeMoBot%20using%20a%20Spot%20robot%20in%20several%20simulated%20and%20real-world%0Asettings%2C%20demonstrating%20its%20effectiveness%20and%20generalizability.%20With%20only%2020%0Ademonstrations%2C%20DeMoBot%20significantly%20outperforms%20the%20baselines%2C%20reaching%20a%0A50%5C%25%20success%20rate%20in%20curtain%20opening%20and%2085%5C%25%20in%20gap%20covering%20in%20simulation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15919v1&entry.124074799=Read"},
{"title": "The Fault in our Stars: Quality Assessment of Code Generation Benchmarks", "author": "Mohammed Latif Siddiq and Simantika Dristi and Joy Saha and Joanna C. S. Santos", "abstract": "  Large Language Models (LLMs) are gaining popularity among software engineers.\nA crucial aspect of developing effective code generation LLMs is to evaluate\nthese models using a robust benchmark. Evaluation benchmarks with quality\nissues can provide a false sense of performance. In this work, we conduct the\nfirst-of-its-kind study of the quality of prompts within benchmarks used to\ncompare the performance of different code generation models. To conduct this\nstudy, we analyzed 3,566 prompts from 9 code generation benchmarks to identify\nquality issues in them. We also investigated whether fixing the identified\nquality issues in the benchmarks' prompts affects a model's performance. We\nalso studied memorization issues of the evaluation dataset, which can put into\nquestion a benchmark's trustworthiness. We found that code generation\nevaluation benchmarks mainly focused on Python and coding exercises and had\nvery limited contextual dependencies to challenge the model. These datasets and\nthe developers' prompts suffer from quality issues like spelling and\ngrammatical errors, unclear sentences to express developers' intent, and not\nusing proper documentation style. Fixing all these issues in the benchmarks can\nlead to a better performance for Python code generation, but not a significant\nimprovement was observed for Java code generation. We also found evidence that\nGPT-3.5-Turbo and CodeGen-2.5 models may have data contamination issues.\n", "link": "http://arxiv.org/abs/2404.10155v2", "date": "2024-08-28", "relevancy": 1.726, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4424}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4276}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Fault%20in%20our%20Stars%3A%20Quality%20Assessment%20of%20Code%20Generation%20Benchmarks&body=Title%3A%20The%20Fault%20in%20our%20Stars%3A%20Quality%20Assessment%20of%20Code%20Generation%20Benchmarks%0AAuthor%3A%20Mohammed%20Latif%20Siddiq%20and%20Simantika%20Dristi%20and%20Joy%20Saha%20and%20Joanna%20C.%20S.%20Santos%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20gaining%20popularity%20among%20software%20engineers.%0AA%20crucial%20aspect%20of%20developing%20effective%20code%20generation%20LLMs%20is%20to%20evaluate%0Athese%20models%20using%20a%20robust%20benchmark.%20Evaluation%20benchmarks%20with%20quality%0Aissues%20can%20provide%20a%20false%20sense%20of%20performance.%20In%20this%20work%2C%20we%20conduct%20the%0Afirst-of-its-kind%20study%20of%20the%20quality%20of%20prompts%20within%20benchmarks%20used%20to%0Acompare%20the%20performance%20of%20different%20code%20generation%20models.%20To%20conduct%20this%0Astudy%2C%20we%20analyzed%203%2C566%20prompts%20from%209%20code%20generation%20benchmarks%20to%20identify%0Aquality%20issues%20in%20them.%20We%20also%20investigated%20whether%20fixing%20the%20identified%0Aquality%20issues%20in%20the%20benchmarks%27%20prompts%20affects%20a%20model%27s%20performance.%20We%0Aalso%20studied%20memorization%20issues%20of%20the%20evaluation%20dataset%2C%20which%20can%20put%20into%0Aquestion%20a%20benchmark%27s%20trustworthiness.%20We%20found%20that%20code%20generation%0Aevaluation%20benchmarks%20mainly%20focused%20on%20Python%20and%20coding%20exercises%20and%20had%0Avery%20limited%20contextual%20dependencies%20to%20challenge%20the%20model.%20These%20datasets%20and%0Athe%20developers%27%20prompts%20suffer%20from%20quality%20issues%20like%20spelling%20and%0Agrammatical%20errors%2C%20unclear%20sentences%20to%20express%20developers%27%20intent%2C%20and%20not%0Ausing%20proper%20documentation%20style.%20Fixing%20all%20these%20issues%20in%20the%20benchmarks%20can%0Alead%20to%20a%20better%20performance%20for%20Python%20code%20generation%2C%20but%20not%20a%20significant%0Aimprovement%20was%20observed%20for%20Java%20code%20generation.%20We%20also%20found%20evidence%20that%0AGPT-3.5-Turbo%20and%20CodeGen-2.5%20models%20may%20have%20data%20contamination%20issues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10155v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Fault%2520in%2520our%2520Stars%253A%2520Quality%2520Assessment%2520of%2520Code%2520Generation%2520Benchmarks%26entry.906535625%3DMohammed%2520Latif%2520Siddiq%2520and%2520Simantika%2520Dristi%2520and%2520Joy%2520Saha%2520and%2520Joanna%2520C.%2520S.%2520Santos%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520gaining%2520popularity%2520among%2520software%2520engineers.%250AA%2520crucial%2520aspect%2520of%2520developing%2520effective%2520code%2520generation%2520LLMs%2520is%2520to%2520evaluate%250Athese%2520models%2520using%2520a%2520robust%2520benchmark.%2520Evaluation%2520benchmarks%2520with%2520quality%250Aissues%2520can%2520provide%2520a%2520false%2520sense%2520of%2520performance.%2520In%2520this%2520work%252C%2520we%2520conduct%2520the%250Afirst-of-its-kind%2520study%2520of%2520the%2520quality%2520of%2520prompts%2520within%2520benchmarks%2520used%2520to%250Acompare%2520the%2520performance%2520of%2520different%2520code%2520generation%2520models.%2520To%2520conduct%2520this%250Astudy%252C%2520we%2520analyzed%25203%252C566%2520prompts%2520from%25209%2520code%2520generation%2520benchmarks%2520to%2520identify%250Aquality%2520issues%2520in%2520them.%2520We%2520also%2520investigated%2520whether%2520fixing%2520the%2520identified%250Aquality%2520issues%2520in%2520the%2520benchmarks%2527%2520prompts%2520affects%2520a%2520model%2527s%2520performance.%2520We%250Aalso%2520studied%2520memorization%2520issues%2520of%2520the%2520evaluation%2520dataset%252C%2520which%2520can%2520put%2520into%250Aquestion%2520a%2520benchmark%2527s%2520trustworthiness.%2520We%2520found%2520that%2520code%2520generation%250Aevaluation%2520benchmarks%2520mainly%2520focused%2520on%2520Python%2520and%2520coding%2520exercises%2520and%2520had%250Avery%2520limited%2520contextual%2520dependencies%2520to%2520challenge%2520the%2520model.%2520These%2520datasets%2520and%250Athe%2520developers%2527%2520prompts%2520suffer%2520from%2520quality%2520issues%2520like%2520spelling%2520and%250Agrammatical%2520errors%252C%2520unclear%2520sentences%2520to%2520express%2520developers%2527%2520intent%252C%2520and%2520not%250Ausing%2520proper%2520documentation%2520style.%2520Fixing%2520all%2520these%2520issues%2520in%2520the%2520benchmarks%2520can%250Alead%2520to%2520a%2520better%2520performance%2520for%2520Python%2520code%2520generation%252C%2520but%2520not%2520a%2520significant%250Aimprovement%2520was%2520observed%2520for%2520Java%2520code%2520generation.%2520We%2520also%2520found%2520evidence%2520that%250AGPT-3.5-Turbo%2520and%2520CodeGen-2.5%2520models%2520may%2520have%2520data%2520contamination%2520issues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10155v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Fault%20in%20our%20Stars%3A%20Quality%20Assessment%20of%20Code%20Generation%20Benchmarks&entry.906535625=Mohammed%20Latif%20Siddiq%20and%20Simantika%20Dristi%20and%20Joy%20Saha%20and%20Joanna%20C.%20S.%20Santos&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20gaining%20popularity%20among%20software%20engineers.%0AA%20crucial%20aspect%20of%20developing%20effective%20code%20generation%20LLMs%20is%20to%20evaluate%0Athese%20models%20using%20a%20robust%20benchmark.%20Evaluation%20benchmarks%20with%20quality%0Aissues%20can%20provide%20a%20false%20sense%20of%20performance.%20In%20this%20work%2C%20we%20conduct%20the%0Afirst-of-its-kind%20study%20of%20the%20quality%20of%20prompts%20within%20benchmarks%20used%20to%0Acompare%20the%20performance%20of%20different%20code%20generation%20models.%20To%20conduct%20this%0Astudy%2C%20we%20analyzed%203%2C566%20prompts%20from%209%20code%20generation%20benchmarks%20to%20identify%0Aquality%20issues%20in%20them.%20We%20also%20investigated%20whether%20fixing%20the%20identified%0Aquality%20issues%20in%20the%20benchmarks%27%20prompts%20affects%20a%20model%27s%20performance.%20We%0Aalso%20studied%20memorization%20issues%20of%20the%20evaluation%20dataset%2C%20which%20can%20put%20into%0Aquestion%20a%20benchmark%27s%20trustworthiness.%20We%20found%20that%20code%20generation%0Aevaluation%20benchmarks%20mainly%20focused%20on%20Python%20and%20coding%20exercises%20and%20had%0Avery%20limited%20contextual%20dependencies%20to%20challenge%20the%20model.%20These%20datasets%20and%0Athe%20developers%27%20prompts%20suffer%20from%20quality%20issues%20like%20spelling%20and%0Agrammatical%20errors%2C%20unclear%20sentences%20to%20express%20developers%27%20intent%2C%20and%20not%0Ausing%20proper%20documentation%20style.%20Fixing%20all%20these%20issues%20in%20the%20benchmarks%20can%0Alead%20to%20a%20better%20performance%20for%20Python%20code%20generation%2C%20but%20not%20a%20significant%0Aimprovement%20was%20observed%20for%20Java%20code%20generation.%20We%20also%20found%20evidence%20that%0AGPT-3.5-Turbo%20and%20CodeGen-2.5%20models%20may%20have%20data%20contamination%20issues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10155v2&entry.124074799=Read"},
{"title": "DeepDelveAI: Identifying AI Related Documents in Large Scale Literature\n  Data", "author": "Zhou Xiaochen and Liang Xingzhou and Zou Hui and Lu Yi and Qu Jingjing", "abstract": "  This paper presents DeepDelveAI, a comprehensive dataset specifically curated\nto identify AI-related research papers from a large-scale academic literature\ndatabase. The dataset was created using an advanced Long Short-Term Memory\n(LSTM) model trained on a binary classification task to distinguish between\nAI-related and non-AI-related papers. The model was trained and validated on a\nvast dataset, achieving high accuracy, precision, recall, and F1-score. The\nresulting DeepDelveAI dataset comprises over 9.4 million AI-related papers\npublished since Dartmouth Conference, from 1956 to 2024, providing a crucial\nresource for analyzing trends, thematic developments, and the evolution of AI\nresearch across various disciplines.\n", "link": "http://arxiv.org/abs/2408.12871v2", "date": "2024-08-28", "relevancy": 1.7223, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4844}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4223}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepDelveAI%3A%20Identifying%20AI%20Related%20Documents%20in%20Large%20Scale%20Literature%0A%20%20Data&body=Title%3A%20DeepDelveAI%3A%20Identifying%20AI%20Related%20Documents%20in%20Large%20Scale%20Literature%0A%20%20Data%0AAuthor%3A%20Zhou%20Xiaochen%20and%20Liang%20Xingzhou%20and%20Zou%20Hui%20and%20Lu%20Yi%20and%20Qu%20Jingjing%0AAbstract%3A%20%20%20This%20paper%20presents%20DeepDelveAI%2C%20a%20comprehensive%20dataset%20specifically%20curated%0Ato%20identify%20AI-related%20research%20papers%20from%20a%20large-scale%20academic%20literature%0Adatabase.%20The%20dataset%20was%20created%20using%20an%20advanced%20Long%20Short-Term%20Memory%0A%28LSTM%29%20model%20trained%20on%20a%20binary%20classification%20task%20to%20distinguish%20between%0AAI-related%20and%20non-AI-related%20papers.%20The%20model%20was%20trained%20and%20validated%20on%20a%0Avast%20dataset%2C%20achieving%20high%20accuracy%2C%20precision%2C%20recall%2C%20and%20F1-score.%20The%0Aresulting%20DeepDelveAI%20dataset%20comprises%20over%209.4%20million%20AI-related%20papers%0Apublished%20since%20Dartmouth%20Conference%2C%20from%201956%20to%202024%2C%20providing%20a%20crucial%0Aresource%20for%20analyzing%20trends%2C%20thematic%20developments%2C%20and%20the%20evolution%20of%20AI%0Aresearch%20across%20various%20disciplines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12871v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepDelveAI%253A%2520Identifying%2520AI%2520Related%2520Documents%2520in%2520Large%2520Scale%2520Literature%250A%2520%2520Data%26entry.906535625%3DZhou%2520Xiaochen%2520and%2520Liang%2520Xingzhou%2520and%2520Zou%2520Hui%2520and%2520Lu%2520Yi%2520and%2520Qu%2520Jingjing%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520DeepDelveAI%252C%2520a%2520comprehensive%2520dataset%2520specifically%2520curated%250Ato%2520identify%2520AI-related%2520research%2520papers%2520from%2520a%2520large-scale%2520academic%2520literature%250Adatabase.%2520The%2520dataset%2520was%2520created%2520using%2520an%2520advanced%2520Long%2520Short-Term%2520Memory%250A%2528LSTM%2529%2520model%2520trained%2520on%2520a%2520binary%2520classification%2520task%2520to%2520distinguish%2520between%250AAI-related%2520and%2520non-AI-related%2520papers.%2520The%2520model%2520was%2520trained%2520and%2520validated%2520on%2520a%250Avast%2520dataset%252C%2520achieving%2520high%2520accuracy%252C%2520precision%252C%2520recall%252C%2520and%2520F1-score.%2520The%250Aresulting%2520DeepDelveAI%2520dataset%2520comprises%2520over%25209.4%2520million%2520AI-related%2520papers%250Apublished%2520since%2520Dartmouth%2520Conference%252C%2520from%25201956%2520to%25202024%252C%2520providing%2520a%2520crucial%250Aresource%2520for%2520analyzing%2520trends%252C%2520thematic%2520developments%252C%2520and%2520the%2520evolution%2520of%2520AI%250Aresearch%2520across%2520various%2520disciplines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12871v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepDelveAI%3A%20Identifying%20AI%20Related%20Documents%20in%20Large%20Scale%20Literature%0A%20%20Data&entry.906535625=Zhou%20Xiaochen%20and%20Liang%20Xingzhou%20and%20Zou%20Hui%20and%20Lu%20Yi%20and%20Qu%20Jingjing&entry.1292438233=%20%20This%20paper%20presents%20DeepDelveAI%2C%20a%20comprehensive%20dataset%20specifically%20curated%0Ato%20identify%20AI-related%20research%20papers%20from%20a%20large-scale%20academic%20literature%0Adatabase.%20The%20dataset%20was%20created%20using%20an%20advanced%20Long%20Short-Term%20Memory%0A%28LSTM%29%20model%20trained%20on%20a%20binary%20classification%20task%20to%20distinguish%20between%0AAI-related%20and%20non-AI-related%20papers.%20The%20model%20was%20trained%20and%20validated%20on%20a%0Avast%20dataset%2C%20achieving%20high%20accuracy%2C%20precision%2C%20recall%2C%20and%20F1-score.%20The%0Aresulting%20DeepDelveAI%20dataset%20comprises%20over%209.4%20million%20AI-related%20papers%0Apublished%20since%20Dartmouth%20Conference%2C%20from%201956%20to%202024%2C%20providing%20a%20crucial%0Aresource%20for%20analyzing%20trends%2C%20thematic%20developments%2C%20and%20the%20evolution%20of%20AI%0Aresearch%20across%20various%20disciplines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12871v2&entry.124074799=Read"},
{"title": "From Complexity to Clarity: How AI Enhances Perceptions of Scientists\n  and the Public's Understanding of Science", "author": "David M. Markowitz", "abstract": "  This paper evaluated the effectiveness of using generative AI to simplify\nscience communication and enhance the public's understanding of science. By\ncomparing lay summaries of journal articles from PNAS, yoked to those generated\nby AI, this work first assessed linguistic simplicity differences across such\nsummaries and public perceptions in follow-up experiments. Specifically, Study\n1a analyzed simplicity features of PNAS abstracts (scientific summaries) and\nsignificance statements (lay summaries), observing that lay summaries were\nindeed linguistically simpler, but effect size differences were small. Study 1b\nused a large language model, GPT-4, to create significance statements based on\npaper abstracts and this more than doubled the average effect size without\nfine-tuning. Study 2 experimentally demonstrated that simply-written GPT\nsummaries facilitated more favorable perceptions of scientists (they were\nperceived as more credible and trustworthy, but less intelligent) than more\ncomplexly-written human PNAS summaries. Crucially, Study 3 experimentally\ndemonstrated that participants comprehended scientific writing better after\nreading simple GPT summaries compared to complex PNAS summaries. In their own\nwords, participants also summarized scientific papers in a more detailed and\nconcrete manner after reading GPT summaries compared to PNAS summaries of the\nsame article. AI has the potential to engage scientific communities and the\npublic via a simple language heuristic, advocating for its integration into\nscientific dissemination for a more informed society.\n", "link": "http://arxiv.org/abs/2405.00706v3", "date": "2024-08-28", "relevancy": 1.7164, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4339}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4263}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4254}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Complexity%20to%20Clarity%3A%20How%20AI%20Enhances%20Perceptions%20of%20Scientists%0A%20%20and%20the%20Public%27s%20Understanding%20of%20Science&body=Title%3A%20From%20Complexity%20to%20Clarity%3A%20How%20AI%20Enhances%20Perceptions%20of%20Scientists%0A%20%20and%20the%20Public%27s%20Understanding%20of%20Science%0AAuthor%3A%20David%20M.%20Markowitz%0AAbstract%3A%20%20%20This%20paper%20evaluated%20the%20effectiveness%20of%20using%20generative%20AI%20to%20simplify%0Ascience%20communication%20and%20enhance%20the%20public%27s%20understanding%20of%20science.%20By%0Acomparing%20lay%20summaries%20of%20journal%20articles%20from%20PNAS%2C%20yoked%20to%20those%20generated%0Aby%20AI%2C%20this%20work%20first%20assessed%20linguistic%20simplicity%20differences%20across%20such%0Asummaries%20and%20public%20perceptions%20in%20follow-up%20experiments.%20Specifically%2C%20Study%0A1a%20analyzed%20simplicity%20features%20of%20PNAS%20abstracts%20%28scientific%20summaries%29%20and%0Asignificance%20statements%20%28lay%20summaries%29%2C%20observing%20that%20lay%20summaries%20were%0Aindeed%20linguistically%20simpler%2C%20but%20effect%20size%20differences%20were%20small.%20Study%201b%0Aused%20a%20large%20language%20model%2C%20GPT-4%2C%20to%20create%20significance%20statements%20based%20on%0Apaper%20abstracts%20and%20this%20more%20than%20doubled%20the%20average%20effect%20size%20without%0Afine-tuning.%20Study%202%20experimentally%20demonstrated%20that%20simply-written%20GPT%0Asummaries%20facilitated%20more%20favorable%20perceptions%20of%20scientists%20%28they%20were%0Aperceived%20as%20more%20credible%20and%20trustworthy%2C%20but%20less%20intelligent%29%20than%20more%0Acomplexly-written%20human%20PNAS%20summaries.%20Crucially%2C%20Study%203%20experimentally%0Ademonstrated%20that%20participants%20comprehended%20scientific%20writing%20better%20after%0Areading%20simple%20GPT%20summaries%20compared%20to%20complex%20PNAS%20summaries.%20In%20their%20own%0Awords%2C%20participants%20also%20summarized%20scientific%20papers%20in%20a%20more%20detailed%20and%0Aconcrete%20manner%20after%20reading%20GPT%20summaries%20compared%20to%20PNAS%20summaries%20of%20the%0Asame%20article.%20AI%20has%20the%20potential%20to%20engage%20scientific%20communities%20and%20the%0Apublic%20via%20a%20simple%20language%20heuristic%2C%20advocating%20for%20its%20integration%20into%0Ascientific%20dissemination%20for%20a%20more%20informed%20society.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00706v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Complexity%2520to%2520Clarity%253A%2520How%2520AI%2520Enhances%2520Perceptions%2520of%2520Scientists%250A%2520%2520and%2520the%2520Public%2527s%2520Understanding%2520of%2520Science%26entry.906535625%3DDavid%2520M.%2520Markowitz%26entry.1292438233%3D%2520%2520This%2520paper%2520evaluated%2520the%2520effectiveness%2520of%2520using%2520generative%2520AI%2520to%2520simplify%250Ascience%2520communication%2520and%2520enhance%2520the%2520public%2527s%2520understanding%2520of%2520science.%2520By%250Acomparing%2520lay%2520summaries%2520of%2520journal%2520articles%2520from%2520PNAS%252C%2520yoked%2520to%2520those%2520generated%250Aby%2520AI%252C%2520this%2520work%2520first%2520assessed%2520linguistic%2520simplicity%2520differences%2520across%2520such%250Asummaries%2520and%2520public%2520perceptions%2520in%2520follow-up%2520experiments.%2520Specifically%252C%2520Study%250A1a%2520analyzed%2520simplicity%2520features%2520of%2520PNAS%2520abstracts%2520%2528scientific%2520summaries%2529%2520and%250Asignificance%2520statements%2520%2528lay%2520summaries%2529%252C%2520observing%2520that%2520lay%2520summaries%2520were%250Aindeed%2520linguistically%2520simpler%252C%2520but%2520effect%2520size%2520differences%2520were%2520small.%2520Study%25201b%250Aused%2520a%2520large%2520language%2520model%252C%2520GPT-4%252C%2520to%2520create%2520significance%2520statements%2520based%2520on%250Apaper%2520abstracts%2520and%2520this%2520more%2520than%2520doubled%2520the%2520average%2520effect%2520size%2520without%250Afine-tuning.%2520Study%25202%2520experimentally%2520demonstrated%2520that%2520simply-written%2520GPT%250Asummaries%2520facilitated%2520more%2520favorable%2520perceptions%2520of%2520scientists%2520%2528they%2520were%250Aperceived%2520as%2520more%2520credible%2520and%2520trustworthy%252C%2520but%2520less%2520intelligent%2529%2520than%2520more%250Acomplexly-written%2520human%2520PNAS%2520summaries.%2520Crucially%252C%2520Study%25203%2520experimentally%250Ademonstrated%2520that%2520participants%2520comprehended%2520scientific%2520writing%2520better%2520after%250Areading%2520simple%2520GPT%2520summaries%2520compared%2520to%2520complex%2520PNAS%2520summaries.%2520In%2520their%2520own%250Awords%252C%2520participants%2520also%2520summarized%2520scientific%2520papers%2520in%2520a%2520more%2520detailed%2520and%250Aconcrete%2520manner%2520after%2520reading%2520GPT%2520summaries%2520compared%2520to%2520PNAS%2520summaries%2520of%2520the%250Asame%2520article.%2520AI%2520has%2520the%2520potential%2520to%2520engage%2520scientific%2520communities%2520and%2520the%250Apublic%2520via%2520a%2520simple%2520language%2520heuristic%252C%2520advocating%2520for%2520its%2520integration%2520into%250Ascientific%2520dissemination%2520for%2520a%2520more%2520informed%2520society.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00706v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Complexity%20to%20Clarity%3A%20How%20AI%20Enhances%20Perceptions%20of%20Scientists%0A%20%20and%20the%20Public%27s%20Understanding%20of%20Science&entry.906535625=David%20M.%20Markowitz&entry.1292438233=%20%20This%20paper%20evaluated%20the%20effectiveness%20of%20using%20generative%20AI%20to%20simplify%0Ascience%20communication%20and%20enhance%20the%20public%27s%20understanding%20of%20science.%20By%0Acomparing%20lay%20summaries%20of%20journal%20articles%20from%20PNAS%2C%20yoked%20to%20those%20generated%0Aby%20AI%2C%20this%20work%20first%20assessed%20linguistic%20simplicity%20differences%20across%20such%0Asummaries%20and%20public%20perceptions%20in%20follow-up%20experiments.%20Specifically%2C%20Study%0A1a%20analyzed%20simplicity%20features%20of%20PNAS%20abstracts%20%28scientific%20summaries%29%20and%0Asignificance%20statements%20%28lay%20summaries%29%2C%20observing%20that%20lay%20summaries%20were%0Aindeed%20linguistically%20simpler%2C%20but%20effect%20size%20differences%20were%20small.%20Study%201b%0Aused%20a%20large%20language%20model%2C%20GPT-4%2C%20to%20create%20significance%20statements%20based%20on%0Apaper%20abstracts%20and%20this%20more%20than%20doubled%20the%20average%20effect%20size%20without%0Afine-tuning.%20Study%202%20experimentally%20demonstrated%20that%20simply-written%20GPT%0Asummaries%20facilitated%20more%20favorable%20perceptions%20of%20scientists%20%28they%20were%0Aperceived%20as%20more%20credible%20and%20trustworthy%2C%20but%20less%20intelligent%29%20than%20more%0Acomplexly-written%20human%20PNAS%20summaries.%20Crucially%2C%20Study%203%20experimentally%0Ademonstrated%20that%20participants%20comprehended%20scientific%20writing%20better%20after%0Areading%20simple%20GPT%20summaries%20compared%20to%20complex%20PNAS%20summaries.%20In%20their%20own%0Awords%2C%20participants%20also%20summarized%20scientific%20papers%20in%20a%20more%20detailed%20and%0Aconcrete%20manner%20after%20reading%20GPT%20summaries%20compared%20to%20PNAS%20summaries%20of%20the%0Asame%20article.%20AI%20has%20the%20potential%20to%20engage%20scientific%20communities%20and%20the%0Apublic%20via%20a%20simple%20language%20heuristic%2C%20advocating%20for%20its%20integration%20into%0Ascientific%20dissemination%20for%20a%20more%20informed%20society.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00706v3&entry.124074799=Read"},
{"title": "Explicit Modelling of Theory of Mind for Belief Prediction in Nonverbal\n  Social Interactions", "author": "Matteo Bortoletto and Constantin Ruhdorfer and Lei Shi and Andreas Bulling", "abstract": "  We propose MToMnet - a Theory of Mind (ToM) neural network for predicting\nbeliefs and their dynamics during human social interactions from multimodal\ninput. ToM is key for effective nonverbal human communication and\ncollaboration, yet, existing methods for belief modelling have not included\nexplicit ToM modelling or have typically been limited to one or two modalities.\nMToMnet encodes contextual cues (scene videos and object locations) and\nintegrates them with person-specific cues (human gaze and body language) in a\nseparate MindNet for each person. Inspired by prior research on social\ncognition and computational ToM, we propose three different MToMnet variants:\ntwo involving fusion of latent representations and one involving re-ranking of\nclassification scores. We evaluate our approach on two challenging real-world\ndatasets, one focusing on belief prediction, while the other examining belief\ndynamics prediction. Our results demonstrate that MToMnet surpasses existing\nmethods by a large margin while at the same time requiring a significantly\nsmaller number of parameters. Taken together, our method opens up a highly\npromising direction for future work on artificial intelligent systems that can\nrobustly predict human beliefs from their non-verbal behaviour and, as such,\nmore effectively collaborate with humans.\n", "link": "http://arxiv.org/abs/2407.06762v3", "date": "2024-08-28", "relevancy": 1.7146, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6251}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5602}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explicit%20Modelling%20of%20Theory%20of%20Mind%20for%20Belief%20Prediction%20in%20Nonverbal%0A%20%20Social%20Interactions&body=Title%3A%20Explicit%20Modelling%20of%20Theory%20of%20Mind%20for%20Belief%20Prediction%20in%20Nonverbal%0A%20%20Social%20Interactions%0AAuthor%3A%20Matteo%20Bortoletto%20and%20Constantin%20Ruhdorfer%20and%20Lei%20Shi%20and%20Andreas%20Bulling%0AAbstract%3A%20%20%20We%20propose%20MToMnet%20-%20a%20Theory%20of%20Mind%20%28ToM%29%20neural%20network%20for%20predicting%0Abeliefs%20and%20their%20dynamics%20during%20human%20social%20interactions%20from%20multimodal%0Ainput.%20ToM%20is%20key%20for%20effective%20nonverbal%20human%20communication%20and%0Acollaboration%2C%20yet%2C%20existing%20methods%20for%20belief%20modelling%20have%20not%20included%0Aexplicit%20ToM%20modelling%20or%20have%20typically%20been%20limited%20to%20one%20or%20two%20modalities.%0AMToMnet%20encodes%20contextual%20cues%20%28scene%20videos%20and%20object%20locations%29%20and%0Aintegrates%20them%20with%20person-specific%20cues%20%28human%20gaze%20and%20body%20language%29%20in%20a%0Aseparate%20MindNet%20for%20each%20person.%20Inspired%20by%20prior%20research%20on%20social%0Acognition%20and%20computational%20ToM%2C%20we%20propose%20three%20different%20MToMnet%20variants%3A%0Atwo%20involving%20fusion%20of%20latent%20representations%20and%20one%20involving%20re-ranking%20of%0Aclassification%20scores.%20We%20evaluate%20our%20approach%20on%20two%20challenging%20real-world%0Adatasets%2C%20one%20focusing%20on%20belief%20prediction%2C%20while%20the%20other%20examining%20belief%0Adynamics%20prediction.%20Our%20results%20demonstrate%20that%20MToMnet%20surpasses%20existing%0Amethods%20by%20a%20large%20margin%20while%20at%20the%20same%20time%20requiring%20a%20significantly%0Asmaller%20number%20of%20parameters.%20Taken%20together%2C%20our%20method%20opens%20up%20a%20highly%0Apromising%20direction%20for%20future%20work%20on%20artificial%20intelligent%20systems%20that%20can%0Arobustly%20predict%20human%20beliefs%20from%20their%20non-verbal%20behaviour%20and%2C%20as%20such%2C%0Amore%20effectively%20collaborate%20with%20humans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06762v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplicit%2520Modelling%2520of%2520Theory%2520of%2520Mind%2520for%2520Belief%2520Prediction%2520in%2520Nonverbal%250A%2520%2520Social%2520Interactions%26entry.906535625%3DMatteo%2520Bortoletto%2520and%2520Constantin%2520Ruhdorfer%2520and%2520Lei%2520Shi%2520and%2520Andreas%2520Bulling%26entry.1292438233%3D%2520%2520We%2520propose%2520MToMnet%2520-%2520a%2520Theory%2520of%2520Mind%2520%2528ToM%2529%2520neural%2520network%2520for%2520predicting%250Abeliefs%2520and%2520their%2520dynamics%2520during%2520human%2520social%2520interactions%2520from%2520multimodal%250Ainput.%2520ToM%2520is%2520key%2520for%2520effective%2520nonverbal%2520human%2520communication%2520and%250Acollaboration%252C%2520yet%252C%2520existing%2520methods%2520for%2520belief%2520modelling%2520have%2520not%2520included%250Aexplicit%2520ToM%2520modelling%2520or%2520have%2520typically%2520been%2520limited%2520to%2520one%2520or%2520two%2520modalities.%250AMToMnet%2520encodes%2520contextual%2520cues%2520%2528scene%2520videos%2520and%2520object%2520locations%2529%2520and%250Aintegrates%2520them%2520with%2520person-specific%2520cues%2520%2528human%2520gaze%2520and%2520body%2520language%2529%2520in%2520a%250Aseparate%2520MindNet%2520for%2520each%2520person.%2520Inspired%2520by%2520prior%2520research%2520on%2520social%250Acognition%2520and%2520computational%2520ToM%252C%2520we%2520propose%2520three%2520different%2520MToMnet%2520variants%253A%250Atwo%2520involving%2520fusion%2520of%2520latent%2520representations%2520and%2520one%2520involving%2520re-ranking%2520of%250Aclassification%2520scores.%2520We%2520evaluate%2520our%2520approach%2520on%2520two%2520challenging%2520real-world%250Adatasets%252C%2520one%2520focusing%2520on%2520belief%2520prediction%252C%2520while%2520the%2520other%2520examining%2520belief%250Adynamics%2520prediction.%2520Our%2520results%2520demonstrate%2520that%2520MToMnet%2520surpasses%2520existing%250Amethods%2520by%2520a%2520large%2520margin%2520while%2520at%2520the%2520same%2520time%2520requiring%2520a%2520significantly%250Asmaller%2520number%2520of%2520parameters.%2520Taken%2520together%252C%2520our%2520method%2520opens%2520up%2520a%2520highly%250Apromising%2520direction%2520for%2520future%2520work%2520on%2520artificial%2520intelligent%2520systems%2520that%2520can%250Arobustly%2520predict%2520human%2520beliefs%2520from%2520their%2520non-verbal%2520behaviour%2520and%252C%2520as%2520such%252C%250Amore%2520effectively%2520collaborate%2520with%2520humans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06762v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explicit%20Modelling%20of%20Theory%20of%20Mind%20for%20Belief%20Prediction%20in%20Nonverbal%0A%20%20Social%20Interactions&entry.906535625=Matteo%20Bortoletto%20and%20Constantin%20Ruhdorfer%20and%20Lei%20Shi%20and%20Andreas%20Bulling&entry.1292438233=%20%20We%20propose%20MToMnet%20-%20a%20Theory%20of%20Mind%20%28ToM%29%20neural%20network%20for%20predicting%0Abeliefs%20and%20their%20dynamics%20during%20human%20social%20interactions%20from%20multimodal%0Ainput.%20ToM%20is%20key%20for%20effective%20nonverbal%20human%20communication%20and%0Acollaboration%2C%20yet%2C%20existing%20methods%20for%20belief%20modelling%20have%20not%20included%0Aexplicit%20ToM%20modelling%20or%20have%20typically%20been%20limited%20to%20one%20or%20two%20modalities.%0AMToMnet%20encodes%20contextual%20cues%20%28scene%20videos%20and%20object%20locations%29%20and%0Aintegrates%20them%20with%20person-specific%20cues%20%28human%20gaze%20and%20body%20language%29%20in%20a%0Aseparate%20MindNet%20for%20each%20person.%20Inspired%20by%20prior%20research%20on%20social%0Acognition%20and%20computational%20ToM%2C%20we%20propose%20three%20different%20MToMnet%20variants%3A%0Atwo%20involving%20fusion%20of%20latent%20representations%20and%20one%20involving%20re-ranking%20of%0Aclassification%20scores.%20We%20evaluate%20our%20approach%20on%20two%20challenging%20real-world%0Adatasets%2C%20one%20focusing%20on%20belief%20prediction%2C%20while%20the%20other%20examining%20belief%0Adynamics%20prediction.%20Our%20results%20demonstrate%20that%20MToMnet%20surpasses%20existing%0Amethods%20by%20a%20large%20margin%20while%20at%20the%20same%20time%20requiring%20a%20significantly%0Asmaller%20number%20of%20parameters.%20Taken%20together%2C%20our%20method%20opens%20up%20a%20highly%0Apromising%20direction%20for%20future%20work%20on%20artificial%20intelligent%20systems%20that%20can%0Arobustly%20predict%20human%20beliefs%20from%20their%20non-verbal%20behaviour%20and%2C%20as%20such%2C%0Amore%20effectively%20collaborate%20with%20humans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06762v3&entry.124074799=Read"},
{"title": "Evaluating Named Entity Recognition Using Few-Shot Prompting with Large\n  Language Models", "author": "H\u00e9di Zhegidi and Ludovic Moncla", "abstract": "  This paper evaluates Few-Shot Prompting with Large Language Models for Named\nEntity Recognition (NER). Traditional NER systems rely on extensive labeled\ndatasets, which are costly and time-consuming to obtain. Few-Shot Prompting or\nin-context learning enables models to recognize entities with minimal examples.\nWe assess state-of-the-art models like GPT-4 in NER tasks, comparing their\nfew-shot performance to fully supervised benchmarks. Results show that while\nthere is a performance gap, large models excel in adapting to new entity types\nand domains with very limited data. We also explore the effects of prompt\nengineering, guided output format and context length on performance. This study\nunderscores Few-Shot Learning's potential to reduce the need for large labeled\ndatasets, enhancing NER scalability and accessibility.\n", "link": "http://arxiv.org/abs/2408.15796v1", "date": "2024-08-28", "relevancy": 1.7074, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4356}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4355}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Named%20Entity%20Recognition%20Using%20Few-Shot%20Prompting%20with%20Large%0A%20%20Language%20Models&body=Title%3A%20Evaluating%20Named%20Entity%20Recognition%20Using%20Few-Shot%20Prompting%20with%20Large%0A%20%20Language%20Models%0AAuthor%3A%20H%C3%A9di%20Zhegidi%20and%20Ludovic%20Moncla%0AAbstract%3A%20%20%20This%20paper%20evaluates%20Few-Shot%20Prompting%20with%20Large%20Language%20Models%20for%20Named%0AEntity%20Recognition%20%28NER%29.%20Traditional%20NER%20systems%20rely%20on%20extensive%20labeled%0Adatasets%2C%20which%20are%20costly%20and%20time-consuming%20to%20obtain.%20Few-Shot%20Prompting%20or%0Ain-context%20learning%20enables%20models%20to%20recognize%20entities%20with%20minimal%20examples.%0AWe%20assess%20state-of-the-art%20models%20like%20GPT-4%20in%20NER%20tasks%2C%20comparing%20their%0Afew-shot%20performance%20to%20fully%20supervised%20benchmarks.%20Results%20show%20that%20while%0Athere%20is%20a%20performance%20gap%2C%20large%20models%20excel%20in%20adapting%20to%20new%20entity%20types%0Aand%20domains%20with%20very%20limited%20data.%20We%20also%20explore%20the%20effects%20of%20prompt%0Aengineering%2C%20guided%20output%20format%20and%20context%20length%20on%20performance.%20This%20study%0Aunderscores%20Few-Shot%20Learning%27s%20potential%20to%20reduce%20the%20need%20for%20large%20labeled%0Adatasets%2C%20enhancing%20NER%20scalability%20and%20accessibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Named%2520Entity%2520Recognition%2520Using%2520Few-Shot%2520Prompting%2520with%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DH%25C3%25A9di%2520Zhegidi%2520and%2520Ludovic%2520Moncla%26entry.1292438233%3D%2520%2520This%2520paper%2520evaluates%2520Few-Shot%2520Prompting%2520with%2520Large%2520Language%2520Models%2520for%2520Named%250AEntity%2520Recognition%2520%2528NER%2529.%2520Traditional%2520NER%2520systems%2520rely%2520on%2520extensive%2520labeled%250Adatasets%252C%2520which%2520are%2520costly%2520and%2520time-consuming%2520to%2520obtain.%2520Few-Shot%2520Prompting%2520or%250Ain-context%2520learning%2520enables%2520models%2520to%2520recognize%2520entities%2520with%2520minimal%2520examples.%250AWe%2520assess%2520state-of-the-art%2520models%2520like%2520GPT-4%2520in%2520NER%2520tasks%252C%2520comparing%2520their%250Afew-shot%2520performance%2520to%2520fully%2520supervised%2520benchmarks.%2520Results%2520show%2520that%2520while%250Athere%2520is%2520a%2520performance%2520gap%252C%2520large%2520models%2520excel%2520in%2520adapting%2520to%2520new%2520entity%2520types%250Aand%2520domains%2520with%2520very%2520limited%2520data.%2520We%2520also%2520explore%2520the%2520effects%2520of%2520prompt%250Aengineering%252C%2520guided%2520output%2520format%2520and%2520context%2520length%2520on%2520performance.%2520This%2520study%250Aunderscores%2520Few-Shot%2520Learning%2527s%2520potential%2520to%2520reduce%2520the%2520need%2520for%2520large%2520labeled%250Adatasets%252C%2520enhancing%2520NER%2520scalability%2520and%2520accessibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Named%20Entity%20Recognition%20Using%20Few-Shot%20Prompting%20with%20Large%0A%20%20Language%20Models&entry.906535625=H%C3%A9di%20Zhegidi%20and%20Ludovic%20Moncla&entry.1292438233=%20%20This%20paper%20evaluates%20Few-Shot%20Prompting%20with%20Large%20Language%20Models%20for%20Named%0AEntity%20Recognition%20%28NER%29.%20Traditional%20NER%20systems%20rely%20on%20extensive%20labeled%0Adatasets%2C%20which%20are%20costly%20and%20time-consuming%20to%20obtain.%20Few-Shot%20Prompting%20or%0Ain-context%20learning%20enables%20models%20to%20recognize%20entities%20with%20minimal%20examples.%0AWe%20assess%20state-of-the-art%20models%20like%20GPT-4%20in%20NER%20tasks%2C%20comparing%20their%0Afew-shot%20performance%20to%20fully%20supervised%20benchmarks.%20Results%20show%20that%20while%0Athere%20is%20a%20performance%20gap%2C%20large%20models%20excel%20in%20adapting%20to%20new%20entity%20types%0Aand%20domains%20with%20very%20limited%20data.%20We%20also%20explore%20the%20effects%20of%20prompt%0Aengineering%2C%20guided%20output%20format%20and%20context%20length%20on%20performance.%20This%20study%0Aunderscores%20Few-Shot%20Learning%27s%20potential%20to%20reduce%20the%20need%20for%20large%20labeled%0Adatasets%2C%20enhancing%20NER%20scalability%20and%20accessibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15796v1&entry.124074799=Read"},
{"title": "Multi-view Pose Fusion for Occlusion-Aware 3D Human Pose Estimation", "author": "Laura Bragagnolo and Matteo Terreran and Davide Allegro and Stefano Ghidoni", "abstract": "  Robust 3D human pose estimation is crucial to ensure safe and effective\nhuman-robot collaboration. Accurate human perception,however, is particularly\nchallenging in these scenarios due to strong occlusions and limited camera\nviewpoints. Current 3D human pose estimation approaches are rather vulnerable\nin such conditions. In this work we present a novel approach for robust 3D\nhuman pose estimation in the context of human-robot collaboration. Instead of\nrelying on noisy 2D features triangulation, we perform multi-view fusion on 3D\nskeletons provided by absolute monocular methods. Accurate 3D pose estimation\nis then obtained via reprojection error optimization, introducing limbs length\nsymmetry constraints. We evaluate our approach on the public dataset Human3.6M\nand on a novel version Human3.6M-Occluded, derived adding synthetic occlusions\non the camera views with the purpose of testing pose estimation algorithms\nunder severe occlusions. We further validate our method on real human-robot\ncollaboration workcells, in which we strongly surpass current 3D human pose\nestimation methods. Our approach outperforms state-of-the-art multi-view human\npose estimation techniques and demonstrates superior capabilities in handling\nchallenging scenarios with strong occlusions, representing a reliable and\neffective solution for real human-robot collaboration setups.\n", "link": "http://arxiv.org/abs/2408.15810v1", "date": "2024-08-28", "relevancy": 1.7022, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5781}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5754}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-view%20Pose%20Fusion%20for%20Occlusion-Aware%203D%20Human%20Pose%20Estimation&body=Title%3A%20Multi-view%20Pose%20Fusion%20for%20Occlusion-Aware%203D%20Human%20Pose%20Estimation%0AAuthor%3A%20Laura%20Bragagnolo%20and%20Matteo%20Terreran%20and%20Davide%20Allegro%20and%20Stefano%20Ghidoni%0AAbstract%3A%20%20%20Robust%203D%20human%20pose%20estimation%20is%20crucial%20to%20ensure%20safe%20and%20effective%0Ahuman-robot%20collaboration.%20Accurate%20human%20perception%2Chowever%2C%20is%20particularly%0Achallenging%20in%20these%20scenarios%20due%20to%20strong%20occlusions%20and%20limited%20camera%0Aviewpoints.%20Current%203D%20human%20pose%20estimation%20approaches%20are%20rather%20vulnerable%0Ain%20such%20conditions.%20In%20this%20work%20we%20present%20a%20novel%20approach%20for%20robust%203D%0Ahuman%20pose%20estimation%20in%20the%20context%20of%20human-robot%20collaboration.%20Instead%20of%0Arelying%20on%20noisy%202D%20features%20triangulation%2C%20we%20perform%20multi-view%20fusion%20on%203D%0Askeletons%20provided%20by%20absolute%20monocular%20methods.%20Accurate%203D%20pose%20estimation%0Ais%20then%20obtained%20via%20reprojection%20error%20optimization%2C%20introducing%20limbs%20length%0Asymmetry%20constraints.%20We%20evaluate%20our%20approach%20on%20the%20public%20dataset%20Human3.6M%0Aand%20on%20a%20novel%20version%20Human3.6M-Occluded%2C%20derived%20adding%20synthetic%20occlusions%0Aon%20the%20camera%20views%20with%20the%20purpose%20of%20testing%20pose%20estimation%20algorithms%0Aunder%20severe%20occlusions.%20We%20further%20validate%20our%20method%20on%20real%20human-robot%0Acollaboration%20workcells%2C%20in%20which%20we%20strongly%20surpass%20current%203D%20human%20pose%0Aestimation%20methods.%20Our%20approach%20outperforms%20state-of-the-art%20multi-view%20human%0Apose%20estimation%20techniques%20and%20demonstrates%20superior%20capabilities%20in%20handling%0Achallenging%20scenarios%20with%20strong%20occlusions%2C%20representing%20a%20reliable%20and%0Aeffective%20solution%20for%20real%20human-robot%20collaboration%20setups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-view%2520Pose%2520Fusion%2520for%2520Occlusion-Aware%25203D%2520Human%2520Pose%2520Estimation%26entry.906535625%3DLaura%2520Bragagnolo%2520and%2520Matteo%2520Terreran%2520and%2520Davide%2520Allegro%2520and%2520Stefano%2520Ghidoni%26entry.1292438233%3D%2520%2520Robust%25203D%2520human%2520pose%2520estimation%2520is%2520crucial%2520to%2520ensure%2520safe%2520and%2520effective%250Ahuman-robot%2520collaboration.%2520Accurate%2520human%2520perception%252Chowever%252C%2520is%2520particularly%250Achallenging%2520in%2520these%2520scenarios%2520due%2520to%2520strong%2520occlusions%2520and%2520limited%2520camera%250Aviewpoints.%2520Current%25203D%2520human%2520pose%2520estimation%2520approaches%2520are%2520rather%2520vulnerable%250Ain%2520such%2520conditions.%2520In%2520this%2520work%2520we%2520present%2520a%2520novel%2520approach%2520for%2520robust%25203D%250Ahuman%2520pose%2520estimation%2520in%2520the%2520context%2520of%2520human-robot%2520collaboration.%2520Instead%2520of%250Arelying%2520on%2520noisy%25202D%2520features%2520triangulation%252C%2520we%2520perform%2520multi-view%2520fusion%2520on%25203D%250Askeletons%2520provided%2520by%2520absolute%2520monocular%2520methods.%2520Accurate%25203D%2520pose%2520estimation%250Ais%2520then%2520obtained%2520via%2520reprojection%2520error%2520optimization%252C%2520introducing%2520limbs%2520length%250Asymmetry%2520constraints.%2520We%2520evaluate%2520our%2520approach%2520on%2520the%2520public%2520dataset%2520Human3.6M%250Aand%2520on%2520a%2520novel%2520version%2520Human3.6M-Occluded%252C%2520derived%2520adding%2520synthetic%2520occlusions%250Aon%2520the%2520camera%2520views%2520with%2520the%2520purpose%2520of%2520testing%2520pose%2520estimation%2520algorithms%250Aunder%2520severe%2520occlusions.%2520We%2520further%2520validate%2520our%2520method%2520on%2520real%2520human-robot%250Acollaboration%2520workcells%252C%2520in%2520which%2520we%2520strongly%2520surpass%2520current%25203D%2520human%2520pose%250Aestimation%2520methods.%2520Our%2520approach%2520outperforms%2520state-of-the-art%2520multi-view%2520human%250Apose%2520estimation%2520techniques%2520and%2520demonstrates%2520superior%2520capabilities%2520in%2520handling%250Achallenging%2520scenarios%2520with%2520strong%2520occlusions%252C%2520representing%2520a%2520reliable%2520and%250Aeffective%2520solution%2520for%2520real%2520human-robot%2520collaboration%2520setups.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-view%20Pose%20Fusion%20for%20Occlusion-Aware%203D%20Human%20Pose%20Estimation&entry.906535625=Laura%20Bragagnolo%20and%20Matteo%20Terreran%20and%20Davide%20Allegro%20and%20Stefano%20Ghidoni&entry.1292438233=%20%20Robust%203D%20human%20pose%20estimation%20is%20crucial%20to%20ensure%20safe%20and%20effective%0Ahuman-robot%20collaboration.%20Accurate%20human%20perception%2Chowever%2C%20is%20particularly%0Achallenging%20in%20these%20scenarios%20due%20to%20strong%20occlusions%20and%20limited%20camera%0Aviewpoints.%20Current%203D%20human%20pose%20estimation%20approaches%20are%20rather%20vulnerable%0Ain%20such%20conditions.%20In%20this%20work%20we%20present%20a%20novel%20approach%20for%20robust%203D%0Ahuman%20pose%20estimation%20in%20the%20context%20of%20human-robot%20collaboration.%20Instead%20of%0Arelying%20on%20noisy%202D%20features%20triangulation%2C%20we%20perform%20multi-view%20fusion%20on%203D%0Askeletons%20provided%20by%20absolute%20monocular%20methods.%20Accurate%203D%20pose%20estimation%0Ais%20then%20obtained%20via%20reprojection%20error%20optimization%2C%20introducing%20limbs%20length%0Asymmetry%20constraints.%20We%20evaluate%20our%20approach%20on%20the%20public%20dataset%20Human3.6M%0Aand%20on%20a%20novel%20version%20Human3.6M-Occluded%2C%20derived%20adding%20synthetic%20occlusions%0Aon%20the%20camera%20views%20with%20the%20purpose%20of%20testing%20pose%20estimation%20algorithms%0Aunder%20severe%20occlusions.%20We%20further%20validate%20our%20method%20on%20real%20human-robot%0Acollaboration%20workcells%2C%20in%20which%20we%20strongly%20surpass%20current%203D%20human%20pose%0Aestimation%20methods.%20Our%20approach%20outperforms%20state-of-the-art%20multi-view%20human%0Apose%20estimation%20techniques%20and%20demonstrates%20superior%20capabilities%20in%20handling%0Achallenging%20scenarios%20with%20strong%20occlusions%2C%20representing%20a%20reliable%20and%0Aeffective%20solution%20for%20real%20human-robot%20collaboration%20setups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15810v1&entry.124074799=Read"},
{"title": "Disentangled Diffusion Autoencoder for Harmonization of Multi-site\n  Neuroimaging Data", "author": "Ayodeji Ijishakin and Ana Lawry Aguila and Elizabeth Levitis and Ahmed Abdulaal and Andre Altmann and James Cole", "abstract": "  Combining neuroimaging datasets from multiple sites and scanners can help\nincrease statistical power and thus provide greater insight into subtle\nneuroanatomical effects. However, site-specific effects pose a challenge by\npotentially obscuring the biological signal and introducing unwanted variance.\nExisting harmonization techniques, which use statistical models to remove such\neffects, have been shown to incompletely remove site effects while also failing\nto preserve biological variability. More recently, generative models using GANs\nor autoencoder-based approaches, have been proposed for site adjustment.\nHowever, such methods are known for instability during training or blurry image\ngeneration. In recent years, diffusion models have become increasingly popular\nfor their ability to generate high-quality synthetic images. In this work, we\nintroduce the disentangled diffusion autoencoder (DDAE), a novel diffusion\nmodel designed for controlling specific aspects of an image. We apply the DDAE\nto the task of harmonizing MR images by generating high-quality site-adjusted\nimages that preserve biological variability. We use data from 7 different sites\nand demonstrate the DDAE's superiority in generating high-resolution,\nharmonized 2D MR images over previous approaches. As far as we are aware, this\nwork marks the first diffusion-based model for site adjustment of neuroimaging\ndata.\n", "link": "http://arxiv.org/abs/2408.15890v1", "date": "2024-08-28", "relevancy": 1.6863, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5937}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5531}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangled%20Diffusion%20Autoencoder%20for%20Harmonization%20of%20Multi-site%0A%20%20Neuroimaging%20Data&body=Title%3A%20Disentangled%20Diffusion%20Autoencoder%20for%20Harmonization%20of%20Multi-site%0A%20%20Neuroimaging%20Data%0AAuthor%3A%20Ayodeji%20Ijishakin%20and%20Ana%20Lawry%20Aguila%20and%20Elizabeth%20Levitis%20and%20Ahmed%20Abdulaal%20and%20Andre%20Altmann%20and%20James%20Cole%0AAbstract%3A%20%20%20Combining%20neuroimaging%20datasets%20from%20multiple%20sites%20and%20scanners%20can%20help%0Aincrease%20statistical%20power%20and%20thus%20provide%20greater%20insight%20into%20subtle%0Aneuroanatomical%20effects.%20However%2C%20site-specific%20effects%20pose%20a%20challenge%20by%0Apotentially%20obscuring%20the%20biological%20signal%20and%20introducing%20unwanted%20variance.%0AExisting%20harmonization%20techniques%2C%20which%20use%20statistical%20models%20to%20remove%20such%0Aeffects%2C%20have%20been%20shown%20to%20incompletely%20remove%20site%20effects%20while%20also%20failing%0Ato%20preserve%20biological%20variability.%20More%20recently%2C%20generative%20models%20using%20GANs%0Aor%20autoencoder-based%20approaches%2C%20have%20been%20proposed%20for%20site%20adjustment.%0AHowever%2C%20such%20methods%20are%20known%20for%20instability%20during%20training%20or%20blurry%20image%0Ageneration.%20In%20recent%20years%2C%20diffusion%20models%20have%20become%20increasingly%20popular%0Afor%20their%20ability%20to%20generate%20high-quality%20synthetic%20images.%20In%20this%20work%2C%20we%0Aintroduce%20the%20disentangled%20diffusion%20autoencoder%20%28DDAE%29%2C%20a%20novel%20diffusion%0Amodel%20designed%20for%20controlling%20specific%20aspects%20of%20an%20image.%20We%20apply%20the%20DDAE%0Ato%20the%20task%20of%20harmonizing%20MR%20images%20by%20generating%20high-quality%20site-adjusted%0Aimages%20that%20preserve%20biological%20variability.%20We%20use%20data%20from%207%20different%20sites%0Aand%20demonstrate%20the%20DDAE%27s%20superiority%20in%20generating%20high-resolution%2C%0Aharmonized%202D%20MR%20images%20over%20previous%20approaches.%20As%20far%20as%20we%20are%20aware%2C%20this%0Awork%20marks%20the%20first%20diffusion-based%20model%20for%20site%20adjustment%20of%20neuroimaging%0Adata.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15890v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangled%2520Diffusion%2520Autoencoder%2520for%2520Harmonization%2520of%2520Multi-site%250A%2520%2520Neuroimaging%2520Data%26entry.906535625%3DAyodeji%2520Ijishakin%2520and%2520Ana%2520Lawry%2520Aguila%2520and%2520Elizabeth%2520Levitis%2520and%2520Ahmed%2520Abdulaal%2520and%2520Andre%2520Altmann%2520and%2520James%2520Cole%26entry.1292438233%3D%2520%2520Combining%2520neuroimaging%2520datasets%2520from%2520multiple%2520sites%2520and%2520scanners%2520can%2520help%250Aincrease%2520statistical%2520power%2520and%2520thus%2520provide%2520greater%2520insight%2520into%2520subtle%250Aneuroanatomical%2520effects.%2520However%252C%2520site-specific%2520effects%2520pose%2520a%2520challenge%2520by%250Apotentially%2520obscuring%2520the%2520biological%2520signal%2520and%2520introducing%2520unwanted%2520variance.%250AExisting%2520harmonization%2520techniques%252C%2520which%2520use%2520statistical%2520models%2520to%2520remove%2520such%250Aeffects%252C%2520have%2520been%2520shown%2520to%2520incompletely%2520remove%2520site%2520effects%2520while%2520also%2520failing%250Ato%2520preserve%2520biological%2520variability.%2520More%2520recently%252C%2520generative%2520models%2520using%2520GANs%250Aor%2520autoencoder-based%2520approaches%252C%2520have%2520been%2520proposed%2520for%2520site%2520adjustment.%250AHowever%252C%2520such%2520methods%2520are%2520known%2520for%2520instability%2520during%2520training%2520or%2520blurry%2520image%250Ageneration.%2520In%2520recent%2520years%252C%2520diffusion%2520models%2520have%2520become%2520increasingly%2520popular%250Afor%2520their%2520ability%2520to%2520generate%2520high-quality%2520synthetic%2520images.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520the%2520disentangled%2520diffusion%2520autoencoder%2520%2528DDAE%2529%252C%2520a%2520novel%2520diffusion%250Amodel%2520designed%2520for%2520controlling%2520specific%2520aspects%2520of%2520an%2520image.%2520We%2520apply%2520the%2520DDAE%250Ato%2520the%2520task%2520of%2520harmonizing%2520MR%2520images%2520by%2520generating%2520high-quality%2520site-adjusted%250Aimages%2520that%2520preserve%2520biological%2520variability.%2520We%2520use%2520data%2520from%25207%2520different%2520sites%250Aand%2520demonstrate%2520the%2520DDAE%2527s%2520superiority%2520in%2520generating%2520high-resolution%252C%250Aharmonized%25202D%2520MR%2520images%2520over%2520previous%2520approaches.%2520As%2520far%2520as%2520we%2520are%2520aware%252C%2520this%250Awork%2520marks%2520the%2520first%2520diffusion-based%2520model%2520for%2520site%2520adjustment%2520of%2520neuroimaging%250Adata.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15890v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangled%20Diffusion%20Autoencoder%20for%20Harmonization%20of%20Multi-site%0A%20%20Neuroimaging%20Data&entry.906535625=Ayodeji%20Ijishakin%20and%20Ana%20Lawry%20Aguila%20and%20Elizabeth%20Levitis%20and%20Ahmed%20Abdulaal%20and%20Andre%20Altmann%20and%20James%20Cole&entry.1292438233=%20%20Combining%20neuroimaging%20datasets%20from%20multiple%20sites%20and%20scanners%20can%20help%0Aincrease%20statistical%20power%20and%20thus%20provide%20greater%20insight%20into%20subtle%0Aneuroanatomical%20effects.%20However%2C%20site-specific%20effects%20pose%20a%20challenge%20by%0Apotentially%20obscuring%20the%20biological%20signal%20and%20introducing%20unwanted%20variance.%0AExisting%20harmonization%20techniques%2C%20which%20use%20statistical%20models%20to%20remove%20such%0Aeffects%2C%20have%20been%20shown%20to%20incompletely%20remove%20site%20effects%20while%20also%20failing%0Ato%20preserve%20biological%20variability.%20More%20recently%2C%20generative%20models%20using%20GANs%0Aor%20autoencoder-based%20approaches%2C%20have%20been%20proposed%20for%20site%20adjustment.%0AHowever%2C%20such%20methods%20are%20known%20for%20instability%20during%20training%20or%20blurry%20image%0Ageneration.%20In%20recent%20years%2C%20diffusion%20models%20have%20become%20increasingly%20popular%0Afor%20their%20ability%20to%20generate%20high-quality%20synthetic%20images.%20In%20this%20work%2C%20we%0Aintroduce%20the%20disentangled%20diffusion%20autoencoder%20%28DDAE%29%2C%20a%20novel%20diffusion%0Amodel%20designed%20for%20controlling%20specific%20aspects%20of%20an%20image.%20We%20apply%20the%20DDAE%0Ato%20the%20task%20of%20harmonizing%20MR%20images%20by%20generating%20high-quality%20site-adjusted%0Aimages%20that%20preserve%20biological%20variability.%20We%20use%20data%20from%207%20different%20sites%0Aand%20demonstrate%20the%20DDAE%27s%20superiority%20in%20generating%20high-resolution%2C%0Aharmonized%202D%20MR%20images%20over%20previous%20approaches.%20As%20far%20as%20we%20are%20aware%2C%20this%0Awork%20marks%20the%20first%20diffusion-based%20model%20for%20site%20adjustment%20of%20neuroimaging%0Adata.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15890v1&entry.124074799=Read"},
{"title": "Distribution Backtracking Builds A Faster Convergence Trajectory for\n  One-step Diffusion Distillation", "author": "Shengyuan Zhang and Ling Yang and Zejian Li and An Zhao and Chenye Meng and Changyuan Yang and Guang Yang and Zhiyuan Yang and Lingyun Sun", "abstract": "  Accelerating the sampling speed of diffusion models remains a significant\nchallenge. Recent score distillation methods distill a heavy teacher model into\nan one-step student generator, which is optimized by calculating the difference\nbetween the two score functions on the samples generated by the student model.\nHowever, there is a score mismatch issue in the early stage of the distillation\nprocess, because existing methods mainly focus on using the endpoint of\npre-trained diffusion models as teacher models, overlooking the importance of\nthe convergence trajectory between the student generator and the teacher model.\nTo address this issue, we extend the score distillation process by introducing\nthe entire convergence trajectory of teacher models and propose Distribution\nBacktracking Distillation (DisBack) for distilling student generators. DisBask\nis composed of two stages: Degradation Recording and Distribution Backtracking.\nDegradation Recording is designed to obtain the convergence trajectory of\nteacher models, which records the degradation path from the trained teacher\nmodel to the untrained initial student generator. The degradation path\nimplicitly represents the intermediate distributions of teacher models. Then\nDistribution Backtracking trains a student generator to backtrack the\nintermediate distributions for approximating the convergence trajectory of\nteacher models. Extensive experiments show that DisBack achieves faster and\nbetter convergence than the existing distillation method and accomplishes\ncomparable generation performance. Notably, DisBack is easy to implement and\ncan be generalized to existing distillation methods to boost performance. Our\ncode is publicly available on https://github.com/SYZhang0805/DisBack.\n", "link": "http://arxiv.org/abs/2408.15991v1", "date": "2024-08-28", "relevancy": 1.6815, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5709}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5537}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distribution%20Backtracking%20Builds%20A%20Faster%20Convergence%20Trajectory%20for%0A%20%20One-step%20Diffusion%20Distillation&body=Title%3A%20Distribution%20Backtracking%20Builds%20A%20Faster%20Convergence%20Trajectory%20for%0A%20%20One-step%20Diffusion%20Distillation%0AAuthor%3A%20Shengyuan%20Zhang%20and%20Ling%20Yang%20and%20Zejian%20Li%20and%20An%20Zhao%20and%20Chenye%20Meng%20and%20Changyuan%20Yang%20and%20Guang%20Yang%20and%20Zhiyuan%20Yang%20and%20Lingyun%20Sun%0AAbstract%3A%20%20%20Accelerating%20the%20sampling%20speed%20of%20diffusion%20models%20remains%20a%20significant%0Achallenge.%20Recent%20score%20distillation%20methods%20distill%20a%20heavy%20teacher%20model%20into%0Aan%20one-step%20student%20generator%2C%20which%20is%20optimized%20by%20calculating%20the%20difference%0Abetween%20the%20two%20score%20functions%20on%20the%20samples%20generated%20by%20the%20student%20model.%0AHowever%2C%20there%20is%20a%20score%20mismatch%20issue%20in%20the%20early%20stage%20of%20the%20distillation%0Aprocess%2C%20because%20existing%20methods%20mainly%20focus%20on%20using%20the%20endpoint%20of%0Apre-trained%20diffusion%20models%20as%20teacher%20models%2C%20overlooking%20the%20importance%20of%0Athe%20convergence%20trajectory%20between%20the%20student%20generator%20and%20the%20teacher%20model.%0ATo%20address%20this%20issue%2C%20we%20extend%20the%20score%20distillation%20process%20by%20introducing%0Athe%20entire%20convergence%20trajectory%20of%20teacher%20models%20and%20propose%20Distribution%0ABacktracking%20Distillation%20%28DisBack%29%20for%20distilling%20student%20generators.%20DisBask%0Ais%20composed%20of%20two%20stages%3A%20Degradation%20Recording%20and%20Distribution%20Backtracking.%0ADegradation%20Recording%20is%20designed%20to%20obtain%20the%20convergence%20trajectory%20of%0Ateacher%20models%2C%20which%20records%20the%20degradation%20path%20from%20the%20trained%20teacher%0Amodel%20to%20the%20untrained%20initial%20student%20generator.%20The%20degradation%20path%0Aimplicitly%20represents%20the%20intermediate%20distributions%20of%20teacher%20models.%20Then%0ADistribution%20Backtracking%20trains%20a%20student%20generator%20to%20backtrack%20the%0Aintermediate%20distributions%20for%20approximating%20the%20convergence%20trajectory%20of%0Ateacher%20models.%20Extensive%20experiments%20show%20that%20DisBack%20achieves%20faster%20and%0Abetter%20convergence%20than%20the%20existing%20distillation%20method%20and%20accomplishes%0Acomparable%20generation%20performance.%20Notably%2C%20DisBack%20is%20easy%20to%20implement%20and%0Acan%20be%20generalized%20to%20existing%20distillation%20methods%20to%20boost%20performance.%20Our%0Acode%20is%20publicly%20available%20on%20https%3A//github.com/SYZhang0805/DisBack.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistribution%2520Backtracking%2520Builds%2520A%2520Faster%2520Convergence%2520Trajectory%2520for%250A%2520%2520One-step%2520Diffusion%2520Distillation%26entry.906535625%3DShengyuan%2520Zhang%2520and%2520Ling%2520Yang%2520and%2520Zejian%2520Li%2520and%2520An%2520Zhao%2520and%2520Chenye%2520Meng%2520and%2520Changyuan%2520Yang%2520and%2520Guang%2520Yang%2520and%2520Zhiyuan%2520Yang%2520and%2520Lingyun%2520Sun%26entry.1292438233%3D%2520%2520Accelerating%2520the%2520sampling%2520speed%2520of%2520diffusion%2520models%2520remains%2520a%2520significant%250Achallenge.%2520Recent%2520score%2520distillation%2520methods%2520distill%2520a%2520heavy%2520teacher%2520model%2520into%250Aan%2520one-step%2520student%2520generator%252C%2520which%2520is%2520optimized%2520by%2520calculating%2520the%2520difference%250Abetween%2520the%2520two%2520score%2520functions%2520on%2520the%2520samples%2520generated%2520by%2520the%2520student%2520model.%250AHowever%252C%2520there%2520is%2520a%2520score%2520mismatch%2520issue%2520in%2520the%2520early%2520stage%2520of%2520the%2520distillation%250Aprocess%252C%2520because%2520existing%2520methods%2520mainly%2520focus%2520on%2520using%2520the%2520endpoint%2520of%250Apre-trained%2520diffusion%2520models%2520as%2520teacher%2520models%252C%2520overlooking%2520the%2520importance%2520of%250Athe%2520convergence%2520trajectory%2520between%2520the%2520student%2520generator%2520and%2520the%2520teacher%2520model.%250ATo%2520address%2520this%2520issue%252C%2520we%2520extend%2520the%2520score%2520distillation%2520process%2520by%2520introducing%250Athe%2520entire%2520convergence%2520trajectory%2520of%2520teacher%2520models%2520and%2520propose%2520Distribution%250ABacktracking%2520Distillation%2520%2528DisBack%2529%2520for%2520distilling%2520student%2520generators.%2520DisBask%250Ais%2520composed%2520of%2520two%2520stages%253A%2520Degradation%2520Recording%2520and%2520Distribution%2520Backtracking.%250ADegradation%2520Recording%2520is%2520designed%2520to%2520obtain%2520the%2520convergence%2520trajectory%2520of%250Ateacher%2520models%252C%2520which%2520records%2520the%2520degradation%2520path%2520from%2520the%2520trained%2520teacher%250Amodel%2520to%2520the%2520untrained%2520initial%2520student%2520generator.%2520The%2520degradation%2520path%250Aimplicitly%2520represents%2520the%2520intermediate%2520distributions%2520of%2520teacher%2520models.%2520Then%250ADistribution%2520Backtracking%2520trains%2520a%2520student%2520generator%2520to%2520backtrack%2520the%250Aintermediate%2520distributions%2520for%2520approximating%2520the%2520convergence%2520trajectory%2520of%250Ateacher%2520models.%2520Extensive%2520experiments%2520show%2520that%2520DisBack%2520achieves%2520faster%2520and%250Abetter%2520convergence%2520than%2520the%2520existing%2520distillation%2520method%2520and%2520accomplishes%250Acomparable%2520generation%2520performance.%2520Notably%252C%2520DisBack%2520is%2520easy%2520to%2520implement%2520and%250Acan%2520be%2520generalized%2520to%2520existing%2520distillation%2520methods%2520to%2520boost%2520performance.%2520Our%250Acode%2520is%2520publicly%2520available%2520on%2520https%253A//github.com/SYZhang0805/DisBack.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distribution%20Backtracking%20Builds%20A%20Faster%20Convergence%20Trajectory%20for%0A%20%20One-step%20Diffusion%20Distillation&entry.906535625=Shengyuan%20Zhang%20and%20Ling%20Yang%20and%20Zejian%20Li%20and%20An%20Zhao%20and%20Chenye%20Meng%20and%20Changyuan%20Yang%20and%20Guang%20Yang%20and%20Zhiyuan%20Yang%20and%20Lingyun%20Sun&entry.1292438233=%20%20Accelerating%20the%20sampling%20speed%20of%20diffusion%20models%20remains%20a%20significant%0Achallenge.%20Recent%20score%20distillation%20methods%20distill%20a%20heavy%20teacher%20model%20into%0Aan%20one-step%20student%20generator%2C%20which%20is%20optimized%20by%20calculating%20the%20difference%0Abetween%20the%20two%20score%20functions%20on%20the%20samples%20generated%20by%20the%20student%20model.%0AHowever%2C%20there%20is%20a%20score%20mismatch%20issue%20in%20the%20early%20stage%20of%20the%20distillation%0Aprocess%2C%20because%20existing%20methods%20mainly%20focus%20on%20using%20the%20endpoint%20of%0Apre-trained%20diffusion%20models%20as%20teacher%20models%2C%20overlooking%20the%20importance%20of%0Athe%20convergence%20trajectory%20between%20the%20student%20generator%20and%20the%20teacher%20model.%0ATo%20address%20this%20issue%2C%20we%20extend%20the%20score%20distillation%20process%20by%20introducing%0Athe%20entire%20convergence%20trajectory%20of%20teacher%20models%20and%20propose%20Distribution%0ABacktracking%20Distillation%20%28DisBack%29%20for%20distilling%20student%20generators.%20DisBask%0Ais%20composed%20of%20two%20stages%3A%20Degradation%20Recording%20and%20Distribution%20Backtracking.%0ADegradation%20Recording%20is%20designed%20to%20obtain%20the%20convergence%20trajectory%20of%0Ateacher%20models%2C%20which%20records%20the%20degradation%20path%20from%20the%20trained%20teacher%0Amodel%20to%20the%20untrained%20initial%20student%20generator.%20The%20degradation%20path%0Aimplicitly%20represents%20the%20intermediate%20distributions%20of%20teacher%20models.%20Then%0ADistribution%20Backtracking%20trains%20a%20student%20generator%20to%20backtrack%20the%0Aintermediate%20distributions%20for%20approximating%20the%20convergence%20trajectory%20of%0Ateacher%20models.%20Extensive%20experiments%20show%20that%20DisBack%20achieves%20faster%20and%0Abetter%20convergence%20than%20the%20existing%20distillation%20method%20and%20accomplishes%0Acomparable%20generation%20performance.%20Notably%2C%20DisBack%20is%20easy%20to%20implement%20and%0Acan%20be%20generalized%20to%20existing%20distillation%20methods%20to%20boost%20performance.%20Our%0Acode%20is%20publicly%20available%20on%20https%3A//github.com/SYZhang0805/DisBack.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15991v1&entry.124074799=Read"},
{"title": "Receding-Constraint Model Predictive Control using a Learned Approximate\n  Control-Invariant Set", "author": "Gianni Lunardi and Asia La Rocca and Matteo Saveriano and Andrea Del Prete", "abstract": "  In recent years, advanced model-based and data-driven control methods are\nunlocking the potential of complex robotics systems, and we can expect this\ntrend to continue at an exponential rate in the near future. However, ensuring\nsafety with these advanced control methods remains a challenge. A well-known\ntool to make controllers (either Model Predictive Controllers or Reinforcement\nLearning policies) safe, is the so-called control-invariant set (a.k.a. safe\nset). Unfortunately, for nonlinear systems, such a set cannot be exactly\ncomputed in general. Numerical algorithms exist for computing approximate\ncontrol-invariant sets, but classic theoretic control methods break down if the\nset is not exact. This paper presents our recent efforts to address this issue.\nWe present a novel Model Predictive Control scheme that can guarantee recursive\nfeasibility and/or safety under weaker assumptions than classic methods. In\nparticular, recursive feasibility is guaranteed by making the safe-set\nconstraint move backward over the horizon, and assuming that such set satisfies\na condition that is weaker than control invariance. Safety is instead\nguaranteed under an even weaker assumption on the safe set, triggering a safe\ntask-abortion strategy whenever a risk of constraint violation is detected. We\nevaluated our approach on a simulated robot manipulator, empirically\ndemonstrating that it leads to less constraint violations than state-of-the-art\napproaches, while retaining reasonable performance in terms of tracking cost,\nnumber of completed tasks, and computation time.\n", "link": "http://arxiv.org/abs/2309.11124v2", "date": "2024-08-28", "relevancy": 1.6809, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5707}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5683}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Receding-Constraint%20Model%20Predictive%20Control%20using%20a%20Learned%20Approximate%0A%20%20Control-Invariant%20Set&body=Title%3A%20Receding-Constraint%20Model%20Predictive%20Control%20using%20a%20Learned%20Approximate%0A%20%20Control-Invariant%20Set%0AAuthor%3A%20Gianni%20Lunardi%20and%20Asia%20La%20Rocca%20and%20Matteo%20Saveriano%20and%20Andrea%20Del%20Prete%0AAbstract%3A%20%20%20In%20recent%20years%2C%20advanced%20model-based%20and%20data-driven%20control%20methods%20are%0Aunlocking%20the%20potential%20of%20complex%20robotics%20systems%2C%20and%20we%20can%20expect%20this%0Atrend%20to%20continue%20at%20an%20exponential%20rate%20in%20the%20near%20future.%20However%2C%20ensuring%0Asafety%20with%20these%20advanced%20control%20methods%20remains%20a%20challenge.%20A%20well-known%0Atool%20to%20make%20controllers%20%28either%20Model%20Predictive%20Controllers%20or%20Reinforcement%0ALearning%20policies%29%20safe%2C%20is%20the%20so-called%20control-invariant%20set%20%28a.k.a.%20safe%0Aset%29.%20Unfortunately%2C%20for%20nonlinear%20systems%2C%20such%20a%20set%20cannot%20be%20exactly%0Acomputed%20in%20general.%20Numerical%20algorithms%20exist%20for%20computing%20approximate%0Acontrol-invariant%20sets%2C%20but%20classic%20theoretic%20control%20methods%20break%20down%20if%20the%0Aset%20is%20not%20exact.%20This%20paper%20presents%20our%20recent%20efforts%20to%20address%20this%20issue.%0AWe%20present%20a%20novel%20Model%20Predictive%20Control%20scheme%20that%20can%20guarantee%20recursive%0Afeasibility%20and/or%20safety%20under%20weaker%20assumptions%20than%20classic%20methods.%20In%0Aparticular%2C%20recursive%20feasibility%20is%20guaranteed%20by%20making%20the%20safe-set%0Aconstraint%20move%20backward%20over%20the%20horizon%2C%20and%20assuming%20that%20such%20set%20satisfies%0Aa%20condition%20that%20is%20weaker%20than%20control%20invariance.%20Safety%20is%20instead%0Aguaranteed%20under%20an%20even%20weaker%20assumption%20on%20the%20safe%20set%2C%20triggering%20a%20safe%0Atask-abortion%20strategy%20whenever%20a%20risk%20of%20constraint%20violation%20is%20detected.%20We%0Aevaluated%20our%20approach%20on%20a%20simulated%20robot%20manipulator%2C%20empirically%0Ademonstrating%20that%20it%20leads%20to%20less%20constraint%20violations%20than%20state-of-the-art%0Aapproaches%2C%20while%20retaining%20reasonable%20performance%20in%20terms%20of%20tracking%20cost%2C%0Anumber%20of%20completed%20tasks%2C%20and%20computation%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.11124v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReceding-Constraint%2520Model%2520Predictive%2520Control%2520using%2520a%2520Learned%2520Approximate%250A%2520%2520Control-Invariant%2520Set%26entry.906535625%3DGianni%2520Lunardi%2520and%2520Asia%2520La%2520Rocca%2520and%2520Matteo%2520Saveriano%2520and%2520Andrea%2520Del%2520Prete%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520advanced%2520model-based%2520and%2520data-driven%2520control%2520methods%2520are%250Aunlocking%2520the%2520potential%2520of%2520complex%2520robotics%2520systems%252C%2520and%2520we%2520can%2520expect%2520this%250Atrend%2520to%2520continue%2520at%2520an%2520exponential%2520rate%2520in%2520the%2520near%2520future.%2520However%252C%2520ensuring%250Asafety%2520with%2520these%2520advanced%2520control%2520methods%2520remains%2520a%2520challenge.%2520A%2520well-known%250Atool%2520to%2520make%2520controllers%2520%2528either%2520Model%2520Predictive%2520Controllers%2520or%2520Reinforcement%250ALearning%2520policies%2529%2520safe%252C%2520is%2520the%2520so-called%2520control-invariant%2520set%2520%2528a.k.a.%2520safe%250Aset%2529.%2520Unfortunately%252C%2520for%2520nonlinear%2520systems%252C%2520such%2520a%2520set%2520cannot%2520be%2520exactly%250Acomputed%2520in%2520general.%2520Numerical%2520algorithms%2520exist%2520for%2520computing%2520approximate%250Acontrol-invariant%2520sets%252C%2520but%2520classic%2520theoretic%2520control%2520methods%2520break%2520down%2520if%2520the%250Aset%2520is%2520not%2520exact.%2520This%2520paper%2520presents%2520our%2520recent%2520efforts%2520to%2520address%2520this%2520issue.%250AWe%2520present%2520a%2520novel%2520Model%2520Predictive%2520Control%2520scheme%2520that%2520can%2520guarantee%2520recursive%250Afeasibility%2520and/or%2520safety%2520under%2520weaker%2520assumptions%2520than%2520classic%2520methods.%2520In%250Aparticular%252C%2520recursive%2520feasibility%2520is%2520guaranteed%2520by%2520making%2520the%2520safe-set%250Aconstraint%2520move%2520backward%2520over%2520the%2520horizon%252C%2520and%2520assuming%2520that%2520such%2520set%2520satisfies%250Aa%2520condition%2520that%2520is%2520weaker%2520than%2520control%2520invariance.%2520Safety%2520is%2520instead%250Aguaranteed%2520under%2520an%2520even%2520weaker%2520assumption%2520on%2520the%2520safe%2520set%252C%2520triggering%2520a%2520safe%250Atask-abortion%2520strategy%2520whenever%2520a%2520risk%2520of%2520constraint%2520violation%2520is%2520detected.%2520We%250Aevaluated%2520our%2520approach%2520on%2520a%2520simulated%2520robot%2520manipulator%252C%2520empirically%250Ademonstrating%2520that%2520it%2520leads%2520to%2520less%2520constraint%2520violations%2520than%2520state-of-the-art%250Aapproaches%252C%2520while%2520retaining%2520reasonable%2520performance%2520in%2520terms%2520of%2520tracking%2520cost%252C%250Anumber%2520of%2520completed%2520tasks%252C%2520and%2520computation%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.11124v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Receding-Constraint%20Model%20Predictive%20Control%20using%20a%20Learned%20Approximate%0A%20%20Control-Invariant%20Set&entry.906535625=Gianni%20Lunardi%20and%20Asia%20La%20Rocca%20and%20Matteo%20Saveriano%20and%20Andrea%20Del%20Prete&entry.1292438233=%20%20In%20recent%20years%2C%20advanced%20model-based%20and%20data-driven%20control%20methods%20are%0Aunlocking%20the%20potential%20of%20complex%20robotics%20systems%2C%20and%20we%20can%20expect%20this%0Atrend%20to%20continue%20at%20an%20exponential%20rate%20in%20the%20near%20future.%20However%2C%20ensuring%0Asafety%20with%20these%20advanced%20control%20methods%20remains%20a%20challenge.%20A%20well-known%0Atool%20to%20make%20controllers%20%28either%20Model%20Predictive%20Controllers%20or%20Reinforcement%0ALearning%20policies%29%20safe%2C%20is%20the%20so-called%20control-invariant%20set%20%28a.k.a.%20safe%0Aset%29.%20Unfortunately%2C%20for%20nonlinear%20systems%2C%20such%20a%20set%20cannot%20be%20exactly%0Acomputed%20in%20general.%20Numerical%20algorithms%20exist%20for%20computing%20approximate%0Acontrol-invariant%20sets%2C%20but%20classic%20theoretic%20control%20methods%20break%20down%20if%20the%0Aset%20is%20not%20exact.%20This%20paper%20presents%20our%20recent%20efforts%20to%20address%20this%20issue.%0AWe%20present%20a%20novel%20Model%20Predictive%20Control%20scheme%20that%20can%20guarantee%20recursive%0Afeasibility%20and/or%20safety%20under%20weaker%20assumptions%20than%20classic%20methods.%20In%0Aparticular%2C%20recursive%20feasibility%20is%20guaranteed%20by%20making%20the%20safe-set%0Aconstraint%20move%20backward%20over%20the%20horizon%2C%20and%20assuming%20that%20such%20set%20satisfies%0Aa%20condition%20that%20is%20weaker%20than%20control%20invariance.%20Safety%20is%20instead%0Aguaranteed%20under%20an%20even%20weaker%20assumption%20on%20the%20safe%20set%2C%20triggering%20a%20safe%0Atask-abortion%20strategy%20whenever%20a%20risk%20of%20constraint%20violation%20is%20detected.%20We%0Aevaluated%20our%20approach%20on%20a%20simulated%20robot%20manipulator%2C%20empirically%0Ademonstrating%20that%20it%20leads%20to%20less%20constraint%20violations%20than%20state-of-the-art%0Aapproaches%2C%20while%20retaining%20reasonable%20performance%20in%20terms%20of%20tracking%20cost%2C%0Anumber%20of%20completed%20tasks%2C%20and%20computation%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.11124v2&entry.124074799=Read"},
{"title": "Modeling and Analyzing the Influence of Non-Item Pages on Sequential\n  Next-Item Prediction", "author": "Elisabeth Fischer and Daniel Schl\u00f6r and Albin Zehe and Andreas Hotho", "abstract": "  Analyzing the sequence of historical interactions between users and items,\nsequential recommendation models learn user intent and make predictions about\nthe next item of interest. Next to these item interactions, most systems also\nhave interactions with pages not related to specific items, for example\nnavigation pages, account pages, and pages for a specific category, which may\nprovide additional insights into the user's interests. However, while there are\nseveral approaches to integrate additional information about items and users,\nthe topic of integrating non-item pages has been less explored. We use the\nhypotheses testing framework HypTrails to show that there is indeed a\nrelationship between these non-item pages and the items of interest and fill\nthis gap by proposing various approaches of representing non-item pages (e.g,\nbased on their content) to use them as an additional information source for the\ntask of sequential next-item prediction.\n  We create a synthetic dataset with non-item pages highly related to the\nsubsequent item to show that the models are generally capable of learning from\nthese interactions, and subsequently evaluate the improvements gained by\nincluding non-item pages in two real-world datasets.\n  We adapt eight popular sequential recommender models, covering CNN-, RNN- and\ntransformer-based architectures, to integrate non-item pages and investigate\nthe capabilities of these models to leverage their information for next item\nprediction. We also analyze their behavior on noisy data and compare different\nitem representation strategies.\n  Our results show that non-item pages are a valuable source of information,\nbut representing such a page well is the key to successfully leverage them. The\ninclusion of non-item pages can increase the performance for next-item\nprediction in all examined model architectures with a varying degree.\n", "link": "http://arxiv.org/abs/2408.15953v1", "date": "2024-08-28", "relevancy": 1.3625, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4974}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4449}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20and%20Analyzing%20the%20Influence%20of%20Non-Item%20Pages%20on%20Sequential%0A%20%20Next-Item%20Prediction&body=Title%3A%20Modeling%20and%20Analyzing%20the%20Influence%20of%20Non-Item%20Pages%20on%20Sequential%0A%20%20Next-Item%20Prediction%0AAuthor%3A%20Elisabeth%20Fischer%20and%20Daniel%20Schl%C3%B6r%20and%20Albin%20Zehe%20and%20Andreas%20Hotho%0AAbstract%3A%20%20%20Analyzing%20the%20sequence%20of%20historical%20interactions%20between%20users%20and%20items%2C%0Asequential%20recommendation%20models%20learn%20user%20intent%20and%20make%20predictions%20about%0Athe%20next%20item%20of%20interest.%20Next%20to%20these%20item%20interactions%2C%20most%20systems%20also%0Ahave%20interactions%20with%20pages%20not%20related%20to%20specific%20items%2C%20for%20example%0Anavigation%20pages%2C%20account%20pages%2C%20and%20pages%20for%20a%20specific%20category%2C%20which%20may%0Aprovide%20additional%20insights%20into%20the%20user%27s%20interests.%20However%2C%20while%20there%20are%0Aseveral%20approaches%20to%20integrate%20additional%20information%20about%20items%20and%20users%2C%0Athe%20topic%20of%20integrating%20non-item%20pages%20has%20been%20less%20explored.%20We%20use%20the%0Ahypotheses%20testing%20framework%20HypTrails%20to%20show%20that%20there%20is%20indeed%20a%0Arelationship%20between%20these%20non-item%20pages%20and%20the%20items%20of%20interest%20and%20fill%0Athis%20gap%20by%20proposing%20various%20approaches%20of%20representing%20non-item%20pages%20%28e.g%2C%0Abased%20on%20their%20content%29%20to%20use%20them%20as%20an%20additional%20information%20source%20for%20the%0Atask%20of%20sequential%20next-item%20prediction.%0A%20%20We%20create%20a%20synthetic%20dataset%20with%20non-item%20pages%20highly%20related%20to%20the%0Asubsequent%20item%20to%20show%20that%20the%20models%20are%20generally%20capable%20of%20learning%20from%0Athese%20interactions%2C%20and%20subsequently%20evaluate%20the%20improvements%20gained%20by%0Aincluding%20non-item%20pages%20in%20two%20real-world%20datasets.%0A%20%20We%20adapt%20eight%20popular%20sequential%20recommender%20models%2C%20covering%20CNN-%2C%20RNN-%20and%0Atransformer-based%20architectures%2C%20to%20integrate%20non-item%20pages%20and%20investigate%0Athe%20capabilities%20of%20these%20models%20to%20leverage%20their%20information%20for%20next%20item%0Aprediction.%20We%20also%20analyze%20their%20behavior%20on%20noisy%20data%20and%20compare%20different%0Aitem%20representation%20strategies.%0A%20%20Our%20results%20show%20that%20non-item%20pages%20are%20a%20valuable%20source%20of%20information%2C%0Abut%20representing%20such%20a%20page%20well%20is%20the%20key%20to%20successfully%20leverage%20them.%20The%0Ainclusion%20of%20non-item%20pages%20can%20increase%20the%20performance%20for%20next-item%0Aprediction%20in%20all%20examined%20model%20architectures%20with%20a%20varying%20degree.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520and%2520Analyzing%2520the%2520Influence%2520of%2520Non-Item%2520Pages%2520on%2520Sequential%250A%2520%2520Next-Item%2520Prediction%26entry.906535625%3DElisabeth%2520Fischer%2520and%2520Daniel%2520Schl%25C3%25B6r%2520and%2520Albin%2520Zehe%2520and%2520Andreas%2520Hotho%26entry.1292438233%3D%2520%2520Analyzing%2520the%2520sequence%2520of%2520historical%2520interactions%2520between%2520users%2520and%2520items%252C%250Asequential%2520recommendation%2520models%2520learn%2520user%2520intent%2520and%2520make%2520predictions%2520about%250Athe%2520next%2520item%2520of%2520interest.%2520Next%2520to%2520these%2520item%2520interactions%252C%2520most%2520systems%2520also%250Ahave%2520interactions%2520with%2520pages%2520not%2520related%2520to%2520specific%2520items%252C%2520for%2520example%250Anavigation%2520pages%252C%2520account%2520pages%252C%2520and%2520pages%2520for%2520a%2520specific%2520category%252C%2520which%2520may%250Aprovide%2520additional%2520insights%2520into%2520the%2520user%2527s%2520interests.%2520However%252C%2520while%2520there%2520are%250Aseveral%2520approaches%2520to%2520integrate%2520additional%2520information%2520about%2520items%2520and%2520users%252C%250Athe%2520topic%2520of%2520integrating%2520non-item%2520pages%2520has%2520been%2520less%2520explored.%2520We%2520use%2520the%250Ahypotheses%2520testing%2520framework%2520HypTrails%2520to%2520show%2520that%2520there%2520is%2520indeed%2520a%250Arelationship%2520between%2520these%2520non-item%2520pages%2520and%2520the%2520items%2520of%2520interest%2520and%2520fill%250Athis%2520gap%2520by%2520proposing%2520various%2520approaches%2520of%2520representing%2520non-item%2520pages%2520%2528e.g%252C%250Abased%2520on%2520their%2520content%2529%2520to%2520use%2520them%2520as%2520an%2520additional%2520information%2520source%2520for%2520the%250Atask%2520of%2520sequential%2520next-item%2520prediction.%250A%2520%2520We%2520create%2520a%2520synthetic%2520dataset%2520with%2520non-item%2520pages%2520highly%2520related%2520to%2520the%250Asubsequent%2520item%2520to%2520show%2520that%2520the%2520models%2520are%2520generally%2520capable%2520of%2520learning%2520from%250Athese%2520interactions%252C%2520and%2520subsequently%2520evaluate%2520the%2520improvements%2520gained%2520by%250Aincluding%2520non-item%2520pages%2520in%2520two%2520real-world%2520datasets.%250A%2520%2520We%2520adapt%2520eight%2520popular%2520sequential%2520recommender%2520models%252C%2520covering%2520CNN-%252C%2520RNN-%2520and%250Atransformer-based%2520architectures%252C%2520to%2520integrate%2520non-item%2520pages%2520and%2520investigate%250Athe%2520capabilities%2520of%2520these%2520models%2520to%2520leverage%2520their%2520information%2520for%2520next%2520item%250Aprediction.%2520We%2520also%2520analyze%2520their%2520behavior%2520on%2520noisy%2520data%2520and%2520compare%2520different%250Aitem%2520representation%2520strategies.%250A%2520%2520Our%2520results%2520show%2520that%2520non-item%2520pages%2520are%2520a%2520valuable%2520source%2520of%2520information%252C%250Abut%2520representing%2520such%2520a%2520page%2520well%2520is%2520the%2520key%2520to%2520successfully%2520leverage%2520them.%2520The%250Ainclusion%2520of%2520non-item%2520pages%2520can%2520increase%2520the%2520performance%2520for%2520next-item%250Aprediction%2520in%2520all%2520examined%2520model%2520architectures%2520with%2520a%2520varying%2520degree.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20and%20Analyzing%20the%20Influence%20of%20Non-Item%20Pages%20on%20Sequential%0A%20%20Next-Item%20Prediction&entry.906535625=Elisabeth%20Fischer%20and%20Daniel%20Schl%C3%B6r%20and%20Albin%20Zehe%20and%20Andreas%20Hotho&entry.1292438233=%20%20Analyzing%20the%20sequence%20of%20historical%20interactions%20between%20users%20and%20items%2C%0Asequential%20recommendation%20models%20learn%20user%20intent%20and%20make%20predictions%20about%0Athe%20next%20item%20of%20interest.%20Next%20to%20these%20item%20interactions%2C%20most%20systems%20also%0Ahave%20interactions%20with%20pages%20not%20related%20to%20specific%20items%2C%20for%20example%0Anavigation%20pages%2C%20account%20pages%2C%20and%20pages%20for%20a%20specific%20category%2C%20which%20may%0Aprovide%20additional%20insights%20into%20the%20user%27s%20interests.%20However%2C%20while%20there%20are%0Aseveral%20approaches%20to%20integrate%20additional%20information%20about%20items%20and%20users%2C%0Athe%20topic%20of%20integrating%20non-item%20pages%20has%20been%20less%20explored.%20We%20use%20the%0Ahypotheses%20testing%20framework%20HypTrails%20to%20show%20that%20there%20is%20indeed%20a%0Arelationship%20between%20these%20non-item%20pages%20and%20the%20items%20of%20interest%20and%20fill%0Athis%20gap%20by%20proposing%20various%20approaches%20of%20representing%20non-item%20pages%20%28e.g%2C%0Abased%20on%20their%20content%29%20to%20use%20them%20as%20an%20additional%20information%20source%20for%20the%0Atask%20of%20sequential%20next-item%20prediction.%0A%20%20We%20create%20a%20synthetic%20dataset%20with%20non-item%20pages%20highly%20related%20to%20the%0Asubsequent%20item%20to%20show%20that%20the%20models%20are%20generally%20capable%20of%20learning%20from%0Athese%20interactions%2C%20and%20subsequently%20evaluate%20the%20improvements%20gained%20by%0Aincluding%20non-item%20pages%20in%20two%20real-world%20datasets.%0A%20%20We%20adapt%20eight%20popular%20sequential%20recommender%20models%2C%20covering%20CNN-%2C%20RNN-%20and%0Atransformer-based%20architectures%2C%20to%20integrate%20non-item%20pages%20and%20investigate%0Athe%20capabilities%20of%20these%20models%20to%20leverage%20their%20information%20for%20next%20item%0Aprediction.%20We%20also%20analyze%20their%20behavior%20on%20noisy%20data%20and%20compare%20different%0Aitem%20representation%20strategies.%0A%20%20Our%20results%20show%20that%20non-item%20pages%20are%20a%20valuable%20source%20of%20information%2C%0Abut%20representing%20such%20a%20page%20well%20is%20the%20key%20to%20successfully%20leverage%20them.%20The%0Ainclusion%20of%20non-item%20pages%20can%20increase%20the%20performance%20for%20next-item%0Aprediction%20in%20all%20examined%20model%20architectures%20with%20a%20varying%20degree.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15953v1&entry.124074799=Read"},
{"title": "Smooth Path Planning with Subharmonic Artificial Potential Field", "author": "Bo Peng and Lingke Zhang and Rong Xiong", "abstract": "  When a mobile robot plans its path in an environment with obstacles using\nArtificial Potential Field (APF) strategy, it may fall into the local minimum\npoint and fail to reach the goal. Also, the derivatives of APF will explode\nclose to obstacles causing poor planning performance. To solve the problems,\nexponential functions are used to modify potential fields' formulas. The\npotential functions can be subharmonic when the distance between the robot and\nobstacles is above a predefined threshold. Subharmonic functions do not have\nlocal minimum and the derivatives of exponential functions increase mildly when\nthe robot is close to obstacles, thus eliminate the problems in theory.\nCircular sampling technique is used to keep the robot outside a danger distance\nto obstacles and support the construction of subharmonic functions. Through\nsimulations, it is proven that mobile robots can bypass local minimum points\nand construct a smooth path to reach the goal successfully by the proposed\nmethods.\n", "link": "http://arxiv.org/abs/2402.11601v2", "date": "2024-08-28", "relevancy": 1.3913, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4873}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4841}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smooth%20Path%20Planning%20with%20Subharmonic%20Artificial%20Potential%20Field&body=Title%3A%20Smooth%20Path%20Planning%20with%20Subharmonic%20Artificial%20Potential%20Field%0AAuthor%3A%20Bo%20Peng%20and%20Lingke%20Zhang%20and%20Rong%20Xiong%0AAbstract%3A%20%20%20When%20a%20mobile%20robot%20plans%20its%20path%20in%20an%20environment%20with%20obstacles%20using%0AArtificial%20Potential%20Field%20%28APF%29%20strategy%2C%20it%20may%20fall%20into%20the%20local%20minimum%0Apoint%20and%20fail%20to%20reach%20the%20goal.%20Also%2C%20the%20derivatives%20of%20APF%20will%20explode%0Aclose%20to%20obstacles%20causing%20poor%20planning%20performance.%20To%20solve%20the%20problems%2C%0Aexponential%20functions%20are%20used%20to%20modify%20potential%20fields%27%20formulas.%20The%0Apotential%20functions%20can%20be%20subharmonic%20when%20the%20distance%20between%20the%20robot%20and%0Aobstacles%20is%20above%20a%20predefined%20threshold.%20Subharmonic%20functions%20do%20not%20have%0Alocal%20minimum%20and%20the%20derivatives%20of%20exponential%20functions%20increase%20mildly%20when%0Athe%20robot%20is%20close%20to%20obstacles%2C%20thus%20eliminate%20the%20problems%20in%20theory.%0ACircular%20sampling%20technique%20is%20used%20to%20keep%20the%20robot%20outside%20a%20danger%20distance%0Ato%20obstacles%20and%20support%20the%20construction%20of%20subharmonic%20functions.%20Through%0Asimulations%2C%20it%20is%20proven%20that%20mobile%20robots%20can%20bypass%20local%20minimum%20points%0Aand%20construct%20a%20smooth%20path%20to%20reach%20the%20goal%20successfully%20by%20the%20proposed%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11601v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmooth%2520Path%2520Planning%2520with%2520Subharmonic%2520Artificial%2520Potential%2520Field%26entry.906535625%3DBo%2520Peng%2520and%2520Lingke%2520Zhang%2520and%2520Rong%2520Xiong%26entry.1292438233%3D%2520%2520When%2520a%2520mobile%2520robot%2520plans%2520its%2520path%2520in%2520an%2520environment%2520with%2520obstacles%2520using%250AArtificial%2520Potential%2520Field%2520%2528APF%2529%2520strategy%252C%2520it%2520may%2520fall%2520into%2520the%2520local%2520minimum%250Apoint%2520and%2520fail%2520to%2520reach%2520the%2520goal.%2520Also%252C%2520the%2520derivatives%2520of%2520APF%2520will%2520explode%250Aclose%2520to%2520obstacles%2520causing%2520poor%2520planning%2520performance.%2520To%2520solve%2520the%2520problems%252C%250Aexponential%2520functions%2520are%2520used%2520to%2520modify%2520potential%2520fields%2527%2520formulas.%2520The%250Apotential%2520functions%2520can%2520be%2520subharmonic%2520when%2520the%2520distance%2520between%2520the%2520robot%2520and%250Aobstacles%2520is%2520above%2520a%2520predefined%2520threshold.%2520Subharmonic%2520functions%2520do%2520not%2520have%250Alocal%2520minimum%2520and%2520the%2520derivatives%2520of%2520exponential%2520functions%2520increase%2520mildly%2520when%250Athe%2520robot%2520is%2520close%2520to%2520obstacles%252C%2520thus%2520eliminate%2520the%2520problems%2520in%2520theory.%250ACircular%2520sampling%2520technique%2520is%2520used%2520to%2520keep%2520the%2520robot%2520outside%2520a%2520danger%2520distance%250Ato%2520obstacles%2520and%2520support%2520the%2520construction%2520of%2520subharmonic%2520functions.%2520Through%250Asimulations%252C%2520it%2520is%2520proven%2520that%2520mobile%2520robots%2520can%2520bypass%2520local%2520minimum%2520points%250Aand%2520construct%2520a%2520smooth%2520path%2520to%2520reach%2520the%2520goal%2520successfully%2520by%2520the%2520proposed%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11601v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smooth%20Path%20Planning%20with%20Subharmonic%20Artificial%20Potential%20Field&entry.906535625=Bo%20Peng%20and%20Lingke%20Zhang%20and%20Rong%20Xiong&entry.1292438233=%20%20When%20a%20mobile%20robot%20plans%20its%20path%20in%20an%20environment%20with%20obstacles%20using%0AArtificial%20Potential%20Field%20%28APF%29%20strategy%2C%20it%20may%20fall%20into%20the%20local%20minimum%0Apoint%20and%20fail%20to%20reach%20the%20goal.%20Also%2C%20the%20derivatives%20of%20APF%20will%20explode%0Aclose%20to%20obstacles%20causing%20poor%20planning%20performance.%20To%20solve%20the%20problems%2C%0Aexponential%20functions%20are%20used%20to%20modify%20potential%20fields%27%20formulas.%20The%0Apotential%20functions%20can%20be%20subharmonic%20when%20the%20distance%20between%20the%20robot%20and%0Aobstacles%20is%20above%20a%20predefined%20threshold.%20Subharmonic%20functions%20do%20not%20have%0Alocal%20minimum%20and%20the%20derivatives%20of%20exponential%20functions%20increase%20mildly%20when%0Athe%20robot%20is%20close%20to%20obstacles%2C%20thus%20eliminate%20the%20problems%20in%20theory.%0ACircular%20sampling%20technique%20is%20used%20to%20keep%20the%20robot%20outside%20a%20danger%20distance%0Ato%20obstacles%20and%20support%20the%20construction%20of%20subharmonic%20functions.%20Through%0Asimulations%2C%20it%20is%20proven%20that%20mobile%20robots%20can%20bypass%20local%20minimum%20points%0Aand%20construct%20a%20smooth%20path%20to%20reach%20the%20goal%20successfully%20by%20the%20proposed%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11601v2&entry.124074799=Read"},
{"title": "Articulation Work and Tinkering for Fairness in Machine Learning", "author": "Miriam Fahimi and Mayra Russo and Kristen M. Scott and Maria-Esther Vidal and Bettina Berendt and Katharina Kinder-Kurlanda", "abstract": "  The field of fair AI aims to counter biased algorithms through computational\nmodelling. However, it faces increasing criticism for perpetuating the use of\noverly technical and reductionist methods. As a result, novel approaches appear\nin the field to address more socially-oriented and interdisciplinary (SOI)\nperspectives on fair AI. In this paper, we take this dynamic as the starting\npoint to study the tension between computer science (CS) and SOI research. By\ndrawing on STS and CSCW theory, we position fair AI research as a matter of\n'organizational alignment': what makes research 'doable' is the successful\nalignment of three levels of work organization (the social world, the\nlaboratory, and the experiment). Based on qualitative interviews with CS\nresearchers, we analyze the tasks, resources, and actors required for doable\nresearch in the case of fair AI. We find that CS researchers engage with SOI\nresearch to some extent, but organizational conditions, articulation work, and\nambiguities of the social world constrain the doability of SOI research for\nthem. Based on our findings, we identify and discuss problems for aligning CS\nand SOI as fair AI continues to evolve.\n", "link": "http://arxiv.org/abs/2407.16496v2", "date": "2024-08-28", "relevancy": 1.2795, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4403}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4308}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Articulation%20Work%20and%20Tinkering%20for%20Fairness%20in%20Machine%20Learning&body=Title%3A%20Articulation%20Work%20and%20Tinkering%20for%20Fairness%20in%20Machine%20Learning%0AAuthor%3A%20Miriam%20Fahimi%20and%20Mayra%20Russo%20and%20Kristen%20M.%20Scott%20and%20Maria-Esther%20Vidal%20and%20Bettina%20Berendt%20and%20Katharina%20Kinder-Kurlanda%0AAbstract%3A%20%20%20The%20field%20of%20fair%20AI%20aims%20to%20counter%20biased%20algorithms%20through%20computational%0Amodelling.%20However%2C%20it%20faces%20increasing%20criticism%20for%20perpetuating%20the%20use%20of%0Aoverly%20technical%20and%20reductionist%20methods.%20As%20a%20result%2C%20novel%20approaches%20appear%0Ain%20the%20field%20to%20address%20more%20socially-oriented%20and%20interdisciplinary%20%28SOI%29%0Aperspectives%20on%20fair%20AI.%20In%20this%20paper%2C%20we%20take%20this%20dynamic%20as%20the%20starting%0Apoint%20to%20study%20the%20tension%20between%20computer%20science%20%28CS%29%20and%20SOI%20research.%20By%0Adrawing%20on%20STS%20and%20CSCW%20theory%2C%20we%20position%20fair%20AI%20research%20as%20a%20matter%20of%0A%27organizational%20alignment%27%3A%20what%20makes%20research%20%27doable%27%20is%20the%20successful%0Aalignment%20of%20three%20levels%20of%20work%20organization%20%28the%20social%20world%2C%20the%0Alaboratory%2C%20and%20the%20experiment%29.%20Based%20on%20qualitative%20interviews%20with%20CS%0Aresearchers%2C%20we%20analyze%20the%20tasks%2C%20resources%2C%20and%20actors%20required%20for%20doable%0Aresearch%20in%20the%20case%20of%20fair%20AI.%20We%20find%20that%20CS%20researchers%20engage%20with%20SOI%0Aresearch%20to%20some%20extent%2C%20but%20organizational%20conditions%2C%20articulation%20work%2C%20and%0Aambiguities%20of%20the%20social%20world%20constrain%20the%20doability%20of%20SOI%20research%20for%0Athem.%20Based%20on%20our%20findings%2C%20we%20identify%20and%20discuss%20problems%20for%20aligning%20CS%0Aand%20SOI%20as%20fair%20AI%20continues%20to%20evolve.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.16496v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArticulation%2520Work%2520and%2520Tinkering%2520for%2520Fairness%2520in%2520Machine%2520Learning%26entry.906535625%3DMiriam%2520Fahimi%2520and%2520Mayra%2520Russo%2520and%2520Kristen%2520M.%2520Scott%2520and%2520Maria-Esther%2520Vidal%2520and%2520Bettina%2520Berendt%2520and%2520Katharina%2520Kinder-Kurlanda%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520fair%2520AI%2520aims%2520to%2520counter%2520biased%2520algorithms%2520through%2520computational%250Amodelling.%2520However%252C%2520it%2520faces%2520increasing%2520criticism%2520for%2520perpetuating%2520the%2520use%2520of%250Aoverly%2520technical%2520and%2520reductionist%2520methods.%2520As%2520a%2520result%252C%2520novel%2520approaches%2520appear%250Ain%2520the%2520field%2520to%2520address%2520more%2520socially-oriented%2520and%2520interdisciplinary%2520%2528SOI%2529%250Aperspectives%2520on%2520fair%2520AI.%2520In%2520this%2520paper%252C%2520we%2520take%2520this%2520dynamic%2520as%2520the%2520starting%250Apoint%2520to%2520study%2520the%2520tension%2520between%2520computer%2520science%2520%2528CS%2529%2520and%2520SOI%2520research.%2520By%250Adrawing%2520on%2520STS%2520and%2520CSCW%2520theory%252C%2520we%2520position%2520fair%2520AI%2520research%2520as%2520a%2520matter%2520of%250A%2527organizational%2520alignment%2527%253A%2520what%2520makes%2520research%2520%2527doable%2527%2520is%2520the%2520successful%250Aalignment%2520of%2520three%2520levels%2520of%2520work%2520organization%2520%2528the%2520social%2520world%252C%2520the%250Alaboratory%252C%2520and%2520the%2520experiment%2529.%2520Based%2520on%2520qualitative%2520interviews%2520with%2520CS%250Aresearchers%252C%2520we%2520analyze%2520the%2520tasks%252C%2520resources%252C%2520and%2520actors%2520required%2520for%2520doable%250Aresearch%2520in%2520the%2520case%2520of%2520fair%2520AI.%2520We%2520find%2520that%2520CS%2520researchers%2520engage%2520with%2520SOI%250Aresearch%2520to%2520some%2520extent%252C%2520but%2520organizational%2520conditions%252C%2520articulation%2520work%252C%2520and%250Aambiguities%2520of%2520the%2520social%2520world%2520constrain%2520the%2520doability%2520of%2520SOI%2520research%2520for%250Athem.%2520Based%2520on%2520our%2520findings%252C%2520we%2520identify%2520and%2520discuss%2520problems%2520for%2520aligning%2520CS%250Aand%2520SOI%2520as%2520fair%2520AI%2520continues%2520to%2520evolve.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.16496v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Articulation%20Work%20and%20Tinkering%20for%20Fairness%20in%20Machine%20Learning&entry.906535625=Miriam%20Fahimi%20and%20Mayra%20Russo%20and%20Kristen%20M.%20Scott%20and%20Maria-Esther%20Vidal%20and%20Bettina%20Berendt%20and%20Katharina%20Kinder-Kurlanda&entry.1292438233=%20%20The%20field%20of%20fair%20AI%20aims%20to%20counter%20biased%20algorithms%20through%20computational%0Amodelling.%20However%2C%20it%20faces%20increasing%20criticism%20for%20perpetuating%20the%20use%20of%0Aoverly%20technical%20and%20reductionist%20methods.%20As%20a%20result%2C%20novel%20approaches%20appear%0Ain%20the%20field%20to%20address%20more%20socially-oriented%20and%20interdisciplinary%20%28SOI%29%0Aperspectives%20on%20fair%20AI.%20In%20this%20paper%2C%20we%20take%20this%20dynamic%20as%20the%20starting%0Apoint%20to%20study%20the%20tension%20between%20computer%20science%20%28CS%29%20and%20SOI%20research.%20By%0Adrawing%20on%20STS%20and%20CSCW%20theory%2C%20we%20position%20fair%20AI%20research%20as%20a%20matter%20of%0A%27organizational%20alignment%27%3A%20what%20makes%20research%20%27doable%27%20is%20the%20successful%0Aalignment%20of%20three%20levels%20of%20work%20organization%20%28the%20social%20world%2C%20the%0Alaboratory%2C%20and%20the%20experiment%29.%20Based%20on%20qualitative%20interviews%20with%20CS%0Aresearchers%2C%20we%20analyze%20the%20tasks%2C%20resources%2C%20and%20actors%20required%20for%20doable%0Aresearch%20in%20the%20case%20of%20fair%20AI.%20We%20find%20that%20CS%20researchers%20engage%20with%20SOI%0Aresearch%20to%20some%20extent%2C%20but%20organizational%20conditions%2C%20articulation%20work%2C%20and%0Aambiguities%20of%20the%20social%20world%20constrain%20the%20doability%20of%20SOI%20research%20for%0Athem.%20Based%20on%20our%20findings%2C%20we%20identify%20and%20discuss%20problems%20for%20aligning%20CS%0Aand%20SOI%20as%20fair%20AI%20continues%20to%20evolve.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.16496v2&entry.124074799=Read"},
{"title": "SCP: Soft Conditional Prompt Learning for Aerial Video Action\n  Recognition", "author": "Xijun Wang and Ruiqi Xian and Tianrui Guan and Fuxiao Liu and Dinesh Manocha", "abstract": "  We present a new learning approach, Soft Conditional Prompt Learning (SCP),\nwhich leverages the strengths of prompt learning for aerial video action\nrecognition. Our approach is designed to predict the action of each agent by\nhelping the models focus on the descriptions or instructions associated with\nactions in the input videos for aerial/robot visual perception. Our formulation\nsupports various prompts, including learnable prompts, auxiliary visual\ninformation, and large vision models to improve the recognition performance. We\npresent a soft conditional prompt method that learns to dynamically generate\nprompts from a pool of prompt experts under different video inputs. By sharing\nthe same objective with the task, our proposed SCP can optimize prompts that\nguide the model's predictions while explicitly learning input-invariant (prompt\nexperts pool) and input-specific (data-dependent) prompt knowledge. In\npractice, we observe a 3.17-10.2% accuracy improvement on the aerial video\ndatasets (Okutama, NECDrone), which consist of scenes with single-agent and\nmulti-agent actions. We further evaluate our approach on ground camera videos\nto verify the effectiveness and generalization and achieve a 1.0-3.6%\nimprovement on dataset SSV2. We integrate our method into the ROS2 as well.\n", "link": "http://arxiv.org/abs/2305.12437v4", "date": "2024-08-28", "relevancy": 1.5803, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5614}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5445}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SCP%3A%20Soft%20Conditional%20Prompt%20Learning%20for%20Aerial%20Video%20Action%0A%20%20Recognition&body=Title%3A%20SCP%3A%20Soft%20Conditional%20Prompt%20Learning%20for%20Aerial%20Video%20Action%0A%20%20Recognition%0AAuthor%3A%20Xijun%20Wang%20and%20Ruiqi%20Xian%20and%20Tianrui%20Guan%20and%20Fuxiao%20Liu%20and%20Dinesh%20Manocha%0AAbstract%3A%20%20%20We%20present%20a%20new%20learning%20approach%2C%20Soft%20Conditional%20Prompt%20Learning%20%28SCP%29%2C%0Awhich%20leverages%20the%20strengths%20of%20prompt%20learning%20for%20aerial%20video%20action%0Arecognition.%20Our%20approach%20is%20designed%20to%20predict%20the%20action%20of%20each%20agent%20by%0Ahelping%20the%20models%20focus%20on%20the%20descriptions%20or%20instructions%20associated%20with%0Aactions%20in%20the%20input%20videos%20for%20aerial/robot%20visual%20perception.%20Our%20formulation%0Asupports%20various%20prompts%2C%20including%20learnable%20prompts%2C%20auxiliary%20visual%0Ainformation%2C%20and%20large%20vision%20models%20to%20improve%20the%20recognition%20performance.%20We%0Apresent%20a%20soft%20conditional%20prompt%20method%20that%20learns%20to%20dynamically%20generate%0Aprompts%20from%20a%20pool%20of%20prompt%20experts%20under%20different%20video%20inputs.%20By%20sharing%0Athe%20same%20objective%20with%20the%20task%2C%20our%20proposed%20SCP%20can%20optimize%20prompts%20that%0Aguide%20the%20model%27s%20predictions%20while%20explicitly%20learning%20input-invariant%20%28prompt%0Aexperts%20pool%29%20and%20input-specific%20%28data-dependent%29%20prompt%20knowledge.%20In%0Apractice%2C%20we%20observe%20a%203.17-10.2%25%20accuracy%20improvement%20on%20the%20aerial%20video%0Adatasets%20%28Okutama%2C%20NECDrone%29%2C%20which%20consist%20of%20scenes%20with%20single-agent%20and%0Amulti-agent%20actions.%20We%20further%20evaluate%20our%20approach%20on%20ground%20camera%20videos%0Ato%20verify%20the%20effectiveness%20and%20generalization%20and%20achieve%20a%201.0-3.6%25%0Aimprovement%20on%20dataset%20SSV2.%20We%20integrate%20our%20method%20into%20the%20ROS2%20as%20well.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.12437v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSCP%253A%2520Soft%2520Conditional%2520Prompt%2520Learning%2520for%2520Aerial%2520Video%2520Action%250A%2520%2520Recognition%26entry.906535625%3DXijun%2520Wang%2520and%2520Ruiqi%2520Xian%2520and%2520Tianrui%2520Guan%2520and%2520Fuxiao%2520Liu%2520and%2520Dinesh%2520Manocha%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520learning%2520approach%252C%2520Soft%2520Conditional%2520Prompt%2520Learning%2520%2528SCP%2529%252C%250Awhich%2520leverages%2520the%2520strengths%2520of%2520prompt%2520learning%2520for%2520aerial%2520video%2520action%250Arecognition.%2520Our%2520approach%2520is%2520designed%2520to%2520predict%2520the%2520action%2520of%2520each%2520agent%2520by%250Ahelping%2520the%2520models%2520focus%2520on%2520the%2520descriptions%2520or%2520instructions%2520associated%2520with%250Aactions%2520in%2520the%2520input%2520videos%2520for%2520aerial/robot%2520visual%2520perception.%2520Our%2520formulation%250Asupports%2520various%2520prompts%252C%2520including%2520learnable%2520prompts%252C%2520auxiliary%2520visual%250Ainformation%252C%2520and%2520large%2520vision%2520models%2520to%2520improve%2520the%2520recognition%2520performance.%2520We%250Apresent%2520a%2520soft%2520conditional%2520prompt%2520method%2520that%2520learns%2520to%2520dynamically%2520generate%250Aprompts%2520from%2520a%2520pool%2520of%2520prompt%2520experts%2520under%2520different%2520video%2520inputs.%2520By%2520sharing%250Athe%2520same%2520objective%2520with%2520the%2520task%252C%2520our%2520proposed%2520SCP%2520can%2520optimize%2520prompts%2520that%250Aguide%2520the%2520model%2527s%2520predictions%2520while%2520explicitly%2520learning%2520input-invariant%2520%2528prompt%250Aexperts%2520pool%2529%2520and%2520input-specific%2520%2528data-dependent%2529%2520prompt%2520knowledge.%2520In%250Apractice%252C%2520we%2520observe%2520a%25203.17-10.2%2525%2520accuracy%2520improvement%2520on%2520the%2520aerial%2520video%250Adatasets%2520%2528Okutama%252C%2520NECDrone%2529%252C%2520which%2520consist%2520of%2520scenes%2520with%2520single-agent%2520and%250Amulti-agent%2520actions.%2520We%2520further%2520evaluate%2520our%2520approach%2520on%2520ground%2520camera%2520videos%250Ato%2520verify%2520the%2520effectiveness%2520and%2520generalization%2520and%2520achieve%2520a%25201.0-3.6%2525%250Aimprovement%2520on%2520dataset%2520SSV2.%2520We%2520integrate%2520our%2520method%2520into%2520the%2520ROS2%2520as%2520well.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.12437v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCP%3A%20Soft%20Conditional%20Prompt%20Learning%20for%20Aerial%20Video%20Action%0A%20%20Recognition&entry.906535625=Xijun%20Wang%20and%20Ruiqi%20Xian%20and%20Tianrui%20Guan%20and%20Fuxiao%20Liu%20and%20Dinesh%20Manocha&entry.1292438233=%20%20We%20present%20a%20new%20learning%20approach%2C%20Soft%20Conditional%20Prompt%20Learning%20%28SCP%29%2C%0Awhich%20leverages%20the%20strengths%20of%20prompt%20learning%20for%20aerial%20video%20action%0Arecognition.%20Our%20approach%20is%20designed%20to%20predict%20the%20action%20of%20each%20agent%20by%0Ahelping%20the%20models%20focus%20on%20the%20descriptions%20or%20instructions%20associated%20with%0Aactions%20in%20the%20input%20videos%20for%20aerial/robot%20visual%20perception.%20Our%20formulation%0Asupports%20various%20prompts%2C%20including%20learnable%20prompts%2C%20auxiliary%20visual%0Ainformation%2C%20and%20large%20vision%20models%20to%20improve%20the%20recognition%20performance.%20We%0Apresent%20a%20soft%20conditional%20prompt%20method%20that%20learns%20to%20dynamically%20generate%0Aprompts%20from%20a%20pool%20of%20prompt%20experts%20under%20different%20video%20inputs.%20By%20sharing%0Athe%20same%20objective%20with%20the%20task%2C%20our%20proposed%20SCP%20can%20optimize%20prompts%20that%0Aguide%20the%20model%27s%20predictions%20while%20explicitly%20learning%20input-invariant%20%28prompt%0Aexperts%20pool%29%20and%20input-specific%20%28data-dependent%29%20prompt%20knowledge.%20In%0Apractice%2C%20we%20observe%20a%203.17-10.2%25%20accuracy%20improvement%20on%20the%20aerial%20video%0Adatasets%20%28Okutama%2C%20NECDrone%29%2C%20which%20consist%20of%20scenes%20with%20single-agent%20and%0Amulti-agent%20actions.%20We%20further%20evaluate%20our%20approach%20on%20ground%20camera%20videos%0Ato%20verify%20the%20effectiveness%20and%20generalization%20and%20achieve%20a%201.0-3.6%25%0Aimprovement%20on%20dataset%20SSV2.%20We%20integrate%20our%20method%20into%20the%20ROS2%20as%20well.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.12437v4&entry.124074799=Read"},
{"title": "Mamba or Transformer for Time Series Forecasting? Mixture of Universals\n  (MoU) Is All You Need", "author": "Sijia Peng and Yun Xiong and Yangyong Zhu and Zhiqiang Shen", "abstract": "  Time series forecasting requires balancing short-term and long-term\ndependencies for accurate predictions. Existing methods mainly focus on\nlong-term dependency modeling, neglecting the complexities of short-term\ndynamics, which may hinder performance. Transformers are superior in modeling\nlong-term dependencies but are criticized for their quadratic computational\ncost. Mamba provides a near-linear alternative but is reported less effective\nin time series longterm forecasting due to potential information loss. Current\narchitectures fall short in offering both high efficiency and strong\nperformance for long-term dependency modeling. To address these challenges, we\nintroduce Mixture of Universals (MoU), a versatile model to capture both\nshort-term and long-term dependencies for enhancing performance in time series\nforecasting. MoU is composed of two novel designs: Mixture of Feature\nExtractors (MoF), an adaptive method designed to improve time series patch\nrepresentations for short-term dependency, and Mixture of Architectures (MoA),\nwhich hierarchically integrates Mamba, FeedForward, Convolution, and\nSelf-Attention architectures in a specialized order to model long-term\ndependency from a hybrid perspective. The proposed approach achieves\nstate-of-the-art performance while maintaining relatively low computational\ncosts. Extensive experiments on seven real-world datasets demonstrate the\nsuperiority of MoU. Code is available at https://github.com/lunaaa95/mou/.\n", "link": "http://arxiv.org/abs/2408.15997v1", "date": "2024-08-28", "relevancy": 1.4035, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4773}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4704}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mamba%20or%20Transformer%20for%20Time%20Series%20Forecasting%3F%20Mixture%20of%20Universals%0A%20%20%28MoU%29%20Is%20All%20You%20Need&body=Title%3A%20Mamba%20or%20Transformer%20for%20Time%20Series%20Forecasting%3F%20Mixture%20of%20Universals%0A%20%20%28MoU%29%20Is%20All%20You%20Need%0AAuthor%3A%20Sijia%20Peng%20and%20Yun%20Xiong%20and%20Yangyong%20Zhu%20and%20Zhiqiang%20Shen%0AAbstract%3A%20%20%20Time%20series%20forecasting%20requires%20balancing%20short-term%20and%20long-term%0Adependencies%20for%20accurate%20predictions.%20Existing%20methods%20mainly%20focus%20on%0Along-term%20dependency%20modeling%2C%20neglecting%20the%20complexities%20of%20short-term%0Adynamics%2C%20which%20may%20hinder%20performance.%20Transformers%20are%20superior%20in%20modeling%0Along-term%20dependencies%20but%20are%20criticized%20for%20their%20quadratic%20computational%0Acost.%20Mamba%20provides%20a%20near-linear%20alternative%20but%20is%20reported%20less%20effective%0Ain%20time%20series%20longterm%20forecasting%20due%20to%20potential%20information%20loss.%20Current%0Aarchitectures%20fall%20short%20in%20offering%20both%20high%20efficiency%20and%20strong%0Aperformance%20for%20long-term%20dependency%20modeling.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20Mixture%20of%20Universals%20%28MoU%29%2C%20a%20versatile%20model%20to%20capture%20both%0Ashort-term%20and%20long-term%20dependencies%20for%20enhancing%20performance%20in%20time%20series%0Aforecasting.%20MoU%20is%20composed%20of%20two%20novel%20designs%3A%20Mixture%20of%20Feature%0AExtractors%20%28MoF%29%2C%20an%20adaptive%20method%20designed%20to%20improve%20time%20series%20patch%0Arepresentations%20for%20short-term%20dependency%2C%20and%20Mixture%20of%20Architectures%20%28MoA%29%2C%0Awhich%20hierarchically%20integrates%20Mamba%2C%20FeedForward%2C%20Convolution%2C%20and%0ASelf-Attention%20architectures%20in%20a%20specialized%20order%20to%20model%20long-term%0Adependency%20from%20a%20hybrid%20perspective.%20The%20proposed%20approach%20achieves%0Astate-of-the-art%20performance%20while%20maintaining%20relatively%20low%20computational%0Acosts.%20Extensive%20experiments%20on%20seven%20real-world%20datasets%20demonstrate%20the%0Asuperiority%20of%20MoU.%20Code%20is%20available%20at%20https%3A//github.com/lunaaa95/mou/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMamba%2520or%2520Transformer%2520for%2520Time%2520Series%2520Forecasting%253F%2520Mixture%2520of%2520Universals%250A%2520%2520%2528MoU%2529%2520Is%2520All%2520You%2520Need%26entry.906535625%3DSijia%2520Peng%2520and%2520Yun%2520Xiong%2520and%2520Yangyong%2520Zhu%2520and%2520Zhiqiang%2520Shen%26entry.1292438233%3D%2520%2520Time%2520series%2520forecasting%2520requires%2520balancing%2520short-term%2520and%2520long-term%250Adependencies%2520for%2520accurate%2520predictions.%2520Existing%2520methods%2520mainly%2520focus%2520on%250Along-term%2520dependency%2520modeling%252C%2520neglecting%2520the%2520complexities%2520of%2520short-term%250Adynamics%252C%2520which%2520may%2520hinder%2520performance.%2520Transformers%2520are%2520superior%2520in%2520modeling%250Along-term%2520dependencies%2520but%2520are%2520criticized%2520for%2520their%2520quadratic%2520computational%250Acost.%2520Mamba%2520provides%2520a%2520near-linear%2520alternative%2520but%2520is%2520reported%2520less%2520effective%250Ain%2520time%2520series%2520longterm%2520forecasting%2520due%2520to%2520potential%2520information%2520loss.%2520Current%250Aarchitectures%2520fall%2520short%2520in%2520offering%2520both%2520high%2520efficiency%2520and%2520strong%250Aperformance%2520for%2520long-term%2520dependency%2520modeling.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520Mixture%2520of%2520Universals%2520%2528MoU%2529%252C%2520a%2520versatile%2520model%2520to%2520capture%2520both%250Ashort-term%2520and%2520long-term%2520dependencies%2520for%2520enhancing%2520performance%2520in%2520time%2520series%250Aforecasting.%2520MoU%2520is%2520composed%2520of%2520two%2520novel%2520designs%253A%2520Mixture%2520of%2520Feature%250AExtractors%2520%2528MoF%2529%252C%2520an%2520adaptive%2520method%2520designed%2520to%2520improve%2520time%2520series%2520patch%250Arepresentations%2520for%2520short-term%2520dependency%252C%2520and%2520Mixture%2520of%2520Architectures%2520%2528MoA%2529%252C%250Awhich%2520hierarchically%2520integrates%2520Mamba%252C%2520FeedForward%252C%2520Convolution%252C%2520and%250ASelf-Attention%2520architectures%2520in%2520a%2520specialized%2520order%2520to%2520model%2520long-term%250Adependency%2520from%2520a%2520hybrid%2520perspective.%2520The%2520proposed%2520approach%2520achieves%250Astate-of-the-art%2520performance%2520while%2520maintaining%2520relatively%2520low%2520computational%250Acosts.%2520Extensive%2520experiments%2520on%2520seven%2520real-world%2520datasets%2520demonstrate%2520the%250Asuperiority%2520of%2520MoU.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/lunaaa95/mou/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mamba%20or%20Transformer%20for%20Time%20Series%20Forecasting%3F%20Mixture%20of%20Universals%0A%20%20%28MoU%29%20Is%20All%20You%20Need&entry.906535625=Sijia%20Peng%20and%20Yun%20Xiong%20and%20Yangyong%20Zhu%20and%20Zhiqiang%20Shen&entry.1292438233=%20%20Time%20series%20forecasting%20requires%20balancing%20short-term%20and%20long-term%0Adependencies%20for%20accurate%20predictions.%20Existing%20methods%20mainly%20focus%20on%0Along-term%20dependency%20modeling%2C%20neglecting%20the%20complexities%20of%20short-term%0Adynamics%2C%20which%20may%20hinder%20performance.%20Transformers%20are%20superior%20in%20modeling%0Along-term%20dependencies%20but%20are%20criticized%20for%20their%20quadratic%20computational%0Acost.%20Mamba%20provides%20a%20near-linear%20alternative%20but%20is%20reported%20less%20effective%0Ain%20time%20series%20longterm%20forecasting%20due%20to%20potential%20information%20loss.%20Current%0Aarchitectures%20fall%20short%20in%20offering%20both%20high%20efficiency%20and%20strong%0Aperformance%20for%20long-term%20dependency%20modeling.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20Mixture%20of%20Universals%20%28MoU%29%2C%20a%20versatile%20model%20to%20capture%20both%0Ashort-term%20and%20long-term%20dependencies%20for%20enhancing%20performance%20in%20time%20series%0Aforecasting.%20MoU%20is%20composed%20of%20two%20novel%20designs%3A%20Mixture%20of%20Feature%0AExtractors%20%28MoF%29%2C%20an%20adaptive%20method%20designed%20to%20improve%20time%20series%20patch%0Arepresentations%20for%20short-term%20dependency%2C%20and%20Mixture%20of%20Architectures%20%28MoA%29%2C%0Awhich%20hierarchically%20integrates%20Mamba%2C%20FeedForward%2C%20Convolution%2C%20and%0ASelf-Attention%20architectures%20in%20a%20specialized%20order%20to%20model%20long-term%0Adependency%20from%20a%20hybrid%20perspective.%20The%20proposed%20approach%20achieves%0Astate-of-the-art%20performance%20while%20maintaining%20relatively%20low%20computational%0Acosts.%20Extensive%20experiments%20on%20seven%20real-world%20datasets%20demonstrate%20the%0Asuperiority%20of%20MoU.%20Code%20is%20available%20at%20https%3A//github.com/lunaaa95/mou/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15997v1&entry.124074799=Read"},
{"title": "Q-MRS: A Deep Learning Framework for Quantitative Magnetic Resonance\n  Spectra Analysis", "author": "Christopher J. Wu and Lawrence S. Kegeles and Jia Guo", "abstract": "  Magnetic resonance spectroscopy (MRS) is an established technique for\nstudying tissue metabolism, particularly in central nervous system disorders.\nWhile powerful and versatile, MRS is often limited by challenges associated\nwith data quality, processing, and quantification. Existing MRS quantification\nmethods face difficulties in balancing model complexity and reproducibility\nduring spectral modeling, often falling into the trap of either\noversimplification or over-parameterization. To address these limitations, this\nstudy introduces a deep learning (DL) framework that employs transfer learning,\nin which the model is pre-trained on simulated datasets before it undergoes\nfine-tuning on in vivo data. The proposed framework showed promising\nperformance when applied to the Philips dataset from the BIG GABA repository\nand represents an exciting advancement in MRS data analysis.\n", "link": "http://arxiv.org/abs/2408.15999v1", "date": "2024-08-28", "relevancy": 0.9686, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5059}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4755}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Q-MRS%3A%20A%20Deep%20Learning%20Framework%20for%20Quantitative%20Magnetic%20Resonance%0A%20%20Spectra%20Analysis&body=Title%3A%20Q-MRS%3A%20A%20Deep%20Learning%20Framework%20for%20Quantitative%20Magnetic%20Resonance%0A%20%20Spectra%20Analysis%0AAuthor%3A%20Christopher%20J.%20Wu%20and%20Lawrence%20S.%20Kegeles%20and%20Jia%20Guo%0AAbstract%3A%20%20%20Magnetic%20resonance%20spectroscopy%20%28MRS%29%20is%20an%20established%20technique%20for%0Astudying%20tissue%20metabolism%2C%20particularly%20in%20central%20nervous%20system%20disorders.%0AWhile%20powerful%20and%20versatile%2C%20MRS%20is%20often%20limited%20by%20challenges%20associated%0Awith%20data%20quality%2C%20processing%2C%20and%20quantification.%20Existing%20MRS%20quantification%0Amethods%20face%20difficulties%20in%20balancing%20model%20complexity%20and%20reproducibility%0Aduring%20spectral%20modeling%2C%20often%20falling%20into%20the%20trap%20of%20either%0Aoversimplification%20or%20over-parameterization.%20To%20address%20these%20limitations%2C%20this%0Astudy%20introduces%20a%20deep%20learning%20%28DL%29%20framework%20that%20employs%20transfer%20learning%2C%0Ain%20which%20the%20model%20is%20pre-trained%20on%20simulated%20datasets%20before%20it%20undergoes%0Afine-tuning%20on%20in%20vivo%20data.%20The%20proposed%20framework%20showed%20promising%0Aperformance%20when%20applied%20to%20the%20Philips%20dataset%20from%20the%20BIG%20GABA%20repository%0Aand%20represents%20an%20exciting%20advancement%20in%20MRS%20data%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQ-MRS%253A%2520A%2520Deep%2520Learning%2520Framework%2520for%2520Quantitative%2520Magnetic%2520Resonance%250A%2520%2520Spectra%2520Analysis%26entry.906535625%3DChristopher%2520J.%2520Wu%2520and%2520Lawrence%2520S.%2520Kegeles%2520and%2520Jia%2520Guo%26entry.1292438233%3D%2520%2520Magnetic%2520resonance%2520spectroscopy%2520%2528MRS%2529%2520is%2520an%2520established%2520technique%2520for%250Astudying%2520tissue%2520metabolism%252C%2520particularly%2520in%2520central%2520nervous%2520system%2520disorders.%250AWhile%2520powerful%2520and%2520versatile%252C%2520MRS%2520is%2520often%2520limited%2520by%2520challenges%2520associated%250Awith%2520data%2520quality%252C%2520processing%252C%2520and%2520quantification.%2520Existing%2520MRS%2520quantification%250Amethods%2520face%2520difficulties%2520in%2520balancing%2520model%2520complexity%2520and%2520reproducibility%250Aduring%2520spectral%2520modeling%252C%2520often%2520falling%2520into%2520the%2520trap%2520of%2520either%250Aoversimplification%2520or%2520over-parameterization.%2520To%2520address%2520these%2520limitations%252C%2520this%250Astudy%2520introduces%2520a%2520deep%2520learning%2520%2528DL%2529%2520framework%2520that%2520employs%2520transfer%2520learning%252C%250Ain%2520which%2520the%2520model%2520is%2520pre-trained%2520on%2520simulated%2520datasets%2520before%2520it%2520undergoes%250Afine-tuning%2520on%2520in%2520vivo%2520data.%2520The%2520proposed%2520framework%2520showed%2520promising%250Aperformance%2520when%2520applied%2520to%2520the%2520Philips%2520dataset%2520from%2520the%2520BIG%2520GABA%2520repository%250Aand%2520represents%2520an%2520exciting%2520advancement%2520in%2520MRS%2520data%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q-MRS%3A%20A%20Deep%20Learning%20Framework%20for%20Quantitative%20Magnetic%20Resonance%0A%20%20Spectra%20Analysis&entry.906535625=Christopher%20J.%20Wu%20and%20Lawrence%20S.%20Kegeles%20and%20Jia%20Guo&entry.1292438233=%20%20Magnetic%20resonance%20spectroscopy%20%28MRS%29%20is%20an%20established%20technique%20for%0Astudying%20tissue%20metabolism%2C%20particularly%20in%20central%20nervous%20system%20disorders.%0AWhile%20powerful%20and%20versatile%2C%20MRS%20is%20often%20limited%20by%20challenges%20associated%0Awith%20data%20quality%2C%20processing%2C%20and%20quantification.%20Existing%20MRS%20quantification%0Amethods%20face%20difficulties%20in%20balancing%20model%20complexity%20and%20reproducibility%0Aduring%20spectral%20modeling%2C%20often%20falling%20into%20the%20trap%20of%20either%0Aoversimplification%20or%20over-parameterization.%20To%20address%20these%20limitations%2C%20this%0Astudy%20introduces%20a%20deep%20learning%20%28DL%29%20framework%20that%20employs%20transfer%20learning%2C%0Ain%20which%20the%20model%20is%20pre-trained%20on%20simulated%20datasets%20before%20it%20undergoes%0Afine-tuning%20on%20in%20vivo%20data.%20The%20proposed%20framework%20showed%20promising%0Aperformance%20when%20applied%20to%20the%20Philips%20dataset%20from%20the%20BIG%20GABA%20repository%0Aand%20represents%20an%20exciting%20advancement%20in%20MRS%20data%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15999v1&entry.124074799=Read"},
{"title": "Benchmarking foundation models as feature extractors for\n  weakly-supervised computational pathology", "author": "Peter Neidlinger and Omar S. M. El Nahhas and Hannah Sophie Muti and Tim Lenz and Michael Hoffmeister and Hermann Brenner and Marko van Treeck and Rupert Langer and Bastian Dislich and Hans Michael Behrens and Christoph R\u00f6cken and Sebastian Foersch and Daniel Truhn and Antonio Marra and Oliver Lester Saldanha and Jakob Nikolas Kather", "abstract": "  Advancements in artificial intelligence have driven the development of\nnumerous pathology foundation models capable of extracting clinically relevant\ninformation. However, there is currently limited literature independently\nevaluating these foundation models on truly external cohorts and\nclinically-relevant tasks to uncover adjustments for future improvements. In\nthis study, we benchmarked ten histopathology foundation models on 13 patient\ncohorts with 6,791 patients and 9,493 slides from lung, colorectal, gastric,\nand breast cancers. The models were evaluated on weakly-supervised tasks\nrelated to biomarkers, morphological properties, and prognostic outcomes. We\nshow that a vision-language foundation model, CONCH, yielded the highest\nperformance in 42% of tasks when compared to vision-only foundation models. The\nexperiments reveal that foundation models trained on distinct cohorts learn\ncomplementary features to predict the same label, and can be fused to\noutperform the current state of the art. Creating an ensemble of complementary\nfoundation models outperformed CONCH in 66% of tasks. Moreover, our findings\nsuggest that data diversity outweighs data volume for foundation models. Our\nwork highlights actionable adjustments to improve pathology foundation models.\n", "link": "http://arxiv.org/abs/2408.15823v1", "date": "2024-08-28", "relevancy": 0.9267, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4755}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4588}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20foundation%20models%20as%20feature%20extractors%20for%0A%20%20weakly-supervised%20computational%20pathology&body=Title%3A%20Benchmarking%20foundation%20models%20as%20feature%20extractors%20for%0A%20%20weakly-supervised%20computational%20pathology%0AAuthor%3A%20Peter%20Neidlinger%20and%20Omar%20S.%20M.%20El%20Nahhas%20and%20Hannah%20Sophie%20Muti%20and%20Tim%20Lenz%20and%20Michael%20Hoffmeister%20and%20Hermann%20Brenner%20and%20Marko%20van%20Treeck%20and%20Rupert%20Langer%20and%20Bastian%20Dislich%20and%20Hans%20Michael%20Behrens%20and%20Christoph%20R%C3%B6cken%20and%20Sebastian%20Foersch%20and%20Daniel%20Truhn%20and%20Antonio%20Marra%20and%20Oliver%20Lester%20Saldanha%20and%20Jakob%20Nikolas%20Kather%0AAbstract%3A%20%20%20Advancements%20in%20artificial%20intelligence%20have%20driven%20the%20development%20of%0Anumerous%20pathology%20foundation%20models%20capable%20of%20extracting%20clinically%20relevant%0Ainformation.%20However%2C%20there%20is%20currently%20limited%20literature%20independently%0Aevaluating%20these%20foundation%20models%20on%20truly%20external%20cohorts%20and%0Aclinically-relevant%20tasks%20to%20uncover%20adjustments%20for%20future%20improvements.%20In%0Athis%20study%2C%20we%20benchmarked%20ten%20histopathology%20foundation%20models%20on%2013%20patient%0Acohorts%20with%206%2C791%20patients%20and%209%2C493%20slides%20from%20lung%2C%20colorectal%2C%20gastric%2C%0Aand%20breast%20cancers.%20The%20models%20were%20evaluated%20on%20weakly-supervised%20tasks%0Arelated%20to%20biomarkers%2C%20morphological%20properties%2C%20and%20prognostic%20outcomes.%20We%0Ashow%20that%20a%20vision-language%20foundation%20model%2C%20CONCH%2C%20yielded%20the%20highest%0Aperformance%20in%2042%25%20of%20tasks%20when%20compared%20to%20vision-only%20foundation%20models.%20The%0Aexperiments%20reveal%20that%20foundation%20models%20trained%20on%20distinct%20cohorts%20learn%0Acomplementary%20features%20to%20predict%20the%20same%20label%2C%20and%20can%20be%20fused%20to%0Aoutperform%20the%20current%20state%20of%20the%20art.%20Creating%20an%20ensemble%20of%20complementary%0Afoundation%20models%20outperformed%20CONCH%20in%2066%25%20of%20tasks.%20Moreover%2C%20our%20findings%0Asuggest%20that%20data%20diversity%20outweighs%20data%20volume%20for%20foundation%20models.%20Our%0Awork%20highlights%20actionable%20adjustments%20to%20improve%20pathology%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15823v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520foundation%2520models%2520as%2520feature%2520extractors%2520for%250A%2520%2520weakly-supervised%2520computational%2520pathology%26entry.906535625%3DPeter%2520Neidlinger%2520and%2520Omar%2520S.%2520M.%2520El%2520Nahhas%2520and%2520Hannah%2520Sophie%2520Muti%2520and%2520Tim%2520Lenz%2520and%2520Michael%2520Hoffmeister%2520and%2520Hermann%2520Brenner%2520and%2520Marko%2520van%2520Treeck%2520and%2520Rupert%2520Langer%2520and%2520Bastian%2520Dislich%2520and%2520Hans%2520Michael%2520Behrens%2520and%2520Christoph%2520R%25C3%25B6cken%2520and%2520Sebastian%2520Foersch%2520and%2520Daniel%2520Truhn%2520and%2520Antonio%2520Marra%2520and%2520Oliver%2520Lester%2520Saldanha%2520and%2520Jakob%2520Nikolas%2520Kather%26entry.1292438233%3D%2520%2520Advancements%2520in%2520artificial%2520intelligence%2520have%2520driven%2520the%2520development%2520of%250Anumerous%2520pathology%2520foundation%2520models%2520capable%2520of%2520extracting%2520clinically%2520relevant%250Ainformation.%2520However%252C%2520there%2520is%2520currently%2520limited%2520literature%2520independently%250Aevaluating%2520these%2520foundation%2520models%2520on%2520truly%2520external%2520cohorts%2520and%250Aclinically-relevant%2520tasks%2520to%2520uncover%2520adjustments%2520for%2520future%2520improvements.%2520In%250Athis%2520study%252C%2520we%2520benchmarked%2520ten%2520histopathology%2520foundation%2520models%2520on%252013%2520patient%250Acohorts%2520with%25206%252C791%2520patients%2520and%25209%252C493%2520slides%2520from%2520lung%252C%2520colorectal%252C%2520gastric%252C%250Aand%2520breast%2520cancers.%2520The%2520models%2520were%2520evaluated%2520on%2520weakly-supervised%2520tasks%250Arelated%2520to%2520biomarkers%252C%2520morphological%2520properties%252C%2520and%2520prognostic%2520outcomes.%2520We%250Ashow%2520that%2520a%2520vision-language%2520foundation%2520model%252C%2520CONCH%252C%2520yielded%2520the%2520highest%250Aperformance%2520in%252042%2525%2520of%2520tasks%2520when%2520compared%2520to%2520vision-only%2520foundation%2520models.%2520The%250Aexperiments%2520reveal%2520that%2520foundation%2520models%2520trained%2520on%2520distinct%2520cohorts%2520learn%250Acomplementary%2520features%2520to%2520predict%2520the%2520same%2520label%252C%2520and%2520can%2520be%2520fused%2520to%250Aoutperform%2520the%2520current%2520state%2520of%2520the%2520art.%2520Creating%2520an%2520ensemble%2520of%2520complementary%250Afoundation%2520models%2520outperformed%2520CONCH%2520in%252066%2525%2520of%2520tasks.%2520Moreover%252C%2520our%2520findings%250Asuggest%2520that%2520data%2520diversity%2520outweighs%2520data%2520volume%2520for%2520foundation%2520models.%2520Our%250Awork%2520highlights%2520actionable%2520adjustments%2520to%2520improve%2520pathology%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15823v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20foundation%20models%20as%20feature%20extractors%20for%0A%20%20weakly-supervised%20computational%20pathology&entry.906535625=Peter%20Neidlinger%20and%20Omar%20S.%20M.%20El%20Nahhas%20and%20Hannah%20Sophie%20Muti%20and%20Tim%20Lenz%20and%20Michael%20Hoffmeister%20and%20Hermann%20Brenner%20and%20Marko%20van%20Treeck%20and%20Rupert%20Langer%20and%20Bastian%20Dislich%20and%20Hans%20Michael%20Behrens%20and%20Christoph%20R%C3%B6cken%20and%20Sebastian%20Foersch%20and%20Daniel%20Truhn%20and%20Antonio%20Marra%20and%20Oliver%20Lester%20Saldanha%20and%20Jakob%20Nikolas%20Kather&entry.1292438233=%20%20Advancements%20in%20artificial%20intelligence%20have%20driven%20the%20development%20of%0Anumerous%20pathology%20foundation%20models%20capable%20of%20extracting%20clinically%20relevant%0Ainformation.%20However%2C%20there%20is%20currently%20limited%20literature%20independently%0Aevaluating%20these%20foundation%20models%20on%20truly%20external%20cohorts%20and%0Aclinically-relevant%20tasks%20to%20uncover%20adjustments%20for%20future%20improvements.%20In%0Athis%20study%2C%20we%20benchmarked%20ten%20histopathology%20foundation%20models%20on%2013%20patient%0Acohorts%20with%206%2C791%20patients%20and%209%2C493%20slides%20from%20lung%2C%20colorectal%2C%20gastric%2C%0Aand%20breast%20cancers.%20The%20models%20were%20evaluated%20on%20weakly-supervised%20tasks%0Arelated%20to%20biomarkers%2C%20morphological%20properties%2C%20and%20prognostic%20outcomes.%20We%0Ashow%20that%20a%20vision-language%20foundation%20model%2C%20CONCH%2C%20yielded%20the%20highest%0Aperformance%20in%2042%25%20of%20tasks%20when%20compared%20to%20vision-only%20foundation%20models.%20The%0Aexperiments%20reveal%20that%20foundation%20models%20trained%20on%20distinct%20cohorts%20learn%0Acomplementary%20features%20to%20predict%20the%20same%20label%2C%20and%20can%20be%20fused%20to%0Aoutperform%20the%20current%20state%20of%20the%20art.%20Creating%20an%20ensemble%20of%20complementary%0Afoundation%20models%20outperformed%20CONCH%20in%2066%25%20of%20tasks.%20Moreover%2C%20our%20findings%0Asuggest%20that%20data%20diversity%20outweighs%20data%20volume%20for%20foundation%20models.%20Our%0Awork%20highlights%20actionable%20adjustments%20to%20improve%20pathology%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15823v1&entry.124074799=Read"},
{"title": "Language-specific Calibration for Pruning Multilingual Language Models", "author": "Simon Kurz and Jian-Jia Chen and Lucie Flek and Zhixue Zhao", "abstract": "  Recent advances in large language model (LLM) pruning have shown\nstate-of-the-art compression results in post-training and retraining-free\nsettings while maintaining high predictive performance. However, such research\nmainly considers calibrating pruning using English text, despite the\nmultilingual nature of modern LLMs and their frequent uses in non-English\nlanguages. In this paper, we set out to explore effective strategies for\ncalibrating the pruning of multilingual language models. We present the first\ncomprehensive empirical study, comparing different calibration languages for\npruning multilingual models across diverse tasks, models, and state-of-the-art\npruning techniques. Our results present practical suggestions, for example,\ncalibrating in the target language can efficiently yield lower perplexity, but\ndoes not necessarily benefit downstream tasks. Our further analysis experiments\nunveil that calibration in the target language mainly contributes to preserving\nlanguage-specific features related to fluency and coherence, but might not\ncontribute to capturing language-agnostic features such as language\nunderstanding and reasoning. Last, we provide practical recommendations for\nfuture practitioners.\n", "link": "http://arxiv.org/abs/2408.14398v2", "date": "2024-08-28", "relevancy": 1.6691, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4267}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.418}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-specific%20Calibration%20for%20Pruning%20Multilingual%20Language%20Models&body=Title%3A%20Language-specific%20Calibration%20for%20Pruning%20Multilingual%20Language%20Models%0AAuthor%3A%20Simon%20Kurz%20and%20Jian-Jia%20Chen%20and%20Lucie%20Flek%20and%20Zhixue%20Zhao%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20model%20%28LLM%29%20pruning%20have%20shown%0Astate-of-the-art%20compression%20results%20in%20post-training%20and%20retraining-free%0Asettings%20while%20maintaining%20high%20predictive%20performance.%20However%2C%20such%20research%0Amainly%20considers%20calibrating%20pruning%20using%20English%20text%2C%20despite%20the%0Amultilingual%20nature%20of%20modern%20LLMs%20and%20their%20frequent%20uses%20in%20non-English%0Alanguages.%20In%20this%20paper%2C%20we%20set%20out%20to%20explore%20effective%20strategies%20for%0Acalibrating%20the%20pruning%20of%20multilingual%20language%20models.%20We%20present%20the%20first%0Acomprehensive%20empirical%20study%2C%20comparing%20different%20calibration%20languages%20for%0Apruning%20multilingual%20models%20across%20diverse%20tasks%2C%20models%2C%20and%20state-of-the-art%0Apruning%20techniques.%20Our%20results%20present%20practical%20suggestions%2C%20for%20example%2C%0Acalibrating%20in%20the%20target%20language%20can%20efficiently%20yield%20lower%20perplexity%2C%20but%0Adoes%20not%20necessarily%20benefit%20downstream%20tasks.%20Our%20further%20analysis%20experiments%0Aunveil%20that%20calibration%20in%20the%20target%20language%20mainly%20contributes%20to%20preserving%0Alanguage-specific%20features%20related%20to%20fluency%20and%20coherence%2C%20but%20might%20not%0Acontribute%20to%20capturing%20language-agnostic%20features%20such%20as%20language%0Aunderstanding%20and%20reasoning.%20Last%2C%20we%20provide%20practical%20recommendations%20for%0Afuture%20practitioners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14398v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-specific%2520Calibration%2520for%2520Pruning%2520Multilingual%2520Language%2520Models%26entry.906535625%3DSimon%2520Kurz%2520and%2520Jian-Jia%2520Chen%2520and%2520Lucie%2520Flek%2520and%2520Zhixue%2520Zhao%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520model%2520%2528LLM%2529%2520pruning%2520have%2520shown%250Astate-of-the-art%2520compression%2520results%2520in%2520post-training%2520and%2520retraining-free%250Asettings%2520while%2520maintaining%2520high%2520predictive%2520performance.%2520However%252C%2520such%2520research%250Amainly%2520considers%2520calibrating%2520pruning%2520using%2520English%2520text%252C%2520despite%2520the%250Amultilingual%2520nature%2520of%2520modern%2520LLMs%2520and%2520their%2520frequent%2520uses%2520in%2520non-English%250Alanguages.%2520In%2520this%2520paper%252C%2520we%2520set%2520out%2520to%2520explore%2520effective%2520strategies%2520for%250Acalibrating%2520the%2520pruning%2520of%2520multilingual%2520language%2520models.%2520We%2520present%2520the%2520first%250Acomprehensive%2520empirical%2520study%252C%2520comparing%2520different%2520calibration%2520languages%2520for%250Apruning%2520multilingual%2520models%2520across%2520diverse%2520tasks%252C%2520models%252C%2520and%2520state-of-the-art%250Apruning%2520techniques.%2520Our%2520results%2520present%2520practical%2520suggestions%252C%2520for%2520example%252C%250Acalibrating%2520in%2520the%2520target%2520language%2520can%2520efficiently%2520yield%2520lower%2520perplexity%252C%2520but%250Adoes%2520not%2520necessarily%2520benefit%2520downstream%2520tasks.%2520Our%2520further%2520analysis%2520experiments%250Aunveil%2520that%2520calibration%2520in%2520the%2520target%2520language%2520mainly%2520contributes%2520to%2520preserving%250Alanguage-specific%2520features%2520related%2520to%2520fluency%2520and%2520coherence%252C%2520but%2520might%2520not%250Acontribute%2520to%2520capturing%2520language-agnostic%2520features%2520such%2520as%2520language%250Aunderstanding%2520and%2520reasoning.%2520Last%252C%2520we%2520provide%2520practical%2520recommendations%2520for%250Afuture%2520practitioners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14398v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-specific%20Calibration%20for%20Pruning%20Multilingual%20Language%20Models&entry.906535625=Simon%20Kurz%20and%20Jian-Jia%20Chen%20and%20Lucie%20Flek%20and%20Zhixue%20Zhao&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20model%20%28LLM%29%20pruning%20have%20shown%0Astate-of-the-art%20compression%20results%20in%20post-training%20and%20retraining-free%0Asettings%20while%20maintaining%20high%20predictive%20performance.%20However%2C%20such%20research%0Amainly%20considers%20calibrating%20pruning%20using%20English%20text%2C%20despite%20the%0Amultilingual%20nature%20of%20modern%20LLMs%20and%20their%20frequent%20uses%20in%20non-English%0Alanguages.%20In%20this%20paper%2C%20we%20set%20out%20to%20explore%20effective%20strategies%20for%0Acalibrating%20the%20pruning%20of%20multilingual%20language%20models.%20We%20present%20the%20first%0Acomprehensive%20empirical%20study%2C%20comparing%20different%20calibration%20languages%20for%0Apruning%20multilingual%20models%20across%20diverse%20tasks%2C%20models%2C%20and%20state-of-the-art%0Apruning%20techniques.%20Our%20results%20present%20practical%20suggestions%2C%20for%20example%2C%0Acalibrating%20in%20the%20target%20language%20can%20efficiently%20yield%20lower%20perplexity%2C%20but%0Adoes%20not%20necessarily%20benefit%20downstream%20tasks.%20Our%20further%20analysis%20experiments%0Aunveil%20that%20calibration%20in%20the%20target%20language%20mainly%20contributes%20to%20preserving%0Alanguage-specific%20features%20related%20to%20fluency%20and%20coherence%2C%20but%20might%20not%0Acontribute%20to%20capturing%20language-agnostic%20features%20such%20as%20language%0Aunderstanding%20and%20reasoning.%20Last%2C%20we%20provide%20practical%20recommendations%20for%0Afuture%20practitioners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14398v2&entry.124074799=Read"},
{"title": "Fall Detection for Smart Living using YOLOv5", "author": "Gracile Astlin Pereira", "abstract": "  This work introduces a fall detection system using the YOLOv5mu model, which\nachieved a mean average precision (mAP) of 0.995, demonstrating exceptional\naccuracy in identifying fall events within smart home environments. Enhanced by\nadvanced data augmentation techniques, the model demonstrates significant\nrobustness and adaptability across various conditions. The integration of\nYOLOv5mu offers precise, real-time fall detection, which is crucial for\nimproving safety and emergency response for residents. Future research will\nfocus on refining the system by incorporating contextual data and exploring\nmulti-sensor approaches to enhance its performance and practical applicability\nin diverse environments.\n", "link": "http://arxiv.org/abs/2408.15955v1", "date": "2024-08-28", "relevancy": 1.4252, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5056}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4664}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fall%20Detection%20for%20Smart%20Living%20using%20YOLOv5&body=Title%3A%20Fall%20Detection%20for%20Smart%20Living%20using%20YOLOv5%0AAuthor%3A%20Gracile%20Astlin%20Pereira%0AAbstract%3A%20%20%20This%20work%20introduces%20a%20fall%20detection%20system%20using%20the%20YOLOv5mu%20model%2C%20which%0Aachieved%20a%20mean%20average%20precision%20%28mAP%29%20of%200.995%2C%20demonstrating%20exceptional%0Aaccuracy%20in%20identifying%20fall%20events%20within%20smart%20home%20environments.%20Enhanced%20by%0Aadvanced%20data%20augmentation%20techniques%2C%20the%20model%20demonstrates%20significant%0Arobustness%20and%20adaptability%20across%20various%20conditions.%20The%20integration%20of%0AYOLOv5mu%20offers%20precise%2C%20real-time%20fall%20detection%2C%20which%20is%20crucial%20for%0Aimproving%20safety%20and%20emergency%20response%20for%20residents.%20Future%20research%20will%0Afocus%20on%20refining%20the%20system%20by%20incorporating%20contextual%20data%20and%20exploring%0Amulti-sensor%20approaches%20to%20enhance%20its%20performance%20and%20practical%20applicability%0Ain%20diverse%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFall%2520Detection%2520for%2520Smart%2520Living%2520using%2520YOLOv5%26entry.906535625%3DGracile%2520Astlin%2520Pereira%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520a%2520fall%2520detection%2520system%2520using%2520the%2520YOLOv5mu%2520model%252C%2520which%250Aachieved%2520a%2520mean%2520average%2520precision%2520%2528mAP%2529%2520of%25200.995%252C%2520demonstrating%2520exceptional%250Aaccuracy%2520in%2520identifying%2520fall%2520events%2520within%2520smart%2520home%2520environments.%2520Enhanced%2520by%250Aadvanced%2520data%2520augmentation%2520techniques%252C%2520the%2520model%2520demonstrates%2520significant%250Arobustness%2520and%2520adaptability%2520across%2520various%2520conditions.%2520The%2520integration%2520of%250AYOLOv5mu%2520offers%2520precise%252C%2520real-time%2520fall%2520detection%252C%2520which%2520is%2520crucial%2520for%250Aimproving%2520safety%2520and%2520emergency%2520response%2520for%2520residents.%2520Future%2520research%2520will%250Afocus%2520on%2520refining%2520the%2520system%2520by%2520incorporating%2520contextual%2520data%2520and%2520exploring%250Amulti-sensor%2520approaches%2520to%2520enhance%2520its%2520performance%2520and%2520practical%2520applicability%250Ain%2520diverse%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fall%20Detection%20for%20Smart%20Living%20using%20YOLOv5&entry.906535625=Gracile%20Astlin%20Pereira&entry.1292438233=%20%20This%20work%20introduces%20a%20fall%20detection%20system%20using%20the%20YOLOv5mu%20model%2C%20which%0Aachieved%20a%20mean%20average%20precision%20%28mAP%29%20of%200.995%2C%20demonstrating%20exceptional%0Aaccuracy%20in%20identifying%20fall%20events%20within%20smart%20home%20environments.%20Enhanced%20by%0Aadvanced%20data%20augmentation%20techniques%2C%20the%20model%20demonstrates%20significant%0Arobustness%20and%20adaptability%20across%20various%20conditions.%20The%20integration%20of%0AYOLOv5mu%20offers%20precise%2C%20real-time%20fall%20detection%2C%20which%20is%20crucial%20for%0Aimproving%20safety%20and%20emergency%20response%20for%20residents.%20Future%20research%20will%0Afocus%20on%20refining%20the%20system%20by%20incorporating%20contextual%20data%20and%20exploring%0Amulti-sensor%20approaches%20to%20enhance%20its%20performance%20and%20practical%20applicability%0Ain%20diverse%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15955v1&entry.124074799=Read"},
{"title": "Analysis of Diagnostics (Part I): Prevalence, Uncertainty\n  Quantification, and Machine Learning", "author": "Paul N. Patrone and Raquel A. Binder and Catherine S. Forconi and Ann M. Moormann and Anthony J. Kearsley", "abstract": "  Diagnostic testing provides a unique setting for studying and developing\ntools in classification theory. In such contexts, the concept of prevalence,\ni.e. the number of individuals with a given condition, is fundamental, both as\nan inherent quantity of interest and as a parameter that controls\nclassification accuracy. This manuscript is the first in a two-part series that\nstudies deeper connections between classification theory and prevalence,\nshowing how the latter establishes a more complete theory of uncertainty\nquantification (UQ) for certain types of machine learning (ML). We motivate\nthis analysis via a lemma demonstrating that general classifiers minimizing a\nprevalence-weighted error contain the same probabilistic information as\nBayes-optimal classifiers, which depend on conditional probability densities.\nThis leads us to study relative probability level-sets $B^\\star (q)$, which are\nreinterpreted as both classification boundaries and useful tools for\nquantifying uncertainty in class labels. To realize this in practice, we also\npropose a numerical, homotopy algorithm that estimates the $B^\\star (q)$ by\nminimizing a prevalence-weighted empirical error. The successes and\nshortcomings of this method motivate us to revisit properties of the level\nsets, and we deduce the corresponding classifiers obey a useful monotonicity\nproperty that stabilizes the numerics and points to important extensions to UQ\nof ML. Throughout, we validate our methods in the context of synthetic data and\na research-use-only SARS-CoV-2 enzyme-linked immunosorbent (ELISA) assay.\n", "link": "http://arxiv.org/abs/2309.00645v2", "date": "2024-08-28", "relevancy": 1.4102, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4894}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4682}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20of%20Diagnostics%20%28Part%20I%29%3A%20Prevalence%2C%20Uncertainty%0A%20%20Quantification%2C%20and%20Machine%20Learning&body=Title%3A%20Analysis%20of%20Diagnostics%20%28Part%20I%29%3A%20Prevalence%2C%20Uncertainty%0A%20%20Quantification%2C%20and%20Machine%20Learning%0AAuthor%3A%20Paul%20N.%20Patrone%20and%20Raquel%20A.%20Binder%20and%20Catherine%20S.%20Forconi%20and%20Ann%20M.%20Moormann%20and%20Anthony%20J.%20Kearsley%0AAbstract%3A%20%20%20Diagnostic%20testing%20provides%20a%20unique%20setting%20for%20studying%20and%20developing%0Atools%20in%20classification%20theory.%20In%20such%20contexts%2C%20the%20concept%20of%20prevalence%2C%0Ai.e.%20the%20number%20of%20individuals%20with%20a%20given%20condition%2C%20is%20fundamental%2C%20both%20as%0Aan%20inherent%20quantity%20of%20interest%20and%20as%20a%20parameter%20that%20controls%0Aclassification%20accuracy.%20This%20manuscript%20is%20the%20first%20in%20a%20two-part%20series%20that%0Astudies%20deeper%20connections%20between%20classification%20theory%20and%20prevalence%2C%0Ashowing%20how%20the%20latter%20establishes%20a%20more%20complete%20theory%20of%20uncertainty%0Aquantification%20%28UQ%29%20for%20certain%20types%20of%20machine%20learning%20%28ML%29.%20We%20motivate%0Athis%20analysis%20via%20a%20lemma%20demonstrating%20that%20general%20classifiers%20minimizing%20a%0Aprevalence-weighted%20error%20contain%20the%20same%20probabilistic%20information%20as%0ABayes-optimal%20classifiers%2C%20which%20depend%20on%20conditional%20probability%20densities.%0AThis%20leads%20us%20to%20study%20relative%20probability%20level-sets%20%24B%5E%5Cstar%20%28q%29%24%2C%20which%20are%0Areinterpreted%20as%20both%20classification%20boundaries%20and%20useful%20tools%20for%0Aquantifying%20uncertainty%20in%20class%20labels.%20To%20realize%20this%20in%20practice%2C%20we%20also%0Apropose%20a%20numerical%2C%20homotopy%20algorithm%20that%20estimates%20the%20%24B%5E%5Cstar%20%28q%29%24%20by%0Aminimizing%20a%20prevalence-weighted%20empirical%20error.%20The%20successes%20and%0Ashortcomings%20of%20this%20method%20motivate%20us%20to%20revisit%20properties%20of%20the%20level%0Asets%2C%20and%20we%20deduce%20the%20corresponding%20classifiers%20obey%20a%20useful%20monotonicity%0Aproperty%20that%20stabilizes%20the%20numerics%20and%20points%20to%20important%20extensions%20to%20UQ%0Aof%20ML.%20Throughout%2C%20we%20validate%20our%20methods%20in%20the%20context%20of%20synthetic%20data%20and%0Aa%20research-use-only%20SARS-CoV-2%20enzyme-linked%20immunosorbent%20%28ELISA%29%20assay.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.00645v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520of%2520Diagnostics%2520%2528Part%2520I%2529%253A%2520Prevalence%252C%2520Uncertainty%250A%2520%2520Quantification%252C%2520and%2520Machine%2520Learning%26entry.906535625%3DPaul%2520N.%2520Patrone%2520and%2520Raquel%2520A.%2520Binder%2520and%2520Catherine%2520S.%2520Forconi%2520and%2520Ann%2520M.%2520Moormann%2520and%2520Anthony%2520J.%2520Kearsley%26entry.1292438233%3D%2520%2520Diagnostic%2520testing%2520provides%2520a%2520unique%2520setting%2520for%2520studying%2520and%2520developing%250Atools%2520in%2520classification%2520theory.%2520In%2520such%2520contexts%252C%2520the%2520concept%2520of%2520prevalence%252C%250Ai.e.%2520the%2520number%2520of%2520individuals%2520with%2520a%2520given%2520condition%252C%2520is%2520fundamental%252C%2520both%2520as%250Aan%2520inherent%2520quantity%2520of%2520interest%2520and%2520as%2520a%2520parameter%2520that%2520controls%250Aclassification%2520accuracy.%2520This%2520manuscript%2520is%2520the%2520first%2520in%2520a%2520two-part%2520series%2520that%250Astudies%2520deeper%2520connections%2520between%2520classification%2520theory%2520and%2520prevalence%252C%250Ashowing%2520how%2520the%2520latter%2520establishes%2520a%2520more%2520complete%2520theory%2520of%2520uncertainty%250Aquantification%2520%2528UQ%2529%2520for%2520certain%2520types%2520of%2520machine%2520learning%2520%2528ML%2529.%2520We%2520motivate%250Athis%2520analysis%2520via%2520a%2520lemma%2520demonstrating%2520that%2520general%2520classifiers%2520minimizing%2520a%250Aprevalence-weighted%2520error%2520contain%2520the%2520same%2520probabilistic%2520information%2520as%250ABayes-optimal%2520classifiers%252C%2520which%2520depend%2520on%2520conditional%2520probability%2520densities.%250AThis%2520leads%2520us%2520to%2520study%2520relative%2520probability%2520level-sets%2520%2524B%255E%255Cstar%2520%2528q%2529%2524%252C%2520which%2520are%250Areinterpreted%2520as%2520both%2520classification%2520boundaries%2520and%2520useful%2520tools%2520for%250Aquantifying%2520uncertainty%2520in%2520class%2520labels.%2520To%2520realize%2520this%2520in%2520practice%252C%2520we%2520also%250Apropose%2520a%2520numerical%252C%2520homotopy%2520algorithm%2520that%2520estimates%2520the%2520%2524B%255E%255Cstar%2520%2528q%2529%2524%2520by%250Aminimizing%2520a%2520prevalence-weighted%2520empirical%2520error.%2520The%2520successes%2520and%250Ashortcomings%2520of%2520this%2520method%2520motivate%2520us%2520to%2520revisit%2520properties%2520of%2520the%2520level%250Asets%252C%2520and%2520we%2520deduce%2520the%2520corresponding%2520classifiers%2520obey%2520a%2520useful%2520monotonicity%250Aproperty%2520that%2520stabilizes%2520the%2520numerics%2520and%2520points%2520to%2520important%2520extensions%2520to%2520UQ%250Aof%2520ML.%2520Throughout%252C%2520we%2520validate%2520our%2520methods%2520in%2520the%2520context%2520of%2520synthetic%2520data%2520and%250Aa%2520research-use-only%2520SARS-CoV-2%2520enzyme-linked%2520immunosorbent%2520%2528ELISA%2529%2520assay.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.00645v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20of%20Diagnostics%20%28Part%20I%29%3A%20Prevalence%2C%20Uncertainty%0A%20%20Quantification%2C%20and%20Machine%20Learning&entry.906535625=Paul%20N.%20Patrone%20and%20Raquel%20A.%20Binder%20and%20Catherine%20S.%20Forconi%20and%20Ann%20M.%20Moormann%20and%20Anthony%20J.%20Kearsley&entry.1292438233=%20%20Diagnostic%20testing%20provides%20a%20unique%20setting%20for%20studying%20and%20developing%0Atools%20in%20classification%20theory.%20In%20such%20contexts%2C%20the%20concept%20of%20prevalence%2C%0Ai.e.%20the%20number%20of%20individuals%20with%20a%20given%20condition%2C%20is%20fundamental%2C%20both%20as%0Aan%20inherent%20quantity%20of%20interest%20and%20as%20a%20parameter%20that%20controls%0Aclassification%20accuracy.%20This%20manuscript%20is%20the%20first%20in%20a%20two-part%20series%20that%0Astudies%20deeper%20connections%20between%20classification%20theory%20and%20prevalence%2C%0Ashowing%20how%20the%20latter%20establishes%20a%20more%20complete%20theory%20of%20uncertainty%0Aquantification%20%28UQ%29%20for%20certain%20types%20of%20machine%20learning%20%28ML%29.%20We%20motivate%0Athis%20analysis%20via%20a%20lemma%20demonstrating%20that%20general%20classifiers%20minimizing%20a%0Aprevalence-weighted%20error%20contain%20the%20same%20probabilistic%20information%20as%0ABayes-optimal%20classifiers%2C%20which%20depend%20on%20conditional%20probability%20densities.%0AThis%20leads%20us%20to%20study%20relative%20probability%20level-sets%20%24B%5E%5Cstar%20%28q%29%24%2C%20which%20are%0Areinterpreted%20as%20both%20classification%20boundaries%20and%20useful%20tools%20for%0Aquantifying%20uncertainty%20in%20class%20labels.%20To%20realize%20this%20in%20practice%2C%20we%20also%0Apropose%20a%20numerical%2C%20homotopy%20algorithm%20that%20estimates%20the%20%24B%5E%5Cstar%20%28q%29%24%20by%0Aminimizing%20a%20prevalence-weighted%20empirical%20error.%20The%20successes%20and%0Ashortcomings%20of%20this%20method%20motivate%20us%20to%20revisit%20properties%20of%20the%20level%0Asets%2C%20and%20we%20deduce%20the%20corresponding%20classifiers%20obey%20a%20useful%20monotonicity%0Aproperty%20that%20stabilizes%20the%20numerics%20and%20points%20to%20important%20extensions%20to%20UQ%0Aof%20ML.%20Throughout%2C%20we%20validate%20our%20methods%20in%20the%20context%20of%20synthetic%20data%20and%0Aa%20research-use-only%20SARS-CoV-2%20enzyme-linked%20immunosorbent%20%28ELISA%29%20assay.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.00645v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


