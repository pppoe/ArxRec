<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241031.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "WildGaussians: 3D Gaussian Splatting in the Wild", "author": "Jonas Kulhanek and Songyou Peng and Zuzana Kukelova and Marc Pollefeys and Torsten Sattler", "abstract": "  While the field of 3D scene reconstruction is dominated by NeRFs due to their\nphotorealistic quality, 3D Gaussian Splatting (3DGS) has recently emerged,\noffering similar quality with real-time rendering speeds. However, both methods\nprimarily excel with well-controlled 3D scenes, while in-the-wild data -\ncharacterized by occlusions, dynamic objects, and varying illumination -\nremains challenging. NeRFs can adapt to such conditions easily through\nper-image embedding vectors, but 3DGS struggles due to its explicit\nrepresentation and lack of shared parameters. To address this, we introduce\nWildGaussians, a novel approach to handle occlusions and appearance changes\nwith 3DGS. By leveraging robust DINO features and integrating an appearance\nmodeling module within 3DGS, our method achieves state-of-the-art results. We\ndemonstrate that WildGaussians matches the real-time rendering speed of 3DGS\nwhile surpassing both 3DGS and NeRF baselines in handling in-the-wild data, all\nwithin a simple architectural framework.\n", "link": "http://arxiv.org/abs/2407.08447v2", "date": "2024-10-31", "relevancy": 3.552, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7321}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7125}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WildGaussians%3A%203D%20Gaussian%20Splatting%20in%20the%20Wild&body=Title%3A%20WildGaussians%3A%203D%20Gaussian%20Splatting%20in%20the%20Wild%0AAuthor%3A%20Jonas%20Kulhanek%20and%20Songyou%20Peng%20and%20Zuzana%20Kukelova%20and%20Marc%20Pollefeys%20and%20Torsten%20Sattler%0AAbstract%3A%20%20%20While%20the%20field%20of%203D%20scene%20reconstruction%20is%20dominated%20by%20NeRFs%20due%20to%20their%0Aphotorealistic%20quality%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20emerged%2C%0Aoffering%20similar%20quality%20with%20real-time%20rendering%20speeds.%20However%2C%20both%20methods%0Aprimarily%20excel%20with%20well-controlled%203D%20scenes%2C%20while%20in-the-wild%20data%20-%0Acharacterized%20by%20occlusions%2C%20dynamic%20objects%2C%20and%20varying%20illumination%20-%0Aremains%20challenging.%20NeRFs%20can%20adapt%20to%20such%20conditions%20easily%20through%0Aper-image%20embedding%20vectors%2C%20but%203DGS%20struggles%20due%20to%20its%20explicit%0Arepresentation%20and%20lack%20of%20shared%20parameters.%20To%20address%20this%2C%20we%20introduce%0AWildGaussians%2C%20a%20novel%20approach%20to%20handle%20occlusions%20and%20appearance%20changes%0Awith%203DGS.%20By%20leveraging%20robust%20DINO%20features%20and%20integrating%20an%20appearance%0Amodeling%20module%20within%203DGS%2C%20our%20method%20achieves%20state-of-the-art%20results.%20We%0Ademonstrate%20that%20WildGaussians%20matches%20the%20real-time%20rendering%20speed%20of%203DGS%0Awhile%20surpassing%20both%203DGS%20and%20NeRF%20baselines%20in%20handling%20in-the-wild%20data%2C%20all%0Awithin%20a%20simple%20architectural%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.08447v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWildGaussians%253A%25203D%2520Gaussian%2520Splatting%2520in%2520the%2520Wild%26entry.906535625%3DJonas%2520Kulhanek%2520and%2520Songyou%2520Peng%2520and%2520Zuzana%2520Kukelova%2520and%2520Marc%2520Pollefeys%2520and%2520Torsten%2520Sattler%26entry.1292438233%3D%2520%2520While%2520the%2520field%2520of%25203D%2520scene%2520reconstruction%2520is%2520dominated%2520by%2520NeRFs%2520due%2520to%2520their%250Aphotorealistic%2520quality%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520recently%2520emerged%252C%250Aoffering%2520similar%2520quality%2520with%2520real-time%2520rendering%2520speeds.%2520However%252C%2520both%2520methods%250Aprimarily%2520excel%2520with%2520well-controlled%25203D%2520scenes%252C%2520while%2520in-the-wild%2520data%2520-%250Acharacterized%2520by%2520occlusions%252C%2520dynamic%2520objects%252C%2520and%2520varying%2520illumination%2520-%250Aremains%2520challenging.%2520NeRFs%2520can%2520adapt%2520to%2520such%2520conditions%2520easily%2520through%250Aper-image%2520embedding%2520vectors%252C%2520but%25203DGS%2520struggles%2520due%2520to%2520its%2520explicit%250Arepresentation%2520and%2520lack%2520of%2520shared%2520parameters.%2520To%2520address%2520this%252C%2520we%2520introduce%250AWildGaussians%252C%2520a%2520novel%2520approach%2520to%2520handle%2520occlusions%2520and%2520appearance%2520changes%250Awith%25203DGS.%2520By%2520leveraging%2520robust%2520DINO%2520features%2520and%2520integrating%2520an%2520appearance%250Amodeling%2520module%2520within%25203DGS%252C%2520our%2520method%2520achieves%2520state-of-the-art%2520results.%2520We%250Ademonstrate%2520that%2520WildGaussians%2520matches%2520the%2520real-time%2520rendering%2520speed%2520of%25203DGS%250Awhile%2520surpassing%2520both%25203DGS%2520and%2520NeRF%2520baselines%2520in%2520handling%2520in-the-wild%2520data%252C%2520all%250Awithin%2520a%2520simple%2520architectural%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.08447v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WildGaussians%3A%203D%20Gaussian%20Splatting%20in%20the%20Wild&entry.906535625=Jonas%20Kulhanek%20and%20Songyou%20Peng%20and%20Zuzana%20Kukelova%20and%20Marc%20Pollefeys%20and%20Torsten%20Sattler&entry.1292438233=%20%20While%20the%20field%20of%203D%20scene%20reconstruction%20is%20dominated%20by%20NeRFs%20due%20to%20their%0Aphotorealistic%20quality%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20recently%20emerged%2C%0Aoffering%20similar%20quality%20with%20real-time%20rendering%20speeds.%20However%2C%20both%20methods%0Aprimarily%20excel%20with%20well-controlled%203D%20scenes%2C%20while%20in-the-wild%20data%20-%0Acharacterized%20by%20occlusions%2C%20dynamic%20objects%2C%20and%20varying%20illumination%20-%0Aremains%20challenging.%20NeRFs%20can%20adapt%20to%20such%20conditions%20easily%20through%0Aper-image%20embedding%20vectors%2C%20but%203DGS%20struggles%20due%20to%20its%20explicit%0Arepresentation%20and%20lack%20of%20shared%20parameters.%20To%20address%20this%2C%20we%20introduce%0AWildGaussians%2C%20a%20novel%20approach%20to%20handle%20occlusions%20and%20appearance%20changes%0Awith%203DGS.%20By%20leveraging%20robust%20DINO%20features%20and%20integrating%20an%20appearance%0Amodeling%20module%20within%203DGS%2C%20our%20method%20achieves%20state-of-the-art%20results.%20We%0Ademonstrate%20that%20WildGaussians%20matches%20the%20real-time%20rendering%20speed%20of%203DGS%0Awhile%20surpassing%20both%203DGS%20and%20NeRF%20baselines%20in%20handling%20in-the-wild%20data%2C%20all%0Awithin%20a%20simple%20architectural%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.08447v2&entry.124074799=Read"},
{"title": "No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse\n  Unposed Images", "author": "Botao Ye and Sifei Liu and Haofei Xu and Xueting Li and Marc Pollefeys and Ming-Hsuan Yang and Songyou Peng", "abstract": "  We introduce NoPoSplat, a feed-forward model capable of reconstructing 3D\nscenes parameterized by 3D Gaussians from \\textit{unposed} sparse multi-view\nimages. Our model, trained exclusively with photometric loss, achieves\nreal-time 3D Gaussian reconstruction during inference. To eliminate the need\nfor accurate pose input during reconstruction, we anchor one input view's local\ncamera coordinates as the canonical space and train the network to predict\nGaussian primitives for all views within this space. This approach obviates the\nneed to transform Gaussian primitives from local coordinates into a global\ncoordinate system, thus avoiding errors associated with per-frame Gaussians and\npose estimation. To resolve scale ambiguity, we design and compare various\nintrinsic embedding methods, ultimately opting to convert camera intrinsics\ninto a token embedding and concatenate it with image tokens as input to the\nmodel, enabling accurate scene scale prediction. We utilize the reconstructed\n3D Gaussians for novel view synthesis and pose estimation tasks and propose a\ntwo-stage coarse-to-fine pipeline for accurate pose estimation. Experimental\nresults demonstrate that our pose-free approach can achieve superior novel view\nsynthesis quality compared to pose-required methods, particularly in scenarios\nwith limited input image overlap. For pose estimation, our method, trained\nwithout ground truth depth or explicit matching loss, significantly outperforms\nthe state-of-the-art methods with substantial improvements. This work makes\nsignificant advances in pose-free generalizable 3D reconstruction and\ndemonstrates its applicability to real-world scenarios. Code and trained models\nare available at https://noposplat.github.io/.\n", "link": "http://arxiv.org/abs/2410.24207v1", "date": "2024-10-31", "relevancy": 3.4537, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7315}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6914}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20Pose%2C%20No%20Problem%3A%20Surprisingly%20Simple%203D%20Gaussian%20Splats%20from%20Sparse%0A%20%20Unposed%20Images&body=Title%3A%20No%20Pose%2C%20No%20Problem%3A%20Surprisingly%20Simple%203D%20Gaussian%20Splats%20from%20Sparse%0A%20%20Unposed%20Images%0AAuthor%3A%20Botao%20Ye%20and%20Sifei%20Liu%20and%20Haofei%20Xu%20and%20Xueting%20Li%20and%20Marc%20Pollefeys%20and%20Ming-Hsuan%20Yang%20and%20Songyou%20Peng%0AAbstract%3A%20%20%20We%20introduce%20NoPoSplat%2C%20a%20feed-forward%20model%20capable%20of%20reconstructing%203D%0Ascenes%20parameterized%20by%203D%20Gaussians%20from%20%5Ctextit%7Bunposed%7D%20sparse%20multi-view%0Aimages.%20Our%20model%2C%20trained%20exclusively%20with%20photometric%20loss%2C%20achieves%0Areal-time%203D%20Gaussian%20reconstruction%20during%20inference.%20To%20eliminate%20the%20need%0Afor%20accurate%20pose%20input%20during%20reconstruction%2C%20we%20anchor%20one%20input%20view%27s%20local%0Acamera%20coordinates%20as%20the%20canonical%20space%20and%20train%20the%20network%20to%20predict%0AGaussian%20primitives%20for%20all%20views%20within%20this%20space.%20This%20approach%20obviates%20the%0Aneed%20to%20transform%20Gaussian%20primitives%20from%20local%20coordinates%20into%20a%20global%0Acoordinate%20system%2C%20thus%20avoiding%20errors%20associated%20with%20per-frame%20Gaussians%20and%0Apose%20estimation.%20To%20resolve%20scale%20ambiguity%2C%20we%20design%20and%20compare%20various%0Aintrinsic%20embedding%20methods%2C%20ultimately%20opting%20to%20convert%20camera%20intrinsics%0Ainto%20a%20token%20embedding%20and%20concatenate%20it%20with%20image%20tokens%20as%20input%20to%20the%0Amodel%2C%20enabling%20accurate%20scene%20scale%20prediction.%20We%20utilize%20the%20reconstructed%0A3D%20Gaussians%20for%20novel%20view%20synthesis%20and%20pose%20estimation%20tasks%20and%20propose%20a%0Atwo-stage%20coarse-to-fine%20pipeline%20for%20accurate%20pose%20estimation.%20Experimental%0Aresults%20demonstrate%20that%20our%20pose-free%20approach%20can%20achieve%20superior%20novel%20view%0Asynthesis%20quality%20compared%20to%20pose-required%20methods%2C%20particularly%20in%20scenarios%0Awith%20limited%20input%20image%20overlap.%20For%20pose%20estimation%2C%20our%20method%2C%20trained%0Awithout%20ground%20truth%20depth%20or%20explicit%20matching%20loss%2C%20significantly%20outperforms%0Athe%20state-of-the-art%20methods%20with%20substantial%20improvements.%20This%20work%20makes%0Asignificant%20advances%20in%20pose-free%20generalizable%203D%20reconstruction%20and%0Ademonstrates%20its%20applicability%20to%20real-world%20scenarios.%20Code%20and%20trained%20models%0Aare%20available%20at%20https%3A//noposplat.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520Pose%252C%2520No%2520Problem%253A%2520Surprisingly%2520Simple%25203D%2520Gaussian%2520Splats%2520from%2520Sparse%250A%2520%2520Unposed%2520Images%26entry.906535625%3DBotao%2520Ye%2520and%2520Sifei%2520Liu%2520and%2520Haofei%2520Xu%2520and%2520Xueting%2520Li%2520and%2520Marc%2520Pollefeys%2520and%2520Ming-Hsuan%2520Yang%2520and%2520Songyou%2520Peng%26entry.1292438233%3D%2520%2520We%2520introduce%2520NoPoSplat%252C%2520a%2520feed-forward%2520model%2520capable%2520of%2520reconstructing%25203D%250Ascenes%2520parameterized%2520by%25203D%2520Gaussians%2520from%2520%255Ctextit%257Bunposed%257D%2520sparse%2520multi-view%250Aimages.%2520Our%2520model%252C%2520trained%2520exclusively%2520with%2520photometric%2520loss%252C%2520achieves%250Areal-time%25203D%2520Gaussian%2520reconstruction%2520during%2520inference.%2520To%2520eliminate%2520the%2520need%250Afor%2520accurate%2520pose%2520input%2520during%2520reconstruction%252C%2520we%2520anchor%2520one%2520input%2520view%2527s%2520local%250Acamera%2520coordinates%2520as%2520the%2520canonical%2520space%2520and%2520train%2520the%2520network%2520to%2520predict%250AGaussian%2520primitives%2520for%2520all%2520views%2520within%2520this%2520space.%2520This%2520approach%2520obviates%2520the%250Aneed%2520to%2520transform%2520Gaussian%2520primitives%2520from%2520local%2520coordinates%2520into%2520a%2520global%250Acoordinate%2520system%252C%2520thus%2520avoiding%2520errors%2520associated%2520with%2520per-frame%2520Gaussians%2520and%250Apose%2520estimation.%2520To%2520resolve%2520scale%2520ambiguity%252C%2520we%2520design%2520and%2520compare%2520various%250Aintrinsic%2520embedding%2520methods%252C%2520ultimately%2520opting%2520to%2520convert%2520camera%2520intrinsics%250Ainto%2520a%2520token%2520embedding%2520and%2520concatenate%2520it%2520with%2520image%2520tokens%2520as%2520input%2520to%2520the%250Amodel%252C%2520enabling%2520accurate%2520scene%2520scale%2520prediction.%2520We%2520utilize%2520the%2520reconstructed%250A3D%2520Gaussians%2520for%2520novel%2520view%2520synthesis%2520and%2520pose%2520estimation%2520tasks%2520and%2520propose%2520a%250Atwo-stage%2520coarse-to-fine%2520pipeline%2520for%2520accurate%2520pose%2520estimation.%2520Experimental%250Aresults%2520demonstrate%2520that%2520our%2520pose-free%2520approach%2520can%2520achieve%2520superior%2520novel%2520view%250Asynthesis%2520quality%2520compared%2520to%2520pose-required%2520methods%252C%2520particularly%2520in%2520scenarios%250Awith%2520limited%2520input%2520image%2520overlap.%2520For%2520pose%2520estimation%252C%2520our%2520method%252C%2520trained%250Awithout%2520ground%2520truth%2520depth%2520or%2520explicit%2520matching%2520loss%252C%2520significantly%2520outperforms%250Athe%2520state-of-the-art%2520methods%2520with%2520substantial%2520improvements.%2520This%2520work%2520makes%250Asignificant%2520advances%2520in%2520pose-free%2520generalizable%25203D%2520reconstruction%2520and%250Ademonstrates%2520its%2520applicability%2520to%2520real-world%2520scenarios.%2520Code%2520and%2520trained%2520models%250Aare%2520available%2520at%2520https%253A//noposplat.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20Pose%2C%20No%20Problem%3A%20Surprisingly%20Simple%203D%20Gaussian%20Splats%20from%20Sparse%0A%20%20Unposed%20Images&entry.906535625=Botao%20Ye%20and%20Sifei%20Liu%20and%20Haofei%20Xu%20and%20Xueting%20Li%20and%20Marc%20Pollefeys%20and%20Ming-Hsuan%20Yang%20and%20Songyou%20Peng&entry.1292438233=%20%20We%20introduce%20NoPoSplat%2C%20a%20feed-forward%20model%20capable%20of%20reconstructing%203D%0Ascenes%20parameterized%20by%203D%20Gaussians%20from%20%5Ctextit%7Bunposed%7D%20sparse%20multi-view%0Aimages.%20Our%20model%2C%20trained%20exclusively%20with%20photometric%20loss%2C%20achieves%0Areal-time%203D%20Gaussian%20reconstruction%20during%20inference.%20To%20eliminate%20the%20need%0Afor%20accurate%20pose%20input%20during%20reconstruction%2C%20we%20anchor%20one%20input%20view%27s%20local%0Acamera%20coordinates%20as%20the%20canonical%20space%20and%20train%20the%20network%20to%20predict%0AGaussian%20primitives%20for%20all%20views%20within%20this%20space.%20This%20approach%20obviates%20the%0Aneed%20to%20transform%20Gaussian%20primitives%20from%20local%20coordinates%20into%20a%20global%0Acoordinate%20system%2C%20thus%20avoiding%20errors%20associated%20with%20per-frame%20Gaussians%20and%0Apose%20estimation.%20To%20resolve%20scale%20ambiguity%2C%20we%20design%20and%20compare%20various%0Aintrinsic%20embedding%20methods%2C%20ultimately%20opting%20to%20convert%20camera%20intrinsics%0Ainto%20a%20token%20embedding%20and%20concatenate%20it%20with%20image%20tokens%20as%20input%20to%20the%0Amodel%2C%20enabling%20accurate%20scene%20scale%20prediction.%20We%20utilize%20the%20reconstructed%0A3D%20Gaussians%20for%20novel%20view%20synthesis%20and%20pose%20estimation%20tasks%20and%20propose%20a%0Atwo-stage%20coarse-to-fine%20pipeline%20for%20accurate%20pose%20estimation.%20Experimental%0Aresults%20demonstrate%20that%20our%20pose-free%20approach%20can%20achieve%20superior%20novel%20view%0Asynthesis%20quality%20compared%20to%20pose-required%20methods%2C%20particularly%20in%20scenarios%0Awith%20limited%20input%20image%20overlap.%20For%20pose%20estimation%2C%20our%20method%2C%20trained%0Awithout%20ground%20truth%20depth%20or%20explicit%20matching%20loss%2C%20significantly%20outperforms%0Athe%20state-of-the-art%20methods%20with%20substantial%20improvements.%20This%20work%20makes%0Asignificant%20advances%20in%20pose-free%20generalizable%203D%20reconstruction%20and%0Ademonstrates%20its%20applicability%20to%20real-world%20scenarios.%20Code%20and%20trained%20models%0Aare%20available%20at%20https%3A//noposplat.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24207v1&entry.124074799=Read"},
{"title": "Subsurface Scattering for 3D Gaussian Splatting", "author": "Jan-Niklas Dihlmann and Arjun Majumdar and Andreas Engelhardt and Raphael Braun and Hendrik P. A. Lensch", "abstract": "  3D reconstruction and relighting of objects made from scattering materials\npresent a significant challenge due to the complex light transport beneath the\nsurface. 3D Gaussian Splatting introduced high-quality novel view synthesis at\nreal-time speeds. While 3D Gaussians efficiently approximate an object's\nsurface, they fail to capture the volumetric properties of subsurface\nscattering. We propose a framework for optimizing an object's shape together\nwith the radiance transfer field given multi-view OLAT (one light at a time)\ndata. Our method decomposes the scene into an explicit surface represented as\n3D Gaussians, with a spatially varying BRDF, and an implicit volumetric\nrepresentation of the scattering component. A learned incident light field\naccounts for shadowing. We optimize all parameters jointly via ray-traced\ndifferentiable rendering. Our approach enables material editing, relighting and\nnovel view synthesis at interactive rates. We show successful application on\nsynthetic data and introduce a newly acquired multi-view multi-light dataset of\nobjects in a light-stage setup. Compared to previous work we achieve comparable\nor better results at a fraction of optimization and rendering time while\nenabling detailed control over material attributes. Project page\nhttps://sss.jdihlmann.com/\n", "link": "http://arxiv.org/abs/2408.12282v2", "date": "2024-10-31", "relevancy": 3.3581, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7284}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6657}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subsurface%20Scattering%20for%203D%20Gaussian%20Splatting&body=Title%3A%20Subsurface%20Scattering%20for%203D%20Gaussian%20Splatting%0AAuthor%3A%20Jan-Niklas%20Dihlmann%20and%20Arjun%20Majumdar%20and%20Andreas%20Engelhardt%20and%20Raphael%20Braun%20and%20Hendrik%20P.%20A.%20Lensch%0AAbstract%3A%20%20%203D%20reconstruction%20and%20relighting%20of%20objects%20made%20from%20scattering%20materials%0Apresent%20a%20significant%20challenge%20due%20to%20the%20complex%20light%20transport%20beneath%20the%0Asurface.%203D%20Gaussian%20Splatting%20introduced%20high-quality%20novel%20view%20synthesis%20at%0Areal-time%20speeds.%20While%203D%20Gaussians%20efficiently%20approximate%20an%20object%27s%0Asurface%2C%20they%20fail%20to%20capture%20the%20volumetric%20properties%20of%20subsurface%0Ascattering.%20We%20propose%20a%20framework%20for%20optimizing%20an%20object%27s%20shape%20together%0Awith%20the%20radiance%20transfer%20field%20given%20multi-view%20OLAT%20%28one%20light%20at%20a%20time%29%0Adata.%20Our%20method%20decomposes%20the%20scene%20into%20an%20explicit%20surface%20represented%20as%0A3D%20Gaussians%2C%20with%20a%20spatially%20varying%20BRDF%2C%20and%20an%20implicit%20volumetric%0Arepresentation%20of%20the%20scattering%20component.%20A%20learned%20incident%20light%20field%0Aaccounts%20for%20shadowing.%20We%20optimize%20all%20parameters%20jointly%20via%20ray-traced%0Adifferentiable%20rendering.%20Our%20approach%20enables%20material%20editing%2C%20relighting%20and%0Anovel%20view%20synthesis%20at%20interactive%20rates.%20We%20show%20successful%20application%20on%0Asynthetic%20data%20and%20introduce%20a%20newly%20acquired%20multi-view%20multi-light%20dataset%20of%0Aobjects%20in%20a%20light-stage%20setup.%20Compared%20to%20previous%20work%20we%20achieve%20comparable%0Aor%20better%20results%20at%20a%20fraction%20of%20optimization%20and%20rendering%20time%20while%0Aenabling%20detailed%20control%20over%20material%20attributes.%20Project%20page%0Ahttps%3A//sss.jdihlmann.com/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12282v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubsurface%2520Scattering%2520for%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DJan-Niklas%2520Dihlmann%2520and%2520Arjun%2520Majumdar%2520and%2520Andreas%2520Engelhardt%2520and%2520Raphael%2520Braun%2520and%2520Hendrik%2520P.%2520A.%2520Lensch%26entry.1292438233%3D%2520%25203D%2520reconstruction%2520and%2520relighting%2520of%2520objects%2520made%2520from%2520scattering%2520materials%250Apresent%2520a%2520significant%2520challenge%2520due%2520to%2520the%2520complex%2520light%2520transport%2520beneath%2520the%250Asurface.%25203D%2520Gaussian%2520Splatting%2520introduced%2520high-quality%2520novel%2520view%2520synthesis%2520at%250Areal-time%2520speeds.%2520While%25203D%2520Gaussians%2520efficiently%2520approximate%2520an%2520object%2527s%250Asurface%252C%2520they%2520fail%2520to%2520capture%2520the%2520volumetric%2520properties%2520of%2520subsurface%250Ascattering.%2520We%2520propose%2520a%2520framework%2520for%2520optimizing%2520an%2520object%2527s%2520shape%2520together%250Awith%2520the%2520radiance%2520transfer%2520field%2520given%2520multi-view%2520OLAT%2520%2528one%2520light%2520at%2520a%2520time%2529%250Adata.%2520Our%2520method%2520decomposes%2520the%2520scene%2520into%2520an%2520explicit%2520surface%2520represented%2520as%250A3D%2520Gaussians%252C%2520with%2520a%2520spatially%2520varying%2520BRDF%252C%2520and%2520an%2520implicit%2520volumetric%250Arepresentation%2520of%2520the%2520scattering%2520component.%2520A%2520learned%2520incident%2520light%2520field%250Aaccounts%2520for%2520shadowing.%2520We%2520optimize%2520all%2520parameters%2520jointly%2520via%2520ray-traced%250Adifferentiable%2520rendering.%2520Our%2520approach%2520enables%2520material%2520editing%252C%2520relighting%2520and%250Anovel%2520view%2520synthesis%2520at%2520interactive%2520rates.%2520We%2520show%2520successful%2520application%2520on%250Asynthetic%2520data%2520and%2520introduce%2520a%2520newly%2520acquired%2520multi-view%2520multi-light%2520dataset%2520of%250Aobjects%2520in%2520a%2520light-stage%2520setup.%2520Compared%2520to%2520previous%2520work%2520we%2520achieve%2520comparable%250Aor%2520better%2520results%2520at%2520a%2520fraction%2520of%2520optimization%2520and%2520rendering%2520time%2520while%250Aenabling%2520detailed%2520control%2520over%2520material%2520attributes.%2520Project%2520page%250Ahttps%253A//sss.jdihlmann.com/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12282v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subsurface%20Scattering%20for%203D%20Gaussian%20Splatting&entry.906535625=Jan-Niklas%20Dihlmann%20and%20Arjun%20Majumdar%20and%20Andreas%20Engelhardt%20and%20Raphael%20Braun%20and%20Hendrik%20P.%20A.%20Lensch&entry.1292438233=%20%203D%20reconstruction%20and%20relighting%20of%20objects%20made%20from%20scattering%20materials%0Apresent%20a%20significant%20challenge%20due%20to%20the%20complex%20light%20transport%20beneath%20the%0Asurface.%203D%20Gaussian%20Splatting%20introduced%20high-quality%20novel%20view%20synthesis%20at%0Areal-time%20speeds.%20While%203D%20Gaussians%20efficiently%20approximate%20an%20object%27s%0Asurface%2C%20they%20fail%20to%20capture%20the%20volumetric%20properties%20of%20subsurface%0Ascattering.%20We%20propose%20a%20framework%20for%20optimizing%20an%20object%27s%20shape%20together%0Awith%20the%20radiance%20transfer%20field%20given%20multi-view%20OLAT%20%28one%20light%20at%20a%20time%29%0Adata.%20Our%20method%20decomposes%20the%20scene%20into%20an%20explicit%20surface%20represented%20as%0A3D%20Gaussians%2C%20with%20a%20spatially%20varying%20BRDF%2C%20and%20an%20implicit%20volumetric%0Arepresentation%20of%20the%20scattering%20component.%20A%20learned%20incident%20light%20field%0Aaccounts%20for%20shadowing.%20We%20optimize%20all%20parameters%20jointly%20via%20ray-traced%0Adifferentiable%20rendering.%20Our%20approach%20enables%20material%20editing%2C%20relighting%20and%0Anovel%20view%20synthesis%20at%20interactive%20rates.%20We%20show%20successful%20application%20on%0Asynthetic%20data%20and%20introduce%20a%20newly%20acquired%20multi-view%20multi-light%20dataset%20of%0Aobjects%20in%20a%20light-stage%20setup.%20Compared%20to%20previous%20work%20we%20achieve%20comparable%0Aor%20better%20results%20at%20a%20fraction%20of%20optimization%20and%20rendering%20time%20while%0Aenabling%20detailed%20control%20over%20material%20attributes.%20Project%20page%0Ahttps%3A//sss.jdihlmann.com/%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12282v2&entry.124074799=Read"},
{"title": "GeoSplatting: Towards Geometry Guided Gaussian Splatting for\n  Physically-based Inverse Rendering", "author": "Kai Ye and Chong Gao and Guanbin Li and Wenzheng Chen and Baoquan Chen", "abstract": "  We consider the problem of physically-based inverse rendering using 3D\nGaussian Splatting (3DGS) representations. While recent 3DGS methods have\nachieved remarkable results in novel view synthesis (NVS), accurately capturing\nhigh-fidelity geometry, physically interpretable materials and lighting remains\nchallenging, as it requires precise geometry modeling to provide accurate\nsurface normals, along with physically-based rendering (PBR) techniques to\nensure correct material and lighting disentanglement. Previous 3DGS methods\nresort to approximating surface normals, but often struggle with noisy local\ngeometry, leading to inaccurate normal estimation and suboptimal\nmaterial-lighting decomposition. In this paper, we introduce GeoSplatting, a\nnovel hybrid representation that augments 3DGS with explicit geometric guidance\nand differentiable PBR equations. Specifically, we bridge isosurface and 3DGS\ntogether, where we first extract isosurface mesh from a scalar field, then\nconvert it into 3DGS points and formulate PBR equations for them in a fully\ndifferentiable manner. In GeoSplatting, 3DGS is grounded on the mesh geometry,\nenabling precise surface normal modeling, which facilitates the use of PBR\nframeworks for material decomposition. This approach further maintains the\nefficiency and quality of NVS from 3DGS while ensuring accurate geometry from\nthe isosurface. Comprehensive evaluations across diverse datasets demonstrate\nthe superiority of GeoSplatting, consistently outperforming existing methods\nboth quantitatively and qualitatively.\n", "link": "http://arxiv.org/abs/2410.24204v1", "date": "2024-10-31", "relevancy": 3.3398, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6791}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6713}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoSplatting%3A%20Towards%20Geometry%20Guided%20Gaussian%20Splatting%20for%0A%20%20Physically-based%20Inverse%20Rendering&body=Title%3A%20GeoSplatting%3A%20Towards%20Geometry%20Guided%20Gaussian%20Splatting%20for%0A%20%20Physically-based%20Inverse%20Rendering%0AAuthor%3A%20Kai%20Ye%20and%20Chong%20Gao%20and%20Guanbin%20Li%20and%20Wenzheng%20Chen%20and%20Baoquan%20Chen%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20physically-based%20inverse%20rendering%20using%203D%0AGaussian%20Splatting%20%283DGS%29%20representations.%20While%20recent%203DGS%20methods%20have%0Aachieved%20remarkable%20results%20in%20novel%20view%20synthesis%20%28NVS%29%2C%20accurately%20capturing%0Ahigh-fidelity%20geometry%2C%20physically%20interpretable%20materials%20and%20lighting%20remains%0Achallenging%2C%20as%20it%20requires%20precise%20geometry%20modeling%20to%20provide%20accurate%0Asurface%20normals%2C%20along%20with%20physically-based%20rendering%20%28PBR%29%20techniques%20to%0Aensure%20correct%20material%20and%20lighting%20disentanglement.%20Previous%203DGS%20methods%0Aresort%20to%20approximating%20surface%20normals%2C%20but%20often%20struggle%20with%20noisy%20local%0Ageometry%2C%20leading%20to%20inaccurate%20normal%20estimation%20and%20suboptimal%0Amaterial-lighting%20decomposition.%20In%20this%20paper%2C%20we%20introduce%20GeoSplatting%2C%20a%0Anovel%20hybrid%20representation%20that%20augments%203DGS%20with%20explicit%20geometric%20guidance%0Aand%20differentiable%20PBR%20equations.%20Specifically%2C%20we%20bridge%20isosurface%20and%203DGS%0Atogether%2C%20where%20we%20first%20extract%20isosurface%20mesh%20from%20a%20scalar%20field%2C%20then%0Aconvert%20it%20into%203DGS%20points%20and%20formulate%20PBR%20equations%20for%20them%20in%20a%20fully%0Adifferentiable%20manner.%20In%20GeoSplatting%2C%203DGS%20is%20grounded%20on%20the%20mesh%20geometry%2C%0Aenabling%20precise%20surface%20normal%20modeling%2C%20which%20facilitates%20the%20use%20of%20PBR%0Aframeworks%20for%20material%20decomposition.%20This%20approach%20further%20maintains%20the%0Aefficiency%20and%20quality%20of%20NVS%20from%203DGS%20while%20ensuring%20accurate%20geometry%20from%0Athe%20isosurface.%20Comprehensive%20evaluations%20across%20diverse%20datasets%20demonstrate%0Athe%20superiority%20of%20GeoSplatting%2C%20consistently%20outperforming%20existing%20methods%0Aboth%20quantitatively%20and%20qualitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoSplatting%253A%2520Towards%2520Geometry%2520Guided%2520Gaussian%2520Splatting%2520for%250A%2520%2520Physically-based%2520Inverse%2520Rendering%26entry.906535625%3DKai%2520Ye%2520and%2520Chong%2520Gao%2520and%2520Guanbin%2520Li%2520and%2520Wenzheng%2520Chen%2520and%2520Baoquan%2520Chen%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520physically-based%2520inverse%2520rendering%2520using%25203D%250AGaussian%2520Splatting%2520%25283DGS%2529%2520representations.%2520While%2520recent%25203DGS%2520methods%2520have%250Aachieved%2520remarkable%2520results%2520in%2520novel%2520view%2520synthesis%2520%2528NVS%2529%252C%2520accurately%2520capturing%250Ahigh-fidelity%2520geometry%252C%2520physically%2520interpretable%2520materials%2520and%2520lighting%2520remains%250Achallenging%252C%2520as%2520it%2520requires%2520precise%2520geometry%2520modeling%2520to%2520provide%2520accurate%250Asurface%2520normals%252C%2520along%2520with%2520physically-based%2520rendering%2520%2528PBR%2529%2520techniques%2520to%250Aensure%2520correct%2520material%2520and%2520lighting%2520disentanglement.%2520Previous%25203DGS%2520methods%250Aresort%2520to%2520approximating%2520surface%2520normals%252C%2520but%2520often%2520struggle%2520with%2520noisy%2520local%250Ageometry%252C%2520leading%2520to%2520inaccurate%2520normal%2520estimation%2520and%2520suboptimal%250Amaterial-lighting%2520decomposition.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520GeoSplatting%252C%2520a%250Anovel%2520hybrid%2520representation%2520that%2520augments%25203DGS%2520with%2520explicit%2520geometric%2520guidance%250Aand%2520differentiable%2520PBR%2520equations.%2520Specifically%252C%2520we%2520bridge%2520isosurface%2520and%25203DGS%250Atogether%252C%2520where%2520we%2520first%2520extract%2520isosurface%2520mesh%2520from%2520a%2520scalar%2520field%252C%2520then%250Aconvert%2520it%2520into%25203DGS%2520points%2520and%2520formulate%2520PBR%2520equations%2520for%2520them%2520in%2520a%2520fully%250Adifferentiable%2520manner.%2520In%2520GeoSplatting%252C%25203DGS%2520is%2520grounded%2520on%2520the%2520mesh%2520geometry%252C%250Aenabling%2520precise%2520surface%2520normal%2520modeling%252C%2520which%2520facilitates%2520the%2520use%2520of%2520PBR%250Aframeworks%2520for%2520material%2520decomposition.%2520This%2520approach%2520further%2520maintains%2520the%250Aefficiency%2520and%2520quality%2520of%2520NVS%2520from%25203DGS%2520while%2520ensuring%2520accurate%2520geometry%2520from%250Athe%2520isosurface.%2520Comprehensive%2520evaluations%2520across%2520diverse%2520datasets%2520demonstrate%250Athe%2520superiority%2520of%2520GeoSplatting%252C%2520consistently%2520outperforming%2520existing%2520methods%250Aboth%2520quantitatively%2520and%2520qualitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoSplatting%3A%20Towards%20Geometry%20Guided%20Gaussian%20Splatting%20for%0A%20%20Physically-based%20Inverse%20Rendering&entry.906535625=Kai%20Ye%20and%20Chong%20Gao%20and%20Guanbin%20Li%20and%20Wenzheng%20Chen%20and%20Baoquan%20Chen&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20physically-based%20inverse%20rendering%20using%203D%0AGaussian%20Splatting%20%283DGS%29%20representations.%20While%20recent%203DGS%20methods%20have%0Aachieved%20remarkable%20results%20in%20novel%20view%20synthesis%20%28NVS%29%2C%20accurately%20capturing%0Ahigh-fidelity%20geometry%2C%20physically%20interpretable%20materials%20and%20lighting%20remains%0Achallenging%2C%20as%20it%20requires%20precise%20geometry%20modeling%20to%20provide%20accurate%0Asurface%20normals%2C%20along%20with%20physically-based%20rendering%20%28PBR%29%20techniques%20to%0Aensure%20correct%20material%20and%20lighting%20disentanglement.%20Previous%203DGS%20methods%0Aresort%20to%20approximating%20surface%20normals%2C%20but%20often%20struggle%20with%20noisy%20local%0Ageometry%2C%20leading%20to%20inaccurate%20normal%20estimation%20and%20suboptimal%0Amaterial-lighting%20decomposition.%20In%20this%20paper%2C%20we%20introduce%20GeoSplatting%2C%20a%0Anovel%20hybrid%20representation%20that%20augments%203DGS%20with%20explicit%20geometric%20guidance%0Aand%20differentiable%20PBR%20equations.%20Specifically%2C%20we%20bridge%20isosurface%20and%203DGS%0Atogether%2C%20where%20we%20first%20extract%20isosurface%20mesh%20from%20a%20scalar%20field%2C%20then%0Aconvert%20it%20into%203DGS%20points%20and%20formulate%20PBR%20equations%20for%20them%20in%20a%20fully%0Adifferentiable%20manner.%20In%20GeoSplatting%2C%203DGS%20is%20grounded%20on%20the%20mesh%20geometry%2C%0Aenabling%20precise%20surface%20normal%20modeling%2C%20which%20facilitates%20the%20use%20of%20PBR%0Aframeworks%20for%20material%20decomposition.%20This%20approach%20further%20maintains%20the%0Aefficiency%20and%20quality%20of%20NVS%20from%203DGS%20while%20ensuring%20accurate%20geometry%20from%0Athe%20isosurface.%20Comprehensive%20evaluations%20across%20diverse%20datasets%20demonstrate%0Athe%20superiority%20of%20GeoSplatting%2C%20consistently%20outperforming%20existing%20methods%0Aboth%20quantitatively%20and%20qualitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24204v1&entry.124074799=Read"},
{"title": "URAvatar: Universal Relightable Gaussian Codec Avatars", "author": "Junxuan Li and Chen Cao and Gabriel Schwartz and Rawal Khirodkar and Christian Richardt and Tomas Simon and Yaser Sheikh and Shunsuke Saito", "abstract": "  We present a new approach to creating photorealistic and relightable head\navatars from a phone scan with unknown illumination. The reconstructed avatars\ncan be animated and relit in real time with the global illumination of diverse\nenvironments. Unlike existing approaches that estimate parametric reflectance\nparameters via inverse rendering, our approach directly models learnable\nradiance transfer that incorporates global light transport in an efficient\nmanner for real-time rendering. However, learning such a complex light\ntransport that can generalize across identities is non-trivial. A phone scan in\na single environment lacks sufficient information to infer how the head would\nappear in general environments. To address this, we build a universal\nrelightable avatar model represented by 3D Gaussians. We train on hundreds of\nhigh-quality multi-view human scans with controllable point lights.\nHigh-resolution geometric guidance further enhances the reconstruction accuracy\nand generalization. Once trained, we finetune the pretrained model on a phone\nscan using inverse rendering to obtain a personalized relightable avatar. Our\nexperiments establish the efficacy of our design, outperforming existing\napproaches while retaining real-time rendering capability.\n", "link": "http://arxiv.org/abs/2410.24223v1", "date": "2024-10-31", "relevancy": 3.3224, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6974}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6974}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20URAvatar%3A%20Universal%20Relightable%20Gaussian%20Codec%20Avatars&body=Title%3A%20URAvatar%3A%20Universal%20Relightable%20Gaussian%20Codec%20Avatars%0AAuthor%3A%20Junxuan%20Li%20and%20Chen%20Cao%20and%20Gabriel%20Schwartz%20and%20Rawal%20Khirodkar%20and%20Christian%20Richardt%20and%20Tomas%20Simon%20and%20Yaser%20Sheikh%20and%20Shunsuke%20Saito%0AAbstract%3A%20%20%20We%20present%20a%20new%20approach%20to%20creating%20photorealistic%20and%20relightable%20head%0Aavatars%20from%20a%20phone%20scan%20with%20unknown%20illumination.%20The%20reconstructed%20avatars%0Acan%20be%20animated%20and%20relit%20in%20real%20time%20with%20the%20global%20illumination%20of%20diverse%0Aenvironments.%20Unlike%20existing%20approaches%20that%20estimate%20parametric%20reflectance%0Aparameters%20via%20inverse%20rendering%2C%20our%20approach%20directly%20models%20learnable%0Aradiance%20transfer%20that%20incorporates%20global%20light%20transport%20in%20an%20efficient%0Amanner%20for%20real-time%20rendering.%20However%2C%20learning%20such%20a%20complex%20light%0Atransport%20that%20can%20generalize%20across%20identities%20is%20non-trivial.%20A%20phone%20scan%20in%0Aa%20single%20environment%20lacks%20sufficient%20information%20to%20infer%20how%20the%20head%20would%0Aappear%20in%20general%20environments.%20To%20address%20this%2C%20we%20build%20a%20universal%0Arelightable%20avatar%20model%20represented%20by%203D%20Gaussians.%20We%20train%20on%20hundreds%20of%0Ahigh-quality%20multi-view%20human%20scans%20with%20controllable%20point%20lights.%0AHigh-resolution%20geometric%20guidance%20further%20enhances%20the%20reconstruction%20accuracy%0Aand%20generalization.%20Once%20trained%2C%20we%20finetune%20the%20pretrained%20model%20on%20a%20phone%0Ascan%20using%20inverse%20rendering%20to%20obtain%20a%20personalized%20relightable%20avatar.%20Our%0Aexperiments%20establish%20the%20efficacy%20of%20our%20design%2C%20outperforming%20existing%0Aapproaches%20while%20retaining%20real-time%20rendering%20capability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24223v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DURAvatar%253A%2520Universal%2520Relightable%2520Gaussian%2520Codec%2520Avatars%26entry.906535625%3DJunxuan%2520Li%2520and%2520Chen%2520Cao%2520and%2520Gabriel%2520Schwartz%2520and%2520Rawal%2520Khirodkar%2520and%2520Christian%2520Richardt%2520and%2520Tomas%2520Simon%2520and%2520Yaser%2520Sheikh%2520and%2520Shunsuke%2520Saito%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520approach%2520to%2520creating%2520photorealistic%2520and%2520relightable%2520head%250Aavatars%2520from%2520a%2520phone%2520scan%2520with%2520unknown%2520illumination.%2520The%2520reconstructed%2520avatars%250Acan%2520be%2520animated%2520and%2520relit%2520in%2520real%2520time%2520with%2520the%2520global%2520illumination%2520of%2520diverse%250Aenvironments.%2520Unlike%2520existing%2520approaches%2520that%2520estimate%2520parametric%2520reflectance%250Aparameters%2520via%2520inverse%2520rendering%252C%2520our%2520approach%2520directly%2520models%2520learnable%250Aradiance%2520transfer%2520that%2520incorporates%2520global%2520light%2520transport%2520in%2520an%2520efficient%250Amanner%2520for%2520real-time%2520rendering.%2520However%252C%2520learning%2520such%2520a%2520complex%2520light%250Atransport%2520that%2520can%2520generalize%2520across%2520identities%2520is%2520non-trivial.%2520A%2520phone%2520scan%2520in%250Aa%2520single%2520environment%2520lacks%2520sufficient%2520information%2520to%2520infer%2520how%2520the%2520head%2520would%250Aappear%2520in%2520general%2520environments.%2520To%2520address%2520this%252C%2520we%2520build%2520a%2520universal%250Arelightable%2520avatar%2520model%2520represented%2520by%25203D%2520Gaussians.%2520We%2520train%2520on%2520hundreds%2520of%250Ahigh-quality%2520multi-view%2520human%2520scans%2520with%2520controllable%2520point%2520lights.%250AHigh-resolution%2520geometric%2520guidance%2520further%2520enhances%2520the%2520reconstruction%2520accuracy%250Aand%2520generalization.%2520Once%2520trained%252C%2520we%2520finetune%2520the%2520pretrained%2520model%2520on%2520a%2520phone%250Ascan%2520using%2520inverse%2520rendering%2520to%2520obtain%2520a%2520personalized%2520relightable%2520avatar.%2520Our%250Aexperiments%2520establish%2520the%2520efficacy%2520of%2520our%2520design%252C%2520outperforming%2520existing%250Aapproaches%2520while%2520retaining%2520real-time%2520rendering%2520capability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24223v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=URAvatar%3A%20Universal%20Relightable%20Gaussian%20Codec%20Avatars&entry.906535625=Junxuan%20Li%20and%20Chen%20Cao%20and%20Gabriel%20Schwartz%20and%20Rawal%20Khirodkar%20and%20Christian%20Richardt%20and%20Tomas%20Simon%20and%20Yaser%20Sheikh%20and%20Shunsuke%20Saito&entry.1292438233=%20%20We%20present%20a%20new%20approach%20to%20creating%20photorealistic%20and%20relightable%20head%0Aavatars%20from%20a%20phone%20scan%20with%20unknown%20illumination.%20The%20reconstructed%20avatars%0Acan%20be%20animated%20and%20relit%20in%20real%20time%20with%20the%20global%20illumination%20of%20diverse%0Aenvironments.%20Unlike%20existing%20approaches%20that%20estimate%20parametric%20reflectance%0Aparameters%20via%20inverse%20rendering%2C%20our%20approach%20directly%20models%20learnable%0Aradiance%20transfer%20that%20incorporates%20global%20light%20transport%20in%20an%20efficient%0Amanner%20for%20real-time%20rendering.%20However%2C%20learning%20such%20a%20complex%20light%0Atransport%20that%20can%20generalize%20across%20identities%20is%20non-trivial.%20A%20phone%20scan%20in%0Aa%20single%20environment%20lacks%20sufficient%20information%20to%20infer%20how%20the%20head%20would%0Aappear%20in%20general%20environments.%20To%20address%20this%2C%20we%20build%20a%20universal%0Arelightable%20avatar%20model%20represented%20by%203D%20Gaussians.%20We%20train%20on%20hundreds%20of%0Ahigh-quality%20multi-view%20human%20scans%20with%20controllable%20point%20lights.%0AHigh-resolution%20geometric%20guidance%20further%20enhances%20the%20reconstruction%20accuracy%0Aand%20generalization.%20Once%20trained%2C%20we%20finetune%20the%20pretrained%20model%20on%20a%20phone%0Ascan%20using%20inverse%20rendering%20to%20obtain%20a%20personalized%20relightable%20avatar.%20Our%0Aexperiments%20establish%20the%20efficacy%20of%20our%20design%2C%20outperforming%20existing%0Aapproaches%20while%20retaining%20real-time%20rendering%20capability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24223v1&entry.124074799=Read"},
{"title": "MoVA: Adapting Mixture of Vision Experts to Multimodal Context", "author": "Zhuofan Zong and Bingqi Ma and Dazhong Shen and Guanglu Song and Hao Shao and Dongzhi Jiang and Hongsheng Li and Yu Liu", "abstract": "  As the key component in multimodal large language models (MLLMs), the ability\nof the visual encoder greatly affects MLLM's understanding on diverse image\ncontent. Although some large-scale pretrained vision encoders such as vision\nencoders in CLIP and DINOv2 have brought promising performance, we found that\nthere is still no single vision encoder that can dominate various image content\nunderstanding, e.g., the CLIP vision encoder leads to outstanding results on\ngeneral image understanding but poor performance on document or chart content.\nTo alleviate the bias of CLIP vision encoder, we first delve into the inherent\nbehavior of different pre-trained vision encoders and then propose the MoVA, a\npowerful and novel MLLM, adaptively routing and fusing task-specific vision\nexperts with a coarse-to-fine mechanism. In the coarse-grained stage, we design\na context-aware expert routing strategy to dynamically select the most suitable\nvision experts according to the user instruction, input image, and expertise of\nvision experts. This benefits from the powerful model function understanding\nability of the large language model (LLM). In the fine-grained stage, we\nelaborately conduct the mixture-of-vision-expert adapter (MoV-Adapter) to\nextract and fuse task-specific knowledge from various experts. This\ncoarse-to-fine paradigm effectively leverages representations from experts\nbased on multimodal context and model expertise, further enhancing the\ngeneralization ability. We conduct extensive experiments to evaluate the\neffectiveness of the proposed approach. Without any bells and whistles, MoVA\ncan achieve significant performance gains over current state-of-the-art methods\nin a wide range of challenging multimodal benchmarks.\n", "link": "http://arxiv.org/abs/2404.13046v2", "date": "2024-10-31", "relevancy": 3.1259, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6319}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6319}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoVA%3A%20Adapting%20Mixture%20of%20Vision%20Experts%20to%20Multimodal%20Context&body=Title%3A%20MoVA%3A%20Adapting%20Mixture%20of%20Vision%20Experts%20to%20Multimodal%20Context%0AAuthor%3A%20Zhuofan%20Zong%20and%20Bingqi%20Ma%20and%20Dazhong%20Shen%20and%20Guanglu%20Song%20and%20Hao%20Shao%20and%20Dongzhi%20Jiang%20and%20Hongsheng%20Li%20and%20Yu%20Liu%0AAbstract%3A%20%20%20As%20the%20key%20component%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20the%20ability%0Aof%20the%20visual%20encoder%20greatly%20affects%20MLLM%27s%20understanding%20on%20diverse%20image%0Acontent.%20Although%20some%20large-scale%20pretrained%20vision%20encoders%20such%20as%20vision%0Aencoders%20in%20CLIP%20and%20DINOv2%20have%20brought%20promising%20performance%2C%20we%20found%20that%0Athere%20is%20still%20no%20single%20vision%20encoder%20that%20can%20dominate%20various%20image%20content%0Aunderstanding%2C%20e.g.%2C%20the%20CLIP%20vision%20encoder%20leads%20to%20outstanding%20results%20on%0Ageneral%20image%20understanding%20but%20poor%20performance%20on%20document%20or%20chart%20content.%0ATo%20alleviate%20the%20bias%20of%20CLIP%20vision%20encoder%2C%20we%20first%20delve%20into%20the%20inherent%0Abehavior%20of%20different%20pre-trained%20vision%20encoders%20and%20then%20propose%20the%20MoVA%2C%20a%0Apowerful%20and%20novel%20MLLM%2C%20adaptively%20routing%20and%20fusing%20task-specific%20vision%0Aexperts%20with%20a%20coarse-to-fine%20mechanism.%20In%20the%20coarse-grained%20stage%2C%20we%20design%0Aa%20context-aware%20expert%20routing%20strategy%20to%20dynamically%20select%20the%20most%20suitable%0Avision%20experts%20according%20to%20the%20user%20instruction%2C%20input%20image%2C%20and%20expertise%20of%0Avision%20experts.%20This%20benefits%20from%20the%20powerful%20model%20function%20understanding%0Aability%20of%20the%20large%20language%20model%20%28LLM%29.%20In%20the%20fine-grained%20stage%2C%20we%0Aelaborately%20conduct%20the%20mixture-of-vision-expert%20adapter%20%28MoV-Adapter%29%20to%0Aextract%20and%20fuse%20task-specific%20knowledge%20from%20various%20experts.%20This%0Acoarse-to-fine%20paradigm%20effectively%20leverages%20representations%20from%20experts%0Abased%20on%20multimodal%20context%20and%20model%20expertise%2C%20further%20enhancing%20the%0Ageneralization%20ability.%20We%20conduct%20extensive%20experiments%20to%20evaluate%20the%0Aeffectiveness%20of%20the%20proposed%20approach.%20Without%20any%20bells%20and%20whistles%2C%20MoVA%0Acan%20achieve%20significant%20performance%20gains%20over%20current%20state-of-the-art%20methods%0Ain%20a%20wide%20range%20of%20challenging%20multimodal%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13046v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoVA%253A%2520Adapting%2520Mixture%2520of%2520Vision%2520Experts%2520to%2520Multimodal%2520Context%26entry.906535625%3DZhuofan%2520Zong%2520and%2520Bingqi%2520Ma%2520and%2520Dazhong%2520Shen%2520and%2520Guanglu%2520Song%2520and%2520Hao%2520Shao%2520and%2520Dongzhi%2520Jiang%2520and%2520Hongsheng%2520Li%2520and%2520Yu%2520Liu%26entry.1292438233%3D%2520%2520As%2520the%2520key%2520component%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520the%2520ability%250Aof%2520the%2520visual%2520encoder%2520greatly%2520affects%2520MLLM%2527s%2520understanding%2520on%2520diverse%2520image%250Acontent.%2520Although%2520some%2520large-scale%2520pretrained%2520vision%2520encoders%2520such%2520as%2520vision%250Aencoders%2520in%2520CLIP%2520and%2520DINOv2%2520have%2520brought%2520promising%2520performance%252C%2520we%2520found%2520that%250Athere%2520is%2520still%2520no%2520single%2520vision%2520encoder%2520that%2520can%2520dominate%2520various%2520image%2520content%250Aunderstanding%252C%2520e.g.%252C%2520the%2520CLIP%2520vision%2520encoder%2520leads%2520to%2520outstanding%2520results%2520on%250Ageneral%2520image%2520understanding%2520but%2520poor%2520performance%2520on%2520document%2520or%2520chart%2520content.%250ATo%2520alleviate%2520the%2520bias%2520of%2520CLIP%2520vision%2520encoder%252C%2520we%2520first%2520delve%2520into%2520the%2520inherent%250Abehavior%2520of%2520different%2520pre-trained%2520vision%2520encoders%2520and%2520then%2520propose%2520the%2520MoVA%252C%2520a%250Apowerful%2520and%2520novel%2520MLLM%252C%2520adaptively%2520routing%2520and%2520fusing%2520task-specific%2520vision%250Aexperts%2520with%2520a%2520coarse-to-fine%2520mechanism.%2520In%2520the%2520coarse-grained%2520stage%252C%2520we%2520design%250Aa%2520context-aware%2520expert%2520routing%2520strategy%2520to%2520dynamically%2520select%2520the%2520most%2520suitable%250Avision%2520experts%2520according%2520to%2520the%2520user%2520instruction%252C%2520input%2520image%252C%2520and%2520expertise%2520of%250Avision%2520experts.%2520This%2520benefits%2520from%2520the%2520powerful%2520model%2520function%2520understanding%250Aability%2520of%2520the%2520large%2520language%2520model%2520%2528LLM%2529.%2520In%2520the%2520fine-grained%2520stage%252C%2520we%250Aelaborately%2520conduct%2520the%2520mixture-of-vision-expert%2520adapter%2520%2528MoV-Adapter%2529%2520to%250Aextract%2520and%2520fuse%2520task-specific%2520knowledge%2520from%2520various%2520experts.%2520This%250Acoarse-to-fine%2520paradigm%2520effectively%2520leverages%2520representations%2520from%2520experts%250Abased%2520on%2520multimodal%2520context%2520and%2520model%2520expertise%252C%2520further%2520enhancing%2520the%250Ageneralization%2520ability.%2520We%2520conduct%2520extensive%2520experiments%2520to%2520evaluate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520approach.%2520Without%2520any%2520bells%2520and%2520whistles%252C%2520MoVA%250Acan%2520achieve%2520significant%2520performance%2520gains%2520over%2520current%2520state-of-the-art%2520methods%250Ain%2520a%2520wide%2520range%2520of%2520challenging%2520multimodal%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.13046v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoVA%3A%20Adapting%20Mixture%20of%20Vision%20Experts%20to%20Multimodal%20Context&entry.906535625=Zhuofan%20Zong%20and%20Bingqi%20Ma%20and%20Dazhong%20Shen%20and%20Guanglu%20Song%20and%20Hao%20Shao%20and%20Dongzhi%20Jiang%20and%20Hongsheng%20Li%20and%20Yu%20Liu&entry.1292438233=%20%20As%20the%20key%20component%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20the%20ability%0Aof%20the%20visual%20encoder%20greatly%20affects%20MLLM%27s%20understanding%20on%20diverse%20image%0Acontent.%20Although%20some%20large-scale%20pretrained%20vision%20encoders%20such%20as%20vision%0Aencoders%20in%20CLIP%20and%20DINOv2%20have%20brought%20promising%20performance%2C%20we%20found%20that%0Athere%20is%20still%20no%20single%20vision%20encoder%20that%20can%20dominate%20various%20image%20content%0Aunderstanding%2C%20e.g.%2C%20the%20CLIP%20vision%20encoder%20leads%20to%20outstanding%20results%20on%0Ageneral%20image%20understanding%20but%20poor%20performance%20on%20document%20or%20chart%20content.%0ATo%20alleviate%20the%20bias%20of%20CLIP%20vision%20encoder%2C%20we%20first%20delve%20into%20the%20inherent%0Abehavior%20of%20different%20pre-trained%20vision%20encoders%20and%20then%20propose%20the%20MoVA%2C%20a%0Apowerful%20and%20novel%20MLLM%2C%20adaptively%20routing%20and%20fusing%20task-specific%20vision%0Aexperts%20with%20a%20coarse-to-fine%20mechanism.%20In%20the%20coarse-grained%20stage%2C%20we%20design%0Aa%20context-aware%20expert%20routing%20strategy%20to%20dynamically%20select%20the%20most%20suitable%0Avision%20experts%20according%20to%20the%20user%20instruction%2C%20input%20image%2C%20and%20expertise%20of%0Avision%20experts.%20This%20benefits%20from%20the%20powerful%20model%20function%20understanding%0Aability%20of%20the%20large%20language%20model%20%28LLM%29.%20In%20the%20fine-grained%20stage%2C%20we%0Aelaborately%20conduct%20the%20mixture-of-vision-expert%20adapter%20%28MoV-Adapter%29%20to%0Aextract%20and%20fuse%20task-specific%20knowledge%20from%20various%20experts.%20This%0Acoarse-to-fine%20paradigm%20effectively%20leverages%20representations%20from%20experts%0Abased%20on%20multimodal%20context%20and%20model%20expertise%2C%20further%20enhancing%20the%0Ageneralization%20ability.%20We%20conduct%20extensive%20experiments%20to%20evaluate%20the%0Aeffectiveness%20of%20the%20proposed%20approach.%20Without%20any%20bells%20and%20whistles%2C%20MoVA%0Acan%20achieve%20significant%20performance%20gains%20over%20current%20state-of-the-art%20methods%0Ain%20a%20wide%20range%20of%20challenging%20multimodal%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13046v2&entry.124074799=Read"},
{"title": "Localization, balance and affinity: a stronger multifaceted\n  collaborative salient object detector in remote sensing images", "author": "Yakun Xie and Suning Liu and Hongyu Chen and Shaohan Cao and Huixin Zhang and Dejun Feng and Qian Wan and Jun Zhu and Qing Zhu", "abstract": "  Despite significant advancements in salient object detection(SOD) in optical\nremote sensing images(ORSI), challenges persist due to the intricate edge\nstructures of ORSIs and the complexity of their contextual relationships.\nCurrent deep learning approaches encounter difficulties in accurately\nidentifying boundary features and lack efficiency in collaboratively modeling\nthe foreground and background by leveraging contextual features. To address\nthese challenges, we propose a stronger multifaceted collaborative salient\nobject detector in ORSIs, termed LBA-MCNet, which incorporates aspects of\nlocalization, balance, and affinity. The network focuses on accurately locating\ntargets, balancing detailed features, and modeling image-level global context\ninformation. Specifically, we design the Edge Feature Adaptive Balancing and\nAdjusting(EFABA) module for precise edge localization, using edge features to\nguide attention to boundaries and preserve spatial details. Moreover, we design\nthe Global Distributed Affinity Learning(GDAL) module to model global context.\nIt captures global context by generating an affinity map from the encoders\nfinal layer, ensuring effective modeling of global patterns. Additionally, deep\nsupervision during deconvolution further enhances feature representation.\nFinally, we compared with 28 state of the art approaches on three publicly\navailable datasets. The results clearly demonstrate the superiority of our\nmethod.\n", "link": "http://arxiv.org/abs/2410.23991v1", "date": "2024-10-31", "relevancy": 3.0927, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6435}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6124}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localization%2C%20balance%20and%20affinity%3A%20a%20stronger%20multifaceted%0A%20%20collaborative%20salient%20object%20detector%20in%20remote%20sensing%20images&body=Title%3A%20Localization%2C%20balance%20and%20affinity%3A%20a%20stronger%20multifaceted%0A%20%20collaborative%20salient%20object%20detector%20in%20remote%20sensing%20images%0AAuthor%3A%20Yakun%20Xie%20and%20Suning%20Liu%20and%20Hongyu%20Chen%20and%20Shaohan%20Cao%20and%20Huixin%20Zhang%20and%20Dejun%20Feng%20and%20Qian%20Wan%20and%20Jun%20Zhu%20and%20Qing%20Zhu%0AAbstract%3A%20%20%20Despite%20significant%20advancements%20in%20salient%20object%20detection%28SOD%29%20in%20optical%0Aremote%20sensing%20images%28ORSI%29%2C%20challenges%20persist%20due%20to%20the%20intricate%20edge%0Astructures%20of%20ORSIs%20and%20the%20complexity%20of%20their%20contextual%20relationships.%0ACurrent%20deep%20learning%20approaches%20encounter%20difficulties%20in%20accurately%0Aidentifying%20boundary%20features%20and%20lack%20efficiency%20in%20collaboratively%20modeling%0Athe%20foreground%20and%20background%20by%20leveraging%20contextual%20features.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20stronger%20multifaceted%20collaborative%20salient%0Aobject%20detector%20in%20ORSIs%2C%20termed%20LBA-MCNet%2C%20which%20incorporates%20aspects%20of%0Alocalization%2C%20balance%2C%20and%20affinity.%20The%20network%20focuses%20on%20accurately%20locating%0Atargets%2C%20balancing%20detailed%20features%2C%20and%20modeling%20image-level%20global%20context%0Ainformation.%20Specifically%2C%20we%20design%20the%20Edge%20Feature%20Adaptive%20Balancing%20and%0AAdjusting%28EFABA%29%20module%20for%20precise%20edge%20localization%2C%20using%20edge%20features%20to%0Aguide%20attention%20to%20boundaries%20and%20preserve%20spatial%20details.%20Moreover%2C%20we%20design%0Athe%20Global%20Distributed%20Affinity%20Learning%28GDAL%29%20module%20to%20model%20global%20context.%0AIt%20captures%20global%20context%20by%20generating%20an%20affinity%20map%20from%20the%20encoders%0Afinal%20layer%2C%20ensuring%20effective%20modeling%20of%20global%20patterns.%20Additionally%2C%20deep%0Asupervision%20during%20deconvolution%20further%20enhances%20feature%20representation.%0AFinally%2C%20we%20compared%20with%2028%20state%20of%20the%20art%20approaches%20on%20three%20publicly%0Aavailable%20datasets.%20The%20results%20clearly%20demonstrate%20the%20superiority%20of%20our%0Amethod.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalization%252C%2520balance%2520and%2520affinity%253A%2520a%2520stronger%2520multifaceted%250A%2520%2520collaborative%2520salient%2520object%2520detector%2520in%2520remote%2520sensing%2520images%26entry.906535625%3DYakun%2520Xie%2520and%2520Suning%2520Liu%2520and%2520Hongyu%2520Chen%2520and%2520Shaohan%2520Cao%2520and%2520Huixin%2520Zhang%2520and%2520Dejun%2520Feng%2520and%2520Qian%2520Wan%2520and%2520Jun%2520Zhu%2520and%2520Qing%2520Zhu%26entry.1292438233%3D%2520%2520Despite%2520significant%2520advancements%2520in%2520salient%2520object%2520detection%2528SOD%2529%2520in%2520optical%250Aremote%2520sensing%2520images%2528ORSI%2529%252C%2520challenges%2520persist%2520due%2520to%2520the%2520intricate%2520edge%250Astructures%2520of%2520ORSIs%2520and%2520the%2520complexity%2520of%2520their%2520contextual%2520relationships.%250ACurrent%2520deep%2520learning%2520approaches%2520encounter%2520difficulties%2520in%2520accurately%250Aidentifying%2520boundary%2520features%2520and%2520lack%2520efficiency%2520in%2520collaboratively%2520modeling%250Athe%2520foreground%2520and%2520background%2520by%2520leveraging%2520contextual%2520features.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520a%2520stronger%2520multifaceted%2520collaborative%2520salient%250Aobject%2520detector%2520in%2520ORSIs%252C%2520termed%2520LBA-MCNet%252C%2520which%2520incorporates%2520aspects%2520of%250Alocalization%252C%2520balance%252C%2520and%2520affinity.%2520The%2520network%2520focuses%2520on%2520accurately%2520locating%250Atargets%252C%2520balancing%2520detailed%2520features%252C%2520and%2520modeling%2520image-level%2520global%2520context%250Ainformation.%2520Specifically%252C%2520we%2520design%2520the%2520Edge%2520Feature%2520Adaptive%2520Balancing%2520and%250AAdjusting%2528EFABA%2529%2520module%2520for%2520precise%2520edge%2520localization%252C%2520using%2520edge%2520features%2520to%250Aguide%2520attention%2520to%2520boundaries%2520and%2520preserve%2520spatial%2520details.%2520Moreover%252C%2520we%2520design%250Athe%2520Global%2520Distributed%2520Affinity%2520Learning%2528GDAL%2529%2520module%2520to%2520model%2520global%2520context.%250AIt%2520captures%2520global%2520context%2520by%2520generating%2520an%2520affinity%2520map%2520from%2520the%2520encoders%250Afinal%2520layer%252C%2520ensuring%2520effective%2520modeling%2520of%2520global%2520patterns.%2520Additionally%252C%2520deep%250Asupervision%2520during%2520deconvolution%2520further%2520enhances%2520feature%2520representation.%250AFinally%252C%2520we%2520compared%2520with%252028%2520state%2520of%2520the%2520art%2520approaches%2520on%2520three%2520publicly%250Aavailable%2520datasets.%2520The%2520results%2520clearly%2520demonstrate%2520the%2520superiority%2520of%2520our%250Amethod.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localization%2C%20balance%20and%20affinity%3A%20a%20stronger%20multifaceted%0A%20%20collaborative%20salient%20object%20detector%20in%20remote%20sensing%20images&entry.906535625=Yakun%20Xie%20and%20Suning%20Liu%20and%20Hongyu%20Chen%20and%20Shaohan%20Cao%20and%20Huixin%20Zhang%20and%20Dejun%20Feng%20and%20Qian%20Wan%20and%20Jun%20Zhu%20and%20Qing%20Zhu&entry.1292438233=%20%20Despite%20significant%20advancements%20in%20salient%20object%20detection%28SOD%29%20in%20optical%0Aremote%20sensing%20images%28ORSI%29%2C%20challenges%20persist%20due%20to%20the%20intricate%20edge%0Astructures%20of%20ORSIs%20and%20the%20complexity%20of%20their%20contextual%20relationships.%0ACurrent%20deep%20learning%20approaches%20encounter%20difficulties%20in%20accurately%0Aidentifying%20boundary%20features%20and%20lack%20efficiency%20in%20collaboratively%20modeling%0Athe%20foreground%20and%20background%20by%20leveraging%20contextual%20features.%20To%20address%0Athese%20challenges%2C%20we%20propose%20a%20stronger%20multifaceted%20collaborative%20salient%0Aobject%20detector%20in%20ORSIs%2C%20termed%20LBA-MCNet%2C%20which%20incorporates%20aspects%20of%0Alocalization%2C%20balance%2C%20and%20affinity.%20The%20network%20focuses%20on%20accurately%20locating%0Atargets%2C%20balancing%20detailed%20features%2C%20and%20modeling%20image-level%20global%20context%0Ainformation.%20Specifically%2C%20we%20design%20the%20Edge%20Feature%20Adaptive%20Balancing%20and%0AAdjusting%28EFABA%29%20module%20for%20precise%20edge%20localization%2C%20using%20edge%20features%20to%0Aguide%20attention%20to%20boundaries%20and%20preserve%20spatial%20details.%20Moreover%2C%20we%20design%0Athe%20Global%20Distributed%20Affinity%20Learning%28GDAL%29%20module%20to%20model%20global%20context.%0AIt%20captures%20global%20context%20by%20generating%20an%20affinity%20map%20from%20the%20encoders%0Afinal%20layer%2C%20ensuring%20effective%20modeling%20of%20global%20patterns.%20Additionally%2C%20deep%0Asupervision%20during%20deconvolution%20further%20enhances%20feature%20representation.%0AFinally%2C%20we%20compared%20with%2028%20state%20of%20the%20art%20approaches%20on%20three%20publicly%0Aavailable%20datasets.%20The%20results%20clearly%20demonstrate%20the%20superiority%20of%20our%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23991v1&entry.124074799=Read"},
{"title": "ImOV3D: Learning Open-Vocabulary Point Clouds 3D Object Detection from\n  Only 2D Images", "author": "Timing Yang and Yuanliang Ju and Li Yi", "abstract": "  Open-vocabulary 3D object detection (OV-3Det) aims to generalize beyond the\nlimited number of base categories labeled during the training phase. The\nbiggest bottleneck is the scarcity of annotated 3D data, whereas 2D image\ndatasets are abundant and richly annotated. Consequently, it is intuitive to\nleverage the wealth of annotations in 2D images to alleviate the inherent data\nscarcity in OV-3Det. In this paper, we push the task setup to its limits by\nexploring the potential of using solely 2D images to learn OV-3Det. The major\nchallenges for this setup is the modality gap between training images and\ntesting point clouds, which prevents effective integration of 2D knowledge into\nOV-3Det. To address this challenge, we propose a novel framework ImOV3D to\nleverage pseudo multimodal representation containing both images and point\nclouds (PC) to close the modality gap. The key of ImOV3D lies in flexible\nmodality conversion where 2D images can be lifted into 3D using monocular depth\nestimation and can also be derived from 3D scenes through rendering. This\nallows unifying both training images and testing point clouds into a common\nimage-PC representation, encompassing a wealth of 2D semantic information and\nalso incorporating the depth and structural characteristics of 3D spatial data.\nWe carefully conduct such conversion to minimize the domain gap between\ntraining and test cases. Extensive experiments on two benchmark datasets,\nSUNRGBD and ScanNet, show that ImOV3D significantly outperforms existing\nmethods, even in the absence of ground truth 3D training data. With the\ninclusion of a minimal amount of real 3D data for fine-tuning, the performance\nalso significantly surpasses previous state-of-the-art. Codes and pre-trained\nmodels are released on the https://github.com/yangtiming/ImOV3D.\n", "link": "http://arxiv.org/abs/2410.24001v1", "date": "2024-10-31", "relevancy": 3.0887, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6203}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6165}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ImOV3D%3A%20Learning%20Open-Vocabulary%20Point%20Clouds%203D%20Object%20Detection%20from%0A%20%20Only%202D%20Images&body=Title%3A%20ImOV3D%3A%20Learning%20Open-Vocabulary%20Point%20Clouds%203D%20Object%20Detection%20from%0A%20%20Only%202D%20Images%0AAuthor%3A%20Timing%20Yang%20and%20Yuanliang%20Ju%20and%20Li%20Yi%0AAbstract%3A%20%20%20Open-vocabulary%203D%20object%20detection%20%28OV-3Det%29%20aims%20to%20generalize%20beyond%20the%0Alimited%20number%20of%20base%20categories%20labeled%20during%20the%20training%20phase.%20The%0Abiggest%20bottleneck%20is%20the%20scarcity%20of%20annotated%203D%20data%2C%20whereas%202D%20image%0Adatasets%20are%20abundant%20and%20richly%20annotated.%20Consequently%2C%20it%20is%20intuitive%20to%0Aleverage%20the%20wealth%20of%20annotations%20in%202D%20images%20to%20alleviate%20the%20inherent%20data%0Ascarcity%20in%20OV-3Det.%20In%20this%20paper%2C%20we%20push%20the%20task%20setup%20to%20its%20limits%20by%0Aexploring%20the%20potential%20of%20using%20solely%202D%20images%20to%20learn%20OV-3Det.%20The%20major%0Achallenges%20for%20this%20setup%20is%20the%20modality%20gap%20between%20training%20images%20and%0Atesting%20point%20clouds%2C%20which%20prevents%20effective%20integration%20of%202D%20knowledge%20into%0AOV-3Det.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20framework%20ImOV3D%20to%0Aleverage%20pseudo%20multimodal%20representation%20containing%20both%20images%20and%20point%0Aclouds%20%28PC%29%20to%20close%20the%20modality%20gap.%20The%20key%20of%20ImOV3D%20lies%20in%20flexible%0Amodality%20conversion%20where%202D%20images%20can%20be%20lifted%20into%203D%20using%20monocular%20depth%0Aestimation%20and%20can%20also%20be%20derived%20from%203D%20scenes%20through%20rendering.%20This%0Aallows%20unifying%20both%20training%20images%20and%20testing%20point%20clouds%20into%20a%20common%0Aimage-PC%20representation%2C%20encompassing%20a%20wealth%20of%202D%20semantic%20information%20and%0Aalso%20incorporating%20the%20depth%20and%20structural%20characteristics%20of%203D%20spatial%20data.%0AWe%20carefully%20conduct%20such%20conversion%20to%20minimize%20the%20domain%20gap%20between%0Atraining%20and%20test%20cases.%20Extensive%20experiments%20on%20two%20benchmark%20datasets%2C%0ASUNRGBD%20and%20ScanNet%2C%20show%20that%20ImOV3D%20significantly%20outperforms%20existing%0Amethods%2C%20even%20in%20the%20absence%20of%20ground%20truth%203D%20training%20data.%20With%20the%0Ainclusion%20of%20a%20minimal%20amount%20of%20real%203D%20data%20for%20fine-tuning%2C%20the%20performance%0Aalso%20significantly%20surpasses%20previous%20state-of-the-art.%20Codes%20and%20pre-trained%0Amodels%20are%20released%20on%20the%20https%3A//github.com/yangtiming/ImOV3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImOV3D%253A%2520Learning%2520Open-Vocabulary%2520Point%2520Clouds%25203D%2520Object%2520Detection%2520from%250A%2520%2520Only%25202D%2520Images%26entry.906535625%3DTiming%2520Yang%2520and%2520Yuanliang%2520Ju%2520and%2520Li%2520Yi%26entry.1292438233%3D%2520%2520Open-vocabulary%25203D%2520object%2520detection%2520%2528OV-3Det%2529%2520aims%2520to%2520generalize%2520beyond%2520the%250Alimited%2520number%2520of%2520base%2520categories%2520labeled%2520during%2520the%2520training%2520phase.%2520The%250Abiggest%2520bottleneck%2520is%2520the%2520scarcity%2520of%2520annotated%25203D%2520data%252C%2520whereas%25202D%2520image%250Adatasets%2520are%2520abundant%2520and%2520richly%2520annotated.%2520Consequently%252C%2520it%2520is%2520intuitive%2520to%250Aleverage%2520the%2520wealth%2520of%2520annotations%2520in%25202D%2520images%2520to%2520alleviate%2520the%2520inherent%2520data%250Ascarcity%2520in%2520OV-3Det.%2520In%2520this%2520paper%252C%2520we%2520push%2520the%2520task%2520setup%2520to%2520its%2520limits%2520by%250Aexploring%2520the%2520potential%2520of%2520using%2520solely%25202D%2520images%2520to%2520learn%2520OV-3Det.%2520The%2520major%250Achallenges%2520for%2520this%2520setup%2520is%2520the%2520modality%2520gap%2520between%2520training%2520images%2520and%250Atesting%2520point%2520clouds%252C%2520which%2520prevents%2520effective%2520integration%2520of%25202D%2520knowledge%2520into%250AOV-3Det.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520novel%2520framework%2520ImOV3D%2520to%250Aleverage%2520pseudo%2520multimodal%2520representation%2520containing%2520both%2520images%2520and%2520point%250Aclouds%2520%2528PC%2529%2520to%2520close%2520the%2520modality%2520gap.%2520The%2520key%2520of%2520ImOV3D%2520lies%2520in%2520flexible%250Amodality%2520conversion%2520where%25202D%2520images%2520can%2520be%2520lifted%2520into%25203D%2520using%2520monocular%2520depth%250Aestimation%2520and%2520can%2520also%2520be%2520derived%2520from%25203D%2520scenes%2520through%2520rendering.%2520This%250Aallows%2520unifying%2520both%2520training%2520images%2520and%2520testing%2520point%2520clouds%2520into%2520a%2520common%250Aimage-PC%2520representation%252C%2520encompassing%2520a%2520wealth%2520of%25202D%2520semantic%2520information%2520and%250Aalso%2520incorporating%2520the%2520depth%2520and%2520structural%2520characteristics%2520of%25203D%2520spatial%2520data.%250AWe%2520carefully%2520conduct%2520such%2520conversion%2520to%2520minimize%2520the%2520domain%2520gap%2520between%250Atraining%2520and%2520test%2520cases.%2520Extensive%2520experiments%2520on%2520two%2520benchmark%2520datasets%252C%250ASUNRGBD%2520and%2520ScanNet%252C%2520show%2520that%2520ImOV3D%2520significantly%2520outperforms%2520existing%250Amethods%252C%2520even%2520in%2520the%2520absence%2520of%2520ground%2520truth%25203D%2520training%2520data.%2520With%2520the%250Ainclusion%2520of%2520a%2520minimal%2520amount%2520of%2520real%25203D%2520data%2520for%2520fine-tuning%252C%2520the%2520performance%250Aalso%2520significantly%2520surpasses%2520previous%2520state-of-the-art.%2520Codes%2520and%2520pre-trained%250Amodels%2520are%2520released%2520on%2520the%2520https%253A//github.com/yangtiming/ImOV3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ImOV3D%3A%20Learning%20Open-Vocabulary%20Point%20Clouds%203D%20Object%20Detection%20from%0A%20%20Only%202D%20Images&entry.906535625=Timing%20Yang%20and%20Yuanliang%20Ju%20and%20Li%20Yi&entry.1292438233=%20%20Open-vocabulary%203D%20object%20detection%20%28OV-3Det%29%20aims%20to%20generalize%20beyond%20the%0Alimited%20number%20of%20base%20categories%20labeled%20during%20the%20training%20phase.%20The%0Abiggest%20bottleneck%20is%20the%20scarcity%20of%20annotated%203D%20data%2C%20whereas%202D%20image%0Adatasets%20are%20abundant%20and%20richly%20annotated.%20Consequently%2C%20it%20is%20intuitive%20to%0Aleverage%20the%20wealth%20of%20annotations%20in%202D%20images%20to%20alleviate%20the%20inherent%20data%0Ascarcity%20in%20OV-3Det.%20In%20this%20paper%2C%20we%20push%20the%20task%20setup%20to%20its%20limits%20by%0Aexploring%20the%20potential%20of%20using%20solely%202D%20images%20to%20learn%20OV-3Det.%20The%20major%0Achallenges%20for%20this%20setup%20is%20the%20modality%20gap%20between%20training%20images%20and%0Atesting%20point%20clouds%2C%20which%20prevents%20effective%20integration%20of%202D%20knowledge%20into%0AOV-3Det.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20framework%20ImOV3D%20to%0Aleverage%20pseudo%20multimodal%20representation%20containing%20both%20images%20and%20point%0Aclouds%20%28PC%29%20to%20close%20the%20modality%20gap.%20The%20key%20of%20ImOV3D%20lies%20in%20flexible%0Amodality%20conversion%20where%202D%20images%20can%20be%20lifted%20into%203D%20using%20monocular%20depth%0Aestimation%20and%20can%20also%20be%20derived%20from%203D%20scenes%20through%20rendering.%20This%0Aallows%20unifying%20both%20training%20images%20and%20testing%20point%20clouds%20into%20a%20common%0Aimage-PC%20representation%2C%20encompassing%20a%20wealth%20of%202D%20semantic%20information%20and%0Aalso%20incorporating%20the%20depth%20and%20structural%20characteristics%20of%203D%20spatial%20data.%0AWe%20carefully%20conduct%20such%20conversion%20to%20minimize%20the%20domain%20gap%20between%0Atraining%20and%20test%20cases.%20Extensive%20experiments%20on%20two%20benchmark%20datasets%2C%0ASUNRGBD%20and%20ScanNet%2C%20show%20that%20ImOV3D%20significantly%20outperforms%20existing%0Amethods%2C%20even%20in%20the%20absence%20of%20ground%20truth%203D%20training%20data.%20With%20the%0Ainclusion%20of%20a%20minimal%20amount%20of%20real%203D%20data%20for%20fine-tuning%2C%20the%20performance%0Aalso%20significantly%20surpasses%20previous%20state-of-the-art.%20Codes%20and%20pre-trained%0Amodels%20are%20released%20on%20the%20https%3A//github.com/yangtiming/ImOV3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24001v1&entry.124074799=Read"},
{"title": "Stereo-Talker: Audio-driven 3D Human Synthesis with Prior-Guided\n  Mixture-of-Experts", "author": "Xiang Deng and Youxin Pang and Xiaochen Zhao and Chao Xu and Lizhen Wang and Hongjiang Xiao and Shi Yan and Hongwen Zhang and Yebin Liu", "abstract": "  This paper introduces Stereo-Talker, a novel one-shot audio-driven human\nvideo synthesis system that generates 3D talking videos with precise lip\nsynchronization, expressive body gestures, temporally consistent\nphoto-realistic quality, and continuous viewpoint control. The process follows\na two-stage approach. In the first stage, the system maps audio input to\nhigh-fidelity motion sequences, encompassing upper-body gestures and facial\nexpressions. To enrich motion diversity and authenticity, large language model\n(LLM) priors are integrated with text-aligned semantic audio features,\nleveraging LLMs' cross-modal generalization power to enhance motion quality. In\nthe second stage, we improve diffusion-based video generation models by\nincorporating a prior-guided Mixture-of-Experts (MoE) mechanism: a view-guided\nMoE focuses on view-specific attributes, while a mask-guided MoE enhances\nregion-based rendering stability. Additionally, a mask prediction module is\ndevised to derive human masks from motion data, enhancing the stability and\naccuracy of masks and enabling mask guiding during inference. We also introduce\na comprehensive human video dataset with 2,203 identities, covering diverse\nbody gestures and detailed annotations, facilitating broad generalization. The\ncode, data, and pre-trained models will be released for research purposes.\n", "link": "http://arxiv.org/abs/2410.23836v1", "date": "2024-10-31", "relevancy": 3.0244, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6086}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6086}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stereo-Talker%3A%20Audio-driven%203D%20Human%20Synthesis%20with%20Prior-Guided%0A%20%20Mixture-of-Experts&body=Title%3A%20Stereo-Talker%3A%20Audio-driven%203D%20Human%20Synthesis%20with%20Prior-Guided%0A%20%20Mixture-of-Experts%0AAuthor%3A%20Xiang%20Deng%20and%20Youxin%20Pang%20and%20Xiaochen%20Zhao%20and%20Chao%20Xu%20and%20Lizhen%20Wang%20and%20Hongjiang%20Xiao%20and%20Shi%20Yan%20and%20Hongwen%20Zhang%20and%20Yebin%20Liu%0AAbstract%3A%20%20%20This%20paper%20introduces%20Stereo-Talker%2C%20a%20novel%20one-shot%20audio-driven%20human%0Avideo%20synthesis%20system%20that%20generates%203D%20talking%20videos%20with%20precise%20lip%0Asynchronization%2C%20expressive%20body%20gestures%2C%20temporally%20consistent%0Aphoto-realistic%20quality%2C%20and%20continuous%20viewpoint%20control.%20The%20process%20follows%0Aa%20two-stage%20approach.%20In%20the%20first%20stage%2C%20the%20system%20maps%20audio%20input%20to%0Ahigh-fidelity%20motion%20sequences%2C%20encompassing%20upper-body%20gestures%20and%20facial%0Aexpressions.%20To%20enrich%20motion%20diversity%20and%20authenticity%2C%20large%20language%20model%0A%28LLM%29%20priors%20are%20integrated%20with%20text-aligned%20semantic%20audio%20features%2C%0Aleveraging%20LLMs%27%20cross-modal%20generalization%20power%20to%20enhance%20motion%20quality.%20In%0Athe%20second%20stage%2C%20we%20improve%20diffusion-based%20video%20generation%20models%20by%0Aincorporating%20a%20prior-guided%20Mixture-of-Experts%20%28MoE%29%20mechanism%3A%20a%20view-guided%0AMoE%20focuses%20on%20view-specific%20attributes%2C%20while%20a%20mask-guided%20MoE%20enhances%0Aregion-based%20rendering%20stability.%20Additionally%2C%20a%20mask%20prediction%20module%20is%0Adevised%20to%20derive%20human%20masks%20from%20motion%20data%2C%20enhancing%20the%20stability%20and%0Aaccuracy%20of%20masks%20and%20enabling%20mask%20guiding%20during%20inference.%20We%20also%20introduce%0Aa%20comprehensive%20human%20video%20dataset%20with%202%2C203%20identities%2C%20covering%20diverse%0Abody%20gestures%20and%20detailed%20annotations%2C%20facilitating%20broad%20generalization.%20The%0Acode%2C%20data%2C%20and%20pre-trained%20models%20will%20be%20released%20for%20research%20purposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStereo-Talker%253A%2520Audio-driven%25203D%2520Human%2520Synthesis%2520with%2520Prior-Guided%250A%2520%2520Mixture-of-Experts%26entry.906535625%3DXiang%2520Deng%2520and%2520Youxin%2520Pang%2520and%2520Xiaochen%2520Zhao%2520and%2520Chao%2520Xu%2520and%2520Lizhen%2520Wang%2520and%2520Hongjiang%2520Xiao%2520and%2520Shi%2520Yan%2520and%2520Hongwen%2520Zhang%2520and%2520Yebin%2520Liu%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520Stereo-Talker%252C%2520a%2520novel%2520one-shot%2520audio-driven%2520human%250Avideo%2520synthesis%2520system%2520that%2520generates%25203D%2520talking%2520videos%2520with%2520precise%2520lip%250Asynchronization%252C%2520expressive%2520body%2520gestures%252C%2520temporally%2520consistent%250Aphoto-realistic%2520quality%252C%2520and%2520continuous%2520viewpoint%2520control.%2520The%2520process%2520follows%250Aa%2520two-stage%2520approach.%2520In%2520the%2520first%2520stage%252C%2520the%2520system%2520maps%2520audio%2520input%2520to%250Ahigh-fidelity%2520motion%2520sequences%252C%2520encompassing%2520upper-body%2520gestures%2520and%2520facial%250Aexpressions.%2520To%2520enrich%2520motion%2520diversity%2520and%2520authenticity%252C%2520large%2520language%2520model%250A%2528LLM%2529%2520priors%2520are%2520integrated%2520with%2520text-aligned%2520semantic%2520audio%2520features%252C%250Aleveraging%2520LLMs%2527%2520cross-modal%2520generalization%2520power%2520to%2520enhance%2520motion%2520quality.%2520In%250Athe%2520second%2520stage%252C%2520we%2520improve%2520diffusion-based%2520video%2520generation%2520models%2520by%250Aincorporating%2520a%2520prior-guided%2520Mixture-of-Experts%2520%2528MoE%2529%2520mechanism%253A%2520a%2520view-guided%250AMoE%2520focuses%2520on%2520view-specific%2520attributes%252C%2520while%2520a%2520mask-guided%2520MoE%2520enhances%250Aregion-based%2520rendering%2520stability.%2520Additionally%252C%2520a%2520mask%2520prediction%2520module%2520is%250Adevised%2520to%2520derive%2520human%2520masks%2520from%2520motion%2520data%252C%2520enhancing%2520the%2520stability%2520and%250Aaccuracy%2520of%2520masks%2520and%2520enabling%2520mask%2520guiding%2520during%2520inference.%2520We%2520also%2520introduce%250Aa%2520comprehensive%2520human%2520video%2520dataset%2520with%25202%252C203%2520identities%252C%2520covering%2520diverse%250Abody%2520gestures%2520and%2520detailed%2520annotations%252C%2520facilitating%2520broad%2520generalization.%2520The%250Acode%252C%2520data%252C%2520and%2520pre-trained%2520models%2520will%2520be%2520released%2520for%2520research%2520purposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stereo-Talker%3A%20Audio-driven%203D%20Human%20Synthesis%20with%20Prior-Guided%0A%20%20Mixture-of-Experts&entry.906535625=Xiang%20Deng%20and%20Youxin%20Pang%20and%20Xiaochen%20Zhao%20and%20Chao%20Xu%20and%20Lizhen%20Wang%20and%20Hongjiang%20Xiao%20and%20Shi%20Yan%20and%20Hongwen%20Zhang%20and%20Yebin%20Liu&entry.1292438233=%20%20This%20paper%20introduces%20Stereo-Talker%2C%20a%20novel%20one-shot%20audio-driven%20human%0Avideo%20synthesis%20system%20that%20generates%203D%20talking%20videos%20with%20precise%20lip%0Asynchronization%2C%20expressive%20body%20gestures%2C%20temporally%20consistent%0Aphoto-realistic%20quality%2C%20and%20continuous%20viewpoint%20control.%20The%20process%20follows%0Aa%20two-stage%20approach.%20In%20the%20first%20stage%2C%20the%20system%20maps%20audio%20input%20to%0Ahigh-fidelity%20motion%20sequences%2C%20encompassing%20upper-body%20gestures%20and%20facial%0Aexpressions.%20To%20enrich%20motion%20diversity%20and%20authenticity%2C%20large%20language%20model%0A%28LLM%29%20priors%20are%20integrated%20with%20text-aligned%20semantic%20audio%20features%2C%0Aleveraging%20LLMs%27%20cross-modal%20generalization%20power%20to%20enhance%20motion%20quality.%20In%0Athe%20second%20stage%2C%20we%20improve%20diffusion-based%20video%20generation%20models%20by%0Aincorporating%20a%20prior-guided%20Mixture-of-Experts%20%28MoE%29%20mechanism%3A%20a%20view-guided%0AMoE%20focuses%20on%20view-specific%20attributes%2C%20while%20a%20mask-guided%20MoE%20enhances%0Aregion-based%20rendering%20stability.%20Additionally%2C%20a%20mask%20prediction%20module%20is%0Adevised%20to%20derive%20human%20masks%20from%20motion%20data%2C%20enhancing%20the%20stability%20and%0Aaccuracy%20of%20masks%20and%20enabling%20mask%20guiding%20during%20inference.%20We%20also%20introduce%0Aa%20comprehensive%20human%20video%20dataset%20with%202%2C203%20identities%2C%20covering%20diverse%0Abody%20gestures%20and%20detailed%20annotations%2C%20facilitating%20broad%20generalization.%20The%0Acode%2C%20data%2C%20and%20pre-trained%20models%20will%20be%20released%20for%20research%20purposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23836v1&entry.124074799=Read"},
{"title": "GIC: Gaussian-Informed Continuum for Physical Property Identification\n  and Simulation", "author": "Junhao Cai and Yuji Yang and Weihao Yuan and Yisheng He and Zilong Dong and Liefeng Bo and Hui Cheng and Qifeng Chen", "abstract": "  This paper studies the problem of estimating physical properties (system\nidentification) through visual observations. To facilitate geometry-aware\nguidance in physical property estimation, we introduce a novel hybrid framework\nthat leverages 3D Gaussian representation to not only capture explicit shapes\nbut also enable the simulated continuum to render object masks as 2D shape\nsurrogates during training. We propose a new dynamic 3D Gaussian framework\nbased on motion factorization to recover the object as 3D Gaussian point sets\nacross different time states. Furthermore, we develop a coarse-to-fine filling\nstrategy to generate the density fields of the object from the Gaussian\nreconstruction, allowing for the extraction of object continuums along with\ntheir surfaces and the integration of Gaussian attributes into these continuum.\nIn addition to the extracted object surfaces, the Gaussian-informed continuum\nalso enables the rendering of object masks during simulations, serving as\n2D-shape guidance for physical property estimation. Extensive experimental\nevaluations demonstrate that our pipeline achieves state-of-the-art performance\nacross multiple benchmarks and metrics. Additionally, we illustrate the\neffectiveness of the proposed method through real-world demonstrations,\nshowcasing its practical utility. Our project page is at\nhttps://jukgei.github.io/project/gic.\n", "link": "http://arxiv.org/abs/2406.14927v3", "date": "2024-10-31", "relevancy": 3.02, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6149}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6055}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GIC%3A%20Gaussian-Informed%20Continuum%20for%20Physical%20Property%20Identification%0A%20%20and%20Simulation&body=Title%3A%20GIC%3A%20Gaussian-Informed%20Continuum%20for%20Physical%20Property%20Identification%0A%20%20and%20Simulation%0AAuthor%3A%20Junhao%20Cai%20and%20Yuji%20Yang%20and%20Weihao%20Yuan%20and%20Yisheng%20He%20and%20Zilong%20Dong%20and%20Liefeng%20Bo%20and%20Hui%20Cheng%20and%20Qifeng%20Chen%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20problem%20of%20estimating%20physical%20properties%20%28system%0Aidentification%29%20through%20visual%20observations.%20To%20facilitate%20geometry-aware%0Aguidance%20in%20physical%20property%20estimation%2C%20we%20introduce%20a%20novel%20hybrid%20framework%0Athat%20leverages%203D%20Gaussian%20representation%20to%20not%20only%20capture%20explicit%20shapes%0Abut%20also%20enable%20the%20simulated%20continuum%20to%20render%20object%20masks%20as%202D%20shape%0Asurrogates%20during%20training.%20We%20propose%20a%20new%20dynamic%203D%20Gaussian%20framework%0Abased%20on%20motion%20factorization%20to%20recover%20the%20object%20as%203D%20Gaussian%20point%20sets%0Aacross%20different%20time%20states.%20Furthermore%2C%20we%20develop%20a%20coarse-to-fine%20filling%0Astrategy%20to%20generate%20the%20density%20fields%20of%20the%20object%20from%20the%20Gaussian%0Areconstruction%2C%20allowing%20for%20the%20extraction%20of%20object%20continuums%20along%20with%0Atheir%20surfaces%20and%20the%20integration%20of%20Gaussian%20attributes%20into%20these%20continuum.%0AIn%20addition%20to%20the%20extracted%20object%20surfaces%2C%20the%20Gaussian-informed%20continuum%0Aalso%20enables%20the%20rendering%20of%20object%20masks%20during%20simulations%2C%20serving%20as%0A2D-shape%20guidance%20for%20physical%20property%20estimation.%20Extensive%20experimental%0Aevaluations%20demonstrate%20that%20our%20pipeline%20achieves%20state-of-the-art%20performance%0Aacross%20multiple%20benchmarks%20and%20metrics.%20Additionally%2C%20we%20illustrate%20the%0Aeffectiveness%20of%20the%20proposed%20method%20through%20real-world%20demonstrations%2C%0Ashowcasing%20its%20practical%20utility.%20Our%20project%20page%20is%20at%0Ahttps%3A//jukgei.github.io/project/gic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14927v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGIC%253A%2520Gaussian-Informed%2520Continuum%2520for%2520Physical%2520Property%2520Identification%250A%2520%2520and%2520Simulation%26entry.906535625%3DJunhao%2520Cai%2520and%2520Yuji%2520Yang%2520and%2520Weihao%2520Yuan%2520and%2520Yisheng%2520He%2520and%2520Zilong%2520Dong%2520and%2520Liefeng%2520Bo%2520and%2520Hui%2520Cheng%2520and%2520Qifeng%2520Chen%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520the%2520problem%2520of%2520estimating%2520physical%2520properties%2520%2528system%250Aidentification%2529%2520through%2520visual%2520observations.%2520To%2520facilitate%2520geometry-aware%250Aguidance%2520in%2520physical%2520property%2520estimation%252C%2520we%2520introduce%2520a%2520novel%2520hybrid%2520framework%250Athat%2520leverages%25203D%2520Gaussian%2520representation%2520to%2520not%2520only%2520capture%2520explicit%2520shapes%250Abut%2520also%2520enable%2520the%2520simulated%2520continuum%2520to%2520render%2520object%2520masks%2520as%25202D%2520shape%250Asurrogates%2520during%2520training.%2520We%2520propose%2520a%2520new%2520dynamic%25203D%2520Gaussian%2520framework%250Abased%2520on%2520motion%2520factorization%2520to%2520recover%2520the%2520object%2520as%25203D%2520Gaussian%2520point%2520sets%250Aacross%2520different%2520time%2520states.%2520Furthermore%252C%2520we%2520develop%2520a%2520coarse-to-fine%2520filling%250Astrategy%2520to%2520generate%2520the%2520density%2520fields%2520of%2520the%2520object%2520from%2520the%2520Gaussian%250Areconstruction%252C%2520allowing%2520for%2520the%2520extraction%2520of%2520object%2520continuums%2520along%2520with%250Atheir%2520surfaces%2520and%2520the%2520integration%2520of%2520Gaussian%2520attributes%2520into%2520these%2520continuum.%250AIn%2520addition%2520to%2520the%2520extracted%2520object%2520surfaces%252C%2520the%2520Gaussian-informed%2520continuum%250Aalso%2520enables%2520the%2520rendering%2520of%2520object%2520masks%2520during%2520simulations%252C%2520serving%2520as%250A2D-shape%2520guidance%2520for%2520physical%2520property%2520estimation.%2520Extensive%2520experimental%250Aevaluations%2520demonstrate%2520that%2520our%2520pipeline%2520achieves%2520state-of-the-art%2520performance%250Aacross%2520multiple%2520benchmarks%2520and%2520metrics.%2520Additionally%252C%2520we%2520illustrate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520method%2520through%2520real-world%2520demonstrations%252C%250Ashowcasing%2520its%2520practical%2520utility.%2520Our%2520project%2520page%2520is%2520at%250Ahttps%253A//jukgei.github.io/project/gic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14927v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GIC%3A%20Gaussian-Informed%20Continuum%20for%20Physical%20Property%20Identification%0A%20%20and%20Simulation&entry.906535625=Junhao%20Cai%20and%20Yuji%20Yang%20and%20Weihao%20Yuan%20and%20Yisheng%20He%20and%20Zilong%20Dong%20and%20Liefeng%20Bo%20and%20Hui%20Cheng%20and%20Qifeng%20Chen&entry.1292438233=%20%20This%20paper%20studies%20the%20problem%20of%20estimating%20physical%20properties%20%28system%0Aidentification%29%20through%20visual%20observations.%20To%20facilitate%20geometry-aware%0Aguidance%20in%20physical%20property%20estimation%2C%20we%20introduce%20a%20novel%20hybrid%20framework%0Athat%20leverages%203D%20Gaussian%20representation%20to%20not%20only%20capture%20explicit%20shapes%0Abut%20also%20enable%20the%20simulated%20continuum%20to%20render%20object%20masks%20as%202D%20shape%0Asurrogates%20during%20training.%20We%20propose%20a%20new%20dynamic%203D%20Gaussian%20framework%0Abased%20on%20motion%20factorization%20to%20recover%20the%20object%20as%203D%20Gaussian%20point%20sets%0Aacross%20different%20time%20states.%20Furthermore%2C%20we%20develop%20a%20coarse-to-fine%20filling%0Astrategy%20to%20generate%20the%20density%20fields%20of%20the%20object%20from%20the%20Gaussian%0Areconstruction%2C%20allowing%20for%20the%20extraction%20of%20object%20continuums%20along%20with%0Atheir%20surfaces%20and%20the%20integration%20of%20Gaussian%20attributes%20into%20these%20continuum.%0AIn%20addition%20to%20the%20extracted%20object%20surfaces%2C%20the%20Gaussian-informed%20continuum%0Aalso%20enables%20the%20rendering%20of%20object%20masks%20during%20simulations%2C%20serving%20as%0A2D-shape%20guidance%20for%20physical%20property%20estimation.%20Extensive%20experimental%0Aevaluations%20demonstrate%20that%20our%20pipeline%20achieves%20state-of-the-art%20performance%0Aacross%20multiple%20benchmarks%20and%20metrics.%20Additionally%2C%20we%20illustrate%20the%0Aeffectiveness%20of%20the%20proposed%20method%20through%20real-world%20demonstrations%2C%0Ashowcasing%20its%20practical%20utility.%20Our%20project%20page%20is%20at%0Ahttps%3A//jukgei.github.io/project/gic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14927v3&entry.124074799=Read"},
{"title": "ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D\n  Facial Animation Synthesis Using VQ-VAE", "author": "Sichun Wu and Kazi Injamamul Haque and Zerrin Yumak", "abstract": "  Audio-driven 3D facial animation synthesis has been an active field of\nresearch with attention from both academia and industry. While there are\npromising results in this area, recent approaches largely focus on lip-sync and\nidentity control, neglecting the role of emotions and emotion control in the\ngenerative process. That is mainly due to the lack of emotionally rich facial\nanimation data and algorithms that can synthesize speech animations with\nemotional expressions at the same time. In addition, majority of the models are\ndeterministic, meaning given the same audio input, they produce the same output\nmotion. We argue that emotions and non-determinism are crucial to generate\ndiverse and emotionally-rich facial animations. In this paper, we propose\nProbTalk3D a non-deterministic neural network approach for emotion controllable\nspeech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and\nan emotionally rich facial animation dataset 3DMEAD. We provide an extensive\ncomparative analysis of our model against the recent 3D facial animation\nsynthesis approaches, by evaluating the results objectively, qualitatively, and\nwith a perceptual user study. We highlight several objective metrics that are\nmore suitable for evaluating stochastic outputs and use both in-the-wild and\nground truth data for subjective evaluation. To our knowledge, that is the\nfirst non-deterministic 3D facial animation synthesis method incorporating a\nrich emotion dataset and emotion control with emotion labels and intensity\nlevels. Our evaluation demonstrates that the proposed model achieves superior\nperformance compared to state-of-the-art emotion-controlled, deterministic and\nnon-deterministic models. We recommend watching the supplementary video for\nquality judgement. The entire codebase is publicly available\n(https://github.com/uuembodiedsocialai/ProbTalk3D/).\n", "link": "http://arxiv.org/abs/2409.07966v3", "date": "2024-10-31", "relevancy": 2.9729, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5985}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5985}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProbTalk3D%3A%20Non-Deterministic%20Emotion%20Controllable%20Speech-Driven%203D%0A%20%20Facial%20Animation%20Synthesis%20Using%20VQ-VAE&body=Title%3A%20ProbTalk3D%3A%20Non-Deterministic%20Emotion%20Controllable%20Speech-Driven%203D%0A%20%20Facial%20Animation%20Synthesis%20Using%20VQ-VAE%0AAuthor%3A%20Sichun%20Wu%20and%20Kazi%20Injamamul%20Haque%20and%20Zerrin%20Yumak%0AAbstract%3A%20%20%20Audio-driven%203D%20facial%20animation%20synthesis%20has%20been%20an%20active%20field%20of%0Aresearch%20with%20attention%20from%20both%20academia%20and%20industry.%20While%20there%20are%0Apromising%20results%20in%20this%20area%2C%20recent%20approaches%20largely%20focus%20on%20lip-sync%20and%0Aidentity%20control%2C%20neglecting%20the%20role%20of%20emotions%20and%20emotion%20control%20in%20the%0Agenerative%20process.%20That%20is%20mainly%20due%20to%20the%20lack%20of%20emotionally%20rich%20facial%0Aanimation%20data%20and%20algorithms%20that%20can%20synthesize%20speech%20animations%20with%0Aemotional%20expressions%20at%20the%20same%20time.%20In%20addition%2C%20majority%20of%20the%20models%20are%0Adeterministic%2C%20meaning%20given%20the%20same%20audio%20input%2C%20they%20produce%20the%20same%20output%0Amotion.%20We%20argue%20that%20emotions%20and%20non-determinism%20are%20crucial%20to%20generate%0Adiverse%20and%20emotionally-rich%20facial%20animations.%20In%20this%20paper%2C%20we%20propose%0AProbTalk3D%20a%20non-deterministic%20neural%20network%20approach%20for%20emotion%20controllable%0Aspeech-driven%203D%20facial%20animation%20synthesis%20using%20a%20two-stage%20VQ-VAE%20model%20and%0Aan%20emotionally%20rich%20facial%20animation%20dataset%203DMEAD.%20We%20provide%20an%20extensive%0Acomparative%20analysis%20of%20our%20model%20against%20the%20recent%203D%20facial%20animation%0Asynthesis%20approaches%2C%20by%20evaluating%20the%20results%20objectively%2C%20qualitatively%2C%20and%0Awith%20a%20perceptual%20user%20study.%20We%20highlight%20several%20objective%20metrics%20that%20are%0Amore%20suitable%20for%20evaluating%20stochastic%20outputs%20and%20use%20both%20in-the-wild%20and%0Aground%20truth%20data%20for%20subjective%20evaluation.%20To%20our%20knowledge%2C%20that%20is%20the%0Afirst%20non-deterministic%203D%20facial%20animation%20synthesis%20method%20incorporating%20a%0Arich%20emotion%20dataset%20and%20emotion%20control%20with%20emotion%20labels%20and%20intensity%0Alevels.%20Our%20evaluation%20demonstrates%20that%20the%20proposed%20model%20achieves%20superior%0Aperformance%20compared%20to%20state-of-the-art%20emotion-controlled%2C%20deterministic%20and%0Anon-deterministic%20models.%20We%20recommend%20watching%20the%20supplementary%20video%20for%0Aquality%20judgement.%20The%20entire%20codebase%20is%20publicly%20available%0A%28https%3A//github.com/uuembodiedsocialai/ProbTalk3D/%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07966v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbTalk3D%253A%2520Non-Deterministic%2520Emotion%2520Controllable%2520Speech-Driven%25203D%250A%2520%2520Facial%2520Animation%2520Synthesis%2520Using%2520VQ-VAE%26entry.906535625%3DSichun%2520Wu%2520and%2520Kazi%2520Injamamul%2520Haque%2520and%2520Zerrin%2520Yumak%26entry.1292438233%3D%2520%2520Audio-driven%25203D%2520facial%2520animation%2520synthesis%2520has%2520been%2520an%2520active%2520field%2520of%250Aresearch%2520with%2520attention%2520from%2520both%2520academia%2520and%2520industry.%2520While%2520there%2520are%250Apromising%2520results%2520in%2520this%2520area%252C%2520recent%2520approaches%2520largely%2520focus%2520on%2520lip-sync%2520and%250Aidentity%2520control%252C%2520neglecting%2520the%2520role%2520of%2520emotions%2520and%2520emotion%2520control%2520in%2520the%250Agenerative%2520process.%2520That%2520is%2520mainly%2520due%2520to%2520the%2520lack%2520of%2520emotionally%2520rich%2520facial%250Aanimation%2520data%2520and%2520algorithms%2520that%2520can%2520synthesize%2520speech%2520animations%2520with%250Aemotional%2520expressions%2520at%2520the%2520same%2520time.%2520In%2520addition%252C%2520majority%2520of%2520the%2520models%2520are%250Adeterministic%252C%2520meaning%2520given%2520the%2520same%2520audio%2520input%252C%2520they%2520produce%2520the%2520same%2520output%250Amotion.%2520We%2520argue%2520that%2520emotions%2520and%2520non-determinism%2520are%2520crucial%2520to%2520generate%250Adiverse%2520and%2520emotionally-rich%2520facial%2520animations.%2520In%2520this%2520paper%252C%2520we%2520propose%250AProbTalk3D%2520a%2520non-deterministic%2520neural%2520network%2520approach%2520for%2520emotion%2520controllable%250Aspeech-driven%25203D%2520facial%2520animation%2520synthesis%2520using%2520a%2520two-stage%2520VQ-VAE%2520model%2520and%250Aan%2520emotionally%2520rich%2520facial%2520animation%2520dataset%25203DMEAD.%2520We%2520provide%2520an%2520extensive%250Acomparative%2520analysis%2520of%2520our%2520model%2520against%2520the%2520recent%25203D%2520facial%2520animation%250Asynthesis%2520approaches%252C%2520by%2520evaluating%2520the%2520results%2520objectively%252C%2520qualitatively%252C%2520and%250Awith%2520a%2520perceptual%2520user%2520study.%2520We%2520highlight%2520several%2520objective%2520metrics%2520that%2520are%250Amore%2520suitable%2520for%2520evaluating%2520stochastic%2520outputs%2520and%2520use%2520both%2520in-the-wild%2520and%250Aground%2520truth%2520data%2520for%2520subjective%2520evaluation.%2520To%2520our%2520knowledge%252C%2520that%2520is%2520the%250Afirst%2520non-deterministic%25203D%2520facial%2520animation%2520synthesis%2520method%2520incorporating%2520a%250Arich%2520emotion%2520dataset%2520and%2520emotion%2520control%2520with%2520emotion%2520labels%2520and%2520intensity%250Alevels.%2520Our%2520evaluation%2520demonstrates%2520that%2520the%2520proposed%2520model%2520achieves%2520superior%250Aperformance%2520compared%2520to%2520state-of-the-art%2520emotion-controlled%252C%2520deterministic%2520and%250Anon-deterministic%2520models.%2520We%2520recommend%2520watching%2520the%2520supplementary%2520video%2520for%250Aquality%2520judgement.%2520The%2520entire%2520codebase%2520is%2520publicly%2520available%250A%2528https%253A//github.com/uuembodiedsocialai/ProbTalk3D/%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07966v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProbTalk3D%3A%20Non-Deterministic%20Emotion%20Controllable%20Speech-Driven%203D%0A%20%20Facial%20Animation%20Synthesis%20Using%20VQ-VAE&entry.906535625=Sichun%20Wu%20and%20Kazi%20Injamamul%20Haque%20and%20Zerrin%20Yumak&entry.1292438233=%20%20Audio-driven%203D%20facial%20animation%20synthesis%20has%20been%20an%20active%20field%20of%0Aresearch%20with%20attention%20from%20both%20academia%20and%20industry.%20While%20there%20are%0Apromising%20results%20in%20this%20area%2C%20recent%20approaches%20largely%20focus%20on%20lip-sync%20and%0Aidentity%20control%2C%20neglecting%20the%20role%20of%20emotions%20and%20emotion%20control%20in%20the%0Agenerative%20process.%20That%20is%20mainly%20due%20to%20the%20lack%20of%20emotionally%20rich%20facial%0Aanimation%20data%20and%20algorithms%20that%20can%20synthesize%20speech%20animations%20with%0Aemotional%20expressions%20at%20the%20same%20time.%20In%20addition%2C%20majority%20of%20the%20models%20are%0Adeterministic%2C%20meaning%20given%20the%20same%20audio%20input%2C%20they%20produce%20the%20same%20output%0Amotion.%20We%20argue%20that%20emotions%20and%20non-determinism%20are%20crucial%20to%20generate%0Adiverse%20and%20emotionally-rich%20facial%20animations.%20In%20this%20paper%2C%20we%20propose%0AProbTalk3D%20a%20non-deterministic%20neural%20network%20approach%20for%20emotion%20controllable%0Aspeech-driven%203D%20facial%20animation%20synthesis%20using%20a%20two-stage%20VQ-VAE%20model%20and%0Aan%20emotionally%20rich%20facial%20animation%20dataset%203DMEAD.%20We%20provide%20an%20extensive%0Acomparative%20analysis%20of%20our%20model%20against%20the%20recent%203D%20facial%20animation%0Asynthesis%20approaches%2C%20by%20evaluating%20the%20results%20objectively%2C%20qualitatively%2C%20and%0Awith%20a%20perceptual%20user%20study.%20We%20highlight%20several%20objective%20metrics%20that%20are%0Amore%20suitable%20for%20evaluating%20stochastic%20outputs%20and%20use%20both%20in-the-wild%20and%0Aground%20truth%20data%20for%20subjective%20evaluation.%20To%20our%20knowledge%2C%20that%20is%20the%0Afirst%20non-deterministic%203D%20facial%20animation%20synthesis%20method%20incorporating%20a%0Arich%20emotion%20dataset%20and%20emotion%20control%20with%20emotion%20labels%20and%20intensity%0Alevels.%20Our%20evaluation%20demonstrates%20that%20the%20proposed%20model%20achieves%20superior%0Aperformance%20compared%20to%20state-of-the-art%20emotion-controlled%2C%20deterministic%20and%0Anon-deterministic%20models.%20We%20recommend%20watching%20the%20supplementary%20video%20for%0Aquality%20judgement.%20The%20entire%20codebase%20is%20publicly%20available%0A%28https%3A//github.com/uuembodiedsocialai/ProbTalk3D/%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07966v3&entry.124074799=Read"},
{"title": "SERF: Fine-Grained Interactive 3D Segmentation and Editing with Radiance\n  Fields", "author": "Kaichen Zhou and Lanqing Hong and Enze Xie and Yongxin Yang and Zhenguo Li and Wei Zhang", "abstract": "  Although significant progress has been made in the field of 2D-based\ninteractive editing, fine-grained 3D-based interactive editing remains\nrelatively unexplored. This limitation can be attributed to two main\nchallenges: the lack of an efficient 3D representation robust to different\nmodifications and the absence of an effective 3D interactive segmentation\nmethod. In this paper, we introduce a novel fine-grained interactive 3D\nsegmentation and editing algorithm with radiance fields, which we refer to as\nSERF. Our method entails creating a neural mesh representation by integrating\nmulti-view algorithms with pre-trained 2D models. Building upon this\nrepresentation, we introduce a novel surface rendering technique that preserves\nlocal information and is robust to deformation. Moreover, this representation\nforms the basis for achieving accurate and interactive 3D segmentation without\nrequiring 3D supervision. Harnessing this representation facilitates a range of\ninteractive 3D editing operations, encompassing tasks such as interactive\ngeometry editing and texture painting. Extensive experiments and visualization\nexamples of editing on both real and synthetic data demonstrate the superiority\nof our method on representation quality and editing ability.\n", "link": "http://arxiv.org/abs/2312.15856v2", "date": "2024-10-31", "relevancy": 2.8807, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5896}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5759}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SERF%3A%20Fine-Grained%20Interactive%203D%20Segmentation%20and%20Editing%20with%20Radiance%0A%20%20Fields&body=Title%3A%20SERF%3A%20Fine-Grained%20Interactive%203D%20Segmentation%20and%20Editing%20with%20Radiance%0A%20%20Fields%0AAuthor%3A%20Kaichen%20Zhou%20and%20Lanqing%20Hong%20and%20Enze%20Xie%20and%20Yongxin%20Yang%20and%20Zhenguo%20Li%20and%20Wei%20Zhang%0AAbstract%3A%20%20%20Although%20significant%20progress%20has%20been%20made%20in%20the%20field%20of%202D-based%0Ainteractive%20editing%2C%20fine-grained%203D-based%20interactive%20editing%20remains%0Arelatively%20unexplored.%20This%20limitation%20can%20be%20attributed%20to%20two%20main%0Achallenges%3A%20the%20lack%20of%20an%20efficient%203D%20representation%20robust%20to%20different%0Amodifications%20and%20the%20absence%20of%20an%20effective%203D%20interactive%20segmentation%0Amethod.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20fine-grained%20interactive%203D%0Asegmentation%20and%20editing%20algorithm%20with%20radiance%20fields%2C%20which%20we%20refer%20to%20as%0ASERF.%20Our%20method%20entails%20creating%20a%20neural%20mesh%20representation%20by%20integrating%0Amulti-view%20algorithms%20with%20pre-trained%202D%20models.%20Building%20upon%20this%0Arepresentation%2C%20we%20introduce%20a%20novel%20surface%20rendering%20technique%20that%20preserves%0Alocal%20information%20and%20is%20robust%20to%20deformation.%20Moreover%2C%20this%20representation%0Aforms%20the%20basis%20for%20achieving%20accurate%20and%20interactive%203D%20segmentation%20without%0Arequiring%203D%20supervision.%20Harnessing%20this%20representation%20facilitates%20a%20range%20of%0Ainteractive%203D%20editing%20operations%2C%20encompassing%20tasks%20such%20as%20interactive%0Ageometry%20editing%20and%20texture%20painting.%20Extensive%20experiments%20and%20visualization%0Aexamples%20of%20editing%20on%20both%20real%20and%20synthetic%20data%20demonstrate%20the%20superiority%0Aof%20our%20method%20on%20representation%20quality%20and%20editing%20ability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.15856v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSERF%253A%2520Fine-Grained%2520Interactive%25203D%2520Segmentation%2520and%2520Editing%2520with%2520Radiance%250A%2520%2520Fields%26entry.906535625%3DKaichen%2520Zhou%2520and%2520Lanqing%2520Hong%2520and%2520Enze%2520Xie%2520and%2520Yongxin%2520Yang%2520and%2520Zhenguo%2520Li%2520and%2520Wei%2520Zhang%26entry.1292438233%3D%2520%2520Although%2520significant%2520progress%2520has%2520been%2520made%2520in%2520the%2520field%2520of%25202D-based%250Ainteractive%2520editing%252C%2520fine-grained%25203D-based%2520interactive%2520editing%2520remains%250Arelatively%2520unexplored.%2520This%2520limitation%2520can%2520be%2520attributed%2520to%2520two%2520main%250Achallenges%253A%2520the%2520lack%2520of%2520an%2520efficient%25203D%2520representation%2520robust%2520to%2520different%250Amodifications%2520and%2520the%2520absence%2520of%2520an%2520effective%25203D%2520interactive%2520segmentation%250Amethod.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520fine-grained%2520interactive%25203D%250Asegmentation%2520and%2520editing%2520algorithm%2520with%2520radiance%2520fields%252C%2520which%2520we%2520refer%2520to%2520as%250ASERF.%2520Our%2520method%2520entails%2520creating%2520a%2520neural%2520mesh%2520representation%2520by%2520integrating%250Amulti-view%2520algorithms%2520with%2520pre-trained%25202D%2520models.%2520Building%2520upon%2520this%250Arepresentation%252C%2520we%2520introduce%2520a%2520novel%2520surface%2520rendering%2520technique%2520that%2520preserves%250Alocal%2520information%2520and%2520is%2520robust%2520to%2520deformation.%2520Moreover%252C%2520this%2520representation%250Aforms%2520the%2520basis%2520for%2520achieving%2520accurate%2520and%2520interactive%25203D%2520segmentation%2520without%250Arequiring%25203D%2520supervision.%2520Harnessing%2520this%2520representation%2520facilitates%2520a%2520range%2520of%250Ainteractive%25203D%2520editing%2520operations%252C%2520encompassing%2520tasks%2520such%2520as%2520interactive%250Ageometry%2520editing%2520and%2520texture%2520painting.%2520Extensive%2520experiments%2520and%2520visualization%250Aexamples%2520of%2520editing%2520on%2520both%2520real%2520and%2520synthetic%2520data%2520demonstrate%2520the%2520superiority%250Aof%2520our%2520method%2520on%2520representation%2520quality%2520and%2520editing%2520ability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.15856v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SERF%3A%20Fine-Grained%20Interactive%203D%20Segmentation%20and%20Editing%20with%20Radiance%0A%20%20Fields&entry.906535625=Kaichen%20Zhou%20and%20Lanqing%20Hong%20and%20Enze%20Xie%20and%20Yongxin%20Yang%20and%20Zhenguo%20Li%20and%20Wei%20Zhang&entry.1292438233=%20%20Although%20significant%20progress%20has%20been%20made%20in%20the%20field%20of%202D-based%0Ainteractive%20editing%2C%20fine-grained%203D-based%20interactive%20editing%20remains%0Arelatively%20unexplored.%20This%20limitation%20can%20be%20attributed%20to%20two%20main%0Achallenges%3A%20the%20lack%20of%20an%20efficient%203D%20representation%20robust%20to%20different%0Amodifications%20and%20the%20absence%20of%20an%20effective%203D%20interactive%20segmentation%0Amethod.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20fine-grained%20interactive%203D%0Asegmentation%20and%20editing%20algorithm%20with%20radiance%20fields%2C%20which%20we%20refer%20to%20as%0ASERF.%20Our%20method%20entails%20creating%20a%20neural%20mesh%20representation%20by%20integrating%0Amulti-view%20algorithms%20with%20pre-trained%202D%20models.%20Building%20upon%20this%0Arepresentation%2C%20we%20introduce%20a%20novel%20surface%20rendering%20technique%20that%20preserves%0Alocal%20information%20and%20is%20robust%20to%20deformation.%20Moreover%2C%20this%20representation%0Aforms%20the%20basis%20for%20achieving%20accurate%20and%20interactive%203D%20segmentation%20without%0Arequiring%203D%20supervision.%20Harnessing%20this%20representation%20facilitates%20a%20range%20of%0Ainteractive%203D%20editing%20operations%2C%20encompassing%20tasks%20such%20as%20interactive%0Ageometry%20editing%20and%20texture%20painting.%20Extensive%20experiments%20and%20visualization%0Aexamples%20of%20editing%20on%20both%20real%20and%20synthetic%20data%20demonstrate%20the%20superiority%0Aof%20our%20method%20on%20representation%20quality%20and%20editing%20ability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.15856v2&entry.124074799=Read"},
{"title": "DELTA: Dense Efficient Long-range 3D Tracking for any video", "author": "Tuan Duc Ngo and Peiye Zhuang and Chuang Gan and Evangelos Kalogerakis and Sergey Tulyakov and Hsin-Ying Lee and Chaoyang Wang", "abstract": "  Tracking dense 3D motion from monocular videos remains challenging,\nparticularly when aiming for pixel-level precision over long sequences. We\nintroduce \\Approach, a novel method that efficiently tracks every pixel in 3D\nspace, enabling accurate motion estimation across entire videos. Our approach\nleverages a joint global-local attention mechanism for reduced-resolution\ntracking, followed by a transformer-based upsampler to achieve high-resolution\npredictions. Unlike existing methods, which are limited by computational\ninefficiency or sparse tracking, \\Approach delivers dense 3D tracking at scale,\nrunning over 8x faster than previous methods while achieving state-of-the-art\naccuracy. Furthermore, we explore the impact of depth representation on\ntracking performance and identify log-depth as the optimal choice. Extensive\nexperiments demonstrate the superiority of \\Approach on multiple benchmarks,\nachieving new state-of-the-art results in both 2D and 3D dense tracking tasks.\nOur method provides a robust solution for applications requiring fine-grained,\nlong-term motion tracking in 3D space.\n", "link": "http://arxiv.org/abs/2410.24211v1", "date": "2024-10-31", "relevancy": 2.8675, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.602}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5658}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DELTA%3A%20Dense%20Efficient%20Long-range%203D%20Tracking%20for%20any%20video&body=Title%3A%20DELTA%3A%20Dense%20Efficient%20Long-range%203D%20Tracking%20for%20any%20video%0AAuthor%3A%20Tuan%20Duc%20Ngo%20and%20Peiye%20Zhuang%20and%20Chuang%20Gan%20and%20Evangelos%20Kalogerakis%20and%20Sergey%20Tulyakov%20and%20Hsin-Ying%20Lee%20and%20Chaoyang%20Wang%0AAbstract%3A%20%20%20Tracking%20dense%203D%20motion%20from%20monocular%20videos%20remains%20challenging%2C%0Aparticularly%20when%20aiming%20for%20pixel-level%20precision%20over%20long%20sequences.%20We%0Aintroduce%20%5CApproach%2C%20a%20novel%20method%20that%20efficiently%20tracks%20every%20pixel%20in%203D%0Aspace%2C%20enabling%20accurate%20motion%20estimation%20across%20entire%20videos.%20Our%20approach%0Aleverages%20a%20joint%20global-local%20attention%20mechanism%20for%20reduced-resolution%0Atracking%2C%20followed%20by%20a%20transformer-based%20upsampler%20to%20achieve%20high-resolution%0Apredictions.%20Unlike%20existing%20methods%2C%20which%20are%20limited%20by%20computational%0Ainefficiency%20or%20sparse%20tracking%2C%20%5CApproach%20delivers%20dense%203D%20tracking%20at%20scale%2C%0Arunning%20over%208x%20faster%20than%20previous%20methods%20while%20achieving%20state-of-the-art%0Aaccuracy.%20Furthermore%2C%20we%20explore%20the%20impact%20of%20depth%20representation%20on%0Atracking%20performance%20and%20identify%20log-depth%20as%20the%20optimal%20choice.%20Extensive%0Aexperiments%20demonstrate%20the%20superiority%20of%20%5CApproach%20on%20multiple%20benchmarks%2C%0Aachieving%20new%20state-of-the-art%20results%20in%20both%202D%20and%203D%20dense%20tracking%20tasks.%0AOur%20method%20provides%20a%20robust%20solution%20for%20applications%20requiring%20fine-grained%2C%0Along-term%20motion%20tracking%20in%203D%20space.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDELTA%253A%2520Dense%2520Efficient%2520Long-range%25203D%2520Tracking%2520for%2520any%2520video%26entry.906535625%3DTuan%2520Duc%2520Ngo%2520and%2520Peiye%2520Zhuang%2520and%2520Chuang%2520Gan%2520and%2520Evangelos%2520Kalogerakis%2520and%2520Sergey%2520Tulyakov%2520and%2520Hsin-Ying%2520Lee%2520and%2520Chaoyang%2520Wang%26entry.1292438233%3D%2520%2520Tracking%2520dense%25203D%2520motion%2520from%2520monocular%2520videos%2520remains%2520challenging%252C%250Aparticularly%2520when%2520aiming%2520for%2520pixel-level%2520precision%2520over%2520long%2520sequences.%2520We%250Aintroduce%2520%255CApproach%252C%2520a%2520novel%2520method%2520that%2520efficiently%2520tracks%2520every%2520pixel%2520in%25203D%250Aspace%252C%2520enabling%2520accurate%2520motion%2520estimation%2520across%2520entire%2520videos.%2520Our%2520approach%250Aleverages%2520a%2520joint%2520global-local%2520attention%2520mechanism%2520for%2520reduced-resolution%250Atracking%252C%2520followed%2520by%2520a%2520transformer-based%2520upsampler%2520to%2520achieve%2520high-resolution%250Apredictions.%2520Unlike%2520existing%2520methods%252C%2520which%2520are%2520limited%2520by%2520computational%250Ainefficiency%2520or%2520sparse%2520tracking%252C%2520%255CApproach%2520delivers%2520dense%25203D%2520tracking%2520at%2520scale%252C%250Arunning%2520over%25208x%2520faster%2520than%2520previous%2520methods%2520while%2520achieving%2520state-of-the-art%250Aaccuracy.%2520Furthermore%252C%2520we%2520explore%2520the%2520impact%2520of%2520depth%2520representation%2520on%250Atracking%2520performance%2520and%2520identify%2520log-depth%2520as%2520the%2520optimal%2520choice.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520superiority%2520of%2520%255CApproach%2520on%2520multiple%2520benchmarks%252C%250Aachieving%2520new%2520state-of-the-art%2520results%2520in%2520both%25202D%2520and%25203D%2520dense%2520tracking%2520tasks.%250AOur%2520method%2520provides%2520a%2520robust%2520solution%2520for%2520applications%2520requiring%2520fine-grained%252C%250Along-term%2520motion%2520tracking%2520in%25203D%2520space.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DELTA%3A%20Dense%20Efficient%20Long-range%203D%20Tracking%20for%20any%20video&entry.906535625=Tuan%20Duc%20Ngo%20and%20Peiye%20Zhuang%20and%20Chuang%20Gan%20and%20Evangelos%20Kalogerakis%20and%20Sergey%20Tulyakov%20and%20Hsin-Ying%20Lee%20and%20Chaoyang%20Wang&entry.1292438233=%20%20Tracking%20dense%203D%20motion%20from%20monocular%20videos%20remains%20challenging%2C%0Aparticularly%20when%20aiming%20for%20pixel-level%20precision%20over%20long%20sequences.%20We%0Aintroduce%20%5CApproach%2C%20a%20novel%20method%20that%20efficiently%20tracks%20every%20pixel%20in%203D%0Aspace%2C%20enabling%20accurate%20motion%20estimation%20across%20entire%20videos.%20Our%20approach%0Aleverages%20a%20joint%20global-local%20attention%20mechanism%20for%20reduced-resolution%0Atracking%2C%20followed%20by%20a%20transformer-based%20upsampler%20to%20achieve%20high-resolution%0Apredictions.%20Unlike%20existing%20methods%2C%20which%20are%20limited%20by%20computational%0Ainefficiency%20or%20sparse%20tracking%2C%20%5CApproach%20delivers%20dense%203D%20tracking%20at%20scale%2C%0Arunning%20over%208x%20faster%20than%20previous%20methods%20while%20achieving%20state-of-the-art%0Aaccuracy.%20Furthermore%2C%20we%20explore%20the%20impact%20of%20depth%20representation%20on%0Atracking%20performance%20and%20identify%20log-depth%20as%20the%20optimal%20choice.%20Extensive%0Aexperiments%20demonstrate%20the%20superiority%20of%20%5CApproach%20on%20multiple%20benchmarks%2C%0Aachieving%20new%20state-of-the-art%20results%20in%20both%202D%20and%203D%20dense%20tracking%20tasks.%0AOur%20method%20provides%20a%20robust%20solution%20for%20applications%20requiring%20fine-grained%2C%0Along-term%20motion%20tracking%20in%203D%20space.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24211v1&entry.124074799=Read"},
{"title": "SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map\n  Generation", "author": "Hao Dong and Weihao Gu and Xianjing Zhang and Jintao Xu and Rui Ai and Huimin Lu and Juho Kannala and Xieyuanli Chen", "abstract": "  High-definition (HD) semantic map generation of the environment is an\nessential component of autonomous driving. Existing methods have achieved good\nperformance in this task by fusing different sensor modalities, such as LiDAR\nand camera. However, current works are based on raw data or network\nfeature-level fusion and only consider short-range HD map generation, limiting\ntheir deployment to realistic autonomous driving applications. In this paper,\nwe focus on the task of building the HD maps in both short ranges, i.e., within\n30 m, and also predicting long-range HD maps up to 90 m, which is required by\ndownstream path planning and control tasks to improve the smoothness and safety\nof autonomous driving. To this end, we propose a novel network named\nSuperFusion, exploiting the fusion of LiDAR and camera data at multiple levels.\nWe use LiDAR depth to improve image depth estimation and use image features to\nguide long-range LiDAR feature prediction. We benchmark our SuperFusion on the\nnuScenes dataset and a self-recorded dataset and show that it outperforms the\nstate-of-the-art baseline methods with large margins on all intervals.\nAdditionally, we apply the generated HD map to a downstream path planning task,\ndemonstrating that the long-range HD maps predicted by our method can lead to\nbetter path planning for autonomous vehicles. Our code has been released at\nhttps://github.com/haomo-ai/SuperFusion.\n", "link": "http://arxiv.org/abs/2211.15656v3", "date": "2024-10-31", "relevancy": 2.8028, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6017}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5422}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuperFusion%3A%20Multilevel%20LiDAR-Camera%20Fusion%20for%20Long-Range%20HD%20Map%0A%20%20Generation&body=Title%3A%20SuperFusion%3A%20Multilevel%20LiDAR-Camera%20Fusion%20for%20Long-Range%20HD%20Map%0A%20%20Generation%0AAuthor%3A%20Hao%20Dong%20and%20Weihao%20Gu%20and%20Xianjing%20Zhang%20and%20Jintao%20Xu%20and%20Rui%20Ai%20and%20Huimin%20Lu%20and%20Juho%20Kannala%20and%20Xieyuanli%20Chen%0AAbstract%3A%20%20%20High-definition%20%28HD%29%20semantic%20map%20generation%20of%20the%20environment%20is%20an%0Aessential%20component%20of%20autonomous%20driving.%20Existing%20methods%20have%20achieved%20good%0Aperformance%20in%20this%20task%20by%20fusing%20different%20sensor%20modalities%2C%20such%20as%20LiDAR%0Aand%20camera.%20However%2C%20current%20works%20are%20based%20on%20raw%20data%20or%20network%0Afeature-level%20fusion%20and%20only%20consider%20short-range%20HD%20map%20generation%2C%20limiting%0Atheir%20deployment%20to%20realistic%20autonomous%20driving%20applications.%20In%20this%20paper%2C%0Awe%20focus%20on%20the%20task%20of%20building%20the%20HD%20maps%20in%20both%20short%20ranges%2C%20i.e.%2C%20within%0A30%20m%2C%20and%20also%20predicting%20long-range%20HD%20maps%20up%20to%2090%20m%2C%20which%20is%20required%20by%0Adownstream%20path%20planning%20and%20control%20tasks%20to%20improve%20the%20smoothness%20and%20safety%0Aof%20autonomous%20driving.%20To%20this%20end%2C%20we%20propose%20a%20novel%20network%20named%0ASuperFusion%2C%20exploiting%20the%20fusion%20of%20LiDAR%20and%20camera%20data%20at%20multiple%20levels.%0AWe%20use%20LiDAR%20depth%20to%20improve%20image%20depth%20estimation%20and%20use%20image%20features%20to%0Aguide%20long-range%20LiDAR%20feature%20prediction.%20We%20benchmark%20our%20SuperFusion%20on%20the%0AnuScenes%20dataset%20and%20a%20self-recorded%20dataset%20and%20show%20that%20it%20outperforms%20the%0Astate-of-the-art%20baseline%20methods%20with%20large%20margins%20on%20all%20intervals.%0AAdditionally%2C%20we%20apply%20the%20generated%20HD%20map%20to%20a%20downstream%20path%20planning%20task%2C%0Ademonstrating%20that%20the%20long-range%20HD%20maps%20predicted%20by%20our%20method%20can%20lead%20to%0Abetter%20path%20planning%20for%20autonomous%20vehicles.%20Our%20code%20has%20been%20released%20at%0Ahttps%3A//github.com/haomo-ai/SuperFusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.15656v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperFusion%253A%2520Multilevel%2520LiDAR-Camera%2520Fusion%2520for%2520Long-Range%2520HD%2520Map%250A%2520%2520Generation%26entry.906535625%3DHao%2520Dong%2520and%2520Weihao%2520Gu%2520and%2520Xianjing%2520Zhang%2520and%2520Jintao%2520Xu%2520and%2520Rui%2520Ai%2520and%2520Huimin%2520Lu%2520and%2520Juho%2520Kannala%2520and%2520Xieyuanli%2520Chen%26entry.1292438233%3D%2520%2520High-definition%2520%2528HD%2529%2520semantic%2520map%2520generation%2520of%2520the%2520environment%2520is%2520an%250Aessential%2520component%2520of%2520autonomous%2520driving.%2520Existing%2520methods%2520have%2520achieved%2520good%250Aperformance%2520in%2520this%2520task%2520by%2520fusing%2520different%2520sensor%2520modalities%252C%2520such%2520as%2520LiDAR%250Aand%2520camera.%2520However%252C%2520current%2520works%2520are%2520based%2520on%2520raw%2520data%2520or%2520network%250Afeature-level%2520fusion%2520and%2520only%2520consider%2520short-range%2520HD%2520map%2520generation%252C%2520limiting%250Atheir%2520deployment%2520to%2520realistic%2520autonomous%2520driving%2520applications.%2520In%2520this%2520paper%252C%250Awe%2520focus%2520on%2520the%2520task%2520of%2520building%2520the%2520HD%2520maps%2520in%2520both%2520short%2520ranges%252C%2520i.e.%252C%2520within%250A30%2520m%252C%2520and%2520also%2520predicting%2520long-range%2520HD%2520maps%2520up%2520to%252090%2520m%252C%2520which%2520is%2520required%2520by%250Adownstream%2520path%2520planning%2520and%2520control%2520tasks%2520to%2520improve%2520the%2520smoothness%2520and%2520safety%250Aof%2520autonomous%2520driving.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520network%2520named%250ASuperFusion%252C%2520exploiting%2520the%2520fusion%2520of%2520LiDAR%2520and%2520camera%2520data%2520at%2520multiple%2520levels.%250AWe%2520use%2520LiDAR%2520depth%2520to%2520improve%2520image%2520depth%2520estimation%2520and%2520use%2520image%2520features%2520to%250Aguide%2520long-range%2520LiDAR%2520feature%2520prediction.%2520We%2520benchmark%2520our%2520SuperFusion%2520on%2520the%250AnuScenes%2520dataset%2520and%2520a%2520self-recorded%2520dataset%2520and%2520show%2520that%2520it%2520outperforms%2520the%250Astate-of-the-art%2520baseline%2520methods%2520with%2520large%2520margins%2520on%2520all%2520intervals.%250AAdditionally%252C%2520we%2520apply%2520the%2520generated%2520HD%2520map%2520to%2520a%2520downstream%2520path%2520planning%2520task%252C%250Ademonstrating%2520that%2520the%2520long-range%2520HD%2520maps%2520predicted%2520by%2520our%2520method%2520can%2520lead%2520to%250Abetter%2520path%2520planning%2520for%2520autonomous%2520vehicles.%2520Our%2520code%2520has%2520been%2520released%2520at%250Ahttps%253A//github.com/haomo-ai/SuperFusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.15656v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuperFusion%3A%20Multilevel%20LiDAR-Camera%20Fusion%20for%20Long-Range%20HD%20Map%0A%20%20Generation&entry.906535625=Hao%20Dong%20and%20Weihao%20Gu%20and%20Xianjing%20Zhang%20and%20Jintao%20Xu%20and%20Rui%20Ai%20and%20Huimin%20Lu%20and%20Juho%20Kannala%20and%20Xieyuanli%20Chen&entry.1292438233=%20%20High-definition%20%28HD%29%20semantic%20map%20generation%20of%20the%20environment%20is%20an%0Aessential%20component%20of%20autonomous%20driving.%20Existing%20methods%20have%20achieved%20good%0Aperformance%20in%20this%20task%20by%20fusing%20different%20sensor%20modalities%2C%20such%20as%20LiDAR%0Aand%20camera.%20However%2C%20current%20works%20are%20based%20on%20raw%20data%20or%20network%0Afeature-level%20fusion%20and%20only%20consider%20short-range%20HD%20map%20generation%2C%20limiting%0Atheir%20deployment%20to%20realistic%20autonomous%20driving%20applications.%20In%20this%20paper%2C%0Awe%20focus%20on%20the%20task%20of%20building%20the%20HD%20maps%20in%20both%20short%20ranges%2C%20i.e.%2C%20within%0A30%20m%2C%20and%20also%20predicting%20long-range%20HD%20maps%20up%20to%2090%20m%2C%20which%20is%20required%20by%0Adownstream%20path%20planning%20and%20control%20tasks%20to%20improve%20the%20smoothness%20and%20safety%0Aof%20autonomous%20driving.%20To%20this%20end%2C%20we%20propose%20a%20novel%20network%20named%0ASuperFusion%2C%20exploiting%20the%20fusion%20of%20LiDAR%20and%20camera%20data%20at%20multiple%20levels.%0AWe%20use%20LiDAR%20depth%20to%20improve%20image%20depth%20estimation%20and%20use%20image%20features%20to%0Aguide%20long-range%20LiDAR%20feature%20prediction.%20We%20benchmark%20our%20SuperFusion%20on%20the%0AnuScenes%20dataset%20and%20a%20self-recorded%20dataset%20and%20show%20that%20it%20outperforms%20the%0Astate-of-the-art%20baseline%20methods%20with%20large%20margins%20on%20all%20intervals.%0AAdditionally%2C%20we%20apply%20the%20generated%20HD%20map%20to%20a%20downstream%20path%20planning%20task%2C%0Ademonstrating%20that%20the%20long-range%20HD%20maps%20predicted%20by%20our%20method%20can%20lead%20to%0Abetter%20path%20planning%20for%20autonomous%20vehicles.%20Our%20code%20has%20been%20released%20at%0Ahttps%3A//github.com/haomo-ai/SuperFusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.15656v3&entry.124074799=Read"},
{"title": "CAT: Coordinating Anatomical-Textual Prompts for Multi-Organ and Tumor\n  Segmentation", "author": "Zhongzhen Huang and Yankai Jiang and Rongzhao Zhang and Shaoting Zhang and Xiaofan Zhang", "abstract": "  Existing promptable segmentation methods in the medical imaging field\nprimarily consider either textual or visual prompts to segment relevant\nobjects, yet they often fall short when addressing anomalies in medical images,\nlike tumors, which may vary greatly in shape, size, and appearance. Recognizing\nthe complexity of medical scenarios and the limitations of textual or visual\nprompts, we propose a novel dual-prompt schema that leverages the complementary\nstrengths of visual and textual prompts for segmenting various organs and\ntumors. Specifically, we introduce CAT, an innovative model that Coordinates\nAnatomical prompts derived from 3D cropped images with Textual prompts enriched\nby medical domain knowledge. The model architecture adopts a general\nquery-based design, where prompt queries facilitate segmentation queries for\nmask prediction. To synergize two types of prompts within a unified framework,\nwe implement a ShareRefiner, which refines both segmentation and prompt queries\nwhile disentangling the two types of prompts. Trained on a consortium of 10\npublic CT datasets, CAT demonstrates superior performance in multiple\nsegmentation tasks. Further validation on a specialized in-house dataset\nreveals the remarkable capacity of segmenting tumors across multiple cancer\nstages. This approach confirms that coordinating multimodal prompts is a\npromising avenue for addressing complex scenarios in the medical domain.\n", "link": "http://arxiv.org/abs/2406.07085v2", "date": "2024-10-31", "relevancy": 2.7871, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5583}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5583}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5556}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAT%3A%20Coordinating%20Anatomical-Textual%20Prompts%20for%20Multi-Organ%20and%20Tumor%0A%20%20Segmentation&body=Title%3A%20CAT%3A%20Coordinating%20Anatomical-Textual%20Prompts%20for%20Multi-Organ%20and%20Tumor%0A%20%20Segmentation%0AAuthor%3A%20Zhongzhen%20Huang%20and%20Yankai%20Jiang%20and%20Rongzhao%20Zhang%20and%20Shaoting%20Zhang%20and%20Xiaofan%20Zhang%0AAbstract%3A%20%20%20Existing%20promptable%20segmentation%20methods%20in%20the%20medical%20imaging%20field%0Aprimarily%20consider%20either%20textual%20or%20visual%20prompts%20to%20segment%20relevant%0Aobjects%2C%20yet%20they%20often%20fall%20short%20when%20addressing%20anomalies%20in%20medical%20images%2C%0Alike%20tumors%2C%20which%20may%20vary%20greatly%20in%20shape%2C%20size%2C%20and%20appearance.%20Recognizing%0Athe%20complexity%20of%20medical%20scenarios%20and%20the%20limitations%20of%20textual%20or%20visual%0Aprompts%2C%20we%20propose%20a%20novel%20dual-prompt%20schema%20that%20leverages%20the%20complementary%0Astrengths%20of%20visual%20and%20textual%20prompts%20for%20segmenting%20various%20organs%20and%0Atumors.%20Specifically%2C%20we%20introduce%20CAT%2C%20an%20innovative%20model%20that%20Coordinates%0AAnatomical%20prompts%20derived%20from%203D%20cropped%20images%20with%20Textual%20prompts%20enriched%0Aby%20medical%20domain%20knowledge.%20The%20model%20architecture%20adopts%20a%20general%0Aquery-based%20design%2C%20where%20prompt%20queries%20facilitate%20segmentation%20queries%20for%0Amask%20prediction.%20To%20synergize%20two%20types%20of%20prompts%20within%20a%20unified%20framework%2C%0Awe%20implement%20a%20ShareRefiner%2C%20which%20refines%20both%20segmentation%20and%20prompt%20queries%0Awhile%20disentangling%20the%20two%20types%20of%20prompts.%20Trained%20on%20a%20consortium%20of%2010%0Apublic%20CT%20datasets%2C%20CAT%20demonstrates%20superior%20performance%20in%20multiple%0Asegmentation%20tasks.%20Further%20validation%20on%20a%20specialized%20in-house%20dataset%0Areveals%20the%20remarkable%20capacity%20of%20segmenting%20tumors%20across%20multiple%20cancer%0Astages.%20This%20approach%20confirms%20that%20coordinating%20multimodal%20prompts%20is%20a%0Apromising%20avenue%20for%20addressing%20complex%20scenarios%20in%20the%20medical%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07085v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAT%253A%2520Coordinating%2520Anatomical-Textual%2520Prompts%2520for%2520Multi-Organ%2520and%2520Tumor%250A%2520%2520Segmentation%26entry.906535625%3DZhongzhen%2520Huang%2520and%2520Yankai%2520Jiang%2520and%2520Rongzhao%2520Zhang%2520and%2520Shaoting%2520Zhang%2520and%2520Xiaofan%2520Zhang%26entry.1292438233%3D%2520%2520Existing%2520promptable%2520segmentation%2520methods%2520in%2520the%2520medical%2520imaging%2520field%250Aprimarily%2520consider%2520either%2520textual%2520or%2520visual%2520prompts%2520to%2520segment%2520relevant%250Aobjects%252C%2520yet%2520they%2520often%2520fall%2520short%2520when%2520addressing%2520anomalies%2520in%2520medical%2520images%252C%250Alike%2520tumors%252C%2520which%2520may%2520vary%2520greatly%2520in%2520shape%252C%2520size%252C%2520and%2520appearance.%2520Recognizing%250Athe%2520complexity%2520of%2520medical%2520scenarios%2520and%2520the%2520limitations%2520of%2520textual%2520or%2520visual%250Aprompts%252C%2520we%2520propose%2520a%2520novel%2520dual-prompt%2520schema%2520that%2520leverages%2520the%2520complementary%250Astrengths%2520of%2520visual%2520and%2520textual%2520prompts%2520for%2520segmenting%2520various%2520organs%2520and%250Atumors.%2520Specifically%252C%2520we%2520introduce%2520CAT%252C%2520an%2520innovative%2520model%2520that%2520Coordinates%250AAnatomical%2520prompts%2520derived%2520from%25203D%2520cropped%2520images%2520with%2520Textual%2520prompts%2520enriched%250Aby%2520medical%2520domain%2520knowledge.%2520The%2520model%2520architecture%2520adopts%2520a%2520general%250Aquery-based%2520design%252C%2520where%2520prompt%2520queries%2520facilitate%2520segmentation%2520queries%2520for%250Amask%2520prediction.%2520To%2520synergize%2520two%2520types%2520of%2520prompts%2520within%2520a%2520unified%2520framework%252C%250Awe%2520implement%2520a%2520ShareRefiner%252C%2520which%2520refines%2520both%2520segmentation%2520and%2520prompt%2520queries%250Awhile%2520disentangling%2520the%2520two%2520types%2520of%2520prompts.%2520Trained%2520on%2520a%2520consortium%2520of%252010%250Apublic%2520CT%2520datasets%252C%2520CAT%2520demonstrates%2520superior%2520performance%2520in%2520multiple%250Asegmentation%2520tasks.%2520Further%2520validation%2520on%2520a%2520specialized%2520in-house%2520dataset%250Areveals%2520the%2520remarkable%2520capacity%2520of%2520segmenting%2520tumors%2520across%2520multiple%2520cancer%250Astages.%2520This%2520approach%2520confirms%2520that%2520coordinating%2520multimodal%2520prompts%2520is%2520a%250Apromising%2520avenue%2520for%2520addressing%2520complex%2520scenarios%2520in%2520the%2520medical%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07085v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAT%3A%20Coordinating%20Anatomical-Textual%20Prompts%20for%20Multi-Organ%20and%20Tumor%0A%20%20Segmentation&entry.906535625=Zhongzhen%20Huang%20and%20Yankai%20Jiang%20and%20Rongzhao%20Zhang%20and%20Shaoting%20Zhang%20and%20Xiaofan%20Zhang&entry.1292438233=%20%20Existing%20promptable%20segmentation%20methods%20in%20the%20medical%20imaging%20field%0Aprimarily%20consider%20either%20textual%20or%20visual%20prompts%20to%20segment%20relevant%0Aobjects%2C%20yet%20they%20often%20fall%20short%20when%20addressing%20anomalies%20in%20medical%20images%2C%0Alike%20tumors%2C%20which%20may%20vary%20greatly%20in%20shape%2C%20size%2C%20and%20appearance.%20Recognizing%0Athe%20complexity%20of%20medical%20scenarios%20and%20the%20limitations%20of%20textual%20or%20visual%0Aprompts%2C%20we%20propose%20a%20novel%20dual-prompt%20schema%20that%20leverages%20the%20complementary%0Astrengths%20of%20visual%20and%20textual%20prompts%20for%20segmenting%20various%20organs%20and%0Atumors.%20Specifically%2C%20we%20introduce%20CAT%2C%20an%20innovative%20model%20that%20Coordinates%0AAnatomical%20prompts%20derived%20from%203D%20cropped%20images%20with%20Textual%20prompts%20enriched%0Aby%20medical%20domain%20knowledge.%20The%20model%20architecture%20adopts%20a%20general%0Aquery-based%20design%2C%20where%20prompt%20queries%20facilitate%20segmentation%20queries%20for%0Amask%20prediction.%20To%20synergize%20two%20types%20of%20prompts%20within%20a%20unified%20framework%2C%0Awe%20implement%20a%20ShareRefiner%2C%20which%20refines%20both%20segmentation%20and%20prompt%20queries%0Awhile%20disentangling%20the%20two%20types%20of%20prompts.%20Trained%20on%20a%20consortium%20of%2010%0Apublic%20CT%20datasets%2C%20CAT%20demonstrates%20superior%20performance%20in%20multiple%0Asegmentation%20tasks.%20Further%20validation%20on%20a%20specialized%20in-house%20dataset%0Areveals%20the%20remarkable%20capacity%20of%20segmenting%20tumors%20across%20multiple%20cancer%0Astages.%20This%20approach%20confirms%20that%20coordinating%20multimodal%20prompts%20is%20a%0Apromising%20avenue%20for%20addressing%20complex%20scenarios%20in%20the%20medical%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07085v2&entry.124074799=Read"},
{"title": "CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable\n  Remote Sensing Semantic Segmentation", "author": "Ziyang Gong and Zhixiang Wei and Di Wang and Xianzheng Ma and Hongruixuan Chen and Yuru Jia and Yupeng Deng and Zhenming Ji and Xiangwei Zhu and Naoto Yokoya and Jing Zhang and Bo Du and Liangpei Zhang", "abstract": "  The field of Remote Sensing Domain Generalization (RSDG) has emerged as a\ncritical and valuable research frontier, focusing on developing models that\ngeneralize effectively across diverse scenarios. Despite the substantial domain\ngaps in RS images that are characterized by variabilities such as location,\nwavelength, and sensor type, research in this area remains underexplored: (1)\nCurrent cross-domain methods primarily focus on Domain Adaptation (DA), which\nadapts models to predefined domains rather than to unseen ones; (2) Few studies\ntargeting the RSDG issue, especially for semantic segmentation tasks, where\nexisting models are developed for specific unknown domains, struggling with\nissues of underfitting on other unknown scenarios; (3) Existing RS foundation\nmodels tend to prioritize in-domain performance over cross-domain\ngeneralization. To this end, we introduce the first vision foundation model for\nRSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong\ncross-domain generalization through a specially designed data-level Earth-Style\nInjection pipeline and a model-level Multi-Task Training pipeline. In addition,\nfor the semantic segmentation task, we have curated an RSDG benchmark\ncomprising 28 cross-domain settings across various regions, spectral bands,\nplatforms, and climates, providing a comprehensive framework for testing the\ngeneralizability of future RSDG models. Extensive experiments on this benchmark\ndemonstrate the superiority of CrossEarth over existing state-of-the-art\nmethods.\n", "link": "http://arxiv.org/abs/2410.22629v2", "date": "2024-10-31", "relevancy": 2.7727, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5662}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5662}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CrossEarth%3A%20Geospatial%20Vision%20Foundation%20Model%20for%20Domain%20Generalizable%0A%20%20Remote%20Sensing%20Semantic%20Segmentation&body=Title%3A%20CrossEarth%3A%20Geospatial%20Vision%20Foundation%20Model%20for%20Domain%20Generalizable%0A%20%20Remote%20Sensing%20Semantic%20Segmentation%0AAuthor%3A%20Ziyang%20Gong%20and%20Zhixiang%20Wei%20and%20Di%20Wang%20and%20Xianzheng%20Ma%20and%20Hongruixuan%20Chen%20and%20Yuru%20Jia%20and%20Yupeng%20Deng%20and%20Zhenming%20Ji%20and%20Xiangwei%20Zhu%20and%20Naoto%20Yokoya%20and%20Jing%20Zhang%20and%20Bo%20Du%20and%20Liangpei%20Zhang%0AAbstract%3A%20%20%20The%20field%20of%20Remote%20Sensing%20Domain%20Generalization%20%28RSDG%29%20has%20emerged%20as%20a%0Acritical%20and%20valuable%20research%20frontier%2C%20focusing%20on%20developing%20models%20that%0Ageneralize%20effectively%20across%20diverse%20scenarios.%20Despite%20the%20substantial%20domain%0Agaps%20in%20RS%20images%20that%20are%20characterized%20by%20variabilities%20such%20as%20location%2C%0Awavelength%2C%20and%20sensor%20type%2C%20research%20in%20this%20area%20remains%20underexplored%3A%20%281%29%0ACurrent%20cross-domain%20methods%20primarily%20focus%20on%20Domain%20Adaptation%20%28DA%29%2C%20which%0Aadapts%20models%20to%20predefined%20domains%20rather%20than%20to%20unseen%20ones%3B%20%282%29%20Few%20studies%0Atargeting%20the%20RSDG%20issue%2C%20especially%20for%20semantic%20segmentation%20tasks%2C%20where%0Aexisting%20models%20are%20developed%20for%20specific%20unknown%20domains%2C%20struggling%20with%0Aissues%20of%20underfitting%20on%20other%20unknown%20scenarios%3B%20%283%29%20Existing%20RS%20foundation%0Amodels%20tend%20to%20prioritize%20in-domain%20performance%20over%20cross-domain%0Ageneralization.%20To%20this%20end%2C%20we%20introduce%20the%20first%20vision%20foundation%20model%20for%0ARSDG%20semantic%20segmentation%2C%20CrossEarth.%20CrossEarth%20demonstrates%20strong%0Across-domain%20generalization%20through%20a%20specially%20designed%20data-level%20Earth-Style%0AInjection%20pipeline%20and%20a%20model-level%20Multi-Task%20Training%20pipeline.%20In%20addition%2C%0Afor%20the%20semantic%20segmentation%20task%2C%20we%20have%20curated%20an%20RSDG%20benchmark%0Acomprising%2028%20cross-domain%20settings%20across%20various%20regions%2C%20spectral%20bands%2C%0Aplatforms%2C%20and%20climates%2C%20providing%20a%20comprehensive%20framework%20for%20testing%20the%0Ageneralizability%20of%20future%20RSDG%20models.%20Extensive%20experiments%20on%20this%20benchmark%0Ademonstrate%20the%20superiority%20of%20CrossEarth%20over%20existing%20state-of-the-art%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22629v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCrossEarth%253A%2520Geospatial%2520Vision%2520Foundation%2520Model%2520for%2520Domain%2520Generalizable%250A%2520%2520Remote%2520Sensing%2520Semantic%2520Segmentation%26entry.906535625%3DZiyang%2520Gong%2520and%2520Zhixiang%2520Wei%2520and%2520Di%2520Wang%2520and%2520Xianzheng%2520Ma%2520and%2520Hongruixuan%2520Chen%2520and%2520Yuru%2520Jia%2520and%2520Yupeng%2520Deng%2520and%2520Zhenming%2520Ji%2520and%2520Xiangwei%2520Zhu%2520and%2520Naoto%2520Yokoya%2520and%2520Jing%2520Zhang%2520and%2520Bo%2520Du%2520and%2520Liangpei%2520Zhang%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520Remote%2520Sensing%2520Domain%2520Generalization%2520%2528RSDG%2529%2520has%2520emerged%2520as%2520a%250Acritical%2520and%2520valuable%2520research%2520frontier%252C%2520focusing%2520on%2520developing%2520models%2520that%250Ageneralize%2520effectively%2520across%2520diverse%2520scenarios.%2520Despite%2520the%2520substantial%2520domain%250Agaps%2520in%2520RS%2520images%2520that%2520are%2520characterized%2520by%2520variabilities%2520such%2520as%2520location%252C%250Awavelength%252C%2520and%2520sensor%2520type%252C%2520research%2520in%2520this%2520area%2520remains%2520underexplored%253A%2520%25281%2529%250ACurrent%2520cross-domain%2520methods%2520primarily%2520focus%2520on%2520Domain%2520Adaptation%2520%2528DA%2529%252C%2520which%250Aadapts%2520models%2520to%2520predefined%2520domains%2520rather%2520than%2520to%2520unseen%2520ones%253B%2520%25282%2529%2520Few%2520studies%250Atargeting%2520the%2520RSDG%2520issue%252C%2520especially%2520for%2520semantic%2520segmentation%2520tasks%252C%2520where%250Aexisting%2520models%2520are%2520developed%2520for%2520specific%2520unknown%2520domains%252C%2520struggling%2520with%250Aissues%2520of%2520underfitting%2520on%2520other%2520unknown%2520scenarios%253B%2520%25283%2529%2520Existing%2520RS%2520foundation%250Amodels%2520tend%2520to%2520prioritize%2520in-domain%2520performance%2520over%2520cross-domain%250Ageneralization.%2520To%2520this%2520end%252C%2520we%2520introduce%2520the%2520first%2520vision%2520foundation%2520model%2520for%250ARSDG%2520semantic%2520segmentation%252C%2520CrossEarth.%2520CrossEarth%2520demonstrates%2520strong%250Across-domain%2520generalization%2520through%2520a%2520specially%2520designed%2520data-level%2520Earth-Style%250AInjection%2520pipeline%2520and%2520a%2520model-level%2520Multi-Task%2520Training%2520pipeline.%2520In%2520addition%252C%250Afor%2520the%2520semantic%2520segmentation%2520task%252C%2520we%2520have%2520curated%2520an%2520RSDG%2520benchmark%250Acomprising%252028%2520cross-domain%2520settings%2520across%2520various%2520regions%252C%2520spectral%2520bands%252C%250Aplatforms%252C%2520and%2520climates%252C%2520providing%2520a%2520comprehensive%2520framework%2520for%2520testing%2520the%250Ageneralizability%2520of%2520future%2520RSDG%2520models.%2520Extensive%2520experiments%2520on%2520this%2520benchmark%250Ademonstrate%2520the%2520superiority%2520of%2520CrossEarth%2520over%2520existing%2520state-of-the-art%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22629v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CrossEarth%3A%20Geospatial%20Vision%20Foundation%20Model%20for%20Domain%20Generalizable%0A%20%20Remote%20Sensing%20Semantic%20Segmentation&entry.906535625=Ziyang%20Gong%20and%20Zhixiang%20Wei%20and%20Di%20Wang%20and%20Xianzheng%20Ma%20and%20Hongruixuan%20Chen%20and%20Yuru%20Jia%20and%20Yupeng%20Deng%20and%20Zhenming%20Ji%20and%20Xiangwei%20Zhu%20and%20Naoto%20Yokoya%20and%20Jing%20Zhang%20and%20Bo%20Du%20and%20Liangpei%20Zhang&entry.1292438233=%20%20The%20field%20of%20Remote%20Sensing%20Domain%20Generalization%20%28RSDG%29%20has%20emerged%20as%20a%0Acritical%20and%20valuable%20research%20frontier%2C%20focusing%20on%20developing%20models%20that%0Ageneralize%20effectively%20across%20diverse%20scenarios.%20Despite%20the%20substantial%20domain%0Agaps%20in%20RS%20images%20that%20are%20characterized%20by%20variabilities%20such%20as%20location%2C%0Awavelength%2C%20and%20sensor%20type%2C%20research%20in%20this%20area%20remains%20underexplored%3A%20%281%29%0ACurrent%20cross-domain%20methods%20primarily%20focus%20on%20Domain%20Adaptation%20%28DA%29%2C%20which%0Aadapts%20models%20to%20predefined%20domains%20rather%20than%20to%20unseen%20ones%3B%20%282%29%20Few%20studies%0Atargeting%20the%20RSDG%20issue%2C%20especially%20for%20semantic%20segmentation%20tasks%2C%20where%0Aexisting%20models%20are%20developed%20for%20specific%20unknown%20domains%2C%20struggling%20with%0Aissues%20of%20underfitting%20on%20other%20unknown%20scenarios%3B%20%283%29%20Existing%20RS%20foundation%0Amodels%20tend%20to%20prioritize%20in-domain%20performance%20over%20cross-domain%0Ageneralization.%20To%20this%20end%2C%20we%20introduce%20the%20first%20vision%20foundation%20model%20for%0ARSDG%20semantic%20segmentation%2C%20CrossEarth.%20CrossEarth%20demonstrates%20strong%0Across-domain%20generalization%20through%20a%20specially%20designed%20data-level%20Earth-Style%0AInjection%20pipeline%20and%20a%20model-level%20Multi-Task%20Training%20pipeline.%20In%20addition%2C%0Afor%20the%20semantic%20segmentation%20task%2C%20we%20have%20curated%20an%20RSDG%20benchmark%0Acomprising%2028%20cross-domain%20settings%20across%20various%20regions%2C%20spectral%20bands%2C%0Aplatforms%2C%20and%20climates%2C%20providing%20a%20comprehensive%20framework%20for%20testing%20the%0Ageneralizability%20of%20future%20RSDG%20models.%20Extensive%20experiments%20on%20this%20benchmark%0Ademonstrate%20the%20superiority%20of%20CrossEarth%20over%20existing%20state-of-the-art%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22629v2&entry.124074799=Read"},
{"title": "GPTR: Gaussian Process Trajectory Representation for Continuous-Time\n  Motion Estimation", "author": "Thien-Minh Nguyen and Ziyu Cao and Kailai Li and Shenghai Yuan and Lihua Xie", "abstract": "  Continuous-time trajectory representation has gained significant popularity\nin recent years, as it offers an elegant formulation that allows the fusion of\na larger number of sensors and sensing modalities, overcoming limitations of\ntraditional discrete-time frameworks. To bolster the adoption of the\ncontinuous-time paradigm, we propose a so-called Gaussian Process Trajectory\nRepresentation (GPTR) framework for continuous-time motion estimation (CTME)\ntasks. Our approach stands out by employing a third-order random jerk model,\nfeaturing closed-form expressions for both rotational and translational state\nderivatives. This model provides smooth, continuous trajectory representations\nthat are crucial for precise estimation of complex motion. To support the wider\nrobotics and computer vision communities, we have made the source code for GPTR\navailable as a light-weight header-only library. This format was chosen for its\nease of integration, allowing developers to incorporate GPTR into existing\nsystems without needing extensive code modifications. Moreover, we also provide\na set of optimization examples with LiDAR, camera, IMU, UWB factors, and\nclosed-form analytical Jacobians under the proposed GP framework. Our\nexperiments demonstrate the efficacy and efficiency of GP-based trajectory\nrepresentation in various motion estimation tasks, and the examples can serve\nas the prototype to help researchers quickly develop future applications such\nas batch optimization, calibration, sensor fusion, trajectory planning, etc.,\nwith continuous-time trajectory representation. Our project is accessible at\nhttps://github.com/brytsknguyen/gptr .\n", "link": "http://arxiv.org/abs/2410.22931v2", "date": "2024-10-31", "relevancy": 2.7539, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5658}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5479}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPTR%3A%20Gaussian%20Process%20Trajectory%20Representation%20for%20Continuous-Time%0A%20%20Motion%20Estimation&body=Title%3A%20GPTR%3A%20Gaussian%20Process%20Trajectory%20Representation%20for%20Continuous-Time%0A%20%20Motion%20Estimation%0AAuthor%3A%20Thien-Minh%20Nguyen%20and%20Ziyu%20Cao%20and%20Kailai%20Li%20and%20Shenghai%20Yuan%20and%20Lihua%20Xie%0AAbstract%3A%20%20%20Continuous-time%20trajectory%20representation%20has%20gained%20significant%20popularity%0Ain%20recent%20years%2C%20as%20it%20offers%20an%20elegant%20formulation%20that%20allows%20the%20fusion%20of%0Aa%20larger%20number%20of%20sensors%20and%20sensing%20modalities%2C%20overcoming%20limitations%20of%0Atraditional%20discrete-time%20frameworks.%20To%20bolster%20the%20adoption%20of%20the%0Acontinuous-time%20paradigm%2C%20we%20propose%20a%20so-called%20Gaussian%20Process%20Trajectory%0ARepresentation%20%28GPTR%29%20framework%20for%20continuous-time%20motion%20estimation%20%28CTME%29%0Atasks.%20Our%20approach%20stands%20out%20by%20employing%20a%20third-order%20random%20jerk%20model%2C%0Afeaturing%20closed-form%20expressions%20for%20both%20rotational%20and%20translational%20state%0Aderivatives.%20This%20model%20provides%20smooth%2C%20continuous%20trajectory%20representations%0Athat%20are%20crucial%20for%20precise%20estimation%20of%20complex%20motion.%20To%20support%20the%20wider%0Arobotics%20and%20computer%20vision%20communities%2C%20we%20have%20made%20the%20source%20code%20for%20GPTR%0Aavailable%20as%20a%20light-weight%20header-only%20library.%20This%20format%20was%20chosen%20for%20its%0Aease%20of%20integration%2C%20allowing%20developers%20to%20incorporate%20GPTR%20into%20existing%0Asystems%20without%20needing%20extensive%20code%20modifications.%20Moreover%2C%20we%20also%20provide%0Aa%20set%20of%20optimization%20examples%20with%20LiDAR%2C%20camera%2C%20IMU%2C%20UWB%20factors%2C%20and%0Aclosed-form%20analytical%20Jacobians%20under%20the%20proposed%20GP%20framework.%20Our%0Aexperiments%20demonstrate%20the%20efficacy%20and%20efficiency%20of%20GP-based%20trajectory%0Arepresentation%20in%20various%20motion%20estimation%20tasks%2C%20and%20the%20examples%20can%20serve%0Aas%20the%20prototype%20to%20help%20researchers%20quickly%20develop%20future%20applications%20such%0Aas%20batch%20optimization%2C%20calibration%2C%20sensor%20fusion%2C%20trajectory%20planning%2C%20etc.%2C%0Awith%20continuous-time%20trajectory%20representation.%20Our%20project%20is%20accessible%20at%0Ahttps%3A//github.com/brytsknguyen/gptr%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22931v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPTR%253A%2520Gaussian%2520Process%2520Trajectory%2520Representation%2520for%2520Continuous-Time%250A%2520%2520Motion%2520Estimation%26entry.906535625%3DThien-Minh%2520Nguyen%2520and%2520Ziyu%2520Cao%2520and%2520Kailai%2520Li%2520and%2520Shenghai%2520Yuan%2520and%2520Lihua%2520Xie%26entry.1292438233%3D%2520%2520Continuous-time%2520trajectory%2520representation%2520has%2520gained%2520significant%2520popularity%250Ain%2520recent%2520years%252C%2520as%2520it%2520offers%2520an%2520elegant%2520formulation%2520that%2520allows%2520the%2520fusion%2520of%250Aa%2520larger%2520number%2520of%2520sensors%2520and%2520sensing%2520modalities%252C%2520overcoming%2520limitations%2520of%250Atraditional%2520discrete-time%2520frameworks.%2520To%2520bolster%2520the%2520adoption%2520of%2520the%250Acontinuous-time%2520paradigm%252C%2520we%2520propose%2520a%2520so-called%2520Gaussian%2520Process%2520Trajectory%250ARepresentation%2520%2528GPTR%2529%2520framework%2520for%2520continuous-time%2520motion%2520estimation%2520%2528CTME%2529%250Atasks.%2520Our%2520approach%2520stands%2520out%2520by%2520employing%2520a%2520third-order%2520random%2520jerk%2520model%252C%250Afeaturing%2520closed-form%2520expressions%2520for%2520both%2520rotational%2520and%2520translational%2520state%250Aderivatives.%2520This%2520model%2520provides%2520smooth%252C%2520continuous%2520trajectory%2520representations%250Athat%2520are%2520crucial%2520for%2520precise%2520estimation%2520of%2520complex%2520motion.%2520To%2520support%2520the%2520wider%250Arobotics%2520and%2520computer%2520vision%2520communities%252C%2520we%2520have%2520made%2520the%2520source%2520code%2520for%2520GPTR%250Aavailable%2520as%2520a%2520light-weight%2520header-only%2520library.%2520This%2520format%2520was%2520chosen%2520for%2520its%250Aease%2520of%2520integration%252C%2520allowing%2520developers%2520to%2520incorporate%2520GPTR%2520into%2520existing%250Asystems%2520without%2520needing%2520extensive%2520code%2520modifications.%2520Moreover%252C%2520we%2520also%2520provide%250Aa%2520set%2520of%2520optimization%2520examples%2520with%2520LiDAR%252C%2520camera%252C%2520IMU%252C%2520UWB%2520factors%252C%2520and%250Aclosed-form%2520analytical%2520Jacobians%2520under%2520the%2520proposed%2520GP%2520framework.%2520Our%250Aexperiments%2520demonstrate%2520the%2520efficacy%2520and%2520efficiency%2520of%2520GP-based%2520trajectory%250Arepresentation%2520in%2520various%2520motion%2520estimation%2520tasks%252C%2520and%2520the%2520examples%2520can%2520serve%250Aas%2520the%2520prototype%2520to%2520help%2520researchers%2520quickly%2520develop%2520future%2520applications%2520such%250Aas%2520batch%2520optimization%252C%2520calibration%252C%2520sensor%2520fusion%252C%2520trajectory%2520planning%252C%2520etc.%252C%250Awith%2520continuous-time%2520trajectory%2520representation.%2520Our%2520project%2520is%2520accessible%2520at%250Ahttps%253A//github.com/brytsknguyen/gptr%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22931v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPTR%3A%20Gaussian%20Process%20Trajectory%20Representation%20for%20Continuous-Time%0A%20%20Motion%20Estimation&entry.906535625=Thien-Minh%20Nguyen%20and%20Ziyu%20Cao%20and%20Kailai%20Li%20and%20Shenghai%20Yuan%20and%20Lihua%20Xie&entry.1292438233=%20%20Continuous-time%20trajectory%20representation%20has%20gained%20significant%20popularity%0Ain%20recent%20years%2C%20as%20it%20offers%20an%20elegant%20formulation%20that%20allows%20the%20fusion%20of%0Aa%20larger%20number%20of%20sensors%20and%20sensing%20modalities%2C%20overcoming%20limitations%20of%0Atraditional%20discrete-time%20frameworks.%20To%20bolster%20the%20adoption%20of%20the%0Acontinuous-time%20paradigm%2C%20we%20propose%20a%20so-called%20Gaussian%20Process%20Trajectory%0ARepresentation%20%28GPTR%29%20framework%20for%20continuous-time%20motion%20estimation%20%28CTME%29%0Atasks.%20Our%20approach%20stands%20out%20by%20employing%20a%20third-order%20random%20jerk%20model%2C%0Afeaturing%20closed-form%20expressions%20for%20both%20rotational%20and%20translational%20state%0Aderivatives.%20This%20model%20provides%20smooth%2C%20continuous%20trajectory%20representations%0Athat%20are%20crucial%20for%20precise%20estimation%20of%20complex%20motion.%20To%20support%20the%20wider%0Arobotics%20and%20computer%20vision%20communities%2C%20we%20have%20made%20the%20source%20code%20for%20GPTR%0Aavailable%20as%20a%20light-weight%20header-only%20library.%20This%20format%20was%20chosen%20for%20its%0Aease%20of%20integration%2C%20allowing%20developers%20to%20incorporate%20GPTR%20into%20existing%0Asystems%20without%20needing%20extensive%20code%20modifications.%20Moreover%2C%20we%20also%20provide%0Aa%20set%20of%20optimization%20examples%20with%20LiDAR%2C%20camera%2C%20IMU%2C%20UWB%20factors%2C%20and%0Aclosed-form%20analytical%20Jacobians%20under%20the%20proposed%20GP%20framework.%20Our%0Aexperiments%20demonstrate%20the%20efficacy%20and%20efficiency%20of%20GP-based%20trajectory%0Arepresentation%20in%20various%20motion%20estimation%20tasks%2C%20and%20the%20examples%20can%20serve%0Aas%20the%20prototype%20to%20help%20researchers%20quickly%20develop%20future%20applications%20such%0Aas%20batch%20optimization%2C%20calibration%2C%20sensor%20fusion%2C%20trajectory%20planning%2C%20etc.%2C%0Awith%20continuous-time%20trajectory%20representation.%20Our%20project%20is%20accessible%20at%0Ahttps%3A//github.com/brytsknguyen/gptr%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22931v2&entry.124074799=Read"},
{"title": "Dessie: Disentanglement for Articulated 3D Horse Shape and Pose\n  Estimation from Images", "author": "Ci Li and Yi Yang and Zehang Weng and Elin Hernlund and Silvia Zuffi and Hedvig Kjellstr\u00f6m", "abstract": "  In recent years, 3D parametric animal models have been developed to aid in\nestimating 3D shape and pose from images and video. While progress has been\nmade for humans, it's more challenging for animals due to limited annotated\ndata. To address this, we introduce the first method using synthetic data\ngeneration and disentanglement to learn to regress 3D shape and pose. Focusing\non horses, we use text-based texture generation and a synthetic data pipeline\nto create varied shapes, poses, and appearances, learning disentangled spaces.\nOur method, Dessie, surpasses existing 3D horse reconstruction methods and\ngeneralizes to other large animals like zebras, cows, and deer. See the project\nwebsite at: \\url{https://celiali.github.io/Dessie/}.\n", "link": "http://arxiv.org/abs/2410.03438v2", "date": "2024-10-31", "relevancy": 2.7497, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5505}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5497}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dessie%3A%20Disentanglement%20for%20Articulated%203D%20Horse%20Shape%20and%20Pose%0A%20%20Estimation%20from%20Images&body=Title%3A%20Dessie%3A%20Disentanglement%20for%20Articulated%203D%20Horse%20Shape%20and%20Pose%0A%20%20Estimation%20from%20Images%0AAuthor%3A%20Ci%20Li%20and%20Yi%20Yang%20and%20Zehang%20Weng%20and%20Elin%20Hernlund%20and%20Silvia%20Zuffi%20and%20Hedvig%20Kjellstr%C3%B6m%0AAbstract%3A%20%20%20In%20recent%20years%2C%203D%20parametric%20animal%20models%20have%20been%20developed%20to%20aid%20in%0Aestimating%203D%20shape%20and%20pose%20from%20images%20and%20video.%20While%20progress%20has%20been%0Amade%20for%20humans%2C%20it%27s%20more%20challenging%20for%20animals%20due%20to%20limited%20annotated%0Adata.%20To%20address%20this%2C%20we%20introduce%20the%20first%20method%20using%20synthetic%20data%0Ageneration%20and%20disentanglement%20to%20learn%20to%20regress%203D%20shape%20and%20pose.%20Focusing%0Aon%20horses%2C%20we%20use%20text-based%20texture%20generation%20and%20a%20synthetic%20data%20pipeline%0Ato%20create%20varied%20shapes%2C%20poses%2C%20and%20appearances%2C%20learning%20disentangled%20spaces.%0AOur%20method%2C%20Dessie%2C%20surpasses%20existing%203D%20horse%20reconstruction%20methods%20and%0Ageneralizes%20to%20other%20large%20animals%20like%20zebras%2C%20cows%2C%20and%20deer.%20See%20the%20project%0Awebsite%20at%3A%20%5Curl%7Bhttps%3A//celiali.github.io/Dessie/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03438v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDessie%253A%2520Disentanglement%2520for%2520Articulated%25203D%2520Horse%2520Shape%2520and%2520Pose%250A%2520%2520Estimation%2520from%2520Images%26entry.906535625%3DCi%2520Li%2520and%2520Yi%2520Yang%2520and%2520Zehang%2520Weng%2520and%2520Elin%2520Hernlund%2520and%2520Silvia%2520Zuffi%2520and%2520Hedvig%2520Kjellstr%25C3%25B6m%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%25203D%2520parametric%2520animal%2520models%2520have%2520been%2520developed%2520to%2520aid%2520in%250Aestimating%25203D%2520shape%2520and%2520pose%2520from%2520images%2520and%2520video.%2520While%2520progress%2520has%2520been%250Amade%2520for%2520humans%252C%2520it%2527s%2520more%2520challenging%2520for%2520animals%2520due%2520to%2520limited%2520annotated%250Adata.%2520To%2520address%2520this%252C%2520we%2520introduce%2520the%2520first%2520method%2520using%2520synthetic%2520data%250Ageneration%2520and%2520disentanglement%2520to%2520learn%2520to%2520regress%25203D%2520shape%2520and%2520pose.%2520Focusing%250Aon%2520horses%252C%2520we%2520use%2520text-based%2520texture%2520generation%2520and%2520a%2520synthetic%2520data%2520pipeline%250Ato%2520create%2520varied%2520shapes%252C%2520poses%252C%2520and%2520appearances%252C%2520learning%2520disentangled%2520spaces.%250AOur%2520method%252C%2520Dessie%252C%2520surpasses%2520existing%25203D%2520horse%2520reconstruction%2520methods%2520and%250Ageneralizes%2520to%2520other%2520large%2520animals%2520like%2520zebras%252C%2520cows%252C%2520and%2520deer.%2520See%2520the%2520project%250Awebsite%2520at%253A%2520%255Curl%257Bhttps%253A//celiali.github.io/Dessie/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03438v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dessie%3A%20Disentanglement%20for%20Articulated%203D%20Horse%20Shape%20and%20Pose%0A%20%20Estimation%20from%20Images&entry.906535625=Ci%20Li%20and%20Yi%20Yang%20and%20Zehang%20Weng%20and%20Elin%20Hernlund%20and%20Silvia%20Zuffi%20and%20Hedvig%20Kjellstr%C3%B6m&entry.1292438233=%20%20In%20recent%20years%2C%203D%20parametric%20animal%20models%20have%20been%20developed%20to%20aid%20in%0Aestimating%203D%20shape%20and%20pose%20from%20images%20and%20video.%20While%20progress%20has%20been%0Amade%20for%20humans%2C%20it%27s%20more%20challenging%20for%20animals%20due%20to%20limited%20annotated%0Adata.%20To%20address%20this%2C%20we%20introduce%20the%20first%20method%20using%20synthetic%20data%0Ageneration%20and%20disentanglement%20to%20learn%20to%20regress%203D%20shape%20and%20pose.%20Focusing%0Aon%20horses%2C%20we%20use%20text-based%20texture%20generation%20and%20a%20synthetic%20data%20pipeline%0Ato%20create%20varied%20shapes%2C%20poses%2C%20and%20appearances%2C%20learning%20disentangled%20spaces.%0AOur%20method%2C%20Dessie%2C%20surpasses%20existing%203D%20horse%20reconstruction%20methods%20and%0Ageneralizes%20to%20other%20large%20animals%20like%20zebras%2C%20cows%2C%20and%20deer.%20See%20the%20project%0Awebsite%20at%3A%20%5Curl%7Bhttps%3A//celiali.github.io/Dessie/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03438v2&entry.124074799=Read"},
{"title": "EZ-HOI: VLM Adaptation via Guided Prompt Learning for Zero-Shot HOI\n  Detection", "author": "Qinqian Lei and Bo Wang and Robby T. Tan", "abstract": "  Detecting Human-Object Interactions (HOI) in zero-shot settings, where models\nmust handle unseen classes, poses significant challenges. Existing methods that\nrely on aligning visual encoders with large Vision-Language Models (VLMs) to\ntap into the extensive knowledge of VLMs, require large, computationally\nexpensive models and encounter training difficulties. Adapting VLMs with prompt\nlearning offers an alternative to direct alignment. However, fine-tuning on\ntask-specific datasets often leads to overfitting to seen classes and\nsuboptimal performance on unseen classes, due to the absence of unseen class\nlabels. To address these challenges, we introduce a novel prompt learning-based\nframework for Efficient Zero-Shot HOI detection (EZ-HOI). First, we introduce\nLarge Language Model (LLM) and VLM guidance for learnable prompts, integrating\ndetailed HOI descriptions and visual semantics to adapt VLMs to HOI tasks.\nHowever, because training datasets contain seen-class labels alone, fine-tuning\nVLMs on such datasets tends to optimize learnable prompts for seen classes\ninstead of unseen ones. Therefore, we design prompt learning for unseen classes\nusing information from related seen classes, with LLMs utilized to highlight\nthe differences between unseen and related seen classes. Quantitative\nevaluations on benchmark datasets demonstrate that our EZ-HOI achieves\nstate-of-the-art performance across various zero-shot settings with only 10.35%\nto 33.95% of the trainable parameters compared to existing methods. Code is\navailable at https://github.com/ChelsieLei/EZ-HOI.\n", "link": "http://arxiv.org/abs/2410.23904v1", "date": "2024-10-31", "relevancy": 2.7224, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5452}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5452}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EZ-HOI%3A%20VLM%20Adaptation%20via%20Guided%20Prompt%20Learning%20for%20Zero-Shot%20HOI%0A%20%20Detection&body=Title%3A%20EZ-HOI%3A%20VLM%20Adaptation%20via%20Guided%20Prompt%20Learning%20for%20Zero-Shot%20HOI%0A%20%20Detection%0AAuthor%3A%20Qinqian%20Lei%20and%20Bo%20Wang%20and%20Robby%20T.%20Tan%0AAbstract%3A%20%20%20Detecting%20Human-Object%20Interactions%20%28HOI%29%20in%20zero-shot%20settings%2C%20where%20models%0Amust%20handle%20unseen%20classes%2C%20poses%20significant%20challenges.%20Existing%20methods%20that%0Arely%20on%20aligning%20visual%20encoders%20with%20large%20Vision-Language%20Models%20%28VLMs%29%20to%0Atap%20into%20the%20extensive%20knowledge%20of%20VLMs%2C%20require%20large%2C%20computationally%0Aexpensive%20models%20and%20encounter%20training%20difficulties.%20Adapting%20VLMs%20with%20prompt%0Alearning%20offers%20an%20alternative%20to%20direct%20alignment.%20However%2C%20fine-tuning%20on%0Atask-specific%20datasets%20often%20leads%20to%20overfitting%20to%20seen%20classes%20and%0Asuboptimal%20performance%20on%20unseen%20classes%2C%20due%20to%20the%20absence%20of%20unseen%20class%0Alabels.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%20prompt%20learning-based%0Aframework%20for%20Efficient%20Zero-Shot%20HOI%20detection%20%28EZ-HOI%29.%20First%2C%20we%20introduce%0ALarge%20Language%20Model%20%28LLM%29%20and%20VLM%20guidance%20for%20learnable%20prompts%2C%20integrating%0Adetailed%20HOI%20descriptions%20and%20visual%20semantics%20to%20adapt%20VLMs%20to%20HOI%20tasks.%0AHowever%2C%20because%20training%20datasets%20contain%20seen-class%20labels%20alone%2C%20fine-tuning%0AVLMs%20on%20such%20datasets%20tends%20to%20optimize%20learnable%20prompts%20for%20seen%20classes%0Ainstead%20of%20unseen%20ones.%20Therefore%2C%20we%20design%20prompt%20learning%20for%20unseen%20classes%0Ausing%20information%20from%20related%20seen%20classes%2C%20with%20LLMs%20utilized%20to%20highlight%0Athe%20differences%20between%20unseen%20and%20related%20seen%20classes.%20Quantitative%0Aevaluations%20on%20benchmark%20datasets%20demonstrate%20that%20our%20EZ-HOI%20achieves%0Astate-of-the-art%20performance%20across%20various%20zero-shot%20settings%20with%20only%2010.35%25%0Ato%2033.95%25%20of%20the%20trainable%20parameters%20compared%20to%20existing%20methods.%20Code%20is%0Aavailable%20at%20https%3A//github.com/ChelsieLei/EZ-HOI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23904v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEZ-HOI%253A%2520VLM%2520Adaptation%2520via%2520Guided%2520Prompt%2520Learning%2520for%2520Zero-Shot%2520HOI%250A%2520%2520Detection%26entry.906535625%3DQinqian%2520Lei%2520and%2520Bo%2520Wang%2520and%2520Robby%2520T.%2520Tan%26entry.1292438233%3D%2520%2520Detecting%2520Human-Object%2520Interactions%2520%2528HOI%2529%2520in%2520zero-shot%2520settings%252C%2520where%2520models%250Amust%2520handle%2520unseen%2520classes%252C%2520poses%2520significant%2520challenges.%2520Existing%2520methods%2520that%250Arely%2520on%2520aligning%2520visual%2520encoders%2520with%2520large%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520to%250Atap%2520into%2520the%2520extensive%2520knowledge%2520of%2520VLMs%252C%2520require%2520large%252C%2520computationally%250Aexpensive%2520models%2520and%2520encounter%2520training%2520difficulties.%2520Adapting%2520VLMs%2520with%2520prompt%250Alearning%2520offers%2520an%2520alternative%2520to%2520direct%2520alignment.%2520However%252C%2520fine-tuning%2520on%250Atask-specific%2520datasets%2520often%2520leads%2520to%2520overfitting%2520to%2520seen%2520classes%2520and%250Asuboptimal%2520performance%2520on%2520unseen%2520classes%252C%2520due%2520to%2520the%2520absence%2520of%2520unseen%2520class%250Alabels.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520a%2520novel%2520prompt%2520learning-based%250Aframework%2520for%2520Efficient%2520Zero-Shot%2520HOI%2520detection%2520%2528EZ-HOI%2529.%2520First%252C%2520we%2520introduce%250ALarge%2520Language%2520Model%2520%2528LLM%2529%2520and%2520VLM%2520guidance%2520for%2520learnable%2520prompts%252C%2520integrating%250Adetailed%2520HOI%2520descriptions%2520and%2520visual%2520semantics%2520to%2520adapt%2520VLMs%2520to%2520HOI%2520tasks.%250AHowever%252C%2520because%2520training%2520datasets%2520contain%2520seen-class%2520labels%2520alone%252C%2520fine-tuning%250AVLMs%2520on%2520such%2520datasets%2520tends%2520to%2520optimize%2520learnable%2520prompts%2520for%2520seen%2520classes%250Ainstead%2520of%2520unseen%2520ones.%2520Therefore%252C%2520we%2520design%2520prompt%2520learning%2520for%2520unseen%2520classes%250Ausing%2520information%2520from%2520related%2520seen%2520classes%252C%2520with%2520LLMs%2520utilized%2520to%2520highlight%250Athe%2520differences%2520between%2520unseen%2520and%2520related%2520seen%2520classes.%2520Quantitative%250Aevaluations%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%2520our%2520EZ-HOI%2520achieves%250Astate-of-the-art%2520performance%2520across%2520various%2520zero-shot%2520settings%2520with%2520only%252010.35%2525%250Ato%252033.95%2525%2520of%2520the%2520trainable%2520parameters%2520compared%2520to%2520existing%2520methods.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/ChelsieLei/EZ-HOI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23904v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EZ-HOI%3A%20VLM%20Adaptation%20via%20Guided%20Prompt%20Learning%20for%20Zero-Shot%20HOI%0A%20%20Detection&entry.906535625=Qinqian%20Lei%20and%20Bo%20Wang%20and%20Robby%20T.%20Tan&entry.1292438233=%20%20Detecting%20Human-Object%20Interactions%20%28HOI%29%20in%20zero-shot%20settings%2C%20where%20models%0Amust%20handle%20unseen%20classes%2C%20poses%20significant%20challenges.%20Existing%20methods%20that%0Arely%20on%20aligning%20visual%20encoders%20with%20large%20Vision-Language%20Models%20%28VLMs%29%20to%0Atap%20into%20the%20extensive%20knowledge%20of%20VLMs%2C%20require%20large%2C%20computationally%0Aexpensive%20models%20and%20encounter%20training%20difficulties.%20Adapting%20VLMs%20with%20prompt%0Alearning%20offers%20an%20alternative%20to%20direct%20alignment.%20However%2C%20fine-tuning%20on%0Atask-specific%20datasets%20often%20leads%20to%20overfitting%20to%20seen%20classes%20and%0Asuboptimal%20performance%20on%20unseen%20classes%2C%20due%20to%20the%20absence%20of%20unseen%20class%0Alabels.%20To%20address%20these%20challenges%2C%20we%20introduce%20a%20novel%20prompt%20learning-based%0Aframework%20for%20Efficient%20Zero-Shot%20HOI%20detection%20%28EZ-HOI%29.%20First%2C%20we%20introduce%0ALarge%20Language%20Model%20%28LLM%29%20and%20VLM%20guidance%20for%20learnable%20prompts%2C%20integrating%0Adetailed%20HOI%20descriptions%20and%20visual%20semantics%20to%20adapt%20VLMs%20to%20HOI%20tasks.%0AHowever%2C%20because%20training%20datasets%20contain%20seen-class%20labels%20alone%2C%20fine-tuning%0AVLMs%20on%20such%20datasets%20tends%20to%20optimize%20learnable%20prompts%20for%20seen%20classes%0Ainstead%20of%20unseen%20ones.%20Therefore%2C%20we%20design%20prompt%20learning%20for%20unseen%20classes%0Ausing%20information%20from%20related%20seen%20classes%2C%20with%20LLMs%20utilized%20to%20highlight%0Athe%20differences%20between%20unseen%20and%20related%20seen%20classes.%20Quantitative%0Aevaluations%20on%20benchmark%20datasets%20demonstrate%20that%20our%20EZ-HOI%20achieves%0Astate-of-the-art%20performance%20across%20various%20zero-shot%20settings%20with%20only%2010.35%25%0Ato%2033.95%25%20of%20the%20trainable%20parameters%20compared%20to%20existing%20methods.%20Code%20is%0Aavailable%20at%20https%3A//github.com/ChelsieLei/EZ-HOI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23904v1&entry.124074799=Read"},
{"title": "Embracing Events and Frames with Hierarchical Feature Refinement Network\n  for Object Detection", "author": "Hu Cao and Zehua Zhang and Yan Xia and Xinyi Li and Jiahao Xia and Guang Chen and Alois Knoll", "abstract": "  In frame-based vision, object detection faces substantial performance\ndegradation under challenging conditions due to the limited sensing capability\nof conventional cameras. Event cameras output sparse and asynchronous events,\nproviding a potential solution to solve these problems. However, effectively\nfusing two heterogeneous modalities remains an open issue. In this work, we\npropose a novel hierarchical feature refinement network for event-frame fusion.\nThe core concept is the design of the coarse-to-fine fusion module, denoted as\nthe cross-modality adaptive feature refinement (CAFR) module. In the initial\nphase, the bidirectional cross-modality interaction (BCI) part facilitates\ninformation bridging from two distinct sources. Subsequently, the features are\nfurther refined by aligning the channel-level mean and variance in the two-fold\nadaptive feature refinement (TAFR) part. We conducted extensive experiments on\ntwo benchmarks: the low-resolution PKU-DDD17-Car dataset and the\nhigh-resolution DSEC dataset. Experimental results show that our method\nsurpasses the state-of-the-art by an impressive margin of $\\textbf{8.0}\\%$ on\nthe DSEC dataset. Besides, our method exhibits significantly better robustness\n(\\textbf{69.5}\\% versus \\textbf{38.7}\\%) when introducing 15 different\ncorruption types to the frame images. The code can be found at the link\n(https://github.com/HuCaoFighting/FRN).\n", "link": "http://arxiv.org/abs/2407.12582v2", "date": "2024-10-31", "relevancy": 2.7186, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5597}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5404}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embracing%20Events%20and%20Frames%20with%20Hierarchical%20Feature%20Refinement%20Network%0A%20%20for%20Object%20Detection&body=Title%3A%20Embracing%20Events%20and%20Frames%20with%20Hierarchical%20Feature%20Refinement%20Network%0A%20%20for%20Object%20Detection%0AAuthor%3A%20Hu%20Cao%20and%20Zehua%20Zhang%20and%20Yan%20Xia%20and%20Xinyi%20Li%20and%20Jiahao%20Xia%20and%20Guang%20Chen%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20In%20frame-based%20vision%2C%20object%20detection%20faces%20substantial%20performance%0Adegradation%20under%20challenging%20conditions%20due%20to%20the%20limited%20sensing%20capability%0Aof%20conventional%20cameras.%20Event%20cameras%20output%20sparse%20and%20asynchronous%20events%2C%0Aproviding%20a%20potential%20solution%20to%20solve%20these%20problems.%20However%2C%20effectively%0Afusing%20two%20heterogeneous%20modalities%20remains%20an%20open%20issue.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20hierarchical%20feature%20refinement%20network%20for%20event-frame%20fusion.%0AThe%20core%20concept%20is%20the%20design%20of%20the%20coarse-to-fine%20fusion%20module%2C%20denoted%20as%0Athe%20cross-modality%20adaptive%20feature%20refinement%20%28CAFR%29%20module.%20In%20the%20initial%0Aphase%2C%20the%20bidirectional%20cross-modality%20interaction%20%28BCI%29%20part%20facilitates%0Ainformation%20bridging%20from%20two%20distinct%20sources.%20Subsequently%2C%20the%20features%20are%0Afurther%20refined%20by%20aligning%20the%20channel-level%20mean%20and%20variance%20in%20the%20two-fold%0Aadaptive%20feature%20refinement%20%28TAFR%29%20part.%20We%20conducted%20extensive%20experiments%20on%0Atwo%20benchmarks%3A%20the%20low-resolution%20PKU-DDD17-Car%20dataset%20and%20the%0Ahigh-resolution%20DSEC%20dataset.%20Experimental%20results%20show%20that%20our%20method%0Asurpasses%20the%20state-of-the-art%20by%20an%20impressive%20margin%20of%20%24%5Ctextbf%7B8.0%7D%5C%25%24%20on%0Athe%20DSEC%20dataset.%20Besides%2C%20our%20method%20exhibits%20significantly%20better%20robustness%0A%28%5Ctextbf%7B69.5%7D%5C%25%20versus%20%5Ctextbf%7B38.7%7D%5C%25%29%20when%20introducing%2015%20different%0Acorruption%20types%20to%20the%20frame%20images.%20The%20code%20can%20be%20found%20at%20the%20link%0A%28https%3A//github.com/HuCaoFighting/FRN%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12582v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbracing%2520Events%2520and%2520Frames%2520with%2520Hierarchical%2520Feature%2520Refinement%2520Network%250A%2520%2520for%2520Object%2520Detection%26entry.906535625%3DHu%2520Cao%2520and%2520Zehua%2520Zhang%2520and%2520Yan%2520Xia%2520and%2520Xinyi%2520Li%2520and%2520Jiahao%2520Xia%2520and%2520Guang%2520Chen%2520and%2520Alois%2520Knoll%26entry.1292438233%3D%2520%2520In%2520frame-based%2520vision%252C%2520object%2520detection%2520faces%2520substantial%2520performance%250Adegradation%2520under%2520challenging%2520conditions%2520due%2520to%2520the%2520limited%2520sensing%2520capability%250Aof%2520conventional%2520cameras.%2520Event%2520cameras%2520output%2520sparse%2520and%2520asynchronous%2520events%252C%250Aproviding%2520a%2520potential%2520solution%2520to%2520solve%2520these%2520problems.%2520However%252C%2520effectively%250Afusing%2520two%2520heterogeneous%2520modalities%2520remains%2520an%2520open%2520issue.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520hierarchical%2520feature%2520refinement%2520network%2520for%2520event-frame%2520fusion.%250AThe%2520core%2520concept%2520is%2520the%2520design%2520of%2520the%2520coarse-to-fine%2520fusion%2520module%252C%2520denoted%2520as%250Athe%2520cross-modality%2520adaptive%2520feature%2520refinement%2520%2528CAFR%2529%2520module.%2520In%2520the%2520initial%250Aphase%252C%2520the%2520bidirectional%2520cross-modality%2520interaction%2520%2528BCI%2529%2520part%2520facilitates%250Ainformation%2520bridging%2520from%2520two%2520distinct%2520sources.%2520Subsequently%252C%2520the%2520features%2520are%250Afurther%2520refined%2520by%2520aligning%2520the%2520channel-level%2520mean%2520and%2520variance%2520in%2520the%2520two-fold%250Aadaptive%2520feature%2520refinement%2520%2528TAFR%2529%2520part.%2520We%2520conducted%2520extensive%2520experiments%2520on%250Atwo%2520benchmarks%253A%2520the%2520low-resolution%2520PKU-DDD17-Car%2520dataset%2520and%2520the%250Ahigh-resolution%2520DSEC%2520dataset.%2520Experimental%2520results%2520show%2520that%2520our%2520method%250Asurpasses%2520the%2520state-of-the-art%2520by%2520an%2520impressive%2520margin%2520of%2520%2524%255Ctextbf%257B8.0%257D%255C%2525%2524%2520on%250Athe%2520DSEC%2520dataset.%2520Besides%252C%2520our%2520method%2520exhibits%2520significantly%2520better%2520robustness%250A%2528%255Ctextbf%257B69.5%257D%255C%2525%2520versus%2520%255Ctextbf%257B38.7%257D%255C%2525%2529%2520when%2520introducing%252015%2520different%250Acorruption%2520types%2520to%2520the%2520frame%2520images.%2520The%2520code%2520can%2520be%2520found%2520at%2520the%2520link%250A%2528https%253A//github.com/HuCaoFighting/FRN%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12582v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embracing%20Events%20and%20Frames%20with%20Hierarchical%20Feature%20Refinement%20Network%0A%20%20for%20Object%20Detection&entry.906535625=Hu%20Cao%20and%20Zehua%20Zhang%20and%20Yan%20Xia%20and%20Xinyi%20Li%20and%20Jiahao%20Xia%20and%20Guang%20Chen%20and%20Alois%20Knoll&entry.1292438233=%20%20In%20frame-based%20vision%2C%20object%20detection%20faces%20substantial%20performance%0Adegradation%20under%20challenging%20conditions%20due%20to%20the%20limited%20sensing%20capability%0Aof%20conventional%20cameras.%20Event%20cameras%20output%20sparse%20and%20asynchronous%20events%2C%0Aproviding%20a%20potential%20solution%20to%20solve%20these%20problems.%20However%2C%20effectively%0Afusing%20two%20heterogeneous%20modalities%20remains%20an%20open%20issue.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20hierarchical%20feature%20refinement%20network%20for%20event-frame%20fusion.%0AThe%20core%20concept%20is%20the%20design%20of%20the%20coarse-to-fine%20fusion%20module%2C%20denoted%20as%0Athe%20cross-modality%20adaptive%20feature%20refinement%20%28CAFR%29%20module.%20In%20the%20initial%0Aphase%2C%20the%20bidirectional%20cross-modality%20interaction%20%28BCI%29%20part%20facilitates%0Ainformation%20bridging%20from%20two%20distinct%20sources.%20Subsequently%2C%20the%20features%20are%0Afurther%20refined%20by%20aligning%20the%20channel-level%20mean%20and%20variance%20in%20the%20two-fold%0Aadaptive%20feature%20refinement%20%28TAFR%29%20part.%20We%20conducted%20extensive%20experiments%20on%0Atwo%20benchmarks%3A%20the%20low-resolution%20PKU-DDD17-Car%20dataset%20and%20the%0Ahigh-resolution%20DSEC%20dataset.%20Experimental%20results%20show%20that%20our%20method%0Asurpasses%20the%20state-of-the-art%20by%20an%20impressive%20margin%20of%20%24%5Ctextbf%7B8.0%7D%5C%25%24%20on%0Athe%20DSEC%20dataset.%20Besides%2C%20our%20method%20exhibits%20significantly%20better%20robustness%0A%28%5Ctextbf%7B69.5%7D%5C%25%20versus%20%5Ctextbf%7B38.7%7D%5C%25%29%20when%20introducing%2015%20different%0Acorruption%20types%20to%20the%20frame%20images.%20The%20code%20can%20be%20found%20at%20the%20link%0A%28https%3A//github.com/HuCaoFighting/FRN%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12582v2&entry.124074799=Read"},
{"title": "MIO: A Foundation Model on Multimodal Tokens", "author": "Zekun Wang and King Zhu and Chunpu Xu and Wangchunshu Zhou and Jiaheng Liu and Yibo Zhang and Jiashuo Wang and Ning Shi and Siyu Li and Yizhi Li and Haoran Que and Zhaoxiang Zhang and Yuanxing Zhang and Ge Zhang and Ke Xu and Jie Fu and Wenhao Huang", "abstract": "  In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.\n", "link": "http://arxiv.org/abs/2409.17692v2", "date": "2024-10-31", "relevancy": 2.7183, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5459}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5434}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MIO%3A%20A%20Foundation%20Model%20on%20Multimodal%20Tokens&body=Title%3A%20MIO%3A%20A%20Foundation%20Model%20on%20Multimodal%20Tokens%0AAuthor%3A%20Zekun%20Wang%20and%20King%20Zhu%20and%20Chunpu%20Xu%20and%20Wangchunshu%20Zhou%20and%20Jiaheng%20Liu%20and%20Yibo%20Zhang%20and%20Jiashuo%20Wang%20and%20Ning%20Shi%20and%20Siyu%20Li%20and%20Yizhi%20Li%20and%20Haoran%20Que%20and%20Zhaoxiang%20Zhang%20and%20Yuanxing%20Zhang%20and%20Ge%20Zhang%20and%20Ke%20Xu%20and%20Jie%20Fu%20and%20Wenhao%20Huang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20MIO%2C%20a%20novel%20foundation%20model%20built%20on%20multimodal%0Atokens%2C%20capable%20of%20understanding%20and%20generating%20speech%2C%20text%2C%20images%2C%20and%0Avideos%20in%20an%20end-to-end%2C%20autoregressive%20manner.%20While%20the%20emergence%20of%20large%0Alanguage%20models%20%28LLMs%29%20and%20multimodal%20large%20language%20models%20%28MM-LLMs%29%20propels%0Aadvancements%20in%20artificial%20general%20intelligence%20through%20their%20versatile%0Acapabilities%2C%20they%20still%20lack%20true%20any-to-any%20understanding%20and%20generation.%0ARecently%2C%20the%20release%20of%20GPT-4o%20has%20showcased%20the%20remarkable%20potential%20of%0Aany-to-any%20LLMs%20for%20complex%20real-world%20tasks%2C%20enabling%20omnidirectional%20input%0Aand%20output%20across%20images%2C%20speech%2C%20and%20text.%20However%2C%20it%20is%20closed-source%20and%0Adoes%20not%20support%20the%20generation%20of%20multimodal%20interleaved%20sequences.%20To%20address%0Athis%20gap%2C%20we%20present%20MIO%2C%20which%20is%20trained%20on%20a%20mixture%20of%20discrete%20tokens%0Aacross%20four%20modalities%20using%20causal%20multimodal%20modeling.%20MIO%20undergoes%20a%0Afour-stage%20training%20process%3A%20%281%29%20alignment%20pre-training%2C%20%282%29%20interleaved%0Apre-training%2C%20%283%29%20speech-enhanced%20pre-training%2C%20and%20%284%29%20comprehensive%0Asupervised%20fine-tuning%20on%20diverse%20textual%2C%20visual%2C%20and%20speech%20tasks.%20Our%0Aexperimental%20results%20indicate%20that%20MIO%20exhibits%20competitive%2C%20and%20in%20some%20cases%0Asuperior%2C%20performance%20compared%20to%20previous%20dual-modal%20baselines%2C%20any-to-any%0Amodel%20baselines%2C%20and%20even%20modality-specific%20baselines.%20Moreover%2C%20MIO%0Ademonstrates%20advanced%20capabilities%20inherent%20to%20its%20any-to-any%20feature%2C%20such%20as%0Ainterleaved%20video-text%20generation%2C%20chain-of-visual-thought%20reasoning%2C%20visual%0Aguideline%20generation%2C%20instructional%20image%20editing%2C%20etc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17692v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMIO%253A%2520A%2520Foundation%2520Model%2520on%2520Multimodal%2520Tokens%26entry.906535625%3DZekun%2520Wang%2520and%2520King%2520Zhu%2520and%2520Chunpu%2520Xu%2520and%2520Wangchunshu%2520Zhou%2520and%2520Jiaheng%2520Liu%2520and%2520Yibo%2520Zhang%2520and%2520Jiashuo%2520Wang%2520and%2520Ning%2520Shi%2520and%2520Siyu%2520Li%2520and%2520Yizhi%2520Li%2520and%2520Haoran%2520Que%2520and%2520Zhaoxiang%2520Zhang%2520and%2520Yuanxing%2520Zhang%2520and%2520Ge%2520Zhang%2520and%2520Ke%2520Xu%2520and%2520Jie%2520Fu%2520and%2520Wenhao%2520Huang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520MIO%252C%2520a%2520novel%2520foundation%2520model%2520built%2520on%2520multimodal%250Atokens%252C%2520capable%2520of%2520understanding%2520and%2520generating%2520speech%252C%2520text%252C%2520images%252C%2520and%250Avideos%2520in%2520an%2520end-to-end%252C%2520autoregressive%2520manner.%2520While%2520the%2520emergence%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520and%2520multimodal%2520large%2520language%2520models%2520%2528MM-LLMs%2529%2520propels%250Aadvancements%2520in%2520artificial%2520general%2520intelligence%2520through%2520their%2520versatile%250Acapabilities%252C%2520they%2520still%2520lack%2520true%2520any-to-any%2520understanding%2520and%2520generation.%250ARecently%252C%2520the%2520release%2520of%2520GPT-4o%2520has%2520showcased%2520the%2520remarkable%2520potential%2520of%250Aany-to-any%2520LLMs%2520for%2520complex%2520real-world%2520tasks%252C%2520enabling%2520omnidirectional%2520input%250Aand%2520output%2520across%2520images%252C%2520speech%252C%2520and%2520text.%2520However%252C%2520it%2520is%2520closed-source%2520and%250Adoes%2520not%2520support%2520the%2520generation%2520of%2520multimodal%2520interleaved%2520sequences.%2520To%2520address%250Athis%2520gap%252C%2520we%2520present%2520MIO%252C%2520which%2520is%2520trained%2520on%2520a%2520mixture%2520of%2520discrete%2520tokens%250Aacross%2520four%2520modalities%2520using%2520causal%2520multimodal%2520modeling.%2520MIO%2520undergoes%2520a%250Afour-stage%2520training%2520process%253A%2520%25281%2529%2520alignment%2520pre-training%252C%2520%25282%2529%2520interleaved%250Apre-training%252C%2520%25283%2529%2520speech-enhanced%2520pre-training%252C%2520and%2520%25284%2529%2520comprehensive%250Asupervised%2520fine-tuning%2520on%2520diverse%2520textual%252C%2520visual%252C%2520and%2520speech%2520tasks.%2520Our%250Aexperimental%2520results%2520indicate%2520that%2520MIO%2520exhibits%2520competitive%252C%2520and%2520in%2520some%2520cases%250Asuperior%252C%2520performance%2520compared%2520to%2520previous%2520dual-modal%2520baselines%252C%2520any-to-any%250Amodel%2520baselines%252C%2520and%2520even%2520modality-specific%2520baselines.%2520Moreover%252C%2520MIO%250Ademonstrates%2520advanced%2520capabilities%2520inherent%2520to%2520its%2520any-to-any%2520feature%252C%2520such%2520as%250Ainterleaved%2520video-text%2520generation%252C%2520chain-of-visual-thought%2520reasoning%252C%2520visual%250Aguideline%2520generation%252C%2520instructional%2520image%2520editing%252C%2520etc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17692v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MIO%3A%20A%20Foundation%20Model%20on%20Multimodal%20Tokens&entry.906535625=Zekun%20Wang%20and%20King%20Zhu%20and%20Chunpu%20Xu%20and%20Wangchunshu%20Zhou%20and%20Jiaheng%20Liu%20and%20Yibo%20Zhang%20and%20Jiashuo%20Wang%20and%20Ning%20Shi%20and%20Siyu%20Li%20and%20Yizhi%20Li%20and%20Haoran%20Que%20and%20Zhaoxiang%20Zhang%20and%20Yuanxing%20Zhang%20and%20Ge%20Zhang%20and%20Ke%20Xu%20and%20Jie%20Fu%20and%20Wenhao%20Huang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20MIO%2C%20a%20novel%20foundation%20model%20built%20on%20multimodal%0Atokens%2C%20capable%20of%20understanding%20and%20generating%20speech%2C%20text%2C%20images%2C%20and%0Avideos%20in%20an%20end-to-end%2C%20autoregressive%20manner.%20While%20the%20emergence%20of%20large%0Alanguage%20models%20%28LLMs%29%20and%20multimodal%20large%20language%20models%20%28MM-LLMs%29%20propels%0Aadvancements%20in%20artificial%20general%20intelligence%20through%20their%20versatile%0Acapabilities%2C%20they%20still%20lack%20true%20any-to-any%20understanding%20and%20generation.%0ARecently%2C%20the%20release%20of%20GPT-4o%20has%20showcased%20the%20remarkable%20potential%20of%0Aany-to-any%20LLMs%20for%20complex%20real-world%20tasks%2C%20enabling%20omnidirectional%20input%0Aand%20output%20across%20images%2C%20speech%2C%20and%20text.%20However%2C%20it%20is%20closed-source%20and%0Adoes%20not%20support%20the%20generation%20of%20multimodal%20interleaved%20sequences.%20To%20address%0Athis%20gap%2C%20we%20present%20MIO%2C%20which%20is%20trained%20on%20a%20mixture%20of%20discrete%20tokens%0Aacross%20four%20modalities%20using%20causal%20multimodal%20modeling.%20MIO%20undergoes%20a%0Afour-stage%20training%20process%3A%20%281%29%20alignment%20pre-training%2C%20%282%29%20interleaved%0Apre-training%2C%20%283%29%20speech-enhanced%20pre-training%2C%20and%20%284%29%20comprehensive%0Asupervised%20fine-tuning%20on%20diverse%20textual%2C%20visual%2C%20and%20speech%20tasks.%20Our%0Aexperimental%20results%20indicate%20that%20MIO%20exhibits%20competitive%2C%20and%20in%20some%20cases%0Asuperior%2C%20performance%20compared%20to%20previous%20dual-modal%20baselines%2C%20any-to-any%0Amodel%20baselines%2C%20and%20even%20modality-specific%20baselines.%20Moreover%2C%20MIO%0Ademonstrates%20advanced%20capabilities%20inherent%20to%20its%20any-to-any%20feature%2C%20such%20as%0Ainterleaved%20video-text%20generation%2C%20chain-of-visual-thought%20reasoning%2C%20visual%0Aguideline%20generation%2C%20instructional%20image%20editing%2C%20etc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17692v2&entry.124074799=Read"},
{"title": "EraW-Net: Enhance-Refine-Align W-Net for Scene-Associated Driver\n  Attention Estimation", "author": "Jun Zhou and Chunsheng Liu and Faliang Chang and Wenqian Wang and Penghui Hao and Yiming Huang and Zhiqiang Yang", "abstract": "  Associating driver attention with driving scene across two fields of views\n(FOVs) is a hard cross-domain perception problem, which requires comprehensive\nconsideration of cross-view mapping, dynamic driving scene analysis, and driver\nstatus tracking. Previous methods typically focus on a single view or map\nattention to the scene via estimated gaze, failing to exploit the implicit\nconnection between them. Moreover, simple fusion modules are insufficient for\nmodeling the complex relationships between the two views, making information\nintegration challenging. To address these issues, we propose a novel method for\nend-to-end scene-associated driver attention estimation, called EraW-Net. This\nmethod enhances the most discriminative dynamic cues, refines feature\nrepresentations, and facilitates semantically aligned cross-domain integration\nthrough a W-shaped architecture, termed W-Net. Specifically, a Dynamic Adaptive\nFilter Module (DAF-Module) is proposed to address the challenges of frequently\nchanging driving environments by extracting vital regions. It suppresses the\nindiscriminately recorded dynamics and highlights crucial ones by innovative\njoint frequency-spatial analysis, enhancing the model's ability to parse\ncomplex dynamics. Additionally, to track driver states during non-fixed facial\nposes, we propose a Global Context Sharing Module (GCS-Module) to construct\nrefined feature representations by capturing hierarchical features that adapt\nto various scales of head and eye movements. Finally, W-Net achieves systematic\ncross-view information integration through its \"Encoding-Independent Partial\nDecoding-Fusion Decoding\" structure, addressing semantic misalignment in\nheterogeneous data integration. Experiments demonstrate that the proposed\nmethod robustly and accurately estimates the mapping of driver attention in\nscene on large public datasets.\n", "link": "http://arxiv.org/abs/2408.08570v2", "date": "2024-10-31", "relevancy": 2.7138, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5464}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5464}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EraW-Net%3A%20Enhance-Refine-Align%20W-Net%20for%20Scene-Associated%20Driver%0A%20%20Attention%20Estimation&body=Title%3A%20EraW-Net%3A%20Enhance-Refine-Align%20W-Net%20for%20Scene-Associated%20Driver%0A%20%20Attention%20Estimation%0AAuthor%3A%20Jun%20Zhou%20and%20Chunsheng%20Liu%20and%20Faliang%20Chang%20and%20Wenqian%20Wang%20and%20Penghui%20Hao%20and%20Yiming%20Huang%20and%20Zhiqiang%20Yang%0AAbstract%3A%20%20%20Associating%20driver%20attention%20with%20driving%20scene%20across%20two%20fields%20of%20views%0A%28FOVs%29%20is%20a%20hard%20cross-domain%20perception%20problem%2C%20which%20requires%20comprehensive%0Aconsideration%20of%20cross-view%20mapping%2C%20dynamic%20driving%20scene%20analysis%2C%20and%20driver%0Astatus%20tracking.%20Previous%20methods%20typically%20focus%20on%20a%20single%20view%20or%20map%0Aattention%20to%20the%20scene%20via%20estimated%20gaze%2C%20failing%20to%20exploit%20the%20implicit%0Aconnection%20between%20them.%20Moreover%2C%20simple%20fusion%20modules%20are%20insufficient%20for%0Amodeling%20the%20complex%20relationships%20between%20the%20two%20views%2C%20making%20information%0Aintegration%20challenging.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20method%20for%0Aend-to-end%20scene-associated%20driver%20attention%20estimation%2C%20called%20EraW-Net.%20This%0Amethod%20enhances%20the%20most%20discriminative%20dynamic%20cues%2C%20refines%20feature%0Arepresentations%2C%20and%20facilitates%20semantically%20aligned%20cross-domain%20integration%0Athrough%20a%20W-shaped%20architecture%2C%20termed%20W-Net.%20Specifically%2C%20a%20Dynamic%20Adaptive%0AFilter%20Module%20%28DAF-Module%29%20is%20proposed%20to%20address%20the%20challenges%20of%20frequently%0Achanging%20driving%20environments%20by%20extracting%20vital%20regions.%20It%20suppresses%20the%0Aindiscriminately%20recorded%20dynamics%20and%20highlights%20crucial%20ones%20by%20innovative%0Ajoint%20frequency-spatial%20analysis%2C%20enhancing%20the%20model%27s%20ability%20to%20parse%0Acomplex%20dynamics.%20Additionally%2C%20to%20track%20driver%20states%20during%20non-fixed%20facial%0Aposes%2C%20we%20propose%20a%20Global%20Context%20Sharing%20Module%20%28GCS-Module%29%20to%20construct%0Arefined%20feature%20representations%20by%20capturing%20hierarchical%20features%20that%20adapt%0Ato%20various%20scales%20of%20head%20and%20eye%20movements.%20Finally%2C%20W-Net%20achieves%20systematic%0Across-view%20information%20integration%20through%20its%20%22Encoding-Independent%20Partial%0ADecoding-Fusion%20Decoding%22%20structure%2C%20addressing%20semantic%20misalignment%20in%0Aheterogeneous%20data%20integration.%20Experiments%20demonstrate%20that%20the%20proposed%0Amethod%20robustly%20and%20accurately%20estimates%20the%20mapping%20of%20driver%20attention%20in%0Ascene%20on%20large%20public%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08570v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEraW-Net%253A%2520Enhance-Refine-Align%2520W-Net%2520for%2520Scene-Associated%2520Driver%250A%2520%2520Attention%2520Estimation%26entry.906535625%3DJun%2520Zhou%2520and%2520Chunsheng%2520Liu%2520and%2520Faliang%2520Chang%2520and%2520Wenqian%2520Wang%2520and%2520Penghui%2520Hao%2520and%2520Yiming%2520Huang%2520and%2520Zhiqiang%2520Yang%26entry.1292438233%3D%2520%2520Associating%2520driver%2520attention%2520with%2520driving%2520scene%2520across%2520two%2520fields%2520of%2520views%250A%2528FOVs%2529%2520is%2520a%2520hard%2520cross-domain%2520perception%2520problem%252C%2520which%2520requires%2520comprehensive%250Aconsideration%2520of%2520cross-view%2520mapping%252C%2520dynamic%2520driving%2520scene%2520analysis%252C%2520and%2520driver%250Astatus%2520tracking.%2520Previous%2520methods%2520typically%2520focus%2520on%2520a%2520single%2520view%2520or%2520map%250Aattention%2520to%2520the%2520scene%2520via%2520estimated%2520gaze%252C%2520failing%2520to%2520exploit%2520the%2520implicit%250Aconnection%2520between%2520them.%2520Moreover%252C%2520simple%2520fusion%2520modules%2520are%2520insufficient%2520for%250Amodeling%2520the%2520complex%2520relationships%2520between%2520the%2520two%2520views%252C%2520making%2520information%250Aintegration%2520challenging.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520method%2520for%250Aend-to-end%2520scene-associated%2520driver%2520attention%2520estimation%252C%2520called%2520EraW-Net.%2520This%250Amethod%2520enhances%2520the%2520most%2520discriminative%2520dynamic%2520cues%252C%2520refines%2520feature%250Arepresentations%252C%2520and%2520facilitates%2520semantically%2520aligned%2520cross-domain%2520integration%250Athrough%2520a%2520W-shaped%2520architecture%252C%2520termed%2520W-Net.%2520Specifically%252C%2520a%2520Dynamic%2520Adaptive%250AFilter%2520Module%2520%2528DAF-Module%2529%2520is%2520proposed%2520to%2520address%2520the%2520challenges%2520of%2520frequently%250Achanging%2520driving%2520environments%2520by%2520extracting%2520vital%2520regions.%2520It%2520suppresses%2520the%250Aindiscriminately%2520recorded%2520dynamics%2520and%2520highlights%2520crucial%2520ones%2520by%2520innovative%250Ajoint%2520frequency-spatial%2520analysis%252C%2520enhancing%2520the%2520model%2527s%2520ability%2520to%2520parse%250Acomplex%2520dynamics.%2520Additionally%252C%2520to%2520track%2520driver%2520states%2520during%2520non-fixed%2520facial%250Aposes%252C%2520we%2520propose%2520a%2520Global%2520Context%2520Sharing%2520Module%2520%2528GCS-Module%2529%2520to%2520construct%250Arefined%2520feature%2520representations%2520by%2520capturing%2520hierarchical%2520features%2520that%2520adapt%250Ato%2520various%2520scales%2520of%2520head%2520and%2520eye%2520movements.%2520Finally%252C%2520W-Net%2520achieves%2520systematic%250Across-view%2520information%2520integration%2520through%2520its%2520%2522Encoding-Independent%2520Partial%250ADecoding-Fusion%2520Decoding%2522%2520structure%252C%2520addressing%2520semantic%2520misalignment%2520in%250Aheterogeneous%2520data%2520integration.%2520Experiments%2520demonstrate%2520that%2520the%2520proposed%250Amethod%2520robustly%2520and%2520accurately%2520estimates%2520the%2520mapping%2520of%2520driver%2520attention%2520in%250Ascene%2520on%2520large%2520public%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08570v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EraW-Net%3A%20Enhance-Refine-Align%20W-Net%20for%20Scene-Associated%20Driver%0A%20%20Attention%20Estimation&entry.906535625=Jun%20Zhou%20and%20Chunsheng%20Liu%20and%20Faliang%20Chang%20and%20Wenqian%20Wang%20and%20Penghui%20Hao%20and%20Yiming%20Huang%20and%20Zhiqiang%20Yang&entry.1292438233=%20%20Associating%20driver%20attention%20with%20driving%20scene%20across%20two%20fields%20of%20views%0A%28FOVs%29%20is%20a%20hard%20cross-domain%20perception%20problem%2C%20which%20requires%20comprehensive%0Aconsideration%20of%20cross-view%20mapping%2C%20dynamic%20driving%20scene%20analysis%2C%20and%20driver%0Astatus%20tracking.%20Previous%20methods%20typically%20focus%20on%20a%20single%20view%20or%20map%0Aattention%20to%20the%20scene%20via%20estimated%20gaze%2C%20failing%20to%20exploit%20the%20implicit%0Aconnection%20between%20them.%20Moreover%2C%20simple%20fusion%20modules%20are%20insufficient%20for%0Amodeling%20the%20complex%20relationships%20between%20the%20two%20views%2C%20making%20information%0Aintegration%20challenging.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20method%20for%0Aend-to-end%20scene-associated%20driver%20attention%20estimation%2C%20called%20EraW-Net.%20This%0Amethod%20enhances%20the%20most%20discriminative%20dynamic%20cues%2C%20refines%20feature%0Arepresentations%2C%20and%20facilitates%20semantically%20aligned%20cross-domain%20integration%0Athrough%20a%20W-shaped%20architecture%2C%20termed%20W-Net.%20Specifically%2C%20a%20Dynamic%20Adaptive%0AFilter%20Module%20%28DAF-Module%29%20is%20proposed%20to%20address%20the%20challenges%20of%20frequently%0Achanging%20driving%20environments%20by%20extracting%20vital%20regions.%20It%20suppresses%20the%0Aindiscriminately%20recorded%20dynamics%20and%20highlights%20crucial%20ones%20by%20innovative%0Ajoint%20frequency-spatial%20analysis%2C%20enhancing%20the%20model%27s%20ability%20to%20parse%0Acomplex%20dynamics.%20Additionally%2C%20to%20track%20driver%20states%20during%20non-fixed%20facial%0Aposes%2C%20we%20propose%20a%20Global%20Context%20Sharing%20Module%20%28GCS-Module%29%20to%20construct%0Arefined%20feature%20representations%20by%20capturing%20hierarchical%20features%20that%20adapt%0Ato%20various%20scales%20of%20head%20and%20eye%20movements.%20Finally%2C%20W-Net%20achieves%20systematic%0Across-view%20information%20integration%20through%20its%20%22Encoding-Independent%20Partial%0ADecoding-Fusion%20Decoding%22%20structure%2C%20addressing%20semantic%20misalignment%20in%0Aheterogeneous%20data%20integration.%20Experiments%20demonstrate%20that%20the%20proposed%0Amethod%20robustly%20and%20accurately%20estimates%20the%20mapping%20of%20driver%20attention%20in%0Ascene%20on%20large%20public%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08570v2&entry.124074799=Read"},
{"title": "Visual place recognition for aerial imagery: A survey", "author": "Ivan Moskalenko and Anastasiia Kornilova and Gonzalo Ferrer", "abstract": "  Aerial imagery and its direct application to visual localization is an\nessential problem for many Robotics and Computer Vision tasks. While Global\nNavigation Satellite Systems (GNSS) are the standard default solution for\nsolving the aerial localization problem, it is subject to a number of\nlimitations, such as, signal instability or solution unreliability that make\nthis option not so desirable. Consequently, visual geolocalization is emerging\nas a viable alternative. However, adapting Visual Place Recognition (VPR) task\nto aerial imagery presents significant challenges, including weather variations\nand repetitive patterns. Current VPR reviews largely neglect the specific\ncontext of aerial data. This paper introduces a methodology tailored for\nevaluating VPR techniques specifically in the domain of aerial imagery,\nproviding a comprehensive assessment of various methods and their performance.\nHowever, we not only compare various VPR methods, but also demonstrate the\nimportance of selecting appropriate zoom and overlap levels when constructing\nmap tiles to achieve maximum efficiency of VPR algorithms in the case of aerial\nimagery. The code is available on our GitHub repository --\nhttps://github.com/prime-slam/aero-vloc.\n", "link": "http://arxiv.org/abs/2406.00885v2", "date": "2024-10-31", "relevancy": 2.7055, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5838}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5197}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5197}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20place%20recognition%20for%20aerial%20imagery%3A%20A%20survey&body=Title%3A%20Visual%20place%20recognition%20for%20aerial%20imagery%3A%20A%20survey%0AAuthor%3A%20Ivan%20Moskalenko%20and%20Anastasiia%20Kornilova%20and%20Gonzalo%20Ferrer%0AAbstract%3A%20%20%20Aerial%20imagery%20and%20its%20direct%20application%20to%20visual%20localization%20is%20an%0Aessential%20problem%20for%20many%20Robotics%20and%20Computer%20Vision%20tasks.%20While%20Global%0ANavigation%20Satellite%20Systems%20%28GNSS%29%20are%20the%20standard%20default%20solution%20for%0Asolving%20the%20aerial%20localization%20problem%2C%20it%20is%20subject%20to%20a%20number%20of%0Alimitations%2C%20such%20as%2C%20signal%20instability%20or%20solution%20unreliability%20that%20make%0Athis%20option%20not%20so%20desirable.%20Consequently%2C%20visual%20geolocalization%20is%20emerging%0Aas%20a%20viable%20alternative.%20However%2C%20adapting%20Visual%20Place%20Recognition%20%28VPR%29%20task%0Ato%20aerial%20imagery%20presents%20significant%20challenges%2C%20including%20weather%20variations%0Aand%20repetitive%20patterns.%20Current%20VPR%20reviews%20largely%20neglect%20the%20specific%0Acontext%20of%20aerial%20data.%20This%20paper%20introduces%20a%20methodology%20tailored%20for%0Aevaluating%20VPR%20techniques%20specifically%20in%20the%20domain%20of%20aerial%20imagery%2C%0Aproviding%20a%20comprehensive%20assessment%20of%20various%20methods%20and%20their%20performance.%0AHowever%2C%20we%20not%20only%20compare%20various%20VPR%20methods%2C%20but%20also%20demonstrate%20the%0Aimportance%20of%20selecting%20appropriate%20zoom%20and%20overlap%20levels%20when%20constructing%0Amap%20tiles%20to%20achieve%20maximum%20efficiency%20of%20VPR%20algorithms%20in%20the%20case%20of%20aerial%0Aimagery.%20The%20code%20is%20available%20on%20our%20GitHub%20repository%20--%0Ahttps%3A//github.com/prime-slam/aero-vloc.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00885v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520place%2520recognition%2520for%2520aerial%2520imagery%253A%2520A%2520survey%26entry.906535625%3DIvan%2520Moskalenko%2520and%2520Anastasiia%2520Kornilova%2520and%2520Gonzalo%2520Ferrer%26entry.1292438233%3D%2520%2520Aerial%2520imagery%2520and%2520its%2520direct%2520application%2520to%2520visual%2520localization%2520is%2520an%250Aessential%2520problem%2520for%2520many%2520Robotics%2520and%2520Computer%2520Vision%2520tasks.%2520While%2520Global%250ANavigation%2520Satellite%2520Systems%2520%2528GNSS%2529%2520are%2520the%2520standard%2520default%2520solution%2520for%250Asolving%2520the%2520aerial%2520localization%2520problem%252C%2520it%2520is%2520subject%2520to%2520a%2520number%2520of%250Alimitations%252C%2520such%2520as%252C%2520signal%2520instability%2520or%2520solution%2520unreliability%2520that%2520make%250Athis%2520option%2520not%2520so%2520desirable.%2520Consequently%252C%2520visual%2520geolocalization%2520is%2520emerging%250Aas%2520a%2520viable%2520alternative.%2520However%252C%2520adapting%2520Visual%2520Place%2520Recognition%2520%2528VPR%2529%2520task%250Ato%2520aerial%2520imagery%2520presents%2520significant%2520challenges%252C%2520including%2520weather%2520variations%250Aand%2520repetitive%2520patterns.%2520Current%2520VPR%2520reviews%2520largely%2520neglect%2520the%2520specific%250Acontext%2520of%2520aerial%2520data.%2520This%2520paper%2520introduces%2520a%2520methodology%2520tailored%2520for%250Aevaluating%2520VPR%2520techniques%2520specifically%2520in%2520the%2520domain%2520of%2520aerial%2520imagery%252C%250Aproviding%2520a%2520comprehensive%2520assessment%2520of%2520various%2520methods%2520and%2520their%2520performance.%250AHowever%252C%2520we%2520not%2520only%2520compare%2520various%2520VPR%2520methods%252C%2520but%2520also%2520demonstrate%2520the%250Aimportance%2520of%2520selecting%2520appropriate%2520zoom%2520and%2520overlap%2520levels%2520when%2520constructing%250Amap%2520tiles%2520to%2520achieve%2520maximum%2520efficiency%2520of%2520VPR%2520algorithms%2520in%2520the%2520case%2520of%2520aerial%250Aimagery.%2520The%2520code%2520is%2520available%2520on%2520our%2520GitHub%2520repository%2520--%250Ahttps%253A//github.com/prime-slam/aero-vloc.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00885v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20place%20recognition%20for%20aerial%20imagery%3A%20A%20survey&entry.906535625=Ivan%20Moskalenko%20and%20Anastasiia%20Kornilova%20and%20Gonzalo%20Ferrer&entry.1292438233=%20%20Aerial%20imagery%20and%20its%20direct%20application%20to%20visual%20localization%20is%20an%0Aessential%20problem%20for%20many%20Robotics%20and%20Computer%20Vision%20tasks.%20While%20Global%0ANavigation%20Satellite%20Systems%20%28GNSS%29%20are%20the%20standard%20default%20solution%20for%0Asolving%20the%20aerial%20localization%20problem%2C%20it%20is%20subject%20to%20a%20number%20of%0Alimitations%2C%20such%20as%2C%20signal%20instability%20or%20solution%20unreliability%20that%20make%0Athis%20option%20not%20so%20desirable.%20Consequently%2C%20visual%20geolocalization%20is%20emerging%0Aas%20a%20viable%20alternative.%20However%2C%20adapting%20Visual%20Place%20Recognition%20%28VPR%29%20task%0Ato%20aerial%20imagery%20presents%20significant%20challenges%2C%20including%20weather%20variations%0Aand%20repetitive%20patterns.%20Current%20VPR%20reviews%20largely%20neglect%20the%20specific%0Acontext%20of%20aerial%20data.%20This%20paper%20introduces%20a%20methodology%20tailored%20for%0Aevaluating%20VPR%20techniques%20specifically%20in%20the%20domain%20of%20aerial%20imagery%2C%0Aproviding%20a%20comprehensive%20assessment%20of%20various%20methods%20and%20their%20performance.%0AHowever%2C%20we%20not%20only%20compare%20various%20VPR%20methods%2C%20but%20also%20demonstrate%20the%0Aimportance%20of%20selecting%20appropriate%20zoom%20and%20overlap%20levels%20when%20constructing%0Amap%20tiles%20to%20achieve%20maximum%20efficiency%20of%20VPR%20algorithms%20in%20the%20case%20of%20aerial%0Aimagery.%20The%20code%20is%20available%20on%20our%20GitHub%20repository%20--%0Ahttps%3A//github.com/prime-slam/aero-vloc.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00885v2&entry.124074799=Read"},
{"title": "Will LLMs Replace the Encoder-Only Models in Temporal Relation\n  Classification?", "author": "Gabriel Roccabruna and Massimo Rizzoli and Giuseppe Riccardi", "abstract": "  The automatic detection of temporal relations among events has been mainly\ninvestigated with encoder-only models such as RoBERTa. Large Language Models\n(LLM) have recently shown promising performance in temporal reasoning tasks\nsuch as temporal question answering. Nevertheless, recent studies have tested\nthe LLMs' performance in detecting temporal relations of closed-source models\nonly, limiting the interpretability of those results. In this work, we\ninvestigate LLMs' performance and decision process in the Temporal Relation\nClassification task. First, we assess the performance of seven open and\nclosed-sourced LLMs experimenting with in-context learning and lightweight\nfine-tuning approaches. Results show that LLMs with in-context learning\nsignificantly underperform smaller encoder-only models based on RoBERTa. Then,\nwe delve into the possible reasons for this gap by applying explainable\nmethods. The outcome suggests a limitation of LLMs in this task due to their\nautoregressive nature, which causes them to focus only on the last part of the\nsequence. Additionally, we evaluate the word embeddings of these two models to\nbetter understand their pre-training differences. The code and the fine-tuned\nmodels can be found respectively on GitHub.\n", "link": "http://arxiv.org/abs/2410.10476v2", "date": "2024-10-31", "relevancy": 2.6801, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Will%20LLMs%20Replace%20the%20Encoder-Only%20Models%20in%20Temporal%20Relation%0A%20%20Classification%3F&body=Title%3A%20Will%20LLMs%20Replace%20the%20Encoder-Only%20Models%20in%20Temporal%20Relation%0A%20%20Classification%3F%0AAuthor%3A%20Gabriel%20Roccabruna%20and%20Massimo%20Rizzoli%20and%20Giuseppe%20Riccardi%0AAbstract%3A%20%20%20The%20automatic%20detection%20of%20temporal%20relations%20among%20events%20has%20been%20mainly%0Ainvestigated%20with%20encoder-only%20models%20such%20as%20RoBERTa.%20Large%20Language%20Models%0A%28LLM%29%20have%20recently%20shown%20promising%20performance%20in%20temporal%20reasoning%20tasks%0Asuch%20as%20temporal%20question%20answering.%20Nevertheless%2C%20recent%20studies%20have%20tested%0Athe%20LLMs%27%20performance%20in%20detecting%20temporal%20relations%20of%20closed-source%20models%0Aonly%2C%20limiting%20the%20interpretability%20of%20those%20results.%20In%20this%20work%2C%20we%0Ainvestigate%20LLMs%27%20performance%20and%20decision%20process%20in%20the%20Temporal%20Relation%0AClassification%20task.%20First%2C%20we%20assess%20the%20performance%20of%20seven%20open%20and%0Aclosed-sourced%20LLMs%20experimenting%20with%20in-context%20learning%20and%20lightweight%0Afine-tuning%20approaches.%20Results%20show%20that%20LLMs%20with%20in-context%20learning%0Asignificantly%20underperform%20smaller%20encoder-only%20models%20based%20on%20RoBERTa.%20Then%2C%0Awe%20delve%20into%20the%20possible%20reasons%20for%20this%20gap%20by%20applying%20explainable%0Amethods.%20The%20outcome%20suggests%20a%20limitation%20of%20LLMs%20in%20this%20task%20due%20to%20their%0Aautoregressive%20nature%2C%20which%20causes%20them%20to%20focus%20only%20on%20the%20last%20part%20of%20the%0Asequence.%20Additionally%2C%20we%20evaluate%20the%20word%20embeddings%20of%20these%20two%20models%20to%0Abetter%20understand%20their%20pre-training%20differences.%20The%20code%20and%20the%20fine-tuned%0Amodels%20can%20be%20found%20respectively%20on%20GitHub.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.10476v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWill%2520LLMs%2520Replace%2520the%2520Encoder-Only%2520Models%2520in%2520Temporal%2520Relation%250A%2520%2520Classification%253F%26entry.906535625%3DGabriel%2520Roccabruna%2520and%2520Massimo%2520Rizzoli%2520and%2520Giuseppe%2520Riccardi%26entry.1292438233%3D%2520%2520The%2520automatic%2520detection%2520of%2520temporal%2520relations%2520among%2520events%2520has%2520been%2520mainly%250Ainvestigated%2520with%2520encoder-only%2520models%2520such%2520as%2520RoBERTa.%2520Large%2520Language%2520Models%250A%2528LLM%2529%2520have%2520recently%2520shown%2520promising%2520performance%2520in%2520temporal%2520reasoning%2520tasks%250Asuch%2520as%2520temporal%2520question%2520answering.%2520Nevertheless%252C%2520recent%2520studies%2520have%2520tested%250Athe%2520LLMs%2527%2520performance%2520in%2520detecting%2520temporal%2520relations%2520of%2520closed-source%2520models%250Aonly%252C%2520limiting%2520the%2520interpretability%2520of%2520those%2520results.%2520In%2520this%2520work%252C%2520we%250Ainvestigate%2520LLMs%2527%2520performance%2520and%2520decision%2520process%2520in%2520the%2520Temporal%2520Relation%250AClassification%2520task.%2520First%252C%2520we%2520assess%2520the%2520performance%2520of%2520seven%2520open%2520and%250Aclosed-sourced%2520LLMs%2520experimenting%2520with%2520in-context%2520learning%2520and%2520lightweight%250Afine-tuning%2520approaches.%2520Results%2520show%2520that%2520LLMs%2520with%2520in-context%2520learning%250Asignificantly%2520underperform%2520smaller%2520encoder-only%2520models%2520based%2520on%2520RoBERTa.%2520Then%252C%250Awe%2520delve%2520into%2520the%2520possible%2520reasons%2520for%2520this%2520gap%2520by%2520applying%2520explainable%250Amethods.%2520The%2520outcome%2520suggests%2520a%2520limitation%2520of%2520LLMs%2520in%2520this%2520task%2520due%2520to%2520their%250Aautoregressive%2520nature%252C%2520which%2520causes%2520them%2520to%2520focus%2520only%2520on%2520the%2520last%2520part%2520of%2520the%250Asequence.%2520Additionally%252C%2520we%2520evaluate%2520the%2520word%2520embeddings%2520of%2520these%2520two%2520models%2520to%250Abetter%2520understand%2520their%2520pre-training%2520differences.%2520The%2520code%2520and%2520the%2520fine-tuned%250Amodels%2520can%2520be%2520found%2520respectively%2520on%2520GitHub.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.10476v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Will%20LLMs%20Replace%20the%20Encoder-Only%20Models%20in%20Temporal%20Relation%0A%20%20Classification%3F&entry.906535625=Gabriel%20Roccabruna%20and%20Massimo%20Rizzoli%20and%20Giuseppe%20Riccardi&entry.1292438233=%20%20The%20automatic%20detection%20of%20temporal%20relations%20among%20events%20has%20been%20mainly%0Ainvestigated%20with%20encoder-only%20models%20such%20as%20RoBERTa.%20Large%20Language%20Models%0A%28LLM%29%20have%20recently%20shown%20promising%20performance%20in%20temporal%20reasoning%20tasks%0Asuch%20as%20temporal%20question%20answering.%20Nevertheless%2C%20recent%20studies%20have%20tested%0Athe%20LLMs%27%20performance%20in%20detecting%20temporal%20relations%20of%20closed-source%20models%0Aonly%2C%20limiting%20the%20interpretability%20of%20those%20results.%20In%20this%20work%2C%20we%0Ainvestigate%20LLMs%27%20performance%20and%20decision%20process%20in%20the%20Temporal%20Relation%0AClassification%20task.%20First%2C%20we%20assess%20the%20performance%20of%20seven%20open%20and%0Aclosed-sourced%20LLMs%20experimenting%20with%20in-context%20learning%20and%20lightweight%0Afine-tuning%20approaches.%20Results%20show%20that%20LLMs%20with%20in-context%20learning%0Asignificantly%20underperform%20smaller%20encoder-only%20models%20based%20on%20RoBERTa.%20Then%2C%0Awe%20delve%20into%20the%20possible%20reasons%20for%20this%20gap%20by%20applying%20explainable%0Amethods.%20The%20outcome%20suggests%20a%20limitation%20of%20LLMs%20in%20this%20task%20due%20to%20their%0Aautoregressive%20nature%2C%20which%20causes%20them%20to%20focus%20only%20on%20the%20last%20part%20of%20the%0Asequence.%20Additionally%2C%20we%20evaluate%20the%20word%20embeddings%20of%20these%20two%20models%20to%0Abetter%20understand%20their%20pre-training%20differences.%20The%20code%20and%20the%20fine-tuned%0Amodels%20can%20be%20found%20respectively%20on%20GitHub.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.10476v2&entry.124074799=Read"},
{"title": "Exploring Vision Language Models for Facial Attribute Recognition:\n  Emotion, Race, Gender, and Age", "author": "Nouar AlDahoul and Myles Joshua Toledo Tan and Harishwar Reddy Kasireddy and Yasir Zaki", "abstract": "  Technologies for recognizing facial attributes like race, gender, age, and\nemotion have several applications, such as surveillance, advertising content,\nsentiment analysis, and the study of demographic trends and social behaviors.\nAnalyzing demographic characteristics based on images and analyzing facial\nexpressions have several challenges due to the complexity of humans' facial\nattributes. Traditional approaches have employed CNNs and various other deep\nlearning techniques, trained on extensive collections of labeled images. While\nthese methods demonstrated effective performance, there remains potential for\nfurther enhancements. In this paper, we propose to utilize vision language\nmodels (VLMs) such as generative pre-trained transformer (GPT), GEMINI, large\nlanguage and vision assistant (LLAVA), PaliGemma, and Microsoft Florence2 to\nrecognize facial attributes such as race, gender, age, and emotion from images\nwith human faces. Various datasets like FairFace, AffectNet, and UTKFace have\nbeen utilized to evaluate the solutions. The results show that VLMs are\ncompetitive if not superior to traditional techniques. Additionally, we propose\n\"FaceScanPaliGemma\"--a fine-tuned PaliGemma model--for race, gender, age, and\nemotion recognition. The results show an accuracy of 81.1%, 95.8%, 80%, and\n59.4% for race, gender, age group, and emotion classification, respectively,\noutperforming pre-trained version of PaliGemma, other VLMs, and SotA methods.\nFinally, we propose \"FaceScanGPT\", which is a GPT-4o model to recognize the\nabove attributes when several individuals are present in the image using a\nprompt engineered for a person with specific facial and/or physical attributes.\nThe results underscore the superior multitasking capability of FaceScanGPT to\ndetect the individual's attributes like hair cut, clothing color, postures,\netc., using only a prompt to drive the detection and recognition tasks.\n", "link": "http://arxiv.org/abs/2410.24148v1", "date": "2024-10-31", "relevancy": 2.6686, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5438}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5287}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Vision%20Language%20Models%20for%20Facial%20Attribute%20Recognition%3A%0A%20%20Emotion%2C%20Race%2C%20Gender%2C%20and%20Age&body=Title%3A%20Exploring%20Vision%20Language%20Models%20for%20Facial%20Attribute%20Recognition%3A%0A%20%20Emotion%2C%20Race%2C%20Gender%2C%20and%20Age%0AAuthor%3A%20Nouar%20AlDahoul%20and%20Myles%20Joshua%20Toledo%20Tan%20and%20Harishwar%20Reddy%20Kasireddy%20and%20Yasir%20Zaki%0AAbstract%3A%20%20%20Technologies%20for%20recognizing%20facial%20attributes%20like%20race%2C%20gender%2C%20age%2C%20and%0Aemotion%20have%20several%20applications%2C%20such%20as%20surveillance%2C%20advertising%20content%2C%0Asentiment%20analysis%2C%20and%20the%20study%20of%20demographic%20trends%20and%20social%20behaviors.%0AAnalyzing%20demographic%20characteristics%20based%20on%20images%20and%20analyzing%20facial%0Aexpressions%20have%20several%20challenges%20due%20to%20the%20complexity%20of%20humans%27%20facial%0Aattributes.%20Traditional%20approaches%20have%20employed%20CNNs%20and%20various%20other%20deep%0Alearning%20techniques%2C%20trained%20on%20extensive%20collections%20of%20labeled%20images.%20While%0Athese%20methods%20demonstrated%20effective%20performance%2C%20there%20remains%20potential%20for%0Afurther%20enhancements.%20In%20this%20paper%2C%20we%20propose%20to%20utilize%20vision%20language%0Amodels%20%28VLMs%29%20such%20as%20generative%20pre-trained%20transformer%20%28GPT%29%2C%20GEMINI%2C%20large%0Alanguage%20and%20vision%20assistant%20%28LLAVA%29%2C%20PaliGemma%2C%20and%20Microsoft%20Florence2%20to%0Arecognize%20facial%20attributes%20such%20as%20race%2C%20gender%2C%20age%2C%20and%20emotion%20from%20images%0Awith%20human%20faces.%20Various%20datasets%20like%20FairFace%2C%20AffectNet%2C%20and%20UTKFace%20have%0Abeen%20utilized%20to%20evaluate%20the%20solutions.%20The%20results%20show%20that%20VLMs%20are%0Acompetitive%20if%20not%20superior%20to%20traditional%20techniques.%20Additionally%2C%20we%20propose%0A%22FaceScanPaliGemma%22--a%20fine-tuned%20PaliGemma%20model--for%20race%2C%20gender%2C%20age%2C%20and%0Aemotion%20recognition.%20The%20results%20show%20an%20accuracy%20of%2081.1%25%2C%2095.8%25%2C%2080%25%2C%20and%0A59.4%25%20for%20race%2C%20gender%2C%20age%20group%2C%20and%20emotion%20classification%2C%20respectively%2C%0Aoutperforming%20pre-trained%20version%20of%20PaliGemma%2C%20other%20VLMs%2C%20and%20SotA%20methods.%0AFinally%2C%20we%20propose%20%22FaceScanGPT%22%2C%20which%20is%20a%20GPT-4o%20model%20to%20recognize%20the%0Aabove%20attributes%20when%20several%20individuals%20are%20present%20in%20the%20image%20using%20a%0Aprompt%20engineered%20for%20a%20person%20with%20specific%20facial%20and/or%20physical%20attributes.%0AThe%20results%20underscore%20the%20superior%20multitasking%20capability%20of%20FaceScanGPT%20to%0Adetect%20the%20individual%27s%20attributes%20like%20hair%20cut%2C%20clothing%20color%2C%20postures%2C%0Aetc.%2C%20using%20only%20a%20prompt%20to%20drive%20the%20detection%20and%20recognition%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24148v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Vision%2520Language%2520Models%2520for%2520Facial%2520Attribute%2520Recognition%253A%250A%2520%2520Emotion%252C%2520Race%252C%2520Gender%252C%2520and%2520Age%26entry.906535625%3DNouar%2520AlDahoul%2520and%2520Myles%2520Joshua%2520Toledo%2520Tan%2520and%2520Harishwar%2520Reddy%2520Kasireddy%2520and%2520Yasir%2520Zaki%26entry.1292438233%3D%2520%2520Technologies%2520for%2520recognizing%2520facial%2520attributes%2520like%2520race%252C%2520gender%252C%2520age%252C%2520and%250Aemotion%2520have%2520several%2520applications%252C%2520such%2520as%2520surveillance%252C%2520advertising%2520content%252C%250Asentiment%2520analysis%252C%2520and%2520the%2520study%2520of%2520demographic%2520trends%2520and%2520social%2520behaviors.%250AAnalyzing%2520demographic%2520characteristics%2520based%2520on%2520images%2520and%2520analyzing%2520facial%250Aexpressions%2520have%2520several%2520challenges%2520due%2520to%2520the%2520complexity%2520of%2520humans%2527%2520facial%250Aattributes.%2520Traditional%2520approaches%2520have%2520employed%2520CNNs%2520and%2520various%2520other%2520deep%250Alearning%2520techniques%252C%2520trained%2520on%2520extensive%2520collections%2520of%2520labeled%2520images.%2520While%250Athese%2520methods%2520demonstrated%2520effective%2520performance%252C%2520there%2520remains%2520potential%2520for%250Afurther%2520enhancements.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520utilize%2520vision%2520language%250Amodels%2520%2528VLMs%2529%2520such%2520as%2520generative%2520pre-trained%2520transformer%2520%2528GPT%2529%252C%2520GEMINI%252C%2520large%250Alanguage%2520and%2520vision%2520assistant%2520%2528LLAVA%2529%252C%2520PaliGemma%252C%2520and%2520Microsoft%2520Florence2%2520to%250Arecognize%2520facial%2520attributes%2520such%2520as%2520race%252C%2520gender%252C%2520age%252C%2520and%2520emotion%2520from%2520images%250Awith%2520human%2520faces.%2520Various%2520datasets%2520like%2520FairFace%252C%2520AffectNet%252C%2520and%2520UTKFace%2520have%250Abeen%2520utilized%2520to%2520evaluate%2520the%2520solutions.%2520The%2520results%2520show%2520that%2520VLMs%2520are%250Acompetitive%2520if%2520not%2520superior%2520to%2520traditional%2520techniques.%2520Additionally%252C%2520we%2520propose%250A%2522FaceScanPaliGemma%2522--a%2520fine-tuned%2520PaliGemma%2520model--for%2520race%252C%2520gender%252C%2520age%252C%2520and%250Aemotion%2520recognition.%2520The%2520results%2520show%2520an%2520accuracy%2520of%252081.1%2525%252C%252095.8%2525%252C%252080%2525%252C%2520and%250A59.4%2525%2520for%2520race%252C%2520gender%252C%2520age%2520group%252C%2520and%2520emotion%2520classification%252C%2520respectively%252C%250Aoutperforming%2520pre-trained%2520version%2520of%2520PaliGemma%252C%2520other%2520VLMs%252C%2520and%2520SotA%2520methods.%250AFinally%252C%2520we%2520propose%2520%2522FaceScanGPT%2522%252C%2520which%2520is%2520a%2520GPT-4o%2520model%2520to%2520recognize%2520the%250Aabove%2520attributes%2520when%2520several%2520individuals%2520are%2520present%2520in%2520the%2520image%2520using%2520a%250Aprompt%2520engineered%2520for%2520a%2520person%2520with%2520specific%2520facial%2520and/or%2520physical%2520attributes.%250AThe%2520results%2520underscore%2520the%2520superior%2520multitasking%2520capability%2520of%2520FaceScanGPT%2520to%250Adetect%2520the%2520individual%2527s%2520attributes%2520like%2520hair%2520cut%252C%2520clothing%2520color%252C%2520postures%252C%250Aetc.%252C%2520using%2520only%2520a%2520prompt%2520to%2520drive%2520the%2520detection%2520and%2520recognition%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24148v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Vision%20Language%20Models%20for%20Facial%20Attribute%20Recognition%3A%0A%20%20Emotion%2C%20Race%2C%20Gender%2C%20and%20Age&entry.906535625=Nouar%20AlDahoul%20and%20Myles%20Joshua%20Toledo%20Tan%20and%20Harishwar%20Reddy%20Kasireddy%20and%20Yasir%20Zaki&entry.1292438233=%20%20Technologies%20for%20recognizing%20facial%20attributes%20like%20race%2C%20gender%2C%20age%2C%20and%0Aemotion%20have%20several%20applications%2C%20such%20as%20surveillance%2C%20advertising%20content%2C%0Asentiment%20analysis%2C%20and%20the%20study%20of%20demographic%20trends%20and%20social%20behaviors.%0AAnalyzing%20demographic%20characteristics%20based%20on%20images%20and%20analyzing%20facial%0Aexpressions%20have%20several%20challenges%20due%20to%20the%20complexity%20of%20humans%27%20facial%0Aattributes.%20Traditional%20approaches%20have%20employed%20CNNs%20and%20various%20other%20deep%0Alearning%20techniques%2C%20trained%20on%20extensive%20collections%20of%20labeled%20images.%20While%0Athese%20methods%20demonstrated%20effective%20performance%2C%20there%20remains%20potential%20for%0Afurther%20enhancements.%20In%20this%20paper%2C%20we%20propose%20to%20utilize%20vision%20language%0Amodels%20%28VLMs%29%20such%20as%20generative%20pre-trained%20transformer%20%28GPT%29%2C%20GEMINI%2C%20large%0Alanguage%20and%20vision%20assistant%20%28LLAVA%29%2C%20PaliGemma%2C%20and%20Microsoft%20Florence2%20to%0Arecognize%20facial%20attributes%20such%20as%20race%2C%20gender%2C%20age%2C%20and%20emotion%20from%20images%0Awith%20human%20faces.%20Various%20datasets%20like%20FairFace%2C%20AffectNet%2C%20and%20UTKFace%20have%0Abeen%20utilized%20to%20evaluate%20the%20solutions.%20The%20results%20show%20that%20VLMs%20are%0Acompetitive%20if%20not%20superior%20to%20traditional%20techniques.%20Additionally%2C%20we%20propose%0A%22FaceScanPaliGemma%22--a%20fine-tuned%20PaliGemma%20model--for%20race%2C%20gender%2C%20age%2C%20and%0Aemotion%20recognition.%20The%20results%20show%20an%20accuracy%20of%2081.1%25%2C%2095.8%25%2C%2080%25%2C%20and%0A59.4%25%20for%20race%2C%20gender%2C%20age%20group%2C%20and%20emotion%20classification%2C%20respectively%2C%0Aoutperforming%20pre-trained%20version%20of%20PaliGemma%2C%20other%20VLMs%2C%20and%20SotA%20methods.%0AFinally%2C%20we%20propose%20%22FaceScanGPT%22%2C%20which%20is%20a%20GPT-4o%20model%20to%20recognize%20the%0Aabove%20attributes%20when%20several%20individuals%20are%20present%20in%20the%20image%20using%20a%0Aprompt%20engineered%20for%20a%20person%20with%20specific%20facial%20and/or%20physical%20attributes.%0AThe%20results%20underscore%20the%20superior%20multitasking%20capability%20of%20FaceScanGPT%20to%0Adetect%20the%20individual%27s%20attributes%20like%20hair%20cut%2C%20clothing%20color%2C%20postures%2C%0Aetc.%2C%20using%20only%20a%20prompt%20to%20drive%20the%20detection%20and%20recognition%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24148v1&entry.124074799=Read"},
{"title": "AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning", "author": "Shirley Wu and Shiyu Zhao and Qian Huang and Kexin Huang and Michihiro Yasunaga and Kaidi Cao and Vassilis N. Ioannidis and Karthik Subbian and Jure Leskovec and James Zou", "abstract": "  Large language model (LLM) agents have demonstrated impressive capabilities\nin utilizing external tools and knowledge to boost accuracy and reduce\nhallucinations. However, developing prompting techniques that enable LLM agents\nto effectively use these tools and knowledge remains a heuristic and\nlabor-intensive task. Here, we introduce AvaTaR, a novel and automated\nframework that optimizes an LLM agent to effectively leverage provided tools,\nimproving performance on a given task. During optimization, we design a\ncomparator module to iteratively deliver insightful and comprehensive prompts\nto the LLM agent by contrastively reasoning between positive and negative\nexamples sampled from training data. We demonstrate AvaTaR on four complex\nmultimodal retrieval datasets featuring textual, visual, and relational\ninformation, and three general question-answering (QA) datasets. We find AvaTaR\nconsistently outperforms state-of-the-art approaches across all seven tasks,\nexhibiting strong generalization ability when applied to novel cases and\nachieving an average relative improvement of 14% on the Hit@1 metric for the\nretrieval datasets and 13% for the QA datasets. Code and dataset are available\nat https://github.com/zou-group/avatar.\n", "link": "http://arxiv.org/abs/2406.11200v3", "date": "2024-10-31", "relevancy": 2.6676, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5574}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5216}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AvaTaR%3A%20Optimizing%20LLM%20Agents%20for%20Tool%20Usage%20via%20Contrastive%20Reasoning&body=Title%3A%20AvaTaR%3A%20Optimizing%20LLM%20Agents%20for%20Tool%20Usage%20via%20Contrastive%20Reasoning%0AAuthor%3A%20Shirley%20Wu%20and%20Shiyu%20Zhao%20and%20Qian%20Huang%20and%20Kexin%20Huang%20and%20Michihiro%20Yasunaga%20and%20Kaidi%20Cao%20and%20Vassilis%20N.%20Ioannidis%20and%20Karthik%20Subbian%20and%20Jure%20Leskovec%20and%20James%20Zou%0AAbstract%3A%20%20%20Large%20language%20model%20%28LLM%29%20agents%20have%20demonstrated%20impressive%20capabilities%0Ain%20utilizing%20external%20tools%20and%20knowledge%20to%20boost%20accuracy%20and%20reduce%0Ahallucinations.%20However%2C%20developing%20prompting%20techniques%20that%20enable%20LLM%20agents%0Ato%20effectively%20use%20these%20tools%20and%20knowledge%20remains%20a%20heuristic%20and%0Alabor-intensive%20task.%20Here%2C%20we%20introduce%20AvaTaR%2C%20a%20novel%20and%20automated%0Aframework%20that%20optimizes%20an%20LLM%20agent%20to%20effectively%20leverage%20provided%20tools%2C%0Aimproving%20performance%20on%20a%20given%20task.%20During%20optimization%2C%20we%20design%20a%0Acomparator%20module%20to%20iteratively%20deliver%20insightful%20and%20comprehensive%20prompts%0Ato%20the%20LLM%20agent%20by%20contrastively%20reasoning%20between%20positive%20and%20negative%0Aexamples%20sampled%20from%20training%20data.%20We%20demonstrate%20AvaTaR%20on%20four%20complex%0Amultimodal%20retrieval%20datasets%20featuring%20textual%2C%20visual%2C%20and%20relational%0Ainformation%2C%20and%20three%20general%20question-answering%20%28QA%29%20datasets.%20We%20find%20AvaTaR%0Aconsistently%20outperforms%20state-of-the-art%20approaches%20across%20all%20seven%20tasks%2C%0Aexhibiting%20strong%20generalization%20ability%20when%20applied%20to%20novel%20cases%20and%0Aachieving%20an%20average%20relative%20improvement%20of%2014%25%20on%20the%20Hit%401%20metric%20for%20the%0Aretrieval%20datasets%20and%2013%25%20for%20the%20QA%20datasets.%20Code%20and%20dataset%20are%20available%0Aat%20https%3A//github.com/zou-group/avatar.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11200v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAvaTaR%253A%2520Optimizing%2520LLM%2520Agents%2520for%2520Tool%2520Usage%2520via%2520Contrastive%2520Reasoning%26entry.906535625%3DShirley%2520Wu%2520and%2520Shiyu%2520Zhao%2520and%2520Qian%2520Huang%2520and%2520Kexin%2520Huang%2520and%2520Michihiro%2520Yasunaga%2520and%2520Kaidi%2520Cao%2520and%2520Vassilis%2520N.%2520Ioannidis%2520and%2520Karthik%2520Subbian%2520and%2520Jure%2520Leskovec%2520and%2520James%2520Zou%26entry.1292438233%3D%2520%2520Large%2520language%2520model%2520%2528LLM%2529%2520agents%2520have%2520demonstrated%2520impressive%2520capabilities%250Ain%2520utilizing%2520external%2520tools%2520and%2520knowledge%2520to%2520boost%2520accuracy%2520and%2520reduce%250Ahallucinations.%2520However%252C%2520developing%2520prompting%2520techniques%2520that%2520enable%2520LLM%2520agents%250Ato%2520effectively%2520use%2520these%2520tools%2520and%2520knowledge%2520remains%2520a%2520heuristic%2520and%250Alabor-intensive%2520task.%2520Here%252C%2520we%2520introduce%2520AvaTaR%252C%2520a%2520novel%2520and%2520automated%250Aframework%2520that%2520optimizes%2520an%2520LLM%2520agent%2520to%2520effectively%2520leverage%2520provided%2520tools%252C%250Aimproving%2520performance%2520on%2520a%2520given%2520task.%2520During%2520optimization%252C%2520we%2520design%2520a%250Acomparator%2520module%2520to%2520iteratively%2520deliver%2520insightful%2520and%2520comprehensive%2520prompts%250Ato%2520the%2520LLM%2520agent%2520by%2520contrastively%2520reasoning%2520between%2520positive%2520and%2520negative%250Aexamples%2520sampled%2520from%2520training%2520data.%2520We%2520demonstrate%2520AvaTaR%2520on%2520four%2520complex%250Amultimodal%2520retrieval%2520datasets%2520featuring%2520textual%252C%2520visual%252C%2520and%2520relational%250Ainformation%252C%2520and%2520three%2520general%2520question-answering%2520%2528QA%2529%2520datasets.%2520We%2520find%2520AvaTaR%250Aconsistently%2520outperforms%2520state-of-the-art%2520approaches%2520across%2520all%2520seven%2520tasks%252C%250Aexhibiting%2520strong%2520generalization%2520ability%2520when%2520applied%2520to%2520novel%2520cases%2520and%250Aachieving%2520an%2520average%2520relative%2520improvement%2520of%252014%2525%2520on%2520the%2520Hit%25401%2520metric%2520for%2520the%250Aretrieval%2520datasets%2520and%252013%2525%2520for%2520the%2520QA%2520datasets.%2520Code%2520and%2520dataset%2520are%2520available%250Aat%2520https%253A//github.com/zou-group/avatar.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11200v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AvaTaR%3A%20Optimizing%20LLM%20Agents%20for%20Tool%20Usage%20via%20Contrastive%20Reasoning&entry.906535625=Shirley%20Wu%20and%20Shiyu%20Zhao%20and%20Qian%20Huang%20and%20Kexin%20Huang%20and%20Michihiro%20Yasunaga%20and%20Kaidi%20Cao%20and%20Vassilis%20N.%20Ioannidis%20and%20Karthik%20Subbian%20and%20Jure%20Leskovec%20and%20James%20Zou&entry.1292438233=%20%20Large%20language%20model%20%28LLM%29%20agents%20have%20demonstrated%20impressive%20capabilities%0Ain%20utilizing%20external%20tools%20and%20knowledge%20to%20boost%20accuracy%20and%20reduce%0Ahallucinations.%20However%2C%20developing%20prompting%20techniques%20that%20enable%20LLM%20agents%0Ato%20effectively%20use%20these%20tools%20and%20knowledge%20remains%20a%20heuristic%20and%0Alabor-intensive%20task.%20Here%2C%20we%20introduce%20AvaTaR%2C%20a%20novel%20and%20automated%0Aframework%20that%20optimizes%20an%20LLM%20agent%20to%20effectively%20leverage%20provided%20tools%2C%0Aimproving%20performance%20on%20a%20given%20task.%20During%20optimization%2C%20we%20design%20a%0Acomparator%20module%20to%20iteratively%20deliver%20insightful%20and%20comprehensive%20prompts%0Ato%20the%20LLM%20agent%20by%20contrastively%20reasoning%20between%20positive%20and%20negative%0Aexamples%20sampled%20from%20training%20data.%20We%20demonstrate%20AvaTaR%20on%20four%20complex%0Amultimodal%20retrieval%20datasets%20featuring%20textual%2C%20visual%2C%20and%20relational%0Ainformation%2C%20and%20three%20general%20question-answering%20%28QA%29%20datasets.%20We%20find%20AvaTaR%0Aconsistently%20outperforms%20state-of-the-art%20approaches%20across%20all%20seven%20tasks%2C%0Aexhibiting%20strong%20generalization%20ability%20when%20applied%20to%20novel%20cases%20and%0Aachieving%20an%20average%20relative%20improvement%20of%2014%25%20on%20the%20Hit%401%20metric%20for%20the%0Aretrieval%20datasets%20and%2013%25%20for%20the%20QA%20datasets.%20Code%20and%20dataset%20are%20available%0Aat%20https%3A//github.com/zou-group/avatar.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11200v3&entry.124074799=Read"},
{"title": "Human Action Recognition (HAR) Using Skeleton-based Quantum Spatial\n  Temporal Relative Transformer Network: ST-RTR", "author": "Faisal Mehmood and Enqing Chen and Touqeer Abbas and Samah M. Alzanin", "abstract": "  Quantum Human Action Recognition (HAR) is an interesting research area in\nhuman-computer interaction used to monitor the activities of elderly and\ndisabled individuals affected by physical and mental health. In the recent era,\nskeleton-based HAR has received much attention because skeleton data has shown\nthat it can handle changes in striking, body size, camera views, and complex\nbackgrounds. One key characteristic of ST-GCN is automatically learning spatial\nand temporal patterns from skeleton sequences. It has some limitations, as this\nmethod only works for short-range correlation due to its limited receptive\nfield. Consequently, understanding human action requires long-range\ninterconnection. To address this issue, we developed a quantum spatial-temporal\nrelative transformer ST-RTR model. The ST-RTR includes joint and relay nodes,\nwhich allow efficient communication and data transmission within the network.\nThese nodes help to break the inherent spatial and temporal skeleton\ntopologies, which enables the model to understand long-range human action\nbetter. Furthermore, we combine quantum ST-RTR with a fusion model for further\nperformance improvements. To assess the performance of the quantum ST-RTR\nmethod, we conducted experiments on three skeleton-based HAR benchmarks: NTU\nRGB+D 60, NTU RGB+D 120, and UAV-Human. It boosted CS and CV by 2.11 % and\n1.45% on NTU RGB+D 60, 1.25% and 1.05% on NTU RGB+D 120. On UAV-Human datasets,\naccuracy improved by 2.54%. The experimental outcomes explain that the proposed\nST-RTR model significantly improves action recognition associated with the\nstandard ST-GCN method.\n", "link": "http://arxiv.org/abs/2410.23806v1", "date": "2024-10-31", "relevancy": 2.6424, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5509}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5237}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20Action%20Recognition%20%28HAR%29%20Using%20Skeleton-based%20Quantum%20Spatial%0A%20%20Temporal%20Relative%20Transformer%20Network%3A%20ST-RTR&body=Title%3A%20Human%20Action%20Recognition%20%28HAR%29%20Using%20Skeleton-based%20Quantum%20Spatial%0A%20%20Temporal%20Relative%20Transformer%20Network%3A%20ST-RTR%0AAuthor%3A%20Faisal%20Mehmood%20and%20Enqing%20Chen%20and%20Touqeer%20Abbas%20and%20Samah%20M.%20Alzanin%0AAbstract%3A%20%20%20Quantum%20Human%20Action%20Recognition%20%28HAR%29%20is%20an%20interesting%20research%20area%20in%0Ahuman-computer%20interaction%20used%20to%20monitor%20the%20activities%20of%20elderly%20and%0Adisabled%20individuals%20affected%20by%20physical%20and%20mental%20health.%20In%20the%20recent%20era%2C%0Askeleton-based%20HAR%20has%20received%20much%20attention%20because%20skeleton%20data%20has%20shown%0Athat%20it%20can%20handle%20changes%20in%20striking%2C%20body%20size%2C%20camera%20views%2C%20and%20complex%0Abackgrounds.%20One%20key%20characteristic%20of%20ST-GCN%20is%20automatically%20learning%20spatial%0Aand%20temporal%20patterns%20from%20skeleton%20sequences.%20It%20has%20some%20limitations%2C%20as%20this%0Amethod%20only%20works%20for%20short-range%20correlation%20due%20to%20its%20limited%20receptive%0Afield.%20Consequently%2C%20understanding%20human%20action%20requires%20long-range%0Ainterconnection.%20To%20address%20this%20issue%2C%20we%20developed%20a%20quantum%20spatial-temporal%0Arelative%20transformer%20ST-RTR%20model.%20The%20ST-RTR%20includes%20joint%20and%20relay%20nodes%2C%0Awhich%20allow%20efficient%20communication%20and%20data%20transmission%20within%20the%20network.%0AThese%20nodes%20help%20to%20break%20the%20inherent%20spatial%20and%20temporal%20skeleton%0Atopologies%2C%20which%20enables%20the%20model%20to%20understand%20long-range%20human%20action%0Abetter.%20Furthermore%2C%20we%20combine%20quantum%20ST-RTR%20with%20a%20fusion%20model%20for%20further%0Aperformance%20improvements.%20To%20assess%20the%20performance%20of%20the%20quantum%20ST-RTR%0Amethod%2C%20we%20conducted%20experiments%20on%20three%20skeleton-based%20HAR%20benchmarks%3A%20NTU%0ARGB%2BD%2060%2C%20NTU%20RGB%2BD%20120%2C%20and%20UAV-Human.%20It%20boosted%20CS%20and%20CV%20by%202.11%20%25%20and%0A1.45%25%20on%20NTU%20RGB%2BD%2060%2C%201.25%25%20and%201.05%25%20on%20NTU%20RGB%2BD%20120.%20On%20UAV-Human%20datasets%2C%0Aaccuracy%20improved%20by%202.54%25.%20The%20experimental%20outcomes%20explain%20that%20the%20proposed%0AST-RTR%20model%20significantly%20improves%20action%20recognition%20associated%20with%20the%0Astandard%20ST-GCN%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520Action%2520Recognition%2520%2528HAR%2529%2520Using%2520Skeleton-based%2520Quantum%2520Spatial%250A%2520%2520Temporal%2520Relative%2520Transformer%2520Network%253A%2520ST-RTR%26entry.906535625%3DFaisal%2520Mehmood%2520and%2520Enqing%2520Chen%2520and%2520Touqeer%2520Abbas%2520and%2520Samah%2520M.%2520Alzanin%26entry.1292438233%3D%2520%2520Quantum%2520Human%2520Action%2520Recognition%2520%2528HAR%2529%2520is%2520an%2520interesting%2520research%2520area%2520in%250Ahuman-computer%2520interaction%2520used%2520to%2520monitor%2520the%2520activities%2520of%2520elderly%2520and%250Adisabled%2520individuals%2520affected%2520by%2520physical%2520and%2520mental%2520health.%2520In%2520the%2520recent%2520era%252C%250Askeleton-based%2520HAR%2520has%2520received%2520much%2520attention%2520because%2520skeleton%2520data%2520has%2520shown%250Athat%2520it%2520can%2520handle%2520changes%2520in%2520striking%252C%2520body%2520size%252C%2520camera%2520views%252C%2520and%2520complex%250Abackgrounds.%2520One%2520key%2520characteristic%2520of%2520ST-GCN%2520is%2520automatically%2520learning%2520spatial%250Aand%2520temporal%2520patterns%2520from%2520skeleton%2520sequences.%2520It%2520has%2520some%2520limitations%252C%2520as%2520this%250Amethod%2520only%2520works%2520for%2520short-range%2520correlation%2520due%2520to%2520its%2520limited%2520receptive%250Afield.%2520Consequently%252C%2520understanding%2520human%2520action%2520requires%2520long-range%250Ainterconnection.%2520To%2520address%2520this%2520issue%252C%2520we%2520developed%2520a%2520quantum%2520spatial-temporal%250Arelative%2520transformer%2520ST-RTR%2520model.%2520The%2520ST-RTR%2520includes%2520joint%2520and%2520relay%2520nodes%252C%250Awhich%2520allow%2520efficient%2520communication%2520and%2520data%2520transmission%2520within%2520the%2520network.%250AThese%2520nodes%2520help%2520to%2520break%2520the%2520inherent%2520spatial%2520and%2520temporal%2520skeleton%250Atopologies%252C%2520which%2520enables%2520the%2520model%2520to%2520understand%2520long-range%2520human%2520action%250Abetter.%2520Furthermore%252C%2520we%2520combine%2520quantum%2520ST-RTR%2520with%2520a%2520fusion%2520model%2520for%2520further%250Aperformance%2520improvements.%2520To%2520assess%2520the%2520performance%2520of%2520the%2520quantum%2520ST-RTR%250Amethod%252C%2520we%2520conducted%2520experiments%2520on%2520three%2520skeleton-based%2520HAR%2520benchmarks%253A%2520NTU%250ARGB%252BD%252060%252C%2520NTU%2520RGB%252BD%2520120%252C%2520and%2520UAV-Human.%2520It%2520boosted%2520CS%2520and%2520CV%2520by%25202.11%2520%2525%2520and%250A1.45%2525%2520on%2520NTU%2520RGB%252BD%252060%252C%25201.25%2525%2520and%25201.05%2525%2520on%2520NTU%2520RGB%252BD%2520120.%2520On%2520UAV-Human%2520datasets%252C%250Aaccuracy%2520improved%2520by%25202.54%2525.%2520The%2520experimental%2520outcomes%2520explain%2520that%2520the%2520proposed%250AST-RTR%2520model%2520significantly%2520improves%2520action%2520recognition%2520associated%2520with%2520the%250Astandard%2520ST-GCN%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Action%20Recognition%20%28HAR%29%20Using%20Skeleton-based%20Quantum%20Spatial%0A%20%20Temporal%20Relative%20Transformer%20Network%3A%20ST-RTR&entry.906535625=Faisal%20Mehmood%20and%20Enqing%20Chen%20and%20Touqeer%20Abbas%20and%20Samah%20M.%20Alzanin&entry.1292438233=%20%20Quantum%20Human%20Action%20Recognition%20%28HAR%29%20is%20an%20interesting%20research%20area%20in%0Ahuman-computer%20interaction%20used%20to%20monitor%20the%20activities%20of%20elderly%20and%0Adisabled%20individuals%20affected%20by%20physical%20and%20mental%20health.%20In%20the%20recent%20era%2C%0Askeleton-based%20HAR%20has%20received%20much%20attention%20because%20skeleton%20data%20has%20shown%0Athat%20it%20can%20handle%20changes%20in%20striking%2C%20body%20size%2C%20camera%20views%2C%20and%20complex%0Abackgrounds.%20One%20key%20characteristic%20of%20ST-GCN%20is%20automatically%20learning%20spatial%0Aand%20temporal%20patterns%20from%20skeleton%20sequences.%20It%20has%20some%20limitations%2C%20as%20this%0Amethod%20only%20works%20for%20short-range%20correlation%20due%20to%20its%20limited%20receptive%0Afield.%20Consequently%2C%20understanding%20human%20action%20requires%20long-range%0Ainterconnection.%20To%20address%20this%20issue%2C%20we%20developed%20a%20quantum%20spatial-temporal%0Arelative%20transformer%20ST-RTR%20model.%20The%20ST-RTR%20includes%20joint%20and%20relay%20nodes%2C%0Awhich%20allow%20efficient%20communication%20and%20data%20transmission%20within%20the%20network.%0AThese%20nodes%20help%20to%20break%20the%20inherent%20spatial%20and%20temporal%20skeleton%0Atopologies%2C%20which%20enables%20the%20model%20to%20understand%20long-range%20human%20action%0Abetter.%20Furthermore%2C%20we%20combine%20quantum%20ST-RTR%20with%20a%20fusion%20model%20for%20further%0Aperformance%20improvements.%20To%20assess%20the%20performance%20of%20the%20quantum%20ST-RTR%0Amethod%2C%20we%20conducted%20experiments%20on%20three%20skeleton-based%20HAR%20benchmarks%3A%20NTU%0ARGB%2BD%2060%2C%20NTU%20RGB%2BD%20120%2C%20and%20UAV-Human.%20It%20boosted%20CS%20and%20CV%20by%202.11%20%25%20and%0A1.45%25%20on%20NTU%20RGB%2BD%2060%2C%201.25%25%20and%201.05%25%20on%20NTU%20RGB%2BD%20120.%20On%20UAV-Human%20datasets%2C%0Aaccuracy%20improved%20by%202.54%25.%20The%20experimental%20outcomes%20explain%20that%20the%20proposed%0AST-RTR%20model%20significantly%20improves%20action%20recognition%20associated%20with%20the%0Astandard%20ST-GCN%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23806v1&entry.124074799=Read"},
{"title": "'No' Matters: Out-of-Distribution Detection in Multimodality Long\n  Dialogue", "author": "Rena Gao and Xuetong Wu and Siwen Luo and Caren Han and Feng Liu", "abstract": "  Out-of-distribution (OOD) detection in multimodal contexts is essential for\nidentifying deviations in combined inputs from different modalities,\nparticularly in applications like open-domain dialogue systems or real-life\ndialogue interactions. This paper aims to improve the user experience that\ninvolves multi-round long dialogues by efficiently detecting OOD dialogues and\nimages. We introduce a novel scoring framework named Dialogue Image Aligning\nand Enhancing Framework (DIAEF) that integrates the visual language models with\nthe novel proposed scores that detect OOD in two key scenarios (1) mismatches\nbetween the dialogue and image input pair and (2) input pairs with previously\nunseen labels. Our experimental results, derived from various benchmarks,\ndemonstrate that integrating image and multi-round dialogue OOD detection is\nmore effective with previously unseen labels than using either modality\nindependently. In the presence of mismatched pairs, our proposed score\neffectively identifies these mismatches and demonstrates strong robustness in\nlong dialogues. This approach enhances domain-aware, adaptive conversational\nagents and establishes baselines for future studies.\n", "link": "http://arxiv.org/abs/2410.23883v1", "date": "2024-10-31", "relevancy": 2.6401, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5282}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5282}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5276}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%27No%27%20Matters%3A%20Out-of-Distribution%20Detection%20in%20Multimodality%20Long%0A%20%20Dialogue&body=Title%3A%20%27No%27%20Matters%3A%20Out-of-Distribution%20Detection%20in%20Multimodality%20Long%0A%20%20Dialogue%0AAuthor%3A%20Rena%20Gao%20and%20Xuetong%20Wu%20and%20Siwen%20Luo%20and%20Caren%20Han%20and%20Feng%20Liu%0AAbstract%3A%20%20%20Out-of-distribution%20%28OOD%29%20detection%20in%20multimodal%20contexts%20is%20essential%20for%0Aidentifying%20deviations%20in%20combined%20inputs%20from%20different%20modalities%2C%0Aparticularly%20in%20applications%20like%20open-domain%20dialogue%20systems%20or%20real-life%0Adialogue%20interactions.%20This%20paper%20aims%20to%20improve%20the%20user%20experience%20that%0Ainvolves%20multi-round%20long%20dialogues%20by%20efficiently%20detecting%20OOD%20dialogues%20and%0Aimages.%20We%20introduce%20a%20novel%20scoring%20framework%20named%20Dialogue%20Image%20Aligning%0Aand%20Enhancing%20Framework%20%28DIAEF%29%20that%20integrates%20the%20visual%20language%20models%20with%0Athe%20novel%20proposed%20scores%20that%20detect%20OOD%20in%20two%20key%20scenarios%20%281%29%20mismatches%0Abetween%20the%20dialogue%20and%20image%20input%20pair%20and%20%282%29%20input%20pairs%20with%20previously%0Aunseen%20labels.%20Our%20experimental%20results%2C%20derived%20from%20various%20benchmarks%2C%0Ademonstrate%20that%20integrating%20image%20and%20multi-round%20dialogue%20OOD%20detection%20is%0Amore%20effective%20with%20previously%20unseen%20labels%20than%20using%20either%20modality%0Aindependently.%20In%20the%20presence%20of%20mismatched%20pairs%2C%20our%20proposed%20score%0Aeffectively%20identifies%20these%20mismatches%20and%20demonstrates%20strong%20robustness%20in%0Along%20dialogues.%20This%20approach%20enhances%20domain-aware%2C%20adaptive%20conversational%0Aagents%20and%20establishes%20baselines%20for%20future%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23883v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2527No%2527%2520Matters%253A%2520Out-of-Distribution%2520Detection%2520in%2520Multimodality%2520Long%250A%2520%2520Dialogue%26entry.906535625%3DRena%2520Gao%2520and%2520Xuetong%2520Wu%2520and%2520Siwen%2520Luo%2520and%2520Caren%2520Han%2520and%2520Feng%2520Liu%26entry.1292438233%3D%2520%2520Out-of-distribution%2520%2528OOD%2529%2520detection%2520in%2520multimodal%2520contexts%2520is%2520essential%2520for%250Aidentifying%2520deviations%2520in%2520combined%2520inputs%2520from%2520different%2520modalities%252C%250Aparticularly%2520in%2520applications%2520like%2520open-domain%2520dialogue%2520systems%2520or%2520real-life%250Adialogue%2520interactions.%2520This%2520paper%2520aims%2520to%2520improve%2520the%2520user%2520experience%2520that%250Ainvolves%2520multi-round%2520long%2520dialogues%2520by%2520efficiently%2520detecting%2520OOD%2520dialogues%2520and%250Aimages.%2520We%2520introduce%2520a%2520novel%2520scoring%2520framework%2520named%2520Dialogue%2520Image%2520Aligning%250Aand%2520Enhancing%2520Framework%2520%2528DIAEF%2529%2520that%2520integrates%2520the%2520visual%2520language%2520models%2520with%250Athe%2520novel%2520proposed%2520scores%2520that%2520detect%2520OOD%2520in%2520two%2520key%2520scenarios%2520%25281%2529%2520mismatches%250Abetween%2520the%2520dialogue%2520and%2520image%2520input%2520pair%2520and%2520%25282%2529%2520input%2520pairs%2520with%2520previously%250Aunseen%2520labels.%2520Our%2520experimental%2520results%252C%2520derived%2520from%2520various%2520benchmarks%252C%250Ademonstrate%2520that%2520integrating%2520image%2520and%2520multi-round%2520dialogue%2520OOD%2520detection%2520is%250Amore%2520effective%2520with%2520previously%2520unseen%2520labels%2520than%2520using%2520either%2520modality%250Aindependently.%2520In%2520the%2520presence%2520of%2520mismatched%2520pairs%252C%2520our%2520proposed%2520score%250Aeffectively%2520identifies%2520these%2520mismatches%2520and%2520demonstrates%2520strong%2520robustness%2520in%250Along%2520dialogues.%2520This%2520approach%2520enhances%2520domain-aware%252C%2520adaptive%2520conversational%250Aagents%2520and%2520establishes%2520baselines%2520for%2520future%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23883v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%27No%27%20Matters%3A%20Out-of-Distribution%20Detection%20in%20Multimodality%20Long%0A%20%20Dialogue&entry.906535625=Rena%20Gao%20and%20Xuetong%20Wu%20and%20Siwen%20Luo%20and%20Caren%20Han%20and%20Feng%20Liu&entry.1292438233=%20%20Out-of-distribution%20%28OOD%29%20detection%20in%20multimodal%20contexts%20is%20essential%20for%0Aidentifying%20deviations%20in%20combined%20inputs%20from%20different%20modalities%2C%0Aparticularly%20in%20applications%20like%20open-domain%20dialogue%20systems%20or%20real-life%0Adialogue%20interactions.%20This%20paper%20aims%20to%20improve%20the%20user%20experience%20that%0Ainvolves%20multi-round%20long%20dialogues%20by%20efficiently%20detecting%20OOD%20dialogues%20and%0Aimages.%20We%20introduce%20a%20novel%20scoring%20framework%20named%20Dialogue%20Image%20Aligning%0Aand%20Enhancing%20Framework%20%28DIAEF%29%20that%20integrates%20the%20visual%20language%20models%20with%0Athe%20novel%20proposed%20scores%20that%20detect%20OOD%20in%20two%20key%20scenarios%20%281%29%20mismatches%0Abetween%20the%20dialogue%20and%20image%20input%20pair%20and%20%282%29%20input%20pairs%20with%20previously%0Aunseen%20labels.%20Our%20experimental%20results%2C%20derived%20from%20various%20benchmarks%2C%0Ademonstrate%20that%20integrating%20image%20and%20multi-round%20dialogue%20OOD%20detection%20is%0Amore%20effective%20with%20previously%20unseen%20labels%20than%20using%20either%20modality%0Aindependently.%20In%20the%20presence%20of%20mismatched%20pairs%2C%20our%20proposed%20score%0Aeffectively%20identifies%20these%20mismatches%20and%20demonstrates%20strong%20robustness%20in%0Along%20dialogues.%20This%20approach%20enhances%20domain-aware%2C%20adaptive%20conversational%0Aagents%20and%20establishes%20baselines%20for%20future%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23883v1&entry.124074799=Read"},
{"title": "Driving by the Rules: A Benchmark for Integrating Traffic Sign\n  Regulations into Vectorized HD Map", "author": "Xinyuan Chang and Maixuan Xue and Xinran Liu and Zheng Pan and Xing Wei", "abstract": "  Ensuring adherence to traffic sign regulations is essential for both human\nand autonomous vehicle navigation. While current benchmark datasets concentrate\non lane perception or basic traffic sign recognition, they often overlook the\nintricate task of integrating these regulations into lane operations.\nAddressing this gap, we introduce MapDR, a novel dataset designed for the\nextraction of Driving Rules from traffic signs and their association with\nvectorized, locally perceived HD Maps. MapDR features over 10,000 annotated\nvideo clips that capture the intricate correlation between traffic sign\nregulations and lanes. We define two pivotal sub-tasks: 1) Rule Extraction from\nTraffic Sign, which accurately deciphers regulatory instructions, and 2)\nRule-Lane Correspondence Reasoning, which aligns these rules with their\nrespective lanes. Built upon this benchmark, we provide a multimodal solution\nthat offers a strong baseline for advancing autonomous driving technologies. It\nfills a critical gap in the integration of traffic sign rules, contributing to\nthe development of reliable autonomous navigation systems.\n", "link": "http://arxiv.org/abs/2410.23780v1", "date": "2024-10-31", "relevancy": 2.6347, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5357}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5259}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Driving%20by%20the%20Rules%3A%20A%20Benchmark%20for%20Integrating%20Traffic%20Sign%0A%20%20Regulations%20into%20Vectorized%20HD%20Map&body=Title%3A%20Driving%20by%20the%20Rules%3A%20A%20Benchmark%20for%20Integrating%20Traffic%20Sign%0A%20%20Regulations%20into%20Vectorized%20HD%20Map%0AAuthor%3A%20Xinyuan%20Chang%20and%20Maixuan%20Xue%20and%20Xinran%20Liu%20and%20Zheng%20Pan%20and%20Xing%20Wei%0AAbstract%3A%20%20%20Ensuring%20adherence%20to%20traffic%20sign%20regulations%20is%20essential%20for%20both%20human%0Aand%20autonomous%20vehicle%20navigation.%20While%20current%20benchmark%20datasets%20concentrate%0Aon%20lane%20perception%20or%20basic%20traffic%20sign%20recognition%2C%20they%20often%20overlook%20the%0Aintricate%20task%20of%20integrating%20these%20regulations%20into%20lane%20operations.%0AAddressing%20this%20gap%2C%20we%20introduce%20MapDR%2C%20a%20novel%20dataset%20designed%20for%20the%0Aextraction%20of%20Driving%20Rules%20from%20traffic%20signs%20and%20their%20association%20with%0Avectorized%2C%20locally%20perceived%20HD%20Maps.%20MapDR%20features%20over%2010%2C000%20annotated%0Avideo%20clips%20that%20capture%20the%20intricate%20correlation%20between%20traffic%20sign%0Aregulations%20and%20lanes.%20We%20define%20two%20pivotal%20sub-tasks%3A%201%29%20Rule%20Extraction%20from%0ATraffic%20Sign%2C%20which%20accurately%20deciphers%20regulatory%20instructions%2C%20and%202%29%0ARule-Lane%20Correspondence%20Reasoning%2C%20which%20aligns%20these%20rules%20with%20their%0Arespective%20lanes.%20Built%20upon%20this%20benchmark%2C%20we%20provide%20a%20multimodal%20solution%0Athat%20offers%20a%20strong%20baseline%20for%20advancing%20autonomous%20driving%20technologies.%20It%0Afills%20a%20critical%20gap%20in%20the%20integration%20of%20traffic%20sign%20rules%2C%20contributing%20to%0Athe%20development%20of%20reliable%20autonomous%20navigation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23780v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDriving%2520by%2520the%2520Rules%253A%2520A%2520Benchmark%2520for%2520Integrating%2520Traffic%2520Sign%250A%2520%2520Regulations%2520into%2520Vectorized%2520HD%2520Map%26entry.906535625%3DXinyuan%2520Chang%2520and%2520Maixuan%2520Xue%2520and%2520Xinran%2520Liu%2520and%2520Zheng%2520Pan%2520and%2520Xing%2520Wei%26entry.1292438233%3D%2520%2520Ensuring%2520adherence%2520to%2520traffic%2520sign%2520regulations%2520is%2520essential%2520for%2520both%2520human%250Aand%2520autonomous%2520vehicle%2520navigation.%2520While%2520current%2520benchmark%2520datasets%2520concentrate%250Aon%2520lane%2520perception%2520or%2520basic%2520traffic%2520sign%2520recognition%252C%2520they%2520often%2520overlook%2520the%250Aintricate%2520task%2520of%2520integrating%2520these%2520regulations%2520into%2520lane%2520operations.%250AAddressing%2520this%2520gap%252C%2520we%2520introduce%2520MapDR%252C%2520a%2520novel%2520dataset%2520designed%2520for%2520the%250Aextraction%2520of%2520Driving%2520Rules%2520from%2520traffic%2520signs%2520and%2520their%2520association%2520with%250Avectorized%252C%2520locally%2520perceived%2520HD%2520Maps.%2520MapDR%2520features%2520over%252010%252C000%2520annotated%250Avideo%2520clips%2520that%2520capture%2520the%2520intricate%2520correlation%2520between%2520traffic%2520sign%250Aregulations%2520and%2520lanes.%2520We%2520define%2520two%2520pivotal%2520sub-tasks%253A%25201%2529%2520Rule%2520Extraction%2520from%250ATraffic%2520Sign%252C%2520which%2520accurately%2520deciphers%2520regulatory%2520instructions%252C%2520and%25202%2529%250ARule-Lane%2520Correspondence%2520Reasoning%252C%2520which%2520aligns%2520these%2520rules%2520with%2520their%250Arespective%2520lanes.%2520Built%2520upon%2520this%2520benchmark%252C%2520we%2520provide%2520a%2520multimodal%2520solution%250Athat%2520offers%2520a%2520strong%2520baseline%2520for%2520advancing%2520autonomous%2520driving%2520technologies.%2520It%250Afills%2520a%2520critical%2520gap%2520in%2520the%2520integration%2520of%2520traffic%2520sign%2520rules%252C%2520contributing%2520to%250Athe%2520development%2520of%2520reliable%2520autonomous%2520navigation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23780v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Driving%20by%20the%20Rules%3A%20A%20Benchmark%20for%20Integrating%20Traffic%20Sign%0A%20%20Regulations%20into%20Vectorized%20HD%20Map&entry.906535625=Xinyuan%20Chang%20and%20Maixuan%20Xue%20and%20Xinran%20Liu%20and%20Zheng%20Pan%20and%20Xing%20Wei&entry.1292438233=%20%20Ensuring%20adherence%20to%20traffic%20sign%20regulations%20is%20essential%20for%20both%20human%0Aand%20autonomous%20vehicle%20navigation.%20While%20current%20benchmark%20datasets%20concentrate%0Aon%20lane%20perception%20or%20basic%20traffic%20sign%20recognition%2C%20they%20often%20overlook%20the%0Aintricate%20task%20of%20integrating%20these%20regulations%20into%20lane%20operations.%0AAddressing%20this%20gap%2C%20we%20introduce%20MapDR%2C%20a%20novel%20dataset%20designed%20for%20the%0Aextraction%20of%20Driving%20Rules%20from%20traffic%20signs%20and%20their%20association%20with%0Avectorized%2C%20locally%20perceived%20HD%20Maps.%20MapDR%20features%20over%2010%2C000%20annotated%0Avideo%20clips%20that%20capture%20the%20intricate%20correlation%20between%20traffic%20sign%0Aregulations%20and%20lanes.%20We%20define%20two%20pivotal%20sub-tasks%3A%201%29%20Rule%20Extraction%20from%0ATraffic%20Sign%2C%20which%20accurately%20deciphers%20regulatory%20instructions%2C%20and%202%29%0ARule-Lane%20Correspondence%20Reasoning%2C%20which%20aligns%20these%20rules%20with%20their%0Arespective%20lanes.%20Built%20upon%20this%20benchmark%2C%20we%20provide%20a%20multimodal%20solution%0Athat%20offers%20a%20strong%20baseline%20for%20advancing%20autonomous%20driving%20technologies.%20It%0Afills%20a%20critical%20gap%20in%20the%20integration%20of%20traffic%20sign%20rules%2C%20contributing%20to%0Athe%20development%20of%20reliable%20autonomous%20navigation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23780v1&entry.124074799=Read"},
{"title": "SFM-Protein: Integrative Co-evolutionary Pre-training for Advanced\n  Protein Sequence Representation", "author": "Liang He and Peiran Jin and Yaosen Min and Shufang Xie and Lijun Wu and Tao Qin and Xiaozhuan Liang and Kaiyuan Gao and Yuliang Jiang and Tie-Yan Liu", "abstract": "  Proteins, essential to biological systems, perform functions intricately\nlinked to their three-dimensional structures. Understanding the relationship\nbetween protein structures and their amino acid sequences remains a core\nchallenge in protein modeling. While traditional protein foundation models\nbenefit from pre-training on vast unlabeled datasets, they often struggle to\ncapture critical co-evolutionary information, which evolutionary-based methods\nexcel at. In this study, we introduce a novel pre-training strategy for protein\nfoundation models that emphasizes the interactions among amino acid residues to\nenhance the extraction of both short-range and long-range co-evolutionary\nfeatures from sequence data. Trained on a large-scale protein sequence dataset,\nour model demonstrates superior generalization ability, outperforming\nestablished baselines of similar size, including the ESM model, across diverse\ndownstream tasks. Experimental results confirm the model's effectiveness in\nintegrating co-evolutionary information, marking a significant step forward in\nprotein sequence-based modeling.\n", "link": "http://arxiv.org/abs/2410.24022v1", "date": "2024-10-31", "relevancy": 2.634, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5477}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SFM-Protein%3A%20Integrative%20Co-evolutionary%20Pre-training%20for%20Advanced%0A%20%20Protein%20Sequence%20Representation&body=Title%3A%20SFM-Protein%3A%20Integrative%20Co-evolutionary%20Pre-training%20for%20Advanced%0A%20%20Protein%20Sequence%20Representation%0AAuthor%3A%20Liang%20He%20and%20Peiran%20Jin%20and%20Yaosen%20Min%20and%20Shufang%20Xie%20and%20Lijun%20Wu%20and%20Tao%20Qin%20and%20Xiaozhuan%20Liang%20and%20Kaiyuan%20Gao%20and%20Yuliang%20Jiang%20and%20Tie-Yan%20Liu%0AAbstract%3A%20%20%20Proteins%2C%20essential%20to%20biological%20systems%2C%20perform%20functions%20intricately%0Alinked%20to%20their%20three-dimensional%20structures.%20Understanding%20the%20relationship%0Abetween%20protein%20structures%20and%20their%20amino%20acid%20sequences%20remains%20a%20core%0Achallenge%20in%20protein%20modeling.%20While%20traditional%20protein%20foundation%20models%0Abenefit%20from%20pre-training%20on%20vast%20unlabeled%20datasets%2C%20they%20often%20struggle%20to%0Acapture%20critical%20co-evolutionary%20information%2C%20which%20evolutionary-based%20methods%0Aexcel%20at.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20pre-training%20strategy%20for%20protein%0Afoundation%20models%20that%20emphasizes%20the%20interactions%20among%20amino%20acid%20residues%20to%0Aenhance%20the%20extraction%20of%20both%20short-range%20and%20long-range%20co-evolutionary%0Afeatures%20from%20sequence%20data.%20Trained%20on%20a%20large-scale%20protein%20sequence%20dataset%2C%0Aour%20model%20demonstrates%20superior%20generalization%20ability%2C%20outperforming%0Aestablished%20baselines%20of%20similar%20size%2C%20including%20the%20ESM%20model%2C%20across%20diverse%0Adownstream%20tasks.%20Experimental%20results%20confirm%20the%20model%27s%20effectiveness%20in%0Aintegrating%20co-evolutionary%20information%2C%20marking%20a%20significant%20step%20forward%20in%0Aprotein%20sequence-based%20modeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24022v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSFM-Protein%253A%2520Integrative%2520Co-evolutionary%2520Pre-training%2520for%2520Advanced%250A%2520%2520Protein%2520Sequence%2520Representation%26entry.906535625%3DLiang%2520He%2520and%2520Peiran%2520Jin%2520and%2520Yaosen%2520Min%2520and%2520Shufang%2520Xie%2520and%2520Lijun%2520Wu%2520and%2520Tao%2520Qin%2520and%2520Xiaozhuan%2520Liang%2520and%2520Kaiyuan%2520Gao%2520and%2520Yuliang%2520Jiang%2520and%2520Tie-Yan%2520Liu%26entry.1292438233%3D%2520%2520Proteins%252C%2520essential%2520to%2520biological%2520systems%252C%2520perform%2520functions%2520intricately%250Alinked%2520to%2520their%2520three-dimensional%2520structures.%2520Understanding%2520the%2520relationship%250Abetween%2520protein%2520structures%2520and%2520their%2520amino%2520acid%2520sequences%2520remains%2520a%2520core%250Achallenge%2520in%2520protein%2520modeling.%2520While%2520traditional%2520protein%2520foundation%2520models%250Abenefit%2520from%2520pre-training%2520on%2520vast%2520unlabeled%2520datasets%252C%2520they%2520often%2520struggle%2520to%250Acapture%2520critical%2520co-evolutionary%2520information%252C%2520which%2520evolutionary-based%2520methods%250Aexcel%2520at.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520novel%2520pre-training%2520strategy%2520for%2520protein%250Afoundation%2520models%2520that%2520emphasizes%2520the%2520interactions%2520among%2520amino%2520acid%2520residues%2520to%250Aenhance%2520the%2520extraction%2520of%2520both%2520short-range%2520and%2520long-range%2520co-evolutionary%250Afeatures%2520from%2520sequence%2520data.%2520Trained%2520on%2520a%2520large-scale%2520protein%2520sequence%2520dataset%252C%250Aour%2520model%2520demonstrates%2520superior%2520generalization%2520ability%252C%2520outperforming%250Aestablished%2520baselines%2520of%2520similar%2520size%252C%2520including%2520the%2520ESM%2520model%252C%2520across%2520diverse%250Adownstream%2520tasks.%2520Experimental%2520results%2520confirm%2520the%2520model%2527s%2520effectiveness%2520in%250Aintegrating%2520co-evolutionary%2520information%252C%2520marking%2520a%2520significant%2520step%2520forward%2520in%250Aprotein%2520sequence-based%2520modeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24022v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SFM-Protein%3A%20Integrative%20Co-evolutionary%20Pre-training%20for%20Advanced%0A%20%20Protein%20Sequence%20Representation&entry.906535625=Liang%20He%20and%20Peiran%20Jin%20and%20Yaosen%20Min%20and%20Shufang%20Xie%20and%20Lijun%20Wu%20and%20Tao%20Qin%20and%20Xiaozhuan%20Liang%20and%20Kaiyuan%20Gao%20and%20Yuliang%20Jiang%20and%20Tie-Yan%20Liu&entry.1292438233=%20%20Proteins%2C%20essential%20to%20biological%20systems%2C%20perform%20functions%20intricately%0Alinked%20to%20their%20three-dimensional%20structures.%20Understanding%20the%20relationship%0Abetween%20protein%20structures%20and%20their%20amino%20acid%20sequences%20remains%20a%20core%0Achallenge%20in%20protein%20modeling.%20While%20traditional%20protein%20foundation%20models%0Abenefit%20from%20pre-training%20on%20vast%20unlabeled%20datasets%2C%20they%20often%20struggle%20to%0Acapture%20critical%20co-evolutionary%20information%2C%20which%20evolutionary-based%20methods%0Aexcel%20at.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20pre-training%20strategy%20for%20protein%0Afoundation%20models%20that%20emphasizes%20the%20interactions%20among%20amino%20acid%20residues%20to%0Aenhance%20the%20extraction%20of%20both%20short-range%20and%20long-range%20co-evolutionary%0Afeatures%20from%20sequence%20data.%20Trained%20on%20a%20large-scale%20protein%20sequence%20dataset%2C%0Aour%20model%20demonstrates%20superior%20generalization%20ability%2C%20outperforming%0Aestablished%20baselines%20of%20similar%20size%2C%20including%20the%20ESM%20model%2C%20across%20diverse%0Adownstream%20tasks.%20Experimental%20results%20confirm%20the%20model%27s%20effectiveness%20in%0Aintegrating%20co-evolutionary%20information%2C%20marking%20a%20significant%20step%20forward%20in%0Aprotein%20sequence-based%20modeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24022v1&entry.124074799=Read"},
{"title": "FRoundation: Are Foundation Models Ready for Face Recognition?", "author": "Tahar Chettaoui and Naser Damer and Fadi Boutros", "abstract": "  Foundation models are predominantly trained in an unsupervised or\nself-supervised manner on highly diverse and large-scale datasets, making them\nbroadly applicable to various downstream tasks. In this work, we investigate\nfor the first time whether such models are suitable for the specific domain of\nface recognition. We further propose and demonstrate the adaptation of these\nmodels for face recognition across different levels of data availability.\nExtensive experiments are conducted on multiple foundation models and datasets\nof varying scales for training and fine-tuning, with evaluation on a wide range\nof benchmarks. Our results indicate that, despite their versatility,\npre-trained foundation models underperform in face recognition compared to\nsimilar architectures trained specifically for this task. However, fine-tuning\nfoundation models yields promising results, often surpassing models trained\nfrom scratch when training data is limited. Even with access to large-scale\nface recognition training datasets, fine-tuned foundation models perform\ncomparably to models trained from scratch, but with lower training\ncomputational costs and without relying on the assumption of extensive data\navailability. Our analysis also explores bias in face recognition, with\nslightly higher bias observed in some settings when using foundation models.\n", "link": "http://arxiv.org/abs/2410.23831v1", "date": "2024-10-31", "relevancy": 2.6085, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5481}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4688}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FRoundation%3A%20Are%20Foundation%20Models%20Ready%20for%20Face%20Recognition%3F&body=Title%3A%20FRoundation%3A%20Are%20Foundation%20Models%20Ready%20for%20Face%20Recognition%3F%0AAuthor%3A%20Tahar%20Chettaoui%20and%20Naser%20Damer%20and%20Fadi%20Boutros%0AAbstract%3A%20%20%20Foundation%20models%20are%20predominantly%20trained%20in%20an%20unsupervised%20or%0Aself-supervised%20manner%20on%20highly%20diverse%20and%20large-scale%20datasets%2C%20making%20them%0Abroadly%20applicable%20to%20various%20downstream%20tasks.%20In%20this%20work%2C%20we%20investigate%0Afor%20the%20first%20time%20whether%20such%20models%20are%20suitable%20for%20the%20specific%20domain%20of%0Aface%20recognition.%20We%20further%20propose%20and%20demonstrate%20the%20adaptation%20of%20these%0Amodels%20for%20face%20recognition%20across%20different%20levels%20of%20data%20availability.%0AExtensive%20experiments%20are%20conducted%20on%20multiple%20foundation%20models%20and%20datasets%0Aof%20varying%20scales%20for%20training%20and%20fine-tuning%2C%20with%20evaluation%20on%20a%20wide%20range%0Aof%20benchmarks.%20Our%20results%20indicate%20that%2C%20despite%20their%20versatility%2C%0Apre-trained%20foundation%20models%20underperform%20in%20face%20recognition%20compared%20to%0Asimilar%20architectures%20trained%20specifically%20for%20this%20task.%20However%2C%20fine-tuning%0Afoundation%20models%20yields%20promising%20results%2C%20often%20surpassing%20models%20trained%0Afrom%20scratch%20when%20training%20data%20is%20limited.%20Even%20with%20access%20to%20large-scale%0Aface%20recognition%20training%20datasets%2C%20fine-tuned%20foundation%20models%20perform%0Acomparably%20to%20models%20trained%20from%20scratch%2C%20but%20with%20lower%20training%0Acomputational%20costs%20and%20without%20relying%20on%20the%20assumption%20of%20extensive%20data%0Aavailability.%20Our%20analysis%20also%20explores%20bias%20in%20face%20recognition%2C%20with%0Aslightly%20higher%20bias%20observed%20in%20some%20settings%20when%20using%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23831v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFRoundation%253A%2520Are%2520Foundation%2520Models%2520Ready%2520for%2520Face%2520Recognition%253F%26entry.906535625%3DTahar%2520Chettaoui%2520and%2520Naser%2520Damer%2520and%2520Fadi%2520Boutros%26entry.1292438233%3D%2520%2520Foundation%2520models%2520are%2520predominantly%2520trained%2520in%2520an%2520unsupervised%2520or%250Aself-supervised%2520manner%2520on%2520highly%2520diverse%2520and%2520large-scale%2520datasets%252C%2520making%2520them%250Abroadly%2520applicable%2520to%2520various%2520downstream%2520tasks.%2520In%2520this%2520work%252C%2520we%2520investigate%250Afor%2520the%2520first%2520time%2520whether%2520such%2520models%2520are%2520suitable%2520for%2520the%2520specific%2520domain%2520of%250Aface%2520recognition.%2520We%2520further%2520propose%2520and%2520demonstrate%2520the%2520adaptation%2520of%2520these%250Amodels%2520for%2520face%2520recognition%2520across%2520different%2520levels%2520of%2520data%2520availability.%250AExtensive%2520experiments%2520are%2520conducted%2520on%2520multiple%2520foundation%2520models%2520and%2520datasets%250Aof%2520varying%2520scales%2520for%2520training%2520and%2520fine-tuning%252C%2520with%2520evaluation%2520on%2520a%2520wide%2520range%250Aof%2520benchmarks.%2520Our%2520results%2520indicate%2520that%252C%2520despite%2520their%2520versatility%252C%250Apre-trained%2520foundation%2520models%2520underperform%2520in%2520face%2520recognition%2520compared%2520to%250Asimilar%2520architectures%2520trained%2520specifically%2520for%2520this%2520task.%2520However%252C%2520fine-tuning%250Afoundation%2520models%2520yields%2520promising%2520results%252C%2520often%2520surpassing%2520models%2520trained%250Afrom%2520scratch%2520when%2520training%2520data%2520is%2520limited.%2520Even%2520with%2520access%2520to%2520large-scale%250Aface%2520recognition%2520training%2520datasets%252C%2520fine-tuned%2520foundation%2520models%2520perform%250Acomparably%2520to%2520models%2520trained%2520from%2520scratch%252C%2520but%2520with%2520lower%2520training%250Acomputational%2520costs%2520and%2520without%2520relying%2520on%2520the%2520assumption%2520of%2520extensive%2520data%250Aavailability.%2520Our%2520analysis%2520also%2520explores%2520bias%2520in%2520face%2520recognition%252C%2520with%250Aslightly%2520higher%2520bias%2520observed%2520in%2520some%2520settings%2520when%2520using%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23831v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FRoundation%3A%20Are%20Foundation%20Models%20Ready%20for%20Face%20Recognition%3F&entry.906535625=Tahar%20Chettaoui%20and%20Naser%20Damer%20and%20Fadi%20Boutros&entry.1292438233=%20%20Foundation%20models%20are%20predominantly%20trained%20in%20an%20unsupervised%20or%0Aself-supervised%20manner%20on%20highly%20diverse%20and%20large-scale%20datasets%2C%20making%20them%0Abroadly%20applicable%20to%20various%20downstream%20tasks.%20In%20this%20work%2C%20we%20investigate%0Afor%20the%20first%20time%20whether%20such%20models%20are%20suitable%20for%20the%20specific%20domain%20of%0Aface%20recognition.%20We%20further%20propose%20and%20demonstrate%20the%20adaptation%20of%20these%0Amodels%20for%20face%20recognition%20across%20different%20levels%20of%20data%20availability.%0AExtensive%20experiments%20are%20conducted%20on%20multiple%20foundation%20models%20and%20datasets%0Aof%20varying%20scales%20for%20training%20and%20fine-tuning%2C%20with%20evaluation%20on%20a%20wide%20range%0Aof%20benchmarks.%20Our%20results%20indicate%20that%2C%20despite%20their%20versatility%2C%0Apre-trained%20foundation%20models%20underperform%20in%20face%20recognition%20compared%20to%0Asimilar%20architectures%20trained%20specifically%20for%20this%20task.%20However%2C%20fine-tuning%0Afoundation%20models%20yields%20promising%20results%2C%20often%20surpassing%20models%20trained%0Afrom%20scratch%20when%20training%20data%20is%20limited.%20Even%20with%20access%20to%20large-scale%0Aface%20recognition%20training%20datasets%2C%20fine-tuned%20foundation%20models%20perform%0Acomparably%20to%20models%20trained%20from%20scratch%2C%20but%20with%20lower%20training%0Acomputational%20costs%20and%20without%20relying%20on%20the%20assumption%20of%20extensive%20data%0Aavailability.%20Our%20analysis%20also%20explores%20bias%20in%20face%20recognition%2C%20with%0Aslightly%20higher%20bias%20observed%20in%20some%20settings%20when%20using%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23831v1&entry.124074799=Read"},
{"title": "Uncertainty Estimation for 3D Object Detection via Evidential Learning", "author": "Nikita Durasov and Rafid Mahmood and Jiwoong Choi and Marc T. Law and James Lucas and Pascal Fua and Jose M. Alvarez", "abstract": "  3D object detection is an essential task for computer vision applications in\nautonomous vehicles and robotics. However, models often struggle to quantify\ndetection reliability, leading to poor performance on unfamiliar scenes. We\nintroduce a framework for quantifying uncertainty in 3D object detection by\nleveraging an evidential learning loss on Bird's Eye View representations in\nthe 3D detector. These uncertainty estimates require minimal computational\noverhead and are generalizable across different architectures. We demonstrate\nboth the efficacy and importance of these uncertainty estimates on identifying\nout-of-distribution scenes, poorly localized objects, and missing (false\nnegative) detections; our framework consistently improves over baselines by\n10-20% on average. Finally, we integrate this suite of tasks into a system\nwhere a 3D object detector auto-labels driving scenes and our uncertainty\nestimates verify label correctness before the labels are used to train a second\nmodel. Here, our uncertainty-driven verification results in a 1% improvement in\nmAP and a 1-2% improvement in NDS.\n", "link": "http://arxiv.org/abs/2410.23910v1", "date": "2024-10-31", "relevancy": 2.6041, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.788}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6279}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.6194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty%20Estimation%20for%203D%20Object%20Detection%20via%20Evidential%20Learning&body=Title%3A%20Uncertainty%20Estimation%20for%203D%20Object%20Detection%20via%20Evidential%20Learning%0AAuthor%3A%20Nikita%20Durasov%20and%20Rafid%20Mahmood%20and%20Jiwoong%20Choi%20and%20Marc%20T.%20Law%20and%20James%20Lucas%20and%20Pascal%20Fua%20and%20Jose%20M.%20Alvarez%0AAbstract%3A%20%20%203D%20object%20detection%20is%20an%20essential%20task%20for%20computer%20vision%20applications%20in%0Aautonomous%20vehicles%20and%20robotics.%20However%2C%20models%20often%20struggle%20to%20quantify%0Adetection%20reliability%2C%20leading%20to%20poor%20performance%20on%20unfamiliar%20scenes.%20We%0Aintroduce%20a%20framework%20for%20quantifying%20uncertainty%20in%203D%20object%20detection%20by%0Aleveraging%20an%20evidential%20learning%20loss%20on%20Bird%27s%20Eye%20View%20representations%20in%0Athe%203D%20detector.%20These%20uncertainty%20estimates%20require%20minimal%20computational%0Aoverhead%20and%20are%20generalizable%20across%20different%20architectures.%20We%20demonstrate%0Aboth%20the%20efficacy%20and%20importance%20of%20these%20uncertainty%20estimates%20on%20identifying%0Aout-of-distribution%20scenes%2C%20poorly%20localized%20objects%2C%20and%20missing%20%28false%0Anegative%29%20detections%3B%20our%20framework%20consistently%20improves%20over%20baselines%20by%0A10-20%25%20on%20average.%20Finally%2C%20we%20integrate%20this%20suite%20of%20tasks%20into%20a%20system%0Awhere%20a%203D%20object%20detector%20auto-labels%20driving%20scenes%20and%20our%20uncertainty%0Aestimates%20verify%20label%20correctness%20before%20the%20labels%20are%20used%20to%20train%20a%20second%0Amodel.%20Here%2C%20our%20uncertainty-driven%20verification%20results%20in%20a%201%25%20improvement%20in%0AmAP%20and%20a%201-2%25%20improvement%20in%20NDS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23910v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty%2520Estimation%2520for%25203D%2520Object%2520Detection%2520via%2520Evidential%2520Learning%26entry.906535625%3DNikita%2520Durasov%2520and%2520Rafid%2520Mahmood%2520and%2520Jiwoong%2520Choi%2520and%2520Marc%2520T.%2520Law%2520and%2520James%2520Lucas%2520and%2520Pascal%2520Fua%2520and%2520Jose%2520M.%2520Alvarez%26entry.1292438233%3D%2520%25203D%2520object%2520detection%2520is%2520an%2520essential%2520task%2520for%2520computer%2520vision%2520applications%2520in%250Aautonomous%2520vehicles%2520and%2520robotics.%2520However%252C%2520models%2520often%2520struggle%2520to%2520quantify%250Adetection%2520reliability%252C%2520leading%2520to%2520poor%2520performance%2520on%2520unfamiliar%2520scenes.%2520We%250Aintroduce%2520a%2520framework%2520for%2520quantifying%2520uncertainty%2520in%25203D%2520object%2520detection%2520by%250Aleveraging%2520an%2520evidential%2520learning%2520loss%2520on%2520Bird%2527s%2520Eye%2520View%2520representations%2520in%250Athe%25203D%2520detector.%2520These%2520uncertainty%2520estimates%2520require%2520minimal%2520computational%250Aoverhead%2520and%2520are%2520generalizable%2520across%2520different%2520architectures.%2520We%2520demonstrate%250Aboth%2520the%2520efficacy%2520and%2520importance%2520of%2520these%2520uncertainty%2520estimates%2520on%2520identifying%250Aout-of-distribution%2520scenes%252C%2520poorly%2520localized%2520objects%252C%2520and%2520missing%2520%2528false%250Anegative%2529%2520detections%253B%2520our%2520framework%2520consistently%2520improves%2520over%2520baselines%2520by%250A10-20%2525%2520on%2520average.%2520Finally%252C%2520we%2520integrate%2520this%2520suite%2520of%2520tasks%2520into%2520a%2520system%250Awhere%2520a%25203D%2520object%2520detector%2520auto-labels%2520driving%2520scenes%2520and%2520our%2520uncertainty%250Aestimates%2520verify%2520label%2520correctness%2520before%2520the%2520labels%2520are%2520used%2520to%2520train%2520a%2520second%250Amodel.%2520Here%252C%2520our%2520uncertainty-driven%2520verification%2520results%2520in%2520a%25201%2525%2520improvement%2520in%250AmAP%2520and%2520a%25201-2%2525%2520improvement%2520in%2520NDS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23910v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty%20Estimation%20for%203D%20Object%20Detection%20via%20Evidential%20Learning&entry.906535625=Nikita%20Durasov%20and%20Rafid%20Mahmood%20and%20Jiwoong%20Choi%20and%20Marc%20T.%20Law%20and%20James%20Lucas%20and%20Pascal%20Fua%20and%20Jose%20M.%20Alvarez&entry.1292438233=%20%203D%20object%20detection%20is%20an%20essential%20task%20for%20computer%20vision%20applications%20in%0Aautonomous%20vehicles%20and%20robotics.%20However%2C%20models%20often%20struggle%20to%20quantify%0Adetection%20reliability%2C%20leading%20to%20poor%20performance%20on%20unfamiliar%20scenes.%20We%0Aintroduce%20a%20framework%20for%20quantifying%20uncertainty%20in%203D%20object%20detection%20by%0Aleveraging%20an%20evidential%20learning%20loss%20on%20Bird%27s%20Eye%20View%20representations%20in%0Athe%203D%20detector.%20These%20uncertainty%20estimates%20require%20minimal%20computational%0Aoverhead%20and%20are%20generalizable%20across%20different%20architectures.%20We%20demonstrate%0Aboth%20the%20efficacy%20and%20importance%20of%20these%20uncertainty%20estimates%20on%20identifying%0Aout-of-distribution%20scenes%2C%20poorly%20localized%20objects%2C%20and%20missing%20%28false%0Anegative%29%20detections%3B%20our%20framework%20consistently%20improves%20over%20baselines%20by%0A10-20%25%20on%20average.%20Finally%2C%20we%20integrate%20this%20suite%20of%20tasks%20into%20a%20system%0Awhere%20a%203D%20object%20detector%20auto-labels%20driving%20scenes%20and%20our%20uncertainty%0Aestimates%20verify%20label%20correctness%20before%20the%20labels%20are%20used%20to%20train%20a%20second%0Amodel.%20Here%2C%20our%20uncertainty-driven%20verification%20results%20in%20a%201%25%20improvement%20in%0AmAP%20and%20a%201-2%25%20improvement%20in%20NDS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23910v1&entry.124074799=Read"},
{"title": "Deep Graph Neural Networks via Posteriori-Sampling-based Node-Adaptive\n  Residual Module", "author": "Jingbo Zhou and Yixuan Du and Ruqiong Zhang and Jun Xia and Zhizhi Yu and Zelin Zang and Di Jin and Carl Yang and Rui Zhang and Stan Z. Li", "abstract": "  Graph Neural Networks (GNNs), a type of neural network that can learn from\ngraph-structured data through neighborhood information aggregation, have shown\nsuperior performance in various downstream tasks. However, as the number of\nlayers increases, node representations become indistinguishable, which is known\nas over-smoothing. To address this issue, many residual methods have emerged.\nIn this paper, we focus on the over-smoothing issue and related residual\nmethods. Firstly, we revisit over-smoothing from the perspective of overlapping\nneighborhood subgraphs, and based on this, we explain how residual methods can\nalleviate over-smoothing by integrating multiple orders neighborhood subgraphs\nto avoid the indistinguishability of the single high-order neighborhood\nsubgraphs. Additionally, we reveal the drawbacks of previous residual methods,\nsuch as the lack of node adaptability and severe loss of high-order\nneighborhood subgraph information, and propose a\n\\textbf{Posterior-Sampling-based, Node-Adaptive Residual module (PSNR)}. We\ntheoretically demonstrate that PSNR can alleviate the drawbacks of previous\nresidual methods. Furthermore, extensive experiments verify the superiority of\nthe PSNR module in fully observed node classification and missing feature\nscenarios. Our code is available at https://github.com/jingbo02/PSNR-GNN.\n", "link": "http://arxiv.org/abs/2305.05368v3", "date": "2024-10-31", "relevancy": 2.5889, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5335}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5111}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Graph%20Neural%20Networks%20via%20Posteriori-Sampling-based%20Node-Adaptive%0A%20%20Residual%20Module&body=Title%3A%20Deep%20Graph%20Neural%20Networks%20via%20Posteriori-Sampling-based%20Node-Adaptive%0A%20%20Residual%20Module%0AAuthor%3A%20Jingbo%20Zhou%20and%20Yixuan%20Du%20and%20Ruqiong%20Zhang%20and%20Jun%20Xia%20and%20Zhizhi%20Yu%20and%20Zelin%20Zang%20and%20Di%20Jin%20and%20Carl%20Yang%20and%20Rui%20Zhang%20and%20Stan%20Z.%20Li%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20a%20type%20of%20neural%20network%20that%20can%20learn%20from%0Agraph-structured%20data%20through%20neighborhood%20information%20aggregation%2C%20have%20shown%0Asuperior%20performance%20in%20various%20downstream%20tasks.%20However%2C%20as%20the%20number%20of%0Alayers%20increases%2C%20node%20representations%20become%20indistinguishable%2C%20which%20is%20known%0Aas%20over-smoothing.%20To%20address%20this%20issue%2C%20many%20residual%20methods%20have%20emerged.%0AIn%20this%20paper%2C%20we%20focus%20on%20the%20over-smoothing%20issue%20and%20related%20residual%0Amethods.%20Firstly%2C%20we%20revisit%20over-smoothing%20from%20the%20perspective%20of%20overlapping%0Aneighborhood%20subgraphs%2C%20and%20based%20on%20this%2C%20we%20explain%20how%20residual%20methods%20can%0Aalleviate%20over-smoothing%20by%20integrating%20multiple%20orders%20neighborhood%20subgraphs%0Ato%20avoid%20the%20indistinguishability%20of%20the%20single%20high-order%20neighborhood%0Asubgraphs.%20Additionally%2C%20we%20reveal%20the%20drawbacks%20of%20previous%20residual%20methods%2C%0Asuch%20as%20the%20lack%20of%20node%20adaptability%20and%20severe%20loss%20of%20high-order%0Aneighborhood%20subgraph%20information%2C%20and%20propose%20a%0A%5Ctextbf%7BPosterior-Sampling-based%2C%20Node-Adaptive%20Residual%20module%20%28PSNR%29%7D.%20We%0Atheoretically%20demonstrate%20that%20PSNR%20can%20alleviate%20the%20drawbacks%20of%20previous%0Aresidual%20methods.%20Furthermore%2C%20extensive%20experiments%20verify%20the%20superiority%20of%0Athe%20PSNR%20module%20in%20fully%20observed%20node%20classification%20and%20missing%20feature%0Ascenarios.%20Our%20code%20is%20available%20at%20https%3A//github.com/jingbo02/PSNR-GNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.05368v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Graph%2520Neural%2520Networks%2520via%2520Posteriori-Sampling-based%2520Node-Adaptive%250A%2520%2520Residual%2520Module%26entry.906535625%3DJingbo%2520Zhou%2520and%2520Yixuan%2520Du%2520and%2520Ruqiong%2520Zhang%2520and%2520Jun%2520Xia%2520and%2520Zhizhi%2520Yu%2520and%2520Zelin%2520Zang%2520and%2520Di%2520Jin%2520and%2520Carl%2520Yang%2520and%2520Rui%2520Zhang%2520and%2520Stan%2520Z.%2520Li%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%252C%2520a%2520type%2520of%2520neural%2520network%2520that%2520can%2520learn%2520from%250Agraph-structured%2520data%2520through%2520neighborhood%2520information%2520aggregation%252C%2520have%2520shown%250Asuperior%2520performance%2520in%2520various%2520downstream%2520tasks.%2520However%252C%2520as%2520the%2520number%2520of%250Alayers%2520increases%252C%2520node%2520representations%2520become%2520indistinguishable%252C%2520which%2520is%2520known%250Aas%2520over-smoothing.%2520To%2520address%2520this%2520issue%252C%2520many%2520residual%2520methods%2520have%2520emerged.%250AIn%2520this%2520paper%252C%2520we%2520focus%2520on%2520the%2520over-smoothing%2520issue%2520and%2520related%2520residual%250Amethods.%2520Firstly%252C%2520we%2520revisit%2520over-smoothing%2520from%2520the%2520perspective%2520of%2520overlapping%250Aneighborhood%2520subgraphs%252C%2520and%2520based%2520on%2520this%252C%2520we%2520explain%2520how%2520residual%2520methods%2520can%250Aalleviate%2520over-smoothing%2520by%2520integrating%2520multiple%2520orders%2520neighborhood%2520subgraphs%250Ato%2520avoid%2520the%2520indistinguishability%2520of%2520the%2520single%2520high-order%2520neighborhood%250Asubgraphs.%2520Additionally%252C%2520we%2520reveal%2520the%2520drawbacks%2520of%2520previous%2520residual%2520methods%252C%250Asuch%2520as%2520the%2520lack%2520of%2520node%2520adaptability%2520and%2520severe%2520loss%2520of%2520high-order%250Aneighborhood%2520subgraph%2520information%252C%2520and%2520propose%2520a%250A%255Ctextbf%257BPosterior-Sampling-based%252C%2520Node-Adaptive%2520Residual%2520module%2520%2528PSNR%2529%257D.%2520We%250Atheoretically%2520demonstrate%2520that%2520PSNR%2520can%2520alleviate%2520the%2520drawbacks%2520of%2520previous%250Aresidual%2520methods.%2520Furthermore%252C%2520extensive%2520experiments%2520verify%2520the%2520superiority%2520of%250Athe%2520PSNR%2520module%2520in%2520fully%2520observed%2520node%2520classification%2520and%2520missing%2520feature%250Ascenarios.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/jingbo02/PSNR-GNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.05368v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Graph%20Neural%20Networks%20via%20Posteriori-Sampling-based%20Node-Adaptive%0A%20%20Residual%20Module&entry.906535625=Jingbo%20Zhou%20and%20Yixuan%20Du%20and%20Ruqiong%20Zhang%20and%20Jun%20Xia%20and%20Zhizhi%20Yu%20and%20Zelin%20Zang%20and%20Di%20Jin%20and%20Carl%20Yang%20and%20Rui%20Zhang%20and%20Stan%20Z.%20Li&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20a%20type%20of%20neural%20network%20that%20can%20learn%20from%0Agraph-structured%20data%20through%20neighborhood%20information%20aggregation%2C%20have%20shown%0Asuperior%20performance%20in%20various%20downstream%20tasks.%20However%2C%20as%20the%20number%20of%0Alayers%20increases%2C%20node%20representations%20become%20indistinguishable%2C%20which%20is%20known%0Aas%20over-smoothing.%20To%20address%20this%20issue%2C%20many%20residual%20methods%20have%20emerged.%0AIn%20this%20paper%2C%20we%20focus%20on%20the%20over-smoothing%20issue%20and%20related%20residual%0Amethods.%20Firstly%2C%20we%20revisit%20over-smoothing%20from%20the%20perspective%20of%20overlapping%0Aneighborhood%20subgraphs%2C%20and%20based%20on%20this%2C%20we%20explain%20how%20residual%20methods%20can%0Aalleviate%20over-smoothing%20by%20integrating%20multiple%20orders%20neighborhood%20subgraphs%0Ato%20avoid%20the%20indistinguishability%20of%20the%20single%20high-order%20neighborhood%0Asubgraphs.%20Additionally%2C%20we%20reveal%20the%20drawbacks%20of%20previous%20residual%20methods%2C%0Asuch%20as%20the%20lack%20of%20node%20adaptability%20and%20severe%20loss%20of%20high-order%0Aneighborhood%20subgraph%20information%2C%20and%20propose%20a%0A%5Ctextbf%7BPosterior-Sampling-based%2C%20Node-Adaptive%20Residual%20module%20%28PSNR%29%7D.%20We%0Atheoretically%20demonstrate%20that%20PSNR%20can%20alleviate%20the%20drawbacks%20of%20previous%0Aresidual%20methods.%20Furthermore%2C%20extensive%20experiments%20verify%20the%20superiority%20of%0Athe%20PSNR%20module%20in%20fully%20observed%20node%20classification%20and%20missing%20feature%0Ascenarios.%20Our%20code%20is%20available%20at%20https%3A//github.com/jingbo02/PSNR-GNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.05368v3&entry.124074799=Read"},
{"title": "SR-CACO-2: A Dataset for Confocal Fluorescence Microscopy Image\n  Super-Resolution", "author": "Soufiane Belharbi and Mara KM Whitford and Phuong Hoang and Shakeeb Murtaza and Luke McCaffrey and Eric Granger", "abstract": "  Confocal fluorescence microscopy is one of the most accessible and widely\nused imaging techniques for the study of biological processes at the cellular\nand subcellular levels. Scanning confocal microscopy allows the capture of\nhigh-quality images from thick three-dimensional (3D) samples, yet suffers from\nwell-known limitations such as photobleaching and phototoxicity of specimens\ncaused by intense light exposure, limiting its applications. Cellular damage\ncan be alleviated by changing imaging parameters to reduce light exposure,\noften at the expense of image quality. Machine/deep learning methods for\nsingle-image super-resolution (SISR) can be applied to restore image quality by\nupscaling lower-resolution (LR) images to yield high-resolution images (HR).\nThese SISR methods have been successfully applied to photo-realistic images due\npartly to the abundance of publicly available data. In contrast, the lack of\npublicly available data partly limits their application and success in scanning\nconfocal microscopy. In this paper, we introduce a large scanning confocal\nmicroscopy dataset named SR-CACO-2 that is comprised of low- and\nhigh-resolution image pairs marked for three different fluorescent markers. It\nallows the evaluation of performance of SISR methods on three different\nupscaling levels (X2, X4, X8). SR-CACO-2 contains the human epithelial cell\nline Caco-2 (ATCC HTB-37), and it is composed of 2,200 unique images, captured\nwith four resolutions and three markers, forming 9,937 image patches for SISR\nmethods. We provide benchmarking results for 16 state-of-the-art methods of the\nmain SISR families. Results show that these methods have limited success in\nproducing high-resolution textures. The dataset is freely accessible under a\nCreative Commons license (CC BY-NC-SA 4.0). Our dataset, code and pretrained\nweights for SISR methods are available: https://github.com/sbelharbi/sr-caco-2.\n", "link": "http://arxiv.org/abs/2406.09168v2", "date": "2024-10-31", "relevancy": 2.5849, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5283}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5113}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SR-CACO-2%3A%20A%20Dataset%20for%20Confocal%20Fluorescence%20Microscopy%20Image%0A%20%20Super-Resolution&body=Title%3A%20SR-CACO-2%3A%20A%20Dataset%20for%20Confocal%20Fluorescence%20Microscopy%20Image%0A%20%20Super-Resolution%0AAuthor%3A%20Soufiane%20Belharbi%20and%20Mara%20KM%20Whitford%20and%20Phuong%20Hoang%20and%20Shakeeb%20Murtaza%20and%20Luke%20McCaffrey%20and%20Eric%20Granger%0AAbstract%3A%20%20%20Confocal%20fluorescence%20microscopy%20is%20one%20of%20the%20most%20accessible%20and%20widely%0Aused%20imaging%20techniques%20for%20the%20study%20of%20biological%20processes%20at%20the%20cellular%0Aand%20subcellular%20levels.%20Scanning%20confocal%20microscopy%20allows%20the%20capture%20of%0Ahigh-quality%20images%20from%20thick%20three-dimensional%20%283D%29%20samples%2C%20yet%20suffers%20from%0Awell-known%20limitations%20such%20as%20photobleaching%20and%20phototoxicity%20of%20specimens%0Acaused%20by%20intense%20light%20exposure%2C%20limiting%20its%20applications.%20Cellular%20damage%0Acan%20be%20alleviated%20by%20changing%20imaging%20parameters%20to%20reduce%20light%20exposure%2C%0Aoften%20at%20the%20expense%20of%20image%20quality.%20Machine/deep%20learning%20methods%20for%0Asingle-image%20super-resolution%20%28SISR%29%20can%20be%20applied%20to%20restore%20image%20quality%20by%0Aupscaling%20lower-resolution%20%28LR%29%20images%20to%20yield%20high-resolution%20images%20%28HR%29.%0AThese%20SISR%20methods%20have%20been%20successfully%20applied%20to%20photo-realistic%20images%20due%0Apartly%20to%20the%20abundance%20of%20publicly%20available%20data.%20In%20contrast%2C%20the%20lack%20of%0Apublicly%20available%20data%20partly%20limits%20their%20application%20and%20success%20in%20scanning%0Aconfocal%20microscopy.%20In%20this%20paper%2C%20we%20introduce%20a%20large%20scanning%20confocal%0Amicroscopy%20dataset%20named%20SR-CACO-2%20that%20is%20comprised%20of%20low-%20and%0Ahigh-resolution%20image%20pairs%20marked%20for%20three%20different%20fluorescent%20markers.%20It%0Aallows%20the%20evaluation%20of%20performance%20of%20SISR%20methods%20on%20three%20different%0Aupscaling%20levels%20%28X2%2C%20X4%2C%20X8%29.%20SR-CACO-2%20contains%20the%20human%20epithelial%20cell%0Aline%20Caco-2%20%28ATCC%20HTB-37%29%2C%20and%20it%20is%20composed%20of%202%2C200%20unique%20images%2C%20captured%0Awith%20four%20resolutions%20and%20three%20markers%2C%20forming%209%2C937%20image%20patches%20for%20SISR%0Amethods.%20We%20provide%20benchmarking%20results%20for%2016%20state-of-the-art%20methods%20of%20the%0Amain%20SISR%20families.%20Results%20show%20that%20these%20methods%20have%20limited%20success%20in%0Aproducing%20high-resolution%20textures.%20The%20dataset%20is%20freely%20accessible%20under%20a%0ACreative%20Commons%20license%20%28CC%20BY-NC-SA%204.0%29.%20Our%20dataset%2C%20code%20and%20pretrained%0Aweights%20for%20SISR%20methods%20are%20available%3A%20https%3A//github.com/sbelharbi/sr-caco-2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09168v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSR-CACO-2%253A%2520A%2520Dataset%2520for%2520Confocal%2520Fluorescence%2520Microscopy%2520Image%250A%2520%2520Super-Resolution%26entry.906535625%3DSoufiane%2520Belharbi%2520and%2520Mara%2520KM%2520Whitford%2520and%2520Phuong%2520Hoang%2520and%2520Shakeeb%2520Murtaza%2520and%2520Luke%2520McCaffrey%2520and%2520Eric%2520Granger%26entry.1292438233%3D%2520%2520Confocal%2520fluorescence%2520microscopy%2520is%2520one%2520of%2520the%2520most%2520accessible%2520and%2520widely%250Aused%2520imaging%2520techniques%2520for%2520the%2520study%2520of%2520biological%2520processes%2520at%2520the%2520cellular%250Aand%2520subcellular%2520levels.%2520Scanning%2520confocal%2520microscopy%2520allows%2520the%2520capture%2520of%250Ahigh-quality%2520images%2520from%2520thick%2520three-dimensional%2520%25283D%2529%2520samples%252C%2520yet%2520suffers%2520from%250Awell-known%2520limitations%2520such%2520as%2520photobleaching%2520and%2520phototoxicity%2520of%2520specimens%250Acaused%2520by%2520intense%2520light%2520exposure%252C%2520limiting%2520its%2520applications.%2520Cellular%2520damage%250Acan%2520be%2520alleviated%2520by%2520changing%2520imaging%2520parameters%2520to%2520reduce%2520light%2520exposure%252C%250Aoften%2520at%2520the%2520expense%2520of%2520image%2520quality.%2520Machine/deep%2520learning%2520methods%2520for%250Asingle-image%2520super-resolution%2520%2528SISR%2529%2520can%2520be%2520applied%2520to%2520restore%2520image%2520quality%2520by%250Aupscaling%2520lower-resolution%2520%2528LR%2529%2520images%2520to%2520yield%2520high-resolution%2520images%2520%2528HR%2529.%250AThese%2520SISR%2520methods%2520have%2520been%2520successfully%2520applied%2520to%2520photo-realistic%2520images%2520due%250Apartly%2520to%2520the%2520abundance%2520of%2520publicly%2520available%2520data.%2520In%2520contrast%252C%2520the%2520lack%2520of%250Apublicly%2520available%2520data%2520partly%2520limits%2520their%2520application%2520and%2520success%2520in%2520scanning%250Aconfocal%2520microscopy.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520large%2520scanning%2520confocal%250Amicroscopy%2520dataset%2520named%2520SR-CACO-2%2520that%2520is%2520comprised%2520of%2520low-%2520and%250Ahigh-resolution%2520image%2520pairs%2520marked%2520for%2520three%2520different%2520fluorescent%2520markers.%2520It%250Aallows%2520the%2520evaluation%2520of%2520performance%2520of%2520SISR%2520methods%2520on%2520three%2520different%250Aupscaling%2520levels%2520%2528X2%252C%2520X4%252C%2520X8%2529.%2520SR-CACO-2%2520contains%2520the%2520human%2520epithelial%2520cell%250Aline%2520Caco-2%2520%2528ATCC%2520HTB-37%2529%252C%2520and%2520it%2520is%2520composed%2520of%25202%252C200%2520unique%2520images%252C%2520captured%250Awith%2520four%2520resolutions%2520and%2520three%2520markers%252C%2520forming%25209%252C937%2520image%2520patches%2520for%2520SISR%250Amethods.%2520We%2520provide%2520benchmarking%2520results%2520for%252016%2520state-of-the-art%2520methods%2520of%2520the%250Amain%2520SISR%2520families.%2520Results%2520show%2520that%2520these%2520methods%2520have%2520limited%2520success%2520in%250Aproducing%2520high-resolution%2520textures.%2520The%2520dataset%2520is%2520freely%2520accessible%2520under%2520a%250ACreative%2520Commons%2520license%2520%2528CC%2520BY-NC-SA%25204.0%2529.%2520Our%2520dataset%252C%2520code%2520and%2520pretrained%250Aweights%2520for%2520SISR%2520methods%2520are%2520available%253A%2520https%253A//github.com/sbelharbi/sr-caco-2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09168v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SR-CACO-2%3A%20A%20Dataset%20for%20Confocal%20Fluorescence%20Microscopy%20Image%0A%20%20Super-Resolution&entry.906535625=Soufiane%20Belharbi%20and%20Mara%20KM%20Whitford%20and%20Phuong%20Hoang%20and%20Shakeeb%20Murtaza%20and%20Luke%20McCaffrey%20and%20Eric%20Granger&entry.1292438233=%20%20Confocal%20fluorescence%20microscopy%20is%20one%20of%20the%20most%20accessible%20and%20widely%0Aused%20imaging%20techniques%20for%20the%20study%20of%20biological%20processes%20at%20the%20cellular%0Aand%20subcellular%20levels.%20Scanning%20confocal%20microscopy%20allows%20the%20capture%20of%0Ahigh-quality%20images%20from%20thick%20three-dimensional%20%283D%29%20samples%2C%20yet%20suffers%20from%0Awell-known%20limitations%20such%20as%20photobleaching%20and%20phototoxicity%20of%20specimens%0Acaused%20by%20intense%20light%20exposure%2C%20limiting%20its%20applications.%20Cellular%20damage%0Acan%20be%20alleviated%20by%20changing%20imaging%20parameters%20to%20reduce%20light%20exposure%2C%0Aoften%20at%20the%20expense%20of%20image%20quality.%20Machine/deep%20learning%20methods%20for%0Asingle-image%20super-resolution%20%28SISR%29%20can%20be%20applied%20to%20restore%20image%20quality%20by%0Aupscaling%20lower-resolution%20%28LR%29%20images%20to%20yield%20high-resolution%20images%20%28HR%29.%0AThese%20SISR%20methods%20have%20been%20successfully%20applied%20to%20photo-realistic%20images%20due%0Apartly%20to%20the%20abundance%20of%20publicly%20available%20data.%20In%20contrast%2C%20the%20lack%20of%0Apublicly%20available%20data%20partly%20limits%20their%20application%20and%20success%20in%20scanning%0Aconfocal%20microscopy.%20In%20this%20paper%2C%20we%20introduce%20a%20large%20scanning%20confocal%0Amicroscopy%20dataset%20named%20SR-CACO-2%20that%20is%20comprised%20of%20low-%20and%0Ahigh-resolution%20image%20pairs%20marked%20for%20three%20different%20fluorescent%20markers.%20It%0Aallows%20the%20evaluation%20of%20performance%20of%20SISR%20methods%20on%20three%20different%0Aupscaling%20levels%20%28X2%2C%20X4%2C%20X8%29.%20SR-CACO-2%20contains%20the%20human%20epithelial%20cell%0Aline%20Caco-2%20%28ATCC%20HTB-37%29%2C%20and%20it%20is%20composed%20of%202%2C200%20unique%20images%2C%20captured%0Awith%20four%20resolutions%20and%20three%20markers%2C%20forming%209%2C937%20image%20patches%20for%20SISR%0Amethods.%20We%20provide%20benchmarking%20results%20for%2016%20state-of-the-art%20methods%20of%20the%0Amain%20SISR%20families.%20Results%20show%20that%20these%20methods%20have%20limited%20success%20in%0Aproducing%20high-resolution%20textures.%20The%20dataset%20is%20freely%20accessible%20under%20a%0ACreative%20Commons%20license%20%28CC%20BY-NC-SA%204.0%29.%20Our%20dataset%2C%20code%20and%20pretrained%0Aweights%20for%20SISR%20methods%20are%20available%3A%20https%3A//github.com/sbelharbi/sr-caco-2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09168v2&entry.124074799=Read"},
{"title": "Constraint Back-translation Improves Complex Instruction Following of\n  Large Language Models", "author": "Yunjia Qi and Hao Peng and Xiaozhi Wang and Bin Xu and Lei Hou and Juanzi Li", "abstract": "  Large language models (LLMs) struggle to follow instructions with complex\nconstraints in format, length, etc. Following the conventional\ninstruction-tuning practice, previous works conduct post-training on complex\ninstruction-response pairs generated by feeding complex instructions to\nadvanced LLMs. However, even advanced LLMs cannot follow complex instructions\nwell, thus limiting the quality of generated data. In this work, we find that\nexisting datasets inherently contain implicit complex constraints and propose a\nnovel data generation technique, constraint back-translation. Specifically, we\ntake the high-quality instruction-response pairs in existing datasets and only\nadopt advanced LLMs to add complex constraints already met by the responses to\nthe instructions, which naturally reduces costs and data noise. In the\nexperiments, we adopt Llama3-70B-Instruct to back-translate constraints and\ncreate a high-quality complex instruction-response dataset, named CRAB. We\npresent that post-training on CRAB improves multiple backbone LLMs' complex\ninstruction-following ability, evaluated on extensive instruction-following\nbenchmarks. We further find that constraint back-translation also serves as a\nuseful auxiliary training objective in post-training. Our code, data, and\nmodels will be released to facilitate future research.\n", "link": "http://arxiv.org/abs/2410.24175v1", "date": "2024-10-31", "relevancy": 2.5824, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5294}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5294}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Constraint%20Back-translation%20Improves%20Complex%20Instruction%20Following%20of%0A%20%20Large%20Language%20Models&body=Title%3A%20Constraint%20Back-translation%20Improves%20Complex%20Instruction%20Following%20of%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Yunjia%20Qi%20and%20Hao%20Peng%20and%20Xiaozhi%20Wang%20and%20Bin%20Xu%20and%20Lei%20Hou%20and%20Juanzi%20Li%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20struggle%20to%20follow%20instructions%20with%20complex%0Aconstraints%20in%20format%2C%20length%2C%20etc.%20Following%20the%20conventional%0Ainstruction-tuning%20practice%2C%20previous%20works%20conduct%20post-training%20on%20complex%0Ainstruction-response%20pairs%20generated%20by%20feeding%20complex%20instructions%20to%0Aadvanced%20LLMs.%20However%2C%20even%20advanced%20LLMs%20cannot%20follow%20complex%20instructions%0Awell%2C%20thus%20limiting%20the%20quality%20of%20generated%20data.%20In%20this%20work%2C%20we%20find%20that%0Aexisting%20datasets%20inherently%20contain%20implicit%20complex%20constraints%20and%20propose%20a%0Anovel%20data%20generation%20technique%2C%20constraint%20back-translation.%20Specifically%2C%20we%0Atake%20the%20high-quality%20instruction-response%20pairs%20in%20existing%20datasets%20and%20only%0Aadopt%20advanced%20LLMs%20to%20add%20complex%20constraints%20already%20met%20by%20the%20responses%20to%0Athe%20instructions%2C%20which%20naturally%20reduces%20costs%20and%20data%20noise.%20In%20the%0Aexperiments%2C%20we%20adopt%20Llama3-70B-Instruct%20to%20back-translate%20constraints%20and%0Acreate%20a%20high-quality%20complex%20instruction-response%20dataset%2C%20named%20CRAB.%20We%0Apresent%20that%20post-training%20on%20CRAB%20improves%20multiple%20backbone%20LLMs%27%20complex%0Ainstruction-following%20ability%2C%20evaluated%20on%20extensive%20instruction-following%0Abenchmarks.%20We%20further%20find%20that%20constraint%20back-translation%20also%20serves%20as%20a%0Auseful%20auxiliary%20training%20objective%20in%20post-training.%20Our%20code%2C%20data%2C%20and%0Amodels%20will%20be%20released%20to%20facilitate%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConstraint%2520Back-translation%2520Improves%2520Complex%2520Instruction%2520Following%2520of%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DYunjia%2520Qi%2520and%2520Hao%2520Peng%2520and%2520Xiaozhi%2520Wang%2520and%2520Bin%2520Xu%2520and%2520Lei%2520Hou%2520and%2520Juanzi%2520Li%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520struggle%2520to%2520follow%2520instructions%2520with%2520complex%250Aconstraints%2520in%2520format%252C%2520length%252C%2520etc.%2520Following%2520the%2520conventional%250Ainstruction-tuning%2520practice%252C%2520previous%2520works%2520conduct%2520post-training%2520on%2520complex%250Ainstruction-response%2520pairs%2520generated%2520by%2520feeding%2520complex%2520instructions%2520to%250Aadvanced%2520LLMs.%2520However%252C%2520even%2520advanced%2520LLMs%2520cannot%2520follow%2520complex%2520instructions%250Awell%252C%2520thus%2520limiting%2520the%2520quality%2520of%2520generated%2520data.%2520In%2520this%2520work%252C%2520we%2520find%2520that%250Aexisting%2520datasets%2520inherently%2520contain%2520implicit%2520complex%2520constraints%2520and%2520propose%2520a%250Anovel%2520data%2520generation%2520technique%252C%2520constraint%2520back-translation.%2520Specifically%252C%2520we%250Atake%2520the%2520high-quality%2520instruction-response%2520pairs%2520in%2520existing%2520datasets%2520and%2520only%250Aadopt%2520advanced%2520LLMs%2520to%2520add%2520complex%2520constraints%2520already%2520met%2520by%2520the%2520responses%2520to%250Athe%2520instructions%252C%2520which%2520naturally%2520reduces%2520costs%2520and%2520data%2520noise.%2520In%2520the%250Aexperiments%252C%2520we%2520adopt%2520Llama3-70B-Instruct%2520to%2520back-translate%2520constraints%2520and%250Acreate%2520a%2520high-quality%2520complex%2520instruction-response%2520dataset%252C%2520named%2520CRAB.%2520We%250Apresent%2520that%2520post-training%2520on%2520CRAB%2520improves%2520multiple%2520backbone%2520LLMs%2527%2520complex%250Ainstruction-following%2520ability%252C%2520evaluated%2520on%2520extensive%2520instruction-following%250Abenchmarks.%2520We%2520further%2520find%2520that%2520constraint%2520back-translation%2520also%2520serves%2520as%2520a%250Auseful%2520auxiliary%2520training%2520objective%2520in%2520post-training.%2520Our%2520code%252C%2520data%252C%2520and%250Amodels%2520will%2520be%2520released%2520to%2520facilitate%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Constraint%20Back-translation%20Improves%20Complex%20Instruction%20Following%20of%0A%20%20Large%20Language%20Models&entry.906535625=Yunjia%20Qi%20and%20Hao%20Peng%20and%20Xiaozhi%20Wang%20and%20Bin%20Xu%20and%20Lei%20Hou%20and%20Juanzi%20Li&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20struggle%20to%20follow%20instructions%20with%20complex%0Aconstraints%20in%20format%2C%20length%2C%20etc.%20Following%20the%20conventional%0Ainstruction-tuning%20practice%2C%20previous%20works%20conduct%20post-training%20on%20complex%0Ainstruction-response%20pairs%20generated%20by%20feeding%20complex%20instructions%20to%0Aadvanced%20LLMs.%20However%2C%20even%20advanced%20LLMs%20cannot%20follow%20complex%20instructions%0Awell%2C%20thus%20limiting%20the%20quality%20of%20generated%20data.%20In%20this%20work%2C%20we%20find%20that%0Aexisting%20datasets%20inherently%20contain%20implicit%20complex%20constraints%20and%20propose%20a%0Anovel%20data%20generation%20technique%2C%20constraint%20back-translation.%20Specifically%2C%20we%0Atake%20the%20high-quality%20instruction-response%20pairs%20in%20existing%20datasets%20and%20only%0Aadopt%20advanced%20LLMs%20to%20add%20complex%20constraints%20already%20met%20by%20the%20responses%20to%0Athe%20instructions%2C%20which%20naturally%20reduces%20costs%20and%20data%20noise.%20In%20the%0Aexperiments%2C%20we%20adopt%20Llama3-70B-Instruct%20to%20back-translate%20constraints%20and%0Acreate%20a%20high-quality%20complex%20instruction-response%20dataset%2C%20named%20CRAB.%20We%0Apresent%20that%20post-training%20on%20CRAB%20improves%20multiple%20backbone%20LLMs%27%20complex%0Ainstruction-following%20ability%2C%20evaluated%20on%20extensive%20instruction-following%0Abenchmarks.%20We%20further%20find%20that%20constraint%20back-translation%20also%20serves%20as%20a%0Auseful%20auxiliary%20training%20objective%20in%20post-training.%20Our%20code%2C%20data%2C%20and%0Amodels%20will%20be%20released%20to%20facilitate%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24175v1&entry.124074799=Read"},
{"title": "Unlearning as multi-task optimization: A normalized gradient difference\n  approach with an adaptive learning rate", "author": "Zhiqi Bu and Xiaomeng Jin and Bhanukiran Vinzamuri and Anil Ramakrishna and Kai-Wei Chang and Volkan Cevher and Mingyi Hong", "abstract": "  Machine unlearning has been used to remove unwanted knowledge acquired by\nlarge language models (LLMs). In this paper, we examine machine unlearning from\nan optimization perspective, framing it as a regularized multi-task\noptimization problem, where one task optimizes a forgetting objective and\nanother optimizes the model performance. In particular, we introduce a\nnormalized gradient difference (NGDiff) algorithm, enabling us to have better\ncontrol over the trade-off between the objectives, while integrating a new,\nautomatic learning rate scheduler. We provide a theoretical analysis and\nempirically demonstrate the superior performance of NGDiff among\nstate-of-the-art unlearning methods on the TOFU and MUSE datasets while\nexhibiting stable training.\n", "link": "http://arxiv.org/abs/2410.22086v2", "date": "2024-10-31", "relevancy": 2.581, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.526}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5152}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unlearning%20as%20multi-task%20optimization%3A%20A%20normalized%20gradient%20difference%0A%20%20approach%20with%20an%20adaptive%20learning%20rate&body=Title%3A%20Unlearning%20as%20multi-task%20optimization%3A%20A%20normalized%20gradient%20difference%0A%20%20approach%20with%20an%20adaptive%20learning%20rate%0AAuthor%3A%20Zhiqi%20Bu%20and%20Xiaomeng%20Jin%20and%20Bhanukiran%20Vinzamuri%20and%20Anil%20Ramakrishna%20and%20Kai-Wei%20Chang%20and%20Volkan%20Cevher%20and%20Mingyi%20Hong%0AAbstract%3A%20%20%20Machine%20unlearning%20has%20been%20used%20to%20remove%20unwanted%20knowledge%20acquired%20by%0Alarge%20language%20models%20%28LLMs%29.%20In%20this%20paper%2C%20we%20examine%20machine%20unlearning%20from%0Aan%20optimization%20perspective%2C%20framing%20it%20as%20a%20regularized%20multi-task%0Aoptimization%20problem%2C%20where%20one%20task%20optimizes%20a%20forgetting%20objective%20and%0Aanother%20optimizes%20the%20model%20performance.%20In%20particular%2C%20we%20introduce%20a%0Anormalized%20gradient%20difference%20%28NGDiff%29%20algorithm%2C%20enabling%20us%20to%20have%20better%0Acontrol%20over%20the%20trade-off%20between%20the%20objectives%2C%20while%20integrating%20a%20new%2C%0Aautomatic%20learning%20rate%20scheduler.%20We%20provide%20a%20theoretical%20analysis%20and%0Aempirically%20demonstrate%20the%20superior%20performance%20of%20NGDiff%20among%0Astate-of-the-art%20unlearning%20methods%20on%20the%20TOFU%20and%20MUSE%20datasets%20while%0Aexhibiting%20stable%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.22086v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnlearning%2520as%2520multi-task%2520optimization%253A%2520A%2520normalized%2520gradient%2520difference%250A%2520%2520approach%2520with%2520an%2520adaptive%2520learning%2520rate%26entry.906535625%3DZhiqi%2520Bu%2520and%2520Xiaomeng%2520Jin%2520and%2520Bhanukiran%2520Vinzamuri%2520and%2520Anil%2520Ramakrishna%2520and%2520Kai-Wei%2520Chang%2520and%2520Volkan%2520Cevher%2520and%2520Mingyi%2520Hong%26entry.1292438233%3D%2520%2520Machine%2520unlearning%2520has%2520been%2520used%2520to%2520remove%2520unwanted%2520knowledge%2520acquired%2520by%250Alarge%2520language%2520models%2520%2528LLMs%2529.%2520In%2520this%2520paper%252C%2520we%2520examine%2520machine%2520unlearning%2520from%250Aan%2520optimization%2520perspective%252C%2520framing%2520it%2520as%2520a%2520regularized%2520multi-task%250Aoptimization%2520problem%252C%2520where%2520one%2520task%2520optimizes%2520a%2520forgetting%2520objective%2520and%250Aanother%2520optimizes%2520the%2520model%2520performance.%2520In%2520particular%252C%2520we%2520introduce%2520a%250Anormalized%2520gradient%2520difference%2520%2528NGDiff%2529%2520algorithm%252C%2520enabling%2520us%2520to%2520have%2520better%250Acontrol%2520over%2520the%2520trade-off%2520between%2520the%2520objectives%252C%2520while%2520integrating%2520a%2520new%252C%250Aautomatic%2520learning%2520rate%2520scheduler.%2520We%2520provide%2520a%2520theoretical%2520analysis%2520and%250Aempirically%2520demonstrate%2520the%2520superior%2520performance%2520of%2520NGDiff%2520among%250Astate-of-the-art%2520unlearning%2520methods%2520on%2520the%2520TOFU%2520and%2520MUSE%2520datasets%2520while%250Aexhibiting%2520stable%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.22086v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unlearning%20as%20multi-task%20optimization%3A%20A%20normalized%20gradient%20difference%0A%20%20approach%20with%20an%20adaptive%20learning%20rate&entry.906535625=Zhiqi%20Bu%20and%20Xiaomeng%20Jin%20and%20Bhanukiran%20Vinzamuri%20and%20Anil%20Ramakrishna%20and%20Kai-Wei%20Chang%20and%20Volkan%20Cevher%20and%20Mingyi%20Hong&entry.1292438233=%20%20Machine%20unlearning%20has%20been%20used%20to%20remove%20unwanted%20knowledge%20acquired%20by%0Alarge%20language%20models%20%28LLMs%29.%20In%20this%20paper%2C%20we%20examine%20machine%20unlearning%20from%0Aan%20optimization%20perspective%2C%20framing%20it%20as%20a%20regularized%20multi-task%0Aoptimization%20problem%2C%20where%20one%20task%20optimizes%20a%20forgetting%20objective%20and%0Aanother%20optimizes%20the%20model%20performance.%20In%20particular%2C%20we%20introduce%20a%0Anormalized%20gradient%20difference%20%28NGDiff%29%20algorithm%2C%20enabling%20us%20to%20have%20better%0Acontrol%20over%20the%20trade-off%20between%20the%20objectives%2C%20while%20integrating%20a%20new%2C%0Aautomatic%20learning%20rate%20scheduler.%20We%20provide%20a%20theoretical%20analysis%20and%0Aempirically%20demonstrate%20the%20superior%20performance%20of%20NGDiff%20among%0Astate-of-the-art%20unlearning%20methods%20on%20the%20TOFU%20and%20MUSE%20datasets%20while%0Aexhibiting%20stable%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.22086v2&entry.124074799=Read"},
{"title": "RAGraph: A General Retrieval-Augmented Graph Learning Framework", "author": "Xinke Jiang and Rihong Qiu and Yongxin Xu and Wentao Zhang and Yichen Zhu and Ruizhe Zhang and Yuchen Fang and Xu Chu and Junfeng Zhao and Yasha Wang", "abstract": "  Graph Neural Networks (GNNs) have become essential in interpreting relational\ndata across various domains, yet, they often struggle to generalize to unseen\ngraph data that differs markedly from training instances. In this paper, we\nintroduce a novel framework called General Retrieval-Augmented Graph Learning\n(RAGraph), which brings external graph data into the general graph foundation\nmodel to improve model generalization on unseen scenarios. On the top of our\nframework is a toy graph vector library that we established, which captures key\nattributes, such as features and task-specific label information. During\ninference, the RAGraph adeptly retrieves similar toy graphs based on key\nsimilarities in downstream tasks, integrating the retrieved data to enrich the\nlearning context via the message-passing prompting mechanism. Our extensive\nexperimental evaluations demonstrate that RAGraph significantly outperforms\nstate-of-the-art graph learning methods in multiple tasks such as node\nclassification, link prediction, and graph classification across both dynamic\nand static datasets. Furthermore, extensive testing confirms that RAGraph\nconsistently maintains high performance without the need for task-specific\nfine-tuning, highlighting its adaptability, robustness, and broad\napplicability.\n", "link": "http://arxiv.org/abs/2410.23855v1", "date": "2024-10-31", "relevancy": 2.5506, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5301}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5221}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RAGraph%3A%20A%20General%20Retrieval-Augmented%20Graph%20Learning%20Framework&body=Title%3A%20RAGraph%3A%20A%20General%20Retrieval-Augmented%20Graph%20Learning%20Framework%0AAuthor%3A%20Xinke%20Jiang%20and%20Rihong%20Qiu%20and%20Yongxin%20Xu%20and%20Wentao%20Zhang%20and%20Yichen%20Zhu%20and%20Ruizhe%20Zhang%20and%20Yuchen%20Fang%20and%20Xu%20Chu%20and%20Junfeng%20Zhao%20and%20Yasha%20Wang%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20essential%20in%20interpreting%20relational%0Adata%20across%20various%20domains%2C%20yet%2C%20they%20often%20struggle%20to%20generalize%20to%20unseen%0Agraph%20data%20that%20differs%20markedly%20from%20training%20instances.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20framework%20called%20General%20Retrieval-Augmented%20Graph%20Learning%0A%28RAGraph%29%2C%20which%20brings%20external%20graph%20data%20into%20the%20general%20graph%20foundation%0Amodel%20to%20improve%20model%20generalization%20on%20unseen%20scenarios.%20On%20the%20top%20of%20our%0Aframework%20is%20a%20toy%20graph%20vector%20library%20that%20we%20established%2C%20which%20captures%20key%0Aattributes%2C%20such%20as%20features%20and%20task-specific%20label%20information.%20During%0Ainference%2C%20the%20RAGraph%20adeptly%20retrieves%20similar%20toy%20graphs%20based%20on%20key%0Asimilarities%20in%20downstream%20tasks%2C%20integrating%20the%20retrieved%20data%20to%20enrich%20the%0Alearning%20context%20via%20the%20message-passing%20prompting%20mechanism.%20Our%20extensive%0Aexperimental%20evaluations%20demonstrate%20that%20RAGraph%20significantly%20outperforms%0Astate-of-the-art%20graph%20learning%20methods%20in%20multiple%20tasks%20such%20as%20node%0Aclassification%2C%20link%20prediction%2C%20and%20graph%20classification%20across%20both%20dynamic%0Aand%20static%20datasets.%20Furthermore%2C%20extensive%20testing%20confirms%20that%20RAGraph%0Aconsistently%20maintains%20high%20performance%20without%20the%20need%20for%20task-specific%0Afine-tuning%2C%20highlighting%20its%20adaptability%2C%20robustness%2C%20and%20broad%0Aapplicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRAGraph%253A%2520A%2520General%2520Retrieval-Augmented%2520Graph%2520Learning%2520Framework%26entry.906535625%3DXinke%2520Jiang%2520and%2520Rihong%2520Qiu%2520and%2520Yongxin%2520Xu%2520and%2520Wentao%2520Zhang%2520and%2520Yichen%2520Zhu%2520and%2520Ruizhe%2520Zhang%2520and%2520Yuchen%2520Fang%2520and%2520Xu%2520Chu%2520and%2520Junfeng%2520Zhao%2520and%2520Yasha%2520Wang%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520become%2520essential%2520in%2520interpreting%2520relational%250Adata%2520across%2520various%2520domains%252C%2520yet%252C%2520they%2520often%2520struggle%2520to%2520generalize%2520to%2520unseen%250Agraph%2520data%2520that%2520differs%2520markedly%2520from%2520training%2520instances.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520novel%2520framework%2520called%2520General%2520Retrieval-Augmented%2520Graph%2520Learning%250A%2528RAGraph%2529%252C%2520which%2520brings%2520external%2520graph%2520data%2520into%2520the%2520general%2520graph%2520foundation%250Amodel%2520to%2520improve%2520model%2520generalization%2520on%2520unseen%2520scenarios.%2520On%2520the%2520top%2520of%2520our%250Aframework%2520is%2520a%2520toy%2520graph%2520vector%2520library%2520that%2520we%2520established%252C%2520which%2520captures%2520key%250Aattributes%252C%2520such%2520as%2520features%2520and%2520task-specific%2520label%2520information.%2520During%250Ainference%252C%2520the%2520RAGraph%2520adeptly%2520retrieves%2520similar%2520toy%2520graphs%2520based%2520on%2520key%250Asimilarities%2520in%2520downstream%2520tasks%252C%2520integrating%2520the%2520retrieved%2520data%2520to%2520enrich%2520the%250Alearning%2520context%2520via%2520the%2520message-passing%2520prompting%2520mechanism.%2520Our%2520extensive%250Aexperimental%2520evaluations%2520demonstrate%2520that%2520RAGraph%2520significantly%2520outperforms%250Astate-of-the-art%2520graph%2520learning%2520methods%2520in%2520multiple%2520tasks%2520such%2520as%2520node%250Aclassification%252C%2520link%2520prediction%252C%2520and%2520graph%2520classification%2520across%2520both%2520dynamic%250Aand%2520static%2520datasets.%2520Furthermore%252C%2520extensive%2520testing%2520confirms%2520that%2520RAGraph%250Aconsistently%2520maintains%2520high%2520performance%2520without%2520the%2520need%2520for%2520task-specific%250Afine-tuning%252C%2520highlighting%2520its%2520adaptability%252C%2520robustness%252C%2520and%2520broad%250Aapplicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RAGraph%3A%20A%20General%20Retrieval-Augmented%20Graph%20Learning%20Framework&entry.906535625=Xinke%20Jiang%20and%20Rihong%20Qiu%20and%20Yongxin%20Xu%20and%20Wentao%20Zhang%20and%20Yichen%20Zhu%20and%20Ruizhe%20Zhang%20and%20Yuchen%20Fang%20and%20Xu%20Chu%20and%20Junfeng%20Zhao%20and%20Yasha%20Wang&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20essential%20in%20interpreting%20relational%0Adata%20across%20various%20domains%2C%20yet%2C%20they%20often%20struggle%20to%20generalize%20to%20unseen%0Agraph%20data%20that%20differs%20markedly%20from%20training%20instances.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20framework%20called%20General%20Retrieval-Augmented%20Graph%20Learning%0A%28RAGraph%29%2C%20which%20brings%20external%20graph%20data%20into%20the%20general%20graph%20foundation%0Amodel%20to%20improve%20model%20generalization%20on%20unseen%20scenarios.%20On%20the%20top%20of%20our%0Aframework%20is%20a%20toy%20graph%20vector%20library%20that%20we%20established%2C%20which%20captures%20key%0Aattributes%2C%20such%20as%20features%20and%20task-specific%20label%20information.%20During%0Ainference%2C%20the%20RAGraph%20adeptly%20retrieves%20similar%20toy%20graphs%20based%20on%20key%0Asimilarities%20in%20downstream%20tasks%2C%20integrating%20the%20retrieved%20data%20to%20enrich%20the%0Alearning%20context%20via%20the%20message-passing%20prompting%20mechanism.%20Our%20extensive%0Aexperimental%20evaluations%20demonstrate%20that%20RAGraph%20significantly%20outperforms%0Astate-of-the-art%20graph%20learning%20methods%20in%20multiple%20tasks%20such%20as%20node%0Aclassification%2C%20link%20prediction%2C%20and%20graph%20classification%20across%20both%20dynamic%0Aand%20static%20datasets.%20Furthermore%2C%20extensive%20testing%20confirms%20that%20RAGraph%0Aconsistently%20maintains%20high%20performance%20without%20the%20need%20for%20task-specific%0Afine-tuning%2C%20highlighting%20its%20adaptability%2C%20robustness%2C%20and%20broad%0Aapplicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23855v1&entry.124074799=Read"},
{"title": "DiffPano: Scalable and Consistent Text to Panorama Generation with\n  Spherical Epipolar-Aware Diffusion", "author": "Weicai Ye and Chenhao Ji and Zheng Chen and Junyao Gao and Xiaoshui Huang and Song-Hai Zhang and Wanli Ouyang and Tong He and Cairong Zhao and Guofeng Zhang", "abstract": "  Diffusion-based methods have achieved remarkable achievements in 2D image or\n3D object generation, however, the generation of 3D scenes and even\n$360^{\\circ}$ images remains constrained, due to the limited number of scene\ndatasets, the complexity of 3D scenes themselves, and the difficulty of\ngenerating consistent multi-view images. To address these issues, we first\nestablish a large-scale panoramic video-text dataset containing millions of\nconsecutive panoramic keyframes with corresponding panoramic depths, camera\nposes, and text descriptions. Then, we propose a novel text-driven panoramic\ngeneration framework, termed DiffPano, to achieve scalable, consistent, and\ndiverse panoramic scene generation. Specifically, benefiting from the powerful\ngenerative capabilities of stable diffusion, we fine-tune a single-view\ntext-to-panorama diffusion model with LoRA on the established panoramic\nvideo-text dataset. We further design a spherical epipolar-aware multi-view\ndiffusion model to ensure the multi-view consistency of the generated panoramic\nimages. Extensive experiments demonstrate that DiffPano can generate scalable,\nconsistent, and diverse panoramic images with given unseen text descriptions\nand camera poses.\n", "link": "http://arxiv.org/abs/2410.24203v1", "date": "2024-10-31", "relevancy": 2.5453, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6455}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6455}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffPano%3A%20Scalable%20and%20Consistent%20Text%20to%20Panorama%20Generation%20with%0A%20%20Spherical%20Epipolar-Aware%20Diffusion&body=Title%3A%20DiffPano%3A%20Scalable%20and%20Consistent%20Text%20to%20Panorama%20Generation%20with%0A%20%20Spherical%20Epipolar-Aware%20Diffusion%0AAuthor%3A%20Weicai%20Ye%20and%20Chenhao%20Ji%20and%20Zheng%20Chen%20and%20Junyao%20Gao%20and%20Xiaoshui%20Huang%20and%20Song-Hai%20Zhang%20and%20Wanli%20Ouyang%20and%20Tong%20He%20and%20Cairong%20Zhao%20and%20Guofeng%20Zhang%0AAbstract%3A%20%20%20Diffusion-based%20methods%20have%20achieved%20remarkable%20achievements%20in%202D%20image%20or%0A3D%20object%20generation%2C%20however%2C%20the%20generation%20of%203D%20scenes%20and%20even%0A%24360%5E%7B%5Ccirc%7D%24%20images%20remains%20constrained%2C%20due%20to%20the%20limited%20number%20of%20scene%0Adatasets%2C%20the%20complexity%20of%203D%20scenes%20themselves%2C%20and%20the%20difficulty%20of%0Agenerating%20consistent%20multi-view%20images.%20To%20address%20these%20issues%2C%20we%20first%0Aestablish%20a%20large-scale%20panoramic%20video-text%20dataset%20containing%20millions%20of%0Aconsecutive%20panoramic%20keyframes%20with%20corresponding%20panoramic%20depths%2C%20camera%0Aposes%2C%20and%20text%20descriptions.%20Then%2C%20we%20propose%20a%20novel%20text-driven%20panoramic%0Ageneration%20framework%2C%20termed%20DiffPano%2C%20to%20achieve%20scalable%2C%20consistent%2C%20and%0Adiverse%20panoramic%20scene%20generation.%20Specifically%2C%20benefiting%20from%20the%20powerful%0Agenerative%20capabilities%20of%20stable%20diffusion%2C%20we%20fine-tune%20a%20single-view%0Atext-to-panorama%20diffusion%20model%20with%20LoRA%20on%20the%20established%20panoramic%0Avideo-text%20dataset.%20We%20further%20design%20a%20spherical%20epipolar-aware%20multi-view%0Adiffusion%20model%20to%20ensure%20the%20multi-view%20consistency%20of%20the%20generated%20panoramic%0Aimages.%20Extensive%20experiments%20demonstrate%20that%20DiffPano%20can%20generate%20scalable%2C%0Aconsistent%2C%20and%20diverse%20panoramic%20images%20with%20given%20unseen%20text%20descriptions%0Aand%20camera%20poses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24203v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffPano%253A%2520Scalable%2520and%2520Consistent%2520Text%2520to%2520Panorama%2520Generation%2520with%250A%2520%2520Spherical%2520Epipolar-Aware%2520Diffusion%26entry.906535625%3DWeicai%2520Ye%2520and%2520Chenhao%2520Ji%2520and%2520Zheng%2520Chen%2520and%2520Junyao%2520Gao%2520and%2520Xiaoshui%2520Huang%2520and%2520Song-Hai%2520Zhang%2520and%2520Wanli%2520Ouyang%2520and%2520Tong%2520He%2520and%2520Cairong%2520Zhao%2520and%2520Guofeng%2520Zhang%26entry.1292438233%3D%2520%2520Diffusion-based%2520methods%2520have%2520achieved%2520remarkable%2520achievements%2520in%25202D%2520image%2520or%250A3D%2520object%2520generation%252C%2520however%252C%2520the%2520generation%2520of%25203D%2520scenes%2520and%2520even%250A%2524360%255E%257B%255Ccirc%257D%2524%2520images%2520remains%2520constrained%252C%2520due%2520to%2520the%2520limited%2520number%2520of%2520scene%250Adatasets%252C%2520the%2520complexity%2520of%25203D%2520scenes%2520themselves%252C%2520and%2520the%2520difficulty%2520of%250Agenerating%2520consistent%2520multi-view%2520images.%2520To%2520address%2520these%2520issues%252C%2520we%2520first%250Aestablish%2520a%2520large-scale%2520panoramic%2520video-text%2520dataset%2520containing%2520millions%2520of%250Aconsecutive%2520panoramic%2520keyframes%2520with%2520corresponding%2520panoramic%2520depths%252C%2520camera%250Aposes%252C%2520and%2520text%2520descriptions.%2520Then%252C%2520we%2520propose%2520a%2520novel%2520text-driven%2520panoramic%250Ageneration%2520framework%252C%2520termed%2520DiffPano%252C%2520to%2520achieve%2520scalable%252C%2520consistent%252C%2520and%250Adiverse%2520panoramic%2520scene%2520generation.%2520Specifically%252C%2520benefiting%2520from%2520the%2520powerful%250Agenerative%2520capabilities%2520of%2520stable%2520diffusion%252C%2520we%2520fine-tune%2520a%2520single-view%250Atext-to-panorama%2520diffusion%2520model%2520with%2520LoRA%2520on%2520the%2520established%2520panoramic%250Avideo-text%2520dataset.%2520We%2520further%2520design%2520a%2520spherical%2520epipolar-aware%2520multi-view%250Adiffusion%2520model%2520to%2520ensure%2520the%2520multi-view%2520consistency%2520of%2520the%2520generated%2520panoramic%250Aimages.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DiffPano%2520can%2520generate%2520scalable%252C%250Aconsistent%252C%2520and%2520diverse%2520panoramic%2520images%2520with%2520given%2520unseen%2520text%2520descriptions%250Aand%2520camera%2520poses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24203v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffPano%3A%20Scalable%20and%20Consistent%20Text%20to%20Panorama%20Generation%20with%0A%20%20Spherical%20Epipolar-Aware%20Diffusion&entry.906535625=Weicai%20Ye%20and%20Chenhao%20Ji%20and%20Zheng%20Chen%20and%20Junyao%20Gao%20and%20Xiaoshui%20Huang%20and%20Song-Hai%20Zhang%20and%20Wanli%20Ouyang%20and%20Tong%20He%20and%20Cairong%20Zhao%20and%20Guofeng%20Zhang&entry.1292438233=%20%20Diffusion-based%20methods%20have%20achieved%20remarkable%20achievements%20in%202D%20image%20or%0A3D%20object%20generation%2C%20however%2C%20the%20generation%20of%203D%20scenes%20and%20even%0A%24360%5E%7B%5Ccirc%7D%24%20images%20remains%20constrained%2C%20due%20to%20the%20limited%20number%20of%20scene%0Adatasets%2C%20the%20complexity%20of%203D%20scenes%20themselves%2C%20and%20the%20difficulty%20of%0Agenerating%20consistent%20multi-view%20images.%20To%20address%20these%20issues%2C%20we%20first%0Aestablish%20a%20large-scale%20panoramic%20video-text%20dataset%20containing%20millions%20of%0Aconsecutive%20panoramic%20keyframes%20with%20corresponding%20panoramic%20depths%2C%20camera%0Aposes%2C%20and%20text%20descriptions.%20Then%2C%20we%20propose%20a%20novel%20text-driven%20panoramic%0Ageneration%20framework%2C%20termed%20DiffPano%2C%20to%20achieve%20scalable%2C%20consistent%2C%20and%0Adiverse%20panoramic%20scene%20generation.%20Specifically%2C%20benefiting%20from%20the%20powerful%0Agenerative%20capabilities%20of%20stable%20diffusion%2C%20we%20fine-tune%20a%20single-view%0Atext-to-panorama%20diffusion%20model%20with%20LoRA%20on%20the%20established%20panoramic%0Avideo-text%20dataset.%20We%20further%20design%20a%20spherical%20epipolar-aware%20multi-view%0Adiffusion%20model%20to%20ensure%20the%20multi-view%20consistency%20of%20the%20generated%20panoramic%0Aimages.%20Extensive%20experiments%20demonstrate%20that%20DiffPano%20can%20generate%20scalable%2C%0Aconsistent%2C%20and%20diverse%20panoramic%20images%20with%20given%20unseen%20text%20descriptions%0Aand%20camera%20poses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24203v1&entry.124074799=Read"},
{"title": "Discovery of the Hidden World with Large Language Models", "author": "Chenxi Liu and Yongqiang Chen and Tongliang Liu and Mingming Gong and James Cheng and Bo Han and Kun Zhang", "abstract": "  Revealing the underlying causal mechanisms in the real world is the key to\nthe development of science. Despite the progress in the past decades,\ntraditional causal discovery approaches (CDs) mainly rely on high-quality\nmeasured variables, usually given by human experts, to find causal relations.\nThe lack of well-defined high-level variables in many real-world applications\nhas already been a longstanding roadblock to a broader application of CDs. To\nthis end, this paper presents Causal representatiOn AssistanT (COAT) that\nintroduces large language models (LLMs) to bridge the gap. LLMs are trained on\nmassive observations of the world and have demonstrated great capability in\nextracting key information from unstructured data. Therefore, it is natural to\nemploy LLMs to assist with proposing useful high-level factors and crafting\ntheir measurements. Meanwhile, COAT also adopts CDs to find causal relations\namong the identified variables as well as to provide feedback to LLMs to\niteratively refine the proposed factors. We show that LLMs and CDs are mutually\nbeneficial and the constructed feedback provably also helps with the factor\nproposal. We construct and curate several synthetic and real-world benchmarks\nincluding analysis of human reviews and diagnosis of neuropathic and brain\ntumors, to comprehensively evaluate COAT. Extensive empirical results confirm\nthe effectiveness and reliability of COAT with significant improvements.\n", "link": "http://arxiv.org/abs/2402.03941v2", "date": "2024-10-31", "relevancy": 2.53, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5191}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5191}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4798}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovery%20of%20the%20Hidden%20World%20with%20Large%20Language%20Models&body=Title%3A%20Discovery%20of%20the%20Hidden%20World%20with%20Large%20Language%20Models%0AAuthor%3A%20Chenxi%20Liu%20and%20Yongqiang%20Chen%20and%20Tongliang%20Liu%20and%20Mingming%20Gong%20and%20James%20Cheng%20and%20Bo%20Han%20and%20Kun%20Zhang%0AAbstract%3A%20%20%20Revealing%20the%20underlying%20causal%20mechanisms%20in%20the%20real%20world%20is%20the%20key%20to%0Athe%20development%20of%20science.%20Despite%20the%20progress%20in%20the%20past%20decades%2C%0Atraditional%20causal%20discovery%20approaches%20%28CDs%29%20mainly%20rely%20on%20high-quality%0Ameasured%20variables%2C%20usually%20given%20by%20human%20experts%2C%20to%20find%20causal%20relations.%0AThe%20lack%20of%20well-defined%20high-level%20variables%20in%20many%20real-world%20applications%0Ahas%20already%20been%20a%20longstanding%20roadblock%20to%20a%20broader%20application%20of%20CDs.%20To%0Athis%20end%2C%20this%20paper%20presents%20Causal%20representatiOn%20AssistanT%20%28COAT%29%20that%0Aintroduces%20large%20language%20models%20%28LLMs%29%20to%20bridge%20the%20gap.%20LLMs%20are%20trained%20on%0Amassive%20observations%20of%20the%20world%20and%20have%20demonstrated%20great%20capability%20in%0Aextracting%20key%20information%20from%20unstructured%20data.%20Therefore%2C%20it%20is%20natural%20to%0Aemploy%20LLMs%20to%20assist%20with%20proposing%20useful%20high-level%20factors%20and%20crafting%0Atheir%20measurements.%20Meanwhile%2C%20COAT%20also%20adopts%20CDs%20to%20find%20causal%20relations%0Aamong%20the%20identified%20variables%20as%20well%20as%20to%20provide%20feedback%20to%20LLMs%20to%0Aiteratively%20refine%20the%20proposed%20factors.%20We%20show%20that%20LLMs%20and%20CDs%20are%20mutually%0Abeneficial%20and%20the%20constructed%20feedback%20provably%20also%20helps%20with%20the%20factor%0Aproposal.%20We%20construct%20and%20curate%20several%20synthetic%20and%20real-world%20benchmarks%0Aincluding%20analysis%20of%20human%20reviews%20and%20diagnosis%20of%20neuropathic%20and%20brain%0Atumors%2C%20to%20comprehensively%20evaluate%20COAT.%20Extensive%20empirical%20results%20confirm%0Athe%20effectiveness%20and%20reliability%20of%20COAT%20with%20significant%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03941v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovery%2520of%2520the%2520Hidden%2520World%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DChenxi%2520Liu%2520and%2520Yongqiang%2520Chen%2520and%2520Tongliang%2520Liu%2520and%2520Mingming%2520Gong%2520and%2520James%2520Cheng%2520and%2520Bo%2520Han%2520and%2520Kun%2520Zhang%26entry.1292438233%3D%2520%2520Revealing%2520the%2520underlying%2520causal%2520mechanisms%2520in%2520the%2520real%2520world%2520is%2520the%2520key%2520to%250Athe%2520development%2520of%2520science.%2520Despite%2520the%2520progress%2520in%2520the%2520past%2520decades%252C%250Atraditional%2520causal%2520discovery%2520approaches%2520%2528CDs%2529%2520mainly%2520rely%2520on%2520high-quality%250Ameasured%2520variables%252C%2520usually%2520given%2520by%2520human%2520experts%252C%2520to%2520find%2520causal%2520relations.%250AThe%2520lack%2520of%2520well-defined%2520high-level%2520variables%2520in%2520many%2520real-world%2520applications%250Ahas%2520already%2520been%2520a%2520longstanding%2520roadblock%2520to%2520a%2520broader%2520application%2520of%2520CDs.%2520To%250Athis%2520end%252C%2520this%2520paper%2520presents%2520Causal%2520representatiOn%2520AssistanT%2520%2528COAT%2529%2520that%250Aintroduces%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520bridge%2520the%2520gap.%2520LLMs%2520are%2520trained%2520on%250Amassive%2520observations%2520of%2520the%2520world%2520and%2520have%2520demonstrated%2520great%2520capability%2520in%250Aextracting%2520key%2520information%2520from%2520unstructured%2520data.%2520Therefore%252C%2520it%2520is%2520natural%2520to%250Aemploy%2520LLMs%2520to%2520assist%2520with%2520proposing%2520useful%2520high-level%2520factors%2520and%2520crafting%250Atheir%2520measurements.%2520Meanwhile%252C%2520COAT%2520also%2520adopts%2520CDs%2520to%2520find%2520causal%2520relations%250Aamong%2520the%2520identified%2520variables%2520as%2520well%2520as%2520to%2520provide%2520feedback%2520to%2520LLMs%2520to%250Aiteratively%2520refine%2520the%2520proposed%2520factors.%2520We%2520show%2520that%2520LLMs%2520and%2520CDs%2520are%2520mutually%250Abeneficial%2520and%2520the%2520constructed%2520feedback%2520provably%2520also%2520helps%2520with%2520the%2520factor%250Aproposal.%2520We%2520construct%2520and%2520curate%2520several%2520synthetic%2520and%2520real-world%2520benchmarks%250Aincluding%2520analysis%2520of%2520human%2520reviews%2520and%2520diagnosis%2520of%2520neuropathic%2520and%2520brain%250Atumors%252C%2520to%2520comprehensively%2520evaluate%2520COAT.%2520Extensive%2520empirical%2520results%2520confirm%250Athe%2520effectiveness%2520and%2520reliability%2520of%2520COAT%2520with%2520significant%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03941v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovery%20of%20the%20Hidden%20World%20with%20Large%20Language%20Models&entry.906535625=Chenxi%20Liu%20and%20Yongqiang%20Chen%20and%20Tongliang%20Liu%20and%20Mingming%20Gong%20and%20James%20Cheng%20and%20Bo%20Han%20and%20Kun%20Zhang&entry.1292438233=%20%20Revealing%20the%20underlying%20causal%20mechanisms%20in%20the%20real%20world%20is%20the%20key%20to%0Athe%20development%20of%20science.%20Despite%20the%20progress%20in%20the%20past%20decades%2C%0Atraditional%20causal%20discovery%20approaches%20%28CDs%29%20mainly%20rely%20on%20high-quality%0Ameasured%20variables%2C%20usually%20given%20by%20human%20experts%2C%20to%20find%20causal%20relations.%0AThe%20lack%20of%20well-defined%20high-level%20variables%20in%20many%20real-world%20applications%0Ahas%20already%20been%20a%20longstanding%20roadblock%20to%20a%20broader%20application%20of%20CDs.%20To%0Athis%20end%2C%20this%20paper%20presents%20Causal%20representatiOn%20AssistanT%20%28COAT%29%20that%0Aintroduces%20large%20language%20models%20%28LLMs%29%20to%20bridge%20the%20gap.%20LLMs%20are%20trained%20on%0Amassive%20observations%20of%20the%20world%20and%20have%20demonstrated%20great%20capability%20in%0Aextracting%20key%20information%20from%20unstructured%20data.%20Therefore%2C%20it%20is%20natural%20to%0Aemploy%20LLMs%20to%20assist%20with%20proposing%20useful%20high-level%20factors%20and%20crafting%0Atheir%20measurements.%20Meanwhile%2C%20COAT%20also%20adopts%20CDs%20to%20find%20causal%20relations%0Aamong%20the%20identified%20variables%20as%20well%20as%20to%20provide%20feedback%20to%20LLMs%20to%0Aiteratively%20refine%20the%20proposed%20factors.%20We%20show%20that%20LLMs%20and%20CDs%20are%20mutually%0Abeneficial%20and%20the%20constructed%20feedback%20provably%20also%20helps%20with%20the%20factor%0Aproposal.%20We%20construct%20and%20curate%20several%20synthetic%20and%20real-world%20benchmarks%0Aincluding%20analysis%20of%20human%20reviews%20and%20diagnosis%20of%20neuropathic%20and%20brain%0Atumors%2C%20to%20comprehensively%20evaluate%20COAT.%20Extensive%20empirical%20results%20confirm%0Athe%20effectiveness%20and%20reliability%20of%20COAT%20with%20significant%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03941v2&entry.124074799=Read"},
{"title": "Data-Efficient Learning with Neural Programs", "author": "Alaia Solko-Breslin and Seewon Choi and Ziyang Li and Neelay Velingker and Rajeev Alur and Mayur Naik and Eric Wong", "abstract": "  Many computational tasks can be naturally expressed as a composition of a DNN\nfollowed by a program written in a traditional programming language or an API\ncall to an LLM. We call such composites \"neural programs\" and focus on the\nproblem of learning the DNN parameters when the training data consist of\nend-to-end input-output labels for the composite. When the program is written\nin a differentiable logic programming language, techniques from neurosymbolic\nlearning are applicable, but in general, the learning for neural programs\nrequires estimating the gradients of black-box components. We present an\nalgorithm for learning neural programs, called ISED, that only relies on\ninput-output samples of black-box components. For evaluation, we introduce new\nbenchmarks that involve calls to modern LLMs such as GPT-4 and also consider\nbenchmarks from the neurosymbolic learning literature. Our evaluation shows\nthat for the latter benchmarks, ISED has comparable performance to\nstate-of-the-art neurosymbolic frameworks. For the former, we use adaptations\nof prior work on gradient approximations of black-box components as a baseline,\nand show that ISED achieves comparable accuracy but in a more data- and\nsample-efficient manner.\n", "link": "http://arxiv.org/abs/2406.06246v2", "date": "2024-10-31", "relevancy": 2.515, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5416}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4857}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Efficient%20Learning%20with%20Neural%20Programs&body=Title%3A%20Data-Efficient%20Learning%20with%20Neural%20Programs%0AAuthor%3A%20Alaia%20Solko-Breslin%20and%20Seewon%20Choi%20and%20Ziyang%20Li%20and%20Neelay%20Velingker%20and%20Rajeev%20Alur%20and%20Mayur%20Naik%20and%20Eric%20Wong%0AAbstract%3A%20%20%20Many%20computational%20tasks%20can%20be%20naturally%20expressed%20as%20a%20composition%20of%20a%20DNN%0Afollowed%20by%20a%20program%20written%20in%20a%20traditional%20programming%20language%20or%20an%20API%0Acall%20to%20an%20LLM.%20We%20call%20such%20composites%20%22neural%20programs%22%20and%20focus%20on%20the%0Aproblem%20of%20learning%20the%20DNN%20parameters%20when%20the%20training%20data%20consist%20of%0Aend-to-end%20input-output%20labels%20for%20the%20composite.%20When%20the%20program%20is%20written%0Ain%20a%20differentiable%20logic%20programming%20language%2C%20techniques%20from%20neurosymbolic%0Alearning%20are%20applicable%2C%20but%20in%20general%2C%20the%20learning%20for%20neural%20programs%0Arequires%20estimating%20the%20gradients%20of%20black-box%20components.%20We%20present%20an%0Aalgorithm%20for%20learning%20neural%20programs%2C%20called%20ISED%2C%20that%20only%20relies%20on%0Ainput-output%20samples%20of%20black-box%20components.%20For%20evaluation%2C%20we%20introduce%20new%0Abenchmarks%20that%20involve%20calls%20to%20modern%20LLMs%20such%20as%20GPT-4%20and%20also%20consider%0Abenchmarks%20from%20the%20neurosymbolic%20learning%20literature.%20Our%20evaluation%20shows%0Athat%20for%20the%20latter%20benchmarks%2C%20ISED%20has%20comparable%20performance%20to%0Astate-of-the-art%20neurosymbolic%20frameworks.%20For%20the%20former%2C%20we%20use%20adaptations%0Aof%20prior%20work%20on%20gradient%20approximations%20of%20black-box%20components%20as%20a%20baseline%2C%0Aand%20show%20that%20ISED%20achieves%20comparable%20accuracy%20but%20in%20a%20more%20data-%20and%0Asample-efficient%20manner.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06246v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Efficient%2520Learning%2520with%2520Neural%2520Programs%26entry.906535625%3DAlaia%2520Solko-Breslin%2520and%2520Seewon%2520Choi%2520and%2520Ziyang%2520Li%2520and%2520Neelay%2520Velingker%2520and%2520Rajeev%2520Alur%2520and%2520Mayur%2520Naik%2520and%2520Eric%2520Wong%26entry.1292438233%3D%2520%2520Many%2520computational%2520tasks%2520can%2520be%2520naturally%2520expressed%2520as%2520a%2520composition%2520of%2520a%2520DNN%250Afollowed%2520by%2520a%2520program%2520written%2520in%2520a%2520traditional%2520programming%2520language%2520or%2520an%2520API%250Acall%2520to%2520an%2520LLM.%2520We%2520call%2520such%2520composites%2520%2522neural%2520programs%2522%2520and%2520focus%2520on%2520the%250Aproblem%2520of%2520learning%2520the%2520DNN%2520parameters%2520when%2520the%2520training%2520data%2520consist%2520of%250Aend-to-end%2520input-output%2520labels%2520for%2520the%2520composite.%2520When%2520the%2520program%2520is%2520written%250Ain%2520a%2520differentiable%2520logic%2520programming%2520language%252C%2520techniques%2520from%2520neurosymbolic%250Alearning%2520are%2520applicable%252C%2520but%2520in%2520general%252C%2520the%2520learning%2520for%2520neural%2520programs%250Arequires%2520estimating%2520the%2520gradients%2520of%2520black-box%2520components.%2520We%2520present%2520an%250Aalgorithm%2520for%2520learning%2520neural%2520programs%252C%2520called%2520ISED%252C%2520that%2520only%2520relies%2520on%250Ainput-output%2520samples%2520of%2520black-box%2520components.%2520For%2520evaluation%252C%2520we%2520introduce%2520new%250Abenchmarks%2520that%2520involve%2520calls%2520to%2520modern%2520LLMs%2520such%2520as%2520GPT-4%2520and%2520also%2520consider%250Abenchmarks%2520from%2520the%2520neurosymbolic%2520learning%2520literature.%2520Our%2520evaluation%2520shows%250Athat%2520for%2520the%2520latter%2520benchmarks%252C%2520ISED%2520has%2520comparable%2520performance%2520to%250Astate-of-the-art%2520neurosymbolic%2520frameworks.%2520For%2520the%2520former%252C%2520we%2520use%2520adaptations%250Aof%2520prior%2520work%2520on%2520gradient%2520approximations%2520of%2520black-box%2520components%2520as%2520a%2520baseline%252C%250Aand%2520show%2520that%2520ISED%2520achieves%2520comparable%2520accuracy%2520but%2520in%2520a%2520more%2520data-%2520and%250Asample-efficient%2520manner.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06246v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Efficient%20Learning%20with%20Neural%20Programs&entry.906535625=Alaia%20Solko-Breslin%20and%20Seewon%20Choi%20and%20Ziyang%20Li%20and%20Neelay%20Velingker%20and%20Rajeev%20Alur%20and%20Mayur%20Naik%20and%20Eric%20Wong&entry.1292438233=%20%20Many%20computational%20tasks%20can%20be%20naturally%20expressed%20as%20a%20composition%20of%20a%20DNN%0Afollowed%20by%20a%20program%20written%20in%20a%20traditional%20programming%20language%20or%20an%20API%0Acall%20to%20an%20LLM.%20We%20call%20such%20composites%20%22neural%20programs%22%20and%20focus%20on%20the%0Aproblem%20of%20learning%20the%20DNN%20parameters%20when%20the%20training%20data%20consist%20of%0Aend-to-end%20input-output%20labels%20for%20the%20composite.%20When%20the%20program%20is%20written%0Ain%20a%20differentiable%20logic%20programming%20language%2C%20techniques%20from%20neurosymbolic%0Alearning%20are%20applicable%2C%20but%20in%20general%2C%20the%20learning%20for%20neural%20programs%0Arequires%20estimating%20the%20gradients%20of%20black-box%20components.%20We%20present%20an%0Aalgorithm%20for%20learning%20neural%20programs%2C%20called%20ISED%2C%20that%20only%20relies%20on%0Ainput-output%20samples%20of%20black-box%20components.%20For%20evaluation%2C%20we%20introduce%20new%0Abenchmarks%20that%20involve%20calls%20to%20modern%20LLMs%20such%20as%20GPT-4%20and%20also%20consider%0Abenchmarks%20from%20the%20neurosymbolic%20learning%20literature.%20Our%20evaluation%20shows%0Athat%20for%20the%20latter%20benchmarks%2C%20ISED%20has%20comparable%20performance%20to%0Astate-of-the-art%20neurosymbolic%20frameworks.%20For%20the%20former%2C%20we%20use%20adaptations%0Aof%20prior%20work%20on%20gradient%20approximations%20of%20black-box%20components%20as%20a%20baseline%2C%0Aand%20show%20that%20ISED%20achieves%20comparable%20accuracy%20but%20in%20a%20more%20data-%20and%0Asample-efficient%20manner.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06246v2&entry.124074799=Read"},
{"title": "An Empirical Analysis of Speech Self-Supervised Learning at Multiple\n  Resolutions", "author": "Theo Clark and Benedetta Cevoli and Eloy de Jong and Timofey Abramski and Jamie Dougherty", "abstract": "  Self-supervised learning (SSL) models have become crucial in speech\nprocessing, with recent advancements concentrating on developing architectures\nthat capture representations across multiple timescales. The primary goal of\nthese multi-scale architectures is to exploit the hierarchical nature of\nspeech, where lower-resolution components aim to capture representations that\nalign with increasingly abstract concepts (e.g., from phones to words to\nsentences). Although multi-scale approaches have demonstrated some improvements\nover single-scale models, the precise reasons for these enhancements have poor\nempirical support. In this study, we present an initial analysis of layer-wise\nrepresentations in multi-scale architectures, with a focus on Canonical\nCorrelation Analysis (CCA) and Mutual Information (MI). We apply this analysis\nto Multi-Resolution HuBERT (MR-HuBERT) and find that (1) the improved\nperformance on SUPERB tasks is primarily due to the auxiliary low-resolution\nloss rather than the downsampling itself, and (2) downsampling to lower\nresolutions neither improves downstream performance nor correlates with\nhigher-level information (e.g., words), though it does improve computational\nefficiency. These findings challenge assumptions about the multi-scale nature\nof MR-HuBERT and motivate the importance of disentangling computational\nefficiency from learning better representations.\n", "link": "http://arxiv.org/abs/2410.23955v1", "date": "2024-10-31", "relevancy": 2.5092, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5051}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5051}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Empirical%20Analysis%20of%20Speech%20Self-Supervised%20Learning%20at%20Multiple%0A%20%20Resolutions&body=Title%3A%20An%20Empirical%20Analysis%20of%20Speech%20Self-Supervised%20Learning%20at%20Multiple%0A%20%20Resolutions%0AAuthor%3A%20Theo%20Clark%20and%20Benedetta%20Cevoli%20and%20Eloy%20de%20Jong%20and%20Timofey%20Abramski%20and%20Jamie%20Dougherty%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20models%20have%20become%20crucial%20in%20speech%0Aprocessing%2C%20with%20recent%20advancements%20concentrating%20on%20developing%20architectures%0Athat%20capture%20representations%20across%20multiple%20timescales.%20The%20primary%20goal%20of%0Athese%20multi-scale%20architectures%20is%20to%20exploit%20the%20hierarchical%20nature%20of%0Aspeech%2C%20where%20lower-resolution%20components%20aim%20to%20capture%20representations%20that%0Aalign%20with%20increasingly%20abstract%20concepts%20%28e.g.%2C%20from%20phones%20to%20words%20to%0Asentences%29.%20Although%20multi-scale%20approaches%20have%20demonstrated%20some%20improvements%0Aover%20single-scale%20models%2C%20the%20precise%20reasons%20for%20these%20enhancements%20have%20poor%0Aempirical%20support.%20In%20this%20study%2C%20we%20present%20an%20initial%20analysis%20of%20layer-wise%0Arepresentations%20in%20multi-scale%20architectures%2C%20with%20a%20focus%20on%20Canonical%0ACorrelation%20Analysis%20%28CCA%29%20and%20Mutual%20Information%20%28MI%29.%20We%20apply%20this%20analysis%0Ato%20Multi-Resolution%20HuBERT%20%28MR-HuBERT%29%20and%20find%20that%20%281%29%20the%20improved%0Aperformance%20on%20SUPERB%20tasks%20is%20primarily%20due%20to%20the%20auxiliary%20low-resolution%0Aloss%20rather%20than%20the%20downsampling%20itself%2C%20and%20%282%29%20downsampling%20to%20lower%0Aresolutions%20neither%20improves%20downstream%20performance%20nor%20correlates%20with%0Ahigher-level%20information%20%28e.g.%2C%20words%29%2C%20though%20it%20does%20improve%20computational%0Aefficiency.%20These%20findings%20challenge%20assumptions%20about%20the%20multi-scale%20nature%0Aof%20MR-HuBERT%20and%20motivate%20the%20importance%20of%20disentangling%20computational%0Aefficiency%20from%20learning%20better%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23955v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Empirical%2520Analysis%2520of%2520Speech%2520Self-Supervised%2520Learning%2520at%2520Multiple%250A%2520%2520Resolutions%26entry.906535625%3DTheo%2520Clark%2520and%2520Benedetta%2520Cevoli%2520and%2520Eloy%2520de%2520Jong%2520and%2520Timofey%2520Abramski%2520and%2520Jamie%2520Dougherty%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520models%2520have%2520become%2520crucial%2520in%2520speech%250Aprocessing%252C%2520with%2520recent%2520advancements%2520concentrating%2520on%2520developing%2520architectures%250Athat%2520capture%2520representations%2520across%2520multiple%2520timescales.%2520The%2520primary%2520goal%2520of%250Athese%2520multi-scale%2520architectures%2520is%2520to%2520exploit%2520the%2520hierarchical%2520nature%2520of%250Aspeech%252C%2520where%2520lower-resolution%2520components%2520aim%2520to%2520capture%2520representations%2520that%250Aalign%2520with%2520increasingly%2520abstract%2520concepts%2520%2528e.g.%252C%2520from%2520phones%2520to%2520words%2520to%250Asentences%2529.%2520Although%2520multi-scale%2520approaches%2520have%2520demonstrated%2520some%2520improvements%250Aover%2520single-scale%2520models%252C%2520the%2520precise%2520reasons%2520for%2520these%2520enhancements%2520have%2520poor%250Aempirical%2520support.%2520In%2520this%2520study%252C%2520we%2520present%2520an%2520initial%2520analysis%2520of%2520layer-wise%250Arepresentations%2520in%2520multi-scale%2520architectures%252C%2520with%2520a%2520focus%2520on%2520Canonical%250ACorrelation%2520Analysis%2520%2528CCA%2529%2520and%2520Mutual%2520Information%2520%2528MI%2529.%2520We%2520apply%2520this%2520analysis%250Ato%2520Multi-Resolution%2520HuBERT%2520%2528MR-HuBERT%2529%2520and%2520find%2520that%2520%25281%2529%2520the%2520improved%250Aperformance%2520on%2520SUPERB%2520tasks%2520is%2520primarily%2520due%2520to%2520the%2520auxiliary%2520low-resolution%250Aloss%2520rather%2520than%2520the%2520downsampling%2520itself%252C%2520and%2520%25282%2529%2520downsampling%2520to%2520lower%250Aresolutions%2520neither%2520improves%2520downstream%2520performance%2520nor%2520correlates%2520with%250Ahigher-level%2520information%2520%2528e.g.%252C%2520words%2529%252C%2520though%2520it%2520does%2520improve%2520computational%250Aefficiency.%2520These%2520findings%2520challenge%2520assumptions%2520about%2520the%2520multi-scale%2520nature%250Aof%2520MR-HuBERT%2520and%2520motivate%2520the%2520importance%2520of%2520disentangling%2520computational%250Aefficiency%2520from%2520learning%2520better%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23955v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Empirical%20Analysis%20of%20Speech%20Self-Supervised%20Learning%20at%20Multiple%0A%20%20Resolutions&entry.906535625=Theo%20Clark%20and%20Benedetta%20Cevoli%20and%20Eloy%20de%20Jong%20and%20Timofey%20Abramski%20and%20Jamie%20Dougherty&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20models%20have%20become%20crucial%20in%20speech%0Aprocessing%2C%20with%20recent%20advancements%20concentrating%20on%20developing%20architectures%0Athat%20capture%20representations%20across%20multiple%20timescales.%20The%20primary%20goal%20of%0Athese%20multi-scale%20architectures%20is%20to%20exploit%20the%20hierarchical%20nature%20of%0Aspeech%2C%20where%20lower-resolution%20components%20aim%20to%20capture%20representations%20that%0Aalign%20with%20increasingly%20abstract%20concepts%20%28e.g.%2C%20from%20phones%20to%20words%20to%0Asentences%29.%20Although%20multi-scale%20approaches%20have%20demonstrated%20some%20improvements%0Aover%20single-scale%20models%2C%20the%20precise%20reasons%20for%20these%20enhancements%20have%20poor%0Aempirical%20support.%20In%20this%20study%2C%20we%20present%20an%20initial%20analysis%20of%20layer-wise%0Arepresentations%20in%20multi-scale%20architectures%2C%20with%20a%20focus%20on%20Canonical%0ACorrelation%20Analysis%20%28CCA%29%20and%20Mutual%20Information%20%28MI%29.%20We%20apply%20this%20analysis%0Ato%20Multi-Resolution%20HuBERT%20%28MR-HuBERT%29%20and%20find%20that%20%281%29%20the%20improved%0Aperformance%20on%20SUPERB%20tasks%20is%20primarily%20due%20to%20the%20auxiliary%20low-resolution%0Aloss%20rather%20than%20the%20downsampling%20itself%2C%20and%20%282%29%20downsampling%20to%20lower%0Aresolutions%20neither%20improves%20downstream%20performance%20nor%20correlates%20with%0Ahigher-level%20information%20%28e.g.%2C%20words%29%2C%20though%20it%20does%20improve%20computational%0Aefficiency.%20These%20findings%20challenge%20assumptions%20about%20the%20multi-scale%20nature%0Aof%20MR-HuBERT%20and%20motivate%20the%20importance%20of%20disentangling%20computational%0Aefficiency%20from%20learning%20better%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23955v1&entry.124074799=Read"},
{"title": "Enhancing Motion in Text-to-Video Generation with Decomposed Encoding\n  and Conditioning", "author": "Penghui Ruan and Pichao Wang and Divya Saxena and Jiannong Cao and Yuhui Shi", "abstract": "  Despite advancements in Text-to-Video (T2V) generation, producing videos with\nrealistic motion remains challenging. Current models often yield static or\nminimally dynamic outputs, failing to capture complex motions described by\ntext. This issue stems from the internal biases in text encoding, which\noverlooks motions, and inadequate conditioning mechanisms in T2V generation\nmodels. To address this, we propose a novel framework called DEcomposed MOtion\n(DEMO), which enhances motion synthesis in T2V generation by decomposing both\ntext encoding and conditioning into content and motion components. Our method\nincludes a content encoder for static elements and a motion encoder for\ntemporal dynamics, alongside separate content and motion conditioning\nmechanisms. Crucially, we introduce text-motion and video-motion supervision to\nimprove the model's understanding and generation of motion. Evaluations on\nbenchmarks such as MSR-VTT, UCF-101, WebVid-10M, EvalCrafter, and VBench\ndemonstrate DEMO's superior ability to produce videos with enhanced motion\ndynamics while maintaining high visual quality. Our approach significantly\nadvances T2V generation by integrating comprehensive motion understanding\ndirectly from textual descriptions. Project page:\nhttps://PR-Ryan.github.io/DEMO-project/\n", "link": "http://arxiv.org/abs/2410.24219v1", "date": "2024-10-31", "relevancy": 2.5054, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.696}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.624}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Motion%20in%20Text-to-Video%20Generation%20with%20Decomposed%20Encoding%0A%20%20and%20Conditioning&body=Title%3A%20Enhancing%20Motion%20in%20Text-to-Video%20Generation%20with%20Decomposed%20Encoding%0A%20%20and%20Conditioning%0AAuthor%3A%20Penghui%20Ruan%20and%20Pichao%20Wang%20and%20Divya%20Saxena%20and%20Jiannong%20Cao%20and%20Yuhui%20Shi%0AAbstract%3A%20%20%20Despite%20advancements%20in%20Text-to-Video%20%28T2V%29%20generation%2C%20producing%20videos%20with%0Arealistic%20motion%20remains%20challenging.%20Current%20models%20often%20yield%20static%20or%0Aminimally%20dynamic%20outputs%2C%20failing%20to%20capture%20complex%20motions%20described%20by%0Atext.%20This%20issue%20stems%20from%20the%20internal%20biases%20in%20text%20encoding%2C%20which%0Aoverlooks%20motions%2C%20and%20inadequate%20conditioning%20mechanisms%20in%20T2V%20generation%0Amodels.%20To%20address%20this%2C%20we%20propose%20a%20novel%20framework%20called%20DEcomposed%20MOtion%0A%28DEMO%29%2C%20which%20enhances%20motion%20synthesis%20in%20T2V%20generation%20by%20decomposing%20both%0Atext%20encoding%20and%20conditioning%20into%20content%20and%20motion%20components.%20Our%20method%0Aincludes%20a%20content%20encoder%20for%20static%20elements%20and%20a%20motion%20encoder%20for%0Atemporal%20dynamics%2C%20alongside%20separate%20content%20and%20motion%20conditioning%0Amechanisms.%20Crucially%2C%20we%20introduce%20text-motion%20and%20video-motion%20supervision%20to%0Aimprove%20the%20model%27s%20understanding%20and%20generation%20of%20motion.%20Evaluations%20on%0Abenchmarks%20such%20as%20MSR-VTT%2C%20UCF-101%2C%20WebVid-10M%2C%20EvalCrafter%2C%20and%20VBench%0Ademonstrate%20DEMO%27s%20superior%20ability%20to%20produce%20videos%20with%20enhanced%20motion%0Adynamics%20while%20maintaining%20high%20visual%20quality.%20Our%20approach%20significantly%0Aadvances%20T2V%20generation%20by%20integrating%20comprehensive%20motion%20understanding%0Adirectly%20from%20textual%20descriptions.%20Project%20page%3A%0Ahttps%3A//PR-Ryan.github.io/DEMO-project/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24219v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Motion%2520in%2520Text-to-Video%2520Generation%2520with%2520Decomposed%2520Encoding%250A%2520%2520and%2520Conditioning%26entry.906535625%3DPenghui%2520Ruan%2520and%2520Pichao%2520Wang%2520and%2520Divya%2520Saxena%2520and%2520Jiannong%2520Cao%2520and%2520Yuhui%2520Shi%26entry.1292438233%3D%2520%2520Despite%2520advancements%2520in%2520Text-to-Video%2520%2528T2V%2529%2520generation%252C%2520producing%2520videos%2520with%250Arealistic%2520motion%2520remains%2520challenging.%2520Current%2520models%2520often%2520yield%2520static%2520or%250Aminimally%2520dynamic%2520outputs%252C%2520failing%2520to%2520capture%2520complex%2520motions%2520described%2520by%250Atext.%2520This%2520issue%2520stems%2520from%2520the%2520internal%2520biases%2520in%2520text%2520encoding%252C%2520which%250Aoverlooks%2520motions%252C%2520and%2520inadequate%2520conditioning%2520mechanisms%2520in%2520T2V%2520generation%250Amodels.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520framework%2520called%2520DEcomposed%2520MOtion%250A%2528DEMO%2529%252C%2520which%2520enhances%2520motion%2520synthesis%2520in%2520T2V%2520generation%2520by%2520decomposing%2520both%250Atext%2520encoding%2520and%2520conditioning%2520into%2520content%2520and%2520motion%2520components.%2520Our%2520method%250Aincludes%2520a%2520content%2520encoder%2520for%2520static%2520elements%2520and%2520a%2520motion%2520encoder%2520for%250Atemporal%2520dynamics%252C%2520alongside%2520separate%2520content%2520and%2520motion%2520conditioning%250Amechanisms.%2520Crucially%252C%2520we%2520introduce%2520text-motion%2520and%2520video-motion%2520supervision%2520to%250Aimprove%2520the%2520model%2527s%2520understanding%2520and%2520generation%2520of%2520motion.%2520Evaluations%2520on%250Abenchmarks%2520such%2520as%2520MSR-VTT%252C%2520UCF-101%252C%2520WebVid-10M%252C%2520EvalCrafter%252C%2520and%2520VBench%250Ademonstrate%2520DEMO%2527s%2520superior%2520ability%2520to%2520produce%2520videos%2520with%2520enhanced%2520motion%250Adynamics%2520while%2520maintaining%2520high%2520visual%2520quality.%2520Our%2520approach%2520significantly%250Aadvances%2520T2V%2520generation%2520by%2520integrating%2520comprehensive%2520motion%2520understanding%250Adirectly%2520from%2520textual%2520descriptions.%2520Project%2520page%253A%250Ahttps%253A//PR-Ryan.github.io/DEMO-project/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24219v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Motion%20in%20Text-to-Video%20Generation%20with%20Decomposed%20Encoding%0A%20%20and%20Conditioning&entry.906535625=Penghui%20Ruan%20and%20Pichao%20Wang%20and%20Divya%20Saxena%20and%20Jiannong%20Cao%20and%20Yuhui%20Shi&entry.1292438233=%20%20Despite%20advancements%20in%20Text-to-Video%20%28T2V%29%20generation%2C%20producing%20videos%20with%0Arealistic%20motion%20remains%20challenging.%20Current%20models%20often%20yield%20static%20or%0Aminimally%20dynamic%20outputs%2C%20failing%20to%20capture%20complex%20motions%20described%20by%0Atext.%20This%20issue%20stems%20from%20the%20internal%20biases%20in%20text%20encoding%2C%20which%0Aoverlooks%20motions%2C%20and%20inadequate%20conditioning%20mechanisms%20in%20T2V%20generation%0Amodels.%20To%20address%20this%2C%20we%20propose%20a%20novel%20framework%20called%20DEcomposed%20MOtion%0A%28DEMO%29%2C%20which%20enhances%20motion%20synthesis%20in%20T2V%20generation%20by%20decomposing%20both%0Atext%20encoding%20and%20conditioning%20into%20content%20and%20motion%20components.%20Our%20method%0Aincludes%20a%20content%20encoder%20for%20static%20elements%20and%20a%20motion%20encoder%20for%0Atemporal%20dynamics%2C%20alongside%20separate%20content%20and%20motion%20conditioning%0Amechanisms.%20Crucially%2C%20we%20introduce%20text-motion%20and%20video-motion%20supervision%20to%0Aimprove%20the%20model%27s%20understanding%20and%20generation%20of%20motion.%20Evaluations%20on%0Abenchmarks%20such%20as%20MSR-VTT%2C%20UCF-101%2C%20WebVid-10M%2C%20EvalCrafter%2C%20and%20VBench%0Ademonstrate%20DEMO%27s%20superior%20ability%20to%20produce%20videos%20with%20enhanced%20motion%0Adynamics%20while%20maintaining%20high%20visual%20quality.%20Our%20approach%20significantly%0Aadvances%20T2V%20generation%20by%20integrating%20comprehensive%20motion%20understanding%0Adirectly%20from%20textual%20descriptions.%20Project%20page%3A%0Ahttps%3A//PR-Ryan.github.io/DEMO-project/%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24219v1&entry.124074799=Read"},
{"title": "Robust Gaussian Processes via Relevance Pursuit", "author": "Sebastian Ament and Elizabeth Santorella and David Eriksson and Ben Letham and Maximilian Balandat and Eytan Bakshy", "abstract": "  Gaussian processes (GPs) are non-parametric probabilistic regression models\nthat are popular due to their flexibility, data efficiency, and well-calibrated\nuncertainty estimates. However, standard GP models assume homoskedastic\nGaussian noise, while many real-world applications are subject to non-Gaussian\ncorruptions. Variants of GPs that are more robust to alternative noise models\nhave been proposed, and entail significant trade-offs between accuracy and\nrobustness, and between computational requirements and theoretical guarantees.\nIn this work, we propose and study a GP model that achieves robustness against\nsparse outliers by inferring data-point-specific noise levels with a sequential\nselection procedure maximizing the log marginal likelihood that we refer to as\nrelevance pursuit. We show, surprisingly, that the model can be parameterized\nsuch that the associated log marginal likelihood is strongly concave in the\ndata-point-specific noise variances, a property rarely found in either robust\nregression objectives or GP marginal likelihoods. This in turn implies the weak\nsubmodularity of the corresponding subset selection problem, and thereby proves\napproximation guarantees for the proposed algorithm. We compare the model's\nperformance relative to other approaches on diverse regression and Bayesian\noptimization tasks, including the challenging but common setting of sparse\ncorruptions of the labels within or close to the function range.\n", "link": "http://arxiv.org/abs/2410.24222v1", "date": "2024-10-31", "relevancy": 2.4628, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5021}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4931}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Gaussian%20Processes%20via%20Relevance%20Pursuit&body=Title%3A%20Robust%20Gaussian%20Processes%20via%20Relevance%20Pursuit%0AAuthor%3A%20Sebastian%20Ament%20and%20Elizabeth%20Santorella%20and%20David%20Eriksson%20and%20Ben%20Letham%20and%20Maximilian%20Balandat%20and%20Eytan%20Bakshy%0AAbstract%3A%20%20%20Gaussian%20processes%20%28GPs%29%20are%20non-parametric%20probabilistic%20regression%20models%0Athat%20are%20popular%20due%20to%20their%20flexibility%2C%20data%20efficiency%2C%20and%20well-calibrated%0Auncertainty%20estimates.%20However%2C%20standard%20GP%20models%20assume%20homoskedastic%0AGaussian%20noise%2C%20while%20many%20real-world%20applications%20are%20subject%20to%20non-Gaussian%0Acorruptions.%20Variants%20of%20GPs%20that%20are%20more%20robust%20to%20alternative%20noise%20models%0Ahave%20been%20proposed%2C%20and%20entail%20significant%20trade-offs%20between%20accuracy%20and%0Arobustness%2C%20and%20between%20computational%20requirements%20and%20theoretical%20guarantees.%0AIn%20this%20work%2C%20we%20propose%20and%20study%20a%20GP%20model%20that%20achieves%20robustness%20against%0Asparse%20outliers%20by%20inferring%20data-point-specific%20noise%20levels%20with%20a%20sequential%0Aselection%20procedure%20maximizing%20the%20log%20marginal%20likelihood%20that%20we%20refer%20to%20as%0Arelevance%20pursuit.%20We%20show%2C%20surprisingly%2C%20that%20the%20model%20can%20be%20parameterized%0Asuch%20that%20the%20associated%20log%20marginal%20likelihood%20is%20strongly%20concave%20in%20the%0Adata-point-specific%20noise%20variances%2C%20a%20property%20rarely%20found%20in%20either%20robust%0Aregression%20objectives%20or%20GP%20marginal%20likelihoods.%20This%20in%20turn%20implies%20the%20weak%0Asubmodularity%20of%20the%20corresponding%20subset%20selection%20problem%2C%20and%20thereby%20proves%0Aapproximation%20guarantees%20for%20the%20proposed%20algorithm.%20We%20compare%20the%20model%27s%0Aperformance%20relative%20to%20other%20approaches%20on%20diverse%20regression%20and%20Bayesian%0Aoptimization%20tasks%2C%20including%20the%20challenging%20but%20common%20setting%20of%20sparse%0Acorruptions%20of%20the%20labels%20within%20or%20close%20to%20the%20function%20range.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24222v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Gaussian%2520Processes%2520via%2520Relevance%2520Pursuit%26entry.906535625%3DSebastian%2520Ament%2520and%2520Elizabeth%2520Santorella%2520and%2520David%2520Eriksson%2520and%2520Ben%2520Letham%2520and%2520Maximilian%2520Balandat%2520and%2520Eytan%2520Bakshy%26entry.1292438233%3D%2520%2520Gaussian%2520processes%2520%2528GPs%2529%2520are%2520non-parametric%2520probabilistic%2520regression%2520models%250Athat%2520are%2520popular%2520due%2520to%2520their%2520flexibility%252C%2520data%2520efficiency%252C%2520and%2520well-calibrated%250Auncertainty%2520estimates.%2520However%252C%2520standard%2520GP%2520models%2520assume%2520homoskedastic%250AGaussian%2520noise%252C%2520while%2520many%2520real-world%2520applications%2520are%2520subject%2520to%2520non-Gaussian%250Acorruptions.%2520Variants%2520of%2520GPs%2520that%2520are%2520more%2520robust%2520to%2520alternative%2520noise%2520models%250Ahave%2520been%2520proposed%252C%2520and%2520entail%2520significant%2520trade-offs%2520between%2520accuracy%2520and%250Arobustness%252C%2520and%2520between%2520computational%2520requirements%2520and%2520theoretical%2520guarantees.%250AIn%2520this%2520work%252C%2520we%2520propose%2520and%2520study%2520a%2520GP%2520model%2520that%2520achieves%2520robustness%2520against%250Asparse%2520outliers%2520by%2520inferring%2520data-point-specific%2520noise%2520levels%2520with%2520a%2520sequential%250Aselection%2520procedure%2520maximizing%2520the%2520log%2520marginal%2520likelihood%2520that%2520we%2520refer%2520to%2520as%250Arelevance%2520pursuit.%2520We%2520show%252C%2520surprisingly%252C%2520that%2520the%2520model%2520can%2520be%2520parameterized%250Asuch%2520that%2520the%2520associated%2520log%2520marginal%2520likelihood%2520is%2520strongly%2520concave%2520in%2520the%250Adata-point-specific%2520noise%2520variances%252C%2520a%2520property%2520rarely%2520found%2520in%2520either%2520robust%250Aregression%2520objectives%2520or%2520GP%2520marginal%2520likelihoods.%2520This%2520in%2520turn%2520implies%2520the%2520weak%250Asubmodularity%2520of%2520the%2520corresponding%2520subset%2520selection%2520problem%252C%2520and%2520thereby%2520proves%250Aapproximation%2520guarantees%2520for%2520the%2520proposed%2520algorithm.%2520We%2520compare%2520the%2520model%2527s%250Aperformance%2520relative%2520to%2520other%2520approaches%2520on%2520diverse%2520regression%2520and%2520Bayesian%250Aoptimization%2520tasks%252C%2520including%2520the%2520challenging%2520but%2520common%2520setting%2520of%2520sparse%250Acorruptions%2520of%2520the%2520labels%2520within%2520or%2520close%2520to%2520the%2520function%2520range.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24222v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Gaussian%20Processes%20via%20Relevance%20Pursuit&entry.906535625=Sebastian%20Ament%20and%20Elizabeth%20Santorella%20and%20David%20Eriksson%20and%20Ben%20Letham%20and%20Maximilian%20Balandat%20and%20Eytan%20Bakshy&entry.1292438233=%20%20Gaussian%20processes%20%28GPs%29%20are%20non-parametric%20probabilistic%20regression%20models%0Athat%20are%20popular%20due%20to%20their%20flexibility%2C%20data%20efficiency%2C%20and%20well-calibrated%0Auncertainty%20estimates.%20However%2C%20standard%20GP%20models%20assume%20homoskedastic%0AGaussian%20noise%2C%20while%20many%20real-world%20applications%20are%20subject%20to%20non-Gaussian%0Acorruptions.%20Variants%20of%20GPs%20that%20are%20more%20robust%20to%20alternative%20noise%20models%0Ahave%20been%20proposed%2C%20and%20entail%20significant%20trade-offs%20between%20accuracy%20and%0Arobustness%2C%20and%20between%20computational%20requirements%20and%20theoretical%20guarantees.%0AIn%20this%20work%2C%20we%20propose%20and%20study%20a%20GP%20model%20that%20achieves%20robustness%20against%0Asparse%20outliers%20by%20inferring%20data-point-specific%20noise%20levels%20with%20a%20sequential%0Aselection%20procedure%20maximizing%20the%20log%20marginal%20likelihood%20that%20we%20refer%20to%20as%0Arelevance%20pursuit.%20We%20show%2C%20surprisingly%2C%20that%20the%20model%20can%20be%20parameterized%0Asuch%20that%20the%20associated%20log%20marginal%20likelihood%20is%20strongly%20concave%20in%20the%0Adata-point-specific%20noise%20variances%2C%20a%20property%20rarely%20found%20in%20either%20robust%0Aregression%20objectives%20or%20GP%20marginal%20likelihoods.%20This%20in%20turn%20implies%20the%20weak%0Asubmodularity%20of%20the%20corresponding%20subset%20selection%20problem%2C%20and%20thereby%20proves%0Aapproximation%20guarantees%20for%20the%20proposed%20algorithm.%20We%20compare%20the%20model%27s%0Aperformance%20relative%20to%20other%20approaches%20on%20diverse%20regression%20and%20Bayesian%0Aoptimization%20tasks%2C%20including%20the%20challenging%20but%20common%20setting%20of%20sparse%0Acorruptions%20of%20the%20labels%20within%20or%20close%20to%20the%20function%20range.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24222v1&entry.124074799=Read"},
{"title": "DexMimicGen: Automated Data Generation for Bimanual Dexterous\n  Manipulation via Imitation Learning", "author": "Zhenyu Jiang and Yuqi Xie and Kevin Lin and Zhenjia Xu and Weikang Wan and Ajay Mandlekar and Linxi Fan and Yuke Zhu", "abstract": "  Imitation learning from human demonstrations is an effective means to teach\nrobots manipulation skills. But data acquisition is a major bottleneck in\napplying this paradigm more broadly, due to the amount of cost and human effort\ninvolved. There has been significant interest in imitation learning for\nbimanual dexterous robots, like humanoids. Unfortunately, data collection is\neven more challenging here due to the challenges of simultaneously controlling\nmultiple arms and multi-fingered hands. Automated data generation in simulation\nis a compelling, scalable alternative to fuel this need for data. To this end,\nwe introduce DexMimicGen, a large-scale automated data generation system that\nsynthesizes trajectories from a handful of human demonstrations for humanoid\nrobots with dexterous hands. We present a collection of simulation environments\nin the setting of bimanual dexterous manipulation, spanning a range of\nmanipulation behaviors and different requirements for coordination among the\ntwo arms. We generate 21K demos across these tasks from just 60 source human\ndemos and study the effect of several data generation and policy learning\ndecisions on agent performance. Finally, we present a real-to-sim-to-real\npipeline and deploy it on a real-world humanoid can sorting task. Videos and\nmore are at https://dexmimicgen.github.io/\n", "link": "http://arxiv.org/abs/2410.24185v1", "date": "2024-10-31", "relevancy": 2.4503, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6438}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.617}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DexMimicGen%3A%20Automated%20Data%20Generation%20for%20Bimanual%20Dexterous%0A%20%20Manipulation%20via%20Imitation%20Learning&body=Title%3A%20DexMimicGen%3A%20Automated%20Data%20Generation%20for%20Bimanual%20Dexterous%0A%20%20Manipulation%20via%20Imitation%20Learning%0AAuthor%3A%20Zhenyu%20Jiang%20and%20Yuqi%20Xie%20and%20Kevin%20Lin%20and%20Zhenjia%20Xu%20and%20Weikang%20Wan%20and%20Ajay%20Mandlekar%20and%20Linxi%20Fan%20and%20Yuke%20Zhu%0AAbstract%3A%20%20%20Imitation%20learning%20from%20human%20demonstrations%20is%20an%20effective%20means%20to%20teach%0Arobots%20manipulation%20skills.%20But%20data%20acquisition%20is%20a%20major%20bottleneck%20in%0Aapplying%20this%20paradigm%20more%20broadly%2C%20due%20to%20the%20amount%20of%20cost%20and%20human%20effort%0Ainvolved.%20There%20has%20been%20significant%20interest%20in%20imitation%20learning%20for%0Abimanual%20dexterous%20robots%2C%20like%20humanoids.%20Unfortunately%2C%20data%20collection%20is%0Aeven%20more%20challenging%20here%20due%20to%20the%20challenges%20of%20simultaneously%20controlling%0Amultiple%20arms%20and%20multi-fingered%20hands.%20Automated%20data%20generation%20in%20simulation%0Ais%20a%20compelling%2C%20scalable%20alternative%20to%20fuel%20this%20need%20for%20data.%20To%20this%20end%2C%0Awe%20introduce%20DexMimicGen%2C%20a%20large-scale%20automated%20data%20generation%20system%20that%0Asynthesizes%20trajectories%20from%20a%20handful%20of%20human%20demonstrations%20for%20humanoid%0Arobots%20with%20dexterous%20hands.%20We%20present%20a%20collection%20of%20simulation%20environments%0Ain%20the%20setting%20of%20bimanual%20dexterous%20manipulation%2C%20spanning%20a%20range%20of%0Amanipulation%20behaviors%20and%20different%20requirements%20for%20coordination%20among%20the%0Atwo%20arms.%20We%20generate%2021K%20demos%20across%20these%20tasks%20from%20just%2060%20source%20human%0Ademos%20and%20study%20the%20effect%20of%20several%20data%20generation%20and%20policy%20learning%0Adecisions%20on%20agent%20performance.%20Finally%2C%20we%20present%20a%20real-to-sim-to-real%0Apipeline%20and%20deploy%20it%20on%20a%20real-world%20humanoid%20can%20sorting%20task.%20Videos%20and%0Amore%20are%20at%20https%3A//dexmimicgen.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexMimicGen%253A%2520Automated%2520Data%2520Generation%2520for%2520Bimanual%2520Dexterous%250A%2520%2520Manipulation%2520via%2520Imitation%2520Learning%26entry.906535625%3DZhenyu%2520Jiang%2520and%2520Yuqi%2520Xie%2520and%2520Kevin%2520Lin%2520and%2520Zhenjia%2520Xu%2520and%2520Weikang%2520Wan%2520and%2520Ajay%2520Mandlekar%2520and%2520Linxi%2520Fan%2520and%2520Yuke%2520Zhu%26entry.1292438233%3D%2520%2520Imitation%2520learning%2520from%2520human%2520demonstrations%2520is%2520an%2520effective%2520means%2520to%2520teach%250Arobots%2520manipulation%2520skills.%2520But%2520data%2520acquisition%2520is%2520a%2520major%2520bottleneck%2520in%250Aapplying%2520this%2520paradigm%2520more%2520broadly%252C%2520due%2520to%2520the%2520amount%2520of%2520cost%2520and%2520human%2520effort%250Ainvolved.%2520There%2520has%2520been%2520significant%2520interest%2520in%2520imitation%2520learning%2520for%250Abimanual%2520dexterous%2520robots%252C%2520like%2520humanoids.%2520Unfortunately%252C%2520data%2520collection%2520is%250Aeven%2520more%2520challenging%2520here%2520due%2520to%2520the%2520challenges%2520of%2520simultaneously%2520controlling%250Amultiple%2520arms%2520and%2520multi-fingered%2520hands.%2520Automated%2520data%2520generation%2520in%2520simulation%250Ais%2520a%2520compelling%252C%2520scalable%2520alternative%2520to%2520fuel%2520this%2520need%2520for%2520data.%2520To%2520this%2520end%252C%250Awe%2520introduce%2520DexMimicGen%252C%2520a%2520large-scale%2520automated%2520data%2520generation%2520system%2520that%250Asynthesizes%2520trajectories%2520from%2520a%2520handful%2520of%2520human%2520demonstrations%2520for%2520humanoid%250Arobots%2520with%2520dexterous%2520hands.%2520We%2520present%2520a%2520collection%2520of%2520simulation%2520environments%250Ain%2520the%2520setting%2520of%2520bimanual%2520dexterous%2520manipulation%252C%2520spanning%2520a%2520range%2520of%250Amanipulation%2520behaviors%2520and%2520different%2520requirements%2520for%2520coordination%2520among%2520the%250Atwo%2520arms.%2520We%2520generate%252021K%2520demos%2520across%2520these%2520tasks%2520from%2520just%252060%2520source%2520human%250Ademos%2520and%2520study%2520the%2520effect%2520of%2520several%2520data%2520generation%2520and%2520policy%2520learning%250Adecisions%2520on%2520agent%2520performance.%2520Finally%252C%2520we%2520present%2520a%2520real-to-sim-to-real%250Apipeline%2520and%2520deploy%2520it%2520on%2520a%2520real-world%2520humanoid%2520can%2520sorting%2520task.%2520Videos%2520and%250Amore%2520are%2520at%2520https%253A//dexmimicgen.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DexMimicGen%3A%20Automated%20Data%20Generation%20for%20Bimanual%20Dexterous%0A%20%20Manipulation%20via%20Imitation%20Learning&entry.906535625=Zhenyu%20Jiang%20and%20Yuqi%20Xie%20and%20Kevin%20Lin%20and%20Zhenjia%20Xu%20and%20Weikang%20Wan%20and%20Ajay%20Mandlekar%20and%20Linxi%20Fan%20and%20Yuke%20Zhu&entry.1292438233=%20%20Imitation%20learning%20from%20human%20demonstrations%20is%20an%20effective%20means%20to%20teach%0Arobots%20manipulation%20skills.%20But%20data%20acquisition%20is%20a%20major%20bottleneck%20in%0Aapplying%20this%20paradigm%20more%20broadly%2C%20due%20to%20the%20amount%20of%20cost%20and%20human%20effort%0Ainvolved.%20There%20has%20been%20significant%20interest%20in%20imitation%20learning%20for%0Abimanual%20dexterous%20robots%2C%20like%20humanoids.%20Unfortunately%2C%20data%20collection%20is%0Aeven%20more%20challenging%20here%20due%20to%20the%20challenges%20of%20simultaneously%20controlling%0Amultiple%20arms%20and%20multi-fingered%20hands.%20Automated%20data%20generation%20in%20simulation%0Ais%20a%20compelling%2C%20scalable%20alternative%20to%20fuel%20this%20need%20for%20data.%20To%20this%20end%2C%0Awe%20introduce%20DexMimicGen%2C%20a%20large-scale%20automated%20data%20generation%20system%20that%0Asynthesizes%20trajectories%20from%20a%20handful%20of%20human%20demonstrations%20for%20humanoid%0Arobots%20with%20dexterous%20hands.%20We%20present%20a%20collection%20of%20simulation%20environments%0Ain%20the%20setting%20of%20bimanual%20dexterous%20manipulation%2C%20spanning%20a%20range%20of%0Amanipulation%20behaviors%20and%20different%20requirements%20for%20coordination%20among%20the%0Atwo%20arms.%20We%20generate%2021K%20demos%20across%20these%20tasks%20from%20just%2060%20source%20human%0Ademos%20and%20study%20the%20effect%20of%20several%20data%20generation%20and%20policy%20learning%0Adecisions%20on%20agent%20performance.%20Finally%2C%20we%20present%20a%20real-to-sim-to-real%0Apipeline%20and%20deploy%20it%20on%20a%20real-world%20humanoid%20can%20sorting%20task.%20Videos%20and%0Amore%20are%20at%20https%3A//dexmimicgen.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24185v1&entry.124074799=Read"},
{"title": "CaAdam: Improving Adam optimizer using connection aware methods", "author": "Remi Genet and Hugo Inzirillo", "abstract": "  We introduce a new method inspired by Adam that enhances convergence speed\nand achieves better loss function minima. Traditional optimizers, including\nAdam, apply uniform or globally adjusted learning rates across neural networks\nwithout considering their architectural specifics. This architecture-agnostic\napproach is deeply embedded in most deep learning frameworks, where optimizers\nare implemented as standalone modules without direct access to the network's\nstructural information. For instance, in popular frameworks like Keras or\nPyTorch, optimizers operate solely on gradients and parameters, without\nknowledge of layer connectivity or network topology. Our algorithm, CaAdam,\nexplores this overlooked area by introducing connection-aware optimization\nthrough carefully designed proxies of architectural information. We propose\nmultiple scaling methodologies that dynamically adjust learning rates based on\neasily accessible structural properties such as layer depth, connection counts,\nand gradient distributions. This approach enables more granular optimization\nwhile working within the constraints of current deep learning frameworks.\nEmpirical evaluations on standard datasets (e.g., CIFAR-10, Fashion MNIST) show\nthat our method consistently achieves faster convergence and higher accuracy\ncompared to standard Adam optimizer, demonstrating the potential benefits of\nincorporating architectural awareness in optimization strategies.\n", "link": "http://arxiv.org/abs/2410.24216v1", "date": "2024-10-31", "relevancy": 2.4473, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5157}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4788}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaAdam%3A%20Improving%20Adam%20optimizer%20using%20connection%20aware%20methods&body=Title%3A%20CaAdam%3A%20Improving%20Adam%20optimizer%20using%20connection%20aware%20methods%0AAuthor%3A%20Remi%20Genet%20and%20Hugo%20Inzirillo%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20method%20inspired%20by%20Adam%20that%20enhances%20convergence%20speed%0Aand%20achieves%20better%20loss%20function%20minima.%20Traditional%20optimizers%2C%20including%0AAdam%2C%20apply%20uniform%20or%20globally%20adjusted%20learning%20rates%20across%20neural%20networks%0Awithout%20considering%20their%20architectural%20specifics.%20This%20architecture-agnostic%0Aapproach%20is%20deeply%20embedded%20in%20most%20deep%20learning%20frameworks%2C%20where%20optimizers%0Aare%20implemented%20as%20standalone%20modules%20without%20direct%20access%20to%20the%20network%27s%0Astructural%20information.%20For%20instance%2C%20in%20popular%20frameworks%20like%20Keras%20or%0APyTorch%2C%20optimizers%20operate%20solely%20on%20gradients%20and%20parameters%2C%20without%0Aknowledge%20of%20layer%20connectivity%20or%20network%20topology.%20Our%20algorithm%2C%20CaAdam%2C%0Aexplores%20this%20overlooked%20area%20by%20introducing%20connection-aware%20optimization%0Athrough%20carefully%20designed%20proxies%20of%20architectural%20information.%20We%20propose%0Amultiple%20scaling%20methodologies%20that%20dynamically%20adjust%20learning%20rates%20based%20on%0Aeasily%20accessible%20structural%20properties%20such%20as%20layer%20depth%2C%20connection%20counts%2C%0Aand%20gradient%20distributions.%20This%20approach%20enables%20more%20granular%20optimization%0Awhile%20working%20within%20the%20constraints%20of%20current%20deep%20learning%20frameworks.%0AEmpirical%20evaluations%20on%20standard%20datasets%20%28e.g.%2C%20CIFAR-10%2C%20Fashion%20MNIST%29%20show%0Athat%20our%20method%20consistently%20achieves%20faster%20convergence%20and%20higher%20accuracy%0Acompared%20to%20standard%20Adam%20optimizer%2C%20demonstrating%20the%20potential%20benefits%20of%0Aincorporating%20architectural%20awareness%20in%20optimization%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaAdam%253A%2520Improving%2520Adam%2520optimizer%2520using%2520connection%2520aware%2520methods%26entry.906535625%3DRemi%2520Genet%2520and%2520Hugo%2520Inzirillo%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520method%2520inspired%2520by%2520Adam%2520that%2520enhances%2520convergence%2520speed%250Aand%2520achieves%2520better%2520loss%2520function%2520minima.%2520Traditional%2520optimizers%252C%2520including%250AAdam%252C%2520apply%2520uniform%2520or%2520globally%2520adjusted%2520learning%2520rates%2520across%2520neural%2520networks%250Awithout%2520considering%2520their%2520architectural%2520specifics.%2520This%2520architecture-agnostic%250Aapproach%2520is%2520deeply%2520embedded%2520in%2520most%2520deep%2520learning%2520frameworks%252C%2520where%2520optimizers%250Aare%2520implemented%2520as%2520standalone%2520modules%2520without%2520direct%2520access%2520to%2520the%2520network%2527s%250Astructural%2520information.%2520For%2520instance%252C%2520in%2520popular%2520frameworks%2520like%2520Keras%2520or%250APyTorch%252C%2520optimizers%2520operate%2520solely%2520on%2520gradients%2520and%2520parameters%252C%2520without%250Aknowledge%2520of%2520layer%2520connectivity%2520or%2520network%2520topology.%2520Our%2520algorithm%252C%2520CaAdam%252C%250Aexplores%2520this%2520overlooked%2520area%2520by%2520introducing%2520connection-aware%2520optimization%250Athrough%2520carefully%2520designed%2520proxies%2520of%2520architectural%2520information.%2520We%2520propose%250Amultiple%2520scaling%2520methodologies%2520that%2520dynamically%2520adjust%2520learning%2520rates%2520based%2520on%250Aeasily%2520accessible%2520structural%2520properties%2520such%2520as%2520layer%2520depth%252C%2520connection%2520counts%252C%250Aand%2520gradient%2520distributions.%2520This%2520approach%2520enables%2520more%2520granular%2520optimization%250Awhile%2520working%2520within%2520the%2520constraints%2520of%2520current%2520deep%2520learning%2520frameworks.%250AEmpirical%2520evaluations%2520on%2520standard%2520datasets%2520%2528e.g.%252C%2520CIFAR-10%252C%2520Fashion%2520MNIST%2529%2520show%250Athat%2520our%2520method%2520consistently%2520achieves%2520faster%2520convergence%2520and%2520higher%2520accuracy%250Acompared%2520to%2520standard%2520Adam%2520optimizer%252C%2520demonstrating%2520the%2520potential%2520benefits%2520of%250Aincorporating%2520architectural%2520awareness%2520in%2520optimization%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaAdam%3A%20Improving%20Adam%20optimizer%20using%20connection%20aware%20methods&entry.906535625=Remi%20Genet%20and%20Hugo%20Inzirillo&entry.1292438233=%20%20We%20introduce%20a%20new%20method%20inspired%20by%20Adam%20that%20enhances%20convergence%20speed%0Aand%20achieves%20better%20loss%20function%20minima.%20Traditional%20optimizers%2C%20including%0AAdam%2C%20apply%20uniform%20or%20globally%20adjusted%20learning%20rates%20across%20neural%20networks%0Awithout%20considering%20their%20architectural%20specifics.%20This%20architecture-agnostic%0Aapproach%20is%20deeply%20embedded%20in%20most%20deep%20learning%20frameworks%2C%20where%20optimizers%0Aare%20implemented%20as%20standalone%20modules%20without%20direct%20access%20to%20the%20network%27s%0Astructural%20information.%20For%20instance%2C%20in%20popular%20frameworks%20like%20Keras%20or%0APyTorch%2C%20optimizers%20operate%20solely%20on%20gradients%20and%20parameters%2C%20without%0Aknowledge%20of%20layer%20connectivity%20or%20network%20topology.%20Our%20algorithm%2C%20CaAdam%2C%0Aexplores%20this%20overlooked%20area%20by%20introducing%20connection-aware%20optimization%0Athrough%20carefully%20designed%20proxies%20of%20architectural%20information.%20We%20propose%0Amultiple%20scaling%20methodologies%20that%20dynamically%20adjust%20learning%20rates%20based%20on%0Aeasily%20accessible%20structural%20properties%20such%20as%20layer%20depth%2C%20connection%20counts%2C%0Aand%20gradient%20distributions.%20This%20approach%20enables%20more%20granular%20optimization%0Awhile%20working%20within%20the%20constraints%20of%20current%20deep%20learning%20frameworks.%0AEmpirical%20evaluations%20on%20standard%20datasets%20%28e.g.%2C%20CIFAR-10%2C%20Fashion%20MNIST%29%20show%0Athat%20our%20method%20consistently%20achieves%20faster%20convergence%20and%20higher%20accuracy%0Acompared%20to%20standard%20Adam%20optimizer%2C%20demonstrating%20the%20potential%20benefits%20of%0Aincorporating%20architectural%20awareness%20in%20optimization%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24216v1&entry.124074799=Read"},
{"title": "Revealing Fine-Grained Values and Opinions in Large Language Models", "author": "Dustin Wright and Arnav Arora and Nadav Borenstein and Srishti Yadav and Serge Belongie and Isabelle Augenstein", "abstract": "  Uncovering latent values and opinions embedded in large language models\n(LLMs) can help identify biases and mitigate potential harm. Recently, this has\nbeen approached by prompting LLMs with survey questions and quantifying the\nstances in the outputs towards morally and politically charged statements.\nHowever, the stances generated by LLMs can vary greatly depending on how they\nare prompted, and there are many ways to argue for or against a given position.\nIn this work, we propose to address this by analysing a large and robust\ndataset of 156k LLM responses to the 62 propositions of the Political Compass\nTest (PCT) generated by 6 LLMs using 420 prompt variations. We perform\ncoarse-grained analysis of their generated stances and fine-grained analysis of\nthe plain text justifications for those stances. For fine-grained analysis, we\npropose to identify tropes in the responses: semantically similar phrases that\nare recurrent and consistent across different prompts, revealing natural\npatterns in the text that a given LLM is prone to produce. We find that\ndemographic features added to prompts significantly affect outcomes on the PCT,\nreflecting bias, as well as disparities between the results of tests when\neliciting closed-form vs. open domain responses. Additionally, patterns in the\nplain text rationales via tropes show that similar justifications are\nrepeatedly generated across models and prompts even with disparate stances.\n", "link": "http://arxiv.org/abs/2406.19238v2", "date": "2024-10-31", "relevancy": 2.4339, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5031}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5031}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revealing%20Fine-Grained%20Values%20and%20Opinions%20in%20Large%20Language%20Models&body=Title%3A%20Revealing%20Fine-Grained%20Values%20and%20Opinions%20in%20Large%20Language%20Models%0AAuthor%3A%20Dustin%20Wright%20and%20Arnav%20Arora%20and%20Nadav%20Borenstein%20and%20Srishti%20Yadav%20and%20Serge%20Belongie%20and%20Isabelle%20Augenstein%0AAbstract%3A%20%20%20Uncovering%20latent%20values%20and%20opinions%20embedded%20in%20large%20language%20models%0A%28LLMs%29%20can%20help%20identify%20biases%20and%20mitigate%20potential%20harm.%20Recently%2C%20this%20has%0Abeen%20approached%20by%20prompting%20LLMs%20with%20survey%20questions%20and%20quantifying%20the%0Astances%20in%20the%20outputs%20towards%20morally%20and%20politically%20charged%20statements.%0AHowever%2C%20the%20stances%20generated%20by%20LLMs%20can%20vary%20greatly%20depending%20on%20how%20they%0Aare%20prompted%2C%20and%20there%20are%20many%20ways%20to%20argue%20for%20or%20against%20a%20given%20position.%0AIn%20this%20work%2C%20we%20propose%20to%20address%20this%20by%20analysing%20a%20large%20and%20robust%0Adataset%20of%20156k%20LLM%20responses%20to%20the%2062%20propositions%20of%20the%20Political%20Compass%0ATest%20%28PCT%29%20generated%20by%206%20LLMs%20using%20420%20prompt%20variations.%20We%20perform%0Acoarse-grained%20analysis%20of%20their%20generated%20stances%20and%20fine-grained%20analysis%20of%0Athe%20plain%20text%20justifications%20for%20those%20stances.%20For%20fine-grained%20analysis%2C%20we%0Apropose%20to%20identify%20tropes%20in%20the%20responses%3A%20semantically%20similar%20phrases%20that%0Aare%20recurrent%20and%20consistent%20across%20different%20prompts%2C%20revealing%20natural%0Apatterns%20in%20the%20text%20that%20a%20given%20LLM%20is%20prone%20to%20produce.%20We%20find%20that%0Ademographic%20features%20added%20to%20prompts%20significantly%20affect%20outcomes%20on%20the%20PCT%2C%0Areflecting%20bias%2C%20as%20well%20as%20disparities%20between%20the%20results%20of%20tests%20when%0Aeliciting%20closed-form%20vs.%20open%20domain%20responses.%20Additionally%2C%20patterns%20in%20the%0Aplain%20text%20rationales%20via%20tropes%20show%20that%20similar%20justifications%20are%0Arepeatedly%20generated%20across%20models%20and%20prompts%20even%20with%20disparate%20stances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19238v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevealing%2520Fine-Grained%2520Values%2520and%2520Opinions%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DDustin%2520Wright%2520and%2520Arnav%2520Arora%2520and%2520Nadav%2520Borenstein%2520and%2520Srishti%2520Yadav%2520and%2520Serge%2520Belongie%2520and%2520Isabelle%2520Augenstein%26entry.1292438233%3D%2520%2520Uncovering%2520latent%2520values%2520and%2520opinions%2520embedded%2520in%2520large%2520language%2520models%250A%2528LLMs%2529%2520can%2520help%2520identify%2520biases%2520and%2520mitigate%2520potential%2520harm.%2520Recently%252C%2520this%2520has%250Abeen%2520approached%2520by%2520prompting%2520LLMs%2520with%2520survey%2520questions%2520and%2520quantifying%2520the%250Astances%2520in%2520the%2520outputs%2520towards%2520morally%2520and%2520politically%2520charged%2520statements.%250AHowever%252C%2520the%2520stances%2520generated%2520by%2520LLMs%2520can%2520vary%2520greatly%2520depending%2520on%2520how%2520they%250Aare%2520prompted%252C%2520and%2520there%2520are%2520many%2520ways%2520to%2520argue%2520for%2520or%2520against%2520a%2520given%2520position.%250AIn%2520this%2520work%252C%2520we%2520propose%2520to%2520address%2520this%2520by%2520analysing%2520a%2520large%2520and%2520robust%250Adataset%2520of%2520156k%2520LLM%2520responses%2520to%2520the%252062%2520propositions%2520of%2520the%2520Political%2520Compass%250ATest%2520%2528PCT%2529%2520generated%2520by%25206%2520LLMs%2520using%2520420%2520prompt%2520variations.%2520We%2520perform%250Acoarse-grained%2520analysis%2520of%2520their%2520generated%2520stances%2520and%2520fine-grained%2520analysis%2520of%250Athe%2520plain%2520text%2520justifications%2520for%2520those%2520stances.%2520For%2520fine-grained%2520analysis%252C%2520we%250Apropose%2520to%2520identify%2520tropes%2520in%2520the%2520responses%253A%2520semantically%2520similar%2520phrases%2520that%250Aare%2520recurrent%2520and%2520consistent%2520across%2520different%2520prompts%252C%2520revealing%2520natural%250Apatterns%2520in%2520the%2520text%2520that%2520a%2520given%2520LLM%2520is%2520prone%2520to%2520produce.%2520We%2520find%2520that%250Ademographic%2520features%2520added%2520to%2520prompts%2520significantly%2520affect%2520outcomes%2520on%2520the%2520PCT%252C%250Areflecting%2520bias%252C%2520as%2520well%2520as%2520disparities%2520between%2520the%2520results%2520of%2520tests%2520when%250Aeliciting%2520closed-form%2520vs.%2520open%2520domain%2520responses.%2520Additionally%252C%2520patterns%2520in%2520the%250Aplain%2520text%2520rationales%2520via%2520tropes%2520show%2520that%2520similar%2520justifications%2520are%250Arepeatedly%2520generated%2520across%2520models%2520and%2520prompts%2520even%2520with%2520disparate%2520stances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19238v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revealing%20Fine-Grained%20Values%20and%20Opinions%20in%20Large%20Language%20Models&entry.906535625=Dustin%20Wright%20and%20Arnav%20Arora%20and%20Nadav%20Borenstein%20and%20Srishti%20Yadav%20and%20Serge%20Belongie%20and%20Isabelle%20Augenstein&entry.1292438233=%20%20Uncovering%20latent%20values%20and%20opinions%20embedded%20in%20large%20language%20models%0A%28LLMs%29%20can%20help%20identify%20biases%20and%20mitigate%20potential%20harm.%20Recently%2C%20this%20has%0Abeen%20approached%20by%20prompting%20LLMs%20with%20survey%20questions%20and%20quantifying%20the%0Astances%20in%20the%20outputs%20towards%20morally%20and%20politically%20charged%20statements.%0AHowever%2C%20the%20stances%20generated%20by%20LLMs%20can%20vary%20greatly%20depending%20on%20how%20they%0Aare%20prompted%2C%20and%20there%20are%20many%20ways%20to%20argue%20for%20or%20against%20a%20given%20position.%0AIn%20this%20work%2C%20we%20propose%20to%20address%20this%20by%20analysing%20a%20large%20and%20robust%0Adataset%20of%20156k%20LLM%20responses%20to%20the%2062%20propositions%20of%20the%20Political%20Compass%0ATest%20%28PCT%29%20generated%20by%206%20LLMs%20using%20420%20prompt%20variations.%20We%20perform%0Acoarse-grained%20analysis%20of%20their%20generated%20stances%20and%20fine-grained%20analysis%20of%0Athe%20plain%20text%20justifications%20for%20those%20stances.%20For%20fine-grained%20analysis%2C%20we%0Apropose%20to%20identify%20tropes%20in%20the%20responses%3A%20semantically%20similar%20phrases%20that%0Aare%20recurrent%20and%20consistent%20across%20different%20prompts%2C%20revealing%20natural%0Apatterns%20in%20the%20text%20that%20a%20given%20LLM%20is%20prone%20to%20produce.%20We%20find%20that%0Ademographic%20features%20added%20to%20prompts%20significantly%20affect%20outcomes%20on%20the%20PCT%2C%0Areflecting%20bias%2C%20as%20well%20as%20disparities%20between%20the%20results%20of%20tests%20when%0Aeliciting%20closed-form%20vs.%20open%20domain%20responses.%20Additionally%2C%20patterns%20in%20the%0Aplain%20text%20rationales%20via%20tropes%20show%20that%20similar%20justifications%20are%0Arepeatedly%20generated%20across%20models%20and%20prompts%20even%20with%20disparate%20stances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19238v2&entry.124074799=Read"},
{"title": "In-Context Fine-Tuning for Time-Series Foundation Models", "author": "Abhimanyu Das and Matthew Faw and Rajat Sen and Yichen Zhou", "abstract": "  Motivated by the recent success of time-series foundation models for\nzero-shot forecasting, we present a methodology for $\\textit{in-context\nfine-tuning}$ of a time-series foundation model. In particular, we design a\npretrained foundation model that can be prompted (at inference time) with\nmultiple time-series examples, in order to forecast a target time-series into\nthe future. Our foundation model is specifically trained to utilize examples\nfrom multiple related time-series in its context window (in addition to the\nhistory of the target time-series) to help it adapt to the specific\ndistribution of the target domain at inference time. We show that such a\nfoundation model that uses in-context examples at inference time can obtain\nmuch better performance on popular forecasting benchmarks compared to\nsupervised deep learning methods, statistical models, as well as other\ntime-series foundation models. Interestingly, our in-context fine-tuning\napproach even rivals the performance of a foundation model that is explicitly\nfine-tuned on the target domain.\n", "link": "http://arxiv.org/abs/2410.24087v1", "date": "2024-10-31", "relevancy": 2.4121, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4917}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4778}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-Context%20Fine-Tuning%20for%20Time-Series%20Foundation%20Models&body=Title%3A%20In-Context%20Fine-Tuning%20for%20Time-Series%20Foundation%20Models%0AAuthor%3A%20Abhimanyu%20Das%20and%20Matthew%20Faw%20and%20Rajat%20Sen%20and%20Yichen%20Zhou%0AAbstract%3A%20%20%20Motivated%20by%20the%20recent%20success%20of%20time-series%20foundation%20models%20for%0Azero-shot%20forecasting%2C%20we%20present%20a%20methodology%20for%20%24%5Ctextit%7Bin-context%0Afine-tuning%7D%24%20of%20a%20time-series%20foundation%20model.%20In%20particular%2C%20we%20design%20a%0Apretrained%20foundation%20model%20that%20can%20be%20prompted%20%28at%20inference%20time%29%20with%0Amultiple%20time-series%20examples%2C%20in%20order%20to%20forecast%20a%20target%20time-series%20into%0Athe%20future.%20Our%20foundation%20model%20is%20specifically%20trained%20to%20utilize%20examples%0Afrom%20multiple%20related%20time-series%20in%20its%20context%20window%20%28in%20addition%20to%20the%0Ahistory%20of%20the%20target%20time-series%29%20to%20help%20it%20adapt%20to%20the%20specific%0Adistribution%20of%20the%20target%20domain%20at%20inference%20time.%20We%20show%20that%20such%20a%0Afoundation%20model%20that%20uses%20in-context%20examples%20at%20inference%20time%20can%20obtain%0Amuch%20better%20performance%20on%20popular%20forecasting%20benchmarks%20compared%20to%0Asupervised%20deep%20learning%20methods%2C%20statistical%20models%2C%20as%20well%20as%20other%0Atime-series%20foundation%20models.%20Interestingly%2C%20our%20in-context%20fine-tuning%0Aapproach%20even%20rivals%20the%20performance%20of%20a%20foundation%20model%20that%20is%20explicitly%0Afine-tuned%20on%20the%20target%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24087v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-Context%2520Fine-Tuning%2520for%2520Time-Series%2520Foundation%2520Models%26entry.906535625%3DAbhimanyu%2520Das%2520and%2520Matthew%2520Faw%2520and%2520Rajat%2520Sen%2520and%2520Yichen%2520Zhou%26entry.1292438233%3D%2520%2520Motivated%2520by%2520the%2520recent%2520success%2520of%2520time-series%2520foundation%2520models%2520for%250Azero-shot%2520forecasting%252C%2520we%2520present%2520a%2520methodology%2520for%2520%2524%255Ctextit%257Bin-context%250Afine-tuning%257D%2524%2520of%2520a%2520time-series%2520foundation%2520model.%2520In%2520particular%252C%2520we%2520design%2520a%250Apretrained%2520foundation%2520model%2520that%2520can%2520be%2520prompted%2520%2528at%2520inference%2520time%2529%2520with%250Amultiple%2520time-series%2520examples%252C%2520in%2520order%2520to%2520forecast%2520a%2520target%2520time-series%2520into%250Athe%2520future.%2520Our%2520foundation%2520model%2520is%2520specifically%2520trained%2520to%2520utilize%2520examples%250Afrom%2520multiple%2520related%2520time-series%2520in%2520its%2520context%2520window%2520%2528in%2520addition%2520to%2520the%250Ahistory%2520of%2520the%2520target%2520time-series%2529%2520to%2520help%2520it%2520adapt%2520to%2520the%2520specific%250Adistribution%2520of%2520the%2520target%2520domain%2520at%2520inference%2520time.%2520We%2520show%2520that%2520such%2520a%250Afoundation%2520model%2520that%2520uses%2520in-context%2520examples%2520at%2520inference%2520time%2520can%2520obtain%250Amuch%2520better%2520performance%2520on%2520popular%2520forecasting%2520benchmarks%2520compared%2520to%250Asupervised%2520deep%2520learning%2520methods%252C%2520statistical%2520models%252C%2520as%2520well%2520as%2520other%250Atime-series%2520foundation%2520models.%2520Interestingly%252C%2520our%2520in-context%2520fine-tuning%250Aapproach%2520even%2520rivals%2520the%2520performance%2520of%2520a%2520foundation%2520model%2520that%2520is%2520explicitly%250Afine-tuned%2520on%2520the%2520target%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24087v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Context%20Fine-Tuning%20for%20Time-Series%20Foundation%20Models&entry.906535625=Abhimanyu%20Das%20and%20Matthew%20Faw%20and%20Rajat%20Sen%20and%20Yichen%20Zhou&entry.1292438233=%20%20Motivated%20by%20the%20recent%20success%20of%20time-series%20foundation%20models%20for%0Azero-shot%20forecasting%2C%20we%20present%20a%20methodology%20for%20%24%5Ctextit%7Bin-context%0Afine-tuning%7D%24%20of%20a%20time-series%20foundation%20model.%20In%20particular%2C%20we%20design%20a%0Apretrained%20foundation%20model%20that%20can%20be%20prompted%20%28at%20inference%20time%29%20with%0Amultiple%20time-series%20examples%2C%20in%20order%20to%20forecast%20a%20target%20time-series%20into%0Athe%20future.%20Our%20foundation%20model%20is%20specifically%20trained%20to%20utilize%20examples%0Afrom%20multiple%20related%20time-series%20in%20its%20context%20window%20%28in%20addition%20to%20the%0Ahistory%20of%20the%20target%20time-series%29%20to%20help%20it%20adapt%20to%20the%20specific%0Adistribution%20of%20the%20target%20domain%20at%20inference%20time.%20We%20show%20that%20such%20a%0Afoundation%20model%20that%20uses%20in-context%20examples%20at%20inference%20time%20can%20obtain%0Amuch%20better%20performance%20on%20popular%20forecasting%20benchmarks%20compared%20to%0Asupervised%20deep%20learning%20methods%2C%20statistical%20models%2C%20as%20well%20as%20other%0Atime-series%20foundation%20models.%20Interestingly%2C%20our%20in-context%20fine-tuning%0Aapproach%20even%20rivals%20the%20performance%20of%20a%20foundation%20model%20that%20is%20explicitly%0Afine-tuned%20on%20the%20target%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24087v1&entry.124074799=Read"},
{"title": "3D-ViTac: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing", "author": "Binghao Huang and Yixuan Wang and Xinyi Yang and Yiyue Luo and Yunzhu Li", "abstract": "  Tactile and visual perception are both crucial for humans to perform\nfine-grained interactions with their environment. Developing similar\nmulti-modal sensing capabilities for robots can significantly enhance and\nexpand their manipulation skills. This paper introduces \\textbf{3D-ViTac}, a\nmulti-modal sensing and learning system designed for dexterous bimanual\nmanipulation. Our system features tactile sensors equipped with dense sensing\nunits, each covering an area of 3$mm^2$. These sensors are low-cost and\nflexible, providing detailed and extensive coverage of physical contacts,\neffectively complementing visual information. To integrate tactile and visual\ndata, we fuse them into a unified 3D representation space that preserves their\n3D structures and spatial relationships. The multi-modal representation can\nthen be coupled with diffusion policies for imitation learning. Through\nconcrete hardware experiments, we demonstrate that even low-cost robots can\nperform precise manipulations and significantly outperform vision-only\npolicies, particularly in safe interactions with fragile items and executing\nlong-horizon tasks involving in-hand manipulation. Our project page is\navailable at \\url{https://binghao-huang.github.io/3D-ViTac/}.\n", "link": "http://arxiv.org/abs/2410.24091v1", "date": "2024-10-31", "relevancy": 2.4076, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6144}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6064}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5924}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D-ViTac%3A%20Learning%20Fine-Grained%20Manipulation%20with%20Visuo-Tactile%20Sensing&body=Title%3A%203D-ViTac%3A%20Learning%20Fine-Grained%20Manipulation%20with%20Visuo-Tactile%20Sensing%0AAuthor%3A%20Binghao%20Huang%20and%20Yixuan%20Wang%20and%20Xinyi%20Yang%20and%20Yiyue%20Luo%20and%20Yunzhu%20Li%0AAbstract%3A%20%20%20Tactile%20and%20visual%20perception%20are%20both%20crucial%20for%20humans%20to%20perform%0Afine-grained%20interactions%20with%20their%20environment.%20Developing%20similar%0Amulti-modal%20sensing%20capabilities%20for%20robots%20can%20significantly%20enhance%20and%0Aexpand%20their%20manipulation%20skills.%20This%20paper%20introduces%20%5Ctextbf%7B3D-ViTac%7D%2C%20a%0Amulti-modal%20sensing%20and%20learning%20system%20designed%20for%20dexterous%20bimanual%0Amanipulation.%20Our%20system%20features%20tactile%20sensors%20equipped%20with%20dense%20sensing%0Aunits%2C%20each%20covering%20an%20area%20of%203%24mm%5E2%24.%20These%20sensors%20are%20low-cost%20and%0Aflexible%2C%20providing%20detailed%20and%20extensive%20coverage%20of%20physical%20contacts%2C%0Aeffectively%20complementing%20visual%20information.%20To%20integrate%20tactile%20and%20visual%0Adata%2C%20we%20fuse%20them%20into%20a%20unified%203D%20representation%20space%20that%20preserves%20their%0A3D%20structures%20and%20spatial%20relationships.%20The%20multi-modal%20representation%20can%0Athen%20be%20coupled%20with%20diffusion%20policies%20for%20imitation%20learning.%20Through%0Aconcrete%20hardware%20experiments%2C%20we%20demonstrate%20that%20even%20low-cost%20robots%20can%0Aperform%20precise%20manipulations%20and%20significantly%20outperform%20vision-only%0Apolicies%2C%20particularly%20in%20safe%20interactions%20with%20fragile%20items%20and%20executing%0Along-horizon%20tasks%20involving%20in-hand%20manipulation.%20Our%20project%20page%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//binghao-huang.github.io/3D-ViTac/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D-ViTac%253A%2520Learning%2520Fine-Grained%2520Manipulation%2520with%2520Visuo-Tactile%2520Sensing%26entry.906535625%3DBinghao%2520Huang%2520and%2520Yixuan%2520Wang%2520and%2520Xinyi%2520Yang%2520and%2520Yiyue%2520Luo%2520and%2520Yunzhu%2520Li%26entry.1292438233%3D%2520%2520Tactile%2520and%2520visual%2520perception%2520are%2520both%2520crucial%2520for%2520humans%2520to%2520perform%250Afine-grained%2520interactions%2520with%2520their%2520environment.%2520Developing%2520similar%250Amulti-modal%2520sensing%2520capabilities%2520for%2520robots%2520can%2520significantly%2520enhance%2520and%250Aexpand%2520their%2520manipulation%2520skills.%2520This%2520paper%2520introduces%2520%255Ctextbf%257B3D-ViTac%257D%252C%2520a%250Amulti-modal%2520sensing%2520and%2520learning%2520system%2520designed%2520for%2520dexterous%2520bimanual%250Amanipulation.%2520Our%2520system%2520features%2520tactile%2520sensors%2520equipped%2520with%2520dense%2520sensing%250Aunits%252C%2520each%2520covering%2520an%2520area%2520of%25203%2524mm%255E2%2524.%2520These%2520sensors%2520are%2520low-cost%2520and%250Aflexible%252C%2520providing%2520detailed%2520and%2520extensive%2520coverage%2520of%2520physical%2520contacts%252C%250Aeffectively%2520complementing%2520visual%2520information.%2520To%2520integrate%2520tactile%2520and%2520visual%250Adata%252C%2520we%2520fuse%2520them%2520into%2520a%2520unified%25203D%2520representation%2520space%2520that%2520preserves%2520their%250A3D%2520structures%2520and%2520spatial%2520relationships.%2520The%2520multi-modal%2520representation%2520can%250Athen%2520be%2520coupled%2520with%2520diffusion%2520policies%2520for%2520imitation%2520learning.%2520Through%250Aconcrete%2520hardware%2520experiments%252C%2520we%2520demonstrate%2520that%2520even%2520low-cost%2520robots%2520can%250Aperform%2520precise%2520manipulations%2520and%2520significantly%2520outperform%2520vision-only%250Apolicies%252C%2520particularly%2520in%2520safe%2520interactions%2520with%2520fragile%2520items%2520and%2520executing%250Along-horizon%2520tasks%2520involving%2520in-hand%2520manipulation.%2520Our%2520project%2520page%2520is%250Aavailable%2520at%2520%255Curl%257Bhttps%253A//binghao-huang.github.io/3D-ViTac/%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D-ViTac%3A%20Learning%20Fine-Grained%20Manipulation%20with%20Visuo-Tactile%20Sensing&entry.906535625=Binghao%20Huang%20and%20Yixuan%20Wang%20and%20Xinyi%20Yang%20and%20Yiyue%20Luo%20and%20Yunzhu%20Li&entry.1292438233=%20%20Tactile%20and%20visual%20perception%20are%20both%20crucial%20for%20humans%20to%20perform%0Afine-grained%20interactions%20with%20their%20environment.%20Developing%20similar%0Amulti-modal%20sensing%20capabilities%20for%20robots%20can%20significantly%20enhance%20and%0Aexpand%20their%20manipulation%20skills.%20This%20paper%20introduces%20%5Ctextbf%7B3D-ViTac%7D%2C%20a%0Amulti-modal%20sensing%20and%20learning%20system%20designed%20for%20dexterous%20bimanual%0Amanipulation.%20Our%20system%20features%20tactile%20sensors%20equipped%20with%20dense%20sensing%0Aunits%2C%20each%20covering%20an%20area%20of%203%24mm%5E2%24.%20These%20sensors%20are%20low-cost%20and%0Aflexible%2C%20providing%20detailed%20and%20extensive%20coverage%20of%20physical%20contacts%2C%0Aeffectively%20complementing%20visual%20information.%20To%20integrate%20tactile%20and%20visual%0Adata%2C%20we%20fuse%20them%20into%20a%20unified%203D%20representation%20space%20that%20preserves%20their%0A3D%20structures%20and%20spatial%20relationships.%20The%20multi-modal%20representation%20can%0Athen%20be%20coupled%20with%20diffusion%20policies%20for%20imitation%20learning.%20Through%0Aconcrete%20hardware%20experiments%2C%20we%20demonstrate%20that%20even%20low-cost%20robots%20can%0Aperform%20precise%20manipulations%20and%20significantly%20outperform%20vision-only%0Apolicies%2C%20particularly%20in%20safe%20interactions%20with%20fragile%20items%20and%20executing%0Along-horizon%20tasks%20involving%20in-hand%20manipulation.%20Our%20project%20page%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//binghao-huang.github.io/3D-ViTac/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24091v1&entry.124074799=Read"},
{"title": "EPIC: Effective Prompting for Imbalanced-Class Data Synthesis in Tabular\n  Data Classification via Large Language Models", "author": "Jinhee Kim and Taesung Kim and Jaegul Choo", "abstract": "  Large language models (LLMs) have demonstrated remarkable in-context learning\ncapabilities across diverse applications. In this work, we explore the\neffectiveness of LLMs for generating realistic synthetic tabular data,\nidentifying key prompt design elements to optimize performance. We introduce\nEPIC, a novel approach that leverages balanced, grouped data samples and\nconsistent formatting with unique variable mapping to guide LLMs in generating\naccurate synthetic data across all classes, even for imbalanced datasets.\nEvaluations on real-world datasets show that EPIC achieves state-of-the-art\nmachine learning classification performance, significantly improving generation\nefficiency. These findings highlight the effectiveness of EPIC for synthetic\ntabular data generation, particularly in addressing class imbalance. Our source\ncode for our work is available at:\nhttps://seharanul17.github.io/project-synthetic-tabular-llm/\n", "link": "http://arxiv.org/abs/2404.12404v3", "date": "2024-10-31", "relevancy": 2.406, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4999}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4722}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EPIC%3A%20Effective%20Prompting%20for%20Imbalanced-Class%20Data%20Synthesis%20in%20Tabular%0A%20%20Data%20Classification%20via%20Large%20Language%20Models&body=Title%3A%20EPIC%3A%20Effective%20Prompting%20for%20Imbalanced-Class%20Data%20Synthesis%20in%20Tabular%0A%20%20Data%20Classification%20via%20Large%20Language%20Models%0AAuthor%3A%20Jinhee%20Kim%20and%20Taesung%20Kim%20and%20Jaegul%20Choo%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20in-context%20learning%0Acapabilities%20across%20diverse%20applications.%20In%20this%20work%2C%20we%20explore%20the%0Aeffectiveness%20of%20LLMs%20for%20generating%20realistic%20synthetic%20tabular%20data%2C%0Aidentifying%20key%20prompt%20design%20elements%20to%20optimize%20performance.%20We%20introduce%0AEPIC%2C%20a%20novel%20approach%20that%20leverages%20balanced%2C%20grouped%20data%20samples%20and%0Aconsistent%20formatting%20with%20unique%20variable%20mapping%20to%20guide%20LLMs%20in%20generating%0Aaccurate%20synthetic%20data%20across%20all%20classes%2C%20even%20for%20imbalanced%20datasets.%0AEvaluations%20on%20real-world%20datasets%20show%20that%20EPIC%20achieves%20state-of-the-art%0Amachine%20learning%20classification%20performance%2C%20significantly%20improving%20generation%0Aefficiency.%20These%20findings%20highlight%20the%20effectiveness%20of%20EPIC%20for%20synthetic%0Atabular%20data%20generation%2C%20particularly%20in%20addressing%20class%20imbalance.%20Our%20source%0Acode%20for%20our%20work%20is%20available%20at%3A%0Ahttps%3A//seharanul17.github.io/project-synthetic-tabular-llm/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12404v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEPIC%253A%2520Effective%2520Prompting%2520for%2520Imbalanced-Class%2520Data%2520Synthesis%2520in%2520Tabular%250A%2520%2520Data%2520Classification%2520via%2520Large%2520Language%2520Models%26entry.906535625%3DJinhee%2520Kim%2520and%2520Taesung%2520Kim%2520and%2520Jaegul%2520Choo%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520in-context%2520learning%250Acapabilities%2520across%2520diverse%2520applications.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%250Aeffectiveness%2520of%2520LLMs%2520for%2520generating%2520realistic%2520synthetic%2520tabular%2520data%252C%250Aidentifying%2520key%2520prompt%2520design%2520elements%2520to%2520optimize%2520performance.%2520We%2520introduce%250AEPIC%252C%2520a%2520novel%2520approach%2520that%2520leverages%2520balanced%252C%2520grouped%2520data%2520samples%2520and%250Aconsistent%2520formatting%2520with%2520unique%2520variable%2520mapping%2520to%2520guide%2520LLMs%2520in%2520generating%250Aaccurate%2520synthetic%2520data%2520across%2520all%2520classes%252C%2520even%2520for%2520imbalanced%2520datasets.%250AEvaluations%2520on%2520real-world%2520datasets%2520show%2520that%2520EPIC%2520achieves%2520state-of-the-art%250Amachine%2520learning%2520classification%2520performance%252C%2520significantly%2520improving%2520generation%250Aefficiency.%2520These%2520findings%2520highlight%2520the%2520effectiveness%2520of%2520EPIC%2520for%2520synthetic%250Atabular%2520data%2520generation%252C%2520particularly%2520in%2520addressing%2520class%2520imbalance.%2520Our%2520source%250Acode%2520for%2520our%2520work%2520is%2520available%2520at%253A%250Ahttps%253A//seharanul17.github.io/project-synthetic-tabular-llm/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.12404v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EPIC%3A%20Effective%20Prompting%20for%20Imbalanced-Class%20Data%20Synthesis%20in%20Tabular%0A%20%20Data%20Classification%20via%20Large%20Language%20Models&entry.906535625=Jinhee%20Kim%20and%20Taesung%20Kim%20and%20Jaegul%20Choo&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20in-context%20learning%0Acapabilities%20across%20diverse%20applications.%20In%20this%20work%2C%20we%20explore%20the%0Aeffectiveness%20of%20LLMs%20for%20generating%20realistic%20synthetic%20tabular%20data%2C%0Aidentifying%20key%20prompt%20design%20elements%20to%20optimize%20performance.%20We%20introduce%0AEPIC%2C%20a%20novel%20approach%20that%20leverages%20balanced%2C%20grouped%20data%20samples%20and%0Aconsistent%20formatting%20with%20unique%20variable%20mapping%20to%20guide%20LLMs%20in%20generating%0Aaccurate%20synthetic%20data%20across%20all%20classes%2C%20even%20for%20imbalanced%20datasets.%0AEvaluations%20on%20real-world%20datasets%20show%20that%20EPIC%20achieves%20state-of-the-art%0Amachine%20learning%20classification%20performance%2C%20significantly%20improving%20generation%0Aefficiency.%20These%20findings%20highlight%20the%20effectiveness%20of%20EPIC%20for%20synthetic%0Atabular%20data%20generation%2C%20particularly%20in%20addressing%20class%20imbalance.%20Our%20source%0Acode%20for%20our%20work%20is%20available%20at%3A%0Ahttps%3A//seharanul17.github.io/project-synthetic-tabular-llm/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12404v3&entry.124074799=Read"},
{"title": "TPC: Test-time Procrustes Calibration for Diffusion-based Human Image\n  Animation", "author": "Sunjae Yoon and Gwanhyeong Koo and Younghwan Lee and Chang D. Yoo", "abstract": "  Human image animation aims to generate a human motion video from the inputs\nof a reference human image and a target motion video. Current diffusion-based\nimage animation systems exhibit high precision in transferring human identity\ninto targeted motion, yet they still exhibit irregular quality in their\noutputs. Their optimal precision is achieved only when the physical\ncompositions (i.e., scale and rotation) of the human shapes in the reference\nimage and target pose frame are aligned. In the absence of such alignment,\nthere is a noticeable decline in fidelity and consistency. Especially, in\nreal-world environments, this compositional misalignment commonly occurs,\nposing significant challenges to the practical usage of current systems. To\nthis end, we propose Test-time Procrustes Calibration (TPC), which enhances the\nrobustness of diffusion-based image animation systems by maintaining optimal\nperformance even when faced with compositional misalignment, effectively\naddressing real-world scenarios. The TPC provides a calibrated reference image\nfor the diffusion model, enhancing its capability to understand the\ncorrespondence between human shapes in the reference and target images. Our\nmethod is simple and can be applied to any diffusion-based image animation\nsystem in a model-agnostic manner, improving the effectiveness at test time\nwithout additional training.\n", "link": "http://arxiv.org/abs/2410.24037v1", "date": "2024-10-31", "relevancy": 2.3879, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5984}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5961}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TPC%3A%20Test-time%20Procrustes%20Calibration%20for%20Diffusion-based%20Human%20Image%0A%20%20Animation&body=Title%3A%20TPC%3A%20Test-time%20Procrustes%20Calibration%20for%20Diffusion-based%20Human%20Image%0A%20%20Animation%0AAuthor%3A%20Sunjae%20Yoon%20and%20Gwanhyeong%20Koo%20and%20Younghwan%20Lee%20and%20Chang%20D.%20Yoo%0AAbstract%3A%20%20%20Human%20image%20animation%20aims%20to%20generate%20a%20human%20motion%20video%20from%20the%20inputs%0Aof%20a%20reference%20human%20image%20and%20a%20target%20motion%20video.%20Current%20diffusion-based%0Aimage%20animation%20systems%20exhibit%20high%20precision%20in%20transferring%20human%20identity%0Ainto%20targeted%20motion%2C%20yet%20they%20still%20exhibit%20irregular%20quality%20in%20their%0Aoutputs.%20Their%20optimal%20precision%20is%20achieved%20only%20when%20the%20physical%0Acompositions%20%28i.e.%2C%20scale%20and%20rotation%29%20of%20the%20human%20shapes%20in%20the%20reference%0Aimage%20and%20target%20pose%20frame%20are%20aligned.%20In%20the%20absence%20of%20such%20alignment%2C%0Athere%20is%20a%20noticeable%20decline%20in%20fidelity%20and%20consistency.%20Especially%2C%20in%0Areal-world%20environments%2C%20this%20compositional%20misalignment%20commonly%20occurs%2C%0Aposing%20significant%20challenges%20to%20the%20practical%20usage%20of%20current%20systems.%20To%0Athis%20end%2C%20we%20propose%20Test-time%20Procrustes%20Calibration%20%28TPC%29%2C%20which%20enhances%20the%0Arobustness%20of%20diffusion-based%20image%20animation%20systems%20by%20maintaining%20optimal%0Aperformance%20even%20when%20faced%20with%20compositional%20misalignment%2C%20effectively%0Aaddressing%20real-world%20scenarios.%20The%20TPC%20provides%20a%20calibrated%20reference%20image%0Afor%20the%20diffusion%20model%2C%20enhancing%20its%20capability%20to%20understand%20the%0Acorrespondence%20between%20human%20shapes%20in%20the%20reference%20and%20target%20images.%20Our%0Amethod%20is%20simple%20and%20can%20be%20applied%20to%20any%20diffusion-based%20image%20animation%0Asystem%20in%20a%20model-agnostic%20manner%2C%20improving%20the%20effectiveness%20at%20test%20time%0Awithout%20additional%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24037v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTPC%253A%2520Test-time%2520Procrustes%2520Calibration%2520for%2520Diffusion-based%2520Human%2520Image%250A%2520%2520Animation%26entry.906535625%3DSunjae%2520Yoon%2520and%2520Gwanhyeong%2520Koo%2520and%2520Younghwan%2520Lee%2520and%2520Chang%2520D.%2520Yoo%26entry.1292438233%3D%2520%2520Human%2520image%2520animation%2520aims%2520to%2520generate%2520a%2520human%2520motion%2520video%2520from%2520the%2520inputs%250Aof%2520a%2520reference%2520human%2520image%2520and%2520a%2520target%2520motion%2520video.%2520Current%2520diffusion-based%250Aimage%2520animation%2520systems%2520exhibit%2520high%2520precision%2520in%2520transferring%2520human%2520identity%250Ainto%2520targeted%2520motion%252C%2520yet%2520they%2520still%2520exhibit%2520irregular%2520quality%2520in%2520their%250Aoutputs.%2520Their%2520optimal%2520precision%2520is%2520achieved%2520only%2520when%2520the%2520physical%250Acompositions%2520%2528i.e.%252C%2520scale%2520and%2520rotation%2529%2520of%2520the%2520human%2520shapes%2520in%2520the%2520reference%250Aimage%2520and%2520target%2520pose%2520frame%2520are%2520aligned.%2520In%2520the%2520absence%2520of%2520such%2520alignment%252C%250Athere%2520is%2520a%2520noticeable%2520decline%2520in%2520fidelity%2520and%2520consistency.%2520Especially%252C%2520in%250Areal-world%2520environments%252C%2520this%2520compositional%2520misalignment%2520commonly%2520occurs%252C%250Aposing%2520significant%2520challenges%2520to%2520the%2520practical%2520usage%2520of%2520current%2520systems.%2520To%250Athis%2520end%252C%2520we%2520propose%2520Test-time%2520Procrustes%2520Calibration%2520%2528TPC%2529%252C%2520which%2520enhances%2520the%250Arobustness%2520of%2520diffusion-based%2520image%2520animation%2520systems%2520by%2520maintaining%2520optimal%250Aperformance%2520even%2520when%2520faced%2520with%2520compositional%2520misalignment%252C%2520effectively%250Aaddressing%2520real-world%2520scenarios.%2520The%2520TPC%2520provides%2520a%2520calibrated%2520reference%2520image%250Afor%2520the%2520diffusion%2520model%252C%2520enhancing%2520its%2520capability%2520to%2520understand%2520the%250Acorrespondence%2520between%2520human%2520shapes%2520in%2520the%2520reference%2520and%2520target%2520images.%2520Our%250Amethod%2520is%2520simple%2520and%2520can%2520be%2520applied%2520to%2520any%2520diffusion-based%2520image%2520animation%250Asystem%2520in%2520a%2520model-agnostic%2520manner%252C%2520improving%2520the%2520effectiveness%2520at%2520test%2520time%250Awithout%2520additional%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24037v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TPC%3A%20Test-time%20Procrustes%20Calibration%20for%20Diffusion-based%20Human%20Image%0A%20%20Animation&entry.906535625=Sunjae%20Yoon%20and%20Gwanhyeong%20Koo%20and%20Younghwan%20Lee%20and%20Chang%20D.%20Yoo&entry.1292438233=%20%20Human%20image%20animation%20aims%20to%20generate%20a%20human%20motion%20video%20from%20the%20inputs%0Aof%20a%20reference%20human%20image%20and%20a%20target%20motion%20video.%20Current%20diffusion-based%0Aimage%20animation%20systems%20exhibit%20high%20precision%20in%20transferring%20human%20identity%0Ainto%20targeted%20motion%2C%20yet%20they%20still%20exhibit%20irregular%20quality%20in%20their%0Aoutputs.%20Their%20optimal%20precision%20is%20achieved%20only%20when%20the%20physical%0Acompositions%20%28i.e.%2C%20scale%20and%20rotation%29%20of%20the%20human%20shapes%20in%20the%20reference%0Aimage%20and%20target%20pose%20frame%20are%20aligned.%20In%20the%20absence%20of%20such%20alignment%2C%0Athere%20is%20a%20noticeable%20decline%20in%20fidelity%20and%20consistency.%20Especially%2C%20in%0Areal-world%20environments%2C%20this%20compositional%20misalignment%20commonly%20occurs%2C%0Aposing%20significant%20challenges%20to%20the%20practical%20usage%20of%20current%20systems.%20To%0Athis%20end%2C%20we%20propose%20Test-time%20Procrustes%20Calibration%20%28TPC%29%2C%20which%20enhances%20the%0Arobustness%20of%20diffusion-based%20image%20animation%20systems%20by%20maintaining%20optimal%0Aperformance%20even%20when%20faced%20with%20compositional%20misalignment%2C%20effectively%0Aaddressing%20real-world%20scenarios.%20The%20TPC%20provides%20a%20calibrated%20reference%20image%0Afor%20the%20diffusion%20model%2C%20enhancing%20its%20capability%20to%20understand%20the%0Acorrespondence%20between%20human%20shapes%20in%20the%20reference%20and%20target%20images.%20Our%0Amethod%20is%20simple%20and%20can%20be%20applied%20to%20any%20diffusion-based%20image%20animation%0Asystem%20in%20a%20model-agnostic%20manner%2C%20improving%20the%20effectiveness%20at%20test%20time%0Awithout%20additional%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24037v1&entry.124074799=Read"},
{"title": "Group Crosscoders for Mechanistic Analysis of Symmetry", "author": "Liv Gorton", "abstract": "  We introduce group crosscoders, an extension of crosscoders that\nsystematically discover and analyse symmetrical features in neural networks.\nWhile neural networks often develop equivariant representations without\nexplicit architectural constraints, understanding these emergent symmetries has\ntraditionally relied on manual analysis. Group crosscoders automate this\nprocess by performing dictionary learning across transformed versions of inputs\nunder a symmetry group. Applied to InceptionV1's mixed3b layer using the\ndihedral group $\\mathrm{D}_{32}$, our method reveals several key insights:\nFirst, it naturally clusters features into interpretable families that\ncorrespond to previously hypothesised feature types, providing more precise\nseparation than standard sparse autoencoders. Second, our transform block\nanalysis enables the automatic characterisation of feature symmetries,\nrevealing how different geometric features (such as curves versus lines)\nexhibit distinct patterns of invariance and equivariance. These results\ndemonstrate that group crosscoders can provide systematic insights into how\nneural networks represent symmetry, offering a promising new tool for\nmechanistic interpretability.\n", "link": "http://arxiv.org/abs/2410.24184v1", "date": "2024-10-31", "relevancy": 2.3862, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4811}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4811}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Group%20Crosscoders%20for%20Mechanistic%20Analysis%20of%20Symmetry&body=Title%3A%20Group%20Crosscoders%20for%20Mechanistic%20Analysis%20of%20Symmetry%0AAuthor%3A%20Liv%20Gorton%0AAbstract%3A%20%20%20We%20introduce%20group%20crosscoders%2C%20an%20extension%20of%20crosscoders%20that%0Asystematically%20discover%20and%20analyse%20symmetrical%20features%20in%20neural%20networks.%0AWhile%20neural%20networks%20often%20develop%20equivariant%20representations%20without%0Aexplicit%20architectural%20constraints%2C%20understanding%20these%20emergent%20symmetries%20has%0Atraditionally%20relied%20on%20manual%20analysis.%20Group%20crosscoders%20automate%20this%0Aprocess%20by%20performing%20dictionary%20learning%20across%20transformed%20versions%20of%20inputs%0Aunder%20a%20symmetry%20group.%20Applied%20to%20InceptionV1%27s%20mixed3b%20layer%20using%20the%0Adihedral%20group%20%24%5Cmathrm%7BD%7D_%7B32%7D%24%2C%20our%20method%20reveals%20several%20key%20insights%3A%0AFirst%2C%20it%20naturally%20clusters%20features%20into%20interpretable%20families%20that%0Acorrespond%20to%20previously%20hypothesised%20feature%20types%2C%20providing%20more%20precise%0Aseparation%20than%20standard%20sparse%20autoencoders.%20Second%2C%20our%20transform%20block%0Aanalysis%20enables%20the%20automatic%20characterisation%20of%20feature%20symmetries%2C%0Arevealing%20how%20different%20geometric%20features%20%28such%20as%20curves%20versus%20lines%29%0Aexhibit%20distinct%20patterns%20of%20invariance%20and%20equivariance.%20These%20results%0Ademonstrate%20that%20group%20crosscoders%20can%20provide%20systematic%20insights%20into%20how%0Aneural%20networks%20represent%20symmetry%2C%20offering%20a%20promising%20new%20tool%20for%0Amechanistic%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24184v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGroup%2520Crosscoders%2520for%2520Mechanistic%2520Analysis%2520of%2520Symmetry%26entry.906535625%3DLiv%2520Gorton%26entry.1292438233%3D%2520%2520We%2520introduce%2520group%2520crosscoders%252C%2520an%2520extension%2520of%2520crosscoders%2520that%250Asystematically%2520discover%2520and%2520analyse%2520symmetrical%2520features%2520in%2520neural%2520networks.%250AWhile%2520neural%2520networks%2520often%2520develop%2520equivariant%2520representations%2520without%250Aexplicit%2520architectural%2520constraints%252C%2520understanding%2520these%2520emergent%2520symmetries%2520has%250Atraditionally%2520relied%2520on%2520manual%2520analysis.%2520Group%2520crosscoders%2520automate%2520this%250Aprocess%2520by%2520performing%2520dictionary%2520learning%2520across%2520transformed%2520versions%2520of%2520inputs%250Aunder%2520a%2520symmetry%2520group.%2520Applied%2520to%2520InceptionV1%2527s%2520mixed3b%2520layer%2520using%2520the%250Adihedral%2520group%2520%2524%255Cmathrm%257BD%257D_%257B32%257D%2524%252C%2520our%2520method%2520reveals%2520several%2520key%2520insights%253A%250AFirst%252C%2520it%2520naturally%2520clusters%2520features%2520into%2520interpretable%2520families%2520that%250Acorrespond%2520to%2520previously%2520hypothesised%2520feature%2520types%252C%2520providing%2520more%2520precise%250Aseparation%2520than%2520standard%2520sparse%2520autoencoders.%2520Second%252C%2520our%2520transform%2520block%250Aanalysis%2520enables%2520the%2520automatic%2520characterisation%2520of%2520feature%2520symmetries%252C%250Arevealing%2520how%2520different%2520geometric%2520features%2520%2528such%2520as%2520curves%2520versus%2520lines%2529%250Aexhibit%2520distinct%2520patterns%2520of%2520invariance%2520and%2520equivariance.%2520These%2520results%250Ademonstrate%2520that%2520group%2520crosscoders%2520can%2520provide%2520systematic%2520insights%2520into%2520how%250Aneural%2520networks%2520represent%2520symmetry%252C%2520offering%2520a%2520promising%2520new%2520tool%2520for%250Amechanistic%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24184v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Group%20Crosscoders%20for%20Mechanistic%20Analysis%20of%20Symmetry&entry.906535625=Liv%20Gorton&entry.1292438233=%20%20We%20introduce%20group%20crosscoders%2C%20an%20extension%20of%20crosscoders%20that%0Asystematically%20discover%20and%20analyse%20symmetrical%20features%20in%20neural%20networks.%0AWhile%20neural%20networks%20often%20develop%20equivariant%20representations%20without%0Aexplicit%20architectural%20constraints%2C%20understanding%20these%20emergent%20symmetries%20has%0Atraditionally%20relied%20on%20manual%20analysis.%20Group%20crosscoders%20automate%20this%0Aprocess%20by%20performing%20dictionary%20learning%20across%20transformed%20versions%20of%20inputs%0Aunder%20a%20symmetry%20group.%20Applied%20to%20InceptionV1%27s%20mixed3b%20layer%20using%20the%0Adihedral%20group%20%24%5Cmathrm%7BD%7D_%7B32%7D%24%2C%20our%20method%20reveals%20several%20key%20insights%3A%0AFirst%2C%20it%20naturally%20clusters%20features%20into%20interpretable%20families%20that%0Acorrespond%20to%20previously%20hypothesised%20feature%20types%2C%20providing%20more%20precise%0Aseparation%20than%20standard%20sparse%20autoencoders.%20Second%2C%20our%20transform%20block%0Aanalysis%20enables%20the%20automatic%20characterisation%20of%20feature%20symmetries%2C%0Arevealing%20how%20different%20geometric%20features%20%28such%20as%20curves%20versus%20lines%29%0Aexhibit%20distinct%20patterns%20of%20invariance%20and%20equivariance.%20These%20results%0Ademonstrate%20that%20group%20crosscoders%20can%20provide%20systematic%20insights%20into%20how%0Aneural%20networks%20represent%20symmetry%2C%20offering%20a%20promising%20new%20tool%20for%0Amechanistic%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24184v1&entry.124074799=Read"},
{"title": "SelfCodeAlign: Self-Alignment for Code Generation", "author": "Yuxiang Wei and Federico Cassano and Jiawei Liu and Yifeng Ding and Naman Jain and Zachary Mueller and Harm de Vries and Leandro von Werra and Arjun Guha and Lingming Zhang", "abstract": "  Instruction tuning is a supervised fine-tuning approach that significantly\nimproves the ability of large language models (LLMs) to follow human\ninstructions. We propose SelfCodeAlign, the first fully transparent and\npermissive pipeline for self-aligning code LLMs without extensive human\nannotations or distillation. SelfCodeAlign employs the same base model for\ninference throughout the data generation process. It first extracts diverse\ncoding concepts from high-quality seed snippets to generate new tasks. It then\nsamples multiple responses per task, pairs each with test cases, and validates\nthem in a sandbox environment. Finally, passing examples are selected for\ninstruction tuning. In our primary experiments, we use SelfCodeAlign with\nCodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs.\nFinetuning on this dataset leads to a model that achieves a 67.1 pass@1 on\nHumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller.\nAcross all benchmarks, this finetuned model consistently outperforms the\noriginal version trained with OctoPack, the previous state-of-the-art method\nfor instruction tuning without human annotations or distillation. Additionally,\nwe show that SelfCodeAlign is effective across LLMs of various sizes, from 3B\nto 33B, and that the base models can benefit more from alignment with their own\ndata distribution. We further validate each component's effectiveness in our\npipeline, showing that SelfCodeAlign outperforms both direct distillation from\nGPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and\nEvol-Instruct. SelfCodeAlign has also led to the creation of\nStarCoder2-Instruct, the first fully transparent, permissively licensed, and\nself-aligned code LLM that achieves state-of-the-art coding performance.\n", "link": "http://arxiv.org/abs/2410.24198v1", "date": "2024-10-31", "relevancy": 2.3805, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4813}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4799}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SelfCodeAlign%3A%20Self-Alignment%20for%20Code%20Generation&body=Title%3A%20SelfCodeAlign%3A%20Self-Alignment%20for%20Code%20Generation%0AAuthor%3A%20Yuxiang%20Wei%20and%20Federico%20Cassano%20and%20Jiawei%20Liu%20and%20Yifeng%20Ding%20and%20Naman%20Jain%20and%20Zachary%20Mueller%20and%20Harm%20de%20Vries%20and%20Leandro%20von%20Werra%20and%20Arjun%20Guha%20and%20Lingming%20Zhang%0AAbstract%3A%20%20%20Instruction%20tuning%20is%20a%20supervised%20fine-tuning%20approach%20that%20significantly%0Aimproves%20the%20ability%20of%20large%20language%20models%20%28LLMs%29%20to%20follow%20human%0Ainstructions.%20We%20propose%20SelfCodeAlign%2C%20the%20first%20fully%20transparent%20and%0Apermissive%20pipeline%20for%20self-aligning%20code%20LLMs%20without%20extensive%20human%0Aannotations%20or%20distillation.%20SelfCodeAlign%20employs%20the%20same%20base%20model%20for%0Ainference%20throughout%20the%20data%20generation%20process.%20It%20first%20extracts%20diverse%0Acoding%20concepts%20from%20high-quality%20seed%20snippets%20to%20generate%20new%20tasks.%20It%20then%0Asamples%20multiple%20responses%20per%20task%2C%20pairs%20each%20with%20test%20cases%2C%20and%20validates%0Athem%20in%20a%20sandbox%20environment.%20Finally%2C%20passing%20examples%20are%20selected%20for%0Ainstruction%20tuning.%20In%20our%20primary%20experiments%2C%20we%20use%20SelfCodeAlign%20with%0ACodeQwen1.5-7B%20to%20generate%20a%20dataset%20of%2074k%20instruction-response%20pairs.%0AFinetuning%20on%20this%20dataset%20leads%20to%20a%20model%20that%20achieves%20a%2067.1%20pass%401%20on%0AHumanEval%2B%2C%20surpassing%20CodeLlama-70B-Instruct%20despite%20being%20ten%20times%20smaller.%0AAcross%20all%20benchmarks%2C%20this%20finetuned%20model%20consistently%20outperforms%20the%0Aoriginal%20version%20trained%20with%20OctoPack%2C%20the%20previous%20state-of-the-art%20method%0Afor%20instruction%20tuning%20without%20human%20annotations%20or%20distillation.%20Additionally%2C%0Awe%20show%20that%20SelfCodeAlign%20is%20effective%20across%20LLMs%20of%20various%20sizes%2C%20from%203B%0Ato%2033B%2C%20and%20that%20the%20base%20models%20can%20benefit%20more%20from%20alignment%20with%20their%20own%0Adata%20distribution.%20We%20further%20validate%20each%20component%27s%20effectiveness%20in%20our%0Apipeline%2C%20showing%20that%20SelfCodeAlign%20outperforms%20both%20direct%20distillation%20from%0AGPT-4o%20and%20leading%20GPT-3.5-based%20distillation%20methods%2C%20such%20as%20OSS-Instruct%20and%0AEvol-Instruct.%20SelfCodeAlign%20has%20also%20led%20to%20the%20creation%20of%0AStarCoder2-Instruct%2C%20the%20first%20fully%20transparent%2C%20permissively%20licensed%2C%20and%0Aself-aligned%20code%20LLM%20that%20achieves%20state-of-the-art%20coding%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelfCodeAlign%253A%2520Self-Alignment%2520for%2520Code%2520Generation%26entry.906535625%3DYuxiang%2520Wei%2520and%2520Federico%2520Cassano%2520and%2520Jiawei%2520Liu%2520and%2520Yifeng%2520Ding%2520and%2520Naman%2520Jain%2520and%2520Zachary%2520Mueller%2520and%2520Harm%2520de%2520Vries%2520and%2520Leandro%2520von%2520Werra%2520and%2520Arjun%2520Guha%2520and%2520Lingming%2520Zhang%26entry.1292438233%3D%2520%2520Instruction%2520tuning%2520is%2520a%2520supervised%2520fine-tuning%2520approach%2520that%2520significantly%250Aimproves%2520the%2520ability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520follow%2520human%250Ainstructions.%2520We%2520propose%2520SelfCodeAlign%252C%2520the%2520first%2520fully%2520transparent%2520and%250Apermissive%2520pipeline%2520for%2520self-aligning%2520code%2520LLMs%2520without%2520extensive%2520human%250Aannotations%2520or%2520distillation.%2520SelfCodeAlign%2520employs%2520the%2520same%2520base%2520model%2520for%250Ainference%2520throughout%2520the%2520data%2520generation%2520process.%2520It%2520first%2520extracts%2520diverse%250Acoding%2520concepts%2520from%2520high-quality%2520seed%2520snippets%2520to%2520generate%2520new%2520tasks.%2520It%2520then%250Asamples%2520multiple%2520responses%2520per%2520task%252C%2520pairs%2520each%2520with%2520test%2520cases%252C%2520and%2520validates%250Athem%2520in%2520a%2520sandbox%2520environment.%2520Finally%252C%2520passing%2520examples%2520are%2520selected%2520for%250Ainstruction%2520tuning.%2520In%2520our%2520primary%2520experiments%252C%2520we%2520use%2520SelfCodeAlign%2520with%250ACodeQwen1.5-7B%2520to%2520generate%2520a%2520dataset%2520of%252074k%2520instruction-response%2520pairs.%250AFinetuning%2520on%2520this%2520dataset%2520leads%2520to%2520a%2520model%2520that%2520achieves%2520a%252067.1%2520pass%25401%2520on%250AHumanEval%252B%252C%2520surpassing%2520CodeLlama-70B-Instruct%2520despite%2520being%2520ten%2520times%2520smaller.%250AAcross%2520all%2520benchmarks%252C%2520this%2520finetuned%2520model%2520consistently%2520outperforms%2520the%250Aoriginal%2520version%2520trained%2520with%2520OctoPack%252C%2520the%2520previous%2520state-of-the-art%2520method%250Afor%2520instruction%2520tuning%2520without%2520human%2520annotations%2520or%2520distillation.%2520Additionally%252C%250Awe%2520show%2520that%2520SelfCodeAlign%2520is%2520effective%2520across%2520LLMs%2520of%2520various%2520sizes%252C%2520from%25203B%250Ato%252033B%252C%2520and%2520that%2520the%2520base%2520models%2520can%2520benefit%2520more%2520from%2520alignment%2520with%2520their%2520own%250Adata%2520distribution.%2520We%2520further%2520validate%2520each%2520component%2527s%2520effectiveness%2520in%2520our%250Apipeline%252C%2520showing%2520that%2520SelfCodeAlign%2520outperforms%2520both%2520direct%2520distillation%2520from%250AGPT-4o%2520and%2520leading%2520GPT-3.5-based%2520distillation%2520methods%252C%2520such%2520as%2520OSS-Instruct%2520and%250AEvol-Instruct.%2520SelfCodeAlign%2520has%2520also%2520led%2520to%2520the%2520creation%2520of%250AStarCoder2-Instruct%252C%2520the%2520first%2520fully%2520transparent%252C%2520permissively%2520licensed%252C%2520and%250Aself-aligned%2520code%2520LLM%2520that%2520achieves%2520state-of-the-art%2520coding%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SelfCodeAlign%3A%20Self-Alignment%20for%20Code%20Generation&entry.906535625=Yuxiang%20Wei%20and%20Federico%20Cassano%20and%20Jiawei%20Liu%20and%20Yifeng%20Ding%20and%20Naman%20Jain%20and%20Zachary%20Mueller%20and%20Harm%20de%20Vries%20and%20Leandro%20von%20Werra%20and%20Arjun%20Guha%20and%20Lingming%20Zhang&entry.1292438233=%20%20Instruction%20tuning%20is%20a%20supervised%20fine-tuning%20approach%20that%20significantly%0Aimproves%20the%20ability%20of%20large%20language%20models%20%28LLMs%29%20to%20follow%20human%0Ainstructions.%20We%20propose%20SelfCodeAlign%2C%20the%20first%20fully%20transparent%20and%0Apermissive%20pipeline%20for%20self-aligning%20code%20LLMs%20without%20extensive%20human%0Aannotations%20or%20distillation.%20SelfCodeAlign%20employs%20the%20same%20base%20model%20for%0Ainference%20throughout%20the%20data%20generation%20process.%20It%20first%20extracts%20diverse%0Acoding%20concepts%20from%20high-quality%20seed%20snippets%20to%20generate%20new%20tasks.%20It%20then%0Asamples%20multiple%20responses%20per%20task%2C%20pairs%20each%20with%20test%20cases%2C%20and%20validates%0Athem%20in%20a%20sandbox%20environment.%20Finally%2C%20passing%20examples%20are%20selected%20for%0Ainstruction%20tuning.%20In%20our%20primary%20experiments%2C%20we%20use%20SelfCodeAlign%20with%0ACodeQwen1.5-7B%20to%20generate%20a%20dataset%20of%2074k%20instruction-response%20pairs.%0AFinetuning%20on%20this%20dataset%20leads%20to%20a%20model%20that%20achieves%20a%2067.1%20pass%401%20on%0AHumanEval%2B%2C%20surpassing%20CodeLlama-70B-Instruct%20despite%20being%20ten%20times%20smaller.%0AAcross%20all%20benchmarks%2C%20this%20finetuned%20model%20consistently%20outperforms%20the%0Aoriginal%20version%20trained%20with%20OctoPack%2C%20the%20previous%20state-of-the-art%20method%0Afor%20instruction%20tuning%20without%20human%20annotations%20or%20distillation.%20Additionally%2C%0Awe%20show%20that%20SelfCodeAlign%20is%20effective%20across%20LLMs%20of%20various%20sizes%2C%20from%203B%0Ato%2033B%2C%20and%20that%20the%20base%20models%20can%20benefit%20more%20from%20alignment%20with%20their%20own%0Adata%20distribution.%20We%20further%20validate%20each%20component%27s%20effectiveness%20in%20our%0Apipeline%2C%20showing%20that%20SelfCodeAlign%20outperforms%20both%20direct%20distillation%20from%0AGPT-4o%20and%20leading%20GPT-3.5-based%20distillation%20methods%2C%20such%20as%20OSS-Instruct%20and%0AEvol-Instruct.%20SelfCodeAlign%20has%20also%20led%20to%20the%20creation%20of%0AStarCoder2-Instruct%2C%20the%20first%20fully%20transparent%2C%20permissively%20licensed%2C%20and%0Aself-aligned%20code%20LLM%20that%20achieves%20state-of-the-art%20coding%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24198v1&entry.124074799=Read"},
{"title": "Leveraging Large Language Models for Code Translation and Software\n  Development in Scientific Computing", "author": "Akash Dhruv and Anshu Dubey", "abstract": "  The emergence of foundational models and generative artificial intelligence\n(GenAI) is poised to transform productivity in scientific computing, especially\nin code development, refactoring, and translating from one programming language\nto another. However, because the output of GenAI cannot be guaranteed to be\ncorrect, manual intervention remains necessary. Some of this intervention can\nbe automated through task-specific tools, alongside additional methodologies\nfor correctness verification and effective prompt development. We explored the\napplication of GenAI in assisting with code translation, language\ninteroperability, and codebase inspection within a legacy Fortran codebase used\nto simulate particle interactions at the Large Hadron Collider (LHC). In the\nprocess, we developed a tool, CodeScribe, which combines prompt engineering\nwith user supervision to establish an efficient process for code conversion. In\nthis paper, we demonstrate how CodeScribe assists in converting Fortran code to\nC++, generating Fortran-C APIs for integrating legacy systems with modern C++\nlibraries, and providing developer support for code organization and algorithm\nimplementation. We also address the challenges of AI-driven code translation\nand highlight its benefits for enhancing productivity in scientific computing\nworkflows.\n", "link": "http://arxiv.org/abs/2410.24119v1", "date": "2024-10-31", "relevancy": 2.3664, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4756}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4721}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Large%20Language%20Models%20for%20Code%20Translation%20and%20Software%0A%20%20Development%20in%20Scientific%20Computing&body=Title%3A%20Leveraging%20Large%20Language%20Models%20for%20Code%20Translation%20and%20Software%0A%20%20Development%20in%20Scientific%20Computing%0AAuthor%3A%20Akash%20Dhruv%20and%20Anshu%20Dubey%0AAbstract%3A%20%20%20The%20emergence%20of%20foundational%20models%20and%20generative%20artificial%20intelligence%0A%28GenAI%29%20is%20poised%20to%20transform%20productivity%20in%20scientific%20computing%2C%20especially%0Ain%20code%20development%2C%20refactoring%2C%20and%20translating%20from%20one%20programming%20language%0Ato%20another.%20However%2C%20because%20the%20output%20of%20GenAI%20cannot%20be%20guaranteed%20to%20be%0Acorrect%2C%20manual%20intervention%20remains%20necessary.%20Some%20of%20this%20intervention%20can%0Abe%20automated%20through%20task-specific%20tools%2C%20alongside%20additional%20methodologies%0Afor%20correctness%20verification%20and%20effective%20prompt%20development.%20We%20explored%20the%0Aapplication%20of%20GenAI%20in%20assisting%20with%20code%20translation%2C%20language%0Ainteroperability%2C%20and%20codebase%20inspection%20within%20a%20legacy%20Fortran%20codebase%20used%0Ato%20simulate%20particle%20interactions%20at%20the%20Large%20Hadron%20Collider%20%28LHC%29.%20In%20the%0Aprocess%2C%20we%20developed%20a%20tool%2C%20CodeScribe%2C%20which%20combines%20prompt%20engineering%0Awith%20user%20supervision%20to%20establish%20an%20efficient%20process%20for%20code%20conversion.%20In%0Athis%20paper%2C%20we%20demonstrate%20how%20CodeScribe%20assists%20in%20converting%20Fortran%20code%20to%0AC%2B%2B%2C%20generating%20Fortran-C%20APIs%20for%20integrating%20legacy%20systems%20with%20modern%20C%2B%2B%0Alibraries%2C%20and%20providing%20developer%20support%20for%20code%20organization%20and%20algorithm%0Aimplementation.%20We%20also%20address%20the%20challenges%20of%20AI-driven%20code%20translation%0Aand%20highlight%20its%20benefits%20for%20enhancing%20productivity%20in%20scientific%20computing%0Aworkflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Large%2520Language%2520Models%2520for%2520Code%2520Translation%2520and%2520Software%250A%2520%2520Development%2520in%2520Scientific%2520Computing%26entry.906535625%3DAkash%2520Dhruv%2520and%2520Anshu%2520Dubey%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520foundational%2520models%2520and%2520generative%2520artificial%2520intelligence%250A%2528GenAI%2529%2520is%2520poised%2520to%2520transform%2520productivity%2520in%2520scientific%2520computing%252C%2520especially%250Ain%2520code%2520development%252C%2520refactoring%252C%2520and%2520translating%2520from%2520one%2520programming%2520language%250Ato%2520another.%2520However%252C%2520because%2520the%2520output%2520of%2520GenAI%2520cannot%2520be%2520guaranteed%2520to%2520be%250Acorrect%252C%2520manual%2520intervention%2520remains%2520necessary.%2520Some%2520of%2520this%2520intervention%2520can%250Abe%2520automated%2520through%2520task-specific%2520tools%252C%2520alongside%2520additional%2520methodologies%250Afor%2520correctness%2520verification%2520and%2520effective%2520prompt%2520development.%2520We%2520explored%2520the%250Aapplication%2520of%2520GenAI%2520in%2520assisting%2520with%2520code%2520translation%252C%2520language%250Ainteroperability%252C%2520and%2520codebase%2520inspection%2520within%2520a%2520legacy%2520Fortran%2520codebase%2520used%250Ato%2520simulate%2520particle%2520interactions%2520at%2520the%2520Large%2520Hadron%2520Collider%2520%2528LHC%2529.%2520In%2520the%250Aprocess%252C%2520we%2520developed%2520a%2520tool%252C%2520CodeScribe%252C%2520which%2520combines%2520prompt%2520engineering%250Awith%2520user%2520supervision%2520to%2520establish%2520an%2520efficient%2520process%2520for%2520code%2520conversion.%2520In%250Athis%2520paper%252C%2520we%2520demonstrate%2520how%2520CodeScribe%2520assists%2520in%2520converting%2520Fortran%2520code%2520to%250AC%252B%252B%252C%2520generating%2520Fortran-C%2520APIs%2520for%2520integrating%2520legacy%2520systems%2520with%2520modern%2520C%252B%252B%250Alibraries%252C%2520and%2520providing%2520developer%2520support%2520for%2520code%2520organization%2520and%2520algorithm%250Aimplementation.%2520We%2520also%2520address%2520the%2520challenges%2520of%2520AI-driven%2520code%2520translation%250Aand%2520highlight%2520its%2520benefits%2520for%2520enhancing%2520productivity%2520in%2520scientific%2520computing%250Aworkflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Large%20Language%20Models%20for%20Code%20Translation%20and%20Software%0A%20%20Development%20in%20Scientific%20Computing&entry.906535625=Akash%20Dhruv%20and%20Anshu%20Dubey&entry.1292438233=%20%20The%20emergence%20of%20foundational%20models%20and%20generative%20artificial%20intelligence%0A%28GenAI%29%20is%20poised%20to%20transform%20productivity%20in%20scientific%20computing%2C%20especially%0Ain%20code%20development%2C%20refactoring%2C%20and%20translating%20from%20one%20programming%20language%0Ato%20another.%20However%2C%20because%20the%20output%20of%20GenAI%20cannot%20be%20guaranteed%20to%20be%0Acorrect%2C%20manual%20intervention%20remains%20necessary.%20Some%20of%20this%20intervention%20can%0Abe%20automated%20through%20task-specific%20tools%2C%20alongside%20additional%20methodologies%0Afor%20correctness%20verification%20and%20effective%20prompt%20development.%20We%20explored%20the%0Aapplication%20of%20GenAI%20in%20assisting%20with%20code%20translation%2C%20language%0Ainteroperability%2C%20and%20codebase%20inspection%20within%20a%20legacy%20Fortran%20codebase%20used%0Ato%20simulate%20particle%20interactions%20at%20the%20Large%20Hadron%20Collider%20%28LHC%29.%20In%20the%0Aprocess%2C%20we%20developed%20a%20tool%2C%20CodeScribe%2C%20which%20combines%20prompt%20engineering%0Awith%20user%20supervision%20to%20establish%20an%20efficient%20process%20for%20code%20conversion.%20In%0Athis%20paper%2C%20we%20demonstrate%20how%20CodeScribe%20assists%20in%20converting%20Fortran%20code%20to%0AC%2B%2B%2C%20generating%20Fortran-C%20APIs%20for%20integrating%20legacy%20systems%20with%20modern%20C%2B%2B%0Alibraries%2C%20and%20providing%20developer%20support%20for%20code%20organization%20and%20algorithm%0Aimplementation.%20We%20also%20address%20the%20challenges%20of%20AI-driven%20code%20translation%0Aand%20highlight%20its%20benefits%20for%20enhancing%20productivity%20in%20scientific%20computing%0Aworkflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24119v1&entry.124074799=Read"},
{"title": "Improving Linear System Solvers for Hyperparameter Optimisation in\n  Iterative Gaussian Processes", "author": "Jihao Andreas Lin and Shreyas Padhy and Bruno Mlodozeniec and Javier Antor\u00e1n and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "abstract": "  Scaling hyperparameter optimisation to very large datasets remains an open\nproblem in the Gaussian process community. This paper focuses on iterative\nmethods, which use linear system solvers, like conjugate gradients, alternating\nprojections or stochastic gradient descent, to construct an estimate of the\nmarginal likelihood gradient. We discuss three key improvements which are\napplicable across solvers: (i) a pathwise gradient estimator, which reduces the\nrequired number of solver iterations and amortises the computational cost of\nmaking predictions, (ii) warm starting linear system solvers with the solution\nfrom the previous step, which leads to faster solver convergence at the cost of\nnegligible bias, (iii) early stopping linear system solvers after a limited\ncomputational budget, which synergises with warm starting, allowing solver\nprogress to accumulate over multiple marginal likelihood steps. These\ntechniques provide speed-ups of up to $72\\times$ when solving to tolerance, and\ndecrease the average residual norm by up to $7\\times$ when stopping early.\n", "link": "http://arxiv.org/abs/2405.18457v3", "date": "2024-10-31", "relevancy": 2.3445, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4892}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4761}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Linear%20System%20Solvers%20for%20Hyperparameter%20Optimisation%20in%0A%20%20Iterative%20Gaussian%20Processes&body=Title%3A%20Improving%20Linear%20System%20Solvers%20for%20Hyperparameter%20Optimisation%20in%0A%20%20Iterative%20Gaussian%20Processes%0AAuthor%3A%20Jihao%20Andreas%20Lin%20and%20Shreyas%20Padhy%20and%20Bruno%20Mlodozeniec%20and%20Javier%20Antor%C3%A1n%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato%0AAbstract%3A%20%20%20Scaling%20hyperparameter%20optimisation%20to%20very%20large%20datasets%20remains%20an%20open%0Aproblem%20in%20the%20Gaussian%20process%20community.%20This%20paper%20focuses%20on%20iterative%0Amethods%2C%20which%20use%20linear%20system%20solvers%2C%20like%20conjugate%20gradients%2C%20alternating%0Aprojections%20or%20stochastic%20gradient%20descent%2C%20to%20construct%20an%20estimate%20of%20the%0Amarginal%20likelihood%20gradient.%20We%20discuss%20three%20key%20improvements%20which%20are%0Aapplicable%20across%20solvers%3A%20%28i%29%20a%20pathwise%20gradient%20estimator%2C%20which%20reduces%20the%0Arequired%20number%20of%20solver%20iterations%20and%20amortises%20the%20computational%20cost%20of%0Amaking%20predictions%2C%20%28ii%29%20warm%20starting%20linear%20system%20solvers%20with%20the%20solution%0Afrom%20the%20previous%20step%2C%20which%20leads%20to%20faster%20solver%20convergence%20at%20the%20cost%20of%0Anegligible%20bias%2C%20%28iii%29%20early%20stopping%20linear%20system%20solvers%20after%20a%20limited%0Acomputational%20budget%2C%20which%20synergises%20with%20warm%20starting%2C%20allowing%20solver%0Aprogress%20to%20accumulate%20over%20multiple%20marginal%20likelihood%20steps.%20These%0Atechniques%20provide%20speed-ups%20of%20up%20to%20%2472%5Ctimes%24%20when%20solving%20to%20tolerance%2C%20and%0Adecrease%20the%20average%20residual%20norm%20by%20up%20to%20%247%5Ctimes%24%20when%20stopping%20early.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18457v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Linear%2520System%2520Solvers%2520for%2520Hyperparameter%2520Optimisation%2520in%250A%2520%2520Iterative%2520Gaussian%2520Processes%26entry.906535625%3DJihao%2520Andreas%2520Lin%2520and%2520Shreyas%2520Padhy%2520and%2520Bruno%2520Mlodozeniec%2520and%2520Javier%2520Antor%25C3%25A1n%2520and%2520Jos%25C3%25A9%2520Miguel%2520Hern%25C3%25A1ndez-Lobato%26entry.1292438233%3D%2520%2520Scaling%2520hyperparameter%2520optimisation%2520to%2520very%2520large%2520datasets%2520remains%2520an%2520open%250Aproblem%2520in%2520the%2520Gaussian%2520process%2520community.%2520This%2520paper%2520focuses%2520on%2520iterative%250Amethods%252C%2520which%2520use%2520linear%2520system%2520solvers%252C%2520like%2520conjugate%2520gradients%252C%2520alternating%250Aprojections%2520or%2520stochastic%2520gradient%2520descent%252C%2520to%2520construct%2520an%2520estimate%2520of%2520the%250Amarginal%2520likelihood%2520gradient.%2520We%2520discuss%2520three%2520key%2520improvements%2520which%2520are%250Aapplicable%2520across%2520solvers%253A%2520%2528i%2529%2520a%2520pathwise%2520gradient%2520estimator%252C%2520which%2520reduces%2520the%250Arequired%2520number%2520of%2520solver%2520iterations%2520and%2520amortises%2520the%2520computational%2520cost%2520of%250Amaking%2520predictions%252C%2520%2528ii%2529%2520warm%2520starting%2520linear%2520system%2520solvers%2520with%2520the%2520solution%250Afrom%2520the%2520previous%2520step%252C%2520which%2520leads%2520to%2520faster%2520solver%2520convergence%2520at%2520the%2520cost%2520of%250Anegligible%2520bias%252C%2520%2528iii%2529%2520early%2520stopping%2520linear%2520system%2520solvers%2520after%2520a%2520limited%250Acomputational%2520budget%252C%2520which%2520synergises%2520with%2520warm%2520starting%252C%2520allowing%2520solver%250Aprogress%2520to%2520accumulate%2520over%2520multiple%2520marginal%2520likelihood%2520steps.%2520These%250Atechniques%2520provide%2520speed-ups%2520of%2520up%2520to%2520%252472%255Ctimes%2524%2520when%2520solving%2520to%2520tolerance%252C%2520and%250Adecrease%2520the%2520average%2520residual%2520norm%2520by%2520up%2520to%2520%25247%255Ctimes%2524%2520when%2520stopping%2520early.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18457v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Linear%20System%20Solvers%20for%20Hyperparameter%20Optimisation%20in%0A%20%20Iterative%20Gaussian%20Processes&entry.906535625=Jihao%20Andreas%20Lin%20and%20Shreyas%20Padhy%20and%20Bruno%20Mlodozeniec%20and%20Javier%20Antor%C3%A1n%20and%20Jos%C3%A9%20Miguel%20Hern%C3%A1ndez-Lobato&entry.1292438233=%20%20Scaling%20hyperparameter%20optimisation%20to%20very%20large%20datasets%20remains%20an%20open%0Aproblem%20in%20the%20Gaussian%20process%20community.%20This%20paper%20focuses%20on%20iterative%0Amethods%2C%20which%20use%20linear%20system%20solvers%2C%20like%20conjugate%20gradients%2C%20alternating%0Aprojections%20or%20stochastic%20gradient%20descent%2C%20to%20construct%20an%20estimate%20of%20the%0Amarginal%20likelihood%20gradient.%20We%20discuss%20three%20key%20improvements%20which%20are%0Aapplicable%20across%20solvers%3A%20%28i%29%20a%20pathwise%20gradient%20estimator%2C%20which%20reduces%20the%0Arequired%20number%20of%20solver%20iterations%20and%20amortises%20the%20computational%20cost%20of%0Amaking%20predictions%2C%20%28ii%29%20warm%20starting%20linear%20system%20solvers%20with%20the%20solution%0Afrom%20the%20previous%20step%2C%20which%20leads%20to%20faster%20solver%20convergence%20at%20the%20cost%20of%0Anegligible%20bias%2C%20%28iii%29%20early%20stopping%20linear%20system%20solvers%20after%20a%20limited%0Acomputational%20budget%2C%20which%20synergises%20with%20warm%20starting%2C%20allowing%20solver%0Aprogress%20to%20accumulate%20over%20multiple%20marginal%20likelihood%20steps.%20These%0Atechniques%20provide%20speed-ups%20of%20up%20to%20%2472%5Ctimes%24%20when%20solving%20to%20tolerance%2C%20and%0Adecrease%20the%20average%20residual%20norm%20by%20up%20to%20%247%5Ctimes%24%20when%20stopping%20early.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18457v3&entry.124074799=Read"},
{"title": "Reducing Oversmoothing through Informed Weight Initialization in Graph\n  Neural Networks", "author": "Dimitrios Kelesis and Dimitris Fotakis and Georgios Paliouras", "abstract": "  In this work, we generalize the ideas of Kaiming initialization to Graph\nNeural Networks (GNNs) and propose a new scheme (G-Init) that reduces\noversmoothing, leading to very good results in node and graph classification\ntasks. GNNs are commonly initialized using methods designed for other types of\nNeural Networks, overlooking the underlying graph topology. We analyze\ntheoretically the variance of signals flowing forward and gradients flowing\nbackward in the class of convolutional GNNs. We then simplify our analysis to\nthe case of the GCN and propose a new initialization method. Our results\nindicate that the new method (G-Init) reduces oversmoothing in deep GNNs,\nfacilitating their effective use. Experimental validation supports our\ntheoretical findings, demonstrating the advantages of deep networks in\nscenarios with no feature information for unlabeled nodes (i.e., ``cold start''\nscenario).\n", "link": "http://arxiv.org/abs/2410.23830v1", "date": "2024-10-31", "relevancy": 2.3354, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4687}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.467}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reducing%20Oversmoothing%20through%20Informed%20Weight%20Initialization%20in%20Graph%0A%20%20Neural%20Networks&body=Title%3A%20Reducing%20Oversmoothing%20through%20Informed%20Weight%20Initialization%20in%20Graph%0A%20%20Neural%20Networks%0AAuthor%3A%20Dimitrios%20Kelesis%20and%20Dimitris%20Fotakis%20and%20Georgios%20Paliouras%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20generalize%20the%20ideas%20of%20Kaiming%20initialization%20to%20Graph%0ANeural%20Networks%20%28GNNs%29%20and%20propose%20a%20new%20scheme%20%28G-Init%29%20that%20reduces%0Aoversmoothing%2C%20leading%20to%20very%20good%20results%20in%20node%20and%20graph%20classification%0Atasks.%20GNNs%20are%20commonly%20initialized%20using%20methods%20designed%20for%20other%20types%20of%0ANeural%20Networks%2C%20overlooking%20the%20underlying%20graph%20topology.%20We%20analyze%0Atheoretically%20the%20variance%20of%20signals%20flowing%20forward%20and%20gradients%20flowing%0Abackward%20in%20the%20class%20of%20convolutional%20GNNs.%20We%20then%20simplify%20our%20analysis%20to%0Athe%20case%20of%20the%20GCN%20and%20propose%20a%20new%20initialization%20method.%20Our%20results%0Aindicate%20that%20the%20new%20method%20%28G-Init%29%20reduces%20oversmoothing%20in%20deep%20GNNs%2C%0Afacilitating%20their%20effective%20use.%20Experimental%20validation%20supports%20our%0Atheoretical%20findings%2C%20demonstrating%20the%20advantages%20of%20deep%20networks%20in%0Ascenarios%20with%20no%20feature%20information%20for%20unlabeled%20nodes%20%28i.e.%2C%20%60%60cold%20start%27%27%0Ascenario%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReducing%2520Oversmoothing%2520through%2520Informed%2520Weight%2520Initialization%2520in%2520Graph%250A%2520%2520Neural%2520Networks%26entry.906535625%3DDimitrios%2520Kelesis%2520and%2520Dimitris%2520Fotakis%2520and%2520Georgios%2520Paliouras%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520generalize%2520the%2520ideas%2520of%2520Kaiming%2520initialization%2520to%2520Graph%250ANeural%2520Networks%2520%2528GNNs%2529%2520and%2520propose%2520a%2520new%2520scheme%2520%2528G-Init%2529%2520that%2520reduces%250Aoversmoothing%252C%2520leading%2520to%2520very%2520good%2520results%2520in%2520node%2520and%2520graph%2520classification%250Atasks.%2520GNNs%2520are%2520commonly%2520initialized%2520using%2520methods%2520designed%2520for%2520other%2520types%2520of%250ANeural%2520Networks%252C%2520overlooking%2520the%2520underlying%2520graph%2520topology.%2520We%2520analyze%250Atheoretically%2520the%2520variance%2520of%2520signals%2520flowing%2520forward%2520and%2520gradients%2520flowing%250Abackward%2520in%2520the%2520class%2520of%2520convolutional%2520GNNs.%2520We%2520then%2520simplify%2520our%2520analysis%2520to%250Athe%2520case%2520of%2520the%2520GCN%2520and%2520propose%2520a%2520new%2520initialization%2520method.%2520Our%2520results%250Aindicate%2520that%2520the%2520new%2520method%2520%2528G-Init%2529%2520reduces%2520oversmoothing%2520in%2520deep%2520GNNs%252C%250Afacilitating%2520their%2520effective%2520use.%2520Experimental%2520validation%2520supports%2520our%250Atheoretical%2520findings%252C%2520demonstrating%2520the%2520advantages%2520of%2520deep%2520networks%2520in%250Ascenarios%2520with%2520no%2520feature%2520information%2520for%2520unlabeled%2520nodes%2520%2528i.e.%252C%2520%2560%2560cold%2520start%2527%2527%250Ascenario%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reducing%20Oversmoothing%20through%20Informed%20Weight%20Initialization%20in%20Graph%0A%20%20Neural%20Networks&entry.906535625=Dimitrios%20Kelesis%20and%20Dimitris%20Fotakis%20and%20Georgios%20Paliouras&entry.1292438233=%20%20In%20this%20work%2C%20we%20generalize%20the%20ideas%20of%20Kaiming%20initialization%20to%20Graph%0ANeural%20Networks%20%28GNNs%29%20and%20propose%20a%20new%20scheme%20%28G-Init%29%20that%20reduces%0Aoversmoothing%2C%20leading%20to%20very%20good%20results%20in%20node%20and%20graph%20classification%0Atasks.%20GNNs%20are%20commonly%20initialized%20using%20methods%20designed%20for%20other%20types%20of%0ANeural%20Networks%2C%20overlooking%20the%20underlying%20graph%20topology.%20We%20analyze%0Atheoretically%20the%20variance%20of%20signals%20flowing%20forward%20and%20gradients%20flowing%0Abackward%20in%20the%20class%20of%20convolutional%20GNNs.%20We%20then%20simplify%20our%20analysis%20to%0Athe%20case%20of%20the%20GCN%20and%20propose%20a%20new%20initialization%20method.%20Our%20results%0Aindicate%20that%20the%20new%20method%20%28G-Init%29%20reduces%20oversmoothing%20in%20deep%20GNNs%2C%0Afacilitating%20their%20effective%20use.%20Experimental%20validation%20supports%20our%0Atheoretical%20findings%2C%20demonstrating%20the%20advantages%20of%20deep%20networks%20in%0Ascenarios%20with%20no%20feature%20information%20for%20unlabeled%20nodes%20%28i.e.%2C%20%60%60cold%20start%27%27%0Ascenario%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23830v1&entry.124074799=Read"},
{"title": "Learning Video Representations without Natural Videos", "author": "Xueyang Yu and Xinlei Chen and Yossi Gandelsman", "abstract": "  In this paper, we show that useful video representations can be learned from\nsynthetic videos and natural images, without incorporating natural videos in\nthe training. We propose a progression of video datasets synthesized by simple\ngenerative processes, that model a growing set of natural video properties\n(e.g. motion, acceleration, and shape transformations). The downstream\nperformance of video models pre-trained on these generated datasets gradually\nincreases with the dataset progression. A VideoMAE model pre-trained on our\nsynthetic videos closes 97.2% of the performance gap on UCF101 action\nclassification between training from scratch and self-supervised pre-training\nfrom natural videos, and outperforms the pre-trained model on HMDB51.\nIntroducing crops of static images to the pre-training stage results in similar\nperformance to UCF101 pre-training and outperforms the UCF101 pre-trained model\non 11 out of 14 out-of-distribution datasets of UCF101-P. Analyzing the\nlow-level properties of the datasets, we identify correlations between frame\ndiversity, frame similarity to natural data, and downstream performance. Our\napproach provides a more controllable and transparent alternative to video data\ncuration processes for pre-training.\n", "link": "http://arxiv.org/abs/2410.24213v1", "date": "2024-10-31", "relevancy": 2.3127, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5885}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5712}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5698}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Video%20Representations%20without%20Natural%20Videos&body=Title%3A%20Learning%20Video%20Representations%20without%20Natural%20Videos%0AAuthor%3A%20Xueyang%20Yu%20and%20Xinlei%20Chen%20and%20Yossi%20Gandelsman%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20show%20that%20useful%20video%20representations%20can%20be%20learned%20from%0Asynthetic%20videos%20and%20natural%20images%2C%20without%20incorporating%20natural%20videos%20in%0Athe%20training.%20We%20propose%20a%20progression%20of%20video%20datasets%20synthesized%20by%20simple%0Agenerative%20processes%2C%20that%20model%20a%20growing%20set%20of%20natural%20video%20properties%0A%28e.g.%20motion%2C%20acceleration%2C%20and%20shape%20transformations%29.%20The%20downstream%0Aperformance%20of%20video%20models%20pre-trained%20on%20these%20generated%20datasets%20gradually%0Aincreases%20with%20the%20dataset%20progression.%20A%20VideoMAE%20model%20pre-trained%20on%20our%0Asynthetic%20videos%20closes%2097.2%25%20of%20the%20performance%20gap%20on%20UCF101%20action%0Aclassification%20between%20training%20from%20scratch%20and%20self-supervised%20pre-training%0Afrom%20natural%20videos%2C%20and%20outperforms%20the%20pre-trained%20model%20on%20HMDB51.%0AIntroducing%20crops%20of%20static%20images%20to%20the%20pre-training%20stage%20results%20in%20similar%0Aperformance%20to%20UCF101%20pre-training%20and%20outperforms%20the%20UCF101%20pre-trained%20model%0Aon%2011%20out%20of%2014%20out-of-distribution%20datasets%20of%20UCF101-P.%20Analyzing%20the%0Alow-level%20properties%20of%20the%20datasets%2C%20we%20identify%20correlations%20between%20frame%0Adiversity%2C%20frame%20similarity%20to%20natural%20data%2C%20and%20downstream%20performance.%20Our%0Aapproach%20provides%20a%20more%20controllable%20and%20transparent%20alternative%20to%20video%20data%0Acuration%20processes%20for%20pre-training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24213v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Video%2520Representations%2520without%2520Natural%2520Videos%26entry.906535625%3DXueyang%2520Yu%2520and%2520Xinlei%2520Chen%2520and%2520Yossi%2520Gandelsman%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520show%2520that%2520useful%2520video%2520representations%2520can%2520be%2520learned%2520from%250Asynthetic%2520videos%2520and%2520natural%2520images%252C%2520without%2520incorporating%2520natural%2520videos%2520in%250Athe%2520training.%2520We%2520propose%2520a%2520progression%2520of%2520video%2520datasets%2520synthesized%2520by%2520simple%250Agenerative%2520processes%252C%2520that%2520model%2520a%2520growing%2520set%2520of%2520natural%2520video%2520properties%250A%2528e.g.%2520motion%252C%2520acceleration%252C%2520and%2520shape%2520transformations%2529.%2520The%2520downstream%250Aperformance%2520of%2520video%2520models%2520pre-trained%2520on%2520these%2520generated%2520datasets%2520gradually%250Aincreases%2520with%2520the%2520dataset%2520progression.%2520A%2520VideoMAE%2520model%2520pre-trained%2520on%2520our%250Asynthetic%2520videos%2520closes%252097.2%2525%2520of%2520the%2520performance%2520gap%2520on%2520UCF101%2520action%250Aclassification%2520between%2520training%2520from%2520scratch%2520and%2520self-supervised%2520pre-training%250Afrom%2520natural%2520videos%252C%2520and%2520outperforms%2520the%2520pre-trained%2520model%2520on%2520HMDB51.%250AIntroducing%2520crops%2520of%2520static%2520images%2520to%2520the%2520pre-training%2520stage%2520results%2520in%2520similar%250Aperformance%2520to%2520UCF101%2520pre-training%2520and%2520outperforms%2520the%2520UCF101%2520pre-trained%2520model%250Aon%252011%2520out%2520of%252014%2520out-of-distribution%2520datasets%2520of%2520UCF101-P.%2520Analyzing%2520the%250Alow-level%2520properties%2520of%2520the%2520datasets%252C%2520we%2520identify%2520correlations%2520between%2520frame%250Adiversity%252C%2520frame%2520similarity%2520to%2520natural%2520data%252C%2520and%2520downstream%2520performance.%2520Our%250Aapproach%2520provides%2520a%2520more%2520controllable%2520and%2520transparent%2520alternative%2520to%2520video%2520data%250Acuration%2520processes%2520for%2520pre-training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24213v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Video%20Representations%20without%20Natural%20Videos&entry.906535625=Xueyang%20Yu%20and%20Xinlei%20Chen%20and%20Yossi%20Gandelsman&entry.1292438233=%20%20In%20this%20paper%2C%20we%20show%20that%20useful%20video%20representations%20can%20be%20learned%20from%0Asynthetic%20videos%20and%20natural%20images%2C%20without%20incorporating%20natural%20videos%20in%0Athe%20training.%20We%20propose%20a%20progression%20of%20video%20datasets%20synthesized%20by%20simple%0Agenerative%20processes%2C%20that%20model%20a%20growing%20set%20of%20natural%20video%20properties%0A%28e.g.%20motion%2C%20acceleration%2C%20and%20shape%20transformations%29.%20The%20downstream%0Aperformance%20of%20video%20models%20pre-trained%20on%20these%20generated%20datasets%20gradually%0Aincreases%20with%20the%20dataset%20progression.%20A%20VideoMAE%20model%20pre-trained%20on%20our%0Asynthetic%20videos%20closes%2097.2%25%20of%20the%20performance%20gap%20on%20UCF101%20action%0Aclassification%20between%20training%20from%20scratch%20and%20self-supervised%20pre-training%0Afrom%20natural%20videos%2C%20and%20outperforms%20the%20pre-trained%20model%20on%20HMDB51.%0AIntroducing%20crops%20of%20static%20images%20to%20the%20pre-training%20stage%20results%20in%20similar%0Aperformance%20to%20UCF101%20pre-training%20and%20outperforms%20the%20UCF101%20pre-trained%20model%0Aon%2011%20out%20of%2014%20out-of-distribution%20datasets%20of%20UCF101-P.%20Analyzing%20the%0Alow-level%20properties%20of%20the%20datasets%2C%20we%20identify%20correlations%20between%20frame%0Adiversity%2C%20frame%20similarity%20to%20natural%20data%2C%20and%20downstream%20performance.%20Our%0Aapproach%20provides%20a%20more%20controllable%20and%20transparent%20alternative%20to%20video%20data%0Acuration%20processes%20for%20pre-training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24213v1&entry.124074799=Read"},
{"title": "Matchmaker: Self-Improving Large Language Model Programs for Schema\n  Matching", "author": "Nabeel Seedat and Mihaela van der Schaar", "abstract": "  Schema matching -- the task of finding matches between attributes across\ndisparate data sources with different tables and hierarchies -- is critical for\ncreating interoperable machine learning (ML)-ready data. Addressing this\nfundamental data-centric problem has wide implications, especially in domains\nlike healthcare, finance and e-commerce -- but also has the potential to\nbenefit ML models more generally, by increasing the data available for ML model\ntraining. However, schema matching is a challenging ML task due to\nstructural/hierarchical and semantic heterogeneity between different schemas.\nPrevious ML approaches to automate schema matching have either required\nsignificant labeled data for model training, which is often unrealistic or\nsuffer from poor zero-shot performance. To this end, we propose Matchmaker - a\ncompositional language model program for schema matching, comprised of\ncandidate generation, refinement and confidence scoring. Matchmaker also\nself-improves in a zero-shot manner without the need for labeled demonstrations\nvia a novel optimization approach, which constructs synthetic in-context\ndemonstrations to guide the language model's reasoning process. Empirically, we\ndemonstrate on real-world medical schema matching benchmarks that Matchmaker\noutperforms previous ML-based approaches, highlighting its potential to\naccelerate data integration and interoperability of ML-ready data.\n", "link": "http://arxiv.org/abs/2410.24105v1", "date": "2024-10-31", "relevancy": 2.3116, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4673}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4656}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matchmaker%3A%20Self-Improving%20Large%20Language%20Model%20Programs%20for%20Schema%0A%20%20Matching&body=Title%3A%20Matchmaker%3A%20Self-Improving%20Large%20Language%20Model%20Programs%20for%20Schema%0A%20%20Matching%0AAuthor%3A%20Nabeel%20Seedat%20and%20Mihaela%20van%20der%20Schaar%0AAbstract%3A%20%20%20Schema%20matching%20--%20the%20task%20of%20finding%20matches%20between%20attributes%20across%0Adisparate%20data%20sources%20with%20different%20tables%20and%20hierarchies%20--%20is%20critical%20for%0Acreating%20interoperable%20machine%20learning%20%28ML%29-ready%20data.%20Addressing%20this%0Afundamental%20data-centric%20problem%20has%20wide%20implications%2C%20especially%20in%20domains%0Alike%20healthcare%2C%20finance%20and%20e-commerce%20--%20but%20also%20has%20the%20potential%20to%0Abenefit%20ML%20models%20more%20generally%2C%20by%20increasing%20the%20data%20available%20for%20ML%20model%0Atraining.%20However%2C%20schema%20matching%20is%20a%20challenging%20ML%20task%20due%20to%0Astructural/hierarchical%20and%20semantic%20heterogeneity%20between%20different%20schemas.%0APrevious%20ML%20approaches%20to%20automate%20schema%20matching%20have%20either%20required%0Asignificant%20labeled%20data%20for%20model%20training%2C%20which%20is%20often%20unrealistic%20or%0Asuffer%20from%20poor%20zero-shot%20performance.%20To%20this%20end%2C%20we%20propose%20Matchmaker%20-%20a%0Acompositional%20language%20model%20program%20for%20schema%20matching%2C%20comprised%20of%0Acandidate%20generation%2C%20refinement%20and%20confidence%20scoring.%20Matchmaker%20also%0Aself-improves%20in%20a%20zero-shot%20manner%20without%20the%20need%20for%20labeled%20demonstrations%0Avia%20a%20novel%20optimization%20approach%2C%20which%20constructs%20synthetic%20in-context%0Ademonstrations%20to%20guide%20the%20language%20model%27s%20reasoning%20process.%20Empirically%2C%20we%0Ademonstrate%20on%20real-world%20medical%20schema%20matching%20benchmarks%20that%20Matchmaker%0Aoutperforms%20previous%20ML-based%20approaches%2C%20highlighting%20its%20potential%20to%0Aaccelerate%20data%20integration%20and%20interoperability%20of%20ML-ready%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatchmaker%253A%2520Self-Improving%2520Large%2520Language%2520Model%2520Programs%2520for%2520Schema%250A%2520%2520Matching%26entry.906535625%3DNabeel%2520Seedat%2520and%2520Mihaela%2520van%2520der%2520Schaar%26entry.1292438233%3D%2520%2520Schema%2520matching%2520--%2520the%2520task%2520of%2520finding%2520matches%2520between%2520attributes%2520across%250Adisparate%2520data%2520sources%2520with%2520different%2520tables%2520and%2520hierarchies%2520--%2520is%2520critical%2520for%250Acreating%2520interoperable%2520machine%2520learning%2520%2528ML%2529-ready%2520data.%2520Addressing%2520this%250Afundamental%2520data-centric%2520problem%2520has%2520wide%2520implications%252C%2520especially%2520in%2520domains%250Alike%2520healthcare%252C%2520finance%2520and%2520e-commerce%2520--%2520but%2520also%2520has%2520the%2520potential%2520to%250Abenefit%2520ML%2520models%2520more%2520generally%252C%2520by%2520increasing%2520the%2520data%2520available%2520for%2520ML%2520model%250Atraining.%2520However%252C%2520schema%2520matching%2520is%2520a%2520challenging%2520ML%2520task%2520due%2520to%250Astructural/hierarchical%2520and%2520semantic%2520heterogeneity%2520between%2520different%2520schemas.%250APrevious%2520ML%2520approaches%2520to%2520automate%2520schema%2520matching%2520have%2520either%2520required%250Asignificant%2520labeled%2520data%2520for%2520model%2520training%252C%2520which%2520is%2520often%2520unrealistic%2520or%250Asuffer%2520from%2520poor%2520zero-shot%2520performance.%2520To%2520this%2520end%252C%2520we%2520propose%2520Matchmaker%2520-%2520a%250Acompositional%2520language%2520model%2520program%2520for%2520schema%2520matching%252C%2520comprised%2520of%250Acandidate%2520generation%252C%2520refinement%2520and%2520confidence%2520scoring.%2520Matchmaker%2520also%250Aself-improves%2520in%2520a%2520zero-shot%2520manner%2520without%2520the%2520need%2520for%2520labeled%2520demonstrations%250Avia%2520a%2520novel%2520optimization%2520approach%252C%2520which%2520constructs%2520synthetic%2520in-context%250Ademonstrations%2520to%2520guide%2520the%2520language%2520model%2527s%2520reasoning%2520process.%2520Empirically%252C%2520we%250Ademonstrate%2520on%2520real-world%2520medical%2520schema%2520matching%2520benchmarks%2520that%2520Matchmaker%250Aoutperforms%2520previous%2520ML-based%2520approaches%252C%2520highlighting%2520its%2520potential%2520to%250Aaccelerate%2520data%2520integration%2520and%2520interoperability%2520of%2520ML-ready%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matchmaker%3A%20Self-Improving%20Large%20Language%20Model%20Programs%20for%20Schema%0A%20%20Matching&entry.906535625=Nabeel%20Seedat%20and%20Mihaela%20van%20der%20Schaar&entry.1292438233=%20%20Schema%20matching%20--%20the%20task%20of%20finding%20matches%20between%20attributes%20across%0Adisparate%20data%20sources%20with%20different%20tables%20and%20hierarchies%20--%20is%20critical%20for%0Acreating%20interoperable%20machine%20learning%20%28ML%29-ready%20data.%20Addressing%20this%0Afundamental%20data-centric%20problem%20has%20wide%20implications%2C%20especially%20in%20domains%0Alike%20healthcare%2C%20finance%20and%20e-commerce%20--%20but%20also%20has%20the%20potential%20to%0Abenefit%20ML%20models%20more%20generally%2C%20by%20increasing%20the%20data%20available%20for%20ML%20model%0Atraining.%20However%2C%20schema%20matching%20is%20a%20challenging%20ML%20task%20due%20to%0Astructural/hierarchical%20and%20semantic%20heterogeneity%20between%20different%20schemas.%0APrevious%20ML%20approaches%20to%20automate%20schema%20matching%20have%20either%20required%0Asignificant%20labeled%20data%20for%20model%20training%2C%20which%20is%20often%20unrealistic%20or%0Asuffer%20from%20poor%20zero-shot%20performance.%20To%20this%20end%2C%20we%20propose%20Matchmaker%20-%20a%0Acompositional%20language%20model%20program%20for%20schema%20matching%2C%20comprised%20of%0Acandidate%20generation%2C%20refinement%20and%20confidence%20scoring.%20Matchmaker%20also%0Aself-improves%20in%20a%20zero-shot%20manner%20without%20the%20need%20for%20labeled%20demonstrations%0Avia%20a%20novel%20optimization%20approach%2C%20which%20constructs%20synthetic%20in-context%0Ademonstrations%20to%20guide%20the%20language%20model%27s%20reasoning%20process.%20Empirically%2C%20we%0Ademonstrate%20on%20real-world%20medical%20schema%20matching%20benchmarks%20that%20Matchmaker%0Aoutperforms%20previous%20ML-based%20approaches%2C%20highlighting%20its%20potential%20to%0Aaccelerate%20data%20integration%20and%20interoperability%20of%20ML-ready%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24105v1&entry.124074799=Read"},
{"title": "Adversarial Score identity Distillation: Rapidly Surpassing the Teacher\n  in One Step", "author": "Mingyuan Zhou and Huangjie Zheng and Yi Gu and Zhendong Wang and Hai Huang", "abstract": "  Score identity Distillation (SiD) is a data-free method that has achieved\nstate-of-the-art performance in image generation by leveraging only a\npretrained diffusion model, without requiring any training data. However, the\nultimate performance of SiD is constrained by the accuracy with which the\npretrained model captures the true data scores at different stages of the\ndiffusion process. In this paper, we introduce SiDA (SiD with Adversarial\nLoss), which not only enhances generation quality but also improves\ndistillation efficiency by incorporating real images and adversarial loss. SiDA\nutilizes the encoder from the generator's score network as a discriminator,\nboosting its ability to distinguish between real images and those generated by\nSiD. The adversarial loss is batch-normalized within each GPU and then combined\nwith the original SiD loss. This integration effectively incorporates the\naverage \"fakeness\" per GPU batch into the pixel-based SiD loss, enabling SiDA\nto distill a single-step generator either from scratch or by fine-tuning an\nexisting one. SiDA converges significantly faster than its predecessor when\ntrained from scratch, and swiftly improves upon the original model's\nperformance after an initial warmup period during fine-tuning from a\npre-distilled SiD generator. This one-step adversarial distillation method\nestablishes new benchmarks in generation performance when distilling EDM\ndiffusion models pretrained on CIFAR-10 (32x32) and ImageNet (64x64), achieving\nFID score of 1.110 on ImageNet 64x64. It sets record-low FID scores when\ndistilling EDM2 models trained on ImageNet (512x512), surpassing even the\nlargest teacher model, EDM2-XXL. Our SiDA's results record FID scores of 2.156\nfor EDM2-XS, 1.669 for EDM2-S, 1.488 for EDM2-M, and 1.465 for EDM2-L,\ndemonstrating significant improvements across all model sizes. Our open-source\ncode will be integrated into the SiD codebase.\n", "link": "http://arxiv.org/abs/2410.14919v2", "date": "2024-10-31", "relevancy": 2.2959, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6001}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5849}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Score%20identity%20Distillation%3A%20Rapidly%20Surpassing%20the%20Teacher%0A%20%20in%20One%20Step&body=Title%3A%20Adversarial%20Score%20identity%20Distillation%3A%20Rapidly%20Surpassing%20the%20Teacher%0A%20%20in%20One%20Step%0AAuthor%3A%20Mingyuan%20Zhou%20and%20Huangjie%20Zheng%20and%20Yi%20Gu%20and%20Zhendong%20Wang%20and%20Hai%20Huang%0AAbstract%3A%20%20%20Score%20identity%20Distillation%20%28SiD%29%20is%20a%20data-free%20method%20that%20has%20achieved%0Astate-of-the-art%20performance%20in%20image%20generation%20by%20leveraging%20only%20a%0Apretrained%20diffusion%20model%2C%20without%20requiring%20any%20training%20data.%20However%2C%20the%0Aultimate%20performance%20of%20SiD%20is%20constrained%20by%20the%20accuracy%20with%20which%20the%0Apretrained%20model%20captures%20the%20true%20data%20scores%20at%20different%20stages%20of%20the%0Adiffusion%20process.%20In%20this%20paper%2C%20we%20introduce%20SiDA%20%28SiD%20with%20Adversarial%0ALoss%29%2C%20which%20not%20only%20enhances%20generation%20quality%20but%20also%20improves%0Adistillation%20efficiency%20by%20incorporating%20real%20images%20and%20adversarial%20loss.%20SiDA%0Autilizes%20the%20encoder%20from%20the%20generator%27s%20score%20network%20as%20a%20discriminator%2C%0Aboosting%20its%20ability%20to%20distinguish%20between%20real%20images%20and%20those%20generated%20by%0ASiD.%20The%20adversarial%20loss%20is%20batch-normalized%20within%20each%20GPU%20and%20then%20combined%0Awith%20the%20original%20SiD%20loss.%20This%20integration%20effectively%20incorporates%20the%0Aaverage%20%22fakeness%22%20per%20GPU%20batch%20into%20the%20pixel-based%20SiD%20loss%2C%20enabling%20SiDA%0Ato%20distill%20a%20single-step%20generator%20either%20from%20scratch%20or%20by%20fine-tuning%20an%0Aexisting%20one.%20SiDA%20converges%20significantly%20faster%20than%20its%20predecessor%20when%0Atrained%20from%20scratch%2C%20and%20swiftly%20improves%20upon%20the%20original%20model%27s%0Aperformance%20after%20an%20initial%20warmup%20period%20during%20fine-tuning%20from%20a%0Apre-distilled%20SiD%20generator.%20This%20one-step%20adversarial%20distillation%20method%0Aestablishes%20new%20benchmarks%20in%20generation%20performance%20when%20distilling%20EDM%0Adiffusion%20models%20pretrained%20on%20CIFAR-10%20%2832x32%29%20and%20ImageNet%20%2864x64%29%2C%20achieving%0AFID%20score%20of%201.110%20on%20ImageNet%2064x64.%20It%20sets%20record-low%20FID%20scores%20when%0Adistilling%20EDM2%20models%20trained%20on%20ImageNet%20%28512x512%29%2C%20surpassing%20even%20the%0Alargest%20teacher%20model%2C%20EDM2-XXL.%20Our%20SiDA%27s%20results%20record%20FID%20scores%20of%202.156%0Afor%20EDM2-XS%2C%201.669%20for%20EDM2-S%2C%201.488%20for%20EDM2-M%2C%20and%201.465%20for%20EDM2-L%2C%0Ademonstrating%20significant%20improvements%20across%20all%20model%20sizes.%20Our%20open-source%0Acode%20will%20be%20integrated%20into%20the%20SiD%20codebase.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14919v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Score%2520identity%2520Distillation%253A%2520Rapidly%2520Surpassing%2520the%2520Teacher%250A%2520%2520in%2520One%2520Step%26entry.906535625%3DMingyuan%2520Zhou%2520and%2520Huangjie%2520Zheng%2520and%2520Yi%2520Gu%2520and%2520Zhendong%2520Wang%2520and%2520Hai%2520Huang%26entry.1292438233%3D%2520%2520Score%2520identity%2520Distillation%2520%2528SiD%2529%2520is%2520a%2520data-free%2520method%2520that%2520has%2520achieved%250Astate-of-the-art%2520performance%2520in%2520image%2520generation%2520by%2520leveraging%2520only%2520a%250Apretrained%2520diffusion%2520model%252C%2520without%2520requiring%2520any%2520training%2520data.%2520However%252C%2520the%250Aultimate%2520performance%2520of%2520SiD%2520is%2520constrained%2520by%2520the%2520accuracy%2520with%2520which%2520the%250Apretrained%2520model%2520captures%2520the%2520true%2520data%2520scores%2520at%2520different%2520stages%2520of%2520the%250Adiffusion%2520process.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SiDA%2520%2528SiD%2520with%2520Adversarial%250ALoss%2529%252C%2520which%2520not%2520only%2520enhances%2520generation%2520quality%2520but%2520also%2520improves%250Adistillation%2520efficiency%2520by%2520incorporating%2520real%2520images%2520and%2520adversarial%2520loss.%2520SiDA%250Autilizes%2520the%2520encoder%2520from%2520the%2520generator%2527s%2520score%2520network%2520as%2520a%2520discriminator%252C%250Aboosting%2520its%2520ability%2520to%2520distinguish%2520between%2520real%2520images%2520and%2520those%2520generated%2520by%250ASiD.%2520The%2520adversarial%2520loss%2520is%2520batch-normalized%2520within%2520each%2520GPU%2520and%2520then%2520combined%250Awith%2520the%2520original%2520SiD%2520loss.%2520This%2520integration%2520effectively%2520incorporates%2520the%250Aaverage%2520%2522fakeness%2522%2520per%2520GPU%2520batch%2520into%2520the%2520pixel-based%2520SiD%2520loss%252C%2520enabling%2520SiDA%250Ato%2520distill%2520a%2520single-step%2520generator%2520either%2520from%2520scratch%2520or%2520by%2520fine-tuning%2520an%250Aexisting%2520one.%2520SiDA%2520converges%2520significantly%2520faster%2520than%2520its%2520predecessor%2520when%250Atrained%2520from%2520scratch%252C%2520and%2520swiftly%2520improves%2520upon%2520the%2520original%2520model%2527s%250Aperformance%2520after%2520an%2520initial%2520warmup%2520period%2520during%2520fine-tuning%2520from%2520a%250Apre-distilled%2520SiD%2520generator.%2520This%2520one-step%2520adversarial%2520distillation%2520method%250Aestablishes%2520new%2520benchmarks%2520in%2520generation%2520performance%2520when%2520distilling%2520EDM%250Adiffusion%2520models%2520pretrained%2520on%2520CIFAR-10%2520%252832x32%2529%2520and%2520ImageNet%2520%252864x64%2529%252C%2520achieving%250AFID%2520score%2520of%25201.110%2520on%2520ImageNet%252064x64.%2520It%2520sets%2520record-low%2520FID%2520scores%2520when%250Adistilling%2520EDM2%2520models%2520trained%2520on%2520ImageNet%2520%2528512x512%2529%252C%2520surpassing%2520even%2520the%250Alargest%2520teacher%2520model%252C%2520EDM2-XXL.%2520Our%2520SiDA%2527s%2520results%2520record%2520FID%2520scores%2520of%25202.156%250Afor%2520EDM2-XS%252C%25201.669%2520for%2520EDM2-S%252C%25201.488%2520for%2520EDM2-M%252C%2520and%25201.465%2520for%2520EDM2-L%252C%250Ademonstrating%2520significant%2520improvements%2520across%2520all%2520model%2520sizes.%2520Our%2520open-source%250Acode%2520will%2520be%2520integrated%2520into%2520the%2520SiD%2520codebase.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14919v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Score%20identity%20Distillation%3A%20Rapidly%20Surpassing%20the%20Teacher%0A%20%20in%20One%20Step&entry.906535625=Mingyuan%20Zhou%20and%20Huangjie%20Zheng%20and%20Yi%20Gu%20and%20Zhendong%20Wang%20and%20Hai%20Huang&entry.1292438233=%20%20Score%20identity%20Distillation%20%28SiD%29%20is%20a%20data-free%20method%20that%20has%20achieved%0Astate-of-the-art%20performance%20in%20image%20generation%20by%20leveraging%20only%20a%0Apretrained%20diffusion%20model%2C%20without%20requiring%20any%20training%20data.%20However%2C%20the%0Aultimate%20performance%20of%20SiD%20is%20constrained%20by%20the%20accuracy%20with%20which%20the%0Apretrained%20model%20captures%20the%20true%20data%20scores%20at%20different%20stages%20of%20the%0Adiffusion%20process.%20In%20this%20paper%2C%20we%20introduce%20SiDA%20%28SiD%20with%20Adversarial%0ALoss%29%2C%20which%20not%20only%20enhances%20generation%20quality%20but%20also%20improves%0Adistillation%20efficiency%20by%20incorporating%20real%20images%20and%20adversarial%20loss.%20SiDA%0Autilizes%20the%20encoder%20from%20the%20generator%27s%20score%20network%20as%20a%20discriminator%2C%0Aboosting%20its%20ability%20to%20distinguish%20between%20real%20images%20and%20those%20generated%20by%0ASiD.%20The%20adversarial%20loss%20is%20batch-normalized%20within%20each%20GPU%20and%20then%20combined%0Awith%20the%20original%20SiD%20loss.%20This%20integration%20effectively%20incorporates%20the%0Aaverage%20%22fakeness%22%20per%20GPU%20batch%20into%20the%20pixel-based%20SiD%20loss%2C%20enabling%20SiDA%0Ato%20distill%20a%20single-step%20generator%20either%20from%20scratch%20or%20by%20fine-tuning%20an%0Aexisting%20one.%20SiDA%20converges%20significantly%20faster%20than%20its%20predecessor%20when%0Atrained%20from%20scratch%2C%20and%20swiftly%20improves%20upon%20the%20original%20model%27s%0Aperformance%20after%20an%20initial%20warmup%20period%20during%20fine-tuning%20from%20a%0Apre-distilled%20SiD%20generator.%20This%20one-step%20adversarial%20distillation%20method%0Aestablishes%20new%20benchmarks%20in%20generation%20performance%20when%20distilling%20EDM%0Adiffusion%20models%20pretrained%20on%20CIFAR-10%20%2832x32%29%20and%20ImageNet%20%2864x64%29%2C%20achieving%0AFID%20score%20of%201.110%20on%20ImageNet%2064x64.%20It%20sets%20record-low%20FID%20scores%20when%0Adistilling%20EDM2%20models%20trained%20on%20ImageNet%20%28512x512%29%2C%20surpassing%20even%20the%0Alargest%20teacher%20model%2C%20EDM2-XXL.%20Our%20SiDA%27s%20results%20record%20FID%20scores%20of%202.156%0Afor%20EDM2-XS%2C%201.669%20for%20EDM2-S%2C%201.488%20for%20EDM2-M%2C%20and%201.465%20for%20EDM2-L%2C%0Ademonstrating%20significant%20improvements%20across%20all%20model%20sizes.%20Our%20open-source%0Acode%20will%20be%20integrated%20into%20the%20SiD%20codebase.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14919v2&entry.124074799=Read"},
{"title": "UNION: Unsupervised 3D Object Detection using Object Appearance-based\n  Pseudo-Classes", "author": "Ted Lentsch and Holger Caesar and Dariu M. Gavrila", "abstract": "  Unsupervised 3D object detection methods have emerged to leverage vast\namounts of data without requiring manual labels for training. Recent approaches\nrely on dynamic objects for learning to detect mobile objects but penalize the\ndetections of static instances during training. Multiple rounds of (self)\ntraining are used to add detected static instances to the set of training\ntargets; this procedure to improve performance is computationally expensive. To\naddress this, we propose the method UNION. We use spatial clustering and\nself-supervised scene flow to obtain a set of static and dynamic object\nproposals from LiDAR. Subsequently, object proposals' visual appearances are\nencoded to distinguish static objects in the foreground and background by\nselecting static instances that are visually similar to dynamic objects. As a\nresult, static and dynamic mobile objects are obtained together, and existing\ndetectors can be trained with a single training. In addition, we extend 3D\nobject discovery to detection by using object appearance-based cluster labels\nas pseudo-class labels for training object classification. We conduct extensive\nexperiments on the nuScenes dataset and increase the state-of-the-art\nperformance for unsupervised 3D object discovery, i.e. UNION more than doubles\nthe average precision to 38.4. The code is available at\ngithub.com/TedLentsch/UNION.\n", "link": "http://arxiv.org/abs/2405.15688v2", "date": "2024-10-31", "relevancy": 2.2925, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5886}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.57}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.57}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UNION%3A%20Unsupervised%203D%20Object%20Detection%20using%20Object%20Appearance-based%0A%20%20Pseudo-Classes&body=Title%3A%20UNION%3A%20Unsupervised%203D%20Object%20Detection%20using%20Object%20Appearance-based%0A%20%20Pseudo-Classes%0AAuthor%3A%20Ted%20Lentsch%20and%20Holger%20Caesar%20and%20Dariu%20M.%20Gavrila%0AAbstract%3A%20%20%20Unsupervised%203D%20object%20detection%20methods%20have%20emerged%20to%20leverage%20vast%0Aamounts%20of%20data%20without%20requiring%20manual%20labels%20for%20training.%20Recent%20approaches%0Arely%20on%20dynamic%20objects%20for%20learning%20to%20detect%20mobile%20objects%20but%20penalize%20the%0Adetections%20of%20static%20instances%20during%20training.%20Multiple%20rounds%20of%20%28self%29%0Atraining%20are%20used%20to%20add%20detected%20static%20instances%20to%20the%20set%20of%20training%0Atargets%3B%20this%20procedure%20to%20improve%20performance%20is%20computationally%20expensive.%20To%0Aaddress%20this%2C%20we%20propose%20the%20method%20UNION.%20We%20use%20spatial%20clustering%20and%0Aself-supervised%20scene%20flow%20to%20obtain%20a%20set%20of%20static%20and%20dynamic%20object%0Aproposals%20from%20LiDAR.%20Subsequently%2C%20object%20proposals%27%20visual%20appearances%20are%0Aencoded%20to%20distinguish%20static%20objects%20in%20the%20foreground%20and%20background%20by%0Aselecting%20static%20instances%20that%20are%20visually%20similar%20to%20dynamic%20objects.%20As%20a%0Aresult%2C%20static%20and%20dynamic%20mobile%20objects%20are%20obtained%20together%2C%20and%20existing%0Adetectors%20can%20be%20trained%20with%20a%20single%20training.%20In%20addition%2C%20we%20extend%203D%0Aobject%20discovery%20to%20detection%20by%20using%20object%20appearance-based%20cluster%20labels%0Aas%20pseudo-class%20labels%20for%20training%20object%20classification.%20We%20conduct%20extensive%0Aexperiments%20on%20the%20nuScenes%20dataset%20and%20increase%20the%20state-of-the-art%0Aperformance%20for%20unsupervised%203D%20object%20discovery%2C%20i.e.%20UNION%20more%20than%20doubles%0Athe%20average%20precision%20to%2038.4.%20The%20code%20is%20available%20at%0Agithub.com/TedLentsch/UNION.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15688v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUNION%253A%2520Unsupervised%25203D%2520Object%2520Detection%2520using%2520Object%2520Appearance-based%250A%2520%2520Pseudo-Classes%26entry.906535625%3DTed%2520Lentsch%2520and%2520Holger%2520Caesar%2520and%2520Dariu%2520M.%2520Gavrila%26entry.1292438233%3D%2520%2520Unsupervised%25203D%2520object%2520detection%2520methods%2520have%2520emerged%2520to%2520leverage%2520vast%250Aamounts%2520of%2520data%2520without%2520requiring%2520manual%2520labels%2520for%2520training.%2520Recent%2520approaches%250Arely%2520on%2520dynamic%2520objects%2520for%2520learning%2520to%2520detect%2520mobile%2520objects%2520but%2520penalize%2520the%250Adetections%2520of%2520static%2520instances%2520during%2520training.%2520Multiple%2520rounds%2520of%2520%2528self%2529%250Atraining%2520are%2520used%2520to%2520add%2520detected%2520static%2520instances%2520to%2520the%2520set%2520of%2520training%250Atargets%253B%2520this%2520procedure%2520to%2520improve%2520performance%2520is%2520computationally%2520expensive.%2520To%250Aaddress%2520this%252C%2520we%2520propose%2520the%2520method%2520UNION.%2520We%2520use%2520spatial%2520clustering%2520and%250Aself-supervised%2520scene%2520flow%2520to%2520obtain%2520a%2520set%2520of%2520static%2520and%2520dynamic%2520object%250Aproposals%2520from%2520LiDAR.%2520Subsequently%252C%2520object%2520proposals%2527%2520visual%2520appearances%2520are%250Aencoded%2520to%2520distinguish%2520static%2520objects%2520in%2520the%2520foreground%2520and%2520background%2520by%250Aselecting%2520static%2520instances%2520that%2520are%2520visually%2520similar%2520to%2520dynamic%2520objects.%2520As%2520a%250Aresult%252C%2520static%2520and%2520dynamic%2520mobile%2520objects%2520are%2520obtained%2520together%252C%2520and%2520existing%250Adetectors%2520can%2520be%2520trained%2520with%2520a%2520single%2520training.%2520In%2520addition%252C%2520we%2520extend%25203D%250Aobject%2520discovery%2520to%2520detection%2520by%2520using%2520object%2520appearance-based%2520cluster%2520labels%250Aas%2520pseudo-class%2520labels%2520for%2520training%2520object%2520classification.%2520We%2520conduct%2520extensive%250Aexperiments%2520on%2520the%2520nuScenes%2520dataset%2520and%2520increase%2520the%2520state-of-the-art%250Aperformance%2520for%2520unsupervised%25203D%2520object%2520discovery%252C%2520i.e.%2520UNION%2520more%2520than%2520doubles%250Athe%2520average%2520precision%2520to%252038.4.%2520The%2520code%2520is%2520available%2520at%250Agithub.com/TedLentsch/UNION.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15688v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UNION%3A%20Unsupervised%203D%20Object%20Detection%20using%20Object%20Appearance-based%0A%20%20Pseudo-Classes&entry.906535625=Ted%20Lentsch%20and%20Holger%20Caesar%20and%20Dariu%20M.%20Gavrila&entry.1292438233=%20%20Unsupervised%203D%20object%20detection%20methods%20have%20emerged%20to%20leverage%20vast%0Aamounts%20of%20data%20without%20requiring%20manual%20labels%20for%20training.%20Recent%20approaches%0Arely%20on%20dynamic%20objects%20for%20learning%20to%20detect%20mobile%20objects%20but%20penalize%20the%0Adetections%20of%20static%20instances%20during%20training.%20Multiple%20rounds%20of%20%28self%29%0Atraining%20are%20used%20to%20add%20detected%20static%20instances%20to%20the%20set%20of%20training%0Atargets%3B%20this%20procedure%20to%20improve%20performance%20is%20computationally%20expensive.%20To%0Aaddress%20this%2C%20we%20propose%20the%20method%20UNION.%20We%20use%20spatial%20clustering%20and%0Aself-supervised%20scene%20flow%20to%20obtain%20a%20set%20of%20static%20and%20dynamic%20object%0Aproposals%20from%20LiDAR.%20Subsequently%2C%20object%20proposals%27%20visual%20appearances%20are%0Aencoded%20to%20distinguish%20static%20objects%20in%20the%20foreground%20and%20background%20by%0Aselecting%20static%20instances%20that%20are%20visually%20similar%20to%20dynamic%20objects.%20As%20a%0Aresult%2C%20static%20and%20dynamic%20mobile%20objects%20are%20obtained%20together%2C%20and%20existing%0Adetectors%20can%20be%20trained%20with%20a%20single%20training.%20In%20addition%2C%20we%20extend%203D%0Aobject%20discovery%20to%20detection%20by%20using%20object%20appearance-based%20cluster%20labels%0Aas%20pseudo-class%20labels%20for%20training%20object%20classification.%20We%20conduct%20extensive%0Aexperiments%20on%20the%20nuScenes%20dataset%20and%20increase%20the%20state-of-the-art%0Aperformance%20for%20unsupervised%203D%20object%20discovery%2C%20i.e.%20UNION%20more%20than%20doubles%0Athe%20average%20precision%20to%2038.4.%20The%20code%20is%20available%20at%0Agithub.com/TedLentsch/UNION.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15688v2&entry.124074799=Read"},
{"title": "HoloChrome: Polychromatic Illumination for Speckle Reduction in\n  Holographic Near-Eye Displays", "author": "Florian Schiffers and Grace Kuo and Nathan Matsuda and Douglas Lanman and Oliver Cossairt", "abstract": "  Holographic displays hold the promise of providing authentic depth cues,\nresulting in enhanced immersive visual experiences for near-eye applications.\nHowever, current holographic displays are hindered by speckle noise, which\nlimits accurate reproduction of color and texture in displayed images. We\npresent HoloChrome, a polychromatic holographic display framework designed to\nmitigate these limitations. HoloChrome utilizes an ultrafast,\nwavelength-adjustable laser and a dual-Spatial Light Modulator (SLM)\narchitecture, enabling the multiplexing of a large set of discrete wavelengths\nacross the visible spectrum. By leveraging spatial separation in our dual-SLM\nsetup, we independently manipulate speckle patterns across multiple\nwavelengths. This novel approach effectively reduces speckle noise through\nincoherent averaging achieved by wavelength multiplexing. Our method is\ncomplementary to existing speckle reduction techniques, offering a new pathway\nto address this challenge. Furthermore, the use of polychromatic illumination\nbroadens the achievable color gamut compared to traditional three-color primary\nholographic displays.\n  Our simulations and tabletop experiments validate that HoloChrome\nsignificantly reduces speckle noise and expands the color gamut. These\nadvancements enhance the performance of holographic near-eye displays, moving\nus closer to practical, immersive next-generation visual experiences.\n", "link": "http://arxiv.org/abs/2410.24144v1", "date": "2024-10-31", "relevancy": 2.2835, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4669}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4516}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HoloChrome%3A%20Polychromatic%20Illumination%20for%20Speckle%20Reduction%20in%0A%20%20Holographic%20Near-Eye%20Displays&body=Title%3A%20HoloChrome%3A%20Polychromatic%20Illumination%20for%20Speckle%20Reduction%20in%0A%20%20Holographic%20Near-Eye%20Displays%0AAuthor%3A%20Florian%20Schiffers%20and%20Grace%20Kuo%20and%20Nathan%20Matsuda%20and%20Douglas%20Lanman%20and%20Oliver%20Cossairt%0AAbstract%3A%20%20%20Holographic%20displays%20hold%20the%20promise%20of%20providing%20authentic%20depth%20cues%2C%0Aresulting%20in%20enhanced%20immersive%20visual%20experiences%20for%20near-eye%20applications.%0AHowever%2C%20current%20holographic%20displays%20are%20hindered%20by%20speckle%20noise%2C%20which%0Alimits%20accurate%20reproduction%20of%20color%20and%20texture%20in%20displayed%20images.%20We%0Apresent%20HoloChrome%2C%20a%20polychromatic%20holographic%20display%20framework%20designed%20to%0Amitigate%20these%20limitations.%20HoloChrome%20utilizes%20an%20ultrafast%2C%0Awavelength-adjustable%20laser%20and%20a%20dual-Spatial%20Light%20Modulator%20%28SLM%29%0Aarchitecture%2C%20enabling%20the%20multiplexing%20of%20a%20large%20set%20of%20discrete%20wavelengths%0Aacross%20the%20visible%20spectrum.%20By%20leveraging%20spatial%20separation%20in%20our%20dual-SLM%0Asetup%2C%20we%20independently%20manipulate%20speckle%20patterns%20across%20multiple%0Awavelengths.%20This%20novel%20approach%20effectively%20reduces%20speckle%20noise%20through%0Aincoherent%20averaging%20achieved%20by%20wavelength%20multiplexing.%20Our%20method%20is%0Acomplementary%20to%20existing%20speckle%20reduction%20techniques%2C%20offering%20a%20new%20pathway%0Ato%20address%20this%20challenge.%20Furthermore%2C%20the%20use%20of%20polychromatic%20illumination%0Abroadens%20the%20achievable%20color%20gamut%20compared%20to%20traditional%20three-color%20primary%0Aholographic%20displays.%0A%20%20Our%20simulations%20and%20tabletop%20experiments%20validate%20that%20HoloChrome%0Asignificantly%20reduces%20speckle%20noise%20and%20expands%20the%20color%20gamut.%20These%0Aadvancements%20enhance%20the%20performance%20of%20holographic%20near-eye%20displays%2C%20moving%0Aus%20closer%20to%20practical%2C%20immersive%20next-generation%20visual%20experiences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24144v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoloChrome%253A%2520Polychromatic%2520Illumination%2520for%2520Speckle%2520Reduction%2520in%250A%2520%2520Holographic%2520Near-Eye%2520Displays%26entry.906535625%3DFlorian%2520Schiffers%2520and%2520Grace%2520Kuo%2520and%2520Nathan%2520Matsuda%2520and%2520Douglas%2520Lanman%2520and%2520Oliver%2520Cossairt%26entry.1292438233%3D%2520%2520Holographic%2520displays%2520hold%2520the%2520promise%2520of%2520providing%2520authentic%2520depth%2520cues%252C%250Aresulting%2520in%2520enhanced%2520immersive%2520visual%2520experiences%2520for%2520near-eye%2520applications.%250AHowever%252C%2520current%2520holographic%2520displays%2520are%2520hindered%2520by%2520speckle%2520noise%252C%2520which%250Alimits%2520accurate%2520reproduction%2520of%2520color%2520and%2520texture%2520in%2520displayed%2520images.%2520We%250Apresent%2520HoloChrome%252C%2520a%2520polychromatic%2520holographic%2520display%2520framework%2520designed%2520to%250Amitigate%2520these%2520limitations.%2520HoloChrome%2520utilizes%2520an%2520ultrafast%252C%250Awavelength-adjustable%2520laser%2520and%2520a%2520dual-Spatial%2520Light%2520Modulator%2520%2528SLM%2529%250Aarchitecture%252C%2520enabling%2520the%2520multiplexing%2520of%2520a%2520large%2520set%2520of%2520discrete%2520wavelengths%250Aacross%2520the%2520visible%2520spectrum.%2520By%2520leveraging%2520spatial%2520separation%2520in%2520our%2520dual-SLM%250Asetup%252C%2520we%2520independently%2520manipulate%2520speckle%2520patterns%2520across%2520multiple%250Awavelengths.%2520This%2520novel%2520approach%2520effectively%2520reduces%2520speckle%2520noise%2520through%250Aincoherent%2520averaging%2520achieved%2520by%2520wavelength%2520multiplexing.%2520Our%2520method%2520is%250Acomplementary%2520to%2520existing%2520speckle%2520reduction%2520techniques%252C%2520offering%2520a%2520new%2520pathway%250Ato%2520address%2520this%2520challenge.%2520Furthermore%252C%2520the%2520use%2520of%2520polychromatic%2520illumination%250Abroadens%2520the%2520achievable%2520color%2520gamut%2520compared%2520to%2520traditional%2520three-color%2520primary%250Aholographic%2520displays.%250A%2520%2520Our%2520simulations%2520and%2520tabletop%2520experiments%2520validate%2520that%2520HoloChrome%250Asignificantly%2520reduces%2520speckle%2520noise%2520and%2520expands%2520the%2520color%2520gamut.%2520These%250Aadvancements%2520enhance%2520the%2520performance%2520of%2520holographic%2520near-eye%2520displays%252C%2520moving%250Aus%2520closer%2520to%2520practical%252C%2520immersive%2520next-generation%2520visual%2520experiences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24144v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoloChrome%3A%20Polychromatic%20Illumination%20for%20Speckle%20Reduction%20in%0A%20%20Holographic%20Near-Eye%20Displays&entry.906535625=Florian%20Schiffers%20and%20Grace%20Kuo%20and%20Nathan%20Matsuda%20and%20Douglas%20Lanman%20and%20Oliver%20Cossairt&entry.1292438233=%20%20Holographic%20displays%20hold%20the%20promise%20of%20providing%20authentic%20depth%20cues%2C%0Aresulting%20in%20enhanced%20immersive%20visual%20experiences%20for%20near-eye%20applications.%0AHowever%2C%20current%20holographic%20displays%20are%20hindered%20by%20speckle%20noise%2C%20which%0Alimits%20accurate%20reproduction%20of%20color%20and%20texture%20in%20displayed%20images.%20We%0Apresent%20HoloChrome%2C%20a%20polychromatic%20holographic%20display%20framework%20designed%20to%0Amitigate%20these%20limitations.%20HoloChrome%20utilizes%20an%20ultrafast%2C%0Awavelength-adjustable%20laser%20and%20a%20dual-Spatial%20Light%20Modulator%20%28SLM%29%0Aarchitecture%2C%20enabling%20the%20multiplexing%20of%20a%20large%20set%20of%20discrete%20wavelengths%0Aacross%20the%20visible%20spectrum.%20By%20leveraging%20spatial%20separation%20in%20our%20dual-SLM%0Asetup%2C%20we%20independently%20manipulate%20speckle%20patterns%20across%20multiple%0Awavelengths.%20This%20novel%20approach%20effectively%20reduces%20speckle%20noise%20through%0Aincoherent%20averaging%20achieved%20by%20wavelength%20multiplexing.%20Our%20method%20is%0Acomplementary%20to%20existing%20speckle%20reduction%20techniques%2C%20offering%20a%20new%20pathway%0Ato%20address%20this%20challenge.%20Furthermore%2C%20the%20use%20of%20polychromatic%20illumination%0Abroadens%20the%20achievable%20color%20gamut%20compared%20to%20traditional%20three-color%20primary%0Aholographic%20displays.%0A%20%20Our%20simulations%20and%20tabletop%20experiments%20validate%20that%20HoloChrome%0Asignificantly%20reduces%20speckle%20noise%20and%20expands%20the%20color%20gamut.%20These%0Aadvancements%20enhance%20the%20performance%20of%20holographic%20near-eye%20displays%2C%20moving%0Aus%20closer%20to%20practical%2C%20immersive%20next-generation%20visual%20experiences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24144v1&entry.124074799=Read"},
{"title": "Persistent Homology for High-dimensional Data Based on Spectral Methods", "author": "Sebastian Damrich and Philipp Berens and Dmitry Kobak", "abstract": "  Persistent homology is a popular computational tool for analyzing the\ntopology of point clouds, such as the presence of loops or voids. However, many\nreal-world datasets with low intrinsic dimensionality reside in an ambient\nspace of much higher dimensionality. We show that in this case traditional\npersistent homology becomes very sensitive to noise and fails to detect the\ncorrect topology. The same holds true for existing refinements of persistent\nhomology. As a remedy, we find that spectral distances on the\nk-nearest-neighbor graph of the data, such as diffusion distance and effective\nresistance, allow to detect the correct topology even in the presence of\nhigh-dimensional noise. Moreover, we derive a novel closed-form formula for\neffective resistance, and describe its relation to diffusion distances.\nFinally, we apply these methods to high-dimensional single-cell RNA-sequencing\ndata and show that spectral distances allow robust detection of cell cycle\nloops.\n", "link": "http://arxiv.org/abs/2311.03087v3", "date": "2024-10-31", "relevancy": 2.283, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.465}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.4534}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Persistent%20Homology%20for%20High-dimensional%20Data%20Based%20on%20Spectral%20Methods&body=Title%3A%20Persistent%20Homology%20for%20High-dimensional%20Data%20Based%20on%20Spectral%20Methods%0AAuthor%3A%20Sebastian%20Damrich%20and%20Philipp%20Berens%20and%20Dmitry%20Kobak%0AAbstract%3A%20%20%20Persistent%20homology%20is%20a%20popular%20computational%20tool%20for%20analyzing%20the%0Atopology%20of%20point%20clouds%2C%20such%20as%20the%20presence%20of%20loops%20or%20voids.%20However%2C%20many%0Areal-world%20datasets%20with%20low%20intrinsic%20dimensionality%20reside%20in%20an%20ambient%0Aspace%20of%20much%20higher%20dimensionality.%20We%20show%20that%20in%20this%20case%20traditional%0Apersistent%20homology%20becomes%20very%20sensitive%20to%20noise%20and%20fails%20to%20detect%20the%0Acorrect%20topology.%20The%20same%20holds%20true%20for%20existing%20refinements%20of%20persistent%0Ahomology.%20As%20a%20remedy%2C%20we%20find%20that%20spectral%20distances%20on%20the%0Ak-nearest-neighbor%20graph%20of%20the%20data%2C%20such%20as%20diffusion%20distance%20and%20effective%0Aresistance%2C%20allow%20to%20detect%20the%20correct%20topology%20even%20in%20the%20presence%20of%0Ahigh-dimensional%20noise.%20Moreover%2C%20we%20derive%20a%20novel%20closed-form%20formula%20for%0Aeffective%20resistance%2C%20and%20describe%20its%20relation%20to%20diffusion%20distances.%0AFinally%2C%20we%20apply%20these%20methods%20to%20high-dimensional%20single-cell%20RNA-sequencing%0Adata%20and%20show%20that%20spectral%20distances%20allow%20robust%20detection%20of%20cell%20cycle%0Aloops.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.03087v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersistent%2520Homology%2520for%2520High-dimensional%2520Data%2520Based%2520on%2520Spectral%2520Methods%26entry.906535625%3DSebastian%2520Damrich%2520and%2520Philipp%2520Berens%2520and%2520Dmitry%2520Kobak%26entry.1292438233%3D%2520%2520Persistent%2520homology%2520is%2520a%2520popular%2520computational%2520tool%2520for%2520analyzing%2520the%250Atopology%2520of%2520point%2520clouds%252C%2520such%2520as%2520the%2520presence%2520of%2520loops%2520or%2520voids.%2520However%252C%2520many%250Areal-world%2520datasets%2520with%2520low%2520intrinsic%2520dimensionality%2520reside%2520in%2520an%2520ambient%250Aspace%2520of%2520much%2520higher%2520dimensionality.%2520We%2520show%2520that%2520in%2520this%2520case%2520traditional%250Apersistent%2520homology%2520becomes%2520very%2520sensitive%2520to%2520noise%2520and%2520fails%2520to%2520detect%2520the%250Acorrect%2520topology.%2520The%2520same%2520holds%2520true%2520for%2520existing%2520refinements%2520of%2520persistent%250Ahomology.%2520As%2520a%2520remedy%252C%2520we%2520find%2520that%2520spectral%2520distances%2520on%2520the%250Ak-nearest-neighbor%2520graph%2520of%2520the%2520data%252C%2520such%2520as%2520diffusion%2520distance%2520and%2520effective%250Aresistance%252C%2520allow%2520to%2520detect%2520the%2520correct%2520topology%2520even%2520in%2520the%2520presence%2520of%250Ahigh-dimensional%2520noise.%2520Moreover%252C%2520we%2520derive%2520a%2520novel%2520closed-form%2520formula%2520for%250Aeffective%2520resistance%252C%2520and%2520describe%2520its%2520relation%2520to%2520diffusion%2520distances.%250AFinally%252C%2520we%2520apply%2520these%2520methods%2520to%2520high-dimensional%2520single-cell%2520RNA-sequencing%250Adata%2520and%2520show%2520that%2520spectral%2520distances%2520allow%2520robust%2520detection%2520of%2520cell%2520cycle%250Aloops.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.03087v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Persistent%20Homology%20for%20High-dimensional%20Data%20Based%20on%20Spectral%20Methods&entry.906535625=Sebastian%20Damrich%20and%20Philipp%20Berens%20and%20Dmitry%20Kobak&entry.1292438233=%20%20Persistent%20homology%20is%20a%20popular%20computational%20tool%20for%20analyzing%20the%0Atopology%20of%20point%20clouds%2C%20such%20as%20the%20presence%20of%20loops%20or%20voids.%20However%2C%20many%0Areal-world%20datasets%20with%20low%20intrinsic%20dimensionality%20reside%20in%20an%20ambient%0Aspace%20of%20much%20higher%20dimensionality.%20We%20show%20that%20in%20this%20case%20traditional%0Apersistent%20homology%20becomes%20very%20sensitive%20to%20noise%20and%20fails%20to%20detect%20the%0Acorrect%20topology.%20The%20same%20holds%20true%20for%20existing%20refinements%20of%20persistent%0Ahomology.%20As%20a%20remedy%2C%20we%20find%20that%20spectral%20distances%20on%20the%0Ak-nearest-neighbor%20graph%20of%20the%20data%2C%20such%20as%20diffusion%20distance%20and%20effective%0Aresistance%2C%20allow%20to%20detect%20the%20correct%20topology%20even%20in%20the%20presence%20of%0Ahigh-dimensional%20noise.%20Moreover%2C%20we%20derive%20a%20novel%20closed-form%20formula%20for%0Aeffective%20resistance%2C%20and%20describe%20its%20relation%20to%20diffusion%20distances.%0AFinally%2C%20we%20apply%20these%20methods%20to%20high-dimensional%20single-cell%20RNA-sequencing%0Adata%20and%20show%20that%20spectral%20distances%20allow%20robust%20detection%20of%20cell%20cycle%0Aloops.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.03087v3&entry.124074799=Read"},
{"title": "Parameter-free Clipped Gradient Descent Meets Polyak", "author": "Yuki Takezawa and Han Bao and Ryoma Sato and Kenta Niwa and Makoto Yamada", "abstract": "  Gradient descent and its variants are de facto standard algorithms for\ntraining machine learning models. As gradient descent is sensitive to its\nhyperparameters, we need to tune the hyperparameters carefully using a grid\nsearch. However, the method is time-consuming, particularly when multiple\nhyperparameters exist. Therefore, recent studies have analyzed parameter-free\nmethods that adjust the hyperparameters on the fly. However, the existing work\nis limited to investigations of parameter-free methods for the stepsize, and\nparameter-free methods for other hyperparameters have not been explored. For\ninstance, although the gradient clipping threshold is a crucial hyperparameter\nin addition to the stepsize for preventing gradient explosion issues, none of\nthe existing studies have investigated parameter-free methods for clipped\ngradient descent. Therefore, in this study, we investigate the parameter-free\nmethods for clipped gradient descent. Specifically, we propose Inexact Polyak\nStepsize, which converges to the optimal solution without any hyperparameters\ntuning, and its convergence rate is asymptotically independent of $L$ under\n$L$-smooth and $(L_0, L_1)$-smooth assumptions of the loss function, similar to\nthat of clipped gradient descent with well-tuned hyperparameters. We\nnumerically validated our convergence results using a synthetic function and\ndemonstrated the effectiveness of our proposed methods using LSTM, Nano-GPT,\nand T5.\n", "link": "http://arxiv.org/abs/2405.15010v2", "date": "2024-10-31", "relevancy": 2.2824, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.463}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4532}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parameter-free%20Clipped%20Gradient%20Descent%20Meets%20Polyak&body=Title%3A%20Parameter-free%20Clipped%20Gradient%20Descent%20Meets%20Polyak%0AAuthor%3A%20Yuki%20Takezawa%20and%20Han%20Bao%20and%20Ryoma%20Sato%20and%20Kenta%20Niwa%20and%20Makoto%20Yamada%0AAbstract%3A%20%20%20Gradient%20descent%20and%20its%20variants%20are%20de%20facto%20standard%20algorithms%20for%0Atraining%20machine%20learning%20models.%20As%20gradient%20descent%20is%20sensitive%20to%20its%0Ahyperparameters%2C%20we%20need%20to%20tune%20the%20hyperparameters%20carefully%20using%20a%20grid%0Asearch.%20However%2C%20the%20method%20is%20time-consuming%2C%20particularly%20when%20multiple%0Ahyperparameters%20exist.%20Therefore%2C%20recent%20studies%20have%20analyzed%20parameter-free%0Amethods%20that%20adjust%20the%20hyperparameters%20on%20the%20fly.%20However%2C%20the%20existing%20work%0Ais%20limited%20to%20investigations%20of%20parameter-free%20methods%20for%20the%20stepsize%2C%20and%0Aparameter-free%20methods%20for%20other%20hyperparameters%20have%20not%20been%20explored.%20For%0Ainstance%2C%20although%20the%20gradient%20clipping%20threshold%20is%20a%20crucial%20hyperparameter%0Ain%20addition%20to%20the%20stepsize%20for%20preventing%20gradient%20explosion%20issues%2C%20none%20of%0Athe%20existing%20studies%20have%20investigated%20parameter-free%20methods%20for%20clipped%0Agradient%20descent.%20Therefore%2C%20in%20this%20study%2C%20we%20investigate%20the%20parameter-free%0Amethods%20for%20clipped%20gradient%20descent.%20Specifically%2C%20we%20propose%20Inexact%20Polyak%0AStepsize%2C%20which%20converges%20to%20the%20optimal%20solution%20without%20any%20hyperparameters%0Atuning%2C%20and%20its%20convergence%20rate%20is%20asymptotically%20independent%20of%20%24L%24%20under%0A%24L%24-smooth%20and%20%24%28L_0%2C%20L_1%29%24-smooth%20assumptions%20of%20the%20loss%20function%2C%20similar%20to%0Athat%20of%20clipped%20gradient%20descent%20with%20well-tuned%20hyperparameters.%20We%0Anumerically%20validated%20our%20convergence%20results%20using%20a%20synthetic%20function%20and%0Ademonstrated%20the%20effectiveness%20of%20our%20proposed%20methods%20using%20LSTM%2C%20Nano-GPT%2C%0Aand%20T5.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15010v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParameter-free%2520Clipped%2520Gradient%2520Descent%2520Meets%2520Polyak%26entry.906535625%3DYuki%2520Takezawa%2520and%2520Han%2520Bao%2520and%2520Ryoma%2520Sato%2520and%2520Kenta%2520Niwa%2520and%2520Makoto%2520Yamada%26entry.1292438233%3D%2520%2520Gradient%2520descent%2520and%2520its%2520variants%2520are%2520de%2520facto%2520standard%2520algorithms%2520for%250Atraining%2520machine%2520learning%2520models.%2520As%2520gradient%2520descent%2520is%2520sensitive%2520to%2520its%250Ahyperparameters%252C%2520we%2520need%2520to%2520tune%2520the%2520hyperparameters%2520carefully%2520using%2520a%2520grid%250Asearch.%2520However%252C%2520the%2520method%2520is%2520time-consuming%252C%2520particularly%2520when%2520multiple%250Ahyperparameters%2520exist.%2520Therefore%252C%2520recent%2520studies%2520have%2520analyzed%2520parameter-free%250Amethods%2520that%2520adjust%2520the%2520hyperparameters%2520on%2520the%2520fly.%2520However%252C%2520the%2520existing%2520work%250Ais%2520limited%2520to%2520investigations%2520of%2520parameter-free%2520methods%2520for%2520the%2520stepsize%252C%2520and%250Aparameter-free%2520methods%2520for%2520other%2520hyperparameters%2520have%2520not%2520been%2520explored.%2520For%250Ainstance%252C%2520although%2520the%2520gradient%2520clipping%2520threshold%2520is%2520a%2520crucial%2520hyperparameter%250Ain%2520addition%2520to%2520the%2520stepsize%2520for%2520preventing%2520gradient%2520explosion%2520issues%252C%2520none%2520of%250Athe%2520existing%2520studies%2520have%2520investigated%2520parameter-free%2520methods%2520for%2520clipped%250Agradient%2520descent.%2520Therefore%252C%2520in%2520this%2520study%252C%2520we%2520investigate%2520the%2520parameter-free%250Amethods%2520for%2520clipped%2520gradient%2520descent.%2520Specifically%252C%2520we%2520propose%2520Inexact%2520Polyak%250AStepsize%252C%2520which%2520converges%2520to%2520the%2520optimal%2520solution%2520without%2520any%2520hyperparameters%250Atuning%252C%2520and%2520its%2520convergence%2520rate%2520is%2520asymptotically%2520independent%2520of%2520%2524L%2524%2520under%250A%2524L%2524-smooth%2520and%2520%2524%2528L_0%252C%2520L_1%2529%2524-smooth%2520assumptions%2520of%2520the%2520loss%2520function%252C%2520similar%2520to%250Athat%2520of%2520clipped%2520gradient%2520descent%2520with%2520well-tuned%2520hyperparameters.%2520We%250Anumerically%2520validated%2520our%2520convergence%2520results%2520using%2520a%2520synthetic%2520function%2520and%250Ademonstrated%2520the%2520effectiveness%2520of%2520our%2520proposed%2520methods%2520using%2520LSTM%252C%2520Nano-GPT%252C%250Aand%2520T5.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15010v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parameter-free%20Clipped%20Gradient%20Descent%20Meets%20Polyak&entry.906535625=Yuki%20Takezawa%20and%20Han%20Bao%20and%20Ryoma%20Sato%20and%20Kenta%20Niwa%20and%20Makoto%20Yamada&entry.1292438233=%20%20Gradient%20descent%20and%20its%20variants%20are%20de%20facto%20standard%20algorithms%20for%0Atraining%20machine%20learning%20models.%20As%20gradient%20descent%20is%20sensitive%20to%20its%0Ahyperparameters%2C%20we%20need%20to%20tune%20the%20hyperparameters%20carefully%20using%20a%20grid%0Asearch.%20However%2C%20the%20method%20is%20time-consuming%2C%20particularly%20when%20multiple%0Ahyperparameters%20exist.%20Therefore%2C%20recent%20studies%20have%20analyzed%20parameter-free%0Amethods%20that%20adjust%20the%20hyperparameters%20on%20the%20fly.%20However%2C%20the%20existing%20work%0Ais%20limited%20to%20investigations%20of%20parameter-free%20methods%20for%20the%20stepsize%2C%20and%0Aparameter-free%20methods%20for%20other%20hyperparameters%20have%20not%20been%20explored.%20For%0Ainstance%2C%20although%20the%20gradient%20clipping%20threshold%20is%20a%20crucial%20hyperparameter%0Ain%20addition%20to%20the%20stepsize%20for%20preventing%20gradient%20explosion%20issues%2C%20none%20of%0Athe%20existing%20studies%20have%20investigated%20parameter-free%20methods%20for%20clipped%0Agradient%20descent.%20Therefore%2C%20in%20this%20study%2C%20we%20investigate%20the%20parameter-free%0Amethods%20for%20clipped%20gradient%20descent.%20Specifically%2C%20we%20propose%20Inexact%20Polyak%0AStepsize%2C%20which%20converges%20to%20the%20optimal%20solution%20without%20any%20hyperparameters%0Atuning%2C%20and%20its%20convergence%20rate%20is%20asymptotically%20independent%20of%20%24L%24%20under%0A%24L%24-smooth%20and%20%24%28L_0%2C%20L_1%29%24-smooth%20assumptions%20of%20the%20loss%20function%2C%20similar%20to%0Athat%20of%20clipped%20gradient%20descent%20with%20well-tuned%20hyperparameters.%20We%0Anumerically%20validated%20our%20convergence%20results%20using%20a%20synthetic%20function%20and%0Ademonstrated%20the%20effectiveness%20of%20our%20proposed%20methods%20using%20LSTM%2C%20Nano-GPT%2C%0Aand%20T5.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15010v2&entry.124074799=Read"},
{"title": "Bayesian-guided Label Mapping for Visual Reprogramming", "author": "Chengyi Cai and Zesheng Ye and Lei Feng and Jianzhong Qi and Feng Liu", "abstract": "  Visual reprogramming (VR) leverages the intrinsic capabilities of pretrained\nvision models by adapting their input or output interfaces to solve downstream\ntasks whose labels (i.e., downstream labels) might be totally different from\nthe labels associated with the pretrained models (i.e., pretrained labels).\nWhen adapting the output interface, label mapping methods transform the\npretrained labels to downstream labels by establishing a gradient-free\none-to-one correspondence between the two sets of labels. However, in this\npaper, we reveal that one-to-one mappings may overlook the complex relationship\nbetween pretrained and downstream labels. Motivated by this observation, we\npropose a Bayesian-guided Label Mapping (BLM) method. BLM constructs an\niteratively-updated probabilistic label mapping matrix, with each element\nquantifying a pairwise relationship between pretrained and downstream labels.\nThe assignment of values to the constructed matrix is guided by Bayesian\nconditional probability, considering the joint distribution of the downstream\nlabels and the labels predicted by the pretrained model on downstream samples.\nExperiments conducted on both pretrained vision models (e.g., ResNeXt) and\nvision-language models (e.g., CLIP) demonstrate the superior performance of BLM\nover existing label mapping methods. The success of BLM also offers a\nprobabilistic lens through which to understand and analyze the effectiveness of\nVR. Our code is available at https://github.com/tmlr-group/BayesianLM.\n", "link": "http://arxiv.org/abs/2410.24018v1", "date": "2024-10-31", "relevancy": 2.2667, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6137}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5707}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian-guided%20Label%20Mapping%20for%20Visual%20Reprogramming&body=Title%3A%20Bayesian-guided%20Label%20Mapping%20for%20Visual%20Reprogramming%0AAuthor%3A%20Chengyi%20Cai%20and%20Zesheng%20Ye%20and%20Lei%20Feng%20and%20Jianzhong%20Qi%20and%20Feng%20Liu%0AAbstract%3A%20%20%20Visual%20reprogramming%20%28VR%29%20leverages%20the%20intrinsic%20capabilities%20of%20pretrained%0Avision%20models%20by%20adapting%20their%20input%20or%20output%20interfaces%20to%20solve%20downstream%0Atasks%20whose%20labels%20%28i.e.%2C%20downstream%20labels%29%20might%20be%20totally%20different%20from%0Athe%20labels%20associated%20with%20the%20pretrained%20models%20%28i.e.%2C%20pretrained%20labels%29.%0AWhen%20adapting%20the%20output%20interface%2C%20label%20mapping%20methods%20transform%20the%0Apretrained%20labels%20to%20downstream%20labels%20by%20establishing%20a%20gradient-free%0Aone-to-one%20correspondence%20between%20the%20two%20sets%20of%20labels.%20However%2C%20in%20this%0Apaper%2C%20we%20reveal%20that%20one-to-one%20mappings%20may%20overlook%20the%20complex%20relationship%0Abetween%20pretrained%20and%20downstream%20labels.%20Motivated%20by%20this%20observation%2C%20we%0Apropose%20a%20Bayesian-guided%20Label%20Mapping%20%28BLM%29%20method.%20BLM%20constructs%20an%0Aiteratively-updated%20probabilistic%20label%20mapping%20matrix%2C%20with%20each%20element%0Aquantifying%20a%20pairwise%20relationship%20between%20pretrained%20and%20downstream%20labels.%0AThe%20assignment%20of%20values%20to%20the%20constructed%20matrix%20is%20guided%20by%20Bayesian%0Aconditional%20probability%2C%20considering%20the%20joint%20distribution%20of%20the%20downstream%0Alabels%20and%20the%20labels%20predicted%20by%20the%20pretrained%20model%20on%20downstream%20samples.%0AExperiments%20conducted%20on%20both%20pretrained%20vision%20models%20%28e.g.%2C%20ResNeXt%29%20and%0Avision-language%20models%20%28e.g.%2C%20CLIP%29%20demonstrate%20the%20superior%20performance%20of%20BLM%0Aover%20existing%20label%20mapping%20methods.%20The%20success%20of%20BLM%20also%20offers%20a%0Aprobabilistic%20lens%20through%20which%20to%20understand%20and%20analyze%20the%20effectiveness%20of%0AVR.%20Our%20code%20is%20available%20at%20https%3A//github.com/tmlr-group/BayesianLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24018v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian-guided%2520Label%2520Mapping%2520for%2520Visual%2520Reprogramming%26entry.906535625%3DChengyi%2520Cai%2520and%2520Zesheng%2520Ye%2520and%2520Lei%2520Feng%2520and%2520Jianzhong%2520Qi%2520and%2520Feng%2520Liu%26entry.1292438233%3D%2520%2520Visual%2520reprogramming%2520%2528VR%2529%2520leverages%2520the%2520intrinsic%2520capabilities%2520of%2520pretrained%250Avision%2520models%2520by%2520adapting%2520their%2520input%2520or%2520output%2520interfaces%2520to%2520solve%2520downstream%250Atasks%2520whose%2520labels%2520%2528i.e.%252C%2520downstream%2520labels%2529%2520might%2520be%2520totally%2520different%2520from%250Athe%2520labels%2520associated%2520with%2520the%2520pretrained%2520models%2520%2528i.e.%252C%2520pretrained%2520labels%2529.%250AWhen%2520adapting%2520the%2520output%2520interface%252C%2520label%2520mapping%2520methods%2520transform%2520the%250Apretrained%2520labels%2520to%2520downstream%2520labels%2520by%2520establishing%2520a%2520gradient-free%250Aone-to-one%2520correspondence%2520between%2520the%2520two%2520sets%2520of%2520labels.%2520However%252C%2520in%2520this%250Apaper%252C%2520we%2520reveal%2520that%2520one-to-one%2520mappings%2520may%2520overlook%2520the%2520complex%2520relationship%250Abetween%2520pretrained%2520and%2520downstream%2520labels.%2520Motivated%2520by%2520this%2520observation%252C%2520we%250Apropose%2520a%2520Bayesian-guided%2520Label%2520Mapping%2520%2528BLM%2529%2520method.%2520BLM%2520constructs%2520an%250Aiteratively-updated%2520probabilistic%2520label%2520mapping%2520matrix%252C%2520with%2520each%2520element%250Aquantifying%2520a%2520pairwise%2520relationship%2520between%2520pretrained%2520and%2520downstream%2520labels.%250AThe%2520assignment%2520of%2520values%2520to%2520the%2520constructed%2520matrix%2520is%2520guided%2520by%2520Bayesian%250Aconditional%2520probability%252C%2520considering%2520the%2520joint%2520distribution%2520of%2520the%2520downstream%250Alabels%2520and%2520the%2520labels%2520predicted%2520by%2520the%2520pretrained%2520model%2520on%2520downstream%2520samples.%250AExperiments%2520conducted%2520on%2520both%2520pretrained%2520vision%2520models%2520%2528e.g.%252C%2520ResNeXt%2529%2520and%250Avision-language%2520models%2520%2528e.g.%252C%2520CLIP%2529%2520demonstrate%2520the%2520superior%2520performance%2520of%2520BLM%250Aover%2520existing%2520label%2520mapping%2520methods.%2520The%2520success%2520of%2520BLM%2520also%2520offers%2520a%250Aprobabilistic%2520lens%2520through%2520which%2520to%2520understand%2520and%2520analyze%2520the%2520effectiveness%2520of%250AVR.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/tmlr-group/BayesianLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24018v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian-guided%20Label%20Mapping%20for%20Visual%20Reprogramming&entry.906535625=Chengyi%20Cai%20and%20Zesheng%20Ye%20and%20Lei%20Feng%20and%20Jianzhong%20Qi%20and%20Feng%20Liu&entry.1292438233=%20%20Visual%20reprogramming%20%28VR%29%20leverages%20the%20intrinsic%20capabilities%20of%20pretrained%0Avision%20models%20by%20adapting%20their%20input%20or%20output%20interfaces%20to%20solve%20downstream%0Atasks%20whose%20labels%20%28i.e.%2C%20downstream%20labels%29%20might%20be%20totally%20different%20from%0Athe%20labels%20associated%20with%20the%20pretrained%20models%20%28i.e.%2C%20pretrained%20labels%29.%0AWhen%20adapting%20the%20output%20interface%2C%20label%20mapping%20methods%20transform%20the%0Apretrained%20labels%20to%20downstream%20labels%20by%20establishing%20a%20gradient-free%0Aone-to-one%20correspondence%20between%20the%20two%20sets%20of%20labels.%20However%2C%20in%20this%0Apaper%2C%20we%20reveal%20that%20one-to-one%20mappings%20may%20overlook%20the%20complex%20relationship%0Abetween%20pretrained%20and%20downstream%20labels.%20Motivated%20by%20this%20observation%2C%20we%0Apropose%20a%20Bayesian-guided%20Label%20Mapping%20%28BLM%29%20method.%20BLM%20constructs%20an%0Aiteratively-updated%20probabilistic%20label%20mapping%20matrix%2C%20with%20each%20element%0Aquantifying%20a%20pairwise%20relationship%20between%20pretrained%20and%20downstream%20labels.%0AThe%20assignment%20of%20values%20to%20the%20constructed%20matrix%20is%20guided%20by%20Bayesian%0Aconditional%20probability%2C%20considering%20the%20joint%20distribution%20of%20the%20downstream%0Alabels%20and%20the%20labels%20predicted%20by%20the%20pretrained%20model%20on%20downstream%20samples.%0AExperiments%20conducted%20on%20both%20pretrained%20vision%20models%20%28e.g.%2C%20ResNeXt%29%20and%0Avision-language%20models%20%28e.g.%2C%20CLIP%29%20demonstrate%20the%20superior%20performance%20of%20BLM%0Aover%20existing%20label%20mapping%20methods.%20The%20success%20of%20BLM%20also%20offers%20a%0Aprobabilistic%20lens%20through%20which%20to%20understand%20and%20analyze%20the%20effectiveness%20of%0AVR.%20Our%20code%20is%20available%20at%20https%3A//github.com/tmlr-group/BayesianLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24018v1&entry.124074799=Read"},
{"title": "Generative AI for Accessible and Inclusive Extended Reality", "author": "Jens Grubert and Junlong Chen and Per Ola Kristensson", "abstract": "  Artificial Intelligence-Generated Content (AIGC) has the potential to\ntransform how people build and interact with virtual environments. Within this\npaper, we discuss potential benefits but also challenges that AIGC has for the\ncreation of inclusive and accessible virtual environments. Specifically, we\ntouch upon the decreased need for 3D modeling expertise, benefits of\nsymbolic-only as well as multimodal input, 3D content editing, and 3D model\naccessibility as well as foundation model-specific challenges.\n", "link": "http://arxiv.org/abs/2410.23803v1", "date": "2024-10-31", "relevancy": 2.2522, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6044}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5408}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20for%20Accessible%20and%20Inclusive%20Extended%20Reality&body=Title%3A%20Generative%20AI%20for%20Accessible%20and%20Inclusive%20Extended%20Reality%0AAuthor%3A%20Jens%20Grubert%20and%20Junlong%20Chen%20and%20Per%20Ola%20Kristensson%0AAbstract%3A%20%20%20Artificial%20Intelligence-Generated%20Content%20%28AIGC%29%20has%20the%20potential%20to%0Atransform%20how%20people%20build%20and%20interact%20with%20virtual%20environments.%20Within%20this%0Apaper%2C%20we%20discuss%20potential%20benefits%20but%20also%20challenges%20that%20AIGC%20has%20for%20the%0Acreation%20of%20inclusive%20and%20accessible%20virtual%20environments.%20Specifically%2C%20we%0Atouch%20upon%20the%20decreased%20need%20for%203D%20modeling%20expertise%2C%20benefits%20of%0Asymbolic-only%20as%20well%20as%20multimodal%20input%2C%203D%20content%20editing%2C%20and%203D%20model%0Aaccessibility%20as%20well%20as%20foundation%20model-specific%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520for%2520Accessible%2520and%2520Inclusive%2520Extended%2520Reality%26entry.906535625%3DJens%2520Grubert%2520and%2520Junlong%2520Chen%2520and%2520Per%2520Ola%2520Kristensson%26entry.1292438233%3D%2520%2520Artificial%2520Intelligence-Generated%2520Content%2520%2528AIGC%2529%2520has%2520the%2520potential%2520to%250Atransform%2520how%2520people%2520build%2520and%2520interact%2520with%2520virtual%2520environments.%2520Within%2520this%250Apaper%252C%2520we%2520discuss%2520potential%2520benefits%2520but%2520also%2520challenges%2520that%2520AIGC%2520has%2520for%2520the%250Acreation%2520of%2520inclusive%2520and%2520accessible%2520virtual%2520environments.%2520Specifically%252C%2520we%250Atouch%2520upon%2520the%2520decreased%2520need%2520for%25203D%2520modeling%2520expertise%252C%2520benefits%2520of%250Asymbolic-only%2520as%2520well%2520as%2520multimodal%2520input%252C%25203D%2520content%2520editing%252C%2520and%25203D%2520model%250Aaccessibility%2520as%2520well%2520as%2520foundation%2520model-specific%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20for%20Accessible%20and%20Inclusive%20Extended%20Reality&entry.906535625=Jens%20Grubert%20and%20Junlong%20Chen%20and%20Per%20Ola%20Kristensson&entry.1292438233=%20%20Artificial%20Intelligence-Generated%20Content%20%28AIGC%29%20has%20the%20potential%20to%0Atransform%20how%20people%20build%20and%20interact%20with%20virtual%20environments.%20Within%20this%0Apaper%2C%20we%20discuss%20potential%20benefits%20but%20also%20challenges%20that%20AIGC%20has%20for%20the%0Acreation%20of%20inclusive%20and%20accessible%20virtual%20environments.%20Specifically%2C%20we%0Atouch%20upon%20the%20decreased%20need%20for%203D%20modeling%20expertise%2C%20benefits%20of%0Asymbolic-only%20as%20well%20as%20multimodal%20input%2C%203D%20content%20editing%2C%20and%203D%20model%0Aaccessibility%20as%20well%20as%20foundation%20model-specific%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23803v1&entry.124074799=Read"},
{"title": "JEMA: A Joint Embedding Framework for Scalable Co-Learning with\n  Multimodal Alignment", "author": "Joao Sousa and Roya Darabi and Armando Sousa and Frank Brueckner and Lu\u00eds Paulo Reis and Ana Reis", "abstract": "  This work introduces JEMA (Joint Embedding with Multimodal Alignment), a\nnovel co-learning framework tailored for laser metal deposition (LMD), a\npivotal process in metal additive manufacturing. As Industry 5.0 gains traction\nin industrial applications, efficient process monitoring becomes increasingly\ncrucial. However, limited data and the opaque nature of AI present challenges\nfor its application in an industrial setting. JEMA addresses this challenges by\nleveraging multimodal data, including multi-view images and metadata such as\nprocess parameters, to learn transferable semantic representations. By applying\na supervised contrastive loss function, JEMA enables robust learning and\nsubsequent process monitoring using only the primary modality, simplifying\nhardware requirements and computational overhead. We investigate the\neffectiveness of JEMA in LMD process monitoring, focusing specifically on its\ngeneralization to downstream tasks such as melt pool geometry prediction,\nachieved without extensive fine-tuning. Our empirical evaluation demonstrates\nthe high scalability and performance of JEMA, particularly when combined with\nVision Transformer models. We report an 8% increase in performance in\nmultimodal settings and a 1% improvement in unimodal settings compared to\nsupervised contrastive learning. Additionally, the learned embedding\nrepresentation enables the prediction of metadata, enhancing interpretability\nand making possible the assessment of the added metadata's contributions. Our\nframework lays the foundation for integrating multisensor data with metadata,\nenabling diverse downstream tasks within the LMD domain and beyond.\n", "link": "http://arxiv.org/abs/2410.23988v1", "date": "2024-10-31", "relevancy": 2.2464, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5773}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5694}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JEMA%3A%20A%20Joint%20Embedding%20Framework%20for%20Scalable%20Co-Learning%20with%0A%20%20Multimodal%20Alignment&body=Title%3A%20JEMA%3A%20A%20Joint%20Embedding%20Framework%20for%20Scalable%20Co-Learning%20with%0A%20%20Multimodal%20Alignment%0AAuthor%3A%20Joao%20Sousa%20and%20Roya%20Darabi%20and%20Armando%20Sousa%20and%20Frank%20Brueckner%20and%20Lu%C3%ADs%20Paulo%20Reis%20and%20Ana%20Reis%0AAbstract%3A%20%20%20This%20work%20introduces%20JEMA%20%28Joint%20Embedding%20with%20Multimodal%20Alignment%29%2C%20a%0Anovel%20co-learning%20framework%20tailored%20for%20laser%20metal%20deposition%20%28LMD%29%2C%20a%0Apivotal%20process%20in%20metal%20additive%20manufacturing.%20As%20Industry%205.0%20gains%20traction%0Ain%20industrial%20applications%2C%20efficient%20process%20monitoring%20becomes%20increasingly%0Acrucial.%20However%2C%20limited%20data%20and%20the%20opaque%20nature%20of%20AI%20present%20challenges%0Afor%20its%20application%20in%20an%20industrial%20setting.%20JEMA%20addresses%20this%20challenges%20by%0Aleveraging%20multimodal%20data%2C%20including%20multi-view%20images%20and%20metadata%20such%20as%0Aprocess%20parameters%2C%20to%20learn%20transferable%20semantic%20representations.%20By%20applying%0Aa%20supervised%20contrastive%20loss%20function%2C%20JEMA%20enables%20robust%20learning%20and%0Asubsequent%20process%20monitoring%20using%20only%20the%20primary%20modality%2C%20simplifying%0Ahardware%20requirements%20and%20computational%20overhead.%20We%20investigate%20the%0Aeffectiveness%20of%20JEMA%20in%20LMD%20process%20monitoring%2C%20focusing%20specifically%20on%20its%0Ageneralization%20to%20downstream%20tasks%20such%20as%20melt%20pool%20geometry%20prediction%2C%0Aachieved%20without%20extensive%20fine-tuning.%20Our%20empirical%20evaluation%20demonstrates%0Athe%20high%20scalability%20and%20performance%20of%20JEMA%2C%20particularly%20when%20combined%20with%0AVision%20Transformer%20models.%20We%20report%20an%208%25%20increase%20in%20performance%20in%0Amultimodal%20settings%20and%20a%201%25%20improvement%20in%20unimodal%20settings%20compared%20to%0Asupervised%20contrastive%20learning.%20Additionally%2C%20the%20learned%20embedding%0Arepresentation%20enables%20the%20prediction%20of%20metadata%2C%20enhancing%20interpretability%0Aand%20making%20possible%20the%20assessment%20of%20the%20added%20metadata%27s%20contributions.%20Our%0Aframework%20lays%20the%20foundation%20for%20integrating%20multisensor%20data%20with%20metadata%2C%0Aenabling%20diverse%20downstream%20tasks%20within%20the%20LMD%20domain%20and%20beyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23988v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJEMA%253A%2520A%2520Joint%2520Embedding%2520Framework%2520for%2520Scalable%2520Co-Learning%2520with%250A%2520%2520Multimodal%2520Alignment%26entry.906535625%3DJoao%2520Sousa%2520and%2520Roya%2520Darabi%2520and%2520Armando%2520Sousa%2520and%2520Frank%2520Brueckner%2520and%2520Lu%25C3%25ADs%2520Paulo%2520Reis%2520and%2520Ana%2520Reis%26entry.1292438233%3D%2520%2520This%2520work%2520introduces%2520JEMA%2520%2528Joint%2520Embedding%2520with%2520Multimodal%2520Alignment%2529%252C%2520a%250Anovel%2520co-learning%2520framework%2520tailored%2520for%2520laser%2520metal%2520deposition%2520%2528LMD%2529%252C%2520a%250Apivotal%2520process%2520in%2520metal%2520additive%2520manufacturing.%2520As%2520Industry%25205.0%2520gains%2520traction%250Ain%2520industrial%2520applications%252C%2520efficient%2520process%2520monitoring%2520becomes%2520increasingly%250Acrucial.%2520However%252C%2520limited%2520data%2520and%2520the%2520opaque%2520nature%2520of%2520AI%2520present%2520challenges%250Afor%2520its%2520application%2520in%2520an%2520industrial%2520setting.%2520JEMA%2520addresses%2520this%2520challenges%2520by%250Aleveraging%2520multimodal%2520data%252C%2520including%2520multi-view%2520images%2520and%2520metadata%2520such%2520as%250Aprocess%2520parameters%252C%2520to%2520learn%2520transferable%2520semantic%2520representations.%2520By%2520applying%250Aa%2520supervised%2520contrastive%2520loss%2520function%252C%2520JEMA%2520enables%2520robust%2520learning%2520and%250Asubsequent%2520process%2520monitoring%2520using%2520only%2520the%2520primary%2520modality%252C%2520simplifying%250Ahardware%2520requirements%2520and%2520computational%2520overhead.%2520We%2520investigate%2520the%250Aeffectiveness%2520of%2520JEMA%2520in%2520LMD%2520process%2520monitoring%252C%2520focusing%2520specifically%2520on%2520its%250Ageneralization%2520to%2520downstream%2520tasks%2520such%2520as%2520melt%2520pool%2520geometry%2520prediction%252C%250Aachieved%2520without%2520extensive%2520fine-tuning.%2520Our%2520empirical%2520evaluation%2520demonstrates%250Athe%2520high%2520scalability%2520and%2520performance%2520of%2520JEMA%252C%2520particularly%2520when%2520combined%2520with%250AVision%2520Transformer%2520models.%2520We%2520report%2520an%25208%2525%2520increase%2520in%2520performance%2520in%250Amultimodal%2520settings%2520and%2520a%25201%2525%2520improvement%2520in%2520unimodal%2520settings%2520compared%2520to%250Asupervised%2520contrastive%2520learning.%2520Additionally%252C%2520the%2520learned%2520embedding%250Arepresentation%2520enables%2520the%2520prediction%2520of%2520metadata%252C%2520enhancing%2520interpretability%250Aand%2520making%2520possible%2520the%2520assessment%2520of%2520the%2520added%2520metadata%2527s%2520contributions.%2520Our%250Aframework%2520lays%2520the%2520foundation%2520for%2520integrating%2520multisensor%2520data%2520with%2520metadata%252C%250Aenabling%2520diverse%2520downstream%2520tasks%2520within%2520the%2520LMD%2520domain%2520and%2520beyond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23988v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JEMA%3A%20A%20Joint%20Embedding%20Framework%20for%20Scalable%20Co-Learning%20with%0A%20%20Multimodal%20Alignment&entry.906535625=Joao%20Sousa%20and%20Roya%20Darabi%20and%20Armando%20Sousa%20and%20Frank%20Brueckner%20and%20Lu%C3%ADs%20Paulo%20Reis%20and%20Ana%20Reis&entry.1292438233=%20%20This%20work%20introduces%20JEMA%20%28Joint%20Embedding%20with%20Multimodal%20Alignment%29%2C%20a%0Anovel%20co-learning%20framework%20tailored%20for%20laser%20metal%20deposition%20%28LMD%29%2C%20a%0Apivotal%20process%20in%20metal%20additive%20manufacturing.%20As%20Industry%205.0%20gains%20traction%0Ain%20industrial%20applications%2C%20efficient%20process%20monitoring%20becomes%20increasingly%0Acrucial.%20However%2C%20limited%20data%20and%20the%20opaque%20nature%20of%20AI%20present%20challenges%0Afor%20its%20application%20in%20an%20industrial%20setting.%20JEMA%20addresses%20this%20challenges%20by%0Aleveraging%20multimodal%20data%2C%20including%20multi-view%20images%20and%20metadata%20such%20as%0Aprocess%20parameters%2C%20to%20learn%20transferable%20semantic%20representations.%20By%20applying%0Aa%20supervised%20contrastive%20loss%20function%2C%20JEMA%20enables%20robust%20learning%20and%0Asubsequent%20process%20monitoring%20using%20only%20the%20primary%20modality%2C%20simplifying%0Ahardware%20requirements%20and%20computational%20overhead.%20We%20investigate%20the%0Aeffectiveness%20of%20JEMA%20in%20LMD%20process%20monitoring%2C%20focusing%20specifically%20on%20its%0Ageneralization%20to%20downstream%20tasks%20such%20as%20melt%20pool%20geometry%20prediction%2C%0Aachieved%20without%20extensive%20fine-tuning.%20Our%20empirical%20evaluation%20demonstrates%0Athe%20high%20scalability%20and%20performance%20of%20JEMA%2C%20particularly%20when%20combined%20with%0AVision%20Transformer%20models.%20We%20report%20an%208%25%20increase%20in%20performance%20in%0Amultimodal%20settings%20and%20a%201%25%20improvement%20in%20unimodal%20settings%20compared%20to%0Asupervised%20contrastive%20learning.%20Additionally%2C%20the%20learned%20embedding%0Arepresentation%20enables%20the%20prediction%20of%20metadata%2C%20enhancing%20interpretability%0Aand%20making%20possible%20the%20assessment%20of%20the%20added%20metadata%27s%20contributions.%20Our%0Aframework%20lays%20the%20foundation%20for%20integrating%20multisensor%20data%20with%20metadata%2C%0Aenabling%20diverse%20downstream%20tasks%20within%20the%20LMD%20domain%20and%20beyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23988v1&entry.124074799=Read"},
{"title": "EgoMimic: Scaling Imitation Learning via Egocentric Video", "author": "Simar Kareer and Dhruv Patel and Ryan Punamiya and Pranay Mathur and Shuo Cheng and Chen Wang and Judy Hoffman and Danfei Xu", "abstract": "  The scale and diversity of demonstration data required for imitation learning\nis a significant challenge. We present EgoMimic, a full-stack framework which\nscales manipulation via human embodiment data, specifically egocentric human\nvideos paired with 3D hand tracking. EgoMimic achieves this through: (1) a\nsystem to capture human embodiment data using the ergonomic Project Aria\nglasses, (2) a low-cost bimanual manipulator that minimizes the kinematic gap\nto human data, (3) cross-domain data alignment techniques, and (4) an imitation\nlearning architecture that co-trains on human and robot data. Compared to prior\nworks that only extract high-level intent from human videos, our approach\ntreats human and robot data equally as embodied demonstration data and learns a\nunified policy from both data sources. EgoMimic achieves significant\nimprovement on a diverse set of long-horizon, single-arm and bimanual\nmanipulation tasks over state-of-the-art imitation learning methods and enables\ngeneralization to entirely new scenes. Finally, we show a favorable scaling\ntrend for EgoMimic, where adding 1 hour of additional hand data is\nsignificantly more valuable than 1 hour of additional robot data. Videos and\nadditional information can be found at https://egomimic.github.io/\n", "link": "http://arxiv.org/abs/2410.24221v1", "date": "2024-10-31", "relevancy": 2.2446, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5649}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5609}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoMimic%3A%20Scaling%20Imitation%20Learning%20via%20Egocentric%20Video&body=Title%3A%20EgoMimic%3A%20Scaling%20Imitation%20Learning%20via%20Egocentric%20Video%0AAuthor%3A%20Simar%20Kareer%20and%20Dhruv%20Patel%20and%20Ryan%20Punamiya%20and%20Pranay%20Mathur%20and%20Shuo%20Cheng%20and%20Chen%20Wang%20and%20Judy%20Hoffman%20and%20Danfei%20Xu%0AAbstract%3A%20%20%20The%20scale%20and%20diversity%20of%20demonstration%20data%20required%20for%20imitation%20learning%0Ais%20a%20significant%20challenge.%20We%20present%20EgoMimic%2C%20a%20full-stack%20framework%20which%0Ascales%20manipulation%20via%20human%20embodiment%20data%2C%20specifically%20egocentric%20human%0Avideos%20paired%20with%203D%20hand%20tracking.%20EgoMimic%20achieves%20this%20through%3A%20%281%29%20a%0Asystem%20to%20capture%20human%20embodiment%20data%20using%20the%20ergonomic%20Project%20Aria%0Aglasses%2C%20%282%29%20a%20low-cost%20bimanual%20manipulator%20that%20minimizes%20the%20kinematic%20gap%0Ato%20human%20data%2C%20%283%29%20cross-domain%20data%20alignment%20techniques%2C%20and%20%284%29%20an%20imitation%0Alearning%20architecture%20that%20co-trains%20on%20human%20and%20robot%20data.%20Compared%20to%20prior%0Aworks%20that%20only%20extract%20high-level%20intent%20from%20human%20videos%2C%20our%20approach%0Atreats%20human%20and%20robot%20data%20equally%20as%20embodied%20demonstration%20data%20and%20learns%20a%0Aunified%20policy%20from%20both%20data%20sources.%20EgoMimic%20achieves%20significant%0Aimprovement%20on%20a%20diverse%20set%20of%20long-horizon%2C%20single-arm%20and%20bimanual%0Amanipulation%20tasks%20over%20state-of-the-art%20imitation%20learning%20methods%20and%20enables%0Ageneralization%20to%20entirely%20new%20scenes.%20Finally%2C%20we%20show%20a%20favorable%20scaling%0Atrend%20for%20EgoMimic%2C%20where%20adding%201%20hour%20of%20additional%20hand%20data%20is%0Asignificantly%20more%20valuable%20than%201%20hour%20of%20additional%20robot%20data.%20Videos%20and%0Aadditional%20information%20can%20be%20found%20at%20https%3A//egomimic.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoMimic%253A%2520Scaling%2520Imitation%2520Learning%2520via%2520Egocentric%2520Video%26entry.906535625%3DSimar%2520Kareer%2520and%2520Dhruv%2520Patel%2520and%2520Ryan%2520Punamiya%2520and%2520Pranay%2520Mathur%2520and%2520Shuo%2520Cheng%2520and%2520Chen%2520Wang%2520and%2520Judy%2520Hoffman%2520and%2520Danfei%2520Xu%26entry.1292438233%3D%2520%2520The%2520scale%2520and%2520diversity%2520of%2520demonstration%2520data%2520required%2520for%2520imitation%2520learning%250Ais%2520a%2520significant%2520challenge.%2520We%2520present%2520EgoMimic%252C%2520a%2520full-stack%2520framework%2520which%250Ascales%2520manipulation%2520via%2520human%2520embodiment%2520data%252C%2520specifically%2520egocentric%2520human%250Avideos%2520paired%2520with%25203D%2520hand%2520tracking.%2520EgoMimic%2520achieves%2520this%2520through%253A%2520%25281%2529%2520a%250Asystem%2520to%2520capture%2520human%2520embodiment%2520data%2520using%2520the%2520ergonomic%2520Project%2520Aria%250Aglasses%252C%2520%25282%2529%2520a%2520low-cost%2520bimanual%2520manipulator%2520that%2520minimizes%2520the%2520kinematic%2520gap%250Ato%2520human%2520data%252C%2520%25283%2529%2520cross-domain%2520data%2520alignment%2520techniques%252C%2520and%2520%25284%2529%2520an%2520imitation%250Alearning%2520architecture%2520that%2520co-trains%2520on%2520human%2520and%2520robot%2520data.%2520Compared%2520to%2520prior%250Aworks%2520that%2520only%2520extract%2520high-level%2520intent%2520from%2520human%2520videos%252C%2520our%2520approach%250Atreats%2520human%2520and%2520robot%2520data%2520equally%2520as%2520embodied%2520demonstration%2520data%2520and%2520learns%2520a%250Aunified%2520policy%2520from%2520both%2520data%2520sources.%2520EgoMimic%2520achieves%2520significant%250Aimprovement%2520on%2520a%2520diverse%2520set%2520of%2520long-horizon%252C%2520single-arm%2520and%2520bimanual%250Amanipulation%2520tasks%2520over%2520state-of-the-art%2520imitation%2520learning%2520methods%2520and%2520enables%250Ageneralization%2520to%2520entirely%2520new%2520scenes.%2520Finally%252C%2520we%2520show%2520a%2520favorable%2520scaling%250Atrend%2520for%2520EgoMimic%252C%2520where%2520adding%25201%2520hour%2520of%2520additional%2520hand%2520data%2520is%250Asignificantly%2520more%2520valuable%2520than%25201%2520hour%2520of%2520additional%2520robot%2520data.%2520Videos%2520and%250Aadditional%2520information%2520can%2520be%2520found%2520at%2520https%253A//egomimic.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoMimic%3A%20Scaling%20Imitation%20Learning%20via%20Egocentric%20Video&entry.906535625=Simar%20Kareer%20and%20Dhruv%20Patel%20and%20Ryan%20Punamiya%20and%20Pranay%20Mathur%20and%20Shuo%20Cheng%20and%20Chen%20Wang%20and%20Judy%20Hoffman%20and%20Danfei%20Xu&entry.1292438233=%20%20The%20scale%20and%20diversity%20of%20demonstration%20data%20required%20for%20imitation%20learning%0Ais%20a%20significant%20challenge.%20We%20present%20EgoMimic%2C%20a%20full-stack%20framework%20which%0Ascales%20manipulation%20via%20human%20embodiment%20data%2C%20specifically%20egocentric%20human%0Avideos%20paired%20with%203D%20hand%20tracking.%20EgoMimic%20achieves%20this%20through%3A%20%281%29%20a%0Asystem%20to%20capture%20human%20embodiment%20data%20using%20the%20ergonomic%20Project%20Aria%0Aglasses%2C%20%282%29%20a%20low-cost%20bimanual%20manipulator%20that%20minimizes%20the%20kinematic%20gap%0Ato%20human%20data%2C%20%283%29%20cross-domain%20data%20alignment%20techniques%2C%20and%20%284%29%20an%20imitation%0Alearning%20architecture%20that%20co-trains%20on%20human%20and%20robot%20data.%20Compared%20to%20prior%0Aworks%20that%20only%20extract%20high-level%20intent%20from%20human%20videos%2C%20our%20approach%0Atreats%20human%20and%20robot%20data%20equally%20as%20embodied%20demonstration%20data%20and%20learns%20a%0Aunified%20policy%20from%20both%20data%20sources.%20EgoMimic%20achieves%20significant%0Aimprovement%20on%20a%20diverse%20set%20of%20long-horizon%2C%20single-arm%20and%20bimanual%0Amanipulation%20tasks%20over%20state-of-the-art%20imitation%20learning%20methods%20and%20enables%0Ageneralization%20to%20entirely%20new%20scenes.%20Finally%2C%20we%20show%20a%20favorable%20scaling%0Atrend%20for%20EgoMimic%2C%20where%20adding%201%20hour%20of%20additional%20hand%20data%20is%0Asignificantly%20more%20valuable%20than%201%20hour%20of%20additional%20robot%20data.%20Videos%20and%0Aadditional%20information%20can%20be%20found%20at%20https%3A//egomimic.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24221v1&entry.124074799=Read"},
{"title": "Noise as a Double-Edged Sword: Reinforcement Learning Exploits\n  Randomized Defenses in Neural Networks", "author": "Steve Bakos and Pooria Madani and Heidar Davoudi", "abstract": "  This study investigates a counterintuitive phenomenon in adversarial machine\nlearning: the potential for noise-based defenses to inadvertently aid evasion\nattacks in certain scenarios. While randomness is often employed as a defensive\nstrategy against adversarial examples, our research reveals that this approach\ncan sometimes backfire, particularly when facing adaptive attackers using\nreinforcement learning (RL). Our findings show that in specific cases,\nespecially with visually noisy classes, the introduction of noise in the\nclassifier's confidence values can be exploited by the RL attacker, leading to\na significant increase in evasion success rates. In some instances, the\nnoise-based defense scenario outperformed other strategies by up to 20\\% on a\nsubset of classes. However, this effect was not consistent across all\nclassifiers tested, highlighting the complexity of the interaction between\nnoise-based defenses and different models. These results suggest that in some\ncases, noise-based defenses can inadvertently create an adversarial training\nloop beneficial to the RL attacker. Our study emphasizes the need for a more\nnuanced approach to defensive strategies in adversarial machine learning,\nparticularly in safety-critical applications. It challenges the assumption that\nrandomness universally enhances defense against evasion attacks and highlights\nthe importance of considering adaptive, RL-based attackers when designing\nrobust defense mechanisms.\n", "link": "http://arxiv.org/abs/2410.23870v1", "date": "2024-10-31", "relevancy": 2.2399, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.463}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.442}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Noise%20as%20a%20Double-Edged%20Sword%3A%20Reinforcement%20Learning%20Exploits%0A%20%20Randomized%20Defenses%20in%20Neural%20Networks&body=Title%3A%20Noise%20as%20a%20Double-Edged%20Sword%3A%20Reinforcement%20Learning%20Exploits%0A%20%20Randomized%20Defenses%20in%20Neural%20Networks%0AAuthor%3A%20Steve%20Bakos%20and%20Pooria%20Madani%20and%20Heidar%20Davoudi%0AAbstract%3A%20%20%20This%20study%20investigates%20a%20counterintuitive%20phenomenon%20in%20adversarial%20machine%0Alearning%3A%20the%20potential%20for%20noise-based%20defenses%20to%20inadvertently%20aid%20evasion%0Aattacks%20in%20certain%20scenarios.%20While%20randomness%20is%20often%20employed%20as%20a%20defensive%0Astrategy%20against%20adversarial%20examples%2C%20our%20research%20reveals%20that%20this%20approach%0Acan%20sometimes%20backfire%2C%20particularly%20when%20facing%20adaptive%20attackers%20using%0Areinforcement%20learning%20%28RL%29.%20Our%20findings%20show%20that%20in%20specific%20cases%2C%0Aespecially%20with%20visually%20noisy%20classes%2C%20the%20introduction%20of%20noise%20in%20the%0Aclassifier%27s%20confidence%20values%20can%20be%20exploited%20by%20the%20RL%20attacker%2C%20leading%20to%0Aa%20significant%20increase%20in%20evasion%20success%20rates.%20In%20some%20instances%2C%20the%0Anoise-based%20defense%20scenario%20outperformed%20other%20strategies%20by%20up%20to%2020%5C%25%20on%20a%0Asubset%20of%20classes.%20However%2C%20this%20effect%20was%20not%20consistent%20across%20all%0Aclassifiers%20tested%2C%20highlighting%20the%20complexity%20of%20the%20interaction%20between%0Anoise-based%20defenses%20and%20different%20models.%20These%20results%20suggest%20that%20in%20some%0Acases%2C%20noise-based%20defenses%20can%20inadvertently%20create%20an%20adversarial%20training%0Aloop%20beneficial%20to%20the%20RL%20attacker.%20Our%20study%20emphasizes%20the%20need%20for%20a%20more%0Anuanced%20approach%20to%20defensive%20strategies%20in%20adversarial%20machine%20learning%2C%0Aparticularly%20in%20safety-critical%20applications.%20It%20challenges%20the%20assumption%20that%0Arandomness%20universally%20enhances%20defense%20against%20evasion%20attacks%20and%20highlights%0Athe%20importance%20of%20considering%20adaptive%2C%20RL-based%20attackers%20when%20designing%0Arobust%20defense%20mechanisms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoise%2520as%2520a%2520Double-Edged%2520Sword%253A%2520Reinforcement%2520Learning%2520Exploits%250A%2520%2520Randomized%2520Defenses%2520in%2520Neural%2520Networks%26entry.906535625%3DSteve%2520Bakos%2520and%2520Pooria%2520Madani%2520and%2520Heidar%2520Davoudi%26entry.1292438233%3D%2520%2520This%2520study%2520investigates%2520a%2520counterintuitive%2520phenomenon%2520in%2520adversarial%2520machine%250Alearning%253A%2520the%2520potential%2520for%2520noise-based%2520defenses%2520to%2520inadvertently%2520aid%2520evasion%250Aattacks%2520in%2520certain%2520scenarios.%2520While%2520randomness%2520is%2520often%2520employed%2520as%2520a%2520defensive%250Astrategy%2520against%2520adversarial%2520examples%252C%2520our%2520research%2520reveals%2520that%2520this%2520approach%250Acan%2520sometimes%2520backfire%252C%2520particularly%2520when%2520facing%2520adaptive%2520attackers%2520using%250Areinforcement%2520learning%2520%2528RL%2529.%2520Our%2520findings%2520show%2520that%2520in%2520specific%2520cases%252C%250Aespecially%2520with%2520visually%2520noisy%2520classes%252C%2520the%2520introduction%2520of%2520noise%2520in%2520the%250Aclassifier%2527s%2520confidence%2520values%2520can%2520be%2520exploited%2520by%2520the%2520RL%2520attacker%252C%2520leading%2520to%250Aa%2520significant%2520increase%2520in%2520evasion%2520success%2520rates.%2520In%2520some%2520instances%252C%2520the%250Anoise-based%2520defense%2520scenario%2520outperformed%2520other%2520strategies%2520by%2520up%2520to%252020%255C%2525%2520on%2520a%250Asubset%2520of%2520classes.%2520However%252C%2520this%2520effect%2520was%2520not%2520consistent%2520across%2520all%250Aclassifiers%2520tested%252C%2520highlighting%2520the%2520complexity%2520of%2520the%2520interaction%2520between%250Anoise-based%2520defenses%2520and%2520different%2520models.%2520These%2520results%2520suggest%2520that%2520in%2520some%250Acases%252C%2520noise-based%2520defenses%2520can%2520inadvertently%2520create%2520an%2520adversarial%2520training%250Aloop%2520beneficial%2520to%2520the%2520RL%2520attacker.%2520Our%2520study%2520emphasizes%2520the%2520need%2520for%2520a%2520more%250Anuanced%2520approach%2520to%2520defensive%2520strategies%2520in%2520adversarial%2520machine%2520learning%252C%250Aparticularly%2520in%2520safety-critical%2520applications.%2520It%2520challenges%2520the%2520assumption%2520that%250Arandomness%2520universally%2520enhances%2520defense%2520against%2520evasion%2520attacks%2520and%2520highlights%250Athe%2520importance%2520of%2520considering%2520adaptive%252C%2520RL-based%2520attackers%2520when%2520designing%250Arobust%2520defense%2520mechanisms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Noise%20as%20a%20Double-Edged%20Sword%3A%20Reinforcement%20Learning%20Exploits%0A%20%20Randomized%20Defenses%20in%20Neural%20Networks&entry.906535625=Steve%20Bakos%20and%20Pooria%20Madani%20and%20Heidar%20Davoudi&entry.1292438233=%20%20This%20study%20investigates%20a%20counterintuitive%20phenomenon%20in%20adversarial%20machine%0Alearning%3A%20the%20potential%20for%20noise-based%20defenses%20to%20inadvertently%20aid%20evasion%0Aattacks%20in%20certain%20scenarios.%20While%20randomness%20is%20often%20employed%20as%20a%20defensive%0Astrategy%20against%20adversarial%20examples%2C%20our%20research%20reveals%20that%20this%20approach%0Acan%20sometimes%20backfire%2C%20particularly%20when%20facing%20adaptive%20attackers%20using%0Areinforcement%20learning%20%28RL%29.%20Our%20findings%20show%20that%20in%20specific%20cases%2C%0Aespecially%20with%20visually%20noisy%20classes%2C%20the%20introduction%20of%20noise%20in%20the%0Aclassifier%27s%20confidence%20values%20can%20be%20exploited%20by%20the%20RL%20attacker%2C%20leading%20to%0Aa%20significant%20increase%20in%20evasion%20success%20rates.%20In%20some%20instances%2C%20the%0Anoise-based%20defense%20scenario%20outperformed%20other%20strategies%20by%20up%20to%2020%5C%25%20on%20a%0Asubset%20of%20classes.%20However%2C%20this%20effect%20was%20not%20consistent%20across%20all%0Aclassifiers%20tested%2C%20highlighting%20the%20complexity%20of%20the%20interaction%20between%0Anoise-based%20defenses%20and%20different%20models.%20These%20results%20suggest%20that%20in%20some%0Acases%2C%20noise-based%20defenses%20can%20inadvertently%20create%20an%20adversarial%20training%0Aloop%20beneficial%20to%20the%20RL%20attacker.%20Our%20study%20emphasizes%20the%20need%20for%20a%20more%0Anuanced%20approach%20to%20defensive%20strategies%20in%20adversarial%20machine%20learning%2C%0Aparticularly%20in%20safety-critical%20applications.%20It%20challenges%20the%20assumption%20that%0Arandomness%20universally%20enhances%20defense%20against%20evasion%20attacks%20and%20highlights%0Athe%20importance%20of%20considering%20adaptive%2C%20RL-based%20attackers%20when%20designing%0Arobust%20defense%20mechanisms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23870v1&entry.124074799=Read"},
{"title": "An Optimism-based Approach to Online Evaluation of Generative Models", "author": "Xiaoyan Hu and Ho-fung Leung and Farzan Farnia", "abstract": "  Existing frameworks for evaluating and comparing generative models typically\ntarget an offline setting, where the evaluator has access to full batches of\ndata produced by the models. However, in many practical scenarios, the goal is\nto identify the best model using the fewest generated samples to minimize the\ncosts of querying data from the models. Such an online comparison is\nchallenging with current offline assessment methods. In this work, we propose\nan online evaluation framework to find the generative model that maximizes a\nstandard assessment score among a group of available models. Our method uses an\noptimism-based multi-armed bandit framework to identify the model producing\ndata with the highest evaluation score, quantifying the quality and diversity\nof generated data. Specifically, we study the online assessment of generative\nmodels based on the Fr\\'echet Inception Distance (FID) and Inception Score (IS)\nmetrics and propose the FID-UCB and IS-UCB algorithms leveraging the upper\nconfidence bound approach in online learning. We prove sub-linear regret bounds\nfor these algorithms and present numerical results on standard image datasets,\ndemonstrating their effectiveness in identifying the score-maximizing\ngenerative model.\n", "link": "http://arxiv.org/abs/2406.07451v2", "date": "2024-10-31", "relevancy": 2.2358, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5797}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5613}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Optimism-based%20Approach%20to%20Online%20Evaluation%20of%20Generative%20Models&body=Title%3A%20An%20Optimism-based%20Approach%20to%20Online%20Evaluation%20of%20Generative%20Models%0AAuthor%3A%20Xiaoyan%20Hu%20and%20Ho-fung%20Leung%20and%20Farzan%20Farnia%0AAbstract%3A%20%20%20Existing%20frameworks%20for%20evaluating%20and%20comparing%20generative%20models%20typically%0Atarget%20an%20offline%20setting%2C%20where%20the%20evaluator%20has%20access%20to%20full%20batches%20of%0Adata%20produced%20by%20the%20models.%20However%2C%20in%20many%20practical%20scenarios%2C%20the%20goal%20is%0Ato%20identify%20the%20best%20model%20using%20the%20fewest%20generated%20samples%20to%20minimize%20the%0Acosts%20of%20querying%20data%20from%20the%20models.%20Such%20an%20online%20comparison%20is%0Achallenging%20with%20current%20offline%20assessment%20methods.%20In%20this%20work%2C%20we%20propose%0Aan%20online%20evaluation%20framework%20to%20find%20the%20generative%20model%20that%20maximizes%20a%0Astandard%20assessment%20score%20among%20a%20group%20of%20available%20models.%20Our%20method%20uses%20an%0Aoptimism-based%20multi-armed%20bandit%20framework%20to%20identify%20the%20model%20producing%0Adata%20with%20the%20highest%20evaluation%20score%2C%20quantifying%20the%20quality%20and%20diversity%0Aof%20generated%20data.%20Specifically%2C%20we%20study%20the%20online%20assessment%20of%20generative%0Amodels%20based%20on%20the%20Fr%5C%27echet%20Inception%20Distance%20%28FID%29%20and%20Inception%20Score%20%28IS%29%0Ametrics%20and%20propose%20the%20FID-UCB%20and%20IS-UCB%20algorithms%20leveraging%20the%20upper%0Aconfidence%20bound%20approach%20in%20online%20learning.%20We%20prove%20sub-linear%20regret%20bounds%0Afor%20these%20algorithms%20and%20present%20numerical%20results%20on%20standard%20image%20datasets%2C%0Ademonstrating%20their%20effectiveness%20in%20identifying%20the%20score-maximizing%0Agenerative%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07451v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Optimism-based%2520Approach%2520to%2520Online%2520Evaluation%2520of%2520Generative%2520Models%26entry.906535625%3DXiaoyan%2520Hu%2520and%2520Ho-fung%2520Leung%2520and%2520Farzan%2520Farnia%26entry.1292438233%3D%2520%2520Existing%2520frameworks%2520for%2520evaluating%2520and%2520comparing%2520generative%2520models%2520typically%250Atarget%2520an%2520offline%2520setting%252C%2520where%2520the%2520evaluator%2520has%2520access%2520to%2520full%2520batches%2520of%250Adata%2520produced%2520by%2520the%2520models.%2520However%252C%2520in%2520many%2520practical%2520scenarios%252C%2520the%2520goal%2520is%250Ato%2520identify%2520the%2520best%2520model%2520using%2520the%2520fewest%2520generated%2520samples%2520to%2520minimize%2520the%250Acosts%2520of%2520querying%2520data%2520from%2520the%2520models.%2520Such%2520an%2520online%2520comparison%2520is%250Achallenging%2520with%2520current%2520offline%2520assessment%2520methods.%2520In%2520this%2520work%252C%2520we%2520propose%250Aan%2520online%2520evaluation%2520framework%2520to%2520find%2520the%2520generative%2520model%2520that%2520maximizes%2520a%250Astandard%2520assessment%2520score%2520among%2520a%2520group%2520of%2520available%2520models.%2520Our%2520method%2520uses%2520an%250Aoptimism-based%2520multi-armed%2520bandit%2520framework%2520to%2520identify%2520the%2520model%2520producing%250Adata%2520with%2520the%2520highest%2520evaluation%2520score%252C%2520quantifying%2520the%2520quality%2520and%2520diversity%250Aof%2520generated%2520data.%2520Specifically%252C%2520we%2520study%2520the%2520online%2520assessment%2520of%2520generative%250Amodels%2520based%2520on%2520the%2520Fr%255C%2527echet%2520Inception%2520Distance%2520%2528FID%2529%2520and%2520Inception%2520Score%2520%2528IS%2529%250Ametrics%2520and%2520propose%2520the%2520FID-UCB%2520and%2520IS-UCB%2520algorithms%2520leveraging%2520the%2520upper%250Aconfidence%2520bound%2520approach%2520in%2520online%2520learning.%2520We%2520prove%2520sub-linear%2520regret%2520bounds%250Afor%2520these%2520algorithms%2520and%2520present%2520numerical%2520results%2520on%2520standard%2520image%2520datasets%252C%250Ademonstrating%2520their%2520effectiveness%2520in%2520identifying%2520the%2520score-maximizing%250Agenerative%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07451v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Optimism-based%20Approach%20to%20Online%20Evaluation%20of%20Generative%20Models&entry.906535625=Xiaoyan%20Hu%20and%20Ho-fung%20Leung%20and%20Farzan%20Farnia&entry.1292438233=%20%20Existing%20frameworks%20for%20evaluating%20and%20comparing%20generative%20models%20typically%0Atarget%20an%20offline%20setting%2C%20where%20the%20evaluator%20has%20access%20to%20full%20batches%20of%0Adata%20produced%20by%20the%20models.%20However%2C%20in%20many%20practical%20scenarios%2C%20the%20goal%20is%0Ato%20identify%20the%20best%20model%20using%20the%20fewest%20generated%20samples%20to%20minimize%20the%0Acosts%20of%20querying%20data%20from%20the%20models.%20Such%20an%20online%20comparison%20is%0Achallenging%20with%20current%20offline%20assessment%20methods.%20In%20this%20work%2C%20we%20propose%0Aan%20online%20evaluation%20framework%20to%20find%20the%20generative%20model%20that%20maximizes%20a%0Astandard%20assessment%20score%20among%20a%20group%20of%20available%20models.%20Our%20method%20uses%20an%0Aoptimism-based%20multi-armed%20bandit%20framework%20to%20identify%20the%20model%20producing%0Adata%20with%20the%20highest%20evaluation%20score%2C%20quantifying%20the%20quality%20and%20diversity%0Aof%20generated%20data.%20Specifically%2C%20we%20study%20the%20online%20assessment%20of%20generative%0Amodels%20based%20on%20the%20Fr%5C%27echet%20Inception%20Distance%20%28FID%29%20and%20Inception%20Score%20%28IS%29%0Ametrics%20and%20propose%20the%20FID-UCB%20and%20IS-UCB%20algorithms%20leveraging%20the%20upper%0Aconfidence%20bound%20approach%20in%20online%20learning.%20We%20prove%20sub-linear%20regret%20bounds%0Afor%20these%20algorithms%20and%20present%20numerical%20results%20on%20standard%20image%20datasets%2C%0Ademonstrating%20their%20effectiveness%20in%20identifying%20the%20score-maximizing%0Agenerative%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07451v2&entry.124074799=Read"},
{"title": "NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and\n  Benchmarking", "author": "Daniel Dauner and Marcel Hallgarten and Tianyu Li and Xinshuo Weng and Zhiyu Huang and Zetong Yang and Hongyang Li and Igor Gilitschenski and Boris Ivanovic and Marco Pavone and Andreas Geiger and Kashyap Chitta", "abstract": "  Benchmarking vision-based driving policies is challenging. On one hand,\nopen-loop evaluation with real data is easy, but these results do not reflect\nclosed-loop performance. On the other, closed-loop evaluation is possible in\nsimulation, but is hard to scale due to its significant computational demands.\nFurther, the simulators available today exhibit a large domain gap to real\ndata. This has resulted in an inability to draw clear conclusions from the\nrapidly growing body of research on end-to-end autonomous driving. In this\npaper, we present NAVSIM, a middle ground between these evaluation paradigms,\nwhere we use large datasets in combination with a non-reactive simulator to\nenable large-scale real-world benchmarking. Specifically, we gather\nsimulation-based metrics, such as progress and time to collision, by unrolling\nbird's eye view abstractions of the test scenes for a short simulation horizon.\nOur simulation is non-reactive, i.e., the evaluated policy and environment do\nnot influence each other. As we demonstrate empirically, this decoupling allows\nopen-loop metric computation while being better aligned with closed-loop\nevaluations than traditional displacement errors. NAVSIM enabled a new\ncompetition held at CVPR 2024, where 143 teams submitted 463 entries, resulting\nin several new insights. On a large set of challenging scenarios, we observe\nthat simple methods with moderate compute requirements such as TransFuser can\nmatch recent large-scale end-to-end driving architectures such as UniAD. Our\nmodular framework can potentially be extended with new datasets, data curation\nstrategies, and metrics, and will be continually maintained to host future\nchallenges. Our code is available at\nhttps://github.com/autonomousvision/navsim.\n", "link": "http://arxiv.org/abs/2406.15349v2", "date": "2024-10-31", "relevancy": 2.2344, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5695}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.557}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5484}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NAVSIM%3A%20Data-Driven%20Non-Reactive%20Autonomous%20Vehicle%20Simulation%20and%0A%20%20Benchmarking&body=Title%3A%20NAVSIM%3A%20Data-Driven%20Non-Reactive%20Autonomous%20Vehicle%20Simulation%20and%0A%20%20Benchmarking%0AAuthor%3A%20Daniel%20Dauner%20and%20Marcel%20Hallgarten%20and%20Tianyu%20Li%20and%20Xinshuo%20Weng%20and%20Zhiyu%20Huang%20and%20Zetong%20Yang%20and%20Hongyang%20Li%20and%20Igor%20Gilitschenski%20and%20Boris%20Ivanovic%20and%20Marco%20Pavone%20and%20Andreas%20Geiger%20and%20Kashyap%20Chitta%0AAbstract%3A%20%20%20Benchmarking%20vision-based%20driving%20policies%20is%20challenging.%20On%20one%20hand%2C%0Aopen-loop%20evaluation%20with%20real%20data%20is%20easy%2C%20but%20these%20results%20do%20not%20reflect%0Aclosed-loop%20performance.%20On%20the%20other%2C%20closed-loop%20evaluation%20is%20possible%20in%0Asimulation%2C%20but%20is%20hard%20to%20scale%20due%20to%20its%20significant%20computational%20demands.%0AFurther%2C%20the%20simulators%20available%20today%20exhibit%20a%20large%20domain%20gap%20to%20real%0Adata.%20This%20has%20resulted%20in%20an%20inability%20to%20draw%20clear%20conclusions%20from%20the%0Arapidly%20growing%20body%20of%20research%20on%20end-to-end%20autonomous%20driving.%20In%20this%0Apaper%2C%20we%20present%20NAVSIM%2C%20a%20middle%20ground%20between%20these%20evaluation%20paradigms%2C%0Awhere%20we%20use%20large%20datasets%20in%20combination%20with%20a%20non-reactive%20simulator%20to%0Aenable%20large-scale%20real-world%20benchmarking.%20Specifically%2C%20we%20gather%0Asimulation-based%20metrics%2C%20such%20as%20progress%20and%20time%20to%20collision%2C%20by%20unrolling%0Abird%27s%20eye%20view%20abstractions%20of%20the%20test%20scenes%20for%20a%20short%20simulation%20horizon.%0AOur%20simulation%20is%20non-reactive%2C%20i.e.%2C%20the%20evaluated%20policy%20and%20environment%20do%0Anot%20influence%20each%20other.%20As%20we%20demonstrate%20empirically%2C%20this%20decoupling%20allows%0Aopen-loop%20metric%20computation%20while%20being%20better%20aligned%20with%20closed-loop%0Aevaluations%20than%20traditional%20displacement%20errors.%20NAVSIM%20enabled%20a%20new%0Acompetition%20held%20at%20CVPR%202024%2C%20where%20143%20teams%20submitted%20463%20entries%2C%20resulting%0Ain%20several%20new%20insights.%20On%20a%20large%20set%20of%20challenging%20scenarios%2C%20we%20observe%0Athat%20simple%20methods%20with%20moderate%20compute%20requirements%20such%20as%20TransFuser%20can%0Amatch%20recent%20large-scale%20end-to-end%20driving%20architectures%20such%20as%20UniAD.%20Our%0Amodular%20framework%20can%20potentially%20be%20extended%20with%20new%20datasets%2C%20data%20curation%0Astrategies%2C%20and%20metrics%2C%20and%20will%20be%20continually%20maintained%20to%20host%20future%0Achallenges.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/autonomousvision/navsim.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.15349v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNAVSIM%253A%2520Data-Driven%2520Non-Reactive%2520Autonomous%2520Vehicle%2520Simulation%2520and%250A%2520%2520Benchmarking%26entry.906535625%3DDaniel%2520Dauner%2520and%2520Marcel%2520Hallgarten%2520and%2520Tianyu%2520Li%2520and%2520Xinshuo%2520Weng%2520and%2520Zhiyu%2520Huang%2520and%2520Zetong%2520Yang%2520and%2520Hongyang%2520Li%2520and%2520Igor%2520Gilitschenski%2520and%2520Boris%2520Ivanovic%2520and%2520Marco%2520Pavone%2520and%2520Andreas%2520Geiger%2520and%2520Kashyap%2520Chitta%26entry.1292438233%3D%2520%2520Benchmarking%2520vision-based%2520driving%2520policies%2520is%2520challenging.%2520On%2520one%2520hand%252C%250Aopen-loop%2520evaluation%2520with%2520real%2520data%2520is%2520easy%252C%2520but%2520these%2520results%2520do%2520not%2520reflect%250Aclosed-loop%2520performance.%2520On%2520the%2520other%252C%2520closed-loop%2520evaluation%2520is%2520possible%2520in%250Asimulation%252C%2520but%2520is%2520hard%2520to%2520scale%2520due%2520to%2520its%2520significant%2520computational%2520demands.%250AFurther%252C%2520the%2520simulators%2520available%2520today%2520exhibit%2520a%2520large%2520domain%2520gap%2520to%2520real%250Adata.%2520This%2520has%2520resulted%2520in%2520an%2520inability%2520to%2520draw%2520clear%2520conclusions%2520from%2520the%250Arapidly%2520growing%2520body%2520of%2520research%2520on%2520end-to-end%2520autonomous%2520driving.%2520In%2520this%250Apaper%252C%2520we%2520present%2520NAVSIM%252C%2520a%2520middle%2520ground%2520between%2520these%2520evaluation%2520paradigms%252C%250Awhere%2520we%2520use%2520large%2520datasets%2520in%2520combination%2520with%2520a%2520non-reactive%2520simulator%2520to%250Aenable%2520large-scale%2520real-world%2520benchmarking.%2520Specifically%252C%2520we%2520gather%250Asimulation-based%2520metrics%252C%2520such%2520as%2520progress%2520and%2520time%2520to%2520collision%252C%2520by%2520unrolling%250Abird%2527s%2520eye%2520view%2520abstractions%2520of%2520the%2520test%2520scenes%2520for%2520a%2520short%2520simulation%2520horizon.%250AOur%2520simulation%2520is%2520non-reactive%252C%2520i.e.%252C%2520the%2520evaluated%2520policy%2520and%2520environment%2520do%250Anot%2520influence%2520each%2520other.%2520As%2520we%2520demonstrate%2520empirically%252C%2520this%2520decoupling%2520allows%250Aopen-loop%2520metric%2520computation%2520while%2520being%2520better%2520aligned%2520with%2520closed-loop%250Aevaluations%2520than%2520traditional%2520displacement%2520errors.%2520NAVSIM%2520enabled%2520a%2520new%250Acompetition%2520held%2520at%2520CVPR%25202024%252C%2520where%2520143%2520teams%2520submitted%2520463%2520entries%252C%2520resulting%250Ain%2520several%2520new%2520insights.%2520On%2520a%2520large%2520set%2520of%2520challenging%2520scenarios%252C%2520we%2520observe%250Athat%2520simple%2520methods%2520with%2520moderate%2520compute%2520requirements%2520such%2520as%2520TransFuser%2520can%250Amatch%2520recent%2520large-scale%2520end-to-end%2520driving%2520architectures%2520such%2520as%2520UniAD.%2520Our%250Amodular%2520framework%2520can%2520potentially%2520be%2520extended%2520with%2520new%2520datasets%252C%2520data%2520curation%250Astrategies%252C%2520and%2520metrics%252C%2520and%2520will%2520be%2520continually%2520maintained%2520to%2520host%2520future%250Achallenges.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/autonomousvision/navsim.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.15349v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NAVSIM%3A%20Data-Driven%20Non-Reactive%20Autonomous%20Vehicle%20Simulation%20and%0A%20%20Benchmarking&entry.906535625=Daniel%20Dauner%20and%20Marcel%20Hallgarten%20and%20Tianyu%20Li%20and%20Xinshuo%20Weng%20and%20Zhiyu%20Huang%20and%20Zetong%20Yang%20and%20Hongyang%20Li%20and%20Igor%20Gilitschenski%20and%20Boris%20Ivanovic%20and%20Marco%20Pavone%20and%20Andreas%20Geiger%20and%20Kashyap%20Chitta&entry.1292438233=%20%20Benchmarking%20vision-based%20driving%20policies%20is%20challenging.%20On%20one%20hand%2C%0Aopen-loop%20evaluation%20with%20real%20data%20is%20easy%2C%20but%20these%20results%20do%20not%20reflect%0Aclosed-loop%20performance.%20On%20the%20other%2C%20closed-loop%20evaluation%20is%20possible%20in%0Asimulation%2C%20but%20is%20hard%20to%20scale%20due%20to%20its%20significant%20computational%20demands.%0AFurther%2C%20the%20simulators%20available%20today%20exhibit%20a%20large%20domain%20gap%20to%20real%0Adata.%20This%20has%20resulted%20in%20an%20inability%20to%20draw%20clear%20conclusions%20from%20the%0Arapidly%20growing%20body%20of%20research%20on%20end-to-end%20autonomous%20driving.%20In%20this%0Apaper%2C%20we%20present%20NAVSIM%2C%20a%20middle%20ground%20between%20these%20evaluation%20paradigms%2C%0Awhere%20we%20use%20large%20datasets%20in%20combination%20with%20a%20non-reactive%20simulator%20to%0Aenable%20large-scale%20real-world%20benchmarking.%20Specifically%2C%20we%20gather%0Asimulation-based%20metrics%2C%20such%20as%20progress%20and%20time%20to%20collision%2C%20by%20unrolling%0Abird%27s%20eye%20view%20abstractions%20of%20the%20test%20scenes%20for%20a%20short%20simulation%20horizon.%0AOur%20simulation%20is%20non-reactive%2C%20i.e.%2C%20the%20evaluated%20policy%20and%20environment%20do%0Anot%20influence%20each%20other.%20As%20we%20demonstrate%20empirically%2C%20this%20decoupling%20allows%0Aopen-loop%20metric%20computation%20while%20being%20better%20aligned%20with%20closed-loop%0Aevaluations%20than%20traditional%20displacement%20errors.%20NAVSIM%20enabled%20a%20new%0Acompetition%20held%20at%20CVPR%202024%2C%20where%20143%20teams%20submitted%20463%20entries%2C%20resulting%0Ain%20several%20new%20insights.%20On%20a%20large%20set%20of%20challenging%20scenarios%2C%20we%20observe%0Athat%20simple%20methods%20with%20moderate%20compute%20requirements%20such%20as%20TransFuser%20can%0Amatch%20recent%20large-scale%20end-to-end%20driving%20architectures%20such%20as%20UniAD.%20Our%0Amodular%20framework%20can%20potentially%20be%20extended%20with%20new%20datasets%2C%20data%20curation%0Astrategies%2C%20and%20metrics%2C%20and%20will%20be%20continually%20maintained%20to%20host%20future%0Achallenges.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/autonomousvision/navsim.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.15349v2&entry.124074799=Read"},
{"title": "Federated Learning over Connected Modes", "author": "Dennis Grinwald and Philipp Wiesner and Shinichi Nakajima", "abstract": "  Statistical heterogeneity in federated learning poses two major challenges:\nslow global training due to conflicting gradient signals, and the need of\npersonalization for local distributions. In this work, we tackle both\nchallenges by leveraging recent advances in \\emph{linear mode connectivity} --\nidentifying a linearly connected low-loss region in the parameter space of\nneural networks, which we call solution simplex. We propose federated learning\nover connected modes (\\textsc{Floco}), where clients are assigned local\nsubregions in this simplex based on their gradient signals, and together learn\nthe shared global solution simplex. This allows personalization of the client\nmodels to fit their local distributions within the degrees of freedom in the\nsolution simplex and homogenizes the update signals for the global simplex\ntraining. Our experiments show that \\textsc{Floco} accelerates the global\ntraining process, and significantly improves the local accuracy with minimal\ncomputational overhead in cross-silo federated learning settings.\n", "link": "http://arxiv.org/abs/2403.03333v3", "date": "2024-10-31", "relevancy": 2.2285, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4478}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4462}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Learning%20over%20Connected%20Modes&body=Title%3A%20Federated%20Learning%20over%20Connected%20Modes%0AAuthor%3A%20Dennis%20Grinwald%20and%20Philipp%20Wiesner%20and%20Shinichi%20Nakajima%0AAbstract%3A%20%20%20Statistical%20heterogeneity%20in%20federated%20learning%20poses%20two%20major%20challenges%3A%0Aslow%20global%20training%20due%20to%20conflicting%20gradient%20signals%2C%20and%20the%20need%20of%0Apersonalization%20for%20local%20distributions.%20In%20this%20work%2C%20we%20tackle%20both%0Achallenges%20by%20leveraging%20recent%20advances%20in%20%5Cemph%7Blinear%20mode%20connectivity%7D%20--%0Aidentifying%20a%20linearly%20connected%20low-loss%20region%20in%20the%20parameter%20space%20of%0Aneural%20networks%2C%20which%20we%20call%20solution%20simplex.%20We%20propose%20federated%20learning%0Aover%20connected%20modes%20%28%5Ctextsc%7BFloco%7D%29%2C%20where%20clients%20are%20assigned%20local%0Asubregions%20in%20this%20simplex%20based%20on%20their%20gradient%20signals%2C%20and%20together%20learn%0Athe%20shared%20global%20solution%20simplex.%20This%20allows%20personalization%20of%20the%20client%0Amodels%20to%20fit%20their%20local%20distributions%20within%20the%20degrees%20of%20freedom%20in%20the%0Asolution%20simplex%20and%20homogenizes%20the%20update%20signals%20for%20the%20global%20simplex%0Atraining.%20Our%20experiments%20show%20that%20%5Ctextsc%7BFloco%7D%20accelerates%20the%20global%0Atraining%20process%2C%20and%20significantly%20improves%20the%20local%20accuracy%20with%20minimal%0Acomputational%20overhead%20in%20cross-silo%20federated%20learning%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03333v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Learning%2520over%2520Connected%2520Modes%26entry.906535625%3DDennis%2520Grinwald%2520and%2520Philipp%2520Wiesner%2520and%2520Shinichi%2520Nakajima%26entry.1292438233%3D%2520%2520Statistical%2520heterogeneity%2520in%2520federated%2520learning%2520poses%2520two%2520major%2520challenges%253A%250Aslow%2520global%2520training%2520due%2520to%2520conflicting%2520gradient%2520signals%252C%2520and%2520the%2520need%2520of%250Apersonalization%2520for%2520local%2520distributions.%2520In%2520this%2520work%252C%2520we%2520tackle%2520both%250Achallenges%2520by%2520leveraging%2520recent%2520advances%2520in%2520%255Cemph%257Blinear%2520mode%2520connectivity%257D%2520--%250Aidentifying%2520a%2520linearly%2520connected%2520low-loss%2520region%2520in%2520the%2520parameter%2520space%2520of%250Aneural%2520networks%252C%2520which%2520we%2520call%2520solution%2520simplex.%2520We%2520propose%2520federated%2520learning%250Aover%2520connected%2520modes%2520%2528%255Ctextsc%257BFloco%257D%2529%252C%2520where%2520clients%2520are%2520assigned%2520local%250Asubregions%2520in%2520this%2520simplex%2520based%2520on%2520their%2520gradient%2520signals%252C%2520and%2520together%2520learn%250Athe%2520shared%2520global%2520solution%2520simplex.%2520This%2520allows%2520personalization%2520of%2520the%2520client%250Amodels%2520to%2520fit%2520their%2520local%2520distributions%2520within%2520the%2520degrees%2520of%2520freedom%2520in%2520the%250Asolution%2520simplex%2520and%2520homogenizes%2520the%2520update%2520signals%2520for%2520the%2520global%2520simplex%250Atraining.%2520Our%2520experiments%2520show%2520that%2520%255Ctextsc%257BFloco%257D%2520accelerates%2520the%2520global%250Atraining%2520process%252C%2520and%2520significantly%2520improves%2520the%2520local%2520accuracy%2520with%2520minimal%250Acomputational%2520overhead%2520in%2520cross-silo%2520federated%2520learning%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03333v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Learning%20over%20Connected%20Modes&entry.906535625=Dennis%20Grinwald%20and%20Philipp%20Wiesner%20and%20Shinichi%20Nakajima&entry.1292438233=%20%20Statistical%20heterogeneity%20in%20federated%20learning%20poses%20two%20major%20challenges%3A%0Aslow%20global%20training%20due%20to%20conflicting%20gradient%20signals%2C%20and%20the%20need%20of%0Apersonalization%20for%20local%20distributions.%20In%20this%20work%2C%20we%20tackle%20both%0Achallenges%20by%20leveraging%20recent%20advances%20in%20%5Cemph%7Blinear%20mode%20connectivity%7D%20--%0Aidentifying%20a%20linearly%20connected%20low-loss%20region%20in%20the%20parameter%20space%20of%0Aneural%20networks%2C%20which%20we%20call%20solution%20simplex.%20We%20propose%20federated%20learning%0Aover%20connected%20modes%20%28%5Ctextsc%7BFloco%7D%29%2C%20where%20clients%20are%20assigned%20local%0Asubregions%20in%20this%20simplex%20based%20on%20their%20gradient%20signals%2C%20and%20together%20learn%0Athe%20shared%20global%20solution%20simplex.%20This%20allows%20personalization%20of%20the%20client%0Amodels%20to%20fit%20their%20local%20distributions%20within%20the%20degrees%20of%20freedom%20in%20the%0Asolution%20simplex%20and%20homogenizes%20the%20update%20signals%20for%20the%20global%20simplex%0Atraining.%20Our%20experiments%20show%20that%20%5Ctextsc%7BFloco%7D%20accelerates%20the%20global%0Atraining%20process%2C%20and%20significantly%20improves%20the%20local%20accuracy%20with%20minimal%0Acomputational%20overhead%20in%20cross-silo%20federated%20learning%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03333v3&entry.124074799=Read"},
{"title": "Efficient Inference and Computation of Optimal Alternatives for\n  Preference Languages Based On Lexicographic Models", "author": "Nic Wilson and Anne-Marie George", "abstract": "  We analyse preference inference, through consistency, for general preference\nlanguages based on lexicographic models. We identify a property, which we call\nstrong compositionality, that applies for many natural kinds of preference\nstatement, and that allows a greedy algorithm for determining consistency of a\nset of preference statements. We also consider different natural definitions of\noptimality, and their relations to each other, for general preference languages\nbased on lexicographic models. Based on our framework, we show that testing\nconsistency, and thus inference, is polynomial for a specific preference\nlanguage LpqT, which allows strict and non-strict statements, comparisons\nbetween outcomes and between partial tuples, both ceteris paribus and strong\nstatements, and their combination. Computing different kinds of optimal sets is\nalso shown to be polynomial; this is backed up by our experimental results.\n", "link": "http://arxiv.org/abs/2410.23913v1", "date": "2024-10-31", "relevancy": 2.2276, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4544}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4544}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Inference%20and%20Computation%20of%20Optimal%20Alternatives%20for%0A%20%20Preference%20Languages%20Based%20On%20Lexicographic%20Models&body=Title%3A%20Efficient%20Inference%20and%20Computation%20of%20Optimal%20Alternatives%20for%0A%20%20Preference%20Languages%20Based%20On%20Lexicographic%20Models%0AAuthor%3A%20Nic%20Wilson%20and%20Anne-Marie%20George%0AAbstract%3A%20%20%20We%20analyse%20preference%20inference%2C%20through%20consistency%2C%20for%20general%20preference%0Alanguages%20based%20on%20lexicographic%20models.%20We%20identify%20a%20property%2C%20which%20we%20call%0Astrong%20compositionality%2C%20that%20applies%20for%20many%20natural%20kinds%20of%20preference%0Astatement%2C%20and%20that%20allows%20a%20greedy%20algorithm%20for%20determining%20consistency%20of%20a%0Aset%20of%20preference%20statements.%20We%20also%20consider%20different%20natural%20definitions%20of%0Aoptimality%2C%20and%20their%20relations%20to%20each%20other%2C%20for%20general%20preference%20languages%0Abased%20on%20lexicographic%20models.%20Based%20on%20our%20framework%2C%20we%20show%20that%20testing%0Aconsistency%2C%20and%20thus%20inference%2C%20is%20polynomial%20for%20a%20specific%20preference%0Alanguage%20LpqT%2C%20which%20allows%20strict%20and%20non-strict%20statements%2C%20comparisons%0Abetween%20outcomes%20and%20between%20partial%20tuples%2C%20both%20ceteris%20paribus%20and%20strong%0Astatements%2C%20and%20their%20combination.%20Computing%20different%20kinds%20of%20optimal%20sets%20is%0Aalso%20shown%20to%20be%20polynomial%3B%20this%20is%20backed%20up%20by%20our%20experimental%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Inference%2520and%2520Computation%2520of%2520Optimal%2520Alternatives%2520for%250A%2520%2520Preference%2520Languages%2520Based%2520On%2520Lexicographic%2520Models%26entry.906535625%3DNic%2520Wilson%2520and%2520Anne-Marie%2520George%26entry.1292438233%3D%2520%2520We%2520analyse%2520preference%2520inference%252C%2520through%2520consistency%252C%2520for%2520general%2520preference%250Alanguages%2520based%2520on%2520lexicographic%2520models.%2520We%2520identify%2520a%2520property%252C%2520which%2520we%2520call%250Astrong%2520compositionality%252C%2520that%2520applies%2520for%2520many%2520natural%2520kinds%2520of%2520preference%250Astatement%252C%2520and%2520that%2520allows%2520a%2520greedy%2520algorithm%2520for%2520determining%2520consistency%2520of%2520a%250Aset%2520of%2520preference%2520statements.%2520We%2520also%2520consider%2520different%2520natural%2520definitions%2520of%250Aoptimality%252C%2520and%2520their%2520relations%2520to%2520each%2520other%252C%2520for%2520general%2520preference%2520languages%250Abased%2520on%2520lexicographic%2520models.%2520Based%2520on%2520our%2520framework%252C%2520we%2520show%2520that%2520testing%250Aconsistency%252C%2520and%2520thus%2520inference%252C%2520is%2520polynomial%2520for%2520a%2520specific%2520preference%250Alanguage%2520LpqT%252C%2520which%2520allows%2520strict%2520and%2520non-strict%2520statements%252C%2520comparisons%250Abetween%2520outcomes%2520and%2520between%2520partial%2520tuples%252C%2520both%2520ceteris%2520paribus%2520and%2520strong%250Astatements%252C%2520and%2520their%2520combination.%2520Computing%2520different%2520kinds%2520of%2520optimal%2520sets%2520is%250Aalso%2520shown%2520to%2520be%2520polynomial%253B%2520this%2520is%2520backed%2520up%2520by%2520our%2520experimental%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Inference%20and%20Computation%20of%20Optimal%20Alternatives%20for%0A%20%20Preference%20Languages%20Based%20On%20Lexicographic%20Models&entry.906535625=Nic%20Wilson%20and%20Anne-Marie%20George&entry.1292438233=%20%20We%20analyse%20preference%20inference%2C%20through%20consistency%2C%20for%20general%20preference%0Alanguages%20based%20on%20lexicographic%20models.%20We%20identify%20a%20property%2C%20which%20we%20call%0Astrong%20compositionality%2C%20that%20applies%20for%20many%20natural%20kinds%20of%20preference%0Astatement%2C%20and%20that%20allows%20a%20greedy%20algorithm%20for%20determining%20consistency%20of%20a%0Aset%20of%20preference%20statements.%20We%20also%20consider%20different%20natural%20definitions%20of%0Aoptimality%2C%20and%20their%20relations%20to%20each%20other%2C%20for%20general%20preference%20languages%0Abased%20on%20lexicographic%20models.%20Based%20on%20our%20framework%2C%20we%20show%20that%20testing%0Aconsistency%2C%20and%20thus%20inference%2C%20is%20polynomial%20for%20a%20specific%20preference%0Alanguage%20LpqT%2C%20which%20allows%20strict%20and%20non-strict%20statements%2C%20comparisons%0Abetween%20outcomes%20and%20between%20partial%20tuples%2C%20both%20ceteris%20paribus%20and%20strong%0Astatements%2C%20and%20their%20combination.%20Computing%20different%20kinds%20of%20optimal%20sets%20is%0Aalso%20shown%20to%20be%20polynomial%3B%20this%20is%20backed%20up%20by%20our%20experimental%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23913v1&entry.124074799=Read"},
{"title": "Improving snore detection under limited dataset through\n  harmonic/percussive source separation and convolutional neural networks", "author": "F. D. Gonzalez-Martinez and J. J. Carabias-Orti and F. J. Canadas-Quesada and N. Ruiz-Reyes and D. Martinez-Munoz and S. Garcia-Galan", "abstract": "  Snoring, an acoustic biomarker commonly observed in individuals with\nObstructive Sleep Apnoea Syndrome (OSAS), holds significant potential for\ndiagnosing and monitoring this recognized clinical disorder. Irrespective of\nsnoring types, most snoring instances exhibit identifiable harmonic patterns\nmanifested through distinctive energy distributions over time. In this work, we\npropose a novel method to differentiate monaural snoring from non-snoring\nsounds by analyzing the harmonic content of the input sound using\nharmonic/percussive sound source separation (HPSS). The resulting feature,\nbased on the harmonic spectrogram from HPSS, is employed as input data for\nconventional neural network architectures, aiming to enhance snoring detection\nperformance even under a limited data learning framework. To evaluate the\nperformance of our proposal, we studied two different scenarios: 1) using a\nlarge dataset of snoring and interfering sounds, and 2) using a reduced\ntraining set composed of around 1% of the data material. In the former\nscenario, the proposed HPSS-based feature provides competitive results compared\nto other input features from the literature. However, the key advantage of the\nproposed method lies in the superior performance of the harmonic spectrogram\nderived from HPSS in a limited data learning context. In this particular\nscenario, using the proposed harmonic feature significantly enhances the\nperformance of all the studied architectures in comparison to the classical\ninput features documented in the existing literature. This finding clearly\ndemonstrates that incorporating harmonic content enables more reliable learning\nof the essential time-frequency characteristics that are prevalent in most\nsnoring sounds, even in scenarios where the amount of training data is limited.\n", "link": "http://arxiv.org/abs/2410.23796v1", "date": "2024-10-31", "relevancy": 2.22, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4698}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4397}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20snore%20detection%20under%20limited%20dataset%20through%0A%20%20harmonic/percussive%20source%20separation%20and%20convolutional%20neural%20networks&body=Title%3A%20Improving%20snore%20detection%20under%20limited%20dataset%20through%0A%20%20harmonic/percussive%20source%20separation%20and%20convolutional%20neural%20networks%0AAuthor%3A%20F.%20D.%20Gonzalez-Martinez%20and%20J.%20J.%20Carabias-Orti%20and%20F.%20J.%20Canadas-Quesada%20and%20N.%20Ruiz-Reyes%20and%20D.%20Martinez-Munoz%20and%20S.%20Garcia-Galan%0AAbstract%3A%20%20%20Snoring%2C%20an%20acoustic%20biomarker%20commonly%20observed%20in%20individuals%20with%0AObstructive%20Sleep%20Apnoea%20Syndrome%20%28OSAS%29%2C%20holds%20significant%20potential%20for%0Adiagnosing%20and%20monitoring%20this%20recognized%20clinical%20disorder.%20Irrespective%20of%0Asnoring%20types%2C%20most%20snoring%20instances%20exhibit%20identifiable%20harmonic%20patterns%0Amanifested%20through%20distinctive%20energy%20distributions%20over%20time.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20method%20to%20differentiate%20monaural%20snoring%20from%20non-snoring%0Asounds%20by%20analyzing%20the%20harmonic%20content%20of%20the%20input%20sound%20using%0Aharmonic/percussive%20sound%20source%20separation%20%28HPSS%29.%20The%20resulting%20feature%2C%0Abased%20on%20the%20harmonic%20spectrogram%20from%20HPSS%2C%20is%20employed%20as%20input%20data%20for%0Aconventional%20neural%20network%20architectures%2C%20aiming%20to%20enhance%20snoring%20detection%0Aperformance%20even%20under%20a%20limited%20data%20learning%20framework.%20To%20evaluate%20the%0Aperformance%20of%20our%20proposal%2C%20we%20studied%20two%20different%20scenarios%3A%201%29%20using%20a%0Alarge%20dataset%20of%20snoring%20and%20interfering%20sounds%2C%20and%202%29%20using%20a%20reduced%0Atraining%20set%20composed%20of%20around%201%25%20of%20the%20data%20material.%20In%20the%20former%0Ascenario%2C%20the%20proposed%20HPSS-based%20feature%20provides%20competitive%20results%20compared%0Ato%20other%20input%20features%20from%20the%20literature.%20However%2C%20the%20key%20advantage%20of%20the%0Aproposed%20method%20lies%20in%20the%20superior%20performance%20of%20the%20harmonic%20spectrogram%0Aderived%20from%20HPSS%20in%20a%20limited%20data%20learning%20context.%20In%20this%20particular%0Ascenario%2C%20using%20the%20proposed%20harmonic%20feature%20significantly%20enhances%20the%0Aperformance%20of%20all%20the%20studied%20architectures%20in%20comparison%20to%20the%20classical%0Ainput%20features%20documented%20in%20the%20existing%20literature.%20This%20finding%20clearly%0Ademonstrates%20that%20incorporating%20harmonic%20content%20enables%20more%20reliable%20learning%0Aof%20the%20essential%20time-frequency%20characteristics%20that%20are%20prevalent%20in%20most%0Asnoring%20sounds%2C%20even%20in%20scenarios%20where%20the%20amount%20of%20training%20data%20is%20limited.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520snore%2520detection%2520under%2520limited%2520dataset%2520through%250A%2520%2520harmonic/percussive%2520source%2520separation%2520and%2520convolutional%2520neural%2520networks%26entry.906535625%3DF.%2520D.%2520Gonzalez-Martinez%2520and%2520J.%2520J.%2520Carabias-Orti%2520and%2520F.%2520J.%2520Canadas-Quesada%2520and%2520N.%2520Ruiz-Reyes%2520and%2520D.%2520Martinez-Munoz%2520and%2520S.%2520Garcia-Galan%26entry.1292438233%3D%2520%2520Snoring%252C%2520an%2520acoustic%2520biomarker%2520commonly%2520observed%2520in%2520individuals%2520with%250AObstructive%2520Sleep%2520Apnoea%2520Syndrome%2520%2528OSAS%2529%252C%2520holds%2520significant%2520potential%2520for%250Adiagnosing%2520and%2520monitoring%2520this%2520recognized%2520clinical%2520disorder.%2520Irrespective%2520of%250Asnoring%2520types%252C%2520most%2520snoring%2520instances%2520exhibit%2520identifiable%2520harmonic%2520patterns%250Amanifested%2520through%2520distinctive%2520energy%2520distributions%2520over%2520time.%2520In%2520this%2520work%252C%2520we%250Apropose%2520a%2520novel%2520method%2520to%2520differentiate%2520monaural%2520snoring%2520from%2520non-snoring%250Asounds%2520by%2520analyzing%2520the%2520harmonic%2520content%2520of%2520the%2520input%2520sound%2520using%250Aharmonic/percussive%2520sound%2520source%2520separation%2520%2528HPSS%2529.%2520The%2520resulting%2520feature%252C%250Abased%2520on%2520the%2520harmonic%2520spectrogram%2520from%2520HPSS%252C%2520is%2520employed%2520as%2520input%2520data%2520for%250Aconventional%2520neural%2520network%2520architectures%252C%2520aiming%2520to%2520enhance%2520snoring%2520detection%250Aperformance%2520even%2520under%2520a%2520limited%2520data%2520learning%2520framework.%2520To%2520evaluate%2520the%250Aperformance%2520of%2520our%2520proposal%252C%2520we%2520studied%2520two%2520different%2520scenarios%253A%25201%2529%2520using%2520a%250Alarge%2520dataset%2520of%2520snoring%2520and%2520interfering%2520sounds%252C%2520and%25202%2529%2520using%2520a%2520reduced%250Atraining%2520set%2520composed%2520of%2520around%25201%2525%2520of%2520the%2520data%2520material.%2520In%2520the%2520former%250Ascenario%252C%2520the%2520proposed%2520HPSS-based%2520feature%2520provides%2520competitive%2520results%2520compared%250Ato%2520other%2520input%2520features%2520from%2520the%2520literature.%2520However%252C%2520the%2520key%2520advantage%2520of%2520the%250Aproposed%2520method%2520lies%2520in%2520the%2520superior%2520performance%2520of%2520the%2520harmonic%2520spectrogram%250Aderived%2520from%2520HPSS%2520in%2520a%2520limited%2520data%2520learning%2520context.%2520In%2520this%2520particular%250Ascenario%252C%2520using%2520the%2520proposed%2520harmonic%2520feature%2520significantly%2520enhances%2520the%250Aperformance%2520of%2520all%2520the%2520studied%2520architectures%2520in%2520comparison%2520to%2520the%2520classical%250Ainput%2520features%2520documented%2520in%2520the%2520existing%2520literature.%2520This%2520finding%2520clearly%250Ademonstrates%2520that%2520incorporating%2520harmonic%2520content%2520enables%2520more%2520reliable%2520learning%250Aof%2520the%2520essential%2520time-frequency%2520characteristics%2520that%2520are%2520prevalent%2520in%2520most%250Asnoring%2520sounds%252C%2520even%2520in%2520scenarios%2520where%2520the%2520amount%2520of%2520training%2520data%2520is%2520limited.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20snore%20detection%20under%20limited%20dataset%20through%0A%20%20harmonic/percussive%20source%20separation%20and%20convolutional%20neural%20networks&entry.906535625=F.%20D.%20Gonzalez-Martinez%20and%20J.%20J.%20Carabias-Orti%20and%20F.%20J.%20Canadas-Quesada%20and%20N.%20Ruiz-Reyes%20and%20D.%20Martinez-Munoz%20and%20S.%20Garcia-Galan&entry.1292438233=%20%20Snoring%2C%20an%20acoustic%20biomarker%20commonly%20observed%20in%20individuals%20with%0AObstructive%20Sleep%20Apnoea%20Syndrome%20%28OSAS%29%2C%20holds%20significant%20potential%20for%0Adiagnosing%20and%20monitoring%20this%20recognized%20clinical%20disorder.%20Irrespective%20of%0Asnoring%20types%2C%20most%20snoring%20instances%20exhibit%20identifiable%20harmonic%20patterns%0Amanifested%20through%20distinctive%20energy%20distributions%20over%20time.%20In%20this%20work%2C%20we%0Apropose%20a%20novel%20method%20to%20differentiate%20monaural%20snoring%20from%20non-snoring%0Asounds%20by%20analyzing%20the%20harmonic%20content%20of%20the%20input%20sound%20using%0Aharmonic/percussive%20sound%20source%20separation%20%28HPSS%29.%20The%20resulting%20feature%2C%0Abased%20on%20the%20harmonic%20spectrogram%20from%20HPSS%2C%20is%20employed%20as%20input%20data%20for%0Aconventional%20neural%20network%20architectures%2C%20aiming%20to%20enhance%20snoring%20detection%0Aperformance%20even%20under%20a%20limited%20data%20learning%20framework.%20To%20evaluate%20the%0Aperformance%20of%20our%20proposal%2C%20we%20studied%20two%20different%20scenarios%3A%201%29%20using%20a%0Alarge%20dataset%20of%20snoring%20and%20interfering%20sounds%2C%20and%202%29%20using%20a%20reduced%0Atraining%20set%20composed%20of%20around%201%25%20of%20the%20data%20material.%20In%20the%20former%0Ascenario%2C%20the%20proposed%20HPSS-based%20feature%20provides%20competitive%20results%20compared%0Ato%20other%20input%20features%20from%20the%20literature.%20However%2C%20the%20key%20advantage%20of%20the%0Aproposed%20method%20lies%20in%20the%20superior%20performance%20of%20the%20harmonic%20spectrogram%0Aderived%20from%20HPSS%20in%20a%20limited%20data%20learning%20context.%20In%20this%20particular%0Ascenario%2C%20using%20the%20proposed%20harmonic%20feature%20significantly%20enhances%20the%0Aperformance%20of%20all%20the%20studied%20architectures%20in%20comparison%20to%20the%20classical%0Ainput%20features%20documented%20in%20the%20existing%20literature.%20This%20finding%20clearly%0Ademonstrates%20that%20incorporating%20harmonic%20content%20enables%20more%20reliable%20learning%0Aof%20the%20essential%20time-frequency%20characteristics%20that%20are%20prevalent%20in%20most%0Asnoring%20sounds%2C%20even%20in%20scenarios%20where%20the%20amount%20of%20training%20data%20is%20limited.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23796v1&entry.124074799=Read"},
{"title": "Video Token Merging for Long-form Video Understanding", "author": "Seon-Ho Lee and Jue Wang and Zhikang Zhang and David Fan and Xinyu Li", "abstract": "  As the scale of data and models for video understanding rapidly expand,\nhandling long-form video input in transformer-based models presents a practical\nchallenge. Rather than resorting to input sampling or token dropping, which may\nresult in information loss, token merging shows promising results when used in\ncollaboration with transformers. However, the application of token merging for\nlong-form video processing is not trivial. We begin with the premise that token\nmerging should not rely solely on the similarity of video tokens; the saliency\nof tokens should also be considered. To address this, we explore various video\ntoken merging strategies for long-form video classification, starting with a\nsimple extension of image token merging, moving to region-concentrated merging,\nand finally proposing a learnable video token merging (VTM) algorithm that\ndynamically merges tokens based on their saliency. Extensive experimental\nresults show that we achieve better or comparable performances on the LVU,\nCOIN, and Breakfast datasets. Moreover, our approach significantly reduces\nmemory costs by 84% and boosts throughput by approximately 6.89 times compared\nto baseline algorithms.\n", "link": "http://arxiv.org/abs/2410.23782v1", "date": "2024-10-31", "relevancy": 2.1973, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5617}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5412}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Video%20Token%20Merging%20for%20Long-form%20Video%20Understanding&body=Title%3A%20Video%20Token%20Merging%20for%20Long-form%20Video%20Understanding%0AAuthor%3A%20Seon-Ho%20Lee%20and%20Jue%20Wang%20and%20Zhikang%20Zhang%20and%20David%20Fan%20and%20Xinyu%20Li%0AAbstract%3A%20%20%20As%20the%20scale%20of%20data%20and%20models%20for%20video%20understanding%20rapidly%20expand%2C%0Ahandling%20long-form%20video%20input%20in%20transformer-based%20models%20presents%20a%20practical%0Achallenge.%20Rather%20than%20resorting%20to%20input%20sampling%20or%20token%20dropping%2C%20which%20may%0Aresult%20in%20information%20loss%2C%20token%20merging%20shows%20promising%20results%20when%20used%20in%0Acollaboration%20with%20transformers.%20However%2C%20the%20application%20of%20token%20merging%20for%0Along-form%20video%20processing%20is%20not%20trivial.%20We%20begin%20with%20the%20premise%20that%20token%0Amerging%20should%20not%20rely%20solely%20on%20the%20similarity%20of%20video%20tokens%3B%20the%20saliency%0Aof%20tokens%20should%20also%20be%20considered.%20To%20address%20this%2C%20we%20explore%20various%20video%0Atoken%20merging%20strategies%20for%20long-form%20video%20classification%2C%20starting%20with%20a%0Asimple%20extension%20of%20image%20token%20merging%2C%20moving%20to%20region-concentrated%20merging%2C%0Aand%20finally%20proposing%20a%20learnable%20video%20token%20merging%20%28VTM%29%20algorithm%20that%0Adynamically%20merges%20tokens%20based%20on%20their%20saliency.%20Extensive%20experimental%0Aresults%20show%20that%20we%20achieve%20better%20or%20comparable%20performances%20on%20the%20LVU%2C%0ACOIN%2C%20and%20Breakfast%20datasets.%20Moreover%2C%20our%20approach%20significantly%20reduces%0Amemory%20costs%20by%2084%25%20and%20boosts%20throughput%20by%20approximately%206.89%20times%20compared%0Ato%20baseline%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23782v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideo%2520Token%2520Merging%2520for%2520Long-form%2520Video%2520Understanding%26entry.906535625%3DSeon-Ho%2520Lee%2520and%2520Jue%2520Wang%2520and%2520Zhikang%2520Zhang%2520and%2520David%2520Fan%2520and%2520Xinyu%2520Li%26entry.1292438233%3D%2520%2520As%2520the%2520scale%2520of%2520data%2520and%2520models%2520for%2520video%2520understanding%2520rapidly%2520expand%252C%250Ahandling%2520long-form%2520video%2520input%2520in%2520transformer-based%2520models%2520presents%2520a%2520practical%250Achallenge.%2520Rather%2520than%2520resorting%2520to%2520input%2520sampling%2520or%2520token%2520dropping%252C%2520which%2520may%250Aresult%2520in%2520information%2520loss%252C%2520token%2520merging%2520shows%2520promising%2520results%2520when%2520used%2520in%250Acollaboration%2520with%2520transformers.%2520However%252C%2520the%2520application%2520of%2520token%2520merging%2520for%250Along-form%2520video%2520processing%2520is%2520not%2520trivial.%2520We%2520begin%2520with%2520the%2520premise%2520that%2520token%250Amerging%2520should%2520not%2520rely%2520solely%2520on%2520the%2520similarity%2520of%2520video%2520tokens%253B%2520the%2520saliency%250Aof%2520tokens%2520should%2520also%2520be%2520considered.%2520To%2520address%2520this%252C%2520we%2520explore%2520various%2520video%250Atoken%2520merging%2520strategies%2520for%2520long-form%2520video%2520classification%252C%2520starting%2520with%2520a%250Asimple%2520extension%2520of%2520image%2520token%2520merging%252C%2520moving%2520to%2520region-concentrated%2520merging%252C%250Aand%2520finally%2520proposing%2520a%2520learnable%2520video%2520token%2520merging%2520%2528VTM%2529%2520algorithm%2520that%250Adynamically%2520merges%2520tokens%2520based%2520on%2520their%2520saliency.%2520Extensive%2520experimental%250Aresults%2520show%2520that%2520we%2520achieve%2520better%2520or%2520comparable%2520performances%2520on%2520the%2520LVU%252C%250ACOIN%252C%2520and%2520Breakfast%2520datasets.%2520Moreover%252C%2520our%2520approach%2520significantly%2520reduces%250Amemory%2520costs%2520by%252084%2525%2520and%2520boosts%2520throughput%2520by%2520approximately%25206.89%2520times%2520compared%250Ato%2520baseline%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23782v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Video%20Token%20Merging%20for%20Long-form%20Video%20Understanding&entry.906535625=Seon-Ho%20Lee%20and%20Jue%20Wang%20and%20Zhikang%20Zhang%20and%20David%20Fan%20and%20Xinyu%20Li&entry.1292438233=%20%20As%20the%20scale%20of%20data%20and%20models%20for%20video%20understanding%20rapidly%20expand%2C%0Ahandling%20long-form%20video%20input%20in%20transformer-based%20models%20presents%20a%20practical%0Achallenge.%20Rather%20than%20resorting%20to%20input%20sampling%20or%20token%20dropping%2C%20which%20may%0Aresult%20in%20information%20loss%2C%20token%20merging%20shows%20promising%20results%20when%20used%20in%0Acollaboration%20with%20transformers.%20However%2C%20the%20application%20of%20token%20merging%20for%0Along-form%20video%20processing%20is%20not%20trivial.%20We%20begin%20with%20the%20premise%20that%20token%0Amerging%20should%20not%20rely%20solely%20on%20the%20similarity%20of%20video%20tokens%3B%20the%20saliency%0Aof%20tokens%20should%20also%20be%20considered.%20To%20address%20this%2C%20we%20explore%20various%20video%0Atoken%20merging%20strategies%20for%20long-form%20video%20classification%2C%20starting%20with%20a%0Asimple%20extension%20of%20image%20token%20merging%2C%20moving%20to%20region-concentrated%20merging%2C%0Aand%20finally%20proposing%20a%20learnable%20video%20token%20merging%20%28VTM%29%20algorithm%20that%0Adynamically%20merges%20tokens%20based%20on%20their%20saliency.%20Extensive%20experimental%0Aresults%20show%20that%20we%20achieve%20better%20or%20comparable%20performances%20on%20the%20LVU%2C%0ACOIN%2C%20and%20Breakfast%20datasets.%20Moreover%2C%20our%20approach%20significantly%20reduces%0Amemory%20costs%20by%2084%25%20and%20boosts%20throughput%20by%20approximately%206.89%20times%20compared%0Ato%20baseline%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23782v1&entry.124074799=Read"},
{"title": "Deep Learning Frameworks for Cognitive Radio Networks: Review and Open\n  Research Challenges", "author": "Senthil Kumar Jagatheesaperumal and Ijaz Ahmad and Marko H\u00f6yhty\u00e4 and Suleman Khan and Andrei Gurtov", "abstract": "  Deep learning has been proven to be a powerful tool for addressing the most\nsignificant issues in cognitive radio networks, such as spectrum sensing,\nspectrum sharing, resource allocation, and security attacks. The utilization of\ndeep learning techniques in cognitive radio networks can significantly enhance\nthe network's capability to adapt to changing environments and improve the\noverall system's efficiency and reliability. As the demand for higher data\nrates and connectivity increases, B5G/6G wireless networks are expected to\nenable new services and applications significantly. Therefore, the significance\nof deep learning in addressing cognitive radio network challenges cannot be\noverstated. This review article provides valuable insights into potential\nsolutions that can serve as a foundation for the development of future B5G/6G\nservices. By leveraging the power of deep learning, cognitive radio networks\ncan pave the way for the next generation of wireless networks capable of\nmeeting the ever-increasing demands for higher data rates, improved\nreliability, and security.\n", "link": "http://arxiv.org/abs/2410.23949v1", "date": "2024-10-31", "relevancy": 2.1939, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4406}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4378}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4378}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20Frameworks%20for%20Cognitive%20Radio%20Networks%3A%20Review%20and%20Open%0A%20%20Research%20Challenges&body=Title%3A%20Deep%20Learning%20Frameworks%20for%20Cognitive%20Radio%20Networks%3A%20Review%20and%20Open%0A%20%20Research%20Challenges%0AAuthor%3A%20Senthil%20Kumar%20Jagatheesaperumal%20and%20Ijaz%20Ahmad%20and%20Marko%20H%C3%B6yhty%C3%A4%20and%20Suleman%20Khan%20and%20Andrei%20Gurtov%0AAbstract%3A%20%20%20Deep%20learning%20has%20been%20proven%20to%20be%20a%20powerful%20tool%20for%20addressing%20the%20most%0Asignificant%20issues%20in%20cognitive%20radio%20networks%2C%20such%20as%20spectrum%20sensing%2C%0Aspectrum%20sharing%2C%20resource%20allocation%2C%20and%20security%20attacks.%20The%20utilization%20of%0Adeep%20learning%20techniques%20in%20cognitive%20radio%20networks%20can%20significantly%20enhance%0Athe%20network%27s%20capability%20to%20adapt%20to%20changing%20environments%20and%20improve%20the%0Aoverall%20system%27s%20efficiency%20and%20reliability.%20As%20the%20demand%20for%20higher%20data%0Arates%20and%20connectivity%20increases%2C%20B5G/6G%20wireless%20networks%20are%20expected%20to%0Aenable%20new%20services%20and%20applications%20significantly.%20Therefore%2C%20the%20significance%0Aof%20deep%20learning%20in%20addressing%20cognitive%20radio%20network%20challenges%20cannot%20be%0Aoverstated.%20This%20review%20article%20provides%20valuable%20insights%20into%20potential%0Asolutions%20that%20can%20serve%20as%20a%20foundation%20for%20the%20development%20of%20future%20B5G/6G%0Aservices.%20By%20leveraging%20the%20power%20of%20deep%20learning%2C%20cognitive%20radio%20networks%0Acan%20pave%20the%20way%20for%20the%20next%20generation%20of%20wireless%20networks%20capable%20of%0Ameeting%20the%20ever-increasing%20demands%20for%20higher%20data%20rates%2C%20improved%0Areliability%2C%20and%20security.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23949v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520Frameworks%2520for%2520Cognitive%2520Radio%2520Networks%253A%2520Review%2520and%2520Open%250A%2520%2520Research%2520Challenges%26entry.906535625%3DSenthil%2520Kumar%2520Jagatheesaperumal%2520and%2520Ijaz%2520Ahmad%2520and%2520Marko%2520H%25C3%25B6yhty%25C3%25A4%2520and%2520Suleman%2520Khan%2520and%2520Andrei%2520Gurtov%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520been%2520proven%2520to%2520be%2520a%2520powerful%2520tool%2520for%2520addressing%2520the%2520most%250Asignificant%2520issues%2520in%2520cognitive%2520radio%2520networks%252C%2520such%2520as%2520spectrum%2520sensing%252C%250Aspectrum%2520sharing%252C%2520resource%2520allocation%252C%2520and%2520security%2520attacks.%2520The%2520utilization%2520of%250Adeep%2520learning%2520techniques%2520in%2520cognitive%2520radio%2520networks%2520can%2520significantly%2520enhance%250Athe%2520network%2527s%2520capability%2520to%2520adapt%2520to%2520changing%2520environments%2520and%2520improve%2520the%250Aoverall%2520system%2527s%2520efficiency%2520and%2520reliability.%2520As%2520the%2520demand%2520for%2520higher%2520data%250Arates%2520and%2520connectivity%2520increases%252C%2520B5G/6G%2520wireless%2520networks%2520are%2520expected%2520to%250Aenable%2520new%2520services%2520and%2520applications%2520significantly.%2520Therefore%252C%2520the%2520significance%250Aof%2520deep%2520learning%2520in%2520addressing%2520cognitive%2520radio%2520network%2520challenges%2520cannot%2520be%250Aoverstated.%2520This%2520review%2520article%2520provides%2520valuable%2520insights%2520into%2520potential%250Asolutions%2520that%2520can%2520serve%2520as%2520a%2520foundation%2520for%2520the%2520development%2520of%2520future%2520B5G/6G%250Aservices.%2520By%2520leveraging%2520the%2520power%2520of%2520deep%2520learning%252C%2520cognitive%2520radio%2520networks%250Acan%2520pave%2520the%2520way%2520for%2520the%2520next%2520generation%2520of%2520wireless%2520networks%2520capable%2520of%250Ameeting%2520the%2520ever-increasing%2520demands%2520for%2520higher%2520data%2520rates%252C%2520improved%250Areliability%252C%2520and%2520security.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23949v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Frameworks%20for%20Cognitive%20Radio%20Networks%3A%20Review%20and%20Open%0A%20%20Research%20Challenges&entry.906535625=Senthil%20Kumar%20Jagatheesaperumal%20and%20Ijaz%20Ahmad%20and%20Marko%20H%C3%B6yhty%C3%A4%20and%20Suleman%20Khan%20and%20Andrei%20Gurtov&entry.1292438233=%20%20Deep%20learning%20has%20been%20proven%20to%20be%20a%20powerful%20tool%20for%20addressing%20the%20most%0Asignificant%20issues%20in%20cognitive%20radio%20networks%2C%20such%20as%20spectrum%20sensing%2C%0Aspectrum%20sharing%2C%20resource%20allocation%2C%20and%20security%20attacks.%20The%20utilization%20of%0Adeep%20learning%20techniques%20in%20cognitive%20radio%20networks%20can%20significantly%20enhance%0Athe%20network%27s%20capability%20to%20adapt%20to%20changing%20environments%20and%20improve%20the%0Aoverall%20system%27s%20efficiency%20and%20reliability.%20As%20the%20demand%20for%20higher%20data%0Arates%20and%20connectivity%20increases%2C%20B5G/6G%20wireless%20networks%20are%20expected%20to%0Aenable%20new%20services%20and%20applications%20significantly.%20Therefore%2C%20the%20significance%0Aof%20deep%20learning%20in%20addressing%20cognitive%20radio%20network%20challenges%20cannot%20be%0Aoverstated.%20This%20review%20article%20provides%20valuable%20insights%20into%20potential%0Asolutions%20that%20can%20serve%20as%20a%20foundation%20for%20the%20development%20of%20future%20B5G/6G%0Aservices.%20By%20leveraging%20the%20power%20of%20deep%20learning%2C%20cognitive%20radio%20networks%0Acan%20pave%20the%20way%20for%20the%20next%20generation%20of%20wireless%20networks%20capable%20of%0Ameeting%20the%20ever-increasing%20demands%20for%20higher%20data%20rates%2C%20improved%0Areliability%2C%20and%20security.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23949v1&entry.124074799=Read"},
{"title": "Repository-Level Compositional Code Translation and Validation", "author": "Ali Reza Ibrahimzada and Kaiyao Ke and Mrigank Pawagi and Muhammad Salman Abid and Rangeet Pan and Saurabh Sinha and Reyhaneh Jabbarvand", "abstract": "  Code translation transforms programs from one programming language (PL) to\nanother. Several rule-based transpilers have been designed to automate code\ntranslation between different pairs of PLs. However, the rules can become\nobsolete as the PLs evolve and cannot generalize to other PLs. Recent studies\nhave explored the automation of code translation using Large Language Models\n(LLMs). One key observation is that such techniques may work well for crafted\nbenchmarks but fail to generalize to the scale and complexity of real-world\nprojects with dependencies, custom types, PL-specific features, etc.\n  We propose AlphaTrans, a neuro-symbolic approach to automate repository-level\ncode translation. AlphaTrans translates both source and test code, and employs\nmultiple levels of validation to ensure the translation preserves the\nfunctionality of the source program. To break down the problem for LLMs,\nAlphaTrans leverages program analysis to decompose the program into fragments\nand translates them in the reverse call order. We leveraged AlphaTrans to\ntranslate ten real-world open-source projects consisting of <836, 8575, 2719>\nclasses, methods, and tests. AlphaTrans translated the entire repository of\nthese projects consisting of 6899 source code fragments. 99.1% of the\ntranslated code fragments are syntactically correct, and AlphaTrans validates\nthe translations' runtime behavior and functional correctness for 25.8%. On\naverage, the integrated translation and validation take 36 hours to translate a\nproject, showing its scalability in practice. For the syntactically or\nsemantically incorrect translations, AlphaTrans generates a report including\nexisting translation, stack trace, test errors, or assertion failures. We\nprovided these artifacts to two developers to fix the translation bugs in four\nprojects. They were able to fix the issues in 20.1 hours on average and achieve\nall passing tests.\n", "link": "http://arxiv.org/abs/2410.24117v1", "date": "2024-10-31", "relevancy": 2.1907, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4459}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Repository-Level%20Compositional%20Code%20Translation%20and%20Validation&body=Title%3A%20Repository-Level%20Compositional%20Code%20Translation%20and%20Validation%0AAuthor%3A%20Ali%20Reza%20Ibrahimzada%20and%20Kaiyao%20Ke%20and%20Mrigank%20Pawagi%20and%20Muhammad%20Salman%20Abid%20and%20Rangeet%20Pan%20and%20Saurabh%20Sinha%20and%20Reyhaneh%20Jabbarvand%0AAbstract%3A%20%20%20Code%20translation%20transforms%20programs%20from%20one%20programming%20language%20%28PL%29%20to%0Aanother.%20Several%20rule-based%20transpilers%20have%20been%20designed%20to%20automate%20code%0Atranslation%20between%20different%20pairs%20of%20PLs.%20However%2C%20the%20rules%20can%20become%0Aobsolete%20as%20the%20PLs%20evolve%20and%20cannot%20generalize%20to%20other%20PLs.%20Recent%20studies%0Ahave%20explored%20the%20automation%20of%20code%20translation%20using%20Large%20Language%20Models%0A%28LLMs%29.%20One%20key%20observation%20is%20that%20such%20techniques%20may%20work%20well%20for%20crafted%0Abenchmarks%20but%20fail%20to%20generalize%20to%20the%20scale%20and%20complexity%20of%20real-world%0Aprojects%20with%20dependencies%2C%20custom%20types%2C%20PL-specific%20features%2C%20etc.%0A%20%20We%20propose%20AlphaTrans%2C%20a%20neuro-symbolic%20approach%20to%20automate%20repository-level%0Acode%20translation.%20AlphaTrans%20translates%20both%20source%20and%20test%20code%2C%20and%20employs%0Amultiple%20levels%20of%20validation%20to%20ensure%20the%20translation%20preserves%20the%0Afunctionality%20of%20the%20source%20program.%20To%20break%20down%20the%20problem%20for%20LLMs%2C%0AAlphaTrans%20leverages%20program%20analysis%20to%20decompose%20the%20program%20into%20fragments%0Aand%20translates%20them%20in%20the%20reverse%20call%20order.%20We%20leveraged%20AlphaTrans%20to%0Atranslate%20ten%20real-world%20open-source%20projects%20consisting%20of%20%3C836%2C%208575%2C%202719%3E%0Aclasses%2C%20methods%2C%20and%20tests.%20AlphaTrans%20translated%20the%20entire%20repository%20of%0Athese%20projects%20consisting%20of%206899%20source%20code%20fragments.%2099.1%25%20of%20the%0Atranslated%20code%20fragments%20are%20syntactically%20correct%2C%20and%20AlphaTrans%20validates%0Athe%20translations%27%20runtime%20behavior%20and%20functional%20correctness%20for%2025.8%25.%20On%0Aaverage%2C%20the%20integrated%20translation%20and%20validation%20take%2036%20hours%20to%20translate%20a%0Aproject%2C%20showing%20its%20scalability%20in%20practice.%20For%20the%20syntactically%20or%0Asemantically%20incorrect%20translations%2C%20AlphaTrans%20generates%20a%20report%20including%0Aexisting%20translation%2C%20stack%20trace%2C%20test%20errors%2C%20or%20assertion%20failures.%20We%0Aprovided%20these%20artifacts%20to%20two%20developers%20to%20fix%20the%20translation%20bugs%20in%20four%0Aprojects.%20They%20were%20able%20to%20fix%20the%20issues%20in%2020.1%20hours%20on%20average%20and%20achieve%0Aall%20passing%20tests.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepository-Level%2520Compositional%2520Code%2520Translation%2520and%2520Validation%26entry.906535625%3DAli%2520Reza%2520Ibrahimzada%2520and%2520Kaiyao%2520Ke%2520and%2520Mrigank%2520Pawagi%2520and%2520Muhammad%2520Salman%2520Abid%2520and%2520Rangeet%2520Pan%2520and%2520Saurabh%2520Sinha%2520and%2520Reyhaneh%2520Jabbarvand%26entry.1292438233%3D%2520%2520Code%2520translation%2520transforms%2520programs%2520from%2520one%2520programming%2520language%2520%2528PL%2529%2520to%250Aanother.%2520Several%2520rule-based%2520transpilers%2520have%2520been%2520designed%2520to%2520automate%2520code%250Atranslation%2520between%2520different%2520pairs%2520of%2520PLs.%2520However%252C%2520the%2520rules%2520can%2520become%250Aobsolete%2520as%2520the%2520PLs%2520evolve%2520and%2520cannot%2520generalize%2520to%2520other%2520PLs.%2520Recent%2520studies%250Ahave%2520explored%2520the%2520automation%2520of%2520code%2520translation%2520using%2520Large%2520Language%2520Models%250A%2528LLMs%2529.%2520One%2520key%2520observation%2520is%2520that%2520such%2520techniques%2520may%2520work%2520well%2520for%2520crafted%250Abenchmarks%2520but%2520fail%2520to%2520generalize%2520to%2520the%2520scale%2520and%2520complexity%2520of%2520real-world%250Aprojects%2520with%2520dependencies%252C%2520custom%2520types%252C%2520PL-specific%2520features%252C%2520etc.%250A%2520%2520We%2520propose%2520AlphaTrans%252C%2520a%2520neuro-symbolic%2520approach%2520to%2520automate%2520repository-level%250Acode%2520translation.%2520AlphaTrans%2520translates%2520both%2520source%2520and%2520test%2520code%252C%2520and%2520employs%250Amultiple%2520levels%2520of%2520validation%2520to%2520ensure%2520the%2520translation%2520preserves%2520the%250Afunctionality%2520of%2520the%2520source%2520program.%2520To%2520break%2520down%2520the%2520problem%2520for%2520LLMs%252C%250AAlphaTrans%2520leverages%2520program%2520analysis%2520to%2520decompose%2520the%2520program%2520into%2520fragments%250Aand%2520translates%2520them%2520in%2520the%2520reverse%2520call%2520order.%2520We%2520leveraged%2520AlphaTrans%2520to%250Atranslate%2520ten%2520real-world%2520open-source%2520projects%2520consisting%2520of%2520%253C836%252C%25208575%252C%25202719%253E%250Aclasses%252C%2520methods%252C%2520and%2520tests.%2520AlphaTrans%2520translated%2520the%2520entire%2520repository%2520of%250Athese%2520projects%2520consisting%2520of%25206899%2520source%2520code%2520fragments.%252099.1%2525%2520of%2520the%250Atranslated%2520code%2520fragments%2520are%2520syntactically%2520correct%252C%2520and%2520AlphaTrans%2520validates%250Athe%2520translations%2527%2520runtime%2520behavior%2520and%2520functional%2520correctness%2520for%252025.8%2525.%2520On%250Aaverage%252C%2520the%2520integrated%2520translation%2520and%2520validation%2520take%252036%2520hours%2520to%2520translate%2520a%250Aproject%252C%2520showing%2520its%2520scalability%2520in%2520practice.%2520For%2520the%2520syntactically%2520or%250Asemantically%2520incorrect%2520translations%252C%2520AlphaTrans%2520generates%2520a%2520report%2520including%250Aexisting%2520translation%252C%2520stack%2520trace%252C%2520test%2520errors%252C%2520or%2520assertion%2520failures.%2520We%250Aprovided%2520these%2520artifacts%2520to%2520two%2520developers%2520to%2520fix%2520the%2520translation%2520bugs%2520in%2520four%250Aprojects.%2520They%2520were%2520able%2520to%2520fix%2520the%2520issues%2520in%252020.1%2520hours%2520on%2520average%2520and%2520achieve%250Aall%2520passing%2520tests.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Repository-Level%20Compositional%20Code%20Translation%20and%20Validation&entry.906535625=Ali%20Reza%20Ibrahimzada%20and%20Kaiyao%20Ke%20and%20Mrigank%20Pawagi%20and%20Muhammad%20Salman%20Abid%20and%20Rangeet%20Pan%20and%20Saurabh%20Sinha%20and%20Reyhaneh%20Jabbarvand&entry.1292438233=%20%20Code%20translation%20transforms%20programs%20from%20one%20programming%20language%20%28PL%29%20to%0Aanother.%20Several%20rule-based%20transpilers%20have%20been%20designed%20to%20automate%20code%0Atranslation%20between%20different%20pairs%20of%20PLs.%20However%2C%20the%20rules%20can%20become%0Aobsolete%20as%20the%20PLs%20evolve%20and%20cannot%20generalize%20to%20other%20PLs.%20Recent%20studies%0Ahave%20explored%20the%20automation%20of%20code%20translation%20using%20Large%20Language%20Models%0A%28LLMs%29.%20One%20key%20observation%20is%20that%20such%20techniques%20may%20work%20well%20for%20crafted%0Abenchmarks%20but%20fail%20to%20generalize%20to%20the%20scale%20and%20complexity%20of%20real-world%0Aprojects%20with%20dependencies%2C%20custom%20types%2C%20PL-specific%20features%2C%20etc.%0A%20%20We%20propose%20AlphaTrans%2C%20a%20neuro-symbolic%20approach%20to%20automate%20repository-level%0Acode%20translation.%20AlphaTrans%20translates%20both%20source%20and%20test%20code%2C%20and%20employs%0Amultiple%20levels%20of%20validation%20to%20ensure%20the%20translation%20preserves%20the%0Afunctionality%20of%20the%20source%20program.%20To%20break%20down%20the%20problem%20for%20LLMs%2C%0AAlphaTrans%20leverages%20program%20analysis%20to%20decompose%20the%20program%20into%20fragments%0Aand%20translates%20them%20in%20the%20reverse%20call%20order.%20We%20leveraged%20AlphaTrans%20to%0Atranslate%20ten%20real-world%20open-source%20projects%20consisting%20of%20%3C836%2C%208575%2C%202719%3E%0Aclasses%2C%20methods%2C%20and%20tests.%20AlphaTrans%20translated%20the%20entire%20repository%20of%0Athese%20projects%20consisting%20of%206899%20source%20code%20fragments.%2099.1%25%20of%20the%0Atranslated%20code%20fragments%20are%20syntactically%20correct%2C%20and%20AlphaTrans%20validates%0Athe%20translations%27%20runtime%20behavior%20and%20functional%20correctness%20for%2025.8%25.%20On%0Aaverage%2C%20the%20integrated%20translation%20and%20validation%20take%2036%20hours%20to%20translate%20a%0Aproject%2C%20showing%20its%20scalability%20in%20practice.%20For%20the%20syntactically%20or%0Asemantically%20incorrect%20translations%2C%20AlphaTrans%20generates%20a%20report%20including%0Aexisting%20translation%2C%20stack%20trace%2C%20test%20errors%2C%20or%20assertion%20failures.%20We%0Aprovided%20these%20artifacts%20to%20two%20developers%20to%20fix%20the%20translation%20bugs%20in%20four%0Aprojects.%20They%20were%20able%20to%20fix%20the%20issues%20in%2020.1%20hours%20on%20average%20and%20achieve%0Aall%20passing%20tests.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24117v1&entry.124074799=Read"},
{"title": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically", "author": "Anay Mehrotra and Manolis Zampetakis and Paul Kassianik and Blaine Nelson and Hyrum Anderson and Yaron Singer and Amin Karbasi", "abstract": "  While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an attacker LLM\nto iteratively refine candidate (attack) prompts until one of the refined\nprompts jailbreaks the target. In addition, before sending prompts to the\ntarget, TAP assesses them and prunes the ones unlikely to result in jailbreaks,\nreducing the number of queries sent to the target LLM. In empirical\nevaluations, we observe that TAP generates prompts that jailbreak\nstate-of-the-art LLMs (including GPT4-Turbo and GPT4o) for more than 80% of the\nprompts. This significantly improves upon the previous state-of-the-art\nblack-box methods for generating jailbreaks while using a smaller number of\nqueries than them. Furthermore, TAP is also capable of jailbreaking LLMs\nprotected by state-of-the-art guardrails, e.g., LlamaGuard.\n", "link": "http://arxiv.org/abs/2312.02119v3", "date": "2024-10-31", "relevancy": 2.1907, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4506}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4375}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tree%20of%20Attacks%3A%20Jailbreaking%20Black-Box%20LLMs%20Automatically&body=Title%3A%20Tree%20of%20Attacks%3A%20Jailbreaking%20Black-Box%20LLMs%20Automatically%0AAuthor%3A%20Anay%20Mehrotra%20and%20Manolis%20Zampetakis%20and%20Paul%20Kassianik%20and%20Blaine%20Nelson%20and%20Hyrum%20Anderson%20and%20Yaron%20Singer%20and%20Amin%20Karbasi%0AAbstract%3A%20%20%20While%20Large%20Language%20Models%20%28LLMs%29%20display%20versatile%20functionality%2C%20they%0Acontinue%20to%20generate%20harmful%2C%20biased%2C%20and%20toxic%20content%2C%20as%20demonstrated%20by%20the%0Aprevalence%20of%20human-designed%20jailbreaks.%20In%20this%20work%2C%20we%20present%20Tree%20of%0AAttacks%20with%20Pruning%20%28TAP%29%2C%20an%20automated%20method%20for%20generating%20jailbreaks%20that%0Aonly%20requires%20black-box%20access%20to%20the%20target%20LLM.%20TAP%20utilizes%20an%20attacker%20LLM%0Ato%20iteratively%20refine%20candidate%20%28attack%29%20prompts%20until%20one%20of%20the%20refined%0Aprompts%20jailbreaks%20the%20target.%20In%20addition%2C%20before%20sending%20prompts%20to%20the%0Atarget%2C%20TAP%20assesses%20them%20and%20prunes%20the%20ones%20unlikely%20to%20result%20in%20jailbreaks%2C%0Areducing%20the%20number%20of%20queries%20sent%20to%20the%20target%20LLM.%20In%20empirical%0Aevaluations%2C%20we%20observe%20that%20TAP%20generates%20prompts%20that%20jailbreak%0Astate-of-the-art%20LLMs%20%28including%20GPT4-Turbo%20and%20GPT4o%29%20for%20more%20than%2080%25%20of%20the%0Aprompts.%20This%20significantly%20improves%20upon%20the%20previous%20state-of-the-art%0Ablack-box%20methods%20for%20generating%20jailbreaks%20while%20using%20a%20smaller%20number%20of%0Aqueries%20than%20them.%20Furthermore%2C%20TAP%20is%20also%20capable%20of%20jailbreaking%20LLMs%0Aprotected%20by%20state-of-the-art%20guardrails%2C%20e.g.%2C%20LlamaGuard.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02119v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTree%2520of%2520Attacks%253A%2520Jailbreaking%2520Black-Box%2520LLMs%2520Automatically%26entry.906535625%3DAnay%2520Mehrotra%2520and%2520Manolis%2520Zampetakis%2520and%2520Paul%2520Kassianik%2520and%2520Blaine%2520Nelson%2520and%2520Hyrum%2520Anderson%2520and%2520Yaron%2520Singer%2520and%2520Amin%2520Karbasi%26entry.1292438233%3D%2520%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520display%2520versatile%2520functionality%252C%2520they%250Acontinue%2520to%2520generate%2520harmful%252C%2520biased%252C%2520and%2520toxic%2520content%252C%2520as%2520demonstrated%2520by%2520the%250Aprevalence%2520of%2520human-designed%2520jailbreaks.%2520In%2520this%2520work%252C%2520we%2520present%2520Tree%2520of%250AAttacks%2520with%2520Pruning%2520%2528TAP%2529%252C%2520an%2520automated%2520method%2520for%2520generating%2520jailbreaks%2520that%250Aonly%2520requires%2520black-box%2520access%2520to%2520the%2520target%2520LLM.%2520TAP%2520utilizes%2520an%2520attacker%2520LLM%250Ato%2520iteratively%2520refine%2520candidate%2520%2528attack%2529%2520prompts%2520until%2520one%2520of%2520the%2520refined%250Aprompts%2520jailbreaks%2520the%2520target.%2520In%2520addition%252C%2520before%2520sending%2520prompts%2520to%2520the%250Atarget%252C%2520TAP%2520assesses%2520them%2520and%2520prunes%2520the%2520ones%2520unlikely%2520to%2520result%2520in%2520jailbreaks%252C%250Areducing%2520the%2520number%2520of%2520queries%2520sent%2520to%2520the%2520target%2520LLM.%2520In%2520empirical%250Aevaluations%252C%2520we%2520observe%2520that%2520TAP%2520generates%2520prompts%2520that%2520jailbreak%250Astate-of-the-art%2520LLMs%2520%2528including%2520GPT4-Turbo%2520and%2520GPT4o%2529%2520for%2520more%2520than%252080%2525%2520of%2520the%250Aprompts.%2520This%2520significantly%2520improves%2520upon%2520the%2520previous%2520state-of-the-art%250Ablack-box%2520methods%2520for%2520generating%2520jailbreaks%2520while%2520using%2520a%2520smaller%2520number%2520of%250Aqueries%2520than%2520them.%2520Furthermore%252C%2520TAP%2520is%2520also%2520capable%2520of%2520jailbreaking%2520LLMs%250Aprotected%2520by%2520state-of-the-art%2520guardrails%252C%2520e.g.%252C%2520LlamaGuard.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02119v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tree%20of%20Attacks%3A%20Jailbreaking%20Black-Box%20LLMs%20Automatically&entry.906535625=Anay%20Mehrotra%20and%20Manolis%20Zampetakis%20and%20Paul%20Kassianik%20and%20Blaine%20Nelson%20and%20Hyrum%20Anderson%20and%20Yaron%20Singer%20and%20Amin%20Karbasi&entry.1292438233=%20%20While%20Large%20Language%20Models%20%28LLMs%29%20display%20versatile%20functionality%2C%20they%0Acontinue%20to%20generate%20harmful%2C%20biased%2C%20and%20toxic%20content%2C%20as%20demonstrated%20by%20the%0Aprevalence%20of%20human-designed%20jailbreaks.%20In%20this%20work%2C%20we%20present%20Tree%20of%0AAttacks%20with%20Pruning%20%28TAP%29%2C%20an%20automated%20method%20for%20generating%20jailbreaks%20that%0Aonly%20requires%20black-box%20access%20to%20the%20target%20LLM.%20TAP%20utilizes%20an%20attacker%20LLM%0Ato%20iteratively%20refine%20candidate%20%28attack%29%20prompts%20until%20one%20of%20the%20refined%0Aprompts%20jailbreaks%20the%20target.%20In%20addition%2C%20before%20sending%20prompts%20to%20the%0Atarget%2C%20TAP%20assesses%20them%20and%20prunes%20the%20ones%20unlikely%20to%20result%20in%20jailbreaks%2C%0Areducing%20the%20number%20of%20queries%20sent%20to%20the%20target%20LLM.%20In%20empirical%0Aevaluations%2C%20we%20observe%20that%20TAP%20generates%20prompts%20that%20jailbreak%0Astate-of-the-art%20LLMs%20%28including%20GPT4-Turbo%20and%20GPT4o%29%20for%20more%20than%2080%25%20of%20the%0Aprompts.%20This%20significantly%20improves%20upon%20the%20previous%20state-of-the-art%0Ablack-box%20methods%20for%20generating%20jailbreaks%20while%20using%20a%20smaller%20number%20of%0Aqueries%20than%20them.%20Furthermore%2C%20TAP%20is%20also%20capable%20of%20jailbreaking%20LLMs%0Aprotected%20by%20state-of-the-art%20guardrails%2C%20e.g.%2C%20LlamaGuard.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02119v3&entry.124074799=Read"},
{"title": "Flex-MoE: Modeling Arbitrary Modality Combination via the Flexible\n  Mixture-of-Experts", "author": "Sukwon Yun and Inyoung Choi and Jie Peng and Yangfan Wu and Jingxuan Bao and Qiyiwen Zhang and Jiayi Xin and Qi Long and Tianlong Chen", "abstract": "  Multimodal learning has gained increasing importance across various fields,\noffering the ability to integrate data from diverse sources such as images,\ntext, and personalized records, which are frequently observed in medical\ndomains. However, in scenarios where some modalities are missing, many existing\nframeworks struggle to accommodate arbitrary modality combinations, often\nrelying heavily on a single modality or complete data. This oversight of\npotential modality combinations limits their applicability in real-world\nsituations. To address this challenge, we propose Flex-MoE (Flexible\nMixture-of-Experts), a new framework designed to flexibly incorporate arbitrary\nmodality combinations while maintaining robustness to missing data. The core\nidea of Flex-MoE is to first address missing modalities using a new missing\nmodality bank that integrates observed modality combinations with the\ncorresponding missing ones. This is followed by a uniquely designed Sparse MoE\nframework. Specifically, Flex-MoE first trains experts using samples with all\nmodalities to inject generalized knowledge through the generalized router\n($\\mathcal{G}$-Router). The $\\mathcal{S}$-Router then specializes in handling\nfewer modality combinations by assigning the top-1 gate to the expert\ncorresponding to the observed modality combination. We evaluate Flex-MoE on the\nADNI dataset, which encompasses four modalities in the Alzheimer's Disease\ndomain, as well as on the MIMIC-IV dataset. The results demonstrate the\neffectiveness of Flex-MoE highlighting its ability to model arbitrary modality\ncombinations in diverse missing modality scenarios. Code is available at\nhttps://github.com/UNITES-Lab/flex-moe.\n", "link": "http://arxiv.org/abs/2410.08245v2", "date": "2024-10-31", "relevancy": 2.1854, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5666}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5351}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flex-MoE%3A%20Modeling%20Arbitrary%20Modality%20Combination%20via%20the%20Flexible%0A%20%20Mixture-of-Experts&body=Title%3A%20Flex-MoE%3A%20Modeling%20Arbitrary%20Modality%20Combination%20via%20the%20Flexible%0A%20%20Mixture-of-Experts%0AAuthor%3A%20Sukwon%20Yun%20and%20Inyoung%20Choi%20and%20Jie%20Peng%20and%20Yangfan%20Wu%20and%20Jingxuan%20Bao%20and%20Qiyiwen%20Zhang%20and%20Jiayi%20Xin%20and%20Qi%20Long%20and%20Tianlong%20Chen%0AAbstract%3A%20%20%20Multimodal%20learning%20has%20gained%20increasing%20importance%20across%20various%20fields%2C%0Aoffering%20the%20ability%20to%20integrate%20data%20from%20diverse%20sources%20such%20as%20images%2C%0Atext%2C%20and%20personalized%20records%2C%20which%20are%20frequently%20observed%20in%20medical%0Adomains.%20However%2C%20in%20scenarios%20where%20some%20modalities%20are%20missing%2C%20many%20existing%0Aframeworks%20struggle%20to%20accommodate%20arbitrary%20modality%20combinations%2C%20often%0Arelying%20heavily%20on%20a%20single%20modality%20or%20complete%20data.%20This%20oversight%20of%0Apotential%20modality%20combinations%20limits%20their%20applicability%20in%20real-world%0Asituations.%20To%20address%20this%20challenge%2C%20we%20propose%20Flex-MoE%20%28Flexible%0AMixture-of-Experts%29%2C%20a%20new%20framework%20designed%20to%20flexibly%20incorporate%20arbitrary%0Amodality%20combinations%20while%20maintaining%20robustness%20to%20missing%20data.%20The%20core%0Aidea%20of%20Flex-MoE%20is%20to%20first%20address%20missing%20modalities%20using%20a%20new%20missing%0Amodality%20bank%20that%20integrates%20observed%20modality%20combinations%20with%20the%0Acorresponding%20missing%20ones.%20This%20is%20followed%20by%20a%20uniquely%20designed%20Sparse%20MoE%0Aframework.%20Specifically%2C%20Flex-MoE%20first%20trains%20experts%20using%20samples%20with%20all%0Amodalities%20to%20inject%20generalized%20knowledge%20through%20the%20generalized%20router%0A%28%24%5Cmathcal%7BG%7D%24-Router%29.%20The%20%24%5Cmathcal%7BS%7D%24-Router%20then%20specializes%20in%20handling%0Afewer%20modality%20combinations%20by%20assigning%20the%20top-1%20gate%20to%20the%20expert%0Acorresponding%20to%20the%20observed%20modality%20combination.%20We%20evaluate%20Flex-MoE%20on%20the%0AADNI%20dataset%2C%20which%20encompasses%20four%20modalities%20in%20the%20Alzheimer%27s%20Disease%0Adomain%2C%20as%20well%20as%20on%20the%20MIMIC-IV%20dataset.%20The%20results%20demonstrate%20the%0Aeffectiveness%20of%20Flex-MoE%20highlighting%20its%20ability%20to%20model%20arbitrary%20modality%0Acombinations%20in%20diverse%20missing%20modality%20scenarios.%20Code%20is%20available%20at%0Ahttps%3A//github.com/UNITES-Lab/flex-moe.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08245v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlex-MoE%253A%2520Modeling%2520Arbitrary%2520Modality%2520Combination%2520via%2520the%2520Flexible%250A%2520%2520Mixture-of-Experts%26entry.906535625%3DSukwon%2520Yun%2520and%2520Inyoung%2520Choi%2520and%2520Jie%2520Peng%2520and%2520Yangfan%2520Wu%2520and%2520Jingxuan%2520Bao%2520and%2520Qiyiwen%2520Zhang%2520and%2520Jiayi%2520Xin%2520and%2520Qi%2520Long%2520and%2520Tianlong%2520Chen%26entry.1292438233%3D%2520%2520Multimodal%2520learning%2520has%2520gained%2520increasing%2520importance%2520across%2520various%2520fields%252C%250Aoffering%2520the%2520ability%2520to%2520integrate%2520data%2520from%2520diverse%2520sources%2520such%2520as%2520images%252C%250Atext%252C%2520and%2520personalized%2520records%252C%2520which%2520are%2520frequently%2520observed%2520in%2520medical%250Adomains.%2520However%252C%2520in%2520scenarios%2520where%2520some%2520modalities%2520are%2520missing%252C%2520many%2520existing%250Aframeworks%2520struggle%2520to%2520accommodate%2520arbitrary%2520modality%2520combinations%252C%2520often%250Arelying%2520heavily%2520on%2520a%2520single%2520modality%2520or%2520complete%2520data.%2520This%2520oversight%2520of%250Apotential%2520modality%2520combinations%2520limits%2520their%2520applicability%2520in%2520real-world%250Asituations.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520Flex-MoE%2520%2528Flexible%250AMixture-of-Experts%2529%252C%2520a%2520new%2520framework%2520designed%2520to%2520flexibly%2520incorporate%2520arbitrary%250Amodality%2520combinations%2520while%2520maintaining%2520robustness%2520to%2520missing%2520data.%2520The%2520core%250Aidea%2520of%2520Flex-MoE%2520is%2520to%2520first%2520address%2520missing%2520modalities%2520using%2520a%2520new%2520missing%250Amodality%2520bank%2520that%2520integrates%2520observed%2520modality%2520combinations%2520with%2520the%250Acorresponding%2520missing%2520ones.%2520This%2520is%2520followed%2520by%2520a%2520uniquely%2520designed%2520Sparse%2520MoE%250Aframework.%2520Specifically%252C%2520Flex-MoE%2520first%2520trains%2520experts%2520using%2520samples%2520with%2520all%250Amodalities%2520to%2520inject%2520generalized%2520knowledge%2520through%2520the%2520generalized%2520router%250A%2528%2524%255Cmathcal%257BG%257D%2524-Router%2529.%2520The%2520%2524%255Cmathcal%257BS%257D%2524-Router%2520then%2520specializes%2520in%2520handling%250Afewer%2520modality%2520combinations%2520by%2520assigning%2520the%2520top-1%2520gate%2520to%2520the%2520expert%250Acorresponding%2520to%2520the%2520observed%2520modality%2520combination.%2520We%2520evaluate%2520Flex-MoE%2520on%2520the%250AADNI%2520dataset%252C%2520which%2520encompasses%2520four%2520modalities%2520in%2520the%2520Alzheimer%2527s%2520Disease%250Adomain%252C%2520as%2520well%2520as%2520on%2520the%2520MIMIC-IV%2520dataset.%2520The%2520results%2520demonstrate%2520the%250Aeffectiveness%2520of%2520Flex-MoE%2520highlighting%2520its%2520ability%2520to%2520model%2520arbitrary%2520modality%250Acombinations%2520in%2520diverse%2520missing%2520modality%2520scenarios.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/UNITES-Lab/flex-moe.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08245v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flex-MoE%3A%20Modeling%20Arbitrary%20Modality%20Combination%20via%20the%20Flexible%0A%20%20Mixture-of-Experts&entry.906535625=Sukwon%20Yun%20and%20Inyoung%20Choi%20and%20Jie%20Peng%20and%20Yangfan%20Wu%20and%20Jingxuan%20Bao%20and%20Qiyiwen%20Zhang%20and%20Jiayi%20Xin%20and%20Qi%20Long%20and%20Tianlong%20Chen&entry.1292438233=%20%20Multimodal%20learning%20has%20gained%20increasing%20importance%20across%20various%20fields%2C%0Aoffering%20the%20ability%20to%20integrate%20data%20from%20diverse%20sources%20such%20as%20images%2C%0Atext%2C%20and%20personalized%20records%2C%20which%20are%20frequently%20observed%20in%20medical%0Adomains.%20However%2C%20in%20scenarios%20where%20some%20modalities%20are%20missing%2C%20many%20existing%0Aframeworks%20struggle%20to%20accommodate%20arbitrary%20modality%20combinations%2C%20often%0Arelying%20heavily%20on%20a%20single%20modality%20or%20complete%20data.%20This%20oversight%20of%0Apotential%20modality%20combinations%20limits%20their%20applicability%20in%20real-world%0Asituations.%20To%20address%20this%20challenge%2C%20we%20propose%20Flex-MoE%20%28Flexible%0AMixture-of-Experts%29%2C%20a%20new%20framework%20designed%20to%20flexibly%20incorporate%20arbitrary%0Amodality%20combinations%20while%20maintaining%20robustness%20to%20missing%20data.%20The%20core%0Aidea%20of%20Flex-MoE%20is%20to%20first%20address%20missing%20modalities%20using%20a%20new%20missing%0Amodality%20bank%20that%20integrates%20observed%20modality%20combinations%20with%20the%0Acorresponding%20missing%20ones.%20This%20is%20followed%20by%20a%20uniquely%20designed%20Sparse%20MoE%0Aframework.%20Specifically%2C%20Flex-MoE%20first%20trains%20experts%20using%20samples%20with%20all%0Amodalities%20to%20inject%20generalized%20knowledge%20through%20the%20generalized%20router%0A%28%24%5Cmathcal%7BG%7D%24-Router%29.%20The%20%24%5Cmathcal%7BS%7D%24-Router%20then%20specializes%20in%20handling%0Afewer%20modality%20combinations%20by%20assigning%20the%20top-1%20gate%20to%20the%20expert%0Acorresponding%20to%20the%20observed%20modality%20combination.%20We%20evaluate%20Flex-MoE%20on%20the%0AADNI%20dataset%2C%20which%20encompasses%20four%20modalities%20in%20the%20Alzheimer%27s%20Disease%0Adomain%2C%20as%20well%20as%20on%20the%20MIMIC-IV%20dataset.%20The%20results%20demonstrate%20the%0Aeffectiveness%20of%20Flex-MoE%20highlighting%20its%20ability%20to%20model%20arbitrary%20modality%0Acombinations%20in%20diverse%20missing%20modality%20scenarios.%20Code%20is%20available%20at%0Ahttps%3A//github.com/UNITES-Lab/flex-moe.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08245v2&entry.124074799=Read"},
{"title": "MV-CC: Mask Enhanced Video Model for Remote Sensing Change Caption", "author": "Ruixun Liu and Kaiyu Li and Jiayi Song and Dongwei Sun and Xiangyong Cao", "abstract": "  Remote sensing image change caption (RSICC) aims to provide natural language\ndescriptions for bi-temporal remote sensing images. Since Change Caption (CC)\ntask requires both spatial and temporal features, previous works follow an\nencoder-fusion-decoder architecture. They use an image encoder to extract\nspatial features and the fusion module to integrate spatial features and\nextract temporal features, which leads to increasingly complex manual design of\nthe fusion module. In this paper, we introduce a novel video model-based\nparadigm without design of the fusion module and propose a Mask-enhanced Video\nmodel for Change Caption (MV-CC). Specifically, we use the off-the-shelf video\nencoder to simultaneously extract the temporal and spatial features of\nbi-temporal images. Furthermore, the types of changes in the CC are set based\non specific task requirements, and to enable the model to better focus on the\nregions of interest, we employ masks obtained from the Change Detection (CD)\nmethod to explicitly guide the CC model. Experimental results demonstrate that\nour proposed method can obtain better performance compared with other\nstate-of-the-art RSICC methods. The code is available at\nhttps://github.com/liuruixun/MV-CC.\n", "link": "http://arxiv.org/abs/2410.23946v1", "date": "2024-10-31", "relevancy": 2.1743, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5694}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5476}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MV-CC%3A%20Mask%20Enhanced%20Video%20Model%20for%20Remote%20Sensing%20Change%20Caption&body=Title%3A%20MV-CC%3A%20Mask%20Enhanced%20Video%20Model%20for%20Remote%20Sensing%20Change%20Caption%0AAuthor%3A%20Ruixun%20Liu%20and%20Kaiyu%20Li%20and%20Jiayi%20Song%20and%20Dongwei%20Sun%20and%20Xiangyong%20Cao%0AAbstract%3A%20%20%20Remote%20sensing%20image%20change%20caption%20%28RSICC%29%20aims%20to%20provide%20natural%20language%0Adescriptions%20for%20bi-temporal%20remote%20sensing%20images.%20Since%20Change%20Caption%20%28CC%29%0Atask%20requires%20both%20spatial%20and%20temporal%20features%2C%20previous%20works%20follow%20an%0Aencoder-fusion-decoder%20architecture.%20They%20use%20an%20image%20encoder%20to%20extract%0Aspatial%20features%20and%20the%20fusion%20module%20to%20integrate%20spatial%20features%20and%0Aextract%20temporal%20features%2C%20which%20leads%20to%20increasingly%20complex%20manual%20design%20of%0Athe%20fusion%20module.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20video%20model-based%0Aparadigm%20without%20design%20of%20the%20fusion%20module%20and%20propose%20a%20Mask-enhanced%20Video%0Amodel%20for%20Change%20Caption%20%28MV-CC%29.%20Specifically%2C%20we%20use%20the%20off-the-shelf%20video%0Aencoder%20to%20simultaneously%20extract%20the%20temporal%20and%20spatial%20features%20of%0Abi-temporal%20images.%20Furthermore%2C%20the%20types%20of%20changes%20in%20the%20CC%20are%20set%20based%0Aon%20specific%20task%20requirements%2C%20and%20to%20enable%20the%20model%20to%20better%20focus%20on%20the%0Aregions%20of%20interest%2C%20we%20employ%20masks%20obtained%20from%20the%20Change%20Detection%20%28CD%29%0Amethod%20to%20explicitly%20guide%20the%20CC%20model.%20Experimental%20results%20demonstrate%20that%0Aour%20proposed%20method%20can%20obtain%20better%20performance%20compared%20with%20other%0Astate-of-the-art%20RSICC%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/liuruixun/MV-CC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23946v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMV-CC%253A%2520Mask%2520Enhanced%2520Video%2520Model%2520for%2520Remote%2520Sensing%2520Change%2520Caption%26entry.906535625%3DRuixun%2520Liu%2520and%2520Kaiyu%2520Li%2520and%2520Jiayi%2520Song%2520and%2520Dongwei%2520Sun%2520and%2520Xiangyong%2520Cao%26entry.1292438233%3D%2520%2520Remote%2520sensing%2520image%2520change%2520caption%2520%2528RSICC%2529%2520aims%2520to%2520provide%2520natural%2520language%250Adescriptions%2520for%2520bi-temporal%2520remote%2520sensing%2520images.%2520Since%2520Change%2520Caption%2520%2528CC%2529%250Atask%2520requires%2520both%2520spatial%2520and%2520temporal%2520features%252C%2520previous%2520works%2520follow%2520an%250Aencoder-fusion-decoder%2520architecture.%2520They%2520use%2520an%2520image%2520encoder%2520to%2520extract%250Aspatial%2520features%2520and%2520the%2520fusion%2520module%2520to%2520integrate%2520spatial%2520features%2520and%250Aextract%2520temporal%2520features%252C%2520which%2520leads%2520to%2520increasingly%2520complex%2520manual%2520design%2520of%250Athe%2520fusion%2520module.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520video%2520model-based%250Aparadigm%2520without%2520design%2520of%2520the%2520fusion%2520module%2520and%2520propose%2520a%2520Mask-enhanced%2520Video%250Amodel%2520for%2520Change%2520Caption%2520%2528MV-CC%2529.%2520Specifically%252C%2520we%2520use%2520the%2520off-the-shelf%2520video%250Aencoder%2520to%2520simultaneously%2520extract%2520the%2520temporal%2520and%2520spatial%2520features%2520of%250Abi-temporal%2520images.%2520Furthermore%252C%2520the%2520types%2520of%2520changes%2520in%2520the%2520CC%2520are%2520set%2520based%250Aon%2520specific%2520task%2520requirements%252C%2520and%2520to%2520enable%2520the%2520model%2520to%2520better%2520focus%2520on%2520the%250Aregions%2520of%2520interest%252C%2520we%2520employ%2520masks%2520obtained%2520from%2520the%2520Change%2520Detection%2520%2528CD%2529%250Amethod%2520to%2520explicitly%2520guide%2520the%2520CC%2520model.%2520Experimental%2520results%2520demonstrate%2520that%250Aour%2520proposed%2520method%2520can%2520obtain%2520better%2520performance%2520compared%2520with%2520other%250Astate-of-the-art%2520RSICC%2520methods.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/liuruixun/MV-CC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23946v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MV-CC%3A%20Mask%20Enhanced%20Video%20Model%20for%20Remote%20Sensing%20Change%20Caption&entry.906535625=Ruixun%20Liu%20and%20Kaiyu%20Li%20and%20Jiayi%20Song%20and%20Dongwei%20Sun%20and%20Xiangyong%20Cao&entry.1292438233=%20%20Remote%20sensing%20image%20change%20caption%20%28RSICC%29%20aims%20to%20provide%20natural%20language%0Adescriptions%20for%20bi-temporal%20remote%20sensing%20images.%20Since%20Change%20Caption%20%28CC%29%0Atask%20requires%20both%20spatial%20and%20temporal%20features%2C%20previous%20works%20follow%20an%0Aencoder-fusion-decoder%20architecture.%20They%20use%20an%20image%20encoder%20to%20extract%0Aspatial%20features%20and%20the%20fusion%20module%20to%20integrate%20spatial%20features%20and%0Aextract%20temporal%20features%2C%20which%20leads%20to%20increasingly%20complex%20manual%20design%20of%0Athe%20fusion%20module.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20video%20model-based%0Aparadigm%20without%20design%20of%20the%20fusion%20module%20and%20propose%20a%20Mask-enhanced%20Video%0Amodel%20for%20Change%20Caption%20%28MV-CC%29.%20Specifically%2C%20we%20use%20the%20off-the-shelf%20video%0Aencoder%20to%20simultaneously%20extract%20the%20temporal%20and%20spatial%20features%20of%0Abi-temporal%20images.%20Furthermore%2C%20the%20types%20of%20changes%20in%20the%20CC%20are%20set%20based%0Aon%20specific%20task%20requirements%2C%20and%20to%20enable%20the%20model%20to%20better%20focus%20on%20the%0Aregions%20of%20interest%2C%20we%20employ%20masks%20obtained%20from%20the%20Change%20Detection%20%28CD%29%0Amethod%20to%20explicitly%20guide%20the%20CC%20model.%20Experimental%20results%20demonstrate%20that%0Aour%20proposed%20method%20can%20obtain%20better%20performance%20compared%20with%20other%0Astate-of-the-art%20RSICC%20methods.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/liuruixun/MV-CC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23946v1&entry.124074799=Read"},
{"title": "Sparsh: Self-supervised touch representations for vision-based tactile\n  sensing", "author": "Carolina Higuera and Akash Sharma and Chaithanya Krishna Bodduluri and Taosha Fan and Patrick Lancaster and Mrinal Kalakrishnan and Michael Kaess and Byron Boots and Mike Lambeta and Tingfan Wu and Mustafa Mukadam", "abstract": "  In this work, we introduce general purpose touch representations for the\nincreasingly accessible class of vision-based tactile sensors. Such sensors\nhave led to many recent advances in robot manipulation as they markedly\ncomplement vision, yet solutions today often rely on task and sensor specific\nhandcrafted perception models. Collecting real data at scale with task centric\nground truth labels, like contact forces and slip, is a challenge further\ncompounded by sensors of various form factor differing in aspects like lighting\nand gel markings. To tackle this we turn to self-supervised learning (SSL) that\nhas demonstrated remarkable performance in computer vision. We present Sparsh,\na family of SSL models that can support various vision-based tactile sensors,\nalleviating the need for custom labels through pre-training on 460k+ tactile\nimages with masking and self-distillation in pixel and latent spaces. We also\nbuild TacBench, to facilitate standardized benchmarking across sensors and\nmodels, comprising of six tasks ranging from comprehending tactile properties\nto enabling physical perception and manipulation planning. In evaluations, we\nfind that SSL pre-training for touch representation outperforms task and\nsensor-specific end-to-end training by 95.1% on average over TacBench, and\nSparsh (DINO) and Sparsh (IJEPA) are the most competitive, indicating the\nmerits of learning in latent space for tactile images. Project page:\nhttps://sparsh-ssl.github.io/\n", "link": "http://arxiv.org/abs/2410.24090v1", "date": "2024-10-31", "relevancy": 2.1705, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5507}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.541}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparsh%3A%20Self-supervised%20touch%20representations%20for%20vision-based%20tactile%0A%20%20sensing&body=Title%3A%20Sparsh%3A%20Self-supervised%20touch%20representations%20for%20vision-based%20tactile%0A%20%20sensing%0AAuthor%3A%20Carolina%20Higuera%20and%20Akash%20Sharma%20and%20Chaithanya%20Krishna%20Bodduluri%20and%20Taosha%20Fan%20and%20Patrick%20Lancaster%20and%20Mrinal%20Kalakrishnan%20and%20Michael%20Kaess%20and%20Byron%20Boots%20and%20Mike%20Lambeta%20and%20Tingfan%20Wu%20and%20Mustafa%20Mukadam%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20general%20purpose%20touch%20representations%20for%20the%0Aincreasingly%20accessible%20class%20of%20vision-based%20tactile%20sensors.%20Such%20sensors%0Ahave%20led%20to%20many%20recent%20advances%20in%20robot%20manipulation%20as%20they%20markedly%0Acomplement%20vision%2C%20yet%20solutions%20today%20often%20rely%20on%20task%20and%20sensor%20specific%0Ahandcrafted%20perception%20models.%20Collecting%20real%20data%20at%20scale%20with%20task%20centric%0Aground%20truth%20labels%2C%20like%20contact%20forces%20and%20slip%2C%20is%20a%20challenge%20further%0Acompounded%20by%20sensors%20of%20various%20form%20factor%20differing%20in%20aspects%20like%20lighting%0Aand%20gel%20markings.%20To%20tackle%20this%20we%20turn%20to%20self-supervised%20learning%20%28SSL%29%20that%0Ahas%20demonstrated%20remarkable%20performance%20in%20computer%20vision.%20We%20present%20Sparsh%2C%0Aa%20family%20of%20SSL%20models%20that%20can%20support%20various%20vision-based%20tactile%20sensors%2C%0Aalleviating%20the%20need%20for%20custom%20labels%20through%20pre-training%20on%20460k%2B%20tactile%0Aimages%20with%20masking%20and%20self-distillation%20in%20pixel%20and%20latent%20spaces.%20We%20also%0Abuild%20TacBench%2C%20to%20facilitate%20standardized%20benchmarking%20across%20sensors%20and%0Amodels%2C%20comprising%20of%20six%20tasks%20ranging%20from%20comprehending%20tactile%20properties%0Ato%20enabling%20physical%20perception%20and%20manipulation%20planning.%20In%20evaluations%2C%20we%0Afind%20that%20SSL%20pre-training%20for%20touch%20representation%20outperforms%20task%20and%0Asensor-specific%20end-to-end%20training%20by%2095.1%25%20on%20average%20over%20TacBench%2C%20and%0ASparsh%20%28DINO%29%20and%20Sparsh%20%28IJEPA%29%20are%20the%20most%20competitive%2C%20indicating%20the%0Amerits%20of%20learning%20in%20latent%20space%20for%20tactile%20images.%20Project%20page%3A%0Ahttps%3A//sparsh-ssl.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24090v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparsh%253A%2520Self-supervised%2520touch%2520representations%2520for%2520vision-based%2520tactile%250A%2520%2520sensing%26entry.906535625%3DCarolina%2520Higuera%2520and%2520Akash%2520Sharma%2520and%2520Chaithanya%2520Krishna%2520Bodduluri%2520and%2520Taosha%2520Fan%2520and%2520Patrick%2520Lancaster%2520and%2520Mrinal%2520Kalakrishnan%2520and%2520Michael%2520Kaess%2520and%2520Byron%2520Boots%2520and%2520Mike%2520Lambeta%2520and%2520Tingfan%2520Wu%2520and%2520Mustafa%2520Mukadam%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520general%2520purpose%2520touch%2520representations%2520for%2520the%250Aincreasingly%2520accessible%2520class%2520of%2520vision-based%2520tactile%2520sensors.%2520Such%2520sensors%250Ahave%2520led%2520to%2520many%2520recent%2520advances%2520in%2520robot%2520manipulation%2520as%2520they%2520markedly%250Acomplement%2520vision%252C%2520yet%2520solutions%2520today%2520often%2520rely%2520on%2520task%2520and%2520sensor%2520specific%250Ahandcrafted%2520perception%2520models.%2520Collecting%2520real%2520data%2520at%2520scale%2520with%2520task%2520centric%250Aground%2520truth%2520labels%252C%2520like%2520contact%2520forces%2520and%2520slip%252C%2520is%2520a%2520challenge%2520further%250Acompounded%2520by%2520sensors%2520of%2520various%2520form%2520factor%2520differing%2520in%2520aspects%2520like%2520lighting%250Aand%2520gel%2520markings.%2520To%2520tackle%2520this%2520we%2520turn%2520to%2520self-supervised%2520learning%2520%2528SSL%2529%2520that%250Ahas%2520demonstrated%2520remarkable%2520performance%2520in%2520computer%2520vision.%2520We%2520present%2520Sparsh%252C%250Aa%2520family%2520of%2520SSL%2520models%2520that%2520can%2520support%2520various%2520vision-based%2520tactile%2520sensors%252C%250Aalleviating%2520the%2520need%2520for%2520custom%2520labels%2520through%2520pre-training%2520on%2520460k%252B%2520tactile%250Aimages%2520with%2520masking%2520and%2520self-distillation%2520in%2520pixel%2520and%2520latent%2520spaces.%2520We%2520also%250Abuild%2520TacBench%252C%2520to%2520facilitate%2520standardized%2520benchmarking%2520across%2520sensors%2520and%250Amodels%252C%2520comprising%2520of%2520six%2520tasks%2520ranging%2520from%2520comprehending%2520tactile%2520properties%250Ato%2520enabling%2520physical%2520perception%2520and%2520manipulation%2520planning.%2520In%2520evaluations%252C%2520we%250Afind%2520that%2520SSL%2520pre-training%2520for%2520touch%2520representation%2520outperforms%2520task%2520and%250Asensor-specific%2520end-to-end%2520training%2520by%252095.1%2525%2520on%2520average%2520over%2520TacBench%252C%2520and%250ASparsh%2520%2528DINO%2529%2520and%2520Sparsh%2520%2528IJEPA%2529%2520are%2520the%2520most%2520competitive%252C%2520indicating%2520the%250Amerits%2520of%2520learning%2520in%2520latent%2520space%2520for%2520tactile%2520images.%2520Project%2520page%253A%250Ahttps%253A//sparsh-ssl.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24090v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparsh%3A%20Self-supervised%20touch%20representations%20for%20vision-based%20tactile%0A%20%20sensing&entry.906535625=Carolina%20Higuera%20and%20Akash%20Sharma%20and%20Chaithanya%20Krishna%20Bodduluri%20and%20Taosha%20Fan%20and%20Patrick%20Lancaster%20and%20Mrinal%20Kalakrishnan%20and%20Michael%20Kaess%20and%20Byron%20Boots%20and%20Mike%20Lambeta%20and%20Tingfan%20Wu%20and%20Mustafa%20Mukadam&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20general%20purpose%20touch%20representations%20for%20the%0Aincreasingly%20accessible%20class%20of%20vision-based%20tactile%20sensors.%20Such%20sensors%0Ahave%20led%20to%20many%20recent%20advances%20in%20robot%20manipulation%20as%20they%20markedly%0Acomplement%20vision%2C%20yet%20solutions%20today%20often%20rely%20on%20task%20and%20sensor%20specific%0Ahandcrafted%20perception%20models.%20Collecting%20real%20data%20at%20scale%20with%20task%20centric%0Aground%20truth%20labels%2C%20like%20contact%20forces%20and%20slip%2C%20is%20a%20challenge%20further%0Acompounded%20by%20sensors%20of%20various%20form%20factor%20differing%20in%20aspects%20like%20lighting%0Aand%20gel%20markings.%20To%20tackle%20this%20we%20turn%20to%20self-supervised%20learning%20%28SSL%29%20that%0Ahas%20demonstrated%20remarkable%20performance%20in%20computer%20vision.%20We%20present%20Sparsh%2C%0Aa%20family%20of%20SSL%20models%20that%20can%20support%20various%20vision-based%20tactile%20sensors%2C%0Aalleviating%20the%20need%20for%20custom%20labels%20through%20pre-training%20on%20460k%2B%20tactile%0Aimages%20with%20masking%20and%20self-distillation%20in%20pixel%20and%20latent%20spaces.%20We%20also%0Abuild%20TacBench%2C%20to%20facilitate%20standardized%20benchmarking%20across%20sensors%20and%0Amodels%2C%20comprising%20of%20six%20tasks%20ranging%20from%20comprehending%20tactile%20properties%0Ato%20enabling%20physical%20perception%20and%20manipulation%20planning.%20In%20evaluations%2C%20we%0Afind%20that%20SSL%20pre-training%20for%20touch%20representation%20outperforms%20task%20and%0Asensor-specific%20end-to-end%20training%20by%2095.1%25%20on%20average%20over%20TacBench%2C%20and%0ASparsh%20%28DINO%29%20and%20Sparsh%20%28IJEPA%29%20are%20the%20most%20competitive%2C%20indicating%20the%0Amerits%20of%20learning%20in%20latent%20space%20for%20tactile%20images.%20Project%20page%3A%0Ahttps%3A//sparsh-ssl.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24090v1&entry.124074799=Read"},
{"title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models", "author": "Heng-Jui Chang and Hongyu Gong and Changhan Wang and James Glass and Yu-An Chung", "abstract": "  Spoken language models (SLMs) have gained increasing attention with\nadvancements in text-based, decoder-only language models. SLMs process text and\nspeech, enabling simultaneous speech understanding and generation. This paper\npresents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to\nimprove speech tokenization by bridging audio signals and SLM tokens. DC-Spin\nextracts speaker-invariant tokens rich in phonetic information and resilient to\ninput variations, enhancing zero-shot SLM tasks and speech resynthesis. We\npropose a chunk-wise approach to enable streamable DC-Spin without retraining\nand degradation. Comparisons of tokenization methods (self-supervised and\nneural audio codecs), model scalability, and downstream task proxies show that\ntokens easily modeled by an n-gram LM or aligned with phonemes offer strong\nperformance, providing insights for designing speech tokenizers for SLMs.\n", "link": "http://arxiv.org/abs/2410.24177v1", "date": "2024-10-31", "relevancy": 2.1693, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4342}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4342}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DC-Spin%3A%20A%20Speaker-invariant%20Speech%20Tokenizer%20for%20Spoken%20Language%20Models&body=Title%3A%20DC-Spin%3A%20A%20Speaker-invariant%20Speech%20Tokenizer%20for%20Spoken%20Language%20Models%0AAuthor%3A%20Heng-Jui%20Chang%20and%20Hongyu%20Gong%20and%20Changhan%20Wang%20and%20James%20Glass%20and%20Yu-An%20Chung%0AAbstract%3A%20%20%20Spoken%20language%20models%20%28SLMs%29%20have%20gained%20increasing%20attention%20with%0Aadvancements%20in%20text-based%2C%20decoder-only%20language%20models.%20SLMs%20process%20text%20and%0Aspeech%2C%20enabling%20simultaneous%20speech%20understanding%20and%20generation.%20This%20paper%0Apresents%20Double-Codebook%20Speaker-invariant%20Clustering%20%28DC-Spin%29%2C%20which%20aims%20to%0Aimprove%20speech%20tokenization%20by%20bridging%20audio%20signals%20and%20SLM%20tokens.%20DC-Spin%0Aextracts%20speaker-invariant%20tokens%20rich%20in%20phonetic%20information%20and%20resilient%20to%0Ainput%20variations%2C%20enhancing%20zero-shot%20SLM%20tasks%20and%20speech%20resynthesis.%20We%0Apropose%20a%20chunk-wise%20approach%20to%20enable%20streamable%20DC-Spin%20without%20retraining%0Aand%20degradation.%20Comparisons%20of%20tokenization%20methods%20%28self-supervised%20and%0Aneural%20audio%20codecs%29%2C%20model%20scalability%2C%20and%20downstream%20task%20proxies%20show%20that%0Atokens%20easily%20modeled%20by%20an%20n-gram%20LM%20or%20aligned%20with%20phonemes%20offer%20strong%0Aperformance%2C%20providing%20insights%20for%20designing%20speech%20tokenizers%20for%20SLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDC-Spin%253A%2520A%2520Speaker-invariant%2520Speech%2520Tokenizer%2520for%2520Spoken%2520Language%2520Models%26entry.906535625%3DHeng-Jui%2520Chang%2520and%2520Hongyu%2520Gong%2520and%2520Changhan%2520Wang%2520and%2520James%2520Glass%2520and%2520Yu-An%2520Chung%26entry.1292438233%3D%2520%2520Spoken%2520language%2520models%2520%2528SLMs%2529%2520have%2520gained%2520increasing%2520attention%2520with%250Aadvancements%2520in%2520text-based%252C%2520decoder-only%2520language%2520models.%2520SLMs%2520process%2520text%2520and%250Aspeech%252C%2520enabling%2520simultaneous%2520speech%2520understanding%2520and%2520generation.%2520This%2520paper%250Apresents%2520Double-Codebook%2520Speaker-invariant%2520Clustering%2520%2528DC-Spin%2529%252C%2520which%2520aims%2520to%250Aimprove%2520speech%2520tokenization%2520by%2520bridging%2520audio%2520signals%2520and%2520SLM%2520tokens.%2520DC-Spin%250Aextracts%2520speaker-invariant%2520tokens%2520rich%2520in%2520phonetic%2520information%2520and%2520resilient%2520to%250Ainput%2520variations%252C%2520enhancing%2520zero-shot%2520SLM%2520tasks%2520and%2520speech%2520resynthesis.%2520We%250Apropose%2520a%2520chunk-wise%2520approach%2520to%2520enable%2520streamable%2520DC-Spin%2520without%2520retraining%250Aand%2520degradation.%2520Comparisons%2520of%2520tokenization%2520methods%2520%2528self-supervised%2520and%250Aneural%2520audio%2520codecs%2529%252C%2520model%2520scalability%252C%2520and%2520downstream%2520task%2520proxies%2520show%2520that%250Atokens%2520easily%2520modeled%2520by%2520an%2520n-gram%2520LM%2520or%2520aligned%2520with%2520phonemes%2520offer%2520strong%250Aperformance%252C%2520providing%2520insights%2520for%2520designing%2520speech%2520tokenizers%2520for%2520SLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DC-Spin%3A%20A%20Speaker-invariant%20Speech%20Tokenizer%20for%20Spoken%20Language%20Models&entry.906535625=Heng-Jui%20Chang%20and%20Hongyu%20Gong%20and%20Changhan%20Wang%20and%20James%20Glass%20and%20Yu-An%20Chung&entry.1292438233=%20%20Spoken%20language%20models%20%28SLMs%29%20have%20gained%20increasing%20attention%20with%0Aadvancements%20in%20text-based%2C%20decoder-only%20language%20models.%20SLMs%20process%20text%20and%0Aspeech%2C%20enabling%20simultaneous%20speech%20understanding%20and%20generation.%20This%20paper%0Apresents%20Double-Codebook%20Speaker-invariant%20Clustering%20%28DC-Spin%29%2C%20which%20aims%20to%0Aimprove%20speech%20tokenization%20by%20bridging%20audio%20signals%20and%20SLM%20tokens.%20DC-Spin%0Aextracts%20speaker-invariant%20tokens%20rich%20in%20phonetic%20information%20and%20resilient%20to%0Ainput%20variations%2C%20enhancing%20zero-shot%20SLM%20tasks%20and%20speech%20resynthesis.%20We%0Apropose%20a%20chunk-wise%20approach%20to%20enable%20streamable%20DC-Spin%20without%20retraining%0Aand%20degradation.%20Comparisons%20of%20tokenization%20methods%20%28self-supervised%20and%0Aneural%20audio%20codecs%29%2C%20model%20scalability%2C%20and%20downstream%20task%20proxies%20show%20that%0Atokens%20easily%20modeled%20by%20an%20n-gram%20LM%20or%20aligned%20with%20phonemes%20offer%20strong%0Aperformance%2C%20providing%20insights%20for%20designing%20speech%20tokenizers%20for%20SLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24177v1&entry.124074799=Read"},
{"title": "AIDOVECL: AI-generated Dataset of Outpainted Vehicles for Eye-level\n  Classification and Localization", "author": "Amir Kazemi and Qurat ul ain Fatima and Volodymyr Kindratenko and Christopher Tessum", "abstract": "  Image labeling is a critical bottleneck in the development of computer vision\ntechnologies, often constraining the potential of machine learning models due\nto the time-intensive nature of manual annotations. This work introduces a\nnovel approach that leverages outpainting to address the problem of annotated\ndata scarcity by generating artificial contexts and annotations, significantly\nreducing manual labeling efforts. We apply this technique to a particularly\nacute challenge in autonomous driving, urban planning, and environmental\nmonitoring: the lack of diverse, eye-level vehicle images in desired classes.\nOur dataset comprises AI-generated vehicle images obtained by detecting and\ncropping vehicles from manually selected seed images, which are then outpainted\nonto larger canvases to simulate varied real-world conditions. The outpainted\nimages include detailed annotations, providing high-quality ground truth data.\nAdvanced outpainting techniques and image quality assessments ensure visual\nfidelity and contextual relevance. Augmentation with outpainted vehicles\nimproves overall performance metrics by up to 8\\% and enhances prediction of\nunderrepresented classes by up to 20\\%. This approach, exemplifying outpainting\nas a self-annotating paradigm, presents a solution that enhances dataset\nversatility across multiple domains of machine learning. The code and links to\ndatasets used in this study are available for further research and replication\nat https://github.com/amir-kazemi/aidovecl.\n", "link": "http://arxiv.org/abs/2410.24116v1", "date": "2024-10-31", "relevancy": 2.161, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5507}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5387}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIDOVECL%3A%20AI-generated%20Dataset%20of%20Outpainted%20Vehicles%20for%20Eye-level%0A%20%20Classification%20and%20Localization&body=Title%3A%20AIDOVECL%3A%20AI-generated%20Dataset%20of%20Outpainted%20Vehicles%20for%20Eye-level%0A%20%20Classification%20and%20Localization%0AAuthor%3A%20Amir%20Kazemi%20and%20Qurat%20ul%20ain%20Fatima%20and%20Volodymyr%20Kindratenko%20and%20Christopher%20Tessum%0AAbstract%3A%20%20%20Image%20labeling%20is%20a%20critical%20bottleneck%20in%20the%20development%20of%20computer%20vision%0Atechnologies%2C%20often%20constraining%20the%20potential%20of%20machine%20learning%20models%20due%0Ato%20the%20time-intensive%20nature%20of%20manual%20annotations.%20This%20work%20introduces%20a%0Anovel%20approach%20that%20leverages%20outpainting%20to%20address%20the%20problem%20of%20annotated%0Adata%20scarcity%20by%20generating%20artificial%20contexts%20and%20annotations%2C%20significantly%0Areducing%20manual%20labeling%20efforts.%20We%20apply%20this%20technique%20to%20a%20particularly%0Aacute%20challenge%20in%20autonomous%20driving%2C%20urban%20planning%2C%20and%20environmental%0Amonitoring%3A%20the%20lack%20of%20diverse%2C%20eye-level%20vehicle%20images%20in%20desired%20classes.%0AOur%20dataset%20comprises%20AI-generated%20vehicle%20images%20obtained%20by%20detecting%20and%0Acropping%20vehicles%20from%20manually%20selected%20seed%20images%2C%20which%20are%20then%20outpainted%0Aonto%20larger%20canvases%20to%20simulate%20varied%20real-world%20conditions.%20The%20outpainted%0Aimages%20include%20detailed%20annotations%2C%20providing%20high-quality%20ground%20truth%20data.%0AAdvanced%20outpainting%20techniques%20and%20image%20quality%20assessments%20ensure%20visual%0Afidelity%20and%20contextual%20relevance.%20Augmentation%20with%20outpainted%20vehicles%0Aimproves%20overall%20performance%20metrics%20by%20up%20to%208%5C%25%20and%20enhances%20prediction%20of%0Aunderrepresented%20classes%20by%20up%20to%2020%5C%25.%20This%20approach%2C%20exemplifying%20outpainting%0Aas%20a%20self-annotating%20paradigm%2C%20presents%20a%20solution%20that%20enhances%20dataset%0Aversatility%20across%20multiple%20domains%20of%20machine%20learning.%20The%20code%20and%20links%20to%0Adatasets%20used%20in%20this%20study%20are%20available%20for%20further%20research%20and%20replication%0Aat%20https%3A//github.com/amir-kazemi/aidovecl.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24116v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIDOVECL%253A%2520AI-generated%2520Dataset%2520of%2520Outpainted%2520Vehicles%2520for%2520Eye-level%250A%2520%2520Classification%2520and%2520Localization%26entry.906535625%3DAmir%2520Kazemi%2520and%2520Qurat%2520ul%2520ain%2520Fatima%2520and%2520Volodymyr%2520Kindratenko%2520and%2520Christopher%2520Tessum%26entry.1292438233%3D%2520%2520Image%2520labeling%2520is%2520a%2520critical%2520bottleneck%2520in%2520the%2520development%2520of%2520computer%2520vision%250Atechnologies%252C%2520often%2520constraining%2520the%2520potential%2520of%2520machine%2520learning%2520models%2520due%250Ato%2520the%2520time-intensive%2520nature%2520of%2520manual%2520annotations.%2520This%2520work%2520introduces%2520a%250Anovel%2520approach%2520that%2520leverages%2520outpainting%2520to%2520address%2520the%2520problem%2520of%2520annotated%250Adata%2520scarcity%2520by%2520generating%2520artificial%2520contexts%2520and%2520annotations%252C%2520significantly%250Areducing%2520manual%2520labeling%2520efforts.%2520We%2520apply%2520this%2520technique%2520to%2520a%2520particularly%250Aacute%2520challenge%2520in%2520autonomous%2520driving%252C%2520urban%2520planning%252C%2520and%2520environmental%250Amonitoring%253A%2520the%2520lack%2520of%2520diverse%252C%2520eye-level%2520vehicle%2520images%2520in%2520desired%2520classes.%250AOur%2520dataset%2520comprises%2520AI-generated%2520vehicle%2520images%2520obtained%2520by%2520detecting%2520and%250Acropping%2520vehicles%2520from%2520manually%2520selected%2520seed%2520images%252C%2520which%2520are%2520then%2520outpainted%250Aonto%2520larger%2520canvases%2520to%2520simulate%2520varied%2520real-world%2520conditions.%2520The%2520outpainted%250Aimages%2520include%2520detailed%2520annotations%252C%2520providing%2520high-quality%2520ground%2520truth%2520data.%250AAdvanced%2520outpainting%2520techniques%2520and%2520image%2520quality%2520assessments%2520ensure%2520visual%250Afidelity%2520and%2520contextual%2520relevance.%2520Augmentation%2520with%2520outpainted%2520vehicles%250Aimproves%2520overall%2520performance%2520metrics%2520by%2520up%2520to%25208%255C%2525%2520and%2520enhances%2520prediction%2520of%250Aunderrepresented%2520classes%2520by%2520up%2520to%252020%255C%2525.%2520This%2520approach%252C%2520exemplifying%2520outpainting%250Aas%2520a%2520self-annotating%2520paradigm%252C%2520presents%2520a%2520solution%2520that%2520enhances%2520dataset%250Aversatility%2520across%2520multiple%2520domains%2520of%2520machine%2520learning.%2520The%2520code%2520and%2520links%2520to%250Adatasets%2520used%2520in%2520this%2520study%2520are%2520available%2520for%2520further%2520research%2520and%2520replication%250Aat%2520https%253A//github.com/amir-kazemi/aidovecl.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24116v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIDOVECL%3A%20AI-generated%20Dataset%20of%20Outpainted%20Vehicles%20for%20Eye-level%0A%20%20Classification%20and%20Localization&entry.906535625=Amir%20Kazemi%20and%20Qurat%20ul%20ain%20Fatima%20and%20Volodymyr%20Kindratenko%20and%20Christopher%20Tessum&entry.1292438233=%20%20Image%20labeling%20is%20a%20critical%20bottleneck%20in%20the%20development%20of%20computer%20vision%0Atechnologies%2C%20often%20constraining%20the%20potential%20of%20machine%20learning%20models%20due%0Ato%20the%20time-intensive%20nature%20of%20manual%20annotations.%20This%20work%20introduces%20a%0Anovel%20approach%20that%20leverages%20outpainting%20to%20address%20the%20problem%20of%20annotated%0Adata%20scarcity%20by%20generating%20artificial%20contexts%20and%20annotations%2C%20significantly%0Areducing%20manual%20labeling%20efforts.%20We%20apply%20this%20technique%20to%20a%20particularly%0Aacute%20challenge%20in%20autonomous%20driving%2C%20urban%20planning%2C%20and%20environmental%0Amonitoring%3A%20the%20lack%20of%20diverse%2C%20eye-level%20vehicle%20images%20in%20desired%20classes.%0AOur%20dataset%20comprises%20AI-generated%20vehicle%20images%20obtained%20by%20detecting%20and%0Acropping%20vehicles%20from%20manually%20selected%20seed%20images%2C%20which%20are%20then%20outpainted%0Aonto%20larger%20canvases%20to%20simulate%20varied%20real-world%20conditions.%20The%20outpainted%0Aimages%20include%20detailed%20annotations%2C%20providing%20high-quality%20ground%20truth%20data.%0AAdvanced%20outpainting%20techniques%20and%20image%20quality%20assessments%20ensure%20visual%0Afidelity%20and%20contextual%20relevance.%20Augmentation%20with%20outpainted%20vehicles%0Aimproves%20overall%20performance%20metrics%20by%20up%20to%208%5C%25%20and%20enhances%20prediction%20of%0Aunderrepresented%20classes%20by%20up%20to%2020%5C%25.%20This%20approach%2C%20exemplifying%20outpainting%0Aas%20a%20self-annotating%20paradigm%2C%20presents%20a%20solution%20that%20enhances%20dataset%0Aversatility%20across%20multiple%20domains%20of%20machine%20learning.%20The%20code%20and%20links%20to%0Adatasets%20used%20in%20this%20study%20are%20available%20for%20further%20research%20and%20replication%0Aat%20https%3A//github.com/amir-kazemi/aidovecl.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24116v1&entry.124074799=Read"},
{"title": "MambaEviScrib: Mamba and Evidence-Guided Consistency Enhance CNN\n  Robustness for Scribble-Based Weakly Supervised Ultrasound Image Segmentation", "author": "Xiaoxiang Han and Xinyu Li and Jiang Shang and Yiman Liu and Keyan Chen and Shugong Xu and Qiaohong Liu and Qi Zhang", "abstract": "  Segmenting anatomical structures and lesions from ultrasound images\ncontributes to disease assessment. Weakly supervised learning (WSL) based on\nsparse annotation has achieved encouraging performance and demonstrated the\npotential to reduce annotation costs. This study attempts to introduce\nscribble-based WSL into ultrasound image segmentation tasks. However,\nultrasound images often suffer from poor contrast and unclear edges, coupled\nwith insufficient supervison signals for edges, posing challenges to edge\nprediction. Uncertainty modeling has been proven to facilitate models in\ndealing with these issues. Nevertheless, existing uncertainty estimation\nparadigms are not robust enough and often filter out predictions near decision\nboundaries, resulting in unstable edge predictions. Therefore, we propose\nleveraging predictions near decision boundaries effectively. Specifically, we\nintroduce Dempster-Shafer Theory (DST) of evidence to design an Evidence-Guided\nConsistency strategy. This strategy utilizes high-evidence predictions, which\nare more likely to occur near high-density regions, to guide the optimization\nof low-evidence predictions that may appear near decision boundaries.\nFurthermore, the diverse sizes and locations of lesions in ultrasound images\npose a challenge for CNNs with local receptive fields, as they struggle to\nmodel global information. Therefore, we introduce Visual Mamba based on\nstructured state space sequence models, which achieves long-range dependency\nwith linear computational complexity, and we construct a novel hybrid CNN-Mamba\nframework. During training, the collaboration between the CNN branch and the\nMamba branch in the proposed framework draws inspiration from each other based\non the EGC strategy. Experiments demonstrate the competitiveness of the\nproposed method. Dataset and code will be available on\nhttps://github.com/GtLinyer/MambaEviScrib.\n", "link": "http://arxiv.org/abs/2409.19370v2", "date": "2024-10-31", "relevancy": 2.1548, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5818}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.552}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MambaEviScrib%3A%20Mamba%20and%20Evidence-Guided%20Consistency%20Enhance%20CNN%0A%20%20Robustness%20for%20Scribble-Based%20Weakly%20Supervised%20Ultrasound%20Image%20Segmentation&body=Title%3A%20MambaEviScrib%3A%20Mamba%20and%20Evidence-Guided%20Consistency%20Enhance%20CNN%0A%20%20Robustness%20for%20Scribble-Based%20Weakly%20Supervised%20Ultrasound%20Image%20Segmentation%0AAuthor%3A%20Xiaoxiang%20Han%20and%20Xinyu%20Li%20and%20Jiang%20Shang%20and%20Yiman%20Liu%20and%20Keyan%20Chen%20and%20Shugong%20Xu%20and%20Qiaohong%20Liu%20and%20Qi%20Zhang%0AAbstract%3A%20%20%20Segmenting%20anatomical%20structures%20and%20lesions%20from%20ultrasound%20images%0Acontributes%20to%20disease%20assessment.%20Weakly%20supervised%20learning%20%28WSL%29%20based%20on%0Asparse%20annotation%20has%20achieved%20encouraging%20performance%20and%20demonstrated%20the%0Apotential%20to%20reduce%20annotation%20costs.%20This%20study%20attempts%20to%20introduce%0Ascribble-based%20WSL%20into%20ultrasound%20image%20segmentation%20tasks.%20However%2C%0Aultrasound%20images%20often%20suffer%20from%20poor%20contrast%20and%20unclear%20edges%2C%20coupled%0Awith%20insufficient%20supervison%20signals%20for%20edges%2C%20posing%20challenges%20to%20edge%0Aprediction.%20Uncertainty%20modeling%20has%20been%20proven%20to%20facilitate%20models%20in%0Adealing%20with%20these%20issues.%20Nevertheless%2C%20existing%20uncertainty%20estimation%0Aparadigms%20are%20not%20robust%20enough%20and%20often%20filter%20out%20predictions%20near%20decision%0Aboundaries%2C%20resulting%20in%20unstable%20edge%20predictions.%20Therefore%2C%20we%20propose%0Aleveraging%20predictions%20near%20decision%20boundaries%20effectively.%20Specifically%2C%20we%0Aintroduce%20Dempster-Shafer%20Theory%20%28DST%29%20of%20evidence%20to%20design%20an%20Evidence-Guided%0AConsistency%20strategy.%20This%20strategy%20utilizes%20high-evidence%20predictions%2C%20which%0Aare%20more%20likely%20to%20occur%20near%20high-density%20regions%2C%20to%20guide%20the%20optimization%0Aof%20low-evidence%20predictions%20that%20may%20appear%20near%20decision%20boundaries.%0AFurthermore%2C%20the%20diverse%20sizes%20and%20locations%20of%20lesions%20in%20ultrasound%20images%0Apose%20a%20challenge%20for%20CNNs%20with%20local%20receptive%20fields%2C%20as%20they%20struggle%20to%0Amodel%20global%20information.%20Therefore%2C%20we%20introduce%20Visual%20Mamba%20based%20on%0Astructured%20state%20space%20sequence%20models%2C%20which%20achieves%20long-range%20dependency%0Awith%20linear%20computational%20complexity%2C%20and%20we%20construct%20a%20novel%20hybrid%20CNN-Mamba%0Aframework.%20During%20training%2C%20the%20collaboration%20between%20the%20CNN%20branch%20and%20the%0AMamba%20branch%20in%20the%20proposed%20framework%20draws%20inspiration%20from%20each%20other%20based%0Aon%20the%20EGC%20strategy.%20Experiments%20demonstrate%20the%20competitiveness%20of%20the%0Aproposed%20method.%20Dataset%20and%20code%20will%20be%20available%20on%0Ahttps%3A//github.com/GtLinyer/MambaEviScrib.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.19370v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMambaEviScrib%253A%2520Mamba%2520and%2520Evidence-Guided%2520Consistency%2520Enhance%2520CNN%250A%2520%2520Robustness%2520for%2520Scribble-Based%2520Weakly%2520Supervised%2520Ultrasound%2520Image%2520Segmentation%26entry.906535625%3DXiaoxiang%2520Han%2520and%2520Xinyu%2520Li%2520and%2520Jiang%2520Shang%2520and%2520Yiman%2520Liu%2520and%2520Keyan%2520Chen%2520and%2520Shugong%2520Xu%2520and%2520Qiaohong%2520Liu%2520and%2520Qi%2520Zhang%26entry.1292438233%3D%2520%2520Segmenting%2520anatomical%2520structures%2520and%2520lesions%2520from%2520ultrasound%2520images%250Acontributes%2520to%2520disease%2520assessment.%2520Weakly%2520supervised%2520learning%2520%2528WSL%2529%2520based%2520on%250Asparse%2520annotation%2520has%2520achieved%2520encouraging%2520performance%2520and%2520demonstrated%2520the%250Apotential%2520to%2520reduce%2520annotation%2520costs.%2520This%2520study%2520attempts%2520to%2520introduce%250Ascribble-based%2520WSL%2520into%2520ultrasound%2520image%2520segmentation%2520tasks.%2520However%252C%250Aultrasound%2520images%2520often%2520suffer%2520from%2520poor%2520contrast%2520and%2520unclear%2520edges%252C%2520coupled%250Awith%2520insufficient%2520supervison%2520signals%2520for%2520edges%252C%2520posing%2520challenges%2520to%2520edge%250Aprediction.%2520Uncertainty%2520modeling%2520has%2520been%2520proven%2520to%2520facilitate%2520models%2520in%250Adealing%2520with%2520these%2520issues.%2520Nevertheless%252C%2520existing%2520uncertainty%2520estimation%250Aparadigms%2520are%2520not%2520robust%2520enough%2520and%2520often%2520filter%2520out%2520predictions%2520near%2520decision%250Aboundaries%252C%2520resulting%2520in%2520unstable%2520edge%2520predictions.%2520Therefore%252C%2520we%2520propose%250Aleveraging%2520predictions%2520near%2520decision%2520boundaries%2520effectively.%2520Specifically%252C%2520we%250Aintroduce%2520Dempster-Shafer%2520Theory%2520%2528DST%2529%2520of%2520evidence%2520to%2520design%2520an%2520Evidence-Guided%250AConsistency%2520strategy.%2520This%2520strategy%2520utilizes%2520high-evidence%2520predictions%252C%2520which%250Aare%2520more%2520likely%2520to%2520occur%2520near%2520high-density%2520regions%252C%2520to%2520guide%2520the%2520optimization%250Aof%2520low-evidence%2520predictions%2520that%2520may%2520appear%2520near%2520decision%2520boundaries.%250AFurthermore%252C%2520the%2520diverse%2520sizes%2520and%2520locations%2520of%2520lesions%2520in%2520ultrasound%2520images%250Apose%2520a%2520challenge%2520for%2520CNNs%2520with%2520local%2520receptive%2520fields%252C%2520as%2520they%2520struggle%2520to%250Amodel%2520global%2520information.%2520Therefore%252C%2520we%2520introduce%2520Visual%2520Mamba%2520based%2520on%250Astructured%2520state%2520space%2520sequence%2520models%252C%2520which%2520achieves%2520long-range%2520dependency%250Awith%2520linear%2520computational%2520complexity%252C%2520and%2520we%2520construct%2520a%2520novel%2520hybrid%2520CNN-Mamba%250Aframework.%2520During%2520training%252C%2520the%2520collaboration%2520between%2520the%2520CNN%2520branch%2520and%2520the%250AMamba%2520branch%2520in%2520the%2520proposed%2520framework%2520draws%2520inspiration%2520from%2520each%2520other%2520based%250Aon%2520the%2520EGC%2520strategy.%2520Experiments%2520demonstrate%2520the%2520competitiveness%2520of%2520the%250Aproposed%2520method.%2520Dataset%2520and%2520code%2520will%2520be%2520available%2520on%250Ahttps%253A//github.com/GtLinyer/MambaEviScrib.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.19370v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MambaEviScrib%3A%20Mamba%20and%20Evidence-Guided%20Consistency%20Enhance%20CNN%0A%20%20Robustness%20for%20Scribble-Based%20Weakly%20Supervised%20Ultrasound%20Image%20Segmentation&entry.906535625=Xiaoxiang%20Han%20and%20Xinyu%20Li%20and%20Jiang%20Shang%20and%20Yiman%20Liu%20and%20Keyan%20Chen%20and%20Shugong%20Xu%20and%20Qiaohong%20Liu%20and%20Qi%20Zhang&entry.1292438233=%20%20Segmenting%20anatomical%20structures%20and%20lesions%20from%20ultrasound%20images%0Acontributes%20to%20disease%20assessment.%20Weakly%20supervised%20learning%20%28WSL%29%20based%20on%0Asparse%20annotation%20has%20achieved%20encouraging%20performance%20and%20demonstrated%20the%0Apotential%20to%20reduce%20annotation%20costs.%20This%20study%20attempts%20to%20introduce%0Ascribble-based%20WSL%20into%20ultrasound%20image%20segmentation%20tasks.%20However%2C%0Aultrasound%20images%20often%20suffer%20from%20poor%20contrast%20and%20unclear%20edges%2C%20coupled%0Awith%20insufficient%20supervison%20signals%20for%20edges%2C%20posing%20challenges%20to%20edge%0Aprediction.%20Uncertainty%20modeling%20has%20been%20proven%20to%20facilitate%20models%20in%0Adealing%20with%20these%20issues.%20Nevertheless%2C%20existing%20uncertainty%20estimation%0Aparadigms%20are%20not%20robust%20enough%20and%20often%20filter%20out%20predictions%20near%20decision%0Aboundaries%2C%20resulting%20in%20unstable%20edge%20predictions.%20Therefore%2C%20we%20propose%0Aleveraging%20predictions%20near%20decision%20boundaries%20effectively.%20Specifically%2C%20we%0Aintroduce%20Dempster-Shafer%20Theory%20%28DST%29%20of%20evidence%20to%20design%20an%20Evidence-Guided%0AConsistency%20strategy.%20This%20strategy%20utilizes%20high-evidence%20predictions%2C%20which%0Aare%20more%20likely%20to%20occur%20near%20high-density%20regions%2C%20to%20guide%20the%20optimization%0Aof%20low-evidence%20predictions%20that%20may%20appear%20near%20decision%20boundaries.%0AFurthermore%2C%20the%20diverse%20sizes%20and%20locations%20of%20lesions%20in%20ultrasound%20images%0Apose%20a%20challenge%20for%20CNNs%20with%20local%20receptive%20fields%2C%20as%20they%20struggle%20to%0Amodel%20global%20information.%20Therefore%2C%20we%20introduce%20Visual%20Mamba%20based%20on%0Astructured%20state%20space%20sequence%20models%2C%20which%20achieves%20long-range%20dependency%0Awith%20linear%20computational%20complexity%2C%20and%20we%20construct%20a%20novel%20hybrid%20CNN-Mamba%0Aframework.%20During%20training%2C%20the%20collaboration%20between%20the%20CNN%20branch%20and%20the%0AMamba%20branch%20in%20the%20proposed%20framework%20draws%20inspiration%20from%20each%20other%20based%0Aon%20the%20EGC%20strategy.%20Experiments%20demonstrate%20the%20competitiveness%20of%20the%0Aproposed%20method.%20Dataset%20and%20code%20will%20be%20available%20on%0Ahttps%3A//github.com/GtLinyer/MambaEviScrib.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.19370v2&entry.124074799=Read"},
{"title": "Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self\n  Attention at the Threadblock Level", "author": "Ali Hassani and Wen-Mei Hwu and Humphrey Shi", "abstract": "  Neighborhood attention reduces the cost of self attention by restricting each\ntoken's attention span to its nearest neighbors. This restriction,\nparameterized by a window size and dilation factor, draws a spectrum of\npossible attention patterns between linear projection and self attention.\nNeighborhood attention, and more generally sliding window attention patterns,\nhave long been bounded by infrastructure, particularly in higher-rank spaces\n(2-D and 3-D), calling for the development of custom kernels, which have been\nlimited in either functionality, or performance, if not both. In this work, we\naim to massively improve upon existing infrastructure by providing two new\nmethods for implementing neighborhood attention. We first show that\nneighborhood attention can be represented as a batched GEMM problem, similar to\nstandard attention, and implement it for 1-D and 2-D neighborhood attention.\nThese kernels on average provide 895% and 272% improvement in full precision\nruntime compared to existing naive CUDA kernels for 1-D and 2-D neighborhood\nattention respectively. We find that aside from being heavily bound by memory\nbandwidth, certain inherent inefficiencies exist in all unfused implementations\nof neighborhood attention, which in most cases undo their theoretical\nefficiency gain. Motivated by the progress made into fused dot-product\nattention kernels, we developed fused neighborhood attention; an adaptation of\nfused dot-product attention kernels that allow fine-grained control over\nattention across different spatial axes. Known for reducing the quadratic time\ncomplexity of self attention to a linear complexity, neighborhood attention can\nnow enjoy a reduced and constant memory footprint, and record-breaking half\nprecision runtime. We observe that our fused implementation successfully\ncircumvents some of the unavoidable inefficiencies in unfused\nimplementations...\n", "link": "http://arxiv.org/abs/2403.04690v3", "date": "2024-10-31", "relevancy": 2.1534, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5904}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5044}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Faster%20Neighborhood%20Attention%3A%20Reducing%20the%20O%28n%5E2%29%20Cost%20of%20Self%0A%20%20Attention%20at%20the%20Threadblock%20Level&body=Title%3A%20Faster%20Neighborhood%20Attention%3A%20Reducing%20the%20O%28n%5E2%29%20Cost%20of%20Self%0A%20%20Attention%20at%20the%20Threadblock%20Level%0AAuthor%3A%20Ali%20Hassani%20and%20Wen-Mei%20Hwu%20and%20Humphrey%20Shi%0AAbstract%3A%20%20%20Neighborhood%20attention%20reduces%20the%20cost%20of%20self%20attention%20by%20restricting%20each%0Atoken%27s%20attention%20span%20to%20its%20nearest%20neighbors.%20This%20restriction%2C%0Aparameterized%20by%20a%20window%20size%20and%20dilation%20factor%2C%20draws%20a%20spectrum%20of%0Apossible%20attention%20patterns%20between%20linear%20projection%20and%20self%20attention.%0ANeighborhood%20attention%2C%20and%20more%20generally%20sliding%20window%20attention%20patterns%2C%0Ahave%20long%20been%20bounded%20by%20infrastructure%2C%20particularly%20in%20higher-rank%20spaces%0A%282-D%20and%203-D%29%2C%20calling%20for%20the%20development%20of%20custom%20kernels%2C%20which%20have%20been%0Alimited%20in%20either%20functionality%2C%20or%20performance%2C%20if%20not%20both.%20In%20this%20work%2C%20we%0Aaim%20to%20massively%20improve%20upon%20existing%20infrastructure%20by%20providing%20two%20new%0Amethods%20for%20implementing%20neighborhood%20attention.%20We%20first%20show%20that%0Aneighborhood%20attention%20can%20be%20represented%20as%20a%20batched%20GEMM%20problem%2C%20similar%20to%0Astandard%20attention%2C%20and%20implement%20it%20for%201-D%20and%202-D%20neighborhood%20attention.%0AThese%20kernels%20on%20average%20provide%20895%25%20and%20272%25%20improvement%20in%20full%20precision%0Aruntime%20compared%20to%20existing%20naive%20CUDA%20kernels%20for%201-D%20and%202-D%20neighborhood%0Aattention%20respectively.%20We%20find%20that%20aside%20from%20being%20heavily%20bound%20by%20memory%0Abandwidth%2C%20certain%20inherent%20inefficiencies%20exist%20in%20all%20unfused%20implementations%0Aof%20neighborhood%20attention%2C%20which%20in%20most%20cases%20undo%20their%20theoretical%0Aefficiency%20gain.%20Motivated%20by%20the%20progress%20made%20into%20fused%20dot-product%0Aattention%20kernels%2C%20we%20developed%20fused%20neighborhood%20attention%3B%20an%20adaptation%20of%0Afused%20dot-product%20attention%20kernels%20that%20allow%20fine-grained%20control%20over%0Aattention%20across%20different%20spatial%20axes.%20Known%20for%20reducing%20the%20quadratic%20time%0Acomplexity%20of%20self%20attention%20to%20a%20linear%20complexity%2C%20neighborhood%20attention%20can%0Anow%20enjoy%20a%20reduced%20and%20constant%20memory%20footprint%2C%20and%20record-breaking%20half%0Aprecision%20runtime.%20We%20observe%20that%20our%20fused%20implementation%20successfully%0Acircumvents%20some%20of%20the%20unavoidable%20inefficiencies%20in%20unfused%0Aimplementations...%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04690v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaster%2520Neighborhood%2520Attention%253A%2520Reducing%2520the%2520O%2528n%255E2%2529%2520Cost%2520of%2520Self%250A%2520%2520Attention%2520at%2520the%2520Threadblock%2520Level%26entry.906535625%3DAli%2520Hassani%2520and%2520Wen-Mei%2520Hwu%2520and%2520Humphrey%2520Shi%26entry.1292438233%3D%2520%2520Neighborhood%2520attention%2520reduces%2520the%2520cost%2520of%2520self%2520attention%2520by%2520restricting%2520each%250Atoken%2527s%2520attention%2520span%2520to%2520its%2520nearest%2520neighbors.%2520This%2520restriction%252C%250Aparameterized%2520by%2520a%2520window%2520size%2520and%2520dilation%2520factor%252C%2520draws%2520a%2520spectrum%2520of%250Apossible%2520attention%2520patterns%2520between%2520linear%2520projection%2520and%2520self%2520attention.%250ANeighborhood%2520attention%252C%2520and%2520more%2520generally%2520sliding%2520window%2520attention%2520patterns%252C%250Ahave%2520long%2520been%2520bounded%2520by%2520infrastructure%252C%2520particularly%2520in%2520higher-rank%2520spaces%250A%25282-D%2520and%25203-D%2529%252C%2520calling%2520for%2520the%2520development%2520of%2520custom%2520kernels%252C%2520which%2520have%2520been%250Alimited%2520in%2520either%2520functionality%252C%2520or%2520performance%252C%2520if%2520not%2520both.%2520In%2520this%2520work%252C%2520we%250Aaim%2520to%2520massively%2520improve%2520upon%2520existing%2520infrastructure%2520by%2520providing%2520two%2520new%250Amethods%2520for%2520implementing%2520neighborhood%2520attention.%2520We%2520first%2520show%2520that%250Aneighborhood%2520attention%2520can%2520be%2520represented%2520as%2520a%2520batched%2520GEMM%2520problem%252C%2520similar%2520to%250Astandard%2520attention%252C%2520and%2520implement%2520it%2520for%25201-D%2520and%25202-D%2520neighborhood%2520attention.%250AThese%2520kernels%2520on%2520average%2520provide%2520895%2525%2520and%2520272%2525%2520improvement%2520in%2520full%2520precision%250Aruntime%2520compared%2520to%2520existing%2520naive%2520CUDA%2520kernels%2520for%25201-D%2520and%25202-D%2520neighborhood%250Aattention%2520respectively.%2520We%2520find%2520that%2520aside%2520from%2520being%2520heavily%2520bound%2520by%2520memory%250Abandwidth%252C%2520certain%2520inherent%2520inefficiencies%2520exist%2520in%2520all%2520unfused%2520implementations%250Aof%2520neighborhood%2520attention%252C%2520which%2520in%2520most%2520cases%2520undo%2520their%2520theoretical%250Aefficiency%2520gain.%2520Motivated%2520by%2520the%2520progress%2520made%2520into%2520fused%2520dot-product%250Aattention%2520kernels%252C%2520we%2520developed%2520fused%2520neighborhood%2520attention%253B%2520an%2520adaptation%2520of%250Afused%2520dot-product%2520attention%2520kernels%2520that%2520allow%2520fine-grained%2520control%2520over%250Aattention%2520across%2520different%2520spatial%2520axes.%2520Known%2520for%2520reducing%2520the%2520quadratic%2520time%250Acomplexity%2520of%2520self%2520attention%2520to%2520a%2520linear%2520complexity%252C%2520neighborhood%2520attention%2520can%250Anow%2520enjoy%2520a%2520reduced%2520and%2520constant%2520memory%2520footprint%252C%2520and%2520record-breaking%2520half%250Aprecision%2520runtime.%2520We%2520observe%2520that%2520our%2520fused%2520implementation%2520successfully%250Acircumvents%2520some%2520of%2520the%2520unavoidable%2520inefficiencies%2520in%2520unfused%250Aimplementations...%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.04690v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faster%20Neighborhood%20Attention%3A%20Reducing%20the%20O%28n%5E2%29%20Cost%20of%20Self%0A%20%20Attention%20at%20the%20Threadblock%20Level&entry.906535625=Ali%20Hassani%20and%20Wen-Mei%20Hwu%20and%20Humphrey%20Shi&entry.1292438233=%20%20Neighborhood%20attention%20reduces%20the%20cost%20of%20self%20attention%20by%20restricting%20each%0Atoken%27s%20attention%20span%20to%20its%20nearest%20neighbors.%20This%20restriction%2C%0Aparameterized%20by%20a%20window%20size%20and%20dilation%20factor%2C%20draws%20a%20spectrum%20of%0Apossible%20attention%20patterns%20between%20linear%20projection%20and%20self%20attention.%0ANeighborhood%20attention%2C%20and%20more%20generally%20sliding%20window%20attention%20patterns%2C%0Ahave%20long%20been%20bounded%20by%20infrastructure%2C%20particularly%20in%20higher-rank%20spaces%0A%282-D%20and%203-D%29%2C%20calling%20for%20the%20development%20of%20custom%20kernels%2C%20which%20have%20been%0Alimited%20in%20either%20functionality%2C%20or%20performance%2C%20if%20not%20both.%20In%20this%20work%2C%20we%0Aaim%20to%20massively%20improve%20upon%20existing%20infrastructure%20by%20providing%20two%20new%0Amethods%20for%20implementing%20neighborhood%20attention.%20We%20first%20show%20that%0Aneighborhood%20attention%20can%20be%20represented%20as%20a%20batched%20GEMM%20problem%2C%20similar%20to%0Astandard%20attention%2C%20and%20implement%20it%20for%201-D%20and%202-D%20neighborhood%20attention.%0AThese%20kernels%20on%20average%20provide%20895%25%20and%20272%25%20improvement%20in%20full%20precision%0Aruntime%20compared%20to%20existing%20naive%20CUDA%20kernels%20for%201-D%20and%202-D%20neighborhood%0Aattention%20respectively.%20We%20find%20that%20aside%20from%20being%20heavily%20bound%20by%20memory%0Abandwidth%2C%20certain%20inherent%20inefficiencies%20exist%20in%20all%20unfused%20implementations%0Aof%20neighborhood%20attention%2C%20which%20in%20most%20cases%20undo%20their%20theoretical%0Aefficiency%20gain.%20Motivated%20by%20the%20progress%20made%20into%20fused%20dot-product%0Aattention%20kernels%2C%20we%20developed%20fused%20neighborhood%20attention%3B%20an%20adaptation%20of%0Afused%20dot-product%20attention%20kernels%20that%20allow%20fine-grained%20control%20over%0Aattention%20across%20different%20spatial%20axes.%20Known%20for%20reducing%20the%20quadratic%20time%0Acomplexity%20of%20self%20attention%20to%20a%20linear%20complexity%2C%20neighborhood%20attention%20can%0Anow%20enjoy%20a%20reduced%20and%20constant%20memory%20footprint%2C%20and%20record-breaking%20half%0Aprecision%20runtime.%20We%20observe%20that%20our%20fused%20implementation%20successfully%0Acircumvents%20some%20of%20the%20unavoidable%20inefficiencies%20in%20unfused%0Aimplementations...%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04690v3&entry.124074799=Read"},
{"title": "OmniJARVIS: Unified Vision-Language-Action Tokenization Enables\n  Open-World Instruction Following Agents", "author": "Zihao Wang and Shaofei Cai and Zhancun Mu and Haowei Lin and Ceyao Zhang and Xuejie Liu and Qing Li and Anji Liu and Xiaojian Ma and Yitao Liang", "abstract": "  This paper presents OmniJARVIS, a novel Vision-Language-Action (VLA) model\nfor open-world instruction-following agents in Minecraft. Compared to prior\nworks that either emit textual goals to separate controllers or produce the\ncontrol command directly, OmniJARVIS seeks a different path to ensure both\nstrong reasoning and efficient decision-making capabilities via unified\ntokenization of multimodal interaction data. First, we introduce a\nself-supervised approach to learn a behavior encoder that produces discretized\ntokens for behavior trajectories $\\tau = \\{o_0, a_0, \\dots\\}$ and an imitation\nlearning policy decoder conditioned on these tokens. These additional behavior\ntokens will be augmented to the vocabulary of pretrained Multimodal Language\nModels. With this encoder, we then pack long-term multimodal interactions\ninvolving task instructions, memories, thoughts, observations, textual\nresponses, behavior trajectories, etc into unified token sequences and model\nthem with autoregressive transformers. Thanks to the semantically meaningful\nbehavior tokens, the resulting VLA model, OmniJARVIS, can reason (by producing\nchain-of-thoughts), plan, answer questions, and act (by producing behavior\ntokens for the imitation learning policy decoder). OmniJARVIS demonstrates\nexcellent performances on a comprehensive collection of atomic, programmatic,\nand open-ended tasks in open-world Minecraft. Our analysis further unveils the\ncrucial design principles in interaction data formation, unified tokenization,\nand its scaling potentials. The dataset, models, and code will be released at\nhttps://craftjarvis.org/OmniJARVIS.\n", "link": "http://arxiv.org/abs/2407.00114v2", "date": "2024-10-31", "relevancy": 2.1454, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5378}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5378}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniJARVIS%3A%20Unified%20Vision-Language-Action%20Tokenization%20Enables%0A%20%20Open-World%20Instruction%20Following%20Agents&body=Title%3A%20OmniJARVIS%3A%20Unified%20Vision-Language-Action%20Tokenization%20Enables%0A%20%20Open-World%20Instruction%20Following%20Agents%0AAuthor%3A%20Zihao%20Wang%20and%20Shaofei%20Cai%20and%20Zhancun%20Mu%20and%20Haowei%20Lin%20and%20Ceyao%20Zhang%20and%20Xuejie%20Liu%20and%20Qing%20Li%20and%20Anji%20Liu%20and%20Xiaojian%20Ma%20and%20Yitao%20Liang%0AAbstract%3A%20%20%20This%20paper%20presents%20OmniJARVIS%2C%20a%20novel%20Vision-Language-Action%20%28VLA%29%20model%0Afor%20open-world%20instruction-following%20agents%20in%20Minecraft.%20Compared%20to%20prior%0Aworks%20that%20either%20emit%20textual%20goals%20to%20separate%20controllers%20or%20produce%20the%0Acontrol%20command%20directly%2C%20OmniJARVIS%20seeks%20a%20different%20path%20to%20ensure%20both%0Astrong%20reasoning%20and%20efficient%20decision-making%20capabilities%20via%20unified%0Atokenization%20of%20multimodal%20interaction%20data.%20First%2C%20we%20introduce%20a%0Aself-supervised%20approach%20to%20learn%20a%20behavior%20encoder%20that%20produces%20discretized%0Atokens%20for%20behavior%20trajectories%20%24%5Ctau%20%3D%20%5C%7Bo_0%2C%20a_0%2C%20%5Cdots%5C%7D%24%20and%20an%20imitation%0Alearning%20policy%20decoder%20conditioned%20on%20these%20tokens.%20These%20additional%20behavior%0Atokens%20will%20be%20augmented%20to%20the%20vocabulary%20of%20pretrained%20Multimodal%20Language%0AModels.%20With%20this%20encoder%2C%20we%20then%20pack%20long-term%20multimodal%20interactions%0Ainvolving%20task%20instructions%2C%20memories%2C%20thoughts%2C%20observations%2C%20textual%0Aresponses%2C%20behavior%20trajectories%2C%20etc%20into%20unified%20token%20sequences%20and%20model%0Athem%20with%20autoregressive%20transformers.%20Thanks%20to%20the%20semantically%20meaningful%0Abehavior%20tokens%2C%20the%20resulting%20VLA%20model%2C%20OmniJARVIS%2C%20can%20reason%20%28by%20producing%0Achain-of-thoughts%29%2C%20plan%2C%20answer%20questions%2C%20and%20act%20%28by%20producing%20behavior%0Atokens%20for%20the%20imitation%20learning%20policy%20decoder%29.%20OmniJARVIS%20demonstrates%0Aexcellent%20performances%20on%20a%20comprehensive%20collection%20of%20atomic%2C%20programmatic%2C%0Aand%20open-ended%20tasks%20in%20open-world%20Minecraft.%20Our%20analysis%20further%20unveils%20the%0Acrucial%20design%20principles%20in%20interaction%20data%20formation%2C%20unified%20tokenization%2C%0Aand%20its%20scaling%20potentials.%20The%20dataset%2C%20models%2C%20and%20code%20will%20be%20released%20at%0Ahttps%3A//craftjarvis.org/OmniJARVIS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00114v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniJARVIS%253A%2520Unified%2520Vision-Language-Action%2520Tokenization%2520Enables%250A%2520%2520Open-World%2520Instruction%2520Following%2520Agents%26entry.906535625%3DZihao%2520Wang%2520and%2520Shaofei%2520Cai%2520and%2520Zhancun%2520Mu%2520and%2520Haowei%2520Lin%2520and%2520Ceyao%2520Zhang%2520and%2520Xuejie%2520Liu%2520and%2520Qing%2520Li%2520and%2520Anji%2520Liu%2520and%2520Xiaojian%2520Ma%2520and%2520Yitao%2520Liang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520OmniJARVIS%252C%2520a%2520novel%2520Vision-Language-Action%2520%2528VLA%2529%2520model%250Afor%2520open-world%2520instruction-following%2520agents%2520in%2520Minecraft.%2520Compared%2520to%2520prior%250Aworks%2520that%2520either%2520emit%2520textual%2520goals%2520to%2520separate%2520controllers%2520or%2520produce%2520the%250Acontrol%2520command%2520directly%252C%2520OmniJARVIS%2520seeks%2520a%2520different%2520path%2520to%2520ensure%2520both%250Astrong%2520reasoning%2520and%2520efficient%2520decision-making%2520capabilities%2520via%2520unified%250Atokenization%2520of%2520multimodal%2520interaction%2520data.%2520First%252C%2520we%2520introduce%2520a%250Aself-supervised%2520approach%2520to%2520learn%2520a%2520behavior%2520encoder%2520that%2520produces%2520discretized%250Atokens%2520for%2520behavior%2520trajectories%2520%2524%255Ctau%2520%253D%2520%255C%257Bo_0%252C%2520a_0%252C%2520%255Cdots%255C%257D%2524%2520and%2520an%2520imitation%250Alearning%2520policy%2520decoder%2520conditioned%2520on%2520these%2520tokens.%2520These%2520additional%2520behavior%250Atokens%2520will%2520be%2520augmented%2520to%2520the%2520vocabulary%2520of%2520pretrained%2520Multimodal%2520Language%250AModels.%2520With%2520this%2520encoder%252C%2520we%2520then%2520pack%2520long-term%2520multimodal%2520interactions%250Ainvolving%2520task%2520instructions%252C%2520memories%252C%2520thoughts%252C%2520observations%252C%2520textual%250Aresponses%252C%2520behavior%2520trajectories%252C%2520etc%2520into%2520unified%2520token%2520sequences%2520and%2520model%250Athem%2520with%2520autoregressive%2520transformers.%2520Thanks%2520to%2520the%2520semantically%2520meaningful%250Abehavior%2520tokens%252C%2520the%2520resulting%2520VLA%2520model%252C%2520OmniJARVIS%252C%2520can%2520reason%2520%2528by%2520producing%250Achain-of-thoughts%2529%252C%2520plan%252C%2520answer%2520questions%252C%2520and%2520act%2520%2528by%2520producing%2520behavior%250Atokens%2520for%2520the%2520imitation%2520learning%2520policy%2520decoder%2529.%2520OmniJARVIS%2520demonstrates%250Aexcellent%2520performances%2520on%2520a%2520comprehensive%2520collection%2520of%2520atomic%252C%2520programmatic%252C%250Aand%2520open-ended%2520tasks%2520in%2520open-world%2520Minecraft.%2520Our%2520analysis%2520further%2520unveils%2520the%250Acrucial%2520design%2520principles%2520in%2520interaction%2520data%2520formation%252C%2520unified%2520tokenization%252C%250Aand%2520its%2520scaling%2520potentials.%2520The%2520dataset%252C%2520models%252C%2520and%2520code%2520will%2520be%2520released%2520at%250Ahttps%253A//craftjarvis.org/OmniJARVIS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00114v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniJARVIS%3A%20Unified%20Vision-Language-Action%20Tokenization%20Enables%0A%20%20Open-World%20Instruction%20Following%20Agents&entry.906535625=Zihao%20Wang%20and%20Shaofei%20Cai%20and%20Zhancun%20Mu%20and%20Haowei%20Lin%20and%20Ceyao%20Zhang%20and%20Xuejie%20Liu%20and%20Qing%20Li%20and%20Anji%20Liu%20and%20Xiaojian%20Ma%20and%20Yitao%20Liang&entry.1292438233=%20%20This%20paper%20presents%20OmniJARVIS%2C%20a%20novel%20Vision-Language-Action%20%28VLA%29%20model%0Afor%20open-world%20instruction-following%20agents%20in%20Minecraft.%20Compared%20to%20prior%0Aworks%20that%20either%20emit%20textual%20goals%20to%20separate%20controllers%20or%20produce%20the%0Acontrol%20command%20directly%2C%20OmniJARVIS%20seeks%20a%20different%20path%20to%20ensure%20both%0Astrong%20reasoning%20and%20efficient%20decision-making%20capabilities%20via%20unified%0Atokenization%20of%20multimodal%20interaction%20data.%20First%2C%20we%20introduce%20a%0Aself-supervised%20approach%20to%20learn%20a%20behavior%20encoder%20that%20produces%20discretized%0Atokens%20for%20behavior%20trajectories%20%24%5Ctau%20%3D%20%5C%7Bo_0%2C%20a_0%2C%20%5Cdots%5C%7D%24%20and%20an%20imitation%0Alearning%20policy%20decoder%20conditioned%20on%20these%20tokens.%20These%20additional%20behavior%0Atokens%20will%20be%20augmented%20to%20the%20vocabulary%20of%20pretrained%20Multimodal%20Language%0AModels.%20With%20this%20encoder%2C%20we%20then%20pack%20long-term%20multimodal%20interactions%0Ainvolving%20task%20instructions%2C%20memories%2C%20thoughts%2C%20observations%2C%20textual%0Aresponses%2C%20behavior%20trajectories%2C%20etc%20into%20unified%20token%20sequences%20and%20model%0Athem%20with%20autoregressive%20transformers.%20Thanks%20to%20the%20semantically%20meaningful%0Abehavior%20tokens%2C%20the%20resulting%20VLA%20model%2C%20OmniJARVIS%2C%20can%20reason%20%28by%20producing%0Achain-of-thoughts%29%2C%20plan%2C%20answer%20questions%2C%20and%20act%20%28by%20producing%20behavior%0Atokens%20for%20the%20imitation%20learning%20policy%20decoder%29.%20OmniJARVIS%20demonstrates%0Aexcellent%20performances%20on%20a%20comprehensive%20collection%20of%20atomic%2C%20programmatic%2C%0Aand%20open-ended%20tasks%20in%20open-world%20Minecraft.%20Our%20analysis%20further%20unveils%20the%0Acrucial%20design%20principles%20in%20interaction%20data%20formation%2C%20unified%20tokenization%2C%0Aand%20its%20scaling%20potentials.%20The%20dataset%2C%20models%2C%20and%20code%20will%20be%20released%20at%0Ahttps%3A//craftjarvis.org/OmniJARVIS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00114v2&entry.124074799=Read"},
{"title": "Chasing Better Deep Image Priors between Over- and\n  Under-parameterization", "author": "Qiming Wu and Xiaohan Chen and Yifan Jiang and Zhangyang Wang", "abstract": "  Deep Neural Networks (DNNs) are well-known to act as over-parameterized deep\nimage priors (DIP) that regularize various image inverse problems. Meanwhile,\nresearchers also proposed extremely compact, under-parameterized image priors\n(e.g., deep decoder) that are strikingly competent for image restoration too,\ndespite a loss of accuracy. These two extremes push us to think whether there\nexists a better solution in the middle: between over- and under-parameterized\nimage priors, can one identify \"intermediate\" parameterized image priors that\nachieve better trade-offs between performance, efficiency, and even preserving\nstrong transferability? Drawing inspirations from the lottery ticket hypothesis\n(LTH), we conjecture and study a novel \"lottery image prior\" (LIP) by\nexploiting DNN inherent sparsity, stated as: given an over-parameterized\nDNN-based image prior, it will contain a sparse subnetwork that can be trained\nin isolation, to match the original DNN's performance when being applied as a\nprior to various image inverse problems. Our results validate the superiority\nof LIPs: we can successfully locate the LIP subnetworks from over-parameterized\nDIPs at substantial sparsity ranges. Those LIP subnetworks significantly\noutperform deep decoders under comparably compact model sizes (by often fully\npreserving the effectiveness of their over-parameterized counterparts), and\nthey also possess high transferability across different images as well as\nrestoration task types. Besides, we also extend LIP to compressive sensing\nimage reconstruction, where a pre-trained GAN generator is used as the prior\n(in contrast to untrained DIP or deep decoder), and confirm its validity in\nthis setting too. To our best knowledge, this is the first time that LTH is\ndemonstrated to be relevant in the context of inverse problems or image priors.\n", "link": "http://arxiv.org/abs/2410.24187v1", "date": "2024-10-31", "relevancy": 2.1276, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5421}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5319}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chasing%20Better%20Deep%20Image%20Priors%20between%20Over-%20and%0A%20%20Under-parameterization&body=Title%3A%20Chasing%20Better%20Deep%20Image%20Priors%20between%20Over-%20and%0A%20%20Under-parameterization%0AAuthor%3A%20Qiming%20Wu%20and%20Xiaohan%20Chen%20and%20Yifan%20Jiang%20and%20Zhangyang%20Wang%0AAbstract%3A%20%20%20Deep%20Neural%20Networks%20%28DNNs%29%20are%20well-known%20to%20act%20as%20over-parameterized%20deep%0Aimage%20priors%20%28DIP%29%20that%20regularize%20various%20image%20inverse%20problems.%20Meanwhile%2C%0Aresearchers%20also%20proposed%20extremely%20compact%2C%20under-parameterized%20image%20priors%0A%28e.g.%2C%20deep%20decoder%29%20that%20are%20strikingly%20competent%20for%20image%20restoration%20too%2C%0Adespite%20a%20loss%20of%20accuracy.%20These%20two%20extremes%20push%20us%20to%20think%20whether%20there%0Aexists%20a%20better%20solution%20in%20the%20middle%3A%20between%20over-%20and%20under-parameterized%0Aimage%20priors%2C%20can%20one%20identify%20%22intermediate%22%20parameterized%20image%20priors%20that%0Aachieve%20better%20trade-offs%20between%20performance%2C%20efficiency%2C%20and%20even%20preserving%0Astrong%20transferability%3F%20Drawing%20inspirations%20from%20the%20lottery%20ticket%20hypothesis%0A%28LTH%29%2C%20we%20conjecture%20and%20study%20a%20novel%20%22lottery%20image%20prior%22%20%28LIP%29%20by%0Aexploiting%20DNN%20inherent%20sparsity%2C%20stated%20as%3A%20given%20an%20over-parameterized%0ADNN-based%20image%20prior%2C%20it%20will%20contain%20a%20sparse%20subnetwork%20that%20can%20be%20trained%0Ain%20isolation%2C%20to%20match%20the%20original%20DNN%27s%20performance%20when%20being%20applied%20as%20a%0Aprior%20to%20various%20image%20inverse%20problems.%20Our%20results%20validate%20the%20superiority%0Aof%20LIPs%3A%20we%20can%20successfully%20locate%20the%20LIP%20subnetworks%20from%20over-parameterized%0ADIPs%20at%20substantial%20sparsity%20ranges.%20Those%20LIP%20subnetworks%20significantly%0Aoutperform%20deep%20decoders%20under%20comparably%20compact%20model%20sizes%20%28by%20often%20fully%0Apreserving%20the%20effectiveness%20of%20their%20over-parameterized%20counterparts%29%2C%20and%0Athey%20also%20possess%20high%20transferability%20across%20different%20images%20as%20well%20as%0Arestoration%20task%20types.%20Besides%2C%20we%20also%20extend%20LIP%20to%20compressive%20sensing%0Aimage%20reconstruction%2C%20where%20a%20pre-trained%20GAN%20generator%20is%20used%20as%20the%20prior%0A%28in%20contrast%20to%20untrained%20DIP%20or%20deep%20decoder%29%2C%20and%20confirm%20its%20validity%20in%0Athis%20setting%20too.%20To%20our%20best%20knowledge%2C%20this%20is%20the%20first%20time%20that%20LTH%20is%0Ademonstrated%20to%20be%20relevant%20in%20the%20context%20of%20inverse%20problems%20or%20image%20priors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24187v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChasing%2520Better%2520Deep%2520Image%2520Priors%2520between%2520Over-%2520and%250A%2520%2520Under-parameterization%26entry.906535625%3DQiming%2520Wu%2520and%2520Xiaohan%2520Chen%2520and%2520Yifan%2520Jiang%2520and%2520Zhangyang%2520Wang%26entry.1292438233%3D%2520%2520Deep%2520Neural%2520Networks%2520%2528DNNs%2529%2520are%2520well-known%2520to%2520act%2520as%2520over-parameterized%2520deep%250Aimage%2520priors%2520%2528DIP%2529%2520that%2520regularize%2520various%2520image%2520inverse%2520problems.%2520Meanwhile%252C%250Aresearchers%2520also%2520proposed%2520extremely%2520compact%252C%2520under-parameterized%2520image%2520priors%250A%2528e.g.%252C%2520deep%2520decoder%2529%2520that%2520are%2520strikingly%2520competent%2520for%2520image%2520restoration%2520too%252C%250Adespite%2520a%2520loss%2520of%2520accuracy.%2520These%2520two%2520extremes%2520push%2520us%2520to%2520think%2520whether%2520there%250Aexists%2520a%2520better%2520solution%2520in%2520the%2520middle%253A%2520between%2520over-%2520and%2520under-parameterized%250Aimage%2520priors%252C%2520can%2520one%2520identify%2520%2522intermediate%2522%2520parameterized%2520image%2520priors%2520that%250Aachieve%2520better%2520trade-offs%2520between%2520performance%252C%2520efficiency%252C%2520and%2520even%2520preserving%250Astrong%2520transferability%253F%2520Drawing%2520inspirations%2520from%2520the%2520lottery%2520ticket%2520hypothesis%250A%2528LTH%2529%252C%2520we%2520conjecture%2520and%2520study%2520a%2520novel%2520%2522lottery%2520image%2520prior%2522%2520%2528LIP%2529%2520by%250Aexploiting%2520DNN%2520inherent%2520sparsity%252C%2520stated%2520as%253A%2520given%2520an%2520over-parameterized%250ADNN-based%2520image%2520prior%252C%2520it%2520will%2520contain%2520a%2520sparse%2520subnetwork%2520that%2520can%2520be%2520trained%250Ain%2520isolation%252C%2520to%2520match%2520the%2520original%2520DNN%2527s%2520performance%2520when%2520being%2520applied%2520as%2520a%250Aprior%2520to%2520various%2520image%2520inverse%2520problems.%2520Our%2520results%2520validate%2520the%2520superiority%250Aof%2520LIPs%253A%2520we%2520can%2520successfully%2520locate%2520the%2520LIP%2520subnetworks%2520from%2520over-parameterized%250ADIPs%2520at%2520substantial%2520sparsity%2520ranges.%2520Those%2520LIP%2520subnetworks%2520significantly%250Aoutperform%2520deep%2520decoders%2520under%2520comparably%2520compact%2520model%2520sizes%2520%2528by%2520often%2520fully%250Apreserving%2520the%2520effectiveness%2520of%2520their%2520over-parameterized%2520counterparts%2529%252C%2520and%250Athey%2520also%2520possess%2520high%2520transferability%2520across%2520different%2520images%2520as%2520well%2520as%250Arestoration%2520task%2520types.%2520Besides%252C%2520we%2520also%2520extend%2520LIP%2520to%2520compressive%2520sensing%250Aimage%2520reconstruction%252C%2520where%2520a%2520pre-trained%2520GAN%2520generator%2520is%2520used%2520as%2520the%2520prior%250A%2528in%2520contrast%2520to%2520untrained%2520DIP%2520or%2520deep%2520decoder%2529%252C%2520and%2520confirm%2520its%2520validity%2520in%250Athis%2520setting%2520too.%2520To%2520our%2520best%2520knowledge%252C%2520this%2520is%2520the%2520first%2520time%2520that%2520LTH%2520is%250Ademonstrated%2520to%2520be%2520relevant%2520in%2520the%2520context%2520of%2520inverse%2520problems%2520or%2520image%2520priors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24187v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chasing%20Better%20Deep%20Image%20Priors%20between%20Over-%20and%0A%20%20Under-parameterization&entry.906535625=Qiming%20Wu%20and%20Xiaohan%20Chen%20and%20Yifan%20Jiang%20and%20Zhangyang%20Wang&entry.1292438233=%20%20Deep%20Neural%20Networks%20%28DNNs%29%20are%20well-known%20to%20act%20as%20over-parameterized%20deep%0Aimage%20priors%20%28DIP%29%20that%20regularize%20various%20image%20inverse%20problems.%20Meanwhile%2C%0Aresearchers%20also%20proposed%20extremely%20compact%2C%20under-parameterized%20image%20priors%0A%28e.g.%2C%20deep%20decoder%29%20that%20are%20strikingly%20competent%20for%20image%20restoration%20too%2C%0Adespite%20a%20loss%20of%20accuracy.%20These%20two%20extremes%20push%20us%20to%20think%20whether%20there%0Aexists%20a%20better%20solution%20in%20the%20middle%3A%20between%20over-%20and%20under-parameterized%0Aimage%20priors%2C%20can%20one%20identify%20%22intermediate%22%20parameterized%20image%20priors%20that%0Aachieve%20better%20trade-offs%20between%20performance%2C%20efficiency%2C%20and%20even%20preserving%0Astrong%20transferability%3F%20Drawing%20inspirations%20from%20the%20lottery%20ticket%20hypothesis%0A%28LTH%29%2C%20we%20conjecture%20and%20study%20a%20novel%20%22lottery%20image%20prior%22%20%28LIP%29%20by%0Aexploiting%20DNN%20inherent%20sparsity%2C%20stated%20as%3A%20given%20an%20over-parameterized%0ADNN-based%20image%20prior%2C%20it%20will%20contain%20a%20sparse%20subnetwork%20that%20can%20be%20trained%0Ain%20isolation%2C%20to%20match%20the%20original%20DNN%27s%20performance%20when%20being%20applied%20as%20a%0Aprior%20to%20various%20image%20inverse%20problems.%20Our%20results%20validate%20the%20superiority%0Aof%20LIPs%3A%20we%20can%20successfully%20locate%20the%20LIP%20subnetworks%20from%20over-parameterized%0ADIPs%20at%20substantial%20sparsity%20ranges.%20Those%20LIP%20subnetworks%20significantly%0Aoutperform%20deep%20decoders%20under%20comparably%20compact%20model%20sizes%20%28by%20often%20fully%0Apreserving%20the%20effectiveness%20of%20their%20over-parameterized%20counterparts%29%2C%20and%0Athey%20also%20possess%20high%20transferability%20across%20different%20images%20as%20well%20as%0Arestoration%20task%20types.%20Besides%2C%20we%20also%20extend%20LIP%20to%20compressive%20sensing%0Aimage%20reconstruction%2C%20where%20a%20pre-trained%20GAN%20generator%20is%20used%20as%20the%20prior%0A%28in%20contrast%20to%20untrained%20DIP%20or%20deep%20decoder%29%2C%20and%20confirm%20its%20validity%20in%0Athis%20setting%20too.%20To%20our%20best%20knowledge%2C%20this%20is%20the%20first%20time%20that%20LTH%20is%0Ademonstrated%20to%20be%20relevant%20in%20the%20context%20of%20inverse%20problems%20or%20image%20priors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24187v1&entry.124074799=Read"},
{"title": "Breaking the Hourglass Phenomenon of Residual Quantization: Enhancing\n  the Upper Bound of Generative Retrieval", "author": "Zhirui Kuai and Zuxu Chen and Huimu Wang and Mingming Li and Dadong Miao and Binbin Wang and Xusong Chen and Li Kuang and Yuxing Han and Jiaxing Wang and Guoyu Tang and Lin Liu and Songlin Wang and Jingwei Zhuo", "abstract": "  Generative retrieval (GR) has emerged as a transformative paradigm in search\nand recommender systems, leveraging numeric-based identifier representations to\nenhance efficiency and generalization. Notably, methods like TIGER employing\nResidual Quantization-based Semantic Identifiers (RQ-SID), have shown\nsignificant promise in e-commerce scenarios by effectively managing item IDs.\nHowever, a critical issue termed the \"\\textbf{Hourglass}\" phenomenon, occurs in\nRQ-SID, where intermediate codebook tokens become overly concentrated,\nhindering the full utilization of generative retrieval methods. This paper\nanalyses and addresses this problem by identifying data sparsity and\nlong-tailed distribution as the primary causes. Through comprehensive\nexperiments and detailed ablation studies, we analyze the impact of these\nfactors on codebook utilization and data distribution. Our findings reveal that\nthe \"Hourglass\" phenomenon substantially impacts the performance of RQ-SID in\ngenerative retrieval. We propose effective solutions to mitigate this issue,\nthereby significantly enhancing the effectiveness of generative retrieval in\nreal-world E-commerce applications.\n", "link": "http://arxiv.org/abs/2407.21488v2", "date": "2024-10-31", "relevancy": 2.1228, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5524}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5154}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20the%20Hourglass%20Phenomenon%20of%20Residual%20Quantization%3A%20Enhancing%0A%20%20the%20Upper%20Bound%20of%20Generative%20Retrieval&body=Title%3A%20Breaking%20the%20Hourglass%20Phenomenon%20of%20Residual%20Quantization%3A%20Enhancing%0A%20%20the%20Upper%20Bound%20of%20Generative%20Retrieval%0AAuthor%3A%20Zhirui%20Kuai%20and%20Zuxu%20Chen%20and%20Huimu%20Wang%20and%20Mingming%20Li%20and%20Dadong%20Miao%20and%20Binbin%20Wang%20and%20Xusong%20Chen%20and%20Li%20Kuang%20and%20Yuxing%20Han%20and%20Jiaxing%20Wang%20and%20Guoyu%20Tang%20and%20Lin%20Liu%20and%20Songlin%20Wang%20and%20Jingwei%20Zhuo%0AAbstract%3A%20%20%20Generative%20retrieval%20%28GR%29%20has%20emerged%20as%20a%20transformative%20paradigm%20in%20search%0Aand%20recommender%20systems%2C%20leveraging%20numeric-based%20identifier%20representations%20to%0Aenhance%20efficiency%20and%20generalization.%20Notably%2C%20methods%20like%20TIGER%20employing%0AResidual%20Quantization-based%20Semantic%20Identifiers%20%28RQ-SID%29%2C%20have%20shown%0Asignificant%20promise%20in%20e-commerce%20scenarios%20by%20effectively%20managing%20item%20IDs.%0AHowever%2C%20a%20critical%20issue%20termed%20the%20%22%5Ctextbf%7BHourglass%7D%22%20phenomenon%2C%20occurs%20in%0ARQ-SID%2C%20where%20intermediate%20codebook%20tokens%20become%20overly%20concentrated%2C%0Ahindering%20the%20full%20utilization%20of%20generative%20retrieval%20methods.%20This%20paper%0Aanalyses%20and%20addresses%20this%20problem%20by%20identifying%20data%20sparsity%20and%0Along-tailed%20distribution%20as%20the%20primary%20causes.%20Through%20comprehensive%0Aexperiments%20and%20detailed%20ablation%20studies%2C%20we%20analyze%20the%20impact%20of%20these%0Afactors%20on%20codebook%20utilization%20and%20data%20distribution.%20Our%20findings%20reveal%20that%0Athe%20%22Hourglass%22%20phenomenon%20substantially%20impacts%20the%20performance%20of%20RQ-SID%20in%0Agenerative%20retrieval.%20We%20propose%20effective%20solutions%20to%20mitigate%20this%20issue%2C%0Athereby%20significantly%20enhancing%20the%20effectiveness%20of%20generative%20retrieval%20in%0Areal-world%20E-commerce%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21488v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520the%2520Hourglass%2520Phenomenon%2520of%2520Residual%2520Quantization%253A%2520Enhancing%250A%2520%2520the%2520Upper%2520Bound%2520of%2520Generative%2520Retrieval%26entry.906535625%3DZhirui%2520Kuai%2520and%2520Zuxu%2520Chen%2520and%2520Huimu%2520Wang%2520and%2520Mingming%2520Li%2520and%2520Dadong%2520Miao%2520and%2520Binbin%2520Wang%2520and%2520Xusong%2520Chen%2520and%2520Li%2520Kuang%2520and%2520Yuxing%2520Han%2520and%2520Jiaxing%2520Wang%2520and%2520Guoyu%2520Tang%2520and%2520Lin%2520Liu%2520and%2520Songlin%2520Wang%2520and%2520Jingwei%2520Zhuo%26entry.1292438233%3D%2520%2520Generative%2520retrieval%2520%2528GR%2529%2520has%2520emerged%2520as%2520a%2520transformative%2520paradigm%2520in%2520search%250Aand%2520recommender%2520systems%252C%2520leveraging%2520numeric-based%2520identifier%2520representations%2520to%250Aenhance%2520efficiency%2520and%2520generalization.%2520Notably%252C%2520methods%2520like%2520TIGER%2520employing%250AResidual%2520Quantization-based%2520Semantic%2520Identifiers%2520%2528RQ-SID%2529%252C%2520have%2520shown%250Asignificant%2520promise%2520in%2520e-commerce%2520scenarios%2520by%2520effectively%2520managing%2520item%2520IDs.%250AHowever%252C%2520a%2520critical%2520issue%2520termed%2520the%2520%2522%255Ctextbf%257BHourglass%257D%2522%2520phenomenon%252C%2520occurs%2520in%250ARQ-SID%252C%2520where%2520intermediate%2520codebook%2520tokens%2520become%2520overly%2520concentrated%252C%250Ahindering%2520the%2520full%2520utilization%2520of%2520generative%2520retrieval%2520methods.%2520This%2520paper%250Aanalyses%2520and%2520addresses%2520this%2520problem%2520by%2520identifying%2520data%2520sparsity%2520and%250Along-tailed%2520distribution%2520as%2520the%2520primary%2520causes.%2520Through%2520comprehensive%250Aexperiments%2520and%2520detailed%2520ablation%2520studies%252C%2520we%2520analyze%2520the%2520impact%2520of%2520these%250Afactors%2520on%2520codebook%2520utilization%2520and%2520data%2520distribution.%2520Our%2520findings%2520reveal%2520that%250Athe%2520%2522Hourglass%2522%2520phenomenon%2520substantially%2520impacts%2520the%2520performance%2520of%2520RQ-SID%2520in%250Agenerative%2520retrieval.%2520We%2520propose%2520effective%2520solutions%2520to%2520mitigate%2520this%2520issue%252C%250Athereby%2520significantly%2520enhancing%2520the%2520effectiveness%2520of%2520generative%2520retrieval%2520in%250Areal-world%2520E-commerce%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21488v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20the%20Hourglass%20Phenomenon%20of%20Residual%20Quantization%3A%20Enhancing%0A%20%20the%20Upper%20Bound%20of%20Generative%20Retrieval&entry.906535625=Zhirui%20Kuai%20and%20Zuxu%20Chen%20and%20Huimu%20Wang%20and%20Mingming%20Li%20and%20Dadong%20Miao%20and%20Binbin%20Wang%20and%20Xusong%20Chen%20and%20Li%20Kuang%20and%20Yuxing%20Han%20and%20Jiaxing%20Wang%20and%20Guoyu%20Tang%20and%20Lin%20Liu%20and%20Songlin%20Wang%20and%20Jingwei%20Zhuo&entry.1292438233=%20%20Generative%20retrieval%20%28GR%29%20has%20emerged%20as%20a%20transformative%20paradigm%20in%20search%0Aand%20recommender%20systems%2C%20leveraging%20numeric-based%20identifier%20representations%20to%0Aenhance%20efficiency%20and%20generalization.%20Notably%2C%20methods%20like%20TIGER%20employing%0AResidual%20Quantization-based%20Semantic%20Identifiers%20%28RQ-SID%29%2C%20have%20shown%0Asignificant%20promise%20in%20e-commerce%20scenarios%20by%20effectively%20managing%20item%20IDs.%0AHowever%2C%20a%20critical%20issue%20termed%20the%20%22%5Ctextbf%7BHourglass%7D%22%20phenomenon%2C%20occurs%20in%0ARQ-SID%2C%20where%20intermediate%20codebook%20tokens%20become%20overly%20concentrated%2C%0Ahindering%20the%20full%20utilization%20of%20generative%20retrieval%20methods.%20This%20paper%0Aanalyses%20and%20addresses%20this%20problem%20by%20identifying%20data%20sparsity%20and%0Along-tailed%20distribution%20as%20the%20primary%20causes.%20Through%20comprehensive%0Aexperiments%20and%20detailed%20ablation%20studies%2C%20we%20analyze%20the%20impact%20of%20these%0Afactors%20on%20codebook%20utilization%20and%20data%20distribution.%20Our%20findings%20reveal%20that%0Athe%20%22Hourglass%22%20phenomenon%20substantially%20impacts%20the%20performance%20of%20RQ-SID%20in%0Agenerative%20retrieval.%20We%20propose%20effective%20solutions%20to%20mitigate%20this%20issue%2C%0Athereby%20significantly%20enhancing%20the%20effectiveness%20of%20generative%20retrieval%20in%0Areal-world%20E-commerce%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21488v2&entry.124074799=Read"},
{"title": "BERTs are Generative In-Context Learners", "author": "David Samuel", "abstract": "  While in-context learning is commonly associated with causal language models,\nsuch as GPT, we demonstrate that this capability also 'emerges' in masked\nlanguage models. Through an embarrassingly simple inference technique, we\nenable an existing masked model, DeBERTa, to perform generative tasks without\nadditional training or architectural changes. Our evaluation reveals that the\nmasked and causal language models behave very differently, as they clearly\noutperform each other on different categories of tasks. These complementary\nstrengths suggest that the field's focus on causal models for in-context\nlearning may be limiting - both architectures can develop these capabilities,\nbut with distinct advantages; pointing toward promising hybrid approaches that\ncombine the strengths of both objectives.\n", "link": "http://arxiv.org/abs/2406.04823v2", "date": "2024-10-31", "relevancy": 2.1227, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5818}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5293}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.48}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BERTs%20are%20Generative%20In-Context%20Learners&body=Title%3A%20BERTs%20are%20Generative%20In-Context%20Learners%0AAuthor%3A%20David%20Samuel%0AAbstract%3A%20%20%20While%20in-context%20learning%20is%20commonly%20associated%20with%20causal%20language%20models%2C%0Asuch%20as%20GPT%2C%20we%20demonstrate%20that%20this%20capability%20also%20%27emerges%27%20in%20masked%0Alanguage%20models.%20Through%20an%20embarrassingly%20simple%20inference%20technique%2C%20we%0Aenable%20an%20existing%20masked%20model%2C%20DeBERTa%2C%20to%20perform%20generative%20tasks%20without%0Aadditional%20training%20or%20architectural%20changes.%20Our%20evaluation%20reveals%20that%20the%0Amasked%20and%20causal%20language%20models%20behave%20very%20differently%2C%20as%20they%20clearly%0Aoutperform%20each%20other%20on%20different%20categories%20of%20tasks.%20These%20complementary%0Astrengths%20suggest%20that%20the%20field%27s%20focus%20on%20causal%20models%20for%20in-context%0Alearning%20may%20be%20limiting%20-%20both%20architectures%20can%20develop%20these%20capabilities%2C%0Abut%20with%20distinct%20advantages%3B%20pointing%20toward%20promising%20hybrid%20approaches%20that%0Acombine%20the%20strengths%20of%20both%20objectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04823v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBERTs%2520are%2520Generative%2520In-Context%2520Learners%26entry.906535625%3DDavid%2520Samuel%26entry.1292438233%3D%2520%2520While%2520in-context%2520learning%2520is%2520commonly%2520associated%2520with%2520causal%2520language%2520models%252C%250Asuch%2520as%2520GPT%252C%2520we%2520demonstrate%2520that%2520this%2520capability%2520also%2520%2527emerges%2527%2520in%2520masked%250Alanguage%2520models.%2520Through%2520an%2520embarrassingly%2520simple%2520inference%2520technique%252C%2520we%250Aenable%2520an%2520existing%2520masked%2520model%252C%2520DeBERTa%252C%2520to%2520perform%2520generative%2520tasks%2520without%250Aadditional%2520training%2520or%2520architectural%2520changes.%2520Our%2520evaluation%2520reveals%2520that%2520the%250Amasked%2520and%2520causal%2520language%2520models%2520behave%2520very%2520differently%252C%2520as%2520they%2520clearly%250Aoutperform%2520each%2520other%2520on%2520different%2520categories%2520of%2520tasks.%2520These%2520complementary%250Astrengths%2520suggest%2520that%2520the%2520field%2527s%2520focus%2520on%2520causal%2520models%2520for%2520in-context%250Alearning%2520may%2520be%2520limiting%2520-%2520both%2520architectures%2520can%2520develop%2520these%2520capabilities%252C%250Abut%2520with%2520distinct%2520advantages%253B%2520pointing%2520toward%2520promising%2520hybrid%2520approaches%2520that%250Acombine%2520the%2520strengths%2520of%2520both%2520objectives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04823v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BERTs%20are%20Generative%20In-Context%20Learners&entry.906535625=David%20Samuel&entry.1292438233=%20%20While%20in-context%20learning%20is%20commonly%20associated%20with%20causal%20language%20models%2C%0Asuch%20as%20GPT%2C%20we%20demonstrate%20that%20this%20capability%20also%20%27emerges%27%20in%20masked%0Alanguage%20models.%20Through%20an%20embarrassingly%20simple%20inference%20technique%2C%20we%0Aenable%20an%20existing%20masked%20model%2C%20DeBERTa%2C%20to%20perform%20generative%20tasks%20without%0Aadditional%20training%20or%20architectural%20changes.%20Our%20evaluation%20reveals%20that%20the%0Amasked%20and%20causal%20language%20models%20behave%20very%20differently%2C%20as%20they%20clearly%0Aoutperform%20each%20other%20on%20different%20categories%20of%20tasks.%20These%20complementary%0Astrengths%20suggest%20that%20the%20field%27s%20focus%20on%20causal%20models%20for%20in-context%0Alearning%20may%20be%20limiting%20-%20both%20architectures%20can%20develop%20these%20capabilities%2C%0Abut%20with%20distinct%20advantages%3B%20pointing%20toward%20promising%20hybrid%20approaches%20that%0Acombine%20the%20strengths%20of%20both%20objectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04823v2&entry.124074799=Read"},
{"title": "A Multi-Modal Approach for Face Anti-Spoofing in Non-Calibrated Systems\n  using Disparity Maps", "author": "Ariel Larey and Eyal Rond and Omer Achrack", "abstract": "  Face recognition technologies are increasingly used in various applications,\nyet they are vulnerable to face spoofing attacks. These spoofing attacks often\ninvolve unique 3D structures, such as printed papers or mobile device screens.\nAlthough stereo-depth cameras can detect such attacks effectively, their\nhigh-cost limits their widespread adoption. Conversely, two-sensor systems\nwithout extrinsic calibration offer a cost-effective alternative but are unable\nto calculate depth using stereo techniques. In this work, we propose a method\nto overcome this challenge by leveraging facial attributes to derive disparity\ninformation and estimate relative depth for anti-spoofing purposes, using\nnon-calibrated systems. We introduce a multi-modal anti-spoofing model, coined\nDisparity Model, that incorporates created disparity maps as a third modality\nalongside the two original sensor modalities. We demonstrate the effectiveness\nof the Disparity Model in countering various spoof attacks using a\ncomprehensive dataset collected from the Intel RealSense ID Solution F455. Our\nmethod outperformed existing methods in the literature, achieving an Equal\nError Rate (EER) of 1.71% and a False Negative Rate (FNR) of 2.77% at a False\nPositive Rate (FPR) of 1%. These errors are lower by 2.45% and 7.94% than the\nerrors of the best comparison method, respectively. Additionally, we introduce\na model ensemble that addresses 3D spoof attacks as well, achieving an EER of\n2.04% and an FNR of 3.83% at an FPR of 1%. Overall, our work provides a\nstate-of-the-art solution for the challenging task of anti-spoofing in\nnon-calibrated systems that lack depth information.\n", "link": "http://arxiv.org/abs/2410.24031v1", "date": "2024-10-31", "relevancy": 2.1027, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.533}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.526}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Modal%20Approach%20for%20Face%20Anti-Spoofing%20in%20Non-Calibrated%20Systems%0A%20%20using%20Disparity%20Maps&body=Title%3A%20A%20Multi-Modal%20Approach%20for%20Face%20Anti-Spoofing%20in%20Non-Calibrated%20Systems%0A%20%20using%20Disparity%20Maps%0AAuthor%3A%20Ariel%20Larey%20and%20Eyal%20Rond%20and%20Omer%20Achrack%0AAbstract%3A%20%20%20Face%20recognition%20technologies%20are%20increasingly%20used%20in%20various%20applications%2C%0Ayet%20they%20are%20vulnerable%20to%20face%20spoofing%20attacks.%20These%20spoofing%20attacks%20often%0Ainvolve%20unique%203D%20structures%2C%20such%20as%20printed%20papers%20or%20mobile%20device%20screens.%0AAlthough%20stereo-depth%20cameras%20can%20detect%20such%20attacks%20effectively%2C%20their%0Ahigh-cost%20limits%20their%20widespread%20adoption.%20Conversely%2C%20two-sensor%20systems%0Awithout%20extrinsic%20calibration%20offer%20a%20cost-effective%20alternative%20but%20are%20unable%0Ato%20calculate%20depth%20using%20stereo%20techniques.%20In%20this%20work%2C%20we%20propose%20a%20method%0Ato%20overcome%20this%20challenge%20by%20leveraging%20facial%20attributes%20to%20derive%20disparity%0Ainformation%20and%20estimate%20relative%20depth%20for%20anti-spoofing%20purposes%2C%20using%0Anon-calibrated%20systems.%20We%20introduce%20a%20multi-modal%20anti-spoofing%20model%2C%20coined%0ADisparity%20Model%2C%20that%20incorporates%20created%20disparity%20maps%20as%20a%20third%20modality%0Aalongside%20the%20two%20original%20sensor%20modalities.%20We%20demonstrate%20the%20effectiveness%0Aof%20the%20Disparity%20Model%20in%20countering%20various%20spoof%20attacks%20using%20a%0Acomprehensive%20dataset%20collected%20from%20the%20Intel%20RealSense%20ID%20Solution%20F455.%20Our%0Amethod%20outperformed%20existing%20methods%20in%20the%20literature%2C%20achieving%20an%20Equal%0AError%20Rate%20%28EER%29%20of%201.71%25%20and%20a%20False%20Negative%20Rate%20%28FNR%29%20of%202.77%25%20at%20a%20False%0APositive%20Rate%20%28FPR%29%20of%201%25.%20These%20errors%20are%20lower%20by%202.45%25%20and%207.94%25%20than%20the%0Aerrors%20of%20the%20best%20comparison%20method%2C%20respectively.%20Additionally%2C%20we%20introduce%0Aa%20model%20ensemble%20that%20addresses%203D%20spoof%20attacks%20as%20well%2C%20achieving%20an%20EER%20of%0A2.04%25%20and%20an%20FNR%20of%203.83%25%20at%20an%20FPR%20of%201%25.%20Overall%2C%20our%20work%20provides%20a%0Astate-of-the-art%20solution%20for%20the%20challenging%20task%20of%20anti-spoofing%20in%0Anon-calibrated%20systems%20that%20lack%20depth%20information.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Multi-Modal%2520Approach%2520for%2520Face%2520Anti-Spoofing%2520in%2520Non-Calibrated%2520Systems%250A%2520%2520using%2520Disparity%2520Maps%26entry.906535625%3DAriel%2520Larey%2520and%2520Eyal%2520Rond%2520and%2520Omer%2520Achrack%26entry.1292438233%3D%2520%2520Face%2520recognition%2520technologies%2520are%2520increasingly%2520used%2520in%2520various%2520applications%252C%250Ayet%2520they%2520are%2520vulnerable%2520to%2520face%2520spoofing%2520attacks.%2520These%2520spoofing%2520attacks%2520often%250Ainvolve%2520unique%25203D%2520structures%252C%2520such%2520as%2520printed%2520papers%2520or%2520mobile%2520device%2520screens.%250AAlthough%2520stereo-depth%2520cameras%2520can%2520detect%2520such%2520attacks%2520effectively%252C%2520their%250Ahigh-cost%2520limits%2520their%2520widespread%2520adoption.%2520Conversely%252C%2520two-sensor%2520systems%250Awithout%2520extrinsic%2520calibration%2520offer%2520a%2520cost-effective%2520alternative%2520but%2520are%2520unable%250Ato%2520calculate%2520depth%2520using%2520stereo%2520techniques.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520method%250Ato%2520overcome%2520this%2520challenge%2520by%2520leveraging%2520facial%2520attributes%2520to%2520derive%2520disparity%250Ainformation%2520and%2520estimate%2520relative%2520depth%2520for%2520anti-spoofing%2520purposes%252C%2520using%250Anon-calibrated%2520systems.%2520We%2520introduce%2520a%2520multi-modal%2520anti-spoofing%2520model%252C%2520coined%250ADisparity%2520Model%252C%2520that%2520incorporates%2520created%2520disparity%2520maps%2520as%2520a%2520third%2520modality%250Aalongside%2520the%2520two%2520original%2520sensor%2520modalities.%2520We%2520demonstrate%2520the%2520effectiveness%250Aof%2520the%2520Disparity%2520Model%2520in%2520countering%2520various%2520spoof%2520attacks%2520using%2520a%250Acomprehensive%2520dataset%2520collected%2520from%2520the%2520Intel%2520RealSense%2520ID%2520Solution%2520F455.%2520Our%250Amethod%2520outperformed%2520existing%2520methods%2520in%2520the%2520literature%252C%2520achieving%2520an%2520Equal%250AError%2520Rate%2520%2528EER%2529%2520of%25201.71%2525%2520and%2520a%2520False%2520Negative%2520Rate%2520%2528FNR%2529%2520of%25202.77%2525%2520at%2520a%2520False%250APositive%2520Rate%2520%2528FPR%2529%2520of%25201%2525.%2520These%2520errors%2520are%2520lower%2520by%25202.45%2525%2520and%25207.94%2525%2520than%2520the%250Aerrors%2520of%2520the%2520best%2520comparison%2520method%252C%2520respectively.%2520Additionally%252C%2520we%2520introduce%250Aa%2520model%2520ensemble%2520that%2520addresses%25203D%2520spoof%2520attacks%2520as%2520well%252C%2520achieving%2520an%2520EER%2520of%250A2.04%2525%2520and%2520an%2520FNR%2520of%25203.83%2525%2520at%2520an%2520FPR%2520of%25201%2525.%2520Overall%252C%2520our%2520work%2520provides%2520a%250Astate-of-the-art%2520solution%2520for%2520the%2520challenging%2520task%2520of%2520anti-spoofing%2520in%250Anon-calibrated%2520systems%2520that%2520lack%2520depth%2520information.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Modal%20Approach%20for%20Face%20Anti-Spoofing%20in%20Non-Calibrated%20Systems%0A%20%20using%20Disparity%20Maps&entry.906535625=Ariel%20Larey%20and%20Eyal%20Rond%20and%20Omer%20Achrack&entry.1292438233=%20%20Face%20recognition%20technologies%20are%20increasingly%20used%20in%20various%20applications%2C%0Ayet%20they%20are%20vulnerable%20to%20face%20spoofing%20attacks.%20These%20spoofing%20attacks%20often%0Ainvolve%20unique%203D%20structures%2C%20such%20as%20printed%20papers%20or%20mobile%20device%20screens.%0AAlthough%20stereo-depth%20cameras%20can%20detect%20such%20attacks%20effectively%2C%20their%0Ahigh-cost%20limits%20their%20widespread%20adoption.%20Conversely%2C%20two-sensor%20systems%0Awithout%20extrinsic%20calibration%20offer%20a%20cost-effective%20alternative%20but%20are%20unable%0Ato%20calculate%20depth%20using%20stereo%20techniques.%20In%20this%20work%2C%20we%20propose%20a%20method%0Ato%20overcome%20this%20challenge%20by%20leveraging%20facial%20attributes%20to%20derive%20disparity%0Ainformation%20and%20estimate%20relative%20depth%20for%20anti-spoofing%20purposes%2C%20using%0Anon-calibrated%20systems.%20We%20introduce%20a%20multi-modal%20anti-spoofing%20model%2C%20coined%0ADisparity%20Model%2C%20that%20incorporates%20created%20disparity%20maps%20as%20a%20third%20modality%0Aalongside%20the%20two%20original%20sensor%20modalities.%20We%20demonstrate%20the%20effectiveness%0Aof%20the%20Disparity%20Model%20in%20countering%20various%20spoof%20attacks%20using%20a%0Acomprehensive%20dataset%20collected%20from%20the%20Intel%20RealSense%20ID%20Solution%20F455.%20Our%0Amethod%20outperformed%20existing%20methods%20in%20the%20literature%2C%20achieving%20an%20Equal%0AError%20Rate%20%28EER%29%20of%201.71%25%20and%20a%20False%20Negative%20Rate%20%28FNR%29%20of%202.77%25%20at%20a%20False%0APositive%20Rate%20%28FPR%29%20of%201%25.%20These%20errors%20are%20lower%20by%202.45%25%20and%207.94%25%20than%20the%0Aerrors%20of%20the%20best%20comparison%20method%2C%20respectively.%20Additionally%2C%20we%20introduce%0Aa%20model%20ensemble%20that%20addresses%203D%20spoof%20attacks%20as%20well%2C%20achieving%20an%20EER%20of%0A2.04%25%20and%20an%20FNR%20of%203.83%25%20at%20an%20FPR%20of%201%25.%20Overall%2C%20our%20work%20provides%20a%0Astate-of-the-art%20solution%20for%20the%20challenging%20task%20of%20anti-spoofing%20in%0Anon-calibrated%20systems%20that%20lack%20depth%20information.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24031v1&entry.124074799=Read"},
{"title": "Language-Driven Policy Distillation for Cooperative Driving in\n  Multi-Agent Reinforcement Learning", "author": "Jiaqi Liu and Chengkai Xu and Peng Hang and Jian Sun and Mingyu Ding and Wei Zhan and Masayoshi Tomizuka", "abstract": "  The cooperative driving technology of Connected and Autonomous Vehicles\n(CAVs) is crucial for improving the efficiency and safety of transportation\nsystems. Learning-based methods, such as Multi-Agent Reinforcement Learning\n(MARL), have demonstrated strong capabilities in cooperative decision-making\ntasks. However, existing MARL approaches still face challenges in terms of\nlearning efficiency and performance. In recent years, Large Language Models\n(LLMs) have rapidly advanced and shown remarkable abilities in various\nsequential decision-making tasks. To enhance the learning capabilities of\ncooperative agents while ensuring decision-making efficiency and\ncost-effectiveness, we propose LDPD, a language-driven policy distillation\nmethod for guiding MARL exploration. In this framework, a teacher agent based\non LLM trains smaller student agents to achieve cooperative decision-making\nthrough its own decision-making demonstrations. The teacher agent enhances the\nobservation information of CAVs and utilizes LLMs to perform complex\ncooperative decision-making reasoning, which also leverages carefully designed\ndecision-making tools to achieve expert-level decisions, providing high-quality\nteaching experiences. The student agent then refines the teacher's prior\nknowledge into its own model through gradient policy updates. The experiments\ndemonstrate that the students can rapidly improve their capabilities with\nminimal guidance from the teacher and eventually surpass the teacher's\nperformance. Extensive experiments show that our approach demonstrates better\nperformance and learning efficiency compared to baseline methods.\n", "link": "http://arxiv.org/abs/2410.24152v1", "date": "2024-10-31", "relevancy": 2.1018, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5563}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5227}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Language-Driven%20Policy%20Distillation%20for%20Cooperative%20Driving%20in%0A%20%20Multi-Agent%20Reinforcement%20Learning&body=Title%3A%20Language-Driven%20Policy%20Distillation%20for%20Cooperative%20Driving%20in%0A%20%20Multi-Agent%20Reinforcement%20Learning%0AAuthor%3A%20Jiaqi%20Liu%20and%20Chengkai%20Xu%20and%20Peng%20Hang%20and%20Jian%20Sun%20and%20Mingyu%20Ding%20and%20Wei%20Zhan%20and%20Masayoshi%20Tomizuka%0AAbstract%3A%20%20%20The%20cooperative%20driving%20technology%20of%20Connected%20and%20Autonomous%20Vehicles%0A%28CAVs%29%20is%20crucial%20for%20improving%20the%20efficiency%20and%20safety%20of%20transportation%0Asystems.%20Learning-based%20methods%2C%20such%20as%20Multi-Agent%20Reinforcement%20Learning%0A%28MARL%29%2C%20have%20demonstrated%20strong%20capabilities%20in%20cooperative%20decision-making%0Atasks.%20However%2C%20existing%20MARL%20approaches%20still%20face%20challenges%20in%20terms%20of%0Alearning%20efficiency%20and%20performance.%20In%20recent%20years%2C%20Large%20Language%20Models%0A%28LLMs%29%20have%20rapidly%20advanced%20and%20shown%20remarkable%20abilities%20in%20various%0Asequential%20decision-making%20tasks.%20To%20enhance%20the%20learning%20capabilities%20of%0Acooperative%20agents%20while%20ensuring%20decision-making%20efficiency%20and%0Acost-effectiveness%2C%20we%20propose%20LDPD%2C%20a%20language-driven%20policy%20distillation%0Amethod%20for%20guiding%20MARL%20exploration.%20In%20this%20framework%2C%20a%20teacher%20agent%20based%0Aon%20LLM%20trains%20smaller%20student%20agents%20to%20achieve%20cooperative%20decision-making%0Athrough%20its%20own%20decision-making%20demonstrations.%20The%20teacher%20agent%20enhances%20the%0Aobservation%20information%20of%20CAVs%20and%20utilizes%20LLMs%20to%20perform%20complex%0Acooperative%20decision-making%20reasoning%2C%20which%20also%20leverages%20carefully%20designed%0Adecision-making%20tools%20to%20achieve%20expert-level%20decisions%2C%20providing%20high-quality%0Ateaching%20experiences.%20The%20student%20agent%20then%20refines%20the%20teacher%27s%20prior%0Aknowledge%20into%20its%20own%20model%20through%20gradient%20policy%20updates.%20The%20experiments%0Ademonstrate%20that%20the%20students%20can%20rapidly%20improve%20their%20capabilities%20with%0Aminimal%20guidance%20from%20the%20teacher%20and%20eventually%20surpass%20the%20teacher%27s%0Aperformance.%20Extensive%20experiments%20show%20that%20our%20approach%20demonstrates%20better%0Aperformance%20and%20learning%20efficiency%20compared%20to%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLanguage-Driven%2520Policy%2520Distillation%2520for%2520Cooperative%2520Driving%2520in%250A%2520%2520Multi-Agent%2520Reinforcement%2520Learning%26entry.906535625%3DJiaqi%2520Liu%2520and%2520Chengkai%2520Xu%2520and%2520Peng%2520Hang%2520and%2520Jian%2520Sun%2520and%2520Mingyu%2520Ding%2520and%2520Wei%2520Zhan%2520and%2520Masayoshi%2520Tomizuka%26entry.1292438233%3D%2520%2520The%2520cooperative%2520driving%2520technology%2520of%2520Connected%2520and%2520Autonomous%2520Vehicles%250A%2528CAVs%2529%2520is%2520crucial%2520for%2520improving%2520the%2520efficiency%2520and%2520safety%2520of%2520transportation%250Asystems.%2520Learning-based%2520methods%252C%2520such%2520as%2520Multi-Agent%2520Reinforcement%2520Learning%250A%2528MARL%2529%252C%2520have%2520demonstrated%2520strong%2520capabilities%2520in%2520cooperative%2520decision-making%250Atasks.%2520However%252C%2520existing%2520MARL%2520approaches%2520still%2520face%2520challenges%2520in%2520terms%2520of%250Alearning%2520efficiency%2520and%2520performance.%2520In%2520recent%2520years%252C%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520have%2520rapidly%2520advanced%2520and%2520shown%2520remarkable%2520abilities%2520in%2520various%250Asequential%2520decision-making%2520tasks.%2520To%2520enhance%2520the%2520learning%2520capabilities%2520of%250Acooperative%2520agents%2520while%2520ensuring%2520decision-making%2520efficiency%2520and%250Acost-effectiveness%252C%2520we%2520propose%2520LDPD%252C%2520a%2520language-driven%2520policy%2520distillation%250Amethod%2520for%2520guiding%2520MARL%2520exploration.%2520In%2520this%2520framework%252C%2520a%2520teacher%2520agent%2520based%250Aon%2520LLM%2520trains%2520smaller%2520student%2520agents%2520to%2520achieve%2520cooperative%2520decision-making%250Athrough%2520its%2520own%2520decision-making%2520demonstrations.%2520The%2520teacher%2520agent%2520enhances%2520the%250Aobservation%2520information%2520of%2520CAVs%2520and%2520utilizes%2520LLMs%2520to%2520perform%2520complex%250Acooperative%2520decision-making%2520reasoning%252C%2520which%2520also%2520leverages%2520carefully%2520designed%250Adecision-making%2520tools%2520to%2520achieve%2520expert-level%2520decisions%252C%2520providing%2520high-quality%250Ateaching%2520experiences.%2520The%2520student%2520agent%2520then%2520refines%2520the%2520teacher%2527s%2520prior%250Aknowledge%2520into%2520its%2520own%2520model%2520through%2520gradient%2520policy%2520updates.%2520The%2520experiments%250Ademonstrate%2520that%2520the%2520students%2520can%2520rapidly%2520improve%2520their%2520capabilities%2520with%250Aminimal%2520guidance%2520from%2520the%2520teacher%2520and%2520eventually%2520surpass%2520the%2520teacher%2527s%250Aperformance.%2520Extensive%2520experiments%2520show%2520that%2520our%2520approach%2520demonstrates%2520better%250Aperformance%2520and%2520learning%2520efficiency%2520compared%2520to%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Language-Driven%20Policy%20Distillation%20for%20Cooperative%20Driving%20in%0A%20%20Multi-Agent%20Reinforcement%20Learning&entry.906535625=Jiaqi%20Liu%20and%20Chengkai%20Xu%20and%20Peng%20Hang%20and%20Jian%20Sun%20and%20Mingyu%20Ding%20and%20Wei%20Zhan%20and%20Masayoshi%20Tomizuka&entry.1292438233=%20%20The%20cooperative%20driving%20technology%20of%20Connected%20and%20Autonomous%20Vehicles%0A%28CAVs%29%20is%20crucial%20for%20improving%20the%20efficiency%20and%20safety%20of%20transportation%0Asystems.%20Learning-based%20methods%2C%20such%20as%20Multi-Agent%20Reinforcement%20Learning%0A%28MARL%29%2C%20have%20demonstrated%20strong%20capabilities%20in%20cooperative%20decision-making%0Atasks.%20However%2C%20existing%20MARL%20approaches%20still%20face%20challenges%20in%20terms%20of%0Alearning%20efficiency%20and%20performance.%20In%20recent%20years%2C%20Large%20Language%20Models%0A%28LLMs%29%20have%20rapidly%20advanced%20and%20shown%20remarkable%20abilities%20in%20various%0Asequential%20decision-making%20tasks.%20To%20enhance%20the%20learning%20capabilities%20of%0Acooperative%20agents%20while%20ensuring%20decision-making%20efficiency%20and%0Acost-effectiveness%2C%20we%20propose%20LDPD%2C%20a%20language-driven%20policy%20distillation%0Amethod%20for%20guiding%20MARL%20exploration.%20In%20this%20framework%2C%20a%20teacher%20agent%20based%0Aon%20LLM%20trains%20smaller%20student%20agents%20to%20achieve%20cooperative%20decision-making%0Athrough%20its%20own%20decision-making%20demonstrations.%20The%20teacher%20agent%20enhances%20the%0Aobservation%20information%20of%20CAVs%20and%20utilizes%20LLMs%20to%20perform%20complex%0Acooperative%20decision-making%20reasoning%2C%20which%20also%20leverages%20carefully%20designed%0Adecision-making%20tools%20to%20achieve%20expert-level%20decisions%2C%20providing%20high-quality%0Ateaching%20experiences.%20The%20student%20agent%20then%20refines%20the%20teacher%27s%20prior%0Aknowledge%20into%20its%20own%20model%20through%20gradient%20policy%20updates.%20The%20experiments%0Ademonstrate%20that%20the%20students%20can%20rapidly%20improve%20their%20capabilities%20with%0Aminimal%20guidance%20from%20the%20teacher%20and%20eventually%20surpass%20the%20teacher%27s%0Aperformance.%20Extensive%20experiments%20show%20that%20our%20approach%20demonstrates%20better%0Aperformance%20and%20learning%20efficiency%20compared%20to%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24152v1&entry.124074799=Read"},
{"title": "Advancing Video Anomaly Detection: A Concise Review and a New Dataset", "author": "Liyun Zhu and Lei Wang and Arjun Raj and Tom Gedeon and Chen Chen", "abstract": "  Video Anomaly Detection (VAD) finds widespread applications in security\nsurveillance, traffic monitoring, industrial monitoring, and healthcare.\nDespite extensive research efforts, there remains a lack of concise reviews\nthat provide insightful guidance for researchers. Such reviews would serve as\nquick references to grasp current challenges, research trends, and future\ndirections. In this paper, we present such a review, examining models and\ndatasets from various perspectives. We emphasize the critical relationship\nbetween model and dataset, where the quality and diversity of datasets\nprofoundly influence model performance, and dataset development adapts to the\nevolving needs of emerging approaches. Our review identifies practical issues,\nincluding the absence of comprehensive datasets with diverse scenarios. To\naddress this, we introduce a new dataset, Multi-Scenario Anomaly Detection\n(MSAD), comprising 14 distinct scenarios captured from various camera views.\nOur dataset has diverse motion patterns and challenging variations, such as\ndifferent lighting and weather conditions, providing a robust foundation for\ntraining superior models. We conduct an in-depth analysis of recent\nrepresentative models using MSAD and highlight its potential in addressing the\nchallenges of detecting anomalies across diverse and evolving surveillance\nscenarios. [Project website: https://msad-dataset.github.io/]\n", "link": "http://arxiv.org/abs/2402.04857v4", "date": "2024-10-31", "relevancy": 2.0973, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5246}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5246}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Video%20Anomaly%20Detection%3A%20A%20Concise%20Review%20and%20a%20New%20Dataset&body=Title%3A%20Advancing%20Video%20Anomaly%20Detection%3A%20A%20Concise%20Review%20and%20a%20New%20Dataset%0AAuthor%3A%20Liyun%20Zhu%20and%20Lei%20Wang%20and%20Arjun%20Raj%20and%20Tom%20Gedeon%20and%20Chen%20Chen%0AAbstract%3A%20%20%20Video%20Anomaly%20Detection%20%28VAD%29%20finds%20widespread%20applications%20in%20security%0Asurveillance%2C%20traffic%20monitoring%2C%20industrial%20monitoring%2C%20and%20healthcare.%0ADespite%20extensive%20research%20efforts%2C%20there%20remains%20a%20lack%20of%20concise%20reviews%0Athat%20provide%20insightful%20guidance%20for%20researchers.%20Such%20reviews%20would%20serve%20as%0Aquick%20references%20to%20grasp%20current%20challenges%2C%20research%20trends%2C%20and%20future%0Adirections.%20In%20this%20paper%2C%20we%20present%20such%20a%20review%2C%20examining%20models%20and%0Adatasets%20from%20various%20perspectives.%20We%20emphasize%20the%20critical%20relationship%0Abetween%20model%20and%20dataset%2C%20where%20the%20quality%20and%20diversity%20of%20datasets%0Aprofoundly%20influence%20model%20performance%2C%20and%20dataset%20development%20adapts%20to%20the%0Aevolving%20needs%20of%20emerging%20approaches.%20Our%20review%20identifies%20practical%20issues%2C%0Aincluding%20the%20absence%20of%20comprehensive%20datasets%20with%20diverse%20scenarios.%20To%0Aaddress%20this%2C%20we%20introduce%20a%20new%20dataset%2C%20Multi-Scenario%20Anomaly%20Detection%0A%28MSAD%29%2C%20comprising%2014%20distinct%20scenarios%20captured%20from%20various%20camera%20views.%0AOur%20dataset%20has%20diverse%20motion%20patterns%20and%20challenging%20variations%2C%20such%20as%0Adifferent%20lighting%20and%20weather%20conditions%2C%20providing%20a%20robust%20foundation%20for%0Atraining%20superior%20models.%20We%20conduct%20an%20in-depth%20analysis%20of%20recent%0Arepresentative%20models%20using%20MSAD%20and%20highlight%20its%20potential%20in%20addressing%20the%0Achallenges%20of%20detecting%20anomalies%20across%20diverse%20and%20evolving%20surveillance%0Ascenarios.%20%5BProject%20website%3A%20https%3A//msad-dataset.github.io/%5D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04857v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Video%2520Anomaly%2520Detection%253A%2520A%2520Concise%2520Review%2520and%2520a%2520New%2520Dataset%26entry.906535625%3DLiyun%2520Zhu%2520and%2520Lei%2520Wang%2520and%2520Arjun%2520Raj%2520and%2520Tom%2520Gedeon%2520and%2520Chen%2520Chen%26entry.1292438233%3D%2520%2520Video%2520Anomaly%2520Detection%2520%2528VAD%2529%2520finds%2520widespread%2520applications%2520in%2520security%250Asurveillance%252C%2520traffic%2520monitoring%252C%2520industrial%2520monitoring%252C%2520and%2520healthcare.%250ADespite%2520extensive%2520research%2520efforts%252C%2520there%2520remains%2520a%2520lack%2520of%2520concise%2520reviews%250Athat%2520provide%2520insightful%2520guidance%2520for%2520researchers.%2520Such%2520reviews%2520would%2520serve%2520as%250Aquick%2520references%2520to%2520grasp%2520current%2520challenges%252C%2520research%2520trends%252C%2520and%2520future%250Adirections.%2520In%2520this%2520paper%252C%2520we%2520present%2520such%2520a%2520review%252C%2520examining%2520models%2520and%250Adatasets%2520from%2520various%2520perspectives.%2520We%2520emphasize%2520the%2520critical%2520relationship%250Abetween%2520model%2520and%2520dataset%252C%2520where%2520the%2520quality%2520and%2520diversity%2520of%2520datasets%250Aprofoundly%2520influence%2520model%2520performance%252C%2520and%2520dataset%2520development%2520adapts%2520to%2520the%250Aevolving%2520needs%2520of%2520emerging%2520approaches.%2520Our%2520review%2520identifies%2520practical%2520issues%252C%250Aincluding%2520the%2520absence%2520of%2520comprehensive%2520datasets%2520with%2520diverse%2520scenarios.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520a%2520new%2520dataset%252C%2520Multi-Scenario%2520Anomaly%2520Detection%250A%2528MSAD%2529%252C%2520comprising%252014%2520distinct%2520scenarios%2520captured%2520from%2520various%2520camera%2520views.%250AOur%2520dataset%2520has%2520diverse%2520motion%2520patterns%2520and%2520challenging%2520variations%252C%2520such%2520as%250Adifferent%2520lighting%2520and%2520weather%2520conditions%252C%2520providing%2520a%2520robust%2520foundation%2520for%250Atraining%2520superior%2520models.%2520We%2520conduct%2520an%2520in-depth%2520analysis%2520of%2520recent%250Arepresentative%2520models%2520using%2520MSAD%2520and%2520highlight%2520its%2520potential%2520in%2520addressing%2520the%250Achallenges%2520of%2520detecting%2520anomalies%2520across%2520diverse%2520and%2520evolving%2520surveillance%250Ascenarios.%2520%255BProject%2520website%253A%2520https%253A//msad-dataset.github.io/%255D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04857v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Video%20Anomaly%20Detection%3A%20A%20Concise%20Review%20and%20a%20New%20Dataset&entry.906535625=Liyun%20Zhu%20and%20Lei%20Wang%20and%20Arjun%20Raj%20and%20Tom%20Gedeon%20and%20Chen%20Chen&entry.1292438233=%20%20Video%20Anomaly%20Detection%20%28VAD%29%20finds%20widespread%20applications%20in%20security%0Asurveillance%2C%20traffic%20monitoring%2C%20industrial%20monitoring%2C%20and%20healthcare.%0ADespite%20extensive%20research%20efforts%2C%20there%20remains%20a%20lack%20of%20concise%20reviews%0Athat%20provide%20insightful%20guidance%20for%20researchers.%20Such%20reviews%20would%20serve%20as%0Aquick%20references%20to%20grasp%20current%20challenges%2C%20research%20trends%2C%20and%20future%0Adirections.%20In%20this%20paper%2C%20we%20present%20such%20a%20review%2C%20examining%20models%20and%0Adatasets%20from%20various%20perspectives.%20We%20emphasize%20the%20critical%20relationship%0Abetween%20model%20and%20dataset%2C%20where%20the%20quality%20and%20diversity%20of%20datasets%0Aprofoundly%20influence%20model%20performance%2C%20and%20dataset%20development%20adapts%20to%20the%0Aevolving%20needs%20of%20emerging%20approaches.%20Our%20review%20identifies%20practical%20issues%2C%0Aincluding%20the%20absence%20of%20comprehensive%20datasets%20with%20diverse%20scenarios.%20To%0Aaddress%20this%2C%20we%20introduce%20a%20new%20dataset%2C%20Multi-Scenario%20Anomaly%20Detection%0A%28MSAD%29%2C%20comprising%2014%20distinct%20scenarios%20captured%20from%20various%20camera%20views.%0AOur%20dataset%20has%20diverse%20motion%20patterns%20and%20challenging%20variations%2C%20such%20as%0Adifferent%20lighting%20and%20weather%20conditions%2C%20providing%20a%20robust%20foundation%20for%0Atraining%20superior%20models.%20We%20conduct%20an%20in-depth%20analysis%20of%20recent%0Arepresentative%20models%20using%20MSAD%20and%20highlight%20its%20potential%20in%20addressing%20the%0Achallenges%20of%20detecting%20anomalies%20across%20diverse%20and%20evolving%20surveillance%0Ascenarios.%20%5BProject%20website%3A%20https%3A//msad-dataset.github.io/%5D%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04857v4&entry.124074799=Read"},
{"title": "COSNet: A Novel Semantic Segmentation Network using Enhanced Boundaries\n  in Cluttered Scenes", "author": "Muhammad Ali and Mamoona Javaid and Mubashir Noman and Mustansar Fiaz and Salman Khan", "abstract": "  Automated waste recycling aims to efficiently separate the recyclable objects\nfrom the waste by employing vision-based systems. However, the presence of\nvarying shaped objects having different material types makes it a challenging\nproblem, especially in cluttered environments. Existing segmentation methods\nperform reasonably on many semantic segmentation datasets by employing\nmulti-contextual representations, however, their performance is degraded when\nutilized for waste object segmentation in cluttered scenarios. In addition,\nplastic objects further increase the complexity of the problem due to their\ntranslucent nature. To address these limitations, we introduce an efficacious\nsegmentation network, named COSNet, that uses boundary cues along with\nmulti-contextual information to accurately segment the objects in cluttered\nscenes. COSNet introduces novel components including feature sharpening block\n(FSB) and boundary enhancement module (BEM) for enhancing the features and\nhighlighting the boundary information of irregular waste objects in cluttered\nenvironment. Extensive experiments on three challenging datasets including\nZeroWaste-f, SpectralWaste, and ADE20K demonstrate the effectiveness of the\nproposed method. Our COSNet achieves a significant gain of 1.8% on ZeroWaste-f\nand 2.1% on SpectralWaste datasets respectively in terms of mIoU metric.\n", "link": "http://arxiv.org/abs/2410.24139v1", "date": "2024-10-31", "relevancy": 2.0693, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5194}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5169}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COSNet%3A%20A%20Novel%20Semantic%20Segmentation%20Network%20using%20Enhanced%20Boundaries%0A%20%20in%20Cluttered%20Scenes&body=Title%3A%20COSNet%3A%20A%20Novel%20Semantic%20Segmentation%20Network%20using%20Enhanced%20Boundaries%0A%20%20in%20Cluttered%20Scenes%0AAuthor%3A%20Muhammad%20Ali%20and%20Mamoona%20Javaid%20and%20Mubashir%20Noman%20and%20Mustansar%20Fiaz%20and%20Salman%20Khan%0AAbstract%3A%20%20%20Automated%20waste%20recycling%20aims%20to%20efficiently%20separate%20the%20recyclable%20objects%0Afrom%20the%20waste%20by%20employing%20vision-based%20systems.%20However%2C%20the%20presence%20of%0Avarying%20shaped%20objects%20having%20different%20material%20types%20makes%20it%20a%20challenging%0Aproblem%2C%20especially%20in%20cluttered%20environments.%20Existing%20segmentation%20methods%0Aperform%20reasonably%20on%20many%20semantic%20segmentation%20datasets%20by%20employing%0Amulti-contextual%20representations%2C%20however%2C%20their%20performance%20is%20degraded%20when%0Autilized%20for%20waste%20object%20segmentation%20in%20cluttered%20scenarios.%20In%20addition%2C%0Aplastic%20objects%20further%20increase%20the%20complexity%20of%20the%20problem%20due%20to%20their%0Atranslucent%20nature.%20To%20address%20these%20limitations%2C%20we%20introduce%20an%20efficacious%0Asegmentation%20network%2C%20named%20COSNet%2C%20that%20uses%20boundary%20cues%20along%20with%0Amulti-contextual%20information%20to%20accurately%20segment%20the%20objects%20in%20cluttered%0Ascenes.%20COSNet%20introduces%20novel%20components%20including%20feature%20sharpening%20block%0A%28FSB%29%20and%20boundary%20enhancement%20module%20%28BEM%29%20for%20enhancing%20the%20features%20and%0Ahighlighting%20the%20boundary%20information%20of%20irregular%20waste%20objects%20in%20cluttered%0Aenvironment.%20Extensive%20experiments%20on%20three%20challenging%20datasets%20including%0AZeroWaste-f%2C%20SpectralWaste%2C%20and%20ADE20K%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20method.%20Our%20COSNet%20achieves%20a%20significant%20gain%20of%201.8%25%20on%20ZeroWaste-f%0Aand%202.1%25%20on%20SpectralWaste%20datasets%20respectively%20in%20terms%20of%20mIoU%20metric.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOSNet%253A%2520A%2520Novel%2520Semantic%2520Segmentation%2520Network%2520using%2520Enhanced%2520Boundaries%250A%2520%2520in%2520Cluttered%2520Scenes%26entry.906535625%3DMuhammad%2520Ali%2520and%2520Mamoona%2520Javaid%2520and%2520Mubashir%2520Noman%2520and%2520Mustansar%2520Fiaz%2520and%2520Salman%2520Khan%26entry.1292438233%3D%2520%2520Automated%2520waste%2520recycling%2520aims%2520to%2520efficiently%2520separate%2520the%2520recyclable%2520objects%250Afrom%2520the%2520waste%2520by%2520employing%2520vision-based%2520systems.%2520However%252C%2520the%2520presence%2520of%250Avarying%2520shaped%2520objects%2520having%2520different%2520material%2520types%2520makes%2520it%2520a%2520challenging%250Aproblem%252C%2520especially%2520in%2520cluttered%2520environments.%2520Existing%2520segmentation%2520methods%250Aperform%2520reasonably%2520on%2520many%2520semantic%2520segmentation%2520datasets%2520by%2520employing%250Amulti-contextual%2520representations%252C%2520however%252C%2520their%2520performance%2520is%2520degraded%2520when%250Autilized%2520for%2520waste%2520object%2520segmentation%2520in%2520cluttered%2520scenarios.%2520In%2520addition%252C%250Aplastic%2520objects%2520further%2520increase%2520the%2520complexity%2520of%2520the%2520problem%2520due%2520to%2520their%250Atranslucent%2520nature.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520an%2520efficacious%250Asegmentation%2520network%252C%2520named%2520COSNet%252C%2520that%2520uses%2520boundary%2520cues%2520along%2520with%250Amulti-contextual%2520information%2520to%2520accurately%2520segment%2520the%2520objects%2520in%2520cluttered%250Ascenes.%2520COSNet%2520introduces%2520novel%2520components%2520including%2520feature%2520sharpening%2520block%250A%2528FSB%2529%2520and%2520boundary%2520enhancement%2520module%2520%2528BEM%2529%2520for%2520enhancing%2520the%2520features%2520and%250Ahighlighting%2520the%2520boundary%2520information%2520of%2520irregular%2520waste%2520objects%2520in%2520cluttered%250Aenvironment.%2520Extensive%2520experiments%2520on%2520three%2520challenging%2520datasets%2520including%250AZeroWaste-f%252C%2520SpectralWaste%252C%2520and%2520ADE20K%2520demonstrate%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520method.%2520Our%2520COSNet%2520achieves%2520a%2520significant%2520gain%2520of%25201.8%2525%2520on%2520ZeroWaste-f%250Aand%25202.1%2525%2520on%2520SpectralWaste%2520datasets%2520respectively%2520in%2520terms%2520of%2520mIoU%2520metric.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COSNet%3A%20A%20Novel%20Semantic%20Segmentation%20Network%20using%20Enhanced%20Boundaries%0A%20%20in%20Cluttered%20Scenes&entry.906535625=Muhammad%20Ali%20and%20Mamoona%20Javaid%20and%20Mubashir%20Noman%20and%20Mustansar%20Fiaz%20and%20Salman%20Khan&entry.1292438233=%20%20Automated%20waste%20recycling%20aims%20to%20efficiently%20separate%20the%20recyclable%20objects%0Afrom%20the%20waste%20by%20employing%20vision-based%20systems.%20However%2C%20the%20presence%20of%0Avarying%20shaped%20objects%20having%20different%20material%20types%20makes%20it%20a%20challenging%0Aproblem%2C%20especially%20in%20cluttered%20environments.%20Existing%20segmentation%20methods%0Aperform%20reasonably%20on%20many%20semantic%20segmentation%20datasets%20by%20employing%0Amulti-contextual%20representations%2C%20however%2C%20their%20performance%20is%20degraded%20when%0Autilized%20for%20waste%20object%20segmentation%20in%20cluttered%20scenarios.%20In%20addition%2C%0Aplastic%20objects%20further%20increase%20the%20complexity%20of%20the%20problem%20due%20to%20their%0Atranslucent%20nature.%20To%20address%20these%20limitations%2C%20we%20introduce%20an%20efficacious%0Asegmentation%20network%2C%20named%20COSNet%2C%20that%20uses%20boundary%20cues%20along%20with%0Amulti-contextual%20information%20to%20accurately%20segment%20the%20objects%20in%20cluttered%0Ascenes.%20COSNet%20introduces%20novel%20components%20including%20feature%20sharpening%20block%0A%28FSB%29%20and%20boundary%20enhancement%20module%20%28BEM%29%20for%20enhancing%20the%20features%20and%0Ahighlighting%20the%20boundary%20information%20of%20irregular%20waste%20objects%20in%20cluttered%0Aenvironment.%20Extensive%20experiments%20on%20three%20challenging%20datasets%20including%0AZeroWaste-f%2C%20SpectralWaste%2C%20and%20ADE20K%20demonstrate%20the%20effectiveness%20of%20the%0Aproposed%20method.%20Our%20COSNet%20achieves%20a%20significant%20gain%20of%201.8%25%20on%20ZeroWaste-f%0Aand%202.1%25%20on%20SpectralWaste%20datasets%20respectively%20in%20terms%20of%20mIoU%20metric.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24139v1&entry.124074799=Read"},
{"title": "Deterministic Exploration via Stationary Bellman Error Maximization", "author": "Sebastian Griesbach and Carlo D'Eramo", "abstract": "  Exploration is a crucial and distinctive aspect of reinforcement learning\n(RL) that remains a fundamental open problem. Several methods have been\nproposed to tackle this challenge. Commonly used methods inject random noise\ndirectly into the actions, indirectly via entropy maximization, or add\nintrinsic rewards that encourage the agent to steer to novel regions of the\nstate space. Another previously seen idea is to use the Bellman error as a\nseparate optimization objective for exploration. In this paper, we introduce\nthree modifications to stabilize the latter and arrive at a deterministic\nexploration policy. Our separate exploration agent is informed about the state\nof the exploitation, thus enabling it to account for previous experiences.\nFurther components are introduced to make the exploration objective agnostic\ntoward the episode length and to mitigate instability introduced by\nfar-off-policy learning. Our experimental results show that our approach can\noutperform $\\varepsilon$-greedy in dense and sparse reward settings.\n", "link": "http://arxiv.org/abs/2410.23840v1", "date": "2024-10-31", "relevancy": 1.44, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5341}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.468}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deterministic%20Exploration%20via%20Stationary%20Bellman%20Error%20Maximization&body=Title%3A%20Deterministic%20Exploration%20via%20Stationary%20Bellman%20Error%20Maximization%0AAuthor%3A%20Sebastian%20Griesbach%20and%20Carlo%20D%27Eramo%0AAbstract%3A%20%20%20Exploration%20is%20a%20crucial%20and%20distinctive%20aspect%20of%20reinforcement%20learning%0A%28RL%29%20that%20remains%20a%20fundamental%20open%20problem.%20Several%20methods%20have%20been%0Aproposed%20to%20tackle%20this%20challenge.%20Commonly%20used%20methods%20inject%20random%20noise%0Adirectly%20into%20the%20actions%2C%20indirectly%20via%20entropy%20maximization%2C%20or%20add%0Aintrinsic%20rewards%20that%20encourage%20the%20agent%20to%20steer%20to%20novel%20regions%20of%20the%0Astate%20space.%20Another%20previously%20seen%20idea%20is%20to%20use%20the%20Bellman%20error%20as%20a%0Aseparate%20optimization%20objective%20for%20exploration.%20In%20this%20paper%2C%20we%20introduce%0Athree%20modifications%20to%20stabilize%20the%20latter%20and%20arrive%20at%20a%20deterministic%0Aexploration%20policy.%20Our%20separate%20exploration%20agent%20is%20informed%20about%20the%20state%0Aof%20the%20exploitation%2C%20thus%20enabling%20it%20to%20account%20for%20previous%20experiences.%0AFurther%20components%20are%20introduced%20to%20make%20the%20exploration%20objective%20agnostic%0Atoward%20the%20episode%20length%20and%20to%20mitigate%20instability%20introduced%20by%0Afar-off-policy%20learning.%20Our%20experimental%20results%20show%20that%20our%20approach%20can%0Aoutperform%20%24%5Cvarepsilon%24-greedy%20in%20dense%20and%20sparse%20reward%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeterministic%2520Exploration%2520via%2520Stationary%2520Bellman%2520Error%2520Maximization%26entry.906535625%3DSebastian%2520Griesbach%2520and%2520Carlo%2520D%2527Eramo%26entry.1292438233%3D%2520%2520Exploration%2520is%2520a%2520crucial%2520and%2520distinctive%2520aspect%2520of%2520reinforcement%2520learning%250A%2528RL%2529%2520that%2520remains%2520a%2520fundamental%2520open%2520problem.%2520Several%2520methods%2520have%2520been%250Aproposed%2520to%2520tackle%2520this%2520challenge.%2520Commonly%2520used%2520methods%2520inject%2520random%2520noise%250Adirectly%2520into%2520the%2520actions%252C%2520indirectly%2520via%2520entropy%2520maximization%252C%2520or%2520add%250Aintrinsic%2520rewards%2520that%2520encourage%2520the%2520agent%2520to%2520steer%2520to%2520novel%2520regions%2520of%2520the%250Astate%2520space.%2520Another%2520previously%2520seen%2520idea%2520is%2520to%2520use%2520the%2520Bellman%2520error%2520as%2520a%250Aseparate%2520optimization%2520objective%2520for%2520exploration.%2520In%2520this%2520paper%252C%2520we%2520introduce%250Athree%2520modifications%2520to%2520stabilize%2520the%2520latter%2520and%2520arrive%2520at%2520a%2520deterministic%250Aexploration%2520policy.%2520Our%2520separate%2520exploration%2520agent%2520is%2520informed%2520about%2520the%2520state%250Aof%2520the%2520exploitation%252C%2520thus%2520enabling%2520it%2520to%2520account%2520for%2520previous%2520experiences.%250AFurther%2520components%2520are%2520introduced%2520to%2520make%2520the%2520exploration%2520objective%2520agnostic%250Atoward%2520the%2520episode%2520length%2520and%2520to%2520mitigate%2520instability%2520introduced%2520by%250Afar-off-policy%2520learning.%2520Our%2520experimental%2520results%2520show%2520that%2520our%2520approach%2520can%250Aoutperform%2520%2524%255Cvarepsilon%2524-greedy%2520in%2520dense%2520and%2520sparse%2520reward%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deterministic%20Exploration%20via%20Stationary%20Bellman%20Error%20Maximization&entry.906535625=Sebastian%20Griesbach%20and%20Carlo%20D%27Eramo&entry.1292438233=%20%20Exploration%20is%20a%20crucial%20and%20distinctive%20aspect%20of%20reinforcement%20learning%0A%28RL%29%20that%20remains%20a%20fundamental%20open%20problem.%20Several%20methods%20have%20been%0Aproposed%20to%20tackle%20this%20challenge.%20Commonly%20used%20methods%20inject%20random%20noise%0Adirectly%20into%20the%20actions%2C%20indirectly%20via%20entropy%20maximization%2C%20or%20add%0Aintrinsic%20rewards%20that%20encourage%20the%20agent%20to%20steer%20to%20novel%20regions%20of%20the%0Astate%20space.%20Another%20previously%20seen%20idea%20is%20to%20use%20the%20Bellman%20error%20as%20a%0Aseparate%20optimization%20objective%20for%20exploration.%20In%20this%20paper%2C%20we%20introduce%0Athree%20modifications%20to%20stabilize%20the%20latter%20and%20arrive%20at%20a%20deterministic%0Aexploration%20policy.%20Our%20separate%20exploration%20agent%20is%20informed%20about%20the%20state%0Aof%20the%20exploitation%2C%20thus%20enabling%20it%20to%20account%20for%20previous%20experiences.%0AFurther%20components%20are%20introduced%20to%20make%20the%20exploration%20objective%20agnostic%0Atoward%20the%20episode%20length%20and%20to%20mitigate%20instability%20introduced%20by%0Afar-off-policy%20learning.%20Our%20experimental%20results%20show%20that%20our%20approach%20can%0Aoutperform%20%24%5Cvarepsilon%24-greedy%20in%20dense%20and%20sparse%20reward%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23840v1&entry.124074799=Read"},
{"title": "Exploiting Information Theory for Intuitive Robot Programming of Manual\n  Activities", "author": "Elena Merlo and Marta Lagomarsino and Edoardo Lamon and Arash Ajoudani", "abstract": "  Observational learning is a promising approach to enable people without\nexpertise in programming to transfer skills to robots in a user-friendly\nmanner, since it mirrors how humans learn new behaviors by observing others.\nMany existing methods focus on instructing robots to mimic human trajectories,\nbut motion-level strategies often pose challenges in skills generalization\nacross diverse environments. This paper proposes a novel framework that allows\nrobots to achieve a \\textit{higher-level} understanding of human-demonstrated\nmanual tasks recorded in RGB videos. By recognizing the task structure and\ngoals, robots generalize what observed to unseen scenarios. We found our task\nrepresentation on Shannon's Information Theory (IT), which is applied for the\nfirst time to manual tasks. IT helps extract the active scene elements and\nquantify the information shared between hands and objects. We exploit scene\ngraph properties to encode the extracted interaction features in a compact\nstructure and segment the demonstration into blocks, streamlining the\ngeneration of Behavior Trees for robot replicas. Experiments validated the\neffectiveness of IT to automatically generate robot execution plans from a\nsingle human demonstration. Additionally, we provide HANDSOME, an open-source\ndataset of HAND Skills demOnstrated by Multi-subjEcts, to promote further\nresearch and evaluation in this field.\n", "link": "http://arxiv.org/abs/2410.23963v1", "date": "2024-10-31", "relevancy": 1.8109, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6642}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6138}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Information%20Theory%20for%20Intuitive%20Robot%20Programming%20of%20Manual%0A%20%20Activities&body=Title%3A%20Exploiting%20Information%20Theory%20for%20Intuitive%20Robot%20Programming%20of%20Manual%0A%20%20Activities%0AAuthor%3A%20Elena%20Merlo%20and%20Marta%20Lagomarsino%20and%20Edoardo%20Lamon%20and%20Arash%20Ajoudani%0AAbstract%3A%20%20%20Observational%20learning%20is%20a%20promising%20approach%20to%20enable%20people%20without%0Aexpertise%20in%20programming%20to%20transfer%20skills%20to%20robots%20in%20a%20user-friendly%0Amanner%2C%20since%20it%20mirrors%20how%20humans%20learn%20new%20behaviors%20by%20observing%20others.%0AMany%20existing%20methods%20focus%20on%20instructing%20robots%20to%20mimic%20human%20trajectories%2C%0Abut%20motion-level%20strategies%20often%20pose%20challenges%20in%20skills%20generalization%0Aacross%20diverse%20environments.%20This%20paper%20proposes%20a%20novel%20framework%20that%20allows%0Arobots%20to%20achieve%20a%20%5Ctextit%7Bhigher-level%7D%20understanding%20of%20human-demonstrated%0Amanual%20tasks%20recorded%20in%20RGB%20videos.%20By%20recognizing%20the%20task%20structure%20and%0Agoals%2C%20robots%20generalize%20what%20observed%20to%20unseen%20scenarios.%20We%20found%20our%20task%0Arepresentation%20on%20Shannon%27s%20Information%20Theory%20%28IT%29%2C%20which%20is%20applied%20for%20the%0Afirst%20time%20to%20manual%20tasks.%20IT%20helps%20extract%20the%20active%20scene%20elements%20and%0Aquantify%20the%20information%20shared%20between%20hands%20and%20objects.%20We%20exploit%20scene%0Agraph%20properties%20to%20encode%20the%20extracted%20interaction%20features%20in%20a%20compact%0Astructure%20and%20segment%20the%20demonstration%20into%20blocks%2C%20streamlining%20the%0Ageneration%20of%20Behavior%20Trees%20for%20robot%20replicas.%20Experiments%20validated%20the%0Aeffectiveness%20of%20IT%20to%20automatically%20generate%20robot%20execution%20plans%20from%20a%0Asingle%20human%20demonstration.%20Additionally%2C%20we%20provide%20HANDSOME%2C%20an%20open-source%0Adataset%20of%20HAND%20Skills%20demOnstrated%20by%20Multi-subjEcts%2C%20to%20promote%20further%0Aresearch%20and%20evaluation%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23963v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Information%2520Theory%2520for%2520Intuitive%2520Robot%2520Programming%2520of%2520Manual%250A%2520%2520Activities%26entry.906535625%3DElena%2520Merlo%2520and%2520Marta%2520Lagomarsino%2520and%2520Edoardo%2520Lamon%2520and%2520Arash%2520Ajoudani%26entry.1292438233%3D%2520%2520Observational%2520learning%2520is%2520a%2520promising%2520approach%2520to%2520enable%2520people%2520without%250Aexpertise%2520in%2520programming%2520to%2520transfer%2520skills%2520to%2520robots%2520in%2520a%2520user-friendly%250Amanner%252C%2520since%2520it%2520mirrors%2520how%2520humans%2520learn%2520new%2520behaviors%2520by%2520observing%2520others.%250AMany%2520existing%2520methods%2520focus%2520on%2520instructing%2520robots%2520to%2520mimic%2520human%2520trajectories%252C%250Abut%2520motion-level%2520strategies%2520often%2520pose%2520challenges%2520in%2520skills%2520generalization%250Aacross%2520diverse%2520environments.%2520This%2520paper%2520proposes%2520a%2520novel%2520framework%2520that%2520allows%250Arobots%2520to%2520achieve%2520a%2520%255Ctextit%257Bhigher-level%257D%2520understanding%2520of%2520human-demonstrated%250Amanual%2520tasks%2520recorded%2520in%2520RGB%2520videos.%2520By%2520recognizing%2520the%2520task%2520structure%2520and%250Agoals%252C%2520robots%2520generalize%2520what%2520observed%2520to%2520unseen%2520scenarios.%2520We%2520found%2520our%2520task%250Arepresentation%2520on%2520Shannon%2527s%2520Information%2520Theory%2520%2528IT%2529%252C%2520which%2520is%2520applied%2520for%2520the%250Afirst%2520time%2520to%2520manual%2520tasks.%2520IT%2520helps%2520extract%2520the%2520active%2520scene%2520elements%2520and%250Aquantify%2520the%2520information%2520shared%2520between%2520hands%2520and%2520objects.%2520We%2520exploit%2520scene%250Agraph%2520properties%2520to%2520encode%2520the%2520extracted%2520interaction%2520features%2520in%2520a%2520compact%250Astructure%2520and%2520segment%2520the%2520demonstration%2520into%2520blocks%252C%2520streamlining%2520the%250Ageneration%2520of%2520Behavior%2520Trees%2520for%2520robot%2520replicas.%2520Experiments%2520validated%2520the%250Aeffectiveness%2520of%2520IT%2520to%2520automatically%2520generate%2520robot%2520execution%2520plans%2520from%2520a%250Asingle%2520human%2520demonstration.%2520Additionally%252C%2520we%2520provide%2520HANDSOME%252C%2520an%2520open-source%250Adataset%2520of%2520HAND%2520Skills%2520demOnstrated%2520by%2520Multi-subjEcts%252C%2520to%2520promote%2520further%250Aresearch%2520and%2520evaluation%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23963v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Information%20Theory%20for%20Intuitive%20Robot%20Programming%20of%20Manual%0A%20%20Activities&entry.906535625=Elena%20Merlo%20and%20Marta%20Lagomarsino%20and%20Edoardo%20Lamon%20and%20Arash%20Ajoudani&entry.1292438233=%20%20Observational%20learning%20is%20a%20promising%20approach%20to%20enable%20people%20without%0Aexpertise%20in%20programming%20to%20transfer%20skills%20to%20robots%20in%20a%20user-friendly%0Amanner%2C%20since%20it%20mirrors%20how%20humans%20learn%20new%20behaviors%20by%20observing%20others.%0AMany%20existing%20methods%20focus%20on%20instructing%20robots%20to%20mimic%20human%20trajectories%2C%0Abut%20motion-level%20strategies%20often%20pose%20challenges%20in%20skills%20generalization%0Aacross%20diverse%20environments.%20This%20paper%20proposes%20a%20novel%20framework%20that%20allows%0Arobots%20to%20achieve%20a%20%5Ctextit%7Bhigher-level%7D%20understanding%20of%20human-demonstrated%0Amanual%20tasks%20recorded%20in%20RGB%20videos.%20By%20recognizing%20the%20task%20structure%20and%0Agoals%2C%20robots%20generalize%20what%20observed%20to%20unseen%20scenarios.%20We%20found%20our%20task%0Arepresentation%20on%20Shannon%27s%20Information%20Theory%20%28IT%29%2C%20which%20is%20applied%20for%20the%0Afirst%20time%20to%20manual%20tasks.%20IT%20helps%20extract%20the%20active%20scene%20elements%20and%0Aquantify%20the%20information%20shared%20between%20hands%20and%20objects.%20We%20exploit%20scene%0Agraph%20properties%20to%20encode%20the%20extracted%20interaction%20features%20in%20a%20compact%0Astructure%20and%20segment%20the%20demonstration%20into%20blocks%2C%20streamlining%20the%0Ageneration%20of%20Behavior%20Trees%20for%20robot%20replicas.%20Experiments%20validated%20the%0Aeffectiveness%20of%20IT%20to%20automatically%20generate%20robot%20execution%20plans%20from%20a%0Asingle%20human%20demonstration.%20Additionally%2C%20we%20provide%20HANDSOME%2C%20an%20open-source%0Adataset%20of%20HAND%20Skills%20demOnstrated%20by%20Multi-subjEcts%2C%20to%20promote%20further%0Aresearch%20and%20evaluation%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23963v1&entry.124074799=Read"},
{"title": "Ensemble sampling for linear bandits: small ensembles suffice", "author": "David Janz and Alexander E. Litvak and Csaba Szepesv\u00e1ri", "abstract": "  We provide the first useful and rigorous analysis of ensemble sampling for\nthe stochastic linear bandit setting. In particular, we show that, under\nstandard assumptions, for a $d$-dimensional stochastic linear bandit with an\ninteraction horizon $T$, ensemble sampling with an ensemble of size of order\n$\\smash{d \\log T}$ incurs regret at most of the order $\\smash{(d \\log T)^{5/2}\n\\sqrt{T}}$. Ours is the first result in any structured setting not to require\nthe size of the ensemble to scale linearly with $T$ -- which defeats the\npurpose of ensemble sampling -- while obtaining near $\\smash{\\sqrt{T}}$ order\nregret. Ours is also the first result that allows infinite action sets.\n", "link": "http://arxiv.org/abs/2311.08376v3", "date": "2024-10-31", "relevancy": 1.6392, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4495}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3861}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3796}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ensemble%20sampling%20for%20linear%20bandits%3A%20small%20ensembles%20suffice&body=Title%3A%20Ensemble%20sampling%20for%20linear%20bandits%3A%20small%20ensembles%20suffice%0AAuthor%3A%20David%20Janz%20and%20Alexander%20E.%20Litvak%20and%20Csaba%20Szepesv%C3%A1ri%0AAbstract%3A%20%20%20We%20provide%20the%20first%20useful%20and%20rigorous%20analysis%20of%20ensemble%20sampling%20for%0Athe%20stochastic%20linear%20bandit%20setting.%20In%20particular%2C%20we%20show%20that%2C%20under%0Astandard%20assumptions%2C%20for%20a%20%24d%24-dimensional%20stochastic%20linear%20bandit%20with%20an%0Ainteraction%20horizon%20%24T%24%2C%20ensemble%20sampling%20with%20an%20ensemble%20of%20size%20of%20order%0A%24%5Csmash%7Bd%20%5Clog%20T%7D%24%20incurs%20regret%20at%20most%20of%20the%20order%20%24%5Csmash%7B%28d%20%5Clog%20T%29%5E%7B5/2%7D%0A%5Csqrt%7BT%7D%7D%24.%20Ours%20is%20the%20first%20result%20in%20any%20structured%20setting%20not%20to%20require%0Athe%20size%20of%20the%20ensemble%20to%20scale%20linearly%20with%20%24T%24%20--%20which%20defeats%20the%0Apurpose%20of%20ensemble%20sampling%20--%20while%20obtaining%20near%20%24%5Csmash%7B%5Csqrt%7BT%7D%7D%24%20order%0Aregret.%20Ours%20is%20also%20the%20first%20result%20that%20allows%20infinite%20action%20sets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.08376v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnsemble%2520sampling%2520for%2520linear%2520bandits%253A%2520small%2520ensembles%2520suffice%26entry.906535625%3DDavid%2520Janz%2520and%2520Alexander%2520E.%2520Litvak%2520and%2520Csaba%2520Szepesv%25C3%25A1ri%26entry.1292438233%3D%2520%2520We%2520provide%2520the%2520first%2520useful%2520and%2520rigorous%2520analysis%2520of%2520ensemble%2520sampling%2520for%250Athe%2520stochastic%2520linear%2520bandit%2520setting.%2520In%2520particular%252C%2520we%2520show%2520that%252C%2520under%250Astandard%2520assumptions%252C%2520for%2520a%2520%2524d%2524-dimensional%2520stochastic%2520linear%2520bandit%2520with%2520an%250Ainteraction%2520horizon%2520%2524T%2524%252C%2520ensemble%2520sampling%2520with%2520an%2520ensemble%2520of%2520size%2520of%2520order%250A%2524%255Csmash%257Bd%2520%255Clog%2520T%257D%2524%2520incurs%2520regret%2520at%2520most%2520of%2520the%2520order%2520%2524%255Csmash%257B%2528d%2520%255Clog%2520T%2529%255E%257B5/2%257D%250A%255Csqrt%257BT%257D%257D%2524.%2520Ours%2520is%2520the%2520first%2520result%2520in%2520any%2520structured%2520setting%2520not%2520to%2520require%250Athe%2520size%2520of%2520the%2520ensemble%2520to%2520scale%2520linearly%2520with%2520%2524T%2524%2520--%2520which%2520defeats%2520the%250Apurpose%2520of%2520ensemble%2520sampling%2520--%2520while%2520obtaining%2520near%2520%2524%255Csmash%257B%255Csqrt%257BT%257D%257D%2524%2520order%250Aregret.%2520Ours%2520is%2520also%2520the%2520first%2520result%2520that%2520allows%2520infinite%2520action%2520sets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.08376v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ensemble%20sampling%20for%20linear%20bandits%3A%20small%20ensembles%20suffice&entry.906535625=David%20Janz%20and%20Alexander%20E.%20Litvak%20and%20Csaba%20Szepesv%C3%A1ri&entry.1292438233=%20%20We%20provide%20the%20first%20useful%20and%20rigorous%20analysis%20of%20ensemble%20sampling%20for%0Athe%20stochastic%20linear%20bandit%20setting.%20In%20particular%2C%20we%20show%20that%2C%20under%0Astandard%20assumptions%2C%20for%20a%20%24d%24-dimensional%20stochastic%20linear%20bandit%20with%20an%0Ainteraction%20horizon%20%24T%24%2C%20ensemble%20sampling%20with%20an%20ensemble%20of%20size%20of%20order%0A%24%5Csmash%7Bd%20%5Clog%20T%7D%24%20incurs%20regret%20at%20most%20of%20the%20order%20%24%5Csmash%7B%28d%20%5Clog%20T%29%5E%7B5/2%7D%0A%5Csqrt%7BT%7D%7D%24.%20Ours%20is%20the%20first%20result%20in%20any%20structured%20setting%20not%20to%20require%0Athe%20size%20of%20the%20ensemble%20to%20scale%20linearly%20with%20%24T%24%20--%20which%20defeats%20the%0Apurpose%20of%20ensemble%20sampling%20--%20while%20obtaining%20near%20%24%5Csmash%7B%5Csqrt%7BT%7D%7D%24%20order%0Aregret.%20Ours%20is%20also%20the%20first%20result%20that%20allows%20infinite%20action%20sets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.08376v3&entry.124074799=Read"},
{"title": "Conformal prediction of circular data", "author": "Paulo C. Marques F. and Rinaldo Artes and Helton Graziadei", "abstract": "  Split conformal prediction techniques are applied to regression problems with\ncircular responses by introducing a suitable conformity score, leading to\nprediction sets with adaptive arc length and finite-sample coverage guarantees\nfor any circular predictive model under exchangeable data. Leveraging the high\nperformance of existing predictive models designed for linear responses, we\nanalyze a general projection procedure that converts any linear response\nregression model into one suitable for circular responses. When random forests\nserve as basis models in this projection procedure, we harness the out-of-bag\ndynamics to eliminate the necessity for a separate calibration sample in the\nconstruction of prediction sets. For synthetic and real datasets the resulting\nprojected random forests model produces more efficient out-of-bag conformal\nprediction sets, with shorter median arc length, when compared to the split\nconformal prediction sets generated by two existing alternative models.\n", "link": "http://arxiv.org/abs/2410.24145v1", "date": "2024-10-31", "relevancy": 0.8733, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4574}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4299}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4226}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conformal%20prediction%20of%20circular%20data&body=Title%3A%20Conformal%20prediction%20of%20circular%20data%0AAuthor%3A%20Paulo%20C.%20Marques%20F.%20and%20Rinaldo%20Artes%20and%20Helton%20Graziadei%0AAbstract%3A%20%20%20Split%20conformal%20prediction%20techniques%20are%20applied%20to%20regression%20problems%20with%0Acircular%20responses%20by%20introducing%20a%20suitable%20conformity%20score%2C%20leading%20to%0Aprediction%20sets%20with%20adaptive%20arc%20length%20and%20finite-sample%20coverage%20guarantees%0Afor%20any%20circular%20predictive%20model%20under%20exchangeable%20data.%20Leveraging%20the%20high%0Aperformance%20of%20existing%20predictive%20models%20designed%20for%20linear%20responses%2C%20we%0Aanalyze%20a%20general%20projection%20procedure%20that%20converts%20any%20linear%20response%0Aregression%20model%20into%20one%20suitable%20for%20circular%20responses.%20When%20random%20forests%0Aserve%20as%20basis%20models%20in%20this%20projection%20procedure%2C%20we%20harness%20the%20out-of-bag%0Adynamics%20to%20eliminate%20the%20necessity%20for%20a%20separate%20calibration%20sample%20in%20the%0Aconstruction%20of%20prediction%20sets.%20For%20synthetic%20and%20real%20datasets%20the%20resulting%0Aprojected%20random%20forests%20model%20produces%20more%20efficient%20out-of-bag%20conformal%0Aprediction%20sets%2C%20with%20shorter%20median%20arc%20length%2C%20when%20compared%20to%20the%20split%0Aconformal%20prediction%20sets%20generated%20by%20two%20existing%20alternative%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConformal%2520prediction%2520of%2520circular%2520data%26entry.906535625%3DPaulo%2520C.%2520Marques%2520F.%2520and%2520Rinaldo%2520Artes%2520and%2520Helton%2520Graziadei%26entry.1292438233%3D%2520%2520Split%2520conformal%2520prediction%2520techniques%2520are%2520applied%2520to%2520regression%2520problems%2520with%250Acircular%2520responses%2520by%2520introducing%2520a%2520suitable%2520conformity%2520score%252C%2520leading%2520to%250Aprediction%2520sets%2520with%2520adaptive%2520arc%2520length%2520and%2520finite-sample%2520coverage%2520guarantees%250Afor%2520any%2520circular%2520predictive%2520model%2520under%2520exchangeable%2520data.%2520Leveraging%2520the%2520high%250Aperformance%2520of%2520existing%2520predictive%2520models%2520designed%2520for%2520linear%2520responses%252C%2520we%250Aanalyze%2520a%2520general%2520projection%2520procedure%2520that%2520converts%2520any%2520linear%2520response%250Aregression%2520model%2520into%2520one%2520suitable%2520for%2520circular%2520responses.%2520When%2520random%2520forests%250Aserve%2520as%2520basis%2520models%2520in%2520this%2520projection%2520procedure%252C%2520we%2520harness%2520the%2520out-of-bag%250Adynamics%2520to%2520eliminate%2520the%2520necessity%2520for%2520a%2520separate%2520calibration%2520sample%2520in%2520the%250Aconstruction%2520of%2520prediction%2520sets.%2520For%2520synthetic%2520and%2520real%2520datasets%2520the%2520resulting%250Aprojected%2520random%2520forests%2520model%2520produces%2520more%2520efficient%2520out-of-bag%2520conformal%250Aprediction%2520sets%252C%2520with%2520shorter%2520median%2520arc%2520length%252C%2520when%2520compared%2520to%2520the%2520split%250Aconformal%2520prediction%2520sets%2520generated%2520by%2520two%2520existing%2520alternative%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conformal%20prediction%20of%20circular%20data&entry.906535625=Paulo%20C.%20Marques%20F.%20and%20Rinaldo%20Artes%20and%20Helton%20Graziadei&entry.1292438233=%20%20Split%20conformal%20prediction%20techniques%20are%20applied%20to%20regression%20problems%20with%0Acircular%20responses%20by%20introducing%20a%20suitable%20conformity%20score%2C%20leading%20to%0Aprediction%20sets%20with%20adaptive%20arc%20length%20and%20finite-sample%20coverage%20guarantees%0Afor%20any%20circular%20predictive%20model%20under%20exchangeable%20data.%20Leveraging%20the%20high%0Aperformance%20of%20existing%20predictive%20models%20designed%20for%20linear%20responses%2C%20we%0Aanalyze%20a%20general%20projection%20procedure%20that%20converts%20any%20linear%20response%0Aregression%20model%20into%20one%20suitable%20for%20circular%20responses.%20When%20random%20forests%0Aserve%20as%20basis%20models%20in%20this%20projection%20procedure%2C%20we%20harness%20the%20out-of-bag%0Adynamics%20to%20eliminate%20the%20necessity%20for%20a%20separate%20calibration%20sample%20in%20the%0Aconstruction%20of%20prediction%20sets.%20For%20synthetic%20and%20real%20datasets%20the%20resulting%0Aprojected%20random%20forests%20model%20produces%20more%20efficient%20out-of-bag%20conformal%0Aprediction%20sets%2C%20with%20shorter%20median%20arc%20length%2C%20when%20compared%20to%20the%20split%0Aconformal%20prediction%20sets%20generated%20by%20two%20existing%20alternative%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24145v1&entry.124074799=Read"},
{"title": "Transformers to Predict the Applicability of Symbolic Integration\n  Routines", "author": "Rashid Barket and Uzma Shafiq and Matthew England and Juergen Gerhard", "abstract": "  Symbolic integration is a fundamental problem in mathematics: we consider how\nmachine learning may be used to optimise this task in a Computer Algebra System\n(CAS). We train transformers that predict whether a particular integration\nmethod will be successful, and compare against the existing human-made\nheuristics (called guards) that perform this task in a leading CAS. We find the\ntransformer can outperform these guards, gaining up to 30% accuracy and 70%\nprecision. We further show that the inference time of the transformer is\ninconsequential which shows that it is well-suited to include as a guard in a\nCAS. Furthermore, we use Layer Integrated Gradients to interpret the decisions\nthat the transformer is making. If guided by a subject-matter expert, the\ntechnique can explain some of the predictions based on the input tokens, which\ncan lead to further optimisations.\n", "link": "http://arxiv.org/abs/2410.23948v1", "date": "2024-10-31", "relevancy": 1.4413, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5112}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4733}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformers%20to%20Predict%20the%20Applicability%20of%20Symbolic%20Integration%0A%20%20Routines&body=Title%3A%20Transformers%20to%20Predict%20the%20Applicability%20of%20Symbolic%20Integration%0A%20%20Routines%0AAuthor%3A%20Rashid%20Barket%20and%20Uzma%20Shafiq%20and%20Matthew%20England%20and%20Juergen%20Gerhard%0AAbstract%3A%20%20%20Symbolic%20integration%20is%20a%20fundamental%20problem%20in%20mathematics%3A%20we%20consider%20how%0Amachine%20learning%20may%20be%20used%20to%20optimise%20this%20task%20in%20a%20Computer%20Algebra%20System%0A%28CAS%29.%20We%20train%20transformers%20that%20predict%20whether%20a%20particular%20integration%0Amethod%20will%20be%20successful%2C%20and%20compare%20against%20the%20existing%20human-made%0Aheuristics%20%28called%20guards%29%20that%20perform%20this%20task%20in%20a%20leading%20CAS.%20We%20find%20the%0Atransformer%20can%20outperform%20these%20guards%2C%20gaining%20up%20to%2030%25%20accuracy%20and%2070%25%0Aprecision.%20We%20further%20show%20that%20the%20inference%20time%20of%20the%20transformer%20is%0Ainconsequential%20which%20shows%20that%20it%20is%20well-suited%20to%20include%20as%20a%20guard%20in%20a%0ACAS.%20Furthermore%2C%20we%20use%20Layer%20Integrated%20Gradients%20to%20interpret%20the%20decisions%0Athat%20the%20transformer%20is%20making.%20If%20guided%20by%20a%20subject-matter%20expert%2C%20the%0Atechnique%20can%20explain%20some%20of%20the%20predictions%20based%20on%20the%20input%20tokens%2C%20which%0Acan%20lead%20to%20further%20optimisations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23948v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformers%2520to%2520Predict%2520the%2520Applicability%2520of%2520Symbolic%2520Integration%250A%2520%2520Routines%26entry.906535625%3DRashid%2520Barket%2520and%2520Uzma%2520Shafiq%2520and%2520Matthew%2520England%2520and%2520Juergen%2520Gerhard%26entry.1292438233%3D%2520%2520Symbolic%2520integration%2520is%2520a%2520fundamental%2520problem%2520in%2520mathematics%253A%2520we%2520consider%2520how%250Amachine%2520learning%2520may%2520be%2520used%2520to%2520optimise%2520this%2520task%2520in%2520a%2520Computer%2520Algebra%2520System%250A%2528CAS%2529.%2520We%2520train%2520transformers%2520that%2520predict%2520whether%2520a%2520particular%2520integration%250Amethod%2520will%2520be%2520successful%252C%2520and%2520compare%2520against%2520the%2520existing%2520human-made%250Aheuristics%2520%2528called%2520guards%2529%2520that%2520perform%2520this%2520task%2520in%2520a%2520leading%2520CAS.%2520We%2520find%2520the%250Atransformer%2520can%2520outperform%2520these%2520guards%252C%2520gaining%2520up%2520to%252030%2525%2520accuracy%2520and%252070%2525%250Aprecision.%2520We%2520further%2520show%2520that%2520the%2520inference%2520time%2520of%2520the%2520transformer%2520is%250Ainconsequential%2520which%2520shows%2520that%2520it%2520is%2520well-suited%2520to%2520include%2520as%2520a%2520guard%2520in%2520a%250ACAS.%2520Furthermore%252C%2520we%2520use%2520Layer%2520Integrated%2520Gradients%2520to%2520interpret%2520the%2520decisions%250Athat%2520the%2520transformer%2520is%2520making.%2520If%2520guided%2520by%2520a%2520subject-matter%2520expert%252C%2520the%250Atechnique%2520can%2520explain%2520some%2520of%2520the%2520predictions%2520based%2520on%2520the%2520input%2520tokens%252C%2520which%250Acan%2520lead%2520to%2520further%2520optimisations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23948v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformers%20to%20Predict%20the%20Applicability%20of%20Symbolic%20Integration%0A%20%20Routines&entry.906535625=Rashid%20Barket%20and%20Uzma%20Shafiq%20and%20Matthew%20England%20and%20Juergen%20Gerhard&entry.1292438233=%20%20Symbolic%20integration%20is%20a%20fundamental%20problem%20in%20mathematics%3A%20we%20consider%20how%0Amachine%20learning%20may%20be%20used%20to%20optimise%20this%20task%20in%20a%20Computer%20Algebra%20System%0A%28CAS%29.%20We%20train%20transformers%20that%20predict%20whether%20a%20particular%20integration%0Amethod%20will%20be%20successful%2C%20and%20compare%20against%20the%20existing%20human-made%0Aheuristics%20%28called%20guards%29%20that%20perform%20this%20task%20in%20a%20leading%20CAS.%20We%20find%20the%0Atransformer%20can%20outperform%20these%20guards%2C%20gaining%20up%20to%2030%25%20accuracy%20and%2070%25%0Aprecision.%20We%20further%20show%20that%20the%20inference%20time%20of%20the%20transformer%20is%0Ainconsequential%20which%20shows%20that%20it%20is%20well-suited%20to%20include%20as%20a%20guard%20in%20a%0ACAS.%20Furthermore%2C%20we%20use%20Layer%20Integrated%20Gradients%20to%20interpret%20the%20decisions%0Athat%20the%20transformer%20is%20making.%20If%20guided%20by%20a%20subject-matter%20expert%2C%20the%0Atechnique%20can%20explain%20some%20of%20the%20predictions%20based%20on%20the%20input%20tokens%2C%20which%0Acan%20lead%20to%20further%20optimisations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23948v1&entry.124074799=Read"},
{"title": "Quantum-Assisted Simulation: A Framework for Developing Machine Learning\n  Models in Quantum Computing", "author": "Minati Rath and Hema Date", "abstract": "  Machine Learning (ML) models are trained using historical data to classify\nnew, unseen data. However, traditional computing resources often struggle to\nhandle the immense amount of data, commonly known as Big Data, within a\nreasonable time frame. Quantum Computing (QC) provides a novel approach to\ninformation processing, offering the potential to process classical data\nexponentially faster than classical computing through quantum algorithms. By\nmapping Quantum Machine Learning (QML) algorithms into the quantum mechanical\ndomain, we can potentially achieve exponential improvements in data processing\nspeed, reduced resource requirements, and enhanced accuracy and efficiency. In\nthis article, we delve into both the QC and ML fields, exploring the interplay\nof ideas between them, as well as the current capabilities and limitations of\nhardware. We investigate the history of quantum computing, examine existing QML\nalgorithms, and present a simplified procedure for setting up simulations of\nQML algorithms, making it accessible and understandable for readers.\nFurthermore, we conduct simulations on a dataset using both traditional machine\nlearning and quantum machine learning approaches. We then compare their\nrespective performances by utilizing a quantum simulator.\n", "link": "http://arxiv.org/abs/2311.10363v2", "date": "2024-10-31", "relevancy": 1.3435, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4679}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4476}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum-Assisted%20Simulation%3A%20A%20Framework%20for%20Developing%20Machine%20Learning%0A%20%20Models%20in%20Quantum%20Computing&body=Title%3A%20Quantum-Assisted%20Simulation%3A%20A%20Framework%20for%20Developing%20Machine%20Learning%0A%20%20Models%20in%20Quantum%20Computing%0AAuthor%3A%20Minati%20Rath%20and%20Hema%20Date%0AAbstract%3A%20%20%20Machine%20Learning%20%28ML%29%20models%20are%20trained%20using%20historical%20data%20to%20classify%0Anew%2C%20unseen%20data.%20However%2C%20traditional%20computing%20resources%20often%20struggle%20to%0Ahandle%20the%20immense%20amount%20of%20data%2C%20commonly%20known%20as%20Big%20Data%2C%20within%20a%0Areasonable%20time%20frame.%20Quantum%20Computing%20%28QC%29%20provides%20a%20novel%20approach%20to%0Ainformation%20processing%2C%20offering%20the%20potential%20to%20process%20classical%20data%0Aexponentially%20faster%20than%20classical%20computing%20through%20quantum%20algorithms.%20By%0Amapping%20Quantum%20Machine%20Learning%20%28QML%29%20algorithms%20into%20the%20quantum%20mechanical%0Adomain%2C%20we%20can%20potentially%20achieve%20exponential%20improvements%20in%20data%20processing%0Aspeed%2C%20reduced%20resource%20requirements%2C%20and%20enhanced%20accuracy%20and%20efficiency.%20In%0Athis%20article%2C%20we%20delve%20into%20both%20the%20QC%20and%20ML%20fields%2C%20exploring%20the%20interplay%0Aof%20ideas%20between%20them%2C%20as%20well%20as%20the%20current%20capabilities%20and%20limitations%20of%0Ahardware.%20We%20investigate%20the%20history%20of%20quantum%20computing%2C%20examine%20existing%20QML%0Aalgorithms%2C%20and%20present%20a%20simplified%20procedure%20for%20setting%20up%20simulations%20of%0AQML%20algorithms%2C%20making%20it%20accessible%20and%20understandable%20for%20readers.%0AFurthermore%2C%20we%20conduct%20simulations%20on%20a%20dataset%20using%20both%20traditional%20machine%0Alearning%20and%20quantum%20machine%20learning%20approaches.%20We%20then%20compare%20their%0Arespective%20performances%20by%20utilizing%20a%20quantum%20simulator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.10363v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum-Assisted%2520Simulation%253A%2520A%2520Framework%2520for%2520Developing%2520Machine%2520Learning%250A%2520%2520Models%2520in%2520Quantum%2520Computing%26entry.906535625%3DMinati%2520Rath%2520and%2520Hema%2520Date%26entry.1292438233%3D%2520%2520Machine%2520Learning%2520%2528ML%2529%2520models%2520are%2520trained%2520using%2520historical%2520data%2520to%2520classify%250Anew%252C%2520unseen%2520data.%2520However%252C%2520traditional%2520computing%2520resources%2520often%2520struggle%2520to%250Ahandle%2520the%2520immense%2520amount%2520of%2520data%252C%2520commonly%2520known%2520as%2520Big%2520Data%252C%2520within%2520a%250Areasonable%2520time%2520frame.%2520Quantum%2520Computing%2520%2528QC%2529%2520provides%2520a%2520novel%2520approach%2520to%250Ainformation%2520processing%252C%2520offering%2520the%2520potential%2520to%2520process%2520classical%2520data%250Aexponentially%2520faster%2520than%2520classical%2520computing%2520through%2520quantum%2520algorithms.%2520By%250Amapping%2520Quantum%2520Machine%2520Learning%2520%2528QML%2529%2520algorithms%2520into%2520the%2520quantum%2520mechanical%250Adomain%252C%2520we%2520can%2520potentially%2520achieve%2520exponential%2520improvements%2520in%2520data%2520processing%250Aspeed%252C%2520reduced%2520resource%2520requirements%252C%2520and%2520enhanced%2520accuracy%2520and%2520efficiency.%2520In%250Athis%2520article%252C%2520we%2520delve%2520into%2520both%2520the%2520QC%2520and%2520ML%2520fields%252C%2520exploring%2520the%2520interplay%250Aof%2520ideas%2520between%2520them%252C%2520as%2520well%2520as%2520the%2520current%2520capabilities%2520and%2520limitations%2520of%250Ahardware.%2520We%2520investigate%2520the%2520history%2520of%2520quantum%2520computing%252C%2520examine%2520existing%2520QML%250Aalgorithms%252C%2520and%2520present%2520a%2520simplified%2520procedure%2520for%2520setting%2520up%2520simulations%2520of%250AQML%2520algorithms%252C%2520making%2520it%2520accessible%2520and%2520understandable%2520for%2520readers.%250AFurthermore%252C%2520we%2520conduct%2520simulations%2520on%2520a%2520dataset%2520using%2520both%2520traditional%2520machine%250Alearning%2520and%2520quantum%2520machine%2520learning%2520approaches.%2520We%2520then%2520compare%2520their%250Arespective%2520performances%2520by%2520utilizing%2520a%2520quantum%2520simulator.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.10363v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum-Assisted%20Simulation%3A%20A%20Framework%20for%20Developing%20Machine%20Learning%0A%20%20Models%20in%20Quantum%20Computing&entry.906535625=Minati%20Rath%20and%20Hema%20Date&entry.1292438233=%20%20Machine%20Learning%20%28ML%29%20models%20are%20trained%20using%20historical%20data%20to%20classify%0Anew%2C%20unseen%20data.%20However%2C%20traditional%20computing%20resources%20often%20struggle%20to%0Ahandle%20the%20immense%20amount%20of%20data%2C%20commonly%20known%20as%20Big%20Data%2C%20within%20a%0Areasonable%20time%20frame.%20Quantum%20Computing%20%28QC%29%20provides%20a%20novel%20approach%20to%0Ainformation%20processing%2C%20offering%20the%20potential%20to%20process%20classical%20data%0Aexponentially%20faster%20than%20classical%20computing%20through%20quantum%20algorithms.%20By%0Amapping%20Quantum%20Machine%20Learning%20%28QML%29%20algorithms%20into%20the%20quantum%20mechanical%0Adomain%2C%20we%20can%20potentially%20achieve%20exponential%20improvements%20in%20data%20processing%0Aspeed%2C%20reduced%20resource%20requirements%2C%20and%20enhanced%20accuracy%20and%20efficiency.%20In%0Athis%20article%2C%20we%20delve%20into%20both%20the%20QC%20and%20ML%20fields%2C%20exploring%20the%20interplay%0Aof%20ideas%20between%20them%2C%20as%20well%20as%20the%20current%20capabilities%20and%20limitations%20of%0Ahardware.%20We%20investigate%20the%20history%20of%20quantum%20computing%2C%20examine%20existing%20QML%0Aalgorithms%2C%20and%20present%20a%20simplified%20procedure%20for%20setting%20up%20simulations%20of%0AQML%20algorithms%2C%20making%20it%20accessible%20and%20understandable%20for%20readers.%0AFurthermore%2C%20we%20conduct%20simulations%20on%20a%20dataset%20using%20both%20traditional%20machine%0Alearning%20and%20quantum%20machine%20learning%20approaches.%20We%20then%20compare%20their%0Arespective%20performances%20by%20utilizing%20a%20quantum%20simulator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.10363v2&entry.124074799=Read"},
{"title": "Analysing the Interplay of Vision and Touch for Dexterous Insertion\n  Tasks", "author": "Janis Lenz and Theo Gruner and Daniel Palenicek and Tim Schneider and Jan Peters", "abstract": "  Robotic insertion tasks remain challenging due to uncertainties in perception\nand the need for precise control, particularly in unstructured environments.\nWhile humans seamlessly combine vision and touch for such tasks, effectively\nintegrating these modalities in robotic systems is still an open problem. Our\nwork presents an extensive analysis of the interplay between visual and tactile\nfeedback during dexterous insertion tasks, showing that tactile sensing can\ngreatly enhance success rates on challenging insertions with tight tolerances\nand varied hole orientations that vision alone cannot solve. These findings\nprovide valuable insights for designing more effective multi-modal robotic\ncontrol systems and highlight the critical role of tactile feedback in\ncontact-rich manipulation tasks.\n", "link": "http://arxiv.org/abs/2410.23860v1", "date": "2024-10-31", "relevancy": 1.6961, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5806}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5689}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysing%20the%20Interplay%20of%20Vision%20and%20Touch%20for%20Dexterous%20Insertion%0A%20%20Tasks&body=Title%3A%20Analysing%20the%20Interplay%20of%20Vision%20and%20Touch%20for%20Dexterous%20Insertion%0A%20%20Tasks%0AAuthor%3A%20Janis%20Lenz%20and%20Theo%20Gruner%20and%20Daniel%20Palenicek%20and%20Tim%20Schneider%20and%20Jan%20Peters%0AAbstract%3A%20%20%20Robotic%20insertion%20tasks%20remain%20challenging%20due%20to%20uncertainties%20in%20perception%0Aand%20the%20need%20for%20precise%20control%2C%20particularly%20in%20unstructured%20environments.%0AWhile%20humans%20seamlessly%20combine%20vision%20and%20touch%20for%20such%20tasks%2C%20effectively%0Aintegrating%20these%20modalities%20in%20robotic%20systems%20is%20still%20an%20open%20problem.%20Our%0Awork%20presents%20an%20extensive%20analysis%20of%20the%20interplay%20between%20visual%20and%20tactile%0Afeedback%20during%20dexterous%20insertion%20tasks%2C%20showing%20that%20tactile%20sensing%20can%0Agreatly%20enhance%20success%20rates%20on%20challenging%20insertions%20with%20tight%20tolerances%0Aand%20varied%20hole%20orientations%20that%20vision%20alone%20cannot%20solve.%20These%20findings%0Aprovide%20valuable%20insights%20for%20designing%20more%20effective%20multi-modal%20robotic%0Acontrol%20systems%20and%20highlight%20the%20critical%20role%20of%20tactile%20feedback%20in%0Acontact-rich%20manipulation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.23860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysing%2520the%2520Interplay%2520of%2520Vision%2520and%2520Touch%2520for%2520Dexterous%2520Insertion%250A%2520%2520Tasks%26entry.906535625%3DJanis%2520Lenz%2520and%2520Theo%2520Gruner%2520and%2520Daniel%2520Palenicek%2520and%2520Tim%2520Schneider%2520and%2520Jan%2520Peters%26entry.1292438233%3D%2520%2520Robotic%2520insertion%2520tasks%2520remain%2520challenging%2520due%2520to%2520uncertainties%2520in%2520perception%250Aand%2520the%2520need%2520for%2520precise%2520control%252C%2520particularly%2520in%2520unstructured%2520environments.%250AWhile%2520humans%2520seamlessly%2520combine%2520vision%2520and%2520touch%2520for%2520such%2520tasks%252C%2520effectively%250Aintegrating%2520these%2520modalities%2520in%2520robotic%2520systems%2520is%2520still%2520an%2520open%2520problem.%2520Our%250Awork%2520presents%2520an%2520extensive%2520analysis%2520of%2520the%2520interplay%2520between%2520visual%2520and%2520tactile%250Afeedback%2520during%2520dexterous%2520insertion%2520tasks%252C%2520showing%2520that%2520tactile%2520sensing%2520can%250Agreatly%2520enhance%2520success%2520rates%2520on%2520challenging%2520insertions%2520with%2520tight%2520tolerances%250Aand%2520varied%2520hole%2520orientations%2520that%2520vision%2520alone%2520cannot%2520solve.%2520These%2520findings%250Aprovide%2520valuable%2520insights%2520for%2520designing%2520more%2520effective%2520multi-modal%2520robotic%250Acontrol%2520systems%2520and%2520highlight%2520the%2520critical%2520role%2520of%2520tactile%2520feedback%2520in%250Acontact-rich%2520manipulation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.23860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysing%20the%20Interplay%20of%20Vision%20and%20Touch%20for%20Dexterous%20Insertion%0A%20%20Tasks&entry.906535625=Janis%20Lenz%20and%20Theo%20Gruner%20and%20Daniel%20Palenicek%20and%20Tim%20Schneider%20and%20Jan%20Peters&entry.1292438233=%20%20Robotic%20insertion%20tasks%20remain%20challenging%20due%20to%20uncertainties%20in%20perception%0Aand%20the%20need%20for%20precise%20control%2C%20particularly%20in%20unstructured%20environments.%0AWhile%20humans%20seamlessly%20combine%20vision%20and%20touch%20for%20such%20tasks%2C%20effectively%0Aintegrating%20these%20modalities%20in%20robotic%20systems%20is%20still%20an%20open%20problem.%20Our%0Awork%20presents%20an%20extensive%20analysis%20of%20the%20interplay%20between%20visual%20and%20tactile%0Afeedback%20during%20dexterous%20insertion%20tasks%2C%20showing%20that%20tactile%20sensing%20can%0Agreatly%20enhance%20success%20rates%20on%20challenging%20insertions%20with%20tight%20tolerances%0Aand%20varied%20hole%20orientations%20that%20vision%20alone%20cannot%20solve.%20These%20findings%0Aprovide%20valuable%20insights%20for%20designing%20more%20effective%20multi-modal%20robotic%0Acontrol%20systems%20and%20highlight%20the%20critical%20role%20of%20tactile%20feedback%20in%0Acontact-rich%20manipulation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.23860v1&entry.124074799=Read"},
{"title": "Transition Constrained Bayesian Optimization via Markov Decision\n  Processes", "author": "Jose Pablo Folch and Calvin Tsay and Robert M Lee and Behrang Shafei and Weronika Ormaniec and Andreas Krause and Mark van der Wilk and Ruth Misener and Mojm\u00edr Mutn\u00fd", "abstract": "  Bayesian optimization is a methodology to optimize black-box functions.\nTraditionally, it focuses on the setting where you can arbitrarily query the\nsearch space. However, many real-life problems do not offer this flexibility;\nin particular, the search space of the next query may depend on previous ones.\nExample challenges arise in the physical sciences in the form of local movement\nconstraints, required monotonicity in certain variables, and transitions\ninfluencing the accuracy of measurements. Altogether, such transition\nconstraints necessitate a form of planning. This work extends classical\nBayesian optimization via the framework of Markov Decision Processes. We\niteratively solve a tractable linearization of our utility function using\nreinforcement learning to obtain a policy that plans ahead for the entire\nhorizon. This is a parallel to the optimization of an acquisition function in\npolicy space. The resulting policy is potentially history-dependent and\nnon-Markovian. We showcase applications in chemical reactor optimization,\ninformative path planning, machine calibration, and other synthetic examples.\n", "link": "http://arxiv.org/abs/2402.08406v3", "date": "2024-10-31", "relevancy": 1.0107, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.589}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4652}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transition%20Constrained%20Bayesian%20Optimization%20via%20Markov%20Decision%0A%20%20Processes&body=Title%3A%20Transition%20Constrained%20Bayesian%20Optimization%20via%20Markov%20Decision%0A%20%20Processes%0AAuthor%3A%20Jose%20Pablo%20Folch%20and%20Calvin%20Tsay%20and%20Robert%20M%20Lee%20and%20Behrang%20Shafei%20and%20Weronika%20Ormaniec%20and%20Andreas%20Krause%20and%20Mark%20van%20der%20Wilk%20and%20Ruth%20Misener%20and%20Mojm%C3%ADr%20Mutn%C3%BD%0AAbstract%3A%20%20%20Bayesian%20optimization%20is%20a%20methodology%20to%20optimize%20black-box%20functions.%0ATraditionally%2C%20it%20focuses%20on%20the%20setting%20where%20you%20can%20arbitrarily%20query%20the%0Asearch%20space.%20However%2C%20many%20real-life%20problems%20do%20not%20offer%20this%20flexibility%3B%0Ain%20particular%2C%20the%20search%20space%20of%20the%20next%20query%20may%20depend%20on%20previous%20ones.%0AExample%20challenges%20arise%20in%20the%20physical%20sciences%20in%20the%20form%20of%20local%20movement%0Aconstraints%2C%20required%20monotonicity%20in%20certain%20variables%2C%20and%20transitions%0Ainfluencing%20the%20accuracy%20of%20measurements.%20Altogether%2C%20such%20transition%0Aconstraints%20necessitate%20a%20form%20of%20planning.%20This%20work%20extends%20classical%0ABayesian%20optimization%20via%20the%20framework%20of%20Markov%20Decision%20Processes.%20We%0Aiteratively%20solve%20a%20tractable%20linearization%20of%20our%20utility%20function%20using%0Areinforcement%20learning%20to%20obtain%20a%20policy%20that%20plans%20ahead%20for%20the%20entire%0Ahorizon.%20This%20is%20a%20parallel%20to%20the%20optimization%20of%20an%20acquisition%20function%20in%0Apolicy%20space.%20The%20resulting%20policy%20is%20potentially%20history-dependent%20and%0Anon-Markovian.%20We%20showcase%20applications%20in%20chemical%20reactor%20optimization%2C%0Ainformative%20path%20planning%2C%20machine%20calibration%2C%20and%20other%20synthetic%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08406v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransition%2520Constrained%2520Bayesian%2520Optimization%2520via%2520Markov%2520Decision%250A%2520%2520Processes%26entry.906535625%3DJose%2520Pablo%2520Folch%2520and%2520Calvin%2520Tsay%2520and%2520Robert%2520M%2520Lee%2520and%2520Behrang%2520Shafei%2520and%2520Weronika%2520Ormaniec%2520and%2520Andreas%2520Krause%2520and%2520Mark%2520van%2520der%2520Wilk%2520and%2520Ruth%2520Misener%2520and%2520Mojm%25C3%25ADr%2520Mutn%25C3%25BD%26entry.1292438233%3D%2520%2520Bayesian%2520optimization%2520is%2520a%2520methodology%2520to%2520optimize%2520black-box%2520functions.%250ATraditionally%252C%2520it%2520focuses%2520on%2520the%2520setting%2520where%2520you%2520can%2520arbitrarily%2520query%2520the%250Asearch%2520space.%2520However%252C%2520many%2520real-life%2520problems%2520do%2520not%2520offer%2520this%2520flexibility%253B%250Ain%2520particular%252C%2520the%2520search%2520space%2520of%2520the%2520next%2520query%2520may%2520depend%2520on%2520previous%2520ones.%250AExample%2520challenges%2520arise%2520in%2520the%2520physical%2520sciences%2520in%2520the%2520form%2520of%2520local%2520movement%250Aconstraints%252C%2520required%2520monotonicity%2520in%2520certain%2520variables%252C%2520and%2520transitions%250Ainfluencing%2520the%2520accuracy%2520of%2520measurements.%2520Altogether%252C%2520such%2520transition%250Aconstraints%2520necessitate%2520a%2520form%2520of%2520planning.%2520This%2520work%2520extends%2520classical%250ABayesian%2520optimization%2520via%2520the%2520framework%2520of%2520Markov%2520Decision%2520Processes.%2520We%250Aiteratively%2520solve%2520a%2520tractable%2520linearization%2520of%2520our%2520utility%2520function%2520using%250Areinforcement%2520learning%2520to%2520obtain%2520a%2520policy%2520that%2520plans%2520ahead%2520for%2520the%2520entire%250Ahorizon.%2520This%2520is%2520a%2520parallel%2520to%2520the%2520optimization%2520of%2520an%2520acquisition%2520function%2520in%250Apolicy%2520space.%2520The%2520resulting%2520policy%2520is%2520potentially%2520history-dependent%2520and%250Anon-Markovian.%2520We%2520showcase%2520applications%2520in%2520chemical%2520reactor%2520optimization%252C%250Ainformative%2520path%2520planning%252C%2520machine%2520calibration%252C%2520and%2520other%2520synthetic%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.08406v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transition%20Constrained%20Bayesian%20Optimization%20via%20Markov%20Decision%0A%20%20Processes&entry.906535625=Jose%20Pablo%20Folch%20and%20Calvin%20Tsay%20and%20Robert%20M%20Lee%20and%20Behrang%20Shafei%20and%20Weronika%20Ormaniec%20and%20Andreas%20Krause%20and%20Mark%20van%20der%20Wilk%20and%20Ruth%20Misener%20and%20Mojm%C3%ADr%20Mutn%C3%BD&entry.1292438233=%20%20Bayesian%20optimization%20is%20a%20methodology%20to%20optimize%20black-box%20functions.%0ATraditionally%2C%20it%20focuses%20on%20the%20setting%20where%20you%20can%20arbitrarily%20query%20the%0Asearch%20space.%20However%2C%20many%20real-life%20problems%20do%20not%20offer%20this%20flexibility%3B%0Ain%20particular%2C%20the%20search%20space%20of%20the%20next%20query%20may%20depend%20on%20previous%20ones.%0AExample%20challenges%20arise%20in%20the%20physical%20sciences%20in%20the%20form%20of%20local%20movement%0Aconstraints%2C%20required%20monotonicity%20in%20certain%20variables%2C%20and%20transitions%0Ainfluencing%20the%20accuracy%20of%20measurements.%20Altogether%2C%20such%20transition%0Aconstraints%20necessitate%20a%20form%20of%20planning.%20This%20work%20extends%20classical%0ABayesian%20optimization%20via%20the%20framework%20of%20Markov%20Decision%20Processes.%20We%0Aiteratively%20solve%20a%20tractable%20linearization%20of%20our%20utility%20function%20using%0Areinforcement%20learning%20to%20obtain%20a%20policy%20that%20plans%20ahead%20for%20the%20entire%0Ahorizon.%20This%20is%20a%20parallel%20to%20the%20optimization%20of%20an%20acquisition%20function%20in%0Apolicy%20space.%20The%20resulting%20policy%20is%20potentially%20history-dependent%20and%0Anon-Markovian.%20We%20showcase%20applications%20in%20chemical%20reactor%20optimization%2C%0Ainformative%20path%20planning%2C%20machine%20calibration%2C%20and%20other%20synthetic%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08406v3&entry.124074799=Read"},
{"title": "Graph Learning for Numeric Planning", "author": "Dillon Z. Chen and Sylvie Thi\u00e9baux", "abstract": "  Graph learning is naturally well suited for use in symbolic, object-centric\nplanning due to its ability to exploit relational structures exhibited in\nplanning domains and to take as input planning instances with arbitrary numbers\nof objects. Numeric planning is an extension of symbolic planning in which\nstates may now also exhibit numeric variables. In this work, we propose\ndata-efficient and interpretable machine learning models for learning to solve\nnumeric planning tasks. This involves constructing a new graph kernel for\ngraphs with both continuous and categorical attributes, as well as new\noptimisation methods for learning heuristic functions for numeric planning.\nExperiments show that our graph kernels are vastly more efficient and\ngeneralise better than graph neural networks for numeric planning, and also\nyield competitive coverage performance compared to domain-independent numeric\nplanners. Code is available at https://github.com/DillonZChen/goose\n", "link": "http://arxiv.org/abs/2410.24080v1", "date": "2024-10-31", "relevancy": 1.4356, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4995}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4545}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Learning%20for%20Numeric%20Planning&body=Title%3A%20Graph%20Learning%20for%20Numeric%20Planning%0AAuthor%3A%20Dillon%20Z.%20Chen%20and%20Sylvie%20Thi%C3%A9baux%0AAbstract%3A%20%20%20Graph%20learning%20is%20naturally%20well%20suited%20for%20use%20in%20symbolic%2C%20object-centric%0Aplanning%20due%20to%20its%20ability%20to%20exploit%20relational%20structures%20exhibited%20in%0Aplanning%20domains%20and%20to%20take%20as%20input%20planning%20instances%20with%20arbitrary%20numbers%0Aof%20objects.%20Numeric%20planning%20is%20an%20extension%20of%20symbolic%20planning%20in%20which%0Astates%20may%20now%20also%20exhibit%20numeric%20variables.%20In%20this%20work%2C%20we%20propose%0Adata-efficient%20and%20interpretable%20machine%20learning%20models%20for%20learning%20to%20solve%0Anumeric%20planning%20tasks.%20This%20involves%20constructing%20a%20new%20graph%20kernel%20for%0Agraphs%20with%20both%20continuous%20and%20categorical%20attributes%2C%20as%20well%20as%20new%0Aoptimisation%20methods%20for%20learning%20heuristic%20functions%20for%20numeric%20planning.%0AExperiments%20show%20that%20our%20graph%20kernels%20are%20vastly%20more%20efficient%20and%0Ageneralise%20better%20than%20graph%20neural%20networks%20for%20numeric%20planning%2C%20and%20also%0Ayield%20competitive%20coverage%20performance%20compared%20to%20domain-independent%20numeric%0Aplanners.%20Code%20is%20available%20at%20https%3A//github.com/DillonZChen/goose%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.24080v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Learning%2520for%2520Numeric%2520Planning%26entry.906535625%3DDillon%2520Z.%2520Chen%2520and%2520Sylvie%2520Thi%25C3%25A9baux%26entry.1292438233%3D%2520%2520Graph%2520learning%2520is%2520naturally%2520well%2520suited%2520for%2520use%2520in%2520symbolic%252C%2520object-centric%250Aplanning%2520due%2520to%2520its%2520ability%2520to%2520exploit%2520relational%2520structures%2520exhibited%2520in%250Aplanning%2520domains%2520and%2520to%2520take%2520as%2520input%2520planning%2520instances%2520with%2520arbitrary%2520numbers%250Aof%2520objects.%2520Numeric%2520planning%2520is%2520an%2520extension%2520of%2520symbolic%2520planning%2520in%2520which%250Astates%2520may%2520now%2520also%2520exhibit%2520numeric%2520variables.%2520In%2520this%2520work%252C%2520we%2520propose%250Adata-efficient%2520and%2520interpretable%2520machine%2520learning%2520models%2520for%2520learning%2520to%2520solve%250Anumeric%2520planning%2520tasks.%2520This%2520involves%2520constructing%2520a%2520new%2520graph%2520kernel%2520for%250Agraphs%2520with%2520both%2520continuous%2520and%2520categorical%2520attributes%252C%2520as%2520well%2520as%2520new%250Aoptimisation%2520methods%2520for%2520learning%2520heuristic%2520functions%2520for%2520numeric%2520planning.%250AExperiments%2520show%2520that%2520our%2520graph%2520kernels%2520are%2520vastly%2520more%2520efficient%2520and%250Ageneralise%2520better%2520than%2520graph%2520neural%2520networks%2520for%2520numeric%2520planning%252C%2520and%2520also%250Ayield%2520competitive%2520coverage%2520performance%2520compared%2520to%2520domain-independent%2520numeric%250Aplanners.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/DillonZChen/goose%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24080v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Learning%20for%20Numeric%20Planning&entry.906535625=Dillon%20Z.%20Chen%20and%20Sylvie%20Thi%C3%A9baux&entry.1292438233=%20%20Graph%20learning%20is%20naturally%20well%20suited%20for%20use%20in%20symbolic%2C%20object-centric%0Aplanning%20due%20to%20its%20ability%20to%20exploit%20relational%20structures%20exhibited%20in%0Aplanning%20domains%20and%20to%20take%20as%20input%20planning%20instances%20with%20arbitrary%20numbers%0Aof%20objects.%20Numeric%20planning%20is%20an%20extension%20of%20symbolic%20planning%20in%20which%0Astates%20may%20now%20also%20exhibit%20numeric%20variables.%20In%20this%20work%2C%20we%20propose%0Adata-efficient%20and%20interpretable%20machine%20learning%20models%20for%20learning%20to%20solve%0Anumeric%20planning%20tasks.%20This%20involves%20constructing%20a%20new%20graph%20kernel%20for%0Agraphs%20with%20both%20continuous%20and%20categorical%20attributes%2C%20as%20well%20as%20new%0Aoptimisation%20methods%20for%20learning%20heuristic%20functions%20for%20numeric%20planning.%0AExperiments%20show%20that%20our%20graph%20kernels%20are%20vastly%20more%20efficient%20and%0Ageneralise%20better%20than%20graph%20neural%20networks%20for%20numeric%20planning%2C%20and%20also%0Ayield%20competitive%20coverage%20performance%20compared%20to%20domain-independent%20numeric%0Aplanners.%20Code%20is%20available%20at%20https%3A//github.com/DillonZChen/goose%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.24080v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


