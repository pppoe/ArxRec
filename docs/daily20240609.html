<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240606.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Flash3D: Feed-Forward Generalisable 3D Scene Reconstruction from a\n  Single Image", "author": "Stanislaw Szymanowicz and Eldar Insafutdinov and Chuanxia Zheng and Dylan Campbell and Jo\u00e3o F. Henriques and Christian Rupprecht and Andrea Vedaldi", "abstract": "  In this paper, we propose Flash3D, a method for scene reconstruction and\nnovel view synthesis from a single image which is both very generalisable and\nefficient. For generalisability, we start from a \"foundation\" model for\nmonocular depth estimation and extend it to a full 3D shape and appearance\nreconstructor. For efficiency, we base this extension on feed-forward Gaussian\nSplatting. Specifically, we predict a first layer of 3D Gaussians at the\npredicted depth, and then add additional layers of Gaussians that are offset in\nspace, allowing the model to complete the reconstruction behind occlusions and\ntruncations. Flash3D is very efficient, trainable on a single GPU in a day, and\nthus accessible to most researchers. It achieves state-of-the-art results when\ntrained and tested on RealEstate10k. When transferred to unseen datasets like\nNYU it outperforms competitors by a large margin. More impressively, when\ntransferred to KITTI, Flash3D achieves better PSNR than methods trained\nspecifically on that dataset. In some instances, it even outperforms recent\nmethods that use multiple views as input. Code, models, demo, and more results\nare available at https://www.robots.ox.ac.uk/~vgg/research/flash3d/.\n", "link": "http://arxiv.org/abs/2406.04343v1", "date": "2024-06-06", "relevancy": 3.271, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.655}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.655}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flash3D%3A%20Feed-Forward%20Generalisable%203D%20Scene%20Reconstruction%20from%20a%0A%20%20Single%20Image&body=Title%3A%20Flash3D%3A%20Feed-Forward%20Generalisable%203D%20Scene%20Reconstruction%20from%20a%0A%20%20Single%20Image%0AAuthor%3A%20Stanislaw%20Szymanowicz%20and%20Eldar%20Insafutdinov%20and%20Chuanxia%20Zheng%20and%20Dylan%20Campbell%20and%20Jo%C3%A3o%20F.%20Henriques%20and%20Christian%20Rupprecht%20and%20Andrea%20Vedaldi%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20propose%20Flash3D%2C%20a%20method%20for%20scene%20reconstruction%20and%0Anovel%20view%20synthesis%20from%20a%20single%20image%20which%20is%20both%20very%20generalisable%20and%0Aefficient.%20For%20generalisability%2C%20we%20start%20from%20a%20%22foundation%22%20model%20for%0Amonocular%20depth%20estimation%20and%20extend%20it%20to%20a%20full%203D%20shape%20and%20appearance%0Areconstructor.%20For%20efficiency%2C%20we%20base%20this%20extension%20on%20feed-forward%20Gaussian%0ASplatting.%20Specifically%2C%20we%20predict%20a%20first%20layer%20of%203D%20Gaussians%20at%20the%0Apredicted%20depth%2C%20and%20then%20add%20additional%20layers%20of%20Gaussians%20that%20are%20offset%20in%0Aspace%2C%20allowing%20the%20model%20to%20complete%20the%20reconstruction%20behind%20occlusions%20and%0Atruncations.%20Flash3D%20is%20very%20efficient%2C%20trainable%20on%20a%20single%20GPU%20in%20a%20day%2C%20and%0Athus%20accessible%20to%20most%20researchers.%20It%20achieves%20state-of-the-art%20results%20when%0Atrained%20and%20tested%20on%20RealEstate10k.%20When%20transferred%20to%20unseen%20datasets%20like%0ANYU%20it%20outperforms%20competitors%20by%20a%20large%20margin.%20More%20impressively%2C%20when%0Atransferred%20to%20KITTI%2C%20Flash3D%20achieves%20better%20PSNR%20than%20methods%20trained%0Aspecifically%20on%20that%20dataset.%20In%20some%20instances%2C%20it%20even%20outperforms%20recent%0Amethods%20that%20use%20multiple%20views%20as%20input.%20Code%2C%20models%2C%20demo%2C%20and%20more%20results%0Aare%20available%20at%20https%3A//www.robots.ox.ac.uk/~vgg/research/flash3d/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlash3D%253A%2520Feed-Forward%2520Generalisable%25203D%2520Scene%2520Reconstruction%2520from%2520a%250A%2520%2520Single%2520Image%26entry.906535625%3DStanislaw%2520Szymanowicz%2520and%2520Eldar%2520Insafutdinov%2520and%2520Chuanxia%2520Zheng%2520and%2520Dylan%2520Campbell%2520and%2520Jo%25C3%25A3o%2520F.%2520Henriques%2520and%2520Christian%2520Rupprecht%2520and%2520Andrea%2520Vedaldi%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520Flash3D%252C%2520a%2520method%2520for%2520scene%2520reconstruction%2520and%250Anovel%2520view%2520synthesis%2520from%2520a%2520single%2520image%2520which%2520is%2520both%2520very%2520generalisable%2520and%250Aefficient.%2520For%2520generalisability%252C%2520we%2520start%2520from%2520a%2520%2522foundation%2522%2520model%2520for%250Amonocular%2520depth%2520estimation%2520and%2520extend%2520it%2520to%2520a%2520full%25203D%2520shape%2520and%2520appearance%250Areconstructor.%2520For%2520efficiency%252C%2520we%2520base%2520this%2520extension%2520on%2520feed-forward%2520Gaussian%250ASplatting.%2520Specifically%252C%2520we%2520predict%2520a%2520first%2520layer%2520of%25203D%2520Gaussians%2520at%2520the%250Apredicted%2520depth%252C%2520and%2520then%2520add%2520additional%2520layers%2520of%2520Gaussians%2520that%2520are%2520offset%2520in%250Aspace%252C%2520allowing%2520the%2520model%2520to%2520complete%2520the%2520reconstruction%2520behind%2520occlusions%2520and%250Atruncations.%2520Flash3D%2520is%2520very%2520efficient%252C%2520trainable%2520on%2520a%2520single%2520GPU%2520in%2520a%2520day%252C%2520and%250Athus%2520accessible%2520to%2520most%2520researchers.%2520It%2520achieves%2520state-of-the-art%2520results%2520when%250Atrained%2520and%2520tested%2520on%2520RealEstate10k.%2520When%2520transferred%2520to%2520unseen%2520datasets%2520like%250ANYU%2520it%2520outperforms%2520competitors%2520by%2520a%2520large%2520margin.%2520More%2520impressively%252C%2520when%250Atransferred%2520to%2520KITTI%252C%2520Flash3D%2520achieves%2520better%2520PSNR%2520than%2520methods%2520trained%250Aspecifically%2520on%2520that%2520dataset.%2520In%2520some%2520instances%252C%2520it%2520even%2520outperforms%2520recent%250Amethods%2520that%2520use%2520multiple%2520views%2520as%2520input.%2520Code%252C%2520models%252C%2520demo%252C%2520and%2520more%2520results%250Aare%2520available%2520at%2520https%253A//www.robots.ox.ac.uk/~vgg/research/flash3d/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flash3D%3A%20Feed-Forward%20Generalisable%203D%20Scene%20Reconstruction%20from%20a%0A%20%20Single%20Image&entry.906535625=Stanislaw%20Szymanowicz%20and%20Eldar%20Insafutdinov%20and%20Chuanxia%20Zheng%20and%20Dylan%20Campbell%20and%20Jo%C3%A3o%20F.%20Henriques%20and%20Christian%20Rupprecht%20and%20Andrea%20Vedaldi&entry.1292438233=%20%20In%20this%20paper%2C%20we%20propose%20Flash3D%2C%20a%20method%20for%20scene%20reconstruction%20and%0Anovel%20view%20synthesis%20from%20a%20single%20image%20which%20is%20both%20very%20generalisable%20and%0Aefficient.%20For%20generalisability%2C%20we%20start%20from%20a%20%22foundation%22%20model%20for%0Amonocular%20depth%20estimation%20and%20extend%20it%20to%20a%20full%203D%20shape%20and%20appearance%0Areconstructor.%20For%20efficiency%2C%20we%20base%20this%20extension%20on%20feed-forward%20Gaussian%0ASplatting.%20Specifically%2C%20we%20predict%20a%20first%20layer%20of%203D%20Gaussians%20at%20the%0Apredicted%20depth%2C%20and%20then%20add%20additional%20layers%20of%20Gaussians%20that%20are%20offset%20in%0Aspace%2C%20allowing%20the%20model%20to%20complete%20the%20reconstruction%20behind%20occlusions%20and%0Atruncations.%20Flash3D%20is%20very%20efficient%2C%20trainable%20on%20a%20single%20GPU%20in%20a%20day%2C%20and%0Athus%20accessible%20to%20most%20researchers.%20It%20achieves%20state-of-the-art%20results%20when%0Atrained%20and%20tested%20on%20RealEstate10k.%20When%20transferred%20to%20unseen%20datasets%20like%0ANYU%20it%20outperforms%20competitors%20by%20a%20large%20margin.%20More%20impressively%2C%20when%0Atransferred%20to%20KITTI%2C%20Flash3D%20achieves%20better%20PSNR%20than%20methods%20trained%0Aspecifically%20on%20that%20dataset.%20In%20some%20instances%2C%20it%20even%20outperforms%20recent%0Amethods%20that%20use%20multiple%20views%20as%20input.%20Code%2C%20models%2C%20demo%2C%20and%20more%20results%0Aare%20available%20at%20https%3A//www.robots.ox.ac.uk/~vgg/research/flash3d/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04343v1&entry.124074799=Read"},
{"title": "Sync4D: Video Guided Controllable Dynamics for Physics-Based 4D\n  Generation", "author": "Zhoujie Fu and Jiacheng Wei and Wenhao Shen and Chaoyue Song and Xiaofeng Yang and Fayao Liu and Xulei Yang and Guosheng Lin", "abstract": "  In this work, we introduce a novel approach for creating controllable\ndynamics in 3D-generated Gaussians using casually captured reference videos.\nOur method transfers the motion of objects from reference videos to a variety\nof generated 3D Gaussians across different categories, ensuring precise and\ncustomizable motion transfer. We achieve this by employing blend skinning-based\nnon-parametric shape reconstruction to extract the shape and motion of\nreference objects. This process involves segmenting the reference objects into\nmotion-related parts based on skinning weights and establishing shape\ncorrespondences with generated target shapes. To address shape and temporal\ninconsistencies prevalent in existing methods, we integrate physical\nsimulation, driving the target shapes with matched motion. This integration is\noptimized through a displacement loss to ensure reliable and genuine dynamics.\nOur approach supports diverse reference inputs, including humans, quadrupeds,\nand articulated objects, and can generate dynamics of arbitrary length,\nproviding enhanced fidelity and applicability. Unlike methods heavily reliant\non diffusion video generation models, our technique offers specific and\nhigh-quality motion transfer, maintaining both shape integrity and temporal\nconsistency.\n", "link": "http://arxiv.org/abs/2405.16849v2", "date": "2024-06-06", "relevancy": 3.1786, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6649}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6211}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sync4D%3A%20Video%20Guided%20Controllable%20Dynamics%20for%20Physics-Based%204D%0A%20%20Generation&body=Title%3A%20Sync4D%3A%20Video%20Guided%20Controllable%20Dynamics%20for%20Physics-Based%204D%0A%20%20Generation%0AAuthor%3A%20Zhoujie%20Fu%20and%20Jiacheng%20Wei%20and%20Wenhao%20Shen%20and%20Chaoyue%20Song%20and%20Xiaofeng%20Yang%20and%20Fayao%20Liu%20and%20Xulei%20Yang%20and%20Guosheng%20Lin%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20a%20novel%20approach%20for%20creating%20controllable%0Adynamics%20in%203D-generated%20Gaussians%20using%20casually%20captured%20reference%20videos.%0AOur%20method%20transfers%20the%20motion%20of%20objects%20from%20reference%20videos%20to%20a%20variety%0Aof%20generated%203D%20Gaussians%20across%20different%20categories%2C%20ensuring%20precise%20and%0Acustomizable%20motion%20transfer.%20We%20achieve%20this%20by%20employing%20blend%20skinning-based%0Anon-parametric%20shape%20reconstruction%20to%20extract%20the%20shape%20and%20motion%20of%0Areference%20objects.%20This%20process%20involves%20segmenting%20the%20reference%20objects%20into%0Amotion-related%20parts%20based%20on%20skinning%20weights%20and%20establishing%20shape%0Acorrespondences%20with%20generated%20target%20shapes.%20To%20address%20shape%20and%20temporal%0Ainconsistencies%20prevalent%20in%20existing%20methods%2C%20we%20integrate%20physical%0Asimulation%2C%20driving%20the%20target%20shapes%20with%20matched%20motion.%20This%20integration%20is%0Aoptimized%20through%20a%20displacement%20loss%20to%20ensure%20reliable%20and%20genuine%20dynamics.%0AOur%20approach%20supports%20diverse%20reference%20inputs%2C%20including%20humans%2C%20quadrupeds%2C%0Aand%20articulated%20objects%2C%20and%20can%20generate%20dynamics%20of%20arbitrary%20length%2C%0Aproviding%20enhanced%20fidelity%20and%20applicability.%20Unlike%20methods%20heavily%20reliant%0Aon%20diffusion%20video%20generation%20models%2C%20our%20technique%20offers%20specific%20and%0Ahigh-quality%20motion%20transfer%2C%20maintaining%20both%20shape%20integrity%20and%20temporal%0Aconsistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16849v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSync4D%253A%2520Video%2520Guided%2520Controllable%2520Dynamics%2520for%2520Physics-Based%25204D%250A%2520%2520Generation%26entry.906535625%3DZhoujie%2520Fu%2520and%2520Jiacheng%2520Wei%2520and%2520Wenhao%2520Shen%2520and%2520Chaoyue%2520Song%2520and%2520Xiaofeng%2520Yang%2520and%2520Fayao%2520Liu%2520and%2520Xulei%2520Yang%2520and%2520Guosheng%2520Lin%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520for%2520creating%2520controllable%250Adynamics%2520in%25203D-generated%2520Gaussians%2520using%2520casually%2520captured%2520reference%2520videos.%250AOur%2520method%2520transfers%2520the%2520motion%2520of%2520objects%2520from%2520reference%2520videos%2520to%2520a%2520variety%250Aof%2520generated%25203D%2520Gaussians%2520across%2520different%2520categories%252C%2520ensuring%2520precise%2520and%250Acustomizable%2520motion%2520transfer.%2520We%2520achieve%2520this%2520by%2520employing%2520blend%2520skinning-based%250Anon-parametric%2520shape%2520reconstruction%2520to%2520extract%2520the%2520shape%2520and%2520motion%2520of%250Areference%2520objects.%2520This%2520process%2520involves%2520segmenting%2520the%2520reference%2520objects%2520into%250Amotion-related%2520parts%2520based%2520on%2520skinning%2520weights%2520and%2520establishing%2520shape%250Acorrespondences%2520with%2520generated%2520target%2520shapes.%2520To%2520address%2520shape%2520and%2520temporal%250Ainconsistencies%2520prevalent%2520in%2520existing%2520methods%252C%2520we%2520integrate%2520physical%250Asimulation%252C%2520driving%2520the%2520target%2520shapes%2520with%2520matched%2520motion.%2520This%2520integration%2520is%250Aoptimized%2520through%2520a%2520displacement%2520loss%2520to%2520ensure%2520reliable%2520and%2520genuine%2520dynamics.%250AOur%2520approach%2520supports%2520diverse%2520reference%2520inputs%252C%2520including%2520humans%252C%2520quadrupeds%252C%250Aand%2520articulated%2520objects%252C%2520and%2520can%2520generate%2520dynamics%2520of%2520arbitrary%2520length%252C%250Aproviding%2520enhanced%2520fidelity%2520and%2520applicability.%2520Unlike%2520methods%2520heavily%2520reliant%250Aon%2520diffusion%2520video%2520generation%2520models%252C%2520our%2520technique%2520offers%2520specific%2520and%250Ahigh-quality%2520motion%2520transfer%252C%2520maintaining%2520both%2520shape%2520integrity%2520and%2520temporal%250Aconsistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16849v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sync4D%3A%20Video%20Guided%20Controllable%20Dynamics%20for%20Physics-Based%204D%0A%20%20Generation&entry.906535625=Zhoujie%20Fu%20and%20Jiacheng%20Wei%20and%20Wenhao%20Shen%20and%20Chaoyue%20Song%20and%20Xiaofeng%20Yang%20and%20Fayao%20Liu%20and%20Xulei%20Yang%20and%20Guosheng%20Lin&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20a%20novel%20approach%20for%20creating%20controllable%0Adynamics%20in%203D-generated%20Gaussians%20using%20casually%20captured%20reference%20videos.%0AOur%20method%20transfers%20the%20motion%20of%20objects%20from%20reference%20videos%20to%20a%20variety%0Aof%20generated%203D%20Gaussians%20across%20different%20categories%2C%20ensuring%20precise%20and%0Acustomizable%20motion%20transfer.%20We%20achieve%20this%20by%20employing%20blend%20skinning-based%0Anon-parametric%20shape%20reconstruction%20to%20extract%20the%20shape%20and%20motion%20of%0Areference%20objects.%20This%20process%20involves%20segmenting%20the%20reference%20objects%20into%0Amotion-related%20parts%20based%20on%20skinning%20weights%20and%20establishing%20shape%0Acorrespondences%20with%20generated%20target%20shapes.%20To%20address%20shape%20and%20temporal%0Ainconsistencies%20prevalent%20in%20existing%20methods%2C%20we%20integrate%20physical%0Asimulation%2C%20driving%20the%20target%20shapes%20with%20matched%20motion.%20This%20integration%20is%0Aoptimized%20through%20a%20displacement%20loss%20to%20ensure%20reliable%20and%20genuine%20dynamics.%0AOur%20approach%20supports%20diverse%20reference%20inputs%2C%20including%20humans%2C%20quadrupeds%2C%0Aand%20articulated%20objects%2C%20and%20can%20generate%20dynamics%20of%20arbitrary%20length%2C%0Aproviding%20enhanced%20fidelity%20and%20applicability.%20Unlike%20methods%20heavily%20reliant%0Aon%20diffusion%20video%20generation%20models%2C%20our%20technique%20offers%20specific%20and%0Ahigh-quality%20motion%20transfer%2C%20maintaining%20both%20shape%20integrity%20and%20temporal%0Aconsistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16849v2&entry.124074799=Read"},
{"title": "A Survey on 3D Human Avatar Modeling -- From Reconstruction to\n  Generation", "author": "Ruihe Wang and Yukang Cao and Kai Han and Kwan-Yee K. Wong", "abstract": "  3D modeling has long been an important area in computer vision and computer\ngraphics. Recently, thanks to the breakthroughs in neural representations and\ngenerative models, we witnessed a rapid development of 3D modeling. 3D human\nmodeling, lying at the core of many real-world applications, such as gaming and\nanimation, has attracted significant attention. Over the past few years, a\nlarge body of work on creating 3D human avatars has been introduced, forming a\nnew and abundant knowledge base for 3D human modeling. The scale of the\nliterature makes it difficult for individuals to keep track of all the works.\nThis survey aims to provide a comprehensive overview of these emerging\ntechniques for 3D human avatar modeling, from both reconstruction and\ngeneration perspectives. Firstly, we review representative methods for 3D human\nreconstruction, including methods based on pixel-aligned implicit function,\nneural radiance field, and 3D Gaussian Splatting, etc. We then summarize\nrepresentative methods for 3D human generation, especially those using large\nlanguage models like CLIP, diffusion models, and various 3D representations,\nwhich demonstrate state-of-the-art performance. Finally, we discuss our\nreflection on existing methods and open challenges for 3D human avatar\nmodeling, shedding light on future research.\n", "link": "http://arxiv.org/abs/2406.04253v1", "date": "2024-06-06", "relevancy": 3.1607, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6449}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6449}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%203D%20Human%20Avatar%20Modeling%20--%20From%20Reconstruction%20to%0A%20%20Generation&body=Title%3A%20A%20Survey%20on%203D%20Human%20Avatar%20Modeling%20--%20From%20Reconstruction%20to%0A%20%20Generation%0AAuthor%3A%20Ruihe%20Wang%20and%20Yukang%20Cao%20and%20Kai%20Han%20and%20Kwan-Yee%20K.%20Wong%0AAbstract%3A%20%20%203D%20modeling%20has%20long%20been%20an%20important%20area%20in%20computer%20vision%20and%20computer%0Agraphics.%20Recently%2C%20thanks%20to%20the%20breakthroughs%20in%20neural%20representations%20and%0Agenerative%20models%2C%20we%20witnessed%20a%20rapid%20development%20of%203D%20modeling.%203D%20human%0Amodeling%2C%20lying%20at%20the%20core%20of%20many%20real-world%20applications%2C%20such%20as%20gaming%20and%0Aanimation%2C%20has%20attracted%20significant%20attention.%20Over%20the%20past%20few%20years%2C%20a%0Alarge%20body%20of%20work%20on%20creating%203D%20human%20avatars%20has%20been%20introduced%2C%20forming%20a%0Anew%20and%20abundant%20knowledge%20base%20for%203D%20human%20modeling.%20The%20scale%20of%20the%0Aliterature%20makes%20it%20difficult%20for%20individuals%20to%20keep%20track%20of%20all%20the%20works.%0AThis%20survey%20aims%20to%20provide%20a%20comprehensive%20overview%20of%20these%20emerging%0Atechniques%20for%203D%20human%20avatar%20modeling%2C%20from%20both%20reconstruction%20and%0Ageneration%20perspectives.%20Firstly%2C%20we%20review%20representative%20methods%20for%203D%20human%0Areconstruction%2C%20including%20methods%20based%20on%20pixel-aligned%20implicit%20function%2C%0Aneural%20radiance%20field%2C%20and%203D%20Gaussian%20Splatting%2C%20etc.%20We%20then%20summarize%0Arepresentative%20methods%20for%203D%20human%20generation%2C%20especially%20those%20using%20large%0Alanguage%20models%20like%20CLIP%2C%20diffusion%20models%2C%20and%20various%203D%20representations%2C%0Awhich%20demonstrate%20state-of-the-art%20performance.%20Finally%2C%20we%20discuss%20our%0Areflection%20on%20existing%20methods%20and%20open%20challenges%20for%203D%20human%20avatar%0Amodeling%2C%20shedding%20light%20on%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%25203D%2520Human%2520Avatar%2520Modeling%2520--%2520From%2520Reconstruction%2520to%250A%2520%2520Generation%26entry.906535625%3DRuihe%2520Wang%2520and%2520Yukang%2520Cao%2520and%2520Kai%2520Han%2520and%2520Kwan-Yee%2520K.%2520Wong%26entry.1292438233%3D%2520%25203D%2520modeling%2520has%2520long%2520been%2520an%2520important%2520area%2520in%2520computer%2520vision%2520and%2520computer%250Agraphics.%2520Recently%252C%2520thanks%2520to%2520the%2520breakthroughs%2520in%2520neural%2520representations%2520and%250Agenerative%2520models%252C%2520we%2520witnessed%2520a%2520rapid%2520development%2520of%25203D%2520modeling.%25203D%2520human%250Amodeling%252C%2520lying%2520at%2520the%2520core%2520of%2520many%2520real-world%2520applications%252C%2520such%2520as%2520gaming%2520and%250Aanimation%252C%2520has%2520attracted%2520significant%2520attention.%2520Over%2520the%2520past%2520few%2520years%252C%2520a%250Alarge%2520body%2520of%2520work%2520on%2520creating%25203D%2520human%2520avatars%2520has%2520been%2520introduced%252C%2520forming%2520a%250Anew%2520and%2520abundant%2520knowledge%2520base%2520for%25203D%2520human%2520modeling.%2520The%2520scale%2520of%2520the%250Aliterature%2520makes%2520it%2520difficult%2520for%2520individuals%2520to%2520keep%2520track%2520of%2520all%2520the%2520works.%250AThis%2520survey%2520aims%2520to%2520provide%2520a%2520comprehensive%2520overview%2520of%2520these%2520emerging%250Atechniques%2520for%25203D%2520human%2520avatar%2520modeling%252C%2520from%2520both%2520reconstruction%2520and%250Ageneration%2520perspectives.%2520Firstly%252C%2520we%2520review%2520representative%2520methods%2520for%25203D%2520human%250Areconstruction%252C%2520including%2520methods%2520based%2520on%2520pixel-aligned%2520implicit%2520function%252C%250Aneural%2520radiance%2520field%252C%2520and%25203D%2520Gaussian%2520Splatting%252C%2520etc.%2520We%2520then%2520summarize%250Arepresentative%2520methods%2520for%25203D%2520human%2520generation%252C%2520especially%2520those%2520using%2520large%250Alanguage%2520models%2520like%2520CLIP%252C%2520diffusion%2520models%252C%2520and%2520various%25203D%2520representations%252C%250Awhich%2520demonstrate%2520state-of-the-art%2520performance.%2520Finally%252C%2520we%2520discuss%2520our%250Areflection%2520on%2520existing%2520methods%2520and%2520open%2520challenges%2520for%25203D%2520human%2520avatar%250Amodeling%252C%2520shedding%2520light%2520on%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%203D%20Human%20Avatar%20Modeling%20--%20From%20Reconstruction%20to%0A%20%20Generation&entry.906535625=Ruihe%20Wang%20and%20Yukang%20Cao%20and%20Kai%20Han%20and%20Kwan-Yee%20K.%20Wong&entry.1292438233=%20%203D%20modeling%20has%20long%20been%20an%20important%20area%20in%20computer%20vision%20and%20computer%0Agraphics.%20Recently%2C%20thanks%20to%20the%20breakthroughs%20in%20neural%20representations%20and%0Agenerative%20models%2C%20we%20witnessed%20a%20rapid%20development%20of%203D%20modeling.%203D%20human%0Amodeling%2C%20lying%20at%20the%20core%20of%20many%20real-world%20applications%2C%20such%20as%20gaming%20and%0Aanimation%2C%20has%20attracted%20significant%20attention.%20Over%20the%20past%20few%20years%2C%20a%0Alarge%20body%20of%20work%20on%20creating%203D%20human%20avatars%20has%20been%20introduced%2C%20forming%20a%0Anew%20and%20abundant%20knowledge%20base%20for%203D%20human%20modeling.%20The%20scale%20of%20the%0Aliterature%20makes%20it%20difficult%20for%20individuals%20to%20keep%20track%20of%20all%20the%20works.%0AThis%20survey%20aims%20to%20provide%20a%20comprehensive%20overview%20of%20these%20emerging%0Atechniques%20for%203D%20human%20avatar%20modeling%2C%20from%20both%20reconstruction%20and%0Ageneration%20perspectives.%20Firstly%2C%20we%20review%20representative%20methods%20for%203D%20human%0Areconstruction%2C%20including%20methods%20based%20on%20pixel-aligned%20implicit%20function%2C%0Aneural%20radiance%20field%2C%20and%203D%20Gaussian%20Splatting%2C%20etc.%20We%20then%20summarize%0Arepresentative%20methods%20for%203D%20human%20generation%2C%20especially%20those%20using%20large%0Alanguage%20models%20like%20CLIP%2C%20diffusion%20models%2C%20and%20various%203D%20representations%2C%0Awhich%20demonstrate%20state-of-the-art%20performance.%20Finally%2C%20we%20discuss%20our%0Areflection%20on%20existing%20methods%20and%20open%20challenges%20for%203D%20human%20avatar%0Amodeling%2C%20shedding%20light%20on%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04253v1&entry.124074799=Read"},
{"title": "Localized Gaussian Point Management", "author": "Haosen Yang and Chenhao Zhang and Wenqing Wang and Marco Volino and Adrian Hilton and Li Zhang and Xiatian Zhu", "abstract": "  Point management is a critical component in optimizing 3D Gaussian Splatting\n(3DGS) models, as the point initiation (e.g., via structure from motion) is\ndistributionally inappropriate. Typically, the Adaptive Density Control (ADC)\nalgorithm is applied, leveraging view-averaged gradient magnitude thresholding\nfor point densification, opacity thresholding for pruning, and regular\nall-points opacity reset. However, we reveal that this strategy is limited in\ntackling intricate/special image regions (e.g., transparent) as it is unable to\nidentify all the 3D zones that require point densification, and lacking an\nappropriate mechanism to handle the ill-conditioned points with negative\nimpacts (occlusion due to false high opacity). To address these limitations, we\npropose a Localized Point Management (LPM) strategy, capable of identifying\nthose error-contributing zones in the highest demand for both point addition\nand geometry calibration. Zone identification is achieved by leveraging the\nunderlying multiview geometry constraints, with the guidance of image rendering\nerrors. We apply point densification in the identified zone, whilst resetting\nthe opacity of those points residing in front of these regions so that a new\nopportunity is created to correct ill-conditioned points. Serving as a\nversatile plugin, LPM can be seamlessly integrated into existing 3D Gaussian\nSplatting models. Experimental evaluation across both static 3D and dynamic 4D\nscenes validate the efficacy of our LPM strategy in boosting a variety of\nexisting 3DGS models both quantitatively and qualitatively. Notably, LPM\nimproves both vanilla 3DGS and SpaceTimeGS to achieve state-of-the-art\nrendering quality while retaining real-time speeds, outperforming on\nchallenging datasets such as Tanks & Temples and the Neural 3D Video Dataset.\n", "link": "http://arxiv.org/abs/2406.04251v1", "date": "2024-06-06", "relevancy": 3.1317, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6621}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6468}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localized%20Gaussian%20Point%20Management&body=Title%3A%20Localized%20Gaussian%20Point%20Management%0AAuthor%3A%20Haosen%20Yang%20and%20Chenhao%20Zhang%20and%20Wenqing%20Wang%20and%20Marco%20Volino%20and%20Adrian%20Hilton%20and%20Li%20Zhang%20and%20Xiatian%20Zhu%0AAbstract%3A%20%20%20Point%20management%20is%20a%20critical%20component%20in%20optimizing%203D%20Gaussian%20Splatting%0A%283DGS%29%20models%2C%20as%20the%20point%20initiation%20%28e.g.%2C%20via%20structure%20from%20motion%29%20is%0Adistributionally%20inappropriate.%20Typically%2C%20the%20Adaptive%20Density%20Control%20%28ADC%29%0Aalgorithm%20is%20applied%2C%20leveraging%20view-averaged%20gradient%20magnitude%20thresholding%0Afor%20point%20densification%2C%20opacity%20thresholding%20for%20pruning%2C%20and%20regular%0Aall-points%20opacity%20reset.%20However%2C%20we%20reveal%20that%20this%20strategy%20is%20limited%20in%0Atackling%20intricate/special%20image%20regions%20%28e.g.%2C%20transparent%29%20as%20it%20is%20unable%20to%0Aidentify%20all%20the%203D%20zones%20that%20require%20point%20densification%2C%20and%20lacking%20an%0Aappropriate%20mechanism%20to%20handle%20the%20ill-conditioned%20points%20with%20negative%0Aimpacts%20%28occlusion%20due%20to%20false%20high%20opacity%29.%20To%20address%20these%20limitations%2C%20we%0Apropose%20a%20Localized%20Point%20Management%20%28LPM%29%20strategy%2C%20capable%20of%20identifying%0Athose%20error-contributing%20zones%20in%20the%20highest%20demand%20for%20both%20point%20addition%0Aand%20geometry%20calibration.%20Zone%20identification%20is%20achieved%20by%20leveraging%20the%0Aunderlying%20multiview%20geometry%20constraints%2C%20with%20the%20guidance%20of%20image%20rendering%0Aerrors.%20We%20apply%20point%20densification%20in%20the%20identified%20zone%2C%20whilst%20resetting%0Athe%20opacity%20of%20those%20points%20residing%20in%20front%20of%20these%20regions%20so%20that%20a%20new%0Aopportunity%20is%20created%20to%20correct%20ill-conditioned%20points.%20Serving%20as%20a%0Aversatile%20plugin%2C%20LPM%20can%20be%20seamlessly%20integrated%20into%20existing%203D%20Gaussian%0ASplatting%20models.%20Experimental%20evaluation%20across%20both%20static%203D%20and%20dynamic%204D%0Ascenes%20validate%20the%20efficacy%20of%20our%20LPM%20strategy%20in%20boosting%20a%20variety%20of%0Aexisting%203DGS%20models%20both%20quantitatively%20and%20qualitatively.%20Notably%2C%20LPM%0Aimproves%20both%20vanilla%203DGS%20and%20SpaceTimeGS%20to%20achieve%20state-of-the-art%0Arendering%20quality%20while%20retaining%20real-time%20speeds%2C%20outperforming%20on%0Achallenging%20datasets%20such%20as%20Tanks%20%26%20Temples%20and%20the%20Neural%203D%20Video%20Dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalized%2520Gaussian%2520Point%2520Management%26entry.906535625%3DHaosen%2520Yang%2520and%2520Chenhao%2520Zhang%2520and%2520Wenqing%2520Wang%2520and%2520Marco%2520Volino%2520and%2520Adrian%2520Hilton%2520and%2520Li%2520Zhang%2520and%2520Xiatian%2520Zhu%26entry.1292438233%3D%2520%2520Point%2520management%2520is%2520a%2520critical%2520component%2520in%2520optimizing%25203D%2520Gaussian%2520Splatting%250A%25283DGS%2529%2520models%252C%2520as%2520the%2520point%2520initiation%2520%2528e.g.%252C%2520via%2520structure%2520from%2520motion%2529%2520is%250Adistributionally%2520inappropriate.%2520Typically%252C%2520the%2520Adaptive%2520Density%2520Control%2520%2528ADC%2529%250Aalgorithm%2520is%2520applied%252C%2520leveraging%2520view-averaged%2520gradient%2520magnitude%2520thresholding%250Afor%2520point%2520densification%252C%2520opacity%2520thresholding%2520for%2520pruning%252C%2520and%2520regular%250Aall-points%2520opacity%2520reset.%2520However%252C%2520we%2520reveal%2520that%2520this%2520strategy%2520is%2520limited%2520in%250Atackling%2520intricate/special%2520image%2520regions%2520%2528e.g.%252C%2520transparent%2529%2520as%2520it%2520is%2520unable%2520to%250Aidentify%2520all%2520the%25203D%2520zones%2520that%2520require%2520point%2520densification%252C%2520and%2520lacking%2520an%250Aappropriate%2520mechanism%2520to%2520handle%2520the%2520ill-conditioned%2520points%2520with%2520negative%250Aimpacts%2520%2528occlusion%2520due%2520to%2520false%2520high%2520opacity%2529.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520a%2520Localized%2520Point%2520Management%2520%2528LPM%2529%2520strategy%252C%2520capable%2520of%2520identifying%250Athose%2520error-contributing%2520zones%2520in%2520the%2520highest%2520demand%2520for%2520both%2520point%2520addition%250Aand%2520geometry%2520calibration.%2520Zone%2520identification%2520is%2520achieved%2520by%2520leveraging%2520the%250Aunderlying%2520multiview%2520geometry%2520constraints%252C%2520with%2520the%2520guidance%2520of%2520image%2520rendering%250Aerrors.%2520We%2520apply%2520point%2520densification%2520in%2520the%2520identified%2520zone%252C%2520whilst%2520resetting%250Athe%2520opacity%2520of%2520those%2520points%2520residing%2520in%2520front%2520of%2520these%2520regions%2520so%2520that%2520a%2520new%250Aopportunity%2520is%2520created%2520to%2520correct%2520ill-conditioned%2520points.%2520Serving%2520as%2520a%250Aversatile%2520plugin%252C%2520LPM%2520can%2520be%2520seamlessly%2520integrated%2520into%2520existing%25203D%2520Gaussian%250ASplatting%2520models.%2520Experimental%2520evaluation%2520across%2520both%2520static%25203D%2520and%2520dynamic%25204D%250Ascenes%2520validate%2520the%2520efficacy%2520of%2520our%2520LPM%2520strategy%2520in%2520boosting%2520a%2520variety%2520of%250Aexisting%25203DGS%2520models%2520both%2520quantitatively%2520and%2520qualitatively.%2520Notably%252C%2520LPM%250Aimproves%2520both%2520vanilla%25203DGS%2520and%2520SpaceTimeGS%2520to%2520achieve%2520state-of-the-art%250Arendering%2520quality%2520while%2520retaining%2520real-time%2520speeds%252C%2520outperforming%2520on%250Achallenging%2520datasets%2520such%2520as%2520Tanks%2520%2526%2520Temples%2520and%2520the%2520Neural%25203D%2520Video%2520Dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localized%20Gaussian%20Point%20Management&entry.906535625=Haosen%20Yang%20and%20Chenhao%20Zhang%20and%20Wenqing%20Wang%20and%20Marco%20Volino%20and%20Adrian%20Hilton%20and%20Li%20Zhang%20and%20Xiatian%20Zhu&entry.1292438233=%20%20Point%20management%20is%20a%20critical%20component%20in%20optimizing%203D%20Gaussian%20Splatting%0A%283DGS%29%20models%2C%20as%20the%20point%20initiation%20%28e.g.%2C%20via%20structure%20from%20motion%29%20is%0Adistributionally%20inappropriate.%20Typically%2C%20the%20Adaptive%20Density%20Control%20%28ADC%29%0Aalgorithm%20is%20applied%2C%20leveraging%20view-averaged%20gradient%20magnitude%20thresholding%0Afor%20point%20densification%2C%20opacity%20thresholding%20for%20pruning%2C%20and%20regular%0Aall-points%20opacity%20reset.%20However%2C%20we%20reveal%20that%20this%20strategy%20is%20limited%20in%0Atackling%20intricate/special%20image%20regions%20%28e.g.%2C%20transparent%29%20as%20it%20is%20unable%20to%0Aidentify%20all%20the%203D%20zones%20that%20require%20point%20densification%2C%20and%20lacking%20an%0Aappropriate%20mechanism%20to%20handle%20the%20ill-conditioned%20points%20with%20negative%0Aimpacts%20%28occlusion%20due%20to%20false%20high%20opacity%29.%20To%20address%20these%20limitations%2C%20we%0Apropose%20a%20Localized%20Point%20Management%20%28LPM%29%20strategy%2C%20capable%20of%20identifying%0Athose%20error-contributing%20zones%20in%20the%20highest%20demand%20for%20both%20point%20addition%0Aand%20geometry%20calibration.%20Zone%20identification%20is%20achieved%20by%20leveraging%20the%0Aunderlying%20multiview%20geometry%20constraints%2C%20with%20the%20guidance%20of%20image%20rendering%0Aerrors.%20We%20apply%20point%20densification%20in%20the%20identified%20zone%2C%20whilst%20resetting%0Athe%20opacity%20of%20those%20points%20residing%20in%20front%20of%20these%20regions%20so%20that%20a%20new%0Aopportunity%20is%20created%20to%20correct%20ill-conditioned%20points.%20Serving%20as%20a%0Aversatile%20plugin%2C%20LPM%20can%20be%20seamlessly%20integrated%20into%20existing%203D%20Gaussian%0ASplatting%20models.%20Experimental%20evaluation%20across%20both%20static%203D%20and%20dynamic%204D%0Ascenes%20validate%20the%20efficacy%20of%20our%20LPM%20strategy%20in%20boosting%20a%20variety%20of%0Aexisting%203DGS%20models%20both%20quantitatively%20and%20qualitatively.%20Notably%2C%20LPM%0Aimproves%20both%20vanilla%203DGS%20and%20SpaceTimeGS%20to%20achieve%20state-of-the-art%0Arendering%20quality%20while%20retaining%20real-time%20speeds%2C%20outperforming%20on%0Achallenging%20datasets%20such%20as%20Tanks%20%26%20Temples%20and%20the%20Neural%203D%20Video%20Dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04251v1&entry.124074799=Read"},
{"title": "Physics3D: Learning Physical Properties of 3D Gaussians via Video\n  Diffusion", "author": "Fangfu Liu and Hanyang Wang and Shunyu Yao and Shengjun Zhang and Jie Zhou and Yueqi Duan", "abstract": "  In recent years, there has been rapid development in 3D generation models,\nopening up new possibilities for applications such as simulating the dynamic\nmovements of 3D objects and customizing their behaviors. However, current 3D\ngenerative models tend to focus only on surface features such as color and\nshape, neglecting the inherent physical properties that govern the behavior of\nobjects in the real world. To accurately simulate physics-aligned dynamics, it\nis essential to predict the physical properties of materials and incorporate\nthem into the behavior prediction process. Nonetheless, predicting the diverse\nmaterials of real-world objects is still challenging due to the complex nature\nof their physical attributes. In this paper, we propose \\textbf{Physics3D}, a\nnovel method for learning various physical properties of 3D objects through a\nvideo diffusion model. Our approach involves designing a highly generalizable\nphysical simulation system based on a viscoelastic material model, which\nenables us to simulate a wide range of materials with high-fidelity\ncapabilities. Moreover, we distill the physical priors from a video diffusion\nmodel that contains more understanding of realistic object materials. Extensive\nexperiments demonstrate the effectiveness of our method with both elastic and\nplastic materials. Physics3D shows great potential for bridging the gap between\nthe physical world and virtual neural space, providing a better integration and\napplication of realistic physical principles in virtual environments. Project\npage: https://liuff19.github.io/Physics3D.\n", "link": "http://arxiv.org/abs/2406.04338v1", "date": "2024-06-06", "relevancy": 3.1177, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.631}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6198}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics3D%3A%20Learning%20Physical%20Properties%20of%203D%20Gaussians%20via%20Video%0A%20%20Diffusion&body=Title%3A%20Physics3D%3A%20Learning%20Physical%20Properties%20of%203D%20Gaussians%20via%20Video%0A%20%20Diffusion%0AAuthor%3A%20Fangfu%20Liu%20and%20Hanyang%20Wang%20and%20Shunyu%20Yao%20and%20Shengjun%20Zhang%20and%20Jie%20Zhou%20and%20Yueqi%20Duan%0AAbstract%3A%20%20%20In%20recent%20years%2C%20there%20has%20been%20rapid%20development%20in%203D%20generation%20models%2C%0Aopening%20up%20new%20possibilities%20for%20applications%20such%20as%20simulating%20the%20dynamic%0Amovements%20of%203D%20objects%20and%20customizing%20their%20behaviors.%20However%2C%20current%203D%0Agenerative%20models%20tend%20to%20focus%20only%20on%20surface%20features%20such%20as%20color%20and%0Ashape%2C%20neglecting%20the%20inherent%20physical%20properties%20that%20govern%20the%20behavior%20of%0Aobjects%20in%20the%20real%20world.%20To%20accurately%20simulate%20physics-aligned%20dynamics%2C%20it%0Ais%20essential%20to%20predict%20the%20physical%20properties%20of%20materials%20and%20incorporate%0Athem%20into%20the%20behavior%20prediction%20process.%20Nonetheless%2C%20predicting%20the%20diverse%0Amaterials%20of%20real-world%20objects%20is%20still%20challenging%20due%20to%20the%20complex%20nature%0Aof%20their%20physical%20attributes.%20In%20this%20paper%2C%20we%20propose%20%5Ctextbf%7BPhysics3D%7D%2C%20a%0Anovel%20method%20for%20learning%20various%20physical%20properties%20of%203D%20objects%20through%20a%0Avideo%20diffusion%20model.%20Our%20approach%20involves%20designing%20a%20highly%20generalizable%0Aphysical%20simulation%20system%20based%20on%20a%20viscoelastic%20material%20model%2C%20which%0Aenables%20us%20to%20simulate%20a%20wide%20range%20of%20materials%20with%20high-fidelity%0Acapabilities.%20Moreover%2C%20we%20distill%20the%20physical%20priors%20from%20a%20video%20diffusion%0Amodel%20that%20contains%20more%20understanding%20of%20realistic%20object%20materials.%20Extensive%0Aexperiments%20demonstrate%20the%20effectiveness%20of%20our%20method%20with%20both%20elastic%20and%0Aplastic%20materials.%20Physics3D%20shows%20great%20potential%20for%20bridging%20the%20gap%20between%0Athe%20physical%20world%20and%20virtual%20neural%20space%2C%20providing%20a%20better%20integration%20and%0Aapplication%20of%20realistic%20physical%20principles%20in%20virtual%20environments.%20Project%0Apage%3A%20https%3A//liuff19.github.io/Physics3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04338v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics3D%253A%2520Learning%2520Physical%2520Properties%2520of%25203D%2520Gaussians%2520via%2520Video%250A%2520%2520Diffusion%26entry.906535625%3DFangfu%2520Liu%2520and%2520Hanyang%2520Wang%2520and%2520Shunyu%2520Yao%2520and%2520Shengjun%2520Zhang%2520and%2520Jie%2520Zhou%2520and%2520Yueqi%2520Duan%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520there%2520has%2520been%2520rapid%2520development%2520in%25203D%2520generation%2520models%252C%250Aopening%2520up%2520new%2520possibilities%2520for%2520applications%2520such%2520as%2520simulating%2520the%2520dynamic%250Amovements%2520of%25203D%2520objects%2520and%2520customizing%2520their%2520behaviors.%2520However%252C%2520current%25203D%250Agenerative%2520models%2520tend%2520to%2520focus%2520only%2520on%2520surface%2520features%2520such%2520as%2520color%2520and%250Ashape%252C%2520neglecting%2520the%2520inherent%2520physical%2520properties%2520that%2520govern%2520the%2520behavior%2520of%250Aobjects%2520in%2520the%2520real%2520world.%2520To%2520accurately%2520simulate%2520physics-aligned%2520dynamics%252C%2520it%250Ais%2520essential%2520to%2520predict%2520the%2520physical%2520properties%2520of%2520materials%2520and%2520incorporate%250Athem%2520into%2520the%2520behavior%2520prediction%2520process.%2520Nonetheless%252C%2520predicting%2520the%2520diverse%250Amaterials%2520of%2520real-world%2520objects%2520is%2520still%2520challenging%2520due%2520to%2520the%2520complex%2520nature%250Aof%2520their%2520physical%2520attributes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520%255Ctextbf%257BPhysics3D%257D%252C%2520a%250Anovel%2520method%2520for%2520learning%2520various%2520physical%2520properties%2520of%25203D%2520objects%2520through%2520a%250Avideo%2520diffusion%2520model.%2520Our%2520approach%2520involves%2520designing%2520a%2520highly%2520generalizable%250Aphysical%2520simulation%2520system%2520based%2520on%2520a%2520viscoelastic%2520material%2520model%252C%2520which%250Aenables%2520us%2520to%2520simulate%2520a%2520wide%2520range%2520of%2520materials%2520with%2520high-fidelity%250Acapabilities.%2520Moreover%252C%2520we%2520distill%2520the%2520physical%2520priors%2520from%2520a%2520video%2520diffusion%250Amodel%2520that%2520contains%2520more%2520understanding%2520of%2520realistic%2520object%2520materials.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520with%2520both%2520elastic%2520and%250Aplastic%2520materials.%2520Physics3D%2520shows%2520great%2520potential%2520for%2520bridging%2520the%2520gap%2520between%250Athe%2520physical%2520world%2520and%2520virtual%2520neural%2520space%252C%2520providing%2520a%2520better%2520integration%2520and%250Aapplication%2520of%2520realistic%2520physical%2520principles%2520in%2520virtual%2520environments.%2520Project%250Apage%253A%2520https%253A//liuff19.github.io/Physics3D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04338v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics3D%3A%20Learning%20Physical%20Properties%20of%203D%20Gaussians%20via%20Video%0A%20%20Diffusion&entry.906535625=Fangfu%20Liu%20and%20Hanyang%20Wang%20and%20Shunyu%20Yao%20and%20Shengjun%20Zhang%20and%20Jie%20Zhou%20and%20Yueqi%20Duan&entry.1292438233=%20%20In%20recent%20years%2C%20there%20has%20been%20rapid%20development%20in%203D%20generation%20models%2C%0Aopening%20up%20new%20possibilities%20for%20applications%20such%20as%20simulating%20the%20dynamic%0Amovements%20of%203D%20objects%20and%20customizing%20their%20behaviors.%20However%2C%20current%203D%0Agenerative%20models%20tend%20to%20focus%20only%20on%20surface%20features%20such%20as%20color%20and%0Ashape%2C%20neglecting%20the%20inherent%20physical%20properties%20that%20govern%20the%20behavior%20of%0Aobjects%20in%20the%20real%20world.%20To%20accurately%20simulate%20physics-aligned%20dynamics%2C%20it%0Ais%20essential%20to%20predict%20the%20physical%20properties%20of%20materials%20and%20incorporate%0Athem%20into%20the%20behavior%20prediction%20process.%20Nonetheless%2C%20predicting%20the%20diverse%0Amaterials%20of%20real-world%20objects%20is%20still%20challenging%20due%20to%20the%20complex%20nature%0Aof%20their%20physical%20attributes.%20In%20this%20paper%2C%20we%20propose%20%5Ctextbf%7BPhysics3D%7D%2C%20a%0Anovel%20method%20for%20learning%20various%20physical%20properties%20of%203D%20objects%20through%20a%0Avideo%20diffusion%20model.%20Our%20approach%20involves%20designing%20a%20highly%20generalizable%0Aphysical%20simulation%20system%20based%20on%20a%20viscoelastic%20material%20model%2C%20which%0Aenables%20us%20to%20simulate%20a%20wide%20range%20of%20materials%20with%20high-fidelity%0Acapabilities.%20Moreover%2C%20we%20distill%20the%20physical%20priors%20from%20a%20video%20diffusion%0Amodel%20that%20contains%20more%20understanding%20of%20realistic%20object%20materials.%20Extensive%0Aexperiments%20demonstrate%20the%20effectiveness%20of%20our%20method%20with%20both%20elastic%20and%0Aplastic%20materials.%20Physics3D%20shows%20great%20potential%20for%20bridging%20the%20gap%20between%0Athe%20physical%20world%20and%20virtual%20neural%20space%2C%20providing%20a%20better%20integration%20and%0Aapplication%20of%20realistic%20physical%20principles%20in%20virtual%20environments.%20Project%0Apage%3A%20https%3A//liuff19.github.io/Physics3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04338v1&entry.124074799=Read"},
{"title": "MVTN: Learning Multi-View Transformations for 3D Understanding", "author": "Abdullah Hamdi and Faisal AlZahrani and Silvio Giancola and Bernard Ghanem", "abstract": "  Multi-view projection techniques have shown themselves to be highly effective\nin achieving top-performing results in the recognition of 3D shapes. These\nmethods involve learning how to combine information from multiple view-points.\nHowever, the camera view-points from which these views are obtained are often\nfixed for all shapes. To overcome the static nature of current multi-view\ntechniques, we propose learning these view-points. Specifically, we introduce\nthe Multi-View Transformation Network (MVTN), which uses differentiable\nrendering to determine optimal view-points for 3D shape recognition. As a\nresult, MVTN can be trained end-to-end with any multi-view network for 3D shape\nclassification. We integrate MVTN into a novel adaptive multi-view pipeline\nthat is capable of rendering both 3D meshes and point clouds. Our approach\ndemonstrates state-of-the-art performance in 3D classification and shape\nretrieval on several benchmarks (ModelNet40, ScanObjectNN, ShapeNet Core55).\nFurther analysis indicates that our approach exhibits improved robustness to\nocclusion compared to other methods. We also investigate additional aspects of\nMVTN, such as 2D pretraining and its use for segmentation. To support further\nresearch in this area, we have released MVTorch, a PyTorch library for 3D\nunderstanding and generation using multi-view projections.\n", "link": "http://arxiv.org/abs/2212.13462v2", "date": "2024-06-06", "relevancy": 3.084, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6585}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.596}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVTN%3A%20Learning%20Multi-View%20Transformations%20for%203D%20Understanding&body=Title%3A%20MVTN%3A%20Learning%20Multi-View%20Transformations%20for%203D%20Understanding%0AAuthor%3A%20Abdullah%20Hamdi%20and%20Faisal%20AlZahrani%20and%20Silvio%20Giancola%20and%20Bernard%20Ghanem%0AAbstract%3A%20%20%20Multi-view%20projection%20techniques%20have%20shown%20themselves%20to%20be%20highly%20effective%0Ain%20achieving%20top-performing%20results%20in%20the%20recognition%20of%203D%20shapes.%20These%0Amethods%20involve%20learning%20how%20to%20combine%20information%20from%20multiple%20view-points.%0AHowever%2C%20the%20camera%20view-points%20from%20which%20these%20views%20are%20obtained%20are%20often%0Afixed%20for%20all%20shapes.%20To%20overcome%20the%20static%20nature%20of%20current%20multi-view%0Atechniques%2C%20we%20propose%20learning%20these%20view-points.%20Specifically%2C%20we%20introduce%0Athe%20Multi-View%20Transformation%20Network%20%28MVTN%29%2C%20which%20uses%20differentiable%0Arendering%20to%20determine%20optimal%20view-points%20for%203D%20shape%20recognition.%20As%20a%0Aresult%2C%20MVTN%20can%20be%20trained%20end-to-end%20with%20any%20multi-view%20network%20for%203D%20shape%0Aclassification.%20We%20integrate%20MVTN%20into%20a%20novel%20adaptive%20multi-view%20pipeline%0Athat%20is%20capable%20of%20rendering%20both%203D%20meshes%20and%20point%20clouds.%20Our%20approach%0Ademonstrates%20state-of-the-art%20performance%20in%203D%20classification%20and%20shape%0Aretrieval%20on%20several%20benchmarks%20%28ModelNet40%2C%20ScanObjectNN%2C%20ShapeNet%20Core55%29.%0AFurther%20analysis%20indicates%20that%20our%20approach%20exhibits%20improved%20robustness%20to%0Aocclusion%20compared%20to%20other%20methods.%20We%20also%20investigate%20additional%20aspects%20of%0AMVTN%2C%20such%20as%202D%20pretraining%20and%20its%20use%20for%20segmentation.%20To%20support%20further%0Aresearch%20in%20this%20area%2C%20we%20have%20released%20MVTorch%2C%20a%20PyTorch%20library%20for%203D%0Aunderstanding%20and%20generation%20using%20multi-view%20projections.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.13462v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVTN%253A%2520Learning%2520Multi-View%2520Transformations%2520for%25203D%2520Understanding%26entry.906535625%3DAbdullah%2520Hamdi%2520and%2520Faisal%2520AlZahrani%2520and%2520Silvio%2520Giancola%2520and%2520Bernard%2520Ghanem%26entry.1292438233%3D%2520%2520Multi-view%2520projection%2520techniques%2520have%2520shown%2520themselves%2520to%2520be%2520highly%2520effective%250Ain%2520achieving%2520top-performing%2520results%2520in%2520the%2520recognition%2520of%25203D%2520shapes.%2520These%250Amethods%2520involve%2520learning%2520how%2520to%2520combine%2520information%2520from%2520multiple%2520view-points.%250AHowever%252C%2520the%2520camera%2520view-points%2520from%2520which%2520these%2520views%2520are%2520obtained%2520are%2520often%250Afixed%2520for%2520all%2520shapes.%2520To%2520overcome%2520the%2520static%2520nature%2520of%2520current%2520multi-view%250Atechniques%252C%2520we%2520propose%2520learning%2520these%2520view-points.%2520Specifically%252C%2520we%2520introduce%250Athe%2520Multi-View%2520Transformation%2520Network%2520%2528MVTN%2529%252C%2520which%2520uses%2520differentiable%250Arendering%2520to%2520determine%2520optimal%2520view-points%2520for%25203D%2520shape%2520recognition.%2520As%2520a%250Aresult%252C%2520MVTN%2520can%2520be%2520trained%2520end-to-end%2520with%2520any%2520multi-view%2520network%2520for%25203D%2520shape%250Aclassification.%2520We%2520integrate%2520MVTN%2520into%2520a%2520novel%2520adaptive%2520multi-view%2520pipeline%250Athat%2520is%2520capable%2520of%2520rendering%2520both%25203D%2520meshes%2520and%2520point%2520clouds.%2520Our%2520approach%250Ademonstrates%2520state-of-the-art%2520performance%2520in%25203D%2520classification%2520and%2520shape%250Aretrieval%2520on%2520several%2520benchmarks%2520%2528ModelNet40%252C%2520ScanObjectNN%252C%2520ShapeNet%2520Core55%2529.%250AFurther%2520analysis%2520indicates%2520that%2520our%2520approach%2520exhibits%2520improved%2520robustness%2520to%250Aocclusion%2520compared%2520to%2520other%2520methods.%2520We%2520also%2520investigate%2520additional%2520aspects%2520of%250AMVTN%252C%2520such%2520as%25202D%2520pretraining%2520and%2520its%2520use%2520for%2520segmentation.%2520To%2520support%2520further%250Aresearch%2520in%2520this%2520area%252C%2520we%2520have%2520released%2520MVTorch%252C%2520a%2520PyTorch%2520library%2520for%25203D%250Aunderstanding%2520and%2520generation%2520using%2520multi-view%2520projections.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.13462v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVTN%3A%20Learning%20Multi-View%20Transformations%20for%203D%20Understanding&entry.906535625=Abdullah%20Hamdi%20and%20Faisal%20AlZahrani%20and%20Silvio%20Giancola%20and%20Bernard%20Ghanem&entry.1292438233=%20%20Multi-view%20projection%20techniques%20have%20shown%20themselves%20to%20be%20highly%20effective%0Ain%20achieving%20top-performing%20results%20in%20the%20recognition%20of%203D%20shapes.%20These%0Amethods%20involve%20learning%20how%20to%20combine%20information%20from%20multiple%20view-points.%0AHowever%2C%20the%20camera%20view-points%20from%20which%20these%20views%20are%20obtained%20are%20often%0Afixed%20for%20all%20shapes.%20To%20overcome%20the%20static%20nature%20of%20current%20multi-view%0Atechniques%2C%20we%20propose%20learning%20these%20view-points.%20Specifically%2C%20we%20introduce%0Athe%20Multi-View%20Transformation%20Network%20%28MVTN%29%2C%20which%20uses%20differentiable%0Arendering%20to%20determine%20optimal%20view-points%20for%203D%20shape%20recognition.%20As%20a%0Aresult%2C%20MVTN%20can%20be%20trained%20end-to-end%20with%20any%20multi-view%20network%20for%203D%20shape%0Aclassification.%20We%20integrate%20MVTN%20into%20a%20novel%20adaptive%20multi-view%20pipeline%0Athat%20is%20capable%20of%20rendering%20both%203D%20meshes%20and%20point%20clouds.%20Our%20approach%0Ademonstrates%20state-of-the-art%20performance%20in%203D%20classification%20and%20shape%0Aretrieval%20on%20several%20benchmarks%20%28ModelNet40%2C%20ScanObjectNN%2C%20ShapeNet%20Core55%29.%0AFurther%20analysis%20indicates%20that%20our%20approach%20exhibits%20improved%20robustness%20to%0Aocclusion%20compared%20to%20other%20methods.%20We%20also%20investigate%20additional%20aspects%20of%0AMVTN%2C%20such%20as%202D%20pretraining%20and%20its%20use%20for%20segmentation.%20To%20support%20further%0Aresearch%20in%20this%20area%2C%20we%20have%20released%20MVTorch%2C%20a%20PyTorch%20library%20for%203D%0Aunderstanding%20and%20generation%20using%20multi-view%20projections.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.13462v2&entry.124074799=Read"},
{"title": "GLACE: Global Local Accelerated Coordinate Encoding", "author": "Fangjinhua Wang and Xudong Jiang and Silvano Galliani and Christoph Vogel and Marc Pollefeys", "abstract": "  Scene coordinate regression (SCR) methods are a family of visual localization\nmethods that directly regress 2D-3D matches for camera pose estimation. They\nare effective in small-scale scenes but face significant challenges in\nlarge-scale scenes that are further amplified in the absence of ground truth 3D\npoint clouds for supervision. Here, the model can only rely on reprojection\nconstraints and needs to implicitly triangulate the points. The challenges stem\nfrom a fundamental dilemma: The network has to be invariant to observations of\nthe same landmark at different viewpoints and lighting conditions, etc., but at\nthe same time discriminate unrelated but similar observations. The latter\nbecomes more relevant and severe in larger scenes. In this work, we tackle this\nproblem by introducing the concept of co-visibility to the network. We propose\nGLACE, which integrates pre-trained global and local encodings and enables SCR\nto scale to large scenes with only a single small-sized network. Specifically,\nwe propose a novel feature diffusion technique that implicitly groups the\nreprojection constraints with co-visibility and avoids overfitting to trivial\nsolutions. Additionally, our position decoder parameterizes the output\npositions for large-scale scenes more effectively. Without using 3D models or\ndepth maps for supervision, our method achieves state-of-the-art results on\nlarge-scale scenes with a low-map-size model. On Cambridge landmarks, with a\nsingle model, we achieve 17% lower median position error than Poker, the\nensemble variant of the state-of-the-art SCR method ACE. Code is available at:\nhttps://github.com/cvg/glace.\n", "link": "http://arxiv.org/abs/2406.04340v1", "date": "2024-06-06", "relevancy": 2.9632, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6352}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.575}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLACE%3A%20Global%20Local%20Accelerated%20Coordinate%20Encoding&body=Title%3A%20GLACE%3A%20Global%20Local%20Accelerated%20Coordinate%20Encoding%0AAuthor%3A%20Fangjinhua%20Wang%20and%20Xudong%20Jiang%20and%20Silvano%20Galliani%20and%20Christoph%20Vogel%20and%20Marc%20Pollefeys%0AAbstract%3A%20%20%20Scene%20coordinate%20regression%20%28SCR%29%20methods%20are%20a%20family%20of%20visual%20localization%0Amethods%20that%20directly%20regress%202D-3D%20matches%20for%20camera%20pose%20estimation.%20They%0Aare%20effective%20in%20small-scale%20scenes%20but%20face%20significant%20challenges%20in%0Alarge-scale%20scenes%20that%20are%20further%20amplified%20in%20the%20absence%20of%20ground%20truth%203D%0Apoint%20clouds%20for%20supervision.%20Here%2C%20the%20model%20can%20only%20rely%20on%20reprojection%0Aconstraints%20and%20needs%20to%20implicitly%20triangulate%20the%20points.%20The%20challenges%20stem%0Afrom%20a%20fundamental%20dilemma%3A%20The%20network%20has%20to%20be%20invariant%20to%20observations%20of%0Athe%20same%20landmark%20at%20different%20viewpoints%20and%20lighting%20conditions%2C%20etc.%2C%20but%20at%0Athe%20same%20time%20discriminate%20unrelated%20but%20similar%20observations.%20The%20latter%0Abecomes%20more%20relevant%20and%20severe%20in%20larger%20scenes.%20In%20this%20work%2C%20we%20tackle%20this%0Aproblem%20by%20introducing%20the%20concept%20of%20co-visibility%20to%20the%20network.%20We%20propose%0AGLACE%2C%20which%20integrates%20pre-trained%20global%20and%20local%20encodings%20and%20enables%20SCR%0Ato%20scale%20to%20large%20scenes%20with%20only%20a%20single%20small-sized%20network.%20Specifically%2C%0Awe%20propose%20a%20novel%20feature%20diffusion%20technique%20that%20implicitly%20groups%20the%0Areprojection%20constraints%20with%20co-visibility%20and%20avoids%20overfitting%20to%20trivial%0Asolutions.%20Additionally%2C%20our%20position%20decoder%20parameterizes%20the%20output%0Apositions%20for%20large-scale%20scenes%20more%20effectively.%20Without%20using%203D%20models%20or%0Adepth%20maps%20for%20supervision%2C%20our%20method%20achieves%20state-of-the-art%20results%20on%0Alarge-scale%20scenes%20with%20a%20low-map-size%20model.%20On%20Cambridge%20landmarks%2C%20with%20a%0Asingle%20model%2C%20we%20achieve%2017%25%20lower%20median%20position%20error%20than%20Poker%2C%20the%0Aensemble%20variant%20of%20the%20state-of-the-art%20SCR%20method%20ACE.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/cvg/glace.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04340v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLACE%253A%2520Global%2520Local%2520Accelerated%2520Coordinate%2520Encoding%26entry.906535625%3DFangjinhua%2520Wang%2520and%2520Xudong%2520Jiang%2520and%2520Silvano%2520Galliani%2520and%2520Christoph%2520Vogel%2520and%2520Marc%2520Pollefeys%26entry.1292438233%3D%2520%2520Scene%2520coordinate%2520regression%2520%2528SCR%2529%2520methods%2520are%2520a%2520family%2520of%2520visual%2520localization%250Amethods%2520that%2520directly%2520regress%25202D-3D%2520matches%2520for%2520camera%2520pose%2520estimation.%2520They%250Aare%2520effective%2520in%2520small-scale%2520scenes%2520but%2520face%2520significant%2520challenges%2520in%250Alarge-scale%2520scenes%2520that%2520are%2520further%2520amplified%2520in%2520the%2520absence%2520of%2520ground%2520truth%25203D%250Apoint%2520clouds%2520for%2520supervision.%2520Here%252C%2520the%2520model%2520can%2520only%2520rely%2520on%2520reprojection%250Aconstraints%2520and%2520needs%2520to%2520implicitly%2520triangulate%2520the%2520points.%2520The%2520challenges%2520stem%250Afrom%2520a%2520fundamental%2520dilemma%253A%2520The%2520network%2520has%2520to%2520be%2520invariant%2520to%2520observations%2520of%250Athe%2520same%2520landmark%2520at%2520different%2520viewpoints%2520and%2520lighting%2520conditions%252C%2520etc.%252C%2520but%2520at%250Athe%2520same%2520time%2520discriminate%2520unrelated%2520but%2520similar%2520observations.%2520The%2520latter%250Abecomes%2520more%2520relevant%2520and%2520severe%2520in%2520larger%2520scenes.%2520In%2520this%2520work%252C%2520we%2520tackle%2520this%250Aproblem%2520by%2520introducing%2520the%2520concept%2520of%2520co-visibility%2520to%2520the%2520network.%2520We%2520propose%250AGLACE%252C%2520which%2520integrates%2520pre-trained%2520global%2520and%2520local%2520encodings%2520and%2520enables%2520SCR%250Ato%2520scale%2520to%2520large%2520scenes%2520with%2520only%2520a%2520single%2520small-sized%2520network.%2520Specifically%252C%250Awe%2520propose%2520a%2520novel%2520feature%2520diffusion%2520technique%2520that%2520implicitly%2520groups%2520the%250Areprojection%2520constraints%2520with%2520co-visibility%2520and%2520avoids%2520overfitting%2520to%2520trivial%250Asolutions.%2520Additionally%252C%2520our%2520position%2520decoder%2520parameterizes%2520the%2520output%250Apositions%2520for%2520large-scale%2520scenes%2520more%2520effectively.%2520Without%2520using%25203D%2520models%2520or%250Adepth%2520maps%2520for%2520supervision%252C%2520our%2520method%2520achieves%2520state-of-the-art%2520results%2520on%250Alarge-scale%2520scenes%2520with%2520a%2520low-map-size%2520model.%2520On%2520Cambridge%2520landmarks%252C%2520with%2520a%250Asingle%2520model%252C%2520we%2520achieve%252017%2525%2520lower%2520median%2520position%2520error%2520than%2520Poker%252C%2520the%250Aensemble%2520variant%2520of%2520the%2520state-of-the-art%2520SCR%2520method%2520ACE.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/cvg/glace.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04340v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLACE%3A%20Global%20Local%20Accelerated%20Coordinate%20Encoding&entry.906535625=Fangjinhua%20Wang%20and%20Xudong%20Jiang%20and%20Silvano%20Galliani%20and%20Christoph%20Vogel%20and%20Marc%20Pollefeys&entry.1292438233=%20%20Scene%20coordinate%20regression%20%28SCR%29%20methods%20are%20a%20family%20of%20visual%20localization%0Amethods%20that%20directly%20regress%202D-3D%20matches%20for%20camera%20pose%20estimation.%20They%0Aare%20effective%20in%20small-scale%20scenes%20but%20face%20significant%20challenges%20in%0Alarge-scale%20scenes%20that%20are%20further%20amplified%20in%20the%20absence%20of%20ground%20truth%203D%0Apoint%20clouds%20for%20supervision.%20Here%2C%20the%20model%20can%20only%20rely%20on%20reprojection%0Aconstraints%20and%20needs%20to%20implicitly%20triangulate%20the%20points.%20The%20challenges%20stem%0Afrom%20a%20fundamental%20dilemma%3A%20The%20network%20has%20to%20be%20invariant%20to%20observations%20of%0Athe%20same%20landmark%20at%20different%20viewpoints%20and%20lighting%20conditions%2C%20etc.%2C%20but%20at%0Athe%20same%20time%20discriminate%20unrelated%20but%20similar%20observations.%20The%20latter%0Abecomes%20more%20relevant%20and%20severe%20in%20larger%20scenes.%20In%20this%20work%2C%20we%20tackle%20this%0Aproblem%20by%20introducing%20the%20concept%20of%20co-visibility%20to%20the%20network.%20We%20propose%0AGLACE%2C%20which%20integrates%20pre-trained%20global%20and%20local%20encodings%20and%20enables%20SCR%0Ato%20scale%20to%20large%20scenes%20with%20only%20a%20single%20small-sized%20network.%20Specifically%2C%0Awe%20propose%20a%20novel%20feature%20diffusion%20technique%20that%20implicitly%20groups%20the%0Areprojection%20constraints%20with%20co-visibility%20and%20avoids%20overfitting%20to%20trivial%0Asolutions.%20Additionally%2C%20our%20position%20decoder%20parameterizes%20the%20output%0Apositions%20for%20large-scale%20scenes%20more%20effectively.%20Without%20using%203D%20models%20or%0Adepth%20maps%20for%20supervision%2C%20our%20method%20achieves%20state-of-the-art%20results%20on%0Alarge-scale%20scenes%20with%20a%20low-map-size%20model.%20On%20Cambridge%20landmarks%2C%20with%20a%0Asingle%20model%2C%20we%20achieve%2017%25%20lower%20median%20position%20error%20than%20Poker%2C%20the%0Aensemble%20variant%20of%20the%20state-of-the-art%20SCR%20method%20ACE.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/cvg/glace.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04340v1&entry.124074799=Read"},
{"title": "Unleashing Generalization of End-to-End Autonomous Driving with\n  Controllable Long Video Generation", "author": "Enhui Ma and Lijun Zhou and Tao Tang and Zhan Zhang and Dong Han and Junpeng Jiang and Kun Zhan and Peng Jia and Xianpeng Lang and Haiyang Sun and Di Lin and Kaicheng Yu", "abstract": "  Using generative models to synthesize new data has become a de-facto standard\nin autonomous driving to address the data scarcity issue. Though existing\napproaches are able to boost perception models, we discover that these\napproaches fail to improve the performance of planning of end-to-end autonomous\ndriving models as the generated videos are usually less than 8 frames and the\nspatial and temporal inconsistencies are not negligible. To this end, we\npropose Delphi, a novel diffusion-based long video generation method with a\nshared noise modeling mechanism across the multi-views to increase spatial\nconsistency, and a feature-aligned module to achieves both precise\ncontrollability and temporal consistency. Our method can generate up to 40\nframes of video without loss of consistency which is about 5 times longer\ncompared with state-of-the-art methods. Instead of randomly generating new\ndata, we further design a sampling policy to let Delphi generate new data that\nare similar to those failure cases to improve the sample efficiency. This is\nachieved by building a failure-case driven framework with the help of\npre-trained visual language models. Our extensive experiment demonstrates that\nour Delphi generates a higher quality of long videos surpassing previous\nstate-of-the-art methods. Consequentially, with only generating 4% of the\ntraining dataset size, our framework is able to go beyond perception and\nprediction tasks, for the first time to the best of our knowledge, boost the\nplanning performance of the end-to-end autonomous driving model by a margin of\n25%.\n", "link": "http://arxiv.org/abs/2406.01349v3", "date": "2024-06-06", "relevancy": 2.9505, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6042}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6002}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20Generalization%20of%20End-to-End%20Autonomous%20Driving%20with%0A%20%20Controllable%20Long%20Video%20Generation&body=Title%3A%20Unleashing%20Generalization%20of%20End-to-End%20Autonomous%20Driving%20with%0A%20%20Controllable%20Long%20Video%20Generation%0AAuthor%3A%20Enhui%20Ma%20and%20Lijun%20Zhou%20and%20Tao%20Tang%20and%20Zhan%20Zhang%20and%20Dong%20Han%20and%20Junpeng%20Jiang%20and%20Kun%20Zhan%20and%20Peng%20Jia%20and%20Xianpeng%20Lang%20and%20Haiyang%20Sun%20and%20Di%20Lin%20and%20Kaicheng%20Yu%0AAbstract%3A%20%20%20Using%20generative%20models%20to%20synthesize%20new%20data%20has%20become%20a%20de-facto%20standard%0Ain%20autonomous%20driving%20to%20address%20the%20data%20scarcity%20issue.%20Though%20existing%0Aapproaches%20are%20able%20to%20boost%20perception%20models%2C%20we%20discover%20that%20these%0Aapproaches%20fail%20to%20improve%20the%20performance%20of%20planning%20of%20end-to-end%20autonomous%0Adriving%20models%20as%20the%20generated%20videos%20are%20usually%20less%20than%208%20frames%20and%20the%0Aspatial%20and%20temporal%20inconsistencies%20are%20not%20negligible.%20To%20this%20end%2C%20we%0Apropose%20Delphi%2C%20a%20novel%20diffusion-based%20long%20video%20generation%20method%20with%20a%0Ashared%20noise%20modeling%20mechanism%20across%20the%20multi-views%20to%20increase%20spatial%0Aconsistency%2C%20and%20a%20feature-aligned%20module%20to%20achieves%20both%20precise%0Acontrollability%20and%20temporal%20consistency.%20Our%20method%20can%20generate%20up%20to%2040%0Aframes%20of%20video%20without%20loss%20of%20consistency%20which%20is%20about%205%20times%20longer%0Acompared%20with%20state-of-the-art%20methods.%20Instead%20of%20randomly%20generating%20new%0Adata%2C%20we%20further%20design%20a%20sampling%20policy%20to%20let%20Delphi%20generate%20new%20data%20that%0Aare%20similar%20to%20those%20failure%20cases%20to%20improve%20the%20sample%20efficiency.%20This%20is%0Aachieved%20by%20building%20a%20failure-case%20driven%20framework%20with%20the%20help%20of%0Apre-trained%20visual%20language%20models.%20Our%20extensive%20experiment%20demonstrates%20that%0Aour%20Delphi%20generates%20a%20higher%20quality%20of%20long%20videos%20surpassing%20previous%0Astate-of-the-art%20methods.%20Consequentially%2C%20with%20only%20generating%204%25%20of%20the%0Atraining%20dataset%20size%2C%20our%20framework%20is%20able%20to%20go%20beyond%20perception%20and%0Aprediction%20tasks%2C%20for%20the%20first%20time%20to%20the%20best%20of%20our%20knowledge%2C%20boost%20the%0Aplanning%20performance%20of%20the%20end-to-end%20autonomous%20driving%20model%20by%20a%20margin%20of%0A25%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.01349v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520Generalization%2520of%2520End-to-End%2520Autonomous%2520Driving%2520with%250A%2520%2520Controllable%2520Long%2520Video%2520Generation%26entry.906535625%3DEnhui%2520Ma%2520and%2520Lijun%2520Zhou%2520and%2520Tao%2520Tang%2520and%2520Zhan%2520Zhang%2520and%2520Dong%2520Han%2520and%2520Junpeng%2520Jiang%2520and%2520Kun%2520Zhan%2520and%2520Peng%2520Jia%2520and%2520Xianpeng%2520Lang%2520and%2520Haiyang%2520Sun%2520and%2520Di%2520Lin%2520and%2520Kaicheng%2520Yu%26entry.1292438233%3D%2520%2520Using%2520generative%2520models%2520to%2520synthesize%2520new%2520data%2520has%2520become%2520a%2520de-facto%2520standard%250Ain%2520autonomous%2520driving%2520to%2520address%2520the%2520data%2520scarcity%2520issue.%2520Though%2520existing%250Aapproaches%2520are%2520able%2520to%2520boost%2520perception%2520models%252C%2520we%2520discover%2520that%2520these%250Aapproaches%2520fail%2520to%2520improve%2520the%2520performance%2520of%2520planning%2520of%2520end-to-end%2520autonomous%250Adriving%2520models%2520as%2520the%2520generated%2520videos%2520are%2520usually%2520less%2520than%25208%2520frames%2520and%2520the%250Aspatial%2520and%2520temporal%2520inconsistencies%2520are%2520not%2520negligible.%2520To%2520this%2520end%252C%2520we%250Apropose%2520Delphi%252C%2520a%2520novel%2520diffusion-based%2520long%2520video%2520generation%2520method%2520with%2520a%250Ashared%2520noise%2520modeling%2520mechanism%2520across%2520the%2520multi-views%2520to%2520increase%2520spatial%250Aconsistency%252C%2520and%2520a%2520feature-aligned%2520module%2520to%2520achieves%2520both%2520precise%250Acontrollability%2520and%2520temporal%2520consistency.%2520Our%2520method%2520can%2520generate%2520up%2520to%252040%250Aframes%2520of%2520video%2520without%2520loss%2520of%2520consistency%2520which%2520is%2520about%25205%2520times%2520longer%250Acompared%2520with%2520state-of-the-art%2520methods.%2520Instead%2520of%2520randomly%2520generating%2520new%250Adata%252C%2520we%2520further%2520design%2520a%2520sampling%2520policy%2520to%2520let%2520Delphi%2520generate%2520new%2520data%2520that%250Aare%2520similar%2520to%2520those%2520failure%2520cases%2520to%2520improve%2520the%2520sample%2520efficiency.%2520This%2520is%250Aachieved%2520by%2520building%2520a%2520failure-case%2520driven%2520framework%2520with%2520the%2520help%2520of%250Apre-trained%2520visual%2520language%2520models.%2520Our%2520extensive%2520experiment%2520demonstrates%2520that%250Aour%2520Delphi%2520generates%2520a%2520higher%2520quality%2520of%2520long%2520videos%2520surpassing%2520previous%250Astate-of-the-art%2520methods.%2520Consequentially%252C%2520with%2520only%2520generating%25204%2525%2520of%2520the%250Atraining%2520dataset%2520size%252C%2520our%2520framework%2520is%2520able%2520to%2520go%2520beyond%2520perception%2520and%250Aprediction%2520tasks%252C%2520for%2520the%2520first%2520time%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%2520boost%2520the%250Aplanning%2520performance%2520of%2520the%2520end-to-end%2520autonomous%2520driving%2520model%2520by%2520a%2520margin%2520of%250A25%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.01349v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20Generalization%20of%20End-to-End%20Autonomous%20Driving%20with%0A%20%20Controllable%20Long%20Video%20Generation&entry.906535625=Enhui%20Ma%20and%20Lijun%20Zhou%20and%20Tao%20Tang%20and%20Zhan%20Zhang%20and%20Dong%20Han%20and%20Junpeng%20Jiang%20and%20Kun%20Zhan%20and%20Peng%20Jia%20and%20Xianpeng%20Lang%20and%20Haiyang%20Sun%20and%20Di%20Lin%20and%20Kaicheng%20Yu&entry.1292438233=%20%20Using%20generative%20models%20to%20synthesize%20new%20data%20has%20become%20a%20de-facto%20standard%0Ain%20autonomous%20driving%20to%20address%20the%20data%20scarcity%20issue.%20Though%20existing%0Aapproaches%20are%20able%20to%20boost%20perception%20models%2C%20we%20discover%20that%20these%0Aapproaches%20fail%20to%20improve%20the%20performance%20of%20planning%20of%20end-to-end%20autonomous%0Adriving%20models%20as%20the%20generated%20videos%20are%20usually%20less%20than%208%20frames%20and%20the%0Aspatial%20and%20temporal%20inconsistencies%20are%20not%20negligible.%20To%20this%20end%2C%20we%0Apropose%20Delphi%2C%20a%20novel%20diffusion-based%20long%20video%20generation%20method%20with%20a%0Ashared%20noise%20modeling%20mechanism%20across%20the%20multi-views%20to%20increase%20spatial%0Aconsistency%2C%20and%20a%20feature-aligned%20module%20to%20achieves%20both%20precise%0Acontrollability%20and%20temporal%20consistency.%20Our%20method%20can%20generate%20up%20to%2040%0Aframes%20of%20video%20without%20loss%20of%20consistency%20which%20is%20about%205%20times%20longer%0Acompared%20with%20state-of-the-art%20methods.%20Instead%20of%20randomly%20generating%20new%0Adata%2C%20we%20further%20design%20a%20sampling%20policy%20to%20let%20Delphi%20generate%20new%20data%20that%0Aare%20similar%20to%20those%20failure%20cases%20to%20improve%20the%20sample%20efficiency.%20This%20is%0Aachieved%20by%20building%20a%20failure-case%20driven%20framework%20with%20the%20help%20of%0Apre-trained%20visual%20language%20models.%20Our%20extensive%20experiment%20demonstrates%20that%0Aour%20Delphi%20generates%20a%20higher%20quality%20of%20long%20videos%20surpassing%20previous%0Astate-of-the-art%20methods.%20Consequentially%2C%20with%20only%20generating%204%25%20of%20the%0Atraining%20dataset%20size%2C%20our%20framework%20is%20able%20to%20go%20beyond%20perception%20and%0Aprediction%20tasks%2C%20for%20the%20first%20time%20to%20the%20best%20of%20our%20knowledge%2C%20boost%20the%0Aplanning%20performance%20of%20the%20end-to-end%20autonomous%20driving%20model%20by%20a%20margin%20of%0A25%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.01349v3&entry.124074799=Read"},
{"title": "GeoGen: Geometry-Aware Generative Modeling via Signed Distance Functions", "author": "Salvatore Esposito and Qingshan Xu and Kacper Kania and Charlie Hewitt and Octave Mariotti and Lohit Petikam and Julien Valentin and Arno Onken and Oisin Mac Aodha", "abstract": "  We introduce a new generative approach for synthesizing 3D geometry and\nimages from single-view collections. Most existing approaches predict\nvolumetric density to render multi-view consistent images. By employing\nvolumetric rendering using neural radiance fields, they inherit a key\nlimitation: the generated geometry is noisy and unconstrained, limiting the\nquality and utility of the output meshes. To address this issue, we propose\nGeoGen, a new SDF-based 3D generative model trained in an end-to-end manner.\nInitially, we reinterpret the volumetric density as a Signed Distance Function\n(SDF). This allows us to introduce useful priors to generate valid meshes.\nHowever, those priors prevent the generative model from learning details,\nlimiting the applicability of the method to real-world scenarios. To alleviate\nthat problem, we make the transformation learnable and constrain the rendered\ndepth map to be consistent with the zero-level set of the SDF. Through the lens\nof adversarial training, we encourage the network to produce higher fidelity\ndetails on the output meshes. For evaluation, we introduce a synthetic dataset\nof human avatars captured from 360-degree camera angles, to overcome the\nchallenges presented by real-world datasets, which often lack 3D consistency\nand do not cover all camera angles. Our experiments on multiple datasets show\nthat GeoGen produces visually and quantitatively better geometry than the\nprevious generative models based on neural radiance fields.\n", "link": "http://arxiv.org/abs/2406.04254v1", "date": "2024-06-06", "relevancy": 2.9382, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6019}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5882}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoGen%3A%20Geometry-Aware%20Generative%20Modeling%20via%20Signed%20Distance%20Functions&body=Title%3A%20GeoGen%3A%20Geometry-Aware%20Generative%20Modeling%20via%20Signed%20Distance%20Functions%0AAuthor%3A%20Salvatore%20Esposito%20and%20Qingshan%20Xu%20and%20Kacper%20Kania%20and%20Charlie%20Hewitt%20and%20Octave%20Mariotti%20and%20Lohit%20Petikam%20and%20Julien%20Valentin%20and%20Arno%20Onken%20and%20Oisin%20Mac%20Aodha%0AAbstract%3A%20%20%20We%20introduce%20a%20new%20generative%20approach%20for%20synthesizing%203D%20geometry%20and%0Aimages%20from%20single-view%20collections.%20Most%20existing%20approaches%20predict%0Avolumetric%20density%20to%20render%20multi-view%20consistent%20images.%20By%20employing%0Avolumetric%20rendering%20using%20neural%20radiance%20fields%2C%20they%20inherit%20a%20key%0Alimitation%3A%20the%20generated%20geometry%20is%20noisy%20and%20unconstrained%2C%20limiting%20the%0Aquality%20and%20utility%20of%20the%20output%20meshes.%20To%20address%20this%20issue%2C%20we%20propose%0AGeoGen%2C%20a%20new%20SDF-based%203D%20generative%20model%20trained%20in%20an%20end-to-end%20manner.%0AInitially%2C%20we%20reinterpret%20the%20volumetric%20density%20as%20a%20Signed%20Distance%20Function%0A%28SDF%29.%20This%20allows%20us%20to%20introduce%20useful%20priors%20to%20generate%20valid%20meshes.%0AHowever%2C%20those%20priors%20prevent%20the%20generative%20model%20from%20learning%20details%2C%0Alimiting%20the%20applicability%20of%20the%20method%20to%20real-world%20scenarios.%20To%20alleviate%0Athat%20problem%2C%20we%20make%20the%20transformation%20learnable%20and%20constrain%20the%20rendered%0Adepth%20map%20to%20be%20consistent%20with%20the%20zero-level%20set%20of%20the%20SDF.%20Through%20the%20lens%0Aof%20adversarial%20training%2C%20we%20encourage%20the%20network%20to%20produce%20higher%20fidelity%0Adetails%20on%20the%20output%20meshes.%20For%20evaluation%2C%20we%20introduce%20a%20synthetic%20dataset%0Aof%20human%20avatars%20captured%20from%20360-degree%20camera%20angles%2C%20to%20overcome%20the%0Achallenges%20presented%20by%20real-world%20datasets%2C%20which%20often%20lack%203D%20consistency%0Aand%20do%20not%20cover%20all%20camera%20angles.%20Our%20experiments%20on%20multiple%20datasets%20show%0Athat%20GeoGen%20produces%20visually%20and%20quantitatively%20better%20geometry%20than%20the%0Aprevious%20generative%20models%20based%20on%20neural%20radiance%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04254v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoGen%253A%2520Geometry-Aware%2520Generative%2520Modeling%2520via%2520Signed%2520Distance%2520Functions%26entry.906535625%3DSalvatore%2520Esposito%2520and%2520Qingshan%2520Xu%2520and%2520Kacper%2520Kania%2520and%2520Charlie%2520Hewitt%2520and%2520Octave%2520Mariotti%2520and%2520Lohit%2520Petikam%2520and%2520Julien%2520Valentin%2520and%2520Arno%2520Onken%2520and%2520Oisin%2520Mac%2520Aodha%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520new%2520generative%2520approach%2520for%2520synthesizing%25203D%2520geometry%2520and%250Aimages%2520from%2520single-view%2520collections.%2520Most%2520existing%2520approaches%2520predict%250Avolumetric%2520density%2520to%2520render%2520multi-view%2520consistent%2520images.%2520By%2520employing%250Avolumetric%2520rendering%2520using%2520neural%2520radiance%2520fields%252C%2520they%2520inherit%2520a%2520key%250Alimitation%253A%2520the%2520generated%2520geometry%2520is%2520noisy%2520and%2520unconstrained%252C%2520limiting%2520the%250Aquality%2520and%2520utility%2520of%2520the%2520output%2520meshes.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%250AGeoGen%252C%2520a%2520new%2520SDF-based%25203D%2520generative%2520model%2520trained%2520in%2520an%2520end-to-end%2520manner.%250AInitially%252C%2520we%2520reinterpret%2520the%2520volumetric%2520density%2520as%2520a%2520Signed%2520Distance%2520Function%250A%2528SDF%2529.%2520This%2520allows%2520us%2520to%2520introduce%2520useful%2520priors%2520to%2520generate%2520valid%2520meshes.%250AHowever%252C%2520those%2520priors%2520prevent%2520the%2520generative%2520model%2520from%2520learning%2520details%252C%250Alimiting%2520the%2520applicability%2520of%2520the%2520method%2520to%2520real-world%2520scenarios.%2520To%2520alleviate%250Athat%2520problem%252C%2520we%2520make%2520the%2520transformation%2520learnable%2520and%2520constrain%2520the%2520rendered%250Adepth%2520map%2520to%2520be%2520consistent%2520with%2520the%2520zero-level%2520set%2520of%2520the%2520SDF.%2520Through%2520the%2520lens%250Aof%2520adversarial%2520training%252C%2520we%2520encourage%2520the%2520network%2520to%2520produce%2520higher%2520fidelity%250Adetails%2520on%2520the%2520output%2520meshes.%2520For%2520evaluation%252C%2520we%2520introduce%2520a%2520synthetic%2520dataset%250Aof%2520human%2520avatars%2520captured%2520from%2520360-degree%2520camera%2520angles%252C%2520to%2520overcome%2520the%250Achallenges%2520presented%2520by%2520real-world%2520datasets%252C%2520which%2520often%2520lack%25203D%2520consistency%250Aand%2520do%2520not%2520cover%2520all%2520camera%2520angles.%2520Our%2520experiments%2520on%2520multiple%2520datasets%2520show%250Athat%2520GeoGen%2520produces%2520visually%2520and%2520quantitatively%2520better%2520geometry%2520than%2520the%250Aprevious%2520generative%2520models%2520based%2520on%2520neural%2520radiance%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04254v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoGen%3A%20Geometry-Aware%20Generative%20Modeling%20via%20Signed%20Distance%20Functions&entry.906535625=Salvatore%20Esposito%20and%20Qingshan%20Xu%20and%20Kacper%20Kania%20and%20Charlie%20Hewitt%20and%20Octave%20Mariotti%20and%20Lohit%20Petikam%20and%20Julien%20Valentin%20and%20Arno%20Onken%20and%20Oisin%20Mac%20Aodha&entry.1292438233=%20%20We%20introduce%20a%20new%20generative%20approach%20for%20synthesizing%203D%20geometry%20and%0Aimages%20from%20single-view%20collections.%20Most%20existing%20approaches%20predict%0Avolumetric%20density%20to%20render%20multi-view%20consistent%20images.%20By%20employing%0Avolumetric%20rendering%20using%20neural%20radiance%20fields%2C%20they%20inherit%20a%20key%0Alimitation%3A%20the%20generated%20geometry%20is%20noisy%20and%20unconstrained%2C%20limiting%20the%0Aquality%20and%20utility%20of%20the%20output%20meshes.%20To%20address%20this%20issue%2C%20we%20propose%0AGeoGen%2C%20a%20new%20SDF-based%203D%20generative%20model%20trained%20in%20an%20end-to-end%20manner.%0AInitially%2C%20we%20reinterpret%20the%20volumetric%20density%20as%20a%20Signed%20Distance%20Function%0A%28SDF%29.%20This%20allows%20us%20to%20introduce%20useful%20priors%20to%20generate%20valid%20meshes.%0AHowever%2C%20those%20priors%20prevent%20the%20generative%20model%20from%20learning%20details%2C%0Alimiting%20the%20applicability%20of%20the%20method%20to%20real-world%20scenarios.%20To%20alleviate%0Athat%20problem%2C%20we%20make%20the%20transformation%20learnable%20and%20constrain%20the%20rendered%0Adepth%20map%20to%20be%20consistent%20with%20the%20zero-level%20set%20of%20the%20SDF.%20Through%20the%20lens%0Aof%20adversarial%20training%2C%20we%20encourage%20the%20network%20to%20produce%20higher%20fidelity%0Adetails%20on%20the%20output%20meshes.%20For%20evaluation%2C%20we%20introduce%20a%20synthetic%20dataset%0Aof%20human%20avatars%20captured%20from%20360-degree%20camera%20angles%2C%20to%20overcome%20the%0Achallenges%20presented%20by%20real-world%20datasets%2C%20which%20often%20lack%203D%20consistency%0Aand%20do%20not%20cover%20all%20camera%20angles.%20Our%20experiments%20on%20multiple%20datasets%20show%0Athat%20GeoGen%20produces%20visually%20and%20quantitatively%20better%20geometry%20than%20the%0Aprevious%20generative%20models%20based%20on%20neural%20radiance%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04254v1&entry.124074799=Read"},
{"title": "Sparse Multi-baseline SAR Cross-modal 3D Reconstruction of Vehicle\n  Targets", "author": "Da Li and Guoqiang Zhao and Houjun Sun and Jiacheng Bao", "abstract": "  Multi-baseline SAR 3D imaging faces significant challenges due to data\nsparsity. In recent years, deep learning techniques have achieved notable\nsuccess in enhancing the quality of sparse SAR 3D imaging. However, previous\nwork typically rely on full-aperture high-resolution radar images to supervise\nthe training of deep neural networks (DNNs), utilizing only single-modal\ninformation from radar data. Consequently, imaging performance is limited, and\nacquiring full-aperture data for multi-baseline SAR is costly and sometimes\nimpractical in real-world applications. In this paper, we propose a Cross-Modal\nReconstruction Network (CMR-Net), which integrates differentiable render and\ncross-modal supervision with optical images to reconstruct highly sparse\nmulti-baseline SAR 3D images of vehicle targets into visually structured and\nhigh-resolution images. We meticulously designed the network architecture and\ntraining strategies to enhance network generalization capability. Remarkably,\nCMR-Net, trained solely on simulated data, demonstrates high-resolution\nreconstruction capabilities on both publicly available simulation datasets and\nreal measured datasets, outperforming traditional sparse reconstruction\nalgorithms based on compressed sensing and other learning-based methods.\nAdditionally, using optical images as supervision provides a cost-effective way\nto build training datasets, reducing the difficulty of method dissemination.\nOur work showcases the broad prospects of deep learning in multi-baseline SAR\n3D imaging and offers a novel path for researching radar imaging based on\ncross-modal learning theory.\n", "link": "http://arxiv.org/abs/2406.04158v1", "date": "2024-06-06", "relevancy": 2.8862, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5826}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5777}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5713}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Multi-baseline%20SAR%20Cross-modal%203D%20Reconstruction%20of%20Vehicle%0A%20%20Targets&body=Title%3A%20Sparse%20Multi-baseline%20SAR%20Cross-modal%203D%20Reconstruction%20of%20Vehicle%0A%20%20Targets%0AAuthor%3A%20Da%20Li%20and%20Guoqiang%20Zhao%20and%20Houjun%20Sun%20and%20Jiacheng%20Bao%0AAbstract%3A%20%20%20Multi-baseline%20SAR%203D%20imaging%20faces%20significant%20challenges%20due%20to%20data%0Asparsity.%20In%20recent%20years%2C%20deep%20learning%20techniques%20have%20achieved%20notable%0Asuccess%20in%20enhancing%20the%20quality%20of%20sparse%20SAR%203D%20imaging.%20However%2C%20previous%0Awork%20typically%20rely%20on%20full-aperture%20high-resolution%20radar%20images%20to%20supervise%0Athe%20training%20of%20deep%20neural%20networks%20%28DNNs%29%2C%20utilizing%20only%20single-modal%0Ainformation%20from%20radar%20data.%20Consequently%2C%20imaging%20performance%20is%20limited%2C%20and%0Aacquiring%20full-aperture%20data%20for%20multi-baseline%20SAR%20is%20costly%20and%20sometimes%0Aimpractical%20in%20real-world%20applications.%20In%20this%20paper%2C%20we%20propose%20a%20Cross-Modal%0AReconstruction%20Network%20%28CMR-Net%29%2C%20which%20integrates%20differentiable%20render%20and%0Across-modal%20supervision%20with%20optical%20images%20to%20reconstruct%20highly%20sparse%0Amulti-baseline%20SAR%203D%20images%20of%20vehicle%20targets%20into%20visually%20structured%20and%0Ahigh-resolution%20images.%20We%20meticulously%20designed%20the%20network%20architecture%20and%0Atraining%20strategies%20to%20enhance%20network%20generalization%20capability.%20Remarkably%2C%0ACMR-Net%2C%20trained%20solely%20on%20simulated%20data%2C%20demonstrates%20high-resolution%0Areconstruction%20capabilities%20on%20both%20publicly%20available%20simulation%20datasets%20and%0Areal%20measured%20datasets%2C%20outperforming%20traditional%20sparse%20reconstruction%0Aalgorithms%20based%20on%20compressed%20sensing%20and%20other%20learning-based%20methods.%0AAdditionally%2C%20using%20optical%20images%20as%20supervision%20provides%20a%20cost-effective%20way%0Ato%20build%20training%20datasets%2C%20reducing%20the%20difficulty%20of%20method%20dissemination.%0AOur%20work%20showcases%20the%20broad%20prospects%20of%20deep%20learning%20in%20multi-baseline%20SAR%0A3D%20imaging%20and%20offers%20a%20novel%20path%20for%20researching%20radar%20imaging%20based%20on%0Across-modal%20learning%20theory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04158v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Multi-baseline%2520SAR%2520Cross-modal%25203D%2520Reconstruction%2520of%2520Vehicle%250A%2520%2520Targets%26entry.906535625%3DDa%2520Li%2520and%2520Guoqiang%2520Zhao%2520and%2520Houjun%2520Sun%2520and%2520Jiacheng%2520Bao%26entry.1292438233%3D%2520%2520Multi-baseline%2520SAR%25203D%2520imaging%2520faces%2520significant%2520challenges%2520due%2520to%2520data%250Asparsity.%2520In%2520recent%2520years%252C%2520deep%2520learning%2520techniques%2520have%2520achieved%2520notable%250Asuccess%2520in%2520enhancing%2520the%2520quality%2520of%2520sparse%2520SAR%25203D%2520imaging.%2520However%252C%2520previous%250Awork%2520typically%2520rely%2520on%2520full-aperture%2520high-resolution%2520radar%2520images%2520to%2520supervise%250Athe%2520training%2520of%2520deep%2520neural%2520networks%2520%2528DNNs%2529%252C%2520utilizing%2520only%2520single-modal%250Ainformation%2520from%2520radar%2520data.%2520Consequently%252C%2520imaging%2520performance%2520is%2520limited%252C%2520and%250Aacquiring%2520full-aperture%2520data%2520for%2520multi-baseline%2520SAR%2520is%2520costly%2520and%2520sometimes%250Aimpractical%2520in%2520real-world%2520applications.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Cross-Modal%250AReconstruction%2520Network%2520%2528CMR-Net%2529%252C%2520which%2520integrates%2520differentiable%2520render%2520and%250Across-modal%2520supervision%2520with%2520optical%2520images%2520to%2520reconstruct%2520highly%2520sparse%250Amulti-baseline%2520SAR%25203D%2520images%2520of%2520vehicle%2520targets%2520into%2520visually%2520structured%2520and%250Ahigh-resolution%2520images.%2520We%2520meticulously%2520designed%2520the%2520network%2520architecture%2520and%250Atraining%2520strategies%2520to%2520enhance%2520network%2520generalization%2520capability.%2520Remarkably%252C%250ACMR-Net%252C%2520trained%2520solely%2520on%2520simulated%2520data%252C%2520demonstrates%2520high-resolution%250Areconstruction%2520capabilities%2520on%2520both%2520publicly%2520available%2520simulation%2520datasets%2520and%250Areal%2520measured%2520datasets%252C%2520outperforming%2520traditional%2520sparse%2520reconstruction%250Aalgorithms%2520based%2520on%2520compressed%2520sensing%2520and%2520other%2520learning-based%2520methods.%250AAdditionally%252C%2520using%2520optical%2520images%2520as%2520supervision%2520provides%2520a%2520cost-effective%2520way%250Ato%2520build%2520training%2520datasets%252C%2520reducing%2520the%2520difficulty%2520of%2520method%2520dissemination.%250AOur%2520work%2520showcases%2520the%2520broad%2520prospects%2520of%2520deep%2520learning%2520in%2520multi-baseline%2520SAR%250A3D%2520imaging%2520and%2520offers%2520a%2520novel%2520path%2520for%2520researching%2520radar%2520imaging%2520based%2520on%250Across-modal%2520learning%2520theory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04158v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Multi-baseline%20SAR%20Cross-modal%203D%20Reconstruction%20of%20Vehicle%0A%20%20Targets&entry.906535625=Da%20Li%20and%20Guoqiang%20Zhao%20and%20Houjun%20Sun%20and%20Jiacheng%20Bao&entry.1292438233=%20%20Multi-baseline%20SAR%203D%20imaging%20faces%20significant%20challenges%20due%20to%20data%0Asparsity.%20In%20recent%20years%2C%20deep%20learning%20techniques%20have%20achieved%20notable%0Asuccess%20in%20enhancing%20the%20quality%20of%20sparse%20SAR%203D%20imaging.%20However%2C%20previous%0Awork%20typically%20rely%20on%20full-aperture%20high-resolution%20radar%20images%20to%20supervise%0Athe%20training%20of%20deep%20neural%20networks%20%28DNNs%29%2C%20utilizing%20only%20single-modal%0Ainformation%20from%20radar%20data.%20Consequently%2C%20imaging%20performance%20is%20limited%2C%20and%0Aacquiring%20full-aperture%20data%20for%20multi-baseline%20SAR%20is%20costly%20and%20sometimes%0Aimpractical%20in%20real-world%20applications.%20In%20this%20paper%2C%20we%20propose%20a%20Cross-Modal%0AReconstruction%20Network%20%28CMR-Net%29%2C%20which%20integrates%20differentiable%20render%20and%0Across-modal%20supervision%20with%20optical%20images%20to%20reconstruct%20highly%20sparse%0Amulti-baseline%20SAR%203D%20images%20of%20vehicle%20targets%20into%20visually%20structured%20and%0Ahigh-resolution%20images.%20We%20meticulously%20designed%20the%20network%20architecture%20and%0Atraining%20strategies%20to%20enhance%20network%20generalization%20capability.%20Remarkably%2C%0ACMR-Net%2C%20trained%20solely%20on%20simulated%20data%2C%20demonstrates%20high-resolution%0Areconstruction%20capabilities%20on%20both%20publicly%20available%20simulation%20datasets%20and%0Areal%20measured%20datasets%2C%20outperforming%20traditional%20sparse%20reconstruction%0Aalgorithms%20based%20on%20compressed%20sensing%20and%20other%20learning-based%20methods.%0AAdditionally%2C%20using%20optical%20images%20as%20supervision%20provides%20a%20cost-effective%20way%0Ato%20build%20training%20datasets%2C%20reducing%20the%20difficulty%20of%20method%20dissemination.%0AOur%20work%20showcases%20the%20broad%20prospects%20of%20deep%20learning%20in%20multi-baseline%20SAR%0A3D%20imaging%20and%20offers%20a%20novel%20path%20for%20researching%20radar%20imaging%20based%20on%0Across-modal%20learning%20theory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04158v1&entry.124074799=Read"},
{"title": "Whole Heart 3D+T Representation Learning Through Sparse 2D Cardiac MR\n  Images", "author": "Yundi Zhang and Chen Chen and Suprosanna Shit and Sophie Starck and Daniel Rueckert and Jiazhen Pan", "abstract": "  Cardiac Magnetic Resonance (CMR) imaging serves as the gold-standard for\nevaluating cardiac morphology and function. Typically, a multi-view CMR stack,\ncovering short-axis (SA) and 2/3/4-chamber long-axis (LA) views, is acquired\nfor a thorough cardiac assessment. However, efficiently streamlining the\ncomplex, high-dimensional 3D+T CMR data and distilling compact, coherent\nrepresentation remains a challenge. In this work, we introduce a whole-heart\nself-supervised learning framework that utilizes masked imaging modeling to\nautomatically uncover the correlations between spatial and temporal patches\nthroughout the cardiac stacks. This process facilitates the generation of\nmeaningful and well-clustered heart representations without relying on the\ntraditionally required, and often costly, labeled data. The learned heart\nrepresentation can be directly used for various downstream tasks. Furthermore,\nour method demonstrates remarkable robustness, ensuring consistent\nrepresentations even when certain CMR planes are missing/flawed. We train our\nmodel on 14,000 unlabeled CMR data from UK BioBank and evaluate it on 1,000\nannotated data. The proposed method demonstrates superior performance to\nbaselines in tasks that demand comprehensive 3D+T cardiac information, e.g.\ncardiac phenotype (ejection fraction and ventricle volume) prediction and\nmulti-plane/multi-frame CMR segmentation, highlighting its effectiveness in\nextracting comprehensive cardiac features that are both anatomically and\npathologically relevant.\n", "link": "http://arxiv.org/abs/2406.00329v2", "date": "2024-06-06", "relevancy": 2.8848, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6104}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5631}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Whole%20Heart%203D%2BT%20Representation%20Learning%20Through%20Sparse%202D%20Cardiac%20MR%0A%20%20Images&body=Title%3A%20Whole%20Heart%203D%2BT%20Representation%20Learning%20Through%20Sparse%202D%20Cardiac%20MR%0A%20%20Images%0AAuthor%3A%20Yundi%20Zhang%20and%20Chen%20Chen%20and%20Suprosanna%20Shit%20and%20Sophie%20Starck%20and%20Daniel%20Rueckert%20and%20Jiazhen%20Pan%0AAbstract%3A%20%20%20Cardiac%20Magnetic%20Resonance%20%28CMR%29%20imaging%20serves%20as%20the%20gold-standard%20for%0Aevaluating%20cardiac%20morphology%20and%20function.%20Typically%2C%20a%20multi-view%20CMR%20stack%2C%0Acovering%20short-axis%20%28SA%29%20and%202/3/4-chamber%20long-axis%20%28LA%29%20views%2C%20is%20acquired%0Afor%20a%20thorough%20cardiac%20assessment.%20However%2C%20efficiently%20streamlining%20the%0Acomplex%2C%20high-dimensional%203D%2BT%20CMR%20data%20and%20distilling%20compact%2C%20coherent%0Arepresentation%20remains%20a%20challenge.%20In%20this%20work%2C%20we%20introduce%20a%20whole-heart%0Aself-supervised%20learning%20framework%20that%20utilizes%20masked%20imaging%20modeling%20to%0Aautomatically%20uncover%20the%20correlations%20between%20spatial%20and%20temporal%20patches%0Athroughout%20the%20cardiac%20stacks.%20This%20process%20facilitates%20the%20generation%20of%0Ameaningful%20and%20well-clustered%20heart%20representations%20without%20relying%20on%20the%0Atraditionally%20required%2C%20and%20often%20costly%2C%20labeled%20data.%20The%20learned%20heart%0Arepresentation%20can%20be%20directly%20used%20for%20various%20downstream%20tasks.%20Furthermore%2C%0Aour%20method%20demonstrates%20remarkable%20robustness%2C%20ensuring%20consistent%0Arepresentations%20even%20when%20certain%20CMR%20planes%20are%20missing/flawed.%20We%20train%20our%0Amodel%20on%2014%2C000%20unlabeled%20CMR%20data%20from%20UK%20BioBank%20and%20evaluate%20it%20on%201%2C000%0Aannotated%20data.%20The%20proposed%20method%20demonstrates%20superior%20performance%20to%0Abaselines%20in%20tasks%20that%20demand%20comprehensive%203D%2BT%20cardiac%20information%2C%20e.g.%0Acardiac%20phenotype%20%28ejection%20fraction%20and%20ventricle%20volume%29%20prediction%20and%0Amulti-plane/multi-frame%20CMR%20segmentation%2C%20highlighting%20its%20effectiveness%20in%0Aextracting%20comprehensive%20cardiac%20features%20that%20are%20both%20anatomically%20and%0Apathologically%20relevant.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00329v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhole%2520Heart%25203D%252BT%2520Representation%2520Learning%2520Through%2520Sparse%25202D%2520Cardiac%2520MR%250A%2520%2520Images%26entry.906535625%3DYundi%2520Zhang%2520and%2520Chen%2520Chen%2520and%2520Suprosanna%2520Shit%2520and%2520Sophie%2520Starck%2520and%2520Daniel%2520Rueckert%2520and%2520Jiazhen%2520Pan%26entry.1292438233%3D%2520%2520Cardiac%2520Magnetic%2520Resonance%2520%2528CMR%2529%2520imaging%2520serves%2520as%2520the%2520gold-standard%2520for%250Aevaluating%2520cardiac%2520morphology%2520and%2520function.%2520Typically%252C%2520a%2520multi-view%2520CMR%2520stack%252C%250Acovering%2520short-axis%2520%2528SA%2529%2520and%25202/3/4-chamber%2520long-axis%2520%2528LA%2529%2520views%252C%2520is%2520acquired%250Afor%2520a%2520thorough%2520cardiac%2520assessment.%2520However%252C%2520efficiently%2520streamlining%2520the%250Acomplex%252C%2520high-dimensional%25203D%252BT%2520CMR%2520data%2520and%2520distilling%2520compact%252C%2520coherent%250Arepresentation%2520remains%2520a%2520challenge.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520whole-heart%250Aself-supervised%2520learning%2520framework%2520that%2520utilizes%2520masked%2520imaging%2520modeling%2520to%250Aautomatically%2520uncover%2520the%2520correlations%2520between%2520spatial%2520and%2520temporal%2520patches%250Athroughout%2520the%2520cardiac%2520stacks.%2520This%2520process%2520facilitates%2520the%2520generation%2520of%250Ameaningful%2520and%2520well-clustered%2520heart%2520representations%2520without%2520relying%2520on%2520the%250Atraditionally%2520required%252C%2520and%2520often%2520costly%252C%2520labeled%2520data.%2520The%2520learned%2520heart%250Arepresentation%2520can%2520be%2520directly%2520used%2520for%2520various%2520downstream%2520tasks.%2520Furthermore%252C%250Aour%2520method%2520demonstrates%2520remarkable%2520robustness%252C%2520ensuring%2520consistent%250Arepresentations%2520even%2520when%2520certain%2520CMR%2520planes%2520are%2520missing/flawed.%2520We%2520train%2520our%250Amodel%2520on%252014%252C000%2520unlabeled%2520CMR%2520data%2520from%2520UK%2520BioBank%2520and%2520evaluate%2520it%2520on%25201%252C000%250Aannotated%2520data.%2520The%2520proposed%2520method%2520demonstrates%2520superior%2520performance%2520to%250Abaselines%2520in%2520tasks%2520that%2520demand%2520comprehensive%25203D%252BT%2520cardiac%2520information%252C%2520e.g.%250Acardiac%2520phenotype%2520%2528ejection%2520fraction%2520and%2520ventricle%2520volume%2529%2520prediction%2520and%250Amulti-plane/multi-frame%2520CMR%2520segmentation%252C%2520highlighting%2520its%2520effectiveness%2520in%250Aextracting%2520comprehensive%2520cardiac%2520features%2520that%2520are%2520both%2520anatomically%2520and%250Apathologically%2520relevant.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00329v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Whole%20Heart%203D%2BT%20Representation%20Learning%20Through%20Sparse%202D%20Cardiac%20MR%0A%20%20Images&entry.906535625=Yundi%20Zhang%20and%20Chen%20Chen%20and%20Suprosanna%20Shit%20and%20Sophie%20Starck%20and%20Daniel%20Rueckert%20and%20Jiazhen%20Pan&entry.1292438233=%20%20Cardiac%20Magnetic%20Resonance%20%28CMR%29%20imaging%20serves%20as%20the%20gold-standard%20for%0Aevaluating%20cardiac%20morphology%20and%20function.%20Typically%2C%20a%20multi-view%20CMR%20stack%2C%0Acovering%20short-axis%20%28SA%29%20and%202/3/4-chamber%20long-axis%20%28LA%29%20views%2C%20is%20acquired%0Afor%20a%20thorough%20cardiac%20assessment.%20However%2C%20efficiently%20streamlining%20the%0Acomplex%2C%20high-dimensional%203D%2BT%20CMR%20data%20and%20distilling%20compact%2C%20coherent%0Arepresentation%20remains%20a%20challenge.%20In%20this%20work%2C%20we%20introduce%20a%20whole-heart%0Aself-supervised%20learning%20framework%20that%20utilizes%20masked%20imaging%20modeling%20to%0Aautomatically%20uncover%20the%20correlations%20between%20spatial%20and%20temporal%20patches%0Athroughout%20the%20cardiac%20stacks.%20This%20process%20facilitates%20the%20generation%20of%0Ameaningful%20and%20well-clustered%20heart%20representations%20without%20relying%20on%20the%0Atraditionally%20required%2C%20and%20often%20costly%2C%20labeled%20data.%20The%20learned%20heart%0Arepresentation%20can%20be%20directly%20used%20for%20various%20downstream%20tasks.%20Furthermore%2C%0Aour%20method%20demonstrates%20remarkable%20robustness%2C%20ensuring%20consistent%0Arepresentations%20even%20when%20certain%20CMR%20planes%20are%20missing/flawed.%20We%20train%20our%0Amodel%20on%2014%2C000%20unlabeled%20CMR%20data%20from%20UK%20BioBank%20and%20evaluate%20it%20on%201%2C000%0Aannotated%20data.%20The%20proposed%20method%20demonstrates%20superior%20performance%20to%0Abaselines%20in%20tasks%20that%20demand%20comprehensive%203D%2BT%20cardiac%20information%2C%20e.g.%0Acardiac%20phenotype%20%28ejection%20fraction%20and%20ventricle%20volume%29%20prediction%20and%0Amulti-plane/multi-frame%20CMR%20segmentation%2C%20highlighting%20its%20effectiveness%20in%0Aextracting%20comprehensive%20cardiac%20features%20that%20are%20both%20anatomically%20and%0Apathologically%20relevant.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00329v2&entry.124074799=Read"},
{"title": "The 3D-PC: a benchmark for visual perspective taking in humans and\n  machines", "author": "Drew Linsley and Peisen Zhou and Alekh Karkada Ashok and Akash Nagaraj and Gaurav Gaonkar and Francis E Lewis and Zygmunt Pizlo and Thomas Serre", "abstract": "  Visual perspective taking (VPT) is the ability to perceive and reason about\nthe perspectives of others. It is an essential feature of human intelligence,\nwhich develops over the first decade of life and requires an ability to process\nthe 3D structure of visual scenes. A growing number of reports have indicated\nthat deep neural networks (DNNs) become capable of analyzing 3D scenes after\ntraining on large image datasets. We investigated if this emergent ability for\n3D analysis in DNNs is sufficient for VPT with the 3D perception challenge\n(3D-PC): a novel benchmark for 3D perception in humans and DNNs. The 3D-PC is\ncomprised of three 3D-analysis tasks posed within natural scene images: 1. a\nsimple test of object depth order, 2. a basic VPT task (VPT-basic), and 3.\nanother version of VPT (VPT-Strategy) designed to limit the effectiveness of\n\"shortcut\" visual strategies. We tested human participants (N=33) and linearly\nprobed or text-prompted over 300 DNNs on the challenge and found that nearly\nall of the DNNs approached or exceeded human accuracy in analyzing object depth\norder. Surprisingly, DNN accuracy on this task correlated with their object\nrecognition performance. In contrast, there was an extraordinary gap between\nDNNs and humans on VPT-basic. Humans were nearly perfect, whereas most DNNs\nwere near chance. Fine-tuning DNNs on VPT-basic brought them close to human\nperformance, but they, unlike humans, dropped back to chance when tested on\nVPT-perturb. Our challenge demonstrates that the training routines and\narchitectures of today's DNNs are well-suited for learning basic 3D properties\nof scenes and objects but are ill-suited for reasoning about these properties\nlike humans do. We release our 3D-PC datasets and code to help bridge this gap\nin 3D perception between humans and machines.\n", "link": "http://arxiv.org/abs/2406.04138v1", "date": "2024-06-06", "relevancy": 2.8092, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5709}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5709}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%203D-PC%3A%20a%20benchmark%20for%20visual%20perspective%20taking%20in%20humans%20and%0A%20%20machines&body=Title%3A%20The%203D-PC%3A%20a%20benchmark%20for%20visual%20perspective%20taking%20in%20humans%20and%0A%20%20machines%0AAuthor%3A%20Drew%20Linsley%20and%20Peisen%20Zhou%20and%20Alekh%20Karkada%20Ashok%20and%20Akash%20Nagaraj%20and%20Gaurav%20Gaonkar%20and%20Francis%20E%20Lewis%20and%20Zygmunt%20Pizlo%20and%20Thomas%20Serre%0AAbstract%3A%20%20%20Visual%20perspective%20taking%20%28VPT%29%20is%20the%20ability%20to%20perceive%20and%20reason%20about%0Athe%20perspectives%20of%20others.%20It%20is%20an%20essential%20feature%20of%20human%20intelligence%2C%0Awhich%20develops%20over%20the%20first%20decade%20of%20life%20and%20requires%20an%20ability%20to%20process%0Athe%203D%20structure%20of%20visual%20scenes.%20A%20growing%20number%20of%20reports%20have%20indicated%0Athat%20deep%20neural%20networks%20%28DNNs%29%20become%20capable%20of%20analyzing%203D%20scenes%20after%0Atraining%20on%20large%20image%20datasets.%20We%20investigated%20if%20this%20emergent%20ability%20for%0A3D%20analysis%20in%20DNNs%20is%20sufficient%20for%20VPT%20with%20the%203D%20perception%20challenge%0A%283D-PC%29%3A%20a%20novel%20benchmark%20for%203D%20perception%20in%20humans%20and%20DNNs.%20The%203D-PC%20is%0Acomprised%20of%20three%203D-analysis%20tasks%20posed%20within%20natural%20scene%20images%3A%201.%20a%0Asimple%20test%20of%20object%20depth%20order%2C%202.%20a%20basic%20VPT%20task%20%28VPT-basic%29%2C%20and%203.%0Aanother%20version%20of%20VPT%20%28VPT-Strategy%29%20designed%20to%20limit%20the%20effectiveness%20of%0A%22shortcut%22%20visual%20strategies.%20We%20tested%20human%20participants%20%28N%3D33%29%20and%20linearly%0Aprobed%20or%20text-prompted%20over%20300%20DNNs%20on%20the%20challenge%20and%20found%20that%20nearly%0Aall%20of%20the%20DNNs%20approached%20or%20exceeded%20human%20accuracy%20in%20analyzing%20object%20depth%0Aorder.%20Surprisingly%2C%20DNN%20accuracy%20on%20this%20task%20correlated%20with%20their%20object%0Arecognition%20performance.%20In%20contrast%2C%20there%20was%20an%20extraordinary%20gap%20between%0ADNNs%20and%20humans%20on%20VPT-basic.%20Humans%20were%20nearly%20perfect%2C%20whereas%20most%20DNNs%0Awere%20near%20chance.%20Fine-tuning%20DNNs%20on%20VPT-basic%20brought%20them%20close%20to%20human%0Aperformance%2C%20but%20they%2C%20unlike%20humans%2C%20dropped%20back%20to%20chance%20when%20tested%20on%0AVPT-perturb.%20Our%20challenge%20demonstrates%20that%20the%20training%20routines%20and%0Aarchitectures%20of%20today%27s%20DNNs%20are%20well-suited%20for%20learning%20basic%203D%20properties%0Aof%20scenes%20and%20objects%20but%20are%20ill-suited%20for%20reasoning%20about%20these%20properties%0Alike%20humans%20do.%20We%20release%20our%203D-PC%20datasets%20and%20code%20to%20help%20bridge%20this%20gap%0Ain%203D%20perception%20between%20humans%20and%20machines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04138v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%25203D-PC%253A%2520a%2520benchmark%2520for%2520visual%2520perspective%2520taking%2520in%2520humans%2520and%250A%2520%2520machines%26entry.906535625%3DDrew%2520Linsley%2520and%2520Peisen%2520Zhou%2520and%2520Alekh%2520Karkada%2520Ashok%2520and%2520Akash%2520Nagaraj%2520and%2520Gaurav%2520Gaonkar%2520and%2520Francis%2520E%2520Lewis%2520and%2520Zygmunt%2520Pizlo%2520and%2520Thomas%2520Serre%26entry.1292438233%3D%2520%2520Visual%2520perspective%2520taking%2520%2528VPT%2529%2520is%2520the%2520ability%2520to%2520perceive%2520and%2520reason%2520about%250Athe%2520perspectives%2520of%2520others.%2520It%2520is%2520an%2520essential%2520feature%2520of%2520human%2520intelligence%252C%250Awhich%2520develops%2520over%2520the%2520first%2520decade%2520of%2520life%2520and%2520requires%2520an%2520ability%2520to%2520process%250Athe%25203D%2520structure%2520of%2520visual%2520scenes.%2520A%2520growing%2520number%2520of%2520reports%2520have%2520indicated%250Athat%2520deep%2520neural%2520networks%2520%2528DNNs%2529%2520become%2520capable%2520of%2520analyzing%25203D%2520scenes%2520after%250Atraining%2520on%2520large%2520image%2520datasets.%2520We%2520investigated%2520if%2520this%2520emergent%2520ability%2520for%250A3D%2520analysis%2520in%2520DNNs%2520is%2520sufficient%2520for%2520VPT%2520with%2520the%25203D%2520perception%2520challenge%250A%25283D-PC%2529%253A%2520a%2520novel%2520benchmark%2520for%25203D%2520perception%2520in%2520humans%2520and%2520DNNs.%2520The%25203D-PC%2520is%250Acomprised%2520of%2520three%25203D-analysis%2520tasks%2520posed%2520within%2520natural%2520scene%2520images%253A%25201.%2520a%250Asimple%2520test%2520of%2520object%2520depth%2520order%252C%25202.%2520a%2520basic%2520VPT%2520task%2520%2528VPT-basic%2529%252C%2520and%25203.%250Aanother%2520version%2520of%2520VPT%2520%2528VPT-Strategy%2529%2520designed%2520to%2520limit%2520the%2520effectiveness%2520of%250A%2522shortcut%2522%2520visual%2520strategies.%2520We%2520tested%2520human%2520participants%2520%2528N%253D33%2529%2520and%2520linearly%250Aprobed%2520or%2520text-prompted%2520over%2520300%2520DNNs%2520on%2520the%2520challenge%2520and%2520found%2520that%2520nearly%250Aall%2520of%2520the%2520DNNs%2520approached%2520or%2520exceeded%2520human%2520accuracy%2520in%2520analyzing%2520object%2520depth%250Aorder.%2520Surprisingly%252C%2520DNN%2520accuracy%2520on%2520this%2520task%2520correlated%2520with%2520their%2520object%250Arecognition%2520performance.%2520In%2520contrast%252C%2520there%2520was%2520an%2520extraordinary%2520gap%2520between%250ADNNs%2520and%2520humans%2520on%2520VPT-basic.%2520Humans%2520were%2520nearly%2520perfect%252C%2520whereas%2520most%2520DNNs%250Awere%2520near%2520chance.%2520Fine-tuning%2520DNNs%2520on%2520VPT-basic%2520brought%2520them%2520close%2520to%2520human%250Aperformance%252C%2520but%2520they%252C%2520unlike%2520humans%252C%2520dropped%2520back%2520to%2520chance%2520when%2520tested%2520on%250AVPT-perturb.%2520Our%2520challenge%2520demonstrates%2520that%2520the%2520training%2520routines%2520and%250Aarchitectures%2520of%2520today%2527s%2520DNNs%2520are%2520well-suited%2520for%2520learning%2520basic%25203D%2520properties%250Aof%2520scenes%2520and%2520objects%2520but%2520are%2520ill-suited%2520for%2520reasoning%2520about%2520these%2520properties%250Alike%2520humans%2520do.%2520We%2520release%2520our%25203D-PC%2520datasets%2520and%2520code%2520to%2520help%2520bridge%2520this%2520gap%250Ain%25203D%2520perception%2520between%2520humans%2520and%2520machines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04138v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%203D-PC%3A%20a%20benchmark%20for%20visual%20perspective%20taking%20in%20humans%20and%0A%20%20machines&entry.906535625=Drew%20Linsley%20and%20Peisen%20Zhou%20and%20Alekh%20Karkada%20Ashok%20and%20Akash%20Nagaraj%20and%20Gaurav%20Gaonkar%20and%20Francis%20E%20Lewis%20and%20Zygmunt%20Pizlo%20and%20Thomas%20Serre&entry.1292438233=%20%20Visual%20perspective%20taking%20%28VPT%29%20is%20the%20ability%20to%20perceive%20and%20reason%20about%0Athe%20perspectives%20of%20others.%20It%20is%20an%20essential%20feature%20of%20human%20intelligence%2C%0Awhich%20develops%20over%20the%20first%20decade%20of%20life%20and%20requires%20an%20ability%20to%20process%0Athe%203D%20structure%20of%20visual%20scenes.%20A%20growing%20number%20of%20reports%20have%20indicated%0Athat%20deep%20neural%20networks%20%28DNNs%29%20become%20capable%20of%20analyzing%203D%20scenes%20after%0Atraining%20on%20large%20image%20datasets.%20We%20investigated%20if%20this%20emergent%20ability%20for%0A3D%20analysis%20in%20DNNs%20is%20sufficient%20for%20VPT%20with%20the%203D%20perception%20challenge%0A%283D-PC%29%3A%20a%20novel%20benchmark%20for%203D%20perception%20in%20humans%20and%20DNNs.%20The%203D-PC%20is%0Acomprised%20of%20three%203D-analysis%20tasks%20posed%20within%20natural%20scene%20images%3A%201.%20a%0Asimple%20test%20of%20object%20depth%20order%2C%202.%20a%20basic%20VPT%20task%20%28VPT-basic%29%2C%20and%203.%0Aanother%20version%20of%20VPT%20%28VPT-Strategy%29%20designed%20to%20limit%20the%20effectiveness%20of%0A%22shortcut%22%20visual%20strategies.%20We%20tested%20human%20participants%20%28N%3D33%29%20and%20linearly%0Aprobed%20or%20text-prompted%20over%20300%20DNNs%20on%20the%20challenge%20and%20found%20that%20nearly%0Aall%20of%20the%20DNNs%20approached%20or%20exceeded%20human%20accuracy%20in%20analyzing%20object%20depth%0Aorder.%20Surprisingly%2C%20DNN%20accuracy%20on%20this%20task%20correlated%20with%20their%20object%0Arecognition%20performance.%20In%20contrast%2C%20there%20was%20an%20extraordinary%20gap%20between%0ADNNs%20and%20humans%20on%20VPT-basic.%20Humans%20were%20nearly%20perfect%2C%20whereas%20most%20DNNs%0Awere%20near%20chance.%20Fine-tuning%20DNNs%20on%20VPT-basic%20brought%20them%20close%20to%20human%0Aperformance%2C%20but%20they%2C%20unlike%20humans%2C%20dropped%20back%20to%20chance%20when%20tested%20on%0AVPT-perturb.%20Our%20challenge%20demonstrates%20that%20the%20training%20routines%20and%0Aarchitectures%20of%20today%27s%20DNNs%20are%20well-suited%20for%20learning%20basic%203D%20properties%0Aof%20scenes%20and%20objects%20but%20are%20ill-suited%20for%20reasoning%20about%20these%20properties%0Alike%20humans%20do.%20We%20release%20our%203D-PC%20datasets%20and%20code%20to%20help%20bridge%20this%20gap%0Ain%203D%20perception%20between%20humans%20and%20machines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04138v1&entry.124074799=Read"},
{"title": "SpectralZoom: Efficient Segmentation with an Adaptive Hyperspectral\n  Camera", "author": "Jackson Arnold and Sophia Rossi and Chloe Petrosino and Ethan Mitchell and Sanjeev J. Koppal", "abstract": "  Hyperspectral image segmentation is crucial for many fields such as\nagriculture, remote sensing, biomedical imaging, battlefield sensing and\nastronomy. However, the challenge of hyper and multi spectral imaging is its\nlarge data footprint. We propose both a novel camera design and a vision\ntransformer-based (ViT) algorithm that alleviate both the captured data\nfootprint and the computational load for hyperspectral segmentation. Our camera\nis able to adaptively sample image regions or patches at different resolutions,\ninstead of capturing the entire hyperspectral cube at one high resolution. Our\nsegmentation algorithm works in concert with the camera, applying ViT-based\nsegmentation only to adaptively selected patches. We show results both in\nsimulation and on a real hardware platform demonstrating both accurate\nsegmentation results and reduced computational burden.\n", "link": "http://arxiv.org/abs/2406.04287v1", "date": "2024-06-06", "relevancy": 2.7914, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5935}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5407}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpectralZoom%3A%20Efficient%20Segmentation%20with%20an%20Adaptive%20Hyperspectral%0A%20%20Camera&body=Title%3A%20SpectralZoom%3A%20Efficient%20Segmentation%20with%20an%20Adaptive%20Hyperspectral%0A%20%20Camera%0AAuthor%3A%20Jackson%20Arnold%20and%20Sophia%20Rossi%20and%20Chloe%20Petrosino%20and%20Ethan%20Mitchell%20and%20Sanjeev%20J.%20Koppal%0AAbstract%3A%20%20%20Hyperspectral%20image%20segmentation%20is%20crucial%20for%20many%20fields%20such%20as%0Aagriculture%2C%20remote%20sensing%2C%20biomedical%20imaging%2C%20battlefield%20sensing%20and%0Aastronomy.%20However%2C%20the%20challenge%20of%20hyper%20and%20multi%20spectral%20imaging%20is%20its%0Alarge%20data%20footprint.%20We%20propose%20both%20a%20novel%20camera%20design%20and%20a%20vision%0Atransformer-based%20%28ViT%29%20algorithm%20that%20alleviate%20both%20the%20captured%20data%0Afootprint%20and%20the%20computational%20load%20for%20hyperspectral%20segmentation.%20Our%20camera%0Ais%20able%20to%20adaptively%20sample%20image%20regions%20or%20patches%20at%20different%20resolutions%2C%0Ainstead%20of%20capturing%20the%20entire%20hyperspectral%20cube%20at%20one%20high%20resolution.%20Our%0Asegmentation%20algorithm%20works%20in%20concert%20with%20the%20camera%2C%20applying%20ViT-based%0Asegmentation%20only%20to%20adaptively%20selected%20patches.%20We%20show%20results%20both%20in%0Asimulation%20and%20on%20a%20real%20hardware%20platform%20demonstrating%20both%20accurate%0Asegmentation%20results%20and%20reduced%20computational%20burden.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04287v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectralZoom%253A%2520Efficient%2520Segmentation%2520with%2520an%2520Adaptive%2520Hyperspectral%250A%2520%2520Camera%26entry.906535625%3DJackson%2520Arnold%2520and%2520Sophia%2520Rossi%2520and%2520Chloe%2520Petrosino%2520and%2520Ethan%2520Mitchell%2520and%2520Sanjeev%2520J.%2520Koppal%26entry.1292438233%3D%2520%2520Hyperspectral%2520image%2520segmentation%2520is%2520crucial%2520for%2520many%2520fields%2520such%2520as%250Aagriculture%252C%2520remote%2520sensing%252C%2520biomedical%2520imaging%252C%2520battlefield%2520sensing%2520and%250Aastronomy.%2520However%252C%2520the%2520challenge%2520of%2520hyper%2520and%2520multi%2520spectral%2520imaging%2520is%2520its%250Alarge%2520data%2520footprint.%2520We%2520propose%2520both%2520a%2520novel%2520camera%2520design%2520and%2520a%2520vision%250Atransformer-based%2520%2528ViT%2529%2520algorithm%2520that%2520alleviate%2520both%2520the%2520captured%2520data%250Afootprint%2520and%2520the%2520computational%2520load%2520for%2520hyperspectral%2520segmentation.%2520Our%2520camera%250Ais%2520able%2520to%2520adaptively%2520sample%2520image%2520regions%2520or%2520patches%2520at%2520different%2520resolutions%252C%250Ainstead%2520of%2520capturing%2520the%2520entire%2520hyperspectral%2520cube%2520at%2520one%2520high%2520resolution.%2520Our%250Asegmentation%2520algorithm%2520works%2520in%2520concert%2520with%2520the%2520camera%252C%2520applying%2520ViT-based%250Asegmentation%2520only%2520to%2520adaptively%2520selected%2520patches.%2520We%2520show%2520results%2520both%2520in%250Asimulation%2520and%2520on%2520a%2520real%2520hardware%2520platform%2520demonstrating%2520both%2520accurate%250Asegmentation%2520results%2520and%2520reduced%2520computational%2520burden.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04287v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpectralZoom%3A%20Efficient%20Segmentation%20with%20an%20Adaptive%20Hyperspectral%0A%20%20Camera&entry.906535625=Jackson%20Arnold%20and%20Sophia%20Rossi%20and%20Chloe%20Petrosino%20and%20Ethan%20Mitchell%20and%20Sanjeev%20J.%20Koppal&entry.1292438233=%20%20Hyperspectral%20image%20segmentation%20is%20crucial%20for%20many%20fields%20such%20as%0Aagriculture%2C%20remote%20sensing%2C%20biomedical%20imaging%2C%20battlefield%20sensing%20and%0Aastronomy.%20However%2C%20the%20challenge%20of%20hyper%20and%20multi%20spectral%20imaging%20is%20its%0Alarge%20data%20footprint.%20We%20propose%20both%20a%20novel%20camera%20design%20and%20a%20vision%0Atransformer-based%20%28ViT%29%20algorithm%20that%20alleviate%20both%20the%20captured%20data%0Afootprint%20and%20the%20computational%20load%20for%20hyperspectral%20segmentation.%20Our%20camera%0Ais%20able%20to%20adaptively%20sample%20image%20regions%20or%20patches%20at%20different%20resolutions%2C%0Ainstead%20of%20capturing%20the%20entire%20hyperspectral%20cube%20at%20one%20high%20resolution.%20Our%0Asegmentation%20algorithm%20works%20in%20concert%20with%20the%20camera%2C%20applying%20ViT-based%0Asegmentation%20only%20to%20adaptively%20selected%20patches.%20We%20show%20results%20both%20in%0Asimulation%20and%20on%20a%20real%20hardware%20platform%20demonstrating%20both%20accurate%0Asegmentation%20results%20and%20reduced%20computational%20burden.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04287v1&entry.124074799=Read"},
{"title": "Stereo-Depth Fusion through Virtual Pattern Projection", "author": "Luca Bartolomei and Matteo Poggi and Fabio Tosi and Andrea Conti and Stefano Mattoccia", "abstract": "  This paper presents a novel general-purpose stereo and depth data fusion\nparadigm that mimics the active stereo principle by replacing the unreliable\nphysical pattern projector with a depth sensor. It works by projecting virtual\npatterns consistent with the scene geometry onto the left and right images\nacquired by a conventional stereo camera, using the sparse hints obtained from\na depth sensor, to facilitate the visual correspondence. Purposely, any depth\nsensing device can be seamlessly plugged into our framework, enabling the\ndeployment of a virtual active stereo setup in any possible environment and\novercoming the severe limitations of physical pattern projection, such as the\nlimited working range and environmental conditions. Exhaustive experiments on\nindoor and outdoor datasets featuring both long and close range, including\nthose providing raw, unfiltered depth hints from off-the-shelf depth sensors,\nhighlight the effectiveness of our approach in notably boosting the robustness\nand accuracy of algorithms and deep stereo without any code modification and\neven without re-training. Additionally, we assess the performance of our\nstrategy on active stereo evaluation datasets with conventional pattern\nprojection. Indeed, in all these scenarios, our virtual pattern projection\nparadigm achieves state-of-the-art performance. The source code is available\nat: https://github.com/bartn8/vppstereo.\n", "link": "http://arxiv.org/abs/2406.04345v1", "date": "2024-06-06", "relevancy": 2.7615, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5553}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5553}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stereo-Depth%20Fusion%20through%20Virtual%20Pattern%20Projection&body=Title%3A%20Stereo-Depth%20Fusion%20through%20Virtual%20Pattern%20Projection%0AAuthor%3A%20Luca%20Bartolomei%20and%20Matteo%20Poggi%20and%20Fabio%20Tosi%20and%20Andrea%20Conti%20and%20Stefano%20Mattoccia%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20general-purpose%20stereo%20and%20depth%20data%20fusion%0Aparadigm%20that%20mimics%20the%20active%20stereo%20principle%20by%20replacing%20the%20unreliable%0Aphysical%20pattern%20projector%20with%20a%20depth%20sensor.%20It%20works%20by%20projecting%20virtual%0Apatterns%20consistent%20with%20the%20scene%20geometry%20onto%20the%20left%20and%20right%20images%0Aacquired%20by%20a%20conventional%20stereo%20camera%2C%20using%20the%20sparse%20hints%20obtained%20from%0Aa%20depth%20sensor%2C%20to%20facilitate%20the%20visual%20correspondence.%20Purposely%2C%20any%20depth%0Asensing%20device%20can%20be%20seamlessly%20plugged%20into%20our%20framework%2C%20enabling%20the%0Adeployment%20of%20a%20virtual%20active%20stereo%20setup%20in%20any%20possible%20environment%20and%0Aovercoming%20the%20severe%20limitations%20of%20physical%20pattern%20projection%2C%20such%20as%20the%0Alimited%20working%20range%20and%20environmental%20conditions.%20Exhaustive%20experiments%20on%0Aindoor%20and%20outdoor%20datasets%20featuring%20both%20long%20and%20close%20range%2C%20including%0Athose%20providing%20raw%2C%20unfiltered%20depth%20hints%20from%20off-the-shelf%20depth%20sensors%2C%0Ahighlight%20the%20effectiveness%20of%20our%20approach%20in%20notably%20boosting%20the%20robustness%0Aand%20accuracy%20of%20algorithms%20and%20deep%20stereo%20without%20any%20code%20modification%20and%0Aeven%20without%20re-training.%20Additionally%2C%20we%20assess%20the%20performance%20of%20our%0Astrategy%20on%20active%20stereo%20evaluation%20datasets%20with%20conventional%20pattern%0Aprojection.%20Indeed%2C%20in%20all%20these%20scenarios%2C%20our%20virtual%20pattern%20projection%0Aparadigm%20achieves%20state-of-the-art%20performance.%20The%20source%20code%20is%20available%0Aat%3A%20https%3A//github.com/bartn8/vppstereo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04345v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStereo-Depth%2520Fusion%2520through%2520Virtual%2520Pattern%2520Projection%26entry.906535625%3DLuca%2520Bartolomei%2520and%2520Matteo%2520Poggi%2520and%2520Fabio%2520Tosi%2520and%2520Andrea%2520Conti%2520and%2520Stefano%2520Mattoccia%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520general-purpose%2520stereo%2520and%2520depth%2520data%2520fusion%250Aparadigm%2520that%2520mimics%2520the%2520active%2520stereo%2520principle%2520by%2520replacing%2520the%2520unreliable%250Aphysical%2520pattern%2520projector%2520with%2520a%2520depth%2520sensor.%2520It%2520works%2520by%2520projecting%2520virtual%250Apatterns%2520consistent%2520with%2520the%2520scene%2520geometry%2520onto%2520the%2520left%2520and%2520right%2520images%250Aacquired%2520by%2520a%2520conventional%2520stereo%2520camera%252C%2520using%2520the%2520sparse%2520hints%2520obtained%2520from%250Aa%2520depth%2520sensor%252C%2520to%2520facilitate%2520the%2520visual%2520correspondence.%2520Purposely%252C%2520any%2520depth%250Asensing%2520device%2520can%2520be%2520seamlessly%2520plugged%2520into%2520our%2520framework%252C%2520enabling%2520the%250Adeployment%2520of%2520a%2520virtual%2520active%2520stereo%2520setup%2520in%2520any%2520possible%2520environment%2520and%250Aovercoming%2520the%2520severe%2520limitations%2520of%2520physical%2520pattern%2520projection%252C%2520such%2520as%2520the%250Alimited%2520working%2520range%2520and%2520environmental%2520conditions.%2520Exhaustive%2520experiments%2520on%250Aindoor%2520and%2520outdoor%2520datasets%2520featuring%2520both%2520long%2520and%2520close%2520range%252C%2520including%250Athose%2520providing%2520raw%252C%2520unfiltered%2520depth%2520hints%2520from%2520off-the-shelf%2520depth%2520sensors%252C%250Ahighlight%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%2520notably%2520boosting%2520the%2520robustness%250Aand%2520accuracy%2520of%2520algorithms%2520and%2520deep%2520stereo%2520without%2520any%2520code%2520modification%2520and%250Aeven%2520without%2520re-training.%2520Additionally%252C%2520we%2520assess%2520the%2520performance%2520of%2520our%250Astrategy%2520on%2520active%2520stereo%2520evaluation%2520datasets%2520with%2520conventional%2520pattern%250Aprojection.%2520Indeed%252C%2520in%2520all%2520these%2520scenarios%252C%2520our%2520virtual%2520pattern%2520projection%250Aparadigm%2520achieves%2520state-of-the-art%2520performance.%2520The%2520source%2520code%2520is%2520available%250Aat%253A%2520https%253A//github.com/bartn8/vppstereo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04345v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stereo-Depth%20Fusion%20through%20Virtual%20Pattern%20Projection&entry.906535625=Luca%20Bartolomei%20and%20Matteo%20Poggi%20and%20Fabio%20Tosi%20and%20Andrea%20Conti%20and%20Stefano%20Mattoccia&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20general-purpose%20stereo%20and%20depth%20data%20fusion%0Aparadigm%20that%20mimics%20the%20active%20stereo%20principle%20by%20replacing%20the%20unreliable%0Aphysical%20pattern%20projector%20with%20a%20depth%20sensor.%20It%20works%20by%20projecting%20virtual%0Apatterns%20consistent%20with%20the%20scene%20geometry%20onto%20the%20left%20and%20right%20images%0Aacquired%20by%20a%20conventional%20stereo%20camera%2C%20using%20the%20sparse%20hints%20obtained%20from%0Aa%20depth%20sensor%2C%20to%20facilitate%20the%20visual%20correspondence.%20Purposely%2C%20any%20depth%0Asensing%20device%20can%20be%20seamlessly%20plugged%20into%20our%20framework%2C%20enabling%20the%0Adeployment%20of%20a%20virtual%20active%20stereo%20setup%20in%20any%20possible%20environment%20and%0Aovercoming%20the%20severe%20limitations%20of%20physical%20pattern%20projection%2C%20such%20as%20the%0Alimited%20working%20range%20and%20environmental%20conditions.%20Exhaustive%20experiments%20on%0Aindoor%20and%20outdoor%20datasets%20featuring%20both%20long%20and%20close%20range%2C%20including%0Athose%20providing%20raw%2C%20unfiltered%20depth%20hints%20from%20off-the-shelf%20depth%20sensors%2C%0Ahighlight%20the%20effectiveness%20of%20our%20approach%20in%20notably%20boosting%20the%20robustness%0Aand%20accuracy%20of%20algorithms%20and%20deep%20stereo%20without%20any%20code%20modification%20and%0Aeven%20without%20re-training.%20Additionally%2C%20we%20assess%20the%20performance%20of%20our%0Astrategy%20on%20active%20stereo%20evaluation%20datasets%20with%20conventional%20pattern%0Aprojection.%20Indeed%2C%20in%20all%20these%20scenarios%2C%20our%20virtual%20pattern%20projection%0Aparadigm%20achieves%20state-of-the-art%20performance.%20The%20source%20code%20is%20available%0Aat%3A%20https%3A//github.com/bartn8/vppstereo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04345v1&entry.124074799=Read"},
{"title": "GenCO: Generating Diverse Designs with Combinatorial Constraints", "author": "Aaron Ferber and Arman Zharmagambetov and Taoan Huang and Bistra Dilkina and Yuandong Tian", "abstract": "  Deep generative models like GAN and VAE have shown impressive results in\ngenerating unconstrained objects like images. However, many design settings\narising in industrial design, material science, computer graphics and more\nrequire that the generated objects satisfy hard combinatorial constraints or\nmeet objectives in addition to modeling a data distribution. To address this,\nwe propose GenCO, a generative framework that guarantees constraint\nsatisfaction throughout training by leveraging differentiable combinatorial\nsolvers to enforce feasibility. GenCO imposes the generative loss on provably\nfeasible solutions rather than intermediate soft solutions, meaning that the\ndeep generative network can focus on ensuring the generated objects match the\ndata distribution without having to also capture feasibility. This shift\nenables practitioners to enforce hard constraints on the generated outputs\nduring end-to-end training, enabling assessments of their feasibility and\nintroducing additional combinatorial loss components to deep generative\ntraining. We demonstrate the effectiveness of our approach on a variety of\ngenerative combinatorial tasks, including game level generation, map creation\nfor path planning, and photonic device design, consistently demonstrating its\ncapability to yield diverse, high-quality solutions that verifiably adhere to\nuser-specified combinatorial properties.\n", "link": "http://arxiv.org/abs/2310.02442v2", "date": "2024-06-06", "relevancy": 2.6984, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5578}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5489}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenCO%3A%20Generating%20Diverse%20Designs%20with%20Combinatorial%20Constraints&body=Title%3A%20GenCO%3A%20Generating%20Diverse%20Designs%20with%20Combinatorial%20Constraints%0AAuthor%3A%20Aaron%20Ferber%20and%20Arman%20Zharmagambetov%20and%20Taoan%20Huang%20and%20Bistra%20Dilkina%20and%20Yuandong%20Tian%0AAbstract%3A%20%20%20Deep%20generative%20models%20like%20GAN%20and%20VAE%20have%20shown%20impressive%20results%20in%0Agenerating%20unconstrained%20objects%20like%20images.%20However%2C%20many%20design%20settings%0Aarising%20in%20industrial%20design%2C%20material%20science%2C%20computer%20graphics%20and%20more%0Arequire%20that%20the%20generated%20objects%20satisfy%20hard%20combinatorial%20constraints%20or%0Ameet%20objectives%20in%20addition%20to%20modeling%20a%20data%20distribution.%20To%20address%20this%2C%0Awe%20propose%20GenCO%2C%20a%20generative%20framework%20that%20guarantees%20constraint%0Asatisfaction%20throughout%20training%20by%20leveraging%20differentiable%20combinatorial%0Asolvers%20to%20enforce%20feasibility.%20GenCO%20imposes%20the%20generative%20loss%20on%20provably%0Afeasible%20solutions%20rather%20than%20intermediate%20soft%20solutions%2C%20meaning%20that%20the%0Adeep%20generative%20network%20can%20focus%20on%20ensuring%20the%20generated%20objects%20match%20the%0Adata%20distribution%20without%20having%20to%20also%20capture%20feasibility.%20This%20shift%0Aenables%20practitioners%20to%20enforce%20hard%20constraints%20on%20the%20generated%20outputs%0Aduring%20end-to-end%20training%2C%20enabling%20assessments%20of%20their%20feasibility%20and%0Aintroducing%20additional%20combinatorial%20loss%20components%20to%20deep%20generative%0Atraining.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20on%20a%20variety%20of%0Agenerative%20combinatorial%20tasks%2C%20including%20game%20level%20generation%2C%20map%20creation%0Afor%20path%20planning%2C%20and%20photonic%20device%20design%2C%20consistently%20demonstrating%20its%0Acapability%20to%20yield%20diverse%2C%20high-quality%20solutions%20that%20verifiably%20adhere%20to%0Auser-specified%20combinatorial%20properties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02442v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenCO%253A%2520Generating%2520Diverse%2520Designs%2520with%2520Combinatorial%2520Constraints%26entry.906535625%3DAaron%2520Ferber%2520and%2520Arman%2520Zharmagambetov%2520and%2520Taoan%2520Huang%2520and%2520Bistra%2520Dilkina%2520and%2520Yuandong%2520Tian%26entry.1292438233%3D%2520%2520Deep%2520generative%2520models%2520like%2520GAN%2520and%2520VAE%2520have%2520shown%2520impressive%2520results%2520in%250Agenerating%2520unconstrained%2520objects%2520like%2520images.%2520However%252C%2520many%2520design%2520settings%250Aarising%2520in%2520industrial%2520design%252C%2520material%2520science%252C%2520computer%2520graphics%2520and%2520more%250Arequire%2520that%2520the%2520generated%2520objects%2520satisfy%2520hard%2520combinatorial%2520constraints%2520or%250Ameet%2520objectives%2520in%2520addition%2520to%2520modeling%2520a%2520data%2520distribution.%2520To%2520address%2520this%252C%250Awe%2520propose%2520GenCO%252C%2520a%2520generative%2520framework%2520that%2520guarantees%2520constraint%250Asatisfaction%2520throughout%2520training%2520by%2520leveraging%2520differentiable%2520combinatorial%250Asolvers%2520to%2520enforce%2520feasibility.%2520GenCO%2520imposes%2520the%2520generative%2520loss%2520on%2520provably%250Afeasible%2520solutions%2520rather%2520than%2520intermediate%2520soft%2520solutions%252C%2520meaning%2520that%2520the%250Adeep%2520generative%2520network%2520can%2520focus%2520on%2520ensuring%2520the%2520generated%2520objects%2520match%2520the%250Adata%2520distribution%2520without%2520having%2520to%2520also%2520capture%2520feasibility.%2520This%2520shift%250Aenables%2520practitioners%2520to%2520enforce%2520hard%2520constraints%2520on%2520the%2520generated%2520outputs%250Aduring%2520end-to-end%2520training%252C%2520enabling%2520assessments%2520of%2520their%2520feasibility%2520and%250Aintroducing%2520additional%2520combinatorial%2520loss%2520components%2520to%2520deep%2520generative%250Atraining.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520on%2520a%2520variety%2520of%250Agenerative%2520combinatorial%2520tasks%252C%2520including%2520game%2520level%2520generation%252C%2520map%2520creation%250Afor%2520path%2520planning%252C%2520and%2520photonic%2520device%2520design%252C%2520consistently%2520demonstrating%2520its%250Acapability%2520to%2520yield%2520diverse%252C%2520high-quality%2520solutions%2520that%2520verifiably%2520adhere%2520to%250Auser-specified%2520combinatorial%2520properties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02442v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenCO%3A%20Generating%20Diverse%20Designs%20with%20Combinatorial%20Constraints&entry.906535625=Aaron%20Ferber%20and%20Arman%20Zharmagambetov%20and%20Taoan%20Huang%20and%20Bistra%20Dilkina%20and%20Yuandong%20Tian&entry.1292438233=%20%20Deep%20generative%20models%20like%20GAN%20and%20VAE%20have%20shown%20impressive%20results%20in%0Agenerating%20unconstrained%20objects%20like%20images.%20However%2C%20many%20design%20settings%0Aarising%20in%20industrial%20design%2C%20material%20science%2C%20computer%20graphics%20and%20more%0Arequire%20that%20the%20generated%20objects%20satisfy%20hard%20combinatorial%20constraints%20or%0Ameet%20objectives%20in%20addition%20to%20modeling%20a%20data%20distribution.%20To%20address%20this%2C%0Awe%20propose%20GenCO%2C%20a%20generative%20framework%20that%20guarantees%20constraint%0Asatisfaction%20throughout%20training%20by%20leveraging%20differentiable%20combinatorial%0Asolvers%20to%20enforce%20feasibility.%20GenCO%20imposes%20the%20generative%20loss%20on%20provably%0Afeasible%20solutions%20rather%20than%20intermediate%20soft%20solutions%2C%20meaning%20that%20the%0Adeep%20generative%20network%20can%20focus%20on%20ensuring%20the%20generated%20objects%20match%20the%0Adata%20distribution%20without%20having%20to%20also%20capture%20feasibility.%20This%20shift%0Aenables%20practitioners%20to%20enforce%20hard%20constraints%20on%20the%20generated%20outputs%0Aduring%20end-to-end%20training%2C%20enabling%20assessments%20of%20their%20feasibility%20and%0Aintroducing%20additional%20combinatorial%20loss%20components%20to%20deep%20generative%0Atraining.%20We%20demonstrate%20the%20effectiveness%20of%20our%20approach%20on%20a%20variety%20of%0Agenerative%20combinatorial%20tasks%2C%20including%20game%20level%20generation%2C%20map%20creation%0Afor%20path%20planning%2C%20and%20photonic%20device%20design%2C%20consistently%20demonstrating%20its%0Acapability%20to%20yield%20diverse%2C%20high-quality%20solutions%20that%20verifiably%20adhere%20to%0Auser-specified%20combinatorial%20properties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02442v2&entry.124074799=Read"},
{"title": "DIRECT-3D: Learning Direct Text-to-3D Generation on Massive Noisy 3D\n  Data", "author": "Qihao Liu and Yi Zhang and Song Bai and Adam Kortylewski and Alan Yuille", "abstract": "  We present DIRECT-3D, a diffusion-based 3D generative model for creating\nhigh-quality 3D assets (represented by Neural Radiance Fields) from text\nprompts. Unlike recent 3D generative models that rely on clean and well-aligned\n3D data, limiting them to single or few-class generation, our model is directly\ntrained on extensive noisy and unaligned `in-the-wild' 3D assets, mitigating\nthe key challenge (i.e., data scarcity) in large-scale 3D generation. In\nparticular, DIRECT-3D is a tri-plane diffusion model that integrates two\ninnovations: 1) A novel learning framework where noisy data are filtered and\naligned automatically during the training process. Specifically, after an\ninitial warm-up phase using a small set of clean data, an iterative\noptimization is introduced in the diffusion process to explicitly estimate the\n3D pose of objects and select beneficial data based on conditional density. 2)\nAn efficient 3D representation that is achieved by disentangling object\ngeometry and color features with two separate conditional diffusion models that\nare optimized hierarchically. Given a prompt input, our model generates\nhigh-quality, high-resolution, realistic, and complex 3D objects with accurate\ngeometric details in seconds. We achieve state-of-the-art performance in both\nsingle-class generation and text-to-3D generation. We also demonstrate that\nDIRECT-3D can serve as a useful 3D geometric prior of objects, for example to\nalleviate the well-known Janus problem in 2D-lifting methods such as\nDreamFusion. The code and models are available for research purposes at:\nhttps://github.com/qihao067/direct3d.\n", "link": "http://arxiv.org/abs/2406.04322v1", "date": "2024-06-06", "relevancy": 2.6974, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6801}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6801}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIRECT-3D%3A%20Learning%20Direct%20Text-to-3D%20Generation%20on%20Massive%20Noisy%203D%0A%20%20Data&body=Title%3A%20DIRECT-3D%3A%20Learning%20Direct%20Text-to-3D%20Generation%20on%20Massive%20Noisy%203D%0A%20%20Data%0AAuthor%3A%20Qihao%20Liu%20and%20Yi%20Zhang%20and%20Song%20Bai%20and%20Adam%20Kortylewski%20and%20Alan%20Yuille%0AAbstract%3A%20%20%20We%20present%20DIRECT-3D%2C%20a%20diffusion-based%203D%20generative%20model%20for%20creating%0Ahigh-quality%203D%20assets%20%28represented%20by%20Neural%20Radiance%20Fields%29%20from%20text%0Aprompts.%20Unlike%20recent%203D%20generative%20models%20that%20rely%20on%20clean%20and%20well-aligned%0A3D%20data%2C%20limiting%20them%20to%20single%20or%20few-class%20generation%2C%20our%20model%20is%20directly%0Atrained%20on%20extensive%20noisy%20and%20unaligned%20%60in-the-wild%27%203D%20assets%2C%20mitigating%0Athe%20key%20challenge%20%28i.e.%2C%20data%20scarcity%29%20in%20large-scale%203D%20generation.%20In%0Aparticular%2C%20DIRECT-3D%20is%20a%20tri-plane%20diffusion%20model%20that%20integrates%20two%0Ainnovations%3A%201%29%20A%20novel%20learning%20framework%20where%20noisy%20data%20are%20filtered%20and%0Aaligned%20automatically%20during%20the%20training%20process.%20Specifically%2C%20after%20an%0Ainitial%20warm-up%20phase%20using%20a%20small%20set%20of%20clean%20data%2C%20an%20iterative%0Aoptimization%20is%20introduced%20in%20the%20diffusion%20process%20to%20explicitly%20estimate%20the%0A3D%20pose%20of%20objects%20and%20select%20beneficial%20data%20based%20on%20conditional%20density.%202%29%0AAn%20efficient%203D%20representation%20that%20is%20achieved%20by%20disentangling%20object%0Ageometry%20and%20color%20features%20with%20two%20separate%20conditional%20diffusion%20models%20that%0Aare%20optimized%20hierarchically.%20Given%20a%20prompt%20input%2C%20our%20model%20generates%0Ahigh-quality%2C%20high-resolution%2C%20realistic%2C%20and%20complex%203D%20objects%20with%20accurate%0Ageometric%20details%20in%20seconds.%20We%20achieve%20state-of-the-art%20performance%20in%20both%0Asingle-class%20generation%20and%20text-to-3D%20generation.%20We%20also%20demonstrate%20that%0ADIRECT-3D%20can%20serve%20as%20a%20useful%203D%20geometric%20prior%20of%20objects%2C%20for%20example%20to%0Aalleviate%20the%20well-known%20Janus%20problem%20in%202D-lifting%20methods%20such%20as%0ADreamFusion.%20The%20code%20and%20models%20are%20available%20for%20research%20purposes%20at%3A%0Ahttps%3A//github.com/qihao067/direct3d.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04322v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIRECT-3D%253A%2520Learning%2520Direct%2520Text-to-3D%2520Generation%2520on%2520Massive%2520Noisy%25203D%250A%2520%2520Data%26entry.906535625%3DQihao%2520Liu%2520and%2520Yi%2520Zhang%2520and%2520Song%2520Bai%2520and%2520Adam%2520Kortylewski%2520and%2520Alan%2520Yuille%26entry.1292438233%3D%2520%2520We%2520present%2520DIRECT-3D%252C%2520a%2520diffusion-based%25203D%2520generative%2520model%2520for%2520creating%250Ahigh-quality%25203D%2520assets%2520%2528represented%2520by%2520Neural%2520Radiance%2520Fields%2529%2520from%2520text%250Aprompts.%2520Unlike%2520recent%25203D%2520generative%2520models%2520that%2520rely%2520on%2520clean%2520and%2520well-aligned%250A3D%2520data%252C%2520limiting%2520them%2520to%2520single%2520or%2520few-class%2520generation%252C%2520our%2520model%2520is%2520directly%250Atrained%2520on%2520extensive%2520noisy%2520and%2520unaligned%2520%2560in-the-wild%2527%25203D%2520assets%252C%2520mitigating%250Athe%2520key%2520challenge%2520%2528i.e.%252C%2520data%2520scarcity%2529%2520in%2520large-scale%25203D%2520generation.%2520In%250Aparticular%252C%2520DIRECT-3D%2520is%2520a%2520tri-plane%2520diffusion%2520model%2520that%2520integrates%2520two%250Ainnovations%253A%25201%2529%2520A%2520novel%2520learning%2520framework%2520where%2520noisy%2520data%2520are%2520filtered%2520and%250Aaligned%2520automatically%2520during%2520the%2520training%2520process.%2520Specifically%252C%2520after%2520an%250Ainitial%2520warm-up%2520phase%2520using%2520a%2520small%2520set%2520of%2520clean%2520data%252C%2520an%2520iterative%250Aoptimization%2520is%2520introduced%2520in%2520the%2520diffusion%2520process%2520to%2520explicitly%2520estimate%2520the%250A3D%2520pose%2520of%2520objects%2520and%2520select%2520beneficial%2520data%2520based%2520on%2520conditional%2520density.%25202%2529%250AAn%2520efficient%25203D%2520representation%2520that%2520is%2520achieved%2520by%2520disentangling%2520object%250Ageometry%2520and%2520color%2520features%2520with%2520two%2520separate%2520conditional%2520diffusion%2520models%2520that%250Aare%2520optimized%2520hierarchically.%2520Given%2520a%2520prompt%2520input%252C%2520our%2520model%2520generates%250Ahigh-quality%252C%2520high-resolution%252C%2520realistic%252C%2520and%2520complex%25203D%2520objects%2520with%2520accurate%250Ageometric%2520details%2520in%2520seconds.%2520We%2520achieve%2520state-of-the-art%2520performance%2520in%2520both%250Asingle-class%2520generation%2520and%2520text-to-3D%2520generation.%2520We%2520also%2520demonstrate%2520that%250ADIRECT-3D%2520can%2520serve%2520as%2520a%2520useful%25203D%2520geometric%2520prior%2520of%2520objects%252C%2520for%2520example%2520to%250Aalleviate%2520the%2520well-known%2520Janus%2520problem%2520in%25202D-lifting%2520methods%2520such%2520as%250ADreamFusion.%2520The%2520code%2520and%2520models%2520are%2520available%2520for%2520research%2520purposes%2520at%253A%250Ahttps%253A//github.com/qihao067/direct3d.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04322v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIRECT-3D%3A%20Learning%20Direct%20Text-to-3D%20Generation%20on%20Massive%20Noisy%203D%0A%20%20Data&entry.906535625=Qihao%20Liu%20and%20Yi%20Zhang%20and%20Song%20Bai%20and%20Adam%20Kortylewski%20and%20Alan%20Yuille&entry.1292438233=%20%20We%20present%20DIRECT-3D%2C%20a%20diffusion-based%203D%20generative%20model%20for%20creating%0Ahigh-quality%203D%20assets%20%28represented%20by%20Neural%20Radiance%20Fields%29%20from%20text%0Aprompts.%20Unlike%20recent%203D%20generative%20models%20that%20rely%20on%20clean%20and%20well-aligned%0A3D%20data%2C%20limiting%20them%20to%20single%20or%20few-class%20generation%2C%20our%20model%20is%20directly%0Atrained%20on%20extensive%20noisy%20and%20unaligned%20%60in-the-wild%27%203D%20assets%2C%20mitigating%0Athe%20key%20challenge%20%28i.e.%2C%20data%20scarcity%29%20in%20large-scale%203D%20generation.%20In%0Aparticular%2C%20DIRECT-3D%20is%20a%20tri-plane%20diffusion%20model%20that%20integrates%20two%0Ainnovations%3A%201%29%20A%20novel%20learning%20framework%20where%20noisy%20data%20are%20filtered%20and%0Aaligned%20automatically%20during%20the%20training%20process.%20Specifically%2C%20after%20an%0Ainitial%20warm-up%20phase%20using%20a%20small%20set%20of%20clean%20data%2C%20an%20iterative%0Aoptimization%20is%20introduced%20in%20the%20diffusion%20process%20to%20explicitly%20estimate%20the%0A3D%20pose%20of%20objects%20and%20select%20beneficial%20data%20based%20on%20conditional%20density.%202%29%0AAn%20efficient%203D%20representation%20that%20is%20achieved%20by%20disentangling%20object%0Ageometry%20and%20color%20features%20with%20two%20separate%20conditional%20diffusion%20models%20that%0Aare%20optimized%20hierarchically.%20Given%20a%20prompt%20input%2C%20our%20model%20generates%0Ahigh-quality%2C%20high-resolution%2C%20realistic%2C%20and%20complex%203D%20objects%20with%20accurate%0Ageometric%20details%20in%20seconds.%20We%20achieve%20state-of-the-art%20performance%20in%20both%0Asingle-class%20generation%20and%20text-to-3D%20generation.%20We%20also%20demonstrate%20that%0ADIRECT-3D%20can%20serve%20as%20a%20useful%203D%20geometric%20prior%20of%20objects%2C%20for%20example%20to%0Aalleviate%20the%20well-known%20Janus%20problem%20in%202D-lifting%20methods%20such%20as%0ADreamFusion.%20The%20code%20and%20models%20are%20available%20for%20research%20purposes%20at%3A%0Ahttps%3A//github.com/qihao067/direct3d.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04322v1&entry.124074799=Read"},
{"title": "Omni6DPose: A Benchmark and Model for Universal 6D Object Pose\n  Estimation and Tracking", "author": "Jiyao Zhang and Weiyao Huang and Bo Peng and Mingdong Wu and Fei Hu and Zijian Chen and Bo Zhao and Hao Dong", "abstract": "  6D Object Pose Estimation is a crucial yet challenging task in computer\nvision, suffering from a significant lack of large-scale datasets. This\nscarcity impedes comprehensive evaluation of model performance, limiting\nresearch advancements. Furthermore, the restricted number of available\ninstances or categories curtails its applications. To address these issues,\nthis paper introduces Omni6DPose, a substantial dataset characterized by its\ndiversity in object categories, large scale, and variety in object materials.\nOmni6DPose is divided into three main components: ROPE (Real 6D Object Pose\nEstimation Dataset), which includes 332K images annotated with over 1.5M\nannotations across 581 instances in 149 categories; SOPE(Simulated 6D Object\nPose Estimation Dataset), consisting of 475K images created in a mixed reality\nsetting with depth simulation, annotated with over 5M annotations across 4162\ninstances in the same 149 categories; and the manually aligned real scanned\nobjects used in both ROPE and SOPE. Omni6DPose is inherently challenging due to\nthe substantial variations and ambiguities. To address this challenge, we\nintroduce GenPose++, an enhanced version of the SOTA category-level pose\nestimation framework, incorporating two pivotal improvements: Semantic-aware\nfeature extraction and Clustering-based aggregation. Moreover, we provide a\ncomprehensive benchmarking analysis to evaluate the performance of previous\nmethods on this large-scale dataset in the realms of 6D object pose estimation\nand pose tracking.\n", "link": "http://arxiv.org/abs/2406.04316v1", "date": "2024-06-06", "relevancy": 2.6908, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.557}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5384}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Omni6DPose%3A%20A%20Benchmark%20and%20Model%20for%20Universal%206D%20Object%20Pose%0A%20%20Estimation%20and%20Tracking&body=Title%3A%20Omni6DPose%3A%20A%20Benchmark%20and%20Model%20for%20Universal%206D%20Object%20Pose%0A%20%20Estimation%20and%20Tracking%0AAuthor%3A%20Jiyao%20Zhang%20and%20Weiyao%20Huang%20and%20Bo%20Peng%20and%20Mingdong%20Wu%20and%20Fei%20Hu%20and%20Zijian%20Chen%20and%20Bo%20Zhao%20and%20Hao%20Dong%0AAbstract%3A%20%20%206D%20Object%20Pose%20Estimation%20is%20a%20crucial%20yet%20challenging%20task%20in%20computer%0Avision%2C%20suffering%20from%20a%20significant%20lack%20of%20large-scale%20datasets.%20This%0Ascarcity%20impedes%20comprehensive%20evaluation%20of%20model%20performance%2C%20limiting%0Aresearch%20advancements.%20Furthermore%2C%20the%20restricted%20number%20of%20available%0Ainstances%20or%20categories%20curtails%20its%20applications.%20To%20address%20these%20issues%2C%0Athis%20paper%20introduces%20Omni6DPose%2C%20a%20substantial%20dataset%20characterized%20by%20its%0Adiversity%20in%20object%20categories%2C%20large%20scale%2C%20and%20variety%20in%20object%20materials.%0AOmni6DPose%20is%20divided%20into%20three%20main%20components%3A%20ROPE%20%28Real%206D%20Object%20Pose%0AEstimation%20Dataset%29%2C%20which%20includes%20332K%20images%20annotated%20with%20over%201.5M%0Aannotations%20across%20581%20instances%20in%20149%20categories%3B%20SOPE%28Simulated%206D%20Object%0APose%20Estimation%20Dataset%29%2C%20consisting%20of%20475K%20images%20created%20in%20a%20mixed%20reality%0Asetting%20with%20depth%20simulation%2C%20annotated%20with%20over%205M%20annotations%20across%204162%0Ainstances%20in%20the%20same%20149%20categories%3B%20and%20the%20manually%20aligned%20real%20scanned%0Aobjects%20used%20in%20both%20ROPE%20and%20SOPE.%20Omni6DPose%20is%20inherently%20challenging%20due%20to%0Athe%20substantial%20variations%20and%20ambiguities.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20GenPose%2B%2B%2C%20an%20enhanced%20version%20of%20the%20SOTA%20category-level%20pose%0Aestimation%20framework%2C%20incorporating%20two%20pivotal%20improvements%3A%20Semantic-aware%0Afeature%20extraction%20and%20Clustering-based%20aggregation.%20Moreover%2C%20we%20provide%20a%0Acomprehensive%20benchmarking%20analysis%20to%20evaluate%20the%20performance%20of%20previous%0Amethods%20on%20this%20large-scale%20dataset%20in%20the%20realms%20of%206D%20object%20pose%20estimation%0Aand%20pose%20tracking.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04316v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmni6DPose%253A%2520A%2520Benchmark%2520and%2520Model%2520for%2520Universal%25206D%2520Object%2520Pose%250A%2520%2520Estimation%2520and%2520Tracking%26entry.906535625%3DJiyao%2520Zhang%2520and%2520Weiyao%2520Huang%2520and%2520Bo%2520Peng%2520and%2520Mingdong%2520Wu%2520and%2520Fei%2520Hu%2520and%2520Zijian%2520Chen%2520and%2520Bo%2520Zhao%2520and%2520Hao%2520Dong%26entry.1292438233%3D%2520%25206D%2520Object%2520Pose%2520Estimation%2520is%2520a%2520crucial%2520yet%2520challenging%2520task%2520in%2520computer%250Avision%252C%2520suffering%2520from%2520a%2520significant%2520lack%2520of%2520large-scale%2520datasets.%2520This%250Ascarcity%2520impedes%2520comprehensive%2520evaluation%2520of%2520model%2520performance%252C%2520limiting%250Aresearch%2520advancements.%2520Furthermore%252C%2520the%2520restricted%2520number%2520of%2520available%250Ainstances%2520or%2520categories%2520curtails%2520its%2520applications.%2520To%2520address%2520these%2520issues%252C%250Athis%2520paper%2520introduces%2520Omni6DPose%252C%2520a%2520substantial%2520dataset%2520characterized%2520by%2520its%250Adiversity%2520in%2520object%2520categories%252C%2520large%2520scale%252C%2520and%2520variety%2520in%2520object%2520materials.%250AOmni6DPose%2520is%2520divided%2520into%2520three%2520main%2520components%253A%2520ROPE%2520%2528Real%25206D%2520Object%2520Pose%250AEstimation%2520Dataset%2529%252C%2520which%2520includes%2520332K%2520images%2520annotated%2520with%2520over%25201.5M%250Aannotations%2520across%2520581%2520instances%2520in%2520149%2520categories%253B%2520SOPE%2528Simulated%25206D%2520Object%250APose%2520Estimation%2520Dataset%2529%252C%2520consisting%2520of%2520475K%2520images%2520created%2520in%2520a%2520mixed%2520reality%250Asetting%2520with%2520depth%2520simulation%252C%2520annotated%2520with%2520over%25205M%2520annotations%2520across%25204162%250Ainstances%2520in%2520the%2520same%2520149%2520categories%253B%2520and%2520the%2520manually%2520aligned%2520real%2520scanned%250Aobjects%2520used%2520in%2520both%2520ROPE%2520and%2520SOPE.%2520Omni6DPose%2520is%2520inherently%2520challenging%2520due%2520to%250Athe%2520substantial%2520variations%2520and%2520ambiguities.%2520To%2520address%2520this%2520challenge%252C%2520we%250Aintroduce%2520GenPose%252B%252B%252C%2520an%2520enhanced%2520version%2520of%2520the%2520SOTA%2520category-level%2520pose%250Aestimation%2520framework%252C%2520incorporating%2520two%2520pivotal%2520improvements%253A%2520Semantic-aware%250Afeature%2520extraction%2520and%2520Clustering-based%2520aggregation.%2520Moreover%252C%2520we%2520provide%2520a%250Acomprehensive%2520benchmarking%2520analysis%2520to%2520evaluate%2520the%2520performance%2520of%2520previous%250Amethods%2520on%2520this%2520large-scale%2520dataset%2520in%2520the%2520realms%2520of%25206D%2520object%2520pose%2520estimation%250Aand%2520pose%2520tracking.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04316v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Omni6DPose%3A%20A%20Benchmark%20and%20Model%20for%20Universal%206D%20Object%20Pose%0A%20%20Estimation%20and%20Tracking&entry.906535625=Jiyao%20Zhang%20and%20Weiyao%20Huang%20and%20Bo%20Peng%20and%20Mingdong%20Wu%20and%20Fei%20Hu%20and%20Zijian%20Chen%20and%20Bo%20Zhao%20and%20Hao%20Dong&entry.1292438233=%20%206D%20Object%20Pose%20Estimation%20is%20a%20crucial%20yet%20challenging%20task%20in%20computer%0Avision%2C%20suffering%20from%20a%20significant%20lack%20of%20large-scale%20datasets.%20This%0Ascarcity%20impedes%20comprehensive%20evaluation%20of%20model%20performance%2C%20limiting%0Aresearch%20advancements.%20Furthermore%2C%20the%20restricted%20number%20of%20available%0Ainstances%20or%20categories%20curtails%20its%20applications.%20To%20address%20these%20issues%2C%0Athis%20paper%20introduces%20Omni6DPose%2C%20a%20substantial%20dataset%20characterized%20by%20its%0Adiversity%20in%20object%20categories%2C%20large%20scale%2C%20and%20variety%20in%20object%20materials.%0AOmni6DPose%20is%20divided%20into%20three%20main%20components%3A%20ROPE%20%28Real%206D%20Object%20Pose%0AEstimation%20Dataset%29%2C%20which%20includes%20332K%20images%20annotated%20with%20over%201.5M%0Aannotations%20across%20581%20instances%20in%20149%20categories%3B%20SOPE%28Simulated%206D%20Object%0APose%20Estimation%20Dataset%29%2C%20consisting%20of%20475K%20images%20created%20in%20a%20mixed%20reality%0Asetting%20with%20depth%20simulation%2C%20annotated%20with%20over%205M%20annotations%20across%204162%0Ainstances%20in%20the%20same%20149%20categories%3B%20and%20the%20manually%20aligned%20real%20scanned%0Aobjects%20used%20in%20both%20ROPE%20and%20SOPE.%20Omni6DPose%20is%20inherently%20challenging%20due%20to%0Athe%20substantial%20variations%20and%20ambiguities.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20GenPose%2B%2B%2C%20an%20enhanced%20version%20of%20the%20SOTA%20category-level%20pose%0Aestimation%20framework%2C%20incorporating%20two%20pivotal%20improvements%3A%20Semantic-aware%0Afeature%20extraction%20and%20Clustering-based%20aggregation.%20Moreover%2C%20we%20provide%20a%0Acomprehensive%20benchmarking%20analysis%20to%20evaluate%20the%20performance%20of%20previous%0Amethods%20on%20this%20large-scale%20dataset%20in%20the%20realms%20of%206D%20object%20pose%20estimation%0Aand%20pose%20tracking.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04316v1&entry.124074799=Read"},
{"title": "Cascade-CLIP: Cascaded Vision-Language Embeddings Alignment for\n  Zero-Shot Semantic Segmentation", "author": "Yunheng Li and ZhongYu Li and Quansheng Zeng and Qibin Hou and Ming-Ming Cheng", "abstract": "  Pre-trained vision-language models, e.g., CLIP, have been successfully\napplied to zero-shot semantic segmentation. Existing CLIP-based approaches\nprimarily utilize visual features from the last layer to align with text\nembeddings, while they neglect the crucial information in intermediate layers\nthat contain rich object details. However, we find that directly aggregating\nthe multi-level visual features weakens the zero-shot ability for novel\nclasses. The large differences between the visual features from different\nlayers make these features hard to align well with the text embeddings. We\nresolve this problem by introducing a series of independent decoders to align\nthe multi-level visual features with the text embeddings in a cascaded way,\nforming a novel but simple framework named Cascade-CLIP. Our Cascade-CLIP is\nflexible and can be easily applied to existing zero-shot semantic segmentation\nmethods. Experimental results show that our simple Cascade-CLIP achieves\nsuperior zero-shot performance on segmentation benchmarks, like COCO-Stuff,\nPascal-VOC, and Pascal-Context. Our code is available at:\nhttps://github.com/HVision-NKU/Cascade-CLIP\n", "link": "http://arxiv.org/abs/2406.00670v2", "date": "2024-06-06", "relevancy": 2.6734, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5994}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5045}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5002}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cascade-CLIP%3A%20Cascaded%20Vision-Language%20Embeddings%20Alignment%20for%0A%20%20Zero-Shot%20Semantic%20Segmentation&body=Title%3A%20Cascade-CLIP%3A%20Cascaded%20Vision-Language%20Embeddings%20Alignment%20for%0A%20%20Zero-Shot%20Semantic%20Segmentation%0AAuthor%3A%20Yunheng%20Li%20and%20ZhongYu%20Li%20and%20Quansheng%20Zeng%20and%20Qibin%20Hou%20and%20Ming-Ming%20Cheng%0AAbstract%3A%20%20%20Pre-trained%20vision-language%20models%2C%20e.g.%2C%20CLIP%2C%20have%20been%20successfully%0Aapplied%20to%20zero-shot%20semantic%20segmentation.%20Existing%20CLIP-based%20approaches%0Aprimarily%20utilize%20visual%20features%20from%20the%20last%20layer%20to%20align%20with%20text%0Aembeddings%2C%20while%20they%20neglect%20the%20crucial%20information%20in%20intermediate%20layers%0Athat%20contain%20rich%20object%20details.%20However%2C%20we%20find%20that%20directly%20aggregating%0Athe%20multi-level%20visual%20features%20weakens%20the%20zero-shot%20ability%20for%20novel%0Aclasses.%20The%20large%20differences%20between%20the%20visual%20features%20from%20different%0Alayers%20make%20these%20features%20hard%20to%20align%20well%20with%20the%20text%20embeddings.%20We%0Aresolve%20this%20problem%20by%20introducing%20a%20series%20of%20independent%20decoders%20to%20align%0Athe%20multi-level%20visual%20features%20with%20the%20text%20embeddings%20in%20a%20cascaded%20way%2C%0Aforming%20a%20novel%20but%20simple%20framework%20named%20Cascade-CLIP.%20Our%20Cascade-CLIP%20is%0Aflexible%20and%20can%20be%20easily%20applied%20to%20existing%20zero-shot%20semantic%20segmentation%0Amethods.%20Experimental%20results%20show%20that%20our%20simple%20Cascade-CLIP%20achieves%0Asuperior%20zero-shot%20performance%20on%20segmentation%20benchmarks%2C%20like%20COCO-Stuff%2C%0APascal-VOC%2C%20and%20Pascal-Context.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/HVision-NKU/Cascade-CLIP%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00670v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCascade-CLIP%253A%2520Cascaded%2520Vision-Language%2520Embeddings%2520Alignment%2520for%250A%2520%2520Zero-Shot%2520Semantic%2520Segmentation%26entry.906535625%3DYunheng%2520Li%2520and%2520ZhongYu%2520Li%2520and%2520Quansheng%2520Zeng%2520and%2520Qibin%2520Hou%2520and%2520Ming-Ming%2520Cheng%26entry.1292438233%3D%2520%2520Pre-trained%2520vision-language%2520models%252C%2520e.g.%252C%2520CLIP%252C%2520have%2520been%2520successfully%250Aapplied%2520to%2520zero-shot%2520semantic%2520segmentation.%2520Existing%2520CLIP-based%2520approaches%250Aprimarily%2520utilize%2520visual%2520features%2520from%2520the%2520last%2520layer%2520to%2520align%2520with%2520text%250Aembeddings%252C%2520while%2520they%2520neglect%2520the%2520crucial%2520information%2520in%2520intermediate%2520layers%250Athat%2520contain%2520rich%2520object%2520details.%2520However%252C%2520we%2520find%2520that%2520directly%2520aggregating%250Athe%2520multi-level%2520visual%2520features%2520weakens%2520the%2520zero-shot%2520ability%2520for%2520novel%250Aclasses.%2520The%2520large%2520differences%2520between%2520the%2520visual%2520features%2520from%2520different%250Alayers%2520make%2520these%2520features%2520hard%2520to%2520align%2520well%2520with%2520the%2520text%2520embeddings.%2520We%250Aresolve%2520this%2520problem%2520by%2520introducing%2520a%2520series%2520of%2520independent%2520decoders%2520to%2520align%250Athe%2520multi-level%2520visual%2520features%2520with%2520the%2520text%2520embeddings%2520in%2520a%2520cascaded%2520way%252C%250Aforming%2520a%2520novel%2520but%2520simple%2520framework%2520named%2520Cascade-CLIP.%2520Our%2520Cascade-CLIP%2520is%250Aflexible%2520and%2520can%2520be%2520easily%2520applied%2520to%2520existing%2520zero-shot%2520semantic%2520segmentation%250Amethods.%2520Experimental%2520results%2520show%2520that%2520our%2520simple%2520Cascade-CLIP%2520achieves%250Asuperior%2520zero-shot%2520performance%2520on%2520segmentation%2520benchmarks%252C%2520like%2520COCO-Stuff%252C%250APascal-VOC%252C%2520and%2520Pascal-Context.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/HVision-NKU/Cascade-CLIP%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00670v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cascade-CLIP%3A%20Cascaded%20Vision-Language%20Embeddings%20Alignment%20for%0A%20%20Zero-Shot%20Semantic%20Segmentation&entry.906535625=Yunheng%20Li%20and%20ZhongYu%20Li%20and%20Quansheng%20Zeng%20and%20Qibin%20Hou%20and%20Ming-Ming%20Cheng&entry.1292438233=%20%20Pre-trained%20vision-language%20models%2C%20e.g.%2C%20CLIP%2C%20have%20been%20successfully%0Aapplied%20to%20zero-shot%20semantic%20segmentation.%20Existing%20CLIP-based%20approaches%0Aprimarily%20utilize%20visual%20features%20from%20the%20last%20layer%20to%20align%20with%20text%0Aembeddings%2C%20while%20they%20neglect%20the%20crucial%20information%20in%20intermediate%20layers%0Athat%20contain%20rich%20object%20details.%20However%2C%20we%20find%20that%20directly%20aggregating%0Athe%20multi-level%20visual%20features%20weakens%20the%20zero-shot%20ability%20for%20novel%0Aclasses.%20The%20large%20differences%20between%20the%20visual%20features%20from%20different%0Alayers%20make%20these%20features%20hard%20to%20align%20well%20with%20the%20text%20embeddings.%20We%0Aresolve%20this%20problem%20by%20introducing%20a%20series%20of%20independent%20decoders%20to%20align%0Athe%20multi-level%20visual%20features%20with%20the%20text%20embeddings%20in%20a%20cascaded%20way%2C%0Aforming%20a%20novel%20but%20simple%20framework%20named%20Cascade-CLIP.%20Our%20Cascade-CLIP%20is%0Aflexible%20and%20can%20be%20easily%20applied%20to%20existing%20zero-shot%20semantic%20segmentation%0Amethods.%20Experimental%20results%20show%20that%20our%20simple%20Cascade-CLIP%20achieves%0Asuperior%20zero-shot%20performance%20on%20segmentation%20benchmarks%2C%20like%20COCO-Stuff%2C%0APascal-VOC%2C%20and%20Pascal-Context.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/HVision-NKU/Cascade-CLIP%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00670v2&entry.124074799=Read"},
{"title": "Neural Surface Reconstruction from Sparse Views Using Epipolar Geometry", "author": "Kaichen Zhou", "abstract": "  This paper addresses the challenge of reconstructing surfaces from sparse\nview inputs, where ambiguity and occlusions due to missing information pose\nsignificant hurdles. We present a novel approach, named EpiS, that incorporates\nEpipolar information into the reconstruction process. Existing methods in\nsparse-view neural surface learning have mainly focused on mean and variance\nconsiderations using cost volumes for feature extraction. In contrast, our\nmethod aggregates coarse information from the cost volume into Epipolar\nfeatures extracted from multiple source views, enabling the generation of\nfine-grained Signal Distance Function (SDF)-aware features. Additionally, we\nemploy an attention mechanism along the line dimension to facilitate feature\nfusion based on the SDF feature. Furthermore, to address the information gaps\nin sparse conditions, we integrate depth information from monocular depth\nestimation using global and local regularization techniques. The global\nregularization utilizes a triplet loss function, while the local regularization\nemploys a derivative loss function. Extensive experiments demonstrate that our\napproach outperforms state-of-the-art methods, especially in cases with sparse\nand generalizable conditions.\n", "link": "http://arxiv.org/abs/2406.04301v1", "date": "2024-06-06", "relevancy": 2.6566, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5428}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5333}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Surface%20Reconstruction%20from%20Sparse%20Views%20Using%20Epipolar%20Geometry&body=Title%3A%20Neural%20Surface%20Reconstruction%20from%20Sparse%20Views%20Using%20Epipolar%20Geometry%0AAuthor%3A%20Kaichen%20Zhou%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20challenge%20of%20reconstructing%20surfaces%20from%20sparse%0Aview%20inputs%2C%20where%20ambiguity%20and%20occlusions%20due%20to%20missing%20information%20pose%0Asignificant%20hurdles.%20We%20present%20a%20novel%20approach%2C%20named%20EpiS%2C%20that%20incorporates%0AEpipolar%20information%20into%20the%20reconstruction%20process.%20Existing%20methods%20in%0Asparse-view%20neural%20surface%20learning%20have%20mainly%20focused%20on%20mean%20and%20variance%0Aconsiderations%20using%20cost%20volumes%20for%20feature%20extraction.%20In%20contrast%2C%20our%0Amethod%20aggregates%20coarse%20information%20from%20the%20cost%20volume%20into%20Epipolar%0Afeatures%20extracted%20from%20multiple%20source%20views%2C%20enabling%20the%20generation%20of%0Afine-grained%20Signal%20Distance%20Function%20%28SDF%29-aware%20features.%20Additionally%2C%20we%0Aemploy%20an%20attention%20mechanism%20along%20the%20line%20dimension%20to%20facilitate%20feature%0Afusion%20based%20on%20the%20SDF%20feature.%20Furthermore%2C%20to%20address%20the%20information%20gaps%0Ain%20sparse%20conditions%2C%20we%20integrate%20depth%20information%20from%20monocular%20depth%0Aestimation%20using%20global%20and%20local%20regularization%20techniques.%20The%20global%0Aregularization%20utilizes%20a%20triplet%20loss%20function%2C%20while%20the%20local%20regularization%0Aemploys%20a%20derivative%20loss%20function.%20Extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20outperforms%20state-of-the-art%20methods%2C%20especially%20in%20cases%20with%20sparse%0Aand%20generalizable%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04301v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Surface%2520Reconstruction%2520from%2520Sparse%2520Views%2520Using%2520Epipolar%2520Geometry%26entry.906535625%3DKaichen%2520Zhou%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520challenge%2520of%2520reconstructing%2520surfaces%2520from%2520sparse%250Aview%2520inputs%252C%2520where%2520ambiguity%2520and%2520occlusions%2520due%2520to%2520missing%2520information%2520pose%250Asignificant%2520hurdles.%2520We%2520present%2520a%2520novel%2520approach%252C%2520named%2520EpiS%252C%2520that%2520incorporates%250AEpipolar%2520information%2520into%2520the%2520reconstruction%2520process.%2520Existing%2520methods%2520in%250Asparse-view%2520neural%2520surface%2520learning%2520have%2520mainly%2520focused%2520on%2520mean%2520and%2520variance%250Aconsiderations%2520using%2520cost%2520volumes%2520for%2520feature%2520extraction.%2520In%2520contrast%252C%2520our%250Amethod%2520aggregates%2520coarse%2520information%2520from%2520the%2520cost%2520volume%2520into%2520Epipolar%250Afeatures%2520extracted%2520from%2520multiple%2520source%2520views%252C%2520enabling%2520the%2520generation%2520of%250Afine-grained%2520Signal%2520Distance%2520Function%2520%2528SDF%2529-aware%2520features.%2520Additionally%252C%2520we%250Aemploy%2520an%2520attention%2520mechanism%2520along%2520the%2520line%2520dimension%2520to%2520facilitate%2520feature%250Afusion%2520based%2520on%2520the%2520SDF%2520feature.%2520Furthermore%252C%2520to%2520address%2520the%2520information%2520gaps%250Ain%2520sparse%2520conditions%252C%2520we%2520integrate%2520depth%2520information%2520from%2520monocular%2520depth%250Aestimation%2520using%2520global%2520and%2520local%2520regularization%2520techniques.%2520The%2520global%250Aregularization%2520utilizes%2520a%2520triplet%2520loss%2520function%252C%2520while%2520the%2520local%2520regularization%250Aemploys%2520a%2520derivative%2520loss%2520function.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Aapproach%2520outperforms%2520state-of-the-art%2520methods%252C%2520especially%2520in%2520cases%2520with%2520sparse%250Aand%2520generalizable%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04301v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Surface%20Reconstruction%20from%20Sparse%20Views%20Using%20Epipolar%20Geometry&entry.906535625=Kaichen%20Zhou&entry.1292438233=%20%20This%20paper%20addresses%20the%20challenge%20of%20reconstructing%20surfaces%20from%20sparse%0Aview%20inputs%2C%20where%20ambiguity%20and%20occlusions%20due%20to%20missing%20information%20pose%0Asignificant%20hurdles.%20We%20present%20a%20novel%20approach%2C%20named%20EpiS%2C%20that%20incorporates%0AEpipolar%20information%20into%20the%20reconstruction%20process.%20Existing%20methods%20in%0Asparse-view%20neural%20surface%20learning%20have%20mainly%20focused%20on%20mean%20and%20variance%0Aconsiderations%20using%20cost%20volumes%20for%20feature%20extraction.%20In%20contrast%2C%20our%0Amethod%20aggregates%20coarse%20information%20from%20the%20cost%20volume%20into%20Epipolar%0Afeatures%20extracted%20from%20multiple%20source%20views%2C%20enabling%20the%20generation%20of%0Afine-grained%20Signal%20Distance%20Function%20%28SDF%29-aware%20features.%20Additionally%2C%20we%0Aemploy%20an%20attention%20mechanism%20along%20the%20line%20dimension%20to%20facilitate%20feature%0Afusion%20based%20on%20the%20SDF%20feature.%20Furthermore%2C%20to%20address%20the%20information%20gaps%0Ain%20sparse%20conditions%2C%20we%20integrate%20depth%20information%20from%20monocular%20depth%0Aestimation%20using%20global%20and%20local%20regularization%20techniques.%20The%20global%0Aregularization%20utilizes%20a%20triplet%20loss%20function%2C%20while%20the%20local%20regularization%0Aemploys%20a%20derivative%20loss%20function.%20Extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20outperforms%20state-of-the-art%20methods%2C%20especially%20in%20cases%20with%20sparse%0Aand%20generalizable%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04301v1&entry.124074799=Read"},
{"title": "Batch-in-Batch: a new adversarial training framework for initial\n  perturbation and sample selection", "author": "Yinting Wu and Pai Peng and Bo Cai and Le Li and  .", "abstract": "  Adversarial training methods commonly generate independent initial\nperturbation for adversarial samples from a simple uniform distribution, and\nobtain the training batch for the classifier without selection. In this work,\nwe propose a simple yet effective training framework called Batch-in-Batch (BB)\nto enhance models robustness. It involves specifically a joint construction of\ninitial values that could simultaneously generates $m$ sets of perturbations\nfrom the original batch set to provide more diversity for adversarial samples;\nand also includes various sample selection strategies that enable the trained\nmodels to have smoother losses and avoid overconfident outputs. Through\nextensive experiments on three benchmark datasets (CIFAR-10, SVHN, CIFAR-100)\nwith two networks (PreActResNet18 and WideResNet28-10) that are used in both\nthe single-step (Noise-Fast Gradient Sign Method, N-FGSM) and multi-step\n(Projected Gradient Descent, PGD-10) adversarial training, we show that models\ntrained within the BB framework consistently have higher adversarial accuracy\nacross various adversarial settings, notably achieving over a 13% improvement\non the SVHN dataset with an attack radius of 8/255 compared to the N-FGSM\nbaseline model. Furthermore, experimental analysis of the efficiency of both\nthe proposed initial perturbation method and sample selection strategies\nvalidates our insights. Finally, we show that our framework is cost-effective\nin terms of computational resources, even with a relatively large value of $m$.\n", "link": "http://arxiv.org/abs/2406.04070v1", "date": "2024-06-06", "relevancy": 2.6484, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5699}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5316}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Batch-in-Batch%3A%20a%20new%20adversarial%20training%20framework%20for%20initial%0A%20%20perturbation%20and%20sample%20selection&body=Title%3A%20Batch-in-Batch%3A%20a%20new%20adversarial%20training%20framework%20for%20initial%0A%20%20perturbation%20and%20sample%20selection%0AAuthor%3A%20Yinting%20Wu%20and%20Pai%20Peng%20and%20Bo%20Cai%20and%20Le%20Li%20and%20%20.%0AAbstract%3A%20%20%20Adversarial%20training%20methods%20commonly%20generate%20independent%20initial%0Aperturbation%20for%20adversarial%20samples%20from%20a%20simple%20uniform%20distribution%2C%20and%0Aobtain%20the%20training%20batch%20for%20the%20classifier%20without%20selection.%20In%20this%20work%2C%0Awe%20propose%20a%20simple%20yet%20effective%20training%20framework%20called%20Batch-in-Batch%20%28BB%29%0Ato%20enhance%20models%20robustness.%20It%20involves%20specifically%20a%20joint%20construction%20of%0Ainitial%20values%20that%20could%20simultaneously%20generates%20%24m%24%20sets%20of%20perturbations%0Afrom%20the%20original%20batch%20set%20to%20provide%20more%20diversity%20for%20adversarial%20samples%3B%0Aand%20also%20includes%20various%20sample%20selection%20strategies%20that%20enable%20the%20trained%0Amodels%20to%20have%20smoother%20losses%20and%20avoid%20overconfident%20outputs.%20Through%0Aextensive%20experiments%20on%20three%20benchmark%20datasets%20%28CIFAR-10%2C%20SVHN%2C%20CIFAR-100%29%0Awith%20two%20networks%20%28PreActResNet18%20and%20WideResNet28-10%29%20that%20are%20used%20in%20both%0Athe%20single-step%20%28Noise-Fast%20Gradient%20Sign%20Method%2C%20N-FGSM%29%20and%20multi-step%0A%28Projected%20Gradient%20Descent%2C%20PGD-10%29%20adversarial%20training%2C%20we%20show%20that%20models%0Atrained%20within%20the%20BB%20framework%20consistently%20have%20higher%20adversarial%20accuracy%0Aacross%20various%20adversarial%20settings%2C%20notably%20achieving%20over%20a%2013%25%20improvement%0Aon%20the%20SVHN%20dataset%20with%20an%20attack%20radius%20of%208/255%20compared%20to%20the%20N-FGSM%0Abaseline%20model.%20Furthermore%2C%20experimental%20analysis%20of%20the%20efficiency%20of%20both%0Athe%20proposed%20initial%20perturbation%20method%20and%20sample%20selection%20strategies%0Avalidates%20our%20insights.%20Finally%2C%20we%20show%20that%20our%20framework%20is%20cost-effective%0Ain%20terms%20of%20computational%20resources%2C%20even%20with%20a%20relatively%20large%20value%20of%20%24m%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04070v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBatch-in-Batch%253A%2520a%2520new%2520adversarial%2520training%2520framework%2520for%2520initial%250A%2520%2520perturbation%2520and%2520sample%2520selection%26entry.906535625%3DYinting%2520Wu%2520and%2520Pai%2520Peng%2520and%2520Bo%2520Cai%2520and%2520Le%2520Li%2520and%2520%2520.%26entry.1292438233%3D%2520%2520Adversarial%2520training%2520methods%2520commonly%2520generate%2520independent%2520initial%250Aperturbation%2520for%2520adversarial%2520samples%2520from%2520a%2520simple%2520uniform%2520distribution%252C%2520and%250Aobtain%2520the%2520training%2520batch%2520for%2520the%2520classifier%2520without%2520selection.%2520In%2520this%2520work%252C%250Awe%2520propose%2520a%2520simple%2520yet%2520effective%2520training%2520framework%2520called%2520Batch-in-Batch%2520%2528BB%2529%250Ato%2520enhance%2520models%2520robustness.%2520It%2520involves%2520specifically%2520a%2520joint%2520construction%2520of%250Ainitial%2520values%2520that%2520could%2520simultaneously%2520generates%2520%2524m%2524%2520sets%2520of%2520perturbations%250Afrom%2520the%2520original%2520batch%2520set%2520to%2520provide%2520more%2520diversity%2520for%2520adversarial%2520samples%253B%250Aand%2520also%2520includes%2520various%2520sample%2520selection%2520strategies%2520that%2520enable%2520the%2520trained%250Amodels%2520to%2520have%2520smoother%2520losses%2520and%2520avoid%2520overconfident%2520outputs.%2520Through%250Aextensive%2520experiments%2520on%2520three%2520benchmark%2520datasets%2520%2528CIFAR-10%252C%2520SVHN%252C%2520CIFAR-100%2529%250Awith%2520two%2520networks%2520%2528PreActResNet18%2520and%2520WideResNet28-10%2529%2520that%2520are%2520used%2520in%2520both%250Athe%2520single-step%2520%2528Noise-Fast%2520Gradient%2520Sign%2520Method%252C%2520N-FGSM%2529%2520and%2520multi-step%250A%2528Projected%2520Gradient%2520Descent%252C%2520PGD-10%2529%2520adversarial%2520training%252C%2520we%2520show%2520that%2520models%250Atrained%2520within%2520the%2520BB%2520framework%2520consistently%2520have%2520higher%2520adversarial%2520accuracy%250Aacross%2520various%2520adversarial%2520settings%252C%2520notably%2520achieving%2520over%2520a%252013%2525%2520improvement%250Aon%2520the%2520SVHN%2520dataset%2520with%2520an%2520attack%2520radius%2520of%25208/255%2520compared%2520to%2520the%2520N-FGSM%250Abaseline%2520model.%2520Furthermore%252C%2520experimental%2520analysis%2520of%2520the%2520efficiency%2520of%2520both%250Athe%2520proposed%2520initial%2520perturbation%2520method%2520and%2520sample%2520selection%2520strategies%250Avalidates%2520our%2520insights.%2520Finally%252C%2520we%2520show%2520that%2520our%2520framework%2520is%2520cost-effective%250Ain%2520terms%2520of%2520computational%2520resources%252C%2520even%2520with%2520a%2520relatively%2520large%2520value%2520of%2520%2524m%2524.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04070v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Batch-in-Batch%3A%20a%20new%20adversarial%20training%20framework%20for%20initial%0A%20%20perturbation%20and%20sample%20selection&entry.906535625=Yinting%20Wu%20and%20Pai%20Peng%20and%20Bo%20Cai%20and%20Le%20Li%20and%20%20.&entry.1292438233=%20%20Adversarial%20training%20methods%20commonly%20generate%20independent%20initial%0Aperturbation%20for%20adversarial%20samples%20from%20a%20simple%20uniform%20distribution%2C%20and%0Aobtain%20the%20training%20batch%20for%20the%20classifier%20without%20selection.%20In%20this%20work%2C%0Awe%20propose%20a%20simple%20yet%20effective%20training%20framework%20called%20Batch-in-Batch%20%28BB%29%0Ato%20enhance%20models%20robustness.%20It%20involves%20specifically%20a%20joint%20construction%20of%0Ainitial%20values%20that%20could%20simultaneously%20generates%20%24m%24%20sets%20of%20perturbations%0Afrom%20the%20original%20batch%20set%20to%20provide%20more%20diversity%20for%20adversarial%20samples%3B%0Aand%20also%20includes%20various%20sample%20selection%20strategies%20that%20enable%20the%20trained%0Amodels%20to%20have%20smoother%20losses%20and%20avoid%20overconfident%20outputs.%20Through%0Aextensive%20experiments%20on%20three%20benchmark%20datasets%20%28CIFAR-10%2C%20SVHN%2C%20CIFAR-100%29%0Awith%20two%20networks%20%28PreActResNet18%20and%20WideResNet28-10%29%20that%20are%20used%20in%20both%0Athe%20single-step%20%28Noise-Fast%20Gradient%20Sign%20Method%2C%20N-FGSM%29%20and%20multi-step%0A%28Projected%20Gradient%20Descent%2C%20PGD-10%29%20adversarial%20training%2C%20we%20show%20that%20models%0Atrained%20within%20the%20BB%20framework%20consistently%20have%20higher%20adversarial%20accuracy%0Aacross%20various%20adversarial%20settings%2C%20notably%20achieving%20over%20a%2013%25%20improvement%0Aon%20the%20SVHN%20dataset%20with%20an%20attack%20radius%20of%208/255%20compared%20to%20the%20N-FGSM%0Abaseline%20model.%20Furthermore%2C%20experimental%20analysis%20of%20the%20efficiency%20of%20both%0Athe%20proposed%20initial%20perturbation%20method%20and%20sample%20selection%20strategies%0Avalidates%20our%20insights.%20Finally%2C%20we%20show%20that%20our%20framework%20is%20cost-effective%0Ain%20terms%20of%20computational%20resources%2C%20even%20with%20a%20relatively%20large%20value%20of%20%24m%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04070v1&entry.124074799=Read"},
{"title": "SchurVINS: Schur Complement-Based Lightweight Visual Inertial Navigation\n  System", "author": "Yunfei Fan and Tianyu Zhao and Guidong Wang", "abstract": "  Accuracy and computational efficiency are the most important metrics to\nVisual Inertial Navigation System (VINS). The existing VINS algorithms with\neither high accuracy or low computational complexity, are difficult to provide\nthe high precision localization in resource-constrained devices. To this end,\nwe propose a novel filter-based VINS framework named SchurVINS, which could\nguarantee both high accuracy by building a complete residual model and low\ncomputational complexity with Schur complement. Technically, we first formulate\nthe full residual model where Gradient, Hessian and observation covariance are\nexplicitly modeled. Then Schur complement is employed to decompose the full\nmodel into ego-motion residual model and landmark residual model. Finally,\nExtended Kalman Filter (EKF) update is implemented in these two models with\nhigh efficiency. Experiments on EuRoC and TUM-VI datasets show that our method\nnotably outperforms state-of-the-art (SOTA) methods in both accuracy and\ncomputational complexity. The experimental code of SchurVINS is available at\nhttps://github.com/bytedance/SchurVINS.\n", "link": "http://arxiv.org/abs/2312.01616v5", "date": "2024-06-06", "relevancy": 2.6236, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5401}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.525}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SchurVINS%3A%20Schur%20Complement-Based%20Lightweight%20Visual%20Inertial%20Navigation%0A%20%20System&body=Title%3A%20SchurVINS%3A%20Schur%20Complement-Based%20Lightweight%20Visual%20Inertial%20Navigation%0A%20%20System%0AAuthor%3A%20Yunfei%20Fan%20and%20Tianyu%20Zhao%20and%20Guidong%20Wang%0AAbstract%3A%20%20%20Accuracy%20and%20computational%20efficiency%20are%20the%20most%20important%20metrics%20to%0AVisual%20Inertial%20Navigation%20System%20%28VINS%29.%20The%20existing%20VINS%20algorithms%20with%0Aeither%20high%20accuracy%20or%20low%20computational%20complexity%2C%20are%20difficult%20to%20provide%0Athe%20high%20precision%20localization%20in%20resource-constrained%20devices.%20To%20this%20end%2C%0Awe%20propose%20a%20novel%20filter-based%20VINS%20framework%20named%20SchurVINS%2C%20which%20could%0Aguarantee%20both%20high%20accuracy%20by%20building%20a%20complete%20residual%20model%20and%20low%0Acomputational%20complexity%20with%20Schur%20complement.%20Technically%2C%20we%20first%20formulate%0Athe%20full%20residual%20model%20where%20Gradient%2C%20Hessian%20and%20observation%20covariance%20are%0Aexplicitly%20modeled.%20Then%20Schur%20complement%20is%20employed%20to%20decompose%20the%20full%0Amodel%20into%20ego-motion%20residual%20model%20and%20landmark%20residual%20model.%20Finally%2C%0AExtended%20Kalman%20Filter%20%28EKF%29%20update%20is%20implemented%20in%20these%20two%20models%20with%0Ahigh%20efficiency.%20Experiments%20on%20EuRoC%20and%20TUM-VI%20datasets%20show%20that%20our%20method%0Anotably%20outperforms%20state-of-the-art%20%28SOTA%29%20methods%20in%20both%20accuracy%20and%0Acomputational%20complexity.%20The%20experimental%20code%20of%20SchurVINS%20is%20available%20at%0Ahttps%3A//github.com/bytedance/SchurVINS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.01616v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSchurVINS%253A%2520Schur%2520Complement-Based%2520Lightweight%2520Visual%2520Inertial%2520Navigation%250A%2520%2520System%26entry.906535625%3DYunfei%2520Fan%2520and%2520Tianyu%2520Zhao%2520and%2520Guidong%2520Wang%26entry.1292438233%3D%2520%2520Accuracy%2520and%2520computational%2520efficiency%2520are%2520the%2520most%2520important%2520metrics%2520to%250AVisual%2520Inertial%2520Navigation%2520System%2520%2528VINS%2529.%2520The%2520existing%2520VINS%2520algorithms%2520with%250Aeither%2520high%2520accuracy%2520or%2520low%2520computational%2520complexity%252C%2520are%2520difficult%2520to%2520provide%250Athe%2520high%2520precision%2520localization%2520in%2520resource-constrained%2520devices.%2520To%2520this%2520end%252C%250Awe%2520propose%2520a%2520novel%2520filter-based%2520VINS%2520framework%2520named%2520SchurVINS%252C%2520which%2520could%250Aguarantee%2520both%2520high%2520accuracy%2520by%2520building%2520a%2520complete%2520residual%2520model%2520and%2520low%250Acomputational%2520complexity%2520with%2520Schur%2520complement.%2520Technically%252C%2520we%2520first%2520formulate%250Athe%2520full%2520residual%2520model%2520where%2520Gradient%252C%2520Hessian%2520and%2520observation%2520covariance%2520are%250Aexplicitly%2520modeled.%2520Then%2520Schur%2520complement%2520is%2520employed%2520to%2520decompose%2520the%2520full%250Amodel%2520into%2520ego-motion%2520residual%2520model%2520and%2520landmark%2520residual%2520model.%2520Finally%252C%250AExtended%2520Kalman%2520Filter%2520%2528EKF%2529%2520update%2520is%2520implemented%2520in%2520these%2520two%2520models%2520with%250Ahigh%2520efficiency.%2520Experiments%2520on%2520EuRoC%2520and%2520TUM-VI%2520datasets%2520show%2520that%2520our%2520method%250Anotably%2520outperforms%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520in%2520both%2520accuracy%2520and%250Acomputational%2520complexity.%2520The%2520experimental%2520code%2520of%2520SchurVINS%2520is%2520available%2520at%250Ahttps%253A//github.com/bytedance/SchurVINS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.01616v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SchurVINS%3A%20Schur%20Complement-Based%20Lightweight%20Visual%20Inertial%20Navigation%0A%20%20System&entry.906535625=Yunfei%20Fan%20and%20Tianyu%20Zhao%20and%20Guidong%20Wang&entry.1292438233=%20%20Accuracy%20and%20computational%20efficiency%20are%20the%20most%20important%20metrics%20to%0AVisual%20Inertial%20Navigation%20System%20%28VINS%29.%20The%20existing%20VINS%20algorithms%20with%0Aeither%20high%20accuracy%20or%20low%20computational%20complexity%2C%20are%20difficult%20to%20provide%0Athe%20high%20precision%20localization%20in%20resource-constrained%20devices.%20To%20this%20end%2C%0Awe%20propose%20a%20novel%20filter-based%20VINS%20framework%20named%20SchurVINS%2C%20which%20could%0Aguarantee%20both%20high%20accuracy%20by%20building%20a%20complete%20residual%20model%20and%20low%0Acomputational%20complexity%20with%20Schur%20complement.%20Technically%2C%20we%20first%20formulate%0Athe%20full%20residual%20model%20where%20Gradient%2C%20Hessian%20and%20observation%20covariance%20are%0Aexplicitly%20modeled.%20Then%20Schur%20complement%20is%20employed%20to%20decompose%20the%20full%0Amodel%20into%20ego-motion%20residual%20model%20and%20landmark%20residual%20model.%20Finally%2C%0AExtended%20Kalman%20Filter%20%28EKF%29%20update%20is%20implemented%20in%20these%20two%20models%20with%0Ahigh%20efficiency.%20Experiments%20on%20EuRoC%20and%20TUM-VI%20datasets%20show%20that%20our%20method%0Anotably%20outperforms%20state-of-the-art%20%28SOTA%29%20methods%20in%20both%20accuracy%20and%0Acomputational%20complexity.%20The%20experimental%20code%20of%20SchurVINS%20is%20available%20at%0Ahttps%3A//github.com/bytedance/SchurVINS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01616v5&entry.124074799=Read"},
{"title": "From Tissue Plane to Organ World: A Benchmark Dataset for Multimodal\n  Biomedical Image Registration using Deep Co-Attention Networks", "author": "Yifeng Wang and Weipeng Li and Thomas Pearce and Haohan Wang", "abstract": "  Correlating neuropathology with neuroimaging findings provides a multiscale\nview of pathologic changes in the human organ spanning the meso- to\nmicro-scales, and is an emerging methodology expected to shed light on numerous\ndisease states. To gain the most information from this multimodal, multiscale\napproach, it is desirable to identify precisely where a histologic tissue\nsection was taken from within the organ in order to correlate with the tissue\nfeatures in exactly the same organ region. Histology-to-organ registration\nposes an extra challenge, as any given histologic section can capture only a\nsmall portion of a human organ. Making use of the capabilities of\nstate-of-the-art deep learning models, we unlock the potential to address and\nsolve such intricate challenges. Therefore, we create the ATOM benchmark\ndataset, sourced from diverse institutions, with the primary objective of\ntransforming this challenge into a machine learning problem and delivering\noutstanding outcomes that enlighten the biomedical community. The performance\nof our RegisMCAN model demonstrates the potential of deep learning to\naccurately predict where a subregion extracted from an organ image was obtained\nfrom within the overall 3D volume. The code and dataset can be found at:\nhttps://github.com/haizailache999/Image-Registration/tree/main\n", "link": "http://arxiv.org/abs/2406.04105v1", "date": "2024-06-06", "relevancy": 2.6208, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5378}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5193}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Tissue%20Plane%20to%20Organ%20World%3A%20A%20Benchmark%20Dataset%20for%20Multimodal%0A%20%20Biomedical%20Image%20Registration%20using%20Deep%20Co-Attention%20Networks&body=Title%3A%20From%20Tissue%20Plane%20to%20Organ%20World%3A%20A%20Benchmark%20Dataset%20for%20Multimodal%0A%20%20Biomedical%20Image%20Registration%20using%20Deep%20Co-Attention%20Networks%0AAuthor%3A%20Yifeng%20Wang%20and%20Weipeng%20Li%20and%20Thomas%20Pearce%20and%20Haohan%20Wang%0AAbstract%3A%20%20%20Correlating%20neuropathology%20with%20neuroimaging%20findings%20provides%20a%20multiscale%0Aview%20of%20pathologic%20changes%20in%20the%20human%20organ%20spanning%20the%20meso-%20to%0Amicro-scales%2C%20and%20is%20an%20emerging%20methodology%20expected%20to%20shed%20light%20on%20numerous%0Adisease%20states.%20To%20gain%20the%20most%20information%20from%20this%20multimodal%2C%20multiscale%0Aapproach%2C%20it%20is%20desirable%20to%20identify%20precisely%20where%20a%20histologic%20tissue%0Asection%20was%20taken%20from%20within%20the%20organ%20in%20order%20to%20correlate%20with%20the%20tissue%0Afeatures%20in%20exactly%20the%20same%20organ%20region.%20Histology-to-organ%20registration%0Aposes%20an%20extra%20challenge%2C%20as%20any%20given%20histologic%20section%20can%20capture%20only%20a%0Asmall%20portion%20of%20a%20human%20organ.%20Making%20use%20of%20the%20capabilities%20of%0Astate-of-the-art%20deep%20learning%20models%2C%20we%20unlock%20the%20potential%20to%20address%20and%0Asolve%20such%20intricate%20challenges.%20Therefore%2C%20we%20create%20the%20ATOM%20benchmark%0Adataset%2C%20sourced%20from%20diverse%20institutions%2C%20with%20the%20primary%20objective%20of%0Atransforming%20this%20challenge%20into%20a%20machine%20learning%20problem%20and%20delivering%0Aoutstanding%20outcomes%20that%20enlighten%20the%20biomedical%20community.%20The%20performance%0Aof%20our%20RegisMCAN%20model%20demonstrates%20the%20potential%20of%20deep%20learning%20to%0Aaccurately%20predict%20where%20a%20subregion%20extracted%20from%20an%20organ%20image%20was%20obtained%0Afrom%20within%20the%20overall%203D%20volume.%20The%20code%20and%20dataset%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/haizailache999/Image-Registration/tree/main%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Tissue%2520Plane%2520to%2520Organ%2520World%253A%2520A%2520Benchmark%2520Dataset%2520for%2520Multimodal%250A%2520%2520Biomedical%2520Image%2520Registration%2520using%2520Deep%2520Co-Attention%2520Networks%26entry.906535625%3DYifeng%2520Wang%2520and%2520Weipeng%2520Li%2520and%2520Thomas%2520Pearce%2520and%2520Haohan%2520Wang%26entry.1292438233%3D%2520%2520Correlating%2520neuropathology%2520with%2520neuroimaging%2520findings%2520provides%2520a%2520multiscale%250Aview%2520of%2520pathologic%2520changes%2520in%2520the%2520human%2520organ%2520spanning%2520the%2520meso-%2520to%250Amicro-scales%252C%2520and%2520is%2520an%2520emerging%2520methodology%2520expected%2520to%2520shed%2520light%2520on%2520numerous%250Adisease%2520states.%2520To%2520gain%2520the%2520most%2520information%2520from%2520this%2520multimodal%252C%2520multiscale%250Aapproach%252C%2520it%2520is%2520desirable%2520to%2520identify%2520precisely%2520where%2520a%2520histologic%2520tissue%250Asection%2520was%2520taken%2520from%2520within%2520the%2520organ%2520in%2520order%2520to%2520correlate%2520with%2520the%2520tissue%250Afeatures%2520in%2520exactly%2520the%2520same%2520organ%2520region.%2520Histology-to-organ%2520registration%250Aposes%2520an%2520extra%2520challenge%252C%2520as%2520any%2520given%2520histologic%2520section%2520can%2520capture%2520only%2520a%250Asmall%2520portion%2520of%2520a%2520human%2520organ.%2520Making%2520use%2520of%2520the%2520capabilities%2520of%250Astate-of-the-art%2520deep%2520learning%2520models%252C%2520we%2520unlock%2520the%2520potential%2520to%2520address%2520and%250Asolve%2520such%2520intricate%2520challenges.%2520Therefore%252C%2520we%2520create%2520the%2520ATOM%2520benchmark%250Adataset%252C%2520sourced%2520from%2520diverse%2520institutions%252C%2520with%2520the%2520primary%2520objective%2520of%250Atransforming%2520this%2520challenge%2520into%2520a%2520machine%2520learning%2520problem%2520and%2520delivering%250Aoutstanding%2520outcomes%2520that%2520enlighten%2520the%2520biomedical%2520community.%2520The%2520performance%250Aof%2520our%2520RegisMCAN%2520model%2520demonstrates%2520the%2520potential%2520of%2520deep%2520learning%2520to%250Aaccurately%2520predict%2520where%2520a%2520subregion%2520extracted%2520from%2520an%2520organ%2520image%2520was%2520obtained%250Afrom%2520within%2520the%2520overall%25203D%2520volume.%2520The%2520code%2520and%2520dataset%2520can%2520be%2520found%2520at%253A%250Ahttps%253A//github.com/haizailache999/Image-Registration/tree/main%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Tissue%20Plane%20to%20Organ%20World%3A%20A%20Benchmark%20Dataset%20for%20Multimodal%0A%20%20Biomedical%20Image%20Registration%20using%20Deep%20Co-Attention%20Networks&entry.906535625=Yifeng%20Wang%20and%20Weipeng%20Li%20and%20Thomas%20Pearce%20and%20Haohan%20Wang&entry.1292438233=%20%20Correlating%20neuropathology%20with%20neuroimaging%20findings%20provides%20a%20multiscale%0Aview%20of%20pathologic%20changes%20in%20the%20human%20organ%20spanning%20the%20meso-%20to%0Amicro-scales%2C%20and%20is%20an%20emerging%20methodology%20expected%20to%20shed%20light%20on%20numerous%0Adisease%20states.%20To%20gain%20the%20most%20information%20from%20this%20multimodal%2C%20multiscale%0Aapproach%2C%20it%20is%20desirable%20to%20identify%20precisely%20where%20a%20histologic%20tissue%0Asection%20was%20taken%20from%20within%20the%20organ%20in%20order%20to%20correlate%20with%20the%20tissue%0Afeatures%20in%20exactly%20the%20same%20organ%20region.%20Histology-to-organ%20registration%0Aposes%20an%20extra%20challenge%2C%20as%20any%20given%20histologic%20section%20can%20capture%20only%20a%0Asmall%20portion%20of%20a%20human%20organ.%20Making%20use%20of%20the%20capabilities%20of%0Astate-of-the-art%20deep%20learning%20models%2C%20we%20unlock%20the%20potential%20to%20address%20and%0Asolve%20such%20intricate%20challenges.%20Therefore%2C%20we%20create%20the%20ATOM%20benchmark%0Adataset%2C%20sourced%20from%20diverse%20institutions%2C%20with%20the%20primary%20objective%20of%0Atransforming%20this%20challenge%20into%20a%20machine%20learning%20problem%20and%20delivering%0Aoutstanding%20outcomes%20that%20enlighten%20the%20biomedical%20community.%20The%20performance%0Aof%20our%20RegisMCAN%20model%20demonstrates%20the%20potential%20of%20deep%20learning%20to%0Aaccurately%20predict%20where%20a%20subregion%20extracted%20from%20an%20organ%20image%20was%20obtained%0Afrom%20within%20the%20overall%203D%20volume.%20The%20code%20and%20dataset%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/haizailache999/Image-Registration/tree/main%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04105v1&entry.124074799=Read"},
{"title": "Pointer-Guided Pre-Training: Infusing Large Language Models with\n  Paragraph-Level Contextual Awareness", "author": "Lars Hillebrand and Prabhupad Pradhan and Christian Bauckhage and Rafet Sifa", "abstract": "  We introduce \"pointer-guided segment ordering\" (SO), a novel pre-training\ntechnique aimed at enhancing the contextual understanding of paragraph-level\ntext representations in large language models. Our methodology leverages a\nself-attention-driven pointer network to restore the original sequence of\nshuffled text segments, addressing the challenge of capturing the structural\ncoherence and contextual dependencies within documents. This pre-training\napproach is complemented by a fine-tuning methodology that incorporates dynamic\nsampling, augmenting the diversity of training instances and improving sample\nefficiency for various downstream applications. We evaluate our method on a\ndiverse set of datasets, demonstrating its efficacy in tasks requiring\nsequential text classification across scientific literature and financial\nreporting domains. Our experiments show that pointer-guided pre-training\nsignificantly enhances the model's ability to understand complex document\nstructures, leading to state-of-the-art performance in downstream\nclassification tasks.\n", "link": "http://arxiv.org/abs/2406.04156v1", "date": "2024-06-06", "relevancy": 2.5874, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5206}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5165}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pointer-Guided%20Pre-Training%3A%20Infusing%20Large%20Language%20Models%20with%0A%20%20Paragraph-Level%20Contextual%20Awareness&body=Title%3A%20Pointer-Guided%20Pre-Training%3A%20Infusing%20Large%20Language%20Models%20with%0A%20%20Paragraph-Level%20Contextual%20Awareness%0AAuthor%3A%20Lars%20Hillebrand%20and%20Prabhupad%20Pradhan%20and%20Christian%20Bauckhage%20and%20Rafet%20Sifa%0AAbstract%3A%20%20%20We%20introduce%20%22pointer-guided%20segment%20ordering%22%20%28SO%29%2C%20a%20novel%20pre-training%0Atechnique%20aimed%20at%20enhancing%20the%20contextual%20understanding%20of%20paragraph-level%0Atext%20representations%20in%20large%20language%20models.%20Our%20methodology%20leverages%20a%0Aself-attention-driven%20pointer%20network%20to%20restore%20the%20original%20sequence%20of%0Ashuffled%20text%20segments%2C%20addressing%20the%20challenge%20of%20capturing%20the%20structural%0Acoherence%20and%20contextual%20dependencies%20within%20documents.%20This%20pre-training%0Aapproach%20is%20complemented%20by%20a%20fine-tuning%20methodology%20that%20incorporates%20dynamic%0Asampling%2C%20augmenting%20the%20diversity%20of%20training%20instances%20and%20improving%20sample%0Aefficiency%20for%20various%20downstream%20applications.%20We%20evaluate%20our%20method%20on%20a%0Adiverse%20set%20of%20datasets%2C%20demonstrating%20its%20efficacy%20in%20tasks%20requiring%0Asequential%20text%20classification%20across%20scientific%20literature%20and%20financial%0Areporting%20domains.%20Our%20experiments%20show%20that%20pointer-guided%20pre-training%0Asignificantly%20enhances%20the%20model%27s%20ability%20to%20understand%20complex%20document%0Astructures%2C%20leading%20to%20state-of-the-art%20performance%20in%20downstream%0Aclassification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04156v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointer-Guided%2520Pre-Training%253A%2520Infusing%2520Large%2520Language%2520Models%2520with%250A%2520%2520Paragraph-Level%2520Contextual%2520Awareness%26entry.906535625%3DLars%2520Hillebrand%2520and%2520Prabhupad%2520Pradhan%2520and%2520Christian%2520Bauckhage%2520and%2520Rafet%2520Sifa%26entry.1292438233%3D%2520%2520We%2520introduce%2520%2522pointer-guided%2520segment%2520ordering%2522%2520%2528SO%2529%252C%2520a%2520novel%2520pre-training%250Atechnique%2520aimed%2520at%2520enhancing%2520the%2520contextual%2520understanding%2520of%2520paragraph-level%250Atext%2520representations%2520in%2520large%2520language%2520models.%2520Our%2520methodology%2520leverages%2520a%250Aself-attention-driven%2520pointer%2520network%2520to%2520restore%2520the%2520original%2520sequence%2520of%250Ashuffled%2520text%2520segments%252C%2520addressing%2520the%2520challenge%2520of%2520capturing%2520the%2520structural%250Acoherence%2520and%2520contextual%2520dependencies%2520within%2520documents.%2520This%2520pre-training%250Aapproach%2520is%2520complemented%2520by%2520a%2520fine-tuning%2520methodology%2520that%2520incorporates%2520dynamic%250Asampling%252C%2520augmenting%2520the%2520diversity%2520of%2520training%2520instances%2520and%2520improving%2520sample%250Aefficiency%2520for%2520various%2520downstream%2520applications.%2520We%2520evaluate%2520our%2520method%2520on%2520a%250Adiverse%2520set%2520of%2520datasets%252C%2520demonstrating%2520its%2520efficacy%2520in%2520tasks%2520requiring%250Asequential%2520text%2520classification%2520across%2520scientific%2520literature%2520and%2520financial%250Areporting%2520domains.%2520Our%2520experiments%2520show%2520that%2520pointer-guided%2520pre-training%250Asignificantly%2520enhances%2520the%2520model%2527s%2520ability%2520to%2520understand%2520complex%2520document%250Astructures%252C%2520leading%2520to%2520state-of-the-art%2520performance%2520in%2520downstream%250Aclassification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04156v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pointer-Guided%20Pre-Training%3A%20Infusing%20Large%20Language%20Models%20with%0A%20%20Paragraph-Level%20Contextual%20Awareness&entry.906535625=Lars%20Hillebrand%20and%20Prabhupad%20Pradhan%20and%20Christian%20Bauckhage%20and%20Rafet%20Sifa&entry.1292438233=%20%20We%20introduce%20%22pointer-guided%20segment%20ordering%22%20%28SO%29%2C%20a%20novel%20pre-training%0Atechnique%20aimed%20at%20enhancing%20the%20contextual%20understanding%20of%20paragraph-level%0Atext%20representations%20in%20large%20language%20models.%20Our%20methodology%20leverages%20a%0Aself-attention-driven%20pointer%20network%20to%20restore%20the%20original%20sequence%20of%0Ashuffled%20text%20segments%2C%20addressing%20the%20challenge%20of%20capturing%20the%20structural%0Acoherence%20and%20contextual%20dependencies%20within%20documents.%20This%20pre-training%0Aapproach%20is%20complemented%20by%20a%20fine-tuning%20methodology%20that%20incorporates%20dynamic%0Asampling%2C%20augmenting%20the%20diversity%20of%20training%20instances%20and%20improving%20sample%0Aefficiency%20for%20various%20downstream%20applications.%20We%20evaluate%20our%20method%20on%20a%0Adiverse%20set%20of%20datasets%2C%20demonstrating%20its%20efficacy%20in%20tasks%20requiring%0Asequential%20text%20classification%20across%20scientific%20literature%20and%20financial%0Areporting%20domains.%20Our%20experiments%20show%20that%20pointer-guided%20pre-training%0Asignificantly%20enhances%20the%20model%27s%20ability%20to%20understand%20complex%20document%0Astructures%2C%20leading%20to%20state-of-the-art%20performance%20in%20downstream%0Aclassification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04156v1&entry.124074799=Read"},
{"title": "VideoTetris: Towards Compositional Text-to-Video Generation", "author": "Ye Tian and Ling Yang and Haotian Yang and Yuan Gao and Yufan Deng and Jingmin Chen and Xintao Wang and Zhaochen Yu and Xin Tao and Pengfei Wan and Di Zhang and Bin Cui", "abstract": "  Diffusion models have demonstrated great success in text-to-video (T2V)\ngeneration. However, existing methods may face challenges when handling complex\n(long) video generation scenarios that involve multiple objects or dynamic\nchanges in object numbers. To address these limitations, we propose\nVideoTetris, a novel framework that enables compositional T2V generation.\nSpecifically, we propose spatio-temporal compositional diffusion to precisely\nfollow complex textual semantics by manipulating and composing the attention\nmaps of denoising networks spatially and temporally. Moreover, we propose an\nenhanced video data preprocessing to enhance the training data regarding motion\ndynamics and prompt understanding, equipped with a new reference frame\nattention mechanism to improve the consistency of auto-regressive video\ngeneration. Extensive experiments demonstrate that our VideoTetris achieves\nimpressive qualitative and quantitative results in compositional T2V\ngeneration. Code is available at: https://github.com/YangLing0818/VideoTetris\n", "link": "http://arxiv.org/abs/2406.04277v1", "date": "2024-06-06", "relevancy": 2.5855, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6666}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6618}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoTetris%3A%20Towards%20Compositional%20Text-to-Video%20Generation&body=Title%3A%20VideoTetris%3A%20Towards%20Compositional%20Text-to-Video%20Generation%0AAuthor%3A%20Ye%20Tian%20and%20Ling%20Yang%20and%20Haotian%20Yang%20and%20Yuan%20Gao%20and%20Yufan%20Deng%20and%20Jingmin%20Chen%20and%20Xintao%20Wang%20and%20Zhaochen%20Yu%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Bin%20Cui%0AAbstract%3A%20%20%20Diffusion%20models%20have%20demonstrated%20great%20success%20in%20text-to-video%20%28T2V%29%0Ageneration.%20However%2C%20existing%20methods%20may%20face%20challenges%20when%20handling%20complex%0A%28long%29%20video%20generation%20scenarios%20that%20involve%20multiple%20objects%20or%20dynamic%0Achanges%20in%20object%20numbers.%20To%20address%20these%20limitations%2C%20we%20propose%0AVideoTetris%2C%20a%20novel%20framework%20that%20enables%20compositional%20T2V%20generation.%0ASpecifically%2C%20we%20propose%20spatio-temporal%20compositional%20diffusion%20to%20precisely%0Afollow%20complex%20textual%20semantics%20by%20manipulating%20and%20composing%20the%20attention%0Amaps%20of%20denoising%20networks%20spatially%20and%20temporally.%20Moreover%2C%20we%20propose%20an%0Aenhanced%20video%20data%20preprocessing%20to%20enhance%20the%20training%20data%20regarding%20motion%0Adynamics%20and%20prompt%20understanding%2C%20equipped%20with%20a%20new%20reference%20frame%0Aattention%20mechanism%20to%20improve%20the%20consistency%20of%20auto-regressive%20video%0Ageneration.%20Extensive%20experiments%20demonstrate%20that%20our%20VideoTetris%20achieves%0Aimpressive%20qualitative%20and%20quantitative%20results%20in%20compositional%20T2V%0Ageneration.%20Code%20is%20available%20at%3A%20https%3A//github.com/YangLing0818/VideoTetris%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04277v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoTetris%253A%2520Towards%2520Compositional%2520Text-to-Video%2520Generation%26entry.906535625%3DYe%2520Tian%2520and%2520Ling%2520Yang%2520and%2520Haotian%2520Yang%2520and%2520Yuan%2520Gao%2520and%2520Yufan%2520Deng%2520and%2520Jingmin%2520Chen%2520and%2520Xintao%2520Wang%2520and%2520Zhaochen%2520Yu%2520and%2520Xin%2520Tao%2520and%2520Pengfei%2520Wan%2520and%2520Di%2520Zhang%2520and%2520Bin%2520Cui%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520demonstrated%2520great%2520success%2520in%2520text-to-video%2520%2528T2V%2529%250Ageneration.%2520However%252C%2520existing%2520methods%2520may%2520face%2520challenges%2520when%2520handling%2520complex%250A%2528long%2529%2520video%2520generation%2520scenarios%2520that%2520involve%2520multiple%2520objects%2520or%2520dynamic%250Achanges%2520in%2520object%2520numbers.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%250AVideoTetris%252C%2520a%2520novel%2520framework%2520that%2520enables%2520compositional%2520T2V%2520generation.%250ASpecifically%252C%2520we%2520propose%2520spatio-temporal%2520compositional%2520diffusion%2520to%2520precisely%250Afollow%2520complex%2520textual%2520semantics%2520by%2520manipulating%2520and%2520composing%2520the%2520attention%250Amaps%2520of%2520denoising%2520networks%2520spatially%2520and%2520temporally.%2520Moreover%252C%2520we%2520propose%2520an%250Aenhanced%2520video%2520data%2520preprocessing%2520to%2520enhance%2520the%2520training%2520data%2520regarding%2520motion%250Adynamics%2520and%2520prompt%2520understanding%252C%2520equipped%2520with%2520a%2520new%2520reference%2520frame%250Aattention%2520mechanism%2520to%2520improve%2520the%2520consistency%2520of%2520auto-regressive%2520video%250Ageneration.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520VideoTetris%2520achieves%250Aimpressive%2520qualitative%2520and%2520quantitative%2520results%2520in%2520compositional%2520T2V%250Ageneration.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/YangLing0818/VideoTetris%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04277v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoTetris%3A%20Towards%20Compositional%20Text-to-Video%20Generation&entry.906535625=Ye%20Tian%20and%20Ling%20Yang%20and%20Haotian%20Yang%20and%20Yuan%20Gao%20and%20Yufan%20Deng%20and%20Jingmin%20Chen%20and%20Xintao%20Wang%20and%20Zhaochen%20Yu%20and%20Xin%20Tao%20and%20Pengfei%20Wan%20and%20Di%20Zhang%20and%20Bin%20Cui&entry.1292438233=%20%20Diffusion%20models%20have%20demonstrated%20great%20success%20in%20text-to-video%20%28T2V%29%0Ageneration.%20However%2C%20existing%20methods%20may%20face%20challenges%20when%20handling%20complex%0A%28long%29%20video%20generation%20scenarios%20that%20involve%20multiple%20objects%20or%20dynamic%0Achanges%20in%20object%20numbers.%20To%20address%20these%20limitations%2C%20we%20propose%0AVideoTetris%2C%20a%20novel%20framework%20that%20enables%20compositional%20T2V%20generation.%0ASpecifically%2C%20we%20propose%20spatio-temporal%20compositional%20diffusion%20to%20precisely%0Afollow%20complex%20textual%20semantics%20by%20manipulating%20and%20composing%20the%20attention%0Amaps%20of%20denoising%20networks%20spatially%20and%20temporally.%20Moreover%2C%20we%20propose%20an%0Aenhanced%20video%20data%20preprocessing%20to%20enhance%20the%20training%20data%20regarding%20motion%0Adynamics%20and%20prompt%20understanding%2C%20equipped%20with%20a%20new%20reference%20frame%0Aattention%20mechanism%20to%20improve%20the%20consistency%20of%20auto-regressive%20video%0Ageneration.%20Extensive%20experiments%20demonstrate%20that%20our%20VideoTetris%20achieves%0Aimpressive%20qualitative%20and%20quantitative%20results%20in%20compositional%20T2V%0Ageneration.%20Code%20is%20available%20at%3A%20https%3A//github.com/YangLing0818/VideoTetris%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04277v1&entry.124074799=Read"},
{"title": "Road Network Representation Learning with the Third Law of Geography", "author": "Haicang Zhou and Weiming Huang and Yile Chen and Tiantian He and Gao Cong and Yew-Soon Ong", "abstract": "  Road network representation learning aims to learn compressed and effective\nvectorized representations for road segments that are applicable to numerous\ntasks. In this paper, we identify the limitations of existing methods,\nparticularly their overemphasis on the distance effect as outlined in the First\nLaw of Geography. In response, we propose to endow road network representation\nwith the principles of the recent Third Law of Geography. To this end, we\npropose a novel graph contrastive learning framework that employs geographic\nconfiguration-aware graph augmentation and spectral negative sampling, ensuring\nthat road segments with similar geographic configurations yield similar\nrepresentations, and vice versa, aligning with the principles stated in the\nThird Law. The framework further fuses the Third Law with the First Law through\na dual contrastive learning objective to effectively balance the implications\nof both laws. We evaluate our framework on two real-world datasets across three\ndownstream tasks. The results show that the integration of the Third Law\nsignificantly improves the performance of road segment representations in\ndownstream tasks.\n", "link": "http://arxiv.org/abs/2406.04038v1", "date": "2024-06-06", "relevancy": 2.5817, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5559}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.517}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Road%20Network%20Representation%20Learning%20with%20the%20Third%20Law%20of%20Geography&body=Title%3A%20Road%20Network%20Representation%20Learning%20with%20the%20Third%20Law%20of%20Geography%0AAuthor%3A%20Haicang%20Zhou%20and%20Weiming%20Huang%20and%20Yile%20Chen%20and%20Tiantian%20He%20and%20Gao%20Cong%20and%20Yew-Soon%20Ong%0AAbstract%3A%20%20%20Road%20network%20representation%20learning%20aims%20to%20learn%20compressed%20and%20effective%0Avectorized%20representations%20for%20road%20segments%20that%20are%20applicable%20to%20numerous%0Atasks.%20In%20this%20paper%2C%20we%20identify%20the%20limitations%20of%20existing%20methods%2C%0Aparticularly%20their%20overemphasis%20on%20the%20distance%20effect%20as%20outlined%20in%20the%20First%0ALaw%20of%20Geography.%20In%20response%2C%20we%20propose%20to%20endow%20road%20network%20representation%0Awith%20the%20principles%20of%20the%20recent%20Third%20Law%20of%20Geography.%20To%20this%20end%2C%20we%0Apropose%20a%20novel%20graph%20contrastive%20learning%20framework%20that%20employs%20geographic%0Aconfiguration-aware%20graph%20augmentation%20and%20spectral%20negative%20sampling%2C%20ensuring%0Athat%20road%20segments%20with%20similar%20geographic%20configurations%20yield%20similar%0Arepresentations%2C%20and%20vice%20versa%2C%20aligning%20with%20the%20principles%20stated%20in%20the%0AThird%20Law.%20The%20framework%20further%20fuses%20the%20Third%20Law%20with%20the%20First%20Law%20through%0Aa%20dual%20contrastive%20learning%20objective%20to%20effectively%20balance%20the%20implications%0Aof%20both%20laws.%20We%20evaluate%20our%20framework%20on%20two%20real-world%20datasets%20across%20three%0Adownstream%20tasks.%20The%20results%20show%20that%20the%20integration%20of%20the%20Third%20Law%0Asignificantly%20improves%20the%20performance%20of%20road%20segment%20representations%20in%0Adownstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoad%2520Network%2520Representation%2520Learning%2520with%2520the%2520Third%2520Law%2520of%2520Geography%26entry.906535625%3DHaicang%2520Zhou%2520and%2520Weiming%2520Huang%2520and%2520Yile%2520Chen%2520and%2520Tiantian%2520He%2520and%2520Gao%2520Cong%2520and%2520Yew-Soon%2520Ong%26entry.1292438233%3D%2520%2520Road%2520network%2520representation%2520learning%2520aims%2520to%2520learn%2520compressed%2520and%2520effective%250Avectorized%2520representations%2520for%2520road%2520segments%2520that%2520are%2520applicable%2520to%2520numerous%250Atasks.%2520In%2520this%2520paper%252C%2520we%2520identify%2520the%2520limitations%2520of%2520existing%2520methods%252C%250Aparticularly%2520their%2520overemphasis%2520on%2520the%2520distance%2520effect%2520as%2520outlined%2520in%2520the%2520First%250ALaw%2520of%2520Geography.%2520In%2520response%252C%2520we%2520propose%2520to%2520endow%2520road%2520network%2520representation%250Awith%2520the%2520principles%2520of%2520the%2520recent%2520Third%2520Law%2520of%2520Geography.%2520To%2520this%2520end%252C%2520we%250Apropose%2520a%2520novel%2520graph%2520contrastive%2520learning%2520framework%2520that%2520employs%2520geographic%250Aconfiguration-aware%2520graph%2520augmentation%2520and%2520spectral%2520negative%2520sampling%252C%2520ensuring%250Athat%2520road%2520segments%2520with%2520similar%2520geographic%2520configurations%2520yield%2520similar%250Arepresentations%252C%2520and%2520vice%2520versa%252C%2520aligning%2520with%2520the%2520principles%2520stated%2520in%2520the%250AThird%2520Law.%2520The%2520framework%2520further%2520fuses%2520the%2520Third%2520Law%2520with%2520the%2520First%2520Law%2520through%250Aa%2520dual%2520contrastive%2520learning%2520objective%2520to%2520effectively%2520balance%2520the%2520implications%250Aof%2520both%2520laws.%2520We%2520evaluate%2520our%2520framework%2520on%2520two%2520real-world%2520datasets%2520across%2520three%250Adownstream%2520tasks.%2520The%2520results%2520show%2520that%2520the%2520integration%2520of%2520the%2520Third%2520Law%250Asignificantly%2520improves%2520the%2520performance%2520of%2520road%2520segment%2520representations%2520in%250Adownstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Road%20Network%20Representation%20Learning%20with%20the%20Third%20Law%20of%20Geography&entry.906535625=Haicang%20Zhou%20and%20Weiming%20Huang%20and%20Yile%20Chen%20and%20Tiantian%20He%20and%20Gao%20Cong%20and%20Yew-Soon%20Ong&entry.1292438233=%20%20Road%20network%20representation%20learning%20aims%20to%20learn%20compressed%20and%20effective%0Avectorized%20representations%20for%20road%20segments%20that%20are%20applicable%20to%20numerous%0Atasks.%20In%20this%20paper%2C%20we%20identify%20the%20limitations%20of%20existing%20methods%2C%0Aparticularly%20their%20overemphasis%20on%20the%20distance%20effect%20as%20outlined%20in%20the%20First%0ALaw%20of%20Geography.%20In%20response%2C%20we%20propose%20to%20endow%20road%20network%20representation%0Awith%20the%20principles%20of%20the%20recent%20Third%20Law%20of%20Geography.%20To%20this%20end%2C%20we%0Apropose%20a%20novel%20graph%20contrastive%20learning%20framework%20that%20employs%20geographic%0Aconfiguration-aware%20graph%20augmentation%20and%20spectral%20negative%20sampling%2C%20ensuring%0Athat%20road%20segments%20with%20similar%20geographic%20configurations%20yield%20similar%0Arepresentations%2C%20and%20vice%20versa%2C%20aligning%20with%20the%20principles%20stated%20in%20the%0AThird%20Law.%20The%20framework%20further%20fuses%20the%20Third%20Law%20with%20the%20First%20Law%20through%0Aa%20dual%20contrastive%20learning%20objective%20to%20effectively%20balance%20the%20implications%0Aof%20both%20laws.%20We%20evaluate%20our%20framework%20on%20two%20real-world%20datasets%20across%20three%0Adownstream%20tasks.%20The%20results%20show%20that%20the%20integration%20of%20the%20Third%20Law%0Asignificantly%20improves%20the%20performance%20of%20road%20segment%20representations%20in%0Adownstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04038v1&entry.124074799=Read"},
{"title": "VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval", "author": "Junjie Zhou and Zheng Liu and Shitao Xiao and Bo Zhao and Yongping Xiong", "abstract": "  Multi-modal retrieval becomes increasingly popular in practice. However, the\nexisting retrievers are mostly text-oriented, which lack the capability to\nprocess visual information. Despite the presence of vision-language models like\nCLIP, the current methods are severely limited in representing the text-only\nand image-only data. In this work, we present a new embedding model VISTA for\nuniversal multi-modal retrieval. Our work brings forth threefold technical\ncontributions. Firstly, we introduce a flexible architecture which extends a\npowerful text encoder with the image understanding capability by introducing\nvisual token embeddings. Secondly, we develop two data generation strategies,\nwhich bring high-quality composed image-text to facilitate the training of the\nembedding model. Thirdly, we introduce a multi-stage training algorithm, which\nfirst aligns the visual token embedding with the text encoder using massive\nweakly labeled data, and then develops multi-modal representation capability\nusing the generated composed image-text data. In our experiments, VISTA\nachieves superior performances across a variety of multi-modal retrieval tasks\nin both zero-shot and supervised settings. Our model, data, and source code are\navailable at https://github.com/FlagOpen/FlagEmbedding.\n", "link": "http://arxiv.org/abs/2406.04292v1", "date": "2024-06-06", "relevancy": 2.5793, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5637}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5025}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VISTA%3A%20Visualized%20Text%20Embedding%20For%20Universal%20Multi-Modal%20Retrieval&body=Title%3A%20VISTA%3A%20Visualized%20Text%20Embedding%20For%20Universal%20Multi-Modal%20Retrieval%0AAuthor%3A%20Junjie%20Zhou%20and%20Zheng%20Liu%20and%20Shitao%20Xiao%20and%20Bo%20Zhao%20and%20Yongping%20Xiong%0AAbstract%3A%20%20%20Multi-modal%20retrieval%20becomes%20increasingly%20popular%20in%20practice.%20However%2C%20the%0Aexisting%20retrievers%20are%20mostly%20text-oriented%2C%20which%20lack%20the%20capability%20to%0Aprocess%20visual%20information.%20Despite%20the%20presence%20of%20vision-language%20models%20like%0ACLIP%2C%20the%20current%20methods%20are%20severely%20limited%20in%20representing%20the%20text-only%0Aand%20image-only%20data.%20In%20this%20work%2C%20we%20present%20a%20new%20embedding%20model%20VISTA%20for%0Auniversal%20multi-modal%20retrieval.%20Our%20work%20brings%20forth%20threefold%20technical%0Acontributions.%20Firstly%2C%20we%20introduce%20a%20flexible%20architecture%20which%20extends%20a%0Apowerful%20text%20encoder%20with%20the%20image%20understanding%20capability%20by%20introducing%0Avisual%20token%20embeddings.%20Secondly%2C%20we%20develop%20two%20data%20generation%20strategies%2C%0Awhich%20bring%20high-quality%20composed%20image-text%20to%20facilitate%20the%20training%20of%20the%0Aembedding%20model.%20Thirdly%2C%20we%20introduce%20a%20multi-stage%20training%20algorithm%2C%20which%0Afirst%20aligns%20the%20visual%20token%20embedding%20with%20the%20text%20encoder%20using%20massive%0Aweakly%20labeled%20data%2C%20and%20then%20develops%20multi-modal%20representation%20capability%0Ausing%20the%20generated%20composed%20image-text%20data.%20In%20our%20experiments%2C%20VISTA%0Aachieves%20superior%20performances%20across%20a%20variety%20of%20multi-modal%20retrieval%20tasks%0Ain%20both%20zero-shot%20and%20supervised%20settings.%20Our%20model%2C%20data%2C%20and%20source%20code%20are%0Aavailable%20at%20https%3A//github.com/FlagOpen/FlagEmbedding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVISTA%253A%2520Visualized%2520Text%2520Embedding%2520For%2520Universal%2520Multi-Modal%2520Retrieval%26entry.906535625%3DJunjie%2520Zhou%2520and%2520Zheng%2520Liu%2520and%2520Shitao%2520Xiao%2520and%2520Bo%2520Zhao%2520and%2520Yongping%2520Xiong%26entry.1292438233%3D%2520%2520Multi-modal%2520retrieval%2520becomes%2520increasingly%2520popular%2520in%2520practice.%2520However%252C%2520the%250Aexisting%2520retrievers%2520are%2520mostly%2520text-oriented%252C%2520which%2520lack%2520the%2520capability%2520to%250Aprocess%2520visual%2520information.%2520Despite%2520the%2520presence%2520of%2520vision-language%2520models%2520like%250ACLIP%252C%2520the%2520current%2520methods%2520are%2520severely%2520limited%2520in%2520representing%2520the%2520text-only%250Aand%2520image-only%2520data.%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520new%2520embedding%2520model%2520VISTA%2520for%250Auniversal%2520multi-modal%2520retrieval.%2520Our%2520work%2520brings%2520forth%2520threefold%2520technical%250Acontributions.%2520Firstly%252C%2520we%2520introduce%2520a%2520flexible%2520architecture%2520which%2520extends%2520a%250Apowerful%2520text%2520encoder%2520with%2520the%2520image%2520understanding%2520capability%2520by%2520introducing%250Avisual%2520token%2520embeddings.%2520Secondly%252C%2520we%2520develop%2520two%2520data%2520generation%2520strategies%252C%250Awhich%2520bring%2520high-quality%2520composed%2520image-text%2520to%2520facilitate%2520the%2520training%2520of%2520the%250Aembedding%2520model.%2520Thirdly%252C%2520we%2520introduce%2520a%2520multi-stage%2520training%2520algorithm%252C%2520which%250Afirst%2520aligns%2520the%2520visual%2520token%2520embedding%2520with%2520the%2520text%2520encoder%2520using%2520massive%250Aweakly%2520labeled%2520data%252C%2520and%2520then%2520develops%2520multi-modal%2520representation%2520capability%250Ausing%2520the%2520generated%2520composed%2520image-text%2520data.%2520In%2520our%2520experiments%252C%2520VISTA%250Aachieves%2520superior%2520performances%2520across%2520a%2520variety%2520of%2520multi-modal%2520retrieval%2520tasks%250Ain%2520both%2520zero-shot%2520and%2520supervised%2520settings.%2520Our%2520model%252C%2520data%252C%2520and%2520source%2520code%2520are%250Aavailable%2520at%2520https%253A//github.com/FlagOpen/FlagEmbedding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VISTA%3A%20Visualized%20Text%20Embedding%20For%20Universal%20Multi-Modal%20Retrieval&entry.906535625=Junjie%20Zhou%20and%20Zheng%20Liu%20and%20Shitao%20Xiao%20and%20Bo%20Zhao%20and%20Yongping%20Xiong&entry.1292438233=%20%20Multi-modal%20retrieval%20becomes%20increasingly%20popular%20in%20practice.%20However%2C%20the%0Aexisting%20retrievers%20are%20mostly%20text-oriented%2C%20which%20lack%20the%20capability%20to%0Aprocess%20visual%20information.%20Despite%20the%20presence%20of%20vision-language%20models%20like%0ACLIP%2C%20the%20current%20methods%20are%20severely%20limited%20in%20representing%20the%20text-only%0Aand%20image-only%20data.%20In%20this%20work%2C%20we%20present%20a%20new%20embedding%20model%20VISTA%20for%0Auniversal%20multi-modal%20retrieval.%20Our%20work%20brings%20forth%20threefold%20technical%0Acontributions.%20Firstly%2C%20we%20introduce%20a%20flexible%20architecture%20which%20extends%20a%0Apowerful%20text%20encoder%20with%20the%20image%20understanding%20capability%20by%20introducing%0Avisual%20token%20embeddings.%20Secondly%2C%20we%20develop%20two%20data%20generation%20strategies%2C%0Awhich%20bring%20high-quality%20composed%20image-text%20to%20facilitate%20the%20training%20of%20the%0Aembedding%20model.%20Thirdly%2C%20we%20introduce%20a%20multi-stage%20training%20algorithm%2C%20which%0Afirst%20aligns%20the%20visual%20token%20embedding%20with%20the%20text%20encoder%20using%20massive%0Aweakly%20labeled%20data%2C%20and%20then%20develops%20multi-modal%20representation%20capability%0Ausing%20the%20generated%20composed%20image-text%20data.%20In%20our%20experiments%2C%20VISTA%0Aachieves%20superior%20performances%20across%20a%20variety%20of%20multi-modal%20retrieval%20tasks%0Ain%20both%20zero-shot%20and%20supervised%20settings.%20Our%20model%2C%20data%2C%20and%20source%20code%20are%0Aavailable%20at%20https%3A//github.com/FlagOpen/FlagEmbedding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04292v1&entry.124074799=Read"},
{"title": "SF-V: Single Forward Video Generation Model", "author": "Zhixing Zhang and Yanyu Li and Yushu Wu and Yanwu Xu and Anil Kag and Ivan Skorokhodov and Willi Menapace and Aliaksandr Siarohin and Junli Cao and Dimitris Metaxas and Sergey Tulyakov and Jian Ren", "abstract": "  Diffusion-based video generation models have demonstrated remarkable success\nin obtaining high-fidelity videos through the iterative denoising process.\nHowever, these models require multiple denoising steps during sampling,\nresulting in high computational costs. In this work, we propose a novel\napproach to obtain single-step video generation models by leveraging\nadversarial training to fine-tune pre-trained video diffusion models. We show\nthat, through the adversarial training, the multi-steps video diffusion model,\ni.e., Stable Video Diffusion (SVD), can be trained to perform single forward\npass to synthesize high-quality videos, capturing both temporal and spatial\ndependencies in the video data. Extensive experiments demonstrate that our\nmethod achieves competitive generation quality of synthesized videos with\nsignificantly reduced computational overhead for the denoising process (i.e.,\naround $23\\times$ speedup compared with SVD and $6\\times$ speedup compared with\nexisting works, with even better generation quality), paving the way for\nreal-time video synthesis and editing. More visualization results are made\npublicly available at https://snap-research.github.io/SF-V.\n", "link": "http://arxiv.org/abs/2406.04324v1", "date": "2024-06-06", "relevancy": 2.5663, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6454}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6418}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SF-V%3A%20Single%20Forward%20Video%20Generation%20Model&body=Title%3A%20SF-V%3A%20Single%20Forward%20Video%20Generation%20Model%0AAuthor%3A%20Zhixing%20Zhang%20and%20Yanyu%20Li%20and%20Yushu%20Wu%20and%20Yanwu%20Xu%20and%20Anil%20Kag%20and%20Ivan%20Skorokhodov%20and%20Willi%20Menapace%20and%20Aliaksandr%20Siarohin%20and%20Junli%20Cao%20and%20Dimitris%20Metaxas%20and%20Sergey%20Tulyakov%20and%20Jian%20Ren%0AAbstract%3A%20%20%20Diffusion-based%20video%20generation%20models%20have%20demonstrated%20remarkable%20success%0Ain%20obtaining%20high-fidelity%20videos%20through%20the%20iterative%20denoising%20process.%0AHowever%2C%20these%20models%20require%20multiple%20denoising%20steps%20during%20sampling%2C%0Aresulting%20in%20high%20computational%20costs.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Aapproach%20to%20obtain%20single-step%20video%20generation%20models%20by%20leveraging%0Aadversarial%20training%20to%20fine-tune%20pre-trained%20video%20diffusion%20models.%20We%20show%0Athat%2C%20through%20the%20adversarial%20training%2C%20the%20multi-steps%20video%20diffusion%20model%2C%0Ai.e.%2C%20Stable%20Video%20Diffusion%20%28SVD%29%2C%20can%20be%20trained%20to%20perform%20single%20forward%0Apass%20to%20synthesize%20high-quality%20videos%2C%20capturing%20both%20temporal%20and%20spatial%0Adependencies%20in%20the%20video%20data.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20achieves%20competitive%20generation%20quality%20of%20synthesized%20videos%20with%0Asignificantly%20reduced%20computational%20overhead%20for%20the%20denoising%20process%20%28i.e.%2C%0Aaround%20%2423%5Ctimes%24%20speedup%20compared%20with%20SVD%20and%20%246%5Ctimes%24%20speedup%20compared%20with%0Aexisting%20works%2C%20with%20even%20better%20generation%20quality%29%2C%20paving%20the%20way%20for%0Areal-time%20video%20synthesis%20and%20editing.%20More%20visualization%20results%20are%20made%0Apublicly%20available%20at%20https%3A//snap-research.github.io/SF-V.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04324v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSF-V%253A%2520Single%2520Forward%2520Video%2520Generation%2520Model%26entry.906535625%3DZhixing%2520Zhang%2520and%2520Yanyu%2520Li%2520and%2520Yushu%2520Wu%2520and%2520Yanwu%2520Xu%2520and%2520Anil%2520Kag%2520and%2520Ivan%2520Skorokhodov%2520and%2520Willi%2520Menapace%2520and%2520Aliaksandr%2520Siarohin%2520and%2520Junli%2520Cao%2520and%2520Dimitris%2520Metaxas%2520and%2520Sergey%2520Tulyakov%2520and%2520Jian%2520Ren%26entry.1292438233%3D%2520%2520Diffusion-based%2520video%2520generation%2520models%2520have%2520demonstrated%2520remarkable%2520success%250Ain%2520obtaining%2520high-fidelity%2520videos%2520through%2520the%2520iterative%2520denoising%2520process.%250AHowever%252C%2520these%2520models%2520require%2520multiple%2520denoising%2520steps%2520during%2520sampling%252C%250Aresulting%2520in%2520high%2520computational%2520costs.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%250Aapproach%2520to%2520obtain%2520single-step%2520video%2520generation%2520models%2520by%2520leveraging%250Aadversarial%2520training%2520to%2520fine-tune%2520pre-trained%2520video%2520diffusion%2520models.%2520We%2520show%250Athat%252C%2520through%2520the%2520adversarial%2520training%252C%2520the%2520multi-steps%2520video%2520diffusion%2520model%252C%250Ai.e.%252C%2520Stable%2520Video%2520Diffusion%2520%2528SVD%2529%252C%2520can%2520be%2520trained%2520to%2520perform%2520single%2520forward%250Apass%2520to%2520synthesize%2520high-quality%2520videos%252C%2520capturing%2520both%2520temporal%2520and%2520spatial%250Adependencies%2520in%2520the%2520video%2520data.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Amethod%2520achieves%2520competitive%2520generation%2520quality%2520of%2520synthesized%2520videos%2520with%250Asignificantly%2520reduced%2520computational%2520overhead%2520for%2520the%2520denoising%2520process%2520%2528i.e.%252C%250Aaround%2520%252423%255Ctimes%2524%2520speedup%2520compared%2520with%2520SVD%2520and%2520%25246%255Ctimes%2524%2520speedup%2520compared%2520with%250Aexisting%2520works%252C%2520with%2520even%2520better%2520generation%2520quality%2529%252C%2520paving%2520the%2520way%2520for%250Areal-time%2520video%2520synthesis%2520and%2520editing.%2520More%2520visualization%2520results%2520are%2520made%250Apublicly%2520available%2520at%2520https%253A//snap-research.github.io/SF-V.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04324v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SF-V%3A%20Single%20Forward%20Video%20Generation%20Model&entry.906535625=Zhixing%20Zhang%20and%20Yanyu%20Li%20and%20Yushu%20Wu%20and%20Yanwu%20Xu%20and%20Anil%20Kag%20and%20Ivan%20Skorokhodov%20and%20Willi%20Menapace%20and%20Aliaksandr%20Siarohin%20and%20Junli%20Cao%20and%20Dimitris%20Metaxas%20and%20Sergey%20Tulyakov%20and%20Jian%20Ren&entry.1292438233=%20%20Diffusion-based%20video%20generation%20models%20have%20demonstrated%20remarkable%20success%0Ain%20obtaining%20high-fidelity%20videos%20through%20the%20iterative%20denoising%20process.%0AHowever%2C%20these%20models%20require%20multiple%20denoising%20steps%20during%20sampling%2C%0Aresulting%20in%20high%20computational%20costs.%20In%20this%20work%2C%20we%20propose%20a%20novel%0Aapproach%20to%20obtain%20single-step%20video%20generation%20models%20by%20leveraging%0Aadversarial%20training%20to%20fine-tune%20pre-trained%20video%20diffusion%20models.%20We%20show%0Athat%2C%20through%20the%20adversarial%20training%2C%20the%20multi-steps%20video%20diffusion%20model%2C%0Ai.e.%2C%20Stable%20Video%20Diffusion%20%28SVD%29%2C%20can%20be%20trained%20to%20perform%20single%20forward%0Apass%20to%20synthesize%20high-quality%20videos%2C%20capturing%20both%20temporal%20and%20spatial%0Adependencies%20in%20the%20video%20data.%20Extensive%20experiments%20demonstrate%20that%20our%0Amethod%20achieves%20competitive%20generation%20quality%20of%20synthesized%20videos%20with%0Asignificantly%20reduced%20computational%20overhead%20for%20the%20denoising%20process%20%28i.e.%2C%0Aaround%20%2423%5Ctimes%24%20speedup%20compared%20with%20SVD%20and%20%246%5Ctimes%24%20speedup%20compared%20with%0Aexisting%20works%2C%20with%20even%20better%20generation%20quality%29%2C%20paving%20the%20way%20for%0Areal-time%20video%20synthesis%20and%20editing.%20More%20visualization%20results%20are%20made%0Apublicly%20available%20at%20https%3A//snap-research.github.io/SF-V.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04324v1&entry.124074799=Read"},
{"title": "Generative AI-in-the-loop: Integrating LLMs and GPTs into the Next\n  Generation Networks", "author": "Han Zhang and Akram Bin Sediq and Ali Afana and Melike Erol-Kantarci", "abstract": "  In recent years, machine learning (ML) techniques have created numerous\nopportunities for intelligent mobile networks and have accelerated the\nautomation of network operations. However, complex network tasks may involve\nvariables and considerations even beyond the capacity of traditional ML\nalgorithms. On the other hand, large language models (LLMs) have recently\nemerged, demonstrating near-human-level performance in cognitive tasks across\nvarious fields. However, they remain prone to hallucinations and often lack\ncommon sense in basic tasks. Therefore, they are regarded as assistive tools\nfor humans. In this work, we propose the concept of \"generative AI-in-the-loop\"\nand utilize the semantic understanding, context awareness, and reasoning\nabilities of LLMs to assist humans in handling complex or unforeseen situations\nin mobile communication networks. We believe that combining LLMs and ML models\nallows both to leverage their respective capabilities and achieve better\nresults than either model alone. To support this idea, we begin by analyzing\nthe capabilities of LLMs and compare them with traditional ML algorithms. We\nthen explore potential LLM-based applications in line with the requirements of\nnext-generation networks. We further examine the integration of ML and LLMs,\ndiscussing how they can be used together in mobile networks. Unlike existing\nstudies, our research emphasizes the fusion of LLMs with traditional ML-driven\nnext-generation networks and serves as a comprehensive refinement of existing\nsurveys. Finally, we provide a case study to enhance ML-based network intrusion\ndetection with synthesized data generated by LLMs. Our case study further\ndemonstrates the advantages of our proposed idea.\n", "link": "http://arxiv.org/abs/2406.04276v1", "date": "2024-06-06", "relevancy": 2.5648, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5596}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4959}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI-in-the-loop%3A%20Integrating%20LLMs%20and%20GPTs%20into%20the%20Next%0A%20%20Generation%20Networks&body=Title%3A%20Generative%20AI-in-the-loop%3A%20Integrating%20LLMs%20and%20GPTs%20into%20the%20Next%0A%20%20Generation%20Networks%0AAuthor%3A%20Han%20Zhang%20and%20Akram%20Bin%20Sediq%20and%20Ali%20Afana%20and%20Melike%20Erol-Kantarci%0AAbstract%3A%20%20%20In%20recent%20years%2C%20machine%20learning%20%28ML%29%20techniques%20have%20created%20numerous%0Aopportunities%20for%20intelligent%20mobile%20networks%20and%20have%20accelerated%20the%0Aautomation%20of%20network%20operations.%20However%2C%20complex%20network%20tasks%20may%20involve%0Avariables%20and%20considerations%20even%20beyond%20the%20capacity%20of%20traditional%20ML%0Aalgorithms.%20On%20the%20other%20hand%2C%20large%20language%20models%20%28LLMs%29%20have%20recently%0Aemerged%2C%20demonstrating%20near-human-level%20performance%20in%20cognitive%20tasks%20across%0Avarious%20fields.%20However%2C%20they%20remain%20prone%20to%20hallucinations%20and%20often%20lack%0Acommon%20sense%20in%20basic%20tasks.%20Therefore%2C%20they%20are%20regarded%20as%20assistive%20tools%0Afor%20humans.%20In%20this%20work%2C%20we%20propose%20the%20concept%20of%20%22generative%20AI-in-the-loop%22%0Aand%20utilize%20the%20semantic%20understanding%2C%20context%20awareness%2C%20and%20reasoning%0Aabilities%20of%20LLMs%20to%20assist%20humans%20in%20handling%20complex%20or%20unforeseen%20situations%0Ain%20mobile%20communication%20networks.%20We%20believe%20that%20combining%20LLMs%20and%20ML%20models%0Aallows%20both%20to%20leverage%20their%20respective%20capabilities%20and%20achieve%20better%0Aresults%20than%20either%20model%20alone.%20To%20support%20this%20idea%2C%20we%20begin%20by%20analyzing%0Athe%20capabilities%20of%20LLMs%20and%20compare%20them%20with%20traditional%20ML%20algorithms.%20We%0Athen%20explore%20potential%20LLM-based%20applications%20in%20line%20with%20the%20requirements%20of%0Anext-generation%20networks.%20We%20further%20examine%20the%20integration%20of%20ML%20and%20LLMs%2C%0Adiscussing%20how%20they%20can%20be%20used%20together%20in%20mobile%20networks.%20Unlike%20existing%0Astudies%2C%20our%20research%20emphasizes%20the%20fusion%20of%20LLMs%20with%20traditional%20ML-driven%0Anext-generation%20networks%20and%20serves%20as%20a%20comprehensive%20refinement%20of%20existing%0Asurveys.%20Finally%2C%20we%20provide%20a%20case%20study%20to%20enhance%20ML-based%20network%20intrusion%0Adetection%20with%20synthesized%20data%20generated%20by%20LLMs.%20Our%20case%20study%20further%0Ademonstrates%20the%20advantages%20of%20our%20proposed%20idea.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04276v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI-in-the-loop%253A%2520Integrating%2520LLMs%2520and%2520GPTs%2520into%2520the%2520Next%250A%2520%2520Generation%2520Networks%26entry.906535625%3DHan%2520Zhang%2520and%2520Akram%2520Bin%2520Sediq%2520and%2520Ali%2520Afana%2520and%2520Melike%2520Erol-Kantarci%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520machine%2520learning%2520%2528ML%2529%2520techniques%2520have%2520created%2520numerous%250Aopportunities%2520for%2520intelligent%2520mobile%2520networks%2520and%2520have%2520accelerated%2520the%250Aautomation%2520of%2520network%2520operations.%2520However%252C%2520complex%2520network%2520tasks%2520may%2520involve%250Avariables%2520and%2520considerations%2520even%2520beyond%2520the%2520capacity%2520of%2520traditional%2520ML%250Aalgorithms.%2520On%2520the%2520other%2520hand%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520recently%250Aemerged%252C%2520demonstrating%2520near-human-level%2520performance%2520in%2520cognitive%2520tasks%2520across%250Avarious%2520fields.%2520However%252C%2520they%2520remain%2520prone%2520to%2520hallucinations%2520and%2520often%2520lack%250Acommon%2520sense%2520in%2520basic%2520tasks.%2520Therefore%252C%2520they%2520are%2520regarded%2520as%2520assistive%2520tools%250Afor%2520humans.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520concept%2520of%2520%2522generative%2520AI-in-the-loop%2522%250Aand%2520utilize%2520the%2520semantic%2520understanding%252C%2520context%2520awareness%252C%2520and%2520reasoning%250Aabilities%2520of%2520LLMs%2520to%2520assist%2520humans%2520in%2520handling%2520complex%2520or%2520unforeseen%2520situations%250Ain%2520mobile%2520communication%2520networks.%2520We%2520believe%2520that%2520combining%2520LLMs%2520and%2520ML%2520models%250Aallows%2520both%2520to%2520leverage%2520their%2520respective%2520capabilities%2520and%2520achieve%2520better%250Aresults%2520than%2520either%2520model%2520alone.%2520To%2520support%2520this%2520idea%252C%2520we%2520begin%2520by%2520analyzing%250Athe%2520capabilities%2520of%2520LLMs%2520and%2520compare%2520them%2520with%2520traditional%2520ML%2520algorithms.%2520We%250Athen%2520explore%2520potential%2520LLM-based%2520applications%2520in%2520line%2520with%2520the%2520requirements%2520of%250Anext-generation%2520networks.%2520We%2520further%2520examine%2520the%2520integration%2520of%2520ML%2520and%2520LLMs%252C%250Adiscussing%2520how%2520they%2520can%2520be%2520used%2520together%2520in%2520mobile%2520networks.%2520Unlike%2520existing%250Astudies%252C%2520our%2520research%2520emphasizes%2520the%2520fusion%2520of%2520LLMs%2520with%2520traditional%2520ML-driven%250Anext-generation%2520networks%2520and%2520serves%2520as%2520a%2520comprehensive%2520refinement%2520of%2520existing%250Asurveys.%2520Finally%252C%2520we%2520provide%2520a%2520case%2520study%2520to%2520enhance%2520ML-based%2520network%2520intrusion%250Adetection%2520with%2520synthesized%2520data%2520generated%2520by%2520LLMs.%2520Our%2520case%2520study%2520further%250Ademonstrates%2520the%2520advantages%2520of%2520our%2520proposed%2520idea.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04276v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI-in-the-loop%3A%20Integrating%20LLMs%20and%20GPTs%20into%20the%20Next%0A%20%20Generation%20Networks&entry.906535625=Han%20Zhang%20and%20Akram%20Bin%20Sediq%20and%20Ali%20Afana%20and%20Melike%20Erol-Kantarci&entry.1292438233=%20%20In%20recent%20years%2C%20machine%20learning%20%28ML%29%20techniques%20have%20created%20numerous%0Aopportunities%20for%20intelligent%20mobile%20networks%20and%20have%20accelerated%20the%0Aautomation%20of%20network%20operations.%20However%2C%20complex%20network%20tasks%20may%20involve%0Avariables%20and%20considerations%20even%20beyond%20the%20capacity%20of%20traditional%20ML%0Aalgorithms.%20On%20the%20other%20hand%2C%20large%20language%20models%20%28LLMs%29%20have%20recently%0Aemerged%2C%20demonstrating%20near-human-level%20performance%20in%20cognitive%20tasks%20across%0Avarious%20fields.%20However%2C%20they%20remain%20prone%20to%20hallucinations%20and%20often%20lack%0Acommon%20sense%20in%20basic%20tasks.%20Therefore%2C%20they%20are%20regarded%20as%20assistive%20tools%0Afor%20humans.%20In%20this%20work%2C%20we%20propose%20the%20concept%20of%20%22generative%20AI-in-the-loop%22%0Aand%20utilize%20the%20semantic%20understanding%2C%20context%20awareness%2C%20and%20reasoning%0Aabilities%20of%20LLMs%20to%20assist%20humans%20in%20handling%20complex%20or%20unforeseen%20situations%0Ain%20mobile%20communication%20networks.%20We%20believe%20that%20combining%20LLMs%20and%20ML%20models%0Aallows%20both%20to%20leverage%20their%20respective%20capabilities%20and%20achieve%20better%0Aresults%20than%20either%20model%20alone.%20To%20support%20this%20idea%2C%20we%20begin%20by%20analyzing%0Athe%20capabilities%20of%20LLMs%20and%20compare%20them%20with%20traditional%20ML%20algorithms.%20We%0Athen%20explore%20potential%20LLM-based%20applications%20in%20line%20with%20the%20requirements%20of%0Anext-generation%20networks.%20We%20further%20examine%20the%20integration%20of%20ML%20and%20LLMs%2C%0Adiscussing%20how%20they%20can%20be%20used%20together%20in%20mobile%20networks.%20Unlike%20existing%0Astudies%2C%20our%20research%20emphasizes%20the%20fusion%20of%20LLMs%20with%20traditional%20ML-driven%0Anext-generation%20networks%20and%20serves%20as%20a%20comprehensive%20refinement%20of%20existing%0Asurveys.%20Finally%2C%20we%20provide%20a%20case%20study%20to%20enhance%20ML-based%20network%20intrusion%0Adetection%20with%20synthesized%20data%20generated%20by%20LLMs.%20Our%20case%20study%20further%0Ademonstrates%20the%20advantages%20of%20our%20proposed%20idea.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04276v1&entry.124074799=Read"},
{"title": "Learning to Generate Instruction Tuning Datasets for Zero-Shot Task\n  Adaptation", "author": "Nihal V. Nayak and Yiyang Nan and Avi Trost and Stephen H. Bach", "abstract": "  We introduce Bonito, an open-source model for conditional task generation\nthat converts unannotated text into task-specific training datasets for\ninstruction tuning. We aim to enable zero-shot task adaptation of large\nlanguage models on users' specialized, private data. We train Bonito by\nfine-tuning a pretrained large language model on a new large-scale dataset with\n1.65M examples created by remixing existing instruction tuning datasets into\nmeta-templates. The meta-templates for a dataset produce training examples\nwhere the input is the unannotated text and the task attribute and the output\nconsists of the instruction and the response. We use Bonito to generate\nsynthetic tasks for seven datasets from specialized domains with unannotated\ntext across three task types -- yes-no question answering, extractive question\nanswering, and natural language inference -- and adapt language models. We show\nthat Bonito significantly improves the average performance of pretrained and\ninstruction tuned models over the de facto self supervised baseline. For\nexample, adapting Mistral-Instruct-v2 and instruction tuned variants of Mistral\nand Llama2 with Bonito improves the strong zero-shot performance by 22.1 F1\npoints whereas the next word prediction objective undoes some of the benefits\nof instruction tuning and reduces the average performance by 0.8 F1 points. We\nconduct additional experiments with Bonito to understand the effects of the\ndomain, the size of the training set, and the choice of alternative synthetic\ntask generators. Overall, we show that learning with synthetic instruction\ntuning datasets is an effective way to adapt language models to new domains.\nThe model, dataset, and code are available at\nhttps://github.com/BatsResearch/bonito.\n", "link": "http://arxiv.org/abs/2402.18334v2", "date": "2024-06-06", "relevancy": 2.5509, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5152}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.508}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Generate%20Instruction%20Tuning%20Datasets%20for%20Zero-Shot%20Task%0A%20%20Adaptation&body=Title%3A%20Learning%20to%20Generate%20Instruction%20Tuning%20Datasets%20for%20Zero-Shot%20Task%0A%20%20Adaptation%0AAuthor%3A%20Nihal%20V.%20Nayak%20and%20Yiyang%20Nan%20and%20Avi%20Trost%20and%20Stephen%20H.%20Bach%0AAbstract%3A%20%20%20We%20introduce%20Bonito%2C%20an%20open-source%20model%20for%20conditional%20task%20generation%0Athat%20converts%20unannotated%20text%20into%20task-specific%20training%20datasets%20for%0Ainstruction%20tuning.%20We%20aim%20to%20enable%20zero-shot%20task%20adaptation%20of%20large%0Alanguage%20models%20on%20users%27%20specialized%2C%20private%20data.%20We%20train%20Bonito%20by%0Afine-tuning%20a%20pretrained%20large%20language%20model%20on%20a%20new%20large-scale%20dataset%20with%0A1.65M%20examples%20created%20by%20remixing%20existing%20instruction%20tuning%20datasets%20into%0Ameta-templates.%20The%20meta-templates%20for%20a%20dataset%20produce%20training%20examples%0Awhere%20the%20input%20is%20the%20unannotated%20text%20and%20the%20task%20attribute%20and%20the%20output%0Aconsists%20of%20the%20instruction%20and%20the%20response.%20We%20use%20Bonito%20to%20generate%0Asynthetic%20tasks%20for%20seven%20datasets%20from%20specialized%20domains%20with%20unannotated%0Atext%20across%20three%20task%20types%20--%20yes-no%20question%20answering%2C%20extractive%20question%0Aanswering%2C%20and%20natural%20language%20inference%20--%20and%20adapt%20language%20models.%20We%20show%0Athat%20Bonito%20significantly%20improves%20the%20average%20performance%20of%20pretrained%20and%0Ainstruction%20tuned%20models%20over%20the%20de%20facto%20self%20supervised%20baseline.%20For%0Aexample%2C%20adapting%20Mistral-Instruct-v2%20and%20instruction%20tuned%20variants%20of%20Mistral%0Aand%20Llama2%20with%20Bonito%20improves%20the%20strong%20zero-shot%20performance%20by%2022.1%20F1%0Apoints%20whereas%20the%20next%20word%20prediction%20objective%20undoes%20some%20of%20the%20benefits%0Aof%20instruction%20tuning%20and%20reduces%20the%20average%20performance%20by%200.8%20F1%20points.%20We%0Aconduct%20additional%20experiments%20with%20Bonito%20to%20understand%20the%20effects%20of%20the%0Adomain%2C%20the%20size%20of%20the%20training%20set%2C%20and%20the%20choice%20of%20alternative%20synthetic%0Atask%20generators.%20Overall%2C%20we%20show%20that%20learning%20with%20synthetic%20instruction%0Atuning%20datasets%20is%20an%20effective%20way%20to%20adapt%20language%20models%20to%20new%20domains.%0AThe%20model%2C%20dataset%2C%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/BatsResearch/bonito.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18334v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Generate%2520Instruction%2520Tuning%2520Datasets%2520for%2520Zero-Shot%2520Task%250A%2520%2520Adaptation%26entry.906535625%3DNihal%2520V.%2520Nayak%2520and%2520Yiyang%2520Nan%2520and%2520Avi%2520Trost%2520and%2520Stephen%2520H.%2520Bach%26entry.1292438233%3D%2520%2520We%2520introduce%2520Bonito%252C%2520an%2520open-source%2520model%2520for%2520conditional%2520task%2520generation%250Athat%2520converts%2520unannotated%2520text%2520into%2520task-specific%2520training%2520datasets%2520for%250Ainstruction%2520tuning.%2520We%2520aim%2520to%2520enable%2520zero-shot%2520task%2520adaptation%2520of%2520large%250Alanguage%2520models%2520on%2520users%2527%2520specialized%252C%2520private%2520data.%2520We%2520train%2520Bonito%2520by%250Afine-tuning%2520a%2520pretrained%2520large%2520language%2520model%2520on%2520a%2520new%2520large-scale%2520dataset%2520with%250A1.65M%2520examples%2520created%2520by%2520remixing%2520existing%2520instruction%2520tuning%2520datasets%2520into%250Ameta-templates.%2520The%2520meta-templates%2520for%2520a%2520dataset%2520produce%2520training%2520examples%250Awhere%2520the%2520input%2520is%2520the%2520unannotated%2520text%2520and%2520the%2520task%2520attribute%2520and%2520the%2520output%250Aconsists%2520of%2520the%2520instruction%2520and%2520the%2520response.%2520We%2520use%2520Bonito%2520to%2520generate%250Asynthetic%2520tasks%2520for%2520seven%2520datasets%2520from%2520specialized%2520domains%2520with%2520unannotated%250Atext%2520across%2520three%2520task%2520types%2520--%2520yes-no%2520question%2520answering%252C%2520extractive%2520question%250Aanswering%252C%2520and%2520natural%2520language%2520inference%2520--%2520and%2520adapt%2520language%2520models.%2520We%2520show%250Athat%2520Bonito%2520significantly%2520improves%2520the%2520average%2520performance%2520of%2520pretrained%2520and%250Ainstruction%2520tuned%2520models%2520over%2520the%2520de%2520facto%2520self%2520supervised%2520baseline.%2520For%250Aexample%252C%2520adapting%2520Mistral-Instruct-v2%2520and%2520instruction%2520tuned%2520variants%2520of%2520Mistral%250Aand%2520Llama2%2520with%2520Bonito%2520improves%2520the%2520strong%2520zero-shot%2520performance%2520by%252022.1%2520F1%250Apoints%2520whereas%2520the%2520next%2520word%2520prediction%2520objective%2520undoes%2520some%2520of%2520the%2520benefits%250Aof%2520instruction%2520tuning%2520and%2520reduces%2520the%2520average%2520performance%2520by%25200.8%2520F1%2520points.%2520We%250Aconduct%2520additional%2520experiments%2520with%2520Bonito%2520to%2520understand%2520the%2520effects%2520of%2520the%250Adomain%252C%2520the%2520size%2520of%2520the%2520training%2520set%252C%2520and%2520the%2520choice%2520of%2520alternative%2520synthetic%250Atask%2520generators.%2520Overall%252C%2520we%2520show%2520that%2520learning%2520with%2520synthetic%2520instruction%250Atuning%2520datasets%2520is%2520an%2520effective%2520way%2520to%2520adapt%2520language%2520models%2520to%2520new%2520domains.%250AThe%2520model%252C%2520dataset%252C%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/BatsResearch/bonito.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18334v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Generate%20Instruction%20Tuning%20Datasets%20for%20Zero-Shot%20Task%0A%20%20Adaptation&entry.906535625=Nihal%20V.%20Nayak%20and%20Yiyang%20Nan%20and%20Avi%20Trost%20and%20Stephen%20H.%20Bach&entry.1292438233=%20%20We%20introduce%20Bonito%2C%20an%20open-source%20model%20for%20conditional%20task%20generation%0Athat%20converts%20unannotated%20text%20into%20task-specific%20training%20datasets%20for%0Ainstruction%20tuning.%20We%20aim%20to%20enable%20zero-shot%20task%20adaptation%20of%20large%0Alanguage%20models%20on%20users%27%20specialized%2C%20private%20data.%20We%20train%20Bonito%20by%0Afine-tuning%20a%20pretrained%20large%20language%20model%20on%20a%20new%20large-scale%20dataset%20with%0A1.65M%20examples%20created%20by%20remixing%20existing%20instruction%20tuning%20datasets%20into%0Ameta-templates.%20The%20meta-templates%20for%20a%20dataset%20produce%20training%20examples%0Awhere%20the%20input%20is%20the%20unannotated%20text%20and%20the%20task%20attribute%20and%20the%20output%0Aconsists%20of%20the%20instruction%20and%20the%20response.%20We%20use%20Bonito%20to%20generate%0Asynthetic%20tasks%20for%20seven%20datasets%20from%20specialized%20domains%20with%20unannotated%0Atext%20across%20three%20task%20types%20--%20yes-no%20question%20answering%2C%20extractive%20question%0Aanswering%2C%20and%20natural%20language%20inference%20--%20and%20adapt%20language%20models.%20We%20show%0Athat%20Bonito%20significantly%20improves%20the%20average%20performance%20of%20pretrained%20and%0Ainstruction%20tuned%20models%20over%20the%20de%20facto%20self%20supervised%20baseline.%20For%0Aexample%2C%20adapting%20Mistral-Instruct-v2%20and%20instruction%20tuned%20variants%20of%20Mistral%0Aand%20Llama2%20with%20Bonito%20improves%20the%20strong%20zero-shot%20performance%20by%2022.1%20F1%0Apoints%20whereas%20the%20next%20word%20prediction%20objective%20undoes%20some%20of%20the%20benefits%0Aof%20instruction%20tuning%20and%20reduces%20the%20average%20performance%20by%200.8%20F1%20points.%20We%0Aconduct%20additional%20experiments%20with%20Bonito%20to%20understand%20the%20effects%20of%20the%0Adomain%2C%20the%20size%20of%20the%20training%20set%2C%20and%20the%20choice%20of%20alternative%20synthetic%0Atask%20generators.%20Overall%2C%20we%20show%20that%20learning%20with%20synthetic%20instruction%0Atuning%20datasets%20is%20an%20effective%20way%20to%20adapt%20language%20models%20to%20new%20domains.%0AThe%20model%2C%20dataset%2C%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/BatsResearch/bonito.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18334v2&entry.124074799=Read"},
{"title": "AdaLomo: Low-memory Optimization with Adaptive Learning Rate", "author": "Kai Lv and Hang Yan and Qipeng Guo and Haijun Lv and Xipeng Qiu", "abstract": "  Large language models have achieved remarkable success, but their extensive\nparameter size necessitates substantial memory for training, thereby setting a\nhigh threshold. While the recently proposed low-memory optimization (LOMO)\nreduces memory footprint, its optimization technique, akin to stochastic\ngradient descent, is sensitive to hyper-parameters and exhibits suboptimal\nconvergence, failing to match the performance of the prevailing optimizer for\nlarge language models, AdamW. Through empirical analysis of the Adam optimizer,\nwe found that, compared to momentum, the adaptive learning rate is more\ncritical for bridging the gap. Building on this insight, we introduce the\nlow-memory optimization with adaptive learning rate (AdaLomo), which offers an\nadaptive learning rate for each parameter. To maintain memory efficiency, we\nemploy non-negative matrix factorization for the second-order moment estimation\nin the optimizer state. Additionally, we suggest the use of a grouped update\nnormalization to stabilize convergence. Our experiments with instruction-tuning\nand further pre-training demonstrate that AdaLomo achieves results on par with\nAdamW, while significantly reducing memory requirements, thereby lowering the\nhardware barrier to training large language models. The code is accessible at\nhttps://github.com/OpenLMLab/LOMO.\n", "link": "http://arxiv.org/abs/2310.10195v3", "date": "2024-06-06", "relevancy": 2.5232, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5151}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5069}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaLomo%3A%20Low-memory%20Optimization%20with%20Adaptive%20Learning%20Rate&body=Title%3A%20AdaLomo%3A%20Low-memory%20Optimization%20with%20Adaptive%20Learning%20Rate%0AAuthor%3A%20Kai%20Lv%20and%20Hang%20Yan%20and%20Qipeng%20Guo%20and%20Haijun%20Lv%20and%20Xipeng%20Qiu%0AAbstract%3A%20%20%20Large%20language%20models%20have%20achieved%20remarkable%20success%2C%20but%20their%20extensive%0Aparameter%20size%20necessitates%20substantial%20memory%20for%20training%2C%20thereby%20setting%20a%0Ahigh%20threshold.%20While%20the%20recently%20proposed%20low-memory%20optimization%20%28LOMO%29%0Areduces%20memory%20footprint%2C%20its%20optimization%20technique%2C%20akin%20to%20stochastic%0Agradient%20descent%2C%20is%20sensitive%20to%20hyper-parameters%20and%20exhibits%20suboptimal%0Aconvergence%2C%20failing%20to%20match%20the%20performance%20of%20the%20prevailing%20optimizer%20for%0Alarge%20language%20models%2C%20AdamW.%20Through%20empirical%20analysis%20of%20the%20Adam%20optimizer%2C%0Awe%20found%20that%2C%20compared%20to%20momentum%2C%20the%20adaptive%20learning%20rate%20is%20more%0Acritical%20for%20bridging%20the%20gap.%20Building%20on%20this%20insight%2C%20we%20introduce%20the%0Alow-memory%20optimization%20with%20adaptive%20learning%20rate%20%28AdaLomo%29%2C%20which%20offers%20an%0Aadaptive%20learning%20rate%20for%20each%20parameter.%20To%20maintain%20memory%20efficiency%2C%20we%0Aemploy%20non-negative%20matrix%20factorization%20for%20the%20second-order%20moment%20estimation%0Ain%20the%20optimizer%20state.%20Additionally%2C%20we%20suggest%20the%20use%20of%20a%20grouped%20update%0Anormalization%20to%20stabilize%20convergence.%20Our%20experiments%20with%20instruction-tuning%0Aand%20further%20pre-training%20demonstrate%20that%20AdaLomo%20achieves%20results%20on%20par%20with%0AAdamW%2C%20while%20significantly%20reducing%20memory%20requirements%2C%20thereby%20lowering%20the%0Ahardware%20barrier%20to%20training%20large%20language%20models.%20The%20code%20is%20accessible%20at%0Ahttps%3A//github.com/OpenLMLab/LOMO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.10195v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaLomo%253A%2520Low-memory%2520Optimization%2520with%2520Adaptive%2520Learning%2520Rate%26entry.906535625%3DKai%2520Lv%2520and%2520Hang%2520Yan%2520and%2520Qipeng%2520Guo%2520and%2520Haijun%2520Lv%2520and%2520Xipeng%2520Qiu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520achieved%2520remarkable%2520success%252C%2520but%2520their%2520extensive%250Aparameter%2520size%2520necessitates%2520substantial%2520memory%2520for%2520training%252C%2520thereby%2520setting%2520a%250Ahigh%2520threshold.%2520While%2520the%2520recently%2520proposed%2520low-memory%2520optimization%2520%2528LOMO%2529%250Areduces%2520memory%2520footprint%252C%2520its%2520optimization%2520technique%252C%2520akin%2520to%2520stochastic%250Agradient%2520descent%252C%2520is%2520sensitive%2520to%2520hyper-parameters%2520and%2520exhibits%2520suboptimal%250Aconvergence%252C%2520failing%2520to%2520match%2520the%2520performance%2520of%2520the%2520prevailing%2520optimizer%2520for%250Alarge%2520language%2520models%252C%2520AdamW.%2520Through%2520empirical%2520analysis%2520of%2520the%2520Adam%2520optimizer%252C%250Awe%2520found%2520that%252C%2520compared%2520to%2520momentum%252C%2520the%2520adaptive%2520learning%2520rate%2520is%2520more%250Acritical%2520for%2520bridging%2520the%2520gap.%2520Building%2520on%2520this%2520insight%252C%2520we%2520introduce%2520the%250Alow-memory%2520optimization%2520with%2520adaptive%2520learning%2520rate%2520%2528AdaLomo%2529%252C%2520which%2520offers%2520an%250Aadaptive%2520learning%2520rate%2520for%2520each%2520parameter.%2520To%2520maintain%2520memory%2520efficiency%252C%2520we%250Aemploy%2520non-negative%2520matrix%2520factorization%2520for%2520the%2520second-order%2520moment%2520estimation%250Ain%2520the%2520optimizer%2520state.%2520Additionally%252C%2520we%2520suggest%2520the%2520use%2520of%2520a%2520grouped%2520update%250Anormalization%2520to%2520stabilize%2520convergence.%2520Our%2520experiments%2520with%2520instruction-tuning%250Aand%2520further%2520pre-training%2520demonstrate%2520that%2520AdaLomo%2520achieves%2520results%2520on%2520par%2520with%250AAdamW%252C%2520while%2520significantly%2520reducing%2520memory%2520requirements%252C%2520thereby%2520lowering%2520the%250Ahardware%2520barrier%2520to%2520training%2520large%2520language%2520models.%2520The%2520code%2520is%2520accessible%2520at%250Ahttps%253A//github.com/OpenLMLab/LOMO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.10195v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaLomo%3A%20Low-memory%20Optimization%20with%20Adaptive%20Learning%20Rate&entry.906535625=Kai%20Lv%20and%20Hang%20Yan%20and%20Qipeng%20Guo%20and%20Haijun%20Lv%20and%20Xipeng%20Qiu&entry.1292438233=%20%20Large%20language%20models%20have%20achieved%20remarkable%20success%2C%20but%20their%20extensive%0Aparameter%20size%20necessitates%20substantial%20memory%20for%20training%2C%20thereby%20setting%20a%0Ahigh%20threshold.%20While%20the%20recently%20proposed%20low-memory%20optimization%20%28LOMO%29%0Areduces%20memory%20footprint%2C%20its%20optimization%20technique%2C%20akin%20to%20stochastic%0Agradient%20descent%2C%20is%20sensitive%20to%20hyper-parameters%20and%20exhibits%20suboptimal%0Aconvergence%2C%20failing%20to%20match%20the%20performance%20of%20the%20prevailing%20optimizer%20for%0Alarge%20language%20models%2C%20AdamW.%20Through%20empirical%20analysis%20of%20the%20Adam%20optimizer%2C%0Awe%20found%20that%2C%20compared%20to%20momentum%2C%20the%20adaptive%20learning%20rate%20is%20more%0Acritical%20for%20bridging%20the%20gap.%20Building%20on%20this%20insight%2C%20we%20introduce%20the%0Alow-memory%20optimization%20with%20adaptive%20learning%20rate%20%28AdaLomo%29%2C%20which%20offers%20an%0Aadaptive%20learning%20rate%20for%20each%20parameter.%20To%20maintain%20memory%20efficiency%2C%20we%0Aemploy%20non-negative%20matrix%20factorization%20for%20the%20second-order%20moment%20estimation%0Ain%20the%20optimizer%20state.%20Additionally%2C%20we%20suggest%20the%20use%20of%20a%20grouped%20update%0Anormalization%20to%20stabilize%20convergence.%20Our%20experiments%20with%20instruction-tuning%0Aand%20further%20pre-training%20demonstrate%20that%20AdaLomo%20achieves%20results%20on%20par%20with%0AAdamW%2C%20while%20significantly%20reducing%20memory%20requirements%2C%20thereby%20lowering%20the%0Ahardware%20barrier%20to%20training%20large%20language%20models.%20The%20code%20is%20accessible%20at%0Ahttps%3A//github.com/OpenLMLab/LOMO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10195v3&entry.124074799=Read"},
{"title": "Fast Iterative Region Inflation for Computing Large 2-D/3-D Convex\n  Regions of Obstacle-Free Space", "author": "Qianhao Wang and Zhepei Wang and Mingyang Wang and Jialin Ji and Zhichao Han and Tianyue Wu and Rui Jin and Yuman Gao and Chao Xu and Fei Gao", "abstract": "  Convex polytopes have compact representations and exhibit convexity, which\nmakes them suitable for abstracting obstacle-free spaces from various\nenvironments. Existing methods for generating convex polytopes always struggle\nto strike a balance between two requirements, producing high-quality polytope\nand efficiency. Moreover, another crucial requirement for convex polytopes to\naccurately contain certain seed point sets, such as a robot or a front-end\npath, is proposed in various tasks, which we refer to as manageability. In this\npaper, we show that we can achieve generation of high-quality convex polytope\nwhile ensuring both efficiency and manageability simultaneously, by introducing\nFast Iterative Regional Inflation (FIRI).FIRI consists of two iteratively\nexecuted submodules: Restrictive Inflation (RsI) and computation of the Maximum\nVolume Inscribed Ellipsoid (MVIE) of convex polytope. By explicitly\nincorporating constraints that include the seed point set, RsI guarantees\nmanageability. Meanwhile, the iterative monotonic optimization of MVIE, which\nserves as a lower bound of the volume of convex polytope, ensures high-quality\nresults of FIRI. In terms of efficiency, we design methods tailored to the\nlow-dimensional and multi-constrained nature of both modules, resulting in\norders of magnitude improvement compared to generic solvers. Notably, for 2-D\nMVIE, we present a novel analytical algorithm that achieves linear-time\ncomplexity for the first time, further enhancing the efficiency of FIRI in the\n2-D scenario. Extensive benchmarks conducted against state-of-the-art methods\nvalidate the superior performance of FIRI in terms of quality, manageability,\nand efficiency. Furthermore, various real-world applications showcase the\ngenerality and practicality of FIRI. The high-performance code of FIRI will be\nopen-sourced for the reference of the community.\n", "link": "http://arxiv.org/abs/2403.02977v2", "date": "2024-06-06", "relevancy": 2.5123, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5036}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5019}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Iterative%20Region%20Inflation%20for%20Computing%20Large%202-D/3-D%20Convex%0A%20%20Regions%20of%20Obstacle-Free%20Space&body=Title%3A%20Fast%20Iterative%20Region%20Inflation%20for%20Computing%20Large%202-D/3-D%20Convex%0A%20%20Regions%20of%20Obstacle-Free%20Space%0AAuthor%3A%20Qianhao%20Wang%20and%20Zhepei%20Wang%20and%20Mingyang%20Wang%20and%20Jialin%20Ji%20and%20Zhichao%20Han%20and%20Tianyue%20Wu%20and%20Rui%20Jin%20and%20Yuman%20Gao%20and%20Chao%20Xu%20and%20Fei%20Gao%0AAbstract%3A%20%20%20Convex%20polytopes%20have%20compact%20representations%20and%20exhibit%20convexity%2C%20which%0Amakes%20them%20suitable%20for%20abstracting%20obstacle-free%20spaces%20from%20various%0Aenvironments.%20Existing%20methods%20for%20generating%20convex%20polytopes%20always%20struggle%0Ato%20strike%20a%20balance%20between%20two%20requirements%2C%20producing%20high-quality%20polytope%0Aand%20efficiency.%20Moreover%2C%20another%20crucial%20requirement%20for%20convex%20polytopes%20to%0Aaccurately%20contain%20certain%20seed%20point%20sets%2C%20such%20as%20a%20robot%20or%20a%20front-end%0Apath%2C%20is%20proposed%20in%20various%20tasks%2C%20which%20we%20refer%20to%20as%20manageability.%20In%20this%0Apaper%2C%20we%20show%20that%20we%20can%20achieve%20generation%20of%20high-quality%20convex%20polytope%0Awhile%20ensuring%20both%20efficiency%20and%20manageability%20simultaneously%2C%20by%20introducing%0AFast%20Iterative%20Regional%20Inflation%20%28FIRI%29.FIRI%20consists%20of%20two%20iteratively%0Aexecuted%20submodules%3A%20Restrictive%20Inflation%20%28RsI%29%20and%20computation%20of%20the%20Maximum%0AVolume%20Inscribed%20Ellipsoid%20%28MVIE%29%20of%20convex%20polytope.%20By%20explicitly%0Aincorporating%20constraints%20that%20include%20the%20seed%20point%20set%2C%20RsI%20guarantees%0Amanageability.%20Meanwhile%2C%20the%20iterative%20monotonic%20optimization%20of%20MVIE%2C%20which%0Aserves%20as%20a%20lower%20bound%20of%20the%20volume%20of%20convex%20polytope%2C%20ensures%20high-quality%0Aresults%20of%20FIRI.%20In%20terms%20of%20efficiency%2C%20we%20design%20methods%20tailored%20to%20the%0Alow-dimensional%20and%20multi-constrained%20nature%20of%20both%20modules%2C%20resulting%20in%0Aorders%20of%20magnitude%20improvement%20compared%20to%20generic%20solvers.%20Notably%2C%20for%202-D%0AMVIE%2C%20we%20present%20a%20novel%20analytical%20algorithm%20that%20achieves%20linear-time%0Acomplexity%20for%20the%20first%20time%2C%20further%20enhancing%20the%20efficiency%20of%20FIRI%20in%20the%0A2-D%20scenario.%20Extensive%20benchmarks%20conducted%20against%20state-of-the-art%20methods%0Avalidate%20the%20superior%20performance%20of%20FIRI%20in%20terms%20of%20quality%2C%20manageability%2C%0Aand%20efficiency.%20Furthermore%2C%20various%20real-world%20applications%20showcase%20the%0Agenerality%20and%20practicality%20of%20FIRI.%20The%20high-performance%20code%20of%20FIRI%20will%20be%0Aopen-sourced%20for%20the%20reference%20of%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02977v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Iterative%2520Region%2520Inflation%2520for%2520Computing%2520Large%25202-D/3-D%2520Convex%250A%2520%2520Regions%2520of%2520Obstacle-Free%2520Space%26entry.906535625%3DQianhao%2520Wang%2520and%2520Zhepei%2520Wang%2520and%2520Mingyang%2520Wang%2520and%2520Jialin%2520Ji%2520and%2520Zhichao%2520Han%2520and%2520Tianyue%2520Wu%2520and%2520Rui%2520Jin%2520and%2520Yuman%2520Gao%2520and%2520Chao%2520Xu%2520and%2520Fei%2520Gao%26entry.1292438233%3D%2520%2520Convex%2520polytopes%2520have%2520compact%2520representations%2520and%2520exhibit%2520convexity%252C%2520which%250Amakes%2520them%2520suitable%2520for%2520abstracting%2520obstacle-free%2520spaces%2520from%2520various%250Aenvironments.%2520Existing%2520methods%2520for%2520generating%2520convex%2520polytopes%2520always%2520struggle%250Ato%2520strike%2520a%2520balance%2520between%2520two%2520requirements%252C%2520producing%2520high-quality%2520polytope%250Aand%2520efficiency.%2520Moreover%252C%2520another%2520crucial%2520requirement%2520for%2520convex%2520polytopes%2520to%250Aaccurately%2520contain%2520certain%2520seed%2520point%2520sets%252C%2520such%2520as%2520a%2520robot%2520or%2520a%2520front-end%250Apath%252C%2520is%2520proposed%2520in%2520various%2520tasks%252C%2520which%2520we%2520refer%2520to%2520as%2520manageability.%2520In%2520this%250Apaper%252C%2520we%2520show%2520that%2520we%2520can%2520achieve%2520generation%2520of%2520high-quality%2520convex%2520polytope%250Awhile%2520ensuring%2520both%2520efficiency%2520and%2520manageability%2520simultaneously%252C%2520by%2520introducing%250AFast%2520Iterative%2520Regional%2520Inflation%2520%2528FIRI%2529.FIRI%2520consists%2520of%2520two%2520iteratively%250Aexecuted%2520submodules%253A%2520Restrictive%2520Inflation%2520%2528RsI%2529%2520and%2520computation%2520of%2520the%2520Maximum%250AVolume%2520Inscribed%2520Ellipsoid%2520%2528MVIE%2529%2520of%2520convex%2520polytope.%2520By%2520explicitly%250Aincorporating%2520constraints%2520that%2520include%2520the%2520seed%2520point%2520set%252C%2520RsI%2520guarantees%250Amanageability.%2520Meanwhile%252C%2520the%2520iterative%2520monotonic%2520optimization%2520of%2520MVIE%252C%2520which%250Aserves%2520as%2520a%2520lower%2520bound%2520of%2520the%2520volume%2520of%2520convex%2520polytope%252C%2520ensures%2520high-quality%250Aresults%2520of%2520FIRI.%2520In%2520terms%2520of%2520efficiency%252C%2520we%2520design%2520methods%2520tailored%2520to%2520the%250Alow-dimensional%2520and%2520multi-constrained%2520nature%2520of%2520both%2520modules%252C%2520resulting%2520in%250Aorders%2520of%2520magnitude%2520improvement%2520compared%2520to%2520generic%2520solvers.%2520Notably%252C%2520for%25202-D%250AMVIE%252C%2520we%2520present%2520a%2520novel%2520analytical%2520algorithm%2520that%2520achieves%2520linear-time%250Acomplexity%2520for%2520the%2520first%2520time%252C%2520further%2520enhancing%2520the%2520efficiency%2520of%2520FIRI%2520in%2520the%250A2-D%2520scenario.%2520Extensive%2520benchmarks%2520conducted%2520against%2520state-of-the-art%2520methods%250Avalidate%2520the%2520superior%2520performance%2520of%2520FIRI%2520in%2520terms%2520of%2520quality%252C%2520manageability%252C%250Aand%2520efficiency.%2520Furthermore%252C%2520various%2520real-world%2520applications%2520showcase%2520the%250Agenerality%2520and%2520practicality%2520of%2520FIRI.%2520The%2520high-performance%2520code%2520of%2520FIRI%2520will%2520be%250Aopen-sourced%2520for%2520the%2520reference%2520of%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.02977v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Iterative%20Region%20Inflation%20for%20Computing%20Large%202-D/3-D%20Convex%0A%20%20Regions%20of%20Obstacle-Free%20Space&entry.906535625=Qianhao%20Wang%20and%20Zhepei%20Wang%20and%20Mingyang%20Wang%20and%20Jialin%20Ji%20and%20Zhichao%20Han%20and%20Tianyue%20Wu%20and%20Rui%20Jin%20and%20Yuman%20Gao%20and%20Chao%20Xu%20and%20Fei%20Gao&entry.1292438233=%20%20Convex%20polytopes%20have%20compact%20representations%20and%20exhibit%20convexity%2C%20which%0Amakes%20them%20suitable%20for%20abstracting%20obstacle-free%20spaces%20from%20various%0Aenvironments.%20Existing%20methods%20for%20generating%20convex%20polytopes%20always%20struggle%0Ato%20strike%20a%20balance%20between%20two%20requirements%2C%20producing%20high-quality%20polytope%0Aand%20efficiency.%20Moreover%2C%20another%20crucial%20requirement%20for%20convex%20polytopes%20to%0Aaccurately%20contain%20certain%20seed%20point%20sets%2C%20such%20as%20a%20robot%20or%20a%20front-end%0Apath%2C%20is%20proposed%20in%20various%20tasks%2C%20which%20we%20refer%20to%20as%20manageability.%20In%20this%0Apaper%2C%20we%20show%20that%20we%20can%20achieve%20generation%20of%20high-quality%20convex%20polytope%0Awhile%20ensuring%20both%20efficiency%20and%20manageability%20simultaneously%2C%20by%20introducing%0AFast%20Iterative%20Regional%20Inflation%20%28FIRI%29.FIRI%20consists%20of%20two%20iteratively%0Aexecuted%20submodules%3A%20Restrictive%20Inflation%20%28RsI%29%20and%20computation%20of%20the%20Maximum%0AVolume%20Inscribed%20Ellipsoid%20%28MVIE%29%20of%20convex%20polytope.%20By%20explicitly%0Aincorporating%20constraints%20that%20include%20the%20seed%20point%20set%2C%20RsI%20guarantees%0Amanageability.%20Meanwhile%2C%20the%20iterative%20monotonic%20optimization%20of%20MVIE%2C%20which%0Aserves%20as%20a%20lower%20bound%20of%20the%20volume%20of%20convex%20polytope%2C%20ensures%20high-quality%0Aresults%20of%20FIRI.%20In%20terms%20of%20efficiency%2C%20we%20design%20methods%20tailored%20to%20the%0Alow-dimensional%20and%20multi-constrained%20nature%20of%20both%20modules%2C%20resulting%20in%0Aorders%20of%20magnitude%20improvement%20compared%20to%20generic%20solvers.%20Notably%2C%20for%202-D%0AMVIE%2C%20we%20present%20a%20novel%20analytical%20algorithm%20that%20achieves%20linear-time%0Acomplexity%20for%20the%20first%20time%2C%20further%20enhancing%20the%20efficiency%20of%20FIRI%20in%20the%0A2-D%20scenario.%20Extensive%20benchmarks%20conducted%20against%20state-of-the-art%20methods%0Avalidate%20the%20superior%20performance%20of%20FIRI%20in%20terms%20of%20quality%2C%20manageability%2C%0Aand%20efficiency.%20Furthermore%2C%20various%20real-world%20applications%20showcase%20the%0Agenerality%20and%20practicality%20of%20FIRI.%20The%20high-performance%20code%20of%20FIRI%20will%20be%0Aopen-sourced%20for%20the%20reference%20of%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02977v2&entry.124074799=Read"},
{"title": "Spatial-Temporal Graph Representation Learning for Tactical Networks\n  Future State Prediction", "author": "Liu Junhua and Albrethsen Justin and Goh Lincoln and Yau David and Lim Kwan Hui", "abstract": "  Resource allocation in tactical ad-hoc networks presents unique challenges\ndue to their dynamic and multi-hop nature. Accurate prediction of future\nnetwork connectivity is essential for effective resource allocation in such\nenvironments. In this paper, we introduce the Spatial-Temporal Graph\nEncoder-Decoder (STGED) framework for Tactical Communication Networks that\nleverages both spatial and temporal features of network states to learn latent\ntactical behaviors effectively. STGED hierarchically utilizes graph-based\nattention mechanism to spatially encode a series of communication network\nstates, leverages a recurrent neural network to temporally encode the evolution\nof states, and a fully-connected feed-forward network to decode the\nconnectivity in the future state. Through extensive experiments, we demonstrate\nthat STGED consistently outperforms baseline models by large margins across\ndifferent time-steps input, achieving an accuracy of up to 99.2\\% for the\nfuture state prediction task of tactical communication networks.\n", "link": "http://arxiv.org/abs/2403.13872v2", "date": "2024-06-06", "relevancy": 2.5015, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5119}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.507}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial-Temporal%20Graph%20Representation%20Learning%20for%20Tactical%20Networks%0A%20%20Future%20State%20Prediction&body=Title%3A%20Spatial-Temporal%20Graph%20Representation%20Learning%20for%20Tactical%20Networks%0A%20%20Future%20State%20Prediction%0AAuthor%3A%20Liu%20Junhua%20and%20Albrethsen%20Justin%20and%20Goh%20Lincoln%20and%20Yau%20David%20and%20Lim%20Kwan%20Hui%0AAbstract%3A%20%20%20Resource%20allocation%20in%20tactical%20ad-hoc%20networks%20presents%20unique%20challenges%0Adue%20to%20their%20dynamic%20and%20multi-hop%20nature.%20Accurate%20prediction%20of%20future%0Anetwork%20connectivity%20is%20essential%20for%20effective%20resource%20allocation%20in%20such%0Aenvironments.%20In%20this%20paper%2C%20we%20introduce%20the%20Spatial-Temporal%20Graph%0AEncoder-Decoder%20%28STGED%29%20framework%20for%20Tactical%20Communication%20Networks%20that%0Aleverages%20both%20spatial%20and%20temporal%20features%20of%20network%20states%20to%20learn%20latent%0Atactical%20behaviors%20effectively.%20STGED%20hierarchically%20utilizes%20graph-based%0Aattention%20mechanism%20to%20spatially%20encode%20a%20series%20of%20communication%20network%0Astates%2C%20leverages%20a%20recurrent%20neural%20network%20to%20temporally%20encode%20the%20evolution%0Aof%20states%2C%20and%20a%20fully-connected%20feed-forward%20network%20to%20decode%20the%0Aconnectivity%20in%20the%20future%20state.%20Through%20extensive%20experiments%2C%20we%20demonstrate%0Athat%20STGED%20consistently%20outperforms%20baseline%20models%20by%20large%20margins%20across%0Adifferent%20time-steps%20input%2C%20achieving%20an%20accuracy%20of%20up%20to%2099.2%5C%25%20for%20the%0Afuture%20state%20prediction%20task%20of%20tactical%20communication%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13872v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial-Temporal%2520Graph%2520Representation%2520Learning%2520for%2520Tactical%2520Networks%250A%2520%2520Future%2520State%2520Prediction%26entry.906535625%3DLiu%2520Junhua%2520and%2520Albrethsen%2520Justin%2520and%2520Goh%2520Lincoln%2520and%2520Yau%2520David%2520and%2520Lim%2520Kwan%2520Hui%26entry.1292438233%3D%2520%2520Resource%2520allocation%2520in%2520tactical%2520ad-hoc%2520networks%2520presents%2520unique%2520challenges%250Adue%2520to%2520their%2520dynamic%2520and%2520multi-hop%2520nature.%2520Accurate%2520prediction%2520of%2520future%250Anetwork%2520connectivity%2520is%2520essential%2520for%2520effective%2520resource%2520allocation%2520in%2520such%250Aenvironments.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Spatial-Temporal%2520Graph%250AEncoder-Decoder%2520%2528STGED%2529%2520framework%2520for%2520Tactical%2520Communication%2520Networks%2520that%250Aleverages%2520both%2520spatial%2520and%2520temporal%2520features%2520of%2520network%2520states%2520to%2520learn%2520latent%250Atactical%2520behaviors%2520effectively.%2520STGED%2520hierarchically%2520utilizes%2520graph-based%250Aattention%2520mechanism%2520to%2520spatially%2520encode%2520a%2520series%2520of%2520communication%2520network%250Astates%252C%2520leverages%2520a%2520recurrent%2520neural%2520network%2520to%2520temporally%2520encode%2520the%2520evolution%250Aof%2520states%252C%2520and%2520a%2520fully-connected%2520feed-forward%2520network%2520to%2520decode%2520the%250Aconnectivity%2520in%2520the%2520future%2520state.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%250Athat%2520STGED%2520consistently%2520outperforms%2520baseline%2520models%2520by%2520large%2520margins%2520across%250Adifferent%2520time-steps%2520input%252C%2520achieving%2520an%2520accuracy%2520of%2520up%2520to%252099.2%255C%2525%2520for%2520the%250Afuture%2520state%2520prediction%2520task%2520of%2520tactical%2520communication%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13872v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial-Temporal%20Graph%20Representation%20Learning%20for%20Tactical%20Networks%0A%20%20Future%20State%20Prediction&entry.906535625=Liu%20Junhua%20and%20Albrethsen%20Justin%20and%20Goh%20Lincoln%20and%20Yau%20David%20and%20Lim%20Kwan%20Hui&entry.1292438233=%20%20Resource%20allocation%20in%20tactical%20ad-hoc%20networks%20presents%20unique%20challenges%0Adue%20to%20their%20dynamic%20and%20multi-hop%20nature.%20Accurate%20prediction%20of%20future%0Anetwork%20connectivity%20is%20essential%20for%20effective%20resource%20allocation%20in%20such%0Aenvironments.%20In%20this%20paper%2C%20we%20introduce%20the%20Spatial-Temporal%20Graph%0AEncoder-Decoder%20%28STGED%29%20framework%20for%20Tactical%20Communication%20Networks%20that%0Aleverages%20both%20spatial%20and%20temporal%20features%20of%20network%20states%20to%20learn%20latent%0Atactical%20behaviors%20effectively.%20STGED%20hierarchically%20utilizes%20graph-based%0Aattention%20mechanism%20to%20spatially%20encode%20a%20series%20of%20communication%20network%0Astates%2C%20leverages%20a%20recurrent%20neural%20network%20to%20temporally%20encode%20the%20evolution%0Aof%20states%2C%20and%20a%20fully-connected%20feed-forward%20network%20to%20decode%20the%0Aconnectivity%20in%20the%20future%20state.%20Through%20extensive%20experiments%2C%20we%20demonstrate%0Athat%20STGED%20consistently%20outperforms%20baseline%20models%20by%20large%20margins%20across%0Adifferent%20time-steps%20input%2C%20achieving%20an%20accuracy%20of%20up%20to%2099.2%5C%25%20for%20the%0Afuture%20state%20prediction%20task%20of%20tactical%20communication%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13872v2&entry.124074799=Read"},
{"title": "Compressible Dynamics in Deep Overparameterized Low-Rank Learning &\n  Adaptation", "author": "Can Yaras and Peng Wang and Laura Balzano and Qing Qu", "abstract": "  While overparameterization in machine learning models offers great benefits\nin terms of optimization and generalization, it also leads to increased\ncomputational requirements as model sizes grow. In this work, we show that by\nleveraging the inherent low-dimensional structures of data and compressible\ndynamics within the model parameters, we can reap the benefits of\noverparameterization without the computational burdens. In practice, we\ndemonstrate the effectiveness of this approach for deep low-rank matrix\ncompletion as well as fine-tuning language models. Our approach is grounded in\ntheoretical findings for deep overparameterized low-rank matrix recovery, where\nwe show that the learning dynamics of each weight matrix are confined to an\ninvariant low-dimensional subspace. Consequently, we can construct and train\ncompact, highly compressed factorizations possessing the same benefits as their\noverparameterized counterparts. In the context of deep matrix completion, our\ntechnique substantially improves training efficiency while retaining the\nadvantages of overparameterization. For language model fine-tuning, we propose\na method called \"Deep LoRA\", which improves the existing low-rank adaptation\n(LoRA) technique, leading to reduced overfitting and a simplified\nhyperparameter setup, while maintaining comparable efficiency. We validate the\neffectiveness of Deep LoRA on natural language tasks, particularly when\nfine-tuning with limited data.\n", "link": "http://arxiv.org/abs/2406.04112v1", "date": "2024-06-06", "relevancy": 2.4675, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.519}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4811}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compressible%20Dynamics%20in%20Deep%20Overparameterized%20Low-Rank%20Learning%20%26%0A%20%20Adaptation&body=Title%3A%20Compressible%20Dynamics%20in%20Deep%20Overparameterized%20Low-Rank%20Learning%20%26%0A%20%20Adaptation%0AAuthor%3A%20Can%20Yaras%20and%20Peng%20Wang%20and%20Laura%20Balzano%20and%20Qing%20Qu%0AAbstract%3A%20%20%20While%20overparameterization%20in%20machine%20learning%20models%20offers%20great%20benefits%0Ain%20terms%20of%20optimization%20and%20generalization%2C%20it%20also%20leads%20to%20increased%0Acomputational%20requirements%20as%20model%20sizes%20grow.%20In%20this%20work%2C%20we%20show%20that%20by%0Aleveraging%20the%20inherent%20low-dimensional%20structures%20of%20data%20and%20compressible%0Adynamics%20within%20the%20model%20parameters%2C%20we%20can%20reap%20the%20benefits%20of%0Aoverparameterization%20without%20the%20computational%20burdens.%20In%20practice%2C%20we%0Ademonstrate%20the%20effectiveness%20of%20this%20approach%20for%20deep%20low-rank%20matrix%0Acompletion%20as%20well%20as%20fine-tuning%20language%20models.%20Our%20approach%20is%20grounded%20in%0Atheoretical%20findings%20for%20deep%20overparameterized%20low-rank%20matrix%20recovery%2C%20where%0Awe%20show%20that%20the%20learning%20dynamics%20of%20each%20weight%20matrix%20are%20confined%20to%20an%0Ainvariant%20low-dimensional%20subspace.%20Consequently%2C%20we%20can%20construct%20and%20train%0Acompact%2C%20highly%20compressed%20factorizations%20possessing%20the%20same%20benefits%20as%20their%0Aoverparameterized%20counterparts.%20In%20the%20context%20of%20deep%20matrix%20completion%2C%20our%0Atechnique%20substantially%20improves%20training%20efficiency%20while%20retaining%20the%0Aadvantages%20of%20overparameterization.%20For%20language%20model%20fine-tuning%2C%20we%20propose%0Aa%20method%20called%20%22Deep%20LoRA%22%2C%20which%20improves%20the%20existing%20low-rank%20adaptation%0A%28LoRA%29%20technique%2C%20leading%20to%20reduced%20overfitting%20and%20a%20simplified%0Ahyperparameter%20setup%2C%20while%20maintaining%20comparable%20efficiency.%20We%20validate%20the%0Aeffectiveness%20of%20Deep%20LoRA%20on%20natural%20language%20tasks%2C%20particularly%20when%0Afine-tuning%20with%20limited%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04112v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompressible%2520Dynamics%2520in%2520Deep%2520Overparameterized%2520Low-Rank%2520Learning%2520%2526%250A%2520%2520Adaptation%26entry.906535625%3DCan%2520Yaras%2520and%2520Peng%2520Wang%2520and%2520Laura%2520Balzano%2520and%2520Qing%2520Qu%26entry.1292438233%3D%2520%2520While%2520overparameterization%2520in%2520machine%2520learning%2520models%2520offers%2520great%2520benefits%250Ain%2520terms%2520of%2520optimization%2520and%2520generalization%252C%2520it%2520also%2520leads%2520to%2520increased%250Acomputational%2520requirements%2520as%2520model%2520sizes%2520grow.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520by%250Aleveraging%2520the%2520inherent%2520low-dimensional%2520structures%2520of%2520data%2520and%2520compressible%250Adynamics%2520within%2520the%2520model%2520parameters%252C%2520we%2520can%2520reap%2520the%2520benefits%2520of%250Aoverparameterization%2520without%2520the%2520computational%2520burdens.%2520In%2520practice%252C%2520we%250Ademonstrate%2520the%2520effectiveness%2520of%2520this%2520approach%2520for%2520deep%2520low-rank%2520matrix%250Acompletion%2520as%2520well%2520as%2520fine-tuning%2520language%2520models.%2520Our%2520approach%2520is%2520grounded%2520in%250Atheoretical%2520findings%2520for%2520deep%2520overparameterized%2520low-rank%2520matrix%2520recovery%252C%2520where%250Awe%2520show%2520that%2520the%2520learning%2520dynamics%2520of%2520each%2520weight%2520matrix%2520are%2520confined%2520to%2520an%250Ainvariant%2520low-dimensional%2520subspace.%2520Consequently%252C%2520we%2520can%2520construct%2520and%2520train%250Acompact%252C%2520highly%2520compressed%2520factorizations%2520possessing%2520the%2520same%2520benefits%2520as%2520their%250Aoverparameterized%2520counterparts.%2520In%2520the%2520context%2520of%2520deep%2520matrix%2520completion%252C%2520our%250Atechnique%2520substantially%2520improves%2520training%2520efficiency%2520while%2520retaining%2520the%250Aadvantages%2520of%2520overparameterization.%2520For%2520language%2520model%2520fine-tuning%252C%2520we%2520propose%250Aa%2520method%2520called%2520%2522Deep%2520LoRA%2522%252C%2520which%2520improves%2520the%2520existing%2520low-rank%2520adaptation%250A%2528LoRA%2529%2520technique%252C%2520leading%2520to%2520reduced%2520overfitting%2520and%2520a%2520simplified%250Ahyperparameter%2520setup%252C%2520while%2520maintaining%2520comparable%2520efficiency.%2520We%2520validate%2520the%250Aeffectiveness%2520of%2520Deep%2520LoRA%2520on%2520natural%2520language%2520tasks%252C%2520particularly%2520when%250Afine-tuning%2520with%2520limited%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04112v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compressible%20Dynamics%20in%20Deep%20Overparameterized%20Low-Rank%20Learning%20%26%0A%20%20Adaptation&entry.906535625=Can%20Yaras%20and%20Peng%20Wang%20and%20Laura%20Balzano%20and%20Qing%20Qu&entry.1292438233=%20%20While%20overparameterization%20in%20machine%20learning%20models%20offers%20great%20benefits%0Ain%20terms%20of%20optimization%20and%20generalization%2C%20it%20also%20leads%20to%20increased%0Acomputational%20requirements%20as%20model%20sizes%20grow.%20In%20this%20work%2C%20we%20show%20that%20by%0Aleveraging%20the%20inherent%20low-dimensional%20structures%20of%20data%20and%20compressible%0Adynamics%20within%20the%20model%20parameters%2C%20we%20can%20reap%20the%20benefits%20of%0Aoverparameterization%20without%20the%20computational%20burdens.%20In%20practice%2C%20we%0Ademonstrate%20the%20effectiveness%20of%20this%20approach%20for%20deep%20low-rank%20matrix%0Acompletion%20as%20well%20as%20fine-tuning%20language%20models.%20Our%20approach%20is%20grounded%20in%0Atheoretical%20findings%20for%20deep%20overparameterized%20low-rank%20matrix%20recovery%2C%20where%0Awe%20show%20that%20the%20learning%20dynamics%20of%20each%20weight%20matrix%20are%20confined%20to%20an%0Ainvariant%20low-dimensional%20subspace.%20Consequently%2C%20we%20can%20construct%20and%20train%0Acompact%2C%20highly%20compressed%20factorizations%20possessing%20the%20same%20benefits%20as%20their%0Aoverparameterized%20counterparts.%20In%20the%20context%20of%20deep%20matrix%20completion%2C%20our%0Atechnique%20substantially%20improves%20training%20efficiency%20while%20retaining%20the%0Aadvantages%20of%20overparameterization.%20For%20language%20model%20fine-tuning%2C%20we%20propose%0Aa%20method%20called%20%22Deep%20LoRA%22%2C%20which%20improves%20the%20existing%20low-rank%20adaptation%0A%28LoRA%29%20technique%2C%20leading%20to%20reduced%20overfitting%20and%20a%20simplified%0Ahyperparameter%20setup%2C%20while%20maintaining%20comparable%20efficiency.%20We%20validate%20the%0Aeffectiveness%20of%20Deep%20LoRA%20on%20natural%20language%20tasks%2C%20particularly%20when%0Afine-tuning%20with%20limited%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04112v1&entry.124074799=Read"},
{"title": "Understanding Retrieval Robustness for Retrieval-Augmented Image\n  Captioning", "author": "Wenyan Li and Jiaang Li and Rita Ramos and Raphael Tang and Desmond Elliott", "abstract": "  Recent advances in retrieval-augmented models for image captioning highlight\nthe benefit of retrieving related captions for efficient, lightweight models\nwith strong domain-transfer capabilities. While these models demonstrate the\nsuccess of retrieval augmentation, retrieval models are still far from perfect\nin practice: the retrieved information can sometimes mislead the model,\nresulting in incorrect generation and worse performance. In this paper, we\nanalyze the robustness of a retrieval-augmented captioning model SmallCap. Our\nanalysis shows that the model is sensitive to tokens that appear in the\nmajority of the retrieved captions, and the input attribution shows that those\ntokens are likely copied into the generated output. Given these findings, we\npropose to train the model by sampling retrieved captions from more diverse\nsets. This decreases the chance that the model learns to copy majority tokens,\nand improves both in-domain and cross-domain performance.\n", "link": "http://arxiv.org/abs/2406.02265v2", "date": "2024-06-06", "relevancy": 2.4565, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4974}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4886}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4879}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Retrieval%20Robustness%20for%20Retrieval-Augmented%20Image%0A%20%20Captioning&body=Title%3A%20Understanding%20Retrieval%20Robustness%20for%20Retrieval-Augmented%20Image%0A%20%20Captioning%0AAuthor%3A%20Wenyan%20Li%20and%20Jiaang%20Li%20and%20Rita%20Ramos%20and%20Raphael%20Tang%20and%20Desmond%20Elliott%0AAbstract%3A%20%20%20Recent%20advances%20in%20retrieval-augmented%20models%20for%20image%20captioning%20highlight%0Athe%20benefit%20of%20retrieving%20related%20captions%20for%20efficient%2C%20lightweight%20models%0Awith%20strong%20domain-transfer%20capabilities.%20While%20these%20models%20demonstrate%20the%0Asuccess%20of%20retrieval%20augmentation%2C%20retrieval%20models%20are%20still%20far%20from%20perfect%0Ain%20practice%3A%20the%20retrieved%20information%20can%20sometimes%20mislead%20the%20model%2C%0Aresulting%20in%20incorrect%20generation%20and%20worse%20performance.%20In%20this%20paper%2C%20we%0Aanalyze%20the%20robustness%20of%20a%20retrieval-augmented%20captioning%20model%20SmallCap.%20Our%0Aanalysis%20shows%20that%20the%20model%20is%20sensitive%20to%20tokens%20that%20appear%20in%20the%0Amajority%20of%20the%20retrieved%20captions%2C%20and%20the%20input%20attribution%20shows%20that%20those%0Atokens%20are%20likely%20copied%20into%20the%20generated%20output.%20Given%20these%20findings%2C%20we%0Apropose%20to%20train%20the%20model%20by%20sampling%20retrieved%20captions%20from%20more%20diverse%0Asets.%20This%20decreases%20the%20chance%20that%20the%20model%20learns%20to%20copy%20majority%20tokens%2C%0Aand%20improves%20both%20in-domain%20and%20cross-domain%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02265v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Retrieval%2520Robustness%2520for%2520Retrieval-Augmented%2520Image%250A%2520%2520Captioning%26entry.906535625%3DWenyan%2520Li%2520and%2520Jiaang%2520Li%2520and%2520Rita%2520Ramos%2520and%2520Raphael%2520Tang%2520and%2520Desmond%2520Elliott%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520retrieval-augmented%2520models%2520for%2520image%2520captioning%2520highlight%250Athe%2520benefit%2520of%2520retrieving%2520related%2520captions%2520for%2520efficient%252C%2520lightweight%2520models%250Awith%2520strong%2520domain-transfer%2520capabilities.%2520While%2520these%2520models%2520demonstrate%2520the%250Asuccess%2520of%2520retrieval%2520augmentation%252C%2520retrieval%2520models%2520are%2520still%2520far%2520from%2520perfect%250Ain%2520practice%253A%2520the%2520retrieved%2520information%2520can%2520sometimes%2520mislead%2520the%2520model%252C%250Aresulting%2520in%2520incorrect%2520generation%2520and%2520worse%2520performance.%2520In%2520this%2520paper%252C%2520we%250Aanalyze%2520the%2520robustness%2520of%2520a%2520retrieval-augmented%2520captioning%2520model%2520SmallCap.%2520Our%250Aanalysis%2520shows%2520that%2520the%2520model%2520is%2520sensitive%2520to%2520tokens%2520that%2520appear%2520in%2520the%250Amajority%2520of%2520the%2520retrieved%2520captions%252C%2520and%2520the%2520input%2520attribution%2520shows%2520that%2520those%250Atokens%2520are%2520likely%2520copied%2520into%2520the%2520generated%2520output.%2520Given%2520these%2520findings%252C%2520we%250Apropose%2520to%2520train%2520the%2520model%2520by%2520sampling%2520retrieved%2520captions%2520from%2520more%2520diverse%250Asets.%2520This%2520decreases%2520the%2520chance%2520that%2520the%2520model%2520learns%2520to%2520copy%2520majority%2520tokens%252C%250Aand%2520improves%2520both%2520in-domain%2520and%2520cross-domain%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02265v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Retrieval%20Robustness%20for%20Retrieval-Augmented%20Image%0A%20%20Captioning&entry.906535625=Wenyan%20Li%20and%20Jiaang%20Li%20and%20Rita%20Ramos%20and%20Raphael%20Tang%20and%20Desmond%20Elliott&entry.1292438233=%20%20Recent%20advances%20in%20retrieval-augmented%20models%20for%20image%20captioning%20highlight%0Athe%20benefit%20of%20retrieving%20related%20captions%20for%20efficient%2C%20lightweight%20models%0Awith%20strong%20domain-transfer%20capabilities.%20While%20these%20models%20demonstrate%20the%0Asuccess%20of%20retrieval%20augmentation%2C%20retrieval%20models%20are%20still%20far%20from%20perfect%0Ain%20practice%3A%20the%20retrieved%20information%20can%20sometimes%20mislead%20the%20model%2C%0Aresulting%20in%20incorrect%20generation%20and%20worse%20performance.%20In%20this%20paper%2C%20we%0Aanalyze%20the%20robustness%20of%20a%20retrieval-augmented%20captioning%20model%20SmallCap.%20Our%0Aanalysis%20shows%20that%20the%20model%20is%20sensitive%20to%20tokens%20that%20appear%20in%20the%0Amajority%20of%20the%20retrieved%20captions%2C%20and%20the%20input%20attribution%20shows%20that%20those%0Atokens%20are%20likely%20copied%20into%20the%20generated%20output.%20Given%20these%20findings%2C%20we%0Apropose%20to%20train%20the%20model%20by%20sampling%20retrieved%20captions%20from%20more%20diverse%0Asets.%20This%20decreases%20the%20chance%20that%20the%20model%20learns%20to%20copy%20majority%20tokens%2C%0Aand%20improves%20both%20in-domain%20and%20cross-domain%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02265v2&entry.124074799=Read"},
{"title": "DiffCAD: Weakly-Supervised Probabilistic CAD Model Retrieval and\n  Alignment from an RGB Image", "author": "Daoyi Gao and D\u00e1vid Rozenberszki and Stefan Leutenegger and Angela Dai", "abstract": "  Perceiving 3D structures from RGB images based on CAD model primitives can\nenable an effective, efficient 3D object-based representation of scenes.\nHowever, current approaches rely on supervision from expensive annotations of\nCAD models associated with real images, and encounter challenges due to the\ninherent ambiguities in the task -- both in depth-scale ambiguity in monocular\nperception, as well as inexact matches of CAD database models to real\nobservations. We thus propose DiffCAD, the first weakly-supervised\nprobabilistic approach to CAD retrieval and alignment from an RGB image. We\nformulate this as a conditional generative task, leveraging diffusion to learn\nimplicit probabilistic models capturing the shape, pose, and scale of CAD\nobjects in an image. This enables multi-hypothesis generation of different\nplausible CAD reconstructions, requiring only a few hypotheses to characterize\nambiguities in depth/scale and inexact shape matches. Our approach is trained\nonly on synthetic data, leveraging monocular depth and mask estimates to enable\nrobust zero-shot adaptation to various real target domains. Despite being\ntrained solely on synthetic data, our multi-hypothesis approach can even\nsurpass the supervised state-of-the-art on the Scan2CAD dataset by 5.9% with 8\nhypotheses.\n", "link": "http://arxiv.org/abs/2311.18610v2", "date": "2024-06-06", "relevancy": 2.4459, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6175}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6175}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffCAD%3A%20Weakly-Supervised%20Probabilistic%20CAD%20Model%20Retrieval%20and%0A%20%20Alignment%20from%20an%20RGB%20Image&body=Title%3A%20DiffCAD%3A%20Weakly-Supervised%20Probabilistic%20CAD%20Model%20Retrieval%20and%0A%20%20Alignment%20from%20an%20RGB%20Image%0AAuthor%3A%20Daoyi%20Gao%20and%20D%C3%A1vid%20Rozenberszki%20and%20Stefan%20Leutenegger%20and%20Angela%20Dai%0AAbstract%3A%20%20%20Perceiving%203D%20structures%20from%20RGB%20images%20based%20on%20CAD%20model%20primitives%20can%0Aenable%20an%20effective%2C%20efficient%203D%20object-based%20representation%20of%20scenes.%0AHowever%2C%20current%20approaches%20rely%20on%20supervision%20from%20expensive%20annotations%20of%0ACAD%20models%20associated%20with%20real%20images%2C%20and%20encounter%20challenges%20due%20to%20the%0Ainherent%20ambiguities%20in%20the%20task%20--%20both%20in%20depth-scale%20ambiguity%20in%20monocular%0Aperception%2C%20as%20well%20as%20inexact%20matches%20of%20CAD%20database%20models%20to%20real%0Aobservations.%20We%20thus%20propose%20DiffCAD%2C%20the%20first%20weakly-supervised%0Aprobabilistic%20approach%20to%20CAD%20retrieval%20and%20alignment%20from%20an%20RGB%20image.%20We%0Aformulate%20this%20as%20a%20conditional%20generative%20task%2C%20leveraging%20diffusion%20to%20learn%0Aimplicit%20probabilistic%20models%20capturing%20the%20shape%2C%20pose%2C%20and%20scale%20of%20CAD%0Aobjects%20in%20an%20image.%20This%20enables%20multi-hypothesis%20generation%20of%20different%0Aplausible%20CAD%20reconstructions%2C%20requiring%20only%20a%20few%20hypotheses%20to%20characterize%0Aambiguities%20in%20depth/scale%20and%20inexact%20shape%20matches.%20Our%20approach%20is%20trained%0Aonly%20on%20synthetic%20data%2C%20leveraging%20monocular%20depth%20and%20mask%20estimates%20to%20enable%0Arobust%20zero-shot%20adaptation%20to%20various%20real%20target%20domains.%20Despite%20being%0Atrained%20solely%20on%20synthetic%20data%2C%20our%20multi-hypothesis%20approach%20can%20even%0Asurpass%20the%20supervised%20state-of-the-art%20on%20the%20Scan2CAD%20dataset%20by%205.9%25%20with%208%0Ahypotheses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18610v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffCAD%253A%2520Weakly-Supervised%2520Probabilistic%2520CAD%2520Model%2520Retrieval%2520and%250A%2520%2520Alignment%2520from%2520an%2520RGB%2520Image%26entry.906535625%3DDaoyi%2520Gao%2520and%2520D%25C3%25A1vid%2520Rozenberszki%2520and%2520Stefan%2520Leutenegger%2520and%2520Angela%2520Dai%26entry.1292438233%3D%2520%2520Perceiving%25203D%2520structures%2520from%2520RGB%2520images%2520based%2520on%2520CAD%2520model%2520primitives%2520can%250Aenable%2520an%2520effective%252C%2520efficient%25203D%2520object-based%2520representation%2520of%2520scenes.%250AHowever%252C%2520current%2520approaches%2520rely%2520on%2520supervision%2520from%2520expensive%2520annotations%2520of%250ACAD%2520models%2520associated%2520with%2520real%2520images%252C%2520and%2520encounter%2520challenges%2520due%2520to%2520the%250Ainherent%2520ambiguities%2520in%2520the%2520task%2520--%2520both%2520in%2520depth-scale%2520ambiguity%2520in%2520monocular%250Aperception%252C%2520as%2520well%2520as%2520inexact%2520matches%2520of%2520CAD%2520database%2520models%2520to%2520real%250Aobservations.%2520We%2520thus%2520propose%2520DiffCAD%252C%2520the%2520first%2520weakly-supervised%250Aprobabilistic%2520approach%2520to%2520CAD%2520retrieval%2520and%2520alignment%2520from%2520an%2520RGB%2520image.%2520We%250Aformulate%2520this%2520as%2520a%2520conditional%2520generative%2520task%252C%2520leveraging%2520diffusion%2520to%2520learn%250Aimplicit%2520probabilistic%2520models%2520capturing%2520the%2520shape%252C%2520pose%252C%2520and%2520scale%2520of%2520CAD%250Aobjects%2520in%2520an%2520image.%2520This%2520enables%2520multi-hypothesis%2520generation%2520of%2520different%250Aplausible%2520CAD%2520reconstructions%252C%2520requiring%2520only%2520a%2520few%2520hypotheses%2520to%2520characterize%250Aambiguities%2520in%2520depth/scale%2520and%2520inexact%2520shape%2520matches.%2520Our%2520approach%2520is%2520trained%250Aonly%2520on%2520synthetic%2520data%252C%2520leveraging%2520monocular%2520depth%2520and%2520mask%2520estimates%2520to%2520enable%250Arobust%2520zero-shot%2520adaptation%2520to%2520various%2520real%2520target%2520domains.%2520Despite%2520being%250Atrained%2520solely%2520on%2520synthetic%2520data%252C%2520our%2520multi-hypothesis%2520approach%2520can%2520even%250Asurpass%2520the%2520supervised%2520state-of-the-art%2520on%2520the%2520Scan2CAD%2520dataset%2520by%25205.9%2525%2520with%25208%250Ahypotheses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.18610v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffCAD%3A%20Weakly-Supervised%20Probabilistic%20CAD%20Model%20Retrieval%20and%0A%20%20Alignment%20from%20an%20RGB%20Image&entry.906535625=Daoyi%20Gao%20and%20D%C3%A1vid%20Rozenberszki%20and%20Stefan%20Leutenegger%20and%20Angela%20Dai&entry.1292438233=%20%20Perceiving%203D%20structures%20from%20RGB%20images%20based%20on%20CAD%20model%20primitives%20can%0Aenable%20an%20effective%2C%20efficient%203D%20object-based%20representation%20of%20scenes.%0AHowever%2C%20current%20approaches%20rely%20on%20supervision%20from%20expensive%20annotations%20of%0ACAD%20models%20associated%20with%20real%20images%2C%20and%20encounter%20challenges%20due%20to%20the%0Ainherent%20ambiguities%20in%20the%20task%20--%20both%20in%20depth-scale%20ambiguity%20in%20monocular%0Aperception%2C%20as%20well%20as%20inexact%20matches%20of%20CAD%20database%20models%20to%20real%0Aobservations.%20We%20thus%20propose%20DiffCAD%2C%20the%20first%20weakly-supervised%0Aprobabilistic%20approach%20to%20CAD%20retrieval%20and%20alignment%20from%20an%20RGB%20image.%20We%0Aformulate%20this%20as%20a%20conditional%20generative%20task%2C%20leveraging%20diffusion%20to%20learn%0Aimplicit%20probabilistic%20models%20capturing%20the%20shape%2C%20pose%2C%20and%20scale%20of%20CAD%0Aobjects%20in%20an%20image.%20This%20enables%20multi-hypothesis%20generation%20of%20different%0Aplausible%20CAD%20reconstructions%2C%20requiring%20only%20a%20few%20hypotheses%20to%20characterize%0Aambiguities%20in%20depth/scale%20and%20inexact%20shape%20matches.%20Our%20approach%20is%20trained%0Aonly%20on%20synthetic%20data%2C%20leveraging%20monocular%20depth%20and%20mask%20estimates%20to%20enable%0Arobust%20zero-shot%20adaptation%20to%20various%20real%20target%20domains.%20Despite%20being%0Atrained%20solely%20on%20synthetic%20data%2C%20our%20multi-hypothesis%20approach%20can%20even%0Asurpass%20the%20supervised%20state-of-the-art%20on%20the%20Scan2CAD%20dataset%20by%205.9%25%20with%208%0Ahypotheses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18610v2&entry.124074799=Read"},
{"title": "NoisyGL: A Comprehensive Benchmark for Graph Neural Networks under Label\n  Noise", "author": "Zhonghao Wang and Danyu Sun and Sheng Zhou and Haobo Wang and Jiapei Fan and Longtao Huang and Jiajun Bu", "abstract": "  Graph Neural Networks (GNNs) exhibit strong potential in node classification\ntask through a message-passing mechanism. However, their performance often\nhinges on high-quality node labels, which are challenging to obtain in\nreal-world scenarios due to unreliable sources or adversarial attacks.\nConsequently, label noise is common in real-world graph data, negatively\nimpacting GNNs by propagating incorrect information during training. To address\nthis issue, the study of Graph Neural Networks under Label Noise (GLN) has\nrecently gained traction. However, due to variations in dataset selection, data\nsplitting, and preprocessing techniques, the community currently lacks a\ncomprehensive benchmark, which impedes deeper understanding and further\ndevelopment of GLN. To fill this gap, we introduce NoisyGL in this paper, the\nfirst comprehensive benchmark for graph neural networks under label noise.\nNoisyGL enables fair comparisons and detailed analyses of GLN methods on noisy\nlabeled graph data across various datasets, with unified experimental settings\nand interface. Our benchmark has uncovered several important insights that were\nmissed in previous research, and we believe these findings will be highly\nbeneficial for future studies. We hope our open-source benchmark library will\nfoster further advancements in this field. The code of the benchmark can be\nfound in https://github.com/eaglelab-zju/NoisyGL.\n", "link": "http://arxiv.org/abs/2406.04299v1", "date": "2024-06-06", "relevancy": 2.44, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4964}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4839}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NoisyGL%3A%20A%20Comprehensive%20Benchmark%20for%20Graph%20Neural%20Networks%20under%20Label%0A%20%20Noise&body=Title%3A%20NoisyGL%3A%20A%20Comprehensive%20Benchmark%20for%20Graph%20Neural%20Networks%20under%20Label%0A%20%20Noise%0AAuthor%3A%20Zhonghao%20Wang%20and%20Danyu%20Sun%20and%20Sheng%20Zhou%20and%20Haobo%20Wang%20and%20Jiapei%20Fan%20and%20Longtao%20Huang%20and%20Jiajun%20Bu%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20exhibit%20strong%20potential%20in%20node%20classification%0Atask%20through%20a%20message-passing%20mechanism.%20However%2C%20their%20performance%20often%0Ahinges%20on%20high-quality%20node%20labels%2C%20which%20are%20challenging%20to%20obtain%20in%0Areal-world%20scenarios%20due%20to%20unreliable%20sources%20or%20adversarial%20attacks.%0AConsequently%2C%20label%20noise%20is%20common%20in%20real-world%20graph%20data%2C%20negatively%0Aimpacting%20GNNs%20by%20propagating%20incorrect%20information%20during%20training.%20To%20address%0Athis%20issue%2C%20the%20study%20of%20Graph%20Neural%20Networks%20under%20Label%20Noise%20%28GLN%29%20has%0Arecently%20gained%20traction.%20However%2C%20due%20to%20variations%20in%20dataset%20selection%2C%20data%0Asplitting%2C%20and%20preprocessing%20techniques%2C%20the%20community%20currently%20lacks%20a%0Acomprehensive%20benchmark%2C%20which%20impedes%20deeper%20understanding%20and%20further%0Adevelopment%20of%20GLN.%20To%20fill%20this%20gap%2C%20we%20introduce%20NoisyGL%20in%20this%20paper%2C%20the%0Afirst%20comprehensive%20benchmark%20for%20graph%20neural%20networks%20under%20label%20noise.%0ANoisyGL%20enables%20fair%20comparisons%20and%20detailed%20analyses%20of%20GLN%20methods%20on%20noisy%0Alabeled%20graph%20data%20across%20various%20datasets%2C%20with%20unified%20experimental%20settings%0Aand%20interface.%20Our%20benchmark%20has%20uncovered%20several%20important%20insights%20that%20were%0Amissed%20in%20previous%20research%2C%20and%20we%20believe%20these%20findings%20will%20be%20highly%0Abeneficial%20for%20future%20studies.%20We%20hope%20our%20open-source%20benchmark%20library%20will%0Afoster%20further%20advancements%20in%20this%20field.%20The%20code%20of%20the%20benchmark%20can%20be%0Afound%20in%20https%3A//github.com/eaglelab-zju/NoisyGL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04299v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoisyGL%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Graph%2520Neural%2520Networks%2520under%2520Label%250A%2520%2520Noise%26entry.906535625%3DZhonghao%2520Wang%2520and%2520Danyu%2520Sun%2520and%2520Sheng%2520Zhou%2520and%2520Haobo%2520Wang%2520and%2520Jiapei%2520Fan%2520and%2520Longtao%2520Huang%2520and%2520Jiajun%2520Bu%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520exhibit%2520strong%2520potential%2520in%2520node%2520classification%250Atask%2520through%2520a%2520message-passing%2520mechanism.%2520However%252C%2520their%2520performance%2520often%250Ahinges%2520on%2520high-quality%2520node%2520labels%252C%2520which%2520are%2520challenging%2520to%2520obtain%2520in%250Areal-world%2520scenarios%2520due%2520to%2520unreliable%2520sources%2520or%2520adversarial%2520attacks.%250AConsequently%252C%2520label%2520noise%2520is%2520common%2520in%2520real-world%2520graph%2520data%252C%2520negatively%250Aimpacting%2520GNNs%2520by%2520propagating%2520incorrect%2520information%2520during%2520training.%2520To%2520address%250Athis%2520issue%252C%2520the%2520study%2520of%2520Graph%2520Neural%2520Networks%2520under%2520Label%2520Noise%2520%2528GLN%2529%2520has%250Arecently%2520gained%2520traction.%2520However%252C%2520due%2520to%2520variations%2520in%2520dataset%2520selection%252C%2520data%250Asplitting%252C%2520and%2520preprocessing%2520techniques%252C%2520the%2520community%2520currently%2520lacks%2520a%250Acomprehensive%2520benchmark%252C%2520which%2520impedes%2520deeper%2520understanding%2520and%2520further%250Adevelopment%2520of%2520GLN.%2520To%2520fill%2520this%2520gap%252C%2520we%2520introduce%2520NoisyGL%2520in%2520this%2520paper%252C%2520the%250Afirst%2520comprehensive%2520benchmark%2520for%2520graph%2520neural%2520networks%2520under%2520label%2520noise.%250ANoisyGL%2520enables%2520fair%2520comparisons%2520and%2520detailed%2520analyses%2520of%2520GLN%2520methods%2520on%2520noisy%250Alabeled%2520graph%2520data%2520across%2520various%2520datasets%252C%2520with%2520unified%2520experimental%2520settings%250Aand%2520interface.%2520Our%2520benchmark%2520has%2520uncovered%2520several%2520important%2520insights%2520that%2520were%250Amissed%2520in%2520previous%2520research%252C%2520and%2520we%2520believe%2520these%2520findings%2520will%2520be%2520highly%250Abeneficial%2520for%2520future%2520studies.%2520We%2520hope%2520our%2520open-source%2520benchmark%2520library%2520will%250Afoster%2520further%2520advancements%2520in%2520this%2520field.%2520The%2520code%2520of%2520the%2520benchmark%2520can%2520be%250Afound%2520in%2520https%253A//github.com/eaglelab-zju/NoisyGL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04299v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NoisyGL%3A%20A%20Comprehensive%20Benchmark%20for%20Graph%20Neural%20Networks%20under%20Label%0A%20%20Noise&entry.906535625=Zhonghao%20Wang%20and%20Danyu%20Sun%20and%20Sheng%20Zhou%20and%20Haobo%20Wang%20and%20Jiapei%20Fan%20and%20Longtao%20Huang%20and%20Jiajun%20Bu&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20exhibit%20strong%20potential%20in%20node%20classification%0Atask%20through%20a%20message-passing%20mechanism.%20However%2C%20their%20performance%20often%0Ahinges%20on%20high-quality%20node%20labels%2C%20which%20are%20challenging%20to%20obtain%20in%0Areal-world%20scenarios%20due%20to%20unreliable%20sources%20or%20adversarial%20attacks.%0AConsequently%2C%20label%20noise%20is%20common%20in%20real-world%20graph%20data%2C%20negatively%0Aimpacting%20GNNs%20by%20propagating%20incorrect%20information%20during%20training.%20To%20address%0Athis%20issue%2C%20the%20study%20of%20Graph%20Neural%20Networks%20under%20Label%20Noise%20%28GLN%29%20has%0Arecently%20gained%20traction.%20However%2C%20due%20to%20variations%20in%20dataset%20selection%2C%20data%0Asplitting%2C%20and%20preprocessing%20techniques%2C%20the%20community%20currently%20lacks%20a%0Acomprehensive%20benchmark%2C%20which%20impedes%20deeper%20understanding%20and%20further%0Adevelopment%20of%20GLN.%20To%20fill%20this%20gap%2C%20we%20introduce%20NoisyGL%20in%20this%20paper%2C%20the%0Afirst%20comprehensive%20benchmark%20for%20graph%20neural%20networks%20under%20label%20noise.%0ANoisyGL%20enables%20fair%20comparisons%20and%20detailed%20analyses%20of%20GLN%20methods%20on%20noisy%0Alabeled%20graph%20data%20across%20various%20datasets%2C%20with%20unified%20experimental%20settings%0Aand%20interface.%20Our%20benchmark%20has%20uncovered%20several%20important%20insights%20that%20were%0Amissed%20in%20previous%20research%2C%20and%20we%20believe%20these%20findings%20will%20be%20highly%0Abeneficial%20for%20future%20studies.%20We%20hope%20our%20open-source%20benchmark%20library%20will%0Afoster%20further%20advancements%20in%20this%20field.%20The%20code%20of%20the%20benchmark%20can%20be%0Afound%20in%20https%3A//github.com/eaglelab-zju/NoisyGL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04299v1&entry.124074799=Read"},
{"title": "Representational Alignment Supports Effective Machine Teaching", "author": "Ilia Sucholutsky and Katherine M. Collins and Maya Malaviya and Nori Jacoby and Weiyang Liu and Theodore R. Sumers and Michalis Korakakis and Umang Bhatt and Mark Ho and Joshua B. Tenenbaum and Brad Love and Zachary A. Pardos and Adrian Weller and Thomas L. Griffiths", "abstract": "  A good teacher should not only be knowledgeable; but should be able to\ncommunicate in a way that the student understands -- to share the student's\nrepresentation of the world. In this work, we integrate insights from machine\nteaching and pragmatic communication with the burgeoning literature on\nrepresentational alignment to characterize a utility curve defining a\nrelationship between representational alignment and teacher capability for\npromoting student learning. To explore the characteristics of this utility\ncurve, we design a supervised learning environment that disentangles\nrepresentational alignment from teacher accuracy. We conduct extensive\ncomputational experiments with machines teaching machines, complemented by a\nseries of experiments in which machines teach humans. Drawing on our findings\nthat improved representational alignment with a student improves student\nlearning outcomes (i.e., task accuracy), we design a classroom matching\nprocedure that assigns students to teachers based on the utility curve. If we\nare to design effective machine teachers, it is not enough to build teachers\nthat are accurate -- we want teachers that can align, representationally, to\ntheir students too.\n", "link": "http://arxiv.org/abs/2406.04302v1", "date": "2024-06-06", "relevancy": 2.4345, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5049}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4887}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representational%20Alignment%20Supports%20Effective%20Machine%20Teaching&body=Title%3A%20Representational%20Alignment%20Supports%20Effective%20Machine%20Teaching%0AAuthor%3A%20Ilia%20Sucholutsky%20and%20Katherine%20M.%20Collins%20and%20Maya%20Malaviya%20and%20Nori%20Jacoby%20and%20Weiyang%20Liu%20and%20Theodore%20R.%20Sumers%20and%20Michalis%20Korakakis%20and%20Umang%20Bhatt%20and%20Mark%20Ho%20and%20Joshua%20B.%20Tenenbaum%20and%20Brad%20Love%20and%20Zachary%20A.%20Pardos%20and%20Adrian%20Weller%20and%20Thomas%20L.%20Griffiths%0AAbstract%3A%20%20%20A%20good%20teacher%20should%20not%20only%20be%20knowledgeable%3B%20but%20should%20be%20able%20to%0Acommunicate%20in%20a%20way%20that%20the%20student%20understands%20--%20to%20share%20the%20student%27s%0Arepresentation%20of%20the%20world.%20In%20this%20work%2C%20we%20integrate%20insights%20from%20machine%0Ateaching%20and%20pragmatic%20communication%20with%20the%20burgeoning%20literature%20on%0Arepresentational%20alignment%20to%20characterize%20a%20utility%20curve%20defining%20a%0Arelationship%20between%20representational%20alignment%20and%20teacher%20capability%20for%0Apromoting%20student%20learning.%20To%20explore%20the%20characteristics%20of%20this%20utility%0Acurve%2C%20we%20design%20a%20supervised%20learning%20environment%20that%20disentangles%0Arepresentational%20alignment%20from%20teacher%20accuracy.%20We%20conduct%20extensive%0Acomputational%20experiments%20with%20machines%20teaching%20machines%2C%20complemented%20by%20a%0Aseries%20of%20experiments%20in%20which%20machines%20teach%20humans.%20Drawing%20on%20our%20findings%0Athat%20improved%20representational%20alignment%20with%20a%20student%20improves%20student%0Alearning%20outcomes%20%28i.e.%2C%20task%20accuracy%29%2C%20we%20design%20a%20classroom%20matching%0Aprocedure%20that%20assigns%20students%20to%20teachers%20based%20on%20the%20utility%20curve.%20If%20we%0Aare%20to%20design%20effective%20machine%20teachers%2C%20it%20is%20not%20enough%20to%20build%20teachers%0Athat%20are%20accurate%20--%20we%20want%20teachers%20that%20can%20align%2C%20representationally%2C%20to%0Atheir%20students%20too.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresentational%2520Alignment%2520Supports%2520Effective%2520Machine%2520Teaching%26entry.906535625%3DIlia%2520Sucholutsky%2520and%2520Katherine%2520M.%2520Collins%2520and%2520Maya%2520Malaviya%2520and%2520Nori%2520Jacoby%2520and%2520Weiyang%2520Liu%2520and%2520Theodore%2520R.%2520Sumers%2520and%2520Michalis%2520Korakakis%2520and%2520Umang%2520Bhatt%2520and%2520Mark%2520Ho%2520and%2520Joshua%2520B.%2520Tenenbaum%2520and%2520Brad%2520Love%2520and%2520Zachary%2520A.%2520Pardos%2520and%2520Adrian%2520Weller%2520and%2520Thomas%2520L.%2520Griffiths%26entry.1292438233%3D%2520%2520A%2520good%2520teacher%2520should%2520not%2520only%2520be%2520knowledgeable%253B%2520but%2520should%2520be%2520able%2520to%250Acommunicate%2520in%2520a%2520way%2520that%2520the%2520student%2520understands%2520--%2520to%2520share%2520the%2520student%2527s%250Arepresentation%2520of%2520the%2520world.%2520In%2520this%2520work%252C%2520we%2520integrate%2520insights%2520from%2520machine%250Ateaching%2520and%2520pragmatic%2520communication%2520with%2520the%2520burgeoning%2520literature%2520on%250Arepresentational%2520alignment%2520to%2520characterize%2520a%2520utility%2520curve%2520defining%2520a%250Arelationship%2520between%2520representational%2520alignment%2520and%2520teacher%2520capability%2520for%250Apromoting%2520student%2520learning.%2520To%2520explore%2520the%2520characteristics%2520of%2520this%2520utility%250Acurve%252C%2520we%2520design%2520a%2520supervised%2520learning%2520environment%2520that%2520disentangles%250Arepresentational%2520alignment%2520from%2520teacher%2520accuracy.%2520We%2520conduct%2520extensive%250Acomputational%2520experiments%2520with%2520machines%2520teaching%2520machines%252C%2520complemented%2520by%2520a%250Aseries%2520of%2520experiments%2520in%2520which%2520machines%2520teach%2520humans.%2520Drawing%2520on%2520our%2520findings%250Athat%2520improved%2520representational%2520alignment%2520with%2520a%2520student%2520improves%2520student%250Alearning%2520outcomes%2520%2528i.e.%252C%2520task%2520accuracy%2529%252C%2520we%2520design%2520a%2520classroom%2520matching%250Aprocedure%2520that%2520assigns%2520students%2520to%2520teachers%2520based%2520on%2520the%2520utility%2520curve.%2520If%2520we%250Aare%2520to%2520design%2520effective%2520machine%2520teachers%252C%2520it%2520is%2520not%2520enough%2520to%2520build%2520teachers%250Athat%2520are%2520accurate%2520--%2520we%2520want%2520teachers%2520that%2520can%2520align%252C%2520representationally%252C%2520to%250Atheir%2520students%2520too.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representational%20Alignment%20Supports%20Effective%20Machine%20Teaching&entry.906535625=Ilia%20Sucholutsky%20and%20Katherine%20M.%20Collins%20and%20Maya%20Malaviya%20and%20Nori%20Jacoby%20and%20Weiyang%20Liu%20and%20Theodore%20R.%20Sumers%20and%20Michalis%20Korakakis%20and%20Umang%20Bhatt%20and%20Mark%20Ho%20and%20Joshua%20B.%20Tenenbaum%20and%20Brad%20Love%20and%20Zachary%20A.%20Pardos%20and%20Adrian%20Weller%20and%20Thomas%20L.%20Griffiths&entry.1292438233=%20%20A%20good%20teacher%20should%20not%20only%20be%20knowledgeable%3B%20but%20should%20be%20able%20to%0Acommunicate%20in%20a%20way%20that%20the%20student%20understands%20--%20to%20share%20the%20student%27s%0Arepresentation%20of%20the%20world.%20In%20this%20work%2C%20we%20integrate%20insights%20from%20machine%0Ateaching%20and%20pragmatic%20communication%20with%20the%20burgeoning%20literature%20on%0Arepresentational%20alignment%20to%20characterize%20a%20utility%20curve%20defining%20a%0Arelationship%20between%20representational%20alignment%20and%20teacher%20capability%20for%0Apromoting%20student%20learning.%20To%20explore%20the%20characteristics%20of%20this%20utility%0Acurve%2C%20we%20design%20a%20supervised%20learning%20environment%20that%20disentangles%0Arepresentational%20alignment%20from%20teacher%20accuracy.%20We%20conduct%20extensive%0Acomputational%20experiments%20with%20machines%20teaching%20machines%2C%20complemented%20by%20a%0Aseries%20of%20experiments%20in%20which%20machines%20teach%20humans.%20Drawing%20on%20our%20findings%0Athat%20improved%20representational%20alignment%20with%20a%20student%20improves%20student%0Alearning%20outcomes%20%28i.e.%2C%20task%20accuracy%29%2C%20we%20design%20a%20classroom%20matching%0Aprocedure%20that%20assigns%20students%20to%20teachers%20based%20on%20the%20utility%20curve.%20If%20we%0Aare%20to%20design%20effective%20machine%20teachers%2C%20it%20is%20not%20enough%20to%20build%20teachers%0Athat%20are%20accurate%20--%20we%20want%20teachers%20that%20can%20align%2C%20representationally%2C%20to%0Atheir%20students%20too.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04302v1&entry.124074799=Read"},
{"title": "Diffusion Tuning: Transferring Diffusion Models via Chain of Forgetting", "author": "Jincheng Zhong and Xingzhuo Guo and Jiaxiang Dong and Mingsheng Long", "abstract": "  Diffusion models have significantly advanced the field of generative\nmodeling. However, training a diffusion model is computationally expensive,\ncreating a pressing need to adapt off-the-shelf diffusion models for downstream\ngeneration tasks. Current fine-tuning methods focus on parameter-efficient\ntransfer learning but overlook the fundamental transfer characteristics of\ndiffusion models. In this paper, we investigate the transferability of\ndiffusion models and observe a monotonous chain of forgetting trend of\ntransferability along the reverse process. Based on this observation and novel\ntheoretical insights, we present Diff-Tuning, a frustratingly simple transfer\napproach that leverages the chain of forgetting tendency. Diff-Tuning\nencourages the fine-tuned model to retain the pre-trained knowledge at the end\nof the denoising chain close to the generated data while discarding the other\nnoise side. We conduct comprehensive experiments to evaluate Diff-Tuning,\nincluding the transfer of pre-trained Diffusion Transformer models to eight\ndownstream generations and the adaptation of Stable Diffusion to five control\nconditions with ControlNet. Diff-Tuning achieves a 26% improvement over\nstandard fine-tuning and enhances the convergence speed of ControlNet by 24%.\nNotably, parameter-efficient transfer learning techniques for diffusion models\ncan also benefit from Diff-Tuning.\n", "link": "http://arxiv.org/abs/2406.00773v2", "date": "2024-06-06", "relevancy": 2.4139, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6159}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6153}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diffusion%20Tuning%3A%20Transferring%20Diffusion%20Models%20via%20Chain%20of%20Forgetting&body=Title%3A%20Diffusion%20Tuning%3A%20Transferring%20Diffusion%20Models%20via%20Chain%20of%20Forgetting%0AAuthor%3A%20Jincheng%20Zhong%20and%20Xingzhuo%20Guo%20and%20Jiaxiang%20Dong%20and%20Mingsheng%20Long%0AAbstract%3A%20%20%20Diffusion%20models%20have%20significantly%20advanced%20the%20field%20of%20generative%0Amodeling.%20However%2C%20training%20a%20diffusion%20model%20is%20computationally%20expensive%2C%0Acreating%20a%20pressing%20need%20to%20adapt%20off-the-shelf%20diffusion%20models%20for%20downstream%0Ageneration%20tasks.%20Current%20fine-tuning%20methods%20focus%20on%20parameter-efficient%0Atransfer%20learning%20but%20overlook%20the%20fundamental%20transfer%20characteristics%20of%0Adiffusion%20models.%20In%20this%20paper%2C%20we%20investigate%20the%20transferability%20of%0Adiffusion%20models%20and%20observe%20a%20monotonous%20chain%20of%20forgetting%20trend%20of%0Atransferability%20along%20the%20reverse%20process.%20Based%20on%20this%20observation%20and%20novel%0Atheoretical%20insights%2C%20we%20present%20Diff-Tuning%2C%20a%20frustratingly%20simple%20transfer%0Aapproach%20that%20leverages%20the%20chain%20of%20forgetting%20tendency.%20Diff-Tuning%0Aencourages%20the%20fine-tuned%20model%20to%20retain%20the%20pre-trained%20knowledge%20at%20the%20end%0Aof%20the%20denoising%20chain%20close%20to%20the%20generated%20data%20while%20discarding%20the%20other%0Anoise%20side.%20We%20conduct%20comprehensive%20experiments%20to%20evaluate%20Diff-Tuning%2C%0Aincluding%20the%20transfer%20of%20pre-trained%20Diffusion%20Transformer%20models%20to%20eight%0Adownstream%20generations%20and%20the%20adaptation%20of%20Stable%20Diffusion%20to%20five%20control%0Aconditions%20with%20ControlNet.%20Diff-Tuning%20achieves%20a%2026%25%20improvement%20over%0Astandard%20fine-tuning%20and%20enhances%20the%20convergence%20speed%20of%20ControlNet%20by%2024%25.%0ANotably%2C%20parameter-efficient%20transfer%20learning%20techniques%20for%20diffusion%20models%0Acan%20also%20benefit%20from%20Diff-Tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00773v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffusion%2520Tuning%253A%2520Transferring%2520Diffusion%2520Models%2520via%2520Chain%2520of%2520Forgetting%26entry.906535625%3DJincheng%2520Zhong%2520and%2520Xingzhuo%2520Guo%2520and%2520Jiaxiang%2520Dong%2520and%2520Mingsheng%2520Long%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520significantly%2520advanced%2520the%2520field%2520of%2520generative%250Amodeling.%2520However%252C%2520training%2520a%2520diffusion%2520model%2520is%2520computationally%2520expensive%252C%250Acreating%2520a%2520pressing%2520need%2520to%2520adapt%2520off-the-shelf%2520diffusion%2520models%2520for%2520downstream%250Ageneration%2520tasks.%2520Current%2520fine-tuning%2520methods%2520focus%2520on%2520parameter-efficient%250Atransfer%2520learning%2520but%2520overlook%2520the%2520fundamental%2520transfer%2520characteristics%2520of%250Adiffusion%2520models.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520transferability%2520of%250Adiffusion%2520models%2520and%2520observe%2520a%2520monotonous%2520chain%2520of%2520forgetting%2520trend%2520of%250Atransferability%2520along%2520the%2520reverse%2520process.%2520Based%2520on%2520this%2520observation%2520and%2520novel%250Atheoretical%2520insights%252C%2520we%2520present%2520Diff-Tuning%252C%2520a%2520frustratingly%2520simple%2520transfer%250Aapproach%2520that%2520leverages%2520the%2520chain%2520of%2520forgetting%2520tendency.%2520Diff-Tuning%250Aencourages%2520the%2520fine-tuned%2520model%2520to%2520retain%2520the%2520pre-trained%2520knowledge%2520at%2520the%2520end%250Aof%2520the%2520denoising%2520chain%2520close%2520to%2520the%2520generated%2520data%2520while%2520discarding%2520the%2520other%250Anoise%2520side.%2520We%2520conduct%2520comprehensive%2520experiments%2520to%2520evaluate%2520Diff-Tuning%252C%250Aincluding%2520the%2520transfer%2520of%2520pre-trained%2520Diffusion%2520Transformer%2520models%2520to%2520eight%250Adownstream%2520generations%2520and%2520the%2520adaptation%2520of%2520Stable%2520Diffusion%2520to%2520five%2520control%250Aconditions%2520with%2520ControlNet.%2520Diff-Tuning%2520achieves%2520a%252026%2525%2520improvement%2520over%250Astandard%2520fine-tuning%2520and%2520enhances%2520the%2520convergence%2520speed%2520of%2520ControlNet%2520by%252024%2525.%250ANotably%252C%2520parameter-efficient%2520transfer%2520learning%2520techniques%2520for%2520diffusion%2520models%250Acan%2520also%2520benefit%2520from%2520Diff-Tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00773v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion%20Tuning%3A%20Transferring%20Diffusion%20Models%20via%20Chain%20of%20Forgetting&entry.906535625=Jincheng%20Zhong%20and%20Xingzhuo%20Guo%20and%20Jiaxiang%20Dong%20and%20Mingsheng%20Long&entry.1292438233=%20%20Diffusion%20models%20have%20significantly%20advanced%20the%20field%20of%20generative%0Amodeling.%20However%2C%20training%20a%20diffusion%20model%20is%20computationally%20expensive%2C%0Acreating%20a%20pressing%20need%20to%20adapt%20off-the-shelf%20diffusion%20models%20for%20downstream%0Ageneration%20tasks.%20Current%20fine-tuning%20methods%20focus%20on%20parameter-efficient%0Atransfer%20learning%20but%20overlook%20the%20fundamental%20transfer%20characteristics%20of%0Adiffusion%20models.%20In%20this%20paper%2C%20we%20investigate%20the%20transferability%20of%0Adiffusion%20models%20and%20observe%20a%20monotonous%20chain%20of%20forgetting%20trend%20of%0Atransferability%20along%20the%20reverse%20process.%20Based%20on%20this%20observation%20and%20novel%0Atheoretical%20insights%2C%20we%20present%20Diff-Tuning%2C%20a%20frustratingly%20simple%20transfer%0Aapproach%20that%20leverages%20the%20chain%20of%20forgetting%20tendency.%20Diff-Tuning%0Aencourages%20the%20fine-tuned%20model%20to%20retain%20the%20pre-trained%20knowledge%20at%20the%20end%0Aof%20the%20denoising%20chain%20close%20to%20the%20generated%20data%20while%20discarding%20the%20other%0Anoise%20side.%20We%20conduct%20comprehensive%20experiments%20to%20evaluate%20Diff-Tuning%2C%0Aincluding%20the%20transfer%20of%20pre-trained%20Diffusion%20Transformer%20models%20to%20eight%0Adownstream%20generations%20and%20the%20adaptation%20of%20Stable%20Diffusion%20to%20five%20control%0Aconditions%20with%20ControlNet.%20Diff-Tuning%20achieves%20a%2026%25%20improvement%20over%0Astandard%20fine-tuning%20and%20enhances%20the%20convergence%20speed%20of%20ControlNet%20by%2024%25.%0ANotably%2C%20parameter-efficient%20transfer%20learning%20techniques%20for%20diffusion%20models%0Acan%20also%20benefit%20from%20Diff-Tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00773v2&entry.124074799=Read"},
{"title": "Unveiling the Dynamics of Information Interplay in Supervised Learning", "author": "Kun Song and Zhiquan Tan and Bochao Zou and Huimin Ma and Weiran Huang", "abstract": "  In this paper, we use matrix information theory as an analytical tool to\nanalyze the dynamics of the information interplay between data representations\nand classification head vectors in the supervised learning process.\nSpecifically, inspired by the theory of Neural Collapse, we introduce matrix\nmutual information ratio (MIR) and matrix entropy difference ratio (HDR) to\nassess the interactions of data representation and class classification heads\nin supervised learning, and we determine the theoretical optimal values for MIR\nand HDR when Neural Collapse happens. Our experiments show that MIR and HDR can\neffectively explain many phenomena occurring in neural networks, for example,\nthe standard supervised training dynamics, linear mode connectivity, and the\nperformance of label smoothing and pruning. Additionally, we use MIR and HDR to\ngain insights into the dynamics of grokking, which is an intriguing phenomenon\nobserved in supervised training, where the model demonstrates generalization\ncapabilities long after it has learned to fit the training data. Furthermore,\nwe introduce MIR and HDR as loss terms in supervised and semi-supervised\nlearning to optimize the information interactions among samples and\nclassification heads. The empirical results provide evidence of the method's\neffectiveness, demonstrating that the utilization of MIR and HDR not only aids\nin comprehending the dynamics throughout the training process but can also\nenhances the training procedure itself.\n", "link": "http://arxiv.org/abs/2406.03999v1", "date": "2024-06-06", "relevancy": 2.409, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5007}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4734}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4714}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20the%20Dynamics%20of%20Information%20Interplay%20in%20Supervised%20Learning&body=Title%3A%20Unveiling%20the%20Dynamics%20of%20Information%20Interplay%20in%20Supervised%20Learning%0AAuthor%3A%20Kun%20Song%20and%20Zhiquan%20Tan%20and%20Bochao%20Zou%20and%20Huimin%20Ma%20and%20Weiran%20Huang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20use%20matrix%20information%20theory%20as%20an%20analytical%20tool%20to%0Aanalyze%20the%20dynamics%20of%20the%20information%20interplay%20between%20data%20representations%0Aand%20classification%20head%20vectors%20in%20the%20supervised%20learning%20process.%0ASpecifically%2C%20inspired%20by%20the%20theory%20of%20Neural%20Collapse%2C%20we%20introduce%20matrix%0Amutual%20information%20ratio%20%28MIR%29%20and%20matrix%20entropy%20difference%20ratio%20%28HDR%29%20to%0Aassess%20the%20interactions%20of%20data%20representation%20and%20class%20classification%20heads%0Ain%20supervised%20learning%2C%20and%20we%20determine%20the%20theoretical%20optimal%20values%20for%20MIR%0Aand%20HDR%20when%20Neural%20Collapse%20happens.%20Our%20experiments%20show%20that%20MIR%20and%20HDR%20can%0Aeffectively%20explain%20many%20phenomena%20occurring%20in%20neural%20networks%2C%20for%20example%2C%0Athe%20standard%20supervised%20training%20dynamics%2C%20linear%20mode%20connectivity%2C%20and%20the%0Aperformance%20of%20label%20smoothing%20and%20pruning.%20Additionally%2C%20we%20use%20MIR%20and%20HDR%20to%0Again%20insights%20into%20the%20dynamics%20of%20grokking%2C%20which%20is%20an%20intriguing%20phenomenon%0Aobserved%20in%20supervised%20training%2C%20where%20the%20model%20demonstrates%20generalization%0Acapabilities%20long%20after%20it%20has%20learned%20to%20fit%20the%20training%20data.%20Furthermore%2C%0Awe%20introduce%20MIR%20and%20HDR%20as%20loss%20terms%20in%20supervised%20and%20semi-supervised%0Alearning%20to%20optimize%20the%20information%20interactions%20among%20samples%20and%0Aclassification%20heads.%20The%20empirical%20results%20provide%20evidence%20of%20the%20method%27s%0Aeffectiveness%2C%20demonstrating%20that%20the%20utilization%20of%20MIR%20and%20HDR%20not%20only%20aids%0Ain%20comprehending%20the%20dynamics%20throughout%20the%20training%20process%20but%20can%20also%0Aenhances%20the%20training%20procedure%20itself.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520the%2520Dynamics%2520of%2520Information%2520Interplay%2520in%2520Supervised%2520Learning%26entry.906535625%3DKun%2520Song%2520and%2520Zhiquan%2520Tan%2520and%2520Bochao%2520Zou%2520and%2520Huimin%2520Ma%2520and%2520Weiran%2520Huang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520use%2520matrix%2520information%2520theory%2520as%2520an%2520analytical%2520tool%2520to%250Aanalyze%2520the%2520dynamics%2520of%2520the%2520information%2520interplay%2520between%2520data%2520representations%250Aand%2520classification%2520head%2520vectors%2520in%2520the%2520supervised%2520learning%2520process.%250ASpecifically%252C%2520inspired%2520by%2520the%2520theory%2520of%2520Neural%2520Collapse%252C%2520we%2520introduce%2520matrix%250Amutual%2520information%2520ratio%2520%2528MIR%2529%2520and%2520matrix%2520entropy%2520difference%2520ratio%2520%2528HDR%2529%2520to%250Aassess%2520the%2520interactions%2520of%2520data%2520representation%2520and%2520class%2520classification%2520heads%250Ain%2520supervised%2520learning%252C%2520and%2520we%2520determine%2520the%2520theoretical%2520optimal%2520values%2520for%2520MIR%250Aand%2520HDR%2520when%2520Neural%2520Collapse%2520happens.%2520Our%2520experiments%2520show%2520that%2520MIR%2520and%2520HDR%2520can%250Aeffectively%2520explain%2520many%2520phenomena%2520occurring%2520in%2520neural%2520networks%252C%2520for%2520example%252C%250Athe%2520standard%2520supervised%2520training%2520dynamics%252C%2520linear%2520mode%2520connectivity%252C%2520and%2520the%250Aperformance%2520of%2520label%2520smoothing%2520and%2520pruning.%2520Additionally%252C%2520we%2520use%2520MIR%2520and%2520HDR%2520to%250Again%2520insights%2520into%2520the%2520dynamics%2520of%2520grokking%252C%2520which%2520is%2520an%2520intriguing%2520phenomenon%250Aobserved%2520in%2520supervised%2520training%252C%2520where%2520the%2520model%2520demonstrates%2520generalization%250Acapabilities%2520long%2520after%2520it%2520has%2520learned%2520to%2520fit%2520the%2520training%2520data.%2520Furthermore%252C%250Awe%2520introduce%2520MIR%2520and%2520HDR%2520as%2520loss%2520terms%2520in%2520supervised%2520and%2520semi-supervised%250Alearning%2520to%2520optimize%2520the%2520information%2520interactions%2520among%2520samples%2520and%250Aclassification%2520heads.%2520The%2520empirical%2520results%2520provide%2520evidence%2520of%2520the%2520method%2527s%250Aeffectiveness%252C%2520demonstrating%2520that%2520the%2520utilization%2520of%2520MIR%2520and%2520HDR%2520not%2520only%2520aids%250Ain%2520comprehending%2520the%2520dynamics%2520throughout%2520the%2520training%2520process%2520but%2520can%2520also%250Aenhances%2520the%2520training%2520procedure%2520itself.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20the%20Dynamics%20of%20Information%20Interplay%20in%20Supervised%20Learning&entry.906535625=Kun%20Song%20and%20Zhiquan%20Tan%20and%20Bochao%20Zou%20and%20Huimin%20Ma%20and%20Weiran%20Huang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20use%20matrix%20information%20theory%20as%20an%20analytical%20tool%20to%0Aanalyze%20the%20dynamics%20of%20the%20information%20interplay%20between%20data%20representations%0Aand%20classification%20head%20vectors%20in%20the%20supervised%20learning%20process.%0ASpecifically%2C%20inspired%20by%20the%20theory%20of%20Neural%20Collapse%2C%20we%20introduce%20matrix%0Amutual%20information%20ratio%20%28MIR%29%20and%20matrix%20entropy%20difference%20ratio%20%28HDR%29%20to%0Aassess%20the%20interactions%20of%20data%20representation%20and%20class%20classification%20heads%0Ain%20supervised%20learning%2C%20and%20we%20determine%20the%20theoretical%20optimal%20values%20for%20MIR%0Aand%20HDR%20when%20Neural%20Collapse%20happens.%20Our%20experiments%20show%20that%20MIR%20and%20HDR%20can%0Aeffectively%20explain%20many%20phenomena%20occurring%20in%20neural%20networks%2C%20for%20example%2C%0Athe%20standard%20supervised%20training%20dynamics%2C%20linear%20mode%20connectivity%2C%20and%20the%0Aperformance%20of%20label%20smoothing%20and%20pruning.%20Additionally%2C%20we%20use%20MIR%20and%20HDR%20to%0Again%20insights%20into%20the%20dynamics%20of%20grokking%2C%20which%20is%20an%20intriguing%20phenomenon%0Aobserved%20in%20supervised%20training%2C%20where%20the%20model%20demonstrates%20generalization%0Acapabilities%20long%20after%20it%20has%20learned%20to%20fit%20the%20training%20data.%20Furthermore%2C%0Awe%20introduce%20MIR%20and%20HDR%20as%20loss%20terms%20in%20supervised%20and%20semi-supervised%0Alearning%20to%20optimize%20the%20information%20interactions%20among%20samples%20and%0Aclassification%20heads.%20The%20empirical%20results%20provide%20evidence%20of%20the%20method%27s%0Aeffectiveness%2C%20demonstrating%20that%20the%20utilization%20of%20MIR%20and%20HDR%20not%20only%20aids%0Ain%20comprehending%20the%20dynamics%20throughout%20the%20training%20process%20but%20can%20also%0Aenhances%20the%20training%20procedure%20itself.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03999v1&entry.124074799=Read"},
{"title": "TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box\n  Identification", "author": "Martin Gubri and Dennis Ulmer and Hwaran Lee and Sangdoo Yun and Seong Joon Oh", "abstract": "  Large Language Model (LLM) services and models often come with legal rules on\nwho can use them and how they must use them. Assessing the compliance of the\nreleased LLMs is crucial, as these rules protect the interests of the LLM\ncontributor and prevent misuse. In this context, we describe the novel\nfingerprinting problem of Black-box Identity Verification (BBIV). The goal is\nto determine whether a third-party application uses a certain LLM through its\nchat function. We propose a method called Targeted Random Adversarial Prompt\n(TRAP) that identifies the specific LLM in use. We repurpose adversarial\nsuffixes, originally proposed for jailbreaking, to get a pre-defined answer\nfrom the target LLM, while other models give random answers. TRAP detects the\ntarget LLMs with over 95% true positive rate at under 0.2% false positive rate\neven after a single interaction. TRAP remains effective even if the LLM has\nminor changes that do not significantly alter the original function.\n", "link": "http://arxiv.org/abs/2402.12991v2", "date": "2024-06-06", "relevancy": 2.4076, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4979}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4973}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRAP%3A%20Targeted%20Random%20Adversarial%20Prompt%20Honeypot%20for%20Black-Box%0A%20%20Identification&body=Title%3A%20TRAP%3A%20Targeted%20Random%20Adversarial%20Prompt%20Honeypot%20for%20Black-Box%0A%20%20Identification%0AAuthor%3A%20Martin%20Gubri%20and%20Dennis%20Ulmer%20and%20Hwaran%20Lee%20and%20Sangdoo%20Yun%20and%20Seong%20Joon%20Oh%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29%20services%20and%20models%20often%20come%20with%20legal%20rules%20on%0Awho%20can%20use%20them%20and%20how%20they%20must%20use%20them.%20Assessing%20the%20compliance%20of%20the%0Areleased%20LLMs%20is%20crucial%2C%20as%20these%20rules%20protect%20the%20interests%20of%20the%20LLM%0Acontributor%20and%20prevent%20misuse.%20In%20this%20context%2C%20we%20describe%20the%20novel%0Afingerprinting%20problem%20of%20Black-box%20Identity%20Verification%20%28BBIV%29.%20The%20goal%20is%0Ato%20determine%20whether%20a%20third-party%20application%20uses%20a%20certain%20LLM%20through%20its%0Achat%20function.%20We%20propose%20a%20method%20called%20Targeted%20Random%20Adversarial%20Prompt%0A%28TRAP%29%20that%20identifies%20the%20specific%20LLM%20in%20use.%20We%20repurpose%20adversarial%0Asuffixes%2C%20originally%20proposed%20for%20jailbreaking%2C%20to%20get%20a%20pre-defined%20answer%0Afrom%20the%20target%20LLM%2C%20while%20other%20models%20give%20random%20answers.%20TRAP%20detects%20the%0Atarget%20LLMs%20with%20over%2095%25%20true%20positive%20rate%20at%20under%200.2%25%20false%20positive%20rate%0Aeven%20after%20a%20single%20interaction.%20TRAP%20remains%20effective%20even%20if%20the%20LLM%20has%0Aminor%20changes%20that%20do%20not%20significantly%20alter%20the%20original%20function.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12991v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRAP%253A%2520Targeted%2520Random%2520Adversarial%2520Prompt%2520Honeypot%2520for%2520Black-Box%250A%2520%2520Identification%26entry.906535625%3DMartin%2520Gubri%2520and%2520Dennis%2520Ulmer%2520and%2520Hwaran%2520Lee%2520and%2520Sangdoo%2520Yun%2520and%2520Seong%2520Joon%2520Oh%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520services%2520and%2520models%2520often%2520come%2520with%2520legal%2520rules%2520on%250Awho%2520can%2520use%2520them%2520and%2520how%2520they%2520must%2520use%2520them.%2520Assessing%2520the%2520compliance%2520of%2520the%250Areleased%2520LLMs%2520is%2520crucial%252C%2520as%2520these%2520rules%2520protect%2520the%2520interests%2520of%2520the%2520LLM%250Acontributor%2520and%2520prevent%2520misuse.%2520In%2520this%2520context%252C%2520we%2520describe%2520the%2520novel%250Afingerprinting%2520problem%2520of%2520Black-box%2520Identity%2520Verification%2520%2528BBIV%2529.%2520The%2520goal%2520is%250Ato%2520determine%2520whether%2520a%2520third-party%2520application%2520uses%2520a%2520certain%2520LLM%2520through%2520its%250Achat%2520function.%2520We%2520propose%2520a%2520method%2520called%2520Targeted%2520Random%2520Adversarial%2520Prompt%250A%2528TRAP%2529%2520that%2520identifies%2520the%2520specific%2520LLM%2520in%2520use.%2520We%2520repurpose%2520adversarial%250Asuffixes%252C%2520originally%2520proposed%2520for%2520jailbreaking%252C%2520to%2520get%2520a%2520pre-defined%2520answer%250Afrom%2520the%2520target%2520LLM%252C%2520while%2520other%2520models%2520give%2520random%2520answers.%2520TRAP%2520detects%2520the%250Atarget%2520LLMs%2520with%2520over%252095%2525%2520true%2520positive%2520rate%2520at%2520under%25200.2%2525%2520false%2520positive%2520rate%250Aeven%2520after%2520a%2520single%2520interaction.%2520TRAP%2520remains%2520effective%2520even%2520if%2520the%2520LLM%2520has%250Aminor%2520changes%2520that%2520do%2520not%2520significantly%2520alter%2520the%2520original%2520function.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12991v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRAP%3A%20Targeted%20Random%20Adversarial%20Prompt%20Honeypot%20for%20Black-Box%0A%20%20Identification&entry.906535625=Martin%20Gubri%20and%20Dennis%20Ulmer%20and%20Hwaran%20Lee%20and%20Sangdoo%20Yun%20and%20Seong%20Joon%20Oh&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29%20services%20and%20models%20often%20come%20with%20legal%20rules%20on%0Awho%20can%20use%20them%20and%20how%20they%20must%20use%20them.%20Assessing%20the%20compliance%20of%20the%0Areleased%20LLMs%20is%20crucial%2C%20as%20these%20rules%20protect%20the%20interests%20of%20the%20LLM%0Acontributor%20and%20prevent%20misuse.%20In%20this%20context%2C%20we%20describe%20the%20novel%0Afingerprinting%20problem%20of%20Black-box%20Identity%20Verification%20%28BBIV%29.%20The%20goal%20is%0Ato%20determine%20whether%20a%20third-party%20application%20uses%20a%20certain%20LLM%20through%20its%0Achat%20function.%20We%20propose%20a%20method%20called%20Targeted%20Random%20Adversarial%20Prompt%0A%28TRAP%29%20that%20identifies%20the%20specific%20LLM%20in%20use.%20We%20repurpose%20adversarial%0Asuffixes%2C%20originally%20proposed%20for%20jailbreaking%2C%20to%20get%20a%20pre-defined%20answer%0Afrom%20the%20target%20LLM%2C%20while%20other%20models%20give%20random%20answers.%20TRAP%20detects%20the%0Atarget%20LLMs%20with%20over%2095%25%20true%20positive%20rate%20at%20under%200.2%25%20false%20positive%20rate%0Aeven%20after%20a%20single%20interaction.%20TRAP%20remains%20effective%20even%20if%20the%20LLM%20has%0Aminor%20changes%20that%20do%20not%20significantly%20alter%20the%20original%20function.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12991v2&entry.124074799=Read"},
{"title": "Global Parameterization-based Texture Space Optimization", "author": "Wei Chen and Yuxue Ren and Na Lei and Zhongxuan Luo and Xianfeng Gu", "abstract": "  Texture mapping is a common technology in the area of computer graphics, it\nmaps the 3D surface space onto the 2D texture space. However, the loose texture\nspace will reduce the efficiency of data storage and GPU memory addressing in\nthe rendering process. Many of the existing methods focus on repacking given\ntextures, but they still suffer from high computational cost and hardly produce\na wholly tight texture space. In this paper, we propose a method to optimize\nthe texture space and produce a new texture mapping which is compact based on\nglobal parameterization. The proposed method is computationally robust and\nefficient. Experiments show the effectiveness of the proposed method and the\npotency in improving the storage and rendering efficiency.\n", "link": "http://arxiv.org/abs/2406.04115v1", "date": "2024-06-06", "relevancy": 2.4066, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5124}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4753}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20Parameterization-based%20Texture%20Space%20Optimization&body=Title%3A%20Global%20Parameterization-based%20Texture%20Space%20Optimization%0AAuthor%3A%20Wei%20Chen%20and%20Yuxue%20Ren%20and%20Na%20Lei%20and%20Zhongxuan%20Luo%20and%20Xianfeng%20Gu%0AAbstract%3A%20%20%20Texture%20mapping%20is%20a%20common%20technology%20in%20the%20area%20of%20computer%20graphics%2C%20it%0Amaps%20the%203D%20surface%20space%20onto%20the%202D%20texture%20space.%20However%2C%20the%20loose%20texture%0Aspace%20will%20reduce%20the%20efficiency%20of%20data%20storage%20and%20GPU%20memory%20addressing%20in%0Athe%20rendering%20process.%20Many%20of%20the%20existing%20methods%20focus%20on%20repacking%20given%0Atextures%2C%20but%20they%20still%20suffer%20from%20high%20computational%20cost%20and%20hardly%20produce%0Aa%20wholly%20tight%20texture%20space.%20In%20this%20paper%2C%20we%20propose%20a%20method%20to%20optimize%0Athe%20texture%20space%20and%20produce%20a%20new%20texture%20mapping%20which%20is%20compact%20based%20on%0Aglobal%20parameterization.%20The%20proposed%20method%20is%20computationally%20robust%20and%0Aefficient.%20Experiments%20show%20the%20effectiveness%20of%20the%20proposed%20method%20and%20the%0Apotency%20in%20improving%20the%20storage%20and%20rendering%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04115v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520Parameterization-based%2520Texture%2520Space%2520Optimization%26entry.906535625%3DWei%2520Chen%2520and%2520Yuxue%2520Ren%2520and%2520Na%2520Lei%2520and%2520Zhongxuan%2520Luo%2520and%2520Xianfeng%2520Gu%26entry.1292438233%3D%2520%2520Texture%2520mapping%2520is%2520a%2520common%2520technology%2520in%2520the%2520area%2520of%2520computer%2520graphics%252C%2520it%250Amaps%2520the%25203D%2520surface%2520space%2520onto%2520the%25202D%2520texture%2520space.%2520However%252C%2520the%2520loose%2520texture%250Aspace%2520will%2520reduce%2520the%2520efficiency%2520of%2520data%2520storage%2520and%2520GPU%2520memory%2520addressing%2520in%250Athe%2520rendering%2520process.%2520Many%2520of%2520the%2520existing%2520methods%2520focus%2520on%2520repacking%2520given%250Atextures%252C%2520but%2520they%2520still%2520suffer%2520from%2520high%2520computational%2520cost%2520and%2520hardly%2520produce%250Aa%2520wholly%2520tight%2520texture%2520space.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520method%2520to%2520optimize%250Athe%2520texture%2520space%2520and%2520produce%2520a%2520new%2520texture%2520mapping%2520which%2520is%2520compact%2520based%2520on%250Aglobal%2520parameterization.%2520The%2520proposed%2520method%2520is%2520computationally%2520robust%2520and%250Aefficient.%2520Experiments%2520show%2520the%2520effectiveness%2520of%2520the%2520proposed%2520method%2520and%2520the%250Apotency%2520in%2520improving%2520the%2520storage%2520and%2520rendering%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04115v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20Parameterization-based%20Texture%20Space%20Optimization&entry.906535625=Wei%20Chen%20and%20Yuxue%20Ren%20and%20Na%20Lei%20and%20Zhongxuan%20Luo%20and%20Xianfeng%20Gu&entry.1292438233=%20%20Texture%20mapping%20is%20a%20common%20technology%20in%20the%20area%20of%20computer%20graphics%2C%20it%0Amaps%20the%203D%20surface%20space%20onto%20the%202D%20texture%20space.%20However%2C%20the%20loose%20texture%0Aspace%20will%20reduce%20the%20efficiency%20of%20data%20storage%20and%20GPU%20memory%20addressing%20in%0Athe%20rendering%20process.%20Many%20of%20the%20existing%20methods%20focus%20on%20repacking%20given%0Atextures%2C%20but%20they%20still%20suffer%20from%20high%20computational%20cost%20and%20hardly%20produce%0Aa%20wholly%20tight%20texture%20space.%20In%20this%20paper%2C%20we%20propose%20a%20method%20to%20optimize%0Athe%20texture%20space%20and%20produce%20a%20new%20texture%20mapping%20which%20is%20compact%20based%20on%0Aglobal%20parameterization.%20The%20proposed%20method%20is%20computationally%20robust%20and%0Aefficient.%20Experiments%20show%20the%20effectiveness%20of%20the%20proposed%20method%20and%20the%0Apotency%20in%20improving%20the%20storage%20and%20rendering%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04115v1&entry.124074799=Read"},
{"title": "Benchmarking General Purpose In-Context Learning", "author": "Fan Wang and Chuan Lin and Yang Cao and Yu Kang", "abstract": "  In-context learning (ICL) is becoming increasingly appealing to the AI\ncommunity due to its flexibility, generality, sample efficiency, and exemption\nfrom artificial optimization skills. It is desirable to further enhance the\ngenerality and capability of ICL, which gives rise to the concept of\ngeneral-purpose in-context learning (GPICL). We aim to extend ICL to address a\nbroader range of tasks with an extended learning horizon and higher improvement\npotential, albeit with relatively limited zero-shot generalization. To this\nend, we introduce two lightweight but insightful benchmarks specifically\ncrafted to train and evaluate GPICL functionalities. Each benchmark includes a\nvast number of tasks characterized by significant task variance, featuring\nminimal transferable knowledge among tasks. These tasks are designed to\nfacilitate lifelong in-context learning through continuous generation and\ninteraction. These features pose significant challenges for models that rely on\ncontext or interactions to improve their proficiency, including language\nmodels, decision models, and world models. Our experiments reveal that the\nscale of parameters alone may not be crucial for ICL or GPICL, suggesting\nalternative approaches such as increasing the scale of contexts and memory\nstates.\n", "link": "http://arxiv.org/abs/2405.17234v3", "date": "2024-06-06", "relevancy": 2.3875, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5118}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4661}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20General%20Purpose%20In-Context%20Learning&body=Title%3A%20Benchmarking%20General%20Purpose%20In-Context%20Learning%0AAuthor%3A%20Fan%20Wang%20and%20Chuan%20Lin%20and%20Yang%20Cao%20and%20Yu%20Kang%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20is%20becoming%20increasingly%20appealing%20to%20the%20AI%0Acommunity%20due%20to%20its%20flexibility%2C%20generality%2C%20sample%20efficiency%2C%20and%20exemption%0Afrom%20artificial%20optimization%20skills.%20It%20is%20desirable%20to%20further%20enhance%20the%0Agenerality%20and%20capability%20of%20ICL%2C%20which%20gives%20rise%20to%20the%20concept%20of%0Ageneral-purpose%20in-context%20learning%20%28GPICL%29.%20We%20aim%20to%20extend%20ICL%20to%20address%20a%0Abroader%20range%20of%20tasks%20with%20an%20extended%20learning%20horizon%20and%20higher%20improvement%0Apotential%2C%20albeit%20with%20relatively%20limited%20zero-shot%20generalization.%20To%20this%0Aend%2C%20we%20introduce%20two%20lightweight%20but%20insightful%20benchmarks%20specifically%0Acrafted%20to%20train%20and%20evaluate%20GPICL%20functionalities.%20Each%20benchmark%20includes%20a%0Avast%20number%20of%20tasks%20characterized%20by%20significant%20task%20variance%2C%20featuring%0Aminimal%20transferable%20knowledge%20among%20tasks.%20These%20tasks%20are%20designed%20to%0Afacilitate%20lifelong%20in-context%20learning%20through%20continuous%20generation%20and%0Ainteraction.%20These%20features%20pose%20significant%20challenges%20for%20models%20that%20rely%20on%0Acontext%20or%20interactions%20to%20improve%20their%20proficiency%2C%20including%20language%0Amodels%2C%20decision%20models%2C%20and%20world%20models.%20Our%20experiments%20reveal%20that%20the%0Ascale%20of%20parameters%20alone%20may%20not%20be%20crucial%20for%20ICL%20or%20GPICL%2C%20suggesting%0Aalternative%20approaches%20such%20as%20increasing%20the%20scale%20of%20contexts%20and%20memory%0Astates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17234v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520General%2520Purpose%2520In-Context%2520Learning%26entry.906535625%3DFan%2520Wang%2520and%2520Chuan%2520Lin%2520and%2520Yang%2520Cao%2520and%2520Yu%2520Kang%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520is%2520becoming%2520increasingly%2520appealing%2520to%2520the%2520AI%250Acommunity%2520due%2520to%2520its%2520flexibility%252C%2520generality%252C%2520sample%2520efficiency%252C%2520and%2520exemption%250Afrom%2520artificial%2520optimization%2520skills.%2520It%2520is%2520desirable%2520to%2520further%2520enhance%2520the%250Agenerality%2520and%2520capability%2520of%2520ICL%252C%2520which%2520gives%2520rise%2520to%2520the%2520concept%2520of%250Ageneral-purpose%2520in-context%2520learning%2520%2528GPICL%2529.%2520We%2520aim%2520to%2520extend%2520ICL%2520to%2520address%2520a%250Abroader%2520range%2520of%2520tasks%2520with%2520an%2520extended%2520learning%2520horizon%2520and%2520higher%2520improvement%250Apotential%252C%2520albeit%2520with%2520relatively%2520limited%2520zero-shot%2520generalization.%2520To%2520this%250Aend%252C%2520we%2520introduce%2520two%2520lightweight%2520but%2520insightful%2520benchmarks%2520specifically%250Acrafted%2520to%2520train%2520and%2520evaluate%2520GPICL%2520functionalities.%2520Each%2520benchmark%2520includes%2520a%250Avast%2520number%2520of%2520tasks%2520characterized%2520by%2520significant%2520task%2520variance%252C%2520featuring%250Aminimal%2520transferable%2520knowledge%2520among%2520tasks.%2520These%2520tasks%2520are%2520designed%2520to%250Afacilitate%2520lifelong%2520in-context%2520learning%2520through%2520continuous%2520generation%2520and%250Ainteraction.%2520These%2520features%2520pose%2520significant%2520challenges%2520for%2520models%2520that%2520rely%2520on%250Acontext%2520or%2520interactions%2520to%2520improve%2520their%2520proficiency%252C%2520including%2520language%250Amodels%252C%2520decision%2520models%252C%2520and%2520world%2520models.%2520Our%2520experiments%2520reveal%2520that%2520the%250Ascale%2520of%2520parameters%2520alone%2520may%2520not%2520be%2520crucial%2520for%2520ICL%2520or%2520GPICL%252C%2520suggesting%250Aalternative%2520approaches%2520such%2520as%2520increasing%2520the%2520scale%2520of%2520contexts%2520and%2520memory%250Astates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17234v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20General%20Purpose%20In-Context%20Learning&entry.906535625=Fan%20Wang%20and%20Chuan%20Lin%20and%20Yang%20Cao%20and%20Yu%20Kang&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20is%20becoming%20increasingly%20appealing%20to%20the%20AI%0Acommunity%20due%20to%20its%20flexibility%2C%20generality%2C%20sample%20efficiency%2C%20and%20exemption%0Afrom%20artificial%20optimization%20skills.%20It%20is%20desirable%20to%20further%20enhance%20the%0Agenerality%20and%20capability%20of%20ICL%2C%20which%20gives%20rise%20to%20the%20concept%20of%0Ageneral-purpose%20in-context%20learning%20%28GPICL%29.%20We%20aim%20to%20extend%20ICL%20to%20address%20a%0Abroader%20range%20of%20tasks%20with%20an%20extended%20learning%20horizon%20and%20higher%20improvement%0Apotential%2C%20albeit%20with%20relatively%20limited%20zero-shot%20generalization.%20To%20this%0Aend%2C%20we%20introduce%20two%20lightweight%20but%20insightful%20benchmarks%20specifically%0Acrafted%20to%20train%20and%20evaluate%20GPICL%20functionalities.%20Each%20benchmark%20includes%20a%0Avast%20number%20of%20tasks%20characterized%20by%20significant%20task%20variance%2C%20featuring%0Aminimal%20transferable%20knowledge%20among%20tasks.%20These%20tasks%20are%20designed%20to%0Afacilitate%20lifelong%20in-context%20learning%20through%20continuous%20generation%20and%0Ainteraction.%20These%20features%20pose%20significant%20challenges%20for%20models%20that%20rely%20on%0Acontext%20or%20interactions%20to%20improve%20their%20proficiency%2C%20including%20language%0Amodels%2C%20decision%20models%2C%20and%20world%20models.%20Our%20experiments%20reveal%20that%20the%0Ascale%20of%20parameters%20alone%20may%20not%20be%20crucial%20for%20ICL%20or%20GPICL%2C%20suggesting%0Aalternative%20approaches%20such%20as%20increasing%20the%20scale%20of%20contexts%20and%20memory%0Astates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17234v3&entry.124074799=Read"},
{"title": "E(n) Equivariant Message Passing Cellular Networks", "author": "Veljko Kova\u010d and Erik J. Bekkers and Pietro Li\u00f2 and Floor Eijkelboom", "abstract": "  This paper introduces E(n) Equivariant Message Passing Cellular Networks\n(EMPCNs), an extension of E(n) Equivariant Graph Neural Networks to\nCW-complexes. Our approach addresses two aspects of geometric message passing\nnetworks: 1) enhancing their expressiveness by incorporating arbitrary cells,\nand 2) achieving this in a computationally efficient way with a decoupled\nEMPCNs technique. We demonstrate that EMPCNs achieve close to state-of-the-art\nperformance on multiple tasks without the need for steerability, including\nmany-body predictions and motion capture. Moreover, ablation studies confirm\nthat decoupled EMPCNs exhibit stronger generalization capabilities than their\nnon-topologically informed counterparts. These findings show that EMPCNs can be\nused as a scalable and expressive framework for higher-order message passing in\ngeometric and topological graphs\n", "link": "http://arxiv.org/abs/2406.03145v2", "date": "2024-06-06", "relevancy": 2.3794, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4977}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4688}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E%28n%29%20Equivariant%20Message%20Passing%20Cellular%20Networks&body=Title%3A%20E%28n%29%20Equivariant%20Message%20Passing%20Cellular%20Networks%0AAuthor%3A%20Veljko%20Kova%C4%8D%20and%20Erik%20J.%20Bekkers%20and%20Pietro%20Li%C3%B2%20and%20Floor%20Eijkelboom%0AAbstract%3A%20%20%20This%20paper%20introduces%20E%28n%29%20Equivariant%20Message%20Passing%20Cellular%20Networks%0A%28EMPCNs%29%2C%20an%20extension%20of%20E%28n%29%20Equivariant%20Graph%20Neural%20Networks%20to%0ACW-complexes.%20Our%20approach%20addresses%20two%20aspects%20of%20geometric%20message%20passing%0Anetworks%3A%201%29%20enhancing%20their%20expressiveness%20by%20incorporating%20arbitrary%20cells%2C%0Aand%202%29%20achieving%20this%20in%20a%20computationally%20efficient%20way%20with%20a%20decoupled%0AEMPCNs%20technique.%20We%20demonstrate%20that%20EMPCNs%20achieve%20close%20to%20state-of-the-art%0Aperformance%20on%20multiple%20tasks%20without%20the%20need%20for%20steerability%2C%20including%0Amany-body%20predictions%20and%20motion%20capture.%20Moreover%2C%20ablation%20studies%20confirm%0Athat%20decoupled%20EMPCNs%20exhibit%20stronger%20generalization%20capabilities%20than%20their%0Anon-topologically%20informed%20counterparts.%20These%20findings%20show%20that%20EMPCNs%20can%20be%0Aused%20as%20a%20scalable%20and%20expressive%20framework%20for%20higher-order%20message%20passing%20in%0Ageometric%20and%20topological%20graphs%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03145v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE%2528n%2529%2520Equivariant%2520Message%2520Passing%2520Cellular%2520Networks%26entry.906535625%3DVeljko%2520Kova%25C4%258D%2520and%2520Erik%2520J.%2520Bekkers%2520and%2520Pietro%2520Li%25C3%25B2%2520and%2520Floor%2520Eijkelboom%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520E%2528n%2529%2520Equivariant%2520Message%2520Passing%2520Cellular%2520Networks%250A%2528EMPCNs%2529%252C%2520an%2520extension%2520of%2520E%2528n%2529%2520Equivariant%2520Graph%2520Neural%2520Networks%2520to%250ACW-complexes.%2520Our%2520approach%2520addresses%2520two%2520aspects%2520of%2520geometric%2520message%2520passing%250Anetworks%253A%25201%2529%2520enhancing%2520their%2520expressiveness%2520by%2520incorporating%2520arbitrary%2520cells%252C%250Aand%25202%2529%2520achieving%2520this%2520in%2520a%2520computationally%2520efficient%2520way%2520with%2520a%2520decoupled%250AEMPCNs%2520technique.%2520We%2520demonstrate%2520that%2520EMPCNs%2520achieve%2520close%2520to%2520state-of-the-art%250Aperformance%2520on%2520multiple%2520tasks%2520without%2520the%2520need%2520for%2520steerability%252C%2520including%250Amany-body%2520predictions%2520and%2520motion%2520capture.%2520Moreover%252C%2520ablation%2520studies%2520confirm%250Athat%2520decoupled%2520EMPCNs%2520exhibit%2520stronger%2520generalization%2520capabilities%2520than%2520their%250Anon-topologically%2520informed%2520counterparts.%2520These%2520findings%2520show%2520that%2520EMPCNs%2520can%2520be%250Aused%2520as%2520a%2520scalable%2520and%2520expressive%2520framework%2520for%2520higher-order%2520message%2520passing%2520in%250Ageometric%2520and%2520topological%2520graphs%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03145v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E%28n%29%20Equivariant%20Message%20Passing%20Cellular%20Networks&entry.906535625=Veljko%20Kova%C4%8D%20and%20Erik%20J.%20Bekkers%20and%20Pietro%20Li%C3%B2%20and%20Floor%20Eijkelboom&entry.1292438233=%20%20This%20paper%20introduces%20E%28n%29%20Equivariant%20Message%20Passing%20Cellular%20Networks%0A%28EMPCNs%29%2C%20an%20extension%20of%20E%28n%29%20Equivariant%20Graph%20Neural%20Networks%20to%0ACW-complexes.%20Our%20approach%20addresses%20two%20aspects%20of%20geometric%20message%20passing%0Anetworks%3A%201%29%20enhancing%20their%20expressiveness%20by%20incorporating%20arbitrary%20cells%2C%0Aand%202%29%20achieving%20this%20in%20a%20computationally%20efficient%20way%20with%20a%20decoupled%0AEMPCNs%20technique.%20We%20demonstrate%20that%20EMPCNs%20achieve%20close%20to%20state-of-the-art%0Aperformance%20on%20multiple%20tasks%20without%20the%20need%20for%20steerability%2C%20including%0Amany-body%20predictions%20and%20motion%20capture.%20Moreover%2C%20ablation%20studies%20confirm%0Athat%20decoupled%20EMPCNs%20exhibit%20stronger%20generalization%20capabilities%20than%20their%0Anon-topologically%20informed%20counterparts.%20These%20findings%20show%20that%20EMPCNs%20can%20be%0Aused%20as%20a%20scalable%20and%20expressive%20framework%20for%20higher-order%20message%20passing%20in%0Ageometric%20and%20topological%20graphs%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03145v2&entry.124074799=Read"},
{"title": "Zero-Painter: Training-Free Layout Control for Text-to-Image Synthesis", "author": "Marianna Ohanyan and Hayk Manukyan and Zhangyang Wang and Shant Navasardyan and Humphrey Shi", "abstract": "  We present Zero-Painter, a novel training-free framework for\nlayout-conditional text-to-image synthesis that facilitates the creation of\ndetailed and controlled imagery from textual prompts. Our method utilizes\nobject masks and individual descriptions, coupled with a global text prompt, to\ngenerate images with high fidelity. Zero-Painter employs a two-stage process\ninvolving our novel Prompt-Adjusted Cross-Attention (PACA) and Region-Grouped\nCross-Attention (ReGCA) blocks, ensuring precise alignment of generated objects\nwith textual prompts and mask shapes. Our extensive experiments demonstrate\nthat Zero-Painter surpasses current state-of-the-art methods in preserving\ntextual details and adhering to mask shapes.\n", "link": "http://arxiv.org/abs/2406.04032v1", "date": "2024-06-06", "relevancy": 2.3396, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5967}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5831}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Painter%3A%20Training-Free%20Layout%20Control%20for%20Text-to-Image%20Synthesis&body=Title%3A%20Zero-Painter%3A%20Training-Free%20Layout%20Control%20for%20Text-to-Image%20Synthesis%0AAuthor%3A%20Marianna%20Ohanyan%20and%20Hayk%20Manukyan%20and%20Zhangyang%20Wang%20and%20Shant%20Navasardyan%20and%20Humphrey%20Shi%0AAbstract%3A%20%20%20We%20present%20Zero-Painter%2C%20a%20novel%20training-free%20framework%20for%0Alayout-conditional%20text-to-image%20synthesis%20that%20facilitates%20the%20creation%20of%0Adetailed%20and%20controlled%20imagery%20from%20textual%20prompts.%20Our%20method%20utilizes%0Aobject%20masks%20and%20individual%20descriptions%2C%20coupled%20with%20a%20global%20text%20prompt%2C%20to%0Agenerate%20images%20with%20high%20fidelity.%20Zero-Painter%20employs%20a%20two-stage%20process%0Ainvolving%20our%20novel%20Prompt-Adjusted%20Cross-Attention%20%28PACA%29%20and%20Region-Grouped%0ACross-Attention%20%28ReGCA%29%20blocks%2C%20ensuring%20precise%20alignment%20of%20generated%20objects%0Awith%20textual%20prompts%20and%20mask%20shapes.%20Our%20extensive%20experiments%20demonstrate%0Athat%20Zero-Painter%20surpasses%20current%20state-of-the-art%20methods%20in%20preserving%0Atextual%20details%20and%20adhering%20to%20mask%20shapes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Painter%253A%2520Training-Free%2520Layout%2520Control%2520for%2520Text-to-Image%2520Synthesis%26entry.906535625%3DMarianna%2520Ohanyan%2520and%2520Hayk%2520Manukyan%2520and%2520Zhangyang%2520Wang%2520and%2520Shant%2520Navasardyan%2520and%2520Humphrey%2520Shi%26entry.1292438233%3D%2520%2520We%2520present%2520Zero-Painter%252C%2520a%2520novel%2520training-free%2520framework%2520for%250Alayout-conditional%2520text-to-image%2520synthesis%2520that%2520facilitates%2520the%2520creation%2520of%250Adetailed%2520and%2520controlled%2520imagery%2520from%2520textual%2520prompts.%2520Our%2520method%2520utilizes%250Aobject%2520masks%2520and%2520individual%2520descriptions%252C%2520coupled%2520with%2520a%2520global%2520text%2520prompt%252C%2520to%250Agenerate%2520images%2520with%2520high%2520fidelity.%2520Zero-Painter%2520employs%2520a%2520two-stage%2520process%250Ainvolving%2520our%2520novel%2520Prompt-Adjusted%2520Cross-Attention%2520%2528PACA%2529%2520and%2520Region-Grouped%250ACross-Attention%2520%2528ReGCA%2529%2520blocks%252C%2520ensuring%2520precise%2520alignment%2520of%2520generated%2520objects%250Awith%2520textual%2520prompts%2520and%2520mask%2520shapes.%2520Our%2520extensive%2520experiments%2520demonstrate%250Athat%2520Zero-Painter%2520surpasses%2520current%2520state-of-the-art%2520methods%2520in%2520preserving%250Atextual%2520details%2520and%2520adhering%2520to%2520mask%2520shapes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Painter%3A%20Training-Free%20Layout%20Control%20for%20Text-to-Image%20Synthesis&entry.906535625=Marianna%20Ohanyan%20and%20Hayk%20Manukyan%20and%20Zhangyang%20Wang%20and%20Shant%20Navasardyan%20and%20Humphrey%20Shi&entry.1292438233=%20%20We%20present%20Zero-Painter%2C%20a%20novel%20training-free%20framework%20for%0Alayout-conditional%20text-to-image%20synthesis%20that%20facilitates%20the%20creation%20of%0Adetailed%20and%20controlled%20imagery%20from%20textual%20prompts.%20Our%20method%20utilizes%0Aobject%20masks%20and%20individual%20descriptions%2C%20coupled%20with%20a%20global%20text%20prompt%2C%20to%0Agenerate%20images%20with%20high%20fidelity.%20Zero-Painter%20employs%20a%20two-stage%20process%0Ainvolving%20our%20novel%20Prompt-Adjusted%20Cross-Attention%20%28PACA%29%20and%20Region-Grouped%0ACross-Attention%20%28ReGCA%29%20blocks%2C%20ensuring%20precise%20alignment%20of%20generated%20objects%0Awith%20textual%20prompts%20and%20mask%20shapes.%20Our%20extensive%20experiments%20demonstrate%0Athat%20Zero-Painter%20surpasses%20current%20state-of-the-art%20methods%20in%20preserving%0Atextual%20details%20and%20adhering%20to%20mask%20shapes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04032v1&entry.124074799=Read"},
{"title": "Unleashing HyDRa: Hybrid Fusion, Depth Consistency and Radar for Unified\n  3D Perception", "author": "Philipp Wolters and Johannes Gilg and Torben Teepe and Fabian Herzog and Anouar Laouichi and Martin Hofmann and Gerhard Rigoll", "abstract": "  Low-cost, vision-centric 3D perception systems for autonomous driving have\nmade significant progress in recent years, narrowing the gap to expensive\nLiDAR-based methods. The primary challenge in becoming a fully reliable\nalternative lies in robust depth prediction capabilities, as camera-based\nsystems struggle with long detection ranges and adverse lighting and weather\nconditions. In this work, we introduce HyDRa, a novel camera-radar fusion\narchitecture for diverse 3D perception tasks. Building upon the principles of\ndense BEV (Bird's Eye View)-based architectures, HyDRa introduces a hybrid\nfusion approach to combine the strengths of complementary camera and radar\nfeatures in two distinct representation spaces. Our Height Association\nTransformer module leverages radar features already in the perspective view to\nproduce more robust and accurate depth predictions. In the BEV, we refine the\ninitial sparse representation by a Radar-weighted Depth Consistency. HyDRa\nachieves a new state-of-the-art for camera-radar fusion of 64.2 NDS (+1.8) and\n58.4 AMOTA (+1.5) on the public nuScenes dataset. Moreover, our new\nsemantically rich and spatially accurate BEV features can be directly converted\ninto a powerful occupancy representation, beating all previous camera-based\nmethods on the Occ3D benchmark by an impressive 3.7 mIoU. Code and models are\navailable at https://github.com/phi-wol/hydra.\n", "link": "http://arxiv.org/abs/2403.07746v2", "date": "2024-06-06", "relevancy": 2.2832, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5796}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5776}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20HyDRa%3A%20Hybrid%20Fusion%2C%20Depth%20Consistency%20and%20Radar%20for%20Unified%0A%20%203D%20Perception&body=Title%3A%20Unleashing%20HyDRa%3A%20Hybrid%20Fusion%2C%20Depth%20Consistency%20and%20Radar%20for%20Unified%0A%20%203D%20Perception%0AAuthor%3A%20Philipp%20Wolters%20and%20Johannes%20Gilg%20and%20Torben%20Teepe%20and%20Fabian%20Herzog%20and%20Anouar%20Laouichi%20and%20Martin%20Hofmann%20and%20Gerhard%20Rigoll%0AAbstract%3A%20%20%20Low-cost%2C%20vision-centric%203D%20perception%20systems%20for%20autonomous%20driving%20have%0Amade%20significant%20progress%20in%20recent%20years%2C%20narrowing%20the%20gap%20to%20expensive%0ALiDAR-based%20methods.%20The%20primary%20challenge%20in%20becoming%20a%20fully%20reliable%0Aalternative%20lies%20in%20robust%20depth%20prediction%20capabilities%2C%20as%20camera-based%0Asystems%20struggle%20with%20long%20detection%20ranges%20and%20adverse%20lighting%20and%20weather%0Aconditions.%20In%20this%20work%2C%20we%20introduce%20HyDRa%2C%20a%20novel%20camera-radar%20fusion%0Aarchitecture%20for%20diverse%203D%20perception%20tasks.%20Building%20upon%20the%20principles%20of%0Adense%20BEV%20%28Bird%27s%20Eye%20View%29-based%20architectures%2C%20HyDRa%20introduces%20a%20hybrid%0Afusion%20approach%20to%20combine%20the%20strengths%20of%20complementary%20camera%20and%20radar%0Afeatures%20in%20two%20distinct%20representation%20spaces.%20Our%20Height%20Association%0ATransformer%20module%20leverages%20radar%20features%20already%20in%20the%20perspective%20view%20to%0Aproduce%20more%20robust%20and%20accurate%20depth%20predictions.%20In%20the%20BEV%2C%20we%20refine%20the%0Ainitial%20sparse%20representation%20by%20a%20Radar-weighted%20Depth%20Consistency.%20HyDRa%0Aachieves%20a%20new%20state-of-the-art%20for%20camera-radar%20fusion%20of%2064.2%20NDS%20%28%2B1.8%29%20and%0A58.4%20AMOTA%20%28%2B1.5%29%20on%20the%20public%20nuScenes%20dataset.%20Moreover%2C%20our%20new%0Asemantically%20rich%20and%20spatially%20accurate%20BEV%20features%20can%20be%20directly%20converted%0Ainto%20a%20powerful%20occupancy%20representation%2C%20beating%20all%20previous%20camera-based%0Amethods%20on%20the%20Occ3D%20benchmark%20by%20an%20impressive%203.7%20mIoU.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/phi-wol/hydra.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07746v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520HyDRa%253A%2520Hybrid%2520Fusion%252C%2520Depth%2520Consistency%2520and%2520Radar%2520for%2520Unified%250A%2520%25203D%2520Perception%26entry.906535625%3DPhilipp%2520Wolters%2520and%2520Johannes%2520Gilg%2520and%2520Torben%2520Teepe%2520and%2520Fabian%2520Herzog%2520and%2520Anouar%2520Laouichi%2520and%2520Martin%2520Hofmann%2520and%2520Gerhard%2520Rigoll%26entry.1292438233%3D%2520%2520Low-cost%252C%2520vision-centric%25203D%2520perception%2520systems%2520for%2520autonomous%2520driving%2520have%250Amade%2520significant%2520progress%2520in%2520recent%2520years%252C%2520narrowing%2520the%2520gap%2520to%2520expensive%250ALiDAR-based%2520methods.%2520The%2520primary%2520challenge%2520in%2520becoming%2520a%2520fully%2520reliable%250Aalternative%2520lies%2520in%2520robust%2520depth%2520prediction%2520capabilities%252C%2520as%2520camera-based%250Asystems%2520struggle%2520with%2520long%2520detection%2520ranges%2520and%2520adverse%2520lighting%2520and%2520weather%250Aconditions.%2520In%2520this%2520work%252C%2520we%2520introduce%2520HyDRa%252C%2520a%2520novel%2520camera-radar%2520fusion%250Aarchitecture%2520for%2520diverse%25203D%2520perception%2520tasks.%2520Building%2520upon%2520the%2520principles%2520of%250Adense%2520BEV%2520%2528Bird%2527s%2520Eye%2520View%2529-based%2520architectures%252C%2520HyDRa%2520introduces%2520a%2520hybrid%250Afusion%2520approach%2520to%2520combine%2520the%2520strengths%2520of%2520complementary%2520camera%2520and%2520radar%250Afeatures%2520in%2520two%2520distinct%2520representation%2520spaces.%2520Our%2520Height%2520Association%250ATransformer%2520module%2520leverages%2520radar%2520features%2520already%2520in%2520the%2520perspective%2520view%2520to%250Aproduce%2520more%2520robust%2520and%2520accurate%2520depth%2520predictions.%2520In%2520the%2520BEV%252C%2520we%2520refine%2520the%250Ainitial%2520sparse%2520representation%2520by%2520a%2520Radar-weighted%2520Depth%2520Consistency.%2520HyDRa%250Aachieves%2520a%2520new%2520state-of-the-art%2520for%2520camera-radar%2520fusion%2520of%252064.2%2520NDS%2520%2528%252B1.8%2529%2520and%250A58.4%2520AMOTA%2520%2528%252B1.5%2529%2520on%2520the%2520public%2520nuScenes%2520dataset.%2520Moreover%252C%2520our%2520new%250Asemantically%2520rich%2520and%2520spatially%2520accurate%2520BEV%2520features%2520can%2520be%2520directly%2520converted%250Ainto%2520a%2520powerful%2520occupancy%2520representation%252C%2520beating%2520all%2520previous%2520camera-based%250Amethods%2520on%2520the%2520Occ3D%2520benchmark%2520by%2520an%2520impressive%25203.7%2520mIoU.%2520Code%2520and%2520models%2520are%250Aavailable%2520at%2520https%253A//github.com/phi-wol/hydra.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07746v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20HyDRa%3A%20Hybrid%20Fusion%2C%20Depth%20Consistency%20and%20Radar%20for%20Unified%0A%20%203D%20Perception&entry.906535625=Philipp%20Wolters%20and%20Johannes%20Gilg%20and%20Torben%20Teepe%20and%20Fabian%20Herzog%20and%20Anouar%20Laouichi%20and%20Martin%20Hofmann%20and%20Gerhard%20Rigoll&entry.1292438233=%20%20Low-cost%2C%20vision-centric%203D%20perception%20systems%20for%20autonomous%20driving%20have%0Amade%20significant%20progress%20in%20recent%20years%2C%20narrowing%20the%20gap%20to%20expensive%0ALiDAR-based%20methods.%20The%20primary%20challenge%20in%20becoming%20a%20fully%20reliable%0Aalternative%20lies%20in%20robust%20depth%20prediction%20capabilities%2C%20as%20camera-based%0Asystems%20struggle%20with%20long%20detection%20ranges%20and%20adverse%20lighting%20and%20weather%0Aconditions.%20In%20this%20work%2C%20we%20introduce%20HyDRa%2C%20a%20novel%20camera-radar%20fusion%0Aarchitecture%20for%20diverse%203D%20perception%20tasks.%20Building%20upon%20the%20principles%20of%0Adense%20BEV%20%28Bird%27s%20Eye%20View%29-based%20architectures%2C%20HyDRa%20introduces%20a%20hybrid%0Afusion%20approach%20to%20combine%20the%20strengths%20of%20complementary%20camera%20and%20radar%0Afeatures%20in%20two%20distinct%20representation%20spaces.%20Our%20Height%20Association%0ATransformer%20module%20leverages%20radar%20features%20already%20in%20the%20perspective%20view%20to%0Aproduce%20more%20robust%20and%20accurate%20depth%20predictions.%20In%20the%20BEV%2C%20we%20refine%20the%0Ainitial%20sparse%20representation%20by%20a%20Radar-weighted%20Depth%20Consistency.%20HyDRa%0Aachieves%20a%20new%20state-of-the-art%20for%20camera-radar%20fusion%20of%2064.2%20NDS%20%28%2B1.8%29%20and%0A58.4%20AMOTA%20%28%2B1.5%29%20on%20the%20public%20nuScenes%20dataset.%20Moreover%2C%20our%20new%0Asemantically%20rich%20and%20spatially%20accurate%20BEV%20features%20can%20be%20directly%20converted%0Ainto%20a%20powerful%20occupancy%20representation%2C%20beating%20all%20previous%20camera-based%0Amethods%20on%20the%20Occ3D%20benchmark%20by%20an%20impressive%203.7%20mIoU.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/phi-wol/hydra.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07746v2&entry.124074799=Read"},
{"title": "A Class-Aware Representation Refinement Framework for Graph\n  Classification", "author": "Jiaxing Xu and Jinjie Ni and Yiping Ke", "abstract": "  Graph Neural Networks (GNNs) are widely used for graph representation\nlearning. Despite its prevalence, GNN suffers from two drawbacks in the graph\nclassification task, the neglect of graph-level relationships, and the\ngeneralization issue. Each graph is treated separately in GNN message\npassing/graph pooling, and existing methods to address overfitting operate on\neach individual graph. This makes the graph representations learnt less\neffective in the downstream classification. In this paper, we propose a\nClass-Aware Representation rEfinement (CARE) framework for the task of graph\nclassification. CARE computes simple yet powerful class representations and\ninjects them to steer the learning of graph representations towards better\nclass separability. CARE is a plug-and-play framework that is highly flexible\nand able to incorporate arbitrary GNN backbones without significantly\nincreasing the computational cost. We also theoretically prove that CARE has a\nbetter generalization upper bound than its GNN backbone through\nVapnik-Chervonenkis (VC) dimension analysis. Our extensive experiments with 11\nwell-known GNN backbones on 9 benchmark datasets validate the superiority and\neffectiveness of CARE over its GNN counterparts.\n", "link": "http://arxiv.org/abs/2209.00936v2", "date": "2024-06-06", "relevancy": 2.281, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4863}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4585}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4237}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Class-Aware%20Representation%20Refinement%20Framework%20for%20Graph%0A%20%20Classification&body=Title%3A%20A%20Class-Aware%20Representation%20Refinement%20Framework%20for%20Graph%0A%20%20Classification%0AAuthor%3A%20Jiaxing%20Xu%20and%20Jinjie%20Ni%20and%20Yiping%20Ke%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20widely%20used%20for%20graph%20representation%0Alearning.%20Despite%20its%20prevalence%2C%20GNN%20suffers%20from%20two%20drawbacks%20in%20the%20graph%0Aclassification%20task%2C%20the%20neglect%20of%20graph-level%20relationships%2C%20and%20the%0Ageneralization%20issue.%20Each%20graph%20is%20treated%20separately%20in%20GNN%20message%0Apassing/graph%20pooling%2C%20and%20existing%20methods%20to%20address%20overfitting%20operate%20on%0Aeach%20individual%20graph.%20This%20makes%20the%20graph%20representations%20learnt%20less%0Aeffective%20in%20the%20downstream%20classification.%20In%20this%20paper%2C%20we%20propose%20a%0AClass-Aware%20Representation%20rEfinement%20%28CARE%29%20framework%20for%20the%20task%20of%20graph%0Aclassification.%20CARE%20computes%20simple%20yet%20powerful%20class%20representations%20and%0Ainjects%20them%20to%20steer%20the%20learning%20of%20graph%20representations%20towards%20better%0Aclass%20separability.%20CARE%20is%20a%20plug-and-play%20framework%20that%20is%20highly%20flexible%0Aand%20able%20to%20incorporate%20arbitrary%20GNN%20backbones%20without%20significantly%0Aincreasing%20the%20computational%20cost.%20We%20also%20theoretically%20prove%20that%20CARE%20has%20a%0Abetter%20generalization%20upper%20bound%20than%20its%20GNN%20backbone%20through%0AVapnik-Chervonenkis%20%28VC%29%20dimension%20analysis.%20Our%20extensive%20experiments%20with%2011%0Awell-known%20GNN%20backbones%20on%209%20benchmark%20datasets%20validate%20the%20superiority%20and%0Aeffectiveness%20of%20CARE%20over%20its%20GNN%20counterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.00936v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Class-Aware%2520Representation%2520Refinement%2520Framework%2520for%2520Graph%250A%2520%2520Classification%26entry.906535625%3DJiaxing%2520Xu%2520and%2520Jinjie%2520Ni%2520and%2520Yiping%2520Ke%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520widely%2520used%2520for%2520graph%2520representation%250Alearning.%2520Despite%2520its%2520prevalence%252C%2520GNN%2520suffers%2520from%2520two%2520drawbacks%2520in%2520the%2520graph%250Aclassification%2520task%252C%2520the%2520neglect%2520of%2520graph-level%2520relationships%252C%2520and%2520the%250Ageneralization%2520issue.%2520Each%2520graph%2520is%2520treated%2520separately%2520in%2520GNN%2520message%250Apassing/graph%2520pooling%252C%2520and%2520existing%2520methods%2520to%2520address%2520overfitting%2520operate%2520on%250Aeach%2520individual%2520graph.%2520This%2520makes%2520the%2520graph%2520representations%2520learnt%2520less%250Aeffective%2520in%2520the%2520downstream%2520classification.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250AClass-Aware%2520Representation%2520rEfinement%2520%2528CARE%2529%2520framework%2520for%2520the%2520task%2520of%2520graph%250Aclassification.%2520CARE%2520computes%2520simple%2520yet%2520powerful%2520class%2520representations%2520and%250Ainjects%2520them%2520to%2520steer%2520the%2520learning%2520of%2520graph%2520representations%2520towards%2520better%250Aclass%2520separability.%2520CARE%2520is%2520a%2520plug-and-play%2520framework%2520that%2520is%2520highly%2520flexible%250Aand%2520able%2520to%2520incorporate%2520arbitrary%2520GNN%2520backbones%2520without%2520significantly%250Aincreasing%2520the%2520computational%2520cost.%2520We%2520also%2520theoretically%2520prove%2520that%2520CARE%2520has%2520a%250Abetter%2520generalization%2520upper%2520bound%2520than%2520its%2520GNN%2520backbone%2520through%250AVapnik-Chervonenkis%2520%2528VC%2529%2520dimension%2520analysis.%2520Our%2520extensive%2520experiments%2520with%252011%250Awell-known%2520GNN%2520backbones%2520on%25209%2520benchmark%2520datasets%2520validate%2520the%2520superiority%2520and%250Aeffectiveness%2520of%2520CARE%2520over%2520its%2520GNN%2520counterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.00936v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Class-Aware%20Representation%20Refinement%20Framework%20for%20Graph%0A%20%20Classification&entry.906535625=Jiaxing%20Xu%20and%20Jinjie%20Ni%20and%20Yiping%20Ke&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20widely%20used%20for%20graph%20representation%0Alearning.%20Despite%20its%20prevalence%2C%20GNN%20suffers%20from%20two%20drawbacks%20in%20the%20graph%0Aclassification%20task%2C%20the%20neglect%20of%20graph-level%20relationships%2C%20and%20the%0Ageneralization%20issue.%20Each%20graph%20is%20treated%20separately%20in%20GNN%20message%0Apassing/graph%20pooling%2C%20and%20existing%20methods%20to%20address%20overfitting%20operate%20on%0Aeach%20individual%20graph.%20This%20makes%20the%20graph%20representations%20learnt%20less%0Aeffective%20in%20the%20downstream%20classification.%20In%20this%20paper%2C%20we%20propose%20a%0AClass-Aware%20Representation%20rEfinement%20%28CARE%29%20framework%20for%20the%20task%20of%20graph%0Aclassification.%20CARE%20computes%20simple%20yet%20powerful%20class%20representations%20and%0Ainjects%20them%20to%20steer%20the%20learning%20of%20graph%20representations%20towards%20better%0Aclass%20separability.%20CARE%20is%20a%20plug-and-play%20framework%20that%20is%20highly%20flexible%0Aand%20able%20to%20incorporate%20arbitrary%20GNN%20backbones%20without%20significantly%0Aincreasing%20the%20computational%20cost.%20We%20also%20theoretically%20prove%20that%20CARE%20has%20a%0Abetter%20generalization%20upper%20bound%20than%20its%20GNN%20backbone%20through%0AVapnik-Chervonenkis%20%28VC%29%20dimension%20analysis.%20Our%20extensive%20experiments%20with%2011%0Awell-known%20GNN%20backbones%20on%209%20benchmark%20datasets%20validate%20the%20superiority%20and%0Aeffectiveness%20of%20CARE%20over%20its%20GNN%20counterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.00936v2&entry.124074799=Read"},
{"title": "BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of\n  Large Language Models", "author": "Jiaqi Xue and Mengxin Zheng and Yebowen Hu and Fei Liu and Xun Chen and Qian Lou", "abstract": "  Large Language Models (LLMs) are constrained by outdated information and a\ntendency to generate incorrect data, commonly referred to as \"hallucinations.\"\nRetrieval-Augmented Generation (RAG) addresses these limitations by combining\nthe strengths of retrieval-based methods and generative models. This approach\ninvolves retrieving relevant information from a large, up-to-date dataset and\nusing it to enhance the generation process, leading to more accurate and\ncontextually appropriate responses. Despite its benefits, RAG introduces a new\nattack surface for LLMs, particularly because RAG databases are often sourced\nfrom public data, such as the web. In this paper, we propose \\TrojRAG{} to\nidentify the vulnerabilities and attacks on retrieval parts (RAG database) and\ntheir indirect attacks on generative parts (LLMs). Specifically, we identify\nthat poisoning several customized content passages could achieve a retrieval\nbackdoor, where the retrieval works well for clean queries but always returns\ncustomized poisoned adversarial queries. Triggers and poisoned passages can be\nhighly customized to implement various attacks. For example, a trigger could be\na semantic group like \"The Republican Party, Donald Trump, etc.\" Adversarial\npassages can be tailored to different contents, not only linked to the triggers\nbut also used to indirectly attack generative LLMs without modifying them.\nThese attacks can include denial-of-service attacks on RAG and semantic\nsteering attacks on LLM generations conditioned by the triggers. Our\nexperiments demonstrate that by just poisoning 10 adversarial passages can\ninduce 98.2\\% success rate to retrieve the adversarial passages. Then, these\npassages can increase the reject ratio of RAG-based GPT-4 from 0.01\\% to 74.6\\%\nor increase the rate of negative responses from 0.22\\% to 72\\% for targeted\nqueries.\n", "link": "http://arxiv.org/abs/2406.00083v2", "date": "2024-06-06", "relevancy": 2.2797, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4734}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4473}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BadRAG%3A%20Identifying%20Vulnerabilities%20in%20Retrieval%20Augmented%20Generation%20of%0A%20%20Large%20Language%20Models&body=Title%3A%20BadRAG%3A%20Identifying%20Vulnerabilities%20in%20Retrieval%20Augmented%20Generation%20of%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Jiaqi%20Xue%20and%20Mengxin%20Zheng%20and%20Yebowen%20Hu%20and%20Fei%20Liu%20and%20Xun%20Chen%20and%20Qian%20Lou%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20constrained%20by%20outdated%20information%20and%20a%0Atendency%20to%20generate%20incorrect%20data%2C%20commonly%20referred%20to%20as%20%22hallucinations.%22%0ARetrieval-Augmented%20Generation%20%28RAG%29%20addresses%20these%20limitations%20by%20combining%0Athe%20strengths%20of%20retrieval-based%20methods%20and%20generative%20models.%20This%20approach%0Ainvolves%20retrieving%20relevant%20information%20from%20a%20large%2C%20up-to-date%20dataset%20and%0Ausing%20it%20to%20enhance%20the%20generation%20process%2C%20leading%20to%20more%20accurate%20and%0Acontextually%20appropriate%20responses.%20Despite%20its%20benefits%2C%20RAG%20introduces%20a%20new%0Aattack%20surface%20for%20LLMs%2C%20particularly%20because%20RAG%20databases%20are%20often%20sourced%0Afrom%20public%20data%2C%20such%20as%20the%20web.%20In%20this%20paper%2C%20we%20propose%20%5CTrojRAG%7B%7D%20to%0Aidentify%20the%20vulnerabilities%20and%20attacks%20on%20retrieval%20parts%20%28RAG%20database%29%20and%0Atheir%20indirect%20attacks%20on%20generative%20parts%20%28LLMs%29.%20Specifically%2C%20we%20identify%0Athat%20poisoning%20several%20customized%20content%20passages%20could%20achieve%20a%20retrieval%0Abackdoor%2C%20where%20the%20retrieval%20works%20well%20for%20clean%20queries%20but%20always%20returns%0Acustomized%20poisoned%20adversarial%20queries.%20Triggers%20and%20poisoned%20passages%20can%20be%0Ahighly%20customized%20to%20implement%20various%20attacks.%20For%20example%2C%20a%20trigger%20could%20be%0Aa%20semantic%20group%20like%20%22The%20Republican%20Party%2C%20Donald%20Trump%2C%20etc.%22%20Adversarial%0Apassages%20can%20be%20tailored%20to%20different%20contents%2C%20not%20only%20linked%20to%20the%20triggers%0Abut%20also%20used%20to%20indirectly%20attack%20generative%20LLMs%20without%20modifying%20them.%0AThese%20attacks%20can%20include%20denial-of-service%20attacks%20on%20RAG%20and%20semantic%0Asteering%20attacks%20on%20LLM%20generations%20conditioned%20by%20the%20triggers.%20Our%0Aexperiments%20demonstrate%20that%20by%20just%20poisoning%2010%20adversarial%20passages%20can%0Ainduce%2098.2%5C%25%20success%20rate%20to%20retrieve%20the%20adversarial%20passages.%20Then%2C%20these%0Apassages%20can%20increase%20the%20reject%20ratio%20of%20RAG-based%20GPT-4%20from%200.01%5C%25%20to%2074.6%5C%25%0Aor%20increase%20the%20rate%20of%20negative%20responses%20from%200.22%5C%25%20to%2072%5C%25%20for%20targeted%0Aqueries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00083v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBadRAG%253A%2520Identifying%2520Vulnerabilities%2520in%2520Retrieval%2520Augmented%2520Generation%2520of%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DJiaqi%2520Xue%2520and%2520Mengxin%2520Zheng%2520and%2520Yebowen%2520Hu%2520and%2520Fei%2520Liu%2520and%2520Xun%2520Chen%2520and%2520Qian%2520Lou%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520constrained%2520by%2520outdated%2520information%2520and%2520a%250Atendency%2520to%2520generate%2520incorrect%2520data%252C%2520commonly%2520referred%2520to%2520as%2520%2522hallucinations.%2522%250ARetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520addresses%2520these%2520limitations%2520by%2520combining%250Athe%2520strengths%2520of%2520retrieval-based%2520methods%2520and%2520generative%2520models.%2520This%2520approach%250Ainvolves%2520retrieving%2520relevant%2520information%2520from%2520a%2520large%252C%2520up-to-date%2520dataset%2520and%250Ausing%2520it%2520to%2520enhance%2520the%2520generation%2520process%252C%2520leading%2520to%2520more%2520accurate%2520and%250Acontextually%2520appropriate%2520responses.%2520Despite%2520its%2520benefits%252C%2520RAG%2520introduces%2520a%2520new%250Aattack%2520surface%2520for%2520LLMs%252C%2520particularly%2520because%2520RAG%2520databases%2520are%2520often%2520sourced%250Afrom%2520public%2520data%252C%2520such%2520as%2520the%2520web.%2520In%2520this%2520paper%252C%2520we%2520propose%2520%255CTrojRAG%257B%257D%2520to%250Aidentify%2520the%2520vulnerabilities%2520and%2520attacks%2520on%2520retrieval%2520parts%2520%2528RAG%2520database%2529%2520and%250Atheir%2520indirect%2520attacks%2520on%2520generative%2520parts%2520%2528LLMs%2529.%2520Specifically%252C%2520we%2520identify%250Athat%2520poisoning%2520several%2520customized%2520content%2520passages%2520could%2520achieve%2520a%2520retrieval%250Abackdoor%252C%2520where%2520the%2520retrieval%2520works%2520well%2520for%2520clean%2520queries%2520but%2520always%2520returns%250Acustomized%2520poisoned%2520adversarial%2520queries.%2520Triggers%2520and%2520poisoned%2520passages%2520can%2520be%250Ahighly%2520customized%2520to%2520implement%2520various%2520attacks.%2520For%2520example%252C%2520a%2520trigger%2520could%2520be%250Aa%2520semantic%2520group%2520like%2520%2522The%2520Republican%2520Party%252C%2520Donald%2520Trump%252C%2520etc.%2522%2520Adversarial%250Apassages%2520can%2520be%2520tailored%2520to%2520different%2520contents%252C%2520not%2520only%2520linked%2520to%2520the%2520triggers%250Abut%2520also%2520used%2520to%2520indirectly%2520attack%2520generative%2520LLMs%2520without%2520modifying%2520them.%250AThese%2520attacks%2520can%2520include%2520denial-of-service%2520attacks%2520on%2520RAG%2520and%2520semantic%250Asteering%2520attacks%2520on%2520LLM%2520generations%2520conditioned%2520by%2520the%2520triggers.%2520Our%250Aexperiments%2520demonstrate%2520that%2520by%2520just%2520poisoning%252010%2520adversarial%2520passages%2520can%250Ainduce%252098.2%255C%2525%2520success%2520rate%2520to%2520retrieve%2520the%2520adversarial%2520passages.%2520Then%252C%2520these%250Apassages%2520can%2520increase%2520the%2520reject%2520ratio%2520of%2520RAG-based%2520GPT-4%2520from%25200.01%255C%2525%2520to%252074.6%255C%2525%250Aor%2520increase%2520the%2520rate%2520of%2520negative%2520responses%2520from%25200.22%255C%2525%2520to%252072%255C%2525%2520for%2520targeted%250Aqueries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00083v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BadRAG%3A%20Identifying%20Vulnerabilities%20in%20Retrieval%20Augmented%20Generation%20of%0A%20%20Large%20Language%20Models&entry.906535625=Jiaqi%20Xue%20and%20Mengxin%20Zheng%20and%20Yebowen%20Hu%20and%20Fei%20Liu%20and%20Xun%20Chen%20and%20Qian%20Lou&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20constrained%20by%20outdated%20information%20and%20a%0Atendency%20to%20generate%20incorrect%20data%2C%20commonly%20referred%20to%20as%20%22hallucinations.%22%0ARetrieval-Augmented%20Generation%20%28RAG%29%20addresses%20these%20limitations%20by%20combining%0Athe%20strengths%20of%20retrieval-based%20methods%20and%20generative%20models.%20This%20approach%0Ainvolves%20retrieving%20relevant%20information%20from%20a%20large%2C%20up-to-date%20dataset%20and%0Ausing%20it%20to%20enhance%20the%20generation%20process%2C%20leading%20to%20more%20accurate%20and%0Acontextually%20appropriate%20responses.%20Despite%20its%20benefits%2C%20RAG%20introduces%20a%20new%0Aattack%20surface%20for%20LLMs%2C%20particularly%20because%20RAG%20databases%20are%20often%20sourced%0Afrom%20public%20data%2C%20such%20as%20the%20web.%20In%20this%20paper%2C%20we%20propose%20%5CTrojRAG%7B%7D%20to%0Aidentify%20the%20vulnerabilities%20and%20attacks%20on%20retrieval%20parts%20%28RAG%20database%29%20and%0Atheir%20indirect%20attacks%20on%20generative%20parts%20%28LLMs%29.%20Specifically%2C%20we%20identify%0Athat%20poisoning%20several%20customized%20content%20passages%20could%20achieve%20a%20retrieval%0Abackdoor%2C%20where%20the%20retrieval%20works%20well%20for%20clean%20queries%20but%20always%20returns%0Acustomized%20poisoned%20adversarial%20queries.%20Triggers%20and%20poisoned%20passages%20can%20be%0Ahighly%20customized%20to%20implement%20various%20attacks.%20For%20example%2C%20a%20trigger%20could%20be%0Aa%20semantic%20group%20like%20%22The%20Republican%20Party%2C%20Donald%20Trump%2C%20etc.%22%20Adversarial%0Apassages%20can%20be%20tailored%20to%20different%20contents%2C%20not%20only%20linked%20to%20the%20triggers%0Abut%20also%20used%20to%20indirectly%20attack%20generative%20LLMs%20without%20modifying%20them.%0AThese%20attacks%20can%20include%20denial-of-service%20attacks%20on%20RAG%20and%20semantic%0Asteering%20attacks%20on%20LLM%20generations%20conditioned%20by%20the%20triggers.%20Our%0Aexperiments%20demonstrate%20that%20by%20just%20poisoning%2010%20adversarial%20passages%20can%0Ainduce%2098.2%5C%25%20success%20rate%20to%20retrieve%20the%20adversarial%20passages.%20Then%2C%20these%0Apassages%20can%20increase%20the%20reject%20ratio%20of%20RAG-based%20GPT-4%20from%200.01%5C%25%20to%2074.6%5C%25%0Aor%20increase%20the%20rate%20of%20negative%20responses%20from%200.22%5C%25%20to%2072%5C%25%20for%20targeted%0Aqueries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00083v2&entry.124074799=Read"},
{"title": "VidMuse: A Simple Video-to-Music Generation Framework with\n  Long-Short-Term Modeling", "author": "Zeyue Tian and Zhaoyang Liu and Ruibin Yuan and Jiahao Pan and Xiaoqiang Huang and Qifeng Liu and Xu Tan and Qifeng Chen and Wei Xue and Yike Guo", "abstract": "  In this work, we systematically study music generation conditioned solely on\nthe video. First, we present a large-scale dataset comprising 190K video-music\npairs, including various genres such as movie trailers, advertisements, and\ndocumentaries. Furthermore, we propose VidMuse, a simple framework for\ngenerating music aligned with video inputs. VidMuse stands out by producing\nhigh-fidelity music that is both acoustically and semantically aligned with the\nvideo. By incorporating local and global visual cues, VidMuse enables the\ncreation of musically coherent audio tracks that consistently match the video\ncontent through Long-Short-Term modeling. Through extensive experiments,\nVidMuse outperforms existing models in terms of audio quality, diversity, and\naudio-visual alignment. The code and datasets will be available at\nhttps://github.com/ZeyueT/VidMuse/.\n", "link": "http://arxiv.org/abs/2406.04321v1", "date": "2024-06-06", "relevancy": 2.2793, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6005}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5547}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VidMuse%3A%20A%20Simple%20Video-to-Music%20Generation%20Framework%20with%0A%20%20Long-Short-Term%20Modeling&body=Title%3A%20VidMuse%3A%20A%20Simple%20Video-to-Music%20Generation%20Framework%20with%0A%20%20Long-Short-Term%20Modeling%0AAuthor%3A%20Zeyue%20Tian%20and%20Zhaoyang%20Liu%20and%20Ruibin%20Yuan%20and%20Jiahao%20Pan%20and%20Xiaoqiang%20Huang%20and%20Qifeng%20Liu%20and%20Xu%20Tan%20and%20Qifeng%20Chen%20and%20Wei%20Xue%20and%20Yike%20Guo%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20systematically%20study%20music%20generation%20conditioned%20solely%20on%0Athe%20video.%20First%2C%20we%20present%20a%20large-scale%20dataset%20comprising%20190K%20video-music%0Apairs%2C%20including%20various%20genres%20such%20as%20movie%20trailers%2C%20advertisements%2C%20and%0Adocumentaries.%20Furthermore%2C%20we%20propose%20VidMuse%2C%20a%20simple%20framework%20for%0Agenerating%20music%20aligned%20with%20video%20inputs.%20VidMuse%20stands%20out%20by%20producing%0Ahigh-fidelity%20music%20that%20is%20both%20acoustically%20and%20semantically%20aligned%20with%20the%0Avideo.%20By%20incorporating%20local%20and%20global%20visual%20cues%2C%20VidMuse%20enables%20the%0Acreation%20of%20musically%20coherent%20audio%20tracks%20that%20consistently%20match%20the%20video%0Acontent%20through%20Long-Short-Term%20modeling.%20Through%20extensive%20experiments%2C%0AVidMuse%20outperforms%20existing%20models%20in%20terms%20of%20audio%20quality%2C%20diversity%2C%20and%0Aaudio-visual%20alignment.%20The%20code%20and%20datasets%20will%20be%20available%20at%0Ahttps%3A//github.com/ZeyueT/VidMuse/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04321v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVidMuse%253A%2520A%2520Simple%2520Video-to-Music%2520Generation%2520Framework%2520with%250A%2520%2520Long-Short-Term%2520Modeling%26entry.906535625%3DZeyue%2520Tian%2520and%2520Zhaoyang%2520Liu%2520and%2520Ruibin%2520Yuan%2520and%2520Jiahao%2520Pan%2520and%2520Xiaoqiang%2520Huang%2520and%2520Qifeng%2520Liu%2520and%2520Xu%2520Tan%2520and%2520Qifeng%2520Chen%2520and%2520Wei%2520Xue%2520and%2520Yike%2520Guo%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520systematically%2520study%2520music%2520generation%2520conditioned%2520solely%2520on%250Athe%2520video.%2520First%252C%2520we%2520present%2520a%2520large-scale%2520dataset%2520comprising%2520190K%2520video-music%250Apairs%252C%2520including%2520various%2520genres%2520such%2520as%2520movie%2520trailers%252C%2520advertisements%252C%2520and%250Adocumentaries.%2520Furthermore%252C%2520we%2520propose%2520VidMuse%252C%2520a%2520simple%2520framework%2520for%250Agenerating%2520music%2520aligned%2520with%2520video%2520inputs.%2520VidMuse%2520stands%2520out%2520by%2520producing%250Ahigh-fidelity%2520music%2520that%2520is%2520both%2520acoustically%2520and%2520semantically%2520aligned%2520with%2520the%250Avideo.%2520By%2520incorporating%2520local%2520and%2520global%2520visual%2520cues%252C%2520VidMuse%2520enables%2520the%250Acreation%2520of%2520musically%2520coherent%2520audio%2520tracks%2520that%2520consistently%2520match%2520the%2520video%250Acontent%2520through%2520Long-Short-Term%2520modeling.%2520Through%2520extensive%2520experiments%252C%250AVidMuse%2520outperforms%2520existing%2520models%2520in%2520terms%2520of%2520audio%2520quality%252C%2520diversity%252C%2520and%250Aaudio-visual%2520alignment.%2520The%2520code%2520and%2520datasets%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/ZeyueT/VidMuse/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04321v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VidMuse%3A%20A%20Simple%20Video-to-Music%20Generation%20Framework%20with%0A%20%20Long-Short-Term%20Modeling&entry.906535625=Zeyue%20Tian%20and%20Zhaoyang%20Liu%20and%20Ruibin%20Yuan%20and%20Jiahao%20Pan%20and%20Xiaoqiang%20Huang%20and%20Qifeng%20Liu%20and%20Xu%20Tan%20and%20Qifeng%20Chen%20and%20Wei%20Xue%20and%20Yike%20Guo&entry.1292438233=%20%20In%20this%20work%2C%20we%20systematically%20study%20music%20generation%20conditioned%20solely%20on%0Athe%20video.%20First%2C%20we%20present%20a%20large-scale%20dataset%20comprising%20190K%20video-music%0Apairs%2C%20including%20various%20genres%20such%20as%20movie%20trailers%2C%20advertisements%2C%20and%0Adocumentaries.%20Furthermore%2C%20we%20propose%20VidMuse%2C%20a%20simple%20framework%20for%0Agenerating%20music%20aligned%20with%20video%20inputs.%20VidMuse%20stands%20out%20by%20producing%0Ahigh-fidelity%20music%20that%20is%20both%20acoustically%20and%20semantically%20aligned%20with%20the%0Avideo.%20By%20incorporating%20local%20and%20global%20visual%20cues%2C%20VidMuse%20enables%20the%0Acreation%20of%20musically%20coherent%20audio%20tracks%20that%20consistently%20match%20the%20video%0Acontent%20through%20Long-Short-Term%20modeling.%20Through%20extensive%20experiments%2C%0AVidMuse%20outperforms%20existing%20models%20in%20terms%20of%20audio%20quality%2C%20diversity%2C%20and%0Aaudio-visual%20alignment.%20The%20code%20and%20datasets%20will%20be%20available%20at%0Ahttps%3A//github.com/ZeyueT/VidMuse/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04321v1&entry.124074799=Read"},
{"title": "Improving Physics-Augmented Continuum Neural Radiance Field-Based\n  Geometry-Agnostic System Identification with Lagrangian Particle Optimization", "author": "Takuhiro Kaneko", "abstract": "  Geometry-agnostic system identification is a technique for identifying the\ngeometry and physical properties of an object from video sequences without any\ngeometric assumptions. Recently, physics-augmented continuum neural radiance\nfields (PAC-NeRF) has demonstrated promising results for this technique by\nutilizing a hybrid Eulerian-Lagrangian representation, in which the geometry is\nrepresented by the Eulerian grid representations of NeRF, the physics is\ndescribed by a material point method (MPM), and they are connected via\nLagrangian particles. However, a notable limitation of PAC-NeRF is that its\nperformance is sensitive to the learning of the geometry from the first frames\nowing to its two-step optimization. First, the grid representations are\noptimized with the first frames of video sequences, and then the physical\nproperties are optimized through video sequences utilizing the fixed\nfirst-frame grid representations. This limitation can be critical when learning\nof the geometric structure is difficult, for example, in a few-shot (sparse\nview) setting. To overcome this limitation, we propose Lagrangian particle\noptimization (LPO), in which the positions and features of particles are\noptimized through video sequences in Lagrangian space. This method allows for\nthe optimization of the geometric structure across the entire video sequence\nwithin the physical constraints imposed by the MPM. The experimental results\ndemonstrate that the LPO is useful for geometric correction and physical\nidentification in sparse-view settings.\n", "link": "http://arxiv.org/abs/2406.04155v1", "date": "2024-06-06", "relevancy": 2.2712, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5782}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5679}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Physics-Augmented%20Continuum%20Neural%20Radiance%20Field-Based%0A%20%20Geometry-Agnostic%20System%20Identification%20with%20Lagrangian%20Particle%20Optimization&body=Title%3A%20Improving%20Physics-Augmented%20Continuum%20Neural%20Radiance%20Field-Based%0A%20%20Geometry-Agnostic%20System%20Identification%20with%20Lagrangian%20Particle%20Optimization%0AAuthor%3A%20Takuhiro%20Kaneko%0AAbstract%3A%20%20%20Geometry-agnostic%20system%20identification%20is%20a%20technique%20for%20identifying%20the%0Ageometry%20and%20physical%20properties%20of%20an%20object%20from%20video%20sequences%20without%20any%0Ageometric%20assumptions.%20Recently%2C%20physics-augmented%20continuum%20neural%20radiance%0Afields%20%28PAC-NeRF%29%20has%20demonstrated%20promising%20results%20for%20this%20technique%20by%0Autilizing%20a%20hybrid%20Eulerian-Lagrangian%20representation%2C%20in%20which%20the%20geometry%20is%0Arepresented%20by%20the%20Eulerian%20grid%20representations%20of%20NeRF%2C%20the%20physics%20is%0Adescribed%20by%20a%20material%20point%20method%20%28MPM%29%2C%20and%20they%20are%20connected%20via%0ALagrangian%20particles.%20However%2C%20a%20notable%20limitation%20of%20PAC-NeRF%20is%20that%20its%0Aperformance%20is%20sensitive%20to%20the%20learning%20of%20the%20geometry%20from%20the%20first%20frames%0Aowing%20to%20its%20two-step%20optimization.%20First%2C%20the%20grid%20representations%20are%0Aoptimized%20with%20the%20first%20frames%20of%20video%20sequences%2C%20and%20then%20the%20physical%0Aproperties%20are%20optimized%20through%20video%20sequences%20utilizing%20the%20fixed%0Afirst-frame%20grid%20representations.%20This%20limitation%20can%20be%20critical%20when%20learning%0Aof%20the%20geometric%20structure%20is%20difficult%2C%20for%20example%2C%20in%20a%20few-shot%20%28sparse%0Aview%29%20setting.%20To%20overcome%20this%20limitation%2C%20we%20propose%20Lagrangian%20particle%0Aoptimization%20%28LPO%29%2C%20in%20which%20the%20positions%20and%20features%20of%20particles%20are%0Aoptimized%20through%20video%20sequences%20in%20Lagrangian%20space.%20This%20method%20allows%20for%0Athe%20optimization%20of%20the%20geometric%20structure%20across%20the%20entire%20video%20sequence%0Awithin%20the%20physical%20constraints%20imposed%20by%20the%20MPM.%20The%20experimental%20results%0Ademonstrate%20that%20the%20LPO%20is%20useful%20for%20geometric%20correction%20and%20physical%0Aidentification%20in%20sparse-view%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04155v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Physics-Augmented%2520Continuum%2520Neural%2520Radiance%2520Field-Based%250A%2520%2520Geometry-Agnostic%2520System%2520Identification%2520with%2520Lagrangian%2520Particle%2520Optimization%26entry.906535625%3DTakuhiro%2520Kaneko%26entry.1292438233%3D%2520%2520Geometry-agnostic%2520system%2520identification%2520is%2520a%2520technique%2520for%2520identifying%2520the%250Ageometry%2520and%2520physical%2520properties%2520of%2520an%2520object%2520from%2520video%2520sequences%2520without%2520any%250Ageometric%2520assumptions.%2520Recently%252C%2520physics-augmented%2520continuum%2520neural%2520radiance%250Afields%2520%2528PAC-NeRF%2529%2520has%2520demonstrated%2520promising%2520results%2520for%2520this%2520technique%2520by%250Autilizing%2520a%2520hybrid%2520Eulerian-Lagrangian%2520representation%252C%2520in%2520which%2520the%2520geometry%2520is%250Arepresented%2520by%2520the%2520Eulerian%2520grid%2520representations%2520of%2520NeRF%252C%2520the%2520physics%2520is%250Adescribed%2520by%2520a%2520material%2520point%2520method%2520%2528MPM%2529%252C%2520and%2520they%2520are%2520connected%2520via%250ALagrangian%2520particles.%2520However%252C%2520a%2520notable%2520limitation%2520of%2520PAC-NeRF%2520is%2520that%2520its%250Aperformance%2520is%2520sensitive%2520to%2520the%2520learning%2520of%2520the%2520geometry%2520from%2520the%2520first%2520frames%250Aowing%2520to%2520its%2520two-step%2520optimization.%2520First%252C%2520the%2520grid%2520representations%2520are%250Aoptimized%2520with%2520the%2520first%2520frames%2520of%2520video%2520sequences%252C%2520and%2520then%2520the%2520physical%250Aproperties%2520are%2520optimized%2520through%2520video%2520sequences%2520utilizing%2520the%2520fixed%250Afirst-frame%2520grid%2520representations.%2520This%2520limitation%2520can%2520be%2520critical%2520when%2520learning%250Aof%2520the%2520geometric%2520structure%2520is%2520difficult%252C%2520for%2520example%252C%2520in%2520a%2520few-shot%2520%2528sparse%250Aview%2529%2520setting.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520Lagrangian%2520particle%250Aoptimization%2520%2528LPO%2529%252C%2520in%2520which%2520the%2520positions%2520and%2520features%2520of%2520particles%2520are%250Aoptimized%2520through%2520video%2520sequences%2520in%2520Lagrangian%2520space.%2520This%2520method%2520allows%2520for%250Athe%2520optimization%2520of%2520the%2520geometric%2520structure%2520across%2520the%2520entire%2520video%2520sequence%250Awithin%2520the%2520physical%2520constraints%2520imposed%2520by%2520the%2520MPM.%2520The%2520experimental%2520results%250Ademonstrate%2520that%2520the%2520LPO%2520is%2520useful%2520for%2520geometric%2520correction%2520and%2520physical%250Aidentification%2520in%2520sparse-view%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04155v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Physics-Augmented%20Continuum%20Neural%20Radiance%20Field-Based%0A%20%20Geometry-Agnostic%20System%20Identification%20with%20Lagrangian%20Particle%20Optimization&entry.906535625=Takuhiro%20Kaneko&entry.1292438233=%20%20Geometry-agnostic%20system%20identification%20is%20a%20technique%20for%20identifying%20the%0Ageometry%20and%20physical%20properties%20of%20an%20object%20from%20video%20sequences%20without%20any%0Ageometric%20assumptions.%20Recently%2C%20physics-augmented%20continuum%20neural%20radiance%0Afields%20%28PAC-NeRF%29%20has%20demonstrated%20promising%20results%20for%20this%20technique%20by%0Autilizing%20a%20hybrid%20Eulerian-Lagrangian%20representation%2C%20in%20which%20the%20geometry%20is%0Arepresented%20by%20the%20Eulerian%20grid%20representations%20of%20NeRF%2C%20the%20physics%20is%0Adescribed%20by%20a%20material%20point%20method%20%28MPM%29%2C%20and%20they%20are%20connected%20via%0ALagrangian%20particles.%20However%2C%20a%20notable%20limitation%20of%20PAC-NeRF%20is%20that%20its%0Aperformance%20is%20sensitive%20to%20the%20learning%20of%20the%20geometry%20from%20the%20first%20frames%0Aowing%20to%20its%20two-step%20optimization.%20First%2C%20the%20grid%20representations%20are%0Aoptimized%20with%20the%20first%20frames%20of%20video%20sequences%2C%20and%20then%20the%20physical%0Aproperties%20are%20optimized%20through%20video%20sequences%20utilizing%20the%20fixed%0Afirst-frame%20grid%20representations.%20This%20limitation%20can%20be%20critical%20when%20learning%0Aof%20the%20geometric%20structure%20is%20difficult%2C%20for%20example%2C%20in%20a%20few-shot%20%28sparse%0Aview%29%20setting.%20To%20overcome%20this%20limitation%2C%20we%20propose%20Lagrangian%20particle%0Aoptimization%20%28LPO%29%2C%20in%20which%20the%20positions%20and%20features%20of%20particles%20are%0Aoptimized%20through%20video%20sequences%20in%20Lagrangian%20space.%20This%20method%20allows%20for%0Athe%20optimization%20of%20the%20geometric%20structure%20across%20the%20entire%20video%20sequence%0Awithin%20the%20physical%20constraints%20imposed%20by%20the%20MPM.%20The%20experimental%20results%0Ademonstrate%20that%20the%20LPO%20is%20useful%20for%20geometric%20correction%20and%20physical%0Aidentification%20in%20sparse-view%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04155v1&entry.124074799=Read"},
{"title": "AgentGym: Evolving Large Language Model-based Agents across Diverse\n  Environments", "author": "Zhiheng Xi and Yiwen Ding and Wenxiang Chen and Boyang Hong and Honglin Guo and Junzhe Wang and Dingwen Yang and Chenyang Liao and Xin Guo and Wei He and Songyang Gao and Lu Chen and Rui Zheng and Yicheng Zou and Tao Gui and Qi Zhang and Xipeng Qiu and Xuanjing Huang and Zuxuan Wu and Yu-Gang Jiang", "abstract": "  Building generalist agents that can handle diverse tasks and evolve\nthemselves across different environments is a long-term goal in the AI\ncommunity. Large language models (LLMs) are considered a promising foundation\nto build such agents due to their generalized capabilities. Current approaches\neither have LLM-based agents imitate expert-provided trajectories step-by-step,\nrequiring human supervision, which is hard to scale and limits environmental\nexploration; or they let agents explore and learn in isolated environments,\nresulting in specialist agents with limited generalization. In this paper, we\ntake the first step towards building generally-capable LLM-based agents with\nself-evolution ability. We identify a trinity of ingredients: 1) diverse\nenvironments for agent exploration and learning, 2) a trajectory set to equip\nagents with basic capabilities and prior knowledge, and 3) an effective and\nscalable evolution method. We propose AgentGym, a new framework featuring a\nvariety of environments and tasks for broad, real-time, uni-format, and\nconcurrent agent exploration. AgentGym also includes a database with expanded\ninstructions, a benchmark suite, and high-quality trajectories across\nenvironments. Next, we propose a novel method, AgentEvol, to investigate the\npotential of agent self-evolution beyond previously seen data across tasks and\nenvironments. Experimental results show that the evolved agents can achieve\nresults comparable to SOTA models. We release the AgentGym suite, including the\nplatform, dataset, benchmark, checkpoints, and algorithm implementations. The\nAgentGym suite is available on https://github.com/WooooDyy/AgentGym.\n", "link": "http://arxiv.org/abs/2406.04151v1", "date": "2024-06-06", "relevancy": 2.2658, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5905}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5631}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentGym%3A%20Evolving%20Large%20Language%20Model-based%20Agents%20across%20Diverse%0A%20%20Environments&body=Title%3A%20AgentGym%3A%20Evolving%20Large%20Language%20Model-based%20Agents%20across%20Diverse%0A%20%20Environments%0AAuthor%3A%20Zhiheng%20Xi%20and%20Yiwen%20Ding%20and%20Wenxiang%20Chen%20and%20Boyang%20Hong%20and%20Honglin%20Guo%20and%20Junzhe%20Wang%20and%20Dingwen%20Yang%20and%20Chenyang%20Liao%20and%20Xin%20Guo%20and%20Wei%20He%20and%20Songyang%20Gao%20and%20Lu%20Chen%20and%20Rui%20Zheng%20and%20Yicheng%20Zou%20and%20Tao%20Gui%20and%20Qi%20Zhang%20and%20Xipeng%20Qiu%20and%20Xuanjing%20Huang%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Building%20generalist%20agents%20that%20can%20handle%20diverse%20tasks%20and%20evolve%0Athemselves%20across%20different%20environments%20is%20a%20long-term%20goal%20in%20the%20AI%0Acommunity.%20Large%20language%20models%20%28LLMs%29%20are%20considered%20a%20promising%20foundation%0Ato%20build%20such%20agents%20due%20to%20their%20generalized%20capabilities.%20Current%20approaches%0Aeither%20have%20LLM-based%20agents%20imitate%20expert-provided%20trajectories%20step-by-step%2C%0Arequiring%20human%20supervision%2C%20which%20is%20hard%20to%20scale%20and%20limits%20environmental%0Aexploration%3B%20or%20they%20let%20agents%20explore%20and%20learn%20in%20isolated%20environments%2C%0Aresulting%20in%20specialist%20agents%20with%20limited%20generalization.%20In%20this%20paper%2C%20we%0Atake%20the%20first%20step%20towards%20building%20generally-capable%20LLM-based%20agents%20with%0Aself-evolution%20ability.%20We%20identify%20a%20trinity%20of%20ingredients%3A%201%29%20diverse%0Aenvironments%20for%20agent%20exploration%20and%20learning%2C%202%29%20a%20trajectory%20set%20to%20equip%0Aagents%20with%20basic%20capabilities%20and%20prior%20knowledge%2C%20and%203%29%20an%20effective%20and%0Ascalable%20evolution%20method.%20We%20propose%20AgentGym%2C%20a%20new%20framework%20featuring%20a%0Avariety%20of%20environments%20and%20tasks%20for%20broad%2C%20real-time%2C%20uni-format%2C%20and%0Aconcurrent%20agent%20exploration.%20AgentGym%20also%20includes%20a%20database%20with%20expanded%0Ainstructions%2C%20a%20benchmark%20suite%2C%20and%20high-quality%20trajectories%20across%0Aenvironments.%20Next%2C%20we%20propose%20a%20novel%20method%2C%20AgentEvol%2C%20to%20investigate%20the%0Apotential%20of%20agent%20self-evolution%20beyond%20previously%20seen%20data%20across%20tasks%20and%0Aenvironments.%20Experimental%20results%20show%20that%20the%20evolved%20agents%20can%20achieve%0Aresults%20comparable%20to%20SOTA%20models.%20We%20release%20the%20AgentGym%20suite%2C%20including%20the%0Aplatform%2C%20dataset%2C%20benchmark%2C%20checkpoints%2C%20and%20algorithm%20implementations.%20The%0AAgentGym%20suite%20is%20available%20on%20https%3A//github.com/WooooDyy/AgentGym.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentGym%253A%2520Evolving%2520Large%2520Language%2520Model-based%2520Agents%2520across%2520Diverse%250A%2520%2520Environments%26entry.906535625%3DZhiheng%2520Xi%2520and%2520Yiwen%2520Ding%2520and%2520Wenxiang%2520Chen%2520and%2520Boyang%2520Hong%2520and%2520Honglin%2520Guo%2520and%2520Junzhe%2520Wang%2520and%2520Dingwen%2520Yang%2520and%2520Chenyang%2520Liao%2520and%2520Xin%2520Guo%2520and%2520Wei%2520He%2520and%2520Songyang%2520Gao%2520and%2520Lu%2520Chen%2520and%2520Rui%2520Zheng%2520and%2520Yicheng%2520Zou%2520and%2520Tao%2520Gui%2520and%2520Qi%2520Zhang%2520and%2520Xipeng%2520Qiu%2520and%2520Xuanjing%2520Huang%2520and%2520Zuxuan%2520Wu%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Building%2520generalist%2520agents%2520that%2520can%2520handle%2520diverse%2520tasks%2520and%2520evolve%250Athemselves%2520across%2520different%2520environments%2520is%2520a%2520long-term%2520goal%2520in%2520the%2520AI%250Acommunity.%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520considered%2520a%2520promising%2520foundation%250Ato%2520build%2520such%2520agents%2520due%2520to%2520their%2520generalized%2520capabilities.%2520Current%2520approaches%250Aeither%2520have%2520LLM-based%2520agents%2520imitate%2520expert-provided%2520trajectories%2520step-by-step%252C%250Arequiring%2520human%2520supervision%252C%2520which%2520is%2520hard%2520to%2520scale%2520and%2520limits%2520environmental%250Aexploration%253B%2520or%2520they%2520let%2520agents%2520explore%2520and%2520learn%2520in%2520isolated%2520environments%252C%250Aresulting%2520in%2520specialist%2520agents%2520with%2520limited%2520generalization.%2520In%2520this%2520paper%252C%2520we%250Atake%2520the%2520first%2520step%2520towards%2520building%2520generally-capable%2520LLM-based%2520agents%2520with%250Aself-evolution%2520ability.%2520We%2520identify%2520a%2520trinity%2520of%2520ingredients%253A%25201%2529%2520diverse%250Aenvironments%2520for%2520agent%2520exploration%2520and%2520learning%252C%25202%2529%2520a%2520trajectory%2520set%2520to%2520equip%250Aagents%2520with%2520basic%2520capabilities%2520and%2520prior%2520knowledge%252C%2520and%25203%2529%2520an%2520effective%2520and%250Ascalable%2520evolution%2520method.%2520We%2520propose%2520AgentGym%252C%2520a%2520new%2520framework%2520featuring%2520a%250Avariety%2520of%2520environments%2520and%2520tasks%2520for%2520broad%252C%2520real-time%252C%2520uni-format%252C%2520and%250Aconcurrent%2520agent%2520exploration.%2520AgentGym%2520also%2520includes%2520a%2520database%2520with%2520expanded%250Ainstructions%252C%2520a%2520benchmark%2520suite%252C%2520and%2520high-quality%2520trajectories%2520across%250Aenvironments.%2520Next%252C%2520we%2520propose%2520a%2520novel%2520method%252C%2520AgentEvol%252C%2520to%2520investigate%2520the%250Apotential%2520of%2520agent%2520self-evolution%2520beyond%2520previously%2520seen%2520data%2520across%2520tasks%2520and%250Aenvironments.%2520Experimental%2520results%2520show%2520that%2520the%2520evolved%2520agents%2520can%2520achieve%250Aresults%2520comparable%2520to%2520SOTA%2520models.%2520We%2520release%2520the%2520AgentGym%2520suite%252C%2520including%2520the%250Aplatform%252C%2520dataset%252C%2520benchmark%252C%2520checkpoints%252C%2520and%2520algorithm%2520implementations.%2520The%250AAgentGym%2520suite%2520is%2520available%2520on%2520https%253A//github.com/WooooDyy/AgentGym.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentGym%3A%20Evolving%20Large%20Language%20Model-based%20Agents%20across%20Diverse%0A%20%20Environments&entry.906535625=Zhiheng%20Xi%20and%20Yiwen%20Ding%20and%20Wenxiang%20Chen%20and%20Boyang%20Hong%20and%20Honglin%20Guo%20and%20Junzhe%20Wang%20and%20Dingwen%20Yang%20and%20Chenyang%20Liao%20and%20Xin%20Guo%20and%20Wei%20He%20and%20Songyang%20Gao%20and%20Lu%20Chen%20and%20Rui%20Zheng%20and%20Yicheng%20Zou%20and%20Tao%20Gui%20and%20Qi%20Zhang%20and%20Xipeng%20Qiu%20and%20Xuanjing%20Huang%20and%20Zuxuan%20Wu%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Building%20generalist%20agents%20that%20can%20handle%20diverse%20tasks%20and%20evolve%0Athemselves%20across%20different%20environments%20is%20a%20long-term%20goal%20in%20the%20AI%0Acommunity.%20Large%20language%20models%20%28LLMs%29%20are%20considered%20a%20promising%20foundation%0Ato%20build%20such%20agents%20due%20to%20their%20generalized%20capabilities.%20Current%20approaches%0Aeither%20have%20LLM-based%20agents%20imitate%20expert-provided%20trajectories%20step-by-step%2C%0Arequiring%20human%20supervision%2C%20which%20is%20hard%20to%20scale%20and%20limits%20environmental%0Aexploration%3B%20or%20they%20let%20agents%20explore%20and%20learn%20in%20isolated%20environments%2C%0Aresulting%20in%20specialist%20agents%20with%20limited%20generalization.%20In%20this%20paper%2C%20we%0Atake%20the%20first%20step%20towards%20building%20generally-capable%20LLM-based%20agents%20with%0Aself-evolution%20ability.%20We%20identify%20a%20trinity%20of%20ingredients%3A%201%29%20diverse%0Aenvironments%20for%20agent%20exploration%20and%20learning%2C%202%29%20a%20trajectory%20set%20to%20equip%0Aagents%20with%20basic%20capabilities%20and%20prior%20knowledge%2C%20and%203%29%20an%20effective%20and%0Ascalable%20evolution%20method.%20We%20propose%20AgentGym%2C%20a%20new%20framework%20featuring%20a%0Avariety%20of%20environments%20and%20tasks%20for%20broad%2C%20real-time%2C%20uni-format%2C%20and%0Aconcurrent%20agent%20exploration.%20AgentGym%20also%20includes%20a%20database%20with%20expanded%0Ainstructions%2C%20a%20benchmark%20suite%2C%20and%20high-quality%20trajectories%20across%0Aenvironments.%20Next%2C%20we%20propose%20a%20novel%20method%2C%20AgentEvol%2C%20to%20investigate%20the%0Apotential%20of%20agent%20self-evolution%20beyond%20previously%20seen%20data%20across%20tasks%20and%0Aenvironments.%20Experimental%20results%20show%20that%20the%20evolved%20agents%20can%20achieve%0Aresults%20comparable%20to%20SOTA%20models.%20We%20release%20the%20AgentGym%20suite%2C%20including%20the%0Aplatform%2C%20dataset%2C%20benchmark%2C%20checkpoints%2C%20and%20algorithm%20implementations.%20The%0AAgentGym%20suite%20is%20available%20on%20https%3A//github.com/WooooDyy/AgentGym.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04151v1&entry.124074799=Read"},
{"title": "Coherent Zero-Shot Visual Instruction Generation", "author": "Quynh Phung and Songwei Ge and Jia-Bin Huang", "abstract": "  Despite the advances in text-to-image synthesis, particularly with diffusion\nmodels, generating visual instructions that require consistent representation\nand smooth state transitions of objects across sequential steps remains a\nformidable challenge. This paper introduces a simple, training-free framework\nto tackle the issues, capitalizing on the advancements in diffusion models and\nlarge language models (LLMs). Our approach systematically integrates text\ncomprehension and image generation to ensure visual instructions are visually\nappealing and maintain consistency and accuracy throughout the instruction\nsequence. We validate the effectiveness by testing multi-step instructions and\ncomparing the text alignment and consistency with several baselines. Our\nexperiments show that our approach can visualize coherent and visually pleasing\ninstructions\n", "link": "http://arxiv.org/abs/2406.04337v1", "date": "2024-06-06", "relevancy": 2.2613, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5748}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5626}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coherent%20Zero-Shot%20Visual%20Instruction%20Generation&body=Title%3A%20Coherent%20Zero-Shot%20Visual%20Instruction%20Generation%0AAuthor%3A%20Quynh%20Phung%20and%20Songwei%20Ge%20and%20Jia-Bin%20Huang%0AAbstract%3A%20%20%20Despite%20the%20advances%20in%20text-to-image%20synthesis%2C%20particularly%20with%20diffusion%0Amodels%2C%20generating%20visual%20instructions%20that%20require%20consistent%20representation%0Aand%20smooth%20state%20transitions%20of%20objects%20across%20sequential%20steps%20remains%20a%0Aformidable%20challenge.%20This%20paper%20introduces%20a%20simple%2C%20training-free%20framework%0Ato%20tackle%20the%20issues%2C%20capitalizing%20on%20the%20advancements%20in%20diffusion%20models%20and%0Alarge%20language%20models%20%28LLMs%29.%20Our%20approach%20systematically%20integrates%20text%0Acomprehension%20and%20image%20generation%20to%20ensure%20visual%20instructions%20are%20visually%0Aappealing%20and%20maintain%20consistency%20and%20accuracy%20throughout%20the%20instruction%0Asequence.%20We%20validate%20the%20effectiveness%20by%20testing%20multi-step%20instructions%20and%0Acomparing%20the%20text%20alignment%20and%20consistency%20with%20several%20baselines.%20Our%0Aexperiments%20show%20that%20our%20approach%20can%20visualize%20coherent%20and%20visually%20pleasing%0Ainstructions%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04337v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoherent%2520Zero-Shot%2520Visual%2520Instruction%2520Generation%26entry.906535625%3DQuynh%2520Phung%2520and%2520Songwei%2520Ge%2520and%2520Jia-Bin%2520Huang%26entry.1292438233%3D%2520%2520Despite%2520the%2520advances%2520in%2520text-to-image%2520synthesis%252C%2520particularly%2520with%2520diffusion%250Amodels%252C%2520generating%2520visual%2520instructions%2520that%2520require%2520consistent%2520representation%250Aand%2520smooth%2520state%2520transitions%2520of%2520objects%2520across%2520sequential%2520steps%2520remains%2520a%250Aformidable%2520challenge.%2520This%2520paper%2520introduces%2520a%2520simple%252C%2520training-free%2520framework%250Ato%2520tackle%2520the%2520issues%252C%2520capitalizing%2520on%2520the%2520advancements%2520in%2520diffusion%2520models%2520and%250Alarge%2520language%2520models%2520%2528LLMs%2529.%2520Our%2520approach%2520systematically%2520integrates%2520text%250Acomprehension%2520and%2520image%2520generation%2520to%2520ensure%2520visual%2520instructions%2520are%2520visually%250Aappealing%2520and%2520maintain%2520consistency%2520and%2520accuracy%2520throughout%2520the%2520instruction%250Asequence.%2520We%2520validate%2520the%2520effectiveness%2520by%2520testing%2520multi-step%2520instructions%2520and%250Acomparing%2520the%2520text%2520alignment%2520and%2520consistency%2520with%2520several%2520baselines.%2520Our%250Aexperiments%2520show%2520that%2520our%2520approach%2520can%2520visualize%2520coherent%2520and%2520visually%2520pleasing%250Ainstructions%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04337v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coherent%20Zero-Shot%20Visual%20Instruction%20Generation&entry.906535625=Quynh%20Phung%20and%20Songwei%20Ge%20and%20Jia-Bin%20Huang&entry.1292438233=%20%20Despite%20the%20advances%20in%20text-to-image%20synthesis%2C%20particularly%20with%20diffusion%0Amodels%2C%20generating%20visual%20instructions%20that%20require%20consistent%20representation%0Aand%20smooth%20state%20transitions%20of%20objects%20across%20sequential%20steps%20remains%20a%0Aformidable%20challenge.%20This%20paper%20introduces%20a%20simple%2C%20training-free%20framework%0Ato%20tackle%20the%20issues%2C%20capitalizing%20on%20the%20advancements%20in%20diffusion%20models%20and%0Alarge%20language%20models%20%28LLMs%29.%20Our%20approach%20systematically%20integrates%20text%0Acomprehension%20and%20image%20generation%20to%20ensure%20visual%20instructions%20are%20visually%0Aappealing%20and%20maintain%20consistency%20and%20accuracy%20throughout%20the%20instruction%0Asequence.%20We%20validate%20the%20effectiveness%20by%20testing%20multi-step%20instructions%20and%0Acomparing%20the%20text%20alignment%20and%20consistency%20with%20several%20baselines.%20Our%0Aexperiments%20show%20that%20our%20approach%20can%20visualize%20coherent%20and%20visually%20pleasing%0Ainstructions%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04337v1&entry.124074799=Read"},
{"title": "Motion-aware Dynamic Graph Neural Network for Video Compressive Sensing", "author": "Ruiying Lu and Ziheng Cheng and Bo Chen and Xin Yuan", "abstract": "  Video snapshot compressive imaging (SCI) utilizes a 2D detector to capture\nsequential video frames and compress them into a single measurement. Various\nreconstruction methods have been developed to recover the high-speed video\nframes from the snapshot measurement. However, most existing reconstruction\nmethods are incapable of efficiently capturing long-range spatial and temporal\ndependencies, which are critical for video processing. In this paper, we\npropose a flexible and robust approach based on the graph neural network (GNN)\nto efficiently model non-local interactions between pixels in space and time\nregardless of the distance. Specifically, we develop a motion-aware dynamic GNN\nfor better video representation, i.e., represent each node as the aggregation\nof relative neighbors under the guidance of frame-by-frame motions, which\nconsists of motion-aware dynamic sampling, cross-scale node sampling, global\nknowledge integration, and graph aggregation. Extensive results on both\nsimulation and real data demonstrate both the effectiveness and efficiency of\nthe proposed approach, and the visualization illustrates the intrinsic dynamic\nsampling operations of our proposed model for boosting the video SCI\nreconstruction results. The code and model will be released.\n", "link": "http://arxiv.org/abs/2203.00387v2", "date": "2024-06-06", "relevancy": 2.2486, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6041}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5377}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion-aware%20Dynamic%20Graph%20Neural%20Network%20for%20Video%20Compressive%20Sensing&body=Title%3A%20Motion-aware%20Dynamic%20Graph%20Neural%20Network%20for%20Video%20Compressive%20Sensing%0AAuthor%3A%20Ruiying%20Lu%20and%20Ziheng%20Cheng%20and%20Bo%20Chen%20and%20Xin%20Yuan%0AAbstract%3A%20%20%20Video%20snapshot%20compressive%20imaging%20%28SCI%29%20utilizes%20a%202D%20detector%20to%20capture%0Asequential%20video%20frames%20and%20compress%20them%20into%20a%20single%20measurement.%20Various%0Areconstruction%20methods%20have%20been%20developed%20to%20recover%20the%20high-speed%20video%0Aframes%20from%20the%20snapshot%20measurement.%20However%2C%20most%20existing%20reconstruction%0Amethods%20are%20incapable%20of%20efficiently%20capturing%20long-range%20spatial%20and%20temporal%0Adependencies%2C%20which%20are%20critical%20for%20video%20processing.%20In%20this%20paper%2C%20we%0Apropose%20a%20flexible%20and%20robust%20approach%20based%20on%20the%20graph%20neural%20network%20%28GNN%29%0Ato%20efficiently%20model%20non-local%20interactions%20between%20pixels%20in%20space%20and%20time%0Aregardless%20of%20the%20distance.%20Specifically%2C%20we%20develop%20a%20motion-aware%20dynamic%20GNN%0Afor%20better%20video%20representation%2C%20i.e.%2C%20represent%20each%20node%20as%20the%20aggregation%0Aof%20relative%20neighbors%20under%20the%20guidance%20of%20frame-by-frame%20motions%2C%20which%0Aconsists%20of%20motion-aware%20dynamic%20sampling%2C%20cross-scale%20node%20sampling%2C%20global%0Aknowledge%20integration%2C%20and%20graph%20aggregation.%20Extensive%20results%20on%20both%0Asimulation%20and%20real%20data%20demonstrate%20both%20the%20effectiveness%20and%20efficiency%20of%0Athe%20proposed%20approach%2C%20and%20the%20visualization%20illustrates%20the%20intrinsic%20dynamic%0Asampling%20operations%20of%20our%20proposed%20model%20for%20boosting%20the%20video%20SCI%0Areconstruction%20results.%20The%20code%20and%20model%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2203.00387v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion-aware%2520Dynamic%2520Graph%2520Neural%2520Network%2520for%2520Video%2520Compressive%2520Sensing%26entry.906535625%3DRuiying%2520Lu%2520and%2520Ziheng%2520Cheng%2520and%2520Bo%2520Chen%2520and%2520Xin%2520Yuan%26entry.1292438233%3D%2520%2520Video%2520snapshot%2520compressive%2520imaging%2520%2528SCI%2529%2520utilizes%2520a%25202D%2520detector%2520to%2520capture%250Asequential%2520video%2520frames%2520and%2520compress%2520them%2520into%2520a%2520single%2520measurement.%2520Various%250Areconstruction%2520methods%2520have%2520been%2520developed%2520to%2520recover%2520the%2520high-speed%2520video%250Aframes%2520from%2520the%2520snapshot%2520measurement.%2520However%252C%2520most%2520existing%2520reconstruction%250Amethods%2520are%2520incapable%2520of%2520efficiently%2520capturing%2520long-range%2520spatial%2520and%2520temporal%250Adependencies%252C%2520which%2520are%2520critical%2520for%2520video%2520processing.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520flexible%2520and%2520robust%2520approach%2520based%2520on%2520the%2520graph%2520neural%2520network%2520%2528GNN%2529%250Ato%2520efficiently%2520model%2520non-local%2520interactions%2520between%2520pixels%2520in%2520space%2520and%2520time%250Aregardless%2520of%2520the%2520distance.%2520Specifically%252C%2520we%2520develop%2520a%2520motion-aware%2520dynamic%2520GNN%250Afor%2520better%2520video%2520representation%252C%2520i.e.%252C%2520represent%2520each%2520node%2520as%2520the%2520aggregation%250Aof%2520relative%2520neighbors%2520under%2520the%2520guidance%2520of%2520frame-by-frame%2520motions%252C%2520which%250Aconsists%2520of%2520motion-aware%2520dynamic%2520sampling%252C%2520cross-scale%2520node%2520sampling%252C%2520global%250Aknowledge%2520integration%252C%2520and%2520graph%2520aggregation.%2520Extensive%2520results%2520on%2520both%250Asimulation%2520and%2520real%2520data%2520demonstrate%2520both%2520the%2520effectiveness%2520and%2520efficiency%2520of%250Athe%2520proposed%2520approach%252C%2520and%2520the%2520visualization%2520illustrates%2520the%2520intrinsic%2520dynamic%250Asampling%2520operations%2520of%2520our%2520proposed%2520model%2520for%2520boosting%2520the%2520video%2520SCI%250Areconstruction%2520results.%2520The%2520code%2520and%2520model%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2203.00387v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion-aware%20Dynamic%20Graph%20Neural%20Network%20for%20Video%20Compressive%20Sensing&entry.906535625=Ruiying%20Lu%20and%20Ziheng%20Cheng%20and%20Bo%20Chen%20and%20Xin%20Yuan&entry.1292438233=%20%20Video%20snapshot%20compressive%20imaging%20%28SCI%29%20utilizes%20a%202D%20detector%20to%20capture%0Asequential%20video%20frames%20and%20compress%20them%20into%20a%20single%20measurement.%20Various%0Areconstruction%20methods%20have%20been%20developed%20to%20recover%20the%20high-speed%20video%0Aframes%20from%20the%20snapshot%20measurement.%20However%2C%20most%20existing%20reconstruction%0Amethods%20are%20incapable%20of%20efficiently%20capturing%20long-range%20spatial%20and%20temporal%0Adependencies%2C%20which%20are%20critical%20for%20video%20processing.%20In%20this%20paper%2C%20we%0Apropose%20a%20flexible%20and%20robust%20approach%20based%20on%20the%20graph%20neural%20network%20%28GNN%29%0Ato%20efficiently%20model%20non-local%20interactions%20between%20pixels%20in%20space%20and%20time%0Aregardless%20of%20the%20distance.%20Specifically%2C%20we%20develop%20a%20motion-aware%20dynamic%20GNN%0Afor%20better%20video%20representation%2C%20i.e.%2C%20represent%20each%20node%20as%20the%20aggregation%0Aof%20relative%20neighbors%20under%20the%20guidance%20of%20frame-by-frame%20motions%2C%20which%0Aconsists%20of%20motion-aware%20dynamic%20sampling%2C%20cross-scale%20node%20sampling%2C%20global%0Aknowledge%20integration%2C%20and%20graph%20aggregation.%20Extensive%20results%20on%20both%0Asimulation%20and%20real%20data%20demonstrate%20both%20the%20effectiveness%20and%20efficiency%20of%0Athe%20proposed%20approach%2C%20and%20the%20visualization%20illustrates%20the%20intrinsic%20dynamic%0Asampling%20operations%20of%20our%20proposed%20model%20for%20boosting%20the%20video%20SCI%0Areconstruction%20results.%20The%20code%20and%20model%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2203.00387v2&entry.124074799=Read"},
{"title": "Aligning Agents like Large Language Models", "author": "Adam Jelley and Yuhan Cao and Dave Bignell and Sam Devlin and Tabish Rashid", "abstract": "  Training agents to behave as desired in complex 3D environments from\nhigh-dimensional sensory information is challenging. Imitation learning from\ndiverse human behavior provides a scalable approach for training an agent with\na sensible behavioral prior, but such an agent may not perform the specific\nbehaviors of interest when deployed. To address this issue, we draw an analogy\nbetween the undesirable behaviors of imitation learning agents and the\nunhelpful responses of unaligned large language models (LLMs). We then\ninvestigate how the procedure for aligning LLMs can be applied to aligning\nagents in a 3D environment from pixels. For our analysis, we utilize an\nacademically illustrative part of a modern console game in which the human\nbehavior distribution is multi-modal, but we want our agent to imitate a single\nmode of this behavior. We demonstrate that we can align our agent to\nconsistently perform the desired mode, while providing insights and advice for\nsuccessfully applying this approach to training agents. Project webpage at\nhttps://adamjelley.github.io/aligning-agents-like-llms .\n", "link": "http://arxiv.org/abs/2406.04208v1", "date": "2024-06-06", "relevancy": 2.2445, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5685}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5564}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aligning%20Agents%20like%20Large%20Language%20Models&body=Title%3A%20Aligning%20Agents%20like%20Large%20Language%20Models%0AAuthor%3A%20Adam%20Jelley%20and%20Yuhan%20Cao%20and%20Dave%20Bignell%20and%20Sam%20Devlin%20and%20Tabish%20Rashid%0AAbstract%3A%20%20%20Training%20agents%20to%20behave%20as%20desired%20in%20complex%203D%20environments%20from%0Ahigh-dimensional%20sensory%20information%20is%20challenging.%20Imitation%20learning%20from%0Adiverse%20human%20behavior%20provides%20a%20scalable%20approach%20for%20training%20an%20agent%20with%0Aa%20sensible%20behavioral%20prior%2C%20but%20such%20an%20agent%20may%20not%20perform%20the%20specific%0Abehaviors%20of%20interest%20when%20deployed.%20To%20address%20this%20issue%2C%20we%20draw%20an%20analogy%0Abetween%20the%20undesirable%20behaviors%20of%20imitation%20learning%20agents%20and%20the%0Aunhelpful%20responses%20of%20unaligned%20large%20language%20models%20%28LLMs%29.%20We%20then%0Ainvestigate%20how%20the%20procedure%20for%20aligning%20LLMs%20can%20be%20applied%20to%20aligning%0Aagents%20in%20a%203D%20environment%20from%20pixels.%20For%20our%20analysis%2C%20we%20utilize%20an%0Aacademically%20illustrative%20part%20of%20a%20modern%20console%20game%20in%20which%20the%20human%0Abehavior%20distribution%20is%20multi-modal%2C%20but%20we%20want%20our%20agent%20to%20imitate%20a%20single%0Amode%20of%20this%20behavior.%20We%20demonstrate%20that%20we%20can%20align%20our%20agent%20to%0Aconsistently%20perform%20the%20desired%20mode%2C%20while%20providing%20insights%20and%20advice%20for%0Asuccessfully%20applying%20this%20approach%20to%20training%20agents.%20Project%20webpage%20at%0Ahttps%3A//adamjelley.github.io/aligning-agents-like-llms%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAligning%2520Agents%2520like%2520Large%2520Language%2520Models%26entry.906535625%3DAdam%2520Jelley%2520and%2520Yuhan%2520Cao%2520and%2520Dave%2520Bignell%2520and%2520Sam%2520Devlin%2520and%2520Tabish%2520Rashid%26entry.1292438233%3D%2520%2520Training%2520agents%2520to%2520behave%2520as%2520desired%2520in%2520complex%25203D%2520environments%2520from%250Ahigh-dimensional%2520sensory%2520information%2520is%2520challenging.%2520Imitation%2520learning%2520from%250Adiverse%2520human%2520behavior%2520provides%2520a%2520scalable%2520approach%2520for%2520training%2520an%2520agent%2520with%250Aa%2520sensible%2520behavioral%2520prior%252C%2520but%2520such%2520an%2520agent%2520may%2520not%2520perform%2520the%2520specific%250Abehaviors%2520of%2520interest%2520when%2520deployed.%2520To%2520address%2520this%2520issue%252C%2520we%2520draw%2520an%2520analogy%250Abetween%2520the%2520undesirable%2520behaviors%2520of%2520imitation%2520learning%2520agents%2520and%2520the%250Aunhelpful%2520responses%2520of%2520unaligned%2520large%2520language%2520models%2520%2528LLMs%2529.%2520We%2520then%250Ainvestigate%2520how%2520the%2520procedure%2520for%2520aligning%2520LLMs%2520can%2520be%2520applied%2520to%2520aligning%250Aagents%2520in%2520a%25203D%2520environment%2520from%2520pixels.%2520For%2520our%2520analysis%252C%2520we%2520utilize%2520an%250Aacademically%2520illustrative%2520part%2520of%2520a%2520modern%2520console%2520game%2520in%2520which%2520the%2520human%250Abehavior%2520distribution%2520is%2520multi-modal%252C%2520but%2520we%2520want%2520our%2520agent%2520to%2520imitate%2520a%2520single%250Amode%2520of%2520this%2520behavior.%2520We%2520demonstrate%2520that%2520we%2520can%2520align%2520our%2520agent%2520to%250Aconsistently%2520perform%2520the%2520desired%2520mode%252C%2520while%2520providing%2520insights%2520and%2520advice%2520for%250Asuccessfully%2520applying%2520this%2520approach%2520to%2520training%2520agents.%2520Project%2520webpage%2520at%250Ahttps%253A//adamjelley.github.io/aligning-agents-like-llms%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aligning%20Agents%20like%20Large%20Language%20Models&entry.906535625=Adam%20Jelley%20and%20Yuhan%20Cao%20and%20Dave%20Bignell%20and%20Sam%20Devlin%20and%20Tabish%20Rashid&entry.1292438233=%20%20Training%20agents%20to%20behave%20as%20desired%20in%20complex%203D%20environments%20from%0Ahigh-dimensional%20sensory%20information%20is%20challenging.%20Imitation%20learning%20from%0Adiverse%20human%20behavior%20provides%20a%20scalable%20approach%20for%20training%20an%20agent%20with%0Aa%20sensible%20behavioral%20prior%2C%20but%20such%20an%20agent%20may%20not%20perform%20the%20specific%0Abehaviors%20of%20interest%20when%20deployed.%20To%20address%20this%20issue%2C%20we%20draw%20an%20analogy%0Abetween%20the%20undesirable%20behaviors%20of%20imitation%20learning%20agents%20and%20the%0Aunhelpful%20responses%20of%20unaligned%20large%20language%20models%20%28LLMs%29.%20We%20then%0Ainvestigate%20how%20the%20procedure%20for%20aligning%20LLMs%20can%20be%20applied%20to%20aligning%0Aagents%20in%20a%203D%20environment%20from%20pixels.%20For%20our%20analysis%2C%20we%20utilize%20an%0Aacademically%20illustrative%20part%20of%20a%20modern%20console%20game%20in%20which%20the%20human%0Abehavior%20distribution%20is%20multi-modal%2C%20but%20we%20want%20our%20agent%20to%20imitate%20a%20single%0Amode%20of%20this%20behavior.%20We%20demonstrate%20that%20we%20can%20align%20our%20agent%20to%0Aconsistently%20perform%20the%20desired%20mode%2C%20while%20providing%20insights%20and%20advice%20for%0Asuccessfully%20applying%20this%20approach%20to%20training%20agents.%20Project%20webpage%20at%0Ahttps%3A//adamjelley.github.io/aligning-agents-like-llms%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04208v1&entry.124074799=Read"},
{"title": "Coarse-To-Fine Tensor Trains for Compact Visual Representations", "author": "Sebastian Loeschcke and Dan Wang and Christian Leth-Espensen and Serge Belongie and Michael J. Kastoryano and Sagie Benaim", "abstract": "  The ability to learn compact, high-quality, and easy-to-optimize\nrepresentations for visual data is paramount to many applications such as novel\nview synthesis and 3D reconstruction. Recent work has shown substantial success\nin using tensor networks to design such compact and high-quality\nrepresentations. However, the ability to optimize tensor-based representations,\nand in particular, the highly compact tensor train representation, is still\nlacking. This has prevented practitioners from deploying the full potential of\ntensor networks for visual data. To this end, we propose 'Prolongation\nUpsampling Tensor Train (PuTT)', a novel method for learning tensor train\nrepresentations in a coarse-to-fine manner. Our method involves the prolonging\nor `upsampling' of a learned tensor train representation, creating a sequence\nof 'coarse-to-fine' tensor trains that are incrementally refined. We evaluate\nour representation along three axes: (1). compression, (2). denoising\ncapability, and (3). image completion capability. To assess these axes, we\nconsider the tasks of image fitting, 3D fitting, and novel view synthesis,\nwhere our method shows an improved performance compared to state-of-the-art\ntensor-based methods. For full results see our project webpage:\nhttps://sebulo.github.io/PuTT_website/\n", "link": "http://arxiv.org/abs/2406.04332v1", "date": "2024-06-06", "relevancy": 2.218, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5656}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5527}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coarse-To-Fine%20Tensor%20Trains%20for%20Compact%20Visual%20Representations&body=Title%3A%20Coarse-To-Fine%20Tensor%20Trains%20for%20Compact%20Visual%20Representations%0AAuthor%3A%20Sebastian%20Loeschcke%20and%20Dan%20Wang%20and%20Christian%20Leth-Espensen%20and%20Serge%20Belongie%20and%20Michael%20J.%20Kastoryano%20and%20Sagie%20Benaim%0AAbstract%3A%20%20%20The%20ability%20to%20learn%20compact%2C%20high-quality%2C%20and%20easy-to-optimize%0Arepresentations%20for%20visual%20data%20is%20paramount%20to%20many%20applications%20such%20as%20novel%0Aview%20synthesis%20and%203D%20reconstruction.%20Recent%20work%20has%20shown%20substantial%20success%0Ain%20using%20tensor%20networks%20to%20design%20such%20compact%20and%20high-quality%0Arepresentations.%20However%2C%20the%20ability%20to%20optimize%20tensor-based%20representations%2C%0Aand%20in%20particular%2C%20the%20highly%20compact%20tensor%20train%20representation%2C%20is%20still%0Alacking.%20This%20has%20prevented%20practitioners%20from%20deploying%20the%20full%20potential%20of%0Atensor%20networks%20for%20visual%20data.%20To%20this%20end%2C%20we%20propose%20%27Prolongation%0AUpsampling%20Tensor%20Train%20%28PuTT%29%27%2C%20a%20novel%20method%20for%20learning%20tensor%20train%0Arepresentations%20in%20a%20coarse-to-fine%20manner.%20Our%20method%20involves%20the%20prolonging%0Aor%20%60upsampling%27%20of%20a%20learned%20tensor%20train%20representation%2C%20creating%20a%20sequence%0Aof%20%27coarse-to-fine%27%20tensor%20trains%20that%20are%20incrementally%20refined.%20We%20evaluate%0Aour%20representation%20along%20three%20axes%3A%20%281%29.%20compression%2C%20%282%29.%20denoising%0Acapability%2C%20and%20%283%29.%20image%20completion%20capability.%20To%20assess%20these%20axes%2C%20we%0Aconsider%20the%20tasks%20of%20image%20fitting%2C%203D%20fitting%2C%20and%20novel%20view%20synthesis%2C%0Awhere%20our%20method%20shows%20an%20improved%20performance%20compared%20to%20state-of-the-art%0Atensor-based%20methods.%20For%20full%20results%20see%20our%20project%20webpage%3A%0Ahttps%3A//sebulo.github.io/PuTT_website/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04332v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoarse-To-Fine%2520Tensor%2520Trains%2520for%2520Compact%2520Visual%2520Representations%26entry.906535625%3DSebastian%2520Loeschcke%2520and%2520Dan%2520Wang%2520and%2520Christian%2520Leth-Espensen%2520and%2520Serge%2520Belongie%2520and%2520Michael%2520J.%2520Kastoryano%2520and%2520Sagie%2520Benaim%26entry.1292438233%3D%2520%2520The%2520ability%2520to%2520learn%2520compact%252C%2520high-quality%252C%2520and%2520easy-to-optimize%250Arepresentations%2520for%2520visual%2520data%2520is%2520paramount%2520to%2520many%2520applications%2520such%2520as%2520novel%250Aview%2520synthesis%2520and%25203D%2520reconstruction.%2520Recent%2520work%2520has%2520shown%2520substantial%2520success%250Ain%2520using%2520tensor%2520networks%2520to%2520design%2520such%2520compact%2520and%2520high-quality%250Arepresentations.%2520However%252C%2520the%2520ability%2520to%2520optimize%2520tensor-based%2520representations%252C%250Aand%2520in%2520particular%252C%2520the%2520highly%2520compact%2520tensor%2520train%2520representation%252C%2520is%2520still%250Alacking.%2520This%2520has%2520prevented%2520practitioners%2520from%2520deploying%2520the%2520full%2520potential%2520of%250Atensor%2520networks%2520for%2520visual%2520data.%2520To%2520this%2520end%252C%2520we%2520propose%2520%2527Prolongation%250AUpsampling%2520Tensor%2520Train%2520%2528PuTT%2529%2527%252C%2520a%2520novel%2520method%2520for%2520learning%2520tensor%2520train%250Arepresentations%2520in%2520a%2520coarse-to-fine%2520manner.%2520Our%2520method%2520involves%2520the%2520prolonging%250Aor%2520%2560upsampling%2527%2520of%2520a%2520learned%2520tensor%2520train%2520representation%252C%2520creating%2520a%2520sequence%250Aof%2520%2527coarse-to-fine%2527%2520tensor%2520trains%2520that%2520are%2520incrementally%2520refined.%2520We%2520evaluate%250Aour%2520representation%2520along%2520three%2520axes%253A%2520%25281%2529.%2520compression%252C%2520%25282%2529.%2520denoising%250Acapability%252C%2520and%2520%25283%2529.%2520image%2520completion%2520capability.%2520To%2520assess%2520these%2520axes%252C%2520we%250Aconsider%2520the%2520tasks%2520of%2520image%2520fitting%252C%25203D%2520fitting%252C%2520and%2520novel%2520view%2520synthesis%252C%250Awhere%2520our%2520method%2520shows%2520an%2520improved%2520performance%2520compared%2520to%2520state-of-the-art%250Atensor-based%2520methods.%2520For%2520full%2520results%2520see%2520our%2520project%2520webpage%253A%250Ahttps%253A//sebulo.github.io/PuTT_website/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04332v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coarse-To-Fine%20Tensor%20Trains%20for%20Compact%20Visual%20Representations&entry.906535625=Sebastian%20Loeschcke%20and%20Dan%20Wang%20and%20Christian%20Leth-Espensen%20and%20Serge%20Belongie%20and%20Michael%20J.%20Kastoryano%20and%20Sagie%20Benaim&entry.1292438233=%20%20The%20ability%20to%20learn%20compact%2C%20high-quality%2C%20and%20easy-to-optimize%0Arepresentations%20for%20visual%20data%20is%20paramount%20to%20many%20applications%20such%20as%20novel%0Aview%20synthesis%20and%203D%20reconstruction.%20Recent%20work%20has%20shown%20substantial%20success%0Ain%20using%20tensor%20networks%20to%20design%20such%20compact%20and%20high-quality%0Arepresentations.%20However%2C%20the%20ability%20to%20optimize%20tensor-based%20representations%2C%0Aand%20in%20particular%2C%20the%20highly%20compact%20tensor%20train%20representation%2C%20is%20still%0Alacking.%20This%20has%20prevented%20practitioners%20from%20deploying%20the%20full%20potential%20of%0Atensor%20networks%20for%20visual%20data.%20To%20this%20end%2C%20we%20propose%20%27Prolongation%0AUpsampling%20Tensor%20Train%20%28PuTT%29%27%2C%20a%20novel%20method%20for%20learning%20tensor%20train%0Arepresentations%20in%20a%20coarse-to-fine%20manner.%20Our%20method%20involves%20the%20prolonging%0Aor%20%60upsampling%27%20of%20a%20learned%20tensor%20train%20representation%2C%20creating%20a%20sequence%0Aof%20%27coarse-to-fine%27%20tensor%20trains%20that%20are%20incrementally%20refined.%20We%20evaluate%0Aour%20representation%20along%20three%20axes%3A%20%281%29.%20compression%2C%20%282%29.%20denoising%0Acapability%2C%20and%20%283%29.%20image%20completion%20capability.%20To%20assess%20these%20axes%2C%20we%0Aconsider%20the%20tasks%20of%20image%20fitting%2C%203D%20fitting%2C%20and%20novel%20view%20synthesis%2C%0Awhere%20our%20method%20shows%20an%20improved%20performance%20compared%20to%20state-of-the-art%0Atensor-based%20methods.%20For%20full%20results%20see%20our%20project%20webpage%3A%0Ahttps%3A//sebulo.github.io/PuTT_website/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04332v1&entry.124074799=Read"},
{"title": "Frequency-based Matcher for Long-tailed Semantic Segmentation", "author": "Shan Li and Lu Yang and Pu Cao and Liulei Li and Huadong Ma", "abstract": "  The successful application of semantic segmentation technology in the real\nworld has been among the most exciting achievements in the computer vision\ncommunity over the past decade. Although the long-tailed phenomenon has been\ninvestigated in many fields, e.g., classification and object detection, it has\nnot received enough attention in semantic segmentation and has become a\nnon-negligible obstacle to applying semantic segmentation technology in\nautonomous driving and virtual reality. Therefore, in this work, we focus on a\nrelatively under-explored task setting, long-tailed semantic segmentation\n(LTSS). We first establish three representative datasets from different\naspects, i.e., scene, object, and human. We further propose a dual-metric\nevaluation system and construct the LTSS benchmark to demonstrate the\nperformance of semantic segmentation methods and long-tailed solutions. We also\npropose a transformer-based algorithm to improve LTSS, frequency-based matcher,\nwhich solves the oversuppression problem by one-to-many matching and\nautomatically determines the number of matching queries for each class. Given\nthe comprehensiveness of this work and the importance of the issues revealed,\nthis work aims to promote the empirical study of semantic segmentation tasks.\nOur datasets, codes, and models will be publicly available.\n", "link": "http://arxiv.org/abs/2406.03917v1", "date": "2024-06-06", "relevancy": 2.2062, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5765}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5408}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5159}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frequency-based%20Matcher%20for%20Long-tailed%20Semantic%20Segmentation&body=Title%3A%20Frequency-based%20Matcher%20for%20Long-tailed%20Semantic%20Segmentation%0AAuthor%3A%20Shan%20Li%20and%20Lu%20Yang%20and%20Pu%20Cao%20and%20Liulei%20Li%20and%20Huadong%20Ma%0AAbstract%3A%20%20%20The%20successful%20application%20of%20semantic%20segmentation%20technology%20in%20the%20real%0Aworld%20has%20been%20among%20the%20most%20exciting%20achievements%20in%20the%20computer%20vision%0Acommunity%20over%20the%20past%20decade.%20Although%20the%20long-tailed%20phenomenon%20has%20been%0Ainvestigated%20in%20many%20fields%2C%20e.g.%2C%20classification%20and%20object%20detection%2C%20it%20has%0Anot%20received%20enough%20attention%20in%20semantic%20segmentation%20and%20has%20become%20a%0Anon-negligible%20obstacle%20to%20applying%20semantic%20segmentation%20technology%20in%0Aautonomous%20driving%20and%20virtual%20reality.%20Therefore%2C%20in%20this%20work%2C%20we%20focus%20on%20a%0Arelatively%20under-explored%20task%20setting%2C%20long-tailed%20semantic%20segmentation%0A%28LTSS%29.%20We%20first%20establish%20three%20representative%20datasets%20from%20different%0Aaspects%2C%20i.e.%2C%20scene%2C%20object%2C%20and%20human.%20We%20further%20propose%20a%20dual-metric%0Aevaluation%20system%20and%20construct%20the%20LTSS%20benchmark%20to%20demonstrate%20the%0Aperformance%20of%20semantic%20segmentation%20methods%20and%20long-tailed%20solutions.%20We%20also%0Apropose%20a%20transformer-based%20algorithm%20to%20improve%20LTSS%2C%20frequency-based%20matcher%2C%0Awhich%20solves%20the%20oversuppression%20problem%20by%20one-to-many%20matching%20and%0Aautomatically%20determines%20the%20number%20of%20matching%20queries%20for%20each%20class.%20Given%0Athe%20comprehensiveness%20of%20this%20work%20and%20the%20importance%20of%20the%20issues%20revealed%2C%0Athis%20work%20aims%20to%20promote%20the%20empirical%20study%20of%20semantic%20segmentation%20tasks.%0AOur%20datasets%2C%20codes%2C%20and%20models%20will%20be%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03917v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrequency-based%2520Matcher%2520for%2520Long-tailed%2520Semantic%2520Segmentation%26entry.906535625%3DShan%2520Li%2520and%2520Lu%2520Yang%2520and%2520Pu%2520Cao%2520and%2520Liulei%2520Li%2520and%2520Huadong%2520Ma%26entry.1292438233%3D%2520%2520The%2520successful%2520application%2520of%2520semantic%2520segmentation%2520technology%2520in%2520the%2520real%250Aworld%2520has%2520been%2520among%2520the%2520most%2520exciting%2520achievements%2520in%2520the%2520computer%2520vision%250Acommunity%2520over%2520the%2520past%2520decade.%2520Although%2520the%2520long-tailed%2520phenomenon%2520has%2520been%250Ainvestigated%2520in%2520many%2520fields%252C%2520e.g.%252C%2520classification%2520and%2520object%2520detection%252C%2520it%2520has%250Anot%2520received%2520enough%2520attention%2520in%2520semantic%2520segmentation%2520and%2520has%2520become%2520a%250Anon-negligible%2520obstacle%2520to%2520applying%2520semantic%2520segmentation%2520technology%2520in%250Aautonomous%2520driving%2520and%2520virtual%2520reality.%2520Therefore%252C%2520in%2520this%2520work%252C%2520we%2520focus%2520on%2520a%250Arelatively%2520under-explored%2520task%2520setting%252C%2520long-tailed%2520semantic%2520segmentation%250A%2528LTSS%2529.%2520We%2520first%2520establish%2520three%2520representative%2520datasets%2520from%2520different%250Aaspects%252C%2520i.e.%252C%2520scene%252C%2520object%252C%2520and%2520human.%2520We%2520further%2520propose%2520a%2520dual-metric%250Aevaluation%2520system%2520and%2520construct%2520the%2520LTSS%2520benchmark%2520to%2520demonstrate%2520the%250Aperformance%2520of%2520semantic%2520segmentation%2520methods%2520and%2520long-tailed%2520solutions.%2520We%2520also%250Apropose%2520a%2520transformer-based%2520algorithm%2520to%2520improve%2520LTSS%252C%2520frequency-based%2520matcher%252C%250Awhich%2520solves%2520the%2520oversuppression%2520problem%2520by%2520one-to-many%2520matching%2520and%250Aautomatically%2520determines%2520the%2520number%2520of%2520matching%2520queries%2520for%2520each%2520class.%2520Given%250Athe%2520comprehensiveness%2520of%2520this%2520work%2520and%2520the%2520importance%2520of%2520the%2520issues%2520revealed%252C%250Athis%2520work%2520aims%2520to%2520promote%2520the%2520empirical%2520study%2520of%2520semantic%2520segmentation%2520tasks.%250AOur%2520datasets%252C%2520codes%252C%2520and%2520models%2520will%2520be%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03917v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frequency-based%20Matcher%20for%20Long-tailed%20Semantic%20Segmentation&entry.906535625=Shan%20Li%20and%20Lu%20Yang%20and%20Pu%20Cao%20and%20Liulei%20Li%20and%20Huadong%20Ma&entry.1292438233=%20%20The%20successful%20application%20of%20semantic%20segmentation%20technology%20in%20the%20real%0Aworld%20has%20been%20among%20the%20most%20exciting%20achievements%20in%20the%20computer%20vision%0Acommunity%20over%20the%20past%20decade.%20Although%20the%20long-tailed%20phenomenon%20has%20been%0Ainvestigated%20in%20many%20fields%2C%20e.g.%2C%20classification%20and%20object%20detection%2C%20it%20has%0Anot%20received%20enough%20attention%20in%20semantic%20segmentation%20and%20has%20become%20a%0Anon-negligible%20obstacle%20to%20applying%20semantic%20segmentation%20technology%20in%0Aautonomous%20driving%20and%20virtual%20reality.%20Therefore%2C%20in%20this%20work%2C%20we%20focus%20on%20a%0Arelatively%20under-explored%20task%20setting%2C%20long-tailed%20semantic%20segmentation%0A%28LTSS%29.%20We%20first%20establish%20three%20representative%20datasets%20from%20different%0Aaspects%2C%20i.e.%2C%20scene%2C%20object%2C%20and%20human.%20We%20further%20propose%20a%20dual-metric%0Aevaluation%20system%20and%20construct%20the%20LTSS%20benchmark%20to%20demonstrate%20the%0Aperformance%20of%20semantic%20segmentation%20methods%20and%20long-tailed%20solutions.%20We%20also%0Apropose%20a%20transformer-based%20algorithm%20to%20improve%20LTSS%2C%20frequency-based%20matcher%2C%0Awhich%20solves%20the%20oversuppression%20problem%20by%20one-to-many%20matching%20and%0Aautomatically%20determines%20the%20number%20of%20matching%20queries%20for%20each%20class.%20Given%0Athe%20comprehensiveness%20of%20this%20work%20and%20the%20importance%20of%20the%20issues%20revealed%2C%0Athis%20work%20aims%20to%20promote%20the%20empirical%20study%20of%20semantic%20segmentation%20tasks.%0AOur%20datasets%2C%20codes%2C%20and%20models%20will%20be%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03917v1&entry.124074799=Read"},
{"title": "Towards Avoiding the Data Mess: Industry Insights from Data Mesh\n  Implementations", "author": "Jan Bode and Niklas K\u00fchl and Dominik Kreuzberger and Sebastian Hirschl and Carsten Holtmann", "abstract": "  With the increasing importance of data and artificial intelligence,\norganizations strive to become more data-driven. However, current data\narchitectures are not necessarily designed to keep up with the scale and scope\nof data and analytics use cases. In fact, existing architectures often fail to\ndeliver the promised value associated with them. Data mesh is a\nsocio-technical, decentralized, distributed concept for enterprise data\nmanagement. As the concept of data mesh is still novel, it lacks empirical\ninsights from the field. Specifically, an understanding of the motivational\nfactors for introducing data mesh, the associated challenges, implementation\nstrategies, its business impact, and potential archetypes is missing. To\naddress this gap, we conduct 15 semi-structured interviews with industry\nexperts. Our results show, among other insights, that organizations have\ndifficulties with the transition toward federated governance associated with\nthe data mesh concept, the shift of responsibility for the development,\nprovision, and maintenance of data products, and the comprehension of the\noverall concept. In our work, we derive multiple implementation strategies and\nsuggest organizations introduce a cross-domain steering unit, observe the data\nproduct usage, create quick wins in the early phases, and favor small dedicated\nteams that prioritize data products. While we acknowledge that organizations\nneed to apply implementation strategies according to their individual needs, we\nalso deduct two archetypes that provide suggestions in more detail. Our\nfindings synthesize insights from industry experts and provide researchers and\nprofessionals with preliminary guidelines for the successful adoption of data\nmesh.\n", "link": "http://arxiv.org/abs/2302.01713v4", "date": "2024-06-06", "relevancy": 2.2055, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4775}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4309}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4149}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Avoiding%20the%20Data%20Mess%3A%20Industry%20Insights%20from%20Data%20Mesh%0A%20%20Implementations&body=Title%3A%20Towards%20Avoiding%20the%20Data%20Mess%3A%20Industry%20Insights%20from%20Data%20Mesh%0A%20%20Implementations%0AAuthor%3A%20Jan%20Bode%20and%20Niklas%20K%C3%BChl%20and%20Dominik%20Kreuzberger%20and%20Sebastian%20Hirschl%20and%20Carsten%20Holtmann%0AAbstract%3A%20%20%20With%20the%20increasing%20importance%20of%20data%20and%20artificial%20intelligence%2C%0Aorganizations%20strive%20to%20become%20more%20data-driven.%20However%2C%20current%20data%0Aarchitectures%20are%20not%20necessarily%20designed%20to%20keep%20up%20with%20the%20scale%20and%20scope%0Aof%20data%20and%20analytics%20use%20cases.%20In%20fact%2C%20existing%20architectures%20often%20fail%20to%0Adeliver%20the%20promised%20value%20associated%20with%20them.%20Data%20mesh%20is%20a%0Asocio-technical%2C%20decentralized%2C%20distributed%20concept%20for%20enterprise%20data%0Amanagement.%20As%20the%20concept%20of%20data%20mesh%20is%20still%20novel%2C%20it%20lacks%20empirical%0Ainsights%20from%20the%20field.%20Specifically%2C%20an%20understanding%20of%20the%20motivational%0Afactors%20for%20introducing%20data%20mesh%2C%20the%20associated%20challenges%2C%20implementation%0Astrategies%2C%20its%20business%20impact%2C%20and%20potential%20archetypes%20is%20missing.%20To%0Aaddress%20this%20gap%2C%20we%20conduct%2015%20semi-structured%20interviews%20with%20industry%0Aexperts.%20Our%20results%20show%2C%20among%20other%20insights%2C%20that%20organizations%20have%0Adifficulties%20with%20the%20transition%20toward%20federated%20governance%20associated%20with%0Athe%20data%20mesh%20concept%2C%20the%20shift%20of%20responsibility%20for%20the%20development%2C%0Aprovision%2C%20and%20maintenance%20of%20data%20products%2C%20and%20the%20comprehension%20of%20the%0Aoverall%20concept.%20In%20our%20work%2C%20we%20derive%20multiple%20implementation%20strategies%20and%0Asuggest%20organizations%20introduce%20a%20cross-domain%20steering%20unit%2C%20observe%20the%20data%0Aproduct%20usage%2C%20create%20quick%20wins%20in%20the%20early%20phases%2C%20and%20favor%20small%20dedicated%0Ateams%20that%20prioritize%20data%20products.%20While%20we%20acknowledge%20that%20organizations%0Aneed%20to%20apply%20implementation%20strategies%20according%20to%20their%20individual%20needs%2C%20we%0Aalso%20deduct%20two%20archetypes%20that%20provide%20suggestions%20in%20more%20detail.%20Our%0Afindings%20synthesize%20insights%20from%20industry%20experts%20and%20provide%20researchers%20and%0Aprofessionals%20with%20preliminary%20guidelines%20for%20the%20successful%20adoption%20of%20data%0Amesh.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.01713v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Avoiding%2520the%2520Data%2520Mess%253A%2520Industry%2520Insights%2520from%2520Data%2520Mesh%250A%2520%2520Implementations%26entry.906535625%3DJan%2520Bode%2520and%2520Niklas%2520K%25C3%25BChl%2520and%2520Dominik%2520Kreuzberger%2520and%2520Sebastian%2520Hirschl%2520and%2520Carsten%2520Holtmann%26entry.1292438233%3D%2520%2520With%2520the%2520increasing%2520importance%2520of%2520data%2520and%2520artificial%2520intelligence%252C%250Aorganizations%2520strive%2520to%2520become%2520more%2520data-driven.%2520However%252C%2520current%2520data%250Aarchitectures%2520are%2520not%2520necessarily%2520designed%2520to%2520keep%2520up%2520with%2520the%2520scale%2520and%2520scope%250Aof%2520data%2520and%2520analytics%2520use%2520cases.%2520In%2520fact%252C%2520existing%2520architectures%2520often%2520fail%2520to%250Adeliver%2520the%2520promised%2520value%2520associated%2520with%2520them.%2520Data%2520mesh%2520is%2520a%250Asocio-technical%252C%2520decentralized%252C%2520distributed%2520concept%2520for%2520enterprise%2520data%250Amanagement.%2520As%2520the%2520concept%2520of%2520data%2520mesh%2520is%2520still%2520novel%252C%2520it%2520lacks%2520empirical%250Ainsights%2520from%2520the%2520field.%2520Specifically%252C%2520an%2520understanding%2520of%2520the%2520motivational%250Afactors%2520for%2520introducing%2520data%2520mesh%252C%2520the%2520associated%2520challenges%252C%2520implementation%250Astrategies%252C%2520its%2520business%2520impact%252C%2520and%2520potential%2520archetypes%2520is%2520missing.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520conduct%252015%2520semi-structured%2520interviews%2520with%2520industry%250Aexperts.%2520Our%2520results%2520show%252C%2520among%2520other%2520insights%252C%2520that%2520organizations%2520have%250Adifficulties%2520with%2520the%2520transition%2520toward%2520federated%2520governance%2520associated%2520with%250Athe%2520data%2520mesh%2520concept%252C%2520the%2520shift%2520of%2520responsibility%2520for%2520the%2520development%252C%250Aprovision%252C%2520and%2520maintenance%2520of%2520data%2520products%252C%2520and%2520the%2520comprehension%2520of%2520the%250Aoverall%2520concept.%2520In%2520our%2520work%252C%2520we%2520derive%2520multiple%2520implementation%2520strategies%2520and%250Asuggest%2520organizations%2520introduce%2520a%2520cross-domain%2520steering%2520unit%252C%2520observe%2520the%2520data%250Aproduct%2520usage%252C%2520create%2520quick%2520wins%2520in%2520the%2520early%2520phases%252C%2520and%2520favor%2520small%2520dedicated%250Ateams%2520that%2520prioritize%2520data%2520products.%2520While%2520we%2520acknowledge%2520that%2520organizations%250Aneed%2520to%2520apply%2520implementation%2520strategies%2520according%2520to%2520their%2520individual%2520needs%252C%2520we%250Aalso%2520deduct%2520two%2520archetypes%2520that%2520provide%2520suggestions%2520in%2520more%2520detail.%2520Our%250Afindings%2520synthesize%2520insights%2520from%2520industry%2520experts%2520and%2520provide%2520researchers%2520and%250Aprofessionals%2520with%2520preliminary%2520guidelines%2520for%2520the%2520successful%2520adoption%2520of%2520data%250Amesh.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.01713v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Avoiding%20the%20Data%20Mess%3A%20Industry%20Insights%20from%20Data%20Mesh%0A%20%20Implementations&entry.906535625=Jan%20Bode%20and%20Niklas%20K%C3%BChl%20and%20Dominik%20Kreuzberger%20and%20Sebastian%20Hirschl%20and%20Carsten%20Holtmann&entry.1292438233=%20%20With%20the%20increasing%20importance%20of%20data%20and%20artificial%20intelligence%2C%0Aorganizations%20strive%20to%20become%20more%20data-driven.%20However%2C%20current%20data%0Aarchitectures%20are%20not%20necessarily%20designed%20to%20keep%20up%20with%20the%20scale%20and%20scope%0Aof%20data%20and%20analytics%20use%20cases.%20In%20fact%2C%20existing%20architectures%20often%20fail%20to%0Adeliver%20the%20promised%20value%20associated%20with%20them.%20Data%20mesh%20is%20a%0Asocio-technical%2C%20decentralized%2C%20distributed%20concept%20for%20enterprise%20data%0Amanagement.%20As%20the%20concept%20of%20data%20mesh%20is%20still%20novel%2C%20it%20lacks%20empirical%0Ainsights%20from%20the%20field.%20Specifically%2C%20an%20understanding%20of%20the%20motivational%0Afactors%20for%20introducing%20data%20mesh%2C%20the%20associated%20challenges%2C%20implementation%0Astrategies%2C%20its%20business%20impact%2C%20and%20potential%20archetypes%20is%20missing.%20To%0Aaddress%20this%20gap%2C%20we%20conduct%2015%20semi-structured%20interviews%20with%20industry%0Aexperts.%20Our%20results%20show%2C%20among%20other%20insights%2C%20that%20organizations%20have%0Adifficulties%20with%20the%20transition%20toward%20federated%20governance%20associated%20with%0Athe%20data%20mesh%20concept%2C%20the%20shift%20of%20responsibility%20for%20the%20development%2C%0Aprovision%2C%20and%20maintenance%20of%20data%20products%2C%20and%20the%20comprehension%20of%20the%0Aoverall%20concept.%20In%20our%20work%2C%20we%20derive%20multiple%20implementation%20strategies%20and%0Asuggest%20organizations%20introduce%20a%20cross-domain%20steering%20unit%2C%20observe%20the%20data%0Aproduct%20usage%2C%20create%20quick%20wins%20in%20the%20early%20phases%2C%20and%20favor%20small%20dedicated%0Ateams%20that%20prioritize%20data%20products.%20While%20we%20acknowledge%20that%20organizations%0Aneed%20to%20apply%20implementation%20strategies%20according%20to%20their%20individual%20needs%2C%20we%0Aalso%20deduct%20two%20archetypes%20that%20provide%20suggestions%20in%20more%20detail.%20Our%0Afindings%20synthesize%20insights%20from%20industry%20experts%20and%20provide%20researchers%20and%0Aprofessionals%20with%20preliminary%20guidelines%20for%20the%20successful%20adoption%20of%20data%0Amesh.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.01713v4&entry.124074799=Read"},
{"title": "Interpretable Lightweight Transformer via Unrolling of Learned Graph\n  Smoothness Priors", "author": "Tam Thuc Do and Parham Eftekhar and Seyed Alireza Hosseini and Gene Cheung and Philip Chou", "abstract": "  We build interpretable and lightweight transformer-like neural networks by\nunrolling iterative optimization algorithms that minimize graph smoothness\npriors -- the quadratic graph Laplacian regularizer (GLR) and the $\\ell_1$-norm\ngraph total variation (GTV) -- subject to an interpolation constraint. The\ncrucial insight is that a normalized signal-dependent graph learning module\namounts to a variant of the basic self-attention mechanism in conventional\ntransformers. Unlike \"black-box\" transformers that require learning of large\nkey, query and value matrices to compute scaled dot products as affinities and\nsubsequent output embeddings, resulting in huge parameter sets, our unrolled\nnetworks employ shallow CNNs to learn low-dimensional features per node to\nestablish pairwise Mahalanobis distances and construct sparse similarity\ngraphs. At each layer, given a learned graph, the target interpolated signal is\nsimply a low-pass filtered output derived from the minimization of an assumed\ngraph smoothness prior, leading to a dramatic reduction in parameter count.\nExperiments for two image interpolation applications verify the restoration\nperformance, parameter efficiency and robustness to covariate shift of our\ngraph-based unrolled networks compared to conventional transformers.\n", "link": "http://arxiv.org/abs/2406.04090v1", "date": "2024-06-06", "relevancy": 2.2004, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6287}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5352}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5335}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Lightweight%20Transformer%20via%20Unrolling%20of%20Learned%20Graph%0A%20%20Smoothness%20Priors&body=Title%3A%20Interpretable%20Lightweight%20Transformer%20via%20Unrolling%20of%20Learned%20Graph%0A%20%20Smoothness%20Priors%0AAuthor%3A%20Tam%20Thuc%20Do%20and%20Parham%20Eftekhar%20and%20Seyed%20Alireza%20Hosseini%20and%20Gene%20Cheung%20and%20Philip%20Chou%0AAbstract%3A%20%20%20We%20build%20interpretable%20and%20lightweight%20transformer-like%20neural%20networks%20by%0Aunrolling%20iterative%20optimization%20algorithms%20that%20minimize%20graph%20smoothness%0Apriors%20--%20the%20quadratic%20graph%20Laplacian%20regularizer%20%28GLR%29%20and%20the%20%24%5Cell_1%24-norm%0Agraph%20total%20variation%20%28GTV%29%20--%20subject%20to%20an%20interpolation%20constraint.%20The%0Acrucial%20insight%20is%20that%20a%20normalized%20signal-dependent%20graph%20learning%20module%0Aamounts%20to%20a%20variant%20of%20the%20basic%20self-attention%20mechanism%20in%20conventional%0Atransformers.%20Unlike%20%22black-box%22%20transformers%20that%20require%20learning%20of%20large%0Akey%2C%20query%20and%20value%20matrices%20to%20compute%20scaled%20dot%20products%20as%20affinities%20and%0Asubsequent%20output%20embeddings%2C%20resulting%20in%20huge%20parameter%20sets%2C%20our%20unrolled%0Anetworks%20employ%20shallow%20CNNs%20to%20learn%20low-dimensional%20features%20per%20node%20to%0Aestablish%20pairwise%20Mahalanobis%20distances%20and%20construct%20sparse%20similarity%0Agraphs.%20At%20each%20layer%2C%20given%20a%20learned%20graph%2C%20the%20target%20interpolated%20signal%20is%0Asimply%20a%20low-pass%20filtered%20output%20derived%20from%20the%20minimization%20of%20an%20assumed%0Agraph%20smoothness%20prior%2C%20leading%20to%20a%20dramatic%20reduction%20in%20parameter%20count.%0AExperiments%20for%20two%20image%20interpolation%20applications%20verify%20the%20restoration%0Aperformance%2C%20parameter%20efficiency%20and%20robustness%20to%20covariate%20shift%20of%20our%0Agraph-based%20unrolled%20networks%20compared%20to%20conventional%20transformers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04090v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Lightweight%2520Transformer%2520via%2520Unrolling%2520of%2520Learned%2520Graph%250A%2520%2520Smoothness%2520Priors%26entry.906535625%3DTam%2520Thuc%2520Do%2520and%2520Parham%2520Eftekhar%2520and%2520Seyed%2520Alireza%2520Hosseini%2520and%2520Gene%2520Cheung%2520and%2520Philip%2520Chou%26entry.1292438233%3D%2520%2520We%2520build%2520interpretable%2520and%2520lightweight%2520transformer-like%2520neural%2520networks%2520by%250Aunrolling%2520iterative%2520optimization%2520algorithms%2520that%2520minimize%2520graph%2520smoothness%250Apriors%2520--%2520the%2520quadratic%2520graph%2520Laplacian%2520regularizer%2520%2528GLR%2529%2520and%2520the%2520%2524%255Cell_1%2524-norm%250Agraph%2520total%2520variation%2520%2528GTV%2529%2520--%2520subject%2520to%2520an%2520interpolation%2520constraint.%2520The%250Acrucial%2520insight%2520is%2520that%2520a%2520normalized%2520signal-dependent%2520graph%2520learning%2520module%250Aamounts%2520to%2520a%2520variant%2520of%2520the%2520basic%2520self-attention%2520mechanism%2520in%2520conventional%250Atransformers.%2520Unlike%2520%2522black-box%2522%2520transformers%2520that%2520require%2520learning%2520of%2520large%250Akey%252C%2520query%2520and%2520value%2520matrices%2520to%2520compute%2520scaled%2520dot%2520products%2520as%2520affinities%2520and%250Asubsequent%2520output%2520embeddings%252C%2520resulting%2520in%2520huge%2520parameter%2520sets%252C%2520our%2520unrolled%250Anetworks%2520employ%2520shallow%2520CNNs%2520to%2520learn%2520low-dimensional%2520features%2520per%2520node%2520to%250Aestablish%2520pairwise%2520Mahalanobis%2520distances%2520and%2520construct%2520sparse%2520similarity%250Agraphs.%2520At%2520each%2520layer%252C%2520given%2520a%2520learned%2520graph%252C%2520the%2520target%2520interpolated%2520signal%2520is%250Asimply%2520a%2520low-pass%2520filtered%2520output%2520derived%2520from%2520the%2520minimization%2520of%2520an%2520assumed%250Agraph%2520smoothness%2520prior%252C%2520leading%2520to%2520a%2520dramatic%2520reduction%2520in%2520parameter%2520count.%250AExperiments%2520for%2520two%2520image%2520interpolation%2520applications%2520verify%2520the%2520restoration%250Aperformance%252C%2520parameter%2520efficiency%2520and%2520robustness%2520to%2520covariate%2520shift%2520of%2520our%250Agraph-based%2520unrolled%2520networks%2520compared%2520to%2520conventional%2520transformers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04090v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Lightweight%20Transformer%20via%20Unrolling%20of%20Learned%20Graph%0A%20%20Smoothness%20Priors&entry.906535625=Tam%20Thuc%20Do%20and%20Parham%20Eftekhar%20and%20Seyed%20Alireza%20Hosseini%20and%20Gene%20Cheung%20and%20Philip%20Chou&entry.1292438233=%20%20We%20build%20interpretable%20and%20lightweight%20transformer-like%20neural%20networks%20by%0Aunrolling%20iterative%20optimization%20algorithms%20that%20minimize%20graph%20smoothness%0Apriors%20--%20the%20quadratic%20graph%20Laplacian%20regularizer%20%28GLR%29%20and%20the%20%24%5Cell_1%24-norm%0Agraph%20total%20variation%20%28GTV%29%20--%20subject%20to%20an%20interpolation%20constraint.%20The%0Acrucial%20insight%20is%20that%20a%20normalized%20signal-dependent%20graph%20learning%20module%0Aamounts%20to%20a%20variant%20of%20the%20basic%20self-attention%20mechanism%20in%20conventional%0Atransformers.%20Unlike%20%22black-box%22%20transformers%20that%20require%20learning%20of%20large%0Akey%2C%20query%20and%20value%20matrices%20to%20compute%20scaled%20dot%20products%20as%20affinities%20and%0Asubsequent%20output%20embeddings%2C%20resulting%20in%20huge%20parameter%20sets%2C%20our%20unrolled%0Anetworks%20employ%20shallow%20CNNs%20to%20learn%20low-dimensional%20features%20per%20node%20to%0Aestablish%20pairwise%20Mahalanobis%20distances%20and%20construct%20sparse%20similarity%0Agraphs.%20At%20each%20layer%2C%20given%20a%20learned%20graph%2C%20the%20target%20interpolated%20signal%20is%0Asimply%20a%20low-pass%20filtered%20output%20derived%20from%20the%20minimization%20of%20an%20assumed%0Agraph%20smoothness%20prior%2C%20leading%20to%20a%20dramatic%20reduction%20in%20parameter%20count.%0AExperiments%20for%20two%20image%20interpolation%20applications%20verify%20the%20restoration%0Aperformance%2C%20parameter%20efficiency%20and%20robustness%20to%20covariate%20shift%20of%20our%0Agraph-based%20unrolled%20networks%20compared%20to%20conventional%20transformers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04090v1&entry.124074799=Read"},
{"title": "Collapse-Aware Triplet Decoupling for Adversarially Robust Image\n  Retrieval", "author": "Qiwei Tian and Chenhao Lin and Zhengyu Zhao and Qian Li and Chao Shen", "abstract": "  Adversarial training has achieved substantial performance in defending image\nretrieval against adversarial examples. However, existing studies in deep\nmetric learning (DML) still suffer from two major limitations: weak adversary\nand model collapse. In this paper, we address these two limitations by\nproposing Collapse-Aware TRIplet DEcoupling (CA-TRIDE). Specifically, TRIDE\nyields a stronger adversary by spatially decoupling the perturbation targets\ninto the anchor and the other candidates. Furthermore, CA prevents the\nconsequential model collapse, based on a novel metric, collapseness, which is\nincorporated into the optimization of perturbation. We also identify two\ndrawbacks of the existing robustness metric in image retrieval and propose a\nnew metric for a more reasonable robustness evaluation. Extensive experiments\non three datasets demonstrate that CA-TRIDE outperforms existing defense\nmethods in both conventional and new metrics. Codes are available at\nhttps://github.com/michaeltian108/CA-TRIDE.\n", "link": "http://arxiv.org/abs/2312.07364v4", "date": "2024-06-06", "relevancy": 2.1925, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5774}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.528}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collapse-Aware%20Triplet%20Decoupling%20for%20Adversarially%20Robust%20Image%0A%20%20Retrieval&body=Title%3A%20Collapse-Aware%20Triplet%20Decoupling%20for%20Adversarially%20Robust%20Image%0A%20%20Retrieval%0AAuthor%3A%20Qiwei%20Tian%20and%20Chenhao%20Lin%20and%20Zhengyu%20Zhao%20and%20Qian%20Li%20and%20Chao%20Shen%0AAbstract%3A%20%20%20Adversarial%20training%20has%20achieved%20substantial%20performance%20in%20defending%20image%0Aretrieval%20against%20adversarial%20examples.%20However%2C%20existing%20studies%20in%20deep%0Ametric%20learning%20%28DML%29%20still%20suffer%20from%20two%20major%20limitations%3A%20weak%20adversary%0Aand%20model%20collapse.%20In%20this%20paper%2C%20we%20address%20these%20two%20limitations%20by%0Aproposing%20Collapse-Aware%20TRIplet%20DEcoupling%20%28CA-TRIDE%29.%20Specifically%2C%20TRIDE%0Ayields%20a%20stronger%20adversary%20by%20spatially%20decoupling%20the%20perturbation%20targets%0Ainto%20the%20anchor%20and%20the%20other%20candidates.%20Furthermore%2C%20CA%20prevents%20the%0Aconsequential%20model%20collapse%2C%20based%20on%20a%20novel%20metric%2C%20collapseness%2C%20which%20is%0Aincorporated%20into%20the%20optimization%20of%20perturbation.%20We%20also%20identify%20two%0Adrawbacks%20of%20the%20existing%20robustness%20metric%20in%20image%20retrieval%20and%20propose%20a%0Anew%20metric%20for%20a%20more%20reasonable%20robustness%20evaluation.%20Extensive%20experiments%0Aon%20three%20datasets%20demonstrate%20that%20CA-TRIDE%20outperforms%20existing%20defense%0Amethods%20in%20both%20conventional%20and%20new%20metrics.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/michaeltian108/CA-TRIDE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07364v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollapse-Aware%2520Triplet%2520Decoupling%2520for%2520Adversarially%2520Robust%2520Image%250A%2520%2520Retrieval%26entry.906535625%3DQiwei%2520Tian%2520and%2520Chenhao%2520Lin%2520and%2520Zhengyu%2520Zhao%2520and%2520Qian%2520Li%2520and%2520Chao%2520Shen%26entry.1292438233%3D%2520%2520Adversarial%2520training%2520has%2520achieved%2520substantial%2520performance%2520in%2520defending%2520image%250Aretrieval%2520against%2520adversarial%2520examples.%2520However%252C%2520existing%2520studies%2520in%2520deep%250Ametric%2520learning%2520%2528DML%2529%2520still%2520suffer%2520from%2520two%2520major%2520limitations%253A%2520weak%2520adversary%250Aand%2520model%2520collapse.%2520In%2520this%2520paper%252C%2520we%2520address%2520these%2520two%2520limitations%2520by%250Aproposing%2520Collapse-Aware%2520TRIplet%2520DEcoupling%2520%2528CA-TRIDE%2529.%2520Specifically%252C%2520TRIDE%250Ayields%2520a%2520stronger%2520adversary%2520by%2520spatially%2520decoupling%2520the%2520perturbation%2520targets%250Ainto%2520the%2520anchor%2520and%2520the%2520other%2520candidates.%2520Furthermore%252C%2520CA%2520prevents%2520the%250Aconsequential%2520model%2520collapse%252C%2520based%2520on%2520a%2520novel%2520metric%252C%2520collapseness%252C%2520which%2520is%250Aincorporated%2520into%2520the%2520optimization%2520of%2520perturbation.%2520We%2520also%2520identify%2520two%250Adrawbacks%2520of%2520the%2520existing%2520robustness%2520metric%2520in%2520image%2520retrieval%2520and%2520propose%2520a%250Anew%2520metric%2520for%2520a%2520more%2520reasonable%2520robustness%2520evaluation.%2520Extensive%2520experiments%250Aon%2520three%2520datasets%2520demonstrate%2520that%2520CA-TRIDE%2520outperforms%2520existing%2520defense%250Amethods%2520in%2520both%2520conventional%2520and%2520new%2520metrics.%2520Codes%2520are%2520available%2520at%250Ahttps%253A//github.com/michaeltian108/CA-TRIDE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07364v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collapse-Aware%20Triplet%20Decoupling%20for%20Adversarially%20Robust%20Image%0A%20%20Retrieval&entry.906535625=Qiwei%20Tian%20and%20Chenhao%20Lin%20and%20Zhengyu%20Zhao%20and%20Qian%20Li%20and%20Chao%20Shen&entry.1292438233=%20%20Adversarial%20training%20has%20achieved%20substantial%20performance%20in%20defending%20image%0Aretrieval%20against%20adversarial%20examples.%20However%2C%20existing%20studies%20in%20deep%0Ametric%20learning%20%28DML%29%20still%20suffer%20from%20two%20major%20limitations%3A%20weak%20adversary%0Aand%20model%20collapse.%20In%20this%20paper%2C%20we%20address%20these%20two%20limitations%20by%0Aproposing%20Collapse-Aware%20TRIplet%20DEcoupling%20%28CA-TRIDE%29.%20Specifically%2C%20TRIDE%0Ayields%20a%20stronger%20adversary%20by%20spatially%20decoupling%20the%20perturbation%20targets%0Ainto%20the%20anchor%20and%20the%20other%20candidates.%20Furthermore%2C%20CA%20prevents%20the%0Aconsequential%20model%20collapse%2C%20based%20on%20a%20novel%20metric%2C%20collapseness%2C%20which%20is%0Aincorporated%20into%20the%20optimization%20of%20perturbation.%20We%20also%20identify%20two%0Adrawbacks%20of%20the%20existing%20robustness%20metric%20in%20image%20retrieval%20and%20propose%20a%0Anew%20metric%20for%20a%20more%20reasonable%20robustness%20evaluation.%20Extensive%20experiments%0Aon%20three%20datasets%20demonstrate%20that%20CA-TRIDE%20outperforms%20existing%20defense%0Amethods%20in%20both%20conventional%20and%20new%20metrics.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/michaeltian108/CA-TRIDE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07364v4&entry.124074799=Read"},
{"title": "Vista: A Generalizable Driving World Model with High Fidelity and\n  Versatile Controllability", "author": "Shenyuan Gao and Jiazhi Yang and Li Chen and Kashyap Chitta and Yihang Qiu and Andreas Geiger and Jun Zhang and Hongyang Li", "abstract": "  World models can foresee the outcomes of different actions, which is of\nparamount importance for autonomous driving. Nevertheless, existing driving\nworld models still have limitations in generalization to unseen environments,\nprediction fidelity of critical details, and action controllability for\nflexible application. In this paper, we present Vista, a generalizable driving\nworld model with high fidelity and versatile controllability. Based on a\nsystematic diagnosis of existing methods, we introduce several key ingredients\nto address these limitations. To accurately predict real-world dynamics at high\nresolution, we propose two novel losses to promote the learning of moving\ninstances and structural information. We also devise an effective latent\nreplacement approach to inject historical frames as priors for coherent\nlong-horizon rollouts. For action controllability, we incorporate a versatile\nset of controls from high-level intentions (command, goal point) to low-level\nmaneuvers (trajectory, angle, and speed) through an efficient learning\nstrategy. After large-scale training, the capabilities of Vista can seamlessly\ngeneralize to different scenarios. Extensive experiments on multiple datasets\nshow that Vista outperforms the most advanced general-purpose video generator\nin over 70% of comparisons and surpasses the best-performing driving world\nmodel by 55% in FID and 27% in FVD. Moreover, for the first time, we utilize\nthe capacity of Vista itself to establish a generalizable reward for real-world\naction evaluation without accessing the ground truth actions.\n", "link": "http://arxiv.org/abs/2405.17398v2", "date": "2024-06-06", "relevancy": 2.1899, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5508}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5477}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5386}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vista%3A%20A%20Generalizable%20Driving%20World%20Model%20with%20High%20Fidelity%20and%0A%20%20Versatile%20Controllability&body=Title%3A%20Vista%3A%20A%20Generalizable%20Driving%20World%20Model%20with%20High%20Fidelity%20and%0A%20%20Versatile%20Controllability%0AAuthor%3A%20Shenyuan%20Gao%20and%20Jiazhi%20Yang%20and%20Li%20Chen%20and%20Kashyap%20Chitta%20and%20Yihang%20Qiu%20and%20Andreas%20Geiger%20and%20Jun%20Zhang%20and%20Hongyang%20Li%0AAbstract%3A%20%20%20World%20models%20can%20foresee%20the%20outcomes%20of%20different%20actions%2C%20which%20is%20of%0Aparamount%20importance%20for%20autonomous%20driving.%20Nevertheless%2C%20existing%20driving%0Aworld%20models%20still%20have%20limitations%20in%20generalization%20to%20unseen%20environments%2C%0Aprediction%20fidelity%20of%20critical%20details%2C%20and%20action%20controllability%20for%0Aflexible%20application.%20In%20this%20paper%2C%20we%20present%20Vista%2C%20a%20generalizable%20driving%0Aworld%20model%20with%20high%20fidelity%20and%20versatile%20controllability.%20Based%20on%20a%0Asystematic%20diagnosis%20of%20existing%20methods%2C%20we%20introduce%20several%20key%20ingredients%0Ato%20address%20these%20limitations.%20To%20accurately%20predict%20real-world%20dynamics%20at%20high%0Aresolution%2C%20we%20propose%20two%20novel%20losses%20to%20promote%20the%20learning%20of%20moving%0Ainstances%20and%20structural%20information.%20We%20also%20devise%20an%20effective%20latent%0Areplacement%20approach%20to%20inject%20historical%20frames%20as%20priors%20for%20coherent%0Along-horizon%20rollouts.%20For%20action%20controllability%2C%20we%20incorporate%20a%20versatile%0Aset%20of%20controls%20from%20high-level%20intentions%20%28command%2C%20goal%20point%29%20to%20low-level%0Amaneuvers%20%28trajectory%2C%20angle%2C%20and%20speed%29%20through%20an%20efficient%20learning%0Astrategy.%20After%20large-scale%20training%2C%20the%20capabilities%20of%20Vista%20can%20seamlessly%0Ageneralize%20to%20different%20scenarios.%20Extensive%20experiments%20on%20multiple%20datasets%0Ashow%20that%20Vista%20outperforms%20the%20most%20advanced%20general-purpose%20video%20generator%0Ain%20over%2070%25%20of%20comparisons%20and%20surpasses%20the%20best-performing%20driving%20world%0Amodel%20by%2055%25%20in%20FID%20and%2027%25%20in%20FVD.%20Moreover%2C%20for%20the%20first%20time%2C%20we%20utilize%0Athe%20capacity%20of%20Vista%20itself%20to%20establish%20a%20generalizable%20reward%20for%20real-world%0Aaction%20evaluation%20without%20accessing%20the%20ground%20truth%20actions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17398v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVista%253A%2520A%2520Generalizable%2520Driving%2520World%2520Model%2520with%2520High%2520Fidelity%2520and%250A%2520%2520Versatile%2520Controllability%26entry.906535625%3DShenyuan%2520Gao%2520and%2520Jiazhi%2520Yang%2520and%2520Li%2520Chen%2520and%2520Kashyap%2520Chitta%2520and%2520Yihang%2520Qiu%2520and%2520Andreas%2520Geiger%2520and%2520Jun%2520Zhang%2520and%2520Hongyang%2520Li%26entry.1292438233%3D%2520%2520World%2520models%2520can%2520foresee%2520the%2520outcomes%2520of%2520different%2520actions%252C%2520which%2520is%2520of%250Aparamount%2520importance%2520for%2520autonomous%2520driving.%2520Nevertheless%252C%2520existing%2520driving%250Aworld%2520models%2520still%2520have%2520limitations%2520in%2520generalization%2520to%2520unseen%2520environments%252C%250Aprediction%2520fidelity%2520of%2520critical%2520details%252C%2520and%2520action%2520controllability%2520for%250Aflexible%2520application.%2520In%2520this%2520paper%252C%2520we%2520present%2520Vista%252C%2520a%2520generalizable%2520driving%250Aworld%2520model%2520with%2520high%2520fidelity%2520and%2520versatile%2520controllability.%2520Based%2520on%2520a%250Asystematic%2520diagnosis%2520of%2520existing%2520methods%252C%2520we%2520introduce%2520several%2520key%2520ingredients%250Ato%2520address%2520these%2520limitations.%2520To%2520accurately%2520predict%2520real-world%2520dynamics%2520at%2520high%250Aresolution%252C%2520we%2520propose%2520two%2520novel%2520losses%2520to%2520promote%2520the%2520learning%2520of%2520moving%250Ainstances%2520and%2520structural%2520information.%2520We%2520also%2520devise%2520an%2520effective%2520latent%250Areplacement%2520approach%2520to%2520inject%2520historical%2520frames%2520as%2520priors%2520for%2520coherent%250Along-horizon%2520rollouts.%2520For%2520action%2520controllability%252C%2520we%2520incorporate%2520a%2520versatile%250Aset%2520of%2520controls%2520from%2520high-level%2520intentions%2520%2528command%252C%2520goal%2520point%2529%2520to%2520low-level%250Amaneuvers%2520%2528trajectory%252C%2520angle%252C%2520and%2520speed%2529%2520through%2520an%2520efficient%2520learning%250Astrategy.%2520After%2520large-scale%2520training%252C%2520the%2520capabilities%2520of%2520Vista%2520can%2520seamlessly%250Ageneralize%2520to%2520different%2520scenarios.%2520Extensive%2520experiments%2520on%2520multiple%2520datasets%250Ashow%2520that%2520Vista%2520outperforms%2520the%2520most%2520advanced%2520general-purpose%2520video%2520generator%250Ain%2520over%252070%2525%2520of%2520comparisons%2520and%2520surpasses%2520the%2520best-performing%2520driving%2520world%250Amodel%2520by%252055%2525%2520in%2520FID%2520and%252027%2525%2520in%2520FVD.%2520Moreover%252C%2520for%2520the%2520first%2520time%252C%2520we%2520utilize%250Athe%2520capacity%2520of%2520Vista%2520itself%2520to%2520establish%2520a%2520generalizable%2520reward%2520for%2520real-world%250Aaction%2520evaluation%2520without%2520accessing%2520the%2520ground%2520truth%2520actions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17398v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vista%3A%20A%20Generalizable%20Driving%20World%20Model%20with%20High%20Fidelity%20and%0A%20%20Versatile%20Controllability&entry.906535625=Shenyuan%20Gao%20and%20Jiazhi%20Yang%20and%20Li%20Chen%20and%20Kashyap%20Chitta%20and%20Yihang%20Qiu%20and%20Andreas%20Geiger%20and%20Jun%20Zhang%20and%20Hongyang%20Li&entry.1292438233=%20%20World%20models%20can%20foresee%20the%20outcomes%20of%20different%20actions%2C%20which%20is%20of%0Aparamount%20importance%20for%20autonomous%20driving.%20Nevertheless%2C%20existing%20driving%0Aworld%20models%20still%20have%20limitations%20in%20generalization%20to%20unseen%20environments%2C%0Aprediction%20fidelity%20of%20critical%20details%2C%20and%20action%20controllability%20for%0Aflexible%20application.%20In%20this%20paper%2C%20we%20present%20Vista%2C%20a%20generalizable%20driving%0Aworld%20model%20with%20high%20fidelity%20and%20versatile%20controllability.%20Based%20on%20a%0Asystematic%20diagnosis%20of%20existing%20methods%2C%20we%20introduce%20several%20key%20ingredients%0Ato%20address%20these%20limitations.%20To%20accurately%20predict%20real-world%20dynamics%20at%20high%0Aresolution%2C%20we%20propose%20two%20novel%20losses%20to%20promote%20the%20learning%20of%20moving%0Ainstances%20and%20structural%20information.%20We%20also%20devise%20an%20effective%20latent%0Areplacement%20approach%20to%20inject%20historical%20frames%20as%20priors%20for%20coherent%0Along-horizon%20rollouts.%20For%20action%20controllability%2C%20we%20incorporate%20a%20versatile%0Aset%20of%20controls%20from%20high-level%20intentions%20%28command%2C%20goal%20point%29%20to%20low-level%0Amaneuvers%20%28trajectory%2C%20angle%2C%20and%20speed%29%20through%20an%20efficient%20learning%0Astrategy.%20After%20large-scale%20training%2C%20the%20capabilities%20of%20Vista%20can%20seamlessly%0Ageneralize%20to%20different%20scenarios.%20Extensive%20experiments%20on%20multiple%20datasets%0Ashow%20that%20Vista%20outperforms%20the%20most%20advanced%20general-purpose%20video%20generator%0Ain%20over%2070%25%20of%20comparisons%20and%20surpasses%20the%20best-performing%20driving%20world%0Amodel%20by%2055%25%20in%20FID%20and%2027%25%20in%20FVD.%20Moreover%2C%20for%20the%20first%20time%2C%20we%20utilize%0Athe%20capacity%20of%20Vista%20itself%20to%20establish%20a%20generalizable%20reward%20for%20real-world%0Aaction%20evaluation%20without%20accessing%20the%20ground%20truth%20actions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17398v2&entry.124074799=Read"},
{"title": "ShareGPT4Video: Improving Video Understanding and Generation with Better\n  Captions", "author": "Lin Chen and Xilin Wei and Jinsong Li and Xiaoyi Dong and Pan Zhang and Yuhang Zang and Zehui Chen and Haodong Duan and Bin Lin and Zhenyu Tang and Li Yuan and Yu Qiao and Dahua Lin and Feng Zhao and Jiaqi Wang", "abstract": "  We present the ShareGPT4Video series, aiming to facilitate the video\nunderstanding of large video-language models (LVLMs) and the video generation\nof text-to-video models (T2VMs) via dense and precise captions. The series\ncomprises: 1) ShareGPT4Video, 40K GPT4V annotated dense captions of videos with\nvarious lengths and sources, developed through carefully designed data\nfiltering and annotating strategy. 2) ShareCaptioner-Video, an efficient and\ncapable captioning model for arbitrary videos, with 4.8M high-quality aesthetic\nvideos annotated by it. 3) ShareGPT4Video-8B, a simple yet superb LVLM that\nreached SOTA performance on three advancing video benchmarks. To achieve this,\ntaking aside the non-scalable costly human annotators, we find using GPT4V to\ncaption video with a naive multi-frame or frame-concatenation input strategy\nleads to less detailed and sometimes temporal-confused results. We argue the\nchallenge of designing a high-quality video captioning strategy lies in three\naspects: 1) Inter-frame precise temporal change understanding. 2) Intra-frame\ndetailed content description. 3) Frame-number scalability for arbitrary-length\nvideos. To this end, we meticulously designed a differential video captioning\nstrategy, which is stable, scalable, and efficient for generating captions for\nvideos with arbitrary resolution, aspect ratios, and length. Based on it, we\nconstruct ShareGPT4Video, which contains 40K high-quality videos spanning a\nwide range of categories, and the resulting captions encompass rich world\nknowledge, object attributes, camera movements, and crucially, detailed and\nprecise temporal descriptions of events. Based on ShareGPT4Video, we further\ndevelop ShareCaptioner-Video, a superior captioner capable of efficiently\ngenerating high-quality captions for arbitrary videos...\n", "link": "http://arxiv.org/abs/2406.04325v1", "date": "2024-06-06", "relevancy": 2.189, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5736}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5678}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShareGPT4Video%3A%20Improving%20Video%20Understanding%20and%20Generation%20with%20Better%0A%20%20Captions&body=Title%3A%20ShareGPT4Video%3A%20Improving%20Video%20Understanding%20and%20Generation%20with%20Better%0A%20%20Captions%0AAuthor%3A%20Lin%20Chen%20and%20Xilin%20Wei%20and%20Jinsong%20Li%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Yuhang%20Zang%20and%20Zehui%20Chen%20and%20Haodong%20Duan%20and%20Bin%20Lin%20and%20Zhenyu%20Tang%20and%20Li%20Yuan%20and%20Yu%20Qiao%20and%20Dahua%20Lin%20and%20Feng%20Zhao%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20We%20present%20the%20ShareGPT4Video%20series%2C%20aiming%20to%20facilitate%20the%20video%0Aunderstanding%20of%20large%20video-language%20models%20%28LVLMs%29%20and%20the%20video%20generation%0Aof%20text-to-video%20models%20%28T2VMs%29%20via%20dense%20and%20precise%20captions.%20The%20series%0Acomprises%3A%201%29%20ShareGPT4Video%2C%2040K%20GPT4V%20annotated%20dense%20captions%20of%20videos%20with%0Avarious%20lengths%20and%20sources%2C%20developed%20through%20carefully%20designed%20data%0Afiltering%20and%20annotating%20strategy.%202%29%20ShareCaptioner-Video%2C%20an%20efficient%20and%0Acapable%20captioning%20model%20for%20arbitrary%20videos%2C%20with%204.8M%20high-quality%20aesthetic%0Avideos%20annotated%20by%20it.%203%29%20ShareGPT4Video-8B%2C%20a%20simple%20yet%20superb%20LVLM%20that%0Areached%20SOTA%20performance%20on%20three%20advancing%20video%20benchmarks.%20To%20achieve%20this%2C%0Ataking%20aside%20the%20non-scalable%20costly%20human%20annotators%2C%20we%20find%20using%20GPT4V%20to%0Acaption%20video%20with%20a%20naive%20multi-frame%20or%20frame-concatenation%20input%20strategy%0Aleads%20to%20less%20detailed%20and%20sometimes%20temporal-confused%20results.%20We%20argue%20the%0Achallenge%20of%20designing%20a%20high-quality%20video%20captioning%20strategy%20lies%20in%20three%0Aaspects%3A%201%29%20Inter-frame%20precise%20temporal%20change%20understanding.%202%29%20Intra-frame%0Adetailed%20content%20description.%203%29%20Frame-number%20scalability%20for%20arbitrary-length%0Avideos.%20To%20this%20end%2C%20we%20meticulously%20designed%20a%20differential%20video%20captioning%0Astrategy%2C%20which%20is%20stable%2C%20scalable%2C%20and%20efficient%20for%20generating%20captions%20for%0Avideos%20with%20arbitrary%20resolution%2C%20aspect%20ratios%2C%20and%20length.%20Based%20on%20it%2C%20we%0Aconstruct%20ShareGPT4Video%2C%20which%20contains%2040K%20high-quality%20videos%20spanning%20a%0Awide%20range%20of%20categories%2C%20and%20the%20resulting%20captions%20encompass%20rich%20world%0Aknowledge%2C%20object%20attributes%2C%20camera%20movements%2C%20and%20crucially%2C%20detailed%20and%0Aprecise%20temporal%20descriptions%20of%20events.%20Based%20on%20ShareGPT4Video%2C%20we%20further%0Adevelop%20ShareCaptioner-Video%2C%20a%20superior%20captioner%20capable%20of%20efficiently%0Agenerating%20high-quality%20captions%20for%20arbitrary%20videos...%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04325v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShareGPT4Video%253A%2520Improving%2520Video%2520Understanding%2520and%2520Generation%2520with%2520Better%250A%2520%2520Captions%26entry.906535625%3DLin%2520Chen%2520and%2520Xilin%2520Wei%2520and%2520Jinsong%2520Li%2520and%2520Xiaoyi%2520Dong%2520and%2520Pan%2520Zhang%2520and%2520Yuhang%2520Zang%2520and%2520Zehui%2520Chen%2520and%2520Haodong%2520Duan%2520and%2520Bin%2520Lin%2520and%2520Zhenyu%2520Tang%2520and%2520Li%2520Yuan%2520and%2520Yu%2520Qiao%2520and%2520Dahua%2520Lin%2520and%2520Feng%2520Zhao%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520ShareGPT4Video%2520series%252C%2520aiming%2520to%2520facilitate%2520the%2520video%250Aunderstanding%2520of%2520large%2520video-language%2520models%2520%2528LVLMs%2529%2520and%2520the%2520video%2520generation%250Aof%2520text-to-video%2520models%2520%2528T2VMs%2529%2520via%2520dense%2520and%2520precise%2520captions.%2520The%2520series%250Acomprises%253A%25201%2529%2520ShareGPT4Video%252C%252040K%2520GPT4V%2520annotated%2520dense%2520captions%2520of%2520videos%2520with%250Avarious%2520lengths%2520and%2520sources%252C%2520developed%2520through%2520carefully%2520designed%2520data%250Afiltering%2520and%2520annotating%2520strategy.%25202%2529%2520ShareCaptioner-Video%252C%2520an%2520efficient%2520and%250Acapable%2520captioning%2520model%2520for%2520arbitrary%2520videos%252C%2520with%25204.8M%2520high-quality%2520aesthetic%250Avideos%2520annotated%2520by%2520it.%25203%2529%2520ShareGPT4Video-8B%252C%2520a%2520simple%2520yet%2520superb%2520LVLM%2520that%250Areached%2520SOTA%2520performance%2520on%2520three%2520advancing%2520video%2520benchmarks.%2520To%2520achieve%2520this%252C%250Ataking%2520aside%2520the%2520non-scalable%2520costly%2520human%2520annotators%252C%2520we%2520find%2520using%2520GPT4V%2520to%250Acaption%2520video%2520with%2520a%2520naive%2520multi-frame%2520or%2520frame-concatenation%2520input%2520strategy%250Aleads%2520to%2520less%2520detailed%2520and%2520sometimes%2520temporal-confused%2520results.%2520We%2520argue%2520the%250Achallenge%2520of%2520designing%2520a%2520high-quality%2520video%2520captioning%2520strategy%2520lies%2520in%2520three%250Aaspects%253A%25201%2529%2520Inter-frame%2520precise%2520temporal%2520change%2520understanding.%25202%2529%2520Intra-frame%250Adetailed%2520content%2520description.%25203%2529%2520Frame-number%2520scalability%2520for%2520arbitrary-length%250Avideos.%2520To%2520this%2520end%252C%2520we%2520meticulously%2520designed%2520a%2520differential%2520video%2520captioning%250Astrategy%252C%2520which%2520is%2520stable%252C%2520scalable%252C%2520and%2520efficient%2520for%2520generating%2520captions%2520for%250Avideos%2520with%2520arbitrary%2520resolution%252C%2520aspect%2520ratios%252C%2520and%2520length.%2520Based%2520on%2520it%252C%2520we%250Aconstruct%2520ShareGPT4Video%252C%2520which%2520contains%252040K%2520high-quality%2520videos%2520spanning%2520a%250Awide%2520range%2520of%2520categories%252C%2520and%2520the%2520resulting%2520captions%2520encompass%2520rich%2520world%250Aknowledge%252C%2520object%2520attributes%252C%2520camera%2520movements%252C%2520and%2520crucially%252C%2520detailed%2520and%250Aprecise%2520temporal%2520descriptions%2520of%2520events.%2520Based%2520on%2520ShareGPT4Video%252C%2520we%2520further%250Adevelop%2520ShareCaptioner-Video%252C%2520a%2520superior%2520captioner%2520capable%2520of%2520efficiently%250Agenerating%2520high-quality%2520captions%2520for%2520arbitrary%2520videos...%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04325v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShareGPT4Video%3A%20Improving%20Video%20Understanding%20and%20Generation%20with%20Better%0A%20%20Captions&entry.906535625=Lin%20Chen%20and%20Xilin%20Wei%20and%20Jinsong%20Li%20and%20Xiaoyi%20Dong%20and%20Pan%20Zhang%20and%20Yuhang%20Zang%20and%20Zehui%20Chen%20and%20Haodong%20Duan%20and%20Bin%20Lin%20and%20Zhenyu%20Tang%20and%20Li%20Yuan%20and%20Yu%20Qiao%20and%20Dahua%20Lin%20and%20Feng%20Zhao%20and%20Jiaqi%20Wang&entry.1292438233=%20%20We%20present%20the%20ShareGPT4Video%20series%2C%20aiming%20to%20facilitate%20the%20video%0Aunderstanding%20of%20large%20video-language%20models%20%28LVLMs%29%20and%20the%20video%20generation%0Aof%20text-to-video%20models%20%28T2VMs%29%20via%20dense%20and%20precise%20captions.%20The%20series%0Acomprises%3A%201%29%20ShareGPT4Video%2C%2040K%20GPT4V%20annotated%20dense%20captions%20of%20videos%20with%0Avarious%20lengths%20and%20sources%2C%20developed%20through%20carefully%20designed%20data%0Afiltering%20and%20annotating%20strategy.%202%29%20ShareCaptioner-Video%2C%20an%20efficient%20and%0Acapable%20captioning%20model%20for%20arbitrary%20videos%2C%20with%204.8M%20high-quality%20aesthetic%0Avideos%20annotated%20by%20it.%203%29%20ShareGPT4Video-8B%2C%20a%20simple%20yet%20superb%20LVLM%20that%0Areached%20SOTA%20performance%20on%20three%20advancing%20video%20benchmarks.%20To%20achieve%20this%2C%0Ataking%20aside%20the%20non-scalable%20costly%20human%20annotators%2C%20we%20find%20using%20GPT4V%20to%0Acaption%20video%20with%20a%20naive%20multi-frame%20or%20frame-concatenation%20input%20strategy%0Aleads%20to%20less%20detailed%20and%20sometimes%20temporal-confused%20results.%20We%20argue%20the%0Achallenge%20of%20designing%20a%20high-quality%20video%20captioning%20strategy%20lies%20in%20three%0Aaspects%3A%201%29%20Inter-frame%20precise%20temporal%20change%20understanding.%202%29%20Intra-frame%0Adetailed%20content%20description.%203%29%20Frame-number%20scalability%20for%20arbitrary-length%0Avideos.%20To%20this%20end%2C%20we%20meticulously%20designed%20a%20differential%20video%20captioning%0Astrategy%2C%20which%20is%20stable%2C%20scalable%2C%20and%20efficient%20for%20generating%20captions%20for%0Avideos%20with%20arbitrary%20resolution%2C%20aspect%20ratios%2C%20and%20length.%20Based%20on%20it%2C%20we%0Aconstruct%20ShareGPT4Video%2C%20which%20contains%2040K%20high-quality%20videos%20spanning%20a%0Awide%20range%20of%20categories%2C%20and%20the%20resulting%20captions%20encompass%20rich%20world%0Aknowledge%2C%20object%20attributes%2C%20camera%20movements%2C%20and%20crucially%2C%20detailed%20and%0Aprecise%20temporal%20descriptions%20of%20events.%20Based%20on%20ShareGPT4Video%2C%20we%20further%0Adevelop%20ShareCaptioner-Video%2C%20a%20superior%20captioner%20capable%20of%20efficiently%0Agenerating%20high-quality%20captions%20for%20arbitrary%20videos...%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04325v1&entry.124074799=Read"},
{"title": "Essentially Sharp Estimates on the Entropy Regularization Error in\n  Discrete Discounted Markov Decision Processes", "author": "Johannes M\u00fcller and Semih Cayci", "abstract": "  We study the error introduced by entropy regularization of infinite-horizon\ndiscrete discounted Markov decision processes. We show that this error\ndecreases exponentially in the inverse regularization strength both in a\nweighted KL-divergence and in value with a problem-specific exponent. We\nprovide a lower bound matching our upper bound up to a polynomial factor. Our\nproof relies on the correspondence of the solutions of entropy-regularized\nMarkov decision processes with gradient flows of the unregularized reward with\nrespect to a Riemannian metric common in natural policy gradient methods.\nFurther, this correspondence allows us to identify the limit of the gradient\nflow as the generalized maximum entropy optimal policy, thereby characterizing\nthe implicit bias of the Kakade gradient flow which corresponds to a\ntime-continuous version of the natural policy gradient method. We use this to\nshow that for entropy-regularized natural policy gradient methods the overall\nerror decays exponentially in the square root of the number of iterations\nimproving existing sublinear guarantees.\n", "link": "http://arxiv.org/abs/2406.04163v1", "date": "2024-06-06", "relevancy": 2.1872, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4387}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.437}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Essentially%20Sharp%20Estimates%20on%20the%20Entropy%20Regularization%20Error%20in%0A%20%20Discrete%20Discounted%20Markov%20Decision%20Processes&body=Title%3A%20Essentially%20Sharp%20Estimates%20on%20the%20Entropy%20Regularization%20Error%20in%0A%20%20Discrete%20Discounted%20Markov%20Decision%20Processes%0AAuthor%3A%20Johannes%20M%C3%BCller%20and%20Semih%20Cayci%0AAbstract%3A%20%20%20We%20study%20the%20error%20introduced%20by%20entropy%20regularization%20of%20infinite-horizon%0Adiscrete%20discounted%20Markov%20decision%20processes.%20We%20show%20that%20this%20error%0Adecreases%20exponentially%20in%20the%20inverse%20regularization%20strength%20both%20in%20a%0Aweighted%20KL-divergence%20and%20in%20value%20with%20a%20problem-specific%20exponent.%20We%0Aprovide%20a%20lower%20bound%20matching%20our%20upper%20bound%20up%20to%20a%20polynomial%20factor.%20Our%0Aproof%20relies%20on%20the%20correspondence%20of%20the%20solutions%20of%20entropy-regularized%0AMarkov%20decision%20processes%20with%20gradient%20flows%20of%20the%20unregularized%20reward%20with%0Arespect%20to%20a%20Riemannian%20metric%20common%20in%20natural%20policy%20gradient%20methods.%0AFurther%2C%20this%20correspondence%20allows%20us%20to%20identify%20the%20limit%20of%20the%20gradient%0Aflow%20as%20the%20generalized%20maximum%20entropy%20optimal%20policy%2C%20thereby%20characterizing%0Athe%20implicit%20bias%20of%20the%20Kakade%20gradient%20flow%20which%20corresponds%20to%20a%0Atime-continuous%20version%20of%20the%20natural%20policy%20gradient%20method.%20We%20use%20this%20to%0Ashow%20that%20for%20entropy-regularized%20natural%20policy%20gradient%20methods%20the%20overall%0Aerror%20decays%20exponentially%20in%20the%20square%20root%20of%20the%20number%20of%20iterations%0Aimproving%20existing%20sublinear%20guarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04163v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEssentially%2520Sharp%2520Estimates%2520on%2520the%2520Entropy%2520Regularization%2520Error%2520in%250A%2520%2520Discrete%2520Discounted%2520Markov%2520Decision%2520Processes%26entry.906535625%3DJohannes%2520M%25C3%25BCller%2520and%2520Semih%2520Cayci%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520error%2520introduced%2520by%2520entropy%2520regularization%2520of%2520infinite-horizon%250Adiscrete%2520discounted%2520Markov%2520decision%2520processes.%2520We%2520show%2520that%2520this%2520error%250Adecreases%2520exponentially%2520in%2520the%2520inverse%2520regularization%2520strength%2520both%2520in%2520a%250Aweighted%2520KL-divergence%2520and%2520in%2520value%2520with%2520a%2520problem-specific%2520exponent.%2520We%250Aprovide%2520a%2520lower%2520bound%2520matching%2520our%2520upper%2520bound%2520up%2520to%2520a%2520polynomial%2520factor.%2520Our%250Aproof%2520relies%2520on%2520the%2520correspondence%2520of%2520the%2520solutions%2520of%2520entropy-regularized%250AMarkov%2520decision%2520processes%2520with%2520gradient%2520flows%2520of%2520the%2520unregularized%2520reward%2520with%250Arespect%2520to%2520a%2520Riemannian%2520metric%2520common%2520in%2520natural%2520policy%2520gradient%2520methods.%250AFurther%252C%2520this%2520correspondence%2520allows%2520us%2520to%2520identify%2520the%2520limit%2520of%2520the%2520gradient%250Aflow%2520as%2520the%2520generalized%2520maximum%2520entropy%2520optimal%2520policy%252C%2520thereby%2520characterizing%250Athe%2520implicit%2520bias%2520of%2520the%2520Kakade%2520gradient%2520flow%2520which%2520corresponds%2520to%2520a%250Atime-continuous%2520version%2520of%2520the%2520natural%2520policy%2520gradient%2520method.%2520We%2520use%2520this%2520to%250Ashow%2520that%2520for%2520entropy-regularized%2520natural%2520policy%2520gradient%2520methods%2520the%2520overall%250Aerror%2520decays%2520exponentially%2520in%2520the%2520square%2520root%2520of%2520the%2520number%2520of%2520iterations%250Aimproving%2520existing%2520sublinear%2520guarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04163v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Essentially%20Sharp%20Estimates%20on%20the%20Entropy%20Regularization%20Error%20in%0A%20%20Discrete%20Discounted%20Markov%20Decision%20Processes&entry.906535625=Johannes%20M%C3%BCller%20and%20Semih%20Cayci&entry.1292438233=%20%20We%20study%20the%20error%20introduced%20by%20entropy%20regularization%20of%20infinite-horizon%0Adiscrete%20discounted%20Markov%20decision%20processes.%20We%20show%20that%20this%20error%0Adecreases%20exponentially%20in%20the%20inverse%20regularization%20strength%20both%20in%20a%0Aweighted%20KL-divergence%20and%20in%20value%20with%20a%20problem-specific%20exponent.%20We%0Aprovide%20a%20lower%20bound%20matching%20our%20upper%20bound%20up%20to%20a%20polynomial%20factor.%20Our%0Aproof%20relies%20on%20the%20correspondence%20of%20the%20solutions%20of%20entropy-regularized%0AMarkov%20decision%20processes%20with%20gradient%20flows%20of%20the%20unregularized%20reward%20with%0Arespect%20to%20a%20Riemannian%20metric%20common%20in%20natural%20policy%20gradient%20methods.%0AFurther%2C%20this%20correspondence%20allows%20us%20to%20identify%20the%20limit%20of%20the%20gradient%0Aflow%20as%20the%20generalized%20maximum%20entropy%20optimal%20policy%2C%20thereby%20characterizing%0Athe%20implicit%20bias%20of%20the%20Kakade%20gradient%20flow%20which%20corresponds%20to%20a%0Atime-continuous%20version%20of%20the%20natural%20policy%20gradient%20method.%20We%20use%20this%20to%0Ashow%20that%20for%20entropy-regularized%20natural%20policy%20gradient%20methods%20the%20overall%0Aerror%20decays%20exponentially%20in%20the%20square%20root%20of%20the%20number%20of%20iterations%0Aimproving%20existing%20sublinear%20guarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04163v1&entry.124074799=Read"},
{"title": "Step-aware Preference Optimization: Aligning Preference with Denoising\n  Performance at Each Step", "author": "Zhanhao Liang and Yuhui Yuan and Shuyang Gu and Bohan Chen and Tiankai Hang and Ji Li and Liang Zheng", "abstract": "  Recently, Direct Preference Optimization (DPO) has extended its success from\naligning large language models (LLMs) to aligning text-to-image diffusion\nmodels with human preferences. Unlike most existing DPO methods that assume all\ndiffusion steps share a consistent preference order with the final generated\nimages, we argue that this assumption neglects step-specific denoising\nperformance and that preference labels should be tailored to each step's\ncontribution. To address this limitation, we propose Step-aware Preference\nOptimization (SPO), a novel post-training approach that independently evaluates\nand adjusts the denoising performance at each step, using a step-aware\npreference model and a step-wise resampler to ensure accurate step-aware\nsupervision. Specifically, at each denoising step, we sample a pool of images,\nfind a suitable win-lose pair, and, most importantly, randomly select a single\nimage from the pool to initialize the next denoising step. This step-wise\nresampler process ensures the next win-lose image pair comes from the same\nimage, making the win-lose comparison independent of the previous step. To\nassess the preferences at each step, we train a separate step-aware preference\nmodel that can be applied to both noisy and clean images. Our experiments with\nStable Diffusion v1.5 and SDXL demonstrate that SPO significantly outperforms\nthe latest Diffusion-DPO in aligning generated images with complex, detailed\nprompts and enhancing aesthetics, while also achieving more than 20x times\nfaster in training efficiency. Code and model:\nhttps://rockeycoss.github.io/spo.github.io/\n", "link": "http://arxiv.org/abs/2406.04314v1", "date": "2024-06-06", "relevancy": 2.1786, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5688}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5574}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Step-aware%20Preference%20Optimization%3A%20Aligning%20Preference%20with%20Denoising%0A%20%20Performance%20at%20Each%20Step&body=Title%3A%20Step-aware%20Preference%20Optimization%3A%20Aligning%20Preference%20with%20Denoising%0A%20%20Performance%20at%20Each%20Step%0AAuthor%3A%20Zhanhao%20Liang%20and%20Yuhui%20Yuan%20and%20Shuyang%20Gu%20and%20Bohan%20Chen%20and%20Tiankai%20Hang%20and%20Ji%20Li%20and%20Liang%20Zheng%0AAbstract%3A%20%20%20Recently%2C%20Direct%20Preference%20Optimization%20%28DPO%29%20has%20extended%20its%20success%20from%0Aaligning%20large%20language%20models%20%28LLMs%29%20to%20aligning%20text-to-image%20diffusion%0Amodels%20with%20human%20preferences.%20Unlike%20most%20existing%20DPO%20methods%20that%20assume%20all%0Adiffusion%20steps%20share%20a%20consistent%20preference%20order%20with%20the%20final%20generated%0Aimages%2C%20we%20argue%20that%20this%20assumption%20neglects%20step-specific%20denoising%0Aperformance%20and%20that%20preference%20labels%20should%20be%20tailored%20to%20each%20step%27s%0Acontribution.%20To%20address%20this%20limitation%2C%20we%20propose%20Step-aware%20Preference%0AOptimization%20%28SPO%29%2C%20a%20novel%20post-training%20approach%20that%20independently%20evaluates%0Aand%20adjusts%20the%20denoising%20performance%20at%20each%20step%2C%20using%20a%20step-aware%0Apreference%20model%20and%20a%20step-wise%20resampler%20to%20ensure%20accurate%20step-aware%0Asupervision.%20Specifically%2C%20at%20each%20denoising%20step%2C%20we%20sample%20a%20pool%20of%20images%2C%0Afind%20a%20suitable%20win-lose%20pair%2C%20and%2C%20most%20importantly%2C%20randomly%20select%20a%20single%0Aimage%20from%20the%20pool%20to%20initialize%20the%20next%20denoising%20step.%20This%20step-wise%0Aresampler%20process%20ensures%20the%20next%20win-lose%20image%20pair%20comes%20from%20the%20same%0Aimage%2C%20making%20the%20win-lose%20comparison%20independent%20of%20the%20previous%20step.%20To%0Aassess%20the%20preferences%20at%20each%20step%2C%20we%20train%20a%20separate%20step-aware%20preference%0Amodel%20that%20can%20be%20applied%20to%20both%20noisy%20and%20clean%20images.%20Our%20experiments%20with%0AStable%20Diffusion%20v1.5%20and%20SDXL%20demonstrate%20that%20SPO%20significantly%20outperforms%0Athe%20latest%20Diffusion-DPO%20in%20aligning%20generated%20images%20with%20complex%2C%20detailed%0Aprompts%20and%20enhancing%20aesthetics%2C%20while%20also%20achieving%20more%20than%2020x%20times%0Afaster%20in%20training%20efficiency.%20Code%20and%20model%3A%0Ahttps%3A//rockeycoss.github.io/spo.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStep-aware%2520Preference%2520Optimization%253A%2520Aligning%2520Preference%2520with%2520Denoising%250A%2520%2520Performance%2520at%2520Each%2520Step%26entry.906535625%3DZhanhao%2520Liang%2520and%2520Yuhui%2520Yuan%2520and%2520Shuyang%2520Gu%2520and%2520Bohan%2520Chen%2520and%2520Tiankai%2520Hang%2520and%2520Ji%2520Li%2520and%2520Liang%2520Zheng%26entry.1292438233%3D%2520%2520Recently%252C%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520has%2520extended%2520its%2520success%2520from%250Aaligning%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520aligning%2520text-to-image%2520diffusion%250Amodels%2520with%2520human%2520preferences.%2520Unlike%2520most%2520existing%2520DPO%2520methods%2520that%2520assume%2520all%250Adiffusion%2520steps%2520share%2520a%2520consistent%2520preference%2520order%2520with%2520the%2520final%2520generated%250Aimages%252C%2520we%2520argue%2520that%2520this%2520assumption%2520neglects%2520step-specific%2520denoising%250Aperformance%2520and%2520that%2520preference%2520labels%2520should%2520be%2520tailored%2520to%2520each%2520step%2527s%250Acontribution.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520Step-aware%2520Preference%250AOptimization%2520%2528SPO%2529%252C%2520a%2520novel%2520post-training%2520approach%2520that%2520independently%2520evaluates%250Aand%2520adjusts%2520the%2520denoising%2520performance%2520at%2520each%2520step%252C%2520using%2520a%2520step-aware%250Apreference%2520model%2520and%2520a%2520step-wise%2520resampler%2520to%2520ensure%2520accurate%2520step-aware%250Asupervision.%2520Specifically%252C%2520at%2520each%2520denoising%2520step%252C%2520we%2520sample%2520a%2520pool%2520of%2520images%252C%250Afind%2520a%2520suitable%2520win-lose%2520pair%252C%2520and%252C%2520most%2520importantly%252C%2520randomly%2520select%2520a%2520single%250Aimage%2520from%2520the%2520pool%2520to%2520initialize%2520the%2520next%2520denoising%2520step.%2520This%2520step-wise%250Aresampler%2520process%2520ensures%2520the%2520next%2520win-lose%2520image%2520pair%2520comes%2520from%2520the%2520same%250Aimage%252C%2520making%2520the%2520win-lose%2520comparison%2520independent%2520of%2520the%2520previous%2520step.%2520To%250Aassess%2520the%2520preferences%2520at%2520each%2520step%252C%2520we%2520train%2520a%2520separate%2520step-aware%2520preference%250Amodel%2520that%2520can%2520be%2520applied%2520to%2520both%2520noisy%2520and%2520clean%2520images.%2520Our%2520experiments%2520with%250AStable%2520Diffusion%2520v1.5%2520and%2520SDXL%2520demonstrate%2520that%2520SPO%2520significantly%2520outperforms%250Athe%2520latest%2520Diffusion-DPO%2520in%2520aligning%2520generated%2520images%2520with%2520complex%252C%2520detailed%250Aprompts%2520and%2520enhancing%2520aesthetics%252C%2520while%2520also%2520achieving%2520more%2520than%252020x%2520times%250Afaster%2520in%2520training%2520efficiency.%2520Code%2520and%2520model%253A%250Ahttps%253A//rockeycoss.github.io/spo.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Step-aware%20Preference%20Optimization%3A%20Aligning%20Preference%20with%20Denoising%0A%20%20Performance%20at%20Each%20Step&entry.906535625=Zhanhao%20Liang%20and%20Yuhui%20Yuan%20and%20Shuyang%20Gu%20and%20Bohan%20Chen%20and%20Tiankai%20Hang%20and%20Ji%20Li%20and%20Liang%20Zheng&entry.1292438233=%20%20Recently%2C%20Direct%20Preference%20Optimization%20%28DPO%29%20has%20extended%20its%20success%20from%0Aaligning%20large%20language%20models%20%28LLMs%29%20to%20aligning%20text-to-image%20diffusion%0Amodels%20with%20human%20preferences.%20Unlike%20most%20existing%20DPO%20methods%20that%20assume%20all%0Adiffusion%20steps%20share%20a%20consistent%20preference%20order%20with%20the%20final%20generated%0Aimages%2C%20we%20argue%20that%20this%20assumption%20neglects%20step-specific%20denoising%0Aperformance%20and%20that%20preference%20labels%20should%20be%20tailored%20to%20each%20step%27s%0Acontribution.%20To%20address%20this%20limitation%2C%20we%20propose%20Step-aware%20Preference%0AOptimization%20%28SPO%29%2C%20a%20novel%20post-training%20approach%20that%20independently%20evaluates%0Aand%20adjusts%20the%20denoising%20performance%20at%20each%20step%2C%20using%20a%20step-aware%0Apreference%20model%20and%20a%20step-wise%20resampler%20to%20ensure%20accurate%20step-aware%0Asupervision.%20Specifically%2C%20at%20each%20denoising%20step%2C%20we%20sample%20a%20pool%20of%20images%2C%0Afind%20a%20suitable%20win-lose%20pair%2C%20and%2C%20most%20importantly%2C%20randomly%20select%20a%20single%0Aimage%20from%20the%20pool%20to%20initialize%20the%20next%20denoising%20step.%20This%20step-wise%0Aresampler%20process%20ensures%20the%20next%20win-lose%20image%20pair%20comes%20from%20the%20same%0Aimage%2C%20making%20the%20win-lose%20comparison%20independent%20of%20the%20previous%20step.%20To%0Aassess%20the%20preferences%20at%20each%20step%2C%20we%20train%20a%20separate%20step-aware%20preference%0Amodel%20that%20can%20be%20applied%20to%20both%20noisy%20and%20clean%20images.%20Our%20experiments%20with%0AStable%20Diffusion%20v1.5%20and%20SDXL%20demonstrate%20that%20SPO%20significantly%20outperforms%0Athe%20latest%20Diffusion-DPO%20in%20aligning%20generated%20images%20with%20complex%2C%20detailed%0Aprompts%20and%20enhancing%20aesthetics%2C%20while%20also%20achieving%20more%20than%2020x%20times%0Afaster%20in%20training%20efficiency.%20Code%20and%20model%3A%0Ahttps%3A//rockeycoss.github.io/spo.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04314v1&entry.124074799=Read"},
{"title": "Verifiably Robust Conformal Prediction", "author": "Linus Jeary and Tom Kuipers and Mehran Hosseini and Nicola Paoletti", "abstract": "  Conformal Prediction (CP) is a popular uncertainty quantification method that\nprovides distribution-free, statistically valid prediction sets, assuming that\ntraining and test data are exchangeable. In such a case, CP's prediction sets\nare guaranteed to cover the (unknown) true test output with a user-specified\nprobability. Nevertheless, this guarantee is violated when the data is\nsubjected to adversarial attacks, which often result in a significant loss of\ncoverage. Recently, several approaches have been put forward to recover CP\nguarantees in this setting. These approaches leverage variations of randomised\nsmoothing to produce conservative sets which account for the effect of the\nadversarial perturbations. They are, however, limited in that they only support\n$\\ell^2$-bounded perturbations and classification tasks. This paper introduces\nVRCP (Verifiably Robust Conformal Prediction), a new framework that leverages\nrecent neural network verification methods to recover coverage guarantees under\nadversarial attacks. Our VRCP method is the first to support perturbations\nbounded by arbitrary norms including $\\ell^1$, $\\ell^2$, and $\\ell^\\infty$, as\nwell as regression tasks. We evaluate and compare our approach on image\nclassification tasks (CIFAR10, CIFAR100, and TinyImageNet) and regression tasks\nfor deep reinforcement learning environments. In every case, VRCP achieves\nabove nominal coverage and yields significantly more efficient and informative\nprediction regions than the SotA.\n", "link": "http://arxiv.org/abs/2405.18942v2", "date": "2024-06-06", "relevancy": 2.1685, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5506}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.546}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Verifiably%20Robust%20Conformal%20Prediction&body=Title%3A%20Verifiably%20Robust%20Conformal%20Prediction%0AAuthor%3A%20Linus%20Jeary%20and%20Tom%20Kuipers%20and%20Mehran%20Hosseini%20and%20Nicola%20Paoletti%0AAbstract%3A%20%20%20Conformal%20Prediction%20%28CP%29%20is%20a%20popular%20uncertainty%20quantification%20method%20that%0Aprovides%20distribution-free%2C%20statistically%20valid%20prediction%20sets%2C%20assuming%20that%0Atraining%20and%20test%20data%20are%20exchangeable.%20In%20such%20a%20case%2C%20CP%27s%20prediction%20sets%0Aare%20guaranteed%20to%20cover%20the%20%28unknown%29%20true%20test%20output%20with%20a%20user-specified%0Aprobability.%20Nevertheless%2C%20this%20guarantee%20is%20violated%20when%20the%20data%20is%0Asubjected%20to%20adversarial%20attacks%2C%20which%20often%20result%20in%20a%20significant%20loss%20of%0Acoverage.%20Recently%2C%20several%20approaches%20have%20been%20put%20forward%20to%20recover%20CP%0Aguarantees%20in%20this%20setting.%20These%20approaches%20leverage%20variations%20of%20randomised%0Asmoothing%20to%20produce%20conservative%20sets%20which%20account%20for%20the%20effect%20of%20the%0Aadversarial%20perturbations.%20They%20are%2C%20however%2C%20limited%20in%20that%20they%20only%20support%0A%24%5Cell%5E2%24-bounded%20perturbations%20and%20classification%20tasks.%20This%20paper%20introduces%0AVRCP%20%28Verifiably%20Robust%20Conformal%20Prediction%29%2C%20a%20new%20framework%20that%20leverages%0Arecent%20neural%20network%20verification%20methods%20to%20recover%20coverage%20guarantees%20under%0Aadversarial%20attacks.%20Our%20VRCP%20method%20is%20the%20first%20to%20support%20perturbations%0Abounded%20by%20arbitrary%20norms%20including%20%24%5Cell%5E1%24%2C%20%24%5Cell%5E2%24%2C%20and%20%24%5Cell%5E%5Cinfty%24%2C%20as%0Awell%20as%20regression%20tasks.%20We%20evaluate%20and%20compare%20our%20approach%20on%20image%0Aclassification%20tasks%20%28CIFAR10%2C%20CIFAR100%2C%20and%20TinyImageNet%29%20and%20regression%20tasks%0Afor%20deep%20reinforcement%20learning%20environments.%20In%20every%20case%2C%20VRCP%20achieves%0Aabove%20nominal%20coverage%20and%20yields%20significantly%20more%20efficient%20and%20informative%0Aprediction%20regions%20than%20the%20SotA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18942v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVerifiably%2520Robust%2520Conformal%2520Prediction%26entry.906535625%3DLinus%2520Jeary%2520and%2520Tom%2520Kuipers%2520and%2520Mehran%2520Hosseini%2520and%2520Nicola%2520Paoletti%26entry.1292438233%3D%2520%2520Conformal%2520Prediction%2520%2528CP%2529%2520is%2520a%2520popular%2520uncertainty%2520quantification%2520method%2520that%250Aprovides%2520distribution-free%252C%2520statistically%2520valid%2520prediction%2520sets%252C%2520assuming%2520that%250Atraining%2520and%2520test%2520data%2520are%2520exchangeable.%2520In%2520such%2520a%2520case%252C%2520CP%2527s%2520prediction%2520sets%250Aare%2520guaranteed%2520to%2520cover%2520the%2520%2528unknown%2529%2520true%2520test%2520output%2520with%2520a%2520user-specified%250Aprobability.%2520Nevertheless%252C%2520this%2520guarantee%2520is%2520violated%2520when%2520the%2520data%2520is%250Asubjected%2520to%2520adversarial%2520attacks%252C%2520which%2520often%2520result%2520in%2520a%2520significant%2520loss%2520of%250Acoverage.%2520Recently%252C%2520several%2520approaches%2520have%2520been%2520put%2520forward%2520to%2520recover%2520CP%250Aguarantees%2520in%2520this%2520setting.%2520These%2520approaches%2520leverage%2520variations%2520of%2520randomised%250Asmoothing%2520to%2520produce%2520conservative%2520sets%2520which%2520account%2520for%2520the%2520effect%2520of%2520the%250Aadversarial%2520perturbations.%2520They%2520are%252C%2520however%252C%2520limited%2520in%2520that%2520they%2520only%2520support%250A%2524%255Cell%255E2%2524-bounded%2520perturbations%2520and%2520classification%2520tasks.%2520This%2520paper%2520introduces%250AVRCP%2520%2528Verifiably%2520Robust%2520Conformal%2520Prediction%2529%252C%2520a%2520new%2520framework%2520that%2520leverages%250Arecent%2520neural%2520network%2520verification%2520methods%2520to%2520recover%2520coverage%2520guarantees%2520under%250Aadversarial%2520attacks.%2520Our%2520VRCP%2520method%2520is%2520the%2520first%2520to%2520support%2520perturbations%250Abounded%2520by%2520arbitrary%2520norms%2520including%2520%2524%255Cell%255E1%2524%252C%2520%2524%255Cell%255E2%2524%252C%2520and%2520%2524%255Cell%255E%255Cinfty%2524%252C%2520as%250Awell%2520as%2520regression%2520tasks.%2520We%2520evaluate%2520and%2520compare%2520our%2520approach%2520on%2520image%250Aclassification%2520tasks%2520%2528CIFAR10%252C%2520CIFAR100%252C%2520and%2520TinyImageNet%2529%2520and%2520regression%2520tasks%250Afor%2520deep%2520reinforcement%2520learning%2520environments.%2520In%2520every%2520case%252C%2520VRCP%2520achieves%250Aabove%2520nominal%2520coverage%2520and%2520yields%2520significantly%2520more%2520efficient%2520and%2520informative%250Aprediction%2520regions%2520than%2520the%2520SotA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18942v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Verifiably%20Robust%20Conformal%20Prediction&entry.906535625=Linus%20Jeary%20and%20Tom%20Kuipers%20and%20Mehran%20Hosseini%20and%20Nicola%20Paoletti&entry.1292438233=%20%20Conformal%20Prediction%20%28CP%29%20is%20a%20popular%20uncertainty%20quantification%20method%20that%0Aprovides%20distribution-free%2C%20statistically%20valid%20prediction%20sets%2C%20assuming%20that%0Atraining%20and%20test%20data%20are%20exchangeable.%20In%20such%20a%20case%2C%20CP%27s%20prediction%20sets%0Aare%20guaranteed%20to%20cover%20the%20%28unknown%29%20true%20test%20output%20with%20a%20user-specified%0Aprobability.%20Nevertheless%2C%20this%20guarantee%20is%20violated%20when%20the%20data%20is%0Asubjected%20to%20adversarial%20attacks%2C%20which%20often%20result%20in%20a%20significant%20loss%20of%0Acoverage.%20Recently%2C%20several%20approaches%20have%20been%20put%20forward%20to%20recover%20CP%0Aguarantees%20in%20this%20setting.%20These%20approaches%20leverage%20variations%20of%20randomised%0Asmoothing%20to%20produce%20conservative%20sets%20which%20account%20for%20the%20effect%20of%20the%0Aadversarial%20perturbations.%20They%20are%2C%20however%2C%20limited%20in%20that%20they%20only%20support%0A%24%5Cell%5E2%24-bounded%20perturbations%20and%20classification%20tasks.%20This%20paper%20introduces%0AVRCP%20%28Verifiably%20Robust%20Conformal%20Prediction%29%2C%20a%20new%20framework%20that%20leverages%0Arecent%20neural%20network%20verification%20methods%20to%20recover%20coverage%20guarantees%20under%0Aadversarial%20attacks.%20Our%20VRCP%20method%20is%20the%20first%20to%20support%20perturbations%0Abounded%20by%20arbitrary%20norms%20including%20%24%5Cell%5E1%24%2C%20%24%5Cell%5E2%24%2C%20and%20%24%5Cell%5E%5Cinfty%24%2C%20as%0Awell%20as%20regression%20tasks.%20We%20evaluate%20and%20compare%20our%20approach%20on%20image%0Aclassification%20tasks%20%28CIFAR10%2C%20CIFAR100%2C%20and%20TinyImageNet%29%20and%20regression%20tasks%0Afor%20deep%20reinforcement%20learning%20environments.%20In%20every%20case%2C%20VRCP%20achieves%0Aabove%20nominal%20coverage%20and%20yields%20significantly%20more%20efficient%20and%20informative%0Aprediction%20regions%20than%20the%20SotA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18942v2&entry.124074799=Read"},
{"title": "MARLander: A Local Path Planning for Drone Swarms using Multiagent Deep\n  Reinforcement Learning", "author": "Demetros Aschu and Robinroy Peter and Sausar Karaf and Aleksey Fedoseev and Dzmitry Tsetserukou", "abstract": "  Achieving safe and precise landings for a swarm of drones poses a significant\nchallenge, primarily attributed to conventional control and planning methods.\nThis paper presents the implementation of multi-agent deep reinforcement\nlearning (MADRL) techniques for the precise landing of a drone swarm at\nrelocated target locations. The system is trained in a realistic simulated\nenvironment with a maximum velocity of 3 m/s in training spaces of 4 x 4 x 4 m\nand deployed utilizing Crazyflie drones with a Vicon indoor localization\nsystem. The experimental results revealed that the proposed approach achieved a\nlanding accuracy of 2.26 cm on stationary and 3.93 cm on moving platforms\nsurpassing a baseline method used with a Proportional-integral-derivative (PID)\ncontroller with an Artificial Potential Field (APF). This research highlights\ndrone landing technologies that eliminate the need for analytical centralized\nsystems, potentially offering scalability and revolutionizing applications in\nlogistics, safety, and rescue missions.\n", "link": "http://arxiv.org/abs/2406.04159v1", "date": "2024-06-06", "relevancy": 2.1648, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5552}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5528}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MARLander%3A%20A%20Local%20Path%20Planning%20for%20Drone%20Swarms%20using%20Multiagent%20Deep%0A%20%20Reinforcement%20Learning&body=Title%3A%20MARLander%3A%20A%20Local%20Path%20Planning%20for%20Drone%20Swarms%20using%20Multiagent%20Deep%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Demetros%20Aschu%20and%20Robinroy%20Peter%20and%20Sausar%20Karaf%20and%20Aleksey%20Fedoseev%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20%20%20Achieving%20safe%20and%20precise%20landings%20for%20a%20swarm%20of%20drones%20poses%20a%20significant%0Achallenge%2C%20primarily%20attributed%20to%20conventional%20control%20and%20planning%20methods.%0AThis%20paper%20presents%20the%20implementation%20of%20multi-agent%20deep%20reinforcement%0Alearning%20%28MADRL%29%20techniques%20for%20the%20precise%20landing%20of%20a%20drone%20swarm%20at%0Arelocated%20target%20locations.%20The%20system%20is%20trained%20in%20a%20realistic%20simulated%0Aenvironment%20with%20a%20maximum%20velocity%20of%203%20m/s%20in%20training%20spaces%20of%204%20x%204%20x%204%20m%0Aand%20deployed%20utilizing%20Crazyflie%20drones%20with%20a%20Vicon%20indoor%20localization%0Asystem.%20The%20experimental%20results%20revealed%20that%20the%20proposed%20approach%20achieved%20a%0Alanding%20accuracy%20of%202.26%20cm%20on%20stationary%20and%203.93%20cm%20on%20moving%20platforms%0Asurpassing%20a%20baseline%20method%20used%20with%20a%20Proportional-integral-derivative%20%28PID%29%0Acontroller%20with%20an%20Artificial%20Potential%20Field%20%28APF%29.%20This%20research%20highlights%0Adrone%20landing%20technologies%20that%20eliminate%20the%20need%20for%20analytical%20centralized%0Asystems%2C%20potentially%20offering%20scalability%20and%20revolutionizing%20applications%20in%0Alogistics%2C%20safety%2C%20and%20rescue%20missions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04159v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMARLander%253A%2520A%2520Local%2520Path%2520Planning%2520for%2520Drone%2520Swarms%2520using%2520Multiagent%2520Deep%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DDemetros%2520Aschu%2520and%2520Robinroy%2520Peter%2520and%2520Sausar%2520Karaf%2520and%2520Aleksey%2520Fedoseev%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3D%2520%2520Achieving%2520safe%2520and%2520precise%2520landings%2520for%2520a%2520swarm%2520of%2520drones%2520poses%2520a%2520significant%250Achallenge%252C%2520primarily%2520attributed%2520to%2520conventional%2520control%2520and%2520planning%2520methods.%250AThis%2520paper%2520presents%2520the%2520implementation%2520of%2520multi-agent%2520deep%2520reinforcement%250Alearning%2520%2528MADRL%2529%2520techniques%2520for%2520the%2520precise%2520landing%2520of%2520a%2520drone%2520swarm%2520at%250Arelocated%2520target%2520locations.%2520The%2520system%2520is%2520trained%2520in%2520a%2520realistic%2520simulated%250Aenvironment%2520with%2520a%2520maximum%2520velocity%2520of%25203%2520m/s%2520in%2520training%2520spaces%2520of%25204%2520x%25204%2520x%25204%2520m%250Aand%2520deployed%2520utilizing%2520Crazyflie%2520drones%2520with%2520a%2520Vicon%2520indoor%2520localization%250Asystem.%2520The%2520experimental%2520results%2520revealed%2520that%2520the%2520proposed%2520approach%2520achieved%2520a%250Alanding%2520accuracy%2520of%25202.26%2520cm%2520on%2520stationary%2520and%25203.93%2520cm%2520on%2520moving%2520platforms%250Asurpassing%2520a%2520baseline%2520method%2520used%2520with%2520a%2520Proportional-integral-derivative%2520%2528PID%2529%250Acontroller%2520with%2520an%2520Artificial%2520Potential%2520Field%2520%2528APF%2529.%2520This%2520research%2520highlights%250Adrone%2520landing%2520technologies%2520that%2520eliminate%2520the%2520need%2520for%2520analytical%2520centralized%250Asystems%252C%2520potentially%2520offering%2520scalability%2520and%2520revolutionizing%2520applications%2520in%250Alogistics%252C%2520safety%252C%2520and%2520rescue%2520missions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04159v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MARLander%3A%20A%20Local%20Path%20Planning%20for%20Drone%20Swarms%20using%20Multiagent%20Deep%0A%20%20Reinforcement%20Learning&entry.906535625=Demetros%20Aschu%20and%20Robinroy%20Peter%20and%20Sausar%20Karaf%20and%20Aleksey%20Fedoseev%20and%20Dzmitry%20Tsetserukou&entry.1292438233=%20%20Achieving%20safe%20and%20precise%20landings%20for%20a%20swarm%20of%20drones%20poses%20a%20significant%0Achallenge%2C%20primarily%20attributed%20to%20conventional%20control%20and%20planning%20methods.%0AThis%20paper%20presents%20the%20implementation%20of%20multi-agent%20deep%20reinforcement%0Alearning%20%28MADRL%29%20techniques%20for%20the%20precise%20landing%20of%20a%20drone%20swarm%20at%0Arelocated%20target%20locations.%20The%20system%20is%20trained%20in%20a%20realistic%20simulated%0Aenvironment%20with%20a%20maximum%20velocity%20of%203%20m/s%20in%20training%20spaces%20of%204%20x%204%20x%204%20m%0Aand%20deployed%20utilizing%20Crazyflie%20drones%20with%20a%20Vicon%20indoor%20localization%0Asystem.%20The%20experimental%20results%20revealed%20that%20the%20proposed%20approach%20achieved%20a%0Alanding%20accuracy%20of%202.26%20cm%20on%20stationary%20and%203.93%20cm%20on%20moving%20platforms%0Asurpassing%20a%20baseline%20method%20used%20with%20a%20Proportional-integral-derivative%20%28PID%29%0Acontroller%20with%20an%20Artificial%20Potential%20Field%20%28APF%29.%20This%20research%20highlights%0Adrone%20landing%20technologies%20that%20eliminate%20the%20need%20for%20analytical%20centralized%0Asystems%2C%20potentially%20offering%20scalability%20and%20revolutionizing%20applications%20in%0Alogistics%2C%20safety%2C%20and%20rescue%20missions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04159v1&entry.124074799=Read"},
{"title": "The Brain's Bitter Lesson: Scaling Speech Decoding With Self-Supervised\n  Learning", "author": "Dulhan Jayalath and Gilad Landau and Brendan Shillingford and Mark Woolrich and Oiwi Parker Jones", "abstract": "  The past few years have produced a series of spectacular advances in the\ndecoding of speech from brain activity. The engine of these advances has been\nthe acquisition of labelled data, with increasingly large datasets acquired\nfrom single subjects. However, participants exhibit anatomical and other\nindividual differences, and datasets use varied scanners and task designs. As a\nresult, prior work has struggled to leverage data from multiple subjects,\nmultiple datasets, multiple tasks, and unlabelled datasets. In turn, the field\nhas not benefited from the rapidly growing number of open neural data\nrepositories to exploit large-scale data and deep learning. To address this, we\ndevelop an initial set of neuroscience-inspired self-supervised objectives,\ntogether with a neural architecture, for representation learning from\nheterogeneous and unlabelled neural recordings. Experimental results show that\nrepresentations learned with these objectives generalise across subjects,\ndatasets, and tasks, and are also learned faster than using only labelled data.\nIn addition, we set new benchmarks for two foundational speech decoding tasks.\nTaken together, these methods now unlock the potential for training speech\ndecoding models with orders of magnitude more existing data.\n", "link": "http://arxiv.org/abs/2406.04328v1", "date": "2024-06-06", "relevancy": 2.1634, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5704}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5219}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Brain%27s%20Bitter%20Lesson%3A%20Scaling%20Speech%20Decoding%20With%20Self-Supervised%0A%20%20Learning&body=Title%3A%20The%20Brain%27s%20Bitter%20Lesson%3A%20Scaling%20Speech%20Decoding%20With%20Self-Supervised%0A%20%20Learning%0AAuthor%3A%20Dulhan%20Jayalath%20and%20Gilad%20Landau%20and%20Brendan%20Shillingford%20and%20Mark%20Woolrich%20and%20Oiwi%20Parker%20Jones%0AAbstract%3A%20%20%20The%20past%20few%20years%20have%20produced%20a%20series%20of%20spectacular%20advances%20in%20the%0Adecoding%20of%20speech%20from%20brain%20activity.%20The%20engine%20of%20these%20advances%20has%20been%0Athe%20acquisition%20of%20labelled%20data%2C%20with%20increasingly%20large%20datasets%20acquired%0Afrom%20single%20subjects.%20However%2C%20participants%20exhibit%20anatomical%20and%20other%0Aindividual%20differences%2C%20and%20datasets%20use%20varied%20scanners%20and%20task%20designs.%20As%20a%0Aresult%2C%20prior%20work%20has%20struggled%20to%20leverage%20data%20from%20multiple%20subjects%2C%0Amultiple%20datasets%2C%20multiple%20tasks%2C%20and%20unlabelled%20datasets.%20In%20turn%2C%20the%20field%0Ahas%20not%20benefited%20from%20the%20rapidly%20growing%20number%20of%20open%20neural%20data%0Arepositories%20to%20exploit%20large-scale%20data%20and%20deep%20learning.%20To%20address%20this%2C%20we%0Adevelop%20an%20initial%20set%20of%20neuroscience-inspired%20self-supervised%20objectives%2C%0Atogether%20with%20a%20neural%20architecture%2C%20for%20representation%20learning%20from%0Aheterogeneous%20and%20unlabelled%20neural%20recordings.%20Experimental%20results%20show%20that%0Arepresentations%20learned%20with%20these%20objectives%20generalise%20across%20subjects%2C%0Adatasets%2C%20and%20tasks%2C%20and%20are%20also%20learned%20faster%20than%20using%20only%20labelled%20data.%0AIn%20addition%2C%20we%20set%20new%20benchmarks%20for%20two%20foundational%20speech%20decoding%20tasks.%0ATaken%20together%2C%20these%20methods%20now%20unlock%20the%20potential%20for%20training%20speech%0Adecoding%20models%20with%20orders%20of%20magnitude%20more%20existing%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04328v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Brain%2527s%2520Bitter%2520Lesson%253A%2520Scaling%2520Speech%2520Decoding%2520With%2520Self-Supervised%250A%2520%2520Learning%26entry.906535625%3DDulhan%2520Jayalath%2520and%2520Gilad%2520Landau%2520and%2520Brendan%2520Shillingford%2520and%2520Mark%2520Woolrich%2520and%2520Oiwi%2520Parker%2520Jones%26entry.1292438233%3D%2520%2520The%2520past%2520few%2520years%2520have%2520produced%2520a%2520series%2520of%2520spectacular%2520advances%2520in%2520the%250Adecoding%2520of%2520speech%2520from%2520brain%2520activity.%2520The%2520engine%2520of%2520these%2520advances%2520has%2520been%250Athe%2520acquisition%2520of%2520labelled%2520data%252C%2520with%2520increasingly%2520large%2520datasets%2520acquired%250Afrom%2520single%2520subjects.%2520However%252C%2520participants%2520exhibit%2520anatomical%2520and%2520other%250Aindividual%2520differences%252C%2520and%2520datasets%2520use%2520varied%2520scanners%2520and%2520task%2520designs.%2520As%2520a%250Aresult%252C%2520prior%2520work%2520has%2520struggled%2520to%2520leverage%2520data%2520from%2520multiple%2520subjects%252C%250Amultiple%2520datasets%252C%2520multiple%2520tasks%252C%2520and%2520unlabelled%2520datasets.%2520In%2520turn%252C%2520the%2520field%250Ahas%2520not%2520benefited%2520from%2520the%2520rapidly%2520growing%2520number%2520of%2520open%2520neural%2520data%250Arepositories%2520to%2520exploit%2520large-scale%2520data%2520and%2520deep%2520learning.%2520To%2520address%2520this%252C%2520we%250Adevelop%2520an%2520initial%2520set%2520of%2520neuroscience-inspired%2520self-supervised%2520objectives%252C%250Atogether%2520with%2520a%2520neural%2520architecture%252C%2520for%2520representation%2520learning%2520from%250Aheterogeneous%2520and%2520unlabelled%2520neural%2520recordings.%2520Experimental%2520results%2520show%2520that%250Arepresentations%2520learned%2520with%2520these%2520objectives%2520generalise%2520across%2520subjects%252C%250Adatasets%252C%2520and%2520tasks%252C%2520and%2520are%2520also%2520learned%2520faster%2520than%2520using%2520only%2520labelled%2520data.%250AIn%2520addition%252C%2520we%2520set%2520new%2520benchmarks%2520for%2520two%2520foundational%2520speech%2520decoding%2520tasks.%250ATaken%2520together%252C%2520these%2520methods%2520now%2520unlock%2520the%2520potential%2520for%2520training%2520speech%250Adecoding%2520models%2520with%2520orders%2520of%2520magnitude%2520more%2520existing%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04328v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Brain%27s%20Bitter%20Lesson%3A%20Scaling%20Speech%20Decoding%20With%20Self-Supervised%0A%20%20Learning&entry.906535625=Dulhan%20Jayalath%20and%20Gilad%20Landau%20and%20Brendan%20Shillingford%20and%20Mark%20Woolrich%20and%20Oiwi%20Parker%20Jones&entry.1292438233=%20%20The%20past%20few%20years%20have%20produced%20a%20series%20of%20spectacular%20advances%20in%20the%0Adecoding%20of%20speech%20from%20brain%20activity.%20The%20engine%20of%20these%20advances%20has%20been%0Athe%20acquisition%20of%20labelled%20data%2C%20with%20increasingly%20large%20datasets%20acquired%0Afrom%20single%20subjects.%20However%2C%20participants%20exhibit%20anatomical%20and%20other%0Aindividual%20differences%2C%20and%20datasets%20use%20varied%20scanners%20and%20task%20designs.%20As%20a%0Aresult%2C%20prior%20work%20has%20struggled%20to%20leverage%20data%20from%20multiple%20subjects%2C%0Amultiple%20datasets%2C%20multiple%20tasks%2C%20and%20unlabelled%20datasets.%20In%20turn%2C%20the%20field%0Ahas%20not%20benefited%20from%20the%20rapidly%20growing%20number%20of%20open%20neural%20data%0Arepositories%20to%20exploit%20large-scale%20data%20and%20deep%20learning.%20To%20address%20this%2C%20we%0Adevelop%20an%20initial%20set%20of%20neuroscience-inspired%20self-supervised%20objectives%2C%0Atogether%20with%20a%20neural%20architecture%2C%20for%20representation%20learning%20from%0Aheterogeneous%20and%20unlabelled%20neural%20recordings.%20Experimental%20results%20show%20that%0Arepresentations%20learned%20with%20these%20objectives%20generalise%20across%20subjects%2C%0Adatasets%2C%20and%20tasks%2C%20and%20are%20also%20learned%20faster%20than%20using%20only%20labelled%20data.%0AIn%20addition%2C%20we%20set%20new%20benchmarks%20for%20two%20foundational%20speech%20decoding%20tasks.%0ATaken%20together%2C%20these%20methods%20now%20unlock%20the%20potential%20for%20training%20speech%0Adecoding%20models%20with%20orders%20of%20magnitude%20more%20existing%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04328v1&entry.124074799=Read"},
{"title": "Solving Inverse Problems in Protein Space Using Diffusion-Based Priors", "author": "Axel Levy and Eric R. Chan and Sara Fridovich-Keil and Fr\u00e9d\u00e9ric Poitevin and Ellen D. Zhong and Gordon Wetzstein", "abstract": "  The interaction of a protein with its environment can be understood and\ncontrolled via its 3D structure. Experimental methods for protein structure\ndetermination, such as X-ray crystallography or cryogenic electron microscopy,\nshed light on biological processes but introduce challenging inverse problems.\nLearning-based approaches have emerged as accurate and efficient methods to\nsolve these inverse problems for 3D structure determination, but are\nspecialized for a predefined type of measurement. Here, we introduce a\nversatile framework to turn raw biophysical measurements of varying types into\n3D atomic models. Our method combines a physics-based forward model of the\nmeasurement process with a pretrained generative model providing a\ntask-agnostic, data-driven prior. Our method outperforms posterior sampling\nbaselines on both linear and non-linear inverse problems. In particular, it is\nthe first diffusion-based method for refining atomic models from cryo-EM\ndensity maps.\n", "link": "http://arxiv.org/abs/2406.04239v1", "date": "2024-06-06", "relevancy": 2.1374, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5347}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5347}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20Inverse%20Problems%20in%20Protein%20Space%20Using%20Diffusion-Based%20Priors&body=Title%3A%20Solving%20Inverse%20Problems%20in%20Protein%20Space%20Using%20Diffusion-Based%20Priors%0AAuthor%3A%20Axel%20Levy%20and%20Eric%20R.%20Chan%20and%20Sara%20Fridovich-Keil%20and%20Fr%C3%A9d%C3%A9ric%20Poitevin%20and%20Ellen%20D.%20Zhong%20and%20Gordon%20Wetzstein%0AAbstract%3A%20%20%20The%20interaction%20of%20a%20protein%20with%20its%20environment%20can%20be%20understood%20and%0Acontrolled%20via%20its%203D%20structure.%20Experimental%20methods%20for%20protein%20structure%0Adetermination%2C%20such%20as%20X-ray%20crystallography%20or%20cryogenic%20electron%20microscopy%2C%0Ashed%20light%20on%20biological%20processes%20but%20introduce%20challenging%20inverse%20problems.%0ALearning-based%20approaches%20have%20emerged%20as%20accurate%20and%20efficient%20methods%20to%0Asolve%20these%20inverse%20problems%20for%203D%20structure%20determination%2C%20but%20are%0Aspecialized%20for%20a%20predefined%20type%20of%20measurement.%20Here%2C%20we%20introduce%20a%0Aversatile%20framework%20to%20turn%20raw%20biophysical%20measurements%20of%20varying%20types%20into%0A3D%20atomic%20models.%20Our%20method%20combines%20a%20physics-based%20forward%20model%20of%20the%0Ameasurement%20process%20with%20a%20pretrained%20generative%20model%20providing%20a%0Atask-agnostic%2C%20data-driven%20prior.%20Our%20method%20outperforms%20posterior%20sampling%0Abaselines%20on%20both%20linear%20and%20non-linear%20inverse%20problems.%20In%20particular%2C%20it%20is%0Athe%20first%20diffusion-based%20method%20for%20refining%20atomic%20models%20from%20cryo-EM%0Adensity%20maps.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520Inverse%2520Problems%2520in%2520Protein%2520Space%2520Using%2520Diffusion-Based%2520Priors%26entry.906535625%3DAxel%2520Levy%2520and%2520Eric%2520R.%2520Chan%2520and%2520Sara%2520Fridovich-Keil%2520and%2520Fr%25C3%25A9d%25C3%25A9ric%2520Poitevin%2520and%2520Ellen%2520D.%2520Zhong%2520and%2520Gordon%2520Wetzstein%26entry.1292438233%3D%2520%2520The%2520interaction%2520of%2520a%2520protein%2520with%2520its%2520environment%2520can%2520be%2520understood%2520and%250Acontrolled%2520via%2520its%25203D%2520structure.%2520Experimental%2520methods%2520for%2520protein%2520structure%250Adetermination%252C%2520such%2520as%2520X-ray%2520crystallography%2520or%2520cryogenic%2520electron%2520microscopy%252C%250Ashed%2520light%2520on%2520biological%2520processes%2520but%2520introduce%2520challenging%2520inverse%2520problems.%250ALearning-based%2520approaches%2520have%2520emerged%2520as%2520accurate%2520and%2520efficient%2520methods%2520to%250Asolve%2520these%2520inverse%2520problems%2520for%25203D%2520structure%2520determination%252C%2520but%2520are%250Aspecialized%2520for%2520a%2520predefined%2520type%2520of%2520measurement.%2520Here%252C%2520we%2520introduce%2520a%250Aversatile%2520framework%2520to%2520turn%2520raw%2520biophysical%2520measurements%2520of%2520varying%2520types%2520into%250A3D%2520atomic%2520models.%2520Our%2520method%2520combines%2520a%2520physics-based%2520forward%2520model%2520of%2520the%250Ameasurement%2520process%2520with%2520a%2520pretrained%2520generative%2520model%2520providing%2520a%250Atask-agnostic%252C%2520data-driven%2520prior.%2520Our%2520method%2520outperforms%2520posterior%2520sampling%250Abaselines%2520on%2520both%2520linear%2520and%2520non-linear%2520inverse%2520problems.%2520In%2520particular%252C%2520it%2520is%250Athe%2520first%2520diffusion-based%2520method%2520for%2520refining%2520atomic%2520models%2520from%2520cryo-EM%250Adensity%2520maps.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Inverse%20Problems%20in%20Protein%20Space%20Using%20Diffusion-Based%20Priors&entry.906535625=Axel%20Levy%20and%20Eric%20R.%20Chan%20and%20Sara%20Fridovich-Keil%20and%20Fr%C3%A9d%C3%A9ric%20Poitevin%20and%20Ellen%20D.%20Zhong%20and%20Gordon%20Wetzstein&entry.1292438233=%20%20The%20interaction%20of%20a%20protein%20with%20its%20environment%20can%20be%20understood%20and%0Acontrolled%20via%20its%203D%20structure.%20Experimental%20methods%20for%20protein%20structure%0Adetermination%2C%20such%20as%20X-ray%20crystallography%20or%20cryogenic%20electron%20microscopy%2C%0Ashed%20light%20on%20biological%20processes%20but%20introduce%20challenging%20inverse%20problems.%0ALearning-based%20approaches%20have%20emerged%20as%20accurate%20and%20efficient%20methods%20to%0Asolve%20these%20inverse%20problems%20for%203D%20structure%20determination%2C%20but%20are%0Aspecialized%20for%20a%20predefined%20type%20of%20measurement.%20Here%2C%20we%20introduce%20a%0Aversatile%20framework%20to%20turn%20raw%20biophysical%20measurements%20of%20varying%20types%20into%0A3D%20atomic%20models.%20Our%20method%20combines%20a%20physics-based%20forward%20model%20of%20the%0Ameasurement%20process%20with%20a%20pretrained%20generative%20model%20providing%20a%0Atask-agnostic%2C%20data-driven%20prior.%20Our%20method%20outperforms%20posterior%20sampling%0Abaselines%20on%20both%20linear%20and%20non-linear%20inverse%20problems.%20In%20particular%2C%20it%20is%0Athe%20first%20diffusion-based%20method%20for%20refining%20atomic%20models%20from%20cryo-EM%0Adensity%20maps.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04239v1&entry.124074799=Read"},
{"title": "Spatio-temporal Early Prediction based on Multi-objective Reinforcement\n  Learning", "author": "Wei Shao and Yufan Kang and Ziyan Peng and Xiao Xiao and Lei Wang and Yuhui Yang and Flora D Salim", "abstract": "  Accuracy and timeliness are indeed often conflicting goals in prediction\ntasks. Premature predictions may yield a higher rate of false alarms, whereas\ndelaying predictions to gather more information can render them too late to be\nuseful. In applications such as wildfires, crimes, and traffic jams, timely\npredictions are vital for safeguarding human life and property. Consequently,\nfinding a balance between accuracy and timeliness is crucial. In this paper, we\npropose a spatio-temporal early prediction model based on Multi-Objective\nreinforcement learning that can either implement an optimal policy given a\npreference or infer the preference based on a small number of samples. The\nmodel addresses two primary challenges: 1) enhancing the accuracy of early\npredictions and 2) providing the optimal policy for determining the most\nsuitable prediction time for each area. Our method demonstrates superior\nperformance on three large-scale real-world datasets, surpassing existing\nmethods in early spatio-temporal prediction tasks.\n", "link": "http://arxiv.org/abs/2406.04035v1", "date": "2024-06-06", "relevancy": 2.1309, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5762}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.547}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatio-temporal%20Early%20Prediction%20based%20on%20Multi-objective%20Reinforcement%0A%20%20Learning&body=Title%3A%20Spatio-temporal%20Early%20Prediction%20based%20on%20Multi-objective%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Wei%20Shao%20and%20Yufan%20Kang%20and%20Ziyan%20Peng%20and%20Xiao%20Xiao%20and%20Lei%20Wang%20and%20Yuhui%20Yang%20and%20Flora%20D%20Salim%0AAbstract%3A%20%20%20Accuracy%20and%20timeliness%20are%20indeed%20often%20conflicting%20goals%20in%20prediction%0Atasks.%20Premature%20predictions%20may%20yield%20a%20higher%20rate%20of%20false%20alarms%2C%20whereas%0Adelaying%20predictions%20to%20gather%20more%20information%20can%20render%20them%20too%20late%20to%20be%0Auseful.%20In%20applications%20such%20as%20wildfires%2C%20crimes%2C%20and%20traffic%20jams%2C%20timely%0Apredictions%20are%20vital%20for%20safeguarding%20human%20life%20and%20property.%20Consequently%2C%0Afinding%20a%20balance%20between%20accuracy%20and%20timeliness%20is%20crucial.%20In%20this%20paper%2C%20we%0Apropose%20a%20spatio-temporal%20early%20prediction%20model%20based%20on%20Multi-Objective%0Areinforcement%20learning%20that%20can%20either%20implement%20an%20optimal%20policy%20given%20a%0Apreference%20or%20infer%20the%20preference%20based%20on%20a%20small%20number%20of%20samples.%20The%0Amodel%20addresses%20two%20primary%20challenges%3A%201%29%20enhancing%20the%20accuracy%20of%20early%0Apredictions%20and%202%29%20providing%20the%20optimal%20policy%20for%20determining%20the%20most%0Asuitable%20prediction%20time%20for%20each%20area.%20Our%20method%20demonstrates%20superior%0Aperformance%20on%20three%20large-scale%20real-world%20datasets%2C%20surpassing%20existing%0Amethods%20in%20early%20spatio-temporal%20prediction%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatio-temporal%2520Early%2520Prediction%2520based%2520on%2520Multi-objective%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DWei%2520Shao%2520and%2520Yufan%2520Kang%2520and%2520Ziyan%2520Peng%2520and%2520Xiao%2520Xiao%2520and%2520Lei%2520Wang%2520and%2520Yuhui%2520Yang%2520and%2520Flora%2520D%2520Salim%26entry.1292438233%3D%2520%2520Accuracy%2520and%2520timeliness%2520are%2520indeed%2520often%2520conflicting%2520goals%2520in%2520prediction%250Atasks.%2520Premature%2520predictions%2520may%2520yield%2520a%2520higher%2520rate%2520of%2520false%2520alarms%252C%2520whereas%250Adelaying%2520predictions%2520to%2520gather%2520more%2520information%2520can%2520render%2520them%2520too%2520late%2520to%2520be%250Auseful.%2520In%2520applications%2520such%2520as%2520wildfires%252C%2520crimes%252C%2520and%2520traffic%2520jams%252C%2520timely%250Apredictions%2520are%2520vital%2520for%2520safeguarding%2520human%2520life%2520and%2520property.%2520Consequently%252C%250Afinding%2520a%2520balance%2520between%2520accuracy%2520and%2520timeliness%2520is%2520crucial.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520spatio-temporal%2520early%2520prediction%2520model%2520based%2520on%2520Multi-Objective%250Areinforcement%2520learning%2520that%2520can%2520either%2520implement%2520an%2520optimal%2520policy%2520given%2520a%250Apreference%2520or%2520infer%2520the%2520preference%2520based%2520on%2520a%2520small%2520number%2520of%2520samples.%2520The%250Amodel%2520addresses%2520two%2520primary%2520challenges%253A%25201%2529%2520enhancing%2520the%2520accuracy%2520of%2520early%250Apredictions%2520and%25202%2529%2520providing%2520the%2520optimal%2520policy%2520for%2520determining%2520the%2520most%250Asuitable%2520prediction%2520time%2520for%2520each%2520area.%2520Our%2520method%2520demonstrates%2520superior%250Aperformance%2520on%2520three%2520large-scale%2520real-world%2520datasets%252C%2520surpassing%2520existing%250Amethods%2520in%2520early%2520spatio-temporal%2520prediction%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatio-temporal%20Early%20Prediction%20based%20on%20Multi-objective%20Reinforcement%0A%20%20Learning&entry.906535625=Wei%20Shao%20and%20Yufan%20Kang%20and%20Ziyan%20Peng%20and%20Xiao%20Xiao%20and%20Lei%20Wang%20and%20Yuhui%20Yang%20and%20Flora%20D%20Salim&entry.1292438233=%20%20Accuracy%20and%20timeliness%20are%20indeed%20often%20conflicting%20goals%20in%20prediction%0Atasks.%20Premature%20predictions%20may%20yield%20a%20higher%20rate%20of%20false%20alarms%2C%20whereas%0Adelaying%20predictions%20to%20gather%20more%20information%20can%20render%20them%20too%20late%20to%20be%0Auseful.%20In%20applications%20such%20as%20wildfires%2C%20crimes%2C%20and%20traffic%20jams%2C%20timely%0Apredictions%20are%20vital%20for%20safeguarding%20human%20life%20and%20property.%20Consequently%2C%0Afinding%20a%20balance%20between%20accuracy%20and%20timeliness%20is%20crucial.%20In%20this%20paper%2C%20we%0Apropose%20a%20spatio-temporal%20early%20prediction%20model%20based%20on%20Multi-Objective%0Areinforcement%20learning%20that%20can%20either%20implement%20an%20optimal%20policy%20given%20a%0Apreference%20or%20infer%20the%20preference%20based%20on%20a%20small%20number%20of%20samples.%20The%0Amodel%20addresses%20two%20primary%20challenges%3A%201%29%20enhancing%20the%20accuracy%20of%20early%0Apredictions%20and%202%29%20providing%20the%20optimal%20policy%20for%20determining%20the%20most%0Asuitable%20prediction%20time%20for%20each%20area.%20Our%20method%20demonstrates%20superior%0Aperformance%20on%20three%20large-scale%20real-world%20datasets%2C%20surpassing%20existing%0Amethods%20in%20early%20spatio-temporal%20prediction%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04035v1&entry.124074799=Read"},
{"title": "LenslessFace: An End-to-End Optimized Lensless System for\n  Privacy-Preserving Face Verification", "author": "Xin Cai and Hailong Zhang and Chenchen Wang and Wentao Liu and Jinwei Gu and Tianfan Xue", "abstract": "  Lensless cameras, innovatively replacing traditional lenses for ultra-thin,\nflat optics, encode light directly onto sensors, producing images that are not\nimmediately recognizable. This compact, lightweight, and cost-effective imaging\nsolution offers inherent privacy advantages, making it attractive for\nprivacy-sensitive applications like face verification. Typical lensless face\nverification adopts a two-stage process of reconstruction followed by\nverification, incurring privacy risks from reconstructed faces and high\ncomputational costs. This paper presents an end-to-end optimization approach\nfor privacy-preserving face verification directly on encoded lensless captures,\nensuring that the entire software pipeline remains encoded with no visible\nfaces as intermediate results. To achieve this, we propose several techniques\nto address unique challenges from the lensless setup which precludes\ntraditional face detection and alignment. Specifically, we propose a face\ncenter alignment scheme, an augmentation curriculum to build robustness against\nvariations, and a knowledge distillation method to smooth optimization and\nenhance performance. Evaluations under both simulation and real environment\ndemonstrate our method outperforms two-stage lensless verification while\nenhancing privacy and efficiency. Project website:\n\\url{lenslessface.github.io}.\n", "link": "http://arxiv.org/abs/2406.04129v1", "date": "2024-06-06", "relevancy": 2.1255, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5364}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5312}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LenslessFace%3A%20An%20End-to-End%20Optimized%20Lensless%20System%20for%0A%20%20Privacy-Preserving%20Face%20Verification&body=Title%3A%20LenslessFace%3A%20An%20End-to-End%20Optimized%20Lensless%20System%20for%0A%20%20Privacy-Preserving%20Face%20Verification%0AAuthor%3A%20Xin%20Cai%20and%20Hailong%20Zhang%20and%20Chenchen%20Wang%20and%20Wentao%20Liu%20and%20Jinwei%20Gu%20and%20Tianfan%20Xue%0AAbstract%3A%20%20%20Lensless%20cameras%2C%20innovatively%20replacing%20traditional%20lenses%20for%20ultra-thin%2C%0Aflat%20optics%2C%20encode%20light%20directly%20onto%20sensors%2C%20producing%20images%20that%20are%20not%0Aimmediately%20recognizable.%20This%20compact%2C%20lightweight%2C%20and%20cost-effective%20imaging%0Asolution%20offers%20inherent%20privacy%20advantages%2C%20making%20it%20attractive%20for%0Aprivacy-sensitive%20applications%20like%20face%20verification.%20Typical%20lensless%20face%0Averification%20adopts%20a%20two-stage%20process%20of%20reconstruction%20followed%20by%0Averification%2C%20incurring%20privacy%20risks%20from%20reconstructed%20faces%20and%20high%0Acomputational%20costs.%20This%20paper%20presents%20an%20end-to-end%20optimization%20approach%0Afor%20privacy-preserving%20face%20verification%20directly%20on%20encoded%20lensless%20captures%2C%0Aensuring%20that%20the%20entire%20software%20pipeline%20remains%20encoded%20with%20no%20visible%0Afaces%20as%20intermediate%20results.%20To%20achieve%20this%2C%20we%20propose%20several%20techniques%0Ato%20address%20unique%20challenges%20from%20the%20lensless%20setup%20which%20precludes%0Atraditional%20face%20detection%20and%20alignment.%20Specifically%2C%20we%20propose%20a%20face%0Acenter%20alignment%20scheme%2C%20an%20augmentation%20curriculum%20to%20build%20robustness%20against%0Avariations%2C%20and%20a%20knowledge%20distillation%20method%20to%20smooth%20optimization%20and%0Aenhance%20performance.%20Evaluations%20under%20both%20simulation%20and%20real%20environment%0Ademonstrate%20our%20method%20outperforms%20two-stage%20lensless%20verification%20while%0Aenhancing%20privacy%20and%20efficiency.%20Project%20website%3A%0A%5Curl%7Blenslessface.github.io%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04129v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLenslessFace%253A%2520An%2520End-to-End%2520Optimized%2520Lensless%2520System%2520for%250A%2520%2520Privacy-Preserving%2520Face%2520Verification%26entry.906535625%3DXin%2520Cai%2520and%2520Hailong%2520Zhang%2520and%2520Chenchen%2520Wang%2520and%2520Wentao%2520Liu%2520and%2520Jinwei%2520Gu%2520and%2520Tianfan%2520Xue%26entry.1292438233%3D%2520%2520Lensless%2520cameras%252C%2520innovatively%2520replacing%2520traditional%2520lenses%2520for%2520ultra-thin%252C%250Aflat%2520optics%252C%2520encode%2520light%2520directly%2520onto%2520sensors%252C%2520producing%2520images%2520that%2520are%2520not%250Aimmediately%2520recognizable.%2520This%2520compact%252C%2520lightweight%252C%2520and%2520cost-effective%2520imaging%250Asolution%2520offers%2520inherent%2520privacy%2520advantages%252C%2520making%2520it%2520attractive%2520for%250Aprivacy-sensitive%2520applications%2520like%2520face%2520verification.%2520Typical%2520lensless%2520face%250Averification%2520adopts%2520a%2520two-stage%2520process%2520of%2520reconstruction%2520followed%2520by%250Averification%252C%2520incurring%2520privacy%2520risks%2520from%2520reconstructed%2520faces%2520and%2520high%250Acomputational%2520costs.%2520This%2520paper%2520presents%2520an%2520end-to-end%2520optimization%2520approach%250Afor%2520privacy-preserving%2520face%2520verification%2520directly%2520on%2520encoded%2520lensless%2520captures%252C%250Aensuring%2520that%2520the%2520entire%2520software%2520pipeline%2520remains%2520encoded%2520with%2520no%2520visible%250Afaces%2520as%2520intermediate%2520results.%2520To%2520achieve%2520this%252C%2520we%2520propose%2520several%2520techniques%250Ato%2520address%2520unique%2520challenges%2520from%2520the%2520lensless%2520setup%2520which%2520precludes%250Atraditional%2520face%2520detection%2520and%2520alignment.%2520Specifically%252C%2520we%2520propose%2520a%2520face%250Acenter%2520alignment%2520scheme%252C%2520an%2520augmentation%2520curriculum%2520to%2520build%2520robustness%2520against%250Avariations%252C%2520and%2520a%2520knowledge%2520distillation%2520method%2520to%2520smooth%2520optimization%2520and%250Aenhance%2520performance.%2520Evaluations%2520under%2520both%2520simulation%2520and%2520real%2520environment%250Ademonstrate%2520our%2520method%2520outperforms%2520two-stage%2520lensless%2520verification%2520while%250Aenhancing%2520privacy%2520and%2520efficiency.%2520Project%2520website%253A%250A%255Curl%257Blenslessface.github.io%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04129v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LenslessFace%3A%20An%20End-to-End%20Optimized%20Lensless%20System%20for%0A%20%20Privacy-Preserving%20Face%20Verification&entry.906535625=Xin%20Cai%20and%20Hailong%20Zhang%20and%20Chenchen%20Wang%20and%20Wentao%20Liu%20and%20Jinwei%20Gu%20and%20Tianfan%20Xue&entry.1292438233=%20%20Lensless%20cameras%2C%20innovatively%20replacing%20traditional%20lenses%20for%20ultra-thin%2C%0Aflat%20optics%2C%20encode%20light%20directly%20onto%20sensors%2C%20producing%20images%20that%20are%20not%0Aimmediately%20recognizable.%20This%20compact%2C%20lightweight%2C%20and%20cost-effective%20imaging%0Asolution%20offers%20inherent%20privacy%20advantages%2C%20making%20it%20attractive%20for%0Aprivacy-sensitive%20applications%20like%20face%20verification.%20Typical%20lensless%20face%0Averification%20adopts%20a%20two-stage%20process%20of%20reconstruction%20followed%20by%0Averification%2C%20incurring%20privacy%20risks%20from%20reconstructed%20faces%20and%20high%0Acomputational%20costs.%20This%20paper%20presents%20an%20end-to-end%20optimization%20approach%0Afor%20privacy-preserving%20face%20verification%20directly%20on%20encoded%20lensless%20captures%2C%0Aensuring%20that%20the%20entire%20software%20pipeline%20remains%20encoded%20with%20no%20visible%0Afaces%20as%20intermediate%20results.%20To%20achieve%20this%2C%20we%20propose%20several%20techniques%0Ato%20address%20unique%20challenges%20from%20the%20lensless%20setup%20which%20precludes%0Atraditional%20face%20detection%20and%20alignment.%20Specifically%2C%20we%20propose%20a%20face%0Acenter%20alignment%20scheme%2C%20an%20augmentation%20curriculum%20to%20build%20robustness%20against%0Avariations%2C%20and%20a%20knowledge%20distillation%20method%20to%20smooth%20optimization%20and%0Aenhance%20performance.%20Evaluations%20under%20both%20simulation%20and%20real%20environment%0Ademonstrate%20our%20method%20outperforms%20two-stage%20lensless%20verification%20while%0Aenhancing%20privacy%20and%20efficiency.%20Project%20website%3A%0A%5Curl%7Blenslessface.github.io%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04129v1&entry.124074799=Read"},
{"title": "How Far Can We Compress Instant-NGP-Based NeRF?", "author": "Yihang Chen and Qianyi Wu and Mehrtash Harandi and Jianfei Cai", "abstract": "  In recent years, Neural Radiance Field (NeRF) has demonstrated remarkable\ncapabilities in representing 3D scenes. To expedite the rendering process,\nlearnable explicit representations have been introduced for combination with\nimplicit NeRF representation, which however results in a large storage space\nrequirement. In this paper, we introduce the Context-based NeRF Compression\n(CNC) framework, which leverages highly efficient context models to provide a\nstorage-friendly NeRF representation. Specifically, we excavate both level-wise\nand dimension-wise context dependencies to enable probability prediction for\ninformation entropy reduction. Additionally, we exploit hash collision and\noccupancy grids as strong prior knowledge for better context modeling. To the\nbest of our knowledge, we are the first to construct and exploit context models\nfor NeRF compression. We achieve a size reduction of 100$\\times$ and 70$\\times$\nwith improved fidelity against the baseline Instant-NGP on Synthesic-NeRF and\nTanks and Temples datasets, respectively. Additionally, we attain 86.7\\% and\n82.3\\% storage size reduction against the SOTA NeRF compression method BiRF.\nOur code is available here: https://github.com/YihangChen-ee/CNC.\n", "link": "http://arxiv.org/abs/2406.04101v1", "date": "2024-06-06", "relevancy": 2.1197, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5542}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5396}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5018}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Far%20Can%20We%20Compress%20Instant-NGP-Based%20NeRF%3F&body=Title%3A%20How%20Far%20Can%20We%20Compress%20Instant-NGP-Based%20NeRF%3F%0AAuthor%3A%20Yihang%20Chen%20and%20Qianyi%20Wu%20and%20Mehrtash%20Harandi%20and%20Jianfei%20Cai%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Neural%20Radiance%20Field%20%28NeRF%29%20has%20demonstrated%20remarkable%0Acapabilities%20in%20representing%203D%20scenes.%20To%20expedite%20the%20rendering%20process%2C%0Alearnable%20explicit%20representations%20have%20been%20introduced%20for%20combination%20with%0Aimplicit%20NeRF%20representation%2C%20which%20however%20results%20in%20a%20large%20storage%20space%0Arequirement.%20In%20this%20paper%2C%20we%20introduce%20the%20Context-based%20NeRF%20Compression%0A%28CNC%29%20framework%2C%20which%20leverages%20highly%20efficient%20context%20models%20to%20provide%20a%0Astorage-friendly%20NeRF%20representation.%20Specifically%2C%20we%20excavate%20both%20level-wise%0Aand%20dimension-wise%20context%20dependencies%20to%20enable%20probability%20prediction%20for%0Ainformation%20entropy%20reduction.%20Additionally%2C%20we%20exploit%20hash%20collision%20and%0Aoccupancy%20grids%20as%20strong%20prior%20knowledge%20for%20better%20context%20modeling.%20To%20the%0Abest%20of%20our%20knowledge%2C%20we%20are%20the%20first%20to%20construct%20and%20exploit%20context%20models%0Afor%20NeRF%20compression.%20We%20achieve%20a%20size%20reduction%20of%20100%24%5Ctimes%24%20and%2070%24%5Ctimes%24%0Awith%20improved%20fidelity%20against%20the%20baseline%20Instant-NGP%20on%20Synthesic-NeRF%20and%0ATanks%20and%20Temples%20datasets%2C%20respectively.%20Additionally%2C%20we%20attain%2086.7%5C%25%20and%0A82.3%5C%25%20storage%20size%20reduction%20against%20the%20SOTA%20NeRF%20compression%20method%20BiRF.%0AOur%20code%20is%20available%20here%3A%20https%3A//github.com/YihangChen-ee/CNC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Far%2520Can%2520We%2520Compress%2520Instant-NGP-Based%2520NeRF%253F%26entry.906535625%3DYihang%2520Chen%2520and%2520Qianyi%2520Wu%2520and%2520Mehrtash%2520Harandi%2520and%2520Jianfei%2520Cai%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Neural%2520Radiance%2520Field%2520%2528NeRF%2529%2520has%2520demonstrated%2520remarkable%250Acapabilities%2520in%2520representing%25203D%2520scenes.%2520To%2520expedite%2520the%2520rendering%2520process%252C%250Alearnable%2520explicit%2520representations%2520have%2520been%2520introduced%2520for%2520combination%2520with%250Aimplicit%2520NeRF%2520representation%252C%2520which%2520however%2520results%2520in%2520a%2520large%2520storage%2520space%250Arequirement.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Context-based%2520NeRF%2520Compression%250A%2528CNC%2529%2520framework%252C%2520which%2520leverages%2520highly%2520efficient%2520context%2520models%2520to%2520provide%2520a%250Astorage-friendly%2520NeRF%2520representation.%2520Specifically%252C%2520we%2520excavate%2520both%2520level-wise%250Aand%2520dimension-wise%2520context%2520dependencies%2520to%2520enable%2520probability%2520prediction%2520for%250Ainformation%2520entropy%2520reduction.%2520Additionally%252C%2520we%2520exploit%2520hash%2520collision%2520and%250Aoccupancy%2520grids%2520as%2520strong%2520prior%2520knowledge%2520for%2520better%2520context%2520modeling.%2520To%2520the%250Abest%2520of%2520our%2520knowledge%252C%2520we%2520are%2520the%2520first%2520to%2520construct%2520and%2520exploit%2520context%2520models%250Afor%2520NeRF%2520compression.%2520We%2520achieve%2520a%2520size%2520reduction%2520of%2520100%2524%255Ctimes%2524%2520and%252070%2524%255Ctimes%2524%250Awith%2520improved%2520fidelity%2520against%2520the%2520baseline%2520Instant-NGP%2520on%2520Synthesic-NeRF%2520and%250ATanks%2520and%2520Temples%2520datasets%252C%2520respectively.%2520Additionally%252C%2520we%2520attain%252086.7%255C%2525%2520and%250A82.3%255C%2525%2520storage%2520size%2520reduction%2520against%2520the%2520SOTA%2520NeRF%2520compression%2520method%2520BiRF.%250AOur%2520code%2520is%2520available%2520here%253A%2520https%253A//github.com/YihangChen-ee/CNC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Far%20Can%20We%20Compress%20Instant-NGP-Based%20NeRF%3F&entry.906535625=Yihang%20Chen%20and%20Qianyi%20Wu%20and%20Mehrtash%20Harandi%20and%20Jianfei%20Cai&entry.1292438233=%20%20In%20recent%20years%2C%20Neural%20Radiance%20Field%20%28NeRF%29%20has%20demonstrated%20remarkable%0Acapabilities%20in%20representing%203D%20scenes.%20To%20expedite%20the%20rendering%20process%2C%0Alearnable%20explicit%20representations%20have%20been%20introduced%20for%20combination%20with%0Aimplicit%20NeRF%20representation%2C%20which%20however%20results%20in%20a%20large%20storage%20space%0Arequirement.%20In%20this%20paper%2C%20we%20introduce%20the%20Context-based%20NeRF%20Compression%0A%28CNC%29%20framework%2C%20which%20leverages%20highly%20efficient%20context%20models%20to%20provide%20a%0Astorage-friendly%20NeRF%20representation.%20Specifically%2C%20we%20excavate%20both%20level-wise%0Aand%20dimension-wise%20context%20dependencies%20to%20enable%20probability%20prediction%20for%0Ainformation%20entropy%20reduction.%20Additionally%2C%20we%20exploit%20hash%20collision%20and%0Aoccupancy%20grids%20as%20strong%20prior%20knowledge%20for%20better%20context%20modeling.%20To%20the%0Abest%20of%20our%20knowledge%2C%20we%20are%20the%20first%20to%20construct%20and%20exploit%20context%20models%0Afor%20NeRF%20compression.%20We%20achieve%20a%20size%20reduction%20of%20100%24%5Ctimes%24%20and%2070%24%5Ctimes%24%0Awith%20improved%20fidelity%20against%20the%20baseline%20Instant-NGP%20on%20Synthesic-NeRF%20and%0ATanks%20and%20Temples%20datasets%2C%20respectively.%20Additionally%2C%20we%20attain%2086.7%5C%25%20and%0A82.3%5C%25%20storage%20size%20reduction%20against%20the%20SOTA%20NeRF%20compression%20method%20BiRF.%0AOur%20code%20is%20available%20here%3A%20https%3A//github.com/YihangChen-ee/CNC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04101v1&entry.124074799=Read"},
{"title": "The Rate-Distortion-Perception-Classification Tradeoff: Joint Source\n  Coding and Modulation via Inverse-Domain GANs", "author": "Junli Fang and Jo\u00e3o F. C. Mota and Baoshan Lu and Weicheng Zhang and Xuemin Hong", "abstract": "  The joint source-channel coding (JSCC) framework leverages deep learning to\nlearn from data the best codes for source and channel coding. When the output\nsignal, rather than being binary, is directly mapped onto the IQ domain\n(complex-valued), we call the resulting framework joint source coding and\nmodulation (JSCM). We consider a JSCM scenario and show the existence of a\nstrict tradeoff between channel rate, distortion, perception, and\nclassification accuracy, a tradeoff that we name RDPC. We then propose two\nimage compression methods to navigate that tradeoff: the RDPCO algorithm which,\nunder simple assumptions, directly solves the optimization problem\ncharacterizing the tradeoff, and an algorithm based on an inverse-domain\ngenerative adversarial network (ID-GAN), which is more general and achieves\nextreme compression. Simulation results corroborate the theoretical findings,\nshowing that both algorithms exhibit the RDPC tradeoff. They also demonstrate\nthat the proposed ID-GAN algorithm effectively balances image distortion,\nperception, and classification accuracy, and significantly outperforms\ntraditional separation-based methods and recent deep JSCM architectures in\nterms of one or more of these metrics.\n", "link": "http://arxiv.org/abs/2312.14792v2", "date": "2024-06-06", "relevancy": 2.1181, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5333}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5315}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Rate-Distortion-Perception-Classification%20Tradeoff%3A%20Joint%20Source%0A%20%20Coding%20and%20Modulation%20via%20Inverse-Domain%20GANs&body=Title%3A%20The%20Rate-Distortion-Perception-Classification%20Tradeoff%3A%20Joint%20Source%0A%20%20Coding%20and%20Modulation%20via%20Inverse-Domain%20GANs%0AAuthor%3A%20Junli%20Fang%20and%20Jo%C3%A3o%20F.%20C.%20Mota%20and%20Baoshan%20Lu%20and%20Weicheng%20Zhang%20and%20Xuemin%20Hong%0AAbstract%3A%20%20%20The%20joint%20source-channel%20coding%20%28JSCC%29%20framework%20leverages%20deep%20learning%20to%0Alearn%20from%20data%20the%20best%20codes%20for%20source%20and%20channel%20coding.%20When%20the%20output%0Asignal%2C%20rather%20than%20being%20binary%2C%20is%20directly%20mapped%20onto%20the%20IQ%20domain%0A%28complex-valued%29%2C%20we%20call%20the%20resulting%20framework%20joint%20source%20coding%20and%0Amodulation%20%28JSCM%29.%20We%20consider%20a%20JSCM%20scenario%20and%20show%20the%20existence%20of%20a%0Astrict%20tradeoff%20between%20channel%20rate%2C%20distortion%2C%20perception%2C%20and%0Aclassification%20accuracy%2C%20a%20tradeoff%20that%20we%20name%20RDPC.%20We%20then%20propose%20two%0Aimage%20compression%20methods%20to%20navigate%20that%20tradeoff%3A%20the%20RDPCO%20algorithm%20which%2C%0Aunder%20simple%20assumptions%2C%20directly%20solves%20the%20optimization%20problem%0Acharacterizing%20the%20tradeoff%2C%20and%20an%20algorithm%20based%20on%20an%20inverse-domain%0Agenerative%20adversarial%20network%20%28ID-GAN%29%2C%20which%20is%20more%20general%20and%20achieves%0Aextreme%20compression.%20Simulation%20results%20corroborate%20the%20theoretical%20findings%2C%0Ashowing%20that%20both%20algorithms%20exhibit%20the%20RDPC%20tradeoff.%20They%20also%20demonstrate%0Athat%20the%20proposed%20ID-GAN%20algorithm%20effectively%20balances%20image%20distortion%2C%0Aperception%2C%20and%20classification%20accuracy%2C%20and%20significantly%20outperforms%0Atraditional%20separation-based%20methods%20and%20recent%20deep%20JSCM%20architectures%20in%0Aterms%20of%20one%20or%20more%20of%20these%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14792v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Rate-Distortion-Perception-Classification%2520Tradeoff%253A%2520Joint%2520Source%250A%2520%2520Coding%2520and%2520Modulation%2520via%2520Inverse-Domain%2520GANs%26entry.906535625%3DJunli%2520Fang%2520and%2520Jo%25C3%25A3o%2520F.%2520C.%2520Mota%2520and%2520Baoshan%2520Lu%2520and%2520Weicheng%2520Zhang%2520and%2520Xuemin%2520Hong%26entry.1292438233%3D%2520%2520The%2520joint%2520source-channel%2520coding%2520%2528JSCC%2529%2520framework%2520leverages%2520deep%2520learning%2520to%250Alearn%2520from%2520data%2520the%2520best%2520codes%2520for%2520source%2520and%2520channel%2520coding.%2520When%2520the%2520output%250Asignal%252C%2520rather%2520than%2520being%2520binary%252C%2520is%2520directly%2520mapped%2520onto%2520the%2520IQ%2520domain%250A%2528complex-valued%2529%252C%2520we%2520call%2520the%2520resulting%2520framework%2520joint%2520source%2520coding%2520and%250Amodulation%2520%2528JSCM%2529.%2520We%2520consider%2520a%2520JSCM%2520scenario%2520and%2520show%2520the%2520existence%2520of%2520a%250Astrict%2520tradeoff%2520between%2520channel%2520rate%252C%2520distortion%252C%2520perception%252C%2520and%250Aclassification%2520accuracy%252C%2520a%2520tradeoff%2520that%2520we%2520name%2520RDPC.%2520We%2520then%2520propose%2520two%250Aimage%2520compression%2520methods%2520to%2520navigate%2520that%2520tradeoff%253A%2520the%2520RDPCO%2520algorithm%2520which%252C%250Aunder%2520simple%2520assumptions%252C%2520directly%2520solves%2520the%2520optimization%2520problem%250Acharacterizing%2520the%2520tradeoff%252C%2520and%2520an%2520algorithm%2520based%2520on%2520an%2520inverse-domain%250Agenerative%2520adversarial%2520network%2520%2528ID-GAN%2529%252C%2520which%2520is%2520more%2520general%2520and%2520achieves%250Aextreme%2520compression.%2520Simulation%2520results%2520corroborate%2520the%2520theoretical%2520findings%252C%250Ashowing%2520that%2520both%2520algorithms%2520exhibit%2520the%2520RDPC%2520tradeoff.%2520They%2520also%2520demonstrate%250Athat%2520the%2520proposed%2520ID-GAN%2520algorithm%2520effectively%2520balances%2520image%2520distortion%252C%250Aperception%252C%2520and%2520classification%2520accuracy%252C%2520and%2520significantly%2520outperforms%250Atraditional%2520separation-based%2520methods%2520and%2520recent%2520deep%2520JSCM%2520architectures%2520in%250Aterms%2520of%2520one%2520or%2520more%2520of%2520these%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14792v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Rate-Distortion-Perception-Classification%20Tradeoff%3A%20Joint%20Source%0A%20%20Coding%20and%20Modulation%20via%20Inverse-Domain%20GANs&entry.906535625=Junli%20Fang%20and%20Jo%C3%A3o%20F.%20C.%20Mota%20and%20Baoshan%20Lu%20and%20Weicheng%20Zhang%20and%20Xuemin%20Hong&entry.1292438233=%20%20The%20joint%20source-channel%20coding%20%28JSCC%29%20framework%20leverages%20deep%20learning%20to%0Alearn%20from%20data%20the%20best%20codes%20for%20source%20and%20channel%20coding.%20When%20the%20output%0Asignal%2C%20rather%20than%20being%20binary%2C%20is%20directly%20mapped%20onto%20the%20IQ%20domain%0A%28complex-valued%29%2C%20we%20call%20the%20resulting%20framework%20joint%20source%20coding%20and%0Amodulation%20%28JSCM%29.%20We%20consider%20a%20JSCM%20scenario%20and%20show%20the%20existence%20of%20a%0Astrict%20tradeoff%20between%20channel%20rate%2C%20distortion%2C%20perception%2C%20and%0Aclassification%20accuracy%2C%20a%20tradeoff%20that%20we%20name%20RDPC.%20We%20then%20propose%20two%0Aimage%20compression%20methods%20to%20navigate%20that%20tradeoff%3A%20the%20RDPCO%20algorithm%20which%2C%0Aunder%20simple%20assumptions%2C%20directly%20solves%20the%20optimization%20problem%0Acharacterizing%20the%20tradeoff%2C%20and%20an%20algorithm%20based%20on%20an%20inverse-domain%0Agenerative%20adversarial%20network%20%28ID-GAN%29%2C%20which%20is%20more%20general%20and%20achieves%0Aextreme%20compression.%20Simulation%20results%20corroborate%20the%20theoretical%20findings%2C%0Ashowing%20that%20both%20algorithms%20exhibit%20the%20RDPC%20tradeoff.%20They%20also%20demonstrate%0Athat%20the%20proposed%20ID-GAN%20algorithm%20effectively%20balances%20image%20distortion%2C%0Aperception%2C%20and%20classification%20accuracy%2C%20and%20significantly%20outperforms%0Atraditional%20separation-based%20methods%20and%20recent%20deep%20JSCM%20architectures%20in%0Aterms%20of%20one%20or%20more%20of%20these%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14792v2&entry.124074799=Read"},
{"title": "DeepStack: Deeply Stacking Visual Tokens is Surprisingly Simple and\n  Effective for LMMs", "author": "Lingchen Meng and Jianwei Yang and Rui Tian and Xiyang Dai and Zuxuan Wu and Jianfeng Gao and Yu-Gang Jiang", "abstract": "  Most large multimodal models (LMMs) are implemented by feeding visual tokens\nas a sequence into the first layer of a large language model (LLM). The\nresulting architecture is simple but significantly increases computation and\nmemory costs, as it has to handle a large number of additional tokens in its\ninput layer. This paper presents a new architecture DeepStack for LMMs.\nConsidering $N$ layers in the language and vision transformer of LMMs, we stack\nthe visual tokens into $N$ groups and feed each group to its aligned\ntransformer layer \\textit{from bottom to top}. Surprisingly, this simple method\ngreatly enhances the power of LMMs to model interactions among visual tokens\nacross layers but with minimal additional cost. We apply DeepStack to both\nlanguage and vision transformer in LMMs, and validate the effectiveness of\nDeepStack LMMs with extensive empirical results. Using the same context length,\nour DeepStack 7B and 13B parameters surpass their counterparts by \\textbf{2.7}\nand \\textbf{2.9} on average across \\textbf{9} benchmarks, respectively. Using\nonly one-fifth of the context length, DeepStack rivals closely to the\ncounterparts that use the full context length. These gains are particularly\npronounced on high-resolution tasks, e.g., \\textbf{4.2}, \\textbf{11.0}, and\n\\textbf{4.0} improvements on TextVQA, DocVQA, and InfoVQA compared to\nLLaVA-1.5-7B, respectively. We further apply DeepStack to vision transformer\nlayers, which brings us a similar amount of improvements, \\textbf{3.8} on\naverage compared with LLaVA-1.5-7B.\n", "link": "http://arxiv.org/abs/2406.04334v1", "date": "2024-06-06", "relevancy": 2.1073, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5479}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5133}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepStack%3A%20Deeply%20Stacking%20Visual%20Tokens%20is%20Surprisingly%20Simple%20and%0A%20%20Effective%20for%20LMMs&body=Title%3A%20DeepStack%3A%20Deeply%20Stacking%20Visual%20Tokens%20is%20Surprisingly%20Simple%20and%0A%20%20Effective%20for%20LMMs%0AAuthor%3A%20Lingchen%20Meng%20and%20Jianwei%20Yang%20and%20Rui%20Tian%20and%20Xiyang%20Dai%20and%20Zuxuan%20Wu%20and%20Jianfeng%20Gao%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Most%20large%20multimodal%20models%20%28LMMs%29%20are%20implemented%20by%20feeding%20visual%20tokens%0Aas%20a%20sequence%20into%20the%20first%20layer%20of%20a%20large%20language%20model%20%28LLM%29.%20The%0Aresulting%20architecture%20is%20simple%20but%20significantly%20increases%20computation%20and%0Amemory%20costs%2C%20as%20it%20has%20to%20handle%20a%20large%20number%20of%20additional%20tokens%20in%20its%0Ainput%20layer.%20This%20paper%20presents%20a%20new%20architecture%20DeepStack%20for%20LMMs.%0AConsidering%20%24N%24%20layers%20in%20the%20language%20and%20vision%20transformer%20of%20LMMs%2C%20we%20stack%0Athe%20visual%20tokens%20into%20%24N%24%20groups%20and%20feed%20each%20group%20to%20its%20aligned%0Atransformer%20layer%20%5Ctextit%7Bfrom%20bottom%20to%20top%7D.%20Surprisingly%2C%20this%20simple%20method%0Agreatly%20enhances%20the%20power%20of%20LMMs%20to%20model%20interactions%20among%20visual%20tokens%0Aacross%20layers%20but%20with%20minimal%20additional%20cost.%20We%20apply%20DeepStack%20to%20both%0Alanguage%20and%20vision%20transformer%20in%20LMMs%2C%20and%20validate%20the%20effectiveness%20of%0ADeepStack%20LMMs%20with%20extensive%20empirical%20results.%20Using%20the%20same%20context%20length%2C%0Aour%20DeepStack%207B%20and%2013B%20parameters%20surpass%20their%20counterparts%20by%20%5Ctextbf%7B2.7%7D%0Aand%20%5Ctextbf%7B2.9%7D%20on%20average%20across%20%5Ctextbf%7B9%7D%20benchmarks%2C%20respectively.%20Using%0Aonly%20one-fifth%20of%20the%20context%20length%2C%20DeepStack%20rivals%20closely%20to%20the%0Acounterparts%20that%20use%20the%20full%20context%20length.%20These%20gains%20are%20particularly%0Apronounced%20on%20high-resolution%20tasks%2C%20e.g.%2C%20%5Ctextbf%7B4.2%7D%2C%20%5Ctextbf%7B11.0%7D%2C%20and%0A%5Ctextbf%7B4.0%7D%20improvements%20on%20TextVQA%2C%20DocVQA%2C%20and%20InfoVQA%20compared%20to%0ALLaVA-1.5-7B%2C%20respectively.%20We%20further%20apply%20DeepStack%20to%20vision%20transformer%0Alayers%2C%20which%20brings%20us%20a%20similar%20amount%20of%20improvements%2C%20%5Ctextbf%7B3.8%7D%20on%0Aaverage%20compared%20with%20LLaVA-1.5-7B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepStack%253A%2520Deeply%2520Stacking%2520Visual%2520Tokens%2520is%2520Surprisingly%2520Simple%2520and%250A%2520%2520Effective%2520for%2520LMMs%26entry.906535625%3DLingchen%2520Meng%2520and%2520Jianwei%2520Yang%2520and%2520Rui%2520Tian%2520and%2520Xiyang%2520Dai%2520and%2520Zuxuan%2520Wu%2520and%2520Jianfeng%2520Gao%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Most%2520large%2520multimodal%2520models%2520%2528LMMs%2529%2520are%2520implemented%2520by%2520feeding%2520visual%2520tokens%250Aas%2520a%2520sequence%2520into%2520the%2520first%2520layer%2520of%2520a%2520large%2520language%2520model%2520%2528LLM%2529.%2520The%250Aresulting%2520architecture%2520is%2520simple%2520but%2520significantly%2520increases%2520computation%2520and%250Amemory%2520costs%252C%2520as%2520it%2520has%2520to%2520handle%2520a%2520large%2520number%2520of%2520additional%2520tokens%2520in%2520its%250Ainput%2520layer.%2520This%2520paper%2520presents%2520a%2520new%2520architecture%2520DeepStack%2520for%2520LMMs.%250AConsidering%2520%2524N%2524%2520layers%2520in%2520the%2520language%2520and%2520vision%2520transformer%2520of%2520LMMs%252C%2520we%2520stack%250Athe%2520visual%2520tokens%2520into%2520%2524N%2524%2520groups%2520and%2520feed%2520each%2520group%2520to%2520its%2520aligned%250Atransformer%2520layer%2520%255Ctextit%257Bfrom%2520bottom%2520to%2520top%257D.%2520Surprisingly%252C%2520this%2520simple%2520method%250Agreatly%2520enhances%2520the%2520power%2520of%2520LMMs%2520to%2520model%2520interactions%2520among%2520visual%2520tokens%250Aacross%2520layers%2520but%2520with%2520minimal%2520additional%2520cost.%2520We%2520apply%2520DeepStack%2520to%2520both%250Alanguage%2520and%2520vision%2520transformer%2520in%2520LMMs%252C%2520and%2520validate%2520the%2520effectiveness%2520of%250ADeepStack%2520LMMs%2520with%2520extensive%2520empirical%2520results.%2520Using%2520the%2520same%2520context%2520length%252C%250Aour%2520DeepStack%25207B%2520and%252013B%2520parameters%2520surpass%2520their%2520counterparts%2520by%2520%255Ctextbf%257B2.7%257D%250Aand%2520%255Ctextbf%257B2.9%257D%2520on%2520average%2520across%2520%255Ctextbf%257B9%257D%2520benchmarks%252C%2520respectively.%2520Using%250Aonly%2520one-fifth%2520of%2520the%2520context%2520length%252C%2520DeepStack%2520rivals%2520closely%2520to%2520the%250Acounterparts%2520that%2520use%2520the%2520full%2520context%2520length.%2520These%2520gains%2520are%2520particularly%250Apronounced%2520on%2520high-resolution%2520tasks%252C%2520e.g.%252C%2520%255Ctextbf%257B4.2%257D%252C%2520%255Ctextbf%257B11.0%257D%252C%2520and%250A%255Ctextbf%257B4.0%257D%2520improvements%2520on%2520TextVQA%252C%2520DocVQA%252C%2520and%2520InfoVQA%2520compared%2520to%250ALLaVA-1.5-7B%252C%2520respectively.%2520We%2520further%2520apply%2520DeepStack%2520to%2520vision%2520transformer%250Alayers%252C%2520which%2520brings%2520us%2520a%2520similar%2520amount%2520of%2520improvements%252C%2520%255Ctextbf%257B3.8%257D%2520on%250Aaverage%2520compared%2520with%2520LLaVA-1.5-7B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepStack%3A%20Deeply%20Stacking%20Visual%20Tokens%20is%20Surprisingly%20Simple%20and%0A%20%20Effective%20for%20LMMs&entry.906535625=Lingchen%20Meng%20and%20Jianwei%20Yang%20and%20Rui%20Tian%20and%20Xiyang%20Dai%20and%20Zuxuan%20Wu%20and%20Jianfeng%20Gao%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Most%20large%20multimodal%20models%20%28LMMs%29%20are%20implemented%20by%20feeding%20visual%20tokens%0Aas%20a%20sequence%20into%20the%20first%20layer%20of%20a%20large%20language%20model%20%28LLM%29.%20The%0Aresulting%20architecture%20is%20simple%20but%20significantly%20increases%20computation%20and%0Amemory%20costs%2C%20as%20it%20has%20to%20handle%20a%20large%20number%20of%20additional%20tokens%20in%20its%0Ainput%20layer.%20This%20paper%20presents%20a%20new%20architecture%20DeepStack%20for%20LMMs.%0AConsidering%20%24N%24%20layers%20in%20the%20language%20and%20vision%20transformer%20of%20LMMs%2C%20we%20stack%0Athe%20visual%20tokens%20into%20%24N%24%20groups%20and%20feed%20each%20group%20to%20its%20aligned%0Atransformer%20layer%20%5Ctextit%7Bfrom%20bottom%20to%20top%7D.%20Surprisingly%2C%20this%20simple%20method%0Agreatly%20enhances%20the%20power%20of%20LMMs%20to%20model%20interactions%20among%20visual%20tokens%0Aacross%20layers%20but%20with%20minimal%20additional%20cost.%20We%20apply%20DeepStack%20to%20both%0Alanguage%20and%20vision%20transformer%20in%20LMMs%2C%20and%20validate%20the%20effectiveness%20of%0ADeepStack%20LMMs%20with%20extensive%20empirical%20results.%20Using%20the%20same%20context%20length%2C%0Aour%20DeepStack%207B%20and%2013B%20parameters%20surpass%20their%20counterparts%20by%20%5Ctextbf%7B2.7%7D%0Aand%20%5Ctextbf%7B2.9%7D%20on%20average%20across%20%5Ctextbf%7B9%7D%20benchmarks%2C%20respectively.%20Using%0Aonly%20one-fifth%20of%20the%20context%20length%2C%20DeepStack%20rivals%20closely%20to%20the%0Acounterparts%20that%20use%20the%20full%20context%20length.%20These%20gains%20are%20particularly%0Apronounced%20on%20high-resolution%20tasks%2C%20e.g.%2C%20%5Ctextbf%7B4.2%7D%2C%20%5Ctextbf%7B11.0%7D%2C%20and%0A%5Ctextbf%7B4.0%7D%20improvements%20on%20TextVQA%2C%20DocVQA%2C%20and%20InfoVQA%20compared%20to%0ALLaVA-1.5-7B%2C%20respectively.%20We%20further%20apply%20DeepStack%20to%20vision%20transformer%0Alayers%2C%20which%20brings%20us%20a%20similar%20amount%20of%20improvements%2C%20%5Ctextbf%7B3.8%7D%20on%0Aaverage%20compared%20with%20LLaVA-1.5-7B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04334v1&entry.124074799=Read"},
{"title": "Leveraging Temporal Graph Networks Using Module Decoupling", "author": "Or Feldman and Chaim Baskin", "abstract": "  Modern approaches for learning on dynamic graphs have adopted the use of\nbatches instead of applying updates one by one. The use of batches allows these\ntechniques to become helpful in streaming scenarios where updates to graphs are\nreceived at extreme speeds. Using batches, however, forces the models to update\ninfrequently, which results in the degradation of their performance. In this\nwork, we suggest a decoupling strategy that enables the models to update\nfrequently while using batches. By decoupling the core modules of temporal\ngraph networks and implementing them using a minimal number of learnable\nparameters, we have developed the Lightweight Decoupled Temporal Graph Network\n(LDTGN), an exceptionally efficient model for learning on dynamic graphs. LDTG\nwas validated on various dynamic graph benchmarks, providing comparable or\nstate-of-the-art results with significantly higher throughput than previous\nart. Notably, our method outperforms previous approaches by more than 20\\% on\nbenchmarks that require rapid model update rates, such as USLegis or UNTrade.\nThe code to reproduce our experiments is available at\n\\href{https://orfeld415.github.io/module-decoupling}{this http url}.\n", "link": "http://arxiv.org/abs/2310.02721v2", "date": "2024-06-06", "relevancy": 2.1007, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5545}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5049}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Temporal%20Graph%20Networks%20Using%20Module%20Decoupling&body=Title%3A%20Leveraging%20Temporal%20Graph%20Networks%20Using%20Module%20Decoupling%0AAuthor%3A%20Or%20Feldman%20and%20Chaim%20Baskin%0AAbstract%3A%20%20%20Modern%20approaches%20for%20learning%20on%20dynamic%20graphs%20have%20adopted%20the%20use%20of%0Abatches%20instead%20of%20applying%20updates%20one%20by%20one.%20The%20use%20of%20batches%20allows%20these%0Atechniques%20to%20become%20helpful%20in%20streaming%20scenarios%20where%20updates%20to%20graphs%20are%0Areceived%20at%20extreme%20speeds.%20Using%20batches%2C%20however%2C%20forces%20the%20models%20to%20update%0Ainfrequently%2C%20which%20results%20in%20the%20degradation%20of%20their%20performance.%20In%20this%0Awork%2C%20we%20suggest%20a%20decoupling%20strategy%20that%20enables%20the%20models%20to%20update%0Afrequently%20while%20using%20batches.%20By%20decoupling%20the%20core%20modules%20of%20temporal%0Agraph%20networks%20and%20implementing%20them%20using%20a%20minimal%20number%20of%20learnable%0Aparameters%2C%20we%20have%20developed%20the%20Lightweight%20Decoupled%20Temporal%20Graph%20Network%0A%28LDTGN%29%2C%20an%20exceptionally%20efficient%20model%20for%20learning%20on%20dynamic%20graphs.%20LDTG%0Awas%20validated%20on%20various%20dynamic%20graph%20benchmarks%2C%20providing%20comparable%20or%0Astate-of-the-art%20results%20with%20significantly%20higher%20throughput%20than%20previous%0Aart.%20Notably%2C%20our%20method%20outperforms%20previous%20approaches%20by%20more%20than%2020%5C%25%20on%0Abenchmarks%20that%20require%20rapid%20model%20update%20rates%2C%20such%20as%20USLegis%20or%20UNTrade.%0AThe%20code%20to%20reproduce%20our%20experiments%20is%20available%20at%0A%5Chref%7Bhttps%3A//orfeld415.github.io/module-decoupling%7D%7Bthis%20http%20url%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02721v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Temporal%2520Graph%2520Networks%2520Using%2520Module%2520Decoupling%26entry.906535625%3DOr%2520Feldman%2520and%2520Chaim%2520Baskin%26entry.1292438233%3D%2520%2520Modern%2520approaches%2520for%2520learning%2520on%2520dynamic%2520graphs%2520have%2520adopted%2520the%2520use%2520of%250Abatches%2520instead%2520of%2520applying%2520updates%2520one%2520by%2520one.%2520The%2520use%2520of%2520batches%2520allows%2520these%250Atechniques%2520to%2520become%2520helpful%2520in%2520streaming%2520scenarios%2520where%2520updates%2520to%2520graphs%2520are%250Areceived%2520at%2520extreme%2520speeds.%2520Using%2520batches%252C%2520however%252C%2520forces%2520the%2520models%2520to%2520update%250Ainfrequently%252C%2520which%2520results%2520in%2520the%2520degradation%2520of%2520their%2520performance.%2520In%2520this%250Awork%252C%2520we%2520suggest%2520a%2520decoupling%2520strategy%2520that%2520enables%2520the%2520models%2520to%2520update%250Afrequently%2520while%2520using%2520batches.%2520By%2520decoupling%2520the%2520core%2520modules%2520of%2520temporal%250Agraph%2520networks%2520and%2520implementing%2520them%2520using%2520a%2520minimal%2520number%2520of%2520learnable%250Aparameters%252C%2520we%2520have%2520developed%2520the%2520Lightweight%2520Decoupled%2520Temporal%2520Graph%2520Network%250A%2528LDTGN%2529%252C%2520an%2520exceptionally%2520efficient%2520model%2520for%2520learning%2520on%2520dynamic%2520graphs.%2520LDTG%250Awas%2520validated%2520on%2520various%2520dynamic%2520graph%2520benchmarks%252C%2520providing%2520comparable%2520or%250Astate-of-the-art%2520results%2520with%2520significantly%2520higher%2520throughput%2520than%2520previous%250Aart.%2520Notably%252C%2520our%2520method%2520outperforms%2520previous%2520approaches%2520by%2520more%2520than%252020%255C%2525%2520on%250Abenchmarks%2520that%2520require%2520rapid%2520model%2520update%2520rates%252C%2520such%2520as%2520USLegis%2520or%2520UNTrade.%250AThe%2520code%2520to%2520reproduce%2520our%2520experiments%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//orfeld415.github.io/module-decoupling%257D%257Bthis%2520http%2520url%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02721v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Temporal%20Graph%20Networks%20Using%20Module%20Decoupling&entry.906535625=Or%20Feldman%20and%20Chaim%20Baskin&entry.1292438233=%20%20Modern%20approaches%20for%20learning%20on%20dynamic%20graphs%20have%20adopted%20the%20use%20of%0Abatches%20instead%20of%20applying%20updates%20one%20by%20one.%20The%20use%20of%20batches%20allows%20these%0Atechniques%20to%20become%20helpful%20in%20streaming%20scenarios%20where%20updates%20to%20graphs%20are%0Areceived%20at%20extreme%20speeds.%20Using%20batches%2C%20however%2C%20forces%20the%20models%20to%20update%0Ainfrequently%2C%20which%20results%20in%20the%20degradation%20of%20their%20performance.%20In%20this%0Awork%2C%20we%20suggest%20a%20decoupling%20strategy%20that%20enables%20the%20models%20to%20update%0Afrequently%20while%20using%20batches.%20By%20decoupling%20the%20core%20modules%20of%20temporal%0Agraph%20networks%20and%20implementing%20them%20using%20a%20minimal%20number%20of%20learnable%0Aparameters%2C%20we%20have%20developed%20the%20Lightweight%20Decoupled%20Temporal%20Graph%20Network%0A%28LDTGN%29%2C%20an%20exceptionally%20efficient%20model%20for%20learning%20on%20dynamic%20graphs.%20LDTG%0Awas%20validated%20on%20various%20dynamic%20graph%20benchmarks%2C%20providing%20comparable%20or%0Astate-of-the-art%20results%20with%20significantly%20higher%20throughput%20than%20previous%0Aart.%20Notably%2C%20our%20method%20outperforms%20previous%20approaches%20by%20more%20than%2020%5C%25%20on%0Abenchmarks%20that%20require%20rapid%20model%20update%20rates%2C%20such%20as%20USLegis%20or%20UNTrade.%0AThe%20code%20to%20reproduce%20our%20experiments%20is%20available%20at%0A%5Chref%7Bhttps%3A//orfeld415.github.io/module-decoupling%7D%7Bthis%20http%20url%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02721v2&entry.124074799=Read"},
{"title": "3rd Place Solution for PVUW Challenge 2024: Video Panoptic Segmentation", "author": "Ruipu Wu and Jifei Che and Han Li and Chengjing Wu and Ting Liu and Luoqi Liu", "abstract": "  Video panoptic segmentation is an advanced task that extends panoptic\nsegmentation by applying its concept to video sequences. In the hope of\naddressing the challenge of video panoptic segmentation in diverse conditions,\nWe utilize DVIS++ as our baseline model and enhance it by introducing a\ncomprehensive approach centered on the query-wise ensemble, supplemented by\nadditional techniques. Our proposed approach achieved a VPQ score of 57.01 on\nthe VIPSeg test set, and ranked 3rd in the VPS track of the 3rd Pixel-level\nVideo Understanding in the Wild Challenge.\n", "link": "http://arxiv.org/abs/2406.04002v1", "date": "2024-06-06", "relevancy": 2.0948, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5567}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5026}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4991}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203rd%20Place%20Solution%20for%20PVUW%20Challenge%202024%3A%20Video%20Panoptic%20Segmentation&body=Title%3A%203rd%20Place%20Solution%20for%20PVUW%20Challenge%202024%3A%20Video%20Panoptic%20Segmentation%0AAuthor%3A%20Ruipu%20Wu%20and%20Jifei%20Che%20and%20Han%20Li%20and%20Chengjing%20Wu%20and%20Ting%20Liu%20and%20Luoqi%20Liu%0AAbstract%3A%20%20%20Video%20panoptic%20segmentation%20is%20an%20advanced%20task%20that%20extends%20panoptic%0Asegmentation%20by%20applying%20its%20concept%20to%20video%20sequences.%20In%20the%20hope%20of%0Aaddressing%20the%20challenge%20of%20video%20panoptic%20segmentation%20in%20diverse%20conditions%2C%0AWe%20utilize%20DVIS%2B%2B%20as%20our%20baseline%20model%20and%20enhance%20it%20by%20introducing%20a%0Acomprehensive%20approach%20centered%20on%20the%20query-wise%20ensemble%2C%20supplemented%20by%0Aadditional%20techniques.%20Our%20proposed%20approach%20achieved%20a%20VPQ%20score%20of%2057.01%20on%0Athe%20VIPSeg%20test%20set%2C%20and%20ranked%203rd%20in%20the%20VPS%20track%20of%20the%203rd%20Pixel-level%0AVideo%20Understanding%20in%20the%20Wild%20Challenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04002v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3rd%2520Place%2520Solution%2520for%2520PVUW%2520Challenge%25202024%253A%2520Video%2520Panoptic%2520Segmentation%26entry.906535625%3DRuipu%2520Wu%2520and%2520Jifei%2520Che%2520and%2520Han%2520Li%2520and%2520Chengjing%2520Wu%2520and%2520Ting%2520Liu%2520and%2520Luoqi%2520Liu%26entry.1292438233%3D%2520%2520Video%2520panoptic%2520segmentation%2520is%2520an%2520advanced%2520task%2520that%2520extends%2520panoptic%250Asegmentation%2520by%2520applying%2520its%2520concept%2520to%2520video%2520sequences.%2520In%2520the%2520hope%2520of%250Aaddressing%2520the%2520challenge%2520of%2520video%2520panoptic%2520segmentation%2520in%2520diverse%2520conditions%252C%250AWe%2520utilize%2520DVIS%252B%252B%2520as%2520our%2520baseline%2520model%2520and%2520enhance%2520it%2520by%2520introducing%2520a%250Acomprehensive%2520approach%2520centered%2520on%2520the%2520query-wise%2520ensemble%252C%2520supplemented%2520by%250Aadditional%2520techniques.%2520Our%2520proposed%2520approach%2520achieved%2520a%2520VPQ%2520score%2520of%252057.01%2520on%250Athe%2520VIPSeg%2520test%2520set%252C%2520and%2520ranked%25203rd%2520in%2520the%2520VPS%2520track%2520of%2520the%25203rd%2520Pixel-level%250AVideo%2520Understanding%2520in%2520the%2520Wild%2520Challenge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04002v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3rd%20Place%20Solution%20for%20PVUW%20Challenge%202024%3A%20Video%20Panoptic%20Segmentation&entry.906535625=Ruipu%20Wu%20and%20Jifei%20Che%20and%20Han%20Li%20and%20Chengjing%20Wu%20and%20Ting%20Liu%20and%20Luoqi%20Liu&entry.1292438233=%20%20Video%20panoptic%20segmentation%20is%20an%20advanced%20task%20that%20extends%20panoptic%0Asegmentation%20by%20applying%20its%20concept%20to%20video%20sequences.%20In%20the%20hope%20of%0Aaddressing%20the%20challenge%20of%20video%20panoptic%20segmentation%20in%20diverse%20conditions%2C%0AWe%20utilize%20DVIS%2B%2B%20as%20our%20baseline%20model%20and%20enhance%20it%20by%20introducing%20a%0Acomprehensive%20approach%20centered%20on%20the%20query-wise%20ensemble%2C%20supplemented%20by%0Aadditional%20techniques.%20Our%20proposed%20approach%20achieved%20a%20VPQ%20score%20of%2057.01%20on%0Athe%20VIPSeg%20test%20set%2C%20and%20ranked%203rd%20in%20the%20VPS%20track%20of%20the%203rd%20Pixel-level%0AVideo%20Understanding%20in%20the%20Wild%20Challenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04002v1&entry.124074799=Read"},
{"title": "MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding", "author": "Junjie Zhou and Yan Shu and Bo Zhao and Boya Wu and Shitao Xiao and Xi Yang and Yongping Xiong and Bo Zhang and Tiejun Huang and Zheng Liu", "abstract": "  The evaluation of Long Video Understanding (LVU) performance poses an\nimportant but challenging research problem. Despite previous efforts, the\nexisting video understanding benchmarks are severely constrained by several\nissues, especially the insufficient lengths of videos, a lack of diversity in\nvideo types and evaluation tasks, and the inappropriateness for evaluating LVU\nperformances. To address the above problems, we propose a new benchmark, called\nMLVU (Multi-task Long Video Understanding Benchmark), for the comprehensive and\nin-depth evaluation of LVU. MLVU presents the following critical values: 1) The\nsubstantial and flexible extension of video lengths, which enables the\nbenchmark to evaluate LVU performance across a wide range of durations. 2) The\ninclusion of various video genres, e.g., movies, surveillance footage,\negocentric videos, cartoons, game videos, etc., which reflects the models' LVU\nperformances in different scenarios. 3) The development of diversified\nevaluation tasks, which enables a comprehensive examination of MLLMs' key\nabilities in long-video understanding. The empirical study with 20 latest MLLMs\nreveals significant room for improvement in today's technique, as all existing\nmethods struggle with most of the evaluation tasks and exhibit severe\nperformance degradation when handling longer videos. Additionally, it suggests\nthat factors such as context length, image-understanding quality, and the\nchoice of LLM backbone can play critical roles in future advancements. We\nanticipate that MLVU will advance the research of long video understanding by\nproviding a comprehensive and in-depth analysis of MLLMs.\n", "link": "http://arxiv.org/abs/2406.04264v1", "date": "2024-06-06", "relevancy": 2.0906, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5674}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5162}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLVU%3A%20A%20Comprehensive%20Benchmark%20for%20Multi-Task%20Long%20Video%20Understanding&body=Title%3A%20MLVU%3A%20A%20Comprehensive%20Benchmark%20for%20Multi-Task%20Long%20Video%20Understanding%0AAuthor%3A%20Junjie%20Zhou%20and%20Yan%20Shu%20and%20Bo%20Zhao%20and%20Boya%20Wu%20and%20Shitao%20Xiao%20and%20Xi%20Yang%20and%20Yongping%20Xiong%20and%20Bo%20Zhang%20and%20Tiejun%20Huang%20and%20Zheng%20Liu%0AAbstract%3A%20%20%20The%20evaluation%20of%20Long%20Video%20Understanding%20%28LVU%29%20performance%20poses%20an%0Aimportant%20but%20challenging%20research%20problem.%20Despite%20previous%20efforts%2C%20the%0Aexisting%20video%20understanding%20benchmarks%20are%20severely%20constrained%20by%20several%0Aissues%2C%20especially%20the%20insufficient%20lengths%20of%20videos%2C%20a%20lack%20of%20diversity%20in%0Avideo%20types%20and%20evaluation%20tasks%2C%20and%20the%20inappropriateness%20for%20evaluating%20LVU%0Aperformances.%20To%20address%20the%20above%20problems%2C%20we%20propose%20a%20new%20benchmark%2C%20called%0AMLVU%20%28Multi-task%20Long%20Video%20Understanding%20Benchmark%29%2C%20for%20the%20comprehensive%20and%0Ain-depth%20evaluation%20of%20LVU.%20MLVU%20presents%20the%20following%20critical%20values%3A%201%29%20The%0Asubstantial%20and%20flexible%20extension%20of%20video%20lengths%2C%20which%20enables%20the%0Abenchmark%20to%20evaluate%20LVU%20performance%20across%20a%20wide%20range%20of%20durations.%202%29%20The%0Ainclusion%20of%20various%20video%20genres%2C%20e.g.%2C%20movies%2C%20surveillance%20footage%2C%0Aegocentric%20videos%2C%20cartoons%2C%20game%20videos%2C%20etc.%2C%20which%20reflects%20the%20models%27%20LVU%0Aperformances%20in%20different%20scenarios.%203%29%20The%20development%20of%20diversified%0Aevaluation%20tasks%2C%20which%20enables%20a%20comprehensive%20examination%20of%20MLLMs%27%20key%0Aabilities%20in%20long-video%20understanding.%20The%20empirical%20study%20with%2020%20latest%20MLLMs%0Areveals%20significant%20room%20for%20improvement%20in%20today%27s%20technique%2C%20as%20all%20existing%0Amethods%20struggle%20with%20most%20of%20the%20evaluation%20tasks%20and%20exhibit%20severe%0Aperformance%20degradation%20when%20handling%20longer%20videos.%20Additionally%2C%20it%20suggests%0Athat%20factors%20such%20as%20context%20length%2C%20image-understanding%20quality%2C%20and%20the%0Achoice%20of%20LLM%20backbone%20can%20play%20critical%20roles%20in%20future%20advancements.%20We%0Aanticipate%20that%20MLVU%20will%20advance%20the%20research%20of%20long%20video%20understanding%20by%0Aproviding%20a%20comprehensive%20and%20in-depth%20analysis%20of%20MLLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04264v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLVU%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Multi-Task%2520Long%2520Video%2520Understanding%26entry.906535625%3DJunjie%2520Zhou%2520and%2520Yan%2520Shu%2520and%2520Bo%2520Zhao%2520and%2520Boya%2520Wu%2520and%2520Shitao%2520Xiao%2520and%2520Xi%2520Yang%2520and%2520Yongping%2520Xiong%2520and%2520Bo%2520Zhang%2520and%2520Tiejun%2520Huang%2520and%2520Zheng%2520Liu%26entry.1292438233%3D%2520%2520The%2520evaluation%2520of%2520Long%2520Video%2520Understanding%2520%2528LVU%2529%2520performance%2520poses%2520an%250Aimportant%2520but%2520challenging%2520research%2520problem.%2520Despite%2520previous%2520efforts%252C%2520the%250Aexisting%2520video%2520understanding%2520benchmarks%2520are%2520severely%2520constrained%2520by%2520several%250Aissues%252C%2520especially%2520the%2520insufficient%2520lengths%2520of%2520videos%252C%2520a%2520lack%2520of%2520diversity%2520in%250Avideo%2520types%2520and%2520evaluation%2520tasks%252C%2520and%2520the%2520inappropriateness%2520for%2520evaluating%2520LVU%250Aperformances.%2520To%2520address%2520the%2520above%2520problems%252C%2520we%2520propose%2520a%2520new%2520benchmark%252C%2520called%250AMLVU%2520%2528Multi-task%2520Long%2520Video%2520Understanding%2520Benchmark%2529%252C%2520for%2520the%2520comprehensive%2520and%250Ain-depth%2520evaluation%2520of%2520LVU.%2520MLVU%2520presents%2520the%2520following%2520critical%2520values%253A%25201%2529%2520The%250Asubstantial%2520and%2520flexible%2520extension%2520of%2520video%2520lengths%252C%2520which%2520enables%2520the%250Abenchmark%2520to%2520evaluate%2520LVU%2520performance%2520across%2520a%2520wide%2520range%2520of%2520durations.%25202%2529%2520The%250Ainclusion%2520of%2520various%2520video%2520genres%252C%2520e.g.%252C%2520movies%252C%2520surveillance%2520footage%252C%250Aegocentric%2520videos%252C%2520cartoons%252C%2520game%2520videos%252C%2520etc.%252C%2520which%2520reflects%2520the%2520models%2527%2520LVU%250Aperformances%2520in%2520different%2520scenarios.%25203%2529%2520The%2520development%2520of%2520diversified%250Aevaluation%2520tasks%252C%2520which%2520enables%2520a%2520comprehensive%2520examination%2520of%2520MLLMs%2527%2520key%250Aabilities%2520in%2520long-video%2520understanding.%2520The%2520empirical%2520study%2520with%252020%2520latest%2520MLLMs%250Areveals%2520significant%2520room%2520for%2520improvement%2520in%2520today%2527s%2520technique%252C%2520as%2520all%2520existing%250Amethods%2520struggle%2520with%2520most%2520of%2520the%2520evaluation%2520tasks%2520and%2520exhibit%2520severe%250Aperformance%2520degradation%2520when%2520handling%2520longer%2520videos.%2520Additionally%252C%2520it%2520suggests%250Athat%2520factors%2520such%2520as%2520context%2520length%252C%2520image-understanding%2520quality%252C%2520and%2520the%250Achoice%2520of%2520LLM%2520backbone%2520can%2520play%2520critical%2520roles%2520in%2520future%2520advancements.%2520We%250Aanticipate%2520that%2520MLVU%2520will%2520advance%2520the%2520research%2520of%2520long%2520video%2520understanding%2520by%250Aproviding%2520a%2520comprehensive%2520and%2520in-depth%2520analysis%2520of%2520MLLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04264v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLVU%3A%20A%20Comprehensive%20Benchmark%20for%20Multi-Task%20Long%20Video%20Understanding&entry.906535625=Junjie%20Zhou%20and%20Yan%20Shu%20and%20Bo%20Zhao%20and%20Boya%20Wu%20and%20Shitao%20Xiao%20and%20Xi%20Yang%20and%20Yongping%20Xiong%20and%20Bo%20Zhang%20and%20Tiejun%20Huang%20and%20Zheng%20Liu&entry.1292438233=%20%20The%20evaluation%20of%20Long%20Video%20Understanding%20%28LVU%29%20performance%20poses%20an%0Aimportant%20but%20challenging%20research%20problem.%20Despite%20previous%20efforts%2C%20the%0Aexisting%20video%20understanding%20benchmarks%20are%20severely%20constrained%20by%20several%0Aissues%2C%20especially%20the%20insufficient%20lengths%20of%20videos%2C%20a%20lack%20of%20diversity%20in%0Avideo%20types%20and%20evaluation%20tasks%2C%20and%20the%20inappropriateness%20for%20evaluating%20LVU%0Aperformances.%20To%20address%20the%20above%20problems%2C%20we%20propose%20a%20new%20benchmark%2C%20called%0AMLVU%20%28Multi-task%20Long%20Video%20Understanding%20Benchmark%29%2C%20for%20the%20comprehensive%20and%0Ain-depth%20evaluation%20of%20LVU.%20MLVU%20presents%20the%20following%20critical%20values%3A%201%29%20The%0Asubstantial%20and%20flexible%20extension%20of%20video%20lengths%2C%20which%20enables%20the%0Abenchmark%20to%20evaluate%20LVU%20performance%20across%20a%20wide%20range%20of%20durations.%202%29%20The%0Ainclusion%20of%20various%20video%20genres%2C%20e.g.%2C%20movies%2C%20surveillance%20footage%2C%0Aegocentric%20videos%2C%20cartoons%2C%20game%20videos%2C%20etc.%2C%20which%20reflects%20the%20models%27%20LVU%0Aperformances%20in%20different%20scenarios.%203%29%20The%20development%20of%20diversified%0Aevaluation%20tasks%2C%20which%20enables%20a%20comprehensive%20examination%20of%20MLLMs%27%20key%0Aabilities%20in%20long-video%20understanding.%20The%20empirical%20study%20with%2020%20latest%20MLLMs%0Areveals%20significant%20room%20for%20improvement%20in%20today%27s%20technique%2C%20as%20all%20existing%0Amethods%20struggle%20with%20most%20of%20the%20evaluation%20tasks%20and%20exhibit%20severe%0Aperformance%20degradation%20when%20handling%20longer%20videos.%20Additionally%2C%20it%20suggests%0Athat%20factors%20such%20as%20context%20length%2C%20image-understanding%20quality%2C%20and%20the%0Achoice%20of%20LLM%20backbone%20can%20play%20critical%20roles%20in%20future%20advancements.%20We%0Aanticipate%20that%20MLVU%20will%20advance%20the%20research%20of%20long%20video%20understanding%20by%0Aproviding%20a%20comprehensive%20and%20in-depth%20analysis%20of%20MLLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04264v1&entry.124074799=Read"},
{"title": "CDMamba: Remote Sensing Image Change Detection with Mamba", "author": "Haotian Zhang and Keyan Chen and Chenyang Liu and Hao Chen and Zhengxia Zou and Zhenwei Shi", "abstract": "  Recently, the Mamba architecture based on state space models has demonstrated\nremarkable performance in a series of natural language processing tasks and has\nbeen rapidly applied to remote sensing change detection (CD) tasks. However,\nmost methods enhance the global receptive field by directly modifying the\nscanning mode of Mamba, neglecting the crucial role that local information\nplays in dense prediction tasks (e.g., CD). In this article, we propose a model\ncalled CDMamba, which effectively combines global and local features for\nhandling CD tasks. Specifically, the Scaled Residual ConvMamba (SRCM) block is\nproposed to utilize the ability of Mamba to extract global features and\nconvolution to enhance the local details, to alleviate the issue that current\nMamba-based methods lack detailed clues and are difficult to achieve fine\ndetection in dense prediction tasks. Furthermore, considering the\ncharacteristics of bi-temporal feature interaction required for CD, the\nAdaptive Global Local Guided Fusion (AGLGF) block is proposed to dynamically\nfacilitate the bi-temporal interaction guided by other temporal global/local\nfeatures. Our intuition is that more discriminative change features can be\nacquired with the guidance of other temporal features. Extensive experiments on\nthree datasets demonstrate that our proposed CDMamba outperforms the current\nstate-of-the-art methods. Our code will be open-sourced at\nhttps://github.com/zmoka-zht/CDMamba.\n", "link": "http://arxiv.org/abs/2406.04207v1", "date": "2024-06-06", "relevancy": 2.0769, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5216}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5187}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CDMamba%3A%20Remote%20Sensing%20Image%20Change%20Detection%20with%20Mamba&body=Title%3A%20CDMamba%3A%20Remote%20Sensing%20Image%20Change%20Detection%20with%20Mamba%0AAuthor%3A%20Haotian%20Zhang%20and%20Keyan%20Chen%20and%20Chenyang%20Liu%20and%20Hao%20Chen%20and%20Zhengxia%20Zou%20and%20Zhenwei%20Shi%0AAbstract%3A%20%20%20Recently%2C%20the%20Mamba%20architecture%20based%20on%20state%20space%20models%20has%20demonstrated%0Aremarkable%20performance%20in%20a%20series%20of%20natural%20language%20processing%20tasks%20and%20has%0Abeen%20rapidly%20applied%20to%20remote%20sensing%20change%20detection%20%28CD%29%20tasks.%20However%2C%0Amost%20methods%20enhance%20the%20global%20receptive%20field%20by%20directly%20modifying%20the%0Ascanning%20mode%20of%20Mamba%2C%20neglecting%20the%20crucial%20role%20that%20local%20information%0Aplays%20in%20dense%20prediction%20tasks%20%28e.g.%2C%20CD%29.%20In%20this%20article%2C%20we%20propose%20a%20model%0Acalled%20CDMamba%2C%20which%20effectively%20combines%20global%20and%20local%20features%20for%0Ahandling%20CD%20tasks.%20Specifically%2C%20the%20Scaled%20Residual%20ConvMamba%20%28SRCM%29%20block%20is%0Aproposed%20to%20utilize%20the%20ability%20of%20Mamba%20to%20extract%20global%20features%20and%0Aconvolution%20to%20enhance%20the%20local%20details%2C%20to%20alleviate%20the%20issue%20that%20current%0AMamba-based%20methods%20lack%20detailed%20clues%20and%20are%20difficult%20to%20achieve%20fine%0Adetection%20in%20dense%20prediction%20tasks.%20Furthermore%2C%20considering%20the%0Acharacteristics%20of%20bi-temporal%20feature%20interaction%20required%20for%20CD%2C%20the%0AAdaptive%20Global%20Local%20Guided%20Fusion%20%28AGLGF%29%20block%20is%20proposed%20to%20dynamically%0Afacilitate%20the%20bi-temporal%20interaction%20guided%20by%20other%20temporal%20global/local%0Afeatures.%20Our%20intuition%20is%20that%20more%20discriminative%20change%20features%20can%20be%0Aacquired%20with%20the%20guidance%20of%20other%20temporal%20features.%20Extensive%20experiments%20on%0Athree%20datasets%20demonstrate%20that%20our%20proposed%20CDMamba%20outperforms%20the%20current%0Astate-of-the-art%20methods.%20Our%20code%20will%20be%20open-sourced%20at%0Ahttps%3A//github.com/zmoka-zht/CDMamba.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCDMamba%253A%2520Remote%2520Sensing%2520Image%2520Change%2520Detection%2520with%2520Mamba%26entry.906535625%3DHaotian%2520Zhang%2520and%2520Keyan%2520Chen%2520and%2520Chenyang%2520Liu%2520and%2520Hao%2520Chen%2520and%2520Zhengxia%2520Zou%2520and%2520Zhenwei%2520Shi%26entry.1292438233%3D%2520%2520Recently%252C%2520the%2520Mamba%2520architecture%2520based%2520on%2520state%2520space%2520models%2520has%2520demonstrated%250Aremarkable%2520performance%2520in%2520a%2520series%2520of%2520natural%2520language%2520processing%2520tasks%2520and%2520has%250Abeen%2520rapidly%2520applied%2520to%2520remote%2520sensing%2520change%2520detection%2520%2528CD%2529%2520tasks.%2520However%252C%250Amost%2520methods%2520enhance%2520the%2520global%2520receptive%2520field%2520by%2520directly%2520modifying%2520the%250Ascanning%2520mode%2520of%2520Mamba%252C%2520neglecting%2520the%2520crucial%2520role%2520that%2520local%2520information%250Aplays%2520in%2520dense%2520prediction%2520tasks%2520%2528e.g.%252C%2520CD%2529.%2520In%2520this%2520article%252C%2520we%2520propose%2520a%2520model%250Acalled%2520CDMamba%252C%2520which%2520effectively%2520combines%2520global%2520and%2520local%2520features%2520for%250Ahandling%2520CD%2520tasks.%2520Specifically%252C%2520the%2520Scaled%2520Residual%2520ConvMamba%2520%2528SRCM%2529%2520block%2520is%250Aproposed%2520to%2520utilize%2520the%2520ability%2520of%2520Mamba%2520to%2520extract%2520global%2520features%2520and%250Aconvolution%2520to%2520enhance%2520the%2520local%2520details%252C%2520to%2520alleviate%2520the%2520issue%2520that%2520current%250AMamba-based%2520methods%2520lack%2520detailed%2520clues%2520and%2520are%2520difficult%2520to%2520achieve%2520fine%250Adetection%2520in%2520dense%2520prediction%2520tasks.%2520Furthermore%252C%2520considering%2520the%250Acharacteristics%2520of%2520bi-temporal%2520feature%2520interaction%2520required%2520for%2520CD%252C%2520the%250AAdaptive%2520Global%2520Local%2520Guided%2520Fusion%2520%2528AGLGF%2529%2520block%2520is%2520proposed%2520to%2520dynamically%250Afacilitate%2520the%2520bi-temporal%2520interaction%2520guided%2520by%2520other%2520temporal%2520global/local%250Afeatures.%2520Our%2520intuition%2520is%2520that%2520more%2520discriminative%2520change%2520features%2520can%2520be%250Aacquired%2520with%2520the%2520guidance%2520of%2520other%2520temporal%2520features.%2520Extensive%2520experiments%2520on%250Athree%2520datasets%2520demonstrate%2520that%2520our%2520proposed%2520CDMamba%2520outperforms%2520the%2520current%250Astate-of-the-art%2520methods.%2520Our%2520code%2520will%2520be%2520open-sourced%2520at%250Ahttps%253A//github.com/zmoka-zht/CDMamba.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CDMamba%3A%20Remote%20Sensing%20Image%20Change%20Detection%20with%20Mamba&entry.906535625=Haotian%20Zhang%20and%20Keyan%20Chen%20and%20Chenyang%20Liu%20and%20Hao%20Chen%20and%20Zhengxia%20Zou%20and%20Zhenwei%20Shi&entry.1292438233=%20%20Recently%2C%20the%20Mamba%20architecture%20based%20on%20state%20space%20models%20has%20demonstrated%0Aremarkable%20performance%20in%20a%20series%20of%20natural%20language%20processing%20tasks%20and%20has%0Abeen%20rapidly%20applied%20to%20remote%20sensing%20change%20detection%20%28CD%29%20tasks.%20However%2C%0Amost%20methods%20enhance%20the%20global%20receptive%20field%20by%20directly%20modifying%20the%0Ascanning%20mode%20of%20Mamba%2C%20neglecting%20the%20crucial%20role%20that%20local%20information%0Aplays%20in%20dense%20prediction%20tasks%20%28e.g.%2C%20CD%29.%20In%20this%20article%2C%20we%20propose%20a%20model%0Acalled%20CDMamba%2C%20which%20effectively%20combines%20global%20and%20local%20features%20for%0Ahandling%20CD%20tasks.%20Specifically%2C%20the%20Scaled%20Residual%20ConvMamba%20%28SRCM%29%20block%20is%0Aproposed%20to%20utilize%20the%20ability%20of%20Mamba%20to%20extract%20global%20features%20and%0Aconvolution%20to%20enhance%20the%20local%20details%2C%20to%20alleviate%20the%20issue%20that%20current%0AMamba-based%20methods%20lack%20detailed%20clues%20and%20are%20difficult%20to%20achieve%20fine%0Adetection%20in%20dense%20prediction%20tasks.%20Furthermore%2C%20considering%20the%0Acharacteristics%20of%20bi-temporal%20feature%20interaction%20required%20for%20CD%2C%20the%0AAdaptive%20Global%20Local%20Guided%20Fusion%20%28AGLGF%29%20block%20is%20proposed%20to%20dynamically%0Afacilitate%20the%20bi-temporal%20interaction%20guided%20by%20other%20temporal%20global/local%0Afeatures.%20Our%20intuition%20is%20that%20more%20discriminative%20change%20features%20can%20be%0Aacquired%20with%20the%20guidance%20of%20other%20temporal%20features.%20Extensive%20experiments%20on%0Athree%20datasets%20demonstrate%20that%20our%20proposed%20CDMamba%20outperforms%20the%20current%0Astate-of-the-art%20methods.%20Our%20code%20will%20be%20open-sourced%20at%0Ahttps%3A//github.com/zmoka-zht/CDMamba.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04207v1&entry.124074799=Read"},
{"title": "Transformers need glasses! Information over-squashing in language tasks", "author": "Federico Barbero and Andrea Banino and Steven Kapturowski and Dharshan Kumaran and Jo\u00e3o G. M. Ara\u00fajo and Alex Vitvitskyi and Razvan Pascanu and Petar Veli\u010dkovi\u0107", "abstract": "  We study how information propagates in decoder-only Transformers, which are\nthe architectural backbone of most existing frontier large language models\n(LLMs). We rely on a theoretical signal propagation analysis -- specifically,\nwe analyse the representations of the last token in the final layer of the\nTransformer, as this is the representation used for next-token prediction. Our\nanalysis reveals a representational collapse phenomenon: we prove that certain\ndistinct sequences of inputs to the Transformer can yield arbitrarily close\nrepresentations in the final token. This effect is exacerbated by the\nlow-precision floating-point formats frequently used in modern LLMs. As a\nresult, the model is provably unable to respond to these sequences in different\nways -- leading to errors in, e.g., tasks involving counting or copying.\nFurther, we show that decoder-only Transformer language models can lose\nsensitivity to specific tokens in the input, which relates to the well-known\nphenomenon of over-squashing in graph neural networks. We provide empirical\nevidence supporting our claims on contemporary LLMs. Our theory also points to\nsimple solutions towards ameliorating these issues.\n", "link": "http://arxiv.org/abs/2406.04267v1", "date": "2024-06-06", "relevancy": 2.0751, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6049}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5075}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformers%20need%20glasses%21%20Information%20over-squashing%20in%20language%20tasks&body=Title%3A%20Transformers%20need%20glasses%21%20Information%20over-squashing%20in%20language%20tasks%0AAuthor%3A%20Federico%20Barbero%20and%20Andrea%20Banino%20and%20Steven%20Kapturowski%20and%20Dharshan%20Kumaran%20and%20Jo%C3%A3o%20G.%20M.%20Ara%C3%BAjo%20and%20Alex%20Vitvitskyi%20and%20Razvan%20Pascanu%20and%20Petar%20Veli%C4%8Dkovi%C4%87%0AAbstract%3A%20%20%20We%20study%20how%20information%20propagates%20in%20decoder-only%20Transformers%2C%20which%20are%0Athe%20architectural%20backbone%20of%20most%20existing%20frontier%20large%20language%20models%0A%28LLMs%29.%20We%20rely%20on%20a%20theoretical%20signal%20propagation%20analysis%20--%20specifically%2C%0Awe%20analyse%20the%20representations%20of%20the%20last%20token%20in%20the%20final%20layer%20of%20the%0ATransformer%2C%20as%20this%20is%20the%20representation%20used%20for%20next-token%20prediction.%20Our%0Aanalysis%20reveals%20a%20representational%20collapse%20phenomenon%3A%20we%20prove%20that%20certain%0Adistinct%20sequences%20of%20inputs%20to%20the%20Transformer%20can%20yield%20arbitrarily%20close%0Arepresentations%20in%20the%20final%20token.%20This%20effect%20is%20exacerbated%20by%20the%0Alow-precision%20floating-point%20formats%20frequently%20used%20in%20modern%20LLMs.%20As%20a%0Aresult%2C%20the%20model%20is%20provably%20unable%20to%20respond%20to%20these%20sequences%20in%20different%0Aways%20--%20leading%20to%20errors%20in%2C%20e.g.%2C%20tasks%20involving%20counting%20or%20copying.%0AFurther%2C%20we%20show%20that%20decoder-only%20Transformer%20language%20models%20can%20lose%0Asensitivity%20to%20specific%20tokens%20in%20the%20input%2C%20which%20relates%20to%20the%20well-known%0Aphenomenon%20of%20over-squashing%20in%20graph%20neural%20networks.%20We%20provide%20empirical%0Aevidence%20supporting%20our%20claims%20on%20contemporary%20LLMs.%20Our%20theory%20also%20points%20to%0Asimple%20solutions%20towards%20ameliorating%20these%20issues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformers%2520need%2520glasses%2521%2520Information%2520over-squashing%2520in%2520language%2520tasks%26entry.906535625%3DFederico%2520Barbero%2520and%2520Andrea%2520Banino%2520and%2520Steven%2520Kapturowski%2520and%2520Dharshan%2520Kumaran%2520and%2520Jo%25C3%25A3o%2520G.%2520M.%2520Ara%25C3%25BAjo%2520and%2520Alex%2520Vitvitskyi%2520and%2520Razvan%2520Pascanu%2520and%2520Petar%2520Veli%25C4%258Dkovi%25C4%2587%26entry.1292438233%3D%2520%2520We%2520study%2520how%2520information%2520propagates%2520in%2520decoder-only%2520Transformers%252C%2520which%2520are%250Athe%2520architectural%2520backbone%2520of%2520most%2520existing%2520frontier%2520large%2520language%2520models%250A%2528LLMs%2529.%2520We%2520rely%2520on%2520a%2520theoretical%2520signal%2520propagation%2520analysis%2520--%2520specifically%252C%250Awe%2520analyse%2520the%2520representations%2520of%2520the%2520last%2520token%2520in%2520the%2520final%2520layer%2520of%2520the%250ATransformer%252C%2520as%2520this%2520is%2520the%2520representation%2520used%2520for%2520next-token%2520prediction.%2520Our%250Aanalysis%2520reveals%2520a%2520representational%2520collapse%2520phenomenon%253A%2520we%2520prove%2520that%2520certain%250Adistinct%2520sequences%2520of%2520inputs%2520to%2520the%2520Transformer%2520can%2520yield%2520arbitrarily%2520close%250Arepresentations%2520in%2520the%2520final%2520token.%2520This%2520effect%2520is%2520exacerbated%2520by%2520the%250Alow-precision%2520floating-point%2520formats%2520frequently%2520used%2520in%2520modern%2520LLMs.%2520As%2520a%250Aresult%252C%2520the%2520model%2520is%2520provably%2520unable%2520to%2520respond%2520to%2520these%2520sequences%2520in%2520different%250Aways%2520--%2520leading%2520to%2520errors%2520in%252C%2520e.g.%252C%2520tasks%2520involving%2520counting%2520or%2520copying.%250AFurther%252C%2520we%2520show%2520that%2520decoder-only%2520Transformer%2520language%2520models%2520can%2520lose%250Asensitivity%2520to%2520specific%2520tokens%2520in%2520the%2520input%252C%2520which%2520relates%2520to%2520the%2520well-known%250Aphenomenon%2520of%2520over-squashing%2520in%2520graph%2520neural%2520networks.%2520We%2520provide%2520empirical%250Aevidence%2520supporting%2520our%2520claims%2520on%2520contemporary%2520LLMs.%2520Our%2520theory%2520also%2520points%2520to%250Asimple%2520solutions%2520towards%2520ameliorating%2520these%2520issues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformers%20need%20glasses%21%20Information%20over-squashing%20in%20language%20tasks&entry.906535625=Federico%20Barbero%20and%20Andrea%20Banino%20and%20Steven%20Kapturowski%20and%20Dharshan%20Kumaran%20and%20Jo%C3%A3o%20G.%20M.%20Ara%C3%BAjo%20and%20Alex%20Vitvitskyi%20and%20Razvan%20Pascanu%20and%20Petar%20Veli%C4%8Dkovi%C4%87&entry.1292438233=%20%20We%20study%20how%20information%20propagates%20in%20decoder-only%20Transformers%2C%20which%20are%0Athe%20architectural%20backbone%20of%20most%20existing%20frontier%20large%20language%20models%0A%28LLMs%29.%20We%20rely%20on%20a%20theoretical%20signal%20propagation%20analysis%20--%20specifically%2C%0Awe%20analyse%20the%20representations%20of%20the%20last%20token%20in%20the%20final%20layer%20of%20the%0ATransformer%2C%20as%20this%20is%20the%20representation%20used%20for%20next-token%20prediction.%20Our%0Aanalysis%20reveals%20a%20representational%20collapse%20phenomenon%3A%20we%20prove%20that%20certain%0Adistinct%20sequences%20of%20inputs%20to%20the%20Transformer%20can%20yield%20arbitrarily%20close%0Arepresentations%20in%20the%20final%20token.%20This%20effect%20is%20exacerbated%20by%20the%0Alow-precision%20floating-point%20formats%20frequently%20used%20in%20modern%20LLMs.%20As%20a%0Aresult%2C%20the%20model%20is%20provably%20unable%20to%20respond%20to%20these%20sequences%20in%20different%0Aways%20--%20leading%20to%20errors%20in%2C%20e.g.%2C%20tasks%20involving%20counting%20or%20copying.%0AFurther%2C%20we%20show%20that%20decoder-only%20Transformer%20language%20models%20can%20lose%0Asensitivity%20to%20specific%20tokens%20in%20the%20input%2C%20which%20relates%20to%20the%20well-known%0Aphenomenon%20of%20over-squashing%20in%20graph%20neural%20networks.%20We%20provide%20empirical%0Aevidence%20supporting%20our%20claims%20on%20contemporary%20LLMs.%20Our%20theory%20also%20points%20to%0Asimple%20solutions%20towards%20ameliorating%20these%20issues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04267v1&entry.124074799=Read"},
{"title": "Communication-Efficient Distributed Deep Learning via Federated Dynamic\n  Averaging", "author": "Michail Theologitis and Georgios Frangias and Georgios Anestis and Vasilis Samoladas and Antonios Deligiannakis", "abstract": "  Driven by the ever-growing volume and decentralized nature of data, coupled\nwith the need to harness this data and generate knowledge from it, has led to\nthe extensive use of distributed deep learning (DDL) techniques for training.\nThese techniques rely on local training that is performed at the distributed\nnodes based on locally collected data, followed by a periodic synchronization\nprocess that combines these models to create a global model. However, frequent\nsynchronization of DL models, encompassing millions to many billions of\nparameters, creates a communication bottleneck, severely hindering scalability.\nWorse yet, DDL algorithms typically waste valuable bandwidth, and make\nthemselves less practical in bandwidth-constrained federated settings, by\nrelying on overly simplistic, periodic, and rigid synchronization schedules.\nThese drawbacks also have a direct impact on the time required for the training\nprocess, necessitating excessive time for data communication. To address these\nshortcomings, we propose Federated Dynamic Averaging (FDA), a\ncommunication-efficient DDL strategy that dynamically triggers synchronization\nbased on the value of the model variance. In essence, the costly\nsynchronization step is triggered only if the local models, which are\ninitialized from a common global model after each synchronization, have\nsignificantly diverged. This decision is facilitated by the communication of a\nsmall local state from each distributed node/worker. Through extensive\nexperiments across a wide range of learning tasks we demonstrate that FDA\nreduces communication cost by orders of magnitude, compared to both traditional\nand cutting-edge communication-efficient algorithms. Additionally, we show that\nFDA maintains robust performance across diverse data heterogeneity settings.\n", "link": "http://arxiv.org/abs/2405.20988v2", "date": "2024-06-06", "relevancy": 2.0747, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5353}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5313}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Communication-Efficient%20Distributed%20Deep%20Learning%20via%20Federated%20Dynamic%0A%20%20Averaging&body=Title%3A%20Communication-Efficient%20Distributed%20Deep%20Learning%20via%20Federated%20Dynamic%0A%20%20Averaging%0AAuthor%3A%20Michail%20Theologitis%20and%20Georgios%20Frangias%20and%20Georgios%20Anestis%20and%20Vasilis%20Samoladas%20and%20Antonios%20Deligiannakis%0AAbstract%3A%20%20%20Driven%20by%20the%20ever-growing%20volume%20and%20decentralized%20nature%20of%20data%2C%20coupled%0Awith%20the%20need%20to%20harness%20this%20data%20and%20generate%20knowledge%20from%20it%2C%20has%20led%20to%0Athe%20extensive%20use%20of%20distributed%20deep%20learning%20%28DDL%29%20techniques%20for%20training.%0AThese%20techniques%20rely%20on%20local%20training%20that%20is%20performed%20at%20the%20distributed%0Anodes%20based%20on%20locally%20collected%20data%2C%20followed%20by%20a%20periodic%20synchronization%0Aprocess%20that%20combines%20these%20models%20to%20create%20a%20global%20model.%20However%2C%20frequent%0Asynchronization%20of%20DL%20models%2C%20encompassing%20millions%20to%20many%20billions%20of%0Aparameters%2C%20creates%20a%20communication%20bottleneck%2C%20severely%20hindering%20scalability.%0AWorse%20yet%2C%20DDL%20algorithms%20typically%20waste%20valuable%20bandwidth%2C%20and%20make%0Athemselves%20less%20practical%20in%20bandwidth-constrained%20federated%20settings%2C%20by%0Arelying%20on%20overly%20simplistic%2C%20periodic%2C%20and%20rigid%20synchronization%20schedules.%0AThese%20drawbacks%20also%20have%20a%20direct%20impact%20on%20the%20time%20required%20for%20the%20training%0Aprocess%2C%20necessitating%20excessive%20time%20for%20data%20communication.%20To%20address%20these%0Ashortcomings%2C%20we%20propose%20Federated%20Dynamic%20Averaging%20%28FDA%29%2C%20a%0Acommunication-efficient%20DDL%20strategy%20that%20dynamically%20triggers%20synchronization%0Abased%20on%20the%20value%20of%20the%20model%20variance.%20In%20essence%2C%20the%20costly%0Asynchronization%20step%20is%20triggered%20only%20if%20the%20local%20models%2C%20which%20are%0Ainitialized%20from%20a%20common%20global%20model%20after%20each%20synchronization%2C%20have%0Asignificantly%20diverged.%20This%20decision%20is%20facilitated%20by%20the%20communication%20of%20a%0Asmall%20local%20state%20from%20each%20distributed%20node/worker.%20Through%20extensive%0Aexperiments%20across%20a%20wide%20range%20of%20learning%20tasks%20we%20demonstrate%20that%20FDA%0Areduces%20communication%20cost%20by%20orders%20of%20magnitude%2C%20compared%20to%20both%20traditional%0Aand%20cutting-edge%20communication-efficient%20algorithms.%20Additionally%2C%20we%20show%20that%0AFDA%20maintains%20robust%20performance%20across%20diverse%20data%20heterogeneity%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20988v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunication-Efficient%2520Distributed%2520Deep%2520Learning%2520via%2520Federated%2520Dynamic%250A%2520%2520Averaging%26entry.906535625%3DMichail%2520Theologitis%2520and%2520Georgios%2520Frangias%2520and%2520Georgios%2520Anestis%2520and%2520Vasilis%2520Samoladas%2520and%2520Antonios%2520Deligiannakis%26entry.1292438233%3D%2520%2520Driven%2520by%2520the%2520ever-growing%2520volume%2520and%2520decentralized%2520nature%2520of%2520data%252C%2520coupled%250Awith%2520the%2520need%2520to%2520harness%2520this%2520data%2520and%2520generate%2520knowledge%2520from%2520it%252C%2520has%2520led%2520to%250Athe%2520extensive%2520use%2520of%2520distributed%2520deep%2520learning%2520%2528DDL%2529%2520techniques%2520for%2520training.%250AThese%2520techniques%2520rely%2520on%2520local%2520training%2520that%2520is%2520performed%2520at%2520the%2520distributed%250Anodes%2520based%2520on%2520locally%2520collected%2520data%252C%2520followed%2520by%2520a%2520periodic%2520synchronization%250Aprocess%2520that%2520combines%2520these%2520models%2520to%2520create%2520a%2520global%2520model.%2520However%252C%2520frequent%250Asynchronization%2520of%2520DL%2520models%252C%2520encompassing%2520millions%2520to%2520many%2520billions%2520of%250Aparameters%252C%2520creates%2520a%2520communication%2520bottleneck%252C%2520severely%2520hindering%2520scalability.%250AWorse%2520yet%252C%2520DDL%2520algorithms%2520typically%2520waste%2520valuable%2520bandwidth%252C%2520and%2520make%250Athemselves%2520less%2520practical%2520in%2520bandwidth-constrained%2520federated%2520settings%252C%2520by%250Arelying%2520on%2520overly%2520simplistic%252C%2520periodic%252C%2520and%2520rigid%2520synchronization%2520schedules.%250AThese%2520drawbacks%2520also%2520have%2520a%2520direct%2520impact%2520on%2520the%2520time%2520required%2520for%2520the%2520training%250Aprocess%252C%2520necessitating%2520excessive%2520time%2520for%2520data%2520communication.%2520To%2520address%2520these%250Ashortcomings%252C%2520we%2520propose%2520Federated%2520Dynamic%2520Averaging%2520%2528FDA%2529%252C%2520a%250Acommunication-efficient%2520DDL%2520strategy%2520that%2520dynamically%2520triggers%2520synchronization%250Abased%2520on%2520the%2520value%2520of%2520the%2520model%2520variance.%2520In%2520essence%252C%2520the%2520costly%250Asynchronization%2520step%2520is%2520triggered%2520only%2520if%2520the%2520local%2520models%252C%2520which%2520are%250Ainitialized%2520from%2520a%2520common%2520global%2520model%2520after%2520each%2520synchronization%252C%2520have%250Asignificantly%2520diverged.%2520This%2520decision%2520is%2520facilitated%2520by%2520the%2520communication%2520of%2520a%250Asmall%2520local%2520state%2520from%2520each%2520distributed%2520node/worker.%2520Through%2520extensive%250Aexperiments%2520across%2520a%2520wide%2520range%2520of%2520learning%2520tasks%2520we%2520demonstrate%2520that%2520FDA%250Areduces%2520communication%2520cost%2520by%2520orders%2520of%2520magnitude%252C%2520compared%2520to%2520both%2520traditional%250Aand%2520cutting-edge%2520communication-efficient%2520algorithms.%2520Additionally%252C%2520we%2520show%2520that%250AFDA%2520maintains%2520robust%2520performance%2520across%2520diverse%2520data%2520heterogeneity%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20988v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Communication-Efficient%20Distributed%20Deep%20Learning%20via%20Federated%20Dynamic%0A%20%20Averaging&entry.906535625=Michail%20Theologitis%20and%20Georgios%20Frangias%20and%20Georgios%20Anestis%20and%20Vasilis%20Samoladas%20and%20Antonios%20Deligiannakis&entry.1292438233=%20%20Driven%20by%20the%20ever-growing%20volume%20and%20decentralized%20nature%20of%20data%2C%20coupled%0Awith%20the%20need%20to%20harness%20this%20data%20and%20generate%20knowledge%20from%20it%2C%20has%20led%20to%0Athe%20extensive%20use%20of%20distributed%20deep%20learning%20%28DDL%29%20techniques%20for%20training.%0AThese%20techniques%20rely%20on%20local%20training%20that%20is%20performed%20at%20the%20distributed%0Anodes%20based%20on%20locally%20collected%20data%2C%20followed%20by%20a%20periodic%20synchronization%0Aprocess%20that%20combines%20these%20models%20to%20create%20a%20global%20model.%20However%2C%20frequent%0Asynchronization%20of%20DL%20models%2C%20encompassing%20millions%20to%20many%20billions%20of%0Aparameters%2C%20creates%20a%20communication%20bottleneck%2C%20severely%20hindering%20scalability.%0AWorse%20yet%2C%20DDL%20algorithms%20typically%20waste%20valuable%20bandwidth%2C%20and%20make%0Athemselves%20less%20practical%20in%20bandwidth-constrained%20federated%20settings%2C%20by%0Arelying%20on%20overly%20simplistic%2C%20periodic%2C%20and%20rigid%20synchronization%20schedules.%0AThese%20drawbacks%20also%20have%20a%20direct%20impact%20on%20the%20time%20required%20for%20the%20training%0Aprocess%2C%20necessitating%20excessive%20time%20for%20data%20communication.%20To%20address%20these%0Ashortcomings%2C%20we%20propose%20Federated%20Dynamic%20Averaging%20%28FDA%29%2C%20a%0Acommunication-efficient%20DDL%20strategy%20that%20dynamically%20triggers%20synchronization%0Abased%20on%20the%20value%20of%20the%20model%20variance.%20In%20essence%2C%20the%20costly%0Asynchronization%20step%20is%20triggered%20only%20if%20the%20local%20models%2C%20which%20are%0Ainitialized%20from%20a%20common%20global%20model%20after%20each%20synchronization%2C%20have%0Asignificantly%20diverged.%20This%20decision%20is%20facilitated%20by%20the%20communication%20of%20a%0Asmall%20local%20state%20from%20each%20distributed%20node/worker.%20Through%20extensive%0Aexperiments%20across%20a%20wide%20range%20of%20learning%20tasks%20we%20demonstrate%20that%20FDA%0Areduces%20communication%20cost%20by%20orders%20of%20magnitude%2C%20compared%20to%20both%20traditional%0Aand%20cutting-edge%20communication-efficient%20algorithms.%20Additionally%2C%20we%20show%20that%0AFDA%20maintains%20robust%20performance%20across%20diverse%20data%20heterogeneity%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20988v2&entry.124074799=Read"},
{"title": "UrbanSARFloods: Sentinel-1 SLC-Based Benchmark Dataset for Urban and\n  Open-Area Flood Mapping", "author": "Jie Zhao and Zhitong Xiong and Xiao Xiang Zhu", "abstract": "  Due to its cloud-penetrating capability and independence from solar\nillumination, satellite Synthetic Aperture Radar (SAR) is the preferred data\nsource for large-scale flood mapping, providing global coverage and including\nvarious land cover classes. However, most studies on large-scale SAR-derived\nflood mapping using deep learning algorithms have primarily focused on flooded\nopen areas, utilizing available open-access datasets (e.g., Sen1Floods11) and\nwith limited attention to urban floods. To address this gap, we introduce\n\\textbf{UrbanSARFloods}, a floodwater dataset featuring pre-processed\nSentinel-1 intensity data and interferometric coherence imagery acquired before\nand during flood events. It contains 8,879 $512\\times 512$ chips covering\n807,500 $km^2$ across 20 land cover classes and 5 continents, spanning 18 flood\nevents. We used UrbanSARFloods to benchmark existing state-of-the-art\nconvolutional neural networks (CNNs) for segmenting open and urban flood areas.\nOur findings indicate that prevalent approaches, including the Weighted\nCross-Entropy (WCE) loss and the application of transfer learning with\npretrained models, fall short in overcoming the obstacles posed by imbalanced\ndata and the constraints of a small training dataset. Urban flood detection\nremains challenging. Future research should explore strategies for addressing\nimbalanced data challenges and investigate transfer learning's potential for\nSAR-based large-scale flood mapping. Besides, expanding this dataset to include\nadditional flood events holds promise for enhancing its utility and\ncontributing to advancements in flood mapping techniques.\n", "link": "http://arxiv.org/abs/2406.04111v1", "date": "2024-06-06", "relevancy": 2.0656, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5738}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4775}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.47}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UrbanSARFloods%3A%20Sentinel-1%20SLC-Based%20Benchmark%20Dataset%20for%20Urban%20and%0A%20%20Open-Area%20Flood%20Mapping&body=Title%3A%20UrbanSARFloods%3A%20Sentinel-1%20SLC-Based%20Benchmark%20Dataset%20for%20Urban%20and%0A%20%20Open-Area%20Flood%20Mapping%0AAuthor%3A%20Jie%20Zhao%20and%20Zhitong%20Xiong%20and%20Xiao%20Xiang%20Zhu%0AAbstract%3A%20%20%20Due%20to%20its%20cloud-penetrating%20capability%20and%20independence%20from%20solar%0Aillumination%2C%20satellite%20Synthetic%20Aperture%20Radar%20%28SAR%29%20is%20the%20preferred%20data%0Asource%20for%20large-scale%20flood%20mapping%2C%20providing%20global%20coverage%20and%20including%0Avarious%20land%20cover%20classes.%20However%2C%20most%20studies%20on%20large-scale%20SAR-derived%0Aflood%20mapping%20using%20deep%20learning%20algorithms%20have%20primarily%20focused%20on%20flooded%0Aopen%20areas%2C%20utilizing%20available%20open-access%20datasets%20%28e.g.%2C%20Sen1Floods11%29%20and%0Awith%20limited%20attention%20to%20urban%20floods.%20To%20address%20this%20gap%2C%20we%20introduce%0A%5Ctextbf%7BUrbanSARFloods%7D%2C%20a%20floodwater%20dataset%20featuring%20pre-processed%0ASentinel-1%20intensity%20data%20and%20interferometric%20coherence%20imagery%20acquired%20before%0Aand%20during%20flood%20events.%20It%20contains%208%2C879%20%24512%5Ctimes%20512%24%20chips%20covering%0A807%2C500%20%24km%5E2%24%20across%2020%20land%20cover%20classes%20and%205%20continents%2C%20spanning%2018%20flood%0Aevents.%20We%20used%20UrbanSARFloods%20to%20benchmark%20existing%20state-of-the-art%0Aconvolutional%20neural%20networks%20%28CNNs%29%20for%20segmenting%20open%20and%20urban%20flood%20areas.%0AOur%20findings%20indicate%20that%20prevalent%20approaches%2C%20including%20the%20Weighted%0ACross-Entropy%20%28WCE%29%20loss%20and%20the%20application%20of%20transfer%20learning%20with%0Apretrained%20models%2C%20fall%20short%20in%20overcoming%20the%20obstacles%20posed%20by%20imbalanced%0Adata%20and%20the%20constraints%20of%20a%20small%20training%20dataset.%20Urban%20flood%20detection%0Aremains%20challenging.%20Future%20research%20should%20explore%20strategies%20for%20addressing%0Aimbalanced%20data%20challenges%20and%20investigate%20transfer%20learning%27s%20potential%20for%0ASAR-based%20large-scale%20flood%20mapping.%20Besides%2C%20expanding%20this%20dataset%20to%20include%0Aadditional%20flood%20events%20holds%20promise%20for%20enhancing%20its%20utility%20and%0Acontributing%20to%20advancements%20in%20flood%20mapping%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04111v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrbanSARFloods%253A%2520Sentinel-1%2520SLC-Based%2520Benchmark%2520Dataset%2520for%2520Urban%2520and%250A%2520%2520Open-Area%2520Flood%2520Mapping%26entry.906535625%3DJie%2520Zhao%2520and%2520Zhitong%2520Xiong%2520and%2520Xiao%2520Xiang%2520Zhu%26entry.1292438233%3D%2520%2520Due%2520to%2520its%2520cloud-penetrating%2520capability%2520and%2520independence%2520from%2520solar%250Aillumination%252C%2520satellite%2520Synthetic%2520Aperture%2520Radar%2520%2528SAR%2529%2520is%2520the%2520preferred%2520data%250Asource%2520for%2520large-scale%2520flood%2520mapping%252C%2520providing%2520global%2520coverage%2520and%2520including%250Avarious%2520land%2520cover%2520classes.%2520However%252C%2520most%2520studies%2520on%2520large-scale%2520SAR-derived%250Aflood%2520mapping%2520using%2520deep%2520learning%2520algorithms%2520have%2520primarily%2520focused%2520on%2520flooded%250Aopen%2520areas%252C%2520utilizing%2520available%2520open-access%2520datasets%2520%2528e.g.%252C%2520Sen1Floods11%2529%2520and%250Awith%2520limited%2520attention%2520to%2520urban%2520floods.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250A%255Ctextbf%257BUrbanSARFloods%257D%252C%2520a%2520floodwater%2520dataset%2520featuring%2520pre-processed%250ASentinel-1%2520intensity%2520data%2520and%2520interferometric%2520coherence%2520imagery%2520acquired%2520before%250Aand%2520during%2520flood%2520events.%2520It%2520contains%25208%252C879%2520%2524512%255Ctimes%2520512%2524%2520chips%2520covering%250A807%252C500%2520%2524km%255E2%2524%2520across%252020%2520land%2520cover%2520classes%2520and%25205%2520continents%252C%2520spanning%252018%2520flood%250Aevents.%2520We%2520used%2520UrbanSARFloods%2520to%2520benchmark%2520existing%2520state-of-the-art%250Aconvolutional%2520neural%2520networks%2520%2528CNNs%2529%2520for%2520segmenting%2520open%2520and%2520urban%2520flood%2520areas.%250AOur%2520findings%2520indicate%2520that%2520prevalent%2520approaches%252C%2520including%2520the%2520Weighted%250ACross-Entropy%2520%2528WCE%2529%2520loss%2520and%2520the%2520application%2520of%2520transfer%2520learning%2520with%250Apretrained%2520models%252C%2520fall%2520short%2520in%2520overcoming%2520the%2520obstacles%2520posed%2520by%2520imbalanced%250Adata%2520and%2520the%2520constraints%2520of%2520a%2520small%2520training%2520dataset.%2520Urban%2520flood%2520detection%250Aremains%2520challenging.%2520Future%2520research%2520should%2520explore%2520strategies%2520for%2520addressing%250Aimbalanced%2520data%2520challenges%2520and%2520investigate%2520transfer%2520learning%2527s%2520potential%2520for%250ASAR-based%2520large-scale%2520flood%2520mapping.%2520Besides%252C%2520expanding%2520this%2520dataset%2520to%2520include%250Aadditional%2520flood%2520events%2520holds%2520promise%2520for%2520enhancing%2520its%2520utility%2520and%250Acontributing%2520to%2520advancements%2520in%2520flood%2520mapping%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04111v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UrbanSARFloods%3A%20Sentinel-1%20SLC-Based%20Benchmark%20Dataset%20for%20Urban%20and%0A%20%20Open-Area%20Flood%20Mapping&entry.906535625=Jie%20Zhao%20and%20Zhitong%20Xiong%20and%20Xiao%20Xiang%20Zhu&entry.1292438233=%20%20Due%20to%20its%20cloud-penetrating%20capability%20and%20independence%20from%20solar%0Aillumination%2C%20satellite%20Synthetic%20Aperture%20Radar%20%28SAR%29%20is%20the%20preferred%20data%0Asource%20for%20large-scale%20flood%20mapping%2C%20providing%20global%20coverage%20and%20including%0Avarious%20land%20cover%20classes.%20However%2C%20most%20studies%20on%20large-scale%20SAR-derived%0Aflood%20mapping%20using%20deep%20learning%20algorithms%20have%20primarily%20focused%20on%20flooded%0Aopen%20areas%2C%20utilizing%20available%20open-access%20datasets%20%28e.g.%2C%20Sen1Floods11%29%20and%0Awith%20limited%20attention%20to%20urban%20floods.%20To%20address%20this%20gap%2C%20we%20introduce%0A%5Ctextbf%7BUrbanSARFloods%7D%2C%20a%20floodwater%20dataset%20featuring%20pre-processed%0ASentinel-1%20intensity%20data%20and%20interferometric%20coherence%20imagery%20acquired%20before%0Aand%20during%20flood%20events.%20It%20contains%208%2C879%20%24512%5Ctimes%20512%24%20chips%20covering%0A807%2C500%20%24km%5E2%24%20across%2020%20land%20cover%20classes%20and%205%20continents%2C%20spanning%2018%20flood%0Aevents.%20We%20used%20UrbanSARFloods%20to%20benchmark%20existing%20state-of-the-art%0Aconvolutional%20neural%20networks%20%28CNNs%29%20for%20segmenting%20open%20and%20urban%20flood%20areas.%0AOur%20findings%20indicate%20that%20prevalent%20approaches%2C%20including%20the%20Weighted%0ACross-Entropy%20%28WCE%29%20loss%20and%20the%20application%20of%20transfer%20learning%20with%0Apretrained%20models%2C%20fall%20short%20in%20overcoming%20the%20obstacles%20posed%20by%20imbalanced%0Adata%20and%20the%20constraints%20of%20a%20small%20training%20dataset.%20Urban%20flood%20detection%0Aremains%20challenging.%20Future%20research%20should%20explore%20strategies%20for%20addressing%0Aimbalanced%20data%20challenges%20and%20investigate%20transfer%20learning%27s%20potential%20for%0ASAR-based%20large-scale%20flood%20mapping.%20Besides%2C%20expanding%20this%20dataset%20to%20include%0Aadditional%20flood%20events%20holds%20promise%20for%20enhancing%20its%20utility%20and%0Acontributing%20to%20advancements%20in%20flood%20mapping%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04111v1&entry.124074799=Read"},
{"title": "HackAtari: Atari Learning Environments for Robust and Continual\n  Reinforcement Learning", "author": "Quentin Delfosse and Jannis Bl\u00fcml and Bjarne Gregori and Kristian Kersting", "abstract": "  Artificial agents' adaptability to novelty and alignment with intended\nbehavior is crucial for their effective deployment. Reinforcement learning (RL)\nleverages novelty as a means of exploration, yet agents often struggle to\nhandle novel situations, hindering generalization. To address these issues, we\npropose HackAtari, a framework introducing controlled novelty to the most\ncommon RL benchmark, the Atari Learning Environment. HackAtari allows us to\ncreate novel game scenarios (including simplification for curriculum learning),\nto swap the game elements' colors, as well as to introduce different reward\nsignals for the agent. We demonstrate that current agents trained on the\noriginal environments include robustness failures, and evaluate HackAtari's\nefficacy in enhancing RL agents' robustness and aligning behavior through\nexperiments using C51 and PPO. Overall, HackAtari can be used to improve the\nrobustness of current and future RL algorithms, allowing Neuro-Symbolic RL,\ncurriculum RL, causal RL, as well as LLM-driven RL. Our work underscores the\nsignificance of developing interpretable in RL agents.\n", "link": "http://arxiv.org/abs/2406.03997v1", "date": "2024-06-06", "relevancy": 2.0595, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5621}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5134}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HackAtari%3A%20Atari%20Learning%20Environments%20for%20Robust%20and%20Continual%0A%20%20Reinforcement%20Learning&body=Title%3A%20HackAtari%3A%20Atari%20Learning%20Environments%20for%20Robust%20and%20Continual%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Quentin%20Delfosse%20and%20Jannis%20Bl%C3%BCml%20and%20Bjarne%20Gregori%20and%20Kristian%20Kersting%0AAbstract%3A%20%20%20Artificial%20agents%27%20adaptability%20to%20novelty%20and%20alignment%20with%20intended%0Abehavior%20is%20crucial%20for%20their%20effective%20deployment.%20Reinforcement%20learning%20%28RL%29%0Aleverages%20novelty%20as%20a%20means%20of%20exploration%2C%20yet%20agents%20often%20struggle%20to%0Ahandle%20novel%20situations%2C%20hindering%20generalization.%20To%20address%20these%20issues%2C%20we%0Apropose%20HackAtari%2C%20a%20framework%20introducing%20controlled%20novelty%20to%20the%20most%0Acommon%20RL%20benchmark%2C%20the%20Atari%20Learning%20Environment.%20HackAtari%20allows%20us%20to%0Acreate%20novel%20game%20scenarios%20%28including%20simplification%20for%20curriculum%20learning%29%2C%0Ato%20swap%20the%20game%20elements%27%20colors%2C%20as%20well%20as%20to%20introduce%20different%20reward%0Asignals%20for%20the%20agent.%20We%20demonstrate%20that%20current%20agents%20trained%20on%20the%0Aoriginal%20environments%20include%20robustness%20failures%2C%20and%20evaluate%20HackAtari%27s%0Aefficacy%20in%20enhancing%20RL%20agents%27%20robustness%20and%20aligning%20behavior%20through%0Aexperiments%20using%20C51%20and%20PPO.%20Overall%2C%20HackAtari%20can%20be%20used%20to%20improve%20the%0Arobustness%20of%20current%20and%20future%20RL%20algorithms%2C%20allowing%20Neuro-Symbolic%20RL%2C%0Acurriculum%20RL%2C%20causal%20RL%2C%20as%20well%20as%20LLM-driven%20RL.%20Our%20work%20underscores%20the%0Asignificance%20of%20developing%20interpretable%20in%20RL%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHackAtari%253A%2520Atari%2520Learning%2520Environments%2520for%2520Robust%2520and%2520Continual%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DQuentin%2520Delfosse%2520and%2520Jannis%2520Bl%25C3%25BCml%2520and%2520Bjarne%2520Gregori%2520and%2520Kristian%2520Kersting%26entry.1292438233%3D%2520%2520Artificial%2520agents%2527%2520adaptability%2520to%2520novelty%2520and%2520alignment%2520with%2520intended%250Abehavior%2520is%2520crucial%2520for%2520their%2520effective%2520deployment.%2520Reinforcement%2520learning%2520%2528RL%2529%250Aleverages%2520novelty%2520as%2520a%2520means%2520of%2520exploration%252C%2520yet%2520agents%2520often%2520struggle%2520to%250Ahandle%2520novel%2520situations%252C%2520hindering%2520generalization.%2520To%2520address%2520these%2520issues%252C%2520we%250Apropose%2520HackAtari%252C%2520a%2520framework%2520introducing%2520controlled%2520novelty%2520to%2520the%2520most%250Acommon%2520RL%2520benchmark%252C%2520the%2520Atari%2520Learning%2520Environment.%2520HackAtari%2520allows%2520us%2520to%250Acreate%2520novel%2520game%2520scenarios%2520%2528including%2520simplification%2520for%2520curriculum%2520learning%2529%252C%250Ato%2520swap%2520the%2520game%2520elements%2527%2520colors%252C%2520as%2520well%2520as%2520to%2520introduce%2520different%2520reward%250Asignals%2520for%2520the%2520agent.%2520We%2520demonstrate%2520that%2520current%2520agents%2520trained%2520on%2520the%250Aoriginal%2520environments%2520include%2520robustness%2520failures%252C%2520and%2520evaluate%2520HackAtari%2527s%250Aefficacy%2520in%2520enhancing%2520RL%2520agents%2527%2520robustness%2520and%2520aligning%2520behavior%2520through%250Aexperiments%2520using%2520C51%2520and%2520PPO.%2520Overall%252C%2520HackAtari%2520can%2520be%2520used%2520to%2520improve%2520the%250Arobustness%2520of%2520current%2520and%2520future%2520RL%2520algorithms%252C%2520allowing%2520Neuro-Symbolic%2520RL%252C%250Acurriculum%2520RL%252C%2520causal%2520RL%252C%2520as%2520well%2520as%2520LLM-driven%2520RL.%2520Our%2520work%2520underscores%2520the%250Asignificance%2520of%2520developing%2520interpretable%2520in%2520RL%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HackAtari%3A%20Atari%20Learning%20Environments%20for%20Robust%20and%20Continual%0A%20%20Reinforcement%20Learning&entry.906535625=Quentin%20Delfosse%20and%20Jannis%20Bl%C3%BCml%20and%20Bjarne%20Gregori%20and%20Kristian%20Kersting&entry.1292438233=%20%20Artificial%20agents%27%20adaptability%20to%20novelty%20and%20alignment%20with%20intended%0Abehavior%20is%20crucial%20for%20their%20effective%20deployment.%20Reinforcement%20learning%20%28RL%29%0Aleverages%20novelty%20as%20a%20means%20of%20exploration%2C%20yet%20agents%20often%20struggle%20to%0Ahandle%20novel%20situations%2C%20hindering%20generalization.%20To%20address%20these%20issues%2C%20we%0Apropose%20HackAtari%2C%20a%20framework%20introducing%20controlled%20novelty%20to%20the%20most%0Acommon%20RL%20benchmark%2C%20the%20Atari%20Learning%20Environment.%20HackAtari%20allows%20us%20to%0Acreate%20novel%20game%20scenarios%20%28including%20simplification%20for%20curriculum%20learning%29%2C%0Ato%20swap%20the%20game%20elements%27%20colors%2C%20as%20well%20as%20to%20introduce%20different%20reward%0Asignals%20for%20the%20agent.%20We%20demonstrate%20that%20current%20agents%20trained%20on%20the%0Aoriginal%20environments%20include%20robustness%20failures%2C%20and%20evaluate%20HackAtari%27s%0Aefficacy%20in%20enhancing%20RL%20agents%27%20robustness%20and%20aligning%20behavior%20through%0Aexperiments%20using%20C51%20and%20PPO.%20Overall%2C%20HackAtari%20can%20be%20used%20to%20improve%20the%0Arobustness%20of%20current%20and%20future%20RL%20algorithms%2C%20allowing%20Neuro-Symbolic%20RL%2C%0Acurriculum%20RL%2C%20causal%20RL%2C%20as%20well%20as%20LLM-driven%20RL.%20Our%20work%20underscores%20the%0Asignificance%20of%20developing%20interpretable%20in%20RL%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03997v1&entry.124074799=Read"},
{"title": "Multivector Neurons: Better and Faster O(n)-Equivariant Clifford Graph\n  Neural Networks", "author": "Cong Liu and David Ruhe and Patrick Forr\u00e9", "abstract": "  Most current deep learning models equivariant to $O(n)$ or $SO(n)$ either\nconsider mostly scalar information such as distances and angles or have a very\nhigh computational complexity. In this work, we test a few novel message\npassing graph neural networks (GNNs) based on Clifford multivectors, structured\nsimilarly to other prevalent equivariant models in geometric deep learning. Our\napproach leverages efficient invariant scalar features while simultaneously\nperforming expressive learning on multivector representations, particularly\nthrough the use of the equivariant geometric product operator. By integrating\nthese elements, our methods outperform established efficient baseline models on\nan N-Body simulation task and protein denoising task while maintaining a high\nefficiency. In particular, we push the state-of-the-art error on the N-body\ndataset to 0.0035 (averaged over 3 runs); an 8% improvement over recent\nmethods. Our implementation is available on Github.\n", "link": "http://arxiv.org/abs/2406.04052v1", "date": "2024-06-06", "relevancy": 2.0587, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5247}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5199}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multivector%20Neurons%3A%20Better%20and%20Faster%20O%28n%29-Equivariant%20Clifford%20Graph%0A%20%20Neural%20Networks&body=Title%3A%20Multivector%20Neurons%3A%20Better%20and%20Faster%20O%28n%29-Equivariant%20Clifford%20Graph%0A%20%20Neural%20Networks%0AAuthor%3A%20Cong%20Liu%20and%20David%20Ruhe%20and%20Patrick%20Forr%C3%A9%0AAbstract%3A%20%20%20Most%20current%20deep%20learning%20models%20equivariant%20to%20%24O%28n%29%24%20or%20%24SO%28n%29%24%20either%0Aconsider%20mostly%20scalar%20information%20such%20as%20distances%20and%20angles%20or%20have%20a%20very%0Ahigh%20computational%20complexity.%20In%20this%20work%2C%20we%20test%20a%20few%20novel%20message%0Apassing%20graph%20neural%20networks%20%28GNNs%29%20based%20on%20Clifford%20multivectors%2C%20structured%0Asimilarly%20to%20other%20prevalent%20equivariant%20models%20in%20geometric%20deep%20learning.%20Our%0Aapproach%20leverages%20efficient%20invariant%20scalar%20features%20while%20simultaneously%0Aperforming%20expressive%20learning%20on%20multivector%20representations%2C%20particularly%0Athrough%20the%20use%20of%20the%20equivariant%20geometric%20product%20operator.%20By%20integrating%0Athese%20elements%2C%20our%20methods%20outperform%20established%20efficient%20baseline%20models%20on%0Aan%20N-Body%20simulation%20task%20and%20protein%20denoising%20task%20while%20maintaining%20a%20high%0Aefficiency.%20In%20particular%2C%20we%20push%20the%20state-of-the-art%20error%20on%20the%20N-body%0Adataset%20to%200.0035%20%28averaged%20over%203%20runs%29%3B%20an%208%25%20improvement%20over%20recent%0Amethods.%20Our%20implementation%20is%20available%20on%20Github.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultivector%2520Neurons%253A%2520Better%2520and%2520Faster%2520O%2528n%2529-Equivariant%2520Clifford%2520Graph%250A%2520%2520Neural%2520Networks%26entry.906535625%3DCong%2520Liu%2520and%2520David%2520Ruhe%2520and%2520Patrick%2520Forr%25C3%25A9%26entry.1292438233%3D%2520%2520Most%2520current%2520deep%2520learning%2520models%2520equivariant%2520to%2520%2524O%2528n%2529%2524%2520or%2520%2524SO%2528n%2529%2524%2520either%250Aconsider%2520mostly%2520scalar%2520information%2520such%2520as%2520distances%2520and%2520angles%2520or%2520have%2520a%2520very%250Ahigh%2520computational%2520complexity.%2520In%2520this%2520work%252C%2520we%2520test%2520a%2520few%2520novel%2520message%250Apassing%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520based%2520on%2520Clifford%2520multivectors%252C%2520structured%250Asimilarly%2520to%2520other%2520prevalent%2520equivariant%2520models%2520in%2520geometric%2520deep%2520learning.%2520Our%250Aapproach%2520leverages%2520efficient%2520invariant%2520scalar%2520features%2520while%2520simultaneously%250Aperforming%2520expressive%2520learning%2520on%2520multivector%2520representations%252C%2520particularly%250Athrough%2520the%2520use%2520of%2520the%2520equivariant%2520geometric%2520product%2520operator.%2520By%2520integrating%250Athese%2520elements%252C%2520our%2520methods%2520outperform%2520established%2520efficient%2520baseline%2520models%2520on%250Aan%2520N-Body%2520simulation%2520task%2520and%2520protein%2520denoising%2520task%2520while%2520maintaining%2520a%2520high%250Aefficiency.%2520In%2520particular%252C%2520we%2520push%2520the%2520state-of-the-art%2520error%2520on%2520the%2520N-body%250Adataset%2520to%25200.0035%2520%2528averaged%2520over%25203%2520runs%2529%253B%2520an%25208%2525%2520improvement%2520over%2520recent%250Amethods.%2520Our%2520implementation%2520is%2520available%2520on%2520Github.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multivector%20Neurons%3A%20Better%20and%20Faster%20O%28n%29-Equivariant%20Clifford%20Graph%0A%20%20Neural%20Networks&entry.906535625=Cong%20Liu%20and%20David%20Ruhe%20and%20Patrick%20Forr%C3%A9&entry.1292438233=%20%20Most%20current%20deep%20learning%20models%20equivariant%20to%20%24O%28n%29%24%20or%20%24SO%28n%29%24%20either%0Aconsider%20mostly%20scalar%20information%20such%20as%20distances%20and%20angles%20or%20have%20a%20very%0Ahigh%20computational%20complexity.%20In%20this%20work%2C%20we%20test%20a%20few%20novel%20message%0Apassing%20graph%20neural%20networks%20%28GNNs%29%20based%20on%20Clifford%20multivectors%2C%20structured%0Asimilarly%20to%20other%20prevalent%20equivariant%20models%20in%20geometric%20deep%20learning.%20Our%0Aapproach%20leverages%20efficient%20invariant%20scalar%20features%20while%20simultaneously%0Aperforming%20expressive%20learning%20on%20multivector%20representations%2C%20particularly%0Athrough%20the%20use%20of%20the%20equivariant%20geometric%20product%20operator.%20By%20integrating%0Athese%20elements%2C%20our%20methods%20outperform%20established%20efficient%20baseline%20models%20on%0Aan%20N-Body%20simulation%20task%20and%20protein%20denoising%20task%20while%20maintaining%20a%20high%0Aefficiency.%20In%20particular%2C%20we%20push%20the%20state-of-the-art%20error%20on%20the%20N-body%0Adataset%20to%200.0035%20%28averaged%20over%203%20runs%29%3B%20an%208%25%20improvement%20over%20recent%0Amethods.%20Our%20implementation%20is%20available%20on%20Github.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04052v1&entry.124074799=Read"},
{"title": "Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced\n  Optimization Problems", "author": "David T. Hoffmann and Simon Schrodi and Jelena Bratuli\u0107 and Nadine Behrmann and Volker Fischer and Thomas Brox", "abstract": "  In this work, we study rapid improvements of the training loss in\ntransformers when being confronted with multi-step decision tasks. We found\nthat transformers struggle to learn the intermediate task and both training and\nvalidation loss saturate for hundreds of epochs. When transformers finally\nlearn the intermediate task, they do this rapidly and unexpectedly. We call\nthese abrupt improvements Eureka-moments, since the transformer appears to\nsuddenly learn a previously incomprehensible concept. We designed synthetic\ntasks to study the problem in detail, but the leaps in performance can be\nobserved also for language modeling and in-context learning (ICL). We suspect\nthat these abrupt transitions are caused by the multi-step nature of these\ntasks. Indeed, we find connections and show that ways to improve on the\nsynthetic multi-step tasks can be used to improve the training of language\nmodeling and ICL. Using the synthetic data we trace the problem back to the\nSoftmax function in the self-attention block of transformers and show ways to\nalleviate the problem. These fixes reduce the required number of training\nsteps, lead to higher likelihood to learn the intermediate task, to higher\nfinal accuracy and training becomes more robust to hyper-parameters.\n", "link": "http://arxiv.org/abs/2310.12956v2", "date": "2024-06-06", "relevancy": 2.0538, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5527}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5085}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eureka-Moments%20in%20Transformers%3A%20Multi-Step%20Tasks%20Reveal%20Softmax%20Induced%0A%20%20Optimization%20Problems&body=Title%3A%20Eureka-Moments%20in%20Transformers%3A%20Multi-Step%20Tasks%20Reveal%20Softmax%20Induced%0A%20%20Optimization%20Problems%0AAuthor%3A%20David%20T.%20Hoffmann%20and%20Simon%20Schrodi%20and%20Jelena%20Bratuli%C4%87%20and%20Nadine%20Behrmann%20and%20Volker%20Fischer%20and%20Thomas%20Brox%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20study%20rapid%20improvements%20of%20the%20training%20loss%20in%0Atransformers%20when%20being%20confronted%20with%20multi-step%20decision%20tasks.%20We%20found%0Athat%20transformers%20struggle%20to%20learn%20the%20intermediate%20task%20and%20both%20training%20and%0Avalidation%20loss%20saturate%20for%20hundreds%20of%20epochs.%20When%20transformers%20finally%0Alearn%20the%20intermediate%20task%2C%20they%20do%20this%20rapidly%20and%20unexpectedly.%20We%20call%0Athese%20abrupt%20improvements%20Eureka-moments%2C%20since%20the%20transformer%20appears%20to%0Asuddenly%20learn%20a%20previously%20incomprehensible%20concept.%20We%20designed%20synthetic%0Atasks%20to%20study%20the%20problem%20in%20detail%2C%20but%20the%20leaps%20in%20performance%20can%20be%0Aobserved%20also%20for%20language%20modeling%20and%20in-context%20learning%20%28ICL%29.%20We%20suspect%0Athat%20these%20abrupt%20transitions%20are%20caused%20by%20the%20multi-step%20nature%20of%20these%0Atasks.%20Indeed%2C%20we%20find%20connections%20and%20show%20that%20ways%20to%20improve%20on%20the%0Asynthetic%20multi-step%20tasks%20can%20be%20used%20to%20improve%20the%20training%20of%20language%0Amodeling%20and%20ICL.%20Using%20the%20synthetic%20data%20we%20trace%20the%20problem%20back%20to%20the%0ASoftmax%20function%20in%20the%20self-attention%20block%20of%20transformers%20and%20show%20ways%20to%0Aalleviate%20the%20problem.%20These%20fixes%20reduce%20the%20required%20number%20of%20training%0Asteps%2C%20lead%20to%20higher%20likelihood%20to%20learn%20the%20intermediate%20task%2C%20to%20higher%0Afinal%20accuracy%20and%20training%20becomes%20more%20robust%20to%20hyper-parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.12956v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEureka-Moments%2520in%2520Transformers%253A%2520Multi-Step%2520Tasks%2520Reveal%2520Softmax%2520Induced%250A%2520%2520Optimization%2520Problems%26entry.906535625%3DDavid%2520T.%2520Hoffmann%2520and%2520Simon%2520Schrodi%2520and%2520Jelena%2520Bratuli%25C4%2587%2520and%2520Nadine%2520Behrmann%2520and%2520Volker%2520Fischer%2520and%2520Thomas%2520Brox%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520study%2520rapid%2520improvements%2520of%2520the%2520training%2520loss%2520in%250Atransformers%2520when%2520being%2520confronted%2520with%2520multi-step%2520decision%2520tasks.%2520We%2520found%250Athat%2520transformers%2520struggle%2520to%2520learn%2520the%2520intermediate%2520task%2520and%2520both%2520training%2520and%250Avalidation%2520loss%2520saturate%2520for%2520hundreds%2520of%2520epochs.%2520When%2520transformers%2520finally%250Alearn%2520the%2520intermediate%2520task%252C%2520they%2520do%2520this%2520rapidly%2520and%2520unexpectedly.%2520We%2520call%250Athese%2520abrupt%2520improvements%2520Eureka-moments%252C%2520since%2520the%2520transformer%2520appears%2520to%250Asuddenly%2520learn%2520a%2520previously%2520incomprehensible%2520concept.%2520We%2520designed%2520synthetic%250Atasks%2520to%2520study%2520the%2520problem%2520in%2520detail%252C%2520but%2520the%2520leaps%2520in%2520performance%2520can%2520be%250Aobserved%2520also%2520for%2520language%2520modeling%2520and%2520in-context%2520learning%2520%2528ICL%2529.%2520We%2520suspect%250Athat%2520these%2520abrupt%2520transitions%2520are%2520caused%2520by%2520the%2520multi-step%2520nature%2520of%2520these%250Atasks.%2520Indeed%252C%2520we%2520find%2520connections%2520and%2520show%2520that%2520ways%2520to%2520improve%2520on%2520the%250Asynthetic%2520multi-step%2520tasks%2520can%2520be%2520used%2520to%2520improve%2520the%2520training%2520of%2520language%250Amodeling%2520and%2520ICL.%2520Using%2520the%2520synthetic%2520data%2520we%2520trace%2520the%2520problem%2520back%2520to%2520the%250ASoftmax%2520function%2520in%2520the%2520self-attention%2520block%2520of%2520transformers%2520and%2520show%2520ways%2520to%250Aalleviate%2520the%2520problem.%2520These%2520fixes%2520reduce%2520the%2520required%2520number%2520of%2520training%250Asteps%252C%2520lead%2520to%2520higher%2520likelihood%2520to%2520learn%2520the%2520intermediate%2520task%252C%2520to%2520higher%250Afinal%2520accuracy%2520and%2520training%2520becomes%2520more%2520robust%2520to%2520hyper-parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.12956v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eureka-Moments%20in%20Transformers%3A%20Multi-Step%20Tasks%20Reveal%20Softmax%20Induced%0A%20%20Optimization%20Problems&entry.906535625=David%20T.%20Hoffmann%20and%20Simon%20Schrodi%20and%20Jelena%20Bratuli%C4%87%20and%20Nadine%20Behrmann%20and%20Volker%20Fischer%20and%20Thomas%20Brox&entry.1292438233=%20%20In%20this%20work%2C%20we%20study%20rapid%20improvements%20of%20the%20training%20loss%20in%0Atransformers%20when%20being%20confronted%20with%20multi-step%20decision%20tasks.%20We%20found%0Athat%20transformers%20struggle%20to%20learn%20the%20intermediate%20task%20and%20both%20training%20and%0Avalidation%20loss%20saturate%20for%20hundreds%20of%20epochs.%20When%20transformers%20finally%0Alearn%20the%20intermediate%20task%2C%20they%20do%20this%20rapidly%20and%20unexpectedly.%20We%20call%0Athese%20abrupt%20improvements%20Eureka-moments%2C%20since%20the%20transformer%20appears%20to%0Asuddenly%20learn%20a%20previously%20incomprehensible%20concept.%20We%20designed%20synthetic%0Atasks%20to%20study%20the%20problem%20in%20detail%2C%20but%20the%20leaps%20in%20performance%20can%20be%0Aobserved%20also%20for%20language%20modeling%20and%20in-context%20learning%20%28ICL%29.%20We%20suspect%0Athat%20these%20abrupt%20transitions%20are%20caused%20by%20the%20multi-step%20nature%20of%20these%0Atasks.%20Indeed%2C%20we%20find%20connections%20and%20show%20that%20ways%20to%20improve%20on%20the%0Asynthetic%20multi-step%20tasks%20can%20be%20used%20to%20improve%20the%20training%20of%20language%0Amodeling%20and%20ICL.%20Using%20the%20synthetic%20data%20we%20trace%20the%20problem%20back%20to%20the%0ASoftmax%20function%20in%20the%20self-attention%20block%20of%20transformers%20and%20show%20ways%20to%0Aalleviate%20the%20problem.%20These%20fixes%20reduce%20the%20required%20number%20of%20training%0Asteps%2C%20lead%20to%20higher%20likelihood%20to%20learn%20the%20intermediate%20task%2C%20to%20higher%0Afinal%20accuracy%20and%20training%20becomes%20more%20robust%20to%20hyper-parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.12956v2&entry.124074799=Read"},
{"title": "A Voxel-based Approach for Simulating Microbial Decomposition in Soil:\n  Comparison with LBM and Improvement of Morphological Models", "author": "Mouad Klai and Olivier Monga and Mohamed Soufiane Jouini and Val\u00e9rie Pot", "abstract": "  This study presents a new computational approach for simulating the microbial\ndecomposition of organic matter, from 3D micro-computed tomography (micro-CT)\nimages of soil. The method employs a valuated graph of connected voxels to\nsimulate transformation and diffusion processes involved in microbial\ndecomposition within the complex soil matrix. The resulting model can be\nadapted to simulate any diffusion-transformation processes in porous media. We\nimplemented parallelization strategies and explored different numerical\nmethods, including implicit, explicit, synchronous, and asynchronous schemes.\nTo validate our method, we compared simulation outputs with those provided by\nLBioS and by Mosaic models. LBioS uses a lattice-Boltzmann method for diffusion\nand Mosaic takes benefit of Pore Network Geometrical Modelling (PNGM) by means\nof geometrical primitives such as spheres and ellipsoids. This approach\nachieved comparable results to traditional LBM-based simulations, but required\nonly one-fourth of the computing time. Compared to Mosaic simulation, the\nproposed method is slower but more accurate and does not require any\ncalibration. Furthermore, we present a theoretical framework and an application\nexample to enhance PNGM-based simulations. This is accomplished by\napproximating the diffusional conductance coefficients using stochastic\ngradient descent and data generated by the current approach.\n", "link": "http://arxiv.org/abs/2406.04177v1", "date": "2024-06-06", "relevancy": 2.0534, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5162}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5162}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Voxel-based%20Approach%20for%20Simulating%20Microbial%20Decomposition%20in%20Soil%3A%0A%20%20Comparison%20with%20LBM%20and%20Improvement%20of%20Morphological%20Models&body=Title%3A%20A%20Voxel-based%20Approach%20for%20Simulating%20Microbial%20Decomposition%20in%20Soil%3A%0A%20%20Comparison%20with%20LBM%20and%20Improvement%20of%20Morphological%20Models%0AAuthor%3A%20Mouad%20Klai%20and%20Olivier%20Monga%20and%20Mohamed%20Soufiane%20Jouini%20and%20Val%C3%A9rie%20Pot%0AAbstract%3A%20%20%20This%20study%20presents%20a%20new%20computational%20approach%20for%20simulating%20the%20microbial%0Adecomposition%20of%20organic%20matter%2C%20from%203D%20micro-computed%20tomography%20%28micro-CT%29%0Aimages%20of%20soil.%20The%20method%20employs%20a%20valuated%20graph%20of%20connected%20voxels%20to%0Asimulate%20transformation%20and%20diffusion%20processes%20involved%20in%20microbial%0Adecomposition%20within%20the%20complex%20soil%20matrix.%20The%20resulting%20model%20can%20be%0Aadapted%20to%20simulate%20any%20diffusion-transformation%20processes%20in%20porous%20media.%20We%0Aimplemented%20parallelization%20strategies%20and%20explored%20different%20numerical%0Amethods%2C%20including%20implicit%2C%20explicit%2C%20synchronous%2C%20and%20asynchronous%20schemes.%0ATo%20validate%20our%20method%2C%20we%20compared%20simulation%20outputs%20with%20those%20provided%20by%0ALBioS%20and%20by%20Mosaic%20models.%20LBioS%20uses%20a%20lattice-Boltzmann%20method%20for%20diffusion%0Aand%20Mosaic%20takes%20benefit%20of%20Pore%20Network%20Geometrical%20Modelling%20%28PNGM%29%20by%20means%0Aof%20geometrical%20primitives%20such%20as%20spheres%20and%20ellipsoids.%20This%20approach%0Aachieved%20comparable%20results%20to%20traditional%20LBM-based%20simulations%2C%20but%20required%0Aonly%20one-fourth%20of%20the%20computing%20time.%20Compared%20to%20Mosaic%20simulation%2C%20the%0Aproposed%20method%20is%20slower%20but%20more%20accurate%20and%20does%20not%20require%20any%0Acalibration.%20Furthermore%2C%20we%20present%20a%20theoretical%20framework%20and%20an%20application%0Aexample%20to%20enhance%20PNGM-based%20simulations.%20This%20is%20accomplished%20by%0Aapproximating%20the%20diffusional%20conductance%20coefficients%20using%20stochastic%0Agradient%20descent%20and%20data%20generated%20by%20the%20current%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Voxel-based%2520Approach%2520for%2520Simulating%2520Microbial%2520Decomposition%2520in%2520Soil%253A%250A%2520%2520Comparison%2520with%2520LBM%2520and%2520Improvement%2520of%2520Morphological%2520Models%26entry.906535625%3DMouad%2520Klai%2520and%2520Olivier%2520Monga%2520and%2520Mohamed%2520Soufiane%2520Jouini%2520and%2520Val%25C3%25A9rie%2520Pot%26entry.1292438233%3D%2520%2520This%2520study%2520presents%2520a%2520new%2520computational%2520approach%2520for%2520simulating%2520the%2520microbial%250Adecomposition%2520of%2520organic%2520matter%252C%2520from%25203D%2520micro-computed%2520tomography%2520%2528micro-CT%2529%250Aimages%2520of%2520soil.%2520The%2520method%2520employs%2520a%2520valuated%2520graph%2520of%2520connected%2520voxels%2520to%250Asimulate%2520transformation%2520and%2520diffusion%2520processes%2520involved%2520in%2520microbial%250Adecomposition%2520within%2520the%2520complex%2520soil%2520matrix.%2520The%2520resulting%2520model%2520can%2520be%250Aadapted%2520to%2520simulate%2520any%2520diffusion-transformation%2520processes%2520in%2520porous%2520media.%2520We%250Aimplemented%2520parallelization%2520strategies%2520and%2520explored%2520different%2520numerical%250Amethods%252C%2520including%2520implicit%252C%2520explicit%252C%2520synchronous%252C%2520and%2520asynchronous%2520schemes.%250ATo%2520validate%2520our%2520method%252C%2520we%2520compared%2520simulation%2520outputs%2520with%2520those%2520provided%2520by%250ALBioS%2520and%2520by%2520Mosaic%2520models.%2520LBioS%2520uses%2520a%2520lattice-Boltzmann%2520method%2520for%2520diffusion%250Aand%2520Mosaic%2520takes%2520benefit%2520of%2520Pore%2520Network%2520Geometrical%2520Modelling%2520%2528PNGM%2529%2520by%2520means%250Aof%2520geometrical%2520primitives%2520such%2520as%2520spheres%2520and%2520ellipsoids.%2520This%2520approach%250Aachieved%2520comparable%2520results%2520to%2520traditional%2520LBM-based%2520simulations%252C%2520but%2520required%250Aonly%2520one-fourth%2520of%2520the%2520computing%2520time.%2520Compared%2520to%2520Mosaic%2520simulation%252C%2520the%250Aproposed%2520method%2520is%2520slower%2520but%2520more%2520accurate%2520and%2520does%2520not%2520require%2520any%250Acalibration.%2520Furthermore%252C%2520we%2520present%2520a%2520theoretical%2520framework%2520and%2520an%2520application%250Aexample%2520to%2520enhance%2520PNGM-based%2520simulations.%2520This%2520is%2520accomplished%2520by%250Aapproximating%2520the%2520diffusional%2520conductance%2520coefficients%2520using%2520stochastic%250Agradient%2520descent%2520and%2520data%2520generated%2520by%2520the%2520current%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Voxel-based%20Approach%20for%20Simulating%20Microbial%20Decomposition%20in%20Soil%3A%0A%20%20Comparison%20with%20LBM%20and%20Improvement%20of%20Morphological%20Models&entry.906535625=Mouad%20Klai%20and%20Olivier%20Monga%20and%20Mohamed%20Soufiane%20Jouini%20and%20Val%C3%A9rie%20Pot&entry.1292438233=%20%20This%20study%20presents%20a%20new%20computational%20approach%20for%20simulating%20the%20microbial%0Adecomposition%20of%20organic%20matter%2C%20from%203D%20micro-computed%20tomography%20%28micro-CT%29%0Aimages%20of%20soil.%20The%20method%20employs%20a%20valuated%20graph%20of%20connected%20voxels%20to%0Asimulate%20transformation%20and%20diffusion%20processes%20involved%20in%20microbial%0Adecomposition%20within%20the%20complex%20soil%20matrix.%20The%20resulting%20model%20can%20be%0Aadapted%20to%20simulate%20any%20diffusion-transformation%20processes%20in%20porous%20media.%20We%0Aimplemented%20parallelization%20strategies%20and%20explored%20different%20numerical%0Amethods%2C%20including%20implicit%2C%20explicit%2C%20synchronous%2C%20and%20asynchronous%20schemes.%0ATo%20validate%20our%20method%2C%20we%20compared%20simulation%20outputs%20with%20those%20provided%20by%0ALBioS%20and%20by%20Mosaic%20models.%20LBioS%20uses%20a%20lattice-Boltzmann%20method%20for%20diffusion%0Aand%20Mosaic%20takes%20benefit%20of%20Pore%20Network%20Geometrical%20Modelling%20%28PNGM%29%20by%20means%0Aof%20geometrical%20primitives%20such%20as%20spheres%20and%20ellipsoids.%20This%20approach%0Aachieved%20comparable%20results%20to%20traditional%20LBM-based%20simulations%2C%20but%20required%0Aonly%20one-fourth%20of%20the%20computing%20time.%20Compared%20to%20Mosaic%20simulation%2C%20the%0Aproposed%20method%20is%20slower%20but%20more%20accurate%20and%20does%20not%20require%20any%0Acalibration.%20Furthermore%2C%20we%20present%20a%20theoretical%20framework%20and%20an%20application%0Aexample%20to%20enhance%20PNGM-based%20simulations.%20This%20is%20accomplished%20by%0Aapproximating%20the%20diffusional%20conductance%20coefficients%20using%20stochastic%0Agradient%20descent%20and%20data%20generated%20by%20the%20current%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04177v1&entry.124074799=Read"},
{"title": "RECAP: Retrieval-Augmented Audio Captioning", "author": "Sreyan Ghosh and Sonal Kumar and Chandra Kiran Reddy Evuru and Ramani Duraiswami and Dinesh Manocha", "abstract": "  We present RECAP (REtrieval-Augmented Audio CAPtioning), a novel and\neffective audio captioning system that generates captions conditioned on an\ninput audio and other captions similar to the audio retrieved from a datastore.\nAdditionally, our proposed method can transfer to any domain without the need\nfor any additional fine-tuning. To generate a caption for an audio sample, we\nleverage an audio-text model CLAP to retrieve captions similar to it from a\nreplaceable datastore, which are then used to construct a prompt. Next, we feed\nthis prompt to a GPT-2 decoder and introduce cross-attention layers between the\nCLAP encoder and GPT-2 to condition the audio for caption generation.\nExperiments on two benchmark datasets, Clotho and AudioCaps, show that RECAP\nachieves competitive performance in in-domain settings and significant\nimprovements in out-of-domain settings. Additionally, due to its capability to\nexploit a large text-captions-only datastore in a training-free fashion, RECAP\nshows unique capabilities of captioning novel audio events never seen during\ntraining and compositional audios with multiple events. To promote research in\nthis space, we also release 150,000+ new weakly labeled captions for AudioSet,\nAudioCaps, and Clotho.\n", "link": "http://arxiv.org/abs/2309.09836v2", "date": "2024-06-06", "relevancy": 2.0483, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5214}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5076}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RECAP%3A%20Retrieval-Augmented%20Audio%20Captioning&body=Title%3A%20RECAP%3A%20Retrieval-Augmented%20Audio%20Captioning%0AAuthor%3A%20Sreyan%20Ghosh%20and%20Sonal%20Kumar%20and%20Chandra%20Kiran%20Reddy%20Evuru%20and%20Ramani%20Duraiswami%20and%20Dinesh%20Manocha%0AAbstract%3A%20%20%20We%20present%20RECAP%20%28REtrieval-Augmented%20Audio%20CAPtioning%29%2C%20a%20novel%20and%0Aeffective%20audio%20captioning%20system%20that%20generates%20captions%20conditioned%20on%20an%0Ainput%20audio%20and%20other%20captions%20similar%20to%20the%20audio%20retrieved%20from%20a%20datastore.%0AAdditionally%2C%20our%20proposed%20method%20can%20transfer%20to%20any%20domain%20without%20the%20need%0Afor%20any%20additional%20fine-tuning.%20To%20generate%20a%20caption%20for%20an%20audio%20sample%2C%20we%0Aleverage%20an%20audio-text%20model%20CLAP%20to%20retrieve%20captions%20similar%20to%20it%20from%20a%0Areplaceable%20datastore%2C%20which%20are%20then%20used%20to%20construct%20a%20prompt.%20Next%2C%20we%20feed%0Athis%20prompt%20to%20a%20GPT-2%20decoder%20and%20introduce%20cross-attention%20layers%20between%20the%0ACLAP%20encoder%20and%20GPT-2%20to%20condition%20the%20audio%20for%20caption%20generation.%0AExperiments%20on%20two%20benchmark%20datasets%2C%20Clotho%20and%20AudioCaps%2C%20show%20that%20RECAP%0Aachieves%20competitive%20performance%20in%20in-domain%20settings%20and%20significant%0Aimprovements%20in%20out-of-domain%20settings.%20Additionally%2C%20due%20to%20its%20capability%20to%0Aexploit%20a%20large%20text-captions-only%20datastore%20in%20a%20training-free%20fashion%2C%20RECAP%0Ashows%20unique%20capabilities%20of%20captioning%20novel%20audio%20events%20never%20seen%20during%0Atraining%20and%20compositional%20audios%20with%20multiple%20events.%20To%20promote%20research%20in%0Athis%20space%2C%20we%20also%20release%20150%2C000%2B%20new%20weakly%20labeled%20captions%20for%20AudioSet%2C%0AAudioCaps%2C%20and%20Clotho.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.09836v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRECAP%253A%2520Retrieval-Augmented%2520Audio%2520Captioning%26entry.906535625%3DSreyan%2520Ghosh%2520and%2520Sonal%2520Kumar%2520and%2520Chandra%2520Kiran%2520Reddy%2520Evuru%2520and%2520Ramani%2520Duraiswami%2520and%2520Dinesh%2520Manocha%26entry.1292438233%3D%2520%2520We%2520present%2520RECAP%2520%2528REtrieval-Augmented%2520Audio%2520CAPtioning%2529%252C%2520a%2520novel%2520and%250Aeffective%2520audio%2520captioning%2520system%2520that%2520generates%2520captions%2520conditioned%2520on%2520an%250Ainput%2520audio%2520and%2520other%2520captions%2520similar%2520to%2520the%2520audio%2520retrieved%2520from%2520a%2520datastore.%250AAdditionally%252C%2520our%2520proposed%2520method%2520can%2520transfer%2520to%2520any%2520domain%2520without%2520the%2520need%250Afor%2520any%2520additional%2520fine-tuning.%2520To%2520generate%2520a%2520caption%2520for%2520an%2520audio%2520sample%252C%2520we%250Aleverage%2520an%2520audio-text%2520model%2520CLAP%2520to%2520retrieve%2520captions%2520similar%2520to%2520it%2520from%2520a%250Areplaceable%2520datastore%252C%2520which%2520are%2520then%2520used%2520to%2520construct%2520a%2520prompt.%2520Next%252C%2520we%2520feed%250Athis%2520prompt%2520to%2520a%2520GPT-2%2520decoder%2520and%2520introduce%2520cross-attention%2520layers%2520between%2520the%250ACLAP%2520encoder%2520and%2520GPT-2%2520to%2520condition%2520the%2520audio%2520for%2520caption%2520generation.%250AExperiments%2520on%2520two%2520benchmark%2520datasets%252C%2520Clotho%2520and%2520AudioCaps%252C%2520show%2520that%2520RECAP%250Aachieves%2520competitive%2520performance%2520in%2520in-domain%2520settings%2520and%2520significant%250Aimprovements%2520in%2520out-of-domain%2520settings.%2520Additionally%252C%2520due%2520to%2520its%2520capability%2520to%250Aexploit%2520a%2520large%2520text-captions-only%2520datastore%2520in%2520a%2520training-free%2520fashion%252C%2520RECAP%250Ashows%2520unique%2520capabilities%2520of%2520captioning%2520novel%2520audio%2520events%2520never%2520seen%2520during%250Atraining%2520and%2520compositional%2520audios%2520with%2520multiple%2520events.%2520To%2520promote%2520research%2520in%250Athis%2520space%252C%2520we%2520also%2520release%2520150%252C000%252B%2520new%2520weakly%2520labeled%2520captions%2520for%2520AudioSet%252C%250AAudioCaps%252C%2520and%2520Clotho.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.09836v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RECAP%3A%20Retrieval-Augmented%20Audio%20Captioning&entry.906535625=Sreyan%20Ghosh%20and%20Sonal%20Kumar%20and%20Chandra%20Kiran%20Reddy%20Evuru%20and%20Ramani%20Duraiswami%20and%20Dinesh%20Manocha&entry.1292438233=%20%20We%20present%20RECAP%20%28REtrieval-Augmented%20Audio%20CAPtioning%29%2C%20a%20novel%20and%0Aeffective%20audio%20captioning%20system%20that%20generates%20captions%20conditioned%20on%20an%0Ainput%20audio%20and%20other%20captions%20similar%20to%20the%20audio%20retrieved%20from%20a%20datastore.%0AAdditionally%2C%20our%20proposed%20method%20can%20transfer%20to%20any%20domain%20without%20the%20need%0Afor%20any%20additional%20fine-tuning.%20To%20generate%20a%20caption%20for%20an%20audio%20sample%2C%20we%0Aleverage%20an%20audio-text%20model%20CLAP%20to%20retrieve%20captions%20similar%20to%20it%20from%20a%0Areplaceable%20datastore%2C%20which%20are%20then%20used%20to%20construct%20a%20prompt.%20Next%2C%20we%20feed%0Athis%20prompt%20to%20a%20GPT-2%20decoder%20and%20introduce%20cross-attention%20layers%20between%20the%0ACLAP%20encoder%20and%20GPT-2%20to%20condition%20the%20audio%20for%20caption%20generation.%0AExperiments%20on%20two%20benchmark%20datasets%2C%20Clotho%20and%20AudioCaps%2C%20show%20that%20RECAP%0Aachieves%20competitive%20performance%20in%20in-domain%20settings%20and%20significant%0Aimprovements%20in%20out-of-domain%20settings.%20Additionally%2C%20due%20to%20its%20capability%20to%0Aexploit%20a%20large%20text-captions-only%20datastore%20in%20a%20training-free%20fashion%2C%20RECAP%0Ashows%20unique%20capabilities%20of%20captioning%20novel%20audio%20events%20never%20seen%20during%0Atraining%20and%20compositional%20audios%20with%20multiple%20events.%20To%20promote%20research%20in%0Athis%20space%2C%20we%20also%20release%20150%2C000%2B%20new%20weakly%20labeled%20captions%20for%20AudioSet%2C%0AAudioCaps%2C%20and%20Clotho.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.09836v2&entry.124074799=Read"},
{"title": "ELFS: Enhancing Label-Free Coreset Selection via Clustering-based\n  Pseudo-Labeling", "author": "Haizhong Zheng and Elisa Tsai and Yifu Lu and Jiachen Sun and Brian R. Bartoldson and Bhavya Kailkhura and Atul Prakash", "abstract": "  High-quality human-annotated data is crucial for modern deep learning\npipelines, yet the human annotation process is both costly and time-consuming.\nGiven a constrained human labeling budget, selecting an informative and\nrepresentative data subset for labeling can significantly reduce human\nannotation effort. Well-performing state-of-the-art (SOTA) coreset selection\nmethods require ground-truth labels over the whole dataset, failing to reduce\nthe human labeling burden. Meanwhile, SOTA label-free coreset selection methods\ndeliver inferior performance due to poor geometry-based scores. In this paper,\nwe introduce ELFS, a novel label-free coreset selection method. ELFS employs\ndeep clustering to estimate data difficulty scores without ground-truth labels.\nFurthermore, ELFS uses a simple but effective double-end pruning method to\nmitigate bias on calculated scores, which further improves the performance on\nselected coresets. We evaluate ELFS on five vision benchmarks and show that\nELFS consistently outperforms SOTA label-free baselines. For instance, at a 90%\npruning rate, ELFS surpasses the best-performing baseline by 5.3% on CIFAR10\nand 7.1% on CIFAR100. Moreover, ELFS even achieves comparable performance to\nsupervised coreset selection at low pruning rates (e.g., 30% and 50%) on\nCIFAR10 and ImageNet-1K.\n", "link": "http://arxiv.org/abs/2406.04273v1", "date": "2024-06-06", "relevancy": 2.0476, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5297}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5256}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ELFS%3A%20Enhancing%20Label-Free%20Coreset%20Selection%20via%20Clustering-based%0A%20%20Pseudo-Labeling&body=Title%3A%20ELFS%3A%20Enhancing%20Label-Free%20Coreset%20Selection%20via%20Clustering-based%0A%20%20Pseudo-Labeling%0AAuthor%3A%20Haizhong%20Zheng%20and%20Elisa%20Tsai%20and%20Yifu%20Lu%20and%20Jiachen%20Sun%20and%20Brian%20R.%20Bartoldson%20and%20Bhavya%20Kailkhura%20and%20Atul%20Prakash%0AAbstract%3A%20%20%20High-quality%20human-annotated%20data%20is%20crucial%20for%20modern%20deep%20learning%0Apipelines%2C%20yet%20the%20human%20annotation%20process%20is%20both%20costly%20and%20time-consuming.%0AGiven%20a%20constrained%20human%20labeling%20budget%2C%20selecting%20an%20informative%20and%0Arepresentative%20data%20subset%20for%20labeling%20can%20significantly%20reduce%20human%0Aannotation%20effort.%20Well-performing%20state-of-the-art%20%28SOTA%29%20coreset%20selection%0Amethods%20require%20ground-truth%20labels%20over%20the%20whole%20dataset%2C%20failing%20to%20reduce%0Athe%20human%20labeling%20burden.%20Meanwhile%2C%20SOTA%20label-free%20coreset%20selection%20methods%0Adeliver%20inferior%20performance%20due%20to%20poor%20geometry-based%20scores.%20In%20this%20paper%2C%0Awe%20introduce%20ELFS%2C%20a%20novel%20label-free%20coreset%20selection%20method.%20ELFS%20employs%0Adeep%20clustering%20to%20estimate%20data%20difficulty%20scores%20without%20ground-truth%20labels.%0AFurthermore%2C%20ELFS%20uses%20a%20simple%20but%20effective%20double-end%20pruning%20method%20to%0Amitigate%20bias%20on%20calculated%20scores%2C%20which%20further%20improves%20the%20performance%20on%0Aselected%20coresets.%20We%20evaluate%20ELFS%20on%20five%20vision%20benchmarks%20and%20show%20that%0AELFS%20consistently%20outperforms%20SOTA%20label-free%20baselines.%20For%20instance%2C%20at%20a%2090%25%0Apruning%20rate%2C%20ELFS%20surpasses%20the%20best-performing%20baseline%20by%205.3%25%20on%20CIFAR10%0Aand%207.1%25%20on%20CIFAR100.%20Moreover%2C%20ELFS%20even%20achieves%20comparable%20performance%20to%0Asupervised%20coreset%20selection%20at%20low%20pruning%20rates%20%28e.g.%2C%2030%25%20and%2050%25%29%20on%0ACIFAR10%20and%20ImageNet-1K.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04273v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DELFS%253A%2520Enhancing%2520Label-Free%2520Coreset%2520Selection%2520via%2520Clustering-based%250A%2520%2520Pseudo-Labeling%26entry.906535625%3DHaizhong%2520Zheng%2520and%2520Elisa%2520Tsai%2520and%2520Yifu%2520Lu%2520and%2520Jiachen%2520Sun%2520and%2520Brian%2520R.%2520Bartoldson%2520and%2520Bhavya%2520Kailkhura%2520and%2520Atul%2520Prakash%26entry.1292438233%3D%2520%2520High-quality%2520human-annotated%2520data%2520is%2520crucial%2520for%2520modern%2520deep%2520learning%250Apipelines%252C%2520yet%2520the%2520human%2520annotation%2520process%2520is%2520both%2520costly%2520and%2520time-consuming.%250AGiven%2520a%2520constrained%2520human%2520labeling%2520budget%252C%2520selecting%2520an%2520informative%2520and%250Arepresentative%2520data%2520subset%2520for%2520labeling%2520can%2520significantly%2520reduce%2520human%250Aannotation%2520effort.%2520Well-performing%2520state-of-the-art%2520%2528SOTA%2529%2520coreset%2520selection%250Amethods%2520require%2520ground-truth%2520labels%2520over%2520the%2520whole%2520dataset%252C%2520failing%2520to%2520reduce%250Athe%2520human%2520labeling%2520burden.%2520Meanwhile%252C%2520SOTA%2520label-free%2520coreset%2520selection%2520methods%250Adeliver%2520inferior%2520performance%2520due%2520to%2520poor%2520geometry-based%2520scores.%2520In%2520this%2520paper%252C%250Awe%2520introduce%2520ELFS%252C%2520a%2520novel%2520label-free%2520coreset%2520selection%2520method.%2520ELFS%2520employs%250Adeep%2520clustering%2520to%2520estimate%2520data%2520difficulty%2520scores%2520without%2520ground-truth%2520labels.%250AFurthermore%252C%2520ELFS%2520uses%2520a%2520simple%2520but%2520effective%2520double-end%2520pruning%2520method%2520to%250Amitigate%2520bias%2520on%2520calculated%2520scores%252C%2520which%2520further%2520improves%2520the%2520performance%2520on%250Aselected%2520coresets.%2520We%2520evaluate%2520ELFS%2520on%2520five%2520vision%2520benchmarks%2520and%2520show%2520that%250AELFS%2520consistently%2520outperforms%2520SOTA%2520label-free%2520baselines.%2520For%2520instance%252C%2520at%2520a%252090%2525%250Apruning%2520rate%252C%2520ELFS%2520surpasses%2520the%2520best-performing%2520baseline%2520by%25205.3%2525%2520on%2520CIFAR10%250Aand%25207.1%2525%2520on%2520CIFAR100.%2520Moreover%252C%2520ELFS%2520even%2520achieves%2520comparable%2520performance%2520to%250Asupervised%2520coreset%2520selection%2520at%2520low%2520pruning%2520rates%2520%2528e.g.%252C%252030%2525%2520and%252050%2525%2529%2520on%250ACIFAR10%2520and%2520ImageNet-1K.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04273v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ELFS%3A%20Enhancing%20Label-Free%20Coreset%20Selection%20via%20Clustering-based%0A%20%20Pseudo-Labeling&entry.906535625=Haizhong%20Zheng%20and%20Elisa%20Tsai%20and%20Yifu%20Lu%20and%20Jiachen%20Sun%20and%20Brian%20R.%20Bartoldson%20and%20Bhavya%20Kailkhura%20and%20Atul%20Prakash&entry.1292438233=%20%20High-quality%20human-annotated%20data%20is%20crucial%20for%20modern%20deep%20learning%0Apipelines%2C%20yet%20the%20human%20annotation%20process%20is%20both%20costly%20and%20time-consuming.%0AGiven%20a%20constrained%20human%20labeling%20budget%2C%20selecting%20an%20informative%20and%0Arepresentative%20data%20subset%20for%20labeling%20can%20significantly%20reduce%20human%0Aannotation%20effort.%20Well-performing%20state-of-the-art%20%28SOTA%29%20coreset%20selection%0Amethods%20require%20ground-truth%20labels%20over%20the%20whole%20dataset%2C%20failing%20to%20reduce%0Athe%20human%20labeling%20burden.%20Meanwhile%2C%20SOTA%20label-free%20coreset%20selection%20methods%0Adeliver%20inferior%20performance%20due%20to%20poor%20geometry-based%20scores.%20In%20this%20paper%2C%0Awe%20introduce%20ELFS%2C%20a%20novel%20label-free%20coreset%20selection%20method.%20ELFS%20employs%0Adeep%20clustering%20to%20estimate%20data%20difficulty%20scores%20without%20ground-truth%20labels.%0AFurthermore%2C%20ELFS%20uses%20a%20simple%20but%20effective%20double-end%20pruning%20method%20to%0Amitigate%20bias%20on%20calculated%20scores%2C%20which%20further%20improves%20the%20performance%20on%0Aselected%20coresets.%20We%20evaluate%20ELFS%20on%20five%20vision%20benchmarks%20and%20show%20that%0AELFS%20consistently%20outperforms%20SOTA%20label-free%20baselines.%20For%20instance%2C%20at%20a%2090%25%0Apruning%20rate%2C%20ELFS%20surpasses%20the%20best-performing%20baseline%20by%205.3%25%20on%20CIFAR10%0Aand%207.1%25%20on%20CIFAR100.%20Moreover%2C%20ELFS%20even%20achieves%20comparable%20performance%20to%0Asupervised%20coreset%20selection%20at%20low%20pruning%20rates%20%28e.g.%2C%2030%25%20and%2050%25%29%20on%0ACIFAR10%20and%20ImageNet-1K.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04273v1&entry.124074799=Read"},
{"title": "Scaling and evaluating sparse autoencoders", "author": "Leo Gao and Tom Dupr\u00e9 la Tour and Henk Tillman and Gabriel Goh and Rajan Troll and Alec Radford and Ilya Sutskever and Jan Leike and Jeffrey Wu", "abstract": "  Sparse autoencoders provide a promising unsupervised approach for extracting\ninterpretable features from a language model by reconstructing activations from\na sparse bottleneck layer. Since language models learn many concepts,\nautoencoders need to be very large to recover all relevant features. However,\nstudying the properties of autoencoder scaling is difficult due to the need to\nbalance reconstruction and sparsity objectives and the presence of dead\nlatents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to\ndirectly control sparsity, simplifying tuning and improving the\nreconstruction-sparsity frontier. Additionally, we find modifications that\nresult in few dead latents, even at the largest scales we tried. Using these\ntechniques, we find clean scaling laws with respect to autoencoder size and\nsparsity. We also introduce several new metrics for evaluating feature quality\nbased on the recovery of hypothesized features, the explainability of\nactivation patterns, and the sparsity of downstream effects. These metrics all\ngenerally improve with autoencoder size. To demonstrate the scalability of our\napproach, we train a 16 million latent autoencoder on GPT-4 activations for 40\nbillion tokens. We release training code and autoencoders for open-source\nmodels, as well as a visualizer.\n", "link": "http://arxiv.org/abs/2406.04093v1", "date": "2024-06-06", "relevancy": 2.0453, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5323}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5217}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20and%20evaluating%20sparse%20autoencoders&body=Title%3A%20Scaling%20and%20evaluating%20sparse%20autoencoders%0AAuthor%3A%20Leo%20Gao%20and%20Tom%20Dupr%C3%A9%20la%20Tour%20and%20Henk%20Tillman%20and%20Gabriel%20Goh%20and%20Rajan%20Troll%20and%20Alec%20Radford%20and%20Ilya%20Sutskever%20and%20Jan%20Leike%20and%20Jeffrey%20Wu%0AAbstract%3A%20%20%20Sparse%20autoencoders%20provide%20a%20promising%20unsupervised%20approach%20for%20extracting%0Ainterpretable%20features%20from%20a%20language%20model%20by%20reconstructing%20activations%20from%0Aa%20sparse%20bottleneck%20layer.%20Since%20language%20models%20learn%20many%20concepts%2C%0Aautoencoders%20need%20to%20be%20very%20large%20to%20recover%20all%20relevant%20features.%20However%2C%0Astudying%20the%20properties%20of%20autoencoder%20scaling%20is%20difficult%20due%20to%20the%20need%20to%0Abalance%20reconstruction%20and%20sparsity%20objectives%20and%20the%20presence%20of%20dead%0Alatents.%20We%20propose%20using%20k-sparse%20autoencoders%20%5BMakhzani%20and%20Frey%2C%202013%5D%20to%0Adirectly%20control%20sparsity%2C%20simplifying%20tuning%20and%20improving%20the%0Areconstruction-sparsity%20frontier.%20Additionally%2C%20we%20find%20modifications%20that%0Aresult%20in%20few%20dead%20latents%2C%20even%20at%20the%20largest%20scales%20we%20tried.%20Using%20these%0Atechniques%2C%20we%20find%20clean%20scaling%20laws%20with%20respect%20to%20autoencoder%20size%20and%0Asparsity.%20We%20also%20introduce%20several%20new%20metrics%20for%20evaluating%20feature%20quality%0Abased%20on%20the%20recovery%20of%20hypothesized%20features%2C%20the%20explainability%20of%0Aactivation%20patterns%2C%20and%20the%20sparsity%20of%20downstream%20effects.%20These%20metrics%20all%0Agenerally%20improve%20with%20autoencoder%20size.%20To%20demonstrate%20the%20scalability%20of%20our%0Aapproach%2C%20we%20train%20a%2016%20million%20latent%20autoencoder%20on%20GPT-4%20activations%20for%2040%0Abillion%20tokens.%20We%20release%20training%20code%20and%20autoencoders%20for%20open-source%0Amodels%2C%20as%20well%20as%20a%20visualizer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520and%2520evaluating%2520sparse%2520autoencoders%26entry.906535625%3DLeo%2520Gao%2520and%2520Tom%2520Dupr%25C3%25A9%2520la%2520Tour%2520and%2520Henk%2520Tillman%2520and%2520Gabriel%2520Goh%2520and%2520Rajan%2520Troll%2520and%2520Alec%2520Radford%2520and%2520Ilya%2520Sutskever%2520and%2520Jan%2520Leike%2520and%2520Jeffrey%2520Wu%26entry.1292438233%3D%2520%2520Sparse%2520autoencoders%2520provide%2520a%2520promising%2520unsupervised%2520approach%2520for%2520extracting%250Ainterpretable%2520features%2520from%2520a%2520language%2520model%2520by%2520reconstructing%2520activations%2520from%250Aa%2520sparse%2520bottleneck%2520layer.%2520Since%2520language%2520models%2520learn%2520many%2520concepts%252C%250Aautoencoders%2520need%2520to%2520be%2520very%2520large%2520to%2520recover%2520all%2520relevant%2520features.%2520However%252C%250Astudying%2520the%2520properties%2520of%2520autoencoder%2520scaling%2520is%2520difficult%2520due%2520to%2520the%2520need%2520to%250Abalance%2520reconstruction%2520and%2520sparsity%2520objectives%2520and%2520the%2520presence%2520of%2520dead%250Alatents.%2520We%2520propose%2520using%2520k-sparse%2520autoencoders%2520%255BMakhzani%2520and%2520Frey%252C%25202013%255D%2520to%250Adirectly%2520control%2520sparsity%252C%2520simplifying%2520tuning%2520and%2520improving%2520the%250Areconstruction-sparsity%2520frontier.%2520Additionally%252C%2520we%2520find%2520modifications%2520that%250Aresult%2520in%2520few%2520dead%2520latents%252C%2520even%2520at%2520the%2520largest%2520scales%2520we%2520tried.%2520Using%2520these%250Atechniques%252C%2520we%2520find%2520clean%2520scaling%2520laws%2520with%2520respect%2520to%2520autoencoder%2520size%2520and%250Asparsity.%2520We%2520also%2520introduce%2520several%2520new%2520metrics%2520for%2520evaluating%2520feature%2520quality%250Abased%2520on%2520the%2520recovery%2520of%2520hypothesized%2520features%252C%2520the%2520explainability%2520of%250Aactivation%2520patterns%252C%2520and%2520the%2520sparsity%2520of%2520downstream%2520effects.%2520These%2520metrics%2520all%250Agenerally%2520improve%2520with%2520autoencoder%2520size.%2520To%2520demonstrate%2520the%2520scalability%2520of%2520our%250Aapproach%252C%2520we%2520train%2520a%252016%2520million%2520latent%2520autoencoder%2520on%2520GPT-4%2520activations%2520for%252040%250Abillion%2520tokens.%2520We%2520release%2520training%2520code%2520and%2520autoencoders%2520for%2520open-source%250Amodels%252C%2520as%2520well%2520as%2520a%2520visualizer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20and%20evaluating%20sparse%20autoencoders&entry.906535625=Leo%20Gao%20and%20Tom%20Dupr%C3%A9%20la%20Tour%20and%20Henk%20Tillman%20and%20Gabriel%20Goh%20and%20Rajan%20Troll%20and%20Alec%20Radford%20and%20Ilya%20Sutskever%20and%20Jan%20Leike%20and%20Jeffrey%20Wu&entry.1292438233=%20%20Sparse%20autoencoders%20provide%20a%20promising%20unsupervised%20approach%20for%20extracting%0Ainterpretable%20features%20from%20a%20language%20model%20by%20reconstructing%20activations%20from%0Aa%20sparse%20bottleneck%20layer.%20Since%20language%20models%20learn%20many%20concepts%2C%0Aautoencoders%20need%20to%20be%20very%20large%20to%20recover%20all%20relevant%20features.%20However%2C%0Astudying%20the%20properties%20of%20autoencoder%20scaling%20is%20difficult%20due%20to%20the%20need%20to%0Abalance%20reconstruction%20and%20sparsity%20objectives%20and%20the%20presence%20of%20dead%0Alatents.%20We%20propose%20using%20k-sparse%20autoencoders%20%5BMakhzani%20and%20Frey%2C%202013%5D%20to%0Adirectly%20control%20sparsity%2C%20simplifying%20tuning%20and%20improving%20the%0Areconstruction-sparsity%20frontier.%20Additionally%2C%20we%20find%20modifications%20that%0Aresult%20in%20few%20dead%20latents%2C%20even%20at%20the%20largest%20scales%20we%20tried.%20Using%20these%0Atechniques%2C%20we%20find%20clean%20scaling%20laws%20with%20respect%20to%20autoencoder%20size%20and%0Asparsity.%20We%20also%20introduce%20several%20new%20metrics%20for%20evaluating%20feature%20quality%0Abased%20on%20the%20recovery%20of%20hypothesized%20features%2C%20the%20explainability%20of%0Aactivation%20patterns%2C%20and%20the%20sparsity%20of%20downstream%20effects.%20These%20metrics%20all%0Agenerally%20improve%20with%20autoencoder%20size.%20To%20demonstrate%20the%20scalability%20of%20our%0Aapproach%2C%20we%20train%20a%2016%20million%20latent%20autoencoder%20on%20GPT-4%20activations%20for%2040%0Abillion%20tokens.%20We%20release%20training%20code%20and%20autoencoders%20for%20open-source%0Amodels%2C%20as%20well%20as%20a%20visualizer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04093v1&entry.124074799=Read"},
{"title": "Conv-INR: Convolutional Implicit Neural Representation for Multimodal\n  Visual Signals", "author": "Zhicheng Cai", "abstract": "  Implicit neural representation (INR) has recently emerged as a promising\nparadigm for signal representations. Typically, INR is parameterized by a\nmultiplayer perceptron (MLP) which takes the coordinates as the inputs and\ngenerates corresponding attributes of a signal. However, MLP-based INRs face\ntwo critical issues: i) individually considering each coordinate while ignoring\nthe connections; ii) suffering from the spectral bias thus failing to learn\nhigh-frequency components. While target visual signals usually exhibit strong\nlocal structures and neighborhood dependencies, and high-frequency components\nare significant in these signals, the issues harm the representational capacity\nof INRs. This paper proposes Conv-INR, the first INR model fully based on\nconvolution. Due to the inherent attributes of convolution, Conv-INR can\nsimultaneously consider adjacent coordinates and learn high-frequency\ncomponents effectively. Compared to existing MLP-based INRs, Conv-INR has\nbetter representational capacity and trainability without requiring primary\nfunction expansion. We conduct extensive experiments on four tasks, including\nimage fitting, CT/MRI reconstruction, and novel view synthesis, Conv-INR all\nsignificantly surpasses existing MLP-based INRs, validating the effectiveness.\nFinally, we raise three reparameterization methods that can further enhance the\nperformance of the vanilla Conv-INR without introducing any extra inference\ncost.\n", "link": "http://arxiv.org/abs/2406.04249v1", "date": "2024-06-06", "relevancy": 2.0402, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.529}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5141}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conv-INR%3A%20Convolutional%20Implicit%20Neural%20Representation%20for%20Multimodal%0A%20%20Visual%20Signals&body=Title%3A%20Conv-INR%3A%20Convolutional%20Implicit%20Neural%20Representation%20for%20Multimodal%0A%20%20Visual%20Signals%0AAuthor%3A%20Zhicheng%20Cai%0AAbstract%3A%20%20%20Implicit%20neural%20representation%20%28INR%29%20has%20recently%20emerged%20as%20a%20promising%0Aparadigm%20for%20signal%20representations.%20Typically%2C%20INR%20is%20parameterized%20by%20a%0Amultiplayer%20perceptron%20%28MLP%29%20which%20takes%20the%20coordinates%20as%20the%20inputs%20and%0Agenerates%20corresponding%20attributes%20of%20a%20signal.%20However%2C%20MLP-based%20INRs%20face%0Atwo%20critical%20issues%3A%20i%29%20individually%20considering%20each%20coordinate%20while%20ignoring%0Athe%20connections%3B%20ii%29%20suffering%20from%20the%20spectral%20bias%20thus%20failing%20to%20learn%0Ahigh-frequency%20components.%20While%20target%20visual%20signals%20usually%20exhibit%20strong%0Alocal%20structures%20and%20neighborhood%20dependencies%2C%20and%20high-frequency%20components%0Aare%20significant%20in%20these%20signals%2C%20the%20issues%20harm%20the%20representational%20capacity%0Aof%20INRs.%20This%20paper%20proposes%20Conv-INR%2C%20the%20first%20INR%20model%20fully%20based%20on%0Aconvolution.%20Due%20to%20the%20inherent%20attributes%20of%20convolution%2C%20Conv-INR%20can%0Asimultaneously%20consider%20adjacent%20coordinates%20and%20learn%20high-frequency%0Acomponents%20effectively.%20Compared%20to%20existing%20MLP-based%20INRs%2C%20Conv-INR%20has%0Abetter%20representational%20capacity%20and%20trainability%20without%20requiring%20primary%0Afunction%20expansion.%20We%20conduct%20extensive%20experiments%20on%20four%20tasks%2C%20including%0Aimage%20fitting%2C%20CT/MRI%20reconstruction%2C%20and%20novel%20view%20synthesis%2C%20Conv-INR%20all%0Asignificantly%20surpasses%20existing%20MLP-based%20INRs%2C%20validating%20the%20effectiveness.%0AFinally%2C%20we%20raise%20three%20reparameterization%20methods%20that%20can%20further%20enhance%20the%0Aperformance%20of%20the%20vanilla%20Conv-INR%20without%20introducing%20any%20extra%20inference%0Acost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConv-INR%253A%2520Convolutional%2520Implicit%2520Neural%2520Representation%2520for%2520Multimodal%250A%2520%2520Visual%2520Signals%26entry.906535625%3DZhicheng%2520Cai%26entry.1292438233%3D%2520%2520Implicit%2520neural%2520representation%2520%2528INR%2529%2520has%2520recently%2520emerged%2520as%2520a%2520promising%250Aparadigm%2520for%2520signal%2520representations.%2520Typically%252C%2520INR%2520is%2520parameterized%2520by%2520a%250Amultiplayer%2520perceptron%2520%2528MLP%2529%2520which%2520takes%2520the%2520coordinates%2520as%2520the%2520inputs%2520and%250Agenerates%2520corresponding%2520attributes%2520of%2520a%2520signal.%2520However%252C%2520MLP-based%2520INRs%2520face%250Atwo%2520critical%2520issues%253A%2520i%2529%2520individually%2520considering%2520each%2520coordinate%2520while%2520ignoring%250Athe%2520connections%253B%2520ii%2529%2520suffering%2520from%2520the%2520spectral%2520bias%2520thus%2520failing%2520to%2520learn%250Ahigh-frequency%2520components.%2520While%2520target%2520visual%2520signals%2520usually%2520exhibit%2520strong%250Alocal%2520structures%2520and%2520neighborhood%2520dependencies%252C%2520and%2520high-frequency%2520components%250Aare%2520significant%2520in%2520these%2520signals%252C%2520the%2520issues%2520harm%2520the%2520representational%2520capacity%250Aof%2520INRs.%2520This%2520paper%2520proposes%2520Conv-INR%252C%2520the%2520first%2520INR%2520model%2520fully%2520based%2520on%250Aconvolution.%2520Due%2520to%2520the%2520inherent%2520attributes%2520of%2520convolution%252C%2520Conv-INR%2520can%250Asimultaneously%2520consider%2520adjacent%2520coordinates%2520and%2520learn%2520high-frequency%250Acomponents%2520effectively.%2520Compared%2520to%2520existing%2520MLP-based%2520INRs%252C%2520Conv-INR%2520has%250Abetter%2520representational%2520capacity%2520and%2520trainability%2520without%2520requiring%2520primary%250Afunction%2520expansion.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520four%2520tasks%252C%2520including%250Aimage%2520fitting%252C%2520CT/MRI%2520reconstruction%252C%2520and%2520novel%2520view%2520synthesis%252C%2520Conv-INR%2520all%250Asignificantly%2520surpasses%2520existing%2520MLP-based%2520INRs%252C%2520validating%2520the%2520effectiveness.%250AFinally%252C%2520we%2520raise%2520three%2520reparameterization%2520methods%2520that%2520can%2520further%2520enhance%2520the%250Aperformance%2520of%2520the%2520vanilla%2520Conv-INR%2520without%2520introducing%2520any%2520extra%2520inference%250Acost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conv-INR%3A%20Convolutional%20Implicit%20Neural%20Representation%20for%20Multimodal%0A%20%20Visual%20Signals&entry.906535625=Zhicheng%20Cai&entry.1292438233=%20%20Implicit%20neural%20representation%20%28INR%29%20has%20recently%20emerged%20as%20a%20promising%0Aparadigm%20for%20signal%20representations.%20Typically%2C%20INR%20is%20parameterized%20by%20a%0Amultiplayer%20perceptron%20%28MLP%29%20which%20takes%20the%20coordinates%20as%20the%20inputs%20and%0Agenerates%20corresponding%20attributes%20of%20a%20signal.%20However%2C%20MLP-based%20INRs%20face%0Atwo%20critical%20issues%3A%20i%29%20individually%20considering%20each%20coordinate%20while%20ignoring%0Athe%20connections%3B%20ii%29%20suffering%20from%20the%20spectral%20bias%20thus%20failing%20to%20learn%0Ahigh-frequency%20components.%20While%20target%20visual%20signals%20usually%20exhibit%20strong%0Alocal%20structures%20and%20neighborhood%20dependencies%2C%20and%20high-frequency%20components%0Aare%20significant%20in%20these%20signals%2C%20the%20issues%20harm%20the%20representational%20capacity%0Aof%20INRs.%20This%20paper%20proposes%20Conv-INR%2C%20the%20first%20INR%20model%20fully%20based%20on%0Aconvolution.%20Due%20to%20the%20inherent%20attributes%20of%20convolution%2C%20Conv-INR%20can%0Asimultaneously%20consider%20adjacent%20coordinates%20and%20learn%20high-frequency%0Acomponents%20effectively.%20Compared%20to%20existing%20MLP-based%20INRs%2C%20Conv-INR%20has%0Abetter%20representational%20capacity%20and%20trainability%20without%20requiring%20primary%0Afunction%20expansion.%20We%20conduct%20extensive%20experiments%20on%20four%20tasks%2C%20including%0Aimage%20fitting%2C%20CT/MRI%20reconstruction%2C%20and%20novel%20view%20synthesis%2C%20Conv-INR%20all%0Asignificantly%20surpasses%20existing%20MLP-based%20INRs%2C%20validating%20the%20effectiveness.%0AFinally%2C%20we%20raise%20three%20reparameterization%20methods%20that%20can%20further%20enhance%20the%0Aperformance%20of%20the%20vanilla%20Conv-INR%20without%20introducing%20any%20extra%20inference%0Acost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04249v1&entry.124074799=Read"},
{"title": "Encoding Semantic Priors into the Weights of Implicit Neural\n  Representation", "author": "Zhicheng Cai and Qiu Shen", "abstract": "  Implicit neural representation (INR) has recently emerged as a promising\nparadigm for signal representations, which takes coordinates as inputs and\ngenerates corresponding signal values. Since these coordinates contain no\nsemantic features, INR fails to take any semantic information into\nconsideration. However, semantic information has been proven critical in many\nvision tasks, especially for visual signal representation. This paper proposes\na reparameterization method termed as SPW, which encodes the semantic priors to\nthe weights of INR, thus making INR contain semantic information implicitly and\nenhancing its representational capacity. Specifically, SPW uses the Semantic\nNeural Network (SNN) to extract both low- and high-level semantic information\nof the target visual signal and generates the semantic vector, which is input\ninto the Weight Generation Network (WGN) to generate the weights of INR model.\nFinally, INR uses the generated weights with semantic priors to map the\ncoordinates to the signal values. After training, we only retain the generated\nweights while abandoning both SNN and WGN, thus SPW introduces no extra costs\nin inference. Experimental results show that SPW can improve the performance of\nvarious INR models significantly on various tasks, including image fitting, CT\nreconstruction, MRI reconstruction, and novel view synthesis. Further\nexperiments illustrate that model with SPW has lower weight redundancy and\nlearns more novel representations, validating the effectiveness of SPW.\n", "link": "http://arxiv.org/abs/2406.04178v1", "date": "2024-06-06", "relevancy": 2.0386, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.538}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.527}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4744}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Encoding%20Semantic%20Priors%20into%20the%20Weights%20of%20Implicit%20Neural%0A%20%20Representation&body=Title%3A%20Encoding%20Semantic%20Priors%20into%20the%20Weights%20of%20Implicit%20Neural%0A%20%20Representation%0AAuthor%3A%20Zhicheng%20Cai%20and%20Qiu%20Shen%0AAbstract%3A%20%20%20Implicit%20neural%20representation%20%28INR%29%20has%20recently%20emerged%20as%20a%20promising%0Aparadigm%20for%20signal%20representations%2C%20which%20takes%20coordinates%20as%20inputs%20and%0Agenerates%20corresponding%20signal%20values.%20Since%20these%20coordinates%20contain%20no%0Asemantic%20features%2C%20INR%20fails%20to%20take%20any%20semantic%20information%20into%0Aconsideration.%20However%2C%20semantic%20information%20has%20been%20proven%20critical%20in%20many%0Avision%20tasks%2C%20especially%20for%20visual%20signal%20representation.%20This%20paper%20proposes%0Aa%20reparameterization%20method%20termed%20as%20SPW%2C%20which%20encodes%20the%20semantic%20priors%20to%0Athe%20weights%20of%20INR%2C%20thus%20making%20INR%20contain%20semantic%20information%20implicitly%20and%0Aenhancing%20its%20representational%20capacity.%20Specifically%2C%20SPW%20uses%20the%20Semantic%0ANeural%20Network%20%28SNN%29%20to%20extract%20both%20low-%20and%20high-level%20semantic%20information%0Aof%20the%20target%20visual%20signal%20and%20generates%20the%20semantic%20vector%2C%20which%20is%20input%0Ainto%20the%20Weight%20Generation%20Network%20%28WGN%29%20to%20generate%20the%20weights%20of%20INR%20model.%0AFinally%2C%20INR%20uses%20the%20generated%20weights%20with%20semantic%20priors%20to%20map%20the%0Acoordinates%20to%20the%20signal%20values.%20After%20training%2C%20we%20only%20retain%20the%20generated%0Aweights%20while%20abandoning%20both%20SNN%20and%20WGN%2C%20thus%20SPW%20introduces%20no%20extra%20costs%0Ain%20inference.%20Experimental%20results%20show%20that%20SPW%20can%20improve%20the%20performance%20of%0Avarious%20INR%20models%20significantly%20on%20various%20tasks%2C%20including%20image%20fitting%2C%20CT%0Areconstruction%2C%20MRI%20reconstruction%2C%20and%20novel%20view%20synthesis.%20Further%0Aexperiments%20illustrate%20that%20model%20with%20SPW%20has%20lower%20weight%20redundancy%20and%0Alearns%20more%20novel%20representations%2C%20validating%20the%20effectiveness%20of%20SPW.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04178v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEncoding%2520Semantic%2520Priors%2520into%2520the%2520Weights%2520of%2520Implicit%2520Neural%250A%2520%2520Representation%26entry.906535625%3DZhicheng%2520Cai%2520and%2520Qiu%2520Shen%26entry.1292438233%3D%2520%2520Implicit%2520neural%2520representation%2520%2528INR%2529%2520has%2520recently%2520emerged%2520as%2520a%2520promising%250Aparadigm%2520for%2520signal%2520representations%252C%2520which%2520takes%2520coordinates%2520as%2520inputs%2520and%250Agenerates%2520corresponding%2520signal%2520values.%2520Since%2520these%2520coordinates%2520contain%2520no%250Asemantic%2520features%252C%2520INR%2520fails%2520to%2520take%2520any%2520semantic%2520information%2520into%250Aconsideration.%2520However%252C%2520semantic%2520information%2520has%2520been%2520proven%2520critical%2520in%2520many%250Avision%2520tasks%252C%2520especially%2520for%2520visual%2520signal%2520representation.%2520This%2520paper%2520proposes%250Aa%2520reparameterization%2520method%2520termed%2520as%2520SPW%252C%2520which%2520encodes%2520the%2520semantic%2520priors%2520to%250Athe%2520weights%2520of%2520INR%252C%2520thus%2520making%2520INR%2520contain%2520semantic%2520information%2520implicitly%2520and%250Aenhancing%2520its%2520representational%2520capacity.%2520Specifically%252C%2520SPW%2520uses%2520the%2520Semantic%250ANeural%2520Network%2520%2528SNN%2529%2520to%2520extract%2520both%2520low-%2520and%2520high-level%2520semantic%2520information%250Aof%2520the%2520target%2520visual%2520signal%2520and%2520generates%2520the%2520semantic%2520vector%252C%2520which%2520is%2520input%250Ainto%2520the%2520Weight%2520Generation%2520Network%2520%2528WGN%2529%2520to%2520generate%2520the%2520weights%2520of%2520INR%2520model.%250AFinally%252C%2520INR%2520uses%2520the%2520generated%2520weights%2520with%2520semantic%2520priors%2520to%2520map%2520the%250Acoordinates%2520to%2520the%2520signal%2520values.%2520After%2520training%252C%2520we%2520only%2520retain%2520the%2520generated%250Aweights%2520while%2520abandoning%2520both%2520SNN%2520and%2520WGN%252C%2520thus%2520SPW%2520introduces%2520no%2520extra%2520costs%250Ain%2520inference.%2520Experimental%2520results%2520show%2520that%2520SPW%2520can%2520improve%2520the%2520performance%2520of%250Avarious%2520INR%2520models%2520significantly%2520on%2520various%2520tasks%252C%2520including%2520image%2520fitting%252C%2520CT%250Areconstruction%252C%2520MRI%2520reconstruction%252C%2520and%2520novel%2520view%2520synthesis.%2520Further%250Aexperiments%2520illustrate%2520that%2520model%2520with%2520SPW%2520has%2520lower%2520weight%2520redundancy%2520and%250Alearns%2520more%2520novel%2520representations%252C%2520validating%2520the%2520effectiveness%2520of%2520SPW.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04178v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Encoding%20Semantic%20Priors%20into%20the%20Weights%20of%20Implicit%20Neural%0A%20%20Representation&entry.906535625=Zhicheng%20Cai%20and%20Qiu%20Shen&entry.1292438233=%20%20Implicit%20neural%20representation%20%28INR%29%20has%20recently%20emerged%20as%20a%20promising%0Aparadigm%20for%20signal%20representations%2C%20which%20takes%20coordinates%20as%20inputs%20and%0Agenerates%20corresponding%20signal%20values.%20Since%20these%20coordinates%20contain%20no%0Asemantic%20features%2C%20INR%20fails%20to%20take%20any%20semantic%20information%20into%0Aconsideration.%20However%2C%20semantic%20information%20has%20been%20proven%20critical%20in%20many%0Avision%20tasks%2C%20especially%20for%20visual%20signal%20representation.%20This%20paper%20proposes%0Aa%20reparameterization%20method%20termed%20as%20SPW%2C%20which%20encodes%20the%20semantic%20priors%20to%0Athe%20weights%20of%20INR%2C%20thus%20making%20INR%20contain%20semantic%20information%20implicitly%20and%0Aenhancing%20its%20representational%20capacity.%20Specifically%2C%20SPW%20uses%20the%20Semantic%0ANeural%20Network%20%28SNN%29%20to%20extract%20both%20low-%20and%20high-level%20semantic%20information%0Aof%20the%20target%20visual%20signal%20and%20generates%20the%20semantic%20vector%2C%20which%20is%20input%0Ainto%20the%20Weight%20Generation%20Network%20%28WGN%29%20to%20generate%20the%20weights%20of%20INR%20model.%0AFinally%2C%20INR%20uses%20the%20generated%20weights%20with%20semantic%20priors%20to%20map%20the%0Acoordinates%20to%20the%20signal%20values.%20After%20training%2C%20we%20only%20retain%20the%20generated%0Aweights%20while%20abandoning%20both%20SNN%20and%20WGN%2C%20thus%20SPW%20introduces%20no%20extra%20costs%0Ain%20inference.%20Experimental%20results%20show%20that%20SPW%20can%20improve%20the%20performance%20of%0Avarious%20INR%20models%20significantly%20on%20various%20tasks%2C%20including%20image%20fitting%2C%20CT%0Areconstruction%2C%20MRI%20reconstruction%2C%20and%20novel%20view%20synthesis.%20Further%0Aexperiments%20illustrate%20that%20model%20with%20SPW%20has%20lower%20weight%20redundancy%20and%0Alearns%20more%20novel%20representations%2C%20validating%20the%20effectiveness%20of%20SPW.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04178v1&entry.124074799=Read"},
{"title": "Statistically Optimal Generative Modeling with Maximum Deviation from\n  the Empirical Distribution", "author": "Elen Vardanyan and Sona Hunanyan and Tigran Galstyan and Arshak Minasyan and Arnak Dalalyan", "abstract": "  This paper explores the problem of generative modeling, aiming to simulate\ndiverse examples from an unknown distribution based on observed examples. While\nrecent studies have focused on quantifying the statistical precision of popular\nalgorithms, there is a lack of mathematical evaluation regarding the\nnon-replication of observed examples and the creativity of the generative\nmodel. We present theoretical insights into this aspect, demonstrating that the\nWasserstein GAN, constrained to left-invertible push-forward maps, generates\ndistributions that avoid replication and significantly deviate from the\nempirical distribution. Importantly, we show that left-invertibility achieves\nthis without compromising the statistical optimality of the resulting\ngenerator. Our most important contribution provides a finite-sample lower bound\non the Wasserstein-1 distance between the generative distribution and the\nempirical one. We also establish a finite-sample upper bound on the distance\nbetween the generative distribution and the true data-generating one. Both\nbounds are explicit and show the impact of key parameters such as sample size,\ndimensions of the ambient and latent spaces, noise level, and smoothness\nmeasured by the Lipschitz constant.\n", "link": "http://arxiv.org/abs/2307.16422v2", "date": "2024-06-06", "relevancy": 2.0325, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.529}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5132}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4947}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Statistically%20Optimal%20Generative%20Modeling%20with%20Maximum%20Deviation%20from%0A%20%20the%20Empirical%20Distribution&body=Title%3A%20Statistically%20Optimal%20Generative%20Modeling%20with%20Maximum%20Deviation%20from%0A%20%20the%20Empirical%20Distribution%0AAuthor%3A%20Elen%20Vardanyan%20and%20Sona%20Hunanyan%20and%20Tigran%20Galstyan%20and%20Arshak%20Minasyan%20and%20Arnak%20Dalalyan%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20problem%20of%20generative%20modeling%2C%20aiming%20to%20simulate%0Adiverse%20examples%20from%20an%20unknown%20distribution%20based%20on%20observed%20examples.%20While%0Arecent%20studies%20have%20focused%20on%20quantifying%20the%20statistical%20precision%20of%20popular%0Aalgorithms%2C%20there%20is%20a%20lack%20of%20mathematical%20evaluation%20regarding%20the%0Anon-replication%20of%20observed%20examples%20and%20the%20creativity%20of%20the%20generative%0Amodel.%20We%20present%20theoretical%20insights%20into%20this%20aspect%2C%20demonstrating%20that%20the%0AWasserstein%20GAN%2C%20constrained%20to%20left-invertible%20push-forward%20maps%2C%20generates%0Adistributions%20that%20avoid%20replication%20and%20significantly%20deviate%20from%20the%0Aempirical%20distribution.%20Importantly%2C%20we%20show%20that%20left-invertibility%20achieves%0Athis%20without%20compromising%20the%20statistical%20optimality%20of%20the%20resulting%0Agenerator.%20Our%20most%20important%20contribution%20provides%20a%20finite-sample%20lower%20bound%0Aon%20the%20Wasserstein-1%20distance%20between%20the%20generative%20distribution%20and%20the%0Aempirical%20one.%20We%20also%20establish%20a%20finite-sample%20upper%20bound%20on%20the%20distance%0Abetween%20the%20generative%20distribution%20and%20the%20true%20data-generating%20one.%20Both%0Abounds%20are%20explicit%20and%20show%20the%20impact%20of%20key%20parameters%20such%20as%20sample%20size%2C%0Adimensions%20of%20the%20ambient%20and%20latent%20spaces%2C%20noise%20level%2C%20and%20smoothness%0Ameasured%20by%20the%20Lipschitz%20constant.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.16422v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStatistically%2520Optimal%2520Generative%2520Modeling%2520with%2520Maximum%2520Deviation%2520from%250A%2520%2520the%2520Empirical%2520Distribution%26entry.906535625%3DElen%2520Vardanyan%2520and%2520Sona%2520Hunanyan%2520and%2520Tigran%2520Galstyan%2520and%2520Arshak%2520Minasyan%2520and%2520Arnak%2520Dalalyan%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520problem%2520of%2520generative%2520modeling%252C%2520aiming%2520to%2520simulate%250Adiverse%2520examples%2520from%2520an%2520unknown%2520distribution%2520based%2520on%2520observed%2520examples.%2520While%250Arecent%2520studies%2520have%2520focused%2520on%2520quantifying%2520the%2520statistical%2520precision%2520of%2520popular%250Aalgorithms%252C%2520there%2520is%2520a%2520lack%2520of%2520mathematical%2520evaluation%2520regarding%2520the%250Anon-replication%2520of%2520observed%2520examples%2520and%2520the%2520creativity%2520of%2520the%2520generative%250Amodel.%2520We%2520present%2520theoretical%2520insights%2520into%2520this%2520aspect%252C%2520demonstrating%2520that%2520the%250AWasserstein%2520GAN%252C%2520constrained%2520to%2520left-invertible%2520push-forward%2520maps%252C%2520generates%250Adistributions%2520that%2520avoid%2520replication%2520and%2520significantly%2520deviate%2520from%2520the%250Aempirical%2520distribution.%2520Importantly%252C%2520we%2520show%2520that%2520left-invertibility%2520achieves%250Athis%2520without%2520compromising%2520the%2520statistical%2520optimality%2520of%2520the%2520resulting%250Agenerator.%2520Our%2520most%2520important%2520contribution%2520provides%2520a%2520finite-sample%2520lower%2520bound%250Aon%2520the%2520Wasserstein-1%2520distance%2520between%2520the%2520generative%2520distribution%2520and%2520the%250Aempirical%2520one.%2520We%2520also%2520establish%2520a%2520finite-sample%2520upper%2520bound%2520on%2520the%2520distance%250Abetween%2520the%2520generative%2520distribution%2520and%2520the%2520true%2520data-generating%2520one.%2520Both%250Abounds%2520are%2520explicit%2520and%2520show%2520the%2520impact%2520of%2520key%2520parameters%2520such%2520as%2520sample%2520size%252C%250Adimensions%2520of%2520the%2520ambient%2520and%2520latent%2520spaces%252C%2520noise%2520level%252C%2520and%2520smoothness%250Ameasured%2520by%2520the%2520Lipschitz%2520constant.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.16422v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Statistically%20Optimal%20Generative%20Modeling%20with%20Maximum%20Deviation%20from%0A%20%20the%20Empirical%20Distribution&entry.906535625=Elen%20Vardanyan%20and%20Sona%20Hunanyan%20and%20Tigran%20Galstyan%20and%20Arshak%20Minasyan%20and%20Arnak%20Dalalyan&entry.1292438233=%20%20This%20paper%20explores%20the%20problem%20of%20generative%20modeling%2C%20aiming%20to%20simulate%0Adiverse%20examples%20from%20an%20unknown%20distribution%20based%20on%20observed%20examples.%20While%0Arecent%20studies%20have%20focused%20on%20quantifying%20the%20statistical%20precision%20of%20popular%0Aalgorithms%2C%20there%20is%20a%20lack%20of%20mathematical%20evaluation%20regarding%20the%0Anon-replication%20of%20observed%20examples%20and%20the%20creativity%20of%20the%20generative%0Amodel.%20We%20present%20theoretical%20insights%20into%20this%20aspect%2C%20demonstrating%20that%20the%0AWasserstein%20GAN%2C%20constrained%20to%20left-invertible%20push-forward%20maps%2C%20generates%0Adistributions%20that%20avoid%20replication%20and%20significantly%20deviate%20from%20the%0Aempirical%20distribution.%20Importantly%2C%20we%20show%20that%20left-invertibility%20achieves%0Athis%20without%20compromising%20the%20statistical%20optimality%20of%20the%20resulting%0Agenerator.%20Our%20most%20important%20contribution%20provides%20a%20finite-sample%20lower%20bound%0Aon%20the%20Wasserstein-1%20distance%20between%20the%20generative%20distribution%20and%20the%0Aempirical%20one.%20We%20also%20establish%20a%20finite-sample%20upper%20bound%20on%20the%20distance%0Abetween%20the%20generative%20distribution%20and%20the%20true%20data-generating%20one.%20Both%0Abounds%20are%20explicit%20and%20show%20the%20impact%20of%20key%20parameters%20such%20as%20sample%20size%2C%0Adimensions%20of%20the%20ambient%20and%20latent%20spaces%2C%20noise%20level%2C%20and%20smoothness%0Ameasured%20by%20the%20Lipschitz%20constant.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.16422v2&entry.124074799=Read"},
{"title": "Pre-trained Transformer Uncovers Meaningful Patterns in Human Mobility\n  Data", "author": "Alameen Najjar", "abstract": "  We empirically demonstrate that a transformer pre-trained on country-scale\nunlabeled human mobility data learns embeddings capable, through fine-tuning,\nof developing a deep understanding of the target geography and its\ncorresponding mobility patterns. Utilizing an adaptation framework, we evaluate\nthe performance of our pre-trained embeddings in encapsulating a broad spectrum\nof concepts directly and indirectly related to human mobility. This includes\nbasic notions, such as geographic location and distance, and extends to more\ncomplex constructs, such as administrative divisions and land cover. Our\nextensive empirical analysis reveals a substantial performance boost gained\nfrom pre-training, reaching up to 38% in tasks such as tree-cover regression.\nWe attribute this result to the ability of the pre-training to uncover\nmeaningful patterns hidden in the raw data, beneficial for modeling relevant\nhigh-level concepts. The pre-trained embeddings emerge as robust\nrepresentations of regions and trajectories, potentially valuable for a wide\nrange of downstream applications.\n", "link": "http://arxiv.org/abs/2406.04029v1", "date": "2024-06-06", "relevancy": 2.03, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5264}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5183}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pre-trained%20Transformer%20Uncovers%20Meaningful%20Patterns%20in%20Human%20Mobility%0A%20%20Data&body=Title%3A%20Pre-trained%20Transformer%20Uncovers%20Meaningful%20Patterns%20in%20Human%20Mobility%0A%20%20Data%0AAuthor%3A%20Alameen%20Najjar%0AAbstract%3A%20%20%20We%20empirically%20demonstrate%20that%20a%20transformer%20pre-trained%20on%20country-scale%0Aunlabeled%20human%20mobility%20data%20learns%20embeddings%20capable%2C%20through%20fine-tuning%2C%0Aof%20developing%20a%20deep%20understanding%20of%20the%20target%20geography%20and%20its%0Acorresponding%20mobility%20patterns.%20Utilizing%20an%20adaptation%20framework%2C%20we%20evaluate%0Athe%20performance%20of%20our%20pre-trained%20embeddings%20in%20encapsulating%20a%20broad%20spectrum%0Aof%20concepts%20directly%20and%20indirectly%20related%20to%20human%20mobility.%20This%20includes%0Abasic%20notions%2C%20such%20as%20geographic%20location%20and%20distance%2C%20and%20extends%20to%20more%0Acomplex%20constructs%2C%20such%20as%20administrative%20divisions%20and%20land%20cover.%20Our%0Aextensive%20empirical%20analysis%20reveals%20a%20substantial%20performance%20boost%20gained%0Afrom%20pre-training%2C%20reaching%20up%20to%2038%25%20in%20tasks%20such%20as%20tree-cover%20regression.%0AWe%20attribute%20this%20result%20to%20the%20ability%20of%20the%20pre-training%20to%20uncover%0Ameaningful%20patterns%20hidden%20in%20the%20raw%20data%2C%20beneficial%20for%20modeling%20relevant%0Ahigh-level%20concepts.%20The%20pre-trained%20embeddings%20emerge%20as%20robust%0Arepresentations%20of%20regions%20and%20trajectories%2C%20potentially%20valuable%20for%20a%20wide%0Arange%20of%20downstream%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04029v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPre-trained%2520Transformer%2520Uncovers%2520Meaningful%2520Patterns%2520in%2520Human%2520Mobility%250A%2520%2520Data%26entry.906535625%3DAlameen%2520Najjar%26entry.1292438233%3D%2520%2520We%2520empirically%2520demonstrate%2520that%2520a%2520transformer%2520pre-trained%2520on%2520country-scale%250Aunlabeled%2520human%2520mobility%2520data%2520learns%2520embeddings%2520capable%252C%2520through%2520fine-tuning%252C%250Aof%2520developing%2520a%2520deep%2520understanding%2520of%2520the%2520target%2520geography%2520and%2520its%250Acorresponding%2520mobility%2520patterns.%2520Utilizing%2520an%2520adaptation%2520framework%252C%2520we%2520evaluate%250Athe%2520performance%2520of%2520our%2520pre-trained%2520embeddings%2520in%2520encapsulating%2520a%2520broad%2520spectrum%250Aof%2520concepts%2520directly%2520and%2520indirectly%2520related%2520to%2520human%2520mobility.%2520This%2520includes%250Abasic%2520notions%252C%2520such%2520as%2520geographic%2520location%2520and%2520distance%252C%2520and%2520extends%2520to%2520more%250Acomplex%2520constructs%252C%2520such%2520as%2520administrative%2520divisions%2520and%2520land%2520cover.%2520Our%250Aextensive%2520empirical%2520analysis%2520reveals%2520a%2520substantial%2520performance%2520boost%2520gained%250Afrom%2520pre-training%252C%2520reaching%2520up%2520to%252038%2525%2520in%2520tasks%2520such%2520as%2520tree-cover%2520regression.%250AWe%2520attribute%2520this%2520result%2520to%2520the%2520ability%2520of%2520the%2520pre-training%2520to%2520uncover%250Ameaningful%2520patterns%2520hidden%2520in%2520the%2520raw%2520data%252C%2520beneficial%2520for%2520modeling%2520relevant%250Ahigh-level%2520concepts.%2520The%2520pre-trained%2520embeddings%2520emerge%2520as%2520robust%250Arepresentations%2520of%2520regions%2520and%2520trajectories%252C%2520potentially%2520valuable%2520for%2520a%2520wide%250Arange%2520of%2520downstream%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04029v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-trained%20Transformer%20Uncovers%20Meaningful%20Patterns%20in%20Human%20Mobility%0A%20%20Data&entry.906535625=Alameen%20Najjar&entry.1292438233=%20%20We%20empirically%20demonstrate%20that%20a%20transformer%20pre-trained%20on%20country-scale%0Aunlabeled%20human%20mobility%20data%20learns%20embeddings%20capable%2C%20through%20fine-tuning%2C%0Aof%20developing%20a%20deep%20understanding%20of%20the%20target%20geography%20and%20its%0Acorresponding%20mobility%20patterns.%20Utilizing%20an%20adaptation%20framework%2C%20we%20evaluate%0Athe%20performance%20of%20our%20pre-trained%20embeddings%20in%20encapsulating%20a%20broad%20spectrum%0Aof%20concepts%20directly%20and%20indirectly%20related%20to%20human%20mobility.%20This%20includes%0Abasic%20notions%2C%20such%20as%20geographic%20location%20and%20distance%2C%20and%20extends%20to%20more%0Acomplex%20constructs%2C%20such%20as%20administrative%20divisions%20and%20land%20cover.%20Our%0Aextensive%20empirical%20analysis%20reveals%20a%20substantial%20performance%20boost%20gained%0Afrom%20pre-training%2C%20reaching%20up%20to%2038%25%20in%20tasks%20such%20as%20tree-cover%20regression.%0AWe%20attribute%20this%20result%20to%20the%20ability%20of%20the%20pre-training%20to%20uncover%0Ameaningful%20patterns%20hidden%20in%20the%20raw%20data%2C%20beneficial%20for%20modeling%20relevant%0Ahigh-level%20concepts.%20The%20pre-trained%20embeddings%20emerge%20as%20robust%0Arepresentations%20of%20regions%20and%20trajectories%2C%20potentially%20valuable%20for%20a%20wide%0Arange%20of%20downstream%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04029v1&entry.124074799=Read"},
{"title": "Lever LM: Configuring In-Context Sequence to Lever Large Vision Language\n  Models", "author": "Xu Yang and Yingzhe Peng and Haoxuan Ma and Shuo Xu and Chi Zhang and Yucheng Han and Hanwang Zhang", "abstract": "  As Archimedes famously said, ``Give me a lever long enough and a fulcrum on\nwhich to place it, and I shall move the world'', in this study, we propose to\nuse a tiny Language Model (LM), \\eg, a Transformer with 67M parameters, to\nlever much larger Vision-Language Models (LVLMs) with 9B parameters.\nSpecifically, we use this tiny \\textbf{Lever-LM} to configure effective\nin-context demonstration (ICD) sequences to improve the In-Context Learinng\n(ICL) performance of LVLMs. Previous studies show that diverse ICD\nconfigurations like the selection and ordering of the demonstrations heavily\naffect the ICL performance, highlighting the significance of configuring\neffective ICD sequences. Motivated by this and by re-considering the the\nprocess of configuring ICD sequence, we find this is a mirror process of human\nsentence composition and further assume that effective ICD configurations may\ncontain internal statistical patterns that can be captured by Lever-LM. Then a\ndataset with effective ICD sequences is constructed to train Lever-LM. After\ntraining, given novel queries, new ICD sequences are configured by the trained\nLever-LM to solve vision-language tasks through ICL. Experiments show that\nthese ICD sequences can improve the ICL performance of two LVLMs compared with\nsome strong baselines in Visual Question Answering and Image Captioning,\nvalidating that Lever-LM can really capture the statistical patterns for\nlevering LVLMs.\n", "link": "http://arxiv.org/abs/2312.10104v3", "date": "2024-06-06", "relevancy": 2.0273, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5099}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5051}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lever%20LM%3A%20Configuring%20In-Context%20Sequence%20to%20Lever%20Large%20Vision%20Language%0A%20%20Models&body=Title%3A%20Lever%20LM%3A%20Configuring%20In-Context%20Sequence%20to%20Lever%20Large%20Vision%20Language%0A%20%20Models%0AAuthor%3A%20Xu%20Yang%20and%20Yingzhe%20Peng%20and%20Haoxuan%20Ma%20and%20Shuo%20Xu%20and%20Chi%20Zhang%20and%20Yucheng%20Han%20and%20Hanwang%20Zhang%0AAbstract%3A%20%20%20As%20Archimedes%20famously%20said%2C%20%60%60Give%20me%20a%20lever%20long%20enough%20and%20a%20fulcrum%20on%0Awhich%20to%20place%20it%2C%20and%20I%20shall%20move%20the%20world%27%27%2C%20in%20this%20study%2C%20we%20propose%20to%0Ause%20a%20tiny%20Language%20Model%20%28LM%29%2C%20%5Ceg%2C%20a%20Transformer%20with%2067M%20parameters%2C%20to%0Alever%20much%20larger%20Vision-Language%20Models%20%28LVLMs%29%20with%209B%20parameters.%0ASpecifically%2C%20we%20use%20this%20tiny%20%5Ctextbf%7BLever-LM%7D%20to%20configure%20effective%0Ain-context%20demonstration%20%28ICD%29%20sequences%20to%20improve%20the%20In-Context%20Learinng%0A%28ICL%29%20performance%20of%20LVLMs.%20Previous%20studies%20show%20that%20diverse%20ICD%0Aconfigurations%20like%20the%20selection%20and%20ordering%20of%20the%20demonstrations%20heavily%0Aaffect%20the%20ICL%20performance%2C%20highlighting%20the%20significance%20of%20configuring%0Aeffective%20ICD%20sequences.%20Motivated%20by%20this%20and%20by%20re-considering%20the%20the%0Aprocess%20of%20configuring%20ICD%20sequence%2C%20we%20find%20this%20is%20a%20mirror%20process%20of%20human%0Asentence%20composition%20and%20further%20assume%20that%20effective%20ICD%20configurations%20may%0Acontain%20internal%20statistical%20patterns%20that%20can%20be%20captured%20by%20Lever-LM.%20Then%20a%0Adataset%20with%20effective%20ICD%20sequences%20is%20constructed%20to%20train%20Lever-LM.%20After%0Atraining%2C%20given%20novel%20queries%2C%20new%20ICD%20sequences%20are%20configured%20by%20the%20trained%0ALever-LM%20to%20solve%20vision-language%20tasks%20through%20ICL.%20Experiments%20show%20that%0Athese%20ICD%20sequences%20can%20improve%20the%20ICL%20performance%20of%20two%20LVLMs%20compared%20with%0Asome%20strong%20baselines%20in%20Visual%20Question%20Answering%20and%20Image%20Captioning%2C%0Avalidating%20that%20Lever-LM%20can%20really%20capture%20the%20statistical%20patterns%20for%0Alevering%20LVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10104v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLever%2520LM%253A%2520Configuring%2520In-Context%2520Sequence%2520to%2520Lever%2520Large%2520Vision%2520Language%250A%2520%2520Models%26entry.906535625%3DXu%2520Yang%2520and%2520Yingzhe%2520Peng%2520and%2520Haoxuan%2520Ma%2520and%2520Shuo%2520Xu%2520and%2520Chi%2520Zhang%2520and%2520Yucheng%2520Han%2520and%2520Hanwang%2520Zhang%26entry.1292438233%3D%2520%2520As%2520Archimedes%2520famously%2520said%252C%2520%2560%2560Give%2520me%2520a%2520lever%2520long%2520enough%2520and%2520a%2520fulcrum%2520on%250Awhich%2520to%2520place%2520it%252C%2520and%2520I%2520shall%2520move%2520the%2520world%2527%2527%252C%2520in%2520this%2520study%252C%2520we%2520propose%2520to%250Ause%2520a%2520tiny%2520Language%2520Model%2520%2528LM%2529%252C%2520%255Ceg%252C%2520a%2520Transformer%2520with%252067M%2520parameters%252C%2520to%250Alever%2520much%2520larger%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520with%25209B%2520parameters.%250ASpecifically%252C%2520we%2520use%2520this%2520tiny%2520%255Ctextbf%257BLever-LM%257D%2520to%2520configure%2520effective%250Ain-context%2520demonstration%2520%2528ICD%2529%2520sequences%2520to%2520improve%2520the%2520In-Context%2520Learinng%250A%2528ICL%2529%2520performance%2520of%2520LVLMs.%2520Previous%2520studies%2520show%2520that%2520diverse%2520ICD%250Aconfigurations%2520like%2520the%2520selection%2520and%2520ordering%2520of%2520the%2520demonstrations%2520heavily%250Aaffect%2520the%2520ICL%2520performance%252C%2520highlighting%2520the%2520significance%2520of%2520configuring%250Aeffective%2520ICD%2520sequences.%2520Motivated%2520by%2520this%2520and%2520by%2520re-considering%2520the%2520the%250Aprocess%2520of%2520configuring%2520ICD%2520sequence%252C%2520we%2520find%2520this%2520is%2520a%2520mirror%2520process%2520of%2520human%250Asentence%2520composition%2520and%2520further%2520assume%2520that%2520effective%2520ICD%2520configurations%2520may%250Acontain%2520internal%2520statistical%2520patterns%2520that%2520can%2520be%2520captured%2520by%2520Lever-LM.%2520Then%2520a%250Adataset%2520with%2520effective%2520ICD%2520sequences%2520is%2520constructed%2520to%2520train%2520Lever-LM.%2520After%250Atraining%252C%2520given%2520novel%2520queries%252C%2520new%2520ICD%2520sequences%2520are%2520configured%2520by%2520the%2520trained%250ALever-LM%2520to%2520solve%2520vision-language%2520tasks%2520through%2520ICL.%2520Experiments%2520show%2520that%250Athese%2520ICD%2520sequences%2520can%2520improve%2520the%2520ICL%2520performance%2520of%2520two%2520LVLMs%2520compared%2520with%250Asome%2520strong%2520baselines%2520in%2520Visual%2520Question%2520Answering%2520and%2520Image%2520Captioning%252C%250Avalidating%2520that%2520Lever-LM%2520can%2520really%2520capture%2520the%2520statistical%2520patterns%2520for%250Alevering%2520LVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.10104v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lever%20LM%3A%20Configuring%20In-Context%20Sequence%20to%20Lever%20Large%20Vision%20Language%0A%20%20Models&entry.906535625=Xu%20Yang%20and%20Yingzhe%20Peng%20and%20Haoxuan%20Ma%20and%20Shuo%20Xu%20and%20Chi%20Zhang%20and%20Yucheng%20Han%20and%20Hanwang%20Zhang&entry.1292438233=%20%20As%20Archimedes%20famously%20said%2C%20%60%60Give%20me%20a%20lever%20long%20enough%20and%20a%20fulcrum%20on%0Awhich%20to%20place%20it%2C%20and%20I%20shall%20move%20the%20world%27%27%2C%20in%20this%20study%2C%20we%20propose%20to%0Ause%20a%20tiny%20Language%20Model%20%28LM%29%2C%20%5Ceg%2C%20a%20Transformer%20with%2067M%20parameters%2C%20to%0Alever%20much%20larger%20Vision-Language%20Models%20%28LVLMs%29%20with%209B%20parameters.%0ASpecifically%2C%20we%20use%20this%20tiny%20%5Ctextbf%7BLever-LM%7D%20to%20configure%20effective%0Ain-context%20demonstration%20%28ICD%29%20sequences%20to%20improve%20the%20In-Context%20Learinng%0A%28ICL%29%20performance%20of%20LVLMs.%20Previous%20studies%20show%20that%20diverse%20ICD%0Aconfigurations%20like%20the%20selection%20and%20ordering%20of%20the%20demonstrations%20heavily%0Aaffect%20the%20ICL%20performance%2C%20highlighting%20the%20significance%20of%20configuring%0Aeffective%20ICD%20sequences.%20Motivated%20by%20this%20and%20by%20re-considering%20the%20the%0Aprocess%20of%20configuring%20ICD%20sequence%2C%20we%20find%20this%20is%20a%20mirror%20process%20of%20human%0Asentence%20composition%20and%20further%20assume%20that%20effective%20ICD%20configurations%20may%0Acontain%20internal%20statistical%20patterns%20that%20can%20be%20captured%20by%20Lever-LM.%20Then%20a%0Adataset%20with%20effective%20ICD%20sequences%20is%20constructed%20to%20train%20Lever-LM.%20After%0Atraining%2C%20given%20novel%20queries%2C%20new%20ICD%20sequences%20are%20configured%20by%20the%20trained%0ALever-LM%20to%20solve%20vision-language%20tasks%20through%20ICL.%20Experiments%20show%20that%0Athese%20ICD%20sequences%20can%20improve%20the%20ICL%20performance%20of%20two%20LVLMs%20compared%20with%0Asome%20strong%20baselines%20in%20Visual%20Question%20Answering%20and%20Image%20Captioning%2C%0Avalidating%20that%20Lever-LM%20can%20really%20capture%20the%20statistical%20patterns%20for%0Alevering%20LVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10104v3&entry.124074799=Read"},
{"title": "Causal Estimation of Memorisation Profiles", "author": "Pietro Lesci and Clara Meister and Thomas Hofmann and Andreas Vlachos and Tiago Pimentel", "abstract": "  Understanding memorisation in language models has practical and societal\nimplications, e.g., studying models' training dynamics or preventing copyright\ninfringements. Prior work defines memorisation as the causal effect of training\nwith an instance on the model's ability to predict that instance. This\ndefinition relies on a counterfactual: the ability to observe what would have\nhappened had the model not seen that instance. Existing methods struggle to\nprovide computationally efficient and accurate estimates of this\ncounterfactual. Further, they often estimate memorisation for a model\narchitecture rather than for a specific model instance. This paper fills an\nimportant gap in the literature, proposing a new, principled, and efficient\nmethod to estimate memorisation based on the difference-in-differences design\nfrom econometrics. Using this method, we characterise a model's memorisation\nprofile--its memorisation trends across training--by only observing its\nbehaviour on a small set of instances throughout training. In experiments with\nthe Pythia model suite, we find that memorisation (i) is stronger and more\npersistent in larger models, (ii) is determined by data order and learning\nrate, and (iii) has stable trends across model sizes, thus making memorisation\nin larger models predictable from smaller ones.\n", "link": "http://arxiv.org/abs/2406.04327v1", "date": "2024-06-06", "relevancy": 1.6637, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4358}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4144}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Estimation%20of%20Memorisation%20Profiles&body=Title%3A%20Causal%20Estimation%20of%20Memorisation%20Profiles%0AAuthor%3A%20Pietro%20Lesci%20and%20Clara%20Meister%20and%20Thomas%20Hofmann%20and%20Andreas%20Vlachos%20and%20Tiago%20Pimentel%0AAbstract%3A%20%20%20Understanding%20memorisation%20in%20language%20models%20has%20practical%20and%20societal%0Aimplications%2C%20e.g.%2C%20studying%20models%27%20training%20dynamics%20or%20preventing%20copyright%0Ainfringements.%20Prior%20work%20defines%20memorisation%20as%20the%20causal%20effect%20of%20training%0Awith%20an%20instance%20on%20the%20model%27s%20ability%20to%20predict%20that%20instance.%20This%0Adefinition%20relies%20on%20a%20counterfactual%3A%20the%20ability%20to%20observe%20what%20would%20have%0Ahappened%20had%20the%20model%20not%20seen%20that%20instance.%20Existing%20methods%20struggle%20to%0Aprovide%20computationally%20efficient%20and%20accurate%20estimates%20of%20this%0Acounterfactual.%20Further%2C%20they%20often%20estimate%20memorisation%20for%20a%20model%0Aarchitecture%20rather%20than%20for%20a%20specific%20model%20instance.%20This%20paper%20fills%20an%0Aimportant%20gap%20in%20the%20literature%2C%20proposing%20a%20new%2C%20principled%2C%20and%20efficient%0Amethod%20to%20estimate%20memorisation%20based%20on%20the%20difference-in-differences%20design%0Afrom%20econometrics.%20Using%20this%20method%2C%20we%20characterise%20a%20model%27s%20memorisation%0Aprofile--its%20memorisation%20trends%20across%20training--by%20only%20observing%20its%0Abehaviour%20on%20a%20small%20set%20of%20instances%20throughout%20training.%20In%20experiments%20with%0Athe%20Pythia%20model%20suite%2C%20we%20find%20that%20memorisation%20%28i%29%20is%20stronger%20and%20more%0Apersistent%20in%20larger%20models%2C%20%28ii%29%20is%20determined%20by%20data%20order%20and%20learning%0Arate%2C%20and%20%28iii%29%20has%20stable%20trends%20across%20model%20sizes%2C%20thus%20making%20memorisation%0Ain%20larger%20models%20predictable%20from%20smaller%20ones.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Estimation%2520of%2520Memorisation%2520Profiles%26entry.906535625%3DPietro%2520Lesci%2520and%2520Clara%2520Meister%2520and%2520Thomas%2520Hofmann%2520and%2520Andreas%2520Vlachos%2520and%2520Tiago%2520Pimentel%26entry.1292438233%3D%2520%2520Understanding%2520memorisation%2520in%2520language%2520models%2520has%2520practical%2520and%2520societal%250Aimplications%252C%2520e.g.%252C%2520studying%2520models%2527%2520training%2520dynamics%2520or%2520preventing%2520copyright%250Ainfringements.%2520Prior%2520work%2520defines%2520memorisation%2520as%2520the%2520causal%2520effect%2520of%2520training%250Awith%2520an%2520instance%2520on%2520the%2520model%2527s%2520ability%2520to%2520predict%2520that%2520instance.%2520This%250Adefinition%2520relies%2520on%2520a%2520counterfactual%253A%2520the%2520ability%2520to%2520observe%2520what%2520would%2520have%250Ahappened%2520had%2520the%2520model%2520not%2520seen%2520that%2520instance.%2520Existing%2520methods%2520struggle%2520to%250Aprovide%2520computationally%2520efficient%2520and%2520accurate%2520estimates%2520of%2520this%250Acounterfactual.%2520Further%252C%2520they%2520often%2520estimate%2520memorisation%2520for%2520a%2520model%250Aarchitecture%2520rather%2520than%2520for%2520a%2520specific%2520model%2520instance.%2520This%2520paper%2520fills%2520an%250Aimportant%2520gap%2520in%2520the%2520literature%252C%2520proposing%2520a%2520new%252C%2520principled%252C%2520and%2520efficient%250Amethod%2520to%2520estimate%2520memorisation%2520based%2520on%2520the%2520difference-in-differences%2520design%250Afrom%2520econometrics.%2520Using%2520this%2520method%252C%2520we%2520characterise%2520a%2520model%2527s%2520memorisation%250Aprofile--its%2520memorisation%2520trends%2520across%2520training--by%2520only%2520observing%2520its%250Abehaviour%2520on%2520a%2520small%2520set%2520of%2520instances%2520throughout%2520training.%2520In%2520experiments%2520with%250Athe%2520Pythia%2520model%2520suite%252C%2520we%2520find%2520that%2520memorisation%2520%2528i%2529%2520is%2520stronger%2520and%2520more%250Apersistent%2520in%2520larger%2520models%252C%2520%2528ii%2529%2520is%2520determined%2520by%2520data%2520order%2520and%2520learning%250Arate%252C%2520and%2520%2528iii%2529%2520has%2520stable%2520trends%2520across%2520model%2520sizes%252C%2520thus%2520making%2520memorisation%250Ain%2520larger%2520models%2520predictable%2520from%2520smaller%2520ones.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Estimation%20of%20Memorisation%20Profiles&entry.906535625=Pietro%20Lesci%20and%20Clara%20Meister%20and%20Thomas%20Hofmann%20and%20Andreas%20Vlachos%20and%20Tiago%20Pimentel&entry.1292438233=%20%20Understanding%20memorisation%20in%20language%20models%20has%20practical%20and%20societal%0Aimplications%2C%20e.g.%2C%20studying%20models%27%20training%20dynamics%20or%20preventing%20copyright%0Ainfringements.%20Prior%20work%20defines%20memorisation%20as%20the%20causal%20effect%20of%20training%0Awith%20an%20instance%20on%20the%20model%27s%20ability%20to%20predict%20that%20instance.%20This%0Adefinition%20relies%20on%20a%20counterfactual%3A%20the%20ability%20to%20observe%20what%20would%20have%0Ahappened%20had%20the%20model%20not%20seen%20that%20instance.%20Existing%20methods%20struggle%20to%0Aprovide%20computationally%20efficient%20and%20accurate%20estimates%20of%20this%0Acounterfactual.%20Further%2C%20they%20often%20estimate%20memorisation%20for%20a%20model%0Aarchitecture%20rather%20than%20for%20a%20specific%20model%20instance.%20This%20paper%20fills%20an%0Aimportant%20gap%20in%20the%20literature%2C%20proposing%20a%20new%2C%20principled%2C%20and%20efficient%0Amethod%20to%20estimate%20memorisation%20based%20on%20the%20difference-in-differences%20design%0Afrom%20econometrics.%20Using%20this%20method%2C%20we%20characterise%20a%20model%27s%20memorisation%0Aprofile--its%20memorisation%20trends%20across%20training--by%20only%20observing%20its%0Abehaviour%20on%20a%20small%20set%20of%20instances%20throughout%20training.%20In%20experiments%20with%0Athe%20Pythia%20model%20suite%2C%20we%20find%20that%20memorisation%20%28i%29%20is%20stronger%20and%20more%0Apersistent%20in%20larger%20models%2C%20%28ii%29%20is%20determined%20by%20data%20order%20and%20learning%0Arate%2C%20and%20%28iii%29%20has%20stable%20trends%20across%20model%20sizes%2C%20thus%20making%20memorisation%0Ain%20larger%20models%20predictable%20from%20smaller%20ones.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04327v1&entry.124074799=Read"},
{"title": "Matching Anything by Segmenting Anything", "author": "Siyuan Li and Lei Ke and Martin Danelljan and Luigi Piccinelli and Mattia Segu and Luc Van Gool and Fisher Yu", "abstract": "  The robust association of the same objects across video frames in complex\nscenes is crucial for many applications, especially Multiple Object Tracking\n(MOT). Current methods predominantly rely on labeled domain-specific video\ndatasets, which limits the cross-domain generalization of learned similarity\nembeddings. We propose MASA, a novel method for robust instance association\nlearning, capable of matching any objects within videos across diverse domains\nwithout tracking labels. Leveraging the rich object segmentation from the\nSegment Anything Model (SAM), MASA learns instance-level correspondence through\nexhaustive data transformations. We treat the SAM outputs as dense object\nregion proposals and learn to match those regions from a vast image collection.\nWe further design a universal MASA adapter which can work in tandem with\nfoundational segmentation or detection models and enable them to track any\ndetected objects. Those combinations present strong zero-shot tracking ability\nin complex domains. Extensive tests on multiple challenging MOT and MOTS\nbenchmarks indicate that the proposed method, using only unlabeled static\nimages, achieves even better performance than state-of-the-art methods trained\nwith fully annotated in-domain video sequences, in zero-shot association.\nProject Page: https://matchinganything.github.io/\n", "link": "http://arxiv.org/abs/2406.04221v1", "date": "2024-06-06", "relevancy": 1.6869, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.584}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5409}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matching%20Anything%20by%20Segmenting%20Anything&body=Title%3A%20Matching%20Anything%20by%20Segmenting%20Anything%0AAuthor%3A%20Siyuan%20Li%20and%20Lei%20Ke%20and%20Martin%20Danelljan%20and%20Luigi%20Piccinelli%20and%20Mattia%20Segu%20and%20Luc%20Van%20Gool%20and%20Fisher%20Yu%0AAbstract%3A%20%20%20The%20robust%20association%20of%20the%20same%20objects%20across%20video%20frames%20in%20complex%0Ascenes%20is%20crucial%20for%20many%20applications%2C%20especially%20Multiple%20Object%20Tracking%0A%28MOT%29.%20Current%20methods%20predominantly%20rely%20on%20labeled%20domain-specific%20video%0Adatasets%2C%20which%20limits%20the%20cross-domain%20generalization%20of%20learned%20similarity%0Aembeddings.%20We%20propose%20MASA%2C%20a%20novel%20method%20for%20robust%20instance%20association%0Alearning%2C%20capable%20of%20matching%20any%20objects%20within%20videos%20across%20diverse%20domains%0Awithout%20tracking%20labels.%20Leveraging%20the%20rich%20object%20segmentation%20from%20the%0ASegment%20Anything%20Model%20%28SAM%29%2C%20MASA%20learns%20instance-level%20correspondence%20through%0Aexhaustive%20data%20transformations.%20We%20treat%20the%20SAM%20outputs%20as%20dense%20object%0Aregion%20proposals%20and%20learn%20to%20match%20those%20regions%20from%20a%20vast%20image%20collection.%0AWe%20further%20design%20a%20universal%20MASA%20adapter%20which%20can%20work%20in%20tandem%20with%0Afoundational%20segmentation%20or%20detection%20models%20and%20enable%20them%20to%20track%20any%0Adetected%20objects.%20Those%20combinations%20present%20strong%20zero-shot%20tracking%20ability%0Ain%20complex%20domains.%20Extensive%20tests%20on%20multiple%20challenging%20MOT%20and%20MOTS%0Abenchmarks%20indicate%20that%20the%20proposed%20method%2C%20using%20only%20unlabeled%20static%0Aimages%2C%20achieves%20even%20better%20performance%20than%20state-of-the-art%20methods%20trained%0Awith%20fully%20annotated%20in-domain%20video%20sequences%2C%20in%20zero-shot%20association.%0AProject%20Page%3A%20https%3A//matchinganything.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatching%2520Anything%2520by%2520Segmenting%2520Anything%26entry.906535625%3DSiyuan%2520Li%2520and%2520Lei%2520Ke%2520and%2520Martin%2520Danelljan%2520and%2520Luigi%2520Piccinelli%2520and%2520Mattia%2520Segu%2520and%2520Luc%2520Van%2520Gool%2520and%2520Fisher%2520Yu%26entry.1292438233%3D%2520%2520The%2520robust%2520association%2520of%2520the%2520same%2520objects%2520across%2520video%2520frames%2520in%2520complex%250Ascenes%2520is%2520crucial%2520for%2520many%2520applications%252C%2520especially%2520Multiple%2520Object%2520Tracking%250A%2528MOT%2529.%2520Current%2520methods%2520predominantly%2520rely%2520on%2520labeled%2520domain-specific%2520video%250Adatasets%252C%2520which%2520limits%2520the%2520cross-domain%2520generalization%2520of%2520learned%2520similarity%250Aembeddings.%2520We%2520propose%2520MASA%252C%2520a%2520novel%2520method%2520for%2520robust%2520instance%2520association%250Alearning%252C%2520capable%2520of%2520matching%2520any%2520objects%2520within%2520videos%2520across%2520diverse%2520domains%250Awithout%2520tracking%2520labels.%2520Leveraging%2520the%2520rich%2520object%2520segmentation%2520from%2520the%250ASegment%2520Anything%2520Model%2520%2528SAM%2529%252C%2520MASA%2520learns%2520instance-level%2520correspondence%2520through%250Aexhaustive%2520data%2520transformations.%2520We%2520treat%2520the%2520SAM%2520outputs%2520as%2520dense%2520object%250Aregion%2520proposals%2520and%2520learn%2520to%2520match%2520those%2520regions%2520from%2520a%2520vast%2520image%2520collection.%250AWe%2520further%2520design%2520a%2520universal%2520MASA%2520adapter%2520which%2520can%2520work%2520in%2520tandem%2520with%250Afoundational%2520segmentation%2520or%2520detection%2520models%2520and%2520enable%2520them%2520to%2520track%2520any%250Adetected%2520objects.%2520Those%2520combinations%2520present%2520strong%2520zero-shot%2520tracking%2520ability%250Ain%2520complex%2520domains.%2520Extensive%2520tests%2520on%2520multiple%2520challenging%2520MOT%2520and%2520MOTS%250Abenchmarks%2520indicate%2520that%2520the%2520proposed%2520method%252C%2520using%2520only%2520unlabeled%2520static%250Aimages%252C%2520achieves%2520even%2520better%2520performance%2520than%2520state-of-the-art%2520methods%2520trained%250Awith%2520fully%2520annotated%2520in-domain%2520video%2520sequences%252C%2520in%2520zero-shot%2520association.%250AProject%2520Page%253A%2520https%253A//matchinganything.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matching%20Anything%20by%20Segmenting%20Anything&entry.906535625=Siyuan%20Li%20and%20Lei%20Ke%20and%20Martin%20Danelljan%20and%20Luigi%20Piccinelli%20and%20Mattia%20Segu%20and%20Luc%20Van%20Gool%20and%20Fisher%20Yu&entry.1292438233=%20%20The%20robust%20association%20of%20the%20same%20objects%20across%20video%20frames%20in%20complex%0Ascenes%20is%20crucial%20for%20many%20applications%2C%20especially%20Multiple%20Object%20Tracking%0A%28MOT%29.%20Current%20methods%20predominantly%20rely%20on%20labeled%20domain-specific%20video%0Adatasets%2C%20which%20limits%20the%20cross-domain%20generalization%20of%20learned%20similarity%0Aembeddings.%20We%20propose%20MASA%2C%20a%20novel%20method%20for%20robust%20instance%20association%0Alearning%2C%20capable%20of%20matching%20any%20objects%20within%20videos%20across%20diverse%20domains%0Awithout%20tracking%20labels.%20Leveraging%20the%20rich%20object%20segmentation%20from%20the%0ASegment%20Anything%20Model%20%28SAM%29%2C%20MASA%20learns%20instance-level%20correspondence%20through%0Aexhaustive%20data%20transformations.%20We%20treat%20the%20SAM%20outputs%20as%20dense%20object%0Aregion%20proposals%20and%20learn%20to%20match%20those%20regions%20from%20a%20vast%20image%20collection.%0AWe%20further%20design%20a%20universal%20MASA%20adapter%20which%20can%20work%20in%20tandem%20with%0Afoundational%20segmentation%20or%20detection%20models%20and%20enable%20them%20to%20track%20any%0Adetected%20objects.%20Those%20combinations%20present%20strong%20zero-shot%20tracking%20ability%0Ain%20complex%20domains.%20Extensive%20tests%20on%20multiple%20challenging%20MOT%20and%20MOTS%0Abenchmarks%20indicate%20that%20the%20proposed%20method%2C%20using%20only%20unlabeled%20static%0Aimages%2C%20achieves%20even%20better%20performance%20than%20state-of-the-art%20methods%20trained%0Awith%20fully%20annotated%20in-domain%20video%20sequences%2C%20in%20zero-shot%20association.%0AProject%20Page%3A%20https%3A//matchinganything.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04221v1&entry.124074799=Read"},
{"title": "Resilient Distributed Optimization for Multi-Agent Cyberphysical Systems", "author": "Michal Yemini and Angelia Nedi\u0107 and Andrea J. Goldsmith and Stephanie Gil", "abstract": "  This work focuses on the problem of distributed optimization in multi-agent\ncyberphysical systems, where a legitimate agents' iterates are influenced both\nby the values it receives from potentially malicious neighboring agents, and by\nits own self-serving target function. We develop a new algorithmic and\nanalytical framework to achieve resilience for the class of problems where\nstochastic values of trust between agents exist and can be exploited. In this\ncase we show that convergence to the true global optimal point can be\nrecovered, both in mean and almost surely, even in the presence of malicious\nagents. Furthermore, we provide expected convergence rate guarantees in the\nform of upper bounds on the expected squared distance to the optimal value.\nFinally, numerical results are presented that validate our analytical\nconvergence guarantees even when the malicious agents compose the majority of\nagents in the network and where existing methods fail to converge to the\noptimal nominal points.\n", "link": "http://arxiv.org/abs/2212.02459v2", "date": "2024-06-06", "relevancy": 1.9328, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5174}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4898}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4629}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resilient%20Distributed%20Optimization%20for%20Multi-Agent%20Cyberphysical%20Systems&body=Title%3A%20Resilient%20Distributed%20Optimization%20for%20Multi-Agent%20Cyberphysical%20Systems%0AAuthor%3A%20Michal%20Yemini%20and%20Angelia%20Nedi%C4%87%20and%20Andrea%20J.%20Goldsmith%20and%20Stephanie%20Gil%0AAbstract%3A%20%20%20This%20work%20focuses%20on%20the%20problem%20of%20distributed%20optimization%20in%20multi-agent%0Acyberphysical%20systems%2C%20where%20a%20legitimate%20agents%27%20iterates%20are%20influenced%20both%0Aby%20the%20values%20it%20receives%20from%20potentially%20malicious%20neighboring%20agents%2C%20and%20by%0Aits%20own%20self-serving%20target%20function.%20We%20develop%20a%20new%20algorithmic%20and%0Aanalytical%20framework%20to%20achieve%20resilience%20for%20the%20class%20of%20problems%20where%0Astochastic%20values%20of%20trust%20between%20agents%20exist%20and%20can%20be%20exploited.%20In%20this%0Acase%20we%20show%20that%20convergence%20to%20the%20true%20global%20optimal%20point%20can%20be%0Arecovered%2C%20both%20in%20mean%20and%20almost%20surely%2C%20even%20in%20the%20presence%20of%20malicious%0Aagents.%20Furthermore%2C%20we%20provide%20expected%20convergence%20rate%20guarantees%20in%20the%0Aform%20of%20upper%20bounds%20on%20the%20expected%20squared%20distance%20to%20the%20optimal%20value.%0AFinally%2C%20numerical%20results%20are%20presented%20that%20validate%20our%20analytical%0Aconvergence%20guarantees%20even%20when%20the%20malicious%20agents%20compose%20the%20majority%20of%0Aagents%20in%20the%20network%20and%20where%20existing%20methods%20fail%20to%20converge%20to%20the%0Aoptimal%20nominal%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.02459v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResilient%2520Distributed%2520Optimization%2520for%2520Multi-Agent%2520Cyberphysical%2520Systems%26entry.906535625%3DMichal%2520Yemini%2520and%2520Angelia%2520Nedi%25C4%2587%2520and%2520Andrea%2520J.%2520Goldsmith%2520and%2520Stephanie%2520Gil%26entry.1292438233%3D%2520%2520This%2520work%2520focuses%2520on%2520the%2520problem%2520of%2520distributed%2520optimization%2520in%2520multi-agent%250Acyberphysical%2520systems%252C%2520where%2520a%2520legitimate%2520agents%2527%2520iterates%2520are%2520influenced%2520both%250Aby%2520the%2520values%2520it%2520receives%2520from%2520potentially%2520malicious%2520neighboring%2520agents%252C%2520and%2520by%250Aits%2520own%2520self-serving%2520target%2520function.%2520We%2520develop%2520a%2520new%2520algorithmic%2520and%250Aanalytical%2520framework%2520to%2520achieve%2520resilience%2520for%2520the%2520class%2520of%2520problems%2520where%250Astochastic%2520values%2520of%2520trust%2520between%2520agents%2520exist%2520and%2520can%2520be%2520exploited.%2520In%2520this%250Acase%2520we%2520show%2520that%2520convergence%2520to%2520the%2520true%2520global%2520optimal%2520point%2520can%2520be%250Arecovered%252C%2520both%2520in%2520mean%2520and%2520almost%2520surely%252C%2520even%2520in%2520the%2520presence%2520of%2520malicious%250Aagents.%2520Furthermore%252C%2520we%2520provide%2520expected%2520convergence%2520rate%2520guarantees%2520in%2520the%250Aform%2520of%2520upper%2520bounds%2520on%2520the%2520expected%2520squared%2520distance%2520to%2520the%2520optimal%2520value.%250AFinally%252C%2520numerical%2520results%2520are%2520presented%2520that%2520validate%2520our%2520analytical%250Aconvergence%2520guarantees%2520even%2520when%2520the%2520malicious%2520agents%2520compose%2520the%2520majority%2520of%250Aagents%2520in%2520the%2520network%2520and%2520where%2520existing%2520methods%2520fail%2520to%2520converge%2520to%2520the%250Aoptimal%2520nominal%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.02459v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resilient%20Distributed%20Optimization%20for%20Multi-Agent%20Cyberphysical%20Systems&entry.906535625=Michal%20Yemini%20and%20Angelia%20Nedi%C4%87%20and%20Andrea%20J.%20Goldsmith%20and%20Stephanie%20Gil&entry.1292438233=%20%20This%20work%20focuses%20on%20the%20problem%20of%20distributed%20optimization%20in%20multi-agent%0Acyberphysical%20systems%2C%20where%20a%20legitimate%20agents%27%20iterates%20are%20influenced%20both%0Aby%20the%20values%20it%20receives%20from%20potentially%20malicious%20neighboring%20agents%2C%20and%20by%0Aits%20own%20self-serving%20target%20function.%20We%20develop%20a%20new%20algorithmic%20and%0Aanalytical%20framework%20to%20achieve%20resilience%20for%20the%20class%20of%20problems%20where%0Astochastic%20values%20of%20trust%20between%20agents%20exist%20and%20can%20be%20exploited.%20In%20this%0Acase%20we%20show%20that%20convergence%20to%20the%20true%20global%20optimal%20point%20can%20be%0Arecovered%2C%20both%20in%20mean%20and%20almost%20surely%2C%20even%20in%20the%20presence%20of%20malicious%0Aagents.%20Furthermore%2C%20we%20provide%20expected%20convergence%20rate%20guarantees%20in%20the%0Aform%20of%20upper%20bounds%20on%20the%20expected%20squared%20distance%20to%20the%20optimal%20value.%0AFinally%2C%20numerical%20results%20are%20presented%20that%20validate%20our%20analytical%0Aconvergence%20guarantees%20even%20when%20the%20malicious%20agents%20compose%20the%20majority%20of%0Aagents%20in%20the%20network%20and%20where%20existing%20methods%20fail%20to%20converge%20to%20the%0Aoptimal%20nominal%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.02459v2&entry.124074799=Read"},
{"title": "Are We Done with MMLU?", "author": "Aryo Pradipta Gema and Joshua Ong Jun Leang and Giwon Hong and Alessio Devoto and Alberto Carlo Maria Mancino and Rohit Saxena and Xuanli He and Yu Zhao and Xiaotang Du and Mohammad Reza Ghasemi Madani and Claire Barale and Robert McHardy and Joshua Harris and Jean Kaddour and Emile van Krieken and Pasquale Minervini", "abstract": "  Maybe not. We identify and analyse errors in the popular Massive Multitask\nLanguage Understanding (MMLU) benchmark. Even though MMLU is widely adopted,\nour analysis demonstrates numerous ground truth errors that obscure the true\ncapabilities of LLMs. For example, we find that 57% of the analysed questions\nin the Virology subset contain errors. To address this issue, we introduce a\ncomprehensive framework for identifying dataset errors using a novel error\ntaxonomy. Then, we create MMLU-Redux, which is a subset of 3,000 manually\nre-annotated questions across 30 MMLU subjects. Using MMLU-Redux, we\ndemonstrate significant discrepancies with the model performance metrics that\nwere originally reported. Our results strongly advocate for revising MMLU's\nerror-ridden questions to enhance its future utility and reliability as a\nbenchmark. Therefore, we open up MMLU-Redux for additional annotation\nhttps://huggingface.co/datasets/edinburgh-dawg/mmlu-redux.\n", "link": "http://arxiv.org/abs/2406.04127v1", "date": "2024-06-06", "relevancy": 1.9268, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5769}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4629}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20We%20Done%20with%20MMLU%3F&body=Title%3A%20Are%20We%20Done%20with%20MMLU%3F%0AAuthor%3A%20Aryo%20Pradipta%20Gema%20and%20Joshua%20Ong%20Jun%20Leang%20and%20Giwon%20Hong%20and%20Alessio%20Devoto%20and%20Alberto%20Carlo%20Maria%20Mancino%20and%20Rohit%20Saxena%20and%20Xuanli%20He%20and%20Yu%20Zhao%20and%20Xiaotang%20Du%20and%20Mohammad%20Reza%20Ghasemi%20Madani%20and%20Claire%20Barale%20and%20Robert%20McHardy%20and%20Joshua%20Harris%20and%20Jean%20Kaddour%20and%20Emile%20van%20Krieken%20and%20Pasquale%20Minervini%0AAbstract%3A%20%20%20Maybe%20not.%20We%20identify%20and%20analyse%20errors%20in%20the%20popular%20Massive%20Multitask%0ALanguage%20Understanding%20%28MMLU%29%20benchmark.%20Even%20though%20MMLU%20is%20widely%20adopted%2C%0Aour%20analysis%20demonstrates%20numerous%20ground%20truth%20errors%20that%20obscure%20the%20true%0Acapabilities%20of%20LLMs.%20For%20example%2C%20we%20find%20that%2057%25%20of%20the%20analysed%20questions%0Ain%20the%20Virology%20subset%20contain%20errors.%20To%20address%20this%20issue%2C%20we%20introduce%20a%0Acomprehensive%20framework%20for%20identifying%20dataset%20errors%20using%20a%20novel%20error%0Ataxonomy.%20Then%2C%20we%20create%20MMLU-Redux%2C%20which%20is%20a%20subset%20of%203%2C000%20manually%0Are-annotated%20questions%20across%2030%20MMLU%20subjects.%20Using%20MMLU-Redux%2C%20we%0Ademonstrate%20significant%20discrepancies%20with%20the%20model%20performance%20metrics%20that%0Awere%20originally%20reported.%20Our%20results%20strongly%20advocate%20for%20revising%20MMLU%27s%0Aerror-ridden%20questions%20to%20enhance%20its%20future%20utility%20and%20reliability%20as%20a%0Abenchmark.%20Therefore%2C%20we%20open%20up%20MMLU-Redux%20for%20additional%20annotation%0Ahttps%3A//huggingface.co/datasets/edinburgh-dawg/mmlu-redux.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04127v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520We%2520Done%2520with%2520MMLU%253F%26entry.906535625%3DAryo%2520Pradipta%2520Gema%2520and%2520Joshua%2520Ong%2520Jun%2520Leang%2520and%2520Giwon%2520Hong%2520and%2520Alessio%2520Devoto%2520and%2520Alberto%2520Carlo%2520Maria%2520Mancino%2520and%2520Rohit%2520Saxena%2520and%2520Xuanli%2520He%2520and%2520Yu%2520Zhao%2520and%2520Xiaotang%2520Du%2520and%2520Mohammad%2520Reza%2520Ghasemi%2520Madani%2520and%2520Claire%2520Barale%2520and%2520Robert%2520McHardy%2520and%2520Joshua%2520Harris%2520and%2520Jean%2520Kaddour%2520and%2520Emile%2520van%2520Krieken%2520and%2520Pasquale%2520Minervini%26entry.1292438233%3D%2520%2520Maybe%2520not.%2520We%2520identify%2520and%2520analyse%2520errors%2520in%2520the%2520popular%2520Massive%2520Multitask%250ALanguage%2520Understanding%2520%2528MMLU%2529%2520benchmark.%2520Even%2520though%2520MMLU%2520is%2520widely%2520adopted%252C%250Aour%2520analysis%2520demonstrates%2520numerous%2520ground%2520truth%2520errors%2520that%2520obscure%2520the%2520true%250Acapabilities%2520of%2520LLMs.%2520For%2520example%252C%2520we%2520find%2520that%252057%2525%2520of%2520the%2520analysed%2520questions%250Ain%2520the%2520Virology%2520subset%2520contain%2520errors.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520a%250Acomprehensive%2520framework%2520for%2520identifying%2520dataset%2520errors%2520using%2520a%2520novel%2520error%250Ataxonomy.%2520Then%252C%2520we%2520create%2520MMLU-Redux%252C%2520which%2520is%2520a%2520subset%2520of%25203%252C000%2520manually%250Are-annotated%2520questions%2520across%252030%2520MMLU%2520subjects.%2520Using%2520MMLU-Redux%252C%2520we%250Ademonstrate%2520significant%2520discrepancies%2520with%2520the%2520model%2520performance%2520metrics%2520that%250Awere%2520originally%2520reported.%2520Our%2520results%2520strongly%2520advocate%2520for%2520revising%2520MMLU%2527s%250Aerror-ridden%2520questions%2520to%2520enhance%2520its%2520future%2520utility%2520and%2520reliability%2520as%2520a%250Abenchmark.%2520Therefore%252C%2520we%2520open%2520up%2520MMLU-Redux%2520for%2520additional%2520annotation%250Ahttps%253A//huggingface.co/datasets/edinburgh-dawg/mmlu-redux.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04127v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20We%20Done%20with%20MMLU%3F&entry.906535625=Aryo%20Pradipta%20Gema%20and%20Joshua%20Ong%20Jun%20Leang%20and%20Giwon%20Hong%20and%20Alessio%20Devoto%20and%20Alberto%20Carlo%20Maria%20Mancino%20and%20Rohit%20Saxena%20and%20Xuanli%20He%20and%20Yu%20Zhao%20and%20Xiaotang%20Du%20and%20Mohammad%20Reza%20Ghasemi%20Madani%20and%20Claire%20Barale%20and%20Robert%20McHardy%20and%20Joshua%20Harris%20and%20Jean%20Kaddour%20and%20Emile%20van%20Krieken%20and%20Pasquale%20Minervini&entry.1292438233=%20%20Maybe%20not.%20We%20identify%20and%20analyse%20errors%20in%20the%20popular%20Massive%20Multitask%0ALanguage%20Understanding%20%28MMLU%29%20benchmark.%20Even%20though%20MMLU%20is%20widely%20adopted%2C%0Aour%20analysis%20demonstrates%20numerous%20ground%20truth%20errors%20that%20obscure%20the%20true%0Acapabilities%20of%20LLMs.%20For%20example%2C%20we%20find%20that%2057%25%20of%20the%20analysed%20questions%0Ain%20the%20Virology%20subset%20contain%20errors.%20To%20address%20this%20issue%2C%20we%20introduce%20a%0Acomprehensive%20framework%20for%20identifying%20dataset%20errors%20using%20a%20novel%20error%0Ataxonomy.%20Then%2C%20we%20create%20MMLU-Redux%2C%20which%20is%20a%20subset%20of%203%2C000%20manually%0Are-annotated%20questions%20across%2030%20MMLU%20subjects.%20Using%20MMLU-Redux%2C%20we%0Ademonstrate%20significant%20discrepancies%20with%20the%20model%20performance%20metrics%20that%0Awere%20originally%20reported.%20Our%20results%20strongly%20advocate%20for%20revising%20MMLU%27s%0Aerror-ridden%20questions%20to%20enhance%20its%20future%20utility%20and%20reliability%20as%20a%0Abenchmark.%20Therefore%2C%20we%20open%20up%20MMLU-Redux%20for%20additional%20annotation%0Ahttps%3A//huggingface.co/datasets/edinburgh-dawg/mmlu-redux.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04127v1&entry.124074799=Read"},
{"title": "Deep Probabilistic Movement Primitives with a Bayesian Aggregator", "author": "Michael Przystupa and Faezeh Haghverd and Martin Jagersand and Samuele Tosatto", "abstract": "  Movement primitives are trainable parametric models that reproduce robotic\nmovements starting from a limited set of demonstrations. Previous works\nproposed simple linear models that exhibited high sample efficiency and\ngeneralization power by allowing temporal modulation of movements (reproducing\nmovements faster or slower), blending (merging two movements into one),\nvia-point conditioning (constraining a movement to meet some particular\nvia-points) and context conditioning (generation of movements based on an\nobserved variable, e.g., position of an object). Previous works have proposed\nneural network-based motor primitive models, having demonstrated their capacity\nto perform tasks with some forms of input conditioning or time-modulation\nrepresentations. However, there has not been a single unified deep motor\nprimitive's model proposed that is capable of all previous operations, limiting\nneural motor primitive's potential applications. This paper proposes a deep\nmovement primitive architecture that encodes all the operations above and uses\na Bayesian context aggregator that allows a more sound context conditioning and\nblending. Our results demonstrate our approach can scale to reproduce complex\nmotions on a larger variety of input choices compared to baselines while\nmaintaining operations of linear movement primitives provide.\n", "link": "http://arxiv.org/abs/2307.05141v3", "date": "2024-06-06", "relevancy": 1.6229, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6263}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5168}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Probabilistic%20Movement%20Primitives%20with%20a%20Bayesian%20Aggregator&body=Title%3A%20Deep%20Probabilistic%20Movement%20Primitives%20with%20a%20Bayesian%20Aggregator%0AAuthor%3A%20Michael%20Przystupa%20and%20Faezeh%20Haghverd%20and%20Martin%20Jagersand%20and%20Samuele%20Tosatto%0AAbstract%3A%20%20%20Movement%20primitives%20are%20trainable%20parametric%20models%20that%20reproduce%20robotic%0Amovements%20starting%20from%20a%20limited%20set%20of%20demonstrations.%20Previous%20works%0Aproposed%20simple%20linear%20models%20that%20exhibited%20high%20sample%20efficiency%20and%0Ageneralization%20power%20by%20allowing%20temporal%20modulation%20of%20movements%20%28reproducing%0Amovements%20faster%20or%20slower%29%2C%20blending%20%28merging%20two%20movements%20into%20one%29%2C%0Avia-point%20conditioning%20%28constraining%20a%20movement%20to%20meet%20some%20particular%0Avia-points%29%20and%20context%20conditioning%20%28generation%20of%20movements%20based%20on%20an%0Aobserved%20variable%2C%20e.g.%2C%20position%20of%20an%20object%29.%20Previous%20works%20have%20proposed%0Aneural%20network-based%20motor%20primitive%20models%2C%20having%20demonstrated%20their%20capacity%0Ato%20perform%20tasks%20with%20some%20forms%20of%20input%20conditioning%20or%20time-modulation%0Arepresentations.%20However%2C%20there%20has%20not%20been%20a%20single%20unified%20deep%20motor%0Aprimitive%27s%20model%20proposed%20that%20is%20capable%20of%20all%20previous%20operations%2C%20limiting%0Aneural%20motor%20primitive%27s%20potential%20applications.%20This%20paper%20proposes%20a%20deep%0Amovement%20primitive%20architecture%20that%20encodes%20all%20the%20operations%20above%20and%20uses%0Aa%20Bayesian%20context%20aggregator%20that%20allows%20a%20more%20sound%20context%20conditioning%20and%0Ablending.%20Our%20results%20demonstrate%20our%20approach%20can%20scale%20to%20reproduce%20complex%0Amotions%20on%20a%20larger%20variety%20of%20input%20choices%20compared%20to%20baselines%20while%0Amaintaining%20operations%20of%20linear%20movement%20primitives%20provide.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.05141v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Probabilistic%2520Movement%2520Primitives%2520with%2520a%2520Bayesian%2520Aggregator%26entry.906535625%3DMichael%2520Przystupa%2520and%2520Faezeh%2520Haghverd%2520and%2520Martin%2520Jagersand%2520and%2520Samuele%2520Tosatto%26entry.1292438233%3D%2520%2520Movement%2520primitives%2520are%2520trainable%2520parametric%2520models%2520that%2520reproduce%2520robotic%250Amovements%2520starting%2520from%2520a%2520limited%2520set%2520of%2520demonstrations.%2520Previous%2520works%250Aproposed%2520simple%2520linear%2520models%2520that%2520exhibited%2520high%2520sample%2520efficiency%2520and%250Ageneralization%2520power%2520by%2520allowing%2520temporal%2520modulation%2520of%2520movements%2520%2528reproducing%250Amovements%2520faster%2520or%2520slower%2529%252C%2520blending%2520%2528merging%2520two%2520movements%2520into%2520one%2529%252C%250Avia-point%2520conditioning%2520%2528constraining%2520a%2520movement%2520to%2520meet%2520some%2520particular%250Avia-points%2529%2520and%2520context%2520conditioning%2520%2528generation%2520of%2520movements%2520based%2520on%2520an%250Aobserved%2520variable%252C%2520e.g.%252C%2520position%2520of%2520an%2520object%2529.%2520Previous%2520works%2520have%2520proposed%250Aneural%2520network-based%2520motor%2520primitive%2520models%252C%2520having%2520demonstrated%2520their%2520capacity%250Ato%2520perform%2520tasks%2520with%2520some%2520forms%2520of%2520input%2520conditioning%2520or%2520time-modulation%250Arepresentations.%2520However%252C%2520there%2520has%2520not%2520been%2520a%2520single%2520unified%2520deep%2520motor%250Aprimitive%2527s%2520model%2520proposed%2520that%2520is%2520capable%2520of%2520all%2520previous%2520operations%252C%2520limiting%250Aneural%2520motor%2520primitive%2527s%2520potential%2520applications.%2520This%2520paper%2520proposes%2520a%2520deep%250Amovement%2520primitive%2520architecture%2520that%2520encodes%2520all%2520the%2520operations%2520above%2520and%2520uses%250Aa%2520Bayesian%2520context%2520aggregator%2520that%2520allows%2520a%2520more%2520sound%2520context%2520conditioning%2520and%250Ablending.%2520Our%2520results%2520demonstrate%2520our%2520approach%2520can%2520scale%2520to%2520reproduce%2520complex%250Amotions%2520on%2520a%2520larger%2520variety%2520of%2520input%2520choices%2520compared%2520to%2520baselines%2520while%250Amaintaining%2520operations%2520of%2520linear%2520movement%2520primitives%2520provide.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.05141v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Probabilistic%20Movement%20Primitives%20with%20a%20Bayesian%20Aggregator&entry.906535625=Michael%20Przystupa%20and%20Faezeh%20Haghverd%20and%20Martin%20Jagersand%20and%20Samuele%20Tosatto&entry.1292438233=%20%20Movement%20primitives%20are%20trainable%20parametric%20models%20that%20reproduce%20robotic%0Amovements%20starting%20from%20a%20limited%20set%20of%20demonstrations.%20Previous%20works%0Aproposed%20simple%20linear%20models%20that%20exhibited%20high%20sample%20efficiency%20and%0Ageneralization%20power%20by%20allowing%20temporal%20modulation%20of%20movements%20%28reproducing%0Amovements%20faster%20or%20slower%29%2C%20blending%20%28merging%20two%20movements%20into%20one%29%2C%0Avia-point%20conditioning%20%28constraining%20a%20movement%20to%20meet%20some%20particular%0Avia-points%29%20and%20context%20conditioning%20%28generation%20of%20movements%20based%20on%20an%0Aobserved%20variable%2C%20e.g.%2C%20position%20of%20an%20object%29.%20Previous%20works%20have%20proposed%0Aneural%20network-based%20motor%20primitive%20models%2C%20having%20demonstrated%20their%20capacity%0Ato%20perform%20tasks%20with%20some%20forms%20of%20input%20conditioning%20or%20time-modulation%0Arepresentations.%20However%2C%20there%20has%20not%20been%20a%20single%20unified%20deep%20motor%0Aprimitive%27s%20model%20proposed%20that%20is%20capable%20of%20all%20previous%20operations%2C%20limiting%0Aneural%20motor%20primitive%27s%20potential%20applications.%20This%20paper%20proposes%20a%20deep%0Amovement%20primitive%20architecture%20that%20encodes%20all%20the%20operations%20above%20and%20uses%0Aa%20Bayesian%20context%20aggregator%20that%20allows%20a%20more%20sound%20context%20conditioning%20and%0Ablending.%20Our%20results%20demonstrate%20our%20approach%20can%20scale%20to%20reproduce%20complex%0Amotions%20on%20a%20larger%20variety%20of%20input%20choices%20compared%20to%20baselines%20while%0Amaintaining%20operations%20of%20linear%20movement%20primitives%20provide.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.05141v3&entry.124074799=Read"},
{"title": "Multistep Distillation of Diffusion Models via Moment Matching", "author": "Tim Salimans and Thomas Mensink and Jonathan Heek and Emiel Hoogeboom", "abstract": "  We present a new method for making diffusion models faster to sample. The\nmethod distills many-step diffusion models into few-step models by matching\nconditional expectations of the clean data given noisy data along the sampling\ntrajectory. Our approach extends recently proposed one-step methods to the\nmulti-step case, and provides a new perspective by interpreting these\napproaches in terms of moment matching. By using up to 8 sampling steps, we\nobtain distilled models that outperform not only their one-step versions but\nalso their original many-step teacher models, obtaining new state-of-the-art\nresults on the Imagenet dataset. We also show promising results on a large\ntext-to-image model where we achieve fast generation of high resolution images\ndirectly in image space, without needing autoencoders or upsamplers.\n", "link": "http://arxiv.org/abs/2406.04103v1", "date": "2024-06-06", "relevancy": 1.8159, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.681}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5864}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multistep%20Distillation%20of%20Diffusion%20Models%20via%20Moment%20Matching&body=Title%3A%20Multistep%20Distillation%20of%20Diffusion%20Models%20via%20Moment%20Matching%0AAuthor%3A%20Tim%20Salimans%20and%20Thomas%20Mensink%20and%20Jonathan%20Heek%20and%20Emiel%20Hoogeboom%0AAbstract%3A%20%20%20We%20present%20a%20new%20method%20for%20making%20diffusion%20models%20faster%20to%20sample.%20The%0Amethod%20distills%20many-step%20diffusion%20models%20into%20few-step%20models%20by%20matching%0Aconditional%20expectations%20of%20the%20clean%20data%20given%20noisy%20data%20along%20the%20sampling%0Atrajectory.%20Our%20approach%20extends%20recently%20proposed%20one-step%20methods%20to%20the%0Amulti-step%20case%2C%20and%20provides%20a%20new%20perspective%20by%20interpreting%20these%0Aapproaches%20in%20terms%20of%20moment%20matching.%20By%20using%20up%20to%208%20sampling%20steps%2C%20we%0Aobtain%20distilled%20models%20that%20outperform%20not%20only%20their%20one-step%20versions%20but%0Aalso%20their%20original%20many-step%20teacher%20models%2C%20obtaining%20new%20state-of-the-art%0Aresults%20on%20the%20Imagenet%20dataset.%20We%20also%20show%20promising%20results%20on%20a%20large%0Atext-to-image%20model%20where%20we%20achieve%20fast%20generation%20of%20high%20resolution%20images%0Adirectly%20in%20image%20space%2C%20without%20needing%20autoencoders%20or%20upsamplers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04103v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultistep%2520Distillation%2520of%2520Diffusion%2520Models%2520via%2520Moment%2520Matching%26entry.906535625%3DTim%2520Salimans%2520and%2520Thomas%2520Mensink%2520and%2520Jonathan%2520Heek%2520and%2520Emiel%2520Hoogeboom%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520method%2520for%2520making%2520diffusion%2520models%2520faster%2520to%2520sample.%2520The%250Amethod%2520distills%2520many-step%2520diffusion%2520models%2520into%2520few-step%2520models%2520by%2520matching%250Aconditional%2520expectations%2520of%2520the%2520clean%2520data%2520given%2520noisy%2520data%2520along%2520the%2520sampling%250Atrajectory.%2520Our%2520approach%2520extends%2520recently%2520proposed%2520one-step%2520methods%2520to%2520the%250Amulti-step%2520case%252C%2520and%2520provides%2520a%2520new%2520perspective%2520by%2520interpreting%2520these%250Aapproaches%2520in%2520terms%2520of%2520moment%2520matching.%2520By%2520using%2520up%2520to%25208%2520sampling%2520steps%252C%2520we%250Aobtain%2520distilled%2520models%2520that%2520outperform%2520not%2520only%2520their%2520one-step%2520versions%2520but%250Aalso%2520their%2520original%2520many-step%2520teacher%2520models%252C%2520obtaining%2520new%2520state-of-the-art%250Aresults%2520on%2520the%2520Imagenet%2520dataset.%2520We%2520also%2520show%2520promising%2520results%2520on%2520a%2520large%250Atext-to-image%2520model%2520where%2520we%2520achieve%2520fast%2520generation%2520of%2520high%2520resolution%2520images%250Adirectly%2520in%2520image%2520space%252C%2520without%2520needing%2520autoencoders%2520or%2520upsamplers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04103v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multistep%20Distillation%20of%20Diffusion%20Models%20via%20Moment%20Matching&entry.906535625=Tim%20Salimans%20and%20Thomas%20Mensink%20and%20Jonathan%20Heek%20and%20Emiel%20Hoogeboom&entry.1292438233=%20%20We%20present%20a%20new%20method%20for%20making%20diffusion%20models%20faster%20to%20sample.%20The%0Amethod%20distills%20many-step%20diffusion%20models%20into%20few-step%20models%20by%20matching%0Aconditional%20expectations%20of%20the%20clean%20data%20given%20noisy%20data%20along%20the%20sampling%0Atrajectory.%20Our%20approach%20extends%20recently%20proposed%20one-step%20methods%20to%20the%0Amulti-step%20case%2C%20and%20provides%20a%20new%20perspective%20by%20interpreting%20these%0Aapproaches%20in%20terms%20of%20moment%20matching.%20By%20using%20up%20to%208%20sampling%20steps%2C%20we%0Aobtain%20distilled%20models%20that%20outperform%20not%20only%20their%20one-step%20versions%20but%0Aalso%20their%20original%20many-step%20teacher%20models%2C%20obtaining%20new%20state-of-the-art%0Aresults%20on%20the%20Imagenet%20dataset.%20We%20also%20show%20promising%20results%20on%20a%20large%0Atext-to-image%20model%20where%20we%20achieve%20fast%20generation%20of%20high%20resolution%20images%0Adirectly%20in%20image%20space%2C%20without%20needing%20autoencoders%20or%20upsamplers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04103v1&entry.124074799=Read"},
{"title": "Simulating infinite-dimensional nonlinear diffusion bridges", "author": "Gefan Yang and Elizabeth Louise Baker and Michael L. Severinsen and Christy Anna Hipsley and Stefan Sommer", "abstract": "  The diffusion bridge is a type of diffusion process that conditions on\nhitting a specific state within a finite time period. It has broad applications\nin fields such as Bayesian inference, financial mathematics, control theory,\nand shape analysis. However, simulating the diffusion bridge for natural data\ncan be challenging due to both the intractability of the drift term and\ncontinuous representations of the data. Although several methods are available\nto simulate finite-dimensional diffusion bridges, infinite-dimensional cases\nremain unresolved. In the paper, we present a solution to this problem by\nmerging score-matching techniques with operator learning, enabling a direct\napproach to score-matching for the infinite-dimensional bridge. We construct\nthe score to be discretization invariant, which is natural given the underlying\nspatially continuous process. We conduct a series of experiments, ranging from\nsynthetic examples with closed-form solutions to the stochastic nonlinear\nevolution of real-world biological shape data, and our method demonstrates high\nefficacy, particularly due to its ability to adapt to any resolution without\nextra training.\n", "link": "http://arxiv.org/abs/2405.18353v2", "date": "2024-06-06", "relevancy": 1.0085, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5416}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5006}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4705}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simulating%20infinite-dimensional%20nonlinear%20diffusion%20bridges&body=Title%3A%20Simulating%20infinite-dimensional%20nonlinear%20diffusion%20bridges%0AAuthor%3A%20Gefan%20Yang%20and%20Elizabeth%20Louise%20Baker%20and%20Michael%20L.%20Severinsen%20and%20Christy%20Anna%20Hipsley%20and%20Stefan%20Sommer%0AAbstract%3A%20%20%20The%20diffusion%20bridge%20is%20a%20type%20of%20diffusion%20process%20that%20conditions%20on%0Ahitting%20a%20specific%20state%20within%20a%20finite%20time%20period.%20It%20has%20broad%20applications%0Ain%20fields%20such%20as%20Bayesian%20inference%2C%20financial%20mathematics%2C%20control%20theory%2C%0Aand%20shape%20analysis.%20However%2C%20simulating%20the%20diffusion%20bridge%20for%20natural%20data%0Acan%20be%20challenging%20due%20to%20both%20the%20intractability%20of%20the%20drift%20term%20and%0Acontinuous%20representations%20of%20the%20data.%20Although%20several%20methods%20are%20available%0Ato%20simulate%20finite-dimensional%20diffusion%20bridges%2C%20infinite-dimensional%20cases%0Aremain%20unresolved.%20In%20the%20paper%2C%20we%20present%20a%20solution%20to%20this%20problem%20by%0Amerging%20score-matching%20techniques%20with%20operator%20learning%2C%20enabling%20a%20direct%0Aapproach%20to%20score-matching%20for%20the%20infinite-dimensional%20bridge.%20We%20construct%0Athe%20score%20to%20be%20discretization%20invariant%2C%20which%20is%20natural%20given%20the%20underlying%0Aspatially%20continuous%20process.%20We%20conduct%20a%20series%20of%20experiments%2C%20ranging%20from%0Asynthetic%20examples%20with%20closed-form%20solutions%20to%20the%20stochastic%20nonlinear%0Aevolution%20of%20real-world%20biological%20shape%20data%2C%20and%20our%20method%20demonstrates%20high%0Aefficacy%2C%20particularly%20due%20to%20its%20ability%20to%20adapt%20to%20any%20resolution%20without%0Aextra%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18353v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimulating%2520infinite-dimensional%2520nonlinear%2520diffusion%2520bridges%26entry.906535625%3DGefan%2520Yang%2520and%2520Elizabeth%2520Louise%2520Baker%2520and%2520Michael%2520L.%2520Severinsen%2520and%2520Christy%2520Anna%2520Hipsley%2520and%2520Stefan%2520Sommer%26entry.1292438233%3D%2520%2520The%2520diffusion%2520bridge%2520is%2520a%2520type%2520of%2520diffusion%2520process%2520that%2520conditions%2520on%250Ahitting%2520a%2520specific%2520state%2520within%2520a%2520finite%2520time%2520period.%2520It%2520has%2520broad%2520applications%250Ain%2520fields%2520such%2520as%2520Bayesian%2520inference%252C%2520financial%2520mathematics%252C%2520control%2520theory%252C%250Aand%2520shape%2520analysis.%2520However%252C%2520simulating%2520the%2520diffusion%2520bridge%2520for%2520natural%2520data%250Acan%2520be%2520challenging%2520due%2520to%2520both%2520the%2520intractability%2520of%2520the%2520drift%2520term%2520and%250Acontinuous%2520representations%2520of%2520the%2520data.%2520Although%2520several%2520methods%2520are%2520available%250Ato%2520simulate%2520finite-dimensional%2520diffusion%2520bridges%252C%2520infinite-dimensional%2520cases%250Aremain%2520unresolved.%2520In%2520the%2520paper%252C%2520we%2520present%2520a%2520solution%2520to%2520this%2520problem%2520by%250Amerging%2520score-matching%2520techniques%2520with%2520operator%2520learning%252C%2520enabling%2520a%2520direct%250Aapproach%2520to%2520score-matching%2520for%2520the%2520infinite-dimensional%2520bridge.%2520We%2520construct%250Athe%2520score%2520to%2520be%2520discretization%2520invariant%252C%2520which%2520is%2520natural%2520given%2520the%2520underlying%250Aspatially%2520continuous%2520process.%2520We%2520conduct%2520a%2520series%2520of%2520experiments%252C%2520ranging%2520from%250Asynthetic%2520examples%2520with%2520closed-form%2520solutions%2520to%2520the%2520stochastic%2520nonlinear%250Aevolution%2520of%2520real-world%2520biological%2520shape%2520data%252C%2520and%2520our%2520method%2520demonstrates%2520high%250Aefficacy%252C%2520particularly%2520due%2520to%2520its%2520ability%2520to%2520adapt%2520to%2520any%2520resolution%2520without%250Aextra%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18353v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simulating%20infinite-dimensional%20nonlinear%20diffusion%20bridges&entry.906535625=Gefan%20Yang%20and%20Elizabeth%20Louise%20Baker%20and%20Michael%20L.%20Severinsen%20and%20Christy%20Anna%20Hipsley%20and%20Stefan%20Sommer&entry.1292438233=%20%20The%20diffusion%20bridge%20is%20a%20type%20of%20diffusion%20process%20that%20conditions%20on%0Ahitting%20a%20specific%20state%20within%20a%20finite%20time%20period.%20It%20has%20broad%20applications%0Ain%20fields%20such%20as%20Bayesian%20inference%2C%20financial%20mathematics%2C%20control%20theory%2C%0Aand%20shape%20analysis.%20However%2C%20simulating%20the%20diffusion%20bridge%20for%20natural%20data%0Acan%20be%20challenging%20due%20to%20both%20the%20intractability%20of%20the%20drift%20term%20and%0Acontinuous%20representations%20of%20the%20data.%20Although%20several%20methods%20are%20available%0Ato%20simulate%20finite-dimensional%20diffusion%20bridges%2C%20infinite-dimensional%20cases%0Aremain%20unresolved.%20In%20the%20paper%2C%20we%20present%20a%20solution%20to%20this%20problem%20by%0Amerging%20score-matching%20techniques%20with%20operator%20learning%2C%20enabling%20a%20direct%0Aapproach%20to%20score-matching%20for%20the%20infinite-dimensional%20bridge.%20We%20construct%0Athe%20score%20to%20be%20discretization%20invariant%2C%20which%20is%20natural%20given%20the%20underlying%0Aspatially%20continuous%20process.%20We%20conduct%20a%20series%20of%20experiments%2C%20ranging%20from%0Asynthetic%20examples%20with%20closed-form%20solutions%20to%20the%20stochastic%20nonlinear%0Aevolution%20of%20real-world%20biological%20shape%20data%2C%20and%20our%20method%20demonstrates%20high%0Aefficacy%2C%20particularly%20due%20to%20its%20ability%20to%20adapt%20to%20any%20resolution%20without%0Aextra%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18353v2&entry.124074799=Read"},
{"title": "Accelerated Policy Gradient: On the Convergence Rates of the Nesterov\n  Momentum for Reinforcement Learning", "author": "Yen-Ju Chen and Nai-Chieh Huang and Ching-Pei Lee and Ping-Chun Hsieh", "abstract": "  Various acceleration approaches for Policy Gradient (PG) have been analyzed\nwithin the realm of Reinforcement Learning (RL). However, the theoretical\nunderstanding of the widely used momentum-based acceleration method on PG\nremains largely open. In response to this gap, we adapt the celebrated\nNesterov's accelerated gradient (NAG) method to policy optimization in RL,\ntermed \\textit{Accelerated Policy Gradient} (APG). To demonstrate the potential\nof APG in achieving fast convergence, we formally prove that with the true\ngradient and under the softmax policy parametrization, APG converges to an\noptimal policy at rates: (i) $\\tilde{O}(1/t^2)$ with constant step sizes; (ii)\n$O(e^{-ct})$ with exponentially-growing step sizes. To the best of our\nknowledge, this is the first characterization of the convergence rates of NAG\nin the context of RL. Notably, our analysis relies on one interesting finding:\nRegardless of the parameter initialization, APG ends up entering a locally\nnearly-concave regime, where APG can significantly benefit from the momentum,\nwithin finite iterations. Through numerical validation and experiments on the\nAtari 2600 benchmarks, we confirm that APG exhibits a $\\tilde{O}(1/t^2)$ rate\nwith constant step sizes and a linear convergence rate with\nexponentially-growing step sizes, significantly improving convergence over the\nstandard PG.\n", "link": "http://arxiv.org/abs/2310.11897v3", "date": "2024-06-06", "relevancy": 1.7831, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4566}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4432}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerated%20Policy%20Gradient%3A%20On%20the%20Convergence%20Rates%20of%20the%20Nesterov%0A%20%20Momentum%20for%20Reinforcement%20Learning&body=Title%3A%20Accelerated%20Policy%20Gradient%3A%20On%20the%20Convergence%20Rates%20of%20the%20Nesterov%0A%20%20Momentum%20for%20Reinforcement%20Learning%0AAuthor%3A%20Yen-Ju%20Chen%20and%20Nai-Chieh%20Huang%20and%20Ching-Pei%20Lee%20and%20Ping-Chun%20Hsieh%0AAbstract%3A%20%20%20Various%20acceleration%20approaches%20for%20Policy%20Gradient%20%28PG%29%20have%20been%20analyzed%0Awithin%20the%20realm%20of%20Reinforcement%20Learning%20%28RL%29.%20However%2C%20the%20theoretical%0Aunderstanding%20of%20the%20widely%20used%20momentum-based%20acceleration%20method%20on%20PG%0Aremains%20largely%20open.%20In%20response%20to%20this%20gap%2C%20we%20adapt%20the%20celebrated%0ANesterov%27s%20accelerated%20gradient%20%28NAG%29%20method%20to%20policy%20optimization%20in%20RL%2C%0Atermed%20%5Ctextit%7BAccelerated%20Policy%20Gradient%7D%20%28APG%29.%20To%20demonstrate%20the%20potential%0Aof%20APG%20in%20achieving%20fast%20convergence%2C%20we%20formally%20prove%20that%20with%20the%20true%0Agradient%20and%20under%20the%20softmax%20policy%20parametrization%2C%20APG%20converges%20to%20an%0Aoptimal%20policy%20at%20rates%3A%20%28i%29%20%24%5Ctilde%7BO%7D%281/t%5E2%29%24%20with%20constant%20step%20sizes%3B%20%28ii%29%0A%24O%28e%5E%7B-ct%7D%29%24%20with%20exponentially-growing%20step%20sizes.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20characterization%20of%20the%20convergence%20rates%20of%20NAG%0Ain%20the%20context%20of%20RL.%20Notably%2C%20our%20analysis%20relies%20on%20one%20interesting%20finding%3A%0ARegardless%20of%20the%20parameter%20initialization%2C%20APG%20ends%20up%20entering%20a%20locally%0Anearly-concave%20regime%2C%20where%20APG%20can%20significantly%20benefit%20from%20the%20momentum%2C%0Awithin%20finite%20iterations.%20Through%20numerical%20validation%20and%20experiments%20on%20the%0AAtari%202600%20benchmarks%2C%20we%20confirm%20that%20APG%20exhibits%20a%20%24%5Ctilde%7BO%7D%281/t%5E2%29%24%20rate%0Awith%20constant%20step%20sizes%20and%20a%20linear%20convergence%20rate%20with%0Aexponentially-growing%20step%20sizes%2C%20significantly%20improving%20convergence%20over%20the%0Astandard%20PG.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.11897v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerated%2520Policy%2520Gradient%253A%2520On%2520the%2520Convergence%2520Rates%2520of%2520the%2520Nesterov%250A%2520%2520Momentum%2520for%2520Reinforcement%2520Learning%26entry.906535625%3DYen-Ju%2520Chen%2520and%2520Nai-Chieh%2520Huang%2520and%2520Ching-Pei%2520Lee%2520and%2520Ping-Chun%2520Hsieh%26entry.1292438233%3D%2520%2520Various%2520acceleration%2520approaches%2520for%2520Policy%2520Gradient%2520%2528PG%2529%2520have%2520been%2520analyzed%250Awithin%2520the%2520realm%2520of%2520Reinforcement%2520Learning%2520%2528RL%2529.%2520However%252C%2520the%2520theoretical%250Aunderstanding%2520of%2520the%2520widely%2520used%2520momentum-based%2520acceleration%2520method%2520on%2520PG%250Aremains%2520largely%2520open.%2520In%2520response%2520to%2520this%2520gap%252C%2520we%2520adapt%2520the%2520celebrated%250ANesterov%2527s%2520accelerated%2520gradient%2520%2528NAG%2529%2520method%2520to%2520policy%2520optimization%2520in%2520RL%252C%250Atermed%2520%255Ctextit%257BAccelerated%2520Policy%2520Gradient%257D%2520%2528APG%2529.%2520To%2520demonstrate%2520the%2520potential%250Aof%2520APG%2520in%2520achieving%2520fast%2520convergence%252C%2520we%2520formally%2520prove%2520that%2520with%2520the%2520true%250Agradient%2520and%2520under%2520the%2520softmax%2520policy%2520parametrization%252C%2520APG%2520converges%2520to%2520an%250Aoptimal%2520policy%2520at%2520rates%253A%2520%2528i%2529%2520%2524%255Ctilde%257BO%257D%25281/t%255E2%2529%2524%2520with%2520constant%2520step%2520sizes%253B%2520%2528ii%2529%250A%2524O%2528e%255E%257B-ct%257D%2529%2524%2520with%2520exponentially-growing%2520step%2520sizes.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520characterization%2520of%2520the%2520convergence%2520rates%2520of%2520NAG%250Ain%2520the%2520context%2520of%2520RL.%2520Notably%252C%2520our%2520analysis%2520relies%2520on%2520one%2520interesting%2520finding%253A%250ARegardless%2520of%2520the%2520parameter%2520initialization%252C%2520APG%2520ends%2520up%2520entering%2520a%2520locally%250Anearly-concave%2520regime%252C%2520where%2520APG%2520can%2520significantly%2520benefit%2520from%2520the%2520momentum%252C%250Awithin%2520finite%2520iterations.%2520Through%2520numerical%2520validation%2520and%2520experiments%2520on%2520the%250AAtari%25202600%2520benchmarks%252C%2520we%2520confirm%2520that%2520APG%2520exhibits%2520a%2520%2524%255Ctilde%257BO%257D%25281/t%255E2%2529%2524%2520rate%250Awith%2520constant%2520step%2520sizes%2520and%2520a%2520linear%2520convergence%2520rate%2520with%250Aexponentially-growing%2520step%2520sizes%252C%2520significantly%2520improving%2520convergence%2520over%2520the%250Astandard%2520PG.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.11897v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerated%20Policy%20Gradient%3A%20On%20the%20Convergence%20Rates%20of%20the%20Nesterov%0A%20%20Momentum%20for%20Reinforcement%20Learning&entry.906535625=Yen-Ju%20Chen%20and%20Nai-Chieh%20Huang%20and%20Ching-Pei%20Lee%20and%20Ping-Chun%20Hsieh&entry.1292438233=%20%20Various%20acceleration%20approaches%20for%20Policy%20Gradient%20%28PG%29%20have%20been%20analyzed%0Awithin%20the%20realm%20of%20Reinforcement%20Learning%20%28RL%29.%20However%2C%20the%20theoretical%0Aunderstanding%20of%20the%20widely%20used%20momentum-based%20acceleration%20method%20on%20PG%0Aremains%20largely%20open.%20In%20response%20to%20this%20gap%2C%20we%20adapt%20the%20celebrated%0ANesterov%27s%20accelerated%20gradient%20%28NAG%29%20method%20to%20policy%20optimization%20in%20RL%2C%0Atermed%20%5Ctextit%7BAccelerated%20Policy%20Gradient%7D%20%28APG%29.%20To%20demonstrate%20the%20potential%0Aof%20APG%20in%20achieving%20fast%20convergence%2C%20we%20formally%20prove%20that%20with%20the%20true%0Agradient%20and%20under%20the%20softmax%20policy%20parametrization%2C%20APG%20converges%20to%20an%0Aoptimal%20policy%20at%20rates%3A%20%28i%29%20%24%5Ctilde%7BO%7D%281/t%5E2%29%24%20with%20constant%20step%20sizes%3B%20%28ii%29%0A%24O%28e%5E%7B-ct%7D%29%24%20with%20exponentially-growing%20step%20sizes.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20characterization%20of%20the%20convergence%20rates%20of%20NAG%0Ain%20the%20context%20of%20RL.%20Notably%2C%20our%20analysis%20relies%20on%20one%20interesting%20finding%3A%0ARegardless%20of%20the%20parameter%20initialization%2C%20APG%20ends%20up%20entering%20a%20locally%0Anearly-concave%20regime%2C%20where%20APG%20can%20significantly%20benefit%20from%20the%20momentum%2C%0Awithin%20finite%20iterations.%20Through%20numerical%20validation%20and%20experiments%20on%20the%0AAtari%202600%20benchmarks%2C%20we%20confirm%20that%20APG%20exhibits%20a%20%24%5Ctilde%7BO%7D%281/t%5E2%29%24%20rate%0Awith%20constant%20step%20sizes%20and%20a%20linear%20convergence%20rate%20with%0Aexponentially-growing%20step%20sizes%2C%20significantly%20improving%20convergence%20over%20the%0Astandard%20PG.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.11897v3&entry.124074799=Read"},
{"title": "Semmeldetector: Application of Machine Learning in Commercial Bakeries", "author": "Thomas H. Schmitt and Maximilian Bundscherer and Tobias Bocklet", "abstract": "  The Semmeldetector, is a machine learning application that utilizes object\ndetection models to detect, classify and count baked goods in images. Our\napplication allows commercial bakers to track unsold baked goods, which allows\nthem to optimize production and increase resource efficiency. We compiled a\ndataset comprising 1151 images that distinguishes between 18 different types of\nbaked goods to train our detection models. To facilitate model training, we\nused a Copy-Paste augmentation pipeline to expand our dataset. We trained the\nstate-of-the-art object detection model YOLOv8 on our detection task. We tested\nthe impact of different training data, model scale, and online image\naugmentation pipelines on model performance. Our overall best performing model,\nachieved an AP@0.5 of 89.1% on our test set. Based on our results, we conclude\nthat machine learning can be a valuable tool even for unforeseen industries\nlike bakeries, even with very limited datasets.\n", "link": "http://arxiv.org/abs/2406.04050v1", "date": "2024-06-06", "relevancy": 2.0255, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5347}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4887}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semmeldetector%3A%20Application%20of%20Machine%20Learning%20in%20Commercial%20Bakeries&body=Title%3A%20Semmeldetector%3A%20Application%20of%20Machine%20Learning%20in%20Commercial%20Bakeries%0AAuthor%3A%20Thomas%20H.%20Schmitt%20and%20Maximilian%20Bundscherer%20and%20Tobias%20Bocklet%0AAbstract%3A%20%20%20The%20Semmeldetector%2C%20is%20a%20machine%20learning%20application%20that%20utilizes%20object%0Adetection%20models%20to%20detect%2C%20classify%20and%20count%20baked%20goods%20in%20images.%20Our%0Aapplication%20allows%20commercial%20bakers%20to%20track%20unsold%20baked%20goods%2C%20which%20allows%0Athem%20to%20optimize%20production%20and%20increase%20resource%20efficiency.%20We%20compiled%20a%0Adataset%20comprising%201151%20images%20that%20distinguishes%20between%2018%20different%20types%20of%0Abaked%20goods%20to%20train%20our%20detection%20models.%20To%20facilitate%20model%20training%2C%20we%0Aused%20a%20Copy-Paste%20augmentation%20pipeline%20to%20expand%20our%20dataset.%20We%20trained%20the%0Astate-of-the-art%20object%20detection%20model%20YOLOv8%20on%20our%20detection%20task.%20We%20tested%0Athe%20impact%20of%20different%20training%20data%2C%20model%20scale%2C%20and%20online%20image%0Aaugmentation%20pipelines%20on%20model%20performance.%20Our%20overall%20best%20performing%20model%2C%0Aachieved%20an%20AP%400.5%20of%2089.1%25%20on%20our%20test%20set.%20Based%20on%20our%20results%2C%20we%20conclude%0Athat%20machine%20learning%20can%20be%20a%20valuable%20tool%20even%20for%20unforeseen%20industries%0Alike%20bakeries%2C%20even%20with%20very%20limited%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04050v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemmeldetector%253A%2520Application%2520of%2520Machine%2520Learning%2520in%2520Commercial%2520Bakeries%26entry.906535625%3DThomas%2520H.%2520Schmitt%2520and%2520Maximilian%2520Bundscherer%2520and%2520Tobias%2520Bocklet%26entry.1292438233%3D%2520%2520The%2520Semmeldetector%252C%2520is%2520a%2520machine%2520learning%2520application%2520that%2520utilizes%2520object%250Adetection%2520models%2520to%2520detect%252C%2520classify%2520and%2520count%2520baked%2520goods%2520in%2520images.%2520Our%250Aapplication%2520allows%2520commercial%2520bakers%2520to%2520track%2520unsold%2520baked%2520goods%252C%2520which%2520allows%250Athem%2520to%2520optimize%2520production%2520and%2520increase%2520resource%2520efficiency.%2520We%2520compiled%2520a%250Adataset%2520comprising%25201151%2520images%2520that%2520distinguishes%2520between%252018%2520different%2520types%2520of%250Abaked%2520goods%2520to%2520train%2520our%2520detection%2520models.%2520To%2520facilitate%2520model%2520training%252C%2520we%250Aused%2520a%2520Copy-Paste%2520augmentation%2520pipeline%2520to%2520expand%2520our%2520dataset.%2520We%2520trained%2520the%250Astate-of-the-art%2520object%2520detection%2520model%2520YOLOv8%2520on%2520our%2520detection%2520task.%2520We%2520tested%250Athe%2520impact%2520of%2520different%2520training%2520data%252C%2520model%2520scale%252C%2520and%2520online%2520image%250Aaugmentation%2520pipelines%2520on%2520model%2520performance.%2520Our%2520overall%2520best%2520performing%2520model%252C%250Aachieved%2520an%2520AP%25400.5%2520of%252089.1%2525%2520on%2520our%2520test%2520set.%2520Based%2520on%2520our%2520results%252C%2520we%2520conclude%250Athat%2520machine%2520learning%2520can%2520be%2520a%2520valuable%2520tool%2520even%2520for%2520unforeseen%2520industries%250Alike%2520bakeries%252C%2520even%2520with%2520very%2520limited%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04050v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semmeldetector%3A%20Application%20of%20Machine%20Learning%20in%20Commercial%20Bakeries&entry.906535625=Thomas%20H.%20Schmitt%20and%20Maximilian%20Bundscherer%20and%20Tobias%20Bocklet&entry.1292438233=%20%20The%20Semmeldetector%2C%20is%20a%20machine%20learning%20application%20that%20utilizes%20object%0Adetection%20models%20to%20detect%2C%20classify%20and%20count%20baked%20goods%20in%20images.%20Our%0Aapplication%20allows%20commercial%20bakers%20to%20track%20unsold%20baked%20goods%2C%20which%20allows%0Athem%20to%20optimize%20production%20and%20increase%20resource%20efficiency.%20We%20compiled%20a%0Adataset%20comprising%201151%20images%20that%20distinguishes%20between%2018%20different%20types%20of%0Abaked%20goods%20to%20train%20our%20detection%20models.%20To%20facilitate%20model%20training%2C%20we%0Aused%20a%20Copy-Paste%20augmentation%20pipeline%20to%20expand%20our%20dataset.%20We%20trained%20the%0Astate-of-the-art%20object%20detection%20model%20YOLOv8%20on%20our%20detection%20task.%20We%20tested%0Athe%20impact%20of%20different%20training%20data%2C%20model%20scale%2C%20and%20online%20image%0Aaugmentation%20pipelines%20on%20model%20performance.%20Our%20overall%20best%20performing%20model%2C%0Aachieved%20an%20AP%400.5%20of%2089.1%25%20on%20our%20test%20set.%20Based%20on%20our%20results%2C%20we%20conclude%0Athat%20machine%20learning%20can%20be%20a%20valuable%20tool%20even%20for%20unforeseen%20industries%0Alike%20bakeries%2C%20even%20with%20very%20limited%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04050v1&entry.124074799=Read"},
{"title": "Knowledge Graph Question Answering for Materials Science (KGQA4MAT):\n  Developing Natural Language Interface for Metal-Organic Frameworks Knowledge\n  Graph (MOF-KG) Using LLM", "author": "Yuan An and Jane Greenberg and Alex Kalinowski and Xintong Zhao and Xiaohua Hu and Fernando J. Uribe-Romo and Kyle Langlois and Jacob Furst and Diego A. G\u00f3mez-Gualdr\u00f3n", "abstract": "  We present a comprehensive benchmark dataset for Knowledge Graph Question\nAnswering in Materials Science (KGQA4MAT), with a focus on metal-organic\nframeworks (MOFs). A knowledge graph for metal-organic frameworks (MOF-KG) has\nbeen constructed by integrating structured databases and knowledge extracted\nfrom the literature. To enhance MOF-KG accessibility for domain experts, we aim\nto develop a natural language interface for querying the knowledge graph. We\nhave developed a benchmark comprised of 161 complex questions involving\ncomparison, aggregation, and complicated graph structures. Each question is\nrephrased in three additional variations, resulting in 644 questions and 161 KG\nqueries. To evaluate the benchmark, we have developed a systematic approach for\nutilizing the LLM, ChatGPT, to translate natural language questions into formal\nKG queries. We also apply the approach to the well-known QALD-9 dataset,\ndemonstrating ChatGPT's potential in addressing KGQA issues for different\nplatforms and query languages. The benchmark and the proposed approach aim to\nstimulate further research and development of user-friendly and efficient\ninterfaces for querying domain-specific materials science knowledge graphs,\nthereby accelerating the discovery of novel materials.\n", "link": "http://arxiv.org/abs/2309.11361v2", "date": "2024-06-06", "relevancy": 1.2959, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4562}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4308}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4228}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Graph%20Question%20Answering%20for%20Materials%20Science%20%28KGQA4MAT%29%3A%0A%20%20Developing%20Natural%20Language%20Interface%20for%20Metal-Organic%20Frameworks%20Knowledge%0A%20%20Graph%20%28MOF-KG%29%20Using%20LLM&body=Title%3A%20Knowledge%20Graph%20Question%20Answering%20for%20Materials%20Science%20%28KGQA4MAT%29%3A%0A%20%20Developing%20Natural%20Language%20Interface%20for%20Metal-Organic%20Frameworks%20Knowledge%0A%20%20Graph%20%28MOF-KG%29%20Using%20LLM%0AAuthor%3A%20Yuan%20An%20and%20Jane%20Greenberg%20and%20Alex%20Kalinowski%20and%20Xintong%20Zhao%20and%20Xiaohua%20Hu%20and%20Fernando%20J.%20Uribe-Romo%20and%20Kyle%20Langlois%20and%20Jacob%20Furst%20and%20Diego%20A.%20G%C3%B3mez-Gualdr%C3%B3n%0AAbstract%3A%20%20%20We%20present%20a%20comprehensive%20benchmark%20dataset%20for%20Knowledge%20Graph%20Question%0AAnswering%20in%20Materials%20Science%20%28KGQA4MAT%29%2C%20with%20a%20focus%20on%20metal-organic%0Aframeworks%20%28MOFs%29.%20A%20knowledge%20graph%20for%20metal-organic%20frameworks%20%28MOF-KG%29%20has%0Abeen%20constructed%20by%20integrating%20structured%20databases%20and%20knowledge%20extracted%0Afrom%20the%20literature.%20To%20enhance%20MOF-KG%20accessibility%20for%20domain%20experts%2C%20we%20aim%0Ato%20develop%20a%20natural%20language%20interface%20for%20querying%20the%20knowledge%20graph.%20We%0Ahave%20developed%20a%20benchmark%20comprised%20of%20161%20complex%20questions%20involving%0Acomparison%2C%20aggregation%2C%20and%20complicated%20graph%20structures.%20Each%20question%20is%0Arephrased%20in%20three%20additional%20variations%2C%20resulting%20in%20644%20questions%20and%20161%20KG%0Aqueries.%20To%20evaluate%20the%20benchmark%2C%20we%20have%20developed%20a%20systematic%20approach%20for%0Autilizing%20the%20LLM%2C%20ChatGPT%2C%20to%20translate%20natural%20language%20questions%20into%20formal%0AKG%20queries.%20We%20also%20apply%20the%20approach%20to%20the%20well-known%20QALD-9%20dataset%2C%0Ademonstrating%20ChatGPT%27s%20potential%20in%20addressing%20KGQA%20issues%20for%20different%0Aplatforms%20and%20query%20languages.%20The%20benchmark%20and%20the%20proposed%20approach%20aim%20to%0Astimulate%20further%20research%20and%20development%20of%20user-friendly%20and%20efficient%0Ainterfaces%20for%20querying%20domain-specific%20materials%20science%20knowledge%20graphs%2C%0Athereby%20accelerating%20the%20discovery%20of%20novel%20materials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.11361v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Graph%2520Question%2520Answering%2520for%2520Materials%2520Science%2520%2528KGQA4MAT%2529%253A%250A%2520%2520Developing%2520Natural%2520Language%2520Interface%2520for%2520Metal-Organic%2520Frameworks%2520Knowledge%250A%2520%2520Graph%2520%2528MOF-KG%2529%2520Using%2520LLM%26entry.906535625%3DYuan%2520An%2520and%2520Jane%2520Greenberg%2520and%2520Alex%2520Kalinowski%2520and%2520Xintong%2520Zhao%2520and%2520Xiaohua%2520Hu%2520and%2520Fernando%2520J.%2520Uribe-Romo%2520and%2520Kyle%2520Langlois%2520and%2520Jacob%2520Furst%2520and%2520Diego%2520A.%2520G%25C3%25B3mez-Gualdr%25C3%25B3n%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520comprehensive%2520benchmark%2520dataset%2520for%2520Knowledge%2520Graph%2520Question%250AAnswering%2520in%2520Materials%2520Science%2520%2528KGQA4MAT%2529%252C%2520with%2520a%2520focus%2520on%2520metal-organic%250Aframeworks%2520%2528MOFs%2529.%2520A%2520knowledge%2520graph%2520for%2520metal-organic%2520frameworks%2520%2528MOF-KG%2529%2520has%250Abeen%2520constructed%2520by%2520integrating%2520structured%2520databases%2520and%2520knowledge%2520extracted%250Afrom%2520the%2520literature.%2520To%2520enhance%2520MOF-KG%2520accessibility%2520for%2520domain%2520experts%252C%2520we%2520aim%250Ato%2520develop%2520a%2520natural%2520language%2520interface%2520for%2520querying%2520the%2520knowledge%2520graph.%2520We%250Ahave%2520developed%2520a%2520benchmark%2520comprised%2520of%2520161%2520complex%2520questions%2520involving%250Acomparison%252C%2520aggregation%252C%2520and%2520complicated%2520graph%2520structures.%2520Each%2520question%2520is%250Arephrased%2520in%2520three%2520additional%2520variations%252C%2520resulting%2520in%2520644%2520questions%2520and%2520161%2520KG%250Aqueries.%2520To%2520evaluate%2520the%2520benchmark%252C%2520we%2520have%2520developed%2520a%2520systematic%2520approach%2520for%250Autilizing%2520the%2520LLM%252C%2520ChatGPT%252C%2520to%2520translate%2520natural%2520language%2520questions%2520into%2520formal%250AKG%2520queries.%2520We%2520also%2520apply%2520the%2520approach%2520to%2520the%2520well-known%2520QALD-9%2520dataset%252C%250Ademonstrating%2520ChatGPT%2527s%2520potential%2520in%2520addressing%2520KGQA%2520issues%2520for%2520different%250Aplatforms%2520and%2520query%2520languages.%2520The%2520benchmark%2520and%2520the%2520proposed%2520approach%2520aim%2520to%250Astimulate%2520further%2520research%2520and%2520development%2520of%2520user-friendly%2520and%2520efficient%250Ainterfaces%2520for%2520querying%2520domain-specific%2520materials%2520science%2520knowledge%2520graphs%252C%250Athereby%2520accelerating%2520the%2520discovery%2520of%2520novel%2520materials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.11361v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Graph%20Question%20Answering%20for%20Materials%20Science%20%28KGQA4MAT%29%3A%0A%20%20Developing%20Natural%20Language%20Interface%20for%20Metal-Organic%20Frameworks%20Knowledge%0A%20%20Graph%20%28MOF-KG%29%20Using%20LLM&entry.906535625=Yuan%20An%20and%20Jane%20Greenberg%20and%20Alex%20Kalinowski%20and%20Xintong%20Zhao%20and%20Xiaohua%20Hu%20and%20Fernando%20J.%20Uribe-Romo%20and%20Kyle%20Langlois%20and%20Jacob%20Furst%20and%20Diego%20A.%20G%C3%B3mez-Gualdr%C3%B3n&entry.1292438233=%20%20We%20present%20a%20comprehensive%20benchmark%20dataset%20for%20Knowledge%20Graph%20Question%0AAnswering%20in%20Materials%20Science%20%28KGQA4MAT%29%2C%20with%20a%20focus%20on%20metal-organic%0Aframeworks%20%28MOFs%29.%20A%20knowledge%20graph%20for%20metal-organic%20frameworks%20%28MOF-KG%29%20has%0Abeen%20constructed%20by%20integrating%20structured%20databases%20and%20knowledge%20extracted%0Afrom%20the%20literature.%20To%20enhance%20MOF-KG%20accessibility%20for%20domain%20experts%2C%20we%20aim%0Ato%20develop%20a%20natural%20language%20interface%20for%20querying%20the%20knowledge%20graph.%20We%0Ahave%20developed%20a%20benchmark%20comprised%20of%20161%20complex%20questions%20involving%0Acomparison%2C%20aggregation%2C%20and%20complicated%20graph%20structures.%20Each%20question%20is%0Arephrased%20in%20three%20additional%20variations%2C%20resulting%20in%20644%20questions%20and%20161%20KG%0Aqueries.%20To%20evaluate%20the%20benchmark%2C%20we%20have%20developed%20a%20systematic%20approach%20for%0Autilizing%20the%20LLM%2C%20ChatGPT%2C%20to%20translate%20natural%20language%20questions%20into%20formal%0AKG%20queries.%20We%20also%20apply%20the%20approach%20to%20the%20well-known%20QALD-9%20dataset%2C%0Ademonstrating%20ChatGPT%27s%20potential%20in%20addressing%20KGQA%20issues%20for%20different%0Aplatforms%20and%20query%20languages.%20The%20benchmark%20and%20the%20proposed%20approach%20aim%20to%0Astimulate%20further%20research%20and%20development%20of%20user-friendly%20and%20efficient%0Ainterfaces%20for%20querying%20domain-specific%20materials%20science%20knowledge%20graphs%2C%0Athereby%20accelerating%20the%20discovery%20of%20novel%20materials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.11361v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


