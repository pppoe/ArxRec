<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240924.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Go-SLAM: Grounded Object Segmentation and Localization with Gaussian\n  Splatting SLAM", "author": "Phu Pham and Dipam Patel and Damon Conover and Aniket Bera", "abstract": "  We introduce Go-SLAM, a novel framework that utilizes 3D Gaussian Splatting\nSLAM to reconstruct dynamic environments while embedding object-level\ninformation within the scene representations. This framework employs advanced\nobject segmentation techniques, assigning a unique identifier to each Gaussian\nsplat that corresponds to the object it represents. Consequently, our system\nfacilitates open-vocabulary querying, allowing users to locate objects using\nnatural language descriptions. Furthermore, the framework features an optimal\npath generation module that calculates efficient navigation paths for robots\ntoward queried objects, considering obstacles and environmental uncertainties.\nComprehensive evaluations in various scene settings demonstrate the\neffectiveness of our approach in delivering high-fidelity scene\nreconstructions, precise object segmentation, flexible object querying, and\nefficient robot path planning. This work represents an additional step forward\nin bridging the gap between 3D scene reconstruction, semantic object\nunderstanding, and real-time environment interactions.\n", "link": "http://arxiv.org/abs/2409.16944v1", "date": "2024-09-25", "relevancy": 3.3532, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.758}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6615}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5925}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Go-SLAM%3A%20Grounded%20Object%20Segmentation%20and%20Localization%20with%20Gaussian%0A%20%20Splatting%20SLAM&body=Title%3A%20Go-SLAM%3A%20Grounded%20Object%20Segmentation%20and%20Localization%20with%20Gaussian%0A%20%20Splatting%20SLAM%0AAuthor%3A%20Phu%20Pham%20and%20Dipam%20Patel%20and%20Damon%20Conover%20and%20Aniket%20Bera%0AAbstract%3A%20%20%20We%20introduce%20Go-SLAM%2C%20a%20novel%20framework%20that%20utilizes%203D%20Gaussian%20Splatting%0ASLAM%20to%20reconstruct%20dynamic%20environments%20while%20embedding%20object-level%0Ainformation%20within%20the%20scene%20representations.%20This%20framework%20employs%20advanced%0Aobject%20segmentation%20techniques%2C%20assigning%20a%20unique%20identifier%20to%20each%20Gaussian%0Asplat%20that%20corresponds%20to%20the%20object%20it%20represents.%20Consequently%2C%20our%20system%0Afacilitates%20open-vocabulary%20querying%2C%20allowing%20users%20to%20locate%20objects%20using%0Anatural%20language%20descriptions.%20Furthermore%2C%20the%20framework%20features%20an%20optimal%0Apath%20generation%20module%20that%20calculates%20efficient%20navigation%20paths%20for%20robots%0Atoward%20queried%20objects%2C%20considering%20obstacles%20and%20environmental%20uncertainties.%0AComprehensive%20evaluations%20in%20various%20scene%20settings%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20in%20delivering%20high-fidelity%20scene%0Areconstructions%2C%20precise%20object%20segmentation%2C%20flexible%20object%20querying%2C%20and%0Aefficient%20robot%20path%20planning.%20This%20work%20represents%20an%20additional%20step%20forward%0Ain%20bridging%20the%20gap%20between%203D%20scene%20reconstruction%2C%20semantic%20object%0Aunderstanding%2C%20and%20real-time%20environment%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16944v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGo-SLAM%253A%2520Grounded%2520Object%2520Segmentation%2520and%2520Localization%2520with%2520Gaussian%250A%2520%2520Splatting%2520SLAM%26entry.906535625%3DPhu%2520Pham%2520and%2520Dipam%2520Patel%2520and%2520Damon%2520Conover%2520and%2520Aniket%2520Bera%26entry.1292438233%3D%2520%2520We%2520introduce%2520Go-SLAM%252C%2520a%2520novel%2520framework%2520that%2520utilizes%25203D%2520Gaussian%2520Splatting%250ASLAM%2520to%2520reconstruct%2520dynamic%2520environments%2520while%2520embedding%2520object-level%250Ainformation%2520within%2520the%2520scene%2520representations.%2520This%2520framework%2520employs%2520advanced%250Aobject%2520segmentation%2520techniques%252C%2520assigning%2520a%2520unique%2520identifier%2520to%2520each%2520Gaussian%250Asplat%2520that%2520corresponds%2520to%2520the%2520object%2520it%2520represents.%2520Consequently%252C%2520our%2520system%250Afacilitates%2520open-vocabulary%2520querying%252C%2520allowing%2520users%2520to%2520locate%2520objects%2520using%250Anatural%2520language%2520descriptions.%2520Furthermore%252C%2520the%2520framework%2520features%2520an%2520optimal%250Apath%2520generation%2520module%2520that%2520calculates%2520efficient%2520navigation%2520paths%2520for%2520robots%250Atoward%2520queried%2520objects%252C%2520considering%2520obstacles%2520and%2520environmental%2520uncertainties.%250AComprehensive%2520evaluations%2520in%2520various%2520scene%2520settings%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520in%2520delivering%2520high-fidelity%2520scene%250Areconstructions%252C%2520precise%2520object%2520segmentation%252C%2520flexible%2520object%2520querying%252C%2520and%250Aefficient%2520robot%2520path%2520planning.%2520This%2520work%2520represents%2520an%2520additional%2520step%2520forward%250Ain%2520bridging%2520the%2520gap%2520between%25203D%2520scene%2520reconstruction%252C%2520semantic%2520object%250Aunderstanding%252C%2520and%2520real-time%2520environment%2520interactions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16944v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Go-SLAM%3A%20Grounded%20Object%20Segmentation%20and%20Localization%20with%20Gaussian%0A%20%20Splatting%20SLAM&entry.906535625=Phu%20Pham%20and%20Dipam%20Patel%20and%20Damon%20Conover%20and%20Aniket%20Bera&entry.1292438233=%20%20We%20introduce%20Go-SLAM%2C%20a%20novel%20framework%20that%20utilizes%203D%20Gaussian%20Splatting%0ASLAM%20to%20reconstruct%20dynamic%20environments%20while%20embedding%20object-level%0Ainformation%20within%20the%20scene%20representations.%20This%20framework%20employs%20advanced%0Aobject%20segmentation%20techniques%2C%20assigning%20a%20unique%20identifier%20to%20each%20Gaussian%0Asplat%20that%20corresponds%20to%20the%20object%20it%20represents.%20Consequently%2C%20our%20system%0Afacilitates%20open-vocabulary%20querying%2C%20allowing%20users%20to%20locate%20objects%20using%0Anatural%20language%20descriptions.%20Furthermore%2C%20the%20framework%20features%20an%20optimal%0Apath%20generation%20module%20that%20calculates%20efficient%20navigation%20paths%20for%20robots%0Atoward%20queried%20objects%2C%20considering%20obstacles%20and%20environmental%20uncertainties.%0AComprehensive%20evaluations%20in%20various%20scene%20settings%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20in%20delivering%20high-fidelity%20scene%0Areconstructions%2C%20precise%20object%20segmentation%2C%20flexible%20object%20querying%2C%20and%0Aefficient%20robot%20path%20planning.%20This%20work%20represents%20an%20additional%20step%20forward%0Ain%20bridging%20the%20gap%20between%203D%20scene%20reconstruction%2C%20semantic%20object%0Aunderstanding%2C%20and%20real-time%20environment%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16944v1&entry.124074799=Read"},
{"title": "DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D\n  Diffusion", "author": "Yukun Huang and Jianan Wang and Ailing Zeng and Zheng-Jun Zha and Lei Zhang and Xihui Liu", "abstract": "  Leveraging pretrained 2D diffusion models and score distillation sampling\n(SDS), recent methods have shown promising results for text-to-3D avatar\ngeneration. However, generating high-quality 3D avatars capable of expressive\nanimation remains challenging. In this work, we present DreamWaltz-G, a novel\nlearning framework for animatable 3D avatar generation from text. The core of\nthis framework lies in Skeleton-guided Score Distillation and Hybrid 3D\nGaussian Avatar representation. Specifically, the proposed skeleton-guided\nscore distillation integrates skeleton controls from 3D human templates into 2D\ndiffusion models, enhancing the consistency of SDS supervision in terms of view\nand human pose. This facilitates the generation of high-quality avatars,\nmitigating issues such as multiple faces, extra limbs, and blurring. The\nproposed hybrid 3D Gaussian avatar representation builds on the efficient 3D\nGaussians, combining neural implicit fields and parameterized 3D meshes to\nenable real-time rendering, stable SDS optimization, and expressive animation.\nExtensive experiments demonstrate that DreamWaltz-G is highly effective in\ngenerating and animating 3D avatars, outperforming existing methods in both\nvisual quality and animation expressiveness. Our framework further supports\ndiverse applications, including human video reenactment and multi-subject scene\ncomposition.\n", "link": "http://arxiv.org/abs/2409.17145v1", "date": "2024-09-25", "relevancy": 3.3531, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.694}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.694}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamWaltz-G%3A%20Expressive%203D%20Gaussian%20Avatars%20from%20Skeleton-Guided%202D%0A%20%20Diffusion&body=Title%3A%20DreamWaltz-G%3A%20Expressive%203D%20Gaussian%20Avatars%20from%20Skeleton-Guided%202D%0A%20%20Diffusion%0AAuthor%3A%20Yukun%20Huang%20and%20Jianan%20Wang%20and%20Ailing%20Zeng%20and%20Zheng-Jun%20Zha%20and%20Lei%20Zhang%20and%20Xihui%20Liu%0AAbstract%3A%20%20%20Leveraging%20pretrained%202D%20diffusion%20models%20and%20score%20distillation%20sampling%0A%28SDS%29%2C%20recent%20methods%20have%20shown%20promising%20results%20for%20text-to-3D%20avatar%0Ageneration.%20However%2C%20generating%20high-quality%203D%20avatars%20capable%20of%20expressive%0Aanimation%20remains%20challenging.%20In%20this%20work%2C%20we%20present%20DreamWaltz-G%2C%20a%20novel%0Alearning%20framework%20for%20animatable%203D%20avatar%20generation%20from%20text.%20The%20core%20of%0Athis%20framework%20lies%20in%20Skeleton-guided%20Score%20Distillation%20and%20Hybrid%203D%0AGaussian%20Avatar%20representation.%20Specifically%2C%20the%20proposed%20skeleton-guided%0Ascore%20distillation%20integrates%20skeleton%20controls%20from%203D%20human%20templates%20into%202D%0Adiffusion%20models%2C%20enhancing%20the%20consistency%20of%20SDS%20supervision%20in%20terms%20of%20view%0Aand%20human%20pose.%20This%20facilitates%20the%20generation%20of%20high-quality%20avatars%2C%0Amitigating%20issues%20such%20as%20multiple%20faces%2C%20extra%20limbs%2C%20and%20blurring.%20The%0Aproposed%20hybrid%203D%20Gaussian%20avatar%20representation%20builds%20on%20the%20efficient%203D%0AGaussians%2C%20combining%20neural%20implicit%20fields%20and%20parameterized%203D%20meshes%20to%0Aenable%20real-time%20rendering%2C%20stable%20SDS%20optimization%2C%20and%20expressive%20animation.%0AExtensive%20experiments%20demonstrate%20that%20DreamWaltz-G%20is%20highly%20effective%20in%0Agenerating%20and%20animating%203D%20avatars%2C%20outperforming%20existing%20methods%20in%20both%0Avisual%20quality%20and%20animation%20expressiveness.%20Our%20framework%20further%20supports%0Adiverse%20applications%2C%20including%20human%20video%20reenactment%20and%20multi-subject%20scene%0Acomposition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17145v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamWaltz-G%253A%2520Expressive%25203D%2520Gaussian%2520Avatars%2520from%2520Skeleton-Guided%25202D%250A%2520%2520Diffusion%26entry.906535625%3DYukun%2520Huang%2520and%2520Jianan%2520Wang%2520and%2520Ailing%2520Zeng%2520and%2520Zheng-Jun%2520Zha%2520and%2520Lei%2520Zhang%2520and%2520Xihui%2520Liu%26entry.1292438233%3D%2520%2520Leveraging%2520pretrained%25202D%2520diffusion%2520models%2520and%2520score%2520distillation%2520sampling%250A%2528SDS%2529%252C%2520recent%2520methods%2520have%2520shown%2520promising%2520results%2520for%2520text-to-3D%2520avatar%250Ageneration.%2520However%252C%2520generating%2520high-quality%25203D%2520avatars%2520capable%2520of%2520expressive%250Aanimation%2520remains%2520challenging.%2520In%2520this%2520work%252C%2520we%2520present%2520DreamWaltz-G%252C%2520a%2520novel%250Alearning%2520framework%2520for%2520animatable%25203D%2520avatar%2520generation%2520from%2520text.%2520The%2520core%2520of%250Athis%2520framework%2520lies%2520in%2520Skeleton-guided%2520Score%2520Distillation%2520and%2520Hybrid%25203D%250AGaussian%2520Avatar%2520representation.%2520Specifically%252C%2520the%2520proposed%2520skeleton-guided%250Ascore%2520distillation%2520integrates%2520skeleton%2520controls%2520from%25203D%2520human%2520templates%2520into%25202D%250Adiffusion%2520models%252C%2520enhancing%2520the%2520consistency%2520of%2520SDS%2520supervision%2520in%2520terms%2520of%2520view%250Aand%2520human%2520pose.%2520This%2520facilitates%2520the%2520generation%2520of%2520high-quality%2520avatars%252C%250Amitigating%2520issues%2520such%2520as%2520multiple%2520faces%252C%2520extra%2520limbs%252C%2520and%2520blurring.%2520The%250Aproposed%2520hybrid%25203D%2520Gaussian%2520avatar%2520representation%2520builds%2520on%2520the%2520efficient%25203D%250AGaussians%252C%2520combining%2520neural%2520implicit%2520fields%2520and%2520parameterized%25203D%2520meshes%2520to%250Aenable%2520real-time%2520rendering%252C%2520stable%2520SDS%2520optimization%252C%2520and%2520expressive%2520animation.%250AExtensive%2520experiments%2520demonstrate%2520that%2520DreamWaltz-G%2520is%2520highly%2520effective%2520in%250Agenerating%2520and%2520animating%25203D%2520avatars%252C%2520outperforming%2520existing%2520methods%2520in%2520both%250Avisual%2520quality%2520and%2520animation%2520expressiveness.%2520Our%2520framework%2520further%2520supports%250Adiverse%2520applications%252C%2520including%2520human%2520video%2520reenactment%2520and%2520multi-subject%2520scene%250Acomposition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17145v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamWaltz-G%3A%20Expressive%203D%20Gaussian%20Avatars%20from%20Skeleton-Guided%202D%0A%20%20Diffusion&entry.906535625=Yukun%20Huang%20and%20Jianan%20Wang%20and%20Ailing%20Zeng%20and%20Zheng-Jun%20Zha%20and%20Lei%20Zhang%20and%20Xihui%20Liu&entry.1292438233=%20%20Leveraging%20pretrained%202D%20diffusion%20models%20and%20score%20distillation%20sampling%0A%28SDS%29%2C%20recent%20methods%20have%20shown%20promising%20results%20for%20text-to-3D%20avatar%0Ageneration.%20However%2C%20generating%20high-quality%203D%20avatars%20capable%20of%20expressive%0Aanimation%20remains%20challenging.%20In%20this%20work%2C%20we%20present%20DreamWaltz-G%2C%20a%20novel%0Alearning%20framework%20for%20animatable%203D%20avatar%20generation%20from%20text.%20The%20core%20of%0Athis%20framework%20lies%20in%20Skeleton-guided%20Score%20Distillation%20and%20Hybrid%203D%0AGaussian%20Avatar%20representation.%20Specifically%2C%20the%20proposed%20skeleton-guided%0Ascore%20distillation%20integrates%20skeleton%20controls%20from%203D%20human%20templates%20into%202D%0Adiffusion%20models%2C%20enhancing%20the%20consistency%20of%20SDS%20supervision%20in%20terms%20of%20view%0Aand%20human%20pose.%20This%20facilitates%20the%20generation%20of%20high-quality%20avatars%2C%0Amitigating%20issues%20such%20as%20multiple%20faces%2C%20extra%20limbs%2C%20and%20blurring.%20The%0Aproposed%20hybrid%203D%20Gaussian%20avatar%20representation%20builds%20on%20the%20efficient%203D%0AGaussians%2C%20combining%20neural%20implicit%20fields%20and%20parameterized%203D%20meshes%20to%0Aenable%20real-time%20rendering%2C%20stable%20SDS%20optimization%2C%20and%20expressive%20animation.%0AExtensive%20experiments%20demonstrate%20that%20DreamWaltz-G%20is%20highly%20effective%20in%0Agenerating%20and%20animating%203D%20avatars%2C%20outperforming%20existing%20methods%20in%20both%0Avisual%20quality%20and%20animation%20expressiveness.%20Our%20framework%20further%20supports%0Adiverse%20applications%2C%20including%20human%20video%20reenactment%20and%20multi-subject%20scene%0Acomposition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17145v1&entry.124074799=Read"},
{"title": "Single Image, Any Face: Generalisable 3D Face Generation", "author": "Wenqing Wang and Haosen Yang and Josef Kittler and Xiatian Zhu", "abstract": "  The creation of 3D human face avatars from a single unconstrained image is a\nfundamental task that underlies numerous real-world vision and graphics\napplications. Despite the significant progress made in generative models,\nexisting methods are either less suited in design for human faces or fail to\ngeneralise from the restrictive training domain to unconstrained facial images.\nTo address these limitations, we propose a novel model, Gen3D-Face, which\ngenerates 3D human faces with unconstrained single image input within a\nmulti-view consistent diffusion framework. Given a specific input image, our\nmodel first produces multi-view images, followed by neural surface\nconstruction. To incorporate face geometry information in a generalisable\nmanner, we utilise input-conditioned mesh estimation instead of ground-truth\nmesh along with synthetic multi-view training data. Importantly, we introduce a\nmulti-view joint generation scheme to enhance appearance consistency among\ndifferent views. To the best of our knowledge, this is the first attempt and\nbenchmark for creating photorealistic 3D human face avatars from single images\nfor generic human subject across domains. Extensive experiments demonstrate the\nsuperiority of our method over previous alternatives for out-of-domain singe\nimage 3D face generation and top competition for in-domain setting.\n", "link": "http://arxiv.org/abs/2409.16990v1", "date": "2024-09-25", "relevancy": 3.3042, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6629}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6629}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single%20Image%2C%20Any%20Face%3A%20Generalisable%203D%20Face%20Generation&body=Title%3A%20Single%20Image%2C%20Any%20Face%3A%20Generalisable%203D%20Face%20Generation%0AAuthor%3A%20Wenqing%20Wang%20and%20Haosen%20Yang%20and%20Josef%20Kittler%20and%20Xiatian%20Zhu%0AAbstract%3A%20%20%20The%20creation%20of%203D%20human%20face%20avatars%20from%20a%20single%20unconstrained%20image%20is%20a%0Afundamental%20task%20that%20underlies%20numerous%20real-world%20vision%20and%20graphics%0Aapplications.%20Despite%20the%20significant%20progress%20made%20in%20generative%20models%2C%0Aexisting%20methods%20are%20either%20less%20suited%20in%20design%20for%20human%20faces%20or%20fail%20to%0Ageneralise%20from%20the%20restrictive%20training%20domain%20to%20unconstrained%20facial%20images.%0ATo%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20model%2C%20Gen3D-Face%2C%20which%0Agenerates%203D%20human%20faces%20with%20unconstrained%20single%20image%20input%20within%20a%0Amulti-view%20consistent%20diffusion%20framework.%20Given%20a%20specific%20input%20image%2C%20our%0Amodel%20first%20produces%20multi-view%20images%2C%20followed%20by%20neural%20surface%0Aconstruction.%20To%20incorporate%20face%20geometry%20information%20in%20a%20generalisable%0Amanner%2C%20we%20utilise%20input-conditioned%20mesh%20estimation%20instead%20of%20ground-truth%0Amesh%20along%20with%20synthetic%20multi-view%20training%20data.%20Importantly%2C%20we%20introduce%20a%0Amulti-view%20joint%20generation%20scheme%20to%20enhance%20appearance%20consistency%20among%0Adifferent%20views.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20attempt%20and%0Abenchmark%20for%20creating%20photorealistic%203D%20human%20face%20avatars%20from%20single%20images%0Afor%20generic%20human%20subject%20across%20domains.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20method%20over%20previous%20alternatives%20for%20out-of-domain%20singe%0Aimage%203D%20face%20generation%20and%20top%20competition%20for%20in-domain%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16990v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle%2520Image%252C%2520Any%2520Face%253A%2520Generalisable%25203D%2520Face%2520Generation%26entry.906535625%3DWenqing%2520Wang%2520and%2520Haosen%2520Yang%2520and%2520Josef%2520Kittler%2520and%2520Xiatian%2520Zhu%26entry.1292438233%3D%2520%2520The%2520creation%2520of%25203D%2520human%2520face%2520avatars%2520from%2520a%2520single%2520unconstrained%2520image%2520is%2520a%250Afundamental%2520task%2520that%2520underlies%2520numerous%2520real-world%2520vision%2520and%2520graphics%250Aapplications.%2520Despite%2520the%2520significant%2520progress%2520made%2520in%2520generative%2520models%252C%250Aexisting%2520methods%2520are%2520either%2520less%2520suited%2520in%2520design%2520for%2520human%2520faces%2520or%2520fail%2520to%250Ageneralise%2520from%2520the%2520restrictive%2520training%2520domain%2520to%2520unconstrained%2520facial%2520images.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520model%252C%2520Gen3D-Face%252C%2520which%250Agenerates%25203D%2520human%2520faces%2520with%2520unconstrained%2520single%2520image%2520input%2520within%2520a%250Amulti-view%2520consistent%2520diffusion%2520framework.%2520Given%2520a%2520specific%2520input%2520image%252C%2520our%250Amodel%2520first%2520produces%2520multi-view%2520images%252C%2520followed%2520by%2520neural%2520surface%250Aconstruction.%2520To%2520incorporate%2520face%2520geometry%2520information%2520in%2520a%2520generalisable%250Amanner%252C%2520we%2520utilise%2520input-conditioned%2520mesh%2520estimation%2520instead%2520of%2520ground-truth%250Amesh%2520along%2520with%2520synthetic%2520multi-view%2520training%2520data.%2520Importantly%252C%2520we%2520introduce%2520a%250Amulti-view%2520joint%2520generation%2520scheme%2520to%2520enhance%2520appearance%2520consistency%2520among%250Adifferent%2520views.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520attempt%2520and%250Abenchmark%2520for%2520creating%2520photorealistic%25203D%2520human%2520face%2520avatars%2520from%2520single%2520images%250Afor%2520generic%2520human%2520subject%2520across%2520domains.%2520Extensive%2520experiments%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520method%2520over%2520previous%2520alternatives%2520for%2520out-of-domain%2520singe%250Aimage%25203D%2520face%2520generation%2520and%2520top%2520competition%2520for%2520in-domain%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16990v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single%20Image%2C%20Any%20Face%3A%20Generalisable%203D%20Face%20Generation&entry.906535625=Wenqing%20Wang%20and%20Haosen%20Yang%20and%20Josef%20Kittler%20and%20Xiatian%20Zhu&entry.1292438233=%20%20The%20creation%20of%203D%20human%20face%20avatars%20from%20a%20single%20unconstrained%20image%20is%20a%0Afundamental%20task%20that%20underlies%20numerous%20real-world%20vision%20and%20graphics%0Aapplications.%20Despite%20the%20significant%20progress%20made%20in%20generative%20models%2C%0Aexisting%20methods%20are%20either%20less%20suited%20in%20design%20for%20human%20faces%20or%20fail%20to%0Ageneralise%20from%20the%20restrictive%20training%20domain%20to%20unconstrained%20facial%20images.%0ATo%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20model%2C%20Gen3D-Face%2C%20which%0Agenerates%203D%20human%20faces%20with%20unconstrained%20single%20image%20input%20within%20a%0Amulti-view%20consistent%20diffusion%20framework.%20Given%20a%20specific%20input%20image%2C%20our%0Amodel%20first%20produces%20multi-view%20images%2C%20followed%20by%20neural%20surface%0Aconstruction.%20To%20incorporate%20face%20geometry%20information%20in%20a%20generalisable%0Amanner%2C%20we%20utilise%20input-conditioned%20mesh%20estimation%20instead%20of%20ground-truth%0Amesh%20along%20with%20synthetic%20multi-view%20training%20data.%20Importantly%2C%20we%20introduce%20a%0Amulti-view%20joint%20generation%20scheme%20to%20enhance%20appearance%20consistency%20among%0Adifferent%20views.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20attempt%20and%0Abenchmark%20for%20creating%20photorealistic%203D%20human%20face%20avatars%20from%20single%20images%0Afor%20generic%20human%20subject%20across%20domains.%20Extensive%20experiments%20demonstrate%20the%0Asuperiority%20of%20our%20method%20over%20previous%20alternatives%20for%20out-of-domain%20singe%0Aimage%203D%20face%20generation%20and%20top%20competition%20for%20in-domain%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16990v1&entry.124074799=Read"},
{"title": "Can Vision Language Models Learn from Visual Demonstrations of Ambiguous\n  Spatial Reasoning?", "author": "Bowen Zhao and Leo Parker Dirac and Paulina Varshavskaya", "abstract": "  Large vision-language models (VLMs) have become state-of-the-art for many\ncomputer vision tasks, with in-context learning (ICL) as a popular adaptation\nstrategy for new ones. But can VLMs learn novel concepts purely from visual\ndemonstrations, or are they limited to adapting to the output format of ICL\nexamples? We propose a new benchmark we call Spatial Visual Ambiguity Tasks\n(SVAT) that challenges state-of-the-art VLMs to learn new visuospatial tasks\nin-context. We find that VLMs fail to do this zero-shot, and sometimes continue\nto fail after finetuning. However, adding simpler data to the training by\ncurriculum learning leads to improved ICL performance.\n", "link": "http://arxiv.org/abs/2409.17080v1", "date": "2024-09-25", "relevancy": 3.0514, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6241}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6241}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Vision%20Language%20Models%20Learn%20from%20Visual%20Demonstrations%20of%20Ambiguous%0A%20%20Spatial%20Reasoning%3F&body=Title%3A%20Can%20Vision%20Language%20Models%20Learn%20from%20Visual%20Demonstrations%20of%20Ambiguous%0A%20%20Spatial%20Reasoning%3F%0AAuthor%3A%20Bowen%20Zhao%20and%20Leo%20Parker%20Dirac%20and%20Paulina%20Varshavskaya%0AAbstract%3A%20%20%20Large%20vision-language%20models%20%28VLMs%29%20have%20become%20state-of-the-art%20for%20many%0Acomputer%20vision%20tasks%2C%20with%20in-context%20learning%20%28ICL%29%20as%20a%20popular%20adaptation%0Astrategy%20for%20new%20ones.%20But%20can%20VLMs%20learn%20novel%20concepts%20purely%20from%20visual%0Ademonstrations%2C%20or%20are%20they%20limited%20to%20adapting%20to%20the%20output%20format%20of%20ICL%0Aexamples%3F%20We%20propose%20a%20new%20benchmark%20we%20call%20Spatial%20Visual%20Ambiguity%20Tasks%0A%28SVAT%29%20that%20challenges%20state-of-the-art%20VLMs%20to%20learn%20new%20visuospatial%20tasks%0Ain-context.%20We%20find%20that%20VLMs%20fail%20to%20do%20this%20zero-shot%2C%20and%20sometimes%20continue%0Ato%20fail%20after%20finetuning.%20However%2C%20adding%20simpler%20data%20to%20the%20training%20by%0Acurriculum%20learning%20leads%20to%20improved%20ICL%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17080v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Vision%2520Language%2520Models%2520Learn%2520from%2520Visual%2520Demonstrations%2520of%2520Ambiguous%250A%2520%2520Spatial%2520Reasoning%253F%26entry.906535625%3DBowen%2520Zhao%2520and%2520Leo%2520Parker%2520Dirac%2520and%2520Paulina%2520Varshavskaya%26entry.1292438233%3D%2520%2520Large%2520vision-language%2520models%2520%2528VLMs%2529%2520have%2520become%2520state-of-the-art%2520for%2520many%250Acomputer%2520vision%2520tasks%252C%2520with%2520in-context%2520learning%2520%2528ICL%2529%2520as%2520a%2520popular%2520adaptation%250Astrategy%2520for%2520new%2520ones.%2520But%2520can%2520VLMs%2520learn%2520novel%2520concepts%2520purely%2520from%2520visual%250Ademonstrations%252C%2520or%2520are%2520they%2520limited%2520to%2520adapting%2520to%2520the%2520output%2520format%2520of%2520ICL%250Aexamples%253F%2520We%2520propose%2520a%2520new%2520benchmark%2520we%2520call%2520Spatial%2520Visual%2520Ambiguity%2520Tasks%250A%2528SVAT%2529%2520that%2520challenges%2520state-of-the-art%2520VLMs%2520to%2520learn%2520new%2520visuospatial%2520tasks%250Ain-context.%2520We%2520find%2520that%2520VLMs%2520fail%2520to%2520do%2520this%2520zero-shot%252C%2520and%2520sometimes%2520continue%250Ato%2520fail%2520after%2520finetuning.%2520However%252C%2520adding%2520simpler%2520data%2520to%2520the%2520training%2520by%250Acurriculum%2520learning%2520leads%2520to%2520improved%2520ICL%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17080v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Vision%20Language%20Models%20Learn%20from%20Visual%20Demonstrations%20of%20Ambiguous%0A%20%20Spatial%20Reasoning%3F&entry.906535625=Bowen%20Zhao%20and%20Leo%20Parker%20Dirac%20and%20Paulina%20Varshavskaya&entry.1292438233=%20%20Large%20vision-language%20models%20%28VLMs%29%20have%20become%20state-of-the-art%20for%20many%0Acomputer%20vision%20tasks%2C%20with%20in-context%20learning%20%28ICL%29%20as%20a%20popular%20adaptation%0Astrategy%20for%20new%20ones.%20But%20can%20VLMs%20learn%20novel%20concepts%20purely%20from%20visual%0Ademonstrations%2C%20or%20are%20they%20limited%20to%20adapting%20to%20the%20output%20format%20of%20ICL%0Aexamples%3F%20We%20propose%20a%20new%20benchmark%20we%20call%20Spatial%20Visual%20Ambiguity%20Tasks%0A%28SVAT%29%20that%20challenges%20state-of-the-art%20VLMs%20to%20learn%20new%20visuospatial%20tasks%0Ain-context.%20We%20find%20that%20VLMs%20fail%20to%20do%20this%20zero-shot%2C%20and%20sometimes%20continue%0Ato%20fail%20after%20finetuning.%20However%2C%20adding%20simpler%20data%20to%20the%20training%20by%0Acurriculum%20learning%20leads%20to%20improved%20ICL%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17080v1&entry.124074799=Read"},
{"title": "Text2CAD: Generating Sequential CAD Models from Beginner-to-Expert Level\n  Text Prompts", "author": "Mohammad Sadil Khan and Sankalp Sinha and Talha Uddin Sheikh and Didier Stricker and Sk Aziz Ali and Muhammad Zeshan Afzal", "abstract": "  Prototyping complex computer-aided design (CAD) models in modern softwares\ncan be very time-consuming. This is due to the lack of intelligent systems that\ncan quickly generate simpler intermediate parts. We propose Text2CAD, the first\nAI framework for generating text-to-parametric CAD models using\ndesigner-friendly instructions for all skill levels. Furthermore, we introduce\na data annotation pipeline for generating text prompts based on natural\nlanguage instructions for the DeepCAD dataset using Mistral and LLaVA-NeXT. The\ndataset contains $\\sim170$K models and $\\sim660$K text annotations, from\nabstract CAD descriptions (e.g., generate two concentric cylinders) to detailed\nspecifications (e.g., draw two circles with center $(x,y)$ and radius $r_{1}$,\n$r_{2}$, and extrude along the normal by $d$...). Within the Text2CAD\nframework, we propose an end-to-end transformer-based auto-regressive network\nto generate parametric CAD models from input texts. We evaluate the performance\nof our model through a mixture of metrics, including visual quality, parametric\nprecision, and geometrical accuracy. Our proposed framework shows great\npotential in AI-aided design applications. Our source code and annotations will\nbe publicly available.\n", "link": "http://arxiv.org/abs/2409.17106v1", "date": "2024-09-25", "relevancy": 2.9683, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6548}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5631}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text2CAD%3A%20Generating%20Sequential%20CAD%20Models%20from%20Beginner-to-Expert%20Level%0A%20%20Text%20Prompts&body=Title%3A%20Text2CAD%3A%20Generating%20Sequential%20CAD%20Models%20from%20Beginner-to-Expert%20Level%0A%20%20Text%20Prompts%0AAuthor%3A%20Mohammad%20Sadil%20Khan%20and%20Sankalp%20Sinha%20and%20Talha%20Uddin%20Sheikh%20and%20Didier%20Stricker%20and%20Sk%20Aziz%20Ali%20and%20Muhammad%20Zeshan%20Afzal%0AAbstract%3A%20%20%20Prototyping%20complex%20computer-aided%20design%20%28CAD%29%20models%20in%20modern%20softwares%0Acan%20be%20very%20time-consuming.%20This%20is%20due%20to%20the%20lack%20of%20intelligent%20systems%20that%0Acan%20quickly%20generate%20simpler%20intermediate%20parts.%20We%20propose%20Text2CAD%2C%20the%20first%0AAI%20framework%20for%20generating%20text-to-parametric%20CAD%20models%20using%0Adesigner-friendly%20instructions%20for%20all%20skill%20levels.%20Furthermore%2C%20we%20introduce%0Aa%20data%20annotation%20pipeline%20for%20generating%20text%20prompts%20based%20on%20natural%0Alanguage%20instructions%20for%20the%20DeepCAD%20dataset%20using%20Mistral%20and%20LLaVA-NeXT.%20The%0Adataset%20contains%20%24%5Csim170%24K%20models%20and%20%24%5Csim660%24K%20text%20annotations%2C%20from%0Aabstract%20CAD%20descriptions%20%28e.g.%2C%20generate%20two%20concentric%20cylinders%29%20to%20detailed%0Aspecifications%20%28e.g.%2C%20draw%20two%20circles%20with%20center%20%24%28x%2Cy%29%24%20and%20radius%20%24r_%7B1%7D%24%2C%0A%24r_%7B2%7D%24%2C%20and%20extrude%20along%20the%20normal%20by%20%24d%24...%29.%20Within%20the%20Text2CAD%0Aframework%2C%20we%20propose%20an%20end-to-end%20transformer-based%20auto-regressive%20network%0Ato%20generate%20parametric%20CAD%20models%20from%20input%20texts.%20We%20evaluate%20the%20performance%0Aof%20our%20model%20through%20a%20mixture%20of%20metrics%2C%20including%20visual%20quality%2C%20parametric%0Aprecision%2C%20and%20geometrical%20accuracy.%20Our%20proposed%20framework%20shows%20great%0Apotential%20in%20AI-aided%20design%20applications.%20Our%20source%20code%20and%20annotations%20will%0Abe%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText2CAD%253A%2520Generating%2520Sequential%2520CAD%2520Models%2520from%2520Beginner-to-Expert%2520Level%250A%2520%2520Text%2520Prompts%26entry.906535625%3DMohammad%2520Sadil%2520Khan%2520and%2520Sankalp%2520Sinha%2520and%2520Talha%2520Uddin%2520Sheikh%2520and%2520Didier%2520Stricker%2520and%2520Sk%2520Aziz%2520Ali%2520and%2520Muhammad%2520Zeshan%2520Afzal%26entry.1292438233%3D%2520%2520Prototyping%2520complex%2520computer-aided%2520design%2520%2528CAD%2529%2520models%2520in%2520modern%2520softwares%250Acan%2520be%2520very%2520time-consuming.%2520This%2520is%2520due%2520to%2520the%2520lack%2520of%2520intelligent%2520systems%2520that%250Acan%2520quickly%2520generate%2520simpler%2520intermediate%2520parts.%2520We%2520propose%2520Text2CAD%252C%2520the%2520first%250AAI%2520framework%2520for%2520generating%2520text-to-parametric%2520CAD%2520models%2520using%250Adesigner-friendly%2520instructions%2520for%2520all%2520skill%2520levels.%2520Furthermore%252C%2520we%2520introduce%250Aa%2520data%2520annotation%2520pipeline%2520for%2520generating%2520text%2520prompts%2520based%2520on%2520natural%250Alanguage%2520instructions%2520for%2520the%2520DeepCAD%2520dataset%2520using%2520Mistral%2520and%2520LLaVA-NeXT.%2520The%250Adataset%2520contains%2520%2524%255Csim170%2524K%2520models%2520and%2520%2524%255Csim660%2524K%2520text%2520annotations%252C%2520from%250Aabstract%2520CAD%2520descriptions%2520%2528e.g.%252C%2520generate%2520two%2520concentric%2520cylinders%2529%2520to%2520detailed%250Aspecifications%2520%2528e.g.%252C%2520draw%2520two%2520circles%2520with%2520center%2520%2524%2528x%252Cy%2529%2524%2520and%2520radius%2520%2524r_%257B1%257D%2524%252C%250A%2524r_%257B2%257D%2524%252C%2520and%2520extrude%2520along%2520the%2520normal%2520by%2520%2524d%2524...%2529.%2520Within%2520the%2520Text2CAD%250Aframework%252C%2520we%2520propose%2520an%2520end-to-end%2520transformer-based%2520auto-regressive%2520network%250Ato%2520generate%2520parametric%2520CAD%2520models%2520from%2520input%2520texts.%2520We%2520evaluate%2520the%2520performance%250Aof%2520our%2520model%2520through%2520a%2520mixture%2520of%2520metrics%252C%2520including%2520visual%2520quality%252C%2520parametric%250Aprecision%252C%2520and%2520geometrical%2520accuracy.%2520Our%2520proposed%2520framework%2520shows%2520great%250Apotential%2520in%2520AI-aided%2520design%2520applications.%2520Our%2520source%2520code%2520and%2520annotations%2520will%250Abe%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text2CAD%3A%20Generating%20Sequential%20CAD%20Models%20from%20Beginner-to-Expert%20Level%0A%20%20Text%20Prompts&entry.906535625=Mohammad%20Sadil%20Khan%20and%20Sankalp%20Sinha%20and%20Talha%20Uddin%20Sheikh%20and%20Didier%20Stricker%20and%20Sk%20Aziz%20Ali%20and%20Muhammad%20Zeshan%20Afzal&entry.1292438233=%20%20Prototyping%20complex%20computer-aided%20design%20%28CAD%29%20models%20in%20modern%20softwares%0Acan%20be%20very%20time-consuming.%20This%20is%20due%20to%20the%20lack%20of%20intelligent%20systems%20that%0Acan%20quickly%20generate%20simpler%20intermediate%20parts.%20We%20propose%20Text2CAD%2C%20the%20first%0AAI%20framework%20for%20generating%20text-to-parametric%20CAD%20models%20using%0Adesigner-friendly%20instructions%20for%20all%20skill%20levels.%20Furthermore%2C%20we%20introduce%0Aa%20data%20annotation%20pipeline%20for%20generating%20text%20prompts%20based%20on%20natural%0Alanguage%20instructions%20for%20the%20DeepCAD%20dataset%20using%20Mistral%20and%20LLaVA-NeXT.%20The%0Adataset%20contains%20%24%5Csim170%24K%20models%20and%20%24%5Csim660%24K%20text%20annotations%2C%20from%0Aabstract%20CAD%20descriptions%20%28e.g.%2C%20generate%20two%20concentric%20cylinders%29%20to%20detailed%0Aspecifications%20%28e.g.%2C%20draw%20two%20circles%20with%20center%20%24%28x%2Cy%29%24%20and%20radius%20%24r_%7B1%7D%24%2C%0A%24r_%7B2%7D%24%2C%20and%20extrude%20along%20the%20normal%20by%20%24d%24...%29.%20Within%20the%20Text2CAD%0Aframework%2C%20we%20propose%20an%20end-to-end%20transformer-based%20auto-regressive%20network%0Ato%20generate%20parametric%20CAD%20models%20from%20input%20texts.%20We%20evaluate%20the%20performance%0Aof%20our%20model%20through%20a%20mixture%20of%20metrics%2C%20including%20visual%20quality%2C%20parametric%0Aprecision%2C%20and%20geometrical%20accuracy.%20Our%20proposed%20framework%20shows%20great%0Apotential%20in%20AI-aided%20design%20applications.%20Our%20source%20code%20and%20annotations%20will%0Abe%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17106v1&entry.124074799=Read"},
{"title": "Towards Unified 3D Hair Reconstruction from Single-View Portraits", "author": "Yujian Zheng and Yuda Qiu and Leyang Jin and Chongyang Ma and Haibin Huang and Di Zhang and Pengfei Wan and Xiaoguang Han", "abstract": "  Single-view 3D hair reconstruction is challenging, due to the wide range of\nshape variations among diverse hairstyles. Current state-of-the-art methods are\nspecialized in recovering un-braided 3D hairs and often take braided styles as\ntheir failure cases, because of the inherent difficulty to define priors for\ncomplex hairstyles, whether rule-based or data-based. We propose a novel\nstrategy to enable single-view 3D reconstruction for a variety of hair types\nvia a unified pipeline. To achieve this, we first collect a large-scale\nsynthetic multi-view hair dataset SynMvHair with diverse 3D hair in both\nbraided and un-braided styles, and learn two diffusion priors specialized on\nhair. Then we optimize 3D Gaussian-based hair from the priors with two\nspecially designed modules, i.e. view-wise and pixel-wise Gaussian refinement.\nOur experiments demonstrate that reconstructing braided and un-braided 3D hair\nfrom single-view images via a unified approach is possible and our method\nachieves the state-of-the-art performance in recovering complex hairstyles. It\nis worth to mention that our method shows good generalization ability to real\nimages, although it learns hair priors from synthetic data.\n", "link": "http://arxiv.org/abs/2409.16863v1", "date": "2024-09-25", "relevancy": 2.9394, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5978}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5843}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Unified%203D%20Hair%20Reconstruction%20from%20Single-View%20Portraits&body=Title%3A%20Towards%20Unified%203D%20Hair%20Reconstruction%20from%20Single-View%20Portraits%0AAuthor%3A%20Yujian%20Zheng%20and%20Yuda%20Qiu%20and%20Leyang%20Jin%20and%20Chongyang%20Ma%20and%20Haibin%20Huang%20and%20Di%20Zhang%20and%20Pengfei%20Wan%20and%20Xiaoguang%20Han%0AAbstract%3A%20%20%20Single-view%203D%20hair%20reconstruction%20is%20challenging%2C%20due%20to%20the%20wide%20range%20of%0Ashape%20variations%20among%20diverse%20hairstyles.%20Current%20state-of-the-art%20methods%20are%0Aspecialized%20in%20recovering%20un-braided%203D%20hairs%20and%20often%20take%20braided%20styles%20as%0Atheir%20failure%20cases%2C%20because%20of%20the%20inherent%20difficulty%20to%20define%20priors%20for%0Acomplex%20hairstyles%2C%20whether%20rule-based%20or%20data-based.%20We%20propose%20a%20novel%0Astrategy%20to%20enable%20single-view%203D%20reconstruction%20for%20a%20variety%20of%20hair%20types%0Avia%20a%20unified%20pipeline.%20To%20achieve%20this%2C%20we%20first%20collect%20a%20large-scale%0Asynthetic%20multi-view%20hair%20dataset%20SynMvHair%20with%20diverse%203D%20hair%20in%20both%0Abraided%20and%20un-braided%20styles%2C%20and%20learn%20two%20diffusion%20priors%20specialized%20on%0Ahair.%20Then%20we%20optimize%203D%20Gaussian-based%20hair%20from%20the%20priors%20with%20two%0Aspecially%20designed%20modules%2C%20i.e.%20view-wise%20and%20pixel-wise%20Gaussian%20refinement.%0AOur%20experiments%20demonstrate%20that%20reconstructing%20braided%20and%20un-braided%203D%20hair%0Afrom%20single-view%20images%20via%20a%20unified%20approach%20is%20possible%20and%20our%20method%0Aachieves%20the%20state-of-the-art%20performance%20in%20recovering%20complex%20hairstyles.%20It%0Ais%20worth%20to%20mention%20that%20our%20method%20shows%20good%20generalization%20ability%20to%20real%0Aimages%2C%20although%20it%20learns%20hair%20priors%20from%20synthetic%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Unified%25203D%2520Hair%2520Reconstruction%2520from%2520Single-View%2520Portraits%26entry.906535625%3DYujian%2520Zheng%2520and%2520Yuda%2520Qiu%2520and%2520Leyang%2520Jin%2520and%2520Chongyang%2520Ma%2520and%2520Haibin%2520Huang%2520and%2520Di%2520Zhang%2520and%2520Pengfei%2520Wan%2520and%2520Xiaoguang%2520Han%26entry.1292438233%3D%2520%2520Single-view%25203D%2520hair%2520reconstruction%2520is%2520challenging%252C%2520due%2520to%2520the%2520wide%2520range%2520of%250Ashape%2520variations%2520among%2520diverse%2520hairstyles.%2520Current%2520state-of-the-art%2520methods%2520are%250Aspecialized%2520in%2520recovering%2520un-braided%25203D%2520hairs%2520and%2520often%2520take%2520braided%2520styles%2520as%250Atheir%2520failure%2520cases%252C%2520because%2520of%2520the%2520inherent%2520difficulty%2520to%2520define%2520priors%2520for%250Acomplex%2520hairstyles%252C%2520whether%2520rule-based%2520or%2520data-based.%2520We%2520propose%2520a%2520novel%250Astrategy%2520to%2520enable%2520single-view%25203D%2520reconstruction%2520for%2520a%2520variety%2520of%2520hair%2520types%250Avia%2520a%2520unified%2520pipeline.%2520To%2520achieve%2520this%252C%2520we%2520first%2520collect%2520a%2520large-scale%250Asynthetic%2520multi-view%2520hair%2520dataset%2520SynMvHair%2520with%2520diverse%25203D%2520hair%2520in%2520both%250Abraided%2520and%2520un-braided%2520styles%252C%2520and%2520learn%2520two%2520diffusion%2520priors%2520specialized%2520on%250Ahair.%2520Then%2520we%2520optimize%25203D%2520Gaussian-based%2520hair%2520from%2520the%2520priors%2520with%2520two%250Aspecially%2520designed%2520modules%252C%2520i.e.%2520view-wise%2520and%2520pixel-wise%2520Gaussian%2520refinement.%250AOur%2520experiments%2520demonstrate%2520that%2520reconstructing%2520braided%2520and%2520un-braided%25203D%2520hair%250Afrom%2520single-view%2520images%2520via%2520a%2520unified%2520approach%2520is%2520possible%2520and%2520our%2520method%250Aachieves%2520the%2520state-of-the-art%2520performance%2520in%2520recovering%2520complex%2520hairstyles.%2520It%250Ais%2520worth%2520to%2520mention%2520that%2520our%2520method%2520shows%2520good%2520generalization%2520ability%2520to%2520real%250Aimages%252C%2520although%2520it%2520learns%2520hair%2520priors%2520from%2520synthetic%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Unified%203D%20Hair%20Reconstruction%20from%20Single-View%20Portraits&entry.906535625=Yujian%20Zheng%20and%20Yuda%20Qiu%20and%20Leyang%20Jin%20and%20Chongyang%20Ma%20and%20Haibin%20Huang%20and%20Di%20Zhang%20and%20Pengfei%20Wan%20and%20Xiaoguang%20Han&entry.1292438233=%20%20Single-view%203D%20hair%20reconstruction%20is%20challenging%2C%20due%20to%20the%20wide%20range%20of%0Ashape%20variations%20among%20diverse%20hairstyles.%20Current%20state-of-the-art%20methods%20are%0Aspecialized%20in%20recovering%20un-braided%203D%20hairs%20and%20often%20take%20braided%20styles%20as%0Atheir%20failure%20cases%2C%20because%20of%20the%20inherent%20difficulty%20to%20define%20priors%20for%0Acomplex%20hairstyles%2C%20whether%20rule-based%20or%20data-based.%20We%20propose%20a%20novel%0Astrategy%20to%20enable%20single-view%203D%20reconstruction%20for%20a%20variety%20of%20hair%20types%0Avia%20a%20unified%20pipeline.%20To%20achieve%20this%2C%20we%20first%20collect%20a%20large-scale%0Asynthetic%20multi-view%20hair%20dataset%20SynMvHair%20with%20diverse%203D%20hair%20in%20both%0Abraided%20and%20un-braided%20styles%2C%20and%20learn%20two%20diffusion%20priors%20specialized%20on%0Ahair.%20Then%20we%20optimize%203D%20Gaussian-based%20hair%20from%20the%20priors%20with%20two%0Aspecially%20designed%20modules%2C%20i.e.%20view-wise%20and%20pixel-wise%20Gaussian%20refinement.%0AOur%20experiments%20demonstrate%20that%20reconstructing%20braided%20and%20un-braided%203D%20hair%0Afrom%20single-view%20images%20via%20a%20unified%20approach%20is%20possible%20and%20our%20method%0Aachieves%20the%20state-of-the-art%20performance%20in%20recovering%20complex%20hairstyles.%20It%0Ais%20worth%20to%20mention%20that%20our%20method%20shows%20good%20generalization%20ability%20to%20real%0Aimages%2C%20although%20it%20learns%20hair%20priors%20from%20synthetic%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16863v1&entry.124074799=Read"},
{"title": "ControlCity: A Multimodal Diffusion Model Based Approach for Accurate\n  Geospatial Data Generation and Urban Morphology Analysis", "author": "Fangshuo Zhou and Huaxia Li and Rui Hu and Sensen Wu and Hailin Feng and Zhenhong Du and Liuchang Xu", "abstract": "  Volunteer Geographic Information (VGI), with its rich variety, large volume,\nrapid updates, and diverse sources, has become a critical source of geospatial\ndata. However, VGI data from platforms like OSM exhibit significant quality\nheterogeneity across different data types, particularly with urban building\ndata. To address this, we propose a multi-source geographic data transformation\nsolution, utilizing accessible and complete VGI data to assist in generating\nurban building footprint data. We also employ a multimodal data generation\nframework to improve accuracy. First, we introduce a pipeline for constructing\nan 'image-text-metadata-building footprint' dataset, primarily based on road\nnetwork data and supplemented by other multimodal data. We then present\nControlCity, a geographic data transformation method based on a multimodal\ndiffusion model. This method first uses a pre-trained text-to-image model to\nalign text, metadata, and building footprint data. An improved ControlNet\nfurther integrates road network and land-use imagery, producing refined\nbuilding footprint data. Experiments across 22 global cities demonstrate that\nControlCity successfully simulates real urban building patterns, achieving\nstate-of-the-art performance. Specifically, our method achieves an average FID\nscore of 50.94, reducing error by 71.01% compared to leading methods, and a\nMIoU score of 0.36, an improvement of 38.46%. Additionally, our model excels in\ntasks like urban morphology transfer, zero-shot city generation, and spatial\ndata completeness assessment. In the zero-shot city task, our method accurately\npredicts and generates similar urban structures, demonstrating strong\ngeneralization. This study confirms the effectiveness of our approach in\ngenerating urban building footprint data and capturing complex city\ncharacteristics.\n", "link": "http://arxiv.org/abs/2409.17049v1", "date": "2024-09-25", "relevancy": 2.8658, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5735}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5735}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ControlCity%3A%20A%20Multimodal%20Diffusion%20Model%20Based%20Approach%20for%20Accurate%0A%20%20Geospatial%20Data%20Generation%20and%20Urban%20Morphology%20Analysis&body=Title%3A%20ControlCity%3A%20A%20Multimodal%20Diffusion%20Model%20Based%20Approach%20for%20Accurate%0A%20%20Geospatial%20Data%20Generation%20and%20Urban%20Morphology%20Analysis%0AAuthor%3A%20Fangshuo%20Zhou%20and%20Huaxia%20Li%20and%20Rui%20Hu%20and%20Sensen%20Wu%20and%20Hailin%20Feng%20and%20Zhenhong%20Du%20and%20Liuchang%20Xu%0AAbstract%3A%20%20%20Volunteer%20Geographic%20Information%20%28VGI%29%2C%20with%20its%20rich%20variety%2C%20large%20volume%2C%0Arapid%20updates%2C%20and%20diverse%20sources%2C%20has%20become%20a%20critical%20source%20of%20geospatial%0Adata.%20However%2C%20VGI%20data%20from%20platforms%20like%20OSM%20exhibit%20significant%20quality%0Aheterogeneity%20across%20different%20data%20types%2C%20particularly%20with%20urban%20building%0Adata.%20To%20address%20this%2C%20we%20propose%20a%20multi-source%20geographic%20data%20transformation%0Asolution%2C%20utilizing%20accessible%20and%20complete%20VGI%20data%20to%20assist%20in%20generating%0Aurban%20building%20footprint%20data.%20We%20also%20employ%20a%20multimodal%20data%20generation%0Aframework%20to%20improve%20accuracy.%20First%2C%20we%20introduce%20a%20pipeline%20for%20constructing%0Aan%20%27image-text-metadata-building%20footprint%27%20dataset%2C%20primarily%20based%20on%20road%0Anetwork%20data%20and%20supplemented%20by%20other%20multimodal%20data.%20We%20then%20present%0AControlCity%2C%20a%20geographic%20data%20transformation%20method%20based%20on%20a%20multimodal%0Adiffusion%20model.%20This%20method%20first%20uses%20a%20pre-trained%20text-to-image%20model%20to%0Aalign%20text%2C%20metadata%2C%20and%20building%20footprint%20data.%20An%20improved%20ControlNet%0Afurther%20integrates%20road%20network%20and%20land-use%20imagery%2C%20producing%20refined%0Abuilding%20footprint%20data.%20Experiments%20across%2022%20global%20cities%20demonstrate%20that%0AControlCity%20successfully%20simulates%20real%20urban%20building%20patterns%2C%20achieving%0Astate-of-the-art%20performance.%20Specifically%2C%20our%20method%20achieves%20an%20average%20FID%0Ascore%20of%2050.94%2C%20reducing%20error%20by%2071.01%25%20compared%20to%20leading%20methods%2C%20and%20a%0AMIoU%20score%20of%200.36%2C%20an%20improvement%20of%2038.46%25.%20Additionally%2C%20our%20model%20excels%20in%0Atasks%20like%20urban%20morphology%20transfer%2C%20zero-shot%20city%20generation%2C%20and%20spatial%0Adata%20completeness%20assessment.%20In%20the%20zero-shot%20city%20task%2C%20our%20method%20accurately%0Apredicts%20and%20generates%20similar%20urban%20structures%2C%20demonstrating%20strong%0Ageneralization.%20This%20study%20confirms%20the%20effectiveness%20of%20our%20approach%20in%0Agenerating%20urban%20building%20footprint%20data%20and%20capturing%20complex%20city%0Acharacteristics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControlCity%253A%2520A%2520Multimodal%2520Diffusion%2520Model%2520Based%2520Approach%2520for%2520Accurate%250A%2520%2520Geospatial%2520Data%2520Generation%2520and%2520Urban%2520Morphology%2520Analysis%26entry.906535625%3DFangshuo%2520Zhou%2520and%2520Huaxia%2520Li%2520and%2520Rui%2520Hu%2520and%2520Sensen%2520Wu%2520and%2520Hailin%2520Feng%2520and%2520Zhenhong%2520Du%2520and%2520Liuchang%2520Xu%26entry.1292438233%3D%2520%2520Volunteer%2520Geographic%2520Information%2520%2528VGI%2529%252C%2520with%2520its%2520rich%2520variety%252C%2520large%2520volume%252C%250Arapid%2520updates%252C%2520and%2520diverse%2520sources%252C%2520has%2520become%2520a%2520critical%2520source%2520of%2520geospatial%250Adata.%2520However%252C%2520VGI%2520data%2520from%2520platforms%2520like%2520OSM%2520exhibit%2520significant%2520quality%250Aheterogeneity%2520across%2520different%2520data%2520types%252C%2520particularly%2520with%2520urban%2520building%250Adata.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520multi-source%2520geographic%2520data%2520transformation%250Asolution%252C%2520utilizing%2520accessible%2520and%2520complete%2520VGI%2520data%2520to%2520assist%2520in%2520generating%250Aurban%2520building%2520footprint%2520data.%2520We%2520also%2520employ%2520a%2520multimodal%2520data%2520generation%250Aframework%2520to%2520improve%2520accuracy.%2520First%252C%2520we%2520introduce%2520a%2520pipeline%2520for%2520constructing%250Aan%2520%2527image-text-metadata-building%2520footprint%2527%2520dataset%252C%2520primarily%2520based%2520on%2520road%250Anetwork%2520data%2520and%2520supplemented%2520by%2520other%2520multimodal%2520data.%2520We%2520then%2520present%250AControlCity%252C%2520a%2520geographic%2520data%2520transformation%2520method%2520based%2520on%2520a%2520multimodal%250Adiffusion%2520model.%2520This%2520method%2520first%2520uses%2520a%2520pre-trained%2520text-to-image%2520model%2520to%250Aalign%2520text%252C%2520metadata%252C%2520and%2520building%2520footprint%2520data.%2520An%2520improved%2520ControlNet%250Afurther%2520integrates%2520road%2520network%2520and%2520land-use%2520imagery%252C%2520producing%2520refined%250Abuilding%2520footprint%2520data.%2520Experiments%2520across%252022%2520global%2520cities%2520demonstrate%2520that%250AControlCity%2520successfully%2520simulates%2520real%2520urban%2520building%2520patterns%252C%2520achieving%250Astate-of-the-art%2520performance.%2520Specifically%252C%2520our%2520method%2520achieves%2520an%2520average%2520FID%250Ascore%2520of%252050.94%252C%2520reducing%2520error%2520by%252071.01%2525%2520compared%2520to%2520leading%2520methods%252C%2520and%2520a%250AMIoU%2520score%2520of%25200.36%252C%2520an%2520improvement%2520of%252038.46%2525.%2520Additionally%252C%2520our%2520model%2520excels%2520in%250Atasks%2520like%2520urban%2520morphology%2520transfer%252C%2520zero-shot%2520city%2520generation%252C%2520and%2520spatial%250Adata%2520completeness%2520assessment.%2520In%2520the%2520zero-shot%2520city%2520task%252C%2520our%2520method%2520accurately%250Apredicts%2520and%2520generates%2520similar%2520urban%2520structures%252C%2520demonstrating%2520strong%250Ageneralization.%2520This%2520study%2520confirms%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%250Agenerating%2520urban%2520building%2520footprint%2520data%2520and%2520capturing%2520complex%2520city%250Acharacteristics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ControlCity%3A%20A%20Multimodal%20Diffusion%20Model%20Based%20Approach%20for%20Accurate%0A%20%20Geospatial%20Data%20Generation%20and%20Urban%20Morphology%20Analysis&entry.906535625=Fangshuo%20Zhou%20and%20Huaxia%20Li%20and%20Rui%20Hu%20and%20Sensen%20Wu%20and%20Hailin%20Feng%20and%20Zhenhong%20Du%20and%20Liuchang%20Xu&entry.1292438233=%20%20Volunteer%20Geographic%20Information%20%28VGI%29%2C%20with%20its%20rich%20variety%2C%20large%20volume%2C%0Arapid%20updates%2C%20and%20diverse%20sources%2C%20has%20become%20a%20critical%20source%20of%20geospatial%0Adata.%20However%2C%20VGI%20data%20from%20platforms%20like%20OSM%20exhibit%20significant%20quality%0Aheterogeneity%20across%20different%20data%20types%2C%20particularly%20with%20urban%20building%0Adata.%20To%20address%20this%2C%20we%20propose%20a%20multi-source%20geographic%20data%20transformation%0Asolution%2C%20utilizing%20accessible%20and%20complete%20VGI%20data%20to%20assist%20in%20generating%0Aurban%20building%20footprint%20data.%20We%20also%20employ%20a%20multimodal%20data%20generation%0Aframework%20to%20improve%20accuracy.%20First%2C%20we%20introduce%20a%20pipeline%20for%20constructing%0Aan%20%27image-text-metadata-building%20footprint%27%20dataset%2C%20primarily%20based%20on%20road%0Anetwork%20data%20and%20supplemented%20by%20other%20multimodal%20data.%20We%20then%20present%0AControlCity%2C%20a%20geographic%20data%20transformation%20method%20based%20on%20a%20multimodal%0Adiffusion%20model.%20This%20method%20first%20uses%20a%20pre-trained%20text-to-image%20model%20to%0Aalign%20text%2C%20metadata%2C%20and%20building%20footprint%20data.%20An%20improved%20ControlNet%0Afurther%20integrates%20road%20network%20and%20land-use%20imagery%2C%20producing%20refined%0Abuilding%20footprint%20data.%20Experiments%20across%2022%20global%20cities%20demonstrate%20that%0AControlCity%20successfully%20simulates%20real%20urban%20building%20patterns%2C%20achieving%0Astate-of-the-art%20performance.%20Specifically%2C%20our%20method%20achieves%20an%20average%20FID%0Ascore%20of%2050.94%2C%20reducing%20error%20by%2071.01%25%20compared%20to%20leading%20methods%2C%20and%20a%0AMIoU%20score%20of%200.36%2C%20an%20improvement%20of%2038.46%25.%20Additionally%2C%20our%20model%20excels%20in%0Atasks%20like%20urban%20morphology%20transfer%2C%20zero-shot%20city%20generation%2C%20and%20spatial%0Adata%20completeness%20assessment.%20In%20the%20zero-shot%20city%20task%2C%20our%20method%20accurately%0Apredicts%20and%20generates%20similar%20urban%20structures%2C%20demonstrating%20strong%0Ageneralization.%20This%20study%20confirms%20the%20effectiveness%20of%20our%20approach%20in%0Agenerating%20urban%20building%20footprint%20data%20and%20capturing%20complex%20city%0Acharacteristics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17049v1&entry.124074799=Read"},
{"title": "An Adaptive Screen-Space Meshing Approach for Normal Integration", "author": "Moritz Heep and Eduard Zell", "abstract": "  Reconstructing surfaces from normals is a key component of photometric\nstereo. This work introduces an adaptive surface triangulation in the image\ndomain and afterwards performs the normal integration on a triangle mesh. Our\nkey insight is that surface curvature can be computed from normals. Based on\nthe curvature, we identify flat areas and aggregate pixels into triangles. The\napproximation quality is controlled by a single user parameter facilitating a\nseamless generation of low- to high-resolution meshes. Compared to pixel grids,\nour triangle meshes adapt locally to surface details and allow for a sparser\nrepresentation. Our new mesh-based formulation of the normal integration\nproblem is strictly derived from discrete differential geometry and leads to\nwell-conditioned linear systems. Results on real and synthetic data show that\n10 to 100 times less vertices are required than pixels. Experiments suggest\nthat this sparsity translates into a sublinear runtime in the number of pixels.\nFor 64 MP normal maps, our meshing-first approach generates and integrates\nmeshes in minutes while pixel-based approaches require hours just for the\nintegration.\n", "link": "http://arxiv.org/abs/2409.16907v1", "date": "2024-09-25", "relevancy": 2.862, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6215}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5594}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Adaptive%20Screen-Space%20Meshing%20Approach%20for%20Normal%20Integration&body=Title%3A%20An%20Adaptive%20Screen-Space%20Meshing%20Approach%20for%20Normal%20Integration%0AAuthor%3A%20Moritz%20Heep%20and%20Eduard%20Zell%0AAbstract%3A%20%20%20Reconstructing%20surfaces%20from%20normals%20is%20a%20key%20component%20of%20photometric%0Astereo.%20This%20work%20introduces%20an%20adaptive%20surface%20triangulation%20in%20the%20image%0Adomain%20and%20afterwards%20performs%20the%20normal%20integration%20on%20a%20triangle%20mesh.%20Our%0Akey%20insight%20is%20that%20surface%20curvature%20can%20be%20computed%20from%20normals.%20Based%20on%0Athe%20curvature%2C%20we%20identify%20flat%20areas%20and%20aggregate%20pixels%20into%20triangles.%20The%0Aapproximation%20quality%20is%20controlled%20by%20a%20single%20user%20parameter%20facilitating%20a%0Aseamless%20generation%20of%20low-%20to%20high-resolution%20meshes.%20Compared%20to%20pixel%20grids%2C%0Aour%20triangle%20meshes%20adapt%20locally%20to%20surface%20details%20and%20allow%20for%20a%20sparser%0Arepresentation.%20Our%20new%20mesh-based%20formulation%20of%20the%20normal%20integration%0Aproblem%20is%20strictly%20derived%20from%20discrete%20differential%20geometry%20and%20leads%20to%0Awell-conditioned%20linear%20systems.%20Results%20on%20real%20and%20synthetic%20data%20show%20that%0A10%20to%20100%20times%20less%20vertices%20are%20required%20than%20pixels.%20Experiments%20suggest%0Athat%20this%20sparsity%20translates%20into%20a%20sublinear%20runtime%20in%20the%20number%20of%20pixels.%0AFor%2064%20MP%20normal%20maps%2C%20our%20meshing-first%20approach%20generates%20and%20integrates%0Ameshes%20in%20minutes%20while%20pixel-based%20approaches%20require%20hours%20just%20for%20the%0Aintegration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16907v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Adaptive%2520Screen-Space%2520Meshing%2520Approach%2520for%2520Normal%2520Integration%26entry.906535625%3DMoritz%2520Heep%2520and%2520Eduard%2520Zell%26entry.1292438233%3D%2520%2520Reconstructing%2520surfaces%2520from%2520normals%2520is%2520a%2520key%2520component%2520of%2520photometric%250Astereo.%2520This%2520work%2520introduces%2520an%2520adaptive%2520surface%2520triangulation%2520in%2520the%2520image%250Adomain%2520and%2520afterwards%2520performs%2520the%2520normal%2520integration%2520on%2520a%2520triangle%2520mesh.%2520Our%250Akey%2520insight%2520is%2520that%2520surface%2520curvature%2520can%2520be%2520computed%2520from%2520normals.%2520Based%2520on%250Athe%2520curvature%252C%2520we%2520identify%2520flat%2520areas%2520and%2520aggregate%2520pixels%2520into%2520triangles.%2520The%250Aapproximation%2520quality%2520is%2520controlled%2520by%2520a%2520single%2520user%2520parameter%2520facilitating%2520a%250Aseamless%2520generation%2520of%2520low-%2520to%2520high-resolution%2520meshes.%2520Compared%2520to%2520pixel%2520grids%252C%250Aour%2520triangle%2520meshes%2520adapt%2520locally%2520to%2520surface%2520details%2520and%2520allow%2520for%2520a%2520sparser%250Arepresentation.%2520Our%2520new%2520mesh-based%2520formulation%2520of%2520the%2520normal%2520integration%250Aproblem%2520is%2520strictly%2520derived%2520from%2520discrete%2520differential%2520geometry%2520and%2520leads%2520to%250Awell-conditioned%2520linear%2520systems.%2520Results%2520on%2520real%2520and%2520synthetic%2520data%2520show%2520that%250A10%2520to%2520100%2520times%2520less%2520vertices%2520are%2520required%2520than%2520pixels.%2520Experiments%2520suggest%250Athat%2520this%2520sparsity%2520translates%2520into%2520a%2520sublinear%2520runtime%2520in%2520the%2520number%2520of%2520pixels.%250AFor%252064%2520MP%2520normal%2520maps%252C%2520our%2520meshing-first%2520approach%2520generates%2520and%2520integrates%250Ameshes%2520in%2520minutes%2520while%2520pixel-based%2520approaches%2520require%2520hours%2520just%2520for%2520the%250Aintegration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16907v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Adaptive%20Screen-Space%20Meshing%20Approach%20for%20Normal%20Integration&entry.906535625=Moritz%20Heep%20and%20Eduard%20Zell&entry.1292438233=%20%20Reconstructing%20surfaces%20from%20normals%20is%20a%20key%20component%20of%20photometric%0Astereo.%20This%20work%20introduces%20an%20adaptive%20surface%20triangulation%20in%20the%20image%0Adomain%20and%20afterwards%20performs%20the%20normal%20integration%20on%20a%20triangle%20mesh.%20Our%0Akey%20insight%20is%20that%20surface%20curvature%20can%20be%20computed%20from%20normals.%20Based%20on%0Athe%20curvature%2C%20we%20identify%20flat%20areas%20and%20aggregate%20pixels%20into%20triangles.%20The%0Aapproximation%20quality%20is%20controlled%20by%20a%20single%20user%20parameter%20facilitating%20a%0Aseamless%20generation%20of%20low-%20to%20high-resolution%20meshes.%20Compared%20to%20pixel%20grids%2C%0Aour%20triangle%20meshes%20adapt%20locally%20to%20surface%20details%20and%20allow%20for%20a%20sparser%0Arepresentation.%20Our%20new%20mesh-based%20formulation%20of%20the%20normal%20integration%0Aproblem%20is%20strictly%20derived%20from%20discrete%20differential%20geometry%20and%20leads%20to%0Awell-conditioned%20linear%20systems.%20Results%20on%20real%20and%20synthetic%20data%20show%20that%0A10%20to%20100%20times%20less%20vertices%20are%20required%20than%20pixels.%20Experiments%20suggest%0Athat%20this%20sparsity%20translates%20into%20a%20sublinear%20runtime%20in%20the%20number%20of%20pixels.%0AFor%2064%20MP%20normal%20maps%2C%20our%20meshing-first%20approach%20generates%20and%20integrates%0Ameshes%20in%20minutes%20while%20pixel-based%20approaches%20require%20hours%20just%20for%20the%0Aintegration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16907v1&entry.124074799=Read"},
{"title": "Topological SLAM in colonoscopies leveraging deep features and\n  topological priors", "author": "Javier Morlana and Juan D. Tard\u00f3s and Jos\u00e9 M. M. Montiel", "abstract": "  We introduce ColonSLAM, a system that combines classical multiple-map metric\nSLAM with deep features and topological priors to create topological maps of\nthe whole colon. The SLAM pipeline by itself is able to create disconnected\nindividual metric submaps representing locations from short video subsections\nof the colon, but is not able to merge covisible submaps due to deformations\nand the limited performance of the SIFT descriptor in the medical domain.\nColonSLAM is guided by topological priors and combines a deep localization\nnetwork trained to distinguish if two images come from the same place or not\nand the soft verification of a transformer-based matching network, being able\nto relate far-in-time submaps during an exploration, grouping them in nodes\nimaging the same colon place, building more complex maps than any other\napproach in the literature. We demonstrate our approach in the Endomapper\ndataset, showing its potential for producing maps of the whole colon in real\nhuman explorations. Code and models are available at:\nhttps://github.com/endomapper/ColonSLAM.\n", "link": "http://arxiv.org/abs/2409.16806v1", "date": "2024-09-25", "relevancy": 2.8321, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5678}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5668}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topological%20SLAM%20in%20colonoscopies%20leveraging%20deep%20features%20and%0A%20%20topological%20priors&body=Title%3A%20Topological%20SLAM%20in%20colonoscopies%20leveraging%20deep%20features%20and%0A%20%20topological%20priors%0AAuthor%3A%20Javier%20Morlana%20and%20Juan%20D.%20Tard%C3%B3s%20and%20Jos%C3%A9%20M.%20M.%20Montiel%0AAbstract%3A%20%20%20We%20introduce%20ColonSLAM%2C%20a%20system%20that%20combines%20classical%20multiple-map%20metric%0ASLAM%20with%20deep%20features%20and%20topological%20priors%20to%20create%20topological%20maps%20of%0Athe%20whole%20colon.%20The%20SLAM%20pipeline%20by%20itself%20is%20able%20to%20create%20disconnected%0Aindividual%20metric%20submaps%20representing%20locations%20from%20short%20video%20subsections%0Aof%20the%20colon%2C%20but%20is%20not%20able%20to%20merge%20covisible%20submaps%20due%20to%20deformations%0Aand%20the%20limited%20performance%20of%20the%20SIFT%20descriptor%20in%20the%20medical%20domain.%0AColonSLAM%20is%20guided%20by%20topological%20priors%20and%20combines%20a%20deep%20localization%0Anetwork%20trained%20to%20distinguish%20if%20two%20images%20come%20from%20the%20same%20place%20or%20not%0Aand%20the%20soft%20verification%20of%20a%20transformer-based%20matching%20network%2C%20being%20able%0Ato%20relate%20far-in-time%20submaps%20during%20an%20exploration%2C%20grouping%20them%20in%20nodes%0Aimaging%20the%20same%20colon%20place%2C%20building%20more%20complex%20maps%20than%20any%20other%0Aapproach%20in%20the%20literature.%20We%20demonstrate%20our%20approach%20in%20the%20Endomapper%0Adataset%2C%20showing%20its%20potential%20for%20producing%20maps%20of%20the%20whole%20colon%20in%20real%0Ahuman%20explorations.%20Code%20and%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/endomapper/ColonSLAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopological%2520SLAM%2520in%2520colonoscopies%2520leveraging%2520deep%2520features%2520and%250A%2520%2520topological%2520priors%26entry.906535625%3DJavier%2520Morlana%2520and%2520Juan%2520D.%2520Tard%25C3%25B3s%2520and%2520Jos%25C3%25A9%2520M.%2520M.%2520Montiel%26entry.1292438233%3D%2520%2520We%2520introduce%2520ColonSLAM%252C%2520a%2520system%2520that%2520combines%2520classical%2520multiple-map%2520metric%250ASLAM%2520with%2520deep%2520features%2520and%2520topological%2520priors%2520to%2520create%2520topological%2520maps%2520of%250Athe%2520whole%2520colon.%2520The%2520SLAM%2520pipeline%2520by%2520itself%2520is%2520able%2520to%2520create%2520disconnected%250Aindividual%2520metric%2520submaps%2520representing%2520locations%2520from%2520short%2520video%2520subsections%250Aof%2520the%2520colon%252C%2520but%2520is%2520not%2520able%2520to%2520merge%2520covisible%2520submaps%2520due%2520to%2520deformations%250Aand%2520the%2520limited%2520performance%2520of%2520the%2520SIFT%2520descriptor%2520in%2520the%2520medical%2520domain.%250AColonSLAM%2520is%2520guided%2520by%2520topological%2520priors%2520and%2520combines%2520a%2520deep%2520localization%250Anetwork%2520trained%2520to%2520distinguish%2520if%2520two%2520images%2520come%2520from%2520the%2520same%2520place%2520or%2520not%250Aand%2520the%2520soft%2520verification%2520of%2520a%2520transformer-based%2520matching%2520network%252C%2520being%2520able%250Ato%2520relate%2520far-in-time%2520submaps%2520during%2520an%2520exploration%252C%2520grouping%2520them%2520in%2520nodes%250Aimaging%2520the%2520same%2520colon%2520place%252C%2520building%2520more%2520complex%2520maps%2520than%2520any%2520other%250Aapproach%2520in%2520the%2520literature.%2520We%2520demonstrate%2520our%2520approach%2520in%2520the%2520Endomapper%250Adataset%252C%2520showing%2520its%2520potential%2520for%2520producing%2520maps%2520of%2520the%2520whole%2520colon%2520in%2520real%250Ahuman%2520explorations.%2520Code%2520and%2520models%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/endomapper/ColonSLAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topological%20SLAM%20in%20colonoscopies%20leveraging%20deep%20features%20and%0A%20%20topological%20priors&entry.906535625=Javier%20Morlana%20and%20Juan%20D.%20Tard%C3%B3s%20and%20Jos%C3%A9%20M.%20M.%20Montiel&entry.1292438233=%20%20We%20introduce%20ColonSLAM%2C%20a%20system%20that%20combines%20classical%20multiple-map%20metric%0ASLAM%20with%20deep%20features%20and%20topological%20priors%20to%20create%20topological%20maps%20of%0Athe%20whole%20colon.%20The%20SLAM%20pipeline%20by%20itself%20is%20able%20to%20create%20disconnected%0Aindividual%20metric%20submaps%20representing%20locations%20from%20short%20video%20subsections%0Aof%20the%20colon%2C%20but%20is%20not%20able%20to%20merge%20covisible%20submaps%20due%20to%20deformations%0Aand%20the%20limited%20performance%20of%20the%20SIFT%20descriptor%20in%20the%20medical%20domain.%0AColonSLAM%20is%20guided%20by%20topological%20priors%20and%20combines%20a%20deep%20localization%0Anetwork%20trained%20to%20distinguish%20if%20two%20images%20come%20from%20the%20same%20place%20or%20not%0Aand%20the%20soft%20verification%20of%20a%20transformer-based%20matching%20network%2C%20being%20able%0Ato%20relate%20far-in-time%20submaps%20during%20an%20exploration%2C%20grouping%20them%20in%20nodes%0Aimaging%20the%20same%20colon%20place%2C%20building%20more%20complex%20maps%20than%20any%20other%0Aapproach%20in%20the%20literature.%20We%20demonstrate%20our%20approach%20in%20the%20Endomapper%0Adataset%2C%20showing%20its%20potential%20for%20producing%20maps%20of%20the%20whole%20colon%20in%20real%0Ahuman%20explorations.%20Code%20and%20models%20are%20available%20at%3A%0Ahttps%3A//github.com/endomapper/ColonSLAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16806v1&entry.124074799=Read"},
{"title": "Bits-to-Photon: End-to-End Learned Scalable Point Cloud Compression for\n  Direct Rendering", "author": "Yueyu Hu and Ran Gong and Yao Wang", "abstract": "  Point cloud is a promising 3D representation for volumetric streaming in\nemerging AR/VR applications. Despite recent advances in point cloud\ncompression, decoding and rendering high-quality images from lossy compressed\npoint clouds is still challenging in terms of quality and complexity, making it\na major roadblock to achieve real-time 6-Degree-of-Freedom video streaming. In\nthis paper, we address this problem by developing a point cloud compression\nscheme that generates a bit stream that can be directly decoded to renderable\n3D Gaussians. The encoder and decoder are jointly optimized to consider both\nbit-rates and rendering quality. It significantly improves the rendering\nquality while substantially reducing decoding and rendering time, compared to\nexisting point cloud compression methods. Furthermore, the proposed scheme\ngenerates a scalable bit stream, allowing multiple levels of details at\ndifferent bit-rate ranges. Our method supports real-time color decoding and\nrendering of high quality point clouds, thus paving the way for interactive 3D\nstreaming applications with free view points.\n", "link": "http://arxiv.org/abs/2406.05915v2", "date": "2024-09-25", "relevancy": 2.8188, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5849}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5553}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bits-to-Photon%3A%20End-to-End%20Learned%20Scalable%20Point%20Cloud%20Compression%20for%0A%20%20Direct%20Rendering&body=Title%3A%20Bits-to-Photon%3A%20End-to-End%20Learned%20Scalable%20Point%20Cloud%20Compression%20for%0A%20%20Direct%20Rendering%0AAuthor%3A%20Yueyu%20Hu%20and%20Ran%20Gong%20and%20Yao%20Wang%0AAbstract%3A%20%20%20Point%20cloud%20is%20a%20promising%203D%20representation%20for%20volumetric%20streaming%20in%0Aemerging%20AR/VR%20applications.%20Despite%20recent%20advances%20in%20point%20cloud%0Acompression%2C%20decoding%20and%20rendering%20high-quality%20images%20from%20lossy%20compressed%0Apoint%20clouds%20is%20still%20challenging%20in%20terms%20of%20quality%20and%20complexity%2C%20making%20it%0Aa%20major%20roadblock%20to%20achieve%20real-time%206-Degree-of-Freedom%20video%20streaming.%20In%0Athis%20paper%2C%20we%20address%20this%20problem%20by%20developing%20a%20point%20cloud%20compression%0Ascheme%20that%20generates%20a%20bit%20stream%20that%20can%20be%20directly%20decoded%20to%20renderable%0A3D%20Gaussians.%20The%20encoder%20and%20decoder%20are%20jointly%20optimized%20to%20consider%20both%0Abit-rates%20and%20rendering%20quality.%20It%20significantly%20improves%20the%20rendering%0Aquality%20while%20substantially%20reducing%20decoding%20and%20rendering%20time%2C%20compared%20to%0Aexisting%20point%20cloud%20compression%20methods.%20Furthermore%2C%20the%20proposed%20scheme%0Agenerates%20a%20scalable%20bit%20stream%2C%20allowing%20multiple%20levels%20of%20details%20at%0Adifferent%20bit-rate%20ranges.%20Our%20method%20supports%20real-time%20color%20decoding%20and%0Arendering%20of%20high%20quality%20point%20clouds%2C%20thus%20paving%20the%20way%20for%20interactive%203D%0Astreaming%20applications%20with%20free%20view%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05915v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBits-to-Photon%253A%2520End-to-End%2520Learned%2520Scalable%2520Point%2520Cloud%2520Compression%2520for%250A%2520%2520Direct%2520Rendering%26entry.906535625%3DYueyu%2520Hu%2520and%2520Ran%2520Gong%2520and%2520Yao%2520Wang%26entry.1292438233%3D%2520%2520Point%2520cloud%2520is%2520a%2520promising%25203D%2520representation%2520for%2520volumetric%2520streaming%2520in%250Aemerging%2520AR/VR%2520applications.%2520Despite%2520recent%2520advances%2520in%2520point%2520cloud%250Acompression%252C%2520decoding%2520and%2520rendering%2520high-quality%2520images%2520from%2520lossy%2520compressed%250Apoint%2520clouds%2520is%2520still%2520challenging%2520in%2520terms%2520of%2520quality%2520and%2520complexity%252C%2520making%2520it%250Aa%2520major%2520roadblock%2520to%2520achieve%2520real-time%25206-Degree-of-Freedom%2520video%2520streaming.%2520In%250Athis%2520paper%252C%2520we%2520address%2520this%2520problem%2520by%2520developing%2520a%2520point%2520cloud%2520compression%250Ascheme%2520that%2520generates%2520a%2520bit%2520stream%2520that%2520can%2520be%2520directly%2520decoded%2520to%2520renderable%250A3D%2520Gaussians.%2520The%2520encoder%2520and%2520decoder%2520are%2520jointly%2520optimized%2520to%2520consider%2520both%250Abit-rates%2520and%2520rendering%2520quality.%2520It%2520significantly%2520improves%2520the%2520rendering%250Aquality%2520while%2520substantially%2520reducing%2520decoding%2520and%2520rendering%2520time%252C%2520compared%2520to%250Aexisting%2520point%2520cloud%2520compression%2520methods.%2520Furthermore%252C%2520the%2520proposed%2520scheme%250Agenerates%2520a%2520scalable%2520bit%2520stream%252C%2520allowing%2520multiple%2520levels%2520of%2520details%2520at%250Adifferent%2520bit-rate%2520ranges.%2520Our%2520method%2520supports%2520real-time%2520color%2520decoding%2520and%250Arendering%2520of%2520high%2520quality%2520point%2520clouds%252C%2520thus%2520paving%2520the%2520way%2520for%2520interactive%25203D%250Astreaming%2520applications%2520with%2520free%2520view%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05915v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bits-to-Photon%3A%20End-to-End%20Learned%20Scalable%20Point%20Cloud%20Compression%20for%0A%20%20Direct%20Rendering&entry.906535625=Yueyu%20Hu%20and%20Ran%20Gong%20and%20Yao%20Wang&entry.1292438233=%20%20Point%20cloud%20is%20a%20promising%203D%20representation%20for%20volumetric%20streaming%20in%0Aemerging%20AR/VR%20applications.%20Despite%20recent%20advances%20in%20point%20cloud%0Acompression%2C%20decoding%20and%20rendering%20high-quality%20images%20from%20lossy%20compressed%0Apoint%20clouds%20is%20still%20challenging%20in%20terms%20of%20quality%20and%20complexity%2C%20making%20it%0Aa%20major%20roadblock%20to%20achieve%20real-time%206-Degree-of-Freedom%20video%20streaming.%20In%0Athis%20paper%2C%20we%20address%20this%20problem%20by%20developing%20a%20point%20cloud%20compression%0Ascheme%20that%20generates%20a%20bit%20stream%20that%20can%20be%20directly%20decoded%20to%20renderable%0A3D%20Gaussians.%20The%20encoder%20and%20decoder%20are%20jointly%20optimized%20to%20consider%20both%0Abit-rates%20and%20rendering%20quality.%20It%20significantly%20improves%20the%20rendering%0Aquality%20while%20substantially%20reducing%20decoding%20and%20rendering%20time%2C%20compared%20to%0Aexisting%20point%20cloud%20compression%20methods.%20Furthermore%2C%20the%20proposed%20scheme%0Agenerates%20a%20scalable%20bit%20stream%2C%20allowing%20multiple%20levels%20of%20details%20at%0Adifferent%20bit-rate%20ranges.%20Our%20method%20supports%20real-time%20color%20decoding%20and%0Arendering%20of%20high%20quality%20point%20clouds%2C%20thus%20paving%20the%20way%20for%20interactive%203D%0Astreaming%20applications%20with%20free%20view%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05915v2&entry.124074799=Read"},
{"title": "HVT: A Comprehensive Vision Framework for Learning in Non-Euclidean\n  Space", "author": "Jacob Fein-Ashley and Ethan Feng and Minh Pham", "abstract": "  Data representation in non-Euclidean spaces has proven effective for\ncapturing hierarchical and complex relationships in real-world datasets.\nHyperbolic spaces, in particular, provide efficient embeddings for hierarchical\nstructures. This paper introduces the Hyperbolic Vision Transformer (HVT), a\nnovel extension of the Vision Transformer (ViT) that integrates hyperbolic\ngeometry. While traditional ViTs operate in Euclidean space, our method\nenhances the self-attention mechanism by leveraging hyperbolic distance and\nM\\\"obius transformations. This enables more effective modeling of hierarchical\nand relational dependencies in image data. We present rigorous mathematical\nformulations, showing how hyperbolic geometry can be incorporated into\nattention layers, feed-forward networks, and optimization. We offer improved\nperformance for image classification using the ImageNet dataset.\n", "link": "http://arxiv.org/abs/2409.16897v1", "date": "2024-09-25", "relevancy": 2.801, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5665}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HVT%3A%20A%20Comprehensive%20Vision%20Framework%20for%20Learning%20in%20Non-Euclidean%0A%20%20Space&body=Title%3A%20HVT%3A%20A%20Comprehensive%20Vision%20Framework%20for%20Learning%20in%20Non-Euclidean%0A%20%20Space%0AAuthor%3A%20Jacob%20Fein-Ashley%20and%20Ethan%20Feng%20and%20Minh%20Pham%0AAbstract%3A%20%20%20Data%20representation%20in%20non-Euclidean%20spaces%20has%20proven%20effective%20for%0Acapturing%20hierarchical%20and%20complex%20relationships%20in%20real-world%20datasets.%0AHyperbolic%20spaces%2C%20in%20particular%2C%20provide%20efficient%20embeddings%20for%20hierarchical%0Astructures.%20This%20paper%20introduces%20the%20Hyperbolic%20Vision%20Transformer%20%28HVT%29%2C%20a%0Anovel%20extension%20of%20the%20Vision%20Transformer%20%28ViT%29%20that%20integrates%20hyperbolic%0Ageometry.%20While%20traditional%20ViTs%20operate%20in%20Euclidean%20space%2C%20our%20method%0Aenhances%20the%20self-attention%20mechanism%20by%20leveraging%20hyperbolic%20distance%20and%0AM%5C%22obius%20transformations.%20This%20enables%20more%20effective%20modeling%20of%20hierarchical%0Aand%20relational%20dependencies%20in%20image%20data.%20We%20present%20rigorous%20mathematical%0Aformulations%2C%20showing%20how%20hyperbolic%20geometry%20can%20be%20incorporated%20into%0Aattention%20layers%2C%20feed-forward%20networks%2C%20and%20optimization.%20We%20offer%20improved%0Aperformance%20for%20image%20classification%20using%20the%20ImageNet%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHVT%253A%2520A%2520Comprehensive%2520Vision%2520Framework%2520for%2520Learning%2520in%2520Non-Euclidean%250A%2520%2520Space%26entry.906535625%3DJacob%2520Fein-Ashley%2520and%2520Ethan%2520Feng%2520and%2520Minh%2520Pham%26entry.1292438233%3D%2520%2520Data%2520representation%2520in%2520non-Euclidean%2520spaces%2520has%2520proven%2520effective%2520for%250Acapturing%2520hierarchical%2520and%2520complex%2520relationships%2520in%2520real-world%2520datasets.%250AHyperbolic%2520spaces%252C%2520in%2520particular%252C%2520provide%2520efficient%2520embeddings%2520for%2520hierarchical%250Astructures.%2520This%2520paper%2520introduces%2520the%2520Hyperbolic%2520Vision%2520Transformer%2520%2528HVT%2529%252C%2520a%250Anovel%2520extension%2520of%2520the%2520Vision%2520Transformer%2520%2528ViT%2529%2520that%2520integrates%2520hyperbolic%250Ageometry.%2520While%2520traditional%2520ViTs%2520operate%2520in%2520Euclidean%2520space%252C%2520our%2520method%250Aenhances%2520the%2520self-attention%2520mechanism%2520by%2520leveraging%2520hyperbolic%2520distance%2520and%250AM%255C%2522obius%2520transformations.%2520This%2520enables%2520more%2520effective%2520modeling%2520of%2520hierarchical%250Aand%2520relational%2520dependencies%2520in%2520image%2520data.%2520We%2520present%2520rigorous%2520mathematical%250Aformulations%252C%2520showing%2520how%2520hyperbolic%2520geometry%2520can%2520be%2520incorporated%2520into%250Aattention%2520layers%252C%2520feed-forward%2520networks%252C%2520and%2520optimization.%2520We%2520offer%2520improved%250Aperformance%2520for%2520image%2520classification%2520using%2520the%2520ImageNet%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HVT%3A%20A%20Comprehensive%20Vision%20Framework%20for%20Learning%20in%20Non-Euclidean%0A%20%20Space&entry.906535625=Jacob%20Fein-Ashley%20and%20Ethan%20Feng%20and%20Minh%20Pham&entry.1292438233=%20%20Data%20representation%20in%20non-Euclidean%20spaces%20has%20proven%20effective%20for%0Acapturing%20hierarchical%20and%20complex%20relationships%20in%20real-world%20datasets.%0AHyperbolic%20spaces%2C%20in%20particular%2C%20provide%20efficient%20embeddings%20for%20hierarchical%0Astructures.%20This%20paper%20introduces%20the%20Hyperbolic%20Vision%20Transformer%20%28HVT%29%2C%20a%0Anovel%20extension%20of%20the%20Vision%20Transformer%20%28ViT%29%20that%20integrates%20hyperbolic%0Ageometry.%20While%20traditional%20ViTs%20operate%20in%20Euclidean%20space%2C%20our%20method%0Aenhances%20the%20self-attention%20mechanism%20by%20leveraging%20hyperbolic%20distance%20and%0AM%5C%22obius%20transformations.%20This%20enables%20more%20effective%20modeling%20of%20hierarchical%0Aand%20relational%20dependencies%20in%20image%20data.%20We%20present%20rigorous%20mathematical%0Aformulations%2C%20showing%20how%20hyperbolic%20geometry%20can%20be%20incorporated%20into%0Aattention%20layers%2C%20feed-forward%20networks%2C%20and%20optimization.%20We%20offer%20improved%0Aperformance%20for%20image%20classification%20using%20the%20ImageNet%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16897v1&entry.124074799=Read"},
{"title": "Game4Loc: A UAV Geo-Localization Benchmark from Game Data", "author": "Yuxiang Ji and Boyong He and Zhuoyue Tan and Liaoni Wu", "abstract": "  The vision-based geo-localization technology for UAV, serving as a secondary\nsource of GPS information in addition to the global navigation satellite\nsystems (GNSS), can still operate independently in the GPS-denied environment.\nRecent deep learning based methods attribute this as the task of image matching\nand retrieval. By retrieving drone-view images in geo-tagged satellite image\ndatabase, approximate localization information can be obtained. However, due to\nhigh costs and privacy concerns, it is usually difficult to obtain large\nquantities of drone-view images from a continuous area. Existing drone-view\ndatasets are mostly composed of small-scale aerial photography with a strong\nassumption that there exists a perfect one-to-one aligned reference image for\nany query, leaving a significant gap from the practical localization scenario.\nIn this work, we construct a large-range contiguous area UAV geo-localization\ndataset named GTA-UAV, featuring multiple flight altitudes, attitudes, scenes,\nand targets using modern computer games. Based on this dataset, we introduce a\nmore practical UAV geo-localization task including partial matches of\ncross-view paired data, and expand the image-level retrieval to the actual\nlocalization in terms of distance (meters). For the construction of drone-view\nand satellite-view pairs, we adopt a weight-based contrastive learning\napproach, which allows for effective learning while avoiding additional\npost-processing matching steps. Experiments demonstrate the effectiveness of\nour data and training method for UAV geo-localization, as well as the\ngeneralization capabilities to real-world scenarios.\n", "link": "http://arxiv.org/abs/2409.16925v1", "date": "2024-09-25", "relevancy": 2.7982, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5928}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5654}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5208}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Game4Loc%3A%20A%20UAV%20Geo-Localization%20Benchmark%20from%20Game%20Data&body=Title%3A%20Game4Loc%3A%20A%20UAV%20Geo-Localization%20Benchmark%20from%20Game%20Data%0AAuthor%3A%20Yuxiang%20Ji%20and%20Boyong%20He%20and%20Zhuoyue%20Tan%20and%20Liaoni%20Wu%0AAbstract%3A%20%20%20The%20vision-based%20geo-localization%20technology%20for%20UAV%2C%20serving%20as%20a%20secondary%0Asource%20of%20GPS%20information%20in%20addition%20to%20the%20global%20navigation%20satellite%0Asystems%20%28GNSS%29%2C%20can%20still%20operate%20independently%20in%20the%20GPS-denied%20environment.%0ARecent%20deep%20learning%20based%20methods%20attribute%20this%20as%20the%20task%20of%20image%20matching%0Aand%20retrieval.%20By%20retrieving%20drone-view%20images%20in%20geo-tagged%20satellite%20image%0Adatabase%2C%20approximate%20localization%20information%20can%20be%20obtained.%20However%2C%20due%20to%0Ahigh%20costs%20and%20privacy%20concerns%2C%20it%20is%20usually%20difficult%20to%20obtain%20large%0Aquantities%20of%20drone-view%20images%20from%20a%20continuous%20area.%20Existing%20drone-view%0Adatasets%20are%20mostly%20composed%20of%20small-scale%20aerial%20photography%20with%20a%20strong%0Aassumption%20that%20there%20exists%20a%20perfect%20one-to-one%20aligned%20reference%20image%20for%0Aany%20query%2C%20leaving%20a%20significant%20gap%20from%20the%20practical%20localization%20scenario.%0AIn%20this%20work%2C%20we%20construct%20a%20large-range%20contiguous%20area%20UAV%20geo-localization%0Adataset%20named%20GTA-UAV%2C%20featuring%20multiple%20flight%20altitudes%2C%20attitudes%2C%20scenes%2C%0Aand%20targets%20using%20modern%20computer%20games.%20Based%20on%20this%20dataset%2C%20we%20introduce%20a%0Amore%20practical%20UAV%20geo-localization%20task%20including%20partial%20matches%20of%0Across-view%20paired%20data%2C%20and%20expand%20the%20image-level%20retrieval%20to%20the%20actual%0Alocalization%20in%20terms%20of%20distance%20%28meters%29.%20For%20the%20construction%20of%20drone-view%0Aand%20satellite-view%20pairs%2C%20we%20adopt%20a%20weight-based%20contrastive%20learning%0Aapproach%2C%20which%20allows%20for%20effective%20learning%20while%20avoiding%20additional%0Apost-processing%20matching%20steps.%20Experiments%20demonstrate%20the%20effectiveness%20of%0Aour%20data%20and%20training%20method%20for%20UAV%20geo-localization%2C%20as%20well%20as%20the%0Ageneralization%20capabilities%20to%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGame4Loc%253A%2520A%2520UAV%2520Geo-Localization%2520Benchmark%2520from%2520Game%2520Data%26entry.906535625%3DYuxiang%2520Ji%2520and%2520Boyong%2520He%2520and%2520Zhuoyue%2520Tan%2520and%2520Liaoni%2520Wu%26entry.1292438233%3D%2520%2520The%2520vision-based%2520geo-localization%2520technology%2520for%2520UAV%252C%2520serving%2520as%2520a%2520secondary%250Asource%2520of%2520GPS%2520information%2520in%2520addition%2520to%2520the%2520global%2520navigation%2520satellite%250Asystems%2520%2528GNSS%2529%252C%2520can%2520still%2520operate%2520independently%2520in%2520the%2520GPS-denied%2520environment.%250ARecent%2520deep%2520learning%2520based%2520methods%2520attribute%2520this%2520as%2520the%2520task%2520of%2520image%2520matching%250Aand%2520retrieval.%2520By%2520retrieving%2520drone-view%2520images%2520in%2520geo-tagged%2520satellite%2520image%250Adatabase%252C%2520approximate%2520localization%2520information%2520can%2520be%2520obtained.%2520However%252C%2520due%2520to%250Ahigh%2520costs%2520and%2520privacy%2520concerns%252C%2520it%2520is%2520usually%2520difficult%2520to%2520obtain%2520large%250Aquantities%2520of%2520drone-view%2520images%2520from%2520a%2520continuous%2520area.%2520Existing%2520drone-view%250Adatasets%2520are%2520mostly%2520composed%2520of%2520small-scale%2520aerial%2520photography%2520with%2520a%2520strong%250Aassumption%2520that%2520there%2520exists%2520a%2520perfect%2520one-to-one%2520aligned%2520reference%2520image%2520for%250Aany%2520query%252C%2520leaving%2520a%2520significant%2520gap%2520from%2520the%2520practical%2520localization%2520scenario.%250AIn%2520this%2520work%252C%2520we%2520construct%2520a%2520large-range%2520contiguous%2520area%2520UAV%2520geo-localization%250Adataset%2520named%2520GTA-UAV%252C%2520featuring%2520multiple%2520flight%2520altitudes%252C%2520attitudes%252C%2520scenes%252C%250Aand%2520targets%2520using%2520modern%2520computer%2520games.%2520Based%2520on%2520this%2520dataset%252C%2520we%2520introduce%2520a%250Amore%2520practical%2520UAV%2520geo-localization%2520task%2520including%2520partial%2520matches%2520of%250Across-view%2520paired%2520data%252C%2520and%2520expand%2520the%2520image-level%2520retrieval%2520to%2520the%2520actual%250Alocalization%2520in%2520terms%2520of%2520distance%2520%2528meters%2529.%2520For%2520the%2520construction%2520of%2520drone-view%250Aand%2520satellite-view%2520pairs%252C%2520we%2520adopt%2520a%2520weight-based%2520contrastive%2520learning%250Aapproach%252C%2520which%2520allows%2520for%2520effective%2520learning%2520while%2520avoiding%2520additional%250Apost-processing%2520matching%2520steps.%2520Experiments%2520demonstrate%2520the%2520effectiveness%2520of%250Aour%2520data%2520and%2520training%2520method%2520for%2520UAV%2520geo-localization%252C%2520as%2520well%2520as%2520the%250Ageneralization%2520capabilities%2520to%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Game4Loc%3A%20A%20UAV%20Geo-Localization%20Benchmark%20from%20Game%20Data&entry.906535625=Yuxiang%20Ji%20and%20Boyong%20He%20and%20Zhuoyue%20Tan%20and%20Liaoni%20Wu&entry.1292438233=%20%20The%20vision-based%20geo-localization%20technology%20for%20UAV%2C%20serving%20as%20a%20secondary%0Asource%20of%20GPS%20information%20in%20addition%20to%20the%20global%20navigation%20satellite%0Asystems%20%28GNSS%29%2C%20can%20still%20operate%20independently%20in%20the%20GPS-denied%20environment.%0ARecent%20deep%20learning%20based%20methods%20attribute%20this%20as%20the%20task%20of%20image%20matching%0Aand%20retrieval.%20By%20retrieving%20drone-view%20images%20in%20geo-tagged%20satellite%20image%0Adatabase%2C%20approximate%20localization%20information%20can%20be%20obtained.%20However%2C%20due%20to%0Ahigh%20costs%20and%20privacy%20concerns%2C%20it%20is%20usually%20difficult%20to%20obtain%20large%0Aquantities%20of%20drone-view%20images%20from%20a%20continuous%20area.%20Existing%20drone-view%0Adatasets%20are%20mostly%20composed%20of%20small-scale%20aerial%20photography%20with%20a%20strong%0Aassumption%20that%20there%20exists%20a%20perfect%20one-to-one%20aligned%20reference%20image%20for%0Aany%20query%2C%20leaving%20a%20significant%20gap%20from%20the%20practical%20localization%20scenario.%0AIn%20this%20work%2C%20we%20construct%20a%20large-range%20contiguous%20area%20UAV%20geo-localization%0Adataset%20named%20GTA-UAV%2C%20featuring%20multiple%20flight%20altitudes%2C%20attitudes%2C%20scenes%2C%0Aand%20targets%20using%20modern%20computer%20games.%20Based%20on%20this%20dataset%2C%20we%20introduce%20a%0Amore%20practical%20UAV%20geo-localization%20task%20including%20partial%20matches%20of%0Across-view%20paired%20data%2C%20and%20expand%20the%20image-level%20retrieval%20to%20the%20actual%0Alocalization%20in%20terms%20of%20distance%20%28meters%29.%20For%20the%20construction%20of%20drone-view%0Aand%20satellite-view%20pairs%2C%20we%20adopt%20a%20weight-based%20contrastive%20learning%0Aapproach%2C%20which%20allows%20for%20effective%20learning%20while%20avoiding%20additional%0Apost-processing%20matching%20steps.%20Experiments%20demonstrate%20the%20effectiveness%20of%0Aour%20data%20and%20training%20method%20for%20UAV%20geo-localization%2C%20as%20well%20as%20the%0Ageneralization%20capabilities%20to%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16925v1&entry.124074799=Read"},
{"title": "Limitations of (Procrustes) Alignment in Assessing Multi-Person Human\n  Pose and Shape Estimation", "author": "Drazic Martin and Pierre Perrault", "abstract": "  We delve into the challenges of accurately estimating 3D human pose and shape\nin video surveillance scenarios. Beginning with the advocacy for metrics like\nW-MPJPE and W-PVE, which omit the (Procrustes) realignment step, to improve\nmodel evaluation, we then introduce RotAvat. This technique aims to enhance\nthese metrics by refining the alignment of 3D meshes with the ground plane.\nThrough qualitative comparisons, we demonstrate RotAvat's effectiveness in\naddressing the limitations of existing aproaches.\n", "link": "http://arxiv.org/abs/2409.16861v1", "date": "2024-09-25", "relevancy": 2.7962, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5812}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5524}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Limitations%20of%20%28Procrustes%29%20Alignment%20in%20Assessing%20Multi-Person%20Human%0A%20%20Pose%20and%20Shape%20Estimation&body=Title%3A%20Limitations%20of%20%28Procrustes%29%20Alignment%20in%20Assessing%20Multi-Person%20Human%0A%20%20Pose%20and%20Shape%20Estimation%0AAuthor%3A%20Drazic%20Martin%20and%20Pierre%20Perrault%0AAbstract%3A%20%20%20We%20delve%20into%20the%20challenges%20of%20accurately%20estimating%203D%20human%20pose%20and%20shape%0Ain%20video%20surveillance%20scenarios.%20Beginning%20with%20the%20advocacy%20for%20metrics%20like%0AW-MPJPE%20and%20W-PVE%2C%20which%20omit%20the%20%28Procrustes%29%20realignment%20step%2C%20to%20improve%0Amodel%20evaluation%2C%20we%20then%20introduce%20RotAvat.%20This%20technique%20aims%20to%20enhance%0Athese%20metrics%20by%20refining%20the%20alignment%20of%203D%20meshes%20with%20the%20ground%20plane.%0AThrough%20qualitative%20comparisons%2C%20we%20demonstrate%20RotAvat%27s%20effectiveness%20in%0Aaddressing%20the%20limitations%20of%20existing%20aproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16861v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLimitations%2520of%2520%2528Procrustes%2529%2520Alignment%2520in%2520Assessing%2520Multi-Person%2520Human%250A%2520%2520Pose%2520and%2520Shape%2520Estimation%26entry.906535625%3DDrazic%2520Martin%2520and%2520Pierre%2520Perrault%26entry.1292438233%3D%2520%2520We%2520delve%2520into%2520the%2520challenges%2520of%2520accurately%2520estimating%25203D%2520human%2520pose%2520and%2520shape%250Ain%2520video%2520surveillance%2520scenarios.%2520Beginning%2520with%2520the%2520advocacy%2520for%2520metrics%2520like%250AW-MPJPE%2520and%2520W-PVE%252C%2520which%2520omit%2520the%2520%2528Procrustes%2529%2520realignment%2520step%252C%2520to%2520improve%250Amodel%2520evaluation%252C%2520we%2520then%2520introduce%2520RotAvat.%2520This%2520technique%2520aims%2520to%2520enhance%250Athese%2520metrics%2520by%2520refining%2520the%2520alignment%2520of%25203D%2520meshes%2520with%2520the%2520ground%2520plane.%250AThrough%2520qualitative%2520comparisons%252C%2520we%2520demonstrate%2520RotAvat%2527s%2520effectiveness%2520in%250Aaddressing%2520the%2520limitations%2520of%2520existing%2520aproaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16861v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Limitations%20of%20%28Procrustes%29%20Alignment%20in%20Assessing%20Multi-Person%20Human%0A%20%20Pose%20and%20Shape%20Estimation&entry.906535625=Drazic%20Martin%20and%20Pierre%20Perrault&entry.1292438233=%20%20We%20delve%20into%20the%20challenges%20of%20accurately%20estimating%203D%20human%20pose%20and%20shape%0Ain%20video%20surveillance%20scenarios.%20Beginning%20with%20the%20advocacy%20for%20metrics%20like%0AW-MPJPE%20and%20W-PVE%2C%20which%20omit%20the%20%28Procrustes%29%20realignment%20step%2C%20to%20improve%0Amodel%20evaluation%2C%20we%20then%20introduce%20RotAvat.%20This%20technique%20aims%20to%20enhance%0Athese%20metrics%20by%20refining%20the%20alignment%20of%203D%20meshes%20with%20the%20ground%20plane.%0AThrough%20qualitative%20comparisons%2C%20we%20demonstrate%20RotAvat%27s%20effectiveness%20in%0Aaddressing%20the%20limitations%20of%20existing%20aproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16861v1&entry.124074799=Read"},
{"title": "Linking in Style: Understanding learned features in deep learning models", "author": "Maren H. Wehrheim and Pamela Osuna-Vargas and Matthias Kaschube", "abstract": "  Convolutional neural networks (CNNs) learn abstract features to perform\nobject classification, but understanding these features remains challenging due\nto difficult-to-interpret results or high computational costs. We propose an\nautomatic method to visualize and systematically analyze learned features in\nCNNs. Specifically, we introduce a linking network that maps the penultimate\nlayer of a pre-trained classifier to the latent space of a generative model\n(StyleGAN-XL), thereby enabling an interpretable, human-friendly visualization\nof the classifier's representations. Our findings indicate a congruent semantic\norder in both spaces, enabling a direct linear mapping between them. Training\nthe linking network is computationally inexpensive and decoupled from training\nboth the GAN and the classifier. We introduce an automatic pipeline that\nutilizes such GAN-based visualizations to quantify learned representations by\nanalyzing activation changes in the classifier in the image domain. This\nquantification allows us to systematically study the learned representations in\nseveral thousand units simultaneously and to extract and visualize units\nselective for specific semantic concepts. Further, we illustrate how our method\ncan be used to quantify and interpret the classifier's decision boundary using\ncounterfactual examples. Overall, our method offers systematic and objective\nperspectives on learned abstract representations in CNNs.\nhttps://github.com/kaschube-lab/LinkingInStyle.git\n", "link": "http://arxiv.org/abs/2409.16865v1", "date": "2024-09-25", "relevancy": 2.7649, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5606}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linking%20in%20Style%3A%20Understanding%20learned%20features%20in%20deep%20learning%20models&body=Title%3A%20Linking%20in%20Style%3A%20Understanding%20learned%20features%20in%20deep%20learning%20models%0AAuthor%3A%20Maren%20H.%20Wehrheim%20and%20Pamela%20Osuna-Vargas%20and%20Matthias%20Kaschube%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNNs%29%20learn%20abstract%20features%20to%20perform%0Aobject%20classification%2C%20but%20understanding%20these%20features%20remains%20challenging%20due%0Ato%20difficult-to-interpret%20results%20or%20high%20computational%20costs.%20We%20propose%20an%0Aautomatic%20method%20to%20visualize%20and%20systematically%20analyze%20learned%20features%20in%0ACNNs.%20Specifically%2C%20we%20introduce%20a%20linking%20network%20that%20maps%20the%20penultimate%0Alayer%20of%20a%20pre-trained%20classifier%20to%20the%20latent%20space%20of%20a%20generative%20model%0A%28StyleGAN-XL%29%2C%20thereby%20enabling%20an%20interpretable%2C%20human-friendly%20visualization%0Aof%20the%20classifier%27s%20representations.%20Our%20findings%20indicate%20a%20congruent%20semantic%0Aorder%20in%20both%20spaces%2C%20enabling%20a%20direct%20linear%20mapping%20between%20them.%20Training%0Athe%20linking%20network%20is%20computationally%20inexpensive%20and%20decoupled%20from%20training%0Aboth%20the%20GAN%20and%20the%20classifier.%20We%20introduce%20an%20automatic%20pipeline%20that%0Autilizes%20such%20GAN-based%20visualizations%20to%20quantify%20learned%20representations%20by%0Aanalyzing%20activation%20changes%20in%20the%20classifier%20in%20the%20image%20domain.%20This%0Aquantification%20allows%20us%20to%20systematically%20study%20the%20learned%20representations%20in%0Aseveral%20thousand%20units%20simultaneously%20and%20to%20extract%20and%20visualize%20units%0Aselective%20for%20specific%20semantic%20concepts.%20Further%2C%20we%20illustrate%20how%20our%20method%0Acan%20be%20used%20to%20quantify%20and%20interpret%20the%20classifier%27s%20decision%20boundary%20using%0Acounterfactual%20examples.%20Overall%2C%20our%20method%20offers%20systematic%20and%20objective%0Aperspectives%20on%20learned%20abstract%20representations%20in%20CNNs.%0Ahttps%3A//github.com/kaschube-lab/LinkingInStyle.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16865v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinking%2520in%2520Style%253A%2520Understanding%2520learned%2520features%2520in%2520deep%2520learning%2520models%26entry.906535625%3DMaren%2520H.%2520Wehrheim%2520and%2520Pamela%2520Osuna-Vargas%2520and%2520Matthias%2520Kaschube%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520learn%2520abstract%2520features%2520to%2520perform%250Aobject%2520classification%252C%2520but%2520understanding%2520these%2520features%2520remains%2520challenging%2520due%250Ato%2520difficult-to-interpret%2520results%2520or%2520high%2520computational%2520costs.%2520We%2520propose%2520an%250Aautomatic%2520method%2520to%2520visualize%2520and%2520systematically%2520analyze%2520learned%2520features%2520in%250ACNNs.%2520Specifically%252C%2520we%2520introduce%2520a%2520linking%2520network%2520that%2520maps%2520the%2520penultimate%250Alayer%2520of%2520a%2520pre-trained%2520classifier%2520to%2520the%2520latent%2520space%2520of%2520a%2520generative%2520model%250A%2528StyleGAN-XL%2529%252C%2520thereby%2520enabling%2520an%2520interpretable%252C%2520human-friendly%2520visualization%250Aof%2520the%2520classifier%2527s%2520representations.%2520Our%2520findings%2520indicate%2520a%2520congruent%2520semantic%250Aorder%2520in%2520both%2520spaces%252C%2520enabling%2520a%2520direct%2520linear%2520mapping%2520between%2520them.%2520Training%250Athe%2520linking%2520network%2520is%2520computationally%2520inexpensive%2520and%2520decoupled%2520from%2520training%250Aboth%2520the%2520GAN%2520and%2520the%2520classifier.%2520We%2520introduce%2520an%2520automatic%2520pipeline%2520that%250Autilizes%2520such%2520GAN-based%2520visualizations%2520to%2520quantify%2520learned%2520representations%2520by%250Aanalyzing%2520activation%2520changes%2520in%2520the%2520classifier%2520in%2520the%2520image%2520domain.%2520This%250Aquantification%2520allows%2520us%2520to%2520systematically%2520study%2520the%2520learned%2520representations%2520in%250Aseveral%2520thousand%2520units%2520simultaneously%2520and%2520to%2520extract%2520and%2520visualize%2520units%250Aselective%2520for%2520specific%2520semantic%2520concepts.%2520Further%252C%2520we%2520illustrate%2520how%2520our%2520method%250Acan%2520be%2520used%2520to%2520quantify%2520and%2520interpret%2520the%2520classifier%2527s%2520decision%2520boundary%2520using%250Acounterfactual%2520examples.%2520Overall%252C%2520our%2520method%2520offers%2520systematic%2520and%2520objective%250Aperspectives%2520on%2520learned%2520abstract%2520representations%2520in%2520CNNs.%250Ahttps%253A//github.com/kaschube-lab/LinkingInStyle.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16865v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linking%20in%20Style%3A%20Understanding%20learned%20features%20in%20deep%20learning%20models&entry.906535625=Maren%20H.%20Wehrheim%20and%20Pamela%20Osuna-Vargas%20and%20Matthias%20Kaschube&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNNs%29%20learn%20abstract%20features%20to%20perform%0Aobject%20classification%2C%20but%20understanding%20these%20features%20remains%20challenging%20due%0Ato%20difficult-to-interpret%20results%20or%20high%20computational%20costs.%20We%20propose%20an%0Aautomatic%20method%20to%20visualize%20and%20systematically%20analyze%20learned%20features%20in%0ACNNs.%20Specifically%2C%20we%20introduce%20a%20linking%20network%20that%20maps%20the%20penultimate%0Alayer%20of%20a%20pre-trained%20classifier%20to%20the%20latent%20space%20of%20a%20generative%20model%0A%28StyleGAN-XL%29%2C%20thereby%20enabling%20an%20interpretable%2C%20human-friendly%20visualization%0Aof%20the%20classifier%27s%20representations.%20Our%20findings%20indicate%20a%20congruent%20semantic%0Aorder%20in%20both%20spaces%2C%20enabling%20a%20direct%20linear%20mapping%20between%20them.%20Training%0Athe%20linking%20network%20is%20computationally%20inexpensive%20and%20decoupled%20from%20training%0Aboth%20the%20GAN%20and%20the%20classifier.%20We%20introduce%20an%20automatic%20pipeline%20that%0Autilizes%20such%20GAN-based%20visualizations%20to%20quantify%20learned%20representations%20by%0Aanalyzing%20activation%20changes%20in%20the%20classifier%20in%20the%20image%20domain.%20This%0Aquantification%20allows%20us%20to%20systematically%20study%20the%20learned%20representations%20in%0Aseveral%20thousand%20units%20simultaneously%20and%20to%20extract%20and%20visualize%20units%0Aselective%20for%20specific%20semantic%20concepts.%20Further%2C%20we%20illustrate%20how%20our%20method%0Acan%20be%20used%20to%20quantify%20and%20interpret%20the%20classifier%27s%20decision%20boundary%20using%0Acounterfactual%20examples.%20Overall%2C%20our%20method%20offers%20systematic%20and%20objective%0Aperspectives%20on%20learned%20abstract%20representations%20in%20CNNs.%0Ahttps%3A//github.com/kaschube-lab/LinkingInStyle.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16865v1&entry.124074799=Read"},
{"title": "Focus Entirety and Perceive Environment for Arbitrary-Shaped Text\n  Detection", "author": "Xu Han and Junyu Gao and Chuang Yang and Yuan Yuan and Qi Wang", "abstract": "  Due to the diversity of scene text in aspects such as font, color, shape, and\nsize, accurately and efficiently detecting text is still a formidable\nchallenge. Among the various detection approaches, segmentation-based\napproaches have emerged as prominent contenders owing to their flexible\npixel-level predictions. However, these methods typically model text instances\nin a bottom-up manner, which is highly susceptible to noise. In addition, the\nprediction of pixels is isolated without introducing pixel-feature interaction,\nwhich also influences the detection performance. To alleviate these problems,\nwe propose a multi-information level arbitrary-shaped text detector consisting\nof a focus entirety module (FEM) and a perceive environment module (PEM). The\nformer extracts instance-level features and adopts a top-down scheme to model\ntexts to reduce the influence of noises. Specifically, it assigns consistent\nentirety information to pixels within the same instance to improve their\ncohesion. In addition, it emphasizes the scale information, enabling the model\nto distinguish varying scale texts effectively. The latter extracts\nregion-level information and encourages the model to focus on the distribution\nof positive samples in the vicinity of a pixel, which perceives environment\ninformation. It treats the kernel pixels as positive samples and helps the\nmodel differentiate text and kernel features. Extensive experiments demonstrate\nthe FEM's ability to efficiently support the model in handling different scale\ntexts and confirm the PEM can assist in perceiving pixels more accurately by\nfocusing on pixel vicinities. Comparisons show the proposed model outperforms\nexisting state-of-the-art approaches on four public datasets.\n", "link": "http://arxiv.org/abs/2409.16827v1", "date": "2024-09-25", "relevancy": 2.7142, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5436}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5425}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5425}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Focus%20Entirety%20and%20Perceive%20Environment%20for%20Arbitrary-Shaped%20Text%0A%20%20Detection&body=Title%3A%20Focus%20Entirety%20and%20Perceive%20Environment%20for%20Arbitrary-Shaped%20Text%0A%20%20Detection%0AAuthor%3A%20Xu%20Han%20and%20Junyu%20Gao%20and%20Chuang%20Yang%20and%20Yuan%20Yuan%20and%20Qi%20Wang%0AAbstract%3A%20%20%20Due%20to%20the%20diversity%20of%20scene%20text%20in%20aspects%20such%20as%20font%2C%20color%2C%20shape%2C%20and%0Asize%2C%20accurately%20and%20efficiently%20detecting%20text%20is%20still%20a%20formidable%0Achallenge.%20Among%20the%20various%20detection%20approaches%2C%20segmentation-based%0Aapproaches%20have%20emerged%20as%20prominent%20contenders%20owing%20to%20their%20flexible%0Apixel-level%20predictions.%20However%2C%20these%20methods%20typically%20model%20text%20instances%0Ain%20a%20bottom-up%20manner%2C%20which%20is%20highly%20susceptible%20to%20noise.%20In%20addition%2C%20the%0Aprediction%20of%20pixels%20is%20isolated%20without%20introducing%20pixel-feature%20interaction%2C%0Awhich%20also%20influences%20the%20detection%20performance.%20To%20alleviate%20these%20problems%2C%0Awe%20propose%20a%20multi-information%20level%20arbitrary-shaped%20text%20detector%20consisting%0Aof%20a%20focus%20entirety%20module%20%28FEM%29%20and%20a%20perceive%20environment%20module%20%28PEM%29.%20The%0Aformer%20extracts%20instance-level%20features%20and%20adopts%20a%20top-down%20scheme%20to%20model%0Atexts%20to%20reduce%20the%20influence%20of%20noises.%20Specifically%2C%20it%20assigns%20consistent%0Aentirety%20information%20to%20pixels%20within%20the%20same%20instance%20to%20improve%20their%0Acohesion.%20In%20addition%2C%20it%20emphasizes%20the%20scale%20information%2C%20enabling%20the%20model%0Ato%20distinguish%20varying%20scale%20texts%20effectively.%20The%20latter%20extracts%0Aregion-level%20information%20and%20encourages%20the%20model%20to%20focus%20on%20the%20distribution%0Aof%20positive%20samples%20in%20the%20vicinity%20of%20a%20pixel%2C%20which%20perceives%20environment%0Ainformation.%20It%20treats%20the%20kernel%20pixels%20as%20positive%20samples%20and%20helps%20the%0Amodel%20differentiate%20text%20and%20kernel%20features.%20Extensive%20experiments%20demonstrate%0Athe%20FEM%27s%20ability%20to%20efficiently%20support%20the%20model%20in%20handling%20different%20scale%0Atexts%20and%20confirm%20the%20PEM%20can%20assist%20in%20perceiving%20pixels%20more%20accurately%20by%0Afocusing%20on%20pixel%20vicinities.%20Comparisons%20show%20the%20proposed%20model%20outperforms%0Aexisting%20state-of-the-art%20approaches%20on%20four%20public%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFocus%2520Entirety%2520and%2520Perceive%2520Environment%2520for%2520Arbitrary-Shaped%2520Text%250A%2520%2520Detection%26entry.906535625%3DXu%2520Han%2520and%2520Junyu%2520Gao%2520and%2520Chuang%2520Yang%2520and%2520Yuan%2520Yuan%2520and%2520Qi%2520Wang%26entry.1292438233%3D%2520%2520Due%2520to%2520the%2520diversity%2520of%2520scene%2520text%2520in%2520aspects%2520such%2520as%2520font%252C%2520color%252C%2520shape%252C%2520and%250Asize%252C%2520accurately%2520and%2520efficiently%2520detecting%2520text%2520is%2520still%2520a%2520formidable%250Achallenge.%2520Among%2520the%2520various%2520detection%2520approaches%252C%2520segmentation-based%250Aapproaches%2520have%2520emerged%2520as%2520prominent%2520contenders%2520owing%2520to%2520their%2520flexible%250Apixel-level%2520predictions.%2520However%252C%2520these%2520methods%2520typically%2520model%2520text%2520instances%250Ain%2520a%2520bottom-up%2520manner%252C%2520which%2520is%2520highly%2520susceptible%2520to%2520noise.%2520In%2520addition%252C%2520the%250Aprediction%2520of%2520pixels%2520is%2520isolated%2520without%2520introducing%2520pixel-feature%2520interaction%252C%250Awhich%2520also%2520influences%2520the%2520detection%2520performance.%2520To%2520alleviate%2520these%2520problems%252C%250Awe%2520propose%2520a%2520multi-information%2520level%2520arbitrary-shaped%2520text%2520detector%2520consisting%250Aof%2520a%2520focus%2520entirety%2520module%2520%2528FEM%2529%2520and%2520a%2520perceive%2520environment%2520module%2520%2528PEM%2529.%2520The%250Aformer%2520extracts%2520instance-level%2520features%2520and%2520adopts%2520a%2520top-down%2520scheme%2520to%2520model%250Atexts%2520to%2520reduce%2520the%2520influence%2520of%2520noises.%2520Specifically%252C%2520it%2520assigns%2520consistent%250Aentirety%2520information%2520to%2520pixels%2520within%2520the%2520same%2520instance%2520to%2520improve%2520their%250Acohesion.%2520In%2520addition%252C%2520it%2520emphasizes%2520the%2520scale%2520information%252C%2520enabling%2520the%2520model%250Ato%2520distinguish%2520varying%2520scale%2520texts%2520effectively.%2520The%2520latter%2520extracts%250Aregion-level%2520information%2520and%2520encourages%2520the%2520model%2520to%2520focus%2520on%2520the%2520distribution%250Aof%2520positive%2520samples%2520in%2520the%2520vicinity%2520of%2520a%2520pixel%252C%2520which%2520perceives%2520environment%250Ainformation.%2520It%2520treats%2520the%2520kernel%2520pixels%2520as%2520positive%2520samples%2520and%2520helps%2520the%250Amodel%2520differentiate%2520text%2520and%2520kernel%2520features.%2520Extensive%2520experiments%2520demonstrate%250Athe%2520FEM%2527s%2520ability%2520to%2520efficiently%2520support%2520the%2520model%2520in%2520handling%2520different%2520scale%250Atexts%2520and%2520confirm%2520the%2520PEM%2520can%2520assist%2520in%2520perceiving%2520pixels%2520more%2520accurately%2520by%250Afocusing%2520on%2520pixel%2520vicinities.%2520Comparisons%2520show%2520the%2520proposed%2520model%2520outperforms%250Aexisting%2520state-of-the-art%2520approaches%2520on%2520four%2520public%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Focus%20Entirety%20and%20Perceive%20Environment%20for%20Arbitrary-Shaped%20Text%0A%20%20Detection&entry.906535625=Xu%20Han%20and%20Junyu%20Gao%20and%20Chuang%20Yang%20and%20Yuan%20Yuan%20and%20Qi%20Wang&entry.1292438233=%20%20Due%20to%20the%20diversity%20of%20scene%20text%20in%20aspects%20such%20as%20font%2C%20color%2C%20shape%2C%20and%0Asize%2C%20accurately%20and%20efficiently%20detecting%20text%20is%20still%20a%20formidable%0Achallenge.%20Among%20the%20various%20detection%20approaches%2C%20segmentation-based%0Aapproaches%20have%20emerged%20as%20prominent%20contenders%20owing%20to%20their%20flexible%0Apixel-level%20predictions.%20However%2C%20these%20methods%20typically%20model%20text%20instances%0Ain%20a%20bottom-up%20manner%2C%20which%20is%20highly%20susceptible%20to%20noise.%20In%20addition%2C%20the%0Aprediction%20of%20pixels%20is%20isolated%20without%20introducing%20pixel-feature%20interaction%2C%0Awhich%20also%20influences%20the%20detection%20performance.%20To%20alleviate%20these%20problems%2C%0Awe%20propose%20a%20multi-information%20level%20arbitrary-shaped%20text%20detector%20consisting%0Aof%20a%20focus%20entirety%20module%20%28FEM%29%20and%20a%20perceive%20environment%20module%20%28PEM%29.%20The%0Aformer%20extracts%20instance-level%20features%20and%20adopts%20a%20top-down%20scheme%20to%20model%0Atexts%20to%20reduce%20the%20influence%20of%20noises.%20Specifically%2C%20it%20assigns%20consistent%0Aentirety%20information%20to%20pixels%20within%20the%20same%20instance%20to%20improve%20their%0Acohesion.%20In%20addition%2C%20it%20emphasizes%20the%20scale%20information%2C%20enabling%20the%20model%0Ato%20distinguish%20varying%20scale%20texts%20effectively.%20The%20latter%20extracts%0Aregion-level%20information%20and%20encourages%20the%20model%20to%20focus%20on%20the%20distribution%0Aof%20positive%20samples%20in%20the%20vicinity%20of%20a%20pixel%2C%20which%20perceives%20environment%0Ainformation.%20It%20treats%20the%20kernel%20pixels%20as%20positive%20samples%20and%20helps%20the%0Amodel%20differentiate%20text%20and%20kernel%20features.%20Extensive%20experiments%20demonstrate%0Athe%20FEM%27s%20ability%20to%20efficiently%20support%20the%20model%20in%20handling%20different%20scale%0Atexts%20and%20confirm%20the%20PEM%20can%20assist%20in%20perceiving%20pixels%20more%20accurately%20by%0Afocusing%20on%20pixel%20vicinities.%20Comparisons%20show%20the%20proposed%20model%20outperforms%0Aexisting%20state-of-the-art%20approaches%20on%20four%20public%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16827v1&entry.124074799=Read"},
{"title": "Explicitly Modeling Pre-Cortical Vision with a Neuro-Inspired Front-End\n  Improves CNN Robustness", "author": "Lucas Piper and Arlindo L. Oliveira and Tiago Marques", "abstract": "  While convolutional neural networks (CNNs) excel at clean image\nclassification, they struggle to classify images corrupted with different\ncommon corruptions, limiting their real-world applicability. Recent work has\nshown that incorporating a CNN front-end block that simulates some features of\nthe primate primary visual cortex (V1) can improve overall model robustness.\nHere, we expand on this approach by introducing two novel biologically-inspired\nCNN model families that incorporate a new front-end block designed to simulate\npre-cortical visual processing. RetinaNet, a hybrid architecture containing the\nnovel front-end followed by a standard CNN back-end, shows a relative\nrobustness improvement of 12.3% when compared to the standard model; and EVNet,\nwhich further adds a V1 block after the pre-cortical front-end, shows a\nrelative gain of 18.5%. The improvement in robustness was observed for all the\ndifferent corruption categories, though accompanied by a small decrease in\nclean image accuracy, and generalized to a different back-end architecture.\nThese findings show that simulating multiple stages of early visual processing\nin CNN early layers provides cumulative benefits for model robustness.\n", "link": "http://arxiv.org/abs/2409.16838v1", "date": "2024-09-25", "relevancy": 2.7081, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5518}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5365}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explicitly%20Modeling%20Pre-Cortical%20Vision%20with%20a%20Neuro-Inspired%20Front-End%0A%20%20Improves%20CNN%20Robustness&body=Title%3A%20Explicitly%20Modeling%20Pre-Cortical%20Vision%20with%20a%20Neuro-Inspired%20Front-End%0A%20%20Improves%20CNN%20Robustness%0AAuthor%3A%20Lucas%20Piper%20and%20Arlindo%20L.%20Oliveira%20and%20Tiago%20Marques%0AAbstract%3A%20%20%20While%20convolutional%20neural%20networks%20%28CNNs%29%20excel%20at%20clean%20image%0Aclassification%2C%20they%20struggle%20to%20classify%20images%20corrupted%20with%20different%0Acommon%20corruptions%2C%20limiting%20their%20real-world%20applicability.%20Recent%20work%20has%0Ashown%20that%20incorporating%20a%20CNN%20front-end%20block%20that%20simulates%20some%20features%20of%0Athe%20primate%20primary%20visual%20cortex%20%28V1%29%20can%20improve%20overall%20model%20robustness.%0AHere%2C%20we%20expand%20on%20this%20approach%20by%20introducing%20two%20novel%20biologically-inspired%0ACNN%20model%20families%20that%20incorporate%20a%20new%20front-end%20block%20designed%20to%20simulate%0Apre-cortical%20visual%20processing.%20RetinaNet%2C%20a%20hybrid%20architecture%20containing%20the%0Anovel%20front-end%20followed%20by%20a%20standard%20CNN%20back-end%2C%20shows%20a%20relative%0Arobustness%20improvement%20of%2012.3%25%20when%20compared%20to%20the%20standard%20model%3B%20and%20EVNet%2C%0Awhich%20further%20adds%20a%20V1%20block%20after%20the%20pre-cortical%20front-end%2C%20shows%20a%0Arelative%20gain%20of%2018.5%25.%20The%20improvement%20in%20robustness%20was%20observed%20for%20all%20the%0Adifferent%20corruption%20categories%2C%20though%20accompanied%20by%20a%20small%20decrease%20in%0Aclean%20image%20accuracy%2C%20and%20generalized%20to%20a%20different%20back-end%20architecture.%0AThese%20findings%20show%20that%20simulating%20multiple%20stages%20of%20early%20visual%20processing%0Ain%20CNN%20early%20layers%20provides%20cumulative%20benefits%20for%20model%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16838v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplicitly%2520Modeling%2520Pre-Cortical%2520Vision%2520with%2520a%2520Neuro-Inspired%2520Front-End%250A%2520%2520Improves%2520CNN%2520Robustness%26entry.906535625%3DLucas%2520Piper%2520and%2520Arlindo%2520L.%2520Oliveira%2520and%2520Tiago%2520Marques%26entry.1292438233%3D%2520%2520While%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520excel%2520at%2520clean%2520image%250Aclassification%252C%2520they%2520struggle%2520to%2520classify%2520images%2520corrupted%2520with%2520different%250Acommon%2520corruptions%252C%2520limiting%2520their%2520real-world%2520applicability.%2520Recent%2520work%2520has%250Ashown%2520that%2520incorporating%2520a%2520CNN%2520front-end%2520block%2520that%2520simulates%2520some%2520features%2520of%250Athe%2520primate%2520primary%2520visual%2520cortex%2520%2528V1%2529%2520can%2520improve%2520overall%2520model%2520robustness.%250AHere%252C%2520we%2520expand%2520on%2520this%2520approach%2520by%2520introducing%2520two%2520novel%2520biologically-inspired%250ACNN%2520model%2520families%2520that%2520incorporate%2520a%2520new%2520front-end%2520block%2520designed%2520to%2520simulate%250Apre-cortical%2520visual%2520processing.%2520RetinaNet%252C%2520a%2520hybrid%2520architecture%2520containing%2520the%250Anovel%2520front-end%2520followed%2520by%2520a%2520standard%2520CNN%2520back-end%252C%2520shows%2520a%2520relative%250Arobustness%2520improvement%2520of%252012.3%2525%2520when%2520compared%2520to%2520the%2520standard%2520model%253B%2520and%2520EVNet%252C%250Awhich%2520further%2520adds%2520a%2520V1%2520block%2520after%2520the%2520pre-cortical%2520front-end%252C%2520shows%2520a%250Arelative%2520gain%2520of%252018.5%2525.%2520The%2520improvement%2520in%2520robustness%2520was%2520observed%2520for%2520all%2520the%250Adifferent%2520corruption%2520categories%252C%2520though%2520accompanied%2520by%2520a%2520small%2520decrease%2520in%250Aclean%2520image%2520accuracy%252C%2520and%2520generalized%2520to%2520a%2520different%2520back-end%2520architecture.%250AThese%2520findings%2520show%2520that%2520simulating%2520multiple%2520stages%2520of%2520early%2520visual%2520processing%250Ain%2520CNN%2520early%2520layers%2520provides%2520cumulative%2520benefits%2520for%2520model%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16838v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explicitly%20Modeling%20Pre-Cortical%20Vision%20with%20a%20Neuro-Inspired%20Front-End%0A%20%20Improves%20CNN%20Robustness&entry.906535625=Lucas%20Piper%20and%20Arlindo%20L.%20Oliveira%20and%20Tiago%20Marques&entry.1292438233=%20%20While%20convolutional%20neural%20networks%20%28CNNs%29%20excel%20at%20clean%20image%0Aclassification%2C%20they%20struggle%20to%20classify%20images%20corrupted%20with%20different%0Acommon%20corruptions%2C%20limiting%20their%20real-world%20applicability.%20Recent%20work%20has%0Ashown%20that%20incorporating%20a%20CNN%20front-end%20block%20that%20simulates%20some%20features%20of%0Athe%20primate%20primary%20visual%20cortex%20%28V1%29%20can%20improve%20overall%20model%20robustness.%0AHere%2C%20we%20expand%20on%20this%20approach%20by%20introducing%20two%20novel%20biologically-inspired%0ACNN%20model%20families%20that%20incorporate%20a%20new%20front-end%20block%20designed%20to%20simulate%0Apre-cortical%20visual%20processing.%20RetinaNet%2C%20a%20hybrid%20architecture%20containing%20the%0Anovel%20front-end%20followed%20by%20a%20standard%20CNN%20back-end%2C%20shows%20a%20relative%0Arobustness%20improvement%20of%2012.3%25%20when%20compared%20to%20the%20standard%20model%3B%20and%20EVNet%2C%0Awhich%20further%20adds%20a%20V1%20block%20after%20the%20pre-cortical%20front-end%2C%20shows%20a%0Arelative%20gain%20of%2018.5%25.%20The%20improvement%20in%20robustness%20was%20observed%20for%20all%20the%0Adifferent%20corruption%20categories%2C%20though%20accompanied%20by%20a%20small%20decrease%20in%0Aclean%20image%20accuracy%2C%20and%20generalized%20to%20a%20different%20back-end%20architecture.%0AThese%20findings%20show%20that%20simulating%20multiple%20stages%20of%20early%20visual%20processing%0Ain%20CNN%20early%20layers%20provides%20cumulative%20benefits%20for%20model%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16838v1&entry.124074799=Read"},
{"title": "Spotlight Text Detector: Spotlight on Candidate Regions Like a Camera", "author": "Xu Han and Junyu Gao and Chuang Yang and Yuan Yuan and Qi Wang", "abstract": "  The irregular contour representation is one of the tough challenges in scene\ntext detection. Although segmentation-based methods have achieved significant\nprogress with the help of flexible pixel prediction, the overlap of\ngeographically close texts hinders detecting them separately. To alleviate this\nproblem, some shrink-based methods predict text kernels and expand them to\nrestructure texts. However, the text kernel is an artificial object with\nincomplete semantic features that are prone to incorrect or missing detection.\nIn addition, different from the general objects, the geometry features (aspect\nratio, scale, and shape) of scene texts vary significantly, which makes it\ndifficult to detect them accurately. To consider the above problems, we propose\nan effective spotlight text detector (STD), which consists of a spotlight\ncalibration module (SCM) and a multivariate information extraction module\n(MIEM). The former concentrates efforts on the candidate kernel, like a camera\nfocus on the target. It obtains candidate features through a mapping filter and\ncalibrates them precisely to eliminate some false positive samples. The latter\ndesigns different shape schemes to explore multiple geometric features for\nscene texts. It helps extract various spatial relationships to improve the\nmodel's ability to recognize kernel regions. Ablation studies prove the\neffectiveness of the designed SCM and MIEM. Extensive experiments verify that\nour STD is superior to existing state-of-the-art methods on various datasets,\nincluding ICDAR2015, CTW1500, MSRA-TD500, and Total-Text.\n", "link": "http://arxiv.org/abs/2409.16820v1", "date": "2024-09-25", "relevancy": 2.6848, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5403}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5403}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spotlight%20Text%20Detector%3A%20Spotlight%20on%20Candidate%20Regions%20Like%20a%20Camera&body=Title%3A%20Spotlight%20Text%20Detector%3A%20Spotlight%20on%20Candidate%20Regions%20Like%20a%20Camera%0AAuthor%3A%20Xu%20Han%20and%20Junyu%20Gao%20and%20Chuang%20Yang%20and%20Yuan%20Yuan%20and%20Qi%20Wang%0AAbstract%3A%20%20%20The%20irregular%20contour%20representation%20is%20one%20of%20the%20tough%20challenges%20in%20scene%0Atext%20detection.%20Although%20segmentation-based%20methods%20have%20achieved%20significant%0Aprogress%20with%20the%20help%20of%20flexible%20pixel%20prediction%2C%20the%20overlap%20of%0Ageographically%20close%20texts%20hinders%20detecting%20them%20separately.%20To%20alleviate%20this%0Aproblem%2C%20some%20shrink-based%20methods%20predict%20text%20kernels%20and%20expand%20them%20to%0Arestructure%20texts.%20However%2C%20the%20text%20kernel%20is%20an%20artificial%20object%20with%0Aincomplete%20semantic%20features%20that%20are%20prone%20to%20incorrect%20or%20missing%20detection.%0AIn%20addition%2C%20different%20from%20the%20general%20objects%2C%20the%20geometry%20features%20%28aspect%0Aratio%2C%20scale%2C%20and%20shape%29%20of%20scene%20texts%20vary%20significantly%2C%20which%20makes%20it%0Adifficult%20to%20detect%20them%20accurately.%20To%20consider%20the%20above%20problems%2C%20we%20propose%0Aan%20effective%20spotlight%20text%20detector%20%28STD%29%2C%20which%20consists%20of%20a%20spotlight%0Acalibration%20module%20%28SCM%29%20and%20a%20multivariate%20information%20extraction%20module%0A%28MIEM%29.%20The%20former%20concentrates%20efforts%20on%20the%20candidate%20kernel%2C%20like%20a%20camera%0Afocus%20on%20the%20target.%20It%20obtains%20candidate%20features%20through%20a%20mapping%20filter%20and%0Acalibrates%20them%20precisely%20to%20eliminate%20some%20false%20positive%20samples.%20The%20latter%0Adesigns%20different%20shape%20schemes%20to%20explore%20multiple%20geometric%20features%20for%0Ascene%20texts.%20It%20helps%20extract%20various%20spatial%20relationships%20to%20improve%20the%0Amodel%27s%20ability%20to%20recognize%20kernel%20regions.%20Ablation%20studies%20prove%20the%0Aeffectiveness%20of%20the%20designed%20SCM%20and%20MIEM.%20Extensive%20experiments%20verify%20that%0Aour%20STD%20is%20superior%20to%20existing%20state-of-the-art%20methods%20on%20various%20datasets%2C%0Aincluding%20ICDAR2015%2C%20CTW1500%2C%20MSRA-TD500%2C%20and%20Total-Text.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpotlight%2520Text%2520Detector%253A%2520Spotlight%2520on%2520Candidate%2520Regions%2520Like%2520a%2520Camera%26entry.906535625%3DXu%2520Han%2520and%2520Junyu%2520Gao%2520and%2520Chuang%2520Yang%2520and%2520Yuan%2520Yuan%2520and%2520Qi%2520Wang%26entry.1292438233%3D%2520%2520The%2520irregular%2520contour%2520representation%2520is%2520one%2520of%2520the%2520tough%2520challenges%2520in%2520scene%250Atext%2520detection.%2520Although%2520segmentation-based%2520methods%2520have%2520achieved%2520significant%250Aprogress%2520with%2520the%2520help%2520of%2520flexible%2520pixel%2520prediction%252C%2520the%2520overlap%2520of%250Ageographically%2520close%2520texts%2520hinders%2520detecting%2520them%2520separately.%2520To%2520alleviate%2520this%250Aproblem%252C%2520some%2520shrink-based%2520methods%2520predict%2520text%2520kernels%2520and%2520expand%2520them%2520to%250Arestructure%2520texts.%2520However%252C%2520the%2520text%2520kernel%2520is%2520an%2520artificial%2520object%2520with%250Aincomplete%2520semantic%2520features%2520that%2520are%2520prone%2520to%2520incorrect%2520or%2520missing%2520detection.%250AIn%2520addition%252C%2520different%2520from%2520the%2520general%2520objects%252C%2520the%2520geometry%2520features%2520%2528aspect%250Aratio%252C%2520scale%252C%2520and%2520shape%2529%2520of%2520scene%2520texts%2520vary%2520significantly%252C%2520which%2520makes%2520it%250Adifficult%2520to%2520detect%2520them%2520accurately.%2520To%2520consider%2520the%2520above%2520problems%252C%2520we%2520propose%250Aan%2520effective%2520spotlight%2520text%2520detector%2520%2528STD%2529%252C%2520which%2520consists%2520of%2520a%2520spotlight%250Acalibration%2520module%2520%2528SCM%2529%2520and%2520a%2520multivariate%2520information%2520extraction%2520module%250A%2528MIEM%2529.%2520The%2520former%2520concentrates%2520efforts%2520on%2520the%2520candidate%2520kernel%252C%2520like%2520a%2520camera%250Afocus%2520on%2520the%2520target.%2520It%2520obtains%2520candidate%2520features%2520through%2520a%2520mapping%2520filter%2520and%250Acalibrates%2520them%2520precisely%2520to%2520eliminate%2520some%2520false%2520positive%2520samples.%2520The%2520latter%250Adesigns%2520different%2520shape%2520schemes%2520to%2520explore%2520multiple%2520geometric%2520features%2520for%250Ascene%2520texts.%2520It%2520helps%2520extract%2520various%2520spatial%2520relationships%2520to%2520improve%2520the%250Amodel%2527s%2520ability%2520to%2520recognize%2520kernel%2520regions.%2520Ablation%2520studies%2520prove%2520the%250Aeffectiveness%2520of%2520the%2520designed%2520SCM%2520and%2520MIEM.%2520Extensive%2520experiments%2520verify%2520that%250Aour%2520STD%2520is%2520superior%2520to%2520existing%2520state-of-the-art%2520methods%2520on%2520various%2520datasets%252C%250Aincluding%2520ICDAR2015%252C%2520CTW1500%252C%2520MSRA-TD500%252C%2520and%2520Total-Text.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spotlight%20Text%20Detector%3A%20Spotlight%20on%20Candidate%20Regions%20Like%20a%20Camera&entry.906535625=Xu%20Han%20and%20Junyu%20Gao%20and%20Chuang%20Yang%20and%20Yuan%20Yuan%20and%20Qi%20Wang&entry.1292438233=%20%20The%20irregular%20contour%20representation%20is%20one%20of%20the%20tough%20challenges%20in%20scene%0Atext%20detection.%20Although%20segmentation-based%20methods%20have%20achieved%20significant%0Aprogress%20with%20the%20help%20of%20flexible%20pixel%20prediction%2C%20the%20overlap%20of%0Ageographically%20close%20texts%20hinders%20detecting%20them%20separately.%20To%20alleviate%20this%0Aproblem%2C%20some%20shrink-based%20methods%20predict%20text%20kernels%20and%20expand%20them%20to%0Arestructure%20texts.%20However%2C%20the%20text%20kernel%20is%20an%20artificial%20object%20with%0Aincomplete%20semantic%20features%20that%20are%20prone%20to%20incorrect%20or%20missing%20detection.%0AIn%20addition%2C%20different%20from%20the%20general%20objects%2C%20the%20geometry%20features%20%28aspect%0Aratio%2C%20scale%2C%20and%20shape%29%20of%20scene%20texts%20vary%20significantly%2C%20which%20makes%20it%0Adifficult%20to%20detect%20them%20accurately.%20To%20consider%20the%20above%20problems%2C%20we%20propose%0Aan%20effective%20spotlight%20text%20detector%20%28STD%29%2C%20which%20consists%20of%20a%20spotlight%0Acalibration%20module%20%28SCM%29%20and%20a%20multivariate%20information%20extraction%20module%0A%28MIEM%29.%20The%20former%20concentrates%20efforts%20on%20the%20candidate%20kernel%2C%20like%20a%20camera%0Afocus%20on%20the%20target.%20It%20obtains%20candidate%20features%20through%20a%20mapping%20filter%20and%0Acalibrates%20them%20precisely%20to%20eliminate%20some%20false%20positive%20samples.%20The%20latter%0Adesigns%20different%20shape%20schemes%20to%20explore%20multiple%20geometric%20features%20for%0Ascene%20texts.%20It%20helps%20extract%20various%20spatial%20relationships%20to%20improve%20the%0Amodel%27s%20ability%20to%20recognize%20kernel%20regions.%20Ablation%20studies%20prove%20the%0Aeffectiveness%20of%20the%20designed%20SCM%20and%20MIEM.%20Extensive%20experiments%20verify%20that%0Aour%20STD%20is%20superior%20to%20existing%20state-of-the-art%20methods%20on%20various%20datasets%2C%0Aincluding%20ICDAR2015%2C%20CTW1500%2C%20MSRA-TD500%2C%20and%20Total-Text.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16820v1&entry.124074799=Read"},
{"title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse\n  Autoencoders", "author": "David Chanin and James Wilken-Smith and Tom\u00e1\u0161 Dulka and Hardik Bhatnagar and Joseph Bloom", "abstract": "  Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose\nthe activations of Large Language Models (LLMs) into human-interpretable\nlatents. In this paper, we pose two questions. First, to what extent do SAEs\nextract monosemantic and interpretable latents? Second, to what extent does\nvarying the sparsity or the size of the SAE affect monosemanticity /\ninterpretability? By investigating these questions in the context of a simple\nfirst-letter identification task where we have complete access to ground truth\nlabels for all tokens in the vocabulary, we are able to provide more detail\nthan prior investigations. Critically, we identify a problematic form of\nfeature-splitting we call feature absorption where seemingly monosemantic\nlatents fail to fire in cases where they clearly should. Our investigation\nsuggests that varying SAE size or sparsity is insufficient to solve this issue,\nand that there are deeper conceptual issues in need of resolution.\n", "link": "http://arxiv.org/abs/2409.14507v3", "date": "2024-09-25", "relevancy": 2.6811, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5417}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5417}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20is%20for%20Absorption%3A%20Studying%20Feature%20Splitting%20and%20Absorption%20in%20Sparse%0A%20%20Autoencoders&body=Title%3A%20A%20is%20for%20Absorption%3A%20Studying%20Feature%20Splitting%20and%20Absorption%20in%20Sparse%0A%20%20Autoencoders%0AAuthor%3A%20David%20Chanin%20and%20James%20Wilken-Smith%20and%20Tom%C3%A1%C5%A1%20Dulka%20and%20Hardik%20Bhatnagar%20and%20Joseph%20Bloom%0AAbstract%3A%20%20%20Sparse%20Autoencoders%20%28SAEs%29%20have%20emerged%20as%20a%20promising%20approach%20to%20decompose%0Athe%20activations%20of%20Large%20Language%20Models%20%28LLMs%29%20into%20human-interpretable%0Alatents.%20In%20this%20paper%2C%20we%20pose%20two%20questions.%20First%2C%20to%20what%20extent%20do%20SAEs%0Aextract%20monosemantic%20and%20interpretable%20latents%3F%20Second%2C%20to%20what%20extent%20does%0Avarying%20the%20sparsity%20or%20the%20size%20of%20the%20SAE%20affect%20monosemanticity%20/%0Ainterpretability%3F%20By%20investigating%20these%20questions%20in%20the%20context%20of%20a%20simple%0Afirst-letter%20identification%20task%20where%20we%20have%20complete%20access%20to%20ground%20truth%0Alabels%20for%20all%20tokens%20in%20the%20vocabulary%2C%20we%20are%20able%20to%20provide%20more%20detail%0Athan%20prior%20investigations.%20Critically%2C%20we%20identify%20a%20problematic%20form%20of%0Afeature-splitting%20we%20call%20feature%20absorption%20where%20seemingly%20monosemantic%0Alatents%20fail%20to%20fire%20in%20cases%20where%20they%20clearly%20should.%20Our%20investigation%0Asuggests%20that%20varying%20SAE%20size%20or%20sparsity%20is%20insufficient%20to%20solve%20this%20issue%2C%0Aand%20that%20there%20are%20deeper%20conceptual%20issues%20in%20need%20of%20resolution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.14507v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520is%2520for%2520Absorption%253A%2520Studying%2520Feature%2520Splitting%2520and%2520Absorption%2520in%2520Sparse%250A%2520%2520Autoencoders%26entry.906535625%3DDavid%2520Chanin%2520and%2520James%2520Wilken-Smith%2520and%2520Tom%25C3%25A1%25C5%25A1%2520Dulka%2520and%2520Hardik%2520Bhatnagar%2520and%2520Joseph%2520Bloom%26entry.1292438233%3D%2520%2520Sparse%2520Autoencoders%2520%2528SAEs%2529%2520have%2520emerged%2520as%2520a%2520promising%2520approach%2520to%2520decompose%250Athe%2520activations%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520into%2520human-interpretable%250Alatents.%2520In%2520this%2520paper%252C%2520we%2520pose%2520two%2520questions.%2520First%252C%2520to%2520what%2520extent%2520do%2520SAEs%250Aextract%2520monosemantic%2520and%2520interpretable%2520latents%253F%2520Second%252C%2520to%2520what%2520extent%2520does%250Avarying%2520the%2520sparsity%2520or%2520the%2520size%2520of%2520the%2520SAE%2520affect%2520monosemanticity%2520/%250Ainterpretability%253F%2520By%2520investigating%2520these%2520questions%2520in%2520the%2520context%2520of%2520a%2520simple%250Afirst-letter%2520identification%2520task%2520where%2520we%2520have%2520complete%2520access%2520to%2520ground%2520truth%250Alabels%2520for%2520all%2520tokens%2520in%2520the%2520vocabulary%252C%2520we%2520are%2520able%2520to%2520provide%2520more%2520detail%250Athan%2520prior%2520investigations.%2520Critically%252C%2520we%2520identify%2520a%2520problematic%2520form%2520of%250Afeature-splitting%2520we%2520call%2520feature%2520absorption%2520where%2520seemingly%2520monosemantic%250Alatents%2520fail%2520to%2520fire%2520in%2520cases%2520where%2520they%2520clearly%2520should.%2520Our%2520investigation%250Asuggests%2520that%2520varying%2520SAE%2520size%2520or%2520sparsity%2520is%2520insufficient%2520to%2520solve%2520this%2520issue%252C%250Aand%2520that%2520there%2520are%2520deeper%2520conceptual%2520issues%2520in%2520need%2520of%2520resolution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.14507v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20is%20for%20Absorption%3A%20Studying%20Feature%20Splitting%20and%20Absorption%20in%20Sparse%0A%20%20Autoencoders&entry.906535625=David%20Chanin%20and%20James%20Wilken-Smith%20and%20Tom%C3%A1%C5%A1%20Dulka%20and%20Hardik%20Bhatnagar%20and%20Joseph%20Bloom&entry.1292438233=%20%20Sparse%20Autoencoders%20%28SAEs%29%20have%20emerged%20as%20a%20promising%20approach%20to%20decompose%0Athe%20activations%20of%20Large%20Language%20Models%20%28LLMs%29%20into%20human-interpretable%0Alatents.%20In%20this%20paper%2C%20we%20pose%20two%20questions.%20First%2C%20to%20what%20extent%20do%20SAEs%0Aextract%20monosemantic%20and%20interpretable%20latents%3F%20Second%2C%20to%20what%20extent%20does%0Avarying%20the%20sparsity%20or%20the%20size%20of%20the%20SAE%20affect%20monosemanticity%20/%0Ainterpretability%3F%20By%20investigating%20these%20questions%20in%20the%20context%20of%20a%20simple%0Afirst-letter%20identification%20task%20where%20we%20have%20complete%20access%20to%20ground%20truth%0Alabels%20for%20all%20tokens%20in%20the%20vocabulary%2C%20we%20are%20able%20to%20provide%20more%20detail%0Athan%20prior%20investigations.%20Critically%2C%20we%20identify%20a%20problematic%20form%20of%0Afeature-splitting%20we%20call%20feature%20absorption%20where%20seemingly%20monosemantic%0Alatents%20fail%20to%20fire%20in%20cases%20where%20they%20clearly%20should.%20Our%20investigation%0Asuggests%20that%20varying%20SAE%20size%20or%20sparsity%20is%20insufficient%20to%20solve%20this%20issue%2C%0Aand%20that%20there%20are%20deeper%20conceptual%20issues%20in%20need%20of%20resolution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.14507v3&entry.124074799=Read"},
{"title": "How to Connect Speech Foundation Models and Large Language Models? What\n  Matters and What Does Not", "author": "Francesco Verdini and Pierfrancesco Melucci and Stefano Perna and Francesco Cariaggi and Marco Gaido and Sara Papi and Szymon Mazurek and Marek Kasztelnik and Luisa Bentivogli and S\u00e9bastien Brati\u00e8res and Paolo Merialdo and Simone Scardapane", "abstract": "  The remarkable performance achieved by Large Language Models (LLM) has driven\nresearch efforts to leverage them for a wide range of tasks and input\nmodalities. In speech-to-text (S2T) tasks, the emerging solution consists of\nprojecting the output of the encoder of a Speech Foundational Model (SFM) into\nthe LLM embedding space through an adapter module. However, no work has yet\ninvestigated how much the downstream-task performance depends on each component\n(SFM, adapter, LLM) nor whether the best design of the adapter depends on the\nchosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter\nmodules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on\ntwo widespread S2T tasks, namely Automatic Speech Recognition and Speech\nTranslation. Our results demonstrate that the SFM plays a pivotal role in\ndownstream performance, while the adapter choice has moderate impact and\ndepends on the SFM and LLM.\n", "link": "http://arxiv.org/abs/2409.17044v1", "date": "2024-09-25", "relevancy": 2.6511, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5347}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5347}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Connect%20Speech%20Foundation%20Models%20and%20Large%20Language%20Models%3F%20What%0A%20%20Matters%20and%20What%20Does%20Not&body=Title%3A%20How%20to%20Connect%20Speech%20Foundation%20Models%20and%20Large%20Language%20Models%3F%20What%0A%20%20Matters%20and%20What%20Does%20Not%0AAuthor%3A%20Francesco%20Verdini%20and%20Pierfrancesco%20Melucci%20and%20Stefano%20Perna%20and%20Francesco%20Cariaggi%20and%20Marco%20Gaido%20and%20Sara%20Papi%20and%20Szymon%20Mazurek%20and%20Marek%20Kasztelnik%20and%20Luisa%20Bentivogli%20and%20S%C3%A9bastien%20Brati%C3%A8res%20and%20Paolo%20Merialdo%20and%20Simone%20Scardapane%0AAbstract%3A%20%20%20The%20remarkable%20performance%20achieved%20by%20Large%20Language%20Models%20%28LLM%29%20has%20driven%0Aresearch%20efforts%20to%20leverage%20them%20for%20a%20wide%20range%20of%20tasks%20and%20input%0Amodalities.%20In%20speech-to-text%20%28S2T%29%20tasks%2C%20the%20emerging%20solution%20consists%20of%0Aprojecting%20the%20output%20of%20the%20encoder%20of%20a%20Speech%20Foundational%20Model%20%28SFM%29%20into%0Athe%20LLM%20embedding%20space%20through%20an%20adapter%20module.%20However%2C%20no%20work%20has%20yet%0Ainvestigated%20how%20much%20the%20downstream-task%20performance%20depends%20on%20each%20component%0A%28SFM%2C%20adapter%2C%20LLM%29%20nor%20whether%20the%20best%20design%20of%20the%20adapter%20depends%20on%20the%0Achosen%20SFM%20and%20LLM.%20To%20fill%20this%20gap%2C%20we%20evaluate%20the%20combination%20of%205%20adapter%0Amodules%2C%202%20LLMs%20%28Mistral%20and%20Llama%29%2C%20and%202%20SFMs%20%28Whisper%20and%20SeamlessM4T%29%20on%0Atwo%20widespread%20S2T%20tasks%2C%20namely%20Automatic%20Speech%20Recognition%20and%20Speech%0ATranslation.%20Our%20results%20demonstrate%20that%20the%20SFM%20plays%20a%20pivotal%20role%20in%0Adownstream%20performance%2C%20while%20the%20adapter%20choice%20has%20moderate%20impact%20and%0Adepends%20on%20the%20SFM%20and%20LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17044v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Connect%2520Speech%2520Foundation%2520Models%2520and%2520Large%2520Language%2520Models%253F%2520What%250A%2520%2520Matters%2520and%2520What%2520Does%2520Not%26entry.906535625%3DFrancesco%2520Verdini%2520and%2520Pierfrancesco%2520Melucci%2520and%2520Stefano%2520Perna%2520and%2520Francesco%2520Cariaggi%2520and%2520Marco%2520Gaido%2520and%2520Sara%2520Papi%2520and%2520Szymon%2520Mazurek%2520and%2520Marek%2520Kasztelnik%2520and%2520Luisa%2520Bentivogli%2520and%2520S%25C3%25A9bastien%2520Brati%25C3%25A8res%2520and%2520Paolo%2520Merialdo%2520and%2520Simone%2520Scardapane%26entry.1292438233%3D%2520%2520The%2520remarkable%2520performance%2520achieved%2520by%2520Large%2520Language%2520Models%2520%2528LLM%2529%2520has%2520driven%250Aresearch%2520efforts%2520to%2520leverage%2520them%2520for%2520a%2520wide%2520range%2520of%2520tasks%2520and%2520input%250Amodalities.%2520In%2520speech-to-text%2520%2528S2T%2529%2520tasks%252C%2520the%2520emerging%2520solution%2520consists%2520of%250Aprojecting%2520the%2520output%2520of%2520the%2520encoder%2520of%2520a%2520Speech%2520Foundational%2520Model%2520%2528SFM%2529%2520into%250Athe%2520LLM%2520embedding%2520space%2520through%2520an%2520adapter%2520module.%2520However%252C%2520no%2520work%2520has%2520yet%250Ainvestigated%2520how%2520much%2520the%2520downstream-task%2520performance%2520depends%2520on%2520each%2520component%250A%2528SFM%252C%2520adapter%252C%2520LLM%2529%2520nor%2520whether%2520the%2520best%2520design%2520of%2520the%2520adapter%2520depends%2520on%2520the%250Achosen%2520SFM%2520and%2520LLM.%2520To%2520fill%2520this%2520gap%252C%2520we%2520evaluate%2520the%2520combination%2520of%25205%2520adapter%250Amodules%252C%25202%2520LLMs%2520%2528Mistral%2520and%2520Llama%2529%252C%2520and%25202%2520SFMs%2520%2528Whisper%2520and%2520SeamlessM4T%2529%2520on%250Atwo%2520widespread%2520S2T%2520tasks%252C%2520namely%2520Automatic%2520Speech%2520Recognition%2520and%2520Speech%250ATranslation.%2520Our%2520results%2520demonstrate%2520that%2520the%2520SFM%2520plays%2520a%2520pivotal%2520role%2520in%250Adownstream%2520performance%252C%2520while%2520the%2520adapter%2520choice%2520has%2520moderate%2520impact%2520and%250Adepends%2520on%2520the%2520SFM%2520and%2520LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17044v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Connect%20Speech%20Foundation%20Models%20and%20Large%20Language%20Models%3F%20What%0A%20%20Matters%20and%20What%20Does%20Not&entry.906535625=Francesco%20Verdini%20and%20Pierfrancesco%20Melucci%20and%20Stefano%20Perna%20and%20Francesco%20Cariaggi%20and%20Marco%20Gaido%20and%20Sara%20Papi%20and%20Szymon%20Mazurek%20and%20Marek%20Kasztelnik%20and%20Luisa%20Bentivogli%20and%20S%C3%A9bastien%20Brati%C3%A8res%20and%20Paolo%20Merialdo%20and%20Simone%20Scardapane&entry.1292438233=%20%20The%20remarkable%20performance%20achieved%20by%20Large%20Language%20Models%20%28LLM%29%20has%20driven%0Aresearch%20efforts%20to%20leverage%20them%20for%20a%20wide%20range%20of%20tasks%20and%20input%0Amodalities.%20In%20speech-to-text%20%28S2T%29%20tasks%2C%20the%20emerging%20solution%20consists%20of%0Aprojecting%20the%20output%20of%20the%20encoder%20of%20a%20Speech%20Foundational%20Model%20%28SFM%29%20into%0Athe%20LLM%20embedding%20space%20through%20an%20adapter%20module.%20However%2C%20no%20work%20has%20yet%0Ainvestigated%20how%20much%20the%20downstream-task%20performance%20depends%20on%20each%20component%0A%28SFM%2C%20adapter%2C%20LLM%29%20nor%20whether%20the%20best%20design%20of%20the%20adapter%20depends%20on%20the%0Achosen%20SFM%20and%20LLM.%20To%20fill%20this%20gap%2C%20we%20evaluate%20the%20combination%20of%205%20adapter%0Amodules%2C%202%20LLMs%20%28Mistral%20and%20Llama%29%2C%20and%202%20SFMs%20%28Whisper%20and%20SeamlessM4T%29%20on%0Atwo%20widespread%20S2T%20tasks%2C%20namely%20Automatic%20Speech%20Recognition%20and%20Speech%0ATranslation.%20Our%20results%20demonstrate%20that%20the%20SFM%20plays%20a%20pivotal%20role%20in%0Adownstream%20performance%2C%20while%20the%20adapter%20choice%20has%20moderate%20impact%20and%0Adepends%20on%20the%20SFM%20and%20LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17044v1&entry.124074799=Read"},
{"title": "Generative Object Insertion in Gaussian Splatting with a Multi-View\n  Diffusion Model", "author": "Hongliang Zhong and Can Wang and Jingbo Zhang and Jing Liao", "abstract": "  Generating and inserting new objects into 3D content is a compelling approach\nfor achieving versatile scene recreation. Existing methods, which rely on SDS\noptimization or single-view inpainting, often struggle to produce high-quality\nresults. To address this, we propose a novel method for object insertion in 3D\ncontent represented by Gaussian Splatting. Our approach introduces a multi-view\ndiffusion model, dubbed MVInpainter, which is built upon a pre-trained stable\nvideo diffusion model to facilitate view-consistent object inpainting. Within\nMVInpainter, we incorporate a ControlNet-based conditional injection module to\nenable controlled and more predictable multi-view generation. After generating\nthe multi-view inpainted results, we further propose a mask-aware 3D\nreconstruction technique to refine Gaussian Splatting reconstruction from these\nsparse inpainted views. By leveraging these fabricate techniques, our approach\nyields diverse results, ensures view-consistent and harmonious insertions, and\nproduces better object quality. Extensive experiments demonstrate that our\napproach outperforms existing methods.\n", "link": "http://arxiv.org/abs/2409.16938v1", "date": "2024-09-25", "relevancy": 2.6124, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6905}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6629}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Object%20Insertion%20in%20Gaussian%20Splatting%20with%20a%20Multi-View%0A%20%20Diffusion%20Model&body=Title%3A%20Generative%20Object%20Insertion%20in%20Gaussian%20Splatting%20with%20a%20Multi-View%0A%20%20Diffusion%20Model%0AAuthor%3A%20Hongliang%20Zhong%20and%20Can%20Wang%20and%20Jingbo%20Zhang%20and%20Jing%20Liao%0AAbstract%3A%20%20%20Generating%20and%20inserting%20new%20objects%20into%203D%20content%20is%20a%20compelling%20approach%0Afor%20achieving%20versatile%20scene%20recreation.%20Existing%20methods%2C%20which%20rely%20on%20SDS%0Aoptimization%20or%20single-view%20inpainting%2C%20often%20struggle%20to%20produce%20high-quality%0Aresults.%20To%20address%20this%2C%20we%20propose%20a%20novel%20method%20for%20object%20insertion%20in%203D%0Acontent%20represented%20by%20Gaussian%20Splatting.%20Our%20approach%20introduces%20a%20multi-view%0Adiffusion%20model%2C%20dubbed%20MVInpainter%2C%20which%20is%20built%20upon%20a%20pre-trained%20stable%0Avideo%20diffusion%20model%20to%20facilitate%20view-consistent%20object%20inpainting.%20Within%0AMVInpainter%2C%20we%20incorporate%20a%20ControlNet-based%20conditional%20injection%20module%20to%0Aenable%20controlled%20and%20more%20predictable%20multi-view%20generation.%20After%20generating%0Athe%20multi-view%20inpainted%20results%2C%20we%20further%20propose%20a%20mask-aware%203D%0Areconstruction%20technique%20to%20refine%20Gaussian%20Splatting%20reconstruction%20from%20these%0Asparse%20inpainted%20views.%20By%20leveraging%20these%20fabricate%20techniques%2C%20our%20approach%0Ayields%20diverse%20results%2C%20ensures%20view-consistent%20and%20harmonious%20insertions%2C%20and%0Aproduces%20better%20object%20quality.%20Extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20outperforms%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16938v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Object%2520Insertion%2520in%2520Gaussian%2520Splatting%2520with%2520a%2520Multi-View%250A%2520%2520Diffusion%2520Model%26entry.906535625%3DHongliang%2520Zhong%2520and%2520Can%2520Wang%2520and%2520Jingbo%2520Zhang%2520and%2520Jing%2520Liao%26entry.1292438233%3D%2520%2520Generating%2520and%2520inserting%2520new%2520objects%2520into%25203D%2520content%2520is%2520a%2520compelling%2520approach%250Afor%2520achieving%2520versatile%2520scene%2520recreation.%2520Existing%2520methods%252C%2520which%2520rely%2520on%2520SDS%250Aoptimization%2520or%2520single-view%2520inpainting%252C%2520often%2520struggle%2520to%2520produce%2520high-quality%250Aresults.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520method%2520for%2520object%2520insertion%2520in%25203D%250Acontent%2520represented%2520by%2520Gaussian%2520Splatting.%2520Our%2520approach%2520introduces%2520a%2520multi-view%250Adiffusion%2520model%252C%2520dubbed%2520MVInpainter%252C%2520which%2520is%2520built%2520upon%2520a%2520pre-trained%2520stable%250Avideo%2520diffusion%2520model%2520to%2520facilitate%2520view-consistent%2520object%2520inpainting.%2520Within%250AMVInpainter%252C%2520we%2520incorporate%2520a%2520ControlNet-based%2520conditional%2520injection%2520module%2520to%250Aenable%2520controlled%2520and%2520more%2520predictable%2520multi-view%2520generation.%2520After%2520generating%250Athe%2520multi-view%2520inpainted%2520results%252C%2520we%2520further%2520propose%2520a%2520mask-aware%25203D%250Areconstruction%2520technique%2520to%2520refine%2520Gaussian%2520Splatting%2520reconstruction%2520from%2520these%250Asparse%2520inpainted%2520views.%2520By%2520leveraging%2520these%2520fabricate%2520techniques%252C%2520our%2520approach%250Ayields%2520diverse%2520results%252C%2520ensures%2520view-consistent%2520and%2520harmonious%2520insertions%252C%2520and%250Aproduces%2520better%2520object%2520quality.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250Aapproach%2520outperforms%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16938v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Object%20Insertion%20in%20Gaussian%20Splatting%20with%20a%20Multi-View%0A%20%20Diffusion%20Model&entry.906535625=Hongliang%20Zhong%20and%20Can%20Wang%20and%20Jingbo%20Zhang%20and%20Jing%20Liao&entry.1292438233=%20%20Generating%20and%20inserting%20new%20objects%20into%203D%20content%20is%20a%20compelling%20approach%0Afor%20achieving%20versatile%20scene%20recreation.%20Existing%20methods%2C%20which%20rely%20on%20SDS%0Aoptimization%20or%20single-view%20inpainting%2C%20often%20struggle%20to%20produce%20high-quality%0Aresults.%20To%20address%20this%2C%20we%20propose%20a%20novel%20method%20for%20object%20insertion%20in%203D%0Acontent%20represented%20by%20Gaussian%20Splatting.%20Our%20approach%20introduces%20a%20multi-view%0Adiffusion%20model%2C%20dubbed%20MVInpainter%2C%20which%20is%20built%20upon%20a%20pre-trained%20stable%0Avideo%20diffusion%20model%20to%20facilitate%20view-consistent%20object%20inpainting.%20Within%0AMVInpainter%2C%20we%20incorporate%20a%20ControlNet-based%20conditional%20injection%20module%20to%0Aenable%20controlled%20and%20more%20predictable%20multi-view%20generation.%20After%20generating%0Athe%20multi-view%20inpainted%20results%2C%20we%20further%20propose%20a%20mask-aware%203D%0Areconstruction%20technique%20to%20refine%20Gaussian%20Splatting%20reconstruction%20from%20these%0Asparse%20inpainted%20views.%20By%20leveraging%20these%20fabricate%20techniques%2C%20our%20approach%0Ayields%20diverse%20results%2C%20ensures%20view-consistent%20and%20harmonious%20insertions%2C%20and%0Aproduces%20better%20object%20quality.%20Extensive%20experiments%20demonstrate%20that%20our%0Aapproach%20outperforms%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16938v1&entry.124074799=Read"},
{"title": "AI-driven View Guidance System in Intra-cardiac Echocardiography Imaging", "author": "Jaeyoung Huh and Paul Klein and Gareth Funka-Lea and Puneet Sharma and Ankur Kapoor and Young-Ho Kim", "abstract": "  Intra-cardiac Echocardiography (ICE) is a crucial imaging modality used in\nelectrophysiology (EP) and structural heart disease (SHD) interventions,\nproviding real-time, high-resolution views from within the heart. Despite its\nadvantages, effective manipulation of the ICE catheter requires significant\nexpertise, which can lead to inconsistent outcomes, particularly among less\nexperienced operators. To address this challenge, we propose an AI-driven\nclosed-loop view guidance system with human-in-the-loop feedback, designed to\nassist users in navigating ICE imaging without requiring specialized knowledge.\nOur method models the relative position and orientation vectors between\narbitrary views and clinically defined ICE views in a spatial coordinate\nsystem, guiding users on how to manipulate the ICE catheter to transition from\nthe current view to the desired view over time. Operating in a closed-loop\nconfiguration, the system continuously predicts and updates the necessary\ncatheter manipulations, ensuring seamless integration into existing clinical\nworkflows. The effectiveness of the proposed system is demonstrated through a\nsimulation-based evaluation, achieving an 89% success rate with the 6532 test\ndataset, highlighting its potential to improve the accuracy and efficiency of\nICE imaging procedures.\n", "link": "http://arxiv.org/abs/2409.16898v1", "date": "2024-09-25", "relevancy": 2.6069, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5527}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5159}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4955}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-driven%20View%20Guidance%20System%20in%20Intra-cardiac%20Echocardiography%20Imaging&body=Title%3A%20AI-driven%20View%20Guidance%20System%20in%20Intra-cardiac%20Echocardiography%20Imaging%0AAuthor%3A%20Jaeyoung%20Huh%20and%20Paul%20Klein%20and%20Gareth%20Funka-Lea%20and%20Puneet%20Sharma%20and%20Ankur%20Kapoor%20and%20Young-Ho%20Kim%0AAbstract%3A%20%20%20Intra-cardiac%20Echocardiography%20%28ICE%29%20is%20a%20crucial%20imaging%20modality%20used%20in%0Aelectrophysiology%20%28EP%29%20and%20structural%20heart%20disease%20%28SHD%29%20interventions%2C%0Aproviding%20real-time%2C%20high-resolution%20views%20from%20within%20the%20heart.%20Despite%20its%0Aadvantages%2C%20effective%20manipulation%20of%20the%20ICE%20catheter%20requires%20significant%0Aexpertise%2C%20which%20can%20lead%20to%20inconsistent%20outcomes%2C%20particularly%20among%20less%0Aexperienced%20operators.%20To%20address%20this%20challenge%2C%20we%20propose%20an%20AI-driven%0Aclosed-loop%20view%20guidance%20system%20with%20human-in-the-loop%20feedback%2C%20designed%20to%0Aassist%20users%20in%20navigating%20ICE%20imaging%20without%20requiring%20specialized%20knowledge.%0AOur%20method%20models%20the%20relative%20position%20and%20orientation%20vectors%20between%0Aarbitrary%20views%20and%20clinically%20defined%20ICE%20views%20in%20a%20spatial%20coordinate%0Asystem%2C%20guiding%20users%20on%20how%20to%20manipulate%20the%20ICE%20catheter%20to%20transition%20from%0Athe%20current%20view%20to%20the%20desired%20view%20over%20time.%20Operating%20in%20a%20closed-loop%0Aconfiguration%2C%20the%20system%20continuously%20predicts%20and%20updates%20the%20necessary%0Acatheter%20manipulations%2C%20ensuring%20seamless%20integration%20into%20existing%20clinical%0Aworkflows.%20The%20effectiveness%20of%20the%20proposed%20system%20is%20demonstrated%20through%20a%0Asimulation-based%20evaluation%2C%20achieving%20an%2089%25%20success%20rate%20with%20the%206532%20test%0Adataset%2C%20highlighting%20its%20potential%20to%20improve%20the%20accuracy%20and%20efficiency%20of%0AICE%20imaging%20procedures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16898v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-driven%2520View%2520Guidance%2520System%2520in%2520Intra-cardiac%2520Echocardiography%2520Imaging%26entry.906535625%3DJaeyoung%2520Huh%2520and%2520Paul%2520Klein%2520and%2520Gareth%2520Funka-Lea%2520and%2520Puneet%2520Sharma%2520and%2520Ankur%2520Kapoor%2520and%2520Young-Ho%2520Kim%26entry.1292438233%3D%2520%2520Intra-cardiac%2520Echocardiography%2520%2528ICE%2529%2520is%2520a%2520crucial%2520imaging%2520modality%2520used%2520in%250Aelectrophysiology%2520%2528EP%2529%2520and%2520structural%2520heart%2520disease%2520%2528SHD%2529%2520interventions%252C%250Aproviding%2520real-time%252C%2520high-resolution%2520views%2520from%2520within%2520the%2520heart.%2520Despite%2520its%250Aadvantages%252C%2520effective%2520manipulation%2520of%2520the%2520ICE%2520catheter%2520requires%2520significant%250Aexpertise%252C%2520which%2520can%2520lead%2520to%2520inconsistent%2520outcomes%252C%2520particularly%2520among%2520less%250Aexperienced%2520operators.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520an%2520AI-driven%250Aclosed-loop%2520view%2520guidance%2520system%2520with%2520human-in-the-loop%2520feedback%252C%2520designed%2520to%250Aassist%2520users%2520in%2520navigating%2520ICE%2520imaging%2520without%2520requiring%2520specialized%2520knowledge.%250AOur%2520method%2520models%2520the%2520relative%2520position%2520and%2520orientation%2520vectors%2520between%250Aarbitrary%2520views%2520and%2520clinically%2520defined%2520ICE%2520views%2520in%2520a%2520spatial%2520coordinate%250Asystem%252C%2520guiding%2520users%2520on%2520how%2520to%2520manipulate%2520the%2520ICE%2520catheter%2520to%2520transition%2520from%250Athe%2520current%2520view%2520to%2520the%2520desired%2520view%2520over%2520time.%2520Operating%2520in%2520a%2520closed-loop%250Aconfiguration%252C%2520the%2520system%2520continuously%2520predicts%2520and%2520updates%2520the%2520necessary%250Acatheter%2520manipulations%252C%2520ensuring%2520seamless%2520integration%2520into%2520existing%2520clinical%250Aworkflows.%2520The%2520effectiveness%2520of%2520the%2520proposed%2520system%2520is%2520demonstrated%2520through%2520a%250Asimulation-based%2520evaluation%252C%2520achieving%2520an%252089%2525%2520success%2520rate%2520with%2520the%25206532%2520test%250Adataset%252C%2520highlighting%2520its%2520potential%2520to%2520improve%2520the%2520accuracy%2520and%2520efficiency%2520of%250AICE%2520imaging%2520procedures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16898v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-driven%20View%20Guidance%20System%20in%20Intra-cardiac%20Echocardiography%20Imaging&entry.906535625=Jaeyoung%20Huh%20and%20Paul%20Klein%20and%20Gareth%20Funka-Lea%20and%20Puneet%20Sharma%20and%20Ankur%20Kapoor%20and%20Young-Ho%20Kim&entry.1292438233=%20%20Intra-cardiac%20Echocardiography%20%28ICE%29%20is%20a%20crucial%20imaging%20modality%20used%20in%0Aelectrophysiology%20%28EP%29%20and%20structural%20heart%20disease%20%28SHD%29%20interventions%2C%0Aproviding%20real-time%2C%20high-resolution%20views%20from%20within%20the%20heart.%20Despite%20its%0Aadvantages%2C%20effective%20manipulation%20of%20the%20ICE%20catheter%20requires%20significant%0Aexpertise%2C%20which%20can%20lead%20to%20inconsistent%20outcomes%2C%20particularly%20among%20less%0Aexperienced%20operators.%20To%20address%20this%20challenge%2C%20we%20propose%20an%20AI-driven%0Aclosed-loop%20view%20guidance%20system%20with%20human-in-the-loop%20feedback%2C%20designed%20to%0Aassist%20users%20in%20navigating%20ICE%20imaging%20without%20requiring%20specialized%20knowledge.%0AOur%20method%20models%20the%20relative%20position%20and%20orientation%20vectors%20between%0Aarbitrary%20views%20and%20clinically%20defined%20ICE%20views%20in%20a%20spatial%20coordinate%0Asystem%2C%20guiding%20users%20on%20how%20to%20manipulate%20the%20ICE%20catheter%20to%20transition%20from%0Athe%20current%20view%20to%20the%20desired%20view%20over%20time.%20Operating%20in%20a%20closed-loop%0Aconfiguration%2C%20the%20system%20continuously%20predicts%20and%20updates%20the%20necessary%0Acatheter%20manipulations%2C%20ensuring%20seamless%20integration%20into%20existing%20clinical%0Aworkflows.%20The%20effectiveness%20of%20the%20proposed%20system%20is%20demonstrated%20through%20a%0Asimulation-based%20evaluation%2C%20achieving%20an%2089%25%20success%20rate%20with%20the%206532%20test%0Adataset%2C%20highlighting%20its%20potential%20to%20improve%20the%20accuracy%20and%20efficiency%20of%0AICE%20imaging%20procedures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16898v1&entry.124074799=Read"},
{"title": "Enhancing Temporal Sensitivity and Reasoning for Time-Sensitive Question\n  Answering", "author": "Wanqi Yang and Yanda Li and Meng Fang and Ling Chen", "abstract": "  Time-Sensitive Question Answering (TSQA) demands the effective utilization of\nspecific temporal contexts, encompassing multiple time-evolving facts, to\naddress time-sensitive questions. This necessitates not only the parsing of\ntemporal information within questions but also the identification and\nunderstanding of time-evolving facts to generate accurate answers. However,\ncurrent large language models still have limited sensitivity to temporal\ninformation and their inadequate temporal reasoning capabilities.In this paper,\nwe propose a novel framework that enhances temporal awareness and reasoning\nthrough Temporal Information-Aware Embedding and Granular Contrastive\nReinforcement Learning. Experimental results on four TSQA datasets demonstrate\nthat our framework significantly outperforms existing LLMs in TSQA tasks,\nmarking a step forward in bridging the performance gap between machine and\nhuman temporal understanding and reasoning.\n", "link": "http://arxiv.org/abs/2409.16909v1", "date": "2024-09-25", "relevancy": 2.576, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5208}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5124}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Temporal%20Sensitivity%20and%20Reasoning%20for%20Time-Sensitive%20Question%0A%20%20Answering&body=Title%3A%20Enhancing%20Temporal%20Sensitivity%20and%20Reasoning%20for%20Time-Sensitive%20Question%0A%20%20Answering%0AAuthor%3A%20Wanqi%20Yang%20and%20Yanda%20Li%20and%20Meng%20Fang%20and%20Ling%20Chen%0AAbstract%3A%20%20%20Time-Sensitive%20Question%20Answering%20%28TSQA%29%20demands%20the%20effective%20utilization%20of%0Aspecific%20temporal%20contexts%2C%20encompassing%20multiple%20time-evolving%20facts%2C%20to%0Aaddress%20time-sensitive%20questions.%20This%20necessitates%20not%20only%20the%20parsing%20of%0Atemporal%20information%20within%20questions%20but%20also%20the%20identification%20and%0Aunderstanding%20of%20time-evolving%20facts%20to%20generate%20accurate%20answers.%20However%2C%0Acurrent%20large%20language%20models%20still%20have%20limited%20sensitivity%20to%20temporal%0Ainformation%20and%20their%20inadequate%20temporal%20reasoning%20capabilities.In%20this%20paper%2C%0Awe%20propose%20a%20novel%20framework%20that%20enhances%20temporal%20awareness%20and%20reasoning%0Athrough%20Temporal%20Information-Aware%20Embedding%20and%20Granular%20Contrastive%0AReinforcement%20Learning.%20Experimental%20results%20on%20four%20TSQA%20datasets%20demonstrate%0Athat%20our%20framework%20significantly%20outperforms%20existing%20LLMs%20in%20TSQA%20tasks%2C%0Amarking%20a%20step%20forward%20in%20bridging%20the%20performance%20gap%20between%20machine%20and%0Ahuman%20temporal%20understanding%20and%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16909v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Temporal%2520Sensitivity%2520and%2520Reasoning%2520for%2520Time-Sensitive%2520Question%250A%2520%2520Answering%26entry.906535625%3DWanqi%2520Yang%2520and%2520Yanda%2520Li%2520and%2520Meng%2520Fang%2520and%2520Ling%2520Chen%26entry.1292438233%3D%2520%2520Time-Sensitive%2520Question%2520Answering%2520%2528TSQA%2529%2520demands%2520the%2520effective%2520utilization%2520of%250Aspecific%2520temporal%2520contexts%252C%2520encompassing%2520multiple%2520time-evolving%2520facts%252C%2520to%250Aaddress%2520time-sensitive%2520questions.%2520This%2520necessitates%2520not%2520only%2520the%2520parsing%2520of%250Atemporal%2520information%2520within%2520questions%2520but%2520also%2520the%2520identification%2520and%250Aunderstanding%2520of%2520time-evolving%2520facts%2520to%2520generate%2520accurate%2520answers.%2520However%252C%250Acurrent%2520large%2520language%2520models%2520still%2520have%2520limited%2520sensitivity%2520to%2520temporal%250Ainformation%2520and%2520their%2520inadequate%2520temporal%2520reasoning%2520capabilities.In%2520this%2520paper%252C%250Awe%2520propose%2520a%2520novel%2520framework%2520that%2520enhances%2520temporal%2520awareness%2520and%2520reasoning%250Athrough%2520Temporal%2520Information-Aware%2520Embedding%2520and%2520Granular%2520Contrastive%250AReinforcement%2520Learning.%2520Experimental%2520results%2520on%2520four%2520TSQA%2520datasets%2520demonstrate%250Athat%2520our%2520framework%2520significantly%2520outperforms%2520existing%2520LLMs%2520in%2520TSQA%2520tasks%252C%250Amarking%2520a%2520step%2520forward%2520in%2520bridging%2520the%2520performance%2520gap%2520between%2520machine%2520and%250Ahuman%2520temporal%2520understanding%2520and%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16909v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Temporal%20Sensitivity%20and%20Reasoning%20for%20Time-Sensitive%20Question%0A%20%20Answering&entry.906535625=Wanqi%20Yang%20and%20Yanda%20Li%20and%20Meng%20Fang%20and%20Ling%20Chen&entry.1292438233=%20%20Time-Sensitive%20Question%20Answering%20%28TSQA%29%20demands%20the%20effective%20utilization%20of%0Aspecific%20temporal%20contexts%2C%20encompassing%20multiple%20time-evolving%20facts%2C%20to%0Aaddress%20time-sensitive%20questions.%20This%20necessitates%20not%20only%20the%20parsing%20of%0Atemporal%20information%20within%20questions%20but%20also%20the%20identification%20and%0Aunderstanding%20of%20time-evolving%20facts%20to%20generate%20accurate%20answers.%20However%2C%0Acurrent%20large%20language%20models%20still%20have%20limited%20sensitivity%20to%20temporal%0Ainformation%20and%20their%20inadequate%20temporal%20reasoning%20capabilities.In%20this%20paper%2C%0Awe%20propose%20a%20novel%20framework%20that%20enhances%20temporal%20awareness%20and%20reasoning%0Athrough%20Temporal%20Information-Aware%20Embedding%20and%20Granular%20Contrastive%0AReinforcement%20Learning.%20Experimental%20results%20on%20four%20TSQA%20datasets%20demonstrate%0Athat%20our%20framework%20significantly%20outperforms%20existing%20LLMs%20in%20TSQA%20tasks%2C%0Amarking%20a%20step%20forward%20in%20bridging%20the%20performance%20gap%20between%20machine%20and%0Ahuman%20temporal%20understanding%20and%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16909v1&entry.124074799=Read"},
{"title": "Locally Regularized Sparse Graph by Fast Proximal Gradient Descent", "author": "Dongfang Sun and Yingzhen Yang", "abstract": "  Sparse graphs built by sparse representation has been demonstrated to be\neffective in clustering high-dimensional data. Albeit the compelling empirical\nperformance, the vanilla sparse graph ignores the geometric information of the\ndata by performing sparse representation for each datum separately. In order to\nobtain a sparse graph aligned with the local geometric structure of data, we\npropose a novel Support Regularized Sparse Graph, abbreviated as SRSG, for data\nclustering. SRSG encourages local smoothness on the neighborhoods of nearby\ndata points by a well-defined support regularization term. We propose a fast\nproximal gradient descent method to solve the non-convex optimization problem\nof SRSG with the convergence matching the Nesterov's optimal convergence rate\nof first-order methods on smooth and convex objective function with Lipschitz\ncontinuous gradient. Extensive experimental results on various real data sets\ndemonstrate the superiority of SRSG over other competing clustering methods.\n", "link": "http://arxiv.org/abs/2409.17090v1", "date": "2024-09-25", "relevancy": 2.575, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5518}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.498}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locally%20Regularized%20Sparse%20Graph%20by%20Fast%20Proximal%20Gradient%20Descent&body=Title%3A%20Locally%20Regularized%20Sparse%20Graph%20by%20Fast%20Proximal%20Gradient%20Descent%0AAuthor%3A%20Dongfang%20Sun%20and%20Yingzhen%20Yang%0AAbstract%3A%20%20%20Sparse%20graphs%20built%20by%20sparse%20representation%20has%20been%20demonstrated%20to%20be%0Aeffective%20in%20clustering%20high-dimensional%20data.%20Albeit%20the%20compelling%20empirical%0Aperformance%2C%20the%20vanilla%20sparse%20graph%20ignores%20the%20geometric%20information%20of%20the%0Adata%20by%20performing%20sparse%20representation%20for%20each%20datum%20separately.%20In%20order%20to%0Aobtain%20a%20sparse%20graph%20aligned%20with%20the%20local%20geometric%20structure%20of%20data%2C%20we%0Apropose%20a%20novel%20Support%20Regularized%20Sparse%20Graph%2C%20abbreviated%20as%20SRSG%2C%20for%20data%0Aclustering.%20SRSG%20encourages%20local%20smoothness%20on%20the%20neighborhoods%20of%20nearby%0Adata%20points%20by%20a%20well-defined%20support%20regularization%20term.%20We%20propose%20a%20fast%0Aproximal%20gradient%20descent%20method%20to%20solve%20the%20non-convex%20optimization%20problem%0Aof%20SRSG%20with%20the%20convergence%20matching%20the%20Nesterov%27s%20optimal%20convergence%20rate%0Aof%20first-order%20methods%20on%20smooth%20and%20convex%20objective%20function%20with%20Lipschitz%0Acontinuous%20gradient.%20Extensive%20experimental%20results%20on%20various%20real%20data%20sets%0Ademonstrate%20the%20superiority%20of%20SRSG%20over%20other%20competing%20clustering%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17090v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocally%2520Regularized%2520Sparse%2520Graph%2520by%2520Fast%2520Proximal%2520Gradient%2520Descent%26entry.906535625%3DDongfang%2520Sun%2520and%2520Yingzhen%2520Yang%26entry.1292438233%3D%2520%2520Sparse%2520graphs%2520built%2520by%2520sparse%2520representation%2520has%2520been%2520demonstrated%2520to%2520be%250Aeffective%2520in%2520clustering%2520high-dimensional%2520data.%2520Albeit%2520the%2520compelling%2520empirical%250Aperformance%252C%2520the%2520vanilla%2520sparse%2520graph%2520ignores%2520the%2520geometric%2520information%2520of%2520the%250Adata%2520by%2520performing%2520sparse%2520representation%2520for%2520each%2520datum%2520separately.%2520In%2520order%2520to%250Aobtain%2520a%2520sparse%2520graph%2520aligned%2520with%2520the%2520local%2520geometric%2520structure%2520of%2520data%252C%2520we%250Apropose%2520a%2520novel%2520Support%2520Regularized%2520Sparse%2520Graph%252C%2520abbreviated%2520as%2520SRSG%252C%2520for%2520data%250Aclustering.%2520SRSG%2520encourages%2520local%2520smoothness%2520on%2520the%2520neighborhoods%2520of%2520nearby%250Adata%2520points%2520by%2520a%2520well-defined%2520support%2520regularization%2520term.%2520We%2520propose%2520a%2520fast%250Aproximal%2520gradient%2520descent%2520method%2520to%2520solve%2520the%2520non-convex%2520optimization%2520problem%250Aof%2520SRSG%2520with%2520the%2520convergence%2520matching%2520the%2520Nesterov%2527s%2520optimal%2520convergence%2520rate%250Aof%2520first-order%2520methods%2520on%2520smooth%2520and%2520convex%2520objective%2520function%2520with%2520Lipschitz%250Acontinuous%2520gradient.%2520Extensive%2520experimental%2520results%2520on%2520various%2520real%2520data%2520sets%250Ademonstrate%2520the%2520superiority%2520of%2520SRSG%2520over%2520other%2520competing%2520clustering%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17090v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locally%20Regularized%20Sparse%20Graph%20by%20Fast%20Proximal%20Gradient%20Descent&entry.906535625=Dongfang%20Sun%20and%20Yingzhen%20Yang&entry.1292438233=%20%20Sparse%20graphs%20built%20by%20sparse%20representation%20has%20been%20demonstrated%20to%20be%0Aeffective%20in%20clustering%20high-dimensional%20data.%20Albeit%20the%20compelling%20empirical%0Aperformance%2C%20the%20vanilla%20sparse%20graph%20ignores%20the%20geometric%20information%20of%20the%0Adata%20by%20performing%20sparse%20representation%20for%20each%20datum%20separately.%20In%20order%20to%0Aobtain%20a%20sparse%20graph%20aligned%20with%20the%20local%20geometric%20structure%20of%20data%2C%20we%0Apropose%20a%20novel%20Support%20Regularized%20Sparse%20Graph%2C%20abbreviated%20as%20SRSG%2C%20for%20data%0Aclustering.%20SRSG%20encourages%20local%20smoothness%20on%20the%20neighborhoods%20of%20nearby%0Adata%20points%20by%20a%20well-defined%20support%20regularization%20term.%20We%20propose%20a%20fast%0Aproximal%20gradient%20descent%20method%20to%20solve%20the%20non-convex%20optimization%20problem%0Aof%20SRSG%20with%20the%20convergence%20matching%20the%20Nesterov%27s%20optimal%20convergence%20rate%0Aof%20first-order%20methods%20on%20smooth%20and%20convex%20objective%20function%20with%20Lipschitz%0Acontinuous%20gradient.%20Extensive%20experimental%20results%20on%20various%20real%20data%20sets%0Ademonstrate%20the%20superiority%20of%20SRSG%20over%20other%20competing%20clustering%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17090v1&entry.124074799=Read"},
{"title": "Discriminative Anchor Learning for Efficient Multi-view Clustering", "author": "Yalan Qin and Nan Pu and Hanzhou Wu and Nicu Sebe", "abstract": "  Multi-view clustering aims to study the complementary information across\nviews and discover the underlying structure. For solving the relatively high\ncomputational cost for the existing approaches, works based on anchor have been\npresented recently. Even with acceptable clustering performance, these methods\ntend to map the original representation from multiple views into a fixed shared\ngraph based on the original dataset. However, most studies ignore the\ndiscriminative property of the learned anchors, which ruin the representation\ncapability of the built model. Moreover, the complementary information among\nanchors across views is neglected to be ensured by simply learning the shared\nanchor graph without considering the quality of view-specific anchors. In this\npaper, we propose discriminative anchor learning for multi-view clustering\n(DALMC) for handling the above issues. We learn discriminative view-specific\nfeature representations according to the original dataset and build anchors\nfrom different views based on these representations, which increase the quality\nof the shared anchor graph. The discriminative feature learning and consensus\nanchor graph construction are integrated into a unified framework to improve\neach other for realizing the refinement. The optimal anchors from multiple\nviews and the consensus anchor graph are learned with the orthogonal\nconstraints. We give an iterative algorithm to deal with the formulated\nproblem. Extensive experiments on different datasets show the effectiveness and\nefficiency of our method compared with other methods.\n", "link": "http://arxiv.org/abs/2409.16904v1", "date": "2024-09-25", "relevancy": 2.5692, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5474}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5116}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4825}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discriminative%20Anchor%20Learning%20for%20Efficient%20Multi-view%20Clustering&body=Title%3A%20Discriminative%20Anchor%20Learning%20for%20Efficient%20Multi-view%20Clustering%0AAuthor%3A%20Yalan%20Qin%20and%20Nan%20Pu%20and%20Hanzhou%20Wu%20and%20Nicu%20Sebe%0AAbstract%3A%20%20%20Multi-view%20clustering%20aims%20to%20study%20the%20complementary%20information%20across%0Aviews%20and%20discover%20the%20underlying%20structure.%20For%20solving%20the%20relatively%20high%0Acomputational%20cost%20for%20the%20existing%20approaches%2C%20works%20based%20on%20anchor%20have%20been%0Apresented%20recently.%20Even%20with%20acceptable%20clustering%20performance%2C%20these%20methods%0Atend%20to%20map%20the%20original%20representation%20from%20multiple%20views%20into%20a%20fixed%20shared%0Agraph%20based%20on%20the%20original%20dataset.%20However%2C%20most%20studies%20ignore%20the%0Adiscriminative%20property%20of%20the%20learned%20anchors%2C%20which%20ruin%20the%20representation%0Acapability%20of%20the%20built%20model.%20Moreover%2C%20the%20complementary%20information%20among%0Aanchors%20across%20views%20is%20neglected%20to%20be%20ensured%20by%20simply%20learning%20the%20shared%0Aanchor%20graph%20without%20considering%20the%20quality%20of%20view-specific%20anchors.%20In%20this%0Apaper%2C%20we%20propose%20discriminative%20anchor%20learning%20for%20multi-view%20clustering%0A%28DALMC%29%20for%20handling%20the%20above%20issues.%20We%20learn%20discriminative%20view-specific%0Afeature%20representations%20according%20to%20the%20original%20dataset%20and%20build%20anchors%0Afrom%20different%20views%20based%20on%20these%20representations%2C%20which%20increase%20the%20quality%0Aof%20the%20shared%20anchor%20graph.%20The%20discriminative%20feature%20learning%20and%20consensus%0Aanchor%20graph%20construction%20are%20integrated%20into%20a%20unified%20framework%20to%20improve%0Aeach%20other%20for%20realizing%20the%20refinement.%20The%20optimal%20anchors%20from%20multiple%0Aviews%20and%20the%20consensus%20anchor%20graph%20are%20learned%20with%20the%20orthogonal%0Aconstraints.%20We%20give%20an%20iterative%20algorithm%20to%20deal%20with%20the%20formulated%0Aproblem.%20Extensive%20experiments%20on%20different%20datasets%20show%20the%20effectiveness%20and%0Aefficiency%20of%20our%20method%20compared%20with%20other%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16904v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscriminative%2520Anchor%2520Learning%2520for%2520Efficient%2520Multi-view%2520Clustering%26entry.906535625%3DYalan%2520Qin%2520and%2520Nan%2520Pu%2520and%2520Hanzhou%2520Wu%2520and%2520Nicu%2520Sebe%26entry.1292438233%3D%2520%2520Multi-view%2520clustering%2520aims%2520to%2520study%2520the%2520complementary%2520information%2520across%250Aviews%2520and%2520discover%2520the%2520underlying%2520structure.%2520For%2520solving%2520the%2520relatively%2520high%250Acomputational%2520cost%2520for%2520the%2520existing%2520approaches%252C%2520works%2520based%2520on%2520anchor%2520have%2520been%250Apresented%2520recently.%2520Even%2520with%2520acceptable%2520clustering%2520performance%252C%2520these%2520methods%250Atend%2520to%2520map%2520the%2520original%2520representation%2520from%2520multiple%2520views%2520into%2520a%2520fixed%2520shared%250Agraph%2520based%2520on%2520the%2520original%2520dataset.%2520However%252C%2520most%2520studies%2520ignore%2520the%250Adiscriminative%2520property%2520of%2520the%2520learned%2520anchors%252C%2520which%2520ruin%2520the%2520representation%250Acapability%2520of%2520the%2520built%2520model.%2520Moreover%252C%2520the%2520complementary%2520information%2520among%250Aanchors%2520across%2520views%2520is%2520neglected%2520to%2520be%2520ensured%2520by%2520simply%2520learning%2520the%2520shared%250Aanchor%2520graph%2520without%2520considering%2520the%2520quality%2520of%2520view-specific%2520anchors.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520discriminative%2520anchor%2520learning%2520for%2520multi-view%2520clustering%250A%2528DALMC%2529%2520for%2520handling%2520the%2520above%2520issues.%2520We%2520learn%2520discriminative%2520view-specific%250Afeature%2520representations%2520according%2520to%2520the%2520original%2520dataset%2520and%2520build%2520anchors%250Afrom%2520different%2520views%2520based%2520on%2520these%2520representations%252C%2520which%2520increase%2520the%2520quality%250Aof%2520the%2520shared%2520anchor%2520graph.%2520The%2520discriminative%2520feature%2520learning%2520and%2520consensus%250Aanchor%2520graph%2520construction%2520are%2520integrated%2520into%2520a%2520unified%2520framework%2520to%2520improve%250Aeach%2520other%2520for%2520realizing%2520the%2520refinement.%2520The%2520optimal%2520anchors%2520from%2520multiple%250Aviews%2520and%2520the%2520consensus%2520anchor%2520graph%2520are%2520learned%2520with%2520the%2520orthogonal%250Aconstraints.%2520We%2520give%2520an%2520iterative%2520algorithm%2520to%2520deal%2520with%2520the%2520formulated%250Aproblem.%2520Extensive%2520experiments%2520on%2520different%2520datasets%2520show%2520the%2520effectiveness%2520and%250Aefficiency%2520of%2520our%2520method%2520compared%2520with%2520other%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16904v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discriminative%20Anchor%20Learning%20for%20Efficient%20Multi-view%20Clustering&entry.906535625=Yalan%20Qin%20and%20Nan%20Pu%20and%20Hanzhou%20Wu%20and%20Nicu%20Sebe&entry.1292438233=%20%20Multi-view%20clustering%20aims%20to%20study%20the%20complementary%20information%20across%0Aviews%20and%20discover%20the%20underlying%20structure.%20For%20solving%20the%20relatively%20high%0Acomputational%20cost%20for%20the%20existing%20approaches%2C%20works%20based%20on%20anchor%20have%20been%0Apresented%20recently.%20Even%20with%20acceptable%20clustering%20performance%2C%20these%20methods%0Atend%20to%20map%20the%20original%20representation%20from%20multiple%20views%20into%20a%20fixed%20shared%0Agraph%20based%20on%20the%20original%20dataset.%20However%2C%20most%20studies%20ignore%20the%0Adiscriminative%20property%20of%20the%20learned%20anchors%2C%20which%20ruin%20the%20representation%0Acapability%20of%20the%20built%20model.%20Moreover%2C%20the%20complementary%20information%20among%0Aanchors%20across%20views%20is%20neglected%20to%20be%20ensured%20by%20simply%20learning%20the%20shared%0Aanchor%20graph%20without%20considering%20the%20quality%20of%20view-specific%20anchors.%20In%20this%0Apaper%2C%20we%20propose%20discriminative%20anchor%20learning%20for%20multi-view%20clustering%0A%28DALMC%29%20for%20handling%20the%20above%20issues.%20We%20learn%20discriminative%20view-specific%0Afeature%20representations%20according%20to%20the%20original%20dataset%20and%20build%20anchors%0Afrom%20different%20views%20based%20on%20these%20representations%2C%20which%20increase%20the%20quality%0Aof%20the%20shared%20anchor%20graph.%20The%20discriminative%20feature%20learning%20and%20consensus%0Aanchor%20graph%20construction%20are%20integrated%20into%20a%20unified%20framework%20to%20improve%0Aeach%20other%20for%20realizing%20the%20refinement.%20The%20optimal%20anchors%20from%20multiple%0Aviews%20and%20the%20consensus%20anchor%20graph%20are%20learned%20with%20the%20orthogonal%0Aconstraints.%20We%20give%20an%20iterative%20algorithm%20to%20deal%20with%20the%20formulated%0Aproblem.%20Extensive%20experiments%20on%20different%20datasets%20show%20the%20effectiveness%20and%0Aefficiency%20of%20our%20method%20compared%20with%20other%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16904v1&entry.124074799=Read"},
{"title": "Moner: Motion Correction in Undersampled Radial MRI with Unsupervised\n  Neural Representation", "author": "Qing Wu and Chenhe Du and XuanYu Tian and Jingyi Yu and Yuyao Zhang and Hongjiang Wei", "abstract": "  Motion correction (MoCo) in radial MRI is a challenging problem due to the\nunpredictability of subject's motion. Current state-of-the-art (SOTA) MoCo\nalgorithms often use extensive high-quality MR images to pre-train neural\nnetworks, obtaining excellent reconstructions. However, the need for\nlarge-scale datasets significantly increases costs and limits model\ngeneralization. In this work, we propose Moner, an unsupervised MoCo method\nthat jointly solves artifact-free MR images and accurate motion from\nundersampled, rigid motion-corrupted k-space data, without requiring training\ndata. Our core idea is to leverage the continuous prior of implicit neural\nrepresentation (INR) to constrain this ill-posed inverse problem, enabling\nideal solutions. Specifically, we incorporate a quasi-static motion model into\nthe INR, granting its ability to correct subject's motion. To stabilize model\noptimization, we reformulate radial MRI as a back-projection problem using the\nFourier-slice theorem. Additionally, we propose a novel coarse-to-fine hash\nencoding strategy, significantly enhancing MoCo accuracy. Experiments on\nmultiple MRI datasets show our Moner achieves performance comparable to SOTA\nMoCo techniques on in-domain data, while demonstrating significant improvements\non out-of-domain data.\n", "link": "http://arxiv.org/abs/2409.16921v1", "date": "2024-09-25", "relevancy": 2.5681, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5226}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5123}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Moner%3A%20Motion%20Correction%20in%20Undersampled%20Radial%20MRI%20with%20Unsupervised%0A%20%20Neural%20Representation&body=Title%3A%20Moner%3A%20Motion%20Correction%20in%20Undersampled%20Radial%20MRI%20with%20Unsupervised%0A%20%20Neural%20Representation%0AAuthor%3A%20Qing%20Wu%20and%20Chenhe%20Du%20and%20XuanYu%20Tian%20and%20Jingyi%20Yu%20and%20Yuyao%20Zhang%20and%20Hongjiang%20Wei%0AAbstract%3A%20%20%20Motion%20correction%20%28MoCo%29%20in%20radial%20MRI%20is%20a%20challenging%20problem%20due%20to%20the%0Aunpredictability%20of%20subject%27s%20motion.%20Current%20state-of-the-art%20%28SOTA%29%20MoCo%0Aalgorithms%20often%20use%20extensive%20high-quality%20MR%20images%20to%20pre-train%20neural%0Anetworks%2C%20obtaining%20excellent%20reconstructions.%20However%2C%20the%20need%20for%0Alarge-scale%20datasets%20significantly%20increases%20costs%20and%20limits%20model%0Ageneralization.%20In%20this%20work%2C%20we%20propose%20Moner%2C%20an%20unsupervised%20MoCo%20method%0Athat%20jointly%20solves%20artifact-free%20MR%20images%20and%20accurate%20motion%20from%0Aundersampled%2C%20rigid%20motion-corrupted%20k-space%20data%2C%20without%20requiring%20training%0Adata.%20Our%20core%20idea%20is%20to%20leverage%20the%20continuous%20prior%20of%20implicit%20neural%0Arepresentation%20%28INR%29%20to%20constrain%20this%20ill-posed%20inverse%20problem%2C%20enabling%0Aideal%20solutions.%20Specifically%2C%20we%20incorporate%20a%20quasi-static%20motion%20model%20into%0Athe%20INR%2C%20granting%20its%20ability%20to%20correct%20subject%27s%20motion.%20To%20stabilize%20model%0Aoptimization%2C%20we%20reformulate%20radial%20MRI%20as%20a%20back-projection%20problem%20using%20the%0AFourier-slice%20theorem.%20Additionally%2C%20we%20propose%20a%20novel%20coarse-to-fine%20hash%0Aencoding%20strategy%2C%20significantly%20enhancing%20MoCo%20accuracy.%20Experiments%20on%0Amultiple%20MRI%20datasets%20show%20our%20Moner%20achieves%20performance%20comparable%20to%20SOTA%0AMoCo%20techniques%20on%20in-domain%20data%2C%20while%20demonstrating%20significant%20improvements%0Aon%20out-of-domain%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16921v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoner%253A%2520Motion%2520Correction%2520in%2520Undersampled%2520Radial%2520MRI%2520with%2520Unsupervised%250A%2520%2520Neural%2520Representation%26entry.906535625%3DQing%2520Wu%2520and%2520Chenhe%2520Du%2520and%2520XuanYu%2520Tian%2520and%2520Jingyi%2520Yu%2520and%2520Yuyao%2520Zhang%2520and%2520Hongjiang%2520Wei%26entry.1292438233%3D%2520%2520Motion%2520correction%2520%2528MoCo%2529%2520in%2520radial%2520MRI%2520is%2520a%2520challenging%2520problem%2520due%2520to%2520the%250Aunpredictability%2520of%2520subject%2527s%2520motion.%2520Current%2520state-of-the-art%2520%2528SOTA%2529%2520MoCo%250Aalgorithms%2520often%2520use%2520extensive%2520high-quality%2520MR%2520images%2520to%2520pre-train%2520neural%250Anetworks%252C%2520obtaining%2520excellent%2520reconstructions.%2520However%252C%2520the%2520need%2520for%250Alarge-scale%2520datasets%2520significantly%2520increases%2520costs%2520and%2520limits%2520model%250Ageneralization.%2520In%2520this%2520work%252C%2520we%2520propose%2520Moner%252C%2520an%2520unsupervised%2520MoCo%2520method%250Athat%2520jointly%2520solves%2520artifact-free%2520MR%2520images%2520and%2520accurate%2520motion%2520from%250Aundersampled%252C%2520rigid%2520motion-corrupted%2520k-space%2520data%252C%2520without%2520requiring%2520training%250Adata.%2520Our%2520core%2520idea%2520is%2520to%2520leverage%2520the%2520continuous%2520prior%2520of%2520implicit%2520neural%250Arepresentation%2520%2528INR%2529%2520to%2520constrain%2520this%2520ill-posed%2520inverse%2520problem%252C%2520enabling%250Aideal%2520solutions.%2520Specifically%252C%2520we%2520incorporate%2520a%2520quasi-static%2520motion%2520model%2520into%250Athe%2520INR%252C%2520granting%2520its%2520ability%2520to%2520correct%2520subject%2527s%2520motion.%2520To%2520stabilize%2520model%250Aoptimization%252C%2520we%2520reformulate%2520radial%2520MRI%2520as%2520a%2520back-projection%2520problem%2520using%2520the%250AFourier-slice%2520theorem.%2520Additionally%252C%2520we%2520propose%2520a%2520novel%2520coarse-to-fine%2520hash%250Aencoding%2520strategy%252C%2520significantly%2520enhancing%2520MoCo%2520accuracy.%2520Experiments%2520on%250Amultiple%2520MRI%2520datasets%2520show%2520our%2520Moner%2520achieves%2520performance%2520comparable%2520to%2520SOTA%250AMoCo%2520techniques%2520on%2520in-domain%2520data%252C%2520while%2520demonstrating%2520significant%2520improvements%250Aon%2520out-of-domain%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16921v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moner%3A%20Motion%20Correction%20in%20Undersampled%20Radial%20MRI%20with%20Unsupervised%0A%20%20Neural%20Representation&entry.906535625=Qing%20Wu%20and%20Chenhe%20Du%20and%20XuanYu%20Tian%20and%20Jingyi%20Yu%20and%20Yuyao%20Zhang%20and%20Hongjiang%20Wei&entry.1292438233=%20%20Motion%20correction%20%28MoCo%29%20in%20radial%20MRI%20is%20a%20challenging%20problem%20due%20to%20the%0Aunpredictability%20of%20subject%27s%20motion.%20Current%20state-of-the-art%20%28SOTA%29%20MoCo%0Aalgorithms%20often%20use%20extensive%20high-quality%20MR%20images%20to%20pre-train%20neural%0Anetworks%2C%20obtaining%20excellent%20reconstructions.%20However%2C%20the%20need%20for%0Alarge-scale%20datasets%20significantly%20increases%20costs%20and%20limits%20model%0Ageneralization.%20In%20this%20work%2C%20we%20propose%20Moner%2C%20an%20unsupervised%20MoCo%20method%0Athat%20jointly%20solves%20artifact-free%20MR%20images%20and%20accurate%20motion%20from%0Aundersampled%2C%20rigid%20motion-corrupted%20k-space%20data%2C%20without%20requiring%20training%0Adata.%20Our%20core%20idea%20is%20to%20leverage%20the%20continuous%20prior%20of%20implicit%20neural%0Arepresentation%20%28INR%29%20to%20constrain%20this%20ill-posed%20inverse%20problem%2C%20enabling%0Aideal%20solutions.%20Specifically%2C%20we%20incorporate%20a%20quasi-static%20motion%20model%20into%0Athe%20INR%2C%20granting%20its%20ability%20to%20correct%20subject%27s%20motion.%20To%20stabilize%20model%0Aoptimization%2C%20we%20reformulate%20radial%20MRI%20as%20a%20back-projection%20problem%20using%20the%0AFourier-slice%20theorem.%20Additionally%2C%20we%20propose%20a%20novel%20coarse-to-fine%20hash%0Aencoding%20strategy%2C%20significantly%20enhancing%20MoCo%20accuracy.%20Experiments%20on%0Amultiple%20MRI%20datasets%20show%20our%20Moner%20achieves%20performance%20comparable%20to%20SOTA%0AMoCo%20techniques%20on%20in-domain%20data%2C%20while%20demonstrating%20significant%20improvements%0Aon%20out-of-domain%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16921v1&entry.124074799=Read"},
{"title": "IRASNet: Improved Feature-Level Clutter Reduction for Domain Generalized\n  SAR-ATR", "author": "Oh-Tae Jang and Hae-Kang Song and Min-Jun Kim and Kyung-Hwan Lee and Geon Lee and Sung-Ho Kim and Kyung-Tae Kim", "abstract": "  Recently, computer-aided design models and electromagnetic simulations have\nbeen used to augment synthetic aperture radar (SAR) data for deep learning.\nHowever, an automatic target recognition (ATR) model struggles with domain\nshift when using synthetic data because the model learns specific clutter\npatterns present in such data, which disturbs performance when applied to\nmeasured data with different clutter distributions. This study proposes a\nframework particularly designed for domain-generalized SAR-ATR called IRASNet,\nenabling effective feature-level clutter reduction and domain-invariant feature\nlearning. First, we propose a clutter reduction module (CRM) that maximizes the\nsignal-to-clutter ratio on feature maps. The module reduces the impact of\nclutter at the feature level while preserving target and shadow information,\nthereby improving ATR performance. Second, we integrate adversarial learning\nwith CRM to extract clutter-reduced domain-invariant features. The integration\nbridges the gap between synthetic and measured datasets without requiring\nmeasured data during training. Third, we improve feature extraction from target\nand shadow regions by implementing a positional supervision task using mask\nground truth encoding. The improvement enhances the ability of the model to\ndiscriminate between classes. Our proposed IRASNet presents new\nstate-of-the-art public SAR datasets utilizing target and shadow information to\nachieve superior performance across various test conditions. IRASNet not only\nenhances generalization performance but also significantly improves\nfeature-level clutter reduction, making it a valuable advancement in the field\nof radar image pattern recognition.\n", "link": "http://arxiv.org/abs/2409.16845v1", "date": "2024-09-25", "relevancy": 2.5339, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5314}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4949}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IRASNet%3A%20Improved%20Feature-Level%20Clutter%20Reduction%20for%20Domain%20Generalized%0A%20%20SAR-ATR&body=Title%3A%20IRASNet%3A%20Improved%20Feature-Level%20Clutter%20Reduction%20for%20Domain%20Generalized%0A%20%20SAR-ATR%0AAuthor%3A%20Oh-Tae%20Jang%20and%20Hae-Kang%20Song%20and%20Min-Jun%20Kim%20and%20Kyung-Hwan%20Lee%20and%20Geon%20Lee%20and%20Sung-Ho%20Kim%20and%20Kyung-Tae%20Kim%0AAbstract%3A%20%20%20Recently%2C%20computer-aided%20design%20models%20and%20electromagnetic%20simulations%20have%0Abeen%20used%20to%20augment%20synthetic%20aperture%20radar%20%28SAR%29%20data%20for%20deep%20learning.%0AHowever%2C%20an%20automatic%20target%20recognition%20%28ATR%29%20model%20struggles%20with%20domain%0Ashift%20when%20using%20synthetic%20data%20because%20the%20model%20learns%20specific%20clutter%0Apatterns%20present%20in%20such%20data%2C%20which%20disturbs%20performance%20when%20applied%20to%0Ameasured%20data%20with%20different%20clutter%20distributions.%20This%20study%20proposes%20a%0Aframework%20particularly%20designed%20for%20domain-generalized%20SAR-ATR%20called%20IRASNet%2C%0Aenabling%20effective%20feature-level%20clutter%20reduction%20and%20domain-invariant%20feature%0Alearning.%20First%2C%20we%20propose%20a%20clutter%20reduction%20module%20%28CRM%29%20that%20maximizes%20the%0Asignal-to-clutter%20ratio%20on%20feature%20maps.%20The%20module%20reduces%20the%20impact%20of%0Aclutter%20at%20the%20feature%20level%20while%20preserving%20target%20and%20shadow%20information%2C%0Athereby%20improving%20ATR%20performance.%20Second%2C%20we%20integrate%20adversarial%20learning%0Awith%20CRM%20to%20extract%20clutter-reduced%20domain-invariant%20features.%20The%20integration%0Abridges%20the%20gap%20between%20synthetic%20and%20measured%20datasets%20without%20requiring%0Ameasured%20data%20during%20training.%20Third%2C%20we%20improve%20feature%20extraction%20from%20target%0Aand%20shadow%20regions%20by%20implementing%20a%20positional%20supervision%20task%20using%20mask%0Aground%20truth%20encoding.%20The%20improvement%20enhances%20the%20ability%20of%20the%20model%20to%0Adiscriminate%20between%20classes.%20Our%20proposed%20IRASNet%20presents%20new%0Astate-of-the-art%20public%20SAR%20datasets%20utilizing%20target%20and%20shadow%20information%20to%0Aachieve%20superior%20performance%20across%20various%20test%20conditions.%20IRASNet%20not%20only%0Aenhances%20generalization%20performance%20but%20also%20significantly%20improves%0Afeature-level%20clutter%20reduction%2C%20making%20it%20a%20valuable%20advancement%20in%20the%20field%0Aof%20radar%20image%20pattern%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16845v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIRASNet%253A%2520Improved%2520Feature-Level%2520Clutter%2520Reduction%2520for%2520Domain%2520Generalized%250A%2520%2520SAR-ATR%26entry.906535625%3DOh-Tae%2520Jang%2520and%2520Hae-Kang%2520Song%2520and%2520Min-Jun%2520Kim%2520and%2520Kyung-Hwan%2520Lee%2520and%2520Geon%2520Lee%2520and%2520Sung-Ho%2520Kim%2520and%2520Kyung-Tae%2520Kim%26entry.1292438233%3D%2520%2520Recently%252C%2520computer-aided%2520design%2520models%2520and%2520electromagnetic%2520simulations%2520have%250Abeen%2520used%2520to%2520augment%2520synthetic%2520aperture%2520radar%2520%2528SAR%2529%2520data%2520for%2520deep%2520learning.%250AHowever%252C%2520an%2520automatic%2520target%2520recognition%2520%2528ATR%2529%2520model%2520struggles%2520with%2520domain%250Ashift%2520when%2520using%2520synthetic%2520data%2520because%2520the%2520model%2520learns%2520specific%2520clutter%250Apatterns%2520present%2520in%2520such%2520data%252C%2520which%2520disturbs%2520performance%2520when%2520applied%2520to%250Ameasured%2520data%2520with%2520different%2520clutter%2520distributions.%2520This%2520study%2520proposes%2520a%250Aframework%2520particularly%2520designed%2520for%2520domain-generalized%2520SAR-ATR%2520called%2520IRASNet%252C%250Aenabling%2520effective%2520feature-level%2520clutter%2520reduction%2520and%2520domain-invariant%2520feature%250Alearning.%2520First%252C%2520we%2520propose%2520a%2520clutter%2520reduction%2520module%2520%2528CRM%2529%2520that%2520maximizes%2520the%250Asignal-to-clutter%2520ratio%2520on%2520feature%2520maps.%2520The%2520module%2520reduces%2520the%2520impact%2520of%250Aclutter%2520at%2520the%2520feature%2520level%2520while%2520preserving%2520target%2520and%2520shadow%2520information%252C%250Athereby%2520improving%2520ATR%2520performance.%2520Second%252C%2520we%2520integrate%2520adversarial%2520learning%250Awith%2520CRM%2520to%2520extract%2520clutter-reduced%2520domain-invariant%2520features.%2520The%2520integration%250Abridges%2520the%2520gap%2520between%2520synthetic%2520and%2520measured%2520datasets%2520without%2520requiring%250Ameasured%2520data%2520during%2520training.%2520Third%252C%2520we%2520improve%2520feature%2520extraction%2520from%2520target%250Aand%2520shadow%2520regions%2520by%2520implementing%2520a%2520positional%2520supervision%2520task%2520using%2520mask%250Aground%2520truth%2520encoding.%2520The%2520improvement%2520enhances%2520the%2520ability%2520of%2520the%2520model%2520to%250Adiscriminate%2520between%2520classes.%2520Our%2520proposed%2520IRASNet%2520presents%2520new%250Astate-of-the-art%2520public%2520SAR%2520datasets%2520utilizing%2520target%2520and%2520shadow%2520information%2520to%250Aachieve%2520superior%2520performance%2520across%2520various%2520test%2520conditions.%2520IRASNet%2520not%2520only%250Aenhances%2520generalization%2520performance%2520but%2520also%2520significantly%2520improves%250Afeature-level%2520clutter%2520reduction%252C%2520making%2520it%2520a%2520valuable%2520advancement%2520in%2520the%2520field%250Aof%2520radar%2520image%2520pattern%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16845v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRASNet%3A%20Improved%20Feature-Level%20Clutter%20Reduction%20for%20Domain%20Generalized%0A%20%20SAR-ATR&entry.906535625=Oh-Tae%20Jang%20and%20Hae-Kang%20Song%20and%20Min-Jun%20Kim%20and%20Kyung-Hwan%20Lee%20and%20Geon%20Lee%20and%20Sung-Ho%20Kim%20and%20Kyung-Tae%20Kim&entry.1292438233=%20%20Recently%2C%20computer-aided%20design%20models%20and%20electromagnetic%20simulations%20have%0Abeen%20used%20to%20augment%20synthetic%20aperture%20radar%20%28SAR%29%20data%20for%20deep%20learning.%0AHowever%2C%20an%20automatic%20target%20recognition%20%28ATR%29%20model%20struggles%20with%20domain%0Ashift%20when%20using%20synthetic%20data%20because%20the%20model%20learns%20specific%20clutter%0Apatterns%20present%20in%20such%20data%2C%20which%20disturbs%20performance%20when%20applied%20to%0Ameasured%20data%20with%20different%20clutter%20distributions.%20This%20study%20proposes%20a%0Aframework%20particularly%20designed%20for%20domain-generalized%20SAR-ATR%20called%20IRASNet%2C%0Aenabling%20effective%20feature-level%20clutter%20reduction%20and%20domain-invariant%20feature%0Alearning.%20First%2C%20we%20propose%20a%20clutter%20reduction%20module%20%28CRM%29%20that%20maximizes%20the%0Asignal-to-clutter%20ratio%20on%20feature%20maps.%20The%20module%20reduces%20the%20impact%20of%0Aclutter%20at%20the%20feature%20level%20while%20preserving%20target%20and%20shadow%20information%2C%0Athereby%20improving%20ATR%20performance.%20Second%2C%20we%20integrate%20adversarial%20learning%0Awith%20CRM%20to%20extract%20clutter-reduced%20domain-invariant%20features.%20The%20integration%0Abridges%20the%20gap%20between%20synthetic%20and%20measured%20datasets%20without%20requiring%0Ameasured%20data%20during%20training.%20Third%2C%20we%20improve%20feature%20extraction%20from%20target%0Aand%20shadow%20regions%20by%20implementing%20a%20positional%20supervision%20task%20using%20mask%0Aground%20truth%20encoding.%20The%20improvement%20enhances%20the%20ability%20of%20the%20model%20to%0Adiscriminate%20between%20classes.%20Our%20proposed%20IRASNet%20presents%20new%0Astate-of-the-art%20public%20SAR%20datasets%20utilizing%20target%20and%20shadow%20information%20to%0Aachieve%20superior%20performance%20across%20various%20test%20conditions.%20IRASNet%20not%20only%0Aenhances%20generalization%20performance%20but%20also%20significantly%20improves%0Afeature-level%20clutter%20reduction%2C%20making%20it%20a%20valuable%20advancement%20in%20the%20field%0Aof%20radar%20image%20pattern%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16845v1&entry.124074799=Read"},
{"title": "Informed deep hierarchical classification: a non-standard analysis\n  inspired approach", "author": "Lorenzo Fiaschi and Marco Cococcioni", "abstract": "  This work proposes a novel approach to the deep hierarchical classification\ntask, i.e., the problem of classifying data according to multiple labels\norganized in a rigid parent-child structure. It consists in a multi-output deep\nneural network equipped with specific projection operators placed before each\noutput layer. The design of such an architecture, called lexicographic hybrid\ndeep neural network (LH-DNN), has been possible by combining tools from\ndifferent and quite distant research fields: lexicographic multi-objective\noptimization, non-standard analysis, and deep learning. To assess the efficacy\nof the approach, the resulting network is compared against the B-CNN, a\nconvolutional neural network tailored for hierarchical classification tasks, on\nthe CIFAR10, CIFAR100 (where it has been originally and recently proposed\nbefore being adopted and tuned for multiple real-world applications) and\nFashion-MNIST benchmarks. Evidence states that an LH-DNN can achieve comparable\nif not superior performance, especially in the learning of the hierarchical\nrelations, in the face of a drastic reduction of the learning parameters,\ntraining epochs, and computational time, without the need for ad-hoc loss\nfunctions weighting values.\n", "link": "http://arxiv.org/abs/2409.16956v1", "date": "2024-09-25", "relevancy": 2.522, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5179}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5112}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Informed%20deep%20hierarchical%20classification%3A%20a%20non-standard%20analysis%0A%20%20inspired%20approach&body=Title%3A%20Informed%20deep%20hierarchical%20classification%3A%20a%20non-standard%20analysis%0A%20%20inspired%20approach%0AAuthor%3A%20Lorenzo%20Fiaschi%20and%20Marco%20Cococcioni%0AAbstract%3A%20%20%20This%20work%20proposes%20a%20novel%20approach%20to%20the%20deep%20hierarchical%20classification%0Atask%2C%20i.e.%2C%20the%20problem%20of%20classifying%20data%20according%20to%20multiple%20labels%0Aorganized%20in%20a%20rigid%20parent-child%20structure.%20It%20consists%20in%20a%20multi-output%20deep%0Aneural%20network%20equipped%20with%20specific%20projection%20operators%20placed%20before%20each%0Aoutput%20layer.%20The%20design%20of%20such%20an%20architecture%2C%20called%20lexicographic%20hybrid%0Adeep%20neural%20network%20%28LH-DNN%29%2C%20has%20been%20possible%20by%20combining%20tools%20from%0Adifferent%20and%20quite%20distant%20research%20fields%3A%20lexicographic%20multi-objective%0Aoptimization%2C%20non-standard%20analysis%2C%20and%20deep%20learning.%20To%20assess%20the%20efficacy%0Aof%20the%20approach%2C%20the%20resulting%20network%20is%20compared%20against%20the%20B-CNN%2C%20a%0Aconvolutional%20neural%20network%20tailored%20for%20hierarchical%20classification%20tasks%2C%20on%0Athe%20CIFAR10%2C%20CIFAR100%20%28where%20it%20has%20been%20originally%20and%20recently%20proposed%0Abefore%20being%20adopted%20and%20tuned%20for%20multiple%20real-world%20applications%29%20and%0AFashion-MNIST%20benchmarks.%20Evidence%20states%20that%20an%20LH-DNN%20can%20achieve%20comparable%0Aif%20not%20superior%20performance%2C%20especially%20in%20the%20learning%20of%20the%20hierarchical%0Arelations%2C%20in%20the%20face%20of%20a%20drastic%20reduction%20of%20the%20learning%20parameters%2C%0Atraining%20epochs%2C%20and%20computational%20time%2C%20without%20the%20need%20for%20ad-hoc%20loss%0Afunctions%20weighting%20values.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16956v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInformed%2520deep%2520hierarchical%2520classification%253A%2520a%2520non-standard%2520analysis%250A%2520%2520inspired%2520approach%26entry.906535625%3DLorenzo%2520Fiaschi%2520and%2520Marco%2520Cococcioni%26entry.1292438233%3D%2520%2520This%2520work%2520proposes%2520a%2520novel%2520approach%2520to%2520the%2520deep%2520hierarchical%2520classification%250Atask%252C%2520i.e.%252C%2520the%2520problem%2520of%2520classifying%2520data%2520according%2520to%2520multiple%2520labels%250Aorganized%2520in%2520a%2520rigid%2520parent-child%2520structure.%2520It%2520consists%2520in%2520a%2520multi-output%2520deep%250Aneural%2520network%2520equipped%2520with%2520specific%2520projection%2520operators%2520placed%2520before%2520each%250Aoutput%2520layer.%2520The%2520design%2520of%2520such%2520an%2520architecture%252C%2520called%2520lexicographic%2520hybrid%250Adeep%2520neural%2520network%2520%2528LH-DNN%2529%252C%2520has%2520been%2520possible%2520by%2520combining%2520tools%2520from%250Adifferent%2520and%2520quite%2520distant%2520research%2520fields%253A%2520lexicographic%2520multi-objective%250Aoptimization%252C%2520non-standard%2520analysis%252C%2520and%2520deep%2520learning.%2520To%2520assess%2520the%2520efficacy%250Aof%2520the%2520approach%252C%2520the%2520resulting%2520network%2520is%2520compared%2520against%2520the%2520B-CNN%252C%2520a%250Aconvolutional%2520neural%2520network%2520tailored%2520for%2520hierarchical%2520classification%2520tasks%252C%2520on%250Athe%2520CIFAR10%252C%2520CIFAR100%2520%2528where%2520it%2520has%2520been%2520originally%2520and%2520recently%2520proposed%250Abefore%2520being%2520adopted%2520and%2520tuned%2520for%2520multiple%2520real-world%2520applications%2529%2520and%250AFashion-MNIST%2520benchmarks.%2520Evidence%2520states%2520that%2520an%2520LH-DNN%2520can%2520achieve%2520comparable%250Aif%2520not%2520superior%2520performance%252C%2520especially%2520in%2520the%2520learning%2520of%2520the%2520hierarchical%250Arelations%252C%2520in%2520the%2520face%2520of%2520a%2520drastic%2520reduction%2520of%2520the%2520learning%2520parameters%252C%250Atraining%2520epochs%252C%2520and%2520computational%2520time%252C%2520without%2520the%2520need%2520for%2520ad-hoc%2520loss%250Afunctions%2520weighting%2520values.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16956v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Informed%20deep%20hierarchical%20classification%3A%20a%20non-standard%20analysis%0A%20%20inspired%20approach&entry.906535625=Lorenzo%20Fiaschi%20and%20Marco%20Cococcioni&entry.1292438233=%20%20This%20work%20proposes%20a%20novel%20approach%20to%20the%20deep%20hierarchical%20classification%0Atask%2C%20i.e.%2C%20the%20problem%20of%20classifying%20data%20according%20to%20multiple%20labels%0Aorganized%20in%20a%20rigid%20parent-child%20structure.%20It%20consists%20in%20a%20multi-output%20deep%0Aneural%20network%20equipped%20with%20specific%20projection%20operators%20placed%20before%20each%0Aoutput%20layer.%20The%20design%20of%20such%20an%20architecture%2C%20called%20lexicographic%20hybrid%0Adeep%20neural%20network%20%28LH-DNN%29%2C%20has%20been%20possible%20by%20combining%20tools%20from%0Adifferent%20and%20quite%20distant%20research%20fields%3A%20lexicographic%20multi-objective%0Aoptimization%2C%20non-standard%20analysis%2C%20and%20deep%20learning.%20To%20assess%20the%20efficacy%0Aof%20the%20approach%2C%20the%20resulting%20network%20is%20compared%20against%20the%20B-CNN%2C%20a%0Aconvolutional%20neural%20network%20tailored%20for%20hierarchical%20classification%20tasks%2C%20on%0Athe%20CIFAR10%2C%20CIFAR100%20%28where%20it%20has%20been%20originally%20and%20recently%20proposed%0Abefore%20being%20adopted%20and%20tuned%20for%20multiple%20real-world%20applications%29%20and%0AFashion-MNIST%20benchmarks.%20Evidence%20states%20that%20an%20LH-DNN%20can%20achieve%20comparable%0Aif%20not%20superior%20performance%2C%20especially%20in%20the%20learning%20of%20the%20hierarchical%0Arelations%2C%20in%20the%20face%20of%20a%20drastic%20reduction%20of%20the%20learning%20parameters%2C%0Atraining%20epochs%2C%20and%20computational%20time%2C%20without%20the%20need%20for%20ad-hoc%20loss%0Afunctions%20weighting%20values.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16956v1&entry.124074799=Read"},
{"title": "Robust Scene Change Detection Using Visual Foundation Models and\n  Cross-Attention Mechanisms", "author": "Chun-Jung Lin and Sourav Garg and Tat-Jun Chin and Feras Dayoub", "abstract": "  We present a novel method for scene change detection that leverages the\nrobust feature extraction capabilities of a visual foundational model, DINOv2,\nand integrates full-image cross-attention to address key challenges such as\nvarying lighting, seasonal variations, and viewpoint differences. In order to\neffectively learn correspondences and mis-correspondences between an image pair\nfor the change detection task, we propose to a) ``freeze'' the backbone in\norder to retain the generality of dense foundation features, and b) employ\n``full-image'' cross-attention to better tackle the viewpoint variations\nbetween the image pair. We evaluate our approach on two benchmark datasets,\nVL-CMU-CD and PSCD, along with their viewpoint-varied versions. Our experiments\ndemonstrate significant improvements in F1-score, particularly in scenarios\ninvolving geometric changes between image pairs. The results indicate our\nmethod's superior generalization capabilities over existing state-of-the-art\napproaches, showing robustness against photometric and geometric variations as\nwell as better overall generalization when fine-tuned to adapt to new\nenvironments. Detailed ablation studies further validate the contributions of\neach component in our architecture. Source code will be made publicly available\nupon acceptance.\n", "link": "http://arxiv.org/abs/2409.16850v1", "date": "2024-09-25", "relevancy": 2.5086, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6364}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6364}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Scene%20Change%20Detection%20Using%20Visual%20Foundation%20Models%20and%0A%20%20Cross-Attention%20Mechanisms&body=Title%3A%20Robust%20Scene%20Change%20Detection%20Using%20Visual%20Foundation%20Models%20and%0A%20%20Cross-Attention%20Mechanisms%0AAuthor%3A%20Chun-Jung%20Lin%20and%20Sourav%20Garg%20and%20Tat-Jun%20Chin%20and%20Feras%20Dayoub%0AAbstract%3A%20%20%20We%20present%20a%20novel%20method%20for%20scene%20change%20detection%20that%20leverages%20the%0Arobust%20feature%20extraction%20capabilities%20of%20a%20visual%20foundational%20model%2C%20DINOv2%2C%0Aand%20integrates%20full-image%20cross-attention%20to%20address%20key%20challenges%20such%20as%0Avarying%20lighting%2C%20seasonal%20variations%2C%20and%20viewpoint%20differences.%20In%20order%20to%0Aeffectively%20learn%20correspondences%20and%20mis-correspondences%20between%20an%20image%20pair%0Afor%20the%20change%20detection%20task%2C%20we%20propose%20to%20a%29%20%60%60freeze%27%27%20the%20backbone%20in%0Aorder%20to%20retain%20the%20generality%20of%20dense%20foundation%20features%2C%20and%20b%29%20employ%0A%60%60full-image%27%27%20cross-attention%20to%20better%20tackle%20the%20viewpoint%20variations%0Abetween%20the%20image%20pair.%20We%20evaluate%20our%20approach%20on%20two%20benchmark%20datasets%2C%0AVL-CMU-CD%20and%20PSCD%2C%20along%20with%20their%20viewpoint-varied%20versions.%20Our%20experiments%0Ademonstrate%20significant%20improvements%20in%20F1-score%2C%20particularly%20in%20scenarios%0Ainvolving%20geometric%20changes%20between%20image%20pairs.%20The%20results%20indicate%20our%0Amethod%27s%20superior%20generalization%20capabilities%20over%20existing%20state-of-the-art%0Aapproaches%2C%20showing%20robustness%20against%20photometric%20and%20geometric%20variations%20as%0Awell%20as%20better%20overall%20generalization%20when%20fine-tuned%20to%20adapt%20to%20new%0Aenvironments.%20Detailed%20ablation%20studies%20further%20validate%20the%20contributions%20of%0Aeach%20component%20in%20our%20architecture.%20Source%20code%20will%20be%20made%20publicly%20available%0Aupon%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Scene%2520Change%2520Detection%2520Using%2520Visual%2520Foundation%2520Models%2520and%250A%2520%2520Cross-Attention%2520Mechanisms%26entry.906535625%3DChun-Jung%2520Lin%2520and%2520Sourav%2520Garg%2520and%2520Tat-Jun%2520Chin%2520and%2520Feras%2520Dayoub%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520method%2520for%2520scene%2520change%2520detection%2520that%2520leverages%2520the%250Arobust%2520feature%2520extraction%2520capabilities%2520of%2520a%2520visual%2520foundational%2520model%252C%2520DINOv2%252C%250Aand%2520integrates%2520full-image%2520cross-attention%2520to%2520address%2520key%2520challenges%2520such%2520as%250Avarying%2520lighting%252C%2520seasonal%2520variations%252C%2520and%2520viewpoint%2520differences.%2520In%2520order%2520to%250Aeffectively%2520learn%2520correspondences%2520and%2520mis-correspondences%2520between%2520an%2520image%2520pair%250Afor%2520the%2520change%2520detection%2520task%252C%2520we%2520propose%2520to%2520a%2529%2520%2560%2560freeze%2527%2527%2520the%2520backbone%2520in%250Aorder%2520to%2520retain%2520the%2520generality%2520of%2520dense%2520foundation%2520features%252C%2520and%2520b%2529%2520employ%250A%2560%2560full-image%2527%2527%2520cross-attention%2520to%2520better%2520tackle%2520the%2520viewpoint%2520variations%250Abetween%2520the%2520image%2520pair.%2520We%2520evaluate%2520our%2520approach%2520on%2520two%2520benchmark%2520datasets%252C%250AVL-CMU-CD%2520and%2520PSCD%252C%2520along%2520with%2520their%2520viewpoint-varied%2520versions.%2520Our%2520experiments%250Ademonstrate%2520significant%2520improvements%2520in%2520F1-score%252C%2520particularly%2520in%2520scenarios%250Ainvolving%2520geometric%2520changes%2520between%2520image%2520pairs.%2520The%2520results%2520indicate%2520our%250Amethod%2527s%2520superior%2520generalization%2520capabilities%2520over%2520existing%2520state-of-the-art%250Aapproaches%252C%2520showing%2520robustness%2520against%2520photometric%2520and%2520geometric%2520variations%2520as%250Awell%2520as%2520better%2520overall%2520generalization%2520when%2520fine-tuned%2520to%2520adapt%2520to%2520new%250Aenvironments.%2520Detailed%2520ablation%2520studies%2520further%2520validate%2520the%2520contributions%2520of%250Aeach%2520component%2520in%2520our%2520architecture.%2520Source%2520code%2520will%2520be%2520made%2520publicly%2520available%250Aupon%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Scene%20Change%20Detection%20Using%20Visual%20Foundation%20Models%20and%0A%20%20Cross-Attention%20Mechanisms&entry.906535625=Chun-Jung%20Lin%20and%20Sourav%20Garg%20and%20Tat-Jun%20Chin%20and%20Feras%20Dayoub&entry.1292438233=%20%20We%20present%20a%20novel%20method%20for%20scene%20change%20detection%20that%20leverages%20the%0Arobust%20feature%20extraction%20capabilities%20of%20a%20visual%20foundational%20model%2C%20DINOv2%2C%0Aand%20integrates%20full-image%20cross-attention%20to%20address%20key%20challenges%20such%20as%0Avarying%20lighting%2C%20seasonal%20variations%2C%20and%20viewpoint%20differences.%20In%20order%20to%0Aeffectively%20learn%20correspondences%20and%20mis-correspondences%20between%20an%20image%20pair%0Afor%20the%20change%20detection%20task%2C%20we%20propose%20to%20a%29%20%60%60freeze%27%27%20the%20backbone%20in%0Aorder%20to%20retain%20the%20generality%20of%20dense%20foundation%20features%2C%20and%20b%29%20employ%0A%60%60full-image%27%27%20cross-attention%20to%20better%20tackle%20the%20viewpoint%20variations%0Abetween%20the%20image%20pair.%20We%20evaluate%20our%20approach%20on%20two%20benchmark%20datasets%2C%0AVL-CMU-CD%20and%20PSCD%2C%20along%20with%20their%20viewpoint-varied%20versions.%20Our%20experiments%0Ademonstrate%20significant%20improvements%20in%20F1-score%2C%20particularly%20in%20scenarios%0Ainvolving%20geometric%20changes%20between%20image%20pairs.%20The%20results%20indicate%20our%0Amethod%27s%20superior%20generalization%20capabilities%20over%20existing%20state-of-the-art%0Aapproaches%2C%20showing%20robustness%20against%20photometric%20and%20geometric%20variations%20as%0Awell%20as%20better%20overall%20generalization%20when%20fine-tuned%20to%20adapt%20to%20new%0Aenvironments.%20Detailed%20ablation%20studies%20further%20validate%20the%20contributions%20of%0Aeach%20component%20in%20our%20architecture.%20Source%20code%20will%20be%20made%20publicly%20available%0Aupon%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16850v1&entry.124074799=Read"},
{"title": "General Detection-based Text Line Recognition", "author": "Raphael Baena and Syrine Kalleli and Mathieu Aubry", "abstract": "  We introduce a general detection-based approach to text line recognition, be\nit printed (OCR) or handwritten (HTR), with Latin, Chinese, or ciphered\ncharacters. Detection-based approaches have until now been largely discarded\nfor HTR because reading characters separately is often challenging, and\ncharacter-level annotation is difficult and expensive. We overcome these\nchallenges thanks to three main insights: (i) synthetic pre-training with\nsufficiently diverse data enables learning reasonable character localization\nfor any script; (ii) modern transformer-based detectors can jointly detect a\nlarge number of instances, and, if trained with an adequate masking strategy,\nleverage consistency between the different detections; (iii) once a pre-trained\ndetection model with approximate character localization is available, it is\npossible to fine-tune it with line-level annotation on real data, even with a\ndifferent alphabet. Our approach, dubbed DTLR, builds on a completely different\nparadigm than state-of-the-art HTR methods, which rely on autoregressive\ndecoding, predicting character values one by one, while we treat a complete\nline in parallel. Remarkably, we demonstrate good performance on a large range\nof scripts, usually tackled with specialized approaches. In particular, we\nimprove state-of-the-art performances for Chinese script recognition on the\nCASIA v2 dataset, and for cipher recognition on the Borg and Copiale datasets.\nOur code and models are available at https://github.com/raphael-baena/DTLR.\n", "link": "http://arxiv.org/abs/2409.17095v1", "date": "2024-09-25", "relevancy": 2.5009, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5094}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4968}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4944}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20General%20Detection-based%20Text%20Line%20Recognition&body=Title%3A%20General%20Detection-based%20Text%20Line%20Recognition%0AAuthor%3A%20Raphael%20Baena%20and%20Syrine%20Kalleli%20and%20Mathieu%20Aubry%0AAbstract%3A%20%20%20We%20introduce%20a%20general%20detection-based%20approach%20to%20text%20line%20recognition%2C%20be%0Ait%20printed%20%28OCR%29%20or%20handwritten%20%28HTR%29%2C%20with%20Latin%2C%20Chinese%2C%20or%20ciphered%0Acharacters.%20Detection-based%20approaches%20have%20until%20now%20been%20largely%20discarded%0Afor%20HTR%20because%20reading%20characters%20separately%20is%20often%20challenging%2C%20and%0Acharacter-level%20annotation%20is%20difficult%20and%20expensive.%20We%20overcome%20these%0Achallenges%20thanks%20to%20three%20main%20insights%3A%20%28i%29%20synthetic%20pre-training%20with%0Asufficiently%20diverse%20data%20enables%20learning%20reasonable%20character%20localization%0Afor%20any%20script%3B%20%28ii%29%20modern%20transformer-based%20detectors%20can%20jointly%20detect%20a%0Alarge%20number%20of%20instances%2C%20and%2C%20if%20trained%20with%20an%20adequate%20masking%20strategy%2C%0Aleverage%20consistency%20between%20the%20different%20detections%3B%20%28iii%29%20once%20a%20pre-trained%0Adetection%20model%20with%20approximate%20character%20localization%20is%20available%2C%20it%20is%0Apossible%20to%20fine-tune%20it%20with%20line-level%20annotation%20on%20real%20data%2C%20even%20with%20a%0Adifferent%20alphabet.%20Our%20approach%2C%20dubbed%20DTLR%2C%20builds%20on%20a%20completely%20different%0Aparadigm%20than%20state-of-the-art%20HTR%20methods%2C%20which%20rely%20on%20autoregressive%0Adecoding%2C%20predicting%20character%20values%20one%20by%20one%2C%20while%20we%20treat%20a%20complete%0Aline%20in%20parallel.%20Remarkably%2C%20we%20demonstrate%20good%20performance%20on%20a%20large%20range%0Aof%20scripts%2C%20usually%20tackled%20with%20specialized%20approaches.%20In%20particular%2C%20we%0Aimprove%20state-of-the-art%20performances%20for%20Chinese%20script%20recognition%20on%20the%0ACASIA%20v2%20dataset%2C%20and%20for%20cipher%20recognition%20on%20the%20Borg%20and%20Copiale%20datasets.%0AOur%20code%20and%20models%20are%20available%20at%20https%3A//github.com/raphael-baena/DTLR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneral%2520Detection-based%2520Text%2520Line%2520Recognition%26entry.906535625%3DRaphael%2520Baena%2520and%2520Syrine%2520Kalleli%2520and%2520Mathieu%2520Aubry%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520general%2520detection-based%2520approach%2520to%2520text%2520line%2520recognition%252C%2520be%250Ait%2520printed%2520%2528OCR%2529%2520or%2520handwritten%2520%2528HTR%2529%252C%2520with%2520Latin%252C%2520Chinese%252C%2520or%2520ciphered%250Acharacters.%2520Detection-based%2520approaches%2520have%2520until%2520now%2520been%2520largely%2520discarded%250Afor%2520HTR%2520because%2520reading%2520characters%2520separately%2520is%2520often%2520challenging%252C%2520and%250Acharacter-level%2520annotation%2520is%2520difficult%2520and%2520expensive.%2520We%2520overcome%2520these%250Achallenges%2520thanks%2520to%2520three%2520main%2520insights%253A%2520%2528i%2529%2520synthetic%2520pre-training%2520with%250Asufficiently%2520diverse%2520data%2520enables%2520learning%2520reasonable%2520character%2520localization%250Afor%2520any%2520script%253B%2520%2528ii%2529%2520modern%2520transformer-based%2520detectors%2520can%2520jointly%2520detect%2520a%250Alarge%2520number%2520of%2520instances%252C%2520and%252C%2520if%2520trained%2520with%2520an%2520adequate%2520masking%2520strategy%252C%250Aleverage%2520consistency%2520between%2520the%2520different%2520detections%253B%2520%2528iii%2529%2520once%2520a%2520pre-trained%250Adetection%2520model%2520with%2520approximate%2520character%2520localization%2520is%2520available%252C%2520it%2520is%250Apossible%2520to%2520fine-tune%2520it%2520with%2520line-level%2520annotation%2520on%2520real%2520data%252C%2520even%2520with%2520a%250Adifferent%2520alphabet.%2520Our%2520approach%252C%2520dubbed%2520DTLR%252C%2520builds%2520on%2520a%2520completely%2520different%250Aparadigm%2520than%2520state-of-the-art%2520HTR%2520methods%252C%2520which%2520rely%2520on%2520autoregressive%250Adecoding%252C%2520predicting%2520character%2520values%2520one%2520by%2520one%252C%2520while%2520we%2520treat%2520a%2520complete%250Aline%2520in%2520parallel.%2520Remarkably%252C%2520we%2520demonstrate%2520good%2520performance%2520on%2520a%2520large%2520range%250Aof%2520scripts%252C%2520usually%2520tackled%2520with%2520specialized%2520approaches.%2520In%2520particular%252C%2520we%250Aimprove%2520state-of-the-art%2520performances%2520for%2520Chinese%2520script%2520recognition%2520on%2520the%250ACASIA%2520v2%2520dataset%252C%2520and%2520for%2520cipher%2520recognition%2520on%2520the%2520Borg%2520and%2520Copiale%2520datasets.%250AOur%2520code%2520and%2520models%2520are%2520available%2520at%2520https%253A//github.com/raphael-baena/DTLR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=General%20Detection-based%20Text%20Line%20Recognition&entry.906535625=Raphael%20Baena%20and%20Syrine%20Kalleli%20and%20Mathieu%20Aubry&entry.1292438233=%20%20We%20introduce%20a%20general%20detection-based%20approach%20to%20text%20line%20recognition%2C%20be%0Ait%20printed%20%28OCR%29%20or%20handwritten%20%28HTR%29%2C%20with%20Latin%2C%20Chinese%2C%20or%20ciphered%0Acharacters.%20Detection-based%20approaches%20have%20until%20now%20been%20largely%20discarded%0Afor%20HTR%20because%20reading%20characters%20separately%20is%20often%20challenging%2C%20and%0Acharacter-level%20annotation%20is%20difficult%20and%20expensive.%20We%20overcome%20these%0Achallenges%20thanks%20to%20three%20main%20insights%3A%20%28i%29%20synthetic%20pre-training%20with%0Asufficiently%20diverse%20data%20enables%20learning%20reasonable%20character%20localization%0Afor%20any%20script%3B%20%28ii%29%20modern%20transformer-based%20detectors%20can%20jointly%20detect%20a%0Alarge%20number%20of%20instances%2C%20and%2C%20if%20trained%20with%20an%20adequate%20masking%20strategy%2C%0Aleverage%20consistency%20between%20the%20different%20detections%3B%20%28iii%29%20once%20a%20pre-trained%0Adetection%20model%20with%20approximate%20character%20localization%20is%20available%2C%20it%20is%0Apossible%20to%20fine-tune%20it%20with%20line-level%20annotation%20on%20real%20data%2C%20even%20with%20a%0Adifferent%20alphabet.%20Our%20approach%2C%20dubbed%20DTLR%2C%20builds%20on%20a%20completely%20different%0Aparadigm%20than%20state-of-the-art%20HTR%20methods%2C%20which%20rely%20on%20autoregressive%0Adecoding%2C%20predicting%20character%20values%20one%20by%20one%2C%20while%20we%20treat%20a%20complete%0Aline%20in%20parallel.%20Remarkably%2C%20we%20demonstrate%20good%20performance%20on%20a%20large%20range%0Aof%20scripts%2C%20usually%20tackled%20with%20specialized%20approaches.%20In%20particular%2C%20we%0Aimprove%20state-of-the-art%20performances%20for%20Chinese%20script%20recognition%20on%20the%0ACASIA%20v2%20dataset%2C%20and%20for%20cipher%20recognition%20on%20the%20Borg%20and%20Copiale%20datasets.%0AOur%20code%20and%20models%20are%20available%20at%20https%3A//github.com/raphael-baena/DTLR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17095v1&entry.124074799=Read"},
{"title": "Guide-and-Rescale: Self-Guidance Mechanism for Effective Tuning-Free\n  Real Image Editing", "author": "Vadim Titov and Madina Khalmatova and Alexandra Ivanova and Dmitry Vetrov and Aibek Alanov", "abstract": "  Despite recent advances in large-scale text-to-image generative models,\nmanipulating real images with these models remains a challenging problem. The\nmain limitations of existing editing methods are that they either fail to\nperform with consistent quality on a wide range of image edits or require\ntime-consuming hyperparameter tuning or fine-tuning of the diffusion model to\npreserve the image-specific appearance of the input image. We propose a novel\napproach that is built upon a modified diffusion sampling process via the\nguidance mechanism. In this work, we explore the self-guidance technique to\npreserve the overall structure of the input image and its local regions\nappearance that should not be edited. In particular, we explicitly introduce\nlayout-preserving energy functions that are aimed to save local and global\nstructures of the source image. Additionally, we propose a noise rescaling\nmechanism that allows to preserve noise distribution by balancing the norms of\nclassifier-free guidance and our proposed guiders during generation. Such a\nguiding approach does not require fine-tuning the diffusion model and exact\ninversion process. As a result, the proposed method provides a fast and\nhigh-quality editing mechanism. In our experiments, we show through human\nevaluation and quantitative analysis that the proposed method allows to produce\ndesired editing which is more preferable by humans and also achieves a better\ntrade-off between editing quality and preservation of the original image. Our\ncode is available at https://github.com/MACderRu/Guide-and-Rescale.\n", "link": "http://arxiv.org/abs/2409.01322v3", "date": "2024-09-25", "relevancy": 2.4895, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6559}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6321}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guide-and-Rescale%3A%20Self-Guidance%20Mechanism%20for%20Effective%20Tuning-Free%0A%20%20Real%20Image%20Editing&body=Title%3A%20Guide-and-Rescale%3A%20Self-Guidance%20Mechanism%20for%20Effective%20Tuning-Free%0A%20%20Real%20Image%20Editing%0AAuthor%3A%20Vadim%20Titov%20and%20Madina%20Khalmatova%20and%20Alexandra%20Ivanova%20and%20Dmitry%20Vetrov%20and%20Aibek%20Alanov%0AAbstract%3A%20%20%20Despite%20recent%20advances%20in%20large-scale%20text-to-image%20generative%20models%2C%0Amanipulating%20real%20images%20with%20these%20models%20remains%20a%20challenging%20problem.%20The%0Amain%20limitations%20of%20existing%20editing%20methods%20are%20that%20they%20either%20fail%20to%0Aperform%20with%20consistent%20quality%20on%20a%20wide%20range%20of%20image%20edits%20or%20require%0Atime-consuming%20hyperparameter%20tuning%20or%20fine-tuning%20of%20the%20diffusion%20model%20to%0Apreserve%20the%20image-specific%20appearance%20of%20the%20input%20image.%20We%20propose%20a%20novel%0Aapproach%20that%20is%20built%20upon%20a%20modified%20diffusion%20sampling%20process%20via%20the%0Aguidance%20mechanism.%20In%20this%20work%2C%20we%20explore%20the%20self-guidance%20technique%20to%0Apreserve%20the%20overall%20structure%20of%20the%20input%20image%20and%20its%20local%20regions%0Aappearance%20that%20should%20not%20be%20edited.%20In%20particular%2C%20we%20explicitly%20introduce%0Alayout-preserving%20energy%20functions%20that%20are%20aimed%20to%20save%20local%20and%20global%0Astructures%20of%20the%20source%20image.%20Additionally%2C%20we%20propose%20a%20noise%20rescaling%0Amechanism%20that%20allows%20to%20preserve%20noise%20distribution%20by%20balancing%20the%20norms%20of%0Aclassifier-free%20guidance%20and%20our%20proposed%20guiders%20during%20generation.%20Such%20a%0Aguiding%20approach%20does%20not%20require%20fine-tuning%20the%20diffusion%20model%20and%20exact%0Ainversion%20process.%20As%20a%20result%2C%20the%20proposed%20method%20provides%20a%20fast%20and%0Ahigh-quality%20editing%20mechanism.%20In%20our%20experiments%2C%20we%20show%20through%20human%0Aevaluation%20and%20quantitative%20analysis%20that%20the%20proposed%20method%20allows%20to%20produce%0Adesired%20editing%20which%20is%20more%20preferable%20by%20humans%20and%20also%20achieves%20a%20better%0Atrade-off%20between%20editing%20quality%20and%20preservation%20of%20the%20original%20image.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/MACderRu/Guide-and-Rescale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.01322v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuide-and-Rescale%253A%2520Self-Guidance%2520Mechanism%2520for%2520Effective%2520Tuning-Free%250A%2520%2520Real%2520Image%2520Editing%26entry.906535625%3DVadim%2520Titov%2520and%2520Madina%2520Khalmatova%2520and%2520Alexandra%2520Ivanova%2520and%2520Dmitry%2520Vetrov%2520and%2520Aibek%2520Alanov%26entry.1292438233%3D%2520%2520Despite%2520recent%2520advances%2520in%2520large-scale%2520text-to-image%2520generative%2520models%252C%250Amanipulating%2520real%2520images%2520with%2520these%2520models%2520remains%2520a%2520challenging%2520problem.%2520The%250Amain%2520limitations%2520of%2520existing%2520editing%2520methods%2520are%2520that%2520they%2520either%2520fail%2520to%250Aperform%2520with%2520consistent%2520quality%2520on%2520a%2520wide%2520range%2520of%2520image%2520edits%2520or%2520require%250Atime-consuming%2520hyperparameter%2520tuning%2520or%2520fine-tuning%2520of%2520the%2520diffusion%2520model%2520to%250Apreserve%2520the%2520image-specific%2520appearance%2520of%2520the%2520input%2520image.%2520We%2520propose%2520a%2520novel%250Aapproach%2520that%2520is%2520built%2520upon%2520a%2520modified%2520diffusion%2520sampling%2520process%2520via%2520the%250Aguidance%2520mechanism.%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520self-guidance%2520technique%2520to%250Apreserve%2520the%2520overall%2520structure%2520of%2520the%2520input%2520image%2520and%2520its%2520local%2520regions%250Aappearance%2520that%2520should%2520not%2520be%2520edited.%2520In%2520particular%252C%2520we%2520explicitly%2520introduce%250Alayout-preserving%2520energy%2520functions%2520that%2520are%2520aimed%2520to%2520save%2520local%2520and%2520global%250Astructures%2520of%2520the%2520source%2520image.%2520Additionally%252C%2520we%2520propose%2520a%2520noise%2520rescaling%250Amechanism%2520that%2520allows%2520to%2520preserve%2520noise%2520distribution%2520by%2520balancing%2520the%2520norms%2520of%250Aclassifier-free%2520guidance%2520and%2520our%2520proposed%2520guiders%2520during%2520generation.%2520Such%2520a%250Aguiding%2520approach%2520does%2520not%2520require%2520fine-tuning%2520the%2520diffusion%2520model%2520and%2520exact%250Ainversion%2520process.%2520As%2520a%2520result%252C%2520the%2520proposed%2520method%2520provides%2520a%2520fast%2520and%250Ahigh-quality%2520editing%2520mechanism.%2520In%2520our%2520experiments%252C%2520we%2520show%2520through%2520human%250Aevaluation%2520and%2520quantitative%2520analysis%2520that%2520the%2520proposed%2520method%2520allows%2520to%2520produce%250Adesired%2520editing%2520which%2520is%2520more%2520preferable%2520by%2520humans%2520and%2520also%2520achieves%2520a%2520better%250Atrade-off%2520between%2520editing%2520quality%2520and%2520preservation%2520of%2520the%2520original%2520image.%2520Our%250Acode%2520is%2520available%2520at%2520https%253A//github.com/MACderRu/Guide-and-Rescale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.01322v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guide-and-Rescale%3A%20Self-Guidance%20Mechanism%20for%20Effective%20Tuning-Free%0A%20%20Real%20Image%20Editing&entry.906535625=Vadim%20Titov%20and%20Madina%20Khalmatova%20and%20Alexandra%20Ivanova%20and%20Dmitry%20Vetrov%20and%20Aibek%20Alanov&entry.1292438233=%20%20Despite%20recent%20advances%20in%20large-scale%20text-to-image%20generative%20models%2C%0Amanipulating%20real%20images%20with%20these%20models%20remains%20a%20challenging%20problem.%20The%0Amain%20limitations%20of%20existing%20editing%20methods%20are%20that%20they%20either%20fail%20to%0Aperform%20with%20consistent%20quality%20on%20a%20wide%20range%20of%20image%20edits%20or%20require%0Atime-consuming%20hyperparameter%20tuning%20or%20fine-tuning%20of%20the%20diffusion%20model%20to%0Apreserve%20the%20image-specific%20appearance%20of%20the%20input%20image.%20We%20propose%20a%20novel%0Aapproach%20that%20is%20built%20upon%20a%20modified%20diffusion%20sampling%20process%20via%20the%0Aguidance%20mechanism.%20In%20this%20work%2C%20we%20explore%20the%20self-guidance%20technique%20to%0Apreserve%20the%20overall%20structure%20of%20the%20input%20image%20and%20its%20local%20regions%0Aappearance%20that%20should%20not%20be%20edited.%20In%20particular%2C%20we%20explicitly%20introduce%0Alayout-preserving%20energy%20functions%20that%20are%20aimed%20to%20save%20local%20and%20global%0Astructures%20of%20the%20source%20image.%20Additionally%2C%20we%20propose%20a%20noise%20rescaling%0Amechanism%20that%20allows%20to%20preserve%20noise%20distribution%20by%20balancing%20the%20norms%20of%0Aclassifier-free%20guidance%20and%20our%20proposed%20guiders%20during%20generation.%20Such%20a%0Aguiding%20approach%20does%20not%20require%20fine-tuning%20the%20diffusion%20model%20and%20exact%0Ainversion%20process.%20As%20a%20result%2C%20the%20proposed%20method%20provides%20a%20fast%20and%0Ahigh-quality%20editing%20mechanism.%20In%20our%20experiments%2C%20we%20show%20through%20human%0Aevaluation%20and%20quantitative%20analysis%20that%20the%20proposed%20method%20allows%20to%20produce%0Adesired%20editing%20which%20is%20more%20preferable%20by%20humans%20and%20also%20achieves%20a%20better%0Atrade-off%20between%20editing%20quality%20and%20preservation%20of%20the%20original%20image.%20Our%0Acode%20is%20available%20at%20https%3A//github.com/MACderRu/Guide-and-Rescale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.01322v3&entry.124074799=Read"},
{"title": "Harnessing Diversity for Important Data Selection in Pretraining Large\n  Language Models", "author": "Chi Zhang and Huaping Zhong and Kuan Zhang and Chengliang Chai and Rui Wang and Xinlin Zhuang and Tianyi Bai and Jiantao Qiu and Lei Cao and Ye Yuan and Guoren Wang and Conghui He", "abstract": "  Data selection is of great significance in pre-training large language\nmodels, given the variation in quality within the large-scale available\ntraining corpora. To achieve this, researchers are currently investigating the\nuse of data influence to measure the importance of data instances, $i.e.,$ a\nhigh influence score indicates that incorporating this instance to the training\nset is likely to enhance the model performance. Consequently, they select the\ntop-$k$ instances with the highest scores. However, this approach has several\nlimitations. (1) Computing the influence of all available data is\ntime-consuming. (2) The selected data instances are not diverse enough, which\nmay hinder the pre-trained model's ability to generalize effectively to various\ndownstream tasks. In this paper, we introduce \\texttt{Quad}, a data selection\napproach that considers both quality and diversity by using data influence to\nachieve state-of-the-art pre-training results. In particular, noting that\nattention layers capture extensive semantic details, we have adapted the\naccelerated $iHVP$ computation methods for attention layers, enhancing our\nability to evaluate the influence of data, $i.e.,$ its quality. For the\ndiversity, \\texttt{Quad} clusters the dataset into similar data instances\nwithin each cluster and diverse instances across different clusters. For each\ncluster, if we opt to select data from it, we take some samples to evaluate the\ninfluence to prevent processing all instances. To determine which clusters to\nselect, we utilize the classic Multi-Armed Bandit method, treating each cluster\nas an arm. This approach favors clusters with highly influential instances\n(ensuring high quality) or clusters that have been selected less frequently\n(ensuring diversity), thereby well balancing between quality and diversity.\n", "link": "http://arxiv.org/abs/2409.16986v1", "date": "2024-09-25", "relevancy": 2.4836, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5029}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5005}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Diversity%20for%20Important%20Data%20Selection%20in%20Pretraining%20Large%0A%20%20Language%20Models&body=Title%3A%20Harnessing%20Diversity%20for%20Important%20Data%20Selection%20in%20Pretraining%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Chi%20Zhang%20and%20Huaping%20Zhong%20and%20Kuan%20Zhang%20and%20Chengliang%20Chai%20and%20Rui%20Wang%20and%20Xinlin%20Zhuang%20and%20Tianyi%20Bai%20and%20Jiantao%20Qiu%20and%20Lei%20Cao%20and%20Ye%20Yuan%20and%20Guoren%20Wang%20and%20Conghui%20He%0AAbstract%3A%20%20%20Data%20selection%20is%20of%20great%20significance%20in%20pre-training%20large%20language%0Amodels%2C%20given%20the%20variation%20in%20quality%20within%20the%20large-scale%20available%0Atraining%20corpora.%20To%20achieve%20this%2C%20researchers%20are%20currently%20investigating%20the%0Ause%20of%20data%20influence%20to%20measure%20the%20importance%20of%20data%20instances%2C%20%24i.e.%2C%24%20a%0Ahigh%20influence%20score%20indicates%20that%20incorporating%20this%20instance%20to%20the%20training%0Aset%20is%20likely%20to%20enhance%20the%20model%20performance.%20Consequently%2C%20they%20select%20the%0Atop-%24k%24%20instances%20with%20the%20highest%20scores.%20However%2C%20this%20approach%20has%20several%0Alimitations.%20%281%29%20Computing%20the%20influence%20of%20all%20available%20data%20is%0Atime-consuming.%20%282%29%20The%20selected%20data%20instances%20are%20not%20diverse%20enough%2C%20which%0Amay%20hinder%20the%20pre-trained%20model%27s%20ability%20to%20generalize%20effectively%20to%20various%0Adownstream%20tasks.%20In%20this%20paper%2C%20we%20introduce%20%5Ctexttt%7BQuad%7D%2C%20a%20data%20selection%0Aapproach%20that%20considers%20both%20quality%20and%20diversity%20by%20using%20data%20influence%20to%0Aachieve%20state-of-the-art%20pre-training%20results.%20In%20particular%2C%20noting%20that%0Aattention%20layers%20capture%20extensive%20semantic%20details%2C%20we%20have%20adapted%20the%0Aaccelerated%20%24iHVP%24%20computation%20methods%20for%20attention%20layers%2C%20enhancing%20our%0Aability%20to%20evaluate%20the%20influence%20of%20data%2C%20%24i.e.%2C%24%20its%20quality.%20For%20the%0Adiversity%2C%20%5Ctexttt%7BQuad%7D%20clusters%20the%20dataset%20into%20similar%20data%20instances%0Awithin%20each%20cluster%20and%20diverse%20instances%20across%20different%20clusters.%20For%20each%0Acluster%2C%20if%20we%20opt%20to%20select%20data%20from%20it%2C%20we%20take%20some%20samples%20to%20evaluate%20the%0Ainfluence%20to%20prevent%20processing%20all%20instances.%20To%20determine%20which%20clusters%20to%0Aselect%2C%20we%20utilize%20the%20classic%20Multi-Armed%20Bandit%20method%2C%20treating%20each%20cluster%0Aas%20an%20arm.%20This%20approach%20favors%20clusters%20with%20highly%20influential%20instances%0A%28ensuring%20high%20quality%29%20or%20clusters%20that%20have%20been%20selected%20less%20frequently%0A%28ensuring%20diversity%29%2C%20thereby%20well%20balancing%20between%20quality%20and%20diversity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16986v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Diversity%2520for%2520Important%2520Data%2520Selection%2520in%2520Pretraining%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DChi%2520Zhang%2520and%2520Huaping%2520Zhong%2520and%2520Kuan%2520Zhang%2520and%2520Chengliang%2520Chai%2520and%2520Rui%2520Wang%2520and%2520Xinlin%2520Zhuang%2520and%2520Tianyi%2520Bai%2520and%2520Jiantao%2520Qiu%2520and%2520Lei%2520Cao%2520and%2520Ye%2520Yuan%2520and%2520Guoren%2520Wang%2520and%2520Conghui%2520He%26entry.1292438233%3D%2520%2520Data%2520selection%2520is%2520of%2520great%2520significance%2520in%2520pre-training%2520large%2520language%250Amodels%252C%2520given%2520the%2520variation%2520in%2520quality%2520within%2520the%2520large-scale%2520available%250Atraining%2520corpora.%2520To%2520achieve%2520this%252C%2520researchers%2520are%2520currently%2520investigating%2520the%250Ause%2520of%2520data%2520influence%2520to%2520measure%2520the%2520importance%2520of%2520data%2520instances%252C%2520%2524i.e.%252C%2524%2520a%250Ahigh%2520influence%2520score%2520indicates%2520that%2520incorporating%2520this%2520instance%2520to%2520the%2520training%250Aset%2520is%2520likely%2520to%2520enhance%2520the%2520model%2520performance.%2520Consequently%252C%2520they%2520select%2520the%250Atop-%2524k%2524%2520instances%2520with%2520the%2520highest%2520scores.%2520However%252C%2520this%2520approach%2520has%2520several%250Alimitations.%2520%25281%2529%2520Computing%2520the%2520influence%2520of%2520all%2520available%2520data%2520is%250Atime-consuming.%2520%25282%2529%2520The%2520selected%2520data%2520instances%2520are%2520not%2520diverse%2520enough%252C%2520which%250Amay%2520hinder%2520the%2520pre-trained%2520model%2527s%2520ability%2520to%2520generalize%2520effectively%2520to%2520various%250Adownstream%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520%255Ctexttt%257BQuad%257D%252C%2520a%2520data%2520selection%250Aapproach%2520that%2520considers%2520both%2520quality%2520and%2520diversity%2520by%2520using%2520data%2520influence%2520to%250Aachieve%2520state-of-the-art%2520pre-training%2520results.%2520In%2520particular%252C%2520noting%2520that%250Aattention%2520layers%2520capture%2520extensive%2520semantic%2520details%252C%2520we%2520have%2520adapted%2520the%250Aaccelerated%2520%2524iHVP%2524%2520computation%2520methods%2520for%2520attention%2520layers%252C%2520enhancing%2520our%250Aability%2520to%2520evaluate%2520the%2520influence%2520of%2520data%252C%2520%2524i.e.%252C%2524%2520its%2520quality.%2520For%2520the%250Adiversity%252C%2520%255Ctexttt%257BQuad%257D%2520clusters%2520the%2520dataset%2520into%2520similar%2520data%2520instances%250Awithin%2520each%2520cluster%2520and%2520diverse%2520instances%2520across%2520different%2520clusters.%2520For%2520each%250Acluster%252C%2520if%2520we%2520opt%2520to%2520select%2520data%2520from%2520it%252C%2520we%2520take%2520some%2520samples%2520to%2520evaluate%2520the%250Ainfluence%2520to%2520prevent%2520processing%2520all%2520instances.%2520To%2520determine%2520which%2520clusters%2520to%250Aselect%252C%2520we%2520utilize%2520the%2520classic%2520Multi-Armed%2520Bandit%2520method%252C%2520treating%2520each%2520cluster%250Aas%2520an%2520arm.%2520This%2520approach%2520favors%2520clusters%2520with%2520highly%2520influential%2520instances%250A%2528ensuring%2520high%2520quality%2529%2520or%2520clusters%2520that%2520have%2520been%2520selected%2520less%2520frequently%250A%2528ensuring%2520diversity%2529%252C%2520thereby%2520well%2520balancing%2520between%2520quality%2520and%2520diversity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16986v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Diversity%20for%20Important%20Data%20Selection%20in%20Pretraining%20Large%0A%20%20Language%20Models&entry.906535625=Chi%20Zhang%20and%20Huaping%20Zhong%20and%20Kuan%20Zhang%20and%20Chengliang%20Chai%20and%20Rui%20Wang%20and%20Xinlin%20Zhuang%20and%20Tianyi%20Bai%20and%20Jiantao%20Qiu%20and%20Lei%20Cao%20and%20Ye%20Yuan%20and%20Guoren%20Wang%20and%20Conghui%20He&entry.1292438233=%20%20Data%20selection%20is%20of%20great%20significance%20in%20pre-training%20large%20language%0Amodels%2C%20given%20the%20variation%20in%20quality%20within%20the%20large-scale%20available%0Atraining%20corpora.%20To%20achieve%20this%2C%20researchers%20are%20currently%20investigating%20the%0Ause%20of%20data%20influence%20to%20measure%20the%20importance%20of%20data%20instances%2C%20%24i.e.%2C%24%20a%0Ahigh%20influence%20score%20indicates%20that%20incorporating%20this%20instance%20to%20the%20training%0Aset%20is%20likely%20to%20enhance%20the%20model%20performance.%20Consequently%2C%20they%20select%20the%0Atop-%24k%24%20instances%20with%20the%20highest%20scores.%20However%2C%20this%20approach%20has%20several%0Alimitations.%20%281%29%20Computing%20the%20influence%20of%20all%20available%20data%20is%0Atime-consuming.%20%282%29%20The%20selected%20data%20instances%20are%20not%20diverse%20enough%2C%20which%0Amay%20hinder%20the%20pre-trained%20model%27s%20ability%20to%20generalize%20effectively%20to%20various%0Adownstream%20tasks.%20In%20this%20paper%2C%20we%20introduce%20%5Ctexttt%7BQuad%7D%2C%20a%20data%20selection%0Aapproach%20that%20considers%20both%20quality%20and%20diversity%20by%20using%20data%20influence%20to%0Aachieve%20state-of-the-art%20pre-training%20results.%20In%20particular%2C%20noting%20that%0Aattention%20layers%20capture%20extensive%20semantic%20details%2C%20we%20have%20adapted%20the%0Aaccelerated%20%24iHVP%24%20computation%20methods%20for%20attention%20layers%2C%20enhancing%20our%0Aability%20to%20evaluate%20the%20influence%20of%20data%2C%20%24i.e.%2C%24%20its%20quality.%20For%20the%0Adiversity%2C%20%5Ctexttt%7BQuad%7D%20clusters%20the%20dataset%20into%20similar%20data%20instances%0Awithin%20each%20cluster%20and%20diverse%20instances%20across%20different%20clusters.%20For%20each%0Acluster%2C%20if%20we%20opt%20to%20select%20data%20from%20it%2C%20we%20take%20some%20samples%20to%20evaluate%20the%0Ainfluence%20to%20prevent%20processing%20all%20instances.%20To%20determine%20which%20clusters%20to%0Aselect%2C%20we%20utilize%20the%20classic%20Multi-Armed%20Bandit%20method%2C%20treating%20each%20cluster%0Aas%20an%20arm.%20This%20approach%20favors%20clusters%20with%20highly%20influential%20instances%0A%28ensuring%20high%20quality%29%20or%20clusters%20that%20have%20been%20selected%20less%20frequently%0A%28ensuring%20diversity%29%2C%20thereby%20well%20balancing%20between%20quality%20and%20diversity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16986v1&entry.124074799=Read"},
{"title": "SimTxtSeg: Weakly-Supervised Medical Image Segmentation with Simple Text\n  Cues", "author": "Yuxin Xie and Tao Zhou and Yi Zhou and Geng Chen", "abstract": "  Weakly-supervised medical image segmentation is a challenging task that aims\nto reduce the annotation cost while keep the segmentation performance. In this\npaper, we present a novel framework, SimTxtSeg, that leverages simple text cues\nto generate high-quality pseudo-labels and study the cross-modal fusion in\ntraining segmentation models, simultaneously. Our contribution consists of two\nkey components: an effective Textual-to-Visual Cue Converter that produces\nvisual prompts from text prompts on medical images, and a text-guided\nsegmentation model with Text-Vision Hybrid Attention that fuses text and image\nfeatures. We evaluate our framework on two medical image segmentation tasks:\ncolonic polyp segmentation and MRI brain tumor segmentation, and achieve\nconsistent state-of-the-art performance. Source code is available at:\nhttps://github.com/xyx1024/SimTxtSeg.\n", "link": "http://arxiv.org/abs/2406.19364v3", "date": "2024-09-25", "relevancy": 2.4758, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5024}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4915}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimTxtSeg%3A%20Weakly-Supervised%20Medical%20Image%20Segmentation%20with%20Simple%20Text%0A%20%20Cues&body=Title%3A%20SimTxtSeg%3A%20Weakly-Supervised%20Medical%20Image%20Segmentation%20with%20Simple%20Text%0A%20%20Cues%0AAuthor%3A%20Yuxin%20Xie%20and%20Tao%20Zhou%20and%20Yi%20Zhou%20and%20Geng%20Chen%0AAbstract%3A%20%20%20Weakly-supervised%20medical%20image%20segmentation%20is%20a%20challenging%20task%20that%20aims%0Ato%20reduce%20the%20annotation%20cost%20while%20keep%20the%20segmentation%20performance.%20In%20this%0Apaper%2C%20we%20present%20a%20novel%20framework%2C%20SimTxtSeg%2C%20that%20leverages%20simple%20text%20cues%0Ato%20generate%20high-quality%20pseudo-labels%20and%20study%20the%20cross-modal%20fusion%20in%0Atraining%20segmentation%20models%2C%20simultaneously.%20Our%20contribution%20consists%20of%20two%0Akey%20components%3A%20an%20effective%20Textual-to-Visual%20Cue%20Converter%20that%20produces%0Avisual%20prompts%20from%20text%20prompts%20on%20medical%20images%2C%20and%20a%20text-guided%0Asegmentation%20model%20with%20Text-Vision%20Hybrid%20Attention%20that%20fuses%20text%20and%20image%0Afeatures.%20We%20evaluate%20our%20framework%20on%20two%20medical%20image%20segmentation%20tasks%3A%0Acolonic%20polyp%20segmentation%20and%20MRI%20brain%20tumor%20segmentation%2C%20and%20achieve%0Aconsistent%20state-of-the-art%20performance.%20Source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/xyx1024/SimTxtSeg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19364v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimTxtSeg%253A%2520Weakly-Supervised%2520Medical%2520Image%2520Segmentation%2520with%2520Simple%2520Text%250A%2520%2520Cues%26entry.906535625%3DYuxin%2520Xie%2520and%2520Tao%2520Zhou%2520and%2520Yi%2520Zhou%2520and%2520Geng%2520Chen%26entry.1292438233%3D%2520%2520Weakly-supervised%2520medical%2520image%2520segmentation%2520is%2520a%2520challenging%2520task%2520that%2520aims%250Ato%2520reduce%2520the%2520annotation%2520cost%2520while%2520keep%2520the%2520segmentation%2520performance.%2520In%2520this%250Apaper%252C%2520we%2520present%2520a%2520novel%2520framework%252C%2520SimTxtSeg%252C%2520that%2520leverages%2520simple%2520text%2520cues%250Ato%2520generate%2520high-quality%2520pseudo-labels%2520and%2520study%2520the%2520cross-modal%2520fusion%2520in%250Atraining%2520segmentation%2520models%252C%2520simultaneously.%2520Our%2520contribution%2520consists%2520of%2520two%250Akey%2520components%253A%2520an%2520effective%2520Textual-to-Visual%2520Cue%2520Converter%2520that%2520produces%250Avisual%2520prompts%2520from%2520text%2520prompts%2520on%2520medical%2520images%252C%2520and%2520a%2520text-guided%250Asegmentation%2520model%2520with%2520Text-Vision%2520Hybrid%2520Attention%2520that%2520fuses%2520text%2520and%2520image%250Afeatures.%2520We%2520evaluate%2520our%2520framework%2520on%2520two%2520medical%2520image%2520segmentation%2520tasks%253A%250Acolonic%2520polyp%2520segmentation%2520and%2520MRI%2520brain%2520tumor%2520segmentation%252C%2520and%2520achieve%250Aconsistent%2520state-of-the-art%2520performance.%2520Source%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/xyx1024/SimTxtSeg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19364v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimTxtSeg%3A%20Weakly-Supervised%20Medical%20Image%20Segmentation%20with%20Simple%20Text%0A%20%20Cues&entry.906535625=Yuxin%20Xie%20and%20Tao%20Zhou%20and%20Yi%20Zhou%20and%20Geng%20Chen&entry.1292438233=%20%20Weakly-supervised%20medical%20image%20segmentation%20is%20a%20challenging%20task%20that%20aims%0Ato%20reduce%20the%20annotation%20cost%20while%20keep%20the%20segmentation%20performance.%20In%20this%0Apaper%2C%20we%20present%20a%20novel%20framework%2C%20SimTxtSeg%2C%20that%20leverages%20simple%20text%20cues%0Ato%20generate%20high-quality%20pseudo-labels%20and%20study%20the%20cross-modal%20fusion%20in%0Atraining%20segmentation%20models%2C%20simultaneously.%20Our%20contribution%20consists%20of%20two%0Akey%20components%3A%20an%20effective%20Textual-to-Visual%20Cue%20Converter%20that%20produces%0Avisual%20prompts%20from%20text%20prompts%20on%20medical%20images%2C%20and%20a%20text-guided%0Asegmentation%20model%20with%20Text-Vision%20Hybrid%20Attention%20that%20fuses%20text%20and%20image%0Afeatures.%20We%20evaluate%20our%20framework%20on%20two%20medical%20image%20segmentation%20tasks%3A%0Acolonic%20polyp%20segmentation%20and%20MRI%20brain%20tumor%20segmentation%2C%20and%20achieve%0Aconsistent%20state-of-the-art%20performance.%20Source%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/xyx1024/SimTxtSeg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19364v3&entry.124074799=Read"},
{"title": "Let's Make a Splan: Risk-Aware Trajectory Optimization in a Normalized\n  Gaussian Splat", "author": "Jonathan Michaux and Seth Isaacson and Challen Enninful Adu and Adam Li and Rahul Kashyap Swayampakula and Parker Ewen and Sean Rice and Katherine A. Skinner and Ram Vasudevan", "abstract": "  Neural Radiance Fields and Gaussian Splatting have transformed the field of\ncomputer vision by enabling photo-realistic representation of complex scenes.\nDespite this success, they have seen only limited use in real-world robotics\ntasks such as trajectory optimization. Two key factors have contributed to this\nlimited success. First, it is challenging to reason about collisions in\nradiance models. Second, it is difficult to perform inference of radiance\nmodels fast enough for real-time trajectory synthesis. This paper addresses\nthese challenges by proposing SPLANNING, a risk-aware trajectory optimizer that\noperates in a Gaussian Splatting model. This paper first derives a method for\nrigorously upper-bounding the probability of collision between a robot and a\nradiance field. Second, this paper introduces a normalized reformulation of\nGaussian Splatting that enables the efficient computation of the collision\nbound in a Gaussian Splat. Third, a method is presented to optimize\ntrajectories while avoiding collisions with a scene represented by a Gaussian\nSplat. Experiments demonstrate that SPLANNING outperforms state-of-the-art\nmethods in generating collision-free trajectories in highly cluttered\nenvironments. The proposed system is also tested on a real-world robot\nmanipulator. A project page is available at\nhttps://roahmlab.github.io/splanning.\n", "link": "http://arxiv.org/abs/2409.16915v1", "date": "2024-09-25", "relevancy": 2.4411, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.659}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.583}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5566}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Let%27s%20Make%20a%20Splan%3A%20Risk-Aware%20Trajectory%20Optimization%20in%20a%20Normalized%0A%20%20Gaussian%20Splat&body=Title%3A%20Let%27s%20Make%20a%20Splan%3A%20Risk-Aware%20Trajectory%20Optimization%20in%20a%20Normalized%0A%20%20Gaussian%20Splat%0AAuthor%3A%20Jonathan%20Michaux%20and%20Seth%20Isaacson%20and%20Challen%20Enninful%20Adu%20and%20Adam%20Li%20and%20Rahul%20Kashyap%20Swayampakula%20and%20Parker%20Ewen%20and%20Sean%20Rice%20and%20Katherine%20A.%20Skinner%20and%20Ram%20Vasudevan%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20and%20Gaussian%20Splatting%20have%20transformed%20the%20field%20of%0Acomputer%20vision%20by%20enabling%20photo-realistic%20representation%20of%20complex%20scenes.%0ADespite%20this%20success%2C%20they%20have%20seen%20only%20limited%20use%20in%20real-world%20robotics%0Atasks%20such%20as%20trajectory%20optimization.%20Two%20key%20factors%20have%20contributed%20to%20this%0Alimited%20success.%20First%2C%20it%20is%20challenging%20to%20reason%20about%20collisions%20in%0Aradiance%20models.%20Second%2C%20it%20is%20difficult%20to%20perform%20inference%20of%20radiance%0Amodels%20fast%20enough%20for%20real-time%20trajectory%20synthesis.%20This%20paper%20addresses%0Athese%20challenges%20by%20proposing%20SPLANNING%2C%20a%20risk-aware%20trajectory%20optimizer%20that%0Aoperates%20in%20a%20Gaussian%20Splatting%20model.%20This%20paper%20first%20derives%20a%20method%20for%0Arigorously%20upper-bounding%20the%20probability%20of%20collision%20between%20a%20robot%20and%20a%0Aradiance%20field.%20Second%2C%20this%20paper%20introduces%20a%20normalized%20reformulation%20of%0AGaussian%20Splatting%20that%20enables%20the%20efficient%20computation%20of%20the%20collision%0Abound%20in%20a%20Gaussian%20Splat.%20Third%2C%20a%20method%20is%20presented%20to%20optimize%0Atrajectories%20while%20avoiding%20collisions%20with%20a%20scene%20represented%20by%20a%20Gaussian%0ASplat.%20Experiments%20demonstrate%20that%20SPLANNING%20outperforms%20state-of-the-art%0Amethods%20in%20generating%20collision-free%20trajectories%20in%20highly%20cluttered%0Aenvironments.%20The%20proposed%20system%20is%20also%20tested%20on%20a%20real-world%20robot%0Amanipulator.%20A%20project%20page%20is%20available%20at%0Ahttps%3A//roahmlab.github.io/splanning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16915v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLet%2527s%2520Make%2520a%2520Splan%253A%2520Risk-Aware%2520Trajectory%2520Optimization%2520in%2520a%2520Normalized%250A%2520%2520Gaussian%2520Splat%26entry.906535625%3DJonathan%2520Michaux%2520and%2520Seth%2520Isaacson%2520and%2520Challen%2520Enninful%2520Adu%2520and%2520Adam%2520Li%2520and%2520Rahul%2520Kashyap%2520Swayampakula%2520and%2520Parker%2520Ewen%2520and%2520Sean%2520Rice%2520and%2520Katherine%2520A.%2520Skinner%2520and%2520Ram%2520Vasudevan%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520and%2520Gaussian%2520Splatting%2520have%2520transformed%2520the%2520field%2520of%250Acomputer%2520vision%2520by%2520enabling%2520photo-realistic%2520representation%2520of%2520complex%2520scenes.%250ADespite%2520this%2520success%252C%2520they%2520have%2520seen%2520only%2520limited%2520use%2520in%2520real-world%2520robotics%250Atasks%2520such%2520as%2520trajectory%2520optimization.%2520Two%2520key%2520factors%2520have%2520contributed%2520to%2520this%250Alimited%2520success.%2520First%252C%2520it%2520is%2520challenging%2520to%2520reason%2520about%2520collisions%2520in%250Aradiance%2520models.%2520Second%252C%2520it%2520is%2520difficult%2520to%2520perform%2520inference%2520of%2520radiance%250Amodels%2520fast%2520enough%2520for%2520real-time%2520trajectory%2520synthesis.%2520This%2520paper%2520addresses%250Athese%2520challenges%2520by%2520proposing%2520SPLANNING%252C%2520a%2520risk-aware%2520trajectory%2520optimizer%2520that%250Aoperates%2520in%2520a%2520Gaussian%2520Splatting%2520model.%2520This%2520paper%2520first%2520derives%2520a%2520method%2520for%250Arigorously%2520upper-bounding%2520the%2520probability%2520of%2520collision%2520between%2520a%2520robot%2520and%2520a%250Aradiance%2520field.%2520Second%252C%2520this%2520paper%2520introduces%2520a%2520normalized%2520reformulation%2520of%250AGaussian%2520Splatting%2520that%2520enables%2520the%2520efficient%2520computation%2520of%2520the%2520collision%250Abound%2520in%2520a%2520Gaussian%2520Splat.%2520Third%252C%2520a%2520method%2520is%2520presented%2520to%2520optimize%250Atrajectories%2520while%2520avoiding%2520collisions%2520with%2520a%2520scene%2520represented%2520by%2520a%2520Gaussian%250ASplat.%2520Experiments%2520demonstrate%2520that%2520SPLANNING%2520outperforms%2520state-of-the-art%250Amethods%2520in%2520generating%2520collision-free%2520trajectories%2520in%2520highly%2520cluttered%250Aenvironments.%2520The%2520proposed%2520system%2520is%2520also%2520tested%2520on%2520a%2520real-world%2520robot%250Amanipulator.%2520A%2520project%2520page%2520is%2520available%2520at%250Ahttps%253A//roahmlab.github.io/splanning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16915v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Let%27s%20Make%20a%20Splan%3A%20Risk-Aware%20Trajectory%20Optimization%20in%20a%20Normalized%0A%20%20Gaussian%20Splat&entry.906535625=Jonathan%20Michaux%20and%20Seth%20Isaacson%20and%20Challen%20Enninful%20Adu%20and%20Adam%20Li%20and%20Rahul%20Kashyap%20Swayampakula%20and%20Parker%20Ewen%20and%20Sean%20Rice%20and%20Katherine%20A.%20Skinner%20and%20Ram%20Vasudevan&entry.1292438233=%20%20Neural%20Radiance%20Fields%20and%20Gaussian%20Splatting%20have%20transformed%20the%20field%20of%0Acomputer%20vision%20by%20enabling%20photo-realistic%20representation%20of%20complex%20scenes.%0ADespite%20this%20success%2C%20they%20have%20seen%20only%20limited%20use%20in%20real-world%20robotics%0Atasks%20such%20as%20trajectory%20optimization.%20Two%20key%20factors%20have%20contributed%20to%20this%0Alimited%20success.%20First%2C%20it%20is%20challenging%20to%20reason%20about%20collisions%20in%0Aradiance%20models.%20Second%2C%20it%20is%20difficult%20to%20perform%20inference%20of%20radiance%0Amodels%20fast%20enough%20for%20real-time%20trajectory%20synthesis.%20This%20paper%20addresses%0Athese%20challenges%20by%20proposing%20SPLANNING%2C%20a%20risk-aware%20trajectory%20optimizer%20that%0Aoperates%20in%20a%20Gaussian%20Splatting%20model.%20This%20paper%20first%20derives%20a%20method%20for%0Arigorously%20upper-bounding%20the%20probability%20of%20collision%20between%20a%20robot%20and%20a%0Aradiance%20field.%20Second%2C%20this%20paper%20introduces%20a%20normalized%20reformulation%20of%0AGaussian%20Splatting%20that%20enables%20the%20efficient%20computation%20of%20the%20collision%0Abound%20in%20a%20Gaussian%20Splat.%20Third%2C%20a%20method%20is%20presented%20to%20optimize%0Atrajectories%20while%20avoiding%20collisions%20with%20a%20scene%20represented%20by%20a%20Gaussian%0ASplat.%20Experiments%20demonstrate%20that%20SPLANNING%20outperforms%20state-of-the-art%0Amethods%20in%20generating%20collision-free%20trajectories%20in%20highly%20cluttered%0Aenvironments.%20The%20proposed%20system%20is%20also%20tested%20on%20a%20real-world%20robot%0Amanipulator.%20A%20project%20page%20is%20available%20at%0Ahttps%3A//roahmlab.github.io/splanning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16915v1&entry.124074799=Read"},
{"title": "Non-backtracking Graph Neural Networks", "author": "Seonghyun Park and Narae Ryu and Gahee Kim and Dongyeop Woo and Se-Young Yun and Sungsoo Ahn", "abstract": "  The celebrated message-passing updates for graph neural networks allow\nrepresenting large-scale graphs with local and computationally tractable\nupdates. However, the updates suffer from backtracking, i.e., a message flowing\nthrough the same edge twice and revisiting the previously visited node. Since\nthe number of message flows increases exponentially with the number of updates,\nthe redundancy in local updates prevents the graph neural network from\naccurately recognizing a particular message flow relevant for downstream tasks.\nIn this work, we propose to resolve such a redundancy issue via the\nnon-backtracking graph neural network (NBA-GNN) that updates a message without\nincorporating the message from the previously visited node. We theoretically\ninvestigate how NBA-GNN alleviates the over-squashing of GNNs, and establish a\nconnection between NBA-GNN and the impressive performance of non-backtracking\nupdates for stochastic block model recovery. Furthermore, we empirically verify\nthe effectiveness of our NBA-GNN on the long-range graph benchmark and\ntransductive node classification problems.\n", "link": "http://arxiv.org/abs/2310.07430v2", "date": "2024-09-25", "relevancy": 2.4375, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5259}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4703}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-backtracking%20Graph%20Neural%20Networks&body=Title%3A%20Non-backtracking%20Graph%20Neural%20Networks%0AAuthor%3A%20Seonghyun%20Park%20and%20Narae%20Ryu%20and%20Gahee%20Kim%20and%20Dongyeop%20Woo%20and%20Se-Young%20Yun%20and%20Sungsoo%20Ahn%0AAbstract%3A%20%20%20The%20celebrated%20message-passing%20updates%20for%20graph%20neural%20networks%20allow%0Arepresenting%20large-scale%20graphs%20with%20local%20and%20computationally%20tractable%0Aupdates.%20However%2C%20the%20updates%20suffer%20from%20backtracking%2C%20i.e.%2C%20a%20message%20flowing%0Athrough%20the%20same%20edge%20twice%20and%20revisiting%20the%20previously%20visited%20node.%20Since%0Athe%20number%20of%20message%20flows%20increases%20exponentially%20with%20the%20number%20of%20updates%2C%0Athe%20redundancy%20in%20local%20updates%20prevents%20the%20graph%20neural%20network%20from%0Aaccurately%20recognizing%20a%20particular%20message%20flow%20relevant%20for%20downstream%20tasks.%0AIn%20this%20work%2C%20we%20propose%20to%20resolve%20such%20a%20redundancy%20issue%20via%20the%0Anon-backtracking%20graph%20neural%20network%20%28NBA-GNN%29%20that%20updates%20a%20message%20without%0Aincorporating%20the%20message%20from%20the%20previously%20visited%20node.%20We%20theoretically%0Ainvestigate%20how%20NBA-GNN%20alleviates%20the%20over-squashing%20of%20GNNs%2C%20and%20establish%20a%0Aconnection%20between%20NBA-GNN%20and%20the%20impressive%20performance%20of%20non-backtracking%0Aupdates%20for%20stochastic%20block%20model%20recovery.%20Furthermore%2C%20we%20empirically%20verify%0Athe%20effectiveness%20of%20our%20NBA-GNN%20on%20the%20long-range%20graph%20benchmark%20and%0Atransductive%20node%20classification%20problems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07430v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-backtracking%2520Graph%2520Neural%2520Networks%26entry.906535625%3DSeonghyun%2520Park%2520and%2520Narae%2520Ryu%2520and%2520Gahee%2520Kim%2520and%2520Dongyeop%2520Woo%2520and%2520Se-Young%2520Yun%2520and%2520Sungsoo%2520Ahn%26entry.1292438233%3D%2520%2520The%2520celebrated%2520message-passing%2520updates%2520for%2520graph%2520neural%2520networks%2520allow%250Arepresenting%2520large-scale%2520graphs%2520with%2520local%2520and%2520computationally%2520tractable%250Aupdates.%2520However%252C%2520the%2520updates%2520suffer%2520from%2520backtracking%252C%2520i.e.%252C%2520a%2520message%2520flowing%250Athrough%2520the%2520same%2520edge%2520twice%2520and%2520revisiting%2520the%2520previously%2520visited%2520node.%2520Since%250Athe%2520number%2520of%2520message%2520flows%2520increases%2520exponentially%2520with%2520the%2520number%2520of%2520updates%252C%250Athe%2520redundancy%2520in%2520local%2520updates%2520prevents%2520the%2520graph%2520neural%2520network%2520from%250Aaccurately%2520recognizing%2520a%2520particular%2520message%2520flow%2520relevant%2520for%2520downstream%2520tasks.%250AIn%2520this%2520work%252C%2520we%2520propose%2520to%2520resolve%2520such%2520a%2520redundancy%2520issue%2520via%2520the%250Anon-backtracking%2520graph%2520neural%2520network%2520%2528NBA-GNN%2529%2520that%2520updates%2520a%2520message%2520without%250Aincorporating%2520the%2520message%2520from%2520the%2520previously%2520visited%2520node.%2520We%2520theoretically%250Ainvestigate%2520how%2520NBA-GNN%2520alleviates%2520the%2520over-squashing%2520of%2520GNNs%252C%2520and%2520establish%2520a%250Aconnection%2520between%2520NBA-GNN%2520and%2520the%2520impressive%2520performance%2520of%2520non-backtracking%250Aupdates%2520for%2520stochastic%2520block%2520model%2520recovery.%2520Furthermore%252C%2520we%2520empirically%2520verify%250Athe%2520effectiveness%2520of%2520our%2520NBA-GNN%2520on%2520the%2520long-range%2520graph%2520benchmark%2520and%250Atransductive%2520node%2520classification%2520problems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.07430v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-backtracking%20Graph%20Neural%20Networks&entry.906535625=Seonghyun%20Park%20and%20Narae%20Ryu%20and%20Gahee%20Kim%20and%20Dongyeop%20Woo%20and%20Se-Young%20Yun%20and%20Sungsoo%20Ahn&entry.1292438233=%20%20The%20celebrated%20message-passing%20updates%20for%20graph%20neural%20networks%20allow%0Arepresenting%20large-scale%20graphs%20with%20local%20and%20computationally%20tractable%0Aupdates.%20However%2C%20the%20updates%20suffer%20from%20backtracking%2C%20i.e.%2C%20a%20message%20flowing%0Athrough%20the%20same%20edge%20twice%20and%20revisiting%20the%20previously%20visited%20node.%20Since%0Athe%20number%20of%20message%20flows%20increases%20exponentially%20with%20the%20number%20of%20updates%2C%0Athe%20redundancy%20in%20local%20updates%20prevents%20the%20graph%20neural%20network%20from%0Aaccurately%20recognizing%20a%20particular%20message%20flow%20relevant%20for%20downstream%20tasks.%0AIn%20this%20work%2C%20we%20propose%20to%20resolve%20such%20a%20redundancy%20issue%20via%20the%0Anon-backtracking%20graph%20neural%20network%20%28NBA-GNN%29%20that%20updates%20a%20message%20without%0Aincorporating%20the%20message%20from%20the%20previously%20visited%20node.%20We%20theoretically%0Ainvestigate%20how%20NBA-GNN%20alleviates%20the%20over-squashing%20of%20GNNs%2C%20and%20establish%20a%0Aconnection%20between%20NBA-GNN%20and%20the%20impressive%20performance%20of%20non-backtracking%0Aupdates%20for%20stochastic%20block%20model%20recovery.%20Furthermore%2C%20we%20empirically%20verify%0Athe%20effectiveness%20of%20our%20NBA-GNN%20on%20the%20long-range%20graph%20benchmark%20and%0Atransductive%20node%20classification%20problems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07430v2&entry.124074799=Read"},
{"title": "NTIRE 2024 Challenge on Stereo Image Super-Resolution: Methods and\n  Results", "author": "Longguang Wang and Yulan Guo and Juncheng Li and Hongda Liu and Yang Zhao and Yingqian Wang and Zhi Jin and Shuhang Gu and Radu Timofte", "abstract": "  This paper summarizes the 3rd NTIRE challenge on stereo image\nsuper-resolution (SR) with a focus on new solutions and results. The task of\nthis challenge is to super-resolve a low-resolution stereo image pair to a\nhigh-resolution one with a magnification factor of x4 under a limited\ncomputational budget. Compared with single image SR, the major challenge of\nthis challenge lies in how to exploit additional information in another\nviewpoint and how to maintain stereo consistency in the results. This challenge\nhas 2 tracks, including one track on bicubic degradation and one track on real\ndegradations. In total, 108 and 70 participants were successfully registered\nfor each track, respectively. In the test phase, 14 and 13 teams successfully\nsubmitted valid results with PSNR (RGB) scores better than the baseline. This\nchallenge establishes a new benchmark for stereo image SR.\n", "link": "http://arxiv.org/abs/2409.16947v1", "date": "2024-09-25", "relevancy": 2.3768, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5076}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4664}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4521}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NTIRE%202024%20Challenge%20on%20Stereo%20Image%20Super-Resolution%3A%20Methods%20and%0A%20%20Results&body=Title%3A%20NTIRE%202024%20Challenge%20on%20Stereo%20Image%20Super-Resolution%3A%20Methods%20and%0A%20%20Results%0AAuthor%3A%20Longguang%20Wang%20and%20Yulan%20Guo%20and%20Juncheng%20Li%20and%20Hongda%20Liu%20and%20Yang%20Zhao%20and%20Yingqian%20Wang%20and%20Zhi%20Jin%20and%20Shuhang%20Gu%20and%20Radu%20Timofte%0AAbstract%3A%20%20%20This%20paper%20summarizes%20the%203rd%20NTIRE%20challenge%20on%20stereo%20image%0Asuper-resolution%20%28SR%29%20with%20a%20focus%20on%20new%20solutions%20and%20results.%20The%20task%20of%0Athis%20challenge%20is%20to%20super-resolve%20a%20low-resolution%20stereo%20image%20pair%20to%20a%0Ahigh-resolution%20one%20with%20a%20magnification%20factor%20of%20x4%20under%20a%20limited%0Acomputational%20budget.%20Compared%20with%20single%20image%20SR%2C%20the%20major%20challenge%20of%0Athis%20challenge%20lies%20in%20how%20to%20exploit%20additional%20information%20in%20another%0Aviewpoint%20and%20how%20to%20maintain%20stereo%20consistency%20in%20the%20results.%20This%20challenge%0Ahas%202%20tracks%2C%20including%20one%20track%20on%20bicubic%20degradation%20and%20one%20track%20on%20real%0Adegradations.%20In%20total%2C%20108%20and%2070%20participants%20were%20successfully%20registered%0Afor%20each%20track%2C%20respectively.%20In%20the%20test%20phase%2C%2014%20and%2013%20teams%20successfully%0Asubmitted%20valid%20results%20with%20PSNR%20%28RGB%29%20scores%20better%20than%20the%20baseline.%20This%0Achallenge%20establishes%20a%20new%20benchmark%20for%20stereo%20image%20SR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16947v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNTIRE%25202024%2520Challenge%2520on%2520Stereo%2520Image%2520Super-Resolution%253A%2520Methods%2520and%250A%2520%2520Results%26entry.906535625%3DLongguang%2520Wang%2520and%2520Yulan%2520Guo%2520and%2520Juncheng%2520Li%2520and%2520Hongda%2520Liu%2520and%2520Yang%2520Zhao%2520and%2520Yingqian%2520Wang%2520and%2520Zhi%2520Jin%2520and%2520Shuhang%2520Gu%2520and%2520Radu%2520Timofte%26entry.1292438233%3D%2520%2520This%2520paper%2520summarizes%2520the%25203rd%2520NTIRE%2520challenge%2520on%2520stereo%2520image%250Asuper-resolution%2520%2528SR%2529%2520with%2520a%2520focus%2520on%2520new%2520solutions%2520and%2520results.%2520The%2520task%2520of%250Athis%2520challenge%2520is%2520to%2520super-resolve%2520a%2520low-resolution%2520stereo%2520image%2520pair%2520to%2520a%250Ahigh-resolution%2520one%2520with%2520a%2520magnification%2520factor%2520of%2520x4%2520under%2520a%2520limited%250Acomputational%2520budget.%2520Compared%2520with%2520single%2520image%2520SR%252C%2520the%2520major%2520challenge%2520of%250Athis%2520challenge%2520lies%2520in%2520how%2520to%2520exploit%2520additional%2520information%2520in%2520another%250Aviewpoint%2520and%2520how%2520to%2520maintain%2520stereo%2520consistency%2520in%2520the%2520results.%2520This%2520challenge%250Ahas%25202%2520tracks%252C%2520including%2520one%2520track%2520on%2520bicubic%2520degradation%2520and%2520one%2520track%2520on%2520real%250Adegradations.%2520In%2520total%252C%2520108%2520and%252070%2520participants%2520were%2520successfully%2520registered%250Afor%2520each%2520track%252C%2520respectively.%2520In%2520the%2520test%2520phase%252C%252014%2520and%252013%2520teams%2520successfully%250Asubmitted%2520valid%2520results%2520with%2520PSNR%2520%2528RGB%2529%2520scores%2520better%2520than%2520the%2520baseline.%2520This%250Achallenge%2520establishes%2520a%2520new%2520benchmark%2520for%2520stereo%2520image%2520SR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16947v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NTIRE%202024%20Challenge%20on%20Stereo%20Image%20Super-Resolution%3A%20Methods%20and%0A%20%20Results&entry.906535625=Longguang%20Wang%20and%20Yulan%20Guo%20and%20Juncheng%20Li%20and%20Hongda%20Liu%20and%20Yang%20Zhao%20and%20Yingqian%20Wang%20and%20Zhi%20Jin%20and%20Shuhang%20Gu%20and%20Radu%20Timofte&entry.1292438233=%20%20This%20paper%20summarizes%20the%203rd%20NTIRE%20challenge%20on%20stereo%20image%0Asuper-resolution%20%28SR%29%20with%20a%20focus%20on%20new%20solutions%20and%20results.%20The%20task%20of%0Athis%20challenge%20is%20to%20super-resolve%20a%20low-resolution%20stereo%20image%20pair%20to%20a%0Ahigh-resolution%20one%20with%20a%20magnification%20factor%20of%20x4%20under%20a%20limited%0Acomputational%20budget.%20Compared%20with%20single%20image%20SR%2C%20the%20major%20challenge%20of%0Athis%20challenge%20lies%20in%20how%20to%20exploit%20additional%20information%20in%20another%0Aviewpoint%20and%20how%20to%20maintain%20stereo%20consistency%20in%20the%20results.%20This%20challenge%0Ahas%202%20tracks%2C%20including%20one%20track%20on%20bicubic%20degradation%20and%20one%20track%20on%20real%0Adegradations.%20In%20total%2C%20108%20and%2070%20participants%20were%20successfully%20registered%0Afor%20each%20track%2C%20respectively.%20In%20the%20test%20phase%2C%2014%20and%2013%20teams%20successfully%0Asubmitted%20valid%20results%20with%20PSNR%20%28RGB%29%20scores%20better%20than%20the%20baseline.%20This%0Achallenge%20establishes%20a%20new%20benchmark%20for%20stereo%20image%20SR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16947v1&entry.124074799=Read"},
{"title": "RoboSense: Large-scale Dataset and Benchmark for Multi-sensor Low-speed\n  Autonomous Driving", "author": "Haisheng Su and Feixiang Song and Cong Ma and Wei Wu and Junchi Yan", "abstract": "  Robust object detection and tracking under arbitrary sight of view is\nchallenging yet essential for the development of Autonomous Vehicle technology.\nWith the growing demand of unmanned function vehicles, near-field scene\nunderstanding becomes an important research topic in the areas of low-speed\nautonomous driving. Due to the complexity of driving conditions and diversity\nof near obstacles such as blind spots and high occlusion, the perception\ncapability of near-field environment is still inferior than its farther\ncounterpart. To further enhance the intelligent ability of unmanned vehicles,\nin this paper, we construct a multimodal data collection platform based on 3\nmain types of sensors (Camera, LiDAR and Fisheye), which supports flexible\nsensor configurations to enable dynamic sight of view for ego vehicle, either\nglobal view or local view. Meanwhile, a large-scale multi-sensor dataset is\nbuilt, named RoboSense, to facilitate near-field scene understanding. RoboSense\ncontains more than 133K synchronized data with 1.4M 3D bounding box and IDs\nannotated in the full $360^{\\circ}$ view, forming 216K trajectories across 7.6K\ntemporal sequences. It has $270\\times$ and $18\\times$ as many annotations of\nnear-field obstacles within 5$m$ as the previous single-vehicle datasets such\nas KITTI and nuScenes. Moreover, we define a novel matching criterion for\nnear-field 3D perception and prediction metrics. Based on RoboSense, we\nformulate 6 popular tasks to facilitate the future development of related\nresearch, where the detailed data analysis as well as benchmarks are also\nprovided accordingly. Code and dataset will be available at\nhttps://github.com/suhaisheng/RoboSense.\n", "link": "http://arxiv.org/abs/2408.15503v3", "date": "2024-09-25", "relevancy": 2.3723, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.608}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5982}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoboSense%3A%20Large-scale%20Dataset%20and%20Benchmark%20for%20Multi-sensor%20Low-speed%0A%20%20Autonomous%20Driving&body=Title%3A%20RoboSense%3A%20Large-scale%20Dataset%20and%20Benchmark%20for%20Multi-sensor%20Low-speed%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Haisheng%20Su%20and%20Feixiang%20Song%20and%20Cong%20Ma%20and%20Wei%20Wu%20and%20Junchi%20Yan%0AAbstract%3A%20%20%20Robust%20object%20detection%20and%20tracking%20under%20arbitrary%20sight%20of%20view%20is%0Achallenging%20yet%20essential%20for%20the%20development%20of%20Autonomous%20Vehicle%20technology.%0AWith%20the%20growing%20demand%20of%20unmanned%20function%20vehicles%2C%20near-field%20scene%0Aunderstanding%20becomes%20an%20important%20research%20topic%20in%20the%20areas%20of%20low-speed%0Aautonomous%20driving.%20Due%20to%20the%20complexity%20of%20driving%20conditions%20and%20diversity%0Aof%20near%20obstacles%20such%20as%20blind%20spots%20and%20high%20occlusion%2C%20the%20perception%0Acapability%20of%20near-field%20environment%20is%20still%20inferior%20than%20its%20farther%0Acounterpart.%20To%20further%20enhance%20the%20intelligent%20ability%20of%20unmanned%20vehicles%2C%0Ain%20this%20paper%2C%20we%20construct%20a%20multimodal%20data%20collection%20platform%20based%20on%203%0Amain%20types%20of%20sensors%20%28Camera%2C%20LiDAR%20and%20Fisheye%29%2C%20which%20supports%20flexible%0Asensor%20configurations%20to%20enable%20dynamic%20sight%20of%20view%20for%20ego%20vehicle%2C%20either%0Aglobal%20view%20or%20local%20view.%20Meanwhile%2C%20a%20large-scale%20multi-sensor%20dataset%20is%0Abuilt%2C%20named%20RoboSense%2C%20to%20facilitate%20near-field%20scene%20understanding.%20RoboSense%0Acontains%20more%20than%20133K%20synchronized%20data%20with%201.4M%203D%20bounding%20box%20and%20IDs%0Aannotated%20in%20the%20full%20%24360%5E%7B%5Ccirc%7D%24%20view%2C%20forming%20216K%20trajectories%20across%207.6K%0Atemporal%20sequences.%20It%20has%20%24270%5Ctimes%24%20and%20%2418%5Ctimes%24%20as%20many%20annotations%20of%0Anear-field%20obstacles%20within%205%24m%24%20as%20the%20previous%20single-vehicle%20datasets%20such%0Aas%20KITTI%20and%20nuScenes.%20Moreover%2C%20we%20define%20a%20novel%20matching%20criterion%20for%0Anear-field%203D%20perception%20and%20prediction%20metrics.%20Based%20on%20RoboSense%2C%20we%0Aformulate%206%20popular%20tasks%20to%20facilitate%20the%20future%20development%20of%20related%0Aresearch%2C%20where%20the%20detailed%20data%20analysis%20as%20well%20as%20benchmarks%20are%20also%0Aprovided%20accordingly.%20Code%20and%20dataset%20will%20be%20available%20at%0Ahttps%3A//github.com/suhaisheng/RoboSense.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15503v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoboSense%253A%2520Large-scale%2520Dataset%2520and%2520Benchmark%2520for%2520Multi-sensor%2520Low-speed%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DHaisheng%2520Su%2520and%2520Feixiang%2520Song%2520and%2520Cong%2520Ma%2520and%2520Wei%2520Wu%2520and%2520Junchi%2520Yan%26entry.1292438233%3D%2520%2520Robust%2520object%2520detection%2520and%2520tracking%2520under%2520arbitrary%2520sight%2520of%2520view%2520is%250Achallenging%2520yet%2520essential%2520for%2520the%2520development%2520of%2520Autonomous%2520Vehicle%2520technology.%250AWith%2520the%2520growing%2520demand%2520of%2520unmanned%2520function%2520vehicles%252C%2520near-field%2520scene%250Aunderstanding%2520becomes%2520an%2520important%2520research%2520topic%2520in%2520the%2520areas%2520of%2520low-speed%250Aautonomous%2520driving.%2520Due%2520to%2520the%2520complexity%2520of%2520driving%2520conditions%2520and%2520diversity%250Aof%2520near%2520obstacles%2520such%2520as%2520blind%2520spots%2520and%2520high%2520occlusion%252C%2520the%2520perception%250Acapability%2520of%2520near-field%2520environment%2520is%2520still%2520inferior%2520than%2520its%2520farther%250Acounterpart.%2520To%2520further%2520enhance%2520the%2520intelligent%2520ability%2520of%2520unmanned%2520vehicles%252C%250Ain%2520this%2520paper%252C%2520we%2520construct%2520a%2520multimodal%2520data%2520collection%2520platform%2520based%2520on%25203%250Amain%2520types%2520of%2520sensors%2520%2528Camera%252C%2520LiDAR%2520and%2520Fisheye%2529%252C%2520which%2520supports%2520flexible%250Asensor%2520configurations%2520to%2520enable%2520dynamic%2520sight%2520of%2520view%2520for%2520ego%2520vehicle%252C%2520either%250Aglobal%2520view%2520or%2520local%2520view.%2520Meanwhile%252C%2520a%2520large-scale%2520multi-sensor%2520dataset%2520is%250Abuilt%252C%2520named%2520RoboSense%252C%2520to%2520facilitate%2520near-field%2520scene%2520understanding.%2520RoboSense%250Acontains%2520more%2520than%2520133K%2520synchronized%2520data%2520with%25201.4M%25203D%2520bounding%2520box%2520and%2520IDs%250Aannotated%2520in%2520the%2520full%2520%2524360%255E%257B%255Ccirc%257D%2524%2520view%252C%2520forming%2520216K%2520trajectories%2520across%25207.6K%250Atemporal%2520sequences.%2520It%2520has%2520%2524270%255Ctimes%2524%2520and%2520%252418%255Ctimes%2524%2520as%2520many%2520annotations%2520of%250Anear-field%2520obstacles%2520within%25205%2524m%2524%2520as%2520the%2520previous%2520single-vehicle%2520datasets%2520such%250Aas%2520KITTI%2520and%2520nuScenes.%2520Moreover%252C%2520we%2520define%2520a%2520novel%2520matching%2520criterion%2520for%250Anear-field%25203D%2520perception%2520and%2520prediction%2520metrics.%2520Based%2520on%2520RoboSense%252C%2520we%250Aformulate%25206%2520popular%2520tasks%2520to%2520facilitate%2520the%2520future%2520development%2520of%2520related%250Aresearch%252C%2520where%2520the%2520detailed%2520data%2520analysis%2520as%2520well%2520as%2520benchmarks%2520are%2520also%250Aprovided%2520accordingly.%2520Code%2520and%2520dataset%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/suhaisheng/RoboSense.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15503v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboSense%3A%20Large-scale%20Dataset%20and%20Benchmark%20for%20Multi-sensor%20Low-speed%0A%20%20Autonomous%20Driving&entry.906535625=Haisheng%20Su%20and%20Feixiang%20Song%20and%20Cong%20Ma%20and%20Wei%20Wu%20and%20Junchi%20Yan&entry.1292438233=%20%20Robust%20object%20detection%20and%20tracking%20under%20arbitrary%20sight%20of%20view%20is%0Achallenging%20yet%20essential%20for%20the%20development%20of%20Autonomous%20Vehicle%20technology.%0AWith%20the%20growing%20demand%20of%20unmanned%20function%20vehicles%2C%20near-field%20scene%0Aunderstanding%20becomes%20an%20important%20research%20topic%20in%20the%20areas%20of%20low-speed%0Aautonomous%20driving.%20Due%20to%20the%20complexity%20of%20driving%20conditions%20and%20diversity%0Aof%20near%20obstacles%20such%20as%20blind%20spots%20and%20high%20occlusion%2C%20the%20perception%0Acapability%20of%20near-field%20environment%20is%20still%20inferior%20than%20its%20farther%0Acounterpart.%20To%20further%20enhance%20the%20intelligent%20ability%20of%20unmanned%20vehicles%2C%0Ain%20this%20paper%2C%20we%20construct%20a%20multimodal%20data%20collection%20platform%20based%20on%203%0Amain%20types%20of%20sensors%20%28Camera%2C%20LiDAR%20and%20Fisheye%29%2C%20which%20supports%20flexible%0Asensor%20configurations%20to%20enable%20dynamic%20sight%20of%20view%20for%20ego%20vehicle%2C%20either%0Aglobal%20view%20or%20local%20view.%20Meanwhile%2C%20a%20large-scale%20multi-sensor%20dataset%20is%0Abuilt%2C%20named%20RoboSense%2C%20to%20facilitate%20near-field%20scene%20understanding.%20RoboSense%0Acontains%20more%20than%20133K%20synchronized%20data%20with%201.4M%203D%20bounding%20box%20and%20IDs%0Aannotated%20in%20the%20full%20%24360%5E%7B%5Ccirc%7D%24%20view%2C%20forming%20216K%20trajectories%20across%207.6K%0Atemporal%20sequences.%20It%20has%20%24270%5Ctimes%24%20and%20%2418%5Ctimes%24%20as%20many%20annotations%20of%0Anear-field%20obstacles%20within%205%24m%24%20as%20the%20previous%20single-vehicle%20datasets%20such%0Aas%20KITTI%20and%20nuScenes.%20Moreover%2C%20we%20define%20a%20novel%20matching%20criterion%20for%0Anear-field%203D%20perception%20and%20prediction%20metrics.%20Based%20on%20RoboSense%2C%20we%0Aformulate%206%20popular%20tasks%20to%20facilitate%20the%20future%20development%20of%20related%0Aresearch%2C%20where%20the%20detailed%20data%20analysis%20as%20well%20as%20benchmarks%20are%20also%0Aprovided%20accordingly.%20Code%20and%20dataset%20will%20be%20available%20at%0Ahttps%3A//github.com/suhaisheng/RoboSense.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15503v3&entry.124074799=Read"},
{"title": "Face Forgery Detection with Elaborate Backbone", "author": "Zonghui Guo and Yingjie Liu and Jie Zhang and Haiyong Zheng and Shiguang Shan", "abstract": "  Face Forgery Detection (FFD), or Deepfake detection, aims to determine\nwhether a digital face is real or fake. Due to different face synthesis\nalgorithms with diverse forgery patterns, FFD models often overfit specific\npatterns in training datasets, resulting in poor generalization to other unseen\nforgeries. This severe challenge requires FFD models to possess strong\ncapabilities in representing complex facial features and extracting subtle\nforgery cues. Although previous FFD models directly employ existing backbones\nto represent and extract facial forgery cues, the critical role of backbones is\noften overlooked, particularly as their knowledge and capabilities are\ninsufficient to address FFD challenges, inevitably limiting generalization.\nTherefore, it is essential to integrate the backbone pre-training\nconfigurations and seek practical solutions by revisiting the complete FFD\nworkflow, from backbone pre-training and fine-tuning to inference of\ndiscriminant results. Specifically, we analyze the crucial contributions of\nbackbones with different configurations in FFD task and propose leveraging the\nViT network with self-supervised learning on real-face datasets to pre-train a\nbackbone, equipping it with superior facial representation capabilities. We\nthen build a competitive backbone fine-tuning framework that strengthens the\nbackbone's ability to extract diverse forgery cues within a competitive\nlearning mechanism. Moreover, we devise a threshold optimization mechanism that\nutilizes prediction confidence to improve the inference reliability.\nComprehensive experiments demonstrate that our FFD model with the elaborate\nbackbone achieves excellent performance in FFD and extra face-related tasks,\ni.e., presentation attack detection. Code and models are available at\nhttps://github.com/zhenglab/FFDBackbone.\n", "link": "http://arxiv.org/abs/2409.16945v1", "date": "2024-09-25", "relevancy": 2.3713, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4772}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4772}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Face%20Forgery%20Detection%20with%20Elaborate%20Backbone&body=Title%3A%20Face%20Forgery%20Detection%20with%20Elaborate%20Backbone%0AAuthor%3A%20Zonghui%20Guo%20and%20Yingjie%20Liu%20and%20Jie%20Zhang%20and%20Haiyong%20Zheng%20and%20Shiguang%20Shan%0AAbstract%3A%20%20%20Face%20Forgery%20Detection%20%28FFD%29%2C%20or%20Deepfake%20detection%2C%20aims%20to%20determine%0Awhether%20a%20digital%20face%20is%20real%20or%20fake.%20Due%20to%20different%20face%20synthesis%0Aalgorithms%20with%20diverse%20forgery%20patterns%2C%20FFD%20models%20often%20overfit%20specific%0Apatterns%20in%20training%20datasets%2C%20resulting%20in%20poor%20generalization%20to%20other%20unseen%0Aforgeries.%20This%20severe%20challenge%20requires%20FFD%20models%20to%20possess%20strong%0Acapabilities%20in%20representing%20complex%20facial%20features%20and%20extracting%20subtle%0Aforgery%20cues.%20Although%20previous%20FFD%20models%20directly%20employ%20existing%20backbones%0Ato%20represent%20and%20extract%20facial%20forgery%20cues%2C%20the%20critical%20role%20of%20backbones%20is%0Aoften%20overlooked%2C%20particularly%20as%20their%20knowledge%20and%20capabilities%20are%0Ainsufficient%20to%20address%20FFD%20challenges%2C%20inevitably%20limiting%20generalization.%0ATherefore%2C%20it%20is%20essential%20to%20integrate%20the%20backbone%20pre-training%0Aconfigurations%20and%20seek%20practical%20solutions%20by%20revisiting%20the%20complete%20FFD%0Aworkflow%2C%20from%20backbone%20pre-training%20and%20fine-tuning%20to%20inference%20of%0Adiscriminant%20results.%20Specifically%2C%20we%20analyze%20the%20crucial%20contributions%20of%0Abackbones%20with%20different%20configurations%20in%20FFD%20task%20and%20propose%20leveraging%20the%0AViT%20network%20with%20self-supervised%20learning%20on%20real-face%20datasets%20to%20pre-train%20a%0Abackbone%2C%20equipping%20it%20with%20superior%20facial%20representation%20capabilities.%20We%0Athen%20build%20a%20competitive%20backbone%20fine-tuning%20framework%20that%20strengthens%20the%0Abackbone%27s%20ability%20to%20extract%20diverse%20forgery%20cues%20within%20a%20competitive%0Alearning%20mechanism.%20Moreover%2C%20we%20devise%20a%20threshold%20optimization%20mechanism%20that%0Autilizes%20prediction%20confidence%20to%20improve%20the%20inference%20reliability.%0AComprehensive%20experiments%20demonstrate%20that%20our%20FFD%20model%20with%20the%20elaborate%0Abackbone%20achieves%20excellent%20performance%20in%20FFD%20and%20extra%20face-related%20tasks%2C%0Ai.e.%2C%20presentation%20attack%20detection.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/zhenglab/FFDBackbone.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16945v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFace%2520Forgery%2520Detection%2520with%2520Elaborate%2520Backbone%26entry.906535625%3DZonghui%2520Guo%2520and%2520Yingjie%2520Liu%2520and%2520Jie%2520Zhang%2520and%2520Haiyong%2520Zheng%2520and%2520Shiguang%2520Shan%26entry.1292438233%3D%2520%2520Face%2520Forgery%2520Detection%2520%2528FFD%2529%252C%2520or%2520Deepfake%2520detection%252C%2520aims%2520to%2520determine%250Awhether%2520a%2520digital%2520face%2520is%2520real%2520or%2520fake.%2520Due%2520to%2520different%2520face%2520synthesis%250Aalgorithms%2520with%2520diverse%2520forgery%2520patterns%252C%2520FFD%2520models%2520often%2520overfit%2520specific%250Apatterns%2520in%2520training%2520datasets%252C%2520resulting%2520in%2520poor%2520generalization%2520to%2520other%2520unseen%250Aforgeries.%2520This%2520severe%2520challenge%2520requires%2520FFD%2520models%2520to%2520possess%2520strong%250Acapabilities%2520in%2520representing%2520complex%2520facial%2520features%2520and%2520extracting%2520subtle%250Aforgery%2520cues.%2520Although%2520previous%2520FFD%2520models%2520directly%2520employ%2520existing%2520backbones%250Ato%2520represent%2520and%2520extract%2520facial%2520forgery%2520cues%252C%2520the%2520critical%2520role%2520of%2520backbones%2520is%250Aoften%2520overlooked%252C%2520particularly%2520as%2520their%2520knowledge%2520and%2520capabilities%2520are%250Ainsufficient%2520to%2520address%2520FFD%2520challenges%252C%2520inevitably%2520limiting%2520generalization.%250ATherefore%252C%2520it%2520is%2520essential%2520to%2520integrate%2520the%2520backbone%2520pre-training%250Aconfigurations%2520and%2520seek%2520practical%2520solutions%2520by%2520revisiting%2520the%2520complete%2520FFD%250Aworkflow%252C%2520from%2520backbone%2520pre-training%2520and%2520fine-tuning%2520to%2520inference%2520of%250Adiscriminant%2520results.%2520Specifically%252C%2520we%2520analyze%2520the%2520crucial%2520contributions%2520of%250Abackbones%2520with%2520different%2520configurations%2520in%2520FFD%2520task%2520and%2520propose%2520leveraging%2520the%250AViT%2520network%2520with%2520self-supervised%2520learning%2520on%2520real-face%2520datasets%2520to%2520pre-train%2520a%250Abackbone%252C%2520equipping%2520it%2520with%2520superior%2520facial%2520representation%2520capabilities.%2520We%250Athen%2520build%2520a%2520competitive%2520backbone%2520fine-tuning%2520framework%2520that%2520strengthens%2520the%250Abackbone%2527s%2520ability%2520to%2520extract%2520diverse%2520forgery%2520cues%2520within%2520a%2520competitive%250Alearning%2520mechanism.%2520Moreover%252C%2520we%2520devise%2520a%2520threshold%2520optimization%2520mechanism%2520that%250Autilizes%2520prediction%2520confidence%2520to%2520improve%2520the%2520inference%2520reliability.%250AComprehensive%2520experiments%2520demonstrate%2520that%2520our%2520FFD%2520model%2520with%2520the%2520elaborate%250Abackbone%2520achieves%2520excellent%2520performance%2520in%2520FFD%2520and%2520extra%2520face-related%2520tasks%252C%250Ai.e.%252C%2520presentation%2520attack%2520detection.%2520Code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/zhenglab/FFDBackbone.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16945v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Face%20Forgery%20Detection%20with%20Elaborate%20Backbone&entry.906535625=Zonghui%20Guo%20and%20Yingjie%20Liu%20and%20Jie%20Zhang%20and%20Haiyong%20Zheng%20and%20Shiguang%20Shan&entry.1292438233=%20%20Face%20Forgery%20Detection%20%28FFD%29%2C%20or%20Deepfake%20detection%2C%20aims%20to%20determine%0Awhether%20a%20digital%20face%20is%20real%20or%20fake.%20Due%20to%20different%20face%20synthesis%0Aalgorithms%20with%20diverse%20forgery%20patterns%2C%20FFD%20models%20often%20overfit%20specific%0Apatterns%20in%20training%20datasets%2C%20resulting%20in%20poor%20generalization%20to%20other%20unseen%0Aforgeries.%20This%20severe%20challenge%20requires%20FFD%20models%20to%20possess%20strong%0Acapabilities%20in%20representing%20complex%20facial%20features%20and%20extracting%20subtle%0Aforgery%20cues.%20Although%20previous%20FFD%20models%20directly%20employ%20existing%20backbones%0Ato%20represent%20and%20extract%20facial%20forgery%20cues%2C%20the%20critical%20role%20of%20backbones%20is%0Aoften%20overlooked%2C%20particularly%20as%20their%20knowledge%20and%20capabilities%20are%0Ainsufficient%20to%20address%20FFD%20challenges%2C%20inevitably%20limiting%20generalization.%0ATherefore%2C%20it%20is%20essential%20to%20integrate%20the%20backbone%20pre-training%0Aconfigurations%20and%20seek%20practical%20solutions%20by%20revisiting%20the%20complete%20FFD%0Aworkflow%2C%20from%20backbone%20pre-training%20and%20fine-tuning%20to%20inference%20of%0Adiscriminant%20results.%20Specifically%2C%20we%20analyze%20the%20crucial%20contributions%20of%0Abackbones%20with%20different%20configurations%20in%20FFD%20task%20and%20propose%20leveraging%20the%0AViT%20network%20with%20self-supervised%20learning%20on%20real-face%20datasets%20to%20pre-train%20a%0Abackbone%2C%20equipping%20it%20with%20superior%20facial%20representation%20capabilities.%20We%0Athen%20build%20a%20competitive%20backbone%20fine-tuning%20framework%20that%20strengthens%20the%0Abackbone%27s%20ability%20to%20extract%20diverse%20forgery%20cues%20within%20a%20competitive%0Alearning%20mechanism.%20Moreover%2C%20we%20devise%20a%20threshold%20optimization%20mechanism%20that%0Autilizes%20prediction%20confidence%20to%20improve%20the%20inference%20reliability.%0AComprehensive%20experiments%20demonstrate%20that%20our%20FFD%20model%20with%20the%20elaborate%0Abackbone%20achieves%20excellent%20performance%20in%20FFD%20and%20extra%20face-related%20tasks%2C%0Ai.e.%2C%20presentation%20attack%20detection.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/zhenglab/FFDBackbone.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16945v1&entry.124074799=Read"},
{"title": "Unveiling Ontological Commitment in Multi-Modal Foundation Models", "author": "Mert Keser and Gesina Schwalbe and Niki Amini-Naieni and Matthias Rottmann and Alois Knoll", "abstract": "  Ontological commitment, i.e., used concepts, relations, and assumptions, are\na corner stone of qualitative reasoning (QR) models. The state-of-the-art for\nprocessing raw inputs, though, are deep neural networks (DNNs), nowadays often\nbased off from multimodal foundation models. These automatically learn rich\nrepresentations of concepts and respective reasoning. Unfortunately, the\nlearned qualitative knowledge is opaque, preventing easy inspection,\nvalidation, or adaptation against available QR models. So far, it is possible\nto associate pre-defined concepts with latent representations of DNNs, but\nextractable relations are mostly limited to semantic similarity. As a next step\ntowards QR for validation and verification of DNNs: Concretely, we propose a\nmethod that extracts the learned superclass hierarchy from a multimodal DNN for\na given set of leaf concepts. Under the hood we (1) obtain leaf concept\nembeddings using the DNN's textual input modality; (2) apply hierarchical\nclustering to them, using that DNNs encode semantic similarities via vector\ndistances; and (3) label the such-obtained parent concepts using search in\navailable ontologies from QR. An initial evaluation study shows that meaningful\nontological class hierarchies can be extracted from state-of-the-art foundation\nmodels. Furthermore, we demonstrate how to validate and verify a DNN's learned\nrepresentations against given ontologies. Lastly, we discuss potential future\napplications in the context of QR.\n", "link": "http://arxiv.org/abs/2409.17109v1", "date": "2024-09-25", "relevancy": 2.3639, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.597}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.597}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20Ontological%20Commitment%20in%20Multi-Modal%20Foundation%20Models&body=Title%3A%20Unveiling%20Ontological%20Commitment%20in%20Multi-Modal%20Foundation%20Models%0AAuthor%3A%20Mert%20Keser%20and%20Gesina%20Schwalbe%20and%20Niki%20Amini-Naieni%20and%20Matthias%20Rottmann%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20Ontological%20commitment%2C%20i.e.%2C%20used%20concepts%2C%20relations%2C%20and%20assumptions%2C%20are%0Aa%20corner%20stone%20of%20qualitative%20reasoning%20%28QR%29%20models.%20The%20state-of-the-art%20for%0Aprocessing%20raw%20inputs%2C%20though%2C%20are%20deep%20neural%20networks%20%28DNNs%29%2C%20nowadays%20often%0Abased%20off%20from%20multimodal%20foundation%20models.%20These%20automatically%20learn%20rich%0Arepresentations%20of%20concepts%20and%20respective%20reasoning.%20Unfortunately%2C%20the%0Alearned%20qualitative%20knowledge%20is%20opaque%2C%20preventing%20easy%20inspection%2C%0Avalidation%2C%20or%20adaptation%20against%20available%20QR%20models.%20So%20far%2C%20it%20is%20possible%0Ato%20associate%20pre-defined%20concepts%20with%20latent%20representations%20of%20DNNs%2C%20but%0Aextractable%20relations%20are%20mostly%20limited%20to%20semantic%20similarity.%20As%20a%20next%20step%0Atowards%20QR%20for%20validation%20and%20verification%20of%20DNNs%3A%20Concretely%2C%20we%20propose%20a%0Amethod%20that%20extracts%20the%20learned%20superclass%20hierarchy%20from%20a%20multimodal%20DNN%20for%0Aa%20given%20set%20of%20leaf%20concepts.%20Under%20the%20hood%20we%20%281%29%20obtain%20leaf%20concept%0Aembeddings%20using%20the%20DNN%27s%20textual%20input%20modality%3B%20%282%29%20apply%20hierarchical%0Aclustering%20to%20them%2C%20using%20that%20DNNs%20encode%20semantic%20similarities%20via%20vector%0Adistances%3B%20and%20%283%29%20label%20the%20such-obtained%20parent%20concepts%20using%20search%20in%0Aavailable%20ontologies%20from%20QR.%20An%20initial%20evaluation%20study%20shows%20that%20meaningful%0Aontological%20class%20hierarchies%20can%20be%20extracted%20from%20state-of-the-art%20foundation%0Amodels.%20Furthermore%2C%20we%20demonstrate%20how%20to%20validate%20and%20verify%20a%20DNN%27s%20learned%0Arepresentations%20against%20given%20ontologies.%20Lastly%2C%20we%20discuss%20potential%20future%0Aapplications%20in%20the%20context%20of%20QR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17109v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520Ontological%2520Commitment%2520in%2520Multi-Modal%2520Foundation%2520Models%26entry.906535625%3DMert%2520Keser%2520and%2520Gesina%2520Schwalbe%2520and%2520Niki%2520Amini-Naieni%2520and%2520Matthias%2520Rottmann%2520and%2520Alois%2520Knoll%26entry.1292438233%3D%2520%2520Ontological%2520commitment%252C%2520i.e.%252C%2520used%2520concepts%252C%2520relations%252C%2520and%2520assumptions%252C%2520are%250Aa%2520corner%2520stone%2520of%2520qualitative%2520reasoning%2520%2528QR%2529%2520models.%2520The%2520state-of-the-art%2520for%250Aprocessing%2520raw%2520inputs%252C%2520though%252C%2520are%2520deep%2520neural%2520networks%2520%2528DNNs%2529%252C%2520nowadays%2520often%250Abased%2520off%2520from%2520multimodal%2520foundation%2520models.%2520These%2520automatically%2520learn%2520rich%250Arepresentations%2520of%2520concepts%2520and%2520respective%2520reasoning.%2520Unfortunately%252C%2520the%250Alearned%2520qualitative%2520knowledge%2520is%2520opaque%252C%2520preventing%2520easy%2520inspection%252C%250Avalidation%252C%2520or%2520adaptation%2520against%2520available%2520QR%2520models.%2520So%2520far%252C%2520it%2520is%2520possible%250Ato%2520associate%2520pre-defined%2520concepts%2520with%2520latent%2520representations%2520of%2520DNNs%252C%2520but%250Aextractable%2520relations%2520are%2520mostly%2520limited%2520to%2520semantic%2520similarity.%2520As%2520a%2520next%2520step%250Atowards%2520QR%2520for%2520validation%2520and%2520verification%2520of%2520DNNs%253A%2520Concretely%252C%2520we%2520propose%2520a%250Amethod%2520that%2520extracts%2520the%2520learned%2520superclass%2520hierarchy%2520from%2520a%2520multimodal%2520DNN%2520for%250Aa%2520given%2520set%2520of%2520leaf%2520concepts.%2520Under%2520the%2520hood%2520we%2520%25281%2529%2520obtain%2520leaf%2520concept%250Aembeddings%2520using%2520the%2520DNN%2527s%2520textual%2520input%2520modality%253B%2520%25282%2529%2520apply%2520hierarchical%250Aclustering%2520to%2520them%252C%2520using%2520that%2520DNNs%2520encode%2520semantic%2520similarities%2520via%2520vector%250Adistances%253B%2520and%2520%25283%2529%2520label%2520the%2520such-obtained%2520parent%2520concepts%2520using%2520search%2520in%250Aavailable%2520ontologies%2520from%2520QR.%2520An%2520initial%2520evaluation%2520study%2520shows%2520that%2520meaningful%250Aontological%2520class%2520hierarchies%2520can%2520be%2520extracted%2520from%2520state-of-the-art%2520foundation%250Amodels.%2520Furthermore%252C%2520we%2520demonstrate%2520how%2520to%2520validate%2520and%2520verify%2520a%2520DNN%2527s%2520learned%250Arepresentations%2520against%2520given%2520ontologies.%2520Lastly%252C%2520we%2520discuss%2520potential%2520future%250Aapplications%2520in%2520the%2520context%2520of%2520QR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17109v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20Ontological%20Commitment%20in%20Multi-Modal%20Foundation%20Models&entry.906535625=Mert%20Keser%20and%20Gesina%20Schwalbe%20and%20Niki%20Amini-Naieni%20and%20Matthias%20Rottmann%20and%20Alois%20Knoll&entry.1292438233=%20%20Ontological%20commitment%2C%20i.e.%2C%20used%20concepts%2C%20relations%2C%20and%20assumptions%2C%20are%0Aa%20corner%20stone%20of%20qualitative%20reasoning%20%28QR%29%20models.%20The%20state-of-the-art%20for%0Aprocessing%20raw%20inputs%2C%20though%2C%20are%20deep%20neural%20networks%20%28DNNs%29%2C%20nowadays%20often%0Abased%20off%20from%20multimodal%20foundation%20models.%20These%20automatically%20learn%20rich%0Arepresentations%20of%20concepts%20and%20respective%20reasoning.%20Unfortunately%2C%20the%0Alearned%20qualitative%20knowledge%20is%20opaque%2C%20preventing%20easy%20inspection%2C%0Avalidation%2C%20or%20adaptation%20against%20available%20QR%20models.%20So%20far%2C%20it%20is%20possible%0Ato%20associate%20pre-defined%20concepts%20with%20latent%20representations%20of%20DNNs%2C%20but%0Aextractable%20relations%20are%20mostly%20limited%20to%20semantic%20similarity.%20As%20a%20next%20step%0Atowards%20QR%20for%20validation%20and%20verification%20of%20DNNs%3A%20Concretely%2C%20we%20propose%20a%0Amethod%20that%20extracts%20the%20learned%20superclass%20hierarchy%20from%20a%20multimodal%20DNN%20for%0Aa%20given%20set%20of%20leaf%20concepts.%20Under%20the%20hood%20we%20%281%29%20obtain%20leaf%20concept%0Aembeddings%20using%20the%20DNN%27s%20textual%20input%20modality%3B%20%282%29%20apply%20hierarchical%0Aclustering%20to%20them%2C%20using%20that%20DNNs%20encode%20semantic%20similarities%20via%20vector%0Adistances%3B%20and%20%283%29%20label%20the%20such-obtained%20parent%20concepts%20using%20search%20in%0Aavailable%20ontologies%20from%20QR.%20An%20initial%20evaluation%20study%20shows%20that%20meaningful%0Aontological%20class%20hierarchies%20can%20be%20extracted%20from%20state-of-the-art%20foundation%0Amodels.%20Furthermore%2C%20we%20demonstrate%20how%20to%20validate%20and%20verify%20a%20DNN%27s%20learned%0Arepresentations%20against%20given%20ontologies.%20Lastly%2C%20we%20discuss%20potential%20future%0Aapplications%20in%20the%20context%20of%20QR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17109v1&entry.124074799=Read"},
{"title": "A Versatile and Differentiable Hand-Object Interaction Representation", "author": "Th\u00e9o Morales and Omid Taheri and Gerard Lacey", "abstract": "  Synthesizing accurate hands-object interactions (HOI) is critical for\napplications in Computer Vision, Augmented Reality (AR), and Mixed Reality\n(MR). Despite recent advances, the accuracy of reconstructed or generated HOI\nleaves room for refinement. Some techniques have improved the accuracy of dense\ncorrespondences by shifting focus from generating explicit contacts to using\nrich HOI fields. Still, they lack full differentiability or continuity and are\ntailored to specific tasks. In contrast, we present a Coarse Hand-Object\nInteraction Representation (CHOIR), a novel, versatile and fully differentiable\nfield for HOI modelling. CHOIR leverages discrete unsigned distances for\ncontinuous shape and pose encoding, alongside multivariate Gaussian\ndistributions to represent dense contact maps with few parameters. To\ndemonstrate the versatility of CHOIR we design JointDiffusion, a diffusion\nmodel to learn a grasp distribution conditioned on noisy hand-object\ninteractions or only object geometries, for both refinement and synthesis\napplications. We demonstrate JointDiffusion's improvements over the SOTA in\nboth applications: it increases the contact F1 score by $5\\%$ for refinement\nand decreases the sim. displacement by $46\\%$ for synthesis. Our experiments\nshow that JointDiffusion with CHOIR yield superior contact accuracy and\nphysical realism compared to SOTA methods designed for specific tasks. Our\nmodels and code will be publicly available to the research community.\n", "link": "http://arxiv.org/abs/2409.16855v1", "date": "2024-09-25", "relevancy": 2.3399, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6066}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5784}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Versatile%20and%20Differentiable%20Hand-Object%20Interaction%20Representation&body=Title%3A%20A%20Versatile%20and%20Differentiable%20Hand-Object%20Interaction%20Representation%0AAuthor%3A%20Th%C3%A9o%20Morales%20and%20Omid%20Taheri%20and%20Gerard%20Lacey%0AAbstract%3A%20%20%20Synthesizing%20accurate%20hands-object%20interactions%20%28HOI%29%20is%20critical%20for%0Aapplications%20in%20Computer%20Vision%2C%20Augmented%20Reality%20%28AR%29%2C%20and%20Mixed%20Reality%0A%28MR%29.%20Despite%20recent%20advances%2C%20the%20accuracy%20of%20reconstructed%20or%20generated%20HOI%0Aleaves%20room%20for%20refinement.%20Some%20techniques%20have%20improved%20the%20accuracy%20of%20dense%0Acorrespondences%20by%20shifting%20focus%20from%20generating%20explicit%20contacts%20to%20using%0Arich%20HOI%20fields.%20Still%2C%20they%20lack%20full%20differentiability%20or%20continuity%20and%20are%0Atailored%20to%20specific%20tasks.%20In%20contrast%2C%20we%20present%20a%20Coarse%20Hand-Object%0AInteraction%20Representation%20%28CHOIR%29%2C%20a%20novel%2C%20versatile%20and%20fully%20differentiable%0Afield%20for%20HOI%20modelling.%20CHOIR%20leverages%20discrete%20unsigned%20distances%20for%0Acontinuous%20shape%20and%20pose%20encoding%2C%20alongside%20multivariate%20Gaussian%0Adistributions%20to%20represent%20dense%20contact%20maps%20with%20few%20parameters.%20To%0Ademonstrate%20the%20versatility%20of%20CHOIR%20we%20design%20JointDiffusion%2C%20a%20diffusion%0Amodel%20to%20learn%20a%20grasp%20distribution%20conditioned%20on%20noisy%20hand-object%0Ainteractions%20or%20only%20object%20geometries%2C%20for%20both%20refinement%20and%20synthesis%0Aapplications.%20We%20demonstrate%20JointDiffusion%27s%20improvements%20over%20the%20SOTA%20in%0Aboth%20applications%3A%20it%20increases%20the%20contact%20F1%20score%20by%20%245%5C%25%24%20for%20refinement%0Aand%20decreases%20the%20sim.%20displacement%20by%20%2446%5C%25%24%20for%20synthesis.%20Our%20experiments%0Ashow%20that%20JointDiffusion%20with%20CHOIR%20yield%20superior%20contact%20accuracy%20and%0Aphysical%20realism%20compared%20to%20SOTA%20methods%20designed%20for%20specific%20tasks.%20Our%0Amodels%20and%20code%20will%20be%20publicly%20available%20to%20the%20research%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Versatile%2520and%2520Differentiable%2520Hand-Object%2520Interaction%2520Representation%26entry.906535625%3DTh%25C3%25A9o%2520Morales%2520and%2520Omid%2520Taheri%2520and%2520Gerard%2520Lacey%26entry.1292438233%3D%2520%2520Synthesizing%2520accurate%2520hands-object%2520interactions%2520%2528HOI%2529%2520is%2520critical%2520for%250Aapplications%2520in%2520Computer%2520Vision%252C%2520Augmented%2520Reality%2520%2528AR%2529%252C%2520and%2520Mixed%2520Reality%250A%2528MR%2529.%2520Despite%2520recent%2520advances%252C%2520the%2520accuracy%2520of%2520reconstructed%2520or%2520generated%2520HOI%250Aleaves%2520room%2520for%2520refinement.%2520Some%2520techniques%2520have%2520improved%2520the%2520accuracy%2520of%2520dense%250Acorrespondences%2520by%2520shifting%2520focus%2520from%2520generating%2520explicit%2520contacts%2520to%2520using%250Arich%2520HOI%2520fields.%2520Still%252C%2520they%2520lack%2520full%2520differentiability%2520or%2520continuity%2520and%2520are%250Atailored%2520to%2520specific%2520tasks.%2520In%2520contrast%252C%2520we%2520present%2520a%2520Coarse%2520Hand-Object%250AInteraction%2520Representation%2520%2528CHOIR%2529%252C%2520a%2520novel%252C%2520versatile%2520and%2520fully%2520differentiable%250Afield%2520for%2520HOI%2520modelling.%2520CHOIR%2520leverages%2520discrete%2520unsigned%2520distances%2520for%250Acontinuous%2520shape%2520and%2520pose%2520encoding%252C%2520alongside%2520multivariate%2520Gaussian%250Adistributions%2520to%2520represent%2520dense%2520contact%2520maps%2520with%2520few%2520parameters.%2520To%250Ademonstrate%2520the%2520versatility%2520of%2520CHOIR%2520we%2520design%2520JointDiffusion%252C%2520a%2520diffusion%250Amodel%2520to%2520learn%2520a%2520grasp%2520distribution%2520conditioned%2520on%2520noisy%2520hand-object%250Ainteractions%2520or%2520only%2520object%2520geometries%252C%2520for%2520both%2520refinement%2520and%2520synthesis%250Aapplications.%2520We%2520demonstrate%2520JointDiffusion%2527s%2520improvements%2520over%2520the%2520SOTA%2520in%250Aboth%2520applications%253A%2520it%2520increases%2520the%2520contact%2520F1%2520score%2520by%2520%25245%255C%2525%2524%2520for%2520refinement%250Aand%2520decreases%2520the%2520sim.%2520displacement%2520by%2520%252446%255C%2525%2524%2520for%2520synthesis.%2520Our%2520experiments%250Ashow%2520that%2520JointDiffusion%2520with%2520CHOIR%2520yield%2520superior%2520contact%2520accuracy%2520and%250Aphysical%2520realism%2520compared%2520to%2520SOTA%2520methods%2520designed%2520for%2520specific%2520tasks.%2520Our%250Amodels%2520and%2520code%2520will%2520be%2520publicly%2520available%2520to%2520the%2520research%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Versatile%20and%20Differentiable%20Hand-Object%20Interaction%20Representation&entry.906535625=Th%C3%A9o%20Morales%20and%20Omid%20Taheri%20and%20Gerard%20Lacey&entry.1292438233=%20%20Synthesizing%20accurate%20hands-object%20interactions%20%28HOI%29%20is%20critical%20for%0Aapplications%20in%20Computer%20Vision%2C%20Augmented%20Reality%20%28AR%29%2C%20and%20Mixed%20Reality%0A%28MR%29.%20Despite%20recent%20advances%2C%20the%20accuracy%20of%20reconstructed%20or%20generated%20HOI%0Aleaves%20room%20for%20refinement.%20Some%20techniques%20have%20improved%20the%20accuracy%20of%20dense%0Acorrespondences%20by%20shifting%20focus%20from%20generating%20explicit%20contacts%20to%20using%0Arich%20HOI%20fields.%20Still%2C%20they%20lack%20full%20differentiability%20or%20continuity%20and%20are%0Atailored%20to%20specific%20tasks.%20In%20contrast%2C%20we%20present%20a%20Coarse%20Hand-Object%0AInteraction%20Representation%20%28CHOIR%29%2C%20a%20novel%2C%20versatile%20and%20fully%20differentiable%0Afield%20for%20HOI%20modelling.%20CHOIR%20leverages%20discrete%20unsigned%20distances%20for%0Acontinuous%20shape%20and%20pose%20encoding%2C%20alongside%20multivariate%20Gaussian%0Adistributions%20to%20represent%20dense%20contact%20maps%20with%20few%20parameters.%20To%0Ademonstrate%20the%20versatility%20of%20CHOIR%20we%20design%20JointDiffusion%2C%20a%20diffusion%0Amodel%20to%20learn%20a%20grasp%20distribution%20conditioned%20on%20noisy%20hand-object%0Ainteractions%20or%20only%20object%20geometries%2C%20for%20both%20refinement%20and%20synthesis%0Aapplications.%20We%20demonstrate%20JointDiffusion%27s%20improvements%20over%20the%20SOTA%20in%0Aboth%20applications%3A%20it%20increases%20the%20contact%20F1%20score%20by%20%245%5C%25%24%20for%20refinement%0Aand%20decreases%20the%20sim.%20displacement%20by%20%2446%5C%25%24%20for%20synthesis.%20Our%20experiments%0Ashow%20that%20JointDiffusion%20with%20CHOIR%20yield%20superior%20contact%20accuracy%20and%0Aphysical%20realism%20compared%20to%20SOTA%20methods%20designed%20for%20specific%20tasks.%20Our%0Amodels%20and%20code%20will%20be%20publicly%20available%20to%20the%20research%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16855v1&entry.124074799=Read"},
{"title": "Inline Photometrically Calibrated Hybrid Visual SLAM", "author": "Nicolas Abboud and Malak Sayour and Imad H. Elhajj and John Zelek and Daniel Asmar", "abstract": "  This paper presents an integrated approach to Visual SLAM, merging online\nsequential photometric calibration within a Hybrid direct-indirect visual SLAM\n(H-SLAM). Photometric calibration helps normalize pixel intensity values under\ndifferent lighting conditions, and thereby improves the direct component of our\nH-SLAM. A tangential benefit also results to the indirect component of H-SLAM\ngiven that the detected features are more stable across variable lighting\nconditions. Our proposed photometrically calibrated H-SLAM is tested on several\ndatasets, including the TUM monoVO as well as on a dataset we created.\nCalibrated H-SLAM outperforms other state of the art direct, indirect, and\nhybrid Visual SLAM systems in all the experiments. Furthermore, in online SLAM\ntested at our site, it also significantly outperformed the other SLAM Systems.\n", "link": "http://arxiv.org/abs/2409.16810v1", "date": "2024-09-25", "relevancy": 2.3175, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5994}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5715}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inline%20Photometrically%20Calibrated%20Hybrid%20Visual%20SLAM&body=Title%3A%20Inline%20Photometrically%20Calibrated%20Hybrid%20Visual%20SLAM%0AAuthor%3A%20Nicolas%20Abboud%20and%20Malak%20Sayour%20and%20Imad%20H.%20Elhajj%20and%20John%20Zelek%20and%20Daniel%20Asmar%0AAbstract%3A%20%20%20This%20paper%20presents%20an%20integrated%20approach%20to%20Visual%20SLAM%2C%20merging%20online%0Asequential%20photometric%20calibration%20within%20a%20Hybrid%20direct-indirect%20visual%20SLAM%0A%28H-SLAM%29.%20Photometric%20calibration%20helps%20normalize%20pixel%20intensity%20values%20under%0Adifferent%20lighting%20conditions%2C%20and%20thereby%20improves%20the%20direct%20component%20of%20our%0AH-SLAM.%20A%20tangential%20benefit%20also%20results%20to%20the%20indirect%20component%20of%20H-SLAM%0Agiven%20that%20the%20detected%20features%20are%20more%20stable%20across%20variable%20lighting%0Aconditions.%20Our%20proposed%20photometrically%20calibrated%20H-SLAM%20is%20tested%20on%20several%0Adatasets%2C%20including%20the%20TUM%20monoVO%20as%20well%20as%20on%20a%20dataset%20we%20created.%0ACalibrated%20H-SLAM%20outperforms%20other%20state%20of%20the%20art%20direct%2C%20indirect%2C%20and%0Ahybrid%20Visual%20SLAM%20systems%20in%20all%20the%20experiments.%20Furthermore%2C%20in%20online%20SLAM%0Atested%20at%20our%20site%2C%20it%20also%20significantly%20outperformed%20the%20other%20SLAM%20Systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInline%2520Photometrically%2520Calibrated%2520Hybrid%2520Visual%2520SLAM%26entry.906535625%3DNicolas%2520Abboud%2520and%2520Malak%2520Sayour%2520and%2520Imad%2520H.%2520Elhajj%2520and%2520John%2520Zelek%2520and%2520Daniel%2520Asmar%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520an%2520integrated%2520approach%2520to%2520Visual%2520SLAM%252C%2520merging%2520online%250Asequential%2520photometric%2520calibration%2520within%2520a%2520Hybrid%2520direct-indirect%2520visual%2520SLAM%250A%2528H-SLAM%2529.%2520Photometric%2520calibration%2520helps%2520normalize%2520pixel%2520intensity%2520values%2520under%250Adifferent%2520lighting%2520conditions%252C%2520and%2520thereby%2520improves%2520the%2520direct%2520component%2520of%2520our%250AH-SLAM.%2520A%2520tangential%2520benefit%2520also%2520results%2520to%2520the%2520indirect%2520component%2520of%2520H-SLAM%250Agiven%2520that%2520the%2520detected%2520features%2520are%2520more%2520stable%2520across%2520variable%2520lighting%250Aconditions.%2520Our%2520proposed%2520photometrically%2520calibrated%2520H-SLAM%2520is%2520tested%2520on%2520several%250Adatasets%252C%2520including%2520the%2520TUM%2520monoVO%2520as%2520well%2520as%2520on%2520a%2520dataset%2520we%2520created.%250ACalibrated%2520H-SLAM%2520outperforms%2520other%2520state%2520of%2520the%2520art%2520direct%252C%2520indirect%252C%2520and%250Ahybrid%2520Visual%2520SLAM%2520systems%2520in%2520all%2520the%2520experiments.%2520Furthermore%252C%2520in%2520online%2520SLAM%250Atested%2520at%2520our%2520site%252C%2520it%2520also%2520significantly%2520outperformed%2520the%2520other%2520SLAM%2520Systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inline%20Photometrically%20Calibrated%20Hybrid%20Visual%20SLAM&entry.906535625=Nicolas%20Abboud%20and%20Malak%20Sayour%20and%20Imad%20H.%20Elhajj%20and%20John%20Zelek%20and%20Daniel%20Asmar&entry.1292438233=%20%20This%20paper%20presents%20an%20integrated%20approach%20to%20Visual%20SLAM%2C%20merging%20online%0Asequential%20photometric%20calibration%20within%20a%20Hybrid%20direct-indirect%20visual%20SLAM%0A%28H-SLAM%29.%20Photometric%20calibration%20helps%20normalize%20pixel%20intensity%20values%20under%0Adifferent%20lighting%20conditions%2C%20and%20thereby%20improves%20the%20direct%20component%20of%20our%0AH-SLAM.%20A%20tangential%20benefit%20also%20results%20to%20the%20indirect%20component%20of%20H-SLAM%0Agiven%20that%20the%20detected%20features%20are%20more%20stable%20across%20variable%20lighting%0Aconditions.%20Our%20proposed%20photometrically%20calibrated%20H-SLAM%20is%20tested%20on%20several%0Adatasets%2C%20including%20the%20TUM%20monoVO%20as%20well%20as%20on%20a%20dataset%20we%20created.%0ACalibrated%20H-SLAM%20outperforms%20other%20state%20of%20the%20art%20direct%2C%20indirect%2C%20and%0Ahybrid%20Visual%20SLAM%20systems%20in%20all%20the%20experiments.%20Furthermore%2C%20in%20online%20SLAM%0Atested%20at%20our%20site%2C%20it%20also%20significantly%20outperformed%20the%20other%20SLAM%20Systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16810v1&entry.124074799=Read"},
{"title": "Ctrl-GenAug: Controllable Generative Augmentation for Medical Sequence\n  Classification", "author": "Xinrui Zhou and Yuhao Huang and Haoran Dou and Shijing Chen and Ao Chang and Jia Liu and Weiran Long and Jian Zheng and Erjiao Xu and Jie Ren and Ruobing Huang and Jun Cheng and Wufeng Xue and Dong Ni", "abstract": "  In the medical field, the limited availability of large-scale datasets and\nlabor-intensive annotation processes hinder the performance of deep models.\nDiffusion-based generative augmentation approaches present a promising solution\nto this issue, having been proven effective in advancing downstream medical\nrecognition tasks. Nevertheless, existing works lack sufficient semantic and\nsequential steerability for challenging video/3D sequence generation, and\nneglect quality control of noisy synthesized samples, resulting in unreliable\nsynthetic databases and severely limiting the performance of downstream tasks.\nIn this work, we present Ctrl-GenAug, a novel and general generative\naugmentation framework that enables highly semantic- and sequential-customized\nsequence synthesis and suppresses incorrectly synthesized samples, to aid\nmedical sequence classification. Specifically, we first design a multimodal\nconditions-guided sequence generator for controllably synthesizing\ndiagnosis-promotive samples. A sequential augmentation module is integrated to\nenhance the temporal/stereoscopic coherence of generated samples. Then, we\npropose a noisy synthetic data filter to suppress unreliable cases at semantic\nand sequential levels. Extensive experiments on 3 medical datasets, using 11\nnetworks trained on 3 paradigms, comprehensively analyze the effectiveness and\ngenerality of Ctrl-GenAug, particularly in underrepresented high-risk\npopulations and out-domain conditions.\n", "link": "http://arxiv.org/abs/2409.17091v1", "date": "2024-09-25", "relevancy": 2.2959, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5778}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5725}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ctrl-GenAug%3A%20Controllable%20Generative%20Augmentation%20for%20Medical%20Sequence%0A%20%20Classification&body=Title%3A%20Ctrl-GenAug%3A%20Controllable%20Generative%20Augmentation%20for%20Medical%20Sequence%0A%20%20Classification%0AAuthor%3A%20Xinrui%20Zhou%20and%20Yuhao%20Huang%20and%20Haoran%20Dou%20and%20Shijing%20Chen%20and%20Ao%20Chang%20and%20Jia%20Liu%20and%20Weiran%20Long%20and%20Jian%20Zheng%20and%20Erjiao%20Xu%20and%20Jie%20Ren%20and%20Ruobing%20Huang%20and%20Jun%20Cheng%20and%20Wufeng%20Xue%20and%20Dong%20Ni%0AAbstract%3A%20%20%20In%20the%20medical%20field%2C%20the%20limited%20availability%20of%20large-scale%20datasets%20and%0Alabor-intensive%20annotation%20processes%20hinder%20the%20performance%20of%20deep%20models.%0ADiffusion-based%20generative%20augmentation%20approaches%20present%20a%20promising%20solution%0Ato%20this%20issue%2C%20having%20been%20proven%20effective%20in%20advancing%20downstream%20medical%0Arecognition%20tasks.%20Nevertheless%2C%20existing%20works%20lack%20sufficient%20semantic%20and%0Asequential%20steerability%20for%20challenging%20video/3D%20sequence%20generation%2C%20and%0Aneglect%20quality%20control%20of%20noisy%20synthesized%20samples%2C%20resulting%20in%20unreliable%0Asynthetic%20databases%20and%20severely%20limiting%20the%20performance%20of%20downstream%20tasks.%0AIn%20this%20work%2C%20we%20present%20Ctrl-GenAug%2C%20a%20novel%20and%20general%20generative%0Aaugmentation%20framework%20that%20enables%20highly%20semantic-%20and%20sequential-customized%0Asequence%20synthesis%20and%20suppresses%20incorrectly%20synthesized%20samples%2C%20to%20aid%0Amedical%20sequence%20classification.%20Specifically%2C%20we%20first%20design%20a%20multimodal%0Aconditions-guided%20sequence%20generator%20for%20controllably%20synthesizing%0Adiagnosis-promotive%20samples.%20A%20sequential%20augmentation%20module%20is%20integrated%20to%0Aenhance%20the%20temporal/stereoscopic%20coherence%20of%20generated%20samples.%20Then%2C%20we%0Apropose%20a%20noisy%20synthetic%20data%20filter%20to%20suppress%20unreliable%20cases%20at%20semantic%0Aand%20sequential%20levels.%20Extensive%20experiments%20on%203%20medical%20datasets%2C%20using%2011%0Anetworks%20trained%20on%203%20paradigms%2C%20comprehensively%20analyze%20the%20effectiveness%20and%0Agenerality%20of%20Ctrl-GenAug%2C%20particularly%20in%20underrepresented%20high-risk%0Apopulations%20and%20out-domain%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17091v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCtrl-GenAug%253A%2520Controllable%2520Generative%2520Augmentation%2520for%2520Medical%2520Sequence%250A%2520%2520Classification%26entry.906535625%3DXinrui%2520Zhou%2520and%2520Yuhao%2520Huang%2520and%2520Haoran%2520Dou%2520and%2520Shijing%2520Chen%2520and%2520Ao%2520Chang%2520and%2520Jia%2520Liu%2520and%2520Weiran%2520Long%2520and%2520Jian%2520Zheng%2520and%2520Erjiao%2520Xu%2520and%2520Jie%2520Ren%2520and%2520Ruobing%2520Huang%2520and%2520Jun%2520Cheng%2520and%2520Wufeng%2520Xue%2520and%2520Dong%2520Ni%26entry.1292438233%3D%2520%2520In%2520the%2520medical%2520field%252C%2520the%2520limited%2520availability%2520of%2520large-scale%2520datasets%2520and%250Alabor-intensive%2520annotation%2520processes%2520hinder%2520the%2520performance%2520of%2520deep%2520models.%250ADiffusion-based%2520generative%2520augmentation%2520approaches%2520present%2520a%2520promising%2520solution%250Ato%2520this%2520issue%252C%2520having%2520been%2520proven%2520effective%2520in%2520advancing%2520downstream%2520medical%250Arecognition%2520tasks.%2520Nevertheless%252C%2520existing%2520works%2520lack%2520sufficient%2520semantic%2520and%250Asequential%2520steerability%2520for%2520challenging%2520video/3D%2520sequence%2520generation%252C%2520and%250Aneglect%2520quality%2520control%2520of%2520noisy%2520synthesized%2520samples%252C%2520resulting%2520in%2520unreliable%250Asynthetic%2520databases%2520and%2520severely%2520limiting%2520the%2520performance%2520of%2520downstream%2520tasks.%250AIn%2520this%2520work%252C%2520we%2520present%2520Ctrl-GenAug%252C%2520a%2520novel%2520and%2520general%2520generative%250Aaugmentation%2520framework%2520that%2520enables%2520highly%2520semantic-%2520and%2520sequential-customized%250Asequence%2520synthesis%2520and%2520suppresses%2520incorrectly%2520synthesized%2520samples%252C%2520to%2520aid%250Amedical%2520sequence%2520classification.%2520Specifically%252C%2520we%2520first%2520design%2520a%2520multimodal%250Aconditions-guided%2520sequence%2520generator%2520for%2520controllably%2520synthesizing%250Adiagnosis-promotive%2520samples.%2520A%2520sequential%2520augmentation%2520module%2520is%2520integrated%2520to%250Aenhance%2520the%2520temporal/stereoscopic%2520coherence%2520of%2520generated%2520samples.%2520Then%252C%2520we%250Apropose%2520a%2520noisy%2520synthetic%2520data%2520filter%2520to%2520suppress%2520unreliable%2520cases%2520at%2520semantic%250Aand%2520sequential%2520levels.%2520Extensive%2520experiments%2520on%25203%2520medical%2520datasets%252C%2520using%252011%250Anetworks%2520trained%2520on%25203%2520paradigms%252C%2520comprehensively%2520analyze%2520the%2520effectiveness%2520and%250Agenerality%2520of%2520Ctrl-GenAug%252C%2520particularly%2520in%2520underrepresented%2520high-risk%250Apopulations%2520and%2520out-domain%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17091v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ctrl-GenAug%3A%20Controllable%20Generative%20Augmentation%20for%20Medical%20Sequence%0A%20%20Classification&entry.906535625=Xinrui%20Zhou%20and%20Yuhao%20Huang%20and%20Haoran%20Dou%20and%20Shijing%20Chen%20and%20Ao%20Chang%20and%20Jia%20Liu%20and%20Weiran%20Long%20and%20Jian%20Zheng%20and%20Erjiao%20Xu%20and%20Jie%20Ren%20and%20Ruobing%20Huang%20and%20Jun%20Cheng%20and%20Wufeng%20Xue%20and%20Dong%20Ni&entry.1292438233=%20%20In%20the%20medical%20field%2C%20the%20limited%20availability%20of%20large-scale%20datasets%20and%0Alabor-intensive%20annotation%20processes%20hinder%20the%20performance%20of%20deep%20models.%0ADiffusion-based%20generative%20augmentation%20approaches%20present%20a%20promising%20solution%0Ato%20this%20issue%2C%20having%20been%20proven%20effective%20in%20advancing%20downstream%20medical%0Arecognition%20tasks.%20Nevertheless%2C%20existing%20works%20lack%20sufficient%20semantic%20and%0Asequential%20steerability%20for%20challenging%20video/3D%20sequence%20generation%2C%20and%0Aneglect%20quality%20control%20of%20noisy%20synthesized%20samples%2C%20resulting%20in%20unreliable%0Asynthetic%20databases%20and%20severely%20limiting%20the%20performance%20of%20downstream%20tasks.%0AIn%20this%20work%2C%20we%20present%20Ctrl-GenAug%2C%20a%20novel%20and%20general%20generative%0Aaugmentation%20framework%20that%20enables%20highly%20semantic-%20and%20sequential-customized%0Asequence%20synthesis%20and%20suppresses%20incorrectly%20synthesized%20samples%2C%20to%20aid%0Amedical%20sequence%20classification.%20Specifically%2C%20we%20first%20design%20a%20multimodal%0Aconditions-guided%20sequence%20generator%20for%20controllably%20synthesizing%0Adiagnosis-promotive%20samples.%20A%20sequential%20augmentation%20module%20is%20integrated%20to%0Aenhance%20the%20temporal/stereoscopic%20coherence%20of%20generated%20samples.%20Then%2C%20we%0Apropose%20a%20noisy%20synthetic%20data%20filter%20to%20suppress%20unreliable%20cases%20at%20semantic%0Aand%20sequential%20levels.%20Extensive%20experiments%20on%203%20medical%20datasets%2C%20using%2011%0Anetworks%20trained%20on%203%20paradigms%2C%20comprehensively%20analyze%20the%20effectiveness%20and%0Agenerality%20of%20Ctrl-GenAug%2C%20particularly%20in%20underrepresented%20high-risk%0Apopulations%20and%20out-domain%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17091v1&entry.124074799=Read"},
{"title": "Blox-Net: Generative Design-for-Robot-Assembly Using VLM Supervision,\n  Physics Simulation, and a Robot with Reset", "author": "Andrew Goldberg and Kavish Kondap and Tianshuang Qiu and Zehan Ma and Letian Fu and Justin Kerr and Huang Huang and Kaiyuan Chen and Kuan Fang and Ken Goldberg", "abstract": "  Generative AI systems have shown impressive capabilities in creating text,\ncode, and images. Inspired by the rich history of research in industrial\n''Design for Assembly'', we introduce a novel problem: Generative\nDesign-for-Robot-Assembly (GDfRA). The task is to generate an assembly based on\na natural language prompt (e.g., ''giraffe'') and an image of available\nphysical components, such as 3D-printed blocks. The output is an assembly, a\nspatial arrangement of these components, and instructions for a robot to build\nthis assembly. The output must 1) resemble the requested object and 2) be\nreliably assembled by a 6 DoF robot arm with a suction gripper. We then present\nBlox-Net, a GDfRA system that combines generative vision language models with\nwell-established methods in computer vision, simulation, perturbation analysis,\nmotion planning, and physical robot experimentation to solve a class of GDfRA\nproblems with minimal human supervision. Blox-Net achieved a Top-1 accuracy of\n63.5% in the ''recognizability'' of its designed assemblies (eg, resembling\ngiraffe as judged by a VLM). These designs, after automated perturbation\nredesign, were reliably assembled by a robot, achieving near-perfect success\nacross 10 consecutive assembly iterations with human intervention only during\nreset prior to assembly. Surprisingly, this entire design process from textual\nword (''giraffe'') to reliable physical assembly is performed with zero human\nintervention.\n", "link": "http://arxiv.org/abs/2409.17126v1", "date": "2024-09-25", "relevancy": 2.2951, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5937}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5804}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Blox-Net%3A%20Generative%20Design-for-Robot-Assembly%20Using%20VLM%20Supervision%2C%0A%20%20Physics%20Simulation%2C%20and%20a%20Robot%20with%20Reset&body=Title%3A%20Blox-Net%3A%20Generative%20Design-for-Robot-Assembly%20Using%20VLM%20Supervision%2C%0A%20%20Physics%20Simulation%2C%20and%20a%20Robot%20with%20Reset%0AAuthor%3A%20Andrew%20Goldberg%20and%20Kavish%20Kondap%20and%20Tianshuang%20Qiu%20and%20Zehan%20Ma%20and%20Letian%20Fu%20and%20Justin%20Kerr%20and%20Huang%20Huang%20and%20Kaiyuan%20Chen%20and%20Kuan%20Fang%20and%20Ken%20Goldberg%0AAbstract%3A%20%20%20Generative%20AI%20systems%20have%20shown%20impressive%20capabilities%20in%20creating%20text%2C%0Acode%2C%20and%20images.%20Inspired%20by%20the%20rich%20history%20of%20research%20in%20industrial%0A%27%27Design%20for%20Assembly%27%27%2C%20we%20introduce%20a%20novel%20problem%3A%20Generative%0ADesign-for-Robot-Assembly%20%28GDfRA%29.%20The%20task%20is%20to%20generate%20an%20assembly%20based%20on%0Aa%20natural%20language%20prompt%20%28e.g.%2C%20%27%27giraffe%27%27%29%20and%20an%20image%20of%20available%0Aphysical%20components%2C%20such%20as%203D-printed%20blocks.%20The%20output%20is%20an%20assembly%2C%20a%0Aspatial%20arrangement%20of%20these%20components%2C%20and%20instructions%20for%20a%20robot%20to%20build%0Athis%20assembly.%20The%20output%20must%201%29%20resemble%20the%20requested%20object%20and%202%29%20be%0Areliably%20assembled%20by%20a%206%20DoF%20robot%20arm%20with%20a%20suction%20gripper.%20We%20then%20present%0ABlox-Net%2C%20a%20GDfRA%20system%20that%20combines%20generative%20vision%20language%20models%20with%0Awell-established%20methods%20in%20computer%20vision%2C%20simulation%2C%20perturbation%20analysis%2C%0Amotion%20planning%2C%20and%20physical%20robot%20experimentation%20to%20solve%20a%20class%20of%20GDfRA%0Aproblems%20with%20minimal%20human%20supervision.%20Blox-Net%20achieved%20a%20Top-1%20accuracy%20of%0A63.5%25%20in%20the%20%27%27recognizability%27%27%20of%20its%20designed%20assemblies%20%28eg%2C%20resembling%0Agiraffe%20as%20judged%20by%20a%20VLM%29.%20These%20designs%2C%20after%20automated%20perturbation%0Aredesign%2C%20were%20reliably%20assembled%20by%20a%20robot%2C%20achieving%20near-perfect%20success%0Aacross%2010%20consecutive%20assembly%20iterations%20with%20human%20intervention%20only%20during%0Areset%20prior%20to%20assembly.%20Surprisingly%2C%20this%20entire%20design%20process%20from%20textual%0Aword%20%28%27%27giraffe%27%27%29%20to%20reliable%20physical%20assembly%20is%20performed%20with%20zero%20human%0Aintervention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17126v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlox-Net%253A%2520Generative%2520Design-for-Robot-Assembly%2520Using%2520VLM%2520Supervision%252C%250A%2520%2520Physics%2520Simulation%252C%2520and%2520a%2520Robot%2520with%2520Reset%26entry.906535625%3DAndrew%2520Goldberg%2520and%2520Kavish%2520Kondap%2520and%2520Tianshuang%2520Qiu%2520and%2520Zehan%2520Ma%2520and%2520Letian%2520Fu%2520and%2520Justin%2520Kerr%2520and%2520Huang%2520Huang%2520and%2520Kaiyuan%2520Chen%2520and%2520Kuan%2520Fang%2520and%2520Ken%2520Goldberg%26entry.1292438233%3D%2520%2520Generative%2520AI%2520systems%2520have%2520shown%2520impressive%2520capabilities%2520in%2520creating%2520text%252C%250Acode%252C%2520and%2520images.%2520Inspired%2520by%2520the%2520rich%2520history%2520of%2520research%2520in%2520industrial%250A%2527%2527Design%2520for%2520Assembly%2527%2527%252C%2520we%2520introduce%2520a%2520novel%2520problem%253A%2520Generative%250ADesign-for-Robot-Assembly%2520%2528GDfRA%2529.%2520The%2520task%2520is%2520to%2520generate%2520an%2520assembly%2520based%2520on%250Aa%2520natural%2520language%2520prompt%2520%2528e.g.%252C%2520%2527%2527giraffe%2527%2527%2529%2520and%2520an%2520image%2520of%2520available%250Aphysical%2520components%252C%2520such%2520as%25203D-printed%2520blocks.%2520The%2520output%2520is%2520an%2520assembly%252C%2520a%250Aspatial%2520arrangement%2520of%2520these%2520components%252C%2520and%2520instructions%2520for%2520a%2520robot%2520to%2520build%250Athis%2520assembly.%2520The%2520output%2520must%25201%2529%2520resemble%2520the%2520requested%2520object%2520and%25202%2529%2520be%250Areliably%2520assembled%2520by%2520a%25206%2520DoF%2520robot%2520arm%2520with%2520a%2520suction%2520gripper.%2520We%2520then%2520present%250ABlox-Net%252C%2520a%2520GDfRA%2520system%2520that%2520combines%2520generative%2520vision%2520language%2520models%2520with%250Awell-established%2520methods%2520in%2520computer%2520vision%252C%2520simulation%252C%2520perturbation%2520analysis%252C%250Amotion%2520planning%252C%2520and%2520physical%2520robot%2520experimentation%2520to%2520solve%2520a%2520class%2520of%2520GDfRA%250Aproblems%2520with%2520minimal%2520human%2520supervision.%2520Blox-Net%2520achieved%2520a%2520Top-1%2520accuracy%2520of%250A63.5%2525%2520in%2520the%2520%2527%2527recognizability%2527%2527%2520of%2520its%2520designed%2520assemblies%2520%2528eg%252C%2520resembling%250Agiraffe%2520as%2520judged%2520by%2520a%2520VLM%2529.%2520These%2520designs%252C%2520after%2520automated%2520perturbation%250Aredesign%252C%2520were%2520reliably%2520assembled%2520by%2520a%2520robot%252C%2520achieving%2520near-perfect%2520success%250Aacross%252010%2520consecutive%2520assembly%2520iterations%2520with%2520human%2520intervention%2520only%2520during%250Areset%2520prior%2520to%2520assembly.%2520Surprisingly%252C%2520this%2520entire%2520design%2520process%2520from%2520textual%250Aword%2520%2528%2527%2527giraffe%2527%2527%2529%2520to%2520reliable%2520physical%2520assembly%2520is%2520performed%2520with%2520zero%2520human%250Aintervention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17126v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blox-Net%3A%20Generative%20Design-for-Robot-Assembly%20Using%20VLM%20Supervision%2C%0A%20%20Physics%20Simulation%2C%20and%20a%20Robot%20with%20Reset&entry.906535625=Andrew%20Goldberg%20and%20Kavish%20Kondap%20and%20Tianshuang%20Qiu%20and%20Zehan%20Ma%20and%20Letian%20Fu%20and%20Justin%20Kerr%20and%20Huang%20Huang%20and%20Kaiyuan%20Chen%20and%20Kuan%20Fang%20and%20Ken%20Goldberg&entry.1292438233=%20%20Generative%20AI%20systems%20have%20shown%20impressive%20capabilities%20in%20creating%20text%2C%0Acode%2C%20and%20images.%20Inspired%20by%20the%20rich%20history%20of%20research%20in%20industrial%0A%27%27Design%20for%20Assembly%27%27%2C%20we%20introduce%20a%20novel%20problem%3A%20Generative%0ADesign-for-Robot-Assembly%20%28GDfRA%29.%20The%20task%20is%20to%20generate%20an%20assembly%20based%20on%0Aa%20natural%20language%20prompt%20%28e.g.%2C%20%27%27giraffe%27%27%29%20and%20an%20image%20of%20available%0Aphysical%20components%2C%20such%20as%203D-printed%20blocks.%20The%20output%20is%20an%20assembly%2C%20a%0Aspatial%20arrangement%20of%20these%20components%2C%20and%20instructions%20for%20a%20robot%20to%20build%0Athis%20assembly.%20The%20output%20must%201%29%20resemble%20the%20requested%20object%20and%202%29%20be%0Areliably%20assembled%20by%20a%206%20DoF%20robot%20arm%20with%20a%20suction%20gripper.%20We%20then%20present%0ABlox-Net%2C%20a%20GDfRA%20system%20that%20combines%20generative%20vision%20language%20models%20with%0Awell-established%20methods%20in%20computer%20vision%2C%20simulation%2C%20perturbation%20analysis%2C%0Amotion%20planning%2C%20and%20physical%20robot%20experimentation%20to%20solve%20a%20class%20of%20GDfRA%0Aproblems%20with%20minimal%20human%20supervision.%20Blox-Net%20achieved%20a%20Top-1%20accuracy%20of%0A63.5%25%20in%20the%20%27%27recognizability%27%27%20of%20its%20designed%20assemblies%20%28eg%2C%20resembling%0Agiraffe%20as%20judged%20by%20a%20VLM%29.%20These%20designs%2C%20after%20automated%20perturbation%0Aredesign%2C%20were%20reliably%20assembled%20by%20a%20robot%2C%20achieving%20near-perfect%20success%0Aacross%2010%20consecutive%20assembly%20iterations%20with%20human%20intervention%20only%20during%0Areset%20prior%20to%20assembly.%20Surprisingly%2C%20this%20entire%20design%20process%20from%20textual%0Aword%20%28%27%27giraffe%27%27%29%20to%20reliable%20physical%20assembly%20is%20performed%20with%20zero%20human%0Aintervention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17126v1&entry.124074799=Read"},
{"title": "Path-adaptive Spatio-Temporal State Space Model for Event-based\n  Recognition with Arbitrary Duration", "author": "Jiazhou Zhou and Kanghao Chen and Lei Zhang and Lin Wang", "abstract": "  Event cameras are bio-inspired sensors that capture the intensity changes\nasynchronously and output event streams with distinct advantages, such as high\ntemporal resolution. To exploit event cameras for object/action recognition,\nexisting methods predominantly sample and aggregate events in a second-level\nduration at every fixed temporal interval (or frequency). However, they often\nface difficulties in capturing the spatiotemporal relationships for longer,\ne.g., minute-level, events and generalizing across varying temporal\nfrequencies. To fill the gap, we present a novel framework, dubbed PAST-SSM,\nexhibiting superior capacity in recognizing events with arbitrary duration\n(e.g., 0.1s to 4.5s) and generalizing to varying inference frequencies. Our key\ninsight is to learn the spatiotemporal relationships from the encoded event\nfeatures via the state space model (SSM) -- whose linear complexity makes it\nideal for modeling high temporal resolution events with longer sequences. To\nachieve this goal, we first propose a Path-Adaptive Event Aggregation and Scan\n(PEAS) module to encode events of varying duration into features with fixed\ndimensions by adaptively scanning and selecting aggregated event frames. On top\nof PEAS, we introduce a novel Multi-faceted Selection Guiding (MSG) loss to\nminimize the randomness and redundancy of the encoded features. This subtly\nenhances the model generalization across different inference frequencies.\nLastly, the SSM is employed to better learn the spatiotemporal properties from\nthe encoded features. Moreover, we build a minute-level event-based recognition\ndataset, named ArDVS100, with arbitrary duration for the benefit of the\ncommunity. Extensive experiments prove that our method outperforms prior arts\nby +3.45%, +0.38% and +8.31% on the DVS Action, SeAct and HARDVS datasets,\nrespectively.\n", "link": "http://arxiv.org/abs/2409.16953v1", "date": "2024-09-25", "relevancy": 2.29, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5935}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5599}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Path-adaptive%20Spatio-Temporal%20State%20Space%20Model%20for%20Event-based%0A%20%20Recognition%20with%20Arbitrary%20Duration&body=Title%3A%20Path-adaptive%20Spatio-Temporal%20State%20Space%20Model%20for%20Event-based%0A%20%20Recognition%20with%20Arbitrary%20Duration%0AAuthor%3A%20Jiazhou%20Zhou%20and%20Kanghao%20Chen%20and%20Lei%20Zhang%20and%20Lin%20Wang%0AAbstract%3A%20%20%20Event%20cameras%20are%20bio-inspired%20sensors%20that%20capture%20the%20intensity%20changes%0Aasynchronously%20and%20output%20event%20streams%20with%20distinct%20advantages%2C%20such%20as%20high%0Atemporal%20resolution.%20To%20exploit%20event%20cameras%20for%20object/action%20recognition%2C%0Aexisting%20methods%20predominantly%20sample%20and%20aggregate%20events%20in%20a%20second-level%0Aduration%20at%20every%20fixed%20temporal%20interval%20%28or%20frequency%29.%20However%2C%20they%20often%0Aface%20difficulties%20in%20capturing%20the%20spatiotemporal%20relationships%20for%20longer%2C%0Ae.g.%2C%20minute-level%2C%20events%20and%20generalizing%20across%20varying%20temporal%0Afrequencies.%20To%20fill%20the%20gap%2C%20we%20present%20a%20novel%20framework%2C%20dubbed%20PAST-SSM%2C%0Aexhibiting%20superior%20capacity%20in%20recognizing%20events%20with%20arbitrary%20duration%0A%28e.g.%2C%200.1s%20to%204.5s%29%20and%20generalizing%20to%20varying%20inference%20frequencies.%20Our%20key%0Ainsight%20is%20to%20learn%20the%20spatiotemporal%20relationships%20from%20the%20encoded%20event%0Afeatures%20via%20the%20state%20space%20model%20%28SSM%29%20--%20whose%20linear%20complexity%20makes%20it%0Aideal%20for%20modeling%20high%20temporal%20resolution%20events%20with%20longer%20sequences.%20To%0Aachieve%20this%20goal%2C%20we%20first%20propose%20a%20Path-Adaptive%20Event%20Aggregation%20and%20Scan%0A%28PEAS%29%20module%20to%20encode%20events%20of%20varying%20duration%20into%20features%20with%20fixed%0Adimensions%20by%20adaptively%20scanning%20and%20selecting%20aggregated%20event%20frames.%20On%20top%0Aof%20PEAS%2C%20we%20introduce%20a%20novel%20Multi-faceted%20Selection%20Guiding%20%28MSG%29%20loss%20to%0Aminimize%20the%20randomness%20and%20redundancy%20of%20the%20encoded%20features.%20This%20subtly%0Aenhances%20the%20model%20generalization%20across%20different%20inference%20frequencies.%0ALastly%2C%20the%20SSM%20is%20employed%20to%20better%20learn%20the%20spatiotemporal%20properties%20from%0Athe%20encoded%20features.%20Moreover%2C%20we%20build%20a%20minute-level%20event-based%20recognition%0Adataset%2C%20named%20ArDVS100%2C%20with%20arbitrary%20duration%20for%20the%20benefit%20of%20the%0Acommunity.%20Extensive%20experiments%20prove%20that%20our%20method%20outperforms%20prior%20arts%0Aby%20%2B3.45%25%2C%20%2B0.38%25%20and%20%2B8.31%25%20on%20the%20DVS%20Action%2C%20SeAct%20and%20HARDVS%20datasets%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPath-adaptive%2520Spatio-Temporal%2520State%2520Space%2520Model%2520for%2520Event-based%250A%2520%2520Recognition%2520with%2520Arbitrary%2520Duration%26entry.906535625%3DJiazhou%2520Zhou%2520and%2520Kanghao%2520Chen%2520and%2520Lei%2520Zhang%2520and%2520Lin%2520Wang%26entry.1292438233%3D%2520%2520Event%2520cameras%2520are%2520bio-inspired%2520sensors%2520that%2520capture%2520the%2520intensity%2520changes%250Aasynchronously%2520and%2520output%2520event%2520streams%2520with%2520distinct%2520advantages%252C%2520such%2520as%2520high%250Atemporal%2520resolution.%2520To%2520exploit%2520event%2520cameras%2520for%2520object/action%2520recognition%252C%250Aexisting%2520methods%2520predominantly%2520sample%2520and%2520aggregate%2520events%2520in%2520a%2520second-level%250Aduration%2520at%2520every%2520fixed%2520temporal%2520interval%2520%2528or%2520frequency%2529.%2520However%252C%2520they%2520often%250Aface%2520difficulties%2520in%2520capturing%2520the%2520spatiotemporal%2520relationships%2520for%2520longer%252C%250Ae.g.%252C%2520minute-level%252C%2520events%2520and%2520generalizing%2520across%2520varying%2520temporal%250Afrequencies.%2520To%2520fill%2520the%2520gap%252C%2520we%2520present%2520a%2520novel%2520framework%252C%2520dubbed%2520PAST-SSM%252C%250Aexhibiting%2520superior%2520capacity%2520in%2520recognizing%2520events%2520with%2520arbitrary%2520duration%250A%2528e.g.%252C%25200.1s%2520to%25204.5s%2529%2520and%2520generalizing%2520to%2520varying%2520inference%2520frequencies.%2520Our%2520key%250Ainsight%2520is%2520to%2520learn%2520the%2520spatiotemporal%2520relationships%2520from%2520the%2520encoded%2520event%250Afeatures%2520via%2520the%2520state%2520space%2520model%2520%2528SSM%2529%2520--%2520whose%2520linear%2520complexity%2520makes%2520it%250Aideal%2520for%2520modeling%2520high%2520temporal%2520resolution%2520events%2520with%2520longer%2520sequences.%2520To%250Aachieve%2520this%2520goal%252C%2520we%2520first%2520propose%2520a%2520Path-Adaptive%2520Event%2520Aggregation%2520and%2520Scan%250A%2528PEAS%2529%2520module%2520to%2520encode%2520events%2520of%2520varying%2520duration%2520into%2520features%2520with%2520fixed%250Adimensions%2520by%2520adaptively%2520scanning%2520and%2520selecting%2520aggregated%2520event%2520frames.%2520On%2520top%250Aof%2520PEAS%252C%2520we%2520introduce%2520a%2520novel%2520Multi-faceted%2520Selection%2520Guiding%2520%2528MSG%2529%2520loss%2520to%250Aminimize%2520the%2520randomness%2520and%2520redundancy%2520of%2520the%2520encoded%2520features.%2520This%2520subtly%250Aenhances%2520the%2520model%2520generalization%2520across%2520different%2520inference%2520frequencies.%250ALastly%252C%2520the%2520SSM%2520is%2520employed%2520to%2520better%2520learn%2520the%2520spatiotemporal%2520properties%2520from%250Athe%2520encoded%2520features.%2520Moreover%252C%2520we%2520build%2520a%2520minute-level%2520event-based%2520recognition%250Adataset%252C%2520named%2520ArDVS100%252C%2520with%2520arbitrary%2520duration%2520for%2520the%2520benefit%2520of%2520the%250Acommunity.%2520Extensive%2520experiments%2520prove%2520that%2520our%2520method%2520outperforms%2520prior%2520arts%250Aby%2520%252B3.45%2525%252C%2520%252B0.38%2525%2520and%2520%252B8.31%2525%2520on%2520the%2520DVS%2520Action%252C%2520SeAct%2520and%2520HARDVS%2520datasets%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Path-adaptive%20Spatio-Temporal%20State%20Space%20Model%20for%20Event-based%0A%20%20Recognition%20with%20Arbitrary%20Duration&entry.906535625=Jiazhou%20Zhou%20and%20Kanghao%20Chen%20and%20Lei%20Zhang%20and%20Lin%20Wang&entry.1292438233=%20%20Event%20cameras%20are%20bio-inspired%20sensors%20that%20capture%20the%20intensity%20changes%0Aasynchronously%20and%20output%20event%20streams%20with%20distinct%20advantages%2C%20such%20as%20high%0Atemporal%20resolution.%20To%20exploit%20event%20cameras%20for%20object/action%20recognition%2C%0Aexisting%20methods%20predominantly%20sample%20and%20aggregate%20events%20in%20a%20second-level%0Aduration%20at%20every%20fixed%20temporal%20interval%20%28or%20frequency%29.%20However%2C%20they%20often%0Aface%20difficulties%20in%20capturing%20the%20spatiotemporal%20relationships%20for%20longer%2C%0Ae.g.%2C%20minute-level%2C%20events%20and%20generalizing%20across%20varying%20temporal%0Afrequencies.%20To%20fill%20the%20gap%2C%20we%20present%20a%20novel%20framework%2C%20dubbed%20PAST-SSM%2C%0Aexhibiting%20superior%20capacity%20in%20recognizing%20events%20with%20arbitrary%20duration%0A%28e.g.%2C%200.1s%20to%204.5s%29%20and%20generalizing%20to%20varying%20inference%20frequencies.%20Our%20key%0Ainsight%20is%20to%20learn%20the%20spatiotemporal%20relationships%20from%20the%20encoded%20event%0Afeatures%20via%20the%20state%20space%20model%20%28SSM%29%20--%20whose%20linear%20complexity%20makes%20it%0Aideal%20for%20modeling%20high%20temporal%20resolution%20events%20with%20longer%20sequences.%20To%0Aachieve%20this%20goal%2C%20we%20first%20propose%20a%20Path-Adaptive%20Event%20Aggregation%20and%20Scan%0A%28PEAS%29%20module%20to%20encode%20events%20of%20varying%20duration%20into%20features%20with%20fixed%0Adimensions%20by%20adaptively%20scanning%20and%20selecting%20aggregated%20event%20frames.%20On%20top%0Aof%20PEAS%2C%20we%20introduce%20a%20novel%20Multi-faceted%20Selection%20Guiding%20%28MSG%29%20loss%20to%0Aminimize%20the%20randomness%20and%20redundancy%20of%20the%20encoded%20features.%20This%20subtly%0Aenhances%20the%20model%20generalization%20across%20different%20inference%20frequencies.%0ALastly%2C%20the%20SSM%20is%20employed%20to%20better%20learn%20the%20spatiotemporal%20properties%20from%0Athe%20encoded%20features.%20Moreover%2C%20we%20build%20a%20minute-level%20event-based%20recognition%0Adataset%2C%20named%20ArDVS100%2C%20with%20arbitrary%20duration%20for%20the%20benefit%20of%20the%0Acommunity.%20Extensive%20experiments%20prove%20that%20our%20method%20outperforms%20prior%20arts%0Aby%20%2B3.45%25%2C%20%2B0.38%25%20and%20%2B8.31%25%20on%20the%20DVS%20Action%2C%20SeAct%20and%20HARDVS%20datasets%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16953v1&entry.124074799=Read"},
{"title": "GeoBiked: A Dataset with Geometric Features and Automated Labeling\n  Techniques to Enable Deep Generative Models in Engineering Design", "author": "Phillip Mueller and Sebastian Mueller and Lars Mikelsons", "abstract": "  We provide a dataset for enabling Deep Generative Models (DGMs) in\nengineering design and propose methods to automate data labeling by utilizing\nlarge-scale foundation models. GeoBiked is curated to contain 4 355 bicycle\nimages, annotated with structural and technical features and is used to\ninvestigate two automated labeling techniques: The utilization of consolidated\nlatent features (Hyperfeatures) from image-generation models to detect\ngeometric correspondences (e.g. the position of the wheel center) in structural\nimages and the generation of diverse text descriptions for structural images.\nGPT-4o, a vision-language-model (VLM), is instructed to analyze images and\nproduce diverse descriptions aligned with the system-prompt. By representing\ntechnical images as Diffusion-Hyperfeatures, drawing geometric correspondences\nbetween them is possible. The detection accuracy of geometric points in unseen\nsamples is improved by presenting multiple annotated source images. GPT-4o has\nsufficient capabilities to generate accurate descriptions of technical images.\nGrounding the generation only on images leads to diverse descriptions but\ncauses hallucinations, while grounding it on categorical labels restricts the\ndiversity. Using both as input balances creativity and accuracy. Successfully\nusing Hyperfeatures for geometric correspondence suggests that this approach\ncan be used for general point-detection and annotation tasks in technical\nimages. Labeling such images with text descriptions using VLMs is possible, but\ndependent on the models detection capabilities, careful prompt-engineering and\nthe selection of input information. Applying foundation models in engineering\ndesign is largely unexplored. We aim to bridge this gap with a dataset to\nexplore training, finetuning and conditioning DGMs in this field and suggesting\napproaches to bootstrap foundation models to process technical images.\n", "link": "http://arxiv.org/abs/2409.17045v1", "date": "2024-09-25", "relevancy": 2.2665, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5749}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5611}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoBiked%3A%20A%20Dataset%20with%20Geometric%20Features%20and%20Automated%20Labeling%0A%20%20Techniques%20to%20Enable%20Deep%20Generative%20Models%20in%20Engineering%20Design&body=Title%3A%20GeoBiked%3A%20A%20Dataset%20with%20Geometric%20Features%20and%20Automated%20Labeling%0A%20%20Techniques%20to%20Enable%20Deep%20Generative%20Models%20in%20Engineering%20Design%0AAuthor%3A%20Phillip%20Mueller%20and%20Sebastian%20Mueller%20and%20Lars%20Mikelsons%0AAbstract%3A%20%20%20We%20provide%20a%20dataset%20for%20enabling%20Deep%20Generative%20Models%20%28DGMs%29%20in%0Aengineering%20design%20and%20propose%20methods%20to%20automate%20data%20labeling%20by%20utilizing%0Alarge-scale%20foundation%20models.%20GeoBiked%20is%20curated%20to%20contain%204%20355%20bicycle%0Aimages%2C%20annotated%20with%20structural%20and%20technical%20features%20and%20is%20used%20to%0Ainvestigate%20two%20automated%20labeling%20techniques%3A%20The%20utilization%20of%20consolidated%0Alatent%20features%20%28Hyperfeatures%29%20from%20image-generation%20models%20to%20detect%0Ageometric%20correspondences%20%28e.g.%20the%20position%20of%20the%20wheel%20center%29%20in%20structural%0Aimages%20and%20the%20generation%20of%20diverse%20text%20descriptions%20for%20structural%20images.%0AGPT-4o%2C%20a%20vision-language-model%20%28VLM%29%2C%20is%20instructed%20to%20analyze%20images%20and%0Aproduce%20diverse%20descriptions%20aligned%20with%20the%20system-prompt.%20By%20representing%0Atechnical%20images%20as%20Diffusion-Hyperfeatures%2C%20drawing%20geometric%20correspondences%0Abetween%20them%20is%20possible.%20The%20detection%20accuracy%20of%20geometric%20points%20in%20unseen%0Asamples%20is%20improved%20by%20presenting%20multiple%20annotated%20source%20images.%20GPT-4o%20has%0Asufficient%20capabilities%20to%20generate%20accurate%20descriptions%20of%20technical%20images.%0AGrounding%20the%20generation%20only%20on%20images%20leads%20to%20diverse%20descriptions%20but%0Acauses%20hallucinations%2C%20while%20grounding%20it%20on%20categorical%20labels%20restricts%20the%0Adiversity.%20Using%20both%20as%20input%20balances%20creativity%20and%20accuracy.%20Successfully%0Ausing%20Hyperfeatures%20for%20geometric%20correspondence%20suggests%20that%20this%20approach%0Acan%20be%20used%20for%20general%20point-detection%20and%20annotation%20tasks%20in%20technical%0Aimages.%20Labeling%20such%20images%20with%20text%20descriptions%20using%20VLMs%20is%20possible%2C%20but%0Adependent%20on%20the%20models%20detection%20capabilities%2C%20careful%20prompt-engineering%20and%0Athe%20selection%20of%20input%20information.%20Applying%20foundation%20models%20in%20engineering%0Adesign%20is%20largely%20unexplored.%20We%20aim%20to%20bridge%20this%20gap%20with%20a%20dataset%20to%0Aexplore%20training%2C%20finetuning%20and%20conditioning%20DGMs%20in%20this%20field%20and%20suggesting%0Aapproaches%20to%20bootstrap%20foundation%20models%20to%20process%20technical%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17045v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoBiked%253A%2520A%2520Dataset%2520with%2520Geometric%2520Features%2520and%2520Automated%2520Labeling%250A%2520%2520Techniques%2520to%2520Enable%2520Deep%2520Generative%2520Models%2520in%2520Engineering%2520Design%26entry.906535625%3DPhillip%2520Mueller%2520and%2520Sebastian%2520Mueller%2520and%2520Lars%2520Mikelsons%26entry.1292438233%3D%2520%2520We%2520provide%2520a%2520dataset%2520for%2520enabling%2520Deep%2520Generative%2520Models%2520%2528DGMs%2529%2520in%250Aengineering%2520design%2520and%2520propose%2520methods%2520to%2520automate%2520data%2520labeling%2520by%2520utilizing%250Alarge-scale%2520foundation%2520models.%2520GeoBiked%2520is%2520curated%2520to%2520contain%25204%2520355%2520bicycle%250Aimages%252C%2520annotated%2520with%2520structural%2520and%2520technical%2520features%2520and%2520is%2520used%2520to%250Ainvestigate%2520two%2520automated%2520labeling%2520techniques%253A%2520The%2520utilization%2520of%2520consolidated%250Alatent%2520features%2520%2528Hyperfeatures%2529%2520from%2520image-generation%2520models%2520to%2520detect%250Ageometric%2520correspondences%2520%2528e.g.%2520the%2520position%2520of%2520the%2520wheel%2520center%2529%2520in%2520structural%250Aimages%2520and%2520the%2520generation%2520of%2520diverse%2520text%2520descriptions%2520for%2520structural%2520images.%250AGPT-4o%252C%2520a%2520vision-language-model%2520%2528VLM%2529%252C%2520is%2520instructed%2520to%2520analyze%2520images%2520and%250Aproduce%2520diverse%2520descriptions%2520aligned%2520with%2520the%2520system-prompt.%2520By%2520representing%250Atechnical%2520images%2520as%2520Diffusion-Hyperfeatures%252C%2520drawing%2520geometric%2520correspondences%250Abetween%2520them%2520is%2520possible.%2520The%2520detection%2520accuracy%2520of%2520geometric%2520points%2520in%2520unseen%250Asamples%2520is%2520improved%2520by%2520presenting%2520multiple%2520annotated%2520source%2520images.%2520GPT-4o%2520has%250Asufficient%2520capabilities%2520to%2520generate%2520accurate%2520descriptions%2520of%2520technical%2520images.%250AGrounding%2520the%2520generation%2520only%2520on%2520images%2520leads%2520to%2520diverse%2520descriptions%2520but%250Acauses%2520hallucinations%252C%2520while%2520grounding%2520it%2520on%2520categorical%2520labels%2520restricts%2520the%250Adiversity.%2520Using%2520both%2520as%2520input%2520balances%2520creativity%2520and%2520accuracy.%2520Successfully%250Ausing%2520Hyperfeatures%2520for%2520geometric%2520correspondence%2520suggests%2520that%2520this%2520approach%250Acan%2520be%2520used%2520for%2520general%2520point-detection%2520and%2520annotation%2520tasks%2520in%2520technical%250Aimages.%2520Labeling%2520such%2520images%2520with%2520text%2520descriptions%2520using%2520VLMs%2520is%2520possible%252C%2520but%250Adependent%2520on%2520the%2520models%2520detection%2520capabilities%252C%2520careful%2520prompt-engineering%2520and%250Athe%2520selection%2520of%2520input%2520information.%2520Applying%2520foundation%2520models%2520in%2520engineering%250Adesign%2520is%2520largely%2520unexplored.%2520We%2520aim%2520to%2520bridge%2520this%2520gap%2520with%2520a%2520dataset%2520to%250Aexplore%2520training%252C%2520finetuning%2520and%2520conditioning%2520DGMs%2520in%2520this%2520field%2520and%2520suggesting%250Aapproaches%2520to%2520bootstrap%2520foundation%2520models%2520to%2520process%2520technical%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17045v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoBiked%3A%20A%20Dataset%20with%20Geometric%20Features%20and%20Automated%20Labeling%0A%20%20Techniques%20to%20Enable%20Deep%20Generative%20Models%20in%20Engineering%20Design&entry.906535625=Phillip%20Mueller%20and%20Sebastian%20Mueller%20and%20Lars%20Mikelsons&entry.1292438233=%20%20We%20provide%20a%20dataset%20for%20enabling%20Deep%20Generative%20Models%20%28DGMs%29%20in%0Aengineering%20design%20and%20propose%20methods%20to%20automate%20data%20labeling%20by%20utilizing%0Alarge-scale%20foundation%20models.%20GeoBiked%20is%20curated%20to%20contain%204%20355%20bicycle%0Aimages%2C%20annotated%20with%20structural%20and%20technical%20features%20and%20is%20used%20to%0Ainvestigate%20two%20automated%20labeling%20techniques%3A%20The%20utilization%20of%20consolidated%0Alatent%20features%20%28Hyperfeatures%29%20from%20image-generation%20models%20to%20detect%0Ageometric%20correspondences%20%28e.g.%20the%20position%20of%20the%20wheel%20center%29%20in%20structural%0Aimages%20and%20the%20generation%20of%20diverse%20text%20descriptions%20for%20structural%20images.%0AGPT-4o%2C%20a%20vision-language-model%20%28VLM%29%2C%20is%20instructed%20to%20analyze%20images%20and%0Aproduce%20diverse%20descriptions%20aligned%20with%20the%20system-prompt.%20By%20representing%0Atechnical%20images%20as%20Diffusion-Hyperfeatures%2C%20drawing%20geometric%20correspondences%0Abetween%20them%20is%20possible.%20The%20detection%20accuracy%20of%20geometric%20points%20in%20unseen%0Asamples%20is%20improved%20by%20presenting%20multiple%20annotated%20source%20images.%20GPT-4o%20has%0Asufficient%20capabilities%20to%20generate%20accurate%20descriptions%20of%20technical%20images.%0AGrounding%20the%20generation%20only%20on%20images%20leads%20to%20diverse%20descriptions%20but%0Acauses%20hallucinations%2C%20while%20grounding%20it%20on%20categorical%20labels%20restricts%20the%0Adiversity.%20Using%20both%20as%20input%20balances%20creativity%20and%20accuracy.%20Successfully%0Ausing%20Hyperfeatures%20for%20geometric%20correspondence%20suggests%20that%20this%20approach%0Acan%20be%20used%20for%20general%20point-detection%20and%20annotation%20tasks%20in%20technical%0Aimages.%20Labeling%20such%20images%20with%20text%20descriptions%20using%20VLMs%20is%20possible%2C%20but%0Adependent%20on%20the%20models%20detection%20capabilities%2C%20careful%20prompt-engineering%20and%0Athe%20selection%20of%20input%20information.%20Applying%20foundation%20models%20in%20engineering%0Adesign%20is%20largely%20unexplored.%20We%20aim%20to%20bridge%20this%20gap%20with%20a%20dataset%20to%0Aexplore%20training%2C%20finetuning%20and%20conditioning%20DGMs%20in%20this%20field%20and%20suggesting%0Aapproaches%20to%20bootstrap%20foundation%20models%20to%20process%20technical%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17045v1&entry.124074799=Read"},
{"title": "Towards Underwater Camouflaged Object Tracking: An Experimental\n  Evaluation of SAM and SAM 2", "author": "Chunhui Zhang and Li Liu and Guanjie Huang and Hao Wen and Xi Zhou and Yanfeng Wang", "abstract": "  Over the past decade, significant progress has been made in visual object\ntracking, largely due to the availability of large-scale training datasets.\nHowever, existing tracking datasets are primarily focused on open-air\nscenarios, which greatly limits the development of object tracking in\nunderwater environments. To address this issue, we take a step forward by\nproposing the first large-scale underwater camouflaged object tracking dataset,\nnamely UW-COT. Based on the proposed dataset, this paper presents an\nexperimental evaluation of several advanced visual object tracking methods and\nthe latest advancements in image and video segmentation. Specifically, we\ncompare the performance of the Segment Anything Model (SAM) and its updated\nversion, SAM 2, in challenging underwater environments. Our findings highlight\nthe improvements in SAM 2 over SAM, demonstrating its enhanced capability to\nhandle the complexities of underwater camouflaged objects. Compared to current\nadvanced visual object tracking methods, the latest video segmentation\nfoundation model SAM 2 also exhibits significant advantages, providing valuable\ninsights into the development of more effective tracking technologies for\nunderwater scenarios. The dataset will be accessible at\n\\color{magenta}{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.\n", "link": "http://arxiv.org/abs/2409.16902v1", "date": "2024-09-25", "relevancy": 2.2331, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5663}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5604}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Underwater%20Camouflaged%20Object%20Tracking%3A%20An%20Experimental%0A%20%20Evaluation%20of%20SAM%20and%20SAM%202&body=Title%3A%20Towards%20Underwater%20Camouflaged%20Object%20Tracking%3A%20An%20Experimental%0A%20%20Evaluation%20of%20SAM%20and%20SAM%202%0AAuthor%3A%20Chunhui%20Zhang%20and%20Li%20Liu%20and%20Guanjie%20Huang%20and%20Hao%20Wen%20and%20Xi%20Zhou%20and%20Yanfeng%20Wang%0AAbstract%3A%20%20%20Over%20the%20past%20decade%2C%20significant%20progress%20has%20been%20made%20in%20visual%20object%0Atracking%2C%20largely%20due%20to%20the%20availability%20of%20large-scale%20training%20datasets.%0AHowever%2C%20existing%20tracking%20datasets%20are%20primarily%20focused%20on%20open-air%0Ascenarios%2C%20which%20greatly%20limits%20the%20development%20of%20object%20tracking%20in%0Aunderwater%20environments.%20To%20address%20this%20issue%2C%20we%20take%20a%20step%20forward%20by%0Aproposing%20the%20first%20large-scale%20underwater%20camouflaged%20object%20tracking%20dataset%2C%0Anamely%20UW-COT.%20Based%20on%20the%20proposed%20dataset%2C%20this%20paper%20presents%20an%0Aexperimental%20evaluation%20of%20several%20advanced%20visual%20object%20tracking%20methods%20and%0Athe%20latest%20advancements%20in%20image%20and%20video%20segmentation.%20Specifically%2C%20we%0Acompare%20the%20performance%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%20and%20its%20updated%0Aversion%2C%20SAM%202%2C%20in%20challenging%20underwater%20environments.%20Our%20findings%20highlight%0Athe%20improvements%20in%20SAM%202%20over%20SAM%2C%20demonstrating%20its%20enhanced%20capability%20to%0Ahandle%20the%20complexities%20of%20underwater%20camouflaged%20objects.%20Compared%20to%20current%0Aadvanced%20visual%20object%20tracking%20methods%2C%20the%20latest%20video%20segmentation%0Afoundation%20model%20SAM%202%20also%20exhibits%20significant%20advantages%2C%20providing%20valuable%0Ainsights%20into%20the%20development%20of%20more%20effective%20tracking%20technologies%20for%0Aunderwater%20scenarios.%20The%20dataset%20will%20be%20accessible%20at%0A%5Ccolor%7Bmagenta%7D%7Bhttps%3A//github.com/983632847/Awesome-Multimodal-Object-Tracking%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Underwater%2520Camouflaged%2520Object%2520Tracking%253A%2520An%2520Experimental%250A%2520%2520Evaluation%2520of%2520SAM%2520and%2520SAM%25202%26entry.906535625%3DChunhui%2520Zhang%2520and%2520Li%2520Liu%2520and%2520Guanjie%2520Huang%2520and%2520Hao%2520Wen%2520and%2520Xi%2520Zhou%2520and%2520Yanfeng%2520Wang%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520decade%252C%2520significant%2520progress%2520has%2520been%2520made%2520in%2520visual%2520object%250Atracking%252C%2520largely%2520due%2520to%2520the%2520availability%2520of%2520large-scale%2520training%2520datasets.%250AHowever%252C%2520existing%2520tracking%2520datasets%2520are%2520primarily%2520focused%2520on%2520open-air%250Ascenarios%252C%2520which%2520greatly%2520limits%2520the%2520development%2520of%2520object%2520tracking%2520in%250Aunderwater%2520environments.%2520To%2520address%2520this%2520issue%252C%2520we%2520take%2520a%2520step%2520forward%2520by%250Aproposing%2520the%2520first%2520large-scale%2520underwater%2520camouflaged%2520object%2520tracking%2520dataset%252C%250Anamely%2520UW-COT.%2520Based%2520on%2520the%2520proposed%2520dataset%252C%2520this%2520paper%2520presents%2520an%250Aexperimental%2520evaluation%2520of%2520several%2520advanced%2520visual%2520object%2520tracking%2520methods%2520and%250Athe%2520latest%2520advancements%2520in%2520image%2520and%2520video%2520segmentation.%2520Specifically%252C%2520we%250Acompare%2520the%2520performance%2520of%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520and%2520its%2520updated%250Aversion%252C%2520SAM%25202%252C%2520in%2520challenging%2520underwater%2520environments.%2520Our%2520findings%2520highlight%250Athe%2520improvements%2520in%2520SAM%25202%2520over%2520SAM%252C%2520demonstrating%2520its%2520enhanced%2520capability%2520to%250Ahandle%2520the%2520complexities%2520of%2520underwater%2520camouflaged%2520objects.%2520Compared%2520to%2520current%250Aadvanced%2520visual%2520object%2520tracking%2520methods%252C%2520the%2520latest%2520video%2520segmentation%250Afoundation%2520model%2520SAM%25202%2520also%2520exhibits%2520significant%2520advantages%252C%2520providing%2520valuable%250Ainsights%2520into%2520the%2520development%2520of%2520more%2520effective%2520tracking%2520technologies%2520for%250Aunderwater%2520scenarios.%2520The%2520dataset%2520will%2520be%2520accessible%2520at%250A%255Ccolor%257Bmagenta%257D%257Bhttps%253A//github.com/983632847/Awesome-Multimodal-Object-Tracking%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Underwater%20Camouflaged%20Object%20Tracking%3A%20An%20Experimental%0A%20%20Evaluation%20of%20SAM%20and%20SAM%202&entry.906535625=Chunhui%20Zhang%20and%20Li%20Liu%20and%20Guanjie%20Huang%20and%20Hao%20Wen%20and%20Xi%20Zhou%20and%20Yanfeng%20Wang&entry.1292438233=%20%20Over%20the%20past%20decade%2C%20significant%20progress%20has%20been%20made%20in%20visual%20object%0Atracking%2C%20largely%20due%20to%20the%20availability%20of%20large-scale%20training%20datasets.%0AHowever%2C%20existing%20tracking%20datasets%20are%20primarily%20focused%20on%20open-air%0Ascenarios%2C%20which%20greatly%20limits%20the%20development%20of%20object%20tracking%20in%0Aunderwater%20environments.%20To%20address%20this%20issue%2C%20we%20take%20a%20step%20forward%20by%0Aproposing%20the%20first%20large-scale%20underwater%20camouflaged%20object%20tracking%20dataset%2C%0Anamely%20UW-COT.%20Based%20on%20the%20proposed%20dataset%2C%20this%20paper%20presents%20an%0Aexperimental%20evaluation%20of%20several%20advanced%20visual%20object%20tracking%20methods%20and%0Athe%20latest%20advancements%20in%20image%20and%20video%20segmentation.%20Specifically%2C%20we%0Acompare%20the%20performance%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%20and%20its%20updated%0Aversion%2C%20SAM%202%2C%20in%20challenging%20underwater%20environments.%20Our%20findings%20highlight%0Athe%20improvements%20in%20SAM%202%20over%20SAM%2C%20demonstrating%20its%20enhanced%20capability%20to%0Ahandle%20the%20complexities%20of%20underwater%20camouflaged%20objects.%20Compared%20to%20current%0Aadvanced%20visual%20object%20tracking%20methods%2C%20the%20latest%20video%20segmentation%0Afoundation%20model%20SAM%202%20also%20exhibits%20significant%20advantages%2C%20providing%20valuable%0Ainsights%20into%20the%20development%20of%20more%20effective%20tracking%20technologies%20for%0Aunderwater%20scenarios.%20The%20dataset%20will%20be%20accessible%20at%0A%5Ccolor%7Bmagenta%7D%7Bhttps%3A//github.com/983632847/Awesome-Multimodal-Object-Tracking%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16902v1&entry.124074799=Read"},
{"title": "Going Beyond U-Net: Assessing Vision Transformers for Semantic\n  Segmentation in Microscopy Image Analysis", "author": "Illia Tsiporenko and Pavel Chizhov and Dmytro Fishman", "abstract": "  Segmentation is a crucial step in microscopy image analysis. Numerous\napproaches have been developed over the past years, ranging from classical\nsegmentation algorithms to advanced deep learning models. While U-Net remains\none of the most popular and well-established models for biomedical segmentation\ntasks, recently developed transformer-based models promise to enhance the\nsegmentation process of microscopy images. In this work, we assess the efficacy\nof transformers, including UNETR, the Segment Anything Model, and Swin-UPerNet,\nand compare them with the well-established U-Net model across various image\nmodalities such as electron microscopy, brightfield, histopathology, and\nphase-contrast. Our evaluation identifies several limitations in the original\nSwin Transformer model, which we address through architectural modifications to\noptimise its performance. The results demonstrate that these modifications\nimprove segmentation performance compared to the classical U-Net model and the\nunmodified Swin-UPerNet. This comparative analysis highlights the promise of\ntransformer models for advancing biomedical image segmentation. It demonstrates\nthat their efficiency and applicability can be improved with careful\nmodifications, facilitating their future use in microscopy image analysis\ntools.\n", "link": "http://arxiv.org/abs/2409.16940v1", "date": "2024-09-25", "relevancy": 2.2068, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5576}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5505}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Going%20Beyond%20U-Net%3A%20Assessing%20Vision%20Transformers%20for%20Semantic%0A%20%20Segmentation%20in%20Microscopy%20Image%20Analysis&body=Title%3A%20Going%20Beyond%20U-Net%3A%20Assessing%20Vision%20Transformers%20for%20Semantic%0A%20%20Segmentation%20in%20Microscopy%20Image%20Analysis%0AAuthor%3A%20Illia%20Tsiporenko%20and%20Pavel%20Chizhov%20and%20Dmytro%20Fishman%0AAbstract%3A%20%20%20Segmentation%20is%20a%20crucial%20step%20in%20microscopy%20image%20analysis.%20Numerous%0Aapproaches%20have%20been%20developed%20over%20the%20past%20years%2C%20ranging%20from%20classical%0Asegmentation%20algorithms%20to%20advanced%20deep%20learning%20models.%20While%20U-Net%20remains%0Aone%20of%20the%20most%20popular%20and%20well-established%20models%20for%20biomedical%20segmentation%0Atasks%2C%20recently%20developed%20transformer-based%20models%20promise%20to%20enhance%20the%0Asegmentation%20process%20of%20microscopy%20images.%20In%20this%20work%2C%20we%20assess%20the%20efficacy%0Aof%20transformers%2C%20including%20UNETR%2C%20the%20Segment%20Anything%20Model%2C%20and%20Swin-UPerNet%2C%0Aand%20compare%20them%20with%20the%20well-established%20U-Net%20model%20across%20various%20image%0Amodalities%20such%20as%20electron%20microscopy%2C%20brightfield%2C%20histopathology%2C%20and%0Aphase-contrast.%20Our%20evaluation%20identifies%20several%20limitations%20in%20the%20original%0ASwin%20Transformer%20model%2C%20which%20we%20address%20through%20architectural%20modifications%20to%0Aoptimise%20its%20performance.%20The%20results%20demonstrate%20that%20these%20modifications%0Aimprove%20segmentation%20performance%20compared%20to%20the%20classical%20U-Net%20model%20and%20the%0Aunmodified%20Swin-UPerNet.%20This%20comparative%20analysis%20highlights%20the%20promise%20of%0Atransformer%20models%20for%20advancing%20biomedical%20image%20segmentation.%20It%20demonstrates%0Athat%20their%20efficiency%20and%20applicability%20can%20be%20improved%20with%20careful%0Amodifications%2C%20facilitating%20their%20future%20use%20in%20microscopy%20image%20analysis%0Atools.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16940v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGoing%2520Beyond%2520U-Net%253A%2520Assessing%2520Vision%2520Transformers%2520for%2520Semantic%250A%2520%2520Segmentation%2520in%2520Microscopy%2520Image%2520Analysis%26entry.906535625%3DIllia%2520Tsiporenko%2520and%2520Pavel%2520Chizhov%2520and%2520Dmytro%2520Fishman%26entry.1292438233%3D%2520%2520Segmentation%2520is%2520a%2520crucial%2520step%2520in%2520microscopy%2520image%2520analysis.%2520Numerous%250Aapproaches%2520have%2520been%2520developed%2520over%2520the%2520past%2520years%252C%2520ranging%2520from%2520classical%250Asegmentation%2520algorithms%2520to%2520advanced%2520deep%2520learning%2520models.%2520While%2520U-Net%2520remains%250Aone%2520of%2520the%2520most%2520popular%2520and%2520well-established%2520models%2520for%2520biomedical%2520segmentation%250Atasks%252C%2520recently%2520developed%2520transformer-based%2520models%2520promise%2520to%2520enhance%2520the%250Asegmentation%2520process%2520of%2520microscopy%2520images.%2520In%2520this%2520work%252C%2520we%2520assess%2520the%2520efficacy%250Aof%2520transformers%252C%2520including%2520UNETR%252C%2520the%2520Segment%2520Anything%2520Model%252C%2520and%2520Swin-UPerNet%252C%250Aand%2520compare%2520them%2520with%2520the%2520well-established%2520U-Net%2520model%2520across%2520various%2520image%250Amodalities%2520such%2520as%2520electron%2520microscopy%252C%2520brightfield%252C%2520histopathology%252C%2520and%250Aphase-contrast.%2520Our%2520evaluation%2520identifies%2520several%2520limitations%2520in%2520the%2520original%250ASwin%2520Transformer%2520model%252C%2520which%2520we%2520address%2520through%2520architectural%2520modifications%2520to%250Aoptimise%2520its%2520performance.%2520The%2520results%2520demonstrate%2520that%2520these%2520modifications%250Aimprove%2520segmentation%2520performance%2520compared%2520to%2520the%2520classical%2520U-Net%2520model%2520and%2520the%250Aunmodified%2520Swin-UPerNet.%2520This%2520comparative%2520analysis%2520highlights%2520the%2520promise%2520of%250Atransformer%2520models%2520for%2520advancing%2520biomedical%2520image%2520segmentation.%2520It%2520demonstrates%250Athat%2520their%2520efficiency%2520and%2520applicability%2520can%2520be%2520improved%2520with%2520careful%250Amodifications%252C%2520facilitating%2520their%2520future%2520use%2520in%2520microscopy%2520image%2520analysis%250Atools.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16940v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Going%20Beyond%20U-Net%3A%20Assessing%20Vision%20Transformers%20for%20Semantic%0A%20%20Segmentation%20in%20Microscopy%20Image%20Analysis&entry.906535625=Illia%20Tsiporenko%20and%20Pavel%20Chizhov%20and%20Dmytro%20Fishman&entry.1292438233=%20%20Segmentation%20is%20a%20crucial%20step%20in%20microscopy%20image%20analysis.%20Numerous%0Aapproaches%20have%20been%20developed%20over%20the%20past%20years%2C%20ranging%20from%20classical%0Asegmentation%20algorithms%20to%20advanced%20deep%20learning%20models.%20While%20U-Net%20remains%0Aone%20of%20the%20most%20popular%20and%20well-established%20models%20for%20biomedical%20segmentation%0Atasks%2C%20recently%20developed%20transformer-based%20models%20promise%20to%20enhance%20the%0Asegmentation%20process%20of%20microscopy%20images.%20In%20this%20work%2C%20we%20assess%20the%20efficacy%0Aof%20transformers%2C%20including%20UNETR%2C%20the%20Segment%20Anything%20Model%2C%20and%20Swin-UPerNet%2C%0Aand%20compare%20them%20with%20the%20well-established%20U-Net%20model%20across%20various%20image%0Amodalities%20such%20as%20electron%20microscopy%2C%20brightfield%2C%20histopathology%2C%20and%0Aphase-contrast.%20Our%20evaluation%20identifies%20several%20limitations%20in%20the%20original%0ASwin%20Transformer%20model%2C%20which%20we%20address%20through%20architectural%20modifications%20to%0Aoptimise%20its%20performance.%20The%20results%20demonstrate%20that%20these%20modifications%0Aimprove%20segmentation%20performance%20compared%20to%20the%20classical%20U-Net%20model%20and%20the%0Aunmodified%20Swin-UPerNet.%20This%20comparative%20analysis%20highlights%20the%20promise%20of%0Atransformer%20models%20for%20advancing%20biomedical%20image%20segmentation.%20It%20demonstrates%0Athat%20their%20efficiency%20and%20applicability%20can%20be%20improved%20with%20careful%0Amodifications%2C%20facilitating%20their%20future%20use%20in%20microscopy%20image%20analysis%0Atools.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16940v1&entry.124074799=Read"},
{"title": "LingoQA: Video Question Answering for Autonomous Driving", "author": "Ana-Maria Marcu and Long Chen and Jan H\u00fcnermann and Alice Karnsund and Benoit Hanotte and Prajwal Chidananda and Saurabh Nair and Vijay Badrinarayanan and Alex Kendall and Jamie Shotton and Elahe Arani and Oleg Sinavski", "abstract": "  We introduce LingoQA, a novel dataset and benchmark for visual question\nanswering in autonomous driving. The dataset contains 28K unique short video\nscenarios, and 419K annotations. Evaluating state-of-the-art vision-language\nmodels on our benchmark shows that their performance is below human\ncapabilities, with GPT-4V responding truthfully to 59.6% of the questions\ncompared to 96.6% for humans. For evaluation, we propose a truthfulness\nclassifier, called Lingo-Judge, that achieves a 0.95 Spearman correlation\ncoefficient to human evaluations, surpassing existing techniques like METEOR,\nBLEU, CIDEr, and GPT-4. We establish a baseline vision-language model and run\nextensive ablation studies to understand its performance. We release our\ndataset and benchmark https://github.com/wayveai/LingoQA as an evaluation\nplatform for vision-language models in autonomous driving.\n", "link": "http://arxiv.org/abs/2312.14115v3", "date": "2024-09-25", "relevancy": 2.1961, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5585}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5585}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LingoQA%3A%20Video%20Question%20Answering%20for%20Autonomous%20Driving&body=Title%3A%20LingoQA%3A%20Video%20Question%20Answering%20for%20Autonomous%20Driving%0AAuthor%3A%20Ana-Maria%20Marcu%20and%20Long%20Chen%20and%20Jan%20H%C3%BCnermann%20and%20Alice%20Karnsund%20and%20Benoit%20Hanotte%20and%20Prajwal%20Chidananda%20and%20Saurabh%20Nair%20and%20Vijay%20Badrinarayanan%20and%20Alex%20Kendall%20and%20Jamie%20Shotton%20and%20Elahe%20Arani%20and%20Oleg%20Sinavski%0AAbstract%3A%20%20%20We%20introduce%20LingoQA%2C%20a%20novel%20dataset%20and%20benchmark%20for%20visual%20question%0Aanswering%20in%20autonomous%20driving.%20The%20dataset%20contains%2028K%20unique%20short%20video%0Ascenarios%2C%20and%20419K%20annotations.%20Evaluating%20state-of-the-art%20vision-language%0Amodels%20on%20our%20benchmark%20shows%20that%20their%20performance%20is%20below%20human%0Acapabilities%2C%20with%20GPT-4V%20responding%20truthfully%20to%2059.6%25%20of%20the%20questions%0Acompared%20to%2096.6%25%20for%20humans.%20For%20evaluation%2C%20we%20propose%20a%20truthfulness%0Aclassifier%2C%20called%20Lingo-Judge%2C%20that%20achieves%20a%200.95%20Spearman%20correlation%0Acoefficient%20to%20human%20evaluations%2C%20surpassing%20existing%20techniques%20like%20METEOR%2C%0ABLEU%2C%20CIDEr%2C%20and%20GPT-4.%20We%20establish%20a%20baseline%20vision-language%20model%20and%20run%0Aextensive%20ablation%20studies%20to%20understand%20its%20performance.%20We%20release%20our%0Adataset%20and%20benchmark%20https%3A//github.com/wayveai/LingoQA%20as%20an%20evaluation%0Aplatform%20for%20vision-language%20models%20in%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.14115v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLingoQA%253A%2520Video%2520Question%2520Answering%2520for%2520Autonomous%2520Driving%26entry.906535625%3DAna-Maria%2520Marcu%2520and%2520Long%2520Chen%2520and%2520Jan%2520H%25C3%25BCnermann%2520and%2520Alice%2520Karnsund%2520and%2520Benoit%2520Hanotte%2520and%2520Prajwal%2520Chidananda%2520and%2520Saurabh%2520Nair%2520and%2520Vijay%2520Badrinarayanan%2520and%2520Alex%2520Kendall%2520and%2520Jamie%2520Shotton%2520and%2520Elahe%2520Arani%2520and%2520Oleg%2520Sinavski%26entry.1292438233%3D%2520%2520We%2520introduce%2520LingoQA%252C%2520a%2520novel%2520dataset%2520and%2520benchmark%2520for%2520visual%2520question%250Aanswering%2520in%2520autonomous%2520driving.%2520The%2520dataset%2520contains%252028K%2520unique%2520short%2520video%250Ascenarios%252C%2520and%2520419K%2520annotations.%2520Evaluating%2520state-of-the-art%2520vision-language%250Amodels%2520on%2520our%2520benchmark%2520shows%2520that%2520their%2520performance%2520is%2520below%2520human%250Acapabilities%252C%2520with%2520GPT-4V%2520responding%2520truthfully%2520to%252059.6%2525%2520of%2520the%2520questions%250Acompared%2520to%252096.6%2525%2520for%2520humans.%2520For%2520evaluation%252C%2520we%2520propose%2520a%2520truthfulness%250Aclassifier%252C%2520called%2520Lingo-Judge%252C%2520that%2520achieves%2520a%25200.95%2520Spearman%2520correlation%250Acoefficient%2520to%2520human%2520evaluations%252C%2520surpassing%2520existing%2520techniques%2520like%2520METEOR%252C%250ABLEU%252C%2520CIDEr%252C%2520and%2520GPT-4.%2520We%2520establish%2520a%2520baseline%2520vision-language%2520model%2520and%2520run%250Aextensive%2520ablation%2520studies%2520to%2520understand%2520its%2520performance.%2520We%2520release%2520our%250Adataset%2520and%2520benchmark%2520https%253A//github.com/wayveai/LingoQA%2520as%2520an%2520evaluation%250Aplatform%2520for%2520vision-language%2520models%2520in%2520autonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.14115v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LingoQA%3A%20Video%20Question%20Answering%20for%20Autonomous%20Driving&entry.906535625=Ana-Maria%20Marcu%20and%20Long%20Chen%20and%20Jan%20H%C3%BCnermann%20and%20Alice%20Karnsund%20and%20Benoit%20Hanotte%20and%20Prajwal%20Chidananda%20and%20Saurabh%20Nair%20and%20Vijay%20Badrinarayanan%20and%20Alex%20Kendall%20and%20Jamie%20Shotton%20and%20Elahe%20Arani%20and%20Oleg%20Sinavski&entry.1292438233=%20%20We%20introduce%20LingoQA%2C%20a%20novel%20dataset%20and%20benchmark%20for%20visual%20question%0Aanswering%20in%20autonomous%20driving.%20The%20dataset%20contains%2028K%20unique%20short%20video%0Ascenarios%2C%20and%20419K%20annotations.%20Evaluating%20state-of-the-art%20vision-language%0Amodels%20on%20our%20benchmark%20shows%20that%20their%20performance%20is%20below%20human%0Acapabilities%2C%20with%20GPT-4V%20responding%20truthfully%20to%2059.6%25%20of%20the%20questions%0Acompared%20to%2096.6%25%20for%20humans.%20For%20evaluation%2C%20we%20propose%20a%20truthfulness%0Aclassifier%2C%20called%20Lingo-Judge%2C%20that%20achieves%20a%200.95%20Spearman%20correlation%0Acoefficient%20to%20human%20evaluations%2C%20surpassing%20existing%20techniques%20like%20METEOR%2C%0ABLEU%2C%20CIDEr%2C%20and%20GPT-4.%20We%20establish%20a%20baseline%20vision-language%20model%20and%20run%0Aextensive%20ablation%20studies%20to%20understand%20its%20performance.%20We%20release%20our%0Adataset%20and%20benchmark%20https%3A//github.com/wayveai/LingoQA%20as%20an%20evaluation%0Aplatform%20for%20vision-language%20models%20in%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.14115v3&entry.124074799=Read"},
{"title": "Scalable Ensemble Diversification for OOD Generalization and Detection", "author": "Alexander Rubinstein and Luca Scimeca and Damien Teney and Seong Joon Oh", "abstract": "  Training a diverse ensemble of models has several practical applications such\nas providing candidates for model selection with better out-of-distribution\n(OOD) generalization, and enabling the detection of OOD samples via Bayesian\nprinciples. An existing approach to diverse ensemble training encourages the\nmodels to disagree on provided OOD samples. However, the approach is\ncomputationally expensive and it requires well-separated ID and OOD examples,\nsuch that it has only been demonstrated in small-scale settings.\n  $\\textbf{Method.}$ This work presents a method for Scalable Ensemble\nDiversification (SED) applicable to large-scale settings (e.g. ImageNet) that\ndoes not require OOD samples. Instead, SED identifies hard training samples on\nthe fly and encourages the ensemble members to disagree on these. To improve\nscaling, we show how to avoid the expensive computations in existing methods of\nexhaustive pairwise disagreements across models.\n  $\\textbf{Results.}$ We evaluate the benefits of diversification with\nexperiments on ImageNet. First, for OOD generalization, we observe large\nbenefits from the diversification in multiple settings including output-space\n(classical) ensembles and weight-space ensembles (model soups). Second, for OOD\ndetection, we turn the diversity of ensemble hypotheses into a novel\nuncertainty score estimator that surpasses a large number of OOD detection\nbaselines.\n  Code is available here:\nhttps://github.com/AlexanderRubinstein/diverse-universe-public.\n", "link": "http://arxiv.org/abs/2409.16797v1", "date": "2024-09-25", "relevancy": 2.1791, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5731}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.543}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Ensemble%20Diversification%20for%20OOD%20Generalization%20and%20Detection&body=Title%3A%20Scalable%20Ensemble%20Diversification%20for%20OOD%20Generalization%20and%20Detection%0AAuthor%3A%20Alexander%20Rubinstein%20and%20Luca%20Scimeca%20and%20Damien%20Teney%20and%20Seong%20Joon%20Oh%0AAbstract%3A%20%20%20Training%20a%20diverse%20ensemble%20of%20models%20has%20several%20practical%20applications%20such%0Aas%20providing%20candidates%20for%20model%20selection%20with%20better%20out-of-distribution%0A%28OOD%29%20generalization%2C%20and%20enabling%20the%20detection%20of%20OOD%20samples%20via%20Bayesian%0Aprinciples.%20An%20existing%20approach%20to%20diverse%20ensemble%20training%20encourages%20the%0Amodels%20to%20disagree%20on%20provided%20OOD%20samples.%20However%2C%20the%20approach%20is%0Acomputationally%20expensive%20and%20it%20requires%20well-separated%20ID%20and%20OOD%20examples%2C%0Asuch%20that%20it%20has%20only%20been%20demonstrated%20in%20small-scale%20settings.%0A%20%20%24%5Ctextbf%7BMethod.%7D%24%20This%20work%20presents%20a%20method%20for%20Scalable%20Ensemble%0ADiversification%20%28SED%29%20applicable%20to%20large-scale%20settings%20%28e.g.%20ImageNet%29%20that%0Adoes%20not%20require%20OOD%20samples.%20Instead%2C%20SED%20identifies%20hard%20training%20samples%20on%0Athe%20fly%20and%20encourages%20the%20ensemble%20members%20to%20disagree%20on%20these.%20To%20improve%0Ascaling%2C%20we%20show%20how%20to%20avoid%20the%20expensive%20computations%20in%20existing%20methods%20of%0Aexhaustive%20pairwise%20disagreements%20across%20models.%0A%20%20%24%5Ctextbf%7BResults.%7D%24%20We%20evaluate%20the%20benefits%20of%20diversification%20with%0Aexperiments%20on%20ImageNet.%20First%2C%20for%20OOD%20generalization%2C%20we%20observe%20large%0Abenefits%20from%20the%20diversification%20in%20multiple%20settings%20including%20output-space%0A%28classical%29%20ensembles%20and%20weight-space%20ensembles%20%28model%20soups%29.%20Second%2C%20for%20OOD%0Adetection%2C%20we%20turn%20the%20diversity%20of%20ensemble%20hypotheses%20into%20a%20novel%0Auncertainty%20score%20estimator%20that%20surpasses%20a%20large%20number%20of%20OOD%20detection%0Abaselines.%0A%20%20Code%20is%20available%20here%3A%0Ahttps%3A//github.com/AlexanderRubinstein/diverse-universe-public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16797v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Ensemble%2520Diversification%2520for%2520OOD%2520Generalization%2520and%2520Detection%26entry.906535625%3DAlexander%2520Rubinstein%2520and%2520Luca%2520Scimeca%2520and%2520Damien%2520Teney%2520and%2520Seong%2520Joon%2520Oh%26entry.1292438233%3D%2520%2520Training%2520a%2520diverse%2520ensemble%2520of%2520models%2520has%2520several%2520practical%2520applications%2520such%250Aas%2520providing%2520candidates%2520for%2520model%2520selection%2520with%2520better%2520out-of-distribution%250A%2528OOD%2529%2520generalization%252C%2520and%2520enabling%2520the%2520detection%2520of%2520OOD%2520samples%2520via%2520Bayesian%250Aprinciples.%2520An%2520existing%2520approach%2520to%2520diverse%2520ensemble%2520training%2520encourages%2520the%250Amodels%2520to%2520disagree%2520on%2520provided%2520OOD%2520samples.%2520However%252C%2520the%2520approach%2520is%250Acomputationally%2520expensive%2520and%2520it%2520requires%2520well-separated%2520ID%2520and%2520OOD%2520examples%252C%250Asuch%2520that%2520it%2520has%2520only%2520been%2520demonstrated%2520in%2520small-scale%2520settings.%250A%2520%2520%2524%255Ctextbf%257BMethod.%257D%2524%2520This%2520work%2520presents%2520a%2520method%2520for%2520Scalable%2520Ensemble%250ADiversification%2520%2528SED%2529%2520applicable%2520to%2520large-scale%2520settings%2520%2528e.g.%2520ImageNet%2529%2520that%250Adoes%2520not%2520require%2520OOD%2520samples.%2520Instead%252C%2520SED%2520identifies%2520hard%2520training%2520samples%2520on%250Athe%2520fly%2520and%2520encourages%2520the%2520ensemble%2520members%2520to%2520disagree%2520on%2520these.%2520To%2520improve%250Ascaling%252C%2520we%2520show%2520how%2520to%2520avoid%2520the%2520expensive%2520computations%2520in%2520existing%2520methods%2520of%250Aexhaustive%2520pairwise%2520disagreements%2520across%2520models.%250A%2520%2520%2524%255Ctextbf%257BResults.%257D%2524%2520We%2520evaluate%2520the%2520benefits%2520of%2520diversification%2520with%250Aexperiments%2520on%2520ImageNet.%2520First%252C%2520for%2520OOD%2520generalization%252C%2520we%2520observe%2520large%250Abenefits%2520from%2520the%2520diversification%2520in%2520multiple%2520settings%2520including%2520output-space%250A%2528classical%2529%2520ensembles%2520and%2520weight-space%2520ensembles%2520%2528model%2520soups%2529.%2520Second%252C%2520for%2520OOD%250Adetection%252C%2520we%2520turn%2520the%2520diversity%2520of%2520ensemble%2520hypotheses%2520into%2520a%2520novel%250Auncertainty%2520score%2520estimator%2520that%2520surpasses%2520a%2520large%2520number%2520of%2520OOD%2520detection%250Abaselines.%250A%2520%2520Code%2520is%2520available%2520here%253A%250Ahttps%253A//github.com/AlexanderRubinstein/diverse-universe-public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16797v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Ensemble%20Diversification%20for%20OOD%20Generalization%20and%20Detection&entry.906535625=Alexander%20Rubinstein%20and%20Luca%20Scimeca%20and%20Damien%20Teney%20and%20Seong%20Joon%20Oh&entry.1292438233=%20%20Training%20a%20diverse%20ensemble%20of%20models%20has%20several%20practical%20applications%20such%0Aas%20providing%20candidates%20for%20model%20selection%20with%20better%20out-of-distribution%0A%28OOD%29%20generalization%2C%20and%20enabling%20the%20detection%20of%20OOD%20samples%20via%20Bayesian%0Aprinciples.%20An%20existing%20approach%20to%20diverse%20ensemble%20training%20encourages%20the%0Amodels%20to%20disagree%20on%20provided%20OOD%20samples.%20However%2C%20the%20approach%20is%0Acomputationally%20expensive%20and%20it%20requires%20well-separated%20ID%20and%20OOD%20examples%2C%0Asuch%20that%20it%20has%20only%20been%20demonstrated%20in%20small-scale%20settings.%0A%20%20%24%5Ctextbf%7BMethod.%7D%24%20This%20work%20presents%20a%20method%20for%20Scalable%20Ensemble%0ADiversification%20%28SED%29%20applicable%20to%20large-scale%20settings%20%28e.g.%20ImageNet%29%20that%0Adoes%20not%20require%20OOD%20samples.%20Instead%2C%20SED%20identifies%20hard%20training%20samples%20on%0Athe%20fly%20and%20encourages%20the%20ensemble%20members%20to%20disagree%20on%20these.%20To%20improve%0Ascaling%2C%20we%20show%20how%20to%20avoid%20the%20expensive%20computations%20in%20existing%20methods%20of%0Aexhaustive%20pairwise%20disagreements%20across%20models.%0A%20%20%24%5Ctextbf%7BResults.%7D%24%20We%20evaluate%20the%20benefits%20of%20diversification%20with%0Aexperiments%20on%20ImageNet.%20First%2C%20for%20OOD%20generalization%2C%20we%20observe%20large%0Abenefits%20from%20the%20diversification%20in%20multiple%20settings%20including%20output-space%0A%28classical%29%20ensembles%20and%20weight-space%20ensembles%20%28model%20soups%29.%20Second%2C%20for%20OOD%0Adetection%2C%20we%20turn%20the%20diversity%20of%20ensemble%20hypotheses%20into%20a%20novel%0Auncertainty%20score%20estimator%20that%20surpasses%20a%20large%20number%20of%20OOD%20detection%0Abaselines.%0A%20%20Code%20is%20available%20here%3A%0Ahttps%3A//github.com/AlexanderRubinstein/diverse-universe-public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16797v1&entry.124074799=Read"},
{"title": "EventHDR: from Event to High-Speed HDR Videos and Beyond", "author": "Yunhao Zou and Ying Fu and Tsuyoshi Takatani and Yinqiang Zheng", "abstract": "  Event cameras are innovative neuromorphic sensors that asynchronously capture\nthe scene dynamics. Due to the event-triggering mechanism, such cameras record\nevent streams with much shorter response latency and higher intensity\nsensitivity compared to conventional cameras. On the basis of these features,\nprevious works have attempted to reconstruct high dynamic range (HDR) videos\nfrom events, but have either suffered from unrealistic artifacts or failed to\nprovide sufficiently high frame rates. In this paper, we present a recurrent\nconvolutional neural network that reconstruct high-speed HDR videos from event\nsequences, with a key frame guidance to prevent potential error accumulation\ncaused by the sparse event data. Additionally, to address the problem of\nseverely limited real dataset, we develop a new optical system to collect a\nreal-world dataset with paired high-speed HDR videos and event streams,\nfacilitating future research in this field. Our dataset provides the first real\npaired dataset for event-to-HDR reconstruction, avoiding potential inaccuracies\nfrom simulation strategies. Experimental results demonstrate that our method\ncan generate high-quality, high-speed HDR videos. We further explore the\npotential of our work in cross-camera reconstruction and downstream computer\nvision tasks, including object detection, panoramic segmentation, optical flow\nestimation, and monocular depth estimation under HDR scenarios.\n", "link": "http://arxiv.org/abs/2409.17029v1", "date": "2024-09-25", "relevancy": 2.1691, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5715}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5316}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EventHDR%3A%20from%20Event%20to%20High-Speed%20HDR%20Videos%20and%20Beyond&body=Title%3A%20EventHDR%3A%20from%20Event%20to%20High-Speed%20HDR%20Videos%20and%20Beyond%0AAuthor%3A%20Yunhao%20Zou%20and%20Ying%20Fu%20and%20Tsuyoshi%20Takatani%20and%20Yinqiang%20Zheng%0AAbstract%3A%20%20%20Event%20cameras%20are%20innovative%20neuromorphic%20sensors%20that%20asynchronously%20capture%0Athe%20scene%20dynamics.%20Due%20to%20the%20event-triggering%20mechanism%2C%20such%20cameras%20record%0Aevent%20streams%20with%20much%20shorter%20response%20latency%20and%20higher%20intensity%0Asensitivity%20compared%20to%20conventional%20cameras.%20On%20the%20basis%20of%20these%20features%2C%0Aprevious%20works%20have%20attempted%20to%20reconstruct%20high%20dynamic%20range%20%28HDR%29%20videos%0Afrom%20events%2C%20but%20have%20either%20suffered%20from%20unrealistic%20artifacts%20or%20failed%20to%0Aprovide%20sufficiently%20high%20frame%20rates.%20In%20this%20paper%2C%20we%20present%20a%20recurrent%0Aconvolutional%20neural%20network%20that%20reconstruct%20high-speed%20HDR%20videos%20from%20event%0Asequences%2C%20with%20a%20key%20frame%20guidance%20to%20prevent%20potential%20error%20accumulation%0Acaused%20by%20the%20sparse%20event%20data.%20Additionally%2C%20to%20address%20the%20problem%20of%0Aseverely%20limited%20real%20dataset%2C%20we%20develop%20a%20new%20optical%20system%20to%20collect%20a%0Areal-world%20dataset%20with%20paired%20high-speed%20HDR%20videos%20and%20event%20streams%2C%0Afacilitating%20future%20research%20in%20this%20field.%20Our%20dataset%20provides%20the%20first%20real%0Apaired%20dataset%20for%20event-to-HDR%20reconstruction%2C%20avoiding%20potential%20inaccuracies%0Afrom%20simulation%20strategies.%20Experimental%20results%20demonstrate%20that%20our%20method%0Acan%20generate%20high-quality%2C%20high-speed%20HDR%20videos.%20We%20further%20explore%20the%0Apotential%20of%20our%20work%20in%20cross-camera%20reconstruction%20and%20downstream%20computer%0Avision%20tasks%2C%20including%20object%20detection%2C%20panoramic%20segmentation%2C%20optical%20flow%0Aestimation%2C%20and%20monocular%20depth%20estimation%20under%20HDR%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17029v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEventHDR%253A%2520from%2520Event%2520to%2520High-Speed%2520HDR%2520Videos%2520and%2520Beyond%26entry.906535625%3DYunhao%2520Zou%2520and%2520Ying%2520Fu%2520and%2520Tsuyoshi%2520Takatani%2520and%2520Yinqiang%2520Zheng%26entry.1292438233%3D%2520%2520Event%2520cameras%2520are%2520innovative%2520neuromorphic%2520sensors%2520that%2520asynchronously%2520capture%250Athe%2520scene%2520dynamics.%2520Due%2520to%2520the%2520event-triggering%2520mechanism%252C%2520such%2520cameras%2520record%250Aevent%2520streams%2520with%2520much%2520shorter%2520response%2520latency%2520and%2520higher%2520intensity%250Asensitivity%2520compared%2520to%2520conventional%2520cameras.%2520On%2520the%2520basis%2520of%2520these%2520features%252C%250Aprevious%2520works%2520have%2520attempted%2520to%2520reconstruct%2520high%2520dynamic%2520range%2520%2528HDR%2529%2520videos%250Afrom%2520events%252C%2520but%2520have%2520either%2520suffered%2520from%2520unrealistic%2520artifacts%2520or%2520failed%2520to%250Aprovide%2520sufficiently%2520high%2520frame%2520rates.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520recurrent%250Aconvolutional%2520neural%2520network%2520that%2520reconstruct%2520high-speed%2520HDR%2520videos%2520from%2520event%250Asequences%252C%2520with%2520a%2520key%2520frame%2520guidance%2520to%2520prevent%2520potential%2520error%2520accumulation%250Acaused%2520by%2520the%2520sparse%2520event%2520data.%2520Additionally%252C%2520to%2520address%2520the%2520problem%2520of%250Aseverely%2520limited%2520real%2520dataset%252C%2520we%2520develop%2520a%2520new%2520optical%2520system%2520to%2520collect%2520a%250Areal-world%2520dataset%2520with%2520paired%2520high-speed%2520HDR%2520videos%2520and%2520event%2520streams%252C%250Afacilitating%2520future%2520research%2520in%2520this%2520field.%2520Our%2520dataset%2520provides%2520the%2520first%2520real%250Apaired%2520dataset%2520for%2520event-to-HDR%2520reconstruction%252C%2520avoiding%2520potential%2520inaccuracies%250Afrom%2520simulation%2520strategies.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%250Acan%2520generate%2520high-quality%252C%2520high-speed%2520HDR%2520videos.%2520We%2520further%2520explore%2520the%250Apotential%2520of%2520our%2520work%2520in%2520cross-camera%2520reconstruction%2520and%2520downstream%2520computer%250Avision%2520tasks%252C%2520including%2520object%2520detection%252C%2520panoramic%2520segmentation%252C%2520optical%2520flow%250Aestimation%252C%2520and%2520monocular%2520depth%2520estimation%2520under%2520HDR%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17029v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EventHDR%3A%20from%20Event%20to%20High-Speed%20HDR%20Videos%20and%20Beyond&entry.906535625=Yunhao%20Zou%20and%20Ying%20Fu%20and%20Tsuyoshi%20Takatani%20and%20Yinqiang%20Zheng&entry.1292438233=%20%20Event%20cameras%20are%20innovative%20neuromorphic%20sensors%20that%20asynchronously%20capture%0Athe%20scene%20dynamics.%20Due%20to%20the%20event-triggering%20mechanism%2C%20such%20cameras%20record%0Aevent%20streams%20with%20much%20shorter%20response%20latency%20and%20higher%20intensity%0Asensitivity%20compared%20to%20conventional%20cameras.%20On%20the%20basis%20of%20these%20features%2C%0Aprevious%20works%20have%20attempted%20to%20reconstruct%20high%20dynamic%20range%20%28HDR%29%20videos%0Afrom%20events%2C%20but%20have%20either%20suffered%20from%20unrealistic%20artifacts%20or%20failed%20to%0Aprovide%20sufficiently%20high%20frame%20rates.%20In%20this%20paper%2C%20we%20present%20a%20recurrent%0Aconvolutional%20neural%20network%20that%20reconstruct%20high-speed%20HDR%20videos%20from%20event%0Asequences%2C%20with%20a%20key%20frame%20guidance%20to%20prevent%20potential%20error%20accumulation%0Acaused%20by%20the%20sparse%20event%20data.%20Additionally%2C%20to%20address%20the%20problem%20of%0Aseverely%20limited%20real%20dataset%2C%20we%20develop%20a%20new%20optical%20system%20to%20collect%20a%0Areal-world%20dataset%20with%20paired%20high-speed%20HDR%20videos%20and%20event%20streams%2C%0Afacilitating%20future%20research%20in%20this%20field.%20Our%20dataset%20provides%20the%20first%20real%0Apaired%20dataset%20for%20event-to-HDR%20reconstruction%2C%20avoiding%20potential%20inaccuracies%0Afrom%20simulation%20strategies.%20Experimental%20results%20demonstrate%20that%20our%20method%0Acan%20generate%20high-quality%2C%20high-speed%20HDR%20videos.%20We%20further%20explore%20the%0Apotential%20of%20our%20work%20in%20cross-camera%20reconstruction%20and%20downstream%20computer%0Avision%20tasks%2C%20including%20object%20detection%2C%20panoramic%20segmentation%2C%20optical%20flow%0Aestimation%2C%20and%20monocular%20depth%20estimation%20under%20HDR%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17029v1&entry.124074799=Read"},
{"title": "WasteGAN: Data Augmentation for Robotic Waste Sorting through Generative\n  Adversarial Networks", "author": "Alberto Bacchin and Leonardo Barcellona and Matteo Terreran and Stefano Ghidoni and Emanuele Menegatti and Takuya Kiyokawa", "abstract": "  Robotic waste sorting poses significant challenges in both perception and\nmanipulation, given the extreme variability of objects that should be\nrecognized on a cluttered conveyor belt. While deep learning has proven\neffective in solving complex tasks, the necessity for extensive data collection\nand labeling limits its applicability in real-world scenarios like waste\nsorting. To tackle this issue, we introduce a data augmentation method based on\na novel GAN architecture called wasteGAN. The proposed method allows to\nincrease the performance of semantic segmentation models, starting from a very\nlimited bunch of labeled examples, such as few as 100. The key innovations of\nwasteGAN include a novel loss function, a novel activation function, and a\nlarger generator block. Overall, such innovations helps the network to learn\nfrom limited number of examples and synthesize data that better mirrors\nreal-world distributions. We then leverage the higher-quality segmentation\nmasks predicted from models trained on the wasteGAN synthetic data to compute\nsemantic-aware grasp poses, enabling a robotic arm to effectively recognizing\ncontaminants and separating waste in a real-world scenario. Through\ncomprehensive evaluation encompassing dataset-based assessments and real-world\nexperiments, our methodology demonstrated promising potential for robotic waste\nsorting, yielding performance gains of up to 5.8\\% in picking contaminants. The\nproject page is available at https://github.com/bach05/wasteGAN.git\n", "link": "http://arxiv.org/abs/2409.16999v1", "date": "2024-09-25", "relevancy": 2.1689, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5606}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5326}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WasteGAN%3A%20Data%20Augmentation%20for%20Robotic%20Waste%20Sorting%20through%20Generative%0A%20%20Adversarial%20Networks&body=Title%3A%20WasteGAN%3A%20Data%20Augmentation%20for%20Robotic%20Waste%20Sorting%20through%20Generative%0A%20%20Adversarial%20Networks%0AAuthor%3A%20Alberto%20Bacchin%20and%20Leonardo%20Barcellona%20and%20Matteo%20Terreran%20and%20Stefano%20Ghidoni%20and%20Emanuele%20Menegatti%20and%20Takuya%20Kiyokawa%0AAbstract%3A%20%20%20Robotic%20waste%20sorting%20poses%20significant%20challenges%20in%20both%20perception%20and%0Amanipulation%2C%20given%20the%20extreme%20variability%20of%20objects%20that%20should%20be%0Arecognized%20on%20a%20cluttered%20conveyor%20belt.%20While%20deep%20learning%20has%20proven%0Aeffective%20in%20solving%20complex%20tasks%2C%20the%20necessity%20for%20extensive%20data%20collection%0Aand%20labeling%20limits%20its%20applicability%20in%20real-world%20scenarios%20like%20waste%0Asorting.%20To%20tackle%20this%20issue%2C%20we%20introduce%20a%20data%20augmentation%20method%20based%20on%0Aa%20novel%20GAN%20architecture%20called%20wasteGAN.%20The%20proposed%20method%20allows%20to%0Aincrease%20the%20performance%20of%20semantic%20segmentation%20models%2C%20starting%20from%20a%20very%0Alimited%20bunch%20of%20labeled%20examples%2C%20such%20as%20few%20as%20100.%20The%20key%20innovations%20of%0AwasteGAN%20include%20a%20novel%20loss%20function%2C%20a%20novel%20activation%20function%2C%20and%20a%0Alarger%20generator%20block.%20Overall%2C%20such%20innovations%20helps%20the%20network%20to%20learn%0Afrom%20limited%20number%20of%20examples%20and%20synthesize%20data%20that%20better%20mirrors%0Areal-world%20distributions.%20We%20then%20leverage%20the%20higher-quality%20segmentation%0Amasks%20predicted%20from%20models%20trained%20on%20the%20wasteGAN%20synthetic%20data%20to%20compute%0Asemantic-aware%20grasp%20poses%2C%20enabling%20a%20robotic%20arm%20to%20effectively%20recognizing%0Acontaminants%20and%20separating%20waste%20in%20a%20real-world%20scenario.%20Through%0Acomprehensive%20evaluation%20encompassing%20dataset-based%20assessments%20and%20real-world%0Aexperiments%2C%20our%20methodology%20demonstrated%20promising%20potential%20for%20robotic%20waste%0Asorting%2C%20yielding%20performance%20gains%20of%20up%20to%205.8%5C%25%20in%20picking%20contaminants.%20The%0Aproject%20page%20is%20available%20at%20https%3A//github.com/bach05/wasteGAN.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWasteGAN%253A%2520Data%2520Augmentation%2520for%2520Robotic%2520Waste%2520Sorting%2520through%2520Generative%250A%2520%2520Adversarial%2520Networks%26entry.906535625%3DAlberto%2520Bacchin%2520and%2520Leonardo%2520Barcellona%2520and%2520Matteo%2520Terreran%2520and%2520Stefano%2520Ghidoni%2520and%2520Emanuele%2520Menegatti%2520and%2520Takuya%2520Kiyokawa%26entry.1292438233%3D%2520%2520Robotic%2520waste%2520sorting%2520poses%2520significant%2520challenges%2520in%2520both%2520perception%2520and%250Amanipulation%252C%2520given%2520the%2520extreme%2520variability%2520of%2520objects%2520that%2520should%2520be%250Arecognized%2520on%2520a%2520cluttered%2520conveyor%2520belt.%2520While%2520deep%2520learning%2520has%2520proven%250Aeffective%2520in%2520solving%2520complex%2520tasks%252C%2520the%2520necessity%2520for%2520extensive%2520data%2520collection%250Aand%2520labeling%2520limits%2520its%2520applicability%2520in%2520real-world%2520scenarios%2520like%2520waste%250Asorting.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520introduce%2520a%2520data%2520augmentation%2520method%2520based%2520on%250Aa%2520novel%2520GAN%2520architecture%2520called%2520wasteGAN.%2520The%2520proposed%2520method%2520allows%2520to%250Aincrease%2520the%2520performance%2520of%2520semantic%2520segmentation%2520models%252C%2520starting%2520from%2520a%2520very%250Alimited%2520bunch%2520of%2520labeled%2520examples%252C%2520such%2520as%2520few%2520as%2520100.%2520The%2520key%2520innovations%2520of%250AwasteGAN%2520include%2520a%2520novel%2520loss%2520function%252C%2520a%2520novel%2520activation%2520function%252C%2520and%2520a%250Alarger%2520generator%2520block.%2520Overall%252C%2520such%2520innovations%2520helps%2520the%2520network%2520to%2520learn%250Afrom%2520limited%2520number%2520of%2520examples%2520and%2520synthesize%2520data%2520that%2520better%2520mirrors%250Areal-world%2520distributions.%2520We%2520then%2520leverage%2520the%2520higher-quality%2520segmentation%250Amasks%2520predicted%2520from%2520models%2520trained%2520on%2520the%2520wasteGAN%2520synthetic%2520data%2520to%2520compute%250Asemantic-aware%2520grasp%2520poses%252C%2520enabling%2520a%2520robotic%2520arm%2520to%2520effectively%2520recognizing%250Acontaminants%2520and%2520separating%2520waste%2520in%2520a%2520real-world%2520scenario.%2520Through%250Acomprehensive%2520evaluation%2520encompassing%2520dataset-based%2520assessments%2520and%2520real-world%250Aexperiments%252C%2520our%2520methodology%2520demonstrated%2520promising%2520potential%2520for%2520robotic%2520waste%250Asorting%252C%2520yielding%2520performance%2520gains%2520of%2520up%2520to%25205.8%255C%2525%2520in%2520picking%2520contaminants.%2520The%250Aproject%2520page%2520is%2520available%2520at%2520https%253A//github.com/bach05/wasteGAN.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WasteGAN%3A%20Data%20Augmentation%20for%20Robotic%20Waste%20Sorting%20through%20Generative%0A%20%20Adversarial%20Networks&entry.906535625=Alberto%20Bacchin%20and%20Leonardo%20Barcellona%20and%20Matteo%20Terreran%20and%20Stefano%20Ghidoni%20and%20Emanuele%20Menegatti%20and%20Takuya%20Kiyokawa&entry.1292438233=%20%20Robotic%20waste%20sorting%20poses%20significant%20challenges%20in%20both%20perception%20and%0Amanipulation%2C%20given%20the%20extreme%20variability%20of%20objects%20that%20should%20be%0Arecognized%20on%20a%20cluttered%20conveyor%20belt.%20While%20deep%20learning%20has%20proven%0Aeffective%20in%20solving%20complex%20tasks%2C%20the%20necessity%20for%20extensive%20data%20collection%0Aand%20labeling%20limits%20its%20applicability%20in%20real-world%20scenarios%20like%20waste%0Asorting.%20To%20tackle%20this%20issue%2C%20we%20introduce%20a%20data%20augmentation%20method%20based%20on%0Aa%20novel%20GAN%20architecture%20called%20wasteGAN.%20The%20proposed%20method%20allows%20to%0Aincrease%20the%20performance%20of%20semantic%20segmentation%20models%2C%20starting%20from%20a%20very%0Alimited%20bunch%20of%20labeled%20examples%2C%20such%20as%20few%20as%20100.%20The%20key%20innovations%20of%0AwasteGAN%20include%20a%20novel%20loss%20function%2C%20a%20novel%20activation%20function%2C%20and%20a%0Alarger%20generator%20block.%20Overall%2C%20such%20innovations%20helps%20the%20network%20to%20learn%0Afrom%20limited%20number%20of%20examples%20and%20synthesize%20data%20that%20better%20mirrors%0Areal-world%20distributions.%20We%20then%20leverage%20the%20higher-quality%20segmentation%0Amasks%20predicted%20from%20models%20trained%20on%20the%20wasteGAN%20synthetic%20data%20to%20compute%0Asemantic-aware%20grasp%20poses%2C%20enabling%20a%20robotic%20arm%20to%20effectively%20recognizing%0Acontaminants%20and%20separating%20waste%20in%20a%20real-world%20scenario.%20Through%0Acomprehensive%20evaluation%20encompassing%20dataset-based%20assessments%20and%20real-world%0Aexperiments%2C%20our%20methodology%20demonstrated%20promising%20potential%20for%20robotic%20waste%0Asorting%2C%20yielding%20performance%20gains%20of%20up%20to%205.8%5C%25%20in%20picking%20contaminants.%20The%0Aproject%20page%20is%20available%20at%20https%3A//github.com/bach05/wasteGAN.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16999v1&entry.124074799=Read"},
{"title": "Towards General Text-guided Image Synthesis for Customized Multimodal\n  Brain MRI Generation", "author": "Yulin Wang and Honglin Xiong and Kaicong Sun and Shuwei Bai and Ling Dai and Zhongxiang Ding and Jiameng Liu and Qian Wang and Qian Liu and Dinggang Shen", "abstract": "  Multimodal brain magnetic resonance (MR) imaging is indispensable in\nneuroscience and neurology. However, due to the accessibility of MRI scanners\nand their lengthy acquisition time, multimodal MR images are not commonly\navailable. Current MR image synthesis approaches are typically trained on\nindependent datasets for specific tasks, leading to suboptimal performance when\napplied to novel datasets and tasks. Here, we present TUMSyn, a Text-guided\nUniversal MR image Synthesis generalist model, which can flexibly generate\nbrain MR images with demanded imaging metadata from routinely acquired scans\nguided by text prompts. To ensure TUMSyn's image synthesis precision,\nversatility, and generalizability, we first construct a brain MR database\ncomprising 31,407 3D images with 7 MRI modalities from 13 centers. We then\npre-train an MRI-specific text encoder using contrastive learning to\neffectively control MR image synthesis based on text prompts. Extensive\nexperiments on diverse datasets and physician assessments indicate that TUMSyn\ncan generate clinically meaningful MR images with specified imaging metadata in\nsupervised and zero-shot scenarios. Therefore, TUMSyn can be utilized along\nwith acquired MR scan(s) to facilitate large-scale MRI-based screening and\ndiagnosis of brain diseases.\n", "link": "http://arxiv.org/abs/2409.16818v1", "date": "2024-09-25", "relevancy": 2.1652, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5501}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5364}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20General%20Text-guided%20Image%20Synthesis%20for%20Customized%20Multimodal%0A%20%20Brain%20MRI%20Generation&body=Title%3A%20Towards%20General%20Text-guided%20Image%20Synthesis%20for%20Customized%20Multimodal%0A%20%20Brain%20MRI%20Generation%0AAuthor%3A%20Yulin%20Wang%20and%20Honglin%20Xiong%20and%20Kaicong%20Sun%20and%20Shuwei%20Bai%20and%20Ling%20Dai%20and%20Zhongxiang%20Ding%20and%20Jiameng%20Liu%20and%20Qian%20Wang%20and%20Qian%20Liu%20and%20Dinggang%20Shen%0AAbstract%3A%20%20%20Multimodal%20brain%20magnetic%20resonance%20%28MR%29%20imaging%20is%20indispensable%20in%0Aneuroscience%20and%20neurology.%20However%2C%20due%20to%20the%20accessibility%20of%20MRI%20scanners%0Aand%20their%20lengthy%20acquisition%20time%2C%20multimodal%20MR%20images%20are%20not%20commonly%0Aavailable.%20Current%20MR%20image%20synthesis%20approaches%20are%20typically%20trained%20on%0Aindependent%20datasets%20for%20specific%20tasks%2C%20leading%20to%20suboptimal%20performance%20when%0Aapplied%20to%20novel%20datasets%20and%20tasks.%20Here%2C%20we%20present%20TUMSyn%2C%20a%20Text-guided%0AUniversal%20MR%20image%20Synthesis%20generalist%20model%2C%20which%20can%20flexibly%20generate%0Abrain%20MR%20images%20with%20demanded%20imaging%20metadata%20from%20routinely%20acquired%20scans%0Aguided%20by%20text%20prompts.%20To%20ensure%20TUMSyn%27s%20image%20synthesis%20precision%2C%0Aversatility%2C%20and%20generalizability%2C%20we%20first%20construct%20a%20brain%20MR%20database%0Acomprising%2031%2C407%203D%20images%20with%207%20MRI%20modalities%20from%2013%20centers.%20We%20then%0Apre-train%20an%20MRI-specific%20text%20encoder%20using%20contrastive%20learning%20to%0Aeffectively%20control%20MR%20image%20synthesis%20based%20on%20text%20prompts.%20Extensive%0Aexperiments%20on%20diverse%20datasets%20and%20physician%20assessments%20indicate%20that%20TUMSyn%0Acan%20generate%20clinically%20meaningful%20MR%20images%20with%20specified%20imaging%20metadata%20in%0Asupervised%20and%20zero-shot%20scenarios.%20Therefore%2C%20TUMSyn%20can%20be%20utilized%20along%0Awith%20acquired%20MR%20scan%28s%29%20to%20facilitate%20large-scale%20MRI-based%20screening%20and%0Adiagnosis%20of%20brain%20diseases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520General%2520Text-guided%2520Image%2520Synthesis%2520for%2520Customized%2520Multimodal%250A%2520%2520Brain%2520MRI%2520Generation%26entry.906535625%3DYulin%2520Wang%2520and%2520Honglin%2520Xiong%2520and%2520Kaicong%2520Sun%2520and%2520Shuwei%2520Bai%2520and%2520Ling%2520Dai%2520and%2520Zhongxiang%2520Ding%2520and%2520Jiameng%2520Liu%2520and%2520Qian%2520Wang%2520and%2520Qian%2520Liu%2520and%2520Dinggang%2520Shen%26entry.1292438233%3D%2520%2520Multimodal%2520brain%2520magnetic%2520resonance%2520%2528MR%2529%2520imaging%2520is%2520indispensable%2520in%250Aneuroscience%2520and%2520neurology.%2520However%252C%2520due%2520to%2520the%2520accessibility%2520of%2520MRI%2520scanners%250Aand%2520their%2520lengthy%2520acquisition%2520time%252C%2520multimodal%2520MR%2520images%2520are%2520not%2520commonly%250Aavailable.%2520Current%2520MR%2520image%2520synthesis%2520approaches%2520are%2520typically%2520trained%2520on%250Aindependent%2520datasets%2520for%2520specific%2520tasks%252C%2520leading%2520to%2520suboptimal%2520performance%2520when%250Aapplied%2520to%2520novel%2520datasets%2520and%2520tasks.%2520Here%252C%2520we%2520present%2520TUMSyn%252C%2520a%2520Text-guided%250AUniversal%2520MR%2520image%2520Synthesis%2520generalist%2520model%252C%2520which%2520can%2520flexibly%2520generate%250Abrain%2520MR%2520images%2520with%2520demanded%2520imaging%2520metadata%2520from%2520routinely%2520acquired%2520scans%250Aguided%2520by%2520text%2520prompts.%2520To%2520ensure%2520TUMSyn%2527s%2520image%2520synthesis%2520precision%252C%250Aversatility%252C%2520and%2520generalizability%252C%2520we%2520first%2520construct%2520a%2520brain%2520MR%2520database%250Acomprising%252031%252C407%25203D%2520images%2520with%25207%2520MRI%2520modalities%2520from%252013%2520centers.%2520We%2520then%250Apre-train%2520an%2520MRI-specific%2520text%2520encoder%2520using%2520contrastive%2520learning%2520to%250Aeffectively%2520control%2520MR%2520image%2520synthesis%2520based%2520on%2520text%2520prompts.%2520Extensive%250Aexperiments%2520on%2520diverse%2520datasets%2520and%2520physician%2520assessments%2520indicate%2520that%2520TUMSyn%250Acan%2520generate%2520clinically%2520meaningful%2520MR%2520images%2520with%2520specified%2520imaging%2520metadata%2520in%250Asupervised%2520and%2520zero-shot%2520scenarios.%2520Therefore%252C%2520TUMSyn%2520can%2520be%2520utilized%2520along%250Awith%2520acquired%2520MR%2520scan%2528s%2529%2520to%2520facilitate%2520large-scale%2520MRI-based%2520screening%2520and%250Adiagnosis%2520of%2520brain%2520diseases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20General%20Text-guided%20Image%20Synthesis%20for%20Customized%20Multimodal%0A%20%20Brain%20MRI%20Generation&entry.906535625=Yulin%20Wang%20and%20Honglin%20Xiong%20and%20Kaicong%20Sun%20and%20Shuwei%20Bai%20and%20Ling%20Dai%20and%20Zhongxiang%20Ding%20and%20Jiameng%20Liu%20and%20Qian%20Wang%20and%20Qian%20Liu%20and%20Dinggang%20Shen&entry.1292438233=%20%20Multimodal%20brain%20magnetic%20resonance%20%28MR%29%20imaging%20is%20indispensable%20in%0Aneuroscience%20and%20neurology.%20However%2C%20due%20to%20the%20accessibility%20of%20MRI%20scanners%0Aand%20their%20lengthy%20acquisition%20time%2C%20multimodal%20MR%20images%20are%20not%20commonly%0Aavailable.%20Current%20MR%20image%20synthesis%20approaches%20are%20typically%20trained%20on%0Aindependent%20datasets%20for%20specific%20tasks%2C%20leading%20to%20suboptimal%20performance%20when%0Aapplied%20to%20novel%20datasets%20and%20tasks.%20Here%2C%20we%20present%20TUMSyn%2C%20a%20Text-guided%0AUniversal%20MR%20image%20Synthesis%20generalist%20model%2C%20which%20can%20flexibly%20generate%0Abrain%20MR%20images%20with%20demanded%20imaging%20metadata%20from%20routinely%20acquired%20scans%0Aguided%20by%20text%20prompts.%20To%20ensure%20TUMSyn%27s%20image%20synthesis%20precision%2C%0Aversatility%2C%20and%20generalizability%2C%20we%20first%20construct%20a%20brain%20MR%20database%0Acomprising%2031%2C407%203D%20images%20with%207%20MRI%20modalities%20from%2013%20centers.%20We%20then%0Apre-train%20an%20MRI-specific%20text%20encoder%20using%20contrastive%20learning%20to%0Aeffectively%20control%20MR%20image%20synthesis%20based%20on%20text%20prompts.%20Extensive%0Aexperiments%20on%20diverse%20datasets%20and%20physician%20assessments%20indicate%20that%20TUMSyn%0Acan%20generate%20clinically%20meaningful%20MR%20images%20with%20specified%20imaging%20metadata%20in%0Asupervised%20and%20zero-shot%20scenarios.%20Therefore%2C%20TUMSyn%20can%20be%20utilized%20along%0Awith%20acquired%20MR%20scan%28s%29%20to%20facilitate%20large-scale%20MRI-based%20screening%20and%0Adiagnosis%20of%20brain%20diseases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16818v1&entry.124074799=Read"},
{"title": "Programming Every Example: Lifting Pre-training Data Quality like\n  Experts at Scale", "author": "Fan Zhou and Zengzhi Wang and Qian Liu and Junlong Li and Pengfei Liu", "abstract": "  Large language model pre-training has traditionally relied on human experts\nto craft heuristics for improving the corpora quality, resulting in numerous\nrules developed to date. However, these rules lack the flexibility to address\nthe unique characteristics of individual example effectively. Meanwhile,\napplying tailored rules to every example is impractical for human experts. In\nthis paper, we demonstrate that even small language models, with as few as 0.3B\nparameters, can exhibit substantial data refining capabilities comparable to\nthose of human experts. We introduce Programming Every Example (ProX), a novel\nframework that treats data refinement as a programming task, enabling models to\nrefine corpora by generating and executing fine-grained operations, such as\nstring normalization, for each individual example at scale. Experimental\nresults show that models pre-trained on ProX-curated data outperform either\noriginal data or data filtered by other selection methods by more than 2%\nacross various downstream benchmarks. Its effectiveness spans various model\nsizes and pre-training corpora, including C4, RedPajama-V2, and FineWeb.\nFurthermore, ProX exhibits significant potential in domain-specific continual\npre-training: without domain specific design, models trained on OpenWebMath\nrefined by ProX outperform human-crafted rule-based methods, improving average\naccuracy by 7.6% over Mistral-7B, with 14.6% for Llama-2-7B and 20.3% for\nCodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7B\ntrained on 200B tokens. Further analysis highlights that ProX significantly\nsaves training FLOPs, offering a promising path for efficient LLM\npre-training.We are open-sourcing ProX with >100B corpus, models, and sharing\nall training and implementation details for reproducible research and future\ninnovation. Code: https://github.com/GAIR-NLP/ProX\n", "link": "http://arxiv.org/abs/2409.17115v1", "date": "2024-09-25", "relevancy": 2.1514, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5433}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5433}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Programming%20Every%20Example%3A%20Lifting%20Pre-training%20Data%20Quality%20like%0A%20%20Experts%20at%20Scale&body=Title%3A%20Programming%20Every%20Example%3A%20Lifting%20Pre-training%20Data%20Quality%20like%0A%20%20Experts%20at%20Scale%0AAuthor%3A%20Fan%20Zhou%20and%20Zengzhi%20Wang%20and%20Qian%20Liu%20and%20Junlong%20Li%20and%20Pengfei%20Liu%0AAbstract%3A%20%20%20Large%20language%20model%20pre-training%20has%20traditionally%20relied%20on%20human%20experts%0Ato%20craft%20heuristics%20for%20improving%20the%20corpora%20quality%2C%20resulting%20in%20numerous%0Arules%20developed%20to%20date.%20However%2C%20these%20rules%20lack%20the%20flexibility%20to%20address%0Athe%20unique%20characteristics%20of%20individual%20example%20effectively.%20Meanwhile%2C%0Aapplying%20tailored%20rules%20to%20every%20example%20is%20impractical%20for%20human%20experts.%20In%0Athis%20paper%2C%20we%20demonstrate%20that%20even%20small%20language%20models%2C%20with%20as%20few%20as%200.3B%0Aparameters%2C%20can%20exhibit%20substantial%20data%20refining%20capabilities%20comparable%20to%0Athose%20of%20human%20experts.%20We%20introduce%20Programming%20Every%20Example%20%28ProX%29%2C%20a%20novel%0Aframework%20that%20treats%20data%20refinement%20as%20a%20programming%20task%2C%20enabling%20models%20to%0Arefine%20corpora%20by%20generating%20and%20executing%20fine-grained%20operations%2C%20such%20as%0Astring%20normalization%2C%20for%20each%20individual%20example%20at%20scale.%20Experimental%0Aresults%20show%20that%20models%20pre-trained%20on%20ProX-curated%20data%20outperform%20either%0Aoriginal%20data%20or%20data%20filtered%20by%20other%20selection%20methods%20by%20more%20than%202%25%0Aacross%20various%20downstream%20benchmarks.%20Its%20effectiveness%20spans%20various%20model%0Asizes%20and%20pre-training%20corpora%2C%20including%20C4%2C%20RedPajama-V2%2C%20and%20FineWeb.%0AFurthermore%2C%20ProX%20exhibits%20significant%20potential%20in%20domain-specific%20continual%0Apre-training%3A%20without%20domain%20specific%20design%2C%20models%20trained%20on%20OpenWebMath%0Arefined%20by%20ProX%20outperform%20human-crafted%20rule-based%20methods%2C%20improving%20average%0Aaccuracy%20by%207.6%25%20over%20Mistral-7B%2C%20with%2014.6%25%20for%20Llama-2-7B%20and%2020.3%25%20for%0ACodeLlama-7B%2C%20all%20within%2010B%20tokens%20to%20be%20comparable%20to%20models%20like%20Llemma-7B%0Atrained%20on%20200B%20tokens.%20Further%20analysis%20highlights%20that%20ProX%20significantly%0Asaves%20training%20FLOPs%2C%20offering%20a%20promising%20path%20for%20efficient%20LLM%0Apre-training.We%20are%20open-sourcing%20ProX%20with%20%3E100B%20corpus%2C%20models%2C%20and%20sharing%0Aall%20training%20and%20implementation%20details%20for%20reproducible%20research%20and%20future%0Ainnovation.%20Code%3A%20https%3A//github.com/GAIR-NLP/ProX%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17115v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgramming%2520Every%2520Example%253A%2520Lifting%2520Pre-training%2520Data%2520Quality%2520like%250A%2520%2520Experts%2520at%2520Scale%26entry.906535625%3DFan%2520Zhou%2520and%2520Zengzhi%2520Wang%2520and%2520Qian%2520Liu%2520and%2520Junlong%2520Li%2520and%2520Pengfei%2520Liu%26entry.1292438233%3D%2520%2520Large%2520language%2520model%2520pre-training%2520has%2520traditionally%2520relied%2520on%2520human%2520experts%250Ato%2520craft%2520heuristics%2520for%2520improving%2520the%2520corpora%2520quality%252C%2520resulting%2520in%2520numerous%250Arules%2520developed%2520to%2520date.%2520However%252C%2520these%2520rules%2520lack%2520the%2520flexibility%2520to%2520address%250Athe%2520unique%2520characteristics%2520of%2520individual%2520example%2520effectively.%2520Meanwhile%252C%250Aapplying%2520tailored%2520rules%2520to%2520every%2520example%2520is%2520impractical%2520for%2520human%2520experts.%2520In%250Athis%2520paper%252C%2520we%2520demonstrate%2520that%2520even%2520small%2520language%2520models%252C%2520with%2520as%2520few%2520as%25200.3B%250Aparameters%252C%2520can%2520exhibit%2520substantial%2520data%2520refining%2520capabilities%2520comparable%2520to%250Athose%2520of%2520human%2520experts.%2520We%2520introduce%2520Programming%2520Every%2520Example%2520%2528ProX%2529%252C%2520a%2520novel%250Aframework%2520that%2520treats%2520data%2520refinement%2520as%2520a%2520programming%2520task%252C%2520enabling%2520models%2520to%250Arefine%2520corpora%2520by%2520generating%2520and%2520executing%2520fine-grained%2520operations%252C%2520such%2520as%250Astring%2520normalization%252C%2520for%2520each%2520individual%2520example%2520at%2520scale.%2520Experimental%250Aresults%2520show%2520that%2520models%2520pre-trained%2520on%2520ProX-curated%2520data%2520outperform%2520either%250Aoriginal%2520data%2520or%2520data%2520filtered%2520by%2520other%2520selection%2520methods%2520by%2520more%2520than%25202%2525%250Aacross%2520various%2520downstream%2520benchmarks.%2520Its%2520effectiveness%2520spans%2520various%2520model%250Asizes%2520and%2520pre-training%2520corpora%252C%2520including%2520C4%252C%2520RedPajama-V2%252C%2520and%2520FineWeb.%250AFurthermore%252C%2520ProX%2520exhibits%2520significant%2520potential%2520in%2520domain-specific%2520continual%250Apre-training%253A%2520without%2520domain%2520specific%2520design%252C%2520models%2520trained%2520on%2520OpenWebMath%250Arefined%2520by%2520ProX%2520outperform%2520human-crafted%2520rule-based%2520methods%252C%2520improving%2520average%250Aaccuracy%2520by%25207.6%2525%2520over%2520Mistral-7B%252C%2520with%252014.6%2525%2520for%2520Llama-2-7B%2520and%252020.3%2525%2520for%250ACodeLlama-7B%252C%2520all%2520within%252010B%2520tokens%2520to%2520be%2520comparable%2520to%2520models%2520like%2520Llemma-7B%250Atrained%2520on%2520200B%2520tokens.%2520Further%2520analysis%2520highlights%2520that%2520ProX%2520significantly%250Asaves%2520training%2520FLOPs%252C%2520offering%2520a%2520promising%2520path%2520for%2520efficient%2520LLM%250Apre-training.We%2520are%2520open-sourcing%2520ProX%2520with%2520%253E100B%2520corpus%252C%2520models%252C%2520and%2520sharing%250Aall%2520training%2520and%2520implementation%2520details%2520for%2520reproducible%2520research%2520and%2520future%250Ainnovation.%2520Code%253A%2520https%253A//github.com/GAIR-NLP/ProX%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17115v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Programming%20Every%20Example%3A%20Lifting%20Pre-training%20Data%20Quality%20like%0A%20%20Experts%20at%20Scale&entry.906535625=Fan%20Zhou%20and%20Zengzhi%20Wang%20and%20Qian%20Liu%20and%20Junlong%20Li%20and%20Pengfei%20Liu&entry.1292438233=%20%20Large%20language%20model%20pre-training%20has%20traditionally%20relied%20on%20human%20experts%0Ato%20craft%20heuristics%20for%20improving%20the%20corpora%20quality%2C%20resulting%20in%20numerous%0Arules%20developed%20to%20date.%20However%2C%20these%20rules%20lack%20the%20flexibility%20to%20address%0Athe%20unique%20characteristics%20of%20individual%20example%20effectively.%20Meanwhile%2C%0Aapplying%20tailored%20rules%20to%20every%20example%20is%20impractical%20for%20human%20experts.%20In%0Athis%20paper%2C%20we%20demonstrate%20that%20even%20small%20language%20models%2C%20with%20as%20few%20as%200.3B%0Aparameters%2C%20can%20exhibit%20substantial%20data%20refining%20capabilities%20comparable%20to%0Athose%20of%20human%20experts.%20We%20introduce%20Programming%20Every%20Example%20%28ProX%29%2C%20a%20novel%0Aframework%20that%20treats%20data%20refinement%20as%20a%20programming%20task%2C%20enabling%20models%20to%0Arefine%20corpora%20by%20generating%20and%20executing%20fine-grained%20operations%2C%20such%20as%0Astring%20normalization%2C%20for%20each%20individual%20example%20at%20scale.%20Experimental%0Aresults%20show%20that%20models%20pre-trained%20on%20ProX-curated%20data%20outperform%20either%0Aoriginal%20data%20or%20data%20filtered%20by%20other%20selection%20methods%20by%20more%20than%202%25%0Aacross%20various%20downstream%20benchmarks.%20Its%20effectiveness%20spans%20various%20model%0Asizes%20and%20pre-training%20corpora%2C%20including%20C4%2C%20RedPajama-V2%2C%20and%20FineWeb.%0AFurthermore%2C%20ProX%20exhibits%20significant%20potential%20in%20domain-specific%20continual%0Apre-training%3A%20without%20domain%20specific%20design%2C%20models%20trained%20on%20OpenWebMath%0Arefined%20by%20ProX%20outperform%20human-crafted%20rule-based%20methods%2C%20improving%20average%0Aaccuracy%20by%207.6%25%20over%20Mistral-7B%2C%20with%2014.6%25%20for%20Llama-2-7B%20and%2020.3%25%20for%0ACodeLlama-7B%2C%20all%20within%2010B%20tokens%20to%20be%20comparable%20to%20models%20like%20Llemma-7B%0Atrained%20on%20200B%20tokens.%20Further%20analysis%20highlights%20that%20ProX%20significantly%0Asaves%20training%20FLOPs%2C%20offering%20a%20promising%20path%20for%20efficient%20LLM%0Apre-training.We%20are%20open-sourcing%20ProX%20with%20%3E100B%20corpus%2C%20models%2C%20and%20sharing%0Aall%20training%20and%20implementation%20details%20for%20reproducible%20research%20and%20future%0Ainnovation.%20Code%3A%20https%3A//github.com/GAIR-NLP/ProX%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17115v1&entry.124074799=Read"},
{"title": "Optimal starting point for time series forecasting", "author": "Yiming Zhong and Yinuo Ren and Guangyao Cao and Feng Li and Haobo Qi", "abstract": "  Recent advances on time series forecasting mainly focus on improving the\nforecasting models themselves. However, managing the length of the input data\ncan also significantly enhance prediction performance. In this paper, we\nintroduce a novel approach called Optimal Starting Point Time Series Forecast\n(OSP-TSP) to capture the intrinsic characteristics of time series data. By\nadjusting the sequence length via leveraging the XGBoost and LightGBM models,\nthe proposed approach can determine optimal starting point (OSP) of the time\nseries and thus enhance the prediction performances. The performances of the\nOSP-TSP approach are then evaluated across various frequencies on the M4\ndataset and other real-world datasets. Empirical results indicate that\npredictions based on the OSP-TSP approach consistently outperform those using\nthe complete dataset. Moreover, recognizing the necessity of sufficient data to\neffectively train models for OSP identification, we further propose targeted\nsolutions to address the issue of data insufficiency.\n", "link": "http://arxiv.org/abs/2409.16843v1", "date": "2024-09-25", "relevancy": 2.1391, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.46}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4134}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20starting%20point%20for%20time%20series%20forecasting&body=Title%3A%20Optimal%20starting%20point%20for%20time%20series%20forecasting%0AAuthor%3A%20Yiming%20Zhong%20and%20Yinuo%20Ren%20and%20Guangyao%20Cao%20and%20Feng%20Li%20and%20Haobo%20Qi%0AAbstract%3A%20%20%20Recent%20advances%20on%20time%20series%20forecasting%20mainly%20focus%20on%20improving%20the%0Aforecasting%20models%20themselves.%20However%2C%20managing%20the%20length%20of%20the%20input%20data%0Acan%20also%20significantly%20enhance%20prediction%20performance.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20approach%20called%20Optimal%20Starting%20Point%20Time%20Series%20Forecast%0A%28OSP-TSP%29%20to%20capture%20the%20intrinsic%20characteristics%20of%20time%20series%20data.%20By%0Aadjusting%20the%20sequence%20length%20via%20leveraging%20the%20XGBoost%20and%20LightGBM%20models%2C%0Athe%20proposed%20approach%20can%20determine%20optimal%20starting%20point%20%28OSP%29%20of%20the%20time%0Aseries%20and%20thus%20enhance%20the%20prediction%20performances.%20The%20performances%20of%20the%0AOSP-TSP%20approach%20are%20then%20evaluated%20across%20various%20frequencies%20on%20the%20M4%0Adataset%20and%20other%20real-world%20datasets.%20Empirical%20results%20indicate%20that%0Apredictions%20based%20on%20the%20OSP-TSP%20approach%20consistently%20outperform%20those%20using%0Athe%20complete%20dataset.%20Moreover%2C%20recognizing%20the%20necessity%20of%20sufficient%20data%20to%0Aeffectively%20train%20models%20for%20OSP%20identification%2C%20we%20further%20propose%20targeted%0Asolutions%20to%20address%20the%20issue%20of%20data%20insufficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16843v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520starting%2520point%2520for%2520time%2520series%2520forecasting%26entry.906535625%3DYiming%2520Zhong%2520and%2520Yinuo%2520Ren%2520and%2520Guangyao%2520Cao%2520and%2520Feng%2520Li%2520and%2520Haobo%2520Qi%26entry.1292438233%3D%2520%2520Recent%2520advances%2520on%2520time%2520series%2520forecasting%2520mainly%2520focus%2520on%2520improving%2520the%250Aforecasting%2520models%2520themselves.%2520However%252C%2520managing%2520the%2520length%2520of%2520the%2520input%2520data%250Acan%2520also%2520significantly%2520enhance%2520prediction%2520performance.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520novel%2520approach%2520called%2520Optimal%2520Starting%2520Point%2520Time%2520Series%2520Forecast%250A%2528OSP-TSP%2529%2520to%2520capture%2520the%2520intrinsic%2520characteristics%2520of%2520time%2520series%2520data.%2520By%250Aadjusting%2520the%2520sequence%2520length%2520via%2520leveraging%2520the%2520XGBoost%2520and%2520LightGBM%2520models%252C%250Athe%2520proposed%2520approach%2520can%2520determine%2520optimal%2520starting%2520point%2520%2528OSP%2529%2520of%2520the%2520time%250Aseries%2520and%2520thus%2520enhance%2520the%2520prediction%2520performances.%2520The%2520performances%2520of%2520the%250AOSP-TSP%2520approach%2520are%2520then%2520evaluated%2520across%2520various%2520frequencies%2520on%2520the%2520M4%250Adataset%2520and%2520other%2520real-world%2520datasets.%2520Empirical%2520results%2520indicate%2520that%250Apredictions%2520based%2520on%2520the%2520OSP-TSP%2520approach%2520consistently%2520outperform%2520those%2520using%250Athe%2520complete%2520dataset.%2520Moreover%252C%2520recognizing%2520the%2520necessity%2520of%2520sufficient%2520data%2520to%250Aeffectively%2520train%2520models%2520for%2520OSP%2520identification%252C%2520we%2520further%2520propose%2520targeted%250Asolutions%2520to%2520address%2520the%2520issue%2520of%2520data%2520insufficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16843v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20starting%20point%20for%20time%20series%20forecasting&entry.906535625=Yiming%20Zhong%20and%20Yinuo%20Ren%20and%20Guangyao%20Cao%20and%20Feng%20Li%20and%20Haobo%20Qi&entry.1292438233=%20%20Recent%20advances%20on%20time%20series%20forecasting%20mainly%20focus%20on%20improving%20the%0Aforecasting%20models%20themselves.%20However%2C%20managing%20the%20length%20of%20the%20input%20data%0Acan%20also%20significantly%20enhance%20prediction%20performance.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20approach%20called%20Optimal%20Starting%20Point%20Time%20Series%20Forecast%0A%28OSP-TSP%29%20to%20capture%20the%20intrinsic%20characteristics%20of%20time%20series%20data.%20By%0Aadjusting%20the%20sequence%20length%20via%20leveraging%20the%20XGBoost%20and%20LightGBM%20models%2C%0Athe%20proposed%20approach%20can%20determine%20optimal%20starting%20point%20%28OSP%29%20of%20the%20time%0Aseries%20and%20thus%20enhance%20the%20prediction%20performances.%20The%20performances%20of%20the%0AOSP-TSP%20approach%20are%20then%20evaluated%20across%20various%20frequencies%20on%20the%20M4%0Adataset%20and%20other%20real-world%20datasets.%20Empirical%20results%20indicate%20that%0Apredictions%20based%20on%20the%20OSP-TSP%20approach%20consistently%20outperform%20those%20using%0Athe%20complete%20dataset.%20Moreover%2C%20recognizing%20the%20necessity%20of%20sufficient%20data%20to%0Aeffectively%20train%20models%20for%20OSP%20identification%2C%20we%20further%20propose%20targeted%0Asolutions%20to%20address%20the%20issue%20of%20data%20insufficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16843v1&entry.124074799=Read"},
{"title": "PACE: marrying generalization in PArameter-efficient fine-tuning with\n  Consistency rEgularization", "author": "Yao Ni and Shan Zhang and Piotr Koniusz", "abstract": "  Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained vision\ntransformers to downstream tasks. However, the optimization for tasks\nperformance often comes at the cost of generalizability in fine-tuned models.\nTo address this issue, we theoretically connect smaller weight gradient norms\nduring training and larger datasets to the improved model generalization.\nMotivated by this connection, we propose reducing gradient norms for enhanced\ngeneralization and aligning fine-tuned model with the pre-trained counterpart\nto retain knowledge from large-scale pre-training data. Yet, naive alignment\ndoes not guarantee gradient reduction and can potentially cause gradient\nexplosion, complicating efforts to manage gradients. To address such issues, we\npropose PACE, marrying generalization of PArameter-efficient fine-tuning with\nConsistency rEgularization. We perturb features learned from the adapter with\nthe multiplicative noise and ensure the fine-tuned model remains consistent for\nsame sample under different perturbations. Theoretical analysis shows that PACE\nnot only implicitly regularizes gradients for enhanced generalization, but also\nimplicitly aligns the fine-tuned and pre-trained models to retain knowledge.\nExperimental evidence supports our theories. PACE outperforms existing PEFT\nmethods in four visual adaptation tasks: VTAB-1k, FGVC, few-shot learning and\ndomain adaptation. Code will be available at\nhttps://github.com/MaxwellYaoNi/PACE\n", "link": "http://arxiv.org/abs/2409.17137v1", "date": "2024-09-25", "relevancy": 2.1194, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5402}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5258}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PACE%3A%20marrying%20generalization%20in%20PArameter-efficient%20fine-tuning%20with%0A%20%20Consistency%20rEgularization&body=Title%3A%20PACE%3A%20marrying%20generalization%20in%20PArameter-efficient%20fine-tuning%20with%0A%20%20Consistency%20rEgularization%0AAuthor%3A%20Yao%20Ni%20and%20Shan%20Zhang%20and%20Piotr%20Koniusz%0AAbstract%3A%20%20%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20effectively%20adapts%20pre-trained%20vision%0Atransformers%20to%20downstream%20tasks.%20However%2C%20the%20optimization%20for%20tasks%0Aperformance%20often%20comes%20at%20the%20cost%20of%20generalizability%20in%20fine-tuned%20models.%0ATo%20address%20this%20issue%2C%20we%20theoretically%20connect%20smaller%20weight%20gradient%20norms%0Aduring%20training%20and%20larger%20datasets%20to%20the%20improved%20model%20generalization.%0AMotivated%20by%20this%20connection%2C%20we%20propose%20reducing%20gradient%20norms%20for%20enhanced%0Ageneralization%20and%20aligning%20fine-tuned%20model%20with%20the%20pre-trained%20counterpart%0Ato%20retain%20knowledge%20from%20large-scale%20pre-training%20data.%20Yet%2C%20naive%20alignment%0Adoes%20not%20guarantee%20gradient%20reduction%20and%20can%20potentially%20cause%20gradient%0Aexplosion%2C%20complicating%20efforts%20to%20manage%20gradients.%20To%20address%20such%20issues%2C%20we%0Apropose%20PACE%2C%20marrying%20generalization%20of%20PArameter-efficient%20fine-tuning%20with%0AConsistency%20rEgularization.%20We%20perturb%20features%20learned%20from%20the%20adapter%20with%0Athe%20multiplicative%20noise%20and%20ensure%20the%20fine-tuned%20model%20remains%20consistent%20for%0Asame%20sample%20under%20different%20perturbations.%20Theoretical%20analysis%20shows%20that%20PACE%0Anot%20only%20implicitly%20regularizes%20gradients%20for%20enhanced%20generalization%2C%20but%20also%0Aimplicitly%20aligns%20the%20fine-tuned%20and%20pre-trained%20models%20to%20retain%20knowledge.%0AExperimental%20evidence%20supports%20our%20theories.%20PACE%20outperforms%20existing%20PEFT%0Amethods%20in%20four%20visual%20adaptation%20tasks%3A%20VTAB-1k%2C%20FGVC%2C%20few-shot%20learning%20and%0Adomain%20adaptation.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/MaxwellYaoNi/PACE%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17137v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPACE%253A%2520marrying%2520generalization%2520in%2520PArameter-efficient%2520fine-tuning%2520with%250A%2520%2520Consistency%2520rEgularization%26entry.906535625%3DYao%2520Ni%2520and%2520Shan%2520Zhang%2520and%2520Piotr%2520Koniusz%26entry.1292438233%3D%2520%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520effectively%2520adapts%2520pre-trained%2520vision%250Atransformers%2520to%2520downstream%2520tasks.%2520However%252C%2520the%2520optimization%2520for%2520tasks%250Aperformance%2520often%2520comes%2520at%2520the%2520cost%2520of%2520generalizability%2520in%2520fine-tuned%2520models.%250ATo%2520address%2520this%2520issue%252C%2520we%2520theoretically%2520connect%2520smaller%2520weight%2520gradient%2520norms%250Aduring%2520training%2520and%2520larger%2520datasets%2520to%2520the%2520improved%2520model%2520generalization.%250AMotivated%2520by%2520this%2520connection%252C%2520we%2520propose%2520reducing%2520gradient%2520norms%2520for%2520enhanced%250Ageneralization%2520and%2520aligning%2520fine-tuned%2520model%2520with%2520the%2520pre-trained%2520counterpart%250Ato%2520retain%2520knowledge%2520from%2520large-scale%2520pre-training%2520data.%2520Yet%252C%2520naive%2520alignment%250Adoes%2520not%2520guarantee%2520gradient%2520reduction%2520and%2520can%2520potentially%2520cause%2520gradient%250Aexplosion%252C%2520complicating%2520efforts%2520to%2520manage%2520gradients.%2520To%2520address%2520such%2520issues%252C%2520we%250Apropose%2520PACE%252C%2520marrying%2520generalization%2520of%2520PArameter-efficient%2520fine-tuning%2520with%250AConsistency%2520rEgularization.%2520We%2520perturb%2520features%2520learned%2520from%2520the%2520adapter%2520with%250Athe%2520multiplicative%2520noise%2520and%2520ensure%2520the%2520fine-tuned%2520model%2520remains%2520consistent%2520for%250Asame%2520sample%2520under%2520different%2520perturbations.%2520Theoretical%2520analysis%2520shows%2520that%2520PACE%250Anot%2520only%2520implicitly%2520regularizes%2520gradients%2520for%2520enhanced%2520generalization%252C%2520but%2520also%250Aimplicitly%2520aligns%2520the%2520fine-tuned%2520and%2520pre-trained%2520models%2520to%2520retain%2520knowledge.%250AExperimental%2520evidence%2520supports%2520our%2520theories.%2520PACE%2520outperforms%2520existing%2520PEFT%250Amethods%2520in%2520four%2520visual%2520adaptation%2520tasks%253A%2520VTAB-1k%252C%2520FGVC%252C%2520few-shot%2520learning%2520and%250Adomain%2520adaptation.%2520Code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/MaxwellYaoNi/PACE%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17137v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PACE%3A%20marrying%20generalization%20in%20PArameter-efficient%20fine-tuning%20with%0A%20%20Consistency%20rEgularization&entry.906535625=Yao%20Ni%20and%20Shan%20Zhang%20and%20Piotr%20Koniusz&entry.1292438233=%20%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20effectively%20adapts%20pre-trained%20vision%0Atransformers%20to%20downstream%20tasks.%20However%2C%20the%20optimization%20for%20tasks%0Aperformance%20often%20comes%20at%20the%20cost%20of%20generalizability%20in%20fine-tuned%20models.%0ATo%20address%20this%20issue%2C%20we%20theoretically%20connect%20smaller%20weight%20gradient%20norms%0Aduring%20training%20and%20larger%20datasets%20to%20the%20improved%20model%20generalization.%0AMotivated%20by%20this%20connection%2C%20we%20propose%20reducing%20gradient%20norms%20for%20enhanced%0Ageneralization%20and%20aligning%20fine-tuned%20model%20with%20the%20pre-trained%20counterpart%0Ato%20retain%20knowledge%20from%20large-scale%20pre-training%20data.%20Yet%2C%20naive%20alignment%0Adoes%20not%20guarantee%20gradient%20reduction%20and%20can%20potentially%20cause%20gradient%0Aexplosion%2C%20complicating%20efforts%20to%20manage%20gradients.%20To%20address%20such%20issues%2C%20we%0Apropose%20PACE%2C%20marrying%20generalization%20of%20PArameter-efficient%20fine-tuning%20with%0AConsistency%20rEgularization.%20We%20perturb%20features%20learned%20from%20the%20adapter%20with%0Athe%20multiplicative%20noise%20and%20ensure%20the%20fine-tuned%20model%20remains%20consistent%20for%0Asame%20sample%20under%20different%20perturbations.%20Theoretical%20analysis%20shows%20that%20PACE%0Anot%20only%20implicitly%20regularizes%20gradients%20for%20enhanced%20generalization%2C%20but%20also%0Aimplicitly%20aligns%20the%20fine-tuned%20and%20pre-trained%20models%20to%20retain%20knowledge.%0AExperimental%20evidence%20supports%20our%20theories.%20PACE%20outperforms%20existing%20PEFT%0Amethods%20in%20four%20visual%20adaptation%20tasks%3A%20VTAB-1k%2C%20FGVC%2C%20few-shot%20learning%20and%0Adomain%20adaptation.%20Code%20will%20be%20available%20at%0Ahttps%3A//github.com/MaxwellYaoNi/PACE%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17137v1&entry.124074799=Read"},
{"title": "The $\u03bc\\mathcal{G}$ Language for Programming Graph Neural Networks", "author": "Matteo Belenchia and Flavio Corradini and Michela Quadrini and Michele Loreti", "abstract": "  Graph neural networks form a class of deep learning architectures\nspecifically designed to work with graph-structured data. As such, they share\nthe inherent limitations and problems of deep learning, especially regarding\nthe issues of explainability and trustworthiness. We propose $\\mu\\mathcal{G}$,\nan original domain-specific language for the specification of graph neural\nnetworks that aims to overcome these issues. The language's syntax is\nintroduced, and its meaning is rigorously defined by a denotational semantics.\nAn equivalent characterization in the form of an operational semantics is also\nprovided and, together with a type system, is used to prove the type soundness\nof $\\mu\\mathcal{G}$. We show how $\\mu\\mathcal{G}$ programs can be represented\nin a more user-friendly graphical visualization, and provide examples of its\ngenerality by showing how it can be used to define some of the most popular\ngraph neural network models, or to develop any custom graph processing\napplication.\n", "link": "http://arxiv.org/abs/2407.09441v3", "date": "2024-09-25", "relevancy": 2.1049, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4373}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4285}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.3971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20%24%CE%BC%5Cmathcal%7BG%7D%24%20Language%20for%20Programming%20Graph%20Neural%20Networks&body=Title%3A%20The%20%24%CE%BC%5Cmathcal%7BG%7D%24%20Language%20for%20Programming%20Graph%20Neural%20Networks%0AAuthor%3A%20Matteo%20Belenchia%20and%20Flavio%20Corradini%20and%20Michela%20Quadrini%20and%20Michele%20Loreti%0AAbstract%3A%20%20%20Graph%20neural%20networks%20form%20a%20class%20of%20deep%20learning%20architectures%0Aspecifically%20designed%20to%20work%20with%20graph-structured%20data.%20As%20such%2C%20they%20share%0Athe%20inherent%20limitations%20and%20problems%20of%20deep%20learning%2C%20especially%20regarding%0Athe%20issues%20of%20explainability%20and%20trustworthiness.%20We%20propose%20%24%5Cmu%5Cmathcal%7BG%7D%24%2C%0Aan%20original%20domain-specific%20language%20for%20the%20specification%20of%20graph%20neural%0Anetworks%20that%20aims%20to%20overcome%20these%20issues.%20The%20language%27s%20syntax%20is%0Aintroduced%2C%20and%20its%20meaning%20is%20rigorously%20defined%20by%20a%20denotational%20semantics.%0AAn%20equivalent%20characterization%20in%20the%20form%20of%20an%20operational%20semantics%20is%20also%0Aprovided%20and%2C%20together%20with%20a%20type%20system%2C%20is%20used%20to%20prove%20the%20type%20soundness%0Aof%20%24%5Cmu%5Cmathcal%7BG%7D%24.%20We%20show%20how%20%24%5Cmu%5Cmathcal%7BG%7D%24%20programs%20can%20be%20represented%0Ain%20a%20more%20user-friendly%20graphical%20visualization%2C%20and%20provide%20examples%20of%20its%0Agenerality%20by%20showing%20how%20it%20can%20be%20used%20to%20define%20some%20of%20the%20most%20popular%0Agraph%20neural%20network%20models%2C%20or%20to%20develop%20any%20custom%20graph%20processing%0Aapplication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09441v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520%2524%25CE%25BC%255Cmathcal%257BG%257D%2524%2520Language%2520for%2520Programming%2520Graph%2520Neural%2520Networks%26entry.906535625%3DMatteo%2520Belenchia%2520and%2520Flavio%2520Corradini%2520and%2520Michela%2520Quadrini%2520and%2520Michele%2520Loreti%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520form%2520a%2520class%2520of%2520deep%2520learning%2520architectures%250Aspecifically%2520designed%2520to%2520work%2520with%2520graph-structured%2520data.%2520As%2520such%252C%2520they%2520share%250Athe%2520inherent%2520limitations%2520and%2520problems%2520of%2520deep%2520learning%252C%2520especially%2520regarding%250Athe%2520issues%2520of%2520explainability%2520and%2520trustworthiness.%2520We%2520propose%2520%2524%255Cmu%255Cmathcal%257BG%257D%2524%252C%250Aan%2520original%2520domain-specific%2520language%2520for%2520the%2520specification%2520of%2520graph%2520neural%250Anetworks%2520that%2520aims%2520to%2520overcome%2520these%2520issues.%2520The%2520language%2527s%2520syntax%2520is%250Aintroduced%252C%2520and%2520its%2520meaning%2520is%2520rigorously%2520defined%2520by%2520a%2520denotational%2520semantics.%250AAn%2520equivalent%2520characterization%2520in%2520the%2520form%2520of%2520an%2520operational%2520semantics%2520is%2520also%250Aprovided%2520and%252C%2520together%2520with%2520a%2520type%2520system%252C%2520is%2520used%2520to%2520prove%2520the%2520type%2520soundness%250Aof%2520%2524%255Cmu%255Cmathcal%257BG%257D%2524.%2520We%2520show%2520how%2520%2524%255Cmu%255Cmathcal%257BG%257D%2524%2520programs%2520can%2520be%2520represented%250Ain%2520a%2520more%2520user-friendly%2520graphical%2520visualization%252C%2520and%2520provide%2520examples%2520of%2520its%250Agenerality%2520by%2520showing%2520how%2520it%2520can%2520be%2520used%2520to%2520define%2520some%2520of%2520the%2520most%2520popular%250Agraph%2520neural%2520network%2520models%252C%2520or%2520to%2520develop%2520any%2520custom%2520graph%2520processing%250Aapplication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09441v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20%24%CE%BC%5Cmathcal%7BG%7D%24%20Language%20for%20Programming%20Graph%20Neural%20Networks&entry.906535625=Matteo%20Belenchia%20and%20Flavio%20Corradini%20and%20Michela%20Quadrini%20and%20Michele%20Loreti&entry.1292438233=%20%20Graph%20neural%20networks%20form%20a%20class%20of%20deep%20learning%20architectures%0Aspecifically%20designed%20to%20work%20with%20graph-structured%20data.%20As%20such%2C%20they%20share%0Athe%20inherent%20limitations%20and%20problems%20of%20deep%20learning%2C%20especially%20regarding%0Athe%20issues%20of%20explainability%20and%20trustworthiness.%20We%20propose%20%24%5Cmu%5Cmathcal%7BG%7D%24%2C%0Aan%20original%20domain-specific%20language%20for%20the%20specification%20of%20graph%20neural%0Anetworks%20that%20aims%20to%20overcome%20these%20issues.%20The%20language%27s%20syntax%20is%0Aintroduced%2C%20and%20its%20meaning%20is%20rigorously%20defined%20by%20a%20denotational%20semantics.%0AAn%20equivalent%20characterization%20in%20the%20form%20of%20an%20operational%20semantics%20is%20also%0Aprovided%20and%2C%20together%20with%20a%20type%20system%2C%20is%20used%20to%20prove%20the%20type%20soundness%0Aof%20%24%5Cmu%5Cmathcal%7BG%7D%24.%20We%20show%20how%20%24%5Cmu%5Cmathcal%7BG%7D%24%20programs%20can%20be%20represented%0Ain%20a%20more%20user-friendly%20graphical%20visualization%2C%20and%20provide%20examples%20of%20its%0Agenerality%20by%20showing%20how%20it%20can%20be%20used%20to%20define%20some%20of%20the%20most%20popular%0Agraph%20neural%20network%20models%2C%20or%20to%20develop%20any%20custom%20graph%20processing%0Aapplication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09441v3&entry.124074799=Read"},
{"title": "Interpretable Vision-Language Survival Analysis with Ordinal Inductive\n  Bias for Computational Pathology", "author": "Pei Liu and Luping Ji and Jiaxiang Gou and Bo Fu and Mao Ye", "abstract": "  Histopathology Whole-Slide Images (WSIs) provide an important tool to assess\ncancer prognosis in computational pathology (CPATH). While existing survival\nanalysis (SA) approaches have made exciting progress, they are generally\nlimited to adopting highly-expressive architectures and only coarse-grained\npatient-level labels to learn prognostic visual representations from gigapixel\nWSIs. Such learning paradigm suffers from important performance bottlenecks,\nwhen facing present scarce training data and standard multi-instance learning\n(MIL) framework in CPATH. To overcome it, this paper, for the first time,\nproposes a new Vision-Language-based SA (VLSA) paradigm. Concretely, (1) VLSA\nis driven by pathology VL foundation models. It no longer relies on\nhigh-capability networks and shows the advantage of data efficiency. (2) In\nvision-end, VLSA encodes prognostic language prior and then employs it as\nauxiliary signals to guide the aggregating of prognostic visual features at\ninstance level, thereby compensating for the weak supervision in MIL. Moreover,\ngiven the characteristics of SA, we propose i) ordinal survival prompt learning\nto transform continuous survival labels into textual prompts; and ii) ordinal\nincidence function as prediction target to make SA compatible with VL-based\nprediction. Notably, VLSA's predictions can be interpreted intuitively by our\nShapley values-based method. The extensive experiments on five datasets confirm\nthe effectiveness of our scheme. Our VLSA could pave a new way for SA in CPATH\nby offering weakly-supervised MIL an effective means to learn valuable\nprognostic clues from gigapixel WSIs. Our source code is available at\nhttps://github.com/liupei101/VLSA.\n", "link": "http://arxiv.org/abs/2409.09369v2", "date": "2024-09-25", "relevancy": 2.1045, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5373}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Vision-Language%20Survival%20Analysis%20with%20Ordinal%20Inductive%0A%20%20Bias%20for%20Computational%20Pathology&body=Title%3A%20Interpretable%20Vision-Language%20Survival%20Analysis%20with%20Ordinal%20Inductive%0A%20%20Bias%20for%20Computational%20Pathology%0AAuthor%3A%20Pei%20Liu%20and%20Luping%20Ji%20and%20Jiaxiang%20Gou%20and%20Bo%20Fu%20and%20Mao%20Ye%0AAbstract%3A%20%20%20Histopathology%20Whole-Slide%20Images%20%28WSIs%29%20provide%20an%20important%20tool%20to%20assess%0Acancer%20prognosis%20in%20computational%20pathology%20%28CPATH%29.%20While%20existing%20survival%0Aanalysis%20%28SA%29%20approaches%20have%20made%20exciting%20progress%2C%20they%20are%20generally%0Alimited%20to%20adopting%20highly-expressive%20architectures%20and%20only%20coarse-grained%0Apatient-level%20labels%20to%20learn%20prognostic%20visual%20representations%20from%20gigapixel%0AWSIs.%20Such%20learning%20paradigm%20suffers%20from%20important%20performance%20bottlenecks%2C%0Awhen%20facing%20present%20scarce%20training%20data%20and%20standard%20multi-instance%20learning%0A%28MIL%29%20framework%20in%20CPATH.%20To%20overcome%20it%2C%20this%20paper%2C%20for%20the%20first%20time%2C%0Aproposes%20a%20new%20Vision-Language-based%20SA%20%28VLSA%29%20paradigm.%20Concretely%2C%20%281%29%20VLSA%0Ais%20driven%20by%20pathology%20VL%20foundation%20models.%20It%20no%20longer%20relies%20on%0Ahigh-capability%20networks%20and%20shows%20the%20advantage%20of%20data%20efficiency.%20%282%29%20In%0Avision-end%2C%20VLSA%20encodes%20prognostic%20language%20prior%20and%20then%20employs%20it%20as%0Aauxiliary%20signals%20to%20guide%20the%20aggregating%20of%20prognostic%20visual%20features%20at%0Ainstance%20level%2C%20thereby%20compensating%20for%20the%20weak%20supervision%20in%20MIL.%20Moreover%2C%0Agiven%20the%20characteristics%20of%20SA%2C%20we%20propose%20i%29%20ordinal%20survival%20prompt%20learning%0Ato%20transform%20continuous%20survival%20labels%20into%20textual%20prompts%3B%20and%20ii%29%20ordinal%0Aincidence%20function%20as%20prediction%20target%20to%20make%20SA%20compatible%20with%20VL-based%0Aprediction.%20Notably%2C%20VLSA%27s%20predictions%20can%20be%20interpreted%20intuitively%20by%20our%0AShapley%20values-based%20method.%20The%20extensive%20experiments%20on%20five%20datasets%20confirm%0Athe%20effectiveness%20of%20our%20scheme.%20Our%20VLSA%20could%20pave%20a%20new%20way%20for%20SA%20in%20CPATH%0Aby%20offering%20weakly-supervised%20MIL%20an%20effective%20means%20to%20learn%20valuable%0Aprognostic%20clues%20from%20gigapixel%20WSIs.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/liupei101/VLSA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.09369v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Vision-Language%2520Survival%2520Analysis%2520with%2520Ordinal%2520Inductive%250A%2520%2520Bias%2520for%2520Computational%2520Pathology%26entry.906535625%3DPei%2520Liu%2520and%2520Luping%2520Ji%2520and%2520Jiaxiang%2520Gou%2520and%2520Bo%2520Fu%2520and%2520Mao%2520Ye%26entry.1292438233%3D%2520%2520Histopathology%2520Whole-Slide%2520Images%2520%2528WSIs%2529%2520provide%2520an%2520important%2520tool%2520to%2520assess%250Acancer%2520prognosis%2520in%2520computational%2520pathology%2520%2528CPATH%2529.%2520While%2520existing%2520survival%250Aanalysis%2520%2528SA%2529%2520approaches%2520have%2520made%2520exciting%2520progress%252C%2520they%2520are%2520generally%250Alimited%2520to%2520adopting%2520highly-expressive%2520architectures%2520and%2520only%2520coarse-grained%250Apatient-level%2520labels%2520to%2520learn%2520prognostic%2520visual%2520representations%2520from%2520gigapixel%250AWSIs.%2520Such%2520learning%2520paradigm%2520suffers%2520from%2520important%2520performance%2520bottlenecks%252C%250Awhen%2520facing%2520present%2520scarce%2520training%2520data%2520and%2520standard%2520multi-instance%2520learning%250A%2528MIL%2529%2520framework%2520in%2520CPATH.%2520To%2520overcome%2520it%252C%2520this%2520paper%252C%2520for%2520the%2520first%2520time%252C%250Aproposes%2520a%2520new%2520Vision-Language-based%2520SA%2520%2528VLSA%2529%2520paradigm.%2520Concretely%252C%2520%25281%2529%2520VLSA%250Ais%2520driven%2520by%2520pathology%2520VL%2520foundation%2520models.%2520It%2520no%2520longer%2520relies%2520on%250Ahigh-capability%2520networks%2520and%2520shows%2520the%2520advantage%2520of%2520data%2520efficiency.%2520%25282%2529%2520In%250Avision-end%252C%2520VLSA%2520encodes%2520prognostic%2520language%2520prior%2520and%2520then%2520employs%2520it%2520as%250Aauxiliary%2520signals%2520to%2520guide%2520the%2520aggregating%2520of%2520prognostic%2520visual%2520features%2520at%250Ainstance%2520level%252C%2520thereby%2520compensating%2520for%2520the%2520weak%2520supervision%2520in%2520MIL.%2520Moreover%252C%250Agiven%2520the%2520characteristics%2520of%2520SA%252C%2520we%2520propose%2520i%2529%2520ordinal%2520survival%2520prompt%2520learning%250Ato%2520transform%2520continuous%2520survival%2520labels%2520into%2520textual%2520prompts%253B%2520and%2520ii%2529%2520ordinal%250Aincidence%2520function%2520as%2520prediction%2520target%2520to%2520make%2520SA%2520compatible%2520with%2520VL-based%250Aprediction.%2520Notably%252C%2520VLSA%2527s%2520predictions%2520can%2520be%2520interpreted%2520intuitively%2520by%2520our%250AShapley%2520values-based%2520method.%2520The%2520extensive%2520experiments%2520on%2520five%2520datasets%2520confirm%250Athe%2520effectiveness%2520of%2520our%2520scheme.%2520Our%2520VLSA%2520could%2520pave%2520a%2520new%2520way%2520for%2520SA%2520in%2520CPATH%250Aby%2520offering%2520weakly-supervised%2520MIL%2520an%2520effective%2520means%2520to%2520learn%2520valuable%250Aprognostic%2520clues%2520from%2520gigapixel%2520WSIs.%2520Our%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/liupei101/VLSA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.09369v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Vision-Language%20Survival%20Analysis%20with%20Ordinal%20Inductive%0A%20%20Bias%20for%20Computational%20Pathology&entry.906535625=Pei%20Liu%20and%20Luping%20Ji%20and%20Jiaxiang%20Gou%20and%20Bo%20Fu%20and%20Mao%20Ye&entry.1292438233=%20%20Histopathology%20Whole-Slide%20Images%20%28WSIs%29%20provide%20an%20important%20tool%20to%20assess%0Acancer%20prognosis%20in%20computational%20pathology%20%28CPATH%29.%20While%20existing%20survival%0Aanalysis%20%28SA%29%20approaches%20have%20made%20exciting%20progress%2C%20they%20are%20generally%0Alimited%20to%20adopting%20highly-expressive%20architectures%20and%20only%20coarse-grained%0Apatient-level%20labels%20to%20learn%20prognostic%20visual%20representations%20from%20gigapixel%0AWSIs.%20Such%20learning%20paradigm%20suffers%20from%20important%20performance%20bottlenecks%2C%0Awhen%20facing%20present%20scarce%20training%20data%20and%20standard%20multi-instance%20learning%0A%28MIL%29%20framework%20in%20CPATH.%20To%20overcome%20it%2C%20this%20paper%2C%20for%20the%20first%20time%2C%0Aproposes%20a%20new%20Vision-Language-based%20SA%20%28VLSA%29%20paradigm.%20Concretely%2C%20%281%29%20VLSA%0Ais%20driven%20by%20pathology%20VL%20foundation%20models.%20It%20no%20longer%20relies%20on%0Ahigh-capability%20networks%20and%20shows%20the%20advantage%20of%20data%20efficiency.%20%282%29%20In%0Avision-end%2C%20VLSA%20encodes%20prognostic%20language%20prior%20and%20then%20employs%20it%20as%0Aauxiliary%20signals%20to%20guide%20the%20aggregating%20of%20prognostic%20visual%20features%20at%0Ainstance%20level%2C%20thereby%20compensating%20for%20the%20weak%20supervision%20in%20MIL.%20Moreover%2C%0Agiven%20the%20characteristics%20of%20SA%2C%20we%20propose%20i%29%20ordinal%20survival%20prompt%20learning%0Ato%20transform%20continuous%20survival%20labels%20into%20textual%20prompts%3B%20and%20ii%29%20ordinal%0Aincidence%20function%20as%20prediction%20target%20to%20make%20SA%20compatible%20with%20VL-based%0Aprediction.%20Notably%2C%20VLSA%27s%20predictions%20can%20be%20interpreted%20intuitively%20by%20our%0AShapley%20values-based%20method.%20The%20extensive%20experiments%20on%20five%20datasets%20confirm%0Athe%20effectiveness%20of%20our%20scheme.%20Our%20VLSA%20could%20pave%20a%20new%20way%20for%20SA%20in%20CPATH%0Aby%20offering%20weakly-supervised%20MIL%20an%20effective%20means%20to%20learn%20valuable%0Aprognostic%20clues%20from%20gigapixel%20WSIs.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/liupei101/VLSA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.09369v2&entry.124074799=Read"},
{"title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art\n  Multimodal Models", "author": "Matt Deitke and Christopher Clark and Sangho Lee and Rohun Tripathi and Yue Yang and Jae Sung Park and Mohammadreza Salehi and Niklas Muennighoff and Kyle Lo and Luca Soldaini and Jiasen Lu and Taira Anderson and Erin Bransom and Kiana Ehsani and Huong Ngo and YenSung Chen and Ajay Patel and Mark Yatskar and Chris Callison-Burch and Andrew Head and Rose Hendrix and Favyen Bastani and Eli VanderBilt and Nathan Lambert and Yvonne Chou and Arnavi Chheda and Jenna Sparks and Sam Skjonsberg and Michael Schmitz and Aaron Sarnat and Byron Bischoff and Pete Walsh and Chris Newell and Piper Wolters and Tanmay Gupta and Kuo-Hao Zeng and Jon Borchardt and Dirk Groeneveld and Jen Dumas and Crystal Nam and Sophie Lebrecht and Caitlin Wittlif and Carissa Schoenick and Oscar Michel and Ranjay Krishna and Luca Weihs and Noah A. Smith and Hannaneh Hajishirzi and Ross Girshick and Ali Farhadi and Aniruddha Kembhavi", "abstract": "  Today's most advanced multimodal models remain proprietary. The strongest\nopen-weight models rely heavily on synthetic data from proprietary VLMs to\nachieve good performance, effectively distilling these closed models into open\nones. As a result, the community is still missing foundational knowledge about\nhow to build performant VLMs from scratch. We present Molmo, a new family of\nVLMs that are state-of-the-art in their class of openness. Our key innovation\nis a novel, highly detailed image caption dataset collected entirely from human\nannotators using speech-based descriptions. To enable a wide array of user\ninteractions, we also introduce a diverse dataset mixture for fine-tuning that\nincludes in-the-wild Q&A and innovative 2D pointing data. The success of our\napproach relies on careful choices for the model architecture details, a\nwell-tuned training pipeline, and, most critically, the quality of our newly\ncollected datasets, all of which will be released. The best-in-class 72B model\nwithin the Molmo family not only outperforms others in the class of open weight\nand data models but also compares favorably against proprietary systems like\nGPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human\nevaluation.\n  We will be releasing all of our model weights, captioning and fine-tuning\ndata, and source code in the near future. Select model weights, inference code,\nand demo are available at https://molmo.allenai.org.\n", "link": "http://arxiv.org/abs/2409.17146v1", "date": "2024-09-25", "relevancy": 2.1036, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5446}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5132}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Molmo%20and%20PixMo%3A%20Open%20Weights%20and%20Open%20Data%20for%20State-of-the-Art%0A%20%20Multimodal%20Models&body=Title%3A%20Molmo%20and%20PixMo%3A%20Open%20Weights%20and%20Open%20Data%20for%20State-of-the-Art%0A%20%20Multimodal%20Models%0AAuthor%3A%20Matt%20Deitke%20and%20Christopher%20Clark%20and%20Sangho%20Lee%20and%20Rohun%20Tripathi%20and%20Yue%20Yang%20and%20Jae%20Sung%20Park%20and%20Mohammadreza%20Salehi%20and%20Niklas%20Muennighoff%20and%20Kyle%20Lo%20and%20Luca%20Soldaini%20and%20Jiasen%20Lu%20and%20Taira%20Anderson%20and%20Erin%20Bransom%20and%20Kiana%20Ehsani%20and%20Huong%20Ngo%20and%20YenSung%20Chen%20and%20Ajay%20Patel%20and%20Mark%20Yatskar%20and%20Chris%20Callison-Burch%20and%20Andrew%20Head%20and%20Rose%20Hendrix%20and%20Favyen%20Bastani%20and%20Eli%20VanderBilt%20and%20Nathan%20Lambert%20and%20Yvonne%20Chou%20and%20Arnavi%20Chheda%20and%20Jenna%20Sparks%20and%20Sam%20Skjonsberg%20and%20Michael%20Schmitz%20and%20Aaron%20Sarnat%20and%20Byron%20Bischoff%20and%20Pete%20Walsh%20and%20Chris%20Newell%20and%20Piper%20Wolters%20and%20Tanmay%20Gupta%20and%20Kuo-Hao%20Zeng%20and%20Jon%20Borchardt%20and%20Dirk%20Groeneveld%20and%20Jen%20Dumas%20and%20Crystal%20Nam%20and%20Sophie%20Lebrecht%20and%20Caitlin%20Wittlif%20and%20Carissa%20Schoenick%20and%20Oscar%20Michel%20and%20Ranjay%20Krishna%20and%20Luca%20Weihs%20and%20Noah%20A.%20Smith%20and%20Hannaneh%20Hajishirzi%20and%20Ross%20Girshick%20and%20Ali%20Farhadi%20and%20Aniruddha%20Kembhavi%0AAbstract%3A%20%20%20Today%27s%20most%20advanced%20multimodal%20models%20remain%20proprietary.%20The%20strongest%0Aopen-weight%20models%20rely%20heavily%20on%20synthetic%20data%20from%20proprietary%20VLMs%20to%0Aachieve%20good%20performance%2C%20effectively%20distilling%20these%20closed%20models%20into%20open%0Aones.%20As%20a%20result%2C%20the%20community%20is%20still%20missing%20foundational%20knowledge%20about%0Ahow%20to%20build%20performant%20VLMs%20from%20scratch.%20We%20present%20Molmo%2C%20a%20new%20family%20of%0AVLMs%20that%20are%20state-of-the-art%20in%20their%20class%20of%20openness.%20Our%20key%20innovation%0Ais%20a%20novel%2C%20highly%20detailed%20image%20caption%20dataset%20collected%20entirely%20from%20human%0Aannotators%20using%20speech-based%20descriptions.%20To%20enable%20a%20wide%20array%20of%20user%0Ainteractions%2C%20we%20also%20introduce%20a%20diverse%20dataset%20mixture%20for%20fine-tuning%20that%0Aincludes%20in-the-wild%20Q%26A%20and%20innovative%202D%20pointing%20data.%20The%20success%20of%20our%0Aapproach%20relies%20on%20careful%20choices%20for%20the%20model%20architecture%20details%2C%20a%0Awell-tuned%20training%20pipeline%2C%20and%2C%20most%20critically%2C%20the%20quality%20of%20our%20newly%0Acollected%20datasets%2C%20all%20of%20which%20will%20be%20released.%20The%20best-in-class%2072B%20model%0Awithin%20the%20Molmo%20family%20not%20only%20outperforms%20others%20in%20the%20class%20of%20open%20weight%0Aand%20data%20models%20but%20also%20compares%20favorably%20against%20proprietary%20systems%20like%0AGPT-4o%2C%20Claude%203.5%2C%20and%20Gemini%201.5%20on%20both%20academic%20benchmarks%20and%20human%0Aevaluation.%0A%20%20We%20will%20be%20releasing%20all%20of%20our%20model%20weights%2C%20captioning%20and%20fine-tuning%0Adata%2C%20and%20source%20code%20in%20the%20near%20future.%20Select%20model%20weights%2C%20inference%20code%2C%0Aand%20demo%20are%20available%20at%20https%3A//molmo.allenai.org.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMolmo%2520and%2520PixMo%253A%2520Open%2520Weights%2520and%2520Open%2520Data%2520for%2520State-of-the-Art%250A%2520%2520Multimodal%2520Models%26entry.906535625%3DMatt%2520Deitke%2520and%2520Christopher%2520Clark%2520and%2520Sangho%2520Lee%2520and%2520Rohun%2520Tripathi%2520and%2520Yue%2520Yang%2520and%2520Jae%2520Sung%2520Park%2520and%2520Mohammadreza%2520Salehi%2520and%2520Niklas%2520Muennighoff%2520and%2520Kyle%2520Lo%2520and%2520Luca%2520Soldaini%2520and%2520Jiasen%2520Lu%2520and%2520Taira%2520Anderson%2520and%2520Erin%2520Bransom%2520and%2520Kiana%2520Ehsani%2520and%2520Huong%2520Ngo%2520and%2520YenSung%2520Chen%2520and%2520Ajay%2520Patel%2520and%2520Mark%2520Yatskar%2520and%2520Chris%2520Callison-Burch%2520and%2520Andrew%2520Head%2520and%2520Rose%2520Hendrix%2520and%2520Favyen%2520Bastani%2520and%2520Eli%2520VanderBilt%2520and%2520Nathan%2520Lambert%2520and%2520Yvonne%2520Chou%2520and%2520Arnavi%2520Chheda%2520and%2520Jenna%2520Sparks%2520and%2520Sam%2520Skjonsberg%2520and%2520Michael%2520Schmitz%2520and%2520Aaron%2520Sarnat%2520and%2520Byron%2520Bischoff%2520and%2520Pete%2520Walsh%2520and%2520Chris%2520Newell%2520and%2520Piper%2520Wolters%2520and%2520Tanmay%2520Gupta%2520and%2520Kuo-Hao%2520Zeng%2520and%2520Jon%2520Borchardt%2520and%2520Dirk%2520Groeneveld%2520and%2520Jen%2520Dumas%2520and%2520Crystal%2520Nam%2520and%2520Sophie%2520Lebrecht%2520and%2520Caitlin%2520Wittlif%2520and%2520Carissa%2520Schoenick%2520and%2520Oscar%2520Michel%2520and%2520Ranjay%2520Krishna%2520and%2520Luca%2520Weihs%2520and%2520Noah%2520A.%2520Smith%2520and%2520Hannaneh%2520Hajishirzi%2520and%2520Ross%2520Girshick%2520and%2520Ali%2520Farhadi%2520and%2520Aniruddha%2520Kembhavi%26entry.1292438233%3D%2520%2520Today%2527s%2520most%2520advanced%2520multimodal%2520models%2520remain%2520proprietary.%2520The%2520strongest%250Aopen-weight%2520models%2520rely%2520heavily%2520on%2520synthetic%2520data%2520from%2520proprietary%2520VLMs%2520to%250Aachieve%2520good%2520performance%252C%2520effectively%2520distilling%2520these%2520closed%2520models%2520into%2520open%250Aones.%2520As%2520a%2520result%252C%2520the%2520community%2520is%2520still%2520missing%2520foundational%2520knowledge%2520about%250Ahow%2520to%2520build%2520performant%2520VLMs%2520from%2520scratch.%2520We%2520present%2520Molmo%252C%2520a%2520new%2520family%2520of%250AVLMs%2520that%2520are%2520state-of-the-art%2520in%2520their%2520class%2520of%2520openness.%2520Our%2520key%2520innovation%250Ais%2520a%2520novel%252C%2520highly%2520detailed%2520image%2520caption%2520dataset%2520collected%2520entirely%2520from%2520human%250Aannotators%2520using%2520speech-based%2520descriptions.%2520To%2520enable%2520a%2520wide%2520array%2520of%2520user%250Ainteractions%252C%2520we%2520also%2520introduce%2520a%2520diverse%2520dataset%2520mixture%2520for%2520fine-tuning%2520that%250Aincludes%2520in-the-wild%2520Q%2526A%2520and%2520innovative%25202D%2520pointing%2520data.%2520The%2520success%2520of%2520our%250Aapproach%2520relies%2520on%2520careful%2520choices%2520for%2520the%2520model%2520architecture%2520details%252C%2520a%250Awell-tuned%2520training%2520pipeline%252C%2520and%252C%2520most%2520critically%252C%2520the%2520quality%2520of%2520our%2520newly%250Acollected%2520datasets%252C%2520all%2520of%2520which%2520will%2520be%2520released.%2520The%2520best-in-class%252072B%2520model%250Awithin%2520the%2520Molmo%2520family%2520not%2520only%2520outperforms%2520others%2520in%2520the%2520class%2520of%2520open%2520weight%250Aand%2520data%2520models%2520but%2520also%2520compares%2520favorably%2520against%2520proprietary%2520systems%2520like%250AGPT-4o%252C%2520Claude%25203.5%252C%2520and%2520Gemini%25201.5%2520on%2520both%2520academic%2520benchmarks%2520and%2520human%250Aevaluation.%250A%2520%2520We%2520will%2520be%2520releasing%2520all%2520of%2520our%2520model%2520weights%252C%2520captioning%2520and%2520fine-tuning%250Adata%252C%2520and%2520source%2520code%2520in%2520the%2520near%2520future.%2520Select%2520model%2520weights%252C%2520inference%2520code%252C%250Aand%2520demo%2520are%2520available%2520at%2520https%253A//molmo.allenai.org.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Molmo%20and%20PixMo%3A%20Open%20Weights%20and%20Open%20Data%20for%20State-of-the-Art%0A%20%20Multimodal%20Models&entry.906535625=Matt%20Deitke%20and%20Christopher%20Clark%20and%20Sangho%20Lee%20and%20Rohun%20Tripathi%20and%20Yue%20Yang%20and%20Jae%20Sung%20Park%20and%20Mohammadreza%20Salehi%20and%20Niklas%20Muennighoff%20and%20Kyle%20Lo%20and%20Luca%20Soldaini%20and%20Jiasen%20Lu%20and%20Taira%20Anderson%20and%20Erin%20Bransom%20and%20Kiana%20Ehsani%20and%20Huong%20Ngo%20and%20YenSung%20Chen%20and%20Ajay%20Patel%20and%20Mark%20Yatskar%20and%20Chris%20Callison-Burch%20and%20Andrew%20Head%20and%20Rose%20Hendrix%20and%20Favyen%20Bastani%20and%20Eli%20VanderBilt%20and%20Nathan%20Lambert%20and%20Yvonne%20Chou%20and%20Arnavi%20Chheda%20and%20Jenna%20Sparks%20and%20Sam%20Skjonsberg%20and%20Michael%20Schmitz%20and%20Aaron%20Sarnat%20and%20Byron%20Bischoff%20and%20Pete%20Walsh%20and%20Chris%20Newell%20and%20Piper%20Wolters%20and%20Tanmay%20Gupta%20and%20Kuo-Hao%20Zeng%20and%20Jon%20Borchardt%20and%20Dirk%20Groeneveld%20and%20Jen%20Dumas%20and%20Crystal%20Nam%20and%20Sophie%20Lebrecht%20and%20Caitlin%20Wittlif%20and%20Carissa%20Schoenick%20and%20Oscar%20Michel%20and%20Ranjay%20Krishna%20and%20Luca%20Weihs%20and%20Noah%20A.%20Smith%20and%20Hannaneh%20Hajishirzi%20and%20Ross%20Girshick%20and%20Ali%20Farhadi%20and%20Aniruddha%20Kembhavi&entry.1292438233=%20%20Today%27s%20most%20advanced%20multimodal%20models%20remain%20proprietary.%20The%20strongest%0Aopen-weight%20models%20rely%20heavily%20on%20synthetic%20data%20from%20proprietary%20VLMs%20to%0Aachieve%20good%20performance%2C%20effectively%20distilling%20these%20closed%20models%20into%20open%0Aones.%20As%20a%20result%2C%20the%20community%20is%20still%20missing%20foundational%20knowledge%20about%0Ahow%20to%20build%20performant%20VLMs%20from%20scratch.%20We%20present%20Molmo%2C%20a%20new%20family%20of%0AVLMs%20that%20are%20state-of-the-art%20in%20their%20class%20of%20openness.%20Our%20key%20innovation%0Ais%20a%20novel%2C%20highly%20detailed%20image%20caption%20dataset%20collected%20entirely%20from%20human%0Aannotators%20using%20speech-based%20descriptions.%20To%20enable%20a%20wide%20array%20of%20user%0Ainteractions%2C%20we%20also%20introduce%20a%20diverse%20dataset%20mixture%20for%20fine-tuning%20that%0Aincludes%20in-the-wild%20Q%26A%20and%20innovative%202D%20pointing%20data.%20The%20success%20of%20our%0Aapproach%20relies%20on%20careful%20choices%20for%20the%20model%20architecture%20details%2C%20a%0Awell-tuned%20training%20pipeline%2C%20and%2C%20most%20critically%2C%20the%20quality%20of%20our%20newly%0Acollected%20datasets%2C%20all%20of%20which%20will%20be%20released.%20The%20best-in-class%2072B%20model%0Awithin%20the%20Molmo%20family%20not%20only%20outperforms%20others%20in%20the%20class%20of%20open%20weight%0Aand%20data%20models%20but%20also%20compares%20favorably%20against%20proprietary%20systems%20like%0AGPT-4o%2C%20Claude%203.5%2C%20and%20Gemini%201.5%20on%20both%20academic%20benchmarks%20and%20human%0Aevaluation.%0A%20%20We%20will%20be%20releasing%20all%20of%20our%20model%20weights%2C%20captioning%20and%20fine-tuning%0Adata%2C%20and%20source%20code%20in%20the%20near%20future.%20Select%20model%20weights%2C%20inference%20code%2C%0Aand%20demo%20are%20available%20at%20https%3A//molmo.allenai.org.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17146v1&entry.124074799=Read"},
{"title": "Decoding Large-Language Models: A Systematic Overview of Socio-Technical\n  Impacts, Constraints, and Emerging Questions", "author": "Zeyneb N. Kaya and Souvick Ghosh", "abstract": "  There have been rapid advancements in the capabilities of large language\nmodels (LLMs) in recent years, greatly revolutionizing the field of natural\nlanguage processing (NLP) and artificial intelligence (AI) to understand and\ninteract with human language. Therefore, in this work, we conduct a systematic\ninvestigation of the literature to identify the prominent themes and directions\nof LLM developments, impacts, and limitations. Our findings illustrate the\naims, methodologies, limitations, and future directions of LLM research. It\nincludes responsible development considerations, algorithmic improvements,\nethical challenges, and societal implications of LLM development. Overall, this\npaper provides a rigorous and comprehensive overview of current research in LLM\nand identifies potential directions for future development. The article\nhighlights the application areas that could have a positive impact on society\nalong with the ethical considerations.\n", "link": "http://arxiv.org/abs/2409.16974v1", "date": "2024-09-25", "relevancy": 2.0971, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5442}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5442}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoding%20Large-Language%20Models%3A%20A%20Systematic%20Overview%20of%20Socio-Technical%0A%20%20Impacts%2C%20Constraints%2C%20and%20Emerging%20Questions&body=Title%3A%20Decoding%20Large-Language%20Models%3A%20A%20Systematic%20Overview%20of%20Socio-Technical%0A%20%20Impacts%2C%20Constraints%2C%20and%20Emerging%20Questions%0AAuthor%3A%20Zeyneb%20N.%20Kaya%20and%20Souvick%20Ghosh%0AAbstract%3A%20%20%20There%20have%20been%20rapid%20advancements%20in%20the%20capabilities%20of%20large%20language%0Amodels%20%28LLMs%29%20in%20recent%20years%2C%20greatly%20revolutionizing%20the%20field%20of%20natural%0Alanguage%20processing%20%28NLP%29%20and%20artificial%20intelligence%20%28AI%29%20to%20understand%20and%0Ainteract%20with%20human%20language.%20Therefore%2C%20in%20this%20work%2C%20we%20conduct%20a%20systematic%0Ainvestigation%20of%20the%20literature%20to%20identify%20the%20prominent%20themes%20and%20directions%0Aof%20LLM%20developments%2C%20impacts%2C%20and%20limitations.%20Our%20findings%20illustrate%20the%0Aaims%2C%20methodologies%2C%20limitations%2C%20and%20future%20directions%20of%20LLM%20research.%20It%0Aincludes%20responsible%20development%20considerations%2C%20algorithmic%20improvements%2C%0Aethical%20challenges%2C%20and%20societal%20implications%20of%20LLM%20development.%20Overall%2C%20this%0Apaper%20provides%20a%20rigorous%20and%20comprehensive%20overview%20of%20current%20research%20in%20LLM%0Aand%20identifies%20potential%20directions%20for%20future%20development.%20The%20article%0Ahighlights%20the%20application%20areas%20that%20could%20have%20a%20positive%20impact%20on%20society%0Aalong%20with%20the%20ethical%20considerations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoding%2520Large-Language%2520Models%253A%2520A%2520Systematic%2520Overview%2520of%2520Socio-Technical%250A%2520%2520Impacts%252C%2520Constraints%252C%2520and%2520Emerging%2520Questions%26entry.906535625%3DZeyneb%2520N.%2520Kaya%2520and%2520Souvick%2520Ghosh%26entry.1292438233%3D%2520%2520There%2520have%2520been%2520rapid%2520advancements%2520in%2520the%2520capabilities%2520of%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520in%2520recent%2520years%252C%2520greatly%2520revolutionizing%2520the%2520field%2520of%2520natural%250Alanguage%2520processing%2520%2528NLP%2529%2520and%2520artificial%2520intelligence%2520%2528AI%2529%2520to%2520understand%2520and%250Ainteract%2520with%2520human%2520language.%2520Therefore%252C%2520in%2520this%2520work%252C%2520we%2520conduct%2520a%2520systematic%250Ainvestigation%2520of%2520the%2520literature%2520to%2520identify%2520the%2520prominent%2520themes%2520and%2520directions%250Aof%2520LLM%2520developments%252C%2520impacts%252C%2520and%2520limitations.%2520Our%2520findings%2520illustrate%2520the%250Aaims%252C%2520methodologies%252C%2520limitations%252C%2520and%2520future%2520directions%2520of%2520LLM%2520research.%2520It%250Aincludes%2520responsible%2520development%2520considerations%252C%2520algorithmic%2520improvements%252C%250Aethical%2520challenges%252C%2520and%2520societal%2520implications%2520of%2520LLM%2520development.%2520Overall%252C%2520this%250Apaper%2520provides%2520a%2520rigorous%2520and%2520comprehensive%2520overview%2520of%2520current%2520research%2520in%2520LLM%250Aand%2520identifies%2520potential%2520directions%2520for%2520future%2520development.%2520The%2520article%250Ahighlights%2520the%2520application%2520areas%2520that%2520could%2520have%2520a%2520positive%2520impact%2520on%2520society%250Aalong%2520with%2520the%2520ethical%2520considerations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoding%20Large-Language%20Models%3A%20A%20Systematic%20Overview%20of%20Socio-Technical%0A%20%20Impacts%2C%20Constraints%2C%20and%20Emerging%20Questions&entry.906535625=Zeyneb%20N.%20Kaya%20and%20Souvick%20Ghosh&entry.1292438233=%20%20There%20have%20been%20rapid%20advancements%20in%20the%20capabilities%20of%20large%20language%0Amodels%20%28LLMs%29%20in%20recent%20years%2C%20greatly%20revolutionizing%20the%20field%20of%20natural%0Alanguage%20processing%20%28NLP%29%20and%20artificial%20intelligence%20%28AI%29%20to%20understand%20and%0Ainteract%20with%20human%20language.%20Therefore%2C%20in%20this%20work%2C%20we%20conduct%20a%20systematic%0Ainvestigation%20of%20the%20literature%20to%20identify%20the%20prominent%20themes%20and%20directions%0Aof%20LLM%20developments%2C%20impacts%2C%20and%20limitations.%20Our%20findings%20illustrate%20the%0Aaims%2C%20methodologies%2C%20limitations%2C%20and%20future%20directions%20of%20LLM%20research.%20It%0Aincludes%20responsible%20development%20considerations%2C%20algorithmic%20improvements%2C%0Aethical%20challenges%2C%20and%20societal%20implications%20of%20LLM%20development.%20Overall%2C%20this%0Apaper%20provides%20a%20rigorous%20and%20comprehensive%20overview%20of%20current%20research%20in%20LLM%0Aand%20identifies%20potential%20directions%20for%20future%20development.%20The%20article%0Ahighlights%20the%20application%20areas%20that%20could%20have%20a%20positive%20impact%20on%20society%0Aalong%20with%20the%20ethical%20considerations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16974v1&entry.124074799=Read"},
{"title": "Spacewalker: Traversing Representation Spaces for Fast Interactive\n  Exploration and Annotation of Unstructured Data", "author": "Lukas Heine and Fabian H\u00f6rst and Jana Fragemann and Gijs Luijten and Miriam Balzer and Jan Egger and Fin Bahnsen and M. Saquib Sarfraz and Jens Kleesiek and Constantin Seibold", "abstract": "  Unstructured data in industries such as healthcare, finance, and\nmanufacturing presents significant challenges for efficient analysis and\ndecision making. Detecting patterns within this data and understanding their\nimpact is critical but complex without the right tools. Traditionally, these\ntasks relied on the expertise of data analysts or labor-intensive manual\nreviews. In response, we introduce Spacewalker, an interactive tool designed to\nexplore and annotate data across multiple modalities. Spacewalker allows users\nto extract data representations and visualize them in low-dimensional spaces,\nenabling the detection of semantic similarities. Through extensive user\nstudies, we assess Spacewalker's effectiveness in data annotation and integrity\nverification. Results show that the tool's ability to traverse latent spaces\nand perform multi-modal queries significantly enhances the user's capacity to\nquickly identify relevant data. Moreover, Spacewalker allows for annotation\nspeed-ups far superior to conventional methods, making it a promising tool for\nefficiently navigating unstructured data and improving decision making\nprocesses. The code of this work is open-source and can be found at:\nhttps://github.com/code-lukas/Spacewalker\n", "link": "http://arxiv.org/abs/2409.16793v1", "date": "2024-09-25", "relevancy": 2.0923, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5305}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5253}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spacewalker%3A%20Traversing%20Representation%20Spaces%20for%20Fast%20Interactive%0A%20%20Exploration%20and%20Annotation%20of%20Unstructured%20Data&body=Title%3A%20Spacewalker%3A%20Traversing%20Representation%20Spaces%20for%20Fast%20Interactive%0A%20%20Exploration%20and%20Annotation%20of%20Unstructured%20Data%0AAuthor%3A%20Lukas%20Heine%20and%20Fabian%20H%C3%B6rst%20and%20Jana%20Fragemann%20and%20Gijs%20Luijten%20and%20Miriam%20Balzer%20and%20Jan%20Egger%20and%20Fin%20Bahnsen%20and%20M.%20Saquib%20Sarfraz%20and%20Jens%20Kleesiek%20and%20Constantin%20Seibold%0AAbstract%3A%20%20%20Unstructured%20data%20in%20industries%20such%20as%20healthcare%2C%20finance%2C%20and%0Amanufacturing%20presents%20significant%20challenges%20for%20efficient%20analysis%20and%0Adecision%20making.%20Detecting%20patterns%20within%20this%20data%20and%20understanding%20their%0Aimpact%20is%20critical%20but%20complex%20without%20the%20right%20tools.%20Traditionally%2C%20these%0Atasks%20relied%20on%20the%20expertise%20of%20data%20analysts%20or%20labor-intensive%20manual%0Areviews.%20In%20response%2C%20we%20introduce%20Spacewalker%2C%20an%20interactive%20tool%20designed%20to%0Aexplore%20and%20annotate%20data%20across%20multiple%20modalities.%20Spacewalker%20allows%20users%0Ato%20extract%20data%20representations%20and%20visualize%20them%20in%20low-dimensional%20spaces%2C%0Aenabling%20the%20detection%20of%20semantic%20similarities.%20Through%20extensive%20user%0Astudies%2C%20we%20assess%20Spacewalker%27s%20effectiveness%20in%20data%20annotation%20and%20integrity%0Averification.%20Results%20show%20that%20the%20tool%27s%20ability%20to%20traverse%20latent%20spaces%0Aand%20perform%20multi-modal%20queries%20significantly%20enhances%20the%20user%27s%20capacity%20to%0Aquickly%20identify%20relevant%20data.%20Moreover%2C%20Spacewalker%20allows%20for%20annotation%0Aspeed-ups%20far%20superior%20to%20conventional%20methods%2C%20making%20it%20a%20promising%20tool%20for%0Aefficiently%20navigating%20unstructured%20data%20and%20improving%20decision%20making%0Aprocesses.%20The%20code%20of%20this%20work%20is%20open-source%20and%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/code-lukas/Spacewalker%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16793v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpacewalker%253A%2520Traversing%2520Representation%2520Spaces%2520for%2520Fast%2520Interactive%250A%2520%2520Exploration%2520and%2520Annotation%2520of%2520Unstructured%2520Data%26entry.906535625%3DLukas%2520Heine%2520and%2520Fabian%2520H%25C3%25B6rst%2520and%2520Jana%2520Fragemann%2520and%2520Gijs%2520Luijten%2520and%2520Miriam%2520Balzer%2520and%2520Jan%2520Egger%2520and%2520Fin%2520Bahnsen%2520and%2520M.%2520Saquib%2520Sarfraz%2520and%2520Jens%2520Kleesiek%2520and%2520Constantin%2520Seibold%26entry.1292438233%3D%2520%2520Unstructured%2520data%2520in%2520industries%2520such%2520as%2520healthcare%252C%2520finance%252C%2520and%250Amanufacturing%2520presents%2520significant%2520challenges%2520for%2520efficient%2520analysis%2520and%250Adecision%2520making.%2520Detecting%2520patterns%2520within%2520this%2520data%2520and%2520understanding%2520their%250Aimpact%2520is%2520critical%2520but%2520complex%2520without%2520the%2520right%2520tools.%2520Traditionally%252C%2520these%250Atasks%2520relied%2520on%2520the%2520expertise%2520of%2520data%2520analysts%2520or%2520labor-intensive%2520manual%250Areviews.%2520In%2520response%252C%2520we%2520introduce%2520Spacewalker%252C%2520an%2520interactive%2520tool%2520designed%2520to%250Aexplore%2520and%2520annotate%2520data%2520across%2520multiple%2520modalities.%2520Spacewalker%2520allows%2520users%250Ato%2520extract%2520data%2520representations%2520and%2520visualize%2520them%2520in%2520low-dimensional%2520spaces%252C%250Aenabling%2520the%2520detection%2520of%2520semantic%2520similarities.%2520Through%2520extensive%2520user%250Astudies%252C%2520we%2520assess%2520Spacewalker%2527s%2520effectiveness%2520in%2520data%2520annotation%2520and%2520integrity%250Averification.%2520Results%2520show%2520that%2520the%2520tool%2527s%2520ability%2520to%2520traverse%2520latent%2520spaces%250Aand%2520perform%2520multi-modal%2520queries%2520significantly%2520enhances%2520the%2520user%2527s%2520capacity%2520to%250Aquickly%2520identify%2520relevant%2520data.%2520Moreover%252C%2520Spacewalker%2520allows%2520for%2520annotation%250Aspeed-ups%2520far%2520superior%2520to%2520conventional%2520methods%252C%2520making%2520it%2520a%2520promising%2520tool%2520for%250Aefficiently%2520navigating%2520unstructured%2520data%2520and%2520improving%2520decision%2520making%250Aprocesses.%2520The%2520code%2520of%2520this%2520work%2520is%2520open-source%2520and%2520can%2520be%2520found%2520at%253A%250Ahttps%253A//github.com/code-lukas/Spacewalker%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16793v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spacewalker%3A%20Traversing%20Representation%20Spaces%20for%20Fast%20Interactive%0A%20%20Exploration%20and%20Annotation%20of%20Unstructured%20Data&entry.906535625=Lukas%20Heine%20and%20Fabian%20H%C3%B6rst%20and%20Jana%20Fragemann%20and%20Gijs%20Luijten%20and%20Miriam%20Balzer%20and%20Jan%20Egger%20and%20Fin%20Bahnsen%20and%20M.%20Saquib%20Sarfraz%20and%20Jens%20Kleesiek%20and%20Constantin%20Seibold&entry.1292438233=%20%20Unstructured%20data%20in%20industries%20such%20as%20healthcare%2C%20finance%2C%20and%0Amanufacturing%20presents%20significant%20challenges%20for%20efficient%20analysis%20and%0Adecision%20making.%20Detecting%20patterns%20within%20this%20data%20and%20understanding%20their%0Aimpact%20is%20critical%20but%20complex%20without%20the%20right%20tools.%20Traditionally%2C%20these%0Atasks%20relied%20on%20the%20expertise%20of%20data%20analysts%20or%20labor-intensive%20manual%0Areviews.%20In%20response%2C%20we%20introduce%20Spacewalker%2C%20an%20interactive%20tool%20designed%20to%0Aexplore%20and%20annotate%20data%20across%20multiple%20modalities.%20Spacewalker%20allows%20users%0Ato%20extract%20data%20representations%20and%20visualize%20them%20in%20low-dimensional%20spaces%2C%0Aenabling%20the%20detection%20of%20semantic%20similarities.%20Through%20extensive%20user%0Astudies%2C%20we%20assess%20Spacewalker%27s%20effectiveness%20in%20data%20annotation%20and%20integrity%0Averification.%20Results%20show%20that%20the%20tool%27s%20ability%20to%20traverse%20latent%20spaces%0Aand%20perform%20multi-modal%20queries%20significantly%20enhances%20the%20user%27s%20capacity%20to%0Aquickly%20identify%20relevant%20data.%20Moreover%2C%20Spacewalker%20allows%20for%20annotation%0Aspeed-ups%20far%20superior%20to%20conventional%20methods%2C%20making%20it%20a%20promising%20tool%20for%0Aefficiently%20navigating%20unstructured%20data%20and%20improving%20decision%20making%0Aprocesses.%20The%20code%20of%20this%20work%20is%20open-source%20and%20can%20be%20found%20at%3A%0Ahttps%3A//github.com/code-lukas/Spacewalker%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16793v1&entry.124074799=Read"},
{"title": "MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale", "author": "Anton Andreychuk and Konstantin Yakovlev and Aleksandr Panov and Alexey Skrynnik", "abstract": "  Multi-agent pathfinding (MAPF) is a challenging computational problem that\ntypically requires to find collision-free paths for multiple agents in a shared\nenvironment. Solving MAPF optimally is NP-hard, yet efficient solutions are\ncritical for numerous applications, including automated warehouses and\ntransportation systems. Recently, learning-based approaches to MAPF have gained\nattention, particularly those leveraging deep reinforcement learning. Following\ncurrent trends in machine learning, we have created a foundation model for the\nMAPF problems called MAPF-GPT. Using imitation learning, we have trained a\npolicy on a set of pre-collected sub-optimal expert trajectories that can\ngenerate actions in conditions of partial observability without additional\nheuristics, reward functions, or communication with other agents. The resulting\nMAPF-GPT model demonstrates zero-shot learning abilities when solving the MAPF\nproblem instances that were not present in the training dataset. We show that\nMAPF-GPT notably outperforms the current best-performing learnable-MAPF solvers\non a diverse range of problem instances and is efficient in terms of\ncomputation (in the inference mode).\n", "link": "http://arxiv.org/abs/2409.00134v3", "date": "2024-09-25", "relevancy": 2.0807, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5467}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5223}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5075}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAPF-GPT%3A%20Imitation%20Learning%20for%20Multi-Agent%20Pathfinding%20at%20Scale&body=Title%3A%20MAPF-GPT%3A%20Imitation%20Learning%20for%20Multi-Agent%20Pathfinding%20at%20Scale%0AAuthor%3A%20Anton%20Andreychuk%20and%20Konstantin%20Yakovlev%20and%20Aleksandr%20Panov%20and%20Alexey%20Skrynnik%0AAbstract%3A%20%20%20Multi-agent%20pathfinding%20%28MAPF%29%20is%20a%20challenging%20computational%20problem%20that%0Atypically%20requires%20to%20find%20collision-free%20paths%20for%20multiple%20agents%20in%20a%20shared%0Aenvironment.%20Solving%20MAPF%20optimally%20is%20NP-hard%2C%20yet%20efficient%20solutions%20are%0Acritical%20for%20numerous%20applications%2C%20including%20automated%20warehouses%20and%0Atransportation%20systems.%20Recently%2C%20learning-based%20approaches%20to%20MAPF%20have%20gained%0Aattention%2C%20particularly%20those%20leveraging%20deep%20reinforcement%20learning.%20Following%0Acurrent%20trends%20in%20machine%20learning%2C%20we%20have%20created%20a%20foundation%20model%20for%20the%0AMAPF%20problems%20called%20MAPF-GPT.%20Using%20imitation%20learning%2C%20we%20have%20trained%20a%0Apolicy%20on%20a%20set%20of%20pre-collected%20sub-optimal%20expert%20trajectories%20that%20can%0Agenerate%20actions%20in%20conditions%20of%20partial%20observability%20without%20additional%0Aheuristics%2C%20reward%20functions%2C%20or%20communication%20with%20other%20agents.%20The%20resulting%0AMAPF-GPT%20model%20demonstrates%20zero-shot%20learning%20abilities%20when%20solving%20the%20MAPF%0Aproblem%20instances%20that%20were%20not%20present%20in%20the%20training%20dataset.%20We%20show%20that%0AMAPF-GPT%20notably%20outperforms%20the%20current%20best-performing%20learnable-MAPF%20solvers%0Aon%20a%20diverse%20range%20of%20problem%20instances%20and%20is%20efficient%20in%20terms%20of%0Acomputation%20%28in%20the%20inference%20mode%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.00134v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAPF-GPT%253A%2520Imitation%2520Learning%2520for%2520Multi-Agent%2520Pathfinding%2520at%2520Scale%26entry.906535625%3DAnton%2520Andreychuk%2520and%2520Konstantin%2520Yakovlev%2520and%2520Aleksandr%2520Panov%2520and%2520Alexey%2520Skrynnik%26entry.1292438233%3D%2520%2520Multi-agent%2520pathfinding%2520%2528MAPF%2529%2520is%2520a%2520challenging%2520computational%2520problem%2520that%250Atypically%2520requires%2520to%2520find%2520collision-free%2520paths%2520for%2520multiple%2520agents%2520in%2520a%2520shared%250Aenvironment.%2520Solving%2520MAPF%2520optimally%2520is%2520NP-hard%252C%2520yet%2520efficient%2520solutions%2520are%250Acritical%2520for%2520numerous%2520applications%252C%2520including%2520automated%2520warehouses%2520and%250Atransportation%2520systems.%2520Recently%252C%2520learning-based%2520approaches%2520to%2520MAPF%2520have%2520gained%250Aattention%252C%2520particularly%2520those%2520leveraging%2520deep%2520reinforcement%2520learning.%2520Following%250Acurrent%2520trends%2520in%2520machine%2520learning%252C%2520we%2520have%2520created%2520a%2520foundation%2520model%2520for%2520the%250AMAPF%2520problems%2520called%2520MAPF-GPT.%2520Using%2520imitation%2520learning%252C%2520we%2520have%2520trained%2520a%250Apolicy%2520on%2520a%2520set%2520of%2520pre-collected%2520sub-optimal%2520expert%2520trajectories%2520that%2520can%250Agenerate%2520actions%2520in%2520conditions%2520of%2520partial%2520observability%2520without%2520additional%250Aheuristics%252C%2520reward%2520functions%252C%2520or%2520communication%2520with%2520other%2520agents.%2520The%2520resulting%250AMAPF-GPT%2520model%2520demonstrates%2520zero-shot%2520learning%2520abilities%2520when%2520solving%2520the%2520MAPF%250Aproblem%2520instances%2520that%2520were%2520not%2520present%2520in%2520the%2520training%2520dataset.%2520We%2520show%2520that%250AMAPF-GPT%2520notably%2520outperforms%2520the%2520current%2520best-performing%2520learnable-MAPF%2520solvers%250Aon%2520a%2520diverse%2520range%2520of%2520problem%2520instances%2520and%2520is%2520efficient%2520in%2520terms%2520of%250Acomputation%2520%2528in%2520the%2520inference%2520mode%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.00134v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAPF-GPT%3A%20Imitation%20Learning%20for%20Multi-Agent%20Pathfinding%20at%20Scale&entry.906535625=Anton%20Andreychuk%20and%20Konstantin%20Yakovlev%20and%20Aleksandr%20Panov%20and%20Alexey%20Skrynnik&entry.1292438233=%20%20Multi-agent%20pathfinding%20%28MAPF%29%20is%20a%20challenging%20computational%20problem%20that%0Atypically%20requires%20to%20find%20collision-free%20paths%20for%20multiple%20agents%20in%20a%20shared%0Aenvironment.%20Solving%20MAPF%20optimally%20is%20NP-hard%2C%20yet%20efficient%20solutions%20are%0Acritical%20for%20numerous%20applications%2C%20including%20automated%20warehouses%20and%0Atransportation%20systems.%20Recently%2C%20learning-based%20approaches%20to%20MAPF%20have%20gained%0Aattention%2C%20particularly%20those%20leveraging%20deep%20reinforcement%20learning.%20Following%0Acurrent%20trends%20in%20machine%20learning%2C%20we%20have%20created%20a%20foundation%20model%20for%20the%0AMAPF%20problems%20called%20MAPF-GPT.%20Using%20imitation%20learning%2C%20we%20have%20trained%20a%0Apolicy%20on%20a%20set%20of%20pre-collected%20sub-optimal%20expert%20trajectories%20that%20can%0Agenerate%20actions%20in%20conditions%20of%20partial%20observability%20without%20additional%0Aheuristics%2C%20reward%20functions%2C%20or%20communication%20with%20other%20agents.%20The%20resulting%0AMAPF-GPT%20model%20demonstrates%20zero-shot%20learning%20abilities%20when%20solving%20the%20MAPF%0Aproblem%20instances%20that%20were%20not%20present%20in%20the%20training%20dataset.%20We%20show%20that%0AMAPF-GPT%20notably%20outperforms%20the%20current%20best-performing%20learnable-MAPF%20solvers%0Aon%20a%20diverse%20range%20of%20problem%20instances%20and%20is%20efficient%20in%20terms%20of%0Acomputation%20%28in%20the%20inference%20mode%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.00134v3&entry.124074799=Read"},
{"title": "LiverUSRecon: Automatic 3D Reconstruction and Volumetry of the Liver\n  with a Few Partial Ultrasound Scans", "author": "Kaushalya Sivayogaraj and Sahan T. Guruge and Udari Liyanage and Jeevani Udupihille and Saroj Jayasinghe and Gerard Fernando and Ranga Rodrigo and M. Rukshani Liyanaarachchi", "abstract": "  3D reconstruction of the liver for volumetry is important for qualitative\nanalysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,\nalthough advantageous due to less acquisition time and safety, is challenging\ndue to the inherent noisiness in US scans, blurry boundaries, and partial liver\nvisibility. We address these challenges by using the segmentation masks of a\nfew incomplete sagittal-plane US scans of the liver in conjunction with a\nstatistical shape model (SSM) built using a set of CT scans of the liver. We\ncompute the shape parameters needed to warp this canonical SSM to fit the US\nscans through a parametric regression network. The resulting 3D liver\nreconstruction is accurate and leads to automatic liver volume calculation. We\nevaluate the accuracy of the estimated liver volumes with respect to CT\nsegmentation volumes using RMSE. Our volume computation is statistically much\ncloser to the volume estimated using CT scans than the volume computed using\nChilds' method by radiologists: p-value of 0.094 (>0.05) says that there is no\nsignificant difference between CT segmentation volumes and ours in contrast to\nChilds' method. We validate our method using investigations (ablation studies)\non the US image resolution, the number of CT scans used for SSM, the number of\nprincipal components, and the number of input US scans. To the best of our\nknowledge, this is the first automatic liver volumetry system using a few\nincomplete US scans given a set of CT scans of livers for SSM.\n", "link": "http://arxiv.org/abs/2406.19336v3", "date": "2024-09-25", "relevancy": 2.0763, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5324}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5145}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiverUSRecon%3A%20Automatic%203D%20Reconstruction%20and%20Volumetry%20of%20the%20Liver%0A%20%20with%20a%20Few%20Partial%20Ultrasound%20Scans&body=Title%3A%20LiverUSRecon%3A%20Automatic%203D%20Reconstruction%20and%20Volumetry%20of%20the%20Liver%0A%20%20with%20a%20Few%20Partial%20Ultrasound%20Scans%0AAuthor%3A%20Kaushalya%20Sivayogaraj%20and%20Sahan%20T.%20Guruge%20and%20Udari%20Liyanage%20and%20Jeevani%20Udupihille%20and%20Saroj%20Jayasinghe%20and%20Gerard%20Fernando%20and%20Ranga%20Rodrigo%20and%20M.%20Rukshani%20Liyanaarachchi%0AAbstract%3A%20%20%203D%20reconstruction%20of%20the%20liver%20for%20volumetry%20is%20important%20for%20qualitative%0Aanalysis%20and%20disease%20diagnosis.%20Liver%20volumetry%20using%20ultrasound%20%28US%29%20scans%2C%0Aalthough%20advantageous%20due%20to%20less%20acquisition%20time%20and%20safety%2C%20is%20challenging%0Adue%20to%20the%20inherent%20noisiness%20in%20US%20scans%2C%20blurry%20boundaries%2C%20and%20partial%20liver%0Avisibility.%20We%20address%20these%20challenges%20by%20using%20the%20segmentation%20masks%20of%20a%0Afew%20incomplete%20sagittal-plane%20US%20scans%20of%20the%20liver%20in%20conjunction%20with%20a%0Astatistical%20shape%20model%20%28SSM%29%20built%20using%20a%20set%20of%20CT%20scans%20of%20the%20liver.%20We%0Acompute%20the%20shape%20parameters%20needed%20to%20warp%20this%20canonical%20SSM%20to%20fit%20the%20US%0Ascans%20through%20a%20parametric%20regression%20network.%20The%20resulting%203D%20liver%0Areconstruction%20is%20accurate%20and%20leads%20to%20automatic%20liver%20volume%20calculation.%20We%0Aevaluate%20the%20accuracy%20of%20the%20estimated%20liver%20volumes%20with%20respect%20to%20CT%0Asegmentation%20volumes%20using%20RMSE.%20Our%20volume%20computation%20is%20statistically%20much%0Acloser%20to%20the%20volume%20estimated%20using%20CT%20scans%20than%20the%20volume%20computed%20using%0AChilds%27%20method%20by%20radiologists%3A%20p-value%20of%200.094%20%28%3E0.05%29%20says%20that%20there%20is%20no%0Asignificant%20difference%20between%20CT%20segmentation%20volumes%20and%20ours%20in%20contrast%20to%0AChilds%27%20method.%20We%20validate%20our%20method%20using%20investigations%20%28ablation%20studies%29%0Aon%20the%20US%20image%20resolution%2C%20the%20number%20of%20CT%20scans%20used%20for%20SSM%2C%20the%20number%20of%0Aprincipal%20components%2C%20and%20the%20number%20of%20input%20US%20scans.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20automatic%20liver%20volumetry%20system%20using%20a%20few%0Aincomplete%20US%20scans%20given%20a%20set%20of%20CT%20scans%20of%20livers%20for%20SSM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19336v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiverUSRecon%253A%2520Automatic%25203D%2520Reconstruction%2520and%2520Volumetry%2520of%2520the%2520Liver%250A%2520%2520with%2520a%2520Few%2520Partial%2520Ultrasound%2520Scans%26entry.906535625%3DKaushalya%2520Sivayogaraj%2520and%2520Sahan%2520T.%2520Guruge%2520and%2520Udari%2520Liyanage%2520and%2520Jeevani%2520Udupihille%2520and%2520Saroj%2520Jayasinghe%2520and%2520Gerard%2520Fernando%2520and%2520Ranga%2520Rodrigo%2520and%2520M.%2520Rukshani%2520Liyanaarachchi%26entry.1292438233%3D%2520%25203D%2520reconstruction%2520of%2520the%2520liver%2520for%2520volumetry%2520is%2520important%2520for%2520qualitative%250Aanalysis%2520and%2520disease%2520diagnosis.%2520Liver%2520volumetry%2520using%2520ultrasound%2520%2528US%2529%2520scans%252C%250Aalthough%2520advantageous%2520due%2520to%2520less%2520acquisition%2520time%2520and%2520safety%252C%2520is%2520challenging%250Adue%2520to%2520the%2520inherent%2520noisiness%2520in%2520US%2520scans%252C%2520blurry%2520boundaries%252C%2520and%2520partial%2520liver%250Avisibility.%2520We%2520address%2520these%2520challenges%2520by%2520using%2520the%2520segmentation%2520masks%2520of%2520a%250Afew%2520incomplete%2520sagittal-plane%2520US%2520scans%2520of%2520the%2520liver%2520in%2520conjunction%2520with%2520a%250Astatistical%2520shape%2520model%2520%2528SSM%2529%2520built%2520using%2520a%2520set%2520of%2520CT%2520scans%2520of%2520the%2520liver.%2520We%250Acompute%2520the%2520shape%2520parameters%2520needed%2520to%2520warp%2520this%2520canonical%2520SSM%2520to%2520fit%2520the%2520US%250Ascans%2520through%2520a%2520parametric%2520regression%2520network.%2520The%2520resulting%25203D%2520liver%250Areconstruction%2520is%2520accurate%2520and%2520leads%2520to%2520automatic%2520liver%2520volume%2520calculation.%2520We%250Aevaluate%2520the%2520accuracy%2520of%2520the%2520estimated%2520liver%2520volumes%2520with%2520respect%2520to%2520CT%250Asegmentation%2520volumes%2520using%2520RMSE.%2520Our%2520volume%2520computation%2520is%2520statistically%2520much%250Acloser%2520to%2520the%2520volume%2520estimated%2520using%2520CT%2520scans%2520than%2520the%2520volume%2520computed%2520using%250AChilds%2527%2520method%2520by%2520radiologists%253A%2520p-value%2520of%25200.094%2520%2528%253E0.05%2529%2520says%2520that%2520there%2520is%2520no%250Asignificant%2520difference%2520between%2520CT%2520segmentation%2520volumes%2520and%2520ours%2520in%2520contrast%2520to%250AChilds%2527%2520method.%2520We%2520validate%2520our%2520method%2520using%2520investigations%2520%2528ablation%2520studies%2529%250Aon%2520the%2520US%2520image%2520resolution%252C%2520the%2520number%2520of%2520CT%2520scans%2520used%2520for%2520SSM%252C%2520the%2520number%2520of%250Aprincipal%2520components%252C%2520and%2520the%2520number%2520of%2520input%2520US%2520scans.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520this%2520is%2520the%2520first%2520automatic%2520liver%2520volumetry%2520system%2520using%2520a%2520few%250Aincomplete%2520US%2520scans%2520given%2520a%2520set%2520of%2520CT%2520scans%2520of%2520livers%2520for%2520SSM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19336v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiverUSRecon%3A%20Automatic%203D%20Reconstruction%20and%20Volumetry%20of%20the%20Liver%0A%20%20with%20a%20Few%20Partial%20Ultrasound%20Scans&entry.906535625=Kaushalya%20Sivayogaraj%20and%20Sahan%20T.%20Guruge%20and%20Udari%20Liyanage%20and%20Jeevani%20Udupihille%20and%20Saroj%20Jayasinghe%20and%20Gerard%20Fernando%20and%20Ranga%20Rodrigo%20and%20M.%20Rukshani%20Liyanaarachchi&entry.1292438233=%20%203D%20reconstruction%20of%20the%20liver%20for%20volumetry%20is%20important%20for%20qualitative%0Aanalysis%20and%20disease%20diagnosis.%20Liver%20volumetry%20using%20ultrasound%20%28US%29%20scans%2C%0Aalthough%20advantageous%20due%20to%20less%20acquisition%20time%20and%20safety%2C%20is%20challenging%0Adue%20to%20the%20inherent%20noisiness%20in%20US%20scans%2C%20blurry%20boundaries%2C%20and%20partial%20liver%0Avisibility.%20We%20address%20these%20challenges%20by%20using%20the%20segmentation%20masks%20of%20a%0Afew%20incomplete%20sagittal-plane%20US%20scans%20of%20the%20liver%20in%20conjunction%20with%20a%0Astatistical%20shape%20model%20%28SSM%29%20built%20using%20a%20set%20of%20CT%20scans%20of%20the%20liver.%20We%0Acompute%20the%20shape%20parameters%20needed%20to%20warp%20this%20canonical%20SSM%20to%20fit%20the%20US%0Ascans%20through%20a%20parametric%20regression%20network.%20The%20resulting%203D%20liver%0Areconstruction%20is%20accurate%20and%20leads%20to%20automatic%20liver%20volume%20calculation.%20We%0Aevaluate%20the%20accuracy%20of%20the%20estimated%20liver%20volumes%20with%20respect%20to%20CT%0Asegmentation%20volumes%20using%20RMSE.%20Our%20volume%20computation%20is%20statistically%20much%0Acloser%20to%20the%20volume%20estimated%20using%20CT%20scans%20than%20the%20volume%20computed%20using%0AChilds%27%20method%20by%20radiologists%3A%20p-value%20of%200.094%20%28%3E0.05%29%20says%20that%20there%20is%20no%0Asignificant%20difference%20between%20CT%20segmentation%20volumes%20and%20ours%20in%20contrast%20to%0AChilds%27%20method.%20We%20validate%20our%20method%20using%20investigations%20%28ablation%20studies%29%0Aon%20the%20US%20image%20resolution%2C%20the%20number%20of%20CT%20scans%20used%20for%20SSM%2C%20the%20number%20of%0Aprincipal%20components%2C%20and%20the%20number%20of%20input%20US%20scans.%20To%20the%20best%20of%20our%0Aknowledge%2C%20this%20is%20the%20first%20automatic%20liver%20volumetry%20system%20using%20a%20few%0Aincomplete%20US%20scans%20given%20a%20set%20of%20CT%20scans%20of%20livers%20for%20SSM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19336v3&entry.124074799=Read"},
{"title": "Hyperbolic Metric Learning for Visual Outlier Detection", "author": "Alvaro Gonzalez-Jimenez and Simone Lionetti and Dena Bazazian and Philippe Gottfrois and Fabian Gr\u00f6ger and Marc Pouly and Alexander Navarini", "abstract": "  Out-Of-Distribution (OOD) detection is critical to deploy deep learning\nmodels in safety-critical applications. However, the inherent hierarchical\nconcept structure of visual data, which is instrumental to OOD detection, is\noften poorly captured by conventional methods based on Euclidean geometry. This\nwork proposes a metric framework that leverages the strengths of Hyperbolic\ngeometry for OOD detection. Inspired by previous works that refine the decision\nboundary for OOD data with synthetic outliers, we extend this method to\nHyperbolic space. Interestingly, we find that synthetic outliers do not benefit\nOOD detection in Hyperbolic space as they do in Euclidean space. Furthermore we\nexplore the relationship between OOD detection performance and Hyperbolic\nembedding dimension, addressing practical concerns in resource-constrained\nenvironments. Extensive experiments show that our framework improves the FPR95\nfor OOD detection from 22\\% to 15\\% and from 49% to 28% on CIFAR-10 and\nCIFAR-100 respectively compared to Euclidean methods.\n", "link": "http://arxiv.org/abs/2403.15260v2", "date": "2024-09-25", "relevancy": 2.0684, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5303}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5089}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperbolic%20Metric%20Learning%20for%20Visual%20Outlier%20Detection&body=Title%3A%20Hyperbolic%20Metric%20Learning%20for%20Visual%20Outlier%20Detection%0AAuthor%3A%20Alvaro%20Gonzalez-Jimenez%20and%20Simone%20Lionetti%20and%20Dena%20Bazazian%20and%20Philippe%20Gottfrois%20and%20Fabian%20Gr%C3%B6ger%20and%20Marc%20Pouly%20and%20Alexander%20Navarini%0AAbstract%3A%20%20%20Out-Of-Distribution%20%28OOD%29%20detection%20is%20critical%20to%20deploy%20deep%20learning%0Amodels%20in%20safety-critical%20applications.%20However%2C%20the%20inherent%20hierarchical%0Aconcept%20structure%20of%20visual%20data%2C%20which%20is%20instrumental%20to%20OOD%20detection%2C%20is%0Aoften%20poorly%20captured%20by%20conventional%20methods%20based%20on%20Euclidean%20geometry.%20This%0Awork%20proposes%20a%20metric%20framework%20that%20leverages%20the%20strengths%20of%20Hyperbolic%0Ageometry%20for%20OOD%20detection.%20Inspired%20by%20previous%20works%20that%20refine%20the%20decision%0Aboundary%20for%20OOD%20data%20with%20synthetic%20outliers%2C%20we%20extend%20this%20method%20to%0AHyperbolic%20space.%20Interestingly%2C%20we%20find%20that%20synthetic%20outliers%20do%20not%20benefit%0AOOD%20detection%20in%20Hyperbolic%20space%20as%20they%20do%20in%20Euclidean%20space.%20Furthermore%20we%0Aexplore%20the%20relationship%20between%20OOD%20detection%20performance%20and%20Hyperbolic%0Aembedding%20dimension%2C%20addressing%20practical%20concerns%20in%20resource-constrained%0Aenvironments.%20Extensive%20experiments%20show%20that%20our%20framework%20improves%20the%20FPR95%0Afor%20OOD%20detection%20from%2022%5C%25%20to%2015%5C%25%20and%20from%2049%25%20to%2028%25%20on%20CIFAR-10%20and%0ACIFAR-100%20respectively%20compared%20to%20Euclidean%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15260v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperbolic%2520Metric%2520Learning%2520for%2520Visual%2520Outlier%2520Detection%26entry.906535625%3DAlvaro%2520Gonzalez-Jimenez%2520and%2520Simone%2520Lionetti%2520and%2520Dena%2520Bazazian%2520and%2520Philippe%2520Gottfrois%2520and%2520Fabian%2520Gr%25C3%25B6ger%2520and%2520Marc%2520Pouly%2520and%2520Alexander%2520Navarini%26entry.1292438233%3D%2520%2520Out-Of-Distribution%2520%2528OOD%2529%2520detection%2520is%2520critical%2520to%2520deploy%2520deep%2520learning%250Amodels%2520in%2520safety-critical%2520applications.%2520However%252C%2520the%2520inherent%2520hierarchical%250Aconcept%2520structure%2520of%2520visual%2520data%252C%2520which%2520is%2520instrumental%2520to%2520OOD%2520detection%252C%2520is%250Aoften%2520poorly%2520captured%2520by%2520conventional%2520methods%2520based%2520on%2520Euclidean%2520geometry.%2520This%250Awork%2520proposes%2520a%2520metric%2520framework%2520that%2520leverages%2520the%2520strengths%2520of%2520Hyperbolic%250Ageometry%2520for%2520OOD%2520detection.%2520Inspired%2520by%2520previous%2520works%2520that%2520refine%2520the%2520decision%250Aboundary%2520for%2520OOD%2520data%2520with%2520synthetic%2520outliers%252C%2520we%2520extend%2520this%2520method%2520to%250AHyperbolic%2520space.%2520Interestingly%252C%2520we%2520find%2520that%2520synthetic%2520outliers%2520do%2520not%2520benefit%250AOOD%2520detection%2520in%2520Hyperbolic%2520space%2520as%2520they%2520do%2520in%2520Euclidean%2520space.%2520Furthermore%2520we%250Aexplore%2520the%2520relationship%2520between%2520OOD%2520detection%2520performance%2520and%2520Hyperbolic%250Aembedding%2520dimension%252C%2520addressing%2520practical%2520concerns%2520in%2520resource-constrained%250Aenvironments.%2520Extensive%2520experiments%2520show%2520that%2520our%2520framework%2520improves%2520the%2520FPR95%250Afor%2520OOD%2520detection%2520from%252022%255C%2525%2520to%252015%255C%2525%2520and%2520from%252049%2525%2520to%252028%2525%2520on%2520CIFAR-10%2520and%250ACIFAR-100%2520respectively%2520compared%2520to%2520Euclidean%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15260v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperbolic%20Metric%20Learning%20for%20Visual%20Outlier%20Detection&entry.906535625=Alvaro%20Gonzalez-Jimenez%20and%20Simone%20Lionetti%20and%20Dena%20Bazazian%20and%20Philippe%20Gottfrois%20and%20Fabian%20Gr%C3%B6ger%20and%20Marc%20Pouly%20and%20Alexander%20Navarini&entry.1292438233=%20%20Out-Of-Distribution%20%28OOD%29%20detection%20is%20critical%20to%20deploy%20deep%20learning%0Amodels%20in%20safety-critical%20applications.%20However%2C%20the%20inherent%20hierarchical%0Aconcept%20structure%20of%20visual%20data%2C%20which%20is%20instrumental%20to%20OOD%20detection%2C%20is%0Aoften%20poorly%20captured%20by%20conventional%20methods%20based%20on%20Euclidean%20geometry.%20This%0Awork%20proposes%20a%20metric%20framework%20that%20leverages%20the%20strengths%20of%20Hyperbolic%0Ageometry%20for%20OOD%20detection.%20Inspired%20by%20previous%20works%20that%20refine%20the%20decision%0Aboundary%20for%20OOD%20data%20with%20synthetic%20outliers%2C%20we%20extend%20this%20method%20to%0AHyperbolic%20space.%20Interestingly%2C%20we%20find%20that%20synthetic%20outliers%20do%20not%20benefit%0AOOD%20detection%20in%20Hyperbolic%20space%20as%20they%20do%20in%20Euclidean%20space.%20Furthermore%20we%0Aexplore%20the%20relationship%20between%20OOD%20detection%20performance%20and%20Hyperbolic%0Aembedding%20dimension%2C%20addressing%20practical%20concerns%20in%20resource-constrained%0Aenvironments.%20Extensive%20experiments%20show%20that%20our%20framework%20improves%20the%20FPR95%0Afor%20OOD%20detection%20from%2022%5C%25%20to%2015%5C%25%20and%20from%2049%25%20to%2028%25%20on%20CIFAR-10%20and%0ACIFAR-100%20respectively%20compared%20to%20Euclidean%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15260v2&entry.124074799=Read"},
{"title": "Attention Prompting on Image for Large Vision-Language Models", "author": "Runpeng Yu and Weihao Yu and Xinchao Wang", "abstract": "  Compared with Large Language Models (LLMs), Large Vision-Language Models\n(LVLMs) can also accept images as input, thus showcasing more interesting\nemergent capabilities and demonstrating impressive performance on various\nvision-language tasks. Motivated by text prompting in LLMs, visual prompting\nhas been explored to enhance LVLMs' capabilities of perceiving visual\ninformation. However, previous visual prompting techniques solely process\nvisual inputs without considering text queries, limiting the models' ability to\nfollow text instructions to complete tasks. To fill this gap, in this work, we\npropose a new prompting technique named Attention Prompting on Image, which\njust simply overlays a text-query-guided attention heatmap on the original\ninput image and effectively enhances LVLM on various tasks. Specifically, we\ngenerate an attention heatmap for the input image dependent on the text query\nwith an auxiliary model like CLIP. Then the heatmap simply multiplies the pixel\nvalues of the original image to obtain the actual input image for the LVLM.\nExtensive experiments on various vison-language benchmarks verify the\neffectiveness of our technique. For example, Attention Prompting on Image\nimproves LLaVA-1.5 by 3.8% and 2.9% on MM-Vet and LLaVA-Wild benchmarks,\nrespectively.\n", "link": "http://arxiv.org/abs/2409.17143v1", "date": "2024-09-25", "relevancy": 2.0635, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5184}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5154}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5154}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20Prompting%20on%20Image%20for%20Large%20Vision-Language%20Models&body=Title%3A%20Attention%20Prompting%20on%20Image%20for%20Large%20Vision-Language%20Models%0AAuthor%3A%20Runpeng%20Yu%20and%20Weihao%20Yu%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Compared%20with%20Large%20Language%20Models%20%28LLMs%29%2C%20Large%20Vision-Language%20Models%0A%28LVLMs%29%20can%20also%20accept%20images%20as%20input%2C%20thus%20showcasing%20more%20interesting%0Aemergent%20capabilities%20and%20demonstrating%20impressive%20performance%20on%20various%0Avision-language%20tasks.%20Motivated%20by%20text%20prompting%20in%20LLMs%2C%20visual%20prompting%0Ahas%20been%20explored%20to%20enhance%20LVLMs%27%20capabilities%20of%20perceiving%20visual%0Ainformation.%20However%2C%20previous%20visual%20prompting%20techniques%20solely%20process%0Avisual%20inputs%20without%20considering%20text%20queries%2C%20limiting%20the%20models%27%20ability%20to%0Afollow%20text%20instructions%20to%20complete%20tasks.%20To%20fill%20this%20gap%2C%20in%20this%20work%2C%20we%0Apropose%20a%20new%20prompting%20technique%20named%20Attention%20Prompting%20on%20Image%2C%20which%0Ajust%20simply%20overlays%20a%20text-query-guided%20attention%20heatmap%20on%20the%20original%0Ainput%20image%20and%20effectively%20enhances%20LVLM%20on%20various%20tasks.%20Specifically%2C%20we%0Agenerate%20an%20attention%20heatmap%20for%20the%20input%20image%20dependent%20on%20the%20text%20query%0Awith%20an%20auxiliary%20model%20like%20CLIP.%20Then%20the%20heatmap%20simply%20multiplies%20the%20pixel%0Avalues%20of%20the%20original%20image%20to%20obtain%20the%20actual%20input%20image%20for%20the%20LVLM.%0AExtensive%20experiments%20on%20various%20vison-language%20benchmarks%20verify%20the%0Aeffectiveness%20of%20our%20technique.%20For%20example%2C%20Attention%20Prompting%20on%20Image%0Aimproves%20LLaVA-1.5%20by%203.8%25%20and%202.9%25%20on%20MM-Vet%20and%20LLaVA-Wild%20benchmarks%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17143v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520Prompting%2520on%2520Image%2520for%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DRunpeng%2520Yu%2520and%2520Weihao%2520Yu%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Compared%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520Large%2520Vision-Language%2520Models%250A%2528LVLMs%2529%2520can%2520also%2520accept%2520images%2520as%2520input%252C%2520thus%2520showcasing%2520more%2520interesting%250Aemergent%2520capabilities%2520and%2520demonstrating%2520impressive%2520performance%2520on%2520various%250Avision-language%2520tasks.%2520Motivated%2520by%2520text%2520prompting%2520in%2520LLMs%252C%2520visual%2520prompting%250Ahas%2520been%2520explored%2520to%2520enhance%2520LVLMs%2527%2520capabilities%2520of%2520perceiving%2520visual%250Ainformation.%2520However%252C%2520previous%2520visual%2520prompting%2520techniques%2520solely%2520process%250Avisual%2520inputs%2520without%2520considering%2520text%2520queries%252C%2520limiting%2520the%2520models%2527%2520ability%2520to%250Afollow%2520text%2520instructions%2520to%2520complete%2520tasks.%2520To%2520fill%2520this%2520gap%252C%2520in%2520this%2520work%252C%2520we%250Apropose%2520a%2520new%2520prompting%2520technique%2520named%2520Attention%2520Prompting%2520on%2520Image%252C%2520which%250Ajust%2520simply%2520overlays%2520a%2520text-query-guided%2520attention%2520heatmap%2520on%2520the%2520original%250Ainput%2520image%2520and%2520effectively%2520enhances%2520LVLM%2520on%2520various%2520tasks.%2520Specifically%252C%2520we%250Agenerate%2520an%2520attention%2520heatmap%2520for%2520the%2520input%2520image%2520dependent%2520on%2520the%2520text%2520query%250Awith%2520an%2520auxiliary%2520model%2520like%2520CLIP.%2520Then%2520the%2520heatmap%2520simply%2520multiplies%2520the%2520pixel%250Avalues%2520of%2520the%2520original%2520image%2520to%2520obtain%2520the%2520actual%2520input%2520image%2520for%2520the%2520LVLM.%250AExtensive%2520experiments%2520on%2520various%2520vison-language%2520benchmarks%2520verify%2520the%250Aeffectiveness%2520of%2520our%2520technique.%2520For%2520example%252C%2520Attention%2520Prompting%2520on%2520Image%250Aimproves%2520LLaVA-1.5%2520by%25203.8%2525%2520and%25202.9%2525%2520on%2520MM-Vet%2520and%2520LLaVA-Wild%2520benchmarks%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17143v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Prompting%20on%20Image%20for%20Large%20Vision-Language%20Models&entry.906535625=Runpeng%20Yu%20and%20Weihao%20Yu%20and%20Xinchao%20Wang&entry.1292438233=%20%20Compared%20with%20Large%20Language%20Models%20%28LLMs%29%2C%20Large%20Vision-Language%20Models%0A%28LVLMs%29%20can%20also%20accept%20images%20as%20input%2C%20thus%20showcasing%20more%20interesting%0Aemergent%20capabilities%20and%20demonstrating%20impressive%20performance%20on%20various%0Avision-language%20tasks.%20Motivated%20by%20text%20prompting%20in%20LLMs%2C%20visual%20prompting%0Ahas%20been%20explored%20to%20enhance%20LVLMs%27%20capabilities%20of%20perceiving%20visual%0Ainformation.%20However%2C%20previous%20visual%20prompting%20techniques%20solely%20process%0Avisual%20inputs%20without%20considering%20text%20queries%2C%20limiting%20the%20models%27%20ability%20to%0Afollow%20text%20instructions%20to%20complete%20tasks.%20To%20fill%20this%20gap%2C%20in%20this%20work%2C%20we%0Apropose%20a%20new%20prompting%20technique%20named%20Attention%20Prompting%20on%20Image%2C%20which%0Ajust%20simply%20overlays%20a%20text-query-guided%20attention%20heatmap%20on%20the%20original%0Ainput%20image%20and%20effectively%20enhances%20LVLM%20on%20various%20tasks.%20Specifically%2C%20we%0Agenerate%20an%20attention%20heatmap%20for%20the%20input%20image%20dependent%20on%20the%20text%20query%0Awith%20an%20auxiliary%20model%20like%20CLIP.%20Then%20the%20heatmap%20simply%20multiplies%20the%20pixel%0Avalues%20of%20the%20original%20image%20to%20obtain%20the%20actual%20input%20image%20for%20the%20LVLM.%0AExtensive%20experiments%20on%20various%20vison-language%20benchmarks%20verify%20the%0Aeffectiveness%20of%20our%20technique.%20For%20example%2C%20Attention%20Prompting%20on%20Image%0Aimproves%20LLaVA-1.5%20by%203.8%25%20and%202.9%25%20on%20MM-Vet%20and%20LLaVA-Wild%20benchmarks%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17143v1&entry.124074799=Read"},
{"title": "Enhanced Wavelet Scattering Network for image inpainting detection", "author": "Barglazan Adrian-Alin and Brad Remus", "abstract": "  The rapid advancement of image inpainting tools, especially those aimed at\nremoving artifacts, has made digital image manipulation alarmingly accessible.\nThis paper proposes several innovative ideas for detecting inpainting forgeries\nbased on low level noise analysis by combining Dual-Tree Complex Wavelet\nTransform (DT-CWT) for feature extraction with convolutional neural networks\n(CNN) for forged area detection and localization, and lastly by employing an\ninnovative combination of texture segmentation with noise variance estimations.\nThe DT-CWT offers significant advantages due to its shift-invariance, enhancing\nits robustness against subtle manipulations during the inpainting process.\nFurthermore, its directional selectivity allows for the detection of subtle\nartifacts introduced by inpainting within specific frequency bands and\norientations. Various neural network architectures were evaluated and proposed.\nLastly, we propose a fusion detection module that combines texture analysis\nwith noise variance estimation to give the forged area. Our approach was\nbenchmarked against state-of-the-art methods and demonstrated superior\nperformance over all cited alternatives. The training code (with pretrained\nmodel weights) as long as the dataset will be available at\nhttps://github.com/jmaba/Deep-dual-tree-complex-neural-network-for-image-inpainting-detection\n", "link": "http://arxiv.org/abs/2409.17023v1", "date": "2024-09-25", "relevancy": 2.057, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5246}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5129}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Wavelet%20Scattering%20Network%20for%20image%20inpainting%20detection&body=Title%3A%20Enhanced%20Wavelet%20Scattering%20Network%20for%20image%20inpainting%20detection%0AAuthor%3A%20Barglazan%20Adrian-Alin%20and%20Brad%20Remus%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20image%20inpainting%20tools%2C%20especially%20those%20aimed%20at%0Aremoving%20artifacts%2C%20has%20made%20digital%20image%20manipulation%20alarmingly%20accessible.%0AThis%20paper%20proposes%20several%20innovative%20ideas%20for%20detecting%20inpainting%20forgeries%0Abased%20on%20low%20level%20noise%20analysis%20by%20combining%20Dual-Tree%20Complex%20Wavelet%0ATransform%20%28DT-CWT%29%20for%20feature%20extraction%20with%20convolutional%20neural%20networks%0A%28CNN%29%20for%20forged%20area%20detection%20and%20localization%2C%20and%20lastly%20by%20employing%20an%0Ainnovative%20combination%20of%20texture%20segmentation%20with%20noise%20variance%20estimations.%0AThe%20DT-CWT%20offers%20significant%20advantages%20due%20to%20its%20shift-invariance%2C%20enhancing%0Aits%20robustness%20against%20subtle%20manipulations%20during%20the%20inpainting%20process.%0AFurthermore%2C%20its%20directional%20selectivity%20allows%20for%20the%20detection%20of%20subtle%0Aartifacts%20introduced%20by%20inpainting%20within%20specific%20frequency%20bands%20and%0Aorientations.%20Various%20neural%20network%20architectures%20were%20evaluated%20and%20proposed.%0ALastly%2C%20we%20propose%20a%20fusion%20detection%20module%20that%20combines%20texture%20analysis%0Awith%20noise%20variance%20estimation%20to%20give%20the%20forged%20area.%20Our%20approach%20was%0Abenchmarked%20against%20state-of-the-art%20methods%20and%20demonstrated%20superior%0Aperformance%20over%20all%20cited%20alternatives.%20The%20training%20code%20%28with%20pretrained%0Amodel%20weights%29%20as%20long%20as%20the%20dataset%20will%20be%20available%20at%0Ahttps%3A//github.com/jmaba/Deep-dual-tree-complex-neural-network-for-image-inpainting-detection%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17023v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Wavelet%2520Scattering%2520Network%2520for%2520image%2520inpainting%2520detection%26entry.906535625%3DBarglazan%2520Adrian-Alin%2520and%2520Brad%2520Remus%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520image%2520inpainting%2520tools%252C%2520especially%2520those%2520aimed%2520at%250Aremoving%2520artifacts%252C%2520has%2520made%2520digital%2520image%2520manipulation%2520alarmingly%2520accessible.%250AThis%2520paper%2520proposes%2520several%2520innovative%2520ideas%2520for%2520detecting%2520inpainting%2520forgeries%250Abased%2520on%2520low%2520level%2520noise%2520analysis%2520by%2520combining%2520Dual-Tree%2520Complex%2520Wavelet%250ATransform%2520%2528DT-CWT%2529%2520for%2520feature%2520extraction%2520with%2520convolutional%2520neural%2520networks%250A%2528CNN%2529%2520for%2520forged%2520area%2520detection%2520and%2520localization%252C%2520and%2520lastly%2520by%2520employing%2520an%250Ainnovative%2520combination%2520of%2520texture%2520segmentation%2520with%2520noise%2520variance%2520estimations.%250AThe%2520DT-CWT%2520offers%2520significant%2520advantages%2520due%2520to%2520its%2520shift-invariance%252C%2520enhancing%250Aits%2520robustness%2520against%2520subtle%2520manipulations%2520during%2520the%2520inpainting%2520process.%250AFurthermore%252C%2520its%2520directional%2520selectivity%2520allows%2520for%2520the%2520detection%2520of%2520subtle%250Aartifacts%2520introduced%2520by%2520inpainting%2520within%2520specific%2520frequency%2520bands%2520and%250Aorientations.%2520Various%2520neural%2520network%2520architectures%2520were%2520evaluated%2520and%2520proposed.%250ALastly%252C%2520we%2520propose%2520a%2520fusion%2520detection%2520module%2520that%2520combines%2520texture%2520analysis%250Awith%2520noise%2520variance%2520estimation%2520to%2520give%2520the%2520forged%2520area.%2520Our%2520approach%2520was%250Abenchmarked%2520against%2520state-of-the-art%2520methods%2520and%2520demonstrated%2520superior%250Aperformance%2520over%2520all%2520cited%2520alternatives.%2520The%2520training%2520code%2520%2528with%2520pretrained%250Amodel%2520weights%2529%2520as%2520long%2520as%2520the%2520dataset%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/jmaba/Deep-dual-tree-complex-neural-network-for-image-inpainting-detection%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17023v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Wavelet%20Scattering%20Network%20for%20image%20inpainting%20detection&entry.906535625=Barglazan%20Adrian-Alin%20and%20Brad%20Remus&entry.1292438233=%20%20The%20rapid%20advancement%20of%20image%20inpainting%20tools%2C%20especially%20those%20aimed%20at%0Aremoving%20artifacts%2C%20has%20made%20digital%20image%20manipulation%20alarmingly%20accessible.%0AThis%20paper%20proposes%20several%20innovative%20ideas%20for%20detecting%20inpainting%20forgeries%0Abased%20on%20low%20level%20noise%20analysis%20by%20combining%20Dual-Tree%20Complex%20Wavelet%0ATransform%20%28DT-CWT%29%20for%20feature%20extraction%20with%20convolutional%20neural%20networks%0A%28CNN%29%20for%20forged%20area%20detection%20and%20localization%2C%20and%20lastly%20by%20employing%20an%0Ainnovative%20combination%20of%20texture%20segmentation%20with%20noise%20variance%20estimations.%0AThe%20DT-CWT%20offers%20significant%20advantages%20due%20to%20its%20shift-invariance%2C%20enhancing%0Aits%20robustness%20against%20subtle%20manipulations%20during%20the%20inpainting%20process.%0AFurthermore%2C%20its%20directional%20selectivity%20allows%20for%20the%20detection%20of%20subtle%0Aartifacts%20introduced%20by%20inpainting%20within%20specific%20frequency%20bands%20and%0Aorientations.%20Various%20neural%20network%20architectures%20were%20evaluated%20and%20proposed.%0ALastly%2C%20we%20propose%20a%20fusion%20detection%20module%20that%20combines%20texture%20analysis%0Awith%20noise%20variance%20estimation%20to%20give%20the%20forged%20area.%20Our%20approach%20was%0Abenchmarked%20against%20state-of-the-art%20methods%20and%20demonstrated%20superior%0Aperformance%20over%20all%20cited%20alternatives.%20The%20training%20code%20%28with%20pretrained%0Amodel%20weights%29%20as%20long%20as%20the%20dataset%20will%20be%20available%20at%0Ahttps%3A//github.com/jmaba/Deep-dual-tree-complex-neural-network-for-image-inpainting-detection%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17023v1&entry.124074799=Read"},
{"title": "Predictive Covert Communication Against Multi-UAV Surveillance Using\n  Graph Koopman Autoencoder", "author": "Sivaram Krishnan and Jihong Park and Gregory Sherman and Benjamin Campbell and Jinho Choi", "abstract": "  Low Probability of Detection (LPD) communication aims to obscure the presence\nof radio frequency (RF) signals to evade surveillance. In the context of mobile\nsurveillance utilizing unmanned aerial vehicles (UAVs), achieving LPD\ncommunication presents significant challenges due to the UAVs' rapid and\ncontinuous movements, which are characterized by unknown nonlinear dynamics.\nTherefore, accurately predicting future locations of UAVs is essential for\nenabling real-time LPD communication. In this paper, we introduce a novel\nframework termed predictive covert communication, aimed at minimizing\ndetectability in terrestrial ad-hoc networks under multi-UAV surveillance. Our\ndata-driven method synergistically integrates graph neural networks (GNN) with\nKoopman theory to model the complex interactions within a multi-UAV network and\nfacilitating long-term predictions by linearizing the dynamics, even with\nlimited historical data. Extensive simulation results substantiate that the\npredicted trajectories using our method result in at least 63%-75% lower\nprobability of detection when compared to well-known state-of-the-art baseline\napproaches, showing promise in enabling low-latency covert operations in\npractical scenarios.\n", "link": "http://arxiv.org/abs/2409.17048v1", "date": "2024-09-25", "relevancy": 2.0513, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5339}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5046}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predictive%20Covert%20Communication%20Against%20Multi-UAV%20Surveillance%20Using%0A%20%20Graph%20Koopman%20Autoencoder&body=Title%3A%20Predictive%20Covert%20Communication%20Against%20Multi-UAV%20Surveillance%20Using%0A%20%20Graph%20Koopman%20Autoencoder%0AAuthor%3A%20Sivaram%20Krishnan%20and%20Jihong%20Park%20and%20Gregory%20Sherman%20and%20Benjamin%20Campbell%20and%20Jinho%20Choi%0AAbstract%3A%20%20%20Low%20Probability%20of%20Detection%20%28LPD%29%20communication%20aims%20to%20obscure%20the%20presence%0Aof%20radio%20frequency%20%28RF%29%20signals%20to%20evade%20surveillance.%20In%20the%20context%20of%20mobile%0Asurveillance%20utilizing%20unmanned%20aerial%20vehicles%20%28UAVs%29%2C%20achieving%20LPD%0Acommunication%20presents%20significant%20challenges%20due%20to%20the%20UAVs%27%20rapid%20and%0Acontinuous%20movements%2C%20which%20are%20characterized%20by%20unknown%20nonlinear%20dynamics.%0ATherefore%2C%20accurately%20predicting%20future%20locations%20of%20UAVs%20is%20essential%20for%0Aenabling%20real-time%20LPD%20communication.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Aframework%20termed%20predictive%20covert%20communication%2C%20aimed%20at%20minimizing%0Adetectability%20in%20terrestrial%20ad-hoc%20networks%20under%20multi-UAV%20surveillance.%20Our%0Adata-driven%20method%20synergistically%20integrates%20graph%20neural%20networks%20%28GNN%29%20with%0AKoopman%20theory%20to%20model%20the%20complex%20interactions%20within%20a%20multi-UAV%20network%20and%0Afacilitating%20long-term%20predictions%20by%20linearizing%20the%20dynamics%2C%20even%20with%0Alimited%20historical%20data.%20Extensive%20simulation%20results%20substantiate%20that%20the%0Apredicted%20trajectories%20using%20our%20method%20result%20in%20at%20least%2063%25-75%25%20lower%0Aprobability%20of%20detection%20when%20compared%20to%20well-known%20state-of-the-art%20baseline%0Aapproaches%2C%20showing%20promise%20in%20enabling%20low-latency%20covert%20operations%20in%0Apractical%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17048v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredictive%2520Covert%2520Communication%2520Against%2520Multi-UAV%2520Surveillance%2520Using%250A%2520%2520Graph%2520Koopman%2520Autoencoder%26entry.906535625%3DSivaram%2520Krishnan%2520and%2520Jihong%2520Park%2520and%2520Gregory%2520Sherman%2520and%2520Benjamin%2520Campbell%2520and%2520Jinho%2520Choi%26entry.1292438233%3D%2520%2520Low%2520Probability%2520of%2520Detection%2520%2528LPD%2529%2520communication%2520aims%2520to%2520obscure%2520the%2520presence%250Aof%2520radio%2520frequency%2520%2528RF%2529%2520signals%2520to%2520evade%2520surveillance.%2520In%2520the%2520context%2520of%2520mobile%250Asurveillance%2520utilizing%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%252C%2520achieving%2520LPD%250Acommunication%2520presents%2520significant%2520challenges%2520due%2520to%2520the%2520UAVs%2527%2520rapid%2520and%250Acontinuous%2520movements%252C%2520which%2520are%2520characterized%2520by%2520unknown%2520nonlinear%2520dynamics.%250ATherefore%252C%2520accurately%2520predicting%2520future%2520locations%2520of%2520UAVs%2520is%2520essential%2520for%250Aenabling%2520real-time%2520LPD%2520communication.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%250Aframework%2520termed%2520predictive%2520covert%2520communication%252C%2520aimed%2520at%2520minimizing%250Adetectability%2520in%2520terrestrial%2520ad-hoc%2520networks%2520under%2520multi-UAV%2520surveillance.%2520Our%250Adata-driven%2520method%2520synergistically%2520integrates%2520graph%2520neural%2520networks%2520%2528GNN%2529%2520with%250AKoopman%2520theory%2520to%2520model%2520the%2520complex%2520interactions%2520within%2520a%2520multi-UAV%2520network%2520and%250Afacilitating%2520long-term%2520predictions%2520by%2520linearizing%2520the%2520dynamics%252C%2520even%2520with%250Alimited%2520historical%2520data.%2520Extensive%2520simulation%2520results%2520substantiate%2520that%2520the%250Apredicted%2520trajectories%2520using%2520our%2520method%2520result%2520in%2520at%2520least%252063%2525-75%2525%2520lower%250Aprobability%2520of%2520detection%2520when%2520compared%2520to%2520well-known%2520state-of-the-art%2520baseline%250Aapproaches%252C%2520showing%2520promise%2520in%2520enabling%2520low-latency%2520covert%2520operations%2520in%250Apractical%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17048v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predictive%20Covert%20Communication%20Against%20Multi-UAV%20Surveillance%20Using%0A%20%20Graph%20Koopman%20Autoencoder&entry.906535625=Sivaram%20Krishnan%20and%20Jihong%20Park%20and%20Gregory%20Sherman%20and%20Benjamin%20Campbell%20and%20Jinho%20Choi&entry.1292438233=%20%20Low%20Probability%20of%20Detection%20%28LPD%29%20communication%20aims%20to%20obscure%20the%20presence%0Aof%20radio%20frequency%20%28RF%29%20signals%20to%20evade%20surveillance.%20In%20the%20context%20of%20mobile%0Asurveillance%20utilizing%20unmanned%20aerial%20vehicles%20%28UAVs%29%2C%20achieving%20LPD%0Acommunication%20presents%20significant%20challenges%20due%20to%20the%20UAVs%27%20rapid%20and%0Acontinuous%20movements%2C%20which%20are%20characterized%20by%20unknown%20nonlinear%20dynamics.%0ATherefore%2C%20accurately%20predicting%20future%20locations%20of%20UAVs%20is%20essential%20for%0Aenabling%20real-time%20LPD%20communication.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Aframework%20termed%20predictive%20covert%20communication%2C%20aimed%20at%20minimizing%0Adetectability%20in%20terrestrial%20ad-hoc%20networks%20under%20multi-UAV%20surveillance.%20Our%0Adata-driven%20method%20synergistically%20integrates%20graph%20neural%20networks%20%28GNN%29%20with%0AKoopman%20theory%20to%20model%20the%20complex%20interactions%20within%20a%20multi-UAV%20network%20and%0Afacilitating%20long-term%20predictions%20by%20linearizing%20the%20dynamics%2C%20even%20with%0Alimited%20historical%20data.%20Extensive%20simulation%20results%20substantiate%20that%20the%0Apredicted%20trajectories%20using%20our%20method%20result%20in%20at%20least%2063%25-75%25%20lower%0Aprobability%20of%20detection%20when%20compared%20to%20well-known%20state-of-the-art%20baseline%0Aapproaches%2C%20showing%20promise%20in%20enabling%20low-latency%20covert%20operations%20in%0Apractical%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17048v1&entry.124074799=Read"},
{"title": "GEIC: Universal and Multilingual Named Entity Recognition with Large\n  Language Models", "author": "Hanjun Luo and Yingbin Jin and Xuecheng Liu and Tong Shang and Ruizhe Chen and Zuozhu Liu", "abstract": "  Large Language Models (LLMs) have supplanted traditional methods in numerous\nnatural language processing tasks. Nonetheless, in Named Entity Recognition\n(NER), existing LLM-based methods underperform compared to baselines and\nrequire significantly more computational resources, limiting their application.\nIn this paper, we introduce the task of generation-based extraction and\nin-context classification (GEIC), designed to leverage LLMs' prior knowledge\nand self-attention mechanisms for NER tasks. We then propose CascadeNER, a\nuniversal and multilingual GEIC framework for few-shot and zero-shot NER.\nCascadeNER employs model cascading to utilize two small-parameter LLMs to\nextract and classify independently, reducing resource consumption while\nenhancing accuracy. We also introduce AnythingNER, the first NER dataset\nspecifically designed for LLMs, including 8 languages, 155 entity types and a\nnovel dynamic categorization system. Experiments show that CascadeNER achieves\nstate-of-the-art performance on low-resource and fine-grained scenarios,\nincluding CrossNER and FewNERD. Our work is openly accessible.\n", "link": "http://arxiv.org/abs/2409.11022v3", "date": "2024-09-25", "relevancy": 2.0436, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.511}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GEIC%3A%20Universal%20and%20Multilingual%20Named%20Entity%20Recognition%20with%20Large%0A%20%20Language%20Models&body=Title%3A%20GEIC%3A%20Universal%20and%20Multilingual%20Named%20Entity%20Recognition%20with%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Hanjun%20Luo%20and%20Yingbin%20Jin%20and%20Xuecheng%20Liu%20and%20Tong%20Shang%20and%20Ruizhe%20Chen%20and%20Zuozhu%20Liu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20supplanted%20traditional%20methods%20in%20numerous%0Anatural%20language%20processing%20tasks.%20Nonetheless%2C%20in%20Named%20Entity%20Recognition%0A%28NER%29%2C%20existing%20LLM-based%20methods%20underperform%20compared%20to%20baselines%20and%0Arequire%20significantly%20more%20computational%20resources%2C%20limiting%20their%20application.%0AIn%20this%20paper%2C%20we%20introduce%20the%20task%20of%20generation-based%20extraction%20and%0Ain-context%20classification%20%28GEIC%29%2C%20designed%20to%20leverage%20LLMs%27%20prior%20knowledge%0Aand%20self-attention%20mechanisms%20for%20NER%20tasks.%20We%20then%20propose%20CascadeNER%2C%20a%0Auniversal%20and%20multilingual%20GEIC%20framework%20for%20few-shot%20and%20zero-shot%20NER.%0ACascadeNER%20employs%20model%20cascading%20to%20utilize%20two%20small-parameter%20LLMs%20to%0Aextract%20and%20classify%20independently%2C%20reducing%20resource%20consumption%20while%0Aenhancing%20accuracy.%20We%20also%20introduce%20AnythingNER%2C%20the%20first%20NER%20dataset%0Aspecifically%20designed%20for%20LLMs%2C%20including%208%20languages%2C%20155%20entity%20types%20and%20a%0Anovel%20dynamic%20categorization%20system.%20Experiments%20show%20that%20CascadeNER%20achieves%0Astate-of-the-art%20performance%20on%20low-resource%20and%20fine-grained%20scenarios%2C%0Aincluding%20CrossNER%20and%20FewNERD.%20Our%20work%20is%20openly%20accessible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.11022v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGEIC%253A%2520Universal%2520and%2520Multilingual%2520Named%2520Entity%2520Recognition%2520with%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DHanjun%2520Luo%2520and%2520Yingbin%2520Jin%2520and%2520Xuecheng%2520Liu%2520and%2520Tong%2520Shang%2520and%2520Ruizhe%2520Chen%2520and%2520Zuozhu%2520Liu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520supplanted%2520traditional%2520methods%2520in%2520numerous%250Anatural%2520language%2520processing%2520tasks.%2520Nonetheless%252C%2520in%2520Named%2520Entity%2520Recognition%250A%2528NER%2529%252C%2520existing%2520LLM-based%2520methods%2520underperform%2520compared%2520to%2520baselines%2520and%250Arequire%2520significantly%2520more%2520computational%2520resources%252C%2520limiting%2520their%2520application.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520the%2520task%2520of%2520generation-based%2520extraction%2520and%250Ain-context%2520classification%2520%2528GEIC%2529%252C%2520designed%2520to%2520leverage%2520LLMs%2527%2520prior%2520knowledge%250Aand%2520self-attention%2520mechanisms%2520for%2520NER%2520tasks.%2520We%2520then%2520propose%2520CascadeNER%252C%2520a%250Auniversal%2520and%2520multilingual%2520GEIC%2520framework%2520for%2520few-shot%2520and%2520zero-shot%2520NER.%250ACascadeNER%2520employs%2520model%2520cascading%2520to%2520utilize%2520two%2520small-parameter%2520LLMs%2520to%250Aextract%2520and%2520classify%2520independently%252C%2520reducing%2520resource%2520consumption%2520while%250Aenhancing%2520accuracy.%2520We%2520also%2520introduce%2520AnythingNER%252C%2520the%2520first%2520NER%2520dataset%250Aspecifically%2520designed%2520for%2520LLMs%252C%2520including%25208%2520languages%252C%2520155%2520entity%2520types%2520and%2520a%250Anovel%2520dynamic%2520categorization%2520system.%2520Experiments%2520show%2520that%2520CascadeNER%2520achieves%250Astate-of-the-art%2520performance%2520on%2520low-resource%2520and%2520fine-grained%2520scenarios%252C%250Aincluding%2520CrossNER%2520and%2520FewNERD.%2520Our%2520work%2520is%2520openly%2520accessible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.11022v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GEIC%3A%20Universal%20and%20Multilingual%20Named%20Entity%20Recognition%20with%20Large%0A%20%20Language%20Models&entry.906535625=Hanjun%20Luo%20and%20Yingbin%20Jin%20and%20Xuecheng%20Liu%20and%20Tong%20Shang%20and%20Ruizhe%20Chen%20and%20Zuozhu%20Liu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20supplanted%20traditional%20methods%20in%20numerous%0Anatural%20language%20processing%20tasks.%20Nonetheless%2C%20in%20Named%20Entity%20Recognition%0A%28NER%29%2C%20existing%20LLM-based%20methods%20underperform%20compared%20to%20baselines%20and%0Arequire%20significantly%20more%20computational%20resources%2C%20limiting%20their%20application.%0AIn%20this%20paper%2C%20we%20introduce%20the%20task%20of%20generation-based%20extraction%20and%0Ain-context%20classification%20%28GEIC%29%2C%20designed%20to%20leverage%20LLMs%27%20prior%20knowledge%0Aand%20self-attention%20mechanisms%20for%20NER%20tasks.%20We%20then%20propose%20CascadeNER%2C%20a%0Auniversal%20and%20multilingual%20GEIC%20framework%20for%20few-shot%20and%20zero-shot%20NER.%0ACascadeNER%20employs%20model%20cascading%20to%20utilize%20two%20small-parameter%20LLMs%20to%0Aextract%20and%20classify%20independently%2C%20reducing%20resource%20consumption%20while%0Aenhancing%20accuracy.%20We%20also%20introduce%20AnythingNER%2C%20the%20first%20NER%20dataset%0Aspecifically%20designed%20for%20LLMs%2C%20including%208%20languages%2C%20155%20entity%20types%20and%20a%0Anovel%20dynamic%20categorization%20system.%20Experiments%20show%20that%20CascadeNER%20achieves%0Astate-of-the-art%20performance%20on%20low-resource%20and%20fine-grained%20scenarios%2C%0Aincluding%20CrossNER%20and%20FewNERD.%20Our%20work%20is%20openly%20accessible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.11022v3&entry.124074799=Read"},
{"title": "CNN Mixture-of-Depths", "author": "Rinor Cakaj and Jens Mehnert and Bin Yang", "abstract": "  We introduce Mixture-of-Depths (MoD) for Convolutional Neural Networks\n(CNNs), a novel approach that enhances the computational efficiency of CNNs by\nselectively processing channels based on their relevance to the current\nprediction. This method optimizes computational resources by dynamically\nselecting key channels in feature maps for focused processing within the\nconvolutional blocks (Conv-Blocks), while skipping less relevant channels.\nUnlike conditional computation methods that require dynamic computation graphs,\nCNN MoD uses a static computation graph with fixed tensor sizes which improve\nhardware efficiency. It speeds up the training and inference processes without\nthe need for customized CUDA kernels, unique loss functions, or finetuning. CNN\nMoD either matches the performance of traditional CNNs with reduced inference\ntimes, GMACs, and parameters, or exceeds their performance while maintaining\nsimilar inference times, GMACs, and parameters. For example, on ImageNet,\nResNet86-MoD exceeds the performance of the standard ResNet50 by 0.45% with a\n6% speedup on CPU and 5% on GPU. Moreover, ResNet75-MoD achieves the same\nperformance as ResNet50 with a 25% speedup on CPU and 15% on GPU.\n", "link": "http://arxiv.org/abs/2409.17016v1", "date": "2024-09-25", "relevancy": 2.0389, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5331}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5144}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CNN%20Mixture-of-Depths&body=Title%3A%20CNN%20Mixture-of-Depths%0AAuthor%3A%20Rinor%20Cakaj%20and%20Jens%20Mehnert%20and%20Bin%20Yang%0AAbstract%3A%20%20%20We%20introduce%20Mixture-of-Depths%20%28MoD%29%20for%20Convolutional%20Neural%20Networks%0A%28CNNs%29%2C%20a%20novel%20approach%20that%20enhances%20the%20computational%20efficiency%20of%20CNNs%20by%0Aselectively%20processing%20channels%20based%20on%20their%20relevance%20to%20the%20current%0Aprediction.%20This%20method%20optimizes%20computational%20resources%20by%20dynamically%0Aselecting%20key%20channels%20in%20feature%20maps%20for%20focused%20processing%20within%20the%0Aconvolutional%20blocks%20%28Conv-Blocks%29%2C%20while%20skipping%20less%20relevant%20channels.%0AUnlike%20conditional%20computation%20methods%20that%20require%20dynamic%20computation%20graphs%2C%0ACNN%20MoD%20uses%20a%20static%20computation%20graph%20with%20fixed%20tensor%20sizes%20which%20improve%0Ahardware%20efficiency.%20It%20speeds%20up%20the%20training%20and%20inference%20processes%20without%0Athe%20need%20for%20customized%20CUDA%20kernels%2C%20unique%20loss%20functions%2C%20or%20finetuning.%20CNN%0AMoD%20either%20matches%20the%20performance%20of%20traditional%20CNNs%20with%20reduced%20inference%0Atimes%2C%20GMACs%2C%20and%20parameters%2C%20or%20exceeds%20their%20performance%20while%20maintaining%0Asimilar%20inference%20times%2C%20GMACs%2C%20and%20parameters.%20For%20example%2C%20on%20ImageNet%2C%0AResNet86-MoD%20exceeds%20the%20performance%20of%20the%20standard%20ResNet50%20by%200.45%25%20with%20a%0A6%25%20speedup%20on%20CPU%20and%205%25%20on%20GPU.%20Moreover%2C%20ResNet75-MoD%20achieves%20the%20same%0Aperformance%20as%20ResNet50%20with%20a%2025%25%20speedup%20on%20CPU%20and%2015%25%20on%20GPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17016v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCNN%2520Mixture-of-Depths%26entry.906535625%3DRinor%2520Cakaj%2520and%2520Jens%2520Mehnert%2520and%2520Bin%2520Yang%26entry.1292438233%3D%2520%2520We%2520introduce%2520Mixture-of-Depths%2520%2528MoD%2529%2520for%2520Convolutional%2520Neural%2520Networks%250A%2528CNNs%2529%252C%2520a%2520novel%2520approach%2520that%2520enhances%2520the%2520computational%2520efficiency%2520of%2520CNNs%2520by%250Aselectively%2520processing%2520channels%2520based%2520on%2520their%2520relevance%2520to%2520the%2520current%250Aprediction.%2520This%2520method%2520optimizes%2520computational%2520resources%2520by%2520dynamically%250Aselecting%2520key%2520channels%2520in%2520feature%2520maps%2520for%2520focused%2520processing%2520within%2520the%250Aconvolutional%2520blocks%2520%2528Conv-Blocks%2529%252C%2520while%2520skipping%2520less%2520relevant%2520channels.%250AUnlike%2520conditional%2520computation%2520methods%2520that%2520require%2520dynamic%2520computation%2520graphs%252C%250ACNN%2520MoD%2520uses%2520a%2520static%2520computation%2520graph%2520with%2520fixed%2520tensor%2520sizes%2520which%2520improve%250Ahardware%2520efficiency.%2520It%2520speeds%2520up%2520the%2520training%2520and%2520inference%2520processes%2520without%250Athe%2520need%2520for%2520customized%2520CUDA%2520kernels%252C%2520unique%2520loss%2520functions%252C%2520or%2520finetuning.%2520CNN%250AMoD%2520either%2520matches%2520the%2520performance%2520of%2520traditional%2520CNNs%2520with%2520reduced%2520inference%250Atimes%252C%2520GMACs%252C%2520and%2520parameters%252C%2520or%2520exceeds%2520their%2520performance%2520while%2520maintaining%250Asimilar%2520inference%2520times%252C%2520GMACs%252C%2520and%2520parameters.%2520For%2520example%252C%2520on%2520ImageNet%252C%250AResNet86-MoD%2520exceeds%2520the%2520performance%2520of%2520the%2520standard%2520ResNet50%2520by%25200.45%2525%2520with%2520a%250A6%2525%2520speedup%2520on%2520CPU%2520and%25205%2525%2520on%2520GPU.%2520Moreover%252C%2520ResNet75-MoD%2520achieves%2520the%2520same%250Aperformance%2520as%2520ResNet50%2520with%2520a%252025%2525%2520speedup%2520on%2520CPU%2520and%252015%2525%2520on%2520GPU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17016v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CNN%20Mixture-of-Depths&entry.906535625=Rinor%20Cakaj%20and%20Jens%20Mehnert%20and%20Bin%20Yang&entry.1292438233=%20%20We%20introduce%20Mixture-of-Depths%20%28MoD%29%20for%20Convolutional%20Neural%20Networks%0A%28CNNs%29%2C%20a%20novel%20approach%20that%20enhances%20the%20computational%20efficiency%20of%20CNNs%20by%0Aselectively%20processing%20channels%20based%20on%20their%20relevance%20to%20the%20current%0Aprediction.%20This%20method%20optimizes%20computational%20resources%20by%20dynamically%0Aselecting%20key%20channels%20in%20feature%20maps%20for%20focused%20processing%20within%20the%0Aconvolutional%20blocks%20%28Conv-Blocks%29%2C%20while%20skipping%20less%20relevant%20channels.%0AUnlike%20conditional%20computation%20methods%20that%20require%20dynamic%20computation%20graphs%2C%0ACNN%20MoD%20uses%20a%20static%20computation%20graph%20with%20fixed%20tensor%20sizes%20which%20improve%0Ahardware%20efficiency.%20It%20speeds%20up%20the%20training%20and%20inference%20processes%20without%0Athe%20need%20for%20customized%20CUDA%20kernels%2C%20unique%20loss%20functions%2C%20or%20finetuning.%20CNN%0AMoD%20either%20matches%20the%20performance%20of%20traditional%20CNNs%20with%20reduced%20inference%0Atimes%2C%20GMACs%2C%20and%20parameters%2C%20or%20exceeds%20their%20performance%20while%20maintaining%0Asimilar%20inference%20times%2C%20GMACs%2C%20and%20parameters.%20For%20example%2C%20on%20ImageNet%2C%0AResNet86-MoD%20exceeds%20the%20performance%20of%20the%20standard%20ResNet50%20by%200.45%25%20with%20a%0A6%25%20speedup%20on%20CPU%20and%205%25%20on%20GPU.%20Moreover%2C%20ResNet75-MoD%20achieves%20the%20same%0Aperformance%20as%20ResNet50%20with%20a%2025%25%20speedup%20on%20CPU%20and%2015%25%20on%20GPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17016v1&entry.124074799=Read"},
{"title": "CREVE: An Acceleration-based Constraint Approach for Robust Radar\n  Ego-Velocity Estimation", "author": "Hoang Viet Do and Bo Sung Ko and Jin Woo Song", "abstract": "  Ego-velocity estimation from point cloud measurements of a millimeter-wave\nfrequency-modulated continuous wave (mmWave FMCW) radar has become a crucial\ncomponent of radar-inertial odometry (RIO) systems. Conventional approaches\noften perform poorly when the number of point cloud outliers exceeds that of\ninliers. In this paper, we propose CREVE, an acceleration-based inequality\nconstraints filter that leverages additional measurements from an inertial\nmeasurement unit (IMU) to achieve robust ego-velocity estimations. To further\nenhance accuracy and robustness against sensor errors, we introduce a practical\naccelerometer bias estimation method and a parameter adaptation rule. The\neffectiveness of the proposed method is evaluated using five open-source drone\ndatasets. Experimental results demonstrate that our algorithm significantly\noutperforms three existing state-of-the-art methods, achieving reductions in\nabsolute trajectory error of approximately 53%, 84%, and 35% compared to them.\n", "link": "http://arxiv.org/abs/2409.16847v1", "date": "2024-09-25", "relevancy": 2.0299, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5283}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5124}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CREVE%3A%20An%20Acceleration-based%20Constraint%20Approach%20for%20Robust%20Radar%0A%20%20Ego-Velocity%20Estimation&body=Title%3A%20CREVE%3A%20An%20Acceleration-based%20Constraint%20Approach%20for%20Robust%20Radar%0A%20%20Ego-Velocity%20Estimation%0AAuthor%3A%20Hoang%20Viet%20Do%20and%20Bo%20Sung%20Ko%20and%20Jin%20Woo%20Song%0AAbstract%3A%20%20%20Ego-velocity%20estimation%20from%20point%20cloud%20measurements%20of%20a%20millimeter-wave%0Afrequency-modulated%20continuous%20wave%20%28mmWave%20FMCW%29%20radar%20has%20become%20a%20crucial%0Acomponent%20of%20radar-inertial%20odometry%20%28RIO%29%20systems.%20Conventional%20approaches%0Aoften%20perform%20poorly%20when%20the%20number%20of%20point%20cloud%20outliers%20exceeds%20that%20of%0Ainliers.%20In%20this%20paper%2C%20we%20propose%20CREVE%2C%20an%20acceleration-based%20inequality%0Aconstraints%20filter%20that%20leverages%20additional%20measurements%20from%20an%20inertial%0Ameasurement%20unit%20%28IMU%29%20to%20achieve%20robust%20ego-velocity%20estimations.%20To%20further%0Aenhance%20accuracy%20and%20robustness%20against%20sensor%20errors%2C%20we%20introduce%20a%20practical%0Aaccelerometer%20bias%20estimation%20method%20and%20a%20parameter%20adaptation%20rule.%20The%0Aeffectiveness%20of%20the%20proposed%20method%20is%20evaluated%20using%20five%20open-source%20drone%0Adatasets.%20Experimental%20results%20demonstrate%20that%20our%20algorithm%20significantly%0Aoutperforms%20three%20existing%20state-of-the-art%20methods%2C%20achieving%20reductions%20in%0Aabsolute%20trajectory%20error%20of%20approximately%2053%25%2C%2084%25%2C%20and%2035%25%20compared%20to%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCREVE%253A%2520An%2520Acceleration-based%2520Constraint%2520Approach%2520for%2520Robust%2520Radar%250A%2520%2520Ego-Velocity%2520Estimation%26entry.906535625%3DHoang%2520Viet%2520Do%2520and%2520Bo%2520Sung%2520Ko%2520and%2520Jin%2520Woo%2520Song%26entry.1292438233%3D%2520%2520Ego-velocity%2520estimation%2520from%2520point%2520cloud%2520measurements%2520of%2520a%2520millimeter-wave%250Afrequency-modulated%2520continuous%2520wave%2520%2528mmWave%2520FMCW%2529%2520radar%2520has%2520become%2520a%2520crucial%250Acomponent%2520of%2520radar-inertial%2520odometry%2520%2528RIO%2529%2520systems.%2520Conventional%2520approaches%250Aoften%2520perform%2520poorly%2520when%2520the%2520number%2520of%2520point%2520cloud%2520outliers%2520exceeds%2520that%2520of%250Ainliers.%2520In%2520this%2520paper%252C%2520we%2520propose%2520CREVE%252C%2520an%2520acceleration-based%2520inequality%250Aconstraints%2520filter%2520that%2520leverages%2520additional%2520measurements%2520from%2520an%2520inertial%250Ameasurement%2520unit%2520%2528IMU%2529%2520to%2520achieve%2520robust%2520ego-velocity%2520estimations.%2520To%2520further%250Aenhance%2520accuracy%2520and%2520robustness%2520against%2520sensor%2520errors%252C%2520we%2520introduce%2520a%2520practical%250Aaccelerometer%2520bias%2520estimation%2520method%2520and%2520a%2520parameter%2520adaptation%2520rule.%2520The%250Aeffectiveness%2520of%2520the%2520proposed%2520method%2520is%2520evaluated%2520using%2520five%2520open-source%2520drone%250Adatasets.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520algorithm%2520significantly%250Aoutperforms%2520three%2520existing%2520state-of-the-art%2520methods%252C%2520achieving%2520reductions%2520in%250Aabsolute%2520trajectory%2520error%2520of%2520approximately%252053%2525%252C%252084%2525%252C%2520and%252035%2525%2520compared%2520to%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CREVE%3A%20An%20Acceleration-based%20Constraint%20Approach%20for%20Robust%20Radar%0A%20%20Ego-Velocity%20Estimation&entry.906535625=Hoang%20Viet%20Do%20and%20Bo%20Sung%20Ko%20and%20Jin%20Woo%20Song&entry.1292438233=%20%20Ego-velocity%20estimation%20from%20point%20cloud%20measurements%20of%20a%20millimeter-wave%0Afrequency-modulated%20continuous%20wave%20%28mmWave%20FMCW%29%20radar%20has%20become%20a%20crucial%0Acomponent%20of%20radar-inertial%20odometry%20%28RIO%29%20systems.%20Conventional%20approaches%0Aoften%20perform%20poorly%20when%20the%20number%20of%20point%20cloud%20outliers%20exceeds%20that%20of%0Ainliers.%20In%20this%20paper%2C%20we%20propose%20CREVE%2C%20an%20acceleration-based%20inequality%0Aconstraints%20filter%20that%20leverages%20additional%20measurements%20from%20an%20inertial%0Ameasurement%20unit%20%28IMU%29%20to%20achieve%20robust%20ego-velocity%20estimations.%20To%20further%0Aenhance%20accuracy%20and%20robustness%20against%20sensor%20errors%2C%20we%20introduce%20a%20practical%0Aaccelerometer%20bias%20estimation%20method%20and%20a%20parameter%20adaptation%20rule.%20The%0Aeffectiveness%20of%20the%20proposed%20method%20is%20evaluated%20using%20five%20open-source%20drone%0Adatasets.%20Experimental%20results%20demonstrate%20that%20our%20algorithm%20significantly%0Aoutperforms%20three%20existing%20state-of-the-art%20methods%2C%20achieving%20reductions%20in%0Aabsolute%20trajectory%20error%20of%20approximately%2053%25%2C%2084%25%2C%20and%2035%25%20compared%20to%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16847v1&entry.124074799=Read"},
{"title": "Feedforward Controllers from Learned Dynamic Local Model Networks with\n  Application to Excavator Assistance Functions", "author": "Leon Greiser and Ozan Demir and Benjamin Hartmann and Henrik Hose and Sebastian Trimpe", "abstract": "  Complicated first principles modelling and controller synthesis can be\nprohibitively slow and expensive for high-mix, low-volume products such as\nhydraulic excavators. Instead, in a data-driven approach, recorded trajectories\nfrom the real system can be used to train local model networks (LMNs), for\nwhich feedforward controllers are derived via feedback linearization. However,\nprevious works required LMNs without zero dynamics for feedback linearization,\nwhich restricts the model structure and thus modelling capacity of LMNs. In\nthis paper, we overcome this restriction by providing a criterion for when\nfeedback linearization of LMNs with zero dynamics yields a valid controller. As\na criterion we propose the bounded-input bounded-output stability of the\nresulting controller. In two additional contributions, we extend this approach\nto consider measured disturbance signals and multiple inputs and outputs. We\nillustrate the effectiveness of our contributions in a hydraulic excavator\ncontrol application with hardware experiments. To this end, we train LMNs from\nrecorded, noisy data and derive feedforward controllers used as part of a\nleveling assistance system on the excavator. In our experiments, incorporating\ndisturbance signals and multiple inputs and outputs enhances tracking\nperformance of the learned controller. A video of our experiments is available\nat https://youtu.be/lrrWBx2ASaE.\n", "link": "http://arxiv.org/abs/2409.16875v1", "date": "2024-09-25", "relevancy": 2.0298, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5163}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5154}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feedforward%20Controllers%20from%20Learned%20Dynamic%20Local%20Model%20Networks%20with%0A%20%20Application%20to%20Excavator%20Assistance%20Functions&body=Title%3A%20Feedforward%20Controllers%20from%20Learned%20Dynamic%20Local%20Model%20Networks%20with%0A%20%20Application%20to%20Excavator%20Assistance%20Functions%0AAuthor%3A%20Leon%20Greiser%20and%20Ozan%20Demir%20and%20Benjamin%20Hartmann%20and%20Henrik%20Hose%20and%20Sebastian%20Trimpe%0AAbstract%3A%20%20%20Complicated%20first%20principles%20modelling%20and%20controller%20synthesis%20can%20be%0Aprohibitively%20slow%20and%20expensive%20for%20high-mix%2C%20low-volume%20products%20such%20as%0Ahydraulic%20excavators.%20Instead%2C%20in%20a%20data-driven%20approach%2C%20recorded%20trajectories%0Afrom%20the%20real%20system%20can%20be%20used%20to%20train%20local%20model%20networks%20%28LMNs%29%2C%20for%0Awhich%20feedforward%20controllers%20are%20derived%20via%20feedback%20linearization.%20However%2C%0Aprevious%20works%20required%20LMNs%20without%20zero%20dynamics%20for%20feedback%20linearization%2C%0Awhich%20restricts%20the%20model%20structure%20and%20thus%20modelling%20capacity%20of%20LMNs.%20In%0Athis%20paper%2C%20we%20overcome%20this%20restriction%20by%20providing%20a%20criterion%20for%20when%0Afeedback%20linearization%20of%20LMNs%20with%20zero%20dynamics%20yields%20a%20valid%20controller.%20As%0Aa%20criterion%20we%20propose%20the%20bounded-input%20bounded-output%20stability%20of%20the%0Aresulting%20controller.%20In%20two%20additional%20contributions%2C%20we%20extend%20this%20approach%0Ato%20consider%20measured%20disturbance%20signals%20and%20multiple%20inputs%20and%20outputs.%20We%0Aillustrate%20the%20effectiveness%20of%20our%20contributions%20in%20a%20hydraulic%20excavator%0Acontrol%20application%20with%20hardware%20experiments.%20To%20this%20end%2C%20we%20train%20LMNs%20from%0Arecorded%2C%20noisy%20data%20and%20derive%20feedforward%20controllers%20used%20as%20part%20of%20a%0Aleveling%20assistance%20system%20on%20the%20excavator.%20In%20our%20experiments%2C%20incorporating%0Adisturbance%20signals%20and%20multiple%20inputs%20and%20outputs%20enhances%20tracking%0Aperformance%20of%20the%20learned%20controller.%20A%20video%20of%20our%20experiments%20is%20available%0Aat%20https%3A//youtu.be/lrrWBx2ASaE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeedforward%2520Controllers%2520from%2520Learned%2520Dynamic%2520Local%2520Model%2520Networks%2520with%250A%2520%2520Application%2520to%2520Excavator%2520Assistance%2520Functions%26entry.906535625%3DLeon%2520Greiser%2520and%2520Ozan%2520Demir%2520and%2520Benjamin%2520Hartmann%2520and%2520Henrik%2520Hose%2520and%2520Sebastian%2520Trimpe%26entry.1292438233%3D%2520%2520Complicated%2520first%2520principles%2520modelling%2520and%2520controller%2520synthesis%2520can%2520be%250Aprohibitively%2520slow%2520and%2520expensive%2520for%2520high-mix%252C%2520low-volume%2520products%2520such%2520as%250Ahydraulic%2520excavators.%2520Instead%252C%2520in%2520a%2520data-driven%2520approach%252C%2520recorded%2520trajectories%250Afrom%2520the%2520real%2520system%2520can%2520be%2520used%2520to%2520train%2520local%2520model%2520networks%2520%2528LMNs%2529%252C%2520for%250Awhich%2520feedforward%2520controllers%2520are%2520derived%2520via%2520feedback%2520linearization.%2520However%252C%250Aprevious%2520works%2520required%2520LMNs%2520without%2520zero%2520dynamics%2520for%2520feedback%2520linearization%252C%250Awhich%2520restricts%2520the%2520model%2520structure%2520and%2520thus%2520modelling%2520capacity%2520of%2520LMNs.%2520In%250Athis%2520paper%252C%2520we%2520overcome%2520this%2520restriction%2520by%2520providing%2520a%2520criterion%2520for%2520when%250Afeedback%2520linearization%2520of%2520LMNs%2520with%2520zero%2520dynamics%2520yields%2520a%2520valid%2520controller.%2520As%250Aa%2520criterion%2520we%2520propose%2520the%2520bounded-input%2520bounded-output%2520stability%2520of%2520the%250Aresulting%2520controller.%2520In%2520two%2520additional%2520contributions%252C%2520we%2520extend%2520this%2520approach%250Ato%2520consider%2520measured%2520disturbance%2520signals%2520and%2520multiple%2520inputs%2520and%2520outputs.%2520We%250Aillustrate%2520the%2520effectiveness%2520of%2520our%2520contributions%2520in%2520a%2520hydraulic%2520excavator%250Acontrol%2520application%2520with%2520hardware%2520experiments.%2520To%2520this%2520end%252C%2520we%2520train%2520LMNs%2520from%250Arecorded%252C%2520noisy%2520data%2520and%2520derive%2520feedforward%2520controllers%2520used%2520as%2520part%2520of%2520a%250Aleveling%2520assistance%2520system%2520on%2520the%2520excavator.%2520In%2520our%2520experiments%252C%2520incorporating%250Adisturbance%2520signals%2520and%2520multiple%2520inputs%2520and%2520outputs%2520enhances%2520tracking%250Aperformance%2520of%2520the%2520learned%2520controller.%2520A%2520video%2520of%2520our%2520experiments%2520is%2520available%250Aat%2520https%253A//youtu.be/lrrWBx2ASaE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feedforward%20Controllers%20from%20Learned%20Dynamic%20Local%20Model%20Networks%20with%0A%20%20Application%20to%20Excavator%20Assistance%20Functions&entry.906535625=Leon%20Greiser%20and%20Ozan%20Demir%20and%20Benjamin%20Hartmann%20and%20Henrik%20Hose%20and%20Sebastian%20Trimpe&entry.1292438233=%20%20Complicated%20first%20principles%20modelling%20and%20controller%20synthesis%20can%20be%0Aprohibitively%20slow%20and%20expensive%20for%20high-mix%2C%20low-volume%20products%20such%20as%0Ahydraulic%20excavators.%20Instead%2C%20in%20a%20data-driven%20approach%2C%20recorded%20trajectories%0Afrom%20the%20real%20system%20can%20be%20used%20to%20train%20local%20model%20networks%20%28LMNs%29%2C%20for%0Awhich%20feedforward%20controllers%20are%20derived%20via%20feedback%20linearization.%20However%2C%0Aprevious%20works%20required%20LMNs%20without%20zero%20dynamics%20for%20feedback%20linearization%2C%0Awhich%20restricts%20the%20model%20structure%20and%20thus%20modelling%20capacity%20of%20LMNs.%20In%0Athis%20paper%2C%20we%20overcome%20this%20restriction%20by%20providing%20a%20criterion%20for%20when%0Afeedback%20linearization%20of%20LMNs%20with%20zero%20dynamics%20yields%20a%20valid%20controller.%20As%0Aa%20criterion%20we%20propose%20the%20bounded-input%20bounded-output%20stability%20of%20the%0Aresulting%20controller.%20In%20two%20additional%20contributions%2C%20we%20extend%20this%20approach%0Ato%20consider%20measured%20disturbance%20signals%20and%20multiple%20inputs%20and%20outputs.%20We%0Aillustrate%20the%20effectiveness%20of%20our%20contributions%20in%20a%20hydraulic%20excavator%0Acontrol%20application%20with%20hardware%20experiments.%20To%20this%20end%2C%20we%20train%20LMNs%20from%0Arecorded%2C%20noisy%20data%20and%20derive%20feedforward%20controllers%20used%20as%20part%20of%20a%0Aleveling%20assistance%20system%20on%20the%20excavator.%20In%20our%20experiments%2C%20incorporating%0Adisturbance%20signals%20and%20multiple%20inputs%20and%20outputs%20enhances%20tracking%0Aperformance%20of%20the%20learned%20controller.%20A%20video%20of%20our%20experiments%20is%20available%0Aat%20https%3A//youtu.be/lrrWBx2ASaE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16875v1&entry.124074799=Read"},
{"title": "Adaptive Self-Supervised Learning Strategies for Dynamic On-Device LLM\n  Personalization", "author": "Rafael Mendoza and Isabella Cruz and Richard Liu and Aarav Deshmukh and David Williams and Jesscia Peng and Rohan Iyer", "abstract": "  Large language models (LLMs) have revolutionized how we interact with\ntechnology, but their personalization to individual user preferences remains a\nsignificant challenge, particularly in on-device applications. Traditional\nmethods often depend heavily on labeled datasets and can be resource-intensive.\nTo address these issues, we present Adaptive Self-Supervised Learning\nStrategies (ASLS), which utilizes self-supervised learning techniques to\npersonalize LLMs dynamically. The framework comprises a user profiling layer\nfor collecting interaction data and a neural adaptation layer for real-time\nmodel fine-tuning. This innovative approach enables continuous learning from\nuser feedback, allowing the model to generate responses that align closely with\nuser-specific contexts. The adaptive mechanisms of ASLS minimize computational\ndemands and enhance personalization efficiency. Experimental results across\nvarious user scenarios illustrate the superior performance of ASLS in boosting\nuser engagement and satisfaction, highlighting its potential to redefine LLMs\nas highly responsive and context-aware systems on-device.\n", "link": "http://arxiv.org/abs/2409.16973v1", "date": "2024-09-25", "relevancy": 2.0166, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.518}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4949}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Self-Supervised%20Learning%20Strategies%20for%20Dynamic%20On-Device%20LLM%0A%20%20Personalization&body=Title%3A%20Adaptive%20Self-Supervised%20Learning%20Strategies%20for%20Dynamic%20On-Device%20LLM%0A%20%20Personalization%0AAuthor%3A%20Rafael%20Mendoza%20and%20Isabella%20Cruz%20and%20Richard%20Liu%20and%20Aarav%20Deshmukh%20and%20David%20Williams%20and%20Jesscia%20Peng%20and%20Rohan%20Iyer%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20revolutionized%20how%20we%20interact%20with%0Atechnology%2C%20but%20their%20personalization%20to%20individual%20user%20preferences%20remains%20a%0Asignificant%20challenge%2C%20particularly%20in%20on-device%20applications.%20Traditional%0Amethods%20often%20depend%20heavily%20on%20labeled%20datasets%20and%20can%20be%20resource-intensive.%0ATo%20address%20these%20issues%2C%20we%20present%20Adaptive%20Self-Supervised%20Learning%0AStrategies%20%28ASLS%29%2C%20which%20utilizes%20self-supervised%20learning%20techniques%20to%0Apersonalize%20LLMs%20dynamically.%20The%20framework%20comprises%20a%20user%20profiling%20layer%0Afor%20collecting%20interaction%20data%20and%20a%20neural%20adaptation%20layer%20for%20real-time%0Amodel%20fine-tuning.%20This%20innovative%20approach%20enables%20continuous%20learning%20from%0Auser%20feedback%2C%20allowing%20the%20model%20to%20generate%20responses%20that%20align%20closely%20with%0Auser-specific%20contexts.%20The%20adaptive%20mechanisms%20of%20ASLS%20minimize%20computational%0Ademands%20and%20enhance%20personalization%20efficiency.%20Experimental%20results%20across%0Avarious%20user%20scenarios%20illustrate%20the%20superior%20performance%20of%20ASLS%20in%20boosting%0Auser%20engagement%20and%20satisfaction%2C%20highlighting%20its%20potential%20to%20redefine%20LLMs%0Aas%20highly%20responsive%20and%20context-aware%20systems%20on-device.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16973v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Self-Supervised%2520Learning%2520Strategies%2520for%2520Dynamic%2520On-Device%2520LLM%250A%2520%2520Personalization%26entry.906535625%3DRafael%2520Mendoza%2520and%2520Isabella%2520Cruz%2520and%2520Richard%2520Liu%2520and%2520Aarav%2520Deshmukh%2520and%2520David%2520Williams%2520and%2520Jesscia%2520Peng%2520and%2520Rohan%2520Iyer%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520revolutionized%2520how%2520we%2520interact%2520with%250Atechnology%252C%2520but%2520their%2520personalization%2520to%2520individual%2520user%2520preferences%2520remains%2520a%250Asignificant%2520challenge%252C%2520particularly%2520in%2520on-device%2520applications.%2520Traditional%250Amethods%2520often%2520depend%2520heavily%2520on%2520labeled%2520datasets%2520and%2520can%2520be%2520resource-intensive.%250ATo%2520address%2520these%2520issues%252C%2520we%2520present%2520Adaptive%2520Self-Supervised%2520Learning%250AStrategies%2520%2528ASLS%2529%252C%2520which%2520utilizes%2520self-supervised%2520learning%2520techniques%2520to%250Apersonalize%2520LLMs%2520dynamically.%2520The%2520framework%2520comprises%2520a%2520user%2520profiling%2520layer%250Afor%2520collecting%2520interaction%2520data%2520and%2520a%2520neural%2520adaptation%2520layer%2520for%2520real-time%250Amodel%2520fine-tuning.%2520This%2520innovative%2520approach%2520enables%2520continuous%2520learning%2520from%250Auser%2520feedback%252C%2520allowing%2520the%2520model%2520to%2520generate%2520responses%2520that%2520align%2520closely%2520with%250Auser-specific%2520contexts.%2520The%2520adaptive%2520mechanisms%2520of%2520ASLS%2520minimize%2520computational%250Ademands%2520and%2520enhance%2520personalization%2520efficiency.%2520Experimental%2520results%2520across%250Avarious%2520user%2520scenarios%2520illustrate%2520the%2520superior%2520performance%2520of%2520ASLS%2520in%2520boosting%250Auser%2520engagement%2520and%2520satisfaction%252C%2520highlighting%2520its%2520potential%2520to%2520redefine%2520LLMs%250Aas%2520highly%2520responsive%2520and%2520context-aware%2520systems%2520on-device.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16973v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Self-Supervised%20Learning%20Strategies%20for%20Dynamic%20On-Device%20LLM%0A%20%20Personalization&entry.906535625=Rafael%20Mendoza%20and%20Isabella%20Cruz%20and%20Richard%20Liu%20and%20Aarav%20Deshmukh%20and%20David%20Williams%20and%20Jesscia%20Peng%20and%20Rohan%20Iyer&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20revolutionized%20how%20we%20interact%20with%0Atechnology%2C%20but%20their%20personalization%20to%20individual%20user%20preferences%20remains%20a%0Asignificant%20challenge%2C%20particularly%20in%20on-device%20applications.%20Traditional%0Amethods%20often%20depend%20heavily%20on%20labeled%20datasets%20and%20can%20be%20resource-intensive.%0ATo%20address%20these%20issues%2C%20we%20present%20Adaptive%20Self-Supervised%20Learning%0AStrategies%20%28ASLS%29%2C%20which%20utilizes%20self-supervised%20learning%20techniques%20to%0Apersonalize%20LLMs%20dynamically.%20The%20framework%20comprises%20a%20user%20profiling%20layer%0Afor%20collecting%20interaction%20data%20and%20a%20neural%20adaptation%20layer%20for%20real-time%0Amodel%20fine-tuning.%20This%20innovative%20approach%20enables%20continuous%20learning%20from%0Auser%20feedback%2C%20allowing%20the%20model%20to%20generate%20responses%20that%20align%20closely%20with%0Auser-specific%20contexts.%20The%20adaptive%20mechanisms%20of%20ASLS%20minimize%20computational%0Ademands%20and%20enhance%20personalization%20efficiency.%20Experimental%20results%20across%0Avarious%20user%20scenarios%20illustrate%20the%20superior%20performance%20of%20ASLS%20in%20boosting%0Auser%20engagement%20and%20satisfaction%2C%20highlighting%20its%20potential%20to%20redefine%20LLMs%0Aas%20highly%20responsive%20and%20context-aware%20systems%20on-device.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16973v1&entry.124074799=Read"},
{"title": "FineZip : Pushing the Limits of Large Language Models for Practical\n  Lossless Text Compression", "author": "Fazal Mittu and Yihuan Bu and Akshat Gupta and Ashok Devireddy and Alp Eren Ozdarendeli and Anant Singh and Gopala Anumanchipalli", "abstract": "  While the language modeling objective has been shown to be deeply connected\nwith compression, it is surprising that modern LLMs are not employed in\npractical text compression systems. In this paper, we provide an in-depth\nanalysis of neural network and transformer-based compression techniques to\nanswer this question. We compare traditional text compression systems with\nneural network and LLM-based text compression methods. Although LLM-based\nsystems significantly outperform conventional compression methods, they are\nhighly impractical. Specifically, LLMZip, a recent text compression system\nusing Llama3-8B requires 9.5 days to compress just 10 MB of text, although with\nhuge improvements in compression ratios. To overcome this, we present FineZip -\na novel LLM-based text compression system that combines ideas of online\nmemorization and dynamic context to reduce the compression time immensely.\nFineZip can compress the above corpus in approximately 4 hours compared to 9.5\ndays, a 54 times improvement over LLMZip and comparable performance. FineZip\noutperforms traditional algorithmic compression methods with a large margin,\nimproving compression ratios by approximately 50\\%. With this work, we take the\nfirst step towards making lossless text compression with LLMs a reality. While\nFineZip presents a significant step in that direction, LLMs are still not a\nviable solution for large-scale text compression. We hope our work paves the\nway for future research and innovation to solve this problem.\n", "link": "http://arxiv.org/abs/2409.17141v1", "date": "2024-09-25", "relevancy": 2.0139, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5085}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5025}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FineZip%20%3A%20Pushing%20the%20Limits%20of%20Large%20Language%20Models%20for%20Practical%0A%20%20Lossless%20Text%20Compression&body=Title%3A%20FineZip%20%3A%20Pushing%20the%20Limits%20of%20Large%20Language%20Models%20for%20Practical%0A%20%20Lossless%20Text%20Compression%0AAuthor%3A%20Fazal%20Mittu%20and%20Yihuan%20Bu%20and%20Akshat%20Gupta%20and%20Ashok%20Devireddy%20and%20Alp%20Eren%20Ozdarendeli%20and%20Anant%20Singh%20and%20Gopala%20Anumanchipalli%0AAbstract%3A%20%20%20While%20the%20language%20modeling%20objective%20has%20been%20shown%20to%20be%20deeply%20connected%0Awith%20compression%2C%20it%20is%20surprising%20that%20modern%20LLMs%20are%20not%20employed%20in%0Apractical%20text%20compression%20systems.%20In%20this%20paper%2C%20we%20provide%20an%20in-depth%0Aanalysis%20of%20neural%20network%20and%20transformer-based%20compression%20techniques%20to%0Aanswer%20this%20question.%20We%20compare%20traditional%20text%20compression%20systems%20with%0Aneural%20network%20and%20LLM-based%20text%20compression%20methods.%20Although%20LLM-based%0Asystems%20significantly%20outperform%20conventional%20compression%20methods%2C%20they%20are%0Ahighly%20impractical.%20Specifically%2C%20LLMZip%2C%20a%20recent%20text%20compression%20system%0Ausing%20Llama3-8B%20requires%209.5%20days%20to%20compress%20just%2010%20MB%20of%20text%2C%20although%20with%0Ahuge%20improvements%20in%20compression%20ratios.%20To%20overcome%20this%2C%20we%20present%20FineZip%20-%0Aa%20novel%20LLM-based%20text%20compression%20system%20that%20combines%20ideas%20of%20online%0Amemorization%20and%20dynamic%20context%20to%20reduce%20the%20compression%20time%20immensely.%0AFineZip%20can%20compress%20the%20above%20corpus%20in%20approximately%204%20hours%20compared%20to%209.5%0Adays%2C%20a%2054%20times%20improvement%20over%20LLMZip%20and%20comparable%20performance.%20FineZip%0Aoutperforms%20traditional%20algorithmic%20compression%20methods%20with%20a%20large%20margin%2C%0Aimproving%20compression%20ratios%20by%20approximately%2050%5C%25.%20With%20this%20work%2C%20we%20take%20the%0Afirst%20step%20towards%20making%20lossless%20text%20compression%20with%20LLMs%20a%20reality.%20While%0AFineZip%20presents%20a%20significant%20step%20in%20that%20direction%2C%20LLMs%20are%20still%20not%20a%0Aviable%20solution%20for%20large-scale%20text%20compression.%20We%20hope%20our%20work%20paves%20the%0Away%20for%20future%20research%20and%20innovation%20to%20solve%20this%20problem.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17141v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFineZip%2520%253A%2520Pushing%2520the%2520Limits%2520of%2520Large%2520Language%2520Models%2520for%2520Practical%250A%2520%2520Lossless%2520Text%2520Compression%26entry.906535625%3DFazal%2520Mittu%2520and%2520Yihuan%2520Bu%2520and%2520Akshat%2520Gupta%2520and%2520Ashok%2520Devireddy%2520and%2520Alp%2520Eren%2520Ozdarendeli%2520and%2520Anant%2520Singh%2520and%2520Gopala%2520Anumanchipalli%26entry.1292438233%3D%2520%2520While%2520the%2520language%2520modeling%2520objective%2520has%2520been%2520shown%2520to%2520be%2520deeply%2520connected%250Awith%2520compression%252C%2520it%2520is%2520surprising%2520that%2520modern%2520LLMs%2520are%2520not%2520employed%2520in%250Apractical%2520text%2520compression%2520systems.%2520In%2520this%2520paper%252C%2520we%2520provide%2520an%2520in-depth%250Aanalysis%2520of%2520neural%2520network%2520and%2520transformer-based%2520compression%2520techniques%2520to%250Aanswer%2520this%2520question.%2520We%2520compare%2520traditional%2520text%2520compression%2520systems%2520with%250Aneural%2520network%2520and%2520LLM-based%2520text%2520compression%2520methods.%2520Although%2520LLM-based%250Asystems%2520significantly%2520outperform%2520conventional%2520compression%2520methods%252C%2520they%2520are%250Ahighly%2520impractical.%2520Specifically%252C%2520LLMZip%252C%2520a%2520recent%2520text%2520compression%2520system%250Ausing%2520Llama3-8B%2520requires%25209.5%2520days%2520to%2520compress%2520just%252010%2520MB%2520of%2520text%252C%2520although%2520with%250Ahuge%2520improvements%2520in%2520compression%2520ratios.%2520To%2520overcome%2520this%252C%2520we%2520present%2520FineZip%2520-%250Aa%2520novel%2520LLM-based%2520text%2520compression%2520system%2520that%2520combines%2520ideas%2520of%2520online%250Amemorization%2520and%2520dynamic%2520context%2520to%2520reduce%2520the%2520compression%2520time%2520immensely.%250AFineZip%2520can%2520compress%2520the%2520above%2520corpus%2520in%2520approximately%25204%2520hours%2520compared%2520to%25209.5%250Adays%252C%2520a%252054%2520times%2520improvement%2520over%2520LLMZip%2520and%2520comparable%2520performance.%2520FineZip%250Aoutperforms%2520traditional%2520algorithmic%2520compression%2520methods%2520with%2520a%2520large%2520margin%252C%250Aimproving%2520compression%2520ratios%2520by%2520approximately%252050%255C%2525.%2520With%2520this%2520work%252C%2520we%2520take%2520the%250Afirst%2520step%2520towards%2520making%2520lossless%2520text%2520compression%2520with%2520LLMs%2520a%2520reality.%2520While%250AFineZip%2520presents%2520a%2520significant%2520step%2520in%2520that%2520direction%252C%2520LLMs%2520are%2520still%2520not%2520a%250Aviable%2520solution%2520for%2520large-scale%2520text%2520compression.%2520We%2520hope%2520our%2520work%2520paves%2520the%250Away%2520for%2520future%2520research%2520and%2520innovation%2520to%2520solve%2520this%2520problem.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17141v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FineZip%20%3A%20Pushing%20the%20Limits%20of%20Large%20Language%20Models%20for%20Practical%0A%20%20Lossless%20Text%20Compression&entry.906535625=Fazal%20Mittu%20and%20Yihuan%20Bu%20and%20Akshat%20Gupta%20and%20Ashok%20Devireddy%20and%20Alp%20Eren%20Ozdarendeli%20and%20Anant%20Singh%20and%20Gopala%20Anumanchipalli&entry.1292438233=%20%20While%20the%20language%20modeling%20objective%20has%20been%20shown%20to%20be%20deeply%20connected%0Awith%20compression%2C%20it%20is%20surprising%20that%20modern%20LLMs%20are%20not%20employed%20in%0Apractical%20text%20compression%20systems.%20In%20this%20paper%2C%20we%20provide%20an%20in-depth%0Aanalysis%20of%20neural%20network%20and%20transformer-based%20compression%20techniques%20to%0Aanswer%20this%20question.%20We%20compare%20traditional%20text%20compression%20systems%20with%0Aneural%20network%20and%20LLM-based%20text%20compression%20methods.%20Although%20LLM-based%0Asystems%20significantly%20outperform%20conventional%20compression%20methods%2C%20they%20are%0Ahighly%20impractical.%20Specifically%2C%20LLMZip%2C%20a%20recent%20text%20compression%20system%0Ausing%20Llama3-8B%20requires%209.5%20days%20to%20compress%20just%2010%20MB%20of%20text%2C%20although%20with%0Ahuge%20improvements%20in%20compression%20ratios.%20To%20overcome%20this%2C%20we%20present%20FineZip%20-%0Aa%20novel%20LLM-based%20text%20compression%20system%20that%20combines%20ideas%20of%20online%0Amemorization%20and%20dynamic%20context%20to%20reduce%20the%20compression%20time%20immensely.%0AFineZip%20can%20compress%20the%20above%20corpus%20in%20approximately%204%20hours%20compared%20to%209.5%0Adays%2C%20a%2054%20times%20improvement%20over%20LLMZip%20and%20comparable%20performance.%20FineZip%0Aoutperforms%20traditional%20algorithmic%20compression%20methods%20with%20a%20large%20margin%2C%0Aimproving%20compression%20ratios%20by%20approximately%2050%5C%25.%20With%20this%20work%2C%20we%20take%20the%0Afirst%20step%20towards%20making%20lossless%20text%20compression%20with%20LLMs%20a%20reality.%20While%0AFineZip%20presents%20a%20significant%20step%20in%20that%20direction%2C%20LLMs%20are%20still%20not%20a%0Aviable%20solution%20for%20large-scale%20text%20compression.%20We%20hope%20our%20work%20paves%20the%0Away%20for%20future%20research%20and%20innovation%20to%20solve%20this%20problem.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17141v1&entry.124074799=Read"},
{"title": "Adaptive Error-Bounded Hierarchical Matrices for Efficient Neural\n  Network Compression", "author": "John Mango and Ronald Katende", "abstract": "  This paper introduces a dynamic, error-bounded hierarchical matrix (H-matrix)\ncompression method tailored for Physics-Informed Neural Networks (PINNs). The\nproposed approach reduces the computational complexity and memory demands of\nlarge-scale physics-based models while preserving the essential properties of\nthe Neural Tangent Kernel (NTK). By adaptively refining hierarchical matrix\napproximations based on local error estimates, our method ensures efficient\ntraining and robust model performance. Empirical results demonstrate that this\ntechnique outperforms traditional compression methods, such as Singular Value\nDecomposition (SVD), pruning, and quantization, by maintaining high accuracy\nand improving generalization capabilities. Additionally, the dynamic H-matrix\nmethod enhances inference speed, making it suitable for real-time applications.\nThis approach offers a scalable and efficient solution for deploying PINNs in\ncomplex scientific and engineering domains, bridging the gap between\ncomputational feasibility and real-world applicability.\n", "link": "http://arxiv.org/abs/2409.07028v2", "date": "2024-09-25", "relevancy": 2.0137, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5199}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.496}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Error-Bounded%20Hierarchical%20Matrices%20for%20Efficient%20Neural%0A%20%20Network%20Compression&body=Title%3A%20Adaptive%20Error-Bounded%20Hierarchical%20Matrices%20for%20Efficient%20Neural%0A%20%20Network%20Compression%0AAuthor%3A%20John%20Mango%20and%20Ronald%20Katende%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20dynamic%2C%20error-bounded%20hierarchical%20matrix%20%28H-matrix%29%0Acompression%20method%20tailored%20for%20Physics-Informed%20Neural%20Networks%20%28PINNs%29.%20The%0Aproposed%20approach%20reduces%20the%20computational%20complexity%20and%20memory%20demands%20of%0Alarge-scale%20physics-based%20models%20while%20preserving%20the%20essential%20properties%20of%0Athe%20Neural%20Tangent%20Kernel%20%28NTK%29.%20By%20adaptively%20refining%20hierarchical%20matrix%0Aapproximations%20based%20on%20local%20error%20estimates%2C%20our%20method%20ensures%20efficient%0Atraining%20and%20robust%20model%20performance.%20Empirical%20results%20demonstrate%20that%20this%0Atechnique%20outperforms%20traditional%20compression%20methods%2C%20such%20as%20Singular%20Value%0ADecomposition%20%28SVD%29%2C%20pruning%2C%20and%20quantization%2C%20by%20maintaining%20high%20accuracy%0Aand%20improving%20generalization%20capabilities.%20Additionally%2C%20the%20dynamic%20H-matrix%0Amethod%20enhances%20inference%20speed%2C%20making%20it%20suitable%20for%20real-time%20applications.%0AThis%20approach%20offers%20a%20scalable%20and%20efficient%20solution%20for%20deploying%20PINNs%20in%0Acomplex%20scientific%20and%20engineering%20domains%2C%20bridging%20the%20gap%20between%0Acomputational%20feasibility%20and%20real-world%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.07028v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Error-Bounded%2520Hierarchical%2520Matrices%2520for%2520Efficient%2520Neural%250A%2520%2520Network%2520Compression%26entry.906535625%3DJohn%2520Mango%2520and%2520Ronald%2520Katende%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520dynamic%252C%2520error-bounded%2520hierarchical%2520matrix%2520%2528H-matrix%2529%250Acompression%2520method%2520tailored%2520for%2520Physics-Informed%2520Neural%2520Networks%2520%2528PINNs%2529.%2520The%250Aproposed%2520approach%2520reduces%2520the%2520computational%2520complexity%2520and%2520memory%2520demands%2520of%250Alarge-scale%2520physics-based%2520models%2520while%2520preserving%2520the%2520essential%2520properties%2520of%250Athe%2520Neural%2520Tangent%2520Kernel%2520%2528NTK%2529.%2520By%2520adaptively%2520refining%2520hierarchical%2520matrix%250Aapproximations%2520based%2520on%2520local%2520error%2520estimates%252C%2520our%2520method%2520ensures%2520efficient%250Atraining%2520and%2520robust%2520model%2520performance.%2520Empirical%2520results%2520demonstrate%2520that%2520this%250Atechnique%2520outperforms%2520traditional%2520compression%2520methods%252C%2520such%2520as%2520Singular%2520Value%250ADecomposition%2520%2528SVD%2529%252C%2520pruning%252C%2520and%2520quantization%252C%2520by%2520maintaining%2520high%2520accuracy%250Aand%2520improving%2520generalization%2520capabilities.%2520Additionally%252C%2520the%2520dynamic%2520H-matrix%250Amethod%2520enhances%2520inference%2520speed%252C%2520making%2520it%2520suitable%2520for%2520real-time%2520applications.%250AThis%2520approach%2520offers%2520a%2520scalable%2520and%2520efficient%2520solution%2520for%2520deploying%2520PINNs%2520in%250Acomplex%2520scientific%2520and%2520engineering%2520domains%252C%2520bridging%2520the%2520gap%2520between%250Acomputational%2520feasibility%2520and%2520real-world%2520applicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.07028v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Error-Bounded%20Hierarchical%20Matrices%20for%20Efficient%20Neural%0A%20%20Network%20Compression&entry.906535625=John%20Mango%20and%20Ronald%20Katende&entry.1292438233=%20%20This%20paper%20introduces%20a%20dynamic%2C%20error-bounded%20hierarchical%20matrix%20%28H-matrix%29%0Acompression%20method%20tailored%20for%20Physics-Informed%20Neural%20Networks%20%28PINNs%29.%20The%0Aproposed%20approach%20reduces%20the%20computational%20complexity%20and%20memory%20demands%20of%0Alarge-scale%20physics-based%20models%20while%20preserving%20the%20essential%20properties%20of%0Athe%20Neural%20Tangent%20Kernel%20%28NTK%29.%20By%20adaptively%20refining%20hierarchical%20matrix%0Aapproximations%20based%20on%20local%20error%20estimates%2C%20our%20method%20ensures%20efficient%0Atraining%20and%20robust%20model%20performance.%20Empirical%20results%20demonstrate%20that%20this%0Atechnique%20outperforms%20traditional%20compression%20methods%2C%20such%20as%20Singular%20Value%0ADecomposition%20%28SVD%29%2C%20pruning%2C%20and%20quantization%2C%20by%20maintaining%20high%20accuracy%0Aand%20improving%20generalization%20capabilities.%20Additionally%2C%20the%20dynamic%20H-matrix%0Amethod%20enhances%20inference%20speed%2C%20making%20it%20suitable%20for%20real-time%20applications.%0AThis%20approach%20offers%20a%20scalable%20and%20efficient%20solution%20for%20deploying%20PINNs%20in%0Acomplex%20scientific%20and%20engineering%20domains%2C%20bridging%20the%20gap%20between%0Acomputational%20feasibility%20and%20real-world%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.07028v2&entry.124074799=Read"},
{"title": "BitQ: Tailoring Block Floating Point Precision for Improved DNN\n  Efficiency on Resource-Constrained Devices", "author": "Yongqi Xu and Yujian Lee and Gao Yi and Bosheng Liu and Yucong Chen and Peng Liu and Jigang Wu and Xiaoming Chen and Yinhe Han", "abstract": "  Deep neural networks (DNNs) are powerful for cognitive tasks such as image\nclassification, object detection, and scene segmentation. One drawback however\nis the significant high computational complexity and memory consumption, which\nmakes them unfeasible to run real-time on embedded platforms because of the\nlimited hardware resources. Block floating point (BFP) quantization is one of\nthe representative compression approaches for reducing the memory and\ncomputational burden owing to their capability to effectively capture the broad\ndata distribution of DNN models. Unfortunately, prior works on BFP-based\nquantization empirically choose the block size and the precision that preserve\naccuracy. In this paper, we develop a BFP-based bitwidth-aware analytical\nmodeling framework (called ``BitQ'') for the best BFP implementation of DNN\ninference on embedded platforms. We formulate and resolve an optimization\nproblem to identify the optimal BFP block size and bitwidth distribution by the\ntrade-off of both accuracy and performance loss. Experimental results show that\ncompared with an equal bitwidth setting, the BFP DNNs with optimized bitwidth\nallocation provide efficient computation, preserving accuracy on famous\nbenchmarks. The source code and data are available at\nhttps://github.com/Cheliosoops/BitQ.\n", "link": "http://arxiv.org/abs/2409.17093v1", "date": "2024-09-25", "relevancy": 2.0122, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5175}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5148}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BitQ%3A%20Tailoring%20Block%20Floating%20Point%20Precision%20for%20Improved%20DNN%0A%20%20Efficiency%20on%20Resource-Constrained%20Devices&body=Title%3A%20BitQ%3A%20Tailoring%20Block%20Floating%20Point%20Precision%20for%20Improved%20DNN%0A%20%20Efficiency%20on%20Resource-Constrained%20Devices%0AAuthor%3A%20Yongqi%20Xu%20and%20Yujian%20Lee%20and%20Gao%20Yi%20and%20Bosheng%20Liu%20and%20Yucong%20Chen%20and%20Peng%20Liu%20and%20Jigang%20Wu%20and%20Xiaoming%20Chen%20and%20Yinhe%20Han%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20powerful%20for%20cognitive%20tasks%20such%20as%20image%0Aclassification%2C%20object%20detection%2C%20and%20scene%20segmentation.%20One%20drawback%20however%0Ais%20the%20significant%20high%20computational%20complexity%20and%20memory%20consumption%2C%20which%0Amakes%20them%20unfeasible%20to%20run%20real-time%20on%20embedded%20platforms%20because%20of%20the%0Alimited%20hardware%20resources.%20Block%20floating%20point%20%28BFP%29%20quantization%20is%20one%20of%0Athe%20representative%20compression%20approaches%20for%20reducing%20the%20memory%20and%0Acomputational%20burden%20owing%20to%20their%20capability%20to%20effectively%20capture%20the%20broad%0Adata%20distribution%20of%20DNN%20models.%20Unfortunately%2C%20prior%20works%20on%20BFP-based%0Aquantization%20empirically%20choose%20the%20block%20size%20and%20the%20precision%20that%20preserve%0Aaccuracy.%20In%20this%20paper%2C%20we%20develop%20a%20BFP-based%20bitwidth-aware%20analytical%0Amodeling%20framework%20%28called%20%60%60BitQ%27%27%29%20for%20the%20best%20BFP%20implementation%20of%20DNN%0Ainference%20on%20embedded%20platforms.%20We%20formulate%20and%20resolve%20an%20optimization%0Aproblem%20to%20identify%20the%20optimal%20BFP%20block%20size%20and%20bitwidth%20distribution%20by%20the%0Atrade-off%20of%20both%20accuracy%20and%20performance%20loss.%20Experimental%20results%20show%20that%0Acompared%20with%20an%20equal%20bitwidth%20setting%2C%20the%20BFP%20DNNs%20with%20optimized%20bitwidth%0Aallocation%20provide%20efficient%20computation%2C%20preserving%20accuracy%20on%20famous%0Abenchmarks.%20The%20source%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/Cheliosoops/BitQ.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBitQ%253A%2520Tailoring%2520Block%2520Floating%2520Point%2520Precision%2520for%2520Improved%2520DNN%250A%2520%2520Efficiency%2520on%2520Resource-Constrained%2520Devices%26entry.906535625%3DYongqi%2520Xu%2520and%2520Yujian%2520Lee%2520and%2520Gao%2520Yi%2520and%2520Bosheng%2520Liu%2520and%2520Yucong%2520Chen%2520and%2520Peng%2520Liu%2520and%2520Jigang%2520Wu%2520and%2520Xiaoming%2520Chen%2520and%2520Yinhe%2520Han%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520are%2520powerful%2520for%2520cognitive%2520tasks%2520such%2520as%2520image%250Aclassification%252C%2520object%2520detection%252C%2520and%2520scene%2520segmentation.%2520One%2520drawback%2520however%250Ais%2520the%2520significant%2520high%2520computational%2520complexity%2520and%2520memory%2520consumption%252C%2520which%250Amakes%2520them%2520unfeasible%2520to%2520run%2520real-time%2520on%2520embedded%2520platforms%2520because%2520of%2520the%250Alimited%2520hardware%2520resources.%2520Block%2520floating%2520point%2520%2528BFP%2529%2520quantization%2520is%2520one%2520of%250Athe%2520representative%2520compression%2520approaches%2520for%2520reducing%2520the%2520memory%2520and%250Acomputational%2520burden%2520owing%2520to%2520their%2520capability%2520to%2520effectively%2520capture%2520the%2520broad%250Adata%2520distribution%2520of%2520DNN%2520models.%2520Unfortunately%252C%2520prior%2520works%2520on%2520BFP-based%250Aquantization%2520empirically%2520choose%2520the%2520block%2520size%2520and%2520the%2520precision%2520that%2520preserve%250Aaccuracy.%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%2520BFP-based%2520bitwidth-aware%2520analytical%250Amodeling%2520framework%2520%2528called%2520%2560%2560BitQ%2527%2527%2529%2520for%2520the%2520best%2520BFP%2520implementation%2520of%2520DNN%250Ainference%2520on%2520embedded%2520platforms.%2520We%2520formulate%2520and%2520resolve%2520an%2520optimization%250Aproblem%2520to%2520identify%2520the%2520optimal%2520BFP%2520block%2520size%2520and%2520bitwidth%2520distribution%2520by%2520the%250Atrade-off%2520of%2520both%2520accuracy%2520and%2520performance%2520loss.%2520Experimental%2520results%2520show%2520that%250Acompared%2520with%2520an%2520equal%2520bitwidth%2520setting%252C%2520the%2520BFP%2520DNNs%2520with%2520optimized%2520bitwidth%250Aallocation%2520provide%2520efficient%2520computation%252C%2520preserving%2520accuracy%2520on%2520famous%250Abenchmarks.%2520The%2520source%2520code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/Cheliosoops/BitQ.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BitQ%3A%20Tailoring%20Block%20Floating%20Point%20Precision%20for%20Improved%20DNN%0A%20%20Efficiency%20on%20Resource-Constrained%20Devices&entry.906535625=Yongqi%20Xu%20and%20Yujian%20Lee%20and%20Gao%20Yi%20and%20Bosheng%20Liu%20and%20Yucong%20Chen%20and%20Peng%20Liu%20and%20Jigang%20Wu%20and%20Xiaoming%20Chen%20and%20Yinhe%20Han&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20powerful%20for%20cognitive%20tasks%20such%20as%20image%0Aclassification%2C%20object%20detection%2C%20and%20scene%20segmentation.%20One%20drawback%20however%0Ais%20the%20significant%20high%20computational%20complexity%20and%20memory%20consumption%2C%20which%0Amakes%20them%20unfeasible%20to%20run%20real-time%20on%20embedded%20platforms%20because%20of%20the%0Alimited%20hardware%20resources.%20Block%20floating%20point%20%28BFP%29%20quantization%20is%20one%20of%0Athe%20representative%20compression%20approaches%20for%20reducing%20the%20memory%20and%0Acomputational%20burden%20owing%20to%20their%20capability%20to%20effectively%20capture%20the%20broad%0Adata%20distribution%20of%20DNN%20models.%20Unfortunately%2C%20prior%20works%20on%20BFP-based%0Aquantization%20empirically%20choose%20the%20block%20size%20and%20the%20precision%20that%20preserve%0Aaccuracy.%20In%20this%20paper%2C%20we%20develop%20a%20BFP-based%20bitwidth-aware%20analytical%0Amodeling%20framework%20%28called%20%60%60BitQ%27%27%29%20for%20the%20best%20BFP%20implementation%20of%20DNN%0Ainference%20on%20embedded%20platforms.%20We%20formulate%20and%20resolve%20an%20optimization%0Aproblem%20to%20identify%20the%20optimal%20BFP%20block%20size%20and%20bitwidth%20distribution%20by%20the%0Atrade-off%20of%20both%20accuracy%20and%20performance%20loss.%20Experimental%20results%20show%20that%0Acompared%20with%20an%20equal%20bitwidth%20setting%2C%20the%20BFP%20DNNs%20with%20optimized%20bitwidth%0Aallocation%20provide%20efficient%20computation%2C%20preserving%20accuracy%20on%20famous%0Abenchmarks.%20The%20source%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/Cheliosoops/BitQ.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17093v1&entry.124074799=Read"},
{"title": "Simple Image Signal Processing using Global Context Guidance", "author": "Omar Elezabi and Marcos V. Conde and Radu Timofte", "abstract": "  In modern smartphone cameras, the Image Signal Processor (ISP) is the core\nelement that converts the RAW readings from the sensor into perceptually\npleasant RGB images for the end users. The ISP is typically proprietary and\nhandcrafted and consists of several blocks such as white balance, color\ncorrection, and tone mapping. Deep learning-based ISPs aim to transform RAW\nimages into DSLR-like RGB images using deep neural networks. However, most\nlearned ISPs are trained using patches (small regions) due to computational\nlimitations. Such methods lack global context, which limits their efficacy on\nfull-resolution images and harms their ability to capture global properties\nsuch as color constancy or illumination. First, we propose a novel module that\ncan be integrated into any neural ISP to capture the global context information\nfrom the full RAW images. Second, we propose an efficient and simple neural ISP\nthat utilizes our proposed module. Our model achieves state-of-the-art results\non different benchmarks using diverse and real smartphone images.\n", "link": "http://arxiv.org/abs/2404.11569v2", "date": "2024-09-25", "relevancy": 2.0079, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5427}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4985}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simple%20Image%20Signal%20Processing%20using%20Global%20Context%20Guidance&body=Title%3A%20Simple%20Image%20Signal%20Processing%20using%20Global%20Context%20Guidance%0AAuthor%3A%20Omar%20Elezabi%20and%20Marcos%20V.%20Conde%20and%20Radu%20Timofte%0AAbstract%3A%20%20%20In%20modern%20smartphone%20cameras%2C%20the%20Image%20Signal%20Processor%20%28ISP%29%20is%20the%20core%0Aelement%20that%20converts%20the%20RAW%20readings%20from%20the%20sensor%20into%20perceptually%0Apleasant%20RGB%20images%20for%20the%20end%20users.%20The%20ISP%20is%20typically%20proprietary%20and%0Ahandcrafted%20and%20consists%20of%20several%20blocks%20such%20as%20white%20balance%2C%20color%0Acorrection%2C%20and%20tone%20mapping.%20Deep%20learning-based%20ISPs%20aim%20to%20transform%20RAW%0Aimages%20into%20DSLR-like%20RGB%20images%20using%20deep%20neural%20networks.%20However%2C%20most%0Alearned%20ISPs%20are%20trained%20using%20patches%20%28small%20regions%29%20due%20to%20computational%0Alimitations.%20Such%20methods%20lack%20global%20context%2C%20which%20limits%20their%20efficacy%20on%0Afull-resolution%20images%20and%20harms%20their%20ability%20to%20capture%20global%20properties%0Asuch%20as%20color%20constancy%20or%20illumination.%20First%2C%20we%20propose%20a%20novel%20module%20that%0Acan%20be%20integrated%20into%20any%20neural%20ISP%20to%20capture%20the%20global%20context%20information%0Afrom%20the%20full%20RAW%20images.%20Second%2C%20we%20propose%20an%20efficient%20and%20simple%20neural%20ISP%0Athat%20utilizes%20our%20proposed%20module.%20Our%20model%20achieves%20state-of-the-art%20results%0Aon%20different%20benchmarks%20using%20diverse%20and%20real%20smartphone%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11569v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimple%2520Image%2520Signal%2520Processing%2520using%2520Global%2520Context%2520Guidance%26entry.906535625%3DOmar%2520Elezabi%2520and%2520Marcos%2520V.%2520Conde%2520and%2520Radu%2520Timofte%26entry.1292438233%3D%2520%2520In%2520modern%2520smartphone%2520cameras%252C%2520the%2520Image%2520Signal%2520Processor%2520%2528ISP%2529%2520is%2520the%2520core%250Aelement%2520that%2520converts%2520the%2520RAW%2520readings%2520from%2520the%2520sensor%2520into%2520perceptually%250Apleasant%2520RGB%2520images%2520for%2520the%2520end%2520users.%2520The%2520ISP%2520is%2520typically%2520proprietary%2520and%250Ahandcrafted%2520and%2520consists%2520of%2520several%2520blocks%2520such%2520as%2520white%2520balance%252C%2520color%250Acorrection%252C%2520and%2520tone%2520mapping.%2520Deep%2520learning-based%2520ISPs%2520aim%2520to%2520transform%2520RAW%250Aimages%2520into%2520DSLR-like%2520RGB%2520images%2520using%2520deep%2520neural%2520networks.%2520However%252C%2520most%250Alearned%2520ISPs%2520are%2520trained%2520using%2520patches%2520%2528small%2520regions%2529%2520due%2520to%2520computational%250Alimitations.%2520Such%2520methods%2520lack%2520global%2520context%252C%2520which%2520limits%2520their%2520efficacy%2520on%250Afull-resolution%2520images%2520and%2520harms%2520their%2520ability%2520to%2520capture%2520global%2520properties%250Asuch%2520as%2520color%2520constancy%2520or%2520illumination.%2520First%252C%2520we%2520propose%2520a%2520novel%2520module%2520that%250Acan%2520be%2520integrated%2520into%2520any%2520neural%2520ISP%2520to%2520capture%2520the%2520global%2520context%2520information%250Afrom%2520the%2520full%2520RAW%2520images.%2520Second%252C%2520we%2520propose%2520an%2520efficient%2520and%2520simple%2520neural%2520ISP%250Athat%2520utilizes%2520our%2520proposed%2520module.%2520Our%2520model%2520achieves%2520state-of-the-art%2520results%250Aon%2520different%2520benchmarks%2520using%2520diverse%2520and%2520real%2520smartphone%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11569v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%20Image%20Signal%20Processing%20using%20Global%20Context%20Guidance&entry.906535625=Omar%20Elezabi%20and%20Marcos%20V.%20Conde%20and%20Radu%20Timofte&entry.1292438233=%20%20In%20modern%20smartphone%20cameras%2C%20the%20Image%20Signal%20Processor%20%28ISP%29%20is%20the%20core%0Aelement%20that%20converts%20the%20RAW%20readings%20from%20the%20sensor%20into%20perceptually%0Apleasant%20RGB%20images%20for%20the%20end%20users.%20The%20ISP%20is%20typically%20proprietary%20and%0Ahandcrafted%20and%20consists%20of%20several%20blocks%20such%20as%20white%20balance%2C%20color%0Acorrection%2C%20and%20tone%20mapping.%20Deep%20learning-based%20ISPs%20aim%20to%20transform%20RAW%0Aimages%20into%20DSLR-like%20RGB%20images%20using%20deep%20neural%20networks.%20However%2C%20most%0Alearned%20ISPs%20are%20trained%20using%20patches%20%28small%20regions%29%20due%20to%20computational%0Alimitations.%20Such%20methods%20lack%20global%20context%2C%20which%20limits%20their%20efficacy%20on%0Afull-resolution%20images%20and%20harms%20their%20ability%20to%20capture%20global%20properties%0Asuch%20as%20color%20constancy%20or%20illumination.%20First%2C%20we%20propose%20a%20novel%20module%20that%0Acan%20be%20integrated%20into%20any%20neural%20ISP%20to%20capture%20the%20global%20context%20information%0Afrom%20the%20full%20RAW%20images.%20Second%2C%20we%20propose%20an%20efficient%20and%20simple%20neural%20ISP%0Athat%20utilizes%20our%20proposed%20module.%20Our%20model%20achieves%20state-of-the-art%20results%0Aon%20different%20benchmarks%20using%20diverse%20and%20real%20smartphone%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11569v2&entry.124074799=Read"},
{"title": "Exposing Assumptions in AI Benchmarks through Cognitive Modelling", "author": "Jonathan H. Rystr\u00f8m and Kenneth C. Enevoldsen", "abstract": "  Cultural AI benchmarks often rely on implicit assumptions about measured\nconstructs, leading to vague formulations with poor validity and unclear\ninterrelations. We propose exposing these assumptions using explicit cognitive\nmodels formulated as Structural Equation Models. Using cross-lingual alignment\ntransfer as an example, we show how this approach can answer key research\nquestions and identify missing datasets. This framework grounds benchmark\nconstruction theoretically and guides dataset development to improve construct\nmeasurement. By embracing transparency, we move towards more rigorous,\ncumulative AI evaluation science, challenging researchers to critically examine\ntheir assessment foundations.\n", "link": "http://arxiv.org/abs/2409.16849v1", "date": "2024-09-25", "relevancy": 2.0065, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5077}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5004}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exposing%20Assumptions%20in%20AI%20Benchmarks%20through%20Cognitive%20Modelling&body=Title%3A%20Exposing%20Assumptions%20in%20AI%20Benchmarks%20through%20Cognitive%20Modelling%0AAuthor%3A%20Jonathan%20H.%20Rystr%C3%B8m%20and%20Kenneth%20C.%20Enevoldsen%0AAbstract%3A%20%20%20Cultural%20AI%20benchmarks%20often%20rely%20on%20implicit%20assumptions%20about%20measured%0Aconstructs%2C%20leading%20to%20vague%20formulations%20with%20poor%20validity%20and%20unclear%0Ainterrelations.%20We%20propose%20exposing%20these%20assumptions%20using%20explicit%20cognitive%0Amodels%20formulated%20as%20Structural%20Equation%20Models.%20Using%20cross-lingual%20alignment%0Atransfer%20as%20an%20example%2C%20we%20show%20how%20this%20approach%20can%20answer%20key%20research%0Aquestions%20and%20identify%20missing%20datasets.%20This%20framework%20grounds%20benchmark%0Aconstruction%20theoretically%20and%20guides%20dataset%20development%20to%20improve%20construct%0Ameasurement.%20By%20embracing%20transparency%2C%20we%20move%20towards%20more%20rigorous%2C%0Acumulative%20AI%20evaluation%20science%2C%20challenging%20researchers%20to%20critically%20examine%0Atheir%20assessment%20foundations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16849v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExposing%2520Assumptions%2520in%2520AI%2520Benchmarks%2520through%2520Cognitive%2520Modelling%26entry.906535625%3DJonathan%2520H.%2520Rystr%25C3%25B8m%2520and%2520Kenneth%2520C.%2520Enevoldsen%26entry.1292438233%3D%2520%2520Cultural%2520AI%2520benchmarks%2520often%2520rely%2520on%2520implicit%2520assumptions%2520about%2520measured%250Aconstructs%252C%2520leading%2520to%2520vague%2520formulations%2520with%2520poor%2520validity%2520and%2520unclear%250Ainterrelations.%2520We%2520propose%2520exposing%2520these%2520assumptions%2520using%2520explicit%2520cognitive%250Amodels%2520formulated%2520as%2520Structural%2520Equation%2520Models.%2520Using%2520cross-lingual%2520alignment%250Atransfer%2520as%2520an%2520example%252C%2520we%2520show%2520how%2520this%2520approach%2520can%2520answer%2520key%2520research%250Aquestions%2520and%2520identify%2520missing%2520datasets.%2520This%2520framework%2520grounds%2520benchmark%250Aconstruction%2520theoretically%2520and%2520guides%2520dataset%2520development%2520to%2520improve%2520construct%250Ameasurement.%2520By%2520embracing%2520transparency%252C%2520we%2520move%2520towards%2520more%2520rigorous%252C%250Acumulative%2520AI%2520evaluation%2520science%252C%2520challenging%2520researchers%2520to%2520critically%2520examine%250Atheir%2520assessment%2520foundations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16849v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exposing%20Assumptions%20in%20AI%20Benchmarks%20through%20Cognitive%20Modelling&entry.906535625=Jonathan%20H.%20Rystr%C3%B8m%20and%20Kenneth%20C.%20Enevoldsen&entry.1292438233=%20%20Cultural%20AI%20benchmarks%20often%20rely%20on%20implicit%20assumptions%20about%20measured%0Aconstructs%2C%20leading%20to%20vague%20formulations%20with%20poor%20validity%20and%20unclear%0Ainterrelations.%20We%20propose%20exposing%20these%20assumptions%20using%20explicit%20cognitive%0Amodels%20formulated%20as%20Structural%20Equation%20Models.%20Using%20cross-lingual%20alignment%0Atransfer%20as%20an%20example%2C%20we%20show%20how%20this%20approach%20can%20answer%20key%20research%0Aquestions%20and%20identify%20missing%20datasets.%20This%20framework%20grounds%20benchmark%0Aconstruction%20theoretically%20and%20guides%20dataset%20development%20to%20improve%20construct%0Ameasurement.%20By%20embracing%20transparency%2C%20we%20move%20towards%20more%20rigorous%2C%0Acumulative%20AI%20evaluation%20science%2C%20challenging%20researchers%20to%20critically%20examine%0Atheir%20assessment%20foundations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16849v1&entry.124074799=Read"},
{"title": "Counterfactual Token Generation in Large Language Models", "author": "Ivi Chatzi and Nina Corvelo Benz and Eleni Straitouri and Stratis Tsirtsis and Manuel Gomez-Rodriguez", "abstract": "  \"Sure, I am happy to generate a story for you: Captain Lyra stood at the helm\nof her trusty ship, the Maelstrom's Fury, gazing out at the endless sea. [...]\nLyra's eyes welled up with tears as she realized the bitter truth - she had\nsacrificed everything for fleeting riches, and lost the love of her crew, her\nfamily, and herself.\" Although this story, generated by a large language model,\nis captivating, one may wonder -- how would the story have unfolded if the\nmodel had chosen \"Captain Maeve\" as the protagonist instead? We cannot know.\nState-of-the-art large language models are stateless -- they maintain no\ninternal memory or state. Given a prompt, they generate a sequence of tokens as\nan output using an autoregressive process. As a consequence, they cannot reason\nabout counterfactual alternatives to tokens they have generated in the past. In\nthis work, our goal is to enhance them with this functionality. To this end, we\ndevelop a causal model of token generation that builds upon the Gumbel-Max\nstructural causal model. Our model allows any large language model to perform\ncounterfactual token generation at almost no cost in comparison with vanilla\ntoken generation, it is embarrassingly simple to implement, and it does not\nrequire any fine-tuning nor prompt engineering. We implement our model on Llama\n3 8B-instruct and conduct both qualitative and quantitative analyses of\ncounterfactually generated text. We conclude with a demonstrative application\nof counterfactual token generation for bias detection, unveiling interesting\ninsights about the model of the world constructed by large language models.\n", "link": "http://arxiv.org/abs/2409.17027v1", "date": "2024-09-25", "relevancy": 2.0046, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5157}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4911}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Counterfactual%20Token%20Generation%20in%20Large%20Language%20Models&body=Title%3A%20Counterfactual%20Token%20Generation%20in%20Large%20Language%20Models%0AAuthor%3A%20Ivi%20Chatzi%20and%20Nina%20Corvelo%20Benz%20and%20Eleni%20Straitouri%20and%20Stratis%20Tsirtsis%20and%20Manuel%20Gomez-Rodriguez%0AAbstract%3A%20%20%20%22Sure%2C%20I%20am%20happy%20to%20generate%20a%20story%20for%20you%3A%20Captain%20Lyra%20stood%20at%20the%20helm%0Aof%20her%20trusty%20ship%2C%20the%20Maelstrom%27s%20Fury%2C%20gazing%20out%20at%20the%20endless%20sea.%20%5B...%5D%0ALyra%27s%20eyes%20welled%20up%20with%20tears%20as%20she%20realized%20the%20bitter%20truth%20-%20she%20had%0Asacrificed%20everything%20for%20fleeting%20riches%2C%20and%20lost%20the%20love%20of%20her%20crew%2C%20her%0Afamily%2C%20and%20herself.%22%20Although%20this%20story%2C%20generated%20by%20a%20large%20language%20model%2C%0Ais%20captivating%2C%20one%20may%20wonder%20--%20how%20would%20the%20story%20have%20unfolded%20if%20the%0Amodel%20had%20chosen%20%22Captain%20Maeve%22%20as%20the%20protagonist%20instead%3F%20We%20cannot%20know.%0AState-of-the-art%20large%20language%20models%20are%20stateless%20--%20they%20maintain%20no%0Ainternal%20memory%20or%20state.%20Given%20a%20prompt%2C%20they%20generate%20a%20sequence%20of%20tokens%20as%0Aan%20output%20using%20an%20autoregressive%20process.%20As%20a%20consequence%2C%20they%20cannot%20reason%0Aabout%20counterfactual%20alternatives%20to%20tokens%20they%20have%20generated%20in%20the%20past.%20In%0Athis%20work%2C%20our%20goal%20is%20to%20enhance%20them%20with%20this%20functionality.%20To%20this%20end%2C%20we%0Adevelop%20a%20causal%20model%20of%20token%20generation%20that%20builds%20upon%20the%20Gumbel-Max%0Astructural%20causal%20model.%20Our%20model%20allows%20any%20large%20language%20model%20to%20perform%0Acounterfactual%20token%20generation%20at%20almost%20no%20cost%20in%20comparison%20with%20vanilla%0Atoken%20generation%2C%20it%20is%20embarrassingly%20simple%20to%20implement%2C%20and%20it%20does%20not%0Arequire%20any%20fine-tuning%20nor%20prompt%20engineering.%20We%20implement%20our%20model%20on%20Llama%0A3%208B-instruct%20and%20conduct%20both%20qualitative%20and%20quantitative%20analyses%20of%0Acounterfactually%20generated%20text.%20We%20conclude%20with%20a%20demonstrative%20application%0Aof%20counterfactual%20token%20generation%20for%20bias%20detection%2C%20unveiling%20interesting%0Ainsights%20about%20the%20model%20of%20the%20world%20constructed%20by%20large%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17027v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCounterfactual%2520Token%2520Generation%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DIvi%2520Chatzi%2520and%2520Nina%2520Corvelo%2520Benz%2520and%2520Eleni%2520Straitouri%2520and%2520Stratis%2520Tsirtsis%2520and%2520Manuel%2520Gomez-Rodriguez%26entry.1292438233%3D%2520%2520%2522Sure%252C%2520I%2520am%2520happy%2520to%2520generate%2520a%2520story%2520for%2520you%253A%2520Captain%2520Lyra%2520stood%2520at%2520the%2520helm%250Aof%2520her%2520trusty%2520ship%252C%2520the%2520Maelstrom%2527s%2520Fury%252C%2520gazing%2520out%2520at%2520the%2520endless%2520sea.%2520%255B...%255D%250ALyra%2527s%2520eyes%2520welled%2520up%2520with%2520tears%2520as%2520she%2520realized%2520the%2520bitter%2520truth%2520-%2520she%2520had%250Asacrificed%2520everything%2520for%2520fleeting%2520riches%252C%2520and%2520lost%2520the%2520love%2520of%2520her%2520crew%252C%2520her%250Afamily%252C%2520and%2520herself.%2522%2520Although%2520this%2520story%252C%2520generated%2520by%2520a%2520large%2520language%2520model%252C%250Ais%2520captivating%252C%2520one%2520may%2520wonder%2520--%2520how%2520would%2520the%2520story%2520have%2520unfolded%2520if%2520the%250Amodel%2520had%2520chosen%2520%2522Captain%2520Maeve%2522%2520as%2520the%2520protagonist%2520instead%253F%2520We%2520cannot%2520know.%250AState-of-the-art%2520large%2520language%2520models%2520are%2520stateless%2520--%2520they%2520maintain%2520no%250Ainternal%2520memory%2520or%2520state.%2520Given%2520a%2520prompt%252C%2520they%2520generate%2520a%2520sequence%2520of%2520tokens%2520as%250Aan%2520output%2520using%2520an%2520autoregressive%2520process.%2520As%2520a%2520consequence%252C%2520they%2520cannot%2520reason%250Aabout%2520counterfactual%2520alternatives%2520to%2520tokens%2520they%2520have%2520generated%2520in%2520the%2520past.%2520In%250Athis%2520work%252C%2520our%2520goal%2520is%2520to%2520enhance%2520them%2520with%2520this%2520functionality.%2520To%2520this%2520end%252C%2520we%250Adevelop%2520a%2520causal%2520model%2520of%2520token%2520generation%2520that%2520builds%2520upon%2520the%2520Gumbel-Max%250Astructural%2520causal%2520model.%2520Our%2520model%2520allows%2520any%2520large%2520language%2520model%2520to%2520perform%250Acounterfactual%2520token%2520generation%2520at%2520almost%2520no%2520cost%2520in%2520comparison%2520with%2520vanilla%250Atoken%2520generation%252C%2520it%2520is%2520embarrassingly%2520simple%2520to%2520implement%252C%2520and%2520it%2520does%2520not%250Arequire%2520any%2520fine-tuning%2520nor%2520prompt%2520engineering.%2520We%2520implement%2520our%2520model%2520on%2520Llama%250A3%25208B-instruct%2520and%2520conduct%2520both%2520qualitative%2520and%2520quantitative%2520analyses%2520of%250Acounterfactually%2520generated%2520text.%2520We%2520conclude%2520with%2520a%2520demonstrative%2520application%250Aof%2520counterfactual%2520token%2520generation%2520for%2520bias%2520detection%252C%2520unveiling%2520interesting%250Ainsights%2520about%2520the%2520model%2520of%2520the%2520world%2520constructed%2520by%2520large%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17027v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Counterfactual%20Token%20Generation%20in%20Large%20Language%20Models&entry.906535625=Ivi%20Chatzi%20and%20Nina%20Corvelo%20Benz%20and%20Eleni%20Straitouri%20and%20Stratis%20Tsirtsis%20and%20Manuel%20Gomez-Rodriguez&entry.1292438233=%20%20%22Sure%2C%20I%20am%20happy%20to%20generate%20a%20story%20for%20you%3A%20Captain%20Lyra%20stood%20at%20the%20helm%0Aof%20her%20trusty%20ship%2C%20the%20Maelstrom%27s%20Fury%2C%20gazing%20out%20at%20the%20endless%20sea.%20%5B...%5D%0ALyra%27s%20eyes%20welled%20up%20with%20tears%20as%20she%20realized%20the%20bitter%20truth%20-%20she%20had%0Asacrificed%20everything%20for%20fleeting%20riches%2C%20and%20lost%20the%20love%20of%20her%20crew%2C%20her%0Afamily%2C%20and%20herself.%22%20Although%20this%20story%2C%20generated%20by%20a%20large%20language%20model%2C%0Ais%20captivating%2C%20one%20may%20wonder%20--%20how%20would%20the%20story%20have%20unfolded%20if%20the%0Amodel%20had%20chosen%20%22Captain%20Maeve%22%20as%20the%20protagonist%20instead%3F%20We%20cannot%20know.%0AState-of-the-art%20large%20language%20models%20are%20stateless%20--%20they%20maintain%20no%0Ainternal%20memory%20or%20state.%20Given%20a%20prompt%2C%20they%20generate%20a%20sequence%20of%20tokens%20as%0Aan%20output%20using%20an%20autoregressive%20process.%20As%20a%20consequence%2C%20they%20cannot%20reason%0Aabout%20counterfactual%20alternatives%20to%20tokens%20they%20have%20generated%20in%20the%20past.%20In%0Athis%20work%2C%20our%20goal%20is%20to%20enhance%20them%20with%20this%20functionality.%20To%20this%20end%2C%20we%0Adevelop%20a%20causal%20model%20of%20token%20generation%20that%20builds%20upon%20the%20Gumbel-Max%0Astructural%20causal%20model.%20Our%20model%20allows%20any%20large%20language%20model%20to%20perform%0Acounterfactual%20token%20generation%20at%20almost%20no%20cost%20in%20comparison%20with%20vanilla%0Atoken%20generation%2C%20it%20is%20embarrassingly%20simple%20to%20implement%2C%20and%20it%20does%20not%0Arequire%20any%20fine-tuning%20nor%20prompt%20engineering.%20We%20implement%20our%20model%20on%20Llama%0A3%208B-instruct%20and%20conduct%20both%20qualitative%20and%20quantitative%20analyses%20of%0Acounterfactually%20generated%20text.%20We%20conclude%20with%20a%20demonstrative%20application%0Aof%20counterfactual%20token%20generation%20for%20bias%20detection%2C%20unveiling%20interesting%0Ainsights%20about%20the%20model%20of%20the%20world%20constructed%20by%20large%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17027v1&entry.124074799=Read"},
{"title": "Accumulator-Aware Post-Training Quantization", "author": "Ian Colbert and Fabian Grob and Giuseppe Franco and Jinjie Zhang and Rayan Saab", "abstract": "  Several recent studies have investigated low-precision accumulation,\nreporting improvements in throughput, power, and area across various platforms.\nHowever, the accompanying proposals have only considered the quantization-aware\ntraining (QAT) paradigm, in which models are fine-tuned or trained from scratch\nwith quantization in the loop. As models continue to grow in size, QAT\ntechniques become increasingly more expensive, which has motivated the recent\nsurge in post-training quantization (PTQ) research. To the best of our\nknowledge, ours marks the first formal study of accumulator-aware quantization\nin the PTQ setting. To bridge this gap, we introduce AXE, a practical framework\nof accumulator-aware extensions designed to endow overflow avoidance guarantees\nto existing layer-wise PTQ algorithms. We theoretically motivate AXE and\ndemonstrate its flexibility by implementing it on top of two state-of-the-art\nPTQ algorithms: GPFQ and OPTQ. We further generalize AXE to support multi-stage\naccumulation for the first time, opening the door for full datapath\noptimization and scaling to large language models (LLMs). We evaluate AXE\nacross image classification and language generation models, and observe\nsignificant improvements in the trade-off between accumulator bit width and\nmodel accuracy over baseline methods.\n", "link": "http://arxiv.org/abs/2409.17092v1", "date": "2024-09-25", "relevancy": 1.9914, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5285}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4772}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accumulator-Aware%20Post-Training%20Quantization&body=Title%3A%20Accumulator-Aware%20Post-Training%20Quantization%0AAuthor%3A%20Ian%20Colbert%20and%20Fabian%20Grob%20and%20Giuseppe%20Franco%20and%20Jinjie%20Zhang%20and%20Rayan%20Saab%0AAbstract%3A%20%20%20Several%20recent%20studies%20have%20investigated%20low-precision%20accumulation%2C%0Areporting%20improvements%20in%20throughput%2C%20power%2C%20and%20area%20across%20various%20platforms.%0AHowever%2C%20the%20accompanying%20proposals%20have%20only%20considered%20the%20quantization-aware%0Atraining%20%28QAT%29%20paradigm%2C%20in%20which%20models%20are%20fine-tuned%20or%20trained%20from%20scratch%0Awith%20quantization%20in%20the%20loop.%20As%20models%20continue%20to%20grow%20in%20size%2C%20QAT%0Atechniques%20become%20increasingly%20more%20expensive%2C%20which%20has%20motivated%20the%20recent%0Asurge%20in%20post-training%20quantization%20%28PTQ%29%20research.%20To%20the%20best%20of%20our%0Aknowledge%2C%20ours%20marks%20the%20first%20formal%20study%20of%20accumulator-aware%20quantization%0Ain%20the%20PTQ%20setting.%20To%20bridge%20this%20gap%2C%20we%20introduce%20AXE%2C%20a%20practical%20framework%0Aof%20accumulator-aware%20extensions%20designed%20to%20endow%20overflow%20avoidance%20guarantees%0Ato%20existing%20layer-wise%20PTQ%20algorithms.%20We%20theoretically%20motivate%20AXE%20and%0Ademonstrate%20its%20flexibility%20by%20implementing%20it%20on%20top%20of%20two%20state-of-the-art%0APTQ%20algorithms%3A%20GPFQ%20and%20OPTQ.%20We%20further%20generalize%20AXE%20to%20support%20multi-stage%0Aaccumulation%20for%20the%20first%20time%2C%20opening%20the%20door%20for%20full%20datapath%0Aoptimization%20and%20scaling%20to%20large%20language%20models%20%28LLMs%29.%20We%20evaluate%20AXE%0Aacross%20image%20classification%20and%20language%20generation%20models%2C%20and%20observe%0Asignificant%20improvements%20in%20the%20trade-off%20between%20accumulator%20bit%20width%20and%0Amodel%20accuracy%20over%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccumulator-Aware%2520Post-Training%2520Quantization%26entry.906535625%3DIan%2520Colbert%2520and%2520Fabian%2520Grob%2520and%2520Giuseppe%2520Franco%2520and%2520Jinjie%2520Zhang%2520and%2520Rayan%2520Saab%26entry.1292438233%3D%2520%2520Several%2520recent%2520studies%2520have%2520investigated%2520low-precision%2520accumulation%252C%250Areporting%2520improvements%2520in%2520throughput%252C%2520power%252C%2520and%2520area%2520across%2520various%2520platforms.%250AHowever%252C%2520the%2520accompanying%2520proposals%2520have%2520only%2520considered%2520the%2520quantization-aware%250Atraining%2520%2528QAT%2529%2520paradigm%252C%2520in%2520which%2520models%2520are%2520fine-tuned%2520or%2520trained%2520from%2520scratch%250Awith%2520quantization%2520in%2520the%2520loop.%2520As%2520models%2520continue%2520to%2520grow%2520in%2520size%252C%2520QAT%250Atechniques%2520become%2520increasingly%2520more%2520expensive%252C%2520which%2520has%2520motivated%2520the%2520recent%250Asurge%2520in%2520post-training%2520quantization%2520%2528PTQ%2529%2520research.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520ours%2520marks%2520the%2520first%2520formal%2520study%2520of%2520accumulator-aware%2520quantization%250Ain%2520the%2520PTQ%2520setting.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520AXE%252C%2520a%2520practical%2520framework%250Aof%2520accumulator-aware%2520extensions%2520designed%2520to%2520endow%2520overflow%2520avoidance%2520guarantees%250Ato%2520existing%2520layer-wise%2520PTQ%2520algorithms.%2520We%2520theoretically%2520motivate%2520AXE%2520and%250Ademonstrate%2520its%2520flexibility%2520by%2520implementing%2520it%2520on%2520top%2520of%2520two%2520state-of-the-art%250APTQ%2520algorithms%253A%2520GPFQ%2520and%2520OPTQ.%2520We%2520further%2520generalize%2520AXE%2520to%2520support%2520multi-stage%250Aaccumulation%2520for%2520the%2520first%2520time%252C%2520opening%2520the%2520door%2520for%2520full%2520datapath%250Aoptimization%2520and%2520scaling%2520to%2520large%2520language%2520models%2520%2528LLMs%2529.%2520We%2520evaluate%2520AXE%250Aacross%2520image%2520classification%2520and%2520language%2520generation%2520models%252C%2520and%2520observe%250Asignificant%2520improvements%2520in%2520the%2520trade-off%2520between%2520accumulator%2520bit%2520width%2520and%250Amodel%2520accuracy%2520over%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accumulator-Aware%20Post-Training%20Quantization&entry.906535625=Ian%20Colbert%20and%20Fabian%20Grob%20and%20Giuseppe%20Franco%20and%20Jinjie%20Zhang%20and%20Rayan%20Saab&entry.1292438233=%20%20Several%20recent%20studies%20have%20investigated%20low-precision%20accumulation%2C%0Areporting%20improvements%20in%20throughput%2C%20power%2C%20and%20area%20across%20various%20platforms.%0AHowever%2C%20the%20accompanying%20proposals%20have%20only%20considered%20the%20quantization-aware%0Atraining%20%28QAT%29%20paradigm%2C%20in%20which%20models%20are%20fine-tuned%20or%20trained%20from%20scratch%0Awith%20quantization%20in%20the%20loop.%20As%20models%20continue%20to%20grow%20in%20size%2C%20QAT%0Atechniques%20become%20increasingly%20more%20expensive%2C%20which%20has%20motivated%20the%20recent%0Asurge%20in%20post-training%20quantization%20%28PTQ%29%20research.%20To%20the%20best%20of%20our%0Aknowledge%2C%20ours%20marks%20the%20first%20formal%20study%20of%20accumulator-aware%20quantization%0Ain%20the%20PTQ%20setting.%20To%20bridge%20this%20gap%2C%20we%20introduce%20AXE%2C%20a%20practical%20framework%0Aof%20accumulator-aware%20extensions%20designed%20to%20endow%20overflow%20avoidance%20guarantees%0Ato%20existing%20layer-wise%20PTQ%20algorithms.%20We%20theoretically%20motivate%20AXE%20and%0Ademonstrate%20its%20flexibility%20by%20implementing%20it%20on%20top%20of%20two%20state-of-the-art%0APTQ%20algorithms%3A%20GPFQ%20and%20OPTQ.%20We%20further%20generalize%20AXE%20to%20support%20multi-stage%0Aaccumulation%20for%20the%20first%20time%2C%20opening%20the%20door%20for%20full%20datapath%0Aoptimization%20and%20scaling%20to%20large%20language%20models%20%28LLMs%29.%20We%20evaluate%20AXE%0Aacross%20image%20classification%20and%20language%20generation%20models%2C%20and%20observe%0Asignificant%20improvements%20in%20the%20trade-off%20between%20accumulator%20bit%20width%20and%0Amodel%20accuracy%20over%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17092v1&entry.124074799=Read"},
{"title": "Automated Surgical Skill Assessment in Endoscopic Pituitary Surgery\n  using Real-time Instrument Tracking on a High-fidelity Bench-top Phantom", "author": "Adrito Das and Bilal Sidiqi and Laurent Mennillo and Zhehua Mao and Mikael Brudfors and Miguel Xochicale and Danyal Z. Khan and Nicola Newall and John G. Hanrahan and Matthew J. Clarkson and Danail Stoyanov and Hani J. Marcus and Sophia Bano", "abstract": "  Improved surgical skill is generally associated with improved patient\noutcomes, although assessment is subjective; labour-intensive; and requires\ndomain specific expertise. Automated data driven metrics can alleviate these\ndifficulties, as demonstrated by existing machine learning instrument tracking\nmodels in minimally invasive surgery. However, these models have been tested on\nlimited datasets of laparoscopic surgery, with a focus on isolated tasks and\nrobotic surgery. In this paper, a new public dataset is introduced, focusing on\nsimulated surgery, using the nasal phase of endoscopic pituitary surgery as an\nexemplar. Simulated surgery allows for a realistic yet repeatable environment,\nmeaning the insights gained from automated assessment can be used by novice\nsurgeons to hone their skills on the simulator before moving to real surgery.\nPRINTNet (Pituitary Real-time INstrument Tracking Network) has been created as\na baseline model for this automated assessment. Consisting of DeepLabV3 for\nclassification and segmentation; StrongSORT for tracking; and the NVIDIA\nHoloscan SDK for real-time performance, PRINTNet achieved 71.9% Multiple Object\nTracking Precision running at 22 Frames Per Second. Using this tracking output,\na Multilayer Perceptron achieved 87% accuracy in predicting surgical skill\nlevel (novice or expert), with the \"ratio of total procedure time to instrument\nvisible time\" correlated with higher surgical skill. This therefore\ndemonstrates the feasibility of automated surgical skill assessment in\nsimulated endoscopic pituitary surgery. The new publicly available dataset can\nbe found here: https://doi.org/10.5522/04/26511049.\n", "link": "http://arxiv.org/abs/2409.17025v1", "date": "2024-09-25", "relevancy": 1.9845, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5012}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4935}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.49}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Surgical%20Skill%20Assessment%20in%20Endoscopic%20Pituitary%20Surgery%0A%20%20using%20Real-time%20Instrument%20Tracking%20on%20a%20High-fidelity%20Bench-top%20Phantom&body=Title%3A%20Automated%20Surgical%20Skill%20Assessment%20in%20Endoscopic%20Pituitary%20Surgery%0A%20%20using%20Real-time%20Instrument%20Tracking%20on%20a%20High-fidelity%20Bench-top%20Phantom%0AAuthor%3A%20Adrito%20Das%20and%20Bilal%20Sidiqi%20and%20Laurent%20Mennillo%20and%20Zhehua%20Mao%20and%20Mikael%20Brudfors%20and%20Miguel%20Xochicale%20and%20Danyal%20Z.%20Khan%20and%20Nicola%20Newall%20and%20John%20G.%20Hanrahan%20and%20Matthew%20J.%20Clarkson%20and%20Danail%20Stoyanov%20and%20Hani%20J.%20Marcus%20and%20Sophia%20Bano%0AAbstract%3A%20%20%20Improved%20surgical%20skill%20is%20generally%20associated%20with%20improved%20patient%0Aoutcomes%2C%20although%20assessment%20is%20subjective%3B%20labour-intensive%3B%20and%20requires%0Adomain%20specific%20expertise.%20Automated%20data%20driven%20metrics%20can%20alleviate%20these%0Adifficulties%2C%20as%20demonstrated%20by%20existing%20machine%20learning%20instrument%20tracking%0Amodels%20in%20minimally%20invasive%20surgery.%20However%2C%20these%20models%20have%20been%20tested%20on%0Alimited%20datasets%20of%20laparoscopic%20surgery%2C%20with%20a%20focus%20on%20isolated%20tasks%20and%0Arobotic%20surgery.%20In%20this%20paper%2C%20a%20new%20public%20dataset%20is%20introduced%2C%20focusing%20on%0Asimulated%20surgery%2C%20using%20the%20nasal%20phase%20of%20endoscopic%20pituitary%20surgery%20as%20an%0Aexemplar.%20Simulated%20surgery%20allows%20for%20a%20realistic%20yet%20repeatable%20environment%2C%0Ameaning%20the%20insights%20gained%20from%20automated%20assessment%20can%20be%20used%20by%20novice%0Asurgeons%20to%20hone%20their%20skills%20on%20the%20simulator%20before%20moving%20to%20real%20surgery.%0APRINTNet%20%28Pituitary%20Real-time%20INstrument%20Tracking%20Network%29%20has%20been%20created%20as%0Aa%20baseline%20model%20for%20this%20automated%20assessment.%20Consisting%20of%20DeepLabV3%20for%0Aclassification%20and%20segmentation%3B%20StrongSORT%20for%20tracking%3B%20and%20the%20NVIDIA%0AHoloscan%20SDK%20for%20real-time%20performance%2C%20PRINTNet%20achieved%2071.9%25%20Multiple%20Object%0ATracking%20Precision%20running%20at%2022%20Frames%20Per%20Second.%20Using%20this%20tracking%20output%2C%0Aa%20Multilayer%20Perceptron%20achieved%2087%25%20accuracy%20in%20predicting%20surgical%20skill%0Alevel%20%28novice%20or%20expert%29%2C%20with%20the%20%22ratio%20of%20total%20procedure%20time%20to%20instrument%0Avisible%20time%22%20correlated%20with%20higher%20surgical%20skill.%20This%20therefore%0Ademonstrates%20the%20feasibility%20of%20automated%20surgical%20skill%20assessment%20in%0Asimulated%20endoscopic%20pituitary%20surgery.%20The%20new%20publicly%20available%20dataset%20can%0Abe%20found%20here%3A%20https%3A//doi.org/10.5522/04/26511049.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17025v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Surgical%2520Skill%2520Assessment%2520in%2520Endoscopic%2520Pituitary%2520Surgery%250A%2520%2520using%2520Real-time%2520Instrument%2520Tracking%2520on%2520a%2520High-fidelity%2520Bench-top%2520Phantom%26entry.906535625%3DAdrito%2520Das%2520and%2520Bilal%2520Sidiqi%2520and%2520Laurent%2520Mennillo%2520and%2520Zhehua%2520Mao%2520and%2520Mikael%2520Brudfors%2520and%2520Miguel%2520Xochicale%2520and%2520Danyal%2520Z.%2520Khan%2520and%2520Nicola%2520Newall%2520and%2520John%2520G.%2520Hanrahan%2520and%2520Matthew%2520J.%2520Clarkson%2520and%2520Danail%2520Stoyanov%2520and%2520Hani%2520J.%2520Marcus%2520and%2520Sophia%2520Bano%26entry.1292438233%3D%2520%2520Improved%2520surgical%2520skill%2520is%2520generally%2520associated%2520with%2520improved%2520patient%250Aoutcomes%252C%2520although%2520assessment%2520is%2520subjective%253B%2520labour-intensive%253B%2520and%2520requires%250Adomain%2520specific%2520expertise.%2520Automated%2520data%2520driven%2520metrics%2520can%2520alleviate%2520these%250Adifficulties%252C%2520as%2520demonstrated%2520by%2520existing%2520machine%2520learning%2520instrument%2520tracking%250Amodels%2520in%2520minimally%2520invasive%2520surgery.%2520However%252C%2520these%2520models%2520have%2520been%2520tested%2520on%250Alimited%2520datasets%2520of%2520laparoscopic%2520surgery%252C%2520with%2520a%2520focus%2520on%2520isolated%2520tasks%2520and%250Arobotic%2520surgery.%2520In%2520this%2520paper%252C%2520a%2520new%2520public%2520dataset%2520is%2520introduced%252C%2520focusing%2520on%250Asimulated%2520surgery%252C%2520using%2520the%2520nasal%2520phase%2520of%2520endoscopic%2520pituitary%2520surgery%2520as%2520an%250Aexemplar.%2520Simulated%2520surgery%2520allows%2520for%2520a%2520realistic%2520yet%2520repeatable%2520environment%252C%250Ameaning%2520the%2520insights%2520gained%2520from%2520automated%2520assessment%2520can%2520be%2520used%2520by%2520novice%250Asurgeons%2520to%2520hone%2520their%2520skills%2520on%2520the%2520simulator%2520before%2520moving%2520to%2520real%2520surgery.%250APRINTNet%2520%2528Pituitary%2520Real-time%2520INstrument%2520Tracking%2520Network%2529%2520has%2520been%2520created%2520as%250Aa%2520baseline%2520model%2520for%2520this%2520automated%2520assessment.%2520Consisting%2520of%2520DeepLabV3%2520for%250Aclassification%2520and%2520segmentation%253B%2520StrongSORT%2520for%2520tracking%253B%2520and%2520the%2520NVIDIA%250AHoloscan%2520SDK%2520for%2520real-time%2520performance%252C%2520PRINTNet%2520achieved%252071.9%2525%2520Multiple%2520Object%250ATracking%2520Precision%2520running%2520at%252022%2520Frames%2520Per%2520Second.%2520Using%2520this%2520tracking%2520output%252C%250Aa%2520Multilayer%2520Perceptron%2520achieved%252087%2525%2520accuracy%2520in%2520predicting%2520surgical%2520skill%250Alevel%2520%2528novice%2520or%2520expert%2529%252C%2520with%2520the%2520%2522ratio%2520of%2520total%2520procedure%2520time%2520to%2520instrument%250Avisible%2520time%2522%2520correlated%2520with%2520higher%2520surgical%2520skill.%2520This%2520therefore%250Ademonstrates%2520the%2520feasibility%2520of%2520automated%2520surgical%2520skill%2520assessment%2520in%250Asimulated%2520endoscopic%2520pituitary%2520surgery.%2520The%2520new%2520publicly%2520available%2520dataset%2520can%250Abe%2520found%2520here%253A%2520https%253A//doi.org/10.5522/04/26511049.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17025v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Surgical%20Skill%20Assessment%20in%20Endoscopic%20Pituitary%20Surgery%0A%20%20using%20Real-time%20Instrument%20Tracking%20on%20a%20High-fidelity%20Bench-top%20Phantom&entry.906535625=Adrito%20Das%20and%20Bilal%20Sidiqi%20and%20Laurent%20Mennillo%20and%20Zhehua%20Mao%20and%20Mikael%20Brudfors%20and%20Miguel%20Xochicale%20and%20Danyal%20Z.%20Khan%20and%20Nicola%20Newall%20and%20John%20G.%20Hanrahan%20and%20Matthew%20J.%20Clarkson%20and%20Danail%20Stoyanov%20and%20Hani%20J.%20Marcus%20and%20Sophia%20Bano&entry.1292438233=%20%20Improved%20surgical%20skill%20is%20generally%20associated%20with%20improved%20patient%0Aoutcomes%2C%20although%20assessment%20is%20subjective%3B%20labour-intensive%3B%20and%20requires%0Adomain%20specific%20expertise.%20Automated%20data%20driven%20metrics%20can%20alleviate%20these%0Adifficulties%2C%20as%20demonstrated%20by%20existing%20machine%20learning%20instrument%20tracking%0Amodels%20in%20minimally%20invasive%20surgery.%20However%2C%20these%20models%20have%20been%20tested%20on%0Alimited%20datasets%20of%20laparoscopic%20surgery%2C%20with%20a%20focus%20on%20isolated%20tasks%20and%0Arobotic%20surgery.%20In%20this%20paper%2C%20a%20new%20public%20dataset%20is%20introduced%2C%20focusing%20on%0Asimulated%20surgery%2C%20using%20the%20nasal%20phase%20of%20endoscopic%20pituitary%20surgery%20as%20an%0Aexemplar.%20Simulated%20surgery%20allows%20for%20a%20realistic%20yet%20repeatable%20environment%2C%0Ameaning%20the%20insights%20gained%20from%20automated%20assessment%20can%20be%20used%20by%20novice%0Asurgeons%20to%20hone%20their%20skills%20on%20the%20simulator%20before%20moving%20to%20real%20surgery.%0APRINTNet%20%28Pituitary%20Real-time%20INstrument%20Tracking%20Network%29%20has%20been%20created%20as%0Aa%20baseline%20model%20for%20this%20automated%20assessment.%20Consisting%20of%20DeepLabV3%20for%0Aclassification%20and%20segmentation%3B%20StrongSORT%20for%20tracking%3B%20and%20the%20NVIDIA%0AHoloscan%20SDK%20for%20real-time%20performance%2C%20PRINTNet%20achieved%2071.9%25%20Multiple%20Object%0ATracking%20Precision%20running%20at%2022%20Frames%20Per%20Second.%20Using%20this%20tracking%20output%2C%0Aa%20Multilayer%20Perceptron%20achieved%2087%25%20accuracy%20in%20predicting%20surgical%20skill%0Alevel%20%28novice%20or%20expert%29%2C%20with%20the%20%22ratio%20of%20total%20procedure%20time%20to%20instrument%0Avisible%20time%22%20correlated%20with%20higher%20surgical%20skill.%20This%20therefore%0Ademonstrates%20the%20feasibility%20of%20automated%20surgical%20skill%20assessment%20in%0Asimulated%20endoscopic%20pituitary%20surgery.%20The%20new%20publicly%20available%20dataset%20can%0Abe%20found%20here%3A%20https%3A//doi.org/10.5522/04/26511049.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17025v1&entry.124074799=Read"},
{"title": "Learning with Dynamics: Autonomous Regulation of UAV Based Communication\n  Networks with Dynamic UAV Crew", "author": "Ran Zhang and Bowei Li and Liyuan Zhang and  Jiang and  Xie and Miao Wang", "abstract": "  Unmanned Aerial Vehicle (UAV) based communication networks (UCNs) are a key\ncomponent in future mobile networking. To handle the dynamic environments in\nUCNs, reinforcement learning (RL) has been a promising solution attributed to\nits strong capability of adaptive decision-making free of the environment\nmodels. However, most existing RL-based research focus on control strategy\ndesign assuming a fixed set of UAVs. Few works have investigated how UCNs\nshould be adaptively regulated when the serving UAVs change dynamically. This\narticle discusses RL-based strategy design for adaptive UCN regulation given a\ndynamic UAV set, addressing both reactive strategies in general UCNs and\nproactive strategies in solar-powered UCNs. An overview of the UCN and the RL\nframework is first provided. Potential research directions with key challenges\nand possible solutions are then elaborated. Some of our recent works are\npresented as case studies to inspire innovative ways to handle dynamic UAV crew\nwith different RL algorithms.\n", "link": "http://arxiv.org/abs/2409.17139v1", "date": "2024-09-25", "relevancy": 1.9791, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5015}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4974}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20with%20Dynamics%3A%20Autonomous%20Regulation%20of%20UAV%20Based%20Communication%0A%20%20Networks%20with%20Dynamic%20UAV%20Crew&body=Title%3A%20Learning%20with%20Dynamics%3A%20Autonomous%20Regulation%20of%20UAV%20Based%20Communication%0A%20%20Networks%20with%20Dynamic%20UAV%20Crew%0AAuthor%3A%20Ran%20Zhang%20and%20Bowei%20Li%20and%20Liyuan%20Zhang%20and%20%20Jiang%20and%20%20Xie%20and%20Miao%20Wang%0AAbstract%3A%20%20%20Unmanned%20Aerial%20Vehicle%20%28UAV%29%20based%20communication%20networks%20%28UCNs%29%20are%20a%20key%0Acomponent%20in%20future%20mobile%20networking.%20To%20handle%20the%20dynamic%20environments%20in%0AUCNs%2C%20reinforcement%20learning%20%28RL%29%20has%20been%20a%20promising%20solution%20attributed%20to%0Aits%20strong%20capability%20of%20adaptive%20decision-making%20free%20of%20the%20environment%0Amodels.%20However%2C%20most%20existing%20RL-based%20research%20focus%20on%20control%20strategy%0Adesign%20assuming%20a%20fixed%20set%20of%20UAVs.%20Few%20works%20have%20investigated%20how%20UCNs%0Ashould%20be%20adaptively%20regulated%20when%20the%20serving%20UAVs%20change%20dynamically.%20This%0Aarticle%20discusses%20RL-based%20strategy%20design%20for%20adaptive%20UCN%20regulation%20given%20a%0Adynamic%20UAV%20set%2C%20addressing%20both%20reactive%20strategies%20in%20general%20UCNs%20and%0Aproactive%20strategies%20in%20solar-powered%20UCNs.%20An%20overview%20of%20the%20UCN%20and%20the%20RL%0Aframework%20is%20first%20provided.%20Potential%20research%20directions%20with%20key%20challenges%0Aand%20possible%20solutions%20are%20then%20elaborated.%20Some%20of%20our%20recent%20works%20are%0Apresented%20as%20case%20studies%20to%20inspire%20innovative%20ways%20to%20handle%20dynamic%20UAV%20crew%0Awith%20different%20RL%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520with%2520Dynamics%253A%2520Autonomous%2520Regulation%2520of%2520UAV%2520Based%2520Communication%250A%2520%2520Networks%2520with%2520Dynamic%2520UAV%2520Crew%26entry.906535625%3DRan%2520Zhang%2520and%2520Bowei%2520Li%2520and%2520Liyuan%2520Zhang%2520and%2520%2520Jiang%2520and%2520%2520Xie%2520and%2520Miao%2520Wang%26entry.1292438233%3D%2520%2520Unmanned%2520Aerial%2520Vehicle%2520%2528UAV%2529%2520based%2520communication%2520networks%2520%2528UCNs%2529%2520are%2520a%2520key%250Acomponent%2520in%2520future%2520mobile%2520networking.%2520To%2520handle%2520the%2520dynamic%2520environments%2520in%250AUCNs%252C%2520reinforcement%2520learning%2520%2528RL%2529%2520has%2520been%2520a%2520promising%2520solution%2520attributed%2520to%250Aits%2520strong%2520capability%2520of%2520adaptive%2520decision-making%2520free%2520of%2520the%2520environment%250Amodels.%2520However%252C%2520most%2520existing%2520RL-based%2520research%2520focus%2520on%2520control%2520strategy%250Adesign%2520assuming%2520a%2520fixed%2520set%2520of%2520UAVs.%2520Few%2520works%2520have%2520investigated%2520how%2520UCNs%250Ashould%2520be%2520adaptively%2520regulated%2520when%2520the%2520serving%2520UAVs%2520change%2520dynamically.%2520This%250Aarticle%2520discusses%2520RL-based%2520strategy%2520design%2520for%2520adaptive%2520UCN%2520regulation%2520given%2520a%250Adynamic%2520UAV%2520set%252C%2520addressing%2520both%2520reactive%2520strategies%2520in%2520general%2520UCNs%2520and%250Aproactive%2520strategies%2520in%2520solar-powered%2520UCNs.%2520An%2520overview%2520of%2520the%2520UCN%2520and%2520the%2520RL%250Aframework%2520is%2520first%2520provided.%2520Potential%2520research%2520directions%2520with%2520key%2520challenges%250Aand%2520possible%2520solutions%2520are%2520then%2520elaborated.%2520Some%2520of%2520our%2520recent%2520works%2520are%250Apresented%2520as%2520case%2520studies%2520to%2520inspire%2520innovative%2520ways%2520to%2520handle%2520dynamic%2520UAV%2520crew%250Awith%2520different%2520RL%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20with%20Dynamics%3A%20Autonomous%20Regulation%20of%20UAV%20Based%20Communication%0A%20%20Networks%20with%20Dynamic%20UAV%20Crew&entry.906535625=Ran%20Zhang%20and%20Bowei%20Li%20and%20Liyuan%20Zhang%20and%20%20Jiang%20and%20%20Xie%20and%20Miao%20Wang&entry.1292438233=%20%20Unmanned%20Aerial%20Vehicle%20%28UAV%29%20based%20communication%20networks%20%28UCNs%29%20are%20a%20key%0Acomponent%20in%20future%20mobile%20networking.%20To%20handle%20the%20dynamic%20environments%20in%0AUCNs%2C%20reinforcement%20learning%20%28RL%29%20has%20been%20a%20promising%20solution%20attributed%20to%0Aits%20strong%20capability%20of%20adaptive%20decision-making%20free%20of%20the%20environment%0Amodels.%20However%2C%20most%20existing%20RL-based%20research%20focus%20on%20control%20strategy%0Adesign%20assuming%20a%20fixed%20set%20of%20UAVs.%20Few%20works%20have%20investigated%20how%20UCNs%0Ashould%20be%20adaptively%20regulated%20when%20the%20serving%20UAVs%20change%20dynamically.%20This%0Aarticle%20discusses%20RL-based%20strategy%20design%20for%20adaptive%20UCN%20regulation%20given%20a%0Adynamic%20UAV%20set%2C%20addressing%20both%20reactive%20strategies%20in%20general%20UCNs%20and%0Aproactive%20strategies%20in%20solar-powered%20UCNs.%20An%20overview%20of%20the%20UCN%20and%20the%20RL%0Aframework%20is%20first%20provided.%20Potential%20research%20directions%20with%20key%20challenges%0Aand%20possible%20solutions%20are%20then%20elaborated.%20Some%20of%20our%20recent%20works%20are%0Apresented%20as%20case%20studies%20to%20inspire%20innovative%20ways%20to%20handle%20dynamic%20UAV%20crew%0Awith%20different%20RL%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17139v1&entry.124074799=Read"},
{"title": "Scalable Learning of Segment-Level Traffic Congestion Functions", "author": "Shushman Choudhury and Abdul Rahman Kreidieh and Iveel Tsogsuren and Neha Arora and Carolina Osorio and Alexandre Bayen", "abstract": "  We propose and study a data-driven framework for identifying traffic\ncongestion functions (numerical relationships between observations of traffic\nvariables) at global scale and segment-level granularity. In contrast to\nmethods that estimate a separate set of parameters for each roadway, ours\nlearns a single black-box function over all roadways in a metropolitan area.\nFirst, we pool traffic data from all segments into one dataset, combining\nstatic attributes with dynamic time-dependent features. Second, we train a\nfeed-forward neural network on this dataset, which we can then use on any\nsegment in the area. We evaluate how well our framework identifies congestion\nfunctions on observed segments and how it generalizes to unobserved segments\nand predicts segment attributes on a large dataset covering multiple cities\nworldwide. For identification error on observed segments, our single\ndata-driven congestion function compares favorably to segment-specific\nmodel-based functions on highway roads, but has room to improve on arterial\nroads. For generalization, our approach shows strong performance across cities\nand road types: both on unobserved segments in the same city and on zero-shot\ntransfer learning between cities. Finally, for predicting segment attributes,\nwe find that our approach can approximate critical densities for individual\nsegments using their static properties.\n", "link": "http://arxiv.org/abs/2405.06080v2", "date": "2024-09-25", "relevancy": 1.971, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5166}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.494}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20Learning%20of%20Segment-Level%20Traffic%20Congestion%20Functions&body=Title%3A%20Scalable%20Learning%20of%20Segment-Level%20Traffic%20Congestion%20Functions%0AAuthor%3A%20Shushman%20Choudhury%20and%20Abdul%20Rahman%20Kreidieh%20and%20Iveel%20Tsogsuren%20and%20Neha%20Arora%20and%20Carolina%20Osorio%20and%20Alexandre%20Bayen%0AAbstract%3A%20%20%20We%20propose%20and%20study%20a%20data-driven%20framework%20for%20identifying%20traffic%0Acongestion%20functions%20%28numerical%20relationships%20between%20observations%20of%20traffic%0Avariables%29%20at%20global%20scale%20and%20segment-level%20granularity.%20In%20contrast%20to%0Amethods%20that%20estimate%20a%20separate%20set%20of%20parameters%20for%20each%20roadway%2C%20ours%0Alearns%20a%20single%20black-box%20function%20over%20all%20roadways%20in%20a%20metropolitan%20area.%0AFirst%2C%20we%20pool%20traffic%20data%20from%20all%20segments%20into%20one%20dataset%2C%20combining%0Astatic%20attributes%20with%20dynamic%20time-dependent%20features.%20Second%2C%20we%20train%20a%0Afeed-forward%20neural%20network%20on%20this%20dataset%2C%20which%20we%20can%20then%20use%20on%20any%0Asegment%20in%20the%20area.%20We%20evaluate%20how%20well%20our%20framework%20identifies%20congestion%0Afunctions%20on%20observed%20segments%20and%20how%20it%20generalizes%20to%20unobserved%20segments%0Aand%20predicts%20segment%20attributes%20on%20a%20large%20dataset%20covering%20multiple%20cities%0Aworldwide.%20For%20identification%20error%20on%20observed%20segments%2C%20our%20single%0Adata-driven%20congestion%20function%20compares%20favorably%20to%20segment-specific%0Amodel-based%20functions%20on%20highway%20roads%2C%20but%20has%20room%20to%20improve%20on%20arterial%0Aroads.%20For%20generalization%2C%20our%20approach%20shows%20strong%20performance%20across%20cities%0Aand%20road%20types%3A%20both%20on%20unobserved%20segments%20in%20the%20same%20city%20and%20on%20zero-shot%0Atransfer%20learning%20between%20cities.%20Finally%2C%20for%20predicting%20segment%20attributes%2C%0Awe%20find%20that%20our%20approach%20can%20approximate%20critical%20densities%20for%20individual%0Asegments%20using%20their%20static%20properties.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.06080v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520Learning%2520of%2520Segment-Level%2520Traffic%2520Congestion%2520Functions%26entry.906535625%3DShushman%2520Choudhury%2520and%2520Abdul%2520Rahman%2520Kreidieh%2520and%2520Iveel%2520Tsogsuren%2520and%2520Neha%2520Arora%2520and%2520Carolina%2520Osorio%2520and%2520Alexandre%2520Bayen%26entry.1292438233%3D%2520%2520We%2520propose%2520and%2520study%2520a%2520data-driven%2520framework%2520for%2520identifying%2520traffic%250Acongestion%2520functions%2520%2528numerical%2520relationships%2520between%2520observations%2520of%2520traffic%250Avariables%2529%2520at%2520global%2520scale%2520and%2520segment-level%2520granularity.%2520In%2520contrast%2520to%250Amethods%2520that%2520estimate%2520a%2520separate%2520set%2520of%2520parameters%2520for%2520each%2520roadway%252C%2520ours%250Alearns%2520a%2520single%2520black-box%2520function%2520over%2520all%2520roadways%2520in%2520a%2520metropolitan%2520area.%250AFirst%252C%2520we%2520pool%2520traffic%2520data%2520from%2520all%2520segments%2520into%2520one%2520dataset%252C%2520combining%250Astatic%2520attributes%2520with%2520dynamic%2520time-dependent%2520features.%2520Second%252C%2520we%2520train%2520a%250Afeed-forward%2520neural%2520network%2520on%2520this%2520dataset%252C%2520which%2520we%2520can%2520then%2520use%2520on%2520any%250Asegment%2520in%2520the%2520area.%2520We%2520evaluate%2520how%2520well%2520our%2520framework%2520identifies%2520congestion%250Afunctions%2520on%2520observed%2520segments%2520and%2520how%2520it%2520generalizes%2520to%2520unobserved%2520segments%250Aand%2520predicts%2520segment%2520attributes%2520on%2520a%2520large%2520dataset%2520covering%2520multiple%2520cities%250Aworldwide.%2520For%2520identification%2520error%2520on%2520observed%2520segments%252C%2520our%2520single%250Adata-driven%2520congestion%2520function%2520compares%2520favorably%2520to%2520segment-specific%250Amodel-based%2520functions%2520on%2520highway%2520roads%252C%2520but%2520has%2520room%2520to%2520improve%2520on%2520arterial%250Aroads.%2520For%2520generalization%252C%2520our%2520approach%2520shows%2520strong%2520performance%2520across%2520cities%250Aand%2520road%2520types%253A%2520both%2520on%2520unobserved%2520segments%2520in%2520the%2520same%2520city%2520and%2520on%2520zero-shot%250Atransfer%2520learning%2520between%2520cities.%2520Finally%252C%2520for%2520predicting%2520segment%2520attributes%252C%250Awe%2520find%2520that%2520our%2520approach%2520can%2520approximate%2520critical%2520densities%2520for%2520individual%250Asegments%2520using%2520their%2520static%2520properties.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.06080v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20Learning%20of%20Segment-Level%20Traffic%20Congestion%20Functions&entry.906535625=Shushman%20Choudhury%20and%20Abdul%20Rahman%20Kreidieh%20and%20Iveel%20Tsogsuren%20and%20Neha%20Arora%20and%20Carolina%20Osorio%20and%20Alexandre%20Bayen&entry.1292438233=%20%20We%20propose%20and%20study%20a%20data-driven%20framework%20for%20identifying%20traffic%0Acongestion%20functions%20%28numerical%20relationships%20between%20observations%20of%20traffic%0Avariables%29%20at%20global%20scale%20and%20segment-level%20granularity.%20In%20contrast%20to%0Amethods%20that%20estimate%20a%20separate%20set%20of%20parameters%20for%20each%20roadway%2C%20ours%0Alearns%20a%20single%20black-box%20function%20over%20all%20roadways%20in%20a%20metropolitan%20area.%0AFirst%2C%20we%20pool%20traffic%20data%20from%20all%20segments%20into%20one%20dataset%2C%20combining%0Astatic%20attributes%20with%20dynamic%20time-dependent%20features.%20Second%2C%20we%20train%20a%0Afeed-forward%20neural%20network%20on%20this%20dataset%2C%20which%20we%20can%20then%20use%20on%20any%0Asegment%20in%20the%20area.%20We%20evaluate%20how%20well%20our%20framework%20identifies%20congestion%0Afunctions%20on%20observed%20segments%20and%20how%20it%20generalizes%20to%20unobserved%20segments%0Aand%20predicts%20segment%20attributes%20on%20a%20large%20dataset%20covering%20multiple%20cities%0Aworldwide.%20For%20identification%20error%20on%20observed%20segments%2C%20our%20single%0Adata-driven%20congestion%20function%20compares%20favorably%20to%20segment-specific%0Amodel-based%20functions%20on%20highway%20roads%2C%20but%20has%20room%20to%20improve%20on%20arterial%0Aroads.%20For%20generalization%2C%20our%20approach%20shows%20strong%20performance%20across%20cities%0Aand%20road%20types%3A%20both%20on%20unobserved%20segments%20in%20the%20same%20city%20and%20on%20zero-shot%0Atransfer%20learning%20between%20cities.%20Finally%2C%20for%20predicting%20segment%20attributes%2C%0Awe%20find%20that%20our%20approach%20can%20approximate%20critical%20densities%20for%20individual%0Asegments%20using%20their%20static%20properties.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.06080v2&entry.124074799=Read"},
{"title": "Cross-lingual Speech Emotion Recognition: Humans vs. Self-Supervised\n  Models", "author": "Zhichen Han and Tianqi Geng and Hui Feng and Jiahong Yuan and Korin Richmond and Yuanchao Li", "abstract": "  Utilizing Self-Supervised Learning (SSL) models for Speech Emotion\nRecognition (SER) has proven effective, yet limited research has explored\ncross-lingual scenarios. This study presents a comparative analysis between\nhuman performance and SSL models, beginning with a layer-wise analysis and an\nexploration of parameter-efficient fine-tuning strategies in monolingual,\ncross-lingual, and transfer learning contexts. We further compare the SER\nability of models and humans at both utterance- and segment-levels.\nAdditionally, we investigate the impact of dialect on cross-lingual SER through\nhuman evaluation. Our findings reveal that models, with appropriate knowledge\ntransfer, can adapt to the target language and achieve performance comparable\nto native speakers. We also demonstrate the significant effect of dialect on\nSER for individuals without prior linguistic and paralinguistic background.\nMoreover, both humans and models exhibit distinct behaviors across different\nemotions. These results offer new insights into the cross-lingual SER\ncapabilities of SSL models, underscoring both their similarities to and\ndifferences from human emotion perception.\n", "link": "http://arxiv.org/abs/2409.16920v1", "date": "2024-09-25", "relevancy": 1.9504, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4969}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-lingual%20Speech%20Emotion%20Recognition%3A%20Humans%20vs.%20Self-Supervised%0A%20%20Models&body=Title%3A%20Cross-lingual%20Speech%20Emotion%20Recognition%3A%20Humans%20vs.%20Self-Supervised%0A%20%20Models%0AAuthor%3A%20Zhichen%20Han%20and%20Tianqi%20Geng%20and%20Hui%20Feng%20and%20Jiahong%20Yuan%20and%20Korin%20Richmond%20and%20Yuanchao%20Li%0AAbstract%3A%20%20%20Utilizing%20Self-Supervised%20Learning%20%28SSL%29%20models%20for%20Speech%20Emotion%0ARecognition%20%28SER%29%20has%20proven%20effective%2C%20yet%20limited%20research%20has%20explored%0Across-lingual%20scenarios.%20This%20study%20presents%20a%20comparative%20analysis%20between%0Ahuman%20performance%20and%20SSL%20models%2C%20beginning%20with%20a%20layer-wise%20analysis%20and%20an%0Aexploration%20of%20parameter-efficient%20fine-tuning%20strategies%20in%20monolingual%2C%0Across-lingual%2C%20and%20transfer%20learning%20contexts.%20We%20further%20compare%20the%20SER%0Aability%20of%20models%20and%20humans%20at%20both%20utterance-%20and%20segment-levels.%0AAdditionally%2C%20we%20investigate%20the%20impact%20of%20dialect%20on%20cross-lingual%20SER%20through%0Ahuman%20evaluation.%20Our%20findings%20reveal%20that%20models%2C%20with%20appropriate%20knowledge%0Atransfer%2C%20can%20adapt%20to%20the%20target%20language%20and%20achieve%20performance%20comparable%0Ato%20native%20speakers.%20We%20also%20demonstrate%20the%20significant%20effect%20of%20dialect%20on%0ASER%20for%20individuals%20without%20prior%20linguistic%20and%20paralinguistic%20background.%0AMoreover%2C%20both%20humans%20and%20models%20exhibit%20distinct%20behaviors%20across%20different%0Aemotions.%20These%20results%20offer%20new%20insights%20into%20the%20cross-lingual%20SER%0Acapabilities%20of%20SSL%20models%2C%20underscoring%20both%20their%20similarities%20to%20and%0Adifferences%20from%20human%20emotion%20perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-lingual%2520Speech%2520Emotion%2520Recognition%253A%2520Humans%2520vs.%2520Self-Supervised%250A%2520%2520Models%26entry.906535625%3DZhichen%2520Han%2520and%2520Tianqi%2520Geng%2520and%2520Hui%2520Feng%2520and%2520Jiahong%2520Yuan%2520and%2520Korin%2520Richmond%2520and%2520Yuanchao%2520Li%26entry.1292438233%3D%2520%2520Utilizing%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520models%2520for%2520Speech%2520Emotion%250ARecognition%2520%2528SER%2529%2520has%2520proven%2520effective%252C%2520yet%2520limited%2520research%2520has%2520explored%250Across-lingual%2520scenarios.%2520This%2520study%2520presents%2520a%2520comparative%2520analysis%2520between%250Ahuman%2520performance%2520and%2520SSL%2520models%252C%2520beginning%2520with%2520a%2520layer-wise%2520analysis%2520and%2520an%250Aexploration%2520of%2520parameter-efficient%2520fine-tuning%2520strategies%2520in%2520monolingual%252C%250Across-lingual%252C%2520and%2520transfer%2520learning%2520contexts.%2520We%2520further%2520compare%2520the%2520SER%250Aability%2520of%2520models%2520and%2520humans%2520at%2520both%2520utterance-%2520and%2520segment-levels.%250AAdditionally%252C%2520we%2520investigate%2520the%2520impact%2520of%2520dialect%2520on%2520cross-lingual%2520SER%2520through%250Ahuman%2520evaluation.%2520Our%2520findings%2520reveal%2520that%2520models%252C%2520with%2520appropriate%2520knowledge%250Atransfer%252C%2520can%2520adapt%2520to%2520the%2520target%2520language%2520and%2520achieve%2520performance%2520comparable%250Ato%2520native%2520speakers.%2520We%2520also%2520demonstrate%2520the%2520significant%2520effect%2520of%2520dialect%2520on%250ASER%2520for%2520individuals%2520without%2520prior%2520linguistic%2520and%2520paralinguistic%2520background.%250AMoreover%252C%2520both%2520humans%2520and%2520models%2520exhibit%2520distinct%2520behaviors%2520across%2520different%250Aemotions.%2520These%2520results%2520offer%2520new%2520insights%2520into%2520the%2520cross-lingual%2520SER%250Acapabilities%2520of%2520SSL%2520models%252C%2520underscoring%2520both%2520their%2520similarities%2520to%2520and%250Adifferences%2520from%2520human%2520emotion%2520perception.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-lingual%20Speech%20Emotion%20Recognition%3A%20Humans%20vs.%20Self-Supervised%0A%20%20Models&entry.906535625=Zhichen%20Han%20and%20Tianqi%20Geng%20and%20Hui%20Feng%20and%20Jiahong%20Yuan%20and%20Korin%20Richmond%20and%20Yuanchao%20Li&entry.1292438233=%20%20Utilizing%20Self-Supervised%20Learning%20%28SSL%29%20models%20for%20Speech%20Emotion%0ARecognition%20%28SER%29%20has%20proven%20effective%2C%20yet%20limited%20research%20has%20explored%0Across-lingual%20scenarios.%20This%20study%20presents%20a%20comparative%20analysis%20between%0Ahuman%20performance%20and%20SSL%20models%2C%20beginning%20with%20a%20layer-wise%20analysis%20and%20an%0Aexploration%20of%20parameter-efficient%20fine-tuning%20strategies%20in%20monolingual%2C%0Across-lingual%2C%20and%20transfer%20learning%20contexts.%20We%20further%20compare%20the%20SER%0Aability%20of%20models%20and%20humans%20at%20both%20utterance-%20and%20segment-levels.%0AAdditionally%2C%20we%20investigate%20the%20impact%20of%20dialect%20on%20cross-lingual%20SER%20through%0Ahuman%20evaluation.%20Our%20findings%20reveal%20that%20models%2C%20with%20appropriate%20knowledge%0Atransfer%2C%20can%20adapt%20to%20the%20target%20language%20and%20achieve%20performance%20comparable%0Ato%20native%20speakers.%20We%20also%20demonstrate%20the%20significant%20effect%20of%20dialect%20on%0ASER%20for%20individuals%20without%20prior%20linguistic%20and%20paralinguistic%20background.%0AMoreover%2C%20both%20humans%20and%20models%20exhibit%20distinct%20behaviors%20across%20different%0Aemotions.%20These%20results%20offer%20new%20insights%20into%20the%20cross-lingual%20SER%0Acapabilities%20of%20SSL%20models%2C%20underscoring%20both%20their%20similarities%20to%20and%0Adifferences%20from%20human%20emotion%20perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16920v1&entry.124074799=Read"},
{"title": "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large\n  Language Models", "author": "Yifei Liu and Jicheng Wen and Yang Wang and Shengyu Ye and Li Lyna Zhang and Ting Cao and Cheng Li and Mao Yang", "abstract": "  Scaling model size significantly challenges the deployment and inference of\nLarge Language Models (LLMs). Due to the redundancy in LLM weights, recent\nresearch has focused on pushing weight-only quantization to extremely low-bit\n(even down to 2 bits). It reduces memory requirements, optimizes storage costs,\nand decreases memory bandwidth needs during inference. However, due to\nnumerical representation limitations, traditional scalar-based weight\nquantization struggles to achieve such extreme low-bit. Recent research on\nVector Quantization (VQ) for LLMs has demonstrated the potential for extremely\nlow-bit model quantization by compressing vectors into indices using lookup\ntables.\n  In this paper, we introduce Vector Post-Training Quantization (VPTQ) for\nextremely low-bit quantization of LLMs. We use Second-Order Optimization to\nformulate the LLM VQ problem and guide our quantization algorithm design by\nsolving the optimization. We further refine the weights using\nChannel-Independent Second-Order Optimization for a granular VQ. In addition,\nby decomposing the optimization problem, we propose a brief and effective\ncodebook initialization algorithm. We also extend VPTQ to support residual and\noutlier quantization, which enhances model accuracy and further compresses the\nmodel. Our experimental results show that VPTQ reduces model quantization\nperplexity by $0.01$-$0.34$ on LLaMA-2, $0.38$-$0.68$ on Mistral-7B,\n$4.41$-$7.34$ on LLaMA-3 over SOTA at 2-bit, with an average accuracy\nimprovement of $0.79$-$1.5\\%$ on LLaMA-2, $1\\%$ on Mistral-7B, $11$-$22\\%$ on\nLLaMA-3 on QA tasks on average. We only utilize $10.4$-$18.6\\%$ of the\nquantization algorithm execution time, resulting in a $1.6$-$1.8\\times$\nincrease in inference throughput compared to SOTA.\n", "link": "http://arxiv.org/abs/2409.17066v1", "date": "2024-09-25", "relevancy": 1.9408, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4972}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4926}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VPTQ%3A%20Extreme%20Low-bit%20Vector%20Post-Training%20Quantization%20for%20Large%0A%20%20Language%20Models&body=Title%3A%20VPTQ%3A%20Extreme%20Low-bit%20Vector%20Post-Training%20Quantization%20for%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Yifei%20Liu%20and%20Jicheng%20Wen%20and%20Yang%20Wang%20and%20Shengyu%20Ye%20and%20Li%20Lyna%20Zhang%20and%20Ting%20Cao%20and%20Cheng%20Li%20and%20Mao%20Yang%0AAbstract%3A%20%20%20Scaling%20model%20size%20significantly%20challenges%20the%20deployment%20and%20inference%20of%0ALarge%20Language%20Models%20%28LLMs%29.%20Due%20to%20the%20redundancy%20in%20LLM%20weights%2C%20recent%0Aresearch%20has%20focused%20on%20pushing%20weight-only%20quantization%20to%20extremely%20low-bit%0A%28even%20down%20to%202%20bits%29.%20It%20reduces%20memory%20requirements%2C%20optimizes%20storage%20costs%2C%0Aand%20decreases%20memory%20bandwidth%20needs%20during%20inference.%20However%2C%20due%20to%0Anumerical%20representation%20limitations%2C%20traditional%20scalar-based%20weight%0Aquantization%20struggles%20to%20achieve%20such%20extreme%20low-bit.%20Recent%20research%20on%0AVector%20Quantization%20%28VQ%29%20for%20LLMs%20has%20demonstrated%20the%20potential%20for%20extremely%0Alow-bit%20model%20quantization%20by%20compressing%20vectors%20into%20indices%20using%20lookup%0Atables.%0A%20%20In%20this%20paper%2C%20we%20introduce%20Vector%20Post-Training%20Quantization%20%28VPTQ%29%20for%0Aextremely%20low-bit%20quantization%20of%20LLMs.%20We%20use%20Second-Order%20Optimization%20to%0Aformulate%20the%20LLM%20VQ%20problem%20and%20guide%20our%20quantization%20algorithm%20design%20by%0Asolving%20the%20optimization.%20We%20further%20refine%20the%20weights%20using%0AChannel-Independent%20Second-Order%20Optimization%20for%20a%20granular%20VQ.%20In%20addition%2C%0Aby%20decomposing%20the%20optimization%20problem%2C%20we%20propose%20a%20brief%20and%20effective%0Acodebook%20initialization%20algorithm.%20We%20also%20extend%20VPTQ%20to%20support%20residual%20and%0Aoutlier%20quantization%2C%20which%20enhances%20model%20accuracy%20and%20further%20compresses%20the%0Amodel.%20Our%20experimental%20results%20show%20that%20VPTQ%20reduces%20model%20quantization%0Aperplexity%20by%20%240.01%24-%240.34%24%20on%20LLaMA-2%2C%20%240.38%24-%240.68%24%20on%20Mistral-7B%2C%0A%244.41%24-%247.34%24%20on%20LLaMA-3%20over%20SOTA%20at%202-bit%2C%20with%20an%20average%20accuracy%0Aimprovement%20of%20%240.79%24-%241.5%5C%25%24%20on%20LLaMA-2%2C%20%241%5C%25%24%20on%20Mistral-7B%2C%20%2411%24-%2422%5C%25%24%20on%0ALLaMA-3%20on%20QA%20tasks%20on%20average.%20We%20only%20utilize%20%2410.4%24-%2418.6%5C%25%24%20of%20the%0Aquantization%20algorithm%20execution%20time%2C%20resulting%20in%20a%20%241.6%24-%241.8%5Ctimes%24%0Aincrease%20in%20inference%20throughput%20compared%20to%20SOTA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17066v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVPTQ%253A%2520Extreme%2520Low-bit%2520Vector%2520Post-Training%2520Quantization%2520for%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DYifei%2520Liu%2520and%2520Jicheng%2520Wen%2520and%2520Yang%2520Wang%2520and%2520Shengyu%2520Ye%2520and%2520Li%2520Lyna%2520Zhang%2520and%2520Ting%2520Cao%2520and%2520Cheng%2520Li%2520and%2520Mao%2520Yang%26entry.1292438233%3D%2520%2520Scaling%2520model%2520size%2520significantly%2520challenges%2520the%2520deployment%2520and%2520inference%2520of%250ALarge%2520Language%2520Models%2520%2528LLMs%2529.%2520Due%2520to%2520the%2520redundancy%2520in%2520LLM%2520weights%252C%2520recent%250Aresearch%2520has%2520focused%2520on%2520pushing%2520weight-only%2520quantization%2520to%2520extremely%2520low-bit%250A%2528even%2520down%2520to%25202%2520bits%2529.%2520It%2520reduces%2520memory%2520requirements%252C%2520optimizes%2520storage%2520costs%252C%250Aand%2520decreases%2520memory%2520bandwidth%2520needs%2520during%2520inference.%2520However%252C%2520due%2520to%250Anumerical%2520representation%2520limitations%252C%2520traditional%2520scalar-based%2520weight%250Aquantization%2520struggles%2520to%2520achieve%2520such%2520extreme%2520low-bit.%2520Recent%2520research%2520on%250AVector%2520Quantization%2520%2528VQ%2529%2520for%2520LLMs%2520has%2520demonstrated%2520the%2520potential%2520for%2520extremely%250Alow-bit%2520model%2520quantization%2520by%2520compressing%2520vectors%2520into%2520indices%2520using%2520lookup%250Atables.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Vector%2520Post-Training%2520Quantization%2520%2528VPTQ%2529%2520for%250Aextremely%2520low-bit%2520quantization%2520of%2520LLMs.%2520We%2520use%2520Second-Order%2520Optimization%2520to%250Aformulate%2520the%2520LLM%2520VQ%2520problem%2520and%2520guide%2520our%2520quantization%2520algorithm%2520design%2520by%250Asolving%2520the%2520optimization.%2520We%2520further%2520refine%2520the%2520weights%2520using%250AChannel-Independent%2520Second-Order%2520Optimization%2520for%2520a%2520granular%2520VQ.%2520In%2520addition%252C%250Aby%2520decomposing%2520the%2520optimization%2520problem%252C%2520we%2520propose%2520a%2520brief%2520and%2520effective%250Acodebook%2520initialization%2520algorithm.%2520We%2520also%2520extend%2520VPTQ%2520to%2520support%2520residual%2520and%250Aoutlier%2520quantization%252C%2520which%2520enhances%2520model%2520accuracy%2520and%2520further%2520compresses%2520the%250Amodel.%2520Our%2520experimental%2520results%2520show%2520that%2520VPTQ%2520reduces%2520model%2520quantization%250Aperplexity%2520by%2520%25240.01%2524-%25240.34%2524%2520on%2520LLaMA-2%252C%2520%25240.38%2524-%25240.68%2524%2520on%2520Mistral-7B%252C%250A%25244.41%2524-%25247.34%2524%2520on%2520LLaMA-3%2520over%2520SOTA%2520at%25202-bit%252C%2520with%2520an%2520average%2520accuracy%250Aimprovement%2520of%2520%25240.79%2524-%25241.5%255C%2525%2524%2520on%2520LLaMA-2%252C%2520%25241%255C%2525%2524%2520on%2520Mistral-7B%252C%2520%252411%2524-%252422%255C%2525%2524%2520on%250ALLaMA-3%2520on%2520QA%2520tasks%2520on%2520average.%2520We%2520only%2520utilize%2520%252410.4%2524-%252418.6%255C%2525%2524%2520of%2520the%250Aquantization%2520algorithm%2520execution%2520time%252C%2520resulting%2520in%2520a%2520%25241.6%2524-%25241.8%255Ctimes%2524%250Aincrease%2520in%2520inference%2520throughput%2520compared%2520to%2520SOTA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17066v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VPTQ%3A%20Extreme%20Low-bit%20Vector%20Post-Training%20Quantization%20for%20Large%0A%20%20Language%20Models&entry.906535625=Yifei%20Liu%20and%20Jicheng%20Wen%20and%20Yang%20Wang%20and%20Shengyu%20Ye%20and%20Li%20Lyna%20Zhang%20and%20Ting%20Cao%20and%20Cheng%20Li%20and%20Mao%20Yang&entry.1292438233=%20%20Scaling%20model%20size%20significantly%20challenges%20the%20deployment%20and%20inference%20of%0ALarge%20Language%20Models%20%28LLMs%29.%20Due%20to%20the%20redundancy%20in%20LLM%20weights%2C%20recent%0Aresearch%20has%20focused%20on%20pushing%20weight-only%20quantization%20to%20extremely%20low-bit%0A%28even%20down%20to%202%20bits%29.%20It%20reduces%20memory%20requirements%2C%20optimizes%20storage%20costs%2C%0Aand%20decreases%20memory%20bandwidth%20needs%20during%20inference.%20However%2C%20due%20to%0Anumerical%20representation%20limitations%2C%20traditional%20scalar-based%20weight%0Aquantization%20struggles%20to%20achieve%20such%20extreme%20low-bit.%20Recent%20research%20on%0AVector%20Quantization%20%28VQ%29%20for%20LLMs%20has%20demonstrated%20the%20potential%20for%20extremely%0Alow-bit%20model%20quantization%20by%20compressing%20vectors%20into%20indices%20using%20lookup%0Atables.%0A%20%20In%20this%20paper%2C%20we%20introduce%20Vector%20Post-Training%20Quantization%20%28VPTQ%29%20for%0Aextremely%20low-bit%20quantization%20of%20LLMs.%20We%20use%20Second-Order%20Optimization%20to%0Aformulate%20the%20LLM%20VQ%20problem%20and%20guide%20our%20quantization%20algorithm%20design%20by%0Asolving%20the%20optimization.%20We%20further%20refine%20the%20weights%20using%0AChannel-Independent%20Second-Order%20Optimization%20for%20a%20granular%20VQ.%20In%20addition%2C%0Aby%20decomposing%20the%20optimization%20problem%2C%20we%20propose%20a%20brief%20and%20effective%0Acodebook%20initialization%20algorithm.%20We%20also%20extend%20VPTQ%20to%20support%20residual%20and%0Aoutlier%20quantization%2C%20which%20enhances%20model%20accuracy%20and%20further%20compresses%20the%0Amodel.%20Our%20experimental%20results%20show%20that%20VPTQ%20reduces%20model%20quantization%0Aperplexity%20by%20%240.01%24-%240.34%24%20on%20LLaMA-2%2C%20%240.38%24-%240.68%24%20on%20Mistral-7B%2C%0A%244.41%24-%247.34%24%20on%20LLaMA-3%20over%20SOTA%20at%202-bit%2C%20with%20an%20average%20accuracy%0Aimprovement%20of%20%240.79%24-%241.5%5C%25%24%20on%20LLaMA-2%2C%20%241%5C%25%24%20on%20Mistral-7B%2C%20%2411%24-%2422%5C%25%24%20on%0ALLaMA-3%20on%20QA%20tasks%20on%20average.%20We%20only%20utilize%20%2410.4%24-%2418.6%5C%25%24%20of%20the%0Aquantization%20algorithm%20execution%20time%2C%20resulting%20in%20a%20%241.6%24-%241.8%5Ctimes%24%0Aincrease%20in%20inference%20throughput%20compared%20to%20SOTA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17066v1&entry.124074799=Read"},
{"title": "Efficient Feature Interactions with Transformers: Improving User\n  Spending Propensity Predictions in Gaming", "author": "Ved Prakash and Kartavya Kothari", "abstract": "  Dream11 is a fantasy sports platform that allows users to create their own\nvirtual teams for real-life sports events. We host multiple sports and matches\nfor our 200M+ user base. In this RMG (real money gaming) setting, users pay an\nentry amount to participate in various contest products that we provide to\nusers. In our current work, we discuss the problem of predicting the user's\npropensity to spend in a gaming round, so it can be utilized for various\ndownstream applications. e.g. Upselling users by incentivizing them marginally\nas per their spending propensity, or personalizing the product listing based on\nthe user's propensity to spend.\n  We aim to model the spending propensity of each user based on past\ntransaction data. In this paper, we benchmark tree-based and deep-learning\nmodels that show good results on structured data, and we propose a new\narchitecture change that is specifically designed to capture the rich\ninteractions among the input features. We show that our proposed architecture\noutperforms the existing models on the task of predicting the user's propensity\nto spend in a gaming round. Our new transformer model surpasses the\nstate-of-the-art FT-Transformer, improving MAE by 2.5\\% and MSE by 21.8\\%.\n", "link": "http://arxiv.org/abs/2409.17077v1", "date": "2024-09-25", "relevancy": 1.9334, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5155}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4781}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Feature%20Interactions%20with%20Transformers%3A%20Improving%20User%0A%20%20Spending%20Propensity%20Predictions%20in%20Gaming&body=Title%3A%20Efficient%20Feature%20Interactions%20with%20Transformers%3A%20Improving%20User%0A%20%20Spending%20Propensity%20Predictions%20in%20Gaming%0AAuthor%3A%20Ved%20Prakash%20and%20Kartavya%20Kothari%0AAbstract%3A%20%20%20Dream11%20is%20a%20fantasy%20sports%20platform%20that%20allows%20users%20to%20create%20their%20own%0Avirtual%20teams%20for%20real-life%20sports%20events.%20We%20host%20multiple%20sports%20and%20matches%0Afor%20our%20200M%2B%20user%20base.%20In%20this%20RMG%20%28real%20money%20gaming%29%20setting%2C%20users%20pay%20an%0Aentry%20amount%20to%20participate%20in%20various%20contest%20products%20that%20we%20provide%20to%0Ausers.%20In%20our%20current%20work%2C%20we%20discuss%20the%20problem%20of%20predicting%20the%20user%27s%0Apropensity%20to%20spend%20in%20a%20gaming%20round%2C%20so%20it%20can%20be%20utilized%20for%20various%0Adownstream%20applications.%20e.g.%20Upselling%20users%20by%20incentivizing%20them%20marginally%0Aas%20per%20their%20spending%20propensity%2C%20or%20personalizing%20the%20product%20listing%20based%20on%0Athe%20user%27s%20propensity%20to%20spend.%0A%20%20We%20aim%20to%20model%20the%20spending%20propensity%20of%20each%20user%20based%20on%20past%0Atransaction%20data.%20In%20this%20paper%2C%20we%20benchmark%20tree-based%20and%20deep-learning%0Amodels%20that%20show%20good%20results%20on%20structured%20data%2C%20and%20we%20propose%20a%20new%0Aarchitecture%20change%20that%20is%20specifically%20designed%20to%20capture%20the%20rich%0Ainteractions%20among%20the%20input%20features.%20We%20show%20that%20our%20proposed%20architecture%0Aoutperforms%20the%20existing%20models%20on%20the%20task%20of%20predicting%20the%20user%27s%20propensity%0Ato%20spend%20in%20a%20gaming%20round.%20Our%20new%20transformer%20model%20surpasses%20the%0Astate-of-the-art%20FT-Transformer%2C%20improving%20MAE%20by%202.5%5C%25%20and%20MSE%20by%2021.8%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17077v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Feature%2520Interactions%2520with%2520Transformers%253A%2520Improving%2520User%250A%2520%2520Spending%2520Propensity%2520Predictions%2520in%2520Gaming%26entry.906535625%3DVed%2520Prakash%2520and%2520Kartavya%2520Kothari%26entry.1292438233%3D%2520%2520Dream11%2520is%2520a%2520fantasy%2520sports%2520platform%2520that%2520allows%2520users%2520to%2520create%2520their%2520own%250Avirtual%2520teams%2520for%2520real-life%2520sports%2520events.%2520We%2520host%2520multiple%2520sports%2520and%2520matches%250Afor%2520our%2520200M%252B%2520user%2520base.%2520In%2520this%2520RMG%2520%2528real%2520money%2520gaming%2529%2520setting%252C%2520users%2520pay%2520an%250Aentry%2520amount%2520to%2520participate%2520in%2520various%2520contest%2520products%2520that%2520we%2520provide%2520to%250Ausers.%2520In%2520our%2520current%2520work%252C%2520we%2520discuss%2520the%2520problem%2520of%2520predicting%2520the%2520user%2527s%250Apropensity%2520to%2520spend%2520in%2520a%2520gaming%2520round%252C%2520so%2520it%2520can%2520be%2520utilized%2520for%2520various%250Adownstream%2520applications.%2520e.g.%2520Upselling%2520users%2520by%2520incentivizing%2520them%2520marginally%250Aas%2520per%2520their%2520spending%2520propensity%252C%2520or%2520personalizing%2520the%2520product%2520listing%2520based%2520on%250Athe%2520user%2527s%2520propensity%2520to%2520spend.%250A%2520%2520We%2520aim%2520to%2520model%2520the%2520spending%2520propensity%2520of%2520each%2520user%2520based%2520on%2520past%250Atransaction%2520data.%2520In%2520this%2520paper%252C%2520we%2520benchmark%2520tree-based%2520and%2520deep-learning%250Amodels%2520that%2520show%2520good%2520results%2520on%2520structured%2520data%252C%2520and%2520we%2520propose%2520a%2520new%250Aarchitecture%2520change%2520that%2520is%2520specifically%2520designed%2520to%2520capture%2520the%2520rich%250Ainteractions%2520among%2520the%2520input%2520features.%2520We%2520show%2520that%2520our%2520proposed%2520architecture%250Aoutperforms%2520the%2520existing%2520models%2520on%2520the%2520task%2520of%2520predicting%2520the%2520user%2527s%2520propensity%250Ato%2520spend%2520in%2520a%2520gaming%2520round.%2520Our%2520new%2520transformer%2520model%2520surpasses%2520the%250Astate-of-the-art%2520FT-Transformer%252C%2520improving%2520MAE%2520by%25202.5%255C%2525%2520and%2520MSE%2520by%252021.8%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17077v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Feature%20Interactions%20with%20Transformers%3A%20Improving%20User%0A%20%20Spending%20Propensity%20Predictions%20in%20Gaming&entry.906535625=Ved%20Prakash%20and%20Kartavya%20Kothari&entry.1292438233=%20%20Dream11%20is%20a%20fantasy%20sports%20platform%20that%20allows%20users%20to%20create%20their%20own%0Avirtual%20teams%20for%20real-life%20sports%20events.%20We%20host%20multiple%20sports%20and%20matches%0Afor%20our%20200M%2B%20user%20base.%20In%20this%20RMG%20%28real%20money%20gaming%29%20setting%2C%20users%20pay%20an%0Aentry%20amount%20to%20participate%20in%20various%20contest%20products%20that%20we%20provide%20to%0Ausers.%20In%20our%20current%20work%2C%20we%20discuss%20the%20problem%20of%20predicting%20the%20user%27s%0Apropensity%20to%20spend%20in%20a%20gaming%20round%2C%20so%20it%20can%20be%20utilized%20for%20various%0Adownstream%20applications.%20e.g.%20Upselling%20users%20by%20incentivizing%20them%20marginally%0Aas%20per%20their%20spending%20propensity%2C%20or%20personalizing%20the%20product%20listing%20based%20on%0Athe%20user%27s%20propensity%20to%20spend.%0A%20%20We%20aim%20to%20model%20the%20spending%20propensity%20of%20each%20user%20based%20on%20past%0Atransaction%20data.%20In%20this%20paper%2C%20we%20benchmark%20tree-based%20and%20deep-learning%0Amodels%20that%20show%20good%20results%20on%20structured%20data%2C%20and%20we%20propose%20a%20new%0Aarchitecture%20change%20that%20is%20specifically%20designed%20to%20capture%20the%20rich%0Ainteractions%20among%20the%20input%20features.%20We%20show%20that%20our%20proposed%20architecture%0Aoutperforms%20the%20existing%20models%20on%20the%20task%20of%20predicting%20the%20user%27s%20propensity%0Ato%20spend%20in%20a%20gaming%20round.%20Our%20new%20transformer%20model%20surpasses%20the%0Astate-of-the-art%20FT-Transformer%2C%20improving%20MAE%20by%202.5%5C%25%20and%20MSE%20by%2021.8%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17077v1&entry.124074799=Read"},
{"title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators", "author": "Ryan Koo and Minhwa Lee and Vipul Raheja and Jong Inn Park and Zae Myung Kim and Dongyeop Kang", "abstract": "  Large Language Models are cognitively biased judges. Large Language Models\n(LLMs) have recently been shown to be effective as automatic evaluators with\nsimple prompting and in-context learning. In this work, we assemble 15 LLMs of\nfour different size ranges and evaluate their output responses by preference\nranking from the other LLMs as evaluators, such as System Star is better than\nSystem Square. We then evaluate the quality of ranking outputs introducing the\nCognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to\nmeasure six different cognitive biases in LLM evaluation outputs, such as the\nEgocentric bias where a model prefers to rank its own outputs highly in\nevaluation. We find that LLMs are biased text quality evaluators, exhibiting\nstrong indications on our bias benchmark (average of 40% of comparisons across\nall models) within each of their evaluations that question their robustness as\nevaluators. Furthermore, we examine the correlation between human and machine\npreferences and calculate the average Rank-Biased Overlap (RBO) score to be\n49.6%, indicating that machine preferences are misaligned with humans.\nAccording to our findings, LLMs may still be unable to be utilized for\nautomatic annotation aligned with human preferences. Our project page is at:\nhttps://minnesotanlp.github.io/cobbler.\n", "link": "http://arxiv.org/abs/2309.17012v3", "date": "2024-09-25", "relevancy": 1.9248, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4823}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4823}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Cognitive%20Biases%20in%20Large%20Language%20Models%20as%20Evaluators&body=Title%3A%20Benchmarking%20Cognitive%20Biases%20in%20Large%20Language%20Models%20as%20Evaluators%0AAuthor%3A%20Ryan%20Koo%20and%20Minhwa%20Lee%20and%20Vipul%20Raheja%20and%20Jong%20Inn%20Park%20and%20Zae%20Myung%20Kim%20and%20Dongyeop%20Kang%0AAbstract%3A%20%20%20Large%20Language%20Models%20are%20cognitively%20biased%20judges.%20Large%20Language%20Models%0A%28LLMs%29%20have%20recently%20been%20shown%20to%20be%20effective%20as%20automatic%20evaluators%20with%0Asimple%20prompting%20and%20in-context%20learning.%20In%20this%20work%2C%20we%20assemble%2015%20LLMs%20of%0Afour%20different%20size%20ranges%20and%20evaluate%20their%20output%20responses%20by%20preference%0Aranking%20from%20the%20other%20LLMs%20as%20evaluators%2C%20such%20as%20System%20Star%20is%20better%20than%0ASystem%20Square.%20We%20then%20evaluate%20the%20quality%20of%20ranking%20outputs%20introducing%20the%0ACognitive%20Bias%20Benchmark%20for%20LLMs%20as%20Evaluators%20%28CoBBLEr%29%2C%20a%20benchmark%20to%0Ameasure%20six%20different%20cognitive%20biases%20in%20LLM%20evaluation%20outputs%2C%20such%20as%20the%0AEgocentric%20bias%20where%20a%20model%20prefers%20to%20rank%20its%20own%20outputs%20highly%20in%0Aevaluation.%20We%20find%20that%20LLMs%20are%20biased%20text%20quality%20evaluators%2C%20exhibiting%0Astrong%20indications%20on%20our%20bias%20benchmark%20%28average%20of%2040%25%20of%20comparisons%20across%0Aall%20models%29%20within%20each%20of%20their%20evaluations%20that%20question%20their%20robustness%20as%0Aevaluators.%20Furthermore%2C%20we%20examine%20the%20correlation%20between%20human%20and%20machine%0Apreferences%20and%20calculate%20the%20average%20Rank-Biased%20Overlap%20%28RBO%29%20score%20to%20be%0A49.6%25%2C%20indicating%20that%20machine%20preferences%20are%20misaligned%20with%20humans.%0AAccording%20to%20our%20findings%2C%20LLMs%20may%20still%20be%20unable%20to%20be%20utilized%20for%0Aautomatic%20annotation%20aligned%20with%20human%20preferences.%20Our%20project%20page%20is%20at%3A%0Ahttps%3A//minnesotanlp.github.io/cobbler.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.17012v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Cognitive%2520Biases%2520in%2520Large%2520Language%2520Models%2520as%2520Evaluators%26entry.906535625%3DRyan%2520Koo%2520and%2520Minhwa%2520Lee%2520and%2520Vipul%2520Raheja%2520and%2520Jong%2520Inn%2520Park%2520and%2520Zae%2520Myung%2520Kim%2520and%2520Dongyeop%2520Kang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520are%2520cognitively%2520biased%2520judges.%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520have%2520recently%2520been%2520shown%2520to%2520be%2520effective%2520as%2520automatic%2520evaluators%2520with%250Asimple%2520prompting%2520and%2520in-context%2520learning.%2520In%2520this%2520work%252C%2520we%2520assemble%252015%2520LLMs%2520of%250Afour%2520different%2520size%2520ranges%2520and%2520evaluate%2520their%2520output%2520responses%2520by%2520preference%250Aranking%2520from%2520the%2520other%2520LLMs%2520as%2520evaluators%252C%2520such%2520as%2520System%2520Star%2520is%2520better%2520than%250ASystem%2520Square.%2520We%2520then%2520evaluate%2520the%2520quality%2520of%2520ranking%2520outputs%2520introducing%2520the%250ACognitive%2520Bias%2520Benchmark%2520for%2520LLMs%2520as%2520Evaluators%2520%2528CoBBLEr%2529%252C%2520a%2520benchmark%2520to%250Ameasure%2520six%2520different%2520cognitive%2520biases%2520in%2520LLM%2520evaluation%2520outputs%252C%2520such%2520as%2520the%250AEgocentric%2520bias%2520where%2520a%2520model%2520prefers%2520to%2520rank%2520its%2520own%2520outputs%2520highly%2520in%250Aevaluation.%2520We%2520find%2520that%2520LLMs%2520are%2520biased%2520text%2520quality%2520evaluators%252C%2520exhibiting%250Astrong%2520indications%2520on%2520our%2520bias%2520benchmark%2520%2528average%2520of%252040%2525%2520of%2520comparisons%2520across%250Aall%2520models%2529%2520within%2520each%2520of%2520their%2520evaluations%2520that%2520question%2520their%2520robustness%2520as%250Aevaluators.%2520Furthermore%252C%2520we%2520examine%2520the%2520correlation%2520between%2520human%2520and%2520machine%250Apreferences%2520and%2520calculate%2520the%2520average%2520Rank-Biased%2520Overlap%2520%2528RBO%2529%2520score%2520to%2520be%250A49.6%2525%252C%2520indicating%2520that%2520machine%2520preferences%2520are%2520misaligned%2520with%2520humans.%250AAccording%2520to%2520our%2520findings%252C%2520LLMs%2520may%2520still%2520be%2520unable%2520to%2520be%2520utilized%2520for%250Aautomatic%2520annotation%2520aligned%2520with%2520human%2520preferences.%2520Our%2520project%2520page%2520is%2520at%253A%250Ahttps%253A//minnesotanlp.github.io/cobbler.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.17012v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Cognitive%20Biases%20in%20Large%20Language%20Models%20as%20Evaluators&entry.906535625=Ryan%20Koo%20and%20Minhwa%20Lee%20and%20Vipul%20Raheja%20and%20Jong%20Inn%20Park%20and%20Zae%20Myung%20Kim%20and%20Dongyeop%20Kang&entry.1292438233=%20%20Large%20Language%20Models%20are%20cognitively%20biased%20judges.%20Large%20Language%20Models%0A%28LLMs%29%20have%20recently%20been%20shown%20to%20be%20effective%20as%20automatic%20evaluators%20with%0Asimple%20prompting%20and%20in-context%20learning.%20In%20this%20work%2C%20we%20assemble%2015%20LLMs%20of%0Afour%20different%20size%20ranges%20and%20evaluate%20their%20output%20responses%20by%20preference%0Aranking%20from%20the%20other%20LLMs%20as%20evaluators%2C%20such%20as%20System%20Star%20is%20better%20than%0ASystem%20Square.%20We%20then%20evaluate%20the%20quality%20of%20ranking%20outputs%20introducing%20the%0ACognitive%20Bias%20Benchmark%20for%20LLMs%20as%20Evaluators%20%28CoBBLEr%29%2C%20a%20benchmark%20to%0Ameasure%20six%20different%20cognitive%20biases%20in%20LLM%20evaluation%20outputs%2C%20such%20as%20the%0AEgocentric%20bias%20where%20a%20model%20prefers%20to%20rank%20its%20own%20outputs%20highly%20in%0Aevaluation.%20We%20find%20that%20LLMs%20are%20biased%20text%20quality%20evaluators%2C%20exhibiting%0Astrong%20indications%20on%20our%20bias%20benchmark%20%28average%20of%2040%25%20of%20comparisons%20across%0Aall%20models%29%20within%20each%20of%20their%20evaluations%20that%20question%20their%20robustness%20as%0Aevaluators.%20Furthermore%2C%20we%20examine%20the%20correlation%20between%20human%20and%20machine%0Apreferences%20and%20calculate%20the%20average%20Rank-Biased%20Overlap%20%28RBO%29%20score%20to%20be%0A49.6%25%2C%20indicating%20that%20machine%20preferences%20are%20misaligned%20with%20humans.%0AAccording%20to%20our%20findings%2C%20LLMs%20may%20still%20be%20unable%20to%20be%20utilized%20for%0Aautomatic%20annotation%20aligned%20with%20human%20preferences.%20Our%20project%20page%20is%20at%3A%0Ahttps%3A//minnesotanlp.github.io/cobbler.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.17012v3&entry.124074799=Read"},
{"title": "Are LLMs Ready for Real-World Materials Discovery?", "author": "Santiago Miret and N M Anoop Krishnan", "abstract": "  Large Language Models (LLMs) create exciting possibilities for powerful\nlanguage processing tools to accelerate research in materials science. While\nLLMs have great potential to accelerate materials understanding and discovery,\nthey currently fall short in being practical materials science tools. In this\nposition paper, we show relevant failure cases of LLMs in materials science\nthat reveal current limitations of LLMs related to comprehending and reasoning\nover complex, interconnected materials science knowledge. Given those\nshortcomings, we outline a framework for developing Materials Science LLMs\n(MatSci-LLMs) that are grounded in materials science knowledge and hypothesis\ngeneration followed by hypothesis testing. The path to attaining performant\nMatSci-LLMs rests in large part on building high-quality, multi-modal datasets\nsourced from scientific literature where various information extraction\nchallenges persist. As such, we describe key materials science information\nextraction challenges which need to be overcome in order to build large-scale,\nmulti-modal datasets that capture valuable materials science knowledge.\nFinally, we outline a roadmap for applying future MatSci-LLMs for real-world\nmaterials discovery via: 1. Automated Knowledge Base Generation; 2. Automated\nIn-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials\nLaboratories.\n", "link": "http://arxiv.org/abs/2402.05200v2", "date": "2024-09-25", "relevancy": 1.9237, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.482}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.482}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20LLMs%20Ready%20for%20Real-World%20Materials%20Discovery%3F&body=Title%3A%20Are%20LLMs%20Ready%20for%20Real-World%20Materials%20Discovery%3F%0AAuthor%3A%20Santiago%20Miret%20and%20N%20M%20Anoop%20Krishnan%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20create%20exciting%20possibilities%20for%20powerful%0Alanguage%20processing%20tools%20to%20accelerate%20research%20in%20materials%20science.%20While%0ALLMs%20have%20great%20potential%20to%20accelerate%20materials%20understanding%20and%20discovery%2C%0Athey%20currently%20fall%20short%20in%20being%20practical%20materials%20science%20tools.%20In%20this%0Aposition%20paper%2C%20we%20show%20relevant%20failure%20cases%20of%20LLMs%20in%20materials%20science%0Athat%20reveal%20current%20limitations%20of%20LLMs%20related%20to%20comprehending%20and%20reasoning%0Aover%20complex%2C%20interconnected%20materials%20science%20knowledge.%20Given%20those%0Ashortcomings%2C%20we%20outline%20a%20framework%20for%20developing%20Materials%20Science%20LLMs%0A%28MatSci-LLMs%29%20that%20are%20grounded%20in%20materials%20science%20knowledge%20and%20hypothesis%0Ageneration%20followed%20by%20hypothesis%20testing.%20The%20path%20to%20attaining%20performant%0AMatSci-LLMs%20rests%20in%20large%20part%20on%20building%20high-quality%2C%20multi-modal%20datasets%0Asourced%20from%20scientific%20literature%20where%20various%20information%20extraction%0Achallenges%20persist.%20As%20such%2C%20we%20describe%20key%20materials%20science%20information%0Aextraction%20challenges%20which%20need%20to%20be%20overcome%20in%20order%20to%20build%20large-scale%2C%0Amulti-modal%20datasets%20that%20capture%20valuable%20materials%20science%20knowledge.%0AFinally%2C%20we%20outline%20a%20roadmap%20for%20applying%20future%20MatSci-LLMs%20for%20real-world%0Amaterials%20discovery%20via%3A%201.%20Automated%20Knowledge%20Base%20Generation%3B%202.%20Automated%0AIn-Silico%20Material%20Design%3B%20and%203.%20MatSci-LLM%20Integrated%20Self-Driving%20Materials%0ALaboratories.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05200v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520LLMs%2520Ready%2520for%2520Real-World%2520Materials%2520Discovery%253F%26entry.906535625%3DSantiago%2520Miret%2520and%2520N%2520M%2520Anoop%2520Krishnan%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520create%2520exciting%2520possibilities%2520for%2520powerful%250Alanguage%2520processing%2520tools%2520to%2520accelerate%2520research%2520in%2520materials%2520science.%2520While%250ALLMs%2520have%2520great%2520potential%2520to%2520accelerate%2520materials%2520understanding%2520and%2520discovery%252C%250Athey%2520currently%2520fall%2520short%2520in%2520being%2520practical%2520materials%2520science%2520tools.%2520In%2520this%250Aposition%2520paper%252C%2520we%2520show%2520relevant%2520failure%2520cases%2520of%2520LLMs%2520in%2520materials%2520science%250Athat%2520reveal%2520current%2520limitations%2520of%2520LLMs%2520related%2520to%2520comprehending%2520and%2520reasoning%250Aover%2520complex%252C%2520interconnected%2520materials%2520science%2520knowledge.%2520Given%2520those%250Ashortcomings%252C%2520we%2520outline%2520a%2520framework%2520for%2520developing%2520Materials%2520Science%2520LLMs%250A%2528MatSci-LLMs%2529%2520that%2520are%2520grounded%2520in%2520materials%2520science%2520knowledge%2520and%2520hypothesis%250Ageneration%2520followed%2520by%2520hypothesis%2520testing.%2520The%2520path%2520to%2520attaining%2520performant%250AMatSci-LLMs%2520rests%2520in%2520large%2520part%2520on%2520building%2520high-quality%252C%2520multi-modal%2520datasets%250Asourced%2520from%2520scientific%2520literature%2520where%2520various%2520information%2520extraction%250Achallenges%2520persist.%2520As%2520such%252C%2520we%2520describe%2520key%2520materials%2520science%2520information%250Aextraction%2520challenges%2520which%2520need%2520to%2520be%2520overcome%2520in%2520order%2520to%2520build%2520large-scale%252C%250Amulti-modal%2520datasets%2520that%2520capture%2520valuable%2520materials%2520science%2520knowledge.%250AFinally%252C%2520we%2520outline%2520a%2520roadmap%2520for%2520applying%2520future%2520MatSci-LLMs%2520for%2520real-world%250Amaterials%2520discovery%2520via%253A%25201.%2520Automated%2520Knowledge%2520Base%2520Generation%253B%25202.%2520Automated%250AIn-Silico%2520Material%2520Design%253B%2520and%25203.%2520MatSci-LLM%2520Integrated%2520Self-Driving%2520Materials%250ALaboratories.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05200v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20LLMs%20Ready%20for%20Real-World%20Materials%20Discovery%3F&entry.906535625=Santiago%20Miret%20and%20N%20M%20Anoop%20Krishnan&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20create%20exciting%20possibilities%20for%20powerful%0Alanguage%20processing%20tools%20to%20accelerate%20research%20in%20materials%20science.%20While%0ALLMs%20have%20great%20potential%20to%20accelerate%20materials%20understanding%20and%20discovery%2C%0Athey%20currently%20fall%20short%20in%20being%20practical%20materials%20science%20tools.%20In%20this%0Aposition%20paper%2C%20we%20show%20relevant%20failure%20cases%20of%20LLMs%20in%20materials%20science%0Athat%20reveal%20current%20limitations%20of%20LLMs%20related%20to%20comprehending%20and%20reasoning%0Aover%20complex%2C%20interconnected%20materials%20science%20knowledge.%20Given%20those%0Ashortcomings%2C%20we%20outline%20a%20framework%20for%20developing%20Materials%20Science%20LLMs%0A%28MatSci-LLMs%29%20that%20are%20grounded%20in%20materials%20science%20knowledge%20and%20hypothesis%0Ageneration%20followed%20by%20hypothesis%20testing.%20The%20path%20to%20attaining%20performant%0AMatSci-LLMs%20rests%20in%20large%20part%20on%20building%20high-quality%2C%20multi-modal%20datasets%0Asourced%20from%20scientific%20literature%20where%20various%20information%20extraction%0Achallenges%20persist.%20As%20such%2C%20we%20describe%20key%20materials%20science%20information%0Aextraction%20challenges%20which%20need%20to%20be%20overcome%20in%20order%20to%20build%20large-scale%2C%0Amulti-modal%20datasets%20that%20capture%20valuable%20materials%20science%20knowledge.%0AFinally%2C%20we%20outline%20a%20roadmap%20for%20applying%20future%20MatSci-LLMs%20for%20real-world%0Amaterials%20discovery%20via%3A%201.%20Automated%20Knowledge%20Base%20Generation%3B%202.%20Automated%0AIn-Silico%20Material%20Design%3B%20and%203.%20MatSci-LLM%20Integrated%20Self-Driving%20Materials%0ALaboratories.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05200v2&entry.124074799=Read"},
{"title": "Investigating OCR-Sensitive Neurons to Improve Entity Recognition in\n  Historical Documents", "author": "Emanuela Boros and Maud Ehrmann", "abstract": "  This paper investigates the presence of OCR-sensitive neurons within the\nTransformer architecture and their influence on named entity recognition (NER)\nperformance on historical documents. By analysing neuron activation patterns in\nresponse to clean and noisy text inputs, we identify and then neutralise\nOCR-sensitive neurons to improve model performance. Based on two open access\nlarge language models (Llama2 and Mistral), experiments demonstrate the\nexistence of OCR-sensitive regions and show improvements in NER performance on\nhistorical newspapers and classical commentaries, highlighting the potential of\ntargeted neuron modulation to improve models' performance on noisy text.\n", "link": "http://arxiv.org/abs/2409.16934v1", "date": "2024-09-25", "relevancy": 1.923, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5095}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4751}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20OCR-Sensitive%20Neurons%20to%20Improve%20Entity%20Recognition%20in%0A%20%20Historical%20Documents&body=Title%3A%20Investigating%20OCR-Sensitive%20Neurons%20to%20Improve%20Entity%20Recognition%20in%0A%20%20Historical%20Documents%0AAuthor%3A%20Emanuela%20Boros%20and%20Maud%20Ehrmann%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20presence%20of%20OCR-sensitive%20neurons%20within%20the%0ATransformer%20architecture%20and%20their%20influence%20on%20named%20entity%20recognition%20%28NER%29%0Aperformance%20on%20historical%20documents.%20By%20analysing%20neuron%20activation%20patterns%20in%0Aresponse%20to%20clean%20and%20noisy%20text%20inputs%2C%20we%20identify%20and%20then%20neutralise%0AOCR-sensitive%20neurons%20to%20improve%20model%20performance.%20Based%20on%20two%20open%20access%0Alarge%20language%20models%20%28Llama2%20and%20Mistral%29%2C%20experiments%20demonstrate%20the%0Aexistence%20of%20OCR-sensitive%20regions%20and%20show%20improvements%20in%20NER%20performance%20on%0Ahistorical%20newspapers%20and%20classical%20commentaries%2C%20highlighting%20the%20potential%20of%0Atargeted%20neuron%20modulation%20to%20improve%20models%27%20performance%20on%20noisy%20text.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16934v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520OCR-Sensitive%2520Neurons%2520to%2520Improve%2520Entity%2520Recognition%2520in%250A%2520%2520Historical%2520Documents%26entry.906535625%3DEmanuela%2520Boros%2520and%2520Maud%2520Ehrmann%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520presence%2520of%2520OCR-sensitive%2520neurons%2520within%2520the%250ATransformer%2520architecture%2520and%2520their%2520influence%2520on%2520named%2520entity%2520recognition%2520%2528NER%2529%250Aperformance%2520on%2520historical%2520documents.%2520By%2520analysing%2520neuron%2520activation%2520patterns%2520in%250Aresponse%2520to%2520clean%2520and%2520noisy%2520text%2520inputs%252C%2520we%2520identify%2520and%2520then%2520neutralise%250AOCR-sensitive%2520neurons%2520to%2520improve%2520model%2520performance.%2520Based%2520on%2520two%2520open%2520access%250Alarge%2520language%2520models%2520%2528Llama2%2520and%2520Mistral%2529%252C%2520experiments%2520demonstrate%2520the%250Aexistence%2520of%2520OCR-sensitive%2520regions%2520and%2520show%2520improvements%2520in%2520NER%2520performance%2520on%250Ahistorical%2520newspapers%2520and%2520classical%2520commentaries%252C%2520highlighting%2520the%2520potential%2520of%250Atargeted%2520neuron%2520modulation%2520to%2520improve%2520models%2527%2520performance%2520on%2520noisy%2520text.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16934v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20OCR-Sensitive%20Neurons%20to%20Improve%20Entity%20Recognition%20in%0A%20%20Historical%20Documents&entry.906535625=Emanuela%20Boros%20and%20Maud%20Ehrmann&entry.1292438233=%20%20This%20paper%20investigates%20the%20presence%20of%20OCR-sensitive%20neurons%20within%20the%0ATransformer%20architecture%20and%20their%20influence%20on%20named%20entity%20recognition%20%28NER%29%0Aperformance%20on%20historical%20documents.%20By%20analysing%20neuron%20activation%20patterns%20in%0Aresponse%20to%20clean%20and%20noisy%20text%20inputs%2C%20we%20identify%20and%20then%20neutralise%0AOCR-sensitive%20neurons%20to%20improve%20model%20performance.%20Based%20on%20two%20open%20access%0Alarge%20language%20models%20%28Llama2%20and%20Mistral%29%2C%20experiments%20demonstrate%20the%0Aexistence%20of%20OCR-sensitive%20regions%20and%20show%20improvements%20in%20NER%20performance%20on%0Ahistorical%20newspapers%20and%20classical%20commentaries%2C%20highlighting%20the%20potential%20of%0Atargeted%20neuron%20modulation%20to%20improve%20models%27%20performance%20on%20noisy%20text.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16934v1&entry.124074799=Read"},
{"title": "AXCEL: Automated eXplainable Consistency Evaluation using LLMs", "author": "P Aditya Sreekar and Sahil Verma and Suransh Chopra and Sarik Ghazarian and Abhishek Persad and Narayanan Sadagopan", "abstract": "  Large Language Models (LLMs) are widely used in both industry and academia\nfor various tasks, yet evaluating the consistency of generated text responses\ncontinues to be a challenge. Traditional metrics like ROUGE and BLEU show a\nweak correlation with human judgment. More sophisticated metrics using Natural\nLanguage Inference (NLI) have shown improved correlations but are complex to\nimplement, require domain-specific training due to poor cross-domain\ngeneralization, and lack explainability. More recently, prompt-based metrics\nusing LLMs as evaluators have emerged; while they are easier to implement, they\nstill lack explainability and depend on task-specific prompts, which limits\ntheir generalizability. This work introduces Automated eXplainable Consistency\nEvaluation using LLMs (AXCEL), a prompt-based consistency metric which offers\nexplanations for the consistency scores by providing detailed reasoning and\npinpointing inconsistent text spans. AXCEL is also a generalizable metric which\ncan be adopted to multiple tasks without changing the prompt. AXCEL outperforms\nboth non-prompt and prompt-based state-of-the-art (SOTA) metrics in detecting\ninconsistencies across summarization by 8.7%, free text generation by 6.2%, and\ndata-to-text conversion tasks by 29.4%. We also evaluate the influence of\nunderlying LLMs on prompt based metric performance and recalibrate the SOTA\nprompt-based metrics with the latest LLMs for fair comparison. Further, we show\nthat AXCEL demonstrates strong performance using open source LLMs.\n", "link": "http://arxiv.org/abs/2409.16984v1", "date": "2024-09-25", "relevancy": 1.9229, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4877}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4793}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AXCEL%3A%20Automated%20eXplainable%20Consistency%20Evaluation%20using%20LLMs&body=Title%3A%20AXCEL%3A%20Automated%20eXplainable%20Consistency%20Evaluation%20using%20LLMs%0AAuthor%3A%20P%20Aditya%20Sreekar%20and%20Sahil%20Verma%20and%20Suransh%20Chopra%20and%20Sarik%20Ghazarian%20and%20Abhishek%20Persad%20and%20Narayanan%20Sadagopan%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20widely%20used%20in%20both%20industry%20and%20academia%0Afor%20various%20tasks%2C%20yet%20evaluating%20the%20consistency%20of%20generated%20text%20responses%0Acontinues%20to%20be%20a%20challenge.%20Traditional%20metrics%20like%20ROUGE%20and%20BLEU%20show%20a%0Aweak%20correlation%20with%20human%20judgment.%20More%20sophisticated%20metrics%20using%20Natural%0ALanguage%20Inference%20%28NLI%29%20have%20shown%20improved%20correlations%20but%20are%20complex%20to%0Aimplement%2C%20require%20domain-specific%20training%20due%20to%20poor%20cross-domain%0Ageneralization%2C%20and%20lack%20explainability.%20More%20recently%2C%20prompt-based%20metrics%0Ausing%20LLMs%20as%20evaluators%20have%20emerged%3B%20while%20they%20are%20easier%20to%20implement%2C%20they%0Astill%20lack%20explainability%20and%20depend%20on%20task-specific%20prompts%2C%20which%20limits%0Atheir%20generalizability.%20This%20work%20introduces%20Automated%20eXplainable%20Consistency%0AEvaluation%20using%20LLMs%20%28AXCEL%29%2C%20a%20prompt-based%20consistency%20metric%20which%20offers%0Aexplanations%20for%20the%20consistency%20scores%20by%20providing%20detailed%20reasoning%20and%0Apinpointing%20inconsistent%20text%20spans.%20AXCEL%20is%20also%20a%20generalizable%20metric%20which%0Acan%20be%20adopted%20to%20multiple%20tasks%20without%20changing%20the%20prompt.%20AXCEL%20outperforms%0Aboth%20non-prompt%20and%20prompt-based%20state-of-the-art%20%28SOTA%29%20metrics%20in%20detecting%0Ainconsistencies%20across%20summarization%20by%208.7%25%2C%20free%20text%20generation%20by%206.2%25%2C%20and%0Adata-to-text%20conversion%20tasks%20by%2029.4%25.%20We%20also%20evaluate%20the%20influence%20of%0Aunderlying%20LLMs%20on%20prompt%20based%20metric%20performance%20and%20recalibrate%20the%20SOTA%0Aprompt-based%20metrics%20with%20the%20latest%20LLMs%20for%20fair%20comparison.%20Further%2C%20we%20show%0Athat%20AXCEL%20demonstrates%20strong%20performance%20using%20open%20source%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAXCEL%253A%2520Automated%2520eXplainable%2520Consistency%2520Evaluation%2520using%2520LLMs%26entry.906535625%3DP%2520Aditya%2520Sreekar%2520and%2520Sahil%2520Verma%2520and%2520Suransh%2520Chopra%2520and%2520Sarik%2520Ghazarian%2520and%2520Abhishek%2520Persad%2520and%2520Narayanan%2520Sadagopan%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520widely%2520used%2520in%2520both%2520industry%2520and%2520academia%250Afor%2520various%2520tasks%252C%2520yet%2520evaluating%2520the%2520consistency%2520of%2520generated%2520text%2520responses%250Acontinues%2520to%2520be%2520a%2520challenge.%2520Traditional%2520metrics%2520like%2520ROUGE%2520and%2520BLEU%2520show%2520a%250Aweak%2520correlation%2520with%2520human%2520judgment.%2520More%2520sophisticated%2520metrics%2520using%2520Natural%250ALanguage%2520Inference%2520%2528NLI%2529%2520have%2520shown%2520improved%2520correlations%2520but%2520are%2520complex%2520to%250Aimplement%252C%2520require%2520domain-specific%2520training%2520due%2520to%2520poor%2520cross-domain%250Ageneralization%252C%2520and%2520lack%2520explainability.%2520More%2520recently%252C%2520prompt-based%2520metrics%250Ausing%2520LLMs%2520as%2520evaluators%2520have%2520emerged%253B%2520while%2520they%2520are%2520easier%2520to%2520implement%252C%2520they%250Astill%2520lack%2520explainability%2520and%2520depend%2520on%2520task-specific%2520prompts%252C%2520which%2520limits%250Atheir%2520generalizability.%2520This%2520work%2520introduces%2520Automated%2520eXplainable%2520Consistency%250AEvaluation%2520using%2520LLMs%2520%2528AXCEL%2529%252C%2520a%2520prompt-based%2520consistency%2520metric%2520which%2520offers%250Aexplanations%2520for%2520the%2520consistency%2520scores%2520by%2520providing%2520detailed%2520reasoning%2520and%250Apinpointing%2520inconsistent%2520text%2520spans.%2520AXCEL%2520is%2520also%2520a%2520generalizable%2520metric%2520which%250Acan%2520be%2520adopted%2520to%2520multiple%2520tasks%2520without%2520changing%2520the%2520prompt.%2520AXCEL%2520outperforms%250Aboth%2520non-prompt%2520and%2520prompt-based%2520state-of-the-art%2520%2528SOTA%2529%2520metrics%2520in%2520detecting%250Ainconsistencies%2520across%2520summarization%2520by%25208.7%2525%252C%2520free%2520text%2520generation%2520by%25206.2%2525%252C%2520and%250Adata-to-text%2520conversion%2520tasks%2520by%252029.4%2525.%2520We%2520also%2520evaluate%2520the%2520influence%2520of%250Aunderlying%2520LLMs%2520on%2520prompt%2520based%2520metric%2520performance%2520and%2520recalibrate%2520the%2520SOTA%250Aprompt-based%2520metrics%2520with%2520the%2520latest%2520LLMs%2520for%2520fair%2520comparison.%2520Further%252C%2520we%2520show%250Athat%2520AXCEL%2520demonstrates%2520strong%2520performance%2520using%2520open%2520source%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AXCEL%3A%20Automated%20eXplainable%20Consistency%20Evaluation%20using%20LLMs&entry.906535625=P%20Aditya%20Sreekar%20and%20Sahil%20Verma%20and%20Suransh%20Chopra%20and%20Sarik%20Ghazarian%20and%20Abhishek%20Persad%20and%20Narayanan%20Sadagopan&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20widely%20used%20in%20both%20industry%20and%20academia%0Afor%20various%20tasks%2C%20yet%20evaluating%20the%20consistency%20of%20generated%20text%20responses%0Acontinues%20to%20be%20a%20challenge.%20Traditional%20metrics%20like%20ROUGE%20and%20BLEU%20show%20a%0Aweak%20correlation%20with%20human%20judgment.%20More%20sophisticated%20metrics%20using%20Natural%0ALanguage%20Inference%20%28NLI%29%20have%20shown%20improved%20correlations%20but%20are%20complex%20to%0Aimplement%2C%20require%20domain-specific%20training%20due%20to%20poor%20cross-domain%0Ageneralization%2C%20and%20lack%20explainability.%20More%20recently%2C%20prompt-based%20metrics%0Ausing%20LLMs%20as%20evaluators%20have%20emerged%3B%20while%20they%20are%20easier%20to%20implement%2C%20they%0Astill%20lack%20explainability%20and%20depend%20on%20task-specific%20prompts%2C%20which%20limits%0Atheir%20generalizability.%20This%20work%20introduces%20Automated%20eXplainable%20Consistency%0AEvaluation%20using%20LLMs%20%28AXCEL%29%2C%20a%20prompt-based%20consistency%20metric%20which%20offers%0Aexplanations%20for%20the%20consistency%20scores%20by%20providing%20detailed%20reasoning%20and%0Apinpointing%20inconsistent%20text%20spans.%20AXCEL%20is%20also%20a%20generalizable%20metric%20which%0Acan%20be%20adopted%20to%20multiple%20tasks%20without%20changing%20the%20prompt.%20AXCEL%20outperforms%0Aboth%20non-prompt%20and%20prompt-based%20state-of-the-art%20%28SOTA%29%20metrics%20in%20detecting%0Ainconsistencies%20across%20summarization%20by%208.7%25%2C%20free%20text%20generation%20by%206.2%25%2C%20and%0Adata-to-text%20conversion%20tasks%20by%2029.4%25.%20We%20also%20evaluate%20the%20influence%20of%0Aunderlying%20LLMs%20on%20prompt%20based%20metric%20performance%20and%20recalibrate%20the%20SOTA%0Aprompt-based%20metrics%20with%20the%20latest%20LLMs%20for%20fair%20comparison.%20Further%2C%20we%20show%0Athat%20AXCEL%20demonstrates%20strong%20performance%20using%20open%20source%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16984v1&entry.124074799=Read"},
{"title": "Efficient Submap-based Autonomous MAV Exploration using Visual-Inertial\n  SLAM Configurable for LiDARs or Depth Cameras", "author": "Sotiris Papatheodorou and Simon Boche and Sebasti\u00e1n Barbas Laina and Stefan Leutenegger", "abstract": "  Autonomous exploration of unknown space is an essential component for the\ndeployment of mobile robots in the real world. Safe navigation is crucial for\nall robotics applications and requires accurate and consistent maps of the\nrobot's surroundings. To achieve full autonomy and allow deployment in a wide\nvariety of environments, the robot must rely on on-board state estimation which\nis prone to drift over time. We propose a Micro Aerial Vehicle (MAV)\nexploration framework based on local submaps to allow retaining global\nconsistency by applying loop-closure corrections to the relative submap poses.\nTo enable large-scale exploration we efficiently compute global,\nenvironment-wide frontiers from the local submap frontiers and use a\nsampling-based next-best-view exploration planner. Our method seamlessly\nsupports using either a LiDAR sensor or a depth camera, making it suitable for\ndifferent kinds of MAV platforms. We perform comparative evaluations in\nsimulation against a state-of-the-art submap-based exploration framework to\nshowcase the efficiency and reconstruction quality of our approach. Finally, we\ndemonstrate the applicability of our method to real-world MAVs, one equipped\nwith a LiDAR and the other with a depth camera. Video available at\nhttps://youtu.be/Uf5fwmYcuq4 .\n", "link": "http://arxiv.org/abs/2409.16972v1", "date": "2024-09-25", "relevancy": 1.9172, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6645}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6087}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Submap-based%20Autonomous%20MAV%20Exploration%20using%20Visual-Inertial%0A%20%20SLAM%20Configurable%20for%20LiDARs%20or%20Depth%20Cameras&body=Title%3A%20Efficient%20Submap-based%20Autonomous%20MAV%20Exploration%20using%20Visual-Inertial%0A%20%20SLAM%20Configurable%20for%20LiDARs%20or%20Depth%20Cameras%0AAuthor%3A%20Sotiris%20Papatheodorou%20and%20Simon%20Boche%20and%20Sebasti%C3%A1n%20Barbas%20Laina%20and%20Stefan%20Leutenegger%0AAbstract%3A%20%20%20Autonomous%20exploration%20of%20unknown%20space%20is%20an%20essential%20component%20for%20the%0Adeployment%20of%20mobile%20robots%20in%20the%20real%20world.%20Safe%20navigation%20is%20crucial%20for%0Aall%20robotics%20applications%20and%20requires%20accurate%20and%20consistent%20maps%20of%20the%0Arobot%27s%20surroundings.%20To%20achieve%20full%20autonomy%20and%20allow%20deployment%20in%20a%20wide%0Avariety%20of%20environments%2C%20the%20robot%20must%20rely%20on%20on-board%20state%20estimation%20which%0Ais%20prone%20to%20drift%20over%20time.%20We%20propose%20a%20Micro%20Aerial%20Vehicle%20%28MAV%29%0Aexploration%20framework%20based%20on%20local%20submaps%20to%20allow%20retaining%20global%0Aconsistency%20by%20applying%20loop-closure%20corrections%20to%20the%20relative%20submap%20poses.%0ATo%20enable%20large-scale%20exploration%20we%20efficiently%20compute%20global%2C%0Aenvironment-wide%20frontiers%20from%20the%20local%20submap%20frontiers%20and%20use%20a%0Asampling-based%20next-best-view%20exploration%20planner.%20Our%20method%20seamlessly%0Asupports%20using%20either%20a%20LiDAR%20sensor%20or%20a%20depth%20camera%2C%20making%20it%20suitable%20for%0Adifferent%20kinds%20of%20MAV%20platforms.%20We%20perform%20comparative%20evaluations%20in%0Asimulation%20against%20a%20state-of-the-art%20submap-based%20exploration%20framework%20to%0Ashowcase%20the%20efficiency%20and%20reconstruction%20quality%20of%20our%20approach.%20Finally%2C%20we%0Ademonstrate%20the%20applicability%20of%20our%20method%20to%20real-world%20MAVs%2C%20one%20equipped%0Awith%20a%20LiDAR%20and%20the%20other%20with%20a%20depth%20camera.%20Video%20available%20at%0Ahttps%3A//youtu.be/Uf5fwmYcuq4%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16972v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Submap-based%2520Autonomous%2520MAV%2520Exploration%2520using%2520Visual-Inertial%250A%2520%2520SLAM%2520Configurable%2520for%2520LiDARs%2520or%2520Depth%2520Cameras%26entry.906535625%3DSotiris%2520Papatheodorou%2520and%2520Simon%2520Boche%2520and%2520Sebasti%25C3%25A1n%2520Barbas%2520Laina%2520and%2520Stefan%2520Leutenegger%26entry.1292438233%3D%2520%2520Autonomous%2520exploration%2520of%2520unknown%2520space%2520is%2520an%2520essential%2520component%2520for%2520the%250Adeployment%2520of%2520mobile%2520robots%2520in%2520the%2520real%2520world.%2520Safe%2520navigation%2520is%2520crucial%2520for%250Aall%2520robotics%2520applications%2520and%2520requires%2520accurate%2520and%2520consistent%2520maps%2520of%2520the%250Arobot%2527s%2520surroundings.%2520To%2520achieve%2520full%2520autonomy%2520and%2520allow%2520deployment%2520in%2520a%2520wide%250Avariety%2520of%2520environments%252C%2520the%2520robot%2520must%2520rely%2520on%2520on-board%2520state%2520estimation%2520which%250Ais%2520prone%2520to%2520drift%2520over%2520time.%2520We%2520propose%2520a%2520Micro%2520Aerial%2520Vehicle%2520%2528MAV%2529%250Aexploration%2520framework%2520based%2520on%2520local%2520submaps%2520to%2520allow%2520retaining%2520global%250Aconsistency%2520by%2520applying%2520loop-closure%2520corrections%2520to%2520the%2520relative%2520submap%2520poses.%250ATo%2520enable%2520large-scale%2520exploration%2520we%2520efficiently%2520compute%2520global%252C%250Aenvironment-wide%2520frontiers%2520from%2520the%2520local%2520submap%2520frontiers%2520and%2520use%2520a%250Asampling-based%2520next-best-view%2520exploration%2520planner.%2520Our%2520method%2520seamlessly%250Asupports%2520using%2520either%2520a%2520LiDAR%2520sensor%2520or%2520a%2520depth%2520camera%252C%2520making%2520it%2520suitable%2520for%250Adifferent%2520kinds%2520of%2520MAV%2520platforms.%2520We%2520perform%2520comparative%2520evaluations%2520in%250Asimulation%2520against%2520a%2520state-of-the-art%2520submap-based%2520exploration%2520framework%2520to%250Ashowcase%2520the%2520efficiency%2520and%2520reconstruction%2520quality%2520of%2520our%2520approach.%2520Finally%252C%2520we%250Ademonstrate%2520the%2520applicability%2520of%2520our%2520method%2520to%2520real-world%2520MAVs%252C%2520one%2520equipped%250Awith%2520a%2520LiDAR%2520and%2520the%2520other%2520with%2520a%2520depth%2520camera.%2520Video%2520available%2520at%250Ahttps%253A//youtu.be/Uf5fwmYcuq4%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16972v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Submap-based%20Autonomous%20MAV%20Exploration%20using%20Visual-Inertial%0A%20%20SLAM%20Configurable%20for%20LiDARs%20or%20Depth%20Cameras&entry.906535625=Sotiris%20Papatheodorou%20and%20Simon%20Boche%20and%20Sebasti%C3%A1n%20Barbas%20Laina%20and%20Stefan%20Leutenegger&entry.1292438233=%20%20Autonomous%20exploration%20of%20unknown%20space%20is%20an%20essential%20component%20for%20the%0Adeployment%20of%20mobile%20robots%20in%20the%20real%20world.%20Safe%20navigation%20is%20crucial%20for%0Aall%20robotics%20applications%20and%20requires%20accurate%20and%20consistent%20maps%20of%20the%0Arobot%27s%20surroundings.%20To%20achieve%20full%20autonomy%20and%20allow%20deployment%20in%20a%20wide%0Avariety%20of%20environments%2C%20the%20robot%20must%20rely%20on%20on-board%20state%20estimation%20which%0Ais%20prone%20to%20drift%20over%20time.%20We%20propose%20a%20Micro%20Aerial%20Vehicle%20%28MAV%29%0Aexploration%20framework%20based%20on%20local%20submaps%20to%20allow%20retaining%20global%0Aconsistency%20by%20applying%20loop-closure%20corrections%20to%20the%20relative%20submap%20poses.%0ATo%20enable%20large-scale%20exploration%20we%20efficiently%20compute%20global%2C%0Aenvironment-wide%20frontiers%20from%20the%20local%20submap%20frontiers%20and%20use%20a%0Asampling-based%20next-best-view%20exploration%20planner.%20Our%20method%20seamlessly%0Asupports%20using%20either%20a%20LiDAR%20sensor%20or%20a%20depth%20camera%2C%20making%20it%20suitable%20for%0Adifferent%20kinds%20of%20MAV%20platforms.%20We%20perform%20comparative%20evaluations%20in%0Asimulation%20against%20a%20state-of-the-art%20submap-based%20exploration%20framework%20to%0Ashowcase%20the%20efficiency%20and%20reconstruction%20quality%20of%20our%20approach.%20Finally%2C%20we%0Ademonstrate%20the%20applicability%20of%20our%20method%20to%20real-world%20MAVs%2C%20one%20equipped%0Awith%20a%20LiDAR%20and%20the%20other%20with%20a%20depth%20camera.%20Video%20available%20at%0Ahttps%3A//youtu.be/Uf5fwmYcuq4%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16972v1&entry.124074799=Read"},
{"title": "Models Can and Should Embrace the Communicative Nature of\n  Human-Generated Math", "author": "Sasha Boguraev and Ben Lipkin and Leonie Weissweiler and Kyle Mahowald", "abstract": "  Math is constructed by people for people: just as natural language corpora\nreflect not just propositions but the communicative goals of language users,\nthe math data that models are trained on reflects not just idealized\nmathematical entities but rich communicative intentions. While there are\nimportant advantages to treating math in a purely symbolic manner, we here\nhypothesize that there are benefits to treating math as situated linguistic\ncommunication and that language models are well suited for this goal, in ways\nthat are not fully appreciated. We illustrate these points with two case\nstudies. First, we ran an experiment in which we found that language models\ninterpret the equals sign in a humanlike way -- generating systematically\ndifferent word problems for the same underlying equation arranged in different\nways. Second, we found that language models prefer proofs to be ordered in\nnaturalistic ways, even though other orders would be logically equivalent. We\nadvocate for AI systems that learn from and represent the communicative\nintentions latent in human-generated math.\n", "link": "http://arxiv.org/abs/2409.17005v1", "date": "2024-09-25", "relevancy": 1.9133, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4865}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4767}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Models%20Can%20and%20Should%20Embrace%20the%20Communicative%20Nature%20of%0A%20%20Human-Generated%20Math&body=Title%3A%20Models%20Can%20and%20Should%20Embrace%20the%20Communicative%20Nature%20of%0A%20%20Human-Generated%20Math%0AAuthor%3A%20Sasha%20Boguraev%20and%20Ben%20Lipkin%20and%20Leonie%20Weissweiler%20and%20Kyle%20Mahowald%0AAbstract%3A%20%20%20Math%20is%20constructed%20by%20people%20for%20people%3A%20just%20as%20natural%20language%20corpora%0Areflect%20not%20just%20propositions%20but%20the%20communicative%20goals%20of%20language%20users%2C%0Athe%20math%20data%20that%20models%20are%20trained%20on%20reflects%20not%20just%20idealized%0Amathematical%20entities%20but%20rich%20communicative%20intentions.%20While%20there%20are%0Aimportant%20advantages%20to%20treating%20math%20in%20a%20purely%20symbolic%20manner%2C%20we%20here%0Ahypothesize%20that%20there%20are%20benefits%20to%20treating%20math%20as%20situated%20linguistic%0Acommunication%20and%20that%20language%20models%20are%20well%20suited%20for%20this%20goal%2C%20in%20ways%0Athat%20are%20not%20fully%20appreciated.%20We%20illustrate%20these%20points%20with%20two%20case%0Astudies.%20First%2C%20we%20ran%20an%20experiment%20in%20which%20we%20found%20that%20language%20models%0Ainterpret%20the%20equals%20sign%20in%20a%20humanlike%20way%20--%20generating%20systematically%0Adifferent%20word%20problems%20for%20the%20same%20underlying%20equation%20arranged%20in%20different%0Aways.%20Second%2C%20we%20found%20that%20language%20models%20prefer%20proofs%20to%20be%20ordered%20in%0Anaturalistic%20ways%2C%20even%20though%20other%20orders%20would%20be%20logically%20equivalent.%20We%0Aadvocate%20for%20AI%20systems%20that%20learn%20from%20and%20represent%20the%20communicative%0Aintentions%20latent%20in%20human-generated%20math.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17005v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModels%2520Can%2520and%2520Should%2520Embrace%2520the%2520Communicative%2520Nature%2520of%250A%2520%2520Human-Generated%2520Math%26entry.906535625%3DSasha%2520Boguraev%2520and%2520Ben%2520Lipkin%2520and%2520Leonie%2520Weissweiler%2520and%2520Kyle%2520Mahowald%26entry.1292438233%3D%2520%2520Math%2520is%2520constructed%2520by%2520people%2520for%2520people%253A%2520just%2520as%2520natural%2520language%2520corpora%250Areflect%2520not%2520just%2520propositions%2520but%2520the%2520communicative%2520goals%2520of%2520language%2520users%252C%250Athe%2520math%2520data%2520that%2520models%2520are%2520trained%2520on%2520reflects%2520not%2520just%2520idealized%250Amathematical%2520entities%2520but%2520rich%2520communicative%2520intentions.%2520While%2520there%2520are%250Aimportant%2520advantages%2520to%2520treating%2520math%2520in%2520a%2520purely%2520symbolic%2520manner%252C%2520we%2520here%250Ahypothesize%2520that%2520there%2520are%2520benefits%2520to%2520treating%2520math%2520as%2520situated%2520linguistic%250Acommunication%2520and%2520that%2520language%2520models%2520are%2520well%2520suited%2520for%2520this%2520goal%252C%2520in%2520ways%250Athat%2520are%2520not%2520fully%2520appreciated.%2520We%2520illustrate%2520these%2520points%2520with%2520two%2520case%250Astudies.%2520First%252C%2520we%2520ran%2520an%2520experiment%2520in%2520which%2520we%2520found%2520that%2520language%2520models%250Ainterpret%2520the%2520equals%2520sign%2520in%2520a%2520humanlike%2520way%2520--%2520generating%2520systematically%250Adifferent%2520word%2520problems%2520for%2520the%2520same%2520underlying%2520equation%2520arranged%2520in%2520different%250Aways.%2520Second%252C%2520we%2520found%2520that%2520language%2520models%2520prefer%2520proofs%2520to%2520be%2520ordered%2520in%250Anaturalistic%2520ways%252C%2520even%2520though%2520other%2520orders%2520would%2520be%2520logically%2520equivalent.%2520We%250Aadvocate%2520for%2520AI%2520systems%2520that%2520learn%2520from%2520and%2520represent%2520the%2520communicative%250Aintentions%2520latent%2520in%2520human-generated%2520math.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17005v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Models%20Can%20and%20Should%20Embrace%20the%20Communicative%20Nature%20of%0A%20%20Human-Generated%20Math&entry.906535625=Sasha%20Boguraev%20and%20Ben%20Lipkin%20and%20Leonie%20Weissweiler%20and%20Kyle%20Mahowald&entry.1292438233=%20%20Math%20is%20constructed%20by%20people%20for%20people%3A%20just%20as%20natural%20language%20corpora%0Areflect%20not%20just%20propositions%20but%20the%20communicative%20goals%20of%20language%20users%2C%0Athe%20math%20data%20that%20models%20are%20trained%20on%20reflects%20not%20just%20idealized%0Amathematical%20entities%20but%20rich%20communicative%20intentions.%20While%20there%20are%0Aimportant%20advantages%20to%20treating%20math%20in%20a%20purely%20symbolic%20manner%2C%20we%20here%0Ahypothesize%20that%20there%20are%20benefits%20to%20treating%20math%20as%20situated%20linguistic%0Acommunication%20and%20that%20language%20models%20are%20well%20suited%20for%20this%20goal%2C%20in%20ways%0Athat%20are%20not%20fully%20appreciated.%20We%20illustrate%20these%20points%20with%20two%20case%0Astudies.%20First%2C%20we%20ran%20an%20experiment%20in%20which%20we%20found%20that%20language%20models%0Ainterpret%20the%20equals%20sign%20in%20a%20humanlike%20way%20--%20generating%20systematically%0Adifferent%20word%20problems%20for%20the%20same%20underlying%20equation%20arranged%20in%20different%0Aways.%20Second%2C%20we%20found%20that%20language%20models%20prefer%20proofs%20to%20be%20ordered%20in%0Anaturalistic%20ways%2C%20even%20though%20other%20orders%20would%20be%20logically%20equivalent.%20We%0Aadvocate%20for%20AI%20systems%20that%20learn%20from%20and%20represent%20the%20communicative%0Aintentions%20latent%20in%20human-generated%20math.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17005v1&entry.124074799=Read"},
{"title": "Evaluating Usability and Engagement of Large Language Models in Virtual\n  Reality for Traditional Scottish Curling", "author": "Ka Hei Carrie Lau and Efe Bozkir and Hong Gao and Enkelejda Kasneci", "abstract": "  This paper explores the innovative application of Large Language Models\n(LLMs) in Virtual Reality (VR) environments to promote heritage education,\nfocusing on traditional Scottish curling presented in the game ``Scottish\nBonspiel VR''. Our study compares the effectiveness of LLM-based chatbots with\npre-defined scripted chatbots, evaluating key criteria such as usability, user\nengagement, and learning outcomes. The results show that LLM-based chatbots\nsignificantly improve interactivity and engagement, creating a more dynamic and\nimmersive learning environment. This integration helps document and preserve\ncultural heritage and enhances dissemination processes, which are crucial for\nsafeguarding intangible cultural heritage (ICH) amid environmental changes.\nFurthermore, the study highlights the potential of novel technologies in\neducation to provide immersive experiences that foster a deeper appreciation of\ncultural heritage. These findings support the wider application of LLMs and VR\nin cultural education to address global challenges and promote sustainable\npractices to preserve and enhance cultural heritage.\n", "link": "http://arxiv.org/abs/2408.09285v2", "date": "2024-09-25", "relevancy": 1.9064, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4784}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4784}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4674}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Usability%20and%20Engagement%20of%20Large%20Language%20Models%20in%20Virtual%0A%20%20Reality%20for%20Traditional%20Scottish%20Curling&body=Title%3A%20Evaluating%20Usability%20and%20Engagement%20of%20Large%20Language%20Models%20in%20Virtual%0A%20%20Reality%20for%20Traditional%20Scottish%20Curling%0AAuthor%3A%20Ka%20Hei%20Carrie%20Lau%20and%20Efe%20Bozkir%20and%20Hong%20Gao%20and%20Enkelejda%20Kasneci%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20innovative%20application%20of%20Large%20Language%20Models%0A%28LLMs%29%20in%20Virtual%20Reality%20%28VR%29%20environments%20to%20promote%20heritage%20education%2C%0Afocusing%20on%20traditional%20Scottish%20curling%20presented%20in%20the%20game%20%60%60Scottish%0ABonspiel%20VR%27%27.%20Our%20study%20compares%20the%20effectiveness%20of%20LLM-based%20chatbots%20with%0Apre-defined%20scripted%20chatbots%2C%20evaluating%20key%20criteria%20such%20as%20usability%2C%20user%0Aengagement%2C%20and%20learning%20outcomes.%20The%20results%20show%20that%20LLM-based%20chatbots%0Asignificantly%20improve%20interactivity%20and%20engagement%2C%20creating%20a%20more%20dynamic%20and%0Aimmersive%20learning%20environment.%20This%20integration%20helps%20document%20and%20preserve%0Acultural%20heritage%20and%20enhances%20dissemination%20processes%2C%20which%20are%20crucial%20for%0Asafeguarding%20intangible%20cultural%20heritage%20%28ICH%29%20amid%20environmental%20changes.%0AFurthermore%2C%20the%20study%20highlights%20the%20potential%20of%20novel%20technologies%20in%0Aeducation%20to%20provide%20immersive%20experiences%20that%20foster%20a%20deeper%20appreciation%20of%0Acultural%20heritage.%20These%20findings%20support%20the%20wider%20application%20of%20LLMs%20and%20VR%0Ain%20cultural%20education%20to%20address%20global%20challenges%20and%20promote%20sustainable%0Apractices%20to%20preserve%20and%20enhance%20cultural%20heritage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09285v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Usability%2520and%2520Engagement%2520of%2520Large%2520Language%2520Models%2520in%2520Virtual%250A%2520%2520Reality%2520for%2520Traditional%2520Scottish%2520Curling%26entry.906535625%3DKa%2520Hei%2520Carrie%2520Lau%2520and%2520Efe%2520Bozkir%2520and%2520Hong%2520Gao%2520and%2520Enkelejda%2520Kasneci%26entry.1292438233%3D%2520%2520This%2520paper%2520explores%2520the%2520innovative%2520application%2520of%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520in%2520Virtual%2520Reality%2520%2528VR%2529%2520environments%2520to%2520promote%2520heritage%2520education%252C%250Afocusing%2520on%2520traditional%2520Scottish%2520curling%2520presented%2520in%2520the%2520game%2520%2560%2560Scottish%250ABonspiel%2520VR%2527%2527.%2520Our%2520study%2520compares%2520the%2520effectiveness%2520of%2520LLM-based%2520chatbots%2520with%250Apre-defined%2520scripted%2520chatbots%252C%2520evaluating%2520key%2520criteria%2520such%2520as%2520usability%252C%2520user%250Aengagement%252C%2520and%2520learning%2520outcomes.%2520The%2520results%2520show%2520that%2520LLM-based%2520chatbots%250Asignificantly%2520improve%2520interactivity%2520and%2520engagement%252C%2520creating%2520a%2520more%2520dynamic%2520and%250Aimmersive%2520learning%2520environment.%2520This%2520integration%2520helps%2520document%2520and%2520preserve%250Acultural%2520heritage%2520and%2520enhances%2520dissemination%2520processes%252C%2520which%2520are%2520crucial%2520for%250Asafeguarding%2520intangible%2520cultural%2520heritage%2520%2528ICH%2529%2520amid%2520environmental%2520changes.%250AFurthermore%252C%2520the%2520study%2520highlights%2520the%2520potential%2520of%2520novel%2520technologies%2520in%250Aeducation%2520to%2520provide%2520immersive%2520experiences%2520that%2520foster%2520a%2520deeper%2520appreciation%2520of%250Acultural%2520heritage.%2520These%2520findings%2520support%2520the%2520wider%2520application%2520of%2520LLMs%2520and%2520VR%250Ain%2520cultural%2520education%2520to%2520address%2520global%2520challenges%2520and%2520promote%2520sustainable%250Apractices%2520to%2520preserve%2520and%2520enhance%2520cultural%2520heritage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09285v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Usability%20and%20Engagement%20of%20Large%20Language%20Models%20in%20Virtual%0A%20%20Reality%20for%20Traditional%20Scottish%20Curling&entry.906535625=Ka%20Hei%20Carrie%20Lau%20and%20Efe%20Bozkir%20and%20Hong%20Gao%20and%20Enkelejda%20Kasneci&entry.1292438233=%20%20This%20paper%20explores%20the%20innovative%20application%20of%20Large%20Language%20Models%0A%28LLMs%29%20in%20Virtual%20Reality%20%28VR%29%20environments%20to%20promote%20heritage%20education%2C%0Afocusing%20on%20traditional%20Scottish%20curling%20presented%20in%20the%20game%20%60%60Scottish%0ABonspiel%20VR%27%27.%20Our%20study%20compares%20the%20effectiveness%20of%20LLM-based%20chatbots%20with%0Apre-defined%20scripted%20chatbots%2C%20evaluating%20key%20criteria%20such%20as%20usability%2C%20user%0Aengagement%2C%20and%20learning%20outcomes.%20The%20results%20show%20that%20LLM-based%20chatbots%0Asignificantly%20improve%20interactivity%20and%20engagement%2C%20creating%20a%20more%20dynamic%20and%0Aimmersive%20learning%20environment.%20This%20integration%20helps%20document%20and%20preserve%0Acultural%20heritage%20and%20enhances%20dissemination%20processes%2C%20which%20are%20crucial%20for%0Asafeguarding%20intangible%20cultural%20heritage%20%28ICH%29%20amid%20environmental%20changes.%0AFurthermore%2C%20the%20study%20highlights%20the%20potential%20of%20novel%20technologies%20in%0Aeducation%20to%20provide%20immersive%20experiences%20that%20foster%20a%20deeper%20appreciation%20of%0Acultural%20heritage.%20These%20findings%20support%20the%20wider%20application%20of%20LLMs%20and%20VR%0Ain%20cultural%20education%20to%20address%20global%20challenges%20and%20promote%20sustainable%0Apractices%20to%20preserve%20and%20enhance%20cultural%20heritage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09285v2&entry.124074799=Read"},
{"title": "Large Language Model Predicts Above Normal All India Summer Monsoon\n  Rainfall in 2024", "author": "Ujjawal Sharma and Madhav Biyani and Akhil Dev Suresh and Debi Prasad Bhuyan and Saroj Kanta Mishra and Tanmoy Chakraborty", "abstract": "  Reliable prediction of the All India Summer Monsoon Rainfall (AISMR) is\npivotal for informed policymaking for the country, impacting the lives of\nbillions of people. However, accurate simulation of AISMR has been a persistent\nchallenge due to the complex interplay of various muti-scale factors and the\ninherent variability of the monsoon system. This research focuses on adapting\nand fine-tuning the latest LLM model, PatchTST, to accurately predict AISMR\nwith a lead time of three months. The fine-tuned PatchTST model, trained with\nhistorical AISMR data, the Ni\\~no3.4 index, and categorical Indian Ocean Dipole\nvalues, outperforms several popular neural network models and statistical\nmodels. This fine-tuned LLM model exhibits an exceptionally low RMSE percentage\nof 0.07% and a Spearman correlation of 0.976. This is particularly impressive,\nsince it is nearly 80% more accurate than the best-performing NN models. The\nmodel predicts an above-normal monsoon for the year 2024, with an accumulated\nrainfall of 921.6 mm in the month of June-September for the entire country.\n", "link": "http://arxiv.org/abs/2409.16799v1", "date": "2024-09-25", "relevancy": 0.7688, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3901}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.383}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.38}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Model%20Predicts%20Above%20Normal%20All%20India%20Summer%20Monsoon%0A%20%20Rainfall%20in%202024&body=Title%3A%20Large%20Language%20Model%20Predicts%20Above%20Normal%20All%20India%20Summer%20Monsoon%0A%20%20Rainfall%20in%202024%0AAuthor%3A%20Ujjawal%20Sharma%20and%20Madhav%20Biyani%20and%20Akhil%20Dev%20Suresh%20and%20Debi%20Prasad%20Bhuyan%20and%20Saroj%20Kanta%20Mishra%20and%20Tanmoy%20Chakraborty%0AAbstract%3A%20%20%20Reliable%20prediction%20of%20the%20All%20India%20Summer%20Monsoon%20Rainfall%20%28AISMR%29%20is%0Apivotal%20for%20informed%20policymaking%20for%20the%20country%2C%20impacting%20the%20lives%20of%0Abillions%20of%20people.%20However%2C%20accurate%20simulation%20of%20AISMR%20has%20been%20a%20persistent%0Achallenge%20due%20to%20the%20complex%20interplay%20of%20various%20muti-scale%20factors%20and%20the%0Ainherent%20variability%20of%20the%20monsoon%20system.%20This%20research%20focuses%20on%20adapting%0Aand%20fine-tuning%20the%20latest%20LLM%20model%2C%20PatchTST%2C%20to%20accurately%20predict%20AISMR%0Awith%20a%20lead%20time%20of%20three%20months.%20The%20fine-tuned%20PatchTST%20model%2C%20trained%20with%0Ahistorical%20AISMR%20data%2C%20the%20Ni%5C~no3.4%20index%2C%20and%20categorical%20Indian%20Ocean%20Dipole%0Avalues%2C%20outperforms%20several%20popular%20neural%20network%20models%20and%20statistical%0Amodels.%20This%20fine-tuned%20LLM%20model%20exhibits%20an%20exceptionally%20low%20RMSE%20percentage%0Aof%200.07%25%20and%20a%20Spearman%20correlation%20of%200.976.%20This%20is%20particularly%20impressive%2C%0Asince%20it%20is%20nearly%2080%25%20more%20accurate%20than%20the%20best-performing%20NN%20models.%20The%0Amodel%20predicts%20an%20above-normal%20monsoon%20for%20the%20year%202024%2C%20with%20an%20accumulated%0Arainfall%20of%20921.6%20mm%20in%20the%20month%20of%20June-September%20for%20the%20entire%20country.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Model%2520Predicts%2520Above%2520Normal%2520All%2520India%2520Summer%2520Monsoon%250A%2520%2520Rainfall%2520in%25202024%26entry.906535625%3DUjjawal%2520Sharma%2520and%2520Madhav%2520Biyani%2520and%2520Akhil%2520Dev%2520Suresh%2520and%2520Debi%2520Prasad%2520Bhuyan%2520and%2520Saroj%2520Kanta%2520Mishra%2520and%2520Tanmoy%2520Chakraborty%26entry.1292438233%3D%2520%2520Reliable%2520prediction%2520of%2520the%2520All%2520India%2520Summer%2520Monsoon%2520Rainfall%2520%2528AISMR%2529%2520is%250Apivotal%2520for%2520informed%2520policymaking%2520for%2520the%2520country%252C%2520impacting%2520the%2520lives%2520of%250Abillions%2520of%2520people.%2520However%252C%2520accurate%2520simulation%2520of%2520AISMR%2520has%2520been%2520a%2520persistent%250Achallenge%2520due%2520to%2520the%2520complex%2520interplay%2520of%2520various%2520muti-scale%2520factors%2520and%2520the%250Ainherent%2520variability%2520of%2520the%2520monsoon%2520system.%2520This%2520research%2520focuses%2520on%2520adapting%250Aand%2520fine-tuning%2520the%2520latest%2520LLM%2520model%252C%2520PatchTST%252C%2520to%2520accurately%2520predict%2520AISMR%250Awith%2520a%2520lead%2520time%2520of%2520three%2520months.%2520The%2520fine-tuned%2520PatchTST%2520model%252C%2520trained%2520with%250Ahistorical%2520AISMR%2520data%252C%2520the%2520Ni%255C~no3.4%2520index%252C%2520and%2520categorical%2520Indian%2520Ocean%2520Dipole%250Avalues%252C%2520outperforms%2520several%2520popular%2520neural%2520network%2520models%2520and%2520statistical%250Amodels.%2520This%2520fine-tuned%2520LLM%2520model%2520exhibits%2520an%2520exceptionally%2520low%2520RMSE%2520percentage%250Aof%25200.07%2525%2520and%2520a%2520Spearman%2520correlation%2520of%25200.976.%2520This%2520is%2520particularly%2520impressive%252C%250Asince%2520it%2520is%2520nearly%252080%2525%2520more%2520accurate%2520than%2520the%2520best-performing%2520NN%2520models.%2520The%250Amodel%2520predicts%2520an%2520above-normal%2520monsoon%2520for%2520the%2520year%25202024%252C%2520with%2520an%2520accumulated%250Arainfall%2520of%2520921.6%2520mm%2520in%2520the%2520month%2520of%2520June-September%2520for%2520the%2520entire%2520country.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Model%20Predicts%20Above%20Normal%20All%20India%20Summer%20Monsoon%0A%20%20Rainfall%20in%202024&entry.906535625=Ujjawal%20Sharma%20and%20Madhav%20Biyani%20and%20Akhil%20Dev%20Suresh%20and%20Debi%20Prasad%20Bhuyan%20and%20Saroj%20Kanta%20Mishra%20and%20Tanmoy%20Chakraborty&entry.1292438233=%20%20Reliable%20prediction%20of%20the%20All%20India%20Summer%20Monsoon%20Rainfall%20%28AISMR%29%20is%0Apivotal%20for%20informed%20policymaking%20for%20the%20country%2C%20impacting%20the%20lives%20of%0Abillions%20of%20people.%20However%2C%20accurate%20simulation%20of%20AISMR%20has%20been%20a%20persistent%0Achallenge%20due%20to%20the%20complex%20interplay%20of%20various%20muti-scale%20factors%20and%20the%0Ainherent%20variability%20of%20the%20monsoon%20system.%20This%20research%20focuses%20on%20adapting%0Aand%20fine-tuning%20the%20latest%20LLM%20model%2C%20PatchTST%2C%20to%20accurately%20predict%20AISMR%0Awith%20a%20lead%20time%20of%20three%20months.%20The%20fine-tuned%20PatchTST%20model%2C%20trained%20with%0Ahistorical%20AISMR%20data%2C%20the%20Ni%5C~no3.4%20index%2C%20and%20categorical%20Indian%20Ocean%20Dipole%0Avalues%2C%20outperforms%20several%20popular%20neural%20network%20models%20and%20statistical%0Amodels.%20This%20fine-tuned%20LLM%20model%20exhibits%20an%20exceptionally%20low%20RMSE%20percentage%0Aof%200.07%25%20and%20a%20Spearman%20correlation%20of%200.976.%20This%20is%20particularly%20impressive%2C%0Asince%20it%20is%20nearly%2080%25%20more%20accurate%20than%20the%20best-performing%20NN%20models.%20The%0Amodel%20predicts%20an%20above-normal%20monsoon%20for%20the%20year%202024%2C%20with%20an%20accumulated%0Arainfall%20of%20921.6%20mm%20in%20the%20month%20of%20June-September%20for%20the%20entire%20country.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16799v1&entry.124074799=Read"},
{"title": "Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey", "author": "Jinghong Li and Huy Phan and Wen Gu and Koichi Ota and Shinobu Hasegawa", "abstract": "  Research surveys have always posed a challenge for beginner researchers who\nlack of research training. These researchers struggle to understand the\ndirections within their research topic, and the discovery of new research\nfindings within a short time. One way to provide intuitive assistance to\nbeginner researchers is by offering relevant knowledge graphs(KG) and\nrecommending related academic papers. However, existing navigation knowledge\ngraphs primarily rely on keywords in the research field and often fail to\npresent the logical hierarchy among multiple related papers clearly. Moreover,\nmost recommendation systems for academic papers simply rely on high text\nsimilarity, which can leave researchers confused as to why a particular article\nis being recommended. They may lack of grasp important information about the\ninsight connection between \"Issue resolved\" and \"Issue finding\" that they hope\nto obtain. To address these issues, this study aims to support research insight\nsurveys for beginner researchers by establishing a hierarchical tree-structured\nknowledge graph that reflects the inheritance insight of research topics and\nthe relevance insight among the academic papers.\n", "link": "http://arxiv.org/abs/2402.04854v7", "date": "2024-09-25", "relevancy": 1.673, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4451}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4186}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4072}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Tree-structured%20Knowledge%20Graph%20For%20Academic%20Insight%20Survey&body=Title%3A%20Hierarchical%20Tree-structured%20Knowledge%20Graph%20For%20Academic%20Insight%20Survey%0AAuthor%3A%20Jinghong%20Li%20and%20Huy%20Phan%20and%20Wen%20Gu%20and%20Koichi%20Ota%20and%20Shinobu%20Hasegawa%0AAbstract%3A%20%20%20Research%20surveys%20have%20always%20posed%20a%20challenge%20for%20beginner%20researchers%20who%0Alack%20of%20research%20training.%20These%20researchers%20struggle%20to%20understand%20the%0Adirections%20within%20their%20research%20topic%2C%20and%20the%20discovery%20of%20new%20research%0Afindings%20within%20a%20short%20time.%20One%20way%20to%20provide%20intuitive%20assistance%20to%0Abeginner%20researchers%20is%20by%20offering%20relevant%20knowledge%20graphs%28KG%29%20and%0Arecommending%20related%20academic%20papers.%20However%2C%20existing%20navigation%20knowledge%0Agraphs%20primarily%20rely%20on%20keywords%20in%20the%20research%20field%20and%20often%20fail%20to%0Apresent%20the%20logical%20hierarchy%20among%20multiple%20related%20papers%20clearly.%20Moreover%2C%0Amost%20recommendation%20systems%20for%20academic%20papers%20simply%20rely%20on%20high%20text%0Asimilarity%2C%20which%20can%20leave%20researchers%20confused%20as%20to%20why%20a%20particular%20article%0Ais%20being%20recommended.%20They%20may%20lack%20of%20grasp%20important%20information%20about%20the%0Ainsight%20connection%20between%20%22Issue%20resolved%22%20and%20%22Issue%20finding%22%20that%20they%20hope%0Ato%20obtain.%20To%20address%20these%20issues%2C%20this%20study%20aims%20to%20support%20research%20insight%0Asurveys%20for%20beginner%20researchers%20by%20establishing%20a%20hierarchical%20tree-structured%0Aknowledge%20graph%20that%20reflects%20the%20inheritance%20insight%20of%20research%20topics%20and%0Athe%20relevance%20insight%20among%20the%20academic%20papers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04854v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Tree-structured%2520Knowledge%2520Graph%2520For%2520Academic%2520Insight%2520Survey%26entry.906535625%3DJinghong%2520Li%2520and%2520Huy%2520Phan%2520and%2520Wen%2520Gu%2520and%2520Koichi%2520Ota%2520and%2520Shinobu%2520Hasegawa%26entry.1292438233%3D%2520%2520Research%2520surveys%2520have%2520always%2520posed%2520a%2520challenge%2520for%2520beginner%2520researchers%2520who%250Alack%2520of%2520research%2520training.%2520These%2520researchers%2520struggle%2520to%2520understand%2520the%250Adirections%2520within%2520their%2520research%2520topic%252C%2520and%2520the%2520discovery%2520of%2520new%2520research%250Afindings%2520within%2520a%2520short%2520time.%2520One%2520way%2520to%2520provide%2520intuitive%2520assistance%2520to%250Abeginner%2520researchers%2520is%2520by%2520offering%2520relevant%2520knowledge%2520graphs%2528KG%2529%2520and%250Arecommending%2520related%2520academic%2520papers.%2520However%252C%2520existing%2520navigation%2520knowledge%250Agraphs%2520primarily%2520rely%2520on%2520keywords%2520in%2520the%2520research%2520field%2520and%2520often%2520fail%2520to%250Apresent%2520the%2520logical%2520hierarchy%2520among%2520multiple%2520related%2520papers%2520clearly.%2520Moreover%252C%250Amost%2520recommendation%2520systems%2520for%2520academic%2520papers%2520simply%2520rely%2520on%2520high%2520text%250Asimilarity%252C%2520which%2520can%2520leave%2520researchers%2520confused%2520as%2520to%2520why%2520a%2520particular%2520article%250Ais%2520being%2520recommended.%2520They%2520may%2520lack%2520of%2520grasp%2520important%2520information%2520about%2520the%250Ainsight%2520connection%2520between%2520%2522Issue%2520resolved%2522%2520and%2520%2522Issue%2520finding%2522%2520that%2520they%2520hope%250Ato%2520obtain.%2520To%2520address%2520these%2520issues%252C%2520this%2520study%2520aims%2520to%2520support%2520research%2520insight%250Asurveys%2520for%2520beginner%2520researchers%2520by%2520establishing%2520a%2520hierarchical%2520tree-structured%250Aknowledge%2520graph%2520that%2520reflects%2520the%2520inheritance%2520insight%2520of%2520research%2520topics%2520and%250Athe%2520relevance%2520insight%2520among%2520the%2520academic%2520papers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04854v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Tree-structured%20Knowledge%20Graph%20For%20Academic%20Insight%20Survey&entry.906535625=Jinghong%20Li%20and%20Huy%20Phan%20and%20Wen%20Gu%20and%20Koichi%20Ota%20and%20Shinobu%20Hasegawa&entry.1292438233=%20%20Research%20surveys%20have%20always%20posed%20a%20challenge%20for%20beginner%20researchers%20who%0Alack%20of%20research%20training.%20These%20researchers%20struggle%20to%20understand%20the%0Adirections%20within%20their%20research%20topic%2C%20and%20the%20discovery%20of%20new%20research%0Afindings%20within%20a%20short%20time.%20One%20way%20to%20provide%20intuitive%20assistance%20to%0Abeginner%20researchers%20is%20by%20offering%20relevant%20knowledge%20graphs%28KG%29%20and%0Arecommending%20related%20academic%20papers.%20However%2C%20existing%20navigation%20knowledge%0Agraphs%20primarily%20rely%20on%20keywords%20in%20the%20research%20field%20and%20often%20fail%20to%0Apresent%20the%20logical%20hierarchy%20among%20multiple%20related%20papers%20clearly.%20Moreover%2C%0Amost%20recommendation%20systems%20for%20academic%20papers%20simply%20rely%20on%20high%20text%0Asimilarity%2C%20which%20can%20leave%20researchers%20confused%20as%20to%20why%20a%20particular%20article%0Ais%20being%20recommended.%20They%20may%20lack%20of%20grasp%20important%20information%20about%20the%0Ainsight%20connection%20between%20%22Issue%20resolved%22%20and%20%22Issue%20finding%22%20that%20they%20hope%0Ato%20obtain.%20To%20address%20these%20issues%2C%20this%20study%20aims%20to%20support%20research%20insight%0Asurveys%20for%20beginner%20researchers%20by%20establishing%20a%20hierarchical%20tree-structured%0Aknowledge%20graph%20that%20reflects%20the%20inheritance%20insight%20of%20research%20topics%20and%0Athe%20relevance%20insight%20among%20the%20academic%20papers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04854v7&entry.124074799=Read"},
{"title": "Performance assessment of ADAS in a representative subset of critical\n  traffic situations", "author": "Luigi Di Lillo and Andrea Triscari and Xilin Zhou and Robert Dyro and Ruolin Li and Marco Pavone", "abstract": "  As a variety of automated collision prevention systems gain presence within\npersonal vehicles, rating and differentiating the automated safety performance\nof car models has become increasingly important for consumers, manufacturers,\nand insurers. In 2023, Swiss Re and partners initiated an eight-month long\nvehicle testing campaign conducted on a recognized UNECE type approval\nauthority and Euro NCAP accredited proving ground in Germany. The campaign\nexposed twelve mass-produced vehicle models and one prototype vehicle fitted\nwith collision prevention systems to a selection of safety-critical traffic\nscenarios representative of United States and European Union accident\nlandscape. In this paper, we compare and evaluate the relative safety\nperformance of these thirteen collision prevention systems (hardware and\nsoftware stack) as demonstrated by this testing campaign. We first introduce a\nnew scoring system which represents a test system's predicted impact on overall\nreal-world collision frequency and reduction of collision impact energy,\nweighted based on the real-world relevance of the test scenario. Next, we\nintroduce a novel metric that quantifies the realism of the protocol and\nconfirm that our test protocol is a plausible representation of real-world\ndriving. Finally, we find that the prototype system in its pre-release state\noutperforms the mass-produced (post-consumer-release) vehicles in the majority\nof the tested scenarios on the test track.\n", "link": "http://arxiv.org/abs/2409.16942v1", "date": "2024-09-25", "relevancy": 1.7689, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.464}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4443}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Performance%20assessment%20of%20ADAS%20in%20a%20representative%20subset%20of%20critical%0A%20%20traffic%20situations&body=Title%3A%20Performance%20assessment%20of%20ADAS%20in%20a%20representative%20subset%20of%20critical%0A%20%20traffic%20situations%0AAuthor%3A%20Luigi%20Di%20Lillo%20and%20Andrea%20Triscari%20and%20Xilin%20Zhou%20and%20Robert%20Dyro%20and%20Ruolin%20Li%20and%20Marco%20Pavone%0AAbstract%3A%20%20%20As%20a%20variety%20of%20automated%20collision%20prevention%20systems%20gain%20presence%20within%0Apersonal%20vehicles%2C%20rating%20and%20differentiating%20the%20automated%20safety%20performance%0Aof%20car%20models%20has%20become%20increasingly%20important%20for%20consumers%2C%20manufacturers%2C%0Aand%20insurers.%20In%202023%2C%20Swiss%20Re%20and%20partners%20initiated%20an%20eight-month%20long%0Avehicle%20testing%20campaign%20conducted%20on%20a%20recognized%20UNECE%20type%20approval%0Aauthority%20and%20Euro%20NCAP%20accredited%20proving%20ground%20in%20Germany.%20The%20campaign%0Aexposed%20twelve%20mass-produced%20vehicle%20models%20and%20one%20prototype%20vehicle%20fitted%0Awith%20collision%20prevention%20systems%20to%20a%20selection%20of%20safety-critical%20traffic%0Ascenarios%20representative%20of%20United%20States%20and%20European%20Union%20accident%0Alandscape.%20In%20this%20paper%2C%20we%20compare%20and%20evaluate%20the%20relative%20safety%0Aperformance%20of%20these%20thirteen%20collision%20prevention%20systems%20%28hardware%20and%0Asoftware%20stack%29%20as%20demonstrated%20by%20this%20testing%20campaign.%20We%20first%20introduce%20a%0Anew%20scoring%20system%20which%20represents%20a%20test%20system%27s%20predicted%20impact%20on%20overall%0Areal-world%20collision%20frequency%20and%20reduction%20of%20collision%20impact%20energy%2C%0Aweighted%20based%20on%20the%20real-world%20relevance%20of%20the%20test%20scenario.%20Next%2C%20we%0Aintroduce%20a%20novel%20metric%20that%20quantifies%20the%20realism%20of%20the%20protocol%20and%0Aconfirm%20that%20our%20test%20protocol%20is%20a%20plausible%20representation%20of%20real-world%0Adriving.%20Finally%2C%20we%20find%20that%20the%20prototype%20system%20in%20its%20pre-release%20state%0Aoutperforms%20the%20mass-produced%20%28post-consumer-release%29%20vehicles%20in%20the%20majority%0Aof%20the%20tested%20scenarios%20on%20the%20test%20track.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerformance%2520assessment%2520of%2520ADAS%2520in%2520a%2520representative%2520subset%2520of%2520critical%250A%2520%2520traffic%2520situations%26entry.906535625%3DLuigi%2520Di%2520Lillo%2520and%2520Andrea%2520Triscari%2520and%2520Xilin%2520Zhou%2520and%2520Robert%2520Dyro%2520and%2520Ruolin%2520Li%2520and%2520Marco%2520Pavone%26entry.1292438233%3D%2520%2520As%2520a%2520variety%2520of%2520automated%2520collision%2520prevention%2520systems%2520gain%2520presence%2520within%250Apersonal%2520vehicles%252C%2520rating%2520and%2520differentiating%2520the%2520automated%2520safety%2520performance%250Aof%2520car%2520models%2520has%2520become%2520increasingly%2520important%2520for%2520consumers%252C%2520manufacturers%252C%250Aand%2520insurers.%2520In%25202023%252C%2520Swiss%2520Re%2520and%2520partners%2520initiated%2520an%2520eight-month%2520long%250Avehicle%2520testing%2520campaign%2520conducted%2520on%2520a%2520recognized%2520UNECE%2520type%2520approval%250Aauthority%2520and%2520Euro%2520NCAP%2520accredited%2520proving%2520ground%2520in%2520Germany.%2520The%2520campaign%250Aexposed%2520twelve%2520mass-produced%2520vehicle%2520models%2520and%2520one%2520prototype%2520vehicle%2520fitted%250Awith%2520collision%2520prevention%2520systems%2520to%2520a%2520selection%2520of%2520safety-critical%2520traffic%250Ascenarios%2520representative%2520of%2520United%2520States%2520and%2520European%2520Union%2520accident%250Alandscape.%2520In%2520this%2520paper%252C%2520we%2520compare%2520and%2520evaluate%2520the%2520relative%2520safety%250Aperformance%2520of%2520these%2520thirteen%2520collision%2520prevention%2520systems%2520%2528hardware%2520and%250Asoftware%2520stack%2529%2520as%2520demonstrated%2520by%2520this%2520testing%2520campaign.%2520We%2520first%2520introduce%2520a%250Anew%2520scoring%2520system%2520which%2520represents%2520a%2520test%2520system%2527s%2520predicted%2520impact%2520on%2520overall%250Areal-world%2520collision%2520frequency%2520and%2520reduction%2520of%2520collision%2520impact%2520energy%252C%250Aweighted%2520based%2520on%2520the%2520real-world%2520relevance%2520of%2520the%2520test%2520scenario.%2520Next%252C%2520we%250Aintroduce%2520a%2520novel%2520metric%2520that%2520quantifies%2520the%2520realism%2520of%2520the%2520protocol%2520and%250Aconfirm%2520that%2520our%2520test%2520protocol%2520is%2520a%2520plausible%2520representation%2520of%2520real-world%250Adriving.%2520Finally%252C%2520we%2520find%2520that%2520the%2520prototype%2520system%2520in%2520its%2520pre-release%2520state%250Aoutperforms%2520the%2520mass-produced%2520%2528post-consumer-release%2529%2520vehicles%2520in%2520the%2520majority%250Aof%2520the%2520tested%2520scenarios%2520on%2520the%2520test%2520track.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Performance%20assessment%20of%20ADAS%20in%20a%20representative%20subset%20of%20critical%0A%20%20traffic%20situations&entry.906535625=Luigi%20Di%20Lillo%20and%20Andrea%20Triscari%20and%20Xilin%20Zhou%20and%20Robert%20Dyro%20and%20Ruolin%20Li%20and%20Marco%20Pavone&entry.1292438233=%20%20As%20a%20variety%20of%20automated%20collision%20prevention%20systems%20gain%20presence%20within%0Apersonal%20vehicles%2C%20rating%20and%20differentiating%20the%20automated%20safety%20performance%0Aof%20car%20models%20has%20become%20increasingly%20important%20for%20consumers%2C%20manufacturers%2C%0Aand%20insurers.%20In%202023%2C%20Swiss%20Re%20and%20partners%20initiated%20an%20eight-month%20long%0Avehicle%20testing%20campaign%20conducted%20on%20a%20recognized%20UNECE%20type%20approval%0Aauthority%20and%20Euro%20NCAP%20accredited%20proving%20ground%20in%20Germany.%20The%20campaign%0Aexposed%20twelve%20mass-produced%20vehicle%20models%20and%20one%20prototype%20vehicle%20fitted%0Awith%20collision%20prevention%20systems%20to%20a%20selection%20of%20safety-critical%20traffic%0Ascenarios%20representative%20of%20United%20States%20and%20European%20Union%20accident%0Alandscape.%20In%20this%20paper%2C%20we%20compare%20and%20evaluate%20the%20relative%20safety%0Aperformance%20of%20these%20thirteen%20collision%20prevention%20systems%20%28hardware%20and%0Asoftware%20stack%29%20as%20demonstrated%20by%20this%20testing%20campaign.%20We%20first%20introduce%20a%0Anew%20scoring%20system%20which%20represents%20a%20test%20system%27s%20predicted%20impact%20on%20overall%0Areal-world%20collision%20frequency%20and%20reduction%20of%20collision%20impact%20energy%2C%0Aweighted%20based%20on%20the%20real-world%20relevance%20of%20the%20test%20scenario.%20Next%2C%20we%0Aintroduce%20a%20novel%20metric%20that%20quantifies%20the%20realism%20of%20the%20protocol%20and%0Aconfirm%20that%20our%20test%20protocol%20is%20a%20plausible%20representation%20of%20real-world%0Adriving.%20Finally%2C%20we%20find%20that%20the%20prototype%20system%20in%20its%20pre-release%20state%0Aoutperforms%20the%20mass-produced%20%28post-consumer-release%29%20vehicles%20in%20the%20majority%0Aof%20the%20tested%20scenarios%20on%20the%20test%20track.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16942v1&entry.124074799=Read"},
{"title": "Benchmarking Deep Learning Models for Object Detection on Edge Computing\n  Devices", "author": "Daghash K. Alqahtani and Aamir Cheema and Adel N. Toosi", "abstract": "  Modern applications, such as autonomous vehicles, require deploying deep\nlearning algorithms on resource-constrained edge devices for real-time image\nand video processing. However, there is limited understanding of the efficiency\nand performance of various object detection models on these devices. In this\npaper, we evaluate state-of-the-art object detection models, including YOLOv8\n(Nano, Small, Medium), EfficientDet Lite (Lite0, Lite1, Lite2), and SSD (SSD\nMobileNet V1, SSDLite MobileDet). We deployed these models on popular edge\ndevices like the Raspberry Pi 3, 4, and 5 with/without TPU accelerators, and\nJetson Orin Nano, collecting key performance metrics such as energy\nconsumption, inference time, and Mean Average Precision (mAP). Our findings\nhighlight that lower mAP models such as SSD MobileNet V1 are more\nenergy-efficient and faster in inference, whereas higher mAP models like YOLOv8\nMedium generally consume more energy and have slower inference, though with\nexceptions when accelerators like TPUs are used. Among the edge devices, Jetson\nOrin Nano stands out as the fastest and most energy-efficient option for\nrequest handling, despite having the highest idle energy consumption. These\nresults emphasize the need to balance accuracy, speed, and energy efficiency\nwhen deploying deep learning models on edge devices, offering valuable guidance\nfor practitioners and researchers selecting models and devices for their\napplications.\n", "link": "http://arxiv.org/abs/2409.16808v1", "date": "2024-09-25", "relevancy": 1.5112, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5488}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4913}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Deep%20Learning%20Models%20for%20Object%20Detection%20on%20Edge%20Computing%0A%20%20Devices&body=Title%3A%20Benchmarking%20Deep%20Learning%20Models%20for%20Object%20Detection%20on%20Edge%20Computing%0A%20%20Devices%0AAuthor%3A%20Daghash%20K.%20Alqahtani%20and%20Aamir%20Cheema%20and%20Adel%20N.%20Toosi%0AAbstract%3A%20%20%20Modern%20applications%2C%20such%20as%20autonomous%20vehicles%2C%20require%20deploying%20deep%0Alearning%20algorithms%20on%20resource-constrained%20edge%20devices%20for%20real-time%20image%0Aand%20video%20processing.%20However%2C%20there%20is%20limited%20understanding%20of%20the%20efficiency%0Aand%20performance%20of%20various%20object%20detection%20models%20on%20these%20devices.%20In%20this%0Apaper%2C%20we%20evaluate%20state-of-the-art%20object%20detection%20models%2C%20including%20YOLOv8%0A%28Nano%2C%20Small%2C%20Medium%29%2C%20EfficientDet%20Lite%20%28Lite0%2C%20Lite1%2C%20Lite2%29%2C%20and%20SSD%20%28SSD%0AMobileNet%20V1%2C%20SSDLite%20MobileDet%29.%20We%20deployed%20these%20models%20on%20popular%20edge%0Adevices%20like%20the%20Raspberry%20Pi%203%2C%204%2C%20and%205%20with/without%20TPU%20accelerators%2C%20and%0AJetson%20Orin%20Nano%2C%20collecting%20key%20performance%20metrics%20such%20as%20energy%0Aconsumption%2C%20inference%20time%2C%20and%20Mean%20Average%20Precision%20%28mAP%29.%20Our%20findings%0Ahighlight%20that%20lower%20mAP%20models%20such%20as%20SSD%20MobileNet%20V1%20are%20more%0Aenergy-efficient%20and%20faster%20in%20inference%2C%20whereas%20higher%20mAP%20models%20like%20YOLOv8%0AMedium%20generally%20consume%20more%20energy%20and%20have%20slower%20inference%2C%20though%20with%0Aexceptions%20when%20accelerators%20like%20TPUs%20are%20used.%20Among%20the%20edge%20devices%2C%20Jetson%0AOrin%20Nano%20stands%20out%20as%20the%20fastest%20and%20most%20energy-efficient%20option%20for%0Arequest%20handling%2C%20despite%20having%20the%20highest%20idle%20energy%20consumption.%20These%0Aresults%20emphasize%20the%20need%20to%20balance%20accuracy%2C%20speed%2C%20and%20energy%20efficiency%0Awhen%20deploying%20deep%20learning%20models%20on%20edge%20devices%2C%20offering%20valuable%20guidance%0Afor%20practitioners%20and%20researchers%20selecting%20models%20and%20devices%20for%20their%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16808v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Deep%2520Learning%2520Models%2520for%2520Object%2520Detection%2520on%2520Edge%2520Computing%250A%2520%2520Devices%26entry.906535625%3DDaghash%2520K.%2520Alqahtani%2520and%2520Aamir%2520Cheema%2520and%2520Adel%2520N.%2520Toosi%26entry.1292438233%3D%2520%2520Modern%2520applications%252C%2520such%2520as%2520autonomous%2520vehicles%252C%2520require%2520deploying%2520deep%250Alearning%2520algorithms%2520on%2520resource-constrained%2520edge%2520devices%2520for%2520real-time%2520image%250Aand%2520video%2520processing.%2520However%252C%2520there%2520is%2520limited%2520understanding%2520of%2520the%2520efficiency%250Aand%2520performance%2520of%2520various%2520object%2520detection%2520models%2520on%2520these%2520devices.%2520In%2520this%250Apaper%252C%2520we%2520evaluate%2520state-of-the-art%2520object%2520detection%2520models%252C%2520including%2520YOLOv8%250A%2528Nano%252C%2520Small%252C%2520Medium%2529%252C%2520EfficientDet%2520Lite%2520%2528Lite0%252C%2520Lite1%252C%2520Lite2%2529%252C%2520and%2520SSD%2520%2528SSD%250AMobileNet%2520V1%252C%2520SSDLite%2520MobileDet%2529.%2520We%2520deployed%2520these%2520models%2520on%2520popular%2520edge%250Adevices%2520like%2520the%2520Raspberry%2520Pi%25203%252C%25204%252C%2520and%25205%2520with/without%2520TPU%2520accelerators%252C%2520and%250AJetson%2520Orin%2520Nano%252C%2520collecting%2520key%2520performance%2520metrics%2520such%2520as%2520energy%250Aconsumption%252C%2520inference%2520time%252C%2520and%2520Mean%2520Average%2520Precision%2520%2528mAP%2529.%2520Our%2520findings%250Ahighlight%2520that%2520lower%2520mAP%2520models%2520such%2520as%2520SSD%2520MobileNet%2520V1%2520are%2520more%250Aenergy-efficient%2520and%2520faster%2520in%2520inference%252C%2520whereas%2520higher%2520mAP%2520models%2520like%2520YOLOv8%250AMedium%2520generally%2520consume%2520more%2520energy%2520and%2520have%2520slower%2520inference%252C%2520though%2520with%250Aexceptions%2520when%2520accelerators%2520like%2520TPUs%2520are%2520used.%2520Among%2520the%2520edge%2520devices%252C%2520Jetson%250AOrin%2520Nano%2520stands%2520out%2520as%2520the%2520fastest%2520and%2520most%2520energy-efficient%2520option%2520for%250Arequest%2520handling%252C%2520despite%2520having%2520the%2520highest%2520idle%2520energy%2520consumption.%2520These%250Aresults%2520emphasize%2520the%2520need%2520to%2520balance%2520accuracy%252C%2520speed%252C%2520and%2520energy%2520efficiency%250Awhen%2520deploying%2520deep%2520learning%2520models%2520on%2520edge%2520devices%252C%2520offering%2520valuable%2520guidance%250Afor%2520practitioners%2520and%2520researchers%2520selecting%2520models%2520and%2520devices%2520for%2520their%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16808v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Deep%20Learning%20Models%20for%20Object%20Detection%20on%20Edge%20Computing%0A%20%20Devices&entry.906535625=Daghash%20K.%20Alqahtani%20and%20Aamir%20Cheema%20and%20Adel%20N.%20Toosi&entry.1292438233=%20%20Modern%20applications%2C%20such%20as%20autonomous%20vehicles%2C%20require%20deploying%20deep%0Alearning%20algorithms%20on%20resource-constrained%20edge%20devices%20for%20real-time%20image%0Aand%20video%20processing.%20However%2C%20there%20is%20limited%20understanding%20of%20the%20efficiency%0Aand%20performance%20of%20various%20object%20detection%20models%20on%20these%20devices.%20In%20this%0Apaper%2C%20we%20evaluate%20state-of-the-art%20object%20detection%20models%2C%20including%20YOLOv8%0A%28Nano%2C%20Small%2C%20Medium%29%2C%20EfficientDet%20Lite%20%28Lite0%2C%20Lite1%2C%20Lite2%29%2C%20and%20SSD%20%28SSD%0AMobileNet%20V1%2C%20SSDLite%20MobileDet%29.%20We%20deployed%20these%20models%20on%20popular%20edge%0Adevices%20like%20the%20Raspberry%20Pi%203%2C%204%2C%20and%205%20with/without%20TPU%20accelerators%2C%20and%0AJetson%20Orin%20Nano%2C%20collecting%20key%20performance%20metrics%20such%20as%20energy%0Aconsumption%2C%20inference%20time%2C%20and%20Mean%20Average%20Precision%20%28mAP%29.%20Our%20findings%0Ahighlight%20that%20lower%20mAP%20models%20such%20as%20SSD%20MobileNet%20V1%20are%20more%0Aenergy-efficient%20and%20faster%20in%20inference%2C%20whereas%20higher%20mAP%20models%20like%20YOLOv8%0AMedium%20generally%20consume%20more%20energy%20and%20have%20slower%20inference%2C%20though%20with%0Aexceptions%20when%20accelerators%20like%20TPUs%20are%20used.%20Among%20the%20edge%20devices%2C%20Jetson%0AOrin%20Nano%20stands%20out%20as%20the%20fastest%20and%20most%20energy-efficient%20option%20for%0Arequest%20handling%2C%20despite%20having%20the%20highest%20idle%20energy%20consumption.%20These%0Aresults%20emphasize%20the%20need%20to%20balance%20accuracy%2C%20speed%2C%20and%20energy%20efficiency%0Awhen%20deploying%20deep%20learning%20models%20on%20edge%20devices%2C%20offering%20valuable%20guidance%0Afor%20practitioners%20and%20researchers%20selecting%20models%20and%20devices%20for%20their%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16808v1&entry.124074799=Read"},
{"title": "Differentiating and Integrating ZX Diagrams with Applications to Quantum\n  Machine Learning", "author": "Quanlong Wang and Richie Yeung and Mark Koch", "abstract": "  ZX-calculus has proved to be a useful tool for quantum technology with a wide\nrange of successful applications. Most of these applications are of an\nalgebraic nature. However, other tasks that involve differentiation and\nintegration remain unreachable with current ZX techniques. Here we elevate ZX\nto an analytical perspective by realising differentiation and integration\nentirely within the framework of ZX-calculus. We explicitly illustrate the new\nanalytic framework of ZX-calculus by applying it in context of quantum machine\nlearning for the analysis of barren plateaus.\n", "link": "http://arxiv.org/abs/2201.13250v7", "date": "2024-09-25", "relevancy": 1.236, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4195}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.415}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Differentiating%20and%20Integrating%20ZX%20Diagrams%20with%20Applications%20to%20Quantum%0A%20%20Machine%20Learning&body=Title%3A%20Differentiating%20and%20Integrating%20ZX%20Diagrams%20with%20Applications%20to%20Quantum%0A%20%20Machine%20Learning%0AAuthor%3A%20Quanlong%20Wang%20and%20Richie%20Yeung%20and%20Mark%20Koch%0AAbstract%3A%20%20%20ZX-calculus%20has%20proved%20to%20be%20a%20useful%20tool%20for%20quantum%20technology%20with%20a%20wide%0Arange%20of%20successful%20applications.%20Most%20of%20these%20applications%20are%20of%20an%0Aalgebraic%20nature.%20However%2C%20other%20tasks%20that%20involve%20differentiation%20and%0Aintegration%20remain%20unreachable%20with%20current%20ZX%20techniques.%20Here%20we%20elevate%20ZX%0Ato%20an%20analytical%20perspective%20by%20realising%20differentiation%20and%20integration%0Aentirely%20within%20the%20framework%20of%20ZX-calculus.%20We%20explicitly%20illustrate%20the%20new%0Aanalytic%20framework%20of%20ZX-calculus%20by%20applying%20it%20in%20context%20of%20quantum%20machine%0Alearning%20for%20the%20analysis%20of%20barren%20plateaus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2201.13250v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifferentiating%2520and%2520Integrating%2520ZX%2520Diagrams%2520with%2520Applications%2520to%2520Quantum%250A%2520%2520Machine%2520Learning%26entry.906535625%3DQuanlong%2520Wang%2520and%2520Richie%2520Yeung%2520and%2520Mark%2520Koch%26entry.1292438233%3D%2520%2520ZX-calculus%2520has%2520proved%2520to%2520be%2520a%2520useful%2520tool%2520for%2520quantum%2520technology%2520with%2520a%2520wide%250Arange%2520of%2520successful%2520applications.%2520Most%2520of%2520these%2520applications%2520are%2520of%2520an%250Aalgebraic%2520nature.%2520However%252C%2520other%2520tasks%2520that%2520involve%2520differentiation%2520and%250Aintegration%2520remain%2520unreachable%2520with%2520current%2520ZX%2520techniques.%2520Here%2520we%2520elevate%2520ZX%250Ato%2520an%2520analytical%2520perspective%2520by%2520realising%2520differentiation%2520and%2520integration%250Aentirely%2520within%2520the%2520framework%2520of%2520ZX-calculus.%2520We%2520explicitly%2520illustrate%2520the%2520new%250Aanalytic%2520framework%2520of%2520ZX-calculus%2520by%2520applying%2520it%2520in%2520context%2520of%2520quantum%2520machine%250Alearning%2520for%2520the%2520analysis%2520of%2520barren%2520plateaus.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2201.13250v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Differentiating%20and%20Integrating%20ZX%20Diagrams%20with%20Applications%20to%20Quantum%0A%20%20Machine%20Learning&entry.906535625=Quanlong%20Wang%20and%20Richie%20Yeung%20and%20Mark%20Koch&entry.1292438233=%20%20ZX-calculus%20has%20proved%20to%20be%20a%20useful%20tool%20for%20quantum%20technology%20with%20a%20wide%0Arange%20of%20successful%20applications.%20Most%20of%20these%20applications%20are%20of%20an%0Aalgebraic%20nature.%20However%2C%20other%20tasks%20that%20involve%20differentiation%20and%0Aintegration%20remain%20unreachable%20with%20current%20ZX%20techniques.%20Here%20we%20elevate%20ZX%0Ato%20an%20analytical%20perspective%20by%20realising%20differentiation%20and%20integration%0Aentirely%20within%20the%20framework%20of%20ZX-calculus.%20We%20explicitly%20illustrate%20the%20new%0Aanalytic%20framework%20of%20ZX-calculus%20by%20applying%20it%20in%20context%20of%20quantum%20machine%0Alearning%20for%20the%20analysis%20of%20barren%20plateaus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2201.13250v7&entry.124074799=Read"},
{"title": "Symbolic State Partition for Reinforcement Learning", "author": "Mohsen Ghaffari and Mahsa Varshosaz and Einar Broch Johnsen and Andrzej W\u0105sowski", "abstract": "  Tabular reinforcement learning methods cannot operate directly on continuous\nstate spaces. One solution for this problem is to partition the state space. A\ngood partitioning enables generalization during learning and more efficient\nexploitation of prior experiences. Consequently, the learning process becomes\nfaster and produces more reliable policies. However, partitioning introduces\napproximation, which is particularly harmful in the presence of nonlinear\nrelations between state components. An ideal partition should be as coarse as\npossible, while capturing the key structure of the state space for the given\nproblem. This work extracts partitions from the environment dynamics by\nsymbolic execution. We show that symbolic partitioning improves state space\ncoverage with respect to environmental behavior and allows reinforcement\nlearning to perform better for sparse rewards. We evaluate symbolic state space\npartitioning with respect to precision, scalability, learning agent performance\nand state space coverage for the learnt policies.\n", "link": "http://arxiv.org/abs/2409.16791v1", "date": "2024-09-25", "relevancy": 1.3727, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4611}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4593}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Symbolic%20State%20Partition%20for%20Reinforcement%20Learning&body=Title%3A%20Symbolic%20State%20Partition%20for%20Reinforcement%20Learning%0AAuthor%3A%20Mohsen%20Ghaffari%20and%20Mahsa%20Varshosaz%20and%20Einar%20Broch%20Johnsen%20and%20Andrzej%20W%C4%85sowski%0AAbstract%3A%20%20%20Tabular%20reinforcement%20learning%20methods%20cannot%20operate%20directly%20on%20continuous%0Astate%20spaces.%20One%20solution%20for%20this%20problem%20is%20to%20partition%20the%20state%20space.%20A%0Agood%20partitioning%20enables%20generalization%20during%20learning%20and%20more%20efficient%0Aexploitation%20of%20prior%20experiences.%20Consequently%2C%20the%20learning%20process%20becomes%0Afaster%20and%20produces%20more%20reliable%20policies.%20However%2C%20partitioning%20introduces%0Aapproximation%2C%20which%20is%20particularly%20harmful%20in%20the%20presence%20of%20nonlinear%0Arelations%20between%20state%20components.%20An%20ideal%20partition%20should%20be%20as%20coarse%20as%0Apossible%2C%20while%20capturing%20the%20key%20structure%20of%20the%20state%20space%20for%20the%20given%0Aproblem.%20This%20work%20extracts%20partitions%20from%20the%20environment%20dynamics%20by%0Asymbolic%20execution.%20We%20show%20that%20symbolic%20partitioning%20improves%20state%20space%0Acoverage%20with%20respect%20to%20environmental%20behavior%20and%20allows%20reinforcement%0Alearning%20to%20perform%20better%20for%20sparse%20rewards.%20We%20evaluate%20symbolic%20state%20space%0Apartitioning%20with%20respect%20to%20precision%2C%20scalability%2C%20learning%20agent%20performance%0Aand%20state%20space%20coverage%20for%20the%20learnt%20policies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSymbolic%2520State%2520Partition%2520for%2520Reinforcement%2520Learning%26entry.906535625%3DMohsen%2520Ghaffari%2520and%2520Mahsa%2520Varshosaz%2520and%2520Einar%2520Broch%2520Johnsen%2520and%2520Andrzej%2520W%25C4%2585sowski%26entry.1292438233%3D%2520%2520Tabular%2520reinforcement%2520learning%2520methods%2520cannot%2520operate%2520directly%2520on%2520continuous%250Astate%2520spaces.%2520One%2520solution%2520for%2520this%2520problem%2520is%2520to%2520partition%2520the%2520state%2520space.%2520A%250Agood%2520partitioning%2520enables%2520generalization%2520during%2520learning%2520and%2520more%2520efficient%250Aexploitation%2520of%2520prior%2520experiences.%2520Consequently%252C%2520the%2520learning%2520process%2520becomes%250Afaster%2520and%2520produces%2520more%2520reliable%2520policies.%2520However%252C%2520partitioning%2520introduces%250Aapproximation%252C%2520which%2520is%2520particularly%2520harmful%2520in%2520the%2520presence%2520of%2520nonlinear%250Arelations%2520between%2520state%2520components.%2520An%2520ideal%2520partition%2520should%2520be%2520as%2520coarse%2520as%250Apossible%252C%2520while%2520capturing%2520the%2520key%2520structure%2520of%2520the%2520state%2520space%2520for%2520the%2520given%250Aproblem.%2520This%2520work%2520extracts%2520partitions%2520from%2520the%2520environment%2520dynamics%2520by%250Asymbolic%2520execution.%2520We%2520show%2520that%2520symbolic%2520partitioning%2520improves%2520state%2520space%250Acoverage%2520with%2520respect%2520to%2520environmental%2520behavior%2520and%2520allows%2520reinforcement%250Alearning%2520to%2520perform%2520better%2520for%2520sparse%2520rewards.%2520We%2520evaluate%2520symbolic%2520state%2520space%250Apartitioning%2520with%2520respect%2520to%2520precision%252C%2520scalability%252C%2520learning%2520agent%2520performance%250Aand%2520state%2520space%2520coverage%2520for%2520the%2520learnt%2520policies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Symbolic%20State%20Partition%20for%20Reinforcement%20Learning&entry.906535625=Mohsen%20Ghaffari%20and%20Mahsa%20Varshosaz%20and%20Einar%20Broch%20Johnsen%20and%20Andrzej%20W%C4%85sowski&entry.1292438233=%20%20Tabular%20reinforcement%20learning%20methods%20cannot%20operate%20directly%20on%20continuous%0Astate%20spaces.%20One%20solution%20for%20this%20problem%20is%20to%20partition%20the%20state%20space.%20A%0Agood%20partitioning%20enables%20generalization%20during%20learning%20and%20more%20efficient%0Aexploitation%20of%20prior%20experiences.%20Consequently%2C%20the%20learning%20process%20becomes%0Afaster%20and%20produces%20more%20reliable%20policies.%20However%2C%20partitioning%20introduces%0Aapproximation%2C%20which%20is%20particularly%20harmful%20in%20the%20presence%20of%20nonlinear%0Arelations%20between%20state%20components.%20An%20ideal%20partition%20should%20be%20as%20coarse%20as%0Apossible%2C%20while%20capturing%20the%20key%20structure%20of%20the%20state%20space%20for%20the%20given%0Aproblem.%20This%20work%20extracts%20partitions%20from%20the%20environment%20dynamics%20by%0Asymbolic%20execution.%20We%20show%20that%20symbolic%20partitioning%20improves%20state%20space%0Acoverage%20with%20respect%20to%20environmental%20behavior%20and%20allows%20reinforcement%0Alearning%20to%20perform%20better%20for%20sparse%20rewards.%20We%20evaluate%20symbolic%20state%20space%0Apartitioning%20with%20respect%20to%20precision%2C%20scalability%2C%20learning%20agent%20performance%0Aand%20state%20space%20coverage%20for%20the%20learnt%20policies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16791v1&entry.124074799=Read"},
{"title": "An explicit construction of Kaleidocycles by elliptic theta functions", "author": "Shizuo Kaji and Kenji Kajiwara and Shota Shigetomi", "abstract": "  We consider the configuration space of points on the two-dimensional sphere\nthat satisfy a specific system of quadratic equations. We construct periodic\norbits in this configuration space using elliptic theta functions and show that\nthey satisfy semi-discrete analogues of mKdV and sine-Gordon equations. The\nconfiguration space we investigate corresponds to the state space of a linkage\nmechanism known as the Kaleidocycle, and the constructed orbits describe the\ncharacteristic motion of the Kaleidocycle. Our approach is founded on the\nrelationship between the deformation of spatial curves and integrable systems,\noffering an intriguing example where an integrable system generates an orbit in\nthe space of real solutions to polynomial equations defined by geometric\nconstraints.\n", "link": "http://arxiv.org/abs/2308.04977v2", "date": "2024-09-25", "relevancy": 1.2955, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3251}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.3236}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.3236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20explicit%20construction%20of%20Kaleidocycles%20by%20elliptic%20theta%20functions&body=Title%3A%20An%20explicit%20construction%20of%20Kaleidocycles%20by%20elliptic%20theta%20functions%0AAuthor%3A%20Shizuo%20Kaji%20and%20Kenji%20Kajiwara%20and%20Shota%20Shigetomi%0AAbstract%3A%20%20%20We%20consider%20the%20configuration%20space%20of%20points%20on%20the%20two-dimensional%20sphere%0Athat%20satisfy%20a%20specific%20system%20of%20quadratic%20equations.%20We%20construct%20periodic%0Aorbits%20in%20this%20configuration%20space%20using%20elliptic%20theta%20functions%20and%20show%20that%0Athey%20satisfy%20semi-discrete%20analogues%20of%20mKdV%20and%20sine-Gordon%20equations.%20The%0Aconfiguration%20space%20we%20investigate%20corresponds%20to%20the%20state%20space%20of%20a%20linkage%0Amechanism%20known%20as%20the%20Kaleidocycle%2C%20and%20the%20constructed%20orbits%20describe%20the%0Acharacteristic%20motion%20of%20the%20Kaleidocycle.%20Our%20approach%20is%20founded%20on%20the%0Arelationship%20between%20the%20deformation%20of%20spatial%20curves%20and%20integrable%20systems%2C%0Aoffering%20an%20intriguing%20example%20where%20an%20integrable%20system%20generates%20an%20orbit%20in%0Athe%20space%20of%20real%20solutions%20to%20polynomial%20equations%20defined%20by%20geometric%0Aconstraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.04977v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520explicit%2520construction%2520of%2520Kaleidocycles%2520by%2520elliptic%2520theta%2520functions%26entry.906535625%3DShizuo%2520Kaji%2520and%2520Kenji%2520Kajiwara%2520and%2520Shota%2520Shigetomi%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520configuration%2520space%2520of%2520points%2520on%2520the%2520two-dimensional%2520sphere%250Athat%2520satisfy%2520a%2520specific%2520system%2520of%2520quadratic%2520equations.%2520We%2520construct%2520periodic%250Aorbits%2520in%2520this%2520configuration%2520space%2520using%2520elliptic%2520theta%2520functions%2520and%2520show%2520that%250Athey%2520satisfy%2520semi-discrete%2520analogues%2520of%2520mKdV%2520and%2520sine-Gordon%2520equations.%2520The%250Aconfiguration%2520space%2520we%2520investigate%2520corresponds%2520to%2520the%2520state%2520space%2520of%2520a%2520linkage%250Amechanism%2520known%2520as%2520the%2520Kaleidocycle%252C%2520and%2520the%2520constructed%2520orbits%2520describe%2520the%250Acharacteristic%2520motion%2520of%2520the%2520Kaleidocycle.%2520Our%2520approach%2520is%2520founded%2520on%2520the%250Arelationship%2520between%2520the%2520deformation%2520of%2520spatial%2520curves%2520and%2520integrable%2520systems%252C%250Aoffering%2520an%2520intriguing%2520example%2520where%2520an%2520integrable%2520system%2520generates%2520an%2520orbit%2520in%250Athe%2520space%2520of%2520real%2520solutions%2520to%2520polynomial%2520equations%2520defined%2520by%2520geometric%250Aconstraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.04977v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20explicit%20construction%20of%20Kaleidocycles%20by%20elliptic%20theta%20functions&entry.906535625=Shizuo%20Kaji%20and%20Kenji%20Kajiwara%20and%20Shota%20Shigetomi&entry.1292438233=%20%20We%20consider%20the%20configuration%20space%20of%20points%20on%20the%20two-dimensional%20sphere%0Athat%20satisfy%20a%20specific%20system%20of%20quadratic%20equations.%20We%20construct%20periodic%0Aorbits%20in%20this%20configuration%20space%20using%20elliptic%20theta%20functions%20and%20show%20that%0Athey%20satisfy%20semi-discrete%20analogues%20of%20mKdV%20and%20sine-Gordon%20equations.%20The%0Aconfiguration%20space%20we%20investigate%20corresponds%20to%20the%20state%20space%20of%20a%20linkage%0Amechanism%20known%20as%20the%20Kaleidocycle%2C%20and%20the%20constructed%20orbits%20describe%20the%0Acharacteristic%20motion%20of%20the%20Kaleidocycle.%20Our%20approach%20is%20founded%20on%20the%0Arelationship%20between%20the%20deformation%20of%20spatial%20curves%20and%20integrable%20systems%2C%0Aoffering%20an%20intriguing%20example%20where%20an%20integrable%20system%20generates%20an%20orbit%20in%0Athe%20space%20of%20real%20solutions%20to%20polynomial%20equations%20defined%20by%20geometric%0Aconstraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.04977v2&entry.124074799=Read"},
{"title": "Multi-objective Evolution of Heuristic Using Large Language Model", "author": "Shunyu Yao and Fei Liu and Xi Lin and Zhichao Lu and Zhenkun Wang and Qingfu Zhang", "abstract": "  Heuristics are commonly used to tackle diverse search and optimization\nproblems. Design heuristics usually require tedious manual crafting with domain\nknowledge. Recent works have incorporated large language models (LLMs) into\nautomatic heuristic search leveraging their powerful language and coding\ncapacity. However, existing research focuses on the optimal performance on the\ntarget problem as the sole objective, neglecting other criteria such as\nefficiency and scalability, which are vital in practice. To tackle this\nchallenge, we propose to model heuristic search as a multi-objective\noptimization problem and consider introducing other practical criteria beyond\noptimal performance. Due to the complexity of the search space, conventional\nmulti-objective optimization methods struggle to effectively handle\nmulti-objective heuristic search. We propose the first LLM-based\nmulti-objective heuristic search framework, Multi-objective Evolution of\nHeuristic (MEoH), which integrates LLMs in a zero-shot manner to generate a\nnon-dominated set of heuristics to meet multiple design criteria. We design a\nnew dominance-dissimilarity mechanism for effective population management and\nselection, which incorporates both code dissimilarity in the search space and\ndominance in the objective space. MEoH is demonstrated in two well-known\ncombinatorial optimization problems: the online Bin Packing Problem (BPP) and\nthe Traveling Salesman Problem (TSP). Results indicate that a variety of elite\nheuristics are automatically generated in a single run, offering more trade-off\noptions than existing methods. It successfully achieves competitive or superior\nperformance while improving efficiency up to 10 times. Moreover, we also\nobserve that the multi-objective search introduces novel insights into\nheuristic design and leads to the discovery of diverse heuristics.\n", "link": "http://arxiv.org/abs/2409.16867v1", "date": "2024-09-25", "relevancy": 1.8968, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4745}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4745}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-objective%20Evolution%20of%20Heuristic%20Using%20Large%20Language%20Model&body=Title%3A%20Multi-objective%20Evolution%20of%20Heuristic%20Using%20Large%20Language%20Model%0AAuthor%3A%20Shunyu%20Yao%20and%20Fei%20Liu%20and%20Xi%20Lin%20and%20Zhichao%20Lu%20and%20Zhenkun%20Wang%20and%20Qingfu%20Zhang%0AAbstract%3A%20%20%20Heuristics%20are%20commonly%20used%20to%20tackle%20diverse%20search%20and%20optimization%0Aproblems.%20Design%20heuristics%20usually%20require%20tedious%20manual%20crafting%20with%20domain%0Aknowledge.%20Recent%20works%20have%20incorporated%20large%20language%20models%20%28LLMs%29%20into%0Aautomatic%20heuristic%20search%20leveraging%20their%20powerful%20language%20and%20coding%0Acapacity.%20However%2C%20existing%20research%20focuses%20on%20the%20optimal%20performance%20on%20the%0Atarget%20problem%20as%20the%20sole%20objective%2C%20neglecting%20other%20criteria%20such%20as%0Aefficiency%20and%20scalability%2C%20which%20are%20vital%20in%20practice.%20To%20tackle%20this%0Achallenge%2C%20we%20propose%20to%20model%20heuristic%20search%20as%20a%20multi-objective%0Aoptimization%20problem%20and%20consider%20introducing%20other%20practical%20criteria%20beyond%0Aoptimal%20performance.%20Due%20to%20the%20complexity%20of%20the%20search%20space%2C%20conventional%0Amulti-objective%20optimization%20methods%20struggle%20to%20effectively%20handle%0Amulti-objective%20heuristic%20search.%20We%20propose%20the%20first%20LLM-based%0Amulti-objective%20heuristic%20search%20framework%2C%20Multi-objective%20Evolution%20of%0AHeuristic%20%28MEoH%29%2C%20which%20integrates%20LLMs%20in%20a%20zero-shot%20manner%20to%20generate%20a%0Anon-dominated%20set%20of%20heuristics%20to%20meet%20multiple%20design%20criteria.%20We%20design%20a%0Anew%20dominance-dissimilarity%20mechanism%20for%20effective%20population%20management%20and%0Aselection%2C%20which%20incorporates%20both%20code%20dissimilarity%20in%20the%20search%20space%20and%0Adominance%20in%20the%20objective%20space.%20MEoH%20is%20demonstrated%20in%20two%20well-known%0Acombinatorial%20optimization%20problems%3A%20the%20online%20Bin%20Packing%20Problem%20%28BPP%29%20and%0Athe%20Traveling%20Salesman%20Problem%20%28TSP%29.%20Results%20indicate%20that%20a%20variety%20of%20elite%0Aheuristics%20are%20automatically%20generated%20in%20a%20single%20run%2C%20offering%20more%20trade-off%0Aoptions%20than%20existing%20methods.%20It%20successfully%20achieves%20competitive%20or%20superior%0Aperformance%20while%20improving%20efficiency%20up%20to%2010%20times.%20Moreover%2C%20we%20also%0Aobserve%20that%20the%20multi-objective%20search%20introduces%20novel%20insights%20into%0Aheuristic%20design%20and%20leads%20to%20the%20discovery%20of%20diverse%20heuristics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.16867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-objective%2520Evolution%2520of%2520Heuristic%2520Using%2520Large%2520Language%2520Model%26entry.906535625%3DShunyu%2520Yao%2520and%2520Fei%2520Liu%2520and%2520Xi%2520Lin%2520and%2520Zhichao%2520Lu%2520and%2520Zhenkun%2520Wang%2520and%2520Qingfu%2520Zhang%26entry.1292438233%3D%2520%2520Heuristics%2520are%2520commonly%2520used%2520to%2520tackle%2520diverse%2520search%2520and%2520optimization%250Aproblems.%2520Design%2520heuristics%2520usually%2520require%2520tedious%2520manual%2520crafting%2520with%2520domain%250Aknowledge.%2520Recent%2520works%2520have%2520incorporated%2520large%2520language%2520models%2520%2528LLMs%2529%2520into%250Aautomatic%2520heuristic%2520search%2520leveraging%2520their%2520powerful%2520language%2520and%2520coding%250Acapacity.%2520However%252C%2520existing%2520research%2520focuses%2520on%2520the%2520optimal%2520performance%2520on%2520the%250Atarget%2520problem%2520as%2520the%2520sole%2520objective%252C%2520neglecting%2520other%2520criteria%2520such%2520as%250Aefficiency%2520and%2520scalability%252C%2520which%2520are%2520vital%2520in%2520practice.%2520To%2520tackle%2520this%250Achallenge%252C%2520we%2520propose%2520to%2520model%2520heuristic%2520search%2520as%2520a%2520multi-objective%250Aoptimization%2520problem%2520and%2520consider%2520introducing%2520other%2520practical%2520criteria%2520beyond%250Aoptimal%2520performance.%2520Due%2520to%2520the%2520complexity%2520of%2520the%2520search%2520space%252C%2520conventional%250Amulti-objective%2520optimization%2520methods%2520struggle%2520to%2520effectively%2520handle%250Amulti-objective%2520heuristic%2520search.%2520We%2520propose%2520the%2520first%2520LLM-based%250Amulti-objective%2520heuristic%2520search%2520framework%252C%2520Multi-objective%2520Evolution%2520of%250AHeuristic%2520%2528MEoH%2529%252C%2520which%2520integrates%2520LLMs%2520in%2520a%2520zero-shot%2520manner%2520to%2520generate%2520a%250Anon-dominated%2520set%2520of%2520heuristics%2520to%2520meet%2520multiple%2520design%2520criteria.%2520We%2520design%2520a%250Anew%2520dominance-dissimilarity%2520mechanism%2520for%2520effective%2520population%2520management%2520and%250Aselection%252C%2520which%2520incorporates%2520both%2520code%2520dissimilarity%2520in%2520the%2520search%2520space%2520and%250Adominance%2520in%2520the%2520objective%2520space.%2520MEoH%2520is%2520demonstrated%2520in%2520two%2520well-known%250Acombinatorial%2520optimization%2520problems%253A%2520the%2520online%2520Bin%2520Packing%2520Problem%2520%2528BPP%2529%2520and%250Athe%2520Traveling%2520Salesman%2520Problem%2520%2528TSP%2529.%2520Results%2520indicate%2520that%2520a%2520variety%2520of%2520elite%250Aheuristics%2520are%2520automatically%2520generated%2520in%2520a%2520single%2520run%252C%2520offering%2520more%2520trade-off%250Aoptions%2520than%2520existing%2520methods.%2520It%2520successfully%2520achieves%2520competitive%2520or%2520superior%250Aperformance%2520while%2520improving%2520efficiency%2520up%2520to%252010%2520times.%2520Moreover%252C%2520we%2520also%250Aobserve%2520that%2520the%2520multi-objective%2520search%2520introduces%2520novel%2520insights%2520into%250Aheuristic%2520design%2520and%2520leads%2520to%2520the%2520discovery%2520of%2520diverse%2520heuristics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.16867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-objective%20Evolution%20of%20Heuristic%20Using%20Large%20Language%20Model&entry.906535625=Shunyu%20Yao%20and%20Fei%20Liu%20and%20Xi%20Lin%20and%20Zhichao%20Lu%20and%20Zhenkun%20Wang%20and%20Qingfu%20Zhang&entry.1292438233=%20%20Heuristics%20are%20commonly%20used%20to%20tackle%20diverse%20search%20and%20optimization%0Aproblems.%20Design%20heuristics%20usually%20require%20tedious%20manual%20crafting%20with%20domain%0Aknowledge.%20Recent%20works%20have%20incorporated%20large%20language%20models%20%28LLMs%29%20into%0Aautomatic%20heuristic%20search%20leveraging%20their%20powerful%20language%20and%20coding%0Acapacity.%20However%2C%20existing%20research%20focuses%20on%20the%20optimal%20performance%20on%20the%0Atarget%20problem%20as%20the%20sole%20objective%2C%20neglecting%20other%20criteria%20such%20as%0Aefficiency%20and%20scalability%2C%20which%20are%20vital%20in%20practice.%20To%20tackle%20this%0Achallenge%2C%20we%20propose%20to%20model%20heuristic%20search%20as%20a%20multi-objective%0Aoptimization%20problem%20and%20consider%20introducing%20other%20practical%20criteria%20beyond%0Aoptimal%20performance.%20Due%20to%20the%20complexity%20of%20the%20search%20space%2C%20conventional%0Amulti-objective%20optimization%20methods%20struggle%20to%20effectively%20handle%0Amulti-objective%20heuristic%20search.%20We%20propose%20the%20first%20LLM-based%0Amulti-objective%20heuristic%20search%20framework%2C%20Multi-objective%20Evolution%20of%0AHeuristic%20%28MEoH%29%2C%20which%20integrates%20LLMs%20in%20a%20zero-shot%20manner%20to%20generate%20a%0Anon-dominated%20set%20of%20heuristics%20to%20meet%20multiple%20design%20criteria.%20We%20design%20a%0Anew%20dominance-dissimilarity%20mechanism%20for%20effective%20population%20management%20and%0Aselection%2C%20which%20incorporates%20both%20code%20dissimilarity%20in%20the%20search%20space%20and%0Adominance%20in%20the%20objective%20space.%20MEoH%20is%20demonstrated%20in%20two%20well-known%0Acombinatorial%20optimization%20problems%3A%20the%20online%20Bin%20Packing%20Problem%20%28BPP%29%20and%0Athe%20Traveling%20Salesman%20Problem%20%28TSP%29.%20Results%20indicate%20that%20a%20variety%20of%20elite%0Aheuristics%20are%20automatically%20generated%20in%20a%20single%20run%2C%20offering%20more%20trade-off%0Aoptions%20than%20existing%20methods.%20It%20successfully%20achieves%20competitive%20or%20superior%0Aperformance%20while%20improving%20efficiency%20up%20to%2010%20times.%20Moreover%2C%20we%20also%0Aobserve%20that%20the%20multi-objective%20search%20introduces%20novel%20insights%20into%0Aheuristic%20design%20and%20leads%20to%20the%20discovery%20of%20diverse%20heuristics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.16867v1&entry.124074799=Read"},
{"title": "CombU: A Combined Unit Activation for Fitting Mathematical Expressions\n  with Neural Networks", "author": "Jiayu Li and Zilong Zhao and Kevin Yee and Uzair Javaid and Biplab Sikdar", "abstract": "  The activation functions are fundamental to neural networks as they introduce\nnon-linearity into data relationships, thereby enabling deep networks to\napproximate complex data relations. Existing efforts to enhance neural network\nperformance have predominantly focused on developing new mathematical\nfunctions. However, we find that a well-designed combination of existing\nactivation functions within a neural network can also achieve this objective.\nIn this paper, we introduce the Combined Units activation (CombU), which\nemploys different activation functions at various dimensions across different\nlayers. This approach can be theoretically proven to fit most mathematical\nexpressions accurately. The experiments conducted on four mathematical\nexpression datasets, compared against six State-Of-The-Art (SOTA) activation\nfunction algorithms, demonstrate that CombU outperforms all SOTA algorithms in\n10 out of 16 metrics and ranks in the top three for the remaining six metrics.\n", "link": "http://arxiv.org/abs/2409.17021v1", "date": "2024-09-25", "relevancy": 1.8568, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4753}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4606}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CombU%3A%20A%20Combined%20Unit%20Activation%20for%20Fitting%20Mathematical%20Expressions%0A%20%20with%20Neural%20Networks&body=Title%3A%20CombU%3A%20A%20Combined%20Unit%20Activation%20for%20Fitting%20Mathematical%20Expressions%0A%20%20with%20Neural%20Networks%0AAuthor%3A%20Jiayu%20Li%20and%20Zilong%20Zhao%20and%20Kevin%20Yee%20and%20Uzair%20Javaid%20and%20Biplab%20Sikdar%0AAbstract%3A%20%20%20The%20activation%20functions%20are%20fundamental%20to%20neural%20networks%20as%20they%20introduce%0Anon-linearity%20into%20data%20relationships%2C%20thereby%20enabling%20deep%20networks%20to%0Aapproximate%20complex%20data%20relations.%20Existing%20efforts%20to%20enhance%20neural%20network%0Aperformance%20have%20predominantly%20focused%20on%20developing%20new%20mathematical%0Afunctions.%20However%2C%20we%20find%20that%20a%20well-designed%20combination%20of%20existing%0Aactivation%20functions%20within%20a%20neural%20network%20can%20also%20achieve%20this%20objective.%0AIn%20this%20paper%2C%20we%20introduce%20the%20Combined%20Units%20activation%20%28CombU%29%2C%20which%0Aemploys%20different%20activation%20functions%20at%20various%20dimensions%20across%20different%0Alayers.%20This%20approach%20can%20be%20theoretically%20proven%20to%20fit%20most%20mathematical%0Aexpressions%20accurately.%20The%20experiments%20conducted%20on%20four%20mathematical%0Aexpression%20datasets%2C%20compared%20against%20six%20State-Of-The-Art%20%28SOTA%29%20activation%0Afunction%20algorithms%2C%20demonstrate%20that%20CombU%20outperforms%20all%20SOTA%20algorithms%20in%0A10%20out%20of%2016%20metrics%20and%20ranks%20in%20the%20top%20three%20for%20the%20remaining%20six%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17021v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCombU%253A%2520A%2520Combined%2520Unit%2520Activation%2520for%2520Fitting%2520Mathematical%2520Expressions%250A%2520%2520with%2520Neural%2520Networks%26entry.906535625%3DJiayu%2520Li%2520and%2520Zilong%2520Zhao%2520and%2520Kevin%2520Yee%2520and%2520Uzair%2520Javaid%2520and%2520Biplab%2520Sikdar%26entry.1292438233%3D%2520%2520The%2520activation%2520functions%2520are%2520fundamental%2520to%2520neural%2520networks%2520as%2520they%2520introduce%250Anon-linearity%2520into%2520data%2520relationships%252C%2520thereby%2520enabling%2520deep%2520networks%2520to%250Aapproximate%2520complex%2520data%2520relations.%2520Existing%2520efforts%2520to%2520enhance%2520neural%2520network%250Aperformance%2520have%2520predominantly%2520focused%2520on%2520developing%2520new%2520mathematical%250Afunctions.%2520However%252C%2520we%2520find%2520that%2520a%2520well-designed%2520combination%2520of%2520existing%250Aactivation%2520functions%2520within%2520a%2520neural%2520network%2520can%2520also%2520achieve%2520this%2520objective.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Combined%2520Units%2520activation%2520%2528CombU%2529%252C%2520which%250Aemploys%2520different%2520activation%2520functions%2520at%2520various%2520dimensions%2520across%2520different%250Alayers.%2520This%2520approach%2520can%2520be%2520theoretically%2520proven%2520to%2520fit%2520most%2520mathematical%250Aexpressions%2520accurately.%2520The%2520experiments%2520conducted%2520on%2520four%2520mathematical%250Aexpression%2520datasets%252C%2520compared%2520against%2520six%2520State-Of-The-Art%2520%2528SOTA%2529%2520activation%250Afunction%2520algorithms%252C%2520demonstrate%2520that%2520CombU%2520outperforms%2520all%2520SOTA%2520algorithms%2520in%250A10%2520out%2520of%252016%2520metrics%2520and%2520ranks%2520in%2520the%2520top%2520three%2520for%2520the%2520remaining%2520six%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17021v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CombU%3A%20A%20Combined%20Unit%20Activation%20for%20Fitting%20Mathematical%20Expressions%0A%20%20with%20Neural%20Networks&entry.906535625=Jiayu%20Li%20and%20Zilong%20Zhao%20and%20Kevin%20Yee%20and%20Uzair%20Javaid%20and%20Biplab%20Sikdar&entry.1292438233=%20%20The%20activation%20functions%20are%20fundamental%20to%20neural%20networks%20as%20they%20introduce%0Anon-linearity%20into%20data%20relationships%2C%20thereby%20enabling%20deep%20networks%20to%0Aapproximate%20complex%20data%20relations.%20Existing%20efforts%20to%20enhance%20neural%20network%0Aperformance%20have%20predominantly%20focused%20on%20developing%20new%20mathematical%0Afunctions.%20However%2C%20we%20find%20that%20a%20well-designed%20combination%20of%20existing%0Aactivation%20functions%20within%20a%20neural%20network%20can%20also%20achieve%20this%20objective.%0AIn%20this%20paper%2C%20we%20introduce%20the%20Combined%20Units%20activation%20%28CombU%29%2C%20which%0Aemploys%20different%20activation%20functions%20at%20various%20dimensions%20across%20different%0Alayers.%20This%20approach%20can%20be%20theoretically%20proven%20to%20fit%20most%20mathematical%0Aexpressions%20accurately.%20The%20experiments%20conducted%20on%20four%20mathematical%0Aexpression%20datasets%2C%20compared%20against%20six%20State-Of-The-Art%20%28SOTA%29%20activation%0Afunction%20algorithms%2C%20demonstrate%20that%20CombU%20outperforms%20all%20SOTA%20algorithms%20in%0A10%20out%20of%2016%20metrics%20and%20ranks%20in%20the%20top%20three%20for%20the%20remaining%20six%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17021v1&entry.124074799=Read"},
{"title": "Toward Tiny and High-quality Facial Makeup with Data Amplify Learning", "author": "Qiaoqiao Jin and Xuanhong Chen and Meiguang Jin and Ying Chen and Rui Shi and Yucheng Zheng and Yupeng Zhu and Bingbing Ni", "abstract": "  Contemporary makeup approaches primarily hinge on unpaired learning\nparadigms, yet they grapple with the challenges of inaccurate supervision\n(e.g., face misalignment) and sophisticated facial prompts (including face\nparsing, and landmark detection). These challenges prohibit low-cost deployment\nof facial makeup models, especially on mobile devices. To solve above problems,\nwe propose a brand-new learning paradigm, termed \"Data Amplify Learning (DAL),\"\nalongside a compact makeup model named \"TinyBeauty.\" The core idea of DAL lies\nin employing a Diffusion-based Data Amplifier (DDA) to \"amplify\" limited images\nfor the model training, thereby enabling accurate pixel-to-pixel supervision\nwith merely a handful of annotations. Two pivotal innovations in DDA facilitate\nthe above training approach: (1) A Residual Diffusion Model (RDM) is designed\nto generate high-fidelity detail and circumvent the detail vanishing problem in\nthe vanilla diffusion models; (2) A Fine-Grained Makeup Module (FGMM) is\nproposed to achieve precise makeup control and combination while retaining face\nidentity. Coupled with DAL, TinyBeauty necessitates merely 80K parameters to\nachieve a state-of-the-art performance without intricate face prompts.\nMeanwhile, TinyBeauty achieves a remarkable inference speed of up to 460 fps on\nthe iPhone 13. Extensive experiments show that DAL can produce highly\ncompetitive makeup models using only 5 image pairs.\n", "link": "http://arxiv.org/abs/2403.15033v4", "date": "2024-09-25", "relevancy": 1.7689, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5943}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5883}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5793}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Tiny%20and%20High-quality%20Facial%20Makeup%20with%20Data%20Amplify%20Learning&body=Title%3A%20Toward%20Tiny%20and%20High-quality%20Facial%20Makeup%20with%20Data%20Amplify%20Learning%0AAuthor%3A%20Qiaoqiao%20Jin%20and%20Xuanhong%20Chen%20and%20Meiguang%20Jin%20and%20Ying%20Chen%20and%20Rui%20Shi%20and%20Yucheng%20Zheng%20and%20Yupeng%20Zhu%20and%20Bingbing%20Ni%0AAbstract%3A%20%20%20Contemporary%20makeup%20approaches%20primarily%20hinge%20on%20unpaired%20learning%0Aparadigms%2C%20yet%20they%20grapple%20with%20the%20challenges%20of%20inaccurate%20supervision%0A%28e.g.%2C%20face%20misalignment%29%20and%20sophisticated%20facial%20prompts%20%28including%20face%0Aparsing%2C%20and%20landmark%20detection%29.%20These%20challenges%20prohibit%20low-cost%20deployment%0Aof%20facial%20makeup%20models%2C%20especially%20on%20mobile%20devices.%20To%20solve%20above%20problems%2C%0Awe%20propose%20a%20brand-new%20learning%20paradigm%2C%20termed%20%22Data%20Amplify%20Learning%20%28DAL%29%2C%22%0Aalongside%20a%20compact%20makeup%20model%20named%20%22TinyBeauty.%22%20The%20core%20idea%20of%20DAL%20lies%0Ain%20employing%20a%20Diffusion-based%20Data%20Amplifier%20%28DDA%29%20to%20%22amplify%22%20limited%20images%0Afor%20the%20model%20training%2C%20thereby%20enabling%20accurate%20pixel-to-pixel%20supervision%0Awith%20merely%20a%20handful%20of%20annotations.%20Two%20pivotal%20innovations%20in%20DDA%20facilitate%0Athe%20above%20training%20approach%3A%20%281%29%20A%20Residual%20Diffusion%20Model%20%28RDM%29%20is%20designed%0Ato%20generate%20high-fidelity%20detail%20and%20circumvent%20the%20detail%20vanishing%20problem%20in%0Athe%20vanilla%20diffusion%20models%3B%20%282%29%20A%20Fine-Grained%20Makeup%20Module%20%28FGMM%29%20is%0Aproposed%20to%20achieve%20precise%20makeup%20control%20and%20combination%20while%20retaining%20face%0Aidentity.%20Coupled%20with%20DAL%2C%20TinyBeauty%20necessitates%20merely%2080K%20parameters%20to%0Aachieve%20a%20state-of-the-art%20performance%20without%20intricate%20face%20prompts.%0AMeanwhile%2C%20TinyBeauty%20achieves%20a%20remarkable%20inference%20speed%20of%20up%20to%20460%20fps%20on%0Athe%20iPhone%2013.%20Extensive%20experiments%20show%20that%20DAL%20can%20produce%20highly%0Acompetitive%20makeup%20models%20using%20only%205%20image%20pairs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15033v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Tiny%2520and%2520High-quality%2520Facial%2520Makeup%2520with%2520Data%2520Amplify%2520Learning%26entry.906535625%3DQiaoqiao%2520Jin%2520and%2520Xuanhong%2520Chen%2520and%2520Meiguang%2520Jin%2520and%2520Ying%2520Chen%2520and%2520Rui%2520Shi%2520and%2520Yucheng%2520Zheng%2520and%2520Yupeng%2520Zhu%2520and%2520Bingbing%2520Ni%26entry.1292438233%3D%2520%2520Contemporary%2520makeup%2520approaches%2520primarily%2520hinge%2520on%2520unpaired%2520learning%250Aparadigms%252C%2520yet%2520they%2520grapple%2520with%2520the%2520challenges%2520of%2520inaccurate%2520supervision%250A%2528e.g.%252C%2520face%2520misalignment%2529%2520and%2520sophisticated%2520facial%2520prompts%2520%2528including%2520face%250Aparsing%252C%2520and%2520landmark%2520detection%2529.%2520These%2520challenges%2520prohibit%2520low-cost%2520deployment%250Aof%2520facial%2520makeup%2520models%252C%2520especially%2520on%2520mobile%2520devices.%2520To%2520solve%2520above%2520problems%252C%250Awe%2520propose%2520a%2520brand-new%2520learning%2520paradigm%252C%2520termed%2520%2522Data%2520Amplify%2520Learning%2520%2528DAL%2529%252C%2522%250Aalongside%2520a%2520compact%2520makeup%2520model%2520named%2520%2522TinyBeauty.%2522%2520The%2520core%2520idea%2520of%2520DAL%2520lies%250Ain%2520employing%2520a%2520Diffusion-based%2520Data%2520Amplifier%2520%2528DDA%2529%2520to%2520%2522amplify%2522%2520limited%2520images%250Afor%2520the%2520model%2520training%252C%2520thereby%2520enabling%2520accurate%2520pixel-to-pixel%2520supervision%250Awith%2520merely%2520a%2520handful%2520of%2520annotations.%2520Two%2520pivotal%2520innovations%2520in%2520DDA%2520facilitate%250Athe%2520above%2520training%2520approach%253A%2520%25281%2529%2520A%2520Residual%2520Diffusion%2520Model%2520%2528RDM%2529%2520is%2520designed%250Ato%2520generate%2520high-fidelity%2520detail%2520and%2520circumvent%2520the%2520detail%2520vanishing%2520problem%2520in%250Athe%2520vanilla%2520diffusion%2520models%253B%2520%25282%2529%2520A%2520Fine-Grained%2520Makeup%2520Module%2520%2528FGMM%2529%2520is%250Aproposed%2520to%2520achieve%2520precise%2520makeup%2520control%2520and%2520combination%2520while%2520retaining%2520face%250Aidentity.%2520Coupled%2520with%2520DAL%252C%2520TinyBeauty%2520necessitates%2520merely%252080K%2520parameters%2520to%250Aachieve%2520a%2520state-of-the-art%2520performance%2520without%2520intricate%2520face%2520prompts.%250AMeanwhile%252C%2520TinyBeauty%2520achieves%2520a%2520remarkable%2520inference%2520speed%2520of%2520up%2520to%2520460%2520fps%2520on%250Athe%2520iPhone%252013.%2520Extensive%2520experiments%2520show%2520that%2520DAL%2520can%2520produce%2520highly%250Acompetitive%2520makeup%2520models%2520using%2520only%25205%2520image%2520pairs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15033v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Tiny%20and%20High-quality%20Facial%20Makeup%20with%20Data%20Amplify%20Learning&entry.906535625=Qiaoqiao%20Jin%20and%20Xuanhong%20Chen%20and%20Meiguang%20Jin%20and%20Ying%20Chen%20and%20Rui%20Shi%20and%20Yucheng%20Zheng%20and%20Yupeng%20Zhu%20and%20Bingbing%20Ni&entry.1292438233=%20%20Contemporary%20makeup%20approaches%20primarily%20hinge%20on%20unpaired%20learning%0Aparadigms%2C%20yet%20they%20grapple%20with%20the%20challenges%20of%20inaccurate%20supervision%0A%28e.g.%2C%20face%20misalignment%29%20and%20sophisticated%20facial%20prompts%20%28including%20face%0Aparsing%2C%20and%20landmark%20detection%29.%20These%20challenges%20prohibit%20low-cost%20deployment%0Aof%20facial%20makeup%20models%2C%20especially%20on%20mobile%20devices.%20To%20solve%20above%20problems%2C%0Awe%20propose%20a%20brand-new%20learning%20paradigm%2C%20termed%20%22Data%20Amplify%20Learning%20%28DAL%29%2C%22%0Aalongside%20a%20compact%20makeup%20model%20named%20%22TinyBeauty.%22%20The%20core%20idea%20of%20DAL%20lies%0Ain%20employing%20a%20Diffusion-based%20Data%20Amplifier%20%28DDA%29%20to%20%22amplify%22%20limited%20images%0Afor%20the%20model%20training%2C%20thereby%20enabling%20accurate%20pixel-to-pixel%20supervision%0Awith%20merely%20a%20handful%20of%20annotations.%20Two%20pivotal%20innovations%20in%20DDA%20facilitate%0Athe%20above%20training%20approach%3A%20%281%29%20A%20Residual%20Diffusion%20Model%20%28RDM%29%20is%20designed%0Ato%20generate%20high-fidelity%20detail%20and%20circumvent%20the%20detail%20vanishing%20problem%20in%0Athe%20vanilla%20diffusion%20models%3B%20%282%29%20A%20Fine-Grained%20Makeup%20Module%20%28FGMM%29%20is%0Aproposed%20to%20achieve%20precise%20makeup%20control%20and%20combination%20while%20retaining%20face%0Aidentity.%20Coupled%20with%20DAL%2C%20TinyBeauty%20necessitates%20merely%2080K%20parameters%20to%0Aachieve%20a%20state-of-the-art%20performance%20without%20intricate%20face%20prompts.%0AMeanwhile%2C%20TinyBeauty%20achieves%20a%20remarkable%20inference%20speed%20of%20up%20to%20460%20fps%20on%0Athe%20iPhone%2013.%20Extensive%20experiments%20show%20that%20DAL%20can%20produce%20highly%0Acompetitive%20makeup%20models%20using%20only%205%20image%20pairs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15033v4&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


