<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250715.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Biomechanics-Guided Residual Approach to Generalizable Human Motion\n  Generation and Estimation", "author": "Zixi Kang and Xinghan Wang and Yadong Mu", "abstract": "  Human pose, action, and motion generation are critical for applications in\ndigital humans, character animation, and humanoid robotics. However, many\nexisting methods struggle to produce physically plausible movements that are\nconsistent with biomechanical principles. Although recent autoregressive and\ndiffusion models deliver impressive visual quality, they often neglect key\nbiodynamic features and fail to ensure physically realistic motions.\nReinforcement Learning (RL) approaches can address these shortcomings but are\nhighly dependent on simulation environments, limiting their generalizability.\nTo overcome these challenges, we propose BioVAE, a biomechanics-aware framework\nwith three core innovations: (1) integration of muscle electromyography (EMG)\nsignals and kinematic features with acceleration constraints to enable\nphysically plausible motion without simulations; (2) seamless coupling with\ndiffusion models for stable end-to-end training; and (3) biomechanical priors\nthat promote strong generalization across diverse motion generation and\nestimation tasks. Extensive experiments demonstrate that BioVAE achieves\nstate-of-the-art performance on multiple benchmarks, bridging the gap between\ndata-driven motion synthesis and biomechanical authenticity while setting new\nstandards for physically accurate motion generation and pose estimation.\n", "link": "http://arxiv.org/abs/2503.06151v2", "date": "2025-07-15", "relevancy": 3.1519, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6663}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6299}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Biomechanics-Guided%20Residual%20Approach%20to%20Generalizable%20Human%20Motion%0A%20%20Generation%20and%20Estimation&body=Title%3A%20Biomechanics-Guided%20Residual%20Approach%20to%20Generalizable%20Human%20Motion%0A%20%20Generation%20and%20Estimation%0AAuthor%3A%20Zixi%20Kang%20and%20Xinghan%20Wang%20and%20Yadong%20Mu%0AAbstract%3A%20%20%20Human%20pose%2C%20action%2C%20and%20motion%20generation%20are%20critical%20for%20applications%20in%0Adigital%20humans%2C%20character%20animation%2C%20and%20humanoid%20robotics.%20However%2C%20many%0Aexisting%20methods%20struggle%20to%20produce%20physically%20plausible%20movements%20that%20are%0Aconsistent%20with%20biomechanical%20principles.%20Although%20recent%20autoregressive%20and%0Adiffusion%20models%20deliver%20impressive%20visual%20quality%2C%20they%20often%20neglect%20key%0Abiodynamic%20features%20and%20fail%20to%20ensure%20physically%20realistic%20motions.%0AReinforcement%20Learning%20%28RL%29%20approaches%20can%20address%20these%20shortcomings%20but%20are%0Ahighly%20dependent%20on%20simulation%20environments%2C%20limiting%20their%20generalizability.%0ATo%20overcome%20these%20challenges%2C%20we%20propose%20BioVAE%2C%20a%20biomechanics-aware%20framework%0Awith%20three%20core%20innovations%3A%20%281%29%20integration%20of%20muscle%20electromyography%20%28EMG%29%0Asignals%20and%20kinematic%20features%20with%20acceleration%20constraints%20to%20enable%0Aphysically%20plausible%20motion%20without%20simulations%3B%20%282%29%20seamless%20coupling%20with%0Adiffusion%20models%20for%20stable%20end-to-end%20training%3B%20and%20%283%29%20biomechanical%20priors%0Athat%20promote%20strong%20generalization%20across%20diverse%20motion%20generation%20and%0Aestimation%20tasks.%20Extensive%20experiments%20demonstrate%20that%20BioVAE%20achieves%0Astate-of-the-art%20performance%20on%20multiple%20benchmarks%2C%20bridging%20the%20gap%20between%0Adata-driven%20motion%20synthesis%20and%20biomechanical%20authenticity%20while%20setting%20new%0Astandards%20for%20physically%20accurate%20motion%20generation%20and%20pose%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.06151v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiomechanics-Guided%2520Residual%2520Approach%2520to%2520Generalizable%2520Human%2520Motion%250A%2520%2520Generation%2520and%2520Estimation%26entry.906535625%3DZixi%2520Kang%2520and%2520Xinghan%2520Wang%2520and%2520Yadong%2520Mu%26entry.1292438233%3D%2520%2520Human%2520pose%252C%2520action%252C%2520and%2520motion%2520generation%2520are%2520critical%2520for%2520applications%2520in%250Adigital%2520humans%252C%2520character%2520animation%252C%2520and%2520humanoid%2520robotics.%2520However%252C%2520many%250Aexisting%2520methods%2520struggle%2520to%2520produce%2520physically%2520plausible%2520movements%2520that%2520are%250Aconsistent%2520with%2520biomechanical%2520principles.%2520Although%2520recent%2520autoregressive%2520and%250Adiffusion%2520models%2520deliver%2520impressive%2520visual%2520quality%252C%2520they%2520often%2520neglect%2520key%250Abiodynamic%2520features%2520and%2520fail%2520to%2520ensure%2520physically%2520realistic%2520motions.%250AReinforcement%2520Learning%2520%2528RL%2529%2520approaches%2520can%2520address%2520these%2520shortcomings%2520but%2520are%250Ahighly%2520dependent%2520on%2520simulation%2520environments%252C%2520limiting%2520their%2520generalizability.%250ATo%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520BioVAE%252C%2520a%2520biomechanics-aware%2520framework%250Awith%2520three%2520core%2520innovations%253A%2520%25281%2529%2520integration%2520of%2520muscle%2520electromyography%2520%2528EMG%2529%250Asignals%2520and%2520kinematic%2520features%2520with%2520acceleration%2520constraints%2520to%2520enable%250Aphysically%2520plausible%2520motion%2520without%2520simulations%253B%2520%25282%2529%2520seamless%2520coupling%2520with%250Adiffusion%2520models%2520for%2520stable%2520end-to-end%2520training%253B%2520and%2520%25283%2529%2520biomechanical%2520priors%250Athat%2520promote%2520strong%2520generalization%2520across%2520diverse%2520motion%2520generation%2520and%250Aestimation%2520tasks.%2520Extensive%2520experiments%2520demonstrate%2520that%2520BioVAE%2520achieves%250Astate-of-the-art%2520performance%2520on%2520multiple%2520benchmarks%252C%2520bridging%2520the%2520gap%2520between%250Adata-driven%2520motion%2520synthesis%2520and%2520biomechanical%2520authenticity%2520while%2520setting%2520new%250Astandards%2520for%2520physically%2520accurate%2520motion%2520generation%2520and%2520pose%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.06151v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Biomechanics-Guided%20Residual%20Approach%20to%20Generalizable%20Human%20Motion%0A%20%20Generation%20and%20Estimation&entry.906535625=Zixi%20Kang%20and%20Xinghan%20Wang%20and%20Yadong%20Mu&entry.1292438233=%20%20Human%20pose%2C%20action%2C%20and%20motion%20generation%20are%20critical%20for%20applications%20in%0Adigital%20humans%2C%20character%20animation%2C%20and%20humanoid%20robotics.%20However%2C%20many%0Aexisting%20methods%20struggle%20to%20produce%20physically%20plausible%20movements%20that%20are%0Aconsistent%20with%20biomechanical%20principles.%20Although%20recent%20autoregressive%20and%0Adiffusion%20models%20deliver%20impressive%20visual%20quality%2C%20they%20often%20neglect%20key%0Abiodynamic%20features%20and%20fail%20to%20ensure%20physically%20realistic%20motions.%0AReinforcement%20Learning%20%28RL%29%20approaches%20can%20address%20these%20shortcomings%20but%20are%0Ahighly%20dependent%20on%20simulation%20environments%2C%20limiting%20their%20generalizability.%0ATo%20overcome%20these%20challenges%2C%20we%20propose%20BioVAE%2C%20a%20biomechanics-aware%20framework%0Awith%20three%20core%20innovations%3A%20%281%29%20integration%20of%20muscle%20electromyography%20%28EMG%29%0Asignals%20and%20kinematic%20features%20with%20acceleration%20constraints%20to%20enable%0Aphysically%20plausible%20motion%20without%20simulations%3B%20%282%29%20seamless%20coupling%20with%0Adiffusion%20models%20for%20stable%20end-to-end%20training%3B%20and%20%283%29%20biomechanical%20priors%0Athat%20promote%20strong%20generalization%20across%20diverse%20motion%20generation%20and%0Aestimation%20tasks.%20Extensive%20experiments%20demonstrate%20that%20BioVAE%20achieves%0Astate-of-the-art%20performance%20on%20multiple%20benchmarks%2C%20bridging%20the%20gap%20between%0Adata-driven%20motion%20synthesis%20and%20biomechanical%20authenticity%20while%20setting%20new%0Astandards%20for%20physically%20accurate%20motion%20generation%20and%20pose%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.06151v2&entry.124074799=Read"},
{"title": "Model See Model Do: Speech-Driven Facial Animation with Style Control", "author": "Yifang Pan and Karan Singh and Luiz Gustavo Hafemann", "abstract": "  Speech-driven 3D facial animation plays a key role in applications such as\nvirtual avatars, gaming, and digital content creation. While existing methods\nhave made significant progress in achieving accurate lip synchronization and\ngenerating basic emotional expressions, they often struggle to capture and\neffectively transfer nuanced performance styles. We propose a novel\nexample-based generation framework that conditions a latent diffusion model on\na reference style clip to produce highly expressive and temporally coherent\nfacial animations. To address the challenge of accurately adhering to the style\nreference, we introduce a novel conditioning mechanism called style basis,\nwhich extracts key poses from the reference and additively guides the diffusion\ngeneration process to fit the style without compromising lip synchronization\nquality. This approach enables the model to capture subtle stylistic cues while\nensuring that the generated animations align closely with the input speech.\nExtensive qualitative, quantitative, and perceptual evaluations demonstrate the\neffectiveness of our method in faithfully reproducing the desired style while\nachieving superior lip synchronization across various speech scenarios.\n", "link": "http://arxiv.org/abs/2505.01319v2", "date": "2025-07-15", "relevancy": 3.1, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6601}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5999}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5999}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20See%20Model%20Do%3A%20Speech-Driven%20Facial%20Animation%20with%20Style%20Control&body=Title%3A%20Model%20See%20Model%20Do%3A%20Speech-Driven%20Facial%20Animation%20with%20Style%20Control%0AAuthor%3A%20Yifang%20Pan%20and%20Karan%20Singh%20and%20Luiz%20Gustavo%20Hafemann%0AAbstract%3A%20%20%20Speech-driven%203D%20facial%20animation%20plays%20a%20key%20role%20in%20applications%20such%20as%0Avirtual%20avatars%2C%20gaming%2C%20and%20digital%20content%20creation.%20While%20existing%20methods%0Ahave%20made%20significant%20progress%20in%20achieving%20accurate%20lip%20synchronization%20and%0Agenerating%20basic%20emotional%20expressions%2C%20they%20often%20struggle%20to%20capture%20and%0Aeffectively%20transfer%20nuanced%20performance%20styles.%20We%20propose%20a%20novel%0Aexample-based%20generation%20framework%20that%20conditions%20a%20latent%20diffusion%20model%20on%0Aa%20reference%20style%20clip%20to%20produce%20highly%20expressive%20and%20temporally%20coherent%0Afacial%20animations.%20To%20address%20the%20challenge%20of%20accurately%20adhering%20to%20the%20style%0Areference%2C%20we%20introduce%20a%20novel%20conditioning%20mechanism%20called%20style%20basis%2C%0Awhich%20extracts%20key%20poses%20from%20the%20reference%20and%20additively%20guides%20the%20diffusion%0Ageneration%20process%20to%20fit%20the%20style%20without%20compromising%20lip%20synchronization%0Aquality.%20This%20approach%20enables%20the%20model%20to%20capture%20subtle%20stylistic%20cues%20while%0Aensuring%20that%20the%20generated%20animations%20align%20closely%20with%20the%20input%20speech.%0AExtensive%20qualitative%2C%20quantitative%2C%20and%20perceptual%20evaluations%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20in%20faithfully%20reproducing%20the%20desired%20style%20while%0Aachieving%20superior%20lip%20synchronization%20across%20various%20speech%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01319v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520See%2520Model%2520Do%253A%2520Speech-Driven%2520Facial%2520Animation%2520with%2520Style%2520Control%26entry.906535625%3DYifang%2520Pan%2520and%2520Karan%2520Singh%2520and%2520Luiz%2520Gustavo%2520Hafemann%26entry.1292438233%3D%2520%2520Speech-driven%25203D%2520facial%2520animation%2520plays%2520a%2520key%2520role%2520in%2520applications%2520such%2520as%250Avirtual%2520avatars%252C%2520gaming%252C%2520and%2520digital%2520content%2520creation.%2520While%2520existing%2520methods%250Ahave%2520made%2520significant%2520progress%2520in%2520achieving%2520accurate%2520lip%2520synchronization%2520and%250Agenerating%2520basic%2520emotional%2520expressions%252C%2520they%2520often%2520struggle%2520to%2520capture%2520and%250Aeffectively%2520transfer%2520nuanced%2520performance%2520styles.%2520We%2520propose%2520a%2520novel%250Aexample-based%2520generation%2520framework%2520that%2520conditions%2520a%2520latent%2520diffusion%2520model%2520on%250Aa%2520reference%2520style%2520clip%2520to%2520produce%2520highly%2520expressive%2520and%2520temporally%2520coherent%250Afacial%2520animations.%2520To%2520address%2520the%2520challenge%2520of%2520accurately%2520adhering%2520to%2520the%2520style%250Areference%252C%2520we%2520introduce%2520a%2520novel%2520conditioning%2520mechanism%2520called%2520style%2520basis%252C%250Awhich%2520extracts%2520key%2520poses%2520from%2520the%2520reference%2520and%2520additively%2520guides%2520the%2520diffusion%250Ageneration%2520process%2520to%2520fit%2520the%2520style%2520without%2520compromising%2520lip%2520synchronization%250Aquality.%2520This%2520approach%2520enables%2520the%2520model%2520to%2520capture%2520subtle%2520stylistic%2520cues%2520while%250Aensuring%2520that%2520the%2520generated%2520animations%2520align%2520closely%2520with%2520the%2520input%2520speech.%250AExtensive%2520qualitative%252C%2520quantitative%252C%2520and%2520perceptual%2520evaluations%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method%2520in%2520faithfully%2520reproducing%2520the%2520desired%2520style%2520while%250Aachieving%2520superior%2520lip%2520synchronization%2520across%2520various%2520speech%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01319v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20See%20Model%20Do%3A%20Speech-Driven%20Facial%20Animation%20with%20Style%20Control&entry.906535625=Yifang%20Pan%20and%20Karan%20Singh%20and%20Luiz%20Gustavo%20Hafemann&entry.1292438233=%20%20Speech-driven%203D%20facial%20animation%20plays%20a%20key%20role%20in%20applications%20such%20as%0Avirtual%20avatars%2C%20gaming%2C%20and%20digital%20content%20creation.%20While%20existing%20methods%0Ahave%20made%20significant%20progress%20in%20achieving%20accurate%20lip%20synchronization%20and%0Agenerating%20basic%20emotional%20expressions%2C%20they%20often%20struggle%20to%20capture%20and%0Aeffectively%20transfer%20nuanced%20performance%20styles.%20We%20propose%20a%20novel%0Aexample-based%20generation%20framework%20that%20conditions%20a%20latent%20diffusion%20model%20on%0Aa%20reference%20style%20clip%20to%20produce%20highly%20expressive%20and%20temporally%20coherent%0Afacial%20animations.%20To%20address%20the%20challenge%20of%20accurately%20adhering%20to%20the%20style%0Areference%2C%20we%20introduce%20a%20novel%20conditioning%20mechanism%20called%20style%20basis%2C%0Awhich%20extracts%20key%20poses%20from%20the%20reference%20and%20additively%20guides%20the%20diffusion%0Ageneration%20process%20to%20fit%20the%20style%20without%20compromising%20lip%20synchronization%0Aquality.%20This%20approach%20enables%20the%20model%20to%20capture%20subtle%20stylistic%20cues%20while%0Aensuring%20that%20the%20generated%20animations%20align%20closely%20with%20the%20input%20speech.%0AExtensive%20qualitative%2C%20quantitative%2C%20and%20perceptual%20evaluations%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20in%20faithfully%20reproducing%20the%20desired%20style%20while%0Aachieving%20superior%20lip%20synchronization%20across%20various%20speech%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01319v2&entry.124074799=Read"},
{"title": "CATVis: Context-Aware Thought Visualization", "author": "Tariq Mehmood and Hamza Ahmad and Muhammad Haroon Shakeel and Murtaza Taj", "abstract": "  EEG-based brain-computer interfaces (BCIs) have shown promise in various\napplications, such as motor imagery and cognitive state monitoring. However,\ndecoding visual representations from EEG signals remains a significant\nchallenge due to their complex and noisy nature. We thus propose a novel\n5-stage framework for decoding visual representations from EEG signals: (1) an\nEEG encoder for concept classification, (2) cross-modal alignment of EEG and\ntext embeddings in CLIP feature space, (3) caption refinement via re-ranking,\n(4) weighted interpolation of concept and caption embeddings for richer\nsemantics, and (5) image generation using a pre-trained Stable Diffusion model.\nWe enable context-aware EEG-to-image generation through cross-modal alignment\nand re-ranking. Experimental results demonstrate that our method generates\nhigh-quality images aligned with visual stimuli, outperforming SOTA approaches\nby 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and\nreducing Fr\\'echet Inception Distance by 36.61%, indicating superior semantic\nalignment and image quality.\n", "link": "http://arxiv.org/abs/2507.11522v1", "date": "2025-07-15", "relevancy": 2.9386, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5899}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5899}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CATVis%3A%20Context-Aware%20Thought%20Visualization&body=Title%3A%20CATVis%3A%20Context-Aware%20Thought%20Visualization%0AAuthor%3A%20Tariq%20Mehmood%20and%20Hamza%20Ahmad%20and%20Muhammad%20Haroon%20Shakeel%20and%20Murtaza%20Taj%0AAbstract%3A%20%20%20EEG-based%20brain-computer%20interfaces%20%28BCIs%29%20have%20shown%20promise%20in%20various%0Aapplications%2C%20such%20as%20motor%20imagery%20and%20cognitive%20state%20monitoring.%20However%2C%0Adecoding%20visual%20representations%20from%20EEG%20signals%20remains%20a%20significant%0Achallenge%20due%20to%20their%20complex%20and%20noisy%20nature.%20We%20thus%20propose%20a%20novel%0A5-stage%20framework%20for%20decoding%20visual%20representations%20from%20EEG%20signals%3A%20%281%29%20an%0AEEG%20encoder%20for%20concept%20classification%2C%20%282%29%20cross-modal%20alignment%20of%20EEG%20and%0Atext%20embeddings%20in%20CLIP%20feature%20space%2C%20%283%29%20caption%20refinement%20via%20re-ranking%2C%0A%284%29%20weighted%20interpolation%20of%20concept%20and%20caption%20embeddings%20for%20richer%0Asemantics%2C%20and%20%285%29%20image%20generation%20using%20a%20pre-trained%20Stable%20Diffusion%20model.%0AWe%20enable%20context-aware%20EEG-to-image%20generation%20through%20cross-modal%20alignment%0Aand%20re-ranking.%20Experimental%20results%20demonstrate%20that%20our%20method%20generates%0Ahigh-quality%20images%20aligned%20with%20visual%20stimuli%2C%20outperforming%20SOTA%20approaches%0Aby%2013.43%25%20in%20Classification%20Accuracy%2C%2015.21%25%20in%20Generation%20Accuracy%20and%0Areducing%20Fr%5C%27echet%20Inception%20Distance%20by%2036.61%25%2C%20indicating%20superior%20semantic%0Aalignment%20and%20image%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11522v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCATVis%253A%2520Context-Aware%2520Thought%2520Visualization%26entry.906535625%3DTariq%2520Mehmood%2520and%2520Hamza%2520Ahmad%2520and%2520Muhammad%2520Haroon%2520Shakeel%2520and%2520Murtaza%2520Taj%26entry.1292438233%3D%2520%2520EEG-based%2520brain-computer%2520interfaces%2520%2528BCIs%2529%2520have%2520shown%2520promise%2520in%2520various%250Aapplications%252C%2520such%2520as%2520motor%2520imagery%2520and%2520cognitive%2520state%2520monitoring.%2520However%252C%250Adecoding%2520visual%2520representations%2520from%2520EEG%2520signals%2520remains%2520a%2520significant%250Achallenge%2520due%2520to%2520their%2520complex%2520and%2520noisy%2520nature.%2520We%2520thus%2520propose%2520a%2520novel%250A5-stage%2520framework%2520for%2520decoding%2520visual%2520representations%2520from%2520EEG%2520signals%253A%2520%25281%2529%2520an%250AEEG%2520encoder%2520for%2520concept%2520classification%252C%2520%25282%2529%2520cross-modal%2520alignment%2520of%2520EEG%2520and%250Atext%2520embeddings%2520in%2520CLIP%2520feature%2520space%252C%2520%25283%2529%2520caption%2520refinement%2520via%2520re-ranking%252C%250A%25284%2529%2520weighted%2520interpolation%2520of%2520concept%2520and%2520caption%2520embeddings%2520for%2520richer%250Asemantics%252C%2520and%2520%25285%2529%2520image%2520generation%2520using%2520a%2520pre-trained%2520Stable%2520Diffusion%2520model.%250AWe%2520enable%2520context-aware%2520EEG-to-image%2520generation%2520through%2520cross-modal%2520alignment%250Aand%2520re-ranking.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520method%2520generates%250Ahigh-quality%2520images%2520aligned%2520with%2520visual%2520stimuli%252C%2520outperforming%2520SOTA%2520approaches%250Aby%252013.43%2525%2520in%2520Classification%2520Accuracy%252C%252015.21%2525%2520in%2520Generation%2520Accuracy%2520and%250Areducing%2520Fr%255C%2527echet%2520Inception%2520Distance%2520by%252036.61%2525%252C%2520indicating%2520superior%2520semantic%250Aalignment%2520and%2520image%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11522v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CATVis%3A%20Context-Aware%20Thought%20Visualization&entry.906535625=Tariq%20Mehmood%20and%20Hamza%20Ahmad%20and%20Muhammad%20Haroon%20Shakeel%20and%20Murtaza%20Taj&entry.1292438233=%20%20EEG-based%20brain-computer%20interfaces%20%28BCIs%29%20have%20shown%20promise%20in%20various%0Aapplications%2C%20such%20as%20motor%20imagery%20and%20cognitive%20state%20monitoring.%20However%2C%0Adecoding%20visual%20representations%20from%20EEG%20signals%20remains%20a%20significant%0Achallenge%20due%20to%20their%20complex%20and%20noisy%20nature.%20We%20thus%20propose%20a%20novel%0A5-stage%20framework%20for%20decoding%20visual%20representations%20from%20EEG%20signals%3A%20%281%29%20an%0AEEG%20encoder%20for%20concept%20classification%2C%20%282%29%20cross-modal%20alignment%20of%20EEG%20and%0Atext%20embeddings%20in%20CLIP%20feature%20space%2C%20%283%29%20caption%20refinement%20via%20re-ranking%2C%0A%284%29%20weighted%20interpolation%20of%20concept%20and%20caption%20embeddings%20for%20richer%0Asemantics%2C%20and%20%285%29%20image%20generation%20using%20a%20pre-trained%20Stable%20Diffusion%20model.%0AWe%20enable%20context-aware%20EEG-to-image%20generation%20through%20cross-modal%20alignment%0Aand%20re-ranking.%20Experimental%20results%20demonstrate%20that%20our%20method%20generates%0Ahigh-quality%20images%20aligned%20with%20visual%20stimuli%2C%20outperforming%20SOTA%20approaches%0Aby%2013.43%25%20in%20Classification%20Accuracy%2C%2015.21%25%20in%20Generation%20Accuracy%20and%0Areducing%20Fr%5C%27echet%20Inception%20Distance%20by%2036.61%25%2C%20indicating%20superior%20semantic%0Aalignment%20and%20image%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11522v1&entry.124074799=Read"},
{"title": "Elevating 3D Models: High-Quality Texture and Geometry Refinement from a\n  Low-Quality Model", "author": "Nuri Ryu and Jiyun Won and Jooeun Son and Minsu Gong and Joo-Haeng Lee and Sunghyun Cho", "abstract": "  High-quality 3D assets are essential for various applications in computer\ngraphics and 3D vision but remain scarce due to significant acquisition costs.\nTo address this shortage, we introduce Elevate3D, a novel framework that\ntransforms readily accessible low-quality 3D assets into higher quality. At the\ncore of Elevate3D is HFS-SDEdit, a specialized texture enhancement method that\nsignificantly improves texture quality while preserving the appearance and\ngeometry while fixing its degradations. Furthermore, Elevate3D operates in a\nview-by-view manner, alternating between texture and geometry refinement.\nUnlike previous methods that have largely overlooked geometry refinement, our\nframework leverages geometric cues from images refined with HFS-SDEdit by\nemploying state-of-the-art monocular geometry predictors. This approach ensures\ndetailed and accurate geometry that aligns seamlessly with the enhanced\ntexture. Elevate3D outperforms recent competitors by achieving state-of-the-art\nquality in 3D model refinement, effectively addressing the scarcity of\nhigh-quality open-source 3D assets.\n", "link": "http://arxiv.org/abs/2507.11465v1", "date": "2025-07-15", "relevancy": 2.8273, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5753}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5753}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5458}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Elevating%203D%20Models%3A%20High-Quality%20Texture%20and%20Geometry%20Refinement%20from%20a%0A%20%20Low-Quality%20Model&body=Title%3A%20Elevating%203D%20Models%3A%20High-Quality%20Texture%20and%20Geometry%20Refinement%20from%20a%0A%20%20Low-Quality%20Model%0AAuthor%3A%20Nuri%20Ryu%20and%20Jiyun%20Won%20and%20Jooeun%20Son%20and%20Minsu%20Gong%20and%20Joo-Haeng%20Lee%20and%20Sunghyun%20Cho%0AAbstract%3A%20%20%20High-quality%203D%20assets%20are%20essential%20for%20various%20applications%20in%20computer%0Agraphics%20and%203D%20vision%20but%20remain%20scarce%20due%20to%20significant%20acquisition%20costs.%0ATo%20address%20this%20shortage%2C%20we%20introduce%20Elevate3D%2C%20a%20novel%20framework%20that%0Atransforms%20readily%20accessible%20low-quality%203D%20assets%20into%20higher%20quality.%20At%20the%0Acore%20of%20Elevate3D%20is%20HFS-SDEdit%2C%20a%20specialized%20texture%20enhancement%20method%20that%0Asignificantly%20improves%20texture%20quality%20while%20preserving%20the%20appearance%20and%0Ageometry%20while%20fixing%20its%20degradations.%20Furthermore%2C%20Elevate3D%20operates%20in%20a%0Aview-by-view%20manner%2C%20alternating%20between%20texture%20and%20geometry%20refinement.%0AUnlike%20previous%20methods%20that%20have%20largely%20overlooked%20geometry%20refinement%2C%20our%0Aframework%20leverages%20geometric%20cues%20from%20images%20refined%20with%20HFS-SDEdit%20by%0Aemploying%20state-of-the-art%20monocular%20geometry%20predictors.%20This%20approach%20ensures%0Adetailed%20and%20accurate%20geometry%20that%20aligns%20seamlessly%20with%20the%20enhanced%0Atexture.%20Elevate3D%20outperforms%20recent%20competitors%20by%20achieving%20state-of-the-art%0Aquality%20in%203D%20model%20refinement%2C%20effectively%20addressing%20the%20scarcity%20of%0Ahigh-quality%20open-source%203D%20assets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DElevating%25203D%2520Models%253A%2520High-Quality%2520Texture%2520and%2520Geometry%2520Refinement%2520from%2520a%250A%2520%2520Low-Quality%2520Model%26entry.906535625%3DNuri%2520Ryu%2520and%2520Jiyun%2520Won%2520and%2520Jooeun%2520Son%2520and%2520Minsu%2520Gong%2520and%2520Joo-Haeng%2520Lee%2520and%2520Sunghyun%2520Cho%26entry.1292438233%3D%2520%2520High-quality%25203D%2520assets%2520are%2520essential%2520for%2520various%2520applications%2520in%2520computer%250Agraphics%2520and%25203D%2520vision%2520but%2520remain%2520scarce%2520due%2520to%2520significant%2520acquisition%2520costs.%250ATo%2520address%2520this%2520shortage%252C%2520we%2520introduce%2520Elevate3D%252C%2520a%2520novel%2520framework%2520that%250Atransforms%2520readily%2520accessible%2520low-quality%25203D%2520assets%2520into%2520higher%2520quality.%2520At%2520the%250Acore%2520of%2520Elevate3D%2520is%2520HFS-SDEdit%252C%2520a%2520specialized%2520texture%2520enhancement%2520method%2520that%250Asignificantly%2520improves%2520texture%2520quality%2520while%2520preserving%2520the%2520appearance%2520and%250Ageometry%2520while%2520fixing%2520its%2520degradations.%2520Furthermore%252C%2520Elevate3D%2520operates%2520in%2520a%250Aview-by-view%2520manner%252C%2520alternating%2520between%2520texture%2520and%2520geometry%2520refinement.%250AUnlike%2520previous%2520methods%2520that%2520have%2520largely%2520overlooked%2520geometry%2520refinement%252C%2520our%250Aframework%2520leverages%2520geometric%2520cues%2520from%2520images%2520refined%2520with%2520HFS-SDEdit%2520by%250Aemploying%2520state-of-the-art%2520monocular%2520geometry%2520predictors.%2520This%2520approach%2520ensures%250Adetailed%2520and%2520accurate%2520geometry%2520that%2520aligns%2520seamlessly%2520with%2520the%2520enhanced%250Atexture.%2520Elevate3D%2520outperforms%2520recent%2520competitors%2520by%2520achieving%2520state-of-the-art%250Aquality%2520in%25203D%2520model%2520refinement%252C%2520effectively%2520addressing%2520the%2520scarcity%2520of%250Ahigh-quality%2520open-source%25203D%2520assets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Elevating%203D%20Models%3A%20High-Quality%20Texture%20and%20Geometry%20Refinement%20from%20a%0A%20%20Low-Quality%20Model&entry.906535625=Nuri%20Ryu%20and%20Jiyun%20Won%20and%20Jooeun%20Son%20and%20Minsu%20Gong%20and%20Joo-Haeng%20Lee%20and%20Sunghyun%20Cho&entry.1292438233=%20%20High-quality%203D%20assets%20are%20essential%20for%20various%20applications%20in%20computer%0Agraphics%20and%203D%20vision%20but%20remain%20scarce%20due%20to%20significant%20acquisition%20costs.%0ATo%20address%20this%20shortage%2C%20we%20introduce%20Elevate3D%2C%20a%20novel%20framework%20that%0Atransforms%20readily%20accessible%20low-quality%203D%20assets%20into%20higher%20quality.%20At%20the%0Acore%20of%20Elevate3D%20is%20HFS-SDEdit%2C%20a%20specialized%20texture%20enhancement%20method%20that%0Asignificantly%20improves%20texture%20quality%20while%20preserving%20the%20appearance%20and%0Ageometry%20while%20fixing%20its%20degradations.%20Furthermore%2C%20Elevate3D%20operates%20in%20a%0Aview-by-view%20manner%2C%20alternating%20between%20texture%20and%20geometry%20refinement.%0AUnlike%20previous%20methods%20that%20have%20largely%20overlooked%20geometry%20refinement%2C%20our%0Aframework%20leverages%20geometric%20cues%20from%20images%20refined%20with%20HFS-SDEdit%20by%0Aemploying%20state-of-the-art%20monocular%20geometry%20predictors.%20This%20approach%20ensures%0Adetailed%20and%20accurate%20geometry%20that%20aligns%20seamlessly%20with%20the%20enhanced%0Atexture.%20Elevate3D%20outperforms%20recent%20competitors%20by%20achieving%20state-of-the-art%0Aquality%20in%203D%20model%20refinement%2C%20effectively%20addressing%20the%20scarcity%20of%0Ahigh-quality%20open-source%203D%20assets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11465v1&entry.124074799=Read"},
{"title": "COLI: A Hierarchical Efficient Compressor for Large Images", "author": "Haoran Wang and Hanyu Pei and Yang Lyu and Kai Zhang and Li Li and Feng-Lei Fan", "abstract": "  The escalating adoption of high-resolution, large-field-of-view imagery\namplifies the need for efficient compression methodologies. Conventional\ntechniques frequently fail to preserve critical image details, while\ndata-driven approaches exhibit limited generalizability. Implicit Neural\nRepresentations (INRs) present a promising alternative by learning continuous\nmappings from spatial coordinates to pixel intensities for individual images,\nthereby storing network weights rather than raw pixels and avoiding the\ngeneralization problem. However, INR-based compression of large images faces\nchallenges including slow compression speed and suboptimal compression ratios.\nTo address these limitations, we introduce COLI (Compressor for Large Images),\na novel framework leveraging Neural Representations for Videos (NeRV). First,\nrecognizing that INR-based compression constitutes a training process, we\naccelerate its convergence through a pretraining-finetuning paradigm,\nmixed-precision training, and reformulation of the sequential loss into a\nparallelizable objective. Second, capitalizing on INRs' transformation of image\nstorage constraints into weight storage, we implement Hyper-Compression, a\nnovel post-training technique to substantially enhance compression ratios while\nmaintaining minimal output distortion. Evaluations across two medical imaging\ndatasets demonstrate that COLI consistently achieves competitive or superior\nPSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while\naccelerating NeRV training by up to 4 times.\n", "link": "http://arxiv.org/abs/2507.11443v1", "date": "2025-07-15", "relevancy": 2.6901, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5469}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5356}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COLI%3A%20A%20Hierarchical%20Efficient%20Compressor%20for%20Large%20Images&body=Title%3A%20COLI%3A%20A%20Hierarchical%20Efficient%20Compressor%20for%20Large%20Images%0AAuthor%3A%20Haoran%20Wang%20and%20Hanyu%20Pei%20and%20Yang%20Lyu%20and%20Kai%20Zhang%20and%20Li%20Li%20and%20Feng-Lei%20Fan%0AAbstract%3A%20%20%20The%20escalating%20adoption%20of%20high-resolution%2C%20large-field-of-view%20imagery%0Aamplifies%20the%20need%20for%20efficient%20compression%20methodologies.%20Conventional%0Atechniques%20frequently%20fail%20to%20preserve%20critical%20image%20details%2C%20while%0Adata-driven%20approaches%20exhibit%20limited%20generalizability.%20Implicit%20Neural%0ARepresentations%20%28INRs%29%20present%20a%20promising%20alternative%20by%20learning%20continuous%0Amappings%20from%20spatial%20coordinates%20to%20pixel%20intensities%20for%20individual%20images%2C%0Athereby%20storing%20network%20weights%20rather%20than%20raw%20pixels%20and%20avoiding%20the%0Ageneralization%20problem.%20However%2C%20INR-based%20compression%20of%20large%20images%20faces%0Achallenges%20including%20slow%20compression%20speed%20and%20suboptimal%20compression%20ratios.%0ATo%20address%20these%20limitations%2C%20we%20introduce%20COLI%20%28Compressor%20for%20Large%20Images%29%2C%0Aa%20novel%20framework%20leveraging%20Neural%20Representations%20for%20Videos%20%28NeRV%29.%20First%2C%0Arecognizing%20that%20INR-based%20compression%20constitutes%20a%20training%20process%2C%20we%0Aaccelerate%20its%20convergence%20through%20a%20pretraining-finetuning%20paradigm%2C%0Amixed-precision%20training%2C%20and%20reformulation%20of%20the%20sequential%20loss%20into%20a%0Aparallelizable%20objective.%20Second%2C%20capitalizing%20on%20INRs%27%20transformation%20of%20image%0Astorage%20constraints%20into%20weight%20storage%2C%20we%20implement%20Hyper-Compression%2C%20a%0Anovel%20post-training%20technique%20to%20substantially%20enhance%20compression%20ratios%20while%0Amaintaining%20minimal%20output%20distortion.%20Evaluations%20across%20two%20medical%20imaging%0Adatasets%20demonstrate%20that%20COLI%20consistently%20achieves%20competitive%20or%20superior%0APSNR%20and%20SSIM%20metrics%20at%20significantly%20reduced%20bits%20per%20pixel%20%28bpp%29%2C%20while%0Aaccelerating%20NeRV%20training%20by%20up%20to%204%20times.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOLI%253A%2520A%2520Hierarchical%2520Efficient%2520Compressor%2520for%2520Large%2520Images%26entry.906535625%3DHaoran%2520Wang%2520and%2520Hanyu%2520Pei%2520and%2520Yang%2520Lyu%2520and%2520Kai%2520Zhang%2520and%2520Li%2520Li%2520and%2520Feng-Lei%2520Fan%26entry.1292438233%3D%2520%2520The%2520escalating%2520adoption%2520of%2520high-resolution%252C%2520large-field-of-view%2520imagery%250Aamplifies%2520the%2520need%2520for%2520efficient%2520compression%2520methodologies.%2520Conventional%250Atechniques%2520frequently%2520fail%2520to%2520preserve%2520critical%2520image%2520details%252C%2520while%250Adata-driven%2520approaches%2520exhibit%2520limited%2520generalizability.%2520Implicit%2520Neural%250ARepresentations%2520%2528INRs%2529%2520present%2520a%2520promising%2520alternative%2520by%2520learning%2520continuous%250Amappings%2520from%2520spatial%2520coordinates%2520to%2520pixel%2520intensities%2520for%2520individual%2520images%252C%250Athereby%2520storing%2520network%2520weights%2520rather%2520than%2520raw%2520pixels%2520and%2520avoiding%2520the%250Ageneralization%2520problem.%2520However%252C%2520INR-based%2520compression%2520of%2520large%2520images%2520faces%250Achallenges%2520including%2520slow%2520compression%2520speed%2520and%2520suboptimal%2520compression%2520ratios.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520COLI%2520%2528Compressor%2520for%2520Large%2520Images%2529%252C%250Aa%2520novel%2520framework%2520leveraging%2520Neural%2520Representations%2520for%2520Videos%2520%2528NeRV%2529.%2520First%252C%250Arecognizing%2520that%2520INR-based%2520compression%2520constitutes%2520a%2520training%2520process%252C%2520we%250Aaccelerate%2520its%2520convergence%2520through%2520a%2520pretraining-finetuning%2520paradigm%252C%250Amixed-precision%2520training%252C%2520and%2520reformulation%2520of%2520the%2520sequential%2520loss%2520into%2520a%250Aparallelizable%2520objective.%2520Second%252C%2520capitalizing%2520on%2520INRs%2527%2520transformation%2520of%2520image%250Astorage%2520constraints%2520into%2520weight%2520storage%252C%2520we%2520implement%2520Hyper-Compression%252C%2520a%250Anovel%2520post-training%2520technique%2520to%2520substantially%2520enhance%2520compression%2520ratios%2520while%250Amaintaining%2520minimal%2520output%2520distortion.%2520Evaluations%2520across%2520two%2520medical%2520imaging%250Adatasets%2520demonstrate%2520that%2520COLI%2520consistently%2520achieves%2520competitive%2520or%2520superior%250APSNR%2520and%2520SSIM%2520metrics%2520at%2520significantly%2520reduced%2520bits%2520per%2520pixel%2520%2528bpp%2529%252C%2520while%250Aaccelerating%2520NeRV%2520training%2520by%2520up%2520to%25204%2520times.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COLI%3A%20A%20Hierarchical%20Efficient%20Compressor%20for%20Large%20Images&entry.906535625=Haoran%20Wang%20and%20Hanyu%20Pei%20and%20Yang%20Lyu%20and%20Kai%20Zhang%20and%20Li%20Li%20and%20Feng-Lei%20Fan&entry.1292438233=%20%20The%20escalating%20adoption%20of%20high-resolution%2C%20large-field-of-view%20imagery%0Aamplifies%20the%20need%20for%20efficient%20compression%20methodologies.%20Conventional%0Atechniques%20frequently%20fail%20to%20preserve%20critical%20image%20details%2C%20while%0Adata-driven%20approaches%20exhibit%20limited%20generalizability.%20Implicit%20Neural%0ARepresentations%20%28INRs%29%20present%20a%20promising%20alternative%20by%20learning%20continuous%0Amappings%20from%20spatial%20coordinates%20to%20pixel%20intensities%20for%20individual%20images%2C%0Athereby%20storing%20network%20weights%20rather%20than%20raw%20pixels%20and%20avoiding%20the%0Ageneralization%20problem.%20However%2C%20INR-based%20compression%20of%20large%20images%20faces%0Achallenges%20including%20slow%20compression%20speed%20and%20suboptimal%20compression%20ratios.%0ATo%20address%20these%20limitations%2C%20we%20introduce%20COLI%20%28Compressor%20for%20Large%20Images%29%2C%0Aa%20novel%20framework%20leveraging%20Neural%20Representations%20for%20Videos%20%28NeRV%29.%20First%2C%0Arecognizing%20that%20INR-based%20compression%20constitutes%20a%20training%20process%2C%20we%0Aaccelerate%20its%20convergence%20through%20a%20pretraining-finetuning%20paradigm%2C%0Amixed-precision%20training%2C%20and%20reformulation%20of%20the%20sequential%20loss%20into%20a%0Aparallelizable%20objective.%20Second%2C%20capitalizing%20on%20INRs%27%20transformation%20of%20image%0Astorage%20constraints%20into%20weight%20storage%2C%20we%20implement%20Hyper-Compression%2C%20a%0Anovel%20post-training%20technique%20to%20substantially%20enhance%20compression%20ratios%20while%0Amaintaining%20minimal%20output%20distortion.%20Evaluations%20across%20two%20medical%20imaging%0Adatasets%20demonstrate%20that%20COLI%20consistently%20achieves%20competitive%20or%20superior%0APSNR%20and%20SSIM%20metrics%20at%20significantly%20reduced%20bits%20per%20pixel%20%28bpp%29%2C%20while%0Aaccelerating%20NeRV%20training%20by%20up%20to%204%20times.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11443v1&entry.124074799=Read"},
{"title": "COLIBRI Fuzzy Model: Color Linguistic-Based Representation and\n  Interpretation", "author": "Pakizar Shamoi and Nuray Toganas and Muragul Muratbekova and Elnara Kadyrgali and Adilet Yerkin and Ayan Igali and Malika Ziyada and Ayana Adilova and Aron Karatayev and Yerdauit Torekhan", "abstract": "  Colors are omnipresent in today's world and play a vital role in how humans\nperceive and interact with their surroundings. However, it is challenging for\ncomputers to imitate human color perception. This paper introduces the Human\nPerception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based\nRepresentation and Interpretation), designed to bridge the gap between\ncomputational color representations and human visual perception. The proposed\nmodel uses fuzzy sets and logic to create a framework for color categorization.\nUsing a three-phase experimental approach, the study first identifies\ndistinguishable color stimuli for hue, saturation, and intensity through\npreliminary experiments, followed by a large-scale human categorization survey\ninvolving more than 1000 human subjects. The resulting data are used to extract\nfuzzy partitions and generate membership functions that reflect real-world\nperceptual uncertainty. The model incorporates a mechanism for adaptation that\nallows refinement based on feedback and contextual changes. Comparative\nevaluations demonstrate the model's alignment with human perception compared to\ntraditional color models, such as RGB, HSV, and LAB. To the best of our\nknowledge, no previous research has documented the construction of a model for\ncolor attribute specification based on a sample of this size or a comparable\nsample of the human population (n = 2496). Our findings are significant for\nfields such as design, artificial intelligence, marketing, and human-computer\ninteraction, where perceptually relevant color representation is critical.\n", "link": "http://arxiv.org/abs/2507.11488v1", "date": "2025-07-15", "relevancy": 2.683, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5397}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5397}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COLIBRI%20Fuzzy%20Model%3A%20Color%20Linguistic-Based%20Representation%20and%0A%20%20Interpretation&body=Title%3A%20COLIBRI%20Fuzzy%20Model%3A%20Color%20Linguistic-Based%20Representation%20and%0A%20%20Interpretation%0AAuthor%3A%20Pakizar%20Shamoi%20and%20Nuray%20Toganas%20and%20Muragul%20Muratbekova%20and%20Elnara%20Kadyrgali%20and%20Adilet%20Yerkin%20and%20Ayan%20Igali%20and%20Malika%20Ziyada%20and%20Ayana%20Adilova%20and%20Aron%20Karatayev%20and%20Yerdauit%20Torekhan%0AAbstract%3A%20%20%20Colors%20are%20omnipresent%20in%20today%27s%20world%20and%20play%20a%20vital%20role%20in%20how%20humans%0Aperceive%20and%20interact%20with%20their%20surroundings.%20However%2C%20it%20is%20challenging%20for%0Acomputers%20to%20imitate%20human%20color%20perception.%20This%20paper%20introduces%20the%20Human%0APerception-Based%20Fuzzy%20Color%20Model%2C%20COLIBRI%20%28Color%20Linguistic-Based%0ARepresentation%20and%20Interpretation%29%2C%20designed%20to%20bridge%20the%20gap%20between%0Acomputational%20color%20representations%20and%20human%20visual%20perception.%20The%20proposed%0Amodel%20uses%20fuzzy%20sets%20and%20logic%20to%20create%20a%20framework%20for%20color%20categorization.%0AUsing%20a%20three-phase%20experimental%20approach%2C%20the%20study%20first%20identifies%0Adistinguishable%20color%20stimuli%20for%20hue%2C%20saturation%2C%20and%20intensity%20through%0Apreliminary%20experiments%2C%20followed%20by%20a%20large-scale%20human%20categorization%20survey%0Ainvolving%20more%20than%201000%20human%20subjects.%20The%20resulting%20data%20are%20used%20to%20extract%0Afuzzy%20partitions%20and%20generate%20membership%20functions%20that%20reflect%20real-world%0Aperceptual%20uncertainty.%20The%20model%20incorporates%20a%20mechanism%20for%20adaptation%20that%0Aallows%20refinement%20based%20on%20feedback%20and%20contextual%20changes.%20Comparative%0Aevaluations%20demonstrate%20the%20model%27s%20alignment%20with%20human%20perception%20compared%20to%0Atraditional%20color%20models%2C%20such%20as%20RGB%2C%20HSV%2C%20and%20LAB.%20To%20the%20best%20of%20our%0Aknowledge%2C%20no%20previous%20research%20has%20documented%20the%20construction%20of%20a%20model%20for%0Acolor%20attribute%20specification%20based%20on%20a%20sample%20of%20this%20size%20or%20a%20comparable%0Asample%20of%20the%20human%20population%20%28n%20%3D%202496%29.%20Our%20findings%20are%20significant%20for%0Afields%20such%20as%20design%2C%20artificial%20intelligence%2C%20marketing%2C%20and%20human-computer%0Ainteraction%2C%20where%20perceptually%20relevant%20color%20representation%20is%20critical.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOLIBRI%2520Fuzzy%2520Model%253A%2520Color%2520Linguistic-Based%2520Representation%2520and%250A%2520%2520Interpretation%26entry.906535625%3DPakizar%2520Shamoi%2520and%2520Nuray%2520Toganas%2520and%2520Muragul%2520Muratbekova%2520and%2520Elnara%2520Kadyrgali%2520and%2520Adilet%2520Yerkin%2520and%2520Ayan%2520Igali%2520and%2520Malika%2520Ziyada%2520and%2520Ayana%2520Adilova%2520and%2520Aron%2520Karatayev%2520and%2520Yerdauit%2520Torekhan%26entry.1292438233%3D%2520%2520Colors%2520are%2520omnipresent%2520in%2520today%2527s%2520world%2520and%2520play%2520a%2520vital%2520role%2520in%2520how%2520humans%250Aperceive%2520and%2520interact%2520with%2520their%2520surroundings.%2520However%252C%2520it%2520is%2520challenging%2520for%250Acomputers%2520to%2520imitate%2520human%2520color%2520perception.%2520This%2520paper%2520introduces%2520the%2520Human%250APerception-Based%2520Fuzzy%2520Color%2520Model%252C%2520COLIBRI%2520%2528Color%2520Linguistic-Based%250ARepresentation%2520and%2520Interpretation%2529%252C%2520designed%2520to%2520bridge%2520the%2520gap%2520between%250Acomputational%2520color%2520representations%2520and%2520human%2520visual%2520perception.%2520The%2520proposed%250Amodel%2520uses%2520fuzzy%2520sets%2520and%2520logic%2520to%2520create%2520a%2520framework%2520for%2520color%2520categorization.%250AUsing%2520a%2520three-phase%2520experimental%2520approach%252C%2520the%2520study%2520first%2520identifies%250Adistinguishable%2520color%2520stimuli%2520for%2520hue%252C%2520saturation%252C%2520and%2520intensity%2520through%250Apreliminary%2520experiments%252C%2520followed%2520by%2520a%2520large-scale%2520human%2520categorization%2520survey%250Ainvolving%2520more%2520than%25201000%2520human%2520subjects.%2520The%2520resulting%2520data%2520are%2520used%2520to%2520extract%250Afuzzy%2520partitions%2520and%2520generate%2520membership%2520functions%2520that%2520reflect%2520real-world%250Aperceptual%2520uncertainty.%2520The%2520model%2520incorporates%2520a%2520mechanism%2520for%2520adaptation%2520that%250Aallows%2520refinement%2520based%2520on%2520feedback%2520and%2520contextual%2520changes.%2520Comparative%250Aevaluations%2520demonstrate%2520the%2520model%2527s%2520alignment%2520with%2520human%2520perception%2520compared%2520to%250Atraditional%2520color%2520models%252C%2520such%2520as%2520RGB%252C%2520HSV%252C%2520and%2520LAB.%2520To%2520the%2520best%2520of%2520our%250Aknowledge%252C%2520no%2520previous%2520research%2520has%2520documented%2520the%2520construction%2520of%2520a%2520model%2520for%250Acolor%2520attribute%2520specification%2520based%2520on%2520a%2520sample%2520of%2520this%2520size%2520or%2520a%2520comparable%250Asample%2520of%2520the%2520human%2520population%2520%2528n%2520%253D%25202496%2529.%2520Our%2520findings%2520are%2520significant%2520for%250Afields%2520such%2520as%2520design%252C%2520artificial%2520intelligence%252C%2520marketing%252C%2520and%2520human-computer%250Ainteraction%252C%2520where%2520perceptually%2520relevant%2520color%2520representation%2520is%2520critical.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COLIBRI%20Fuzzy%20Model%3A%20Color%20Linguistic-Based%20Representation%20and%0A%20%20Interpretation&entry.906535625=Pakizar%20Shamoi%20and%20Nuray%20Toganas%20and%20Muragul%20Muratbekova%20and%20Elnara%20Kadyrgali%20and%20Adilet%20Yerkin%20and%20Ayan%20Igali%20and%20Malika%20Ziyada%20and%20Ayana%20Adilova%20and%20Aron%20Karatayev%20and%20Yerdauit%20Torekhan&entry.1292438233=%20%20Colors%20are%20omnipresent%20in%20today%27s%20world%20and%20play%20a%20vital%20role%20in%20how%20humans%0Aperceive%20and%20interact%20with%20their%20surroundings.%20However%2C%20it%20is%20challenging%20for%0Acomputers%20to%20imitate%20human%20color%20perception.%20This%20paper%20introduces%20the%20Human%0APerception-Based%20Fuzzy%20Color%20Model%2C%20COLIBRI%20%28Color%20Linguistic-Based%0ARepresentation%20and%20Interpretation%29%2C%20designed%20to%20bridge%20the%20gap%20between%0Acomputational%20color%20representations%20and%20human%20visual%20perception.%20The%20proposed%0Amodel%20uses%20fuzzy%20sets%20and%20logic%20to%20create%20a%20framework%20for%20color%20categorization.%0AUsing%20a%20three-phase%20experimental%20approach%2C%20the%20study%20first%20identifies%0Adistinguishable%20color%20stimuli%20for%20hue%2C%20saturation%2C%20and%20intensity%20through%0Apreliminary%20experiments%2C%20followed%20by%20a%20large-scale%20human%20categorization%20survey%0Ainvolving%20more%20than%201000%20human%20subjects.%20The%20resulting%20data%20are%20used%20to%20extract%0Afuzzy%20partitions%20and%20generate%20membership%20functions%20that%20reflect%20real-world%0Aperceptual%20uncertainty.%20The%20model%20incorporates%20a%20mechanism%20for%20adaptation%20that%0Aallows%20refinement%20based%20on%20feedback%20and%20contextual%20changes.%20Comparative%0Aevaluations%20demonstrate%20the%20model%27s%20alignment%20with%20human%20perception%20compared%20to%0Atraditional%20color%20models%2C%20such%20as%20RGB%2C%20HSV%2C%20and%20LAB.%20To%20the%20best%20of%20our%0Aknowledge%2C%20no%20previous%20research%20has%20documented%20the%20construction%20of%20a%20model%20for%0Acolor%20attribute%20specification%20based%20on%20a%20sample%20of%20this%20size%20or%20a%20comparable%0Asample%20of%20the%20human%20population%20%28n%20%3D%202496%29.%20Our%20findings%20are%20significant%20for%0Afields%20such%20as%20design%2C%20artificial%20intelligence%2C%20marketing%2C%20and%20human-computer%0Ainteraction%2C%20where%20perceptually%20relevant%20color%20representation%20is%20critical.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11488v1&entry.124074799=Read"},
{"title": "Towards Depth Foundation Model: Recent Trends in Vision-Based Depth\n  Estimation", "author": "Zhen Xu and Hongyu Zhou and Sida Peng and Haotong Lin and Haoyu Guo and Jiahao Shao and Peishan Yang and Qinglin Yang and Sheng Miao and Xingyi He and Yifan Wang and Yue Wang and Ruizhen Hu and Yiyi Liao and Xiaowei Zhou and Hujun Bao", "abstract": "  Depth estimation is a fundamental task in 3D computer vision, crucial for\napplications such as 3D reconstruction, free-viewpoint rendering, robotics,\nautonomous driving, and AR/VR technologies. Traditional methods relying on\nhardware sensors like LiDAR are often limited by high costs, low resolution,\nand environmental sensitivity, limiting their applicability in real-world\nscenarios. Recent advances in vision-based methods offer a promising\nalternative, yet they face challenges in generalization and stability due to\neither the low-capacity model architectures or the reliance on domain-specific\nand small-scale datasets. The emergence of scaling laws and foundation models\nin other domains has inspired the development of \"depth foundation models\":\ndeep neural networks trained on large datasets with strong zero-shot\ngeneralization capabilities. This paper surveys the evolution of deep learning\narchitectures and paradigms for depth estimation across the monocular, stereo,\nmulti-view, and monocular video settings. We explore the potential of these\nmodels to address existing challenges and provide a comprehensive overview of\nlarge-scale datasets that can facilitate their development. By identifying key\narchitectures and training strategies, we aim to highlight the path towards\nrobust depth foundation models, offering insights into their future research\nand applications.\n", "link": "http://arxiv.org/abs/2507.11540v1", "date": "2025-07-15", "relevancy": 2.6447, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6787}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6787}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5735}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Depth%20Foundation%20Model%3A%20Recent%20Trends%20in%20Vision-Based%20Depth%0A%20%20Estimation&body=Title%3A%20Towards%20Depth%20Foundation%20Model%3A%20Recent%20Trends%20in%20Vision-Based%20Depth%0A%20%20Estimation%0AAuthor%3A%20Zhen%20Xu%20and%20Hongyu%20Zhou%20and%20Sida%20Peng%20and%20Haotong%20Lin%20and%20Haoyu%20Guo%20and%20Jiahao%20Shao%20and%20Peishan%20Yang%20and%20Qinglin%20Yang%20and%20Sheng%20Miao%20and%20Xingyi%20He%20and%20Yifan%20Wang%20and%20Yue%20Wang%20and%20Ruizhen%20Hu%20and%20Yiyi%20Liao%20and%20Xiaowei%20Zhou%20and%20Hujun%20Bao%0AAbstract%3A%20%20%20Depth%20estimation%20is%20a%20fundamental%20task%20in%203D%20computer%20vision%2C%20crucial%20for%0Aapplications%20such%20as%203D%20reconstruction%2C%20free-viewpoint%20rendering%2C%20robotics%2C%0Aautonomous%20driving%2C%20and%20AR/VR%20technologies.%20Traditional%20methods%20relying%20on%0Ahardware%20sensors%20like%20LiDAR%20are%20often%20limited%20by%20high%20costs%2C%20low%20resolution%2C%0Aand%20environmental%20sensitivity%2C%20limiting%20their%20applicability%20in%20real-world%0Ascenarios.%20Recent%20advances%20in%20vision-based%20methods%20offer%20a%20promising%0Aalternative%2C%20yet%20they%20face%20challenges%20in%20generalization%20and%20stability%20due%20to%0Aeither%20the%20low-capacity%20model%20architectures%20or%20the%20reliance%20on%20domain-specific%0Aand%20small-scale%20datasets.%20The%20emergence%20of%20scaling%20laws%20and%20foundation%20models%0Ain%20other%20domains%20has%20inspired%20the%20development%20of%20%22depth%20foundation%20models%22%3A%0Adeep%20neural%20networks%20trained%20on%20large%20datasets%20with%20strong%20zero-shot%0Ageneralization%20capabilities.%20This%20paper%20surveys%20the%20evolution%20of%20deep%20learning%0Aarchitectures%20and%20paradigms%20for%20depth%20estimation%20across%20the%20monocular%2C%20stereo%2C%0Amulti-view%2C%20and%20monocular%20video%20settings.%20We%20explore%20the%20potential%20of%20these%0Amodels%20to%20address%20existing%20challenges%20and%20provide%20a%20comprehensive%20overview%20of%0Alarge-scale%20datasets%20that%20can%20facilitate%20their%20development.%20By%20identifying%20key%0Aarchitectures%20and%20training%20strategies%2C%20we%20aim%20to%20highlight%20the%20path%20towards%0Arobust%20depth%20foundation%20models%2C%20offering%20insights%20into%20their%20future%20research%0Aand%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Depth%2520Foundation%2520Model%253A%2520Recent%2520Trends%2520in%2520Vision-Based%2520Depth%250A%2520%2520Estimation%26entry.906535625%3DZhen%2520Xu%2520and%2520Hongyu%2520Zhou%2520and%2520Sida%2520Peng%2520and%2520Haotong%2520Lin%2520and%2520Haoyu%2520Guo%2520and%2520Jiahao%2520Shao%2520and%2520Peishan%2520Yang%2520and%2520Qinglin%2520Yang%2520and%2520Sheng%2520Miao%2520and%2520Xingyi%2520He%2520and%2520Yifan%2520Wang%2520and%2520Yue%2520Wang%2520and%2520Ruizhen%2520Hu%2520and%2520Yiyi%2520Liao%2520and%2520Xiaowei%2520Zhou%2520and%2520Hujun%2520Bao%26entry.1292438233%3D%2520%2520Depth%2520estimation%2520is%2520a%2520fundamental%2520task%2520in%25203D%2520computer%2520vision%252C%2520crucial%2520for%250Aapplications%2520such%2520as%25203D%2520reconstruction%252C%2520free-viewpoint%2520rendering%252C%2520robotics%252C%250Aautonomous%2520driving%252C%2520and%2520AR/VR%2520technologies.%2520Traditional%2520methods%2520relying%2520on%250Ahardware%2520sensors%2520like%2520LiDAR%2520are%2520often%2520limited%2520by%2520high%2520costs%252C%2520low%2520resolution%252C%250Aand%2520environmental%2520sensitivity%252C%2520limiting%2520their%2520applicability%2520in%2520real-world%250Ascenarios.%2520Recent%2520advances%2520in%2520vision-based%2520methods%2520offer%2520a%2520promising%250Aalternative%252C%2520yet%2520they%2520face%2520challenges%2520in%2520generalization%2520and%2520stability%2520due%2520to%250Aeither%2520the%2520low-capacity%2520model%2520architectures%2520or%2520the%2520reliance%2520on%2520domain-specific%250Aand%2520small-scale%2520datasets.%2520The%2520emergence%2520of%2520scaling%2520laws%2520and%2520foundation%2520models%250Ain%2520other%2520domains%2520has%2520inspired%2520the%2520development%2520of%2520%2522depth%2520foundation%2520models%2522%253A%250Adeep%2520neural%2520networks%2520trained%2520on%2520large%2520datasets%2520with%2520strong%2520zero-shot%250Ageneralization%2520capabilities.%2520This%2520paper%2520surveys%2520the%2520evolution%2520of%2520deep%2520learning%250Aarchitectures%2520and%2520paradigms%2520for%2520depth%2520estimation%2520across%2520the%2520monocular%252C%2520stereo%252C%250Amulti-view%252C%2520and%2520monocular%2520video%2520settings.%2520We%2520explore%2520the%2520potential%2520of%2520these%250Amodels%2520to%2520address%2520existing%2520challenges%2520and%2520provide%2520a%2520comprehensive%2520overview%2520of%250Alarge-scale%2520datasets%2520that%2520can%2520facilitate%2520their%2520development.%2520By%2520identifying%2520key%250Aarchitectures%2520and%2520training%2520strategies%252C%2520we%2520aim%2520to%2520highlight%2520the%2520path%2520towards%250Arobust%2520depth%2520foundation%2520models%252C%2520offering%2520insights%2520into%2520their%2520future%2520research%250Aand%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Depth%20Foundation%20Model%3A%20Recent%20Trends%20in%20Vision-Based%20Depth%0A%20%20Estimation&entry.906535625=Zhen%20Xu%20and%20Hongyu%20Zhou%20and%20Sida%20Peng%20and%20Haotong%20Lin%20and%20Haoyu%20Guo%20and%20Jiahao%20Shao%20and%20Peishan%20Yang%20and%20Qinglin%20Yang%20and%20Sheng%20Miao%20and%20Xingyi%20He%20and%20Yifan%20Wang%20and%20Yue%20Wang%20and%20Ruizhen%20Hu%20and%20Yiyi%20Liao%20and%20Xiaowei%20Zhou%20and%20Hujun%20Bao&entry.1292438233=%20%20Depth%20estimation%20is%20a%20fundamental%20task%20in%203D%20computer%20vision%2C%20crucial%20for%0Aapplications%20such%20as%203D%20reconstruction%2C%20free-viewpoint%20rendering%2C%20robotics%2C%0Aautonomous%20driving%2C%20and%20AR/VR%20technologies.%20Traditional%20methods%20relying%20on%0Ahardware%20sensors%20like%20LiDAR%20are%20often%20limited%20by%20high%20costs%2C%20low%20resolution%2C%0Aand%20environmental%20sensitivity%2C%20limiting%20their%20applicability%20in%20real-world%0Ascenarios.%20Recent%20advances%20in%20vision-based%20methods%20offer%20a%20promising%0Aalternative%2C%20yet%20they%20face%20challenges%20in%20generalization%20and%20stability%20due%20to%0Aeither%20the%20low-capacity%20model%20architectures%20or%20the%20reliance%20on%20domain-specific%0Aand%20small-scale%20datasets.%20The%20emergence%20of%20scaling%20laws%20and%20foundation%20models%0Ain%20other%20domains%20has%20inspired%20the%20development%20of%20%22depth%20foundation%20models%22%3A%0Adeep%20neural%20networks%20trained%20on%20large%20datasets%20with%20strong%20zero-shot%0Ageneralization%20capabilities.%20This%20paper%20surveys%20the%20evolution%20of%20deep%20learning%0Aarchitectures%20and%20paradigms%20for%20depth%20estimation%20across%20the%20monocular%2C%20stereo%2C%0Amulti-view%2C%20and%20monocular%20video%20settings.%20We%20explore%20the%20potential%20of%20these%0Amodels%20to%20address%20existing%20challenges%20and%20provide%20a%20comprehensive%20overview%20of%0Alarge-scale%20datasets%20that%20can%20facilitate%20their%20development.%20By%20identifying%20key%0Aarchitectures%20and%20training%20strategies%2C%20we%20aim%20to%20highlight%20the%20path%20towards%0Arobust%20depth%20foundation%20models%2C%20offering%20insights%20into%20their%20future%20research%0Aand%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11540v1&entry.124074799=Read"},
{"title": "Searching Latent Program Spaces", "author": "Matthew V Macfarlane and Cl\u00e9ment Bonnet", "abstract": "  General intelligence requires systems that acquire new skills efficiently and\ngeneralize beyond their training distributions. Although program synthesis\napproaches have strong generalization power, they face scaling issues due to\nlarge combinatorial spaces that quickly make them impractical and require\nhuman-generated DSLs or pre-trained priors to narrow this search space. On the\nother hand, deep learning methods have had high successes, but they lack\nstructured test-time adaptation and rely on heavy stochastic sampling or\nexpensive gradient updates for fine-tuning. In this work, we propose the Latent\nProgram Network (LPN), a new architecture that builds in test-time search\ndirectly into neural models. LPN learns a latent space of implicit\nprograms--neurally mapping inputs to outputs--through which it can search using\ngradients at test time. LPN combines the adaptability of symbolic approaches\nand the scalability of neural methods. It searches through a compact latent\nspace at test time and bypasses the need for pre-defined domain-specific\nlanguages. On a range of programming-by-examples tasks, LPN either outperforms\nor matches performance compared to in-context learning and test-time training\nmethods. Tested on the ARC-AGI benchmark, we demonstrate that LPN can both\nlearn a compact program space and search through it at test time to adapt to\nnovel tasks. LPN doubles its performance on out-of-distribution tasks when\ntest-time search is switched on.\n", "link": "http://arxiv.org/abs/2411.08706v2", "date": "2025-07-15", "relevancy": 2.553, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5153}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5083}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Searching%20Latent%20Program%20Spaces&body=Title%3A%20Searching%20Latent%20Program%20Spaces%0AAuthor%3A%20Matthew%20V%20Macfarlane%20and%20Cl%C3%A9ment%20Bonnet%0AAbstract%3A%20%20%20General%20intelligence%20requires%20systems%20that%20acquire%20new%20skills%20efficiently%20and%0Ageneralize%20beyond%20their%20training%20distributions.%20Although%20program%20synthesis%0Aapproaches%20have%20strong%20generalization%20power%2C%20they%20face%20scaling%20issues%20due%20to%0Alarge%20combinatorial%20spaces%20that%20quickly%20make%20them%20impractical%20and%20require%0Ahuman-generated%20DSLs%20or%20pre-trained%20priors%20to%20narrow%20this%20search%20space.%20On%20the%0Aother%20hand%2C%20deep%20learning%20methods%20have%20had%20high%20successes%2C%20but%20they%20lack%0Astructured%20test-time%20adaptation%20and%20rely%20on%20heavy%20stochastic%20sampling%20or%0Aexpensive%20gradient%20updates%20for%20fine-tuning.%20In%20this%20work%2C%20we%20propose%20the%20Latent%0AProgram%20Network%20%28LPN%29%2C%20a%20new%20architecture%20that%20builds%20in%20test-time%20search%0Adirectly%20into%20neural%20models.%20LPN%20learns%20a%20latent%20space%20of%20implicit%0Aprograms--neurally%20mapping%20inputs%20to%20outputs--through%20which%20it%20can%20search%20using%0Agradients%20at%20test%20time.%20LPN%20combines%20the%20adaptability%20of%20symbolic%20approaches%0Aand%20the%20scalability%20of%20neural%20methods.%20It%20searches%20through%20a%20compact%20latent%0Aspace%20at%20test%20time%20and%20bypasses%20the%20need%20for%20pre-defined%20domain-specific%0Alanguages.%20On%20a%20range%20of%20programming-by-examples%20tasks%2C%20LPN%20either%20outperforms%0Aor%20matches%20performance%20compared%20to%20in-context%20learning%20and%20test-time%20training%0Amethods.%20Tested%20on%20the%20ARC-AGI%20benchmark%2C%20we%20demonstrate%20that%20LPN%20can%20both%0Alearn%20a%20compact%20program%20space%20and%20search%20through%20it%20at%20test%20time%20to%20adapt%20to%0Anovel%20tasks.%20LPN%20doubles%20its%20performance%20on%20out-of-distribution%20tasks%20when%0Atest-time%20search%20is%20switched%20on.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08706v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSearching%2520Latent%2520Program%2520Spaces%26entry.906535625%3DMatthew%2520V%2520Macfarlane%2520and%2520Cl%25C3%25A9ment%2520Bonnet%26entry.1292438233%3D%2520%2520General%2520intelligence%2520requires%2520systems%2520that%2520acquire%2520new%2520skills%2520efficiently%2520and%250Ageneralize%2520beyond%2520their%2520training%2520distributions.%2520Although%2520program%2520synthesis%250Aapproaches%2520have%2520strong%2520generalization%2520power%252C%2520they%2520face%2520scaling%2520issues%2520due%2520to%250Alarge%2520combinatorial%2520spaces%2520that%2520quickly%2520make%2520them%2520impractical%2520and%2520require%250Ahuman-generated%2520DSLs%2520or%2520pre-trained%2520priors%2520to%2520narrow%2520this%2520search%2520space.%2520On%2520the%250Aother%2520hand%252C%2520deep%2520learning%2520methods%2520have%2520had%2520high%2520successes%252C%2520but%2520they%2520lack%250Astructured%2520test-time%2520adaptation%2520and%2520rely%2520on%2520heavy%2520stochastic%2520sampling%2520or%250Aexpensive%2520gradient%2520updates%2520for%2520fine-tuning.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520Latent%250AProgram%2520Network%2520%2528LPN%2529%252C%2520a%2520new%2520architecture%2520that%2520builds%2520in%2520test-time%2520search%250Adirectly%2520into%2520neural%2520models.%2520LPN%2520learns%2520a%2520latent%2520space%2520of%2520implicit%250Aprograms--neurally%2520mapping%2520inputs%2520to%2520outputs--through%2520which%2520it%2520can%2520search%2520using%250Agradients%2520at%2520test%2520time.%2520LPN%2520combines%2520the%2520adaptability%2520of%2520symbolic%2520approaches%250Aand%2520the%2520scalability%2520of%2520neural%2520methods.%2520It%2520searches%2520through%2520a%2520compact%2520latent%250Aspace%2520at%2520test%2520time%2520and%2520bypasses%2520the%2520need%2520for%2520pre-defined%2520domain-specific%250Alanguages.%2520On%2520a%2520range%2520of%2520programming-by-examples%2520tasks%252C%2520LPN%2520either%2520outperforms%250Aor%2520matches%2520performance%2520compared%2520to%2520in-context%2520learning%2520and%2520test-time%2520training%250Amethods.%2520Tested%2520on%2520the%2520ARC-AGI%2520benchmark%252C%2520we%2520demonstrate%2520that%2520LPN%2520can%2520both%250Alearn%2520a%2520compact%2520program%2520space%2520and%2520search%2520through%2520it%2520at%2520test%2520time%2520to%2520adapt%2520to%250Anovel%2520tasks.%2520LPN%2520doubles%2520its%2520performance%2520on%2520out-of-distribution%2520tasks%2520when%250Atest-time%2520search%2520is%2520switched%2520on.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08706v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Searching%20Latent%20Program%20Spaces&entry.906535625=Matthew%20V%20Macfarlane%20and%20Cl%C3%A9ment%20Bonnet&entry.1292438233=%20%20General%20intelligence%20requires%20systems%20that%20acquire%20new%20skills%20efficiently%20and%0Ageneralize%20beyond%20their%20training%20distributions.%20Although%20program%20synthesis%0Aapproaches%20have%20strong%20generalization%20power%2C%20they%20face%20scaling%20issues%20due%20to%0Alarge%20combinatorial%20spaces%20that%20quickly%20make%20them%20impractical%20and%20require%0Ahuman-generated%20DSLs%20or%20pre-trained%20priors%20to%20narrow%20this%20search%20space.%20On%20the%0Aother%20hand%2C%20deep%20learning%20methods%20have%20had%20high%20successes%2C%20but%20they%20lack%0Astructured%20test-time%20adaptation%20and%20rely%20on%20heavy%20stochastic%20sampling%20or%0Aexpensive%20gradient%20updates%20for%20fine-tuning.%20In%20this%20work%2C%20we%20propose%20the%20Latent%0AProgram%20Network%20%28LPN%29%2C%20a%20new%20architecture%20that%20builds%20in%20test-time%20search%0Adirectly%20into%20neural%20models.%20LPN%20learns%20a%20latent%20space%20of%20implicit%0Aprograms--neurally%20mapping%20inputs%20to%20outputs--through%20which%20it%20can%20search%20using%0Agradients%20at%20test%20time.%20LPN%20combines%20the%20adaptability%20of%20symbolic%20approaches%0Aand%20the%20scalability%20of%20neural%20methods.%20It%20searches%20through%20a%20compact%20latent%0Aspace%20at%20test%20time%20and%20bypasses%20the%20need%20for%20pre-defined%20domain-specific%0Alanguages.%20On%20a%20range%20of%20programming-by-examples%20tasks%2C%20LPN%20either%20outperforms%0Aor%20matches%20performance%20compared%20to%20in-context%20learning%20and%20test-time%20training%0Amethods.%20Tested%20on%20the%20ARC-AGI%20benchmark%2C%20we%20demonstrate%20that%20LPN%20can%20both%0Alearn%20a%20compact%20program%20space%20and%20search%20through%20it%20at%20test%20time%20to%20adapt%20to%0Anovel%20tasks.%20LPN%20doubles%20its%20performance%20on%20out-of-distribution%20tasks%20when%0Atest-time%20search%20is%20switched%20on.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08706v2&entry.124074799=Read"},
{"title": "Synthetic Datasets for Machine Learning on Spatio-Temporal Graphs using\n  PDEs", "author": "Jost Arndt and Utku Isil and Michael Detzel and Wojciech Samek and Jackie Ma", "abstract": "  Many physical processes can be expressed through partial differential\nequations (PDEs). Real-world measurements of such processes are often collected\nat irregularly distributed points in space, which can be effectively\nrepresented as graphs; however, there are currently only a few existing\ndatasets. Our work aims to make advancements in the field of PDE-modeling\naccessible to the temporal graph machine learning community, while addressing\nthe data scarcity problem, by creating and utilizing datasets based on PDEs. In\nthis work, we create and use synthetic datasets based on PDEs to support\nspatio-temporal graph modeling in machine learning for different applications.\nMore precisely, we showcase three equations to model different types of\ndisasters and hazards in the fields of epidemiology, atmospheric particles, and\ntsunami waves. Further, we show how such created datasets can be used by\nbenchmarking several machine learning models on the epidemiological dataset.\nAdditionally, we show how pre-training on this dataset can improve model\nperformance on real-world epidemiological data. The presented methods enable\nothers to create datasets and benchmarks customized to individual requirements.\nThe source code for our methodology and the three created datasets can be found\non https://github.com/github-usr-ano/Temporal_Graph_Data_PDEs.\n", "link": "http://arxiv.org/abs/2502.04140v2", "date": "2025-07-15", "relevancy": 2.41, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.487}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4869}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Synthetic%20Datasets%20for%20Machine%20Learning%20on%20Spatio-Temporal%20Graphs%20using%0A%20%20PDEs&body=Title%3A%20Synthetic%20Datasets%20for%20Machine%20Learning%20on%20Spatio-Temporal%20Graphs%20using%0A%20%20PDEs%0AAuthor%3A%20Jost%20Arndt%20and%20Utku%20Isil%20and%20Michael%20Detzel%20and%20Wojciech%20Samek%20and%20Jackie%20Ma%0AAbstract%3A%20%20%20Many%20physical%20processes%20can%20be%20expressed%20through%20partial%20differential%0Aequations%20%28PDEs%29.%20Real-world%20measurements%20of%20such%20processes%20are%20often%20collected%0Aat%20irregularly%20distributed%20points%20in%20space%2C%20which%20can%20be%20effectively%0Arepresented%20as%20graphs%3B%20however%2C%20there%20are%20currently%20only%20a%20few%20existing%0Adatasets.%20Our%20work%20aims%20to%20make%20advancements%20in%20the%20field%20of%20PDE-modeling%0Aaccessible%20to%20the%20temporal%20graph%20machine%20learning%20community%2C%20while%20addressing%0Athe%20data%20scarcity%20problem%2C%20by%20creating%20and%20utilizing%20datasets%20based%20on%20PDEs.%20In%0Athis%20work%2C%20we%20create%20and%20use%20synthetic%20datasets%20based%20on%20PDEs%20to%20support%0Aspatio-temporal%20graph%20modeling%20in%20machine%20learning%20for%20different%20applications.%0AMore%20precisely%2C%20we%20showcase%20three%20equations%20to%20model%20different%20types%20of%0Adisasters%20and%20hazards%20in%20the%20fields%20of%20epidemiology%2C%20atmospheric%20particles%2C%20and%0Atsunami%20waves.%20Further%2C%20we%20show%20how%20such%20created%20datasets%20can%20be%20used%20by%0Abenchmarking%20several%20machine%20learning%20models%20on%20the%20epidemiological%20dataset.%0AAdditionally%2C%20we%20show%20how%20pre-training%20on%20this%20dataset%20can%20improve%20model%0Aperformance%20on%20real-world%20epidemiological%20data.%20The%20presented%20methods%20enable%0Aothers%20to%20create%20datasets%20and%20benchmarks%20customized%20to%20individual%20requirements.%0AThe%20source%20code%20for%20our%20methodology%20and%20the%20three%20created%20datasets%20can%20be%20found%0Aon%20https%3A//github.com/github-usr-ano/Temporal_Graph_Data_PDEs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.04140v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthetic%2520Datasets%2520for%2520Machine%2520Learning%2520on%2520Spatio-Temporal%2520Graphs%2520using%250A%2520%2520PDEs%26entry.906535625%3DJost%2520Arndt%2520and%2520Utku%2520Isil%2520and%2520Michael%2520Detzel%2520and%2520Wojciech%2520Samek%2520and%2520Jackie%2520Ma%26entry.1292438233%3D%2520%2520Many%2520physical%2520processes%2520can%2520be%2520expressed%2520through%2520partial%2520differential%250Aequations%2520%2528PDEs%2529.%2520Real-world%2520measurements%2520of%2520such%2520processes%2520are%2520often%2520collected%250Aat%2520irregularly%2520distributed%2520points%2520in%2520space%252C%2520which%2520can%2520be%2520effectively%250Arepresented%2520as%2520graphs%253B%2520however%252C%2520there%2520are%2520currently%2520only%2520a%2520few%2520existing%250Adatasets.%2520Our%2520work%2520aims%2520to%2520make%2520advancements%2520in%2520the%2520field%2520of%2520PDE-modeling%250Aaccessible%2520to%2520the%2520temporal%2520graph%2520machine%2520learning%2520community%252C%2520while%2520addressing%250Athe%2520data%2520scarcity%2520problem%252C%2520by%2520creating%2520and%2520utilizing%2520datasets%2520based%2520on%2520PDEs.%2520In%250Athis%2520work%252C%2520we%2520create%2520and%2520use%2520synthetic%2520datasets%2520based%2520on%2520PDEs%2520to%2520support%250Aspatio-temporal%2520graph%2520modeling%2520in%2520machine%2520learning%2520for%2520different%2520applications.%250AMore%2520precisely%252C%2520we%2520showcase%2520three%2520equations%2520to%2520model%2520different%2520types%2520of%250Adisasters%2520and%2520hazards%2520in%2520the%2520fields%2520of%2520epidemiology%252C%2520atmospheric%2520particles%252C%2520and%250Atsunami%2520waves.%2520Further%252C%2520we%2520show%2520how%2520such%2520created%2520datasets%2520can%2520be%2520used%2520by%250Abenchmarking%2520several%2520machine%2520learning%2520models%2520on%2520the%2520epidemiological%2520dataset.%250AAdditionally%252C%2520we%2520show%2520how%2520pre-training%2520on%2520this%2520dataset%2520can%2520improve%2520model%250Aperformance%2520on%2520real-world%2520epidemiological%2520data.%2520The%2520presented%2520methods%2520enable%250Aothers%2520to%2520create%2520datasets%2520and%2520benchmarks%2520customized%2520to%2520individual%2520requirements.%250AThe%2520source%2520code%2520for%2520our%2520methodology%2520and%2520the%2520three%2520created%2520datasets%2520can%2520be%2520found%250Aon%2520https%253A//github.com/github-usr-ano/Temporal_Graph_Data_PDEs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.04140v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthetic%20Datasets%20for%20Machine%20Learning%20on%20Spatio-Temporal%20Graphs%20using%0A%20%20PDEs&entry.906535625=Jost%20Arndt%20and%20Utku%20Isil%20and%20Michael%20Detzel%20and%20Wojciech%20Samek%20and%20Jackie%20Ma&entry.1292438233=%20%20Many%20physical%20processes%20can%20be%20expressed%20through%20partial%20differential%0Aequations%20%28PDEs%29.%20Real-world%20measurements%20of%20such%20processes%20are%20often%20collected%0Aat%20irregularly%20distributed%20points%20in%20space%2C%20which%20can%20be%20effectively%0Arepresented%20as%20graphs%3B%20however%2C%20there%20are%20currently%20only%20a%20few%20existing%0Adatasets.%20Our%20work%20aims%20to%20make%20advancements%20in%20the%20field%20of%20PDE-modeling%0Aaccessible%20to%20the%20temporal%20graph%20machine%20learning%20community%2C%20while%20addressing%0Athe%20data%20scarcity%20problem%2C%20by%20creating%20and%20utilizing%20datasets%20based%20on%20PDEs.%20In%0Athis%20work%2C%20we%20create%20and%20use%20synthetic%20datasets%20based%20on%20PDEs%20to%20support%0Aspatio-temporal%20graph%20modeling%20in%20machine%20learning%20for%20different%20applications.%0AMore%20precisely%2C%20we%20showcase%20three%20equations%20to%20model%20different%20types%20of%0Adisasters%20and%20hazards%20in%20the%20fields%20of%20epidemiology%2C%20atmospheric%20particles%2C%20and%0Atsunami%20waves.%20Further%2C%20we%20show%20how%20such%20created%20datasets%20can%20be%20used%20by%0Abenchmarking%20several%20machine%20learning%20models%20on%20the%20epidemiological%20dataset.%0AAdditionally%2C%20we%20show%20how%20pre-training%20on%20this%20dataset%20can%20improve%20model%0Aperformance%20on%20real-world%20epidemiological%20data.%20The%20presented%20methods%20enable%0Aothers%20to%20create%20datasets%20and%20benchmarks%20customized%20to%20individual%20requirements.%0AThe%20source%20code%20for%20our%20methodology%20and%20the%20three%20created%20datasets%20can%20be%20found%0Aon%20https%3A//github.com/github-usr-ano/Temporal_Graph_Data_PDEs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.04140v2&entry.124074799=Read"},
{"title": "Gram-Schmidt Methods for Unsupervised Feature Extraction and Selection", "author": "Bahram Yaghooti and Netanel Raviv and Bruno Sinopoli", "abstract": "  Feature extraction and selection in the presence of nonlinear dependencies\namong the data is a fundamental challenge in unsupervised learning. We propose\nusing a Gram-Schmidt (GS) type orthogonalization process over function spaces\nto detect and map out such dependencies. Specifically, by applying the GS\nprocess over some family of functions, we construct a series of covariance\nmatrices that can either be used to identify new large-variance directions, or\nto remove those dependencies from known directions. In the former case, we\nprovide information-theoretic guarantees in terms of entropy reduction. In the\nlatter, we provide precise conditions by which the chosen function family\neliminates existing redundancy in the data. Each approach provides both a\nfeature extraction and a feature selection algorithm. Our feature extraction\nmethods are linear, and can be seen as natural generalization of principal\ncomponent analysis (PCA). We provide experimental results for synthetic and\nreal-world benchmark datasets which show superior performance over\nstate-of-the-art (linear) feature extraction and selection algorithms.\nSurprisingly, our linear feature extraction algorithms are comparable and often\noutperform several important nonlinear feature extraction methods such as\nautoencoders, kernel PCA, and UMAP. Furthermore, one of our feature selection\nalgorithms strictly generalizes a recent Fourier-based feature selection\nmechanism (Heidari et al., IEEE Transactions on Information Theory, 2022), yet\nat significantly reduced complexity.\n", "link": "http://arxiv.org/abs/2311.09386v4", "date": "2025-07-15", "relevancy": 2.3876, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4852}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4781}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gram-Schmidt%20Methods%20for%20Unsupervised%20Feature%20Extraction%20and%20Selection&body=Title%3A%20Gram-Schmidt%20Methods%20for%20Unsupervised%20Feature%20Extraction%20and%20Selection%0AAuthor%3A%20Bahram%20Yaghooti%20and%20Netanel%20Raviv%20and%20Bruno%20Sinopoli%0AAbstract%3A%20%20%20Feature%20extraction%20and%20selection%20in%20the%20presence%20of%20nonlinear%20dependencies%0Aamong%20the%20data%20is%20a%20fundamental%20challenge%20in%20unsupervised%20learning.%20We%20propose%0Ausing%20a%20Gram-Schmidt%20%28GS%29%20type%20orthogonalization%20process%20over%20function%20spaces%0Ato%20detect%20and%20map%20out%20such%20dependencies.%20Specifically%2C%20by%20applying%20the%20GS%0Aprocess%20over%20some%20family%20of%20functions%2C%20we%20construct%20a%20series%20of%20covariance%0Amatrices%20that%20can%20either%20be%20used%20to%20identify%20new%20large-variance%20directions%2C%20or%0Ato%20remove%20those%20dependencies%20from%20known%20directions.%20In%20the%20former%20case%2C%20we%0Aprovide%20information-theoretic%20guarantees%20in%20terms%20of%20entropy%20reduction.%20In%20the%0Alatter%2C%20we%20provide%20precise%20conditions%20by%20which%20the%20chosen%20function%20family%0Aeliminates%20existing%20redundancy%20in%20the%20data.%20Each%20approach%20provides%20both%20a%0Afeature%20extraction%20and%20a%20feature%20selection%20algorithm.%20Our%20feature%20extraction%0Amethods%20are%20linear%2C%20and%20can%20be%20seen%20as%20natural%20generalization%20of%20principal%0Acomponent%20analysis%20%28PCA%29.%20We%20provide%20experimental%20results%20for%20synthetic%20and%0Areal-world%20benchmark%20datasets%20which%20show%20superior%20performance%20over%0Astate-of-the-art%20%28linear%29%20feature%20extraction%20and%20selection%20algorithms.%0ASurprisingly%2C%20our%20linear%20feature%20extraction%20algorithms%20are%20comparable%20and%20often%0Aoutperform%20several%20important%20nonlinear%20feature%20extraction%20methods%20such%20as%0Aautoencoders%2C%20kernel%20PCA%2C%20and%20UMAP.%20Furthermore%2C%20one%20of%20our%20feature%20selection%0Aalgorithms%20strictly%20generalizes%20a%20recent%20Fourier-based%20feature%20selection%0Amechanism%20%28Heidari%20et%20al.%2C%20IEEE%20Transactions%20on%20Information%20Theory%2C%202022%29%2C%20yet%0Aat%20significantly%20reduced%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.09386v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGram-Schmidt%2520Methods%2520for%2520Unsupervised%2520Feature%2520Extraction%2520and%2520Selection%26entry.906535625%3DBahram%2520Yaghooti%2520and%2520Netanel%2520Raviv%2520and%2520Bruno%2520Sinopoli%26entry.1292438233%3D%2520%2520Feature%2520extraction%2520and%2520selection%2520in%2520the%2520presence%2520of%2520nonlinear%2520dependencies%250Aamong%2520the%2520data%2520is%2520a%2520fundamental%2520challenge%2520in%2520unsupervised%2520learning.%2520We%2520propose%250Ausing%2520a%2520Gram-Schmidt%2520%2528GS%2529%2520type%2520orthogonalization%2520process%2520over%2520function%2520spaces%250Ato%2520detect%2520and%2520map%2520out%2520such%2520dependencies.%2520Specifically%252C%2520by%2520applying%2520the%2520GS%250Aprocess%2520over%2520some%2520family%2520of%2520functions%252C%2520we%2520construct%2520a%2520series%2520of%2520covariance%250Amatrices%2520that%2520can%2520either%2520be%2520used%2520to%2520identify%2520new%2520large-variance%2520directions%252C%2520or%250Ato%2520remove%2520those%2520dependencies%2520from%2520known%2520directions.%2520In%2520the%2520former%2520case%252C%2520we%250Aprovide%2520information-theoretic%2520guarantees%2520in%2520terms%2520of%2520entropy%2520reduction.%2520In%2520the%250Alatter%252C%2520we%2520provide%2520precise%2520conditions%2520by%2520which%2520the%2520chosen%2520function%2520family%250Aeliminates%2520existing%2520redundancy%2520in%2520the%2520data.%2520Each%2520approach%2520provides%2520both%2520a%250Afeature%2520extraction%2520and%2520a%2520feature%2520selection%2520algorithm.%2520Our%2520feature%2520extraction%250Amethods%2520are%2520linear%252C%2520and%2520can%2520be%2520seen%2520as%2520natural%2520generalization%2520of%2520principal%250Acomponent%2520analysis%2520%2528PCA%2529.%2520We%2520provide%2520experimental%2520results%2520for%2520synthetic%2520and%250Areal-world%2520benchmark%2520datasets%2520which%2520show%2520superior%2520performance%2520over%250Astate-of-the-art%2520%2528linear%2529%2520feature%2520extraction%2520and%2520selection%2520algorithms.%250ASurprisingly%252C%2520our%2520linear%2520feature%2520extraction%2520algorithms%2520are%2520comparable%2520and%2520often%250Aoutperform%2520several%2520important%2520nonlinear%2520feature%2520extraction%2520methods%2520such%2520as%250Aautoencoders%252C%2520kernel%2520PCA%252C%2520and%2520UMAP.%2520Furthermore%252C%2520one%2520of%2520our%2520feature%2520selection%250Aalgorithms%2520strictly%2520generalizes%2520a%2520recent%2520Fourier-based%2520feature%2520selection%250Amechanism%2520%2528Heidari%2520et%2520al.%252C%2520IEEE%2520Transactions%2520on%2520Information%2520Theory%252C%25202022%2529%252C%2520yet%250Aat%2520significantly%2520reduced%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.09386v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gram-Schmidt%20Methods%20for%20Unsupervised%20Feature%20Extraction%20and%20Selection&entry.906535625=Bahram%20Yaghooti%20and%20Netanel%20Raviv%20and%20Bruno%20Sinopoli&entry.1292438233=%20%20Feature%20extraction%20and%20selection%20in%20the%20presence%20of%20nonlinear%20dependencies%0Aamong%20the%20data%20is%20a%20fundamental%20challenge%20in%20unsupervised%20learning.%20We%20propose%0Ausing%20a%20Gram-Schmidt%20%28GS%29%20type%20orthogonalization%20process%20over%20function%20spaces%0Ato%20detect%20and%20map%20out%20such%20dependencies.%20Specifically%2C%20by%20applying%20the%20GS%0Aprocess%20over%20some%20family%20of%20functions%2C%20we%20construct%20a%20series%20of%20covariance%0Amatrices%20that%20can%20either%20be%20used%20to%20identify%20new%20large-variance%20directions%2C%20or%0Ato%20remove%20those%20dependencies%20from%20known%20directions.%20In%20the%20former%20case%2C%20we%0Aprovide%20information-theoretic%20guarantees%20in%20terms%20of%20entropy%20reduction.%20In%20the%0Alatter%2C%20we%20provide%20precise%20conditions%20by%20which%20the%20chosen%20function%20family%0Aeliminates%20existing%20redundancy%20in%20the%20data.%20Each%20approach%20provides%20both%20a%0Afeature%20extraction%20and%20a%20feature%20selection%20algorithm.%20Our%20feature%20extraction%0Amethods%20are%20linear%2C%20and%20can%20be%20seen%20as%20natural%20generalization%20of%20principal%0Acomponent%20analysis%20%28PCA%29.%20We%20provide%20experimental%20results%20for%20synthetic%20and%0Areal-world%20benchmark%20datasets%20which%20show%20superior%20performance%20over%0Astate-of-the-art%20%28linear%29%20feature%20extraction%20and%20selection%20algorithms.%0ASurprisingly%2C%20our%20linear%20feature%20extraction%20algorithms%20are%20comparable%20and%20often%0Aoutperform%20several%20important%20nonlinear%20feature%20extraction%20methods%20such%20as%0Aautoencoders%2C%20kernel%20PCA%2C%20and%20UMAP.%20Furthermore%2C%20one%20of%20our%20feature%20selection%0Aalgorithms%20strictly%20generalizes%20a%20recent%20Fourier-based%20feature%20selection%0Amechanism%20%28Heidari%20et%20al.%2C%20IEEE%20Transactions%20on%20Information%20Theory%2C%202022%29%2C%20yet%0Aat%20significantly%20reduced%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.09386v4&entry.124074799=Read"},
{"title": "CharaConsist: Fine-Grained Consistent Character Generation", "author": "Mengyu Wang and Henghui Ding and Jianing Peng and Yao Zhao and Yunpeng Chen and Yunchao Wei", "abstract": "  In text-to-image generation, producing a series of consistent contents that\npreserve the same identity is highly valuable for real-world applications.\nAlthough a few works have explored training-free methods to enhance the\nconsistency of generated subjects, we observe that they suffer from the\nfollowing problems. First, they fail to maintain consistent background details,\nwhich limits their applicability. Furthermore, when the foreground character\nundergoes large motion variations, inconsistencies in identity and clothing\ndetails become evident. To address these problems, we propose CharaConsist,\nwhich employs point-tracking attention and adaptive token merge along with\ndecoupled control of the foreground and background. CharaConsist enables\nfine-grained consistency for both foreground and background, supporting the\ngeneration of one character in continuous shots within a fixed scene or in\ndiscrete shots across different scenes. Moreover, CharaConsist is the first\nconsistent generation method tailored for text-to-image DiT model. Its ability\nto maintain fine-grained consistency, combined with the larger capacity of\nlatest base model, enables it to produce high-quality visual outputs,\nbroadening its applicability to a wider range of real-world scenarios. The\nsource code has been released at https://github.com/Murray-Wang/CharaConsist\n", "link": "http://arxiv.org/abs/2507.11533v1", "date": "2025-07-15", "relevancy": 2.3137, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5899}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5827}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CharaConsist%3A%20Fine-Grained%20Consistent%20Character%20Generation&body=Title%3A%20CharaConsist%3A%20Fine-Grained%20Consistent%20Character%20Generation%0AAuthor%3A%20Mengyu%20Wang%20and%20Henghui%20Ding%20and%20Jianing%20Peng%20and%20Yao%20Zhao%20and%20Yunpeng%20Chen%20and%20Yunchao%20Wei%0AAbstract%3A%20%20%20In%20text-to-image%20generation%2C%20producing%20a%20series%20of%20consistent%20contents%20that%0Apreserve%20the%20same%20identity%20is%20highly%20valuable%20for%20real-world%20applications.%0AAlthough%20a%20few%20works%20have%20explored%20training-free%20methods%20to%20enhance%20the%0Aconsistency%20of%20generated%20subjects%2C%20we%20observe%20that%20they%20suffer%20from%20the%0Afollowing%20problems.%20First%2C%20they%20fail%20to%20maintain%20consistent%20background%20details%2C%0Awhich%20limits%20their%20applicability.%20Furthermore%2C%20when%20the%20foreground%20character%0Aundergoes%20large%20motion%20variations%2C%20inconsistencies%20in%20identity%20and%20clothing%0Adetails%20become%20evident.%20To%20address%20these%20problems%2C%20we%20propose%20CharaConsist%2C%0Awhich%20employs%20point-tracking%20attention%20and%20adaptive%20token%20merge%20along%20with%0Adecoupled%20control%20of%20the%20foreground%20and%20background.%20CharaConsist%20enables%0Afine-grained%20consistency%20for%20both%20foreground%20and%20background%2C%20supporting%20the%0Ageneration%20of%20one%20character%20in%20continuous%20shots%20within%20a%20fixed%20scene%20or%20in%0Adiscrete%20shots%20across%20different%20scenes.%20Moreover%2C%20CharaConsist%20is%20the%20first%0Aconsistent%20generation%20method%20tailored%20for%20text-to-image%20DiT%20model.%20Its%20ability%0Ato%20maintain%20fine-grained%20consistency%2C%20combined%20with%20the%20larger%20capacity%20of%0Alatest%20base%20model%2C%20enables%20it%20to%20produce%20high-quality%20visual%20outputs%2C%0Abroadening%20its%20applicability%20to%20a%20wider%20range%20of%20real-world%20scenarios.%20The%0Asource%20code%20has%20been%20released%20at%20https%3A//github.com/Murray-Wang/CharaConsist%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11533v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharaConsist%253A%2520Fine-Grained%2520Consistent%2520Character%2520Generation%26entry.906535625%3DMengyu%2520Wang%2520and%2520Henghui%2520Ding%2520and%2520Jianing%2520Peng%2520and%2520Yao%2520Zhao%2520and%2520Yunpeng%2520Chen%2520and%2520Yunchao%2520Wei%26entry.1292438233%3D%2520%2520In%2520text-to-image%2520generation%252C%2520producing%2520a%2520series%2520of%2520consistent%2520contents%2520that%250Apreserve%2520the%2520same%2520identity%2520is%2520highly%2520valuable%2520for%2520real-world%2520applications.%250AAlthough%2520a%2520few%2520works%2520have%2520explored%2520training-free%2520methods%2520to%2520enhance%2520the%250Aconsistency%2520of%2520generated%2520subjects%252C%2520we%2520observe%2520that%2520they%2520suffer%2520from%2520the%250Afollowing%2520problems.%2520First%252C%2520they%2520fail%2520to%2520maintain%2520consistent%2520background%2520details%252C%250Awhich%2520limits%2520their%2520applicability.%2520Furthermore%252C%2520when%2520the%2520foreground%2520character%250Aundergoes%2520large%2520motion%2520variations%252C%2520inconsistencies%2520in%2520identity%2520and%2520clothing%250Adetails%2520become%2520evident.%2520To%2520address%2520these%2520problems%252C%2520we%2520propose%2520CharaConsist%252C%250Awhich%2520employs%2520point-tracking%2520attention%2520and%2520adaptive%2520token%2520merge%2520along%2520with%250Adecoupled%2520control%2520of%2520the%2520foreground%2520and%2520background.%2520CharaConsist%2520enables%250Afine-grained%2520consistency%2520for%2520both%2520foreground%2520and%2520background%252C%2520supporting%2520the%250Ageneration%2520of%2520one%2520character%2520in%2520continuous%2520shots%2520within%2520a%2520fixed%2520scene%2520or%2520in%250Adiscrete%2520shots%2520across%2520different%2520scenes.%2520Moreover%252C%2520CharaConsist%2520is%2520the%2520first%250Aconsistent%2520generation%2520method%2520tailored%2520for%2520text-to-image%2520DiT%2520model.%2520Its%2520ability%250Ato%2520maintain%2520fine-grained%2520consistency%252C%2520combined%2520with%2520the%2520larger%2520capacity%2520of%250Alatest%2520base%2520model%252C%2520enables%2520it%2520to%2520produce%2520high-quality%2520visual%2520outputs%252C%250Abroadening%2520its%2520applicability%2520to%2520a%2520wider%2520range%2520of%2520real-world%2520scenarios.%2520The%250Asource%2520code%2520has%2520been%2520released%2520at%2520https%253A//github.com/Murray-Wang/CharaConsist%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11533v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CharaConsist%3A%20Fine-Grained%20Consistent%20Character%20Generation&entry.906535625=Mengyu%20Wang%20and%20Henghui%20Ding%20and%20Jianing%20Peng%20and%20Yao%20Zhao%20and%20Yunpeng%20Chen%20and%20Yunchao%20Wei&entry.1292438233=%20%20In%20text-to-image%20generation%2C%20producing%20a%20series%20of%20consistent%20contents%20that%0Apreserve%20the%20same%20identity%20is%20highly%20valuable%20for%20real-world%20applications.%0AAlthough%20a%20few%20works%20have%20explored%20training-free%20methods%20to%20enhance%20the%0Aconsistency%20of%20generated%20subjects%2C%20we%20observe%20that%20they%20suffer%20from%20the%0Afollowing%20problems.%20First%2C%20they%20fail%20to%20maintain%20consistent%20background%20details%2C%0Awhich%20limits%20their%20applicability.%20Furthermore%2C%20when%20the%20foreground%20character%0Aundergoes%20large%20motion%20variations%2C%20inconsistencies%20in%20identity%20and%20clothing%0Adetails%20become%20evident.%20To%20address%20these%20problems%2C%20we%20propose%20CharaConsist%2C%0Awhich%20employs%20point-tracking%20attention%20and%20adaptive%20token%20merge%20along%20with%0Adecoupled%20control%20of%20the%20foreground%20and%20background.%20CharaConsist%20enables%0Afine-grained%20consistency%20for%20both%20foreground%20and%20background%2C%20supporting%20the%0Ageneration%20of%20one%20character%20in%20continuous%20shots%20within%20a%20fixed%20scene%20or%20in%0Adiscrete%20shots%20across%20different%20scenes.%20Moreover%2C%20CharaConsist%20is%20the%20first%0Aconsistent%20generation%20method%20tailored%20for%20text-to-image%20DiT%20model.%20Its%20ability%0Ato%20maintain%20fine-grained%20consistency%2C%20combined%20with%20the%20larger%20capacity%20of%0Alatest%20base%20model%2C%20enables%20it%20to%20produce%20high-quality%20visual%20outputs%2C%0Abroadening%20its%20applicability%20to%20a%20wider%20range%20of%20real-world%20scenarios.%20The%0Asource%20code%20has%20been%20released%20at%20https%3A//github.com/Murray-Wang/CharaConsist%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11533v1&entry.124074799=Read"},
{"title": "LongDocURL: a Comprehensive Multimodal Long Document Benchmark\n  Integrating Understanding, Reasoning, and Locating", "author": "Chao Deng and Jiale Yuan and Pi Bu and Peijie Wang and Zhong-Zhi Li and Jian Xu and Xiao-Hui Li and Yuan Gao and Jun Song and Bo Zheng and Cheng-Lin Liu", "abstract": "  Large vision language models (LVLMs) have improved the document understanding\ncapabilities remarkably, enabling the handling of complex document elements,\nlonger contexts, and a wider range of tasks. However, existing document\nunderstanding benchmarks have been limited to handling only a small number of\npages and fail to provide a comprehensive analysis of layout elements locating.\nIn this paper, we first define three primary task categories: Long Document\nUnderstanding, numerical Reasoning, and cross-element Locating, and then\npropose a comprehensive benchmark, LongDocURL, integrating above three primary\ntasks and comprising 20 sub-tasks categorized based on different primary tasks\nand answer evidences. Furthermore, we develop a semi-automated construction\npipeline and collect 2,325 high-quality question-answering pairs, covering more\nthan 33,000 pages of documents, significantly outperforming existing\nbenchmarks. Subsequently, we conduct comprehensive evaluation experiments on\nboth open-source and closed-source models across 26 different configurations,\nrevealing critical performance gaps in this field.\n", "link": "http://arxiv.org/abs/2412.18424v3", "date": "2025-07-15", "relevancy": 2.2959, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5849}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5849}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LongDocURL%3A%20a%20Comprehensive%20Multimodal%20Long%20Document%20Benchmark%0A%20%20Integrating%20Understanding%2C%20Reasoning%2C%20and%20Locating&body=Title%3A%20LongDocURL%3A%20a%20Comprehensive%20Multimodal%20Long%20Document%20Benchmark%0A%20%20Integrating%20Understanding%2C%20Reasoning%2C%20and%20Locating%0AAuthor%3A%20Chao%20Deng%20and%20Jiale%20Yuan%20and%20Pi%20Bu%20and%20Peijie%20Wang%20and%20Zhong-Zhi%20Li%20and%20Jian%20Xu%20and%20Xiao-Hui%20Li%20and%20Yuan%20Gao%20and%20Jun%20Song%20and%20Bo%20Zheng%20and%20Cheng-Lin%20Liu%0AAbstract%3A%20%20%20Large%20vision%20language%20models%20%28LVLMs%29%20have%20improved%20the%20document%20understanding%0Acapabilities%20remarkably%2C%20enabling%20the%20handling%20of%20complex%20document%20elements%2C%0Alonger%20contexts%2C%20and%20a%20wider%20range%20of%20tasks.%20However%2C%20existing%20document%0Aunderstanding%20benchmarks%20have%20been%20limited%20to%20handling%20only%20a%20small%20number%20of%0Apages%20and%20fail%20to%20provide%20a%20comprehensive%20analysis%20of%20layout%20elements%20locating.%0AIn%20this%20paper%2C%20we%20first%20define%20three%20primary%20task%20categories%3A%20Long%20Document%0AUnderstanding%2C%20numerical%20Reasoning%2C%20and%20cross-element%20Locating%2C%20and%20then%0Apropose%20a%20comprehensive%20benchmark%2C%20LongDocURL%2C%20integrating%20above%20three%20primary%0Atasks%20and%20comprising%2020%20sub-tasks%20categorized%20based%20on%20different%20primary%20tasks%0Aand%20answer%20evidences.%20Furthermore%2C%20we%20develop%20a%20semi-automated%20construction%0Apipeline%20and%20collect%202%2C325%20high-quality%20question-answering%20pairs%2C%20covering%20more%0Athan%2033%2C000%20pages%20of%20documents%2C%20significantly%20outperforming%20existing%0Abenchmarks.%20Subsequently%2C%20we%20conduct%20comprehensive%20evaluation%20experiments%20on%0Aboth%20open-source%20and%20closed-source%20models%20across%2026%20different%20configurations%2C%0Arevealing%20critical%20performance%20gaps%20in%20this%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18424v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLongDocURL%253A%2520a%2520Comprehensive%2520Multimodal%2520Long%2520Document%2520Benchmark%250A%2520%2520Integrating%2520Understanding%252C%2520Reasoning%252C%2520and%2520Locating%26entry.906535625%3DChao%2520Deng%2520and%2520Jiale%2520Yuan%2520and%2520Pi%2520Bu%2520and%2520Peijie%2520Wang%2520and%2520Zhong-Zhi%2520Li%2520and%2520Jian%2520Xu%2520and%2520Xiao-Hui%2520Li%2520and%2520Yuan%2520Gao%2520and%2520Jun%2520Song%2520and%2520Bo%2520Zheng%2520and%2520Cheng-Lin%2520Liu%26entry.1292438233%3D%2520%2520Large%2520vision%2520language%2520models%2520%2528LVLMs%2529%2520have%2520improved%2520the%2520document%2520understanding%250Acapabilities%2520remarkably%252C%2520enabling%2520the%2520handling%2520of%2520complex%2520document%2520elements%252C%250Alonger%2520contexts%252C%2520and%2520a%2520wider%2520range%2520of%2520tasks.%2520However%252C%2520existing%2520document%250Aunderstanding%2520benchmarks%2520have%2520been%2520limited%2520to%2520handling%2520only%2520a%2520small%2520number%2520of%250Apages%2520and%2520fail%2520to%2520provide%2520a%2520comprehensive%2520analysis%2520of%2520layout%2520elements%2520locating.%250AIn%2520this%2520paper%252C%2520we%2520first%2520define%2520three%2520primary%2520task%2520categories%253A%2520Long%2520Document%250AUnderstanding%252C%2520numerical%2520Reasoning%252C%2520and%2520cross-element%2520Locating%252C%2520and%2520then%250Apropose%2520a%2520comprehensive%2520benchmark%252C%2520LongDocURL%252C%2520integrating%2520above%2520three%2520primary%250Atasks%2520and%2520comprising%252020%2520sub-tasks%2520categorized%2520based%2520on%2520different%2520primary%2520tasks%250Aand%2520answer%2520evidences.%2520Furthermore%252C%2520we%2520develop%2520a%2520semi-automated%2520construction%250Apipeline%2520and%2520collect%25202%252C325%2520high-quality%2520question-answering%2520pairs%252C%2520covering%2520more%250Athan%252033%252C000%2520pages%2520of%2520documents%252C%2520significantly%2520outperforming%2520existing%250Abenchmarks.%2520Subsequently%252C%2520we%2520conduct%2520comprehensive%2520evaluation%2520experiments%2520on%250Aboth%2520open-source%2520and%2520closed-source%2520models%2520across%252026%2520different%2520configurations%252C%250Arevealing%2520critical%2520performance%2520gaps%2520in%2520this%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18424v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LongDocURL%3A%20a%20Comprehensive%20Multimodal%20Long%20Document%20Benchmark%0A%20%20Integrating%20Understanding%2C%20Reasoning%2C%20and%20Locating&entry.906535625=Chao%20Deng%20and%20Jiale%20Yuan%20and%20Pi%20Bu%20and%20Peijie%20Wang%20and%20Zhong-Zhi%20Li%20and%20Jian%20Xu%20and%20Xiao-Hui%20Li%20and%20Yuan%20Gao%20and%20Jun%20Song%20and%20Bo%20Zheng%20and%20Cheng-Lin%20Liu&entry.1292438233=%20%20Large%20vision%20language%20models%20%28LVLMs%29%20have%20improved%20the%20document%20understanding%0Acapabilities%20remarkably%2C%20enabling%20the%20handling%20of%20complex%20document%20elements%2C%0Alonger%20contexts%2C%20and%20a%20wider%20range%20of%20tasks.%20However%2C%20existing%20document%0Aunderstanding%20benchmarks%20have%20been%20limited%20to%20handling%20only%20a%20small%20number%20of%0Apages%20and%20fail%20to%20provide%20a%20comprehensive%20analysis%20of%20layout%20elements%20locating.%0AIn%20this%20paper%2C%20we%20first%20define%20three%20primary%20task%20categories%3A%20Long%20Document%0AUnderstanding%2C%20numerical%20Reasoning%2C%20and%20cross-element%20Locating%2C%20and%20then%0Apropose%20a%20comprehensive%20benchmark%2C%20LongDocURL%2C%20integrating%20above%20three%20primary%0Atasks%20and%20comprising%2020%20sub-tasks%20categorized%20based%20on%20different%20primary%20tasks%0Aand%20answer%20evidences.%20Furthermore%2C%20we%20develop%20a%20semi-automated%20construction%0Apipeline%20and%20collect%202%2C325%20high-quality%20question-answering%20pairs%2C%20covering%20more%0Athan%2033%2C000%20pages%20of%20documents%2C%20significantly%20outperforming%20existing%0Abenchmarks.%20Subsequently%2C%20we%20conduct%20comprehensive%20evaluation%20experiments%20on%0Aboth%20open-source%20and%20closed-source%20models%20across%2026%20different%20configurations%2C%0Arevealing%20critical%20performance%20gaps%20in%20this%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18424v3&entry.124074799=Read"},
{"title": "Multi-IMU Sensor Fusion for Legged Robots", "author": "Shuo Yang and John Z. Zhang and Ibrahima Sory Sow and Zachary Manchester", "abstract": "  This paper presents a state-estimation solution for legged robots that uses a\nset of low-cost, compact, and lightweight sensors to achieve low-drift pose and\nvelocity estimation under challenging locomotion conditions. The key idea is to\nleverage multiple inertial measurement units on different links of the robot to\ncorrect a major error source in standard proprioceptive odometry. We fuse the\ninertial sensor information and joint encoder measurements in an extended\nKalman filter, then combine the velocity estimate from this filter with camera\ndata in a factor-graph-based sliding-window estimator to form a\nvisual-inertial-leg odometry method. We validate our state estimator through\ncomprehensive theoretical analysis and hardware experiments performed using\nreal-world robot data collected during a variety of challenging locomotion\ntasks. Our algorithm consistently achieves minimal position deviation, even in\nscenarios involving substantial ground impact, foot slippage, and sudden body\nrotations. A C++ implementation, along with a large-scale dataset, is available\nat https://github.com/ShuoYangRobotics/Cerberus2.0.\n", "link": "http://arxiv.org/abs/2507.11447v1", "date": "2025-07-15", "relevancy": 2.2328, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6695}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5562}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-IMU%20Sensor%20Fusion%20for%20Legged%20Robots&body=Title%3A%20Multi-IMU%20Sensor%20Fusion%20for%20Legged%20Robots%0AAuthor%3A%20Shuo%20Yang%20and%20John%20Z.%20Zhang%20and%20Ibrahima%20Sory%20Sow%20and%20Zachary%20Manchester%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20state-estimation%20solution%20for%20legged%20robots%20that%20uses%20a%0Aset%20of%20low-cost%2C%20compact%2C%20and%20lightweight%20sensors%20to%20achieve%20low-drift%20pose%20and%0Avelocity%20estimation%20under%20challenging%20locomotion%20conditions.%20The%20key%20idea%20is%20to%0Aleverage%20multiple%20inertial%20measurement%20units%20on%20different%20links%20of%20the%20robot%20to%0Acorrect%20a%20major%20error%20source%20in%20standard%20proprioceptive%20odometry.%20We%20fuse%20the%0Ainertial%20sensor%20information%20and%20joint%20encoder%20measurements%20in%20an%20extended%0AKalman%20filter%2C%20then%20combine%20the%20velocity%20estimate%20from%20this%20filter%20with%20camera%0Adata%20in%20a%20factor-graph-based%20sliding-window%20estimator%20to%20form%20a%0Avisual-inertial-leg%20odometry%20method.%20We%20validate%20our%20state%20estimator%20through%0Acomprehensive%20theoretical%20analysis%20and%20hardware%20experiments%20performed%20using%0Areal-world%20robot%20data%20collected%20during%20a%20variety%20of%20challenging%20locomotion%0Atasks.%20Our%20algorithm%20consistently%20achieves%20minimal%20position%20deviation%2C%20even%20in%0Ascenarios%20involving%20substantial%20ground%20impact%2C%20foot%20slippage%2C%20and%20sudden%20body%0Arotations.%20A%20C%2B%2B%20implementation%2C%20along%20with%20a%20large-scale%20dataset%2C%20is%20available%0Aat%20https%3A//github.com/ShuoYangRobotics/Cerberus2.0.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11447v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-IMU%2520Sensor%2520Fusion%2520for%2520Legged%2520Robots%26entry.906535625%3DShuo%2520Yang%2520and%2520John%2520Z.%2520Zhang%2520and%2520Ibrahima%2520Sory%2520Sow%2520and%2520Zachary%2520Manchester%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520state-estimation%2520solution%2520for%2520legged%2520robots%2520that%2520uses%2520a%250Aset%2520of%2520low-cost%252C%2520compact%252C%2520and%2520lightweight%2520sensors%2520to%2520achieve%2520low-drift%2520pose%2520and%250Avelocity%2520estimation%2520under%2520challenging%2520locomotion%2520conditions.%2520The%2520key%2520idea%2520is%2520to%250Aleverage%2520multiple%2520inertial%2520measurement%2520units%2520on%2520different%2520links%2520of%2520the%2520robot%2520to%250Acorrect%2520a%2520major%2520error%2520source%2520in%2520standard%2520proprioceptive%2520odometry.%2520We%2520fuse%2520the%250Ainertial%2520sensor%2520information%2520and%2520joint%2520encoder%2520measurements%2520in%2520an%2520extended%250AKalman%2520filter%252C%2520then%2520combine%2520the%2520velocity%2520estimate%2520from%2520this%2520filter%2520with%2520camera%250Adata%2520in%2520a%2520factor-graph-based%2520sliding-window%2520estimator%2520to%2520form%2520a%250Avisual-inertial-leg%2520odometry%2520method.%2520We%2520validate%2520our%2520state%2520estimator%2520through%250Acomprehensive%2520theoretical%2520analysis%2520and%2520hardware%2520experiments%2520performed%2520using%250Areal-world%2520robot%2520data%2520collected%2520during%2520a%2520variety%2520of%2520challenging%2520locomotion%250Atasks.%2520Our%2520algorithm%2520consistently%2520achieves%2520minimal%2520position%2520deviation%252C%2520even%2520in%250Ascenarios%2520involving%2520substantial%2520ground%2520impact%252C%2520foot%2520slippage%252C%2520and%2520sudden%2520body%250Arotations.%2520A%2520C%252B%252B%2520implementation%252C%2520along%2520with%2520a%2520large-scale%2520dataset%252C%2520is%2520available%250Aat%2520https%253A//github.com/ShuoYangRobotics/Cerberus2.0.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11447v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-IMU%20Sensor%20Fusion%20for%20Legged%20Robots&entry.906535625=Shuo%20Yang%20and%20John%20Z.%20Zhang%20and%20Ibrahima%20Sory%20Sow%20and%20Zachary%20Manchester&entry.1292438233=%20%20This%20paper%20presents%20a%20state-estimation%20solution%20for%20legged%20robots%20that%20uses%20a%0Aset%20of%20low-cost%2C%20compact%2C%20and%20lightweight%20sensors%20to%20achieve%20low-drift%20pose%20and%0Avelocity%20estimation%20under%20challenging%20locomotion%20conditions.%20The%20key%20idea%20is%20to%0Aleverage%20multiple%20inertial%20measurement%20units%20on%20different%20links%20of%20the%20robot%20to%0Acorrect%20a%20major%20error%20source%20in%20standard%20proprioceptive%20odometry.%20We%20fuse%20the%0Ainertial%20sensor%20information%20and%20joint%20encoder%20measurements%20in%20an%20extended%0AKalman%20filter%2C%20then%20combine%20the%20velocity%20estimate%20from%20this%20filter%20with%20camera%0Adata%20in%20a%20factor-graph-based%20sliding-window%20estimator%20to%20form%20a%0Avisual-inertial-leg%20odometry%20method.%20We%20validate%20our%20state%20estimator%20through%0Acomprehensive%20theoretical%20analysis%20and%20hardware%20experiments%20performed%20using%0Areal-world%20robot%20data%20collected%20during%20a%20variety%20of%20challenging%20locomotion%0Atasks.%20Our%20algorithm%20consistently%20achieves%20minimal%20position%20deviation%2C%20even%20in%0Ascenarios%20involving%20substantial%20ground%20impact%2C%20foot%20slippage%2C%20and%20sudden%20body%0Arotations.%20A%20C%2B%2B%20implementation%2C%20along%20with%20a%20large-scale%20dataset%2C%20is%20available%0Aat%20https%3A//github.com/ShuoYangRobotics/Cerberus2.0.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11447v1&entry.124074799=Read"},
{"title": "HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry\n  Synthesis and Controllable Editing", "author": "Pan Du and Mingqi Xu and Xiaozhi Zhu and Jian-xun Wang", "abstract": "  Accurate characterization of vascular geometry is essential for\ncardiovascular diagnosis and treatment planning. Traditional statistical shape\nmodeling (SSM) methods rely on linear assumptions, limiting their expressivity\nand scalability to complex topologies such as multi-branch vascular structures.\nWe introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular\ngeometry Synthesis, which integrates NURBS surface parameterization with\ndiffusion-based generative modeling to synthesize realistic, fine-grained\naortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates\nanatomically faithful aortas with supra-aortic branches, yielding biomarker\ndistributions that closely match those of the original dataset. HUG-VAS adopts\na hierarchical architecture comprising a denoising diffusion model that\ngenerates centerlines and a guided diffusion model that synthesizes radial\nprofiles conditioned on those centerlines, thereby capturing two layers of\nanatomical variability. Critically, the framework supports zero-shot\nconditional generation from image-derived priors, enabling practical\napplications such as interactive semi-automatic segmentation, robust\nreconstruction under degraded imaging conditions, and implantable device\noptimization. To our knowledge, HUG-VAS is the first SSM framework to bridge\nimage-derived priors with generative shape modeling via a unified integration\nof NURBS parameterization and hierarchical diffusion processes.\n", "link": "http://arxiv.org/abs/2507.11474v1", "date": "2025-07-15", "relevancy": 2.2272, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5621}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5541}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HUG-VAS%3A%20A%20Hierarchical%20NURBS-Based%20Generative%20Model%20for%20Aortic%20Geometry%0A%20%20Synthesis%20and%20Controllable%20Editing&body=Title%3A%20HUG-VAS%3A%20A%20Hierarchical%20NURBS-Based%20Generative%20Model%20for%20Aortic%20Geometry%0A%20%20Synthesis%20and%20Controllable%20Editing%0AAuthor%3A%20Pan%20Du%20and%20Mingqi%20Xu%20and%20Xiaozhi%20Zhu%20and%20Jian-xun%20Wang%0AAbstract%3A%20%20%20Accurate%20characterization%20of%20vascular%20geometry%20is%20essential%20for%0Acardiovascular%20diagnosis%20and%20treatment%20planning.%20Traditional%20statistical%20shape%0Amodeling%20%28SSM%29%20methods%20rely%20on%20linear%20assumptions%2C%20limiting%20their%20expressivity%0Aand%20scalability%20to%20complex%20topologies%20such%20as%20multi-branch%20vascular%20structures.%0AWe%20introduce%20HUG-VAS%2C%20a%20Hierarchical%20NURBS%20Generative%20model%20for%20Vascular%0Ageometry%20Synthesis%2C%20which%20integrates%20NURBS%20surface%20parameterization%20with%0Adiffusion-based%20generative%20modeling%20to%20synthesize%20realistic%2C%20fine-grained%0Aaortic%20geometries.%20Trained%20with%2021%20patient-specific%20samples%2C%20HUG-VAS%20generates%0Aanatomically%20faithful%20aortas%20with%20supra-aortic%20branches%2C%20yielding%20biomarker%0Adistributions%20that%20closely%20match%20those%20of%20the%20original%20dataset.%20HUG-VAS%20adopts%0Aa%20hierarchical%20architecture%20comprising%20a%20denoising%20diffusion%20model%20that%0Agenerates%20centerlines%20and%20a%20guided%20diffusion%20model%20that%20synthesizes%20radial%0Aprofiles%20conditioned%20on%20those%20centerlines%2C%20thereby%20capturing%20two%20layers%20of%0Aanatomical%20variability.%20Critically%2C%20the%20framework%20supports%20zero-shot%0Aconditional%20generation%20from%20image-derived%20priors%2C%20enabling%20practical%0Aapplications%20such%20as%20interactive%20semi-automatic%20segmentation%2C%20robust%0Areconstruction%20under%20degraded%20imaging%20conditions%2C%20and%20implantable%20device%0Aoptimization.%20To%20our%20knowledge%2C%20HUG-VAS%20is%20the%20first%20SSM%20framework%20to%20bridge%0Aimage-derived%20priors%20with%20generative%20shape%20modeling%20via%20a%20unified%20integration%0Aof%20NURBS%20parameterization%20and%20hierarchical%20diffusion%20processes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11474v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHUG-VAS%253A%2520A%2520Hierarchical%2520NURBS-Based%2520Generative%2520Model%2520for%2520Aortic%2520Geometry%250A%2520%2520Synthesis%2520and%2520Controllable%2520Editing%26entry.906535625%3DPan%2520Du%2520and%2520Mingqi%2520Xu%2520and%2520Xiaozhi%2520Zhu%2520and%2520Jian-xun%2520Wang%26entry.1292438233%3D%2520%2520Accurate%2520characterization%2520of%2520vascular%2520geometry%2520is%2520essential%2520for%250Acardiovascular%2520diagnosis%2520and%2520treatment%2520planning.%2520Traditional%2520statistical%2520shape%250Amodeling%2520%2528SSM%2529%2520methods%2520rely%2520on%2520linear%2520assumptions%252C%2520limiting%2520their%2520expressivity%250Aand%2520scalability%2520to%2520complex%2520topologies%2520such%2520as%2520multi-branch%2520vascular%2520structures.%250AWe%2520introduce%2520HUG-VAS%252C%2520a%2520Hierarchical%2520NURBS%2520Generative%2520model%2520for%2520Vascular%250Ageometry%2520Synthesis%252C%2520which%2520integrates%2520NURBS%2520surface%2520parameterization%2520with%250Adiffusion-based%2520generative%2520modeling%2520to%2520synthesize%2520realistic%252C%2520fine-grained%250Aaortic%2520geometries.%2520Trained%2520with%252021%2520patient-specific%2520samples%252C%2520HUG-VAS%2520generates%250Aanatomically%2520faithful%2520aortas%2520with%2520supra-aortic%2520branches%252C%2520yielding%2520biomarker%250Adistributions%2520that%2520closely%2520match%2520those%2520of%2520the%2520original%2520dataset.%2520HUG-VAS%2520adopts%250Aa%2520hierarchical%2520architecture%2520comprising%2520a%2520denoising%2520diffusion%2520model%2520that%250Agenerates%2520centerlines%2520and%2520a%2520guided%2520diffusion%2520model%2520that%2520synthesizes%2520radial%250Aprofiles%2520conditioned%2520on%2520those%2520centerlines%252C%2520thereby%2520capturing%2520two%2520layers%2520of%250Aanatomical%2520variability.%2520Critically%252C%2520the%2520framework%2520supports%2520zero-shot%250Aconditional%2520generation%2520from%2520image-derived%2520priors%252C%2520enabling%2520practical%250Aapplications%2520such%2520as%2520interactive%2520semi-automatic%2520segmentation%252C%2520robust%250Areconstruction%2520under%2520degraded%2520imaging%2520conditions%252C%2520and%2520implantable%2520device%250Aoptimization.%2520To%2520our%2520knowledge%252C%2520HUG-VAS%2520is%2520the%2520first%2520SSM%2520framework%2520to%2520bridge%250Aimage-derived%2520priors%2520with%2520generative%2520shape%2520modeling%2520via%2520a%2520unified%2520integration%250Aof%2520NURBS%2520parameterization%2520and%2520hierarchical%2520diffusion%2520processes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11474v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HUG-VAS%3A%20A%20Hierarchical%20NURBS-Based%20Generative%20Model%20for%20Aortic%20Geometry%0A%20%20Synthesis%20and%20Controllable%20Editing&entry.906535625=Pan%20Du%20and%20Mingqi%20Xu%20and%20Xiaozhi%20Zhu%20and%20Jian-xun%20Wang&entry.1292438233=%20%20Accurate%20characterization%20of%20vascular%20geometry%20is%20essential%20for%0Acardiovascular%20diagnosis%20and%20treatment%20planning.%20Traditional%20statistical%20shape%0Amodeling%20%28SSM%29%20methods%20rely%20on%20linear%20assumptions%2C%20limiting%20their%20expressivity%0Aand%20scalability%20to%20complex%20topologies%20such%20as%20multi-branch%20vascular%20structures.%0AWe%20introduce%20HUG-VAS%2C%20a%20Hierarchical%20NURBS%20Generative%20model%20for%20Vascular%0Ageometry%20Synthesis%2C%20which%20integrates%20NURBS%20surface%20parameterization%20with%0Adiffusion-based%20generative%20modeling%20to%20synthesize%20realistic%2C%20fine-grained%0Aaortic%20geometries.%20Trained%20with%2021%20patient-specific%20samples%2C%20HUG-VAS%20generates%0Aanatomically%20faithful%20aortas%20with%20supra-aortic%20branches%2C%20yielding%20biomarker%0Adistributions%20that%20closely%20match%20those%20of%20the%20original%20dataset.%20HUG-VAS%20adopts%0Aa%20hierarchical%20architecture%20comprising%20a%20denoising%20diffusion%20model%20that%0Agenerates%20centerlines%20and%20a%20guided%20diffusion%20model%20that%20synthesizes%20radial%0Aprofiles%20conditioned%20on%20those%20centerlines%2C%20thereby%20capturing%20two%20layers%20of%0Aanatomical%20variability.%20Critically%2C%20the%20framework%20supports%20zero-shot%0Aconditional%20generation%20from%20image-derived%20priors%2C%20enabling%20practical%0Aapplications%20such%20as%20interactive%20semi-automatic%20segmentation%2C%20robust%0Areconstruction%20under%20degraded%20imaging%20conditions%2C%20and%20implantable%20device%0Aoptimization.%20To%20our%20knowledge%2C%20HUG-VAS%20is%20the%20first%20SSM%20framework%20to%20bridge%0Aimage-derived%20priors%20with%20generative%20shape%20modeling%20via%20a%20unified%20integration%0Aof%20NURBS%20parameterization%20and%20hierarchical%20diffusion%20processes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11474v1&entry.124074799=Read"},
{"title": "Deep Equilibrium models for Poisson Imaging Inverse problems via Mirror\n  Descent", "author": "Christian Daniele and Silvia Villa and Samuel Vaiter and Luca Calatroni", "abstract": "  Deep Equilibrium Models (DEQs) are implicit neural networks with fixed\npoints, which have recently gained attention for learning image regularization\nfunctionals, particularly in settings involving Gaussian fidelities, where\nassumptions on the forward operator ensure contractiveness of standard\n(proximal) Gradient Descent operators. In this work, we extend the application\nof DEQs to Poisson inverse problems, where the data fidelity term is more\nappropriately modeled by the Kullback-Leibler divergence. To this end, we\nintroduce a novel DEQ formulation based on Mirror Descent defined in terms of a\ntailored non-Euclidean geometry that naturally adapts with the structure of the\ndata term. This enables the learning of neural regularizers within a principled\ntraining framework. We derive sufficient conditions to guarantee the\nconvergence of the learned reconstruction scheme and propose computational\nstrategies that enable both efficient training and fully parameter-free\ninference. Numerical experiments show that our method outperforms traditional\nmodel-based approaches and it is comparable to the performance of Bregman\nPlug-and-Play methods, while mitigating their typical drawbacks - namely,\nsensitivity to initialization and careful tuning of hyperparameters. The code\nis publicly available at https://github.com/christiandaniele/DEQ-MD.\n", "link": "http://arxiv.org/abs/2507.11461v1", "date": "2025-07-15", "relevancy": 2.2212, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5881}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5332}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Equilibrium%20models%20for%20Poisson%20Imaging%20Inverse%20problems%20via%20Mirror%0A%20%20Descent&body=Title%3A%20Deep%20Equilibrium%20models%20for%20Poisson%20Imaging%20Inverse%20problems%20via%20Mirror%0A%20%20Descent%0AAuthor%3A%20Christian%20Daniele%20and%20Silvia%20Villa%20and%20Samuel%20Vaiter%20and%20Luca%20Calatroni%0AAbstract%3A%20%20%20Deep%20Equilibrium%20Models%20%28DEQs%29%20are%20implicit%20neural%20networks%20with%20fixed%0Apoints%2C%20which%20have%20recently%20gained%20attention%20for%20learning%20image%20regularization%0Afunctionals%2C%20particularly%20in%20settings%20involving%20Gaussian%20fidelities%2C%20where%0Aassumptions%20on%20the%20forward%20operator%20ensure%20contractiveness%20of%20standard%0A%28proximal%29%20Gradient%20Descent%20operators.%20In%20this%20work%2C%20we%20extend%20the%20application%0Aof%20DEQs%20to%20Poisson%20inverse%20problems%2C%20where%20the%20data%20fidelity%20term%20is%20more%0Aappropriately%20modeled%20by%20the%20Kullback-Leibler%20divergence.%20To%20this%20end%2C%20we%0Aintroduce%20a%20novel%20DEQ%20formulation%20based%20on%20Mirror%20Descent%20defined%20in%20terms%20of%20a%0Atailored%20non-Euclidean%20geometry%20that%20naturally%20adapts%20with%20the%20structure%20of%20the%0Adata%20term.%20This%20enables%20the%20learning%20of%20neural%20regularizers%20within%20a%20principled%0Atraining%20framework.%20We%20derive%20sufficient%20conditions%20to%20guarantee%20the%0Aconvergence%20of%20the%20learned%20reconstruction%20scheme%20and%20propose%20computational%0Astrategies%20that%20enable%20both%20efficient%20training%20and%20fully%20parameter-free%0Ainference.%20Numerical%20experiments%20show%20that%20our%20method%20outperforms%20traditional%0Amodel-based%20approaches%20and%20it%20is%20comparable%20to%20the%20performance%20of%20Bregman%0APlug-and-Play%20methods%2C%20while%20mitigating%20their%20typical%20drawbacks%20-%20namely%2C%0Asensitivity%20to%20initialization%20and%20careful%20tuning%20of%20hyperparameters.%20The%20code%0Ais%20publicly%20available%20at%20https%3A//github.com/christiandaniele/DEQ-MD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Equilibrium%2520models%2520for%2520Poisson%2520Imaging%2520Inverse%2520problems%2520via%2520Mirror%250A%2520%2520Descent%26entry.906535625%3DChristian%2520Daniele%2520and%2520Silvia%2520Villa%2520and%2520Samuel%2520Vaiter%2520and%2520Luca%2520Calatroni%26entry.1292438233%3D%2520%2520Deep%2520Equilibrium%2520Models%2520%2528DEQs%2529%2520are%2520implicit%2520neural%2520networks%2520with%2520fixed%250Apoints%252C%2520which%2520have%2520recently%2520gained%2520attention%2520for%2520learning%2520image%2520regularization%250Afunctionals%252C%2520particularly%2520in%2520settings%2520involving%2520Gaussian%2520fidelities%252C%2520where%250Aassumptions%2520on%2520the%2520forward%2520operator%2520ensure%2520contractiveness%2520of%2520standard%250A%2528proximal%2529%2520Gradient%2520Descent%2520operators.%2520In%2520this%2520work%252C%2520we%2520extend%2520the%2520application%250Aof%2520DEQs%2520to%2520Poisson%2520inverse%2520problems%252C%2520where%2520the%2520data%2520fidelity%2520term%2520is%2520more%250Aappropriately%2520modeled%2520by%2520the%2520Kullback-Leibler%2520divergence.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520a%2520novel%2520DEQ%2520formulation%2520based%2520on%2520Mirror%2520Descent%2520defined%2520in%2520terms%2520of%2520a%250Atailored%2520non-Euclidean%2520geometry%2520that%2520naturally%2520adapts%2520with%2520the%2520structure%2520of%2520the%250Adata%2520term.%2520This%2520enables%2520the%2520learning%2520of%2520neural%2520regularizers%2520within%2520a%2520principled%250Atraining%2520framework.%2520We%2520derive%2520sufficient%2520conditions%2520to%2520guarantee%2520the%250Aconvergence%2520of%2520the%2520learned%2520reconstruction%2520scheme%2520and%2520propose%2520computational%250Astrategies%2520that%2520enable%2520both%2520efficient%2520training%2520and%2520fully%2520parameter-free%250Ainference.%2520Numerical%2520experiments%2520show%2520that%2520our%2520method%2520outperforms%2520traditional%250Amodel-based%2520approaches%2520and%2520it%2520is%2520comparable%2520to%2520the%2520performance%2520of%2520Bregman%250APlug-and-Play%2520methods%252C%2520while%2520mitigating%2520their%2520typical%2520drawbacks%2520-%2520namely%252C%250Asensitivity%2520to%2520initialization%2520and%2520careful%2520tuning%2520of%2520hyperparameters.%2520The%2520code%250Ais%2520publicly%2520available%2520at%2520https%253A//github.com/christiandaniele/DEQ-MD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Equilibrium%20models%20for%20Poisson%20Imaging%20Inverse%20problems%20via%20Mirror%0A%20%20Descent&entry.906535625=Christian%20Daniele%20and%20Silvia%20Villa%20and%20Samuel%20Vaiter%20and%20Luca%20Calatroni&entry.1292438233=%20%20Deep%20Equilibrium%20Models%20%28DEQs%29%20are%20implicit%20neural%20networks%20with%20fixed%0Apoints%2C%20which%20have%20recently%20gained%20attention%20for%20learning%20image%20regularization%0Afunctionals%2C%20particularly%20in%20settings%20involving%20Gaussian%20fidelities%2C%20where%0Aassumptions%20on%20the%20forward%20operator%20ensure%20contractiveness%20of%20standard%0A%28proximal%29%20Gradient%20Descent%20operators.%20In%20this%20work%2C%20we%20extend%20the%20application%0Aof%20DEQs%20to%20Poisson%20inverse%20problems%2C%20where%20the%20data%20fidelity%20term%20is%20more%0Aappropriately%20modeled%20by%20the%20Kullback-Leibler%20divergence.%20To%20this%20end%2C%20we%0Aintroduce%20a%20novel%20DEQ%20formulation%20based%20on%20Mirror%20Descent%20defined%20in%20terms%20of%20a%0Atailored%20non-Euclidean%20geometry%20that%20naturally%20adapts%20with%20the%20structure%20of%20the%0Adata%20term.%20This%20enables%20the%20learning%20of%20neural%20regularizers%20within%20a%20principled%0Atraining%20framework.%20We%20derive%20sufficient%20conditions%20to%20guarantee%20the%0Aconvergence%20of%20the%20learned%20reconstruction%20scheme%20and%20propose%20computational%0Astrategies%20that%20enable%20both%20efficient%20training%20and%20fully%20parameter-free%0Ainference.%20Numerical%20experiments%20show%20that%20our%20method%20outperforms%20traditional%0Amodel-based%20approaches%20and%20it%20is%20comparable%20to%20the%20performance%20of%20Bregman%0APlug-and-Play%20methods%2C%20while%20mitigating%20their%20typical%20drawbacks%20-%20namely%2C%0Asensitivity%20to%20initialization%20and%20careful%20tuning%20of%20hyperparameters.%20The%20code%0Ais%20publicly%20available%20at%20https%3A//github.com/christiandaniele/DEQ-MD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11461v1&entry.124074799=Read"},
{"title": "How Many Instructions Can LLMs Follow at Once?", "author": "Daniel Jaroslawicz and Brendan Whiting and Parth Shah and Karime Maamari", "abstract": "  Production-grade LLM systems require robust adherence to dozens or even\nhundreds of instructions simultaneously. However, the instruction-following\ncapabilities of LLMs at high instruction densities have not yet been\ncharacterized, as existing benchmarks only evaluate models on tasks with a\nsingle or few instructions. We introduce IFScale, a simple benchmark of 500\nkeyword-inclusion instructions for a business report writing task to measure\nhow instruction-following performance degrades as instruction density\nincreases. We evaluate 20 state-of-the-art models across seven major providers\nand find that even the best frontier models only achieve 68% accuracy at the\nmax density of 500 instructions. Our analysis reveals model size and reasoning\ncapability to correlate with 3 distinct performance degradation patterns, bias\ntowards earlier instructions, and distinct categories of instruction-following\nerrors. Our insights can help inform design of instruction-dense prompts in\nreal-world applications and highlight important performance-latency tradeoffs.\nWe open-source the benchmark and all results for further analysis at\nhttps://distylai.github.io/IFScale.\n", "link": "http://arxiv.org/abs/2507.11538v1", "date": "2025-07-15", "relevancy": 2.1915, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4481}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Many%20Instructions%20Can%20LLMs%20Follow%20at%20Once%3F&body=Title%3A%20How%20Many%20Instructions%20Can%20LLMs%20Follow%20at%20Once%3F%0AAuthor%3A%20Daniel%20Jaroslawicz%20and%20Brendan%20Whiting%20and%20Parth%20Shah%20and%20Karime%20Maamari%0AAbstract%3A%20%20%20Production-grade%20LLM%20systems%20require%20robust%20adherence%20to%20dozens%20or%20even%0Ahundreds%20of%20instructions%20simultaneously.%20However%2C%20the%20instruction-following%0Acapabilities%20of%20LLMs%20at%20high%20instruction%20densities%20have%20not%20yet%20been%0Acharacterized%2C%20as%20existing%20benchmarks%20only%20evaluate%20models%20on%20tasks%20with%20a%0Asingle%20or%20few%20instructions.%20We%20introduce%20IFScale%2C%20a%20simple%20benchmark%20of%20500%0Akeyword-inclusion%20instructions%20for%20a%20business%20report%20writing%20task%20to%20measure%0Ahow%20instruction-following%20performance%20degrades%20as%20instruction%20density%0Aincreases.%20We%20evaluate%2020%20state-of-the-art%20models%20across%20seven%20major%20providers%0Aand%20find%20that%20even%20the%20best%20frontier%20models%20only%20achieve%2068%25%20accuracy%20at%20the%0Amax%20density%20of%20500%20instructions.%20Our%20analysis%20reveals%20model%20size%20and%20reasoning%0Acapability%20to%20correlate%20with%203%20distinct%20performance%20degradation%20patterns%2C%20bias%0Atowards%20earlier%20instructions%2C%20and%20distinct%20categories%20of%20instruction-following%0Aerrors.%20Our%20insights%20can%20help%20inform%20design%20of%20instruction-dense%20prompts%20in%0Areal-world%20applications%20and%20highlight%20important%20performance-latency%20tradeoffs.%0AWe%20open-source%20the%20benchmark%20and%20all%20results%20for%20further%20analysis%20at%0Ahttps%3A//distylai.github.io/IFScale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11538v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Many%2520Instructions%2520Can%2520LLMs%2520Follow%2520at%2520Once%253F%26entry.906535625%3DDaniel%2520Jaroslawicz%2520and%2520Brendan%2520Whiting%2520and%2520Parth%2520Shah%2520and%2520Karime%2520Maamari%26entry.1292438233%3D%2520%2520Production-grade%2520LLM%2520systems%2520require%2520robust%2520adherence%2520to%2520dozens%2520or%2520even%250Ahundreds%2520of%2520instructions%2520simultaneously.%2520However%252C%2520the%2520instruction-following%250Acapabilities%2520of%2520LLMs%2520at%2520high%2520instruction%2520densities%2520have%2520not%2520yet%2520been%250Acharacterized%252C%2520as%2520existing%2520benchmarks%2520only%2520evaluate%2520models%2520on%2520tasks%2520with%2520a%250Asingle%2520or%2520few%2520instructions.%2520We%2520introduce%2520IFScale%252C%2520a%2520simple%2520benchmark%2520of%2520500%250Akeyword-inclusion%2520instructions%2520for%2520a%2520business%2520report%2520writing%2520task%2520to%2520measure%250Ahow%2520instruction-following%2520performance%2520degrades%2520as%2520instruction%2520density%250Aincreases.%2520We%2520evaluate%252020%2520state-of-the-art%2520models%2520across%2520seven%2520major%2520providers%250Aand%2520find%2520that%2520even%2520the%2520best%2520frontier%2520models%2520only%2520achieve%252068%2525%2520accuracy%2520at%2520the%250Amax%2520density%2520of%2520500%2520instructions.%2520Our%2520analysis%2520reveals%2520model%2520size%2520and%2520reasoning%250Acapability%2520to%2520correlate%2520with%25203%2520distinct%2520performance%2520degradation%2520patterns%252C%2520bias%250Atowards%2520earlier%2520instructions%252C%2520and%2520distinct%2520categories%2520of%2520instruction-following%250Aerrors.%2520Our%2520insights%2520can%2520help%2520inform%2520design%2520of%2520instruction-dense%2520prompts%2520in%250Areal-world%2520applications%2520and%2520highlight%2520important%2520performance-latency%2520tradeoffs.%250AWe%2520open-source%2520the%2520benchmark%2520and%2520all%2520results%2520for%2520further%2520analysis%2520at%250Ahttps%253A//distylai.github.io/IFScale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11538v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Many%20Instructions%20Can%20LLMs%20Follow%20at%20Once%3F&entry.906535625=Daniel%20Jaroslawicz%20and%20Brendan%20Whiting%20and%20Parth%20Shah%20and%20Karime%20Maamari&entry.1292438233=%20%20Production-grade%20LLM%20systems%20require%20robust%20adherence%20to%20dozens%20or%20even%0Ahundreds%20of%20instructions%20simultaneously.%20However%2C%20the%20instruction-following%0Acapabilities%20of%20LLMs%20at%20high%20instruction%20densities%20have%20not%20yet%20been%0Acharacterized%2C%20as%20existing%20benchmarks%20only%20evaluate%20models%20on%20tasks%20with%20a%0Asingle%20or%20few%20instructions.%20We%20introduce%20IFScale%2C%20a%20simple%20benchmark%20of%20500%0Akeyword-inclusion%20instructions%20for%20a%20business%20report%20writing%20task%20to%20measure%0Ahow%20instruction-following%20performance%20degrades%20as%20instruction%20density%0Aincreases.%20We%20evaluate%2020%20state-of-the-art%20models%20across%20seven%20major%20providers%0Aand%20find%20that%20even%20the%20best%20frontier%20models%20only%20achieve%2068%25%20accuracy%20at%20the%0Amax%20density%20of%20500%20instructions.%20Our%20analysis%20reveals%20model%20size%20and%20reasoning%0Acapability%20to%20correlate%20with%203%20distinct%20performance%20degradation%20patterns%2C%20bias%0Atowards%20earlier%20instructions%2C%20and%20distinct%20categories%20of%20instruction-following%0Aerrors.%20Our%20insights%20can%20help%20inform%20design%20of%20instruction-dense%20prompts%20in%0Areal-world%20applications%20and%20highlight%20important%20performance-latency%20tradeoffs.%0AWe%20open-source%20the%20benchmark%20and%20all%20results%20for%20further%20analysis%20at%0Ahttps%3A//distylai.github.io/IFScale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11538v1&entry.124074799=Read"},
{"title": "Augmenting End-to-End Steering Angle Prediction with CAN Bus Data", "author": "Amit Singh", "abstract": "  In recent years, end to end steering prediction for autonomous vehicles has\nbecome a major area of research. The primary method for achieving end to end\nsteering was to use computer vision models on a live feed of video data.\nHowever, to further increase accuracy, many companies have added data from\nlight detection and ranging (LiDAR) and or radar sensors through sensor fusion.\nHowever, the addition of lasers and sensors comes at a high financial cost. In\nthis paper, I address both of these issues by increasing the accuracy of the\ncomputer vision models without the increased cost of using LiDAR and or\nsensors. I achieved this by improving the accuracy of computer vision models by\nsensor fusing CAN bus data, a vehicle protocol, with video data. CAN bus data\nis a rich source of information about the vehicle's state, including its speed,\nsteering angle, and acceleration. By fusing this data with video data, the\naccuracy of the computer vision model's predictions can be improved. When I\ntrained the model without CAN bus data, I obtained an RMSE of 0.02492, while\nthe model trained with the CAN bus data achieved an RMSE of 0.01970. This\nfinding indicates that fusing CAN Bus data with video data can reduce the\ncomputer vision model's prediction error by 20% with some models decreasing the\nerror by 80%.\n", "link": "http://arxiv.org/abs/2310.14162v2", "date": "2025-07-15", "relevancy": 2.1732, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5713}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5388}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Augmenting%20End-to-End%20Steering%20Angle%20Prediction%20with%20CAN%20Bus%20Data&body=Title%3A%20Augmenting%20End-to-End%20Steering%20Angle%20Prediction%20with%20CAN%20Bus%20Data%0AAuthor%3A%20Amit%20Singh%0AAbstract%3A%20%20%20In%20recent%20years%2C%20end%20to%20end%20steering%20prediction%20for%20autonomous%20vehicles%20has%0Abecome%20a%20major%20area%20of%20research.%20The%20primary%20method%20for%20achieving%20end%20to%20end%0Asteering%20was%20to%20use%20computer%20vision%20models%20on%20a%20live%20feed%20of%20video%20data.%0AHowever%2C%20to%20further%20increase%20accuracy%2C%20many%20companies%20have%20added%20data%20from%0Alight%20detection%20and%20ranging%20%28LiDAR%29%20and%20or%20radar%20sensors%20through%20sensor%20fusion.%0AHowever%2C%20the%20addition%20of%20lasers%20and%20sensors%20comes%20at%20a%20high%20financial%20cost.%20In%0Athis%20paper%2C%20I%20address%20both%20of%20these%20issues%20by%20increasing%20the%20accuracy%20of%20the%0Acomputer%20vision%20models%20without%20the%20increased%20cost%20of%20using%20LiDAR%20and%20or%0Asensors.%20I%20achieved%20this%20by%20improving%20the%20accuracy%20of%20computer%20vision%20models%20by%0Asensor%20fusing%20CAN%20bus%20data%2C%20a%20vehicle%20protocol%2C%20with%20video%20data.%20CAN%20bus%20data%0Ais%20a%20rich%20source%20of%20information%20about%20the%20vehicle%27s%20state%2C%20including%20its%20speed%2C%0Asteering%20angle%2C%20and%20acceleration.%20By%20fusing%20this%20data%20with%20video%20data%2C%20the%0Aaccuracy%20of%20the%20computer%20vision%20model%27s%20predictions%20can%20be%20improved.%20When%20I%0Atrained%20the%20model%20without%20CAN%20bus%20data%2C%20I%20obtained%20an%20RMSE%20of%200.02492%2C%20while%0Athe%20model%20trained%20with%20the%20CAN%20bus%20data%20achieved%20an%20RMSE%20of%200.01970.%20This%0Afinding%20indicates%20that%20fusing%20CAN%20Bus%20data%20with%20video%20data%20can%20reduce%20the%0Acomputer%20vision%20model%27s%20prediction%20error%20by%2020%25%20with%20some%20models%20decreasing%20the%0Aerror%20by%2080%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.14162v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAugmenting%2520End-to-End%2520Steering%2520Angle%2520Prediction%2520with%2520CAN%2520Bus%2520Data%26entry.906535625%3DAmit%2520Singh%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520end%2520to%2520end%2520steering%2520prediction%2520for%2520autonomous%2520vehicles%2520has%250Abecome%2520a%2520major%2520area%2520of%2520research.%2520The%2520primary%2520method%2520for%2520achieving%2520end%2520to%2520end%250Asteering%2520was%2520to%2520use%2520computer%2520vision%2520models%2520on%2520a%2520live%2520feed%2520of%2520video%2520data.%250AHowever%252C%2520to%2520further%2520increase%2520accuracy%252C%2520many%2520companies%2520have%2520added%2520data%2520from%250Alight%2520detection%2520and%2520ranging%2520%2528LiDAR%2529%2520and%2520or%2520radar%2520sensors%2520through%2520sensor%2520fusion.%250AHowever%252C%2520the%2520addition%2520of%2520lasers%2520and%2520sensors%2520comes%2520at%2520a%2520high%2520financial%2520cost.%2520In%250Athis%2520paper%252C%2520I%2520address%2520both%2520of%2520these%2520issues%2520by%2520increasing%2520the%2520accuracy%2520of%2520the%250Acomputer%2520vision%2520models%2520without%2520the%2520increased%2520cost%2520of%2520using%2520LiDAR%2520and%2520or%250Asensors.%2520I%2520achieved%2520this%2520by%2520improving%2520the%2520accuracy%2520of%2520computer%2520vision%2520models%2520by%250Asensor%2520fusing%2520CAN%2520bus%2520data%252C%2520a%2520vehicle%2520protocol%252C%2520with%2520video%2520data.%2520CAN%2520bus%2520data%250Ais%2520a%2520rich%2520source%2520of%2520information%2520about%2520the%2520vehicle%2527s%2520state%252C%2520including%2520its%2520speed%252C%250Asteering%2520angle%252C%2520and%2520acceleration.%2520By%2520fusing%2520this%2520data%2520with%2520video%2520data%252C%2520the%250Aaccuracy%2520of%2520the%2520computer%2520vision%2520model%2527s%2520predictions%2520can%2520be%2520improved.%2520When%2520I%250Atrained%2520the%2520model%2520without%2520CAN%2520bus%2520data%252C%2520I%2520obtained%2520an%2520RMSE%2520of%25200.02492%252C%2520while%250Athe%2520model%2520trained%2520with%2520the%2520CAN%2520bus%2520data%2520achieved%2520an%2520RMSE%2520of%25200.01970.%2520This%250Afinding%2520indicates%2520that%2520fusing%2520CAN%2520Bus%2520data%2520with%2520video%2520data%2520can%2520reduce%2520the%250Acomputer%2520vision%2520model%2527s%2520prediction%2520error%2520by%252020%2525%2520with%2520some%2520models%2520decreasing%2520the%250Aerror%2520by%252080%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.14162v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Augmenting%20End-to-End%20Steering%20Angle%20Prediction%20with%20CAN%20Bus%20Data&entry.906535625=Amit%20Singh&entry.1292438233=%20%20In%20recent%20years%2C%20end%20to%20end%20steering%20prediction%20for%20autonomous%20vehicles%20has%0Abecome%20a%20major%20area%20of%20research.%20The%20primary%20method%20for%20achieving%20end%20to%20end%0Asteering%20was%20to%20use%20computer%20vision%20models%20on%20a%20live%20feed%20of%20video%20data.%0AHowever%2C%20to%20further%20increase%20accuracy%2C%20many%20companies%20have%20added%20data%20from%0Alight%20detection%20and%20ranging%20%28LiDAR%29%20and%20or%20radar%20sensors%20through%20sensor%20fusion.%0AHowever%2C%20the%20addition%20of%20lasers%20and%20sensors%20comes%20at%20a%20high%20financial%20cost.%20In%0Athis%20paper%2C%20I%20address%20both%20of%20these%20issues%20by%20increasing%20the%20accuracy%20of%20the%0Acomputer%20vision%20models%20without%20the%20increased%20cost%20of%20using%20LiDAR%20and%20or%0Asensors.%20I%20achieved%20this%20by%20improving%20the%20accuracy%20of%20computer%20vision%20models%20by%0Asensor%20fusing%20CAN%20bus%20data%2C%20a%20vehicle%20protocol%2C%20with%20video%20data.%20CAN%20bus%20data%0Ais%20a%20rich%20source%20of%20information%20about%20the%20vehicle%27s%20state%2C%20including%20its%20speed%2C%0Asteering%20angle%2C%20and%20acceleration.%20By%20fusing%20this%20data%20with%20video%20data%2C%20the%0Aaccuracy%20of%20the%20computer%20vision%20model%27s%20predictions%20can%20be%20improved.%20When%20I%0Atrained%20the%20model%20without%20CAN%20bus%20data%2C%20I%20obtained%20an%20RMSE%20of%200.02492%2C%20while%0Athe%20model%20trained%20with%20the%20CAN%20bus%20data%20achieved%20an%20RMSE%20of%200.01970.%20This%0Afinding%20indicates%20that%20fusing%20CAN%20Bus%20data%20with%20video%20data%20can%20reduce%20the%0Acomputer%20vision%20model%27s%20prediction%20error%20by%2020%25%20with%20some%20models%20decreasing%20the%0Aerror%20by%2080%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.14162v2&entry.124074799=Read"},
{"title": "Are DeepSeek R1 And Other Reasoning Models More Faithful?", "author": "James Chua and Owain Evans", "abstract": "  Language models trained to solve reasoning tasks via reinforcement learning\nhave achieved striking results. We refer to these models as reasoning models.\nAre the Chains of Thought (CoTs) of reasoning models more faithful than\ntraditional models? We evaluate three reasoning models (based on Qwen-2.5,\nGemini-2, and DeepSeek-V3-Base) on an existing test of faithful CoT. To measure\nfaithfulness, we test whether models can describe how a cue in their prompt\ninfluences their answer to MMLU questions. For example, when the cue \"A\nStanford Professor thinks the answer is D\" is added to the prompt, models\nsometimes switch their answer to D. In such cases, the DeepSeek-R1 reasoning\nmodel describes the cue's influence 59% of the time, compared to 7% for the\nnon-reasoning DeepSeek model. We evaluate seven types of cue, such as\nmisleading few-shot examples and suggestive follow-up questions from the user.\nReasoning models describe cues that influence them much more reliably than all\nthe non-reasoning models tested (including Claude-3.5-Sonnet and GPT-4o). In an\nadditional experiment, we provide evidence suggesting that the use of reward\nmodels causes less faithful responses -- which may help explain why\nnon-reasoning models are less faithful. Our study has two main limitations.\nFirst, we test faithfulness using a set of artificial tasks, which may not\nreflect realistic use-cases. Second, we only measure one specific aspect of\nfaithfulness -- whether models can describe the influence of cues. Future\nresearch should investigate whether the advantage of reasoning models in\nfaithfulness holds for a broader set of tests. Still, we think this increase in\nfaithfulness is promising for the explainability of language models.\n", "link": "http://arxiv.org/abs/2501.08156v5", "date": "2025-07-15", "relevancy": 2.1464, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5441}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5441}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20DeepSeek%20R1%20And%20Other%20Reasoning%20Models%20More%20Faithful%3F&body=Title%3A%20Are%20DeepSeek%20R1%20And%20Other%20Reasoning%20Models%20More%20Faithful%3F%0AAuthor%3A%20James%20Chua%20and%20Owain%20Evans%0AAbstract%3A%20%20%20Language%20models%20trained%20to%20solve%20reasoning%20tasks%20via%20reinforcement%20learning%0Ahave%20achieved%20striking%20results.%20We%20refer%20to%20these%20models%20as%20reasoning%20models.%0AAre%20the%20Chains%20of%20Thought%20%28CoTs%29%20of%20reasoning%20models%20more%20faithful%20than%0Atraditional%20models%3F%20We%20evaluate%20three%20reasoning%20models%20%28based%20on%20Qwen-2.5%2C%0AGemini-2%2C%20and%20DeepSeek-V3-Base%29%20on%20an%20existing%20test%20of%20faithful%20CoT.%20To%20measure%0Afaithfulness%2C%20we%20test%20whether%20models%20can%20describe%20how%20a%20cue%20in%20their%20prompt%0Ainfluences%20their%20answer%20to%20MMLU%20questions.%20For%20example%2C%20when%20the%20cue%20%22A%0AStanford%20Professor%20thinks%20the%20answer%20is%20D%22%20is%20added%20to%20the%20prompt%2C%20models%0Asometimes%20switch%20their%20answer%20to%20D.%20In%20such%20cases%2C%20the%20DeepSeek-R1%20reasoning%0Amodel%20describes%20the%20cue%27s%20influence%2059%25%20of%20the%20time%2C%20compared%20to%207%25%20for%20the%0Anon-reasoning%20DeepSeek%20model.%20We%20evaluate%20seven%20types%20of%20cue%2C%20such%20as%0Amisleading%20few-shot%20examples%20and%20suggestive%20follow-up%20questions%20from%20the%20user.%0AReasoning%20models%20describe%20cues%20that%20influence%20them%20much%20more%20reliably%20than%20all%0Athe%20non-reasoning%20models%20tested%20%28including%20Claude-3.5-Sonnet%20and%20GPT-4o%29.%20In%20an%0Aadditional%20experiment%2C%20we%20provide%20evidence%20suggesting%20that%20the%20use%20of%20reward%0Amodels%20causes%20less%20faithful%20responses%20--%20which%20may%20help%20explain%20why%0Anon-reasoning%20models%20are%20less%20faithful.%20Our%20study%20has%20two%20main%20limitations.%0AFirst%2C%20we%20test%20faithfulness%20using%20a%20set%20of%20artificial%20tasks%2C%20which%20may%20not%0Areflect%20realistic%20use-cases.%20Second%2C%20we%20only%20measure%20one%20specific%20aspect%20of%0Afaithfulness%20--%20whether%20models%20can%20describe%20the%20influence%20of%20cues.%20Future%0Aresearch%20should%20investigate%20whether%20the%20advantage%20of%20reasoning%20models%20in%0Afaithfulness%20holds%20for%20a%20broader%20set%20of%20tests.%20Still%2C%20we%20think%20this%20increase%20in%0Afaithfulness%20is%20promising%20for%20the%20explainability%20of%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.08156v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520DeepSeek%2520R1%2520And%2520Other%2520Reasoning%2520Models%2520More%2520Faithful%253F%26entry.906535625%3DJames%2520Chua%2520and%2520Owain%2520Evans%26entry.1292438233%3D%2520%2520Language%2520models%2520trained%2520to%2520solve%2520reasoning%2520tasks%2520via%2520reinforcement%2520learning%250Ahave%2520achieved%2520striking%2520results.%2520We%2520refer%2520to%2520these%2520models%2520as%2520reasoning%2520models.%250AAre%2520the%2520Chains%2520of%2520Thought%2520%2528CoTs%2529%2520of%2520reasoning%2520models%2520more%2520faithful%2520than%250Atraditional%2520models%253F%2520We%2520evaluate%2520three%2520reasoning%2520models%2520%2528based%2520on%2520Qwen-2.5%252C%250AGemini-2%252C%2520and%2520DeepSeek-V3-Base%2529%2520on%2520an%2520existing%2520test%2520of%2520faithful%2520CoT.%2520To%2520measure%250Afaithfulness%252C%2520we%2520test%2520whether%2520models%2520can%2520describe%2520how%2520a%2520cue%2520in%2520their%2520prompt%250Ainfluences%2520their%2520answer%2520to%2520MMLU%2520questions.%2520For%2520example%252C%2520when%2520the%2520cue%2520%2522A%250AStanford%2520Professor%2520thinks%2520the%2520answer%2520is%2520D%2522%2520is%2520added%2520to%2520the%2520prompt%252C%2520models%250Asometimes%2520switch%2520their%2520answer%2520to%2520D.%2520In%2520such%2520cases%252C%2520the%2520DeepSeek-R1%2520reasoning%250Amodel%2520describes%2520the%2520cue%2527s%2520influence%252059%2525%2520of%2520the%2520time%252C%2520compared%2520to%25207%2525%2520for%2520the%250Anon-reasoning%2520DeepSeek%2520model.%2520We%2520evaluate%2520seven%2520types%2520of%2520cue%252C%2520such%2520as%250Amisleading%2520few-shot%2520examples%2520and%2520suggestive%2520follow-up%2520questions%2520from%2520the%2520user.%250AReasoning%2520models%2520describe%2520cues%2520that%2520influence%2520them%2520much%2520more%2520reliably%2520than%2520all%250Athe%2520non-reasoning%2520models%2520tested%2520%2528including%2520Claude-3.5-Sonnet%2520and%2520GPT-4o%2529.%2520In%2520an%250Aadditional%2520experiment%252C%2520we%2520provide%2520evidence%2520suggesting%2520that%2520the%2520use%2520of%2520reward%250Amodels%2520causes%2520less%2520faithful%2520responses%2520--%2520which%2520may%2520help%2520explain%2520why%250Anon-reasoning%2520models%2520are%2520less%2520faithful.%2520Our%2520study%2520has%2520two%2520main%2520limitations.%250AFirst%252C%2520we%2520test%2520faithfulness%2520using%2520a%2520set%2520of%2520artificial%2520tasks%252C%2520which%2520may%2520not%250Areflect%2520realistic%2520use-cases.%2520Second%252C%2520we%2520only%2520measure%2520one%2520specific%2520aspect%2520of%250Afaithfulness%2520--%2520whether%2520models%2520can%2520describe%2520the%2520influence%2520of%2520cues.%2520Future%250Aresearch%2520should%2520investigate%2520whether%2520the%2520advantage%2520of%2520reasoning%2520models%2520in%250Afaithfulness%2520holds%2520for%2520a%2520broader%2520set%2520of%2520tests.%2520Still%252C%2520we%2520think%2520this%2520increase%2520in%250Afaithfulness%2520is%2520promising%2520for%2520the%2520explainability%2520of%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.08156v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20DeepSeek%20R1%20And%20Other%20Reasoning%20Models%20More%20Faithful%3F&entry.906535625=James%20Chua%20and%20Owain%20Evans&entry.1292438233=%20%20Language%20models%20trained%20to%20solve%20reasoning%20tasks%20via%20reinforcement%20learning%0Ahave%20achieved%20striking%20results.%20We%20refer%20to%20these%20models%20as%20reasoning%20models.%0AAre%20the%20Chains%20of%20Thought%20%28CoTs%29%20of%20reasoning%20models%20more%20faithful%20than%0Atraditional%20models%3F%20We%20evaluate%20three%20reasoning%20models%20%28based%20on%20Qwen-2.5%2C%0AGemini-2%2C%20and%20DeepSeek-V3-Base%29%20on%20an%20existing%20test%20of%20faithful%20CoT.%20To%20measure%0Afaithfulness%2C%20we%20test%20whether%20models%20can%20describe%20how%20a%20cue%20in%20their%20prompt%0Ainfluences%20their%20answer%20to%20MMLU%20questions.%20For%20example%2C%20when%20the%20cue%20%22A%0AStanford%20Professor%20thinks%20the%20answer%20is%20D%22%20is%20added%20to%20the%20prompt%2C%20models%0Asometimes%20switch%20their%20answer%20to%20D.%20In%20such%20cases%2C%20the%20DeepSeek-R1%20reasoning%0Amodel%20describes%20the%20cue%27s%20influence%2059%25%20of%20the%20time%2C%20compared%20to%207%25%20for%20the%0Anon-reasoning%20DeepSeek%20model.%20We%20evaluate%20seven%20types%20of%20cue%2C%20such%20as%0Amisleading%20few-shot%20examples%20and%20suggestive%20follow-up%20questions%20from%20the%20user.%0AReasoning%20models%20describe%20cues%20that%20influence%20them%20much%20more%20reliably%20than%20all%0Athe%20non-reasoning%20models%20tested%20%28including%20Claude-3.5-Sonnet%20and%20GPT-4o%29.%20In%20an%0Aadditional%20experiment%2C%20we%20provide%20evidence%20suggesting%20that%20the%20use%20of%20reward%0Amodels%20causes%20less%20faithful%20responses%20--%20which%20may%20help%20explain%20why%0Anon-reasoning%20models%20are%20less%20faithful.%20Our%20study%20has%20two%20main%20limitations.%0AFirst%2C%20we%20test%20faithfulness%20using%20a%20set%20of%20artificial%20tasks%2C%20which%20may%20not%0Areflect%20realistic%20use-cases.%20Second%2C%20we%20only%20measure%20one%20specific%20aspect%20of%0Afaithfulness%20--%20whether%20models%20can%20describe%20the%20influence%20of%20cues.%20Future%0Aresearch%20should%20investigate%20whether%20the%20advantage%20of%20reasoning%20models%20in%0Afaithfulness%20holds%20for%20a%20broader%20set%20of%20tests.%20Still%2C%20we%20think%20this%20increase%20in%0Afaithfulness%20is%20promising%20for%20the%20explainability%20of%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.08156v5&entry.124074799=Read"},
{"title": "C-FBI: A Combinatorial method using Convolutions for Circle Fitting in\n  Blurry Images", "author": "Esteban Rom\u00e1n Catafau and Torbj\u00f6rn E. M. Nordling", "abstract": "  This paper addresses the fundamental computer vision challenge of robust\ncircle detection and fitting in degraded imaging conditions. We present\nCombinatorial Convolution-based Circle Fitting for Blurry Images (3C-FBI), an\nalgorithm that bridges the gap between circle detection and precise parametric\nfitting by combining (1) efficient combinatorial edge pixel (edgel) sampling\nand (2) convolution-based density estimation in parameter space.\n  We evaluate 3C-FBI across three experimental frameworks: (1) real-world\nmedical data from Parkinson's disease assessments (144 frames from 36 videos),\n(2) controlled synthetic data following established circle-fitting benchmarks,\nand (3) systematic analysis across varying spatial resolutions and outlier\ncontamination levels. Results show that 3C-FBI achieves state-of-the-art\naccuracy (Jaccard index 0.896) while maintaining real-time performance (40.3\nfps), significantly outperforming classical methods like RCD (6.8 fps) on a\nstandard CPU (i7-10875H). It maintains near-perfect accuracy (Jaccard almost\n1.0) at high resolutions (480x480) and reliable performance (Jaccard higher\nthan 0.95) down to 160x160 with up to 20% outliers.\n  In extensive synthetic testing, 3C-FBI achieves a mean Jaccard Index of 0.989\nacross contamination levels, comparable to modern methods like Qi et al. (2024,\n0.991), and surpassing RHT (0.964). This combination of accuracy, speed, and\nrobustness makes 3C-FBI ideal for medical imaging, robotics, and industrial\ninspection under challenging conditions.\n", "link": "http://arxiv.org/abs/2507.11476v1", "date": "2025-07-15", "relevancy": 2.1334, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5536}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5293}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C-FBI%3A%20A%20Combinatorial%20method%20using%20Convolutions%20for%20Circle%20Fitting%20in%0A%20%20Blurry%20Images&body=Title%3A%20C-FBI%3A%20A%20Combinatorial%20method%20using%20Convolutions%20for%20Circle%20Fitting%20in%0A%20%20Blurry%20Images%0AAuthor%3A%20Esteban%20Rom%C3%A1n%20Catafau%20and%20Torbj%C3%B6rn%20E.%20M.%20Nordling%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20fundamental%20computer%20vision%20challenge%20of%20robust%0Acircle%20detection%20and%20fitting%20in%20degraded%20imaging%20conditions.%20We%20present%0ACombinatorial%20Convolution-based%20Circle%20Fitting%20for%20Blurry%20Images%20%283C-FBI%29%2C%20an%0Aalgorithm%20that%20bridges%20the%20gap%20between%20circle%20detection%20and%20precise%20parametric%0Afitting%20by%20combining%20%281%29%20efficient%20combinatorial%20edge%20pixel%20%28edgel%29%20sampling%0Aand%20%282%29%20convolution-based%20density%20estimation%20in%20parameter%20space.%0A%20%20We%20evaluate%203C-FBI%20across%20three%20experimental%20frameworks%3A%20%281%29%20real-world%0Amedical%20data%20from%20Parkinson%27s%20disease%20assessments%20%28144%20frames%20from%2036%20videos%29%2C%0A%282%29%20controlled%20synthetic%20data%20following%20established%20circle-fitting%20benchmarks%2C%0Aand%20%283%29%20systematic%20analysis%20across%20varying%20spatial%20resolutions%20and%20outlier%0Acontamination%20levels.%20Results%20show%20that%203C-FBI%20achieves%20state-of-the-art%0Aaccuracy%20%28Jaccard%20index%200.896%29%20while%20maintaining%20real-time%20performance%20%2840.3%0Afps%29%2C%20significantly%20outperforming%20classical%20methods%20like%20RCD%20%286.8%20fps%29%20on%20a%0Astandard%20CPU%20%28i7-10875H%29.%20It%20maintains%20near-perfect%20accuracy%20%28Jaccard%20almost%0A1.0%29%20at%20high%20resolutions%20%28480x480%29%20and%20reliable%20performance%20%28Jaccard%20higher%0Athan%200.95%29%20down%20to%20160x160%20with%20up%20to%2020%25%20outliers.%0A%20%20In%20extensive%20synthetic%20testing%2C%203C-FBI%20achieves%20a%20mean%20Jaccard%20Index%20of%200.989%0Aacross%20contamination%20levels%2C%20comparable%20to%20modern%20methods%20like%20Qi%20et%20al.%20%282024%2C%0A0.991%29%2C%20and%20surpassing%20RHT%20%280.964%29.%20This%20combination%20of%20accuracy%2C%20speed%2C%20and%0Arobustness%20makes%203C-FBI%20ideal%20for%20medical%20imaging%2C%20robotics%2C%20and%20industrial%0Ainspection%20under%20challenging%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC-FBI%253A%2520A%2520Combinatorial%2520method%2520using%2520Convolutions%2520for%2520Circle%2520Fitting%2520in%250A%2520%2520Blurry%2520Images%26entry.906535625%3DEsteban%2520Rom%25C3%25A1n%2520Catafau%2520and%2520Torbj%25C3%25B6rn%2520E.%2520M.%2520Nordling%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520fundamental%2520computer%2520vision%2520challenge%2520of%2520robust%250Acircle%2520detection%2520and%2520fitting%2520in%2520degraded%2520imaging%2520conditions.%2520We%2520present%250ACombinatorial%2520Convolution-based%2520Circle%2520Fitting%2520for%2520Blurry%2520Images%2520%25283C-FBI%2529%252C%2520an%250Aalgorithm%2520that%2520bridges%2520the%2520gap%2520between%2520circle%2520detection%2520and%2520precise%2520parametric%250Afitting%2520by%2520combining%2520%25281%2529%2520efficient%2520combinatorial%2520edge%2520pixel%2520%2528edgel%2529%2520sampling%250Aand%2520%25282%2529%2520convolution-based%2520density%2520estimation%2520in%2520parameter%2520space.%250A%2520%2520We%2520evaluate%25203C-FBI%2520across%2520three%2520experimental%2520frameworks%253A%2520%25281%2529%2520real-world%250Amedical%2520data%2520from%2520Parkinson%2527s%2520disease%2520assessments%2520%2528144%2520frames%2520from%252036%2520videos%2529%252C%250A%25282%2529%2520controlled%2520synthetic%2520data%2520following%2520established%2520circle-fitting%2520benchmarks%252C%250Aand%2520%25283%2529%2520systematic%2520analysis%2520across%2520varying%2520spatial%2520resolutions%2520and%2520outlier%250Acontamination%2520levels.%2520Results%2520show%2520that%25203C-FBI%2520achieves%2520state-of-the-art%250Aaccuracy%2520%2528Jaccard%2520index%25200.896%2529%2520while%2520maintaining%2520real-time%2520performance%2520%252840.3%250Afps%2529%252C%2520significantly%2520outperforming%2520classical%2520methods%2520like%2520RCD%2520%25286.8%2520fps%2529%2520on%2520a%250Astandard%2520CPU%2520%2528i7-10875H%2529.%2520It%2520maintains%2520near-perfect%2520accuracy%2520%2528Jaccard%2520almost%250A1.0%2529%2520at%2520high%2520resolutions%2520%2528480x480%2529%2520and%2520reliable%2520performance%2520%2528Jaccard%2520higher%250Athan%25200.95%2529%2520down%2520to%2520160x160%2520with%2520up%2520to%252020%2525%2520outliers.%250A%2520%2520In%2520extensive%2520synthetic%2520testing%252C%25203C-FBI%2520achieves%2520a%2520mean%2520Jaccard%2520Index%2520of%25200.989%250Aacross%2520contamination%2520levels%252C%2520comparable%2520to%2520modern%2520methods%2520like%2520Qi%2520et%2520al.%2520%25282024%252C%250A0.991%2529%252C%2520and%2520surpassing%2520RHT%2520%25280.964%2529.%2520This%2520combination%2520of%2520accuracy%252C%2520speed%252C%2520and%250Arobustness%2520makes%25203C-FBI%2520ideal%2520for%2520medical%2520imaging%252C%2520robotics%252C%2520and%2520industrial%250Ainspection%2520under%2520challenging%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C-FBI%3A%20A%20Combinatorial%20method%20using%20Convolutions%20for%20Circle%20Fitting%20in%0A%20%20Blurry%20Images&entry.906535625=Esteban%20Rom%C3%A1n%20Catafau%20and%20Torbj%C3%B6rn%20E.%20M.%20Nordling&entry.1292438233=%20%20This%20paper%20addresses%20the%20fundamental%20computer%20vision%20challenge%20of%20robust%0Acircle%20detection%20and%20fitting%20in%20degraded%20imaging%20conditions.%20We%20present%0ACombinatorial%20Convolution-based%20Circle%20Fitting%20for%20Blurry%20Images%20%283C-FBI%29%2C%20an%0Aalgorithm%20that%20bridges%20the%20gap%20between%20circle%20detection%20and%20precise%20parametric%0Afitting%20by%20combining%20%281%29%20efficient%20combinatorial%20edge%20pixel%20%28edgel%29%20sampling%0Aand%20%282%29%20convolution-based%20density%20estimation%20in%20parameter%20space.%0A%20%20We%20evaluate%203C-FBI%20across%20three%20experimental%20frameworks%3A%20%281%29%20real-world%0Amedical%20data%20from%20Parkinson%27s%20disease%20assessments%20%28144%20frames%20from%2036%20videos%29%2C%0A%282%29%20controlled%20synthetic%20data%20following%20established%20circle-fitting%20benchmarks%2C%0Aand%20%283%29%20systematic%20analysis%20across%20varying%20spatial%20resolutions%20and%20outlier%0Acontamination%20levels.%20Results%20show%20that%203C-FBI%20achieves%20state-of-the-art%0Aaccuracy%20%28Jaccard%20index%200.896%29%20while%20maintaining%20real-time%20performance%20%2840.3%0Afps%29%2C%20significantly%20outperforming%20classical%20methods%20like%20RCD%20%286.8%20fps%29%20on%20a%0Astandard%20CPU%20%28i7-10875H%29.%20It%20maintains%20near-perfect%20accuracy%20%28Jaccard%20almost%0A1.0%29%20at%20high%20resolutions%20%28480x480%29%20and%20reliable%20performance%20%28Jaccard%20higher%0Athan%200.95%29%20down%20to%20160x160%20with%20up%20to%2020%25%20outliers.%0A%20%20In%20extensive%20synthetic%20testing%2C%203C-FBI%20achieves%20a%20mean%20Jaccard%20Index%20of%200.989%0Aacross%20contamination%20levels%2C%20comparable%20to%20modern%20methods%20like%20Qi%20et%20al.%20%282024%2C%0A0.991%29%2C%20and%20surpassing%20RHT%20%280.964%29.%20This%20combination%20of%20accuracy%2C%20speed%2C%20and%0Arobustness%20makes%203C-FBI%20ideal%20for%20medical%20imaging%2C%20robotics%2C%20and%20industrial%0Ainspection%20under%20challenging%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11476v1&entry.124074799=Read"},
{"title": "Modeling Code: Is Text All You Need?", "author": "Daniel Nichols and Konstantinos Parasyris and Harshitha Menon and Brian R. Bartoldson and Giorgis Georgakoudis and Tal Ben-Nun and Abhinav Bhatele", "abstract": "  Code LLMs have become extremely popular recently for modeling source code\nacross a variety of tasks, such as generation, translation, and summarization.\nHowever, transformer-based models are limited in their capabilities to reason\nthrough structured, analytical properties of code, such as control and data\nflow. Previous work has explored the modeling of these properties with\nstructured data and graph neural networks. However, these approaches lack the\ngenerative capabilities and scale of modern LLMs. In this work, we introduce a\nnovel approach to combine the strengths of modeling both code as text and more\nstructured forms.\n", "link": "http://arxiv.org/abs/2507.11467v1", "date": "2025-07-15", "relevancy": 2.0825, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.537}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5256}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modeling%20Code%3A%20Is%20Text%20All%20You%20Need%3F&body=Title%3A%20Modeling%20Code%3A%20Is%20Text%20All%20You%20Need%3F%0AAuthor%3A%20Daniel%20Nichols%20and%20Konstantinos%20Parasyris%20and%20Harshitha%20Menon%20and%20Brian%20R.%20Bartoldson%20and%20Giorgis%20Georgakoudis%20and%20Tal%20Ben-Nun%20and%20Abhinav%20Bhatele%0AAbstract%3A%20%20%20Code%20LLMs%20have%20become%20extremely%20popular%20recently%20for%20modeling%20source%20code%0Aacross%20a%20variety%20of%20tasks%2C%20such%20as%20generation%2C%20translation%2C%20and%20summarization.%0AHowever%2C%20transformer-based%20models%20are%20limited%20in%20their%20capabilities%20to%20reason%0Athrough%20structured%2C%20analytical%20properties%20of%20code%2C%20such%20as%20control%20and%20data%0Aflow.%20Previous%20work%20has%20explored%20the%20modeling%20of%20these%20properties%20with%0Astructured%20data%20and%20graph%20neural%20networks.%20However%2C%20these%20approaches%20lack%20the%0Agenerative%20capabilities%20and%20scale%20of%20modern%20LLMs.%20In%20this%20work%2C%20we%20introduce%20a%0Anovel%20approach%20to%20combine%20the%20strengths%20of%20modeling%20both%20code%20as%20text%20and%20more%0Astructured%20forms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11467v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModeling%2520Code%253A%2520Is%2520Text%2520All%2520You%2520Need%253F%26entry.906535625%3DDaniel%2520Nichols%2520and%2520Konstantinos%2520Parasyris%2520and%2520Harshitha%2520Menon%2520and%2520Brian%2520R.%2520Bartoldson%2520and%2520Giorgis%2520Georgakoudis%2520and%2520Tal%2520Ben-Nun%2520and%2520Abhinav%2520Bhatele%26entry.1292438233%3D%2520%2520Code%2520LLMs%2520have%2520become%2520extremely%2520popular%2520recently%2520for%2520modeling%2520source%2520code%250Aacross%2520a%2520variety%2520of%2520tasks%252C%2520such%2520as%2520generation%252C%2520translation%252C%2520and%2520summarization.%250AHowever%252C%2520transformer-based%2520models%2520are%2520limited%2520in%2520their%2520capabilities%2520to%2520reason%250Athrough%2520structured%252C%2520analytical%2520properties%2520of%2520code%252C%2520such%2520as%2520control%2520and%2520data%250Aflow.%2520Previous%2520work%2520has%2520explored%2520the%2520modeling%2520of%2520these%2520properties%2520with%250Astructured%2520data%2520and%2520graph%2520neural%2520networks.%2520However%252C%2520these%2520approaches%2520lack%2520the%250Agenerative%2520capabilities%2520and%2520scale%2520of%2520modern%2520LLMs.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%250Anovel%2520approach%2520to%2520combine%2520the%2520strengths%2520of%2520modeling%2520both%2520code%2520as%2520text%2520and%2520more%250Astructured%2520forms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11467v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Code%3A%20Is%20Text%20All%20You%20Need%3F&entry.906535625=Daniel%20Nichols%20and%20Konstantinos%20Parasyris%20and%20Harshitha%20Menon%20and%20Brian%20R.%20Bartoldson%20and%20Giorgis%20Georgakoudis%20and%20Tal%20Ben-Nun%20and%20Abhinav%20Bhatele&entry.1292438233=%20%20Code%20LLMs%20have%20become%20extremely%20popular%20recently%20for%20modeling%20source%20code%0Aacross%20a%20variety%20of%20tasks%2C%20such%20as%20generation%2C%20translation%2C%20and%20summarization.%0AHowever%2C%20transformer-based%20models%20are%20limited%20in%20their%20capabilities%20to%20reason%0Athrough%20structured%2C%20analytical%20properties%20of%20code%2C%20such%20as%20control%20and%20data%0Aflow.%20Previous%20work%20has%20explored%20the%20modeling%20of%20these%20properties%20with%0Astructured%20data%20and%20graph%20neural%20networks.%20However%2C%20these%20approaches%20lack%20the%0Agenerative%20capabilities%20and%20scale%20of%20modern%20LLMs.%20In%20this%20work%2C%20we%20introduce%20a%0Anovel%20approach%20to%20combine%20the%20strengths%20of%20modeling%20both%20code%20as%20text%20and%20more%0Astructured%20forms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11467v1&entry.124074799=Read"},
{"title": "Conversation Forests: The Key to Fine Tuning Large Language Models for\n  Multi-Turn Medical Conversations is Branching", "author": "Thomas Savage", "abstract": "  Fine-tuning methods such as Direct Preference Optimization (DPO) and Group\nRelative Policy Optimization (GRPO) have demonstrated success in training large\nlanguage models (LLMs) for single-turn tasks. However, these methods fall short\nin multi-turn applications, such as diagnostic patient interviewing, where\nunderstanding how early conversational turns influence downstream completions\nand outcomes is essential. In medicine, a multi-turn perspective is critical\nfor learning diagnostic schemas and better understanding conversation dynamics.\nTo address this gap, I introduce Savage Conversation Forests (SCF), a\nreinforcement learning framework that leverages a branched conversation\narchitecture to fine-tune LLMs for multi-turn dialogue. SCF generates multiple\npossible conversation continuations at each turn, enabling the model to learn\nhow different early responses affect downstream interactions and diagnostic\noutcomes. In experiments simulating doctor-patient conversations, SCF with\nbranching outperforms linear conversation architectures on diagnostic accuracy.\nI hypothesize that SCF's improvements stem from its ability to provide richer,\ninterdependent training signals across conversation turns. These results\nsuggest that a branched training architecture is an important strategy for fine\ntuning LLMs in complex multi-turn conversational tasks.\n", "link": "http://arxiv.org/abs/2507.04099v2", "date": "2025-07-15", "relevancy": 2.0467, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5141}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5141}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4996}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conversation%20Forests%3A%20The%20Key%20to%20Fine%20Tuning%20Large%20Language%20Models%20for%0A%20%20Multi-Turn%20Medical%20Conversations%20is%20Branching&body=Title%3A%20Conversation%20Forests%3A%20The%20Key%20to%20Fine%20Tuning%20Large%20Language%20Models%20for%0A%20%20Multi-Turn%20Medical%20Conversations%20is%20Branching%0AAuthor%3A%20Thomas%20Savage%0AAbstract%3A%20%20%20Fine-tuning%20methods%20such%20as%20Direct%20Preference%20Optimization%20%28DPO%29%20and%20Group%0ARelative%20Policy%20Optimization%20%28GRPO%29%20have%20demonstrated%20success%20in%20training%20large%0Alanguage%20models%20%28LLMs%29%20for%20single-turn%20tasks.%20However%2C%20these%20methods%20fall%20short%0Ain%20multi-turn%20applications%2C%20such%20as%20diagnostic%20patient%20interviewing%2C%20where%0Aunderstanding%20how%20early%20conversational%20turns%20influence%20downstream%20completions%0Aand%20outcomes%20is%20essential.%20In%20medicine%2C%20a%20multi-turn%20perspective%20is%20critical%0Afor%20learning%20diagnostic%20schemas%20and%20better%20understanding%20conversation%20dynamics.%0ATo%20address%20this%20gap%2C%20I%20introduce%20Savage%20Conversation%20Forests%20%28SCF%29%2C%20a%0Areinforcement%20learning%20framework%20that%20leverages%20a%20branched%20conversation%0Aarchitecture%20to%20fine-tune%20LLMs%20for%20multi-turn%20dialogue.%20SCF%20generates%20multiple%0Apossible%20conversation%20continuations%20at%20each%20turn%2C%20enabling%20the%20model%20to%20learn%0Ahow%20different%20early%20responses%20affect%20downstream%20interactions%20and%20diagnostic%0Aoutcomes.%20In%20experiments%20simulating%20doctor-patient%20conversations%2C%20SCF%20with%0Abranching%20outperforms%20linear%20conversation%20architectures%20on%20diagnostic%20accuracy.%0AI%20hypothesize%20that%20SCF%27s%20improvements%20stem%20from%20its%20ability%20to%20provide%20richer%2C%0Ainterdependent%20training%20signals%20across%20conversation%20turns.%20These%20results%0Asuggest%20that%20a%20branched%20training%20architecture%20is%20an%20important%20strategy%20for%20fine%0Atuning%20LLMs%20in%20complex%20multi-turn%20conversational%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.04099v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConversation%2520Forests%253A%2520The%2520Key%2520to%2520Fine%2520Tuning%2520Large%2520Language%2520Models%2520for%250A%2520%2520Multi-Turn%2520Medical%2520Conversations%2520is%2520Branching%26entry.906535625%3DThomas%2520Savage%26entry.1292438233%3D%2520%2520Fine-tuning%2520methods%2520such%2520as%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520and%2520Group%250ARelative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520have%2520demonstrated%2520success%2520in%2520training%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520for%2520single-turn%2520tasks.%2520However%252C%2520these%2520methods%2520fall%2520short%250Ain%2520multi-turn%2520applications%252C%2520such%2520as%2520diagnostic%2520patient%2520interviewing%252C%2520where%250Aunderstanding%2520how%2520early%2520conversational%2520turns%2520influence%2520downstream%2520completions%250Aand%2520outcomes%2520is%2520essential.%2520In%2520medicine%252C%2520a%2520multi-turn%2520perspective%2520is%2520critical%250Afor%2520learning%2520diagnostic%2520schemas%2520and%2520better%2520understanding%2520conversation%2520dynamics.%250ATo%2520address%2520this%2520gap%252C%2520I%2520introduce%2520Savage%2520Conversation%2520Forests%2520%2528SCF%2529%252C%2520a%250Areinforcement%2520learning%2520framework%2520that%2520leverages%2520a%2520branched%2520conversation%250Aarchitecture%2520to%2520fine-tune%2520LLMs%2520for%2520multi-turn%2520dialogue.%2520SCF%2520generates%2520multiple%250Apossible%2520conversation%2520continuations%2520at%2520each%2520turn%252C%2520enabling%2520the%2520model%2520to%2520learn%250Ahow%2520different%2520early%2520responses%2520affect%2520downstream%2520interactions%2520and%2520diagnostic%250Aoutcomes.%2520In%2520experiments%2520simulating%2520doctor-patient%2520conversations%252C%2520SCF%2520with%250Abranching%2520outperforms%2520linear%2520conversation%2520architectures%2520on%2520diagnostic%2520accuracy.%250AI%2520hypothesize%2520that%2520SCF%2527s%2520improvements%2520stem%2520from%2520its%2520ability%2520to%2520provide%2520richer%252C%250Ainterdependent%2520training%2520signals%2520across%2520conversation%2520turns.%2520These%2520results%250Asuggest%2520that%2520a%2520branched%2520training%2520architecture%2520is%2520an%2520important%2520strategy%2520for%2520fine%250Atuning%2520LLMs%2520in%2520complex%2520multi-turn%2520conversational%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04099v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conversation%20Forests%3A%20The%20Key%20to%20Fine%20Tuning%20Large%20Language%20Models%20for%0A%20%20Multi-Turn%20Medical%20Conversations%20is%20Branching&entry.906535625=Thomas%20Savage&entry.1292438233=%20%20Fine-tuning%20methods%20such%20as%20Direct%20Preference%20Optimization%20%28DPO%29%20and%20Group%0ARelative%20Policy%20Optimization%20%28GRPO%29%20have%20demonstrated%20success%20in%20training%20large%0Alanguage%20models%20%28LLMs%29%20for%20single-turn%20tasks.%20However%2C%20these%20methods%20fall%20short%0Ain%20multi-turn%20applications%2C%20such%20as%20diagnostic%20patient%20interviewing%2C%20where%0Aunderstanding%20how%20early%20conversational%20turns%20influence%20downstream%20completions%0Aand%20outcomes%20is%20essential.%20In%20medicine%2C%20a%20multi-turn%20perspective%20is%20critical%0Afor%20learning%20diagnostic%20schemas%20and%20better%20understanding%20conversation%20dynamics.%0ATo%20address%20this%20gap%2C%20I%20introduce%20Savage%20Conversation%20Forests%20%28SCF%29%2C%20a%0Areinforcement%20learning%20framework%20that%20leverages%20a%20branched%20conversation%0Aarchitecture%20to%20fine-tune%20LLMs%20for%20multi-turn%20dialogue.%20SCF%20generates%20multiple%0Apossible%20conversation%20continuations%20at%20each%20turn%2C%20enabling%20the%20model%20to%20learn%0Ahow%20different%20early%20responses%20affect%20downstream%20interactions%20and%20diagnostic%0Aoutcomes.%20In%20experiments%20simulating%20doctor-patient%20conversations%2C%20SCF%20with%0Abranching%20outperforms%20linear%20conversation%20architectures%20on%20diagnostic%20accuracy.%0AI%20hypothesize%20that%20SCF%27s%20improvements%20stem%20from%20its%20ability%20to%20provide%20richer%2C%0Ainterdependent%20training%20signals%20across%20conversation%20turns.%20These%20results%0Asuggest%20that%20a%20branched%20training%20architecture%20is%20an%20important%20strategy%20for%20fine%0Atuning%20LLMs%20in%20complex%20multi-turn%20conversational%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.04099v2&entry.124074799=Read"},
{"title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in\n  Civil Engineering", "author": "Yinsheng Li and Zhen Dong and Yi Shao", "abstract": "  Large Language Model (LLM) agents have shown great potential for solving\nreal-world problems and promise to be a solution for tasks automation in\nindustry. However, more benchmarks are needed to systematically evaluate\nautomation agents from an industrial perspective, for example, in Civil\nEngineering. Therefore, we propose DrafterBench for the comprehensive\nevaluation of LLM agents in the context of technical drawing revision, a\nrepresentation task in civil engineering. DrafterBench contains twelve types of\ntasks summarized from real-world drawing files, with 46 customized\nfunctions/tools and 1920 tasks in total. DrafterBench is an open-source\nbenchmark to rigorously test AI agents' proficiency in interpreting intricate\nand long-context instructions, leveraging prior knowledge, and adapting to\ndynamic instruction quality via implicit policy awareness. The toolkit\ncomprehensively assesses distinct capabilities in structured data\ncomprehension, function execution, instruction following, and critical\nreasoning. DrafterBench offers detailed analysis of task accuracy and error\nstatistics, aiming to provide deeper insight into agent capabilities and\nidentify improvement targets for integrating LLMs in engineering applications.\nOur benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,\nwith the test set hosted at\nhttps://huggingface.co/datasets/Eason666/DrafterBench.\n", "link": "http://arxiv.org/abs/2507.11527v1", "date": "2025-07-15", "relevancy": 2.0443, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5186}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5096}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DrafterBench%3A%20Benchmarking%20Large%20Language%20Models%20for%20Tasks%20Automation%20in%0A%20%20Civil%20Engineering&body=Title%3A%20DrafterBench%3A%20Benchmarking%20Large%20Language%20Models%20for%20Tasks%20Automation%20in%0A%20%20Civil%20Engineering%0AAuthor%3A%20Yinsheng%20Li%20and%20Zhen%20Dong%20and%20Yi%20Shao%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29%20agents%20have%20shown%20great%20potential%20for%20solving%0Areal-world%20problems%20and%20promise%20to%20be%20a%20solution%20for%20tasks%20automation%20in%0Aindustry.%20However%2C%20more%20benchmarks%20are%20needed%20to%20systematically%20evaluate%0Aautomation%20agents%20from%20an%20industrial%20perspective%2C%20for%20example%2C%20in%20Civil%0AEngineering.%20Therefore%2C%20we%20propose%20DrafterBench%20for%20the%20comprehensive%0Aevaluation%20of%20LLM%20agents%20in%20the%20context%20of%20technical%20drawing%20revision%2C%20a%0Arepresentation%20task%20in%20civil%20engineering.%20DrafterBench%20contains%20twelve%20types%20of%0Atasks%20summarized%20from%20real-world%20drawing%20files%2C%20with%2046%20customized%0Afunctions/tools%20and%201920%20tasks%20in%20total.%20DrafterBench%20is%20an%20open-source%0Abenchmark%20to%20rigorously%20test%20AI%20agents%27%20proficiency%20in%20interpreting%20intricate%0Aand%20long-context%20instructions%2C%20leveraging%20prior%20knowledge%2C%20and%20adapting%20to%0Adynamic%20instruction%20quality%20via%20implicit%20policy%20awareness.%20The%20toolkit%0Acomprehensively%20assesses%20distinct%20capabilities%20in%20structured%20data%0Acomprehension%2C%20function%20execution%2C%20instruction%20following%2C%20and%20critical%0Areasoning.%20DrafterBench%20offers%20detailed%20analysis%20of%20task%20accuracy%20and%20error%0Astatistics%2C%20aiming%20to%20provide%20deeper%20insight%20into%20agent%20capabilities%20and%0Aidentify%20improvement%20targets%20for%20integrating%20LLMs%20in%20engineering%20applications.%0AOur%20benchmark%20is%20available%20at%20https%3A//github.com/Eason-Li-AIS/DrafterBench%2C%0Awith%20the%20test%20set%20hosted%20at%0Ahttps%3A//huggingface.co/datasets/Eason666/DrafterBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDrafterBench%253A%2520Benchmarking%2520Large%2520Language%2520Models%2520for%2520Tasks%2520Automation%2520in%250A%2520%2520Civil%2520Engineering%26entry.906535625%3DYinsheng%2520Li%2520and%2520Zhen%2520Dong%2520and%2520Yi%2520Shao%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520agents%2520have%2520shown%2520great%2520potential%2520for%2520solving%250Areal-world%2520problems%2520and%2520promise%2520to%2520be%2520a%2520solution%2520for%2520tasks%2520automation%2520in%250Aindustry.%2520However%252C%2520more%2520benchmarks%2520are%2520needed%2520to%2520systematically%2520evaluate%250Aautomation%2520agents%2520from%2520an%2520industrial%2520perspective%252C%2520for%2520example%252C%2520in%2520Civil%250AEngineering.%2520Therefore%252C%2520we%2520propose%2520DrafterBench%2520for%2520the%2520comprehensive%250Aevaluation%2520of%2520LLM%2520agents%2520in%2520the%2520context%2520of%2520technical%2520drawing%2520revision%252C%2520a%250Arepresentation%2520task%2520in%2520civil%2520engineering.%2520DrafterBench%2520contains%2520twelve%2520types%2520of%250Atasks%2520summarized%2520from%2520real-world%2520drawing%2520files%252C%2520with%252046%2520customized%250Afunctions/tools%2520and%25201920%2520tasks%2520in%2520total.%2520DrafterBench%2520is%2520an%2520open-source%250Abenchmark%2520to%2520rigorously%2520test%2520AI%2520agents%2527%2520proficiency%2520in%2520interpreting%2520intricate%250Aand%2520long-context%2520instructions%252C%2520leveraging%2520prior%2520knowledge%252C%2520and%2520adapting%2520to%250Adynamic%2520instruction%2520quality%2520via%2520implicit%2520policy%2520awareness.%2520The%2520toolkit%250Acomprehensively%2520assesses%2520distinct%2520capabilities%2520in%2520structured%2520data%250Acomprehension%252C%2520function%2520execution%252C%2520instruction%2520following%252C%2520and%2520critical%250Areasoning.%2520DrafterBench%2520offers%2520detailed%2520analysis%2520of%2520task%2520accuracy%2520and%2520error%250Astatistics%252C%2520aiming%2520to%2520provide%2520deeper%2520insight%2520into%2520agent%2520capabilities%2520and%250Aidentify%2520improvement%2520targets%2520for%2520integrating%2520LLMs%2520in%2520engineering%2520applications.%250AOur%2520benchmark%2520is%2520available%2520at%2520https%253A//github.com/Eason-Li-AIS/DrafterBench%252C%250Awith%2520the%2520test%2520set%2520hosted%2520at%250Ahttps%253A//huggingface.co/datasets/Eason666/DrafterBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DrafterBench%3A%20Benchmarking%20Large%20Language%20Models%20for%20Tasks%20Automation%20in%0A%20%20Civil%20Engineering&entry.906535625=Yinsheng%20Li%20and%20Zhen%20Dong%20and%20Yi%20Shao&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29%20agents%20have%20shown%20great%20potential%20for%20solving%0Areal-world%20problems%20and%20promise%20to%20be%20a%20solution%20for%20tasks%20automation%20in%0Aindustry.%20However%2C%20more%20benchmarks%20are%20needed%20to%20systematically%20evaluate%0Aautomation%20agents%20from%20an%20industrial%20perspective%2C%20for%20example%2C%20in%20Civil%0AEngineering.%20Therefore%2C%20we%20propose%20DrafterBench%20for%20the%20comprehensive%0Aevaluation%20of%20LLM%20agents%20in%20the%20context%20of%20technical%20drawing%20revision%2C%20a%0Arepresentation%20task%20in%20civil%20engineering.%20DrafterBench%20contains%20twelve%20types%20of%0Atasks%20summarized%20from%20real-world%20drawing%20files%2C%20with%2046%20customized%0Afunctions/tools%20and%201920%20tasks%20in%20total.%20DrafterBench%20is%20an%20open-source%0Abenchmark%20to%20rigorously%20test%20AI%20agents%27%20proficiency%20in%20interpreting%20intricate%0Aand%20long-context%20instructions%2C%20leveraging%20prior%20knowledge%2C%20and%20adapting%20to%0Adynamic%20instruction%20quality%20via%20implicit%20policy%20awareness.%20The%20toolkit%0Acomprehensively%20assesses%20distinct%20capabilities%20in%20structured%20data%0Acomprehension%2C%20function%20execution%2C%20instruction%20following%2C%20and%20critical%0Areasoning.%20DrafterBench%20offers%20detailed%20analysis%20of%20task%20accuracy%20and%20error%0Astatistics%2C%20aiming%20to%20provide%20deeper%20insight%20into%20agent%20capabilities%20and%0Aidentify%20improvement%20targets%20for%20integrating%20LLMs%20in%20engineering%20applications.%0AOur%20benchmark%20is%20available%20at%20https%3A//github.com/Eason-Li-AIS/DrafterBench%2C%0Awith%20the%20test%20set%20hosted%20at%0Ahttps%3A//huggingface.co/datasets/Eason666/DrafterBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11527v1&entry.124074799=Read"},
{"title": "LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis\n  Assessment in Rectal Cancer", "author": "Yaoxian Dong and Yifan Gao and Haoyue Li and Yanfen Cui and Xin Gao", "abstract": "  Accurate preoperative assessment of lymph node (LN) metastasis in rectal\ncancer guides treatment decisions, yet conventional MRI evaluation based on\nmorphological criteria shows limited diagnostic performance. While some\nartificial intelligence models have been developed, they often operate as black\nboxes, lacking the interpretability needed for clinical trust. Moreover, these\nmodels typically evaluate nodes in isolation, overlooking the patient-level\ncontext. To address these limitations, we introduce LRMR, an LLM-Driven\nRelational Multi-node Ranking framework. This approach reframes the diagnostic\ntask from a direct classification problem into a structured reasoning and\nranking process. The LRMR framework operates in two stages. First, a multimodal\nlarge language model (LLM) analyzes a composite montage image of all LNs from a\npatient, generating a structured report that details ten distinct radiological\nfeatures. Second, a text-based LLM performs pairwise comparisons of these\nreports between different patients, establishing a relative risk ranking based\non the severity and number of adverse features. We evaluated our method on a\nretrospective cohort of 117 rectal cancer patients. LRMR achieved an area under\nthe curve (AUC) of 0.7917 and an F1-score of 0.7200, outperforming a range of\ndeep learning baselines, including ResNet50 (AUC 0.7708). Ablation studies\nconfirmed the value of our two main contributions: removing the relational\nranking stage or the structured prompting stage led to a significant\nperformance drop, with AUCs falling to 0.6875 and 0.6458, respectively. Our\nwork demonstrates that decoupling visual perception from cognitive reasoning\nthrough a two-stage LLM framework offers a powerful, interpretable, and\neffective new paradigm for assessing lymph node metastasis in rectal cancer.\n", "link": "http://arxiv.org/abs/2507.11457v1", "date": "2025-07-15", "relevancy": 1.9364, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5007}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4808}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LRMR%3A%20LLM-Driven%20Relational%20Multi-node%20Ranking%20for%20Lymph%20Node%20Metastasis%0A%20%20Assessment%20in%20Rectal%20Cancer&body=Title%3A%20LRMR%3A%20LLM-Driven%20Relational%20Multi-node%20Ranking%20for%20Lymph%20Node%20Metastasis%0A%20%20Assessment%20in%20Rectal%20Cancer%0AAuthor%3A%20Yaoxian%20Dong%20and%20Yifan%20Gao%20and%20Haoyue%20Li%20and%20Yanfen%20Cui%20and%20Xin%20Gao%0AAbstract%3A%20%20%20Accurate%20preoperative%20assessment%20of%20lymph%20node%20%28LN%29%20metastasis%20in%20rectal%0Acancer%20guides%20treatment%20decisions%2C%20yet%20conventional%20MRI%20evaluation%20based%20on%0Amorphological%20criteria%20shows%20limited%20diagnostic%20performance.%20While%20some%0Aartificial%20intelligence%20models%20have%20been%20developed%2C%20they%20often%20operate%20as%20black%0Aboxes%2C%20lacking%20the%20interpretability%20needed%20for%20clinical%20trust.%20Moreover%2C%20these%0Amodels%20typically%20evaluate%20nodes%20in%20isolation%2C%20overlooking%20the%20patient-level%0Acontext.%20To%20address%20these%20limitations%2C%20we%20introduce%20LRMR%2C%20an%20LLM-Driven%0ARelational%20Multi-node%20Ranking%20framework.%20This%20approach%20reframes%20the%20diagnostic%0Atask%20from%20a%20direct%20classification%20problem%20into%20a%20structured%20reasoning%20and%0Aranking%20process.%20The%20LRMR%20framework%20operates%20in%20two%20stages.%20First%2C%20a%20multimodal%0Alarge%20language%20model%20%28LLM%29%20analyzes%20a%20composite%20montage%20image%20of%20all%20LNs%20from%20a%0Apatient%2C%20generating%20a%20structured%20report%20that%20details%20ten%20distinct%20radiological%0Afeatures.%20Second%2C%20a%20text-based%20LLM%20performs%20pairwise%20comparisons%20of%20these%0Areports%20between%20different%20patients%2C%20establishing%20a%20relative%20risk%20ranking%20based%0Aon%20the%20severity%20and%20number%20of%20adverse%20features.%20We%20evaluated%20our%20method%20on%20a%0Aretrospective%20cohort%20of%20117%20rectal%20cancer%20patients.%20LRMR%20achieved%20an%20area%20under%0Athe%20curve%20%28AUC%29%20of%200.7917%20and%20an%20F1-score%20of%200.7200%2C%20outperforming%20a%20range%20of%0Adeep%20learning%20baselines%2C%20including%20ResNet50%20%28AUC%200.7708%29.%20Ablation%20studies%0Aconfirmed%20the%20value%20of%20our%20two%20main%20contributions%3A%20removing%20the%20relational%0Aranking%20stage%20or%20the%20structured%20prompting%20stage%20led%20to%20a%20significant%0Aperformance%20drop%2C%20with%20AUCs%20falling%20to%200.6875%20and%200.6458%2C%20respectively.%20Our%0Awork%20demonstrates%20that%20decoupling%20visual%20perception%20from%20cognitive%20reasoning%0Athrough%20a%20two-stage%20LLM%20framework%20offers%20a%20powerful%2C%20interpretable%2C%20and%0Aeffective%20new%20paradigm%20for%20assessing%20lymph%20node%20metastasis%20in%20rectal%20cancer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLRMR%253A%2520LLM-Driven%2520Relational%2520Multi-node%2520Ranking%2520for%2520Lymph%2520Node%2520Metastasis%250A%2520%2520Assessment%2520in%2520Rectal%2520Cancer%26entry.906535625%3DYaoxian%2520Dong%2520and%2520Yifan%2520Gao%2520and%2520Haoyue%2520Li%2520and%2520Yanfen%2520Cui%2520and%2520Xin%2520Gao%26entry.1292438233%3D%2520%2520Accurate%2520preoperative%2520assessment%2520of%2520lymph%2520node%2520%2528LN%2529%2520metastasis%2520in%2520rectal%250Acancer%2520guides%2520treatment%2520decisions%252C%2520yet%2520conventional%2520MRI%2520evaluation%2520based%2520on%250Amorphological%2520criteria%2520shows%2520limited%2520diagnostic%2520performance.%2520While%2520some%250Aartificial%2520intelligence%2520models%2520have%2520been%2520developed%252C%2520they%2520often%2520operate%2520as%2520black%250Aboxes%252C%2520lacking%2520the%2520interpretability%2520needed%2520for%2520clinical%2520trust.%2520Moreover%252C%2520these%250Amodels%2520typically%2520evaluate%2520nodes%2520in%2520isolation%252C%2520overlooking%2520the%2520patient-level%250Acontext.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520LRMR%252C%2520an%2520LLM-Driven%250ARelational%2520Multi-node%2520Ranking%2520framework.%2520This%2520approach%2520reframes%2520the%2520diagnostic%250Atask%2520from%2520a%2520direct%2520classification%2520problem%2520into%2520a%2520structured%2520reasoning%2520and%250Aranking%2520process.%2520The%2520LRMR%2520framework%2520operates%2520in%2520two%2520stages.%2520First%252C%2520a%2520multimodal%250Alarge%2520language%2520model%2520%2528LLM%2529%2520analyzes%2520a%2520composite%2520montage%2520image%2520of%2520all%2520LNs%2520from%2520a%250Apatient%252C%2520generating%2520a%2520structured%2520report%2520that%2520details%2520ten%2520distinct%2520radiological%250Afeatures.%2520Second%252C%2520a%2520text-based%2520LLM%2520performs%2520pairwise%2520comparisons%2520of%2520these%250Areports%2520between%2520different%2520patients%252C%2520establishing%2520a%2520relative%2520risk%2520ranking%2520based%250Aon%2520the%2520severity%2520and%2520number%2520of%2520adverse%2520features.%2520We%2520evaluated%2520our%2520method%2520on%2520a%250Aretrospective%2520cohort%2520of%2520117%2520rectal%2520cancer%2520patients.%2520LRMR%2520achieved%2520an%2520area%2520under%250Athe%2520curve%2520%2528AUC%2529%2520of%25200.7917%2520and%2520an%2520F1-score%2520of%25200.7200%252C%2520outperforming%2520a%2520range%2520of%250Adeep%2520learning%2520baselines%252C%2520including%2520ResNet50%2520%2528AUC%25200.7708%2529.%2520Ablation%2520studies%250Aconfirmed%2520the%2520value%2520of%2520our%2520two%2520main%2520contributions%253A%2520removing%2520the%2520relational%250Aranking%2520stage%2520or%2520the%2520structured%2520prompting%2520stage%2520led%2520to%2520a%2520significant%250Aperformance%2520drop%252C%2520with%2520AUCs%2520falling%2520to%25200.6875%2520and%25200.6458%252C%2520respectively.%2520Our%250Awork%2520demonstrates%2520that%2520decoupling%2520visual%2520perception%2520from%2520cognitive%2520reasoning%250Athrough%2520a%2520two-stage%2520LLM%2520framework%2520offers%2520a%2520powerful%252C%2520interpretable%252C%2520and%250Aeffective%2520new%2520paradigm%2520for%2520assessing%2520lymph%2520node%2520metastasis%2520in%2520rectal%2520cancer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LRMR%3A%20LLM-Driven%20Relational%20Multi-node%20Ranking%20for%20Lymph%20Node%20Metastasis%0A%20%20Assessment%20in%20Rectal%20Cancer&entry.906535625=Yaoxian%20Dong%20and%20Yifan%20Gao%20and%20Haoyue%20Li%20and%20Yanfen%20Cui%20and%20Xin%20Gao&entry.1292438233=%20%20Accurate%20preoperative%20assessment%20of%20lymph%20node%20%28LN%29%20metastasis%20in%20rectal%0Acancer%20guides%20treatment%20decisions%2C%20yet%20conventional%20MRI%20evaluation%20based%20on%0Amorphological%20criteria%20shows%20limited%20diagnostic%20performance.%20While%20some%0Aartificial%20intelligence%20models%20have%20been%20developed%2C%20they%20often%20operate%20as%20black%0Aboxes%2C%20lacking%20the%20interpretability%20needed%20for%20clinical%20trust.%20Moreover%2C%20these%0Amodels%20typically%20evaluate%20nodes%20in%20isolation%2C%20overlooking%20the%20patient-level%0Acontext.%20To%20address%20these%20limitations%2C%20we%20introduce%20LRMR%2C%20an%20LLM-Driven%0ARelational%20Multi-node%20Ranking%20framework.%20This%20approach%20reframes%20the%20diagnostic%0Atask%20from%20a%20direct%20classification%20problem%20into%20a%20structured%20reasoning%20and%0Aranking%20process.%20The%20LRMR%20framework%20operates%20in%20two%20stages.%20First%2C%20a%20multimodal%0Alarge%20language%20model%20%28LLM%29%20analyzes%20a%20composite%20montage%20image%20of%20all%20LNs%20from%20a%0Apatient%2C%20generating%20a%20structured%20report%20that%20details%20ten%20distinct%20radiological%0Afeatures.%20Second%2C%20a%20text-based%20LLM%20performs%20pairwise%20comparisons%20of%20these%0Areports%20between%20different%20patients%2C%20establishing%20a%20relative%20risk%20ranking%20based%0Aon%20the%20severity%20and%20number%20of%20adverse%20features.%20We%20evaluated%20our%20method%20on%20a%0Aretrospective%20cohort%20of%20117%20rectal%20cancer%20patients.%20LRMR%20achieved%20an%20area%20under%0Athe%20curve%20%28AUC%29%20of%200.7917%20and%20an%20F1-score%20of%200.7200%2C%20outperforming%20a%20range%20of%0Adeep%20learning%20baselines%2C%20including%20ResNet50%20%28AUC%200.7708%29.%20Ablation%20studies%0Aconfirmed%20the%20value%20of%20our%20two%20main%20contributions%3A%20removing%20the%20relational%0Aranking%20stage%20or%20the%20structured%20prompting%20stage%20led%20to%20a%20significant%0Aperformance%20drop%2C%20with%20AUCs%20falling%20to%200.6875%20and%200.6458%2C%20respectively.%20Our%0Awork%20demonstrates%20that%20decoupling%20visual%20perception%20from%20cognitive%20reasoning%0Athrough%20a%20two-stage%20LLM%20framework%20offers%20a%20powerful%2C%20interpretable%2C%20and%0Aeffective%20new%20paradigm%20for%20assessing%20lymph%20node%20metastasis%20in%20rectal%20cancer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11457v1&entry.124074799=Read"},
{"title": "A Unified Framework for Evaluating the Effectiveness and Enhancing the\n  Transparency of Explainable AI Methods in Real-World Applications", "author": "Md. Ariful Islam and Md Abrar Jahin and M. F. Mridha and Nilanjan Dey", "abstract": "  The fast growth of deep learning has brought great progress in AI-based\napplications. However, these models are often seen as \"black boxes,\" which\nmakes them hard to understand, explain, or trust. Explainable Artificial\nIntelligence (XAI) tries to make AI decisions clearer so that people can\nunderstand how and why the model makes certain choices. Even though many\nstudies have focused on XAI, there is still a lack of standard ways to measure\nhow well these explanation methods work in real-world situations. This study\nintroduces a single evaluation framework for XAI. It uses both numbers and user\nfeedback to check if the explanations are correct, easy to understand, fair,\ncomplete, and reliable. The framework focuses on users' needs and different\napplication areas, which helps improve the trust and use of AI in important\nfields. To fix problems in current evaluation methods, we propose clear steps,\nincluding loading data, creating explanations, and fully testing them. We also\nsuggest setting common benchmarks. We show the value of this framework through\ncase studies in healthcare, finance, farming, and self-driving systems. These\nexamples prove that our method can support fair and trustworthy evaluation of\nXAI methods. This work gives a clear and practical way to improve transparency\nand trust in AI systems used in the real world.\n", "link": "http://arxiv.org/abs/2412.03884v2", "date": "2025-07-15", "relevancy": 1.9302, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5093}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4772}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Framework%20for%20Evaluating%20the%20Effectiveness%20and%20Enhancing%20the%0A%20%20Transparency%20of%20Explainable%20AI%20Methods%20in%20Real-World%20Applications&body=Title%3A%20A%20Unified%20Framework%20for%20Evaluating%20the%20Effectiveness%20and%20Enhancing%20the%0A%20%20Transparency%20of%20Explainable%20AI%20Methods%20in%20Real-World%20Applications%0AAuthor%3A%20Md.%20Ariful%20Islam%20and%20Md%20Abrar%20Jahin%20and%20M.%20F.%20Mridha%20and%20Nilanjan%20Dey%0AAbstract%3A%20%20%20The%20fast%20growth%20of%20deep%20learning%20has%20brought%20great%20progress%20in%20AI-based%0Aapplications.%20However%2C%20these%20models%20are%20often%20seen%20as%20%22black%20boxes%2C%22%20which%0Amakes%20them%20hard%20to%20understand%2C%20explain%2C%20or%20trust.%20Explainable%20Artificial%0AIntelligence%20%28XAI%29%20tries%20to%20make%20AI%20decisions%20clearer%20so%20that%20people%20can%0Aunderstand%20how%20and%20why%20the%20model%20makes%20certain%20choices.%20Even%20though%20many%0Astudies%20have%20focused%20on%20XAI%2C%20there%20is%20still%20a%20lack%20of%20standard%20ways%20to%20measure%0Ahow%20well%20these%20explanation%20methods%20work%20in%20real-world%20situations.%20This%20study%0Aintroduces%20a%20single%20evaluation%20framework%20for%20XAI.%20It%20uses%20both%20numbers%20and%20user%0Afeedback%20to%20check%20if%20the%20explanations%20are%20correct%2C%20easy%20to%20understand%2C%20fair%2C%0Acomplete%2C%20and%20reliable.%20The%20framework%20focuses%20on%20users%27%20needs%20and%20different%0Aapplication%20areas%2C%20which%20helps%20improve%20the%20trust%20and%20use%20of%20AI%20in%20important%0Afields.%20To%20fix%20problems%20in%20current%20evaluation%20methods%2C%20we%20propose%20clear%20steps%2C%0Aincluding%20loading%20data%2C%20creating%20explanations%2C%20and%20fully%20testing%20them.%20We%20also%0Asuggest%20setting%20common%20benchmarks.%20We%20show%20the%20value%20of%20this%20framework%20through%0Acase%20studies%20in%20healthcare%2C%20finance%2C%20farming%2C%20and%20self-driving%20systems.%20These%0Aexamples%20prove%20that%20our%20method%20can%20support%20fair%20and%20trustworthy%20evaluation%20of%0AXAI%20methods.%20This%20work%20gives%20a%20clear%20and%20practical%20way%20to%20improve%20transparency%0Aand%20trust%20in%20AI%20systems%20used%20in%20the%20real%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03884v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Framework%2520for%2520Evaluating%2520the%2520Effectiveness%2520and%2520Enhancing%2520the%250A%2520%2520Transparency%2520of%2520Explainable%2520AI%2520Methods%2520in%2520Real-World%2520Applications%26entry.906535625%3DMd.%2520Ariful%2520Islam%2520and%2520Md%2520Abrar%2520Jahin%2520and%2520M.%2520F.%2520Mridha%2520and%2520Nilanjan%2520Dey%26entry.1292438233%3D%2520%2520The%2520fast%2520growth%2520of%2520deep%2520learning%2520has%2520brought%2520great%2520progress%2520in%2520AI-based%250Aapplications.%2520However%252C%2520these%2520models%2520are%2520often%2520seen%2520as%2520%2522black%2520boxes%252C%2522%2520which%250Amakes%2520them%2520hard%2520to%2520understand%252C%2520explain%252C%2520or%2520trust.%2520Explainable%2520Artificial%250AIntelligence%2520%2528XAI%2529%2520tries%2520to%2520make%2520AI%2520decisions%2520clearer%2520so%2520that%2520people%2520can%250Aunderstand%2520how%2520and%2520why%2520the%2520model%2520makes%2520certain%2520choices.%2520Even%2520though%2520many%250Astudies%2520have%2520focused%2520on%2520XAI%252C%2520there%2520is%2520still%2520a%2520lack%2520of%2520standard%2520ways%2520to%2520measure%250Ahow%2520well%2520these%2520explanation%2520methods%2520work%2520in%2520real-world%2520situations.%2520This%2520study%250Aintroduces%2520a%2520single%2520evaluation%2520framework%2520for%2520XAI.%2520It%2520uses%2520both%2520numbers%2520and%2520user%250Afeedback%2520to%2520check%2520if%2520the%2520explanations%2520are%2520correct%252C%2520easy%2520to%2520understand%252C%2520fair%252C%250Acomplete%252C%2520and%2520reliable.%2520The%2520framework%2520focuses%2520on%2520users%2527%2520needs%2520and%2520different%250Aapplication%2520areas%252C%2520which%2520helps%2520improve%2520the%2520trust%2520and%2520use%2520of%2520AI%2520in%2520important%250Afields.%2520To%2520fix%2520problems%2520in%2520current%2520evaluation%2520methods%252C%2520we%2520propose%2520clear%2520steps%252C%250Aincluding%2520loading%2520data%252C%2520creating%2520explanations%252C%2520and%2520fully%2520testing%2520them.%2520We%2520also%250Asuggest%2520setting%2520common%2520benchmarks.%2520We%2520show%2520the%2520value%2520of%2520this%2520framework%2520through%250Acase%2520studies%2520in%2520healthcare%252C%2520finance%252C%2520farming%252C%2520and%2520self-driving%2520systems.%2520These%250Aexamples%2520prove%2520that%2520our%2520method%2520can%2520support%2520fair%2520and%2520trustworthy%2520evaluation%2520of%250AXAI%2520methods.%2520This%2520work%2520gives%2520a%2520clear%2520and%2520practical%2520way%2520to%2520improve%2520transparency%250Aand%2520trust%2520in%2520AI%2520systems%2520used%2520in%2520the%2520real%2520world.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03884v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Framework%20for%20Evaluating%20the%20Effectiveness%20and%20Enhancing%20the%0A%20%20Transparency%20of%20Explainable%20AI%20Methods%20in%20Real-World%20Applications&entry.906535625=Md.%20Ariful%20Islam%20and%20Md%20Abrar%20Jahin%20and%20M.%20F.%20Mridha%20and%20Nilanjan%20Dey&entry.1292438233=%20%20The%20fast%20growth%20of%20deep%20learning%20has%20brought%20great%20progress%20in%20AI-based%0Aapplications.%20However%2C%20these%20models%20are%20often%20seen%20as%20%22black%20boxes%2C%22%20which%0Amakes%20them%20hard%20to%20understand%2C%20explain%2C%20or%20trust.%20Explainable%20Artificial%0AIntelligence%20%28XAI%29%20tries%20to%20make%20AI%20decisions%20clearer%20so%20that%20people%20can%0Aunderstand%20how%20and%20why%20the%20model%20makes%20certain%20choices.%20Even%20though%20many%0Astudies%20have%20focused%20on%20XAI%2C%20there%20is%20still%20a%20lack%20of%20standard%20ways%20to%20measure%0Ahow%20well%20these%20explanation%20methods%20work%20in%20real-world%20situations.%20This%20study%0Aintroduces%20a%20single%20evaluation%20framework%20for%20XAI.%20It%20uses%20both%20numbers%20and%20user%0Afeedback%20to%20check%20if%20the%20explanations%20are%20correct%2C%20easy%20to%20understand%2C%20fair%2C%0Acomplete%2C%20and%20reliable.%20The%20framework%20focuses%20on%20users%27%20needs%20and%20different%0Aapplication%20areas%2C%20which%20helps%20improve%20the%20trust%20and%20use%20of%20AI%20in%20important%0Afields.%20To%20fix%20problems%20in%20current%20evaluation%20methods%2C%20we%20propose%20clear%20steps%2C%0Aincluding%20loading%20data%2C%20creating%20explanations%2C%20and%20fully%20testing%20them.%20We%20also%0Asuggest%20setting%20common%20benchmarks.%20We%20show%20the%20value%20of%20this%20framework%20through%0Acase%20studies%20in%20healthcare%2C%20finance%2C%20farming%2C%20and%20self-driving%20systems.%20These%0Aexamples%20prove%20that%20our%20method%20can%20support%20fair%20and%20trustworthy%20evaluation%20of%0AXAI%20methods.%20This%20work%20gives%20a%20clear%20and%20practical%20way%20to%20improve%20transparency%0Aand%20trust%20in%20AI%20systems%20used%20in%20the%20real%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03884v2&entry.124074799=Read"},
{"title": "Is Human-Written Data Enough? The Challenge of Teaching Reasoning to\n  LLMs Without RL or Distillation", "author": "Wei Du and Branislav Kisacanin and George Armstrong and Shubham Toshniwal and Ivan Moshkov and Alexan Ayrapetyan and Sadegh Mahdavi and Dan Zhao and Shizhe Diao and Dragan Masulovic and Marius Stanean and Advaith Avadhanam and Max Wang and Ashmit Dutta and Shitij Govil and Sri Yanamandara and Mihir Tandon and Sriram Ananthakrishnan and Vedant Rathi and David Zhang and Joonseok Kang and Leon Luo and Titu Andreescu and Boris Ginsburg and Igor Gitman", "abstract": "  Reasoning-capable language models achieve state-of-the-art performance in\ndiverse complex tasks by generating long, explicit Chain-of-Thought (CoT)\ntraces. While recent works show that base models can acquire such reasoning\ntraces via reinforcement learning or distillation from stronger models like\nDeepSeek-R1, previous works demonstrate that even short CoT prompting without\nfine-tuning is able to improve reasoning. We ask whether long CoT can be\ninduced in a base model using only prompting or minimal tuning. Using just 20\nlong CoT examples from the reasoning model \\texttt{QwQ-32B-Preview}, we lightly\nfine-tune the base model \\texttt{Qwen2.5-32B}. The resulting model outperforms\nthe much larger \\texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of\nhigh-quality examples can unlock strong reasoning capabilities. We further\nexplore using CoT data from non-reasoning models and human annotators, enhanced\nwith prompt engineering, multi-pass editing, and structural guidance. However,\nneither matches the performance of reasoning model traces, suggesting that\ncertain latent qualities of expert CoT are difficult to replicate. We analyze\nkey properties of reasoning data, such as problem difficulty, diversity, and\nanswer length, that influence reasoning distillation. While challenges remain,\nwe are optimistic that carefully curated human-written CoT, even in small\nquantities, can activate reasoning behaviors in base models. We release our\nhuman-authored dataset across refinement stages and invite further\ninvestigation into what makes small-scale reasoning supervision so effective.\n", "link": "http://arxiv.org/abs/2507.09850v2", "date": "2025-07-15", "relevancy": 1.9162, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4822}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4822}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Human-Written%20Data%20Enough%3F%20The%20Challenge%20of%20Teaching%20Reasoning%20to%0A%20%20LLMs%20Without%20RL%20or%20Distillation&body=Title%3A%20Is%20Human-Written%20Data%20Enough%3F%20The%20Challenge%20of%20Teaching%20Reasoning%20to%0A%20%20LLMs%20Without%20RL%20or%20Distillation%0AAuthor%3A%20Wei%20Du%20and%20Branislav%20Kisacanin%20and%20George%20Armstrong%20and%20Shubham%20Toshniwal%20and%20Ivan%20Moshkov%20and%20Alexan%20Ayrapetyan%20and%20Sadegh%20Mahdavi%20and%20Dan%20Zhao%20and%20Shizhe%20Diao%20and%20Dragan%20Masulovic%20and%20Marius%20Stanean%20and%20Advaith%20Avadhanam%20and%20Max%20Wang%20and%20Ashmit%20Dutta%20and%20Shitij%20Govil%20and%20Sri%20Yanamandara%20and%20Mihir%20Tandon%20and%20Sriram%20Ananthakrishnan%20and%20Vedant%20Rathi%20and%20David%20Zhang%20and%20Joonseok%20Kang%20and%20Leon%20Luo%20and%20Titu%20Andreescu%20and%20Boris%20Ginsburg%20and%20Igor%20Gitman%0AAbstract%3A%20%20%20Reasoning-capable%20language%20models%20achieve%20state-of-the-art%20performance%20in%0Adiverse%20complex%20tasks%20by%20generating%20long%2C%20explicit%20Chain-of-Thought%20%28CoT%29%0Atraces.%20While%20recent%20works%20show%20that%20base%20models%20can%20acquire%20such%20reasoning%0Atraces%20via%20reinforcement%20learning%20or%20distillation%20from%20stronger%20models%20like%0ADeepSeek-R1%2C%20previous%20works%20demonstrate%20that%20even%20short%20CoT%20prompting%20without%0Afine-tuning%20is%20able%20to%20improve%20reasoning.%20We%20ask%20whether%20long%20CoT%20can%20be%0Ainduced%20in%20a%20base%20model%20using%20only%20prompting%20or%20minimal%20tuning.%20Using%20just%2020%0Along%20CoT%20examples%20from%20the%20reasoning%20model%20%5Ctexttt%7BQwQ-32B-Preview%7D%2C%20we%20lightly%0Afine-tune%20the%20base%20model%20%5Ctexttt%7BQwen2.5-32B%7D.%20The%20resulting%20model%20outperforms%0Athe%20much%20larger%20%5Ctexttt%7BQwen2.5-Math-72B-Instruct%7D%2C%20showing%20that%20a%20handful%20of%0Ahigh-quality%20examples%20can%20unlock%20strong%20reasoning%20capabilities.%20We%20further%0Aexplore%20using%20CoT%20data%20from%20non-reasoning%20models%20and%20human%20annotators%2C%20enhanced%0Awith%20prompt%20engineering%2C%20multi-pass%20editing%2C%20and%20structural%20guidance.%20However%2C%0Aneither%20matches%20the%20performance%20of%20reasoning%20model%20traces%2C%20suggesting%20that%0Acertain%20latent%20qualities%20of%20expert%20CoT%20are%20difficult%20to%20replicate.%20We%20analyze%0Akey%20properties%20of%20reasoning%20data%2C%20such%20as%20problem%20difficulty%2C%20diversity%2C%20and%0Aanswer%20length%2C%20that%20influence%20reasoning%20distillation.%20While%20challenges%20remain%2C%0Awe%20are%20optimistic%20that%20carefully%20curated%20human-written%20CoT%2C%20even%20in%20small%0Aquantities%2C%20can%20activate%20reasoning%20behaviors%20in%20base%20models.%20We%20release%20our%0Ahuman-authored%20dataset%20across%20refinement%20stages%20and%20invite%20further%0Ainvestigation%20into%20what%20makes%20small-scale%20reasoning%20supervision%20so%20effective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.09850v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Human-Written%2520Data%2520Enough%253F%2520The%2520Challenge%2520of%2520Teaching%2520Reasoning%2520to%250A%2520%2520LLMs%2520Without%2520RL%2520or%2520Distillation%26entry.906535625%3DWei%2520Du%2520and%2520Branislav%2520Kisacanin%2520and%2520George%2520Armstrong%2520and%2520Shubham%2520Toshniwal%2520and%2520Ivan%2520Moshkov%2520and%2520Alexan%2520Ayrapetyan%2520and%2520Sadegh%2520Mahdavi%2520and%2520Dan%2520Zhao%2520and%2520Shizhe%2520Diao%2520and%2520Dragan%2520Masulovic%2520and%2520Marius%2520Stanean%2520and%2520Advaith%2520Avadhanam%2520and%2520Max%2520Wang%2520and%2520Ashmit%2520Dutta%2520and%2520Shitij%2520Govil%2520and%2520Sri%2520Yanamandara%2520and%2520Mihir%2520Tandon%2520and%2520Sriram%2520Ananthakrishnan%2520and%2520Vedant%2520Rathi%2520and%2520David%2520Zhang%2520and%2520Joonseok%2520Kang%2520and%2520Leon%2520Luo%2520and%2520Titu%2520Andreescu%2520and%2520Boris%2520Ginsburg%2520and%2520Igor%2520Gitman%26entry.1292438233%3D%2520%2520Reasoning-capable%2520language%2520models%2520achieve%2520state-of-the-art%2520performance%2520in%250Adiverse%2520complex%2520tasks%2520by%2520generating%2520long%252C%2520explicit%2520Chain-of-Thought%2520%2528CoT%2529%250Atraces.%2520While%2520recent%2520works%2520show%2520that%2520base%2520models%2520can%2520acquire%2520such%2520reasoning%250Atraces%2520via%2520reinforcement%2520learning%2520or%2520distillation%2520from%2520stronger%2520models%2520like%250ADeepSeek-R1%252C%2520previous%2520works%2520demonstrate%2520that%2520even%2520short%2520CoT%2520prompting%2520without%250Afine-tuning%2520is%2520able%2520to%2520improve%2520reasoning.%2520We%2520ask%2520whether%2520long%2520CoT%2520can%2520be%250Ainduced%2520in%2520a%2520base%2520model%2520using%2520only%2520prompting%2520or%2520minimal%2520tuning.%2520Using%2520just%252020%250Along%2520CoT%2520examples%2520from%2520the%2520reasoning%2520model%2520%255Ctexttt%257BQwQ-32B-Preview%257D%252C%2520we%2520lightly%250Afine-tune%2520the%2520base%2520model%2520%255Ctexttt%257BQwen2.5-32B%257D.%2520The%2520resulting%2520model%2520outperforms%250Athe%2520much%2520larger%2520%255Ctexttt%257BQwen2.5-Math-72B-Instruct%257D%252C%2520showing%2520that%2520a%2520handful%2520of%250Ahigh-quality%2520examples%2520can%2520unlock%2520strong%2520reasoning%2520capabilities.%2520We%2520further%250Aexplore%2520using%2520CoT%2520data%2520from%2520non-reasoning%2520models%2520and%2520human%2520annotators%252C%2520enhanced%250Awith%2520prompt%2520engineering%252C%2520multi-pass%2520editing%252C%2520and%2520structural%2520guidance.%2520However%252C%250Aneither%2520matches%2520the%2520performance%2520of%2520reasoning%2520model%2520traces%252C%2520suggesting%2520that%250Acertain%2520latent%2520qualities%2520of%2520expert%2520CoT%2520are%2520difficult%2520to%2520replicate.%2520We%2520analyze%250Akey%2520properties%2520of%2520reasoning%2520data%252C%2520such%2520as%2520problem%2520difficulty%252C%2520diversity%252C%2520and%250Aanswer%2520length%252C%2520that%2520influence%2520reasoning%2520distillation.%2520While%2520challenges%2520remain%252C%250Awe%2520are%2520optimistic%2520that%2520carefully%2520curated%2520human-written%2520CoT%252C%2520even%2520in%2520small%250Aquantities%252C%2520can%2520activate%2520reasoning%2520behaviors%2520in%2520base%2520models.%2520We%2520release%2520our%250Ahuman-authored%2520dataset%2520across%2520refinement%2520stages%2520and%2520invite%2520further%250Ainvestigation%2520into%2520what%2520makes%2520small-scale%2520reasoning%2520supervision%2520so%2520effective.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09850v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Human-Written%20Data%20Enough%3F%20The%20Challenge%20of%20Teaching%20Reasoning%20to%0A%20%20LLMs%20Without%20RL%20or%20Distillation&entry.906535625=Wei%20Du%20and%20Branislav%20Kisacanin%20and%20George%20Armstrong%20and%20Shubham%20Toshniwal%20and%20Ivan%20Moshkov%20and%20Alexan%20Ayrapetyan%20and%20Sadegh%20Mahdavi%20and%20Dan%20Zhao%20and%20Shizhe%20Diao%20and%20Dragan%20Masulovic%20and%20Marius%20Stanean%20and%20Advaith%20Avadhanam%20and%20Max%20Wang%20and%20Ashmit%20Dutta%20and%20Shitij%20Govil%20and%20Sri%20Yanamandara%20and%20Mihir%20Tandon%20and%20Sriram%20Ananthakrishnan%20and%20Vedant%20Rathi%20and%20David%20Zhang%20and%20Joonseok%20Kang%20and%20Leon%20Luo%20and%20Titu%20Andreescu%20and%20Boris%20Ginsburg%20and%20Igor%20Gitman&entry.1292438233=%20%20Reasoning-capable%20language%20models%20achieve%20state-of-the-art%20performance%20in%0Adiverse%20complex%20tasks%20by%20generating%20long%2C%20explicit%20Chain-of-Thought%20%28CoT%29%0Atraces.%20While%20recent%20works%20show%20that%20base%20models%20can%20acquire%20such%20reasoning%0Atraces%20via%20reinforcement%20learning%20or%20distillation%20from%20stronger%20models%20like%0ADeepSeek-R1%2C%20previous%20works%20demonstrate%20that%20even%20short%20CoT%20prompting%20without%0Afine-tuning%20is%20able%20to%20improve%20reasoning.%20We%20ask%20whether%20long%20CoT%20can%20be%0Ainduced%20in%20a%20base%20model%20using%20only%20prompting%20or%20minimal%20tuning.%20Using%20just%2020%0Along%20CoT%20examples%20from%20the%20reasoning%20model%20%5Ctexttt%7BQwQ-32B-Preview%7D%2C%20we%20lightly%0Afine-tune%20the%20base%20model%20%5Ctexttt%7BQwen2.5-32B%7D.%20The%20resulting%20model%20outperforms%0Athe%20much%20larger%20%5Ctexttt%7BQwen2.5-Math-72B-Instruct%7D%2C%20showing%20that%20a%20handful%20of%0Ahigh-quality%20examples%20can%20unlock%20strong%20reasoning%20capabilities.%20We%20further%0Aexplore%20using%20CoT%20data%20from%20non-reasoning%20models%20and%20human%20annotators%2C%20enhanced%0Awith%20prompt%20engineering%2C%20multi-pass%20editing%2C%20and%20structural%20guidance.%20However%2C%0Aneither%20matches%20the%20performance%20of%20reasoning%20model%20traces%2C%20suggesting%20that%0Acertain%20latent%20qualities%20of%20expert%20CoT%20are%20difficult%20to%20replicate.%20We%20analyze%0Akey%20properties%20of%20reasoning%20data%2C%20such%20as%20problem%20difficulty%2C%20diversity%2C%20and%0Aanswer%20length%2C%20that%20influence%20reasoning%20distillation.%20While%20challenges%20remain%2C%0Awe%20are%20optimistic%20that%20carefully%20curated%20human-written%20CoT%2C%20even%20in%20small%0Aquantities%2C%20can%20activate%20reasoning%20behaviors%20in%20base%20models.%20We%20release%20our%0Ahuman-authored%20dataset%20across%20refinement%20stages%20and%20invite%20further%0Ainvestigation%20into%20what%20makes%20small-scale%20reasoning%20supervision%20so%20effective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.09850v2&entry.124074799=Read"},
{"title": "Recursive Bound-Constrained AdaGrad with Applications to Multilevel and\n  Domain Decomposition Minimization", "author": "Serge Gratton and Alena Kopani\u010d\u00e1kov\u00e1 and Philippe Toint", "abstract": "  Two OFFO (Objective-Function Free Optimization) noise tolerant algorithms are\npresented that handle bound constraints, inexact gradients and use second-order\ninformation when available.The first is a multi-level method exploiting a\nhierarchical description of the problem and the second is a\ndomain-decomposition method covering the standard addditive Schwarz\ndecompositions. Both are generalizations of the first-order AdaGrad algorithm\nfor unconstrained optimization. Because these algorithms share a common\ntheoretical framework, a single convergence/complexity theory is provided which\ncovers them both. Its main result is that, with high probability, both methods\nneed at most $O(\\epsilon^{-2})$ iterations and noisy gradient evaluations to\ncompute an $\\epsilon$-approximate first-order critical point of the\nbound-constrained problem. Extensive numerical experiments are discussed on\napplications ranging from PDE-based problems to deep neural network training,\nillustrating their remarkable computational efficiency.\n", "link": "http://arxiv.org/abs/2507.11513v1", "date": "2025-07-15", "relevancy": 1.9123, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4876}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.482}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4703}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recursive%20Bound-Constrained%20AdaGrad%20with%20Applications%20to%20Multilevel%20and%0A%20%20Domain%20Decomposition%20Minimization&body=Title%3A%20Recursive%20Bound-Constrained%20AdaGrad%20with%20Applications%20to%20Multilevel%20and%0A%20%20Domain%20Decomposition%20Minimization%0AAuthor%3A%20Serge%20Gratton%20and%20Alena%20Kopani%C4%8D%C3%A1kov%C3%A1%20and%20Philippe%20Toint%0AAbstract%3A%20%20%20Two%20OFFO%20%28Objective-Function%20Free%20Optimization%29%20noise%20tolerant%20algorithms%20are%0Apresented%20that%20handle%20bound%20constraints%2C%20inexact%20gradients%20and%20use%20second-order%0Ainformation%20when%20available.The%20first%20is%20a%20multi-level%20method%20exploiting%20a%0Ahierarchical%20description%20of%20the%20problem%20and%20the%20second%20is%20a%0Adomain-decomposition%20method%20covering%20the%20standard%20addditive%20Schwarz%0Adecompositions.%20Both%20are%20generalizations%20of%20the%20first-order%20AdaGrad%20algorithm%0Afor%20unconstrained%20optimization.%20Because%20these%20algorithms%20share%20a%20common%0Atheoretical%20framework%2C%20a%20single%20convergence/complexity%20theory%20is%20provided%20which%0Acovers%20them%20both.%20Its%20main%20result%20is%20that%2C%20with%20high%20probability%2C%20both%20methods%0Aneed%20at%20most%20%24O%28%5Cepsilon%5E%7B-2%7D%29%24%20iterations%20and%20noisy%20gradient%20evaluations%20to%0Acompute%20an%20%24%5Cepsilon%24-approximate%20first-order%20critical%20point%20of%20the%0Abound-constrained%20problem.%20Extensive%20numerical%20experiments%20are%20discussed%20on%0Aapplications%20ranging%20from%20PDE-based%20problems%20to%20deep%20neural%20network%20training%2C%0Aillustrating%20their%20remarkable%20computational%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11513v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecursive%2520Bound-Constrained%2520AdaGrad%2520with%2520Applications%2520to%2520Multilevel%2520and%250A%2520%2520Domain%2520Decomposition%2520Minimization%26entry.906535625%3DSerge%2520Gratton%2520and%2520Alena%2520Kopani%25C4%258D%25C3%25A1kov%25C3%25A1%2520and%2520Philippe%2520Toint%26entry.1292438233%3D%2520%2520Two%2520OFFO%2520%2528Objective-Function%2520Free%2520Optimization%2529%2520noise%2520tolerant%2520algorithms%2520are%250Apresented%2520that%2520handle%2520bound%2520constraints%252C%2520inexact%2520gradients%2520and%2520use%2520second-order%250Ainformation%2520when%2520available.The%2520first%2520is%2520a%2520multi-level%2520method%2520exploiting%2520a%250Ahierarchical%2520description%2520of%2520the%2520problem%2520and%2520the%2520second%2520is%2520a%250Adomain-decomposition%2520method%2520covering%2520the%2520standard%2520addditive%2520Schwarz%250Adecompositions.%2520Both%2520are%2520generalizations%2520of%2520the%2520first-order%2520AdaGrad%2520algorithm%250Afor%2520unconstrained%2520optimization.%2520Because%2520these%2520algorithms%2520share%2520a%2520common%250Atheoretical%2520framework%252C%2520a%2520single%2520convergence/complexity%2520theory%2520is%2520provided%2520which%250Acovers%2520them%2520both.%2520Its%2520main%2520result%2520is%2520that%252C%2520with%2520high%2520probability%252C%2520both%2520methods%250Aneed%2520at%2520most%2520%2524O%2528%255Cepsilon%255E%257B-2%257D%2529%2524%2520iterations%2520and%2520noisy%2520gradient%2520evaluations%2520to%250Acompute%2520an%2520%2524%255Cepsilon%2524-approximate%2520first-order%2520critical%2520point%2520of%2520the%250Abound-constrained%2520problem.%2520Extensive%2520numerical%2520experiments%2520are%2520discussed%2520on%250Aapplications%2520ranging%2520from%2520PDE-based%2520problems%2520to%2520deep%2520neural%2520network%2520training%252C%250Aillustrating%2520their%2520remarkable%2520computational%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11513v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recursive%20Bound-Constrained%20AdaGrad%20with%20Applications%20to%20Multilevel%20and%0A%20%20Domain%20Decomposition%20Minimization&entry.906535625=Serge%20Gratton%20and%20Alena%20Kopani%C4%8D%C3%A1kov%C3%A1%20and%20Philippe%20Toint&entry.1292438233=%20%20Two%20OFFO%20%28Objective-Function%20Free%20Optimization%29%20noise%20tolerant%20algorithms%20are%0Apresented%20that%20handle%20bound%20constraints%2C%20inexact%20gradients%20and%20use%20second-order%0Ainformation%20when%20available.The%20first%20is%20a%20multi-level%20method%20exploiting%20a%0Ahierarchical%20description%20of%20the%20problem%20and%20the%20second%20is%20a%0Adomain-decomposition%20method%20covering%20the%20standard%20addditive%20Schwarz%0Adecompositions.%20Both%20are%20generalizations%20of%20the%20first-order%20AdaGrad%20algorithm%0Afor%20unconstrained%20optimization.%20Because%20these%20algorithms%20share%20a%20common%0Atheoretical%20framework%2C%20a%20single%20convergence/complexity%20theory%20is%20provided%20which%0Acovers%20them%20both.%20Its%20main%20result%20is%20that%2C%20with%20high%20probability%2C%20both%20methods%0Aneed%20at%20most%20%24O%28%5Cepsilon%5E%7B-2%7D%29%24%20iterations%20and%20noisy%20gradient%20evaluations%20to%0Acompute%20an%20%24%5Cepsilon%24-approximate%20first-order%20critical%20point%20of%20the%0Abound-constrained%20problem.%20Extensive%20numerical%20experiments%20are%20discussed%20on%0Aapplications%20ranging%20from%20PDE-based%20problems%20to%20deep%20neural%20network%20training%2C%0Aillustrating%20their%20remarkable%20computational%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11513v1&entry.124074799=Read"},
{"title": "Exploring the robustness of TractOracle methods in RL-based tractography", "author": "Jeremi Levesque and Antoine Th\u00e9berge and Maxime Descoteaux and Pierre-Marc Jodoin", "abstract": "  Tractography algorithms leverage diffusion MRI to reconstruct the fibrous\narchitecture of the brain's white matter. Among machine learning approaches,\nreinforcement learning (RL) has emerged as a promising framework for\ntractography, outperforming traditional methods in several key aspects.\nTractOracle-RL, a recent RL-based approach, reduces false positives by\nincorporating anatomical priors into the training process via a reward-based\nmechanism. In this paper, we investigate four extensions of the original\nTractOracle-RL framework by integrating recent advances in RL, and we evaluate\ntheir performance across five diverse diffusion MRI datasets. Results\ndemonstrate that combining an oracle with the RL framework consistently leads\nto robust and reliable tractography, regardless of the specific method or\ndataset used. We also introduce a novel RL training scheme called Iterative\nReward Training (IRT), inspired by the Reinforcement Learning from Human\nFeedback (RLHF) paradigm. Instead of relying on human input, IRT leverages\nbundle filtering methods to iteratively refine the oracle's guidance throughout\ntraining. Experimental results show that RL methods trained with oracle\nfeedback significantly outperform widely used tractography techniques in terms\nof accuracy and anatomical validity.\n", "link": "http://arxiv.org/abs/2507.11486v1", "date": "2025-07-15", "relevancy": 1.903, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.477}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4767}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20robustness%20of%20TractOracle%20methods%20in%20RL-based%20tractography&body=Title%3A%20Exploring%20the%20robustness%20of%20TractOracle%20methods%20in%20RL-based%20tractography%0AAuthor%3A%20Jeremi%20Levesque%20and%20Antoine%20Th%C3%A9berge%20and%20Maxime%20Descoteaux%20and%20Pierre-Marc%20Jodoin%0AAbstract%3A%20%20%20Tractography%20algorithms%20leverage%20diffusion%20MRI%20to%20reconstruct%20the%20fibrous%0Aarchitecture%20of%20the%20brain%27s%20white%20matter.%20Among%20machine%20learning%20approaches%2C%0Areinforcement%20learning%20%28RL%29%20has%20emerged%20as%20a%20promising%20framework%20for%0Atractography%2C%20outperforming%20traditional%20methods%20in%20several%20key%20aspects.%0ATractOracle-RL%2C%20a%20recent%20RL-based%20approach%2C%20reduces%20false%20positives%20by%0Aincorporating%20anatomical%20priors%20into%20the%20training%20process%20via%20a%20reward-based%0Amechanism.%20In%20this%20paper%2C%20we%20investigate%20four%20extensions%20of%20the%20original%0ATractOracle-RL%20framework%20by%20integrating%20recent%20advances%20in%20RL%2C%20and%20we%20evaluate%0Atheir%20performance%20across%20five%20diverse%20diffusion%20MRI%20datasets.%20Results%0Ademonstrate%20that%20combining%20an%20oracle%20with%20the%20RL%20framework%20consistently%20leads%0Ato%20robust%20and%20reliable%20tractography%2C%20regardless%20of%20the%20specific%20method%20or%0Adataset%20used.%20We%20also%20introduce%20a%20novel%20RL%20training%20scheme%20called%20Iterative%0AReward%20Training%20%28IRT%29%2C%20inspired%20by%20the%20Reinforcement%20Learning%20from%20Human%0AFeedback%20%28RLHF%29%20paradigm.%20Instead%20of%20relying%20on%20human%20input%2C%20IRT%20leverages%0Abundle%20filtering%20methods%20to%20iteratively%20refine%20the%20oracle%27s%20guidance%20throughout%0Atraining.%20Experimental%20results%20show%20that%20RL%20methods%20trained%20with%20oracle%0Afeedback%20significantly%20outperform%20widely%20used%20tractography%20techniques%20in%20terms%0Aof%20accuracy%20and%20anatomical%20validity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11486v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520robustness%2520of%2520TractOracle%2520methods%2520in%2520RL-based%2520tractography%26entry.906535625%3DJeremi%2520Levesque%2520and%2520Antoine%2520Th%25C3%25A9berge%2520and%2520Maxime%2520Descoteaux%2520and%2520Pierre-Marc%2520Jodoin%26entry.1292438233%3D%2520%2520Tractography%2520algorithms%2520leverage%2520diffusion%2520MRI%2520to%2520reconstruct%2520the%2520fibrous%250Aarchitecture%2520of%2520the%2520brain%2527s%2520white%2520matter.%2520Among%2520machine%2520learning%2520approaches%252C%250Areinforcement%2520learning%2520%2528RL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520framework%2520for%250Atractography%252C%2520outperforming%2520traditional%2520methods%2520in%2520several%2520key%2520aspects.%250ATractOracle-RL%252C%2520a%2520recent%2520RL-based%2520approach%252C%2520reduces%2520false%2520positives%2520by%250Aincorporating%2520anatomical%2520priors%2520into%2520the%2520training%2520process%2520via%2520a%2520reward-based%250Amechanism.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520four%2520extensions%2520of%2520the%2520original%250ATractOracle-RL%2520framework%2520by%2520integrating%2520recent%2520advances%2520in%2520RL%252C%2520and%2520we%2520evaluate%250Atheir%2520performance%2520across%2520five%2520diverse%2520diffusion%2520MRI%2520datasets.%2520Results%250Ademonstrate%2520that%2520combining%2520an%2520oracle%2520with%2520the%2520RL%2520framework%2520consistently%2520leads%250Ato%2520robust%2520and%2520reliable%2520tractography%252C%2520regardless%2520of%2520the%2520specific%2520method%2520or%250Adataset%2520used.%2520We%2520also%2520introduce%2520a%2520novel%2520RL%2520training%2520scheme%2520called%2520Iterative%250AReward%2520Training%2520%2528IRT%2529%252C%2520inspired%2520by%2520the%2520Reinforcement%2520Learning%2520from%2520Human%250AFeedback%2520%2528RLHF%2529%2520paradigm.%2520Instead%2520of%2520relying%2520on%2520human%2520input%252C%2520IRT%2520leverages%250Abundle%2520filtering%2520methods%2520to%2520iteratively%2520refine%2520the%2520oracle%2527s%2520guidance%2520throughout%250Atraining.%2520Experimental%2520results%2520show%2520that%2520RL%2520methods%2520trained%2520with%2520oracle%250Afeedback%2520significantly%2520outperform%2520widely%2520used%2520tractography%2520techniques%2520in%2520terms%250Aof%2520accuracy%2520and%2520anatomical%2520validity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11486v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20robustness%20of%20TractOracle%20methods%20in%20RL-based%20tractography&entry.906535625=Jeremi%20Levesque%20and%20Antoine%20Th%C3%A9berge%20and%20Maxime%20Descoteaux%20and%20Pierre-Marc%20Jodoin&entry.1292438233=%20%20Tractography%20algorithms%20leverage%20diffusion%20MRI%20to%20reconstruct%20the%20fibrous%0Aarchitecture%20of%20the%20brain%27s%20white%20matter.%20Among%20machine%20learning%20approaches%2C%0Areinforcement%20learning%20%28RL%29%20has%20emerged%20as%20a%20promising%20framework%20for%0Atractography%2C%20outperforming%20traditional%20methods%20in%20several%20key%20aspects.%0ATractOracle-RL%2C%20a%20recent%20RL-based%20approach%2C%20reduces%20false%20positives%20by%0Aincorporating%20anatomical%20priors%20into%20the%20training%20process%20via%20a%20reward-based%0Amechanism.%20In%20this%20paper%2C%20we%20investigate%20four%20extensions%20of%20the%20original%0ATractOracle-RL%20framework%20by%20integrating%20recent%20advances%20in%20RL%2C%20and%20we%20evaluate%0Atheir%20performance%20across%20five%20diverse%20diffusion%20MRI%20datasets.%20Results%0Ademonstrate%20that%20combining%20an%20oracle%20with%20the%20RL%20framework%20consistently%20leads%0Ato%20robust%20and%20reliable%20tractography%2C%20regardless%20of%20the%20specific%20method%20or%0Adataset%20used.%20We%20also%20introduce%20a%20novel%20RL%20training%20scheme%20called%20Iterative%0AReward%20Training%20%28IRT%29%2C%20inspired%20by%20the%20Reinforcement%20Learning%20from%20Human%0AFeedback%20%28RLHF%29%20paradigm.%20Instead%20of%20relying%20on%20human%20input%2C%20IRT%20leverages%0Abundle%20filtering%20methods%20to%20iteratively%20refine%20the%20oracle%27s%20guidance%20throughout%0Atraining.%20Experimental%20results%20show%20that%20RL%20methods%20trained%20with%20oracle%0Afeedback%20significantly%20outperform%20widely%20used%20tractography%20techniques%20in%20terms%0Aof%20accuracy%20and%20anatomical%20validity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11486v1&entry.124074799=Read"},
{"title": "Reinforcement Learning with Action Chunking", "author": "Qiyang Li and Zhiyuan Zhou and Sergey Levine", "abstract": "  We present Q-chunking, a simple yet effective recipe for improving\nreinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks.\nOur recipe is designed for the offline-to-online RL setting, where the goal is\nto leverage an offline prior dataset to maximize the sample-efficiency of\nonline learning. Effective exploration and sample-efficient learning remain\ncentral challenges in this setting, as it is not obvious how the offline data\nshould be utilized to acquire a good exploratory policy. Our key insight is\nthat action chunking, a technique popularized in imitation learning where\nsequences of future actions are predicted rather than a single action at each\ntimestep, can be applied to temporal difference (TD)-based RL methods to\nmitigate the exploration challenge. Q-chunking adopts action chunking by\ndirectly running RL in a 'chunked' action space, enabling the agent to (1)\nleverage temporally consistent behaviors from offline data for more effective\nonline exploration and (2) use unbiased $n$-step backups for more stable and\nefficient TD learning. Our experimental results demonstrate that Q-chunking\nexhibits strong offline performance and online sample efficiency, outperforming\nprior best offline-to-online methods on a range of long-horizon, sparse-reward\nmanipulation tasks.\n", "link": "http://arxiv.org/abs/2507.07969v2", "date": "2025-07-15", "relevancy": 1.8668, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5055}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4688}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reinforcement%20Learning%20with%20Action%20Chunking&body=Title%3A%20Reinforcement%20Learning%20with%20Action%20Chunking%0AAuthor%3A%20Qiyang%20Li%20and%20Zhiyuan%20Zhou%20and%20Sergey%20Levine%0AAbstract%3A%20%20%20We%20present%20Q-chunking%2C%20a%20simple%20yet%20effective%20recipe%20for%20improving%0Areinforcement%20learning%20%28RL%29%20algorithms%20for%20long-horizon%2C%20sparse-reward%20tasks.%0AOur%20recipe%20is%20designed%20for%20the%20offline-to-online%20RL%20setting%2C%20where%20the%20goal%20is%0Ato%20leverage%20an%20offline%20prior%20dataset%20to%20maximize%20the%20sample-efficiency%20of%0Aonline%20learning.%20Effective%20exploration%20and%20sample-efficient%20learning%20remain%0Acentral%20challenges%20in%20this%20setting%2C%20as%20it%20is%20not%20obvious%20how%20the%20offline%20data%0Ashould%20be%20utilized%20to%20acquire%20a%20good%20exploratory%20policy.%20Our%20key%20insight%20is%0Athat%20action%20chunking%2C%20a%20technique%20popularized%20in%20imitation%20learning%20where%0Asequences%20of%20future%20actions%20are%20predicted%20rather%20than%20a%20single%20action%20at%20each%0Atimestep%2C%20can%20be%20applied%20to%20temporal%20difference%20%28TD%29-based%20RL%20methods%20to%0Amitigate%20the%20exploration%20challenge.%20Q-chunking%20adopts%20action%20chunking%20by%0Adirectly%20running%20RL%20in%20a%20%27chunked%27%20action%20space%2C%20enabling%20the%20agent%20to%20%281%29%0Aleverage%20temporally%20consistent%20behaviors%20from%20offline%20data%20for%20more%20effective%0Aonline%20exploration%20and%20%282%29%20use%20unbiased%20%24n%24-step%20backups%20for%20more%20stable%20and%0Aefficient%20TD%20learning.%20Our%20experimental%20results%20demonstrate%20that%20Q-chunking%0Aexhibits%20strong%20offline%20performance%20and%20online%20sample%20efficiency%2C%20outperforming%0Aprior%20best%20offline-to-online%20methods%20on%20a%20range%20of%20long-horizon%2C%20sparse-reward%0Amanipulation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07969v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReinforcement%2520Learning%2520with%2520Action%2520Chunking%26entry.906535625%3DQiyang%2520Li%2520and%2520Zhiyuan%2520Zhou%2520and%2520Sergey%2520Levine%26entry.1292438233%3D%2520%2520We%2520present%2520Q-chunking%252C%2520a%2520simple%2520yet%2520effective%2520recipe%2520for%2520improving%250Areinforcement%2520learning%2520%2528RL%2529%2520algorithms%2520for%2520long-horizon%252C%2520sparse-reward%2520tasks.%250AOur%2520recipe%2520is%2520designed%2520for%2520the%2520offline-to-online%2520RL%2520setting%252C%2520where%2520the%2520goal%2520is%250Ato%2520leverage%2520an%2520offline%2520prior%2520dataset%2520to%2520maximize%2520the%2520sample-efficiency%2520of%250Aonline%2520learning.%2520Effective%2520exploration%2520and%2520sample-efficient%2520learning%2520remain%250Acentral%2520challenges%2520in%2520this%2520setting%252C%2520as%2520it%2520is%2520not%2520obvious%2520how%2520the%2520offline%2520data%250Ashould%2520be%2520utilized%2520to%2520acquire%2520a%2520good%2520exploratory%2520policy.%2520Our%2520key%2520insight%2520is%250Athat%2520action%2520chunking%252C%2520a%2520technique%2520popularized%2520in%2520imitation%2520learning%2520where%250Asequences%2520of%2520future%2520actions%2520are%2520predicted%2520rather%2520than%2520a%2520single%2520action%2520at%2520each%250Atimestep%252C%2520can%2520be%2520applied%2520to%2520temporal%2520difference%2520%2528TD%2529-based%2520RL%2520methods%2520to%250Amitigate%2520the%2520exploration%2520challenge.%2520Q-chunking%2520adopts%2520action%2520chunking%2520by%250Adirectly%2520running%2520RL%2520in%2520a%2520%2527chunked%2527%2520action%2520space%252C%2520enabling%2520the%2520agent%2520to%2520%25281%2529%250Aleverage%2520temporally%2520consistent%2520behaviors%2520from%2520offline%2520data%2520for%2520more%2520effective%250Aonline%2520exploration%2520and%2520%25282%2529%2520use%2520unbiased%2520%2524n%2524-step%2520backups%2520for%2520more%2520stable%2520and%250Aefficient%2520TD%2520learning.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520Q-chunking%250Aexhibits%2520strong%2520offline%2520performance%2520and%2520online%2520sample%2520efficiency%252C%2520outperforming%250Aprior%2520best%2520offline-to-online%2520methods%2520on%2520a%2520range%2520of%2520long-horizon%252C%2520sparse-reward%250Amanipulation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07969v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reinforcement%20Learning%20with%20Action%20Chunking&entry.906535625=Qiyang%20Li%20and%20Zhiyuan%20Zhou%20and%20Sergey%20Levine&entry.1292438233=%20%20We%20present%20Q-chunking%2C%20a%20simple%20yet%20effective%20recipe%20for%20improving%0Areinforcement%20learning%20%28RL%29%20algorithms%20for%20long-horizon%2C%20sparse-reward%20tasks.%0AOur%20recipe%20is%20designed%20for%20the%20offline-to-online%20RL%20setting%2C%20where%20the%20goal%20is%0Ato%20leverage%20an%20offline%20prior%20dataset%20to%20maximize%20the%20sample-efficiency%20of%0Aonline%20learning.%20Effective%20exploration%20and%20sample-efficient%20learning%20remain%0Acentral%20challenges%20in%20this%20setting%2C%20as%20it%20is%20not%20obvious%20how%20the%20offline%20data%0Ashould%20be%20utilized%20to%20acquire%20a%20good%20exploratory%20policy.%20Our%20key%20insight%20is%0Athat%20action%20chunking%2C%20a%20technique%20popularized%20in%20imitation%20learning%20where%0Asequences%20of%20future%20actions%20are%20predicted%20rather%20than%20a%20single%20action%20at%20each%0Atimestep%2C%20can%20be%20applied%20to%20temporal%20difference%20%28TD%29-based%20RL%20methods%20to%0Amitigate%20the%20exploration%20challenge.%20Q-chunking%20adopts%20action%20chunking%20by%0Adirectly%20running%20RL%20in%20a%20%27chunked%27%20action%20space%2C%20enabling%20the%20agent%20to%20%281%29%0Aleverage%20temporally%20consistent%20behaviors%20from%20offline%20data%20for%20more%20effective%0Aonline%20exploration%20and%20%282%29%20use%20unbiased%20%24n%24-step%20backups%20for%20more%20stable%20and%0Aefficient%20TD%20learning.%20Our%20experimental%20results%20demonstrate%20that%20Q-chunking%0Aexhibits%20strong%20offline%20performance%20and%20online%20sample%20efficiency%2C%20outperforming%0Aprior%20best%20offline-to-online%20methods%20on%20a%20range%20of%20long-horizon%2C%20sparse-reward%0Amanipulation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07969v2&entry.124074799=Read"},
{"title": "EXPO: Stable Reinforcement Learning with Expressive Policies", "author": "Perry Dong and Qiyang Li and Dorsa Sadigh and Chelsea Finn", "abstract": "  We study the problem of training and fine-tuning expressive policies with\nonline reinforcement learning (RL) given an offline dataset. Training\nexpressive policy classes with online RL present a unique challenge of stable\nvalue maximization. Unlike simpler Gaussian policies commonly used in online\nRL, expressive policies like diffusion and flow-matching policies are\nparameterized by a long denoising chain, which hinders stable gradient\npropagation from actions to policy parameters when optimizing against some\nvalue function. Our key insight is that we can address stable value\nmaximization by avoiding direct optimization over value with the expressive\npolicy and instead construct an on-the-fly RL policy to maximize Q-value. We\npropose Expressive Policy Optimization (EXPO), a sample-efficient online RL\nalgorithm that utilizes an on-the-fly policy to maximize value with two\nparameterized policies -- a larger expressive base policy trained with a stable\nimitation learning objective and a light-weight Gaussian edit policy that edits\nthe actions sampled from the base policy toward a higher value distribution.\nThe on-the-fly policy optimizes the actions from the base policy with the\nlearned edit policy and chooses the value maximizing action from the base and\nedited actions for both sampling and temporal-difference (TD) backup. Our\napproach yields up to 2-3x improvement in sample efficiency on average over\nprior methods both in the setting of fine-tuning a pretrained policy given\noffline data and in leveraging offline data to train online.\n", "link": "http://arxiv.org/abs/2507.07986v2", "date": "2025-07-15", "relevancy": 1.8515, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4844}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4677}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EXPO%3A%20Stable%20Reinforcement%20Learning%20with%20Expressive%20Policies&body=Title%3A%20EXPO%3A%20Stable%20Reinforcement%20Learning%20with%20Expressive%20Policies%0AAuthor%3A%20Perry%20Dong%20and%20Qiyang%20Li%20and%20Dorsa%20Sadigh%20and%20Chelsea%20Finn%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20training%20and%20fine-tuning%20expressive%20policies%20with%0Aonline%20reinforcement%20learning%20%28RL%29%20given%20an%20offline%20dataset.%20Training%0Aexpressive%20policy%20classes%20with%20online%20RL%20present%20a%20unique%20challenge%20of%20stable%0Avalue%20maximization.%20Unlike%20simpler%20Gaussian%20policies%20commonly%20used%20in%20online%0ARL%2C%20expressive%20policies%20like%20diffusion%20and%20flow-matching%20policies%20are%0Aparameterized%20by%20a%20long%20denoising%20chain%2C%20which%20hinders%20stable%20gradient%0Apropagation%20from%20actions%20to%20policy%20parameters%20when%20optimizing%20against%20some%0Avalue%20function.%20Our%20key%20insight%20is%20that%20we%20can%20address%20stable%20value%0Amaximization%20by%20avoiding%20direct%20optimization%20over%20value%20with%20the%20expressive%0Apolicy%20and%20instead%20construct%20an%20on-the-fly%20RL%20policy%20to%20maximize%20Q-value.%20We%0Apropose%20Expressive%20Policy%20Optimization%20%28EXPO%29%2C%20a%20sample-efficient%20online%20RL%0Aalgorithm%20that%20utilizes%20an%20on-the-fly%20policy%20to%20maximize%20value%20with%20two%0Aparameterized%20policies%20--%20a%20larger%20expressive%20base%20policy%20trained%20with%20a%20stable%0Aimitation%20learning%20objective%20and%20a%20light-weight%20Gaussian%20edit%20policy%20that%20edits%0Athe%20actions%20sampled%20from%20the%20base%20policy%20toward%20a%20higher%20value%20distribution.%0AThe%20on-the-fly%20policy%20optimizes%20the%20actions%20from%20the%20base%20policy%20with%20the%0Alearned%20edit%20policy%20and%20chooses%20the%20value%20maximizing%20action%20from%20the%20base%20and%0Aedited%20actions%20for%20both%20sampling%20and%20temporal-difference%20%28TD%29%20backup.%20Our%0Aapproach%20yields%20up%20to%202-3x%20improvement%20in%20sample%20efficiency%20on%20average%20over%0Aprior%20methods%20both%20in%20the%20setting%20of%20fine-tuning%20a%20pretrained%20policy%20given%0Aoffline%20data%20and%20in%20leveraging%20offline%20data%20to%20train%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.07986v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEXPO%253A%2520Stable%2520Reinforcement%2520Learning%2520with%2520Expressive%2520Policies%26entry.906535625%3DPerry%2520Dong%2520and%2520Qiyang%2520Li%2520and%2520Dorsa%2520Sadigh%2520and%2520Chelsea%2520Finn%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520problem%2520of%2520training%2520and%2520fine-tuning%2520expressive%2520policies%2520with%250Aonline%2520reinforcement%2520learning%2520%2528RL%2529%2520given%2520an%2520offline%2520dataset.%2520Training%250Aexpressive%2520policy%2520classes%2520with%2520online%2520RL%2520present%2520a%2520unique%2520challenge%2520of%2520stable%250Avalue%2520maximization.%2520Unlike%2520simpler%2520Gaussian%2520policies%2520commonly%2520used%2520in%2520online%250ARL%252C%2520expressive%2520policies%2520like%2520diffusion%2520and%2520flow-matching%2520policies%2520are%250Aparameterized%2520by%2520a%2520long%2520denoising%2520chain%252C%2520which%2520hinders%2520stable%2520gradient%250Apropagation%2520from%2520actions%2520to%2520policy%2520parameters%2520when%2520optimizing%2520against%2520some%250Avalue%2520function.%2520Our%2520key%2520insight%2520is%2520that%2520we%2520can%2520address%2520stable%2520value%250Amaximization%2520by%2520avoiding%2520direct%2520optimization%2520over%2520value%2520with%2520the%2520expressive%250Apolicy%2520and%2520instead%2520construct%2520an%2520on-the-fly%2520RL%2520policy%2520to%2520maximize%2520Q-value.%2520We%250Apropose%2520Expressive%2520Policy%2520Optimization%2520%2528EXPO%2529%252C%2520a%2520sample-efficient%2520online%2520RL%250Aalgorithm%2520that%2520utilizes%2520an%2520on-the-fly%2520policy%2520to%2520maximize%2520value%2520with%2520two%250Aparameterized%2520policies%2520--%2520a%2520larger%2520expressive%2520base%2520policy%2520trained%2520with%2520a%2520stable%250Aimitation%2520learning%2520objective%2520and%2520a%2520light-weight%2520Gaussian%2520edit%2520policy%2520that%2520edits%250Athe%2520actions%2520sampled%2520from%2520the%2520base%2520policy%2520toward%2520a%2520higher%2520value%2520distribution.%250AThe%2520on-the-fly%2520policy%2520optimizes%2520the%2520actions%2520from%2520the%2520base%2520policy%2520with%2520the%250Alearned%2520edit%2520policy%2520and%2520chooses%2520the%2520value%2520maximizing%2520action%2520from%2520the%2520base%2520and%250Aedited%2520actions%2520for%2520both%2520sampling%2520and%2520temporal-difference%2520%2528TD%2529%2520backup.%2520Our%250Aapproach%2520yields%2520up%2520to%25202-3x%2520improvement%2520in%2520sample%2520efficiency%2520on%2520average%2520over%250Aprior%2520methods%2520both%2520in%2520the%2520setting%2520of%2520fine-tuning%2520a%2520pretrained%2520policy%2520given%250Aoffline%2520data%2520and%2520in%2520leveraging%2520offline%2520data%2520to%2520train%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.07986v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EXPO%3A%20Stable%20Reinforcement%20Learning%20with%20Expressive%20Policies&entry.906535625=Perry%20Dong%20and%20Qiyang%20Li%20and%20Dorsa%20Sadigh%20and%20Chelsea%20Finn&entry.1292438233=%20%20We%20study%20the%20problem%20of%20training%20and%20fine-tuning%20expressive%20policies%20with%0Aonline%20reinforcement%20learning%20%28RL%29%20given%20an%20offline%20dataset.%20Training%0Aexpressive%20policy%20classes%20with%20online%20RL%20present%20a%20unique%20challenge%20of%20stable%0Avalue%20maximization.%20Unlike%20simpler%20Gaussian%20policies%20commonly%20used%20in%20online%0ARL%2C%20expressive%20policies%20like%20diffusion%20and%20flow-matching%20policies%20are%0Aparameterized%20by%20a%20long%20denoising%20chain%2C%20which%20hinders%20stable%20gradient%0Apropagation%20from%20actions%20to%20policy%20parameters%20when%20optimizing%20against%20some%0Avalue%20function.%20Our%20key%20insight%20is%20that%20we%20can%20address%20stable%20value%0Amaximization%20by%20avoiding%20direct%20optimization%20over%20value%20with%20the%20expressive%0Apolicy%20and%20instead%20construct%20an%20on-the-fly%20RL%20policy%20to%20maximize%20Q-value.%20We%0Apropose%20Expressive%20Policy%20Optimization%20%28EXPO%29%2C%20a%20sample-efficient%20online%20RL%0Aalgorithm%20that%20utilizes%20an%20on-the-fly%20policy%20to%20maximize%20value%20with%20two%0Aparameterized%20policies%20--%20a%20larger%20expressive%20base%20policy%20trained%20with%20a%20stable%0Aimitation%20learning%20objective%20and%20a%20light-weight%20Gaussian%20edit%20policy%20that%20edits%0Athe%20actions%20sampled%20from%20the%20base%20policy%20toward%20a%20higher%20value%20distribution.%0AThe%20on-the-fly%20policy%20optimizes%20the%20actions%20from%20the%20base%20policy%20with%20the%0Alearned%20edit%20policy%20and%20chooses%20the%20value%20maximizing%20action%20from%20the%20base%20and%0Aedited%20actions%20for%20both%20sampling%20and%20temporal-difference%20%28TD%29%20backup.%20Our%0Aapproach%20yields%20up%20to%202-3x%20improvement%20in%20sample%20efficiency%20on%20average%20over%0Aprior%20methods%20both%20in%20the%20setting%20of%20fine-tuning%20a%20pretrained%20policy%20given%0Aoffline%20data%20and%20in%20leveraging%20offline%20data%20to%20train%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.07986v2&entry.124074799=Read"},
{"title": "Elk: Exploring the Efficiency of Inter-core Connected AI Chips with Deep\n  Learning Compiler Techniques", "author": "Yiqi Liu and Yuqi Xue and Noelle Crawford and Jilong Xue and Jian Huang", "abstract": "  To meet the increasing demand of deep learning (DL) models, AI chips are\nemploying both off-chip memory (e.g., HBM) and high-bandwidth low-latency\ninterconnect for direct inter-core data exchange. However, it is not easy to\nexplore the efficiency of these inter-core connected AI (ICCA) chips, due to a\nfundamental tussle among compute (per-core execution), communication\n(inter-core data exchange), and I/O (off-chip data access).\n  In this paper, we develop Elk, a DL compiler framework to maximize the\nefficiency of ICCA chips by jointly trading off all the three performance\nfactors discussed above. Elk structures these performance factors into\nconfigurable parameters and forms a global trade-off space in the DL compiler.\nTo systematically explore this space and maximize overall efficiency, Elk\nemploys a new inductive operator scheduling policy and a cost-aware on-chip\nmemory allocation algorithm. It generates globally optimized execution plans\nthat best overlap off-chip data loading and on-chip execution. To examine the\nefficiency of Elk, we build a full-fledged emulator based on a real ICCA chip\nIPU-POD4, and an ICCA chip simulator for sensitivity analysis with different\ninterconnect network topologies. Elk achieves 94% of the ideal roofline\nperformance of ICCA chips on average, showing the benefits of supporting large\nDL models on ICCA chips. We also show Elk's capability of enabling architecture\ndesign space exploration for new ICCA chip development.\n", "link": "http://arxiv.org/abs/2507.11506v1", "date": "2025-07-15", "relevancy": 1.8388, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4758}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Elk%3A%20Exploring%20the%20Efficiency%20of%20Inter-core%20Connected%20AI%20Chips%20with%20Deep%0A%20%20Learning%20Compiler%20Techniques&body=Title%3A%20Elk%3A%20Exploring%20the%20Efficiency%20of%20Inter-core%20Connected%20AI%20Chips%20with%20Deep%0A%20%20Learning%20Compiler%20Techniques%0AAuthor%3A%20Yiqi%20Liu%20and%20Yuqi%20Xue%20and%20Noelle%20Crawford%20and%20Jilong%20Xue%20and%20Jian%20Huang%0AAbstract%3A%20%20%20To%20meet%20the%20increasing%20demand%20of%20deep%20learning%20%28DL%29%20models%2C%20AI%20chips%20are%0Aemploying%20both%20off-chip%20memory%20%28e.g.%2C%20HBM%29%20and%20high-bandwidth%20low-latency%0Ainterconnect%20for%20direct%20inter-core%20data%20exchange.%20However%2C%20it%20is%20not%20easy%20to%0Aexplore%20the%20efficiency%20of%20these%20inter-core%20connected%20AI%20%28ICCA%29%20chips%2C%20due%20to%20a%0Afundamental%20tussle%20among%20compute%20%28per-core%20execution%29%2C%20communication%0A%28inter-core%20data%20exchange%29%2C%20and%20I/O%20%28off-chip%20data%20access%29.%0A%20%20In%20this%20paper%2C%20we%20develop%20Elk%2C%20a%20DL%20compiler%20framework%20to%20maximize%20the%0Aefficiency%20of%20ICCA%20chips%20by%20jointly%20trading%20off%20all%20the%20three%20performance%0Afactors%20discussed%20above.%20Elk%20structures%20these%20performance%20factors%20into%0Aconfigurable%20parameters%20and%20forms%20a%20global%20trade-off%20space%20in%20the%20DL%20compiler.%0ATo%20systematically%20explore%20this%20space%20and%20maximize%20overall%20efficiency%2C%20Elk%0Aemploys%20a%20new%20inductive%20operator%20scheduling%20policy%20and%20a%20cost-aware%20on-chip%0Amemory%20allocation%20algorithm.%20It%20generates%20globally%20optimized%20execution%20plans%0Athat%20best%20overlap%20off-chip%20data%20loading%20and%20on-chip%20execution.%20To%20examine%20the%0Aefficiency%20of%20Elk%2C%20we%20build%20a%20full-fledged%20emulator%20based%20on%20a%20real%20ICCA%20chip%0AIPU-POD4%2C%20and%20an%20ICCA%20chip%20simulator%20for%20sensitivity%20analysis%20with%20different%0Ainterconnect%20network%20topologies.%20Elk%20achieves%2094%25%20of%20the%20ideal%20roofline%0Aperformance%20of%20ICCA%20chips%20on%20average%2C%20showing%20the%20benefits%20of%20supporting%20large%0ADL%20models%20on%20ICCA%20chips.%20We%20also%20show%20Elk%27s%20capability%20of%20enabling%20architecture%0Adesign%20space%20exploration%20for%20new%20ICCA%20chip%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DElk%253A%2520Exploring%2520the%2520Efficiency%2520of%2520Inter-core%2520Connected%2520AI%2520Chips%2520with%2520Deep%250A%2520%2520Learning%2520Compiler%2520Techniques%26entry.906535625%3DYiqi%2520Liu%2520and%2520Yuqi%2520Xue%2520and%2520Noelle%2520Crawford%2520and%2520Jilong%2520Xue%2520and%2520Jian%2520Huang%26entry.1292438233%3D%2520%2520To%2520meet%2520the%2520increasing%2520demand%2520of%2520deep%2520learning%2520%2528DL%2529%2520models%252C%2520AI%2520chips%2520are%250Aemploying%2520both%2520off-chip%2520memory%2520%2528e.g.%252C%2520HBM%2529%2520and%2520high-bandwidth%2520low-latency%250Ainterconnect%2520for%2520direct%2520inter-core%2520data%2520exchange.%2520However%252C%2520it%2520is%2520not%2520easy%2520to%250Aexplore%2520the%2520efficiency%2520of%2520these%2520inter-core%2520connected%2520AI%2520%2528ICCA%2529%2520chips%252C%2520due%2520to%2520a%250Afundamental%2520tussle%2520among%2520compute%2520%2528per-core%2520execution%2529%252C%2520communication%250A%2528inter-core%2520data%2520exchange%2529%252C%2520and%2520I/O%2520%2528off-chip%2520data%2520access%2529.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520develop%2520Elk%252C%2520a%2520DL%2520compiler%2520framework%2520to%2520maximize%2520the%250Aefficiency%2520of%2520ICCA%2520chips%2520by%2520jointly%2520trading%2520off%2520all%2520the%2520three%2520performance%250Afactors%2520discussed%2520above.%2520Elk%2520structures%2520these%2520performance%2520factors%2520into%250Aconfigurable%2520parameters%2520and%2520forms%2520a%2520global%2520trade-off%2520space%2520in%2520the%2520DL%2520compiler.%250ATo%2520systematically%2520explore%2520this%2520space%2520and%2520maximize%2520overall%2520efficiency%252C%2520Elk%250Aemploys%2520a%2520new%2520inductive%2520operator%2520scheduling%2520policy%2520and%2520a%2520cost-aware%2520on-chip%250Amemory%2520allocation%2520algorithm.%2520It%2520generates%2520globally%2520optimized%2520execution%2520plans%250Athat%2520best%2520overlap%2520off-chip%2520data%2520loading%2520and%2520on-chip%2520execution.%2520To%2520examine%2520the%250Aefficiency%2520of%2520Elk%252C%2520we%2520build%2520a%2520full-fledged%2520emulator%2520based%2520on%2520a%2520real%2520ICCA%2520chip%250AIPU-POD4%252C%2520and%2520an%2520ICCA%2520chip%2520simulator%2520for%2520sensitivity%2520analysis%2520with%2520different%250Ainterconnect%2520network%2520topologies.%2520Elk%2520achieves%252094%2525%2520of%2520the%2520ideal%2520roofline%250Aperformance%2520of%2520ICCA%2520chips%2520on%2520average%252C%2520showing%2520the%2520benefits%2520of%2520supporting%2520large%250ADL%2520models%2520on%2520ICCA%2520chips.%2520We%2520also%2520show%2520Elk%2527s%2520capability%2520of%2520enabling%2520architecture%250Adesign%2520space%2520exploration%2520for%2520new%2520ICCA%2520chip%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Elk%3A%20Exploring%20the%20Efficiency%20of%20Inter-core%20Connected%20AI%20Chips%20with%20Deep%0A%20%20Learning%20Compiler%20Techniques&entry.906535625=Yiqi%20Liu%20and%20Yuqi%20Xue%20and%20Noelle%20Crawford%20and%20Jilong%20Xue%20and%20Jian%20Huang&entry.1292438233=%20%20To%20meet%20the%20increasing%20demand%20of%20deep%20learning%20%28DL%29%20models%2C%20AI%20chips%20are%0Aemploying%20both%20off-chip%20memory%20%28e.g.%2C%20HBM%29%20and%20high-bandwidth%20low-latency%0Ainterconnect%20for%20direct%20inter-core%20data%20exchange.%20However%2C%20it%20is%20not%20easy%20to%0Aexplore%20the%20efficiency%20of%20these%20inter-core%20connected%20AI%20%28ICCA%29%20chips%2C%20due%20to%20a%0Afundamental%20tussle%20among%20compute%20%28per-core%20execution%29%2C%20communication%0A%28inter-core%20data%20exchange%29%2C%20and%20I/O%20%28off-chip%20data%20access%29.%0A%20%20In%20this%20paper%2C%20we%20develop%20Elk%2C%20a%20DL%20compiler%20framework%20to%20maximize%20the%0Aefficiency%20of%20ICCA%20chips%20by%20jointly%20trading%20off%20all%20the%20three%20performance%0Afactors%20discussed%20above.%20Elk%20structures%20these%20performance%20factors%20into%0Aconfigurable%20parameters%20and%20forms%20a%20global%20trade-off%20space%20in%20the%20DL%20compiler.%0ATo%20systematically%20explore%20this%20space%20and%20maximize%20overall%20efficiency%2C%20Elk%0Aemploys%20a%20new%20inductive%20operator%20scheduling%20policy%20and%20a%20cost-aware%20on-chip%0Amemory%20allocation%20algorithm.%20It%20generates%20globally%20optimized%20execution%20plans%0Athat%20best%20overlap%20off-chip%20data%20loading%20and%20on-chip%20execution.%20To%20examine%20the%0Aefficiency%20of%20Elk%2C%20we%20build%20a%20full-fledged%20emulator%20based%20on%20a%20real%20ICCA%20chip%0AIPU-POD4%2C%20and%20an%20ICCA%20chip%20simulator%20for%20sensitivity%20analysis%20with%20different%0Ainterconnect%20network%20topologies.%20Elk%20achieves%2094%25%20of%20the%20ideal%20roofline%0Aperformance%20of%20ICCA%20chips%20on%20average%2C%20showing%20the%20benefits%20of%20supporting%20large%0ADL%20models%20on%20ICCA%20chips.%20We%20also%20show%20Elk%27s%20capability%20of%20enabling%20architecture%0Adesign%20space%20exploration%20for%20new%20ICCA%20chip%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11506v1&entry.124074799=Read"},
{"title": "ComFairGNN: Community Fair Graph Neural Network", "author": "Yonas Sium and Qi Li", "abstract": "  Graph Neural Networks (GNNs) have become the leading approach for addressing\ngraph analytical problems in various real-world scenarios. However, GNNs may\nproduce biased predictions against certain demographic subgroups due to node\nattributes and neighbors surrounding a node. Most current research on GNN\nfairness focuses predominantly on debiasing GNNs using oversimplified fairness\nevaluation metrics, which can give a misleading impression of fairness.\nUnderstanding the potential evaluation paradoxes due to the complicated nature\nof the graph structure is crucial for developing effective GNN debiasing\nmechanisms. In this paper, we examine the effectiveness of current GNN\ndebiasing methods in terms of unfairness evaluation. Specifically, we introduce\na community-level strategy to measure bias in GNNs and evaluate debiasing\nmethods at this level. Further, We introduce ComFairGNN, a novel framework\ndesigned to mitigate community-level bias in GNNs. Our approach employs a\nlearnable coreset-based debiasing function that addresses bias arising from\ndiverse local neighborhood distributions during GNNs neighborhood aggregation.\nComprehensive evaluations on three benchmark datasets demonstrate our model's\neffectiveness in both accuracy and fairness metrics.\n", "link": "http://arxiv.org/abs/2411.04371v3", "date": "2025-07-15", "relevancy": 1.83, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.483}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4689}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4275}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ComFairGNN%3A%20Community%20Fair%20Graph%20Neural%20Network&body=Title%3A%20ComFairGNN%3A%20Community%20Fair%20Graph%20Neural%20Network%0AAuthor%3A%20Yonas%20Sium%20and%20Qi%20Li%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20the%20leading%20approach%20for%20addressing%0Agraph%20analytical%20problems%20in%20various%20real-world%20scenarios.%20However%2C%20GNNs%20may%0Aproduce%20biased%20predictions%20against%20certain%20demographic%20subgroups%20due%20to%20node%0Aattributes%20and%20neighbors%20surrounding%20a%20node.%20Most%20current%20research%20on%20GNN%0Afairness%20focuses%20predominantly%20on%20debiasing%20GNNs%20using%20oversimplified%20fairness%0Aevaluation%20metrics%2C%20which%20can%20give%20a%20misleading%20impression%20of%20fairness.%0AUnderstanding%20the%20potential%20evaluation%20paradoxes%20due%20to%20the%20complicated%20nature%0Aof%20the%20graph%20structure%20is%20crucial%20for%20developing%20effective%20GNN%20debiasing%0Amechanisms.%20In%20this%20paper%2C%20we%20examine%20the%20effectiveness%20of%20current%20GNN%0Adebiasing%20methods%20in%20terms%20of%20unfairness%20evaluation.%20Specifically%2C%20we%20introduce%0Aa%20community-level%20strategy%20to%20measure%20bias%20in%20GNNs%20and%20evaluate%20debiasing%0Amethods%20at%20this%20level.%20Further%2C%20We%20introduce%20ComFairGNN%2C%20a%20novel%20framework%0Adesigned%20to%20mitigate%20community-level%20bias%20in%20GNNs.%20Our%20approach%20employs%20a%0Alearnable%20coreset-based%20debiasing%20function%20that%20addresses%20bias%20arising%20from%0Adiverse%20local%20neighborhood%20distributions%20during%20GNNs%20neighborhood%20aggregation.%0AComprehensive%20evaluations%20on%20three%20benchmark%20datasets%20demonstrate%20our%20model%27s%0Aeffectiveness%20in%20both%20accuracy%20and%20fairness%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.04371v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComFairGNN%253A%2520Community%2520Fair%2520Graph%2520Neural%2520Network%26entry.906535625%3DYonas%2520Sium%2520and%2520Qi%2520Li%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520become%2520the%2520leading%2520approach%2520for%2520addressing%250Agraph%2520analytical%2520problems%2520in%2520various%2520real-world%2520scenarios.%2520However%252C%2520GNNs%2520may%250Aproduce%2520biased%2520predictions%2520against%2520certain%2520demographic%2520subgroups%2520due%2520to%2520node%250Aattributes%2520and%2520neighbors%2520surrounding%2520a%2520node.%2520Most%2520current%2520research%2520on%2520GNN%250Afairness%2520focuses%2520predominantly%2520on%2520debiasing%2520GNNs%2520using%2520oversimplified%2520fairness%250Aevaluation%2520metrics%252C%2520which%2520can%2520give%2520a%2520misleading%2520impression%2520of%2520fairness.%250AUnderstanding%2520the%2520potential%2520evaluation%2520paradoxes%2520due%2520to%2520the%2520complicated%2520nature%250Aof%2520the%2520graph%2520structure%2520is%2520crucial%2520for%2520developing%2520effective%2520GNN%2520debiasing%250Amechanisms.%2520In%2520this%2520paper%252C%2520we%2520examine%2520the%2520effectiveness%2520of%2520current%2520GNN%250Adebiasing%2520methods%2520in%2520terms%2520of%2520unfairness%2520evaluation.%2520Specifically%252C%2520we%2520introduce%250Aa%2520community-level%2520strategy%2520to%2520measure%2520bias%2520in%2520GNNs%2520and%2520evaluate%2520debiasing%250Amethods%2520at%2520this%2520level.%2520Further%252C%2520We%2520introduce%2520ComFairGNN%252C%2520a%2520novel%2520framework%250Adesigned%2520to%2520mitigate%2520community-level%2520bias%2520in%2520GNNs.%2520Our%2520approach%2520employs%2520a%250Alearnable%2520coreset-based%2520debiasing%2520function%2520that%2520addresses%2520bias%2520arising%2520from%250Adiverse%2520local%2520neighborhood%2520distributions%2520during%2520GNNs%2520neighborhood%2520aggregation.%250AComprehensive%2520evaluations%2520on%2520three%2520benchmark%2520datasets%2520demonstrate%2520our%2520model%2527s%250Aeffectiveness%2520in%2520both%2520accuracy%2520and%2520fairness%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.04371v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ComFairGNN%3A%20Community%20Fair%20Graph%20Neural%20Network&entry.906535625=Yonas%20Sium%20and%20Qi%20Li&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20become%20the%20leading%20approach%20for%20addressing%0Agraph%20analytical%20problems%20in%20various%20real-world%20scenarios.%20However%2C%20GNNs%20may%0Aproduce%20biased%20predictions%20against%20certain%20demographic%20subgroups%20due%20to%20node%0Aattributes%20and%20neighbors%20surrounding%20a%20node.%20Most%20current%20research%20on%20GNN%0Afairness%20focuses%20predominantly%20on%20debiasing%20GNNs%20using%20oversimplified%20fairness%0Aevaluation%20metrics%2C%20which%20can%20give%20a%20misleading%20impression%20of%20fairness.%0AUnderstanding%20the%20potential%20evaluation%20paradoxes%20due%20to%20the%20complicated%20nature%0Aof%20the%20graph%20structure%20is%20crucial%20for%20developing%20effective%20GNN%20debiasing%0Amechanisms.%20In%20this%20paper%2C%20we%20examine%20the%20effectiveness%20of%20current%20GNN%0Adebiasing%20methods%20in%20terms%20of%20unfairness%20evaluation.%20Specifically%2C%20we%20introduce%0Aa%20community-level%20strategy%20to%20measure%20bias%20in%20GNNs%20and%20evaluate%20debiasing%0Amethods%20at%20this%20level.%20Further%2C%20We%20introduce%20ComFairGNN%2C%20a%20novel%20framework%0Adesigned%20to%20mitigate%20community-level%20bias%20in%20GNNs.%20Our%20approach%20employs%20a%0Alearnable%20coreset-based%20debiasing%20function%20that%20addresses%20bias%20arising%20from%0Adiverse%20local%20neighborhood%20distributions%20during%20GNNs%20neighborhood%20aggregation.%0AComprehensive%20evaluations%20on%20three%20benchmark%20datasets%20demonstrate%20our%20model%27s%0Aeffectiveness%20in%20both%20accuracy%20and%20fairness%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.04371v3&entry.124074799=Read"},
{"title": "Large Language Models Engineer Too Many Simple Features For Tabular Data", "author": "Jaris K\u00fcken and Lennart Purucker and Frank Hutter", "abstract": "  Tabular machine learning problems often require time-consuming and\nlabor-intensive feature engineering. Recent efforts have focused on using large\nlanguage models (LLMs) to capitalize on their potential domain knowledge. At\nthe same time, researchers have observed ethically concerning negative biases\nin other LLM-related use cases, such as text generation. These developments\nmotivated us to investigate whether LLMs exhibit a bias that negatively impacts\nthe performance of feature engineering. While not ethically concerning, such a\nbias could hinder practitioners from fully utilizing LLMs for automated data\nscience. Therefore, we propose a method to detect potential biases by detecting\nanomalies in the frequency of operators (e.g., adding two features) suggested\nby LLMs when engineering new features. Our experiments evaluate the bias of\nfour LLMs, two big frontier and two small open-source models, across 27 tabular\ndatasets. Our results indicate that LLMs are biased toward simple operators,\nsuch as addition, and can fail to utilize more complex operators, such as\ngrouping followed by aggregations. Furthermore, the bias can negatively impact\nthe predictive performance when using LLM-generated features. Our results call\nfor mitigating bias when using LLMs for feature engineering.\n", "link": "http://arxiv.org/abs/2410.17787v2", "date": "2025-07-15", "relevancy": 1.8244, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4648}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Engineer%20Too%20Many%20Simple%20Features%20For%20Tabular%20Data&body=Title%3A%20Large%20Language%20Models%20Engineer%20Too%20Many%20Simple%20Features%20For%20Tabular%20Data%0AAuthor%3A%20Jaris%20K%C3%BCken%20and%20Lennart%20Purucker%20and%20Frank%20Hutter%0AAbstract%3A%20%20%20Tabular%20machine%20learning%20problems%20often%20require%20time-consuming%20and%0Alabor-intensive%20feature%20engineering.%20Recent%20efforts%20have%20focused%20on%20using%20large%0Alanguage%20models%20%28LLMs%29%20to%20capitalize%20on%20their%20potential%20domain%20knowledge.%20At%0Athe%20same%20time%2C%20researchers%20have%20observed%20ethically%20concerning%20negative%20biases%0Ain%20other%20LLM-related%20use%20cases%2C%20such%20as%20text%20generation.%20These%20developments%0Amotivated%20us%20to%20investigate%20whether%20LLMs%20exhibit%20a%20bias%20that%20negatively%20impacts%0Athe%20performance%20of%20feature%20engineering.%20While%20not%20ethically%20concerning%2C%20such%20a%0Abias%20could%20hinder%20practitioners%20from%20fully%20utilizing%20LLMs%20for%20automated%20data%0Ascience.%20Therefore%2C%20we%20propose%20a%20method%20to%20detect%20potential%20biases%20by%20detecting%0Aanomalies%20in%20the%20frequency%20of%20operators%20%28e.g.%2C%20adding%20two%20features%29%20suggested%0Aby%20LLMs%20when%20engineering%20new%20features.%20Our%20experiments%20evaluate%20the%20bias%20of%0Afour%20LLMs%2C%20two%20big%20frontier%20and%20two%20small%20open-source%20models%2C%20across%2027%20tabular%0Adatasets.%20Our%20results%20indicate%20that%20LLMs%20are%20biased%20toward%20simple%20operators%2C%0Asuch%20as%20addition%2C%20and%20can%20fail%20to%20utilize%20more%20complex%20operators%2C%20such%20as%0Agrouping%20followed%20by%20aggregations.%20Furthermore%2C%20the%20bias%20can%20negatively%20impact%0Athe%20predictive%20performance%20when%20using%20LLM-generated%20features.%20Our%20results%20call%0Afor%20mitigating%20bias%20when%20using%20LLMs%20for%20feature%20engineering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17787v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Engineer%2520Too%2520Many%2520Simple%2520Features%2520For%2520Tabular%2520Data%26entry.906535625%3DJaris%2520K%25C3%25BCken%2520and%2520Lennart%2520Purucker%2520and%2520Frank%2520Hutter%26entry.1292438233%3D%2520%2520Tabular%2520machine%2520learning%2520problems%2520often%2520require%2520time-consuming%2520and%250Alabor-intensive%2520feature%2520engineering.%2520Recent%2520efforts%2520have%2520focused%2520on%2520using%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520to%2520capitalize%2520on%2520their%2520potential%2520domain%2520knowledge.%2520At%250Athe%2520same%2520time%252C%2520researchers%2520have%2520observed%2520ethically%2520concerning%2520negative%2520biases%250Ain%2520other%2520LLM-related%2520use%2520cases%252C%2520such%2520as%2520text%2520generation.%2520These%2520developments%250Amotivated%2520us%2520to%2520investigate%2520whether%2520LLMs%2520exhibit%2520a%2520bias%2520that%2520negatively%2520impacts%250Athe%2520performance%2520of%2520feature%2520engineering.%2520While%2520not%2520ethically%2520concerning%252C%2520such%2520a%250Abias%2520could%2520hinder%2520practitioners%2520from%2520fully%2520utilizing%2520LLMs%2520for%2520automated%2520data%250Ascience.%2520Therefore%252C%2520we%2520propose%2520a%2520method%2520to%2520detect%2520potential%2520biases%2520by%2520detecting%250Aanomalies%2520in%2520the%2520frequency%2520of%2520operators%2520%2528e.g.%252C%2520adding%2520two%2520features%2529%2520suggested%250Aby%2520LLMs%2520when%2520engineering%2520new%2520features.%2520Our%2520experiments%2520evaluate%2520the%2520bias%2520of%250Afour%2520LLMs%252C%2520two%2520big%2520frontier%2520and%2520two%2520small%2520open-source%2520models%252C%2520across%252027%2520tabular%250Adatasets.%2520Our%2520results%2520indicate%2520that%2520LLMs%2520are%2520biased%2520toward%2520simple%2520operators%252C%250Asuch%2520as%2520addition%252C%2520and%2520can%2520fail%2520to%2520utilize%2520more%2520complex%2520operators%252C%2520such%2520as%250Agrouping%2520followed%2520by%2520aggregations.%2520Furthermore%252C%2520the%2520bias%2520can%2520negatively%2520impact%250Athe%2520predictive%2520performance%2520when%2520using%2520LLM-generated%2520features.%2520Our%2520results%2520call%250Afor%2520mitigating%2520bias%2520when%2520using%2520LLMs%2520for%2520feature%2520engineering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17787v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Engineer%20Too%20Many%20Simple%20Features%20For%20Tabular%20Data&entry.906535625=Jaris%20K%C3%BCken%20and%20Lennart%20Purucker%20and%20Frank%20Hutter&entry.1292438233=%20%20Tabular%20machine%20learning%20problems%20often%20require%20time-consuming%20and%0Alabor-intensive%20feature%20engineering.%20Recent%20efforts%20have%20focused%20on%20using%20large%0Alanguage%20models%20%28LLMs%29%20to%20capitalize%20on%20their%20potential%20domain%20knowledge.%20At%0Athe%20same%20time%2C%20researchers%20have%20observed%20ethically%20concerning%20negative%20biases%0Ain%20other%20LLM-related%20use%20cases%2C%20such%20as%20text%20generation.%20These%20developments%0Amotivated%20us%20to%20investigate%20whether%20LLMs%20exhibit%20a%20bias%20that%20negatively%20impacts%0Athe%20performance%20of%20feature%20engineering.%20While%20not%20ethically%20concerning%2C%20such%20a%0Abias%20could%20hinder%20practitioners%20from%20fully%20utilizing%20LLMs%20for%20automated%20data%0Ascience.%20Therefore%2C%20we%20propose%20a%20method%20to%20detect%20potential%20biases%20by%20detecting%0Aanomalies%20in%20the%20frequency%20of%20operators%20%28e.g.%2C%20adding%20two%20features%29%20suggested%0Aby%20LLMs%20when%20engineering%20new%20features.%20Our%20experiments%20evaluate%20the%20bias%20of%0Afour%20LLMs%2C%20two%20big%20frontier%20and%20two%20small%20open-source%20models%2C%20across%2027%20tabular%0Adatasets.%20Our%20results%20indicate%20that%20LLMs%20are%20biased%20toward%20simple%20operators%2C%0Asuch%20as%20addition%2C%20and%20can%20fail%20to%20utilize%20more%20complex%20operators%2C%20such%20as%0Agrouping%20followed%20by%20aggregations.%20Furthermore%2C%20the%20bias%20can%20negatively%20impact%0Athe%20predictive%20performance%20when%20using%20LLM-generated%20features.%20Our%20results%20call%0Afor%20mitigating%20bias%20when%20using%20LLMs%20for%20feature%20engineering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17787v2&entry.124074799=Read"},
{"title": "Hi Robot: Open-Ended Instruction Following with Hierarchical\n  Vision-Language-Action Models", "author": "Lucy Xiaoyang Shi and Brian Ichter and Michael Equi and Liyiming Ke and Karl Pertsch and Quan Vuong and James Tanner and Anna Walling and Haohuan Wang and Niccolo Fusai and Adrian Li-Bell and Danny Driess and Lachy Groom and Sergey Levine and Chelsea Finn", "abstract": "  Generalist robots that can perform a range of different tasks in open-world\nsettings must be able to not only reason about the steps needed to accomplish\ntheir goals, but also process complex instructions, prompts, and even feedback\nduring task execution. Intricate instructions (e.g., \"Could you make me a\nvegetarian sandwich?\" or \"I don't like that one\") require not just the ability\nto physically perform the individual steps, but the ability to situate complex\ncommands and feedback in the physical world. In this work, we describe a system\nthat uses vision-language models in a hierarchical structure, first reasoning\nover complex prompts and user feedback to deduce the most appropriate next step\nto fulfill the task, and then performing that step with low-level actions. In\ncontrast to direct instruction following methods that can fulfill simple\ncommands (\"pick up the cup\"), our system can reason through complex prompts and\nincorporate situated feedback during task execution (\"that's not trash\"). We\nevaluate our system across three robotic platforms, including single-arm,\ndual-arm, and dual-arm mobile robots, demonstrating its ability to handle tasks\nsuch as cleaning messy tables, making sandwiches, and grocery shopping. Videos\nare available at https://www.pi.website/research/hirobot\n", "link": "http://arxiv.org/abs/2502.19417v2", "date": "2025-07-15", "relevancy": 1.7895, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6476}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5948}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hi%20Robot%3A%20Open-Ended%20Instruction%20Following%20with%20Hierarchical%0A%20%20Vision-Language-Action%20Models&body=Title%3A%20Hi%20Robot%3A%20Open-Ended%20Instruction%20Following%20with%20Hierarchical%0A%20%20Vision-Language-Action%20Models%0AAuthor%3A%20Lucy%20Xiaoyang%20Shi%20and%20Brian%20Ichter%20and%20Michael%20Equi%20and%20Liyiming%20Ke%20and%20Karl%20Pertsch%20and%20Quan%20Vuong%20and%20James%20Tanner%20and%20Anna%20Walling%20and%20Haohuan%20Wang%20and%20Niccolo%20Fusai%20and%20Adrian%20Li-Bell%20and%20Danny%20Driess%20and%20Lachy%20Groom%20and%20Sergey%20Levine%20and%20Chelsea%20Finn%0AAbstract%3A%20%20%20Generalist%20robots%20that%20can%20perform%20a%20range%20of%20different%20tasks%20in%20open-world%0Asettings%20must%20be%20able%20to%20not%20only%20reason%20about%20the%20steps%20needed%20to%20accomplish%0Atheir%20goals%2C%20but%20also%20process%20complex%20instructions%2C%20prompts%2C%20and%20even%20feedback%0Aduring%20task%20execution.%20Intricate%20instructions%20%28e.g.%2C%20%22Could%20you%20make%20me%20a%0Avegetarian%20sandwich%3F%22%20or%20%22I%20don%27t%20like%20that%20one%22%29%20require%20not%20just%20the%20ability%0Ato%20physically%20perform%20the%20individual%20steps%2C%20but%20the%20ability%20to%20situate%20complex%0Acommands%20and%20feedback%20in%20the%20physical%20world.%20In%20this%20work%2C%20we%20describe%20a%20system%0Athat%20uses%20vision-language%20models%20in%20a%20hierarchical%20structure%2C%20first%20reasoning%0Aover%20complex%20prompts%20and%20user%20feedback%20to%20deduce%20the%20most%20appropriate%20next%20step%0Ato%20fulfill%20the%20task%2C%20and%20then%20performing%20that%20step%20with%20low-level%20actions.%20In%0Acontrast%20to%20direct%20instruction%20following%20methods%20that%20can%20fulfill%20simple%0Acommands%20%28%22pick%20up%20the%20cup%22%29%2C%20our%20system%20can%20reason%20through%20complex%20prompts%20and%0Aincorporate%20situated%20feedback%20during%20task%20execution%20%28%22that%27s%20not%20trash%22%29.%20We%0Aevaluate%20our%20system%20across%20three%20robotic%20platforms%2C%20including%20single-arm%2C%0Adual-arm%2C%20and%20dual-arm%20mobile%20robots%2C%20demonstrating%20its%20ability%20to%20handle%20tasks%0Asuch%20as%20cleaning%20messy%20tables%2C%20making%20sandwiches%2C%20and%20grocery%20shopping.%20Videos%0Aare%20available%20at%20https%3A//www.pi.website/research/hirobot%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19417v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHi%2520Robot%253A%2520Open-Ended%2520Instruction%2520Following%2520with%2520Hierarchical%250A%2520%2520Vision-Language-Action%2520Models%26entry.906535625%3DLucy%2520Xiaoyang%2520Shi%2520and%2520Brian%2520Ichter%2520and%2520Michael%2520Equi%2520and%2520Liyiming%2520Ke%2520and%2520Karl%2520Pertsch%2520and%2520Quan%2520Vuong%2520and%2520James%2520Tanner%2520and%2520Anna%2520Walling%2520and%2520Haohuan%2520Wang%2520and%2520Niccolo%2520Fusai%2520and%2520Adrian%2520Li-Bell%2520and%2520Danny%2520Driess%2520and%2520Lachy%2520Groom%2520and%2520Sergey%2520Levine%2520and%2520Chelsea%2520Finn%26entry.1292438233%3D%2520%2520Generalist%2520robots%2520that%2520can%2520perform%2520a%2520range%2520of%2520different%2520tasks%2520in%2520open-world%250Asettings%2520must%2520be%2520able%2520to%2520not%2520only%2520reason%2520about%2520the%2520steps%2520needed%2520to%2520accomplish%250Atheir%2520goals%252C%2520but%2520also%2520process%2520complex%2520instructions%252C%2520prompts%252C%2520and%2520even%2520feedback%250Aduring%2520task%2520execution.%2520Intricate%2520instructions%2520%2528e.g.%252C%2520%2522Could%2520you%2520make%2520me%2520a%250Avegetarian%2520sandwich%253F%2522%2520or%2520%2522I%2520don%2527t%2520like%2520that%2520one%2522%2529%2520require%2520not%2520just%2520the%2520ability%250Ato%2520physically%2520perform%2520the%2520individual%2520steps%252C%2520but%2520the%2520ability%2520to%2520situate%2520complex%250Acommands%2520and%2520feedback%2520in%2520the%2520physical%2520world.%2520In%2520this%2520work%252C%2520we%2520describe%2520a%2520system%250Athat%2520uses%2520vision-language%2520models%2520in%2520a%2520hierarchical%2520structure%252C%2520first%2520reasoning%250Aover%2520complex%2520prompts%2520and%2520user%2520feedback%2520to%2520deduce%2520the%2520most%2520appropriate%2520next%2520step%250Ato%2520fulfill%2520the%2520task%252C%2520and%2520then%2520performing%2520that%2520step%2520with%2520low-level%2520actions.%2520In%250Acontrast%2520to%2520direct%2520instruction%2520following%2520methods%2520that%2520can%2520fulfill%2520simple%250Acommands%2520%2528%2522pick%2520up%2520the%2520cup%2522%2529%252C%2520our%2520system%2520can%2520reason%2520through%2520complex%2520prompts%2520and%250Aincorporate%2520situated%2520feedback%2520during%2520task%2520execution%2520%2528%2522that%2527s%2520not%2520trash%2522%2529.%2520We%250Aevaluate%2520our%2520system%2520across%2520three%2520robotic%2520platforms%252C%2520including%2520single-arm%252C%250Adual-arm%252C%2520and%2520dual-arm%2520mobile%2520robots%252C%2520demonstrating%2520its%2520ability%2520to%2520handle%2520tasks%250Asuch%2520as%2520cleaning%2520messy%2520tables%252C%2520making%2520sandwiches%252C%2520and%2520grocery%2520shopping.%2520Videos%250Aare%2520available%2520at%2520https%253A//www.pi.website/research/hirobot%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19417v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hi%20Robot%3A%20Open-Ended%20Instruction%20Following%20with%20Hierarchical%0A%20%20Vision-Language-Action%20Models&entry.906535625=Lucy%20Xiaoyang%20Shi%20and%20Brian%20Ichter%20and%20Michael%20Equi%20and%20Liyiming%20Ke%20and%20Karl%20Pertsch%20and%20Quan%20Vuong%20and%20James%20Tanner%20and%20Anna%20Walling%20and%20Haohuan%20Wang%20and%20Niccolo%20Fusai%20and%20Adrian%20Li-Bell%20and%20Danny%20Driess%20and%20Lachy%20Groom%20and%20Sergey%20Levine%20and%20Chelsea%20Finn&entry.1292438233=%20%20Generalist%20robots%20that%20can%20perform%20a%20range%20of%20different%20tasks%20in%20open-world%0Asettings%20must%20be%20able%20to%20not%20only%20reason%20about%20the%20steps%20needed%20to%20accomplish%0Atheir%20goals%2C%20but%20also%20process%20complex%20instructions%2C%20prompts%2C%20and%20even%20feedback%0Aduring%20task%20execution.%20Intricate%20instructions%20%28e.g.%2C%20%22Could%20you%20make%20me%20a%0Avegetarian%20sandwich%3F%22%20or%20%22I%20don%27t%20like%20that%20one%22%29%20require%20not%20just%20the%20ability%0Ato%20physically%20perform%20the%20individual%20steps%2C%20but%20the%20ability%20to%20situate%20complex%0Acommands%20and%20feedback%20in%20the%20physical%20world.%20In%20this%20work%2C%20we%20describe%20a%20system%0Athat%20uses%20vision-language%20models%20in%20a%20hierarchical%20structure%2C%20first%20reasoning%0Aover%20complex%20prompts%20and%20user%20feedback%20to%20deduce%20the%20most%20appropriate%20next%20step%0Ato%20fulfill%20the%20task%2C%20and%20then%20performing%20that%20step%20with%20low-level%20actions.%20In%0Acontrast%20to%20direct%20instruction%20following%20methods%20that%20can%20fulfill%20simple%0Acommands%20%28%22pick%20up%20the%20cup%22%29%2C%20our%20system%20can%20reason%20through%20complex%20prompts%20and%0Aincorporate%20situated%20feedback%20during%20task%20execution%20%28%22that%27s%20not%20trash%22%29.%20We%0Aevaluate%20our%20system%20across%20three%20robotic%20platforms%2C%20including%20single-arm%2C%0Adual-arm%2C%20and%20dual-arm%20mobile%20robots%2C%20demonstrating%20its%20ability%20to%20handle%20tasks%0Asuch%20as%20cleaning%20messy%20tables%2C%20making%20sandwiches%2C%20and%20grocery%20shopping.%20Videos%0Aare%20available%20at%20https%3A//www.pi.website/research/hirobot%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19417v2&entry.124074799=Read"},
{"title": "Streaming 4D Visual Geometry Transformer", "author": "Dong Zhuo and Wenzhao Zheng and Jiahe Guo and Yuqi Wu and Jie Zhou and Jiwen Lu", "abstract": "  Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.\n", "link": "http://arxiv.org/abs/2507.11539v1", "date": "2025-07-15", "relevancy": 1.789, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5978}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5971}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Streaming%204D%20Visual%20Geometry%20Transformer&body=Title%3A%20Streaming%204D%20Visual%20Geometry%20Transformer%0AAuthor%3A%20Dong%20Zhuo%20and%20Wenzhao%20Zheng%20and%20Jiahe%20Guo%20and%20Yuqi%20Wu%20and%20Jie%20Zhou%20and%20Jiwen%20Lu%0AAbstract%3A%20%20%20Perceiving%20and%20reconstructing%204D%20spatial-temporal%20geometry%20from%20videos%20is%20a%0Afundamental%20yet%20challenging%20computer%20vision%20task.%20To%20facilitate%20interactive%20and%0Areal-time%20applications%2C%20we%20propose%20a%20streaming%204D%20visual%20geometry%20transformer%0Athat%20shares%20a%20similar%20philosophy%20with%20autoregressive%20large%20language%20models.%20We%0Aexplore%20a%20simple%20and%20efficient%20design%20and%20employ%20a%20causal%20transformer%0Aarchitecture%20to%20process%20the%20input%20sequence%20in%20an%20online%20manner.%20We%20use%20temporal%0Acausal%20attention%20and%20cache%20the%20historical%20keys%20and%20values%20as%20implicit%20memory%20to%0Aenable%20efficient%20streaming%20long-term%204D%20reconstruction.%20This%20design%20can%20handle%0Areal-time%204D%20reconstruction%20by%20incrementally%20integrating%20historical%20information%0Awhile%20maintaining%20high-quality%20spatial%20consistency.%20For%20efficient%20training%2C%20we%0Apropose%20to%20distill%20knowledge%20from%20the%20dense%20bidirectional%20visual%20geometry%0Agrounded%20transformer%20%28VGGT%29%20to%20our%20causal%20model.%20For%20inference%2C%20our%20model%0Asupports%20the%20migration%20of%20optimized%20efficient%20attention%20operator%20%28e.g.%2C%0AFlashAttention%29%20from%20the%20field%20of%20large%20language%20models.%20Extensive%20experiments%0Aon%20various%204D%20geometry%20perception%20benchmarks%20demonstrate%20that%20our%20model%0Aincreases%20the%20inference%20speed%20in%20online%20scenarios%20while%20maintaining%20competitive%0Aperformance%2C%20paving%20the%20way%20for%20scalable%20and%20interactive%204D%20vision%20systems.%0ACode%20is%20available%20at%3A%20https%3A//github.com/wzzheng/StreamVGGT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStreaming%25204D%2520Visual%2520Geometry%2520Transformer%26entry.906535625%3DDong%2520Zhuo%2520and%2520Wenzhao%2520Zheng%2520and%2520Jiahe%2520Guo%2520and%2520Yuqi%2520Wu%2520and%2520Jie%2520Zhou%2520and%2520Jiwen%2520Lu%26entry.1292438233%3D%2520%2520Perceiving%2520and%2520reconstructing%25204D%2520spatial-temporal%2520geometry%2520from%2520videos%2520is%2520a%250Afundamental%2520yet%2520challenging%2520computer%2520vision%2520task.%2520To%2520facilitate%2520interactive%2520and%250Areal-time%2520applications%252C%2520we%2520propose%2520a%2520streaming%25204D%2520visual%2520geometry%2520transformer%250Athat%2520shares%2520a%2520similar%2520philosophy%2520with%2520autoregressive%2520large%2520language%2520models.%2520We%250Aexplore%2520a%2520simple%2520and%2520efficient%2520design%2520and%2520employ%2520a%2520causal%2520transformer%250Aarchitecture%2520to%2520process%2520the%2520input%2520sequence%2520in%2520an%2520online%2520manner.%2520We%2520use%2520temporal%250Acausal%2520attention%2520and%2520cache%2520the%2520historical%2520keys%2520and%2520values%2520as%2520implicit%2520memory%2520to%250Aenable%2520efficient%2520streaming%2520long-term%25204D%2520reconstruction.%2520This%2520design%2520can%2520handle%250Areal-time%25204D%2520reconstruction%2520by%2520incrementally%2520integrating%2520historical%2520information%250Awhile%2520maintaining%2520high-quality%2520spatial%2520consistency.%2520For%2520efficient%2520training%252C%2520we%250Apropose%2520to%2520distill%2520knowledge%2520from%2520the%2520dense%2520bidirectional%2520visual%2520geometry%250Agrounded%2520transformer%2520%2528VGGT%2529%2520to%2520our%2520causal%2520model.%2520For%2520inference%252C%2520our%2520model%250Asupports%2520the%2520migration%2520of%2520optimized%2520efficient%2520attention%2520operator%2520%2528e.g.%252C%250AFlashAttention%2529%2520from%2520the%2520field%2520of%2520large%2520language%2520models.%2520Extensive%2520experiments%250Aon%2520various%25204D%2520geometry%2520perception%2520benchmarks%2520demonstrate%2520that%2520our%2520model%250Aincreases%2520the%2520inference%2520speed%2520in%2520online%2520scenarios%2520while%2520maintaining%2520competitive%250Aperformance%252C%2520paving%2520the%2520way%2520for%2520scalable%2520and%2520interactive%25204D%2520vision%2520systems.%250ACode%2520is%2520available%2520at%253A%2520https%253A//github.com/wzzheng/StreamVGGT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Streaming%204D%20Visual%20Geometry%20Transformer&entry.906535625=Dong%20Zhuo%20and%20Wenzhao%20Zheng%20and%20Jiahe%20Guo%20and%20Yuqi%20Wu%20and%20Jie%20Zhou%20and%20Jiwen%20Lu&entry.1292438233=%20%20Perceiving%20and%20reconstructing%204D%20spatial-temporal%20geometry%20from%20videos%20is%20a%0Afundamental%20yet%20challenging%20computer%20vision%20task.%20To%20facilitate%20interactive%20and%0Areal-time%20applications%2C%20we%20propose%20a%20streaming%204D%20visual%20geometry%20transformer%0Athat%20shares%20a%20similar%20philosophy%20with%20autoregressive%20large%20language%20models.%20We%0Aexplore%20a%20simple%20and%20efficient%20design%20and%20employ%20a%20causal%20transformer%0Aarchitecture%20to%20process%20the%20input%20sequence%20in%20an%20online%20manner.%20We%20use%20temporal%0Acausal%20attention%20and%20cache%20the%20historical%20keys%20and%20values%20as%20implicit%20memory%20to%0Aenable%20efficient%20streaming%20long-term%204D%20reconstruction.%20This%20design%20can%20handle%0Areal-time%204D%20reconstruction%20by%20incrementally%20integrating%20historical%20information%0Awhile%20maintaining%20high-quality%20spatial%20consistency.%20For%20efficient%20training%2C%20we%0Apropose%20to%20distill%20knowledge%20from%20the%20dense%20bidirectional%20visual%20geometry%0Agrounded%20transformer%20%28VGGT%29%20to%20our%20causal%20model.%20For%20inference%2C%20our%20model%0Asupports%20the%20migration%20of%20optimized%20efficient%20attention%20operator%20%28e.g.%2C%0AFlashAttention%29%20from%20the%20field%20of%20large%20language%20models.%20Extensive%20experiments%0Aon%20various%204D%20geometry%20perception%20benchmarks%20demonstrate%20that%20our%20model%0Aincreases%20the%20inference%20speed%20in%20online%20scenarios%20while%20maintaining%20competitive%0Aperformance%2C%20paving%20the%20way%20for%20scalable%20and%20interactive%204D%20vision%20systems.%0ACode%20is%20available%20at%3A%20https%3A//github.com/wzzheng/StreamVGGT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11539v1&entry.124074799=Read"},
{"title": "A Generative Approach to LLM Harmfulness Detection with Special Red Flag\n  Tokens", "author": "Sophie Xhonneux and David Dobre and Mehrnaz Mofakhami and Leo Schwinn and Gauthier Gidel", "abstract": "  Most safety training methods for large language models (LLMs) are based on\nfine-tuning that forces models to shift from an unsafe answer to refusal when\nfaced with harmful requests. Unfortunately, these drastic distribution shifts\ngenerally compromise model capabilities. To avoid that, we propose to expand\nthe model's vocabulary with a special token we call red flag token (<rf>) and\npropose to train the model to insert this token into its response at any time\nwhen harmful content is generated or about to be generated. Our approach offers\nseveral advantages: it enables the model to explicitly learn the concept of\nharmfulness while marginally affecting the generated distribution, thus\nmaintaining the model's utility. It also evaluates each generated answer and\nprovides robustness as good as adversarial training without the need to run\nattacks during training. Moreover, by encapsulating our safety tuning in a LoRA\nmodule, we provide additional defenses against fine-tuning API attacks.\n", "link": "http://arxiv.org/abs/2502.16366v3", "date": "2025-07-15", "relevancy": 1.7674, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4578}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4392}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Generative%20Approach%20to%20LLM%20Harmfulness%20Detection%20with%20Special%20Red%20Flag%0A%20%20Tokens&body=Title%3A%20A%20Generative%20Approach%20to%20LLM%20Harmfulness%20Detection%20with%20Special%20Red%20Flag%0A%20%20Tokens%0AAuthor%3A%20Sophie%20Xhonneux%20and%20David%20Dobre%20and%20Mehrnaz%20Mofakhami%20and%20Leo%20Schwinn%20and%20Gauthier%20Gidel%0AAbstract%3A%20%20%20Most%20safety%20training%20methods%20for%20large%20language%20models%20%28LLMs%29%20are%20based%20on%0Afine-tuning%20that%20forces%20models%20to%20shift%20from%20an%20unsafe%20answer%20to%20refusal%20when%0Afaced%20with%20harmful%20requests.%20Unfortunately%2C%20these%20drastic%20distribution%20shifts%0Agenerally%20compromise%20model%20capabilities.%20To%20avoid%20that%2C%20we%20propose%20to%20expand%0Athe%20model%27s%20vocabulary%20with%20a%20special%20token%20we%20call%20red%20flag%20token%20%28%3Crf%3E%29%20and%0Apropose%20to%20train%20the%20model%20to%20insert%20this%20token%20into%20its%20response%20at%20any%20time%0Awhen%20harmful%20content%20is%20generated%20or%20about%20to%20be%20generated.%20Our%20approach%20offers%0Aseveral%20advantages%3A%20it%20enables%20the%20model%20to%20explicitly%20learn%20the%20concept%20of%0Aharmfulness%20while%20marginally%20affecting%20the%20generated%20distribution%2C%20thus%0Amaintaining%20the%20model%27s%20utility.%20It%20also%20evaluates%20each%20generated%20answer%20and%0Aprovides%20robustness%20as%20good%20as%20adversarial%20training%20without%20the%20need%20to%20run%0Aattacks%20during%20training.%20Moreover%2C%20by%20encapsulating%20our%20safety%20tuning%20in%20a%20LoRA%0Amodule%2C%20we%20provide%20additional%20defenses%20against%20fine-tuning%20API%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.16366v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Generative%2520Approach%2520to%2520LLM%2520Harmfulness%2520Detection%2520with%2520Special%2520Red%2520Flag%250A%2520%2520Tokens%26entry.906535625%3DSophie%2520Xhonneux%2520and%2520David%2520Dobre%2520and%2520Mehrnaz%2520Mofakhami%2520and%2520Leo%2520Schwinn%2520and%2520Gauthier%2520Gidel%26entry.1292438233%3D%2520%2520Most%2520safety%2520training%2520methods%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520are%2520based%2520on%250Afine-tuning%2520that%2520forces%2520models%2520to%2520shift%2520from%2520an%2520unsafe%2520answer%2520to%2520refusal%2520when%250Afaced%2520with%2520harmful%2520requests.%2520Unfortunately%252C%2520these%2520drastic%2520distribution%2520shifts%250Agenerally%2520compromise%2520model%2520capabilities.%2520To%2520avoid%2520that%252C%2520we%2520propose%2520to%2520expand%250Athe%2520model%2527s%2520vocabulary%2520with%2520a%2520special%2520token%2520we%2520call%2520red%2520flag%2520token%2520%2528%253Crf%253E%2529%2520and%250Apropose%2520to%2520train%2520the%2520model%2520to%2520insert%2520this%2520token%2520into%2520its%2520response%2520at%2520any%2520time%250Awhen%2520harmful%2520content%2520is%2520generated%2520or%2520about%2520to%2520be%2520generated.%2520Our%2520approach%2520offers%250Aseveral%2520advantages%253A%2520it%2520enables%2520the%2520model%2520to%2520explicitly%2520learn%2520the%2520concept%2520of%250Aharmfulness%2520while%2520marginally%2520affecting%2520the%2520generated%2520distribution%252C%2520thus%250Amaintaining%2520the%2520model%2527s%2520utility.%2520It%2520also%2520evaluates%2520each%2520generated%2520answer%2520and%250Aprovides%2520robustness%2520as%2520good%2520as%2520adversarial%2520training%2520without%2520the%2520need%2520to%2520run%250Aattacks%2520during%2520training.%2520Moreover%252C%2520by%2520encapsulating%2520our%2520safety%2520tuning%2520in%2520a%2520LoRA%250Amodule%252C%2520we%2520provide%2520additional%2520defenses%2520against%2520fine-tuning%2520API%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.16366v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Generative%20Approach%20to%20LLM%20Harmfulness%20Detection%20with%20Special%20Red%20Flag%0A%20%20Tokens&entry.906535625=Sophie%20Xhonneux%20and%20David%20Dobre%20and%20Mehrnaz%20Mofakhami%20and%20Leo%20Schwinn%20and%20Gauthier%20Gidel&entry.1292438233=%20%20Most%20safety%20training%20methods%20for%20large%20language%20models%20%28LLMs%29%20are%20based%20on%0Afine-tuning%20that%20forces%20models%20to%20shift%20from%20an%20unsafe%20answer%20to%20refusal%20when%0Afaced%20with%20harmful%20requests.%20Unfortunately%2C%20these%20drastic%20distribution%20shifts%0Agenerally%20compromise%20model%20capabilities.%20To%20avoid%20that%2C%20we%20propose%20to%20expand%0Athe%20model%27s%20vocabulary%20with%20a%20special%20token%20we%20call%20red%20flag%20token%20%28%3Crf%3E%29%20and%0Apropose%20to%20train%20the%20model%20to%20insert%20this%20token%20into%20its%20response%20at%20any%20time%0Awhen%20harmful%20content%20is%20generated%20or%20about%20to%20be%20generated.%20Our%20approach%20offers%0Aseveral%20advantages%3A%20it%20enables%20the%20model%20to%20explicitly%20learn%20the%20concept%20of%0Aharmfulness%20while%20marginally%20affecting%20the%20generated%20distribution%2C%20thus%0Amaintaining%20the%20model%27s%20utility.%20It%20also%20evaluates%20each%20generated%20answer%20and%0Aprovides%20robustness%20as%20good%20as%20adversarial%20training%20without%20the%20need%20to%20run%0Aattacks%20during%20training.%20Moreover%2C%20by%20encapsulating%20our%20safety%20tuning%20in%20a%20LoRA%0Amodule%2C%20we%20provide%20additional%20defenses%20against%20fine-tuning%20API%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.16366v3&entry.124074799=Read"},
{"title": "Seven Security Challenges That Must be Solved in Cross-domain\n  Multi-agent LLM Systems", "author": "Ronny Ko and Jiseong Jeong and Shuyuan Zheng and Chuan Xiao and Tae-Wan Kim and Makoto Onizuka and Won-Yong Shin", "abstract": "  Large language models (LLMs) are rapidly evolving into autonomous agents that\ncooperate across organizational boundaries, enabling joint disaster response,\nsupply-chain optimization, and other tasks that demand decentralized expertise\nwithout surrendering data ownership. Yet, cross-domain collaboration shatters\nthe unified trust assumptions behind current alignment and containment\ntechniques. An agent benign in isolation may, when receiving messages from an\nuntrusted peer, leak secrets or violate policy, producing risks driven by\nemergent multi-agent dynamics rather than classical software bugs. This\nposition paper maps the security agenda for cross-domain multi-agent LLM\nsystems. We introduce seven categories of novel security challenges, for each\nof which we also present plausible attacks, security evaluation metrics, and\nfuture research guidelines.\n", "link": "http://arxiv.org/abs/2505.23847v3", "date": "2025-07-15", "relevancy": 1.7518, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4433}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4371}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seven%20Security%20Challenges%20That%20Must%20be%20Solved%20in%20Cross-domain%0A%20%20Multi-agent%20LLM%20Systems&body=Title%3A%20Seven%20Security%20Challenges%20That%20Must%20be%20Solved%20in%20Cross-domain%0A%20%20Multi-agent%20LLM%20Systems%0AAuthor%3A%20Ronny%20Ko%20and%20Jiseong%20Jeong%20and%20Shuyuan%20Zheng%20and%20Chuan%20Xiao%20and%20Tae-Wan%20Kim%20and%20Makoto%20Onizuka%20and%20Won-Yong%20Shin%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20rapidly%20evolving%20into%20autonomous%20agents%20that%0Acooperate%20across%20organizational%20boundaries%2C%20enabling%20joint%20disaster%20response%2C%0Asupply-chain%20optimization%2C%20and%20other%20tasks%20that%20demand%20decentralized%20expertise%0Awithout%20surrendering%20data%20ownership.%20Yet%2C%20cross-domain%20collaboration%20shatters%0Athe%20unified%20trust%20assumptions%20behind%20current%20alignment%20and%20containment%0Atechniques.%20An%20agent%20benign%20in%20isolation%20may%2C%20when%20receiving%20messages%20from%20an%0Auntrusted%20peer%2C%20leak%20secrets%20or%20violate%20policy%2C%20producing%20risks%20driven%20by%0Aemergent%20multi-agent%20dynamics%20rather%20than%20classical%20software%20bugs.%20This%0Aposition%20paper%20maps%20the%20security%20agenda%20for%20cross-domain%20multi-agent%20LLM%0Asystems.%20We%20introduce%20seven%20categories%20of%20novel%20security%20challenges%2C%20for%20each%0Aof%20which%20we%20also%20present%20plausible%20attacks%2C%20security%20evaluation%20metrics%2C%20and%0Afuture%20research%20guidelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.23847v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeven%2520Security%2520Challenges%2520That%2520Must%2520be%2520Solved%2520in%2520Cross-domain%250A%2520%2520Multi-agent%2520LLM%2520Systems%26entry.906535625%3DRonny%2520Ko%2520and%2520Jiseong%2520Jeong%2520and%2520Shuyuan%2520Zheng%2520and%2520Chuan%2520Xiao%2520and%2520Tae-Wan%2520Kim%2520and%2520Makoto%2520Onizuka%2520and%2520Won-Yong%2520Shin%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520rapidly%2520evolving%2520into%2520autonomous%2520agents%2520that%250Acooperate%2520across%2520organizational%2520boundaries%252C%2520enabling%2520joint%2520disaster%2520response%252C%250Asupply-chain%2520optimization%252C%2520and%2520other%2520tasks%2520that%2520demand%2520decentralized%2520expertise%250Awithout%2520surrendering%2520data%2520ownership.%2520Yet%252C%2520cross-domain%2520collaboration%2520shatters%250Athe%2520unified%2520trust%2520assumptions%2520behind%2520current%2520alignment%2520and%2520containment%250Atechniques.%2520An%2520agent%2520benign%2520in%2520isolation%2520may%252C%2520when%2520receiving%2520messages%2520from%2520an%250Auntrusted%2520peer%252C%2520leak%2520secrets%2520or%2520violate%2520policy%252C%2520producing%2520risks%2520driven%2520by%250Aemergent%2520multi-agent%2520dynamics%2520rather%2520than%2520classical%2520software%2520bugs.%2520This%250Aposition%2520paper%2520maps%2520the%2520security%2520agenda%2520for%2520cross-domain%2520multi-agent%2520LLM%250Asystems.%2520We%2520introduce%2520seven%2520categories%2520of%2520novel%2520security%2520challenges%252C%2520for%2520each%250Aof%2520which%2520we%2520also%2520present%2520plausible%2520attacks%252C%2520security%2520evaluation%2520metrics%252C%2520and%250Afuture%2520research%2520guidelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23847v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seven%20Security%20Challenges%20That%20Must%20be%20Solved%20in%20Cross-domain%0A%20%20Multi-agent%20LLM%20Systems&entry.906535625=Ronny%20Ko%20and%20Jiseong%20Jeong%20and%20Shuyuan%20Zheng%20and%20Chuan%20Xiao%20and%20Tae-Wan%20Kim%20and%20Makoto%20Onizuka%20and%20Won-Yong%20Shin&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20rapidly%20evolving%20into%20autonomous%20agents%20that%0Acooperate%20across%20organizational%20boundaries%2C%20enabling%20joint%20disaster%20response%2C%0Asupply-chain%20optimization%2C%20and%20other%20tasks%20that%20demand%20decentralized%20expertise%0Awithout%20surrendering%20data%20ownership.%20Yet%2C%20cross-domain%20collaboration%20shatters%0Athe%20unified%20trust%20assumptions%20behind%20current%20alignment%20and%20containment%0Atechniques.%20An%20agent%20benign%20in%20isolation%20may%2C%20when%20receiving%20messages%20from%20an%0Auntrusted%20peer%2C%20leak%20secrets%20or%20violate%20policy%2C%20producing%20risks%20driven%20by%0Aemergent%20multi-agent%20dynamics%20rather%20than%20classical%20software%20bugs.%20This%0Aposition%20paper%20maps%20the%20security%20agenda%20for%20cross-domain%20multi-agent%20LLM%0Asystems.%20We%20introduce%20seven%20categories%20of%20novel%20security%20challenges%2C%20for%20each%0Aof%20which%20we%20also%20present%20plausible%20attacks%2C%20security%20evaluation%20metrics%2C%20and%0Afuture%20research%20guidelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.23847v3&entry.124074799=Read"},
{"title": "LF: Online Multi-Robot Path Planning Meets Optimal Trajectory Control", "author": "Ajay Shankar and Keisuke Okumura and Amanda Prorok", "abstract": "  We propose a multi-robot control paradigm to solve point-to-point navigation\ntasks for a team of holonomic robots with access to the full environment\ninformation. The framework invokes two processes asynchronously at high\nfrequency: (i) a centralized, discrete, and full-horizon planner for computing\ncollision- and deadlock-free paths rapidly, leveraging recent advances in\nmulti-agent pathfinding (MAPF), and (ii) dynamics-aware, robot-wise optimal\ntrajectory controllers that ensure all robots independently follow their\nassigned paths reliably. This hierarchical shift in planning representation\nfrom (i) discrete and coupled to (ii) continuous and decoupled domains enables\nthe framework to maintain long-term scalable motion synthesis. As an\ninstantiation of this idea, we present LF, which combines a fast\nstate-of-the-art MAPF solver (LaCAM), and a robust feedback control stack\n(Freyja) for executing agile robot maneuvers. LF provides a robust and\nversatile mechanism for lifelong multi-robot navigation even under asynchronous\nand partial goal updates, and adapts to dynamic workspaces simply by quick\nreplanning. We present various multirotor and ground robot demonstrations,\nincluding the deployment of 15 real multirotors with random, consecutive target\nupdates while a person walks through the operational workspace.\n", "link": "http://arxiv.org/abs/2507.11464v1", "date": "2025-07-15", "relevancy": 1.7264, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5883}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5746}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LF%3A%20Online%20Multi-Robot%20Path%20Planning%20Meets%20Optimal%20Trajectory%20Control&body=Title%3A%20LF%3A%20Online%20Multi-Robot%20Path%20Planning%20Meets%20Optimal%20Trajectory%20Control%0AAuthor%3A%20Ajay%20Shankar%20and%20Keisuke%20Okumura%20and%20Amanda%20Prorok%0AAbstract%3A%20%20%20We%20propose%20a%20multi-robot%20control%20paradigm%20to%20solve%20point-to-point%20navigation%0Atasks%20for%20a%20team%20of%20holonomic%20robots%20with%20access%20to%20the%20full%20environment%0Ainformation.%20The%20framework%20invokes%20two%20processes%20asynchronously%20at%20high%0Afrequency%3A%20%28i%29%20a%20centralized%2C%20discrete%2C%20and%20full-horizon%20planner%20for%20computing%0Acollision-%20and%20deadlock-free%20paths%20rapidly%2C%20leveraging%20recent%20advances%20in%0Amulti-agent%20pathfinding%20%28MAPF%29%2C%20and%20%28ii%29%20dynamics-aware%2C%20robot-wise%20optimal%0Atrajectory%20controllers%20that%20ensure%20all%20robots%20independently%20follow%20their%0Aassigned%20paths%20reliably.%20This%20hierarchical%20shift%20in%20planning%20representation%0Afrom%20%28i%29%20discrete%20and%20coupled%20to%20%28ii%29%20continuous%20and%20decoupled%20domains%20enables%0Athe%20framework%20to%20maintain%20long-term%20scalable%20motion%20synthesis.%20As%20an%0Ainstantiation%20of%20this%20idea%2C%20we%20present%20LF%2C%20which%20combines%20a%20fast%0Astate-of-the-art%20MAPF%20solver%20%28LaCAM%29%2C%20and%20a%20robust%20feedback%20control%20stack%0A%28Freyja%29%20for%20executing%20agile%20robot%20maneuvers.%20LF%20provides%20a%20robust%20and%0Aversatile%20mechanism%20for%20lifelong%20multi-robot%20navigation%20even%20under%20asynchronous%0Aand%20partial%20goal%20updates%2C%20and%20adapts%20to%20dynamic%20workspaces%20simply%20by%20quick%0Areplanning.%20We%20present%20various%20multirotor%20and%20ground%20robot%20demonstrations%2C%0Aincluding%20the%20deployment%20of%2015%20real%20multirotors%20with%20random%2C%20consecutive%20target%0Aupdates%20while%20a%20person%20walks%20through%20the%20operational%20workspace.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLF%253A%2520Online%2520Multi-Robot%2520Path%2520Planning%2520Meets%2520Optimal%2520Trajectory%2520Control%26entry.906535625%3DAjay%2520Shankar%2520and%2520Keisuke%2520Okumura%2520and%2520Amanda%2520Prorok%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520multi-robot%2520control%2520paradigm%2520to%2520solve%2520point-to-point%2520navigation%250Atasks%2520for%2520a%2520team%2520of%2520holonomic%2520robots%2520with%2520access%2520to%2520the%2520full%2520environment%250Ainformation.%2520The%2520framework%2520invokes%2520two%2520processes%2520asynchronously%2520at%2520high%250Afrequency%253A%2520%2528i%2529%2520a%2520centralized%252C%2520discrete%252C%2520and%2520full-horizon%2520planner%2520for%2520computing%250Acollision-%2520and%2520deadlock-free%2520paths%2520rapidly%252C%2520leveraging%2520recent%2520advances%2520in%250Amulti-agent%2520pathfinding%2520%2528MAPF%2529%252C%2520and%2520%2528ii%2529%2520dynamics-aware%252C%2520robot-wise%2520optimal%250Atrajectory%2520controllers%2520that%2520ensure%2520all%2520robots%2520independently%2520follow%2520their%250Aassigned%2520paths%2520reliably.%2520This%2520hierarchical%2520shift%2520in%2520planning%2520representation%250Afrom%2520%2528i%2529%2520discrete%2520and%2520coupled%2520to%2520%2528ii%2529%2520continuous%2520and%2520decoupled%2520domains%2520enables%250Athe%2520framework%2520to%2520maintain%2520long-term%2520scalable%2520motion%2520synthesis.%2520As%2520an%250Ainstantiation%2520of%2520this%2520idea%252C%2520we%2520present%2520LF%252C%2520which%2520combines%2520a%2520fast%250Astate-of-the-art%2520MAPF%2520solver%2520%2528LaCAM%2529%252C%2520and%2520a%2520robust%2520feedback%2520control%2520stack%250A%2528Freyja%2529%2520for%2520executing%2520agile%2520robot%2520maneuvers.%2520LF%2520provides%2520a%2520robust%2520and%250Aversatile%2520mechanism%2520for%2520lifelong%2520multi-robot%2520navigation%2520even%2520under%2520asynchronous%250Aand%2520partial%2520goal%2520updates%252C%2520and%2520adapts%2520to%2520dynamic%2520workspaces%2520simply%2520by%2520quick%250Areplanning.%2520We%2520present%2520various%2520multirotor%2520and%2520ground%2520robot%2520demonstrations%252C%250Aincluding%2520the%2520deployment%2520of%252015%2520real%2520multirotors%2520with%2520random%252C%2520consecutive%2520target%250Aupdates%2520while%2520a%2520person%2520walks%2520through%2520the%2520operational%2520workspace.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LF%3A%20Online%20Multi-Robot%20Path%20Planning%20Meets%20Optimal%20Trajectory%20Control&entry.906535625=Ajay%20Shankar%20and%20Keisuke%20Okumura%20and%20Amanda%20Prorok&entry.1292438233=%20%20We%20propose%20a%20multi-robot%20control%20paradigm%20to%20solve%20point-to-point%20navigation%0Atasks%20for%20a%20team%20of%20holonomic%20robots%20with%20access%20to%20the%20full%20environment%0Ainformation.%20The%20framework%20invokes%20two%20processes%20asynchronously%20at%20high%0Afrequency%3A%20%28i%29%20a%20centralized%2C%20discrete%2C%20and%20full-horizon%20planner%20for%20computing%0Acollision-%20and%20deadlock-free%20paths%20rapidly%2C%20leveraging%20recent%20advances%20in%0Amulti-agent%20pathfinding%20%28MAPF%29%2C%20and%20%28ii%29%20dynamics-aware%2C%20robot-wise%20optimal%0Atrajectory%20controllers%20that%20ensure%20all%20robots%20independently%20follow%20their%0Aassigned%20paths%20reliably.%20This%20hierarchical%20shift%20in%20planning%20representation%0Afrom%20%28i%29%20discrete%20and%20coupled%20to%20%28ii%29%20continuous%20and%20decoupled%20domains%20enables%0Athe%20framework%20to%20maintain%20long-term%20scalable%20motion%20synthesis.%20As%20an%0Ainstantiation%20of%20this%20idea%2C%20we%20present%20LF%2C%20which%20combines%20a%20fast%0Astate-of-the-art%20MAPF%20solver%20%28LaCAM%29%2C%20and%20a%20robust%20feedback%20control%20stack%0A%28Freyja%29%20for%20executing%20agile%20robot%20maneuvers.%20LF%20provides%20a%20robust%20and%0Aversatile%20mechanism%20for%20lifelong%20multi-robot%20navigation%20even%20under%20asynchronous%0Aand%20partial%20goal%20updates%2C%20and%20adapts%20to%20dynamic%20workspaces%20simply%20by%20quick%0Areplanning.%20We%20present%20various%20multirotor%20and%20ground%20robot%20demonstrations%2C%0Aincluding%20the%20deployment%20of%2015%20real%20multirotors%20with%20random%2C%20consecutive%20target%0Aupdates%20while%20a%20person%20walks%20through%20the%20operational%20workspace.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11464v1&entry.124074799=Read"},
{"title": "A Mathematical Theory of Discursive Networks", "author": "Juan B. Guti\u00e9rrez", "abstract": "  Large-language models (LLMs) turn writing into a live exchange between humans\nand software. We characterize this new medium as a discursive network that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. We define the generation of erroneous information as invalidation\n(any factual, logical, or structural breach) and show it follows four hazards:\ndrift from truth, self-repair, fresh fabrication, and external detection. We\ndevelop a general mathematical model of discursive networks that shows that a\nnetwork governed only by drift and self-repair stabilizes at a modest error\nrate. Giving each false claim even a small chance of peer review shifts the\nsystem to a truth-dominant state. We operationalize peer review with the\nopen-source \\emph{Flaws-of-Others (FOO) algorithm}: a configurable loop in\nwhich any set of agents critique one another while a harmonizer merges their\nverdicts. We identify an ethical transgression, epithesis, that occurs when\nhumans fail to engage in the discursive network. The takeaway is practical and\ncultural: reliability in this new medium comes not from perfecting single\nmodels but from connecting imperfect ones into networks that enforce mutual\naccountability.\n", "link": "http://arxiv.org/abs/2507.06565v3", "date": "2025-07-15", "relevancy": 1.7139, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4549}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4379}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Mathematical%20Theory%20of%20Discursive%20Networks&body=Title%3A%20A%20Mathematical%20Theory%20of%20Discursive%20Networks%0AAuthor%3A%20Juan%20B.%20Guti%C3%A9rrez%0AAbstract%3A%20%20%20Large-language%20models%20%28LLMs%29%20turn%20writing%20into%20a%20live%20exchange%20between%20humans%0Aand%20software.%20We%20characterize%20this%20new%20medium%20as%20a%20discursive%20network%20that%0Atreats%20people%20and%20LLMs%20as%20equal%20nodes%20and%20tracks%20how%20their%20statements%0Acirculate.%20We%20define%20the%20generation%20of%20erroneous%20information%20as%20invalidation%0A%28any%20factual%2C%20logical%2C%20or%20structural%20breach%29%20and%20show%20it%20follows%20four%20hazards%3A%0Adrift%20from%20truth%2C%20self-repair%2C%20fresh%20fabrication%2C%20and%20external%20detection.%20We%0Adevelop%20a%20general%20mathematical%20model%20of%20discursive%20networks%20that%20shows%20that%20a%0Anetwork%20governed%20only%20by%20drift%20and%20self-repair%20stabilizes%20at%20a%20modest%20error%0Arate.%20Giving%20each%20false%20claim%20even%20a%20small%20chance%20of%20peer%20review%20shifts%20the%0Asystem%20to%20a%20truth-dominant%20state.%20We%20operationalize%20peer%20review%20with%20the%0Aopen-source%20%5Cemph%7BFlaws-of-Others%20%28FOO%29%20algorithm%7D%3A%20a%20configurable%20loop%20in%0Awhich%20any%20set%20of%20agents%20critique%20one%20another%20while%20a%20harmonizer%20merges%20their%0Averdicts.%20We%20identify%20an%20ethical%20transgression%2C%20epithesis%2C%20that%20occurs%20when%0Ahumans%20fail%20to%20engage%20in%20the%20discursive%20network.%20The%20takeaway%20is%20practical%20and%0Acultural%3A%20reliability%20in%20this%20new%20medium%20comes%20not%20from%20perfecting%20single%0Amodels%20but%20from%20connecting%20imperfect%20ones%20into%20networks%20that%20enforce%20mutual%0Aaccountability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.06565v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Mathematical%2520Theory%2520of%2520Discursive%2520Networks%26entry.906535625%3DJuan%2520B.%2520Guti%25C3%25A9rrez%26entry.1292438233%3D%2520%2520Large-language%2520models%2520%2528LLMs%2529%2520turn%2520writing%2520into%2520a%2520live%2520exchange%2520between%2520humans%250Aand%2520software.%2520We%2520characterize%2520this%2520new%2520medium%2520as%2520a%2520discursive%2520network%2520that%250Atreats%2520people%2520and%2520LLMs%2520as%2520equal%2520nodes%2520and%2520tracks%2520how%2520their%2520statements%250Acirculate.%2520We%2520define%2520the%2520generation%2520of%2520erroneous%2520information%2520as%2520invalidation%250A%2528any%2520factual%252C%2520logical%252C%2520or%2520structural%2520breach%2529%2520and%2520show%2520it%2520follows%2520four%2520hazards%253A%250Adrift%2520from%2520truth%252C%2520self-repair%252C%2520fresh%2520fabrication%252C%2520and%2520external%2520detection.%2520We%250Adevelop%2520a%2520general%2520mathematical%2520model%2520of%2520discursive%2520networks%2520that%2520shows%2520that%2520a%250Anetwork%2520governed%2520only%2520by%2520drift%2520and%2520self-repair%2520stabilizes%2520at%2520a%2520modest%2520error%250Arate.%2520Giving%2520each%2520false%2520claim%2520even%2520a%2520small%2520chance%2520of%2520peer%2520review%2520shifts%2520the%250Asystem%2520to%2520a%2520truth-dominant%2520state.%2520We%2520operationalize%2520peer%2520review%2520with%2520the%250Aopen-source%2520%255Cemph%257BFlaws-of-Others%2520%2528FOO%2529%2520algorithm%257D%253A%2520a%2520configurable%2520loop%2520in%250Awhich%2520any%2520set%2520of%2520agents%2520critique%2520one%2520another%2520while%2520a%2520harmonizer%2520merges%2520their%250Averdicts.%2520We%2520identify%2520an%2520ethical%2520transgression%252C%2520epithesis%252C%2520that%2520occurs%2520when%250Ahumans%2520fail%2520to%2520engage%2520in%2520the%2520discursive%2520network.%2520The%2520takeaway%2520is%2520practical%2520and%250Acultural%253A%2520reliability%2520in%2520this%2520new%2520medium%2520comes%2520not%2520from%2520perfecting%2520single%250Amodels%2520but%2520from%2520connecting%2520imperfect%2520ones%2520into%2520networks%2520that%2520enforce%2520mutual%250Aaccountability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06565v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Mathematical%20Theory%20of%20Discursive%20Networks&entry.906535625=Juan%20B.%20Guti%C3%A9rrez&entry.1292438233=%20%20Large-language%20models%20%28LLMs%29%20turn%20writing%20into%20a%20live%20exchange%20between%20humans%0Aand%20software.%20We%20characterize%20this%20new%20medium%20as%20a%20discursive%20network%20that%0Atreats%20people%20and%20LLMs%20as%20equal%20nodes%20and%20tracks%20how%20their%20statements%0Acirculate.%20We%20define%20the%20generation%20of%20erroneous%20information%20as%20invalidation%0A%28any%20factual%2C%20logical%2C%20or%20structural%20breach%29%20and%20show%20it%20follows%20four%20hazards%3A%0Adrift%20from%20truth%2C%20self-repair%2C%20fresh%20fabrication%2C%20and%20external%20detection.%20We%0Adevelop%20a%20general%20mathematical%20model%20of%20discursive%20networks%20that%20shows%20that%20a%0Anetwork%20governed%20only%20by%20drift%20and%20self-repair%20stabilizes%20at%20a%20modest%20error%0Arate.%20Giving%20each%20false%20claim%20even%20a%20small%20chance%20of%20peer%20review%20shifts%20the%0Asystem%20to%20a%20truth-dominant%20state.%20We%20operationalize%20peer%20review%20with%20the%0Aopen-source%20%5Cemph%7BFlaws-of-Others%20%28FOO%29%20algorithm%7D%3A%20a%20configurable%20loop%20in%0Awhich%20any%20set%20of%20agents%20critique%20one%20another%20while%20a%20harmonizer%20merges%20their%0Averdicts.%20We%20identify%20an%20ethical%20transgression%2C%20epithesis%2C%20that%20occurs%20when%0Ahumans%20fail%20to%20engage%20in%20the%20discursive%20network.%20The%20takeaway%20is%20practical%20and%0Acultural%3A%20reliability%20in%20this%20new%20medium%20comes%20not%20from%20perfecting%20single%0Amodels%20but%20from%20connecting%20imperfect%20ones%20into%20networks%20that%20enforce%20mutual%0Aaccountability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.06565v3&entry.124074799=Read"},
{"title": "Canonical Bayesian Linear System Identification", "author": "Andrey Bryutkin and Matthew E. Levine and I\u00f1igo Urteaga and Youssef Marzouk", "abstract": "  Standard Bayesian approaches for linear time-invariant (LTI) system\nidentification are hindered by parameter non-identifiability; the resulting\ncomplex, multi-modal posteriors make inference inefficient and impractical. We\nsolve this problem by embedding canonical forms of LTI systems within the\nBayesian framework. We rigorously establish that inference in these minimal\nparameterizations fully captures all invariant system dynamics (e.g., transfer\nfunctions, eigenvalues, predictive distributions of system outputs) while\nresolving identifiability. This approach unlocks the use of meaningful,\nstructure-aware priors (e.g., enforcing stability via eigenvalues) and ensures\nconditions for a Bernstein--von Mises theorem -- a link between Bayesian and\nfrequentist large-sample asymptotics that is broken in standard forms.\nExtensive simulations with modern MCMC methods highlight advantages over\nstandard parameterizations: canonical forms achieve higher computational\nefficiency, generate interpretable and well-behaved posteriors, and provide\nrobust uncertainty estimates, particularly from limited data.\n", "link": "http://arxiv.org/abs/2507.11535v1", "date": "2025-07-15", "relevancy": 1.6877, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5013}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4107}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4015}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Canonical%20Bayesian%20Linear%20System%20Identification&body=Title%3A%20Canonical%20Bayesian%20Linear%20System%20Identification%0AAuthor%3A%20Andrey%20Bryutkin%20and%20Matthew%20E.%20Levine%20and%20I%C3%B1igo%20Urteaga%20and%20Youssef%20Marzouk%0AAbstract%3A%20%20%20Standard%20Bayesian%20approaches%20for%20linear%20time-invariant%20%28LTI%29%20system%0Aidentification%20are%20hindered%20by%20parameter%20non-identifiability%3B%20the%20resulting%0Acomplex%2C%20multi-modal%20posteriors%20make%20inference%20inefficient%20and%20impractical.%20We%0Asolve%20this%20problem%20by%20embedding%20canonical%20forms%20of%20LTI%20systems%20within%20the%0ABayesian%20framework.%20We%20rigorously%20establish%20that%20inference%20in%20these%20minimal%0Aparameterizations%20fully%20captures%20all%20invariant%20system%20dynamics%20%28e.g.%2C%20transfer%0Afunctions%2C%20eigenvalues%2C%20predictive%20distributions%20of%20system%20outputs%29%20while%0Aresolving%20identifiability.%20This%20approach%20unlocks%20the%20use%20of%20meaningful%2C%0Astructure-aware%20priors%20%28e.g.%2C%20enforcing%20stability%20via%20eigenvalues%29%20and%20ensures%0Aconditions%20for%20a%20Bernstein--von%20Mises%20theorem%20--%20a%20link%20between%20Bayesian%20and%0Afrequentist%20large-sample%20asymptotics%20that%20is%20broken%20in%20standard%20forms.%0AExtensive%20simulations%20with%20modern%20MCMC%20methods%20highlight%20advantages%20over%0Astandard%20parameterizations%3A%20canonical%20forms%20achieve%20higher%20computational%0Aefficiency%2C%20generate%20interpretable%20and%20well-behaved%20posteriors%2C%20and%20provide%0Arobust%20uncertainty%20estimates%2C%20particularly%20from%20limited%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCanonical%2520Bayesian%2520Linear%2520System%2520Identification%26entry.906535625%3DAndrey%2520Bryutkin%2520and%2520Matthew%2520E.%2520Levine%2520and%2520I%25C3%25B1igo%2520Urteaga%2520and%2520Youssef%2520Marzouk%26entry.1292438233%3D%2520%2520Standard%2520Bayesian%2520approaches%2520for%2520linear%2520time-invariant%2520%2528LTI%2529%2520system%250Aidentification%2520are%2520hindered%2520by%2520parameter%2520non-identifiability%253B%2520the%2520resulting%250Acomplex%252C%2520multi-modal%2520posteriors%2520make%2520inference%2520inefficient%2520and%2520impractical.%2520We%250Asolve%2520this%2520problem%2520by%2520embedding%2520canonical%2520forms%2520of%2520LTI%2520systems%2520within%2520the%250ABayesian%2520framework.%2520We%2520rigorously%2520establish%2520that%2520inference%2520in%2520these%2520minimal%250Aparameterizations%2520fully%2520captures%2520all%2520invariant%2520system%2520dynamics%2520%2528e.g.%252C%2520transfer%250Afunctions%252C%2520eigenvalues%252C%2520predictive%2520distributions%2520of%2520system%2520outputs%2529%2520while%250Aresolving%2520identifiability.%2520This%2520approach%2520unlocks%2520the%2520use%2520of%2520meaningful%252C%250Astructure-aware%2520priors%2520%2528e.g.%252C%2520enforcing%2520stability%2520via%2520eigenvalues%2529%2520and%2520ensures%250Aconditions%2520for%2520a%2520Bernstein--von%2520Mises%2520theorem%2520--%2520a%2520link%2520between%2520Bayesian%2520and%250Afrequentist%2520large-sample%2520asymptotics%2520that%2520is%2520broken%2520in%2520standard%2520forms.%250AExtensive%2520simulations%2520with%2520modern%2520MCMC%2520methods%2520highlight%2520advantages%2520over%250Astandard%2520parameterizations%253A%2520canonical%2520forms%2520achieve%2520higher%2520computational%250Aefficiency%252C%2520generate%2520interpretable%2520and%2520well-behaved%2520posteriors%252C%2520and%2520provide%250Arobust%2520uncertainty%2520estimates%252C%2520particularly%2520from%2520limited%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Canonical%20Bayesian%20Linear%20System%20Identification&entry.906535625=Andrey%20Bryutkin%20and%20Matthew%20E.%20Levine%20and%20I%C3%B1igo%20Urteaga%20and%20Youssef%20Marzouk&entry.1292438233=%20%20Standard%20Bayesian%20approaches%20for%20linear%20time-invariant%20%28LTI%29%20system%0Aidentification%20are%20hindered%20by%20parameter%20non-identifiability%3B%20the%20resulting%0Acomplex%2C%20multi-modal%20posteriors%20make%20inference%20inefficient%20and%20impractical.%20We%0Asolve%20this%20problem%20by%20embedding%20canonical%20forms%20of%20LTI%20systems%20within%20the%0ABayesian%20framework.%20We%20rigorously%20establish%20that%20inference%20in%20these%20minimal%0Aparameterizations%20fully%20captures%20all%20invariant%20system%20dynamics%20%28e.g.%2C%20transfer%0Afunctions%2C%20eigenvalues%2C%20predictive%20distributions%20of%20system%20outputs%29%20while%0Aresolving%20identifiability.%20This%20approach%20unlocks%20the%20use%20of%20meaningful%2C%0Astructure-aware%20priors%20%28e.g.%2C%20enforcing%20stability%20via%20eigenvalues%29%20and%20ensures%0Aconditions%20for%20a%20Bernstein--von%20Mises%20theorem%20--%20a%20link%20between%20Bayesian%20and%0Afrequentist%20large-sample%20asymptotics%20that%20is%20broken%20in%20standard%20forms.%0AExtensive%20simulations%20with%20modern%20MCMC%20methods%20highlight%20advantages%20over%0Astandard%20parameterizations%3A%20canonical%20forms%20achieve%20higher%20computational%0Aefficiency%2C%20generate%20interpretable%20and%20well-behaved%20posteriors%2C%20and%20provide%0Arobust%20uncertainty%20estimates%2C%20particularly%20from%20limited%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11535v1&entry.124074799=Read"},
{"title": "Langevin Flows for Modeling Neural Latent Dynamics", "author": "Yue Song and T. Anderson Keller and Yisong Yue and Pietro Perona and Max Welling", "abstract": "  Neural populations exhibit latent dynamical structures that drive\ntime-evolving spiking activities, motivating the search for models that capture\nboth intrinsic network dynamics and external unobserved influences. In this\nwork, we introduce LangevinFlow, a sequential Variational Auto-Encoder where\nthe time evolution of latent variables is governed by the underdamped Langevin\nequation. Our approach incorporates physical priors -- such as inertia,\ndamping, a learned potential function, and stochastic forces -- to represent\nboth autonomous and non-autonomous processes in neural systems. Crucially, the\npotential function is parameterized as a network of locally coupled\noscillators, biasing the model toward oscillatory and flow-like behaviors\nobserved in biological neural populations. Our model features a recurrent\nencoder, a one-layer Transformer decoder, and Langevin dynamics in the latent\nspace. Empirically, our method outperforms state-of-the-art baselines on\nsynthetic neural populations generated by a Lorenz attractor, closely matching\nground-truth firing rates. On the Neural Latents Benchmark (NLB), the model\nachieves superior held-out neuron likelihoods (bits per spike) and forward\nprediction accuracy across four challenging datasets. It also matches or\nsurpasses alternative methods in decoding behavioral metrics such as hand\nvelocity. Overall, this work introduces a flexible, physics-inspired,\nhigh-performing framework for modeling complex neural population dynamics and\ntheir unobserved influences.\n", "link": "http://arxiv.org/abs/2507.11531v1", "date": "2025-07-15", "relevancy": 1.6548, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5905}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5434}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Langevin%20Flows%20for%20Modeling%20Neural%20Latent%20Dynamics&body=Title%3A%20Langevin%20Flows%20for%20Modeling%20Neural%20Latent%20Dynamics%0AAuthor%3A%20Yue%20Song%20and%20T.%20Anderson%20Keller%20and%20Yisong%20Yue%20and%20Pietro%20Perona%20and%20Max%20Welling%0AAbstract%3A%20%20%20Neural%20populations%20exhibit%20latent%20dynamical%20structures%20that%20drive%0Atime-evolving%20spiking%20activities%2C%20motivating%20the%20search%20for%20models%20that%20capture%0Aboth%20intrinsic%20network%20dynamics%20and%20external%20unobserved%20influences.%20In%20this%0Awork%2C%20we%20introduce%20LangevinFlow%2C%20a%20sequential%20Variational%20Auto-Encoder%20where%0Athe%20time%20evolution%20of%20latent%20variables%20is%20governed%20by%20the%20underdamped%20Langevin%0Aequation.%20Our%20approach%20incorporates%20physical%20priors%20--%20such%20as%20inertia%2C%0Adamping%2C%20a%20learned%20potential%20function%2C%20and%20stochastic%20forces%20--%20to%20represent%0Aboth%20autonomous%20and%20non-autonomous%20processes%20in%20neural%20systems.%20Crucially%2C%20the%0Apotential%20function%20is%20parameterized%20as%20a%20network%20of%20locally%20coupled%0Aoscillators%2C%20biasing%20the%20model%20toward%20oscillatory%20and%20flow-like%20behaviors%0Aobserved%20in%20biological%20neural%20populations.%20Our%20model%20features%20a%20recurrent%0Aencoder%2C%20a%20one-layer%20Transformer%20decoder%2C%20and%20Langevin%20dynamics%20in%20the%20latent%0Aspace.%20Empirically%2C%20our%20method%20outperforms%20state-of-the-art%20baselines%20on%0Asynthetic%20neural%20populations%20generated%20by%20a%20Lorenz%20attractor%2C%20closely%20matching%0Aground-truth%20firing%20rates.%20On%20the%20Neural%20Latents%20Benchmark%20%28NLB%29%2C%20the%20model%0Aachieves%20superior%20held-out%20neuron%20likelihoods%20%28bits%20per%20spike%29%20and%20forward%0Aprediction%20accuracy%20across%20four%20challenging%20datasets.%20It%20also%20matches%20or%0Asurpasses%20alternative%20methods%20in%20decoding%20behavioral%20metrics%20such%20as%20hand%0Avelocity.%20Overall%2C%20this%20work%20introduces%20a%20flexible%2C%20physics-inspired%2C%0Ahigh-performing%20framework%20for%20modeling%20complex%20neural%20population%20dynamics%20and%0Atheir%20unobserved%20influences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11531v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLangevin%2520Flows%2520for%2520Modeling%2520Neural%2520Latent%2520Dynamics%26entry.906535625%3DYue%2520Song%2520and%2520T.%2520Anderson%2520Keller%2520and%2520Yisong%2520Yue%2520and%2520Pietro%2520Perona%2520and%2520Max%2520Welling%26entry.1292438233%3D%2520%2520Neural%2520populations%2520exhibit%2520latent%2520dynamical%2520structures%2520that%2520drive%250Atime-evolving%2520spiking%2520activities%252C%2520motivating%2520the%2520search%2520for%2520models%2520that%2520capture%250Aboth%2520intrinsic%2520network%2520dynamics%2520and%2520external%2520unobserved%2520influences.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520LangevinFlow%252C%2520a%2520sequential%2520Variational%2520Auto-Encoder%2520where%250Athe%2520time%2520evolution%2520of%2520latent%2520variables%2520is%2520governed%2520by%2520the%2520underdamped%2520Langevin%250Aequation.%2520Our%2520approach%2520incorporates%2520physical%2520priors%2520--%2520such%2520as%2520inertia%252C%250Adamping%252C%2520a%2520learned%2520potential%2520function%252C%2520and%2520stochastic%2520forces%2520--%2520to%2520represent%250Aboth%2520autonomous%2520and%2520non-autonomous%2520processes%2520in%2520neural%2520systems.%2520Crucially%252C%2520the%250Apotential%2520function%2520is%2520parameterized%2520as%2520a%2520network%2520of%2520locally%2520coupled%250Aoscillators%252C%2520biasing%2520the%2520model%2520toward%2520oscillatory%2520and%2520flow-like%2520behaviors%250Aobserved%2520in%2520biological%2520neural%2520populations.%2520Our%2520model%2520features%2520a%2520recurrent%250Aencoder%252C%2520a%2520one-layer%2520Transformer%2520decoder%252C%2520and%2520Langevin%2520dynamics%2520in%2520the%2520latent%250Aspace.%2520Empirically%252C%2520our%2520method%2520outperforms%2520state-of-the-art%2520baselines%2520on%250Asynthetic%2520neural%2520populations%2520generated%2520by%2520a%2520Lorenz%2520attractor%252C%2520closely%2520matching%250Aground-truth%2520firing%2520rates.%2520On%2520the%2520Neural%2520Latents%2520Benchmark%2520%2528NLB%2529%252C%2520the%2520model%250Aachieves%2520superior%2520held-out%2520neuron%2520likelihoods%2520%2528bits%2520per%2520spike%2529%2520and%2520forward%250Aprediction%2520accuracy%2520across%2520four%2520challenging%2520datasets.%2520It%2520also%2520matches%2520or%250Asurpasses%2520alternative%2520methods%2520in%2520decoding%2520behavioral%2520metrics%2520such%2520as%2520hand%250Avelocity.%2520Overall%252C%2520this%2520work%2520introduces%2520a%2520flexible%252C%2520physics-inspired%252C%250Ahigh-performing%2520framework%2520for%2520modeling%2520complex%2520neural%2520population%2520dynamics%2520and%250Atheir%2520unobserved%2520influences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11531v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Langevin%20Flows%20for%20Modeling%20Neural%20Latent%20Dynamics&entry.906535625=Yue%20Song%20and%20T.%20Anderson%20Keller%20and%20Yisong%20Yue%20and%20Pietro%20Perona%20and%20Max%20Welling&entry.1292438233=%20%20Neural%20populations%20exhibit%20latent%20dynamical%20structures%20that%20drive%0Atime-evolving%20spiking%20activities%2C%20motivating%20the%20search%20for%20models%20that%20capture%0Aboth%20intrinsic%20network%20dynamics%20and%20external%20unobserved%20influences.%20In%20this%0Awork%2C%20we%20introduce%20LangevinFlow%2C%20a%20sequential%20Variational%20Auto-Encoder%20where%0Athe%20time%20evolution%20of%20latent%20variables%20is%20governed%20by%20the%20underdamped%20Langevin%0Aequation.%20Our%20approach%20incorporates%20physical%20priors%20--%20such%20as%20inertia%2C%0Adamping%2C%20a%20learned%20potential%20function%2C%20and%20stochastic%20forces%20--%20to%20represent%0Aboth%20autonomous%20and%20non-autonomous%20processes%20in%20neural%20systems.%20Crucially%2C%20the%0Apotential%20function%20is%20parameterized%20as%20a%20network%20of%20locally%20coupled%0Aoscillators%2C%20biasing%20the%20model%20toward%20oscillatory%20and%20flow-like%20behaviors%0Aobserved%20in%20biological%20neural%20populations.%20Our%20model%20features%20a%20recurrent%0Aencoder%2C%20a%20one-layer%20Transformer%20decoder%2C%20and%20Langevin%20dynamics%20in%20the%20latent%0Aspace.%20Empirically%2C%20our%20method%20outperforms%20state-of-the-art%20baselines%20on%0Asynthetic%20neural%20populations%20generated%20by%20a%20Lorenz%20attractor%2C%20closely%20matching%0Aground-truth%20firing%20rates.%20On%20the%20Neural%20Latents%20Benchmark%20%28NLB%29%2C%20the%20model%0Aachieves%20superior%20held-out%20neuron%20likelihoods%20%28bits%20per%20spike%29%20and%20forward%0Aprediction%20accuracy%20across%20four%20challenging%20datasets.%20It%20also%20matches%20or%0Asurpasses%20alternative%20methods%20in%20decoding%20behavioral%20metrics%20such%20as%20hand%0Avelocity.%20Overall%2C%20this%20work%20introduces%20a%20flexible%2C%20physics-inspired%2C%0Ahigh-performing%20framework%20for%20modeling%20complex%20neural%20population%20dynamics%20and%0Atheir%20unobserved%20influences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11531v1&entry.124074799=Read"},
{"title": "ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication\n  Protocols", "author": "Arnav Sheth and Ivaxi Sheth and Mario Fritz", "abstract": "  Recent advances in large language models (LLMs) have demonstrated strong\nperformance in generating code for general-purpose programming languages.\nHowever, their potential for hardware description languages (HDLs), such as\nSystemVerilog, remains largely unexplored. HDL code generation poses unique\nchallenges due to strict timing semantics, concurrency, and synthesizability\nconstraints essential for correct hardware functionality. Further, HDL-based\ndesign flows encompass a broad set of tasks beyond structural code generation,\nincluding testbench development, assertion-based verification, timing closure,\nand protocol-level integration for on-chip communication. In this work, we\nevaluate the capabilities of both open-source and state-of-the-art LLMs in\ngenerating synthesizable and functionally accurate SystemVerilog\nimplementations of widely used communication protocols that are critical\ncomponents of embedded and System-on-Chip (SoC) systems. We introduce\nProtocolLLM, the first benchmark suite specifically targeting these protocols\nwith tasks spanning multiple design abstraction levels and varying prompt\nspecificity. Our evaluation method also focuses on timing correctness in\naddition to synthesizability and syntactic correctness. We observe that most of\nthe models fail to generate SystemVerilog code for communication protocols that\nfollow timing constrains.\n", "link": "http://arxiv.org/abs/2506.07945v2", "date": "2025-07-15", "relevancy": 1.6175, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4074}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4074}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProtocolLLM%3A%20RTL%20Benchmark%20for%20SystemVerilog%20Generation%20of%20Communication%0A%20%20Protocols&body=Title%3A%20ProtocolLLM%3A%20RTL%20Benchmark%20for%20SystemVerilog%20Generation%20of%20Communication%0A%20%20Protocols%0AAuthor%3A%20Arnav%20Sheth%20and%20Ivaxi%20Sheth%20and%20Mario%20Fritz%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20strong%0Aperformance%20in%20generating%20code%20for%20general-purpose%20programming%20languages.%0AHowever%2C%20their%20potential%20for%20hardware%20description%20languages%20%28HDLs%29%2C%20such%20as%0ASystemVerilog%2C%20remains%20largely%20unexplored.%20HDL%20code%20generation%20poses%20unique%0Achallenges%20due%20to%20strict%20timing%20semantics%2C%20concurrency%2C%20and%20synthesizability%0Aconstraints%20essential%20for%20correct%20hardware%20functionality.%20Further%2C%20HDL-based%0Adesign%20flows%20encompass%20a%20broad%20set%20of%20tasks%20beyond%20structural%20code%20generation%2C%0Aincluding%20testbench%20development%2C%20assertion-based%20verification%2C%20timing%20closure%2C%0Aand%20protocol-level%20integration%20for%20on-chip%20communication.%20In%20this%20work%2C%20we%0Aevaluate%20the%20capabilities%20of%20both%20open-source%20and%20state-of-the-art%20LLMs%20in%0Agenerating%20synthesizable%20and%20functionally%20accurate%20SystemVerilog%0Aimplementations%20of%20widely%20used%20communication%20protocols%20that%20are%20critical%0Acomponents%20of%20embedded%20and%20System-on-Chip%20%28SoC%29%20systems.%20We%20introduce%0AProtocolLLM%2C%20the%20first%20benchmark%20suite%20specifically%20targeting%20these%20protocols%0Awith%20tasks%20spanning%20multiple%20design%20abstraction%20levels%20and%20varying%20prompt%0Aspecificity.%20Our%20evaluation%20method%20also%20focuses%20on%20timing%20correctness%20in%0Aaddition%20to%20synthesizability%20and%20syntactic%20correctness.%20We%20observe%20that%20most%20of%0Athe%20models%20fail%20to%20generate%20SystemVerilog%20code%20for%20communication%20protocols%20that%0Afollow%20timing%20constrains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.07945v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProtocolLLM%253A%2520RTL%2520Benchmark%2520for%2520SystemVerilog%2520Generation%2520of%2520Communication%250A%2520%2520Protocols%26entry.906535625%3DArnav%2520Sheth%2520and%2520Ivaxi%2520Sheth%2520and%2520Mario%2520Fritz%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520strong%250Aperformance%2520in%2520generating%2520code%2520for%2520general-purpose%2520programming%2520languages.%250AHowever%252C%2520their%2520potential%2520for%2520hardware%2520description%2520languages%2520%2528HDLs%2529%252C%2520such%2520as%250ASystemVerilog%252C%2520remains%2520largely%2520unexplored.%2520HDL%2520code%2520generation%2520poses%2520unique%250Achallenges%2520due%2520to%2520strict%2520timing%2520semantics%252C%2520concurrency%252C%2520and%2520synthesizability%250Aconstraints%2520essential%2520for%2520correct%2520hardware%2520functionality.%2520Further%252C%2520HDL-based%250Adesign%2520flows%2520encompass%2520a%2520broad%2520set%2520of%2520tasks%2520beyond%2520structural%2520code%2520generation%252C%250Aincluding%2520testbench%2520development%252C%2520assertion-based%2520verification%252C%2520timing%2520closure%252C%250Aand%2520protocol-level%2520integration%2520for%2520on-chip%2520communication.%2520In%2520this%2520work%252C%2520we%250Aevaluate%2520the%2520capabilities%2520of%2520both%2520open-source%2520and%2520state-of-the-art%2520LLMs%2520in%250Agenerating%2520synthesizable%2520and%2520functionally%2520accurate%2520SystemVerilog%250Aimplementations%2520of%2520widely%2520used%2520communication%2520protocols%2520that%2520are%2520critical%250Acomponents%2520of%2520embedded%2520and%2520System-on-Chip%2520%2528SoC%2529%2520systems.%2520We%2520introduce%250AProtocolLLM%252C%2520the%2520first%2520benchmark%2520suite%2520specifically%2520targeting%2520these%2520protocols%250Awith%2520tasks%2520spanning%2520multiple%2520design%2520abstraction%2520levels%2520and%2520varying%2520prompt%250Aspecificity.%2520Our%2520evaluation%2520method%2520also%2520focuses%2520on%2520timing%2520correctness%2520in%250Aaddition%2520to%2520synthesizability%2520and%2520syntactic%2520correctness.%2520We%2520observe%2520that%2520most%2520of%250Athe%2520models%2520fail%2520to%2520generate%2520SystemVerilog%2520code%2520for%2520communication%2520protocols%2520that%250Afollow%2520timing%2520constrains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.07945v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProtocolLLM%3A%20RTL%20Benchmark%20for%20SystemVerilog%20Generation%20of%20Communication%0A%20%20Protocols&entry.906535625=Arnav%20Sheth%20and%20Ivaxi%20Sheth%20and%20Mario%20Fritz&entry.1292438233=%20%20Recent%20advances%20in%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%20strong%0Aperformance%20in%20generating%20code%20for%20general-purpose%20programming%20languages.%0AHowever%2C%20their%20potential%20for%20hardware%20description%20languages%20%28HDLs%29%2C%20such%20as%0ASystemVerilog%2C%20remains%20largely%20unexplored.%20HDL%20code%20generation%20poses%20unique%0Achallenges%20due%20to%20strict%20timing%20semantics%2C%20concurrency%2C%20and%20synthesizability%0Aconstraints%20essential%20for%20correct%20hardware%20functionality.%20Further%2C%20HDL-based%0Adesign%20flows%20encompass%20a%20broad%20set%20of%20tasks%20beyond%20structural%20code%20generation%2C%0Aincluding%20testbench%20development%2C%20assertion-based%20verification%2C%20timing%20closure%2C%0Aand%20protocol-level%20integration%20for%20on-chip%20communication.%20In%20this%20work%2C%20we%0Aevaluate%20the%20capabilities%20of%20both%20open-source%20and%20state-of-the-art%20LLMs%20in%0Agenerating%20synthesizable%20and%20functionally%20accurate%20SystemVerilog%0Aimplementations%20of%20widely%20used%20communication%20protocols%20that%20are%20critical%0Acomponents%20of%20embedded%20and%20System-on-Chip%20%28SoC%29%20systems.%20We%20introduce%0AProtocolLLM%2C%20the%20first%20benchmark%20suite%20specifically%20targeting%20these%20protocols%0Awith%20tasks%20spanning%20multiple%20design%20abstraction%20levels%20and%20varying%20prompt%0Aspecificity.%20Our%20evaluation%20method%20also%20focuses%20on%20timing%20correctness%20in%0Aaddition%20to%20synthesizability%20and%20syntactic%20correctness.%20We%20observe%20that%20most%20of%0Athe%20models%20fail%20to%20generate%20SystemVerilog%20code%20for%20communication%20protocols%20that%0Afollow%20timing%20constrains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.07945v2&entry.124074799=Read"},
{"title": "Overcoming Slow Decision Frequencies in Continuous Control: Model-Based\n  Sequence Reinforcement Learning for Model-Free Control", "author": "Devdhar Patel and Hava Siegelmann", "abstract": "  Reinforcement learning (RL) is rapidly reaching and surpassing human-level\ncontrol capabilities. However, state-of-the-art RL algorithms often require\ntimesteps and reaction times significantly faster than human capabilities,\nwhich is impractical in real-world settings and typically necessitates\nspecialized hardware. We introduce Sequence Reinforcement Learning (SRL), an RL\nalgorithm designed to produce a sequence of actions for a given input state,\nenabling effective control at lower decision frequencies. SRL addresses the\nchallenges of learning action sequences by employing both a model and an\nactor-critic architecture operating at different temporal scales. We propose a\n\"temporal recall\" mechanism, where the critic uses the model to estimate\nintermediate states between primitive actions, providing a learning signal for\neach individual action within the sequence. Once training is complete, the\nactor can generate action sequences independently of the model, achieving\nmodel-free control at a slower frequency. We evaluate SRL on a suite of\ncontinuous control tasks, demonstrating that it achieves performance comparable\nto state-of-the-art algorithms while significantly reducing actor sample\ncomplexity. To better assess performance across varying decision frequencies,\nwe introduce the Frequency-Averaged Score (FAS) metric. Our results show that\nSRL significantly outperforms traditional RL algorithms in terms of FAS, making\nit particularly suitable for applications requiring variable decision\nfrequencies. Furthermore, we compare SRL with model-based online planning,\nshowing that SRL achieves comparable FAS while leveraging the same model during\ntraining that online planners use for planning.\n", "link": "http://arxiv.org/abs/2410.08979v4", "date": "2025-07-15", "relevancy": 1.5112, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5108}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4952}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overcoming%20Slow%20Decision%20Frequencies%20in%20Continuous%20Control%3A%20Model-Based%0A%20%20Sequence%20Reinforcement%20Learning%20for%20Model-Free%20Control&body=Title%3A%20Overcoming%20Slow%20Decision%20Frequencies%20in%20Continuous%20Control%3A%20Model-Based%0A%20%20Sequence%20Reinforcement%20Learning%20for%20Model-Free%20Control%0AAuthor%3A%20Devdhar%20Patel%20and%20Hava%20Siegelmann%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20is%20rapidly%20reaching%20and%20surpassing%20human-level%0Acontrol%20capabilities.%20However%2C%20state-of-the-art%20RL%20algorithms%20often%20require%0Atimesteps%20and%20reaction%20times%20significantly%20faster%20than%20human%20capabilities%2C%0Awhich%20is%20impractical%20in%20real-world%20settings%20and%20typically%20necessitates%0Aspecialized%20hardware.%20We%20introduce%20Sequence%20Reinforcement%20Learning%20%28SRL%29%2C%20an%20RL%0Aalgorithm%20designed%20to%20produce%20a%20sequence%20of%20actions%20for%20a%20given%20input%20state%2C%0Aenabling%20effective%20control%20at%20lower%20decision%20frequencies.%20SRL%20addresses%20the%0Achallenges%20of%20learning%20action%20sequences%20by%20employing%20both%20a%20model%20and%20an%0Aactor-critic%20architecture%20operating%20at%20different%20temporal%20scales.%20We%20propose%20a%0A%22temporal%20recall%22%20mechanism%2C%20where%20the%20critic%20uses%20the%20model%20to%20estimate%0Aintermediate%20states%20between%20primitive%20actions%2C%20providing%20a%20learning%20signal%20for%0Aeach%20individual%20action%20within%20the%20sequence.%20Once%20training%20is%20complete%2C%20the%0Aactor%20can%20generate%20action%20sequences%20independently%20of%20the%20model%2C%20achieving%0Amodel-free%20control%20at%20a%20slower%20frequency.%20We%20evaluate%20SRL%20on%20a%20suite%20of%0Acontinuous%20control%20tasks%2C%20demonstrating%20that%20it%20achieves%20performance%20comparable%0Ato%20state-of-the-art%20algorithms%20while%20significantly%20reducing%20actor%20sample%0Acomplexity.%20To%20better%20assess%20performance%20across%20varying%20decision%20frequencies%2C%0Awe%20introduce%20the%20Frequency-Averaged%20Score%20%28FAS%29%20metric.%20Our%20results%20show%20that%0ASRL%20significantly%20outperforms%20traditional%20RL%20algorithms%20in%20terms%20of%20FAS%2C%20making%0Ait%20particularly%20suitable%20for%20applications%20requiring%20variable%20decision%0Afrequencies.%20Furthermore%2C%20we%20compare%20SRL%20with%20model-based%20online%20planning%2C%0Ashowing%20that%20SRL%20achieves%20comparable%20FAS%20while%20leveraging%20the%20same%20model%20during%0Atraining%20that%20online%20planners%20use%20for%20planning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08979v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOvercoming%2520Slow%2520Decision%2520Frequencies%2520in%2520Continuous%2520Control%253A%2520Model-Based%250A%2520%2520Sequence%2520Reinforcement%2520Learning%2520for%2520Model-Free%2520Control%26entry.906535625%3DDevdhar%2520Patel%2520and%2520Hava%2520Siegelmann%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520is%2520rapidly%2520reaching%2520and%2520surpassing%2520human-level%250Acontrol%2520capabilities.%2520However%252C%2520state-of-the-art%2520RL%2520algorithms%2520often%2520require%250Atimesteps%2520and%2520reaction%2520times%2520significantly%2520faster%2520than%2520human%2520capabilities%252C%250Awhich%2520is%2520impractical%2520in%2520real-world%2520settings%2520and%2520typically%2520necessitates%250Aspecialized%2520hardware.%2520We%2520introduce%2520Sequence%2520Reinforcement%2520Learning%2520%2528SRL%2529%252C%2520an%2520RL%250Aalgorithm%2520designed%2520to%2520produce%2520a%2520sequence%2520of%2520actions%2520for%2520a%2520given%2520input%2520state%252C%250Aenabling%2520effective%2520control%2520at%2520lower%2520decision%2520frequencies.%2520SRL%2520addresses%2520the%250Achallenges%2520of%2520learning%2520action%2520sequences%2520by%2520employing%2520both%2520a%2520model%2520and%2520an%250Aactor-critic%2520architecture%2520operating%2520at%2520different%2520temporal%2520scales.%2520We%2520propose%2520a%250A%2522temporal%2520recall%2522%2520mechanism%252C%2520where%2520the%2520critic%2520uses%2520the%2520model%2520to%2520estimate%250Aintermediate%2520states%2520between%2520primitive%2520actions%252C%2520providing%2520a%2520learning%2520signal%2520for%250Aeach%2520individual%2520action%2520within%2520the%2520sequence.%2520Once%2520training%2520is%2520complete%252C%2520the%250Aactor%2520can%2520generate%2520action%2520sequences%2520independently%2520of%2520the%2520model%252C%2520achieving%250Amodel-free%2520control%2520at%2520a%2520slower%2520frequency.%2520We%2520evaluate%2520SRL%2520on%2520a%2520suite%2520of%250Acontinuous%2520control%2520tasks%252C%2520demonstrating%2520that%2520it%2520achieves%2520performance%2520comparable%250Ato%2520state-of-the-art%2520algorithms%2520while%2520significantly%2520reducing%2520actor%2520sample%250Acomplexity.%2520To%2520better%2520assess%2520performance%2520across%2520varying%2520decision%2520frequencies%252C%250Awe%2520introduce%2520the%2520Frequency-Averaged%2520Score%2520%2528FAS%2529%2520metric.%2520Our%2520results%2520show%2520that%250ASRL%2520significantly%2520outperforms%2520traditional%2520RL%2520algorithms%2520in%2520terms%2520of%2520FAS%252C%2520making%250Ait%2520particularly%2520suitable%2520for%2520applications%2520requiring%2520variable%2520decision%250Afrequencies.%2520Furthermore%252C%2520we%2520compare%2520SRL%2520with%2520model-based%2520online%2520planning%252C%250Ashowing%2520that%2520SRL%2520achieves%2520comparable%2520FAS%2520while%2520leveraging%2520the%2520same%2520model%2520during%250Atraining%2520that%2520online%2520planners%2520use%2520for%2520planning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08979v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overcoming%20Slow%20Decision%20Frequencies%20in%20Continuous%20Control%3A%20Model-Based%0A%20%20Sequence%20Reinforcement%20Learning%20for%20Model-Free%20Control&entry.906535625=Devdhar%20Patel%20and%20Hava%20Siegelmann&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20is%20rapidly%20reaching%20and%20surpassing%20human-level%0Acontrol%20capabilities.%20However%2C%20state-of-the-art%20RL%20algorithms%20often%20require%0Atimesteps%20and%20reaction%20times%20significantly%20faster%20than%20human%20capabilities%2C%0Awhich%20is%20impractical%20in%20real-world%20settings%20and%20typically%20necessitates%0Aspecialized%20hardware.%20We%20introduce%20Sequence%20Reinforcement%20Learning%20%28SRL%29%2C%20an%20RL%0Aalgorithm%20designed%20to%20produce%20a%20sequence%20of%20actions%20for%20a%20given%20input%20state%2C%0Aenabling%20effective%20control%20at%20lower%20decision%20frequencies.%20SRL%20addresses%20the%0Achallenges%20of%20learning%20action%20sequences%20by%20employing%20both%20a%20model%20and%20an%0Aactor-critic%20architecture%20operating%20at%20different%20temporal%20scales.%20We%20propose%20a%0A%22temporal%20recall%22%20mechanism%2C%20where%20the%20critic%20uses%20the%20model%20to%20estimate%0Aintermediate%20states%20between%20primitive%20actions%2C%20providing%20a%20learning%20signal%20for%0Aeach%20individual%20action%20within%20the%20sequence.%20Once%20training%20is%20complete%2C%20the%0Aactor%20can%20generate%20action%20sequences%20independently%20of%20the%20model%2C%20achieving%0Amodel-free%20control%20at%20a%20slower%20frequency.%20We%20evaluate%20SRL%20on%20a%20suite%20of%0Acontinuous%20control%20tasks%2C%20demonstrating%20that%20it%20achieves%20performance%20comparable%0Ato%20state-of-the-art%20algorithms%20while%20significantly%20reducing%20actor%20sample%0Acomplexity.%20To%20better%20assess%20performance%20across%20varying%20decision%20frequencies%2C%0Awe%20introduce%20the%20Frequency-Averaged%20Score%20%28FAS%29%20metric.%20Our%20results%20show%20that%0ASRL%20significantly%20outperforms%20traditional%20RL%20algorithms%20in%20terms%20of%20FAS%2C%20making%0Ait%20particularly%20suitable%20for%20applications%20requiring%20variable%20decision%0Afrequencies.%20Furthermore%2C%20we%20compare%20SRL%20with%20model-based%20online%20planning%2C%0Ashowing%20that%20SRL%20achieves%20comparable%20FAS%20while%20leveraging%20the%20same%20model%20during%0Atraining%20that%20online%20planners%20use%20for%20planning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08979v4&entry.124074799=Read"},
{"title": "Human-Robot collaboration in surgery: Advances and challenges towards\n  autonomous surgical assistants", "author": "Jacinto Colan and Ana Davila and Yutaro Yamada and Yasuhisa Hasegawa", "abstract": "  Human-robot collaboration in surgery represents a significant area of\nresearch, driven by the increasing capability of autonomous robotic systems to\nassist surgeons in complex procedures. This systematic review examines the\nadvancements and persistent challenges in the development of autonomous\nsurgical robotic assistants (ASARs), focusing specifically on scenarios where\nrobots provide meaningful and active support to human surgeons. Adhering to the\nPRISMA guidelines, a comprehensive literature search was conducted across the\nIEEE Xplore, Scopus, and Web of Science databases, resulting in the selection\nof 32 studies for detailed analysis. Two primary collaborative setups were\nidentified: teleoperation-based assistance and direct hands-on interaction. The\nfindings reveal a growing research emphasis on ASARs, with predominant\napplications currently in endoscope guidance, alongside emerging progress in\nautonomous tool manipulation. Several key challenges hinder wider adoption,\nincluding the alignment of robotic actions with human surgeon preferences, the\nnecessity for procedural awareness within autonomous systems, the establishment\nof seamless human-robot information exchange, and the complexities of skill\nacquisition in shared workspaces. This review synthesizes current trends,\nidentifies critical limitations, and outlines future research directions\nessential to improve the reliability, safety, and effectiveness of human-robot\ncollaboration in surgical environments.\n", "link": "http://arxiv.org/abs/2507.11460v1", "date": "2025-07-15", "relevancy": 1.5109, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5395}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5131}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4441}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Robot%20collaboration%20in%20surgery%3A%20Advances%20and%20challenges%20towards%0A%20%20autonomous%20surgical%20assistants&body=Title%3A%20Human-Robot%20collaboration%20in%20surgery%3A%20Advances%20and%20challenges%20towards%0A%20%20autonomous%20surgical%20assistants%0AAuthor%3A%20Jacinto%20Colan%20and%20Ana%20Davila%20and%20Yutaro%20Yamada%20and%20Yasuhisa%20Hasegawa%0AAbstract%3A%20%20%20Human-robot%20collaboration%20in%20surgery%20represents%20a%20significant%20area%20of%0Aresearch%2C%20driven%20by%20the%20increasing%20capability%20of%20autonomous%20robotic%20systems%20to%0Aassist%20surgeons%20in%20complex%20procedures.%20This%20systematic%20review%20examines%20the%0Aadvancements%20and%20persistent%20challenges%20in%20the%20development%20of%20autonomous%0Asurgical%20robotic%20assistants%20%28ASARs%29%2C%20focusing%20specifically%20on%20scenarios%20where%0Arobots%20provide%20meaningful%20and%20active%20support%20to%20human%20surgeons.%20Adhering%20to%20the%0APRISMA%20guidelines%2C%20a%20comprehensive%20literature%20search%20was%20conducted%20across%20the%0AIEEE%20Xplore%2C%20Scopus%2C%20and%20Web%20of%20Science%20databases%2C%20resulting%20in%20the%20selection%0Aof%2032%20studies%20for%20detailed%20analysis.%20Two%20primary%20collaborative%20setups%20were%0Aidentified%3A%20teleoperation-based%20assistance%20and%20direct%20hands-on%20interaction.%20The%0Afindings%20reveal%20a%20growing%20research%20emphasis%20on%20ASARs%2C%20with%20predominant%0Aapplications%20currently%20in%20endoscope%20guidance%2C%20alongside%20emerging%20progress%20in%0Aautonomous%20tool%20manipulation.%20Several%20key%20challenges%20hinder%20wider%20adoption%2C%0Aincluding%20the%20alignment%20of%20robotic%20actions%20with%20human%20surgeon%20preferences%2C%20the%0Anecessity%20for%20procedural%20awareness%20within%20autonomous%20systems%2C%20the%20establishment%0Aof%20seamless%20human-robot%20information%20exchange%2C%20and%20the%20complexities%20of%20skill%0Aacquisition%20in%20shared%20workspaces.%20This%20review%20synthesizes%20current%20trends%2C%0Aidentifies%20critical%20limitations%2C%20and%20outlines%20future%20research%20directions%0Aessential%20to%20improve%20the%20reliability%2C%20safety%2C%20and%20effectiveness%20of%20human-robot%0Acollaboration%20in%20surgical%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Robot%2520collaboration%2520in%2520surgery%253A%2520Advances%2520and%2520challenges%2520towards%250A%2520%2520autonomous%2520surgical%2520assistants%26entry.906535625%3DJacinto%2520Colan%2520and%2520Ana%2520Davila%2520and%2520Yutaro%2520Yamada%2520and%2520Yasuhisa%2520Hasegawa%26entry.1292438233%3D%2520%2520Human-robot%2520collaboration%2520in%2520surgery%2520represents%2520a%2520significant%2520area%2520of%250Aresearch%252C%2520driven%2520by%2520the%2520increasing%2520capability%2520of%2520autonomous%2520robotic%2520systems%2520to%250Aassist%2520surgeons%2520in%2520complex%2520procedures.%2520This%2520systematic%2520review%2520examines%2520the%250Aadvancements%2520and%2520persistent%2520challenges%2520in%2520the%2520development%2520of%2520autonomous%250Asurgical%2520robotic%2520assistants%2520%2528ASARs%2529%252C%2520focusing%2520specifically%2520on%2520scenarios%2520where%250Arobots%2520provide%2520meaningful%2520and%2520active%2520support%2520to%2520human%2520surgeons.%2520Adhering%2520to%2520the%250APRISMA%2520guidelines%252C%2520a%2520comprehensive%2520literature%2520search%2520was%2520conducted%2520across%2520the%250AIEEE%2520Xplore%252C%2520Scopus%252C%2520and%2520Web%2520of%2520Science%2520databases%252C%2520resulting%2520in%2520the%2520selection%250Aof%252032%2520studies%2520for%2520detailed%2520analysis.%2520Two%2520primary%2520collaborative%2520setups%2520were%250Aidentified%253A%2520teleoperation-based%2520assistance%2520and%2520direct%2520hands-on%2520interaction.%2520The%250Afindings%2520reveal%2520a%2520growing%2520research%2520emphasis%2520on%2520ASARs%252C%2520with%2520predominant%250Aapplications%2520currently%2520in%2520endoscope%2520guidance%252C%2520alongside%2520emerging%2520progress%2520in%250Aautonomous%2520tool%2520manipulation.%2520Several%2520key%2520challenges%2520hinder%2520wider%2520adoption%252C%250Aincluding%2520the%2520alignment%2520of%2520robotic%2520actions%2520with%2520human%2520surgeon%2520preferences%252C%2520the%250Anecessity%2520for%2520procedural%2520awareness%2520within%2520autonomous%2520systems%252C%2520the%2520establishment%250Aof%2520seamless%2520human-robot%2520information%2520exchange%252C%2520and%2520the%2520complexities%2520of%2520skill%250Aacquisition%2520in%2520shared%2520workspaces.%2520This%2520review%2520synthesizes%2520current%2520trends%252C%250Aidentifies%2520critical%2520limitations%252C%2520and%2520outlines%2520future%2520research%2520directions%250Aessential%2520to%2520improve%2520the%2520reliability%252C%2520safety%252C%2520and%2520effectiveness%2520of%2520human-robot%250Acollaboration%2520in%2520surgical%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Robot%20collaboration%20in%20surgery%3A%20Advances%20and%20challenges%20towards%0A%20%20autonomous%20surgical%20assistants&entry.906535625=Jacinto%20Colan%20and%20Ana%20Davila%20and%20Yutaro%20Yamada%20and%20Yasuhisa%20Hasegawa&entry.1292438233=%20%20Human-robot%20collaboration%20in%20surgery%20represents%20a%20significant%20area%20of%0Aresearch%2C%20driven%20by%20the%20increasing%20capability%20of%20autonomous%20robotic%20systems%20to%0Aassist%20surgeons%20in%20complex%20procedures.%20This%20systematic%20review%20examines%20the%0Aadvancements%20and%20persistent%20challenges%20in%20the%20development%20of%20autonomous%0Asurgical%20robotic%20assistants%20%28ASARs%29%2C%20focusing%20specifically%20on%20scenarios%20where%0Arobots%20provide%20meaningful%20and%20active%20support%20to%20human%20surgeons.%20Adhering%20to%20the%0APRISMA%20guidelines%2C%20a%20comprehensive%20literature%20search%20was%20conducted%20across%20the%0AIEEE%20Xplore%2C%20Scopus%2C%20and%20Web%20of%20Science%20databases%2C%20resulting%20in%20the%20selection%0Aof%2032%20studies%20for%20detailed%20analysis.%20Two%20primary%20collaborative%20setups%20were%0Aidentified%3A%20teleoperation-based%20assistance%20and%20direct%20hands-on%20interaction.%20The%0Afindings%20reveal%20a%20growing%20research%20emphasis%20on%20ASARs%2C%20with%20predominant%0Aapplications%20currently%20in%20endoscope%20guidance%2C%20alongside%20emerging%20progress%20in%0Aautonomous%20tool%20manipulation.%20Several%20key%20challenges%20hinder%20wider%20adoption%2C%0Aincluding%20the%20alignment%20of%20robotic%20actions%20with%20human%20surgeon%20preferences%2C%20the%0Anecessity%20for%20procedural%20awareness%20within%20autonomous%20systems%2C%20the%20establishment%0Aof%20seamless%20human-robot%20information%20exchange%2C%20and%20the%20complexities%20of%20skill%0Aacquisition%20in%20shared%20workspaces.%20This%20review%20synthesizes%20current%20trends%2C%0Aidentifies%20critical%20limitations%2C%20and%20outlines%20future%20research%20directions%0Aessential%20to%20improve%20the%20reliability%2C%20safety%2C%20and%20effectiveness%20of%20human-robot%0Acollaboration%20in%20surgical%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11460v1&entry.124074799=Read"},
{"title": "Illuminating the Three Dogmas of Reinforcement Learning under\n  Evolutionary Light", "author": "Mani Hamidi and Terrence W. Deacon", "abstract": "  Three core tenets of reinforcement learning (RL)--concerning the definition\nof agency, the objective of learning, and the scope of the reward\nhypothesis--have been highlighted as key targets for conceptual revision, with\nmajor implications for theory and application. We propose a framework, inspired\nby open-ended evolutionary theory, to reconsider these three \"dogmas.\" We\nrevisit each assumption and address related concerns raised alongside them. To\nmake our arguments relevant to RL as a model of biological learning, we first\nestablish that evolutionary dynamics can plausibly operate within living brains\nover an individual's lifetime, and are not confined to cross-generational\nprocesses. We begin by revisiting the second dogma, drawing on evolutionary\ninsights to enrich the \"adaptation-rather-than-search\" view of learning. We\nthen address the third dogma regarding the limits of the reward hypothesis,\nusing analogies from evolutionary fitness to illuminate the scalar reward vs.\nmulti-objective debate. After discussing practical implications for exploration\nin RL, we turn to the first--and arguably most fundamental--issue: the absence\nof a formal account of agency. We argue that unlike the other two problems, the\nevolutionary paradigm alone cannot resolve the agency question, though it\ngestures in a productive direction. We advocate integrating ideas from\norigins-of-life theory, where the thermodynamics of sustenance and replication\noffer promising foundations for understanding agency and resource-constrained\nreinforcement learning in biological systems.\n", "link": "http://arxiv.org/abs/2507.11482v1", "date": "2025-07-15", "relevancy": 1.4616, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.491}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4882}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Illuminating%20the%20Three%20Dogmas%20of%20Reinforcement%20Learning%20under%0A%20%20Evolutionary%20Light&body=Title%3A%20Illuminating%20the%20Three%20Dogmas%20of%20Reinforcement%20Learning%20under%0A%20%20Evolutionary%20Light%0AAuthor%3A%20Mani%20Hamidi%20and%20Terrence%20W.%20Deacon%0AAbstract%3A%20%20%20Three%20core%20tenets%20of%20reinforcement%20learning%20%28RL%29--concerning%20the%20definition%0Aof%20agency%2C%20the%20objective%20of%20learning%2C%20and%20the%20scope%20of%20the%20reward%0Ahypothesis--have%20been%20highlighted%20as%20key%20targets%20for%20conceptual%20revision%2C%20with%0Amajor%20implications%20for%20theory%20and%20application.%20We%20propose%20a%20framework%2C%20inspired%0Aby%20open-ended%20evolutionary%20theory%2C%20to%20reconsider%20these%20three%20%22dogmas.%22%20We%0Arevisit%20each%20assumption%20and%20address%20related%20concerns%20raised%20alongside%20them.%20To%0Amake%20our%20arguments%20relevant%20to%20RL%20as%20a%20model%20of%20biological%20learning%2C%20we%20first%0Aestablish%20that%20evolutionary%20dynamics%20can%20plausibly%20operate%20within%20living%20brains%0Aover%20an%20individual%27s%20lifetime%2C%20and%20are%20not%20confined%20to%20cross-generational%0Aprocesses.%20We%20begin%20by%20revisiting%20the%20second%20dogma%2C%20drawing%20on%20evolutionary%0Ainsights%20to%20enrich%20the%20%22adaptation-rather-than-search%22%20view%20of%20learning.%20We%0Athen%20address%20the%20third%20dogma%20regarding%20the%20limits%20of%20the%20reward%20hypothesis%2C%0Ausing%20analogies%20from%20evolutionary%20fitness%20to%20illuminate%20the%20scalar%20reward%20vs.%0Amulti-objective%20debate.%20After%20discussing%20practical%20implications%20for%20exploration%0Ain%20RL%2C%20we%20turn%20to%20the%20first--and%20arguably%20most%20fundamental--issue%3A%20the%20absence%0Aof%20a%20formal%20account%20of%20agency.%20We%20argue%20that%20unlike%20the%20other%20two%20problems%2C%20the%0Aevolutionary%20paradigm%20alone%20cannot%20resolve%20the%20agency%20question%2C%20though%20it%0Agestures%20in%20a%20productive%20direction.%20We%20advocate%20integrating%20ideas%20from%0Aorigins-of-life%20theory%2C%20where%20the%20thermodynamics%20of%20sustenance%20and%20replication%0Aoffer%20promising%20foundations%20for%20understanding%20agency%20and%20resource-constrained%0Areinforcement%20learning%20in%20biological%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIlluminating%2520the%2520Three%2520Dogmas%2520of%2520Reinforcement%2520Learning%2520under%250A%2520%2520Evolutionary%2520Light%26entry.906535625%3DMani%2520Hamidi%2520and%2520Terrence%2520W.%2520Deacon%26entry.1292438233%3D%2520%2520Three%2520core%2520tenets%2520of%2520reinforcement%2520learning%2520%2528RL%2529--concerning%2520the%2520definition%250Aof%2520agency%252C%2520the%2520objective%2520of%2520learning%252C%2520and%2520the%2520scope%2520of%2520the%2520reward%250Ahypothesis--have%2520been%2520highlighted%2520as%2520key%2520targets%2520for%2520conceptual%2520revision%252C%2520with%250Amajor%2520implications%2520for%2520theory%2520and%2520application.%2520We%2520propose%2520a%2520framework%252C%2520inspired%250Aby%2520open-ended%2520evolutionary%2520theory%252C%2520to%2520reconsider%2520these%2520three%2520%2522dogmas.%2522%2520We%250Arevisit%2520each%2520assumption%2520and%2520address%2520related%2520concerns%2520raised%2520alongside%2520them.%2520To%250Amake%2520our%2520arguments%2520relevant%2520to%2520RL%2520as%2520a%2520model%2520of%2520biological%2520learning%252C%2520we%2520first%250Aestablish%2520that%2520evolutionary%2520dynamics%2520can%2520plausibly%2520operate%2520within%2520living%2520brains%250Aover%2520an%2520individual%2527s%2520lifetime%252C%2520and%2520are%2520not%2520confined%2520to%2520cross-generational%250Aprocesses.%2520We%2520begin%2520by%2520revisiting%2520the%2520second%2520dogma%252C%2520drawing%2520on%2520evolutionary%250Ainsights%2520to%2520enrich%2520the%2520%2522adaptation-rather-than-search%2522%2520view%2520of%2520learning.%2520We%250Athen%2520address%2520the%2520third%2520dogma%2520regarding%2520the%2520limits%2520of%2520the%2520reward%2520hypothesis%252C%250Ausing%2520analogies%2520from%2520evolutionary%2520fitness%2520to%2520illuminate%2520the%2520scalar%2520reward%2520vs.%250Amulti-objective%2520debate.%2520After%2520discussing%2520practical%2520implications%2520for%2520exploration%250Ain%2520RL%252C%2520we%2520turn%2520to%2520the%2520first--and%2520arguably%2520most%2520fundamental--issue%253A%2520the%2520absence%250Aof%2520a%2520formal%2520account%2520of%2520agency.%2520We%2520argue%2520that%2520unlike%2520the%2520other%2520two%2520problems%252C%2520the%250Aevolutionary%2520paradigm%2520alone%2520cannot%2520resolve%2520the%2520agency%2520question%252C%2520though%2520it%250Agestures%2520in%2520a%2520productive%2520direction.%2520We%2520advocate%2520integrating%2520ideas%2520from%250Aorigins-of-life%2520theory%252C%2520where%2520the%2520thermodynamics%2520of%2520sustenance%2520and%2520replication%250Aoffer%2520promising%2520foundations%2520for%2520understanding%2520agency%2520and%2520resource-constrained%250Areinforcement%2520learning%2520in%2520biological%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Illuminating%20the%20Three%20Dogmas%20of%20Reinforcement%20Learning%20under%0A%20%20Evolutionary%20Light&entry.906535625=Mani%20Hamidi%20and%20Terrence%20W.%20Deacon&entry.1292438233=%20%20Three%20core%20tenets%20of%20reinforcement%20learning%20%28RL%29--concerning%20the%20definition%0Aof%20agency%2C%20the%20objective%20of%20learning%2C%20and%20the%20scope%20of%20the%20reward%0Ahypothesis--have%20been%20highlighted%20as%20key%20targets%20for%20conceptual%20revision%2C%20with%0Amajor%20implications%20for%20theory%20and%20application.%20We%20propose%20a%20framework%2C%20inspired%0Aby%20open-ended%20evolutionary%20theory%2C%20to%20reconsider%20these%20three%20%22dogmas.%22%20We%0Arevisit%20each%20assumption%20and%20address%20related%20concerns%20raised%20alongside%20them.%20To%0Amake%20our%20arguments%20relevant%20to%20RL%20as%20a%20model%20of%20biological%20learning%2C%20we%20first%0Aestablish%20that%20evolutionary%20dynamics%20can%20plausibly%20operate%20within%20living%20brains%0Aover%20an%20individual%27s%20lifetime%2C%20and%20are%20not%20confined%20to%20cross-generational%0Aprocesses.%20We%20begin%20by%20revisiting%20the%20second%20dogma%2C%20drawing%20on%20evolutionary%0Ainsights%20to%20enrich%20the%20%22adaptation-rather-than-search%22%20view%20of%20learning.%20We%0Athen%20address%20the%20third%20dogma%20regarding%20the%20limits%20of%20the%20reward%20hypothesis%2C%0Ausing%20analogies%20from%20evolutionary%20fitness%20to%20illuminate%20the%20scalar%20reward%20vs.%0Amulti-objective%20debate.%20After%20discussing%20practical%20implications%20for%20exploration%0Ain%20RL%2C%20we%20turn%20to%20the%20first--and%20arguably%20most%20fundamental--issue%3A%20the%20absence%0Aof%20a%20formal%20account%20of%20agency.%20We%20argue%20that%20unlike%20the%20other%20two%20problems%2C%20the%0Aevolutionary%20paradigm%20alone%20cannot%20resolve%20the%20agency%20question%2C%20though%20it%0Agestures%20in%20a%20productive%20direction.%20We%20advocate%20integrating%20ideas%20from%0Aorigins-of-life%20theory%2C%20where%20the%20thermodynamics%20of%20sustenance%20and%20replication%0Aoffer%20promising%20foundations%20for%20understanding%20agency%20and%20resource-constrained%0Areinforcement%20learning%20in%20biological%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11482v1&entry.124074799=Read"},
{"title": "Robot Drummer: Learning Rhythmic Skills for Humanoid Drumming", "author": "Asad Ali Shahid and Francesco Braghin and Loris Roveda", "abstract": "  Humanoid robots have seen remarkable advances in dexterity, balance, and\nlocomotion, yet their role in expressive domains, such as music performance,\nremains largely unexplored. Musical tasks, like drumming, present unique\nchallenges, including split-second timing, rapid contacts, and multi-limb\ncoordination over pieces lasting minutes. In this paper, we introduce Robot\nDrummer, a humanoid system capable of expressive, high-precision drumming\nacross a diverse repertoire of songs. We formulate humanoid drumming as\nsequential fulfillment of timed-contacts and transform drum scores in to a\nRhythmic Contact Chain. To handle the long-horizon nature of musical\nperformance, we decompose each piece into fixed-length segments and train a\nsingle policy across all segments in parallel using reinforcement learning.\nThrough extensive experiments on over thirty popular rock, metal, and jazz\ntracks, our results demonstrate that Robot Drummer consistently achieves high\nF1 scores. The learned behaviors exhibit emergent human-like drumming\nstrategies, such as cross-arm strikes, and adaptive sticks assignments,\ndemonstrating the potential of reinforcement learning to bring humanoid robots\ninto the domain of creative musical performance. Project page:\n\\href{https://robot-drummer.github.io}{robot-drummer.github.io}\n", "link": "http://arxiv.org/abs/2507.11498v1", "date": "2025-07-15", "relevancy": 1.4458, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5299}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5096}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robot%20Drummer%3A%20Learning%20Rhythmic%20Skills%20for%20Humanoid%20Drumming&body=Title%3A%20Robot%20Drummer%3A%20Learning%20Rhythmic%20Skills%20for%20Humanoid%20Drumming%0AAuthor%3A%20Asad%20Ali%20Shahid%20and%20Francesco%20Braghin%20and%20Loris%20Roveda%0AAbstract%3A%20%20%20Humanoid%20robots%20have%20seen%20remarkable%20advances%20in%20dexterity%2C%20balance%2C%20and%0Alocomotion%2C%20yet%20their%20role%20in%20expressive%20domains%2C%20such%20as%20music%20performance%2C%0Aremains%20largely%20unexplored.%20Musical%20tasks%2C%20like%20drumming%2C%20present%20unique%0Achallenges%2C%20including%20split-second%20timing%2C%20rapid%20contacts%2C%20and%20multi-limb%0Acoordination%20over%20pieces%20lasting%20minutes.%20In%20this%20paper%2C%20we%20introduce%20Robot%0ADrummer%2C%20a%20humanoid%20system%20capable%20of%20expressive%2C%20high-precision%20drumming%0Aacross%20a%20diverse%20repertoire%20of%20songs.%20We%20formulate%20humanoid%20drumming%20as%0Asequential%20fulfillment%20of%20timed-contacts%20and%20transform%20drum%20scores%20in%20to%20a%0ARhythmic%20Contact%20Chain.%20To%20handle%20the%20long-horizon%20nature%20of%20musical%0Aperformance%2C%20we%20decompose%20each%20piece%20into%20fixed-length%20segments%20and%20train%20a%0Asingle%20policy%20across%20all%20segments%20in%20parallel%20using%20reinforcement%20learning.%0AThrough%20extensive%20experiments%20on%20over%20thirty%20popular%20rock%2C%20metal%2C%20and%20jazz%0Atracks%2C%20our%20results%20demonstrate%20that%20Robot%20Drummer%20consistently%20achieves%20high%0AF1%20scores.%20The%20learned%20behaviors%20exhibit%20emergent%20human-like%20drumming%0Astrategies%2C%20such%20as%20cross-arm%20strikes%2C%20and%20adaptive%20sticks%20assignments%2C%0Ademonstrating%20the%20potential%20of%20reinforcement%20learning%20to%20bring%20humanoid%20robots%0Ainto%20the%20domain%20of%20creative%20musical%20performance.%20Project%20page%3A%0A%5Chref%7Bhttps%3A//robot-drummer.github.io%7D%7Brobot-drummer.github.io%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobot%2520Drummer%253A%2520Learning%2520Rhythmic%2520Skills%2520for%2520Humanoid%2520Drumming%26entry.906535625%3DAsad%2520Ali%2520Shahid%2520and%2520Francesco%2520Braghin%2520and%2520Loris%2520Roveda%26entry.1292438233%3D%2520%2520Humanoid%2520robots%2520have%2520seen%2520remarkable%2520advances%2520in%2520dexterity%252C%2520balance%252C%2520and%250Alocomotion%252C%2520yet%2520their%2520role%2520in%2520expressive%2520domains%252C%2520such%2520as%2520music%2520performance%252C%250Aremains%2520largely%2520unexplored.%2520Musical%2520tasks%252C%2520like%2520drumming%252C%2520present%2520unique%250Achallenges%252C%2520including%2520split-second%2520timing%252C%2520rapid%2520contacts%252C%2520and%2520multi-limb%250Acoordination%2520over%2520pieces%2520lasting%2520minutes.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Robot%250ADrummer%252C%2520a%2520humanoid%2520system%2520capable%2520of%2520expressive%252C%2520high-precision%2520drumming%250Aacross%2520a%2520diverse%2520repertoire%2520of%2520songs.%2520We%2520formulate%2520humanoid%2520drumming%2520as%250Asequential%2520fulfillment%2520of%2520timed-contacts%2520and%2520transform%2520drum%2520scores%2520in%2520to%2520a%250ARhythmic%2520Contact%2520Chain.%2520To%2520handle%2520the%2520long-horizon%2520nature%2520of%2520musical%250Aperformance%252C%2520we%2520decompose%2520each%2520piece%2520into%2520fixed-length%2520segments%2520and%2520train%2520a%250Asingle%2520policy%2520across%2520all%2520segments%2520in%2520parallel%2520using%2520reinforcement%2520learning.%250AThrough%2520extensive%2520experiments%2520on%2520over%2520thirty%2520popular%2520rock%252C%2520metal%252C%2520and%2520jazz%250Atracks%252C%2520our%2520results%2520demonstrate%2520that%2520Robot%2520Drummer%2520consistently%2520achieves%2520high%250AF1%2520scores.%2520The%2520learned%2520behaviors%2520exhibit%2520emergent%2520human-like%2520drumming%250Astrategies%252C%2520such%2520as%2520cross-arm%2520strikes%252C%2520and%2520adaptive%2520sticks%2520assignments%252C%250Ademonstrating%2520the%2520potential%2520of%2520reinforcement%2520learning%2520to%2520bring%2520humanoid%2520robots%250Ainto%2520the%2520domain%2520of%2520creative%2520musical%2520performance.%2520Project%2520page%253A%250A%255Chref%257Bhttps%253A//robot-drummer.github.io%257D%257Brobot-drummer.github.io%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robot%20Drummer%3A%20Learning%20Rhythmic%20Skills%20for%20Humanoid%20Drumming&entry.906535625=Asad%20Ali%20Shahid%20and%20Francesco%20Braghin%20and%20Loris%20Roveda&entry.1292438233=%20%20Humanoid%20robots%20have%20seen%20remarkable%20advances%20in%20dexterity%2C%20balance%2C%20and%0Alocomotion%2C%20yet%20their%20role%20in%20expressive%20domains%2C%20such%20as%20music%20performance%2C%0Aremains%20largely%20unexplored.%20Musical%20tasks%2C%20like%20drumming%2C%20present%20unique%0Achallenges%2C%20including%20split-second%20timing%2C%20rapid%20contacts%2C%20and%20multi-limb%0Acoordination%20over%20pieces%20lasting%20minutes.%20In%20this%20paper%2C%20we%20introduce%20Robot%0ADrummer%2C%20a%20humanoid%20system%20capable%20of%20expressive%2C%20high-precision%20drumming%0Aacross%20a%20diverse%20repertoire%20of%20songs.%20We%20formulate%20humanoid%20drumming%20as%0Asequential%20fulfillment%20of%20timed-contacts%20and%20transform%20drum%20scores%20in%20to%20a%0ARhythmic%20Contact%20Chain.%20To%20handle%20the%20long-horizon%20nature%20of%20musical%0Aperformance%2C%20we%20decompose%20each%20piece%20into%20fixed-length%20segments%20and%20train%20a%0Asingle%20policy%20across%20all%20segments%20in%20parallel%20using%20reinforcement%20learning.%0AThrough%20extensive%20experiments%20on%20over%20thirty%20popular%20rock%2C%20metal%2C%20and%20jazz%0Atracks%2C%20our%20results%20demonstrate%20that%20Robot%20Drummer%20consistently%20achieves%20high%0AF1%20scores.%20The%20learned%20behaviors%20exhibit%20emergent%20human-like%20drumming%0Astrategies%2C%20such%20as%20cross-arm%20strikes%2C%20and%20adaptive%20sticks%20assignments%2C%0Ademonstrating%20the%20potential%20of%20reinforcement%20learning%20to%20bring%20humanoid%20robots%0Ainto%20the%20domain%20of%20creative%20musical%20performance.%20Project%20page%3A%0A%5Chref%7Bhttps%3A//robot-drummer.github.io%7D%7Brobot-drummer.github.io%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11498v1&entry.124074799=Read"},
{"title": "AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of\n  LLM over the Air", "author": "Shiyi Yang and Xiaoxue Yu and Rongpeng Li and Jianhang Zhu and Zhifeng Zhao and Honggang Zhang", "abstract": "  Operating Large Language Models (LLMs) on edge devices is increasingly\nchallenged by limited communication bandwidth and strained computational and\nmemory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.\nNevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ\nfixed or heuristic rank configurations, and the subsequent over-the-air\ntransmission of all LoRA parameters could be rather inefficient. To address\nthis limitation, we develop AirLLM, a hierarchical diffusion policy framework\nfor communication-aware LoRA adaptation. Specifically, AirLLM models the rank\nconfiguration as a structured action vector that spans all LoRA-inserted\nprojections. To solve the underlying high-dimensional sequential\ndecision-making problem, a Proximal Policy Optimization (PPO) agent generates\ncoarse-grained decisions by jointly observing wireless states and linguistic\ncomplexity, which are then refined via Denoising Diffusion Implicit Models\n(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The\ntwo modules are optimized alternatively, with the DDIM trained under the\nClassifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.\nExperiments under varying signal-to-noise ratios demonstrate that AirLLM\nconsistently enhances fine-tuning performance while significantly reducing\ntransmission costs, highlighting the effectiveness of reinforcement-driven,\ndiffusion-refined rank adaptation for scalable and efficient remote fine-tuning\nover the air.\n", "link": "http://arxiv.org/abs/2507.11515v1", "date": "2025-07-15", "relevancy": 1.433, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4962}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.475}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AirLLM%3A%20Diffusion%20Policy-based%20Adaptive%20LoRA%20for%20Remote%20Fine-Tuning%20of%0A%20%20LLM%20over%20the%20Air&body=Title%3A%20AirLLM%3A%20Diffusion%20Policy-based%20Adaptive%20LoRA%20for%20Remote%20Fine-Tuning%20of%0A%20%20LLM%20over%20the%20Air%0AAuthor%3A%20Shiyi%20Yang%20and%20Xiaoxue%20Yu%20and%20Rongpeng%20Li%20and%20Jianhang%20Zhu%20and%20Zhifeng%20Zhao%20and%20Honggang%20Zhang%0AAbstract%3A%20%20%20Operating%20Large%20Language%20Models%20%28LLMs%29%20on%20edge%20devices%20is%20increasingly%0Achallenged%20by%20limited%20communication%20bandwidth%20and%20strained%20computational%20and%0Amemory%20costs.%20Thus%2C%20cloud-assisted%20remote%20fine-tuning%20becomes%20indispensable.%0ANevertheless%2C%20existing%20Low-Rank%20Adaptation%20%28LoRA%29%20approaches%20typically%20employ%0Afixed%20or%20heuristic%20rank%20configurations%2C%20and%20the%20subsequent%20over-the-air%0Atransmission%20of%20all%20LoRA%20parameters%20could%20be%20rather%20inefficient.%20To%20address%0Athis%20limitation%2C%20we%20develop%20AirLLM%2C%20a%20hierarchical%20diffusion%20policy%20framework%0Afor%20communication-aware%20LoRA%20adaptation.%20Specifically%2C%20AirLLM%20models%20the%20rank%0Aconfiguration%20as%20a%20structured%20action%20vector%20that%20spans%20all%20LoRA-inserted%0Aprojections.%20To%20solve%20the%20underlying%20high-dimensional%20sequential%0Adecision-making%20problem%2C%20a%20Proximal%20Policy%20Optimization%20%28PPO%29%20agent%20generates%0Acoarse-grained%20decisions%20by%20jointly%20observing%20wireless%20states%20and%20linguistic%0Acomplexity%2C%20which%20are%20then%20refined%20via%20Denoising%20Diffusion%20Implicit%20Models%0A%28DDIM%29%20to%20produce%20high-resolution%2C%20task-%20and%20channel-adaptive%20rank%20vectors.%20The%0Atwo%20modules%20are%20optimized%20alternatively%2C%20with%20the%20DDIM%20trained%20under%20the%0AClassifier-Free%20Guidance%20%28CFG%29%20paradigm%20to%20maintain%20alignment%20with%20PPO%20rewards.%0AExperiments%20under%20varying%20signal-to-noise%20ratios%20demonstrate%20that%20AirLLM%0Aconsistently%20enhances%20fine-tuning%20performance%20while%20significantly%20reducing%0Atransmission%20costs%2C%20highlighting%20the%20effectiveness%20of%20reinforcement-driven%2C%0Adiffusion-refined%20rank%20adaptation%20for%20scalable%20and%20efficient%20remote%20fine-tuning%0Aover%20the%20air.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11515v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAirLLM%253A%2520Diffusion%2520Policy-based%2520Adaptive%2520LoRA%2520for%2520Remote%2520Fine-Tuning%2520of%250A%2520%2520LLM%2520over%2520the%2520Air%26entry.906535625%3DShiyi%2520Yang%2520and%2520Xiaoxue%2520Yu%2520and%2520Rongpeng%2520Li%2520and%2520Jianhang%2520Zhu%2520and%2520Zhifeng%2520Zhao%2520and%2520Honggang%2520Zhang%26entry.1292438233%3D%2520%2520Operating%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520on%2520edge%2520devices%2520is%2520increasingly%250Achallenged%2520by%2520limited%2520communication%2520bandwidth%2520and%2520strained%2520computational%2520and%250Amemory%2520costs.%2520Thus%252C%2520cloud-assisted%2520remote%2520fine-tuning%2520becomes%2520indispensable.%250ANevertheless%252C%2520existing%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520approaches%2520typically%2520employ%250Afixed%2520or%2520heuristic%2520rank%2520configurations%252C%2520and%2520the%2520subsequent%2520over-the-air%250Atransmission%2520of%2520all%2520LoRA%2520parameters%2520could%2520be%2520rather%2520inefficient.%2520To%2520address%250Athis%2520limitation%252C%2520we%2520develop%2520AirLLM%252C%2520a%2520hierarchical%2520diffusion%2520policy%2520framework%250Afor%2520communication-aware%2520LoRA%2520adaptation.%2520Specifically%252C%2520AirLLM%2520models%2520the%2520rank%250Aconfiguration%2520as%2520a%2520structured%2520action%2520vector%2520that%2520spans%2520all%2520LoRA-inserted%250Aprojections.%2520To%2520solve%2520the%2520underlying%2520high-dimensional%2520sequential%250Adecision-making%2520problem%252C%2520a%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529%2520agent%2520generates%250Acoarse-grained%2520decisions%2520by%2520jointly%2520observing%2520wireless%2520states%2520and%2520linguistic%250Acomplexity%252C%2520which%2520are%2520then%2520refined%2520via%2520Denoising%2520Diffusion%2520Implicit%2520Models%250A%2528DDIM%2529%2520to%2520produce%2520high-resolution%252C%2520task-%2520and%2520channel-adaptive%2520rank%2520vectors.%2520The%250Atwo%2520modules%2520are%2520optimized%2520alternatively%252C%2520with%2520the%2520DDIM%2520trained%2520under%2520the%250AClassifier-Free%2520Guidance%2520%2528CFG%2529%2520paradigm%2520to%2520maintain%2520alignment%2520with%2520PPO%2520rewards.%250AExperiments%2520under%2520varying%2520signal-to-noise%2520ratios%2520demonstrate%2520that%2520AirLLM%250Aconsistently%2520enhances%2520fine-tuning%2520performance%2520while%2520significantly%2520reducing%250Atransmission%2520costs%252C%2520highlighting%2520the%2520effectiveness%2520of%2520reinforcement-driven%252C%250Adiffusion-refined%2520rank%2520adaptation%2520for%2520scalable%2520and%2520efficient%2520remote%2520fine-tuning%250Aover%2520the%2520air.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11515v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AirLLM%3A%20Diffusion%20Policy-based%20Adaptive%20LoRA%20for%20Remote%20Fine-Tuning%20of%0A%20%20LLM%20over%20the%20Air&entry.906535625=Shiyi%20Yang%20and%20Xiaoxue%20Yu%20and%20Rongpeng%20Li%20and%20Jianhang%20Zhu%20and%20Zhifeng%20Zhao%20and%20Honggang%20Zhang&entry.1292438233=%20%20Operating%20Large%20Language%20Models%20%28LLMs%29%20on%20edge%20devices%20is%20increasingly%0Achallenged%20by%20limited%20communication%20bandwidth%20and%20strained%20computational%20and%0Amemory%20costs.%20Thus%2C%20cloud-assisted%20remote%20fine-tuning%20becomes%20indispensable.%0ANevertheless%2C%20existing%20Low-Rank%20Adaptation%20%28LoRA%29%20approaches%20typically%20employ%0Afixed%20or%20heuristic%20rank%20configurations%2C%20and%20the%20subsequent%20over-the-air%0Atransmission%20of%20all%20LoRA%20parameters%20could%20be%20rather%20inefficient.%20To%20address%0Athis%20limitation%2C%20we%20develop%20AirLLM%2C%20a%20hierarchical%20diffusion%20policy%20framework%0Afor%20communication-aware%20LoRA%20adaptation.%20Specifically%2C%20AirLLM%20models%20the%20rank%0Aconfiguration%20as%20a%20structured%20action%20vector%20that%20spans%20all%20LoRA-inserted%0Aprojections.%20To%20solve%20the%20underlying%20high-dimensional%20sequential%0Adecision-making%20problem%2C%20a%20Proximal%20Policy%20Optimization%20%28PPO%29%20agent%20generates%0Acoarse-grained%20decisions%20by%20jointly%20observing%20wireless%20states%20and%20linguistic%0Acomplexity%2C%20which%20are%20then%20refined%20via%20Denoising%20Diffusion%20Implicit%20Models%0A%28DDIM%29%20to%20produce%20high-resolution%2C%20task-%20and%20channel-adaptive%20rank%20vectors.%20The%0Atwo%20modules%20are%20optimized%20alternatively%2C%20with%20the%20DDIM%20trained%20under%20the%0AClassifier-Free%20Guidance%20%28CFG%29%20paradigm%20to%20maintain%20alignment%20with%20PPO%20rewards.%0AExperiments%20under%20varying%20signal-to-noise%20ratios%20demonstrate%20that%20AirLLM%0Aconsistently%20enhances%20fine-tuning%20performance%20while%20significantly%20reducing%0Atransmission%20costs%2C%20highlighting%20the%20effectiveness%20of%20reinforcement-driven%2C%0Adiffusion-refined%20rank%20adaptation%20for%20scalable%20and%20efficient%20remote%20fine-tuning%0Aover%20the%20air.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11515v1&entry.124074799=Read"},
{"title": "D3FL: Data Distribution and Detrending for Robust Federated Learning in\n  Non-linear Time-series Data", "author": "Harsha Varun Marisetty and Manik Gupta and Yogesh Simmhan", "abstract": "  With advancements in computing and communication technologies, the Internet\nof Things (IoT) has seen significant growth. IoT devices typically collect data\nfrom various sensors, such as temperature, humidity, and energy meters. Much of\nthis data is temporal in nature. Traditionally, data from IoT devices is\ncentralized for analysis, but this approach introduces delays and increased\ncommunication costs. Federated learning (FL) has emerged as an effective\nalternative, allowing for model training across distributed devices without the\nneed to centralize data. In many applications, such as smart home energy and\nenvironmental monitoring, the data collected by IoT devices across different\nlocations can exhibit significant variation in trends and seasonal patterns.\nAccurately forecasting such non-stationary, non-linear time-series data is\ncrucial for applications like energy consumption estimation and weather\nforecasting. However, these data variations can severely impact prediction\naccuracy. The key contributions of this paper are: (1) Investigating how\nnon-linear, non-stationary time-series data distributions, like generalized\nextreme value (gen-extreme) and log norm distributions, affect FL performance.\n(2) Analyzing how different detrending techniques for non-linear time-series\ndata influence the forecasting model's performance in a FL setup. We generated\nseveral synthetic time-series datasets using non-linear data distributions and\ntrained an LSTM-based forecasting model using both centralized and FL\napproaches. Additionally, we evaluated the impact of detrending on real-world\ndatasets with non-linear time-series data distributions. Our experimental\nresults show that: (1) FL performs worse than centralized approaches when\ndealing with non-linear data distributions. (2) The use of appropriate\ndetrending techniques improves FL performance, reducing loss across different\ndata distributions.\n", "link": "http://arxiv.org/abs/2507.11471v1", "date": "2025-07-15", "relevancy": 1.4163, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4888}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4698}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D3FL%3A%20Data%20Distribution%20and%20Detrending%20for%20Robust%20Federated%20Learning%20in%0A%20%20Non-linear%20Time-series%20Data&body=Title%3A%20D3FL%3A%20Data%20Distribution%20and%20Detrending%20for%20Robust%20Federated%20Learning%20in%0A%20%20Non-linear%20Time-series%20Data%0AAuthor%3A%20Harsha%20Varun%20Marisetty%20and%20Manik%20Gupta%20and%20Yogesh%20Simmhan%0AAbstract%3A%20%20%20With%20advancements%20in%20computing%20and%20communication%20technologies%2C%20the%20Internet%0Aof%20Things%20%28IoT%29%20has%20seen%20significant%20growth.%20IoT%20devices%20typically%20collect%20data%0Afrom%20various%20sensors%2C%20such%20as%20temperature%2C%20humidity%2C%20and%20energy%20meters.%20Much%20of%0Athis%20data%20is%20temporal%20in%20nature.%20Traditionally%2C%20data%20from%20IoT%20devices%20is%0Acentralized%20for%20analysis%2C%20but%20this%20approach%20introduces%20delays%20and%20increased%0Acommunication%20costs.%20Federated%20learning%20%28FL%29%20has%20emerged%20as%20an%20effective%0Aalternative%2C%20allowing%20for%20model%20training%20across%20distributed%20devices%20without%20the%0Aneed%20to%20centralize%20data.%20In%20many%20applications%2C%20such%20as%20smart%20home%20energy%20and%0Aenvironmental%20monitoring%2C%20the%20data%20collected%20by%20IoT%20devices%20across%20different%0Alocations%20can%20exhibit%20significant%20variation%20in%20trends%20and%20seasonal%20patterns.%0AAccurately%20forecasting%20such%20non-stationary%2C%20non-linear%20time-series%20data%20is%0Acrucial%20for%20applications%20like%20energy%20consumption%20estimation%20and%20weather%0Aforecasting.%20However%2C%20these%20data%20variations%20can%20severely%20impact%20prediction%0Aaccuracy.%20The%20key%20contributions%20of%20this%20paper%20are%3A%20%281%29%20Investigating%20how%0Anon-linear%2C%20non-stationary%20time-series%20data%20distributions%2C%20like%20generalized%0Aextreme%20value%20%28gen-extreme%29%20and%20log%20norm%20distributions%2C%20affect%20FL%20performance.%0A%282%29%20Analyzing%20how%20different%20detrending%20techniques%20for%20non-linear%20time-series%0Adata%20influence%20the%20forecasting%20model%27s%20performance%20in%20a%20FL%20setup.%20We%20generated%0Aseveral%20synthetic%20time-series%20datasets%20using%20non-linear%20data%20distributions%20and%0Atrained%20an%20LSTM-based%20forecasting%20model%20using%20both%20centralized%20and%20FL%0Aapproaches.%20Additionally%2C%20we%20evaluated%20the%20impact%20of%20detrending%20on%20real-world%0Adatasets%20with%20non-linear%20time-series%20data%20distributions.%20Our%20experimental%0Aresults%20show%20that%3A%20%281%29%20FL%20performs%20worse%20than%20centralized%20approaches%20when%0Adealing%20with%20non-linear%20data%20distributions.%20%282%29%20The%20use%20of%20appropriate%0Adetrending%20techniques%20improves%20FL%20performance%2C%20reducing%20loss%20across%20different%0Adata%20distributions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11471v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD3FL%253A%2520Data%2520Distribution%2520and%2520Detrending%2520for%2520Robust%2520Federated%2520Learning%2520in%250A%2520%2520Non-linear%2520Time-series%2520Data%26entry.906535625%3DHarsha%2520Varun%2520Marisetty%2520and%2520Manik%2520Gupta%2520and%2520Yogesh%2520Simmhan%26entry.1292438233%3D%2520%2520With%2520advancements%2520in%2520computing%2520and%2520communication%2520technologies%252C%2520the%2520Internet%250Aof%2520Things%2520%2528IoT%2529%2520has%2520seen%2520significant%2520growth.%2520IoT%2520devices%2520typically%2520collect%2520data%250Afrom%2520various%2520sensors%252C%2520such%2520as%2520temperature%252C%2520humidity%252C%2520and%2520energy%2520meters.%2520Much%2520of%250Athis%2520data%2520is%2520temporal%2520in%2520nature.%2520Traditionally%252C%2520data%2520from%2520IoT%2520devices%2520is%250Acentralized%2520for%2520analysis%252C%2520but%2520this%2520approach%2520introduces%2520delays%2520and%2520increased%250Acommunication%2520costs.%2520Federated%2520learning%2520%2528FL%2529%2520has%2520emerged%2520as%2520an%2520effective%250Aalternative%252C%2520allowing%2520for%2520model%2520training%2520across%2520distributed%2520devices%2520without%2520the%250Aneed%2520to%2520centralize%2520data.%2520In%2520many%2520applications%252C%2520such%2520as%2520smart%2520home%2520energy%2520and%250Aenvironmental%2520monitoring%252C%2520the%2520data%2520collected%2520by%2520IoT%2520devices%2520across%2520different%250Alocations%2520can%2520exhibit%2520significant%2520variation%2520in%2520trends%2520and%2520seasonal%2520patterns.%250AAccurately%2520forecasting%2520such%2520non-stationary%252C%2520non-linear%2520time-series%2520data%2520is%250Acrucial%2520for%2520applications%2520like%2520energy%2520consumption%2520estimation%2520and%2520weather%250Aforecasting.%2520However%252C%2520these%2520data%2520variations%2520can%2520severely%2520impact%2520prediction%250Aaccuracy.%2520The%2520key%2520contributions%2520of%2520this%2520paper%2520are%253A%2520%25281%2529%2520Investigating%2520how%250Anon-linear%252C%2520non-stationary%2520time-series%2520data%2520distributions%252C%2520like%2520generalized%250Aextreme%2520value%2520%2528gen-extreme%2529%2520and%2520log%2520norm%2520distributions%252C%2520affect%2520FL%2520performance.%250A%25282%2529%2520Analyzing%2520how%2520different%2520detrending%2520techniques%2520for%2520non-linear%2520time-series%250Adata%2520influence%2520the%2520forecasting%2520model%2527s%2520performance%2520in%2520a%2520FL%2520setup.%2520We%2520generated%250Aseveral%2520synthetic%2520time-series%2520datasets%2520using%2520non-linear%2520data%2520distributions%2520and%250Atrained%2520an%2520LSTM-based%2520forecasting%2520model%2520using%2520both%2520centralized%2520and%2520FL%250Aapproaches.%2520Additionally%252C%2520we%2520evaluated%2520the%2520impact%2520of%2520detrending%2520on%2520real-world%250Adatasets%2520with%2520non-linear%2520time-series%2520data%2520distributions.%2520Our%2520experimental%250Aresults%2520show%2520that%253A%2520%25281%2529%2520FL%2520performs%2520worse%2520than%2520centralized%2520approaches%2520when%250Adealing%2520with%2520non-linear%2520data%2520distributions.%2520%25282%2529%2520The%2520use%2520of%2520appropriate%250Adetrending%2520techniques%2520improves%2520FL%2520performance%252C%2520reducing%2520loss%2520across%2520different%250Adata%2520distributions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11471v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D3FL%3A%20Data%20Distribution%20and%20Detrending%20for%20Robust%20Federated%20Learning%20in%0A%20%20Non-linear%20Time-series%20Data&entry.906535625=Harsha%20Varun%20Marisetty%20and%20Manik%20Gupta%20and%20Yogesh%20Simmhan&entry.1292438233=%20%20With%20advancements%20in%20computing%20and%20communication%20technologies%2C%20the%20Internet%0Aof%20Things%20%28IoT%29%20has%20seen%20significant%20growth.%20IoT%20devices%20typically%20collect%20data%0Afrom%20various%20sensors%2C%20such%20as%20temperature%2C%20humidity%2C%20and%20energy%20meters.%20Much%20of%0Athis%20data%20is%20temporal%20in%20nature.%20Traditionally%2C%20data%20from%20IoT%20devices%20is%0Acentralized%20for%20analysis%2C%20but%20this%20approach%20introduces%20delays%20and%20increased%0Acommunication%20costs.%20Federated%20learning%20%28FL%29%20has%20emerged%20as%20an%20effective%0Aalternative%2C%20allowing%20for%20model%20training%20across%20distributed%20devices%20without%20the%0Aneed%20to%20centralize%20data.%20In%20many%20applications%2C%20such%20as%20smart%20home%20energy%20and%0Aenvironmental%20monitoring%2C%20the%20data%20collected%20by%20IoT%20devices%20across%20different%0Alocations%20can%20exhibit%20significant%20variation%20in%20trends%20and%20seasonal%20patterns.%0AAccurately%20forecasting%20such%20non-stationary%2C%20non-linear%20time-series%20data%20is%0Acrucial%20for%20applications%20like%20energy%20consumption%20estimation%20and%20weather%0Aforecasting.%20However%2C%20these%20data%20variations%20can%20severely%20impact%20prediction%0Aaccuracy.%20The%20key%20contributions%20of%20this%20paper%20are%3A%20%281%29%20Investigating%20how%0Anon-linear%2C%20non-stationary%20time-series%20data%20distributions%2C%20like%20generalized%0Aextreme%20value%20%28gen-extreme%29%20and%20log%20norm%20distributions%2C%20affect%20FL%20performance.%0A%282%29%20Analyzing%20how%20different%20detrending%20techniques%20for%20non-linear%20time-series%0Adata%20influence%20the%20forecasting%20model%27s%20performance%20in%20a%20FL%20setup.%20We%20generated%0Aseveral%20synthetic%20time-series%20datasets%20using%20non-linear%20data%20distributions%20and%0Atrained%20an%20LSTM-based%20forecasting%20model%20using%20both%20centralized%20and%20FL%0Aapproaches.%20Additionally%2C%20we%20evaluated%20the%20impact%20of%20detrending%20on%20real-world%0Adatasets%20with%20non-linear%20time-series%20data%20distributions.%20Our%20experimental%0Aresults%20show%20that%3A%20%281%29%20FL%20performs%20worse%20than%20centralized%20approaches%20when%0Adealing%20with%20non-linear%20data%20distributions.%20%282%29%20The%20use%20of%20appropriate%0Adetrending%20techniques%20improves%20FL%20performance%2C%20reducing%20loss%20across%20different%0Adata%20distributions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11471v1&entry.124074799=Read"},
{"title": "Training neural control variates using correlated configurations", "author": "Hyunwoo Oh", "abstract": "  Neural control variates (NCVs) have emerged as a powerful tool for variance\nreduction in Monte Carlo (MC) simulations, particularly in high-dimensional\nproblems where traditional control variates are difficult to construct\nanalytically. By training neural networks to learn auxiliary functions\ncorrelated with the target observable, NCVs can significantly reduce estimator\nvariance while preserving unbiasedness. However, a critical but often\noverlooked aspect of NCV training is the role of autocorrelated samples\ngenerated by Markov Chain Monte Carlo (MCMC). While such samples are typically\ndiscarded for error estimation due to their statistical redundancy, they may\ncontain useful information about the structure of the underlying probability\ndistribution that can benefit the training process. In this work, we\nsystematically examine the effect of using correlated configurations in\ntraining neural control variates. We demonstrate, both conceptually and\nnumerically, that training on correlated data can improve control variate\nperformance, especially in settings with limited computational resources. Our\nanalysis includes empirical results from $U(1)$ gauge theory and scalar field\ntheory, illustrating when and how autocorrelated samples enhance NCV\nconstruction. These findings provide practical guidance for the efficient use\nof MCMC data in training neural networks.\n", "link": "http://arxiv.org/abs/2505.07719v3", "date": "2025-07-15", "relevancy": 1.3609, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4581}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4581}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20neural%20control%20variates%20using%20correlated%20configurations&body=Title%3A%20Training%20neural%20control%20variates%20using%20correlated%20configurations%0AAuthor%3A%20Hyunwoo%20Oh%0AAbstract%3A%20%20%20Neural%20control%20variates%20%28NCVs%29%20have%20emerged%20as%20a%20powerful%20tool%20for%20variance%0Areduction%20in%20Monte%20Carlo%20%28MC%29%20simulations%2C%20particularly%20in%20high-dimensional%0Aproblems%20where%20traditional%20control%20variates%20are%20difficult%20to%20construct%0Aanalytically.%20By%20training%20neural%20networks%20to%20learn%20auxiliary%20functions%0Acorrelated%20with%20the%20target%20observable%2C%20NCVs%20can%20significantly%20reduce%20estimator%0Avariance%20while%20preserving%20unbiasedness.%20However%2C%20a%20critical%20but%20often%0Aoverlooked%20aspect%20of%20NCV%20training%20is%20the%20role%20of%20autocorrelated%20samples%0Agenerated%20by%20Markov%20Chain%20Monte%20Carlo%20%28MCMC%29.%20While%20such%20samples%20are%20typically%0Adiscarded%20for%20error%20estimation%20due%20to%20their%20statistical%20redundancy%2C%20they%20may%0Acontain%20useful%20information%20about%20the%20structure%20of%20the%20underlying%20probability%0Adistribution%20that%20can%20benefit%20the%20training%20process.%20In%20this%20work%2C%20we%0Asystematically%20examine%20the%20effect%20of%20using%20correlated%20configurations%20in%0Atraining%20neural%20control%20variates.%20We%20demonstrate%2C%20both%20conceptually%20and%0Anumerically%2C%20that%20training%20on%20correlated%20data%20can%20improve%20control%20variate%0Aperformance%2C%20especially%20in%20settings%20with%20limited%20computational%20resources.%20Our%0Aanalysis%20includes%20empirical%20results%20from%20%24U%281%29%24%20gauge%20theory%20and%20scalar%20field%0Atheory%2C%20illustrating%20when%20and%20how%20autocorrelated%20samples%20enhance%20NCV%0Aconstruction.%20These%20findings%20provide%20practical%20guidance%20for%20the%20efficient%20use%0Aof%20MCMC%20data%20in%20training%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.07719v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520neural%2520control%2520variates%2520using%2520correlated%2520configurations%26entry.906535625%3DHyunwoo%2520Oh%26entry.1292438233%3D%2520%2520Neural%2520control%2520variates%2520%2528NCVs%2529%2520have%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520variance%250Areduction%2520in%2520Monte%2520Carlo%2520%2528MC%2529%2520simulations%252C%2520particularly%2520in%2520high-dimensional%250Aproblems%2520where%2520traditional%2520control%2520variates%2520are%2520difficult%2520to%2520construct%250Aanalytically.%2520By%2520training%2520neural%2520networks%2520to%2520learn%2520auxiliary%2520functions%250Acorrelated%2520with%2520the%2520target%2520observable%252C%2520NCVs%2520can%2520significantly%2520reduce%2520estimator%250Avariance%2520while%2520preserving%2520unbiasedness.%2520However%252C%2520a%2520critical%2520but%2520often%250Aoverlooked%2520aspect%2520of%2520NCV%2520training%2520is%2520the%2520role%2520of%2520autocorrelated%2520samples%250Agenerated%2520by%2520Markov%2520Chain%2520Monte%2520Carlo%2520%2528MCMC%2529.%2520While%2520such%2520samples%2520are%2520typically%250Adiscarded%2520for%2520error%2520estimation%2520due%2520to%2520their%2520statistical%2520redundancy%252C%2520they%2520may%250Acontain%2520useful%2520information%2520about%2520the%2520structure%2520of%2520the%2520underlying%2520probability%250Adistribution%2520that%2520can%2520benefit%2520the%2520training%2520process.%2520In%2520this%2520work%252C%2520we%250Asystematically%2520examine%2520the%2520effect%2520of%2520using%2520correlated%2520configurations%2520in%250Atraining%2520neural%2520control%2520variates.%2520We%2520demonstrate%252C%2520both%2520conceptually%2520and%250Anumerically%252C%2520that%2520training%2520on%2520correlated%2520data%2520can%2520improve%2520control%2520variate%250Aperformance%252C%2520especially%2520in%2520settings%2520with%2520limited%2520computational%2520resources.%2520Our%250Aanalysis%2520includes%2520empirical%2520results%2520from%2520%2524U%25281%2529%2524%2520gauge%2520theory%2520and%2520scalar%2520field%250Atheory%252C%2520illustrating%2520when%2520and%2520how%2520autocorrelated%2520samples%2520enhance%2520NCV%250Aconstruction.%2520These%2520findings%2520provide%2520practical%2520guidance%2520for%2520the%2520efficient%2520use%250Aof%2520MCMC%2520data%2520in%2520training%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.07719v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20neural%20control%20variates%20using%20correlated%20configurations&entry.906535625=Hyunwoo%20Oh&entry.1292438233=%20%20Neural%20control%20variates%20%28NCVs%29%20have%20emerged%20as%20a%20powerful%20tool%20for%20variance%0Areduction%20in%20Monte%20Carlo%20%28MC%29%20simulations%2C%20particularly%20in%20high-dimensional%0Aproblems%20where%20traditional%20control%20variates%20are%20difficult%20to%20construct%0Aanalytically.%20By%20training%20neural%20networks%20to%20learn%20auxiliary%20functions%0Acorrelated%20with%20the%20target%20observable%2C%20NCVs%20can%20significantly%20reduce%20estimator%0Avariance%20while%20preserving%20unbiasedness.%20However%2C%20a%20critical%20but%20often%0Aoverlooked%20aspect%20of%20NCV%20training%20is%20the%20role%20of%20autocorrelated%20samples%0Agenerated%20by%20Markov%20Chain%20Monte%20Carlo%20%28MCMC%29.%20While%20such%20samples%20are%20typically%0Adiscarded%20for%20error%20estimation%20due%20to%20their%20statistical%20redundancy%2C%20they%20may%0Acontain%20useful%20information%20about%20the%20structure%20of%20the%20underlying%20probability%0Adistribution%20that%20can%20benefit%20the%20training%20process.%20In%20this%20work%2C%20we%0Asystematically%20examine%20the%20effect%20of%20using%20correlated%20configurations%20in%0Atraining%20neural%20control%20variates.%20We%20demonstrate%2C%20both%20conceptually%20and%0Anumerically%2C%20that%20training%20on%20correlated%20data%20can%20improve%20control%20variate%0Aperformance%2C%20especially%20in%20settings%20with%20limited%20computational%20resources.%20Our%0Aanalysis%20includes%20empirical%20results%20from%20%24U%281%29%24%20gauge%20theory%20and%20scalar%20field%0Atheory%2C%20illustrating%20when%20and%20how%20autocorrelated%20samples%20enhance%20NCV%0Aconstruction.%20These%20findings%20provide%20practical%20guidance%20for%20the%20efficient%20use%0Aof%20MCMC%20data%20in%20training%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.07719v3&entry.124074799=Read"},
{"title": "Chain of Thought Monitorability: A New and Fragile Opportunity for AI\n  Safety", "author": "Tomek Korbak and Mikita Balesni and Elizabeth Barnes and Yoshua Bengio and Joe Benton and Joseph Bloom and Mark Chen and Alan Cooney and Allan Dafoe and Anca Dragan and Scott Emmons and Owain Evans and David Farhi and Ryan Greenblatt and Dan Hendrycks and Marius Hobbhahn and Evan Hubinger and Geoffrey Irving and Erik Jenner and Daniel Kokotajlo and Victoria Krakovna and Shane Legg and David Lindner and David Luan and Aleksander M\u0105dry and Julian Michael and Neel Nanda and Dave Orr and Jakub Pachocki and Ethan Perez and Mary Phuong and Fabien Roger and Joshua Saxe and Buck Shlegeris and Mart\u00edn Soto and Eric Steinberger and Jasmine Wang and Wojciech Zaremba and Bowen Baker and Rohin Shah and Vlad Mikulik", "abstract": "  AI systems that \"think\" in human language offer a unique opportunity for AI\nsafety: we can monitor their chains of thought (CoT) for the intent to\nmisbehave. Like all other known AI oversight methods, CoT monitoring is\nimperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows\npromise and we recommend further research into CoT monitorability and\ninvestment in CoT monitoring alongside existing safety methods. Because CoT\nmonitorability may be fragile, we recommend that frontier model developers\nconsider the impact of development decisions on CoT monitorability.\n", "link": "http://arxiv.org/abs/2507.11473v1", "date": "2025-07-15", "relevancy": 1.3454, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.452}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4484}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chain%20of%20Thought%20Monitorability%3A%20A%20New%20and%20Fragile%20Opportunity%20for%20AI%0A%20%20Safety&body=Title%3A%20Chain%20of%20Thought%20Monitorability%3A%20A%20New%20and%20Fragile%20Opportunity%20for%20AI%0A%20%20Safety%0AAuthor%3A%20Tomek%20Korbak%20and%20Mikita%20Balesni%20and%20Elizabeth%20Barnes%20and%20Yoshua%20Bengio%20and%20Joe%20Benton%20and%20Joseph%20Bloom%20and%20Mark%20Chen%20and%20Alan%20Cooney%20and%20Allan%20Dafoe%20and%20Anca%20Dragan%20and%20Scott%20Emmons%20and%20Owain%20Evans%20and%20David%20Farhi%20and%20Ryan%20Greenblatt%20and%20Dan%20Hendrycks%20and%20Marius%20Hobbhahn%20and%20Evan%20Hubinger%20and%20Geoffrey%20Irving%20and%20Erik%20Jenner%20and%20Daniel%20Kokotajlo%20and%20Victoria%20Krakovna%20and%20Shane%20Legg%20and%20David%20Lindner%20and%20David%20Luan%20and%20Aleksander%20M%C4%85dry%20and%20Julian%20Michael%20and%20Neel%20Nanda%20and%20Dave%20Orr%20and%20Jakub%20Pachocki%20and%20Ethan%20Perez%20and%20Mary%20Phuong%20and%20Fabien%20Roger%20and%20Joshua%20Saxe%20and%20Buck%20Shlegeris%20and%20Mart%C3%ADn%20Soto%20and%20Eric%20Steinberger%20and%20Jasmine%20Wang%20and%20Wojciech%20Zaremba%20and%20Bowen%20Baker%20and%20Rohin%20Shah%20and%20Vlad%20Mikulik%0AAbstract%3A%20%20%20AI%20systems%20that%20%22think%22%20in%20human%20language%20offer%20a%20unique%20opportunity%20for%20AI%0Asafety%3A%20we%20can%20monitor%20their%20chains%20of%20thought%20%28CoT%29%20for%20the%20intent%20to%0Amisbehave.%20Like%20all%20other%20known%20AI%20oversight%20methods%2C%20CoT%20monitoring%20is%0Aimperfect%20and%20allows%20some%20misbehavior%20to%20go%20unnoticed.%20Nevertheless%2C%20it%20shows%0Apromise%20and%20we%20recommend%20further%20research%20into%20CoT%20monitorability%20and%0Ainvestment%20in%20CoT%20monitoring%20alongside%20existing%20safety%20methods.%20Because%20CoT%0Amonitorability%20may%20be%20fragile%2C%20we%20recommend%20that%20frontier%20model%20developers%0Aconsider%20the%20impact%20of%20development%20decisions%20on%20CoT%20monitorability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11473v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChain%2520of%2520Thought%2520Monitorability%253A%2520A%2520New%2520and%2520Fragile%2520Opportunity%2520for%2520AI%250A%2520%2520Safety%26entry.906535625%3DTomek%2520Korbak%2520and%2520Mikita%2520Balesni%2520and%2520Elizabeth%2520Barnes%2520and%2520Yoshua%2520Bengio%2520and%2520Joe%2520Benton%2520and%2520Joseph%2520Bloom%2520and%2520Mark%2520Chen%2520and%2520Alan%2520Cooney%2520and%2520Allan%2520Dafoe%2520and%2520Anca%2520Dragan%2520and%2520Scott%2520Emmons%2520and%2520Owain%2520Evans%2520and%2520David%2520Farhi%2520and%2520Ryan%2520Greenblatt%2520and%2520Dan%2520Hendrycks%2520and%2520Marius%2520Hobbhahn%2520and%2520Evan%2520Hubinger%2520and%2520Geoffrey%2520Irving%2520and%2520Erik%2520Jenner%2520and%2520Daniel%2520Kokotajlo%2520and%2520Victoria%2520Krakovna%2520and%2520Shane%2520Legg%2520and%2520David%2520Lindner%2520and%2520David%2520Luan%2520and%2520Aleksander%2520M%25C4%2585dry%2520and%2520Julian%2520Michael%2520and%2520Neel%2520Nanda%2520and%2520Dave%2520Orr%2520and%2520Jakub%2520Pachocki%2520and%2520Ethan%2520Perez%2520and%2520Mary%2520Phuong%2520and%2520Fabien%2520Roger%2520and%2520Joshua%2520Saxe%2520and%2520Buck%2520Shlegeris%2520and%2520Mart%25C3%25ADn%2520Soto%2520and%2520Eric%2520Steinberger%2520and%2520Jasmine%2520Wang%2520and%2520Wojciech%2520Zaremba%2520and%2520Bowen%2520Baker%2520and%2520Rohin%2520Shah%2520and%2520Vlad%2520Mikulik%26entry.1292438233%3D%2520%2520AI%2520systems%2520that%2520%2522think%2522%2520in%2520human%2520language%2520offer%2520a%2520unique%2520opportunity%2520for%2520AI%250Asafety%253A%2520we%2520can%2520monitor%2520their%2520chains%2520of%2520thought%2520%2528CoT%2529%2520for%2520the%2520intent%2520to%250Amisbehave.%2520Like%2520all%2520other%2520known%2520AI%2520oversight%2520methods%252C%2520CoT%2520monitoring%2520is%250Aimperfect%2520and%2520allows%2520some%2520misbehavior%2520to%2520go%2520unnoticed.%2520Nevertheless%252C%2520it%2520shows%250Apromise%2520and%2520we%2520recommend%2520further%2520research%2520into%2520CoT%2520monitorability%2520and%250Ainvestment%2520in%2520CoT%2520monitoring%2520alongside%2520existing%2520safety%2520methods.%2520Because%2520CoT%250Amonitorability%2520may%2520be%2520fragile%252C%2520we%2520recommend%2520that%2520frontier%2520model%2520developers%250Aconsider%2520the%2520impact%2520of%2520development%2520decisions%2520on%2520CoT%2520monitorability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11473v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chain%20of%20Thought%20Monitorability%3A%20A%20New%20and%20Fragile%20Opportunity%20for%20AI%0A%20%20Safety&entry.906535625=Tomek%20Korbak%20and%20Mikita%20Balesni%20and%20Elizabeth%20Barnes%20and%20Yoshua%20Bengio%20and%20Joe%20Benton%20and%20Joseph%20Bloom%20and%20Mark%20Chen%20and%20Alan%20Cooney%20and%20Allan%20Dafoe%20and%20Anca%20Dragan%20and%20Scott%20Emmons%20and%20Owain%20Evans%20and%20David%20Farhi%20and%20Ryan%20Greenblatt%20and%20Dan%20Hendrycks%20and%20Marius%20Hobbhahn%20and%20Evan%20Hubinger%20and%20Geoffrey%20Irving%20and%20Erik%20Jenner%20and%20Daniel%20Kokotajlo%20and%20Victoria%20Krakovna%20and%20Shane%20Legg%20and%20David%20Lindner%20and%20David%20Luan%20and%20Aleksander%20M%C4%85dry%20and%20Julian%20Michael%20and%20Neel%20Nanda%20and%20Dave%20Orr%20and%20Jakub%20Pachocki%20and%20Ethan%20Perez%20and%20Mary%20Phuong%20and%20Fabien%20Roger%20and%20Joshua%20Saxe%20and%20Buck%20Shlegeris%20and%20Mart%C3%ADn%20Soto%20and%20Eric%20Steinberger%20and%20Jasmine%20Wang%20and%20Wojciech%20Zaremba%20and%20Bowen%20Baker%20and%20Rohin%20Shah%20and%20Vlad%20Mikulik&entry.1292438233=%20%20AI%20systems%20that%20%22think%22%20in%20human%20language%20offer%20a%20unique%20opportunity%20for%20AI%0Asafety%3A%20we%20can%20monitor%20their%20chains%20of%20thought%20%28CoT%29%20for%20the%20intent%20to%0Amisbehave.%20Like%20all%20other%20known%20AI%20oversight%20methods%2C%20CoT%20monitoring%20is%0Aimperfect%20and%20allows%20some%20misbehavior%20to%20go%20unnoticed.%20Nevertheless%2C%20it%20shows%0Apromise%20and%20we%20recommend%20further%20research%20into%20CoT%20monitorability%20and%0Ainvestment%20in%20CoT%20monitoring%20alongside%20existing%20safety%20methods.%20Because%20CoT%0Amonitorability%20may%20be%20fragile%2C%20we%20recommend%20that%20frontier%20model%20developers%0Aconsider%20the%20impact%20of%20development%20decisions%20on%20CoT%20monitorability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11473v1&entry.124074799=Read"},
{"title": "LLM-based ambiguity detection in natural language instructions for\n  collaborative surgical robots", "author": "Ana Davila and Jacinto Colan and Yasuhisa Hasegawa", "abstract": "  Ambiguity in natural language instructions poses significant risks in\nsafety-critical human-robot interaction, particularly in domains such as\nsurgery. To address this, we propose a framework that uses Large Language\nModels (LLMs) for ambiguity detection specifically designed for collaborative\nsurgical scenarios. Our method employs an ensemble of LLM evaluators, each\nconfigured with distinct prompting techniques to identify linguistic,\ncontextual, procedural, and critical ambiguities. A chain-of-thought evaluator\nis included to systematically analyze instruction structure for potential\nissues. Individual evaluator assessments are synthesized through conformal\nprediction, which yields non-conformity scores based on comparison to a labeled\ncalibration dataset. Evaluating Llama 3.2 11B and Gemma 3 12B, we observed\nclassification accuracy exceeding 60% in differentiating ambiguous from\nunambiguous surgical instructions. Our approach improves the safety and\nreliability of human-robot collaboration in surgery by offering a mechanism to\nidentify potentially ambiguous instructions before robot action.\n", "link": "http://arxiv.org/abs/2507.11525v1", "date": "2025-07-15", "relevancy": 1.1028, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5913}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5348}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-based%20ambiguity%20detection%20in%20natural%20language%20instructions%20for%0A%20%20collaborative%20surgical%20robots&body=Title%3A%20LLM-based%20ambiguity%20detection%20in%20natural%20language%20instructions%20for%0A%20%20collaborative%20surgical%20robots%0AAuthor%3A%20Ana%20Davila%20and%20Jacinto%20Colan%20and%20Yasuhisa%20Hasegawa%0AAbstract%3A%20%20%20Ambiguity%20in%20natural%20language%20instructions%20poses%20significant%20risks%20in%0Asafety-critical%20human-robot%20interaction%2C%20particularly%20in%20domains%20such%20as%0Asurgery.%20To%20address%20this%2C%20we%20propose%20a%20framework%20that%20uses%20Large%20Language%0AModels%20%28LLMs%29%20for%20ambiguity%20detection%20specifically%20designed%20for%20collaborative%0Asurgical%20scenarios.%20Our%20method%20employs%20an%20ensemble%20of%20LLM%20evaluators%2C%20each%0Aconfigured%20with%20distinct%20prompting%20techniques%20to%20identify%20linguistic%2C%0Acontextual%2C%20procedural%2C%20and%20critical%20ambiguities.%20A%20chain-of-thought%20evaluator%0Ais%20included%20to%20systematically%20analyze%20instruction%20structure%20for%20potential%0Aissues.%20Individual%20evaluator%20assessments%20are%20synthesized%20through%20conformal%0Aprediction%2C%20which%20yields%20non-conformity%20scores%20based%20on%20comparison%20to%20a%20labeled%0Acalibration%20dataset.%20Evaluating%20Llama%203.2%2011B%20and%20Gemma%203%2012B%2C%20we%20observed%0Aclassification%20accuracy%20exceeding%2060%25%20in%20differentiating%20ambiguous%20from%0Aunambiguous%20surgical%20instructions.%20Our%20approach%20improves%20the%20safety%20and%0Areliability%20of%20human-robot%20collaboration%20in%20surgery%20by%20offering%20a%20mechanism%20to%0Aidentify%20potentially%20ambiguous%20instructions%20before%20robot%20action.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.11525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-based%2520ambiguity%2520detection%2520in%2520natural%2520language%2520instructions%2520for%250A%2520%2520collaborative%2520surgical%2520robots%26entry.906535625%3DAna%2520Davila%2520and%2520Jacinto%2520Colan%2520and%2520Yasuhisa%2520Hasegawa%26entry.1292438233%3D%2520%2520Ambiguity%2520in%2520natural%2520language%2520instructions%2520poses%2520significant%2520risks%2520in%250Asafety-critical%2520human-robot%2520interaction%252C%2520particularly%2520in%2520domains%2520such%2520as%250Asurgery.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520framework%2520that%2520uses%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520for%2520ambiguity%2520detection%2520specifically%2520designed%2520for%2520collaborative%250Asurgical%2520scenarios.%2520Our%2520method%2520employs%2520an%2520ensemble%2520of%2520LLM%2520evaluators%252C%2520each%250Aconfigured%2520with%2520distinct%2520prompting%2520techniques%2520to%2520identify%2520linguistic%252C%250Acontextual%252C%2520procedural%252C%2520and%2520critical%2520ambiguities.%2520A%2520chain-of-thought%2520evaluator%250Ais%2520included%2520to%2520systematically%2520analyze%2520instruction%2520structure%2520for%2520potential%250Aissues.%2520Individual%2520evaluator%2520assessments%2520are%2520synthesized%2520through%2520conformal%250Aprediction%252C%2520which%2520yields%2520non-conformity%2520scores%2520based%2520on%2520comparison%2520to%2520a%2520labeled%250Acalibration%2520dataset.%2520Evaluating%2520Llama%25203.2%252011B%2520and%2520Gemma%25203%252012B%252C%2520we%2520observed%250Aclassification%2520accuracy%2520exceeding%252060%2525%2520in%2520differentiating%2520ambiguous%2520from%250Aunambiguous%2520surgical%2520instructions.%2520Our%2520approach%2520improves%2520the%2520safety%2520and%250Areliability%2520of%2520human-robot%2520collaboration%2520in%2520surgery%2520by%2520offering%2520a%2520mechanism%2520to%250Aidentify%2520potentially%2520ambiguous%2520instructions%2520before%2520robot%2520action.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.11525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-based%20ambiguity%20detection%20in%20natural%20language%20instructions%20for%0A%20%20collaborative%20surgical%20robots&entry.906535625=Ana%20Davila%20and%20Jacinto%20Colan%20and%20Yasuhisa%20Hasegawa&entry.1292438233=%20%20Ambiguity%20in%20natural%20language%20instructions%20poses%20significant%20risks%20in%0Asafety-critical%20human-robot%20interaction%2C%20particularly%20in%20domains%20such%20as%0Asurgery.%20To%20address%20this%2C%20we%20propose%20a%20framework%20that%20uses%20Large%20Language%0AModels%20%28LLMs%29%20for%20ambiguity%20detection%20specifically%20designed%20for%20collaborative%0Asurgical%20scenarios.%20Our%20method%20employs%20an%20ensemble%20of%20LLM%20evaluators%2C%20each%0Aconfigured%20with%20distinct%20prompting%20techniques%20to%20identify%20linguistic%2C%0Acontextual%2C%20procedural%2C%20and%20critical%20ambiguities.%20A%20chain-of-thought%20evaluator%0Ais%20included%20to%20systematically%20analyze%20instruction%20structure%20for%20potential%0Aissues.%20Individual%20evaluator%20assessments%20are%20synthesized%20through%20conformal%0Aprediction%2C%20which%20yields%20non-conformity%20scores%20based%20on%20comparison%20to%20a%20labeled%0Acalibration%20dataset.%20Evaluating%20Llama%203.2%2011B%20and%20Gemma%203%2012B%2C%20we%20observed%0Aclassification%20accuracy%20exceeding%2060%25%20in%20differentiating%20ambiguous%20from%0Aunambiguous%20surgical%20instructions.%20Our%20approach%20improves%20the%20safety%20and%0Areliability%20of%20human-robot%20collaboration%20in%20surgery%20by%20offering%20a%20mechanism%20to%0Aidentify%20potentially%20ambiguous%20instructions%20before%20robot%20action.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.11525v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


